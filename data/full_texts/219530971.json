{"id": 219530971, "updated": "2023-10-06 14:27:17.069", "metadata": {"title": "ColdGANs: Taming Language GANs with Cautious Sampling Strategies", "authors": "[{\"first\":\"Thomas\",\"last\":\"Scialom\",\"middle\":[]},{\"first\":\"Paul-Alexis\",\"last\":\"Dray\",\"middle\":[]},{\"first\":\"Sylvain\",\"last\":\"Lamprier\",\"middle\":[]},{\"first\":\"Benjamin\",\"last\":\"Piwowarski\",\"middle\":[]},{\"first\":\"Jacopo\",\"last\":\"Staiano\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 6, "day": 8}, "abstract": "Training regimes based on Maximum Likelihood Estimation (MLE) suffer from known limitations, often leading to poorly generated text sequences. At the root of these limitations is the mismatch between training and inference, i.e. the so-called exposure bias, exacerbated by considering only the reference texts as correct, while in practice several alternative formulations could be as good. Generative Adversarial Networks (GANs) can mitigate those limitations but the discrete nature of text has hindered their application to language generation: the approaches proposed so far, based on Reinforcement Learning, have been shown to underperform MLE. Departing from previous works, we analyze the exploration step in GANs applied to text generation, and show how classical sampling results in unstable training. We propose to consider alternative exploration strategies in a GAN framework that we name ColdGANs, where we force the sampling to be close to the distribution modes to get smoother learning dynamics. For the first time, to the best of our knowledge, the proposed language GANs compare favorably to MLE, and obtain improvements over the state-of-the-art on three generative tasks, namely unconditional text generation, question generation, and abstractive summarization.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2006.04643", "mag": "3101141546", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/ScialomDLPS20", "doi": null}}, "content": {"source": {"pdf_hash": "eb78dc546fe64099f1fba742a12551772b97783f", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2006.04643v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "8ce517914cadac9affe6fccf07ccb912db7cff29", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/eb78dc546fe64099f1fba742a12551772b97783f.txt", "contents": "\nColdGANs: Taming Language GANs with Cautious Sampling Strategies\n\n\nThomas Scialom thomas@recital.ai \nSorbonne Universit\u00e9\nCNRS\nFrance reciTALLIP6, F-75005Paris, ParisFrance\n\nPaul-Alexis Dray paul-alexis@recital.ai \nSylvain Lamprier sylvain.lamprier@lip6.fr \nSorbonne Universit\u00e9\nCNRS\nFrance reciTALLIP6, F-75005Paris, ParisFrance\n\nBenjamin Piwowarski benjamin.piwowarski@lip6.fr \nSorbonne Universit\u00e9\nCNRS\nFrance reciTALLIP6, F-75005Paris, ParisFrance\n\nJacopo Staiano jacopo@recital.ai \n\nCNRS\nFrance\n\nColdGANs: Taming Language GANs with Cautious Sampling Strategies\n\nTraining regimes based on Maximum Likelihood Estimation (MLE) suffer from known limitations, often leading to poorly generated text sequences. At the root of these limitations is the mismatch between training and inference, i.e. the so-called exposure bias, exacerbated by considering only the reference texts as correct, while in practice several alternative formulations could be as good. Generative Adversarial Networks (GANs) can mitigate those limitations but the discrete nature of text has hindered their application to language generation: the approaches proposed so far, based on Reinforcement Learning, have been shown to underperform MLE. Departing from previous works, we analyze the exploration step in GANs applied to text generation, and show how classical sampling results in unstable training. We propose to consider alternative exploration strategies in a GAN framework that we name ColdGAN s, where we force the sampling to be close to the distribution modes to get smoother learning dynamics. For the first time, to the best of our knowledge, the proposed language GANs compare favorably to MLE, and obtain improvements over the state-of-the-art on three generative tasks, namely unconditional text generation, question generation, and abstractive summarization.\n\nand do not preserve meaning [40]. Hence, when reinforced on them, models yield to poorer generations and higher degradation compared to their MLE counterparts [30]. To overcome these drawbacks, better rewards are thus necessary [30].\n\nTo this end, Ziegler et al. [61] proposed to directly reward systems using human judgment. Although this approach performs very well and approximates the best possible reward, it is obviously not a viable solution in practice. However, it attests that, with perfect rewards, one can achieve excellent levels of performance. A natural alternative, not requiring human judgments, is to frame the problem under the Generative Adversarial Network (GAN) paradigm [14], which has been used successfully for image generation [3]. For text, modeled as a sequence of discrete symbols, a naive computation of the gradients is however intractable. Hence, Language GANs are based on gradient estimation via RL-based techniques [53].\n\nHowever, the reward in this case can be extremely sparse (as discussed in Section 3.2), yielding to high-variance gradient estimation, which is known to be challenging for optimization [57]. Most previous works have focused on this aspect, and proposed denser rewards [20,23]. Unfortunately, these attempts to apply GANs to text generation obtained limited success [5] and have been found to underperform MLE [39,43,23].\n\nAlthough known to be crucial [42], exploration is surprisingly understudied when RL is applied to text generation. In this work, we propose a new exploration method that aims at sampling more structured rewards and that better suits the GANs' training dynamics, allowing for the first time to successfully train Language GANs. Our main contributions can be summarized as:\n\n1. We study the discriminators' behavior and show that their degree of specialization has important implications on the exploration to stabilize the training process. In particular, we find that reducing the exploration space is essential to successfully train discrete GANs.\n\n2. Based on these observations, we propose ColdGAN s, a GAN architecture using alternative sampling strategies that force the sampling to remain closer to the distribution modes.\n\n3. Finally, we apply our proposed methods on three tasks. We report positive results compared to previous works, including GANs and MLE-based models.\n\n\nRelated Work\n\nRL for text generation Since many metrics of interest in NLP are non-differentiable, several approaches used RL for text generation [8,34,30,6]. To our knowledge, all works based on RL for text generation use standard sampling for policy gradient estimation, following the current policy from the generator they define. Apart from text GANs, they all suffer from the aforementioned limitations of ill-defined reward metrics, such as BLEU or ROUGE [30].\n\nText GANs Tackling this problem by implicitly learning the metric via a discriminator, adversarial approaches have been proposed for text generation. Given the very high dimension of the generative (action) space, and the sparsity of associated rewards provided by the discriminator (see Section 3.2), a large body of works focused on defining denser rewards: ranking and comparative discriminators [20,59], sequential discriminators where the rewards are provided at each time step of the generation [39,23], or using masked language modeling [11]. The policy is usually learned via vanilla Policy Gradient REINFORCE [50], with the exception of MaliGAN [7], which Another difficulty with GANs for discrete sequential data is that discriminators are inaccurate for samples close to the generator distribution modes, as those used for training are usually too scattered over the full space to enable specialization on useful/difficult areas (see Section 3.2 for preliminary experiments on this).\n\nCautious RL Standard works in RL proposed ways to avoid catastrophic moves of the policy parameters [36,37], by enforcing the new policy to stay Importance Sampling for Reinforcement Learning In RL, IS is generally used for sample efficiency purposes: in off-policy policy gradient methods, IS allows to re-use previously sampled sequences more than once [48,44,12]. Conversely, in this work, IS is employed to improve the stability of RL for Text GANs. Closer to our work, MaliGAN [7] proposes to rely on IS to consider an estimation of the data distribution as a target (via a KL objective). Although theoretically appealing, its stability relies on very strong assumptions about discriminator guarantees, which rarely hold in practice. Instead, we propose to rely on IS to stabilize the generator-discriminator min-max game via alternative careful sampling strategies. Note also that our approach could easily be included in the MaliGAN framework.\n\n3 Discriminators and Generators Interaction 3.1 Generating and discriminating as text to text tasks Generator Text generation naturally lends itself to autoregressive modeling [41]. The probability to generate a sequence Y composed of N tokens y 1 , ..., y N is given by:\np \u03b8 (Y |X) = N t=1 p(y t |y 1 , ..., y t\u22121 , X, \u03b8)(1)\nwhere \u03b8 are the learnable parameters of the generator and X the input sequence.\n\nNeural networks typically produce class probabilities by using a \"softmax\" output layer that converts the logit z i , computed for each token of the vocabulary, into a probability q i :\nq i = exp (z i /T ) j exp (z j /T )(2)\nwhere T is a \"temperature\" hyper-parameter, set to 1 unless otherwise specified. The higher the temperature, the more uniform the probability distribution over the vocabulary, resulting in more diversity but also more mistakes [16]. In the following, we note as \u03c0 \u03b8 the distribution defined by the generator with temperature T = 1.\n\nDiscriminator In the following, we consider a discriminator D \u03c6 learned from sets of human and generated texts for each input X as a logistic regression problem:\n1 |H| (X,Y )\u2208H log(D \u03c6 (X, Y )) + 1 |G| (X,Y )\u2208G log(1 \u2212 D \u03c6 (X, Y ))\nwhere H is a set of pairs of input X associated with a human written text Y from the data distribution, and G is a set of pairs with generated outputs Y .\n\nText to text tasks Casting any NLP task as a text-to-text problem, T5 [32] demonstrated state-ofthe-art results on the established GLUE benchmark [47] and on its more challenging successor [46]. Accordingly, we employ the same architecture for both discrimination and generation. This allows for fairer comparisons thereafter, as both generator and discriminator have the same architecture, pre-training and capacity.\n\n\nDiscriminator-Generator Equilibrium\n\nExposure Bias As mentioned above, a discriminator can easily predict the human or machine nature of a text. One reason for this lies in exposure bias. To quantify this statement, we compare the results for a discriminator when trained under the two following generation strategies: Standard Generation, suffering from the exposure bias; and, Teacher Forcing Generation, where the groundtruth tokens y i<t are fed to the generator, so not to expose the model to its own prediction, and only y t is generated by a machine.\n\nWe show the results in Fig. 1. As expected, the two discriminators have the same score for t = 0. We observe that both perform well, and that the Standard Generation discriminator obtains consistently larger improvements, w.r.t. the Teacher Forcing Generation discriminator, as the length of the sequence increases. This could indicate the presence of the exposure bias, for which the errors accumulate over time. Still, the relatively high accuracy observed under Teacher Forcing Generation suggests that additional factors, beyond exposure bias, might be involved: in the following, we show that the extreme specialization of discriminators is among those.   \n\n\nEvaluated on human T\n= 0 T = 1 T = \u221e past T = 0 past T = 1 D T =0 .D perf ect 1 0 0 0 0 0\nDiscriminator's No Free Lunch As defined above, the temperature T of the generator is a hyperparameter which allows to control the randomness of predictions while sampling, by scaling the logits before applying a softmax. Thus, we can define various sampling strategies from the same generator. Low (close to 0) temperatures provide samples close to the sequence s greedy \u03b8 of a greedy procedure that takes the token with max generator probability \u03c0 \u03b8 at each step (the output of a beam search with beam size B = 1). With high temperatures, the distribution of sequences tends to the uniform distribution. We experiment with different temperature settings for the same generator (trained with MLE), and use the obtained samples to train and test a discriminator. This allows us to evaluate the impact of differences in sampling temperatures, between training and inference, on the discriminator performance. In other words, how a discriminator, trained with samples obtained at a specific temperature, performs when faced with samples generated under different sampling setups.\n\nWe train and evaluate discriminators on samples generated under temperatures T = 0, 1 or \u221e, for a conditional generation task (summarization, see Section 5.2), which allows to consider various sequence samples even at low temperatures. We report the results in Table 1. As expected, in all but one case, discriminators perform better if trained and evaluated with sequences generated under the same temperature (no mismatch). However, when the training and evaluation samples are generated with different temperatures, we observe that the discriminator fails to distinguish human from generated ones. More precisely, it considers most sentences to be human-generated (around 90%). Conversely, when trained on the different temperatures together (T \u2208 {0, 1, \u221e}), results are more balanced: robust across the various temperatures, but yielding a drop in accuracy, consistently with the well-known accuracy-robustness trade-off [13,4]. This highlights that individual discriminators are specialized on specific generation pairs (machine/human). Knowing this, it is crucial to orient this specialization on useful areas.\n\nInterestingly, when trained from samples issued from \u03c0 \u03b8 , the discriminator D T =1 is inaccurate at identifying samples close to s greedy \u03b8 as generated ones: D T =1 (s) equals 0.76 on average over these samples. This is particularly bad for a discriminator used as a reward signal of a RL process, since such samples lie in the useful area of the output distribution. They correspond to samples close to the modes of the distribution \u03c0 \u03b8 . Moreover, in many text generation applications, generation strategies such as beam search target these sequences as prediction outputs. A bad reward function at these locations is likely to lead to bad generation performance. Besides, the discriminator trained on samples close to the mode of \u03c0 \u03b8 (i.e., D T =0 ) is bad for samples from \u03c0 \u03b8 (i.e., T = 1), indicating that one cannot simply use such samples to train the discriminator while considering standard sampling for generator training (as rewards would be very inaccurate).\n\nImplications for Discrete GANs Holtzman et al. [17] report that for T = 1, sampling from the tail of the distribution is expected to happen within the first three steps of decoding and with a probability superior to 99.96% within 20 steps. Such unstructured exploration causes a large variance which grows with the number of time steps, and perturbs actions too frequently [35,18]. A less random exploration would thus yield to better structured sequences and lower variance, closer to the distribution learned by the discriminator, and would likely enable better training dynamics between the discriminator and the generator.\n\n\nModels\n\nBased on the findings above, we seek sampling strategies that allow both the discriminator to train on useful samples, and the generator to be trained from reliable rewards from the discriminator, within a policy gradient RL scheme where we are interested at maximizing J(\u03b8) = E \u03c4 \u223c\u03c0 \u03b8 [D \u03c6 (\u03c4 )], according to generator parameters \u03b8. The discriminator is updated at the end of each training epoch, via gradient ascent on human-machine pairs, with new artificial sequences resulting from the generator distribution. In order to introduce cautious sampling that focuses more on modes of distributions, note that it would be useless to consider the policy gradient\n\u2207 \u03b8 E \u03c4 \u223c\u03c0 T =\u03b3 \u03b8 [D \u03c6 (\u03c4 )] = E \u03c4 \u223c\u03c0 T =\u03b3 \u03b8 [D \u03c6 (\u03c4 )\u2207 \u03b8 log \u03c0 T =\u03b3 \u03b8 (\u03c4 )]\nof a generator distribution with modified temperature T = \u03b3, as it would, compared to T = 1, only imply rescaling the network outputs without altering the learning process.\n\nInstead, we propose to employ Importance Sampling for defining our cautious sampling strategies for text GANs, based on the fact that, for any distribution P, Q : X \u2192 [0, 1] such that Q(x) > 0 whenever P (x) > 0, and any function f :\nX \u2192 R, we have E x\u223cP (x) [f (x)] = E x\u223cQ(x) [ P (x) Q(x) f (x)\n]. In our case, this yields the following unbiased policy gradient:\n\u2207 \u03b8 J(\u03b8) = E \u03c4 \u223c\u03c0 \u03b8 \uf8ee \uf8f0 \u03c0 \u03b8 (\u03c4 ) \u03c0 \u03b8 (\u03c4 ) D \u03c6 (\u03c4 ) |\u03c4 |\u22121 t=1 \u2207 \u03b8 log \u03c0 \u03b8 (\u03c4 t |\u03c4 1:t\u22121 ) \uf8f9 \uf8fb(3)\nwhere \u03c4 t \u2208 V is the t-th token from sequence \u03c4 and \u03c4 1:t\u22121 the subsequence of its t \u2212 1 first tokens, \u03c0 \u03b8 the generator probability and\u03c0 \u03b8 a modified sampling distribution, which enables the generation of any possible sequence of tokens given the vocabulary V .\n\nIn this work, we focus on the exploration stage; therefore, conversely to previous works, we can choose the most sober form of reward: 1 if D \u03c6 (\u03c4 ) predicted human, and 0 otherwise. We show that a sparse reward is not a limitation if the sampling strategy is close to the modes of the distribution -provided the initial solution is a good enough bootstrap (which, according to our experiments, is the case). Note that D \u03c6 is trained with samples from\u03c0 \u03b8 to avoid any mismatch with the generator training samples, which would be problematic otherwise (as pointed out in Section 3.2).\n\n\nColdGANs exploration\n\nThe temperature T plays a major role in moderating exploration. Indeed, being a scaling factor applied to the generator outputs, it directly defines the degree of diversity of the generated sequences. The default exploration is obtained by recursively sampling a sequence of tokens from the model distribution with T = 1. The higher T , the more random the sampled sequences, regardless of the model's policy. Conversely, lower temperatures reduce the exploration, with T \u2192 0 ultimately equivalent to the argmax function. Therefore, we consider a distribution \u03c0 \u03b8 = \u03c0 T \u03b8 with lower (colder) temperatures T \u2208]0, 1[. This allows to explore sequences composed of tokens less likely to be sampled from\u03c0 \u03b8 tail. Note that for T > 0,\u03c0 \u03b8 > 0 whenever \u03c0 \u03b8 > 0.\n\nColdGANs nucleus In addition, we consider a more sophisticated technique: nucleus sampling [17]. This decoding method has been shown to produce higher quality texts than previous sampling strategies, including those temperature-based. Sampling from the nucleus of tokens containing the vast majority of the probability mass, the approach dynamically truncates the unreliable tail of the probability distribution and hence is an instance of a cautious generative process. However, with nucleus sampling, many sequences \u03c4 get\u03c0 \u03b8 (\u03c4 ) = 0 while \u03c0 \u03b8 (\u03c4 ) > 0, invalidating the IS. To avoid this, we propose to use a mixture combining low temperatures and nucleus policies:\n\u03c0 \u03b8 (\u03c4 ) = \u03c0 nucleus \u03b8 (\u03c4 ) + (1 \u2212 )\u03c0 T =\u03b3 \u03b8 (\u03c4 )(4)\nwhere is a hyper-parameter, \u03c0 nucleus \u03b8 is the probability under nucleus and \u03c0 T =\u03b3 \u03b8 the probability rescaled for temperature \u03b3, as described in the previous paragraph.\n\nImportance Weight Clipping The importance weights can become large, causing instability. Adapting from [48] (see paragraph 3.2 of their paper for more details), we truncate the importance weights and add a correction term in the computation of \u2207 \u03b8 J(\u03b8):\nE \u03c4 \u223c\u03c0 \u03b8 [min(c, w(\u03c4 ))D \u03c6 (\u03c4 )\u2207 log \u03c0 \u03b8 (\u03c4 )] + E \u03c4 \u223c\u03c0 \u03b8 max 0, w(\u03c4 ) \u2212 c w(\u03c4 ) D \u03c6 (\u03c4 )\u2207 log \u03c0 \u03b8 (\u03c4 )\nwhere w(\u03c4 ) = \u03c0 \u03b8 (\u03c4 ) \u03c0 \u03b8 (\u03c4 ) . In the first term of Eq. 4, by clipping the importance weight, the variance of the gradient estimate is bounded. The second term of the equation ensures that our estimate is unbiased, by re-sampling another sequence from the true policy \u03c0 \u03b8 . In our experiments, we set c = 5. Note that, contrary to off-policy RL, for which such a IS clipping was proposed [48], in our case clipping is very rare: it only occurs for sequences whose probability from the generator is much higher than the one from the sampling distribution, which is designed for sampling close to the mode of \u03c0 \u03b8 . However, if this happens, this clipping ensures that the corresponding gradient does not explode. Table 1, we observed that the performance of the discriminators is lower when evaluated on samples generated from the previous checkpoint of the same model (i.e., evaluated on past T ). We connect this to the failure mode in GANs observed by Metz et al. [25], where the generator and the discriminator oscillate during training, rather than converging to a fixed point. In lifelong learning literature [24], it has been shown that 1% of experience replay is sufficient to avoid catastrophic forgetting. Inspired by this work, we construct a memory buffer which contains samples generated in the last K training steps, and replace 1% of the discriminator training examples with samples from the buffer. This allows the discriminator to remain accurate on the samples from the previous state of the generator, hence preventing such failure loop during training.\n\n\nMemory Replay In\n\n\nExperiments\n\nDue to the computational cost of T5-large (11B parameters), we used T5-small (60M parameters). For all our experiments, we used the validation sets for hyperparameter selection. In more detail, we evaluated our approach with several learning rates, 1 reporting results for a value of 2e-5. From the best performing ColdGAN configuration, we perform ablations to assess the impact of Memory Replay and Importance Weight Clipping. Finally, we experimented with BART [19] instead of T5. 2 \n\n\nUnconditional Language Generation\n\nMost previous works for language GANs have been evaluated on unconditional language generation benchmarks. In this task, no input is provided and the goal is to generate both meaningful and diverse texts. Consistently with [23], we measure these two aspects using, respectively, BLEU [29] and self-BLEU [60] metrics. 3 the To obtain a finer comparison between models, Caccia et al. [5] proposed to draw the curve of (negative) BLEU vs self-BLEU, by sampling with various temperatures at inference. This allows to measure the trade-off between quality and diversity. Following [7,21,39,15,5,23], we used the EMNLP2017 news dataset. 4 We report ColdGAN s results in Figure 2 [5,23] w.r.t. GAN-based approaches for this task.\n\n\nConditional Language Generation\n\nWe evaluate ColdGAN s on two popular tasks where text inputs are given for conditioning the generation, namely Question Generation and Text Summarization. These are highly competitive benchmarks, with recent state-of-the-art results achieved by MLE based on pre-trained transformers [45]. Answer-aware Question Generation (QG) [58] is the task wherein, given a text and a target answer, the goal is to generate a relevant question. Following previous works [9,10], we used the SQuAD dataset [33]. Automatic Summarization aims to produce concise and fluent summaries given a longer text. We used the popular CNN/DM dataset [26], a corpus containing news articles and the corresponding abstractive summaries. For conditional text generation tasks, output sequences are commonly evaluated using BLEU (for e.g. Machine Translation, Question Generation) or ROUGE (for e.g. Summarization) metrics. In contrast to the unconditioned scenario, the diversity is linked to the variety of the inputs, and it is common practice to decode through beam search at inference.  Results For both tasks, we used data and evaluation metrics released by Dong et al. [9]. 5 The results shown in Table 2  Mitigating the Exposure Bias In Figure 3 we report the relative gain obtained, in terms of BLEU-4 for T5-small, for the best configuration (i.e. ColdGAN nucleus , = 0.9) w.r.t. the corresponding MLE baseline. The x-axis gives the length of considered ground truth target sequences. We observe that the longer the target sequence, the more the ColdGAN outperforms MLE. This might indicate that ColdGAN s can successfully mitigate exposure bias.\n\nHuman Evaluation As discussed in Section 1, automatic metrics are known to suffer from key limitations. Therefore, we additionally conducted a human evaluation on the QG task. Three professional English speakers were asked to judge, on a 1-to-5 Likert scale, to what extent the generated questions were: well-posed and natural (Fluency), relevant to their context (Relevance), and answerable, by looking at their context and answer (Answerability). The results in Table 3 show, surprisingly, both MLE-BART and ColdGAN -BART outperform the ground truth for Fluency. A similar result was reported by Yoon et al. [52] (refer to Table 2 in their paper). A plausible explanation is that humans are more inclined to use informal language and make grammar mistakes. For instance the human question \"About how many yellow cabs operate in New York?\" sounds slightly less formal than the one, generated by ColdGAN , \"How many yellow taxicabs are in Manhattan ?\". Compared to MLE, ColdGAN enables to significantly improve in term of fluency, while remaining competitive on other metrics, consistently with our experiments on exposure bias.\n\nAdversarial training curves Figure 4 shows the evolution (during training and for different setups) of the probability of the generated text to be human, according to the discriminator. Consistently with Table 2, ColdGAN nucleus appears to be the most adverse to the discriminator. Conversely, the regular GAN (T = 1) is less and less adversarial, and comparatively more perturbed.\n\n\nConclusion\n\nWe proposed ColdGAN s, a novel approach able to tame the exploration in Language GANs, allowing to obtain performance improvements on both conditional and unconditional text generation, w.r.t to MLE-based training. Our proposed IS method makes it compatible with advanced sampling methods, such as nucleus, or other future decoding methods. In the future, we plan to combine ColdGAN s with orthogonal approaches proposed by previous works, such as denser rewards.\n\n\nBroader Impact\n\nFluent and reliable Natural Language Generation can have significant societal impacts. On the one hand, we envision several applications beneficial for business, research or education: from automatic summarization of news, papers or books, to efficient information access; from automatic and personalized student evaluation tests trough question generation, to responsive conversational interfaces. On the other hand, malicious actors can use the same technology to build tools detrimental to society, e.g. for creation and propagation of misleading (fake) news as discussed in [31], impersonation, and deceit. Nonetheless, keeping this research open and under public scrutiny is arguably one of the best ways to defend against such actors [54].\n\n\nImplementation Details\n\nAll models are implemented in PyText [1]. We used a single RTX 2080 Ti GPU. All our experiments were conducted with T5-small 6 (60 million parameters) for both the generator and the discriminator; these were first trained on the corresponding task with MLE as in [5]. While T5-small underperforms its larger version, T5-11B, the latter has 11 billion parameters. However, BART [19] performs as well with only 400M parameters. Hence, for each task, we chose to train BART following the same procedure, with the best set of hyper-parameters found with T5-small (i.e. ColdGAN nucleus T =.2; =.9 ). For T5 and BART in conditional text generation, we applied, at inference, beam search with K=1 for T5 and K=4 for BART as recommended [19]. One epoch to train ColdGAN takes 2 hours with T5 and 5 hours with BART.\n\nFigure 1 :\n1Accuracy of a discriminator model trained under two different generation modes: Standard (subject to the exposure bias) and Teacher Forcing. The x-axis corresponds to the partial length t of the sequence to discriminate.\n\nFigure 2 :Figure 3 :\n23Results on the EMNLP 2017 News dataset (for all metrics, lower is better). Scores for previous works are taken from[23]. Relative BLEU-4 gains obtained with ColdGAN s over MLE, grouped by ground truth sequence length, on QG.\n\nFigure 4 :\n4Probability that the generated text is human according to D \u03c6 on CNN/DM.\n\nTable 1 :\n1Probability that a text is human according to various discriminators. D perf ect corresponds to a theoretical perfect discriminator with infinite capacity and training data. D T =\u03b3 corresponds to a discriminator trained on samples generated with a temperature T = \u03b3. Past T = 0 and past T = 1correspond to results on samples obtained with the generator weights resumed from a previous stage \nof the training, i.e. a checkpoint one epoch before the final state (see Section 4, Memory Replay). \n\n\n\nTable 2 :\n2Results on Question Generation (QG) and Abstractive Summarization (Summ.) tasks.QG (SQuAD) \nSumm. (CNN/DM) \n\n#params BLEU-1 BLEU-4 ROUGE-1 ROUGE-L BLEU-4 \n\nSemQG [56] \n18.37 \nBertSumAbs [22] \n340M \n41.72 \n38.76 \nUniLM [9] \n340M \n22.78 \n43.33 \n40.41 \nPEGASUS [55] \n568M \n44.17 \n41.11 \nT5-large (MLE) [32] \n11B \n43.52 \n40.69 \nT5-small (MLE) [32] \n60M \n47.72 \n19.65 \n42.34 \n40.37 \n15.94 \n\" (GAN T =1 ) \n60M \n46.44 \n18.84 \n38.98 \n36.42 \n13.23 \n\" (ColdGAN T =.2 ) \n60M \n47.94 \n20.23 \n42.58 \n40.74 \n16.04 \n\" (ColdGAN nucleus T =1; =.1 ) \n60M \n46.82 \n18.97 \n39.05 \n38.01 \n14.04 \n\" (ColdGAN nucleus T =1; =.9 ) \n60M \n47.83 \n20.85 \n42.31 \n40.44 \n16.21 \n\" (ColdGAN nucleus T =.2; =.9 ) \n60M \n48.50 \n20.55 \n42.54 \n40.61 \n16.86 \nw/o Memory Replay \n60M \n48.93 \n20.52 \n42.34 \n40.44 \n16.72 \nw/o IS Weight Clipping \n60M \n48.21 \n20.14 \n42.23 \n40.35 \n16.72 \nBART (MLE) [19] \n400M \n53.13 \n22.68 \n44.16 \n40.90 \n17,87 \n\" (ColdGAN nucleus T =.2; =.9 ) \n400M \n53.73 \n23.05 \n44.46 \n41.12 \n18.17 \n\nprevious works did not use self-supervised pretrained models, while we did (with T5): this explains \nthe improvement of our MLE baseline over theirs (MLE ScratchGAN). As one cannot directly \ncompare our performances with those reported from previous works, we study the performance \nvariations from the corresponding MLE baseline. Consistently with previous works [39, 43, 23], we \nobserve that the model, under the default exploration (i.e. GAN T =1 ), performs strictly worse than \nMLE. As a baseline, we experimented ColdGAN T \u223c]0,1[ , where during the training the temperature \nis randomly sampled between 0 and 1 for each sequence. While it performs better than GAN T =1 , it \nstill does not compare favorably w.r.t. MLE. Finally, both ColdGAN T =0.3 and ColdGAN nucleus \nobtain better results than MLE for the entire curve. To our knowledge, this is the first time that MLE \nfalls short \n\nTable 3 :\n3Human evaluation on QG. ColdGAN corresponds to BART trained with ColdGAN nucleus T = .2; = .9. Twotailed t-test results are reported for each model compared to Human (*: p < .01, **: p < .001).Fluency Relevance Answerability \n\nHuman \n3.66 \n4.31 \n4.22 \nBART (MLE) \n3.80* \n4.43 \n4.11 \nColdGAN \n4.36** \n4.45 \n4.01 \n\n\n\n\nare consistent across the two tasks: again, we observe that exploring under the default temperature yields to poor performances, while ColdGAN s compare favorably to MLE. The best performance is achieved with the experiment emphasizing the ColdGAN nucleus exploration the most, with = .9 and T = .2. Over 10 independent training runs, we also observed very stable results for this model, with a standard deviation of the average BLEU-4 lower than .09 on the test set. Finally, we applied this last ColdGAN s setup to BART[19], achieving a new state-of-the-art on both QG with 23.05 BLEU-4 and summarization with 41.12 ROUGE-L.\n2e-6, 8e-6, 2e-5, 8e-5, 2e-4.2 BART has comparable performance to T5-large, but with 20x fewer parameters.3 Implemented in https://github.com/deepmind/deepmind-research/tree/master/scratchgan 4 http://www.statmt.org/wmt17/\nhttps://github.com/microsoft/unilm/tree/master/unilm-v1\n\nPytext: A seamless path from nlp research to production. Ahmed Aly, Kushal Lakhotia, Shicong Zhao, Mrinal Mohit, Barlas Oguz, Abhinav Arora, Sonal Gupta, Christopher Dewan, Stef Nelson-Lindall, Rushin Shah, arXiv:1812.08729arXiv preprintAhmed Aly, Kushal Lakhotia, Shicong Zhao, Mrinal Mohit, Barlas Oguz, Abhinav Arora, Sonal Gupta, Christopher Dewan, Stef Nelson-Lindall, and Rushin Shah. 2018. Pytext: A seamless path from nlp research to production. arXiv preprint arXiv:1812.08729.\n\nScheduled sampling for sequence prediction with recurrent neural networks. Samy Bengio, Oriol Vinyals, Navdeep Jaitly, Noam Shazeer, Advances in Neural Information Processing Systems. Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. 2015. Scheduled sampling for sequence prediction with recurrent neural networks. In Advances in Neural Information Processing Systems, pages 1171-1179.\n\nLarge scale gan training for high fidelity natural image synthesis. Andrew Brock, Jeff Donahue, Karen Simonyan, arXiv:1809.11096arXiv preprintAndrew Brock, Jeff Donahue, and Karen Simonyan. 2018. Large scale gan training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096.\n\nS\u00e9bastien Bubeck, Eric Price, Ilya Razenshteyn, arXiv:1805.10204Adversarial examples from computational constraints. arXiv preprintS\u00e9bastien Bubeck, Eric Price, and Ilya Razenshteyn. 2018. Adversarial examples from compu- tational constraints. arXiv preprint arXiv:1805.10204.\n\nLanguage gans falling short. Massimo Caccia, Lucas Caccia, William Fedus, Hugo Larochelle, Joelle Pineau, Laurent Charlin, International Conference on Learning Representations. Massimo Caccia, Lucas Caccia, William Fedus, Hugo Larochelle, Joelle Pineau, and Lau- rent Charlin. 2020. Language gans falling short. In International Conference on Learning Representations.\n\nDeep communicating agents for abstractive summarization. Asli Celikyilmaz, Antoine Bosselut, Xiaodong He, Yejin Choi, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies1Long PapersAsli Celikyilmaz, Antoine Bosselut, Xiaodong He, and Yejin Choi. 2018. Deep communicating agents for abstractive summarization. In Proceedings of the 2018 Conference of the North Amer- ican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1662-1675.\n\nYanran Tong Che, Ruixiang Li, Devon Zhang, Wenjie Hjelm, Li, arXiv:1702.07983Yangqiu Song, and Yoshua Bengio. 2017. Maximum-likelihood augmented discrete generative adversarial networks. arXiv preprintTong Che, Yanran Li, Ruixiang Zhang, R Devon Hjelm, Wenjie Li, Yangqiu Song, and Yoshua Bengio. 2017. Maximum-likelihood augmented discrete generative adversarial networks. arXiv preprint arXiv:1702.07983.\n\nHierarchical reinforcement learning for adaptive text generation. Nina Dethlefs, Heriberto Cuay\u00e1huitl, Proceedings of the 6th International Natural Language Generation Conference. the 6th International Natural Language Generation ConferenceAssociation for Computational LinguisticsNina Dethlefs and Heriberto Cuay\u00e1huitl. 2010. Hierarchical reinforcement learning for adap- tive text generation. In Proceedings of the 6th International Natural Language Generation Conference, pages 37-45. Association for Computational Linguistics.\n\nUnified language model pre-training for natural language understanding and generation. Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, Hsiao-Wuen Hon, Advances in Neural Information Processing Systems. Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. 2019. Unified language model pre-training for natural language understanding and generation. In Advances in Neural Information Processing Systems, pages 13042-13054.\n\nLearning to ask: Neural question generation for reading comprehension. Xinya Du, Junru Shao, Claire Cardie, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsLong Papers1Xinya Du, Junru Shao, and Claire Cardie. 2017. Learning to ask: Neural question generation for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1342-1352.\n\nMaskGAN: Better Text Generation via Filling in the. William Fedus, Ian Goodfellow, Andrew M Dai, International Conference on Learning Representations. William Fedus, Ian Goodfellow, and Andrew M. Dai. 2018. MaskGAN: Better Text Generation via Filling in the _______. In International Conference on Learning Representations.\n\nStabilising experience replay for deep multiagent reinforcement learning. Jakob Foerster, Nantas Nardelli, Gregory Farquhar, Triantafyllos Afouras, H S Philip, Pushmeet Torr, Shimon Kohli, Whiteson, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine LearningJMLR. org70Jakob Foerster, Nantas Nardelli, Gregory Farquhar, Triantafyllos Afouras, Philip HS Torr, Pushmeet Kohli, and Shimon Whiteson. 2017. Stabilising experience replay for deep multi- agent reinforcement learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 1146-1155. JMLR. org.\n\nAdversarial spheres. Justin Gilmer, Luke Metz, Fartash Faghri, S Samuel, Maithra Schoenholz, Martin Raghu, Ian Wattenberg, Goodfellow, Justin Gilmer, Luke Metz, Fartash Faghri, Samuel S Schoenholz, Maithra Raghu, Martin Wattenberg, and Ian Goodfellow. 2018. Adversarial spheres.\n\nGenerative adversarial nets. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, Advances in neural information processing systems. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. In Advances in neural information processing systems, pages 2672-2680.\n\nLong text generation via adversarial training with leaked information. Jiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong Yu, Jun Wang, Thirty-Second AAAI Conference on Artificial Intelligence. Jiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong Yu, and Jun Wang. 2018. Long text generation via adversarial training with leaked information. In Thirty-Second AAAI Conference on Artificial Intelligence.\n\nGeoffrey Hinton, Oriol Vinyals, Jeff Dean, arXiv:1503.02531Distilling the knowledge in a neural network. arXiv preprintGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531.\n\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, Yejin Choi, arXiv:1904.09751The curious case of neural text degeneration. arXiv preprintAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degeneration. arXiv preprint arXiv:1904.09751.\n\nPolicy search for motor primitives in robotics. Jens Kober, Jan R Peters, Advances in neural information processing systems. Jens Kober and Jan R Peters. 2009. Policy search for motor primitives in robotics. In Advances in neural information processing systems, pages 849-856.\n\nBart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer, arXiv:1910.13461arXiv preprintMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461.\n\nAdversarial learning for neural dialogue generation. Jiwei Li, Will Monroe, Tianlin Shi, S\u00e9bastien Jean, Alan Ritter, Dan Jurafsky, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingJiwei Li, Will Monroe, Tianlin Shi, S\u00e9bastien Jean, Alan Ritter, and Dan Jurafsky. 2017. Adversarial learning for neural dialogue generation. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2157-2169.\n\nAdversarial ranking for language generation. Kevin Lin, Dianqi Li, Xiaodong He, Zhengyou Zhang, Ming-Ting Sun, Advances in Neural Information Processing Systems. Kevin Lin, Dianqi Li, Xiaodong He, Zhengyou Zhang, and Ming-Ting Sun. 2017. Adversarial ranking for language generation. In Advances in Neural Information Processing Systems, pages 3155-3165.\n\nText summarization with pretrained encoders. Yang Liu, Mirella Lapata, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Yang Liu and Mirella Lapata. 2019. Text summarization with pretrained encoders. In Proceed- ings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3721-3731.\n\nTraining language gans from scratch. Shakir Cyprien De Masson D&apos;autume, Mihaela Mohamed, Jack Rosca, Rae, Advances in Neural Information Processing Systems. Cyprien de Masson d'Autume, Shakir Mohamed, Mihaela Rosca, and Jack Rae. 2019. Training language gans from scratch. In Advances in Neural Information Processing Systems, pages 4302-4313.\n\nEpisodic memory in lifelong language learning. Sebastian Cyprien De Masson D&apos;autume, Lingpeng Ruder, Dani Kong, Yogatama, Advances in Neural Information Processing Systems. Cyprien de Masson d'Autume, Sebastian Ruder, Lingpeng Kong, and Dani Yogatama. 2019. Episodic memory in lifelong language learning. In Advances in Neural Information Processing Systems, pages 13122-13131.\n\nUnrolled generative adversarial networks. Luke Metz, Ben Poole, David Pfau, Jascha Sohl-Dickstein, arXiv:1611.02163arXiv preprintLuke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. 2016. Unrolled generative adversarial networks. arXiv preprint arXiv:1611.02163.\n\nAbstractive text summarization using sequence-to-sequence rnns and beyond. Ramesh Nallapati, Bowen Zhou, Caglar Gulcehre, Bing Xiang, arXiv:1602.06023arXiv preprintRamesh Nallapati, Bowen Zhou, Caglar Gulcehre, Bing Xiang, et al. 2016. Abstractive text summarization using sequence-to-sequence rnns and beyond. arXiv preprint arXiv:1602.06023.\n\nLearning beam search policies via imitation learning. Renato Negrinho, Matthew Gormley, Geoffrey J Gordon, Advances in Neural Information Processing Systems. Renato Negrinho, Matthew Gormley, and Geoffrey J Gordon. 2018. Learning beam search policies via imitation learning. In Advances in Neural Information Processing Systems, pages 10652-10661.\n\nWhy we need new evaluation metrics for NLG. Jekaterina Novikova, Ond\u0159ej Du\u0161ek, Amanda Cercas Curry, Verena Rieser, 10.18653/v1/D17-1238Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingCopenhagen, DenmarkAssociation for Computational LinguisticsJekaterina Novikova, Ond\u0159ej Du\u0161ek, Amanda Cercas Curry, and Verena Rieser. 2017. Why we need new evaluation metrics for NLG. In Proceedings of the 2017 Conference on Em- pirical Methods in Natural Language Processing, pages 2241-2252, Copenhagen, Denmark. Association for Computational Linguistics.\n\nBleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting on association for computational linguistics. the 40th annual meeting on association for computational linguisticsAssociation for Computational LinguisticsKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 311-318. Association for Computational Linguistics.\n\nA deep reinforced model for abstractive summarization. Romain Paulus, Caiming Xiong, Richard Socher, arXiv:1705.04304arXiv preprintRomain Paulus, Caiming Xiong, and Richard Socher. 2017. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304.\n\nLanguage models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI Blog. 189Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI Blog, 1(8):9.\n\nExploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, arXiv:1910.10683arXiv preprintColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683.\n\nSquad: 100,000+ questions for machine comprehension of text. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383-2392.\n\nAurelio Marc, Sumit Ranzato, Michael Chopra, Wojciech Auli, Zaremba, arXiv:1511.06732Sequence level training with recurrent neural networks. arXiv preprintMarc'Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. 2015. Sequence level training with recurrent neural networks. arXiv preprint arXiv:1511.06732.\n\nState-dependent exploration for policy gradient methods. Thomas R\u00fcckstie\u00df, Martin Felder, J\u00fcrgen Schmidhuber, Joint European Conference on Machine Learning and Knowledge Discovery in Databases. SpringerThomas R\u00fcckstie\u00df, Martin Felder, and J\u00fcrgen Schmidhuber. 2008. State-dependent explo- ration for policy gradient methods. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 234-249. Springer.\n\nTrust region policy optimization. John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, Philipp Moritz, International conference on machine learning. John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. 2015. Trust region policy optimization. In International conference on machine learning, pages 1889-1897.\n\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. arXiv preprintJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proxi- mal policy optimization algorithms. arXiv preprint arXiv:1707.06347.\n\nThomas Scialom, Paul-Alexis Dray, arXiv:2002.10375Sylvain Lamprier, Benjamin Piwowarski, and Jacopo Staiano. 2020. Discriminative adversarial search for abstractive summarization. arXiv preprintThomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, and Jacopo Staiano. 2020. Discriminative adversarial search for abstractive summarization. arXiv preprint arXiv:2002.10375.\n\nOn accurate evaluation of gans for language generation. Stanislau Semeniuta, Aliaksei Severyn, Sylvain Gelly, arXiv:1806.04936arXiv preprintStanislau Semeniuta, Aliaksei Severyn, and Sylvain Gelly. 2018. On accurate evaluation of gans for language generation. arXiv preprint arXiv:1806.04936.\n\nBleu is not suitable for the evaluation of text simplification. Elior Sulem, Omri Abend, Ari Rappoport, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingElior Sulem, Omri Abend, and Ari Rappoport. 2018. Bleu is not suitable for the evaluation of text simplification. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 738-744.\n\nSequence to sequence learning with neural networks. Ilya Sutskever, Oriol Vinyals, Quoc V Le, Advances in neural information processing systems. Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104-3112.\n\nReinforcement learning: An introduction. S Richard, Andrew G Sutton, Barto, MIT pressRichard S Sutton and Andrew G Barto. 2018. Reinforcement learning: An introduction. MIT press.\n\nGuy Tevet, Gavriel Habib, Vered Shwartz, Jonathan Berant, arXiv:1810.12686Evaluating text gans as language models. arXiv preprintGuy Tevet, Gavriel Habib, Vered Shwartz, and Jonathan Berant. 2018. Evaluating text gans as language models. arXiv preprint arXiv:1810.12686.\n\nData-efficient off-policy policy evaluation for reinforcement learning. Philip Thomas, Emma Brunskill, International Conference on Machine Learning. Philip Thomas and Emma Brunskill. 2016. Data-efficient off-policy policy evaluation for reinforcement learning. In International Conference on Machine Learning, pages 2139-2148.\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems, pages 5998-6008.\n\nSuperglue: A stickier benchmark for general-purpose language understanding systems. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, Advances in Neural Information Processing Systems. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019. Superglue: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems, pages 3261-3275.\n\nGLUE: A multi-task benchmark and analysis platform for natural language understanding. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, 10.18653/v1/W18-5446Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLPBrussels, BelgiumAssociation for Computational LinguisticsAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353-355, Brussels, Belgium. Association for Computational Linguistics.\n\nSample efficient actor-critic with experience replay. Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu, Nando De Freitas, Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu, and Nando de Freitas. 2016. Sample efficient actor-critic with experience replay.\n\nSean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, Jason Weston, arXiv:1908.04319Neural text generation with unlikelihood training. arXiv preprintSean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston. 2019. Neural text generation with unlikelihood training. arXiv preprint arXiv:1908.04319.\n\nSimple statistical gradient-following algorithms for connectionist reinforcement learning. J Ronald, Williams, Machine learning. 83-4Ronald J Williams. 1992. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229-256.\n\nA learning algorithm for continually running fully recurrent neural networks. J Ronald, David Williams, Zipser, Neural computation. 12Ronald J Williams and David Zipser. 1989. A learning algorithm for continually running fully recurrent neural networks. Neural computation, 1(2):270-280.\n\nLearning by semantic similarity makes abstractive summarization better. Wonjin Yoon, Yoon Sun Yeo, Minbyul Jeong, Bong-Jun Yi, Jaewoo Kang, arXiv:2002.07767arXiv preprintWonjin Yoon, Yoon Sun Yeo, Minbyul Jeong, Bong-Jun Yi, and Jaewoo Kang. 2020. Learning by semantic similarity makes abstractive summarization better. arXiv preprint arXiv:2002.07767.\n\nLantao Yu, Weinan Zhang, Jun Wang, Yong Yu Seqgan, arXiv:1609.05473Sequence generative adversarial nets with policy gradient. arxiv e-prints, page. arXiv preprintLantao Yu, Weinan Zhang, Jun Wang, and Yong Yu SeqGAN. 2016. Sequence generative adversarial nets with policy gradient. arxiv e-prints, page. arXiv preprint arXiv:1609.05473.\n\nDefending against neural fake news. Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, Yejin Choi, Advances in Neural Information Processing Systems. Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. 2019. Defending against neural fake news. In Advances in Neural Information Processing Systems, pages 9051-9062.\n\nPegasus: Pre-training with extracted gap-sentences for abstractive summarization. Jingqing Zhang, Yao Zhao, Mohammad Saleh, Peter J Liu, arXiv:1912.08777arXiv preprintJingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J Liu. 2019. Pegasus: Pre-training with extracted gap-sentences for abstractive summarization. arXiv preprint arXiv:1912.08777.\n\nAddressing semantic drift in question generation for semi-supervised question answering. Shiyue Zhang, Mohit Bansal, arXiv:1909.06356arXiv preprintShiyue Zhang and Mohit Bansal. 2019. Addressing semantic drift in question generation for semi-supervised question answering. arXiv preprint arXiv:1909.06356.\n\nAdversarial feature matching for text generation. Yizhe Zhang, Zhe Gan, Kai Fan, Zhi Chen, Ricardo Henao, Dinghan Shen, Lawrence Carin, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine LearningJMLR. org70Yizhe Zhang, Zhe Gan, Kai Fan, Zhi Chen, Ricardo Henao, Dinghan Shen, and Lawrence Carin. 2017. Adversarial feature matching for text generation. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 4006-4015. JMLR. org.\n\nNeural question generation from text: A preliminary study. Qingyu Zhou, Nan Yang, Furu Wei, Chuanqi Tan, Hangbo Bao, Ming Zhou, National CCF Conference on Natural Language Processing and Chinese Computing. SpringerQingyu Zhou, Nan Yang, Furu Wei, Chuanqi Tan, Hangbo Bao, and Ming Zhou. 2017. Neural question generation from text: A preliminary study. In National CCF Conference on Natural Language Processing and Chinese Computing, pages 662-671. Springer.\n\nSelf-adversarial learning with comparative discrimination for text generation. Wangchunshu Zhou, Tao Ge, Ke Xu, Furu Wei, Ming Zhou, International Conference on Learning Representations. Wangchunshu Zhou, Tao Ge, Ke Xu, Furu Wei, and Ming Zhou. 2020. Self-adversarial learning with comparative discrimination for text generation. In International Conference on Learning Representations.\n\nTexygen: A benchmarking platform for text generation models. Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang, Yong Yu, The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval. Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang, and Yong Yu. 2018. Texygen: A benchmarking platform for text generation models. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, pages 1097-1100.\n\nM Daniel, Nisan Ziegler, Jeffrey Stiennon, Wu, B Tom, Alec Brown, Dario Radford, Paul Amodei, Geoffrey Christiano, Irving, arXiv:1909.08593Fine-tuning language models from human preferences. arXiv preprintDaniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. 2019. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593.\n", "annotations": {"author": "[{\"end\":173,\"start\":68},{\"end\":214,\"start\":174},{\"end\":329,\"start\":215},{\"end\":450,\"start\":330},{\"end\":484,\"start\":451},{\"end\":498,\"start\":485}]", "publisher": null, "author_last_name": "[{\"end\":82,\"start\":75},{\"end\":190,\"start\":186},{\"end\":231,\"start\":223},{\"end\":349,\"start\":339},{\"end\":465,\"start\":458}]", "author_first_name": "[{\"end\":74,\"start\":68},{\"end\":185,\"start\":174},{\"end\":222,\"start\":215},{\"end\":338,\"start\":330},{\"end\":457,\"start\":451}]", "author_affiliation": "[{\"end\":172,\"start\":102},{\"end\":328,\"start\":258},{\"end\":449,\"start\":379},{\"end\":497,\"start\":486}]", "title": "[{\"end\":65,\"start\":1},{\"end\":563,\"start\":499}]", "venue": null, "abstract": "[{\"end\":1847,\"start\":565}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b39\"},\"end\":1881,\"start\":1877},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2012,\"start\":2008},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2081,\"start\":2077},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":2116,\"start\":2112},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2546,\"start\":2542},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2605,\"start\":2602},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":2803,\"start\":2799},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":2995,\"start\":2991},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3078,\"start\":3074},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3081,\"start\":3078},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3174,\"start\":3171},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":3219,\"start\":3215},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":3222,\"start\":3219},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3225,\"start\":3222},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":3261,\"start\":3257},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4359,\"start\":4356},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":4362,\"start\":4359},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4365,\"start\":4362},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4367,\"start\":4365},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4675,\"start\":4671},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5081,\"start\":5077},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":5084,\"start\":5081},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":5183,\"start\":5179},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5186,\"start\":5183},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5226,\"start\":5222},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":5300,\"start\":5296},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5335,\"start\":5332},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":5778,\"start\":5774},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":5781,\"start\":5778},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":6033,\"start\":6029},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":6036,\"start\":6033},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6039,\"start\":6036},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6159,\"start\":6156},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":6806,\"start\":6802},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7489,\"start\":7485},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8053,\"start\":8049},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":8129,\"start\":8125},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":8172,\"start\":8168},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11720,\"start\":11716},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11722,\"start\":11720},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12935,\"start\":12931},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":13261,\"start\":13257},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":13264,\"start\":13261},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":16619,\"start\":16615},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":17524,\"start\":17520},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":18170,\"start\":18166},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":18747,\"start\":18743},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":18895,\"start\":18891},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":19633,\"start\":19632},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":19851,\"start\":19847},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":19868,\"start\":19867},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":20134,\"start\":20130},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":20195,\"start\":20191},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":20214,\"start\":20210},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":20225,\"start\":20224},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":20292,\"start\":20289},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":20486,\"start\":20483},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":20489,\"start\":20486},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":20492,\"start\":20489},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":20495,\"start\":20492},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":20497,\"start\":20495},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":20500,\"start\":20497},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":20539,\"start\":20538},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":20583,\"start\":20580},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":20586,\"start\":20583},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":20952,\"start\":20948},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":20996,\"start\":20992},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":21125,\"start\":21122},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":21128,\"start\":21125},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":21160,\"start\":21156},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":21291,\"start\":21287},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":21812,\"start\":21809},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":21815,\"start\":21814},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":22905,\"start\":22901},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":24881,\"start\":24877},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":25043,\"start\":25039},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":25111,\"start\":25108},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":25337,\"start\":25334},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":25452,\"start\":25448},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":25804,\"start\":25800},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":26255,\"start\":26251},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":29686,\"start\":29682},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":29818,\"start\":29817},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":29895,\"start\":29894}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":26111,\"start\":25878},{\"attributes\":{\"id\":\"fig_1\"},\"end\":26360,\"start\":26112},{\"attributes\":{\"id\":\"fig_2\"},\"end\":26446,\"start\":26361},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":26953,\"start\":26447},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":28832,\"start\":26954},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":29158,\"start\":28833},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":29787,\"start\":29159}]", "paragraph": "[{\"end\":2082,\"start\":1849},{\"end\":2804,\"start\":2084},{\"end\":3226,\"start\":2806},{\"end\":3599,\"start\":3228},{\"end\":3876,\"start\":3601},{\"end\":4056,\"start\":3878},{\"end\":4207,\"start\":4058},{\"end\":4676,\"start\":4224},{\"end\":5672,\"start\":4678},{\"end\":6624,\"start\":5674},{\"end\":6897,\"start\":6626},{\"end\":7031,\"start\":6952},{\"end\":7218,\"start\":7033},{\"end\":7589,\"start\":7258},{\"end\":7752,\"start\":7591},{\"end\":7977,\"start\":7823},{\"end\":8396,\"start\":7979},{\"end\":8956,\"start\":8436},{\"end\":9619,\"start\":8958},{\"end\":10789,\"start\":9712},{\"end\":11907,\"start\":10791},{\"end\":12882,\"start\":11909},{\"end\":13510,\"start\":12884},{\"end\":14183,\"start\":13521},{\"end\":14433,\"start\":14261},{\"end\":14668,\"start\":14435},{\"end\":14799,\"start\":14732},{\"end\":15159,\"start\":14897},{\"end\":15744,\"start\":15161},{\"end\":16522,\"start\":15769},{\"end\":17192,\"start\":16524},{\"end\":17415,\"start\":17246},{\"end\":17670,\"start\":17417},{\"end\":19348,\"start\":17775},{\"end\":19869,\"start\":19383},{\"end\":20629,\"start\":19907},{\"end\":22289,\"start\":20665},{\"end\":23419,\"start\":22291},{\"end\":23802,\"start\":23421},{\"end\":24280,\"start\":23817},{\"end\":25044,\"start\":24299},{\"end\":25877,\"start\":25071}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":6951,\"start\":6898},{\"attributes\":{\"id\":\"formula_1\"},\"end\":7257,\"start\":7219},{\"attributes\":{\"id\":\"formula_2\"},\"end\":7822,\"start\":7753},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9689,\"start\":9643},{\"attributes\":{\"id\":\"formula_4\"},\"end\":9711,\"start\":9689},{\"attributes\":{\"id\":\"formula_5\"},\"end\":14260,\"start\":14184},{\"attributes\":{\"id\":\"formula_6\"},\"end\":14731,\"start\":14669},{\"attributes\":{\"id\":\"formula_7\"},\"end\":14896,\"start\":14800},{\"attributes\":{\"id\":\"formula_8\"},\"end\":17245,\"start\":17193},{\"attributes\":{\"id\":\"formula_9\"},\"end\":17774,\"start\":17671}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":11059,\"start\":11052},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":18496,\"start\":18489},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":21844,\"start\":21837},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":22762,\"start\":22755},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":22923,\"start\":22916},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":23632,\"start\":23625}]", "section_header": "[{\"attributes\":{\"n\":\"2\"},\"end\":4222,\"start\":4210},{\"attributes\":{\"n\":\"3.2\"},\"end\":8434,\"start\":8399},{\"end\":9642,\"start\":9622},{\"attributes\":{\"n\":\"4\"},\"end\":13519,\"start\":13513},{\"end\":15767,\"start\":15747},{\"end\":19367,\"start\":19351},{\"attributes\":{\"n\":\"5\"},\"end\":19381,\"start\":19370},{\"attributes\":{\"n\":\"5.1\"},\"end\":19905,\"start\":19872},{\"attributes\":{\"n\":\"5.2\"},\"end\":20663,\"start\":20632},{\"attributes\":{\"n\":\"6\"},\"end\":23815,\"start\":23805},{\"end\":24297,\"start\":24283},{\"end\":25069,\"start\":25047},{\"end\":25889,\"start\":25879},{\"end\":26133,\"start\":26113},{\"end\":26372,\"start\":26362},{\"end\":26457,\"start\":26448},{\"end\":26964,\"start\":26955},{\"end\":28843,\"start\":28834}]", "table": "[{\"end\":26953,\"start\":26751},{\"end\":28832,\"start\":27046},{\"end\":29158,\"start\":29038}]", "figure_caption": "[{\"end\":26111,\"start\":25891},{\"end\":26360,\"start\":26136},{\"end\":26446,\"start\":26374},{\"end\":26751,\"start\":26459},{\"end\":27046,\"start\":26966},{\"end\":29038,\"start\":28845},{\"end\":29787,\"start\":29161}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8987,\"start\":8981},{\"end\":20579,\"start\":20571},{\"end\":21886,\"start\":21878},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":23457,\"start\":23449}]", "bib_author_first_name": "[{\"end\":30130,\"start\":30125},{\"end\":30142,\"start\":30136},{\"end\":30160,\"start\":30153},{\"end\":30173,\"start\":30167},{\"end\":30187,\"start\":30181},{\"end\":30201,\"start\":30194},{\"end\":30214,\"start\":30209},{\"end\":30233,\"start\":30222},{\"end\":30245,\"start\":30241},{\"end\":30268,\"start\":30262},{\"end\":30635,\"start\":30631},{\"end\":30649,\"start\":30644},{\"end\":30666,\"start\":30659},{\"end\":30679,\"start\":30675},{\"end\":31029,\"start\":31023},{\"end\":31041,\"start\":31037},{\"end\":31056,\"start\":31051},{\"end\":31262,\"start\":31253},{\"end\":31275,\"start\":31271},{\"end\":31287,\"start\":31283},{\"end\":31567,\"start\":31560},{\"end\":31581,\"start\":31576},{\"end\":31597,\"start\":31590},{\"end\":31609,\"start\":31605},{\"end\":31628,\"start\":31622},{\"end\":31644,\"start\":31637},{\"end\":31962,\"start\":31958},{\"end\":31983,\"start\":31976},{\"end\":32002,\"start\":31994},{\"end\":32012,\"start\":32007},{\"end\":32627,\"start\":32621},{\"end\":32646,\"start\":32638},{\"end\":32656,\"start\":32651},{\"end\":32670,\"start\":32664},{\"end\":33099,\"start\":33095},{\"end\":33119,\"start\":33110},{\"end\":33650,\"start\":33648},{\"end\":33660,\"start\":33657},{\"end\":33673,\"start\":33667},{\"end\":33684,\"start\":33680},{\"end\":33698,\"start\":33690},{\"end\":33706,\"start\":33704},{\"end\":33721,\"start\":33713},{\"end\":33731,\"start\":33727},{\"end\":33748,\"start\":33738},{\"end\":34158,\"start\":34153},{\"end\":34168,\"start\":34163},{\"end\":34181,\"start\":34175},{\"end\":34674,\"start\":34667},{\"end\":34685,\"start\":34682},{\"end\":34704,\"start\":34698},{\"end\":34706,\"start\":34705},{\"end\":35019,\"start\":35014},{\"end\":35036,\"start\":35030},{\"end\":35054,\"start\":35047},{\"end\":35078,\"start\":35065},{\"end\":35089,\"start\":35088},{\"end\":35091,\"start\":35090},{\"end\":35108,\"start\":35100},{\"end\":35121,\"start\":35115},{\"end\":35621,\"start\":35615},{\"end\":35634,\"start\":35630},{\"end\":35648,\"start\":35641},{\"end\":35658,\"start\":35657},{\"end\":35674,\"start\":35667},{\"end\":35693,\"start\":35687},{\"end\":35704,\"start\":35701},{\"end\":35906,\"start\":35903},{\"end\":35923,\"start\":35919},{\"end\":35944,\"start\":35939},{\"end\":35956,\"start\":35952},{\"end\":35966,\"start\":35961},{\"end\":35988,\"start\":35981},{\"end\":36001,\"start\":35996},{\"end\":36019,\"start\":36013},{\"end\":36393,\"start\":36386},{\"end\":36403,\"start\":36399},{\"end\":36411,\"start\":36408},{\"end\":36423,\"start\":36417},{\"end\":36435,\"start\":36431},{\"end\":36443,\"start\":36440},{\"end\":36723,\"start\":36715},{\"end\":36737,\"start\":36732},{\"end\":36751,\"start\":36747},{\"end\":36970,\"start\":36967},{\"end\":36984,\"start\":36981},{\"end\":36993,\"start\":36991},{\"end\":37005,\"start\":36998},{\"end\":37019,\"start\":37014},{\"end\":37303,\"start\":37299},{\"end\":37314,\"start\":37311},{\"end\":37316,\"start\":37315},{\"end\":37648,\"start\":37644},{\"end\":37662,\"start\":37656},{\"end\":37673,\"start\":37668},{\"end\":37687,\"start\":37681},{\"end\":37714,\"start\":37703},{\"end\":37728,\"start\":37724},{\"end\":37738,\"start\":37735},{\"end\":37753,\"start\":37749},{\"end\":38137,\"start\":38132},{\"end\":38146,\"start\":38142},{\"end\":38162,\"start\":38155},{\"end\":38177,\"start\":38168},{\"end\":38188,\"start\":38184},{\"end\":38200,\"start\":38197},{\"end\":38671,\"start\":38666},{\"end\":38683,\"start\":38677},{\"end\":38696,\"start\":38688},{\"end\":38709,\"start\":38701},{\"end\":38726,\"start\":38717},{\"end\":39025,\"start\":39021},{\"end\":39038,\"start\":39031},{\"end\":39707,\"start\":39701},{\"end\":39748,\"start\":39741},{\"end\":39762,\"start\":39758},{\"end\":40070,\"start\":40061},{\"end\":40112,\"start\":40104},{\"end\":40124,\"start\":40120},{\"end\":40444,\"start\":40440},{\"end\":40454,\"start\":40451},{\"end\":40467,\"start\":40462},{\"end\":40480,\"start\":40474},{\"end\":40751,\"start\":40745},{\"end\":40768,\"start\":40763},{\"end\":40781,\"start\":40775},{\"end\":40796,\"start\":40792},{\"end\":41075,\"start\":41069},{\"end\":41093,\"start\":41086},{\"end\":41111,\"start\":41103},{\"end\":41113,\"start\":41112},{\"end\":41418,\"start\":41408},{\"end\":41435,\"start\":41429},{\"end\":41449,\"start\":41443},{\"end\":41456,\"start\":41450},{\"end\":41470,\"start\":41464},{\"end\":42089,\"start\":42082},{\"end\":42105,\"start\":42100},{\"end\":42118,\"start\":42114},{\"end\":42133,\"start\":42125},{\"end\":42672,\"start\":42666},{\"end\":42688,\"start\":42681},{\"end\":42703,\"start\":42696},{\"end\":42944,\"start\":42940},{\"end\":42961,\"start\":42954},{\"end\":42971,\"start\":42966},{\"end\":42984,\"start\":42979},{\"end\":42996,\"start\":42991},{\"end\":43009,\"start\":43005},{\"end\":43291,\"start\":43286},{\"end\":43304,\"start\":43300},{\"end\":43318,\"start\":43314},{\"end\":43337,\"start\":43328},{\"end\":43349,\"start\":43343},{\"end\":43365,\"start\":43358},{\"end\":43379,\"start\":43374},{\"end\":43389,\"start\":43386},{\"end\":43401,\"start\":43394},{\"end\":43752,\"start\":43746},{\"end\":43768,\"start\":43764},{\"end\":43786,\"start\":43776},{\"end\":43801,\"start\":43796},{\"end\":44218,\"start\":44211},{\"end\":44230,\"start\":44225},{\"end\":44247,\"start\":44240},{\"end\":44264,\"start\":44256},{\"end\":44597,\"start\":44591},{\"end\":44615,\"start\":44609},{\"end\":44630,\"start\":44624},{\"end\":45009,\"start\":45005},{\"end\":45026,\"start\":45020},{\"end\":45041,\"start\":45035},{\"end\":45057,\"start\":45050},{\"end\":45073,\"start\":45066},{\"end\":45320,\"start\":45316},{\"end\":45336,\"start\":45331},{\"end\":45353,\"start\":45345},{\"end\":45368,\"start\":45364},{\"end\":45382,\"start\":45378},{\"end\":45630,\"start\":45624},{\"end\":45651,\"start\":45640},{\"end\":46081,\"start\":46072},{\"end\":46101,\"start\":46093},{\"end\":46118,\"start\":46111},{\"end\":46379,\"start\":46374},{\"end\":46391,\"start\":46387},{\"end\":46402,\"start\":46399},{\"end\":46850,\"start\":46846},{\"end\":46867,\"start\":46862},{\"end\":46883,\"start\":46877},{\"end\":47157,\"start\":47156},{\"end\":47173,\"start\":47167},{\"end\":47175,\"start\":47174},{\"end\":47299,\"start\":47296},{\"end\":47314,\"start\":47307},{\"end\":47327,\"start\":47322},{\"end\":47345,\"start\":47337},{\"end\":47646,\"start\":47640},{\"end\":47659,\"start\":47655},{\"end\":47929,\"start\":47923},{\"end\":47943,\"start\":47939},{\"end\":47957,\"start\":47953},{\"end\":47971,\"start\":47966},{\"end\":47988,\"start\":47983},{\"end\":48001,\"start\":47996},{\"end\":48003,\"start\":48002},{\"end\":48017,\"start\":48011},{\"end\":48031,\"start\":48026},{\"end\":48413,\"start\":48409},{\"end\":48424,\"start\":48420},{\"end\":48446,\"start\":48440},{\"end\":48464,\"start\":48455},{\"end\":48478,\"start\":48472},{\"end\":48493,\"start\":48488},{\"end\":48504,\"start\":48500},{\"end\":48517,\"start\":48511},{\"end\":48951,\"start\":48947},{\"end\":48967,\"start\":48958},{\"end\":48981,\"start\":48975},{\"end\":48996,\"start\":48991},{\"end\":49007,\"start\":49003},{\"end\":49020,\"start\":49014},{\"end\":49720,\"start\":49716},{\"end\":49733,\"start\":49727},{\"end\":49748,\"start\":49741},{\"end\":49765,\"start\":49756},{\"end\":49776,\"start\":49772},{\"end\":49789,\"start\":49784},{\"end\":49808,\"start\":49803},{\"end\":49995,\"start\":49991},{\"end\":50009,\"start\":50005},{\"end\":50026,\"start\":50019},{\"end\":50040,\"start\":50035},{\"end\":50057,\"start\":50048},{\"end\":50068,\"start\":50063},{\"end\":50431,\"start\":50430},{\"end\":50702,\"start\":50701},{\"end\":50716,\"start\":50711},{\"end\":50990,\"start\":50984},{\"end\":51001,\"start\":50997},{\"end\":51018,\"start\":51011},{\"end\":51034,\"start\":51026},{\"end\":51045,\"start\":51039},{\"end\":51272,\"start\":51266},{\"end\":51283,\"start\":51277},{\"end\":51294,\"start\":51291},{\"end\":51305,\"start\":51301},{\"end\":51308,\"start\":51306},{\"end\":51645,\"start\":51640},{\"end\":51658,\"start\":51655},{\"end\":51675,\"start\":51669},{\"end\":51692,\"start\":51685},{\"end\":51702,\"start\":51699},{\"end\":51721,\"start\":51712},{\"end\":51736,\"start\":51731},{\"end\":52105,\"start\":52097},{\"end\":52116,\"start\":52113},{\"end\":52131,\"start\":52123},{\"end\":52146,\"start\":52139},{\"end\":52458,\"start\":52452},{\"end\":52471,\"start\":52466},{\"end\":52725,\"start\":52720},{\"end\":52736,\"start\":52733},{\"end\":52745,\"start\":52742},{\"end\":52754,\"start\":52751},{\"end\":52768,\"start\":52761},{\"end\":52783,\"start\":52776},{\"end\":52798,\"start\":52790},{\"end\":53263,\"start\":53257},{\"end\":53273,\"start\":53270},{\"end\":53284,\"start\":53280},{\"end\":53297,\"start\":53290},{\"end\":53309,\"start\":53303},{\"end\":53319,\"start\":53315},{\"end\":53747,\"start\":53736},{\"end\":53757,\"start\":53754},{\"end\":53764,\"start\":53762},{\"end\":53773,\"start\":53769},{\"end\":53783,\"start\":53779},{\"end\":54113,\"start\":54106},{\"end\":54123,\"start\":54119},{\"end\":54131,\"start\":54128},{\"end\":54146,\"start\":54139},{\"end\":54158,\"start\":54152},{\"end\":54169,\"start\":54166},{\"end\":54180,\"start\":54176},{\"end\":54549,\"start\":54548},{\"end\":54563,\"start\":54558},{\"end\":54580,\"start\":54573},{\"end\":54596,\"start\":54595},{\"end\":54606,\"start\":54602},{\"end\":54619,\"start\":54614},{\"end\":54633,\"start\":54629},{\"end\":54650,\"start\":54642}]", "bib_author_last_name": "[{\"end\":30134,\"start\":30131},{\"end\":30151,\"start\":30143},{\"end\":30165,\"start\":30161},{\"end\":30179,\"start\":30174},{\"end\":30192,\"start\":30188},{\"end\":30207,\"start\":30202},{\"end\":30220,\"start\":30215},{\"end\":30239,\"start\":30234},{\"end\":30260,\"start\":30246},{\"end\":30273,\"start\":30269},{\"end\":30642,\"start\":30636},{\"end\":30657,\"start\":30650},{\"end\":30673,\"start\":30667},{\"end\":30687,\"start\":30680},{\"end\":31035,\"start\":31030},{\"end\":31049,\"start\":31042},{\"end\":31065,\"start\":31057},{\"end\":31269,\"start\":31263},{\"end\":31281,\"start\":31276},{\"end\":31299,\"start\":31288},{\"end\":31574,\"start\":31568},{\"end\":31588,\"start\":31582},{\"end\":31603,\"start\":31598},{\"end\":31620,\"start\":31610},{\"end\":31635,\"start\":31629},{\"end\":31652,\"start\":31645},{\"end\":31974,\"start\":31963},{\"end\":31992,\"start\":31984},{\"end\":32005,\"start\":32003},{\"end\":32017,\"start\":32013},{\"end\":32636,\"start\":32628},{\"end\":32649,\"start\":32647},{\"end\":32662,\"start\":32657},{\"end\":32676,\"start\":32671},{\"end\":32680,\"start\":32678},{\"end\":33108,\"start\":33100},{\"end\":33130,\"start\":33120},{\"end\":33655,\"start\":33651},{\"end\":33665,\"start\":33661},{\"end\":33678,\"start\":33674},{\"end\":33688,\"start\":33685},{\"end\":33702,\"start\":33699},{\"end\":33711,\"start\":33707},{\"end\":33725,\"start\":33722},{\"end\":33736,\"start\":33732},{\"end\":33752,\"start\":33749},{\"end\":34161,\"start\":34159},{\"end\":34173,\"start\":34169},{\"end\":34188,\"start\":34182},{\"end\":34680,\"start\":34675},{\"end\":34696,\"start\":34686},{\"end\":34710,\"start\":34707},{\"end\":35028,\"start\":35020},{\"end\":35045,\"start\":35037},{\"end\":35063,\"start\":35055},{\"end\":35086,\"start\":35079},{\"end\":35098,\"start\":35092},{\"end\":35113,\"start\":35109},{\"end\":35127,\"start\":35122},{\"end\":35137,\"start\":35129},{\"end\":35628,\"start\":35622},{\"end\":35639,\"start\":35635},{\"end\":35655,\"start\":35649},{\"end\":35665,\"start\":35659},{\"end\":35685,\"start\":35675},{\"end\":35699,\"start\":35694},{\"end\":35715,\"start\":35705},{\"end\":35727,\"start\":35717},{\"end\":35917,\"start\":35907},{\"end\":35937,\"start\":35924},{\"end\":35950,\"start\":35945},{\"end\":35959,\"start\":35957},{\"end\":35979,\"start\":35967},{\"end\":35994,\"start\":35989},{\"end\":36011,\"start\":36002},{\"end\":36026,\"start\":36020},{\"end\":36397,\"start\":36394},{\"end\":36406,\"start\":36404},{\"end\":36415,\"start\":36412},{\"end\":36429,\"start\":36424},{\"end\":36438,\"start\":36436},{\"end\":36448,\"start\":36444},{\"end\":36730,\"start\":36724},{\"end\":36745,\"start\":36738},{\"end\":36756,\"start\":36752},{\"end\":36979,\"start\":36971},{\"end\":36989,\"start\":36985},{\"end\":36996,\"start\":36994},{\"end\":37012,\"start\":37006},{\"end\":37024,\"start\":37020},{\"end\":37309,\"start\":37304},{\"end\":37323,\"start\":37317},{\"end\":37654,\"start\":37649},{\"end\":37666,\"start\":37663},{\"end\":37679,\"start\":37674},{\"end\":37701,\"start\":37688},{\"end\":37722,\"start\":37715},{\"end\":37733,\"start\":37729},{\"end\":37747,\"start\":37739},{\"end\":37765,\"start\":37754},{\"end\":38140,\"start\":38138},{\"end\":38153,\"start\":38147},{\"end\":38166,\"start\":38163},{\"end\":38182,\"start\":38178},{\"end\":38195,\"start\":38189},{\"end\":38209,\"start\":38201},{\"end\":38675,\"start\":38672},{\"end\":38686,\"start\":38684},{\"end\":38699,\"start\":38697},{\"end\":38715,\"start\":38710},{\"end\":38730,\"start\":38727},{\"end\":39029,\"start\":39026},{\"end\":39045,\"start\":39039},{\"end\":39739,\"start\":39708},{\"end\":39756,\"start\":39749},{\"end\":39768,\"start\":39763},{\"end\":39773,\"start\":39770},{\"end\":40102,\"start\":40071},{\"end\":40118,\"start\":40113},{\"end\":40129,\"start\":40125},{\"end\":40139,\"start\":40131},{\"end\":40449,\"start\":40445},{\"end\":40460,\"start\":40455},{\"end\":40472,\"start\":40468},{\"end\":40495,\"start\":40481},{\"end\":40761,\"start\":40752},{\"end\":40773,\"start\":40769},{\"end\":40790,\"start\":40782},{\"end\":40802,\"start\":40797},{\"end\":41084,\"start\":41076},{\"end\":41101,\"start\":41094},{\"end\":41120,\"start\":41114},{\"end\":41427,\"start\":41419},{\"end\":41441,\"start\":41436},{\"end\":41462,\"start\":41457},{\"end\":41477,\"start\":41471},{\"end\":42098,\"start\":42090},{\"end\":42112,\"start\":42106},{\"end\":42123,\"start\":42119},{\"end\":42137,\"start\":42134},{\"end\":42679,\"start\":42673},{\"end\":42694,\"start\":42689},{\"end\":42710,\"start\":42704},{\"end\":42952,\"start\":42945},{\"end\":42964,\"start\":42962},{\"end\":42977,\"start\":42972},{\"end\":42989,\"start\":42985},{\"end\":43003,\"start\":42997},{\"end\":43019,\"start\":43010},{\"end\":43298,\"start\":43292},{\"end\":43312,\"start\":43305},{\"end\":43326,\"start\":43319},{\"end\":43341,\"start\":43338},{\"end\":43356,\"start\":43350},{\"end\":43372,\"start\":43366},{\"end\":43384,\"start\":43380},{\"end\":43392,\"start\":43390},{\"end\":43405,\"start\":43402},{\"end\":43762,\"start\":43753},{\"end\":43774,\"start\":43769},{\"end\":43794,\"start\":43787},{\"end\":43807,\"start\":43802},{\"end\":44223,\"start\":44219},{\"end\":44238,\"start\":44231},{\"end\":44254,\"start\":44248},{\"end\":44269,\"start\":44265},{\"end\":44278,\"start\":44271},{\"end\":44607,\"start\":44598},{\"end\":44622,\"start\":44616},{\"end\":44642,\"start\":44631},{\"end\":45018,\"start\":45010},{\"end\":45033,\"start\":45027},{\"end\":45048,\"start\":45042},{\"end\":45064,\"start\":45058},{\"end\":45080,\"start\":45074},{\"end\":45329,\"start\":45321},{\"end\":45343,\"start\":45337},{\"end\":45362,\"start\":45354},{\"end\":45376,\"start\":45369},{\"end\":45389,\"start\":45383},{\"end\":45638,\"start\":45631},{\"end\":45656,\"start\":45652},{\"end\":46091,\"start\":46082},{\"end\":46109,\"start\":46102},{\"end\":46124,\"start\":46119},{\"end\":46385,\"start\":46380},{\"end\":46397,\"start\":46392},{\"end\":46412,\"start\":46403},{\"end\":46860,\"start\":46851},{\"end\":46875,\"start\":46868},{\"end\":46886,\"start\":46884},{\"end\":47165,\"start\":47158},{\"end\":47182,\"start\":47176},{\"end\":47189,\"start\":47184},{\"end\":47305,\"start\":47300},{\"end\":47320,\"start\":47315},{\"end\":47335,\"start\":47328},{\"end\":47352,\"start\":47346},{\"end\":47653,\"start\":47647},{\"end\":47669,\"start\":47660},{\"end\":47937,\"start\":47930},{\"end\":47951,\"start\":47944},{\"end\":47964,\"start\":47958},{\"end\":47981,\"start\":47972},{\"end\":47994,\"start\":47989},{\"end\":48009,\"start\":48004},{\"end\":48024,\"start\":48018},{\"end\":48042,\"start\":48032},{\"end\":48418,\"start\":48414},{\"end\":48438,\"start\":48425},{\"end\":48453,\"start\":48447},{\"end\":48470,\"start\":48465},{\"end\":48486,\"start\":48479},{\"end\":48498,\"start\":48494},{\"end\":48509,\"start\":48505},{\"end\":48524,\"start\":48518},{\"end\":48956,\"start\":48952},{\"end\":48973,\"start\":48968},{\"end\":48989,\"start\":48982},{\"end\":49001,\"start\":48997},{\"end\":49012,\"start\":49008},{\"end\":49027,\"start\":49021},{\"end\":49725,\"start\":49721},{\"end\":49739,\"start\":49734},{\"end\":49754,\"start\":49749},{\"end\":49770,\"start\":49766},{\"end\":49782,\"start\":49777},{\"end\":49801,\"start\":49790},{\"end\":49819,\"start\":49809},{\"end\":50003,\"start\":49996},{\"end\":50017,\"start\":50010},{\"end\":50033,\"start\":50027},{\"end\":50046,\"start\":50041},{\"end\":50061,\"start\":50058},{\"end\":50075,\"start\":50069},{\"end\":50438,\"start\":50432},{\"end\":50448,\"start\":50440},{\"end\":50709,\"start\":50703},{\"end\":50725,\"start\":50717},{\"end\":50733,\"start\":50727},{\"end\":50995,\"start\":50991},{\"end\":51009,\"start\":51002},{\"end\":51024,\"start\":51019},{\"end\":51037,\"start\":51035},{\"end\":51050,\"start\":51046},{\"end\":51275,\"start\":51273},{\"end\":51289,\"start\":51284},{\"end\":51299,\"start\":51295},{\"end\":51315,\"start\":51309},{\"end\":51653,\"start\":51646},{\"end\":51667,\"start\":51659},{\"end\":51683,\"start\":51676},{\"end\":51697,\"start\":51693},{\"end\":51710,\"start\":51703},{\"end\":51729,\"start\":51722},{\"end\":51741,\"start\":51737},{\"end\":52111,\"start\":52106},{\"end\":52121,\"start\":52117},{\"end\":52137,\"start\":52132},{\"end\":52150,\"start\":52147},{\"end\":52464,\"start\":52459},{\"end\":52478,\"start\":52472},{\"end\":52731,\"start\":52726},{\"end\":52740,\"start\":52737},{\"end\":52749,\"start\":52746},{\"end\":52759,\"start\":52755},{\"end\":52774,\"start\":52769},{\"end\":52788,\"start\":52784},{\"end\":52804,\"start\":52799},{\"end\":53268,\"start\":53264},{\"end\":53278,\"start\":53274},{\"end\":53288,\"start\":53285},{\"end\":53301,\"start\":53298},{\"end\":53313,\"start\":53310},{\"end\":53324,\"start\":53320},{\"end\":53752,\"start\":53748},{\"end\":53760,\"start\":53758},{\"end\":53767,\"start\":53765},{\"end\":53777,\"start\":53774},{\"end\":53788,\"start\":53784},{\"end\":54117,\"start\":54114},{\"end\":54126,\"start\":54124},{\"end\":54137,\"start\":54132},{\"end\":54150,\"start\":54147},{\"end\":54164,\"start\":54159},{\"end\":54174,\"start\":54170},{\"end\":54183,\"start\":54181},{\"end\":54556,\"start\":54550},{\"end\":54571,\"start\":54564},{\"end\":54589,\"start\":54581},{\"end\":54593,\"start\":54591},{\"end\":54600,\"start\":54597},{\"end\":54612,\"start\":54607},{\"end\":54627,\"start\":54620},{\"end\":54640,\"start\":54634},{\"end\":54661,\"start\":54651},{\"end\":54669,\"start\":54663}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1812.08729\",\"id\":\"b0\"},\"end\":30554,\"start\":30068},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":1820089},\"end\":30953,\"start\":30556},{\"attributes\":{\"doi\":\"arXiv:1809.11096\",\"id\":\"b2\"},\"end\":31251,\"start\":30955},{\"attributes\":{\"doi\":\"arXiv:1805.10204\",\"id\":\"b3\"},\"end\":31529,\"start\":31253},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":53208122},\"end\":31899,\"start\":31531},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":4406182},\"end\":32619,\"start\":31901},{\"attributes\":{\"doi\":\"arXiv:1702.07983\",\"id\":\"b6\"},\"end\":33027,\"start\":32621},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":3091656},\"end\":33559,\"start\":33029},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":147704286},\"end\":34080,\"start\":33561},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":2172129},\"end\":34613,\"start\":34082},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":3655946},\"end\":34938,\"start\":34615},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":14631101},\"end\":35592,\"start\":34940},{\"attributes\":{\"id\":\"b12\"},\"end\":35872,\"start\":35594},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":1033682},\"end\":36313,\"start\":35874},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":3389583},\"end\":36713,\"start\":36315},{\"attributes\":{\"doi\":\"arXiv:1503.02531\",\"id\":\"b15\"},\"end\":36965,\"start\":36715},{\"attributes\":{\"doi\":\"arXiv:1904.09751\",\"id\":\"b16\"},\"end\":37249,\"start\":36967},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":3143757},\"end\":37527,\"start\":37251},{\"attributes\":{\"doi\":\"arXiv:1910.13461\",\"id\":\"b18\"},\"end\":38077,\"start\":37529},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":98180},\"end\":38619,\"start\":38079},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":4857922},\"end\":38974,\"start\":38621},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":201304248},\"end\":39662,\"start\":38976},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":165163834},\"end\":40012,\"start\":39664},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":174798232},\"end\":40396,\"start\":40014},{\"attributes\":{\"doi\":\"arXiv:1611.02163\",\"id\":\"b24\"},\"end\":40668,\"start\":40398},{\"attributes\":{\"doi\":\"arXiv:1602.06023\",\"id\":\"b25\"},\"end\":41013,\"start\":40670},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":53200762},\"end\":41362,\"start\":41015},{\"attributes\":{\"doi\":\"10.18653/v1/D17-1238\",\"id\":\"b27\",\"matched_paper_id\":1929239},\"end\":42016,\"start\":41364},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":11080756},\"end\":42609,\"start\":42018},{\"attributes\":{\"doi\":\"arXiv:1705.04304\",\"id\":\"b29\"},\"end\":42885,\"start\":42611},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":160025533},\"end\":43201,\"start\":42887},{\"attributes\":{\"doi\":\"arXiv:1910.10683\",\"id\":\"b31\"},\"end\":43683,\"start\":43203},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":11816014},\"end\":44209,\"start\":43685},{\"attributes\":{\"doi\":\"arXiv:1511.06732\",\"id\":\"b33\"},\"end\":44532,\"start\":44211},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":9008376},\"end\":44969,\"start\":44534},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":16046818},\"end\":45314,\"start\":44971},{\"attributes\":{\"doi\":\"arXiv:1707.06347\",\"id\":\"b36\"},\"end\":45622,\"start\":45316},{\"attributes\":{\"doi\":\"arXiv:2002.10375\",\"id\":\"b37\"},\"end\":46014,\"start\":45624},{\"attributes\":{\"doi\":\"arXiv:1806.04936\",\"id\":\"b38\"},\"end\":46308,\"start\":46016},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":53079743},\"end\":46792,\"start\":46310},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":7961699},\"end\":47113,\"start\":46794},{\"attributes\":{\"id\":\"b41\"},\"end\":47294,\"start\":47115},{\"attributes\":{\"doi\":\"arXiv:1810.12686\",\"id\":\"b42\"},\"end\":47566,\"start\":47296},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":9311215},\"end\":47894,\"start\":47568},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":13756489},\"end\":48323,\"start\":47896},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":143424870},\"end\":48858,\"start\":48325},{\"attributes\":{\"doi\":\"10.18653/v1/W18-5446\",\"id\":\"b46\",\"matched_paper_id\":5034059},\"end\":49660,\"start\":48860},{\"attributes\":{\"id\":\"b47\"},\"end\":49989,\"start\":49662},{\"attributes\":{\"doi\":\"arXiv:1908.04319\",\"id\":\"b48\"},\"end\":50337,\"start\":49991},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":2332513},\"end\":50621,\"start\":50339},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":14711886},\"end\":50910,\"start\":50623},{\"attributes\":{\"doi\":\"arXiv:2002.07767\",\"id\":\"b51\"},\"end\":51264,\"start\":50912},{\"attributes\":{\"doi\":\"arXiv:1609.05473\",\"id\":\"b52\"},\"end\":51602,\"start\":51266},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":168169824},\"end\":52013,\"start\":51604},{\"attributes\":{\"doi\":\"arXiv:1912.08777\",\"id\":\"b54\"},\"end\":52361,\"start\":52015},{\"attributes\":{\"doi\":\"arXiv:1909.06356\",\"id\":\"b55\"},\"end\":52668,\"start\":52363},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":26668832},\"end\":53196,\"start\":52670},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":9745861},\"end\":53655,\"start\":53198},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":211003742},\"end\":54043,\"start\":53657},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":3636178},\"end\":54546,\"start\":54045},{\"attributes\":{\"doi\":\"arXiv:1909.08593\",\"id\":\"b60\"},\"end\":54968,\"start\":54548}]", "bib_title": "[{\"end\":30629,\"start\":30556},{\"end\":31558,\"start\":31531},{\"end\":31956,\"start\":31901},{\"end\":33093,\"start\":33029},{\"end\":33646,\"start\":33561},{\"end\":34151,\"start\":34082},{\"end\":34665,\"start\":34615},{\"end\":35012,\"start\":34940},{\"end\":35901,\"start\":35874},{\"end\":36384,\"start\":36315},{\"end\":37297,\"start\":37251},{\"end\":38130,\"start\":38079},{\"end\":38664,\"start\":38621},{\"end\":39019,\"start\":38976},{\"end\":39699,\"start\":39664},{\"end\":40059,\"start\":40014},{\"end\":41067,\"start\":41015},{\"end\":41406,\"start\":41364},{\"end\":42080,\"start\":42018},{\"end\":42938,\"start\":42887},{\"end\":43744,\"start\":43685},{\"end\":44589,\"start\":44534},{\"end\":45003,\"start\":44971},{\"end\":46372,\"start\":46310},{\"end\":46844,\"start\":46794},{\"end\":47638,\"start\":47568},{\"end\":47921,\"start\":47896},{\"end\":48407,\"start\":48325},{\"end\":48945,\"start\":48860},{\"end\":50428,\"start\":50339},{\"end\":50699,\"start\":50623},{\"end\":51638,\"start\":51604},{\"end\":52718,\"start\":52670},{\"end\":53255,\"start\":53198},{\"end\":53734,\"start\":53657},{\"end\":54104,\"start\":54045}]", "bib_author": "[{\"end\":30136,\"start\":30125},{\"end\":30153,\"start\":30136},{\"end\":30167,\"start\":30153},{\"end\":30181,\"start\":30167},{\"end\":30194,\"start\":30181},{\"end\":30209,\"start\":30194},{\"end\":30222,\"start\":30209},{\"end\":30241,\"start\":30222},{\"end\":30262,\"start\":30241},{\"end\":30275,\"start\":30262},{\"end\":30644,\"start\":30631},{\"end\":30659,\"start\":30644},{\"end\":30675,\"start\":30659},{\"end\":30689,\"start\":30675},{\"end\":31037,\"start\":31023},{\"end\":31051,\"start\":31037},{\"end\":31067,\"start\":31051},{\"end\":31271,\"start\":31253},{\"end\":31283,\"start\":31271},{\"end\":31301,\"start\":31283},{\"end\":31576,\"start\":31560},{\"end\":31590,\"start\":31576},{\"end\":31605,\"start\":31590},{\"end\":31622,\"start\":31605},{\"end\":31637,\"start\":31622},{\"end\":31654,\"start\":31637},{\"end\":31976,\"start\":31958},{\"end\":31994,\"start\":31976},{\"end\":32007,\"start\":31994},{\"end\":32019,\"start\":32007},{\"end\":32638,\"start\":32621},{\"end\":32651,\"start\":32638},{\"end\":32664,\"start\":32651},{\"end\":32678,\"start\":32664},{\"end\":32682,\"start\":32678},{\"end\":33110,\"start\":33095},{\"end\":33132,\"start\":33110},{\"end\":33657,\"start\":33648},{\"end\":33667,\"start\":33657},{\"end\":33680,\"start\":33667},{\"end\":33690,\"start\":33680},{\"end\":33704,\"start\":33690},{\"end\":33713,\"start\":33704},{\"end\":33727,\"start\":33713},{\"end\":33738,\"start\":33727},{\"end\":33754,\"start\":33738},{\"end\":34163,\"start\":34153},{\"end\":34175,\"start\":34163},{\"end\":34190,\"start\":34175},{\"end\":34682,\"start\":34667},{\"end\":34698,\"start\":34682},{\"end\":34712,\"start\":34698},{\"end\":35030,\"start\":35014},{\"end\":35047,\"start\":35030},{\"end\":35065,\"start\":35047},{\"end\":35088,\"start\":35065},{\"end\":35100,\"start\":35088},{\"end\":35115,\"start\":35100},{\"end\":35129,\"start\":35115},{\"end\":35139,\"start\":35129},{\"end\":35630,\"start\":35615},{\"end\":35641,\"start\":35630},{\"end\":35657,\"start\":35641},{\"end\":35667,\"start\":35657},{\"end\":35687,\"start\":35667},{\"end\":35701,\"start\":35687},{\"end\":35717,\"start\":35701},{\"end\":35729,\"start\":35717},{\"end\":35919,\"start\":35903},{\"end\":35939,\"start\":35919},{\"end\":35952,\"start\":35939},{\"end\":35961,\"start\":35952},{\"end\":35981,\"start\":35961},{\"end\":35996,\"start\":35981},{\"end\":36013,\"start\":35996},{\"end\":36028,\"start\":36013},{\"end\":36399,\"start\":36386},{\"end\":36408,\"start\":36399},{\"end\":36417,\"start\":36408},{\"end\":36431,\"start\":36417},{\"end\":36440,\"start\":36431},{\"end\":36450,\"start\":36440},{\"end\":36732,\"start\":36715},{\"end\":36747,\"start\":36732},{\"end\":36758,\"start\":36747},{\"end\":36981,\"start\":36967},{\"end\":36991,\"start\":36981},{\"end\":36998,\"start\":36991},{\"end\":37014,\"start\":36998},{\"end\":37026,\"start\":37014},{\"end\":37311,\"start\":37299},{\"end\":37325,\"start\":37311},{\"end\":37656,\"start\":37644},{\"end\":37668,\"start\":37656},{\"end\":37681,\"start\":37668},{\"end\":37703,\"start\":37681},{\"end\":37724,\"start\":37703},{\"end\":37735,\"start\":37724},{\"end\":37749,\"start\":37735},{\"end\":37767,\"start\":37749},{\"end\":38142,\"start\":38132},{\"end\":38155,\"start\":38142},{\"end\":38168,\"start\":38155},{\"end\":38184,\"start\":38168},{\"end\":38197,\"start\":38184},{\"end\":38211,\"start\":38197},{\"end\":38677,\"start\":38666},{\"end\":38688,\"start\":38677},{\"end\":38701,\"start\":38688},{\"end\":38717,\"start\":38701},{\"end\":38732,\"start\":38717},{\"end\":39031,\"start\":39021},{\"end\":39047,\"start\":39031},{\"end\":39741,\"start\":39701},{\"end\":39758,\"start\":39741},{\"end\":39770,\"start\":39758},{\"end\":39775,\"start\":39770},{\"end\":40104,\"start\":40061},{\"end\":40120,\"start\":40104},{\"end\":40131,\"start\":40120},{\"end\":40141,\"start\":40131},{\"end\":40451,\"start\":40440},{\"end\":40462,\"start\":40451},{\"end\":40474,\"start\":40462},{\"end\":40497,\"start\":40474},{\"end\":40763,\"start\":40745},{\"end\":40775,\"start\":40763},{\"end\":40792,\"start\":40775},{\"end\":40804,\"start\":40792},{\"end\":41086,\"start\":41069},{\"end\":41103,\"start\":41086},{\"end\":41122,\"start\":41103},{\"end\":41429,\"start\":41408},{\"end\":41443,\"start\":41429},{\"end\":41464,\"start\":41443},{\"end\":41479,\"start\":41464},{\"end\":42100,\"start\":42082},{\"end\":42114,\"start\":42100},{\"end\":42125,\"start\":42114},{\"end\":42139,\"start\":42125},{\"end\":42681,\"start\":42666},{\"end\":42696,\"start\":42681},{\"end\":42712,\"start\":42696},{\"end\":42954,\"start\":42940},{\"end\":42966,\"start\":42954},{\"end\":42979,\"start\":42966},{\"end\":42991,\"start\":42979},{\"end\":43005,\"start\":42991},{\"end\":43021,\"start\":43005},{\"end\":43300,\"start\":43286},{\"end\":43314,\"start\":43300},{\"end\":43328,\"start\":43314},{\"end\":43343,\"start\":43328},{\"end\":43358,\"start\":43343},{\"end\":43374,\"start\":43358},{\"end\":43386,\"start\":43374},{\"end\":43394,\"start\":43386},{\"end\":43407,\"start\":43394},{\"end\":43764,\"start\":43746},{\"end\":43776,\"start\":43764},{\"end\":43796,\"start\":43776},{\"end\":43809,\"start\":43796},{\"end\":44225,\"start\":44211},{\"end\":44240,\"start\":44225},{\"end\":44256,\"start\":44240},{\"end\":44271,\"start\":44256},{\"end\":44280,\"start\":44271},{\"end\":44609,\"start\":44591},{\"end\":44624,\"start\":44609},{\"end\":44644,\"start\":44624},{\"end\":45020,\"start\":45005},{\"end\":45035,\"start\":45020},{\"end\":45050,\"start\":45035},{\"end\":45066,\"start\":45050},{\"end\":45082,\"start\":45066},{\"end\":45331,\"start\":45316},{\"end\":45345,\"start\":45331},{\"end\":45364,\"start\":45345},{\"end\":45378,\"start\":45364},{\"end\":45391,\"start\":45378},{\"end\":45640,\"start\":45624},{\"end\":45658,\"start\":45640},{\"end\":46093,\"start\":46072},{\"end\":46111,\"start\":46093},{\"end\":46126,\"start\":46111},{\"end\":46387,\"start\":46374},{\"end\":46399,\"start\":46387},{\"end\":46414,\"start\":46399},{\"end\":46862,\"start\":46846},{\"end\":46877,\"start\":46862},{\"end\":46888,\"start\":46877},{\"end\":47167,\"start\":47156},{\"end\":47184,\"start\":47167},{\"end\":47191,\"start\":47184},{\"end\":47307,\"start\":47296},{\"end\":47322,\"start\":47307},{\"end\":47337,\"start\":47322},{\"end\":47354,\"start\":47337},{\"end\":47655,\"start\":47640},{\"end\":47671,\"start\":47655},{\"end\":47939,\"start\":47923},{\"end\":47953,\"start\":47939},{\"end\":47966,\"start\":47953},{\"end\":47983,\"start\":47966},{\"end\":47996,\"start\":47983},{\"end\":48011,\"start\":47996},{\"end\":48026,\"start\":48011},{\"end\":48044,\"start\":48026},{\"end\":48420,\"start\":48409},{\"end\":48440,\"start\":48420},{\"end\":48455,\"start\":48440},{\"end\":48472,\"start\":48455},{\"end\":48488,\"start\":48472},{\"end\":48500,\"start\":48488},{\"end\":48511,\"start\":48500},{\"end\":48526,\"start\":48511},{\"end\":48958,\"start\":48947},{\"end\":48975,\"start\":48958},{\"end\":48991,\"start\":48975},{\"end\":49003,\"start\":48991},{\"end\":49014,\"start\":49003},{\"end\":49029,\"start\":49014},{\"end\":49727,\"start\":49716},{\"end\":49741,\"start\":49727},{\"end\":49756,\"start\":49741},{\"end\":49772,\"start\":49756},{\"end\":49784,\"start\":49772},{\"end\":49803,\"start\":49784},{\"end\":49821,\"start\":49803},{\"end\":50005,\"start\":49991},{\"end\":50019,\"start\":50005},{\"end\":50035,\"start\":50019},{\"end\":50048,\"start\":50035},{\"end\":50063,\"start\":50048},{\"end\":50077,\"start\":50063},{\"end\":50440,\"start\":50430},{\"end\":50450,\"start\":50440},{\"end\":50711,\"start\":50701},{\"end\":50727,\"start\":50711},{\"end\":50735,\"start\":50727},{\"end\":50997,\"start\":50984},{\"end\":51011,\"start\":50997},{\"end\":51026,\"start\":51011},{\"end\":51039,\"start\":51026},{\"end\":51052,\"start\":51039},{\"end\":51277,\"start\":51266},{\"end\":51291,\"start\":51277},{\"end\":51301,\"start\":51291},{\"end\":51317,\"start\":51301},{\"end\":51655,\"start\":51640},{\"end\":51669,\"start\":51655},{\"end\":51685,\"start\":51669},{\"end\":51699,\"start\":51685},{\"end\":51712,\"start\":51699},{\"end\":51731,\"start\":51712},{\"end\":51743,\"start\":51731},{\"end\":52113,\"start\":52097},{\"end\":52123,\"start\":52113},{\"end\":52139,\"start\":52123},{\"end\":52152,\"start\":52139},{\"end\":52466,\"start\":52452},{\"end\":52480,\"start\":52466},{\"end\":52733,\"start\":52720},{\"end\":52742,\"start\":52733},{\"end\":52751,\"start\":52742},{\"end\":52761,\"start\":52751},{\"end\":52776,\"start\":52761},{\"end\":52790,\"start\":52776},{\"end\":52806,\"start\":52790},{\"end\":53270,\"start\":53257},{\"end\":53280,\"start\":53270},{\"end\":53290,\"start\":53280},{\"end\":53303,\"start\":53290},{\"end\":53315,\"start\":53303},{\"end\":53326,\"start\":53315},{\"end\":53754,\"start\":53736},{\"end\":53762,\"start\":53754},{\"end\":53769,\"start\":53762},{\"end\":53779,\"start\":53769},{\"end\":53790,\"start\":53779},{\"end\":54119,\"start\":54106},{\"end\":54128,\"start\":54119},{\"end\":54139,\"start\":54128},{\"end\":54152,\"start\":54139},{\"end\":54166,\"start\":54152},{\"end\":54176,\"start\":54166},{\"end\":54185,\"start\":54176},{\"end\":54558,\"start\":54548},{\"end\":54573,\"start\":54558},{\"end\":54591,\"start\":54573},{\"end\":54595,\"start\":54591},{\"end\":54602,\"start\":54595},{\"end\":54614,\"start\":54602},{\"end\":54629,\"start\":54614},{\"end\":54642,\"start\":54629},{\"end\":54663,\"start\":54642},{\"end\":54671,\"start\":54663}]", "bib_venue": "[{\"end\":30123,\"start\":30068},{\"end\":30738,\"start\":30689},{\"end\":31021,\"start\":30955},{\"end\":31368,\"start\":31317},{\"end\":31706,\"start\":31654},{\"end\":32161,\"start\":32019},{\"end\":32806,\"start\":32698},{\"end\":33207,\"start\":33132},{\"end\":33803,\"start\":33754},{\"end\":34277,\"start\":34190},{\"end\":34764,\"start\":34712},{\"end\":35207,\"start\":35139},{\"end\":35613,\"start\":35594},{\"end\":36077,\"start\":36028},{\"end\":36506,\"start\":36450},{\"end\":36818,\"start\":36774},{\"end\":37086,\"start\":37042},{\"end\":37374,\"start\":37325},{\"end\":37642,\"start\":37529},{\"end\":38297,\"start\":38211},{\"end\":38781,\"start\":38732},{\"end\":39222,\"start\":39047},{\"end\":39824,\"start\":39775},{\"end\":40190,\"start\":40141},{\"end\":40438,\"start\":40398},{\"end\":40743,\"start\":40670},{\"end\":41171,\"start\":41122},{\"end\":41585,\"start\":41499},{\"end\":42222,\"start\":42139},{\"end\":42664,\"start\":42611},{\"end\":43032,\"start\":43021},{\"end\":43284,\"start\":43203},{\"end\":43895,\"start\":43809},{\"end\":44350,\"start\":44296},{\"end\":44726,\"start\":44644},{\"end\":45126,\"start\":45082},{\"end\":45446,\"start\":45407},{\"end\":45802,\"start\":45674},{\"end\":46070,\"start\":46016},{\"end\":46500,\"start\":46414},{\"end\":46937,\"start\":46888},{\"end\":47154,\"start\":47115},{\"end\":47409,\"start\":47370},{\"end\":47715,\"start\":47671},{\"end\":48093,\"start\":48044},{\"end\":48575,\"start\":48526},{\"end\":49151,\"start\":49049},{\"end\":49714,\"start\":49662},{\"end\":50142,\"start\":50093},{\"end\":50466,\"start\":50450},{\"end\":50753,\"start\":50735},{\"end\":50982,\"start\":50912},{\"end\":51412,\"start\":51333},{\"end\":51792,\"start\":51743},{\"end\":52095,\"start\":52015},{\"end\":52450,\"start\":52363},{\"end\":52874,\"start\":52806},{\"end\":53402,\"start\":53326},{\"end\":53842,\"start\":53790},{\"end\":54279,\"start\":54185},{\"end\":54737,\"start\":54687},{\"end\":32290,\"start\":32163},{\"end\":33269,\"start\":33209},{\"end\":34351,\"start\":34279},{\"end\":35262,\"start\":35209},{\"end\":38370,\"start\":38299},{\"end\":39384,\"start\":39224},{\"end\":41677,\"start\":41587},{\"end\":42292,\"start\":42224},{\"end\":43968,\"start\":43897},{\"end\":46573,\"start\":46502},{\"end\":49257,\"start\":49153},{\"end\":52929,\"start\":52876}]"}}}, "year": 2023, "month": 12, "day": 17}