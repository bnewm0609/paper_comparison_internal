{"id": 5458500, "updated": "2022-10-25 00:53:02.678", "metadata": {"title": "Modeling Relational Data with Graph Convolutional Networks", "authors": "[{\"first\":\"Michael\",\"last\":\"Schlichtkrull\",\"middle\":[]},{\"first\":\"Thomas\",\"last\":\"Kipf\",\"middle\":[\"N.\"]},{\"first\":\"Peter\",\"last\":\"Bloem\",\"middle\":[]},{\"first\":\"Rianne\",\"last\":\"Berg\",\"middle\":[\"van\",\"den\"]},{\"first\":\"Ivan\",\"last\":\"Titov\",\"middle\":[]},{\"first\":\"Max\",\"last\":\"Welling\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2017, "month": null, "day": null}, "abstract": "Knowledge bases play a crucial role in many applications, for example question answering and information retrieval. Despite the great effort invested in creating and maintaining them, even the largest representatives (e.g., Yago, DBPedia or Wikidata) are highly incomplete. We introduce relational graph convolutional networks (R-GCNs) and apply them to two standard knowledge base completion tasks: link prediction (recovery of missing facts, i.e.~subject-predicate-object triples) and entity classification (recovery of missing attributes of entities). R-GCNs are a generalization of graph convolutional networks, a recent class of neural networks operating on graphs, and are developed specifically to deal with highly multi-relational data, characteristic of realistic knowledge bases. Our methods achieve competitive performance on standard benchmarks for both tasks, demonstrating especially promising results on the challenging FB15k-237 subset of Freebase.", "fields_of_study": "[\"Mathematics\",\"Computer Science\"]", "external_ids": {"arxiv": "1703.06103", "mag": "2949926081", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/esws/SchlichtkrullKB18", "doi": "10.1007/978-3-319-93417-4_38"}}, "content": {"source": {"pdf_hash": "9cd6f3bafe12e4445dfd11b7f7dfd11b3d111e73", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://arxiv.org/pdf/1703.06103", "status": "GREEN"}}, "grobid": {"id": "8af1aa04f2ccc99b19a2575f3e164f048498f8c3", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/9cd6f3bafe12e4445dfd11b7f7dfd11b3d111e73.txt", "contents": "\nModeling Relational Data with Graph Convolutional Networks\nSpringerCopyright Springer2018. 2018. June 3-7. 2018\n\nM Schlichtkrull \nT N Kipf \nP Bloem \nVan Den \nR Berg \nI Titov \nM Welling \n\nUniversity of Amsterdam\nUniversity of Amsterdam\nAmsterdam\n\n\nCIFAR \u2020\nUniversity of Amsterdam\nUniversity of Amsterdam\nUniversity of Amsterdam\n\n\nModeling Relational Data with Graph Convolutional Networks\n\nLecture Notes in Computer Science\nHeraklion, Crete, Greece; ChamSpringer108432018. 2018. June 3-7. 201810.1007/978-3-319-93417-4_38Download date: 30 Apr 2020UvA-DARE is a service provided by the library of the University of Amsterdam (http://dare.uva.nl) Published in: The Semantic Web Link to publication Citation for published version (APA): General rights It is not permitted to download or to forward/distribute the text or part of it without the consent of the author(s) and/or copyright holder(s), other than for strictly personal, individual use, unless the work is under an open content license (like Creative Commons). Disclaimer/Complaints regulations If you believe that digital publication of certain material infringes any of your rights or (privacy) interests, please let the Library know, stating your reasons. In case of a legitimate complaint, the Library will make the material inaccessible and/or remove it from the website. Please Ask the Library: https://uba.uva.nl/en/contact, or a letter to: Library of the University of Amsterdam, Secretariat, Singel 425, 1012 WP Amsterdam, The Netherlands. You will be contacted as soon as possible.\nSchlichtkrull, M.; Kipf, T.N.; Bloem, P.; van den Berg, R.; Titov, I.; Welling, M.Abstract Knowledge graphs enable a wide variety of applications, including question answering and information retrieval. Despite the great effort invested in their creation and maintenance, even the largest (e.g., Yago, DBPedia or Wikidata) remain incomplete. We introduce Relational Graph Convolutional Networks (R-GCNs) and apply them to two standard knowledge base completion tasks: Link prediction (recovery of missing facts, i.e. subject-predicate-object triples) and entity classification (recovery of missing entity attributes). R-GCNs are related to a recent class of neural networks operating on graphs, and are developed specifically to deal with the highly multi-relational data characteristic of realistic knowledge bases. We demonstrate the effectiveness of R-GCNs as a stand-alone model for entity classification. We further show that factorization models for link prediction such as DistMult can be significantly improved by enriching them with an encoder model to accumulate evidence over multiple inference steps in the relational graph, demonstrating a large improvement of 29.8% on FB15k-237 over a decoder-only baseline.\n\nIntroduction\n\nKnowledge bases organize and store factual knowledge, enabling a multitude of applications including question answering (Yao and Van Durme 2014; Bao et al. 2014;Seyler, Yahya, and Berberich 2015;Hixon, Clark, and Hajishirzi 2015;Bordes et al. 2015;Dong et al. 2015) and information retrieval (Kotov and Zhai 2012;Dalton, Dietz, and Allan 2014;Xiong and Callan 2015b;2015a). Even the largest knowledge bases (e.g. DBPedia, Wikidata or Yago), despite enormous effort invested in their maintenance, are incomplete, and the lack of coverage harms downstream applications. Predicting missing information in knowledge bases is the main focus of statistical relational learning (SRL).\n\nFollowing previous work on SRL, we assume that knowledge bases store collections of triples of the form (subject, predicate, object). Consider, for example, the triple (Mikhail Baryshnikov, educated at, Vaganova Academy), where we will refer to Baryshnikov and Vaganova Academy as entities and to educated at as a relation. Additionally, we assume that entities are labeled with types (e.g., Vaganova * Equal contribution. The nodes are entities, the edges are relations labeled with their types, the nodes are labeled with entity types (e.g., university). The edge and the node label shown in red are the missing information to be inferred.\n\nAcademy is marked as a university). It is convenient to represent knowledge bases as directed labeled multigraphs with entities corresponding to nodes and triples encoded by labeled edges (see Figure 1). We consider two fundamental SRL tasks: link prediction (recovery of missing triples) and entity classification (assigning types or categorical properties to entities). In both cases, many missing pieces of information can be expected to reside within the graph encoded through the neighborhood structure -i.e. knowing that Mikhail Baryshnikov was educated at the Vaganova Academy implies both that Mikhail Baryshnikov should have the label person, and that the triple (Mikhail Baryshnikov, lived in, Russia) must belong to the knowledge graph. Following this intuition, we develop an encoder model for entities in the relational graph and apply it to both tasks. Our entity classification model, similarly to Kipf and Welling (2017), uses softmax classifiers at each node in the graph. The classifiers take node representations supplied by a relational graph convolutional network (R-GCN) and predict the labels. The model, including R-GCN parameters, is learned by optimizing the cross-entropy loss.\n\nOur link prediction model can be regarded as an autoencoder consisting of (1) an encoder: an R-GCN producing latent feature representations of entities, and (2) a decoder: a tensor factorization model exploiting these representations to predict labeled edges. Though in principle the decoder can rely on any type of factorization (or generally any scoring function), we use one of the simplest and most effective factorization methods: DistMult . We observe that our method achieves competitive results on standard benchmarks, outperforming, among other baselines, direct optimization of the factorization (i.e. vanilla DistMult). This improvement is especially large when we consider the more challenging FB15k-237 dataset (Toutanova and Chen 2015). This result demonstrates that explicit modeling of neighborhoods in R-GCNs is beneficial for recovering missing facts in knowledge bases.\n\nOur main contributions are as follows. To the best of our knowledge, we are the first to show that the GCN framework can be applied to modeling relational data, specifically to link prediction and entity classification tasks. Secondly, we introduce techniques for parameter sharing and to enforce sparsity constraints, and use them to apply R-GCNs to multigraphs with large numbers of relations. Lastly, we show that the performance of factorization models, at the example of DistMult, can be significantly improved by enriching them with an encoder model that performs multiple steps of information propagation in the relational graph.\n\n\nNeural relational modeling\n\nWe introduce the following notation: we denote directed and labeled multi-graphs as G = (V, E, R) with nodes (entities) v i \u2208 V and labeled edges (relations) (v i , r, v j ) \u2208 E, where r \u2208 R is a relation type. 1\n\n\nRelational graph convolutional networks\n\nOur model is primarily motivated as an extension of GCNs that operate on local graph neighborhoods (Duvenaud et al. 2015;Kipf and Welling 2017) to large-scale relational data. These and related methods such as graph neural networks (Scarselli et al. 2009) can be understood as special cases of a simple differentiable message-passing framework (Gilmer et al. 2017):\nh (l+1) i = \u03c3 m\u2208Mi g m (h (l) i , h (l) j ) ,(1)\nwhere h\n(l) i \u2208 R d (l)\nis the hidden state of node v i in the l-th layer of the neural network, with d (l) being the dimensionality of this layer's representations. Incoming messages of the form g m (\u00b7, \u00b7) are accumulated and passed through an element-wise activation function \u03c3(\u00b7), such as the ReLU(\u00b7) = max(0, \u00b7). 2 M i denotes the set of incoming messages for node v i and is often chosen to be identical to the set of incoming edges. g m (\u00b7, \u00b7) is typically chosen to be a (message-specific) neural network-like function or simply a linear transformation g m (h i , h j ) = W h j with a weight matrix W such as in Kipf and Welling (2017).\n\nThis type of transformation has been shown to be very effective at accumulating and encoding features from local, structured neighborhoods, and has led to significant improvements in areas such as graph classification (Duvenaud et al. 2015) and graph-based semi-supervised learning (Kipf and Welling 2017).\n\nMotivated by these architectures, we define the following simple propagation model for calculating the forward-pass update of an entity or node denoted by v i in a relational (directed and labeled) multi-graph:\nh (l+1) i = \u03c3 \uf8eb \uf8ed r\u2208R j\u2208N r i 1 c i,r W (l) r h (l) j + W (l) 0 h (l) i \uf8f6 \uf8f8 ,(2)\nwhere N r i denotes the set of neighbor indices of node i under relation r \u2208 R. c i,r is a problem-specific normalization constant that can either be learned or chosen in advance (such as c i,r = |N r i |). Intuitively, (2) accumulates transformed feature vectors of neighboring nodes through a normalized sum. Different from regular GCNs, we introduce relation-specific transformations, i.e. depending on the type and direction of an edge. To ensure that the representation of a node at layer l + 1 can also be informed by the corresponding representation at layer l, we add a single self-connection of a special relation type to each node in the data. Note that instead of simple linear message transformations, one could choose more flexible functions such as multi-layer neural networks (at the expense of computational efficiency). We leave this for future work.\n\nA neural network layer update consists of evaluating (2) in parallel for every node in the graph. In practice, (2) can be implemented efficiently using sparse matrix multiplications to avoid explicit summation over neighborhoods. Multiple layers can be stacked to allow for dependencies across several relational steps. We refer to this graph encoder model as a relational graph convolutional network (R-GCN). The computation graph for a single node update in the R-GCN model is depicted in Figure 2.\n\n\nRegularization\n\nA central issue with applying (2) to highly multi-relational data is the rapid growth in number of parameters with the number of relations in the graph. In practice this can easily lead to overfitting on rare relations and to models of very large size.\n\nTo address this issue, we introduce two separate methods for regularizing the weights of R-GCN-layers: basisand block-diagonal-decomposition. With the basis decomposition, each W (l) r is defined as follows:\nW (l) r = B b=1 a (l) rb V (l) b ,(3)\ni.e. as a linear combination of basis transformations V\n(l) b \u2208 R d (l+1) \u00d7d (l) with coefficients a (l)\nrb such that only the coefficients depend on r. In the block-diagonal decomposition, we r be defined through the direct sum over a set of low-dimensional matrices:\nW (l) r = B b=1 Q (l) br . (4) Thereby, W (l) r are block-diagonal matrices: diag(Q (l) 1r , . . . , Q (l) Br ) with Q (l) br \u2208 R (d (l+1) /B)\u00d7(d (l) /B) .\nThe basis function decomposition (3) can be seen as a form of effective weight sharing between different relation types, while the block decomposition (4) can be seen as a sparsity constraint on the weight matrices for each relation type. The block decomposition structure encodes an intuition that latent features can be grouped into sets of variables which are more tightly coupled within groups than across groups. Both decompositions reduce the number of parameters needed to learn for highly multi-relational data (such as realistic knowledge bases). At the same time, we expect that the basis parameterization can alleviate overfitting on rare relations, as parameter updates are shared between both rare and more frequent relations.\n\nThe overall R-GCN model then takes the following form: We stack L layers as defined in (2) -the output of the previous layer being the input to the next layer. The input to the first layer can be chosen as a unique one-hot vector for each node in the graph if no other features are present. For the block representation, we map this one-hot vector to a dense representation through a single linear transformation. While Link prediction model with an R-GCN encoder (interspersed with fullyconnected/dense layers) and a DistMult decoder that takes pairs of hidden node representations and produces a score for every (potential) edge in the graph. The loss is evaluated per edge.\n\nwe only consider such a featureless approach in this work, we note that it was shown in Kipf and Welling (2017) that it is possible for this class of models to make use of predefined feature vectors (e.g. a bag-of-words description of a document associated with a specific node).\n\n\nEntity classification\n\nFor (semi-)supervised classification of nodes (entities), we simply stack R-GCN layers of the form (2), with a softmax(\u00b7) activation (per node) on the output of the last layer. We minimize the following cross-entropy loss on all labeled nodes (while ignoring unlabeled nodes):\nL = \u2212 i\u2208Y K k=1 t ik ln h (L) ik ,(5)\nwhere Y is the set of node indices that have labels and h (L) ik is the k-th entry of the network output for the i-th labeled node. t ik denotes its respective ground truth label. In practice, we train the model using (full-batch) gradient descent techniques. A schematic depiction of our entity classification model is given in Figure 3a.\n\n\nLink prediction\n\nLink prediction deals with prediction of new facts (i.e. triples (subject, relation, object)). Formally, the knowledge base is represented by a directed, labeled graph G = (V, E, R). Rather than the full set of edges E, we are given only an incomplete subset\u00ca. The task is to assign scores f (s, r, o) to possible edges (s, r, o) in order to determine how likely those edges are to belong to E.\n\nIn order to tackle this problem, we introduce a graph auto-encoder model, comprised of an entity encoder and a scoring function (decoder). The encoder maps each entity v i \u2208 V to a real-valued vector e i \u2208 R d . The decoder reconstructs edges of the graph relying on the vertex representations; in other words, it scores (subject, relation, object)-triples through a function s :\nR d \u00d7 R \u00d7 R d \u2192 R.\nMost existing approaches to link prediction (for example, tensor and neural factorization methods (Socher et al. 2013;Lin et al. 2015;Toutanova et al. 2016;Yang et al. 2014;Trouillon et al. 2016)) can be interpreted under this framework. The crucial distinguishing characteristic of our work is the reliance on an encoder. Whereas most previous approaches use a single, real-valued vector e i for every v i \u2208 V optimized directly in training, we compute representations through an R-GCN encoder with e i = h (L) i , similar to the graph auto-encoder model introduced in Kipf and Welling (2016) for unlabeled undirected graphs. Our full link prediction model is schematically depicted in Figure 3b.\n\nIn our experiments, we use the DistMult factorization ) as the scoring function, which is known to perform well on standard link prediction benchmarks when used on its own. In DistMult, every relation r is associated with a diagonal matrix R r \u2208 R d\u00d7d and a triple (s, r, o) is scored as\nf (s, r, o) = e T s R r e o .(6)\nAs in previous work on factorization Trouillon et al. 2016), we train the model with negative sampling. For each observed example we sample \u03c9 negative ones. We sample by randomly corrupting either the subject or the object of each positive example. We optimize for cross-entropy loss to push the model to score observable triples higher than the negative ones:\nL = \u2212 1 (1 + \u03c9)|\u00ca| (s,r,o,y)\u2208T y log l f (s, r, o) + (1 \u2212 y) log 1 \u2212 l f (s, r, o) ,(7)\nwhere T is the total set of real and corrupted triples, l is the logistic sigmoid function, and y is an indicator set to y = 1 for positive triples and y = 0 for negative ones.\n\n5 Empirical evaluation\n\n\nEntity classification experiments\n\nHere, we consider the task of classifying entities in a knowledge base. In order to infer, for example, the type of an entity (e.g. person or company), a successful model needs to reason about the relations with other entities that this entity is involved in.\n\nDatasets We evaluate our model on four datasets 3 in Resource Description Framework (RDF) format (Ristoski, de Vries, and Paulheim 2016): AIFB, MUTAG, BGS, and AM. Relations in these datasets need not necessarily encode directed subject-object relations, but are also used to encode the presence, or absence, of a specific feature for a given entity. In each dataset, the targets to be classified are properties of a group of entities represented as nodes. The exact statistics of the datasets can be found in Table 1. For a more detailed description of the datasets the reader is referred to Ristoski, de Vries, and Paulheim (2016). We remove relations that were used to create entity labels: employs and affiliation for AIFB, isMutagenic for MUTAG, hasLithogenesis for BGS, and objectCategory and material for AM.  Rooij 2015), and hand-designed feature extractors (Feat) (Paulheim and F\u00fcmkranz 2012). Feat assembles a feature vector from the in-and out-degree (per relation) of every labeled entity. RDF2Vec extracts walks on labeled graphs which are then processed using the Skipgram (Mikolov et al. 2013) model to generate entity embeddings, used for subsequent classification. See Ristoski and Paulheim (2016) for an in-depth description and discussion of these baseline approaches. All entity classification experiments were run on CPU nodes with 64GB of memory.\n\nResults All results in Table 2   Our model achieves state-of-the-art results on AIFB and AM. To explain the gap in performance on MUTAG and BGS it is important to understand the nature of these datasets. MUTAG is a dataset of molecular graphs, which was later converted to RDF format, where relations either indicate atomic bonds or merely the presence of a certain feature. BGS is a dataset of rock types with hierarchical feature descriptions which was similarly converted to RDF format, where relations encode the presence of a certain feature or feature hierarchy. Labeled entities in MUTAG and BGS are only connected via high-degree hub nodes that encode a certain feature.\n\nWe conjecture that the fixed choice of normalization constant for the aggregation of messages from neighboring nodes is partly to blame for this behavior, which can be particularly problematic for nodes of high degree. A potential way to overcome this limitation is to introduce an attention mechanism, i.e. to replace the normalization constant 1/c i,r with data-dependent attention weights a ij,r , where j,r a ij,r = 1. We expect this to be a promising avenue for future research.\n\n\nLink prediction experiments\n\nAs shown in the previous section, R-GCNs serve as an effective encoder for relational data. We now combine our encoder model with a scoring function (which we will refer to as a decoder, see Figure 3b) to score candidate triples for link prediction in knowledge bases.\n\nDatasets Link prediction algorithms are commonly evaluated on FB15k, a subset of the relational database Freebase, and WN18, a subset of WordNet containing lexical relations between words. In Toutanova and Chen (2015), a serious flaw was observed in both datasets: The presence of inverse triplet pairs t = (e 1 , r, e 2 ) and t = (e 2 , r \u22121 , e 1 ) with t in the training set and t in the test set. This reduces a large part of the prediction task to memorization of affected triplet pairs. A simple baseline LinkFeat employing a linear classifier on top of sparse feature vectors of observed training relations was shown to outperform existing systems by a large margin. To address this issue, Toutanova and Chen proposed a reduced dataset FB15k-237 with all such inverse triplet pairs removed. We therefore choose FB15k-237 as our primary evaluation dataset. Since FB15k and WN18 are still widely used, we also include results on these datasets using the splits introduced by Bordes   Baselines A common baseline for both experiments is direct optimization of DistMult ). This factorization strategy is known to perform well on standard datasets, and furthermore corresponds to a version of our model with fixed entity embeddings in place of the R-GCN encoder as described in Section 4. As a second baseline, we add the simple neighbor-based LinkFeat algorithm proposed in Toutanova and Chen (2015). We further compare to ComplEx (Trouillon et al. 2016) and HolE (Nickel, Rosasco, and Poggio 2015), two state-ofthe-art link prediction models for FB15k and WN18. Com-plEx facilitates modeling of asymmetric relations by generalizing DistMult to the complex domain, while HolE replaces the vector-matrix product with circular correlation. Finally, we include comparisons with two classic algorithms -CP (Hitchcock 1927) and TransE (Bordes et al. 2013).\n\n\nResults\n\nWe provide results using two commonly used evaluation metrics: mean reciprocal rank (MRR) and Hits at n (H@n). Following Bordes et al. (2013), both metrics can be computed in a raw and a filtered setting. We report both filtered and raw MRR (with filtered MRR typically considered more reliable), and filtered Hits at 1, 3, and 10.\n\nWe evaluate hyperparameter choices on the respective validation splits. We found a normalization constant defined as c i,r = c i = r |N r i | -in other words, applied across relation types -to work best. For FB15k and WN18, we report results using basis decomposition (Eq. 3) with two basis functions, and a single encoding layer with 200-dimensional embeddings. For FB15k-237, we found block decomposition (Eq. 4) to perform best, using two layers with block dimension 5 \u00d7 5 and 500-dimensional embeddings. We regularize the encoder through edge dropout applied before normalization, with dropout rate 0.2 for self-loops and 0.4 for other edges. Using edge droupout makes our training objective similar to that of denoising autoencoders (Vincent et al. 2008). We apply l2 regularization to the decoder with a penalty of 0.01.\n\nWe use the Adam optimizer (Kingma and Ba 2014) with a learning rate of 0.01. For the baseline and the other factorizations, we found the parameters from Trouillon et al. (2016) -apart from the dimensionality on FB15k-237to work best, though to make the systems comparable we maintain the same number of negative samples (i.e. \u03c9 = 1). We use full-batch optimization for both the baselines and our model.\n\nOn FB15k, local context in the form of inverse relations  Table 4: Results on the the Freebase and WordNet datasets. Results marked (*) taken from Trouillon et al. (2016). Results marks (**) taken from Nickel, Rosasco, and Poggio (2015). R-GCN+ denotes an ensemble between R-GCN and DistMult -see main text for details.\n\nis expected to dominate the performance of the factorizations, contrasting with the design of the R-GCN model. To better understand the difference, we plot in Figure 4 the FB15k performance of the best R-GCN model and the baseline (DistMult) as functions of degree of nodes corresponding to entities in the considered triple (namely, the average of degrees for the subject and object entities). It can be seen that our model performs better for nodes with high degree where contextual information is abundant. The observation that the two models are complementary suggests combining the strengths of both into a single model, which we refer to as R-GCN+. On FB15k and WN18 where local and longdistance information can both provide strong solutions, we expect R-GCN+ to outperform each individual model. On FB15k-237 where local information is less salient, we do not expect the combination model to outperform a pure R-GCN model significantly. To test this, we evaluate an ensemble (R-GCN+) with a trained R-GCN model and a separately trained DistMult factorization model: f (s, r, t) R-GCN+ = \u03b1f (s, r, t) R-GCN + (1 \u2212 \u03b1)f (s, r, t) DistMult , with \u03b1 = 0.4 selected on FB15k development data.\n\nIn Table 4, we evaluate the R-GCN model and the combination model (R-GCN+) on FB15k and WN18.\n\nOn the FB15k and WN18 datasets, R-GCN and R-GCN+ both outperform the DistMult baseline, but like all other systems underperform on these two datasets compared to the LinkFeat algorithm. The strong result from this baseline highlights the contribution of inverse relation pairs to highperformance solutions on these datasets. Interestingly, R-GCN+ yields better performance than ComplEx for FB15k, even though the R-GCN decoder (DistMult) does not explicitly model asymmetry in relations, as opposed to ComplEx.\n\nThis suggests that combining the R-GCN encoder with the ComplEx scoring function (decoder) may be a promising direction for future work. The choice of scoring function is orthogonal to the choice of encoder; in principle, any scoring function or factorization model could be incorporated as a decoder in our auto-encoder framework.\n\nIn Table 5 Trouillon et al. (2016), while HolE was evaluated using the code published for Nickel, Rosasco, and Poggio (2015).\n\npreviously discussed) inverse relation pairs have been removed and the LinkFeat baseline fails to generalize 4 . Here, our R-GCN model outperforms the DistMult baseline by a large margin of 29.8%, highlighting the importance of a separate encoder model. As expected from our earlier analysis, R-GCN and R-GCN+ show similar performance on this dataset. The R-GCN model further compares favorably against other factorization methods, despite relying on a DistMult decoder which shows comparatively weak performance when used without an encoder.\n\n6 Related Work\n\n\nRelational modeling\n\nOur encoder-decoder approach to link prediction relies on DistMult  in the decoder, a special and simpler case of the RESCAL factorization (Nickel, Tresp, and Kriegel 2011), more effective than the original RESCAL in the context of multi-relational knowledge bases. Numerous alternative factorizations have been proposed and studied in the context of SRL, including both (bi-)linear and nonlinear ones (e.g., (Bordes et al. 2013;Socher et al. 2013;Chang et al. 2014;Nickel, Rosasco, and Poggio 2015;Trouillon et al. 2016)). Many of these approaches can be regarded as modifications or special cases of classic tensor decomposition methods such as CP or Tucker; for a comprehensive overview of tensor decomposition literature we refer the reader to Kolda and Bader (2009). Incorporation of paths between entities in knowledge bases has recently received considerable attention. We can roughly classify previous work into (1) methods creating auxiliary triples, which are then added to the learning objective of a factorization model (Guu, Miller, and Liang 2015;Garcia-Duran, Bordes, and Usunier 2015); (2) approaches using paths (or walks) as features when predicting edges (Lin et al. 2015); or (3) doing both at the same time (Neelakantan, Roth, and McCallum 2015;Toutanova et al. 2016). The first direction is largely orthogonal to ours, as we would also expect improvements from adding similar terms to our loss (in other words, extending our decoder). The second research line is more comparable; R-GCNs provide a computationally cheaper alternative to these path-based models. Direct comparison is somewhat complicated as path-based methods used different datasets (e.g., sub-sampled sets of walks from a knowledge base).\n\n\nNeural networks on graphs\n\nOur R-GCN encoder model is closely related to a number of works in the area of neural networks on graphs. It is primarily motivated as an adaption of previous work on GCNs (Bruna et al. 2014;Duvenaud et al. 2015;Defferrard, Bresson, and Vandergheynst 2016;Kipf and Welling 2017) for large-scale and highly multi-relational data, characteristic of realistic knowledge bases.\n\nEarly work in this area includes the graph neural network by Scarselli et al. (2009). A number of extensions to the original graph neural network have been proposed, most notably (Li et al. 2016) and (Pham et al. 2017), both of which utilize gating mechanisms to facilitate optimization.\n\nR-GCNs can further be seen as a sub-class of message passing neural networks (Gilmer et al. 2017), which encompass a number of previous neural models for graphs, including GCNs, under a differentiable message passing interpretation.\n\n\nConclusions\n\nWe have introduced relational graph convolutional networks (R-GCNs) and demonstrated their effectiveness in the context of two standard statistical relation modeling problems: link prediction and entity classification. For the entity classification problem, we have demonstrated that the R-GCN model can act as a competitive, end-to-end trainable graphbased encoder. For link prediction, the R-GCN model with DistMult factorization as the decoding component outperformed direct optimization of the factorization model, and achieved competitive results on standard link prediction benchmarks. Enriching the factorization model with an R-GCN encoder proved especially valuable for the challenging FB15k-237 dataset, yielding a 29.8% improvement over the decoder-only baseline.\n\nThere are several ways in which our work could be extended. For example, the graph autoencoder model could be considered in combination with other factorization models, such as ComplEx (Trouillon et al. 2016), which can be better suited for modeling asymmetric relations. It is also straightforward to integrate entity features in R-GCNs, which would be beneficial both for link prediction and entity classification problems. To address the scalability of our method, it would be worthwhile to explore subsampling techniques, such as in Hamilton, Ying, and Leskovec (2017). Lastly, it would be promising to replace the current form of summation over neighboring nodes and relation types with a data-dependent attention mechanism. Beyond modeling knowledge bases, R-GCNs can be generalized to other applications where relation factorization models have been shown effective (e.g. relation extraction).\n\n\nFurther experimental details on entity classification\n\nFor the entity classification benchmarks described in our paper, the evaluation process differs subtly between publications. To eliminate these differences, we repeated the baselines in a uniform manner, using the canonical test/train split from (Ristoski, de Vries, and Paulheim 2016). We performed hyperparameter optimization on only the training set, running a single evaluation on the test set after hyperparameters were chosen for each baseline. This explains why the numbers we report differ slightly from those in the original publications (where cross-validation accuracy was reported).\n\nFor WL, we use the tree variant of the Weisfeiler-Lehman subtree kernel from the Mustard library. 5 For RDF2Vec, we use an implementation provided by the authors of (Ristoski and Paulheim 2016) which builds on Mustard. In both cases, we extract explicit feature vectors for the instance nodes, which are classified by a linear SVM.\n\nFor the MUTAG task, our preprocessing differs from that used in (de Vries and de Rooij 2015;Ristoski and Paulheim 2016) where for a given target relation (s, r, o) all triples connecting s to o are removed. Since o is a boolean value in the MUTAG data, one can infer the label after processing from other boolean relations that are still present. This issue is now mentioned in the Mustard documentation. In our preprocessing, we remove only the specific triples encoding the target relation.\n\nHyperparameters for baselines are chosen according to the best model performance in (Ristoski and Paulheim 2016), i.e. WL: 2 (tree depth), 3 (number of iterations); RDF2Vec: 2 (WL tree depth), 4 (WL iterations), 500 (embedding size), 5 (window size), 10 (SkipGram iterations), 25 (number of negative samples). We optimize the SVM regularization constant C \u2208 {0.001, 0.01, 0.1, 1, 10, 100, 1000} based on performance on a 80/20 train/validation split (of the original training set).\n\nFor R-GCN, we choose an l2 penalty on first layer weights C l2 \u2208 {0, 5 \u00b7 10 \u22124 } and the number of basis functions B \u2208 {0, 10, 20, 30, 40} based on validation set performance, where B = 0 refers to using no basis function decomposition. Using the block decomposition did not improve results. Otherwise, hyperparameters are chosen as follows: 50 (number of epochs), 16 (number of hidden units), and c i,r = |N r i | (normalization constant). We do not use dropout. For AM, we use a reduced number of 10 hidden units for R-GCN to reduce the memory footprint.\n\nResults with standard error (omitted in main paper due to spatial constraints) are summarized in Table 7. All entity classification experiments were run on CPU nodes with 64GB of memory.\n\n\nR-GCN setting AIFB MUTAG BGS AM\n\nl2 penalty 0 5 \u00b7 10 \u22124 5 \u00b7 10 \u22124 5 \u00b7 10 \u22124 # basis functions 0 30 40 40 # hidden units 16 16 16 10  Table 7: Entity classification results in accuracy (average and standard error over 10 runs) for a feature-based baseline (see main text for details), WL (Shervashidze et al. 2011;de Vries and de Rooij 2015), RDF2Vec (Ristoski and Paulheim 2016), and R-GCN (this work). Test performance is reported on the train/test set splits provided by (Ristoski, de Vries, and Paulheim 2016).\n\nFigure 1 :\n1A knowledge base fragment:\n\nFigure 2 :\n2Diagram for computing the update of a single graph node/entity (red) in the R-GCN model. Activations (d-dimensional vectors) from neighboring nodes (dark blue) are gathered and then transformed for each relation type individually (for both in-and outgoing edges). The resulting representation (green) is accumulated in a (normalized) sum and passed through an activation function (such as the ReLU). This per-node update can be computed in parallel with shared parameters across the whole graph. let each W (l)\n\nFigure 3 :\n3(a) Depiction of an R-GCN model for entity classification with a per-node loss function. (b)\n\nFigure 4 :\n4Mean reciprocal rank (MRR) for R-GCN and Dist-Mult on the FB15k validation data as a function of the node degree (average of subject and object).\n\nTable 2 :\n2Entity classification results in accuracy (averaged \nover 10 runs) for a feature-based baseline (see main text \nfor details), WL (Shervashidze et al. 2011; de Vries and \nde Rooij 2015), RDF2Vec (Ristoski and Paulheim 2016), \nand R-GCN (this work). Test performance is reported on the \ntrain/test set splits provided by Ristoski, de Vries, and Paul-\nheim (2016). \n\n\n\n\net al. (2013).Dataset \nWN18 \nFB15K FB15k-237 \n\nEntities \n40,943 \n14,951 \n14,541 \nRelations \n18 \n1,345 \n237 \nTrain edges 141,442 483,142 \n272,115 \nVal. edges \n5,000 \n50,000 \n17,535 \nTest edges \n5,000 \n59,071 \n20,466 \n\n\n\nTable 3 :\n3Number of entities and relation types along with the number of edges per split for the three datasets.\n\nTable 5 :\n5Results on FB15k-237, a reduced version of FB15k \nwith problematic inverse relation pairs removed. CP, TransE, \nand ComplEx were evaluated using the code published for \n\n\nTable 6 :\n6Best hyperparameter choices based on validation set performance for 2-layer R-GCN model. Feat 55.55 \u00b1 0.00 77.94 \u00b1 0.00 72.41 \u00b1 0.00 66.66 \u00b1 0.00 WL 80.55 \u00b1 0.00 80.88 \u00b1 0.00 86.20 \u00b1 0.00 87.37 \u00b1 0.00 RDF2Vec 88.88 \u00b1 0.00 67.20 \u00b1 1.24 87.24 \u00b1 0.89 88.33 \u00b1 0.61 R-GCN (Ours) 95.83 \u00b1 0.62 73.23 \u00b1 0.48 83.10 \u00b1 0.80 89.29 \u00b1 0.35Model \nAIFB \nMUTAG \nBGS \nAM \n\n\nR contains relations both in canonical direction (e.g. born in) and in inverse direction (e.g. born in inv).2 Note that this represents a simplification of the message passing neural network proposed in(Gilmer et al. 2017) that suffices to include the aforementioned models as special cases.\nhttp://dws.informatik.uni-mannheim.de/en/research/acollection-of-benchmark-datasets-for-ml\nOur numbers are not directly comparable to those reported inToutanova and Chen (2015), as they use pruning both for training and testing (see their sections 3.3.1 and 4.2). Since their pruning schema is not fully specified (e.g., values of the relation-specific parameter t are not given) and the code is not available, it is not possible to replicate their set-up.\nhttps://github.com/Data2Semantics/mustard\nAcknowledgementsWe would like to thank Diego Marcheggiani, Ethan Fetaya, and Christos Louizos for helpful discussions and comments. This project is supported by the European Research Council (ERC StG BroadSem 678254), the SAP Innovation Center Network and the Dutch National Science Foundation (NWO VIDI 639.022.518).\nKnowledgebased question answering as machine translation. J Bao, N Duan, M Zhou, T Zhao, ACL. Bao, J.; Duan, N.; Zhou, M.; and Zhao, T. 2014. Knowledge- based question answering as machine translation. In ACL.\n\nTranslating embeddings for modeling multi-relational data. A Bordes, N Usunier, A Garcia-Duran, J Weston, O Yakhnenko, NIPS. Bordes, A.; Usunier, N.; Garcia-Duran, A.; Weston, J.; and Yakhnenko, O. 2013. Translating embeddings for modeling multi-relational data. In NIPS.\n\nLarge-scale simple question answering with memory networks. A Bordes, N Usunier, S Chopra, J Weston, arXiv:1506.02075arXiv preprintBordes, A.; Usunier, N.; Chopra, S.; and Weston, J. 2015. Large-scale simple question answering with memory net- works. arXiv preprint arXiv:1506.02075.\n\nSpectral networks and locally connected networks on graphs. J Bruna, W Zaremba, A Szlam, Y Lecun, ICLR. Bruna, J.; Zaremba, W.; Szlam, A.; and LeCun, Y. 2014. Spectral networks and locally connected networks on graphs. In ICLR.\n\nTyped Tensor Decomposition of Knowledge Bases for Relation Extraction. K.-W Chang, W Tau Yih, B Yang, C Meek, EMNLP. Chang, K.-W.; tau Yih, W.; Yang, B.; and Meek, C. 2014. Typed Tensor Decomposition of Knowledge Bases for Rela- tion Extraction. In EMNLP.\n\nEntity query feature expansion using knowledge base links. J Dalton, L Dietz, Allan , J , Proceedings of the 37th international ACM SIGIR conference on Research & development in information retrieval. the 37th international ACM SIGIR conference on Research & development in information retrievalDalton, J.; Dietz, L.; and Allan, J. 2014. Entity query feature expansion using knowledge base links. In Proceedings of the 37th international ACM SIGIR conference on Research & development in information retrieval, 365-374.\n\nSubstructure counting graph kernels for machine learning from rdf data. G K D De Vries, S De Rooij, Web Semantics: Science, Services and Agents on the World Wide Web. 35de Vries, G. K. D., and de Rooij, S. 2015. Substructure counting graph kernels for machine learning from rdf data. Web Semantics: Science, Services and Agents on the World Wide Web 35:71-84.\n\nConvolutional neural networks on graphs with fast localized spectral filtering. M Defferrard, X Bresson, P Vandergheynst, NIPS. Defferrard, M.; Bresson, X.; and Vandergheynst, P. 2016. Convolutional neural networks on graphs with fast localized spectral filtering. In NIPS.\n\nQuestion answering over freebase with multi-column convolutional neural networks. L Dong, F Wei, M Zhou, K Xu, ACL. Dong, L.; Wei, F.; Zhou, M.; and Xu, K. 2015. Question an- swering over freebase with multi-column convolutional neu- ral networks. In ACL.\n\nConvolutional networks on graphs for learning molecular fingerprints. D K Duvenaud, D Maclaurin, J Iparraguirre, R Bombarell, T Hirzel, A Aspuru-Guzik, R P Adams, NIPS. Duvenaud, D. K.; Maclaurin, D.; Iparraguirre, J.; Bombarell, R.; Hirzel, T.; Aspuru-Guzik, A.; and Adams, R. P. 2015. Convolutional networks on graphs for learning molecular fingerprints. In NIPS.\n\nComposing relationships with translations. A Garcia-Duran, A Bordes, N Usunier, CNRS, HeudiasycTechnical ReportGarcia-Duran, A.; Bordes, A.; and Usunier, N. 2015. Com- posing relationships with translations. Technical Report, CNRS, Heudiasyc.\n\nNeural message passing for quantum chemistry. J Gilmer, S S Schoenholz, P F Riley, O Vinyals, G E Dahl, K Guu, J Miller, P Liang, arXiv:1704.01212arXiv:1506.01094Traversing knowledge graphs in vector space. arXiv preprintGilmer, J.; Schoenholz, S. S.; Riley, P. F.; Vinyals, O.; and Dahl, G. E. 2017. Neural message passing for quantum chemistry. arXiv preprint arXiv:1704.01212. Guu, K.; Miller, J.; and Liang, P. 2015. Travers- ing knowledge graphs in vector space. arXiv preprint arXiv:1506.01094.\n\nW L Hamilton, R Ying, J Leskovec, arXiv:1706.02216Inductive representation learning on large graphs. arXiv preprintHamilton, W. L.; Ying, R.; and Leskovec, J. 2017. Induc- tive representation learning on large graphs. arXiv preprint arXiv:1706.02216.\n\nThe expression of a tensor or a polyadic as a sum of products. F L Hitchcock, Studies in Applied Mathematics. 61-4Hitchcock, F. L. 1927. The expression of a tensor or a polyadic as a sum of products. Studies in Applied Mathe- matics 6(1-4):164-189.\n\nLearning knowledge graphs for question answering through conversational dialog. B Hixon, P Clark, H Hajishirzi, Proceedings of NAACL HLT. NAACL HLTHixon, B.; Clark, P.; and Hajishirzi, H. 2015. Learning knowledge graphs for question answering through conver- sational dialog. In Proceedings of NAACL HLT, 851-861.\n\nAdam: A method for stochastic optimization. D Kingma, J Ba, arXiv:1412.6980arXiv preprintKingma, D., and Ba, J. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.\n\nVariational graph autoencoders. T N Kipf, M Welling, arXiv:1611.07308arXiv preprintKipf, T. N., and Welling, M. 2016. Variational graph auto- encoders. arXiv preprint arXiv:1611.07308.\n\nSemi-supervised classification with graph convolutional networks. T N Kipf, M Welling, ICLR. Kipf, T. N., and Welling, M. 2017. Semi-supervised classi- fication with graph convolutional networks. In ICLR.\n\nTensor decompositions and applications. T G Kolda, B W Bader, SIAM review. 513Kolda, T. G., and Bader, B. W. 2009. Tensor decompositions and applications. SIAM review 51(3):455-500.\n\nTapping into knowledge base for concept feedback: leveraging conceptnet to improve search results for difficult queries. A Kotov, C Zhai, WSDM. Kotov, A., and Zhai, C. 2012. Tapping into knowledge base for concept feedback: leveraging conceptnet to improve search results for difficult queries. In WSDM.\n\nGated graph sequence neural networks. Y Li, D Tarlow, M Brockschmidt, R Zemel, ICLR. Li, Y.; Tarlow, D.; Brockschmidt, M.; and Zemel, R. 2016. Gated graph sequence neural networks. In ICLR.\n\n. Y Lin, Z Liu, H Luan, M Sun, S Rao, S Liu, Lin, Y.; Liu, Z.; Luan, H.; Sun, M.; Rao, S.; and Liu, S.\n\nModeling relation paths for representation learning of knowledge bases. arXiv:1506.00379arXiv preprintModeling relation paths for representation learning of knowledge bases. arXiv preprint arXiv:1506.00379.\n\nDistributed representations of words and phrases and their compositionality. T Mikolov, I Sutskever, K Chen, G S Corrado, J Dean, A Neelakantan, B Roth, A Mccallum, M Nickel, L Rosasco, T Poggio, arXiv:1504.06662arXiv:1510.04935Compositional vector space models for knowledge base completion. arXiv preprintHolographic embeddings of knowledge graphsMikolov, T.; Sutskever, I.; Chen, K.; Corrado, G. S.; and Dean, J. 2013. Distributed representations of words and phrases and their compositionality. In NIPS. Neelakantan, A.; Roth, B.; and McCallum, A. 2015. Com- positional vector space models for knowledge base comple- tion. arXiv preprint arXiv:1504.06662. Nickel, M.; Rosasco, L.; and Poggio, T. 2015. Holo- graphic embeddings of knowledge graphs. arXiv preprint arXiv:1510.04935.\n\nA threeway model for collective learning on multi-relational data. M Nickel, V Tresp, H.-P Kriegel, ICML. Nickel, M.; Tresp, V.; and Kriegel, H.-P. 2011. A three- way model for collective learning on multi-relational data. In ICML.\n\nUnsupervised generation of data mining features from linked open data. H Paulheim, J F\u00fcmkranz, Proceedings of the 2nd international conference on web intelligence, mining and semantics. the 2nd international conference on web intelligence, mining and semantics31Paulheim, H., and F\u00fcmkranz, J. 2012. Unsupervised gen- eration of data mining features from linked open data. In Proceedings of the 2nd international conference on web in- telligence, mining and semantics, 31.\n\nColumn networks for collective classification. T Pham, T Tran, D Phung, S Venkatesh, H Paulheim, International Semantic Web Conference. AAAI. Ristoski, P.,SpringerRdf2vec: Rdf graph embeddings for data miningPham, T.; Tran, T.; Phung, D.; and Venkatesh, S. 2017. Col- umn networks for collective classification. In AAAI. Ristoski, P., and Paulheim, H. 2016. Rdf2vec: Rdf graph embeddings for data mining. In International Semantic Web Conference, 498-514. Springer.\n\nA collection of benchmark datasets for systematic evaluations of machine learning on the semantic web. P Ristoski, G K D De Vries, H Paulheim, International Semantic Web Conference. SpringerRistoski, P.; de Vries, G. K. D.; and Paulheim, H. 2016. A collection of benchmark datasets for systematic evaluations of machine learning on the semantic web. In International Semantic Web Conference, 186-194. Springer.\n\nThe graph neural network model. F Scarselli, M Gori, A C Tsoi, M Hagenbuchner, G Monfardini, IEEE Transactions on Neural Networks. 201Scarselli, F.; Gori, M.; Tsoi, A. C.; Hagenbuchner, M.; and Monfardini, G. 2009. The graph neural network model. IEEE Transactions on Neural Networks 20(1):61-80.\n\nGenerating quiz questions from knowledge graphs. D Seyler, M Yahya, K Berberich, N Shervashidze, P Schweitzer, E J V Leeuwen, K Mehlhorn, K M Borgwardt, Proceedings of the 24th International Conference on World Wide Web. the 24th International Conference on World Wide Web12Weisfeilerlehman graph kernelsSeyler, D.; Yahya, M.; and Berberich, K. 2015. Generating quiz questions from knowledge graphs. In Proceedings of the 24th International Conference on World Wide Web. Shervashidze, N.; Schweitzer, P.; Leeuwen, E. J. v.; Mehlhorn, K.; and Borgwardt, K. M. 2011. Weisfeiler- lehman graph kernels. Journal of Machine Learning Re- search 12(Sep):2539-2561.\n\nReasoning with neural tensor networks for knowledge base completion. R Socher, D Chen, C D Manning, A Ng, NIPS. Socher, R.; Chen, D.; Manning, C. D.; and Ng, A. 2013. Reasoning with neural tensor networks for knowledge base completion. In NIPS.\n\nObserved versus latent features for knowledge base and text inference. K Toutanova, Chen , D , Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality. the 3rd Workshop on Continuous Vector Space Models and their CompositionalityToutanova, K., and Chen, D. 2015. Observed versus latent features for knowledge base and text inference. In Proceed- ings of the 3rd Workshop on Continuous Vector Space Mod- els and their Compositionality, 57-66.\n\nCompositional learning of embeddings for relation paths in knowledge base and text. K Toutanova, V Lin, W Yih, H Poon, C Quirk, ACL. Toutanova, K.; Lin, V.; Yih, W.-t.; Poon, H.; and Quirk, C. 2016. Compositional learning of embeddings for relation paths in knowledge base and text. In ACL.\n\nComplex embeddings for simple link prediction. T Trouillon, J Welbl, S Riedel, E Gaussier, G Bouchard, ICML. Trouillon, T.; Welbl, J.; Riedel, S.; Gaussier, E.; and Bouchard, G. 2016. Complex embeddings for simple link prediction. In ICML.\n\nExtracting and composing robust features with denoising autoencoders. P Vincent, H Larochelle, Y Bengio, P.-A Manzagol, ICML. Vincent, P.; Larochelle, H.; Bengio, Y.; and Manzagol, P.- A. 2008. Extracting and composing robust features with denoising autoencoders. In ICML.\n\nEsdrank: Connecting query and documents through external semi-structured data. C Xiong, J Callan, CIKM. Xiong, C., and Callan, J. 2015a. Esdrank: Connecting query and documents through external semi-structured data. In CIKM.\n\nQuery expansion with freebase. C Xiong, J Callan, Proceedings of the 2015 International Conference on The Theory of Information Retrieval. the 2015 International Conference on The Theory of Information RetrievalXiong, C., and Callan, J. 2015b. Query expansion with free- base. In Proceedings of the 2015 International Conference on The Theory of Information Retrieval, 111-120.\n\nEmbedding entities and relations for learning and inference in knowledge bases. B Yang, W Yih, X He, J Gao, L Deng, X Yao, B Van Durme, arXiv:1412.6575ACL. arXiv preprintInformation extraction over structured data: Question answering with freebaseYang, B.; Yih, W.-t.; He, X.; Gao, J.; and Deng, L. 2014. Embedding entities and relations for learning and inference in knowledge bases. arXiv preprint arXiv:1412.6575. Yao, X., and Van Durme, B. 2014. Information extraction over structured data: Question answering with freebase. In ACL.\n", "annotations": {"author": "[{\"end\":130,\"start\":114},{\"end\":140,\"start\":131},{\"end\":149,\"start\":141},{\"end\":158,\"start\":150},{\"end\":166,\"start\":159},{\"end\":175,\"start\":167},{\"end\":186,\"start\":176},{\"end\":246,\"start\":187},{\"end\":329,\"start\":247}]", "publisher": "[{\"end\":68,\"start\":60},{\"end\":462,\"start\":454}]", "author_last_name": "[{\"end\":129,\"start\":116},{\"end\":139,\"start\":135},{\"end\":148,\"start\":143},{\"end\":157,\"start\":150},{\"end\":165,\"start\":161},{\"end\":174,\"start\":169},{\"end\":185,\"start\":178}]", "author_first_name": "[{\"end\":115,\"start\":114},{\"end\":132,\"start\":131},{\"end\":134,\"start\":133},{\"end\":142,\"start\":141},{\"end\":160,\"start\":159},{\"end\":168,\"start\":167},{\"end\":177,\"start\":176}]", "author_affiliation": "[{\"end\":245,\"start\":188},{\"end\":328,\"start\":248}]", "title": "[{\"end\":59,\"start\":1},{\"end\":388,\"start\":330}]", "venue": "[{\"end\":423,\"start\":390}]", "abstract": "[{\"end\":2771,\"start\":1549}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2948,\"start\":2932},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2982,\"start\":2948},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3016,\"start\":2982},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3035,\"start\":3016},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3052,\"start\":3035},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3100,\"start\":3079},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3130,\"start\":3100},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":3153,\"start\":3130},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":3159,\"start\":3153},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5045,\"start\":5022},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6064,\"start\":6039},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7249,\"start\":7227},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7271,\"start\":7249},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7382,\"start\":7360},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7492,\"start\":7472},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8185,\"start\":8162},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8428,\"start\":8406},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8493,\"start\":8470},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12631,\"start\":12608},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":14412,\"start\":14392},{\"end\":14428,\"start\":14412},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":14450,\"start\":14428},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":14467,\"start\":14450},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":14489,\"start\":14467},{\"end\":14872,\"start\":14864},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":15373,\"start\":15351},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":16398,\"start\":16359},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":16894,\"start\":16855},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":17090,\"start\":17079},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":17164,\"start\":17136},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":17370,\"start\":17350},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":17477,\"start\":17449},{\"end\":20084,\"start\":20078},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":20500,\"start\":20475},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":20554,\"start\":20532},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":20599,\"start\":20565},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":20919,\"start\":20903},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":20951,\"start\":20931},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":21105,\"start\":21085},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":22056,\"start\":22035},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":22301,\"start\":22278},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":22699,\"start\":22676},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":22765,\"start\":22731},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":25019,\"start\":24996},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":25109,\"start\":25075},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":25866,\"start\":25833},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":26123,\"start\":26103},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":26142,\"start\":26123},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":26160,\"start\":26142},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":26193,\"start\":26160},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":26215,\"start\":26193},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":26464,\"start\":26442},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":26755,\"start\":26726},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":26794,\"start\":26755},{\"end\":26885,\"start\":26868},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":26960,\"start\":26922},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":26981,\"start\":26960},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":27642,\"start\":27623},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":27663,\"start\":27642},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":27707,\"start\":27663},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":27729,\"start\":27707},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":27910,\"start\":27887},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":28021,\"start\":28005},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":28044,\"start\":28026},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":28212,\"start\":28192},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":29347,\"start\":29324},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":29711,\"start\":29676},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":30382,\"start\":30343},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":31118,\"start\":31107},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":31145,\"start\":31118},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":31632,\"start\":31604},{\"end\":33063,\"start\":33034},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":33090,\"start\":33063},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":33128,\"start\":33100},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":33262,\"start\":33223},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":35576,\"start\":35557},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":35823,\"start\":35798}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":33303,\"start\":33264},{\"attributes\":{\"id\":\"fig_1\"},\"end\":33827,\"start\":33304},{\"attributes\":{\"id\":\"fig_2\"},\"end\":33933,\"start\":33828},{\"attributes\":{\"id\":\"fig_3\"},\"end\":34092,\"start\":33934},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":34469,\"start\":34093},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":34689,\"start\":34470},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":34804,\"start\":34690},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":34986,\"start\":34805},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":35354,\"start\":34987}]", "paragraph": "[{\"end\":3464,\"start\":2787},{\"end\":4107,\"start\":3466},{\"end\":5313,\"start\":4109},{\"end\":6203,\"start\":5315},{\"end\":6841,\"start\":6205},{\"end\":7084,\"start\":6872},{\"end\":7493,\"start\":7128},{\"end\":7550,\"start\":7543},{\"end\":8186,\"start\":7567},{\"end\":8494,\"start\":8188},{\"end\":8706,\"start\":8496},{\"end\":9655,\"start\":8788},{\"end\":10157,\"start\":9657},{\"end\":10428,\"start\":10176},{\"end\":10637,\"start\":10430},{\"end\":10731,\"start\":10676},{\"end\":10944,\"start\":10781},{\"end\":11840,\"start\":11101},{\"end\":12518,\"start\":11842},{\"end\":12799,\"start\":12520},{\"end\":13101,\"start\":12825},{\"end\":13479,\"start\":13140},{\"end\":13893,\"start\":13499},{\"end\":14274,\"start\":13895},{\"end\":14991,\"start\":14294},{\"end\":15280,\"start\":14993},{\"end\":15674,\"start\":15314},{\"end\":15939,\"start\":15763},{\"end\":15963,\"start\":15941},{\"end\":16260,\"start\":16001},{\"end\":17631,\"start\":16262},{\"end\":18311,\"start\":17633},{\"end\":18796,\"start\":18313},{\"end\":19096,\"start\":18828},{\"end\":20952,\"start\":19098},{\"end\":21295,\"start\":20964},{\"end\":22123,\"start\":21297},{\"end\":22527,\"start\":22125},{\"end\":22848,\"start\":22529},{\"end\":24043,\"start\":22850},{\"end\":24138,\"start\":24045},{\"end\":24650,\"start\":24140},{\"end\":24983,\"start\":24652},{\"end\":25110,\"start\":24985},{\"end\":25654,\"start\":25112},{\"end\":25670,\"start\":25656},{\"end\":27421,\"start\":25694},{\"end\":27824,\"start\":27451},{\"end\":28113,\"start\":27826},{\"end\":28347,\"start\":28115},{\"end\":29137,\"start\":28363},{\"end\":30039,\"start\":29139},{\"end\":30691,\"start\":30097},{\"end\":31024,\"start\":30693},{\"end\":31518,\"start\":31026},{\"end\":32001,\"start\":31520},{\"end\":32559,\"start\":32003},{\"end\":32747,\"start\":32561},{\"end\":33263,\"start\":32783}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7542,\"start\":7494},{\"attributes\":{\"id\":\"formula_1\"},\"end\":7566,\"start\":7551},{\"attributes\":{\"id\":\"formula_2\"},\"end\":8787,\"start\":8707},{\"attributes\":{\"id\":\"formula_3\"},\"end\":10675,\"start\":10638},{\"attributes\":{\"id\":\"formula_4\"},\"end\":10780,\"start\":10732},{\"attributes\":{\"id\":\"formula_5\"},\"end\":11100,\"start\":10945},{\"attributes\":{\"id\":\"formula_6\"},\"end\":13139,\"start\":13102},{\"attributes\":{\"id\":\"formula_7\"},\"end\":14293,\"start\":14275},{\"attributes\":{\"id\":\"formula_8\"},\"end\":15313,\"start\":15281},{\"attributes\":{\"id\":\"formula_9\"},\"end\":15762,\"start\":15675}]", "table_ref": "[{\"end\":16779,\"start\":16772},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":17663,\"start\":17656},{\"end\":22594,\"start\":22587},{\"end\":24055,\"start\":24048},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":24995,\"start\":24988},{\"end\":32665,\"start\":32658},{\"end\":32890,\"start\":32883}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2785,\"start\":2773},{\"attributes\":{\"n\":\"2\"},\"end\":6870,\"start\":6844},{\"attributes\":{\"n\":\"2.1\"},\"end\":7126,\"start\":7087},{\"attributes\":{\"n\":\"2.2\"},\"end\":10174,\"start\":10160},{\"attributes\":{\"n\":\"3\"},\"end\":12823,\"start\":12802},{\"attributes\":{\"n\":\"4\"},\"end\":13497,\"start\":13482},{\"attributes\":{\"n\":\"5.1\"},\"end\":15999,\"start\":15966},{\"attributes\":{\"n\":\"5.2\"},\"end\":18826,\"start\":18799},{\"end\":20962,\"start\":20955},{\"attributes\":{\"n\":\"6.1\"},\"end\":25692,\"start\":25673},{\"attributes\":{\"n\":\"6.2\"},\"end\":27449,\"start\":27424},{\"attributes\":{\"n\":\"7\"},\"end\":28361,\"start\":28350},{\"end\":30095,\"start\":30042},{\"end\":32781,\"start\":32750},{\"end\":33275,\"start\":33265},{\"end\":33315,\"start\":33305},{\"end\":33839,\"start\":33829},{\"end\":33945,\"start\":33935},{\"end\":34103,\"start\":34094},{\"end\":34700,\"start\":34691},{\"end\":34815,\"start\":34806},{\"end\":34997,\"start\":34988}]", "table": "[{\"end\":34469,\"start\":34105},{\"end\":34689,\"start\":34486},{\"end\":34986,\"start\":34817},{\"end\":35354,\"start\":35324}]", "figure_caption": "[{\"end\":33303,\"start\":33277},{\"end\":33827,\"start\":33317},{\"end\":33933,\"start\":33841},{\"end\":34092,\"start\":33947},{\"end\":34486,\"start\":34472},{\"end\":34804,\"start\":34702},{\"end\":35324,\"start\":34999}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4310,\"start\":4302},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":10156,\"start\":10148},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":13478,\"start\":13469},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":14990,\"start\":14981},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":19028,\"start\":19019},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":23017,\"start\":23009}]", "bib_author_first_name": "[{\"end\":36523,\"start\":36522},{\"end\":36530,\"start\":36529},{\"end\":36538,\"start\":36537},{\"end\":36546,\"start\":36545},{\"end\":36735,\"start\":36734},{\"end\":36745,\"start\":36744},{\"end\":36756,\"start\":36755},{\"end\":36772,\"start\":36771},{\"end\":36782,\"start\":36781},{\"end\":37009,\"start\":37008},{\"end\":37019,\"start\":37018},{\"end\":37030,\"start\":37029},{\"end\":37040,\"start\":37039},{\"end\":37294,\"start\":37293},{\"end\":37303,\"start\":37302},{\"end\":37314,\"start\":37313},{\"end\":37323,\"start\":37322},{\"end\":37537,\"start\":37533},{\"end\":37546,\"start\":37545},{\"end\":37557,\"start\":37556},{\"end\":37565,\"start\":37564},{\"end\":37779,\"start\":37778},{\"end\":37789,\"start\":37788},{\"end\":37802,\"start\":37797},{\"end\":37806,\"start\":37805},{\"end\":38313,\"start\":38312},{\"end\":38317,\"start\":38314},{\"end\":38329,\"start\":38328},{\"end\":38682,\"start\":38681},{\"end\":38696,\"start\":38695},{\"end\":38707,\"start\":38706},{\"end\":38959,\"start\":38958},{\"end\":38967,\"start\":38966},{\"end\":38974,\"start\":38973},{\"end\":38982,\"start\":38981},{\"end\":39204,\"start\":39203},{\"end\":39206,\"start\":39205},{\"end\":39218,\"start\":39217},{\"end\":39231,\"start\":39230},{\"end\":39247,\"start\":39246},{\"end\":39260,\"start\":39259},{\"end\":39270,\"start\":39269},{\"end\":39286,\"start\":39285},{\"end\":39288,\"start\":39287},{\"end\":39544,\"start\":39543},{\"end\":39560,\"start\":39559},{\"end\":39570,\"start\":39569},{\"end\":39791,\"start\":39790},{\"end\":39801,\"start\":39800},{\"end\":39803,\"start\":39802},{\"end\":39817,\"start\":39816},{\"end\":39819,\"start\":39818},{\"end\":39828,\"start\":39827},{\"end\":39839,\"start\":39838},{\"end\":39841,\"start\":39840},{\"end\":39849,\"start\":39848},{\"end\":39856,\"start\":39855},{\"end\":39866,\"start\":39865},{\"end\":40247,\"start\":40246},{\"end\":40249,\"start\":40248},{\"end\":40261,\"start\":40260},{\"end\":40269,\"start\":40268},{\"end\":40562,\"start\":40561},{\"end\":40564,\"start\":40563},{\"end\":40829,\"start\":40828},{\"end\":40838,\"start\":40837},{\"end\":40847,\"start\":40846},{\"end\":41108,\"start\":41107},{\"end\":41118,\"start\":41117},{\"end\":41291,\"start\":41290},{\"end\":41293,\"start\":41292},{\"end\":41301,\"start\":41300},{\"end\":41511,\"start\":41510},{\"end\":41513,\"start\":41512},{\"end\":41521,\"start\":41520},{\"end\":41691,\"start\":41690},{\"end\":41693,\"start\":41692},{\"end\":41702,\"start\":41701},{\"end\":41704,\"start\":41703},{\"end\":41955,\"start\":41954},{\"end\":41964,\"start\":41963},{\"end\":42177,\"start\":42176},{\"end\":42183,\"start\":42182},{\"end\":42193,\"start\":42192},{\"end\":42209,\"start\":42208},{\"end\":42332,\"start\":42331},{\"end\":42339,\"start\":42338},{\"end\":42346,\"start\":42345},{\"end\":42354,\"start\":42353},{\"end\":42361,\"start\":42360},{\"end\":42368,\"start\":42367},{\"end\":42719,\"start\":42718},{\"end\":42730,\"start\":42729},{\"end\":42743,\"start\":42742},{\"end\":42751,\"start\":42750},{\"end\":42753,\"start\":42752},{\"end\":42764,\"start\":42763},{\"end\":42772,\"start\":42771},{\"end\":42787,\"start\":42786},{\"end\":42795,\"start\":42794},{\"end\":42807,\"start\":42806},{\"end\":42817,\"start\":42816},{\"end\":42828,\"start\":42827},{\"end\":43495,\"start\":43494},{\"end\":43505,\"start\":43504},{\"end\":43517,\"start\":43513},{\"end\":43732,\"start\":43731},{\"end\":43744,\"start\":43743},{\"end\":44181,\"start\":44180},{\"end\":44189,\"start\":44188},{\"end\":44197,\"start\":44196},{\"end\":44206,\"start\":44205},{\"end\":44219,\"start\":44218},{\"end\":44704,\"start\":44703},{\"end\":44716,\"start\":44715},{\"end\":44720,\"start\":44717},{\"end\":44732,\"start\":44731},{\"end\":45045,\"start\":45044},{\"end\":45058,\"start\":45057},{\"end\":45066,\"start\":45065},{\"end\":45068,\"start\":45067},{\"end\":45076,\"start\":45075},{\"end\":45092,\"start\":45091},{\"end\":45360,\"start\":45359},{\"end\":45370,\"start\":45369},{\"end\":45379,\"start\":45378},{\"end\":45392,\"start\":45391},{\"end\":45408,\"start\":45407},{\"end\":45422,\"start\":45421},{\"end\":45426,\"start\":45423},{\"end\":45437,\"start\":45436},{\"end\":45449,\"start\":45448},{\"end\":45451,\"start\":45450},{\"end\":46038,\"start\":46037},{\"end\":46048,\"start\":46047},{\"end\":46056,\"start\":46055},{\"end\":46058,\"start\":46057},{\"end\":46069,\"start\":46068},{\"end\":46286,\"start\":46285},{\"end\":46302,\"start\":46298},{\"end\":46306,\"start\":46305},{\"end\":46779,\"start\":46778},{\"end\":46792,\"start\":46791},{\"end\":46799,\"start\":46798},{\"end\":46806,\"start\":46805},{\"end\":46814,\"start\":46813},{\"end\":47034,\"start\":47033},{\"end\":47047,\"start\":47046},{\"end\":47056,\"start\":47055},{\"end\":47066,\"start\":47065},{\"end\":47078,\"start\":47077},{\"end\":47298,\"start\":47297},{\"end\":47309,\"start\":47308},{\"end\":47323,\"start\":47322},{\"end\":47336,\"start\":47332},{\"end\":47581,\"start\":47580},{\"end\":47590,\"start\":47589},{\"end\":47759,\"start\":47758},{\"end\":47768,\"start\":47767},{\"end\":48187,\"start\":48186},{\"end\":48195,\"start\":48194},{\"end\":48202,\"start\":48201},{\"end\":48208,\"start\":48207},{\"end\":48215,\"start\":48214},{\"end\":48223,\"start\":48222},{\"end\":48230,\"start\":48229}]", "bib_author_last_name": "[{\"end\":36527,\"start\":36524},{\"end\":36535,\"start\":36531},{\"end\":36543,\"start\":36539},{\"end\":36551,\"start\":36547},{\"end\":36742,\"start\":36736},{\"end\":36753,\"start\":36746},{\"end\":36769,\"start\":36757},{\"end\":36779,\"start\":36773},{\"end\":36792,\"start\":36783},{\"end\":37016,\"start\":37010},{\"end\":37027,\"start\":37020},{\"end\":37037,\"start\":37031},{\"end\":37047,\"start\":37041},{\"end\":37300,\"start\":37295},{\"end\":37311,\"start\":37304},{\"end\":37320,\"start\":37315},{\"end\":37329,\"start\":37324},{\"end\":37543,\"start\":37538},{\"end\":37554,\"start\":37547},{\"end\":37562,\"start\":37558},{\"end\":37570,\"start\":37566},{\"end\":37786,\"start\":37780},{\"end\":37795,\"start\":37790},{\"end\":38326,\"start\":38318},{\"end\":38338,\"start\":38330},{\"end\":38693,\"start\":38683},{\"end\":38704,\"start\":38697},{\"end\":38721,\"start\":38708},{\"end\":38964,\"start\":38960},{\"end\":38971,\"start\":38968},{\"end\":38979,\"start\":38975},{\"end\":38985,\"start\":38983},{\"end\":39215,\"start\":39207},{\"end\":39228,\"start\":39219},{\"end\":39244,\"start\":39232},{\"end\":39257,\"start\":39248},{\"end\":39267,\"start\":39261},{\"end\":39283,\"start\":39271},{\"end\":39294,\"start\":39289},{\"end\":39557,\"start\":39545},{\"end\":39567,\"start\":39561},{\"end\":39578,\"start\":39571},{\"end\":39798,\"start\":39792},{\"end\":39814,\"start\":39804},{\"end\":39825,\"start\":39820},{\"end\":39836,\"start\":39829},{\"end\":39846,\"start\":39842},{\"end\":39853,\"start\":39850},{\"end\":39863,\"start\":39857},{\"end\":39872,\"start\":39867},{\"end\":40258,\"start\":40250},{\"end\":40266,\"start\":40262},{\"end\":40278,\"start\":40270},{\"end\":40574,\"start\":40565},{\"end\":40835,\"start\":40830},{\"end\":40844,\"start\":40839},{\"end\":40858,\"start\":40848},{\"end\":41115,\"start\":41109},{\"end\":41121,\"start\":41119},{\"end\":41298,\"start\":41294},{\"end\":41309,\"start\":41302},{\"end\":41518,\"start\":41514},{\"end\":41529,\"start\":41522},{\"end\":41699,\"start\":41694},{\"end\":41710,\"start\":41705},{\"end\":41961,\"start\":41956},{\"end\":41969,\"start\":41965},{\"end\":42180,\"start\":42178},{\"end\":42190,\"start\":42184},{\"end\":42206,\"start\":42194},{\"end\":42215,\"start\":42210},{\"end\":42336,\"start\":42333},{\"end\":42343,\"start\":42340},{\"end\":42351,\"start\":42347},{\"end\":42358,\"start\":42355},{\"end\":42365,\"start\":42362},{\"end\":42372,\"start\":42369},{\"end\":42727,\"start\":42720},{\"end\":42740,\"start\":42731},{\"end\":42748,\"start\":42744},{\"end\":42761,\"start\":42754},{\"end\":42769,\"start\":42765},{\"end\":42784,\"start\":42773},{\"end\":42792,\"start\":42788},{\"end\":42804,\"start\":42796},{\"end\":42814,\"start\":42808},{\"end\":42825,\"start\":42818},{\"end\":42835,\"start\":42829},{\"end\":43502,\"start\":43496},{\"end\":43511,\"start\":43506},{\"end\":43525,\"start\":43518},{\"end\":43741,\"start\":43733},{\"end\":43753,\"start\":43745},{\"end\":44186,\"start\":44182},{\"end\":44194,\"start\":44190},{\"end\":44203,\"start\":44198},{\"end\":44216,\"start\":44207},{\"end\":44228,\"start\":44220},{\"end\":44713,\"start\":44705},{\"end\":44729,\"start\":44721},{\"end\":44741,\"start\":44733},{\"end\":45055,\"start\":45046},{\"end\":45063,\"start\":45059},{\"end\":45073,\"start\":45069},{\"end\":45089,\"start\":45077},{\"end\":45103,\"start\":45093},{\"end\":45367,\"start\":45361},{\"end\":45376,\"start\":45371},{\"end\":45389,\"start\":45380},{\"end\":45405,\"start\":45393},{\"end\":45419,\"start\":45409},{\"end\":45434,\"start\":45427},{\"end\":45446,\"start\":45438},{\"end\":45461,\"start\":45452},{\"end\":46045,\"start\":46039},{\"end\":46053,\"start\":46049},{\"end\":46066,\"start\":46059},{\"end\":46072,\"start\":46070},{\"end\":46296,\"start\":46287},{\"end\":46789,\"start\":46780},{\"end\":46796,\"start\":46793},{\"end\":46803,\"start\":46800},{\"end\":46811,\"start\":46807},{\"end\":46820,\"start\":46815},{\"end\":47044,\"start\":47035},{\"end\":47053,\"start\":47048},{\"end\":47063,\"start\":47057},{\"end\":47075,\"start\":47067},{\"end\":47087,\"start\":47079},{\"end\":47306,\"start\":47299},{\"end\":47320,\"start\":47310},{\"end\":47330,\"start\":47324},{\"end\":47345,\"start\":47337},{\"end\":47587,\"start\":47582},{\"end\":47597,\"start\":47591},{\"end\":47765,\"start\":47760},{\"end\":47775,\"start\":47769},{\"end\":48192,\"start\":48188},{\"end\":48199,\"start\":48196},{\"end\":48205,\"start\":48203},{\"end\":48212,\"start\":48209},{\"end\":48220,\"start\":48216},{\"end\":48227,\"start\":48224},{\"end\":48240,\"start\":48231}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":1756650},\"end\":36673,\"start\":36464},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":14941970},\"end\":36946,\"start\":36675},{\"attributes\":{\"doi\":\"arXiv:1506.02075\",\"id\":\"b2\"},\"end\":37231,\"start\":36948},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":17682909},\"end\":37460,\"start\":37233},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":1077128},\"end\":37717,\"start\":37462},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":12620927},\"end\":38238,\"start\":37719},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":1693548},\"end\":38599,\"start\":38240},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":3016223},\"end\":38874,\"start\":38601},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":12926055},\"end\":39131,\"start\":38876},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":1690180},\"end\":39498,\"start\":39133},{\"attributes\":{\"id\":\"b10\"},\"end\":39742,\"start\":39500},{\"attributes\":{\"doi\":\"arXiv:1704.01212\",\"id\":\"b11\",\"matched_paper_id\":9665943},\"end\":40244,\"start\":39744},{\"attributes\":{\"id\":\"b12\"},\"end\":40496,\"start\":40246},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":124183279},\"end\":40746,\"start\":40498},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":6972449},\"end\":41061,\"start\":40748},{\"attributes\":{\"id\":\"b15\"},\"end\":41256,\"start\":41063},{\"attributes\":{\"id\":\"b16\"},\"end\":41442,\"start\":41258},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":3144218},\"end\":41648,\"start\":41444},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":16074195},\"end\":41831,\"start\":41650},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":2140827},\"end\":42136,\"start\":41833},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":8393918},\"end\":42327,\"start\":42138},{\"attributes\":{\"id\":\"b21\"},\"end\":42431,\"start\":42329},{\"attributes\":{\"id\":\"b22\"},\"end\":42639,\"start\":42433},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":16447573},\"end\":43425,\"start\":42641},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":1157792},\"end\":43658,\"start\":43427},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":1604537},\"end\":44131,\"start\":43660},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":263995},\"end\":44598,\"start\":44133},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":28930965},\"end\":45010,\"start\":44600},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":206756462},\"end\":45308,\"start\":45012},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":7522972},\"end\":45966,\"start\":45310},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":8429835},\"end\":46212,\"start\":45968},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":5378837},\"end\":46692,\"start\":46214},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":13621654},\"end\":46984,\"start\":46694},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":15150247},\"end\":47225,\"start\":46986},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":207168299},\"end\":47499,\"start\":47227},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":14184095},\"end\":47725,\"start\":47501},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":7311615},\"end\":48104,\"start\":47727},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":2768038},\"end\":48642,\"start\":48106}]", "bib_title": "[{\"end\":36520,\"start\":36464},{\"end\":36732,\"start\":36675},{\"end\":37291,\"start\":37233},{\"end\":37531,\"start\":37462},{\"end\":37776,\"start\":37719},{\"end\":38310,\"start\":38240},{\"end\":38679,\"start\":38601},{\"end\":38956,\"start\":38876},{\"end\":39201,\"start\":39133},{\"end\":39788,\"start\":39744},{\"end\":40559,\"start\":40498},{\"end\":40826,\"start\":40748},{\"end\":41508,\"start\":41444},{\"end\":41688,\"start\":41650},{\"end\":41952,\"start\":41833},{\"end\":42174,\"start\":42138},{\"end\":42716,\"start\":42641},{\"end\":43492,\"start\":43427},{\"end\":43729,\"start\":43660},{\"end\":44178,\"start\":44133},{\"end\":44701,\"start\":44600},{\"end\":45042,\"start\":45012},{\"end\":45357,\"start\":45310},{\"end\":46035,\"start\":45968},{\"end\":46283,\"start\":46214},{\"end\":46776,\"start\":46694},{\"end\":47031,\"start\":46986},{\"end\":47295,\"start\":47227},{\"end\":47578,\"start\":47501},{\"end\":47756,\"start\":47727},{\"end\":48184,\"start\":48106}]", "bib_author": "[{\"end\":36529,\"start\":36522},{\"end\":36537,\"start\":36529},{\"end\":36545,\"start\":36537},{\"end\":36553,\"start\":36545},{\"end\":36744,\"start\":36734},{\"end\":36755,\"start\":36744},{\"end\":36771,\"start\":36755},{\"end\":36781,\"start\":36771},{\"end\":36794,\"start\":36781},{\"end\":37018,\"start\":37008},{\"end\":37029,\"start\":37018},{\"end\":37039,\"start\":37029},{\"end\":37049,\"start\":37039},{\"end\":37302,\"start\":37293},{\"end\":37313,\"start\":37302},{\"end\":37322,\"start\":37313},{\"end\":37331,\"start\":37322},{\"end\":37545,\"start\":37533},{\"end\":37556,\"start\":37545},{\"end\":37564,\"start\":37556},{\"end\":37572,\"start\":37564},{\"end\":37788,\"start\":37778},{\"end\":37797,\"start\":37788},{\"end\":37805,\"start\":37797},{\"end\":37809,\"start\":37805},{\"end\":38328,\"start\":38312},{\"end\":38340,\"start\":38328},{\"end\":38695,\"start\":38681},{\"end\":38706,\"start\":38695},{\"end\":38723,\"start\":38706},{\"end\":38966,\"start\":38958},{\"end\":38973,\"start\":38966},{\"end\":38981,\"start\":38973},{\"end\":38987,\"start\":38981},{\"end\":39217,\"start\":39203},{\"end\":39230,\"start\":39217},{\"end\":39246,\"start\":39230},{\"end\":39259,\"start\":39246},{\"end\":39269,\"start\":39259},{\"end\":39285,\"start\":39269},{\"end\":39296,\"start\":39285},{\"end\":39559,\"start\":39543},{\"end\":39569,\"start\":39559},{\"end\":39580,\"start\":39569},{\"end\":39800,\"start\":39790},{\"end\":39816,\"start\":39800},{\"end\":39827,\"start\":39816},{\"end\":39838,\"start\":39827},{\"end\":39848,\"start\":39838},{\"end\":39855,\"start\":39848},{\"end\":39865,\"start\":39855},{\"end\":39874,\"start\":39865},{\"end\":40260,\"start\":40246},{\"end\":40268,\"start\":40260},{\"end\":40280,\"start\":40268},{\"end\":40576,\"start\":40561},{\"end\":40837,\"start\":40828},{\"end\":40846,\"start\":40837},{\"end\":40860,\"start\":40846},{\"end\":41117,\"start\":41107},{\"end\":41123,\"start\":41117},{\"end\":41300,\"start\":41290},{\"end\":41311,\"start\":41300},{\"end\":41520,\"start\":41510},{\"end\":41531,\"start\":41520},{\"end\":41701,\"start\":41690},{\"end\":41712,\"start\":41701},{\"end\":41963,\"start\":41954},{\"end\":41971,\"start\":41963},{\"end\":42182,\"start\":42176},{\"end\":42192,\"start\":42182},{\"end\":42208,\"start\":42192},{\"end\":42217,\"start\":42208},{\"end\":42338,\"start\":42331},{\"end\":42345,\"start\":42338},{\"end\":42353,\"start\":42345},{\"end\":42360,\"start\":42353},{\"end\":42367,\"start\":42360},{\"end\":42374,\"start\":42367},{\"end\":42729,\"start\":42718},{\"end\":42742,\"start\":42729},{\"end\":42750,\"start\":42742},{\"end\":42763,\"start\":42750},{\"end\":42771,\"start\":42763},{\"end\":42786,\"start\":42771},{\"end\":42794,\"start\":42786},{\"end\":42806,\"start\":42794},{\"end\":42816,\"start\":42806},{\"end\":42827,\"start\":42816},{\"end\":42837,\"start\":42827},{\"end\":43504,\"start\":43494},{\"end\":43513,\"start\":43504},{\"end\":43527,\"start\":43513},{\"end\":43743,\"start\":43731},{\"end\":43755,\"start\":43743},{\"end\":44188,\"start\":44180},{\"end\":44196,\"start\":44188},{\"end\":44205,\"start\":44196},{\"end\":44218,\"start\":44205},{\"end\":44230,\"start\":44218},{\"end\":44715,\"start\":44703},{\"end\":44731,\"start\":44715},{\"end\":44743,\"start\":44731},{\"end\":45057,\"start\":45044},{\"end\":45065,\"start\":45057},{\"end\":45075,\"start\":45065},{\"end\":45091,\"start\":45075},{\"end\":45105,\"start\":45091},{\"end\":45369,\"start\":45359},{\"end\":45378,\"start\":45369},{\"end\":45391,\"start\":45378},{\"end\":45407,\"start\":45391},{\"end\":45421,\"start\":45407},{\"end\":45436,\"start\":45421},{\"end\":45448,\"start\":45436},{\"end\":45463,\"start\":45448},{\"end\":46047,\"start\":46037},{\"end\":46055,\"start\":46047},{\"end\":46068,\"start\":46055},{\"end\":46074,\"start\":46068},{\"end\":46298,\"start\":46285},{\"end\":46305,\"start\":46298},{\"end\":46309,\"start\":46305},{\"end\":46791,\"start\":46778},{\"end\":46798,\"start\":46791},{\"end\":46805,\"start\":46798},{\"end\":46813,\"start\":46805},{\"end\":46822,\"start\":46813},{\"end\":47046,\"start\":47033},{\"end\":47055,\"start\":47046},{\"end\":47065,\"start\":47055},{\"end\":47077,\"start\":47065},{\"end\":47089,\"start\":47077},{\"end\":47308,\"start\":47297},{\"end\":47322,\"start\":47308},{\"end\":47332,\"start\":47322},{\"end\":47347,\"start\":47332},{\"end\":47589,\"start\":47580},{\"end\":47599,\"start\":47589},{\"end\":47767,\"start\":47758},{\"end\":47777,\"start\":47767},{\"end\":48194,\"start\":48186},{\"end\":48201,\"start\":48194},{\"end\":48207,\"start\":48201},{\"end\":48214,\"start\":48207},{\"end\":48222,\"start\":48214},{\"end\":48229,\"start\":48222},{\"end\":48242,\"start\":48229}]", "bib_venue": "[{\"end\":36556,\"start\":36553},{\"end\":36798,\"start\":36794},{\"end\":37006,\"start\":36948},{\"end\":37335,\"start\":37331},{\"end\":37577,\"start\":37572},{\"end\":37918,\"start\":37809},{\"end\":38405,\"start\":38340},{\"end\":38727,\"start\":38723},{\"end\":38990,\"start\":38987},{\"end\":39300,\"start\":39296},{\"end\":39541,\"start\":39500},{\"end\":39949,\"start\":39906},{\"end\":40345,\"start\":40296},{\"end\":40606,\"start\":40576},{\"end\":40884,\"start\":40860},{\"end\":41105,\"start\":41063},{\"end\":41288,\"start\":41258},{\"end\":41535,\"start\":41531},{\"end\":41723,\"start\":41712},{\"end\":41975,\"start\":41971},{\"end\":42221,\"start\":42217},{\"end\":42503,\"start\":42433},{\"end\":42932,\"start\":42869},{\"end\":43531,\"start\":43527},{\"end\":43844,\"start\":43755},{\"end\":44267,\"start\":44230},{\"end\":44780,\"start\":44743},{\"end\":45141,\"start\":45105},{\"end\":45529,\"start\":45463},{\"end\":46078,\"start\":46074},{\"end\":46401,\"start\":46309},{\"end\":46825,\"start\":46822},{\"end\":47093,\"start\":47089},{\"end\":47351,\"start\":47347},{\"end\":47603,\"start\":47599},{\"end\":47864,\"start\":47777},{\"end\":48260,\"start\":48257},{\"end\":38014,\"start\":37920},{\"end\":40895,\"start\":40886},{\"end\":43920,\"start\":43846},{\"end\":45582,\"start\":45531},{\"end\":46480,\"start\":46403},{\"end\":47938,\"start\":47866}]"}}}, "year": 2023, "month": 12, "day": 17}