{"id": 236477807, "updated": "2022-05-12 06:11:53.461", "metadata": {"title": "EBERT: Efficient BERT Inference with Dynamic Structured Pruning", "authors": "[{\"first\":\"Zejian\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Fanrong\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Gang\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Jian\",\"last\":\"Cheng\",\"middle\":[]}]", "venue": "FINDINGS", "journal": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Pruning has been demonstrated as an effective way of reducing computational complexity for deep networks, especially CNNs for computer vision tasks. In this paper, we investigate the opportunity to accelerate the inference of large-scale pre-trained language model via pruning. We propose EBERT, a dynamic structured pruning algorithm for efficient BERT inference. Unlike previous methods that randomly prune the model weights for static inference, EBERT dynamically determines and prunes the unimportant heads in multi-head self-attention layers and the unimportant structured computations in feed-forward network for each input sample at run-time. Experimental results show that our proposed EBERT outperforms other state-of-the-art methods on different tasks.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": "2021.findings-acl.425", "pubmed": null, "pubmedcentral": null, "dblp": "conf/acl/LiuLLC21", "doi": "10.18653/v1/2021.findings-acl.425"}}, "content": {"source": {"pdf_hash": "5e6405d9ef0b4da7ee4ffc6d12d616ee356e2f5b", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclanthology.org/2021.findings-acl.425.pdf\"]", "oa_url_match": true, "oa_info": {"license": "CCBY", "open_access_url": "https://aclanthology.org/2021.findings-acl.425.pdf", "status": "HYBRID"}}, "grobid": {"id": "4e5791dc35e3a7507a5054d36ba39bfe25c863c0", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/5e6405d9ef0b4da7ee4ffc6d12d616ee356e2f5b.txt", "contents": "\nEBERT: Efficient BERT Inference with Dynamic Structured Pruning\nAugust 1-6, 2021\n\nZejian Liu \nInstitute of Automation\nNational Laboratory of Pattern Recognition\nCAS\n\nSchool of Future Technology\nUniversity of Chinese Academy of Sciences\n\n\nFanrong Li lifanrong2017@ia.ac.cn \nInstitute of Automation\nNational Laboratory of Pattern Recognition\nCAS\n\nGang Li \nInstitute of Automation\nNational Laboratory of Pattern Recognition\nCAS\n\nSchool of Future Technology\nUniversity of Chinese Academy of Sciences\n\n\nJian Cheng jcheng@nlpr.ia.ac.cn \nInstitute of Automation\nNational Laboratory of Pattern Recognition\nCAS\n\nSchool of Future Technology\nUniversity of Chinese Academy of Sciences\n\n\nEBERT: Efficient BERT Inference with Dynamic Structured Pruning\n\nAssociation for Computational Linguistics: ACL-IJCNLP 2021\nAugust 1-6, 20214814\nPruning has been demonstrated as an effective way of reducing computational complexity for deep networks, especially CNNs for computer vision tasks. In this paper, we investigate the opportunity to accelerate the inference of large-scale pre-trained language model via pruning. We propose EBERT, a dynamic structured pruning algorithm for efficient BERT inference. Unlike previous methods that randomly prune the model weights for static inference, EBERT dynamically determines and prunes the unimportant heads in multi-head self-attention layers and the unimportant structured computations in feed-forward network for each input sample at run-time. Experimental results show that our proposed EBERT outperforms other state-of-the-art methods on different tasks.\n\nIntroduction\n\nIn the last few years, transformer-based (Vaswani et al., 2017) large-scale pre-trained language models, such as BERT (Devlin et al., 2019), RoBERTa , and GPT-3 (Brown et al., 2020), have achieved state-of-the-art results on many NLP tasks, including language understanding, question answering, and reading comprehension. Most recently, researchers also successfully applied transformer-based models to computer vision tasks, achieving comparable or superior performance compared to traditional convolutional networks. For example, Carion et al. (2020) propose detection transformer (DETR) for object detection, Dosovitskiy et al. (2021) design a transformerbased model, namely Vision Transformer (ViT), for image classification. However, due to the notable computational complexity and memory footprint, it is difficult for these models to deploy on hardware platforms under moderate computing and resource budget. Therefore, how to reduce model complexity to enable efficient inference for largescale pre-trained language models is a critical issue.\n\nPruning is a commonly used technique for network compression, which has been widely explored to reduce computation and storage requirements of convolutional neural networks for computer vision tasks (Han et al., 2015(Han et al., , 2016Li et al., 2016). However, can transformer-based models benefit from pruning? Michel et al. (2019) observe that a large percentage of attention heads can be removed with negligible performance drop, which indicates that the importance of different heads in same layer is different.  propose a simple, deterministic first-order weight pruning method which can prune lots of parameters with minimal accuracy loss. Although these methods are able to reduce the memory footprint, they cannot achieve real performance gain on general-purpose hardware, such as GPGPU, due to the unstructured sparsity after pruning.\n\nAdaptive inference strategy is also proposed to accelerate the inference of BERT. It is based on two observations: 1) the input samples usually have different levels of difficulty. For a given model, it may over-calculate the simple samples while fail in complex samples ; 2) similar to convolutional neural networks, the lower and higher layers of transformer extract different information, and features provided by the intermediate layers may be enough for some samples . FastBERT  and DeeBERT  are two state-of-the-art adaptive inference models for compressing BERT. Both of them insert extra classification layers between each layer of the network. During inference, each input sample only goes through part of model when the outputs of extra classifiers meet predefined criteria like entropy and uncertainty. Because the number of executed layers is reduced, real speedup can be achieved. However, skipping all the computations of the remaining layers may be harmful to the accuracy.\n\nIn this paper, we propose EBERT, a hardwarefriendly, simple yet effective algorithm that incorporates structured pruning with adaptive inference for efficient BERT inference. Specifically, EBERT inserts predictors for self-attention sub-layer and feed-forward sub-layer in each transformer block, as illustrated in Figure 1. During inference, the predictors dynamically determine which heads of self-attention layers and channels of feed-forward network can be pruned according to current input. Once a head or a channel is pruned, the corresponding computations and memory cost can be completely avoided. Compared with static pruning methods that permanently prune some parameters, it can avoid prune important parameters for current input samples which will cause large performance drop. To the best of our knowledge, it's the first time to apply dynamically structured pruning to BERT. Experimental results on different benchmark demonstrate that the proposed EBERT can achieve better trade-off between computation reduction and accuracy.\n\n\nRelated Work\n\nAdaptive inference. As different input samples usually have different levels of difficulty, using fixed-size model to process all samples may be non-optimal in terms of computational efficiency. Therefore, the main goal of adaptive inference is to adaptively skip part of the computations according to each input sample to reduce complexity. Fast-BERT  adds student classifiers to the output of each transformer block and use self-distillation strategy to improve performance. The model architecture of DeeBERT ) is similar to FastBERT, but it use entropy of output to decide whether to exit at early stages. PABEE  proposes a novel earlyexit criterion that dynamically stops forward computing when the output of internal classifiers keep unchanged for a pre-defined number of steps.\n\nPruning. Pruning is an intuitively simple yet effective technique for model compression, which removes unimportant computations based on certain criterion. Michel et al. (2019) observe that a large percentage of attention heads can be removed with negligible performance loss and propose a greedy pruning algorithm. Compressing BERT (Gordon et al., 2020) explores the effect of unstructured weight pruning with different levels of pruning and different training stages. McCarley et al. (2019) investigate the relationship between structured pruning and task-specific distillation. SNIP  proposes a structured pruning method to penalize an entire residual module in Transformer model toward an identity mapping.\n\nDistillation. Knowledge Distillation (Hinton et al., 2015) is an effective technique to get light models from heavy models without sacrificing too much performance. DistilBERT (Sanh et al., 2019) leverages knowledge distillation at pre-training phase to get a lighter pre-trained model, then directly fine-tunes on downstream tasks. BERT-PKD (Sun et al., 2019) proposes an incremental knowledge extraction process. Apart from learning from the final output of teacher model, student model also patiently learns from intermediate layers. Tiny-BERT (Jiao et al., 2020) performs distillation at both the pre-training and task-specific fine-tuning phase. Data augmentation is also used to improve the accuracy of student model.\n\n\nMethods\n\nIn this section, we will first introduce the architecture of EBERT. As shown in Figure 1, it can be divided into BERT branch and predictor branch. Then we will describe the training and inference in details. (b) Predictor_f in FFN. Figure 2: The details of predictors in MHA and FFN layer. Here we assume that n = 4, h = 4 and d i = 6. Shadow area means that computations can be skipped.\n\n\nOutput\n\n\nBERT Branch\n\nThe architecture of BERT consists of three parts: the embedding layer, multi-layer bidirectional Transformer encoders and the task-specific classification layer. Given an input sentence S = [s 0 , s 1 , ..., s n ] with length n, where s 0 is usually a special classification token [CLS], the embedding layer will transform it to a sequence of vector representations:\nE = Embedding(S), E \u2208 R n\u00d7d(1)\nThe Transformer encoder contains two sublayers: multi-head self-attention (MHA) layer and position-wise fully connected feed-forward network (FFN),\nH i = LN (M HA(Z i\u22121 ) + Z i\u22121 ) Z i = LN (F F N (H i ) + H i )(2)\nwhere i = 1, 2, ..., L and Z 0 = E. LN is the Layer Normalization operation. The final component of BERT is a task-specific classification layer. It accepts the representation to [CLS] token as input to generate final results, as:\nO = Classif ier(Z L [0, :])(3)\n\nPredictor Branch\n\nIn order to prune unimportant heads and channels for individual input sentence, we add predictors for MHA and FFN in each layer, respectively. The predictor consists of two feed-forward layer, one batch normalization layer and a ReLU activation layer, as depicted in Figure 2. The output t of the second feed-forward layer will be transformed to a 0-1 mask by a function f (\u00b7):\nt = F C2(ReLU (BN (F C1(x)))) m = f (t), m \u2208 {0, 1}(4)\nwhere x = Z[0, :]. It means that the input of predictor is only [CLS] representation. This choice is based on two reasons. 1) Overhead. Although using the whole representation of input sentence may improve the performance of predictors, the amount of computations increases linearly with the sentence length n. When n is large, the computational overhead of predictors can not be ignored.\n\n2) Representation ability. Because the final hidden state to [CLS] token in the last transformer block is used in task-specific classifier to generate classification results, we assume that [CLS] repre-sentation encodes most of the useful information of the sentence. Note that the representation to [CLS] token in the first MHA is independent with the input sentence, so we use average pooling of MHA as input. Intuitively, t represents the probability of heads or channels being selected. In order to train the model end-to-end with back propagation, Gumbel-Softmax trick (Jang et al., 2017;Maddison et al., 2016) is adopted in our model. Given class probabilities \u03c0 1 , \u03c0 2 , ...\u03c0 n , discrete samples z can be drawn as:\nz = one hot(arg max i [g i + log\u03c0 i ])(5)\nwhere g i is a sample drawn from a Gumbel distribution. Gumbel-Softmax trick replaces arg max operation with a softmax function, which is a continuous differentiable approximation to arg max:\ny i = exp((log(\u03c0 i ) + g i )/\u03c4 k j=1 exp((log(\u03c0 j ) + g j )/\u03c4 )(6)\nAs the value of mask m is binary (0 for prune and 1 for preserve), we can simplify the Gumbel-Softmax formulation (Verelst and Tuytelaars, 2020). For the output t[i] \u2208 (\u2212\u221e, \u221e), we can convert it to probabilities \u03c0 1 and \u03c0 2 by using a sigmoid function \u03c3:\n\u03c0 1 = \u03c3(t[i]) \u03c0 2 = 1 \u2212 \u03c3(t[i])(7)\nSubstituting (7) into (6), we can get:\ny 1 = \u03c3( t[i] + g 1 \u2212 g 2 \u03c4 ) y 2 = 1 \u2212 y 1(8)\nAs y 1 < y 2 means the head or channel will be pruned, the final formulation is:\nf (t[i]) = 1, if y 1 > 0.5 0, otherwise(9)\n\nTraining\n\nThe entire training process can be divided into three stages: fine-tune the BERT branch, joint train both branches, and re-train the BERT branch.\n\nFine-tuning. In the first stage, only BERT branch is fine-tuned on downstream tasks with loss L task . The training strategy is the same as BERT in (Devlin et al., 2019).\n\nJoint Training. In this stage, we jointly train the pre-trained BERT branch and randomly initialized predictor branch to make the average ratio of remaining Floating-point operations (FLOPs) reach a target value C t \u2208 [0, 1]. In order to achieve this goal, we add a loss to minimize the difference between real computational cost of the whole network and C t :\nL s = F c F o \u2212 C t 2(10)\nWhere F o is the FLOPs of original network, and F c is the average FLOPs of current model in a mini-batch.\n\nIn addition to the FLOPs constraint, we also add extra loss function to control the sparsity of each MHA and FFN, as in (11). The purpose is to avoid high sparsity of some layers that is harmful to the accuracy of the model.\nL M = 1 L L\u22121 l=0 F lM c F lM o \u2212 C t 2 L F = 1 L L\u22121 l=0 F lF c F lF o \u2212 C t 2(11)\nwhere \nL = L task + \u03bb 1 L s + \u03bb 2 (L M + L F )(12)\nwhere \u03bb 1 and \u03bb 2 control the magnitude of task and sparsity loss, respectively.\n\nRe-training. As different input samples usually activate different parts of heads, the total update of a particular head is less than that of regular training process. As a result, the heads are probably not trained sufficiently. So do the channels in FFNs. Therefore, in this stage, we freeze the parameters of predictors and only re-train the BERT branch.\n\n\nInference\n\nThe computation flow during inference is shown in Figure 2. Given an input sequence, the predictor generates a mask by using the representation to [CLS] token. For MHA, heads with mask '0' will not be executed. For FFN, as matrix-matrix multiplication can be transformed to multiple matrixvector multiplications, we only need to complete part of computations where vector's mask is not zero.  Note that the exponential operation in (8) is typically expensive on hardware. Fortunately, this formulation can be simplified during inference by removing Gumbel noise. f (\u00b7) now can be rewritten as:  (Rajpurkar et al., 2016) and SQuAD2.0 (Rajpurkar et al., 2018), both of which are largescale reading comprehension datasets. SQuAD1.1 consists of more than 100k questions, and the answer to each question is a segment of text from the corresponding reading passage. SQuAD2.0 is more difficult as it contains over 50k unanswerable questions. We mainly report Exact Match (EM) and F1 scores. Implementation details. We apply the proposed methods to both BERT-base and RoBERTa-base, and implement them with the HuggingFace Transformers Library . The detailed setting of BERT and predictors is shown in Table 1. Figure 3 shows the ratio of FLOPs and parameters of each operation in one encoder. We can find that the extra cost of the predictors is very small. All experiments are completed on a single Nvidia GeForce RTX2080Ti GPU.  For the GLUE benchmark, we set batch size to 32, learning rate to 3e-5, training epochs to 3 while other hyperparameters are kept unchanged from the library for all downstream tasks at backbone fine-tune stage. During joint training, we use \u03bb 1 = 4, \u03bb 2 = 20 for BERT while \u03bb 1 = 2, \u03bb 2 = 10 for RoBERTa. The learning rate for predictors' parameters is 0.02 and 0.01, respectively. The hyperparameters in the third stage is the same as the first stage.\nf (t[i]) = 1, if t[i] > 0 0, otherwise(13\nFor SQuAD1.1 and SQuAD2.0, the batch size is 12, learning rate is 3e-5 and training epoch is 2. Other settings are consistent with those for BERT on GLUE benchmark.\n\nBaseline. In order to evaluate the effectiveness of EBERT, we implement a Top-k version of BERT that f (\u00b7) is as (14). We keep the sparsity of each layer the same, so the value of k can be decided by C t . What's more, for a certain k, the sparsity is a fixed value, so no extra loss need to be added. The training objective is just L task . The training methods is the same as EBERT with Gumbel-Softmax.\nf (t[i], k) = 1, if t[i] \u2208 topk(t) 0, otherwise.(14)\nFor convenience, in the following sections we will use the subscript t to represent Top-k version and use subscript g for Gumbel-Softmax version.\n\n\nResults on the GLUE benchmark\n\nThe main results of our proposed method on the development set of GLUE benchmark are shown in Figure 4. For BERT-base, the results of Gumbel-Softmax is always better than Top-k with the same or even smaller ratio of remaining FLOPs on four tasks. For example, when remaining 50% FLOPs, EBERT g only drops 0.6% on QQP task, while EBERT t drops 1.8%. On the MNLI task, EBERT g 's accuracy with 77% remaining FLOPs is higher than the accuracy of EBERT t with 81% remaining FLOPs. Figure 4(b) shows the performance of ER-oBERTa, and we can find the similar result, e.g. with 50% remaining FLOPs, the performance of ERoBERTa g is 2.3% higher than ERoBERTa t on the MNLI task. This proves the generality of our proposed method to different model.\n\n\nResults on the SQuAD benchmark\n\nTo further demonstrate the generality of our method, we conduct experiments on the SQuAD v1.1 and v2.0 benchmark, which are reading comprehension task that the model need to predict the answer text span in the text for a given question. The results are shown in Figure 5. Similar to the observation in Figure 4, our approach achieves consistent improvement on each ratio of remaining FLOPs compared with the Top-k version. For instance, with 50% remaining FLOPs, EBERT g improves the EM and F1 score by 2.8% and 2.4% on SQuAD v1.1, respectively. On SQuAD v2.0, the improvement of EM and F1 score is 3.3% and 3.4%.\n\n\nComparison with Other Methods\n\nWe compare our proposed EBERT with other stateof-the-art compression methods. For distillation methods, we compare with DistilBERT (Sanh et al., 2019), BERT-PKD (Sun et al., 2019) and BERT-of-Theseus . For pruning, we compare with SNIP . We also compare with other two dynamic methods: DeeBERT  and PABEE . We do not compare with FastBERT  as they don't report results on the GLUE and SQuAD benchmark.\n\nNote that other works don't report the FLOPs. However, as all of these methods try to reduce computational cost by reducing the number of layers dynamically or statically, it is reasonable to get FLOPs from speedup ratio or compression ratio under the assumption that the FLOPs is proportional to the execution time for a specific layer. For example, as the DistilBERT-6L only has half number of layers of BERT-base, we assume the ratio of remaining FLOPs is 50%.   The training process of EBERT contains three stages: fine-tuning, joint training and re-training. The purpose of re-training is to make each head and channel sufficiently trained. To evaluate the efficacy of this stage, we conduct experiments with RoBERTa on two tasks. Results are shown in Figure 6, we can see that the performance improvement is obvious. With 50% remaining FLOPs, the performance of the model is improved from 84.4% to 85.0% on MNLI and 92.2% to 92.8% on SST-2, respectively. The average performance improvement on MNLI and SST-2 is 0.4% and 0.8%, respectively. Comparing these two results, we find that the improvement is more obvious on small datasets. The reason for this phenomenon is that the parameters of the model are updated more fre-\n\n\nSST-2(94.2)\n\n\nDev-Acc\n\nRemaining FLOPs%\n\n\nMNLI-m(88.0)\n\n\nRemaining FLOPs%\n\nMatched Dev-Acc quently on large datasets, which makes the training of the model more sufficient at the joint training stage. As a result, re-training can be skipped for large datasets to make trade-offs.\n\n\nMask Distribution\n\nLike in , we investigate the distribution of the learned masks. Although EBERT can dynamically generate mask for each head and channel for different samples, some masks may be constant of all time, which means that these masks are input-independent. Figure 8 is the layer-wise visualization of mask distribution in MHA and FFN on SST-2 task for masks that are 1) always one (on), 2) always zero (off), and 3) input-dependent. We can see that a large subset of the masks are inputdependent for both heads and channels, which indicates that our model learns to predict the im-  portance of heads and channels for different input samples. For head, the proportion of masks that are input-dependent is higher in the shallow layers. For channel, the 2nd, 5th, 8th and 11th layer have higher proportion of input-dependent masks than other layers.\n\n\nLayer Distribution\n\nIn Section 3.3, we add two extra loss L M and L F to prevent some layers from being too sparse. We conduct experiments on SST-2 task with RoBERTa to verify the effectiveness of these constraints. Figure 7 shows the average number of non-pruned heads of MHA and non-pruned channels of FFN with different ratio of remaining FLOPs. We can see that the number in each layer is quite close, which indicates the average amount of calculations is similar. More importantly, this value is near the target C t . For example, when remaining 80% FLOPs, the number of non-pruned heads is around 9, which is exactly 80% of the number of heads in one MHA. Similarly, the number of non-pruned heads are around 4 and 5 when remaining 40% FLOPs. This phenomenon proves that L M and L F do limit the sparsity of each layer.\n\n\nConclusion and Future Works\n\nIn this paper, we propose a novel pruning method for efficient BERT inference, which is called EBERT. With the help of predictor branch, EBERT can dynamically prune unimportant heads in MHA and unimportant channels in FFN for each input sample at run-time. Compared with other compression methods, experiments on GLUE and SQuAD benchmarks demonstrate that EBERT can achieve better accuracy-efficiency trade-off.\n\nAs we talk about in Section 4.1, the performance of our method on small dataset has large variance. Similar observations also have been mentioned in other works (e.g. SNIP). In order to improve the generality of our method, it would be interesting to find out the exact reason and find the corresponding solution.\n\nFigure 1 :\n1The overall architecture of EBERT. For each input sentence, the predictor dynamically determines which heads or channels can be pruned.\n\n\nthe FLOPs of l-th MHA in current model and original model. The definition of F lF c and F lF o is similar. The final loss to be optimized is then given by\n\nFigure 3 :\n3The ratio of FLOPs and parameters of each operation in one encoder.\n\nFigure 4 :\n4Results on the development set of GLUE benchmark.\n\nFigure 5 :\n5Results on the development set of SQuAD benchmark.\n\nFigure 6 :\n6The effectiveness of re-training stage for RoBERTa on MNLI and SST-2.\n\nFigure 7 :Figure 8 :\n78Average number of non-pruned heads of MHA and non-pruned channels of FFN by layer for RoBERTabase with different remaining FLOPs on the SST-The distribution of masks in MHA and FFN for RoBERTa-base with 50% remaining FLOPs on SST-2 tasks. DEP refers to input-dependent.\n\nTable 1 :\n1The detailed setting of BERT and Predictors. RoBERTa is with the same setting.\n\n\nRF%. Acc. RF%. F1/Acc. RF%. Acc. RF%.MNLI-m \nSST-2 \nQQP \nQNLI \nAcc. dev set \nBERT-base \n84.7 \n100 \n93.2 \n100 87.9/91.1 100 \n91.5 \n100 \nDistilBERT-6L \n82.2 \n50 \n91.3 \n50 \n-/88.5 \n50 \n89.2 \n50 \nBERT-PKD \n81.3 \n50 \n91.3 \n50 \n-\n-\n-\n-\nSNIP \n-\n-\n91.8 \n50 \n-/88.9 \n50 \n89.5 \n50 \nDeeBERT \n80.7 \n63 \n90.0 \n63 \n-\n-\n-\n-\nPABEE \n83.6 \n62 \n92.0 \n62 \n-\n-\n-\n-\n\nEBERT g \n82.4 \n51 \n91.6 \n50 \n87.2/90.6 \n50 \n89.6 \n51 \n83.1 \n60 \n92.2 \n60 \n87.5/90.8 \n59 \n90.2 \n59 \ntest set \nBERT-base \n84.7 \n100 \n93.7 \n100 71.5/89.4 100 \n90.8 \n100 \nBERT-PKD \n81.5 \n50 \n92.0 \n50 \n70.7/88.9 \n50 \n89.0 \n50 \nBERT-of-Theseus 82.4 \n50 \n92.2 \n50 \n71.6/89.3 \n50 \n89.6 \n50 \nDeeBERT \n80.0 \n63 \n91.5 \n53 \n69.4/-\n51 \n87.3 \n56 \n\nEBERT g \n82.4 \n50 \n92.8 \n50 \n70.1/88.8 \n50 \n89.2 \n50 \n83.3 \n60 \n93.4 \n60 \n70.0/88.8 \n59 \n89.6 \n59 \n\n\n\nTable 2 :\n2Comparison with other compressed methods on the development and test set of MNLI, SST-2, QQP and QNLI. RF means the ratio of remaining FLOPs.\n\nTable 2\n2lists the results on both development set and test set. The results on test set are provided by the GLUE evaluation server. Compared with other methods, our approach retains competitive performance with less FLOPs. For instance, our approach achieves the accuracy of 92.2% on SST-2 with 60% remaining FLOPs. On the test set of MNLI task, the accuracy of our method is 83.3% with only 60% remaining FLOPs, while DeeBERT' accuracy is 80.0% with 63% remaining FLOPs. 4.5 Further Analysis 4.5.1 Impact of Re-training\n\nLanguage models are few-shot learners. Amodei, arXiv:2005.14165arXiv preprintAmodei. 2020. Language models are few-shot learn- ers. arXiv preprint arXiv:2005.14165.\n\nEnd-to-end object detection with transformers. Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko, https:/link.springer.com/chapter/10.1007/978-3-030-58452-8_13ECCV. ChamSpringer International PublishingNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. 2020. End-to-end object detection with transformers. In ECCV, pages 213-229, Cham. Springer International Publishing.\n\nYou look twice: Gaternet for dynamic filter selection in cnns. Zhourong Chen, Yang Li, Samy Bengio, Si Si, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)Zhourong Chen, Yang Li, Samy Bengio, and Si Si. 2019. You look twice: Gaternet for dynamic filter selection in cnns. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog- nition (CVPR).\n\nBERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaLong and Short Papers1Association for Computational LinguisticsJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Associ- ation for Computational Linguistics.\n\nJakob Uszkoreit, and Neil Houlsby. 2021. An image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, International Conference on Learning Representations. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations.\n\nCompressing bert: Studying the effects of weight pruning on transfer learning. Mitchell A Gordon, arXiv:2002.08307arXiv preprintKevin Duh, and Nicholas AndrewsMitchell A. Gordon, Kevin Duh, and Nicholas An- drews. 2020. Compressing bert: Studying the ef- fects of weight pruning on transfer learning. arXiv preprint arXiv:2002.08307.\n\nDeep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. Song Han, Huizi Mao, William J Dally, International Conference on Learning Representations. ICLRSong Han, Huizi Mao, and William J Dally. 2016. Deep compression: Compressing deep neural net- works with pruning, trained quantization and huff- man coding. International Conference on Learning Representations (ICLR).\n\nLearning both weights and connections for efficient neural network. Song Han, Jeff Pool, John Tran, William Dally, Advances in Neural Information Processing Systems. Curran Associates, Inc28Song Han, Jeff Pool, John Tran, and William Dally. 2015. Learning both weights and connections for ef- ficient neural network. In Advances in Neural Infor- mation Processing Systems, volume 28, pages 1135- 1143. Curran Associates, Inc.\n\nGeoffrey Hinton, Oriol Vinyals, Jeff Dean, arXiv:1503.02531Distilling the knowledge in a neural network. arXiv preprintGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531.\n\nCategorical reparameterization with gumbel-softmax. Eric Jang, Shixiang Gu, Ben Poole, Eric Jang, Shixiang Gu, and Ben Poole. 2017. Categor- ical reparameterization with gumbel-softmax.\n\nTinyBERT: Distilling BERT for natural language understanding. Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, Qun Liu, 10.18653/v1/2020.findings-emnlp.372Findings of the Association for Computational Linguistics: EMNLP 2020. Online. Association for Computational LinguisticsXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. 2020. TinyBERT: Distilling BERT for natural lan- guage understanding. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4163-4174, Online. Association for Computational Linguistics.\n\nPruning filters for efficient convnets. Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, Hans Peter Graf, abs/1608.08710CoRRHao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. 2016. Pruning filters for effi- cient convnets. CoRR, abs/1608.08710.\n\nPruning redundant mappings in transformer models via spectral-normalized identity prior. Zi Lin, Jeremiah Liu, Zi Yang, Nan Hua, Dan Roth, 10.18653/v1/2020.findings-emnlp.64Findings of the Association for Computational Linguistics: EMNLP 2020. Association for Computational LinguisticsZi Lin, Jeremiah Liu, Zi Yang, Nan Hua, and Dan Roth. 2020. Pruning redundant mappings in transformer models via spectral-normalized identity prior. In Findings of the Association for Computational Lin- guistics: EMNLP 2020, pages 719-730. Association for Computational Linguistics.\n\nFastBERT: a selfdistilling BERT with adaptive inference time. Weijie Liu, Peng Zhou, Zhiruo Wang, Zhe Zhao, Haotang Deng, Qi Ju, 10.18653/v1/2020.acl-main.537Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsWeijie Liu, Peng Zhou, Zhiruo Wang, Zhe Zhao, Haotang Deng, and Qi Ju. 2020. FastBERT: a self- distilling BERT with adaptive inference time. In Proceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 6035- 6044, Online. Association for Computational Lin- guistics.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.11692Roberta: A robustly optimized BERT pretraining approach. arXiv preprintYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized BERT pretraining ap- proach. arXiv preprint arXiv:1907.11692.\n\nThe concrete distribution: A continuous relaxation of discrete random variables. Chris J Maddison, Andriy Mnih, Yee Whye Teh, Chris J. Maddison, Andriy Mnih, and Yee Whye Teh. 2016. The concrete distribution: A continuous re- laxation of discrete random variables.\n\nStructed pruning a bert-based question answering model. J S Mccarley, Rishav Chakravarti, Avirup Sil, arXiv:1910.06360arXiv preprintJ.S. McCarley, Rishav Chakravarti, and Avirup Sil. 2019. Structed pruning a bert-based question an- swering model. arXiv preprint arXiv:1910.06360.\n\nAre sixteen heads really better than one?. Paul Michel, Omer Levy, Graham Neubig, Advances in Neural Information Processing Systems. Curran Associates, Inc32Paul Michel, Omer Levy, and Graham Neubig. 2019. Are sixteen heads really better than one? In Ad- vances in Neural Information Processing Systems, volume 32, pages 14014-14024. Curran Associates, Inc.\n\nKnow what you don't know: Unanswerable questions for SQuAD. Pranav Rajpurkar, Robin Jia, Percy Liang, 10.18653/v1/P18-2124Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaShort Papers2Association for Computational LinguisticsPranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don't know: Unanswerable ques- tions for SQuAD. In Proceedings of the 56th An- nual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 784- 789, Melbourne, Australia. Association for Compu- tational Linguistics.\n\nSQuAD: 100,000+ questions for machine comprehension of text. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, 10.18653/v1/D16-1264Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingAustin, TexasAssociation for Computational LinguisticsPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natu- ral Language Processing, pages 2383-2392, Austin, Texas. Association for Computational Linguistics.\n\nV Sanh, L Debut, J Chaumond, T Wolf, arXiv:1910.01108Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprintV. Sanh, L. Debut, J. Chaumond, and T. Wolf. 2019. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108.\n\nMovement pruning: Adaptive sparsity by finetuning. Victor Sanh, Thomas Wolf, Alexander M Rush, Victor Sanh, Thomas Wolf, and Alexander M. Rush. 2020. Movement pruning: Adaptive sparsity by fine- tuning.\n\nPatient knowledge distillation for BERT model compression. Siqi Sun, Yu Cheng, Zhe Gan, Jingjing Liu, 10.18653/v1/D19-1441Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsSiqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019. Patient knowledge distillation for BERT model com- pression. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu- ral Language Processing (EMNLP-IJCNLP), pages 4323-4332, Hong Kong, China. Association for Computational Linguistics.\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Kaiser , Illia Polosukhin, Proceedings of the 31st International Conference on Neural Information Processing Systems. the 31st International Conference on Neural Information Processing SystemsRed Hook, NY, USACurran Associates IncAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, undefine- dukasz Kaiser, and Illia Polosukhin. 2017. Atten- tion is all you need. In Proceedings of the 31st Inter- national Conference on Neural Information Process- ing Systems, page 6000-6010, Red Hook, NY, USA. Curran Associates Inc.\n\nDynamic convolutions: Exploiting spatial sparsity for faster inference. T Verelst, T Tuytelaars, 10.1109/CVPR42600.2020.002392020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). T. Verelst and T. Tuytelaars. 2020. Dynamic convolu- tions: Exploiting spatial sparsity for faster inference. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2317-2326.\n\nGLUE: A multi-task benchmark and analysis platform for natural language understanding. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, 10.18653/v1/W18-5446Proceedings of the 2018 EMNLP Workshop Black-boxNLP: Analyzing and Interpreting Neural Networks for NLP. the 2018 EMNLP Workshop Black-boxNLP: Analyzing and Interpreting Neural Networks for NLPBrussels, BelgiumAssociation for Computational LinguisticsAlex Wang, Amanpreet Singh, Julian Michael, Fe- lix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A multi-task benchmark and analysis plat- form for natural language understanding. In Pro- ceedings of the 2018 EMNLP Workshop Black- boxNLP: Analyzing and Interpreting Neural Net- works for NLP, pages 353-355, Brussels, Belgium. Association for Computational Linguistics.\n\nTransformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Le Xu, Sylvain Scao, Mariama Gugger, Quentin Drame, Alexander M Lhoest, Rush, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsOnline. Association for Computational LinguisticsThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier- ric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtow- icz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. Transformers: State-of-the-art natural language pro- cessing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Asso- ciation for Computational Linguistics.\n\nDeeBERT: Dynamic early exiting for accelerating BERT inference. Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, Jimmy Lin, 10.18653/v1/2020.acl-main.204Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsOnlineJi Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and Jimmy Lin. 2020. DeeBERT: Dynamic early exiting for accelerating BERT inference. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2246-2251, On- line. Association for Computational Linguistics.\n\nBERT-of-theseus: Compressing BERT by progressive module replacing. Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, Ming Zhou, 10.18653/v1/2020.emnlp-main.633Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, and Ming Zhou. 2020. BERT-of-theseus: Com- pressing BERT by progressive module replacing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7859-7869, Online. Association for Computa- tional Linguistics.\n\nBert loses patience: Fast and robust inference with early exit. Wangchunshu Zhou, Canwen Xu, Tao Ge, Julian Mcauley, Ke Xu, Furu Wei, Advances in Neural Information Processing Systems. Wangchunshu Zhou, Canwen Xu, Tao Ge, Julian McAuley, Ke Xu, and Furu Wei. 2020. Bert loses patience: Fast and robust inference with early exit. In Advances in Neural Information Processing Sys- tems.\n", "annotations": {"author": "[{\"end\":238,\"start\":83},{\"end\":345,\"start\":239},{\"end\":498,\"start\":346},{\"end\":675,\"start\":499}]", "publisher": null, "author_last_name": "[{\"end\":93,\"start\":90},{\"end\":249,\"start\":247},{\"end\":353,\"start\":351},{\"end\":509,\"start\":504}]", "author_first_name": "[{\"end\":89,\"start\":83},{\"end\":246,\"start\":239},{\"end\":350,\"start\":346},{\"end\":503,\"start\":499}]", "author_affiliation": "[{\"end\":165,\"start\":95},{\"end\":237,\"start\":167},{\"end\":344,\"start\":274},{\"end\":425,\"start\":355},{\"end\":497,\"start\":427},{\"end\":602,\"start\":532},{\"end\":674,\"start\":604}]", "title": "[{\"end\":64,\"start\":1},{\"end\":739,\"start\":676}]", "venue": "[{\"end\":799,\"start\":741}]", "abstract": "[{\"end\":1583,\"start\":821}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b23\"},\"end\":1662,\"start\":1640},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1738,\"start\":1717},{\"end\":1780,\"start\":1754},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2151,\"start\":2131},{\"end\":2236,\"start\":2211},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2868,\"start\":2851},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2887,\"start\":2868},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2903,\"start\":2887},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2985,\"start\":2965},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6507,\"start\":6487},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6685,\"start\":6664},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6823,\"start\":6801},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7101,\"start\":7080},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7238,\"start\":7219},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7403,\"start\":7385},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7609,\"start\":7590},{\"end\":8987,\"start\":8982},{\"end\":9586,\"start\":9581},{\"end\":9973,\"start\":9968},{\"end\":10212,\"start\":10207},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":10500,\"start\":10481},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10522,\"start\":10500},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11076,\"start\":11046},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11759,\"start\":11738},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":13689,\"start\":13665},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":13727,\"start\":13703},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":17363,\"start\":17344},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":17392,\"start\":17374}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":21723,\"start\":21575},{\"attributes\":{\"id\":\"fig_2\"},\"end\":21880,\"start\":21724},{\"attributes\":{\"id\":\"fig_3\"},\"end\":21961,\"start\":21881},{\"attributes\":{\"id\":\"fig_4\"},\"end\":22024,\"start\":21962},{\"attributes\":{\"id\":\"fig_5\"},\"end\":22088,\"start\":22025},{\"attributes\":{\"id\":\"fig_6\"},\"end\":22171,\"start\":22089},{\"attributes\":{\"id\":\"fig_7\"},\"end\":22465,\"start\":22172},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":22556,\"start\":22466},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":23338,\"start\":22557},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":23492,\"start\":23339},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":24015,\"start\":23493}]", "paragraph": "[{\"end\":2650,\"start\":1599},{\"end\":3496,\"start\":2652},{\"end\":4486,\"start\":3498},{\"end\":5529,\"start\":4488},{\"end\":6329,\"start\":5546},{\"end\":7041,\"start\":6331},{\"end\":7766,\"start\":7043},{\"end\":8165,\"start\":7778},{\"end\":8556,\"start\":8190},{\"end\":8735,\"start\":8588},{\"end\":9033,\"start\":8803},{\"end\":9461,\"start\":9084},{\"end\":9905,\"start\":9517},{\"end\":10630,\"start\":9907},{\"end\":10864,\"start\":10673},{\"end\":11186,\"start\":10932},{\"end\":11260,\"start\":11222},{\"end\":11388,\"start\":11308},{\"end\":11588,\"start\":11443},{\"end\":11760,\"start\":11590},{\"end\":12122,\"start\":11762},{\"end\":12255,\"start\":12149},{\"end\":12481,\"start\":12257},{\"end\":12572,\"start\":12566},{\"end\":12697,\"start\":12617},{\"end\":13056,\"start\":12699},{\"end\":14945,\"start\":13070},{\"end\":15152,\"start\":14988},{\"end\":15558,\"start\":15154},{\"end\":15757,\"start\":15612},{\"end\":16531,\"start\":15791},{\"end\":17179,\"start\":16566},{\"end\":17614,\"start\":17213},{\"end\":18844,\"start\":17616},{\"end\":18886,\"start\":18870},{\"end\":19126,\"start\":18922},{\"end\":19988,\"start\":19148},{\"end\":20816,\"start\":20011},{\"end\":21259,\"start\":20848},{\"end\":21574,\"start\":21261}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8587,\"start\":8557},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8802,\"start\":8736},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9064,\"start\":9034},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9516,\"start\":9462},{\"attributes\":{\"id\":\"formula_4\"},\"end\":10672,\"start\":10631},{\"attributes\":{\"id\":\"formula_5\"},\"end\":10931,\"start\":10865},{\"attributes\":{\"id\":\"formula_6\"},\"end\":11221,\"start\":11187},{\"attributes\":{\"id\":\"formula_7\"},\"end\":11307,\"start\":11261},{\"attributes\":{\"id\":\"formula_8\"},\"end\":11431,\"start\":11389},{\"attributes\":{\"id\":\"formula_9\"},\"end\":12148,\"start\":12123},{\"attributes\":{\"id\":\"formula_10\"},\"end\":12565,\"start\":12482},{\"attributes\":{\"id\":\"formula_11\"},\"end\":12616,\"start\":12573},{\"attributes\":{\"id\":\"formula_12\"},\"end\":14987,\"start\":14946},{\"attributes\":{\"id\":\"formula_13\"},\"end\":15611,\"start\":15559}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":14270,\"start\":14263}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1597,\"start\":1585},{\"attributes\":{\"n\":\"2\"},\"end\":5544,\"start\":5532},{\"attributes\":{\"n\":\"3\"},\"end\":7776,\"start\":7769},{\"end\":8174,\"start\":8168},{\"attributes\":{\"n\":\"3.1\"},\"end\":8188,\"start\":8177},{\"attributes\":{\"n\":\"3.2\"},\"end\":9082,\"start\":9066},{\"attributes\":{\"n\":\"3.3\"},\"end\":11441,\"start\":11433},{\"attributes\":{\"n\":\"3.4\"},\"end\":13068,\"start\":13059},{\"attributes\":{\"n\":\"4.2\"},\"end\":15789,\"start\":15760},{\"attributes\":{\"n\":\"4.3\"},\"end\":16564,\"start\":16534},{\"attributes\":{\"n\":\"4.4\"},\"end\":17211,\"start\":17182},{\"end\":18858,\"start\":18847},{\"end\":18868,\"start\":18861},{\"end\":18901,\"start\":18889},{\"end\":18920,\"start\":18904},{\"attributes\":{\"n\":\"4.5.2\"},\"end\":19146,\"start\":19129},{\"attributes\":{\"n\":\"4.5.3\"},\"end\":20009,\"start\":19991},{\"attributes\":{\"n\":\"5\"},\"end\":20846,\"start\":20819},{\"end\":21586,\"start\":21576},{\"end\":21892,\"start\":21882},{\"end\":21973,\"start\":21963},{\"end\":22036,\"start\":22026},{\"end\":22100,\"start\":22090},{\"end\":22193,\"start\":22173},{\"end\":22476,\"start\":22467},{\"end\":23349,\"start\":23340},{\"end\":23501,\"start\":23494}]", "table": "[{\"end\":23338,\"start\":22596}]", "figure_caption": "[{\"end\":21723,\"start\":21588},{\"end\":21880,\"start\":21726},{\"end\":21961,\"start\":21894},{\"end\":22024,\"start\":21975},{\"end\":22088,\"start\":22038},{\"end\":22171,\"start\":22102},{\"end\":22465,\"start\":22196},{\"end\":22556,\"start\":22478},{\"end\":22596,\"start\":22559},{\"end\":23492,\"start\":23351},{\"end\":24015,\"start\":23503}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4811,\"start\":4803},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7866,\"start\":7858},{\"end\":8018,\"start\":8010},{\"end\":9359,\"start\":9351},{\"end\":13128,\"start\":13120},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":14280,\"start\":14272},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":15893,\"start\":15885},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":16836,\"start\":16828},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":16876,\"start\":16868},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":18381,\"start\":18373},{\"end\":19406,\"start\":19398},{\"end\":20215,\"start\":20207}]", "bib_author_first_name": "[{\"end\":24237,\"start\":24230},{\"end\":24255,\"start\":24246},{\"end\":24270,\"start\":24263},{\"end\":24288,\"start\":24281},{\"end\":24307,\"start\":24298},{\"end\":24324,\"start\":24318},{\"end\":24740,\"start\":24732},{\"end\":24751,\"start\":24747},{\"end\":24760,\"start\":24756},{\"end\":24771,\"start\":24769},{\"end\":25239,\"start\":25234},{\"end\":25256,\"start\":25248},{\"end\":25270,\"start\":25264},{\"end\":25284,\"start\":25276},{\"end\":26219,\"start\":26213},{\"end\":26238,\"start\":26233},{\"end\":26255,\"start\":26246},{\"end\":26272,\"start\":26268},{\"end\":26293,\"start\":26286},{\"end\":26306,\"start\":26300},{\"end\":26327,\"start\":26320},{\"end\":26346,\"start\":26338},{\"end\":26362,\"start\":26357},{\"end\":26379,\"start\":26372},{\"end\":26877,\"start\":26869},{\"end\":26879,\"start\":26878},{\"end\":27235,\"start\":27231},{\"end\":27246,\"start\":27241},{\"end\":27259,\"start\":27252},{\"end\":27261,\"start\":27260},{\"end\":27619,\"start\":27615},{\"end\":27629,\"start\":27625},{\"end\":27640,\"start\":27636},{\"end\":27654,\"start\":27647},{\"end\":27982,\"start\":27974},{\"end\":27996,\"start\":27991},{\"end\":28010,\"start\":28006},{\"end\":28282,\"start\":28278},{\"end\":28297,\"start\":28289},{\"end\":28305,\"start\":28302},{\"end\":28481,\"start\":28475},{\"end\":28494,\"start\":28488},{\"end\":28506,\"start\":28500},{\"end\":28517,\"start\":28514},{\"end\":28529,\"start\":28525},{\"end\":28542,\"start\":28536},{\"end\":28551,\"start\":28547},{\"end\":28561,\"start\":28558},{\"end\":29074,\"start\":29071},{\"end\":29083,\"start\":29079},{\"end\":29095,\"start\":29091},{\"end\":29113,\"start\":29108},{\"end\":29125,\"start\":29121},{\"end\":29131,\"start\":29126},{\"end\":29389,\"start\":29387},{\"end\":29403,\"start\":29395},{\"end\":29411,\"start\":29409},{\"end\":29421,\"start\":29418},{\"end\":29430,\"start\":29427},{\"end\":29935,\"start\":29929},{\"end\":29945,\"start\":29941},{\"end\":29958,\"start\":29952},{\"end\":29968,\"start\":29965},{\"end\":29982,\"start\":29975},{\"end\":29991,\"start\":29989},{\"end\":30499,\"start\":30493},{\"end\":30509,\"start\":30505},{\"end\":30520,\"start\":30515},{\"end\":30535,\"start\":30528},{\"end\":30546,\"start\":30540},{\"end\":30559,\"start\":30554},{\"end\":30570,\"start\":30566},{\"end\":30581,\"start\":30577},{\"end\":30593,\"start\":30589},{\"end\":30614,\"start\":30607},{\"end\":31035,\"start\":31030},{\"end\":31037,\"start\":31036},{\"end\":31054,\"start\":31048},{\"end\":31069,\"start\":31061},{\"end\":31272,\"start\":31271},{\"end\":31274,\"start\":31273},{\"end\":31291,\"start\":31285},{\"end\":31311,\"start\":31305},{\"end\":31543,\"start\":31539},{\"end\":31556,\"start\":31552},{\"end\":31569,\"start\":31563},{\"end\":31921,\"start\":31915},{\"end\":31938,\"start\":31933},{\"end\":31949,\"start\":31944},{\"end\":32596,\"start\":32590},{\"end\":32612,\"start\":32608},{\"end\":32630,\"start\":32620},{\"end\":32645,\"start\":32640},{\"end\":33190,\"start\":33189},{\"end\":33198,\"start\":33197},{\"end\":33207,\"start\":33206},{\"end\":33219,\"start\":33218},{\"end\":33556,\"start\":33550},{\"end\":33569,\"start\":33563},{\"end\":33585,\"start\":33576},{\"end\":33587,\"start\":33586},{\"end\":33766,\"start\":33762},{\"end\":33774,\"start\":33772},{\"end\":33785,\"start\":33782},{\"end\":33799,\"start\":33791},{\"end\":34627,\"start\":34621},{\"end\":34641,\"start\":34637},{\"end\":34655,\"start\":34651},{\"end\":34669,\"start\":34664},{\"end\":34686,\"start\":34681},{\"end\":34699,\"start\":34694},{\"end\":34701,\"start\":34700},{\"end\":34715,\"start\":34709},{\"end\":34723,\"start\":34718},{\"end\":35340,\"start\":35339},{\"end\":35351,\"start\":35350},{\"end\":35766,\"start\":35762},{\"end\":35782,\"start\":35773},{\"end\":35796,\"start\":35790},{\"end\":35811,\"start\":35806},{\"end\":35822,\"start\":35818},{\"end\":35835,\"start\":35829},{\"end\":36555,\"start\":36549},{\"end\":36570,\"start\":36562},{\"end\":36584,\"start\":36578},{\"end\":36597,\"start\":36591},{\"end\":36615,\"start\":36608},{\"end\":36633,\"start\":36626},{\"end\":36646,\"start\":36639},{\"end\":36658,\"start\":36655},{\"end\":36670,\"start\":36666},{\"end\":36683,\"start\":36677},{\"end\":36698,\"start\":36695},{\"end\":36711,\"start\":36708},{\"end\":36727,\"start\":36722},{\"end\":36754,\"start\":36748},{\"end\":36765,\"start\":36759},{\"end\":36781,\"start\":36775},{\"end\":36792,\"start\":36787},{\"end\":36795,\"start\":36793},{\"end\":36807,\"start\":36800},{\"end\":36821,\"start\":36814},{\"end\":36837,\"start\":36830},{\"end\":36854,\"start\":36845},{\"end\":36856,\"start\":36855},{\"end\":37771,\"start\":37769},{\"end\":37784,\"start\":37777},{\"end\":37797,\"start\":37791},{\"end\":37811,\"start\":37803},{\"end\":37821,\"start\":37816},{\"end\":38432,\"start\":38426},{\"end\":38448,\"start\":38437},{\"end\":38458,\"start\":38455},{\"end\":38467,\"start\":38463},{\"end\":38477,\"start\":38473},{\"end\":39072,\"start\":39061},{\"end\":39085,\"start\":39079},{\"end\":39093,\"start\":39090},{\"end\":39104,\"start\":39098},{\"end\":39116,\"start\":39114},{\"end\":39125,\"start\":39121}]", "bib_author_last_name": "[{\"end\":24062,\"start\":24056},{\"end\":24244,\"start\":24238},{\"end\":24261,\"start\":24256},{\"end\":24279,\"start\":24271},{\"end\":24296,\"start\":24289},{\"end\":24316,\"start\":24308},{\"end\":24334,\"start\":24325},{\"end\":24745,\"start\":24741},{\"end\":24754,\"start\":24752},{\"end\":24767,\"start\":24761},{\"end\":24774,\"start\":24772},{\"end\":25246,\"start\":25240},{\"end\":25262,\"start\":25257},{\"end\":25274,\"start\":25271},{\"end\":25294,\"start\":25285},{\"end\":26231,\"start\":26220},{\"end\":26244,\"start\":26239},{\"end\":26266,\"start\":26256},{\"end\":26284,\"start\":26273},{\"end\":26298,\"start\":26294},{\"end\":26318,\"start\":26307},{\"end\":26336,\"start\":26328},{\"end\":26355,\"start\":26347},{\"end\":26370,\"start\":26363},{\"end\":26385,\"start\":26380},{\"end\":26886,\"start\":26880},{\"end\":27239,\"start\":27236},{\"end\":27250,\"start\":27247},{\"end\":27267,\"start\":27262},{\"end\":27623,\"start\":27620},{\"end\":27634,\"start\":27630},{\"end\":27645,\"start\":27641},{\"end\":27660,\"start\":27655},{\"end\":27989,\"start\":27983},{\"end\":28004,\"start\":27997},{\"end\":28015,\"start\":28011},{\"end\":28287,\"start\":28283},{\"end\":28300,\"start\":28298},{\"end\":28311,\"start\":28306},{\"end\":28486,\"start\":28482},{\"end\":28498,\"start\":28495},{\"end\":28512,\"start\":28507},{\"end\":28523,\"start\":28518},{\"end\":28534,\"start\":28530},{\"end\":28545,\"start\":28543},{\"end\":28556,\"start\":28552},{\"end\":28565,\"start\":28562},{\"end\":29077,\"start\":29075},{\"end\":29089,\"start\":29084},{\"end\":29106,\"start\":29096},{\"end\":29119,\"start\":29114},{\"end\":29136,\"start\":29132},{\"end\":29393,\"start\":29390},{\"end\":29407,\"start\":29404},{\"end\":29416,\"start\":29412},{\"end\":29425,\"start\":29422},{\"end\":29435,\"start\":29431},{\"end\":29939,\"start\":29936},{\"end\":29950,\"start\":29946},{\"end\":29963,\"start\":29959},{\"end\":29973,\"start\":29969},{\"end\":29987,\"start\":29983},{\"end\":29994,\"start\":29992},{\"end\":30503,\"start\":30500},{\"end\":30513,\"start\":30510},{\"end\":30526,\"start\":30521},{\"end\":30538,\"start\":30536},{\"end\":30552,\"start\":30547},{\"end\":30564,\"start\":30560},{\"end\":30575,\"start\":30571},{\"end\":30587,\"start\":30582},{\"end\":30605,\"start\":30594},{\"end\":30623,\"start\":30615},{\"end\":31046,\"start\":31038},{\"end\":31059,\"start\":31055},{\"end\":31073,\"start\":31070},{\"end\":31283,\"start\":31275},{\"end\":31303,\"start\":31292},{\"end\":31315,\"start\":31312},{\"end\":31550,\"start\":31544},{\"end\":31561,\"start\":31557},{\"end\":31576,\"start\":31570},{\"end\":31931,\"start\":31922},{\"end\":31942,\"start\":31939},{\"end\":31955,\"start\":31950},{\"end\":32606,\"start\":32597},{\"end\":32618,\"start\":32613},{\"end\":32638,\"start\":32631},{\"end\":32651,\"start\":32646},{\"end\":33195,\"start\":33191},{\"end\":33204,\"start\":33199},{\"end\":33216,\"start\":33208},{\"end\":33224,\"start\":33220},{\"end\":33561,\"start\":33557},{\"end\":33574,\"start\":33570},{\"end\":33592,\"start\":33588},{\"end\":33770,\"start\":33767},{\"end\":33780,\"start\":33775},{\"end\":33789,\"start\":33786},{\"end\":33803,\"start\":33800},{\"end\":34635,\"start\":34628},{\"end\":34649,\"start\":34642},{\"end\":34662,\"start\":34656},{\"end\":34679,\"start\":34670},{\"end\":34692,\"start\":34687},{\"end\":34707,\"start\":34702},{\"end\":34734,\"start\":34724},{\"end\":35348,\"start\":35341},{\"end\":35362,\"start\":35352},{\"end\":35771,\"start\":35767},{\"end\":35788,\"start\":35783},{\"end\":35804,\"start\":35797},{\"end\":35816,\"start\":35812},{\"end\":35827,\"start\":35823},{\"end\":35842,\"start\":35836},{\"end\":36560,\"start\":36556},{\"end\":36576,\"start\":36571},{\"end\":36589,\"start\":36585},{\"end\":36606,\"start\":36598},{\"end\":36624,\"start\":36616},{\"end\":36637,\"start\":36634},{\"end\":36653,\"start\":36647},{\"end\":36664,\"start\":36659},{\"end\":36675,\"start\":36671},{\"end\":36693,\"start\":36684},{\"end\":36706,\"start\":36699},{\"end\":36720,\"start\":36712},{\"end\":36746,\"start\":36728},{\"end\":36757,\"start\":36755},{\"end\":36773,\"start\":36766},{\"end\":36785,\"start\":36782},{\"end\":36798,\"start\":36796},{\"end\":36812,\"start\":36808},{\"end\":36828,\"start\":36822},{\"end\":36843,\"start\":36838},{\"end\":36863,\"start\":36857},{\"end\":36869,\"start\":36865},{\"end\":37775,\"start\":37772},{\"end\":37789,\"start\":37785},{\"end\":37801,\"start\":37798},{\"end\":37814,\"start\":37812},{\"end\":37825,\"start\":37822},{\"end\":38435,\"start\":38433},{\"end\":38453,\"start\":38449},{\"end\":38461,\"start\":38459},{\"end\":38471,\"start\":38468},{\"end\":38482,\"start\":38478},{\"end\":39077,\"start\":39073},{\"end\":39088,\"start\":39086},{\"end\":39096,\"start\":39094},{\"end\":39112,\"start\":39105},{\"end\":39119,\"start\":39117},{\"end\":39129,\"start\":39126}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:2005.14165\",\"id\":\"b0\"},\"end\":24181,\"start\":24017},{\"attributes\":{\"doi\":\"https:/link.springer.com/chapter/10.1007/978-3-030-58452-8_13\",\"id\":\"b1\",\"matched_paper_id\":218889832},\"end\":24667,\"start\":24183},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":90261753},\"end\":25150,\"start\":24669},{\"attributes\":{\"doi\":\"10.18653/v1/N19-1423\",\"id\":\"b3\",\"matched_paper_id\":52967399},\"end\":26094,\"start\":25152},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":225039882},\"end\":26788,\"start\":26096},{\"attributes\":{\"doi\":\"arXiv:2002.08307\",\"id\":\"b5\"},\"end\":27123,\"start\":26790},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":2134321},\"end\":27545,\"start\":27125},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":2238772},\"end\":27972,\"start\":27547},{\"attributes\":{\"doi\":\"arXiv:1503.02531\",\"id\":\"b8\"},\"end\":28224,\"start\":27974},{\"attributes\":{\"id\":\"b9\"},\"end\":28411,\"start\":28226},{\"attributes\":{\"doi\":\"10.18653/v1/2020.findings-emnlp.372\",\"id\":\"b10\",\"matched_paper_id\":202719327},\"end\":29029,\"start\":28413},{\"attributes\":{\"doi\":\"abs/1608.08710\",\"id\":\"b11\"},\"end\":29296,\"start\":29031},{\"attributes\":{\"doi\":\"10.18653/v1/2020.findings-emnlp.64\",\"id\":\"b12\",\"matched_paper_id\":222134166},\"end\":29865,\"start\":29298},{\"attributes\":{\"doi\":\"10.18653/v1/2020.acl-main.537\",\"id\":\"b13\",\"matched_paper_id\":214802887},\"end\":30491,\"start\":29867},{\"attributes\":{\"doi\":\"arXiv:1907.11692\",\"id\":\"b14\"},\"end\":30947,\"start\":30493},{\"attributes\":{\"id\":\"b15\"},\"end\":31213,\"start\":30949},{\"attributes\":{\"doi\":\"arXiv:1910.06360\",\"id\":\"b16\"},\"end\":31494,\"start\":31215},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":166227946},\"end\":31853,\"start\":31496},{\"attributes\":{\"doi\":\"10.18653/v1/P18-2124\",\"id\":\"b18\",\"matched_paper_id\":47018994},\"end\":32527,\"start\":31855},{\"attributes\":{\"doi\":\"10.18653/v1/D16-1264\",\"id\":\"b19\",\"matched_paper_id\":11816014},\"end\":33187,\"start\":32529},{\"attributes\":{\"doi\":\"arXiv:1910.01108\",\"id\":\"b20\"},\"end\":33497,\"start\":33189},{\"attributes\":{\"id\":\"b21\"},\"end\":33701,\"start\":33499},{\"attributes\":{\"doi\":\"10.18653/v1/D19-1441\",\"id\":\"b22\",\"matched_paper_id\":201670719},\"end\":34592,\"start\":33703},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":13756489},\"end\":35265,\"start\":34594},{\"attributes\":{\"doi\":\"10.1109/CVPR42600.2020.00239\",\"id\":\"b24\",\"matched_paper_id\":208857382},\"end\":35673,\"start\":35267},{\"attributes\":{\"doi\":\"10.18653/v1/W18-5446\",\"id\":\"b25\",\"matched_paper_id\":5034059},\"end\":36487,\"start\":35675},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":208117506},\"end\":37703,\"start\":36489},{\"attributes\":{\"doi\":\"10.18653/v1/2020.acl-main.204\",\"id\":\"b27\",\"matched_paper_id\":216552850},\"end\":38357,\"start\":37705},{\"attributes\":{\"doi\":\"10.18653/v1/2020.emnlp-main.633\",\"id\":\"b28\",\"matched_paper_id\":211066200},\"end\":38995,\"start\":38359},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":219531455},\"end\":39381,\"start\":38997}]", "bib_title": "[{\"end\":24228,\"start\":24183},{\"end\":24730,\"start\":24669},{\"end\":25232,\"start\":25152},{\"end\":26211,\"start\":26096},{\"end\":27229,\"start\":27125},{\"end\":27613,\"start\":27547},{\"end\":28473,\"start\":28413},{\"end\":29385,\"start\":29298},{\"end\":29927,\"start\":29867},{\"end\":31537,\"start\":31496},{\"end\":31913,\"start\":31855},{\"end\":32588,\"start\":32529},{\"end\":33760,\"start\":33703},{\"end\":34619,\"start\":34594},{\"end\":35337,\"start\":35267},{\"end\":35760,\"start\":35675},{\"end\":36547,\"start\":36489},{\"end\":37767,\"start\":37705},{\"end\":38424,\"start\":38359},{\"end\":39059,\"start\":38997}]", "bib_author": "[{\"end\":24064,\"start\":24056},{\"end\":24246,\"start\":24230},{\"end\":24263,\"start\":24246},{\"end\":24281,\"start\":24263},{\"end\":24298,\"start\":24281},{\"end\":24318,\"start\":24298},{\"end\":24336,\"start\":24318},{\"end\":24747,\"start\":24732},{\"end\":24756,\"start\":24747},{\"end\":24769,\"start\":24756},{\"end\":24776,\"start\":24769},{\"end\":25248,\"start\":25234},{\"end\":25264,\"start\":25248},{\"end\":25276,\"start\":25264},{\"end\":25296,\"start\":25276},{\"end\":26233,\"start\":26213},{\"end\":26246,\"start\":26233},{\"end\":26268,\"start\":26246},{\"end\":26286,\"start\":26268},{\"end\":26300,\"start\":26286},{\"end\":26320,\"start\":26300},{\"end\":26338,\"start\":26320},{\"end\":26357,\"start\":26338},{\"end\":26372,\"start\":26357},{\"end\":26387,\"start\":26372},{\"end\":26888,\"start\":26869},{\"end\":27241,\"start\":27231},{\"end\":27252,\"start\":27241},{\"end\":27269,\"start\":27252},{\"end\":27625,\"start\":27615},{\"end\":27636,\"start\":27625},{\"end\":27647,\"start\":27636},{\"end\":27662,\"start\":27647},{\"end\":27991,\"start\":27974},{\"end\":28006,\"start\":27991},{\"end\":28017,\"start\":28006},{\"end\":28289,\"start\":28278},{\"end\":28302,\"start\":28289},{\"end\":28313,\"start\":28302},{\"end\":28488,\"start\":28475},{\"end\":28500,\"start\":28488},{\"end\":28514,\"start\":28500},{\"end\":28525,\"start\":28514},{\"end\":28536,\"start\":28525},{\"end\":28547,\"start\":28536},{\"end\":28558,\"start\":28547},{\"end\":28567,\"start\":28558},{\"end\":29079,\"start\":29071},{\"end\":29091,\"start\":29079},{\"end\":29108,\"start\":29091},{\"end\":29121,\"start\":29108},{\"end\":29138,\"start\":29121},{\"end\":29395,\"start\":29387},{\"end\":29409,\"start\":29395},{\"end\":29418,\"start\":29409},{\"end\":29427,\"start\":29418},{\"end\":29437,\"start\":29427},{\"end\":29941,\"start\":29929},{\"end\":29952,\"start\":29941},{\"end\":29965,\"start\":29952},{\"end\":29975,\"start\":29965},{\"end\":29989,\"start\":29975},{\"end\":29996,\"start\":29989},{\"end\":30505,\"start\":30493},{\"end\":30515,\"start\":30505},{\"end\":30528,\"start\":30515},{\"end\":30540,\"start\":30528},{\"end\":30554,\"start\":30540},{\"end\":30566,\"start\":30554},{\"end\":30577,\"start\":30566},{\"end\":30589,\"start\":30577},{\"end\":30607,\"start\":30589},{\"end\":30625,\"start\":30607},{\"end\":31048,\"start\":31030},{\"end\":31061,\"start\":31048},{\"end\":31075,\"start\":31061},{\"end\":31285,\"start\":31271},{\"end\":31305,\"start\":31285},{\"end\":31317,\"start\":31305},{\"end\":31552,\"start\":31539},{\"end\":31563,\"start\":31552},{\"end\":31578,\"start\":31563},{\"end\":31933,\"start\":31915},{\"end\":31944,\"start\":31933},{\"end\":31957,\"start\":31944},{\"end\":32608,\"start\":32590},{\"end\":32620,\"start\":32608},{\"end\":32640,\"start\":32620},{\"end\":32653,\"start\":32640},{\"end\":33197,\"start\":33189},{\"end\":33206,\"start\":33197},{\"end\":33218,\"start\":33206},{\"end\":33226,\"start\":33218},{\"end\":33563,\"start\":33550},{\"end\":33576,\"start\":33563},{\"end\":33594,\"start\":33576},{\"end\":33772,\"start\":33762},{\"end\":33782,\"start\":33772},{\"end\":33791,\"start\":33782},{\"end\":33805,\"start\":33791},{\"end\":34637,\"start\":34621},{\"end\":34651,\"start\":34637},{\"end\":34664,\"start\":34651},{\"end\":34681,\"start\":34664},{\"end\":34694,\"start\":34681},{\"end\":34709,\"start\":34694},{\"end\":34718,\"start\":34709},{\"end\":34736,\"start\":34718},{\"end\":35350,\"start\":35339},{\"end\":35364,\"start\":35350},{\"end\":35773,\"start\":35762},{\"end\":35790,\"start\":35773},{\"end\":35806,\"start\":35790},{\"end\":35818,\"start\":35806},{\"end\":35829,\"start\":35818},{\"end\":35844,\"start\":35829},{\"end\":36562,\"start\":36549},{\"end\":36578,\"start\":36562},{\"end\":36591,\"start\":36578},{\"end\":36608,\"start\":36591},{\"end\":36626,\"start\":36608},{\"end\":36639,\"start\":36626},{\"end\":36655,\"start\":36639},{\"end\":36666,\"start\":36655},{\"end\":36677,\"start\":36666},{\"end\":36695,\"start\":36677},{\"end\":36708,\"start\":36695},{\"end\":36722,\"start\":36708},{\"end\":36748,\"start\":36722},{\"end\":36759,\"start\":36748},{\"end\":36775,\"start\":36759},{\"end\":36787,\"start\":36775},{\"end\":36800,\"start\":36787},{\"end\":36814,\"start\":36800},{\"end\":36830,\"start\":36814},{\"end\":36845,\"start\":36830},{\"end\":36865,\"start\":36845},{\"end\":36871,\"start\":36865},{\"end\":37777,\"start\":37769},{\"end\":37791,\"start\":37777},{\"end\":37803,\"start\":37791},{\"end\":37816,\"start\":37803},{\"end\":37827,\"start\":37816},{\"end\":38437,\"start\":38426},{\"end\":38455,\"start\":38437},{\"end\":38463,\"start\":38455},{\"end\":38473,\"start\":38463},{\"end\":38484,\"start\":38473},{\"end\":39079,\"start\":39061},{\"end\":39090,\"start\":39079},{\"end\":39098,\"start\":39090},{\"end\":39114,\"start\":39098},{\"end\":39121,\"start\":39114},{\"end\":39131,\"start\":39121}]", "bib_venue": "[{\"end\":24054,\"start\":24017},{\"end\":24401,\"start\":24397},{\"end\":24864,\"start\":24776},{\"end\":25458,\"start\":25316},{\"end\":26439,\"start\":26387},{\"end\":26867,\"start\":26790},{\"end\":27321,\"start\":27269},{\"end\":27711,\"start\":27662},{\"end\":28077,\"start\":28033},{\"end\":28276,\"start\":28226},{\"end\":28671,\"start\":28602},{\"end\":29069,\"start\":29031},{\"end\":29540,\"start\":29471},{\"end\":30112,\"start\":30025},{\"end\":30696,\"start\":30641},{\"end\":31028,\"start\":30949},{\"end\":31269,\"start\":31215},{\"end\":31627,\"start\":31578},{\"end\":32064,\"start\":31977},{\"end\":32759,\"start\":32673},{\"end\":33319,\"start\":33242},{\"end\":33548,\"start\":33499},{\"end\":34000,\"start\":33825},{\"end\":34825,\"start\":34736},{\"end\":35466,\"start\":35392},{\"end\":35967,\"start\":35864},{\"end\":36980,\"start\":36871},{\"end\":37943,\"start\":37856},{\"end\":38609,\"start\":38515},{\"end\":39180,\"start\":39131},{\"end\":24407,\"start\":24403},{\"end\":24939,\"start\":24866},{\"end\":25609,\"start\":25460},{\"end\":30186,\"start\":30114},{\"end\":32158,\"start\":32066},{\"end\":32845,\"start\":32761},{\"end\":34178,\"start\":34002},{\"end\":34918,\"start\":34827},{\"end\":36074,\"start\":35969},{\"end\":37076,\"start\":36982},{\"end\":38017,\"start\":37945},{\"end\":38690,\"start\":38611}]"}}}, "year": 2023, "month": 12, "day": 17}