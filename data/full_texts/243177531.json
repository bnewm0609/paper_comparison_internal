{"id": 243177531, "updated": "2022-09-30 03:05:53.567", "metadata": {"title": "Optimizing Federated Learning in Distributed Industrial IoT: A Multi-Agent Approach", "authors": "[{\"first\":\"Weiting\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Dong\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Wen\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Haixia\",\"last\":\"Peng\",\"middle\":[]},{\"first\":\"Ning\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Hongke\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Xuemin\",\"last\":\"Shen\",\"middle\":[]}]", "venue": "IEEE Journal on Selected Areas in Communications", "journal": "IEEE Journal on Selected Areas in Communications", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "In this paper, we aim to make the best joint decision of device selection and computing and spectrum resource allocation for optimizing federated learning (FL) performance in distributed industrial Internet of Things (IIoT) networks. To implement efficient FL over geographically dispersed data, we introduce a three-layer collaborative FL architecture to support deep neural network (DNN) training. Specifically, using the data dispersed in IIoT devices, the industrial gateways locally train the DNN model and the local models can be aggregated by their associated edge servers every FL epoch or by a cloud server every a few FL epochs for obtaining the global model. To optimally select participating devices and allocate computing and spectrum resources for training and transmitting the model parameters, we formulate a stochastic optimization problem with the objective of minimizing FL evaluating loss while satisfying delay and long-term energy consumption requirements. Since the objective function of the FL evaluating loss is implicit and the energy consumption is temporally correlated, it is difficult to solve the problem via traditional optimization methods. Thus, we propose a \u201cReinforcement on Federated\u201d (RoF) scheme, based on deep multi-agent reinforcement learning, to solve the problem. Specifically, the RoF scheme is executed decentralizedly at edge servers, which can cooperatively make the optimal device selection and resource allocation decisions. Moreover, a device refinement subroutine is embedded into the RoF scheme to accelerate convergence while effectively saving the on-device energy. Simulation results demonstrate that the RoF scheme can facilitate efficient FL and achieve better performance compared with state-of-the-art benchmarks.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/jsac/ZhangYWPZZS21", "doi": "10.1109/jsac.2021.3118352"}}, "content": {"source": {"pdf_hash": "bfc58b9b00bb846441c01298160fc50b8b5259a2", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "8fd32db441cb3ca2d45ceff623fdc483b3e67a84", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/bfc58b9b00bb846441c01298160fc50b8b5259a2.txt", "contents": "\nOptimizing Federated Learning in Distributed Industrial IoT: A Multi-Agent Approach\n\n\nWeiting Zhang \nMember, IEEEDong Yang \nMember, IEEEWen Wu \nMember, IEEEHaixia Peng \nSenior Member, IEEENing Zhang \nFellow, IEEEHongke Zhang \nXuemin \nFellow, IEEESherman Shen \nOptimizing Federated Learning in Distributed Industrial IoT: A Multi-Agent Approach\n\nIEEE Journal on Selected Areas in Communications\n110.1109/JSAC.2021.31183520733-8716 ( This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information:Index Terms-Resource allocationfederated learningindus- trial IoTdeep multi-agent reinforcement learning\nIn this paper, we aim to make the best joint decision of device selection and computing and spectrum resource allocation for optimizing federated learning (FL) performance in distributed industrial Internet of Things (IIoT) networks. To implement efficient FL over geographically dispersed data, we introduce a three-layer collaborative FL architecture to support deep neural network (DNN) training. Specifically, using the data dispersed in IIoT devices, the industrial gateways locally train the DNN model and the local models can be aggregated by their associated edge servers every FL epoch or by a cloud server every a few FL epochs for obtaining the global model. To optimally select participating devices and allocate computing and spectrum resources for training and transmitting the model parameters, we formulate a stochastic optimization problem with the objective of minimizing FL evaluating loss while satisfying delay and long-term energy consumption requirements. Since the objective function of the FL evaluating loss is implicit and the energy consumption is temporally correlated, it is difficult to solve the problem via traditional optimization methods. Thus, we propose a \"Reinforcement on Federated\" (RoF) scheme, based on deep multi-agent reinforcement learning, to solve the problem. Specifically, the RoF scheme is executed decentralizedly at edge servers, which can cooperatively make the optimal device selection and resource allocation decisions. Moreover, a device refinement subroutine is embedded into the RoF scheme to accelerate convergence while effectively saving the on-device energy. Simulation results demonstrate that the RoF scheme can facilitate efficient FL and achieve better performance compared with state-of-the-art benchmarks.\n [3]\n. Utilized tremendous data, deep learning approaches, especially deep neural networks (DNNs) can facilitate industrial intelligence [4]. For example, a DNN model can be trained to provide fault diagnostic services for industrial facilities. In industrial scenarios, different factories usually generate a huge amount of data that contains critical information, such as the operation conditions of industrial facilities. Since this data information generally plays a crucial role in safety pre-warning and fault diagnostic, it is of great significance to preserve the privacy of industrial data. Traditional centralized training method needs to collect massive raw data from IIoT devices, e.g., industrial gateways (IGWs), which leads to user privacy concerns [5]. Hence, taking into account devices' privacypreserving requirements, the data would be better processed locally [6]- [8].\n\nFederated learning (FL), as an emerging distributed learning paradigm, can effectively solve the above issue [9]. Generally, FL operates over a two-layer architecture epoch-by-epoch. In each FL epoch, DNN models are locally trained at devices using their private dataset; and then the locally-trained model parameters are aggregated by a centralized server to update the global model parameters via a model aggregation algorithm, such as federated averaging (FedAvg) [10]. By running FL for multiple epochs until desired accuracy is achieved, a well-trained DNN model can be obtained to provide highaccuracy services. With the FL paradigm, DNN models can be efficiently trained based on the dispersed datasets held by devices without uploading raw data to a centralized server, thereby preserving privacy for the devices [11].\n\nVery recently, advanced FL schemes have been widely applied in wireless edge networks [12]- [16]. In particular, applying FL in IIoT can greatly improve industrial data utilization with preserved data privacy. To this end, we focus on facilitating efficient FL in distributed IIoT networks. Smart factories of an industrial company are generally located in different geographical areas. To provide services for all factories, such as intelligent fault diagnostic, the company requires a global DNN model to achieve satisfactory performance over the data generated by these factories. However, due to excessive parameter transmission, traditional two-layer FL architecture will lead to high backbone communication overhead.\n\nAccordingly, we introduce a three-layer architecture, i.e., device-edge-cloud, to support FL in the considered distributed scenario, in which the industrial data held by IGWs are geographically dispersed among factories. In each epoch, a set of IGWs need to be selected to participate in the FL, i.e., device selection. DNN model parameters that are locally trained at the selected IGWs are aggregated by edge servers every FL epoch, and the edge aggregated model parameters are transmitted to the cloud server for further aggregation every a few FL epochs. In this way, the amount of transmitted model parameters between the edge and the cloud can be greatly reduced. To efficiently support local training and parameter transmission, on-device computing and network spectrum resources need to be judiciously allocated for the participating devices, i.e., resource allocation. Due to limited network resources, selecting all devices to participate in each epoch may result in a prohibitive delay for parameter uploading, which in turn consumes excessively high on-device energy. Hence, device selection and resource allocation should be jointly considered to optimize FL with satisfied delay and energy consumption requirements.\n\nOptimizing FL in the distributed and resource-limited IIoT networks encounters several challenges. First, in the FL, the optimization objective is to minimize the FL evaluating loss, which is represented by an implicit function. The mapping relationship between the objective function and decision variables cannot be characterized accurately. As such, it is difficult to directly solve the problem via traditional optimization methods. Also, the decision variables of the device selection and resource allocation are correlated, which further complicates decision making process. Second, the long-term energy consumption constraint couples to device selection and resource allocation decisions over time, and yet the decisions have to be made without foreseeing the future network dynamics. Inappropriate decision making may exhaust the on-device energy, consequently slowing down the FL convergence. To address the above challenges, deep reinforcement learning (RL) is a feasible approach to make prophetic decisions through capturing network dynamics [17]- [19].\n\nIn this work, we investigate how to leverage deep RL approaches to dynamically select the participating devices and allocate cherished spectrum and computing resources for optimizing FL in the distributed IIoT networks. First of all, we formulate a joint device selection and resource allocation (JDSRA) problem with the objective of minimizing the FL evaluating loss while satisfying the epoch delay and energy consumption requirements. This problem cannot be directly solved via traditional optimization methods due to the implicit loss function of FL and the temporal correlation of energy consumption constraint. Therefore, we propose a multi-agent learning-based resource management scheme to solve the formulated problem. The learning agents can judiciously make the optimal device selection and resource allocation decisions to efficiently support the long-term FL process without violating the long-term energy consumption and strict delay constraints. The maximum entropy term is incorporated into the reward function of the learning algorithm to effectively improve the exploration. Through training the multi-agent algorithm centrally, the learning agents, deployed at edge servers, can cooperatively make the optimal decisions. Extensive simulation results demonstrate that the proposed scheme achieves better FL performance, as compared to the benchmarks. The main contributions of this paper are summarized as follows:\n\n\u2022 We introduce a three-layer collaborative FL architecture in the distributed IIoT networks, which can facilitate efficient FL while effectively reducing the backbone network traffic for DNN model parameter transmission; \u2022 We formulate JDSRA as a stochastic optimization problem to optimize FL performance while satisfying the strict epoch delay and long-term energy consumption requirements, which is then transformed into a partiallyobservable Markov decision process (POMDP); \u2022 We propose a \"Reinforcement on Federated\" (RoF) scheme, based on a deep multi-agent RL algorithm and a device refinement subroutine, to solve the POMDP. Through decentralized execution among multiple agents, the RoF scheme can cooperatively make the optimal decisions and accelerate convergence. The remainder of this paper is organized as follows. Section II reviews related works. In Section III, a three-layer collaborative FL architecture is presented. Section IV presents the system model, and then the problem is formulated and transformed in Section V. Section VI proposes the learningbased RoF scheme. Simulation results are given in Section VII, and the concluding remarks are provided in Section VIII.\n\nII. RELATED WORK Recently, there are several research works on FL. A line of works focus on device selection to accelerate FL convergence. Generally, selecting more devices in each FL epoch can improve the convergence rate. To this end, Yang et al. [20] aimed at maximizing the number of participating devices in each FL epoch via exploiting superposition property of wireless multiple-access channels. Nevertheless, a large number of participating devices leads to prohibitive communication cost due to excessive parameter transmission. Wang et al. [21] proposed a learning-based scheme to flexibly select a set of appropriate devices, consequently speeding up FL convergence. Different from the above works, our work focuses on cooperatively selecting devices for three-layer FL architecture via multiple learning agents. In addition, a device refinement subroutine is adopted to obtain a refined set of selected devices, which can ensure the FL can be completed with satisfied delay and energy consumption requirements, thereby accelerating FL convergence.\n\nAnother line of works devote to resource allocation to facilitate efficient FL. In [11], an FL delay minimization problem is studied in cellular networks, in which the spectrum resource is dynamically allocated for parameter transmission. In [22], an opportunistic spectrum access scheme is proposed to minimize the energy consumption of FL. The above works focus on implementing FL based on a traditional two-layer architecture. Different from these works, our paper aims to optimize FL in a distributed IIoT network, in which the industrial data are dispersed geographically and backbone network is congested. In such case, instead of applying the two-layer architecture, we adopt a novel three-layer collaborative architecture to support FL, which can effectively reduce backbone network traffic for parameter transmission.\n\nDeep RL techniques hold the potential to improve resource utilization in wireless networks via learning network dynamics [23]- [25]. Many advanced deep RL based algorithms have been developed to perform dynamic resource management. In [26], a deep Q-network (DQN) based algorithm is proposed to decide the devices to conduct data collection in an unmanned aerial vehicle-assisted mobile edge network. In [17], Chen et al. adopted a double DQN algorithm to make computation offloading and packet scheduling decisions in radio access networks. Peng et al. [19] leveraged a deep deterministic policy gradient (DDPG) based algorithm to allocate spectrum resources in complex urban vehicular networks and learn the optimal task offloading policy. Moreover, aiming at optimizing the energy efficiency in a large-scale cognitive radio network, Kaur et al. [27] proposed a multi-agent Q-Learning based algorithm to obtain a decentralized radio resource allocation strategy. The above works provide valuable insights for achieving dynamic resource management in wireless networks. Different from the above works, we focus on facilitating efficient FL in distributed IIoT via leveraging deep RL approaches to judiciously allocate spectrum and computing resources.\n\nIn our preliminary paper [1], we proposed a centralized resource management scheme, in which a single-agent based algorithm is designed to globally allocate network resources. As an extension of [1], in this paper, we further propose a decentralized learning-based algorithm to perform judicious resource management for FL operation. In this algorithm, each geographically distributed base station (BS) is equipped with an edge controller that can only observe its individual environment information and make the decision for supporting FL independently.\n\n\nIII. THREE-LAYER COLLABORATIVE FL ARCHITECTURE\n\nTo implement FL in distributed IIoT networks, we consider a three-layer collaborative FL architecture empowered with a hierarchical aggregation algorithm [28]. As shown in Fig. 1, the architecture is composed of the device, edge, and cloud layers endowed with different functionalities. The descriptions of each layer in the architecture are as follows.\n\n\u2022 Device Layer: Energy-limited IGWs endowed with computing capability are geographically distributed in smart factories located in different cities. In each FL epoch, DNN models are locally trained at IGWs. \u2022 Edge Layer: Multiple BSs are deployed to provide services for the IGWs. Each BS is equipped with an edge server to perform edge model parameter aggregation every FL epoch. Additionally, an edge controller deployed at each edge server can manage network resources and coordinate FL within the coverage of the edge server. \u2022 Cloud Layer: A cloud server is deployed to perform global parameter aggregation, which is used to aggregate the updated parameters from edge servers after a few edge model parameter aggregations. Within the three-layer architecture, DNN model parameters are infrequently transmitted between the edge and cloud, such that the backbone network traffic can be effectively reduced. 1 In this architecture, we consider a set of edge servers, denoted by M = {1, 2, \u00b7 \u00b7 \u00b7 , M }. Let N m = {1, 2, \u00b7 \u00b7 \u00b7 , |N m |} be the set of IGWs within the coverage of edge server m \u2208 M. Let D m,n = {(x d , y d )} d denote the local data set held by IGW n under edge server m, where x d and y d are the input and the label of each sample, respectively, and d is the sample index. Moreover, D T m,n and D E m,n are the training and evaluating data sets of the IGW, respectively, where D T m,n \u222a D E m,n = D m,n . Note that the training data samples are assumed to follow a non-independent and identical distribution (non-IID). Similarly, D T m and D E m are the total training and evaluating data sets of the participating devices covered by edge server m, respectively, where D T m \u222a D E m = D m . Notably, due to the data privacy preserving requirement, the data samples held by each IGW cannot be shared with others during the FL process.\n\nThe operation procedure of the FL algorithm based on the three-layer architecture is given in Algorithm 1, which Device selection and resource allocation 4 Obtain a refined set of participating IGWs N m , \u2200m \u2208 M; 5 for each F L epoch t \u2208 R do 6 Model parameter distribution (downlink spectrum) 7 Broadcast wm to the selected IGWs; 8 Local model training (on-device computing) 9 for each BS m \u2208 M in parallel do 10 for each IGW n \u2208 N m in parallel do 11 for each training sample d \u2208 D T m,n do 12 wm,n \u2190 wm \u2212 \u03bb\u2207f (x d , y d ; wm,n); 13 Edge aggregation (uplink spectrum) 14 if mod(t, tc) = 0 then Cloud aggregation (uplink spectrum) 18 if mod(t, tc) = 0 then\n19 w \u2190 1/ D E m\u2208M D E m wm; 20 F (w) = Fc(w); 21 return w, F (w).\nis detailed as follows.\n\n\u2022 Task Initialization (Lines 1-2): In the initialization stage, a training task is initiated at the cloud server, and a global DNN model with a set of initialized parameters w 0 is instantiated, such as a diagnostic DNN model that is utilized to classify the health conditions of industrial equipments. \u2022 Device Selection and Resource Allocation (Lines 3-4): Once the training task and the global DNN model are initiated, the edge controllers will make the device selection and resource allocation decisions for FL epoch t \u2208 R, respectively. In specific, the decisions include:\n\n(1) the set of IGWs that are selected to participate in the current FL epoch; and (2) the amounts of downlink/uplink spectrum and computing resources that are allocated to the selected IGWs. Given the decisions, a refined device set N m , \u2200m \u2208 M can be obtained accordingly. \u2022 Global Model Parameter Distribution (Lines 6-7):\n\nWith the refined device set N m , \u2200m \u2208 M, the global model parameters can be distributed from the edge or cloud to the participating IGWs. \nw m = 1 |D E m | m\u2208M,n\u2208N m D E m,n w m,n ,(2)\nwhere D E m,n and D E m are the numbers of evaluating samples held by each participating IGW and by the participating IGWs of edge server m. In the next FL epoch, the aggregated model parameters on each edge server can be distributed to the newly participating IGWs for further update. In such case, the evaluating loss of the DNN model can be described as\nF e (w m ) = 1 M m\u2208M F m (w m ),(3)\nwhere\nF m (w m ) = 1 |D E m | d\u2208D E m f (x d , y d ; w m )+ \u00b5 2 w m 2 ,(4)\nwhere f (x d , y d ; w m ) denotes the loss on each evaluating sample of the participating IGWs with parameters w m aggregated by edge server m, and \u00b5 w m 2 /2 is the regularization term that is added to prevent parameter overfitting, and \u00b5 is the regularization coefficient. \nw = 1 |D E | m\u2208M D E m w m .(5)\nThe evaluating loss is given by\nF c (w) = 1 |D| d\u2208D f (x d , y d ; w) + \u00b5 2 w 2 ,(6)\nwhere f (x d , y d ; w) represents the evaluating loss on each evaluating sample of the participating IGWs with parameters w aggregated by the cloud server, and |D| denotes the total number of evaluating samples held by all participating IGWs in the current FL epoch. Note that when the connection between an IGW and the corresponding edge server is established, the IGW can send its information, including the number of training samples held by the IGW, to the edge server, and then such information is further reported to the centralized node (i.e., the cloud server). As such, the cloud server can have the information of the numbers of training samples held by distributed IGWs.\n\n\nIV. SYSTEM MODEL\n\nIn this section, we present the FL delay and energy consumption models. Then, the FL evaluating loss is defined to measure the FL performance.\n\n\nA. FL Delay Model\n\nAs previously mentioned, the FL process operates epochby-epoch. To facilitate efficient FL, each epoch is constrained by a delay requirement T max , and the total delay of each IGW during the FL operation is constrained by a budget T max m,n . In specific, the delay for FL includes three parts, which are defined as follows.\n\n1) Delay of global model distribution: In FL epoch t, a certain number of IGWs needs to be selected to participate in the FL. Let o m,n,t \u2208 {0, 1} , \u2200m \u2208 M, n \u2208 N m be the binary decision variable of device selection. Specifically, if o m,n,t equals 1, IGW n under edge server m is then selected, otherwise 0. We adopt orthogonal frequency allocation to support model parameter distribution and aggregation, which are operated with the available bandwidth B \u2193 (downlink) and B \u2191 (uplink), respectively. Here, the underlying assumption is that we do not consider other traffic loads in the considered scenario. This assumption is widely adopted in performance analysis of FL frameworks [8].\n\nThus, the downlink spectrum efficiency for edge server m is given by \u03b3 \u2193 m,n,t = log 2 1 + p m,t |g m,n,t | 2 /\u03c3 2 , \u2200m \u2208 M, n \u2208 N m , where p m,t is the transmission power, |g m,n,t | 2 is the channel gain between edge server m and IGW n, and \u03c3 2 is the Gaussian noise power [29]. A fraction \u03be \u2193 m,n,t of bandwidth B \u2193 is allocated to IGW n from edge server m. Here, we have 0 \u2264 \u03be \u2193 m,n,t \u2264 1, and m\u2208M,n\u2208Nm o m,n,t \u03be \u2193 m,n,t = 1. Note that this paper focuses on the spectrum allocation among IIoT devices, i.e., what percentage of resources should be allocated to an IIoT device. As such, we adopt a fractional bandwidth model. Hence, the downlink transmission rate of edge server m for IGW n is R \u2193 m,n,t = \u03be \u2193 m,n,t B \u2193 \u03b3 \u2193 m,n,t , \u2200m \u2208 M, n \u2208 N m . When distributing model parameters to IGW n, the transmission delay d \u2193 m,n,t can be calculated based on the following two cases: (1) d \u2193 m,n,t = S g /R \u2193 m,n,t + S g /R c , when the global model parameters are distributed from the cloud server, where S g (in bits) is the data size of the global DNN model parameter and R c represents the backhaul transmission rate between the cloud and the edge; and (2) d \u2193 m,n,t = S g /R \u2193 m,n,t , when the aggregated model parameters are distributed from edge servers to its covered IGWs.\n\nWith the orthogonal frequency allocation mechanism, the delay for distributing model parameters is determined by the slowest one, namely,\nT \u2193 t,com = max m\u2208M,n\u2208Nm d \u2193 m,n,t ,(7)\nwhere max{\u00b7} is the function to return the largest value.\n\n2) Delay of local model training: In the considered IIoT networks, the distributed IGWs are endowed with a certain on-device computing capabilities, denoted by f (central processing unit (CPU) cycles per second). The IGW's computing resource should be judiciously allocated to ensure that local model training can be completed within the strict delay threshold. Let \u03b7 m,n,t be the decision variable of the fraction of computing resource the IGW used in FL epoch t, where 0 \u2264 \u03b7 m,n,t \u2264 1. Thus, the delay for local model training can be given by\n\u03c4 m,n,t = a D T m,n \u03b7 m,n,t f , \u2200m \u2208 M, n \u2208 N m ,(8)\nwhere a is the number of CPU cycles required for processing per bit data, and D T m,n is the size of training set held by each participating device. For different FL models, parameter a is set accordingly to reflect the effect of the adopted models on the training delay and energy consumption. Similarly, the delay for local model training is determined by the slowest one, i.e.,\nT t,cmp = max m\u2208M,n\u2208Nm \u03c4 m,n,t .(9)\n3) Delay of model parameter uploading: Similar to the model parameter distribution, the uplink spectrum efficiency for parameter aggregation can be expressed as \u03b3 \u2191 m,n,t = log 2 1 + p m,n,t |g m,n,t | 2 /\u03c3 2 , \u2200m \u2208 M, n \u2208 N m , where p m,n,t is the transmission power from IGW n to edge server m [30]. Let \u03be \u2191 m,n,t (0 \u2264 \u03be \u2191 m,n,t \u2264 1) denotes a decision variable of the fraction of uplink bandwidth B \u2191 allocated to IGW n, where m\u2208M,n\u2208Nm o m,n,t \u03be \u2191 m,n,t = 1. Hence, the uplink transmission rate can be described as\nR \u2191 m,n,t = \u03be \u2191 m,n,t B \u2191 \u03b3 \u2191 m,n,t , \u2200m \u2208 M, n \u2208 N m . The transmission delay d \u2191\nm,n,t of uploading DNN model parameters from IGW n to edge server m also has two cases: (1) for the edge aggregation, the updated model parameters of IGWs need to be aggregated by the correlated edge server, and d \u2191 m,n,t = S l /R \u2191 m,n,t ; and (2) for the cloud aggregation, d \u2191 m,n,t = S l /R \u2191 m,n,t + S l /R c . Here, S l (in bits) is the parameter size of the local DNN model. Since the model parameter size is the same during the FL operation, we have S l = S g . In addition, DNN model parameters are infrequently transmitted between the edge and the cloud, thus we assume the backhaul transmission rates R c in uplink and downlink are the same. Due to the synchronous aggregation mechanism, the delay for distributing model parameters is determined by the slowest IGW, i.e.,\nT \u2191 t,com = max m\u2208M,n\u2208Nm d \u2191 m,n,t .(10)\nTaking T \u2193 t,com , T t,cmp , and T \u2191 t,com into consideration, the total delay for each FL epoch can be given by\nT t = T \u2193 t,com + T t,cmp + T \u2191 t,com .(11)\nIn addition, given the model parameter distribution, local training, and parameter uploading delays, the cumulative FL delay of each IGW is described as\nT m,n,t = T m,n,t\u22121 + d \u2193 m,n,t + \u03c4 m,n,t + d \u2191 m,n,t ,(12)\nwhich is used to calculate the remaining time of FL, and thus affecting whether the IGWs can participate in the subsequent FL epochs. Here, T m,n,t\u22121 denotes the cumulative FL delay of each IGW until epoch t \u2212 1.\n\n\nB. Energy Consumption Model\n\nIn the long-term FL process, IGWs are constrained by their own energy capacity E max m,n , which should be satisfied if the IGW is selected to participate in the FL. In specific, the energy consumption for FL includes two parts, which are calculated as follows, respectively. 1) Energy consumption of parameter uploading: In the model parameter uploading stage, the energy consumption of IGW n covered by edge server m is given by\nE com m,n,t = p m,n,t d \u2191 m,n,t , \u2200m \u2208 M, n \u2208 N m .(13)\n2) Energy consumption of local training: Performing local model training at IGWs also consumes energy, which is affected by the allocated computing resource of the participating IGWs and the computing workload of the local training process. Hence, the energy consumption of local training at each IGW can be given by\nE cmp m,n,t = \u03ba 2 a D T m,n (\u03b7 m,n,t f ) 2 , \u2200m \u2208 M, n \u2208 N m ,(14)\nwhere \u03ba is the effective capacitance coefficient of the computing chipset of IGWs [28].\n\nTaking the above two parts into consideration, the cumulative energy consumption can be calculated by\nE m,n,t = E m,n,t\u22121 + E com m,n,t + E cmp m,n,t , \u2200m \u2208 M, n \u2208 N m ,(15)\nwhich is utilized to calculate the remaining energy resource of IGWs, and hence evaluating whether the IGWs satisfy the long-term energy consumption requirements. Here, E m,n,t\u22121 denotes the cumulative energy consumption of each IGW until epoch t \u2212 1.\n\n\nC. FL Evaluating Loss\n\nIn the considered architecture, FL operates epoch-by-epoch with the selected IGWs, which can satisfy the FL epoch delay requirement T max , time budget T max m,n , and energy consumption constraint E max m,n . The iteration process aims to obtain the optimal model parameters w * that can minimize the evaluating loss on the evaluating datasets of the participating IGWs. To optimize FL under both the edge and cloud aggregation cases, the FL evaluating loss can be defined as\nF (w) = F e (w m ), if mod(t, t c ) = 0; F c (w), otherwise.(16)\nIf mod(t, t c ) = 0, edge servers aggregate the covered model parameters from the participating IGWs, respectively, and obtain the evaluating loss F e (w m ) with the aggregated parameters w m , \u2200m \u2208 M; otherwise, the cloud server aggregates the model parameters from edge servers and obtains the evaluating loss F c (w) with the global parameters w.\n\n\nV. PROBLEM FORMULATION AND TRANSFORMATION\n\nIn this section, we formulate an optimization problem to manage the spectrum and computing resources and then transform the formulated problem into a POMDP.\n\n\nA. Problem Formulation\n\nAs previously described, the FL evaluating loss F (w) is used to optimize the performance in the FL process. Thus, to minimize F (w) over the aggregated model parameters and IGWs' private datasets while satisfying the FL epoch delay, time budget, and IGWs' long-term energy consumption requirements, we formulate the problem as follows:\nP 0 : min o,\u03be \u2193 , \u03be \u2191 , \u03b7 F (w) s.t. E m,n,t \u2264 E max m,n , \u2200m \u2208 M, n \u2208 N m , t \u2208 R, (17a) T m,n,t \u2264 T max m,n , \u2200m \u2208 M, n \u2208 N m , t \u2208 R, (17b) T t \u2264 T max , \u2200t \u2208 R, (17c) o m,n,t \u2208 {0, 1} , \u2200m \u2208 M, n \u2208 N m , t \u2208 R, (17d) m\u2208M,n\u2208Nm o m,n,t \u03be \u2193 m,n,t = 1, 0 \u2264 \u03be \u2193 m,n,t \u2264 1, (17e) m\u2208M,n\u2208Nm o m,n,t \u03be \u2191 m,n,t = 1, 0 \u2264 \u03be \u2191 m,n,t \u2264 1, (17f) 0 \u2264 \u03b7 m,n,t \u2264 1, \u2200m \u2208 M, n \u2208 N m , t \u2208 R,(17g)where o = {o m,n,t } \u2200m\u2208M,n\u2208Nm , \u03be \u2193 = {\u03be \u2193 m,n,t } \u2200m\u2208M,n\u2208Nm , \u03be \u2191 = {\u03be \u2191\nm,n,t } \u2200m\u2208M,n\u2208Nm , and \u03b7 = {\u03b7 m,n,t } \u2200m\u2208M,n\u2208Nm . With these decisions, the model parameters w are optimized in the local model training stage via optimization methods, such as the stochastic gradient decent method [31]. Constraints (17a), (17b), and (17c) ensure the cumulative energy consumption and time, and FL epoch delay can be restricted within E max m,n , T max m,n , and T max , respectively. Note that although a synchronous mechanism is adopted to aggregate the distributed model parameters, the cumulative delay for each IGW is different since the IGWs would go to sleep after the stage of parameter distribution, local training, or parameter uploading. Thus, the practical service delay in each epoch is different during the FL operation, and the constraint (17b) ensures the cumulative delay for each IGW can be restricted within T max m,n . In addition, inappropriate resource allocation decisions may lead to a large delay for parameter distribution, local training, and parameter uploading, and therefore reducing the effectiveness of the FL operation. Thus, the constraint (17c) is adopted to ensure the epoch delay can be restricted within T max . Constraint (17d) indicates whether the corresponding IGW is selected to participate in FL epoch t. Constraints (17e)-(17g) guarantee the available bandwidth B \u2193 and B \u2191 , and on-device computing resource f can be judiciously allocated.\n\nThrough solving the formulated loss minimization problem, FL performance can be efficiently optimized in the considered distributed IIoT networks with satisfied multiple requirements. However, there are some challenges in solving the problem.\n\n\u2022 The objective function of problem P 0 is implicit and constraint (17a) is temporally correlated, which prevent traditional optimization methods to solve the problem. Due to the strict delay requirement of FL epochs, problem P 0 should be solved with a low latency. \u2022 Problem P 0 is a mix-integer nonlinear optimization problem, which contains both discrete and continuous decision variables. Also, the considered decision variables are correlated with each other, which further complicates decision making process. The device selection and resource allocation aim at minimizing the long-term evaluating loss, which can be reformulated as a Markov decision process. To obtain the optimal solution, deep RL is an effective approach endowed with strong representation capabilities, which can effectively solve the resource optimization problem with mixed decision variables and temporally correlated constraints [32], [33]. However, single-agent based deep RL algorithms generally require a global controller to collect all the state information from distributed IGWs and observe the entire network environment, which incurs backbone signaling overhead. To address the issue, a deep multi-agent RL algorithm should be adopted, in which each edge controller acts as an agent to cooperatively learn a dynamic resource management policy and solve the problem.\n\n\nB. Problem Transformation\n\nTo optimize FL in the distributed IIoT networks, we first transform the formulated problem P 0 into a POMDP for M agents [34]. In the considered distributed scenario, the edge controllers are modelled as the edge agents, respectively. Each edge agent can observe the state information from their individual environment. For the transformed POMDP, the edge agents can capture the dynamics, i.e., the remaining energy resource of IGWs, from the environment to learn their optimal policies {\u03c0 * \u03b8m } \u2200m\u2208M parameterized by \u03b8 m . The policies can accurately map a set of states {s m,t } \u2200m\u2208M to a set of actions {a m,t } \u2200m\u2208M , which is evaluated by the reward function {r m,t } \u2200m\u2208M . The state, action, and reward can be defined as follows.\n\n1) State: In FL epoch t, each edge agent collects the state information from their individual environment, which indicates the remaining FL time and energy of IGWs. Namely,\ns m,t = {T r m,n,t , E r m,n,t }, \u2200m \u2208 M, n \u2208 N m ,(18)\nwhere T r m,n,t = T max m,n \u2212 T m,n,t\u22121 and E r m,n,t = E max m,n \u2212 E m,n,t\u22121 .\n\n2) Action: According to the observed individual environment states, the edge agents cooperatively make decisions to select the IGWs to participate in the current FL epoch, and allocate the downlink/uplink spectrum and on-device computing resources for supporting the FL, i.e., a m,t = {o m,n,t , \u03be \u2193 m,n,t , \u03be \u2191 m,n,t , \u03b7 m,n,t }, \u2200m \u2208 M, n \u2208 N m .\n\nEach action element is reshaped into a range of [0, 1]. Note that only when o m,n,t equals 1, the IGW then can use the allocated spectrum and computing resources to support FL, such that constraints (17d)-(17g) can be satisfied.\n\n3) Reward: Once the action a m,t of edge agent m has been determined based on the observed individual state s m,t . The agent will obtain a reward to evaluate the quality of the action. To minimize the FL evaluating loss, the reward function of the multi-agent learning algorithm then can be defined as\nr m,t = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 \u2212F e (w m ), if mod(t, t c ) = 0, \u2212F c (w), if mod(t, t c ) = 0, \u2212U, if no aggregation,(20)\nwhere F e (w m ) and F c (w) are the FL evaluating losses of DNN models after the edge and cloud aggregations, respectively. Considering the agents are cooperatively performing the FL operation, F e (w m ) and F c (w) are set as the rewards for each agent in these two aggregation cases. Note that F e (w m ) is obtained by a mean value of F m (w m ) that is defined in Eq. (4). An edge at the edge can obtain the mean loss via the following steps. First, the loss value F m (w m ) evaluated at the edge servers is transmitted to a cloud server. Second, the mean loss, F e (w m ) = 1 M m\u2208M F m (w m ), is calculated by the cloud server. Third, F e (w m ) is broadcasted to each edge server, which is used as the reward of the edge agent. In addition, U is a penalty factor, which should take a relatively large value to penalize the decisions that cannot aggregate any model parameters within constraints (17a)-(17c) in each FL epoch. In particular, the minus before the above three terms is mainly used to transform the original loss minimization problem into a reward maximization problem.\n\nIn the POMDP, each agent m aims at maximizing its own cumulative discounted reward with the optimal device selection and resource allocation policy \u03c0 * \u03b8m . As such, P 0 can be reformulated as multiple distributed optimization subproblems, i.e.,\nP m,1 : max \u03c0 \u03b8m E \u221e \u03b4=0 \u03b3 \u03b4 r m,t+\u03b4 |\u03c0 \u03b8m , s m,t , a m,t s.t. (17a) \u2212 (17g),(21a)\nwhere \u03b3 \u2208 [0, 1] is the discount factor. Due to the cooperative relationship among the edge agents, sub-problems P m,1 (\u2200m \u2208 M) are correlated with each other.\n\n\nVI. ROF: A MULTI-AGENT SCHEME FOR EFFICIENT FL\n\nTaking into account the correlation among sub-problems P m,1 (\u2200m \u2208 M), which should be jointly solved to achieve the same goal, i.e., reward maximization. Moreover, due to the mix-integer decision variables, instead of value-based RL algorithms, we adopt a policy gradient-based algorithm, i.e., soft actor-critic (SAC) [35], for each agent to address its POMDP with more robust learning performance.\n\nIn this section, we propose the RoF scheme, based on a multi-agent SAC (MASAC)-based resource management algorithm and a device refinement subroutine, to cooperatively solve the modelled POMDP for M edge agents. As illustrated in Fig. 2, the proposed MASAC-based algorithm that combines the SAC algorithm with a cooperative multi-agent RL framework [36], is presented. In the following, we first introduce the SAC and multi-agent RL framework, and then present the MASAC and the device refinement subroutine in detail, followed by the computational complexity analysis of the MASAC.\n\n\nA. SAC and Multi-Agent RL Framework\n\nSAC Algorithm: As shown in the right part of Fig. 2, each edge agent is based on the SAC algorithm. In the SAC, three main components are incorporated to enhance the performance: (1) entropy maximization, (2) \"actor-critic\" network architecture, and (3) off-policy formulation, which are detailed as follows.\n\n1) Maximum entropy is considered to effectively enhance the learning stability and exploration, which aims to maximize both the cumulative discounted reward and the expected  entropy of the policy simultaneously. With this maximum entropy objective, the stochasticity of the agent's policy can be greatly improved while enabling more possible optimal decisions to be explored. As such, the reformulated subproblems P m,1 (\u2200m \u2208 M) can be rewritten as\nP m,2 : max \u03c0 \u03b8m E \u221e \u03b4=0 \u03b3 \u03b4 (r m,t+\u03b4 + \u03b1 m H (\u03c0 \u03b8m (\u00b7|s m,t ))) s.t. (17a) \u2212 (17g),(22a)\nwhere H (\u00b7) is the entropy term, and \u03b1 m is a weight that controls the relative importance of both terms.\n\n2) Actor-critic architecture of each edge agent is composed of an actor network, a pair of evaluation critic networks, and a pair of target critic networks. The actor makes decisions a m,t with its own policy \u03c0 \u03b8m . The critics calculate a pair of Q-values for evaluating the policy, respectively. As shown in Fig. 3, the actor and the critics are endowed with a four-layer fully-connected neural network (FCNN), respectively. Some tricks are used in both networks to improve the effectiveness of the algorithm. For the actor network, the input (i.e., individual states of each agent) should be normalized to avoid overfitting. In the output layer, decision elements should also be addressed via discretization or normalization methods, to satisfy the mixed decision variable constraints. For the critic networks, the input (i.e., action-state pairs) should also be normalized to contribute to better Q-value estimation. Note that the target critics have the same structures as the evaluation critics.\n\n3) The off-policy formulation is adopted to improve the sample efficiency, which is based on an experience replay technique. Specifically, an experience replay pool K r endowed with a certain memory capacity is deployed. In the sampling stage, the experiences of all the agents are stored into the K r . In the training stage, the agents randomly sample a batch of experiences from K r to train the network parameters in an off-policy manner, such that the sampled experiences can be effectively utilized to achieve convergence.\n\nMulti-Agent RL Framework: According to the left part of Fig. 2, the edge agents operate at the corresponding edge controllers, respectively. Each agent can only observe the local information from its individual environment, and the decisions made by the agent are insensible to the others.\n\nWe adopt the mechanism of centralized training while decentralized execution [36], as illustrated in the central part of Fig. 2. The mechanism allows the policies to utilize additional information to simplify training process, on condition that this information is not utilized at execution stage. In the centralized training stage, except for the local information, additional information, including the states and actions of all the agents, is also available to each other. Specifically, the actor network of the edge agent captures the environment dynamics only from its own observed local state information, and then makes decisions for the individual environment. The critic network requires the action-state pairs of all the edge agents to generate the Q-values for evaluating the decisions. In the decentralized execution stage, the network parameters of the actor and the critics no longer need to be updated. Thus, only the local state information is required for the actor of each edge agent, the Obtain decision am,t with policy \u03c0 \u03b8m (sm,t); JDSRA decisions then can be obtained independently without being aware of other agents' state information.\n\nRemark: Once well trained, the RL algorithm is deployed to make online decisions for the three-layer FL architecture, which is an inference process. The energy consumption of inference process of the proposed RL algorithm is very low since the adopted neural network has shallow layers, we therefore neglect the energy consumption consumed by the RL algorithm.\n\n\nB. MASAC-Based Algorithm\n\nIn the following, we illustrate the learning procedure of the MASAC in detail, which is presented in Algorithm 2.\n\n1) Initialization (Line 1): At the beginning of the centralized training stage, the parameters of the actor and critic networks of the edge agents are initialized, which will be updated in terms of the learning step. In addition, an experience replay pool K r is instantiated.\n\n2) Experience sampling (Lines 6-9): The experience of the edge agents is denoted by a multi-dimensional tuple of the selected action, state transition, and feedback reward, i.e., {s m,t , a m,t , r m,t , s m,t } \u2200m\u2208M , which is obtained via the following steps. First, the edge agents observe the local state information {s m,t } \u2200m\u2208M from their individual environments, respectively. Second, the actors of the agents independently make the decisions {a m,t } \u2200m\u2208M with their own policy {\u03c0 \u03b8m } \u2200m\u2208M according to the local information. Here, a m,t = {o m,t , \u03be \u2193 m,t , \u03be \u2191 m,t , \u03b7 m,t } represents the JDSRA decisions made by edge agent m for optimizing FL under the correlated BS. Then, the agents obtain the reward {r m,t } \u2200m\u2208M , and individual environments evolve to the next state {s m,t } \u2200m\u2208M . Finally, the formulated transition tuple {s m,t , a m,t , r m,t , s m,t } \u2200m\u2208M is stored in the K r for algorithm parameter updating.\n\n\n3) FCNN parameter updating (Lines 10-17):\n\nIn this stage, the edge agents are coordinated, and the algorithm alternates between collecting experience from all the individual environments with the current agents' policies and updating the FCNNs based on the batch of K b transitions {s k m,t , a k m,t , r k m,t , s k m,t } \u2200m\u2208M sampled from the K r . For edge agent m, the detailed update procedure is given as follows.\n\nFirstly, the parameters \u03c6 m,v , \u2200v \u2208 {1, 2} of the evaluation critic networks are independently updated via minimizing the loss function L(\u03c6 m,v ), which is given by\nL(\u03c6 m,v ) = 1 2K b K b k=1 ( min v=1,2 Q \u03c6m,v (s k t , a k 1,t , \u00b7 \u00b7 \u00b7 , a k M,t ) \u2212 y k m,t ) 2 ,(23)\nwhere\ny k m,t = r k m,t + \u03b3( min v=1,2 Q \u03c6 m,v (s k t , a k 1,t , \u00b7 \u00b7 \u00b7 , a k M,t )\u2212 \u03b1 m log(\u03c0 \u03b8m (a k 1,t , \u00b7 \u00b7 \u00b7 , a k M,t |s k t )))(24)\ndenotes the target Q-value. Here, s k t = {s k m,t } \u2200m\u2208M and s k t = {s k m,t } \u2200m\u2208M are the current and the next state sets of all the agents, log(\u00b7) is the function to return the actions' entropy value, and Q \u03c6m,v (\u00b7) and Q \u03c6 m,v (\u00b7) are the stateaction Q-values calculated by the evaluation and target critic networks. Note that two critics with parameters \u03c6 m,v and \u03c6 m,v , \u2200v \u2208 {1, 2} are deployed in both the evaluation and target networks, respectively, to alleviate positive bias in policy improvement. More importantly, only the minimum of these two Q-values is used for the loss calculation. As such, the stochastic gradient can be obtained for updating the parameter of the critic networks, i.e.,\n\u2207 \u03c6m,v L(\u03c6 m ) = \u2207 \u03c6m,v Q \u03c6m,v (s k t , a k 1,t , \u00b7 \u00b7 \u00b7 , a k M,t )\u00b7 (Q \u03c6m,v (s k t , a k 1,t , \u00b7 \u00b7 \u00b7 , a k M,t )\u2212 (r k m,t + \u03b3(Q \u03c6 m,v (s k t ,\na k 1,t , \u00b7 \u00b7 \u00b7 , a k M,t )\u2212 \u03b1 m log(\u03c0 \u03b8m (a k 1,t , \u00b7 \u00b7 \u00b7 , a k M,t |s k t ))))).\n\nSecondly, the actor network is updated via policy gradient methods, and the objective function is defined as\nJ(\u03b8 m ) = \u2212 E s \u223cK b [E a \u223c\u03c0 \u03b8m [ min v=1,2 Q \u03c6 m,v (s k t , a k 1,t , \u00b7 \u00b7 \u00b7 , a k M,t ) \u2212 \u03b1 m log(\u03c0 \u03b8m (a k 1,t , \u00b7 \u00b7 \u00b7 , a k M,t |s k t ))]].(26)\nHere, the policy is reparameterized via an FCNN denoted by g \u03b8m ( t ; s m,t ), in which an input noise t is added to obtain a lower variance estimation. Hence, Eq. (26) can be rewritten as\nJ(\u03b8 m ) = \u2212 E s \u223cK b , t \u223c\u03c0 \u03b8m [ min v=1,2 Q \u03c6 m,v (s k t , g \u03b8m ( t ; s k t )) \u2212 \u03b1 m log(\u03c0 \u03b8m (g \u03b8m ( t ; s k t )|s k t ))],(27)\nwhere t is sampled from a Gaussian distribution. Thus, the 0733-8716 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\n\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. \n\n\nAlgorithm 3: Device Refinement Subroutine\n\nInput: Decision {o, \u03be \u2193 , \u03be \u2191 , \u03b7} obtained by Algorithm 2; Output: Refined participating device set N m ; 1 Select an initial set of Nm, \u2200m \u2208 M IGWs for each edge server with device selection decision o; 2 Allocate downlink \u03be \u2193 and uplink \u03be \u2191 spectrum and on-device computing \u03b7 resources for the selected IGWs; 3 Calculate the remaining energy E r m,n,t and time budget T r m,n,t , and FL epoch delay Tt; 4 Set N m = \u2205; 5 for each BS m \u2208 M do 6 for each IGW n \u2208 Nm do 7 if E r m,n,t \u2264 E max m,n , T r m,n,t \u2264 T max m,n , and Tt \u2264 T max then\n8 N m \u2190 N m \u222a {(m, n)}.\ngradient of the policy can be calculated, i.e., \u2207 \u03b8m J(\u03b8 m ) = \u2207 \u03b8m \u03b1 m log(\u03c0 \u03b8m (a k 1,t , \u00b7 \u00b7 \u00b7 , a k M,t |s k t ))+ (\u2207 am,t \u03b1 m log(\u03c0 \u03b8m (a k 1,t , \u00b7 \u00b7 \u00b7 , a k M,t |s k t ))\u2212 \u2207 am,t Q(s k t , a k 1,t , \u00b7 \u00b7 \u00b7 , a k M,t ))\u2207 \u03b8m g \u03b8m ( t ; s k t ).\n\n(28) Thirdly, instead of choosing the weight \u03b1 m manually, an FCNN is adopted to automate the weight set for the maximum entropy objective. The gradients of \u03b1 m can be computed by the following objective, i.e.,\nJ(\u03b1 m ) = E at\u223c\u03c0 \u03b8m \u2212\u03b1 m log(\u03c0 \u03b8m (a k t |s k t )) \u2212 \u03b1 m H ,(29)\nwhere a k t = {a k m,t } \u2200m\u2208M is the decision set of all the agents, and H is the value of target entropy.\n\nFinally, to stabilize the learning process, the parameters \u03c6 m,v of target critic networks are updated from the evaluation critics' parameters \u03c6 m,v via a soft-updating method, i.e.,\n\u03c6 m,v = \u03c4 \u03c6 m,v + (1 \u2212 \u03c4 )\u03c6 m,v , \u2200v \u2208 {1, 2},(30)\nwhere \u03c4 \u2208 (0, 1) is the update factor.\n\n\nC. Device Refinement Subroutine\n\nIn each FL epoch, Algorithm 2 generates the JDSRA decisions to support the FL operation. Based on these decisions, a refinement subroutine is adopted to obtain a refined participating device set that can complete the FL with satisfied energy and delay requirements, thereby accelerating FL convergence. The subroutine is operated as the following steps.\n\n1) Remaining on-device resource calculation (Lines 1-4): After obtaining the JDSRA decisions, the edge controllers can select an initial set of N m , \u2200m \u2208 M IGWs to participate in the FL with the allocated spectrum and computing resources. And then, the remaining on-device energy and IGWs' time budget can be calculated, which are consumed by the FL parameter transmission and local model training.\n\n2) Participating device set refinement (Lines 5-8): With the above calculations, the edge controllers can evaluate whether the selected IGWs are able to complete the FL while satisfying the strict requirements. Only when the IGWs can satisfy the above requirement, the system would allow these IGWs to participate in the FL. Thus, a refined set of participating IGWs N m , \u2200m \u2208 M can be obtained to optimize FL.\n\nWith this subroutine, the on-device energy consumption and time budget can be effectively reduced to accelerate convergence in the distributed and resource-limited IIoT networks.\n\n\nD. Computational Complexity Analysis\n\nAs described before, the MASAC algorithm contains M edge agents. Each agent is endowed with one actor network, four critic networks, and one \u03b1 m weight networks. Each of them is represented by an FCNN, as shown in Fig. 3. Generally, the computational complexity of an FCNN is [37], where x \u2208 [0, L] denotes the layer index and I x represents the neuron number of hidden layers. According to the different input-output structures, there are three cases for the above networks. Specifically, 1) For actor networks, I 1 = 2|N m |, \u2200m \u2208 M is the dimension of the corresponding individual environment states, and I L = 4|N m | is the decision dimension of edge agent m;\nK = O( x (2I x \u2212 1)I x+1 )\n2) For critic networks, I 1 = (2 + 4) m |N m | is the dimension of total states and decisions. Here, the total states are composed of all the individual states observed by the agents of dimension 2 m |N m |. The decisions include device selection variable, and uplink/downlink spectrum and computing resource allocation variables with dimension 4 m |N m |. Note that the output of the critic network is an estimated Qvalue with dimension 1, and thus I L = 1;\n\n3) For \u03b1 m weight networks, the network scale is very small, and thus the computational complexity can be neglected.\n\nIn summary, there are 5 \u00d7 M FCNNs should be considered in the MASAC, thus the total computational complexity of the algorithm is 5 \u00d7 M \u00d7 K.\n\nMoreover, the proposed RoF scheme can reduce signaling overhead. During the FL operation, the central controller deployed at the cloud server collects the network state information from IGWs in each epoch. In the single-agent RoF scheme, the state information has to be sent to the central controller. In the RoF scheme, the state information only needs to be sent to the edge controllers that are deployed at edge servers. That is, compared with the single-agent RoF scheme, the RoF scheme can effectively reduce the signaling overhead of reporting all the state information to the cloud server.\n\n\nVII. PERFORMANCE EVALUATION\n\nIn this section, simulations are conducted to evaluate the proposed RoF scheme for optimizing FL in the distributed IIoT networks.\n\n\nA. Experimental Setup\n\nWe consider three smart factories operating in different cities, respectively, and 30 IGWs are distributed in each factory that is covered by a BS unless specified. The channel gain in IIoT networks is modeled by P L(dB) = \u2212128.1 \u2212 37.6log 10 (l), where l denotes the transmission distance between the BS and the covered IGWs [38]. We adopt Raspberry Pi 4B as an IGW [39]. The effective capacitance is set to  be 2\u00d710 \u221228 , and the number of CPU cycles to execute one training sample is set to be 20 cycles/bit [28].  Table I. For the FL, we design a convolutional neural network (CNN)-based model [41], which includes two 5\u00d75 convolutional layers, two fully-connected layers, and a softmax output layer (61,366 total trainable model parameters), to perform classification tasks (i.e., fault diagnostic). In addition, the learning rate and batch size of the CNN model are set to be 2.5 \u00d7 10 \u22123 and 64, respectively. The regularization coefficient is set to be 5\u00d710 \u22125 . In this simulation, the CNN is trained to perform fault diagnostic task, and thus the loss function can be expressed as\nf (x d , y d ; w) = Z z=0 p(y d = z)E x|y d =z [log\u0177 d ],(31)\nwhere Z is the number of health condition classes, y d is the true label, and\u0177 d is the predicted label [21].\n\nThe data size of model parameters S g is 240 KB. In addition, the CNN model is trained on the AWE dataset, 2 which is a vibration-based industrial sensor dataset contains 2 Online available. https://github.com/Intelligent-AWE/DeepHealth. six health condition classes of industrial facilities [42]. In our simulations, the number of data samples in the AWE dataset is 23,436, and the ratio between the training set and evaluating set is 9:1. Each sample contains 1,024 sampling points. Fig. 4 shows two raw samples of the AWE dataset, which are collected from the \"Normal\" and \"Fault-1\" health conditions, respectively. In the considered scenario, FL is operated in a non-IID setting, in which each IGW only holds a few classes of samples. To achieve this, we sort the samples by the class label, partition them into 180 blocks with the same size, and assign 2 blocks to each IGW [10].\n\nTo evaluate the performance of the proposed scheme, the following benchmarks are compared. FL epochs. In particular, lower FL evaluating loss and higher accuracy can be reached if the cloud aggregation frequency is set properly. It can be seen that the lowest evaluating loss and the highest accuracy are respectively obtained when the cloud aggregation frequency t c is set to be 10. Moreover, due to the non-IID setting and the limited on-device energy, the FL may not benefit from a large cloud aggregation frequency. This is because on-device energy might be exhausted before cloud aggregation, such that the model parameters trained on these IGWs cannot be globally aggregated at the cloud server, which slows down global model convergence.\n\nAs shown in Fig. 6, the convergence performance of the CNN-based model in terms of learning rates is evaluated. It can be seen that the lowest evaluating loss and the highest accuracy are respectively achieved when the learning rate is set to be 2.5 \u00d7 10 \u22123 . The result indicates that a larger learning rate may not contribute to achieve a better performance. Also, a smaller value of learning rate can lead to a stable FL process but is not good for achieving a lower loss, and thus may consume more spectrum and computing resources to achieve convergence.\n\nTo demonstrate the scalability of the proposed solution, we evaluates the performance of the RoF scheme in terms of different numbers of IGWs dispersed in each factory. As shown in Fig. 7, the CNN model can achieve a low evaluating loss and a high accuracy within 300 FL epochs. In particular, the achieved evaluating loss and accuracy do not degrade much as the increase of the number of IGWs. The result indicates that the presented RoF scheme can be extended to dense networks with a large number of IGWs. However, with the increasing of the number of IGWs, the decision dimension of the RoF scheme increases, which increases the difficulty for the optimal decision making.  2) Impact of downlink spectrum resources: To demonstrate the effectiveness of the learning-based RoF scheme, we evaluate the average accuracy and evaluating loss of different schemes with respect to different amounts of downlink spectrum resources, as shown in Fig. 8. As expected, the RoF achieves the highest average accuracy and lowest average evaluating loss among all the schemes. Specifically, the accuracies incurred by the learning-based schemes (i.e., RoF, SARoF, and RoF w.o.) increase with the growth of the amounts of downlink spectrum resources. On the contrary, the evaluating loss achieved by the learning-based schemes decreases with   the decrease of the amounts of downlink spectrum resources. The accuracy achieved by the RoF is approximately 1.8% higher than that by the SARoF scheme. The result indicates that the RoF can achieve a higher resource utilization and is more scalable compared with other schemes. In addition, the RoF can increase the average accuracy by 14.6%, as compared with the RoF without edge aggregation. This is because two-layer (i.e., device-cloud) FL architecture leads to higher parameter transmission delay, which prevents the cloud server from aggregating more model parameters with the FL strict epoch delay constraint and thus against FL performance improvement.\n\n3) Impact of computing capabilities: As shown in Fig. 9, the average accuracy of different algorithms in terms of ondevice computing capabilities is evaluated. The result shows that all learning-based schemes, i.e., RoF, SARoF, MADDPG, and RoF w.o., can achieve higher accuracy than that of RPC benchmark. This is because that learning-based schemes make the FL models converge to a high accuracy level via judicious computing resource allocation. In addition, with the increase of the available computing capabilities, the average accuracies achieved by the RoF, SARoF, MADDPG, and RoF w.o. schemes increase. The underlying reason is that the delay of local model training can be greatly reduced with more available computing resources, thereby ensuring the FL can be completed within strict delay requirements. Particularly, the proposed multi-agent empowered RoF scheme can achieve the highest average accuracy than the benchmarks. When the amount of computing capability is 1.5 GHz, the RoF can increase the average accuracy by 1.81%, 5.47%, and 14.56% compared with the SARoF, MADDPG, and RoF w.o. benchmarks, respectively. 4) Impact of uplink spectrum resources: Efficient uplink spectrum resource utilization ensures the parameter aggregation can be completed within the strict FL epoch delay requirement. As shown in Fig. 10, the impact of uplink spectrum resources on the aggregated model number of different schemes is evaluated. Intuitively, more IGWs' model parameters can be aggregated by the edge or cloud servers when there are more available uplink spectrum resources. More importantly, the RoF achieves the maximum average aggregated model number among all the schemes, implying that more aggregated model parameters can be utilized to the optimal global FL model. In particular, when the amount of uplink spectrum resource is 20 MHz, the RoF increases the average number of aggregated model by 30.8% compared with the SARoF scheme. This is because the RoF can better allocate the uplink spectrum resources, thereby enabling the parameter uploading to be completed with an acceptable transmission delay. With more model parameters aggregated from the distributed IGWs, the FL models can achieve a higher accuracy level, as the red curve shown in Fig. 8. The result indicates that implementing FL over more usable data can effectively optimize the FL performance. 5) Energy consumption and time analysis: As shown in Fig. 11, the average energy consumption and time of different algorithms in terms of uplink spectrum resource are presented. The RoF can effectively reduce the average energy consumption and time as compared to the SARoF scheme. This is because the proposed RoF scheme can better utilize uplink spectrum resources to enable FL models to be converged within energy consumption and time requirements in a decentralized resource management manner. In addition, both the energy consumption and time decrease with the increase of the amounts of available spectrum resource. Specifically, when the amount of uplink spectrum resource is 20 MHz, the RoF reduces the average energy consumption and time within the FL process by approximately 14.1% and 4.6%, respectively. The reason is that sufficient spectrum resource enables the model parameters to be aggregated at the edge or cloud with a shorter parameter uploading time, thereby consuming less on-device energy for parameter uploading. In Fig. 12, the cumulative distribution functions of the energy consumption and time of different schemes are compared, which shows that the RoF scheme can better guarantee the energy consumption and time requirements. 6) Comprehensive performance analysis: As listed in Table II, a comprehensive comparison is presented to demonstrate the performance of different schemes. Overall, three important observations can be obtained from the simulation results. Firstly, the RoF achieves the optimal performance over all the metrics given multiple constrained resources. Specifically, the average accuracy of the RoF is about 84.5%, which is higher than that of the SARoF (83.1%), RoF w.o. (73.8%), and RPC (17.0%). The average loss of the RoF is about 0.35, which is lower than that of the SARoF (0.42), RoF w.o. (0.62), and RPC (1.81). Secondly, aiming at achieving fast FL convergence, the RoF is inclined to select more IGWs to participate in the FL. However, it is interesting to note that, the ratio between the number of aggregated model and the number of participating IGW for RoF is 61.4%, which is lower than that of SARoF (69.3%). Constrained computing and spectrum resources have significant effects on the local model training and parameter transmission, which should be better utilized to support the long-term FL process, thereby providing reliable distributed learning service. Thirdly, although more IGWs are selected by the RoF during the FL process, they consumes less energy and time than the SARoF. The result implies that the RoF can allocate the cherished resources in a more judicious way, which endows more persistence to the system. The above simulation results indicate that the considered trade-offs should be better balanced to facilitate efficient FL in the distributed IIoT networks.\n\nBased on the above observations, we can claim that the proposed RoF scheme can effectively improve the FL performance in the distributed and resource-limited IIoT networks.\n\n\nVIII. CONCLUSION\n\nIn this paper, we have investigated the FL performance optimization problem in the distributed IIoT networks. To solve the problem, we have proposed the RoF scheme, based on deep multi-agent reinforcement learning, to make the optimal device selection and resource allocation decisions in an online manner. Simulation results based on real-world dataset demonstrate that the proposed scheme can achieve a high FL accuracy, while satisfying on-device energy consumption requirements. The RoF scheme operates in a distributed manner, which can effectively reduce system signaling overhead of collecting network information, especially in resourceconstrained networks. For the future work, we will study a data importance-aware device selection scheme to optimize FL in a large-scale IIoT network.\n\nF\n(w) = Fe(wm); 17\n\n\u2022\nLocal Model Training (Lines 8-12): Once the global model parameters w m are received, the participating IGWs will respectively perform model training on their local datasets for one FL epoch. Here, the parameters are updated via a stochastic gradient descend method, i.e., w m,n = w m \u2212 \u03bb\u2207f (x d , y d ; w m,n ), \u2200d \u2208 D T m,n , (1) where \u03bb is the learning rate, w m,n is the updated parameters of each participating IGW, and f (x d , y d ; w m,n ) is the loss on each training sample of the IGWs. \u2022 Edge Aggregation (Lines 13-16): When the local update is completed, each edge server synchronously aggregates model parameters from the participating IGWs at each FL epoch, i.e., mod(t, t c ) = 0, and updates its global model parameter via the FedAvg algorithm, namely,\n\n\u2022\nCloud Aggregation (Lines 17-20): The cloud server aggregates model parameters from edge servers every t c FL epochs, i.e., when mod(t, t c ) = 0, and updates the global model via the FedAvg algorithm, i.e.,\n\nFig. 2 .Fig. 3 .\n23The architecture of the proposed MASAC-based resource management algorithm. Actor and critic network structures of MASAC.\n\n8\nExecute decision am,t, and obtain reward rm,t by Algorithm 1 and next state s m,t ; 9Store {sm,t, am,t, rm,t, s m,t } into Kr and replace the oldest experiences in Kr;10 Parameter updating11 Randomly sample a minibatch of K b experiences {s k m,t , a k m,t , r k m,t , s k m,t } from Kr;12 Set the target Q-value y k m,t by Eq.(24);13 Update \u03c6m,1 and \u03c6m,2 by minimizing the loss function in Eq. (23);14 Update \u03b8m via the policy gradient in Eq.(28);15 Adjust \u03b1m via the gradient obtained by Eq.(29);16 Update target networks for the agents by Eq.(30);17 Obtain cooperative decision at = {am,t} \u2200m\u2208M .\n\nFig. 4 .\n4Two classes of samples in AWE dataset. These two samples are collected from rolling bearings of industrial facilities. \"Normal\" means the bearing stays in a health condition, and \"Fault-1\" means the bearing operates under a fault condition with 3 mm severity degree.\n\nFig. 5 .\n5FL evaluating loss and accuracy with respect to different cloud aggregation frequencies.\n\n\u2022Fig. 6 .Fig. 7 .\n67Single-Agent RoF (SARoF): In this scheme, we assume a central controller is deployed at the cloud server and acts as the agent to centrally select participating IGWs and manage the spectrum and computing resources for the FL. In the SARoF, the hidden layers of the actor network are with [256, 512, 384, 360] neurons, and the critic networks have the same structure as that of the RoF.\u2022 The RoF without Edge Aggregation (RoF w.o.): This scheme is designed for a two-layer FL aggregation architecture, in which the FL model parameters are only aggregated by the cloud server in each FL epoch. \u2022 Multi-Agent DDPG (MADDPG): In this scheme, the hidden layers of the actor networks and the critic networks are with [180, 300] and [450, 100] neurons, respectively. Multiple learning agents are deployed at the edge servers to cooperatively make the JDSRA decisions via a DDPG algorithm. \u2022 Random Probabilistic Configuration (RPC): In this scheme, the random policy is adopted, in which the central controller randomly selects the IGWs and allocates the spectrum and computing resources. All available actions are selected with an equal probability.B. Evaluation of RoF Scheme 1) Convergence performance: We evaluate the convergence performance of the three-layer collaborative FL for the CNNbased fault diagnostic model. During the FL operation, the cloud aggregation is performed every t c FL epochs. As shown inFig. 5, the CNN model trained on the AWE dataset can achieve a low evaluating loss and a high accuracy over the considered three-layer collaborative architecture FL evaluating loss and accuracy with respect to different learning rates. FL evaluating loss and accuracy with respect to different device (i.e., the IGW) number.\n\nFig. 11 .\n11Average energy consumption and time of different schemes with respect to uplink spectrum resource.\n\nFig. 12 .\n12Cumulative distribution functions of energy consumption and time during the FL process.\n\n\nFig. 1. The three-layer collaborative FL architecture for distributed IIoT networks.Edge Server 1 \n\nEdge Server m \n\nEdge Server M \n\nSmart Factory (City 1) \n\nSmart Factory (City m) \n\nSmart Factory (City M) \n\nIGW 1 \n\nIGW 2 \n\nIGW N1 \n\nIGW 1 \n\nIGW 2 \n\nIGW Nm \n\nIGW 1 \n\n\n\nAlgorithm 1: Three-Layer FL Algorithm Task initialization 2 Cloud: Initialize global DNN model parameter w0;1 \n\n3 \n\n\n\n\nAlgorithm 2: MASAC-Based Algorithm for the JDSRA Input: Remaining energy and time of all the IGWs Output: JDSRA decision at ={o, \u03be \u2193 , \u03be \u2191 , \u03b7}; 1 Initialize actor, evaluation critic, and target critic networks with weights \u03b8m, \u03c6m,1, \u03c6m,2, \u03c6 m,1 , and \u03c6 m,2 for each agent m \u2208 M, and experience replay pool Kr; 2 for each episode do Receive initial state s1 = {sm,1} \u2200m\u2208M ;{T r \nm,n,t , E r \nm,n,t } \u2200m\u2208M,n\u2208Nm ; \n3 \n\n4 \n\nfor FL epoch t \u2208 R do \n\n5 \n\nfor each agent do \n\n6 \n\nExperience sampling \n\n7 \n\n\n\nTable I SIMULATION\nIPARAMETERS[35],[40].Network Para. \nValue \nLearning Para. \nValue \n\npm,n, pm \n30, 40 dBm \nOptimizer \nAdam \n\n\u03c3 2 \n-104 dBm \nEpisode number \n300 \n\nl \n150 m \nEpoch number \n300 \n\nB \u2191 , B \u2193 \n20, 40 MHz \nKr \n10,000 \n\nf \n1.5 GHz \nK b \n64 \n\nR c \n1 Mbps \n\u03b3 \n0.9 \n\nE max \n\nm,n \n\n[4500, 5000] Joule \n3 \u00d7 10 \u22124 \n\nT max \n\nm,n \n\n[600, 800] s \n\u03c4 \n0.01 \n\nT max \n5 s \nU \n10 \n\n\n\n\nFig. 8. Average accuracy and evaluating loss comparison among different schemes with respect to downlink spectrum resource.Fig. 9. Average accuracy comparison among different schemes with respect to computing capability.10 \n15 \n20 \n25 \n30 \n35 \n40 \n\nDownlink Spectrum Resource (MHz) \n\n0.2 \n\n0.3 \n\n0.4 \n\n0.5 \n\n0.6 \n\n0.7 \n\n0.8 \n\nAverage Accuracy \n\n66.5% \n\n14.6% \n\nRoF \nSARoF \nRoF w.o. \nRPC \n\n(a) Accuracy \n\n5 \n10 \n15 \n20 \n25 \n30 \n35 \n40 \n\nDownlink Spectrum Resource (MHz) \n\n0.4 \n\n0.6 \n\n0.8 \n\n1.0 \n\n1.2 \n\n1.4 \n\n1.6 \n\n1.8 \n\nAverage Evaluating Loss \n\nRoF \nSARoF \nRoF w.o. \nRPC \n\n(b) Loss \n\n0.5 \n0.7 \n0.9 \n1.1 \n1.3 \n1.5 \n\nComputing Capability (GHz) \n\n0.2 \n\n0.3 \n\n0.4 \n\n0.5 \n\n0.6 \n\n0.7 \n\n0.8 \n\nAverage Accuracy \n\nRoF \nSARoF \nMADDPG \nRoF w.o. \nRPC \n\n\n\n\nFig. 10. Average aggregated model number comparison among different schemes with respect to uplink spectrum resource.Average Aggregated Model Number \n\n156.4% \n\n146.4% \n\n152.4% \n152.1% \n147.7% \n18.3% \n\n19.3% \n\n30.6% \n28.4% \n30.8% \n\nRoF \nSARoF \nRPC \n\n4 \n8 \n12 \n16 \n20 \n\nUplink Spectrum Resource (MHz) \n\n2500 \n\n2750 \n\n3000 \n\n3250 \n\n3500 \n\n3750 \n\n4000 \n\nAverage Energy Consumption (Joule) \n\n12.7% \n\n10.1% \n\n8.1% \n\n8.6% \n\n14.1% \n\n380 \n\n400 \n\n420 \n\n440 \n\n460 \n\n480 \n\n500 \n\nAverage Time (s) \n\n5.6% \n\n5.4% \n\n2.8% \n4.5% \n4.6% \n\nRoF \nSARoF \nRoF \nSARoF \n\n\n\nTable II PERFORMANCE\nIICOMPARISON OF DIFFERENT SCHEMES.Average Accuracy \nAverage Loss \nAverage Participating Num. \nAverage Aggregated Num. \nAverage Energy (Joule) \nAverage Time (s) \n\nRoF \n84.5% \n0.3451 \n53.8 \n33.1 \n2550.3 \n413.8 \n\nSARoF \n83.1% \n0.4209 \n36.5 \n25.3 \n2967.2 \n433.5 \n\nRoF w.o. \n73.8% \n0.62 \n36.2 \n10.9 \n2539.3 \n190.5 \n\nRPC \n17.0% \n1.81 \n44.8 \n10.2 \n4800 \n11.5 \n\n\nThe proposed three-layer architecture can also work in the case that not all the IGWs are able to connect the edge server. In such case, the locally trained model parameters can be directly transmitted to the cloud server for the global aggregation.\n\nSpectrum and computing resource management for federated learning in distributed industrial IoT. W Zhang, D Yang, W Wu, H Peng, H Zhang, X Shen, Proc. IEEE ICC Workshop. IEEE ICC WorkshopMontreal, Canada2021W. Zhang, D. Yang, W. Wu, H. Peng, H. Zhang, and X. Shen, \"Spectrum and computing resource management for federated learning in distributed industrial IoT,\" in Proc. IEEE ICC Workshop, Montreal, Canada, 2021.\n\nArtificial neural networks-based machine learning for wireless networks: A tutorial. M Chen, U Challita, W Saad, C Yin, M Debbah, IEEE Commun. Surveys Tuts. 214M. Chen, U. Challita, W. Saad, C. Yin, and M. Debbah, \"Artificial neural networks-based machine learning for wireless networks: A tutorial,\" IEEE Commun. Surveys Tuts., vol. 21, no. 4, pp. 3039-3071, Jul. 2019.\n\nIndustrial Internet of things: Challenges, opportunities, and directions. E Sisinni, A Saifullah, S Han, U Jennehag, M Gidlund, IEEE Trans. Ind. Informat. 1411E. Sisinni, A. Saifullah, S. Han, U. Jennehag, and M. Gidlund, \"Indus- trial Internet of things: Challenges, opportunities, and directions,\" IEEE Trans. Ind. Informat., vol. 14, no. 11, pp. 4724-4734, Nov. 2018.\n\nA vision of 6G wireless systems: Applications, trends, technologies, and open research problems. W Saad, M Bennis, M Chen, IEEE Netw. 343W. Saad, M. Bennis, and M. Chen, \"A vision of 6G wireless systems: Applications, trends, technologies, and open research problems,\" IEEE Netw., vol. 34, no. 3, pp. 134-142, Oct. 2019.\n\nData management for future wireless networks: Architecture, privacy preservation, and regulation. X Shen, C Huang, D Liu, L Xue, W Zhuang, R Sun, B Ying, IEEE Netw. 351X. Shen, C. Huang, D. Liu, L. Xue, W. Zhuang, R. Sun, and B. Ying, \"Data management for future wireless networks: Architecture, privacy preservation, and regulation,\" IEEE Netw., vol. 35, no. 1, pp. 8-15, Feb. 2021.\n\nMachine learning in the air. D G\u00fcnd\u00fcz, P Kerret, N Sidiropoulos, D Gesbert, C Murthy, M Schaar, IEEE J. Sel. Areas Commun. 3710D. G\u00fcnd\u00fcz, P. Kerret, N. Sidiropoulos, D. Gesbert, C. Murthy, and M. Schaar, \"Machine learning in the air,\" IEEE J. Sel. Areas Commun., vol. 37, no. 10, pp. 2184-2199, Oct. 2019.\n\nCommunication, computing, and learning on the edge. K Huang, G Zhu, C You, J Zhang, Y Du, D Liu, Proc. ICCS. ICCSChengdu, ChinaK. Huang, G. Zhu, C. You, J. Zhang, Y. Du, and D. Liu, \"Communi- cation, computing, and learning on the edge,\" in Proc. ICCS, Chengdu, China, 2018.\n\nA joint learning and communications framework for federated learning over wireless networks. M Chen, Z Yang, W Saad, C Yin, H Poor, S Cui, IEEE Trans. Wireless Commun. 201M. Chen, Z. Yang, W. Saad, C. Yin, H. Poor, and S. Cui, \"A joint learning and communications framework for federated learning over wireless networks,\" IEEE Trans. Wireless Commun., vol. 20, no. 1, pp. 269-283, 2020.\n\nFederated learning and control at the wireless network edge. M Bennis, GetMobile: Mobile Comput. Commun. 243M. Bennis, \"Federated learning and control at the wireless network edge,\" GetMobile: Mobile Comput. Commun., vol. 24, no. 3, pp. 9-13, Jan. 2021.\n\nCommunication-efficient learning of deep networks from decentralized data. B Mcmahan, E Moore, D Ramage, S Hampson, B Arcas, Proc. PMLR AIS. PMLR AISFlorida, USAB. McMahan, E. Moore, D. Ramage, S. Hampson, and B. Arcas, \"Communication-efficient learning of deep networks from decentralized data,\" in Proc. PMLR AIS, Florida, USA, pp. 1273-1282, 2017.\n\nConvergence time minimization of federated learning over wireless networks. M Chen, H Poor, W Saad, S Cui, Proc. IEEE ICC. IEEE ICCDublin, IrelandM. Chen, H. Poor, W. Saad, and S. Cui, \"Convergence time minimization of federated learning over wireless networks,\" in Proc. IEEE ICC, Dublin, Ireland, 2020.\n\nJoint client scheduling and resource allocation under channel uncertainty in federated learning. M Wadu, S Samarakoon, M Bennis, 10.1109/TCOMM.2021.3088528IEEE Trans. Commun., Early Access. M. Wadu, S. Samarakoon, and M. Bennis, \"Joint client schedul- ing and resource allocation under channel uncertainty in feder- ated learning,\" IEEE Trans. Commun., Early Access, 2021, doi: 10.1109/TCOMM.2021.3088528.\n\nAdaptive federated learning in resource constrained edge computing systems. S Wang, T Tuor, T Salonidis, K Leung, C Makaya, T He, K Chan, IEEE J. Sel. Areas Commun. 376S. Wang, T. Tuor, T. Salonidis, K. Leung, C. Makaya, T. He, and K. Chan, \"Adaptive federated learning in resource constrained edge computing systems,\" IEEE J. Sel. Areas Commun., vol. 37, no. 6, pp. 1205-1221, June 2019.\n\nA crowdsourcing framework for on-device federated learning. S Pandey, N Tran, M Bennis, Y Tun, A Manzoor, C Hong, IEEE Trans. Wireless Commun. 195S. Pandey, N. Tran, M. Bennis, Y. Tun, A. Manzoor, and C. Hong, \"A crowdsourcing framework for on-device federated learning,\" IEEE Trans. Wireless Commun., vol. 19, no. 5, pp. 3241-3256, May 2020.\n\nReliable federated learning for mobile networks. J Kang, Z Xiong, D Niyato, Y Zou, Y Zhang, M Guizani, IEEE Wireless Commun. 272J. Kang, Z. Xiong, D. Niyato, Y. Zou, Y. Zhang, and M. Guizani, \"Reli- able federated learning for mobile networks,\" IEEE Wireless Commun., vol. 27, no. 2, pp. 72-80, Apr. 2020.\n\nFederated learning in mobile edge networks: A comprehensive survey. W Lim, N Luong, D Hoang, Y Jiao, Y Liang, Q Yang, D Niyato, C Miao, IEEE Commun. Surveys Tuts. 223W. Lim, N. Luong, D. Hoang, Y. Jiao, Y. Liang, Q. Yang, D. Niyato, and C. Miao, \"Federated learning in mobile edge networks: A comprehensive survey,\" IEEE Commun. Surveys Tuts., vol. 22, no. 3, pp. 2031-2063, Apr. 2020.\n\nMulti-tenant cross-slice resource orchestration: A deep reinforcement learning approach. X Chen, Z Zhao, C Wu, M Bennis, H Liu, Y Ji, H Zhang, IEEE J. Sel. Areas Commun. 3710X. Chen, Z. Zhao, C. Wu, M. Bennis, H. Liu, Y. Ji, and H. Zhang, \"Multi-tenant cross-slice resource orchestration: A deep reinforcement learning approach,\" IEEE J. Sel. Areas Commun., vol. 37, no. 10, pp. 2377-2392, Oct. 2019.\n\nSeek common while shelving differences: Orchestrating deep neural networks for edge service provisioning. L Chen, J Xu, IEEE J. Sel. Areas Commun. 391L. Chen and J. Xu, \"Seek common while shelving differences: Orches- trating deep neural networks for edge service provisioning,\" IEEE J. Sel. Areas Commun., vol. 39, no. 1, pp. 251-264, Jan. 2021.\n\nDeep reinforcement learning based resource management for multi-access edge computing in vehicular networks. H Peng, X Shen, IEEE Trans. Netw. Sci. Eng. 74H. Peng and X. Shen, \"Deep reinforcement learning based resource management for multi-access edge computing in vehicular networks,\" IEEE Trans. Netw. Sci. Eng., vol. 7, no. 4, pp. 2416-2428, Mar. 2020.\n\nFederated learning via overthe-air computation. K Yang, T Jiang, Y Shi, Z Ding, IEEE Trans. Wireless Commun. 193K. Yang, T. Jiang, Y. Shi, and Z. Ding, \"Federated learning via over- the-air computation,\" IEEE Trans. Wireless Commun., vol. 19, no. 3, pp. 2022-2035, Mar. 2020.\n\nOptimizing federated learning on non-IID data with reinforcement learning. H Wang, Z Kaplan, D Niu, B Li, Proc. IEEE INFOCOM. IEEE INFOCOMBeijing, ChinaH. Wang, Z. Kaplan, D. Niu, and B. Li, \"Optimizing federated learning on non-IID data with reinforcement learning,\" in Proc. IEEE INFOCOM, Beijing, China, pp. 1698-1707, 2020.\n\nEnergy-efficient radio resource allocation for federated edge learning. Q Zeng, Y Du, K Huang, K Leung, Proc. IEEE ICC. IEEE ICCDublin, IrelandQ. Zeng, Y. Du, K. Huang, and K. Leung, \"Energy-efficient radio resource allocation for federated edge learning,\" in Proc. IEEE ICC, Dublin, Ireland, 2020.\n\nSpectrum sharing in vehicular networks based on multi-agent reinforcement learning. L Liang, H Ye, G Li, IEEE J. Sel. Areas Commun. 3710L. Liang, H. Ye, and G. Li, \"Spectrum sharing in vehicular networks based on multi-agent reinforcement learning,\" IEEE J. Sel. Areas Com- mun., vol. 37, no. 10, pp. 2282-2292, Oct. 2019.\n\nDynamic RAN slicing for service-oriented vehicular networks via constrained learning. W Wu, N Chen, C Zhou, M Li, X Shen, W Zhuang, X Li, IEEE J. Sel. Areas Commun. 397W. Wu, N. Chen, C. Zhou, M. Li, X. Shen, W. Zhuang, and X. Li, \"Dynamic RAN slicing for service-oriented vehicular networks via constrained learning,\" IEEE J. Sel. Areas Commun., vol. 39, no. 7, pp. 2076-2089, Jul. 2021.\n\nDeep reinforcement learning-based latency minimization in edge intelligence over vehicular networks. N Zhao, H Wu, F Yu, L Wang, W Zhang, V Leung, 10.1109/JIOT.2021.3078480IEEE Internet of Things J. 2021Early AccessN. Zhao, H. Wu, F. Yu, L. Wang, W. Zhang, and V. Leung, \"Deep reinforcement learning-based latency minimization in edge intelligence over vehicular networks,\" IEEE Internet of Things J., Early Access, 2021, doi: 10.1109/JIOT.2021.3078480.\n\nOn-board deep Q-network for UAV-assisted online power transfer and data collection. K Li, W Ni, E Tovar, A Jamalipour, IEEE Trans. Veh. Technol. 6812K. Li, W. Ni, E. Tovar, and A. Jamalipour, \"On-board deep Q-network for UAV-assisted online power transfer and data collection,\" IEEE Trans. Veh. Technol., vol. 68, no. 12, pp. 12215-12226, Dec. 2019.\n\nEnergy-efficient resource allocation in cognitive radio networks under cooperative multi-agent model-free reinforcement learning schemes. A Kaur, K Kumar, IEEE Trans. Netw. Service Manag. 173A. Kaur and K. Kumar, \"Energy-efficient resource allocation in cognitive radio networks under cooperative multi-agent model-free reinforcement learning schemes,\" IEEE Trans. Netw. Service Manag., vol. 17, no. 3, pp. 1337-1348, Sept. 2020.\n\nClient-edge-cloud hierarchical federated learning. L Liu, J Zhang, S Song, K Letaief, Proc. IEEE ICC. IEEE ICCDublin, IrelandL. Liu, J. Zhang, S. Song, and K. Letaief, \"Client-edge-cloud hierarchi- cal federated learning,\" in Proc. IEEE ICC, Dublin, Ireland, 2020.\n\nFast mmWave beam alignment via correlated bandit learning. W Wu, N Cheng, N Zhang, P Yang, W Zhuang, X Shen, IEEE Trans. Wireless Commun. 1812W. Wu, N. Cheng, N. Zhang, P. Yang, W. Zhuang, and X. Shen, \"Fast mmWave beam alignment via correlated bandit learning,\" IEEE Trans. Wireless Commun., vol. 18, no. 12, pp. 5894-5908, Dec. 2019.\n\nSignal processing for 5G: algorithms and implementations. F Luo, C Zhang, John Wiley & SonsF. Luo and C. Zhang, Signal processing for 5G: algorithms and implementations. John Wiley & Sons, 2016.\n\nLarge-scale machine learning with stochastic gradient descent. L Bottou, Proc. COMPSTAT. COMPSTATParis, FranceL. Bottou, \"Large-scale machine learning with stochastic gradient de- scent,\" in Proc. COMPSTAT, Paris, France, pp. 177-186, 2010.\n\nDynamical resource allocation in edge for trustable Internet-of-Things systems: A reinforcement learning method. S Deng, Z Xiang, P Zhao, J Taheri, H Gao, J Yin, A Zomaya, IEEE Trans. Ind. Informat. 169S. Deng, Z. Xiang, P. Zhao, J. Taheri, H. Gao, J. Yin, and A. Zomaya, \"Dynamical resource allocation in edge for trustable Internet-of-Things systems: A reinforcement learning method,\" IEEE Trans. Ind. Informat., vol. 16, no. 9, pp. 6103-6113, Sept. 2020.\n\nAI-assisted network-slicing based next-generation wireless networks. X Shen, J Gao, W Wu, K Lyu, M Li, W Zhuang, X Li, J Rao, IEEE Open J. Veh. Technol. 11X. Shen, J. Gao, W. Wu, K. Lyu, M. Li, W. Zhuang, X. Li, and J. Rao, \"AI-assisted network-slicing based next-generation wireless networks,\" IEEE Open J. Veh. Technol., vol. 1, no. 1, pp. 45-66, Jan. 2020.\n\nMulti-agent reinforcement learningbased resource allocation for UAV networks. J Cui, Y Liu, A Nallanathan, IEEE Trans. Wireless Commun. 192J. Cui, Y. Liu, and A. Nallanathan, \"Multi-agent reinforcement learning- based resource allocation for UAV networks,\" IEEE Trans. Wireless Commun., vol. 19, no. 2, pp. 729-743, Feb. 2019.\n\nSoft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor. T Haarnoja, A Zhou, P Abbeel, S Levine, Proc. ICML. ICMLStockholm, SwedenT. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, \"Soft actor-critic: Off- policy maximum entropy deep reinforcement learning with a stochastic actor,\" in Proc. ICML, Stockholm, Sweden, 2018.\n\nMultiagent actor-critic for mixed cooperative-competitive environments. R Lowe, Y Wu, A Tamar, J Harb, P Abbeel, I Mordatch, Proc. NeurIPS, Long Beach. NeurIPS, Long BeachUnited StatesR. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel, and I. Mordatch, \"Multi- agent actor-critic for mixed cooperative-competitive environments,\" in Proc. NeurIPS, Long Beach, United States, 2017.\n\nOn the computational efficiency of training neural networks. R Livni, S Shalev, O Shamir, Proc. NeurIPS. NeurIPSMontr\u00e9al, Canada27R. Livni, S. Shalev, and O. Shamir, \"On the computational efficiency of training neural networks,\" in Proc. NeurIPS, Montr\u00e9al, Canada, vol. 27, pp. 855-863, 2014.\n\nRelay-assisted cross-channel gain estimation for spectrum sharing. G Zhao, B Huang, L Li, X Zhou, IEEE Trans. Commun. 643G. Zhao, B. Huang, L. Li, and X. Zhou, \"Relay-assisted cross-channel gain estimation for spectrum sharing,\" IEEE Trans. Commun., vol. 64, no. 3, pp. 973-986, Mar. 2016.\n\nMulti-agent reinforcement learning based resource management in MEC-and UAV-assisted vehicular networks. H Peng, X Shen, IEEE J. Sel. Areas Commun. 391H. Peng and X. Shen, \"Multi-agent reinforcement learning based re- source management in MEC-and UAV-assisted vehicular networks,\" IEEE J. Sel. Areas Commun., vol. 39, no. 1, pp. 5894-5908, Jan. 2021.\n\nGradient-based learning applied to document recognition. Y Lecun, L Bottou, Y Bengio, P Haffner, Proc. IEEE. IEEE86Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, \"Gradient-based learning applied to document recognition,\" Proc. IEEE, vol. 86, no. 11, pp. 2278- 2324, Nov. 1998.\n\nDeep-Health: A self-attention based method for instant intelligent predictive maintenance in industrial Internet of things. W Zhang, D Yang, Y Xu, X Huang, J Zhang, M Gidlund, IEEE Trans. Ind. Informat. 178W. Zhang, D. Yang, Y. Xu, X. Huang, J. Zhang, and M. Gidlund, \"Deep- Health: A self-attention based method for instant intelligent predictive maintenance in industrial Internet of things,\" IEEE Trans. Ind. Informat., vol. 17, no. 8, pp. 5461-5473, Mar, 2021.\n\nWeiting Zhang, S'20) is currently pursuing the Ph.D. degree in Communication and Information Systems with the Beijing Jiaotong University, Beijing. Department of Electrical and Computer Engineering, University of Waterloohe was a visiting Ph.D student with the. His research interests include mobile edge computing. industrial Internet of Things, and machine learning for wireless networksWeiting Zhang (S'20) is currently pursuing the Ph.D. degree in Communication and Information Systems with the Beijing Jiaotong University, Bei- jing, China. From Nov. 2019 to Nov. 2020, he was a visiting Ph.D student with the Department of Electrical and Computer Engineering, University of Waterloo, Canada. His research interests include mo- bile edge computing, industrial Internet of Things, and machine learning for wireless networks.\n\nHe has also received the Excellent Graduate Supervision Award in 2006 from the University of Waterloo and the Premier's Research Excellence Award (PREA) in 2003 from the Province of Ontario, Canada. He served as the Technical Program Committee Chair/Co-Chair for IEEE Globecom'16, IEEE Infocom'14, IEEE VTC'10 Fall, IEEE Globecom'07, and the Chair for the IEEE Communications Society Technical Committee on Wireless Communications. Dr. Shen is the President Elect of the IEEE Communications Society. He was the Vice President for Technical & Educational Activities. Award of Merit from the Federation of Chinese Canadian Professionals (Ontario) in 2019, James Evans Avant Garde Award in 2018 from the IEEE Vehicular Technology Society, Joseph LoCicero Award in 2015 and Education Award in 2017 from the IEEE Communications Society, and Technical Recognition Award from Wireless Communications Technical Committee (2019) and AHSN Technical Committee. Vice President for PublicationsIEEE Vehicular Technology Society and Communications Society. Dr. Shen received the R.A. Fessenden Award in 2019 from IEEE. Member-at-Large on the Board of Governors, Chair of the Distinguished Lecturer Selection Committee, Member of IEEE Fellow Selection Committee of the ComSocLecturer of the IEEE Vehicular Technology Society and Communications Society. Dr. Shen received the R.A. Fessenden Award in 2019 from IEEE, Canada, Award of Merit from the Federation of Chinese Canadian Professionals (Ontario) in 2019, James Evans Avant Garde Award in 2018 from the IEEE Vehicular Technology Society, Joseph LoCicero Award in 2015 and Education Award in 2017 from the IEEE Communications Society, and Technical Recog- nition Award from Wireless Communications Technical Committee (2019) and AHSN Technical Committee (2013). He has also received the Excellent Graduate Supervision Award in 2006 from the University of Waterloo and the Premier's Research Excellence Award (PREA) in 2003 from the Province of Ontario, Canada. He served as the Technical Program Committee Chair/Co- Chair for IEEE Globecom'16, IEEE Infocom'14, IEEE VTC'10 Fall, IEEE Globecom'07, and the Chair for the IEEE Communications Society Technical Committee on Wireless Communications. Dr. Shen is the President Elect of the IEEE Communications Society. He was the Vice President for Technical & Educational Activities, Vice President for Publications, Member-at-Large on the Board of Governors, Chair of the Distinguished Lecturer Selection Committee, Member of IEEE Fellow Selection Committee of the ComSoc.\n\nShen served as the Editor-in-Chief of the IEEE IoT. Dr, IET CommunicationsDr. Shen served as the Editor-in-Chief of the IEEE IoT JOURNAL, IEEE Network, and IET Communications.\n", "annotations": {"author": "[{\"end\":101,\"start\":87},{\"end\":124,\"start\":102},{\"end\":144,\"start\":125},{\"end\":169,\"start\":145},{\"end\":200,\"start\":170},{\"end\":226,\"start\":201},{\"end\":234,\"start\":227},{\"end\":260,\"start\":235}]", "publisher": null, "author_last_name": "[{\"end\":100,\"start\":95},{\"end\":123,\"start\":119},{\"end\":143,\"start\":141},{\"end\":168,\"start\":164},{\"end\":199,\"start\":194},{\"end\":225,\"start\":220},{\"end\":259,\"start\":255}]", "author_first_name": "[{\"end\":94,\"start\":87},{\"end\":118,\"start\":114},{\"end\":140,\"start\":137},{\"end\":163,\"start\":157},{\"end\":193,\"start\":189},{\"end\":219,\"start\":213},{\"end\":233,\"start\":227},{\"end\":254,\"start\":247}]", "author_affiliation": null, "title": "[{\"end\":84,\"start\":1},{\"end\":344,\"start\":261}]", "venue": "[{\"end\":394,\"start\":346}]", "abstract": "[{\"end\":2492,\"start\":719}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2497,\"start\":2494},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2633,\"start\":2630},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3260,\"start\":3257},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3376,\"start\":3373},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3381,\"start\":3378},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3496,\"start\":3493},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3855,\"start\":3851},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4209,\"start\":4205},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4302,\"start\":4298},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4308,\"start\":4304},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7224,\"start\":7220},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7230,\"start\":7226},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10114,\"start\":10110},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10415,\"start\":10411},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11009,\"start\":11005},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":11168,\"start\":11164},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11881,\"start\":11877},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":11989,\"start\":11985},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12158,\"start\":12154},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12308,\"start\":12304},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":12603,\"start\":12599},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":13033,\"start\":13030},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":13203,\"start\":13200},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":13768,\"start\":13764},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":14876,\"start\":14875},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":16062,\"start\":16061},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":16113,\"start\":16112},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":16150,\"start\":16149},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":16195,\"start\":16194},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":16231,\"start\":16229},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":16270,\"start\":16268},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":16313,\"start\":16311},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":16352,\"start\":16350},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":16390,\"start\":16388},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":16452,\"start\":16450},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":20403,\"start\":20400},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":20686,\"start\":20682},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":23241,\"start\":23237},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":25937,\"start\":25933},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":28525,\"start\":28521},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":30869,\"start\":30865},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":30875,\"start\":30871},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":31464,\"start\":31460},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":33758,\"start\":33755},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":35338,\"start\":35334},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":35769,\"start\":35765},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":38900,\"start\":38896},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":44966,\"start\":44965},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":44991,\"start\":44990},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":47696,\"start\":47692},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":49941,\"start\":49937},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":50126,\"start\":50122},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":50213,\"start\":50209},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":50871,\"start\":50867},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":51046,\"start\":51045},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":51170,\"start\":51166},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":51757,\"start\":51753},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":62592,\"start\":62590},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":62613,\"start\":62611},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":62712,\"start\":62710},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":62757,\"start\":62755},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":62825,\"start\":62823},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":62873,\"start\":62871},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":62923,\"start\":62921},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":62975,\"start\":62973},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":66290,\"start\":66286},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":66295,\"start\":66291}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":61295,\"start\":61276},{\"attributes\":{\"id\":\"fig_1\"},\"end\":62067,\"start\":61296},{\"attributes\":{\"id\":\"fig_2\"},\"end\":62277,\"start\":62068},{\"attributes\":{\"id\":\"fig_3\"},\"end\":62419,\"start\":62278},{\"attributes\":{\"id\":\"fig_4\"},\"end\":63022,\"start\":62420},{\"attributes\":{\"id\":\"fig_5\"},\"end\":63300,\"start\":63023},{\"attributes\":{\"id\":\"fig_6\"},\"end\":63400,\"start\":63301},{\"attributes\":{\"id\":\"fig_7\"},\"end\":65153,\"start\":63401},{\"attributes\":{\"id\":\"fig_10\"},\"end\":65265,\"start\":65154},{\"attributes\":{\"id\":\"fig_11\"},\"end\":65366,\"start\":65266},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":65633,\"start\":65367},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":65752,\"start\":65634},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":66254,\"start\":65753},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":66633,\"start\":66255},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":67377,\"start\":66634},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":67924,\"start\":67378},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":68301,\"start\":67925}]", "paragraph": "[{\"end\":3382,\"start\":2498},{\"end\":4210,\"start\":3384},{\"end\":4934,\"start\":4212},{\"end\":6164,\"start\":4936},{\"end\":7231,\"start\":6166},{\"end\":8665,\"start\":7233},{\"end\":9859,\"start\":8667},{\"end\":10920,\"start\":9861},{\"end\":11748,\"start\":10922},{\"end\":13003,\"start\":11750},{\"end\":13559,\"start\":13005},{\"end\":13963,\"start\":13610},{\"end\":15816,\"start\":13965},{\"end\":16475,\"start\":15818},{\"end\":16565,\"start\":16542},{\"end\":17144,\"start\":16567},{\"end\":17471,\"start\":17146},{\"end\":17612,\"start\":17473},{\"end\":18015,\"start\":17659},{\"end\":18057,\"start\":18052},{\"end\":18403,\"start\":18127},{\"end\":18467,\"start\":18436},{\"end\":19203,\"start\":18521},{\"end\":19366,\"start\":19224},{\"end\":19713,\"start\":19388},{\"end\":20404,\"start\":19715},{\"end\":21686,\"start\":20406},{\"end\":21825,\"start\":21688},{\"end\":21923,\"start\":21866},{\"end\":22469,\"start\":21925},{\"end\":22903,\"start\":22523},{\"end\":23458,\"start\":22940},{\"end\":24324,\"start\":23542},{\"end\":24478,\"start\":24366},{\"end\":24675,\"start\":24523},{\"end\":24948,\"start\":24736},{\"end\":25410,\"start\":24980},{\"end\":25783,\"start\":25467},{\"end\":25938,\"start\":25851},{\"end\":26041,\"start\":25940},{\"end\":26365,\"start\":26114},{\"end\":26867,\"start\":26391},{\"end\":27283,\"start\":26933},{\"end\":27485,\"start\":27329},{\"end\":27848,\"start\":27512},{\"end\":29708,\"start\":28305},{\"end\":29952,\"start\":29710},{\"end\":31309,\"start\":29954},{\"end\":32076,\"start\":31339},{\"end\":32250,\"start\":32078},{\"end\":32386,\"start\":32307},{\"end\":32736,\"start\":32388},{\"end\":32966,\"start\":32738},{\"end\":33270,\"start\":32968},{\"end\":34472,\"start\":33381},{\"end\":34719,\"start\":34474},{\"end\":34963,\"start\":34804},{\"end\":35414,\"start\":35014},{\"end\":35998,\"start\":35416},{\"end\":36346,\"start\":36038},{\"end\":36797,\"start\":36348},{\"end\":36993,\"start\":36888},{\"end\":37996,\"start\":36995},{\"end\":38526,\"start\":37998},{\"end\":38817,\"start\":38528},{\"end\":39978,\"start\":38819},{\"end\":40340,\"start\":39980},{\"end\":40482,\"start\":40369},{\"end\":40760,\"start\":40484},{\"end\":41697,\"start\":40762},{\"end\":42119,\"start\":41743},{\"end\":42286,\"start\":42121},{\"end\":42395,\"start\":42390},{\"end\":43238,\"start\":42530},{\"end\":43466,\"start\":43384},{\"end\":43576,\"start\":43468},{\"end\":43913,\"start\":43725},{\"end\":44313,\"start\":44044},{\"end\":44475,\"start\":44315},{\"end\":45062,\"start\":44521},{\"end\":45334,\"start\":45087},{\"end\":45546,\"start\":45336},{\"end\":45718,\"start\":45612},{\"end\":45902,\"start\":45720},{\"end\":45992,\"start\":45954},{\"end\":46381,\"start\":46028},{\"end\":46782,\"start\":46383},{\"end\":47195,\"start\":46784},{\"end\":47375,\"start\":47197},{\"end\":48080,\"start\":47416},{\"end\":48566,\"start\":48108},{\"end\":48684,\"start\":48568},{\"end\":48825,\"start\":48686},{\"end\":49423,\"start\":48827},{\"end\":49585,\"start\":49455},{\"end\":50700,\"start\":49611},{\"end\":50872,\"start\":50763},{\"end\":51758,\"start\":50874},{\"end\":52505,\"start\":51760},{\"end\":53065,\"start\":52507},{\"end\":55058,\"start\":53067},{\"end\":60286,\"start\":55060},{\"end\":60460,\"start\":60288},{\"end\":61275,\"start\":60481}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":16541,\"start\":16476},{\"attributes\":{\"id\":\"formula_1\"},\"end\":17658,\"start\":17613},{\"attributes\":{\"id\":\"formula_2\"},\"end\":18051,\"start\":18016},{\"attributes\":{\"id\":\"formula_3\"},\"end\":18126,\"start\":18058},{\"attributes\":{\"id\":\"formula_4\"},\"end\":18435,\"start\":18404},{\"attributes\":{\"id\":\"formula_5\"},\"end\":18520,\"start\":18468},{\"attributes\":{\"id\":\"formula_6\"},\"end\":21865,\"start\":21826},{\"attributes\":{\"id\":\"formula_7\"},\"end\":22522,\"start\":22470},{\"attributes\":{\"id\":\"formula_8\"},\"end\":22939,\"start\":22904},{\"attributes\":{\"id\":\"formula_9\"},\"end\":23541,\"start\":23459},{\"attributes\":{\"id\":\"formula_10\"},\"end\":24365,\"start\":24325},{\"attributes\":{\"id\":\"formula_11\"},\"end\":24522,\"start\":24479},{\"attributes\":{\"id\":\"formula_12\"},\"end\":24735,\"start\":24676},{\"attributes\":{\"id\":\"formula_13\"},\"end\":25466,\"start\":25411},{\"attributes\":{\"id\":\"formula_14\"},\"end\":25850,\"start\":25784},{\"attributes\":{\"id\":\"formula_15\"},\"end\":26113,\"start\":26042},{\"attributes\":{\"id\":\"formula_16\"},\"end\":26932,\"start\":26868},{\"attributes\":{\"id\":\"formula_17\"},\"end\":28230,\"start\":27849},{\"attributes\":{\"id\":\"formula_18\"},\"end\":28304,\"start\":28230},{\"attributes\":{\"id\":\"formula_19\"},\"end\":32306,\"start\":32251},{\"attributes\":{\"id\":\"formula_21\"},\"end\":33380,\"start\":33271},{\"attributes\":{\"id\":\"formula_22\"},\"end\":34803,\"start\":34720},{\"attributes\":{\"id\":\"formula_23\"},\"end\":36887,\"start\":36798},{\"attributes\":{\"id\":\"formula_24\"},\"end\":42389,\"start\":42287},{\"attributes\":{\"id\":\"formula_25\"},\"end\":42529,\"start\":42396},{\"attributes\":{\"id\":\"formula_26\"},\"end\":43383,\"start\":43239},{\"attributes\":{\"id\":\"formula_28\"},\"end\":43724,\"start\":43577},{\"attributes\":{\"id\":\"formula_29\"},\"end\":44043,\"start\":43914},{\"attributes\":{\"id\":\"formula_30\"},\"end\":45086,\"start\":45063},{\"attributes\":{\"id\":\"formula_31\"},\"end\":45611,\"start\":45547},{\"attributes\":{\"id\":\"formula_32\"},\"end\":45953,\"start\":45903},{\"attributes\":{\"id\":\"formula_33\"},\"end\":48107,\"start\":48081},{\"attributes\":{\"id\":\"formula_34\"},\"end\":50762,\"start\":50701}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":50136,\"start\":50129}]", "section_header": "[{\"end\":13608,\"start\":13562},{\"end\":19222,\"start\":19206},{\"end\":19386,\"start\":19369},{\"end\":24978,\"start\":24951},{\"end\":26389,\"start\":26368},{\"end\":27327,\"start\":27286},{\"end\":27510,\"start\":27488},{\"end\":31337,\"start\":31312},{\"end\":35012,\"start\":34966},{\"end\":36036,\"start\":36001},{\"end\":40367,\"start\":40343},{\"end\":41741,\"start\":41700},{\"end\":44519,\"start\":44478},{\"end\":46026,\"start\":45995},{\"end\":47414,\"start\":47378},{\"end\":49453,\"start\":49426},{\"end\":49609,\"start\":49588},{\"end\":60479,\"start\":60463},{\"end\":61278,\"start\":61277},{\"end\":61298,\"start\":61297},{\"end\":62070,\"start\":62069},{\"end\":62295,\"start\":62279},{\"end\":62422,\"start\":62421},{\"end\":63032,\"start\":63024},{\"end\":63310,\"start\":63302},{\"end\":63419,\"start\":63402},{\"end\":65164,\"start\":65155},{\"end\":65276,\"start\":65267},{\"end\":66274,\"start\":66256},{\"end\":67946,\"start\":67926}]", "table": "[{\"end\":65633,\"start\":65453},{\"end\":65752,\"start\":65744},{\"end\":66254,\"start\":66128},{\"end\":66633,\"start\":66296},{\"end\":67377,\"start\":66856},{\"end\":67924,\"start\":67497},{\"end\":68301,\"start\":67981}]", "figure_caption": "[{\"end\":61295,\"start\":61279},{\"end\":62067,\"start\":61299},{\"end\":62277,\"start\":62071},{\"end\":62419,\"start\":62298},{\"end\":63022,\"start\":62423},{\"end\":63300,\"start\":63034},{\"end\":63400,\"start\":63312},{\"end\":65153,\"start\":63422},{\"end\":65265,\"start\":65167},{\"end\":65366,\"start\":65279},{\"end\":65453,\"start\":65369},{\"end\":65744,\"start\":65636},{\"end\":66128,\"start\":65755},{\"end\":66296,\"start\":66276},{\"end\":66856,\"start\":66636},{\"end\":67497,\"start\":67380},{\"end\":67981,\"start\":67949}]", "figure_ref": "[{\"end\":13788,\"start\":13782},{\"end\":35652,\"start\":35646},{\"end\":36089,\"start\":36083},{\"end\":37311,\"start\":37305},{\"end\":38590,\"start\":38584},{\"end\":38946,\"start\":38940},{\"end\":47636,\"start\":47630},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":51365,\"start\":51359},{\"end\":52525,\"start\":52519},{\"end\":53254,\"start\":53248},{\"end\":54012,\"start\":54006},{\"end\":55115,\"start\":55109},{\"end\":56392,\"start\":56385},{\"end\":57329,\"start\":57323},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":57500,\"start\":57493},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":58487,\"start\":58480}]", "bib_author_first_name": "[{\"end\":68651,\"start\":68650},{\"end\":68660,\"start\":68659},{\"end\":68668,\"start\":68667},{\"end\":68674,\"start\":68673},{\"end\":68682,\"start\":68681},{\"end\":68691,\"start\":68690},{\"end\":69056,\"start\":69055},{\"end\":69064,\"start\":69063},{\"end\":69076,\"start\":69075},{\"end\":69084,\"start\":69083},{\"end\":69091,\"start\":69090},{\"end\":69417,\"start\":69416},{\"end\":69428,\"start\":69427},{\"end\":69441,\"start\":69440},{\"end\":69448,\"start\":69447},{\"end\":69460,\"start\":69459},{\"end\":69812,\"start\":69811},{\"end\":69820,\"start\":69819},{\"end\":69830,\"start\":69829},{\"end\":70135,\"start\":70134},{\"end\":70143,\"start\":70142},{\"end\":70152,\"start\":70151},{\"end\":70159,\"start\":70158},{\"end\":70166,\"start\":70165},{\"end\":70176,\"start\":70175},{\"end\":70183,\"start\":70182},{\"end\":70451,\"start\":70450},{\"end\":70461,\"start\":70460},{\"end\":70471,\"start\":70470},{\"end\":70487,\"start\":70486},{\"end\":70498,\"start\":70497},{\"end\":70508,\"start\":70507},{\"end\":70781,\"start\":70780},{\"end\":70790,\"start\":70789},{\"end\":70797,\"start\":70796},{\"end\":70804,\"start\":70803},{\"end\":70813,\"start\":70812},{\"end\":70819,\"start\":70818},{\"end\":71098,\"start\":71097},{\"end\":71106,\"start\":71105},{\"end\":71114,\"start\":71113},{\"end\":71122,\"start\":71121},{\"end\":71129,\"start\":71128},{\"end\":71137,\"start\":71136},{\"end\":71454,\"start\":71453},{\"end\":71723,\"start\":71722},{\"end\":71734,\"start\":71733},{\"end\":71743,\"start\":71742},{\"end\":71753,\"start\":71752},{\"end\":71764,\"start\":71763},{\"end\":72076,\"start\":72075},{\"end\":72084,\"start\":72083},{\"end\":72092,\"start\":72091},{\"end\":72100,\"start\":72099},{\"end\":72403,\"start\":72402},{\"end\":72411,\"start\":72410},{\"end\":72425,\"start\":72424},{\"end\":72789,\"start\":72788},{\"end\":72797,\"start\":72796},{\"end\":72805,\"start\":72804},{\"end\":72818,\"start\":72817},{\"end\":72827,\"start\":72826},{\"end\":72837,\"start\":72836},{\"end\":72843,\"start\":72842},{\"end\":73163,\"start\":73162},{\"end\":73173,\"start\":73172},{\"end\":73181,\"start\":73180},{\"end\":73191,\"start\":73190},{\"end\":73198,\"start\":73197},{\"end\":73209,\"start\":73208},{\"end\":73496,\"start\":73495},{\"end\":73504,\"start\":73503},{\"end\":73513,\"start\":73512},{\"end\":73523,\"start\":73522},{\"end\":73530,\"start\":73529},{\"end\":73539,\"start\":73538},{\"end\":73822,\"start\":73821},{\"end\":73829,\"start\":73828},{\"end\":73838,\"start\":73837},{\"end\":73847,\"start\":73846},{\"end\":73855,\"start\":73854},{\"end\":73864,\"start\":73863},{\"end\":73872,\"start\":73871},{\"end\":73882,\"start\":73881},{\"end\":74230,\"start\":74229},{\"end\":74238,\"start\":74237},{\"end\":74246,\"start\":74245},{\"end\":74252,\"start\":74251},{\"end\":74262,\"start\":74261},{\"end\":74269,\"start\":74268},{\"end\":74275,\"start\":74274},{\"end\":74649,\"start\":74648},{\"end\":74657,\"start\":74656},{\"end\":75000,\"start\":74999},{\"end\":75008,\"start\":75007},{\"end\":75297,\"start\":75296},{\"end\":75305,\"start\":75304},{\"end\":75314,\"start\":75313},{\"end\":75321,\"start\":75320},{\"end\":75601,\"start\":75600},{\"end\":75609,\"start\":75608},{\"end\":75619,\"start\":75618},{\"end\":75626,\"start\":75625},{\"end\":75927,\"start\":75926},{\"end\":75935,\"start\":75934},{\"end\":75941,\"start\":75940},{\"end\":75950,\"start\":75949},{\"end\":76239,\"start\":76238},{\"end\":76248,\"start\":76247},{\"end\":76254,\"start\":76253},{\"end\":76565,\"start\":76564},{\"end\":76571,\"start\":76570},{\"end\":76579,\"start\":76578},{\"end\":76587,\"start\":76586},{\"end\":76593,\"start\":76592},{\"end\":76601,\"start\":76600},{\"end\":76611,\"start\":76610},{\"end\":76970,\"start\":76969},{\"end\":76978,\"start\":76977},{\"end\":76984,\"start\":76983},{\"end\":76990,\"start\":76989},{\"end\":76998,\"start\":76997},{\"end\":77007,\"start\":77006},{\"end\":77408,\"start\":77407},{\"end\":77414,\"start\":77413},{\"end\":77420,\"start\":77419},{\"end\":77429,\"start\":77428},{\"end\":77813,\"start\":77812},{\"end\":77821,\"start\":77820},{\"end\":78157,\"start\":78156},{\"end\":78164,\"start\":78163},{\"end\":78173,\"start\":78172},{\"end\":78181,\"start\":78180},{\"end\":78431,\"start\":78430},{\"end\":78437,\"start\":78436},{\"end\":78446,\"start\":78445},{\"end\":78455,\"start\":78454},{\"end\":78463,\"start\":78462},{\"end\":78473,\"start\":78472},{\"end\":78767,\"start\":78766},{\"end\":78774,\"start\":78773},{\"end\":78968,\"start\":78967},{\"end\":79260,\"start\":79259},{\"end\":79268,\"start\":79267},{\"end\":79277,\"start\":79276},{\"end\":79285,\"start\":79284},{\"end\":79295,\"start\":79294},{\"end\":79302,\"start\":79301},{\"end\":79309,\"start\":79308},{\"end\":79675,\"start\":79674},{\"end\":79683,\"start\":79682},{\"end\":79690,\"start\":79689},{\"end\":79696,\"start\":79695},{\"end\":79703,\"start\":79702},{\"end\":79709,\"start\":79708},{\"end\":79719,\"start\":79718},{\"end\":79725,\"start\":79724},{\"end\":80045,\"start\":80044},{\"end\":80052,\"start\":80051},{\"end\":80059,\"start\":80058},{\"end\":80393,\"start\":80392},{\"end\":80405,\"start\":80404},{\"end\":80413,\"start\":80412},{\"end\":80423,\"start\":80422},{\"end\":80729,\"start\":80728},{\"end\":80737,\"start\":80736},{\"end\":80743,\"start\":80742},{\"end\":80752,\"start\":80751},{\"end\":80760,\"start\":80759},{\"end\":80770,\"start\":80769},{\"end\":81093,\"start\":81092},{\"end\":81102,\"start\":81101},{\"end\":81112,\"start\":81111},{\"end\":81393,\"start\":81392},{\"end\":81401,\"start\":81400},{\"end\":81410,\"start\":81409},{\"end\":81416,\"start\":81415},{\"end\":81722,\"start\":81721},{\"end\":81730,\"start\":81729},{\"end\":82026,\"start\":82025},{\"end\":82035,\"start\":82034},{\"end\":82045,\"start\":82044},{\"end\":82055,\"start\":82054},{\"end\":82372,\"start\":82371},{\"end\":82381,\"start\":82380},{\"end\":82389,\"start\":82388},{\"end\":82395,\"start\":82394},{\"end\":82404,\"start\":82403},{\"end\":82413,\"start\":82412},{\"end\":82720,\"start\":82713}]", "bib_author_last_name": "[{\"end\":68657,\"start\":68652},{\"end\":68665,\"start\":68661},{\"end\":68671,\"start\":68669},{\"end\":68679,\"start\":68675},{\"end\":68688,\"start\":68683},{\"end\":68696,\"start\":68692},{\"end\":69061,\"start\":69057},{\"end\":69073,\"start\":69065},{\"end\":69081,\"start\":69077},{\"end\":69088,\"start\":69085},{\"end\":69098,\"start\":69092},{\"end\":69425,\"start\":69418},{\"end\":69438,\"start\":69429},{\"end\":69445,\"start\":69442},{\"end\":69457,\"start\":69449},{\"end\":69468,\"start\":69461},{\"end\":69817,\"start\":69813},{\"end\":69827,\"start\":69821},{\"end\":69835,\"start\":69831},{\"end\":70140,\"start\":70136},{\"end\":70149,\"start\":70144},{\"end\":70156,\"start\":70153},{\"end\":70163,\"start\":70160},{\"end\":70173,\"start\":70167},{\"end\":70180,\"start\":70177},{\"end\":70188,\"start\":70184},{\"end\":70458,\"start\":70452},{\"end\":70468,\"start\":70462},{\"end\":70484,\"start\":70472},{\"end\":70495,\"start\":70488},{\"end\":70505,\"start\":70499},{\"end\":70515,\"start\":70509},{\"end\":70787,\"start\":70782},{\"end\":70794,\"start\":70791},{\"end\":70801,\"start\":70798},{\"end\":70810,\"start\":70805},{\"end\":70816,\"start\":70814},{\"end\":70823,\"start\":70820},{\"end\":71103,\"start\":71099},{\"end\":71111,\"start\":71107},{\"end\":71119,\"start\":71115},{\"end\":71126,\"start\":71123},{\"end\":71134,\"start\":71130},{\"end\":71141,\"start\":71138},{\"end\":71461,\"start\":71455},{\"end\":71731,\"start\":71724},{\"end\":71740,\"start\":71735},{\"end\":71750,\"start\":71744},{\"end\":71761,\"start\":71754},{\"end\":71770,\"start\":71765},{\"end\":72081,\"start\":72077},{\"end\":72089,\"start\":72085},{\"end\":72097,\"start\":72093},{\"end\":72104,\"start\":72101},{\"end\":72408,\"start\":72404},{\"end\":72422,\"start\":72412},{\"end\":72432,\"start\":72426},{\"end\":72794,\"start\":72790},{\"end\":72802,\"start\":72798},{\"end\":72815,\"start\":72806},{\"end\":72824,\"start\":72819},{\"end\":72834,\"start\":72828},{\"end\":72840,\"start\":72838},{\"end\":72848,\"start\":72844},{\"end\":73170,\"start\":73164},{\"end\":73178,\"start\":73174},{\"end\":73188,\"start\":73182},{\"end\":73195,\"start\":73192},{\"end\":73206,\"start\":73199},{\"end\":73214,\"start\":73210},{\"end\":73501,\"start\":73497},{\"end\":73510,\"start\":73505},{\"end\":73520,\"start\":73514},{\"end\":73527,\"start\":73524},{\"end\":73536,\"start\":73531},{\"end\":73547,\"start\":73540},{\"end\":73826,\"start\":73823},{\"end\":73835,\"start\":73830},{\"end\":73844,\"start\":73839},{\"end\":73852,\"start\":73848},{\"end\":73861,\"start\":73856},{\"end\":73869,\"start\":73865},{\"end\":73879,\"start\":73873},{\"end\":73887,\"start\":73883},{\"end\":74235,\"start\":74231},{\"end\":74243,\"start\":74239},{\"end\":74249,\"start\":74247},{\"end\":74259,\"start\":74253},{\"end\":74266,\"start\":74263},{\"end\":74272,\"start\":74270},{\"end\":74281,\"start\":74276},{\"end\":74654,\"start\":74650},{\"end\":74660,\"start\":74658},{\"end\":75005,\"start\":75001},{\"end\":75013,\"start\":75009},{\"end\":75302,\"start\":75298},{\"end\":75311,\"start\":75306},{\"end\":75318,\"start\":75315},{\"end\":75326,\"start\":75322},{\"end\":75606,\"start\":75602},{\"end\":75616,\"start\":75610},{\"end\":75623,\"start\":75620},{\"end\":75629,\"start\":75627},{\"end\":75932,\"start\":75928},{\"end\":75938,\"start\":75936},{\"end\":75947,\"start\":75942},{\"end\":75956,\"start\":75951},{\"end\":76245,\"start\":76240},{\"end\":76251,\"start\":76249},{\"end\":76257,\"start\":76255},{\"end\":76568,\"start\":76566},{\"end\":76576,\"start\":76572},{\"end\":76584,\"start\":76580},{\"end\":76590,\"start\":76588},{\"end\":76598,\"start\":76594},{\"end\":76608,\"start\":76602},{\"end\":76614,\"start\":76612},{\"end\":76975,\"start\":76971},{\"end\":76981,\"start\":76979},{\"end\":76987,\"start\":76985},{\"end\":76995,\"start\":76991},{\"end\":77004,\"start\":76999},{\"end\":77013,\"start\":77008},{\"end\":77411,\"start\":77409},{\"end\":77417,\"start\":77415},{\"end\":77426,\"start\":77421},{\"end\":77440,\"start\":77430},{\"end\":77818,\"start\":77814},{\"end\":77827,\"start\":77822},{\"end\":78161,\"start\":78158},{\"end\":78170,\"start\":78165},{\"end\":78178,\"start\":78174},{\"end\":78189,\"start\":78182},{\"end\":78434,\"start\":78432},{\"end\":78443,\"start\":78438},{\"end\":78452,\"start\":78447},{\"end\":78460,\"start\":78456},{\"end\":78470,\"start\":78464},{\"end\":78478,\"start\":78474},{\"end\":78771,\"start\":78768},{\"end\":78780,\"start\":78775},{\"end\":78975,\"start\":78969},{\"end\":79265,\"start\":79261},{\"end\":79274,\"start\":79269},{\"end\":79282,\"start\":79278},{\"end\":79292,\"start\":79286},{\"end\":79299,\"start\":79296},{\"end\":79306,\"start\":79303},{\"end\":79316,\"start\":79310},{\"end\":79680,\"start\":79676},{\"end\":79687,\"start\":79684},{\"end\":79693,\"start\":79691},{\"end\":79700,\"start\":79697},{\"end\":79706,\"start\":79704},{\"end\":79716,\"start\":79710},{\"end\":79722,\"start\":79720},{\"end\":79729,\"start\":79726},{\"end\":80049,\"start\":80046},{\"end\":80056,\"start\":80053},{\"end\":80071,\"start\":80060},{\"end\":80402,\"start\":80394},{\"end\":80410,\"start\":80406},{\"end\":80420,\"start\":80414},{\"end\":80430,\"start\":80424},{\"end\":80734,\"start\":80730},{\"end\":80740,\"start\":80738},{\"end\":80749,\"start\":80744},{\"end\":80757,\"start\":80753},{\"end\":80767,\"start\":80761},{\"end\":80779,\"start\":80771},{\"end\":81099,\"start\":81094},{\"end\":81109,\"start\":81103},{\"end\":81119,\"start\":81113},{\"end\":81398,\"start\":81394},{\"end\":81407,\"start\":81402},{\"end\":81413,\"start\":81411},{\"end\":81421,\"start\":81417},{\"end\":81727,\"start\":81723},{\"end\":81735,\"start\":81731},{\"end\":82032,\"start\":82027},{\"end\":82042,\"start\":82036},{\"end\":82052,\"start\":82046},{\"end\":82063,\"start\":82056},{\"end\":82378,\"start\":82373},{\"end\":82386,\"start\":82382},{\"end\":82392,\"start\":82390},{\"end\":82401,\"start\":82396},{\"end\":82410,\"start\":82405},{\"end\":82421,\"start\":82414},{\"end\":82726,\"start\":82721},{\"end\":86156,\"start\":86154}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":235813774},\"end\":68968,\"start\":68553},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":195767238},\"end\":69340,\"start\":68970},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":53279822},\"end\":69712,\"start\":69342},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":67856161},\"end\":70034,\"start\":69714},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":231992196},\"end\":70419,\"start\":70036},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":139107046},\"end\":70726,\"start\":70421},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":116863324},\"end\":71002,\"start\":70728},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":202660949},\"end\":71390,\"start\":71004},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":231713669},\"end\":71645,\"start\":71392},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":14955348},\"end\":71997,\"start\":71647},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":220890180},\"end\":72303,\"start\":71999},{\"attributes\":{\"doi\":\"10.1109/TCOMM.2021.3088528\",\"id\":\"b11\",\"matched_paper_id\":235422339},\"end\":72710,\"start\":72305},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":51921962},\"end\":73100,\"start\":72712},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":207870186},\"end\":73444,\"start\":73102},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":204578360},\"end\":73751,\"start\":73446},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":202888951},\"end\":74138,\"start\":73753},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":50775772},\"end\":74540,\"start\":74140},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":229307151},\"end\":74888,\"start\":74542},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":216168000},\"end\":75246,\"start\":74890},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":57189445},\"end\":75523,\"start\":75248},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":220903131},\"end\":75852,\"start\":75525},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":196623634},\"end\":76152,\"start\":75854},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":147703972},\"end\":76476,\"start\":76154},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":227255021},\"end\":76866,\"start\":76478},{\"attributes\":{\"doi\":\"10.1109/JIOT.2021.3078480\",\"id\":\"b24\",\"matched_paper_id\":236583161},\"end\":77321,\"start\":76868},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":189928596},\"end\":77672,\"start\":77323},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":221591508},\"end\":78103,\"start\":77674},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":204978458},\"end\":78369,\"start\":78105},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":202538765},\"end\":78706,\"start\":78371},{\"attributes\":{\"id\":\"b29\"},\"end\":78902,\"start\":78708},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":115963355},\"end\":79144,\"start\":78904},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":212909423},\"end\":79603,\"start\":79146},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":211056793},\"end\":79964,\"start\":79605},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":70348870},\"end\":80292,\"start\":79966},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":28202810},\"end\":80654,\"start\":80294},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":26419660},\"end\":81029,\"start\":80656},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":2133959},\"end\":81323,\"start\":81031},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":19463370},\"end\":81614,\"start\":81325},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":229246268},\"end\":81966,\"start\":81616},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":14542261},\"end\":82245,\"start\":81968},{\"attributes\":{\"id\":\"b40\"},\"end\":82711,\"start\":82247},{\"attributes\":{\"id\":\"b41\"},\"end\":83541,\"start\":82713},{\"attributes\":{\"id\":\"b42\"},\"end\":86100,\"start\":83543},{\"attributes\":{\"id\":\"b43\"},\"end\":86277,\"start\":86102}]", "bib_title": "[{\"end\":68648,\"start\":68553},{\"end\":69053,\"start\":68970},{\"end\":69414,\"start\":69342},{\"end\":69809,\"start\":69714},{\"end\":70132,\"start\":70036},{\"end\":70448,\"start\":70421},{\"end\":70778,\"start\":70728},{\"end\":71095,\"start\":71004},{\"end\":71451,\"start\":71392},{\"end\":71720,\"start\":71647},{\"end\":72073,\"start\":71999},{\"end\":72400,\"start\":72305},{\"end\":72786,\"start\":72712},{\"end\":73160,\"start\":73102},{\"end\":73493,\"start\":73446},{\"end\":73819,\"start\":73753},{\"end\":74227,\"start\":74140},{\"end\":74646,\"start\":74542},{\"end\":74997,\"start\":74890},{\"end\":75294,\"start\":75248},{\"end\":75598,\"start\":75525},{\"end\":75924,\"start\":75854},{\"end\":76236,\"start\":76154},{\"end\":76562,\"start\":76478},{\"end\":76967,\"start\":76868},{\"end\":77405,\"start\":77323},{\"end\":77810,\"start\":77674},{\"end\":78154,\"start\":78105},{\"end\":78428,\"start\":78371},{\"end\":78965,\"start\":78904},{\"end\":79257,\"start\":79146},{\"end\":79672,\"start\":79605},{\"end\":80042,\"start\":79966},{\"end\":80390,\"start\":80294},{\"end\":80726,\"start\":80656},{\"end\":81090,\"start\":81031},{\"end\":81390,\"start\":81325},{\"end\":81719,\"start\":81616},{\"end\":82023,\"start\":81968},{\"end\":82369,\"start\":82247},{\"end\":84107,\"start\":83543}]", "bib_author": "[{\"end\":68659,\"start\":68650},{\"end\":68667,\"start\":68659},{\"end\":68673,\"start\":68667},{\"end\":68681,\"start\":68673},{\"end\":68690,\"start\":68681},{\"end\":68698,\"start\":68690},{\"end\":69063,\"start\":69055},{\"end\":69075,\"start\":69063},{\"end\":69083,\"start\":69075},{\"end\":69090,\"start\":69083},{\"end\":69100,\"start\":69090},{\"end\":69427,\"start\":69416},{\"end\":69440,\"start\":69427},{\"end\":69447,\"start\":69440},{\"end\":69459,\"start\":69447},{\"end\":69470,\"start\":69459},{\"end\":69819,\"start\":69811},{\"end\":69829,\"start\":69819},{\"end\":69837,\"start\":69829},{\"end\":70142,\"start\":70134},{\"end\":70151,\"start\":70142},{\"end\":70158,\"start\":70151},{\"end\":70165,\"start\":70158},{\"end\":70175,\"start\":70165},{\"end\":70182,\"start\":70175},{\"end\":70190,\"start\":70182},{\"end\":70460,\"start\":70450},{\"end\":70470,\"start\":70460},{\"end\":70486,\"start\":70470},{\"end\":70497,\"start\":70486},{\"end\":70507,\"start\":70497},{\"end\":70517,\"start\":70507},{\"end\":70789,\"start\":70780},{\"end\":70796,\"start\":70789},{\"end\":70803,\"start\":70796},{\"end\":70812,\"start\":70803},{\"end\":70818,\"start\":70812},{\"end\":70825,\"start\":70818},{\"end\":71105,\"start\":71097},{\"end\":71113,\"start\":71105},{\"end\":71121,\"start\":71113},{\"end\":71128,\"start\":71121},{\"end\":71136,\"start\":71128},{\"end\":71143,\"start\":71136},{\"end\":71463,\"start\":71453},{\"end\":71733,\"start\":71722},{\"end\":71742,\"start\":71733},{\"end\":71752,\"start\":71742},{\"end\":71763,\"start\":71752},{\"end\":71772,\"start\":71763},{\"end\":72083,\"start\":72075},{\"end\":72091,\"start\":72083},{\"end\":72099,\"start\":72091},{\"end\":72106,\"start\":72099},{\"end\":72410,\"start\":72402},{\"end\":72424,\"start\":72410},{\"end\":72434,\"start\":72424},{\"end\":72796,\"start\":72788},{\"end\":72804,\"start\":72796},{\"end\":72817,\"start\":72804},{\"end\":72826,\"start\":72817},{\"end\":72836,\"start\":72826},{\"end\":72842,\"start\":72836},{\"end\":72850,\"start\":72842},{\"end\":73172,\"start\":73162},{\"end\":73180,\"start\":73172},{\"end\":73190,\"start\":73180},{\"end\":73197,\"start\":73190},{\"end\":73208,\"start\":73197},{\"end\":73216,\"start\":73208},{\"end\":73503,\"start\":73495},{\"end\":73512,\"start\":73503},{\"end\":73522,\"start\":73512},{\"end\":73529,\"start\":73522},{\"end\":73538,\"start\":73529},{\"end\":73549,\"start\":73538},{\"end\":73828,\"start\":73821},{\"end\":73837,\"start\":73828},{\"end\":73846,\"start\":73837},{\"end\":73854,\"start\":73846},{\"end\":73863,\"start\":73854},{\"end\":73871,\"start\":73863},{\"end\":73881,\"start\":73871},{\"end\":73889,\"start\":73881},{\"end\":74237,\"start\":74229},{\"end\":74245,\"start\":74237},{\"end\":74251,\"start\":74245},{\"end\":74261,\"start\":74251},{\"end\":74268,\"start\":74261},{\"end\":74274,\"start\":74268},{\"end\":74283,\"start\":74274},{\"end\":74656,\"start\":74648},{\"end\":74662,\"start\":74656},{\"end\":75007,\"start\":74999},{\"end\":75015,\"start\":75007},{\"end\":75304,\"start\":75296},{\"end\":75313,\"start\":75304},{\"end\":75320,\"start\":75313},{\"end\":75328,\"start\":75320},{\"end\":75608,\"start\":75600},{\"end\":75618,\"start\":75608},{\"end\":75625,\"start\":75618},{\"end\":75631,\"start\":75625},{\"end\":75934,\"start\":75926},{\"end\":75940,\"start\":75934},{\"end\":75949,\"start\":75940},{\"end\":75958,\"start\":75949},{\"end\":76247,\"start\":76238},{\"end\":76253,\"start\":76247},{\"end\":76259,\"start\":76253},{\"end\":76570,\"start\":76564},{\"end\":76578,\"start\":76570},{\"end\":76586,\"start\":76578},{\"end\":76592,\"start\":76586},{\"end\":76600,\"start\":76592},{\"end\":76610,\"start\":76600},{\"end\":76616,\"start\":76610},{\"end\":76977,\"start\":76969},{\"end\":76983,\"start\":76977},{\"end\":76989,\"start\":76983},{\"end\":76997,\"start\":76989},{\"end\":77006,\"start\":76997},{\"end\":77015,\"start\":77006},{\"end\":77413,\"start\":77407},{\"end\":77419,\"start\":77413},{\"end\":77428,\"start\":77419},{\"end\":77442,\"start\":77428},{\"end\":77820,\"start\":77812},{\"end\":77829,\"start\":77820},{\"end\":78163,\"start\":78156},{\"end\":78172,\"start\":78163},{\"end\":78180,\"start\":78172},{\"end\":78191,\"start\":78180},{\"end\":78436,\"start\":78430},{\"end\":78445,\"start\":78436},{\"end\":78454,\"start\":78445},{\"end\":78462,\"start\":78454},{\"end\":78472,\"start\":78462},{\"end\":78480,\"start\":78472},{\"end\":78773,\"start\":78766},{\"end\":78782,\"start\":78773},{\"end\":78977,\"start\":78967},{\"end\":79267,\"start\":79259},{\"end\":79276,\"start\":79267},{\"end\":79284,\"start\":79276},{\"end\":79294,\"start\":79284},{\"end\":79301,\"start\":79294},{\"end\":79308,\"start\":79301},{\"end\":79318,\"start\":79308},{\"end\":79682,\"start\":79674},{\"end\":79689,\"start\":79682},{\"end\":79695,\"start\":79689},{\"end\":79702,\"start\":79695},{\"end\":79708,\"start\":79702},{\"end\":79718,\"start\":79708},{\"end\":79724,\"start\":79718},{\"end\":79731,\"start\":79724},{\"end\":80051,\"start\":80044},{\"end\":80058,\"start\":80051},{\"end\":80073,\"start\":80058},{\"end\":80404,\"start\":80392},{\"end\":80412,\"start\":80404},{\"end\":80422,\"start\":80412},{\"end\":80432,\"start\":80422},{\"end\":80736,\"start\":80728},{\"end\":80742,\"start\":80736},{\"end\":80751,\"start\":80742},{\"end\":80759,\"start\":80751},{\"end\":80769,\"start\":80759},{\"end\":80781,\"start\":80769},{\"end\":81101,\"start\":81092},{\"end\":81111,\"start\":81101},{\"end\":81121,\"start\":81111},{\"end\":81400,\"start\":81392},{\"end\":81409,\"start\":81400},{\"end\":81415,\"start\":81409},{\"end\":81423,\"start\":81415},{\"end\":81729,\"start\":81721},{\"end\":81737,\"start\":81729},{\"end\":82034,\"start\":82025},{\"end\":82044,\"start\":82034},{\"end\":82054,\"start\":82044},{\"end\":82065,\"start\":82054},{\"end\":82380,\"start\":82371},{\"end\":82388,\"start\":82380},{\"end\":82394,\"start\":82388},{\"end\":82403,\"start\":82394},{\"end\":82412,\"start\":82403},{\"end\":82423,\"start\":82412},{\"end\":82728,\"start\":82713},{\"end\":86158,\"start\":86154}]", "bib_venue": "[{\"end\":68721,\"start\":68698},{\"end\":69125,\"start\":69100},{\"end\":69495,\"start\":69470},{\"end\":69846,\"start\":69837},{\"end\":70199,\"start\":70190},{\"end\":70542,\"start\":70517},{\"end\":70835,\"start\":70825},{\"end\":71170,\"start\":71143},{\"end\":71495,\"start\":71463},{\"end\":71786,\"start\":71772},{\"end\":72120,\"start\":72106},{\"end\":72493,\"start\":72460},{\"end\":72875,\"start\":72850},{\"end\":73243,\"start\":73216},{\"end\":73569,\"start\":73549},{\"end\":73914,\"start\":73889},{\"end\":74308,\"start\":74283},{\"end\":74687,\"start\":74662},{\"end\":75041,\"start\":75015},{\"end\":75355,\"start\":75328},{\"end\":75649,\"start\":75631},{\"end\":75972,\"start\":75958},{\"end\":76284,\"start\":76259},{\"end\":76641,\"start\":76616},{\"end\":77065,\"start\":77040},{\"end\":77466,\"start\":77442},{\"end\":77860,\"start\":77829},{\"end\":78205,\"start\":78191},{\"end\":78507,\"start\":78480},{\"end\":78764,\"start\":78708},{\"end\":78991,\"start\":78977},{\"end\":79343,\"start\":79318},{\"end\":79756,\"start\":79731},{\"end\":80100,\"start\":80073},{\"end\":80442,\"start\":80432},{\"end\":80806,\"start\":80781},{\"end\":81134,\"start\":81121},{\"end\":81441,\"start\":81423},{\"end\":81762,\"start\":81737},{\"end\":82075,\"start\":82065},{\"end\":82448,\"start\":82423},{\"end\":82859,\"start\":82728},{\"end\":84491,\"start\":84109},{\"end\":86152,\"start\":86102},{\"end\":68756,\"start\":68723},{\"end\":70855,\"start\":70837},{\"end\":71808,\"start\":71788},{\"end\":72145,\"start\":72122},{\"end\":75677,\"start\":75651},{\"end\":75997,\"start\":75974},{\"end\":78230,\"start\":78207},{\"end\":79014,\"start\":78993},{\"end\":80465,\"start\":80444},{\"end\":80840,\"start\":80808},{\"end\":81159,\"start\":81136},{\"end\":82081,\"start\":82077}]"}}}, "year": 2023, "month": 12, "day": 17}