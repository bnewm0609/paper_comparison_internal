{"id": 9114952, "updated": "2023-09-29 06:28:16.081", "metadata": {"title": "DeepVO: Towards End-to-End Visual Odometry with Deep Recurrent Convolutional Neural Networks", "authors": "[{\"first\":\"Sen\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Ronald\",\"last\":\"Clark\",\"middle\":[]},{\"first\":\"Hongkai\",\"last\":\"Wen\",\"middle\":[]},{\"first\":\"Niki\",\"last\":\"Trigoni\",\"middle\":[]}]", "venue": "2017 IEEE International Conference on Robotics and Automation (ICRA)", "journal": "2017 IEEE International Conference on Robotics and Automation (ICRA)", "publication_date": {"year": 2017, "month": 9, "day": 25}, "abstract": "This paper studies monocular visual odometry (VO) problem. Most of existing VO algorithms are developed under a standard pipeline including feature extraction, feature matching, motion estimation, local optimisation, etc. Although some of them have demonstrated superior performance, they usually need to be carefully designed and specifically fine-tuned to work well in different environments. Some prior knowledge is also required to recover an absolute scale for monocular VO. This paper presents a novel end-to-end framework for monocular VO by using deep Recurrent Convolutional Neural Networks (RCNNs). Since it is trained and deployed in an end-to-end manner, it infers poses directly from a sequence of raw RGB images (videos) without adopting any module in the conventional VO pipeline. Based on the RCNNs, it not only automatically learns effective feature representation for the VO problem through Convolutional Neural Networks, but also implicitly models sequential dynamics and relations using deep Recurrent Neural Networks. Extensive experiments on the KITTI VO dataset show competitive performance to state-of-the-art methods, verifying that the end-to-end Deep Learning technique can be a viable complement to the traditional VO systems.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1709.08429", "mag": "3106440972", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icra/WangCWT17", "doi": "10.1109/icra.2017.7989236"}}, "content": {"source": {"pdf_hash": "9d7874cb850425f5d74ceadae5b99a761c1b1b97", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1709.08429v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1709.08429", "status": "GREEN"}}, "grobid": {"id": "b32eea12e969588c9c7b9bd64082fb63a4c9d430", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/9d7874cb850425f5d74ceadae5b99a761c1b1b97.txt", "contents": "\nDeepVO: Towards End-to-End Visual Odometry with Deep Recurrent Convolutional Neural Networks\n\n\nSen Wang \nRonald Clark \nHongkai Wen \nNiki Trigoni \nDeepVO: Towards End-to-End Visual Odometry with Deep Recurrent Convolutional Neural Networks\n\nThis paper studies monocular visual odometry (VO) problem. Most of existing VO algorithms are developed under a standard pipeline including feature extraction, feature matching, motion estimation, local optimisation, etc. Although some of them have demonstrated superior performance, they usually need to be carefully designed and specifically fine-tuned to work well in different environments. Some prior knowledge is also required to recover an absolute scale for monocular VO. This paper presents a novel end-to-end framework for monocular VO by using deep Recurrent Convolutional Neural Networks (RCNNs) 1 . Since it is trained and deployed in an end-to-end manner, it infers poses directly from a sequence of raw RGB images (videos) without adopting any module in the conventional VO pipeline. Based on the RCNNs, it not only automatically learns effective feature representation for the VO problem through Convolutional Neural Networks, but also implicitly models sequential dynamics and relations using deep Recurrent Neural Networks. Extensive experiments on the KITTI VO dataset show competitive performance to state-ofthe-art methods, verifying that the end-to-end Deep Learning technique can be a viable complement to the traditional VO systems.\n\nI. INTRODUCTION\n\nVisual odometry (VO), as one of the most essential techniques for pose estimation and robot localisation, has attracted significant interest in both the computer vision and robotics communities over the past few decades [1]. It has been widely applied to various robots as a complement to GPS, Inertial Navigation System (INS), wheel odometry, etc.\n\nIn the last thirty years, enormous work has been done to develop an accurate and robust monocular VO system. As shown in Fig. 1, a classic pipeline [1], [2], which typically consists of camera calibration, feature detection, feature matching (or tracking), outlier rejection (e.g., RANSAC), motion estimation, scale estimation and local optimisation (Bundle Adjustment), has been developed and broadly recognised as a golden rule to follow. Although some state-of-theart algorithms based on this pipeline have shown excellent performance in terms of accuracy and robustness, they are usually hard-coded with significant engineering effort and each module in the pipeline needs to be carefully designed and fine-tuned to ensure the performance. Moreover, the monocular VO has to estimate an absolute scale by using some extra information (e,g., height of the camera) or prior knowledge, making it prone to big drift and more challenging than the stereo VO. Fig. 1. Architectures of the conventional feature based monocular VO and the proposed end-to-end method. In the proposed method, RCNN takes a sequence of RGB images (video) as input and learns features by CNN for RNN based sequential modelling to estimate poses. Camera image credit: KITTI dataset [3].\n\nDeep Learning (DL) has recently been dominating many computer vision tasks with promising results. Unfortunately, for the VO problem this has not arrived yet. In fact, there is very limited work on VO, even related to 3D geometry problems. We presume that this is because most of the existing DL architectures and pre-trained models are essentially designed to tackle recognition and classification problems, which drives deep Convolutional Neural Networks (CNNs) to extract high-level appearance information from images. Learning the appearance representation confines the VO to function only in trained environments and seriously hinders the popularisation of the VO to new scenarios. This is why the VO algorithms heavily rely on geometric features rather than appearance ones. Meanwhile, a VO algorithm ideally should model motion dynamics by examining the changes and connections on a sequence of images rather than processing a single image. This means we need sequential learning, which the CNNs are inadequate to.\n\nIn this paper, we propose a novel DL based monocular VO algorithm by leveraging deep Recurrent Convolutional Neural Networks (RCNNs) [4]. Since it is achieved in an end-to-end manner, it does not need any module in the classic VO pipeline (even camera calibration). The main contribution is threefold: 1) We demonstrate that the monocular VO problem can be addressed in an end-to-end fashion based on DL, i.e., directly estimating poses from raw RGB images. Neither prior knowledge nor parameter is needed to recover the absolute scale. To the best of our knowledge, this is the first end-to-end approach on the monocular VO through Deep Neural Networks (DNNs). 2) We propose a RCNN architecture enabling the DL based VO algorithm to be generalised to totally new environments by using the geometric feature representation learnt by the CNN. 3) Sequential dependence and complex motion dynamics of an image sequence, which are of importance to the VO but cannot be explicitly or easily modelled by human, are implicitly encapsulated and automatically learnt by deep Recurrent Neural Networks (RNNs).\n\nThe rest of this paper is organised as follows. Section II reviews related work. The end-to-end monocular VO algorithm is described in Section III, followed by experimental results in Section IV. Conclusion is drawn in Section V.\n\n\nII. RELATED WORK\n\nEarly work on the monocular VO is reviewed in this section, discussing various algorithms and their differences from others. There are mainly two types of algorithms in terms of the technique and framework adopted: geometry based and learning based methods.\n\n\nA. Methods based on Geometry\n\nTheoretically based on geometric theory, geometry based methods, which dominate the area of VO, rely on geometric constraints extracted from imagery to estimate motion. Since they are derived from elegant and established principles and have been extensively investigated, most of state-ofthe-art VO algorithms fall into this family. They can be further divided into sparse feature based methods and direct methods.\n\n1) Sparse Feature based Methods: Sparse feature based methods, whose typical pipeline is shown in Fig. 1, employ multi-view geometry [5] to determine motion after extracting and matching (or tracking) salient feature points from a sequence of images, such as the algorithm in [6] and LIBVISO2 [7]. However, due to the presence of outliers, noises, etc., all VO algorithms suffer from drifts over time.\n\nTo mitigate this problem, visual Simultaneous Localisation and Mapping (SLAM) or Structure from Motion (SfM) can be adopted to maintain a feature map for drift correction along with pose estimation [8]. Examples include keyframe base PTAM [9] and ORB-SLAM [10].\n\n2) Direct Methods: Feature extraction and matching of sparse feature based methods are computationally expensive. More importantly, they only use salient features without benefiting from rich information contained in the whole image. Direct methods, in contrast, are capable of exploiting all the pixels in consecutive images for pose estimation under the assumption of photometric consistency, e.g., DTAM in [11]. Recently, semi-direct approaches which realise superior performance are developed for the monocular VO [12]- [14]. Since the direct methods tend to be more accurate in principle than feature based ones and can work better in texture-less environments, they are increasingly gaining more favour.\n\n\nB. Methods based on Learning\n\nAs data-driven approaches, learning based methods are to learn motion model and infer VO from sensor readings by Machine Learning techniques without explicitly applying geometric theory. Optical flow is used to train K Nearest Neighbour (KNN), Gaussian Process (GP) and Support Vector Machines (SVM) regression algorithms for the monocular VO in [15], [16] and [17], respectively. Since the learning based methods are recently emerging, there is limited amount of work and no one has directly dealt with raw RGB images yet.\n\nIt has been widely recognised that traditional Machine Learning techniques are inefficient when encountering big or highly non-linear, high-dimensional data, e.g., RGB images. DL which automatically learns suitable feature representation from large-scale dataset provides an alternative solution to the VO problem.\n\n1) Deep Learning based Methods: DL has achieved promising results on some localisation related applications. The features of CNNs, for instance, have been utilised for appearance based place recognition [18]. Unfortunately, there is little work on VO or pose estimation. To our knowledge, [19] firstly realises DL based VO through synchrony detection between image sequences and features. After estimating depth from stereo images, the CNN predicts the discretised changes of direction and velocity by the softmax function. Although this work provides a feasible scheme for DL based stereo VO, it inherently formulates the VO as a classification problem rather than pose regression. Camera relocalisation using a single image is solved in [20] by fine-tuning images of a specific scene with CNNs. It suggests to label these images by SfM, which is time-consuming and labourintensive for large-scale scenarios. Because a trained CNN model serves as an appearance \"map\" of the scene, it needs to be re-trained or at least fine-tuned for a new environment. This seriously hampers the technique for widespread usage, which is also one of the biggest difficulties when applying DL for VO. To overcome this problem, the CNNs are provided with dense optical flow instead of RGB images for motion estimation in [21]. Three different architectures of CNNs are developed to learn appropriate features for VO, achieving robust VO even with blurred and under-exposured images. However, the proposed CNNs require pre-processed dense optical flow as input, which cannot benefit from the end-to-end learning and may be inappropriate to real-time applications.\n\nBecause the CNNs are incapable of modelling sequential information, none of the previous work considers image sequences or videos for sequential learning. In this work, we tackle this by leveraging the RNNs.\n\n\nIII. END-TO-END VISUAL ODOMETRY THROUGH RCNN\n\nIn this section, the deep RCNN framework realising the monocular VO in an end-to-end fashion is described in detail. It is mainly composed of CNN based feature extraction and RNN based sequential modelling.\n\n\nA. Architecture of the Proposed RCNN\n\nThere have been some popular and powerful DNN architectures, such as VGGNet [22] and GoogLeNet [23], developed for computer vision tasks, producing remarkable performance. Most of them are designed with tackling recognition, classification and detection problems in mind, which means that they are trained to learn knowledge from appearance and image context. However, as discussed before, VO which is rooted in geometry should not be closely coupled with appearance. Therefore, it is impractical to simply adopt the current popular DNN architectures for the VO problem. A framework which can learn geometric feature representations is of importance to address the VO and other geometric problems. Meanwhile, it is essential to derive connections among consecutive image frames, e.g., motion models, since the VO systems evolve over time and operate on image sequences acquired during movement. Therefore, the proposed RCNN takes these two requirements into consideration.\n\nThe architecture of the proposed end-to-end VO system is shown in Fig. 2. It takes a video clip or a monocular image sequence as input. At each time step, the RGB image frame is pre-processed by subtracting the mean RGB values of the training set and, optionally, resizing to a new size in the multiple of 64. Two consecutive images are stacked together to form a tensor for the deep RCNN to learn how to extract motion information and estimate poses. Specifically, the image tensor is fed into the CNN to produce an effective feature for the monocular VO, which is then passed through a RNN for sequential learning. Each image pair yields a pose estimate at each time step through the network. The VO system develops over time and estimates new poses as images are captured.\n\nThe advantage of the RCNN based architecture is to allow simultaneous feature extraction and sequential modelling of VO through the combination of CNN and RNN. More details \n7 \u00d7 7 3 2 64 Conv2 5 \u00d7 5 2 2 128 Conv3 5 \u00d7 5 2 2 256 Conv3 1 3 \u00d7 3 1 1 256 Conv4 3 \u00d7 3 1 2 512 Conv4 1 3 \u00d7 3 1 1 512 Conv5 3 \u00d7 3 1 2 512 Conv5 1 3 \u00d7 3 1 1 512 Conv6 3 \u00d7 3 1 2 1024\nare given in the subsequent sections.\n\n\nB. CNN based Feature Extraction\n\nIn order to automatically learn effective features that are suitable for the VO problem, a CNN is developed to perform feature extraction on the concatenation of two consecutive monocular RGB images. The feature representation is ideally geometric instead of being associated with appearance or visual context because the VO systems need to be generalised and deployed in unknown environments. The structure of the CNN is inspired by the network for optical flow estimation in [24].\n\nThe configuration of the CNN is outlined in TABLE I and an example of its tensors on KITTI dataset is given in Fig. 2. It has 9 convolutional layers and each layer is followed by a rectified linear unit (ReLU) activation except Conv6, i.e., 17 layers in total. The sizes of the receptive fields in the network gradually reduce from 7 \u00d7 7 to 5 \u00d7 5 and then 3 \u00d7 3 to capture small interesting features. Zeropaddings are introduced to either adapt to the configurations of the receptive fields or preserve the spatial dimension of the tensor after convolution. The number of the channels, i.e., the number of filters for feature detection, increases to learn various features. Fig. 3. Folded and unfolded LSTMs and internal structure of its unit. and \u2295 denote element-wise product and addition of two vectors, respectively.\nh t h t 1 c t 1 c t f t x t input gate forget gate input modulation gate output gate i t g t o t c t LSTM hidden state x t unfold x t 1 x t+1 tanh sigmoid\nThe CNN takes raw RGB images instead of pre-processed counterparts, such as optical flow or depth images, as input because the network is trained to learn an efficient feature representation with reduced dimensionality for the VO. This learnt feature representation not only compresses the original high-dimensional RGB image into a compact description, but also boosts the successive sequential training procedure. Hence, the last convolutional feature Conv6 is passed to the RNN for sequential modelling.\n\n\nC. RNN based Sequential Modelling\n\nFollowing the CNN, a deep RNN is designed to conduct sequential learning, i.e., to model dynamics and relations among a sequence of CNN features. Note that this modelling is performed implicitly by the RNN to automatically discover appropriate sequential knowledge. Therefore, it may come out more than the models we use to describe physical movement and geometry.\n\nSince the RNN is capable of modelling dependencies in a sequence, it is well suited to the VO problem which involves temporal model (motion model) and sequential data (image sequence). For instance, estimating pose of current image frame can benefit from information encapsulated in previous frames. In fact, this insight has already existed in the conventional VO systems. For example, multi-view geometry is able to avoid some issues in two-view geometry [5]. However, RNN is not suitable to directly learn sequential representation from high-dimensional raw data, such as images. Therefore, the proposed system adopts the appealing RCNN architecture with the CNN features as the input of the RNN.\n\nRNN is different from CNN in that it maintains memory of its hidden states over time and has feedback loops among them, which enables its current hidden state to be a function of the previous ones, as the RNN part shown in Fig. 2. Hence, the RNN can find out the connections among the input and the previous states in the sequence. Given a convolutional feature x k at time k, a RNN updates at time step k by\nh k = H(W xh x k + W hh h k\u22121 + b h ) y k = W hy h k + b y(1)\nwhere h k and y k are the hidden state and output at time k respectively, W terms denote corresponding weight matrices, b terms denote bias vectors, and H is an element-wise non-linear activation function, such as sigmoid or hyperbolic tangent. Although in theory the standard RNN can learn sequences with arbitrary lengths, it is limited to short ones in practice due to the known vanishing gradient problem [25]. In order to be able to find and exploit correlations among images taken in long trajectories, Long Short-Term Memory (LSTM) which is capable of learning long-term dependencies by introducing memory gates and units [26] is employed as our RNN. It explicitly determines which previous hidden states to be discarded or retained for updating the current state, being expected to learn the motion during pose estimation. The folded LSTM and its unfolded version over time are shown in Fig. 3 along with the internal structure of a LSTM unit. It can be seen that after unfolding the LSTM, each LSTM unit is associated with a time step. Given the input x k at time k and the hidden state h k\u22121 and the memory cell c k\u22121 of the previous LSTM unit, the LSTM updates at time step k according to\ni k = \u03c3(W xi x k + W hi h k\u22121 + b i ) f k = \u03c3(W xf x k + W hf h k\u22121 + b f ) g k = tanh(W xg x k + W hg h k\u22121 + b g ) c k = f k c k\u22121 + i k g k o k = \u03c3(W xo x k + W ho h k\u22121 + b o ) h k = o k tanh(c k )(2)\nwhere is element-wise product of two vectors, \u03c3 is sigmoid non-linearity, tanh is hyperbolic tangent non-linearity, W terms denote corresponding weight matrices, b terms denote bias vectors, i k , f k , g k , c k and o k are input gate, forget gate, input modulation gate, memory cell and output gate at time k, respectively, Although the LSTM can handle long-term dependencies and has deep temporal structure, it still needs depth on network layers to learn high level representation and model complex dynamics. The advantages of the deep RNN architecture have been proved in [27] for speech recognition using acoustic signal. Therefore, in our case the deep RNN is constructed by stacking two LSTM layers with the hidden states of a LSTM being the input of the other one, as illustrated in Fig. 2. In our network, each of the LSTM layers has 1000 hidden states.\n\nThe deep RNN outputs a pose estimate at each time step based on the visual features generated from the CNN. This progresses over time as the camera moves and images are captured.\n\n\nD. Cost Function and Optimisation\n\nThe proposed RCNN based VO system can be considered to compute the conditional probability of the poses Y t = (y 1 , . . . , y t ) given a sequence of monocular RGB images X t = (x 1 , . . . , x t ) up to time t in the probabilistic perspective:\n\np(Y t |X t ) = p(y 1 , . . . , y t |x 1 , . . . , x t )\n\nThe modelling and probabilistic inference are performed in the deep RCNN. To find the optimal parameters \u03b8 * for the VO, the DNN maximises (3):\n\u03b8 * = argmax \u03b8 p(Y t |X t ; \u03b8)(4)\nTo learn the hyperparameters \u03b8 of the DNNs, the Euclidean distance between the ground truth pose (p k , \u03d5 k ) at time k and its estimated one ( p k , \u03d5 k ) is minimised. The loss function is composed of Mean Square Error (MSE) of all positions p and orientations \u03d5:\n\u03b8 * = argmin \u03b8 1 N N i=1 t k=1 p k \u2212 p k 2 2 + \u03ba \u03d5 k \u2212 \u03d5 k 2 2 (5)\nwhere \u00b7 is 2-norm, \u03ba (100 in the experiments) is a scale factor to balance the weights of positions and orientations, and N is the number of samples. The orientation \u03d5 is represented by Euler angles rather than quaternion since quaternion is subject to an extra unit constraint which hinders the optimisation problem of DL. We also find that in practice using quaternion degrades the orientation estimate to some extent.\n\n\nIV. EXPERIMENTAL RESULTS\n\nIn this section, we evaluate the proposed end-toend monocular VO approach on the well-known KITTI VO/SLAM benchmark [3]. Since most of existing monocular VO algorithms do not estimate an absolute scale, their localisation results have to be manually aligned with ground truth. Therefore, the open-source VO library LIBVISO2 [7] which uses a fixed camera height to recover the scale for the monocular VO is adopted for comparison. Its stereo version which can directly obtain the absolute poses is also employed.\n\nA. Training and Testing 1) Dataset: The KITTI VO/SLAM benchmark [3] has 22 sequences of images, of which 11 ones (Sequence 00-10) are associated with ground truth. The other 10 sequences (Sequence 11-21) are only provided with raw sensor data. Since this dataset was recorded at a relatively low frame rate (10 fps) by driving in urban areas with many dynamic objects and the speed of the driving was up to 90 km/h, it is very challenging for the monocular VO algorithms.\n\n2) Training and Testing: Two separate experiments are conducted to evaluate the proposed method. The first one is based on the Sequence 00-10 to quantitatively analyse its performance by ground truth since the ground truth is only provided for these sequences. In order to have a separate dataset for testing, only the Sequence 00, 02, 08 and 09 which are relatively long are used for training. The trajectories are segmented to different lengths to generate much data for training, producing 7410 samples in total. The trained models are tested on the Sequence 03, 04, 05, 06, 07 and 10 for evaluation.\n\nSince the ability to generalise well to real data is essential for DL based approaches, the next experiment aims to analyse how the proposed method and the trained VO models behave in totally new environments. For the VO problem,  this is further required as aforementioned. Therefore, models trained on all the Sequence 00-10 are tested on the Sequence 11-21 which do not have ground truth available for training. The network is implemented based on the famous DL framework Theano and trained by using a NVIDIA Tesla K40 GPU. The Adagrad optimiser is employed to train the network for up to 200 epochs with learning rate 0.001. Dropout and early stopping techniques are introduced to prevent the models from overfitting. In order to reduce both the training time and data required to converge, the CNN is based on a pre-trained FlowNet model [24].\n\n3) How overfitting affects the VO: It is well known that overfitting is an undesirable behaviour for Machine Learning based methods. However, its meaning and influence are unclear in the context of the VO problem. Concrete discussions on this, which can guide a better training on the VO system, are still missing. Some insights on our training procedure and results are described here. In Fig. 4, the losses and VO results of two models are given. The big gap between the training and validation losses in Fig. 4(a) indicates serious overfitting compared to the proper losses in Fig. 4(b). Reflecting on the estimated VO of the training data, the results of the overfitted model are much more accurate that those of the well-  fitted model, as shown in Fig. 4(c) and Fig. 4(d). However, when applying the trained models on the testing data, the well-fitted model yields much better results, see Fig. 4(e) and Fig. 4(f). This is also very likely to happen when the model is deployed in practice working on real data. Therefore, overfitting should be carefully examined when training a model for the VO. Based on this example, it is can be seen that for the DL based VO problem overfitting has very intuitive outcomes and can seriously degrade the odometry estimation. A well-fitted model is key to ensuring good generalisation and reliable pose estimation of the trained VO models to untrained environments. In our work, we observed found that the orientation is more prone to overfitting than position. This could be because the orientation changes are usually smaller. In terms of underfitting, we assume this is rare because the capacity of the DNN is typically large and the size of training data tends to be limited.\n\n\nB. VO Results\n\nThe performance of the trained VO models is analysed according to the KITTI VO/SLAM evaluation metrics, i.e., averaged Root Mean Square Errors (RMSEs) of the translational and rotational errors for all subsequences of lengths ranging from 100 to 800 meters and different speeds (the range of speeds varies in different sequences).\n\nThe first DL based model is trained on Sequence 00, 02, 08 and 09 and then tested on Sequence 03, 04, 05, 06, 07 and 10. The average RMSEs of the estimated VO on the test sequences are given in Fig. 5 with the translation and rotation against different path lengths and speeds. Although the result of the DeepVO is worst than that of the stereo VISO2 (VISO2 S), it is consistently better than the monocular VISO2 (VISO2 M) except for the translational errors of the DL model at high speeds, which are slightly  higher than the monocular VISO2. We presume that this is because the maximum velocity of the Sequence 00, 02, 08 and 09 is below 60 km/h and there is very limited number of training samples whose speeds are bigger than 50 km/h. Without being trained with enough data covering the highspeed situation, the network tries to regress the VO but probably suffers from high drifts. It is interesting that the rotational errors become smaller on high velocities, which is opposite to the translation. This may be due to the fact that the KITTI dataset was recorded during car driving, which tends to go straight on high speeds yet rotate when slowing down. Moving forward, as a dynamics without significant changes on rotation, can be easily learnt to model by the RNN in terms of orientation, but the velocity varies fast. As the length of the trajectory increases, the errors of both the translation and rotation of the DeepVO significantly decrease, approaching to the stereo VISO2 as shown in Fig.  5(a) and Fig. 5(b). The estimated VO trajectories corresponding to the previous testing are given in Fig. 6. It can be seen that the DeepVO produces relatively accurate and consistent trajectories against to the ground truth, demonstrating that the scale can be better estimated than using prior information, such as camera height. Note that no scale estimation or post alignment to ground truth is performed for the DeepVO to obtain the absolute poses. The scale is completely maintained by the network itself and implicitly learnt during the end-toend training. Since recovering accurate and robust scale is surprisingly difficult for the monocular VO, this suggests an appealing advantage of the DL based VO method. The detailed performance of the algorithms on the testing sequences is summarised in TABLE II. It indicates that the DeepVO achieves more robust results than the monocular VISO2.\n\nAlthough the generalisation of the DeepVO model has  Fig. 7, can degrade the accuracy as well. These reasons also apply to Sequence 21. In order to mitigate these issues, the conventional geometry based methods could increase feature matching and introduce outlier rejection, such as RANSAC. However, for the DL based method, it is unclear how to embed these techniques yet. However, a feasible solution is to train the network with more data which not only reflects these situations but also is deliberatively augmented with noise, outliers, etc., allowing the network itself to figure out how to deal with these problems. \n\n\nV. CONCLUSIONS\n\nThis paper presents a novel end-to-end monocular VO algorithm based on Deep Learning. Leveraging the power of Deep RCNNs, this new paradigm is able to achieve simultaneous representation learning and sequential modelling of the the monocular VO by combining the CNNs with the RNNs. Since it does not depend on any module in the conventional VO algorithms (even camera calibration) for pose estimation and it is trained in an end-to-end manner, there is no need to carefully tune the parameters of the VO system. Based on the KITTI VO benchmark, it is verified that it can produce accurate VO results with precise scales and work well in completely new scenarios.\n\nAlthough the proposed DL based VO method presents some results on this area, we stress that it is not expected as a replacement to the classic geometry based approach. On the contrary, it can be a viable complement, i.e., incorporating geometry with the representation, knowledge and models learnt by the DNNs to further improve the VO in terms of accuracy and, more importantly, robustness.  \n\nFig. 4 .\n4Training losses and VO results of two models. Figures in the left and right columns are about the over-fitted and well-fitted models, respectively. (a)-(b) Training and validation losses. (c)-(d) Estimated VO on training data (Sequence 00). (e)-(f) Estimated VO on testing data (Sequence 05).\n\nFig. 5 .\n5Average errors on translation and rotation against different path lengths and speeds. The DeepVO model used is trained on Sequence 00, 02, 08 and 09.\n\nFig. 6 .\n6Trajectories of VO testing results on Sequence 04, 05, 07 and 10. The DeepVO model used is trained on Sequence 00, 02, 08 and 09.\n\nFig. 7 .\n7Sample images from Sequence 12. There are some moving objects and large open areas in this sequence.\n\nFig. 8 .\n8Trajectories of VO results on the testing Sequence 11, 12, 15, 17, 18 and 19 of the KITTI VO benchmark (no ground truth is available for these testing sequences). The DeepVO model used is trained on the whole training dataset of the KITTI VO benchmark.\n\n\nFig. 2. Architecture of the proposed RCNN based monocular VO system. The dimensions of the tensors shown here are given as an example based on the image size of the KITTI dataset. The CNN ones should vary according to the size of the input image. Camera image credit: KITTI dataset.Video (Image Sequence) \n\nt \n\n20 \u00d7 6 \u00d7 1024 \n\nt+1 \n\n640 \u00d7 192 \u00d7 64 \n\n1280 \u00d7 384 \u00d7 3 \n1280 \u00d7 384 \u00d7 3 \n\nConvolutional Neural Network \n\nStacked \nImages \n\n1241 \u00d7 376 \u00d7 3 \n\n1241 \u00d7 376 \u00d7 3 \n320 \u00d7 96 \u00d7 128 \n160 \u00d7 48 \u00d7 256 \n160 \u00d7 48 \u00d7 256 \n80 \u00d7 24 \u00d7 512 \n80 \u00d7 24 \u00d7 512 \n40 \u00d7 12 \u00d7 512 \n40 \u00d7 12 \u00d7 512 \n\nTime \n\nRecurrent Neural Network \n\n6 \n\nPose \n\nresize \n\nConv1 Conv2 Conv3 Conv3_1 Conv4 Conv4_1 Conv5 Conv5_1 Conv6 \n\n1000 \n\nLSTM1 \n\n1000 \n\nLSTM2 \n\n\n\nTABLE I CONFIGURATION\nIOF THE CNNLayer \nReceptive \nField Size \nPadding Stride \nNumber \nof Channels \nConv1 \n\n\nTABLE II RESULTS\nIION TESTING SEQUENCES. trel(%) rrel( \u2022 ) trel(%) rrel( \u2022 ) trel(%) rrel( \u2022 )\u2022 trel: average translational RMSE drift (%) on length of 100m-800m. \u2022 rrel: average rotational RMSE drift ( \u2022 /100m) on length of 100m-800m. \u2022 The DeepVO model used is trained on Sequence 00, 02, 08 and 09. Its performance is expected to improve when it is trained on more data.been evaluated in the previous experiment, in order to further investigate how it performs in entirely new scenarios with different motion patterns and scenes, the network is tested on the testing dataset of KITTI VO benchmark. The DeepVO model is trained on all the 11 training sequences of the KITTI VO benchmark (i.e., Sequence 00-10), providing more data to avoid overfitting and to maximise the performance of the network. Due to the lack of ground truth for these testing sequences, no quantitative analysis can be performed on the VO results. For qualitative comparison, some predicted trajectories of the DeepVO, the monocular VISO2 and the stereo VISO2 are shown inFig. 8. It can be seen that the results of the DeepVO are much better than these of the monocular VISO2 and roughly similar to the stereo VISO2's. It seems that this larger training dataset boosts the performance of the DeepVO. Taking the stereo properties of the stereo VISO2 into consideration, the DeepVO, as a monocular VO algorithm, achieves an appealing performance, showing that the trained model can generalise well in unknown scenarios. An exception could be the test on Sequence 12 inFig. 8(b) which suffers from rather high localisation errors although the shape of the trajectory is close to the stereo VISO2's. There are several reasons. First, the training dataset does not have enough data on high speeds. Among all the 11 training dataset, only the Sequence 01 has velocities that are higher than 60 km/h. However, the speeds of the Sequence 12 span from 50km/h up to about 90km/h. Moreover, the images are captured at only 10 Hz, which makes the VO estimation more challenging during fast movement. The large open area around highway (lacking of features) and dynamic moving objects, shown inSeq. \nDeepVO \nVISO2 M \nVISO2 S \n03 \n8.49 \n6.89 \n8.47 \n8.82 \n3.21 \n3.25 \n04 \n7.19 \n6.97 \n4.69 \n4.49 \n2.12 \n2.12 \n05 \n2.62 \n3.61 \n19.22 \n17.58 \n1.53 \n1.60 \n06 \n5.42 \n5.82 \n7.30 \n6.14 \n1.48 \n1.58 \n07 \n3.91 \n4.60 \n23.61 \n29.11 \n1.85 \n1.91 \n10 \n8.11 \n8.83 \n41.56 \n32.99 \n1.17 \n1.30 \nmean 5.96 \n6.12 \n17.48 \n16.52 \n1.89 \n1.96 \n\n\nACKNOWLEDGMENT This work is supported by EPSRC Programme Grant \"Mobile Robotics: Enabling a Pervasive Technology of the Future\" (EP/M019918/1). The authors would like to acknowledge the use of the University of Oxford Advanced Research Computing (ARC) facility in carrying out this work (http://dx.doi.org/10.5281/zenodo.22558).\nVisual odometry: Tutorial. D Scaramuzza, F Fraundorfer, IEEE Robotics & Automation Magazine. 184D. Scaramuzza and F. Fraundorfer, \"Visual odometry: Tutorial,\" IEEE Robotics & Automation Magazine, vol. 18, no. 4, pp. 80-92, 2011.\n\nVisual odometry: Part II: Matching, robustness, optimization, and applications. F Fraundorfer, D Scaramuzza, IEEE Robotics & Automation Magazine. 192F. Fraundorfer and D. Scaramuzza, \"Visual odometry: Part II: Match- ing, robustness, optimization, and applications,\" IEEE Robotics & Automation Magazine, vol. 19, no. 2, pp. 78-90, 2012.\n\nAre we ready for autonomous driving? the KITTI vision benchmark suite. A Geiger, P Lenz, R Urtasun, Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Conference on Computer Vision and Pattern Recognition (CVPR)A. Geiger, P. Lenz, and R. Urtasun, \"Are we ready for autonomous driving? the KITTI vision benchmark suite,\" in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2012.\n\nLong-term recurrent convolutional networks for visual recognition and description. J Donahue, L A Hendricks, S Guadarrama, M Rohrbach, S Venugopalan, K Saenko, T Darrell, IEEE Transactions on Pattern Analysis and Machine Intelligence. to appearJ. Donahue, L. A. Hendricks, S. Guadarrama, M. Rohrbach, S. Venu- gopalan, K. Saenko, and T. Darrell, \"Long-term recurrent convolu- tional networks for visual recognition and description,\" IEEE Trans- actions on Pattern Analysis and Machine Intelligence, to appear.\n\nMultiple view geometry in computer vision. R Hartley, A Zisserman, Cambridge university pressR. Hartley and A. Zisserman, Multiple view geometry in computer vision. Cambridge university press, 2003.\n\nVisual odometry. D Nist\u00e9r, O Naroditsky, J Bergen, Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Conference on Computer Vision and Pattern Recognition (CVPR)IEEE1652D. Nist\u00e9r, O. Naroditsky, and J. Bergen, \"Visual odometry,\" in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), vol. 1. IEEE, 2004, pp. I-652.\n\nStereoscan: Dense 3D reconstruction in real-time. A Geiger, J Ziegler, C Stiller, Intelligent Vehicles Symposium (IV). A. Geiger, J. Ziegler, and C. Stiller, \"Stereoscan: Dense 3D recon- struction in real-time,\" in Intelligent Vehicles Symposium (IV), 2011.\n\nMonoSLAM: Real-time single camera SLAM. A J Davison, I D Reid, N D Molton, O Stasse, IEEE Transactions on Pattern Analysis and Machine Intelligence. 296A. J. Davison, I. D. Reid, N. D. Molton, and O. Stasse, \"MonoSLAM: Real-time single camera SLAM,\" IEEE Transactions on Pattern Anal- ysis and Machine Intelligence, vol. 29, no. 6, pp. 1052-1067, 2007.\n\nParallel tracking and mapping for small AR workspaces. G Klein, D Murray, IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR). IEEEG. Klein and D. Murray, \"Parallel tracking and mapping for small AR workspaces,\" in IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR). IEEE, 2007, pp. 225-234.\n\nORB-SLAM: a versatile and accurate monocular SLAM system. R Mur-Artal, J Montiel, J D Tard\u00f3s, IEEE Transactions on Robotics. 315R. Mur-Artal, J. Montiel, and J. D. Tard\u00f3s, \"ORB-SLAM: a versa- tile and accurate monocular SLAM system,\" IEEE Transactions on Robotics, vol. 31, no. 5, pp. 1147-1163, 2015.\n\nDTAM: Dense tracking and mapping in real-time. R A Newcombe, S J Lovegrove, A J Davison, Proceedings of IEEE International Conference on Computer Vision (ICCV). IEEE International Conference on Computer Vision (ICCV)IEEER. A. Newcombe, S. J. Lovegrove, and A. J. Davison, \"DTAM: Dense tracking and mapping in real-time,\" in Proceedings of IEEE International Conference on Computer Vision (ICCV). IEEE, 2011, pp. 2320-2327.\n\nSemi-dense visual odometry for a monocular camera. J Engel, J Sturm, D Cremers, Proceedings of IEEE International Conference on Computer Vision (ICCV). IEEE International Conference on Computer Vision (ICCV)J. Engel, J. Sturm, and D. Cremers, \"Semi-dense visual odometry for a monocular camera,\" in Proceedings of IEEE International Conference on Computer Vision (ICCV), 2013, pp. 1449-1456.\n\nSVO: Fast semi-direct monocular visual odometry. C Forster, M Pizzoli, D Scaramuzza, Proceedings of IEEE International Conference on Robotics and Automation (ICRA). IEEE International Conference on Robotics and Automation (ICRA)IEEEC. Forster, M. Pizzoli, and D. Scaramuzza, \"SVO: Fast semi-direct monocular visual odometry,\" in Proceedings of IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2014, pp. 15-22.\n\nDirect sparse odometry. J Engel, V Koltun, D Cremers, arXiv:1607.02565J. Engel, V. Koltun, and D. Cremers, \"Direct sparse odometry,\" in arXiv:1607.02565, July 2016.\n\nMemorybased learning for visual odometry. R Roberts, H Nguyen, N Krishnamurthi, T Balch, Proceedings of IEEE International Conference on Robotics and Automation (ICRA). IEEE International Conference on Robotics and Automation (ICRA)IEEER. Roberts, H. Nguyen, N. Krishnamurthi, and T. Balch, \"Memory- based learning for visual odometry,\" in Proceedings of IEEE Interna- tional Conference on Robotics and Automation (ICRA). IEEE, 2008, pp. 47-52.\n\nSemi-parametric learning for visual odometry. V Guizilini, F Ramos, The International Journal of Robotics Research. 325V. Guizilini and F. Ramos, \"Semi-parametric learning for visual odometry,\" The International Journal of Robotics Research, vol. 32, no. 5, pp. 526-546, 2013.\n\nEvaluation of non-geometric methods for visual odometry. T A Ciarfuglia, G Costante, P Valigi, E Ricci, Robotics and Autonomous Systems. 6212T. A. Ciarfuglia, G. Costante, P. Valigi, and E. Ricci, \"Evaluation of non-geometric methods for visual odometry,\" Robotics and Au- tonomous Systems, vol. 62, no. 12, pp. 1717-1730, 2014.\n\nPlace recognition with convnet landmarks: Viewpoint-robust, condition-robust, training-free. N S\u00fcnderhauf, S Shirazi, A Jacobson, F Dayoub, E Pepperell, B Upcroft, M Milford, Proceedings of Robotics: Science and Systems (RSS). Robotics: Science and Systems (RSS)N. S\u00fcnderhauf, S. Shirazi, A. Jacobson, F. Dayoub, E. Pepperell, B. Upcroft, and M. Milford, \"Place recognition with convnet land- marks: Viewpoint-robust, condition-robust, training-free,\" in Proceed- ings of Robotics: Science and Systems (RSS), 2015.\n\nLearning visual odometry with a convolutional network. K Konda, R Memisevic, Proceedings of International Conference on Computer Vision Theory and Applications. International Conference on Computer Vision Theory and ApplicationsK. Konda and R. Memisevic, \"Learning visual odometry with a convolutional network,\" in Proceedings of International Conference on Computer Vision Theory and Applications, 2015.\n\nConvolutional networks for real-time 6-DoF camera relocalization. A Kendall, M Grimes, R Cipolla, Proceedings of International Conference on Computer Vision (ICCV). International Conference on Computer Vision (ICCV)A. Kendall, M. Grimes, and R. Cipolla, \"Convolutional networks for real-time 6-DoF camera relocalization,\" in Proceedings of Interna- tional Conference on Computer Vision (ICCV), 2015.\n\nExploring representation learning with CNNs for frame-to-frame ego-motion estimation. G Costante, M Mancini, P Valigi, T A Ciarfuglia, IEEE Robotics and Automation Letters. 11G. Costante, M. Mancini, P. Valigi, and T. A. Ciarfuglia, \"Exploring representation learning with CNNs for frame-to-frame ego-motion estimation,\" IEEE Robotics and Automation Letters, vol. 1, no. 1, pp. 18-25, 2016.\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, arXiv:1409.1556arXiv preprintK. Simonyan and A. Zisserman, \"Very deep convolutional networks for large-scale image recognition,\" arXiv preprint arXiv:1409.1556, 2014.\n\nGoing deeper with convolutions. C Szegedy, W Liu, Y Jia, P Sermanet, S Reed, D Anguelov, D Erhan, V Vanhoucke, A Rabinovich, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich, \"Going deeper with convolutions,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 1-9.\n\nFlownet: Learning optical flow with convolutional networks. A Dosovitskiy, P Fischery, E Ilg, C Hazirbas, V Golkov, P Van Der Smagt, D Cremers, T Brox, Proceedings of IEEE International Conference on Computer Vision (ICCV). IEEE International Conference on Computer Vision (ICCV)IEEEA. Dosovitskiy, P. Fischery, E. Ilg, C. Hazirbas, V. Golkov, P. van der Smagt, D. Cremers, T. Brox et al., \"Flownet: Learning optical flow with convolutional networks,\" in Proceedings of IEEE International Conference on Computer Vision (ICCV). IEEE, 2015, pp. 2758-2766.\n\nDeep learning. I Goodfellow, Y Bengio, A Courville, MIT Pressbook in preparation forI. Goodfellow, Y. Bengio, and A. Courville, \"Deep learning,\" 2016, book in preparation for MIT Press.\n\nLearning to execute. W Zaremba, I Sutskever, arXiv:1410.4615arXiv preprintW. Zaremba and I. Sutskever, \"Learning to execute,\" arXiv preprint arXiv:1410.4615, 2014.\n\nTowards end-to-end speech recognition with recurrent neural networks. A Graves, N Jaitly, Proceedings of International Conference on Machine Learning (ICML). International Conference on Machine Learning (ICML)14A. Graves and N. Jaitly, \"Towards end-to-end speech recognition with recurrent neural networks.\" in Proceedings of International Conference on Machine Learning (ICML), vol. 14, 2014, pp. 1764-1772.\n", "annotations": {"author": "[{\"end\":105,\"start\":96},{\"end\":119,\"start\":106},{\"end\":132,\"start\":120},{\"end\":146,\"start\":133}]", "publisher": null, "author_last_name": "[{\"end\":104,\"start\":100},{\"end\":118,\"start\":113},{\"end\":131,\"start\":128},{\"end\":145,\"start\":138}]", "author_first_name": "[{\"end\":99,\"start\":96},{\"end\":112,\"start\":106},{\"end\":127,\"start\":120},{\"end\":137,\"start\":133}]", "author_affiliation": null, "title": "[{\"end\":93,\"start\":1},{\"end\":239,\"start\":147}]", "venue": null, "abstract": "[{\"end\":1497,\"start\":241}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1739,\"start\":1736},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2017,\"start\":2014},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2022,\"start\":2019},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3123,\"start\":3120},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4285,\"start\":4282},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6342,\"start\":6339},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6485,\"start\":6482},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6502,\"start\":6499},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6810,\"start\":6807},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6851,\"start\":6848},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6869,\"start\":6865},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7285,\"start\":7281},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7394,\"start\":7390},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7400,\"start\":7396},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7964,\"start\":7960},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7970,\"start\":7966},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7979,\"start\":7975},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8662,\"start\":8658},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8748,\"start\":8744},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9198,\"start\":9194},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9762,\"start\":9758},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10684,\"start\":10680},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10703,\"start\":10699},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":13263,\"start\":13259},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":15612,\"start\":15609},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":16737,\"start\":16733},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":16957,\"start\":16953},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":18310,\"start\":18306},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":20193,\"start\":20190},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":20401,\"start\":20398},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":20654,\"start\":20651},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":22512,\"start\":22508}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":28996,\"start\":28693},{\"attributes\":{\"id\":\"fig_3\"},\"end\":29157,\"start\":28997},{\"attributes\":{\"id\":\"fig_5\"},\"end\":29298,\"start\":29158},{\"attributes\":{\"id\":\"fig_6\"},\"end\":29410,\"start\":29299},{\"attributes\":{\"id\":\"fig_8\"},\"end\":29674,\"start\":29411},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":30397,\"start\":29675},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":30506,\"start\":30398},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":32986,\"start\":30507}]", "paragraph": "[{\"end\":1864,\"start\":1516},{\"end\":3124,\"start\":1866},{\"end\":4147,\"start\":3126},{\"end\":5248,\"start\":4149},{\"end\":5479,\"start\":5250},{\"end\":5757,\"start\":5500},{\"end\":6204,\"start\":5790},{\"end\":6607,\"start\":6206},{\"end\":6870,\"start\":6609},{\"end\":7581,\"start\":6872},{\"end\":8137,\"start\":7614},{\"end\":8453,\"start\":8139},{\"end\":10099,\"start\":8455},{\"end\":10308,\"start\":10101},{\"end\":10563,\"start\":10357},{\"end\":11576,\"start\":10604},{\"end\":12353,\"start\":11578},{\"end\":12528,\"start\":12355},{\"end\":12746,\"start\":12709},{\"end\":13264,\"start\":12782},{\"end\":14086,\"start\":13266},{\"end\":14748,\"start\":14242},{\"end\":15150,\"start\":14786},{\"end\":15851,\"start\":15152},{\"end\":16261,\"start\":15853},{\"end\":17523,\"start\":16324},{\"end\":18592,\"start\":17729},{\"end\":18772,\"start\":18594},{\"end\":19055,\"start\":18810},{\"end\":19112,\"start\":19057},{\"end\":19257,\"start\":19114},{\"end\":19557,\"start\":19292},{\"end\":20045,\"start\":19625},{\"end\":20585,\"start\":20074},{\"end\":21058,\"start\":20587},{\"end\":21663,\"start\":21060},{\"end\":22513,\"start\":21665},{\"end\":24236,\"start\":22515},{\"end\":24584,\"start\":24254},{\"end\":26990,\"start\":24586},{\"end\":27616,\"start\":26992},{\"end\":28297,\"start\":27635},{\"end\":28692,\"start\":28299}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12708,\"start\":12529},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14241,\"start\":14087},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16323,\"start\":16262},{\"attributes\":{\"id\":\"formula_3\"},\"end\":17728,\"start\":17524},{\"attributes\":{\"id\":\"formula_5\"},\"end\":19291,\"start\":19258},{\"attributes\":{\"id\":\"formula_6\"},\"end\":19624,\"start\":19558}]", "table_ref": null, "section_header": "[{\"end\":1514,\"start\":1499},{\"end\":5498,\"start\":5482},{\"end\":5788,\"start\":5760},{\"end\":7612,\"start\":7584},{\"end\":10355,\"start\":10311},{\"end\":10602,\"start\":10566},{\"end\":12780,\"start\":12749},{\"end\":14784,\"start\":14751},{\"end\":18808,\"start\":18775},{\"end\":20072,\"start\":20048},{\"end\":24252,\"start\":24239},{\"end\":27633,\"start\":27619},{\"end\":28702,\"start\":28694},{\"end\":29006,\"start\":28998},{\"end\":29167,\"start\":29159},{\"end\":29308,\"start\":29300},{\"end\":29420,\"start\":29412},{\"end\":30420,\"start\":30399},{\"end\":30524,\"start\":30508}]", "table": "[{\"end\":30397,\"start\":29959},{\"end\":30506,\"start\":30432},{\"end\":32986,\"start\":32664}]", "figure_caption": "[{\"end\":28996,\"start\":28704},{\"end\":29157,\"start\":29008},{\"end\":29298,\"start\":29169},{\"end\":29410,\"start\":29310},{\"end\":29674,\"start\":29422},{\"end\":29959,\"start\":29677},{\"end\":30432,\"start\":30422},{\"end\":32664,\"start\":30527}]", "figure_ref": "[{\"end\":1993,\"start\":1987},{\"end\":2828,\"start\":2822},{\"end\":6310,\"start\":6304},{\"end\":11650,\"start\":11644},{\"end\":13383,\"start\":13377},{\"end\":13946,\"start\":13940},{\"end\":16082,\"start\":16076},{\"end\":17225,\"start\":17219},{\"end\":18527,\"start\":18521},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22911,\"start\":22905},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":23031,\"start\":23022},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":23104,\"start\":23095},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":23278,\"start\":23269},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":23292,\"start\":23283},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":23417,\"start\":23411},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":23434,\"start\":23425},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":24786,\"start\":24780},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26094,\"start\":26087},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26111,\"start\":26102},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":26200,\"start\":26194},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":27051,\"start\":27045}]", "bib_author_first_name": "[{\"end\":33344,\"start\":33343},{\"end\":33358,\"start\":33357},{\"end\":33627,\"start\":33626},{\"end\":33642,\"start\":33641},{\"end\":33956,\"start\":33955},{\"end\":33966,\"start\":33965},{\"end\":33974,\"start\":33973},{\"end\":34416,\"start\":34415},{\"end\":34427,\"start\":34426},{\"end\":34429,\"start\":34428},{\"end\":34442,\"start\":34441},{\"end\":34456,\"start\":34455},{\"end\":34468,\"start\":34467},{\"end\":34483,\"start\":34482},{\"end\":34493,\"start\":34492},{\"end\":34887,\"start\":34886},{\"end\":34898,\"start\":34897},{\"end\":35061,\"start\":35060},{\"end\":35071,\"start\":35070},{\"end\":35085,\"start\":35084},{\"end\":35477,\"start\":35476},{\"end\":35487,\"start\":35486},{\"end\":35498,\"start\":35497},{\"end\":35726,\"start\":35725},{\"end\":35728,\"start\":35727},{\"end\":35739,\"start\":35738},{\"end\":35741,\"start\":35740},{\"end\":35749,\"start\":35748},{\"end\":35751,\"start\":35750},{\"end\":35761,\"start\":35760},{\"end\":36095,\"start\":36094},{\"end\":36104,\"start\":36103},{\"end\":36440,\"start\":36439},{\"end\":36453,\"start\":36452},{\"end\":36464,\"start\":36463},{\"end\":36466,\"start\":36465},{\"end\":36732,\"start\":36731},{\"end\":36734,\"start\":36733},{\"end\":36746,\"start\":36745},{\"end\":36748,\"start\":36747},{\"end\":36761,\"start\":36760},{\"end\":36763,\"start\":36762},{\"end\":37160,\"start\":37159},{\"end\":37169,\"start\":37168},{\"end\":37178,\"start\":37177},{\"end\":37551,\"start\":37550},{\"end\":37562,\"start\":37561},{\"end\":37573,\"start\":37572},{\"end\":37959,\"start\":37958},{\"end\":37968,\"start\":37967},{\"end\":37978,\"start\":37977},{\"end\":38143,\"start\":38142},{\"end\":38154,\"start\":38153},{\"end\":38164,\"start\":38163},{\"end\":38181,\"start\":38180},{\"end\":38593,\"start\":38592},{\"end\":38606,\"start\":38605},{\"end\":38882,\"start\":38881},{\"end\":38884,\"start\":38883},{\"end\":38898,\"start\":38897},{\"end\":38910,\"start\":38909},{\"end\":38920,\"start\":38919},{\"end\":39248,\"start\":39247},{\"end\":39262,\"start\":39261},{\"end\":39273,\"start\":39272},{\"end\":39285,\"start\":39284},{\"end\":39295,\"start\":39294},{\"end\":39308,\"start\":39307},{\"end\":39319,\"start\":39318},{\"end\":39726,\"start\":39725},{\"end\":39735,\"start\":39734},{\"end\":40143,\"start\":40142},{\"end\":40154,\"start\":40153},{\"end\":40164,\"start\":40163},{\"end\":40564,\"start\":40563},{\"end\":40576,\"start\":40575},{\"end\":40587,\"start\":40586},{\"end\":40597,\"start\":40596},{\"end\":40599,\"start\":40598},{\"end\":40938,\"start\":40937},{\"end\":40950,\"start\":40949},{\"end\":41163,\"start\":41162},{\"end\":41174,\"start\":41173},{\"end\":41181,\"start\":41180},{\"end\":41188,\"start\":41187},{\"end\":41200,\"start\":41199},{\"end\":41208,\"start\":41207},{\"end\":41220,\"start\":41219},{\"end\":41229,\"start\":41228},{\"end\":41242,\"start\":41241},{\"end\":41716,\"start\":41715},{\"end\":41731,\"start\":41730},{\"end\":41743,\"start\":41742},{\"end\":41750,\"start\":41749},{\"end\":41762,\"start\":41761},{\"end\":41772,\"start\":41771},{\"end\":41789,\"start\":41788},{\"end\":41800,\"start\":41799},{\"end\":42226,\"start\":42225},{\"end\":42240,\"start\":42239},{\"end\":42250,\"start\":42249},{\"end\":42419,\"start\":42418},{\"end\":42430,\"start\":42429},{\"end\":42633,\"start\":42632},{\"end\":42643,\"start\":42642}]", "bib_author_last_name": "[{\"end\":33355,\"start\":33345},{\"end\":33370,\"start\":33359},{\"end\":33639,\"start\":33628},{\"end\":33653,\"start\":33643},{\"end\":33963,\"start\":33957},{\"end\":33971,\"start\":33967},{\"end\":33982,\"start\":33975},{\"end\":34424,\"start\":34417},{\"end\":34439,\"start\":34430},{\"end\":34453,\"start\":34443},{\"end\":34465,\"start\":34457},{\"end\":34480,\"start\":34469},{\"end\":34490,\"start\":34484},{\"end\":34501,\"start\":34494},{\"end\":34895,\"start\":34888},{\"end\":34908,\"start\":34899},{\"end\":35068,\"start\":35062},{\"end\":35082,\"start\":35072},{\"end\":35092,\"start\":35086},{\"end\":35484,\"start\":35478},{\"end\":35495,\"start\":35488},{\"end\":35506,\"start\":35499},{\"end\":35736,\"start\":35729},{\"end\":35746,\"start\":35742},{\"end\":35758,\"start\":35752},{\"end\":35768,\"start\":35762},{\"end\":36101,\"start\":36096},{\"end\":36111,\"start\":36105},{\"end\":36450,\"start\":36441},{\"end\":36461,\"start\":36454},{\"end\":36473,\"start\":36467},{\"end\":36743,\"start\":36735},{\"end\":36758,\"start\":36749},{\"end\":36771,\"start\":36764},{\"end\":37166,\"start\":37161},{\"end\":37175,\"start\":37170},{\"end\":37186,\"start\":37179},{\"end\":37559,\"start\":37552},{\"end\":37570,\"start\":37563},{\"end\":37584,\"start\":37574},{\"end\":37965,\"start\":37960},{\"end\":37975,\"start\":37969},{\"end\":37986,\"start\":37979},{\"end\":38151,\"start\":38144},{\"end\":38161,\"start\":38155},{\"end\":38178,\"start\":38165},{\"end\":38187,\"start\":38182},{\"end\":38603,\"start\":38594},{\"end\":38612,\"start\":38607},{\"end\":38895,\"start\":38885},{\"end\":38907,\"start\":38899},{\"end\":38917,\"start\":38911},{\"end\":38926,\"start\":38921},{\"end\":39259,\"start\":39249},{\"end\":39270,\"start\":39263},{\"end\":39282,\"start\":39274},{\"end\":39292,\"start\":39286},{\"end\":39305,\"start\":39296},{\"end\":39316,\"start\":39309},{\"end\":39327,\"start\":39320},{\"end\":39732,\"start\":39727},{\"end\":39745,\"start\":39736},{\"end\":40151,\"start\":40144},{\"end\":40161,\"start\":40155},{\"end\":40172,\"start\":40165},{\"end\":40573,\"start\":40565},{\"end\":40584,\"start\":40577},{\"end\":40594,\"start\":40588},{\"end\":40610,\"start\":40600},{\"end\":40947,\"start\":40939},{\"end\":40960,\"start\":40951},{\"end\":41171,\"start\":41164},{\"end\":41178,\"start\":41175},{\"end\":41185,\"start\":41182},{\"end\":41197,\"start\":41189},{\"end\":41205,\"start\":41201},{\"end\":41217,\"start\":41209},{\"end\":41226,\"start\":41221},{\"end\":41239,\"start\":41230},{\"end\":41253,\"start\":41243},{\"end\":41728,\"start\":41717},{\"end\":41740,\"start\":41732},{\"end\":41747,\"start\":41744},{\"end\":41759,\"start\":41751},{\"end\":41769,\"start\":41763},{\"end\":41786,\"start\":41773},{\"end\":41797,\"start\":41790},{\"end\":41805,\"start\":41801},{\"end\":42237,\"start\":42227},{\"end\":42247,\"start\":42241},{\"end\":42260,\"start\":42251},{\"end\":42427,\"start\":42420},{\"end\":42440,\"start\":42431},{\"end\":42640,\"start\":42634},{\"end\":42650,\"start\":42644}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":594994},\"end\":33544,\"start\":33316},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":1473690},\"end\":33882,\"start\":33546},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":6724907},\"end\":34330,\"start\":33884},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":5736847},\"end\":34841,\"start\":34332},{\"attributes\":{\"id\":\"b4\"},\"end\":35041,\"start\":34843},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":9224113},\"end\":35424,\"start\":35043},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":16284071},\"end\":35683,\"start\":35426},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":206764185},\"end\":36037,\"start\":35685},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":206986664},\"end\":36379,\"start\":36039},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":206775100},\"end\":36682,\"start\":36381},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":1336659},\"end\":37106,\"start\":36684},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":7110290},\"end\":37499,\"start\":37108},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":206850490},\"end\":37932,\"start\":37501},{\"attributes\":{\"doi\":\"arXiv:1607.02565\",\"id\":\"b13\"},\"end\":38098,\"start\":37934},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":9955291},\"end\":38544,\"start\":38100},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":6051492},\"end\":38822,\"start\":38546},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":16060445},\"end\":39152,\"start\":38824},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":5437340},\"end\":39668,\"start\":39154},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":9527002},\"end\":40074,\"start\":39670},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":15492888},\"end\":40475,\"start\":40076},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":7183629},\"end\":40867,\"start\":40477},{\"attributes\":{\"doi\":\"arXiv:1409.1556\",\"id\":\"b21\"},\"end\":41128,\"start\":40869},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":206592484},\"end\":41653,\"start\":41130},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":12552176},\"end\":42208,\"start\":41655},{\"attributes\":{\"id\":\"b24\"},\"end\":42395,\"start\":42210},{\"attributes\":{\"doi\":\"arXiv:1410.4615\",\"id\":\"b25\"},\"end\":42560,\"start\":42397},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":1166498},\"end\":42970,\"start\":42562}]", "bib_title": "[{\"end\":33341,\"start\":33316},{\"end\":33624,\"start\":33546},{\"end\":33953,\"start\":33884},{\"end\":34413,\"start\":34332},{\"end\":35058,\"start\":35043},{\"end\":35474,\"start\":35426},{\"end\":35723,\"start\":35685},{\"end\":36092,\"start\":36039},{\"end\":36437,\"start\":36381},{\"end\":36729,\"start\":36684},{\"end\":37157,\"start\":37108},{\"end\":37548,\"start\":37501},{\"end\":38140,\"start\":38100},{\"end\":38590,\"start\":38546},{\"end\":38879,\"start\":38824},{\"end\":39245,\"start\":39154},{\"end\":39723,\"start\":39670},{\"end\":40140,\"start\":40076},{\"end\":40561,\"start\":40477},{\"end\":41160,\"start\":41130},{\"end\":41713,\"start\":41655},{\"end\":42630,\"start\":42562}]", "bib_author": "[{\"end\":33357,\"start\":33343},{\"end\":33372,\"start\":33357},{\"end\":33641,\"start\":33626},{\"end\":33655,\"start\":33641},{\"end\":33965,\"start\":33955},{\"end\":33973,\"start\":33965},{\"end\":33984,\"start\":33973},{\"end\":34426,\"start\":34415},{\"end\":34441,\"start\":34426},{\"end\":34455,\"start\":34441},{\"end\":34467,\"start\":34455},{\"end\":34482,\"start\":34467},{\"end\":34492,\"start\":34482},{\"end\":34503,\"start\":34492},{\"end\":34897,\"start\":34886},{\"end\":34910,\"start\":34897},{\"end\":35070,\"start\":35060},{\"end\":35084,\"start\":35070},{\"end\":35094,\"start\":35084},{\"end\":35486,\"start\":35476},{\"end\":35497,\"start\":35486},{\"end\":35508,\"start\":35497},{\"end\":35738,\"start\":35725},{\"end\":35748,\"start\":35738},{\"end\":35760,\"start\":35748},{\"end\":35770,\"start\":35760},{\"end\":36103,\"start\":36094},{\"end\":36113,\"start\":36103},{\"end\":36452,\"start\":36439},{\"end\":36463,\"start\":36452},{\"end\":36475,\"start\":36463},{\"end\":36745,\"start\":36731},{\"end\":36760,\"start\":36745},{\"end\":36773,\"start\":36760},{\"end\":37168,\"start\":37159},{\"end\":37177,\"start\":37168},{\"end\":37188,\"start\":37177},{\"end\":37561,\"start\":37550},{\"end\":37572,\"start\":37561},{\"end\":37586,\"start\":37572},{\"end\":37967,\"start\":37958},{\"end\":37977,\"start\":37967},{\"end\":37988,\"start\":37977},{\"end\":38153,\"start\":38142},{\"end\":38163,\"start\":38153},{\"end\":38180,\"start\":38163},{\"end\":38189,\"start\":38180},{\"end\":38605,\"start\":38592},{\"end\":38614,\"start\":38605},{\"end\":38897,\"start\":38881},{\"end\":38909,\"start\":38897},{\"end\":38919,\"start\":38909},{\"end\":38928,\"start\":38919},{\"end\":39261,\"start\":39247},{\"end\":39272,\"start\":39261},{\"end\":39284,\"start\":39272},{\"end\":39294,\"start\":39284},{\"end\":39307,\"start\":39294},{\"end\":39318,\"start\":39307},{\"end\":39329,\"start\":39318},{\"end\":39734,\"start\":39725},{\"end\":39747,\"start\":39734},{\"end\":40153,\"start\":40142},{\"end\":40163,\"start\":40153},{\"end\":40174,\"start\":40163},{\"end\":40575,\"start\":40563},{\"end\":40586,\"start\":40575},{\"end\":40596,\"start\":40586},{\"end\":40612,\"start\":40596},{\"end\":40949,\"start\":40937},{\"end\":40962,\"start\":40949},{\"end\":41173,\"start\":41162},{\"end\":41180,\"start\":41173},{\"end\":41187,\"start\":41180},{\"end\":41199,\"start\":41187},{\"end\":41207,\"start\":41199},{\"end\":41219,\"start\":41207},{\"end\":41228,\"start\":41219},{\"end\":41241,\"start\":41228},{\"end\":41255,\"start\":41241},{\"end\":41730,\"start\":41715},{\"end\":41742,\"start\":41730},{\"end\":41749,\"start\":41742},{\"end\":41761,\"start\":41749},{\"end\":41771,\"start\":41761},{\"end\":41788,\"start\":41771},{\"end\":41799,\"start\":41788},{\"end\":41807,\"start\":41799},{\"end\":42239,\"start\":42225},{\"end\":42249,\"start\":42239},{\"end\":42262,\"start\":42249},{\"end\":42429,\"start\":42418},{\"end\":42442,\"start\":42429},{\"end\":42642,\"start\":42632},{\"end\":42652,\"start\":42642}]", "bib_venue": "[{\"end\":33407,\"start\":33372},{\"end\":33690,\"start\":33655},{\"end\":34064,\"start\":33984},{\"end\":34565,\"start\":34503},{\"end\":34884,\"start\":34843},{\"end\":35174,\"start\":35094},{\"end\":35543,\"start\":35508},{\"end\":35832,\"start\":35770},{\"end\":36188,\"start\":36113},{\"end\":36504,\"start\":36475},{\"end\":36843,\"start\":36773},{\"end\":37258,\"start\":37188},{\"end\":37664,\"start\":37586},{\"end\":37956,\"start\":37934},{\"end\":38267,\"start\":38189},{\"end\":38660,\"start\":38614},{\"end\":38959,\"start\":38928},{\"end\":39379,\"start\":39329},{\"end\":39829,\"start\":39747},{\"end\":40239,\"start\":40174},{\"end\":40648,\"start\":40612},{\"end\":40935,\"start\":40869},{\"end\":41339,\"start\":41255},{\"end\":41877,\"start\":41807},{\"end\":42223,\"start\":42210},{\"end\":42416,\"start\":42397},{\"end\":42718,\"start\":42652},{\"end\":34131,\"start\":34066},{\"end\":35241,\"start\":35176},{\"end\":36900,\"start\":36845},{\"end\":37315,\"start\":37260},{\"end\":37729,\"start\":37666},{\"end\":38332,\"start\":38269},{\"end\":39416,\"start\":39381},{\"end\":39898,\"start\":39831},{\"end\":40291,\"start\":40241},{\"end\":41410,\"start\":41341},{\"end\":41934,\"start\":41879},{\"end\":42771,\"start\":42720}]"}}}, "year": 2023, "month": 12, "day": 17}