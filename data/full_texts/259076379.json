{"id": 259076379, "updated": "2023-10-04 23:51:37.889", "metadata": {"title": "SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression", "authors": "[{\"first\":\"Tim\",\"last\":\"Dettmers\",\"middle\":[]},{\"first\":\"Ruslan\",\"last\":\"Svirschevski\",\"middle\":[]},{\"first\":\"Vage\",\"last\":\"Egiazarian\",\"middle\":[]},{\"first\":\"Denis\",\"last\":\"Kuznedelev\",\"middle\":[]},{\"first\":\"Elias\",\"last\":\"Frantar\",\"middle\":[]},{\"first\":\"Saleh\",\"last\":\"Ashkboos\",\"middle\":[]},{\"first\":\"Alexander\",\"last\":\"Borzunov\",\"middle\":[]},{\"first\":\"Torsten\",\"last\":\"Hoefler\",\"middle\":[]},{\"first\":\"Dan\",\"last\":\"Alistarh\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Recent advances in large language model (LLM) pretraining have led to high-quality LLMs with impressive abilities. By compressing such LLMs via quantization to 3-4 bits per parameter, they can fit into memory-limited devices such as laptops and mobile phones, enabling personalized use. However, quantization down to 3-4 bits per parameter usually leads to moderate-to-high accuracy losses, especially for smaller models in the 1-10B parameter range, which are well-suited for edge deployments. To address this accuracy issue, we introduce the Sparse-Quantized Representation (SpQR), a new compressed format and quantization technique which enables for the first time near-lossless compression of LLMs across model scales, while reaching similar compression levels to previous methods. SpQR works by identifying and isolating outlier weights, which cause particularly-large quantization errors, and storing them in higher precision, while compressing all other weights to 3-4 bits, and achieves relative accuracy losses of less than 1% in perplexity for highly-accurate LLaMA and Falcon LLMs. This makes it possible to run 33B parameter LLM on a single 24 GB consumer GPU without any performance degradation at 15% speedup thus making powerful LLMs available to consumer without any downsides. SpQR comes with efficient algorithms for both encoding weights into its format, as well as decoding them efficiently at runtime. Specifically, we provide an efficient GPU inference algorithm for SpQR which yields faster inference than 16-bit baselines at similar accuracy, while enabling memory compression gains of more than 4x.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2306.03078", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2306-03078", "doi": "10.48550/arxiv.2306.03078"}}, "content": {"source": {"pdf_hash": "51db4c39dc0bdf5c95c8bbe89bf4211b48d0b4df", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2306.03078v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "8d9c51ff1bac090a9c1ac25ea48c9d58ce54d2c4", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/51db4c39dc0bdf5c95c8bbe89bf4211b48d0b4df.txt", "contents": "\nSpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression\n\n\nTim Dettmers \nRuslan Svirschevski \nVage Egiazarian \nDenis Kuznedelev \nYandex &amp; Skoltech \nElias Frantar \nSaleh Ashkboos \nEth Zurich \nAlexander Borzunov \nTorsten Hoefler \nEth Zurich \nDan Alistarh \n\nUniversity of Washington\nHSE University\n& Yandex\n\n\nHSE University\n& Yandex\n\n\nIST\nAustria\n\n\nHSE University\n& Yandex\n\n\nIST\nAustria & NeuralMagic\n\nSpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression\n\nRecent advances in large language model (LLM) pretraining have led to highquality LLMs with impressive abilities. By compressing such LLMs via quantization to 3-4 bits per parameter, they can fit into memory-limited devices such as laptops and mobile phones, enabling personalized use. However, quantization down to 3-4 bits per parameter usually leads to moderate-to-high accuracy losses, especially for smaller models in the 1-10B parameter range, which are well-suited for edge deployments. To address this accuracy issue, we introduce the Sparse-Quantized Representation (SpQR), a new compressed format and quantization technique which enables for the first time near-lossless compression of LLMs across model scales, while reaching similar compression levels to previous methods. SpQR works by identifying and isolating outlier weights, which cause particularlylarge quantization errors, and storing them in higher precision, while compressing all other weights to 3-4 bits, and achieves relative accuracy losses of less than 1% in perplexity for highly-accurate LLaMA and Falcon LLMs. This makes it possible to run 33B parameter LLM on a single 24 GB consumer GPU without any performance degradation at 15% speedup thus making powerful LLMs available to consumer without any downsides. SpQR comes with efficient algorithms for both encoding weights into its format, as well as decoding them efficiently at runtime 3 . Specifically, we provide an efficient GPU inference algorithm for SpQR which yields faster inference than 16-bit baselines at similar accuracy, while enabling memory compression gains of more than 4x.\n\nIntroduction\n\nPretrained large language models (LLMs) improved rapidly from task-specific performance [WSM + 18, DCLT19,RWC + 19], to performing well on general tasks if prompted with instructions [BMR + 20,WBZ + 21,Ope23]. While the improved performance can be attributed to scaling in training data and parameters [KMH + 20,CND + 22] recent trends focused on smaller models trained on more data, that are easier to use at inference time [HBM + 22, BSA + 23, TLI + 23]. For example, the 7B parameter LLaMA model trained on 1T tokens achieved an average performance only slightly lower than GPT-3 [BMR + 20] despite being 25x smaller. Current techniques for LLM compression can shrink these models further by a factor of about 4x, while preserving their performance [ DLBZ22,XLS + 22,FAHA22,DZ22]. This yields performance levels comparable to the largest GPT-3 model, with major reductions in terms of memory requirements. With such improvements, well-performing models could be efficiently served on end-user devices, such as laptops.\n\nThe main challenge is to compress models enough to fit into such devices while also preserving generative quality. Specifically, studies show that, although accurate, existing techniques for 3 to 4-bit quantization still lead to significant accuracy degradation [DZ22,FAHA22]. Since LLM generation is sequential, depending on previously-generated tokens, small relative errors can accumulate and lead to severely corrupted outputs. To ensure reliable quality, it is critical to design low-bitwidth quantization that does not degrade predictive performance compared to the 16-bit model.\n\nIn this work, we introduce Sparse-Quantized Representations (SpQR), a hybrid sparse-quantized format which can compress accurate pretrained LLMs to 3-4 bits per parameter while staying nearlossless: specifically, SpQR is the first weight quantization method which is able to reach such compression ratios while inducing end-to-end accuracy error as measured in perplexity of less than 1% relative to the dense baseline. SpQR works by combining two innovations. First, we isolate outlier weights, whose quantization we show to induce disproportionately high errors: these weights are kept in high precision, while the other weights are stored in a much lower, e.g. 3-bit, format. Second, we implement a variant of grouped quantization with very small group size, e.g. 16 contiguous elements, but we show that one can quantize the quantization scales themselves to a 3-bit representation.\n\nTo convert a given pretrained LLM into SpQR format, we adopt an extended version of the posttraining quantization (PTQ) approach recently introduced by GPTQ [FAHA22]. Specifically, the method passes calibration data through the uncompressed model; to compress each layer, it applies a layer-wise solver with respect to the L2 error between the outputs of the uncompressed model, and those of the quantized weights. Our approach splits this process into two steps: an \"outlier detection\" step, in which we isolate weights whose direct quantization has outsize impact on layer output behavior, and an actual compression step, in which most (\u2265 99%) of weights are compressed to low-bitwidth, the outliers are extracted, and the whole representation is rendered more efficient by further compressing the quantization metadata.\n\nOur method is motivated by a new analysis showing that LLM weight quantization errors exhibit both vertical and horizontal group correlations, corresponding to systematic large errors corresponding to input feature dimensions and output hidden dimensions. While outlier input features have been observed before [DLBZ22,XLS + 22], our work is the first to demonstrate that similar outliers occur in the weights, for particular output hidden dimensions. Unlike input feature outliers, the output hidden dimension outliers occur only in small segments for a particular output hidden dimension.\n\nOur quantization algorithm isolates such outliers and efficiently encodes a given model in SpQR format. To exploit the resulting structure, we develop a specialized sparse-matrix multiplication algorithm based on the compressed sparse row (CSR) format. To use SpQR for token-by-token generation, we combine this sparse algorithm together with a dense-quantized matrix multiplication for 3-4 bit weights. With this, SpQR reduces the memory footprint of LLMs by a factor of about 3.4x or more without degradation in accuracy, measured as language modeling loss or perplexity, while also being 20-30% faster for LLM generation compared to 16-bit inference.\n\n\nRelated Work\n\nWe focus our discussion on related post-training quantization (PTQ) methods [NAVB + 20], referring the reader to the recent survey of Gholami et al. [GKD + 21] for full background on quantization. PTQ methods are a popular approach for one-shot compression of models with various sizes, based on a limited amount of calibration data, using accurate solvers, usually focused on layeror group-wise compression sub-problems. Most PTQ methods, such as AdaRound [NAVB + 20], BitSplit [WCHC20], AdaQuant [HNH + 21], BRECQ [LGT + 21], or OBQ [FSA22] were designed for vision models or small-scale language models, with less than 100M parameters. All these recent approaches tend to use accurate solvers, which would not scale to GPT-scale models in terms of computational or memory cost, as they are 10-1000x larger in size.\n\nRecently, there has been significant interest in obtaining accurate post-training methods that scale to such massive models. Due to computational constraints, early work such as ZeroQuant [YAZ + 22], LLM.int8() [DLBZ22], and nuQmm [PPK + 22] used direct rounding of weights to the nearest quantization level, while customizing the quantization granularity (i.e., group size) to trade off space for increased accuracy. LLM.int8() [DLBZ22] suggested isolating \"outlier features\" which would be quantized separately to higher bit-width. These approaches are able to induce relatively low quantization error, e.g. 5.5% relative LM Loss increase for LLaMA-7B at 4-bit weight quantization, provided that the quantization granularity is low enough. GPTQ [FAHA22] proposed a higheraccuracy approach (e.g., 4% LM Loss increase in the above setting), which works via an approximate large-scale solver for the problem of minimizing the layer-wise squared error.\n\nDettmers et al. [DZ22] provided an in-depth overview of the accuracy-compression trade-offs underlying these methods, establishing that 4-bit quantization is an optimal point for round-to-nearest-based methods, whereas higher compression can be achieved via data-aware methods such as GPTQ.\n\nSparseGPT [FA23] presented an approach to jointly sparsify LLM weights to medium sparsities, together with quantization of the remaining weights to a fixed given bit-width. One common drawback of existing methods is that the accuracy loss relative to the original model is still significant (see Table 1). This is especially relevant to relatively small but easily deployable models, e.g. in the 7-13B parameter range, where existing methods show drastic accuracy drops. We investigate this question here, and provide a new compression format which can lead to near-lossless 3-4 bits compression in this regime.\n\nA related question is that of performing both activation and weight quantization. There is early work [DLBZ22,XLS + 22,YAZ + 22], showing that both activations and weights could be quantized to 8-bits with relatively low accuracy impact. These complementary investigations yield interesting insights into the causes of compression error in the case of LLMs. Specifically, [DLBZ22,XLS + 22] observe the presence of \"outlier features\" with significantly higher values in the input/output of large LLMs, which induce higher quantization error, and propose different mitigation strategies.\n\nWe analyze this phenomenon from the point of view of weight quantization. In particular, we investigate the outlier structure, beyond input feature outliers in the weight matrix. While we find that input feature outliers of the current layer are correlated to hidden unit outliers weight in the previous layer there is not a strict correspondence. Such partially-structured outlier patterns necessitate a fine-grained hybrid compression format that goes beyond algorithms that exploit the column structure of outlier features found in previous work.\n\nHybrid sparse-quantized formats have been investigated generally for deep networks. Some efficient CPU inference engines [Neu22,GFS + 19] support a different block sparse-and-quantized format, in which each block of 4 consecutive weights is either completely sparse or quantized to 8-bit format, whereas GPUs support a similar compound format in which every group of 4 weights contains 2 zero weights, and the non-zero weights could be quantized. The FBGEMM package [KHB + 21] proposed a format in which certain \"outlier\" weights are quantized separately, to reduce their impact on normalization. However, in this format, \"outlier\" weights are still quantized to exactly the same bit-width (8-bit) as regular weights; moreover, no procedure is given for converting a model to this format post-training. By contrast, 1) we provide an efficient and accurate post-training compression algorithm which identifies outliers as weights inducing high output error, 2) we propose a format compressing outliers to a higher bit-width relative to regular weights, and 3) our format stores outliers in blocks, allowing for efficient implementation of GPU kernels, which we provide as well.\n\n3 Quantization sensitivity of LLM weights 3.1 Parameter sensitivity under quantization Not all parameters in a neural network are equally important. Intuitively, a weight could be seen as sensitive to quantization if its rounding error is large, i.e. it is not close to a quantization point, and/or the inputs it is usually multiplied with a large, amplifying even a small rounding error. These simple notions of sensitivity however disregard the fact that LLMs operate on very large vectors with significant correlations: a weight w a may have a large rounding error while being strongly correlated to another weight w b , meaning that the error of rounding up w a can be well compensated by rounding down w b . This idea is exploited by modern quantization algorithms [FAHA22,YAZ + 22] and can lead to major improvements over vanilla rounding, especially a low bitwidths. Properly capturing this aspect of sensitivity requires a more robust definition.\n\nFor computational tractability, we assess sensitivity on a per-layer level using a small set of calibration inputs X, collected by running them through the model up to the particular layer. We define the sensitivity s ij of some weight w ij in the layer's weight matrix W as the minimum squared difference between the original predictions on X and those of any weight matrix W \u2032 where this weight is quantized, i.e. w \u2032 ij = quant(w ij ):\ns ij = min W \u2032 ||W X \u2212 W \u2032 X|| 2 2 s.t. w \u2032 ij = quant(w ij )(1)\nCrucially, all weights of W \u2032 except for w \u2032 ij may take on arbitrary, not necessarily quantized, values in order to compensate for the quantization error incurred by rounding w ij , thus capturing the correlation aspect discussed above. Further, as we allow continuous values, this problem admits a closed-form solution. This can be determined by following the generalized Optimal Brain Surgeon framework [FSA22], where (XX \u22a4 ) \u22121 is the inverse Hessian matrix corresponding to the optimization problem:\ns ij = (w ij \u2212 quant(w ij )) 2 2(XX \u22a4 ) \u22121 .(2)\nThis saliency measure can be approximated efficiently by quantization solvers, such as GPTQ [FAHA22]. In more detail, GPTQ quantizes weight matrices column-by-column while in each step adjusting the not-yet-quantized part to compensate for the quantization error in a similar sense as defined above. Consequentially, instead of statically deciding all sensitivities in advance, they can be computed dynamically as the algorithm processes each column, by using the inverse of the Hessian subselection corresponding to all not yet quantized weights. This matrix is already efficiently computed by GPTQ and thus does not impose any additional overheads. The main advantage of this approach is that s ij is always determined based on the most current value of w ij and thus accounts for adjustments due to previously quantized weights as well.\n\n\nExploring parameter sensitivity\n\nBefore we define out main method, SpQR, we provide a motivating analysis of parameter sensitivity which uncovers that the location of sensitive weights in the weight matrix are not random but have particular structures. To highlight these structural elements during the quantization process, we calculate the the per-weight sensitivities and visualize them for the popular and highly-accurate LLaMA-65B model [TLI + 23]. As the quantization method, we use GPTQ quantization to 3-bit, without weight grouping, following [FAHA22]. We use C4 [RSR + 20] as the calibration dataset, and we estimate the error on 128 sequences of 2048 tokens each. Figure 2 depicts the output projection of the last self-attention layer of LLaMA-65B.\n\nUsing the sensitivity analysis, we observe several patterns in the weight matrix, often in a single row or column. Since the large weight matrices in LLaMA-65B have too many rows/columuns to be respresentable in a compact image (default: 8k \u00d7 32k pixels) we perform max pooling to visualize the matrices, that is we take the maximum sensitivity in each square of 32 \u00d7 32 rows and columns. This max pooling only affects the leftmost image. Using this visualization, we observe that the quantization error patterns vary both by layer type, for example attention vs multilayer perceptron (MLP), and layer depth. In particular, we find that more sensitive outliers are present for deeper layers.\n\n(Please see Appendix A for additional results.) We now proceed to categorize outlier structures, taking this attention weight matrix as an exemplar. We make the following observations: Column outlier pattern \u2022 Row outliers are shown in Figure 2 bottom-center as regions of high sensitivity within one output unit. Some of these patterns span the entire row, while others are partial. In attention layers, some of the partial row outliers correspond to some subset of attention heads. Column outliers appear in Figure 2, bottom-right, showing high sensitivity in select input dimensions (columns) across all rows. The latter are correlated to the \"outlier feature\" phenomenon reported in Dettmers et al. [DLBZ22].\n\n\u2022 Sensitive attention heads. (Figure 2, top-center) -regular stripes of width 128 highlight all weights corresponding to one attention head. This could be related to some attention heads having more important functions [VTM + 19, Vig19, OEN + 22]. The corresponding \"stripes\" are horizontal for attention Q & K projections, vertical in output projection, and absent from value projections and any MLP weights. Of note, there is significant variation in individual weight sensitivity even within the sensitive heads.\n\n\u2022 The Rotary embedding pattern, a repeating vertical pattern of sensitivity with a period of 64 units. We attribute this to the use of rotary embeddings [SLP + 21]: each attention head (dim = 128) is split into two halves: the first 64 are \"rotated\" with cosine, and the other 64 use sine. Both sine and cosine rotation use the same set of frequencies. Typically, the weights that correspond to low-frequency sines and cosines are more sensitive than their high-frequency counterparts, as shown in Figure 2 (top-right). As expected, this pattern is absent from any layer not using rotary embeddings.\n\n\u2022 Unstructured outliers. Besides the above, each layer has a number of individual sensitivity weights that do not fit into any of the above patterns. These unstructured outliers occur more frequently for columns with largest input index (i.e. on the right side of the images). This effect is difficult to see on a heatmap, so we provide additional figures and statistical tests in Appendix A. We believe is probably an artefact of the GPTQ algorithm, which compresses one by one, using yet-uncompressed weights to compensate the error. Thus, the rightmost batch of weights accumulates the most error.\n\nNext, we will leverage these findings to propose a compressed representation which can support all these different outlier types.\n\n4 SpQR: A Sensitivity-aware compressed representation\n\n\nOverview\n\nExisting LLM quantization algorithms treat low-and high-sensitivity weights equally; however, our above discussion suggests that this may lead to sub-optimal quantization. Ideally, we would want the representation to assign more of its \"size budget\" to sensitive weights. However, these weights are scattered in the weight matrix as either individual weights or small groups, for example, partial rows or attention head. To capture this structure, we are introducing two changes to the quantization procedure: one for capturing small sensitive groups, and another for capturing individual outliers.\n\nCapturing small groups of weights with bilevel quantization. In the previous section, we observed several cases where weights behave similarly in small consecutive groups, with abrupt changes between groups, for example for some attention head and partial row outliers (see Figure 4 left, bottom-center). When applying a standard approach, there will be many cases where these weights will be grouped together, sharing the same quantization statistics. To reduce the number of such cases, we use groupwise quantization with extremely small groups, typically of \u03b2 1 =8 \u2212 32 weights. That is, for every \u03b2 1 consecutive weights, there is a separate quantization scale and zero-point. This choice runs contrary to current intuition: for instance, the recent work of Yao et al.\n\n[YLW + 23] explicitly recommends against small groups, arguing that the overhead for storing quantization statistics would outweigh the precision advantages.\n\nTo circumvent this issue, we quantize the groupwise statistics themselves using the same quantization algorithm as for weights -asymmetric (min-max) quantization. Because of how min-max quantization works, the range of quantized values will fit to the groups with largest (or smallest) quantization scale, quantizing them perfectly. In other words, we group groupwise statistics from \u03b2 2 = 16 consecutive values and quantize them together in the same number of bits, such that groups with atypical quantization parameters end up using more of the \"quantization budget\". Finally, both first and second-level quantization is directly within the quantization process, allowing the algorithm to compensate the second-level quantization error where possible.\n\nHigh-sensitivity outliers. Our analysis showed the existence of cases where a small percentage of sensitive weights come in small groups (in the self-attention) or individual \"outliers\" (in the MLP). In some cases, 1% of the weights account for over 75% of the total quantization error. Since these weights appear to lead to high, irreducible error, we choose to keep these outliers in high precision (16-bit). As these outliers are often unstructured, we encode them individually in a rowwise arrangement similar to a compressed-sparse-row (CSR) representation [HABN + 21]. This can encode both individual outliers and small structures that do not fit into the above definition of groups.\n\nThe procedure for detecting the outliers is described in detail in Alg. 1. If follows a rough two-step procedure: (1) find and isolate outliers as 16-bit weights, (2) quantize the non-outlier \"base\" weights into 3-4 bit and transfer the remaining quantization into the the 16-bit outliers weights. For the outlier isolation step, the algorithm implements a filtering technique based on the sensitivity criterion in Eq.\n\n(2), which is used to isolate and separate outliers from base weights. Globally, for each matrix, the algorithm aims to pick a sensitivity threshold \u03c4 to obtain the desired number of outliers across the whole model, usually around 1% of weights. Specifically, a particular weight is considered an outlier if keeping the weight in 16-bit reduces the error in Eq. (2) by at least \u03c4 .\n\nFollowing this first outlier detection step, we quantize the base weights ignoring all outliers that occur in the same quantization group. As such, the quantization statistics (e.g. scales) are computed by excluding outliers. This results in significant improvements in terms of error, since e.g. the min-max scales will be significantly reduced. The algorithm then proceeds to apply GPTQ to quantize the remaining weights. Interestingly, unlike [DLBZ22], a weight can be chosen to be an outlier not only if it causes error by itself, but also if the GPTQ algorithm can employ this weight to compensate errors from many other weights. Thus, the resulting 16-bit value will contain not the original weight, but a weight that was adjusted to minimize the output error. As such, SpQR goes beyond mere detection of outliers towards the more general notion of isolating and treating outliers that occur during the quantization process. Finally, the algorithm gathers and compresses sparse outlier matrix as well as the final quantization statistics with bilevel quantization and returns the compressed weights and their metadata.\n\nImplementation details. Our algorithm also contains several optimizations. As we are using small group sizes, it is often the case that a group contains all positive (or all negative) values. Standard quantizers [FSA22,FAHA22] require the maximum value to be positive and the minimum value to be negative. For small group sizes, removing this requirement results in slightly better quality. As a by-product of quantizing the quantization statistics, our algorithm allows non-integer zero points. We ablate these and other SpQR components in Section 5.\n\nAlgorithm 1 SpQR quantization algorithm: the left snippet describes the full procedure, the right side contains subroutines for bilevel quantization and finding outliers.\n\nfunc SPQRQUANTIZE(W, X, b, \u03b21, \u03b22, \u03c4, \u03bb) Input: W \u2208 R m\u00d7n -weight matrix, X \u2208 R n\u00d7d -calibration data, b -the base number of quantization bits, \u03b21, \u03b22 -quantization group sizes, \u03c4 -sensitivity outlier threshold \u03bb -hessian regularizer, for j = i, . . . , i + \u03b21 do 11:\n\nQ:,j := quantize(W:,j,\u015d,\u1e91) 12:\n\n\u20d7 wq := dequantize(Q:,j,\u015d,\u1e91) 13:  \nE:,j := (W:,j \u2212 \u20d7 wq)/H ic j,j \u00b7 (1 \u2212 is_outlier(W:,j, O)) 14: W :,j:(i+\u03b2 1 ) := W :,j:(i+\u03b2 1 ) \u2212 E \u00b7 H ic j,j:(i+\u03b2 1 ) 15: W :,(i+\u03b2 1 ):n := W :,(i+\u03b2 1 ):n \u2212 E \u00b7 H ic i:(i+\u03b2 1 ),i:(i+\u03b2 1 ) 16: Sq,func quantize(M, \u20d7 s, \u20d7 z) 1: return \u230aM/\u20d7 s + \u20d7 z + 0.5\u230b func dequantize(Q, \u20d7 s, \u20d7 z) 1: return \u20d7 s \u00b7 (Q \u2212 \u20d7 z) func fit_quantizer(M, \u03b2) 1: \u20d7 m := flatten(M ) 2: \u20d7 s, \u20d7 z := vectors() 3: for i = 1, \u03b21, 2\u03b21, . . . dim(m) do 4: si := max( \u20d7 m i:i+\u03b2 )\u2212min( \u20d7 m i:i+\u03b2 ) 2 b \u22121 5: zi := \u2212min( \u20d7 m i:i+\u03b2 )/si 6: return \u20d7 s, \u20d7 z func error(W, H ic ) 1: \u20d7 s, \u20d7 z := fit_quantizer(W, \u03b21) 2: Wq := quantize(W, \u20d7 s, \u20d7 z) 3: E := (W \u2212 Wq)/H ic 4: return E 2 func outliers(W, H ic , O) 1: Ebase = error(W, H ic ) 2: for i = 1, . . . , \u03b21 do 3: loo := {1, 2, ..., \u03b21}/{i} 4: Eol = error(W:,loo, H ic loo,loo ) 5: Io = select(Ebase \u2212 Eol > \u03c4 ) 6: O := O \u222a Io 7: return W, O func fit_statistics(W, S, O) 1: W := W \u00b7 (1 \u2212 is_outlier(W, O)) 2: \u20d7 s, \u20d7 z := fit_quantizer(W,\n\nImplementing and Leveraging the Sparse Quantized Representation\n\nOur algorithm converts homogeneous weights into several data structures of various sizes and precisions. Overall, the representation consists of (1) quantized weights, (2) first level quantized quantization statistics, second level quantization statistics, and (3) the CSR outlier indices and values. We summarize the overall structure of SpQR in Figure 3 and describe each component below.\n\nStoring quantized groups. All non-outlier weights are encoded as a structure that contains:\n\n\u2022 a b w -bit individual weight;\n\n\u2022 a b q -bit scale and zero point for each group of size B;\n\n\u2022 16-bit statistics for quantizing groups of B q quantization scales and zero-points.\n\nAs a particular example for a SpQR representation, consider b w =b q =3 and B w = B q = 16. The weight matrix is split into groups of B q \u00d7 B w = 256 weights. A group contains 256 individual b w = 3-bit codes. Every 16 weights use a separate 3-bit scale and zero-point. Finally, there are four 16-bit scalars for the entire group used for second level quantization. To simplify GPU memory access, we keep the quantized values for outlier weights in place and adjust the 16-bit versions to compensate for that. We also store both quantized weights and quantized quantization statistics in a contiguous memory region for each group. When running on a different hardware (e.g. mobile CPUs), it is possible to further reduce the memory footprint by removing the quantized version of outliers. We leave this direction for future work.\n\nStoring outliers. Recall that our outliers are unstructured; for storage, we sort them by their row first and column second, so that outliers in the same row are contiguous in memory. For each outlier, we store two scalars: the 16-bit weight value and the 16-bit column index. For each row, we also store a single 32-bit number-the total number of outliers in the rows up to the current one for efficient inference. This results in an average storage cost of 32.03 to 32.1 bits per sensitive weight. This could be reduced significantly by grouping outliers, which we leave as future work.\n\nInference with SpQR. To illustrate the practicality of our approach, we design an efficient GPUbased decoding implementation for the SpQR format, focused on the popular token-by-token LLM generation as a use-case.\n\nWe leverage the fact that autoregressive inference on GPUs is memory-bound, so high compression rates can hide decoding overheads, to a significant extent. At a high level, our algorithm loads group statistics and the quantized weights into shared memory (SRAM), dequantizes to 16-bits, and then performs matrix multiplication with 16-bit inputs. For handling outliers, we design a sparse matrix algorithm that takes advantage of outliers that occur in rows. Roughly, the algorithm works as follows First, (1) we divide the matrix into equally sized blocks. Then, each GPU core (thread block) (2) loads a large slice of outliers into shared memory (SRAM), and each GPU core (3) determines if outliers are part of the segment or not. The corresponding weights are (4) loaded from main memory; finally, the matrix multiplication is performed.\n\nThis algorithm essentially performs load balancing through steps (1-3), while step (4) tends to have contiguous memory access due to the row-like patterns for the outliers. We will show in Section 5 that this custom approach is faster than the sparse matrix algorithms in PyTorch.\n\n\nExperimental Validation\n\nExperimental setup. We focus on three main settings: 1) evaluating what is the most compact representation with which SpQR can replicate the performance of a 16-bit model within 1% perplexity, 2) controlling for the average number of bits per parameter across methods and assess the performance of SpQR compared to round-to-nearest and GPTQ baselines, 3) what is the best trade-off in terms of model size and performance. For these settings, we evaluate the full SpQR algorithm on publicly-available LLMs. We focus on the LLaMA {7, 13, 30, 65}B model family [TLI + 23] and Falcon{7, 40}B model family [UAE23a]. We quantize LLaMa models using the RedPajama dataset and Falcon models on RefinedWeb dataset [UAE23b], publicly-available replicas of the LLaMA and Falcon training data, respectively. In addition, we provide perplexity results for OPT models in Appendix F.\n\nWe compare SpQR against two other post-training quantization schemes: GPTQ [FAHA22] and simple rounding-to-nearest (RTN) quantization, which is used by most other LLM compression methods [DLBZ22,YAZ + 22]. Both baselines use 4-bit quantization since it provides the best quality to size trade-off [DZ22]. For SpQR, we consider both 3-bit and 4-bit base quantization, though the resulting model size can be slightly larger due to the presence of outliers. for SpQR and round-to-nearest (RTN) and GPTQ baselines with LLaMa. We can see that SpQR reaches performances within 1% of the perplexity with less than 4.71 bits per parameter. We also see that for 4-bits per parameter SpQR significantly improves on GPTQ with an improvement as large as the improvement from RTN to GPTQ.\n\nrecommended parameters. We provide full configurations in Appendix B, as well as code which we plan to release publicly. Our implementation takes around 4.5 hours on the largest model size (65B) on an NVIDIA A100 and about 6 on an A6000.\n\nTo control for model size, we evaluate RTN and GPTQ with 4-bit base quantization. For SpQR we use 3-bit base quantization, a group size of 8 with 3-bit for the first quantization, a group size of 64 for the second quantization, and as many outliers as possible to still reach less than 4-bits per parameter on average. We aim to achieve near-lossless compression, for which we adopt the definition of the MLCommons benchmark [RCK + 20]: 1% error relative to the uncompressed baseline. In all SpQR evaluations, we choose \u03c4 such that the proportion of outliers is under 1%.\n\nMain Results. Figure 1 measures actual model size versus perplexity on LLaMa models on WikiText2, and accuracy on zero-shot tasks. We observe that SpQR outperforms GPTQ (and correspondingly RTN) at similar model size by a significant margin, especially on smaller models. This improvement comes from both SpQR achieving more compression, while also reducing loss degradation. In addition, if we measure the bits per parameter needed to come within 1% of the 16-bit performance in terms of perplexity, Figure 1 shows that SpQR with 4.6 to 4.71 bits per parameter approaches the non-quantized models with at most 1% margin of error for all models (see Table 1 and Table 2 for exact values).\n\nThe second set of results, presented in Table 1 for LLaMa and Table 2 for Falcon family models, controls model size by comparing SpQR and baseline methods with 4 bits per parameter. These results show that SpQR improves over previous methods, with the gap between SpQR and the next best method GPTQ being as large as the improvement of GPTQ over naive RTN. For 4-bit, SpQR halves the error relative to the 16-bit baseline compared to GPTQ.\n\nAblations. The SpQR representation differs from standard quantization methods in two main ways: bilevel quantization with small quantization group size and unstructured outliers. To understand the effect of small group sizes, we compare 3-bit SpQR with group size 16, compressed using 3-bit bilevel quantization, versus a setup with group size 48, keeping quantization statistics in 16-bit. Both configurations result in approximately 3.6 average bits per parameter. For simplicity, neither uses outliers. We report both in Table 3, the \"3-bit statistics\" entry corresponds to group size 16 with 3-bit statistics and \"16-bit statistics\" stands for group size 16 with 16-bit statistics. Given the same (slightly smaller) memory footprint, using quantized statistics significantly improves language modeling loss.\n\nNext, we ask whether it is necessary to use unstructured outliers, considering two outlier types. First, we use the criterion of Dettmers et al. [DZ22] to find column outliers and quantize them in higher precision. The alternative is to treat the entire rows (output units / hidden units / neurons) as outliers: we run SpQR without outliers, then select k output units that have the highest quantization error (i.e.    MSE between layer predictions) and treat the entire rows as 16-bit outliers. We compare the three outlier types on top of 3-bit SpQR and report the results in Figure 4. Overall, unstructured outliers reduce perplexity significantly faster than their row counterpart and the criterion of [DZ22], even after accounting for the different memory footprint.\n\nFinally, we analyze the impact of the minor hyperparameter changes that we introduced at the end of Section 4. In Table 3 (bottom), we evaluate quantization errors without these changes. The \"Round zero\" entry corresponds to a version of SpQR where the zero-point is a 3-bit integer. This reduces the memory footprint of SpQR, but results in a moderate increase in perplexity. Similarly, we evaluate SpQR without the \"act order\" flag. This option re-orders the input dimensions by the diagonal of the inverse hessian, which was introduced as a part of the GPTQ algorithm. Using this heuristic slightly improves loss, though not as much as from quantized groups.\n\nTo summarize, both small quantized groups and unstructured outliers independently improve perplexity and perform better than alternative strategies. SpQR also benefits from using the GPTQ activation order heuristic, though the gain is smaller than from outliers or small groups. Still, we opt to use the same activation order heuristic in the GPTQ baselines to ensure a fair comparison. To further explore the design space of SpQR, we provide an additional hyperparameter study in Appendix C.\n\nInference Time. Finally, we evaluate the inference speed of SpQR for autoregressive inference with a focus on measuring the token generation latency with batch size 1 on a single A100 GPU. We measure inference speed in two setups: i) generating 100 tokens from scratch and ii) adding 100 tokens on top of a 1024-token prefix (prompt). We compare our specialized sparse matrix multiplication algorithm with the algorithm implemented in PyTorch (cuSPARSE). We also compare against a 16-bit baseline. We measure the end-to-end latency as inference steps per second for the full SpQR algorithm, that is for both the dense and sparse multiplication part together.\n\nResults are shown in Table 4. We can see that while standard sparse matrix multiplication in PyTorch is not faster than 16-bit inference, our specialized sparse matrix multiplication algorithm yields speedups of about 20-30%. .5 22 \u00b1 0.9 12 \u00b1 0.6 prefix 1024 46 \u00b1 2.4 31 \u00b1 0.9 17 \u00b1 0.8 OOM 27 \u00b1 1.6 21 \u00b1 1.1 6.5 \u00b1 0.7 OOM 55 \u00b1 2.1 37 \u00b1 0.8 22 \u00b1 1.3 11 \u00b1 0.6 Table 4: Inference speed comparison (tokens/s), OOM means the model did not fit in an A100 GPU. We see that our optimized SpQR algorithm is faster than the 16-bit baseline and almost 2.0x faster than quantized matrix multiplication + standard PyTorch sparse matrix multiplication baseline.\n\n\nDiscussion & Limitations\n\nWe have presented SpQR, an quantization approach which quantizes sensitive outliers in higher precision, to achieve near-lossless 16-bit accuracy with less than 4.75 bits per parameter on average. We achieve even better quality-size-tradeoff when compressing to as little as 3.36 bits which makes SpQR an ideal method for compressing models for memory-limited devices. Despite our promising results, there are several limitations. The main limitation is that we do not evaluate the generative quality of quantized LLMs, but only the predictive performance in terms of zero-shot accuracy and perplexity. While we believe that perplexity measurements and generation quality are strongly related, this is a hypothesis we aim to investigate in future work. While we devise a sparse matrix multiplication algorithm to accelerate the computation with outliers, another limitation is that we do not fuse sparse matrix multiplication with regular quantized matrix multiplication. Such an approach would yield even better inference time performance. However, such an approach is also very difficult to implement. We leave the implementation of such an algorithm to future work. A Additional weight sensitivity analysis\n\nIn this section, we provide additional visualizations of LLaMA weight sensitivities, as well as additional plots for different layer roles. As we observed earlier in Section 3.2, the sensitivity matrices vary based on four main factors:\n\n\u2022 the quantization scheme (e.g. row-or group-wise);\n\n\u2022 the layer depth, i.e. the index of the corresponding transformer block;\n\n\u2022 the role of that weight, e.g. self-attn query / key or MLP up / down projection;\n\n\u2022 the location within the chosen weight matrix;\n\nHere, we report additional observations about these factors and elaborate on some of our claims from Section 3.1. We also report raw sensitivity matrices for various weight matrices at the end of the supplementary materials.\n\nRelation between sensitivity and the chosen quantization scheme. We compare two configurations of GPTQ 3-bit. The first configuration uses one quantization scale & zero for each row. The second one uses blockwise quantization with one set of statistics for each block of 128 weights. than the rest of the layer. Please note that the color scale represents sensitivity on a logarithmic scale, with higher sensitivity being darker.\n\nOn a more detailed examination, we found that this specific group contains a \"vertical\" outlier, i.e. the corresponding input feature has significantly higher variance, compared to other input dimensions.\n\nIn this example, the main effect of GPTQ block size 128 is that the problematic dimension leads to increased sensitivity in a group of 8192 \u00d7 128 weights. In turn, GPTQ with per-row statistics has high quantization error across the entire row.\n\nThe effect of rotary embeddings. Earlier in Figure 2 we note that attention query and key have a regular pattern of sensitivity that repeats every 64 rows. We attribute this to the fact that LLaMA uses rotary position embeddings. More specifically, this pattern is likely a side-effect of how rotary embeddings are implemented for this model.\n\nTo recall, rotary position embeddings are a technique that rotates attention head dimensions by an angle that depends on how many tokens are between key and query [SLP + 21]. Furthermore, dimensions within each head are rotated with a different frequency. To implement this rotation, LLaMA multiplies each head by a precomputed tensor of sine and cosine functions with a different period. The first half (64 units) of the matrix is multiplied by cosines and the other half (64 units) is multiplied by sines.\n\nTo recall, sine and cosine components are equivalent up to a phase shift and show similar behavior in our analysis. In general, we observe that weights that correspond to low-frequency heads (bottom of each semi-head) typically have higher sensitivity. One possible explanation is that high-frequency heads can be more dependent on position-specific information, such as attending to the previous token -and less dependent on the weights that represent content information. However, this phenomenon merits further investigation and our current understanding should be treated as an educated guess.\n\nGPTQ and the effect of quantization order. As we observe earlier in Section 3.2, the rightmost weights in each visualization tend to have higher quantization errors. This is likely a side-effect of the GPTQ algorithm, which compresses weights one input feature at a time, i.e. column by column in a left-to-right direction. Once a column is quantized, the algorithm uses the remaining unquantized weights to compensate for the error. Thus, the rightmost batch of weights accumulates the most error from preceding columns and has the least space to compensate it's \"own\" quantization error.\n\nThis difference is most pronounced in the earlier layers, where the quantization error is smaller overall (see Figure 6). To further verify this observation, we observe that this effect disappears if we shuffle the weight quantization order in the GPTQ algorithm. Quantization error distributions first 100 columns last 100 columns Figure 6: The weight log-sensitivities for a deeper upward projection layer (in particular, this is layer #79). The heatmap on the left represents the sensitivities of each weight, with darker being more sensitive; the histogram on the right captures the sensitivities in the first 100 and last 100 columns (sorted across input dimensions). The latter figure clearly shows that later columns are more sensitive on average.\n\nRelation between weight sensitivity and layer depth. In terms of mean squared error, we observe that the first layers of LLaMA tend to have generally lower OBC error (defined as L2 distance between original and quantized layer predictions). To illustrate this, we report the average quantization error of GPTQ-3bit in Figure 7. The absolute quantization error means little by itself since each quantized layer has a different input/output variance. However, we also observe that the first and last few layers have qualitative differences in behavior. Figures 10 and 11 report weight sensitivities for the first, middle (40th), and last (79th) layer of LLaMA model separately to better illustrate this difference.\n\n\nB Experimental Configurations\n\nThe SpQR representations proposed in this work have several adjustable hyperparameters that allow for great flexibility in targeting a desired size of the model. We introduce the notation and list the method hyperparameters below:\n\n\u2022 b w -number of bits per weight \u2022 b s -number of bits per scale \u2022 b z -number of bits per zero \u2022 r o -outlier rate (fraction of weights that are not quantized) \u2022 \u03b2 1 -block size for weight quantization \u2022 \u03b2 2 -block size for statistic quantization;\n\n\u2022 \u03c4 -outlier threshold\n\nThe actual number of outliers depends not only on \u03c4 , but on all other hyperparameters as well. However, for any specific configuration, increasing \u03c4 leads to reduced number of outliers. To achieve the desired number of outliers, we tune \u03c4 in [0.1, 1.0] range by binary search with minumum step size 0.05. The vast majority of our configurations are between \u03c4 = 0.1 and \u03c4 = 0.45].\n\nThe full configuration we use to compress LLaMA-30B model near-losslessly in Table 1 \n\n\nC Hyperparameter sensitivity\n\nIn this section, we analyze how SpQR performance depends on the choice of quantization group sizes. Please recall that the SpQR algorithm uses two types of groups, indexed by parameters \u03b2 1 and \u03b2 2 . The first group dimension \u03b2 1 covers multiple weights for the same input unit, similar to standard blockwise quantization. In turn, the other dimension \u03b2 2 covers multiple output units, and is used when quantizing quantization scales. In our visualizations, \u03b2 1 blocks are always horizontal, while \u03b2 2 are vertical.\n\nIn Table 5, we evaluate SpQR with varying parameters \u03b2 1 and \u03b2 2 . We quantize LLaMA-65B with 3-bit SpQR for weights and statistics and report perplexity on WikiText2, Penn Treebank, and C4 datasets. The upper-left section of the table contains the effective number of bits for each group configuration, and the remaining sections correspond to perplexities on different datasets.\n\n\nD Estimating model size\n\nIn this section, we provide a quick way to estimate the compressed model size before running the quantization. We express this estimate in terms of average bits per parameter defined as: b = model size in bits number of parameters\n\nWhere model size in bits denotes the total amount of memory -the quantized weights, 1st-order and 2nd-order quantization statistics, outliers and the outlier index -required for the storage of the model. According to Section 4.2, each outlier requires memory storage of \u223c 32 bits.  The storage and computational cost in transformer models are dominated by the linear projections in the attention and feedforward blocks. Consider quantization of a weight matrix (any of these)\n\nR dout\u00d7din with input dimension d in and output dimension d out . Then the average number of bits for a given configuration is:\nb \u2243 b w d out d in + (b s + b z ) doutdin \u03b21 + 2(16 + 16) doutdin \u03b21\u03b22 d out d in +32r o = b w + b s + b z \u03b2 1 + 64 \u03b2 1 \u03b2 2 +32r o (4)\nTherefore, to increase (decrease) the size of the model one should either increase (decrease) the precision of model weights and quantization statistics or decrease (increase) the block size.\n\nFor example, for configuration with b w = 3, b s = 3, b z = 3, \u03b2 1 = 16, \u03b2 2 = 32 and 0.4% of outliers, the average number of bits is:\n\n3 + 3 + 3 16 + 64 16 \u00b7 32 + 0.004 \u00b7 32 \u2243 3.63 E Choice of optimal configuration for fixed average number of bits As discussed above our method has multiple options for improvement of model performance at the cost of the increase of the model size: number of bits per weight w b , groupsizes b 1 and b 2 for 1st and 2nd order quantization and the outlier rate. We evaluated several configurations with various options for the aforementioned parameters on perplexity benchmarks. Results are presented on Figure 8. One can observe that small groups and small fraction of outliers allows to considerably improve model performance, but the gain is diminishing with the number of bits added (when the additional budget from small group is of order 0.1-0.5 of bits per parameter). It is better to store weights in higher precision instead of keeping them in lower precision but with very small groups or keeping large fraction of outliers. In our experiments optimal fraction of outliers is 0.2-0.5% depending on the model and groupsize. Perplexity on WikiText2  SpQR and round-to-nearest (RTN) and GPTQ baselines with OPT. We can see that SpQR reaches performances within 1% of the perplexity with less than 4.3 bits per parameter. We also see that for 4-bits per parameter SpQR significantly improves on GPTQ with an improvement as large as the improvement from RTN to GPTQ.\nw b =2 w b =3 w b =4\n\nF Additional results for near-lossless compression\n\nIn this section we report the list of quantization configurations for OPT in Table 6 on WikiText2, Penn Treebank, and C4 datasets.\n\nIn addition we report results for LM eval harness for LLaMa Table 7. and recently released Falcon models -Falcon-7B and Falcon-40B Table 8.\n\nG Choice of optimal LLM configuration for specific hardware\n\nIn the preceding discussion, we were searching for optimal model configuration given some compression target without targeting any specific hardware or device. However, the question practitioner willing to deploy a model for a specific application would ask is: What is the best model and compression setup for a given memory constraint?\n\nIn this section, we provide a list of recommendations for the choice of the best LLaMA model and the corresponding compression level that fits into the device memory (RAM or VRAM) without the need of offloading model parameters and activations. We cover a range of available budgets from mobile devices to high-end workstation GPUs. Recommendations are presented in Table 9.  \n\n\nH Sensitivity to random seed\n\nThe experiments we report throughout Section 5 use one fixed random seed (the default value from the supplementary code). To verify that our results are robust to randomness, we run SpQR with 5 random seeds (0-5) and measure the adjusted standard deviation.\n\nFor this evaluation, we compress LLaMA-65B with SpQR using b w = b z = b s = 3 and \u03b2 1 = \u03b2 2 = 16, which corresponds to 3.625 bits per parameter. The resulting perplexity scores are 3.75 \u00b1 0.003 (WikiText2), 7.03 \u00b1 0.01 (Penn Treebank) and 5.75 \u00b1 0.00086 (C4). In addition to the chosen random seed, these standard deviations can be affected by the inherent nondeterminism of GPU computation. Overall, the standard deviations are at least one order of magnitude smaller than the difference between SpQR, GPTQ, and RTN.\n\n\nI Generative examples\n\nFinally, we showcase several examples of how SpQR quantization affects the generated samples. For this evaluation, we take several prompts and use the compressed language model to continue generating text from these prompts. We compare the original LLaMA-65B and two quantized versions: SpQR and RTN-4bit. More specifically, we use the SpQR configuration that corresponds to near-lossless compression from Table 1. We use greedy autoregressive inference for all generated samples to ensure reproducibility. The examples in Figure 9 show that all models produce a valid text, but SpQR matches the 16-bit model more frequently. The near-lossless algorithm also seems to produce more semantically similar texts.\n\n\nJ Broader impact\n\nOur method enables the deployment of high-quality LLMs in the 7-13B parameters range to memorylimited devices such as laptops and phones. With our method, it is possible to develop specialized 7B LLMs in hassle-free 16-bit and then enable the deployment of such LLMs to phones by applying SpQR. Since SpQR is practically lossless, this ensures a reliable performance level for deployed LLMs which is important for consumer applications. Since mobile phones are ubiquitous and LLMs powerful general-purpose tools, SpQR might have a wide-reaching effect on how LLMs are used by the general population to complete useful tasks.\n\nLLMs are inherently a dual-use technology that can bring both significant benefits and serious harm. The ethical and societal risks of LLMs range from deliberate malicious use (e.g. generating spam) and accidental misuse to adverse economic side-effects [WMR + 21]. However, we believe that the marginal impact of SpQR will be positive or neutral since the LLMs we use are already openly available. Better quantization algorithms like SpQR let users with low-end devices run larger and generally more accurate language models. In other words, our algorithm does not create models with new capabilities (and risks): it only makes existing models more accessible.\n\n\nK On the use of LLMs in this work\n\nFollowing the request in this year's call for papers, we describe the use of large language models in our paper. We used two different chat-based language models: ChatGPT and Claude+. We used these models to accelerate the process of writing LaTeX code in Alg. 1 and Figure 3 (via Tikz). We also used these LLMs to provide slight improvements to the table design throughout the paper.\n\nIn addition to this, we use ChatGPT to generate some prompts for Appendix I. Finally, we used Claude+ to produce possible formulations for the outlier criterion in Alg. 1. In all these cases, we used LLMs through chat-based user interfaces, instructing them to generate code (LaTeX) or suggest improvements. If the suggested changes would not work as expected, we reported them to the model in natural language, using the same chat-based interface.\n\nFigure 1 :\n1Compressed LLM performance for LLaMA models. (left) LM loss on WikiText2 vs model size. (right) Average performance on zero-shot tasks vs model size.\n\nFigure 2 :\n2Weight log-sensitivities from the last attention layer of LLaMA-65B. Dark-blue shades indicate higher sensitivity. The image on the left is a high-level view, resized to 1:32 scale with max-pooling. The two images in the middle are zoomed in from the main figure. The two images on the right are taken from other weight matrices.\n\n1 :\n1E := float_matrix(m, n) // L2 error 2: H := 2XX T // L2 error hessian, R n\u00d7n 3: H ic := Cholesky((H + \u03bbI) \u22121 ) 4: Q := int_matrix(m, n) // quantized weight 5: O := \u2205 // a set of all outliers 6: S := \u2205 // a set of quantization statistics 7: for i = 1, \u03b21, 2\u03b21, . . . n do 8: W :,i:i+\u03b2 1 , O := outliers(W :,i:i+\u03b2 1 , H ic i:(i+\u03b2 1 ),i:(i+\u03b2 1 ) O) 9:\u015d,\u1e91, S := fit_statistics(W :,i:i+\u03b2 1 , S, O) 10:\n\nFigure 3 :\n3\u03b21) 3: // \u20d7 s for scales, \u20d7 z for zero points 4: \u20d7 ss, \u20d7 zs := fit_quantizer(\u20d7 s, \u03b22) 5: \u20d7 sz, \u20d7 zz := fit_quantizer(\u20d7 z, \u03b22) 6: \u20d7 sq := quantize(\u20d7 s, \u20d7 ss, \u20d7 zs) 7: \u20d7 zq := quantize(\u20d7 z, \u20d7 sz, \u20d7 zz) 8: S := S \u222a {sq, ss, sz, zq, sz, zz} 9:\u015d := dequantize(sq, ss, sz) 10:\u1e91 := dequantize(zq, zs, zz) 11: return\u015d,\u1e91, A high-level overview of the SpQR representation for a single weight tensor. The right side of the image depicts all stored data types and their dimensions.\n\nFigure 4 :\n4Different outlier types, LLaMA-65B.\n\nFigure 5 Figure 5 :\n55demonstrates a typical example of how group size affects sensitivity. In the bottom-right plot, we observe that a subset of weights (width 128) has a significantly higher quantization error The weight sensitivities for LLaMA-65B 40th layer, attention query projection. The color scale represents sensitivity on a logarithmic scale, with higher sensitivity being darker. (top) 3-bit GPTQ with per-row quantization scales, (bottom) 3-bit GPTQ with block size 128.\n\nFigure 7 :\n7Figure:mean quantization error (vertical axis) as a function of layer depth (horizontal axis). Each plot corresponds to a different layer role.\n\n\nhas the following hyperparameters: b w = 4, b s = b z = 3, \u03b2 1 = \u03b2 2 = 16, \u03c4 = 0.1 This translates to the following command line arguments in our supplementary code: python main.py $MODEL custom --custom_data_path=$DATA \\ --wbits 4 --groupsize 16 --perchannel --qq_scale_bits 3 \\ --qq_zero_bits 3 --qq_groupsize 16 --outlier_threshold 0.1 \\ --fit_quantizer_without_outliers --permutation_order act_order\n\n\nZq, Ss, Zs, Sz, Zz := gather_statistics(S) 17: Wsparse = gather_outlier_matrix(W, O) 18: return Q, Sq, Zq, Ss, Zs, Sz, Zz, Wsparse\n\nTable 2 :\n2Perplexity on WikiText2[MXBS16], C4 [RSR + 20] and Penn Treebank [MKM + 94] for SpQR and round-to-nearest (RTN) and GPTQ baselines on Falcon model. We can see that SpQR reaches performances within 1% of the perplexity with less than 4.5 bits per parameter. We also see that for 4-bits per parameter SpQR significantly improves on GPTQ with an improvement as large as the improvement from RTN to GPTQ.Name \nWiki2 \nC4 \nPTB Avg bits \n\nUncompressed \n3.53 \n5.62 6.91 \n16 \nGPTQ (4 bit) \n3.83 \n5.80 7.07 \n4 \n\n3-bit statistics \n3.74 \n5.73 7.02 \n3.63 \n16-bit statistics \n3.84 \n5.83 7.12 \n3.67 \n\nRound zero \n3.75 \n5.76 7.01 \n3.63 \nw/o act order \n3.74 \n5.76 7.05 \n3.63 \n\n\n\nTable 3 :\n3Perplexity for LLaMA-65B model.0 \n1 \n2 \n3 \n4 \nOutliers rate (%) \n\n3.66 \n\n3.68 \n\n3.70 \n\n3.72 \n\n3.74 \n\nPerplexity \n\nPerplexity vs Outlier Types (WikiText2) \n\nUnstructured (SpQR) \nRows (MSE) \nDettmers et.al \n\n\n\nTable of contents\nofParameter sensitivity under quantization . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 3.2 Exploring parameter sensitivity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 4.2 Implementing and Leveraging the Sparse Quantized Representation . . . . . . . . . . . . . . 71 Introduction \n1 \n\n2 Related Work \n3 \n\n3 Quantization sensitivity of LLM weights \n4 \n3.1 4 SpQR: A Sensitivity-aware compressed representation \n5 \n4.1 5 Experimental Validation \n8 \n\n6 Discussion & Limitations \n11 \n\n7 Acknowledgements \n11 \n\nA Additional weight sensitivity analysis \n15 \n\nB Experimental Configurations \n18 \n\nC Hyperparameter sensitivity \n18 \n\nD Estimating model size \n18 \n\nE Choice of optimal configuration for fixed average number of bits \n19 \n\nF Additional results for near-lossless compression \n20 \n\nG Choice of optimal LLM configuration for specific hardware \n20 \n\nH Sensitivity to random seed \n22 \n\nI Generative examples \n22 \n\nJ Broader impact \n22 \n\nK On the use of LLMs in this work \n26 \n\n\n\nTable 5 :\n5Weight block size \u03b2 1 and statistic block size \u03b2 2 performance on WikiText2, C4, and Penn Treebank (PTB). The uncompressed baseline value is provided in the corresponding heading.\n\n\nFigure 8: Perplexity of WikiText2 vs average number of bits. Different markers denote different b w . Black colors correspond to quantization configurations without outliers and the brightness of the color is proportional to the outlier rate.0.000 \n\n0.005 \n\n0.010 \n\n0.015 \n\n0.020 \n\n0.025 \n\n0.030 \n\n0.035 \n\n0.040 \n\nOutlier share \n\nOPT \n\nSize Method Avg bits Wiki2 C4 \nPTB \n\n6.7B \n\n-\n16.00 \n10.86 11.74 13.09 \nSpQR \n4.27 \n10.81 11.88 13.17 \n\nRTN \n4 \n12.10 13.38 16.09 \nGPTQ \n4 \n11.39 12.15 13.80 \nSpQR \n3.94 \n11.04 11.98 13.33 \n\n13B \n\n-\n16.00 \n10.12 11.20 12.34 \nSpQR \n4.27 \n10.22 11.27 12.41 \n\nRTN \n4 \n11.32 12.35 15.4 \nGPTQ \n4 \n10.31 11.36 12.58 \nSpQR \n3.93 \n10.28 11.34 12.52 \n\nSize Method Avg bits Wiki2 C4 \nPTB \n\n30B \n\n-\n16.00 \n9.56 10.69 11.84 \nSpQR \n4.26 \n9.50 10.73 11.88 \n\nRTN \n4 \n10.97 11.90 14.17 \nGPTQ \n4 \n9.63 10.80 11.98 \nSpQR \n3.94 \n9.54 10.78 11.93 \n\n66B \n\n-\n16.00 \n9.33 10.28 11.36 \nSpQR \n4.23 \n9.37 10.32 11.40 \n\nRTN \n4 \n110 \n249 \n274 \nGPTQ \n4 \n9.55 10.50 11.58 \nSpQR \n3.91 \n9.32 10.35 11.43 \n\n\n\nTable 6 :\n6Perplexity on WikiText2 [MXBS16], C4 [RSR + 20] and Penn Treebank [MKM + 94] for\n\nTable 9 :\n9Choice of the best LLaMA for a given memory constraint.\nAcknowledgementsPrompt: Last Tuesday, just as the sun was setting, my neighbor's dog, Rover, started to speak to me in perfect French. He said 16bit: \"Je suis un chien.\" I was shocked. I said, \"Moi aussi.\" I'm not sure what to make of this. I'm not sure what to make of anything. I'm not sure what SpQR: \"Je suis un chien.\" I was shocked. I said, \"Rover, you can speak French?\" He said, \"Oui, je parle francais.\" I said, \"How did you learn to speak French?\" RTN 4-bit: \"Bonjour, Monsieur. Je m'appelle Rover. Je suis un chien. Je suis un chien fran\u00e7ais. Je suis un chien fran\u00e7ais qui parle fran\u00e7ais. Je suis un chien fran\u00e7ais qui parle fran\u00e7ais Prompt: In a parallel universe where cats are the rulers of Earth, our feline king was pondering his first decree 16bit: \" to make all dogs illegal. \"I\"m not sure that\"s a good idea,\" said his chief adviser, a cat named Whiskers. \"Why not?\" asked the king. \"Because, SpQR: \" to make all dogs illegal. \"I\"m not sure that\"s a good idea,\" said his chief adviser, a cat named Whiskers. \"Why not?\" asked the king. \"Because, RTN 4-bit: \" to make the world a better place for cats. He was about to sign the decree when he was interrupted by a knock on the door. \"Come in,\" he said. The door opened and a cat entered. Layer:79, mlp.downFigure 11: A grid of weight log-sensitivities for LLaMA-65B for 3-bit GPTQ compression with group-wise quantization of block size 128. Each row corresponds to a specific layer type (e.g. attention query, mlp gate), and the columns represent layer depth.\nLanguage models are few-shot learners. Benjamin Tom Brown, Nick Mann, Melanie Ryder, Jared D Subbiah, Prafulla Kaplan, Arvind Dhariwal, Pranav Neelakantan, Girish Shyam, Amanda Sastry, Askell, Conference on Neural Information Processing Systems (NeurIPS). 2020+ 20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Pra- fulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In Conference on Neural Information Processing Systems (NeurIPS), 2020.\n\nHailey Bsa + 23] Stella Biderman, Quentin Schoelkopf, Herbie Anthony, Bradley, O&apos; Kyle, Eric Brien, Mohammad Aflah Hallahan, Shivanshu Khan, Purohit, Edward Usvsn Sai Prashanth, Raff, arXiv:2304.01373A suite for analyzing large language models across training and scaling. arXiv preprintBSA + 23] Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. arXiv preprint arXiv:2304.01373, 2023.\n\nThink you have solved question answering? try arc, the ai2 reasoning challenge. Cce + 18] Peter, Isaac Clark, Oren Cowhey, Tushar Etzioni, Ashish Khot, Carissa Sabharwal, Oyvind Schoenick, Tafjord, arXiv:1803.05457arXiv preprintCCE + 18] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.\n\nSharan + 22] Aakanksha Chowdhery, Jacob Narang, Maarten Devlin, Gaurav Bosma, Adam Mishra, Paul Roberts, Barham, Hyung Won, Charles Chung, Sebastian Sutton, Gehrmann, arXiv:2204.02311Scaling language modeling with pathways. arXiv preprint+ 22] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebas- tian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n\nBERT: Pretraining of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, North American Chapter of the Association for Computational Linguistics (NAACL). Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre- training of deep bidirectional transformers for language understanding. In North American Chapter of the Association for Computational Linguistics (NAACL), 2019.\n\n8-bit matrix multiplication for transformers at scale. Tim Dettmers, Mike Lewis, Younes Belkada, Luke Zettlemoyer, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022. int8(Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. LLM.int8(): 8-bit matrix multiplication for transformers at scale. Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, 2022.\n\nThe case for 4-bit precision: k-bit inference scaling laws. Tim Dettmers, Luke Zettlemoyer, arXiv:2212.09720arXiv preprintTim Dettmers and Luke Zettlemoyer. The case for 4-bit precision: k-bit inference scaling laws. arXiv preprint arXiv:2212.09720, 2022.\n\nMassive language models can be accurately pruned in one-shot. Elias Frantar, Dan Alistarh, arXiv:2301.00774arXiv preprintElias Frantar and Dan Alistarh. Massive language models can be accurately pruned in one-shot. arXiv preprint arXiv:2301.00774, 2023.\n\nGptq: Accurate post-training quantization for generative pre-trained transformers. Elias Frantar, Torsten Saleh Ashkboos, Dan Hoefler, Alistarh, arXiv:2210.17323arXiv preprintElias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.\n\nOptimal Brain Compression: A framework for accurate post-training quantization and pruning. Elias Frantar, Sidak Pal, Dan Singh, Alistarh, arXiv:2208.11580arXiv preprintAccepted to NeurIPS 2022, to appearElias Frantar, Sidak Pal Singh, and Dan Alistarh. Optimal Brain Compression: A framework for accurate post-training quantization and pruning. arXiv preprint arXiv:2208.11580, 2022. Accepted to NeurIPS 2022, to appear.\n\nOpenvino deep learning workbench: Comprehensive analysis and tuning of neural networks inference. Yury Gorbachev, Mikhail Fedorov, Iliya Slavutin, Artyom Tugarev, Marat Fatekhov, Yaroslav Tarkan, Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops. the IEEE/CVF International Conference on Computer Vision WorkshopsGFS + 19[GFS + 19] Yury Gorbachev, Mikhail Fedorov, Iliya Slavutin, Artyom Tugarev, Marat Fatekhov, and Yaroslav Tarkan. Openvino deep learning workbench: Comprehensive analysis and tuning of neural networks inference. In Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops, pages 0-0, 2019.\n\nA survey of quantization methods for efficient neural network inference. Sehoon Gkd + 21] Amir Gholami, Zhen Kim, Zhewei Dong, Yao, W Michael, Kurt Mahoney, Keutzer, arXiv:2103.13630arXiv preprintGKD + 21] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. A survey of quantization methods for efficient neural network inference. arXiv preprint arXiv:2103.13630, 2021.\n\nA framework for few-shot language model evaluation. Jonathan Gtb + 21] Leo Gao, Stella Tow, Sid Biderman, Anthony Black, Charles Dipofi, Laurence Foster, Jeffrey Golding, Kyle Hsu, Mcdonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy ZouGTB + 21] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, September 2021.\n\nTorsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, Alexandra Peste, arXiv:2102.00554Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks. arXiv preprintHABN + 21[HABN + 21] Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks. arXiv preprint arXiv:2102.00554, 2021.\n\nTraining compute-optimal large language models. Sebastian Hbm + 22] Jordan Hoffmann, Arthur Borgeaud, Elena Mensch, Trevor Buchatskaya, Eliza Cai, Diego Rutherford, De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Clark, arXiv:2203.15556arXiv preprintHBM + 22] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.\n\nAccurate post training quantization with small calibration sets. Yury Hnh + 21] Itay Hubara, Yair Nahshan, Ron Hanani, Daniel Banner, Soudry, International Conference on Machine Learning (ICML). 2021HNH + 21] Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and Daniel Soudry. Accurate post training quantization with small calibration sets. In International Conference on Machine Learning (ICML), 2021.\n\nFbgemm: Enabling high-performance low-precision deep learning inference. Jianyu Khb + 21] Daya Khudia, Protonu Huang, Summer Basu, Haixin Deng, Jongsoo Liu, Mikhail Park, Smelyanskiy, arXiv:2101.05615arXiv preprintKHB + 21] Daya Khudia, Jianyu Huang, Protonu Basu, Summer Deng, Haixin Liu, Jongsoo Park, and Mikhail Smelyanskiy. Fbgemm: Enabling high-performance low-precision deep learning inference. arXiv preprint arXiv:2101.05615, 2021.\n\n+ 20] Jared Kaplan, Sam Mccandlish, Tom Henighan, B Tom, Benjamin Brown, Rewon Chess, Scott Child, Alec Gray, Jeffrey Radford, Dario Wu, Amodei, arXiv:2001.08361Scaling laws for neural language models. arXiv preprint+ 20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.\n\nBRECQ: Pushing the limit of post-training quantization by block reconstruction. Lgt + 21] Yuhang, Ruihao Li, Xu Gong, Yang Tan, Peng Yang, Qi Hu, Fengwei Zhang, Wei Yu, Shi Wang, Gu, International Conference on Learning Representations (ICLR. 2021LGT + 21] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi Gu. BRECQ: Pushing the limit of post-training quantization by block reconstruction. In International Conference on Learning Representations (ICLR), 2021.\n\nThe penn treebank: Annotating predicate argument structure. Mkm + 94] Mitch, Grace Marcus, Mary Ann Kim, Robert Marcinkiewicz, Ann Macintyre, Mark Bies, Karen Ferguson, Britta Katz, Schasberger, Human Language Technology: Proceedings of a Workshop. Plainsboro, New JerseyMKM + 94] Mitch Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. The penn treebank: Annotating predicate argument structure. In Human Language Technology: Proceedings of a Workshop held at Plainsboro, New Jersey, March 8-11, 1994, 1994.\n\nPointer sentinel mixture models. Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher, arXiv:1609.07843arXiv preprintStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.\n\nUp or down? Adaptive rounding for post-training quantization. Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, Tijmen Blankevoort, International Conference on Machine Learning (ICML). 2020NAVB + 20[NAVB + 20] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or down? Adaptive rounding for post-training quantization. In International Conference on Machine Learning (ICML), 2020.\n\n. Neuralmagic, Deepsparse, NeuralMagic. DeepSparse, 2022.\n\nNelson Oen + 22] Catherine Olsson, Neel Elhage, Nicholas Nanda, Nova Joseph, Tom Dassarma, Ben Henighan, Amanda Mann, Yuntao Askell, Anna Bai, Chen, arXiv:2209.11895-context learning and induction heads. arXiv preprintOEN + 22] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads. arXiv preprint arXiv:2209.11895, 2022.\n\n. Openai, arXivOpenAI. Gpt-4 technical report. arXiv, 2023.\n\nPyTorch: An imperative style, high-performance deep learning library. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary Devito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, Soumith Chintala, Conference on Neural Information Processing Systems (NeurIPS). PGM + 19PGM + 19] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmai- son, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An imperative style, high-performance deep learning library. In Conference on Neural Information Processing Systems (NeurIPS). 2019.\n\nnuQmm: Quantized matmul for efficient inference of large-scale generative language models. Gunho Park, Baeseong Park, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee, Dongsoo Lee, arXiv:2206.09557arXiv preprintPPK + 22[PPK + 22] Gunho Park, Baeseong Park, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee, and Dongsoo Lee. nuQmm: Quantized matmul for efficient inference of large-scale genera- tive language models. arXiv preprint arXiv:2206.09557, 2022.\n\nMlperf inference benchmark. Christine Vijay Janapa Reddi, David Cheng, Peter Kanter, Guenther Mattson, Carole-Jean Schmuelling, Brian Wu, Maximilien Anderson, Mark Breughe, William Charlebois, Chou, 2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA). IEEE+ 20] Vijay Janapa Reddi, Christine Cheng, David Kanter, Peter Mattson, Guenther Schmuelling, Carole-Jean Wu, Brian Anderson, Maximilien Breughe, Mark Charlebois, William Chou, et al. Mlperf inference benchmark. In 2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA), pages 446-459. IEEE, 2020.\n\nExploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter Liu, Journal of Machine Learning Research. 21140RSR + 20RSR + 20] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learn- ing with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67, 2020.\n\nLanguage models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 189+ 19] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n\nWinogrande: an adversarial winograd schema challenge at scale. Keisuke Sakaguchi, Le Ronan, Chandra Bras, Yejin Bhagavatula, Choi, Commun. ACM. 649Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: an adversarial winograd schema challenge at scale. Commun. ACM, 64(9):99-106, 2021.\n\nRoformer: Enhanced transformer with rotary position embedding. + 21] Jianlin, Yu Su, Shengfeng Lu, Ahmed Pan, Bo Murtadha, Yunfeng Wen, Liu, arXiv:2104.09864arXiv preprint+ 21] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021.\n\nThibaut Tli + 23] Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timoth\u00e9e Lachaux, Lacroix, Naman Baptiste Rozi\u00e8re, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971Open and efficient foundation language models. arXiv preprintTLI + 23] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n\nPiQA: An algebra for querying protein data sets. Sandeep Tata, M Jignesh, Patel, International Conference on Scientific and Statistical Database Management. Sandeep Tata and Jignesh M Patel. PiQA: An algebra for querying protein data sets. In International Conference on Scientific and Statistical Database Management, 2003.\n\nThe falcon family of large language models. Tii Uae, TII UAE. The falcon family of large language models. https://huggingface.co/ tiiuae/falcon-40b, May 2023.\n\nThe refined web dataset. Tii Uae, TII UAE. The refined web dataset. https://huggingface.co/datasets/tiiuae/ falcon-refinedweb, May 2023.\n\nA multiscale visualization of attention in the transformer model. Jesse Vig, arXiv:1906.05714arXiv preprintJesse Vig. A multiscale visualization of attention in the transformer model. arXiv preprint arXiv:1906.05714, 2019.\n\nAnalyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. Vtm + 19] Elena, David Voita, Fedor Talbot, Rico Moiseev, Ivan Sennrich, Titov, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsVTM + 19] Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5797-5808, Florence, Italy, July 2019. Association for Computational Linguistics.\n\nFinetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Du, M Andrew, Quoc V Dai, Le, arXiv:2109.01652arXiv preprintWBZ + 21[WBZ + 21] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.\n\nTowards accurate posttraining network quantization via bit-split and stitching. Peisong Wang, Qiang Chen, Xiangyu He, Jian Cheng, International Conference on Machine Learning (ICML). 2020Peisong Wang, Qiang Chen, Xiangyu He, and Jian Cheng. Towards accurate post- training network quantization via bit-split and stitching. In International Conference on Machine Learning (ICML), 2020.\n\nEthical and social risks of harm from language models. Wmr + 21] Laura, John Weidinger, Maribeth Mellor, Conor Rauh, Jonathan Griffin, Po-Sen Uesato, Myra Huang, Mia Cheng, Borja Glaese, Atoosa Balle, Zac Kasirzadeh, Kenton, Rimell, Lisa Anne Hendricks, William Isaac, Sean Legassick, Geoffrey Irving, and Iason GabrielSasha Brown, Will Hawkins, Tom Stepleton, Courtney Biles, Abeba Birhane, Julia Haas, LauraWMR + 21] Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, Zac Kenton, Sasha Brown, Will Hawkins, Tom Stepleton, Courtney Biles, Abeba Birhane, Julia Haas, Laura Rimell, Lisa Anne Hendricks, William Isaac, Sean Legassick, Geoffrey Irving, and Iason Gabriel. Ethical and social risks of harm from language models, 2021.\n\nWsm + 18] Alex, Amanpreet Wang, Julian Singh, Felix Michael, Omer Hill, Samuel R Levy, Bowman, arXiv:1804.07461Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprintWSM + 18] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.\n\nSmoothquant: Accurate and efficient post-training quantization for large language models. Ji Xls + 22] Guangxuan Xiao, Mickael Lin, Julien Seznec, Song Demouth, Han, arXiv:2211.10438arXiv preprintXLS + 22] Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. arXiv preprint arXiv:2211.10438, 2022.\n\n+ 22] Zhewei, Reza Yazdani Yao, Minjia Aminabadi, Xiaoxia Zhang, Wu, arXiv:2206.01861Conglong Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. arXiv preprint+ 22] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. arXiv preprint arXiv:2206.01861, 2022.\n\nA comprehensive study on post-training quantization for large language models. Ylw + 23] Zhewei, Cheng Yao, Xiaoxia Li, Stephen Wu, Yuxiong Youn, He, YLW + 23] Zhewei Yao, Cheng Li, Xiaoxia Wu, Stephen Youn, and Yuxiong He. A comprehensive study on post-training quantization for large language models, 2023.\n\nPrompt: Every time data scientist Kim ran her new algorithm, it kept outputting the same unexpected result: a recipe for 16bit: iced tea. Kim was working on a project to predict the likelihood of a patient being readmitted to the hospital within 30 days of discharge. She was using a machine learning algorithm to analyze the data, and the algorithm SpQR: iced tea. Kim was working on a project to predict the likelihood of a patient being readmitted to the hospital within 30 days of discharge. LM eval harness results on Falcon models. 80She had been given a dataset ofTable 8: LM eval harness results on Falcon models. Prompt: Every time data scientist Kim ran her new algorithm, it kept outputting the same unexpected result: a recipe for 16bit: iced tea. Kim was working on a project to predict the likelihood of a patient being readmitted to the hospital within 30 days of discharge. She was using a machine learning algorithm to analyze the data, and the algorithm SpQR: iced tea. Kim was working on a project to predict the likelihood of a patient being readmitted to the hospital within 30 days of discharge. She had been given a dataset of 100,000\n\nI'm not sure what's going on Prompt: Data scientist Grace was working late one night when she discovered a glitch in the matrix of her data. It spelled out 16bit: 100% proof that the world was going to end in 10 days. She tried to tell her boss, but he didn't believe her. She tried to tell her friends, but they didn't believe her SpQR: 100% proof that the world was going to end in 10 days. She tried to tell her boss, but he just laughed and said. RTN 4-bit: iced tea. \"I'm not sure what's going on. She RTN 4-bit: 100% in binary code. She was so excited she ran to tell her boss. but he was busy. She ran to tell her colleague, but he was busy. She ran to tell her friend. but sheRTN 4-bit: iced tea. \"I'm not sure what's going on,\" she said. \"I've been running it for days, and it keeps giving me the same result.\" \"I'm not sure what's going on Prompt: Data scientist Grace was working late one night when she discovered a glitch in the matrix of her data. It spelled out 16bit: 100% proof that the world was going to end in 10 days. She tried to tell her boss, but he didn't believe her. She tried to tell her friends, but they didn't believe her SpQR: 100% proof that the world was going to end in 10 days. She tried to tell her boss, but he just laughed and said, \"I'm not going to believe in some data glitch.\" She RTN 4-bit: 100% in binary code. She was so excited she ran to tell her boss, but he was busy. She ran to tell her colleague, but he was busy. She ran to tell her friend, but she\n", "annotations": {"author": "[{\"end\":97,\"start\":84},{\"end\":118,\"start\":98},{\"end\":135,\"start\":119},{\"end\":153,\"start\":136},{\"end\":176,\"start\":154},{\"end\":191,\"start\":177},{\"end\":207,\"start\":192},{\"end\":219,\"start\":208},{\"end\":239,\"start\":220},{\"end\":256,\"start\":240},{\"end\":268,\"start\":257},{\"end\":282,\"start\":269},{\"end\":333,\"start\":283},{\"end\":359,\"start\":334},{\"end\":373,\"start\":360},{\"end\":399,\"start\":374},{\"end\":427,\"start\":400}]", "publisher": null, "author_last_name": "[{\"end\":96,\"start\":88},{\"end\":117,\"start\":105},{\"end\":134,\"start\":124},{\"end\":152,\"start\":142},{\"end\":175,\"start\":167},{\"end\":190,\"start\":183},{\"end\":206,\"start\":198},{\"end\":218,\"start\":212},{\"end\":238,\"start\":230},{\"end\":255,\"start\":248},{\"end\":267,\"start\":261},{\"end\":281,\"start\":273}]", "author_first_name": "[{\"end\":87,\"start\":84},{\"end\":104,\"start\":98},{\"end\":123,\"start\":119},{\"end\":141,\"start\":136},{\"end\":160,\"start\":154},{\"end\":166,\"start\":161},{\"end\":182,\"start\":177},{\"end\":197,\"start\":192},{\"end\":211,\"start\":208},{\"end\":229,\"start\":220},{\"end\":247,\"start\":240},{\"end\":260,\"start\":257},{\"end\":272,\"start\":269}]", "author_affiliation": "[{\"end\":332,\"start\":284},{\"end\":358,\"start\":335},{\"end\":372,\"start\":361},{\"end\":398,\"start\":375},{\"end\":426,\"start\":401}]", "title": "[{\"end\":81,\"start\":1},{\"end\":508,\"start\":428}]", "venue": null, "abstract": "[{\"end\":2134,\"start\":510}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2256,\"start\":2249},{\"end\":2265,\"start\":2256},{\"end\":2343,\"start\":2333},{\"end\":2352,\"start\":2343},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2358,\"start\":2352},{\"end\":2462,\"start\":2452},{\"end\":2471,\"start\":2462},{\"end\":2743,\"start\":2733},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2911,\"start\":2904},{\"end\":2920,\"start\":2911},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2927,\"start\":2920},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2932,\"start\":2927},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3441,\"start\":3435},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3448,\"start\":3441},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4813,\"start\":4805},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5791,\"start\":5783},{\"end\":5800,\"start\":5791},{\"end\":6893,\"start\":6883},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":7221,\"start\":7213},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7276,\"start\":7269},{\"end\":7751,\"start\":7741},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7772,\"start\":7764},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7990,\"start\":7982},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8308,\"start\":8300},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8527,\"start\":8521},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8813,\"start\":8807},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9520,\"start\":9512},{\"end\":9529,\"start\":9520},{\"end\":9538,\"start\":9529},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9790,\"start\":9782},{\"end\":9799,\"start\":9790},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10676,\"start\":10669},{\"end\":10684,\"start\":10676},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":12504,\"start\":12496},{\"end\":12513,\"start\":12504},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":13599,\"start\":13592},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":13839,\"start\":13831},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":15141,\"start\":15133},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":16747,\"start\":16739},{\"end\":17430,\"start\":17420},{\"end\":21528,\"start\":21517},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":22903,\"start\":22895},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":23794,\"start\":23787},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":23801,\"start\":23794},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":29715,\"start\":29707},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":29818,\"start\":29810},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":30058,\"start\":30050},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":30170,\"start\":30162},{\"end\":30179,\"start\":30170},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":30278,\"start\":30272},{\"end\":31426,\"start\":31416},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":33659,\"start\":33653},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":34220,\"start\":34214},{\"end\":40109,\"start\":40099},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":55252,\"start\":55244}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":52749,\"start\":52587},{\"attributes\":{\"id\":\"fig_2\"},\"end\":53092,\"start\":52750},{\"attributes\":{\"id\":\"fig_3\"},\"end\":53495,\"start\":53093},{\"attributes\":{\"id\":\"fig_4\"},\"end\":53978,\"start\":53496},{\"attributes\":{\"id\":\"fig_5\"},\"end\":54027,\"start\":53979},{\"attributes\":{\"id\":\"fig_6\"},\"end\":54512,\"start\":54028},{\"attributes\":{\"id\":\"fig_8\"},\"end\":54669,\"start\":54513},{\"attributes\":{\"id\":\"fig_9\"},\"end\":55075,\"start\":54670},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":55208,\"start\":55076},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":55881,\"start\":55209},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":56100,\"start\":55882},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":57232,\"start\":56101},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":57424,\"start\":57233},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":58437,\"start\":57425},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":58530,\"start\":58438},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":58598,\"start\":58531}]", "paragraph": "[{\"end\":3171,\"start\":2150},{\"end\":3758,\"start\":3173},{\"end\":4646,\"start\":3760},{\"end\":5470,\"start\":4648},{\"end\":6062,\"start\":5472},{\"end\":6717,\"start\":6064},{\"end\":7551,\"start\":6734},{\"end\":8503,\"start\":7553},{\"end\":8795,\"start\":8505},{\"end\":9408,\"start\":8797},{\"end\":9995,\"start\":9410},{\"end\":10546,\"start\":9997},{\"end\":11724,\"start\":10548},{\"end\":12680,\"start\":11726},{\"end\":13120,\"start\":12682},{\"end\":13690,\"start\":13186},{\"end\":14578,\"start\":13739},{\"end\":15341,\"start\":14614},{\"end\":16034,\"start\":15343},{\"end\":16748,\"start\":16036},{\"end\":17265,\"start\":16750},{\"end\":17866,\"start\":17267},{\"end\":18468,\"start\":17868},{\"end\":18599,\"start\":18470},{\"end\":18654,\"start\":18601},{\"end\":19265,\"start\":18667},{\"end\":20039,\"start\":19267},{\"end\":20198,\"start\":20041},{\"end\":20953,\"start\":20200},{\"end\":21644,\"start\":20955},{\"end\":22064,\"start\":21646},{\"end\":22447,\"start\":22066},{\"end\":23573,\"start\":22449},{\"end\":24126,\"start\":23575},{\"end\":24298,\"start\":24128},{\"end\":24567,\"start\":24300},{\"end\":24599,\"start\":24569},{\"end\":24635,\"start\":24601},{\"end\":26044,\"start\":25654},{\"end\":26137,\"start\":26046},{\"end\":26170,\"start\":26139},{\"end\":26231,\"start\":26172},{\"end\":26318,\"start\":26233},{\"end\":27149,\"start\":26320},{\"end\":27739,\"start\":27151},{\"end\":27954,\"start\":27741},{\"end\":28796,\"start\":27956},{\"end\":29078,\"start\":28798},{\"end\":29973,\"start\":29106},{\"end\":30750,\"start\":29975},{\"end\":30989,\"start\":30752},{\"end\":31562,\"start\":30991},{\"end\":32252,\"start\":31564},{\"end\":32693,\"start\":32254},{\"end\":33506,\"start\":32695},{\"end\":34279,\"start\":33508},{\"end\":34942,\"start\":34281},{\"end\":35436,\"start\":34944},{\"end\":36096,\"start\":35438},{\"end\":36745,\"start\":36098},{\"end\":37983,\"start\":36774},{\"end\":38221,\"start\":37985},{\"end\":38274,\"start\":38223},{\"end\":38349,\"start\":38276},{\"end\":38433,\"start\":38351},{\"end\":38482,\"start\":38435},{\"end\":38708,\"start\":38484},{\"end\":39139,\"start\":38710},{\"end\":39345,\"start\":39141},{\"end\":39590,\"start\":39347},{\"end\":39934,\"start\":39592},{\"end\":40443,\"start\":39936},{\"end\":41042,\"start\":40445},{\"end\":41633,\"start\":41044},{\"end\":42389,\"start\":41635},{\"end\":43103,\"start\":42391},{\"end\":43367,\"start\":43137},{\"end\":43617,\"start\":43369},{\"end\":43641,\"start\":43619},{\"end\":44023,\"start\":43643},{\"end\":44110,\"start\":44025},{\"end\":44658,\"start\":44143},{\"end\":45040,\"start\":44660},{\"end\":45298,\"start\":45068},{\"end\":45775,\"start\":45300},{\"end\":45904,\"start\":45777},{\"end\":46231,\"start\":46040},{\"end\":46367,\"start\":46233},{\"end\":47738,\"start\":46369},{\"end\":47943,\"start\":47813},{\"end\":48084,\"start\":47945},{\"end\":48145,\"start\":48086},{\"end\":48484,\"start\":48147},{\"end\":48862,\"start\":48486},{\"end\":49152,\"start\":48895},{\"end\":49672,\"start\":49154},{\"end\":50406,\"start\":49698},{\"end\":51051,\"start\":50427},{\"end\":51714,\"start\":51053},{\"end\":52136,\"start\":51752},{\"end\":52586,\"start\":52138}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13185,\"start\":13121},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13738,\"start\":13691},{\"attributes\":{\"id\":\"formula_2\"},\"end\":24833,\"start\":24636},{\"attributes\":{\"id\":\"formula_3\"},\"end\":25587,\"start\":24833},{\"attributes\":{\"id\":\"formula_5\"},\"end\":46039,\"start\":45905},{\"attributes\":{\"id\":\"formula_6\"},\"end\":47759,\"start\":47739}]", "table_ref": "[{\"end\":9100,\"start\":9093},{\"end\":32221,\"start\":32214},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":32233,\"start\":32226},{\"end\":32301,\"start\":32294},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":32323,\"start\":32316},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":33226,\"start\":33219},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":34402,\"start\":34395},{\"end\":36126,\"start\":36119},{\"end\":36463,\"start\":36456},{\"end\":44109,\"start\":44102},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":44670,\"start\":44663},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":47897,\"start\":47890},{\"end\":48012,\"start\":48005},{\"end\":48083,\"start\":48076},{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":48859,\"start\":48852},{\"end\":50111,\"start\":50104}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2148,\"start\":2136},{\"attributes\":{\"n\":\"2\"},\"end\":6732,\"start\":6720},{\"attributes\":{\"n\":\"3.2\"},\"end\":14612,\"start\":14581},{\"attributes\":{\"n\":\"4.1\"},\"end\":18665,\"start\":18657},{\"attributes\":{\"n\":\"4.2\"},\"end\":25652,\"start\":25589},{\"attributes\":{\"n\":\"5\"},\"end\":29104,\"start\":29081},{\"attributes\":{\"n\":\"6\"},\"end\":36772,\"start\":36748},{\"end\":43135,\"start\":43106},{\"end\":44141,\"start\":44113},{\"end\":45066,\"start\":45043},{\"end\":47811,\"start\":47761},{\"end\":48893,\"start\":48865},{\"end\":49696,\"start\":49675},{\"end\":50425,\"start\":50409},{\"end\":51750,\"start\":51717},{\"end\":52598,\"start\":52588},{\"end\":52761,\"start\":52751},{\"end\":53097,\"start\":53094},{\"end\":53507,\"start\":53497},{\"end\":53990,\"start\":53980},{\"end\":54048,\"start\":54029},{\"end\":54524,\"start\":54514},{\"end\":55219,\"start\":55210},{\"end\":55892,\"start\":55883},{\"end\":56119,\"start\":56102},{\"end\":57243,\"start\":57234},{\"end\":58448,\"start\":58439},{\"end\":58541,\"start\":58532}]", "table": "[{\"end\":55881,\"start\":55621},{\"end\":56100,\"start\":55925},{\"end\":57232,\"start\":56521},{\"end\":58437,\"start\":57669}]", "figure_caption": "[{\"end\":52749,\"start\":52600},{\"end\":53092,\"start\":52763},{\"end\":53495,\"start\":53099},{\"end\":53978,\"start\":53509},{\"end\":54027,\"start\":53992},{\"end\":54512,\"start\":54051},{\"end\":54669,\"start\":54526},{\"end\":55075,\"start\":54672},{\"end\":55208,\"start\":55078},{\"end\":55621,\"start\":55221},{\"end\":55925,\"start\":55894},{\"end\":56521,\"start\":56122},{\"end\":57424,\"start\":57245},{\"end\":57669,\"start\":57427},{\"end\":58530,\"start\":58450},{\"end\":58598,\"start\":58543}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15264,\"start\":15256},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16280,\"start\":16272},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16554,\"start\":16546},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16788,\"start\":16779},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":17773,\"start\":17765},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":19549,\"start\":19541},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":26009,\"start\":26001},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":31586,\"start\":31578},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":32073,\"start\":32065},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":34094,\"start\":34086},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":39644,\"start\":39636},{\"end\":41754,\"start\":41746},{\"end\":41975,\"start\":41967},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":42717,\"start\":42709},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":42959,\"start\":42942},{\"end\":46879,\"start\":46871},{\"end\":50229,\"start\":50221},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":52027,\"start\":52019}]", "bib_author_first_name": "[{\"end\":60173,\"start\":60165},{\"end\":60189,\"start\":60185},{\"end\":60203,\"start\":60196},{\"end\":60216,\"start\":60211},{\"end\":60218,\"start\":60217},{\"end\":60236,\"start\":60228},{\"end\":60251,\"start\":60245},{\"end\":60268,\"start\":60262},{\"end\":60288,\"start\":60282},{\"end\":60302,\"start\":60296},{\"end\":60673,\"start\":60667},{\"end\":60708,\"start\":60701},{\"end\":60727,\"start\":60721},{\"end\":60753,\"start\":60746},{\"end\":60764,\"start\":60760},{\"end\":60780,\"start\":60772},{\"end\":60786,\"start\":60781},{\"end\":60806,\"start\":60797},{\"end\":60828,\"start\":60822},{\"end\":61371,\"start\":61366},{\"end\":61383,\"start\":61379},{\"end\":61398,\"start\":61392},{\"end\":61414,\"start\":61408},{\"end\":61428,\"start\":61421},{\"end\":61446,\"start\":61440},{\"end\":61744,\"start\":61738},{\"end\":61777,\"start\":61772},{\"end\":61793,\"start\":61786},{\"end\":61808,\"start\":61802},{\"end\":61820,\"start\":61816},{\"end\":61833,\"start\":61829},{\"end\":61869,\"start\":61862},{\"end\":61886,\"start\":61877},{\"end\":62324,\"start\":62319},{\"end\":62341,\"start\":62333},{\"end\":62355,\"start\":62349},{\"end\":62369,\"start\":62361},{\"end\":62760,\"start\":62757},{\"end\":62775,\"start\":62771},{\"end\":62789,\"start\":62783},{\"end\":62803,\"start\":62799},{\"end\":63275,\"start\":63272},{\"end\":63290,\"start\":63286},{\"end\":63536,\"start\":63531},{\"end\":63549,\"start\":63546},{\"end\":63812,\"start\":63807},{\"end\":63829,\"start\":63822},{\"end\":63849,\"start\":63846},{\"end\":64185,\"start\":64180},{\"end\":64209,\"start\":64206},{\"end\":64613,\"start\":64609},{\"end\":64632,\"start\":64625},{\"end\":64647,\"start\":64642},{\"end\":64664,\"start\":64658},{\"end\":64679,\"start\":64674},{\"end\":64698,\"start\":64690},{\"end\":65258,\"start\":65252},{\"end\":65287,\"start\":65283},{\"end\":65299,\"start\":65293},{\"end\":65312,\"start\":65311},{\"end\":65326,\"start\":65322},{\"end\":65644,\"start\":65636},{\"end\":65670,\"start\":65664},{\"end\":65679,\"start\":65676},{\"end\":65697,\"start\":65690},{\"end\":65712,\"start\":65705},{\"end\":65729,\"start\":65721},{\"end\":65745,\"start\":65738},{\"end\":65759,\"start\":65755},{\"end\":66206,\"start\":66199},{\"end\":66219,\"start\":66216},{\"end\":66233,\"start\":66230},{\"end\":66249,\"start\":66243},{\"end\":66267,\"start\":66258},{\"end\":66709,\"start\":66700},{\"end\":66743,\"start\":66737},{\"end\":66759,\"start\":66754},{\"end\":66774,\"start\":66768},{\"end\":66793,\"start\":66788},{\"end\":66804,\"start\":66799},{\"end\":66829,\"start\":66825},{\"end\":66834,\"start\":66830},{\"end\":66850,\"start\":66842},{\"end\":66867,\"start\":66862},{\"end\":67257,\"start\":67253},{\"end\":67285,\"start\":67281},{\"end\":67298,\"start\":67295},{\"end\":67313,\"start\":67307},{\"end\":67675,\"start\":67669},{\"end\":67706,\"start\":67699},{\"end\":67720,\"start\":67714},{\"end\":67733,\"start\":67727},{\"end\":67747,\"start\":67740},{\"end\":67760,\"start\":67753},{\"end\":68049,\"start\":68038},{\"end\":68061,\"start\":68058},{\"end\":68077,\"start\":68074},{\"end\":68089,\"start\":68088},{\"end\":68103,\"start\":68095},{\"end\":68116,\"start\":68111},{\"end\":68129,\"start\":68124},{\"end\":68141,\"start\":68137},{\"end\":68155,\"start\":68148},{\"end\":68170,\"start\":68165},{\"end\":68587,\"start\":68581},{\"end\":68594,\"start\":68592},{\"end\":68605,\"start\":68601},{\"end\":68615,\"start\":68611},{\"end\":68624,\"start\":68622},{\"end\":68636,\"start\":68629},{\"end\":68647,\"start\":68644},{\"end\":68655,\"start\":68652},{\"end\":69069,\"start\":69064},{\"end\":69082,\"start\":69078},{\"end\":69086,\"start\":69083},{\"end\":69098,\"start\":69092},{\"end\":69117,\"start\":69114},{\"end\":69133,\"start\":69129},{\"end\":69145,\"start\":69140},{\"end\":69162,\"start\":69156},{\"end\":69609,\"start\":69602},{\"end\":69625,\"start\":69618},{\"end\":69638,\"start\":69633},{\"end\":69656,\"start\":69649},{\"end\":69903,\"start\":69897},{\"end\":69915,\"start\":69911},{\"end\":69919,\"start\":69916},{\"end\":69931,\"start\":69927},{\"end\":69952,\"start\":69944},{\"end\":69968,\"start\":69962},{\"end\":70339,\"start\":70333},{\"end\":70372,\"start\":70368},{\"end\":70389,\"start\":70381},{\"end\":70401,\"start\":70397},{\"end\":70413,\"start\":70410},{\"end\":70427,\"start\":70424},{\"end\":70444,\"start\":70438},{\"end\":70457,\"start\":70451},{\"end\":70470,\"start\":70466},{\"end\":70923,\"start\":70919},{\"end\":70935,\"start\":70932},{\"end\":70952,\"start\":70943},{\"end\":70964,\"start\":70960},{\"end\":70977,\"start\":70972},{\"end\":70995,\"start\":70988},{\"end\":71010,\"start\":71004},{\"end\":71026,\"start\":71020},{\"end\":71039,\"start\":71032},{\"end\":71056,\"start\":71052},{\"end\":71070,\"start\":71065},{\"end\":71089,\"start\":71082},{\"end\":71102,\"start\":71096},{\"end\":71116,\"start\":71109},{\"end\":71131,\"start\":71125},{\"end\":71147,\"start\":71140},{\"end\":71162,\"start\":71156},{\"end\":71183,\"start\":71177},{\"end\":71195,\"start\":71193},{\"end\":71208,\"start\":71202},{\"end\":71221,\"start\":71214},{\"end\":71871,\"start\":71866},{\"end\":71886,\"start\":71878},{\"end\":71895,\"start\":71893},{\"end\":71917,\"start\":71907},{\"end\":71931,\"start\":71923},{\"end\":71944,\"start\":71937},{\"end\":72257,\"start\":72248},{\"end\":72283,\"start\":72278},{\"end\":72296,\"start\":72291},{\"end\":72313,\"start\":72305},{\"end\":72334,\"start\":72323},{\"end\":72353,\"start\":72348},{\"end\":72368,\"start\":72358},{\"end\":72383,\"start\":72379},{\"end\":72400,\"start\":72393},{\"end\":72920,\"start\":72915},{\"end\":72933,\"start\":72929},{\"end\":72947,\"start\":72943},{\"end\":72966,\"start\":72957},{\"end\":72978,\"start\":72972},{\"end\":72994,\"start\":72987},{\"end\":73008,\"start\":73003},{\"end\":73018,\"start\":73015},{\"end\":73028,\"start\":73023},{\"end\":73419,\"start\":73415},{\"end\":73436,\"start\":73429},{\"end\":73446,\"start\":73441},{\"end\":73459,\"start\":73454},{\"end\":73471,\"start\":73466},{\"end\":73484,\"start\":73480},{\"end\":73754,\"start\":73747},{\"end\":73768,\"start\":73766},{\"end\":73783,\"start\":73776},{\"end\":73795,\"start\":73790},{\"end\":74079,\"start\":74077},{\"end\":74093,\"start\":74084},{\"end\":74103,\"start\":74098},{\"end\":74111,\"start\":74109},{\"end\":74129,\"start\":74122},{\"end\":74361,\"start\":74354},{\"end\":74393,\"start\":74386},{\"end\":74408,\"start\":74402},{\"end\":74428,\"start\":74418},{\"end\":74447,\"start\":74439},{\"end\":74471,\"start\":74466},{\"end\":74494,\"start\":74490},{\"end\":74508,\"start\":74502},{\"end\":74928,\"start\":74921},{\"end\":74936,\"start\":74935},{\"end\":75245,\"start\":75242},{\"end\":75386,\"start\":75383},{\"end\":75567,\"start\":75562},{\"end\":75843,\"start\":75838},{\"end\":75856,\"start\":75851},{\"end\":75869,\"start\":75865},{\"end\":75883,\"start\":75879},{\"end\":76538,\"start\":76533},{\"end\":76551,\"start\":76544},{\"end\":76560,\"start\":76559},{\"end\":76576,\"start\":76570},{\"end\":76588,\"start\":76583},{\"end\":76592,\"start\":76589},{\"end\":76603,\"start\":76598},{\"end\":76611,\"start\":76608},{\"end\":76625,\"start\":76624},{\"end\":76640,\"start\":76634},{\"end\":76995,\"start\":76988},{\"end\":77007,\"start\":77002},{\"end\":77021,\"start\":77014},{\"end\":77030,\"start\":77026},{\"end\":77370,\"start\":77366},{\"end\":77390,\"start\":77382},{\"end\":77404,\"start\":77399},{\"end\":77419,\"start\":77411},{\"end\":77435,\"start\":77429},{\"end\":77448,\"start\":77444},{\"end\":77459,\"start\":77456},{\"end\":77472,\"start\":77467},{\"end\":77487,\"start\":77481},{\"end\":77498,\"start\":77495},{\"end\":78146,\"start\":78137},{\"end\":78159,\"start\":78153},{\"end\":78172,\"start\":78167},{\"end\":78186,\"start\":78182},{\"end\":78201,\"start\":78193},{\"end\":78650,\"start\":78648},{\"end\":78684,\"start\":78677},{\"end\":78696,\"start\":78690},{\"end\":78709,\"start\":78705},{\"end\":78982,\"start\":78978},{\"end\":78990,\"start\":78983},{\"end\":79002,\"start\":78996},{\"end\":79021,\"start\":79014},{\"end\":79517,\"start\":79512},{\"end\":79530,\"start\":79523},{\"end\":79542,\"start\":79535},{\"end\":79554,\"start\":79547}]", "bib_author_last_name": "[{\"end\":60183,\"start\":60174},{\"end\":60194,\"start\":60190},{\"end\":60209,\"start\":60204},{\"end\":60226,\"start\":60219},{\"end\":60243,\"start\":60237},{\"end\":60260,\"start\":60252},{\"end\":60280,\"start\":60269},{\"end\":60294,\"start\":60289},{\"end\":60309,\"start\":60303},{\"end\":60317,\"start\":60311},{\"end\":60699,\"start\":60674},{\"end\":60719,\"start\":60709},{\"end\":60735,\"start\":60728},{\"end\":60744,\"start\":60737},{\"end\":60758,\"start\":60754},{\"end\":60770,\"start\":60765},{\"end\":60795,\"start\":60787},{\"end\":60811,\"start\":60807},{\"end\":60820,\"start\":60813},{\"end\":60848,\"start\":60829},{\"end\":60854,\"start\":60850},{\"end\":61364,\"start\":61349},{\"end\":61377,\"start\":61372},{\"end\":61390,\"start\":61384},{\"end\":61406,\"start\":61399},{\"end\":61419,\"start\":61415},{\"end\":61438,\"start\":61429},{\"end\":61456,\"start\":61447},{\"end\":61465,\"start\":61458},{\"end\":61770,\"start\":61745},{\"end\":61784,\"start\":61778},{\"end\":61800,\"start\":61794},{\"end\":61814,\"start\":61809},{\"end\":61827,\"start\":61821},{\"end\":61841,\"start\":61834},{\"end\":61849,\"start\":61843},{\"end\":61860,\"start\":61851},{\"end\":61875,\"start\":61870},{\"end\":61893,\"start\":61887},{\"end\":61903,\"start\":61895},{\"end\":62331,\"start\":62325},{\"end\":62347,\"start\":62342},{\"end\":62359,\"start\":62356},{\"end\":62379,\"start\":62370},{\"end\":62769,\"start\":62761},{\"end\":62781,\"start\":62776},{\"end\":62797,\"start\":62790},{\"end\":62815,\"start\":62804},{\"end\":63284,\"start\":63276},{\"end\":63302,\"start\":63291},{\"end\":63544,\"start\":63537},{\"end\":63558,\"start\":63550},{\"end\":63820,\"start\":63813},{\"end\":63844,\"start\":63830},{\"end\":63857,\"start\":63850},{\"end\":63867,\"start\":63859},{\"end\":64193,\"start\":64186},{\"end\":64204,\"start\":64195},{\"end\":64215,\"start\":64210},{\"end\":64225,\"start\":64217},{\"end\":64623,\"start\":64614},{\"end\":64640,\"start\":64633},{\"end\":64656,\"start\":64648},{\"end\":64672,\"start\":64665},{\"end\":64688,\"start\":64680},{\"end\":64705,\"start\":64699},{\"end\":65281,\"start\":65259},{\"end\":65291,\"start\":65288},{\"end\":65304,\"start\":65300},{\"end\":65309,\"start\":65306},{\"end\":65320,\"start\":65313},{\"end\":65334,\"start\":65327},{\"end\":65343,\"start\":65336},{\"end\":65662,\"start\":65645},{\"end\":65674,\"start\":65671},{\"end\":65688,\"start\":65680},{\"end\":65703,\"start\":65698},{\"end\":65719,\"start\":65713},{\"end\":65736,\"start\":65730},{\"end\":65753,\"start\":65746},{\"end\":65763,\"start\":65760},{\"end\":65773,\"start\":65765},{\"end\":66214,\"start\":66207},{\"end\":66228,\"start\":66220},{\"end\":66241,\"start\":66234},{\"end\":66256,\"start\":66250},{\"end\":66273,\"start\":66268},{\"end\":66735,\"start\":66710},{\"end\":66752,\"start\":66744},{\"end\":66766,\"start\":66760},{\"end\":66786,\"start\":66775},{\"end\":66797,\"start\":66794},{\"end\":66815,\"start\":66805},{\"end\":66823,\"start\":66817},{\"end\":66840,\"start\":66835},{\"end\":66860,\"start\":66851},{\"end\":66873,\"start\":66868},{\"end\":66880,\"start\":66875},{\"end\":67279,\"start\":67258},{\"end\":67293,\"start\":67286},{\"end\":67305,\"start\":67299},{\"end\":67320,\"start\":67314},{\"end\":67328,\"start\":67322},{\"end\":67697,\"start\":67676},{\"end\":67712,\"start\":67707},{\"end\":67725,\"start\":67721},{\"end\":67738,\"start\":67734},{\"end\":67751,\"start\":67748},{\"end\":67765,\"start\":67761},{\"end\":67778,\"start\":67767},{\"end\":68056,\"start\":68050},{\"end\":68072,\"start\":68062},{\"end\":68086,\"start\":68078},{\"end\":68093,\"start\":68090},{\"end\":68109,\"start\":68104},{\"end\":68122,\"start\":68117},{\"end\":68135,\"start\":68130},{\"end\":68146,\"start\":68142},{\"end\":68163,\"start\":68156},{\"end\":68173,\"start\":68171},{\"end\":68181,\"start\":68175},{\"end\":68579,\"start\":68563},{\"end\":68590,\"start\":68588},{\"end\":68599,\"start\":68595},{\"end\":68609,\"start\":68606},{\"end\":68620,\"start\":68616},{\"end\":68627,\"start\":68625},{\"end\":68642,\"start\":68637},{\"end\":68650,\"start\":68648},{\"end\":68660,\"start\":68656},{\"end\":68664,\"start\":68662},{\"end\":69062,\"start\":69047},{\"end\":69076,\"start\":69070},{\"end\":69090,\"start\":69087},{\"end\":69112,\"start\":69099},{\"end\":69127,\"start\":69118},{\"end\":69138,\"start\":69134},{\"end\":69154,\"start\":69146},{\"end\":69167,\"start\":69163},{\"end\":69180,\"start\":69169},{\"end\":69616,\"start\":69610},{\"end\":69631,\"start\":69626},{\"end\":69647,\"start\":69639},{\"end\":69663,\"start\":69657},{\"end\":69909,\"start\":69904},{\"end\":69925,\"start\":69920},{\"end\":69942,\"start\":69932},{\"end\":69960,\"start\":69953},{\"end\":69980,\"start\":69969},{\"end\":70287,\"start\":70276},{\"end\":70299,\"start\":70289},{\"end\":70366,\"start\":70340},{\"end\":70379,\"start\":70373},{\"end\":70395,\"start\":70390},{\"end\":70408,\"start\":70402},{\"end\":70422,\"start\":70414},{\"end\":70436,\"start\":70428},{\"end\":70449,\"start\":70445},{\"end\":70464,\"start\":70458},{\"end\":70474,\"start\":70471},{\"end\":70480,\"start\":70476},{\"end\":70796,\"start\":70790},{\"end\":70930,\"start\":70924},{\"end\":70941,\"start\":70936},{\"end\":70958,\"start\":70953},{\"end\":70970,\"start\":70965},{\"end\":70986,\"start\":70978},{\"end\":71002,\"start\":70996},{\"end\":71018,\"start\":71011},{\"end\":71030,\"start\":71027},{\"end\":71050,\"start\":71040},{\"end\":71063,\"start\":71057},{\"end\":71080,\"start\":71071},{\"end\":71094,\"start\":71090},{\"end\":71107,\"start\":71103},{\"end\":71123,\"start\":71117},{\"end\":71138,\"start\":71132},{\"end\":71154,\"start\":71148},{\"end\":71175,\"start\":71163},{\"end\":71191,\"start\":71184},{\"end\":71200,\"start\":71196},{\"end\":71212,\"start\":71209},{\"end\":71230,\"start\":71222},{\"end\":71876,\"start\":71872},{\"end\":71891,\"start\":71887},{\"end\":71905,\"start\":71896},{\"end\":71921,\"start\":71918},{\"end\":71935,\"start\":71932},{\"end\":71948,\"start\":71945},{\"end\":72276,\"start\":72258},{\"end\":72289,\"start\":72284},{\"end\":72303,\"start\":72297},{\"end\":72321,\"start\":72314},{\"end\":72346,\"start\":72335},{\"end\":72356,\"start\":72354},{\"end\":72377,\"start\":72369},{\"end\":72391,\"start\":72384},{\"end\":72411,\"start\":72401},{\"end\":72417,\"start\":72413},{\"end\":72927,\"start\":72921},{\"end\":72941,\"start\":72934},{\"end\":72955,\"start\":72948},{\"end\":72970,\"start\":72967},{\"end\":72985,\"start\":72979},{\"end\":73001,\"start\":72995},{\"end\":73013,\"start\":73009},{\"end\":73021,\"start\":73019},{\"end\":73032,\"start\":73029},{\"end\":73427,\"start\":73420},{\"end\":73439,\"start\":73437},{\"end\":73452,\"start\":73447},{\"end\":73464,\"start\":73460},{\"end\":73478,\"start\":73472},{\"end\":73494,\"start\":73485},{\"end\":73764,\"start\":73755},{\"end\":73774,\"start\":73769},{\"end\":73788,\"start\":73784},{\"end\":73807,\"start\":73796},{\"end\":73813,\"start\":73809},{\"end\":74075,\"start\":74062},{\"end\":74082,\"start\":74080},{\"end\":74096,\"start\":74094},{\"end\":74107,\"start\":74104},{\"end\":74120,\"start\":74112},{\"end\":74133,\"start\":74130},{\"end\":74138,\"start\":74135},{\"end\":74384,\"start\":74362},{\"end\":74400,\"start\":74394},{\"end\":74416,\"start\":74409},{\"end\":74437,\"start\":74429},{\"end\":74455,\"start\":74448},{\"end\":74464,\"start\":74457},{\"end\":74488,\"start\":74472},{\"end\":74500,\"start\":74495},{\"end\":74515,\"start\":74509},{\"end\":74522,\"start\":74517},{\"end\":74933,\"start\":74929},{\"end\":74944,\"start\":74937},{\"end\":74951,\"start\":74946},{\"end\":75249,\"start\":75246},{\"end\":75390,\"start\":75387},{\"end\":75571,\"start\":75568},{\"end\":75836,\"start\":75821},{\"end\":75849,\"start\":75844},{\"end\":75863,\"start\":75857},{\"end\":75877,\"start\":75870},{\"end\":75892,\"start\":75884},{\"end\":75899,\"start\":75894},{\"end\":76542,\"start\":76539},{\"end\":76557,\"start\":76552},{\"end\":76568,\"start\":76561},{\"end\":76581,\"start\":76577},{\"end\":76596,\"start\":76593},{\"end\":76606,\"start\":76604},{\"end\":76618,\"start\":76612},{\"end\":76622,\"start\":76620},{\"end\":76632,\"start\":76626},{\"end\":76644,\"start\":76641},{\"end\":76648,\"start\":76646},{\"end\":77000,\"start\":76996},{\"end\":77012,\"start\":77008},{\"end\":77024,\"start\":77022},{\"end\":77036,\"start\":77031},{\"end\":77364,\"start\":77349},{\"end\":77380,\"start\":77371},{\"end\":77397,\"start\":77391},{\"end\":77409,\"start\":77405},{\"end\":77427,\"start\":77420},{\"end\":77442,\"start\":77436},{\"end\":77454,\"start\":77449},{\"end\":77465,\"start\":77460},{\"end\":77479,\"start\":77473},{\"end\":77493,\"start\":77488},{\"end\":77509,\"start\":77499},{\"end\":77517,\"start\":77511},{\"end\":78135,\"start\":78121},{\"end\":78151,\"start\":78147},{\"end\":78165,\"start\":78160},{\"end\":78180,\"start\":78173},{\"end\":78191,\"start\":78187},{\"end\":78206,\"start\":78202},{\"end\":78214,\"start\":78208},{\"end\":78675,\"start\":78651},{\"end\":78688,\"start\":78685},{\"end\":78703,\"start\":78697},{\"end\":78717,\"start\":78710},{\"end\":78722,\"start\":78719},{\"end\":78976,\"start\":78964},{\"end\":78994,\"start\":78991},{\"end\":79012,\"start\":79003},{\"end\":79027,\"start\":79022},{\"end\":79031,\"start\":79029},{\"end\":79510,\"start\":79494},{\"end\":79521,\"start\":79518},{\"end\":79533,\"start\":79531},{\"end\":79545,\"start\":79543},{\"end\":79559,\"start\":79555},{\"end\":79563,\"start\":79561}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":218971783},\"end\":60665,\"start\":60126},{\"attributes\":{\"doi\":\"arXiv:2304.01373\",\"id\":\"b1\"},\"end\":61267,\"start\":60667},{\"attributes\":{\"doi\":\"arXiv:1803.05457\",\"id\":\"b2\"},\"end\":61736,\"start\":61269},{\"attributes\":{\"doi\":\"arXiv:2204.02311\",\"id\":\"b3\"},\"end\":62236,\"start\":61738},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":52967399},\"end\":62700,\"start\":62238},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":251564521},\"end\":63210,\"start\":62702},{\"attributes\":{\"doi\":\"arXiv:2212.09720\",\"id\":\"b6\"},\"end\":63467,\"start\":63212},{\"attributes\":{\"doi\":\"arXiv:2301.00774\",\"id\":\"b7\"},\"end\":63722,\"start\":63469},{\"attributes\":{\"doi\":\"arXiv:2210.17323\",\"id\":\"b8\"},\"end\":64086,\"start\":63724},{\"attributes\":{\"doi\":\"arXiv:2208.11580\",\"id\":\"b9\"},\"end\":64509,\"start\":64088},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":207926507},\"end\":65177,\"start\":64511},{\"attributes\":{\"doi\":\"arXiv:2103.13630\",\"id\":\"b11\"},\"end\":65582,\"start\":65179},{\"attributes\":{\"id\":\"b12\"},\"end\":66197,\"start\":65584},{\"attributes\":{\"doi\":\"arXiv:2102.00554\",\"id\":\"b13\"},\"end\":66650,\"start\":66199},{\"attributes\":{\"doi\":\"arXiv:2203.15556\",\"id\":\"b14\"},\"end\":67186,\"start\":66652},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":235825979},\"end\":67594,\"start\":67188},{\"attributes\":{\"doi\":\"arXiv:2101.05615\",\"id\":\"b16\"},\"end\":68036,\"start\":67596},{\"attributes\":{\"doi\":\"arXiv:2001.08361\",\"id\":\"b17\"},\"end\":68481,\"start\":68038},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":231861390},\"end\":68985,\"start\":68483},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":5151364},\"end\":69567,\"start\":68987},{\"attributes\":{\"doi\":\"arXiv:1609.07843\",\"id\":\"b20\"},\"end\":69833,\"start\":69569},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":216056295},\"end\":70272,\"start\":69835},{\"attributes\":{\"id\":\"b22\"},\"end\":70331,\"start\":70274},{\"attributes\":{\"doi\":\"arXiv:2209.11895\",\"id\":\"b23\"},\"end\":70786,\"start\":70333},{\"attributes\":{\"id\":\"b24\"},\"end\":70847,\"start\":70788},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":202786778},\"end\":71773,\"start\":70849},{\"attributes\":{\"doi\":\"arXiv:2206.09557\",\"id\":\"b26\"},\"end\":72218,\"start\":71775},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":207880425},\"end\":72830,\"start\":72220},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":204838007},\"end\":73360,\"start\":72832},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":160025533},\"end\":73682,\"start\":73362},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":198893658},\"end\":73997,\"start\":73684},{\"attributes\":{\"doi\":\"arXiv:2104.09864\",\"id\":\"b31\"},\"end\":74352,\"start\":73999},{\"attributes\":{\"doi\":\"arXiv:2302.13971\",\"id\":\"b32\"},\"end\":74870,\"start\":74354},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":1545102},\"end\":75196,\"start\":74872},{\"attributes\":{\"id\":\"b34\"},\"end\":75356,\"start\":75198},{\"attributes\":{\"id\":\"b35\"},\"end\":75494,\"start\":75358},{\"attributes\":{\"doi\":\"arXiv:1906.05714\",\"id\":\"b36\"},\"end\":75718,\"start\":75496},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":162183964},\"end\":76481,\"start\":75720},{\"attributes\":{\"doi\":\"arXiv:2109.01652\",\"id\":\"b38\"},\"end\":76906,\"start\":76483},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":221506623},\"end\":77292,\"start\":76908},{\"attributes\":{\"id\":\"b40\"},\"end\":78119,\"start\":77294},{\"attributes\":{\"doi\":\"arXiv:1804.07461\",\"id\":\"b41\"},\"end\":78556,\"start\":78121},{\"attributes\":{\"doi\":\"arXiv:2211.10438\",\"id\":\"b42\"},\"end\":78962,\"start\":78558},{\"attributes\":{\"doi\":\"arXiv:2206.01861\",\"id\":\"b43\"},\"end\":79413,\"start\":78964},{\"attributes\":{\"id\":\"b44\"},\"end\":79723,\"start\":79415},{\"attributes\":{\"id\":\"b45\"},\"end\":80882,\"start\":79725},{\"attributes\":{\"id\":\"b46\"},\"end\":82385,\"start\":80884}]", "bib_title": "[{\"end\":60163,\"start\":60126},{\"end\":62317,\"start\":62238},{\"end\":62755,\"start\":62702},{\"end\":64607,\"start\":64511},{\"end\":67251,\"start\":67188},{\"end\":68561,\"start\":68483},{\"end\":69045,\"start\":68987},{\"end\":69895,\"start\":69835},{\"end\":70917,\"start\":70849},{\"end\":72246,\"start\":72220},{\"end\":72913,\"start\":72832},{\"end\":73413,\"start\":73362},{\"end\":73745,\"start\":73684},{\"end\":74919,\"start\":74872},{\"end\":75819,\"start\":75720},{\"end\":76986,\"start\":76908},{\"end\":80219,\"start\":79725},{\"end\":81333,\"start\":80884}]", "bib_author": "[{\"end\":60185,\"start\":60165},{\"end\":60196,\"start\":60185},{\"end\":60211,\"start\":60196},{\"end\":60228,\"start\":60211},{\"end\":60245,\"start\":60228},{\"end\":60262,\"start\":60245},{\"end\":60282,\"start\":60262},{\"end\":60296,\"start\":60282},{\"end\":60311,\"start\":60296},{\"end\":60319,\"start\":60311},{\"end\":60701,\"start\":60667},{\"end\":60721,\"start\":60701},{\"end\":60737,\"start\":60721},{\"end\":60746,\"start\":60737},{\"end\":60760,\"start\":60746},{\"end\":60772,\"start\":60760},{\"end\":60797,\"start\":60772},{\"end\":60813,\"start\":60797},{\"end\":60822,\"start\":60813},{\"end\":60850,\"start\":60822},{\"end\":60856,\"start\":60850},{\"end\":61366,\"start\":61349},{\"end\":61379,\"start\":61366},{\"end\":61392,\"start\":61379},{\"end\":61408,\"start\":61392},{\"end\":61421,\"start\":61408},{\"end\":61440,\"start\":61421},{\"end\":61458,\"start\":61440},{\"end\":61467,\"start\":61458},{\"end\":61772,\"start\":61738},{\"end\":61786,\"start\":61772},{\"end\":61802,\"start\":61786},{\"end\":61816,\"start\":61802},{\"end\":61829,\"start\":61816},{\"end\":61843,\"start\":61829},{\"end\":61851,\"start\":61843},{\"end\":61862,\"start\":61851},{\"end\":61877,\"start\":61862},{\"end\":61895,\"start\":61877},{\"end\":61905,\"start\":61895},{\"end\":62333,\"start\":62319},{\"end\":62349,\"start\":62333},{\"end\":62361,\"start\":62349},{\"end\":62381,\"start\":62361},{\"end\":62771,\"start\":62757},{\"end\":62783,\"start\":62771},{\"end\":62799,\"start\":62783},{\"end\":62817,\"start\":62799},{\"end\":63286,\"start\":63272},{\"end\":63304,\"start\":63286},{\"end\":63546,\"start\":63531},{\"end\":63560,\"start\":63546},{\"end\":63822,\"start\":63807},{\"end\":63846,\"start\":63822},{\"end\":63859,\"start\":63846},{\"end\":63869,\"start\":63859},{\"end\":64195,\"start\":64180},{\"end\":64206,\"start\":64195},{\"end\":64217,\"start\":64206},{\"end\":64227,\"start\":64217},{\"end\":64625,\"start\":64609},{\"end\":64642,\"start\":64625},{\"end\":64658,\"start\":64642},{\"end\":64674,\"start\":64658},{\"end\":64690,\"start\":64674},{\"end\":64707,\"start\":64690},{\"end\":65283,\"start\":65252},{\"end\":65293,\"start\":65283},{\"end\":65306,\"start\":65293},{\"end\":65311,\"start\":65306},{\"end\":65322,\"start\":65311},{\"end\":65336,\"start\":65322},{\"end\":65345,\"start\":65336},{\"end\":65664,\"start\":65636},{\"end\":65676,\"start\":65664},{\"end\":65690,\"start\":65676},{\"end\":65705,\"start\":65690},{\"end\":65721,\"start\":65705},{\"end\":65738,\"start\":65721},{\"end\":65755,\"start\":65738},{\"end\":65765,\"start\":65755},{\"end\":65775,\"start\":65765},{\"end\":66216,\"start\":66199},{\"end\":66230,\"start\":66216},{\"end\":66243,\"start\":66230},{\"end\":66258,\"start\":66243},{\"end\":66275,\"start\":66258},{\"end\":66737,\"start\":66700},{\"end\":66754,\"start\":66737},{\"end\":66768,\"start\":66754},{\"end\":66788,\"start\":66768},{\"end\":66799,\"start\":66788},{\"end\":66817,\"start\":66799},{\"end\":66825,\"start\":66817},{\"end\":66842,\"start\":66825},{\"end\":66862,\"start\":66842},{\"end\":66875,\"start\":66862},{\"end\":66882,\"start\":66875},{\"end\":67281,\"start\":67253},{\"end\":67295,\"start\":67281},{\"end\":67307,\"start\":67295},{\"end\":67322,\"start\":67307},{\"end\":67330,\"start\":67322},{\"end\":67699,\"start\":67669},{\"end\":67714,\"start\":67699},{\"end\":67727,\"start\":67714},{\"end\":67740,\"start\":67727},{\"end\":67753,\"start\":67740},{\"end\":67767,\"start\":67753},{\"end\":67780,\"start\":67767},{\"end\":68058,\"start\":68038},{\"end\":68074,\"start\":68058},{\"end\":68088,\"start\":68074},{\"end\":68095,\"start\":68088},{\"end\":68111,\"start\":68095},{\"end\":68124,\"start\":68111},{\"end\":68137,\"start\":68124},{\"end\":68148,\"start\":68137},{\"end\":68165,\"start\":68148},{\"end\":68175,\"start\":68165},{\"end\":68183,\"start\":68175},{\"end\":68581,\"start\":68563},{\"end\":68592,\"start\":68581},{\"end\":68601,\"start\":68592},{\"end\":68611,\"start\":68601},{\"end\":68622,\"start\":68611},{\"end\":68629,\"start\":68622},{\"end\":68644,\"start\":68629},{\"end\":68652,\"start\":68644},{\"end\":68662,\"start\":68652},{\"end\":68666,\"start\":68662},{\"end\":69064,\"start\":69047},{\"end\":69078,\"start\":69064},{\"end\":69092,\"start\":69078},{\"end\":69114,\"start\":69092},{\"end\":69129,\"start\":69114},{\"end\":69140,\"start\":69129},{\"end\":69156,\"start\":69140},{\"end\":69169,\"start\":69156},{\"end\":69182,\"start\":69169},{\"end\":69618,\"start\":69602},{\"end\":69633,\"start\":69618},{\"end\":69649,\"start\":69633},{\"end\":69665,\"start\":69649},{\"end\":69911,\"start\":69897},{\"end\":69927,\"start\":69911},{\"end\":69944,\"start\":69927},{\"end\":69962,\"start\":69944},{\"end\":69982,\"start\":69962},{\"end\":70289,\"start\":70276},{\"end\":70301,\"start\":70289},{\"end\":70368,\"start\":70333},{\"end\":70381,\"start\":70368},{\"end\":70397,\"start\":70381},{\"end\":70410,\"start\":70397},{\"end\":70424,\"start\":70410},{\"end\":70438,\"start\":70424},{\"end\":70451,\"start\":70438},{\"end\":70466,\"start\":70451},{\"end\":70476,\"start\":70466},{\"end\":70482,\"start\":70476},{\"end\":70798,\"start\":70790},{\"end\":70932,\"start\":70919},{\"end\":70943,\"start\":70932},{\"end\":70960,\"start\":70943},{\"end\":70972,\"start\":70960},{\"end\":70988,\"start\":70972},{\"end\":71004,\"start\":70988},{\"end\":71020,\"start\":71004},{\"end\":71032,\"start\":71020},{\"end\":71052,\"start\":71032},{\"end\":71065,\"start\":71052},{\"end\":71082,\"start\":71065},{\"end\":71096,\"start\":71082},{\"end\":71109,\"start\":71096},{\"end\":71125,\"start\":71109},{\"end\":71140,\"start\":71125},{\"end\":71156,\"start\":71140},{\"end\":71177,\"start\":71156},{\"end\":71193,\"start\":71177},{\"end\":71202,\"start\":71193},{\"end\":71214,\"start\":71202},{\"end\":71232,\"start\":71214},{\"end\":71878,\"start\":71866},{\"end\":71893,\"start\":71878},{\"end\":71907,\"start\":71893},{\"end\":71923,\"start\":71907},{\"end\":71937,\"start\":71923},{\"end\":71950,\"start\":71937},{\"end\":72278,\"start\":72248},{\"end\":72291,\"start\":72278},{\"end\":72305,\"start\":72291},{\"end\":72323,\"start\":72305},{\"end\":72348,\"start\":72323},{\"end\":72358,\"start\":72348},{\"end\":72379,\"start\":72358},{\"end\":72393,\"start\":72379},{\"end\":72413,\"start\":72393},{\"end\":72419,\"start\":72413},{\"end\":72929,\"start\":72915},{\"end\":72943,\"start\":72929},{\"end\":72957,\"start\":72943},{\"end\":72972,\"start\":72957},{\"end\":72987,\"start\":72972},{\"end\":73003,\"start\":72987},{\"end\":73015,\"start\":73003},{\"end\":73023,\"start\":73015},{\"end\":73034,\"start\":73023},{\"end\":73429,\"start\":73415},{\"end\":73441,\"start\":73429},{\"end\":73454,\"start\":73441},{\"end\":73466,\"start\":73454},{\"end\":73480,\"start\":73466},{\"end\":73496,\"start\":73480},{\"end\":73766,\"start\":73747},{\"end\":73776,\"start\":73766},{\"end\":73790,\"start\":73776},{\"end\":73809,\"start\":73790},{\"end\":73815,\"start\":73809},{\"end\":74077,\"start\":74062},{\"end\":74084,\"start\":74077},{\"end\":74098,\"start\":74084},{\"end\":74109,\"start\":74098},{\"end\":74122,\"start\":74109},{\"end\":74135,\"start\":74122},{\"end\":74140,\"start\":74135},{\"end\":74386,\"start\":74354},{\"end\":74402,\"start\":74386},{\"end\":74418,\"start\":74402},{\"end\":74439,\"start\":74418},{\"end\":74457,\"start\":74439},{\"end\":74466,\"start\":74457},{\"end\":74490,\"start\":74466},{\"end\":74502,\"start\":74490},{\"end\":74517,\"start\":74502},{\"end\":74524,\"start\":74517},{\"end\":74935,\"start\":74921},{\"end\":74946,\"start\":74935},{\"end\":74953,\"start\":74946},{\"end\":75251,\"start\":75242},{\"end\":75392,\"start\":75383},{\"end\":75573,\"start\":75562},{\"end\":75838,\"start\":75821},{\"end\":75851,\"start\":75838},{\"end\":75865,\"start\":75851},{\"end\":75879,\"start\":75865},{\"end\":75894,\"start\":75879},{\"end\":75901,\"start\":75894},{\"end\":76544,\"start\":76533},{\"end\":76559,\"start\":76544},{\"end\":76570,\"start\":76559},{\"end\":76583,\"start\":76570},{\"end\":76598,\"start\":76583},{\"end\":76608,\"start\":76598},{\"end\":76620,\"start\":76608},{\"end\":76624,\"start\":76620},{\"end\":76634,\"start\":76624},{\"end\":76646,\"start\":76634},{\"end\":76650,\"start\":76646},{\"end\":77002,\"start\":76988},{\"end\":77014,\"start\":77002},{\"end\":77026,\"start\":77014},{\"end\":77038,\"start\":77026},{\"end\":77366,\"start\":77349},{\"end\":77382,\"start\":77366},{\"end\":77399,\"start\":77382},{\"end\":77411,\"start\":77399},{\"end\":77429,\"start\":77411},{\"end\":77444,\"start\":77429},{\"end\":77456,\"start\":77444},{\"end\":77467,\"start\":77456},{\"end\":77481,\"start\":77467},{\"end\":77495,\"start\":77481},{\"end\":77511,\"start\":77495},{\"end\":77519,\"start\":77511},{\"end\":78137,\"start\":78121},{\"end\":78153,\"start\":78137},{\"end\":78167,\"start\":78153},{\"end\":78182,\"start\":78167},{\"end\":78193,\"start\":78182},{\"end\":78208,\"start\":78193},{\"end\":78216,\"start\":78208},{\"end\":78677,\"start\":78648},{\"end\":78690,\"start\":78677},{\"end\":78705,\"start\":78690},{\"end\":78719,\"start\":78705},{\"end\":78724,\"start\":78719},{\"end\":78978,\"start\":78964},{\"end\":78996,\"start\":78978},{\"end\":79014,\"start\":78996},{\"end\":79029,\"start\":79014},{\"end\":79033,\"start\":79029},{\"end\":79512,\"start\":79494},{\"end\":79523,\"start\":79512},{\"end\":79535,\"start\":79523},{\"end\":79547,\"start\":79535},{\"end\":79561,\"start\":79547},{\"end\":79565,\"start\":79561}]", "bib_venue": "[{\"end\":64856,\"start\":64790},{\"end\":69258,\"start\":69236},{\"end\":76077,\"start\":75990},{\"end\":60380,\"start\":60319},{\"end\":60943,\"start\":60872},{\"end\":61347,\"start\":61269},{\"end\":61960,\"start\":61921},{\"end\":62460,\"start\":62381},{\"end\":62934,\"start\":62817},{\"end\":63270,\"start\":63212},{\"end\":63529,\"start\":63469},{\"end\":63805,\"start\":63724},{\"end\":64178,\"start\":64088},{\"end\":64788,\"start\":64707},{\"end\":65250,\"start\":65179},{\"end\":65634,\"start\":65584},{\"end\":66392,\"start\":66291},{\"end\":66698,\"start\":66652},{\"end\":67381,\"start\":67330},{\"end\":67667,\"start\":67596},{\"end\":68238,\"start\":68199},{\"end\":68724,\"start\":68666},{\"end\":69234,\"start\":69182},{\"end\":69600,\"start\":69569},{\"end\":70033,\"start\":69982},{\"end\":70535,\"start\":70498},{\"end\":71293,\"start\":71232},{\"end\":71864,\"start\":71775},{\"end\":72500,\"start\":72419},{\"end\":73070,\"start\":73034},{\"end\":73507,\"start\":73496},{\"end\":73826,\"start\":73815},{\"end\":74060,\"start\":73999},{\"end\":74585,\"start\":74540},{\"end\":75027,\"start\":74953},{\"end\":75240,\"start\":75198},{\"end\":75381,\"start\":75358},{\"end\":75560,\"start\":75496},{\"end\":75988,\"start\":75901},{\"end\":76531,\"start\":76483},{\"end\":77089,\"start\":77038},{\"end\":77347,\"start\":77294},{\"end\":78317,\"start\":78232},{\"end\":78646,\"start\":78558},{\"end\":79169,\"start\":79049},{\"end\":79492,\"start\":79415},{\"end\":80261,\"start\":80221},{\"end\":81385,\"start\":81335}]"}}}, "year": 2023, "month": 12, "day": 17}