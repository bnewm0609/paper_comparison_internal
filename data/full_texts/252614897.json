{"id": 252614897, "updated": "2023-02-08 14:29:17.564", "metadata": {"title": "CoScal: Multifaceted Scaling of Microservices With Reinforcement Learning", "authors": "[{\"first\":\"Minxian\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Chenghao\",\"last\":\"Song\",\"middle\":[]},{\"first\":\"Shashikant\",\"last\":\"Ilager\",\"middle\":[]},{\"first\":\"Sukhpal\",\"last\":\"Gill\",\"middle\":[\"Singh\"]},{\"first\":\"Juanjuan\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Kejiang\",\"last\":\"Ye\",\"middle\":[]},{\"first\":\"Chengzhong\",\"last\":\"Xu\",\"middle\":[]}]", "venue": "IEEE Transactions on Network and Service Management", "journal": "IEEE Transactions on Network and Service Management", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "The emerging trend towards moving from monolithic applications to microservices has raised new performance challenges in cloud computing environments. Compared with traditional monolithic applications, the microservices are lightweight, fine-grained, and must be executed in a shorter time. Efficient scaling approaches are required to ensure microservices\u2019 system performance under diverse workloads with strict Quality of Service (QoS) requirements and optimize resource provisioning. To solve this problem, we investigate the trade-offs between the dominant scaling techniques, including horizontal scaling, vertical scaling, and brownout in terms of execution cost and response time. We first present a prediction algorithm based on gradient recurrent units to accurately predict workloads assisting in scaling to achieve efficient scaling. Further, we propose a multi-faceted scaling approach using reinforcement learning called CoScal to learn the scaling techniques efficiently. The proposed CoScal approach takes full advantage of data-driven decisions and improves the system performance in terms of high communication cost and delay. We validate our proposed solution by implementing a containerized microservice prototype system and evaluated with two microservice applications. The extensive experiments demonstrate that CoScal reduces response time by 19%-29% and decreases the connection time of services by 16% when compared with the state-of-the-art scaling techniques for Sock Shop application. CoScal can also improve the number of successful transactions with 6%-10% for Stan\u2019s Robot Shop application.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/tnsm/XuSIGZYX22", "doi": "10.1109/tnsm.2022.3210211"}}, "content": {"source": {"pdf_hash": "e005b7b08cd8282f0e6e2ae0789229d0dd678d37", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://qmro.qmul.ac.uk/xmlui/bitstream/123456789/82006/2/Gill%20CoScal:%20Multi-faceted%20Scaling%202022%20Accepted.pdf", "status": "GREEN"}}, "grobid": {"id": "48cd6352a49498c559200d20a7dd60eb4770380e", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/e005b7b08cd8282f0e6e2ae0789229d0dd678d37.txt", "contents": "\nNational Natural Science Foundation of China (No. 62102408, 62072451), Shenzhen Science and Technology Program (Grant No. RCBS20210609104609044\n\n\nIeee Transactions \nNetwork \nService \nManagement \n\nDepartment of Informatics\nShashikant Ilager is with\nShenzhen Institute of Advanced Technology\nChinese Academy of Sciences\n518055ShenzhenChina\n\n\nSchool of Electronic Engineering and Computer Science\nSukhpal Singh Gill is with\nVienna University of Technology\n1040ViennaQueenAustria\n\n\nXu is with the State Key Laboratory of IoTSC\nMary University of London\nE1 4NSLondonU.K\n\n\nUniversity of Macau\nMacauChina\n\nNational Natural Science Foundation of China (No. 62102408, 62072451), Shenzhen Science and Technology Program (Grant No. RCBS20210609104609044\n\nYouth Innovation Promotion Association CAS (2019349), and CCF-Huawei Innovative Research Plan\n19410.1109/TNSM.2022.3210211Manuscript received 14 March 2022; revised 20 July 2022 and 20 September 2022; accepted 24 September 2022. Date of publica-tion 28 September 2022; date of current version 31 January 2023. This work is supported by3995 The associate editor coordinating the review of this article and approving it for publication was N. Zincir-Heywood. (Corresponding author: Kejiang Ye.)Index Terms-Cloud computingworkload predictionmicroservicesreinforcement learningbrownoutscalability\nThe emerging trend towards moving from monolithic applications to microservices has raised new performance challenges in cloud computing environments. Compared with traditional monolithic applications, the microservices are lightweight, fine-grained, and must be executed in a shorter time. Efficient scaling approaches are required to ensure microservices' system performance under diverse workloads with strict Quality of Service (QoS) requirements and optimize resource provisioning. To solve this problem, we investigate the trade-offs between the dominant scaling techniques, including horizontal scaling, vertical scaling, and brownout in terms of execution cost and response time. We first present a prediction algorithm based on gradient recurrent units to accurately predict workloads assisting in scaling to achieve efficient scaling. Further, we propose a multi-faceted scaling approach using reinforcement learning called CoScal to learn the scaling techniques efficiently. The proposed CoScal approach takes full advantage of data-driven decisions and improves the system performance in terms of high communication cost and delay. We validate our proposed solution by implementing a containerized microservice prototype system and evaluated with two microservice applications. The extensive experiments demonstrate that CoScal reduces response time by 19%-29% and decreases the connection time of services by 16% when compared with the state-of-the-art scaling techniques for Sock Shop application. CoScal can also improve the number of successful transactions with 6%-10% for Stan's Robot Shop application.\n\nI. INTRODUCTION\n\nT HE CLOUD computing paradigm needs to satisfy strict performance requirements for diverse users hosting heterogeneous workloads. These cloud workloads are driven by applications belonging to various domains, including enterprise businesses and government services that require uninterrupted, reliable service delivery [1], [2]. The ever-increasing complexity of these large applications has recently stipulated moving towards microservice-based application development and deployment. The microservice paradigm has shifted the traditional monolithic application design into decomposed and self-contained standalone application components, which generally communicate through a RESTful Application Programming Interface (API) [3], [4]. The features of microservices include lightweight design, flexible development, continuous deployment, and independent management. These attractive features have promoted cloud computing providers, including Amazon, Google, and Alibaba, to adopt microservice-based application service delivery models and platforms.\n\nThe users request on-demand services from cloud service providers under the specified Quality of Service (QoS) requirements with the pay-as-you-go model [5]. If the QoS is unsatisfied, service providers may suffer revenue loss due to a Service Level Agreement (SLA) violation with their customers. The QoS is defined in terms of different performance metrics, including resource availability and latency. Satisfying the QoS of microservices-based applications is more challenging than traditional monolithic applications since the performance of microservices is more sensitive to latency. This is further complicated by dynamic workload levels. Thus, service providers aim to allocate sufficient resources to application workloads to avoid QoS degradation and performance bottlenecks and prevent huge revenue loss.\n\nOver-provisioning is an effective approach to ensure the QoS of microservices. The over-provisioning method aims to allocate additional resources for user workloads to provide guaranteed performance [6]. However, provisioning as many resources as possible is cost-ineffective due to the finite hardware resources, energy budget, and operational costs of the cloud data centers [7]. Maintaining a large number of physical machines with lower utilization can lead to higher costs for service providers. Therefore, resource scaling approaches have been applied to optimize resource planning and provisioning in microservice-based applications. The resource scaling methods are significant for infrastructure providers as they can contribute to cost reduction, increase resource utilization, and simultaneously improve the QoS parameters of microservice applications.\n\nThe dominant resource scaling approaches for microservices can be classified into two categories, horizontal scaling and vertical scaling [8]. Horizontal scaling adjusts the provisioned resource quantities by adding or removing microservice replicas to improve resource usage and QoS parameters (e.g., system availability, latency). In contrast, vertical scaling adjusts the capabilities of existing provisioned resources by increasing or decreasing the amount of CPU, memory, and network resources. These two approaches have both been validated to be effective for resource scaling.\n\nCurrently, Kubernetes (promoted by Google) has become the most popular platform for container-based microservice application deployment in Cloud [9]. Kubernetes supports horizontal scaling based on a runtime usage threshold. However, such a threshold-based scaling method only works well in simple cases, and achieves sub-optimal solutions in complex workload conditions (e.g., dynamic workload conditions). To address this issue, many studies have proposed different auto-scaling algorithms based on the default Kubernete's auto-scaling algorithms [10]. Nevertheless, these algorithms primarily focus on a single type of resource, e.g., CPU, memory, or bandwidth which are infeasible for many workload scenarios. In addition, Horizontal and vertical scaling has many practical limitations when applied individually. For instance, the communication overhead of horizontal scaling are non-trivial, which directly impacts application QoS, especially in microservice-based applications due to their shorter execution time and increased latency sensitivity. Although vertical scaling can increase the capacity of a provisioned physical machine's resource, upgrading the resource capacity in runtime is expensive. A recent study has considered combining these two scaling approaches together [8], although it does not address the complex real world scenarios such as infrequent bursty workloads that leads to overloading of the whole system, thus, affecting overall infrastructure and application QoS [11].\n\nSome recent methods, such as brownout techniques, [12] have explored an alternative scaling method complementing both the horizontal and vertical scaling. Brownout is a selfadaptive approach for managing application components by dynamically activating and deactivating optional application components to be adaptive to the variance of workloads. The brownout can effectively address the limitations of vertical scaling (limited resource capacity of local machines) and horizontal scaling (the resource usage of replicating the microservice to other machines) in overloaded conditions. In the microservice-based application, a brownout can temporarily deactivate some of the optional microservices to reduce resource usage while ensuring the necessary functionalities of microservice applications. Thus, comprehensive scaling techniques addressing the limitations of existing individualistic approaches are required for adaptive and efficient scaling in microservice environments.\n\n\nA. Existing Challenges\n\nThe efficient scaling of microservices poses several significant challenges. Firstly, the micro-service based cloud workloads have high-variance in resource usage and are sensitive to resource types (CPU, network, memory). It is difficult to predict the accurate amount of workloads in a specific time period. An accurate workload prediction is an essential element for auto-scaling approaches in microservices, e.g., in horizontal scaling, the predicted workload can enable service providers to boot up and deploy applications beforehand to avoid startup cost and QoS degradation. Secondly, assigning resources to microservices is an NP-hard problem considering the multi-dimensional resources. Due to the various run-time parameters of microservices and hardware configurations, it is time-consuming to find the optimal results given the large solution space [13]. Thirdly, there are trade-offs considering when and how to trigger the auto-scaling algorithms to handle the situations with high dynamics while ensuring the QoS requirement. We propose multi-faceted, data-driven, and adaptive auto-scaling approaches for microservices to address the above discussed challenges.\n\n\nB. Our Contributions\n\nIn this paper, we investigate the promising approach called CoScal that combines horizontal scaling, vertical scaling, and brownout techniques to address the resource scaling problem in microservice-based cloud computing environments. The proposed approach exploits the advantages of above-mentioned individual techniques, including the high availability of horizontal scaling, fine-grained control of vertical scaling, and self-adaptability of brownout. This combined auto-scaling approach is more effective under diverse and complex workload scenarios derived from Alibaba traces [14]. The key contributions of this paper are:\n\n\u2022 A performance analysis of horizontal scaling, vertical scaling, and Brownout investigates the trade-offs in execution costs and performance of microservices. \u2022 A Reinforcement Learning (RL) based scaling algorithm for decision making to optimize the performance of microservices, such as response time, connection time, and the number of failed requests. \u2022 A prototype system implementing the proposed approach is evaluated with realistic traces and extensive experiments. The rest of the paper is organized as follows: Section II discusses the related work for auto-scaling microservices in the cloud computing environment. The motivation and performance analysis of the applied techniques in this work are introduced in Section III. Section IV depicts the system model of our proposed approach. The proposed algorithm based on RL is detailed introduced in Section V. The Section VI illustrates the details of our experiments conducted using a dataset derived from realistic traces and demonstrates the feasibility of our approach to improve the resource scaling of cloud data centers. Finally, conclusions along with the future directions are given in Section VII.\n\n\nII. RELATED WORK\n\nIn this section, we discuss existing research works in microservices scaling. The current scaling approaches for performance optimization in microservices can be mainly categorized into three categories: the performance model, resource orchestration, and prototype systems.\n\n\nA. Performance Model\n\nThe performance model is applied for modeling the resource usage of the system and the performance of cloud services. Thus, resource management policies can be designed to fit the performance requirements. Nishtala et al. [15], [16] proposed scalable QoS-aware resource management approaches for latency-critical and co-located services in traditional cloud data centers. The proposed approaches are based on heuristic and reinforcement learning techniques to achieve energyefficient goals, which require no service or system-specific information. Liu [17] analyzed the bottlenecks of the mainstream microservice applications and applied multiple machine learning models to schedule resources. The first model is used for finding the resource area that satisfies the microservice performance. The second model investigates the trade-offs between the QoS and allocated resources, and the third model dynamically adjusts resources according to the system states. Chang et al. [18] designed an automatic resource planning approach and modeled application performance based on an empirical model. Khazaei et al. [19] constructed a performance model via analyzing microservice platforms. This approach applies the Markov chain model to represent microservice resources, virtual machine (VM) resources, and physical machine resources. The number of tasks, number of microservices, and number of VMs have all been considered as the system states. Although the performance model is a popular approach for performance optimization, the model needs to be reconfigured or retrained when the service or environment changes dramatically. Gan et al. [20] exploited the prediction method for QoS violations, which utilizes a set of machine learning models and large-scale history data to locate the microservices that lead to QoS violations. The approach can relieve the QoS degradation by reallocating hardware resources. Qiu et al. [10] proposed a fine-grained control framework to relieve resource competition and optimize resource utilization and analyzed the microservice dependency relationship, microservice chains, and key call paths. Kannan et al. [21] modeled the multiple-stage tasks as Directed Acyclic Graph (DAG) and used DAG to estimate the task completion time. Yu et al. [22] proposed Microscaler framework by using service mesh to record the resource usage behaviors and applied online learning and heuristic approaches to obtain the near-optimal solutions for resource demands.\n\nScaling commands must be configured manually based on the performance model in all these approaches. Thus, they are far slower to react to load variations. We apply neural network-based workload prediction to achieve high adaptation to load variances to solve this challenge.\n\n\nB. Resource Orchestration\n\nOther works apply vertical scaling or horizontal scaling that are not required to be configured manually based on the performance model, which utilizes resource orchestration to optimize resource provisioning for microservices by allocating and managing resources efficiently. Suresh et al. [23] investigated the overload control mechanism for a microservice cluster with complex dependency. The proposed approach considers the resource sharing problem under a multi-tenancy scenario, in which all the tasks are modeled as DAG. The scheduling is processed at the workflow level and requests level separately. Zhou et al. [24] pointed out that monitoring and collecting the data of each microservice under largescale and high-dynamicity scenarios is not feasible. Therefore, authors proposed a workload control approach to maintain a self-adaptive threshold and algorithm for each microservice. Each microservice can shed the loads independently with quite limited communication costs. Rzdaca et al. [25] designed Auto-pilot as an auto-scaling approach to scale based on the change of workloads. Auto-pilot combines timeseries analysis and scaling the number of microservices with related CPU and memory amount. Kwan et al. [8] designed an approach that combines vertical and horizontal scaling methods, which deploy microservices to suitable hardware to efficiently reduce service delay due to burst requests. Hou et al. [26] introduced a power-aware and latency-aware scheduling approach to scale resources from micro and macro perspectives. They also apply the decision tree and tagging method to achieve fast resources matching. He et al. [27] took advantage of genetic and heuristic algorithms to find the optimized microservice deployment place under an edgecloud environment. Zhang et al. [28] proposed a predictive RL algorithm to horizontally scale containers based on the Autoregressive Integrated Moving Average (ARIMA) model and neural network model, which can ensure the predictability and accuracy of the scaling process. Rossi et al. [29] introduced RL-based approaches to control the horizontal and vertical scaling for containers to increase system flexibility under varying workloads, which also accelerate the learning process by exploiting different degrees of knowledge about the environment. However, this work was only evaluated with synthetic workloads. Gias et al. [30] proposed a modeldriven scaling approach, named ATOM, for microservices via analyzing a layered queueing network of applications. This approach can dynamically adjust the number of replications via horizontal scaling.\n\nIn resource orchestration approaches, limited hybrid scaling for microservices has been proposed, and most of them focused on a single scaling technique, either vertical or horizontal, but not on both. The existing hybrid approaches are threshold-based or heuristic that requires heavy manual configurations. Unlike these efforts, we apply a hybrid scaling technique that combines vertical, horizontal, and brownout together, making adaptive decisions via RL.\n\n\nC. Prototype Systems and Tools\n\nKubernetes [31] and Docker Swarm have become the dominant systems for managing microservices. However, the  [32] implemented a generic microservice orchestration platform for heterogeneous cloud clusters, which can auto-scale resources without binding to specific applications. Zhou et al. [33] analyzed and compared the interaction patterns of open-source microservicebased applications. They found that the investigated open source applications are small-scale and provided a mediumscale application called TrainTicket. Xu et al. [12] proposed a prototype system for managing co-located interactive and batch microservices based on the brownout approach and workloads deferral to achieve an energy-efficient cloud data center.\n\nIn these prototype systems and tools, threshold-based heuristic algorithms are applied popularly. However, heuristic algorithms can find a solution quickly, the performance needs of manually tuning various configurations, especially in an environment with high dynamics [29]. Unlike these approaches, we utilize the RL-based approach with an adaptive nature to learn and make good decisions via interactions with the environment.\n\n\nD. Critical Analysis\n\nThis paper contributes to the growing body of research related to the microservice area. The Table I compares the proposed approach (CoScal) with related works based on the scaling techniques, workload prediction techniques, and type of scheduling algorithms. Given the contributions of the existing works, it is important to highlight the key difference between our work and the prior ones. To the best of our knowledge, the proposed work is the first to offer a multi-faceted scaling approach based on vertical scaling, horizontal scaling, and brownout. Prior works only applied included one or two techniques. We also utilize gradient recurrent units for accurate workload prediction and apply an RL-based algorithm for resource management to offer flexible adaption for a highly dynamic environment.\n\n\nIII. MOTIVATION: PERFORMANCE TRADE-OFFS IN INDIVIDUAL SCALING TECHNIQUES\n\nIn this section, we conduct experiments to investigate the effects of individual scaling techniques (vertical, horizontal, and brownout) on important performance metrics \n\n\nA. Use Case Setup\n\nIn this motivational study, we have used a microservice application named Sock Shop, 1 which is an online e-commerce website for products sale. The Sock Shop application consists of multiple microservices representing different components of the application, including front-end User Interface (UI), catalog, carts, and some other supporting microservices, which are deployed on two Docker Swarm nodes.\n\nIn this study, we consider the browsing workload for the Sock Shop application. The users can view the information of the items, add or remove items in the carts, pay for the items. The used microservices in the Sock Shop application and the request distribution are summarized in Table II. We have investigated the high load use case representing the system's over-utilized (when utilization is above a predefined threshold) condition from Alibaba's sample trace [14]. The sample trace contains 1200 lines of data representing resource utilization in 6000 seconds, where the data is collected every 5 seconds. We use JMeter 2 toolkit to generate workloads for the Sock Shop application, which simulates the scenario of users visiting the website for shopping. JMeter is an open-source and completely Java-based application. It is mainly used to conduct stress tests of the target server and validate test-case functionalities of service endpoints. Only one node is initially deployed with microservices in our setup, and the second node is kept in an idle state. In the horizontal scaling, the replicates of Sock Shop microservices are deployed on the second node. In the vertical scaling, the CPU is configured to be scaled to a maximum of 8 cores, and the memory can be scaled to a maximum of 3.5 GB. In the brownout approach, the optional microservices (3 out of 14, i.e., the recommendation engine) are temporarily deactivated (due to space limitation, please see the more detailed descriptions in Section V-A).\n\nWe apply the individual scaling techniques separately to evaluate system performance regarding connection time, response time, and success rate of requests. Connection time represents the time required to establish a connection between the user and the target microservice server. Response time represents the total time from the start of the request to the reception of the response. Success rate represents the percentage of successful requests (i.e., expected response is received for request) received by the user.\n\n\nB. Comparison Between Scaling Techniques\n\nFig. 1(a) represents the connection time achieved by three different scaling techniques. The collected data is obtained every 200 ms, and the average value is calculated based on every 5-minute interval. We choose 5-minute interval based on Alibaba's practice where 5-minute minimum time interval is considered for scheduling decisions. A smaller time interval can lead to frequent scaling costs, while a longer time interval cannot respond to the system change promptly. As this is a motivational example, we only conduct the experiments one time. We can observe that the vertical scaling converges faster to a stable connection time than horizontal and brownout scaling. However, at the initial period of scaling, its performance for connection time is not as good as brownout. The brownout approach initially achieves the best connection time as it deactivates optional parts that help provide additional resources for the front-end connections. After some periods, the connection time keeps growing because all the available optional components have been deactivated. Horizontal scaling can achieve better connections than brownout since more resources are provided regarding the number of nodes. However, the extra communication costs involving two replicas of services deployed on different hosts induce higher connection time in horizontal scaling than vertical scaling, as the communication in vertical scaling is negligible as it remain on the same host.\n\nFigs. 1(b) and 1(c) represent the response time and success rate achieved by different scaling methods. The vertical scaling achieves the best performance with the lowest response time and the highest success rate. The reason is that the vertical scaling on the local machine can improve the system performance without incurring additional overheads. In contrast, Brownout can incur some delays induced by its resource optimization technique. Moreover, the horizontal scaling leads to poor performance in response time and success rate of user requests due to its associated overhead, such as bootup cost of new instances and frequent scaling operations, which is crucial in highly time-sensitive microservice environments. Although vertical scaling achieves the best performance when compared to the other two techniques, it is not effective when the local machine is overloaded and constrained by the maximum resource capacity of an individual physical machine.\n\nTo summarize, individual scaling techniques may not yield the desired result in complex conditions. A multi-faceted scaling can solve the limitations of individual techniques. For instance, when a limited capacity is left on a hosted machine (i.e., vertical scaling hits resource limitations), the brownout technique can be triggered to maintain the connection time by relieving resource overloading. Meanwhile, the horizontal scaling can be applied when enough resources are available to host additional workload, and optional components can be reactivated further. However, the horizontal scaling requires more time to take effect compared to brownout as it requires significant time to activate host and initialize new container instances. Thus, under the high-variable workloads, the choice and combination of scaling techniques play an important role in delivering reliable services, especially in latency-sensitive microservice environments. Therefore, to overcome the limitations of individual scaling techniques, a joint, data-driven, and adaptive auto-scaling framework is necessary to provide optimal scaling decisions based on the current infrastructure and workload conditions.\n\n\nIV. SYSTEM MODEL\n\nIn this section, we describe our proposed auto-scaling system named as CoScal that is composed of three modules, including Workload Analyzer, Workload Predictor, and RL-based Resource Scaler, as shown in Fig. 2. It is noted that this is a general system model, and the modules can be extended to new workload analysis techniques and more advanced RL-based resource scheduling solutions. The details of these modules are given in the following sections:  \n\n\nA. Workload Analyzer\n\nThe Workload Analyzer module analyzes system workload and processes the raw data generated by log files. In this module, the Requests Handler handles the usergenerated requests, which allocates the requests to hosts. The Pre-processor extracts the required meta information of workloads (e.g., time series and resource utilization metrics). Later, the Workload Predictor module takes our inputs to perform its stated operations (described in Section IV-B). Fig. 3 shows a schematic diagram of load generation based on JMeter. In this case, The JMeter Master and Workers (Slaves) are regarded as part of the User Requests and Requests Handler components. Web servers are the infrastructure to provision resources. Based on our practice, when the number of requests is huge, a single JMeter server mode may not function well due to overloads, then the master-worker will be used collaboratively. With each test case, the data of collected metrics will be stored locally, such as timestamps, response time, connection time, and the number of failed requests.\n\n\nB. Workload Predictor\n\nThe Workload Predictor module in Fig. 2 is responsible for estimating the expected load in the next scheduling window, which guides scaling methods to decide the amount of resources to be added or removed. This module can be constituted by different predictive models such as Multivariate Time Series Forecaster [34], which accepts the pre-processed information of workloads from the Workload Preprocessor and predicts the future workload based on suitable predictive models, such as long short-term memory (LSTM) or gated recurrent unit (GRU) model. The responsibility of the TensorFlow Server is to manage the Trained Model, including the training process of the models and updating the trained models if necessary, and finally deploying trained models. As shown in Fig. 2, the communications between Workload Analyzer and Workload Predictor transfer the inputs with past system data to the TensorFlow Server (the left-bottom corner of this module). It provides feedback such as predicted workload level to Requests Handler.\n\nTo generate user requests for the prototype system, the load generator toolkits, e.g., JMeter, can be utilized. And the typical microservice application, like Sock Shop [35] as introduced in Section III, can be deployed as benchmark application. The profiling approach based on the stress test can establish a model to represent the relationship between the number of requests and resource utilization (see Section V-A for more details).\n\n\nC. RL-Based Resource Scaler\n\nThe CoScal Controller is the core component in this module which makes decisions on scaling strategies based on the RL framework. Compared with static performance models and heuristic-based approaches that suffer from model reconstructions and retraining problems, the RL-based approach is well suited for learning resource scaling policies to address dynamic system status [36]. The RL-based Resource Scaler in Fig. 2 receives the user requests from Request Handler and information like predicted workloads from Workload Predictor in our system. After executing the scaling techniques, the CoScal Controller collects the data of performance metrics, such as response time, and sends the data back to the users. The data is the response to the user request to the application.\n\nIn the RL-based Resource Scaler module, the supported techniques work in different manners. Vertical scaling is applied to the local machine with multiple resources, e.g., CPU, memory, and network capacity. The vertical scaling is faster compared with other techniques as shown in Section III, it can be leveraged initially. The brownout technique is also applied to the local machine, where the optional microservices can be dynamically deactivated to relieve the overloaded situation that vertical scaling cannot handle alone. Horizontal scaling is applied at the node level by adding or removing additional nodes into the system. Considering the communication costs of horizontal scaling, this technique can be applied when vertical scaling and brownout cannot handle the workloads level and keep the system in a normal state. \n\n\nA. Performance Profiling\n\nTo estimate the approximate resource usage corresponding to a different amount of workloads with the Workload Analyzer module in Fig. 2, we use a deep neural network model to profile the performance of machines. To simulate the resource utilization in a realistic environment, we use the data derived from Alibaba traces, including workload traces of 4000 machines' containing resource usage data for 8 days [14].\n\nThe detailed profiling procedures are as follows: we consider the scheduling interval as 5 minutes, as 5 minutes are the minimum monitoring interval for data collection. A short scheduling interval can make too frequent scaling operations to influence system stability, while a long scheduling interval can delay the adaptive scaling decisions. Firstly, we apply stress test by gradually increasing the number of requests over 5 minutes with 200 requests in each test case and sending these requests to the host. As the number of requests increases, the host's resource utilization also increases. Then we use the nmon [37] performance monitoring toolkit to record the change of utilization in the host as per the number of requests sent. In this way, a detailed mapping relationship between resource utilization and the number of requests is obtained. Based on the profiled data, we apply a Multi-Layer Perceptron (MLP) with three layers to establish a model to represent the relationship efficiently. Compared with the traditional regression-based approaches, the MLP-based approaches can capture a more accurate relationship between workloads and utilization. Finally, we can convert the host utilization into the number of requests dispatching to our system for any utilization level with the trained model.\n\nAs shown in Fig. 4, we demonstrate the results between CPU utilization and the number of requests based on the tested host. We can observe the CPU utilization change under the different number of requests. The host consumes about 25% CPU utilization when it does not handle any requests. This is due to the resource consumption by the operating system and \n\n\nB. Neural Network-Based Workload Prediction\n\nTo address the prediction of resource usage levels in the Workload Predictor module in Fig. 2, we realize a neural network-based workload prediction approach. To reduce the state space of the RL model, we consider dividing the workloads into several levels (the exact number of levels configured) representing different levels of utilization. Moreover, this also helps to apply similar scaling techniques for workloads at the same level. This helps to significantly reduce the state-action space in the RL-based approach. We divide the workloads into five levels that can represent the degree of overloads, as shown in Table III. The overloaded threshold is configured as 75%, as it has been evaluated in our previous work [12] that it can achieve good trade-offs between resource utilization and QoS.\n\nTo accurately predict the server load status in the next time interval, we apply the Multivariate Time Series forecasting (MTFS) method, which converts multivariate time series forecasting into supervised learning. The MTFS algorithm can be applied to any time-related dataset, and it is highly correlated to temporal aspects and contains all the data from the previous time intervals. The MTFS makes each generated supervised learning sequence to have sample labeled datasets in the algorithm.\n\nThe Algorithm 1 shows the pseudocode of the MTFS algorithm, which generates labeled time series data for supervised learning. With the original time series dataset E, the algorithm first generates an empty matrix S to store the supervised learning sequence. After the labeled data is generated for supervised learning, to achieve an accurate prediction for workloads, we apply the Gated Recurrent Unit (GRU) [38] derived from Recurrent Neural Network (RNN) [39], which Algorithm 1: CoScal: Workload Preprocessing Input : Multivariate time series dataset E, time intervals T, k time-related variables, and a dataset F needs to be forecast Output: Supervised learning dataset S, each row of it has 2k + 3 data 1 Initialize an empty matrix S to record supervised time series data 2 for t from 1 to T do 3 19 Remove S(t) 20 Update S 21 end has been validated to have better performance in time series related prediction than traditional RNNs. Although RNN can use its memory to process a set of inputs sequentially, it is inefficient to learn long-term memory dependencies due to gradient vanishing, while GRU can overcome this limitation by merging the data processing elements (gates). Table IV shows the mean square errors (MSE) of actual utilization and predicted utilization for Alibaba workloads during different periods. The MSE has lower values about 3 \u00d7 10 \u22123 to 7 \u00d7 10 \u22123 (in [40], the RNN-based and LSTMbased approaches have the MSE values about 4 \u00d7 10 \u22122 ), which demonstrates that our workload prediction algorithm has a good performance in utilization prediction and validates that the neural network based approach is suitable for cloud workloads prediction.\n\n\nRecord data in current time interval: C (t) \u2190 E (t)\n\n\nC. RL-Based Resource Scaling\n\nThe RL-based approach solves sequential decision-making problems by modeling the problem as Markov Decision Process [41]. At each time interval t, the system is at a state s t \u2208 S , and performs an action a t \u2208 A based on policy \u03c0 \u03b8 , where \u03b8 are parameters configured in model. The statespace S is mapped with action space A. In the following time intervals, the current system state can reach another state by taking actions and obtaining reward r t \u2208 R, calculated via reward function r (s t , a t ). The next state will only rely on the current state and the performed actions on it. The rewards represent the benefits that can be achieved by transiting the state from s t to s t+1 . When transferring states, there is also a transition probability of presenting the possibility to take different actions. The objective of RL is to maximize the expected cumulative reward by optimizing policy. Q-learning [42] is a typical type of RL to maximize the value function Q \u03c0 \u03b8 (s, a). The value function estimates the expected cumulative reward of state s with action a under policy \u03c0 \u03b8 . Consider action a t is selected at time interval t, and at time interval t + 1 with reward r (s t , a t ), the value of Q function can be updated as:\nQ(s t , a t ) = Q(s t , a t ) + \u03b1[r (s t , a t )] + \u03b3max a Q s t+1 , a \u2212 Q(s t , a t )(1)\nwhere \u03b1 \u2208 (0, 1] is the learning rate and \u03b3 \u2208 [0, 1] is the discount factor. The Equation defines a mapping table containing states with actions and their expected value. The learning process happens in the form of S * A\u2212 > R over time to achieve optimized results via iterative trials. We consider the state S as workload level S = {0, 1, 2, . . . , W }, where W \u2208 Z represents the maximum level of workloads and is a non-negative value.\n\nTo illustrate our RL-based multi-faceted scaling framework (CoScal), let us assume that we have a set of physical machines P = (pm 1 , pm 2 , . . . , pm K ) as infrastructure to provision resources for microservices. Each pm k can be represented with a tuple U k = (u 1 k , u 2 k , . . . , u I k ), where u i k represents resource utilization of type i with total I types on physical machines. For each pm k , the actions performed on it, which are denoted as\na i k = {h k , v i k , b k }, where h k \u2208 [\u2212n, n] presents the n \u2208 Z number of replicates in horizontal scal- ing, v i k \u2208 [\u2212m, m],\nwhere m \u2208 R, represents the amount of resources via vertical scaling for resource type i, and b k \u2208 {0, 1} represents the whether brownout is triggered (b k = 1) or not (b k = 0). The positive and negative values of h k and v i k represent more resources are added and removed respectively, and value 0 means no change will be performed. Considering the total number of physical machines is K, the final set of actions is the Cartesian product of the sub-action sets as:\nA = K k =1 I i=1 a i k .\nThe objective of our technique is to find the suitable configuration of resources by dynamically adjusting the provisioned resources to adapt to changes in the environments, e.g., the load fluctuations. However, the amount of scaled resources is limited by the available resources on physical machine pm k and the minimum resources allocated to microservices. To avoid unnecessary vertical scaling, we consider adding an action a g to make decisions from the global view. Let us consider the scenario that when the system with P = {pm 1 , pm 2 } is running at the normal states that vertical scaling is sufficient for adjusting resources. However, when unpredictable workloads arrive, the physical machines are overloaded. To handle such bursts, horizontal scaling must be performed on the system. If both pm 1 and pm 2 are completed with vertical scaling, two more physical machines are added. However, the system may only need one more physical machine to keep the system at the normal state. Therefore, to optimize resource usage, the action a g is required to make scaling decisions based on a global view.\n\nCoScal incorporates the offline training and online training approach together to achieve optimized actions. Once the load change is identified, CoScal can select an action in response to the QoS or performance degradation of deployed applications. Given s t as the observed state, a policy exploits the knowledge of previous decisions (offline). It evaluates the performance of new selected actions (online), improving the mapping relationship between states and actions. We apply the -greedy policy, [36] a standard policy to balance exploration and exploitation [43]. In -greedy policy, a random action with probability equals is selected. Otherwise, it chooses the action with the maximum Q value.\n\nThe final objective of CoScal is to improve the QoS of services and utilization of physical machines. The reward is modeled based on this objective which composes of two parts. For the QoS, we choose to use response time as the metric, representing the latency from submitting the request to completion. We model the reward of response time R qos (rt) based on the maximum acceptable response time with RT max . As shown in Equation (2), when the system is working at the normal status, the reward is 1. However, when the system performance violates the RT max , the reward converges to 0, punishing the actions that lead to overloaded situations.\nR qos (rt) = e \u2212 rt\u2212RTmax RTmax 2 , rt > RT max 1, rt \u2264 RT max(2)\nAs for the reward of resource utilization (please note that the utilization can also be referred as resource costs in our model), we model it as shown in Equation (3). Here, U max k defines the maximum utilization threshold of pm k , which is also the highest utilization of all resource types (CPU and memory utilization are both considered). u k is the current utilization of pm k . Higher utilization without violating the threshold can contribute positively to the reward, while utilization above the threshold can undermine the reward.\nR util (u k ) = \u23a7 \u23a8 \u23a9 K k =1 U max k \u2212u k K + 1, u k \u2264 U max k K k =1 u k \u2212U max k K + 1, u k > U max k (3)\nEquation (4) shows the final reward value based on response time and resource utilization, in which the higher values of R qos and lower values of R util can increase the total reward.\nr (s t , a t ) = R qos (rt) R util (ut)(4)\nAlgorithm 2 shows the general procedure of CoScal. The algorithm initializes the monitoring model to collect system status for the RL process (line 1), including workloads level, utilization, and relevant metrics at each time interval (lines 4-5). These observations constitute a state in the RL framework. If the workload level changes (line 6), scaling approaches should be applied to ensure QoS or optimize resource usage. Thus, the Q-learning process will be started by choosing actions from the experience pool and transiting the current system state to another one (line 7). The actions are executed based on the Algorithm 3 (line 8). CoScal also supports online training, after CoScal transits the state s t to s t+1 , CoScal firstly stores the transition (s t , a t , r t , s t+1 ) into \n5 U t k \u2190 Resource utilization of pm k at time interval t; 6 if W p t \u2212 W t\u22121 = 0 then 7\nChoose a action from action set A with probability, or select an action with the max (Q(s t , a t )); 8 Execute a t according to Algorithm 3; 9 if online training is triggered then 10 s t+1 \u2190 system state at time interval t + 1; 11 r t \u2190 reward calculation by Equation (4); Brownout will not be triggered; 16 end 17 /*Horizontal scaling*/ 18 if n > 0 then 19 Add n microservices replicates; 20 else 21 Remove n microservices replicates; 22 end the experience pool. The reward r t is applied to evaluate the goodness of (s t , a t ) (lines [9][10][11][12][13][14].\n12 Update Q value: Q(s t , a t ) = Q(s t , a t ) + \u03b1[r (s t , a t )] + \u03b3max a Q(s t+1 , a ) \u2212 Q(s t ,\nAlgorithm 3 shows the action execution process of CoScal. The system states and corresponding actions are provided by the Algorithm 2, which includes the decisions of horizontal scaling, vertical scaling, and brownout. The algorithm will first check the decision for vertical scaling. If the vertical scaling should be performed, then resources of the specific type will be added or removed (lines 1-10). After that, the brownout will be examined. If the brownout is triggered, the optional microservices are deactivated in this time interval (lines [11][12][13][14][15][16]. And finally, the horizontal scaling will be checked and executed by increasing or decreasing the number of replications. We consider the execution sequence based on the execution costs of different scaling techniques as discussed in Section III, as the vertical scaling can be completed within the shortest time while the horizontal scaling brings much longer processing time and communication costs (inter-process communication costs between processes on different hosts for user authentication, database access, remote function call, image retrieval and etc.) than the other two techniques as shown in the motivational example in Section III.\n\nDecision-making complexity of CoScal: After the RL model is trained based on historical data, the system states and corresponding actions can be stored in a lookup Notes on model re-training: To be noted, when microservices are updated and new optional components are added, the RL model can be re-trained to improve the performance, as the brownout part in CoScal is influenced by the identification of the optional parts. If the model is not re-trained, the added components will be regarded as the mandatory ones.\n\n\nD. Auto-Adapter for Unexpected Workloads Change\n\nAlthough our workload prediction algorithm can achieve high accuracy, fluctuations of cloud workload level during the stable period (workload level remain unchanged) are usually unpredictable. For instance, Fig. 5(a) shows a sample of workload changes within 500 minutes. It can be observed that during the first 100 minutes, the workloads change from workload level 3 to level 4 and return to level 3 within a short time (as the workload level is the average level during a period of time, the changes represent the total amount of workloads change significantly). This often happens when there are bursts in the system. Such workload changes are difficult to be predicted due to randomness.\n\nMicroservices are more sensitive to resource fluctuations than traditional applications [10]. Therefore, bursts can cause QoS violation, resource wastage, and system overload. To address this challenge and complement the prediction algorithm, we introduce a component named auto-adapter that detects and adapts to these changes. The auto-adapter is integrated into the CoScal Controller module in Fig. 2. Autoadapter collects the information at the first minute of each time interval and examines whether the actual workload level (collected via performance counters) equals the predicted one (via workload predictor module). If not equal, auto-adapter applies vertical scaling to optimize resource usage by adding or removing resources according to the predicted and actual workload difference.\n\nThe auto-adapter needs to be lightweight to avoid excessive resource usage incurred by the auto-adapter. We measure the resource usage in one hour caused by auto-adapter as shown in Fig. 5(b), which shows that auto-adapter only costs about a maximum of 3% extra CPU and 1% extra memory resources. This additional limited resource usage is acceptable considering its optimization effects on resource usage.\n\n\nVI. PERFORMANCE EVALUATION\n\nTo evaluate the performance of our proposed approach for scaling microservices, we conduct experiments in the container-based prototype system. We first present the experimental settings in Section VI-A and then introduce the baselines in Section VI-B. The results and analysis are presented in SectionVI-C.\n\n\nA. Experimental Setup\n\nWe built a cluster with heterogeneous nodes for the microservice-based application deployment. The cluster consists of four physical machines, including a machine with an Intel Core i7-9700 CPU and 16GB of RAM, two machines with Intel Core i7-4790 and 4GB of RAM, and a machine with two Intel Xeon E5-2660 CPUs and 48GB of RAM. The containers or microservices are managed through Docker Swarm, a container orchestrating tool. We deployed the Sock Shop application, a microservice-based application. The computing environment has been configured with Java (version 1.8), Python (version 3.7), and TensorFlow (version 2.2.0) development toolkits. We use the cluster-trace-v2018 of Alibaba dataset 3 that contains workloads of machines. And we use 500-minute data for evaluations.\n\nTo enable our scaling algorithms to support the brownout mechanism, we categorize the individual microservices in Sock Shop into two categories: mandatory microservices and optional microservices. Mandatory microservices include microservices that are the key components (e.g., databaserelated microservices) of the system, and the optional microservices (e.g., recommendation engine) can be dynamically activated or deactivated, In our testbed, the CPU can be scaled from one core to a maximum of 8 cores, memory can be scaled from 2 GB to 3.75 GB, and for horizontal scaling, the maximum number of hosts is set to 3.\n\n\nB. Baselines\n\nWe compare the performance of CoScal with several stateof-the-art algorithms for scaling microservices. Some baselines have been used in dominant microservice platforms, including Docker Swarm and Kubernetes. DoScal [29]: it is derived from the resource scaling approach implemented in native Docker Swarm, which is mainly based on vertical scaling. For instance, when one of the microservice instances is overloaded, DoScal can reallocate the microservices equipped with sufficient resources.\n\nKuScal [31]: this scaling approach has been used in Kubernetes, which is mainly based on horizontal scaling that can dynamically increase or decrease the number of replicates. KuScal decides on how many replicates should be added or removed based on the resource fluctuations, e.g., CPU and memory, on the current servers.\n\nHyScal [8]: it is a hybrid scaling approach for microservices. Apart from scaling CPU utilization, the HyScal algorithm also scales memory resources. The HyScal algorithm applies a similar approach to the KuScal, allocating resources and monitoring resource utilization.\n\n\nC. Experiment Analysis\n\nThe results are collected from JMeter log files, including connection time, response time, and a number of failed requests. The lower values of connection time and response time represent better QoS performance. A lower number of failed requests is desired, indicating a higher number of successful requests processed by the deployed applications.\n\nWe conducted 500-minute experiments to accurately observe and analyze the advantages and disadvantages of the scaling approaches in different periods. We demonstrated the average value of each metric with an interval of 100 minutes (i.e., 100 minutes, 200 minutes, to 500 minutes) to represent the statistical change, and the experiments are repeated 3 times to avoid randomness.\n\nConnection Time: Fig. 6(a) shows the comparison of the average value of connection time during 500 minutes. The results of the baselines vary between 0.75 ms and 1.25 ms. More specifically, HyScal keeps the connection time at a high level, from 1.1 ms to 1.25 ms, and finally reaches around 1.25 ms at peak since the number of requests increases during the observed period. KuScal can achieve a better performance than HyScal with about 1.1 ms. The reason lies in that KuScal can scale resources more sufficiently than HyScal that pre-configures some limitations on resources allocated to microservices. The connection time achieved by DoScal is 1.1 ms. CoScal can achieve the best performance compared with the baselines, and it can reach around 0.8 ms within 500 minutes. Although the results of the CoScal vary slightly, the connection time is still 19.3% to 29.3% lower than the average value of the other baselines.\n\nResponse Time: Fig. 6 Transaction Per Second (TPS): it is an important metric to that shows the number of requests per second a system can handle. The higher TPS value represents the better performance of the algorithm. As DoScal has achieved better performance than KuScal and HyScal, we compare DoScal and CoScal in Fig. 7. We can observe that the TPS value of CoScal is greater than DoScal. CoScal can handle requests stably under heavy loads, and its TPS value fluctuates slightly.\n\nAs experimental results of connection time and response time during first 200 minutes are quite close, to better demonstrate the differences between the values of the four scaling approaches, we use the Cumulative Distribution Function   (CDF) as the metric which is the integral of the probability density function representing the sum of the probability of occurrence of all values less than or equal to x. The formula of the CDF function is F X (x ) = P(X \u2264 x ). Fig. 8 Table VII, which are collected by the log files that record the adopted operations. The first row represents the original workload level and the first column represents the transferred workload level. The contents in other cells represent the techniques that have been applied, where V stands for vertical scaling, B stands for brownout, H stands for horizontal scaling, and NA means no action is conducted. For example, if the workload level needs to be transferred from level 1 to 4, vertical scaling, horizontal scaling, and brownout should be applied together, since the workloads change significantly. While if the workloads are increased from level 2 to 3, only vertical scaling is required. When the workload level is 4, the response time is high thus the reward function in Equation (4) will not guide horizontal scaling to reduce available resources. It indicates that our RL agent in CoScal effectively learns the environment complexity and workload characteristic and accordingly takes the suitable combinations of scaling decisions to increase application QoS and optimize resource usage of physical infrastructure.\n\nRL Convergence Analysis: to observe the convergence behavior of our RL-based approach, we train the RL model following the same steps of resource scaling according to Section IV-C. Fig. 9 shows that the approach updates its scaling policy as the training progresses and improves the total reward value. As the RL agent spends initially in the exploration phase, it exponentially increases the reward. After the 16522 epochs, it converges to a stable reward.\n\n\nD. Experimental Analysis With Stan's Robot Shop\n\nTo further evaluate the effectiveness of CoScal, we also conduct experiments with another Docker-based microservice   4 which is an online system composed of 12 microservices for robots sale. We use Locust, 5 a Python-based testing tool similar to JMeter, to generate requests based on Alibaba's trace [14], and send the requests to the homepage of Stan's Robot Shop. The experiments are conducted in a Docker Swarm cluster with 5 homogeneous machines equipped with Intel Xeon CPU E5-2630 v3 and 64GB RAM.\n\nWe compare several metrics in Table VIII collected from Locust, including TPS, successful TPS, response time and the number of failed requests, which have also been evaluated in our previous experiments with Sock Shop application. The final results of each approach are the average value of 300 minutes, and the experiments have been repeated 3 times to avoid randomness. The results show that CoScal can improve 7%-11% TPS compared with the other three baselines, and CoScal can achieve the highest successful TPS. In terms of the number of failures, CoScal is very close to the best result of HyScal. Moreover, CoScal can obtain the lower response time than HyScal, and achieve close results with KuScal. In conclusion, the experiments with Stan's Robot Shop application also validate the effectiveness of our proposed approach in improving system performance. CoScal can achieve good performance as it is adaptive to the change of environments and it searches large solution space.\n\n\nVII. CONCLUSION AND FUTURE WORK\n\nThe microservice-based applications have been widely adopted in cloud computing environments, converting monolithic applications into lightweight, flexible, and looselycoupled application components. Such a modular design of the application helps achieve CI/CD (continuous integration, continuous delivery, and continuous deployment) of the application life cycle in cloud environments. However, efficient algorithms for resource scaling in cloud infrastructure are required to ensure the sustainable development of microservice technology. In this paper, we proposed a multi-faceted scaling approach, named CoScal, for scaling resources for microservices that combine the three techniques, including horizontal scaling, vertical scaling, and brownout. It ensures cloud service providers optimize their resource usage while guaranteeing QoS. CoScal utilizes deep learning approaches for workload prediction to predict load more accurately compared to traditional regression-based prediction approaches. Furthermore, leveraging the RL framework takes decisions on scaling strategies, adapting to the load changes, and allocating the appropriate resources. To evaluate the performance of CoScal, we deployed our algorithms on a prototype system with representative microservices application. The results are compared with popular state-of-the-art algorithms from research and industry domains with two microservice applications. The experimental results have demonstrated that CoScal can outperform the baseline algorithms in connection time, response time and number of failed requests for Sock Shop application, and improve the number of successful transactions with 6%-10% for Stan's Robot Shop application with slight degradation in response time.\n\nAs for future work, we would like to investigate network resource scaling to extend to the suitable scenarios of our scaling approach. As the size of Q-table can increase significantly when the state-action space increases largely, we would also like to investigate Deep Q-Learning with workloads prediction to improve the speed and accuracy of inference, which can also address the limitation in Q-Learning based approach that heavily relies on Q-table. Moreover, we plan to migrate our system to a Kubernetes-based platform to evaluate mediumscale microservice applications, such as TrainTicket and Hotel Reservation applications.\n\n\nSOFTWARE AVAILABILITY\n\nThe source codes of CoScal project can be found at the GitHub repository: https://github.com/Kminassch/CoScal \n\nFig. 1 .\n1Performance comparison of different scaling techniques when applied separately.\n\nFig. 2 .\n2The CoScal system model.\n\nFig. 3 .\n3Schematic diagram of requests generation by JMeter.\n\nFig. 4 .\n4The number of requests and the corresponding CPU utilization. V. CoScal: A MULTI-FACETED AUTO-SCALING IN MICROSERVICE ENVIRONMENTS This section introduces the key elements to realize our system model, including the performance profiling, neural network-based workload prediction and RL-based policy for scaling microservices.\n\n\nin last time interval:L(t \u2212 1) \u2190 E (t \u2212 1) L(t \u2212 1), C(t) and F(t + 1) together into S(t): 15 S (t) \u2190 {L(t \u2212 1), C (t), F (t + 1)} 16 L(t \u2212 1), C(t)are marked as samples in supervised learning 17 F(t + 1) are marked as labels in supervised learning 18 if S(t) contains NONE then\n\n:\ntransition (s t , a t r t , s t+1 )in experience pool; CoScal: Action ExecutionInput : Time interval t, action sets a t = {a i k (t)|i \u2208 {0, 1, . . . , I }, k \u2208 {1, 2, . . . , K }}, a i k (t) = {h k (t), v i k (t), b k (t)},selected action at time t with horizontal scaling operation h k (t) = n, vertical scaling operation v i k (t) = m, and brownout operation b k (t) \u2208 {0, 1} 1 for k from 1 to K Add m resources of type i on local machine; Remove m resources of type i on local machine;\n\nFig. 5 .\n5Auto-adapter resource consumption to handle workloads fluctuations.\n\nFig. 6 .\n6Performance Comparison of DoScal, KuScal, HyScal, and CoScal: (a) Average Connection Time. (b) Average Response Time. (c) Number of Failed Requests.\n\nFig. 7 .\n7Comparison of TPS for DoScal and CoScal.\n\nFig. 8 .\n8CDF of connection time and response time curves.\n\nFig. 9 .\n9Reward value during learning process.\n\nMinxian\nXu (Member, IEEE) received the B.Sc. and M.Sc. degrees in software engineering from the University of Electronic Science and Technology of China in 2012 and 2015, respectively, and the Ph.D. degree from the University of Melbourne in 2019. He is currently an Associate Professor with the Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences. He has coauthored 40+ peer-reviewed papers published in prominent international journals and conferences, such as ACM Computing Surveys, ACM Transactions on Internet Technology, IEEE TRANSACTIONS ON SUSTAINABLE COMPUTING, IEEE TRANSACTIONS ON CLOUD COMPUTING, IEEE TRANSACTIONS ON AUTOMATION SCIENCE AND ENGINEERING, IEEE TRANSACTIONS ON GREEN COMMUNICATIONS AND NETWORKING, Journal of Parallel and Distributed Computing, Journal of Systems and Software, and ICSOC. His research interests include resource scheduling and optimization in cloud computing. His Ph.D. Thesis was awarded the 2019 IEEE TCSC Outstanding Ph.D. Dissertation Award. More information can be found at: minxianxu.info.\n\nTABLE I COMPARISON\nIOF RELATED WORK scaling techniques in these systems are primarily thresholdbased or static policies. Kiss et al.\n\nTABLE II REQUEST\nIIDISTRIBUTION FOR SOCK SHOP APPLICATION in microservice-based computing environments. The results illustrate that applying individual scaling techniques such as horizontal scaling or vertical scaling does not yield desired results due to trade-offs.\n\nTABLE III DEFINED\nIIIWORKLOAD LEVELS AND CORRESPONDING MEANINGS deployed applications. The CPU utilization increases gradually with the increased number of requests. In this experiment, a host can accept at most 9000 requests. We can also obtain the relationship between other resource types and the number of requests. Based on our experiments for the Sock Shop application, the CPU is a dominant resource type indicating that the deployed application is mainly compute-intensive.\n\nTABLE IV MSE\nIVOF OUR WORKLOAD PREDICTION ALGORITHM\n\nAlgorithm 2 :\n2CoScal: General Procedure Input : Table Q(s, a) contains all state/action pairs from experience pool by offline training, time intervals T, probability of random action , learning rate \u03b1, discount factor \u03b3 1 Initialize system status, monitoring model; 2 for t from 1 to T do W t\u22121 \u2190 Workloads level at time interval t \u2212 1;3 \n\n4 \n\nW \n\np \n\nt \u2190 Predicted workload level according to Algorithm 1; \n\n\n\n\ntable. At each iteration to access and modify the lookup table, CoScal applies the open addressing technique to resolve hash collisions to ensure the access and modification operations to take negligible time. The open addressing technique has the computational complexity as O(1).\n\n\n(b) illustrates the comparison of the average value of response time in the first 500 minutes. The average response time values for all four scaling approaches show a decreasing trend. DoScal remains at the highest value but shows an overall decreasing trend from 419.9 ms to 242.1 ms. KuScal and HyScal show irregular fluctuations, which are caused by the different number of requests during different time periods, and their values worse than CoScal. For CoScal, it maintains the lowest response time throughout the observed time and finally drops to 198.4 ms. CoScal can reduce 16.1% response time compared with the baselines, which shows that combining different techniques can achieve better performance. Number of Failed Requests: Fig. 6(c) demonstrates the comparison of failed requests during the experiments. The results show that CoScal can significantly decrease the failed requests. For instance, after 100 minutes experiments, DoScal has 31574 failed requests, HyScal brings 28666 failed requests, KuScal reduces the number to 22756, and our CoScal only has 2345 failed requests. For other observations with longer time, CoScal always has the lowest number of failed requests with 92% to 96% reduction compared with other baselines. In conclusion, CoScal can ensure more requests are processed successfully.\n\nTABLE V CONNECTION\nVTIME (MS) COMPARISON BASED ON CDF PERCENTILE\n\n\nshows the CDF curves of connection time and response time. There is a clear gap between the CoScal and the baselines. Almost all requests can establish the connection within 2 ms by CoScal, while other baselines can only have about 90% requests in this range. CoScal can respond within 400 ms for most of the requests, while other baselines need more than 500 ms. The increase in response time around 0.43 results from the change of workload level. Therefore, we can conclude that CoScal achieves better performance overall compared to the other baselines. Tables V and VI also demonstrate the connection time and response time at different percentiles. From Table V, we can notice that CoScal outperforms other baselines during the observed period, for instance, the connection time of 95% requests falls into 1.407 ms in CoScal, while other baselines require 2.129 to 2.651 ms. When comparing the response time inTable VI, we can observe that although CoScal is not the best one before 60th percentile, it performs the best at 80th, 90th and 99th percentile. For example, the response time of\n\nTABLE VI RESPONSE\nVITIME (MS) COMPARISON BASED ON CDF PERCENTILETABLE VII MULTI-FACETED SCALING TECHNIQUES ADOPTED UNDER DIFFERENT LOADS BY CoScal (THE FIRST ROW REPRESENTS THE CURRENT WORKLOAD LEVEL, AND THE FIRST COLUMN REPRESENTS THE NEXT WORKLOAD LEVEL) 80% requests are 370.56 ms in CoScal, while other baselines need 464.53 to 476.36 ms. RL Decisions Analysis: to investigate the essential reasons for resource optimization by applying different techniques in CoScal, we summarize the adopted techniques by our RL agent under different workload levels as shown in\n\nTABLE VIII EXPERIMENTAL\nVIIIRESULTS WITH STAN'S ROBOT SHOP application, named Stan's Robot Shop,\nmicroservices-demo.github.io 2 https://jmeter.apache.org/\nhttps://github.com/alibaba/clusterdata\nhttps://www.instana.com/blog/stans-robot-shop-sample-microserviceapplication/ 5 https://locust.io/\nACKNOWLEDGMENTThe authors would like to thank Dr. Zhicheng Cai for his suggestions to improve this work.\nA view of cloud computing. M Armbrust, Commun. ACM. 534M. Armbrust et al., \"A view of cloud computing,\" Commun. ACM, vol. 53, no. 4, pp. 50-58, 2010.\n\nA comprehensive survey for scheduling techniques in cloud computing. M Kumar, S Sharma, A Goel, S Singh, J. Netw. Comput. Appl. 143M. Kumar, S. Sharma, A. Goel, and S. Singh, \"A comprehensive sur- vey for scheduling techniques in cloud computing,\" J. Netw. Comput. Appl., vol. 143, pp. 1-33, Oct. 2019. [Online]. Available: https://www. sciencedirect.com/science/article/pii/S1084804519302036\n\nBuilding Microservices. S Newman, Sebastopol, CA, USA: O'Reilly MediaS. Newman, Building Microservices. Sebastopol, CA, USA: O'Reilly Media, 2015.\n\nAIenabled secure microservices in edge computing: Opportunities and challenges. F Al-Doghman, N Moustafa, I Khalil, Z Tari, A Zomaya, 10.1109/TSC.2022.3155447IEEE Trans. Services Comput. early accessF. Al-Doghman, N. Moustafa, I. Khalil, Z. Tari, and A. Zomaya, \"AI- enabled secure microservices in edge computing: Opportunities and challenges,\" IEEE Trans. Services Comput., early access, Mar. 1, 2022, doi: 10.1109/TSC.2022.3155447.\n\nARPS: An autonomic resource provisioning and scheduling framework for cloud platforms. M Kumar, A Kishor, J Abawajy, P Agarwal, A Singh, A Zomaya, IEEE Trans. Sustain. Comput. 72M. Kumar, A. Kishor, J. Abawajy, P. Agarwal, A. Singh, and A. Zomaya, \"ARPS: An autonomic resource provisioning and scheduling frame- work for cloud platforms,\" IEEE Trans. Sustain. Comput., vol. 7, no. 2, pp. 386-399, Apr.-Jun. 2022.\n\nSDCon: Integrated control platform for softwaredefined clouds. J Son, R Buyya, IEEE Trans. Parallel Distrib. Syst. 301J. Son and R. Buyya, \"SDCon: Integrated control platform for software- defined clouds,\" IEEE Trans. Parallel Distrib. Syst., vol. 30, no. 1, pp. 230-244, Jan. 2019.\n\nInvestigating energy consumption and performance trade-off for interactive cloud application. M S Hasan, F Alvares, T Ledoux, J.-L Pazat, IEEE Trans. Sustain. Comput. 22M. S. Hasan, F. Alvares, T. Ledoux, and J.-L. Pazat, \"Investigating energy consumption and performance trade-off for interactive cloud application,\" IEEE Trans. Sustain. Comput., vol. 2, no. 2, pp. 113-126, Apr.-Jun. 2017.\n\nHyScale: Hybrid and network scaling of dockerized microservices in cloud data centres. A Kwan, J Wong, H.-A Jacobsen, V Muthusamy, Proc. IEEE 39th Int. Conf. Distrib. Comput. Syst. (ICDCS). IEEE 39th Int. Conf. Distrib. Comput. Syst. (ICDCS)A. Kwan, J. Wong, H.-A. Jacobsen, and V. Muthusamy, \"HyScale: Hybrid and network scaling of dockerized microservices in cloud data centres,\" in Proc. IEEE 39th Int. Conf. Distrib. Comput. Syst. (ICDCS), 2019, pp. 80-90.\n\nMachine learning-based scaling management for kubernetes edge clusters. L Toka, G Dobreff, B Fodor, B Sonkoly, IEEE Trans. Netw. Service Manag. 181L. Toka, G. Dobreff, B. Fodor, and B. Sonkoly, \"Machine learning-based scaling management for kubernetes edge clusters,\" IEEE Trans. Netw. Service Manag., vol. 18, no. 1, pp. 958-972, Mar. 2021.\n\nFIRM: An intelligent fine-grained resource management framework for SLOoriented microservices. H Qiu, S S Banerjee, S Jha, Z T Kalbarczyk, R K Iyer, Proc. 14th USENIX Symp. Oper. Syst. Design Implement. (OSDI). 14th USENIX Symp. Oper. Syst. Design Implement. (OSDI)H. Qiu, S. S. Banerjee, S. Jha, Z. T. Kalbarczyk, and R. K. Iyer, \"FIRM: An intelligent fine-grained resource management framework for SLO- oriented microservices,\" in Proc. 14th USENIX Symp. Oper. Syst. Design Implement. (OSDI), Nov. 2020, pp. 805-825. [Online]. Available: https: //www.usenix.org/conference/osdi20/presentation/qiu\n\nThe straw that broke the camel's back: Safe cloud overbooking with application brownout. L Tom\u00e1s, C Klein, J Tordsson, F Hern\u00e1ndez-Rodr\u00edguez, Proc. IEEE Int. Conf. Cloud Auton. IEEE Int. Conf. Cloud AutonL. Tom\u00e1s, C. Klein, J. Tordsson, and F. Hern\u00e1ndez-Rodr\u00edguez, \"The straw that broke the camel's back: Safe cloud overbooking with appli- cation brownout,\" in Proc. IEEE Int. Conf. Cloud Auton. Comput., 2014, pp. 151-160.\n\nA self-adaptive approach for managing applications and harnessing renewable energy for sustainable cloud computing. M Xu, A N Toosi, R Buyya, IEEE Trans. Sustain. Comput. 64M. Xu, A. N. Toosi, and R. Buyya, \"A self-adaptive approach for manag- ing applications and harnessing renewable energy for sustainable cloud computing,\" IEEE Trans. Sustain. Comput., vol. 6, no. 4, pp. 544-558, Oct.-Dec. 2021.\n\nMachine learning-based orchestration of containers: A taxonomy and future directions. Z Zhong, M Xu, M A Rodriguez, C Xu, R Buyya, 10.1145/3510415ACM Comput. Surveys. 5410S217Z. Zhong, M. Xu, M. A. Rodriguez, C. Xu, and R. Buyya, \"Machine learning-based orchestration of containers: A taxonomy and future direc- tions,\" ACM Comput. Surveys, vol. 54, no. 10S, p. 217, Jan. 2022. [Online]. Available: https://doi.org/10.1145/3510415\n\nHow does the workload look like in production cloud? Analysis and clustering of workloads on Alibaba cluster trace. W Chen, K Ye, Y Wang, G Xu, C Xu, Proc. IEEE 24th Int. Conf. Parallel Distrib. Syst. (ICPADS). IEEE 24th Int. Conf. Parallel Distrib. Syst. (ICPADS)W. Chen, K. Ye, Y. Wang, G. Xu, and C. Xu, \"How does the workload look like in production cloud? Analysis and clustering of workloads on Alibaba cluster trace,\" in Proc. IEEE 24th Int. Conf. Parallel Distrib. Syst. (ICPADS), 2018, pp. 102-109.\n\nHipster: Hybrid task manager for latency-critical cloud workloads. R Nishtala, P Carpenter, V Petrucci, X Martorell, Proc. IEEE Int. Symp. High Perform. Comput. Architecture (HPCA. IEEE Int. Symp. High Perform. Comput. Architecture (HPCAR. Nishtala, P. Carpenter, V. Petrucci, and X. Martorell, \"Hipster: Hybrid task manager for latency-critical cloud workloads,\" in Proc. IEEE Int. Symp. High Perform. Comput. Architecture (HPCA), 2017, pp. 409-420.\n\nTwig: Multiagent task management for colocated latency-critical cloud services. R Nishtala, V Petrucci, P Carpenter, M Sjalander, Proc. IEEE Int. Symp. High Perform. Comput. Archit. (HPCA). IEEE Int. Symp. High Perform. Comput. Archit. (HPCA)R. Nishtala, V. Petrucci, P. Carpenter, and M. Sjalander, \"Twig: Multi- agent task management for colocated latency-critical cloud services,\" in Proc. IEEE Int. Symp. High Perform. Comput. Archit. (HPCA), 2020, pp. 167-179.\n\nQoS-aware machine learning-based multiple resources scheduling for Microservices in cloud environment. L Liu, arXiv:1911.13208L. Liu, \"QoS-aware machine learning-based multiple resources schedul- ing for Microservices in cloud environment,\" 2019, arXiv:1911.13208.\n\nThrottleBot-performance without insight. M A Chang, A Panda, Y.-C Tsai, H Wang, S Shenker, arXiv:1711.00618M. A. Chang, A. Panda, Y.-C. Tsai, H. Wang, and S. Shenker, \"ThrottleBot-performance without insight,\" 2017, arXiv:1711.00618.\n\nPerformance modeling of microservice platforms considering the dynamics of the underlying cloud infrastructure. H Khazaei, C Barna, M Litoiu, arXiv:1902.03387H. Khazaei, C. Barna, and M. Litoiu, \"Performance modeling of microservice platforms considering the dynamics of the underlying cloud infrastructure,\" 2019, arXiv:1902.03387.\n\nSeer: Leveraging big data to navigate the complexity of performance debugging in cloud microservices. Y Gan, Proc. 24th Int. Conf. Archit. Support Program. 24th Int. Conf. Archit. Support ProgramY. Gan et al., \"Seer: Leveraging big data to navigate the complexity of performance debugging in cloud microservices,\" in Proc. 24th Int. Conf. Archit. Support Program. Lang. Oper. Syst., 2019, pp. 19-33. [Online].\n\n. 10.1145/3297858.3304004Available: https://doi.org/10.1145/3297858.3304004\n\nGrandSLAm: Guaranteeing SLAs for jobs in Microservices execution frameworks. R S Kannan, L Subramanian, A Raju, J Ahn, J Mars, L Tang, Proc. 14th EuroSys Conf. 14th EuroSys Conf34R. S. Kannan, L. Subramanian, A. Raju, J. Ahn, J. Mars, and L. Tang, \"GrandSLAm: Guaranteeing SLAs for jobs in Microservices execu- tion frameworks,\" in Proc. 14th EuroSys Conf., 2019, p. 34. [Online].\n\n. 10.1145/3302424.3303958Available: https://doi.org/10.1145/3302424.3303958\n\nMicroscaler: Automatic scaling for Microservices with an online learning approach. G Yu, P Chen, Z Zheng, Proc. IEEE Int. Conf. Web Services (ICWS). IEEE Int. Conf. Web Services (ICWS)G. Yu, P. Chen, and Z. Zheng, \"Microscaler: Automatic scaling for Microservices with an online learning approach,\" in Proc. IEEE Int. Conf. Web Services (ICWS), Jul. 2019, pp. 68-75.\n\nDistributed resource management across process boundaries. L Suresh, P Bodik, I Menache, M Canini, F Ciucu, 10.1145/3127479.3132020Proc. Symp. Cloud Comput. Symp. Cloud ComputL. Suresh, P. Bodik, I. Menache, M. Canini, and F. Ciucu, \"Distributed resource management across process boundaries,\" in Proc. Symp. Cloud Comput., 2017, pp. 611-623. [Online]. Available: https://doi.org/10. 1145/3127479.3132020\n\nOverload control for scaling WeChat Microservices. H Zhou, Proc. ACM Symp. ACM SympH. Zhou et al., \"Overload control for scaling WeChat Microservices,\" in Proc. ACM Symp. Cloud Comput., 2018, pp. 149-161. [Online].\n\n. 10.1145/3267809.3267823Available: https://doi.org/10.1145/3267809.3267823\n\nAutopilot: Workload autoscaling at Google. K Rzadca, 10.1145/3342195.3387524Proc. 15th Eur. Conf. Comput. Syst., 2020. 15th Eur. Conf. Comput. Syst., 2020K. Rzadca et al., \"Autopilot: Workload autoscaling at Google,\" in Proc. 15th Eur. Conf. Comput. Syst., 2020, pp. 1-16. [Online]. Available: https://doi.org/10.1145/3342195.3387524\n\nANT-Man: Towards agile power management in the microservice era. X Hou, C Li, J Liu, L Zhang, Y Hu, M Guo, Proc. Int. Conf. High Perform. Comput., Netw., Storage Anal. (SC). Int. Conf. High Perform. Comput., Netw., Storage Anal. (SC)X. Hou, C. Li, J. Liu, L. Zhang, Y. Hu, and M. Guo, \"ANT-Man: Towards agile power management in the microservice era,\" in Proc. Int. Conf. High Perform. Comput., Netw., Storage Anal. (SC), 2020, pp. 1098-1111.\n\nRe-deploying microservices in edge and cloud environment for the optimization of user-perceived service quality. X He, Z Tu, X Xu, Z Wang, Proc. Int. Conf. Service-Oriented Comput. Int. Conf. Service-Oriented ComputX. He, Z. Tu, X. Xu, and Z. Wang, \"Re-deploying microservices in edge and cloud environment for the optimization of user-perceived service quality,\" in Proc. Int. Conf. Service-Oriented Comput., 2019, pp. 555-560.\n\nA-SARSA: A predictive container auto-scaling algorithm based on reinforcement learning. S Zhang, T Wu, M Pan, C Zhang, Y Yu, Proc. IEEE Int. Conf. Web Services (ICWS). IEEE Int. Conf. Web Services (ICWS)S. Zhang, T. Wu, M. Pan, C. Zhang, and Y. Yu, \"A-SARSA: A predictive container auto-scaling algorithm based on reinforcement learning,\" in Proc. IEEE Int. Conf. Web Services (ICWS), 2020, pp. 489-497.\n\nHorizontal and vertical scaling of container-based applications using reinforcement learning. F Rossi, M Nardelli, V Cardellini, Proc. IEEE 12th Int. Conf. Cloud Comput. IEEE 12th Int. Conf. Cloud ComputF. Rossi, M. Nardelli, and V. Cardellini, \"Horizontal and vertical scaling of container-based applications using reinforcement learning,\" in Proc. IEEE 12th Int. Conf. Cloud Comput. (CLOUD), 2019, pp. 329-338.\n\nATOM: Model-driven autoscaling for microservices. A U Gias, G Casale, M Woodside, Proc. IEEE 39th Int. Conf. Distrib. Comput. Syst. (ICDCS). IEEE 39th Int. Conf. Distrib. Comput. Syst. (ICDCS)A. U. Gias, G. Casale, and M. Woodside, \"ATOM: Model-driven autoscaling for microservices,\" in Proc. IEEE 39th Int. Conf. Distrib. Comput. Syst. (ICDCS), 2019, pp. 1994-2004.\n\nKubernetes: Up and Running: Dive into the Future of Infrastructure. B Burns, J Beda, K Hightower, Sebastopol, CA, USA: O'Reilly MediaB. Burns, J. Beda, and K. Hightower, Kubernetes: Up and Running: Dive into the Future of Infrastructure. Sebastopol, CA, USA: O'Reilly Media, 2019.\n\nMICADO-Microservice-based cloud applicationlevel dynamic orchestrator. T Kiss, Future Gener. Comput. Syst. 94T. Kiss et al., \"MICADO-Microservice-based cloud application- level dynamic orchestrator,\" Future Gener. Comput. Syst., vol. 94, pp. 937-946, May 2019.\n\nPoster: Benchmarking microservice systems for software engineering research. X Zhou, Proc. IEEE/ACM 40th Int. Conf. Softw. Eng. Compan. (ICSE-Companion). IEEE/ACM 40th Int. Conf. Softw. Eng. Compan. (ICSE-Companion)X. Zhou et al., \"Poster: Benchmarking microservice systems for software engineering research,\" in Proc. IEEE/ACM 40th Int. Conf. Softw. Eng. Compan. (ICSE-Companion), 2018, pp. 323-324.\n\nTensorFlow: A system for large-scale machine learning. M Abadi, Proc. 12th USENIX Symp. Oper. Syst. Design Implement. (OSDI). 12th USENIX Symp. Oper. Syst. Design Implement. (OSDI)M. Abadi et al., \"TensorFlow: A system for large-scale machine learn- ing,\" in Proc. 12th USENIX Symp. Oper. Syst. Design Implement. (OSDI), Nov. 2016, pp. 265-283.\n\nSock shop: A microservices demo application. \"Sock shop: A microservices demo application.\" 2017. [Online]. Available: https://microservices-demo.github.io/\n\nADRL: A hybrid anomaly-aware deep reinforcement learning-based resource scaling in clouds. S Kardani-Moghaddam, R Buyya, K Ramamohanarao, IEEE Trans. Parallel Distrib. Syst. 323S. Kardani-Moghaddam, R. Buyya, and K. Ramamohanarao, \"ADRL: A hybrid anomaly-aware deep reinforcement learning-based resource scaling in clouds,\" IEEE Trans. Parallel Distrib. Syst., vol. 32, no. 3, pp. 514-526, Mar. 2021.\n\nUsing NMON to monitor SAS applications on AIX servers. A Mendoza, Proc. SAS Global Forum. SAS Global ForumA. Mendoza, \"Using NMON to monitor SAS applications on AIX servers,\" in Proc. SAS Global Forum, 2008, pp. 16-19.\n\nEmpirical evaluation of gated recurrent neural networks on sequence modeling. J Chung, C Gulcehre, K Cho, Y Bengio, Proc. Workshop Deep Learn. Workshop Deep LearnJ. Chung, C. Gulcehre, K. Cho, and Y. Bengio, \"Empirical evaluation of gated recurrent neural networks on sequence modeling,\" in Proc. Workshop Deep Learn., Dec. 2014, pp. 1-9.\n\nExtensions of recurrent neural network language model. T Mikolov, S Kombrink, L Burget, J \u010cernock\u00fd, S Khudanpur, Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP). IEEE Int. Conf. Acoust., Speech Signal ess. (ICASSP)T. Mikolov, S. Kombrink, L. Burget, J.\u010cernock\u00fd, and S. Khudanpur, \"Extensions of recurrent neural network language model,\" in Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP), 2011, pp. 5528-5531.\n\nTowards accurate prediction for high-dimensional and highly-variable cloud workloads with deep learning. Z Chen, J Hu, G Min, A Y Zomaya, T El-Ghazawi, IEEE Trans. Parallel Distrib. Syst. 314Z. Chen, J. Hu, G. Min, A. Y. Zomaya, and T. El-Ghazawi, \"Towards accurate prediction for high-dimensional and highly-variable cloud workloads with deep learning,\" IEEE Trans. Parallel Distrib. Syst., vol. 31, no. 4, pp. 923-934, Apr. 2020.\n\nDelayaware microservice coordination in mobile edge computing: A reinforcement learning approach. S Wang, Y Guo, N Zhang, P Yang, A Zhou, X Shen, IEEE Trans. Mobile Comput. 203S. Wang, Y. Guo, N. Zhang, P. Yang, A. Zhou, and X. Shen, \"Delay- aware microservice coordination in mobile edge computing: A reinforce- ment learning approach,\" IEEE Trans. Mobile Comput., vol. 20, no. 3, pp. 939-951, Mar. 2021.\n\nQ-learning algorithms with random truncation bounds and applications to effective parallel computing. G Yin, C Z Xu, L Y Wang, J. Optim. Theory Appl. 137G. Yin, C. Z. Xu, and L. Y. Wang, \"Q-learning algorithms with random truncation bounds and applications to effective parallel computing,\" J. Optim. Theory Appl., vol. 137, pp. 435-451, May 2008.\n\nOffline reinforcement learning: Tutorial, review, and perspectives on open problems. S Levine, A Kumar, G Tucker, J Fu, arXiv:2005.016432020S. Levine, A. Kumar, G. Tucker, and J. Fu, \"Offline reinforcement learning: Tutorial, review, and perspectives on open problems,\" 2020, arXiv:2005.01643.\n\nChenghao Song received the B.Sc. degree from. Chenghao Song received the B.Sc. degree from\n", "annotations": {"author": "[{\"end\":165,\"start\":147},{\"end\":174,\"start\":166},{\"end\":183,\"start\":175},{\"end\":195,\"start\":184},{\"end\":339,\"start\":196},{\"end\":477,\"start\":340},{\"end\":566,\"start\":478},{\"end\":599,\"start\":567}]", "publisher": null, "author_last_name": "[{\"end\":164,\"start\":152},{\"end\":173,\"start\":166},{\"end\":182,\"start\":175},{\"end\":194,\"start\":184}]", "author_first_name": "[{\"end\":151,\"start\":147}]", "author_affiliation": "[{\"end\":338,\"start\":197},{\"end\":476,\"start\":341},{\"end\":565,\"start\":479},{\"end\":598,\"start\":568}]", "title": "[{\"end\":144,\"start\":1},{\"end\":743,\"start\":600}]", "venue": "[{\"end\":838,\"start\":745}]", "abstract": "[{\"end\":2958,\"start\":1338}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3299,\"start\":3296},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3304,\"start\":3301},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3706,\"start\":3703},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3711,\"start\":3708},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4186,\"start\":4183},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5049,\"start\":5046},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5227,\"start\":5224},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5853,\"start\":5850},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6445,\"start\":6442},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6850,\"start\":6846},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7587,\"start\":7584},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7797,\"start\":7793},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7854,\"start\":7850},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9672,\"start\":9668},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":10595,\"start\":10591},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12352,\"start\":12348},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":12358,\"start\":12354},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12682,\"start\":12678},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":13104,\"start\":13100},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":13238,\"start\":13234},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":13766,\"start\":13762},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":14049,\"start\":14045},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":14272,\"start\":14268},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":14403,\"start\":14399},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":15209,\"start\":15205},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":15539,\"start\":15535},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":15917,\"start\":15913},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":16140,\"start\":16137},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":16339,\"start\":16335},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":16560,\"start\":16556},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":16713,\"start\":16709},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":16966,\"start\":16962},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":17307,\"start\":17303},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":18035,\"start\":18031},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":18132,\"start\":18128},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":18314,\"start\":18310},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":18556,\"start\":18552},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":19024,\"start\":19020},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":21148,\"start\":21144},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":28277,\"start\":28273},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":29162,\"start\":29158},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":29836,\"start\":29832},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":31507,\"start\":31503},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":32133,\"start\":32129},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":33954,\"start\":33950},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":34938,\"start\":34934},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":34987,\"start\":34983},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":35327,\"start\":35326},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":35330,\"start\":35328},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":35345,\"start\":35343},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":35912,\"start\":35908},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":36402,\"start\":36398},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":37195,\"start\":37191},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":40755,\"start\":40751},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":40818,\"start\":40814},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":43531,\"start\":43530},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":43786,\"start\":43784},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":43829,\"start\":43827},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":43970,\"start\":43967},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":43974,\"start\":43970},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":43978,\"start\":43974},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":43982,\"start\":43978},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":43986,\"start\":43982},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":43990,\"start\":43986},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":44648,\"start\":44644},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":44652,\"start\":44648},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":44656,\"start\":44652},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":44660,\"start\":44656},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":44664,\"start\":44660},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":44668,\"start\":44664},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":46670,\"start\":46666},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":49778,\"start\":49774},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":50064,\"start\":50060},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":50387,\"start\":50384},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":55043,\"start\":55042},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":55230,\"start\":55226}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":59061,\"start\":58971},{\"attributes\":{\"id\":\"fig_1\"},\"end\":59097,\"start\":59062},{\"attributes\":{\"id\":\"fig_2\"},\"end\":59160,\"start\":59098},{\"attributes\":{\"id\":\"fig_3\"},\"end\":59497,\"start\":59161},{\"attributes\":{\"id\":\"fig_4\"},\"end\":59778,\"start\":59498},{\"attributes\":{\"id\":\"fig_5\"},\"end\":60271,\"start\":59779},{\"attributes\":{\"id\":\"fig_6\"},\"end\":60350,\"start\":60272},{\"attributes\":{\"id\":\"fig_7\"},\"end\":60510,\"start\":60351},{\"attributes\":{\"id\":\"fig_8\"},\"end\":60562,\"start\":60511},{\"attributes\":{\"id\":\"fig_9\"},\"end\":60622,\"start\":60563},{\"attributes\":{\"id\":\"fig_10\"},\"end\":60671,\"start\":60623},{\"attributes\":{\"id\":\"fig_11\"},\"end\":61731,\"start\":60672},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":61865,\"start\":61732},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":62134,\"start\":61866},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":62617,\"start\":62135},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":62670,\"start\":62618},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":63082,\"start\":62671},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":63366,\"start\":63083},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":64689,\"start\":63367},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":64755,\"start\":64690},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":65852,\"start\":64756},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":66423,\"start\":65853},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":66521,\"start\":66424}]", "paragraph": "[{\"end\":4028,\"start\":2977},{\"end\":4845,\"start\":4030},{\"end\":5710,\"start\":4847},{\"end\":6295,\"start\":5712},{\"end\":7798,\"start\":6297},{\"end\":8780,\"start\":7800},{\"end\":9984,\"start\":8807},{\"end\":10637,\"start\":10009},{\"end\":11807,\"start\":10639},{\"end\":12101,\"start\":11828},{\"end\":14607,\"start\":12126},{\"end\":14884,\"start\":14609},{\"end\":17524,\"start\":14914},{\"end\":17985,\"start\":17526},{\"end\":18748,\"start\":18020},{\"end\":19179,\"start\":18750},{\"end\":20007,\"start\":19204},{\"end\":20254,\"start\":20084},{\"end\":20678,\"start\":20276},{\"end\":22196,\"start\":20680},{\"end\":22716,\"start\":22198},{\"end\":24224,\"start\":22761},{\"end\":25189,\"start\":24226},{\"end\":26380,\"start\":25191},{\"end\":26855,\"start\":26401},{\"end\":27935,\"start\":26880},{\"end\":28987,\"start\":27961},{\"end\":29426,\"start\":28989},{\"end\":30234,\"start\":29458},{\"end\":31066,\"start\":30236},{\"end\":31508,\"start\":31095},{\"end\":32821,\"start\":31510},{\"end\":33179,\"start\":32823},{\"end\":34028,\"start\":33227},{\"end\":34524,\"start\":34030},{\"end\":36195,\"start\":34526},{\"end\":37518,\"start\":36282},{\"end\":38047,\"start\":37609},{\"end\":38508,\"start\":38049},{\"end\":39111,\"start\":38641},{\"end\":40247,\"start\":39137},{\"end\":40950,\"start\":40249},{\"end\":41599,\"start\":40952},{\"end\":42206,\"start\":41666},{\"end\":42499,\"start\":42315},{\"end\":43338,\"start\":42543},{\"end\":43991,\"start\":43428},{\"end\":45314,\"start\":44094},{\"end\":45832,\"start\":45316},{\"end\":46576,\"start\":45884},{\"end\":47373,\"start\":46578},{\"end\":47780,\"start\":47375},{\"end\":48118,\"start\":47811},{\"end\":48921,\"start\":48144},{\"end\":49541,\"start\":48923},{\"end\":50051,\"start\":49558},{\"end\":50375,\"start\":50053},{\"end\":50647,\"start\":50377},{\"end\":51021,\"start\":50674},{\"end\":51402,\"start\":51023},{\"end\":52324,\"start\":51404},{\"end\":52811,\"start\":52326},{\"end\":54413,\"start\":52813},{\"end\":54872,\"start\":54415},{\"end\":55429,\"start\":54924},{\"end\":56415,\"start\":55431},{\"end\":58200,\"start\":56451},{\"end\":58834,\"start\":58202},{\"end\":58970,\"start\":58860}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":37608,\"start\":37519},{\"attributes\":{\"id\":\"formula_1\"},\"end\":38640,\"start\":38509},{\"attributes\":{\"id\":\"formula_2\"},\"end\":39136,\"start\":39112},{\"attributes\":{\"id\":\"formula_3\"},\"end\":41665,\"start\":41600},{\"attributes\":{\"id\":\"formula_4\"},\"end\":42314,\"start\":42207},{\"attributes\":{\"id\":\"formula_5\"},\"end\":42542,\"start\":42500},{\"attributes\":{\"id\":\"formula_6\"},\"end\":43427,\"start\":43339},{\"attributes\":{\"id\":\"formula_7\"},\"end\":44093,\"start\":43992}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":19304,\"start\":19297},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":20969,\"start\":20961},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":33855,\"start\":33846},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":35718,\"start\":35710},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":53295,\"start\":53286},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":55471,\"start\":55461}]", "section_header": "[{\"end\":2975,\"start\":2960},{\"end\":8805,\"start\":8783},{\"end\":10007,\"start\":9987},{\"end\":11826,\"start\":11810},{\"end\":12124,\"start\":12104},{\"end\":14912,\"start\":14887},{\"end\":18018,\"start\":17988},{\"end\":19202,\"start\":19182},{\"end\":20082,\"start\":20010},{\"end\":20274,\"start\":20257},{\"end\":22759,\"start\":22719},{\"end\":26399,\"start\":26383},{\"end\":26878,\"start\":26858},{\"end\":27959,\"start\":27938},{\"end\":29456,\"start\":29429},{\"end\":31093,\"start\":31069},{\"end\":33225,\"start\":33182},{\"end\":36249,\"start\":36198},{\"end\":36280,\"start\":36252},{\"end\":45882,\"start\":45835},{\"end\":47809,\"start\":47783},{\"end\":48142,\"start\":48121},{\"end\":49556,\"start\":49544},{\"end\":50672,\"start\":50650},{\"end\":54922,\"start\":54875},{\"end\":56449,\"start\":56418},{\"end\":58858,\"start\":58837},{\"end\":58980,\"start\":58972},{\"end\":59071,\"start\":59063},{\"end\":59107,\"start\":59099},{\"end\":59170,\"start\":59162},{\"end\":59781,\"start\":59780},{\"end\":60281,\"start\":60273},{\"end\":60360,\"start\":60352},{\"end\":60520,\"start\":60512},{\"end\":60572,\"start\":60564},{\"end\":60632,\"start\":60624},{\"end\":60680,\"start\":60673},{\"end\":61751,\"start\":61733},{\"end\":61883,\"start\":61867},{\"end\":62153,\"start\":62136},{\"end\":62631,\"start\":62619},{\"end\":62685,\"start\":62672},{\"end\":64709,\"start\":64691},{\"end\":65871,\"start\":65854},{\"end\":66448,\"start\":66425}]", "table": "[{\"end\":63082,\"start\":63009}]", "figure_caption": "[{\"end\":59061,\"start\":58982},{\"end\":59097,\"start\":59073},{\"end\":59160,\"start\":59109},{\"end\":59497,\"start\":59172},{\"end\":59778,\"start\":59500},{\"end\":60271,\"start\":59782},{\"end\":60350,\"start\":60283},{\"end\":60510,\"start\":60362},{\"end\":60562,\"start\":60522},{\"end\":60622,\"start\":60574},{\"end\":60671,\"start\":60634},{\"end\":61731,\"start\":60681},{\"end\":61865,\"start\":61753},{\"end\":62134,\"start\":61886},{\"end\":62617,\"start\":62157},{\"end\":62670,\"start\":62634},{\"end\":63009,\"start\":62687},{\"end\":63366,\"start\":63085},{\"end\":64689,\"start\":63369},{\"end\":64755,\"start\":64711},{\"end\":65852,\"start\":64758},{\"end\":66423,\"start\":65874},{\"end\":66521,\"start\":66453}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":26611,\"start\":26605},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":27343,\"start\":27337},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":28000,\"start\":27994},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":28735,\"start\":28729},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":29876,\"start\":29870},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":31230,\"start\":31224},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":32841,\"start\":32835},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":33320,\"start\":33314},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":43332,\"start\":43269},{\"end\":43527,\"start\":43513},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":46100,\"start\":46091},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":46981,\"start\":46975},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":47566,\"start\":47557},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":51430,\"start\":51421},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":52347,\"start\":52341},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":52650,\"start\":52644},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":53285,\"start\":53279},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":54602,\"start\":54596}]", "bib_author_first_name": "[{\"end\":66851,\"start\":66850},{\"end\":67044,\"start\":67043},{\"end\":67053,\"start\":67052},{\"end\":67063,\"start\":67062},{\"end\":67071,\"start\":67070},{\"end\":67393,\"start\":67392},{\"end\":67597,\"start\":67596},{\"end\":67611,\"start\":67610},{\"end\":67623,\"start\":67622},{\"end\":67633,\"start\":67632},{\"end\":67641,\"start\":67640},{\"end\":68040,\"start\":68039},{\"end\":68049,\"start\":68048},{\"end\":68059,\"start\":68058},{\"end\":68070,\"start\":68069},{\"end\":68081,\"start\":68080},{\"end\":68090,\"start\":68089},{\"end\":68430,\"start\":68429},{\"end\":68437,\"start\":68436},{\"end\":68745,\"start\":68744},{\"end\":68747,\"start\":68746},{\"end\":68756,\"start\":68755},{\"end\":68767,\"start\":68766},{\"end\":68780,\"start\":68776},{\"end\":69131,\"start\":69130},{\"end\":69139,\"start\":69138},{\"end\":69150,\"start\":69146},{\"end\":69162,\"start\":69161},{\"end\":69578,\"start\":69577},{\"end\":69586,\"start\":69585},{\"end\":69597,\"start\":69596},{\"end\":69606,\"start\":69605},{\"end\":69944,\"start\":69943},{\"end\":69951,\"start\":69950},{\"end\":69953,\"start\":69952},{\"end\":69965,\"start\":69964},{\"end\":69972,\"start\":69971},{\"end\":69974,\"start\":69973},{\"end\":69988,\"start\":69987},{\"end\":69990,\"start\":69989},{\"end\":70538,\"start\":70537},{\"end\":70547,\"start\":70546},{\"end\":70556,\"start\":70555},{\"end\":70568,\"start\":70567},{\"end\":70990,\"start\":70989},{\"end\":70996,\"start\":70995},{\"end\":70998,\"start\":70997},{\"end\":71007,\"start\":71006},{\"end\":71362,\"start\":71361},{\"end\":71371,\"start\":71370},{\"end\":71377,\"start\":71376},{\"end\":71379,\"start\":71378},{\"end\":71392,\"start\":71391},{\"end\":71398,\"start\":71397},{\"end\":71824,\"start\":71823},{\"end\":71832,\"start\":71831},{\"end\":71838,\"start\":71837},{\"end\":71846,\"start\":71845},{\"end\":71852,\"start\":71851},{\"end\":72284,\"start\":72283},{\"end\":72296,\"start\":72295},{\"end\":72309,\"start\":72308},{\"end\":72321,\"start\":72320},{\"end\":72749,\"start\":72748},{\"end\":72761,\"start\":72760},{\"end\":72773,\"start\":72772},{\"end\":72786,\"start\":72785},{\"end\":73239,\"start\":73238},{\"end\":73443,\"start\":73442},{\"end\":73445,\"start\":73444},{\"end\":73454,\"start\":73453},{\"end\":73466,\"start\":73462},{\"end\":73474,\"start\":73473},{\"end\":73482,\"start\":73481},{\"end\":73749,\"start\":73748},{\"end\":73760,\"start\":73759},{\"end\":73769,\"start\":73768},{\"end\":74073,\"start\":74072},{\"end\":74536,\"start\":74535},{\"end\":74538,\"start\":74537},{\"end\":74548,\"start\":74547},{\"end\":74563,\"start\":74562},{\"end\":74571,\"start\":74570},{\"end\":74578,\"start\":74577},{\"end\":74586,\"start\":74585},{\"end\":75001,\"start\":75000},{\"end\":75007,\"start\":75006},{\"end\":75015,\"start\":75014},{\"end\":75345,\"start\":75344},{\"end\":75355,\"start\":75354},{\"end\":75364,\"start\":75363},{\"end\":75375,\"start\":75374},{\"end\":75385,\"start\":75384},{\"end\":75743,\"start\":75742},{\"end\":76028,\"start\":76027},{\"end\":76385,\"start\":76384},{\"end\":76392,\"start\":76391},{\"end\":76398,\"start\":76397},{\"end\":76405,\"start\":76404},{\"end\":76414,\"start\":76413},{\"end\":76420,\"start\":76419},{\"end\":76877,\"start\":76876},{\"end\":76883,\"start\":76882},{\"end\":76889,\"start\":76888},{\"end\":76895,\"start\":76894},{\"end\":77282,\"start\":77281},{\"end\":77291,\"start\":77290},{\"end\":77297,\"start\":77296},{\"end\":77304,\"start\":77303},{\"end\":77313,\"start\":77312},{\"end\":77693,\"start\":77692},{\"end\":77702,\"start\":77701},{\"end\":77714,\"start\":77713},{\"end\":78063,\"start\":78062},{\"end\":78065,\"start\":78064},{\"end\":78073,\"start\":78072},{\"end\":78083,\"start\":78082},{\"end\":78449,\"start\":78448},{\"end\":78458,\"start\":78457},{\"end\":78466,\"start\":78465},{\"end\":78734,\"start\":78733},{\"end\":79002,\"start\":79001},{\"end\":79382,\"start\":79381},{\"end\":79922,\"start\":79921},{\"end\":79943,\"start\":79942},{\"end\":79952,\"start\":79951},{\"end\":80288,\"start\":80287},{\"end\":80531,\"start\":80530},{\"end\":80540,\"start\":80539},{\"end\":80552,\"start\":80551},{\"end\":80559,\"start\":80558},{\"end\":80848,\"start\":80847},{\"end\":80859,\"start\":80858},{\"end\":80871,\"start\":80870},{\"end\":80881,\"start\":80880},{\"end\":80893,\"start\":80892},{\"end\":81339,\"start\":81338},{\"end\":81347,\"start\":81346},{\"end\":81353,\"start\":81352},{\"end\":81360,\"start\":81359},{\"end\":81362,\"start\":81361},{\"end\":81372,\"start\":81371},{\"end\":81765,\"start\":81764},{\"end\":81773,\"start\":81772},{\"end\":81780,\"start\":81779},{\"end\":81789,\"start\":81788},{\"end\":81797,\"start\":81796},{\"end\":81805,\"start\":81804},{\"end\":82176,\"start\":82175},{\"end\":82183,\"start\":82182},{\"end\":82185,\"start\":82184},{\"end\":82191,\"start\":82190},{\"end\":82193,\"start\":82192},{\"end\":82508,\"start\":82507},{\"end\":82518,\"start\":82517},{\"end\":82527,\"start\":82526},{\"end\":82537,\"start\":82536}]", "bib_author_last_name": "[{\"end\":66860,\"start\":66852},{\"end\":67050,\"start\":67045},{\"end\":67060,\"start\":67054},{\"end\":67068,\"start\":67064},{\"end\":67077,\"start\":67072},{\"end\":67400,\"start\":67394},{\"end\":67608,\"start\":67598},{\"end\":67620,\"start\":67612},{\"end\":67630,\"start\":67624},{\"end\":67638,\"start\":67634},{\"end\":67648,\"start\":67642},{\"end\":68046,\"start\":68041},{\"end\":68056,\"start\":68050},{\"end\":68067,\"start\":68060},{\"end\":68078,\"start\":68071},{\"end\":68087,\"start\":68082},{\"end\":68097,\"start\":68091},{\"end\":68434,\"start\":68431},{\"end\":68443,\"start\":68438},{\"end\":68753,\"start\":68748},{\"end\":68764,\"start\":68757},{\"end\":68774,\"start\":68768},{\"end\":68786,\"start\":68781},{\"end\":69136,\"start\":69132},{\"end\":69144,\"start\":69140},{\"end\":69159,\"start\":69151},{\"end\":69172,\"start\":69163},{\"end\":69583,\"start\":69579},{\"end\":69594,\"start\":69587},{\"end\":69603,\"start\":69598},{\"end\":69614,\"start\":69607},{\"end\":69948,\"start\":69945},{\"end\":69962,\"start\":69954},{\"end\":69969,\"start\":69966},{\"end\":69985,\"start\":69975},{\"end\":69995,\"start\":69991},{\"end\":70544,\"start\":70539},{\"end\":70553,\"start\":70548},{\"end\":70565,\"start\":70557},{\"end\":70588,\"start\":70569},{\"end\":70993,\"start\":70991},{\"end\":71004,\"start\":70999},{\"end\":71013,\"start\":71008},{\"end\":71368,\"start\":71363},{\"end\":71374,\"start\":71372},{\"end\":71389,\"start\":71380},{\"end\":71395,\"start\":71393},{\"end\":71404,\"start\":71399},{\"end\":71829,\"start\":71825},{\"end\":71835,\"start\":71833},{\"end\":71843,\"start\":71839},{\"end\":71849,\"start\":71847},{\"end\":71855,\"start\":71853},{\"end\":72293,\"start\":72285},{\"end\":72306,\"start\":72297},{\"end\":72318,\"start\":72310},{\"end\":72331,\"start\":72322},{\"end\":72758,\"start\":72750},{\"end\":72770,\"start\":72762},{\"end\":72783,\"start\":72774},{\"end\":72796,\"start\":72787},{\"end\":73243,\"start\":73240},{\"end\":73451,\"start\":73446},{\"end\":73460,\"start\":73455},{\"end\":73471,\"start\":73467},{\"end\":73479,\"start\":73475},{\"end\":73490,\"start\":73483},{\"end\":73757,\"start\":73750},{\"end\":73766,\"start\":73761},{\"end\":73776,\"start\":73770},{\"end\":74077,\"start\":74074},{\"end\":74545,\"start\":74539},{\"end\":74560,\"start\":74549},{\"end\":74568,\"start\":74564},{\"end\":74575,\"start\":74572},{\"end\":74583,\"start\":74579},{\"end\":74591,\"start\":74587},{\"end\":75004,\"start\":75002},{\"end\":75012,\"start\":75008},{\"end\":75021,\"start\":75016},{\"end\":75352,\"start\":75346},{\"end\":75361,\"start\":75356},{\"end\":75372,\"start\":75365},{\"end\":75382,\"start\":75376},{\"end\":75391,\"start\":75386},{\"end\":75748,\"start\":75744},{\"end\":76035,\"start\":76029},{\"end\":76389,\"start\":76386},{\"end\":76395,\"start\":76393},{\"end\":76402,\"start\":76399},{\"end\":76411,\"start\":76406},{\"end\":76417,\"start\":76415},{\"end\":76424,\"start\":76421},{\"end\":76880,\"start\":76878},{\"end\":76886,\"start\":76884},{\"end\":76892,\"start\":76890},{\"end\":76900,\"start\":76896},{\"end\":77288,\"start\":77283},{\"end\":77294,\"start\":77292},{\"end\":77301,\"start\":77298},{\"end\":77310,\"start\":77305},{\"end\":77316,\"start\":77314},{\"end\":77699,\"start\":77694},{\"end\":77711,\"start\":77703},{\"end\":77725,\"start\":77715},{\"end\":78070,\"start\":78066},{\"end\":78080,\"start\":78074},{\"end\":78092,\"start\":78084},{\"end\":78455,\"start\":78450},{\"end\":78463,\"start\":78459},{\"end\":78476,\"start\":78467},{\"end\":78739,\"start\":78735},{\"end\":79007,\"start\":79003},{\"end\":79388,\"start\":79383},{\"end\":79940,\"start\":79923},{\"end\":79949,\"start\":79944},{\"end\":79966,\"start\":79953},{\"end\":80296,\"start\":80289},{\"end\":80537,\"start\":80532},{\"end\":80549,\"start\":80541},{\"end\":80556,\"start\":80553},{\"end\":80566,\"start\":80560},{\"end\":80856,\"start\":80849},{\"end\":80868,\"start\":80860},{\"end\":80878,\"start\":80872},{\"end\":80890,\"start\":80882},{\"end\":80903,\"start\":80894},{\"end\":81344,\"start\":81340},{\"end\":81350,\"start\":81348},{\"end\":81357,\"start\":81354},{\"end\":81369,\"start\":81363},{\"end\":81383,\"start\":81373},{\"end\":81770,\"start\":81766},{\"end\":81777,\"start\":81774},{\"end\":81786,\"start\":81781},{\"end\":81794,\"start\":81790},{\"end\":81802,\"start\":81798},{\"end\":81810,\"start\":81806},{\"end\":82180,\"start\":82177},{\"end\":82188,\"start\":82186},{\"end\":82198,\"start\":82194},{\"end\":82515,\"start\":82509},{\"end\":82524,\"start\":82519},{\"end\":82534,\"start\":82528},{\"end\":82540,\"start\":82538}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":1673644},\"end\":66972,\"start\":66823},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":196204037},\"end\":67366,\"start\":66974},{\"attributes\":{\"id\":\"b2\"},\"end\":67514,\"start\":67368},{\"attributes\":{\"doi\":\"10.1109/TSC.2022.3155447\",\"id\":\"b3\"},\"end\":67950,\"start\":67516},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":239761761},\"end\":68364,\"start\":67952},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":51689842},\"end\":68648,\"start\":68366},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":29892305},\"end\":69041,\"start\":68650},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":207756948},\"end\":69503,\"start\":69043},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":232237085},\"end\":69846,\"start\":69505},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":221172892},\"end\":70446,\"start\":69848},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":2295878},\"end\":70871,\"start\":70448},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":108290838},\"end\":71273,\"start\":70873},{\"attributes\":{\"doi\":\"10.1145/3510415\",\"id\":\"b12\",\"matched_paper_id\":235624252},\"end\":71705,\"start\":71275},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":67865107},\"end\":72214,\"start\":71707},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":6563738},\"end\":72666,\"start\":72216},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":215817649},\"end\":73133,\"start\":72668},{\"attributes\":{\"doi\":\"arXiv:1911.13208\",\"id\":\"b16\"},\"end\":73399,\"start\":73135},{\"attributes\":{\"doi\":\"arXiv:1711.00618\",\"id\":\"b17\"},\"end\":73634,\"start\":73401},{\"attributes\":{\"doi\":\"arXiv:1902.03387\",\"id\":\"b18\"},\"end\":73968,\"start\":73636},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":102347800},\"end\":74379,\"start\":73970},{\"attributes\":{\"doi\":\"10.1145/3297858.3304004\",\"id\":\"b20\"},\"end\":74456,\"start\":74381},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":85519074},\"end\":74838,\"start\":74458},{\"attributes\":{\"doi\":\"10.1145/3302424.3303958\",\"id\":\"b22\"},\"end\":74915,\"start\":74840},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":201811382},\"end\":75283,\"start\":74917},{\"attributes\":{\"doi\":\"10.1145/3127479.3132020\",\"id\":\"b24\",\"matched_paper_id\":19712665},\"end\":75689,\"start\":75285},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":47019599},\"end\":75905,\"start\":75691},{\"attributes\":{\"doi\":\"10.1145/3267809.3267823\",\"id\":\"b26\"},\"end\":75982,\"start\":75907},{\"attributes\":{\"doi\":\"10.1145/3342195.3387524\",\"id\":\"b27\",\"matched_paper_id\":218489692},\"end\":76317,\"start\":75984},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":231792881},\"end\":76761,\"start\":76319},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":204945669},\"end\":77191,\"start\":76763},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":229373647},\"end\":77596,\"start\":77193},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":201810325},\"end\":78010,\"start\":77598},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":207758793},\"end\":78378,\"start\":78012},{\"attributes\":{\"id\":\"b33\"},\"end\":78660,\"start\":78380},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":56130811},\"end\":78922,\"start\":78662},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":3591248},\"end\":79324,\"start\":78924},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":6287870},\"end\":79670,\"start\":79326},{\"attributes\":{\"id\":\"b37\"},\"end\":79828,\"start\":79672},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":221093607},\"end\":80230,\"start\":79830},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":61621913},\"end\":80450,\"start\":80232},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":5201925},\"end\":80790,\"start\":80452},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":14850173},\"end\":81231,\"start\":80792},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":209063798},\"end\":81664,\"start\":81233},{\"attributes\":{\"id\":\"b43\"},\"end\":82071,\"start\":81666},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":6213893},\"end\":82420,\"start\":82073},{\"attributes\":{\"doi\":\"arXiv:2005.01643\",\"id\":\"b45\"},\"end\":82715,\"start\":82422},{\"attributes\":{\"id\":\"b46\"},\"end\":82807,\"start\":82717}]", "bib_title": "[{\"end\":66848,\"start\":66823},{\"end\":67041,\"start\":66974},{\"end\":67594,\"start\":67516},{\"end\":68037,\"start\":67952},{\"end\":68427,\"start\":68366},{\"end\":68742,\"start\":68650},{\"end\":69128,\"start\":69043},{\"end\":69575,\"start\":69505},{\"end\":69941,\"start\":69848},{\"end\":70535,\"start\":70448},{\"end\":70987,\"start\":70873},{\"end\":71359,\"start\":71275},{\"end\":71821,\"start\":71707},{\"end\":72281,\"start\":72216},{\"end\":72746,\"start\":72668},{\"end\":74070,\"start\":73970},{\"end\":74533,\"start\":74458},{\"end\":74998,\"start\":74917},{\"end\":75342,\"start\":75285},{\"end\":75740,\"start\":75691},{\"end\":76025,\"start\":75984},{\"end\":76382,\"start\":76319},{\"end\":76874,\"start\":76763},{\"end\":77279,\"start\":77193},{\"end\":77690,\"start\":77598},{\"end\":78060,\"start\":78012},{\"end\":78731,\"start\":78662},{\"end\":78999,\"start\":78924},{\"end\":79379,\"start\":79326},{\"end\":79919,\"start\":79830},{\"end\":80285,\"start\":80232},{\"end\":80528,\"start\":80452},{\"end\":80845,\"start\":80792},{\"end\":81336,\"start\":81233},{\"end\":81762,\"start\":81666},{\"end\":82173,\"start\":82073}]", "bib_author": "[{\"end\":66862,\"start\":66850},{\"end\":67052,\"start\":67043},{\"end\":67062,\"start\":67052},{\"end\":67070,\"start\":67062},{\"end\":67079,\"start\":67070},{\"end\":67402,\"start\":67392},{\"end\":67610,\"start\":67596},{\"end\":67622,\"start\":67610},{\"end\":67632,\"start\":67622},{\"end\":67640,\"start\":67632},{\"end\":67650,\"start\":67640},{\"end\":68048,\"start\":68039},{\"end\":68058,\"start\":68048},{\"end\":68069,\"start\":68058},{\"end\":68080,\"start\":68069},{\"end\":68089,\"start\":68080},{\"end\":68099,\"start\":68089},{\"end\":68436,\"start\":68429},{\"end\":68445,\"start\":68436},{\"end\":68755,\"start\":68744},{\"end\":68766,\"start\":68755},{\"end\":68776,\"start\":68766},{\"end\":68788,\"start\":68776},{\"end\":69138,\"start\":69130},{\"end\":69146,\"start\":69138},{\"end\":69161,\"start\":69146},{\"end\":69174,\"start\":69161},{\"end\":69585,\"start\":69577},{\"end\":69596,\"start\":69585},{\"end\":69605,\"start\":69596},{\"end\":69616,\"start\":69605},{\"end\":69950,\"start\":69943},{\"end\":69964,\"start\":69950},{\"end\":69971,\"start\":69964},{\"end\":69987,\"start\":69971},{\"end\":69997,\"start\":69987},{\"end\":70546,\"start\":70537},{\"end\":70555,\"start\":70546},{\"end\":70567,\"start\":70555},{\"end\":70590,\"start\":70567},{\"end\":70995,\"start\":70989},{\"end\":71006,\"start\":70995},{\"end\":71015,\"start\":71006},{\"end\":71370,\"start\":71361},{\"end\":71376,\"start\":71370},{\"end\":71391,\"start\":71376},{\"end\":71397,\"start\":71391},{\"end\":71406,\"start\":71397},{\"end\":71831,\"start\":71823},{\"end\":71837,\"start\":71831},{\"end\":71845,\"start\":71837},{\"end\":71851,\"start\":71845},{\"end\":71857,\"start\":71851},{\"end\":72295,\"start\":72283},{\"end\":72308,\"start\":72295},{\"end\":72320,\"start\":72308},{\"end\":72333,\"start\":72320},{\"end\":72760,\"start\":72748},{\"end\":72772,\"start\":72760},{\"end\":72785,\"start\":72772},{\"end\":72798,\"start\":72785},{\"end\":73245,\"start\":73238},{\"end\":73453,\"start\":73442},{\"end\":73462,\"start\":73453},{\"end\":73473,\"start\":73462},{\"end\":73481,\"start\":73473},{\"end\":73492,\"start\":73481},{\"end\":73759,\"start\":73748},{\"end\":73768,\"start\":73759},{\"end\":73778,\"start\":73768},{\"end\":74079,\"start\":74072},{\"end\":74547,\"start\":74535},{\"end\":74562,\"start\":74547},{\"end\":74570,\"start\":74562},{\"end\":74577,\"start\":74570},{\"end\":74585,\"start\":74577},{\"end\":74593,\"start\":74585},{\"end\":75006,\"start\":75000},{\"end\":75014,\"start\":75006},{\"end\":75023,\"start\":75014},{\"end\":75354,\"start\":75344},{\"end\":75363,\"start\":75354},{\"end\":75374,\"start\":75363},{\"end\":75384,\"start\":75374},{\"end\":75393,\"start\":75384},{\"end\":75750,\"start\":75742},{\"end\":76037,\"start\":76027},{\"end\":76391,\"start\":76384},{\"end\":76397,\"start\":76391},{\"end\":76404,\"start\":76397},{\"end\":76413,\"start\":76404},{\"end\":76419,\"start\":76413},{\"end\":76426,\"start\":76419},{\"end\":76882,\"start\":76876},{\"end\":76888,\"start\":76882},{\"end\":76894,\"start\":76888},{\"end\":76902,\"start\":76894},{\"end\":77290,\"start\":77281},{\"end\":77296,\"start\":77290},{\"end\":77303,\"start\":77296},{\"end\":77312,\"start\":77303},{\"end\":77318,\"start\":77312},{\"end\":77701,\"start\":77692},{\"end\":77713,\"start\":77701},{\"end\":77727,\"start\":77713},{\"end\":78072,\"start\":78062},{\"end\":78082,\"start\":78072},{\"end\":78094,\"start\":78082},{\"end\":78457,\"start\":78448},{\"end\":78465,\"start\":78457},{\"end\":78478,\"start\":78465},{\"end\":78741,\"start\":78733},{\"end\":79009,\"start\":79001},{\"end\":79390,\"start\":79381},{\"end\":79942,\"start\":79921},{\"end\":79951,\"start\":79942},{\"end\":79968,\"start\":79951},{\"end\":80298,\"start\":80287},{\"end\":80539,\"start\":80530},{\"end\":80551,\"start\":80539},{\"end\":80558,\"start\":80551},{\"end\":80568,\"start\":80558},{\"end\":80858,\"start\":80847},{\"end\":80870,\"start\":80858},{\"end\":80880,\"start\":80870},{\"end\":80892,\"start\":80880},{\"end\":80905,\"start\":80892},{\"end\":81346,\"start\":81338},{\"end\":81352,\"start\":81346},{\"end\":81359,\"start\":81352},{\"end\":81371,\"start\":81359},{\"end\":81385,\"start\":81371},{\"end\":81772,\"start\":81764},{\"end\":81779,\"start\":81772},{\"end\":81788,\"start\":81779},{\"end\":81796,\"start\":81788},{\"end\":81804,\"start\":81796},{\"end\":81812,\"start\":81804},{\"end\":82182,\"start\":82175},{\"end\":82190,\"start\":82182},{\"end\":82200,\"start\":82190},{\"end\":82517,\"start\":82507},{\"end\":82526,\"start\":82517},{\"end\":82536,\"start\":82526},{\"end\":82542,\"start\":82536}]", "bib_venue": "[{\"end\":66873,\"start\":66862},{\"end\":67100,\"start\":67079},{\"end\":67390,\"start\":67368},{\"end\":67701,\"start\":67674},{\"end\":68126,\"start\":68099},{\"end\":68479,\"start\":68445},{\"end\":68815,\"start\":68788},{\"end\":69231,\"start\":69174},{\"end\":69647,\"start\":69616},{\"end\":70057,\"start\":69997},{\"end\":70623,\"start\":70590},{\"end\":71042,\"start\":71015},{\"end\":71440,\"start\":71421},{\"end\":71916,\"start\":71857},{\"end\":72395,\"start\":72333},{\"end\":72856,\"start\":72798},{\"end\":73236,\"start\":73135},{\"end\":73440,\"start\":73401},{\"end\":73746,\"start\":73636},{\"end\":74124,\"start\":74079},{\"end\":74616,\"start\":74593},{\"end\":75064,\"start\":75023},{\"end\":75440,\"start\":75416},{\"end\":75764,\"start\":75750},{\"end\":76101,\"start\":76060},{\"end\":76491,\"start\":76426},{\"end\":76942,\"start\":76902},{\"end\":77359,\"start\":77318},{\"end\":77766,\"start\":77727},{\"end\":78151,\"start\":78094},{\"end\":78446,\"start\":78380},{\"end\":78767,\"start\":78741},{\"end\":79076,\"start\":79009},{\"end\":79450,\"start\":79390},{\"end\":79715,\"start\":79672},{\"end\":80002,\"start\":79968},{\"end\":80320,\"start\":80298},{\"end\":80593,\"start\":80568},{\"end\":80967,\"start\":80905},{\"end\":81419,\"start\":81385},{\"end\":81837,\"start\":81812},{\"end\":82221,\"start\":82200},{\"end\":82505,\"start\":82422},{\"end\":82761,\"start\":82717},{\"end\":69284,\"start\":69233},{\"end\":70113,\"start\":70059},{\"end\":70652,\"start\":70625},{\"end\":71971,\"start\":71918},{\"end\":72453,\"start\":72397},{\"end\":72910,\"start\":72858},{\"end\":74165,\"start\":74126},{\"end\":74635,\"start\":74618},{\"end\":75101,\"start\":75066},{\"end\":75460,\"start\":75442},{\"end\":75774,\"start\":75766},{\"end\":76138,\"start\":76103},{\"end\":76552,\"start\":76493},{\"end\":76978,\"start\":76944},{\"end\":77396,\"start\":77361},{\"end\":77801,\"start\":77768},{\"end\":78204,\"start\":78153},{\"end\":79139,\"start\":79078},{\"end\":79506,\"start\":79452},{\"end\":80338,\"start\":80322},{\"end\":80614,\"start\":80595},{\"end\":81021,\"start\":80969}]"}}}, "year": 2023, "month": 12, "day": 17}