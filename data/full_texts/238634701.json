{"id": 238634701, "updated": "2023-11-11 01:55:56.309", "metadata": {"title": "ABO: Dataset and Benchmarks for Real-World 3D Object Understanding", "authors": "[{\"first\":\"Jasmine\",\"last\":\"Collins\",\"middle\":[]},{\"first\":\"Shubham\",\"last\":\"Goel\",\"middle\":[]},{\"first\":\"Kenan\",\"last\":\"Deng\",\"middle\":[]},{\"first\":\"Achleshwar\",\"last\":\"Luthra\",\"middle\":[]},{\"first\":\"Leon\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Erhan\",\"last\":\"Gundogdu\",\"middle\":[]},{\"first\":\"Xi\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Tomas\",\"last\":\"Vicente\",\"middle\":[\"F.\",\"Yago\"]},{\"first\":\"Thomas\",\"last\":\"Dideriksen\",\"middle\":[]},{\"first\":\"Himanshu\",\"last\":\"Arora\",\"middle\":[]},{\"first\":\"Matthieu\",\"last\":\"Guillaumin\",\"middle\":[]},{\"first\":\"Jitendra\",\"last\":\"Malik\",\"middle\":[]}]", "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2022, "month": 6, "day": 1}, "abstract": "We introduce Amazon Berkeley Objects (ABO), a new large-scale dataset designed to help bridge the gap between real and virtual 3D worlds. ABO contains product catalog images, metadata, and artist-created 3D models with com-plex geometries and physically-based materials that cor-respond to real, household objects. We derive challenging benchmarks that exploit the unique properties of ABO and measure the current limits of the state-of-the-art on three open problems for real-world 3D object understanding: single-view 3D reconstruction, material estimation, and cross-domain multi-view object retrieval.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/Collins0DLXGZVD22", "doi": "10.1109/cvpr52688.2022.02045"}}, "content": {"source": {"pdf_hash": "55dd48f6713ab0b8aad70e576204a278be8aa6d4", "pdf_src": "IEEE", "pdf_uri": "[\"https://arxiv.org/pdf/2110.06199v2.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://arxiv.org/pdf/2110.06199", "status": "GREEN"}}, "grobid": {"id": "131777a2b9cabff9d5716aaec81d515ac8d35a65", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/55dd48f6713ab0b8aad70e576204a278be8aa6d4.txt", "contents": "\nABO: Dataset and Benchmarks for Real-World 3D Object Understanding\n\n\nJasmine Collins \nUC Berkeley\n\n\nShubham Goel \nUC Berkeley\n\n\nKenan Deng \nAmazon\n\n\nAchleshwar Luthra \nBITS Pilani\n\n\nLeon Xu \nUC Berkeley\n\n\nAmazon\n\n\nErhan Gundogdu \nAmazon\n\n\nXi Zhang \nAmazon\n\n\nTomas F Yago Vicente \nAmazon\n\n\nThomas Dideriksen \nAmazon\n\n\nHimanshu Arora \nAmazon\n\n\nMatthieu Guillaumin \nAmazon\n\n\nJitendra Malik \nUC Berkeley\n\n\nABO: Dataset and Benchmarks for Real-World 3D Object Understanding\n10.1109/CVPR52688.2022.02045\ncatalog images high-resolution geometry & texture physically-based renderingsFigure 1. ABO is a dataset of product images and realistic, high-resolution, physically-based 3D models of household objects. We use ABO to benchmark the performance of state-of-the-art methods on a variety of realistic object understanding tasks.AbstractWe introduce Amazon Berkeley Objects (ABO), a new large-scale dataset designed to help bridge the gap between real and virtual 3D worlds. ABO contains product catalog images, metadata, and artist-created 3D models with complex geometries and physically-based materials that correspond to real, household objects. We derive challenging benchmarks that exploit the unique properties of ABO and measure the current limits of the state-of-the-art on three open problems for real-world 3D object understanding: single-view 3D reconstruction, material estimation, and cross-domain multi-view object retrieval.\n\nIntroduction\n\nProgress in 2D image recognition has been driven by large-scale datasets [15,26,37,43,56]. The ease of collecting 2D annotations (such as class labels or segmentation masks) has led to the large scale of these diverse, in-thewild datasets, which in turn has enabled the development of 2D computer vision systems that work in the real world.\n\nTheoretically, progress in 3D computer vision should follow from equally large-scale datasets of 3D objects. However, collecting large amounts of high-quality 3D annotations (such as voxels or meshes) for individual real-world objects poses a challenge. One way around the challenging problem of getting 3D annotations for real images is to focus only on synthetic, computer-aided design (CAD) models [10,35,70]. This has the advantage that the data is large in scale (as there are many 3D CAD models available for download online) but many of the models are low quality or untextured and do not exist in the real world. This has led to a variety of 3D reconstruction methods that work well on clear-background renderings of synthetic objects [13,24,46,65] but do not necessarily generalize to real images, new categories, or more complex object geometries [5,6,58].\n\nTo enable better real-world transfer, another class of 3D datasets aims to link existing 3D models with real-world images [63,64]. These datasets find the closest matching CAD models for the objects in an image and have human annotators align the pose of each model to best match the image. While this has enabled the evaluation of 3D reconstruction methods in-the-wild, the shape (and thus pose) matches are approximate. Further, because this approach relies on matching CAD models to images, it inherits the limitations of the existing CAD model datasets (i.e. poor coverage of real-world objects, basic geometries and textures).\n\nThe IKEA [41] and Pix3D [57] datasets sought to improve upon this by annotating real images with exact, pixelaligned 3D models. The exact nature of such datasets has allowed them to be used as training data for single-view reconstruction [21] and has bridged some of the syntheticto-real domain gap. However, the size of the datasets are relatively small (90 and 395 unique 3D models, respectively), likely due to the difficulty of finding images that exactly match 3D models. Further, the larger of the two datasets [57] only contains 9 categories of objects. The provided 3D models are also untextured, thus the annotations in these datasets are typically used for shape or pose-based tasks, rather than tasks such as material prediction.\n\nRather than trying to match images to synthetic 3D models, another approach to collecting 3D datasets is to start with real images (or video) and reconstruct the scene by classical reconstruction techniques such as structure from motion, multi-view stereo and texture mapping [12,54,55]. The benefit of these methods is that the reconstructed geometry faithfully represents an object of the real world. However, the collection process requires a great deal of manual effort and thus datasets of this nature tend to also be quite small (398, 125, and 1032 unique 3D models, respectively). The objects are also typically imaged in a controlled lab setting and do not have corresponding real images of the object \"in context\". Further, included textured surfaces are assumed to be Lambertian and thus do not display realistic reflectance properties.\n\nMotivated by the lack of large-scale datasets with realistic 3D objects from a diverse set of categories and corresponding real-world multi-view images, we introduce Amazon Berkeley Objects (ABO). This dataset is derived from Amazon.com product listings, and as a result, contains imagery and 3D models that correspond to modern, real-world, household items. Overall, ABO contains 147,702 product listings associated with 398,212 unique catalog images, and up to 18 unique metadata attributes (category, color, material, weight, dimensions, etc.) per product. ABO also includes \"360\u00ba View\" turntable-style images for 8, 222 products and 7,953 products with corresponding artist-designed 3D meshes. In contrast to existing 3D computer vision datasets, the 3D models in ABO have complex geometries and high-resolution, physicallybased materials that allow for photorealistic rendering. A sample of the kinds of real-world images associated with a 3D model from ABO can be found in Figure 1, and sam- ple metadata attributes are shown in Figure 3. The dataset is released under CC BY-NC 4.0 license and can be downloaded at https://amazon-berkeley-objects.s3.\n\namazonaws.com/index.html.\n\nTo facilitate future research, we benchmark the performance of various methods on three computer vision tasks that can benefit from more realistic 3D datasets: (i) singleview shape reconstruction, where we measure the domain gap for networks trained on synthetic objects, (ii) material estimation, where we introduce a baseline for spatiallyvarying BRDF from single-and multi-view images of complex real world objects, and (iii) image-based multi-view object retrieval, where we leverage the 3D nature of ABO to evaluate the robustness of deep metric learning algorithms to object viewpoint and scenes.\n\n\nRelated Work\n\n3D Object Datasets ShapeNet [10] is a large-scale database of synthetic 3D CAD models commonly used for training single-and multi-view reconstruction models. IKEA Objects [42] and Pix3D [57] are image collections with 2D-3D alignment between CAD models and real images, however these images are limited to objects for which there is an exact CAD model match. Similarly, Pascal3D+ [64] and ObjectNet3D [63] provide 2D-3D alignment for images and provide more instances and categories, however  Table 2. Common image retrieval benchmarks for deep metric learning and their statistics. Our proposed multi-view retrieval (MVR) benchmark based on ABO is significantly larger, more diverse and challenging than existing benchmarks, and exploits 3D models.   the 3D annotations are only approximate matches. The Object Scans dataset [12] and Objectron [3] are both video datasets that have the camera operator walk around various objects, but are limited in the number of categories represented. CO3D [53] also offers videos of common objects from 50 different categories, however they do not provide full 3D mesh reconstructions. Existing 3D datasets typically assume very simplistic texture models that are not physically realistic. To improve on this, PhotoShapes [51] augmented ShapeNet CAD models by automatically mapping spatially varying (SV-) bidirectional reflectance distribution functions (BRDFs) to meshes, yet the dataset consists only of chairs. The works in [17,20] provide high-quality SV-BRDF maps, but only for planar surfaces. The dataset used in [32] contains only homogenous BRDFs for various objects. [40] and [7] introduce datasets containing full SV-BRDFs, however their models are procedurally generated shapes that do not correspond to real objects. In contrast, ABO provides shapes and SV-BRDFs created by professional artists for real-life objects that can be directly used for photorealistic rendering. Table 1 compares the 3D subset of ABO with other commonly used 3D datasets in terms of size (number of objects and classes) and properties such as the presence of real images, full 3D meshes and physically-based rendering (PBR) materials. ABO is the only dataset that contains all of these properties and is much more diverse in number of categories than existing 3D datasets.\n\n\n3D Shape Reconstruction\n\nRecent methods for single-view 3D reconstruction differ mainly in the type of supervision and 3D representation used, whether it be voxels, point clouds, meshes, or implicit functions. Methods that require full shape supervision in the single-view [18,22,46,57,69] and multi-view [13,31,65] case are often trained using ShapeNet. There are other approaches that use more natural forms of multi-view supervision such as images, depth maps, and silhouettes [31,59,62,66], with known cameras. Of course, multi-view 3D reconstruction has long been studied with classical computer vision techniques [27] like multi-view stereo and visual hull reconstruction. Learningbased methods are typically trained in a category-specific way and evaluated on new instances from the same category. Out of the works mentioned, only [69] claims to be category-agnostic. In this work we are interested in how well these ShapeNet-trained networks [13,22,46,69] generalize to more realistic objects.\n\nMaterial Estimation Several works have focused on modeling object appearance from a single image, however realistic datasets available for this task are relatively scarce and small in size. [38] use two networks to estimate a homogeneous BRDF and an SV-BRDF of a flat surface from a single image, using a self-augmentation scheme to alleviate the need for a large training set. However, their work is limited to a specific family of materials, and each separate material requires another trained network. [67] extend the idea of self-augmentation to train with unlabeled data, but their work is limited by the same constraints. [16] use a modified U-Net and rendering loss to predict the SV-BRDFs of flash-lit photographs consisting of only a flat sur-Input Image\n\n\nR2N2\n\nOccupancy Networks GenRe Mesh RCNN GT Figure 5. Qualitative 3D reconstruction results for R2N2, Occupancy Networks, GenRe, and Mesh-RCNN on ABO. All methods are pre-trained on ShapeNet and show a decrease in performance on objects from ABO.\n\nface. To enable prediction for arbitrary shapes, [40] propose a cascaded CNN architecture with a single encoder and separate decoder for each SV-BRDF parameter. While the method achieves good results on semi-uncontrolled lighting environments, it requires using the intermediate bounces of global illumination rendering as supervision. More recent works have turned towards using multiple images to improve SV-BRDF estimation, but still only with simplistic object geometries. For instance, [17] and [20] use multiple input images with a flash lit light source, but only for a single planar surface. [7] and [8] both use procedurally generated shapes to estimate SV-BRDFs from multi-view images. ABO addresses the lack of sufficient realistic data for material estimation, and in this work we propose a simple baseline method that can estimate materials from single or multi-view images of complex, real-world shapes.\n\n2D/3D Image Retrieval Learning to represent 3D shapes and natural images of products in a single embedding space has been tackled by [39]. They consider various relevant tasks, including cross-view image retrieval, shape-based image retrieval and image-based shape retrieval, but all are inherently constrained by the limitations of ShapeNet [10] (cross-view image retrieval is only considered for chairs and cars). [36] introduced 3D object representations for finegrained recognition and a dataset of cars with real-world 2D imagery (CARS-196), which is now widely used for deep metric learning (DML) evaluation. Likewise, other datasets for DML focus on instances/fine categories of few object types, such as birds [60], clothes [44], or a few object categories [50]. Due to the limited diversity and the similar nature of query and target images in existing retrieval benchmarks, the performance of state-of-the-art DML algorithms are near saturation. Moreover, since these datasets come with little structure, the opportunities to analyze failure cases and improve algorithms are limited. Motivated by this, we de-rive a challenging large-scale benchmark dataset from ABO with hundreds of diverse categories and a proper validation set. We also leverage the 3D nature of ABO to measure and improve the robustness of representations with respect to changes in viewpoint and scene. A comparison of ABO and existing benchmarks for DML can be found in Table 2.\n\n\nThe ABO Dataset\n\nDataset Properties The ABO dataset originates from worldwide product listings, metadata, images and 3D models provided by Amazon.com. This data consists of 147,702 listings of products from 576 product types sold by various Amazon-owned stores and websites (e.g. Amazon, Pri-meNow, Whole Foods). Each listing is identified by an item ID and is provided with structured metadata corresponding to information that is publicly available on the listing's main webpage (such as product type, material, color, and dimensions) as well as the media available for that product. This includes 398, 212 high-resolution catalog images, and, when available, the turntable images that are used for the \"360\u00ba View\" feature that shows the product imaged at 5\u00ba or 15\u00ba azimuth intervals (8, 222 products).\n\n3D Models ABO also includes 7, 953 artist-created highquality 3D models in glTF 2.0 format. The 3D models are oriented in a canonical coordinate system where the \"front\" (when well defined) of all objects are aligned and each have a scale corresponding to real world units. To enable these meshes to easily be used for comparison with existing methods trained on 3D datasets such as ShapeNet, we have collected category annotations for each 3D model and mapped them to noun synsets under the WordNet [47] taxonomy. Figure 4 shows a histogram of the 3D model categories.\n\nCatalog Image Pose Annotations We additionally provide 6-DOF pose annotations for 6, 334 of the catalog images. To achieve this, we develop an automated pipeline for pose  Table 3. Single-view 3D reconstruction generalization from ShapeNet to ABO. Chamfer distance and absolute normal consistency of predictions made on ABO objects from common ShapeNet classes. We report the same metrics for ShapeNet objects (denoted in gray), following the same evaluation protocol. All methods, with the exception of GenRe, are trained on all of the ShapeNet categories listed. estimation based on the knowledge of the 3D model in the image, off-the-shelf instance masks [28,34], and differentiable rendering. For each mask M, we estimate R \u2208 SO(3) and T \u2208 R 3 such that the following silhouette loss is minimized\nR * , T * = argmin R,T \u2225DR(R, T) \u2212 M\u2225\nwhere DR(\u00b7) is a differentiable renderer implemented in PyTorch3D [52]. Examples of results from this approach can be found in Figure 2. Unlike previous approaches to CAD-to-image alignment [57,63] that use human annotators in-the-loop to provide pose or correspondences, our approach is fully automatic except for a final human verification step.\n\n\nMaterial Estimation Dataset\n\nTo perform material estimation from images, we use the Disney [9] base color, metallic, roughness parameterization given in glTF 2.0 specification [25]. We render 512x512 images from 91 camera positions along an upper icosphere of the object with a 60 \u2022 fieldof-view using Blender's [14] Cycles path-tracer. To ensure diverse realistic lighting conditions and backgrounds, we illuminate the scene using 3 random environment maps out of 108 indoor HDRIs [23]. For these rendered images, we generate the corresponding ground truth base color, metallicness, roughness, and normal maps along with the object depth map and segmentation mask. The resulting dataset consists of 2.1 million rendered images and corresponding camera intrinsics and extrinsics.\n\n\nExperiments\n\n\nEvaluating Single-View 3D Reconstruction\n\nAs existing methods are largely trained in a fully supervised manner using ShapeNet [10], we are interested in how well they will transfer to more real-world objects. To measure how well these models transfer to real object instances, we evaluate the performance of a variety of these methods on objects from ABO. Specifically we evaluate 3D-R2N2 [13], GenRe [69], Occupancy Networks [46], and Mesh R-CNN [22] pre-trained on ShapeNet. We selected these methods because they capture some of the topperforming single-view 3D reconstruction methods from the past few years and are varied in the type of 3D representation that they use (voxels in [13], spherical maps in [69], implicit functions in [46], and meshes in [22]) and the coordinate system used (canonical vs. view-space). While all the models we consider are pre-trained on ShapeNet, GenRe trains on a different set of classes and takes as input a silhouette mask at train and test time.\n\nTo study this question (irrespective of the question of cross-category generalization), we consider only the subset of ABO models objects that fall into ShapeNet training categories. Out of the 63 categories in ABO with 3D models, we consider 6 classes that intersect with commonly used ShapeNet classes, capturing 4,170 of the 7,953 3D models. Some common ShapeNet classes, such as \"airplane\", have no matching ABO category; similarly, some categories in ABO like \"air conditioner\" and \"weights\" do not map well to ShapeNet classes.\n\nFor this experiment, we render a dataset (distinct from the ABO Material Estimation Dataset) of objects on a blank background from a similar distribution of viewpoints as in the rendered ShapeNet training set. We render 30 viewpoints of each mesh using Blender [14], each with a 40 \u2022 field-of-view and such that the entire object is visible. Camera azimuth and elevation are sampled uniformly on the surface of a unit sphere with a \u221210 \u2022 lower limit on elevations to avoid uncommon bottom views.\n\nGenRe and Mesh-RCNN make their predictions in \"view-space\" (i.e. pose aligned to the image view), whereas R2N2 and Occupancy Networks perform predictions in canonical space (predictions are made in the same categoryspecific, canonical pose despite the pose of the object in an image). For each method we evaluate Chamfer Distance and Absolute Normal Consistency and largely follow the evaluation protocol of [22].\n\nResults A quantitative comparison of the four methods we considered on ABO objects can be found in Table 3. We also re-evaluated each method's predictions on the ShapeNet test set from R2N2 [13] with our evaluation protocol and report those metrics. We observe that Mesh R-CNN [22] outperforms all other methods across the board on both ABO and ShapeNet in terms of Chamfer Distance, whereas Occupancy Networks performs the best in terms of Absolute Normal Consistency. As can be seen, there is a large performance gap between all ShapeNet and ABO predictions. This suggests that shapes and textures from ABO, while derived from the same categories but from the real world, are out of distribution and more challenging for the models trained on ShapeNet. Further, we notice that the lamp category has a particularly large performance drop from ShapeNet to ABO. Qualitative results suggest that this is likely due to the difficulty in reconstructing thin structures. We highlight some qualitative results in Figure 5, including one particularly challenging lamp instance.\n\n\nMaterial Prediction\n\nTo date, there are not many available datasets tailored to the material prediction task. Most publicly available datasets with large collections of 3D objects [10,12,19] do not contain physically-accurate reflectance parameters that can be used for physically-based rendering to generate photorealistic images. Datasets like PhotoShape [51] do contain such parameters but are limited to a single category. In contrast, the realistic 3D models in ABO are artist-created and have highly varied shapes and SV-BRDFs. We leverage this unique property to derive a benchmark for material prediction with large amounts of photorealistic synthetic data. We also present a simple baseline approach for both singleand multi-view material estimation of complex geometries.\n\nMethod To evaluate single-view and multi-view material prediction and establish a baseline approach, we use a U-Net-based model with a ResNet-34 backbone to estimate SV-BRDFs from a single viewpoint. The U-Net has a common encoder that takes an RGB image as input and has a multi-head decoder to output each component of the SV-BRDF separately. Inspired by recent networks in [7,17], we align images from multiple viewpoints by projection using depth maps, and bundle the original image and projected image pairs as input data to enable an analogous approach for the multi-view network. We reuse the single-view architecture for the multi-view network and use global max pooling to handle an arbitrary number of input images. Similar to [16], we utilize a differentiable rendering layer to render the flash illuminated ground truth and compare it to similarly rendered images from our predictions to better regularize the network and guide the training process. Ground truth material maps are used for direct supervision.\n\nOur model takes as input 256x256 rendered images. For training, we randomly subsample 40 views on the icosphere  Table 4. ABO material estimation results for the single-view, multi-view, and multi-view network without projection (MVnet no proj.) ablation. Base color, roughness, metallicness and rendering loss are measured using RMSE (lower is better) -normal similarity is measured using cosine similarity (higher is better).\n\nfor each object. In the case of the multi-view network, for each reference view we select its immediate 4 adjacent views as neighboring views. We use mean squared error as the loss function for base color, roughness, metallicness, surface normal and render losses. Each network is trained for 17 epochs using the AdamW optimizer [45] with a learning rate of 1e-3 and weight decay of 1e-4.\n\nResults Results for the single-view network (SV-net) and multi-view network (MV-net) can be found in Table 4. The multi-view network has better performance compared to single-view network in terms of the base color, roughness, metallicness, and surface normal prediction tasks. The multi-view network is especially better at predicting properties that affect view-dependent specular components like roughness and metallicness.\n\nWe also run an ablation study on our multi-view network without using 3D structure to align neighboring views to reference view (denoted as MV-net: no projection). First, we observe that even without 3D structure-based alignment, the network still outperforms the single-view network on roughness and metallic predictions. Comparing to the multi-view network, which uses 3D structure-based alignment, we can see structure information leads to better performance for all parameters. We show some qualitative results from the test set in Figure 6.\n\nAs a focus of ABO is enabling real-world transfer, we also test our multi-view network on catalog images of objects from the test set using the pose annotations gathered by the methodology in Section 3, and use the inferred material parameters to relight the object (Figure 7). Despite the domain gap in lighting and background, and shift from synthetic to real, our network trained on rendered images makes reasonable predictions on the real catalog images. In one case (last row), the network fails to accurately infer the true base color, likely due to the presence of self-shadow.\n\n\nMulti-View Cross-Domain Object Retrieval\n\nMerging the available catalog images and 3D models in ABO, we derive a novel benchmark for object retrieval  with the unique ability to measure the robustness of algorithms with respect to viewpoint changes. Specifically, we leverage the renderings described in Section 3, with known azimuth and elevation, to provide more diverse views and scenes for training deep metric learning (DML) algorithms. We also use these renderings to evaluate the retrieval performance with respect to a large gallery of catalog images from ABO. This new benchmark is very challenging because the rendered images have complex and cluttered indoor backgrounds (compared to the cleaner catalog images) and display products with viewpoints that are not typically present in the catalog images. These two sources of images are indeed two separate image domains, making the test scenario a multi-view cross-domain retrieval task.\n\nMethod To compare the performance of state-of-the-art DML methods on our multi-view cross-domain retrieval benchmark, we use PyTorch Metric Learning [2] implementations that cover the main approaches to DML: NormSoftmax [68] (classification-based), ProxyNCA [48] (proxybased) and Contrastive, TripletMargin, NTXent [11] and Multi-similarity [61] (tuple-based). We leveraged the Powerful Benchmarker framework [1] to run fair and controlled comparisons as in [49], including Bayesian hyperparameter optimization.\n\nWe opted for a ResNet-50 [29] backbone, projected it to 128D after a LayerNorm [4] layer, did not freeze the Batch-Norm parameters and added an image padding transformation to obtain undistorted square images before resizing to 256x256. We used batches of 256 samples with 4 samples per class, except for NormSoftmax and ProxyNCA where we obtained better results with a batch size of 32 and 1 sample per class. After hyperparameter optimization, we trained all losses for 1000 epochs and chose the best epoch based on the validation Recall@1 metric, computing it only every other epoch.\n\nImportantly, whereas catalog and rendered images in the training set are balanced (188K vs 111K), classes with and without renderings are not (4K vs. 45K). Balancing them in each batch proved necessary to obtain good performance: not only do we want to exploit the novel viewpoints and scenes provided by the renderings to improve the retrieval performance, but there are otherwise simply not sufficiently many negative pairs of rendered images being sampled.  Table 5. Test performance of state-of-the-art deep metric learning methods on the ABO retrieval benchmark. Retrieving products from rendered images highlights performance gaps that are not as apparent when using catalog images.\n\nResults As shown in Table 5, the ResNet-50 baseline trained on ImageNet largely fails at the task (Recall@1 of 5%). This confirms the challenging nature of our novel benchmark. DML is thus key to obtain significant improvements. In our experiments, NormSoftmax, ProxyNCA and Contrastive performed better (\u2248 29%) than the Multisimilarity, NTXent or TripletMargin losses (\u2248 23%), a gap which was not apparent in other datasets, and is not as large when using cleaner catalog images as queries. Moreover, it is worth noting that the overall performance on ABO is significantly lower than for existing common benchmarks (see Table 2). This confirms their likely saturation [49], the value in new and more challenging retrieval tasks, and the need for novel metric learning approaches to handle the large scale and unique properties of our new benchmark. Further, the azimuth (\u03b8) and elevation (\u03c6) angles available for rendered test queries allow us to measure how performance degrades as these parameters diverge from typical product viewpoints in ABO's catalog images. Figure 8 highlights two main regimes for both azimuth and elevation: azimuths beyond |\u03b8| = 75 \u2022 and elevations above \u03c6 = 50 \u2022 are significantly more challenging to match, consistently for all approaches. Closing this gap is an interesting direction of future research on DML for multi-view object retrieval. For one, the current losses do not explicitly model the geometric information in training data.\n\n\nConclusion\n\nIn this work we introduced ABO, a new dataset to help bridge the gap between real and synthetic 3D worlds. We demonstrated that the set of real-world derived 3D models in ABO are a challenging test set for ShapeNet-trained 3D reconstruction approaches, and that both view-and canonical-space methods do not generalize well to ABO meshes despite sampling them from the same distribution of training classes. We also trained both single-view and multi-view networks for SV-BRDF material estimation of complex, real-world geometries -a task that is uniquely enabled by the nature of our 3D dataset. We found that incorporating multiple views leads to more accurate disentanglement of SV-BRDF properties. Finally, joining the larger set of products images with synthetic renders from ABO 3D models, we proposed a challenging multi-view retrieval task that alleviates some of the limitations in diversity and structure of existing datasets, which are close to performance saturation. The 3D models in ABO allowed us to exploit novel viewpoints and scenes during training and benchmark the performance of deep metric learning algorithms with respect to the azimuth and elevation of query images.\n\nWhile not considered in this work, the large amounts of text annotations (product descriptions and keywords) and non-rigid products (apparel, home linens) enable a wide array of possible language and vision tasks, such as predicting styles, patterns, captions or keywords from product images. Furthermore, the 3D objects in ABO correspond to items that naturally occur in a home, and have associated object weight and dimensions. This can benefit robotics research and support simulations of manipulation and navigation.\n\nFigure 3 .\n3Sample catalog images and attributes that accompany ABO objects. Each object has up to 18 attribute annotations.\n\nFigure 4 .\n43D model categories. Each category is also mapped to a synset in the WordNet hierarchy. Note the y-axis is in log scale.\n\nFigure 6 .\n6Qualitative material estimation results for single-view (SV-net) and multi-view (MV-net) networks. We show estimated SV-BRDF properties (base color, roughness, metallicness, surface normals) for each input view of an object compared to the ground truth.\n\nFigure 7 .\n7Qualitative multi-view material estimation results on real catalog images. Each of the multiple views is aligned to the reference view using the catalog image pose annotations.\n\nFigure 8 .\n8Recall@1 as a function of the azimuth and elevation of the product view. For all methods, retrieval performance degrades rapidly beyond azimuth |\u03b8| > 75 \u2022 and elevation \u03c6 > 50 \u2022 .\n\n\nFigure 2. Posed 3D models in catalog images. We use instance masks to automatically generate 6-DOF pose annotations.Dataset \n# Models # Classes Real images Full 3D PBR \n\nShapeNet [10] \n51.3K \n55 \n\u2717 \n\u2713 \n\u2717 \n3D-Future [19] \n16.6K \n8 \n\u2717 \n\u2713 \n\u2717 \nGoogle Scans [54] \n1K \n-\n\u2717 \n\u2713 \n\u2717 \nCO3D [53] \n18.6K \n50 \n\u2713 \n\u2717 \n\u2717 \nIKEA [42] \n219 \n11 \n\u2713 \n\u2713 \n\u2717 \nPix3D [57] \n395 \n9 \n\u2713 \n\u2713 \n\u2717 \nPhotoShape [51] \n5.8K \n1 \n\u2717 \n\u2713 \n\u2713 \nABO (Ours) \n8K \n63 \n\u2713 \n\u2713 \n\u2713 \n\nTable 1. A comparison of the 3D models in ABO and other \ncommonly used object-centric 3D datasets. ABO contains \nnearly 8K 3D models with physically-based rendering (PBR) ma-\nterials and corresponding real-world catalog images. \n\n\nAcknowledgements We thank Pietro Perona and Frederic Devernay. This work was funded in part by an NSF GRFP (#1752814) and the Amazon-BAIR Commons Program.\nPytorch metric learning. Ac- cessed: 2022-03-19. 7Pytorch metric learning. https://kevinmusgrave. github . io / pytorch -metric -learning. Ac- cessed: 2022-03-19. 7\n\nObjectron: A large scale dataset of object-centric videos in the wild with pose annotations. Adel Ahmadyan, Liangkai Zhang, Jianing Wei, Artsiom Ablavatski, Matthias Grundmann, arXiv:2012.09988arXiv preprintAdel Ahmadyan, Liangkai Zhang, Jianing Wei, Artsiom Ablavatski, and Matthias Grundmann. Objectron: A large scale dataset of object-centric videos in the wild with pose annotations. arXiv preprint arXiv:2012.09988, 2020. 3\n\n. Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E Hin, arXiv:1607.06450ton. Layer normalization. arXiv preprintJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin- ton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. 7\n\nOn the generalization of learning-based 3d reconstruction. Miguel Angel Bautista, Walter Talbott, Shuangfei Zhai, Nitish Srivastava, Joshua M Susskind, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer VisionMiguel Angel Bautista, Walter Talbott, Shuangfei Zhai, Ni- tish Srivastava, and Joshua M Susskind. On the generaliza- tion of learning-based 3d reconstruction. In Proceedings of the IEEE/CVF Winter Conference on Applications of Com- puter Vision, pages 2180-2189, 2021. 1\n\nFostering generalization in single-view 3d reconstruction by learning a hierarchy of local and global shape priors. Jan Bechtold, Maxim Tatarchenko, Volker Fischer, Thomas Brox, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionJan Bechtold, Maxim Tatarchenko, Volker Fischer, and Thomas Brox. Fostering generalization in single-view 3d reconstruction by learning a hierarchy of local and global shape priors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15880- 15889, 2021. 1\n\nDeep 3d capture: Geometry and reflectance from sparse multi-view images. Sai Bi, Zexiang Xu, Kalyan Sunkavalli, David Kriegman, Ravi Ramamoorthi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition36Sai Bi, Zexiang Xu, Kalyan Sunkavalli, David Kriegman, and Ravi Ramamoorthi. Deep 3d capture: Geometry and re- flectance from sparse multi-view images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5960-5969, 2020. 3, 4, 6\n\nTwo-shot spatially-varying brdf and shape estimation. Mark Boss, Varun Jampani, Kihwan Kim, Hendrik Lensch, Jan Kautz, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionMark Boss, Varun Jampani, Kihwan Kim, Hendrik Lensch, and Jan Kautz. Two-shot spatially-varying brdf and shape estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3982- 3991, 2020. 4\n\nWalt Disney Animation Studios. Physically-based shading at disney. Brent Burley, ACM SIGGRAPH. 2012Brent Burley and Walt Disney Animation Studios. Physically-based shading at disney. In ACM SIGGRAPH, volume 2012, pages 1-7. vol. 2012, 2012. 5\n\nX Angel, Thomas Chang, Leonidas Funkhouser, Pat Guibas, Qixing Hanrahan, Zimo Huang, Silvio Li, Manolis Savarese, Shuran Savva, Hao Song, Su, arXiv:1512.03012An information-rich 3d model repository. 56arXiv preprintAngel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015. 1, 2, 4, 5, 6\n\nA simple framework for contrastive learning of visual representations. Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, PMLR, 2020. 7International conference on machine learning. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge- offrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on ma- chine learning, pages 1597-1607. PMLR, 2020. 7\n\nSungjoon Choi, Qian-Yi Zhou, Stephen Miller, Vladlen Koltun, arXiv:1602.02481A large dataset of object scans. 26arXiv preprintSungjoon Choi, Qian-Yi Zhou, Stephen Miller, and Vladlen Koltun. A large dataset of object scans. arXiv preprint arXiv:1602.02481, 2016. 2, 3, 6\n\n3d-r2n2: A unified approach for single and multi-view 3d object reconstruction. B Christopher, Danfei Choy, Junyoung Xu, Kevin Gwak, Silvio Chen, Savarese, European conference on computer vision. Springer56Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin Chen, and Silvio Savarese. 3d-r2n2: A unified approach for single and multi-view 3d object reconstruction. In European conference on computer vision, pages 628-644. Springer, 2016. 1, 3, 5, 6\n\nBlender -a 3D modelling and rendering package. Blender Foundation, Stichting Blender Foundation. Blender Online Community, AmsterdamBlender Online Community. Blender -a 3D modelling and rendering package. Blender Foundation, Stichting Blender Foundation, Amsterdam, 2018. 5\n\nImagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, 2009 IEEE conference on computer vision and pattern recognition. IeeeJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248-255. Ieee, 2009. 1\n\nSingle-image svbrdf capture with a rendering-aware deep network. Miika Valentin Deschaintre, Fredo Aittala, George Durand, Adrien Drettakis, Bousseau, ACM Transactions on Graphics (TOG). 3746Valentin Deschaintre, Miika Aittala, Fredo Durand, George Drettakis, and Adrien Bousseau. Single-image svbrdf cap- ture with a rendering-aware deep network. ACM Transac- tions on Graphics (TOG), 37(4):128, 2018. 3, 6\n\nFlexible svbrdf capture with a multi-image deep network. Miika Valentin Deschaintre, Fr\u00e9do Aittala, George Durand, Adrien Drettakis, Bousseau, Computer Graphics Forum. Wiley Online Library386Valentin Deschaintre, Miika Aittala, Fr\u00e9do Durand, George Drettakis, and Adrien Bousseau. Flexible svbrdf capture with a multi-image deep network. In Computer Graphics Forum, volume 38, pages 1-13. Wiley Online Library, 2019. 3, 4, 6\n\nA point set generation network for 3d object reconstruction from a single image. Haoqiang Fan, Hao Su, Leonidas J Guibas, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionHaoqiang Fan, Hao Su, and Leonidas J Guibas. A point set generation network for 3d object reconstruction from a single image. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 605-613, 2017. 3\n\nHuan Fu, Rongfei Jia, Lin Gao, Mingming Gong, Binqiang Zhao, Steve Maybank, Dacheng Tao, arXiv:2009.096333d-future: 3d furniture shape with texture. 26arXiv preprintHuan Fu, Rongfei Jia, Lin Gao, Mingming Gong, Binqiang Zhao, Steve Maybank, and Dacheng Tao. 3d-future: 3d fur- niture shape with texture. arXiv preprint arXiv:2009.09633, 2020. 2, 6\n\nDeep inverse rendering for high-resolution svbrdf estimation from an arbitrary number of images. Duan Gao, Xiao Li, Yue Dong, Pieter Peers, Kun Xu, Xin Tong, ACM Transactions on Graphics (TOG). 3844Duan Gao, Xiao Li, Yue Dong, Pieter Peers, Kun Xu, and Xin Tong. Deep inverse rendering for high-resolution svbrdf estimation from an arbitrary number of images. ACM Trans- actions on Graphics (TOG), 38(4):134, 2019. 3, 4\n\nMesh r-cnn. Georgia Gkioxari, Jitendra Malik, Justin Johnson, ICCV. Georgia Gkioxari, Jitendra Malik, and Justin Johnson. Mesh r-cnn. In ICCV, 2019. 2\n\nMesh r-cnn. Georgia Gkioxari, Jitendra Malik, Justin Johnson, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer Vision36Georgia Gkioxari, Jitendra Malik, and Justin Johnson. Mesh r-cnn. In Proceedings of the IEEE International Conference on Computer Vision, pages 9785-9795, 2019. 3, 5, 6\n\n. Andreas Mischok, Greg Zaal, Sergej Majboroda, Andreas Mischok Greg Zaal, Sergej Majboroda. Hdrihaven. https://hdrihaven.com/. Accessed: 2020-11-16. 5\n\nA papier-m\u00e2ch\u00e9 approach to learning 3d surface generation. Thibault Groueix, Matthew Fisher, G Vladimir, Kim, C Bryan, Mathieu Russell, Aubry, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionThibault Groueix, Matthew Fisher, Vladimir G Kim, Bryan C Russell, and Mathieu Aubry. A papier-m\u00e2ch\u00e9 ap- proach to learning 3d surface generation. In Proceedings of the IEEE conference on computer vision and pattern recog- nition, pages 216-224, 2018. 1\n\nKhronos Group. gltf 2.0 specification. Khronos Group. gltf 2.0 specification. https : / / github.com/KhronosGroup/glTF. Accessed: 2020- 11-16. 5\n\nLvis: A dataset for large vocabulary instance segmentation. Agrim Gupta, Piotr Dollar, Ross Girshick, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionAgrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary instance segmentation. In Pro- ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5356-5364, 2019. 1\n\nMultiple view geometry in computer vision. Richard Hartley, Andrew Zisserman, Cambridge university pressRichard Hartley and Andrew Zisserman. Multiple view ge- ometry in computer vision. Cambridge university press, 2003. 3\n\nPiotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. Kaiming He, Georgia Gkioxari, ICCV. Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Gir- shick. Mask r-cnn. In ICCV, 2017. 5\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016. 7\n\nCombination of multiple global descriptors for image retrieval. Heejae Jun, Byungsoo Ko, Youngjoon Kim, Insik Kim, Jongtack Kim, arXiv:1903.10663arXiv preprintHeeJae Jun, ByungSoo Ko, Youngjoon Kim, Insik Kim, and Jongtack Kim. Combination of multiple global descriptors for image retrieval. arXiv preprint arXiv:1903.10663, 2019. 3\n\nLearning a multi-view stereo machine. Abhishek Kar, Christian H\u00e4ne, Jitendra Malik, Advances in neural information processing systems. Abhishek Kar, Christian H\u00e4ne, and Jitendra Malik. Learning a multi-view stereo machine. In Advances in neural infor- mation processing systems, pages 365-376, 2017. 3\n\nA lightweight approach for on-the-fly reflectance estimation. Kihwan Kim, Jinwei Gu, Stephen Tyree, Pavlo Molchanov, Matthias Nie\u00dfner, Jan Kautz, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionKihwan Kim, Jinwei Gu, Stephen Tyree, Pavlo Molchanov, Matthias Nie\u00dfner, and Jan Kautz. A lightweight approach for on-the-fly reflectance estimation. In Proceedings of the IEEE International Conference on Computer Vision, pages 20-28, 2017. 3\n\nProxy anchor loss for deep metric learning. Sungyeon Kim, Dongwon Kim, Minsu Cho, Suha Kwak, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Sungyeon Kim, Dongwon Kim, Minsu Cho, and Suha Kwak. Proxy anchor loss for deep metric learning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020. 3\n\nKaiming He, and Ross Girshick. Pointrend: Image segmentation as rendering. Alexander Kirillov, Yuxin Wu, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionAlexander Kirillov, Yuxin Wu, Kaiming He, and Ross Gir- shick. Pointrend: Image segmentation as rendering. In Pro- ceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9799-9808, 2020. 5\n\nAbc: A big cad model dataset for geometric deep learning. Sebastian Koch, Albert Matveev, Zhongshi Jiang, Francis Williams, Alexey Artemov, Evgeny Burnaev, Marc Alexa, Denis Zorin, Daniele Panozzo, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionSebastian Koch, Albert Matveev, Zhongshi Jiang, Francis Williams, Alexey Artemov, Evgeny Burnaev, Marc Alexa, Denis Zorin, and Daniele Panozzo. Abc: A big cad model dataset for geometric deep learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9601-9611, 2019. 1\n\n3d object representations for fine-grained categorization. Jonathan Krause, Michael Stark, Jia Deng, Li Fei-Fei, 4th International IEEE Workshop on 3D Representation and Recognition. Sydney, AustraliaJonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In 4th International IEEE Workshop on 3D Representation and Recognition (3dRR-13), Sydney, Australia, 2013. 4\n\nLearning multiple layers of features from tiny images. Alex Krizhevsky, Alex Krizhevsky et al. Learning multiple layers of features from tiny images. 2009. 1\n\nModeling surface appearance from a single photograph using selfaugmented convolutional neural networks. Xiao Li, Yue Dong, Pieter Peers, Xin Tong, ACM Transactions on Graphics (TOG). 36445Xiao Li, Yue Dong, Pieter Peers, and Xin Tong. Model- ing surface appearance from a single photograph using self- augmented convolutional neural networks. ACM Transac- tions on Graphics (TOG), 36(4):45, 2017. 3\n\nJoint embeddings of shapes and images via cnn image purification. Yangyan Li, Hao Su, Charles Ruizhongtai Qi, Noa Fish, Daniel Cohen-Or, Leonidas J Guibas, ACM Trans. Graph. 4Yangyan Li, Hao Su, Charles Ruizhongtai Qi, Noa Fish, Daniel Cohen-Or, and Leonidas J. Guibas. Joint embeddings of shapes and images via cnn image purification. ACM Trans. Graph., 2015. 4\n\nLearning to reconstruct shape and spatially-varying reflectance from a single image. Zhengqin Li, Zexiang Xu, Ravi Ramamoorthi, Kalyan Sunkavalli, Manmohan Chandraker, SIGGRAPH Asia 2018 Technical Papers. ACM34Zhengqin Li, Zexiang Xu, Ravi Ramamoorthi, Kalyan Sunkavalli, and Manmohan Chandraker. Learning to recon- struct shape and spatially-varying reflectance from a single image. In SIGGRAPH Asia 2018 Technical Papers, page 269. ACM, 2018. 3, 4\n\nParsing ikea objects: Fine pose estimation. J Joseph, Hamed Lim, Antonio Pirsiavash, Torralba, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionJoseph J Lim, Hamed Pirsiavash, and Antonio Torralba. Parsing ikea objects: Fine pose estimation. In Proceedings of the IEEE International Conference on Computer Vision, pages 2992-2999, 2013. 2\n\nParsing ikea objects: Fine pose estimation. J Joseph, Hamed Lim, Antonio Pirsiavash, Torralba, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionJoseph J Lim, Hamed Pirsiavash, and Antonio Torralba. Parsing ikea objects: Fine pose estimation. In Proceedings of the IEEE International Conference on Computer Vision, pages 2992-2999, 2013. 2\n\nMicrosoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, C Lawrence Zitnick, European conference on computer vision. SpringerTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740-755. Springer, 2014. 1\n\nDeepfashion: Powering robust clothes recognition and retrieval with rich annotations. Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, Xiaoou Tang, Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xi- aoou Tang. Deepfashion: Powering robust clothes recog- nition and retrieval with rich annotations. In Proceedings of IEEE Conference on Computer Vision and Pattern Recogni- tion (CVPR), June 2016. 4\n\n. Ilya Loshchilov, Frank Hutter, arXiv:1711.05101Decoupled weight decay regularization. arXiv preprintIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 6\n\nOccupancy networks: Learning 3d reconstruction in function space. Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, Andreas Geiger, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition15Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se- bastian Nowozin, and Andreas Geiger. Occupancy networks: Learning 3d reconstruction in function space. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4460-4470, 2019. 1, 3, 5\n\nWordnet: a lexical database for english. A George, Miller, Communications of the ACM. 3811George A Miller. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39-41, 1995. 4\n\nNo fuss distance metric learning using proxies. Yair Movshovitz-Attias, Alexander Toshev, Thomas K Leung, Sergey Ioffe, Saurabh Singh, Proceedings of the IEEE International Conference on Computer Vision (ICCV). the IEEE International Conference on Computer Vision (ICCV)Yair Movshovitz-Attias, Alexander Toshev, Thomas K. Le- ung, Sergey Ioffe, and Saurabh Singh. No fuss distance met- ric learning using proxies. In Proceedings of the IEEE Inter- national Conference on Computer Vision (ICCV), Oct 2017. 7\n\nA metric learning reality check. Kevin Musgrave, Serge Belongie, Ser-Nam Lim, European Conference on Computer Vision. Springer7Kevin Musgrave, Serge Belongie, and Ser-Nam Lim. A met- ric learning reality check. In European Conference on Com- puter Vision, pages 681-699. Springer, 2020. 7, 8\n\nDeep metric learning via lifted structured feature embedding. Hyun Oh Song, Yu Xiang, Stefanie Jegelka, Silvio Savarese, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio Savarese. Deep metric learning via lifted structured feature embedding. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR), June 2016. 4\n\nPhotoshape: Photorealistic materials for large-scale shape collections. Keunhong Park, Konstantinos Rematas, Ali Farhadi, Steven M Seitz, arXiv:1809.0976126arXiv preprintKeunhong Park, Konstantinos Rematas, Ali Farhadi, and Steven M Seitz. Photoshape: Photorealistic materi- als for large-scale shape collections. arXiv preprint arXiv:1809.09761, 2018. 2, 3, 6\n\nAccelerating 3d deep learning with pytorch3d. Nikhila Ravi, Jeremy Reizenstein, David Novotny, Taylor Gordon, Wan-Yen Lo, Justin Johnson, Georgia Gkioxari, arXiv:2007.08501arXiv preprintNikhila Ravi, Jeremy Reizenstein, David Novotny, Tay- lor Gordon, Wan-Yen Lo, Justin Johnson, and Georgia Gkioxari. Accelerating 3d deep learning with pytorch3d. arXiv preprint arXiv:2007.08501, 2020. 5\n\nCommon objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction. Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, David Novotny, arXiv:2109.0051223arXiv preprintJeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny. Com- mon objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction. arXiv preprint arXiv:2109.00512, 2021. 2, 3\n\nGoogle scanned objects. Google Research, Google Research. Google scanned objects, August. 2\n\nBigbird: A large-scale 3d database of object instances. Arjun Singh, James Sha, S Karthik, Tudor Narayan, Pieter Achim, Abbeel, 2014 IEEE international conference on robotics and automation (ICRA). Arjun Singh, James Sha, Karthik S Narayan, Tudor Achim, and Pieter Abbeel. Bigbird: A large-scale 3d database of object instances. In 2014 IEEE international conference on robotics and automation (ICRA), pages 509-516. IEEE, 2014. 2\n\nRevisiting unreasonable effectiveness of data in deep learning era. Chen Sun, Abhinav Shrivastava, Saurabh Singh, Abhinav Gupta, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionChen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhi- nav Gupta. Revisiting unreasonable effectiveness of data in deep learning era. In Proceedings of the IEEE international conference on computer vision, pages 843-852, 2017. 1\n\nPix3d: Dataset and methods for single-image 3d shape modeling. Xingyuan Sun, Jiajun Wu, Xiuming Zhang, Zhoutong Zhang, Chengkai Zhang, Tianfan Xue, Joshua B Tenenbaum, William T Freeman, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition25Xingyuan Sun, Jiajun Wu, Xiuming Zhang, Zhoutong Zhang, Chengkai Zhang, Tianfan Xue, Joshua B Tenenbaum, and William T Freeman. Pix3d: Dataset and methods for single-image 3d shape modeling. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2974-2983, 2018. 2, 3, 5\n\nWhat do single-view 3d reconstruction networks learn?. Maxim Tatarchenko, R Stephan, Ren\u00e9 Richter, Zhuwen Ranftl, Vladlen Li, Thomas Koltun, Brox, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionMaxim Tatarchenko, Stephan R Richter, Ren\u00e9 Ranftl, Zhuwen Li, Vladlen Koltun, and Thomas Brox. What do single-view 3d reconstruction networks learn? In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3405-3414, 2019. 1\n\nMulti-view supervision for single-view reconstruction via differentiable ray consistency. Shubham Tulsiani, Tinghui Zhou, Alexei A Efros, Jitendra Malik, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionShubham Tulsiani, Tinghui Zhou, Alexei A Efros, and Ji- tendra Malik. Multi-view supervision for single-view re- construction via differentiable ray consistency. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 2626-2634, 2017. 3\n\nThe Caltech-UCSD Birds-200-2011 Dataset. C Wah, S Branson, P Welinder, P Perona, S Belongie, CNS-TR-2011-001California Institute of TechnologyTechnical ReportC. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The Caltech-UCSD Birds-200-2011 Dataset. Technical Re- port CNS-TR-2011-001, California Institute of Technology, 2011. 4\n\nMulti-similarity loss with general pair weighting for deep metric learning. Xun Wang, Xintong Han, Weilin Huang, Dengke Dong, Matthew R Scott, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionXun Wang, Xintong Han, Weilin Huang, Dengke Dong, and Matthew R Scott. Multi-similarity loss with general pair weighting for deep metric learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5022-5030, 2019. 7\n\nSilnet: Singleand multi-view reconstruction by learning from silhouettes. Olivia Wiles, Andrew Zisserman, arXiv:1711.07888arXiv preprintOlivia Wiles and Andrew Zisserman. Silnet: Single- and multi-view reconstruction by learning from silhouettes. arXiv preprint arXiv:1711.07888, 2017. 3\n\nObjectnet3d: A large scale database for 3d object recognition. Yu Xiang, Wonhui Kim, Wei Chen, Jingwei Ji, Christopher Choy, Hao Su, Roozbeh Mottaghi, Leonidas Guibas, Silvio Savarese, European Conference on Computer Vision. Springer15Yu Xiang, Wonhui Kim, Wei Chen, Jingwei Ji, Christopher Choy, Hao Su, Roozbeh Mottaghi, Leonidas Guibas, and Sil- vio Savarese. Objectnet3d: A large scale database for 3d object recognition. In European Conference on Computer Vision, pages 160-176. Springer, 2016. 1, 2, 5\n\nBeyond pascal: A benchmark for 3d object detection in the wild. Yu Xiang, Roozbeh Mottaghi, Silvio Savarese, IEEE winter conference on applications of computer vision. IEEE1Yu Xiang, Roozbeh Mottaghi, and Silvio Savarese. Beyond pascal: A benchmark for 3d object detection in the wild. In IEEE winter conference on applications of computer vision, pages 75-82. IEEE, 2014. 1, 2\n\nPix2vox: Context-aware 3d reconstruction from single and multi-view images. Haozhe Xie, Hongxun Yao, Xiaoshuai Sun, Shangchen Zhou, Shengping Zhang, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer Vision13Haozhe Xie, Hongxun Yao, Xiaoshuai Sun, Shangchen Zhou, and Shengping Zhang. Pix2vox: Context-aware 3d reconstruction from single and multi-view images. In Pro- ceedings of the IEEE International Conference on Computer Vision, pages 2690-2698, 2019. 1, 3\n\nPerspective transformer nets: Learning singleview 3d object reconstruction without 3d supervision. Xinchen Yan, Jimei Yang, Ersin Yumer, Yijie Guo, Honglak Lee, Advances in neural information processing systems. Xinchen Yan, Jimei Yang, Ersin Yumer, Yijie Guo, and Honglak Lee. Perspective transformer nets: Learning single- view 3d object reconstruction without 3d supervision. In Advances in neural information processing systems, pages 1696-1704, 2016. 3\n\nSingle image surface appearance modeling with selfaugmented cnns and inexact supervision. Wenjie Ye, Xiao Li, Yue Dong, Pieter Peers, Xin Tong, Computer Graphics Forum. Wiley Online Library37Wenjie Ye, Xiao Li, Yue Dong, Pieter Peers, and Xin Tong. Single image surface appearance modeling with self- augmented cnns and inexact supervision. In Computer Graphics Forum, volume 37, pages 201-211. Wiley Online Library, 2018. 3\n\nClassification is a strong baseline for deep metric learning. Andrew Zhai, Hao-Yu Wu, Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (BMVC). IEEE Conference on Computer Vision and Pattern Recognition (BMVC)Andrew Zhai and Hao-Yu Wu. Classification is a strong baseline for deep metric learning. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (BMVC), 2019. 7\n\nLearning to reconstruct shapes from unseen classes. Xiuming Zhang, Zhoutong Zhang, Chengkai Zhang, Josh Tenenbaum, Bill Freeman, Jiajun Wu, Advances in Neural Information Processing Systems. 35Xiuming Zhang, Zhoutong Zhang, Chengkai Zhang, Josh Tenenbaum, Bill Freeman, and Jiajun Wu. Learning to re- construct shapes from unseen classes. In Advances in Neural Information Processing Systems, pages 2257-2268, 2018. 3, 5\n\nQingnan Zhou, Alec Jacobson, arXiv:1605.04797Thingi10k: A dataset of 10,000 3d-printing models. arXiv preprintQingnan Zhou and Alec Jacobson. Thingi10k: A dataset of 10,000 3d-printing models. arXiv preprint arXiv:1605.04797, 2016. 1\n", "annotations": {"author": "[{\"end\":100,\"start\":70},{\"end\":128,\"start\":101},{\"end\":149,\"start\":129},{\"end\":182,\"start\":150},{\"end\":214,\"start\":183},{\"end\":239,\"start\":215},{\"end\":258,\"start\":240},{\"end\":289,\"start\":259},{\"end\":317,\"start\":290},{\"end\":342,\"start\":318},{\"end\":372,\"start\":343},{\"end\":402,\"start\":373}]", "publisher": null, "author_last_name": "[{\"end\":85,\"start\":78},{\"end\":113,\"start\":109},{\"end\":139,\"start\":135},{\"end\":167,\"start\":161},{\"end\":190,\"start\":188},{\"end\":229,\"start\":221},{\"end\":248,\"start\":243},{\"end\":279,\"start\":267},{\"end\":307,\"start\":297},{\"end\":332,\"start\":327},{\"end\":362,\"start\":352},{\"end\":387,\"start\":382}]", "author_first_name": "[{\"end\":77,\"start\":70},{\"end\":108,\"start\":101},{\"end\":134,\"start\":129},{\"end\":160,\"start\":150},{\"end\":187,\"start\":183},{\"end\":220,\"start\":215},{\"end\":242,\"start\":240},{\"end\":264,\"start\":259},{\"end\":266,\"start\":265},{\"end\":296,\"start\":290},{\"end\":326,\"start\":318},{\"end\":351,\"start\":343},{\"end\":381,\"start\":373}]", "author_affiliation": "[{\"end\":99,\"start\":87},{\"end\":127,\"start\":115},{\"end\":148,\"start\":141},{\"end\":181,\"start\":169},{\"end\":204,\"start\":192},{\"end\":213,\"start\":206},{\"end\":238,\"start\":231},{\"end\":257,\"start\":250},{\"end\":288,\"start\":281},{\"end\":316,\"start\":309},{\"end\":341,\"start\":334},{\"end\":371,\"start\":364},{\"end\":401,\"start\":389}]", "title": "[{\"end\":67,\"start\":1},{\"end\":469,\"start\":403}]", "venue": null, "abstract": "[{\"end\":1434,\"start\":499}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b13\"},\"end\":1527,\"start\":1523},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":1530,\"start\":1527},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":1533,\"start\":1530},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":1536,\"start\":1533},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":1539,\"start\":1536},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2197,\"start\":2193},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2200,\"start\":2197},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":2203,\"start\":2200},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2539,\"start\":2535},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2542,\"start\":2539},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":2545,\"start\":2542},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":2548,\"start\":2545},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2652,\"start\":2649},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2654,\"start\":2652},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":2657,\"start\":2654},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":2786,\"start\":2782},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":2789,\"start\":2786},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":3306,\"start\":3302},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":3321,\"start\":3317},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3535,\"start\":3531},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":3814,\"start\":3810},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4315,\"start\":4311},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":4318,\"start\":4315},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":4321,\"start\":4318},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6719,\"start\":6715},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":6862,\"start\":6858},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":6877,\"start\":6873},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":7071,\"start\":7067},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":7092,\"start\":7088},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7517,\"start\":7513},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7535,\"start\":7532},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":7685,\"start\":7681},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":7951,\"start\":7947},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8157,\"start\":8153},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8160,\"start\":8157},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8250,\"start\":8246},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8307,\"start\":8303},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8315,\"start\":8312},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9268,\"start\":9264},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9271,\"start\":9268},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":9274,\"start\":9271},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":9277,\"start\":9274},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":9280,\"start\":9277},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9300,\"start\":9296},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9303,\"start\":9300},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":9306,\"start\":9303},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9475,\"start\":9471},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":9478,\"start\":9475},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":9481,\"start\":9478},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":9484,\"start\":9481},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9614,\"start\":9610},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":9833,\"start\":9829},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9945,\"start\":9941},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9948,\"start\":9945},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":9951,\"start\":9948},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":9954,\"start\":9951},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":10188,\"start\":10184},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":10503,\"start\":10499},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10626,\"start\":10622},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":11061,\"start\":11057},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11503,\"start\":11499},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":11512,\"start\":11508},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":11611,\"start\":11608},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":11619,\"start\":11616},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":12064,\"start\":12060},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":12273,\"start\":12269},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":12347,\"start\":12343},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":12649,\"start\":12645},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":12663,\"start\":12659},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":12696,\"start\":12692},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":14701,\"start\":14697},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":15430,\"start\":15426},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":15433,\"start\":15430},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":15677,\"start\":15673},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":15801,\"start\":15797},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":15804,\"start\":15801},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":16051,\"start\":16048},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":16137,\"start\":16133},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":16273,\"start\":16269},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":16443,\"start\":16439},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":16883,\"start\":16879},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":17146,\"start\":17142},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":17158,\"start\":17154},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":17183,\"start\":17179},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":17204,\"start\":17200},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":17442,\"start\":17438},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":17466,\"start\":17462},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":17494,\"start\":17490},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":17514,\"start\":17510},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":18542,\"start\":18538},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":19186,\"start\":19182},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":19383,\"start\":19379},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":19470,\"start\":19466},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":20446,\"start\":20442},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":20449,\"start\":20446},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":20452,\"start\":20449},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":20623,\"start\":20619},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":21424,\"start\":21421},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":21427,\"start\":21424},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":21786,\"start\":21782},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":22830,\"start\":22826},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":25550,\"start\":25547},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":25622,\"start\":25618},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":25660,\"start\":25656},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":25717,\"start\":25713},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":25743,\"start\":25739},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":25860,\"start\":25856},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":25940,\"start\":25936},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":25993,\"start\":25990},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":27862,\"start\":27858}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":30510,\"start\":30385},{\"attributes\":{\"id\":\"fig_1\"},\"end\":30644,\"start\":30511},{\"attributes\":{\"id\":\"fig_3\"},\"end\":30911,\"start\":30645},{\"attributes\":{\"id\":\"fig_4\"},\"end\":31101,\"start\":30912},{\"attributes\":{\"id\":\"fig_5\"},\"end\":31294,\"start\":31102},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":31955,\"start\":31295}]", "paragraph": "[{\"end\":1790,\"start\":1450},{\"end\":2658,\"start\":1792},{\"end\":3291,\"start\":2660},{\"end\":4033,\"start\":3293},{\"end\":4881,\"start\":4035},{\"end\":6039,\"start\":4883},{\"end\":6066,\"start\":6041},{\"end\":6670,\"start\":6068},{\"end\":8988,\"start\":6687},{\"end\":9992,\"start\":9016},{\"end\":10757,\"start\":9994},{\"end\":11006,\"start\":10766},{\"end\":11925,\"start\":11008},{\"end\":13388,\"start\":11927},{\"end\":14195,\"start\":13408},{\"end\":14766,\"start\":14197},{\"end\":15568,\"start\":14768},{\"end\":15954,\"start\":15607},{\"end\":16736,\"start\":15986},{\"end\":17740,\"start\":16795},{\"end\":18275,\"start\":17742},{\"end\":18772,\"start\":18277},{\"end\":19187,\"start\":18774},{\"end\":20259,\"start\":19189},{\"end\":21043,\"start\":20283},{\"end\":22066,\"start\":21045},{\"end\":22495,\"start\":22068},{\"end\":22885,\"start\":22497},{\"end\":23313,\"start\":22887},{\"end\":23860,\"start\":23315},{\"end\":24446,\"start\":23862},{\"end\":25396,\"start\":24491},{\"end\":25909,\"start\":25398},{\"end\":26497,\"start\":25911},{\"end\":27187,\"start\":26499},{\"end\":28658,\"start\":27189},{\"end\":29862,\"start\":28673},{\"end\":30384,\"start\":29864}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":15606,\"start\":15569}]", "table_ref": "[{\"end\":7187,\"start\":7180},{\"end\":8619,\"start\":8612},{\"end\":13387,\"start\":13380},{\"end\":14947,\"start\":14940},{\"end\":19295,\"start\":19288},{\"end\":22188,\"start\":22181},{\"end\":22995,\"start\":22988},{\"end\":26967,\"start\":26960},{\"end\":27216,\"start\":27209},{\"end\":27817,\"start\":27810}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1448,\"start\":1436},{\"attributes\":{\"n\":\"2.\"},\"end\":6685,\"start\":6673},{\"end\":9014,\"start\":8991},{\"end\":10764,\"start\":10760},{\"attributes\":{\"n\":\"3.\"},\"end\":13406,\"start\":13391},{\"end\":15984,\"start\":15957},{\"attributes\":{\"n\":\"4.\"},\"end\":16750,\"start\":16739},{\"attributes\":{\"n\":\"4.1.\"},\"end\":16793,\"start\":16753},{\"attributes\":{\"n\":\"4.2.\"},\"end\":20281,\"start\":20262},{\"attributes\":{\"n\":\"4.3.\"},\"end\":24489,\"start\":24449},{\"attributes\":{\"n\":\"5.\"},\"end\":28671,\"start\":28661},{\"end\":30396,\"start\":30386},{\"end\":30522,\"start\":30512},{\"end\":30656,\"start\":30646},{\"end\":30923,\"start\":30913},{\"end\":31113,\"start\":31103}]", "table": "[{\"end\":31955,\"start\":31413}]", "figure_caption": "[{\"end\":30510,\"start\":30398},{\"end\":30644,\"start\":30524},{\"end\":30911,\"start\":30658},{\"end\":31101,\"start\":30925},{\"end\":31294,\"start\":31115},{\"end\":31413,\"start\":31297}]", "figure_ref": "[{\"end\":5870,\"start\":5862},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5926,\"start\":5918},{\"end\":10812,\"start\":10804},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14720,\"start\":14712},{\"end\":15742,\"start\":15734},{\"end\":20204,\"start\":20196},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":23859,\"start\":23851},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":24138,\"start\":24128},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":28263,\"start\":28255}]", "bib_author_first_name": "[{\"end\":32374,\"start\":32370},{\"end\":32393,\"start\":32385},{\"end\":32408,\"start\":32401},{\"end\":32421,\"start\":32414},{\"end\":32442,\"start\":32434},{\"end\":32714,\"start\":32709},{\"end\":32718,\"start\":32715},{\"end\":32728,\"start\":32723},{\"end\":32733,\"start\":32729},{\"end\":32749,\"start\":32741},{\"end\":32751,\"start\":32750},{\"end\":32998,\"start\":32992},{\"end\":33004,\"start\":32999},{\"end\":33021,\"start\":33015},{\"end\":33040,\"start\":33031},{\"end\":33053,\"start\":33047},{\"end\":33072,\"start\":33066},{\"end\":33074,\"start\":33073},{\"end\":33624,\"start\":33621},{\"end\":33640,\"start\":33635},{\"end\":33660,\"start\":33654},{\"end\":33676,\"start\":33670},{\"end\":34205,\"start\":34202},{\"end\":34217,\"start\":34210},{\"end\":34228,\"start\":34222},{\"end\":34246,\"start\":34241},{\"end\":34261,\"start\":34257},{\"end\":34754,\"start\":34750},{\"end\":34766,\"start\":34761},{\"end\":34782,\"start\":34776},{\"end\":34795,\"start\":34788},{\"end\":34807,\"start\":34804},{\"end\":35272,\"start\":35267},{\"end\":35445,\"start\":35444},{\"end\":35459,\"start\":35453},{\"end\":35475,\"start\":35467},{\"end\":35491,\"start\":35488},{\"end\":35506,\"start\":35500},{\"end\":35521,\"start\":35517},{\"end\":35535,\"start\":35529},{\"end\":35547,\"start\":35540},{\"end\":35564,\"start\":35558},{\"end\":35575,\"start\":35572},{\"end\":35987,\"start\":35983},{\"end\":35999,\"start\":35994},{\"end\":36019,\"start\":36011},{\"end\":36037,\"start\":36029},{\"end\":36336,\"start\":36328},{\"end\":36350,\"start\":36343},{\"end\":36364,\"start\":36357},{\"end\":36380,\"start\":36373},{\"end\":36681,\"start\":36680},{\"end\":36701,\"start\":36695},{\"end\":36716,\"start\":36708},{\"end\":36726,\"start\":36721},{\"end\":36739,\"start\":36733},{\"end\":37382,\"start\":37379},{\"end\":37392,\"start\":37389},{\"end\":37406,\"start\":37399},{\"end\":37421,\"start\":37415},{\"end\":37429,\"start\":37426},{\"end\":37436,\"start\":37434},{\"end\":37807,\"start\":37802},{\"end\":37835,\"start\":37830},{\"end\":37851,\"start\":37845},{\"end\":37866,\"start\":37860},{\"end\":38208,\"start\":38203},{\"end\":38236,\"start\":38231},{\"end\":38252,\"start\":38246},{\"end\":38267,\"start\":38261},{\"end\":38661,\"start\":38653},{\"end\":38670,\"start\":38667},{\"end\":38683,\"start\":38675},{\"end\":38685,\"start\":38684},{\"end\":39071,\"start\":39067},{\"end\":39083,\"start\":39076},{\"end\":39092,\"start\":39089},{\"end\":39106,\"start\":39098},{\"end\":39121,\"start\":39113},{\"end\":39133,\"start\":39128},{\"end\":39150,\"start\":39143},{\"end\":39517,\"start\":39513},{\"end\":39527,\"start\":39523},{\"end\":39535,\"start\":39532},{\"end\":39548,\"start\":39542},{\"end\":39559,\"start\":39556},{\"end\":39567,\"start\":39564},{\"end\":39856,\"start\":39849},{\"end\":39875,\"start\":39867},{\"end\":39889,\"start\":39883},{\"end\":40008,\"start\":40001},{\"end\":40027,\"start\":40019},{\"end\":40041,\"start\":40035},{\"end\":40353,\"start\":40346},{\"end\":40367,\"start\":40363},{\"end\":40380,\"start\":40374},{\"end\":40564,\"start\":40556},{\"end\":40581,\"start\":40574},{\"end\":40591,\"start\":40590},{\"end\":40608,\"start\":40607},{\"end\":40623,\"start\":40616},{\"end\":41247,\"start\":41242},{\"end\":41260,\"start\":41255},{\"end\":41273,\"start\":41269},{\"end\":41703,\"start\":41696},{\"end\":41719,\"start\":41713},{\"end\":41929,\"start\":41922},{\"end\":41941,\"start\":41934},{\"end\":42106,\"start\":42099},{\"end\":42118,\"start\":42111},{\"end\":42134,\"start\":42126},{\"end\":42144,\"start\":42140},{\"end\":42581,\"start\":42575},{\"end\":42595,\"start\":42587},{\"end\":42609,\"start\":42600},{\"end\":42620,\"start\":42615},{\"end\":42634,\"start\":42626},{\"end\":42891,\"start\":42883},{\"end\":42906,\"start\":42897},{\"end\":42921,\"start\":42913},{\"end\":43216,\"start\":43210},{\"end\":43228,\"start\":43222},{\"end\":43240,\"start\":43233},{\"end\":43253,\"start\":43248},{\"end\":43273,\"start\":43265},{\"end\":43286,\"start\":43283},{\"end\":43711,\"start\":43703},{\"end\":43724,\"start\":43717},{\"end\":43735,\"start\":43730},{\"end\":43745,\"start\":43741},{\"end\":44092,\"start\":44083},{\"end\":44108,\"start\":44103},{\"end\":44550,\"start\":44541},{\"end\":44563,\"start\":44557},{\"end\":44581,\"start\":44573},{\"end\":44596,\"start\":44589},{\"end\":44613,\"start\":44607},{\"end\":44629,\"start\":44623},{\"end\":44643,\"start\":44639},{\"end\":44656,\"start\":44651},{\"end\":44671,\"start\":44664},{\"end\":45211,\"start\":45203},{\"end\":45227,\"start\":45220},{\"end\":45238,\"start\":45235},{\"end\":45247,\"start\":45245},{\"end\":45631,\"start\":45627},{\"end\":45839,\"start\":45835},{\"end\":45847,\"start\":45844},{\"end\":45860,\"start\":45854},{\"end\":45871,\"start\":45868},{\"end\":46204,\"start\":46197},{\"end\":46212,\"start\":46209},{\"end\":46224,\"start\":46217},{\"end\":46236,\"start\":46225},{\"end\":46244,\"start\":46241},{\"end\":46257,\"start\":46251},{\"end\":46276,\"start\":46268},{\"end\":46278,\"start\":46277},{\"end\":46588,\"start\":46580},{\"end\":46600,\"start\":46593},{\"end\":46609,\"start\":46605},{\"end\":46629,\"start\":46623},{\"end\":46650,\"start\":46642},{\"end\":46991,\"start\":46990},{\"end\":47005,\"start\":47000},{\"end\":47018,\"start\":47011},{\"end\":47403,\"start\":47402},{\"end\":47417,\"start\":47412},{\"end\":47430,\"start\":47423},{\"end\":47821,\"start\":47813},{\"end\":47834,\"start\":47827},{\"end\":47847,\"start\":47842},{\"end\":47863,\"start\":47858},{\"end\":47876,\"start\":47870},{\"end\":47889,\"start\":47885},{\"end\":47904,\"start\":47899},{\"end\":47923,\"start\":47913},{\"end\":48316,\"start\":48311},{\"end\":48326,\"start\":48322},{\"end\":48335,\"start\":48332},{\"end\":48349,\"start\":48341},{\"end\":48362,\"start\":48356},{\"end\":48775,\"start\":48771},{\"end\":48793,\"start\":48788},{\"end\":49056,\"start\":49052},{\"end\":49075,\"start\":49068},{\"end\":49092,\"start\":49085},{\"end\":49112,\"start\":49103},{\"end\":49129,\"start\":49122},{\"end\":49597,\"start\":49596},{\"end\":49805,\"start\":49801},{\"end\":49834,\"start\":49825},{\"end\":49849,\"start\":49843},{\"end\":49851,\"start\":49850},{\"end\":49865,\"start\":49859},{\"end\":49880,\"start\":49873},{\"end\":50299,\"start\":50294},{\"end\":50315,\"start\":50310},{\"end\":50333,\"start\":50326},{\"end\":50623,\"start\":50616},{\"end\":50632,\"start\":50630},{\"end\":50648,\"start\":50640},{\"end\":50664,\"start\":50658},{\"end\":51140,\"start\":51132},{\"end\":51159,\"start\":51147},{\"end\":51172,\"start\":51169},{\"end\":51190,\"start\":51182},{\"end\":51475,\"start\":51468},{\"end\":51488,\"start\":51482},{\"end\":51507,\"start\":51502},{\"end\":51523,\"start\":51517},{\"end\":51539,\"start\":51532},{\"end\":51550,\"start\":51544},{\"end\":51567,\"start\":51560},{\"end\":51917,\"start\":51911},{\"end\":51936,\"start\":51931},{\"end\":51956,\"start\":51949},{\"end\":51970,\"start\":51966},{\"end\":51988,\"start\":51981},{\"end\":52003,\"start\":51998},{\"end\":52327,\"start\":52321},{\"end\":52451,\"start\":52446},{\"end\":52464,\"start\":52459},{\"end\":52471,\"start\":52470},{\"end\":52486,\"start\":52481},{\"end\":52502,\"start\":52496},{\"end\":52894,\"start\":52890},{\"end\":52907,\"start\":52900},{\"end\":52928,\"start\":52921},{\"end\":52943,\"start\":52936},{\"end\":53374,\"start\":53366},{\"end\":53386,\"start\":53380},{\"end\":53398,\"start\":53391},{\"end\":53414,\"start\":53406},{\"end\":53430,\"start\":53422},{\"end\":53445,\"start\":53438},{\"end\":53457,\"start\":53451},{\"end\":53459,\"start\":53458},{\"end\":53480,\"start\":53471},{\"end\":53998,\"start\":53993},{\"end\":54013,\"start\":54012},{\"end\":54027,\"start\":54023},{\"end\":54043,\"start\":54037},{\"end\":54059,\"start\":54052},{\"end\":54070,\"start\":54064},{\"end\":54582,\"start\":54575},{\"end\":54600,\"start\":54593},{\"end\":54613,\"start\":54607},{\"end\":54615,\"start\":54614},{\"end\":54631,\"start\":54623},{\"end\":55094,\"start\":55093},{\"end\":55101,\"start\":55100},{\"end\":55112,\"start\":55111},{\"end\":55124,\"start\":55123},{\"end\":55134,\"start\":55133},{\"end\":55472,\"start\":55469},{\"end\":55486,\"start\":55479},{\"end\":55498,\"start\":55492},{\"end\":55512,\"start\":55506},{\"end\":55528,\"start\":55519},{\"end\":56024,\"start\":56018},{\"end\":56038,\"start\":56032},{\"end\":56298,\"start\":56296},{\"end\":56312,\"start\":56306},{\"end\":56321,\"start\":56318},{\"end\":56335,\"start\":56328},{\"end\":56351,\"start\":56340},{\"end\":56361,\"start\":56358},{\"end\":56373,\"start\":56366},{\"end\":56392,\"start\":56384},{\"end\":56407,\"start\":56401},{\"end\":56808,\"start\":56806},{\"end\":56823,\"start\":56816},{\"end\":56840,\"start\":56834},{\"end\":57203,\"start\":57197},{\"end\":57216,\"start\":57209},{\"end\":57231,\"start\":57222},{\"end\":57246,\"start\":57237},{\"end\":57262,\"start\":57253},{\"end\":57755,\"start\":57748},{\"end\":57766,\"start\":57761},{\"end\":57778,\"start\":57773},{\"end\":57791,\"start\":57786},{\"end\":57804,\"start\":57797},{\"end\":58204,\"start\":58198},{\"end\":58213,\"start\":58209},{\"end\":58221,\"start\":58218},{\"end\":58234,\"start\":58228},{\"end\":58245,\"start\":58242},{\"end\":58602,\"start\":58596},{\"end\":58615,\"start\":58609},{\"end\":59009,\"start\":59002},{\"end\":59025,\"start\":59017},{\"end\":59041,\"start\":59033},{\"end\":59053,\"start\":59049},{\"end\":59069,\"start\":59065},{\"end\":59085,\"start\":59079},{\"end\":59379,\"start\":59372},{\"end\":59390,\"start\":59386}]", "bib_author_last_name": "[{\"end\":32383,\"start\":32375},{\"end\":32399,\"start\":32394},{\"end\":32412,\"start\":32409},{\"end\":32432,\"start\":32422},{\"end\":32452,\"start\":32443},{\"end\":32721,\"start\":32719},{\"end\":32739,\"start\":32734},{\"end\":32755,\"start\":32752},{\"end\":33013,\"start\":33005},{\"end\":33029,\"start\":33022},{\"end\":33045,\"start\":33041},{\"end\":33064,\"start\":33054},{\"end\":33083,\"start\":33075},{\"end\":33633,\"start\":33625},{\"end\":33652,\"start\":33641},{\"end\":33668,\"start\":33661},{\"end\":33681,\"start\":33677},{\"end\":34208,\"start\":34206},{\"end\":34220,\"start\":34218},{\"end\":34239,\"start\":34229},{\"end\":34255,\"start\":34247},{\"end\":34273,\"start\":34262},{\"end\":34759,\"start\":34755},{\"end\":34774,\"start\":34767},{\"end\":34786,\"start\":34783},{\"end\":34802,\"start\":34796},{\"end\":34813,\"start\":34808},{\"end\":35279,\"start\":35273},{\"end\":35451,\"start\":35446},{\"end\":35465,\"start\":35460},{\"end\":35486,\"start\":35476},{\"end\":35498,\"start\":35492},{\"end\":35515,\"start\":35507},{\"end\":35527,\"start\":35522},{\"end\":35538,\"start\":35536},{\"end\":35556,\"start\":35548},{\"end\":35570,\"start\":35565},{\"end\":35580,\"start\":35576},{\"end\":35584,\"start\":35582},{\"end\":35992,\"start\":35988},{\"end\":36009,\"start\":36000},{\"end\":36027,\"start\":36020},{\"end\":36044,\"start\":36038},{\"end\":36341,\"start\":36337},{\"end\":36355,\"start\":36351},{\"end\":36371,\"start\":36365},{\"end\":36387,\"start\":36381},{\"end\":36693,\"start\":36682},{\"end\":36706,\"start\":36702},{\"end\":36719,\"start\":36717},{\"end\":36731,\"start\":36727},{\"end\":36744,\"start\":36740},{\"end\":36754,\"start\":36746},{\"end\":37172,\"start\":37148},{\"end\":37387,\"start\":37383},{\"end\":37397,\"start\":37393},{\"end\":37413,\"start\":37407},{\"end\":37424,\"start\":37422},{\"end\":37432,\"start\":37430},{\"end\":37444,\"start\":37437},{\"end\":37828,\"start\":37808},{\"end\":37843,\"start\":37836},{\"end\":37858,\"start\":37852},{\"end\":37876,\"start\":37867},{\"end\":37886,\"start\":37878},{\"end\":38229,\"start\":38209},{\"end\":38244,\"start\":38237},{\"end\":38259,\"start\":38253},{\"end\":38277,\"start\":38268},{\"end\":38287,\"start\":38279},{\"end\":38665,\"start\":38662},{\"end\":38673,\"start\":38671},{\"end\":38692,\"start\":38686},{\"end\":39074,\"start\":39072},{\"end\":39087,\"start\":39084},{\"end\":39096,\"start\":39093},{\"end\":39111,\"start\":39107},{\"end\":39126,\"start\":39122},{\"end\":39141,\"start\":39134},{\"end\":39154,\"start\":39151},{\"end\":39521,\"start\":39518},{\"end\":39530,\"start\":39528},{\"end\":39540,\"start\":39536},{\"end\":39554,\"start\":39549},{\"end\":39562,\"start\":39560},{\"end\":39572,\"start\":39568},{\"end\":39865,\"start\":39857},{\"end\":39881,\"start\":39876},{\"end\":39897,\"start\":39890},{\"end\":40017,\"start\":40009},{\"end\":40033,\"start\":40028},{\"end\":40049,\"start\":40042},{\"end\":40361,\"start\":40354},{\"end\":40372,\"start\":40368},{\"end\":40390,\"start\":40381},{\"end\":40572,\"start\":40565},{\"end\":40588,\"start\":40582},{\"end\":40600,\"start\":40592},{\"end\":40605,\"start\":40602},{\"end\":40614,\"start\":40609},{\"end\":40631,\"start\":40624},{\"end\":40638,\"start\":40633},{\"end\":41253,\"start\":41248},{\"end\":41267,\"start\":41261},{\"end\":41282,\"start\":41274},{\"end\":41711,\"start\":41704},{\"end\":41729,\"start\":41720},{\"end\":41932,\"start\":41930},{\"end\":41950,\"start\":41942},{\"end\":42109,\"start\":42107},{\"end\":42124,\"start\":42119},{\"end\":42138,\"start\":42135},{\"end\":42148,\"start\":42145},{\"end\":42585,\"start\":42582},{\"end\":42598,\"start\":42596},{\"end\":42613,\"start\":42610},{\"end\":42624,\"start\":42621},{\"end\":42638,\"start\":42635},{\"end\":42895,\"start\":42892},{\"end\":42911,\"start\":42907},{\"end\":42927,\"start\":42922},{\"end\":43220,\"start\":43217},{\"end\":43231,\"start\":43229},{\"end\":43246,\"start\":43241},{\"end\":43263,\"start\":43254},{\"end\":43281,\"start\":43274},{\"end\":43292,\"start\":43287},{\"end\":43715,\"start\":43712},{\"end\":43728,\"start\":43725},{\"end\":43739,\"start\":43736},{\"end\":43750,\"start\":43746},{\"end\":44101,\"start\":44093},{\"end\":44111,\"start\":44109},{\"end\":44555,\"start\":44551},{\"end\":44571,\"start\":44564},{\"end\":44587,\"start\":44582},{\"end\":44605,\"start\":44597},{\"end\":44621,\"start\":44614},{\"end\":44637,\"start\":44630},{\"end\":44649,\"start\":44644},{\"end\":44662,\"start\":44657},{\"end\":44679,\"start\":44672},{\"end\":45218,\"start\":45212},{\"end\":45233,\"start\":45228},{\"end\":45243,\"start\":45239},{\"end\":45255,\"start\":45248},{\"end\":45642,\"start\":45632},{\"end\":45842,\"start\":45840},{\"end\":45852,\"start\":45848},{\"end\":45866,\"start\":45861},{\"end\":45876,\"start\":45872},{\"end\":46207,\"start\":46205},{\"end\":46215,\"start\":46213},{\"end\":46239,\"start\":46237},{\"end\":46249,\"start\":46245},{\"end\":46266,\"start\":46258},{\"end\":46285,\"start\":46279},{\"end\":46591,\"start\":46589},{\"end\":46603,\"start\":46601},{\"end\":46621,\"start\":46610},{\"end\":46640,\"start\":46630},{\"end\":46661,\"start\":46651},{\"end\":46998,\"start\":46992},{\"end\":47009,\"start\":47006},{\"end\":47029,\"start\":47019},{\"end\":47039,\"start\":47031},{\"end\":47410,\"start\":47404},{\"end\":47421,\"start\":47418},{\"end\":47441,\"start\":47431},{\"end\":47451,\"start\":47443},{\"end\":47825,\"start\":47822},{\"end\":47840,\"start\":47835},{\"end\":47856,\"start\":47848},{\"end\":47868,\"start\":47864},{\"end\":47883,\"start\":47877},{\"end\":47897,\"start\":47890},{\"end\":47911,\"start\":47905},{\"end\":47931,\"start\":47924},{\"end\":48320,\"start\":48317},{\"end\":48330,\"start\":48327},{\"end\":48339,\"start\":48336},{\"end\":48354,\"start\":48350},{\"end\":48367,\"start\":48363},{\"end\":48786,\"start\":48776},{\"end\":48800,\"start\":48794},{\"end\":49066,\"start\":49057},{\"end\":49083,\"start\":49076},{\"end\":49101,\"start\":49093},{\"end\":49120,\"start\":49113},{\"end\":49136,\"start\":49130},{\"end\":49604,\"start\":49598},{\"end\":49612,\"start\":49606},{\"end\":49823,\"start\":49806},{\"end\":49841,\"start\":49835},{\"end\":49857,\"start\":49852},{\"end\":49871,\"start\":49866},{\"end\":49886,\"start\":49881},{\"end\":50308,\"start\":50300},{\"end\":50324,\"start\":50316},{\"end\":50337,\"start\":50334},{\"end\":50628,\"start\":50624},{\"end\":50638,\"start\":50633},{\"end\":50656,\"start\":50649},{\"end\":50673,\"start\":50665},{\"end\":51145,\"start\":51141},{\"end\":51167,\"start\":51160},{\"end\":51180,\"start\":51173},{\"end\":51196,\"start\":51191},{\"end\":51480,\"start\":51476},{\"end\":51500,\"start\":51489},{\"end\":51515,\"start\":51508},{\"end\":51530,\"start\":51524},{\"end\":51542,\"start\":51540},{\"end\":51558,\"start\":51551},{\"end\":51576,\"start\":51568},{\"end\":51929,\"start\":51918},{\"end\":51947,\"start\":51937},{\"end\":51964,\"start\":51957},{\"end\":51979,\"start\":51971},{\"end\":51996,\"start\":51989},{\"end\":52011,\"start\":52004},{\"end\":52336,\"start\":52328},{\"end\":52457,\"start\":52452},{\"end\":52468,\"start\":52465},{\"end\":52479,\"start\":52472},{\"end\":52494,\"start\":52487},{\"end\":52508,\"start\":52503},{\"end\":52516,\"start\":52510},{\"end\":52898,\"start\":52895},{\"end\":52919,\"start\":52908},{\"end\":52934,\"start\":52929},{\"end\":52949,\"start\":52944},{\"end\":53378,\"start\":53375},{\"end\":53389,\"start\":53387},{\"end\":53404,\"start\":53399},{\"end\":53420,\"start\":53415},{\"end\":53436,\"start\":53431},{\"end\":53449,\"start\":53446},{\"end\":53469,\"start\":53460},{\"end\":53488,\"start\":53481},{\"end\":54010,\"start\":53999},{\"end\":54021,\"start\":54014},{\"end\":54035,\"start\":54028},{\"end\":54050,\"start\":54044},{\"end\":54062,\"start\":54060},{\"end\":54077,\"start\":54071},{\"end\":54083,\"start\":54079},{\"end\":54591,\"start\":54583},{\"end\":54605,\"start\":54601},{\"end\":54621,\"start\":54616},{\"end\":54637,\"start\":54632},{\"end\":55098,\"start\":55095},{\"end\":55109,\"start\":55102},{\"end\":55121,\"start\":55113},{\"end\":55131,\"start\":55125},{\"end\":55143,\"start\":55135},{\"end\":55477,\"start\":55473},{\"end\":55490,\"start\":55487},{\"end\":55504,\"start\":55499},{\"end\":55517,\"start\":55513},{\"end\":55534,\"start\":55529},{\"end\":56030,\"start\":56025},{\"end\":56048,\"start\":56039},{\"end\":56304,\"start\":56299},{\"end\":56316,\"start\":56313},{\"end\":56326,\"start\":56322},{\"end\":56338,\"start\":56336},{\"end\":56356,\"start\":56352},{\"end\":56364,\"start\":56362},{\"end\":56382,\"start\":56374},{\"end\":56399,\"start\":56393},{\"end\":56416,\"start\":56408},{\"end\":56814,\"start\":56809},{\"end\":56832,\"start\":56824},{\"end\":56849,\"start\":56841},{\"end\":57207,\"start\":57204},{\"end\":57220,\"start\":57217},{\"end\":57235,\"start\":57232},{\"end\":57251,\"start\":57247},{\"end\":57268,\"start\":57263},{\"end\":57759,\"start\":57756},{\"end\":57771,\"start\":57767},{\"end\":57784,\"start\":57779},{\"end\":57795,\"start\":57792},{\"end\":57808,\"start\":57805},{\"end\":58207,\"start\":58205},{\"end\":58216,\"start\":58214},{\"end\":58226,\"start\":58222},{\"end\":58240,\"start\":58235},{\"end\":58250,\"start\":58246},{\"end\":58607,\"start\":58603},{\"end\":58618,\"start\":58616},{\"end\":59015,\"start\":59010},{\"end\":59031,\"start\":59026},{\"end\":59047,\"start\":59042},{\"end\":59063,\"start\":59054},{\"end\":59077,\"start\":59070},{\"end\":59088,\"start\":59086},{\"end\":59384,\"start\":59380},{\"end\":59399,\"start\":59391}]", "bib_entry": "[{\"attributes\":{\"doi\":\"Ac- cessed: 2022-03-19. 7\",\"id\":\"b0\"},\"end\":32275,\"start\":32111},{\"attributes\":{\"doi\":\"arXiv:2012.09988\",\"id\":\"b1\"},\"end\":32705,\"start\":32277},{\"attributes\":{\"doi\":\"arXiv:1607.06450\",\"id\":\"b2\"},\"end\":32931,\"start\":32707},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":220249703},\"end\":33503,\"start\":32933},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":232478479},\"end\":34127,\"start\":33505},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":214714109},\"end\":34694,\"start\":34129},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":214743454},\"end\":35198,\"start\":34696},{\"attributes\":{\"id\":\"b7\"},\"end\":35442,\"start\":35200},{\"attributes\":{\"doi\":\"arXiv:1512.03012\",\"id\":\"b8\"},\"end\":35910,\"start\":35444},{\"attributes\":{\"doi\":\"PMLR, 2020. 7\",\"id\":\"b9\",\"matched_paper_id\":211096730},\"end\":36326,\"start\":35912},{\"attributes\":{\"doi\":\"arXiv:1602.02481\",\"id\":\"b10\"},\"end\":36598,\"start\":36328},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":6325059},\"end\":37049,\"start\":36600},{\"attributes\":{\"id\":\"b12\"},\"end\":37324,\"start\":37051},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":57246310},\"end\":37735,\"start\":37326},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":46990252},\"end\":38144,\"start\":37737},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":195699740},\"end\":38570,\"start\":38146},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":6746759},\"end\":39065,\"start\":38572},{\"attributes\":{\"doi\":\"arXiv:2009.09633\",\"id\":\"b17\"},\"end\":39414,\"start\":39067},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":196834610},\"end\":39835,\"start\":39416},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":174802699},\"end\":39987,\"start\":39837},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":174802699},\"end\":40342,\"start\":39989},{\"attributes\":{\"id\":\"b21\"},\"end\":40495,\"start\":40344},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":3656527},\"end\":41034,\"start\":40497},{\"attributes\":{\"id\":\"b23\"},\"end\":41180,\"start\":41036},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":195441339},\"end\":41651,\"start\":41182},{\"attributes\":{\"id\":\"b25\"},\"end\":41875,\"start\":41653},{\"attributes\":{\"id\":\"b26\"},\"end\":42051,\"start\":41877},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":206594692},\"end\":42509,\"start\":42053},{\"attributes\":{\"doi\":\"arXiv:1903.10663\",\"id\":\"b28\"},\"end\":42843,\"start\":42511},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":19285959},\"end\":43146,\"start\":42845},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":3961494},\"end\":43657,\"start\":43148},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":214728050},\"end\":44006,\"start\":43659},{\"attributes\":{\"id\":\"b32\"},\"end\":44481,\"start\":44008},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":56482409},\"end\":45142,\"start\":44483},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":14342571},\"end\":45570,\"start\":45144},{\"attributes\":{\"id\":\"b35\"},\"end\":45729,\"start\":45572},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":4968125},\"end\":46129,\"start\":45731},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":4311592},\"end\":46493,\"start\":46131},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":52841588},\"end\":46944,\"start\":46495},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":152322},\"end\":47356,\"start\":46946},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":152322},\"end\":47768,\"start\":47358},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":14113767},\"end\":48223,\"start\":47770},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":206593370},\"end\":48767,\"start\":48225},{\"attributes\":{\"doi\":\"arXiv:1711.05101\",\"id\":\"b43\"},\"end\":48984,\"start\":48769},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":54465161},\"end\":49553,\"start\":48986},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":1671874},\"end\":49751,\"start\":49555},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":17861456},\"end\":50259,\"start\":49753},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":213175550},\"end\":50552,\"start\":50261},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":5726681},\"end\":51058,\"start\":50554},{\"attributes\":{\"doi\":\"arXiv:1809.09761\",\"id\":\"b49\"},\"end\":51420,\"start\":51060},{\"attributes\":{\"doi\":\"arXiv:2007.08501\",\"id\":\"b50\"},\"end\":51810,\"start\":51422},{\"attributes\":{\"doi\":\"arXiv:2109.00512\",\"id\":\"b51\"},\"end\":52295,\"start\":51812},{\"attributes\":{\"id\":\"b52\"},\"end\":52388,\"start\":52297},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":16619984},\"end\":52820,\"start\":52390},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":6842201},\"end\":53301,\"start\":52822},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":4794860},\"end\":53936,\"start\":53303},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":115147778},\"end\":54483,\"start\":53938},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":73431591},\"end\":55050,\"start\":54485},{\"attributes\":{\"doi\":\"CNS-TR-2011-001\",\"id\":\"b58\"},\"end\":55391,\"start\":55052},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":118646482},\"end\":55942,\"start\":55393},{\"attributes\":{\"doi\":\"arXiv:1711.07888\",\"id\":\"b60\"},\"end\":56231,\"start\":55944},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":10917359},\"end\":56740,\"start\":56233},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":11266650},\"end\":57119,\"start\":56742},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":59523596},\"end\":57647,\"start\":57121},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":1608002},\"end\":58106,\"start\":57649},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":53089436},\"end\":58532,\"start\":58108},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":199442350},\"end\":58948,\"start\":58534},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":53420996},\"end\":59370,\"start\":58950},{\"attributes\":{\"doi\":\"arXiv:1605.04797\",\"id\":\"b68\"},\"end\":59605,\"start\":59372}]", "bib_title": "[{\"end\":32990,\"start\":32933},{\"end\":33619,\"start\":33505},{\"end\":34200,\"start\":34129},{\"end\":34748,\"start\":34696},{\"end\":35265,\"start\":35200},{\"end\":35981,\"start\":35912},{\"end\":36678,\"start\":36600},{\"end\":37377,\"start\":37326},{\"end\":37800,\"start\":37737},{\"end\":38201,\"start\":38146},{\"end\":38651,\"start\":38572},{\"end\":39511,\"start\":39416},{\"end\":39847,\"start\":39837},{\"end\":39999,\"start\":39989},{\"end\":40554,\"start\":40497},{\"end\":41240,\"start\":41182},{\"end\":41920,\"start\":41877},{\"end\":42097,\"start\":42053},{\"end\":42881,\"start\":42845},{\"end\":43208,\"start\":43148},{\"end\":43701,\"start\":43659},{\"end\":44081,\"start\":44008},{\"end\":44539,\"start\":44483},{\"end\":45201,\"start\":45144},{\"end\":45833,\"start\":45731},{\"end\":46195,\"start\":46131},{\"end\":46578,\"start\":46495},{\"end\":46988,\"start\":46946},{\"end\":47400,\"start\":47358},{\"end\":47811,\"start\":47770},{\"end\":48309,\"start\":48225},{\"end\":49050,\"start\":48986},{\"end\":49594,\"start\":49555},{\"end\":49799,\"start\":49753},{\"end\":50292,\"start\":50261},{\"end\":50614,\"start\":50554},{\"end\":52444,\"start\":52390},{\"end\":52888,\"start\":52822},{\"end\":53364,\"start\":53303},{\"end\":53991,\"start\":53938},{\"end\":54573,\"start\":54485},{\"end\":55467,\"start\":55393},{\"end\":56294,\"start\":56233},{\"end\":56804,\"start\":56742},{\"end\":57195,\"start\":57121},{\"end\":57746,\"start\":57649},{\"end\":58196,\"start\":58108},{\"end\":58594,\"start\":58534},{\"end\":59000,\"start\":58950}]", "bib_author": "[{\"end\":32385,\"start\":32370},{\"end\":32401,\"start\":32385},{\"end\":32414,\"start\":32401},{\"end\":32434,\"start\":32414},{\"end\":32454,\"start\":32434},{\"end\":32723,\"start\":32709},{\"end\":32741,\"start\":32723},{\"end\":32757,\"start\":32741},{\"end\":33015,\"start\":32992},{\"end\":33031,\"start\":33015},{\"end\":33047,\"start\":33031},{\"end\":33066,\"start\":33047},{\"end\":33085,\"start\":33066},{\"end\":33635,\"start\":33621},{\"end\":33654,\"start\":33635},{\"end\":33670,\"start\":33654},{\"end\":33683,\"start\":33670},{\"end\":34210,\"start\":34202},{\"end\":34222,\"start\":34210},{\"end\":34241,\"start\":34222},{\"end\":34257,\"start\":34241},{\"end\":34275,\"start\":34257},{\"end\":34761,\"start\":34750},{\"end\":34776,\"start\":34761},{\"end\":34788,\"start\":34776},{\"end\":34804,\"start\":34788},{\"end\":34815,\"start\":34804},{\"end\":35281,\"start\":35267},{\"end\":35453,\"start\":35444},{\"end\":35467,\"start\":35453},{\"end\":35488,\"start\":35467},{\"end\":35500,\"start\":35488},{\"end\":35517,\"start\":35500},{\"end\":35529,\"start\":35517},{\"end\":35540,\"start\":35529},{\"end\":35558,\"start\":35540},{\"end\":35572,\"start\":35558},{\"end\":35582,\"start\":35572},{\"end\":35586,\"start\":35582},{\"end\":35994,\"start\":35983},{\"end\":36011,\"start\":35994},{\"end\":36029,\"start\":36011},{\"end\":36046,\"start\":36029},{\"end\":36343,\"start\":36328},{\"end\":36357,\"start\":36343},{\"end\":36373,\"start\":36357},{\"end\":36389,\"start\":36373},{\"end\":36695,\"start\":36680},{\"end\":36708,\"start\":36695},{\"end\":36721,\"start\":36708},{\"end\":36733,\"start\":36721},{\"end\":36746,\"start\":36733},{\"end\":36756,\"start\":36746},{\"end\":37174,\"start\":37148},{\"end\":37389,\"start\":37379},{\"end\":37399,\"start\":37389},{\"end\":37415,\"start\":37399},{\"end\":37426,\"start\":37415},{\"end\":37434,\"start\":37426},{\"end\":37446,\"start\":37434},{\"end\":37830,\"start\":37802},{\"end\":37845,\"start\":37830},{\"end\":37860,\"start\":37845},{\"end\":37878,\"start\":37860},{\"end\":37888,\"start\":37878},{\"end\":38231,\"start\":38203},{\"end\":38246,\"start\":38231},{\"end\":38261,\"start\":38246},{\"end\":38279,\"start\":38261},{\"end\":38289,\"start\":38279},{\"end\":38667,\"start\":38653},{\"end\":38675,\"start\":38667},{\"end\":38694,\"start\":38675},{\"end\":39076,\"start\":39067},{\"end\":39089,\"start\":39076},{\"end\":39098,\"start\":39089},{\"end\":39113,\"start\":39098},{\"end\":39128,\"start\":39113},{\"end\":39143,\"start\":39128},{\"end\":39156,\"start\":39143},{\"end\":39523,\"start\":39513},{\"end\":39532,\"start\":39523},{\"end\":39542,\"start\":39532},{\"end\":39556,\"start\":39542},{\"end\":39564,\"start\":39556},{\"end\":39574,\"start\":39564},{\"end\":39867,\"start\":39849},{\"end\":39883,\"start\":39867},{\"end\":39899,\"start\":39883},{\"end\":40019,\"start\":40001},{\"end\":40035,\"start\":40019},{\"end\":40051,\"start\":40035},{\"end\":40363,\"start\":40346},{\"end\":40374,\"start\":40363},{\"end\":40392,\"start\":40374},{\"end\":40574,\"start\":40556},{\"end\":40590,\"start\":40574},{\"end\":40602,\"start\":40590},{\"end\":40607,\"start\":40602},{\"end\":40616,\"start\":40607},{\"end\":40633,\"start\":40616},{\"end\":40640,\"start\":40633},{\"end\":41255,\"start\":41242},{\"end\":41269,\"start\":41255},{\"end\":41284,\"start\":41269},{\"end\":41713,\"start\":41696},{\"end\":41731,\"start\":41713},{\"end\":41934,\"start\":41922},{\"end\":41952,\"start\":41934},{\"end\":42111,\"start\":42099},{\"end\":42126,\"start\":42111},{\"end\":42140,\"start\":42126},{\"end\":42150,\"start\":42140},{\"end\":42587,\"start\":42575},{\"end\":42600,\"start\":42587},{\"end\":42615,\"start\":42600},{\"end\":42626,\"start\":42615},{\"end\":42640,\"start\":42626},{\"end\":42897,\"start\":42883},{\"end\":42913,\"start\":42897},{\"end\":42929,\"start\":42913},{\"end\":43222,\"start\":43210},{\"end\":43233,\"start\":43222},{\"end\":43248,\"start\":43233},{\"end\":43265,\"start\":43248},{\"end\":43283,\"start\":43265},{\"end\":43294,\"start\":43283},{\"end\":43717,\"start\":43703},{\"end\":43730,\"start\":43717},{\"end\":43741,\"start\":43730},{\"end\":43752,\"start\":43741},{\"end\":44103,\"start\":44083},{\"end\":44113,\"start\":44103},{\"end\":44557,\"start\":44541},{\"end\":44573,\"start\":44557},{\"end\":44589,\"start\":44573},{\"end\":44607,\"start\":44589},{\"end\":44623,\"start\":44607},{\"end\":44639,\"start\":44623},{\"end\":44651,\"start\":44639},{\"end\":44664,\"start\":44651},{\"end\":44681,\"start\":44664},{\"end\":45220,\"start\":45203},{\"end\":45235,\"start\":45220},{\"end\":45245,\"start\":45235},{\"end\":45257,\"start\":45245},{\"end\":45644,\"start\":45627},{\"end\":45844,\"start\":45835},{\"end\":45854,\"start\":45844},{\"end\":45868,\"start\":45854},{\"end\":45878,\"start\":45868},{\"end\":46209,\"start\":46197},{\"end\":46217,\"start\":46209},{\"end\":46241,\"start\":46217},{\"end\":46251,\"start\":46241},{\"end\":46268,\"start\":46251},{\"end\":46287,\"start\":46268},{\"end\":46593,\"start\":46580},{\"end\":46605,\"start\":46593},{\"end\":46623,\"start\":46605},{\"end\":46642,\"start\":46623},{\"end\":46663,\"start\":46642},{\"end\":47000,\"start\":46990},{\"end\":47011,\"start\":47000},{\"end\":47031,\"start\":47011},{\"end\":47041,\"start\":47031},{\"end\":47412,\"start\":47402},{\"end\":47423,\"start\":47412},{\"end\":47443,\"start\":47423},{\"end\":47453,\"start\":47443},{\"end\":47827,\"start\":47813},{\"end\":47842,\"start\":47827},{\"end\":47858,\"start\":47842},{\"end\":47870,\"start\":47858},{\"end\":47885,\"start\":47870},{\"end\":47899,\"start\":47885},{\"end\":47913,\"start\":47899},{\"end\":47933,\"start\":47913},{\"end\":48322,\"start\":48311},{\"end\":48332,\"start\":48322},{\"end\":48341,\"start\":48332},{\"end\":48356,\"start\":48341},{\"end\":48369,\"start\":48356},{\"end\":48788,\"start\":48771},{\"end\":48802,\"start\":48788},{\"end\":49068,\"start\":49052},{\"end\":49085,\"start\":49068},{\"end\":49103,\"start\":49085},{\"end\":49122,\"start\":49103},{\"end\":49138,\"start\":49122},{\"end\":49606,\"start\":49596},{\"end\":49614,\"start\":49606},{\"end\":49825,\"start\":49801},{\"end\":49843,\"start\":49825},{\"end\":49859,\"start\":49843},{\"end\":49873,\"start\":49859},{\"end\":49888,\"start\":49873},{\"end\":50310,\"start\":50294},{\"end\":50326,\"start\":50310},{\"end\":50339,\"start\":50326},{\"end\":50630,\"start\":50616},{\"end\":50640,\"start\":50630},{\"end\":50658,\"start\":50640},{\"end\":50675,\"start\":50658},{\"end\":51147,\"start\":51132},{\"end\":51169,\"start\":51147},{\"end\":51182,\"start\":51169},{\"end\":51198,\"start\":51182},{\"end\":51482,\"start\":51468},{\"end\":51502,\"start\":51482},{\"end\":51517,\"start\":51502},{\"end\":51532,\"start\":51517},{\"end\":51544,\"start\":51532},{\"end\":51560,\"start\":51544},{\"end\":51578,\"start\":51560},{\"end\":51931,\"start\":51911},{\"end\":51949,\"start\":51931},{\"end\":51966,\"start\":51949},{\"end\":51981,\"start\":51966},{\"end\":51998,\"start\":51981},{\"end\":52013,\"start\":51998},{\"end\":52338,\"start\":52321},{\"end\":52459,\"start\":52446},{\"end\":52470,\"start\":52459},{\"end\":52481,\"start\":52470},{\"end\":52496,\"start\":52481},{\"end\":52510,\"start\":52496},{\"end\":52518,\"start\":52510},{\"end\":52900,\"start\":52890},{\"end\":52921,\"start\":52900},{\"end\":52936,\"start\":52921},{\"end\":52951,\"start\":52936},{\"end\":53380,\"start\":53366},{\"end\":53391,\"start\":53380},{\"end\":53406,\"start\":53391},{\"end\":53422,\"start\":53406},{\"end\":53438,\"start\":53422},{\"end\":53451,\"start\":53438},{\"end\":53471,\"start\":53451},{\"end\":53490,\"start\":53471},{\"end\":54012,\"start\":53993},{\"end\":54023,\"start\":54012},{\"end\":54037,\"start\":54023},{\"end\":54052,\"start\":54037},{\"end\":54064,\"start\":54052},{\"end\":54079,\"start\":54064},{\"end\":54085,\"start\":54079},{\"end\":54593,\"start\":54575},{\"end\":54607,\"start\":54593},{\"end\":54623,\"start\":54607},{\"end\":54639,\"start\":54623},{\"end\":55100,\"start\":55093},{\"end\":55111,\"start\":55100},{\"end\":55123,\"start\":55111},{\"end\":55133,\"start\":55123},{\"end\":55145,\"start\":55133},{\"end\":55479,\"start\":55469},{\"end\":55492,\"start\":55479},{\"end\":55506,\"start\":55492},{\"end\":55519,\"start\":55506},{\"end\":55536,\"start\":55519},{\"end\":56032,\"start\":56018},{\"end\":56050,\"start\":56032},{\"end\":56306,\"start\":56296},{\"end\":56318,\"start\":56306},{\"end\":56328,\"start\":56318},{\"end\":56340,\"start\":56328},{\"end\":56358,\"start\":56340},{\"end\":56366,\"start\":56358},{\"end\":56384,\"start\":56366},{\"end\":56401,\"start\":56384},{\"end\":56418,\"start\":56401},{\"end\":56816,\"start\":56806},{\"end\":56834,\"start\":56816},{\"end\":56851,\"start\":56834},{\"end\":57209,\"start\":57197},{\"end\":57222,\"start\":57209},{\"end\":57237,\"start\":57222},{\"end\":57253,\"start\":57237},{\"end\":57270,\"start\":57253},{\"end\":57761,\"start\":57748},{\"end\":57773,\"start\":57761},{\"end\":57786,\"start\":57773},{\"end\":57797,\"start\":57786},{\"end\":57810,\"start\":57797},{\"end\":58209,\"start\":58198},{\"end\":58218,\"start\":58209},{\"end\":58228,\"start\":58218},{\"end\":58242,\"start\":58228},{\"end\":58252,\"start\":58242},{\"end\":58609,\"start\":58596},{\"end\":58620,\"start\":58609},{\"end\":59017,\"start\":59002},{\"end\":59033,\"start\":59017},{\"end\":59049,\"start\":59033},{\"end\":59065,\"start\":59049},{\"end\":59079,\"start\":59065},{\"end\":59090,\"start\":59079},{\"end\":59386,\"start\":59372},{\"end\":59401,\"start\":59386}]", "bib_venue": "[{\"end\":33232,\"start\":33167},{\"end\":33832,\"start\":33766},{\"end\":34424,\"start\":34358},{\"end\":34964,\"start\":34898},{\"end\":38835,\"start\":38773},{\"end\":40172,\"start\":40120},{\"end\":40781,\"start\":40719},{\"end\":41433,\"start\":41367},{\"end\":42305,\"start\":42236},{\"end\":43415,\"start\":43363},{\"end\":44262,\"start\":44196},{\"end\":44830,\"start\":44764},{\"end\":45344,\"start\":45327},{\"end\":47162,\"start\":47110},{\"end\":47574,\"start\":47522},{\"end\":48516,\"start\":48451},{\"end\":49279,\"start\":49217},{\"end\":50023,\"start\":49964},{\"end\":50830,\"start\":50761},{\"end\":53072,\"start\":53020},{\"end\":53631,\"start\":53569},{\"end\":54226,\"start\":54164},{\"end\":54780,\"start\":54718},{\"end\":55685,\"start\":55619},{\"end\":57391,\"start\":57339},{\"end\":58767,\"start\":58702},{\"end\":32134,\"start\":32111},{\"end\":32368,\"start\":32277},{\"end\":33165,\"start\":33085},{\"end\":33764,\"start\":33683},{\"end\":34356,\"start\":34275},{\"end\":34896,\"start\":34815},{\"end\":35293,\"start\":35281},{\"end\":35641,\"start\":35602},{\"end\":36103,\"start\":36059},{\"end\":36436,\"start\":36405},{\"end\":36794,\"start\":36756},{\"end\":37146,\"start\":37051},{\"end\":37509,\"start\":37446},{\"end\":37922,\"start\":37888},{\"end\":38312,\"start\":38289},{\"end\":38771,\"start\":38694},{\"end\":39214,\"start\":39172},{\"end\":39608,\"start\":39574},{\"end\":39903,\"start\":39899},{\"end\":40118,\"start\":40051},{\"end\":40717,\"start\":40640},{\"end\":41073,\"start\":41036},{\"end\":41365,\"start\":41284},{\"end\":41694,\"start\":41653},{\"end\":41956,\"start\":41952},{\"end\":42234,\"start\":42150},{\"end\":42573,\"start\":42511},{\"end\":42978,\"start\":42929},{\"end\":43361,\"start\":43294},{\"end\":43821,\"start\":43752},{\"end\":44194,\"start\":44113},{\"end\":44762,\"start\":44681},{\"end\":45325,\"start\":45257},{\"end\":45625,\"start\":45572},{\"end\":45912,\"start\":45878},{\"end\":46303,\"start\":46287},{\"end\":46698,\"start\":46663},{\"end\":47108,\"start\":47041},{\"end\":47520,\"start\":47453},{\"end\":47971,\"start\":47933},{\"end\":48449,\"start\":48369},{\"end\":49215,\"start\":49138},{\"end\":49639,\"start\":49614},{\"end\":49962,\"start\":49888},{\"end\":50377,\"start\":50339},{\"end\":50759,\"start\":50675},{\"end\":51130,\"start\":51060},{\"end\":51466,\"start\":51422},{\"end\":51909,\"start\":51812},{\"end\":52319,\"start\":52297},{\"end\":52586,\"start\":52518},{\"end\":53018,\"start\":52951},{\"end\":53567,\"start\":53490},{\"end\":54162,\"start\":54085},{\"end\":54716,\"start\":54639},{\"end\":55091,\"start\":55052},{\"end\":55617,\"start\":55536},{\"end\":56016,\"start\":55944},{\"end\":56456,\"start\":56418},{\"end\":56908,\"start\":56851},{\"end\":57337,\"start\":57270},{\"end\":57859,\"start\":57810},{\"end\":58275,\"start\":58252},{\"end\":58700,\"start\":58620},{\"end\":59139,\"start\":59090},{\"end\":59466,\"start\":59417}]"}}}, "year": 2023, "month": 12, "day": 17}