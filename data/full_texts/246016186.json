{"id": 246016186, "updated": "2023-10-05 17:43:51.354", "metadata": {"title": "Instant Neural Graphics Primitives with a Multiresolution Hash Encoding", "authors": "[{\"first\":\"Thomas\",\"last\":\"Muller\",\"middle\":[]},{\"first\":\"Alex\",\"last\":\"Evans\",\"middle\":[]},{\"first\":\"Christoph\",\"last\":\"Schied\",\"middle\":[]},{\"first\":\"Alexander\",\"last\":\"Keller\",\"middle\":[]}]", "venue": "ACM Trans. Graph. 41, 4, Article 102 (July 2022), 15 pages", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of ${1920\\!\\times\\!1080}$.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2201.05989", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/tog/MullerESK22", "doi": "10.1145/3528223.3530127"}}, "content": {"source": {"pdf_hash": "60e69982ef2920596c6f31d6fd3ca5e9591f3db6", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2201.05989v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "4a65d8b7002e36c901075e1aedf7a3b275474f51", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/60e69982ef2920596c6f31d6fd3ca5e9591f3db6.txt", "contents": "\nInstant Neural Graphics Primitives with a Multiresolution Hash Encoding\nJuly 2022\n\nNVIDIA, SwitzerlandThomas M\u00fcller \nALEX EVANS\nNVIDIA\nUnited Kingdom\n\nCHRISTOPH SCHIED\nNVIDIA\nUSA\n\nInstant Neural Graphics Primitives with a Multiresolution Hash Encoding\n\nACM Trans. Graph\n414102July 202210.1145/3528223.3530127Publication date: July 2022.ALEXANDER KELLER, NVIDIA, Germany https://nvlabs.github.io/instant-ngp This is the author's version of the work. It is posted here for your personal use. Not for redistribution. The definitive Version of Record was published in ACM Transactions on Graphics, https:// ACM Reference Format: Thomas M\u00fcller, Alex Evans, Christoph Schied, and Alexander Keller. 2022. Instant Neural Graphics Primitives with a Multiresolution Hash Encoding. 102:2 \u2022 M\u00fcller et al.NVIDIA, Z\u00fcrich, Switzerland, tmueller@nvidia comAlex Evans, NVIDIA, London, United Kingdom, alexe@nvidiacomChristoph Schied, NVIDIA, Seattle, USA, cschied@nvidiacomAlexander Keller, NVIDIA, Berlin, Germany, akeller@nvidiacom CCS Concepts: \u2022 Computing methodologies \u2192 Massively parallel algo- rithmsVector / streaming algorithmsNeural networks Additional Key Words and Phrases: Image Synthesis, Neural Networks, En- codings, Hashing, GPUs, Parallel Computation, Function Approximation\nTrained for 1 second 15 seconds 1 second 15 seconds 60 seconds reference Gigapixel image SDF NRC NeRF Fig. 1. We demonstrate instant training of neural graphics primitives on a single GPU for multiple tasks. In Gigapixel image we represent a gigapixel image by a neural network. SDF learns a signed distance function in 3D space whose zero level-set represents a 2D surface. Neural radiance caching (NRC) [M\u00fcller et al. 2021] employs a neural network that is trained in real-time to cache costly lighting calculations. Lastly, NeRF [  uses 2D images and their camera poses to reconstruct a volumetric radiance-and-density field that is visualized using ray marching. In all tasks, our encoding and its efficient implementation provide clear benefits: rapid training, high quality, and simplicity. Our encoding is task-agnostic: we use the same implementation and hyperparameters across all tasks and only vary the hash table size which trades off quality and performance. Photograph \u00a9Trevor Dobson (CC BY-NC-ND 2.0)Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple Authors' addresses: Thomas M\u00fcller,  architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of 1920\u00d71080.\n Fig. 1\n. We demonstrate instant training of neural graphics primitives on a single GPU for multiple tasks. In Gigapixel image we represent a gigapixel image by a neural network. SDF learns a signed distance function in 3D space whose zero level-set represents a 2D surface. Neural radiance caching (NRC) ] employs a neural network that is trained in real-time to cache costly lighting calculations. Lastly, NeRF  uses 2D images and their camera poses to reconstruct a volumetric radiance-and-density field that is visualized using ray marching. In all tasks, our encoding and its efficient implementation provide clear benefits: rapid training, high quality, and simplicity. Our encoding is task-agnostic: we use the same implementation and hyperparameters across all tasks and only vary the hash table size which trades off quality and performance. Photograph \u00a9Trevor Dobson (CC BY-NC-ND 2.0) Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple\n\n\nINTRODUCTION\n\nComputer graphics primitives are fundamentally represented by mathematical functions that parameterize appearance. The quality and performance characteristics of the mathematical representation are crucial for visual fidelity: we desire representations that remain fast and compact while capturing high-frequency, local detail. Functions represented by multi-layer perceptrons (MLPs), used as neural graphics primitives, have been shown to match these criteria (to varying degree), for example as representations of shape [Martel et al. 2021;Park et al. 2019] and radiance fields [Liu et al. 2020;M\u00fcller et al. 2020.\n\nThe important commonality of the these approaches is an encoding that maps neural network inputs to a higher-dimensional space, which is key for extracting high approximation quality from compact models. Most successful among these encodings are trainable, task-specific data structures [Liu et al. 2020;Takikawa et al. 2021] that take on a large portion of the learning task. This enables the use of smaller, more efficient MLPs. However, such data structures rely on heuristics and structural modifications (such as pruning, splitting, or merging) that may complicate the training process, limit the method to a specific task, or limit performance on GPUs where control flow and pointer chasing is expensive.\n\nWe address these concerns with our multiresolution hash encoding, which is adaptive and efficient, independent of the task. It is configured by just two values-the number of parameters and the desired finest resolution max -yielding state-of-the-art quality on a variety of tasks ( Figure 1) after a few seconds of training. Key to both the task-independent adaptivity and efficiency is a multiresolution hierarchy of hash tables: \u2022 Adaptivity: we map a cascade of grids to corresponding fixedsize arrays of feature vectors. At coarse resolutions, there is a 1:1 mapping from grid points to array entries. At fine resolutions, the array is treated as a hash table and indexed using a spatial hash function, where multiple grid points alias each array entry. Such hash collisions cause the colliding training gradients to average, meaning that the largest gradients-those most relevant to the loss function-will dominate. The hash tables thus automatically prioritize the sparse areas with the most important fine scale detail. Unlike prior work, no structural updates to the data structure are needed at any point during training. \u2022 Efficiency: our hash table lookups are O (1) and do not require control flow. This maps well to modern GPUs, avoiding execution divergence and serial pointer-chasing inherent in tree traversals. The hash tables for all resolutions may be queried in parallel.\n\nWe validate our multiresolution hash encoding in four representative tasks (see Figure 1):\n\n(1) Gigapixel image: the MLP learns the mapping from 2D coordinates to RGB colors of a high-resolution image.\n\n(2) Neural signed distance functions (SDF): the MLP learns the mapping from 3D coordinates to the distance to a surface. (3) Neural radiance caching (NRC): the MLP learns the 5D light field of a given scene from a Monte Carlo path tracer. (4) Neural radiance and density fields (NeRF): the MLP learns the 3D density and 5D light field of a given scene from image observations and corresponding perspective transforms.\n\nIn the following, we first review prior neural network encodings (Section 2), then we describe our encoding (Section 3) and its implementation (Section 4), followed lastly by our experiments (Section 5) and discussion thereof (Section 6).\n\n\nBACKGROUND AND RELATED WORK\n\nEarly examples of encoding the inputs of a machine learning model into a higher-dimensional space include the one-hot encoding [Harris and Harris 2013] and the kernel trick [Theodoridis 2008] by which complex arrangements of data can be made linearly separable.\n\nFor neural networks, input encodings have proven useful in the attention components of recurrent architectures [Gehring et al. 2017] and, subsequently, transformers [Vaswani et al. 2017], where they help the neural network to identify the location it is currently processing. Vaswani et al. [2017] encode scalar positions \u2208 R as a multiresolution sequence of \u2208 N sine and cosine functions enc( ) = sin(2 0 ), sin(2 1 ), . . . , sin(2 \u22121 ), cos(2 0 ), cos(2 1 ), . . . , cos(2 \u22121 ) .\n\n(\n\nThis has been adopted in computer graphics to encode the spatiodirectionally varying light field and volume density in the NeRF algorithm ]. The five dimensions of this light field are independently encoded using the above formula; this was later extended to randomly oriented parallel wavefronts ] and level-of-detail filtering [Barron et al. 2021a]. We will refer to this family of encodings as frequency encodings. Notably, frequency encodings followed by a linear transformation have been used in other computer graphics tasks, such as approximating the visibility function [Annen et al. 2007;Jansen and Bavoil 2010]. M\u00fcller et al. [2019; suggested a continuous variant of the one-hot encoding based on rasterizing a kernel, the one-blob encoding, which can achieve more accurate results than frequency encodings in bounded domains at the cost of being single-scale.\n\nParametric encodings. Recently, state-of-the-art results have been achieved by parametric encodings which blur the line between classical data structures and neural approaches. The idea is to arrange additional trainable parameters (beyond weights and biases) in an auxiliary data structure, such as a grid [Chabra et al. 2020;Jiang et al. 2020;Liu et al. 2020;Mehta et al. 2021;Peng et al. 2020a;Sun et al. 2021;Tang et al. 2018;Yu et al. 2021a] or a tree [Takikawa et al. 2021], and to look-up and (optionally) interpolate these parameters depending on the input vector x \u2208 R . This arrangement trades a larger memory footprint for a smaller computational cost: whereas for each gradient propagated backwards through the network, every weight in the fully connected MLP network must be updated, for the trainable input encoding parameters (\"feature vectors\"), only a very small number are affected. For example, with a trilinearly interpolated 3D grid of feature vectors, only 8 such grid points need to be updated for each sample back-propagated to the encoding. In this way, although the total number of parameters is much higher for a parametric encoding than a fixed input encoding, the number of FLOPs and memory accesses required for the update during training is not increased significantly. By reducing the size of the MLP, such parametric models can typically be trained to convergence much faster without sacrificing approximation quality. Another parametric approach uses a tree subdivision of the domain R , wherein a large auxiliary coordinate encoder neural network (ACORN) [Martel et al. 2021] is trained to output dense feature grids in the leaf node around x. These dense feature grids, which have on the order of 10 000 entries, are then linearly interpolated, as in Liu et al. [2020]. This approach tends to yield a larger degree of adaptivity compared with the previous parametric encodings, albeit at greater computational cost which can only be amortized when sufficiently many inputs x fall into each leaf node.\n\nSparse parametric encodings. While existing parametric encodings tend to yield much greater accuracy than their non-parametric predecessors, they also come with downsides in efficiency and versatility. Dense grids of trainable features consume much more memory than the neural network weights. To illustrate the trade-offs and to motivate our method, Figure 2 shows the effect on reconstruction quality of a neural radiance field for several different encodings. Without any input encoding at all (a), the network is only able to learn a fairly smooth function of position, resulting in a poor approximation of the light field. The frequency encoding (b) allows the same moderately sized network (8 hidden layers, each 256 wide) to represent the scene much more accurately. The middle image (c) pairs a smaller network with a dense grid of 128 3 trilinearly interpolated, 16-dimensional feature vectors, for a total of 33.6 million trainable parameters. The large number of trainable parameters can be efficiently updated, as each sample only affects 8 grid points.\n\nHowever, the dense grid is wasteful in two ways. First, it allocates as many features to areas of empty space as it does to those areas near the surface. The number of parameters grows as O ( 3 ), while the visible surface of interest has surface area that grows only as O ( 2 ). In this example, the grid has resolution 128 3 , but only 53 807 (2.57%) of its cells touch the visible surface.\n\nSecond, natural scenes exhibit smoothness, motivating the use of a multi-resolution decomposition [Chibane et al. 2020;Hadadan et al. 2021]. Figure 2 (d) shows the result of using an encoding in which interpolated features are stored in eight co-located grids with resolutions from 16 3 to 173 3 , each containing 2-dimensional feature vectors. These are concatenated to form a 16-dimensional (same as (c)) input to the network. Despite having less than half the number of parameters as (c), the reconstruction quality is similar.\n\nIf the surface of interest is known a priori, a data structure such as an octree [Takikawa et al. 2021] or sparse grid [Chabra et al. 2020;Chibane et al. 2020;Hadadan et al. 2021;Jiang et al. 2020;Liu et al. 2020;Peng et al. 2020a] can be used to cull away the unused features in the dense grid. However, in the NeRF setting, surfaces only emerge during training. NSVF [Liu et al. 2020] and several concurrent works [Sun et al. 2021;Yu et al. 2021a] adopt a multistage, coarse to fine strategy in which regions of the feature grid are progressively refined and culled away as necessary. While effective, this leads to a more complex training process in which the sparse data structure must be periodically updated.\n\nOur method- Figure 2 (e,f)-combines both ideas to reduce waste. We store the trainable feature vectors in a compact spatial hash table, whose size is a hyper-parameter which can be tuned to trade the number of parameters for reconstruction quality. It neither relies on progressive pruning during training nor on a priori knowledge of the geometry of the scene. Analogous to the multi-resolution grid in (d), we use multiple separate hash tables indexed at different resolutions, whose interpolated outputs are concatenated before being passed through the MLP. The reconstruction quality is comparable to the dense grid encoding, despite having 20\u00d7 fewer parameters.\n\nUnlike prior work that used spatial hashing [Teschner et al. 2003] for 3D reconstruction [Nie\u00dfner et al. 2013], we do not explicitly handle collisions of the hash functions by typical means like probing, bucketing, or chaining. Instead, we rely on the neural network to learn to disambiguate hash collisions itself, avoiding control flow divergence, reducing implementation complexity and improving performance. Another performance benefit is the predictable memory layout of the hash tables that is independent of the data that is represented. While good caching behavior is often hard to achieve with tree-like data structures, our hash tables can be fine-tuned for low-level architectural details such as cache size. (1) for a given input coordinate x, we find the surrounding voxels at resolution levels and assign indices to their corners by hashing their integer coordinates.\n\n(2) for all resulting corner indices, we look up the corresponding -dimensional feature vectors from the hash tables and (3) linearly interpolate them according to the relative position of x within the respective -th voxel. (4) we concatenate the result of each level, as well as auxiliary inputs \u2208 R , producing the encoded MLP input \u2208 R + , which (5) is evaluated last. To train the encoding, loss gradients are backpropagated through the MLP (5), the concatenation (4), the linear interpolation (3), and then accumulated in the looked-up feature vectors. \n\n\nMULTIRESOLUTION HASH ENCODING\n\nGiven a fully connected neural network (y; \u03a6), we are interested in an encoding of its inputs y = enc(x; ) that improves the approximation quality and training speed across a wide range of applications without incurring a notable performance overhead. Our neural network not only has trainable weight parameters \u03a6, but also trainable encoding parameters . These are arranged into levels, each containing up to feature vectors with dimensionality . Typical values for these hyperparameters are shown in Table 1. Figure 3 illustrates the steps performed in our multiresolution hash encoding. Each level (two of which are shown as red and blue in the figure) is independent and conceptually stores feature vectors at the vertices of a grid, the resolution of which is chosen to be a geometric progression between the coarsest and finest resolutions [ min , max ]:\n:= min \u00b7 ,(2):= exp ln max \u2212 ln min \u2212 1 .(3)\nmax is chosen to match the finest detail in the training data. Due to the large number of levels , the growth factor is usually small. Our use cases have \u2208 [1.26, 2].\n\nConsider a single level . The input coordinate x \u2208 R is scaled by that level's grid resolution before rounding down and up \u230ax \u230b := \u230ax \u00b7 \u230b, \u2308x \u2309 := \u2308x \u00b7 \u2309. \u230ax \u230b and \u2308x \u2309 span a voxel with 2 integer vertices in Z . We map each corner to an entry in the level's respective feature vector array, which has fixed size of at most . For coarse levels where a dense grid requires fewer than parameters, i.e. ( + 1) \u2264 , this mapping is 1:1. At finer levels, we use a hash function \u210e : Z \u2192 Z to index into the array, effectively treating it as a hash table, although there is no explicit collision handling. We rely instead on the gradient-based optimization to store appropriate sparse detail in the array, and the subsequent neural network (y; \u03a6) for collision resolution. The number of trainable encoding parameters is therefore O ( ) and bounded by \u00b7 \u00b7 which in our case is always \u00b7 16 \u00b7 2 (Table 1).\n\nWe use a spatial hash function [Teschner et al. 2003] of the form\n\u210e(x) = =1 mod ,(4)\nwhere \u2295 denotes the bit-wise XOR operation and are unique, large prime numbers. Effectively, this formula XORs the results of a per-dimension linear congruential (pseudo-random) permutation [Lehmer 1951], decorrelating the effect of the dimensions on the hashed value. Notably, to achieve (pseudo-)independence, only \u2212 1 of the dimensions must be permuted, so we choose 1 := 1 for better cache coherence, 2 = 2 654 435 761, and 3 = 805 459 861.\n\nLastly, the feature vectors at each corner are -linearly interpolated according to the relative position of x within its hypercube, i.e. the interpolation weight is w := x \u2212 \u230ax \u230b.\n\nRecall that this process takes place independently for each of the levels. The interpolated feature vectors of each level, as well as auxiliary inputs \u2208 R (such as the encoded view direction and textures in neural radiance caching), are concatenated to produce y \u2208 R + , which is the encoded input enc(x; ) to the MLP (y; \u03a6).\n\nPerformance vs. quality. Choosing the hash To maintain a roughly equal trainable parameter count, the hash table size is set according to \u00b7 \u00b7 = 2 24 for SDF and NeRF, whereas gigapixel image uses 2 28 . Since ( = 2, = 16) is near the best-case performance and quality (top-left) for all applications, we use this configuration in all results. = 1 is slow on our RTX 3090 GPU since atomic half-precision accumulation is only efficient for 2D vectors but not for scalars. For NeRF and Gigapixel image, training finishes after 31 000 steps whereas SDF completes at 11 000 steps.\n\nfootprint is linear in , whereas quality and performance tend to scale sub-linearly. We analyze the impact of in Figure 4, where we report test error vs. training time for a wide range of -values for three neural graphics primitives. We recommend practitioners to use to tweak the encoding to their desired performance characteristics. The hyperparameters (number of levels) and (number of feature dimensions) also trade off quality and performance, which we analyze for an approximately constant number of trainable encoding parameters in Figure 5. In this analysis, we found ( = 2, = 16) to be a favorable Pareto optimum in all our applications, so we use these values in all other results and recommend them as the default.\n\nImplicit hash collision resolution. It may appear counter-intuitive that this encoding is able to reconstruct scenes faithfully in the presence of hash collisions. Key to its success is that the different resolution levels have different strengths that complement each other. The coarser levels, and thus the encoding as a whole, are injective-that is, they suffer from no collisions at all. However, they can only represent a low-resolution version of the scene, since they offer features which are linearly interpolated from a widely spaced grid of points. Conversely, fine levels can capture small features due to their fine grid resolution, but suffer from many collisions-that is, disparate points which hash to the same table entry. Nearby inputs with equal integer coordinates \u230ax \u230b are not considered a collision; a collision occurs when different integer coordinates hash to the same index. Luckily, such collisions are pseudo-randomly scattered across space, and statistically unlikely to occur simultaneously at every level for a given pair of points.\n\nWhen training samples collide in this way, their gradients average. Consider that the importance to the final reconstruction of such samples is rarely equal. For example, a point on a visible surface of a radiance field will contribute strongly to the reconstructed image (having high visibility and high density, both multiplicatively affecting the magnitude of gradients) causing large changes to its table entries, while a point in empty space that happens to refer to the same entry will have a much smaller weight. As a result, the gradients of the more important samples dominate the collision average and the aliased table entry will naturally be optimized in such a way that it reflects the needs of the higher-weighted point.\n\nThe multiresolution aspect of the hash encoding covers the full range from a coarse resolution min that is guaranteed to be collisionfree to the finest resolution max that the task requires. Thereby, it guarantees that all scales at which meaningful learning could take place are included, regardless of sparsity. Geometric scaling allows covering these scales with only O log ( max / min ) many levels, which allows picking a conservatively large value for max .\n\nOnline adaptivity. Note that if the distribution of inputs x changes over time during training, for example if they become concentrated in a small region, then finer grid levels will experience fewer collisions and a more accurate function can be learned. In other words, the multiresolution hash encoding automatically adapts to the training data distribution, inheriting the benefits of tree-based encodings [Takikawa et al. 2021] without task-specific data structure maintenance that might cause discrete jumps during training. One of our applications, neural radiance caching in Section 5.3, continually adapts to animated viewpoints and 3D content, greatly benefitting from this feature.\n\n-linear interpolation. Interpolating the queried hash table entries ensures that the encoding enc(x; ), and by the chain rule its composition with the neural network (enc(x; ); \u03a6), are continuous. Without interpolation, grid-aligned discontinuities would be present in the network output, which would result in an undesirable blocky appearance. One may desire higher-order smoothness, for example when approximating partial differential equations. A concrete example from computer graphics are signed distance functions, in which case the gradient (enc(x; ); \u03a6)/ x, i.e. the surface normal, would ideally also be continuous. If higher-order smoothness must be guaranteed, we describe a low-cost approach in Appendix A, which we however do not employ in any of our results due to a small decrease in reconstruction quality.\n\n\nIMPLEMENTATION\n\nTo demonstrate the speed of the multiresolution hash encoding, we implemented it in CUDA and integrated it with the fast fully-fused MLPs of the tiny-cuda-nn framework [M\u00fcller 2021]. 1 We release the source code of the multiresolution hash encoding as an update to M\u00fcller [2021] and the source code pertaining to the neural graphics primitives at https://github.com/nvlabs/instant-ngp.\n\nPerformance considerations. In order to optimize inference and backpropagation performance, we store hash table entries at half precision (2 bytes per entry). We additionally maintain a master copy of the parameters in full precision for stable mixed-precision parameter updates, following Micikevicius et al. [2018].\n\nTo optimally use the GPU's caches, we evaluate the hash tables level by level: when processing a batch of input positions, we schedule the computation to look up the first level of the multiresolution hash encoding for all inputs, followed by the second level for all inputs, and so on. Thus, only a small number of consecutive hash tables have to reside in caches at any given time, depending on how much parallelism is available on the GPU. Importantly, this structure of computation automatically makes good use of the available caches and parallelism for a wide range of hash table sizes .\n\nOn our hardware, the performance of the encoding remains roughly constant as long as the hash table size stays below \u2264 2 19 . Beyond this threshold, performance starts to drop significantly; see Figure 4. This is explained by the 6 MB L2 cache of our NVIDIA RTX 3090 GPU, which becomes too small for individual levels when 2 \u00b7 \u00b7 > 6 \u00b7 2 20 , with 2 being the size of a half-precision entry.\n\nThe optimal number of feature dimensions per lookup depends on the GPU architecture. On one hand, a small number favors cache locality in the previously mentioned streaming approach, but on the other hand, a large favors memory coherence by allowing for -wide vector load instructions. = 2 gave us the best cost-quality trade-off (see Figure 5) and we use it in all experiments.\n\nArchitecture. In all tasks, except for NeRF which we will describe later, we use an MLP with two hidden layers that have a width of 64 neurons, rectified linear unit (ReLU) activation functions on their hidden layers, and a linear output layer. The maximum resolution max is set to 2048 \u00d7 scene size for NeRF and signed distance functions, to half of the gigapixel image width, and 2 19 in radiance caching (large value to support close-by objects in expansive scenes).\n\nInitialization. We initialize neural network weights according to Glorot and Bengio [2010] to provide a reasonable scaling of activations and their gradients throughout the layers of the neural network. We initialize the hash table entries using the uniform distribution U (\u221210 \u22124 , 10 \u22124 ) to provide a small amount of randomness while encouraging initial predictions close to zero. We also tried a variety of different distributions, including zero-initialization, which all resulted in a very slightly worse initial convergence speed. The hash table appears to be robust to the initialization scheme.\n\nTraining. We jointly train the neural network weights and the hash table entries by applying Adam [Kingma and Ba 2014], where we set 1 = 0.9, 2 = 0.99, = 10 \u221215 , The choice of 1 and 2 makes only a small difference, but the small value of = 10 \u221215 can significantly accelerate the convergence of the hash table entries when their gradients are sparse and weak. To prevent divergence after long training periods, we apply a weak L2 regularization (factor 10 \u22126 ) to the neural network weights, but not to the hash table entries.\n\nWhen fitting gigapixel images or NeRFs, we use the L 2 loss. For signed distance functions, we use the mean absolute percentage error (MAPE), defined as |prediction \u2212 target | |target | + 0.01 , and for neural radiance caching we use a luminance-relative L 2 loss .\n\nWe observed fastest convergence with a learning rate of 10 \u22124 for signed distance functions and 10 \u22122 otherwise, as well a a batch size of 2 14 for neural radiance caching and 2 18 otherwise.\n\nLastly, we skip Adam steps for hash table entries whose gradient is exactly 0. This saves \u223c 10% performance when gradients are sparse, which is a common occurrence with \u226b BatchSize. Even though this heuristic violates some of the assumptions behind Adam, we observe no degradation in convergence.\n\nNon-spatial input dimensions \u2208 R . The multiresolution hash encoding targets spatial coordinates with relatively low dimensionality. All our experiments operate either in 2D or 3D. However, it is frequently useful to input auxiliary dimensions \u2208 R to the neural network, such as the view direction and material parameters when learning a light field. In such cases, the auxiliary dimensions can be encoded with established techniques whose cost does not scale superlinearly with dimensionality; we use the one-blob encoding [M\u00fcller et al. 2019] in neural radiance caching  and the spherical harmonics basis in NeRF, similar to concurrent work [Verbin et al. 2021;Yu et al. 2021a].\n\nHash \n\n\nEXPERIMENTS\n\nTo highlight the versatility and high quality of the encoding, we compare it with previous encodings in four distinct computer graphics primitives that benefit from encoding spatial coordinates. shown impressive results when fitting very large images-up to a billion pixels-with high fidelity at even the smallest scales. We target our multiresolution hash encoding at the same task and converge to high-fidelity images in seconds to minutes (Figure 4). For comparison, on the Tokyo panorama from Figure 1, ACORN achieves a PSNR of 38.59 dB after 36.9 h of training. With a similar number of parameters ( = 2 24 ), our method achieves the same PSNR after 2.5 minutes of training, peaking at 41.9 dB after 4 min. Figure 6 showcases the level of detail contained in our model for a variety of hash table sizes on another image.\n\n\nGigapixel Image Approximation\n\nIt is difficult to directly compare the performance of our encoding to ACORN; a factor of \u223c 10 stems from our use of fully fused CUDA kernels, provided by the tiny-cuda-nn framework [M\u00fcller 2021]. The input encoding allows for the use of a much smaller MLP than with ACORN, which accounts for much of the remaining 10\u00d7-100\u00d7 speedup. That said, we believe that the biggest value-add of the multiresolution hash encoding is its simplicity. ACORN relies on an adaptive subdivision of the scene as part of a learning curriculum, none of which is necessary with our encoding.\n\n\nSigned Distance Functions\n\nSigned distance functions (SDFs), in which a 3D shape is represented as the zero level-set of a function of position x, are used in many applications including simulation, path planning, 3D modeling, and video games. DeepSDF [Park et al. 2019] uses a large MLP to represent one or more SDFs at a time. In contrast, when just a single SDF needs to be fit, a spatially learned encoding, such as ours can be employed and the MLP shrunk significantly. This is the application we investigate in this section. As baseline, we compare with NGLOD [Takikawa et al. 2021], which achieves state-of-the-art results in both quality and speed by prefixing its small MLP with a lookup from an octree of trainable feature vectors. Lookups along the hierarchy of this octree act similarly to our multiresolution cascade of grids: they are a collision-free analog to our technique, with a fixed growth factor = 2. To allow meaningful comparisons in terms of both performance and quality, we implemented an optimized version of NGLOD in our framework, details of which we describe in Appendix B. Details pertaining to real-time training of SDFs are described in Appendix C.\n\nIn Figure 7, we compare NGLOD with our multiresolution hash encoding at roughly equal parameter count. We also show a straightforward application of the frequency encoding ] to provide a baseline, details of which are found in Appendix D. By using a data structure tailored to the reference shape, NGLOD achieves the highest visual reconstruction quality. However, even without such a dedicated data structure, our encoding approaches a similar fidelity to NGLOD in terms of the intersectionover-union metric (IoU 2 ) with similar performance and memory cost. Furthermore, the SDF is defined everywhere within the training volume, as opposed to NGLOD, which is only defined within the octree (i.e. close to the surface). This permits the use of certain SDF rendering techniques such as approximate soft shadows from a small number of off-surface distance samples [Evans 2006], as shown in the adjacent figure.\n\nTo emphasize differences between the compared methods, we visualize the SDF using a shading model. The resulting colors are sensitive to even slight changes in the surface normal, which emphasizes small fluctuations in the prediction more strongly than in other graphics primitives where color is predicted directly. This sensitivity reveals undesired microstructure in our hash encoding on the scale 2 IoU is the ratio of volumes of the interiors of the intersection and union of the pair of shapes being compared. IoU is always \u2264 1 with a perfect fit corresponding to = 1. We measure IoU by comparing the signs of the SDFs at 128 million points uniformly distributed within the bounding box of the scene.  ]. The MLP enc( ; ); \u03a6 is tasked with predicting photorealistic pixel colors from feature buffers independently for each pixel. The feature buffers contain, among other variables, the world-space position x, which we propose to encode with our method. Neural radiance caching is a challenging application, because it is supervised online during real-time rendering. The training data are a sparse set of light paths that are continually spawned from the camera view. As such, the neural network and encoding do not learn a general mapping from features to color, but rather they continually overfit to the current shape and lighting. To support animated content, training has a budget of one millisecond per frame.  9. Neural radiance caching  gains much improved quality from the multiresolution hash encoding with only a mild performance penalty: 133 versus 147 frames per second at a resolution of 1920\u00d71080px. To demonstrate the online adaptivity of the multiple hash resolutions vs. the prior triangle wave encoding, we show screenshots from a smooth camera motion that starts with a far-away view of the scene (left) and zooms onto a close-by view of an intricate shadow (right). Throughout the camera motion, which takes just a few seconds, the neural radiance cache continually learns from sparse camera paths, enabling the cache to learn (\"overfit\") intricate detail at the scale of the content that the camera is momentarily observing. of the finest grid resolution, which is absent in NGLOD and does not disappear with longer training times. Since NGLOD is essentially a collision-free analog to our hash encoding, we attribute this artifact to hash collisions. Upon close inspection, similar microstructure can be seen in other neural graphics primitives, although with significantly lower magnitude.\n\n\nNeural Radiance Caching\n\nIn neural radiance caching , the task of the MLP is to predict photorealistic pixel colors from feature buffers; see Figure 8. The MLP is run independently for each pixel (i.e. the model is not convolutional), so the feature buffers can be treated as per-pixel feature vectors that contain the 3D coordinate x as well as additional features. We can therefore directly apply our multiresolution hash encoding to x while treating all additional features as auxiliary encoded dimensions to be concatenated with the encoded position, using the same encoding as . We integrated our work into M\u00fcller et al. 's implementation of neural radiance caching and therefore refer to their paper for implementation details.\n\nFor photorealistic rendering, the neural radiance cache is typically queried only for indirect path contributions, which masks its reconstruction error behind the first reflection. In contrast, we would like to emphasize the neural radiance cache's error, and thus the improvement that can be obtained by using our multiresolution hash encoding, so we directly visualize the neural radiance cache at the first path vertex. Figure 9 shows that-compared to the triangle wave encoding of -our encoding results in sharper reconstruction while incurring only a mild performance overhead of 0.7 ms that reduces the frame rate from 147 to 133 FPS at a resolution of 1920 \u00d7 1080px. Notably, the neural radiance cache is trained online-during rendering-from a path tracer that runs in the background, which means that the 0.7 ms overhead includes both training and runtime costs of our encoding.\n\n\nNeural Radiance and Density Fields (NeRF)\n\nIn the NeRF setting, a volumetric shape is represented in terms of a spatial (3D) density function and a spatiodirectional (5D) emission\n\n\nOurs (MLP)\n\nLinear MLP Reference Fig. 11. Feeding the result of our encoding through a linear transformation (no neural network) versus an MLP when learning a NeRF. The models were trained for 1 min. The MLP allows for resolving specular details and reduces the amount of background noise caused by hash collisions. Due to the small size and efficient implementation of the MLP, it is only 15% more expensive-well worth the significantly improved quality.\n\nfunction, which we represent by a similar neural network architecture as . We train the model in the same ways as Mildenhall et al.: by backpropagating through a differentiable ray marcher driven by 2D RGB images from known camera poses.\n\nModel Architecture. Unlike the other three applications, our NeRF model consists of two concatenated MLPs: a density MLP followed by a color MLP ]. The density MLP maps the hash encoded position y = enc(x; ) to 16 output values, the first of which we treat as log-space density. The color MLP adds view-dependent color variation. Its input is the concatenation of \u2022 the 16 output values of the density MLP, and \u2022 the view direction projected onto the first 16 coefficients of the spherical harmonics basis (i.e. up to degree 4). This is a natural frequency encoding over unit vectors.\n\nIts output is an RGB color triplet, for which we use either a sigmoid activation when the training data has low dynamic-range (sRGB) or an exponential activation when it has high dynamic range (linear HDR). We prefer HDR training data due to the closer resemblance to physical light transport. This brings numerous advantages as has also been noted in concurrent work . Informed by the analysis in Figure 10, our results were generated with a 1-hidden-layer density MLP and a 2-hidden-layer color MLP, both 64 neurons wide.\n\nAccelerated ray marching. When marching along rays for both training and rendering, we would like to place samples such that they contribute somewhat uniformly to the image, minimizing wasted computation. Thus, we concentrate samples near surfaces by maintaining an occupancy grid that coarsely marks empty vs. nonempty space. In large scenes, we additionally cascade the occupancy grid and distribute samples exponentially rather than uniformly along the ray. Appendix E describes these procedures in detail.\n\nAt HD resolutions, synthetic and even real-world scenes can be trained in seconds and rendered at 60 FPS, without the need of caching of the MLP outputs [Garbin et al. 2021;Wizadwongsa et al. 2021;Yu et al. 2021b]. This high performance makes it tractable to add effects such as anti-aliasing, motion blur and depth of field by brute-force tracing of multiple rays per pixel, as shown in Figure 12. Table 2. Peak signal to noise ratio (PSNR) of our NeRF implementation with multiresolution hash encoding (\"Ours: Hash\") vs. NeRF ], mip-NeRF [Barron et al. 2021a], and NSVF [Liu et al. 2020], which require \u223chours to train (values taken from the respective papers). To demonstrate the comparatively rapid training of our method, we list its results after training for 1 s to 5 min. For each scene, we mark the methods with least error using gold , silver , and bronze medals. To analyze the degree to which our speedup originates from our optimized implementation vs. from our hash encoding, we also report PSNR for a nearly identical version of our implementation, in which the hash encoding has been replaced by the frequency encoding and the MLP correspondingly enlarged to match  (\"Ours: Frequency\"; details in Appendix D). It approaches NeRF's quality after training for just \u223c 5 min, yet is outperformed by our full method after training for 5 s-15 s, amounting to a 20-60\u00d7 improvement that can be attributed to the hash encoding.  Comparison with direct voxel lookups. Figure 11 shows an ablation where we replace the entire neural network with a single linear matrix multiplication, in the spirit of (although not identical to) concurrent direct voxel-based NeRF [Sun et al. 2021;Yu et al. 2021a]. While the linear layer is capable of reproducing view-dependent effects, the quality is significantly compromised as compared to the MLP, which is better able to capture specular effects and to resolve hash collisions across the interpolated multiresolution hash tables (which manifest as high-frequency artifacts). Fortunately, the MLP is only 15% more expensive than the linear layer, thanks to its small size and efficient implementation.\n\n\nMic\n\nComparison with high-quality offline NeRF. In Table 2, we compare the peak signal to noise ratio (PSNR) our NeRF implementation with multiresolution hash encoding (\"Ours: Hash\") with that of NeRF , mip-NeRF [Barron et al. 2021a], and NSVF [Liu et al. 2020], which all require on the order of hours to train. In contrast, we list results of our method after training for 1 s to 5 min. Our PSNR is competitive with NeRF and NSVF after just 15 s of training, and competitive with mip-NeRF after 1 min to 5 min of training.\n\nOn one hand, our method performs best on scenes with high geometric detail, such as Ficus, Drums, Ship and Lego, achieving the best PSNR of all methods. On the other hand, mip-NeRF and NSVF outperform our method on scenes with complex, view-dependent reflections, such as Materials; we attribute this to the much smaller MLP that we necessarily employ to obtain our speedup of several orders of magnitude over these competing implementations.\n\nNext, we analyze the degree to which our speedup originates from our efficient implementation versus from our encoding. To this end, we additionally report PSNR for a nearly identical version of our implementation: we replace the hash encoding by the frequency encoding and enlarge the MLP to approximately match the architecture of  (\"Ours: Frequency\"); see Appendix D for details. This version of our algorithm approaches NeRF's quality after training for just \u223c 5 min, yet is outperformed by our full method after training for a much shorter duration (5 s-15 s), amounting to a 20-60\u00d7 improvement caused by the hash encoding and smaller MLP.\n\nFor \"Ours: Hash\", the cost of each training step is roughly constant at \u223c6 ms per step. This amounts to 50 k steps after 5 min at which point the model is well converged. We decay the learning rate after 20 k steps by a factor of 0.33, which we repeat every further 10 k steps. In contrast, the larger MLP used in \"Ours: Frequency\" requires \u223c30 ms per training step, meaning that the PSNR listed after 5 min corresponds to about 10 k steps. It could thus keep improving slightly if trained for extended periods of time, as in the offline NeRF variants that are often trained for several 100 k steps.\n\nWhile we isolated the performance and convergence impact of our hash encoding and its small MLP, we believe an additional study is required to quantify the impact of advanced ray marching schemes (such as ours, coarse-fine ], or DONeRF [Neff et al. 2021]) independently from the encoding and network architecture. We report additional information in Section E.3 to aid in such an analysis. \n\n\nDISCUSSION AND FUTURE WORK\n\nConcatenation vs. reduction. At the end of the encoding, we concatenate rather than reduce (for example, by summing) the -dimensional feature vectors obtained from each resolution. We prefer concatenation for two reasons. First, it allows for independent, fully parallel processing of each resolution. Second, a reduction of the dimensionality of the encoded result y from to may be too small to encode useful information. While could be increased proportionally, it would make the encoding much more expensive.\n\nHowever, we recognize that there may be applications in which reduction is favorable, such as when the neural network is significantly more expensive than the encoding, in which case the added computational cost of increasing could be insignificant. We thus argue for concatenation by default and not as a hard-and-fast rule. In our applications, concatenation, coupled with = 2 always yielded by far the best results.\n\nChoice of hash function. A good hash function is efficient to compute, leads to coherent look-ups, and uniformly covers the feature vector array regardless of the structure of query points. We chose our hash function for its good mixture of these properties and also experimented with three others:\n\n(1) The PCG32 [O'Neill 2014] RNG, which has superior statistical properties. Unfortunately, it did not yield a higher-quality reconstruction, making its higher cost not worthwhile.\n\n(2) Ordering the least significant bits of Z by a space-filling curve and only hashing the higher bits. This leads to better look-up coherence at the cost of worse reconstruction quality. However, the speed-up is only marginally better than setting 1 := 1 as done in our hash, and is thus not worth the reduced quality. (3) Even better coherence can be achieved by treating the hash function as a tiling of space into dense grids. Like (2), the speedup is small in practice with significant detriment to quality.\n\nAlternatively to hand-crafted hash functions, it is conceivable to optimize the hash function in future work, turning the method into a dictionary-learning approach. Two possible avenues are (1) developing a continuous formulation of indexing that is amenable to analytic differentiation or (2) applying an evolutionary optimization algorithm that can efficiently explore the discrete function space.\n\nMicrostructure due to hash collisions. The salient artifact of our encoding is a small amount of \"grainy\" microstructure, most visible on the learned signed distance functions (Figure 1 and Figure 7). The graininess is a result of hash collisions that the MLP is unable to fully compensate for. We believe that the key to achieving stateof-the-art quality on SDFs with our encoding will be to find a way to overcome this microstructure, for example by filtering hash table lookups or by imposing an additional smoothness prior on the loss.\n\nGenerative setting. Parametric input encodings, when used in a generative setting, typically arrange their features in a dense grid which can then be populated by a separate generator network, typically a CNN such as StyleGAN DeVries et al. 2021;Peng et al. 2020b]. Our hash encoding adds an additional layer of complexity, as the features are not arranged in a regular pattern through the input domain; that is, the features are not bijective with a regular grid of points. We leave it to future work to determine how best to overcome this difficulty.\n\nOther applications. We are interested in applying the multiresolution hash encoding to other low-dimensional tasks that require accurate, high-frequency fits. The frequency encoding originated from the attention mechanism of transformer networks [Vaswani et al. 2017]. We hope that parametric encodings such as ours can lead to a meaningful improvement in general, attention-based tasks.\n\nHeterogenous volumetric density fields, such as cloud and smoke stored in a VDB [Museth 2013[Museth , 2021 data structure, often include empty space on the outside, a solid core on the inside, and sparse detail on the volumetric surface. This makes them a good fit for our encoding. In the code released alongside this paper, we have included a preliminary implementation that fits a radiance and density field directly from the noisy output of a volumetric path tracer. The initial results are promising, as shown in Figure 13, and we intend to pursue this direction further in future work.\n\n\nCONCLUSION\n\nMany graphics problems rely on task specific data structures to exploit the sparsity or smoothness of the problem at hand. Our multi-resolution hash encoding provides a practical learning-based alternative that automatically focuses on relevant detail, independent of the task. Its low overhead allows it to be used even in timeconstrained settings like online training and inference. In the context of neural network input encodings, it is a drop-in replacement, for example speeding up NeRF by several orders of magnitude and matching the performance of concurrent non-neural 3D reconstruction techniques.\n\nSlow computational processes in any setting, from lightmap baking to the training of neural networks, can lead to frustrating workflows due to long iteration times [Enderton and Wexler 2011]. We have demonstrated that single-GPU training times measured in seconds are within reach for many graphics applications, allowing neural approaches to be applied where previously they may have been discounted.\n\nvanishes at 0 and at 1, causing the discontinuity in the derivatives of the encoding to vanish by the chain rule. The encoding thus becomes 1 -smooth.\n\nHowever, by this trick, we have merely traded discontinuities for zero-points in the individual levels which are not necessarily more desirable. So, we offset each level by half of its voxel size 1/(2 ), which prevents the zero derivatives from aligning across all levels.\n\nThe encoding is thus able to learn smooth, non-zero derivatives for all spatial locations x.\n\nFor higher-order smoothness, higher-order smoothstep functions can be used at small additional cost. In practice, the computational cost of the 1st order smoothstep function 1 is hidden by memory bottlenecks, making it essentially free. However, the reconstruction quality tends to decrease as higher-order interpolation is used. This is why we do not use it by default. Future research is needed to explain the loss of quality.\n\n\nB IMPLEMENTATION DETAILS OF NGLOD\n\nWe designed our implementation of NGLOD [Takikawa et al. 2021] such that it closely resembles that of our hash encoding, only differing in the underlying data structure; i.e. using the vertices of an octree around ground-truth triangle mesh to store collision-free feature vectors, rather than relying on hash tables. This results in a notable difference to the original NGLOD: the looked-up feature vectors are concatenated rather than summed, which in our implementation serendipitously resulted in higher reconstruction quality compared to the summation of an equal number of trainable parameters.\n\nThe octree implies a fixed growth factor = 2, which leads to a smaller number of levels than our hash encoding. We obtained the most favorable performance vs. quality trade-off at a roughly equal number of trainable parameters as our method, through the following configuration:\n\n(1) the number of feature dimensions per entry is = 8, (2) the number of levels is = 10, and (3) look-ups start at level = 4.\n\nThe last point is important for two reasons: first, it matches the coarsest resolution of our hash tables 2 4 = 16 = min , and second, it prevents a performance bottleneck that would arise when all threads of the GPU atomically accumulate gradients in few, coarse entries. We experimentally verified that this does not lead to reduced quality, compared to looking up the entire hierarchy.\n\n\nC REAL-TIME SDF TRAINING DATA GENERATION\n\nIn order to not bottleneck our SDF training, we must be able to generate a large number of ground truth signed distances to highresolution meshes very quickly (\u223cmillions per second).\n\n\nC.1 Efficient Sampling of 3D Training Positions\n\nSimilar to prior work [Takikawa et al. 2021], we distribute some (1/8th) of our training positions uniformly in the unit cube, some (4/8ths) uniformly on the surface of the mesh, and the remainder (3/8ths) perturbed from the surface of the mesh.\n\nThe uniform samples in the unit cube are trivial to generate using any pseudorandom number generator; we use a GPU implementation of PCG32 [O'Neill 2014].\n\nTo generate the uniform samples on the surface of the mesh, we compute the area of each triangle in a preprocessing step, normalize the areas to represent a probability distribution, and store the corresponding cumulative distribution function (CDF) in an array. Then, for each sample, we select a triangle proportional to its area by the inversion method-a binary search of a uniform random number over the CDF array-and sample a uniformly random position on that triangle by standard sample warping [Pharr et al. 2016].\n\nLastly, for those surface samples that must be perturbed, we add a random 3D vector, each dimension independently drawn from a logistic distribution (similar shape to a Gaussian, but cheaper to compute) with standard deviation /1024, where is the bounding radius of the mesh.\n\nOctree sampling for NGLOD. When training our implementation of Takikawa et al. [2021], we must be careful to rarely generate training positions outside of octree leaf nodes. To this end, we replace the uniform unit cube sampling routine with one that creates uniform 3D positions in the leaf nodes of the octree by first rejection sampling a uniformly random leaf node from the array of all nodes and then generating a uniform random position within the node's voxel. Fortunately, the standard deviation /1024 of our logistic perturbation is small enough to almost never leave the octree, so we do not need to modify the surface sampling routine.\n\n\nC.2 Efficient Signed Distances to the Triangle Mesh\n\nFor each sampled 3D position x, we must compute the signed distance to the triangle mesh. To this end, we first construct a triangle bounding volume hierarchy (BVH) with which we perform efficient unsigned distance queries; O log triangles on average.\n\nNext, we sign these distances by tracing 32 \"stab rays\" [Nooruddin and Turk 2003], which we distribute uniformly over the sphere using a Fibonacci lattice that is pseudorandomly and independently offset for every training position. If any of these rays reaches infinity, the corresponding position x is deemed \"outside\" of the object and the distance is marked positive. Otherwise, it is marked negative. 3 For maximum efficiency, we use NVIDIA ray tracing hardware through the OptiX 7 framework, which is over an order of magnitude faster than using the previously mentioned triangle BVH for rayshape intersections on our RTX 3090 GPU.\n\n\nD BASELINE MLPS WITH FREQUENCY ENCODING\n\nIn our signed distance function (SDF), neural radiance caching (NRC), and neural radiance and density fields (NeRF) experiments, we use an MLP prefixed by a frequency encoding as baseline. The respective architectures are equal to those in the main text, except that the MLPs are larger and that the hash encoding is replaced by sine and cosine waves (SDF and NeRF) or triangle waves (NRC). The following table lists the number of hidden layers, neurons per hidden layer, frequency cascades (each scaled by a factor of 2 as per Vaswani et al. [2017]), and adjusted learning rates. (2) we perturb training samples with a standard deviation of /128 as opposed to the value of /1024 from Appendix C.1. Both changes smooth the loss landscape, resulting in a better reconstruction with the above configuration. Notably, even though the above configurations have fewer parameters and are slower than our configurations with hash encoding, they represent favorable performance vs. quality trade-offs. An equal parameter count comparison would make pure MLPs too expensive due to their scaling with O ( 2 ) as opposed to the sub-linear scaling of trainable encodings. On the other hand, an equal throughput comparison would require prohibitively small MLPs, thus underselling the reconstruction quality that pure MLPs are capable of.\n\nWe also experimented with Fourier features ] but did not obtain better results compared to the axis-aligned frequency encodings mentioned previously.\n\n\nE ACCELERATED NERF RAY MARCHING\n\nThe performance of ray marching algorithms such as NeRF strongly depends on the marching scheme. We utilize three techniques with imperceivable error to optimize our implementation:\n\n(1) exponential stepping for large scenes, (2) skipping of empty space and occluded regions, and (3) compaction of samples into dense buffers for efficient execution.\n\n\nE.1 Ray Marching Step Size and Stopping\n\nIn synthetic NeRF scenes, which we bound to the unit cube [0, 1] 3 , we use a fixed ray marching step size equal to \u0394 := \u221a 3/1024; \u221a 3 represents the diagonal of the unit cube.\n\nIn all other scenes, based on the intercept theorem 4 , we set the step size proportional to the distance along the ray \u0394 := /256, clamped to the interval \u221a 3/1024, \u00b7 \u221a 3/1024 , where is size of the largest axis of the scene's bounding box. This choice of step size exhibits exponential growth in , which means that the computation cost grows only logarithmically in scene diameter, with no perceivable loss of quality.\n\nLastly, we stop ray marching and set the remaining contribution to zero as soon as the transmittance of the ray drops below a threshold; in our case = 10 \u22124 .\n\nRelated work. Mildenhall et al. [2019] already identified a nonlinear step size as benefitial: they recommend sampling uniformly in the disparity-space of the average camera frame, which is more aggressive than our exponential stepping, requiring on one hand only a constant number of steps, but on the other hand can lead to a loss of fidelity compared to exponential stepping [Neff et al. 2021].\n\nIn addition to non-linear stepping, some prior methods propose to warp the 3D domain of the scene towards the origin, thereby improving the numerical properties of their input encodings [Barron et al. 2021b;Neff et al. 2021]. This causes rays to curve, which leads to a worse reconstruction in our implementation.\n\nIn contrast, we linearly map input coordinates into the unit cube before feeding them to our hash encoding, relying on its exponential multiresolution growth to reach a proportionally scaled maximum resolution max with a constant number of levels (variable as in Equation (3)) or logarithmically many levels (constant ).\n\n\nE.2 Occupancy Grids\n\nTo skip ray marching steps in empty space, we maintain a cascade of multiscale occupancy grids, where = 1 for all synthetic NeRF scenes (single grid) and \u2208 [1, 5] for larger real-world scenes (up to 5 grids, depending on scene size). Each grid has a resolution of 128 3 , spanning a geometrically growing domain [\u22122 \u22121 + 0.5, 2 \u22121 + 0.5] 3 that is centered around (0.5, 0.5, 0.5).\n\nEach grid cell stores occupancy as a single bit. The cells are laid out in Morton (z-curve) order to facilitate memory-coherent traversal by a digital differential analyzer (DDA). During ray marching, whenever a sample is to be placed according to the step size from the previous section, the sample is skipped if its grid cell's bit is low.\n\nWhich one of the grids is queried is determined by both the sample position x and the step size \u0394 : among the grids covering x, the finest one with cell side-length larger than \u0394 is queried.\n\nUpdating the occupancy grids. To continually update the occupancy grids while training, we maintain a second set of grids that have the same layout, except that they store full-precision floating point density values rather than single bits.\n\nWe update the grids after every 16 training iterations by performing the following steps. We\n\n(1) decay the density value in each grid cell by a factor of 0.95, (2) randomly sample candidate cells, and set their value to the maximum of their current value and the density component of the NeRF model at a random location within the cell, and (3) update the occupancy bits by thresholding each cell's density with = 0.01 \u00b7 1024/ \u221a 3, which corresponds to thresholding the opacity of a minimal ray marching step by 1 \u2212 exp(\u22120.01) \u2248 0.01.\n\nThe sampling strategy of the candidate cells depends on the training progress since the occupancy grid does not store reliable information in early iterations. During the first 256 training steps, we sample = \u00b7 128 3 cells uniformly without repetition. For subsequent training steps we set = \u00b7 128 3 /2 which we partition into two sets. The first /2 cells are sampled uniformly among all cells. Rejection sampling is used for the remaining samples to restrict selection to cells that are currently occupied.\n\nRelated work. The idea of constraining the MLP evaluation to occupied cells has already been exploited in prior work on trainable, cell-based encodings [Liu et al. 2020;Sun et al. 2021;Yu et al. 2021a,b]. In contrast to these papers, our occupancy grid is independent from the learned encoding, allowing us to represent it more compactly as a bitfield (and thereby at a resolution that is decoupled from that of the encoding) and to utilize it when comparing against other methods that do not have a trained spatial encoding, e.g. \"Ours: Frequency\" in Table 2.\n\nEmpty space can also be skipped by importance sampling the depth distribution, such as by resampling the result of a coarse Table 3. Batch size, number of rays per batch, and number of samples per ray for our full method (\"Ours: Hash\"), our implementation of frequency encoding NeRF (\"Ours: Freq. \") and mip-NeRF. Since the values corresponding to our method vary by scene, we report minimum and maximum values over the synthetic scenes from Table 2. \n\n( 1 )Fig. 3 .\n13Hashing of voxel vertices(2) Lookup (3) Linear interpolation (4) Concatenation (5) Neural network Illustration of the multiresolution hash encoding in 2D.\n\nFig. 10 .\n10The effect of the MLP size on test error vs. training time (31 000 training steps) on the Lego scene. Other scenes behave almost identically. Each curve represents a different MLP depth, where the color MLP has layers hidden layers and the density MLP has 1 hidden layer; we do not observe an improvement with deeper density MLPs. The curves sweep the number of neurons the hidden layers of the density and color MLPs from 16 to 256. Informed by this analysis, we choose layers = 2 and neurons = 64.\n\nFig. 12 .\n12NeRF reconstruction of a modular synthesizer and large natural 360 scene. The left image took 5 seconds to accumulate 128 samples at 1080p on a single RTX 3090 GPU, allowing for brute force defocus effects. The right image was taken from an interactive session running at 10 frames per second on the same GPU.\n\n\nOffline rendered reference (b) Hash (ours), trained for 10 s (c) Path tracer Rendered in 32 ms (2 samples per pixel) Rendered in 32 ms (16 samples per pixel) Fig. 13. Preliminary results of training a NeRF cloud model (b) from real-time path tracing data. Within 32 ms, a 1024\u00d71024 image of our model convincingly approximates the offline rendered ground truth (a). Our model exhibits less noise than a GPU path tracer that ran for an equal amount of time (c). The cloud data is \u00a9Walt Disney Animation Studios (CC BY-SA 3.0)\n\n\nsimilar total number of trainable parameters as the frequency encoding configuration (b) trains over 8\u00d7 faster, due to the sparsity of updates to the parameters and smaller MLP. Increasing the number of parameters (f) further improves reconstruction accuracy without significantly increasing training time.(a) No encoding \n(b) Frequency \n[Mildenhall et al. 2020] \n\n(c) Dense grid \nSingle resolution \n\n(d) Dense grid \nMulti resolution \n\n(e) Hash table (ours) \n= 2 14 \n\n(f) Hash table (ours) \n= 2 19 \n\n411 k + 0 parameters \n438 k + 0 \n10 k + 33.6 M \n10 k + 16.3 M \n10 k + 494 k \n10 k + 12.6 M \n11:28 (mm:ss) / PSNR 18.56 \n12:45 / PSNR 22.90 \n1:09 / PSNR 22.35 \n1:26 / PSNR 23.62 \n1:48 / PSNR 22.61 \n1:47 / PSNR 24.58 \n\nFig. 2. A demonstration of the reconstruction quality of different encodings and parametric data structures for storing trainable feature embeddings. Each \nconfiguration was trained for 11 000 steps using our fast NeRF implementation (Section 5.4), varying only the input encoding and MLP size. The number of \ntrainable parameters (MLP weights + encoding parameters), training time and reconstruction accuracy (PSNR) are shown below each image. Our encoding (e) \nwith a \n\nTable 1 .\n1Hash encoding parameters and their ranges in our results. Only the hash table size and max. resolution max need to be tuned to the task.Parameter \nSymbol \nValue \n\nNumber of levels \n16 \nMax. entries per level (hash table size) \n2 14 to 2 24 \nNumber of feature dimensions per entry \n2 \nCoarsest resolution \n\nmin \n\n16 \nFinest resolution \n\nmax \n\n512 to 524288 \n\n\n\n\ntable size provides a trade-off between performance, memory and quality. Higher values of result in higher quality and lower performance. The memory Fig. 5. Test error over training time for fixed values of feature dimensionality as the number of hash table levels is varied.0 \n100 \n200 \n\n30 \n\n40 \n\n50 \n\n= 2 16 \n\n= 2 19 \n\n= 2 24 \n\nTraining time (seconds) \n\nPSNR (dB) \n\nGigapixel image \n\nPluto \nMars \nTokyo \n\n0 \n50 \n100 \n150 \n\n20 \n\n25 \n\n30 \n\n= 2 14 \n\n= 2 19 \n\n= 2 21 \n\nTraining time (seconds) \n\nMAPE (dB) \n\nSDF \n\nClockwork \nLizard \nBearded Man \n\n0 \n100 \n200 \n300 \n\n30 \n\n35 \n\n= 2 14 \n\n= 2 19 \n= 2 21 \n\nTraining time (seconds) \n\nPSNR (dB) \n\nNeRF \n\nLego \nShip \n\nFig. 4. The main curves plot test error over training time for varying hash table size which determines the number of trainable encoding parameters. \nIncreasing improves reconstruction, at the cost of higher memory usage and slower training and inference. A performance cliff is visible at > 2 19 where \nthe cache of our RTX 3090 GPU becomes oversubscribed (particularly visible for SDF and NeRF). The plot also shows model convergence over time leading up \nto the final state. This highlights how high quality results are already obtained after only a few seconds. Jumps in the convergence (most visible towards the \nend of SDF training) are caused by learning rate decay. For NeRF and Gigapixel image, training finishes after 31 000 steps and for SDF after 11 000 steps. \n\n200 \n300 \n400 \n\n25 \n\n30 \n\n35 \n\n40 \n\n= 2 \n\n= 4 \n\n= 8 \n\n= 16 \n= 32 \n\nTraining time (seconds) \n\nPSNR (dB) \n\nGigapixel image: Tokyo \n\nF=1 \nF=2 \nF=4 \nF=8 \n\n60 \n80 \n100 \n\n20 \n\n21 \n\n22 \n\n= 4 \n\n= 8 \n\n= 16 \n\n= 32 \n\nTraining time (seconds) \n\nMAPE (dB) \n\nSigned Distance Function: Cow \n\nF=1 \nF=2 \nF=4 \nF=8 \n\n200 \n300 \n400 \n500 \n\n33 \n\n34 \n\n35 \n\n36 \n\n= 4 \n\n= 8 \n\n= 16 \n\n= 32 \n\nTraining time (seconds) \n\nPSNR (dB) \n\nNeural Radiance Field: Lego \n\nF=1 \nF=2 \nF=4 \nF=8 \n\n\n\n\nLearning the 2D to RGB mapping of image coordinates to colors has become a popular benchmark for testing a model's ability to represent high-frequency detail[Martel et al. 2021;M\u00fcller et al. 2019;Sitzmann et al. 2020;. Recent breakthroughs in adaptive coordinate networks (ACORN)[Martel et al. 2021] have\n\n\nFig. 7. Neural signed distance functions trained for 11 000 steps. The frequency encoding struggles to capture the sharp details on these intricate models. NGLOD[Takikawa et al. 2021] achieves the highest visual quality, at the cost of only training the SDF inside the cells of a close-fitting octree. Our hash encoding exhibits similar numeric quality in terms of intersection over union (IoU) and can be evaluated anywhere in the scene. However, it also exhibits visually undesirable surface roughness that we attribute to randomly distributed hash collisions. Bearded Man \u00a9Oliver Laric (CC BY-NC-SA 2.0)Hash (ours) \nNGLOD \nHash (ours) \nFrequency \nFrequency \nHash (ours) \nNGLOD \nHash (ours) \n\n22.3 M (params) \n12.2 M \n124.9 k \n124.9 k \n12.2 M \n16.0 M \n1:56 (mm:ss) \n1:14 \n1:32 \n2:10 \n1:54 \n1:49 \n0.9777 (IoU) \n0.9812 \n0.8432 \n0.9898 \n0.9997 \n0.9998 \n\n11.1 M (params) \n12.2 M \n124.9 k \n124.9 k \n12.2 M \n24.2 M \n1:37 (mm:ss) \n1:19 \n1:35 \n1:21 \n1:04 \n1:50 \n0.9911 (IoU) \n0.9872 \n0.8470 \n0.7575 \n0.9691 \n0.9749 \n\nFeature buffers \n\nenc( ; ); \u03a6 \n\nPredicted color \n\nOnline \nsupervised \ntraining \n\nReal-time sparse path tracer \n\nFig. 8. Summary of the neural radiance caching application \n\n\nPrimitive Hidden layers Neurons Frequencies Learning rateFor NeRF, the first listed number corresponds to the density MLP and the second number to the color MLP. For SDFs, we make two additional changes: (1) we optimize against the relative L 2 loss[Lehtinen et al. 2018] instead of the MAPE described in the main text, andSDF \n8 \n128 \n10 \n3 \u00b7 10 \u22124 \nNRC \n3 \n64 \n10 \n10 \u22122 \nNeRF \n7 / 1 256 / 256 \n16 / 4 \n10 \u22123 \n\n\nWe observe speed-ups on the order of 10\u00d7 compared to a na\u00efve Python implementation. We therefore also release PyTorch bindings around our hash encoding and fully fused MLPs to permit their use in existing projects with little overhead.\nACM Trans.Graph., Vol. 41, No. 4, Article 102. Publication date: July 2022. Instant Neural Graphics Primitives with a Multiresolution Hash Encoding \u2022 102:11\n( ) = 2 (3 \u2212 2 ) ,(5)to the -linear interpolation weights. Crucially, the derivative of the smoothstep, \u2032 1 ( ) = 6 (1 \u2212 ) ,(6)\nIf the mesh is watertight, it is cheaper to sign the distance based on the normal(s) of the closest triangle(s) from the previous step. We also implemented this procedure, but disable it by default due to its incompatibility with typical meshes in the wild.\nThe appearance of objects stays the same as long as their size and distance from the observer remain proportional.\nACM Trans. Graph., Vol. 41, No. 4, Article 102. Publication date: July 2022.\nACKNOWLEDGMENTSWe are grateful to Andrew Tao, Andrew Webb, Anjul Patney, David Luebke, Fabrice Rousselle, Jacob Munkberg, James Lucas, Jonathan Granskog, Jonathan Tremblay, Koki Nagano, Marco Salvi, Nikolaus Binder, and Towaki Takikawa for profound discussions, proofreading, feedback, and early testing. We also thank Arman Toorians and Saurabh Jain for the factory robot dataset inFigure 12(right).A SMOOTH INTERPOLATIONOne may desire smoother interpolation than the -linear interpolation that our multiresolution hash encoding uses by default.In this case, the obvious solution would be using a -quadratic or -cubic interpolation, both of which are however very expensive due to requiring the lookup of 3 and 4 instead of 2 vertices, respectively. As a low-cost alternative, we recommend applying the smoothstep function,   or through neural importance sampling[M\u00fcller et al. 2019] as done in DONeRF[Neff et al. 2021].E.3 Number of Rays Versus Batch SizeThe batch size has a significant effect on the quality and speed of NeRF convergence. We found that training from a larger number of rays, i.e. incorporating more viewpoint variation into the batch, converged to lower error in fewer steps. In our implementation where the number of samples per ray is variable due to occupancy, we therefore include as many rays as possible in batches of fixed size rather than building variable-size batches from a fixed ray count.InTable 3, we list ranges of the resulting number of rays per batch and corresponding samples per ray. We use a batch size of 256 Ki, which resulted in the fastest wall-clock convergence in our experiments. This is 4\u00d7 smaller than the batch size chosen in mip-NeRF, likely due to the larger number of samples each of their rays requires. However, due to the myriad other differences across implementations, a more detailed study must be carried out to draw a definitive conclusion.Lastly, we note that the occupancy grid in our frequency-encoding baseline (\"Ours: Freq. \"; Appendix D) produces even fewer samples than when used alongside our hash encoding. This can be explained by the slightly more detailed reconstruction of the hash encoding: when the extra detail is finer than the occupancy grid resolution, its surrounding empty space can not be effectively culled away and must be traversed by extra steps.\nConvolution Shadow Maps. Thomas Annen, Tom Mertens, Philippe Bekaert, Hans-Peter Seidel, 10.2312/EGWR/EGSR07/051-060The Eurographics Association. Kautz and Sumanta PattanaikRendering TechniquesThomas Annen, Tom Mertens, Philippe Bekaert, Hans-Peter Seidel, and Jan Kautz. 2007. Convolution Shadow Maps. In Rendering Techniques, Jan Kautz and Sumanta Pattanaik (Eds.). The Eurographics Association. https://doi.org/10.2312/EGWR/ EGSR07/051-060\n\nJonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, Pratul P Srinivasan, arXiv:2111.12077Dor Verbin, Pratul P. Srinivasan, and Peter Hedman. 2021b. Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields. Ben MildenhallMip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields. arXivJonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin- Brualla, and Pratul P. Srinivasan. 2021a. Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields. arXiv (2021). https://jonbarron.info/mipnerf/ Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hed- man. 2021b. Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields. arXiv:2111.12077 (Nov. 2021).\n\nDeep Local Shapes: Learning Local SDF Priors for Detailed 3D Reconstruction. Rohan Chabra, Jan E Lenssen, Eddy Ilg, Tanner Schmidt, Julian Straub, Steven Lovegrove, Richard Newcombe, Computer Vision -ECCV 2020. Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael FrahmChamSpringer International PublishingRohan Chabra, Jan E. Lenssen, Eddy Ilg, Tanner Schmidt, Julian Straub, Steven Love- grove, and Richard Newcombe. 2020. Deep Local Shapes: Learning Local SDF Priors for Detailed 3D Reconstruction. In Computer Vision -ECCV 2020, Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm (Eds.). Springer International Publishing, Cham, 608-625.\n\nEric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano, Boxiao Pan, Orazio Shalini De Mello, Gallo, arXiv:2112.07945arXiv:2112.07945Tero Karras, and Gordon Wetzstein. 2021. Efficient Geometry-aware 3D Generative Adversarial Networks. Leonidas Guibas, Jonathan Tremblay, Sameh Khamiscs.CVEric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero Karras, and Gordon Wetzstein. 2021. Efficient Geometry-aware 3D Generative Adversarial Networks. arXiv:2112.07945 (2021). arXiv:2112.07945 [cs.CV]\n\nImplicit Functions in Feature Space for 3D Shape Reconstruction and Completion. Julian Chibane, Thiemo Alldieck, Gerard Pons-Moll, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEEJulian Chibane, Thiemo Alldieck, and Gerard Pons-Moll. 2020. Implicit Functions in Feature Space for 3D Shape Reconstruction and Completion. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE.\n\nTerrance Devries, Miguel Angel Bautista, Nitish Srivastava, Graham W Taylor, Joshua M Susskind, Unconstrained Scene Generation with Locally Conditioned Radiance Fields. arXiv. Terrance DeVries, Miguel Angel Bautista, Nitish Srivastava, Graham W. Taylor, and Joshua M. Susskind. 2021. Unconstrained Scene Generation with Locally Condi- tioned Radiance Fields. arXiv (2021).\n\nThe Workflow Scale. Eric Enderton, Daniel Wexler, Computer Graphics International Workshop on VFX, Computer Animation, and Stereo Movies. Eric Enderton and Daniel Wexler. 2011. The Workflow Scale. In Computer Graphics International Workshop on VFX, Computer Animation, and Stereo Movies.\n\nFast Approximations for Global Illumination on Dynamic Scenes. Alex Evans, 10.1145/1185657.1185834ACM SIGGRAPH 2006 Courses. Boston, Massachusetts; New York, NY, USAAssociation for Computing MachinerySIGGRAPH '06)Alex Evans. 2006. Fast Approximations for Global Illumination on Dynamic Scenes. In ACM SIGGRAPH 2006 Courses (Boston, Massachusetts) (SIGGRAPH '06). Association for Computing Machinery, New York, NY, USA, 153-171. https://doi.org/10.1145/ 1185657.1185834\n\nStephan J Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, Julien Valentin, arXiv:2103.10380FastNeRF: High-Fidelity Neural Rendering at 200FPS. Stephan J. Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, and Julien Valentin. 2021. FastNeRF: High-Fidelity Neural Rendering at 200FPS. arXiv:2103.10380 (March 2021).\n\nConvolutional Sequence to Sequence Learning. Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, Yann N Dauphin, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine LearningSydney, NSW, Australia70Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. 2017. Convolutional Sequence to Sequence Learning. In Proceedings of the 34th International Conference on Machine Learning -Volume 70 (Sydney, NSW, Australia) (ICML'17). JMLR.org, 1243--1252.\n\nUnderstanding the Difficulty of Training Deep Feedforward Neural Networks. Xavier Glorot, Yoshua Bengio, Proc. 13th International Conference on Artificial Intelligence and Statistics. 13th International Conference on Artificial Intelligence and StatisticsSardinia, ItalyXavier Glorot and Yoshua Bengio. 2010. Understanding the Difficulty of Training Deep Feedforward Neural Networks. In Proc. 13th International Conference on Artificial Intelligence and Statistics (Sardinia, Italy, May 13-15). JMLR.org, 249-256.\n\nNeural radiosity. Saeed Hadadan, Shuhong Chen, Matthias Zwicker, 10.1145/3478513.3480569ACM Transactions on Graphics. 40Saeed Hadadan, Shuhong Chen, and Matthias Zwicker. 2021. Neural radiosity. ACM Transactions on Graphics 40, 6 (Dec. 2021), 1--11. https://doi.org/10.1145/3478513. 3480569\n\n2013. 3.4.2 -State Encodings. David Money Harris, Sarah L Harris, 10.1016/B978-0-12-394424-5.00002-1Digital Design and Computer Architecture. BostonMorgan Kaufmannsecond ed.David Money Harris and Sarah L. Harris. 2013. 3.4.2 -State Encodings. In Digital Design and Computer Architecture (second ed.). Morgan Kaufmann, Boston, 129-131. https://doi.org/10.1016/B978-0-12-394424-5.00002-1\n\nFourier Opacity Mapping. Jon Jansen, Louis Bavoil, 10.1145/1730804.1730831Proceedings of the 2010 ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games. the 2010 ACM SIGGRAPH Symposium on Interactive 3D Graphics and GamesWashington, D.C.; New York, NY, USAAssociation for Computing MachineryI3D '10)Jon Jansen and Louis Bavoil. 2010. Fourier Opacity Mapping. In Proceedings of the 2010 ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games (Washington, D.C.) (I3D '10). Association for Computing Machinery, New York, NY, USA, 165--172. https://doi.org/10.1145/1730804.1730831\n\nLocal Implicit Grid Representations for 3D Scenes. Max Chiyu, Avneesh Jiang, Ameesh Sud, Jingwei Makadia, Matthias Huang, Thomas Nie\u00dfner, Funkhouser, Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)Chiyu Max Jiang, Avneesh Sud, Ameesh Makadia, Jingwei Huang, Matthias Nie\u00dfner, and Thomas Funkhouser. 2020. Local Implicit Grid Representations for 3D Scenes. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR).\n\nAdam: A Method for Stochastic Optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980Diederik P. Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic Optimization. arXiv:1412.6980 (June 2014).\n\nMathematical Methods in Large-scale Computing Units. H Derrick, Lehmer, Proceedings of the Second Symposium on Large Scale Digital Computing Machinery. the Second Symposium on Large Scale Digital Computing MachineryCambridge, United KingdomHarvard University PressDerrick H. Lehmer. 1951. Mathematical Methods in Large-scale Computing Units. In Proceedings of the Second Symposium on Large Scale Digital Computing Machinery. Harvard University Press, Cambridge, United Kingdom, 141-146.\n\nJaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli Laine, Tero Karras, Miika Aittala, Timo Aila, arXiv:1803.04189Noise2Noise: Learning Image Restoration without Clean Data. Jaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli Laine, Tero Karras, Miika Aittala, and Timo Aila. 2018. Noise2Noise: Learning Image Restoration without Clean Data. arXiv:1803.04189 (March 2018).\n\nLingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. 2020. Neural Sparse Voxel Fields. NeurIPS. Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. 2020. Neural Sparse Voxel Fields. NeurIPS (2020). https://lingjie0206.github.io/papers/ NSVF/\n\nACORN: Adaptive Coordinate Networks for Neural Representation. N P Julien, David B Martel, Connor Z Lindell, Eric R Lin, Marco Chan, Gordon Monteiro, Wetzstein, ACM Trans. Graph. SIGGRAPHJulien N.P. Martel, David B. Lindell, Connor Z. Lin, Eric R. Chan, Marco Monteiro, and Gordon Wetzstein. 2021. ACORN: Adaptive Coordinate Networks for Neural Representation. ACM Trans. Graph. (SIGGRAPH) (2021).\n\nModulated Periodic Activations for Generalizable Local Functional Representations. Ishit Mehta, Micha\u00ebl Gharbi, Connelly Barnes, Eli Shechtman, Ravi Ramamoorthi, Manmohan Chandraker, IEEE International Conference on Computer Vision. IEEEIshit Mehta, Micha\u00ebl Gharbi, Connelly Barnes, Eli Shechtman, Ravi Ramamoorthi, and Manmohan Chandraker. 2021. Modulated Periodic Activations for Generalizable Local Functional Representations. In IEEE International Conference on Computer Vision. IEEE.\n\nPaulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, Hao Wu, arXiv:1710.03740Mixed Precision Training. Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. 2018. Mixed Precision Training. arXiv:1710.03740 (Oct. 2018).\n\nBen Mildenhall, Peter Hedman, Ricardo Martin-Brualla, Pratul Srinivasan, Jonathan T Barron, arXiv:2111.13679NeRF in the Dark: High Dynamic Range View Synthesis from Noisy Raw Images. Ben Mildenhall, Peter Hedman, Ricardo Martin-Brualla, Pratul Srinivasan, and Jonathan T. Barron. 2021. NeRF in the Dark: High Dynamic Range View Synthesis from Noisy Raw Images. arXiv:2111.13679 (Nov. 2021).\n\nLocal Light Field Fusion: Practical View Synthesis with Prescriptive Sampling Guidelines. Ben Mildenhall, P Pratul, Rodrigo Srinivasan, Nima Khademi Ortiz-Cayon, Ravi Kalantari, Ren Ramamoorthi, Abhishek Ng, Kar, 10.1145/3306346.3322980ACM Trans. Graph. 38ArticleBen Mildenhall, Pratul P. Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and Abhishek Kar. 2019. Local Light Field Fusion: Practical View Synthesis with Prescriptive Sampling Guidelines. ACM Trans. Graph. 38, 4, Article 29 (July 2019), 14 pages. https://doi.org/10.1145/3306346.3322980\n\nNeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. Ben Mildenhall, P Pratul, Matthew Srinivasan, Jonathan T Tancik, Ravi Barron, Ren Ramamoorthi, Ng, ECCV. Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ra- mamoorthi, and Ren Ng. 2020. NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. In ECCV.\n\nTiny CUDA Neural Network Framework. Thomas M\u00fcller, Thomas M\u00fcller. 2021. Tiny CUDA Neural Network Framework. https://github.com/nvlabs/tiny-cuda-nn.\n\nNeural Importance Sampling. Thomas M\u00fcller, Brian Mcwilliams, Fabrice Rousselle, Markus Gross, 10.1145/3341156ACM Trans. Graph. 38145Thomas M\u00fcller, Brian McWilliams, Fabrice Rousselle, Markus Gross, and Jan Nov\u00e1k. 2019. Neural Importance Sampling. ACM Trans. Graph. 38, 5, Article 145 (Oct. 2019), 19 pages. https://doi.org/10.1145/3341156\n\nNeural Control Variates. Thomas M\u00fcller, Fabrice Rousselle, Alexander Keller, 10.1145/3414685.3417804ACM Trans. Graph. 39243Thomas M\u00fcller, Fabrice Rousselle, Alexander Keller, and Jan Nov\u00e1k. 2020. Neural Control Variates. ACM Trans. Graph. 39, 6, Article 243 (Nov. 2020), 19 pages. https: //doi.org/10.1145/3414685.3417804\n\nReal-time Neural Radiance Caching for Path Tracing. Thomas M\u00fcller, Fabrice Rousselle, Jan Nov\u00e1k, Alexander Keller, 10.1145/3450626.3459812ACM Trans. Graph. 4036Thomas M\u00fcller, Fabrice Rousselle, Jan Nov\u00e1k, and Alexander Keller. 2021. Real-time Neural Radiance Caching for Path Tracing. ACM Trans. Graph. 40, 4, Article 36 (Aug. 2021), 16 pages. https://doi.org/10.1145/3450626.3459812\n\nVDB: High-Resolution Sparse Volumes with Dynamic Topology. Ken Museth, 10.1145/2487228.2487235ACM Trans. Graph. 32Ken Museth. 2013. VDB: High-Resolution Sparse Volumes with Dynamic Topology. ACM Trans. Graph. 32, 3, Article 27 (July 2013), 22 pages. https://doi.org/10.1145/ 2487228.2487235\n\nNanoVDB: A GPU-Friendly and Portable VDB Data Structure For Real-Time Rendering And Simulation. Ken Museth, 10.1145/3450623.3464653ACM SIGGRAPH 2021 Talks (Virtual Event, USA) (SIGGRAPH '21). New York, NY, USA, Article 1, 2 pagesAssociation for Computing MachineryKen Museth. 2021. NanoVDB: A GPU-Friendly and Portable VDB Data Structure For Real-Time Rendering And Simulation. In ACM SIGGRAPH 2021 Talks (Virtual Event, USA) (SIGGRAPH '21). Association for Computing Machinery, New York, NY, USA, Article 1, 2 pages. https://doi.org/10.1145/3450623.3464653\n\nDONeRF: Towards Real-Time Rendering of Compact Neural Radiance Fields using Depth Oracle Networks. Thomas Neff, Pascal Stadlbauer, Mathias Parger, Andreas Kurz, H Joerg, Chakravarty R Alla Mueller, Anton S Chaitanya, Markus Kaplanyan, Steinberger, 10.1111/cgf.14340Computer Graphics Forum. 404Thomas Neff, Pascal Stadlbauer, Mathias Parger, Andreas Kurz, Joerg H. Mueller, Chakravarty R. Alla Chaitanya, Anton S. Kaplanyan, and Markus Steinberger. 2021. DONeRF: Towards Real-Time Rendering of Compact Neural Radiance Fields using Depth Oracle Networks. Computer Graphics Forum 40, 4 (2021). https://doi.org/10.1111/cgf.14340\n\nReal-Time 3D Reconstruction at Scale Using Voxel Hashing. Matthias Nie\u00dfner, Michael Zollh\u00f6fer, Shahram Izadi, Marc Stamminger, 10.1145/2508363.2508374ACM Trans. Graph. 32169Matthias Nie\u00dfner, Michael Zollh\u00f6fer, Shahram Izadi, and Marc Stamminger. 2013. Real- Time 3D Reconstruction at Scale Using Voxel Hashing. ACM Trans. Graph. 32, 6, Article 169 (nov 2013), 11 pages. https://doi.org/10.1145/2508363.2508374\n\nSimplification and Repair of Polygonal Models Using Volumetric Techniques. S Fakir, Greg Nooruddin, Turk, 10.1109/TVCG.2003.1196006IEEE Transactions on Visualization and Computer Graphics. 92Fakir S. Nooruddin and Greg Turk. 2003. Simplification and Repair of Polygonal Models Using Volumetric Techniques. IEEE Transactions on Visualization and Computer Graphics 9, 2 (apr 2003), 191--205. https://doi.org/10.1109/TVCG.2003.1196006\n\nPCG: A Family of Simple Fast Space-Efficient Statistically Good Algorithms for Random Number Generation. Melissa E O&apos;neill, HMC-CS-2014-0905Technical ReportMelissa E. O'Neill. 2014. PCG: A Family of Simple Fast Space-Efficient Statistically Good Algorithms for Random Number Generation. Technical Report HMC-CS-2014-0905.\n\n. Harvey Mudd College, Claremont, CAHarvey Mudd College, Claremont, CA.\n\nJeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, Steven Lovegrove, arXiv:1901.05103DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation. Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Love- grove. 2019. DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation. arXiv:1901.05103 (Jan. 2019).\n\nSongyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, Andreas Geiger, Convolutional Occupancy Networks. In European Conference on Computer Vision (ECCV). Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, and Andreas Geiger. 2020a. Convolutional Occupancy Networks. In European Conference on Computer Vision (ECCV).\n\nSongyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, Andreas Geiger, arXiv:2003.04618Convolutional Occupancy Networks. (2020). cs.CVSongyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, and Andreas Geiger. 2020b. Convolutional Occupancy Networks. (2020). arXiv:2003.04618 [cs.CV]\n\nPhysically Based Rendering: From Theory to Implementation. Matt Pharr, Jakob Wenzel, Greg Humphreys, Morgan Kaufmann Publishers IncSan Francisco, CA, USA3rd ed.. 3rd ed.. 1266 pagesMatt Pharr, Wenzel Jakob, and Greg Humphreys. 2016. Physically Based Rendering: From Theory to Implementation (3rd ed.) (3rd ed.). Morgan Kaufmann Publishers Inc., San Francisco, CA, USA. 1266 pages.\n\nImplicit Neural Representations with Periodic Activation Functions. Vincent Sitzmann, N P Julien, Alexander W Martel, David B Bergman, Gordon Lindell, Wetzstein, Proc. NeurIPS. NeurIPSVincent Sitzmann, Julien N.P. Martel, Alexander W. Bergman, David B. Lindell, and Gordon Wetzstein. 2020. Implicit Neural Representations with Periodic Activation Functions. In Proc. NeurIPS.\n\nCheng Sun, Min Sun, Hwann-Tzong Chen, arXiv:2111.11215Direct Voxel Grid Optimization: Super-fast Convergence for Radiance Fields Reconstruction. Cheng Sun, Min Sun, and Hwann-Tzong Chen. 2021. Direct Voxel Grid Optimization: Super-fast Convergence for Radiance Fields Reconstruction. arXiv:2111.11215 (Nov. 2021).\n\nNeural Geometric Level of Detail: Real-time Rendering with Implicit 3D Shapes. Towaki Takikawa, Joey Litalien, Kangxue Yin, Karsten Kreis, Charles Loop, Derek Nowrouzezahrai, Alec Jacobson, Morgan Mcguire, Sanja Fidler, Towaki Takikawa, Joey Litalien, Kangxue Yin, Karsten Kreis, Charles Loop, Derek Nowrouzezahrai, Alec Jacobson, Morgan McGuire, and Sanja Fidler. 2021. Neural Geometric Level of Detail: Real-time Rendering with Implicit 3D Shapes. (2021).\n\nFourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains. Matthew Tancik, P Pratul, Ben Srinivasan, Sara Mildenhall, Nithin Fridovich-Keil, Utkarsh Raghavan, Ravi Singhal, Jonathan T Ramamoorthi, Ren Barron, Ng, 10.1145/3272127.3275096ACM Trans. Graph. 37ArticleMatthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan T. Barron, and Ren Ng. 2020. Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains. NeurIPS (2020). https://bmild.github.io/fourfeat/index.html Danhang Tang, Mingsong Dou, Peter Lincoln, Philip Davidson, Kaiwen Guo, Jonathan Taylor, Sean Fanello, Cem Keskin, Adarsh Kowdle, Sofien Bouaziz, Shahram Izadi, and Andrea Tagliasacchi. 2018. Real-Time Compression and Streaming of 4D Per- formances. ACM Trans. Graph. 37, 6, Article 256 (dec 2018), 11 pages. https: //doi.org/10.1145/3272127.3275096\n\nOptimized Spatial Hashing for Collision Detection of Deformable Objects. Matthias Teschner, Bruno Heidelberger, Matthias M\u00fcller, Proceedings of VMV'03. VMV'03Munich, GermanyDanat Pomeranets, and Markus GrossMatthias Teschner, Bruno Heidelberger, Matthias M\u00fcller, Danat Pomeranets, and Markus Gross. 2003. Optimized Spatial Hashing for Collision Detection of De- formable Objects. In Proceedings of VMV'03, Munich, Germany. 47-54.\n\nSergios Theodoridis, Pattern Recognition. ElsevierSergios Theodoridis. 2008. Pattern Recognition. Elsevier.\n\n. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, arXiv:1706.03762Attention Is All You Need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention Is All You Need. arXiv:1706.03762 (June 2017).\n\nDor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler, Jonathan T Barron, Pratul P Srinivasan, arXiv:2112.03907Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields. Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler, Jonathan T. Barron, and Pratul P. Srinivasan. 2021. Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields. arXiv:2112.03907 (Dec. 2021).\n\nNeX: Real-time View Synthesis with Neural Basis Expansion. Suttisak Wizadwongsa, Pakkapon Phongthawee, Jiraphon Yenphraphai, Supasorn Suwajanakorn, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Suttisak Wizadwongsa, Pakkapon Phongthawee, Jiraphon Yenphraphai, and Supasorn Suwajanakorn. 2021. NeX: Real-time View Synthesis with Neural Basis Expansion. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\n\nAlex Yu, Sara Fridovich-Keil, Matthew Tancik, Qinhong Chen, arXiv:2112.05131Benjamin Recht, and Angjoo Kanazawa. 2021a. Plenoxels: Radiance Fields without Neural Networks. Alex Yu, Sara Fridovich-Keil, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. 2021a. Plenoxels: Radiance Fields without Neural Networks. arXiv:2112.05131 (Dec. 2021).\n\nPlenOctrees for Real-time Rendering of Neural Radiance Fields. Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, Angjoo Kanazawa, ICCV. Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. 2021b. PlenOctrees for Real-time Rendering of Neural Radiance Fields. In ICCV.\n", "annotations": {"author": "[{\"end\":180,\"start\":84}]", "publisher": null, "author_last_name": "[{\"end\":116,\"start\":110}]", "author_first_name": "[{\"end\":109,\"start\":103}]", "author_affiliation": "[{\"end\":150,\"start\":118},{\"end\":179,\"start\":152}]", "title": "[{\"end\":72,\"start\":1},{\"end\":252,\"start\":181}]", "venue": "[{\"end\":270,\"start\":254}]", "abstract": "[{\"end\":3355,\"start\":1277}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5401,\"start\":5381},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":5417,\"start\":5401},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5456,\"start\":5439},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":5474,\"start\":5456},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5781,\"start\":5764},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":5801,\"start\":5781},{\"end\":8625,\"start\":8601},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":8664,\"start\":8647},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8869,\"start\":8848},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":8923,\"start\":8902},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":9034,\"start\":9013},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9573,\"start\":9553},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9821,\"start\":9802},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9844,\"start\":9821},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9866,\"start\":9846},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10423,\"start\":10403},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10441,\"start\":10423},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10457,\"start\":10441},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10475,\"start\":10457},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10493,\"start\":10475},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":10509,\"start\":10493},{\"end\":10526,\"start\":10509},{\"end\":10541,\"start\":10526},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":10575,\"start\":10553},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":11705,\"start\":11686},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":11900,\"start\":11883},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":13714,\"start\":13693},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":13734,\"start\":13714},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":14230,\"start\":14208},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":14266,\"start\":14246},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14286,\"start\":14266},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":14306,\"start\":14286},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":14324,\"start\":14306},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":14340,\"start\":14324},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":14358,\"start\":14340},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":14512,\"start\":14496},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":14560,\"start\":14543},{\"end\":14575,\"start\":14560},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":15576,\"start\":15555},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":15620,\"start\":15600},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":19008,\"start\":18987},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":19244,\"start\":19231},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":23995,\"start\":23974},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":25279,\"start\":25267},{\"end\":25283,\"start\":25282},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":25377,\"start\":25364},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":25802,\"start\":25776},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":27733,\"start\":27709},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":30079,\"start\":30059},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":30198,\"start\":30178},{\"end\":30213,\"start\":30198},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":31291,\"start\":31279},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":31939,\"start\":31922},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":32258,\"start\":32236},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":33727,\"start\":33716},{\"end\":38682,\"start\":38664},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":40584,\"start\":40564},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":40608,\"start\":40584},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":40623,\"start\":40608},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":40972,\"start\":40951},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":41000,\"start\":40983},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":42097,\"start\":42080},{\"end\":42112,\"start\":42097},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":42792,\"start\":42771},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":42820,\"start\":42803},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":45030,\"start\":45012},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":48315,\"start\":48295},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":48332,\"start\":48315},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":48890,\"start\":48869},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":49104,\"start\":49092},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":49118,\"start\":49104},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":50417,\"start\":50391},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":51678,\"start\":51656},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":53335,\"start\":53314},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":53692,\"start\":53678},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":54215,\"start\":54196},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":54580,\"start\":54558},{\"end\":55856,\"start\":55855},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":56679,\"start\":56658},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":58832,\"start\":58808},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":59189,\"start\":59172},{\"end\":59400,\"start\":59379},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":59416,\"start\":59400},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":62227,\"start\":62210},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":62243,\"start\":62227},{\"end\":62261,\"start\":62243},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":68236,\"start\":68216},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":68255,\"start\":68236},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":68276,\"start\":68255},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":68358,\"start\":68338},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":68548,\"start\":68527},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":69821,\"start\":69800}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":63243,\"start\":63072},{\"attributes\":{\"id\":\"fig_1\"},\"end\":63756,\"start\":63244},{\"attributes\":{\"id\":\"fig_2\"},\"end\":64079,\"start\":63757},{\"attributes\":{\"id\":\"fig_3\"},\"end\":64606,\"start\":64080},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":65796,\"start\":64607},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":66167,\"start\":65797},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":68056,\"start\":66168},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":68363,\"start\":68057},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":69548,\"start\":68364},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":69964,\"start\":69549}]", "paragraph": "[{\"end\":4842,\"start\":3364},{\"end\":5475,\"start\":4859},{\"end\":6187,\"start\":5477},{\"end\":7580,\"start\":6189},{\"end\":7672,\"start\":7582},{\"end\":7783,\"start\":7674},{\"end\":8202,\"start\":7785},{\"end\":8442,\"start\":8204},{\"end\":8735,\"start\":8474},{\"end\":9219,\"start\":8737},{\"end\":9222,\"start\":9221},{\"end\":10094,\"start\":9224},{\"end\":12132,\"start\":10096},{\"end\":13199,\"start\":12134},{\"end\":13593,\"start\":13201},{\"end\":14125,\"start\":13595},{\"end\":14841,\"start\":14127},{\"end\":15509,\"start\":14843},{\"end\":16392,\"start\":15511},{\"end\":16952,\"start\":16394},{\"end\":17846,\"start\":16986},{\"end\":18058,\"start\":17892},{\"end\":18954,\"start\":18060},{\"end\":19021,\"start\":18956},{\"end\":19485,\"start\":19041},{\"end\":19666,\"start\":19487},{\"end\":19993,\"start\":19668},{\"end\":20570,\"start\":19995},{\"end\":21298,\"start\":20572},{\"end\":22361,\"start\":21300},{\"end\":23097,\"start\":22363},{\"end\":23562,\"start\":23099},{\"end\":24256,\"start\":23564},{\"end\":25080,\"start\":24258},{\"end\":25484,\"start\":25099},{\"end\":25803,\"start\":25486},{\"end\":26398,\"start\":25805},{\"end\":26790,\"start\":26400},{\"end\":27170,\"start\":26792},{\"end\":27641,\"start\":27172},{\"end\":28246,\"start\":27643},{\"end\":28775,\"start\":28248},{\"end\":29042,\"start\":28777},{\"end\":29235,\"start\":29044},{\"end\":29533,\"start\":29237},{\"end\":30215,\"start\":29535},{\"end\":30222,\"start\":30217},{\"end\":31063,\"start\":30238},{\"end\":31667,\"start\":31097},{\"end\":32851,\"start\":31697},{\"end\":33762,\"start\":32853},{\"end\":36284,\"start\":33764},{\"end\":37020,\"start\":36312},{\"end\":37908,\"start\":37022},{\"end\":38090,\"start\":37954},{\"end\":38548,\"start\":38105},{\"end\":38787,\"start\":38550},{\"end\":39373,\"start\":38789},{\"end\":39898,\"start\":39375},{\"end\":40409,\"start\":39900},{\"end\":42556,\"start\":40411},{\"end\":43083,\"start\":42564},{\"end\":43527,\"start\":43085},{\"end\":44173,\"start\":43529},{\"end\":44774,\"start\":44175},{\"end\":45166,\"start\":44776},{\"end\":45708,\"start\":45197},{\"end\":46128,\"start\":45710},{\"end\":46428,\"start\":46130},{\"end\":46610,\"start\":46430},{\"end\":47124,\"start\":46612},{\"end\":47526,\"start\":47126},{\"end\":48067,\"start\":47528},{\"end\":48621,\"start\":48069},{\"end\":49010,\"start\":48623},{\"end\":49603,\"start\":49012},{\"end\":50225,\"start\":49618},{\"end\":50628,\"start\":50227},{\"end\":50780,\"start\":50630},{\"end\":51054,\"start\":50782},{\"end\":51148,\"start\":51056},{\"end\":51578,\"start\":51150},{\"end\":52216,\"start\":51616},{\"end\":52496,\"start\":52218},{\"end\":52623,\"start\":52498},{\"end\":53013,\"start\":52625},{\"end\":53240,\"start\":53058},{\"end\":53537,\"start\":53292},{\"end\":53693,\"start\":53539},{\"end\":54216,\"start\":53695},{\"end\":54493,\"start\":54218},{\"end\":55141,\"start\":54495},{\"end\":55448,\"start\":55197},{\"end\":56086,\"start\":55450},{\"end\":57455,\"start\":56130},{\"end\":57606,\"start\":57457},{\"end\":57823,\"start\":57642},{\"end\":57991,\"start\":57825},{\"end\":58211,\"start\":58035},{\"end\":58632,\"start\":58213},{\"end\":58792,\"start\":58634},{\"end\":59191,\"start\":58794},{\"end\":59506,\"start\":59193},{\"end\":59828,\"start\":59508},{\"end\":60232,\"start\":59852},{\"end\":60575,\"start\":60234},{\"end\":60767,\"start\":60577},{\"end\":61010,\"start\":60769},{\"end\":61104,\"start\":61012},{\"end\":61547,\"start\":61106},{\"end\":62056,\"start\":61549},{\"end\":62618,\"start\":62058},{\"end\":63071,\"start\":62620}]", "formula": "[{\"attributes\":{\"id\":\"formula_1\"},\"end\":17860,\"start\":17847},{\"attributes\":{\"id\":\"formula_2\"},\"end\":17891,\"start\":17860},{\"attributes\":{\"id\":\"formula_3\"},\"end\":19040,\"start\":19022}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":17495,\"start\":17488},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":18953,\"start\":18944},{\"end\":40817,\"start\":40810},{\"end\":42617,\"start\":42610},{\"end\":62617,\"start\":62610},{\"end\":62751,\"start\":62744},{\"end\":63070,\"start\":63062}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":4857,\"start\":4845},{\"attributes\":{\"n\":\"2\"},\"end\":8472,\"start\":8445},{\"attributes\":{\"n\":\"3\"},\"end\":16984,\"start\":16955},{\"attributes\":{\"n\":\"4\"},\"end\":25097,\"start\":25083},{\"attributes\":{\"n\":\"5\"},\"end\":30236,\"start\":30225},{\"attributes\":{\"n\":\"5.1\"},\"end\":31095,\"start\":31066},{\"attributes\":{\"n\":\"5.2\"},\"end\":31695,\"start\":31670},{\"attributes\":{\"n\":\"5.3\"},\"end\":36310,\"start\":36287},{\"attributes\":{\"n\":\"5.4\"},\"end\":37952,\"start\":37911},{\"end\":38103,\"start\":38093},{\"end\":42562,\"start\":42559},{\"attributes\":{\"n\":\"6\"},\"end\":45195,\"start\":45169},{\"attributes\":{\"n\":\"7\"},\"end\":49616,\"start\":49606},{\"end\":51614,\"start\":51581},{\"end\":53056,\"start\":53016},{\"end\":53290,\"start\":53243},{\"end\":55195,\"start\":55144},{\"end\":56128,\"start\":56089},{\"end\":57640,\"start\":57609},{\"end\":58033,\"start\":57994},{\"end\":59850,\"start\":59831},{\"end\":63086,\"start\":63073},{\"end\":63254,\"start\":63245},{\"end\":63767,\"start\":63758},{\"end\":65807,\"start\":65798}]", "table": "[{\"end\":65796,\"start\":64915},{\"end\":66167,\"start\":65945},{\"end\":68056,\"start\":66445},{\"end\":69548,\"start\":68972},{\"end\":69964,\"start\":69874}]", "figure_caption": "[{\"end\":63243,\"start\":63089},{\"end\":63756,\"start\":63257},{\"end\":64079,\"start\":63770},{\"end\":64606,\"start\":64082},{\"end\":64915,\"start\":64609},{\"end\":65945,\"start\":65809},{\"end\":66445,\"start\":66170},{\"end\":68363,\"start\":68059},{\"end\":68972,\"start\":68366},{\"end\":69874,\"start\":69551}]", "figure_ref": "[{\"end\":3363,\"start\":3357},{\"end\":6479,\"start\":6471},{\"end\":7670,\"start\":7662},{\"end\":12493,\"start\":12485},{\"end\":13744,\"start\":13736},{\"end\":14863,\"start\":14855},{\"end\":17505,\"start\":17497},{\"end\":20693,\"start\":20685},{\"end\":21120,\"start\":21112},{\"end\":26603,\"start\":26595},{\"end\":27135,\"start\":27127},{\"end\":30689,\"start\":30680},{\"end\":30743,\"start\":30735},{\"end\":30958,\"start\":30950},{\"end\":32864,\"start\":32856},{\"end\":35189,\"start\":35188},{\"end\":37453,\"start\":37445},{\"end\":38133,\"start\":38126},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":39782,\"start\":39773},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":40808,\"start\":40799},{\"end\":41894,\"start\":41885},{\"end\":47727,\"start\":47704},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":49539,\"start\":49530}]", "bib_author_first_name": "[{\"end\":73303,\"start\":73297},{\"end\":73314,\"start\":73311},{\"end\":73332,\"start\":73324},{\"end\":73352,\"start\":73342},{\"end\":73724,\"start\":73716},{\"end\":73726,\"start\":73725},{\"end\":73738,\"start\":73735},{\"end\":73758,\"start\":73751},{\"end\":73772,\"start\":73767},{\"end\":73788,\"start\":73781},{\"end\":73811,\"start\":73805},{\"end\":73813,\"start\":73812},{\"end\":74582,\"start\":74577},{\"end\":74594,\"start\":74591},{\"end\":74596,\"start\":74595},{\"end\":74610,\"start\":74606},{\"end\":74622,\"start\":74616},{\"end\":74638,\"start\":74632},{\"end\":74653,\"start\":74647},{\"end\":74672,\"start\":74665},{\"end\":75169,\"start\":75165},{\"end\":75171,\"start\":75170},{\"end\":75184,\"start\":75178},{\"end\":75186,\"start\":75185},{\"end\":75199,\"start\":75192},{\"end\":75201,\"start\":75200},{\"end\":75212,\"start\":75208},{\"end\":75227,\"start\":75221},{\"end\":75239,\"start\":75233},{\"end\":75844,\"start\":75838},{\"end\":75860,\"start\":75854},{\"end\":75877,\"start\":75871},{\"end\":76186,\"start\":76178},{\"end\":76202,\"start\":76196},{\"end\":76208,\"start\":76203},{\"end\":76225,\"start\":76219},{\"end\":76244,\"start\":76238},{\"end\":76246,\"start\":76245},{\"end\":76261,\"start\":76255},{\"end\":76263,\"start\":76262},{\"end\":76576,\"start\":76572},{\"end\":76593,\"start\":76587},{\"end\":76908,\"start\":76904},{\"end\":77318,\"start\":77311},{\"end\":77320,\"start\":77319},{\"end\":77334,\"start\":77329},{\"end\":77352,\"start\":77345},{\"end\":77367,\"start\":77362},{\"end\":77383,\"start\":77377},{\"end\":77690,\"start\":77685},{\"end\":77707,\"start\":77700},{\"end\":77719,\"start\":77714},{\"end\":77735,\"start\":77730},{\"end\":77748,\"start\":77744},{\"end\":77750,\"start\":77749},{\"end\":78261,\"start\":78255},{\"end\":78276,\"start\":78270},{\"end\":78718,\"start\":78713},{\"end\":78735,\"start\":78728},{\"end\":78750,\"start\":78742},{\"end\":79028,\"start\":79017},{\"end\":79042,\"start\":79037},{\"end\":79044,\"start\":79043},{\"end\":79402,\"start\":79399},{\"end\":79416,\"start\":79411},{\"end\":80017,\"start\":80014},{\"end\":80032,\"start\":80025},{\"end\":80046,\"start\":80040},{\"end\":80059,\"start\":80052},{\"end\":80077,\"start\":80069},{\"end\":80091,\"start\":80085},{\"end\":80529,\"start\":80528},{\"end\":80545,\"start\":80540},{\"end\":80740,\"start\":80739},{\"end\":81180,\"start\":81174},{\"end\":81196,\"start\":81191},{\"end\":81210,\"start\":81207},{\"end\":81229,\"start\":81223},{\"end\":81241,\"start\":81237},{\"end\":81255,\"start\":81250},{\"end\":81269,\"start\":81265},{\"end\":81563,\"start\":81556},{\"end\":81575,\"start\":81569},{\"end\":81588,\"start\":81580},{\"end\":81912,\"start\":81911},{\"end\":81914,\"start\":81913},{\"end\":81928,\"start\":81923},{\"end\":81930,\"start\":81929},{\"end\":81945,\"start\":81939},{\"end\":81947,\"start\":81946},{\"end\":81961,\"start\":81957},{\"end\":81963,\"start\":81962},{\"end\":81974,\"start\":81969},{\"end\":81987,\"start\":81981},{\"end\":82335,\"start\":82330},{\"end\":82350,\"start\":82343},{\"end\":82367,\"start\":82359},{\"end\":82379,\"start\":82376},{\"end\":82395,\"start\":82391},{\"end\":82417,\"start\":82409},{\"end\":82744,\"start\":82737},{\"end\":82765,\"start\":82759},{\"end\":82779,\"start\":82774},{\"end\":82794,\"start\":82787},{\"end\":82808,\"start\":82803},{\"end\":82821,\"start\":82816},{\"end\":82835,\"start\":82830},{\"end\":82853,\"start\":82846},{\"end\":82870,\"start\":82863},{\"end\":82887,\"start\":82881},{\"end\":82902,\"start\":82899},{\"end\":83189,\"start\":83186},{\"end\":83207,\"start\":83202},{\"end\":83223,\"start\":83216},{\"end\":83246,\"start\":83240},{\"end\":83267,\"start\":83259},{\"end\":83269,\"start\":83268},{\"end\":83671,\"start\":83668},{\"end\":83685,\"start\":83684},{\"end\":83701,\"start\":83694},{\"end\":83718,\"start\":83714},{\"end\":83726,\"start\":83719},{\"end\":83744,\"start\":83740},{\"end\":83759,\"start\":83756},{\"end\":83781,\"start\":83773},{\"end\":84239,\"start\":84236},{\"end\":84253,\"start\":84252},{\"end\":84269,\"start\":84262},{\"end\":84290,\"start\":84282},{\"end\":84292,\"start\":84291},{\"end\":84305,\"start\":84301},{\"end\":84317,\"start\":84314},{\"end\":84577,\"start\":84571},{\"end\":84718,\"start\":84712},{\"end\":84732,\"start\":84727},{\"end\":84752,\"start\":84745},{\"end\":84770,\"start\":84764},{\"end\":85055,\"start\":85049},{\"end\":85071,\"start\":85064},{\"end\":85092,\"start\":85083},{\"end\":85405,\"start\":85399},{\"end\":85421,\"start\":85414},{\"end\":85436,\"start\":85433},{\"end\":85453,\"start\":85444},{\"end\":85794,\"start\":85791},{\"end\":86123,\"start\":86120},{\"end\":86688,\"start\":86682},{\"end\":86701,\"start\":86695},{\"end\":86721,\"start\":86714},{\"end\":86737,\"start\":86730},{\"end\":86745,\"start\":86744},{\"end\":86764,\"start\":86753},{\"end\":86771,\"start\":86765},{\"end\":86786,\"start\":86781},{\"end\":86788,\"start\":86787},{\"end\":86806,\"start\":86800},{\"end\":87275,\"start\":87267},{\"end\":87292,\"start\":87285},{\"end\":87311,\"start\":87304},{\"end\":87323,\"start\":87319},{\"end\":87696,\"start\":87695},{\"end\":87708,\"start\":87704},{\"end\":88165,\"start\":88158},{\"end\":88167,\"start\":88166},{\"end\":88464,\"start\":88454},{\"end\":88476,\"start\":88471},{\"end\":88493,\"start\":88487},{\"end\":88509,\"start\":88502},{\"end\":88526,\"start\":88520},{\"end\":88850,\"start\":88843},{\"end\":88864,\"start\":88857},{\"end\":88879,\"start\":88875},{\"end\":88895,\"start\":88891},{\"end\":88914,\"start\":88907},{\"end\":89190,\"start\":89183},{\"end\":89204,\"start\":89197},{\"end\":89219,\"start\":89215},{\"end\":89235,\"start\":89231},{\"end\":89254,\"start\":89247},{\"end\":89548,\"start\":89544},{\"end\":89561,\"start\":89556},{\"end\":89574,\"start\":89570},{\"end\":89942,\"start\":89935},{\"end\":89954,\"start\":89953},{\"end\":89956,\"start\":89955},{\"end\":89974,\"start\":89965},{\"end\":89976,\"start\":89975},{\"end\":89990,\"start\":89985},{\"end\":89992,\"start\":89991},{\"end\":90008,\"start\":90002},{\"end\":90249,\"start\":90244},{\"end\":90258,\"start\":90255},{\"end\":90275,\"start\":90264},{\"end\":90644,\"start\":90638},{\"end\":90659,\"start\":90655},{\"end\":90677,\"start\":90670},{\"end\":90690,\"start\":90683},{\"end\":90705,\"start\":90698},{\"end\":90717,\"start\":90712},{\"end\":90738,\"start\":90734},{\"end\":90755,\"start\":90749},{\"end\":90770,\"start\":90765},{\"end\":91114,\"start\":91107},{\"end\":91124,\"start\":91123},{\"end\":91136,\"start\":91133},{\"end\":91153,\"start\":91149},{\"end\":91172,\"start\":91166},{\"end\":91196,\"start\":91189},{\"end\":91211,\"start\":91207},{\"end\":91229,\"start\":91221},{\"end\":91231,\"start\":91230},{\"end\":91248,\"start\":91245},{\"end\":92057,\"start\":92049},{\"end\":92073,\"start\":92068},{\"end\":92096,\"start\":92088},{\"end\":92414,\"start\":92407},{\"end\":92524,\"start\":92518},{\"end\":92538,\"start\":92534},{\"end\":92552,\"start\":92548},{\"end\":92566,\"start\":92561},{\"end\":92583,\"start\":92578},{\"end\":92596,\"start\":92591},{\"end\":92598,\"start\":92597},{\"end\":92612,\"start\":92606},{\"end\":92626,\"start\":92621},{\"end\":92875,\"start\":92872},{\"end\":92889,\"start\":92884},{\"end\":92901,\"start\":92898},{\"end\":92918,\"start\":92914},{\"end\":92936,\"start\":92928},{\"end\":92938,\"start\":92937},{\"end\":92953,\"start\":92947},{\"end\":92955,\"start\":92954},{\"end\":93340,\"start\":93332},{\"end\":93362,\"start\":93354},{\"end\":93384,\"start\":93376},{\"end\":93406,\"start\":93398},{\"end\":93721,\"start\":93717},{\"end\":93730,\"start\":93726},{\"end\":93754,\"start\":93747},{\"end\":93770,\"start\":93763},{\"end\":94143,\"start\":94139},{\"end\":94155,\"start\":94148},{\"end\":94167,\"start\":94160},{\"end\":94179,\"start\":94176},{\"end\":94187,\"start\":94184},{\"end\":94198,\"start\":94192}]", "bib_author_last_name": "[{\"end\":73309,\"start\":73304},{\"end\":73322,\"start\":73315},{\"end\":73340,\"start\":73333},{\"end\":73359,\"start\":73353},{\"end\":73733,\"start\":73727},{\"end\":73749,\"start\":73739},{\"end\":73765,\"start\":73759},{\"end\":73779,\"start\":73773},{\"end\":73803,\"start\":73789},{\"end\":73824,\"start\":73814},{\"end\":74589,\"start\":74583},{\"end\":74604,\"start\":74597},{\"end\":74614,\"start\":74611},{\"end\":74630,\"start\":74623},{\"end\":74645,\"start\":74639},{\"end\":74663,\"start\":74654},{\"end\":74681,\"start\":74673},{\"end\":75176,\"start\":75172},{\"end\":75190,\"start\":75187},{\"end\":75206,\"start\":75202},{\"end\":75219,\"start\":75213},{\"end\":75231,\"start\":75228},{\"end\":75256,\"start\":75240},{\"end\":75263,\"start\":75258},{\"end\":75852,\"start\":75845},{\"end\":75869,\"start\":75861},{\"end\":75887,\"start\":75878},{\"end\":76194,\"start\":76187},{\"end\":76217,\"start\":76209},{\"end\":76236,\"start\":76226},{\"end\":76253,\"start\":76247},{\"end\":76272,\"start\":76264},{\"end\":76585,\"start\":76577},{\"end\":76600,\"start\":76594},{\"end\":76914,\"start\":76909},{\"end\":77327,\"start\":77321},{\"end\":77343,\"start\":77335},{\"end\":77360,\"start\":77353},{\"end\":77375,\"start\":77368},{\"end\":77392,\"start\":77384},{\"end\":77698,\"start\":77691},{\"end\":77712,\"start\":77708},{\"end\":77728,\"start\":77720},{\"end\":77742,\"start\":77736},{\"end\":77758,\"start\":77751},{\"end\":78268,\"start\":78262},{\"end\":78283,\"start\":78277},{\"end\":78726,\"start\":78719},{\"end\":78740,\"start\":78736},{\"end\":78758,\"start\":78751},{\"end\":79035,\"start\":79029},{\"end\":79051,\"start\":79045},{\"end\":79409,\"start\":79403},{\"end\":79423,\"start\":79417},{\"end\":80023,\"start\":80018},{\"end\":80038,\"start\":80033},{\"end\":80050,\"start\":80047},{\"end\":80067,\"start\":80060},{\"end\":80083,\"start\":80078},{\"end\":80099,\"start\":80092},{\"end\":80111,\"start\":80101},{\"end\":80538,\"start\":80530},{\"end\":80552,\"start\":80546},{\"end\":80556,\"start\":80554},{\"end\":80748,\"start\":80741},{\"end\":80756,\"start\":80750},{\"end\":81189,\"start\":81181},{\"end\":81205,\"start\":81197},{\"end\":81221,\"start\":81211},{\"end\":81235,\"start\":81230},{\"end\":81248,\"start\":81242},{\"end\":81263,\"start\":81256},{\"end\":81274,\"start\":81270},{\"end\":81567,\"start\":81564},{\"end\":81578,\"start\":81576},{\"end\":81592,\"start\":81589},{\"end\":81921,\"start\":81915},{\"end\":81937,\"start\":81931},{\"end\":81955,\"start\":81948},{\"end\":81967,\"start\":81964},{\"end\":81979,\"start\":81975},{\"end\":81996,\"start\":81988},{\"end\":82007,\"start\":81998},{\"end\":82341,\"start\":82336},{\"end\":82357,\"start\":82351},{\"end\":82374,\"start\":82368},{\"end\":82389,\"start\":82380},{\"end\":82407,\"start\":82396},{\"end\":82428,\"start\":82418},{\"end\":82757,\"start\":82745},{\"end\":82772,\"start\":82766},{\"end\":82785,\"start\":82780},{\"end\":82801,\"start\":82795},{\"end\":82814,\"start\":82809},{\"end\":82828,\"start\":82822},{\"end\":82844,\"start\":82836},{\"end\":82861,\"start\":82854},{\"end\":82879,\"start\":82871},{\"end\":82897,\"start\":82888},{\"end\":82905,\"start\":82903},{\"end\":83200,\"start\":83190},{\"end\":83214,\"start\":83208},{\"end\":83238,\"start\":83224},{\"end\":83257,\"start\":83247},{\"end\":83276,\"start\":83270},{\"end\":83682,\"start\":83672},{\"end\":83692,\"start\":83686},{\"end\":83712,\"start\":83702},{\"end\":83738,\"start\":83727},{\"end\":83754,\"start\":83745},{\"end\":83771,\"start\":83760},{\"end\":83784,\"start\":83782},{\"end\":83789,\"start\":83786},{\"end\":84250,\"start\":84240},{\"end\":84260,\"start\":84254},{\"end\":84280,\"start\":84270},{\"end\":84299,\"start\":84293},{\"end\":84312,\"start\":84306},{\"end\":84329,\"start\":84318},{\"end\":84333,\"start\":84331},{\"end\":84584,\"start\":84578},{\"end\":84725,\"start\":84719},{\"end\":84743,\"start\":84733},{\"end\":84762,\"start\":84753},{\"end\":84776,\"start\":84771},{\"end\":85062,\"start\":85056},{\"end\":85081,\"start\":85072},{\"end\":85099,\"start\":85093},{\"end\":85412,\"start\":85406},{\"end\":85431,\"start\":85422},{\"end\":85442,\"start\":85437},{\"end\":85460,\"start\":85454},{\"end\":85801,\"start\":85795},{\"end\":86130,\"start\":86124},{\"end\":86693,\"start\":86689},{\"end\":86712,\"start\":86702},{\"end\":86728,\"start\":86722},{\"end\":86742,\"start\":86738},{\"end\":86751,\"start\":86746},{\"end\":86779,\"start\":86772},{\"end\":86798,\"start\":86789},{\"end\":86816,\"start\":86807},{\"end\":86829,\"start\":86818},{\"end\":87283,\"start\":87276},{\"end\":87302,\"start\":87293},{\"end\":87317,\"start\":87312},{\"end\":87334,\"start\":87324},{\"end\":87702,\"start\":87697},{\"end\":87718,\"start\":87709},{\"end\":87724,\"start\":87720},{\"end\":88180,\"start\":88168},{\"end\":88402,\"start\":88383},{\"end\":88469,\"start\":88465},{\"end\":88485,\"start\":88477},{\"end\":88500,\"start\":88494},{\"end\":88518,\"start\":88510},{\"end\":88536,\"start\":88527},{\"end\":88855,\"start\":88851},{\"end\":88873,\"start\":88865},{\"end\":88889,\"start\":88880},{\"end\":88905,\"start\":88896},{\"end\":88921,\"start\":88915},{\"end\":89195,\"start\":89191},{\"end\":89213,\"start\":89205},{\"end\":89229,\"start\":89220},{\"end\":89245,\"start\":89236},{\"end\":89261,\"start\":89255},{\"end\":89554,\"start\":89549},{\"end\":89568,\"start\":89562},{\"end\":89584,\"start\":89575},{\"end\":89951,\"start\":89943},{\"end\":89963,\"start\":89957},{\"end\":89983,\"start\":89977},{\"end\":90000,\"start\":89993},{\"end\":90016,\"start\":90009},{\"end\":90027,\"start\":90018},{\"end\":90253,\"start\":90250},{\"end\":90262,\"start\":90259},{\"end\":90280,\"start\":90276},{\"end\":90653,\"start\":90645},{\"end\":90668,\"start\":90660},{\"end\":90681,\"start\":90678},{\"end\":90696,\"start\":90691},{\"end\":90710,\"start\":90706},{\"end\":90732,\"start\":90718},{\"end\":90747,\"start\":90739},{\"end\":90763,\"start\":90756},{\"end\":90777,\"start\":90771},{\"end\":91121,\"start\":91115},{\"end\":91131,\"start\":91125},{\"end\":91147,\"start\":91137},{\"end\":91164,\"start\":91154},{\"end\":91187,\"start\":91173},{\"end\":91205,\"start\":91197},{\"end\":91219,\"start\":91212},{\"end\":91243,\"start\":91232},{\"end\":91255,\"start\":91249},{\"end\":91259,\"start\":91257},{\"end\":92066,\"start\":92058},{\"end\":92086,\"start\":92074},{\"end\":92103,\"start\":92097},{\"end\":92426,\"start\":92415},{\"end\":92532,\"start\":92525},{\"end\":92546,\"start\":92539},{\"end\":92559,\"start\":92553},{\"end\":92576,\"start\":92567},{\"end\":92589,\"start\":92584},{\"end\":92604,\"start\":92599},{\"end\":92619,\"start\":92613},{\"end\":92637,\"start\":92627},{\"end\":92882,\"start\":92876},{\"end\":92896,\"start\":92890},{\"end\":92912,\"start\":92902},{\"end\":92926,\"start\":92919},{\"end\":92945,\"start\":92939},{\"end\":92966,\"start\":92956},{\"end\":93352,\"start\":93341},{\"end\":93374,\"start\":93363},{\"end\":93396,\"start\":93385},{\"end\":93419,\"start\":93407},{\"end\":93724,\"start\":93722},{\"end\":93745,\"start\":93731},{\"end\":93761,\"start\":93755},{\"end\":93775,\"start\":93771},{\"end\":94146,\"start\":94144},{\"end\":94158,\"start\":94156},{\"end\":94174,\"start\":94168},{\"end\":94182,\"start\":94180},{\"end\":94190,\"start\":94188},{\"end\":94207,\"start\":94199}]", "bib_entry": "[{\"attributes\":{\"doi\":\"10.2312/EGWR/EGSR07/051-060\",\"id\":\"b0\",\"matched_paper_id\":14230351},\"end\":73714,\"start\":73272},{\"attributes\":{\"doi\":\"arXiv:2111.12077\",\"id\":\"b1\"},\"end\":74498,\"start\":73716},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":214623267},\"end\":75163,\"start\":74500},{\"attributes\":{\"doi\":\"arXiv:2112.07945\",\"id\":\"b3\"},\"end\":75756,\"start\":75165},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":211817828},\"end\":76176,\"start\":75758},{\"attributes\":{\"id\":\"b5\"},\"end\":76550,\"start\":76178},{\"attributes\":{\"id\":\"b6\"},\"end\":76839,\"start\":76552},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":15680285},\"end\":77309,\"start\":76841},{\"attributes\":{\"id\":\"b8\"},\"end\":77638,\"start\":77311},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":3648736},\"end\":78178,\"start\":77640},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":5575601},\"end\":78693,\"start\":78180},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":235195683},\"end\":78985,\"start\":78695},{\"attributes\":{\"id\":\"b12\"},\"end\":79372,\"start\":78987},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":726778},\"end\":79961,\"start\":79374},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":214606025},\"end\":80482,\"start\":79963},{\"attributes\":{\"id\":\"b15\"},\"end\":80684,\"start\":80484},{\"attributes\":{\"id\":\"b16\"},\"end\":81172,\"start\":80686},{\"attributes\":{\"id\":\"b17\"},\"end\":81554,\"start\":81174},{\"attributes\":{\"id\":\"b18\"},\"end\":81846,\"start\":81556},{\"attributes\":{\"id\":\"b19\"},\"end\":82245,\"start\":81848},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":233181738},\"end\":82735,\"start\":82247},{\"attributes\":{\"id\":\"b21\"},\"end\":83184,\"start\":82737},{\"attributes\":{\"id\":\"b22\"},\"end\":83576,\"start\":83186},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":219947110},\"end\":84162,\"start\":83578},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":213175590},\"end\":84533,\"start\":84164},{\"attributes\":{\"id\":\"b25\"},\"end\":84682,\"start\":84535},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":51970108},\"end\":85022,\"start\":84684},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":219179383},\"end\":85345,\"start\":85024},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":235606372},\"end\":85730,\"start\":85347},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":15119565},\"end\":86022,\"start\":85732},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":236936956},\"end\":86581,\"start\":86024},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":234365467},\"end\":87207,\"start\":86583},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":207207064},\"end\":87618,\"start\":87209},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":2153745},\"end\":88051,\"start\":87620},{\"attributes\":{\"id\":\"b34\"},\"end\":88379,\"start\":88053},{\"attributes\":{\"id\":\"b35\"},\"end\":88452,\"start\":88381},{\"attributes\":{\"id\":\"b36\"},\"end\":88841,\"start\":88454},{\"attributes\":{\"id\":\"b37\"},\"end\":89181,\"start\":88843},{\"attributes\":{\"id\":\"b38\"},\"end\":89483,\"start\":89183},{\"attributes\":{\"id\":\"b39\"},\"end\":89865,\"start\":89485},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":219720931},\"end\":90242,\"start\":89867},{\"attributes\":{\"id\":\"b41\"},\"end\":90557,\"start\":90244},{\"attributes\":{\"id\":\"b42\"},\"end\":91016,\"start\":90559},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":219791950},\"end\":91974,\"start\":91018},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":12035329},\"end\":92405,\"start\":91976},{\"attributes\":{\"id\":\"b45\"},\"end\":92514,\"start\":92407},{\"attributes\":{\"id\":\"b46\"},\"end\":92870,\"start\":92516},{\"attributes\":{\"id\":\"b47\"},\"end\":93271,\"start\":92872},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":232168851},\"end\":93715,\"start\":93273},{\"attributes\":{\"id\":\"b49\"},\"end\":94074,\"start\":93717},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":232352425},\"end\":94367,\"start\":94076}]", "bib_title": "[{\"end\":73295,\"start\":73272},{\"end\":74575,\"start\":74500},{\"end\":75836,\"start\":75758},{\"end\":76570,\"start\":76552},{\"end\":76902,\"start\":76841},{\"end\":77683,\"start\":77640},{\"end\":78253,\"start\":78180},{\"end\":78711,\"start\":78695},{\"end\":79015,\"start\":78987},{\"end\":79397,\"start\":79374},{\"end\":80012,\"start\":79963},{\"end\":80737,\"start\":80686},{\"end\":81909,\"start\":81848},{\"end\":82328,\"start\":82247},{\"end\":83666,\"start\":83578},{\"end\":84234,\"start\":84164},{\"end\":84710,\"start\":84684},{\"end\":85047,\"start\":85024},{\"end\":85397,\"start\":85347},{\"end\":85789,\"start\":85732},{\"end\":86118,\"start\":86024},{\"end\":86680,\"start\":86583},{\"end\":87265,\"start\":87209},{\"end\":87693,\"start\":87620},{\"end\":89933,\"start\":89867},{\"end\":91105,\"start\":91018},{\"end\":92047,\"start\":91976},{\"end\":93330,\"start\":93273},{\"end\":94137,\"start\":94076}]", "bib_author": "[{\"end\":73311,\"start\":73297},{\"end\":73324,\"start\":73311},{\"end\":73342,\"start\":73324},{\"end\":73361,\"start\":73342},{\"end\":73735,\"start\":73716},{\"end\":73751,\"start\":73735},{\"end\":73767,\"start\":73751},{\"end\":73781,\"start\":73767},{\"end\":73805,\"start\":73781},{\"end\":73826,\"start\":73805},{\"end\":74591,\"start\":74577},{\"end\":74606,\"start\":74591},{\"end\":74616,\"start\":74606},{\"end\":74632,\"start\":74616},{\"end\":74647,\"start\":74632},{\"end\":74665,\"start\":74647},{\"end\":74683,\"start\":74665},{\"end\":75178,\"start\":75165},{\"end\":75192,\"start\":75178},{\"end\":75208,\"start\":75192},{\"end\":75221,\"start\":75208},{\"end\":75233,\"start\":75221},{\"end\":75258,\"start\":75233},{\"end\":75265,\"start\":75258},{\"end\":75854,\"start\":75838},{\"end\":75871,\"start\":75854},{\"end\":75889,\"start\":75871},{\"end\":76196,\"start\":76178},{\"end\":76219,\"start\":76196},{\"end\":76238,\"start\":76219},{\"end\":76255,\"start\":76238},{\"end\":76274,\"start\":76255},{\"end\":76587,\"start\":76572},{\"end\":76602,\"start\":76587},{\"end\":76916,\"start\":76904},{\"end\":77329,\"start\":77311},{\"end\":77345,\"start\":77329},{\"end\":77362,\"start\":77345},{\"end\":77377,\"start\":77362},{\"end\":77394,\"start\":77377},{\"end\":77700,\"start\":77685},{\"end\":77714,\"start\":77700},{\"end\":77730,\"start\":77714},{\"end\":77744,\"start\":77730},{\"end\":77760,\"start\":77744},{\"end\":78270,\"start\":78255},{\"end\":78285,\"start\":78270},{\"end\":78728,\"start\":78713},{\"end\":78742,\"start\":78728},{\"end\":78760,\"start\":78742},{\"end\":79037,\"start\":79017},{\"end\":79053,\"start\":79037},{\"end\":79411,\"start\":79399},{\"end\":79425,\"start\":79411},{\"end\":80025,\"start\":80014},{\"end\":80040,\"start\":80025},{\"end\":80052,\"start\":80040},{\"end\":80069,\"start\":80052},{\"end\":80085,\"start\":80069},{\"end\":80101,\"start\":80085},{\"end\":80113,\"start\":80101},{\"end\":80540,\"start\":80528},{\"end\":80554,\"start\":80540},{\"end\":80558,\"start\":80554},{\"end\":80750,\"start\":80739},{\"end\":80758,\"start\":80750},{\"end\":81191,\"start\":81174},{\"end\":81207,\"start\":81191},{\"end\":81223,\"start\":81207},{\"end\":81237,\"start\":81223},{\"end\":81250,\"start\":81237},{\"end\":81265,\"start\":81250},{\"end\":81276,\"start\":81265},{\"end\":81569,\"start\":81556},{\"end\":81580,\"start\":81569},{\"end\":81594,\"start\":81580},{\"end\":81923,\"start\":81911},{\"end\":81939,\"start\":81923},{\"end\":81957,\"start\":81939},{\"end\":81969,\"start\":81957},{\"end\":81981,\"start\":81969},{\"end\":81998,\"start\":81981},{\"end\":82009,\"start\":81998},{\"end\":82343,\"start\":82330},{\"end\":82359,\"start\":82343},{\"end\":82376,\"start\":82359},{\"end\":82391,\"start\":82376},{\"end\":82409,\"start\":82391},{\"end\":82430,\"start\":82409},{\"end\":82759,\"start\":82737},{\"end\":82774,\"start\":82759},{\"end\":82787,\"start\":82774},{\"end\":82803,\"start\":82787},{\"end\":82816,\"start\":82803},{\"end\":82830,\"start\":82816},{\"end\":82846,\"start\":82830},{\"end\":82863,\"start\":82846},{\"end\":82881,\"start\":82863},{\"end\":82899,\"start\":82881},{\"end\":82907,\"start\":82899},{\"end\":83202,\"start\":83186},{\"end\":83216,\"start\":83202},{\"end\":83240,\"start\":83216},{\"end\":83259,\"start\":83240},{\"end\":83278,\"start\":83259},{\"end\":83684,\"start\":83668},{\"end\":83694,\"start\":83684},{\"end\":83714,\"start\":83694},{\"end\":83740,\"start\":83714},{\"end\":83756,\"start\":83740},{\"end\":83773,\"start\":83756},{\"end\":83786,\"start\":83773},{\"end\":83791,\"start\":83786},{\"end\":84252,\"start\":84236},{\"end\":84262,\"start\":84252},{\"end\":84282,\"start\":84262},{\"end\":84301,\"start\":84282},{\"end\":84314,\"start\":84301},{\"end\":84331,\"start\":84314},{\"end\":84335,\"start\":84331},{\"end\":84586,\"start\":84571},{\"end\":84727,\"start\":84712},{\"end\":84745,\"start\":84727},{\"end\":84764,\"start\":84745},{\"end\":84778,\"start\":84764},{\"end\":85064,\"start\":85049},{\"end\":85083,\"start\":85064},{\"end\":85101,\"start\":85083},{\"end\":85414,\"start\":85399},{\"end\":85433,\"start\":85414},{\"end\":85444,\"start\":85433},{\"end\":85462,\"start\":85444},{\"end\":85803,\"start\":85791},{\"end\":86132,\"start\":86120},{\"end\":86695,\"start\":86682},{\"end\":86714,\"start\":86695},{\"end\":86730,\"start\":86714},{\"end\":86744,\"start\":86730},{\"end\":86753,\"start\":86744},{\"end\":86781,\"start\":86753},{\"end\":86800,\"start\":86781},{\"end\":86818,\"start\":86800},{\"end\":86831,\"start\":86818},{\"end\":87285,\"start\":87267},{\"end\":87304,\"start\":87285},{\"end\":87319,\"start\":87304},{\"end\":87336,\"start\":87319},{\"end\":87704,\"start\":87695},{\"end\":87720,\"start\":87704},{\"end\":87726,\"start\":87720},{\"end\":88182,\"start\":88158},{\"end\":88404,\"start\":88383},{\"end\":88471,\"start\":88454},{\"end\":88487,\"start\":88471},{\"end\":88502,\"start\":88487},{\"end\":88520,\"start\":88502},{\"end\":88538,\"start\":88520},{\"end\":88857,\"start\":88843},{\"end\":88875,\"start\":88857},{\"end\":88891,\"start\":88875},{\"end\":88907,\"start\":88891},{\"end\":88923,\"start\":88907},{\"end\":89197,\"start\":89183},{\"end\":89215,\"start\":89197},{\"end\":89231,\"start\":89215},{\"end\":89247,\"start\":89231},{\"end\":89263,\"start\":89247},{\"end\":89556,\"start\":89544},{\"end\":89570,\"start\":89556},{\"end\":89586,\"start\":89570},{\"end\":89953,\"start\":89935},{\"end\":89965,\"start\":89953},{\"end\":89985,\"start\":89965},{\"end\":90002,\"start\":89985},{\"end\":90018,\"start\":90002},{\"end\":90029,\"start\":90018},{\"end\":90255,\"start\":90244},{\"end\":90264,\"start\":90255},{\"end\":90282,\"start\":90264},{\"end\":90655,\"start\":90638},{\"end\":90670,\"start\":90655},{\"end\":90683,\"start\":90670},{\"end\":90698,\"start\":90683},{\"end\":90712,\"start\":90698},{\"end\":90734,\"start\":90712},{\"end\":90749,\"start\":90734},{\"end\":90765,\"start\":90749},{\"end\":90779,\"start\":90765},{\"end\":91123,\"start\":91107},{\"end\":91133,\"start\":91123},{\"end\":91149,\"start\":91133},{\"end\":91166,\"start\":91149},{\"end\":91189,\"start\":91166},{\"end\":91207,\"start\":91189},{\"end\":91221,\"start\":91207},{\"end\":91245,\"start\":91221},{\"end\":91257,\"start\":91245},{\"end\":91261,\"start\":91257},{\"end\":92068,\"start\":92049},{\"end\":92088,\"start\":92068},{\"end\":92105,\"start\":92088},{\"end\":92428,\"start\":92407},{\"end\":92534,\"start\":92518},{\"end\":92548,\"start\":92534},{\"end\":92561,\"start\":92548},{\"end\":92578,\"start\":92561},{\"end\":92591,\"start\":92578},{\"end\":92606,\"start\":92591},{\"end\":92621,\"start\":92606},{\"end\":92639,\"start\":92621},{\"end\":92884,\"start\":92872},{\"end\":92898,\"start\":92884},{\"end\":92914,\"start\":92898},{\"end\":92928,\"start\":92914},{\"end\":92947,\"start\":92928},{\"end\":92968,\"start\":92947},{\"end\":93354,\"start\":93332},{\"end\":93376,\"start\":93354},{\"end\":93398,\"start\":93376},{\"end\":93421,\"start\":93398},{\"end\":93726,\"start\":93717},{\"end\":93747,\"start\":93726},{\"end\":93763,\"start\":93747},{\"end\":93777,\"start\":93763},{\"end\":94148,\"start\":94139},{\"end\":94160,\"start\":94148},{\"end\":94176,\"start\":94160},{\"end\":94184,\"start\":94176},{\"end\":94192,\"start\":94184},{\"end\":94209,\"start\":94192}]", "bib_venue": "[{\"end\":73416,\"start\":73388},{\"end\":73960,\"start\":73842},{\"end\":74709,\"start\":74683},{\"end\":75397,\"start\":75297},{\"end\":75954,\"start\":75889},{\"end\":76352,\"start\":76274},{\"end\":76688,\"start\":76602},{\"end\":76964,\"start\":76939},{\"end\":77460,\"start\":77410},{\"end\":77828,\"start\":77760},{\"end\":78362,\"start\":78285},{\"end\":78811,\"start\":78783},{\"end\":79127,\"start\":79087},{\"end\":79531,\"start\":79448},{\"end\":80185,\"start\":80113},{\"end\":80526,\"start\":80484},{\"end\":80836,\"start\":80758},{\"end\":81350,\"start\":81292},{\"end\":81674,\"start\":81594},{\"end\":82025,\"start\":82009},{\"end\":82478,\"start\":82430},{\"end\":82947,\"start\":82923},{\"end\":83367,\"start\":83294},{\"end\":83830,\"start\":83814},{\"end\":84339,\"start\":84335},{\"end\":84569,\"start\":84535},{\"end\":84809,\"start\":84793},{\"end\":85140,\"start\":85124},{\"end\":85501,\"start\":85485},{\"end\":85842,\"start\":85826},{\"end\":86214,\"start\":86155},{\"end\":86871,\"start\":86848},{\"end\":87375,\"start\":87359},{\"end\":87807,\"start\":87751},{\"end\":88156,\"start\":88053},{\"end\":88633,\"start\":88554},{\"end\":89005,\"start\":88923},{\"end\":89319,\"start\":89279},{\"end\":89542,\"start\":89485},{\"end\":90042,\"start\":90029},{\"end\":90387,\"start\":90298},{\"end\":90636,\"start\":90559},{\"end\":91300,\"start\":91284},{\"end\":92126,\"start\":92105},{\"end\":92447,\"start\":92428},{\"end\":92680,\"start\":92655},{\"end\":93057,\"start\":92984},{\"end\":93486,\"start\":93421},{\"end\":93887,\"start\":93793},{\"end\":94213,\"start\":94209},{\"end\":73976,\"start\":73962},{\"end\":74780,\"start\":74776},{\"end\":75447,\"start\":75399},{\"end\":77006,\"start\":76966},{\"end\":77905,\"start\":77830},{\"end\":78450,\"start\":78364},{\"end\":79135,\"start\":79129},{\"end\":79636,\"start\":79533},{\"end\":80247,\"start\":80187},{\"end\":80926,\"start\":80838},{\"end\":86253,\"start\":86216},{\"end\":90051,\"start\":90044},{\"end\":92149,\"start\":92128}]"}}}, "year": 2023, "month": 12, "day": 17}