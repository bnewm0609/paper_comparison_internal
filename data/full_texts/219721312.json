{"id": 219721312, "updated": "2023-10-06 14:07:17.263", "metadata": {"title": "Noise or Signal: The Role of Image Backgrounds in Object Recognition", "authors": "[{\"first\":\"Kai\",\"last\":\"Xiao\",\"middle\":[]},{\"first\":\"Logan\",\"last\":\"Engstrom\",\"middle\":[]},{\"first\":\"Andrew\",\"last\":\"Ilyas\",\"middle\":[]},{\"first\":\"Aleksander\",\"last\":\"Madry\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 6, "day": 17}, "abstract": "We assess the tendency of state-of-the-art object recognition models to depend on signals from image backgrounds. We create a toolkit for disentangling foreground and background signal on ImageNet images, and find that (a) models can achieve non-trivial accuracy by relying on the background alone, (b) models often misclassify images even in the presence of correctly classified foregrounds--up to 87.5% of the time with adversarially chosen backgrounds, and (c) more accurate models tend to depend on backgrounds less. Our analysis of backgrounds brings us closer to understanding which correlations machine learning models use, and how they determine models' out of distribution performance.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2006.09994", "mag": "3036438747", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iclr/XiaoEIM21", "doi": null}}, "content": {"source": {"pdf_hash": "5c63fc87400a4d3afea63ab8a068a47249f815c2", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2006.09994v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "59a468d7181a820b3e9bf76c19761e1b7bc37910", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/5c63fc87400a4d3afea63ab8a068a47249f815c2.txt", "contents": "\nNoise or Signal: The Role of Image Backgrounds in Object Recognition\n\n\nKai Xiao kaix@mit.edu \nMIT\n\n\nLogan Engstrom engstrom@mit.edu \nMIT\n\n\nAndrew Ilyas ailyas@mit.edu \nMIT\n\n\nAleksander M\u0105dry madry@mit.edu \nMIT\n\n\nNoise or Signal: The Role of Image Backgrounds in Object Recognition\n\nWe assess the tendency of state-of-the-art object recognition models to depend on signals from image backgrounds. We create a toolkit for disentangling foreground and background signal on ImageNet images, and find that (a) models can achieve non-trivial accuracy by relying on the background alone, (b) models often misclassify images even in the presence of correctly classified foregrounds-up to 87.5% of the time with adversarially chosen backgrounds, and (c) more accurate models tend to depend on backgrounds less. Our analysis of backgrounds brings us closer to understanding which correlations machine learning models use, and how they determine models' out of distribution performance.Preprint. Under review.\n\nIntroduction\n\nObject recognition models are typically trained to minimize loss on a given dataset, and evaluated by the accuracy they attain on the corresponding test set. In this paradigm, model performance can be improved by incorporating any generalizing correlation between images and their labels into decision-making. However, the actual model reliability and robustness depend on the specific set of correlations that is used, and on how those correlations are combined. Indeed, outside of the training distribution, model predictions can deviate wildly from human expectations either due to relying on correlations that humans do not perceive [JLT18; Ily+19; Jac+19], or due to overusing correlations, such as texture [Gei+19;Bak+18] and color [YS02], that humans do use (but to a lesser degree). Characterizing the correlations that models depend on thus has important implications for understanding model behavior, in general.\n\nImage backgrounds are a natural source of correlation between images and their labels in object recognition. Indeed, prior work has shown that models may use backgrounds in classification [Zha+07; RSG16; ZXY17; RZT18; Bar+19; SSF19; Sag+20], and suggests that even human vision makes use of image context for scene and object recognition [Tor03]. In this work, we aim to obtain a deeper understanding of how current state-of-the-art image classifiers utilize image backgrounds. Specifically, we investigate the extent to which models rely on them, the implications of this reliance, and how models' use of backgrounds has evolved over time. Concretely:\n\n\u2022 We create a variety of datasets that help disentangle the impacts of foreground and background signals on classification. The test datasets and a public challenge related to them are available at https://github.com/MadryLab/backgrounds_challenge. \u2022 Using the aforementioned toolkit, we characterize models' reliance on image backgrounds.\n\nWe find that image backgrounds alone suffice for fairly successful classification and that changing background signals decreases average-case performance. In fact, we further show that by choosing backgrounds in an adversarial manner, we can make standard models misclassify 87.5% of images as the background class.  Table 1. We label each image with its pre-trained ResNet-50 classification-green, if corresponding with the original label; red, if not. The model correctly classifies the image as \"insect\" when given: the original image, only the background, and two cases where the original foreground is present but the background changes. Note that, in particular, the model fails in two cases when the original foreground is present but the background changes (as in MIXED-NEXT or ONLY-FG).\n\n\u2022 We demonstrate that standard models not only use but require backgrounds for correctly classifying large portions of test sets (35% on our benchmark).\n\n\u2022 We study the impact of backgrounds on classification for a variety of classifiers, and find that more accurate models tend to simultaneously exploit background correlations more and have greater robustness to changes in image background.\n\n\nMethodology\n\nTo properly gauge image backgrounds' role in image classification, we construct a synthetic dataset for disentangling background from foreground signal: ImageNet-9.\n\nBase dataset: ImageNet-9. We organize a subset of ImageNet into a new dataset with nine coarsegrained classes and call it ImageNet-9 (IN-9) 1 . To create it, we group together ImageNet classes sharing an ancestor in the WordNet [Mil95] hierarchy. We separate out foregrounds and backgrounds via the annotated bounding boxes provided in ImageNet, and remove all candidate images whose bounding boxes are unavailable. We use coarse-grained classes because there are not enough images with bounding boxes to use the standard labels, and the resulting IN-9 dataset has 5045 training and 450 testing images per class. We describe the dataset creation process in detail in Appendix A.\n\nVariations of ImageNet-9 From this base set of images, which we call the ORIGINAL version of IN-9, we create seven other synthetic variations designed to understand the impact of backgrounds. We visualize these variations in Figure 1, and provide a detailed reference in Table 1. These subdatasets of IN-9 differ only in how they process the foregrounds and backgrounds of each constituent image.\n\nLarger dataset: IN-9L We finally create a dataset called IN-9L that consists of all the images in ImageNet corresponding to the classes in ORIGINAL (rather than just the images that have associated bounding boxes). We leverage this larger dataset to train better generalizing models. \n\n\nQuantifying Reliance on Background Signals\n\nWith ImageNet-9 in hand, we now assess the role of image backgrounds in classification.\n\nBackgrounds suffice for classification. Prior work has found that models are able to make accurate predictions based on backgrounds alone; we begin by directly quantifying this ability. Looking at the ONLY-BG-T, ONLY-BG-B, and NO-FG datasets, we find (cf. Figure 2) that models trained on these background-only training sets generalize reasonably well to both their corresponding test sets and the ORIGINAL test set (around 40-50% for every model, far above the baseline of 11% representing random classification). Our results confirm that image backgrounds contain signal that models can accurately classify with.\n\nModels exploit background signal for classification. We discover that models can misclassify due to background signal, especially when the background class does not match that of the foreground. As a demonstration, we study model accuracies on the MIXED-RAND dataset, where image backgrounds are randomized and thus provide no information about the correct label. By comparing test accuracies on MIXED-RAND and MIXED-SAME 2 , where images have class-consistent backgrounds, we can measure classifiers' dependence on the correct background. We denote the resulting accuracy gap between MIXED-SAME and MIXED-RAND as the BG-GAP; this difference represents the drop in Table 2: Performance of state-of-the-art computer vision models on select test sets of ImageNet-9. We include both pre-trained ImageNet models and models of different architectures that we train on IN-9L. The BG-GAP is defined as the difference in test accuracy between MIXED-SAME and MIXED-RAND and helps assess the tendency of such models to rely on background signal. Architectures are sorted by their test accuracies on ImageNet and ORIGINAL for pre-trained and IN-9L-trained models, respectively. Shaded in grey are the two architectures that can be directly compared across datasets (ResNet-50 and Wide-ResNet-50x2).  model accuracy due to changing the class signal from the background. In Table 2, we observe a BG-GAP of 13-22% and 6-14% for models trained on IN-9L and ImageNet, respectively, suggesting that backgrounds often mislead state-of-the-art models even when the correct foreground is present.\n\nOur results indicate that ImageNet-trained models are less dependent on backgrounds than their IN-9L-trained counterparts-they have a smaller (but still significant) BG-GAP, and perform worse when predicting solely based on backgrounds (i.e., on the ONLY-BG-T dataset). An initial hypothesis could be that ImageNet-trained models' lesser dependence results from having either (a) a more fine-grained class structure, or (b) more datapoints than IN-9L. However, a preliminary investigation (Appendix B) ablating both fine-grainedness and dataset size does not find evidence supporting either explanation. Therefore, understanding why pre-trained ImageNet models rely less on backgrounds than IN-9L models remains an open question.\n\nModels are vulnerable to adversarial backgrounds. To understand how worst-case backgrounds impact models' performance, we evaluate model robustness to adversarially chosen backgrounds. We find that 87.5% of foregrounds are susceptible to such backgrounds; that is, for these foregrounds, there is a background that causes the classifier to classify the resulting foreground-background combination as the background class. For a finer grained look, we also evaluate image backgrounds based on their attack success rate (ASR), i.e., how frequently they cause models to predict the (background) class in the presence of a conflicting foreground class. As an example, Figure 3 shows the five backgrounds with the highest ASR for the insect class-these backgrounds (extracted from insect images in ORIGINAL) fool a IN-9L-trained ResNet-50 model into predicting insect on up to 52% of non-insect foregrounds. We plot a histogram of ASR over all insect backgrounds in Figure 4-it has a long tail. Similar results are observed for other classes as well (cf. Appendix G).\n\nTraining on MIXED-RAND reduces background dependence. Next, we explore how to reduce models' dependence on background. To this end, we train models on MIXED-RAND, a synthetic dataset where background signals are decorrelated from class labels. As we would expect, MIXED-RAND-trained models extract less signal from backgrounds: evaluation results show that MIXED-RAND models perform poorly (15% accuracy-barely higher than random) on datasets with only backgrounds and no foregrounds (ONLY-BG-T or ONLY-BG-B).\n\nIndeed, such models are also more accurate on datasets where backgrounds do not match foregrounds.\n\nIn Figure 5, we observe that a MIXED-RAND-trained model has 17.3% higher accuracy than its ORIGINAL-trained counterpart on MIXED-RAND, and 22.3% higher accuracy on MIXED-NEXT, a dataset where background signals class-consistently mismatch foregrounds. (Recall that MIXED-insect, 66.55% 42% 44% 45% 45% 52% Figure 3: The adversarial backgrounds that most frequently fool IN-9L-trained models into classifying a given foreground as insect, ordered by the percentage of foregrounds fooled. The total portion of images that can be fooled (by any background from this class) is 66.55%. NEXT images have foregrounds from class y mixed with backgrounds from class y + 1, labeled as class y.) The MIXED-RAND-trained model also has little variation (at most 3.8%) in accuracy across all five test sets that contain the correct foreground.\n\nQualitatively, the MIXED-RAND-trained model also appears to place more relative importance on foreground pixels than the ORIGINAL-trained model; the saliency maps of the two models in Figure 6 show that the MIXED-RAND-trained model's saliency maps highlight more foreground pixels than those of ORIGINAL-trained models.\n\nA fine grained look at dependence on backgrounds. We now analyze models' reliance on backgrounds at an image-by-image level and ask: for which images does introducing backgrounds help or hurt classifiers' performance? To this end, for each image in ORIGINAL, we decompose how models use foreground and background signals by examining classifiers' predictions on the corresponding image in MIXED-RAND and ONLY-BG-T. Here, we use the MIXED-RAND and ONLY-BG-T predictions as a proxy for which class the foreground and background signals (alone) point towards, respectively. We categorize each image based on how its background and foreground signals impact classification; we list the categories in Table 3 and show the counts for each category as a histogram per classifier in Figure 7. Our results show that while few backgrounds induce misclassification (see Appendix H for examples), a large fraction of images require backgrounds for correct classification-approximately 35% on the ORIGINAL trained classifiers, as calculated by combining the \"BG Required\" and \"BG+FG Required\" categories.\n\nFurther insights derived from IN-9 are discussed in the Appendix D. We focus on key findings in this section, but also include more comprehensive results and examples of other questions that can be explored by using the toolkit of IN-9 in the Appendix.     Table 3). The model trained on ORIGINAL needs the background for correct classification on 35% of images (measured by adding \"BG Required\" and \"BG+FG Required), while a model trained on MIXED-RAND is much less reliant on background. The model trained on ONLY-BG-T requires the background most, as expected; however, the model often misclassifies both the full image and the background, so the \"BG Irrelevant\" subset is still sizable. : Measuring progress on each of the synthetic ImageNet-9 datasets with respect to progress on the standard ImageNet test set. Higher accuracy on ImageNet generally corresponds to higher accuracy on each of the constructed datasets, but the rate at which accuracy grows varies based on the types of features present in each dataset. Each pre-trained model corresponds to a vertical line on the plot-we mark ResNet-50 and MobileNet-v3s models for reference.\n\nAs a first step towards answering these questions, we study the progress made by ImageNet models on our synthetic IN-9 dataset variations. In Figure 8 we plot accuracy on our synthetic datasets against ImageNet accuracy for each of the architectures considered. As evidenced by the lines of best fit in Figure 8, accuracy increases on the original ImageNet benchmark generally correspond to accuracy increases on all of the synthetic datasets. This includes the ONLY-BG datasets-indicating that models do improve at extracting correlations from image backgrounds.\n\nIndeed, the ONLY-BG trend observed in Figure 8 suggests that either (a) image classification models can only attain their reported accuracies in the presence of background signals; or (b) these models carry an implicit bias towards features in the background, as a result of optimization technique, model class, etc.-in this case, we may need explicit regularization (e.g., through distributionally robust optimization [Sag+20] or related techniques) to obtain models invariant to these background features.\n\nStill, models' relative improvement in accuracy across dataset variants is promising-models improve on classifying ONLY-BG-T at a slower (absolute) rate than MIXED-RAND, MIXED-SAME and MIXED-NEXT. Furthermore, the performance gap between the MIXED datasets and the others (most notably, between MIXED-RAND and MIXED-SAME; between MIXED-NEXT and MIXED-RAND; and consequently between MIXED-NEXT and MIXED-SAME) trends towards closing, indicating that models not only are becoming better at using foreground features, but also are becoming more robust to misleading background features (MIXED-RAND and MIXED-NEXT).\n\nOverall, the accuracy trends observed from testing ImageNet models on our synthetic datasets reveal that better models (a) are capable of exploiting background correlations, but (b) are increasingly robust to changes in background, suggesting that invariance to background features may not necessarily come at the cost of benchmark accuracy.\n\n\nRelated Work\n\nWe focus our discussion here on the works most closely related to ours, specifically those investigating contextual bias or background dependence in computer vision (for a more extensive survey of and explicit comparison to prior work, see Appendix E). Prior work has explored the more general phenomenon of contextual bias [TE11; Kho+12; MTW12; SSF19], including studying its prevalence and developing methods to mitigate it. For image backgrounds specifically, prior works show that background correlations can be predictive [Tor03], and that backgrounds can influence model decisions to varying degrees [Zha+07; RSG16; ZXY17; BHP18; RZT18; Bar+19; Sag+20]. The work most similar to ours is that of Zhu, Xie, and Yuille [ZXY17], who also analyze ImageNet classification (focusing on the older, AlexNet model). They find that the AlexNet model achieve small but non-trivial test accuracy on a dataset similar to our ONLY-BG-B dataset. While sufficient for establishing that backgrounds can be used for classification, this dataset also introduces biases by adding large black rectangular patches to all of the images. In comparison to [ZXY17] (and the prior works mentioned earlier): (a) we create a more extensive toolkit that allows us to measure not just model performance without foregrounds but also the relative influence of foregrounds and backgrounds on model predictions; (b) we control for the effect of image artifacts via the MIXED-SAME dataset; (c) we study model robustness to adversarial backgrounds; (d) we study a larger and more recent set of image classifiers [He+16; ZK16; TL19], and how improvements they give on ImageNet relate to background correlations.\n\n\nDiscussion and Conclusion\n\nIn this work, we study the extent to which classifiers rely on image backgrounds. To this end, we create a toolkit for measuring the precise role of background and foreground signal that involves constructing new test datasets that contain different amounts of each. Through these datasets we establish both the usefulness of background signal and the tendency of our models to depend on backgrounds, even when relevant foreground features are present. Our results show that our models are not robust to changes in the background, either in the adversarial case, or in the average case.\n\nAs most ImageNet images have human-recognizable foreground objects, our models appear to rely on background more than humans on that dataset. The fact that models can be fooled by adversarial background changes on 87.5% of all images highlights how poorly computer vision models may perform in an out-of-distribution setting. However, contextual information like the background can still be useful in certain settings. After all, humans do use backgrounds as context in visual processing, and the background may be necessary if the foreground is blurry or distorted [Tor03]. Therefore, reliance on background is a nuanced question that merits further study.\n\nOn one hand, our findings provide evidence that models succeed by using background correlations, which may be undesirable in some applications. On the other hand, we find that advances in classifiers have given rise to models that use foregrounds more effectively and are more robust to changes in the background. To obtain even more robust models, we may want to draw inspiration from successes in training on the MIXED-RAND dataset (a dataset designed to neutralize background signal-cf. Table 1), related data-augmentation techniques [SSF19], and training algorithms like distributionally robust optimization [Sag+20] and model-based robust learning [RHP20]. Overall, the toolkit and findings in this work help us to better understand models and to monitor our progress toward the goal of reliable machine learning.\n\n\nA Datasets Details\n\nWe choose the following 9 high-level classes.\n\n\nClass\n\nWordNet  All datasets used in the paper are balanced by randomly removing images from classes that are over-represented. We only keep as many images as the smallest post-modification synthetic dataset, so all synthetic datasets (except IN-9L) have the same number of images. We also use a custom GUI to manually process the test set to improve data quality. For IN-9L, the only difference from using the corresponding classes in the original ImageNet dataset is that we balance the dataset.\n\nFor all images: we apply the following filters before adding each image to our datasets.\n\n\u2022 The image must have bounding box annotations.\n\n\u2022 For simplicity, each image must have exactly one bounding box. A large majority of images that have bounding box annotations satisfy this.\n\nFor images needing a properly segmented foreground: This includes the 3 MIXED datasets, ONLY-FG, and NO-FG. We filter out images based on the following criteria.\n\n\u2022 Because images are cropped before they are fed into models, we require that less than 50% of the bounding box is removed by the crop, to ensure that the foreground still exists. Almost all images pass this filter. \u2022 The OpenCV function cv2.grabCut (used to extract the foreground shape) must work on the image. We remove images where it fails. \u2022 For the test set only, we manually remove images with foreground segmentations that retain a significant portion of the background signal. \u2022 For the test set only, we manually remove foreground segmentations that are very bad (e.g.\n\nthe segmentation selects part of the image, and that part doesn't contain the foreground object).\n\nFor images needing only background signal: This includes ONLY-BG-B and ONLY-BG-T. In this case, we apply the following criteria:\n\n\u2022 The bounding box must not be too big (more than 90% of the image). The intent here is to avoid ONLY-BG-B images being just a large black rectangle. \u2022 For the test set only, we manually remove ONLY-BG images that still have an instance of the class even after removing the bounding box. This occurs when the bounding boxes are imperfect or incomplete (e.g. only one of two dogs in an image is labeled with a bounding box).\n\nCreating the ONLY-BG-T dataset: We first make a \"tiled\" version of the background by finding the largest rectangular strip (horizontal or vertical) outside the bounding box, and tiling the entire image with that strip. We then replace the removed foreground with the tiled background. A visual Figure 9: Visualization of how ONLY-BG-T is created.\n\nexample is provided in Figure 9. We purposefully choose not to use deep-learning-based inpainting techniques such as [SFS18] to replace the removed foreground, as such methods could lead to biases that the inpainting model has learned from the data. For example, an inpainting model may learn that the best way to inpaint a missing chunk of a flower is to place an insect there, which is something we want to avoid.\n\n\nB Explaining the Decreased BG-GAP of pre-trained ImageNet models\n\nWe investigate two possible explanations for why pre-trained ImageNet models have a smaller BG-GAP than models trained on ImageNet-9. Understanding this phenomenon can help inform how models should be trained to be more background-robust. We find slight improvements to background-robustness from training on more fine-grained classes, and even smaller improvements from training on larger datasets. These two factors thus do not completely explain the increased background-robustness of pre-trained ImageNet models-understanding this further is an interesting empirical mystery to study.\n\n\nB.1 The Effect of Fine-grainedness on the BG-GAP\n\nOne possible explanation is that training models to distinguish between finer-grained classes forces them to focus more on the foreground, which contains relevant features for making those fine-grained distinctions, than the background, which may be fairly similar across sub-classes of a high-level class. This suggests that asking models to solve more fine-grained tasks could improve model robustness to background changes.\n\nTo test the effect of fine-grainedness on ImageNet-9, we make a related dataset called IN-9LB that uses the same 9 high-level classes and can be cleanly modified into more fine-grained versions. Specifically, for IN-9LB we choose exactly 16 sub-classes for each high-level class, for a total of 144 ImageNet classes. To create successively more fine-grained versions of the IN-9LB dataset, we group every n sub-classes together into a higher-level class, for n \u2208 {1, 2, 4, 8, 16}. Here, n = 1 corresponds to keeping all 144 ImageNet classes as they are, while n = 16 corresponds to only having 9 high-level classes, like ImageNet-9. Because we keep all images from those original ImageNet classes, this dataset is the same size as IN-9L.\n\nWe train models on IN-9LB at different levels of fine-grainedness and evaluate the BG-GAP of those models in Figure 10. We find that fine-grained models have a smaller BG-GAP as well as better performance on MIXED-NEXT, but the improvement is very slight and also comes at the cost of decreased accuracy on ORIGINAL. The BG-GAP of the most fine-grained classifier is 2.3% smaller than the BG-GAP of the most coarse-grained classifier, showing that fine-grainedness does improve background-robustness. However, the improvement is still small compared to the size of the BG-GAP (which is 13.3% for the fine-grained classifier).\n\n\nB.2 The Effect of Larger Dataset Size on the BG-GAP\n\nA second possible explanation for why pre-trained ImageNet models have a smaller BG-GAP is that training on larger datasets is important for background-robustness. To evaluate this possibility, we train models on different-sized subsets of IN-9LB. The largest dataset we train on is the full IN-9LB dataset, which is 4 times as large as IN-9, and the smallest is 1/4 as large as IN-9. Figure 11 shows that increasing the dataset size does increase overall performance but does not noticeably decrease the BG-GAP.  The largest training set we use is the full IN-9LB dataset, which is 4 times larger than ImageNet-9. While performance on all test datasets improves as the amount of training data increases, the BG-GAP has almost the same size regardless of the amount of training data used.\n\nIt is possible that even more data for these classes (more than is available from ImageNet) is needed to improve background-robustness, or that more training data (from other classes) is the cause of the increased background-robustness of pre-trained ImageNet models. Understanding this further would be an interesting direction.\n\n\nB.3 Summary of methods investigated to reduce the BG-GAP\n\nIn Figure 12, we compare the BG-GAP of ResNet-50 models trained on different datasets and with different methods to a ResNet-50 pre-trained on ImageNet. We explore p -robust training, increasing dataset size, and making the classification task more fine-grained, and find that none of these methods reduces the BG-GAP as much as pre-training on ImageNet. The only method that reduces the BG-GAP significantly more is training on MIXED-RAND. Furthermore, the same trends hold true if we measure the difference between MIXED-SAME and MIXED-NEXT as opposed to the BG-GAP (the difference between MIXED-SAME and MIXED-RAND).\n\n\nC Training details\n\nFor all models, we use fairly standard training settings for ImageNet-style models. We train for 200 epochs using SGD with a batch size of 256, a learning rate of 0.1 (with learning rate drops every 50 epochs), a momentum parameter of 0.9, a weight decay of 1e\u22124, and data augmentation (random resized crop, random horizontal flip, and color jitter). Unless specified, we always use a standard ResNet-50 architecture [He+16]. For the experiment depicted in Figure 11, we found that using a smaller learning rate of 0.01 was necessary for training to converge on the smaller training sets. Thus, we used that same learning rate for all models in Figure 11.\n\n\nD Additional Evaluation Results\n\nWe include full results of training models on every synthetic IN-9 variation and then testing them on every synthetic IN-9 variation in Table 5. In addition to being more comprehensive, this table can help answer a variety of questions, of which we provide two examples here.\n\nHow much information is leaked from the size of the foreground bounding box?\n\nThe scale of an object already gives signal correlated with the object class [Tor03]. Even though they are designed to avoid having foreground signal, the background-only datasets ONLY-BG-B and ONLY-BG-T may inadvertently leak information about object scale due to the bounding box sizes being recognizable.\n\nTo gauge the extent of this leakage, we can measure how models trained on datasets where only the foreground signal has useful correlation (MIXED-RAND or ONLY-FG) perform on the backgroundonly test sets. We find that there is small signal leakage from bounding box size alone-a model trained on ONLY-FG achieves about 23% background-only test accuracy, suggesting that it is able to exploit the signal leakage to some degree. A model trained on MIXED-RAND achieves about 15% background-only test accuracy, just slightly better than random, perhaps because it is harder for models to measure (and thus, make use of) object scale when training on MIXED-RAND.\n\nThe existence of a small amount of information leakage in this case shows the importance of comparing MIXED-SAME (as opposed to just ORIGINAL) with MIXED-RAND and MIXED-NEXT when assessing model dependence on backgrounds. Indeed, the MIXED datasets may contain (1) image processing artifacts, such as rough edges from the foreground processing, and (2) small traces of the original background. This makes it important to control for both factors when measuring how models react to varying background signal.\n\nHow does more training data affect model performance with and without object shape?\n\nWe already show closely related results on the effect of more training data on the BG-GAP in Figure 11. Here, we compare model test performance on the NO-FG and ONLY-BG-B test sets. Both replace the foreground with black, but only NO-FG retains the foreground shape.\n\nBy comparing the models trained on ORIGINAL and IN-9L (4x more training data), we find that 1. The ORIGINAL-trained model performs similarly on NO-FG and ONLY-BG-B, indicating that it does not use object shape effectively.\n\n2. The IN-9L-trained model performs about 13% better on NO-FG than ONLY-BG-B, showing that it uses object shape more effectively.\n\nThus, this suggests that more training data may allow models to learn to use object shape more effectively. Understanding this phenomena further could help inform model training and dataset collection if the goal is to train models that are able to leverage shape effectively.   What about other ways of modifying the background signal?\n\nOne can modify the background in various other ways-for example, instead of replacing the background with black as in ONLY-FG, the background can be blurred as in the BG-BLURRED image of Figure 13. As expected, blurred backgrounds are still slightly correlated with the correct class. Thus, test accuracies for standard models on this dataset are higher than on ONLY-FG, but lower than on MIXED-SAME (which has signal from random class-aligned backgrounds that are not blurred). While we do not investigate all possible methods of modifying background signal, we believe that the variations we do examine in ImageNet-9 already improve our understanding of how background signals matter. Investigating other variations could provide an even more nuanced understanding of what parts of the background are most important.\n\n\nE Additional Related Works and Explicit Comparisons\n\nThere has been prior work on mitigating contextual bias in image classification, the influence of background signals on various datasets, and techniques like foreground segmentation that we leverage.\n\n\nOriginal\n\nOnly-FG BG-Blurred Figure 13: Backgrounds can also be modified in other ways; for example, it can be blurred. Our evaluations on this dataset show similar results.\n\nMitigating Contextual Bias: [Kho+12] focuses on mitigating dataset-specific contextual bias and proposes learning SVMs with both general weights and dataset-specific weights, while [MTW12] creates an out-of-context detection task with 209 out-of-context images and suggests using graphical models to solve it. [SSF19] focuses on the role of co-occurring objects as context in the MS-COCO dataset, and uses object removal to show that (a) models can still predict a removed object when only co-occurring objects are shown, and (b) special data-augmentation can mitigate this.\n\nUnderstanding the influence of backgrounds: For contextual bias from image backgrounds specifically, prior works have observed that the background of an image can influence model decisions to varying degrees. In particular, [Zha+07] find that (a) a bag-of-features object detection algorithm depends on image backgrounds in the PASCAL dataset and (b) using this algorithm on a training set with varying backgrounds leads to better generalization. [BHP18; Bar+19] collect new test datasets of animals and objects, respectively. [Bar+19] focus on object classes that also exist in ImageNet, and their new test set contains objects photographed in front of unconventional backgrounds and in unfamiliar orientations. Both works show that computer vision models experience significant accuracy drops when trained on data with one set of backgrounds and tested on data with another.\n\n[Sag+20] create a small synthetic dataset of Waterbirds, where waterbirds and landbirds from one dataset are combined with water and land backgrounds from another. They show that a model's reliance on spurious correlations with the background can be harmful for small subgroups of data where those spurious correlations no longer hold (e.g. landbirds on water backgrounds). Furthermore, they propose using distributionally robust optimization to reduce reliance on spurious correlations with the background, but their method assumes that the spurious correlation can be precisely specified in advance. [RZT18] analyzes background dependence for object detection (as opposed to classification) models on the MS-COCO dataset. They transplant an object from one image to another image, and find that object detection models may detect the transplanted object differently depending on its location, and that the transplanted object may also cause mispredictions on other objects in the image.\n\nExplicit Comparison to Prior Works: In comparison to prior works, our work contributes the following.\n\n\u2022 We develop a toolkit for analyzing the background dependence of ImageNet classifiers, the most common benchmark for computer vision progress. Only [ZXY17], which we compare to in Section 5, also focuses on ImageNet.\n\n\u2022 The test datasets we create separate and mix foreground and background signals in various ways (cf. Table 1), allowing us to study the sensitivity of models to these signals in a more fine-grained manner.\n\n\u2022 Our toolkit for separating foreground and background can be applied without humanannotated foreground segmentation, which prior works on MS-COCO and Waterbirds rely on. This is important because foreground segmentation annotations are hard to collect and do not exist for ImageNet.\n\n\u2022 We study the extent of background dependence in the extreme case of adversarial backgrounds.\n\n\u2022 We focus on better vision models, including ResNet [He+16], Wide ResNet [ZK16], and EfficientNet [TL19].\n\n\u2022 We evaluate how improvements on the ImageNet benchmark have affected background dependence (cf. Section 4).\n\nForeground Segmentation and Image Inpainting: In order to create IN-9 and its variants, we rely on OpenCV's implementation of the foreground segmentation algorithm GrabCut [RKB04]. Foreground segmentation is a branch of computer vision that seeks to automatically extract the foreground from an image [HGW01]. After finding the foreground, we remove it and simply replace the foreground with copies of parts of the background. Other works solve this problem, called image inpainting, either using exemplar-based methods [CPT04] or using deep learning [Yu+18;SFS18]. [SFS18] both detects the foreground for removal and inpaints the removed region. However, more advanced inpaintings techniques can be slow and inaccurate when the region that must be inpainted is relatively large [SFS18], which is the case for many ImageNet images. Exploring better ways of segementing the foreground and inpainting the removed foreground could improve our analysis toolkit further.\n\n\nF Additional examples of synthetic datasets\n\nWe randomly sample an image from each class, and display all synthetic variations of that image, as well as the predictions of a pre- \n\n\nG Adversarial Backgrounds\n\nWe include the 5 most fooling backgrounds for all classes, the fool rate for each of those 5 backgrounds, and the total fool rate across all backgrounds from that class (on the left of each row) here.  \n\n\nH Examples of Fooling Backgrounds in Unmodified Images\n\nWe visualize examples of images where the background of the full original image actually fools models in Figure 30. For these images, models classify the foreground alone correctly, but they predict the same wrong class on the full image and the background. We denote these images as \"BG Fools\" in Table 3 and Figure 7. While this category is relatively rare (accounting for just 3% of the ORIGINAL-trained model's predictions), they reveal a subset of original images where background signal hurts classifier performance. Qualitatively, we observe that these images all have confusing or misleading backgrounds. : Images that are incorrectly classified (as the class on the top row, which is the same class that their background alone from ONLY-BG-T is classified as), but are correctly classified (as the class on the bottom row) when the background is randomized. Note that these images have confusing backgrounds that could be associated with another class.\n\nFigure 1 :\n1Variations of the synthetic dataset ImageNet-9, as described in\n\nFigure 4 :\n4Histogram of insect backgrounds grouped by how often they cause (non-insect) foregrounds to be classified as insect by a IN-9L-trained model. We visualize the five backgrounds that fool the classifier on the largest percentage of images inFigure 3.\n\nFigure 5 :\n5We compare the test performance of a model trained on the synthetic MIXED-RAND dataset with a model trained on ORIGINAL. We evaluate these models on variants of IN-9 that contain identical foregrounds. For the ORIGINAL-trained model, test performance decreases significantly when the background signal is modified during testing. However, the MIXED-RAND-trained model is robust to background changes, albeit at the cost of lower accuracy on images from ORIGINAL.ImageOriginal Saliency Mixed-Rand Saliency Image Original Saliency Mixed-Rand Saliency\n\nFigure 6 :\n6Saliency maps for the the ORIGINAL and MIXED-RAND models on two images. As expected, the MIXED-RAND model appears to place more importance on foreground pixels.\n\nFigure 7 :\n7We categorize each test set image based on how a model classifies the full image, the background alone, and the foreground alone (cf.\n\nFigure 8\n8Figure 8: Measuring progress on each of the synthetic ImageNet-9 datasets with respect to progress on the standard ImageNet test set. Higher accuracy on ImageNet generally corresponds to higher accuracy on each of the constructed datasets, but the rate at which accuracy grows varies based on the types of features present in each dataset. Each pre-trained model corresponds to a vertical line on the plot-we mark ResNet-50 and MobileNet-v3s models for reference.\n\nFigure 10 :Figure 11 :\n1011We train models on IN-9LB at different levels of fine-grainedness (more training classes is more fine-grained). The BG-GAP, or the difference between the test accuracies on MIXED-SAME and MIXED-RAND, decreases as we make the classification task more fine-grained, but the decrease is small compared to the size of the BG-GAP. We train models on different-sized subsets of IN-9LB.\n\nFigure 12 :\n12We compare various different methods of training models and measure their BG-GAP, or the difference between MIXED-SAME and MIXED-RAND test accuracy. We find that(1)Pretrained IN models have surprisingly small BG-GAP. (2) Increasing fine-grainedness (IN-9LB Coarse vs. IN-9LB Fine) and dataset size (IN-9 vs. IN-9L) decreases the BG-GAP only slightly. (3) p -robust training does not help. (4) Training on MIXED-RAND (cf. Section 3 appears to be the most effective strategy for reducing the BG-GAP. For such a model, the MIXED-SAME and MIXED-RAND accuracies are nearly identical.\n\n\nMIXED-NEXT MIXED-RAND MIXED-SAME NO-FG ONLY-BG-B ONLY-BG-T ONLY-FG ORIGINAL IN-9L\n\nFigure 14 :Figure 15 :Figure 16 :Figure 17 :Figure 18 :Figure 19 :Figure 20 :Figure 21 :\n1415161718192021ImageNetImageNetImageNetImageNet-9 variations-Reptile. ImageNetImageNet-9 variations-Instrument. ImageNetImageNet-9 variations-Fish.\n\nFigure 29 :\n29Most adversarial backgrounds-Fish.\n\nFigure 30\n30Figure 30: Images that are incorrectly classified (as the class on the top row, which is the same class that their background alone from ONLY-BG-T is classified as), but are correctly classified (as the class on the bottom row) when the background is randomized. Note that these images have confusing backgrounds that could be associated with another class.\n\nTable 1 :\n1The 8 modified subdatasets created from ImageNet-9. The foreground detection method refers to how the pixels corresponding to the foreground are found. ImageNet annotation refers to the annotated bounding boxes found in ImageNet. GrabCut refers to the GrabCut algorithm[RKB04] as implemented in OpenCV2. Random backgrounds in the last three datasets are taken from ONLY-BG-T. For more details see Appendix A. We train models on each of the \"background-only\" datasets, then evaluate each on its corresponding test set as well as the ORIGINAL test set. Even though the model only learns from background signal, it achieves (much) better than random performance on both the corresponding test set and ORIGINAL. Here, random guessing would give 11.11% (the dotted line).Name \nForeground \nBackground \nForeground Detection Method \n\nORIGINAL \nUnmodified \nUnmodified \n-\nONLY-BG-B \nBlack \nUnmodified \nImageNet Annotation \nONLY-BG-T \nTiled background Unmodified \nImageNet Annotation \nNO-FG \nBlack \nUnmodified \nGrabCut \nONLY-FG \nUnmodified \nBlack \nGrabCut \nMIXED-SAME Unmodified \nRandom background of the same class GrabCut \nMIXED-RAND Unmodified \nRandom background of a random class GrabCut \nMIXED-NEXT Unmodified \nRandom background of the next class \nGrabCut \n\nONLY-BG-B \nONLY-BG-T \nNO-FG \n\n0 \n\n20 \n\n40 \n\n60 \n\n80 \n\n100 \n\nTraining dataset \n\nTest Accuracy \n\nTesting dataset \nSame as train \nORIGINAL \n\nFigure 2: \n\nTable 3 :\n3Prediction categories we study for a given image-model pair. For a given image, a model can make differing predictions based on the presence or absence of its foreground/background. We label each possible case based on how the background classification relates to the original image classification and the foreground classification. To proxy classifying full images, foregrounds, and backgrounds separately, we classify on ORIGINAL, MIXED-RAND, and ONLY-BG-T (respectively). \"BG Irrelevant\" demarcates images where the foreground classification result is identical to that of the full image (in terms of correctness).In the previous sections, we demonstrated that standard image classification models exploit signals from backgrounds. Considering that these models result from progress on standard computer vision benchmarks, a natural question is: to what extent have improvements on image classification benchmarks resulted from exploiting background correlations? And relatedly, how has model robustness to misleading background signals evolved over time?Label \nCorrect Prediction Correct Prediction Correct Prediction \non Full Image \non Foreground \non Background \n\nBG Required \n\n\n\nBG Fools \n\n\n\nBG+FG Required \n\n\nBG+FG Fools \n\n\n\nBG Irrelevant \n/ \n/ \n-\n\n4 Benchmark Progress and Background Dependence \n\n\n\nTable 4 :\n4The 9 classes of ImageNet-9.\n\nTable 5 :\n5The test accuracies, in percentages, of models trained on all variants of ImageNet-9.\n\n\nFigure 22: Most adversarial backgrounds-Dog.Figure 23: Most adversarial backgrounds-Bird.Figure 24: Most adversarial backgrounds-Vehicle.Figure 25: Most adversarial backgrounds-Reptile.Figure 26: Most adversarial backgrounds-Carnivore.Figure 27: Most adversarial backgrounds-Instrument.Figure 28: Most adversarial backgrounds-Primate.dog, 61.77% \n\n17% \n18% \n19% \n20% \n21% \n\nbird, 64.22% \n\n32% \n34% \n35% \n36% \n38% \n\nvehicle, 67.47% \n\n41% \n41% \n43% \n44% \n47% \n\nreptile, 54.94% \n\n22% \n22% \n23% \n25% \n28% \n\ncarnivore, 53.11% \n\n17% \n18% \n20% \n21% \n28% \n\ninstrument, 73.19% \n\n45% \n46% \n46% \n46% \n47% \n\nprimate, 63.83% \n\n25% \n27% \n28% \n33% \n39% \n\nfish, 69.69% \n\n43% \n44% \n44% \n44% \n46% \n\n\nThese classes are dog, bird, vehicle, reptile, carnivore, insect, instrument, primate, and fish.\nMIXED-SAME controls for artifacts from image processing present in MIXED-RAND. For further discussion, see Appendix D.\nAcknowledgementsThanks to Kuo-An \"Andy\" Wei, John Cherian, and anonymous conference reviewers for helpful comments on earlier versions of this work. The authors would also like to thank Guillaume LeClerc, Sam Park, and Kuo-An Wei for help with data labeling. KX was supported by the NDSEG Fellowship. LE was supported by the NSF Graduate Research Fellowship. AI was supported by the Open Phil AI Fellowship. Work supported in part by the NSF grants CCF-1553428, CNS-1815221, and the Microsoft Corporation. This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No. HR001120C0015.\nDeep convolutional networks do not classify based on global object shape. Nicholas Baker, PLOS Computational Biology. Nicholas Baker et al. \"Deep convolutional networks do not classify based on global object shape.\" In: PLOS Computational Biology. 2018.\n\nObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models. Andrei Barbu, Neural Information Processing Systems. Andrei Barbu et al. \"ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models\". In: Neural Information Processing Systems. 2019.\n\nRecognition in Terra Incognita. Sara Beery, Pietro Grant Van Horn, Perona, European Conference on Computer Vision (ECCV). Sara Beery, Grant van Horn, and Pietro Perona. \"Recognition in Terra Incognita\". In: European Conference on Computer Vision (ECCV). 2018.\n\nRegion Filling and Object Removal by Exemplar-Based Image Inpainting. Antonio Criminisi, Patrick P\u00e9rez, Kentaro Toyama, IEEE Transactions on Image Processing. Antonio Criminisi, Patrick P\u00e9rez, and Kentaro Toyama. \"Region Filling and Object Removal by Exemplar-Based Image Inpainting\". In: IEEE Transactions on Image Pro- cessing. 2004.\n\nImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness. Robert Geirhos, International Conference on Learning Representations. Robert Geirhos et al. \"ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness.\" In: International Conference on Learning Representations. 2019.\n\nDeep Residual Learning for Image Recognition. Kaiming He, Conference on Computer Vision and Pattern Recognition (CVPR). Kaiming He et al. \"Deep Residual Learning for Image Recognition\". In: Conference on Computer Vision and Pattern Recognition (CVPR). 2016.\n\nForeground segmentation using adaptive mixture models in color and depth. Michael Harville, Gaile Gordon, John Woodfill, IEEE Workshop on Detection and Recognition of Events in Video. Michael Harville, Gaile Gordon, and John Woodfill. \"Foreground segmentation using adaptive mixture models in color and depth\". In: IEEE Workshop on Detection and Recognition of Events in Video. 2001.\n\nAdversarial Examples Are Not Bugs, They Are Features. Andrew Ilyas, Neural Information Processing Systems (NeurIPS). Andrew Ilyas et al. \"Adversarial Examples Are Not Bugs, They Are Features\". In: Neural Information Processing Systems (NeurIPS). 2019.\n\nExcessive Invariance Causes Adversarial Vulnerability. Jorn-Henrik Jacobsen, International Contemporary on Learning Representations (ICLR). Jorn-Henrik Jacobsen et al. \"Excessive Invariance Causes Adversarial Vulnerability\". In: International Contemporary on Learning Representations (ICLR). 2019.\n\nWith friends like these, who needs adversaries. Saumya Jetley, Nicholas Lord, Philip Torr, Advances in Neural Information Processing Systems (NeurIPS). Saumya Jetley, Nicholas Lord, and Philip Torr. \"With friends like these, who needs adversaries?\" In: Advances in Neural Information Processing Systems (NeurIPS). 2018.\n\nUndoing the damage of dataset bias. Aditya Khosla, European Conference on Computer Vision (ECCV). Aditya Khosla et al. \"Undoing the damage of dataset bias\". In: European Conference on Computer Vision (ECCV). 2012.\n\nWordNet: a lexical database for English. George Miller, Communications of the ACM. George Miller. \"WordNet: a lexical database for English\". In: Communications of the ACM. 1995.\n\nContext models and out-ofcontext objects. Jin Choi Myung, Antonio Torralba, Alan S Willsky, Pattern Recognition Letters. Choi Myung Jin, Antonio Torralba, and Alan S. Willsky. \"Context models and out-of- context objects\". In: Pattern Recognition Letters. 2012.\n\nModel-Based Robust Deep Learning. Alexander Robey, Hamed Hassani, George J Pappas, arXiv:2005.10247.2020arXiv preprintAlexander Robey, Hamed Hassani, and George J. Pappas. \"Model-Based Robust Deep Learning\". In: arXiv preprint arXiv:2005.10247. 2020.\n\nGrabCut: Interactive Foreground Extraction Using Iterated Graph Cuts. Carsten Rother, Vladimir Kolmogorov, Andrew Blake, ACM Transactions on Graphics. Carsten Rother, Vladimir Kolmogorov, and Andrew Blake. \"GrabCut: Interactive Fore- ground Extraction Using Iterated Graph Cuts\". In: ACM Transactions on Graphics. 2004.\n\nWhy Should I Trust You?\": Explaining the Predictions of Any Classifier. Sameer Marco Tulio Ribeiro, Carlos Singh, Guestrin, International Conference on Knowledge Discovery and Data Mining. Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. \"\"Why Should I Trust You?\": Explaining the Predictions of Any Classifier\". In: International Conference on Knowl- edge Discovery and Data Mining. 2016.\n\nThe Elephant in the Room. Amir Rosenfeld, Richard S Zemel, John K Tsotsos, arXiv:1808.03305arXiv preprintAmir Rosenfeld, Richard S. Zemel, and John K. Tsotsos. \"The Elephant in the Room\". In: arXiv preprint arXiv:1808.03305. 2018.\n\nDistributionally Robust Neural Networks for Group Shifts: On the Importance of Regularization for Worst-Case Generalization. Shiori Sagawa, International Conference on Learning Representations. Shiori Sagawa et al. \"Distributionally Robust Neural Networks for Group Shifts: On the Importance of Regularization for Worst-Case Generalization\". In: International Conference on Learning Representations. 2020.\n\nAdversarial Scene Editing: Automatic Object Removal from Weak Supervision. Rakshith Shetty, Mario Fritz, Bernt Schiele, Neural Information Processing Systems (NeurIPS). Rakshith Shetty, Mario Fritz, and Bernt Schiele. \"Adversarial Scene Editing: Automatic Object Removal from Weak Supervision\". In: Neural Information Processing Systems (NeurIPS). 2018.\n\nNot Using the Car to See the Sidewalk-Quantifying and Controlling the Effects of Context in Classification and Segmentation. Rakshith Shetty, Bernt Schiele, Mario Fritz, Conference on Computer Vision and Pattern Recognition (CVPR). Rakshith Shetty, Bernt Schiele, and Mario Fritz. \"Not Using the Car to See the Sidewalk- Quantifying and Controlling the Effects of Context in Classification and Segmentation\". In: Conference on Computer Vision and Pattern Recognition (CVPR). 2019.\n\nUnbiased look at dataset bias. Antonio Torralba, Alexei Efros, CVPR 2011. Antonio Torralba and Alexei Efros. \"Unbiased look at dataset bias\". In: CVPR 2011. 2011, pp. 1521-1528.\n\nEfficientNet: Rethinking Model Scaling for Convolutional Neural Networks. Mingxing Tan, Quoc V Le, International Conference on Machine Learning (ICML). Mingxing Tan and Quoc V. Le. \"EfficientNet: Rethinking Model Scaling for Convolu- tional Neural Networks\". In: International Conference on Machine Learning (ICML). 2019.\n\nContextual Priming for Object Detection. Antonio Torralba, International Journal of Computer Vision (IJCV). Antonio Torralba. \"Contextual Priming for Object Detection\". In: International Journal of Computer Vision (IJCV). 2003.\n\nContribution of Color to Face Recognition. Andrew W Yip, Pawan Sinha, Andrew W. Yip and Pawan Sinha. \"Contribution of Color to Face Recognition\". In: Perception. 2002.\n\nGenerative Image Inpainting with Contextual Attention. Jiahui Yu, Conference on Computer Vision and Pattern Recognition (CVPR). Jiahui Yu et al. \"Generative Image Inpainting with Contextual Attention\". In: Conference on Computer Vision and Pattern Recognition (CVPR). 2018.\n\nLocal features and kernels for classification of texture and object categories: A comprehensive study. Jianguo Zhang, International Journal of Computer Vision. Jianguo Zhang et al. \"Local features and kernels for classification of texture and object categories: A comprehensive study\". In: International Journal of Computer Vision (IJCV). 2007.\n\nWide Residual Networks. Sergey Zagoruyko, Nikos Komodakis, British Machine Vision Conference (BMVC). Sergey Zagoruyko and Nikos Komodakis. \"Wide Residual Networks\". In: British Ma- chine Vision Conference (BMVC). 2016.\n\nObject Recognition without and without Objects. Zhuotun Zhu, Lingxi Xie, Alan Yuille, International Joint Conference on Artificial Intelligence. Zhuotun Zhu, Lingxi Xie, and Alan Yuille. \"Object Recognition without and without Objects\". In: International Joint Conference on Artificial Intelligence. 2017.\n", "annotations": {"author": "[{\"end\":100,\"start\":72},{\"end\":139,\"start\":101},{\"end\":174,\"start\":140},{\"end\":212,\"start\":175}]", "publisher": null, "author_last_name": "[{\"end\":80,\"start\":76},{\"end\":115,\"start\":107},{\"end\":152,\"start\":147},{\"end\":191,\"start\":186}]", "author_first_name": "[{\"end\":75,\"start\":72},{\"end\":106,\"start\":101},{\"end\":146,\"start\":140},{\"end\":185,\"start\":175}]", "author_affiliation": "[{\"end\":99,\"start\":95},{\"end\":138,\"start\":134},{\"end\":173,\"start\":169},{\"end\":211,\"start\":207}]", "title": "[{\"end\":69,\"start\":1},{\"end\":281,\"start\":213}]", "venue": null, "abstract": "[{\"end\":999,\"start\":283}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1735,\"start\":1727},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1742,\"start\":1735},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":1759,\"start\":1753},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2284,\"start\":2277},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4541,\"start\":4534},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":14789,\"start\":14781},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":16376,\"start\":16369},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":16571,\"start\":16564},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":16985,\"start\":16978},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":18711,\"start\":18704},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":19341,\"start\":19334},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":19417,\"start\":19409},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":19457,\"start\":19450},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":22336,\"start\":22329},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":27431,\"start\":27424},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":28137,\"start\":28130},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":31862,\"start\":31854},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":32014,\"start\":32007},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":32143,\"start\":32136},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":32634,\"start\":32626},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":32864,\"start\":32857},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":33889,\"start\":33882},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":34529,\"start\":34522},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":35241,\"start\":35234},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":35261,\"start\":35255},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":35286,\"start\":35280},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":35579,\"start\":35572},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":35708,\"start\":35701},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":35927,\"start\":35920},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":35958,\"start\":35951},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":35964,\"start\":35958},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":35973,\"start\":35966},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":36186,\"start\":36179},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":41531,\"start\":41524}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":37876,\"start\":37800},{\"attributes\":{\"id\":\"fig_2\"},\"end\":38138,\"start\":37877},{\"attributes\":{\"id\":\"fig_3\"},\"end\":38700,\"start\":38139},{\"attributes\":{\"id\":\"fig_4\"},\"end\":38874,\"start\":38701},{\"attributes\":{\"id\":\"fig_5\"},\"end\":39021,\"start\":38875},{\"attributes\":{\"id\":\"fig_6\"},\"end\":39496,\"start\":39022},{\"attributes\":{\"id\":\"fig_7\"},\"end\":39904,\"start\":39497},{\"attributes\":{\"id\":\"fig_8\"},\"end\":40498,\"start\":39905},{\"attributes\":{\"id\":\"fig_9\"},\"end\":40582,\"start\":40499},{\"attributes\":{\"id\":\"fig_10\"},\"end\":40821,\"start\":40583},{\"attributes\":{\"id\":\"fig_11\"},\"end\":40871,\"start\":40822},{\"attributes\":{\"id\":\"fig_12\"},\"end\":41242,\"start\":40872},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":42655,\"start\":41243},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":43973,\"start\":42656},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":44014,\"start\":43974},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":44112,\"start\":44015},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":44796,\"start\":44113}]", "paragraph": "[{\"end\":1937,\"start\":1015},{\"end\":2591,\"start\":1939},{\"end\":2932,\"start\":2593},{\"end\":3729,\"start\":2934},{\"end\":3883,\"start\":3731},{\"end\":4124,\"start\":3885},{\"end\":4304,\"start\":4140},{\"end\":4984,\"start\":4306},{\"end\":5382,\"start\":4986},{\"end\":5668,\"start\":5384},{\"end\":5802,\"start\":5715},{\"end\":6418,\"start\":5804},{\"end\":7996,\"start\":6420},{\"end\":8727,\"start\":7998},{\"end\":9791,\"start\":8729},{\"end\":10302,\"start\":9793},{\"end\":10402,\"start\":10304},{\"end\":11233,\"start\":10404},{\"end\":11554,\"start\":11235},{\"end\":12647,\"start\":11556},{\"end\":13795,\"start\":12649},{\"end\":14360,\"start\":13797},{\"end\":14869,\"start\":14362},{\"end\":15482,\"start\":14871},{\"end\":15825,\"start\":15484},{\"end\":17520,\"start\":15842},{\"end\":18136,\"start\":17550},{\"end\":18795,\"start\":18138},{\"end\":19615,\"start\":18797},{\"end\":19683,\"start\":19638},{\"end\":20183,\"start\":19693},{\"end\":20273,\"start\":20185},{\"end\":20322,\"start\":20275},{\"end\":20464,\"start\":20324},{\"end\":20627,\"start\":20466},{\"end\":21208,\"start\":20629},{\"end\":21307,\"start\":21210},{\"end\":21437,\"start\":21309},{\"end\":21862,\"start\":21439},{\"end\":22210,\"start\":21864},{\"end\":22627,\"start\":22212},{\"end\":23284,\"start\":22696},{\"end\":23763,\"start\":23337},{\"end\":24502,\"start\":23765},{\"end\":25129,\"start\":24504},{\"end\":25973,\"start\":25185},{\"end\":26304,\"start\":25975},{\"end\":26984,\"start\":26365},{\"end\":27662,\"start\":27007},{\"end\":27973,\"start\":27698},{\"end\":28051,\"start\":27975},{\"end\":28360,\"start\":28053},{\"end\":29018,\"start\":28362},{\"end\":29527,\"start\":29020},{\"end\":29612,\"start\":29529},{\"end\":29880,\"start\":29614},{\"end\":30104,\"start\":29882},{\"end\":30235,\"start\":30106},{\"end\":30573,\"start\":30237},{\"end\":31393,\"start\":30575},{\"end\":31648,\"start\":31449},{\"end\":31824,\"start\":31661},{\"end\":32400,\"start\":31826},{\"end\":33278,\"start\":32402},{\"end\":34268,\"start\":33280},{\"end\":34371,\"start\":34270},{\"end\":34590,\"start\":34373},{\"end\":34798,\"start\":34592},{\"end\":35083,\"start\":34800},{\"end\":35179,\"start\":35085},{\"end\":35287,\"start\":35181},{\"end\":35398,\"start\":35289},{\"end\":36365,\"start\":35400},{\"end\":36547,\"start\":36413},{\"end\":36779,\"start\":36577},{\"end\":37799,\"start\":36838}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":3258,\"start\":3251},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":5264,\"start\":5257},{\"end\":7092,\"start\":7085},{\"end\":7788,\"start\":7781},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":12259,\"start\":12252},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":12913,\"start\":12906},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":19294,\"start\":19287},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":27841,\"start\":27834},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":34701,\"start\":34694},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":37143,\"start\":37136}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1013,\"start\":1001},{\"attributes\":{\"n\":\"2\"},\"end\":4138,\"start\":4127},{\"attributes\":{\"n\":\"3\"},\"end\":5713,\"start\":5671},{\"attributes\":{\"n\":\"5\"},\"end\":15840,\"start\":15828},{\"attributes\":{\"n\":\"6\"},\"end\":17548,\"start\":17523},{\"end\":19636,\"start\":19618},{\"end\":19691,\"start\":19686},{\"end\":22694,\"start\":22630},{\"end\":23335,\"start\":23287},{\"end\":25183,\"start\":25132},{\"end\":26363,\"start\":26307},{\"end\":27005,\"start\":26987},{\"end\":27696,\"start\":27665},{\"end\":31447,\"start\":31396},{\"end\":31659,\"start\":31651},{\"end\":36411,\"start\":36368},{\"end\":36575,\"start\":36550},{\"end\":36836,\"start\":36782},{\"end\":37811,\"start\":37801},{\"end\":37888,\"start\":37878},{\"end\":38150,\"start\":38140},{\"end\":38712,\"start\":38702},{\"end\":38886,\"start\":38876},{\"end\":39031,\"start\":39023},{\"end\":39520,\"start\":39498},{\"end\":39917,\"start\":39906},{\"end\":40672,\"start\":40584},{\"end\":40834,\"start\":40823},{\"end\":40882,\"start\":40873},{\"end\":41253,\"start\":41244},{\"end\":42666,\"start\":42657},{\"end\":43984,\"start\":43975},{\"end\":44025,\"start\":44016}]", "table": "[{\"end\":42655,\"start\":42021},{\"end\":43973,\"start\":43726},{\"end\":44796,\"start\":44449}]", "figure_caption": "[{\"end\":37876,\"start\":37813},{\"end\":38138,\"start\":37890},{\"end\":38700,\"start\":38152},{\"end\":38874,\"start\":38714},{\"end\":39021,\"start\":38888},{\"end\":39496,\"start\":39033},{\"end\":39904,\"start\":39525},{\"end\":40498,\"start\":39920},{\"end\":40582,\"start\":40501},{\"end\":40821,\"start\":40689},{\"end\":40871,\"start\":40837},{\"end\":41242,\"start\":40885},{\"end\":42021,\"start\":41255},{\"end\":43726,\"start\":42668},{\"end\":44014,\"start\":43986},{\"end\":44112,\"start\":44027},{\"end\":44449,\"start\":44115}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5219,\"start\":5211},{\"end\":6068,\"start\":6060},{\"end\":9401,\"start\":9393},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":9698,\"start\":9690},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":10415,\"start\":10407},{\"end\":10718,\"start\":10710},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":11427,\"start\":11419},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":12339,\"start\":12331},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":13947,\"start\":13939},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":14108,\"start\":14100},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":14408,\"start\":14400},{\"end\":22166,\"start\":22158},{\"end\":22243,\"start\":22235},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":24622,\"start\":24613},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":25579,\"start\":25570},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":26377,\"start\":26368},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":27473,\"start\":27464},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":27661,\"start\":27652},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":29716,\"start\":29707},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":30771,\"start\":30762},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":31689,\"start\":31680},{\"attributes\":{\"ref_id\":\"fig_12\"},\"end\":36952,\"start\":36943},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":37156,\"start\":37148}]", "bib_author_first_name": "[{\"end\":45735,\"start\":45727},{\"end\":46016,\"start\":46010},{\"end\":46273,\"start\":46269},{\"end\":46287,\"start\":46281},{\"end\":46575,\"start\":46568},{\"end\":46594,\"start\":46587},{\"end\":46609,\"start\":46602},{\"end\":46947,\"start\":46941},{\"end\":47259,\"start\":47252},{\"end\":47546,\"start\":47539},{\"end\":47562,\"start\":47557},{\"end\":47575,\"start\":47571},{\"end\":47910,\"start\":47904},{\"end\":48169,\"start\":48158},{\"end\":48456,\"start\":48450},{\"end\":48473,\"start\":48465},{\"end\":48486,\"start\":48480},{\"end\":48765,\"start\":48759},{\"end\":48985,\"start\":48979},{\"end\":49162,\"start\":49159},{\"end\":49182,\"start\":49175},{\"end\":49197,\"start\":49193},{\"end\":49199,\"start\":49198},{\"end\":49422,\"start\":49413},{\"end\":49435,\"start\":49430},{\"end\":49451,\"start\":49445},{\"end\":49453,\"start\":49452},{\"end\":49708,\"start\":49701},{\"end\":49725,\"start\":49717},{\"end\":49744,\"start\":49738},{\"end\":50030,\"start\":50024},{\"end\":50058,\"start\":50052},{\"end\":50380,\"start\":50376},{\"end\":50399,\"start\":50392},{\"end\":50401,\"start\":50400},{\"end\":50413,\"start\":50409},{\"end\":50415,\"start\":50414},{\"end\":50713,\"start\":50707},{\"end\":51072,\"start\":51064},{\"end\":51086,\"start\":51081},{\"end\":51099,\"start\":51094},{\"end\":51477,\"start\":51469},{\"end\":51491,\"start\":51486},{\"end\":51506,\"start\":51501},{\"end\":51864,\"start\":51857},{\"end\":51881,\"start\":51875},{\"end\":52087,\"start\":52079},{\"end\":52097,\"start\":52093},{\"end\":52099,\"start\":52098},{\"end\":52376,\"start\":52369},{\"end\":52606,\"start\":52600},{\"end\":52608,\"start\":52607},{\"end\":52619,\"start\":52614},{\"end\":52787,\"start\":52781},{\"end\":53111,\"start\":53104},{\"end\":53377,\"start\":53371},{\"end\":53394,\"start\":53389},{\"end\":53622,\"start\":53615},{\"end\":53634,\"start\":53628},{\"end\":53644,\"start\":53640}]", "bib_author_last_name": "[{\"end\":45741,\"start\":45736},{\"end\":46022,\"start\":46017},{\"end\":46279,\"start\":46274},{\"end\":46302,\"start\":46288},{\"end\":46310,\"start\":46304},{\"end\":46585,\"start\":46576},{\"end\":46600,\"start\":46595},{\"end\":46616,\"start\":46610},{\"end\":46955,\"start\":46948},{\"end\":47262,\"start\":47260},{\"end\":47555,\"start\":47547},{\"end\":47569,\"start\":47563},{\"end\":47584,\"start\":47576},{\"end\":47916,\"start\":47911},{\"end\":48178,\"start\":48170},{\"end\":48463,\"start\":48457},{\"end\":48478,\"start\":48474},{\"end\":48491,\"start\":48487},{\"end\":48772,\"start\":48766},{\"end\":48992,\"start\":48986},{\"end\":49173,\"start\":49163},{\"end\":49191,\"start\":49183},{\"end\":49207,\"start\":49200},{\"end\":49428,\"start\":49423},{\"end\":49443,\"start\":49436},{\"end\":49460,\"start\":49454},{\"end\":49715,\"start\":49709},{\"end\":49736,\"start\":49726},{\"end\":49750,\"start\":49745},{\"end\":50050,\"start\":50031},{\"end\":50064,\"start\":50059},{\"end\":50074,\"start\":50066},{\"end\":50390,\"start\":50381},{\"end\":50407,\"start\":50402},{\"end\":50423,\"start\":50416},{\"end\":50720,\"start\":50714},{\"end\":51079,\"start\":51073},{\"end\":51092,\"start\":51087},{\"end\":51107,\"start\":51100},{\"end\":51484,\"start\":51478},{\"end\":51499,\"start\":51492},{\"end\":51512,\"start\":51507},{\"end\":51873,\"start\":51865},{\"end\":51887,\"start\":51882},{\"end\":52091,\"start\":52088},{\"end\":52102,\"start\":52100},{\"end\":52385,\"start\":52377},{\"end\":52612,\"start\":52609},{\"end\":52625,\"start\":52620},{\"end\":52790,\"start\":52788},{\"end\":53117,\"start\":53112},{\"end\":53387,\"start\":53378},{\"end\":53404,\"start\":53395},{\"end\":53626,\"start\":53623},{\"end\":53638,\"start\":53635},{\"end\":53651,\"start\":53645}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":54476941},\"end\":45906,\"start\":45653},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":202777185},\"end\":46235,\"start\":45908},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":49744838},\"end\":46496,\"start\":46237},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":757375},\"end\":46833,\"start\":46498},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":54101493},\"end\":47204,\"start\":46835},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":206594692},\"end\":47463,\"start\":47206},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":6353268},\"end\":47848,\"start\":47465},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":146121358},\"end\":48101,\"start\":47850},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":53192805},\"end\":48400,\"start\":48103},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":49668608},\"end\":48721,\"start\":48402},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":9286850},\"end\":48936,\"start\":48723},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":1671874},\"end\":49115,\"start\":48938},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":8354810},\"end\":49377,\"start\":49117},{\"attributes\":{\"doi\":\"arXiv:2005.10247.2020\",\"id\":\"b13\"},\"end\":49629,\"start\":49379},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":6202829},\"end\":49950,\"start\":49631},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":13029170},\"end\":50348,\"start\":49952},{\"attributes\":{\"doi\":\"arXiv:1808.03305\",\"id\":\"b16\"},\"end\":50580,\"start\":50350},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":208176471},\"end\":50987,\"start\":50582},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":46946901},\"end\":51342,\"start\":50989},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":56482344},\"end\":51824,\"start\":51344},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":2777306},\"end\":52003,\"start\":51826},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":167217261},\"end\":52326,\"start\":52005},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":1073705},\"end\":52555,\"start\":52328},{\"attributes\":{\"id\":\"b23\"},\"end\":52724,\"start\":52557},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":4072789},\"end\":52999,\"start\":52726},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":1486613},\"end\":53345,\"start\":53001},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":15276198},\"end\":53565,\"start\":53347},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":15846993},\"end\":53872,\"start\":53567}]", "bib_title": "[{\"end\":45725,\"start\":45653},{\"end\":46008,\"start\":45908},{\"end\":46267,\"start\":46237},{\"end\":46566,\"start\":46498},{\"end\":46939,\"start\":46835},{\"end\":47250,\"start\":47206},{\"end\":47537,\"start\":47465},{\"end\":47902,\"start\":47850},{\"end\":48156,\"start\":48103},{\"end\":48448,\"start\":48402},{\"end\":48757,\"start\":48723},{\"end\":48977,\"start\":48938},{\"end\":49157,\"start\":49117},{\"end\":49699,\"start\":49631},{\"end\":50022,\"start\":49952},{\"end\":50705,\"start\":50582},{\"end\":51062,\"start\":50989},{\"end\":51467,\"start\":51344},{\"end\":51855,\"start\":51826},{\"end\":52077,\"start\":52005},{\"end\":52367,\"start\":52328},{\"end\":52779,\"start\":52726},{\"end\":53102,\"start\":53001},{\"end\":53369,\"start\":53347},{\"end\":53613,\"start\":53567}]", "bib_author": "[{\"end\":45743,\"start\":45727},{\"end\":46024,\"start\":46010},{\"end\":46281,\"start\":46269},{\"end\":46304,\"start\":46281},{\"end\":46312,\"start\":46304},{\"end\":46587,\"start\":46568},{\"end\":46602,\"start\":46587},{\"end\":46618,\"start\":46602},{\"end\":46957,\"start\":46941},{\"end\":47264,\"start\":47252},{\"end\":47557,\"start\":47539},{\"end\":47571,\"start\":47557},{\"end\":47586,\"start\":47571},{\"end\":47918,\"start\":47904},{\"end\":48180,\"start\":48158},{\"end\":48465,\"start\":48450},{\"end\":48480,\"start\":48465},{\"end\":48493,\"start\":48480},{\"end\":48774,\"start\":48759},{\"end\":48994,\"start\":48979},{\"end\":49175,\"start\":49159},{\"end\":49193,\"start\":49175},{\"end\":49209,\"start\":49193},{\"end\":49430,\"start\":49413},{\"end\":49445,\"start\":49430},{\"end\":49462,\"start\":49445},{\"end\":49717,\"start\":49701},{\"end\":49738,\"start\":49717},{\"end\":49752,\"start\":49738},{\"end\":50052,\"start\":50024},{\"end\":50066,\"start\":50052},{\"end\":50076,\"start\":50066},{\"end\":50392,\"start\":50376},{\"end\":50409,\"start\":50392},{\"end\":50425,\"start\":50409},{\"end\":50722,\"start\":50707},{\"end\":51081,\"start\":51064},{\"end\":51094,\"start\":51081},{\"end\":51109,\"start\":51094},{\"end\":51486,\"start\":51469},{\"end\":51501,\"start\":51486},{\"end\":51514,\"start\":51501},{\"end\":51875,\"start\":51857},{\"end\":51889,\"start\":51875},{\"end\":52093,\"start\":52079},{\"end\":52104,\"start\":52093},{\"end\":52387,\"start\":52369},{\"end\":52614,\"start\":52600},{\"end\":52627,\"start\":52614},{\"end\":52792,\"start\":52781},{\"end\":53119,\"start\":53104},{\"end\":53389,\"start\":53371},{\"end\":53406,\"start\":53389},{\"end\":53628,\"start\":53615},{\"end\":53640,\"start\":53628},{\"end\":53653,\"start\":53640}]", "bib_venue": "[{\"end\":45769,\"start\":45743},{\"end\":46061,\"start\":46024},{\"end\":46357,\"start\":46312},{\"end\":46655,\"start\":46618},{\"end\":47009,\"start\":46957},{\"end\":47324,\"start\":47264},{\"end\":47647,\"start\":47586},{\"end\":47965,\"start\":47918},{\"end\":48241,\"start\":48180},{\"end\":48552,\"start\":48493},{\"end\":48819,\"start\":48774},{\"end\":49019,\"start\":48994},{\"end\":49236,\"start\":49209},{\"end\":49411,\"start\":49379},{\"end\":49780,\"start\":49752},{\"end\":50139,\"start\":50076},{\"end\":50374,\"start\":50350},{\"end\":50774,\"start\":50722},{\"end\":51156,\"start\":51109},{\"end\":51574,\"start\":51514},{\"end\":51898,\"start\":51889},{\"end\":52155,\"start\":52104},{\"end\":52434,\"start\":52387},{\"end\":52598,\"start\":52557},{\"end\":52852,\"start\":52792},{\"end\":53159,\"start\":53119},{\"end\":53446,\"start\":53406},{\"end\":53710,\"start\":53653}]"}}}, "year": 2023, "month": 12, "day": 17}