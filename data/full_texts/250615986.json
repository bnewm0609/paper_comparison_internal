{"id": 250615986, "updated": "2022-10-15 13:22:06.607", "metadata": {"title": "Wnet: Audio-Guided Video Object Segmentation via Wavelet-Based Cross- Modal Denoising Networks", "authors": "[{\"first\":\"Wenwen\",\"last\":\"Pan\",\"middle\":[]},{\"first\":\"Haonan\",\"last\":\"Shi\",\"middle\":[]},{\"first\":\"Zhou\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Jieming\",\"last\":\"Zhu\",\"middle\":[]},{\"first\":\"Xiuqiang\",\"last\":\"He\",\"middle\":[]},{\"first\":\"Zhigeng\",\"last\":\"Pan\",\"middle\":[]},{\"first\":\"Lianli\",\"last\":\"Gao\",\"middle\":[]},{\"first\":\"Jun\",\"last\":\"Yu\",\"middle\":[]},{\"first\":\"Fei\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Qi\",\"last\":\"Tian\",\"middle\":[]}]", "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Audio-Guided video object segmentation is a challenging problem in visual analysis and editing, which automatically separates foreground objects from the background in a video sequence according to the referring audio expressions. However, existing referring video object segmentation works mainly focus on the guidance of text-based referring expressions, due to the lack of modeling the semantic representations of audio-video interaction contents. In this paper, we consider the problem of audio-guided video semantic segmentation from the viewpoint of end-to-end denoising encoder-decoder network learning. We propose the wavelet-based encoder network to learn the cross-modal representations of the video contents with audio-form queries. Specifically, we adopt the multi-head cross-modal attention layers to explore the potential relations of video and query contents. A 2-dimension discrete wavelet trans-form is merged into the transformer encoder to decompose the audio-video features. Next, we maximize mutual information between the encoded features and multi-modal features after cross-modal attention layers to enhance the au-dio guidance. Then, a self attention-free decoder network is developed to generate the target masks with frequency-domain transforms. In addition, we construct the first large-scale audio-guided video semantic segmentation dataset. The extensive experiments show the effectiveness of our method11Code is available at: https://github.com/asudahkzj/Wnet.git.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/PanSZZ0PGYW022", "doi": "10.1109/cvpr52688.2022.00138"}}, "content": {"source": {"pdf_hash": "78f18dbd0ffeebbc4b78278fa989260c9846993e", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "8b2df71816200480a82c0dcfd9936c3884d24422", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/78f18dbd0ffeebbc4b78278fa989260c9846993e.txt", "contents": "\nWnet: Audio-Guided Video Object Segmentation via Wavelet-Based Cross-Modal Denoising Networks\n\n\nWenwen Pan \nZhejiang University\n\n\nHaonan Shi \nZhejiang University\n\n\nZhou Zhao zhaozhou@zju.edu.cn \nZhejiang University\n\n\nJieming Zhu jiemingzhu@ieee.org \nHuawei Noah's Ark Lab\n\n\nXiuqiang He hexiuqiang1@huawei.com \nHuawei Noah's Ark Lab\n\n\nZhigeng Pan zgpan@hznu.edu.cn \nHangzhou Normal University\n\n\nLianli Gao lianli.gao@uestc.edu.cn \nThe University of Electronic Science and Technology of China\n\n\nJun Yu yujun@hdu.edu.cn \nHangzhou Dianzi University\n\n\nFei Wu wufei@cs.zju.edu.cn \nZhejiang University\n\n\nShanghai Institute for Advanced Study of Zhejiang University\n7 Huawei CloudAI\n\nQi Tian tian.qi1@huawei.com \nWnet: Audio-Guided Video Object Segmentation via Wavelet-Based Cross-Modal Denoising Networks\n10.1109/CVPR52688.2022.00138\nAudio-Guided video object segmentation is a challenging problem in visual analysis and editing, which automatically separates foreground objects from the background in a video sequence according to the referring audio expressions. However, existing referring video object segmentation works mainly focus on the guidance of text-based referring expressions, due to the lack of modeling the semantic representations of audio-video interaction contents. In this paper, we consider the problem of audio-guided video semantic segmentation from the viewpoint of end-toend denoising encoder-decoder network learning. We propose the wavelet-based encoder network to learn the crossmodal representations of the video contents with audio-form queries. Specifically, we adopt the multi-head cross-modal attention layers to explore the potential relations of video and query contents. A 2-dimension discrete wavelet transform is merged into the transformer encoder to decompose the audio-video features. Next, we maximize mutual information between the encoded features and multi-modal features after cross-modal attention layers to enhance the audio guidance. Then, a self attention-free decoder network is developed to generate the target masks with frequencydomain transforms. In addition, we construct the first largescale audio-guided video semantic segmentation dataset. The extensive experiments show the effectiveness of our method 1 . \u2020 Equal contribution. * Corresponding Author. 1 Code is available at: https://github.com/asudahkzj/Wnet.git a green colored parrot is standing left with the group of parrots in the person hand (emphasis)   (emphasis) (emphasis) Segmentation MasksFigure 1. The audio-guided video object segmentation task.\n\nIntroduction\n\nReferring video object segmentation aims to segment video objects referred by given language expressions, which has attracted wide attention due to its applicability to many practical problems including video analysis and video editing [33,35,49,50,61]. Currently, most referring video object segmentation approaches mainly focus on the guidance of text-guided referring expressions [18,19,31,33,35,49,61,63], which can learn the multi-modal representation from the interaction network layer, and then generate the object masks to the given text references. The existing works have achieved promising performance in text-based video object segmentation, but they may still be ineffectively applied to the audio-guided video object segmentation due to the lack of modeling the semantic representation of audio-video interaction contents.\n\nThe audio-guided video analysis is a simulation of human cognition, comparing with the text-guided analysis [44]. Humankind use speech exclusively long before the invention of writing. People also learn and use language in the real world, as to collaborate, describe and relate their visual environment, talk about each other, and so on. Furthermore, in the natural scene, audio interaction is more convenient and common than text interaction. Although audio inputs can be converted to text inputs through ASR models [3,4,46], the process will produce unavoidable losses. Since Harwath and Glass's collection of spoken captions for Flickr8k [14], more works address cognitive and linguistic questions [8,[10][11][12]. Other work addresses applied tasks, including multi-modal retrieval [22], cross-modality alignment [13,27], retrieving speech in different languages using images as a pivot modality [1,26,39], and speech-to-speech retrieval [1,39]. Our work focuses on the audio-guided video object segmentation tasks, shown as Fig. 1. The audio guidance often contains rich semantic information, such as the accent, emotion and speed. These extra factors can facilitate the object segmentation. The same object can correspond to different pronunciations, while the same pronunciation can point to different objects. Thus, the simple extension of the existing segmentation works based on textbased guidance is difficult for modeling the semantic representation of audio-video interaction contents. Inspired by MulT [51], we use multi-head cross-modal attention layers to fuse the video embeddings and audio embeddings. Different from the MulT model [51], we extend dimensions of inputs and apply it to large-scale natural language datasets. The cross-modal transformers referred to text embeddings are all removed.\n\nOne other bottleneck is the noise problem, derived from acquisition noise and fusing noise [7]. For the acquisition noises, we use a pre-trained MFCC model [5] to extract acoustic features, which is widely used in automatic speech and speaker recognition. In this paper, we focus on the processing of fusing noise. There is a large gap between video and audio representations. The joint representations reflect important information considering multi-modal alignment. Audio and video features have different redundant parts (i.e. irrelevant phonemes and pixels), likewise termed as noise. These noises are difficult to handle only by convolution operations and attention mechanisms in the time domain. As mentioned in [29], noises are likely to concentrate at high frequencies. Recently, Fnet [30] has been proposed to learn the frequency-domain-level representation with Fourier transforms for recognition tasks, while it only aims to speed up the encoder architectures but fails to obtain improvement in performances. Low-pass filtering on Fourier analysis cannot effectively distinguish the high-frequency parts of the required signal from the highfrequency interference caused by noise. If the low-pass filtering is too narrow, parts of the required signal are treated as noise and its morphological information is erased, which leads to the distortion of the original signal [45].\n\nMotivated by this, we integrate the 2-dimension discrete wavelet (DWT) transform into the transformer encoder, which replaces self-attention layers with DWT layers. The DWT denoising has proved its effectiveness in image denoising [25,45,52], but has not been used in multi-modal representation yet to our knowledge. We are the first to devise the DWT-transformer for the audio-visual joint representation to filter the noise and outliers, as a priori. The layers of the whole transformer encoder are reduced, which obtains a sizable performance boost in terms of speed and model consumption. Inspired by the AMDIM [2], we maximize mutual information between the encoded features and multi-modal features after the cross-modal attention to enhance the audio guidance.\n\nThe main contributions of this paper are as follows: (i) Unlike the previous studies, we study the problem of audio-guided video object segmentation from the viewpoint of end-to-end denoising encoder-decoder network learning. (ii) We propose the wavelet-based encoder network to learn the cross-modal representations of the video contents with audio-form queries. (iii) We construct a large-scale dataset for audio-guided video object segmentation and validate the effectiveness of our proposed method through extensive experiments.\n\n\nRelated Work\n\n\nReferring Expression object Segmentation\n\nThe referring expression segmentation task has attracted increasing research interest [18,19,31,33,35,49,61,63] in recent years. Hu et al. [18] formulate this task as an image-region-wise classification problem. Li et al. [31] employ multi-scale image features from multiple convolutional layers. Qiu et al. [41] further enhance visual features and introduce an adversarial mechanism. Some works [33,35,49,50,61] make more interactions between the image and natural language query. Furthermore, the attention module [61,63] is introduced to the segmentation task. To enhance the accuracy, further works successfully model the dependencies of cross-modal information [20], informative words of the expression [21] and localization information of the referent instances [24]. Moreover, Luo et al. [34] achieve a joint learning of referring expression comprehension and segmentation. [28] extends technologies to video data and incorporated temporal coherency. For the video data, existing methods commonly employ dynamic convolutions [9,54] to adaptively generate convolutional filters, or leverage cross-modal attention [38,55,62] to compute the correlations among input visual and linguistic embeddings However, these works cannot handle the noise problem of audio-video joint representations.\n\n\nSpeech-Based Video Analysis\n\nComparing with the text-guided video analysis, audioguided analysis is a more precise simulation of human cognition to the world [44]. Actually, people use speech exclusively long before the invention of writing. Harwath et al. [14] collect spoken captions for Flickr8k, and then much  [7,12,17,47] begins to attach importance to this task. Some works emphasize the cognitive and linguistic questions, such as understanding how different learned layers correspond to visual stimuli [8,10], learning linguistic units [11,12] or how visually grounded representations can help understand lexical competition in phonemic processing [15]. Ramon Sanabria et al. [44] propose dual encoder models that can be used for efficient multimodal retrieval. However, these works take less consideration of video object segmentation.\n\n\nAudio-Guided-VOS Dataset (AVOS)\n\nThere are previous works that constructed referring segmentation datasets for videos. Gavrilyuk et al. [9] extended the A2D [58] and J-HMDB [23] datasets with natural sentences. Seo et al. constructed the first large-scale referring video object segmentation dataset called RVOS [48].\n\nTo facilitate audio-based video object segmentation, we have constructed a large-scale audio-guided dataset, Audio-Guided-VOS (AVOS) 2 , with referring audio expressions as Tab. 1. AVOS is the extension of RVOS [48], A2D [58] and J-HMDB [23]. We select the three datasets for their rich scene information. To obtain audio annotations, we employ 36 speakers to read the sentences totally. To ensure the recording quality, all the speakers are required to read proficiently, do not stammer, stuck and other situations. The sampling rate is 44,100K or above, the sampling number is 16 bits, and the speaking speed is 100-150 words per minute. Speaking speed should be normal speaking speed, or TV announcer speaking speed. The word accuracy of text files and audio files is not less than 99% under manual checking. The average length of each recording is 5 to 6 seconds, about 28 hours in total. Moreover, we have run two rounds of inspections. We not only correct the pronunciation in the recordings, but also correct grammar and spelling errors in the original texts. The ratio of the training set, the test set and the validation set is 75 : 15 : 10.\n\n\nProposed Method\n\nWe present a video sequence as\nv = {v i } n i=1 ,\nwhere v i is the pre-extracted visual feature of the i-th frame and n is the frame number of the video. Each video is associated with an audio query, denoted by q = {q i } m i=1 where q i is the feature of i-th frame and m is the frame number of the audio. The goal of the audio-guided video object segmentation is to predict binary segmentation masks\nS = {S i \u2208 {0, 1} W o \u00d7H o } n i=1 .\n2 https://drive.google.com/drive/folders/Audio-Guide-Segmentation\n\n\nAnalysis on Wavelet Transform\n\nAs to the convolutional neural network, each convolutional layer is composed of several convolutional units, and the parameters of each convolutional unit are optimized by back propagation algorithm. Convolution operations aim to extract different features of inputs, represented as follows.\nW (\u03c4 ) = \u221e \u2212\u221e f (t)g(\u03c4 \u2212 t)dt.(1)\nThe convolution kernel in the convolution layer is relatively fixed. Audio-video joint representations contain rich timefrequency characteristics, which are more suitable for window functions that vary in the time-frequency domain. The wavelet can be represented as follows.\nW (a, \u03c4 ) = 1 \u221a a \u221e \u2212\u221e f (t)\u03c8( t \u2212 \u03c4 a )dt,(2)\nwhere scaling function \u03c8 a,\u03c4 (t) = a \u2212 1 2 \u03c8( t\u2212\u03c4 a ) and a is the scale, which is inversely proportional to the frequency. The operation of the traditional convolution layer and wavelet have the commonality. The difference is g(\u03c4 \u2212 t) and \u03c8( t\u2212\u03c4 a ). Audio and video features have different redundant parts (i.e. irrelevant phoneme and pixel), termed as noises. The noises from the video and audio inputs are distributed among most features after the cross-modal attention. These noises are difficult to handle only by convolution operations in the time domain. As mentioned in [29], noises are likely to concentrate at high frequencies. Fnet [30] proposes to use Fourier sublayers to replace the self-attention layers. However, low-pass filtering on Fourier analysis cannot effectively distinguish the high-frequency part of the required signal from the high-frequency interference caused by noise. Wavelet can well retain the peak value and mutation part of the useful signal required in the original signal. It has good time-frequency localization characteristics and can be expressed linearly as:\nW x = W f + W e ,(3)\nwhere W e is the wavelet coefficients controlled by noise. We can use threshold quantization to reconstruct denoising joint representations. Furthermore, we can obtain improvements by replacing self-attention layers with the DWT layers in terms of the model consumption and speed.\n\n\nOverview\n\nAs Fig. 2 Figure 2. The whole framework for our segmentation model.\n\nTo include spatial information of the visual feature, we augment 3-dimensional spatial coordinates following [56], de-\nnoted as v = {v i } n i=1 .\nThe output of the backbone is B \u2208 R n\u00d7c\u00d7H\u00d7W . c represents the original video dimension. A 1\u00d71 convolution is used to reduce the dimension to R n\u00d7d\u00d7H\u00d7W . We then flatten the dimensions as d \u00d7 (n \u00d7 H \u00d7 W). Audio Encoder. MFCCs are the features widely used in automatic speech and speaker recognition. Following [37], we capture information about lower frequencies than higher frequencies by non-linear scaling and thus acts as a human ear. A set of MFCCs is encoded as a multi-hot vector and projected onto an embedding space using the 1D convolution, denoted as q = {q i } m i=1 . Transformer Encoder and Decoder. We devise a transformer encoder-decoder framework for our audio-guided video object segmentation model. The model is in endto-end manners. The transformer encoder is employed to learn the cross-modal representations of the video contents with audio-form queries. We first apply the layer normalization to the visual features and audio features, respectively. Next, we devise a wavelet-based cross-modal module to fuse the two modalities and achieve denoised joint representations. Each encoder layer consists of a multi-head attention module [53] and a fully connected feed-forward network. Then, we maximize the mutual information between the cross-modal representations and the encoded representations. During this stage, the temporal order is the same as the order of the initial input.\n\nThe transformer decoder aims to generate the top pixel features that can represent the target object of each frame. Motivated by Fnet [30], we also replace the self-attention sublayers with simple linear transformations. The self attention-free decoder can better handle audio-video encoding. Besides the Fourier layers, we follow the standard ar-chitecture of the transformer, using multi-headed encoderdecoder attention mechanisms. Following [56], the decoder then takes a small fixed number of learned positional embeddings (object queries) as inputs, and attends to the encoder output. The overall predictions follow the input frame order. We remove all self-attention layers in the transformer encoder-decoder framework to reduce model computation. Details of the transformer are in 4.3 and 4.4. Object Sequence Segmentation. The module aims to predict the mask sequence for the target object. We capture the object predictions O, backbone features B and encoded feature maps E from the previous layers, shown in Fig. 2. First, we employ an attention module to calculate the similarity map between O and E. Following [56], we only compute the features of its corresponding frame. Next, we fuse the similarity map, B and E of the corresponding frames, following the DETR [6]. B \u2208 R n\u00d7c\u00d7H\u00d7W , E \u2208 R d\u00d7n\u00d7(H\u00d7W) , O \u2208 R n\u00d7d , where n denotes the frame numbers, c and d denote the dimension. Then, we use a deformable convolution as the last layer of the fusion. Thus, the mask features for the target object of different frames are achieved. Finally, the 3D convolution, which three 3D convolutional layers and group normalization layers [57] with ReLU activation function, is employed to obtain the mask sequence.\n\n\nDWT-Based Transformer Encoder\n\nComparing with the text-guided semantic segmentation, audio-based segmentation suffers from severe noise problems [44], derived from acquisition noise and fusing noise. For the acquisition noises, we use a pre-trained MFCC models [37] to extract acoustic features. For the fusing noise, we propose a DWT-based transformer encoder to realize multimodal encoding and joint feature denoising.\n\nWe consider visual modality and audio modality, with  two potentially non-aligned sequences from each of them, denoted as v p \u2208 R T v \u00d7d v and q p \u2208 R T a \u00d7d q . T (\u00b7) represents the sequence length (audio or video), and d (\u00b7) represents the dimension, respectively. Inspired by the multimodal transformer in MulT [51], we attend to interactions between multi-modal sequences across distinct time steps and latently adapt streams from audio modality to visual modality. We assume the input of the cross-modal attention is a sequence of queries Q = v p W Q , keys K = q p W K and values V = q p W V . The cross-modal attention is calculated by\nAttention a\u2192v (Q, K, V) = Softmax( Q K \u221a d k )V ,(4)\nwhere d k is the query dimensions and Softmax operation is performed on every row. We employ multi-head attention layers [53], which consists of H paralleled cross-modal attention layers. Finally, we obtain cross-modal representation f \u2208 R T v \u00d7d v . Noise problem can be derived from acquisition (pause, environment noise, etc.) and alignment. For the acquisition noises, we use a pre-trained MFCC model [5] to extract acoustic features, which is widely used in automatic speech and speaker recognition. There is a large gap between video and audio representations. The joint representations reflect important information considering multi-modal alignment. Attention is an importance estimate in the time domain, while the frequency domain can reflect another granularity of importance estimate. For multi-modal tasks, the importance of the frequency band requires consideration of interactive alignment information, and the high frequency part is more likely to be noisy information unrelated to alignment. For example, the noise can be parts of the audio irrelevant to the content of the video, or parts of the video irrelevant the audio features. To deal with the noise problem, we adopt 2d Discrete Wavelet Transform (DWT) for the joint representation. The reason for using DWT instead of DFT is that DFT is more likely to lose useful information at high frequencies, resulting in a decrease in actual performance.\n\nThe proposed algorithm is a hybrid approach that uses spatial and transform-domain information. Wavelet transform decomposes a signal into its sub-bands using a series of high-pass and low-pass filters. As noise is generally categorized as a high-frequency component, it is easier to separate it from the signal using wavelet transform. The decomposition of frequency content depends on the number of levels of DWT. The cross-modal representations f \u2208 R T v \u00d7d v serve as the input signal. The DWT separates filtering operations on rows and columns. A j,u and C k j,u denote scaling and wavelet coefficients at scale j for the given signal f where k = 1, 2, 3. We'll be working with separable orthonormal filters so 2D filters can be expressed as a product between low pass filter h and high pass filter g. The coefficients at scale j can be obtained from coefficients at scale j + 1. We can obtain A j,u and C k j,u as follows.\nA j,u = \u221a 2 u hh(l \u2212 2u)A j+1,l ; C 1 j,u = \u221a 2 u hg(l \u2212 2u)A j+1,l ; C 2 j,u = \u221a 2 u gh(l \u2212 2u)A j+1,l ; C 3 j,u = \u221a 2 u gg(l \u2212 2u)A j+1,l .(5)\nTo implement the filter bank, we use two-stage filter banks.\n\nIn the first stage, rows of two-dimensional signal are convolved with h, g filters and then we downsample columns by 2. In the next stage, columns are convolved with the filters h, g and we keep only even indexed rows. A n \u00d7 d v cross-modal signal is transformed into four n 2 \u00d7 d v 2 signal after the two stages.\n\nNext, we perform threshold quantization on the highfrequency coefficients C k j,u of wavelet decomposition. For the high-frequency coefficients (in three directions) of each layer from layer 1 to layer N , a threshold value is selected for threshold quantization. We adopt VisuShrink threshold \u03b1 with a soft threshold function. For \u03c6 = max |C k j,u |, the filter operations can be represented as follows (k \u2208 [1,2,3]).\nC k j,u [x, y] = sgn(C k j,u [x, y])( C k j,u [x,\ny] \u2212 \u03b1\u03c6) + (6) Then, we perform the wavelet reconstruction of the signal. The wavelet is reconstructed according to the low frequency coefficients of the N -th layer of wavelet decomposition and the high-frequency coefficients from the 1st layer to the N -th layer after quantization.\nf n = DWTInverse(A j,u , C k j,u ),(7)\nwhere f n is the denoised joint representation. The f n serves as the input of the following feed-forward layers and layer norm. Finally, we obtain the encoded representation E.\n\n\nSelf Attention-Free Decoder\n\nDue to the fusion between the audio and video signal, joint representations are more suitable for processing in the frequency domain. We adopt the decoder without the selfattention layers to achieve speedups. Inspired by [30], each layer consists of a Fourier mixing sublayer followed by a feed-forward sublayer. We replace the self-attention sublayer of each transformer decoder layer with a Fourier sublayer. A 2D DFT is applied to its embedding input. One 1D DFT is along the sequence dimension, F seq , and one 1D DFT is along the hidden dimension F hidden .\ny = R(F seq (R(F hidden (x)))),(8)\nWe only keep the real part of the result. After the Fourier layers, we employ the multi-head attention layers. The object prediction O is obtained.\n\n\nTraining of Wnet\n\nAmong the whole model, the loss function includes the mask loss, the box loss and the mutual loss.\nL = \u03bb 1 L mask + \u03bb 2 L box + L mutual ,(9)\nwhere \u03bb 1 , \u03bb 2 aim to adjust the three losses. The mask loss for supervising the predictions is defined as a combination of the Dice [36] and Focal [32] loss:\nL mask (m i , m \u03c3(i) ) = 1 T T t=0 [L Dice (m i,t , m \u03c3(i),t ) + L F ocal (m i,t , m \u03c3(i),t )],(10)\nwhere m is the predicted mask, m \u03c3 is the target mask and T is the number of frames in the video. L box scores the bounding boxes. We use a linear combination of the sequence level L1 loss and the generalized IOU [43] loss.\nL box (b i , b \u03c3(i) ) = 1 T T t=0 [L iou (b i,t , b \u03c3(i),t ) + ||b i,t \u2212 b \u03c3(i),t || 1 ],(11)\nWe use KL divergence [60] to maximize the mutual information between the cross-modal representation f and the encoded representation E.\nL mutual (E(i, j)||f (i, j)) = E(i, j)(log E(i, j) f (i, j) ),(12)\nwhere i stands for the sequence and j stands for the dimension. E and f are sent to the softmax function before computing KL divergence. The KL divergence is used to pull in the distance between the cross-modal representation and the encoded representation. Thus, the audio guidance is strengthened, which avoids the DWT operation filtering too many audio factors.\n\n\nExperiments\n\n\nPerformance Criteria\n\nWe evaluate the performance of our Wnet method based on two widely-used evaluation criteria for audio-guided video semantic segmentation following [40]. Given the testing video sequence v and audio query q with the groundtruth masks G, we denote the generated masks from our Wnet method by S. We employ the Jaccard index J defined as the intersection-over-union of the generated segmentation and the ground-truth mask (J = S\u2229G S\u222aG ). From a contour-based perspective, one can interpret S as a set of closed contours c(S) delimiting the spatial extent of the mask. Therefore, one can compute the contour-based precision and recall P c and R c between the contour points of c(S) and c(G). We adopt F-measure as a trade-off between the two (F = 2P c R c P c +R c ).\n\n\nImplementation Details\n\nVisual Feature Extraction. We use a ResNet-50 backbone to extract visual features, which has the same settings as DETR [6]. And it is then fed into a 2D convolution with kernel size 1 to map the model dimension and each frame is concatenated to form the clip level feature. Acoustic Feature Extraction. We use a 39-dimensional MFCC to represent its acoustic feature. Then, we use 1D convolution to further extract features and map them to the corresponding dimensions of the model following the implementation by Tsai et al. [51]. The kernel size of 1D convolution is 1.\n\n\nWnet (Ours) VisTR+\n\nA fish in distress with its head out of the water swimming.\n\nA brown turtle is swimming up in the water.\n\nAn atv being ridden down a path. Dataset Processing. Our dataset (AVOS) contains three parts (RVOS, A2D and J-HMDB). For the RVOS part, we use the same videos in Youtube-VIS [59], as VisTR [56]. The mask annotations in validation and test set are unavailable, so we divide the training set into the training, validation and test set in our experiments. However, we also offer the audio queries for original validation and test set. Model Setting. We adopt a 2-layer, 8-head multi-head cross-attention [51] module with the width of 3 to fuse visual and audio features. Between the attention layer and the feed-forward layer, a wavelet transform filter layer is used to remove noise from joint representations.For the transformer decoder, we use Fourier transform [30] instead of the self-attention layer. After obtaining the prediction of the decoder and the encoder, for each corresponding frame, we send them to an attention module to obtain the attention map, which is not multiplied by the value. Then it will be fused with the backbone features and the memory to get the mask features for each instance of each frame, following the same practice with VisTR [56]. We expand the num-ber of frames per video to 36 for end-to-end training, and applied 36 query slots for 36 objects throughout the video. Finally, we use three Conv3d layers and GroupNorm layers [57] with ReLU activation. The Conv3d layers have the kernel size of 3, padding of 2 and dilation of 2. And we use a last Conv3d layer with the kernel size of 1 to obtain the mask. More details are in supplementary material.\n\n\nPerformance Comparisons\n\nWe compare our proposed method with other existing methods for the problem as follows: VisTR+ is the extension of the transformer-based video instance segmentation algorithm [56], where the cross-modal attention layer is added to fuse the two modalities. For the VisTR+, we use the Hungarian loss as [56]. For our Wnet, we use the box and mask loss. URVOS+ is the extension of the unified referring video segmentation network [48], where the MFCC layer [5] is added to encode the audio inputs. PAM+ is the extension of the polar relative positional encoding mechanism [38], where the MFCC layer [5] is added to encode the audio inputs.\n\nTab. 2 and Tab. 3 presents the performance on AVOS. We exceed VisTR+, URVOS+ and PAM+ with 5.0%, 5.9% and 4.4% in the region similarly. Wnet has an absolute improvement of 5.5%, 5.8% and 6.1% for the contour accuracy. These comparisons mean that the audio-guided video object segmentation is quite different from the text-guided task. There is also a vast difference between the recordings collected from the natural environment and those generated from text-to-speech model. Therefore, it is not suitable to treat audio-guided segmentation as the combination of automatic speech recognition and text-based segmentation.\n\nThe visualization of Wnet on the AVOS test dataset is shown in Fig. 4, with each row containing images sampled from the same video. The comparison between Wnet and VisTR+ shows our efficiency in the audio-guided models.   Wnet can segment small objects from the nature environment, while the VisTR+ performs poorly under this circumstance. Furthermore, the visualization of the DWT process in Fig. 5 shows the denoising performance.\n\n\nAblation Study\n\nIn the ablation study, we fine-tune parameters on the validation set. We take the audio-guided RVOS dataset (parts of AVOS dataset) as the example. About the model components. As shown in Tab. 4, we conduct the experiments to verify the effectiveness of our model design, including the DWT-Based denoising (DWT), mutual information maximum (MIM) and self attentionfree decoder (AFD). We use the self-attention layers in the models without the DWT layers. The full model achieves better results than the model (w/o. DWT). It suggests that the DWT layers can filter the noise generated in the audiovideo fusion and improve subsequent segmentation results. About the wavelet basis. Tab. 5 shows the comparison of different wavelet bases. Results verify that the Daubechies wavelet basis is proper for the discrete joint representations. About the threshold parameter selection. We conduct the experiments under high-pass filters and low-pass filters with different threshold parameter selections. Results in Tab. 6 shows that we can get the best performance under the lowpass filters (0.008) with the soft function. About the threshold function selection. Two common threshold functions are the hard and the soft function. The hard threshold method can preserve the local features such as the edge of the signal well, while the soft threshold method is relatively smooth. As Tab. 7 shown, we choose the soft function for our model. About the J selection. Tab. 8 shows the results for the J selection. The performance will be worse when the order is increasing. We select J = 1 for our model, with 1 low frequency and 3 high-frequency coefficients. About the audio-text-segment model. The audio-textsegment model means that we use an ASR model first and then employ the latter referring segmentation model. Tab. 9 shows the results for comparison of Wnet and audio-textsegment model, in terms of speed and quality. We achieve the better performance in these two factors.\n\n\nConclusion\n\nIn this paper, we present the problem of open-ended audio-guided video semantic segmentation, which can be applied in video analysis, video editing, virtual human and so on, from the viewpoint of end-to-end denoising encoderdecoder network learning. We propose the wavelet-based encoder network to learn the cross-modal representations of the video contents with audio-form queries. Then, a self attention-free decoder network is developed to generate the target masks with frequency-domain transforms. In addition, we construct the first large-scale audio-guided video semantic segmentation dataset. The extensive experiments show the effectiveness of our method. \n\nFigure 3 .\n3The process of the denoising operation.\n\nFigure 4 .Figure 5 .\n45Visualization of Wnet and VisTR+ on the AVOS. Visualization of DWT-based denoised features.\n\nTable 1 .\n1The statistics of the AVOS dataset.RVOS \nA2D J-HMDB \nTotal \n\nNumber of Audio 11,226 6,656 \n\n929 \n18,811 \n\nresearch \n\n\nillustrated, our model can be divided into five modules: visual encoder, audio encoder, transformer encoder, transformer decoder and segmentation module. Visual Encoder. We employ ResNet-50[16] as our backbone network to extract visual features from an input frame.\u2026 \n\nCross-Modal \nAttention \nLayer Norm \nLayer Norm \n\n1D Conv \n\nPE \n\nFeed-Froward \nFourier \nAdd & Norm \nFeed-Froward \n\n\u00d7 \n\u00d7 \n\nResNet50 \n\nAudio Waveform \n\n\u2026 \n\nMFCC \n\nEncoded \nFeatures \n\n\u2026 \n\nObject \nPrediction \n\nB \n\nE \nO \n\nObject Segmentation \n\nAttention \nE \n\nO \n\nB \n\n3D Conv \nMask \nSequence \n\nInstance Query \n\nAudio Embeddings \n\nDB Wavelet \n\nDB Wavelet \n\nMaximize Mutual Information \n\n[ , ] \n\nA \n\nH \n\nV \n\nD \n\nA H \n\nV D \n\nMulti-Modal \nFeature \n\n2 \n\n2 \n\n2 \n\n2 \n\n2 \n\n2 \n\nFilter \nInverse \nTransform \n\nMulti-Head \nAttention \nLayer Norm \n\nAdd \nAdd & Norm \nAdd & Norm \n\nE \n\nPE \nPE \n\nLayer Norm \n\nAdd \nAdd \n\nPE \n\n\n\nTable 2 .\n2The comparison of different method for audio-guided semantic segmentation on AVOS.Model \nJ \nF \nJ&F \n\nURVOS+ [48] 37.1% 39.2% 38.2% \nPAM+ [38] \n38.6% 38.9% 38.8% \nVisTR+ [56] \n38.0% 39.5% 38.8% \nWnet (Ours) \n43.0% 45.0% 44.0% \n\nTable 3. The results of different datasets in the AVOS dataset. In \nthis table, we use the same dataset for training and testing. \n\nDataset \nJ \nF \nJ&F \n\nRVOS \n43.0% 44.1% 43.6% \nA2D \n49.8% 55.1% 52.5% \nJ-HMDB 65.6% 56.7% 61.2% \n\nNote: JHMDB-Sentences is only for evaluation, not training, so it is directly evalu-\nated using the checkpoint trained on A2D-Sentences. \n\n\n\nTable 4 .\n4The results for different components. AFD + MIM 41.8% 43.0% 42.4% Base + AFD + MIM + DWT 42.9% 44.0% 43.5% Note: We use self-attention layers to replace DWT layers in Base, Base+AFD and Base+AFD+MIM.Model \nJ \nF \nJ&F \n\nBase \n39.3% 41.6% 40.4% \nBase + AFD \n41.2% 42.5% 41.8% \nBase + \n\nTable 5 .Table 6 .\n56The results for different wavelet basis, mentioned in[42].Wavelet Basis Daubechies Symlets Coiflets Meyer The results for DWT. [a, b] means the retained coefficients (value is in interval [a \u00b7 max value, b \u00b7 max value]) after filters. For the low pass and high-low pass, we use the hard threshold function. For the high pass, we use the soft threshold function. Low Pass [0.008, 0.9] 42.1% 43.7% 42.9% Table 7. The results for threshold function selection. Take high pass [0.008, 1] for example.Table 8. The results for J selection. For different J, the number of the high frequency coefficient matrix is 3J.J \n42.9% \n41.9% \n41.7% \n40.7% \nF \n44.0% \n42.6% \n42.9% \n42.7% \n\nModel \nJ \nF \nJ&F \n\nLow Pass \n\n[0, 0.9] \n41.8% 43.3% \n42.5% \n[0, 0.8] \n42.1% 43.3% \n42.7% \n[0, 0.7] \n40.3% 41.3% \n40.6% \nHigh-High Pass \n\n[0.01, 1] \n42.8% 43.2% \n43.1% \n[0.008, 1] \n42.9% 44.0% 43.5% \n[0.006, 1] \n42.3% 43.8% \n43.1% \n\nFunction Selction for High Pass \nJ \nF \nJ&F \n\nHard Function \n42.1% 41.8% 42.0% \nSoft Function \n42.9% 44.0% 43.5% \n\nJ Number of Coefficient Matrix \nJ \nF \nJ&F \n\n1 \n3 (C k \n1,u ) \n42.9% 44.0% 43.5% \n2 \n6( C k \n1,u ; C k \n2,u ) \n41.0% 41.7% 41.4% \n\n\n\nTable 9 .\n9The comparison of average inference latency among Wnet and audio-text-segmentation model. The evaluation is conducted on a server with 1 NVIDIA 3090Ti GPU, 12 Intel Xeon CPU. The batch size is set to 1.Method \nJ \nLatency Speedup \n\nWnet \n42.9% 0.0014s \n2.10\u00d7 \nASR+RVOS 38.4% 0.0032s \n1.00\u00d7 \n\n\n\n\nIn 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 1307-1315. IEEE Computer Society, 2018. 1, 2\n\nTowards bilingual lexicon discovery from visually grounded speech audio. Emmanuel Azuh, David Harwath, James R Glass, Interspeech 2019, 20th Annual Conference of the International Speech Communication Association. Graz, AustriaEmmanuel Azuh, David Harwath, and James R. Glass. To- wards bilingual lexicon discovery from visually grounded speech audio. In Interspeech 2019, 20th Annual Confer- ence of the International Speech Communication Associa- tion, Graz, Austria, 15-19 September 2019, pages 276-280. ISCA, 2019. 2\n\nLearning representations by maximizing mutual information across views. Philip Bachman, R Devon Hjelm, William Buchwalter, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. NeurIPS; Vancouver, BC, CanadaPhilip Bachman, R. Devon Hjelm, and William Buchwalter. Learning representations by maximizing mutual information across views. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Pro- cessing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 15509-15519, 2019. 2\n\nvq-wav2vec: Self-supervised learning of discrete speech representations. Alexei Baevski, Steffen Schneider, Michael Auli, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020Alexei Baevski, Steffen Schneider, and Michael Auli. vq- wav2vec: Self-supervised learning of discrete speech rep- resentations. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. 2\n\nwav2vec 2.0: A framework for self-supervised learning of speech representations. Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, Michael Auli, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020. 2020Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A framework for self-supervised learning of speech representations. In Advances in Neu- ral Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. 2\n\nMfccs and gabor features for improving continuous arabic speech recognition in mobile communication modified. Lallouani Bouchakour, Mohamed Debyeche, Proceedings of the 3rd International Conference on Advanced Aspects of Software Engineering, ICAASE 2018. the 3rd International Conference on Advanced Aspects of Software Engineering, ICAASE 2018Constantine, Algeria23267CEUR-WS.orgLallouani Bouchakour and Mohamed Debyeche. Mfccs and gabor features for improving continuous arabic speech recognition in mobile communication modified. In Proceed- ings of the 3rd International Conference on Advanced As- pects of Software Engineering, ICAASE 2018, Constantine, Algeria, December 1-2, 2018, volume 2326 of CEUR Work- shop Proceedings, pages 115-121. CEUR-WS.org, 2018. 2, 5, 7\n\nEnd-toend object detection with transformers. Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko, Computer Vision -ECCV 2020 -16th European Conference. Glasgow, UK, AuSpringer123466Proceedings, Part INicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to- end object detection with transformers. In Computer Vision - ECCV 2020 -16th European Conference, Glasgow, UK, Au- gust 23-28, 2020, Proceedings, Part I, volume 12346 of Lec- ture Notes in Computer Science, pages 213-229. Springer, 2020. 4, 6\n\nVisually grounded models of spoken language: A survey of datasets, architectures and evaluation techniques. CoRR, abs. Grzegorz Chrupala, 23Grzegorz Chrupala. Visually grounded models of spoken language: A survey of datasets, architectures and evaluation techniques. CoRR, abs/2104.13225, 2021. 2, 3\n\nRepresentations of language in a model of visually grounded speech signal. Grzegorz Chrupala, Lieke Gelderloos, Afra Alishahi, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaLong Papers13Association for Computational LinguisticsGrzegorz Chrupala, Lieke Gelderloos, and Afra Alishahi. Representations of language in a model of visually grounded speech signal. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 -August 4, Volume 1: Long Papers, pages 613-622. Association for Computational Lin- guistics, 2017. 2, 3\n\nActor and action video segmentation from a sentence. Kirill Gavrilyuk, Amir Ghodrati, Zhenyang Li, G M Cees, Snoek, 2018 IEEE Conference on Computer Vision and Pattern Recognition. Salt Lake City, UT, USAIEEE Computer Society23Kirill Gavrilyuk, Amir Ghodrati, Zhenyang Li, and Cees G. M. Snoek. Actor and action video segmentation from a sentence. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 5958-5966. IEEE Computer Society, 2018. 2, 3\n\nFrom phonemes to images: levels of representation in a recurrent neural model of visually-grounded language learning. Lieke Gelderloos, Grzegorz Chrupala, De- cember 11-16COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers. Osaka, Japan23Lieke Gelderloos and Grzegorz Chrupala. From phonemes to images: levels of representation in a recurrent neural model of visually-grounded language learning. In COLING 2016, 26th International Conference on Computational Linguis- tics, Proceedings of the Conference: Technical Papers, De- cember 11-16, 2016, Osaka, Japan, pages 1309-1319. ACL, 2016. 2, 3\n\nTowards visually grounded sub-word speech unit discovery. David Harwath, James R Glass, IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2019. Brighton, United Kingdom23David Harwath and James R. Glass. Towards visually grounded sub-word speech unit discovery. In IEEE Interna- tional Conference on Acoustics, Speech and Signal Process- ing, ICASSP 2019, Brighton, United Kingdom, May 12-17, 2019, pages 3017-3021. IEEE, 2019. 2, 3\n\nLearning hierarchical discrete linguistic units from visuallygrounded speech. David Harwath, Wei-Ning Hsu, James R Glass, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia20203David Harwath, Wei-Ning Hsu, and James R. Glass. Learn- ing hierarchical discrete linguistic units from visually- grounded speech. In 8th International Conference on Learn- ing Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. 2, 3\n\nJointly discovering visual objects and spoken words from raw sensory input. David Harwath, Adri\u00e0 Recasens, D\u00eddac Sur\u00eds, Galen Chuang, Antonio Torralba, James R Glass, Int. J. Comput. Vis. 1283David Harwath, Adri\u00e0 Recasens, D\u00eddac Sur\u00eds, Galen Chuang, Antonio Torralba, and James R. Glass. Jointly dis- covering visual objects and spoken words from raw sensory input. Int. J. Comput. Vis., 128(3):620-641, 2020. 2\n\nDeep multimodal semantic embeddings for speech and images. F David, James R Harwath, Glass, 2015 IEEE Workshop on Automatic Speech Recognition and Understanding. Scottsdale, AZ, USAASRUDavid F. Harwath and James R. Glass. Deep multimodal se- mantic embeddings for speech and images. In 2015 IEEE Workshop on Automatic Speech Recognition and Under- standing, ASRU 2015, Scottsdale, AZ, USA, December 13- 17, 2015, pages 237-244. IEEE, 2015. 2\n\nWord recognition, competition, and activation in a model of visually grounded speech. William N Havard, Jean-Pierre Chevrot, Laurent Besacier, Proceedings of the 23rd Conference on Computational Natural Language Learning. the 23rd Conference on Computational Natural Language LearningHong Kong, ChinaAssociation for Computational LinguisticsWilliam N. Havard, Jean-Pierre Chevrot, and Laurent Be- sacier. Word recognition, competition, and activation in a model of visually grounded speech. In Proceedings of the 23rd Conference on Computational Natural Language Learning, CoNLL 2019, Hong Kong, China, November 3-4, 2019, pages 339-348. Association for Computational Lin- guistics, 2019. 3\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016. Las Vegas, NV, USAIEEE Computer SocietyKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 770-778. IEEE Computer Society, 2016. 3\n\nDiscrete representations in neural models of spoken language. Bertrand Higy, Lieke Gelderloos, Afra Alishahi, Grzegorz Chrupala, abs/2105.05582CoRRBertrand Higy, Lieke Gelderloos, Afra Alishahi, and Grze- gorz Chrupala. Discrete representations in neural models of spoken language. CoRR, abs/2105.05582, 2021. 3\n\nSegmentation from natural language expressions. Ronghang Hu, Marcus Rohrbach, Trevor Darrell, Computer Vision -ECCV 2016 -14th European Conference, Amsterdam. The NetherlandsSpringer9905Proceedings, Part IRonghang Hu, Marcus Rohrbach, and Trevor Darrell. Seg- mentation from natural language expressions. In Computer Vision -ECCV 2016 -14th European Conference, Amster- dam, The Netherlands, October 11-14, 2016, Proceedings, Part I, volume 9905 of Lecture Notes in Computer Science, pages 108-124. Springer, 2016. 1, 2\n\nUtilizing large scale vision and text datasets for image segmentation from referring expressions. Ronghang Hu, Marcus Rohrbach, Subhashini Venugopalan, Trevor Darrell, abs/1608.08305CoRR1Ronghang Hu, Marcus Rohrbach, Subhashini Venugopalan, and Trevor Darrell. Utilizing large scale vision and text datasets for image segmentation from referring expressions. CoRR, abs/1608.08305, 2016. 1, 2\n\nBi-directional relationship inferring network for referring image segmentation. Zhiwei Hu, Guang Feng, Jiayu Sun, Lihe Zhang, Huchuan Lu, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Seattle, WA, USAIEEE2020Zhiwei Hu, Guang Feng, Jiayu Sun, Lihe Zhang, and Huchuan Lu. Bi-directional relationship inferring network for referring image segmentation. In 2020 IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 4423- 4432. IEEE, 2020. 2\n\nReferring image 1318 segmentation via cross-modal progressive comprehension. Shaofei Huang, Tianrui Hui, Si Liu, Guanbin Li, Yunchao Wei, Jizhong Han, Luoqi Liu, Bo Li, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Seattle, WA, USAIEEE2020Shaofei Huang, Tianrui Hui, Si Liu, Guanbin Li, Yunchao Wei, Jizhong Han, Luoqi Liu, and Bo Li. Referring image 1318 segmentation via cross-modal progressive comprehension. In 2020 IEEE/CVF Conference on Computer Vision and Pat- tern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 10485-10494. IEEE, 2020. 2\n\nLargescale representation learning from visually grounded untranscribed speech. Gabriel Ilharco, Yuan Zhang, Jason Baldridge, Proceedings of the 23rd Conference on Computational Natural Language Learning. the 23rd Conference on Computational Natural Language LearningCoNLL; Hong Kong, ChinaAssociation for Computational LinguisticsGabriel Ilharco, Yuan Zhang, and Jason Baldridge. Large- scale representation learning from visually grounded untran- scribed speech. In Proceedings of the 23rd Conference on Computational Natural Language Learning, CoNLL 2019, Hong Kong, China, November 3-4, 2019, pages 55-65. As- sociation for Computational Linguistics, 2019. 2\n\nTowards understanding action recognition. Hueihan Jhuang, Juergen Gall, Silvia Zuffi, Cordelia Schmid, Michael J Black, IEEE International Conference on Computer Vision, ICCV 2013. Sydney, AustraliaIEEE Computer SocietyHueihan Jhuang, Juergen Gall, Silvia Zuffi, Cordelia Schmid, and Michael J. Black. Towards understanding ac- tion recognition. In IEEE International Conference on Com- puter Vision, ICCV 2013, Sydney, Australia, December 1-8, 2013, pages 3192-3199. IEEE Computer Society, 2013. 3\n\nLocate then segment: A strong pipeline for referring image segmentation. Ya Jing, Tao Kong, Wei Wang, Liang Wang, Lei Li, Tieniu Tan, abs/2103.16284CoRRYa Jing, Tao Kong, Wei Wang, Liang Wang, Lei Li, and Tie- niu Tan. Locate then segment: A strong pipeline for referring image segmentation. CoRR, abs/2103.16284, 2021. 2\n\nA comprehensive performance analysis of EEMD-BLMS and DWT-NN hybrid algorithms for ECG denoising. Kevin Kaergaard, Sadasivan S\u00f8ren Hj\u00f8llund Jensen, Puthusserypady, Biomed. Signal Process. Control. 252Kevin Kaergaard, S\u00f8ren Hj\u00f8llund Jensen, and Sadasivan Puthusserypady. A comprehensive performance analysis of EEMD-BLMS and DWT-NN hybrid algorithms for ECG denoising. Biomed. Signal Process. Control., 25:178-187, 2016. 2\n\nVisually grounded cross-lingual keyword spotting in speech. Herman Kamper, Michael Roth, Shyam S. Herman Kamper and Michael Roth. Visually grounded cross-lingual keyword spotting in speech. In Shyam S.\n\nWorkshop on Spoken Language Technologies for Under-Resourced Languages. Agrawal, SLTU. 20182ISCAAgrawal, editor, 6th Intl. Workshop on Spoken Language Technologies for Under-Resourced Languages, SLTU 2018, 29-31 August 2018, Gurugram, India, pages 253-257. ISCA, 2018. 2\n\nVisually grounded learning of keyword prediction from untranscribed speech. Herman Kamper, Shane Settle, Gregory Shakhnarovich, Karen Livescu, 18th Annual Conference of the International Speech Communication Association. Francisco LacerdaStockholm, SwedenHerman Kamper, Shane Settle, Gregory Shakhnarovich, and Karen Livescu. Visually grounded learning of keyword pre- diction from untranscribed speech. In Francisco Lacerda, editor, Interspeech 2017, 18th Annual Conference of the In- ternational Speech Communication Association, Stockholm, Sweden, August 20-24, 2017, pages 3677-3681. ISCA, 2017. 2\n\nVideo object segmentation with language referring expressions. Anna Khoreva, Anna Rohrbach, Bernt Schiele, Computer Vision -ACCV 2018 -14th Asian Conference on Computer Vision. Perth, AustraliaSpringer11364Revised Selected Papers, Part IVAnna Khoreva, Anna Rohrbach, and Bernt Schiele. Video object segmentation with language referring expressions. In Computer Vision -ACCV 2018 -14th Asian Conference on Computer Vision, Perth, Australia, December 2-6, 2018, Re- vised Selected Papers, Part IV, volume 11364 of Lecture Notes in Computer Science, pages 123-141. Springer, 2018. 2\n\nPhysiological signal denoising with variational mode decomposition and weighted reconstruction after DWT thresholding. Salim Lahmiri, Mounir Boukadoum, 2015 IEEE International Symposium on Circuits and Systems, IS-CAS 2015. Lisbon, Portugal23Salim Lahmiri and Mounir Boukadoum. Physiological signal denoising with variational mode decomposition and weighted reconstruction after DWT thresholding. In 2015 IEEE International Symposium on Circuits and Systems, IS- CAS 2015, Lisbon, Portugal, May 24-27, 2015, pages 806- 809. IEEE, 2015. 2, 3\n\nFnet: Mixing tokens with fourier transforms. James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Onta\u00f1\u00f3n, abs/2105.03824CoRR67James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, and Santi- ago Onta\u00f1\u00f3n. Fnet: Mixing tokens with fourier transforms. CoRR, abs/2105.03824, 2021. 2, 3, 4, 6, 7\n\nReferring image segmentation via recurrent refinement networks. Ruiyu Li, Kai-Can Li, Yi-Chun Kuo, Michelle Shu, Xiaojuan Qi, Xiaoyong Shen, Jiaya Jia, 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018. Salt Lake City, UT, USAIEEE Computer Society1Ruiyu Li, Kai-Can Li, Yi-Chun Kuo, Michelle Shu, Xiao- juan Qi, Xiaoyong Shen, and Jiaya Jia. Referring image segmentation via recurrent refinement networks. In 2018 IEEE Conference on Computer Vision and Pattern Recog- nition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 5745-5753. IEEE Computer Society, 2018. 1, 2\n\nFocal loss for dense object detection. Tsung-Yi Lin, Priya Goyal, Ross B Girshick, Kaiming He, Piotr Doll\u00e1r, IEEE Trans. Pattern Anal. Mach. Intell. 422Tsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. IEEE Trans. Pattern Anal. Mach. Intell., 42(2):318-327, 2020. 6\n\nRecurrent multimodal interaction for referring image segmentation. Chenxi Liu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Alan L Yuille, IEEE International Conference on Computer Vision. Venice, ItalyIEEE Computer Society1Chenxi Liu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, and Alan L. Yuille. Recurrent multimodal interaction for refer- ring image segmentation. In IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22- 29, 2017, pages 1280-1289. IEEE Computer Society, 2017. 1, 2\n\nMulti-task collaborative network for joint referring expression comprehension and segmentation. Gen Luo, Yiyi Zhou, Xiaoshuai Sun, Liujuan Cao, Chenglin Wu, Cheng Deng, Rongrong Ji, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Seattle, WA, USAIEEE2020Gen Luo, Yiyi Zhou, Xiaoshuai Sun, Liujuan Cao, Chenglin Wu, Cheng Deng, and Rongrong Ji. Multi-task collaborative network for joint referring expression comprehension and segmentation. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 10031-10040. IEEE, 2020. 2\n\nDynamic multimodal instance segmentation guided by natural language queries. Edgar Margffoy-Tuay, Juan C P\u00e9rez, Emilio Botero, Pablo Arbel\u00e1ez, Computer Vision -ECCV 2018 -15th European Conference. Munich, GermanySpringer11215Proceedings, Part XIEdgar Margffoy-Tuay, Juan C. P\u00e9rez, Emilio Botero, and Pablo Arbel\u00e1ez. Dynamic multimodal instance segmenta- tion guided by natural language queries. In Computer Vision -ECCV 2018 -15th European Conference, Munich, Ger- many, September 8-14, 2018, Proceedings, Part XI, volume 11215 of Lecture Notes in Computer Science, pages 656- 672. Springer, 2018. 1, 2\n\nV-net: Fully convolutional neural networks for volumetric medical image segmentation. Fausto Milletari, Nassir Navab, Seyed-Ahmad Ahmadi, Fourth International Conference on 3D Vision, 3DV 2016. Stanford, CA, USAIEEE Computer SocietyFausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. V-net: Fully convolutional neural networks for volumetric medical image segmentation. In Fourth International Con- ference on 3D Vision, 3DV 2016, Stanford, CA, USA, Octo- ber 25-28, 2016, pages 565-571. IEEE Computer Society, 2016. 6\n\nEEG based direct speech BCI system using a fusion of SMRT and MFCC/LPCC features with ANN classifier. P P Mini, Tessamma Thomas, R Gopikakumari, Biomed. Signal Process. Control. 684102625P. P. Mini, Tessamma Thomas, and R. Gopikakumari. EEG based direct speech BCI system using a fusion of SMRT and MFCC/LPCC features with ANN classifier. Biomed. Signal Process. Control., 68:102625, 2021. 4\n\nPolar relative positional encoding for video-language segmentation. Ke Ning, Lingxi Xie, Fei Wu, Qi Tian, Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence. the Twenty-Ninth International Joint Conference on Artificial Intelligence20207ijcai.org, 2020. 2, 6Ke Ning, Lingxi Xie, Fei Wu, and Qi Tian. Polar rela- tive positional encoding for video-language segmentation. In Proceedings of the Twenty-Ninth International Joint Confer- ence on Artificial Intelligence, IJCAI 2020, pages 948-954. ijcai.org, 2020. 2, 6, 7\n\nTrilingual semantic embeddings of visually grounded speech with selfattention mechanisms. Yasunori Ohishi, Akisato Kimura, Takahito Kawanishi, Kunio Kashino, David Harwath, James R Glass, 2020 IEEE International Conference on Acoustics, Speech and Signal Processing. Barcelona, SpainIEEE2020Yasunori Ohishi, Akisato Kimura, Takahito Kawanishi, Ku- nio Kashino, David Harwath, and James R. Glass. Trilingual semantic embeddings of visually grounded speech with self- attention mechanisms. In 2020 IEEE International Confer- ence on Acoustics, Speech and Signal Processing, ICASSP 2020, Barcelona, Spain, May 4-8, 2020, pages 4352-4356. IEEE, 2020. 2\n\nA benchmark dataset and evaluation methodology for video object segmentation. Federico Perazzi, Jordi Pont-Tuset, Brian Mcwilliams, Luc Van Gool, Markus H Gross, Alexander Sorkine-Hornung, 2016 IEEE Conference on Computer Vision and Pattern Recognition. Las Vegas, NV, USAIEEE Computer SocietyFederico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus H. Gross, and Alexander Sorkine- Hornung. A benchmark dataset and evaluation methodology for video object segmentation. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 724-732. IEEE Computer Society, 2016. 6\n\nReferring image segmentation by generative adversarial learning. Shuang Qiu, Yao Zhao, Jianbo Jiao, Yunchao Wei, Shikui Wei, IEEE Trans. Multim. 225Shuang Qiu, Yao Zhao, Jianbo Jiao, Yunchao Wei, and Shikui Wei. Referring image segmentation by generative ad- versarial learning. IEEE Trans. Multim., 22(5):1333-1344, 2020. 2\n\nWavelet based image coding schemes : A recent survey. V J Rehna, M K Kumar, abs/1209.2515V. J. Rehna and M. K. Jeya Kumar. Wavelet based image coding schemes : A recent survey. CoRR, abs/1209.2515, 2012. 8\n\nGeneralized intersection over union: A metric and a loss for bounding box regression. Hamid Rezatofighi, Nathan Tsoi, Junyoung Gwak, Amir Sadeghian, Ian D Reid, Silvio Savarese, IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019. Long Beach, CA, USAComputer Vision Foundation / IEEEHamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir Sadeghian, Ian D. Reid, and Silvio Savarese. Generalized in- tersection over union: A metric and a loss for bounding box regression. In IEEE Conference on Computer Vision and Pat- tern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 658-666. Computer Vision Foundation / IEEE, 2019. 6\n\nTalk, don't write: A study of direct speech-based image retrieval. Ramon Sanabria, Austin Waters, Jason Baldridge, abs/2104.01894CoRRRamon Sanabria, Austin Waters, and Jason Baldridge. Talk, don't write: A study of direct speech-based image retrieval. CoRR, abs/2104.01894, 2021. 1, 2, 3, 4\n\nCSD optimized DWT filter for ECG denoising. Gayatri Saripalli, H Priyank, Anand D Prajapati, Darji, 2020 24th International Symposium on VLSI Design and Test (VDAT). Bhubaneswar, IndiaGayatri Saripalli, Priyank H. Prajapati, and Anand D. Darji. CSD optimized DWT filter for ECG denoising. In 2020 24th International Symposium on VLSI Design and Test (VDAT), Bhubaneswar, India, July 23-25, 2020, pages 1-6. IEEE, 2020. 2\n\nwav2vec: Unsupervised pre-training for speech recognition. Steffen Schneider, Alexei Baevski, Ronan Collobert, Michael Auli, 20th Annual Conference of the International Speech Communication Association, Graz, Austria. ISCASteffen Schneider, Alexei Baevski, Ronan Collobert, and Michael Auli. wav2vec: Unsupervised pre-training for speech recognition. In Gernot Kubin and Zdravko Kacic, editors, Interspeech 2019, 20th Annual Conference of the In- ternational Speech Communication Association, Graz, Aus- tria, 15-19 September 2019, pages 3465-3469. ISCA, 2019. 2\n\nLearning to recognise words using visually grounded speech. Sebastiaan Scholten, Danny Merkx, Odette Scharenborg, IEEE International Symposium on Circuits and Systems, ISCAS 2021. Daegu, South KoreaIEEESebastiaan Scholten, Danny Merkx, and Odette Scharen- borg. Learning to recognise words using visually grounded speech. In IEEE International Symposium on Circuits and Systems, ISCAS 2021, Daegu, South Korea, May 22-28, 2021, pages 1-5. IEEE, 2021. 3\n\nURVOS: unified referring video object segmentation network with a large-scale benchmark. Seonguk Seo, Joon-Young Lee, Bohyung Han, Computer Vision -ECCV 2020 -16th European Conference. Glasgow, UKSpringer123607Proceedings, Part XVSeonguk Seo, Joon-Young Lee, and Bohyung Han. URVOS: unified referring video object segmentation network with a large-scale benchmark. In Computer Vision -ECCV 2020 -16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XV, volume 12360 of Lecture Notes in Computer Science, pages 208-223. Springer, 2020. 3, 6, 7\n\nKey-word-aware network for referring expression image segmentation. Hengcan Shi, Hongliang Li, Fanman Meng, Qingbo Wu, Computer Vision -ECCV 2018 -15th. Hengcan Shi, Hongliang Li, Fanman Meng, and Qingbo Wu. Key-word-aware network for referring expression im- age segmentation. In Computer Vision -ECCV 2018 -15th\n\nProceedings, Part VI. Part VIMunich, GermanySpringer11210European ConferenceEuropean Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part VI, volume 11210 of Lecture Notes in Computer Science, pages 38-54. Springer, 2018. 1, 2\n\nQuery reconstruction network for referring expression image segmentation. Hengcan Shi, Hongliang Li, Qingbo Wu, King Ngi Ngan, IEEE Trans. Multim. 232Hengcan Shi, Hongliang Li, Qingbo Wu, and King Ngi Ngan. Query reconstruction network for referring expression image segmentation. IEEE Trans. Multim., 23:995-1007, 2021. 1, 2\n\nMultimodal transformer for unaligned multimodal language sequences. Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang, J Zico Kolter, Louis-Philippe Morency, Ruslan Salakhutdinov, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019. the 57th Conference of the Association for Computational Linguistics, ACL 2019Florence, ItalyLong Papers17Association for Computational Linguistics, 2019. 2, 5Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang, J. Zico Kolter, Louis-Philippe Morency, and Ruslan Salakhutdinov. Multimodal transformer for unaligned multimodal language sequences. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Flo- rence, Italy, July 28-August 2, 2019, Volume 1: Long Papers, pages 6558-6569. Association for Computational Linguis- tics, 2019. 2, 5, 6, 7\n\nTranslation invariant DWT based denoising using goodness of fit test. Ubaid Naveed Ur Rehman, Ur Rehman, IEEE Statistical Signal Processing Workshop. Palma de Mallorca, SpainIEEESyed Zain Abbas, Anum Asif, and Anum JavedNaveed ur Rehman, Ubaid ur Rehman, Syed Zain Abbas, Anum Asif, and Anum Javed. Translation invariant DWT based denoising using goodness of fit test. In IEEE Statistical Signal Processing Workshop, SSP 2016, Palma de Mallorca, Spain, June 26-29, 2016, pages 1-5. IEEE, 2016. 2\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. Long Beach, CA, USA45Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neu- ral Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 5998-6008, 2017. 4, 5\n\nContext modulated dynamic networks for actor and action video segmentation with language queries. Hao Wang, Cheng Deng, Fan Ma, Yi Yang, The Thirty-Second Innovative Applications of Artificial Intelligence Conference. New York, NY, USAAAAI Press2020The Tenth AAAI Symposium on Educational Advances in Artificial IntelligenceHao Wang, Cheng Deng, Fan Ma, and Yi Yang. Con- text modulated dynamic networks for actor and action video segmentation with language queries. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelli- gence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 12152-12159. AAAI Press, 2020. 2\n\nAsymmetric cross-guided attention network for actor and action video segmentation from natural language query. Hao Wang, Cheng Deng, Junchi Yan, Dacheng Tao, 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South). IEEEHao Wang, Cheng Deng, Junchi Yan, and Dacheng Tao. Asymmetric cross-guided attention network for actor and ac- tion video segmentation from natural language query. In 2019 IEEE/CVF International Conference on Computer Vi- sion, ICCV 2019, Seoul, Korea (South), October 27 -Novem- ber 2, 2019, pages 3938-3947. IEEE, 2019. 2\n\nEnd-to-end video instance segmentation with transformers. Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, Baoshan Cheng, Hao Shen, Huaxia Xia, IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual. 67Computer Vision Foundation / IEEEYuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, Baoshan Cheng, Hao Shen, and Huaxia Xia. End-to-end video instance segmentation with transformers. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pages 8741-8750. Computer Vision Foundation / IEEE, 2021. 4, 6, 7\n\nGroup normalization. Yuxin Wu, Kaiming He, Int. J. Comput. Vis. 12837Yuxin Wu and Kaiming He. Group normalization. Int. J. Comput. Vis., 128(3):742-755, 2020. 4, 7\n\nCan humans fly? action understanding with multiple classes of actors. Chenliang Xu, Shao-Hang Hsieh, Caiming Xiong, Jason J Corso, IEEE Conference on Computer Vision and Pattern Recognition. Boston, MA, USAIEEE Computer SocietyChenliang Xu, Shao-Hang Hsieh, Caiming Xiong, and Ja- son J. Corso. Can humans fly? action understanding with multiple classes of actors. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, pages 2264-2273. IEEE Computer Society, 2015. 3\n\nVideo instance segmentation. Linjie Yang, Yuchen Fan, Ning Xu, 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South). IEEELinjie Yang, Yuchen Fan, and Ning Xu. Video instance seg- mentation. In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 -November 2, 2019, pages 5187-5196. IEEE, 2019. 7\n\nLearning high-precision bounding box for rotated object detection via kullbackleibler divergence. Xue Yang, Xiaojiang Yang, Jirui Yang, Qi Ming, Wentao Wang, Qi Tian, Junchi Yan, abs/2106.01883CoRRXue Yang, Xiaojiang Yang, Jirui Yang, Qi Ming, Wentao Wang, Qi Tian, and Junchi Yan. Learning high-precision bounding box for rotated object detection via kullback- leibler divergence. CoRR, abs/2106.01883, 2021. 6\n\nCross-modal self-attention network for referring image segmentation. Linwei Ye, Mrigank Rochan, Zhi Liu, Yang Wang, IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019. Long Beach, CA, USA1Computer Vision Foundation / IEEELinwei Ye, Mrigank Rochan, Zhi Liu, and Yang Wang. Cross-modal self-attention network for referring image seg- mentation. In IEEE Conference on Computer Vision and Pat- tern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 10502-10511. Computer Vision Founda- tion / IEEE, 2019. 1, 2\n\nReferring segmentation in images and videos with cross-modal self-attention network. Linwei Ye, Mrigank Rochan, Zhi Liu, Xiaoqin Zhang, Yang Wang, abs/2102.04762CoRRLinwei Ye, Mrigank Rochan, Zhi Liu, Xiaoqin Zhang, and Yang Wang. Referring segmentation in images and videos with cross-modal self-attention network. CoRR, abs/2102.04762, 2021. 2\n\nMattnet: Modular attention network for referring expression comprehension. Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, Tamara L Berg, Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, and Tamara L. Berg. Mattnet: Modular attention network for referring expression comprehension.\n", "annotations": {"author": "[{\"end\":130,\"start\":97},{\"end\":164,\"start\":131},{\"end\":217,\"start\":165},{\"end\":274,\"start\":218},{\"end\":334,\"start\":275},{\"end\":394,\"start\":335},{\"end\":493,\"start\":395},{\"end\":547,\"start\":494},{\"end\":676,\"start\":548},{\"end\":705,\"start\":677}]", "publisher": null, "author_last_name": "[{\"end\":107,\"start\":104},{\"end\":141,\"start\":138},{\"end\":174,\"start\":170},{\"end\":229,\"start\":226},{\"end\":286,\"start\":284},{\"end\":346,\"start\":343},{\"end\":405,\"start\":402},{\"end\":500,\"start\":498},{\"end\":554,\"start\":552},{\"end\":684,\"start\":680}]", "author_first_name": "[{\"end\":103,\"start\":97},{\"end\":137,\"start\":131},{\"end\":169,\"start\":165},{\"end\":225,\"start\":218},{\"end\":283,\"start\":275},{\"end\":342,\"start\":335},{\"end\":401,\"start\":395},{\"end\":497,\"start\":494},{\"end\":551,\"start\":548},{\"end\":679,\"start\":677}]", "author_affiliation": "[{\"end\":129,\"start\":109},{\"end\":163,\"start\":143},{\"end\":216,\"start\":196},{\"end\":273,\"start\":251},{\"end\":333,\"start\":311},{\"end\":393,\"start\":366},{\"end\":492,\"start\":431},{\"end\":546,\"start\":519},{\"end\":596,\"start\":576},{\"end\":675,\"start\":598}]", "title": "[{\"end\":94,\"start\":1},{\"end\":799,\"start\":706}]", "venue": null, "abstract": "[{\"end\":2565,\"start\":829}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2821,\"start\":2817},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2824,\"start\":2821},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":2827,\"start\":2824},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":2830,\"start\":2827},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":2833,\"start\":2830},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2968,\"start\":2964},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2971,\"start\":2968},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":2974,\"start\":2971},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2977,\"start\":2974},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2980,\"start\":2977},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":2983,\"start\":2980},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":2986,\"start\":2983},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":2989,\"start\":2986},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":3531,\"start\":3527},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3939,\"start\":3936},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3941,\"start\":3939},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":3944,\"start\":3941},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4064,\"start\":4060},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4123,\"start\":4120},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4127,\"start\":4123},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4131,\"start\":4127},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4135,\"start\":4131},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4209,\"start\":4205},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4240,\"start\":4236},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4243,\"start\":4240},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4322,\"start\":4319},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4325,\"start\":4322},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":4328,\"start\":4325},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4364,\"start\":4361},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":4367,\"start\":4364},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":4939,\"start\":4935},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":5073,\"start\":5069},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5330,\"start\":5327},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5395,\"start\":5392},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":5958,\"start\":5954},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6033,\"start\":6029},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":6620,\"start\":6616},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6858,\"start\":6854},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":6861,\"start\":6858},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":6863,\"start\":6861},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7241,\"start\":7238},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8074,\"start\":8070},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8077,\"start\":8074},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8080,\"start\":8077},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8083,\"start\":8080},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8086,\"start\":8083},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":8089,\"start\":8086},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":8092,\"start\":8089},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":8095,\"start\":8092},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8127,\"start\":8123},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8210,\"start\":8206},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8296,\"start\":8292},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8384,\"start\":8380},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8387,\"start\":8384},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":8390,\"start\":8387},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":8393,\"start\":8390},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":8396,\"start\":8393},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":8504,\"start\":8500},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":8507,\"start\":8504},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8654,\"start\":8650},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8696,\"start\":8692},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8756,\"start\":8752},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8783,\"start\":8779},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8869,\"start\":8865},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9019,\"start\":9016},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":9022,\"start\":9019},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":9107,\"start\":9103},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":9110,\"start\":9107},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":9113,\"start\":9110},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":9442,\"start\":9438},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9541,\"start\":9537},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9598,\"start\":9595},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9601,\"start\":9598},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9604,\"start\":9601},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":9607,\"start\":9604},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9794,\"start\":9791},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9797,\"start\":9794},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9829,\"start\":9825},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9832,\"start\":9829},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9941,\"start\":9937},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":9969,\"start\":9965},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10267,\"start\":10264},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":10289,\"start\":10285},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10305,\"start\":10301},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":10444,\"start\":10440},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":10662,\"start\":10658},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":10672,\"start\":10668},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10688,\"start\":10684},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":13386,\"start\":13382},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":13451,\"start\":13447},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":14401,\"start\":14397},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":14749,\"start\":14745},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":15595,\"start\":15591},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":15978,\"start\":15974},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":16288,\"start\":16284},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":16966,\"start\":16962},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":17118,\"start\":17115},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":17482,\"start\":17478},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":17706,\"start\":17702},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":17822,\"start\":17818},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":18297,\"start\":18293},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":18800,\"start\":18796},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":19083,\"start\":19080},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":21959,\"start\":21956},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":21961,\"start\":21959},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":21963,\"start\":21961},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":22774,\"start\":22770},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":23595,\"start\":23591},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":23610,\"start\":23606},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":23934,\"start\":23930},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":24060,\"start\":24056},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":24792,\"start\":24788},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":25552,\"start\":25549},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":25959,\"start\":25955},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":26307,\"start\":26303},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":26322,\"start\":26318},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":26634,\"start\":26630},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":26895,\"start\":26891},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":27294,\"start\":27290},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":27494,\"start\":27490},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":27920,\"start\":27916},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":28046,\"start\":28042},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":28172,\"start\":28168},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":28198,\"start\":28195},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":28314,\"start\":28310},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":28340,\"start\":28337},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":32591,\"start\":32587},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":34248,\"start\":34244}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":32151,\"start\":32099},{\"attributes\":{\"id\":\"fig_1\"},\"end\":32267,\"start\":32152},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":32395,\"start\":32268},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":33266,\"start\":32396},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":33874,\"start\":33267},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":34168,\"start\":33875},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":35338,\"start\":34169},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":35642,\"start\":35339},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":35817,\"start\":35643}]", "paragraph": "[{\"end\":3417,\"start\":2581},{\"end\":5234,\"start\":3419},{\"end\":6621,\"start\":5236},{\"end\":7390,\"start\":6623},{\"end\":7924,\"start\":7392},{\"end\":9277,\"start\":7984},{\"end\":10125,\"start\":9309},{\"end\":10445,\"start\":10161},{\"end\":11597,\"start\":10447},{\"end\":11647,\"start\":11617},{\"end\":12018,\"start\":11667},{\"end\":12121,\"start\":12056},{\"end\":12446,\"start\":12155},{\"end\":12755,\"start\":12481},{\"end\":13904,\"start\":12803},{\"end\":14206,\"start\":13926},{\"end\":14286,\"start\":14219},{\"end\":14406,\"start\":14288},{\"end\":15838,\"start\":14435},{\"end\":17554,\"start\":15840},{\"end\":17977,\"start\":17588},{\"end\":18621,\"start\":17979},{\"end\":20094,\"start\":18675},{\"end\":21024,\"start\":20096},{\"end\":21230,\"start\":21170},{\"end\":21545,\"start\":21232},{\"end\":21965,\"start\":21547},{\"end\":22300,\"start\":22016},{\"end\":22517,\"start\":22340},{\"end\":23111,\"start\":22549},{\"end\":23294,\"start\":23147},{\"end\":23413,\"start\":23315},{\"end\":23616,\"start\":23457},{\"end\":23940,\"start\":23717},{\"end\":24170,\"start\":24035},{\"end\":24602,\"start\":24238},{\"end\":25403,\"start\":24641},{\"end\":26000,\"start\":25430},{\"end\":26082,\"start\":26023},{\"end\":26127,\"start\":26084},{\"end\":27714,\"start\":26129},{\"end\":28377,\"start\":27742},{\"end\":28999,\"start\":28379},{\"end\":29433,\"start\":29001},{\"end\":31418,\"start\":29452},{\"end\":32098,\"start\":31433}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11666,\"start\":11648},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12055,\"start\":12019},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12480,\"start\":12447},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12802,\"start\":12756},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13925,\"start\":13905},{\"attributes\":{\"id\":\"formula_5\"},\"end\":14434,\"start\":14407},{\"attributes\":{\"id\":\"formula_6\"},\"end\":18674,\"start\":18622},{\"attributes\":{\"id\":\"formula_7\"},\"end\":21169,\"start\":21025},{\"attributes\":{\"id\":\"formula_8\"},\"end\":22015,\"start\":21966},{\"attributes\":{\"id\":\"formula_9\"},\"end\":22339,\"start\":22301},{\"attributes\":{\"id\":\"formula_10\"},\"end\":23146,\"start\":23112},{\"attributes\":{\"id\":\"formula_11\"},\"end\":23456,\"start\":23414},{\"attributes\":{\"id\":\"formula_12\"},\"end\":23716,\"start\":23617},{\"attributes\":{\"id\":\"formula_13\"},\"end\":24034,\"start\":23941},{\"attributes\":{\"id\":\"formula_14\"},\"end\":24237,\"start\":24171}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2579,\"start\":2567},{\"attributes\":{\"n\":\"2.\"},\"end\":7939,\"start\":7927},{\"attributes\":{\"n\":\"2.1.\"},\"end\":7982,\"start\":7942},{\"attributes\":{\"n\":\"2.2.\"},\"end\":9307,\"start\":9280},{\"attributes\":{\"n\":\"3.\"},\"end\":10159,\"start\":10128},{\"attributes\":{\"n\":\"4.\"},\"end\":11615,\"start\":11600},{\"attributes\":{\"n\":\"4.1.\"},\"end\":12153,\"start\":12124},{\"attributes\":{\"n\":\"4.2.\"},\"end\":14217,\"start\":14209},{\"attributes\":{\"n\":\"4.3.\"},\"end\":17586,\"start\":17557},{\"attributes\":{\"n\":\"4.4.\"},\"end\":22547,\"start\":22520},{\"attributes\":{\"n\":\"4.5.\"},\"end\":23313,\"start\":23297},{\"attributes\":{\"n\":\"5.\"},\"end\":24616,\"start\":24605},{\"attributes\":{\"n\":\"5.1.\"},\"end\":24639,\"start\":24619},{\"attributes\":{\"n\":\"5.2.\"},\"end\":25428,\"start\":25406},{\"end\":26021,\"start\":26003},{\"attributes\":{\"n\":\"5.3.\"},\"end\":27740,\"start\":27717},{\"attributes\":{\"n\":\"5.4.\"},\"end\":29450,\"start\":29436},{\"attributes\":{\"n\":\"6.\"},\"end\":31431,\"start\":31421},{\"end\":32110,\"start\":32100},{\"end\":32173,\"start\":32153},{\"end\":32278,\"start\":32269},{\"end\":33277,\"start\":33268},{\"end\":33885,\"start\":33876},{\"end\":34188,\"start\":34170},{\"end\":35349,\"start\":35340}]", "table": "[{\"end\":32395,\"start\":32315},{\"end\":33266,\"start\":32663},{\"end\":33874,\"start\":33361},{\"end\":34168,\"start\":34086},{\"end\":35338,\"start\":34799},{\"end\":35642,\"start\":35553}]", "figure_caption": "[{\"end\":32151,\"start\":32112},{\"end\":32267,\"start\":32176},{\"end\":32315,\"start\":32280},{\"end\":32663,\"start\":32398},{\"end\":33361,\"start\":33279},{\"end\":34086,\"start\":33887},{\"end\":34799,\"start\":34191},{\"end\":35553,\"start\":35351},{\"end\":35817,\"start\":35645}]", "figure_ref": "[{\"end\":4454,\"start\":4448},{\"end\":14228,\"start\":14222},{\"end\":14237,\"start\":14229},{\"end\":16864,\"start\":16858},{\"end\":29070,\"start\":29064},{\"end\":29400,\"start\":29394}]", "bib_author_first_name": "[{\"end\":35900,\"start\":35892},{\"end\":35912,\"start\":35907},{\"end\":35927,\"start\":35922},{\"end\":35929,\"start\":35928},{\"end\":36419,\"start\":36413},{\"end\":36430,\"start\":36429},{\"end\":36436,\"start\":36431},{\"end\":36451,\"start\":36444},{\"end\":37025,\"start\":37019},{\"end\":37042,\"start\":37035},{\"end\":37061,\"start\":37054},{\"end\":37506,\"start\":37500},{\"end\":37521,\"start\":37516},{\"end\":37539,\"start\":37528},{\"end\":37556,\"start\":37549},{\"end\":38130,\"start\":38121},{\"end\":38150,\"start\":38143},{\"end\":38840,\"start\":38833},{\"end\":38858,\"start\":38849},{\"end\":38873,\"start\":38866},{\"end\":38891,\"start\":38884},{\"end\":38910,\"start\":38901},{\"end\":38927,\"start\":38921},{\"end\":39530,\"start\":39522},{\"end\":39787,\"start\":39779},{\"end\":39803,\"start\":39798},{\"end\":39820,\"start\":39816},{\"end\":40488,\"start\":40482},{\"end\":40504,\"start\":40500},{\"end\":40523,\"start\":40515},{\"end\":40529,\"start\":40528},{\"end\":40531,\"start\":40530},{\"end\":41074,\"start\":41069},{\"end\":41095,\"start\":41087},{\"end\":41678,\"start\":41673},{\"end\":41693,\"start\":41688},{\"end\":41695,\"start\":41694},{\"end\":42163,\"start\":42158},{\"end\":42181,\"start\":42173},{\"end\":42192,\"start\":42187},{\"end\":42194,\"start\":42193},{\"end\":42642,\"start\":42637},{\"end\":42657,\"start\":42652},{\"end\":42673,\"start\":42668},{\"end\":42686,\"start\":42681},{\"end\":42702,\"start\":42695},{\"end\":42718,\"start\":42713},{\"end\":42720,\"start\":42719},{\"end\":43034,\"start\":43033},{\"end\":43047,\"start\":43042},{\"end\":43049,\"start\":43048},{\"end\":43510,\"start\":43503},{\"end\":43512,\"start\":43511},{\"end\":43532,\"start\":43521},{\"end\":43549,\"start\":43542},{\"end\":44162,\"start\":44155},{\"end\":44174,\"start\":44167},{\"end\":44190,\"start\":44182},{\"end\":44200,\"start\":44196},{\"end\":44656,\"start\":44648},{\"end\":44668,\"start\":44663},{\"end\":44685,\"start\":44681},{\"end\":44704,\"start\":44696},{\"end\":44955,\"start\":44947},{\"end\":44966,\"start\":44960},{\"end\":44983,\"start\":44977},{\"end\":45526,\"start\":45518},{\"end\":45537,\"start\":45531},{\"end\":45558,\"start\":45548},{\"end\":45578,\"start\":45572},{\"end\":45899,\"start\":45893},{\"end\":45909,\"start\":45904},{\"end\":45921,\"start\":45916},{\"end\":45931,\"start\":45927},{\"end\":45946,\"start\":45939},{\"end\":46424,\"start\":46417},{\"end\":46439,\"start\":46432},{\"end\":46447,\"start\":46445},{\"end\":46460,\"start\":46453},{\"end\":46472,\"start\":46465},{\"end\":46485,\"start\":46478},{\"end\":46496,\"start\":46491},{\"end\":46504,\"start\":46502},{\"end\":47017,\"start\":47010},{\"end\":47031,\"start\":47027},{\"end\":47044,\"start\":47039},{\"end\":47643,\"start\":47636},{\"end\":47659,\"start\":47652},{\"end\":47672,\"start\":47666},{\"end\":47688,\"start\":47680},{\"end\":47704,\"start\":47697},{\"end\":47706,\"start\":47705},{\"end\":48169,\"start\":48167},{\"end\":48179,\"start\":48176},{\"end\":48189,\"start\":48186},{\"end\":48201,\"start\":48196},{\"end\":48211,\"start\":48208},{\"end\":48222,\"start\":48216},{\"end\":48520,\"start\":48515},{\"end\":48541,\"start\":48532},{\"end\":48906,\"start\":48900},{\"end\":48922,\"start\":48915},{\"end\":49397,\"start\":49391},{\"end\":49411,\"start\":49406},{\"end\":49427,\"start\":49420},{\"end\":49448,\"start\":49443},{\"end\":49985,\"start\":49981},{\"end\":49999,\"start\":49995},{\"end\":50015,\"start\":50010},{\"end\":50623,\"start\":50618},{\"end\":50639,\"start\":50633},{\"end\":51091,\"start\":51086},{\"end\":51109,\"start\":51103},{\"end\":51123,\"start\":51119},{\"end\":51142,\"start\":51134},{\"end\":51401,\"start\":51396},{\"end\":51413,\"start\":51406},{\"end\":51425,\"start\":51418},{\"end\":51439,\"start\":51431},{\"end\":51453,\"start\":51445},{\"end\":51466,\"start\":51458},{\"end\":51478,\"start\":51473},{\"end\":51986,\"start\":51978},{\"end\":51997,\"start\":51992},{\"end\":52009,\"start\":52005},{\"end\":52011,\"start\":52010},{\"end\":52029,\"start\":52022},{\"end\":52039,\"start\":52034},{\"end\":52343,\"start\":52337},{\"end\":52352,\"start\":52349},{\"end\":52365,\"start\":52358},{\"end\":52377,\"start\":52372},{\"end\":52387,\"start\":52384},{\"end\":52396,\"start\":52392},{\"end\":52398,\"start\":52397},{\"end\":52888,\"start\":52885},{\"end\":52898,\"start\":52894},{\"end\":52914,\"start\":52905},{\"end\":52927,\"start\":52920},{\"end\":52941,\"start\":52933},{\"end\":52951,\"start\":52946},{\"end\":52966,\"start\":52958},{\"end\":53485,\"start\":53480},{\"end\":53505,\"start\":53501},{\"end\":53507,\"start\":53506},{\"end\":53521,\"start\":53515},{\"end\":53535,\"start\":53530},{\"end\":54099,\"start\":54093},{\"end\":54117,\"start\":54111},{\"end\":54136,\"start\":54125},{\"end\":54634,\"start\":54633},{\"end\":54636,\"start\":54635},{\"end\":54651,\"start\":54643},{\"end\":54661,\"start\":54660},{\"end\":54994,\"start\":54992},{\"end\":55007,\"start\":55001},{\"end\":55016,\"start\":55013},{\"end\":55023,\"start\":55021},{\"end\":55580,\"start\":55572},{\"end\":55596,\"start\":55589},{\"end\":55613,\"start\":55605},{\"end\":55630,\"start\":55625},{\"end\":55645,\"start\":55640},{\"end\":55660,\"start\":55655},{\"end\":55662,\"start\":55661},{\"end\":56218,\"start\":56210},{\"end\":56233,\"start\":56228},{\"end\":56251,\"start\":56246},{\"end\":56267,\"start\":56264},{\"end\":56284,\"start\":56278},{\"end\":56286,\"start\":56285},{\"end\":56303,\"start\":56294},{\"end\":56855,\"start\":56849},{\"end\":56864,\"start\":56861},{\"end\":56877,\"start\":56871},{\"end\":56891,\"start\":56884},{\"end\":56903,\"start\":56897},{\"end\":57165,\"start\":57164},{\"end\":57167,\"start\":57166},{\"end\":57176,\"start\":57175},{\"end\":57178,\"start\":57177},{\"end\":57408,\"start\":57403},{\"end\":57428,\"start\":57422},{\"end\":57443,\"start\":57435},{\"end\":57454,\"start\":57450},{\"end\":57469,\"start\":57466},{\"end\":57471,\"start\":57470},{\"end\":57484,\"start\":57478},{\"end\":58049,\"start\":58044},{\"end\":58066,\"start\":58060},{\"end\":58080,\"start\":58075},{\"end\":58320,\"start\":58313},{\"end\":58333,\"start\":58332},{\"end\":58348,\"start\":58343},{\"end\":58350,\"start\":58349},{\"end\":58757,\"start\":58750},{\"end\":58775,\"start\":58769},{\"end\":58790,\"start\":58785},{\"end\":58809,\"start\":58802},{\"end\":59325,\"start\":59315},{\"end\":59341,\"start\":59336},{\"end\":59355,\"start\":59349},{\"end\":59805,\"start\":59798},{\"end\":59821,\"start\":59811},{\"end\":59834,\"start\":59827},{\"end\":60352,\"start\":60345},{\"end\":60367,\"start\":60358},{\"end\":60378,\"start\":60372},{\"end\":60391,\"start\":60385},{\"end\":60917,\"start\":60910},{\"end\":60932,\"start\":60923},{\"end\":60943,\"start\":60937},{\"end\":60952,\"start\":60948},{\"end\":61246,\"start\":61231},{\"end\":61260,\"start\":61253},{\"end\":61270,\"start\":61266},{\"end\":61273,\"start\":61271},{\"end\":61282,\"start\":61281},{\"end\":61287,\"start\":61283},{\"end\":61310,\"start\":61296},{\"end\":61326,\"start\":61320},{\"end\":62099,\"start\":62094},{\"end\":62554,\"start\":62548},{\"end\":62568,\"start\":62564},{\"end\":62582,\"start\":62578},{\"end\":62596,\"start\":62591},{\"end\":62613,\"start\":62608},{\"end\":62626,\"start\":62621},{\"end\":62628,\"start\":62627},{\"end\":62642,\"start\":62636},{\"end\":62656,\"start\":62651},{\"end\":63254,\"start\":63251},{\"end\":63266,\"start\":63261},{\"end\":63276,\"start\":63273},{\"end\":63283,\"start\":63281},{\"end\":64072,\"start\":64069},{\"end\":64084,\"start\":64079},{\"end\":64097,\"start\":64091},{\"end\":64110,\"start\":64103},{\"end\":64601,\"start\":64595},{\"end\":64617,\"start\":64608},{\"end\":64629,\"start\":64622},{\"end\":64643,\"start\":64636},{\"end\":64657,\"start\":64650},{\"end\":64668,\"start\":64665},{\"end\":64681,\"start\":64675},{\"end\":65150,\"start\":65145},{\"end\":65162,\"start\":65155},{\"end\":65368,\"start\":65359},{\"end\":65382,\"start\":65373},{\"end\":65397,\"start\":65390},{\"end\":65410,\"start\":65405},{\"end\":65412,\"start\":65411},{\"end\":65846,\"start\":65840},{\"end\":65859,\"start\":65853},{\"end\":65869,\"start\":65865},{\"end\":66297,\"start\":66294},{\"end\":66313,\"start\":66304},{\"end\":66325,\"start\":66320},{\"end\":66334,\"start\":66332},{\"end\":66347,\"start\":66341},{\"end\":66356,\"start\":66354},{\"end\":66369,\"start\":66363},{\"end\":66684,\"start\":66678},{\"end\":66696,\"start\":66689},{\"end\":66708,\"start\":66705},{\"end\":66718,\"start\":66714},{\"end\":67245,\"start\":67239},{\"end\":67257,\"start\":67250},{\"end\":67269,\"start\":67266},{\"end\":67282,\"start\":67275},{\"end\":67294,\"start\":67290},{\"end\":67583,\"start\":67576},{\"end\":67591,\"start\":67588},{\"end\":67604,\"start\":67597},{\"end\":67616,\"start\":67611},{\"end\":67626,\"start\":67623},{\"end\":67636,\"start\":67631},{\"end\":67651,\"start\":67645},{\"end\":67653,\"start\":67652}]", "bib_author_last_name": "[{\"end\":35905,\"start\":35901},{\"end\":35920,\"start\":35913},{\"end\":35935,\"start\":35930},{\"end\":36427,\"start\":36420},{\"end\":36442,\"start\":36437},{\"end\":36462,\"start\":36452},{\"end\":37033,\"start\":37026},{\"end\":37052,\"start\":37043},{\"end\":37066,\"start\":37062},{\"end\":37514,\"start\":37507},{\"end\":37526,\"start\":37522},{\"end\":37547,\"start\":37540},{\"end\":37561,\"start\":37557},{\"end\":38141,\"start\":38131},{\"end\":38159,\"start\":38151},{\"end\":38847,\"start\":38841},{\"end\":38864,\"start\":38859},{\"end\":38882,\"start\":38874},{\"end\":38899,\"start\":38892},{\"end\":38919,\"start\":38911},{\"end\":38937,\"start\":38928},{\"end\":39539,\"start\":39531},{\"end\":39796,\"start\":39788},{\"end\":39814,\"start\":39804},{\"end\":39829,\"start\":39821},{\"end\":40498,\"start\":40489},{\"end\":40513,\"start\":40505},{\"end\":40526,\"start\":40524},{\"end\":40536,\"start\":40532},{\"end\":40543,\"start\":40538},{\"end\":41085,\"start\":41075},{\"end\":41104,\"start\":41096},{\"end\":41686,\"start\":41679},{\"end\":41701,\"start\":41696},{\"end\":42171,\"start\":42164},{\"end\":42185,\"start\":42182},{\"end\":42200,\"start\":42195},{\"end\":42650,\"start\":42643},{\"end\":42666,\"start\":42658},{\"end\":42679,\"start\":42674},{\"end\":42693,\"start\":42687},{\"end\":42711,\"start\":42703},{\"end\":42726,\"start\":42721},{\"end\":43040,\"start\":43035},{\"end\":43057,\"start\":43050},{\"end\":43064,\"start\":43059},{\"end\":43519,\"start\":43513},{\"end\":43540,\"start\":43533},{\"end\":43558,\"start\":43550},{\"end\":44165,\"start\":44163},{\"end\":44180,\"start\":44175},{\"end\":44194,\"start\":44191},{\"end\":44204,\"start\":44201},{\"end\":44661,\"start\":44657},{\"end\":44679,\"start\":44669},{\"end\":44694,\"start\":44686},{\"end\":44713,\"start\":44705},{\"end\":44958,\"start\":44956},{\"end\":44975,\"start\":44967},{\"end\":44991,\"start\":44984},{\"end\":45529,\"start\":45527},{\"end\":45546,\"start\":45538},{\"end\":45570,\"start\":45559},{\"end\":45586,\"start\":45579},{\"end\":45902,\"start\":45900},{\"end\":45914,\"start\":45910},{\"end\":45925,\"start\":45922},{\"end\":45937,\"start\":45932},{\"end\":45949,\"start\":45947},{\"end\":46430,\"start\":46425},{\"end\":46443,\"start\":46440},{\"end\":46451,\"start\":46448},{\"end\":46463,\"start\":46461},{\"end\":46476,\"start\":46473},{\"end\":46489,\"start\":46486},{\"end\":46500,\"start\":46497},{\"end\":46507,\"start\":46505},{\"end\":47025,\"start\":47018},{\"end\":47037,\"start\":47032},{\"end\":47054,\"start\":47045},{\"end\":47650,\"start\":47644},{\"end\":47664,\"start\":47660},{\"end\":47678,\"start\":47673},{\"end\":47695,\"start\":47689},{\"end\":47712,\"start\":47707},{\"end\":48174,\"start\":48170},{\"end\":48184,\"start\":48180},{\"end\":48194,\"start\":48190},{\"end\":48206,\"start\":48202},{\"end\":48214,\"start\":48212},{\"end\":48226,\"start\":48223},{\"end\":48530,\"start\":48521},{\"end\":48563,\"start\":48542},{\"end\":48579,\"start\":48565},{\"end\":48913,\"start\":48907},{\"end\":48927,\"start\":48923},{\"end\":49122,\"start\":49115},{\"end\":49404,\"start\":49398},{\"end\":49418,\"start\":49412},{\"end\":49441,\"start\":49428},{\"end\":49456,\"start\":49449},{\"end\":49993,\"start\":49986},{\"end\":50008,\"start\":50000},{\"end\":50023,\"start\":50016},{\"end\":50631,\"start\":50624},{\"end\":50649,\"start\":50640},{\"end\":51101,\"start\":51092},{\"end\":51117,\"start\":51110},{\"end\":51132,\"start\":51124},{\"end\":51150,\"start\":51143},{\"end\":51404,\"start\":51402},{\"end\":51416,\"start\":51414},{\"end\":51429,\"start\":51426},{\"end\":51443,\"start\":51440},{\"end\":51456,\"start\":51454},{\"end\":51471,\"start\":51467},{\"end\":51482,\"start\":51479},{\"end\":51990,\"start\":51987},{\"end\":52003,\"start\":51998},{\"end\":52020,\"start\":52012},{\"end\":52032,\"start\":52030},{\"end\":52046,\"start\":52040},{\"end\":52347,\"start\":52344},{\"end\":52356,\"start\":52353},{\"end\":52370,\"start\":52366},{\"end\":52382,\"start\":52378},{\"end\":52390,\"start\":52388},{\"end\":52405,\"start\":52399},{\"end\":52892,\"start\":52889},{\"end\":52903,\"start\":52899},{\"end\":52918,\"start\":52915},{\"end\":52931,\"start\":52928},{\"end\":52944,\"start\":52942},{\"end\":52956,\"start\":52952},{\"end\":52969,\"start\":52967},{\"end\":53499,\"start\":53486},{\"end\":53513,\"start\":53508},{\"end\":53528,\"start\":53522},{\"end\":53544,\"start\":53536},{\"end\":54109,\"start\":54100},{\"end\":54123,\"start\":54118},{\"end\":54143,\"start\":54137},{\"end\":54641,\"start\":54637},{\"end\":54658,\"start\":54652},{\"end\":54674,\"start\":54662},{\"end\":54999,\"start\":54995},{\"end\":55011,\"start\":55008},{\"end\":55019,\"start\":55017},{\"end\":55028,\"start\":55024},{\"end\":55587,\"start\":55581},{\"end\":55603,\"start\":55597},{\"end\":55623,\"start\":55614},{\"end\":55638,\"start\":55631},{\"end\":55653,\"start\":55646},{\"end\":55668,\"start\":55663},{\"end\":56226,\"start\":56219},{\"end\":56244,\"start\":56234},{\"end\":56262,\"start\":56252},{\"end\":56276,\"start\":56268},{\"end\":56292,\"start\":56287},{\"end\":56319,\"start\":56304},{\"end\":56859,\"start\":56856},{\"end\":56869,\"start\":56865},{\"end\":56882,\"start\":56878},{\"end\":56895,\"start\":56892},{\"end\":56907,\"start\":56904},{\"end\":57173,\"start\":57168},{\"end\":57184,\"start\":57179},{\"end\":57420,\"start\":57409},{\"end\":57433,\"start\":57429},{\"end\":57448,\"start\":57444},{\"end\":57464,\"start\":57455},{\"end\":57476,\"start\":57472},{\"end\":57493,\"start\":57485},{\"end\":58058,\"start\":58050},{\"end\":58073,\"start\":58067},{\"end\":58090,\"start\":58081},{\"end\":58330,\"start\":58321},{\"end\":58341,\"start\":58334},{\"end\":58360,\"start\":58351},{\"end\":58367,\"start\":58362},{\"end\":58767,\"start\":58758},{\"end\":58783,\"start\":58776},{\"end\":58800,\"start\":58791},{\"end\":58814,\"start\":58810},{\"end\":59334,\"start\":59326},{\"end\":59347,\"start\":59342},{\"end\":59367,\"start\":59356},{\"end\":59809,\"start\":59806},{\"end\":59825,\"start\":59822},{\"end\":59838,\"start\":59835},{\"end\":60356,\"start\":60353},{\"end\":60370,\"start\":60368},{\"end\":60383,\"start\":60379},{\"end\":60394,\"start\":60392},{\"end\":60921,\"start\":60918},{\"end\":60935,\"start\":60933},{\"end\":60946,\"start\":60944},{\"end\":60961,\"start\":60953},{\"end\":61251,\"start\":61247},{\"end\":61264,\"start\":61261},{\"end\":61279,\"start\":61274},{\"end\":61294,\"start\":61288},{\"end\":61318,\"start\":61311},{\"end\":61340,\"start\":61327},{\"end\":62116,\"start\":62100},{\"end\":62127,\"start\":62118},{\"end\":62562,\"start\":62555},{\"end\":62576,\"start\":62569},{\"end\":62589,\"start\":62583},{\"end\":62606,\"start\":62597},{\"end\":62619,\"start\":62614},{\"end\":62634,\"start\":62629},{\"end\":62649,\"start\":62643},{\"end\":62667,\"start\":62657},{\"end\":63259,\"start\":63255},{\"end\":63271,\"start\":63267},{\"end\":63279,\"start\":63277},{\"end\":63288,\"start\":63284},{\"end\":64077,\"start\":64073},{\"end\":64089,\"start\":64085},{\"end\":64101,\"start\":64098},{\"end\":64114,\"start\":64111},{\"end\":64606,\"start\":64602},{\"end\":64620,\"start\":64618},{\"end\":64634,\"start\":64630},{\"end\":64648,\"start\":64644},{\"end\":64663,\"start\":64658},{\"end\":64673,\"start\":64669},{\"end\":64685,\"start\":64682},{\"end\":65153,\"start\":65151},{\"end\":65165,\"start\":65163},{\"end\":65371,\"start\":65369},{\"end\":65388,\"start\":65383},{\"end\":65403,\"start\":65398},{\"end\":65418,\"start\":65413},{\"end\":65851,\"start\":65847},{\"end\":65863,\"start\":65860},{\"end\":65872,\"start\":65870},{\"end\":66302,\"start\":66298},{\"end\":66318,\"start\":66314},{\"end\":66330,\"start\":66326},{\"end\":66339,\"start\":66335},{\"end\":66352,\"start\":66348},{\"end\":66361,\"start\":66357},{\"end\":66373,\"start\":66370},{\"end\":66687,\"start\":66685},{\"end\":66703,\"start\":66697},{\"end\":66712,\"start\":66709},{\"end\":66723,\"start\":66719},{\"end\":67248,\"start\":67246},{\"end\":67264,\"start\":67258},{\"end\":67273,\"start\":67270},{\"end\":67288,\"start\":67283},{\"end\":67299,\"start\":67295},{\"end\":67586,\"start\":67584},{\"end\":67595,\"start\":67592},{\"end\":67609,\"start\":67605},{\"end\":67621,\"start\":67617},{\"end\":67629,\"start\":67627},{\"end\":67643,\"start\":67637},{\"end\":67658,\"start\":67654}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":203158307},\"end\":36339,\"start\":35819},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":173990164},\"end\":36944,\"start\":36341},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":204512445},\"end\":37417,\"start\":36946},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":219966759},\"end\":38009,\"start\":37419},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":85562755},\"end\":38785,\"start\":38011},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":218889832},\"end\":39401,\"start\":38787},{\"attributes\":{\"id\":\"b6\"},\"end\":39702,\"start\":39403},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":17953812},\"end\":40427,\"start\":39704},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":4002773},\"end\":40949,\"start\":40429},{\"attributes\":{\"doi\":\"De- cember 11-16\",\"id\":\"b9\",\"matched_paper_id\":11631121},\"end\":41613,\"start\":40951},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":67855487},\"end\":42078,\"start\":41615},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":208202182},\"end\":42559,\"start\":42080},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":4607622},\"end\":42972,\"start\":42561},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":79123},\"end\":43415,\"start\":42974},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":202661057},\"end\":44107,\"start\":43417},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":206594692},\"end\":44584,\"start\":44109},{\"attributes\":{\"doi\":\"abs/2105.05582\",\"id\":\"b16\"},\"end\":44897,\"start\":44586},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":1931511},\"end\":45418,\"start\":44899},{\"attributes\":{\"doi\":\"abs/1608.08305\",\"id\":\"b18\"},\"end\":45811,\"start\":45420},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":219616239},\"end\":46338,\"start\":45813},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":219622248},\"end\":46928,\"start\":46340},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":202676699},\"end\":47592,\"start\":46930},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":13000587},\"end\":48092,\"start\":47594},{\"attributes\":{\"doi\":\"abs/2103.16284\",\"id\":\"b23\"},\"end\":48415,\"start\":48094},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":206359138},\"end\":48838,\"start\":48417},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":49185629},\"end\":49041,\"start\":48840},{\"attributes\":{\"id\":\"b26\"},\"end\":49313,\"start\":49043},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":2996070},\"end\":49916,\"start\":49315},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":3992628},\"end\":50497,\"start\":49918},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":20011878},\"end\":51039,\"start\":50499},{\"attributes\":{\"doi\":\"abs/2105.03824\",\"id\":\"b30\"},\"end\":51330,\"start\":51041},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":52831465},\"end\":51937,\"start\":51332},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":47252984},\"end\":52268,\"start\":51939},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":3518818},\"end\":52787,\"start\":52270},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":213176225},\"end\":53401,\"start\":52789},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":49656155},\"end\":54005,\"start\":53403},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":206429151},\"end\":54529,\"start\":54007},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":234842683},\"end\":54922,\"start\":54531},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":220484147},\"end\":55480,\"start\":54924},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":216345683},\"end\":56130,\"start\":55482},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":1949934},\"end\":56782,\"start\":56132},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":203704828},\"end\":57108,\"start\":56784},{\"attributes\":{\"doi\":\"abs/1209.2515\",\"id\":\"b42\"},\"end\":57315,\"start\":57110},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":67855581},\"end\":57975,\"start\":57317},{\"attributes\":{\"doi\":\"abs/2104.01894\",\"id\":\"b44\"},\"end\":58267,\"start\":57977},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":221716045},\"end\":58689,\"start\":58269},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":119188316},\"end\":59253,\"start\":58691},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":219177124},\"end\":59707,\"start\":59255},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":226220136},\"end\":60275,\"start\":59709},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":52955377},\"end\":60590,\"start\":60277},{\"attributes\":{\"id\":\"b50\"},\"end\":60834,\"start\":60592},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":218923747},\"end\":61161,\"start\":60836},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":173990158},\"end\":62022,\"start\":61163},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":13965190},\"end\":62519,\"start\":62024},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":13756489},\"end\":63151,\"start\":62521},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":213109282},\"end\":63956,\"start\":63153},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":204958091},\"end\":64535,\"start\":63958},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":227227748},\"end\":65122,\"start\":64537},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":4076251},\"end\":65287,\"start\":65124},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":12546432},\"end\":65809,\"start\":65289},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":152282473},\"end\":66194,\"start\":65811},{\"attributes\":{\"doi\":\"abs/2106.01883\",\"id\":\"b61\"},\"end\":66607,\"start\":66196},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":104292134},\"end\":67152,\"start\":66609},{\"attributes\":{\"doi\":\"abs/2102.04762\",\"id\":\"b63\"},\"end\":67499,\"start\":67154},{\"attributes\":{\"id\":\"b64\"},\"end\":67823,\"start\":67501}]", "bib_title": "[{\"end\":35890,\"start\":35819},{\"end\":36411,\"start\":36341},{\"end\":37017,\"start\":36946},{\"end\":37498,\"start\":37419},{\"end\":38119,\"start\":38011},{\"end\":38831,\"start\":38787},{\"end\":39777,\"start\":39704},{\"end\":40480,\"start\":40429},{\"end\":41067,\"start\":40951},{\"end\":41671,\"start\":41615},{\"end\":42156,\"start\":42080},{\"end\":42635,\"start\":42561},{\"end\":43031,\"start\":42974},{\"end\":43501,\"start\":43417},{\"end\":44153,\"start\":44109},{\"end\":44945,\"start\":44899},{\"end\":45891,\"start\":45813},{\"end\":46415,\"start\":46340},{\"end\":47008,\"start\":46930},{\"end\":47634,\"start\":47594},{\"end\":48513,\"start\":48417},{\"end\":48898,\"start\":48840},{\"end\":49113,\"start\":49043},{\"end\":49389,\"start\":49315},{\"end\":49979,\"start\":49918},{\"end\":50616,\"start\":50499},{\"end\":51394,\"start\":51332},{\"end\":51976,\"start\":51939},{\"end\":52335,\"start\":52270},{\"end\":52883,\"start\":52789},{\"end\":53478,\"start\":53403},{\"end\":54091,\"start\":54007},{\"end\":54631,\"start\":54531},{\"end\":54990,\"start\":54924},{\"end\":55570,\"start\":55482},{\"end\":56208,\"start\":56132},{\"end\":56847,\"start\":56784},{\"end\":57401,\"start\":57317},{\"end\":58311,\"start\":58269},{\"end\":58748,\"start\":58691},{\"end\":59313,\"start\":59255},{\"end\":59796,\"start\":59709},{\"end\":60343,\"start\":60277},{\"end\":60908,\"start\":60836},{\"end\":61229,\"start\":61163},{\"end\":62092,\"start\":62024},{\"end\":62546,\"start\":62521},{\"end\":63249,\"start\":63153},{\"end\":64067,\"start\":63958},{\"end\":64593,\"start\":64537},{\"end\":65143,\"start\":65124},{\"end\":65357,\"start\":65289},{\"end\":65838,\"start\":65811},{\"end\":66676,\"start\":66609}]", "bib_author": "[{\"end\":35907,\"start\":35892},{\"end\":35922,\"start\":35907},{\"end\":35937,\"start\":35922},{\"end\":36429,\"start\":36413},{\"end\":36444,\"start\":36429},{\"end\":36464,\"start\":36444},{\"end\":37035,\"start\":37019},{\"end\":37054,\"start\":37035},{\"end\":37068,\"start\":37054},{\"end\":37516,\"start\":37500},{\"end\":37528,\"start\":37516},{\"end\":37549,\"start\":37528},{\"end\":37563,\"start\":37549},{\"end\":38143,\"start\":38121},{\"end\":38161,\"start\":38143},{\"end\":38849,\"start\":38833},{\"end\":38866,\"start\":38849},{\"end\":38884,\"start\":38866},{\"end\":38901,\"start\":38884},{\"end\":38921,\"start\":38901},{\"end\":38939,\"start\":38921},{\"end\":39541,\"start\":39522},{\"end\":39798,\"start\":39779},{\"end\":39816,\"start\":39798},{\"end\":39831,\"start\":39816},{\"end\":40500,\"start\":40482},{\"end\":40515,\"start\":40500},{\"end\":40528,\"start\":40515},{\"end\":40538,\"start\":40528},{\"end\":40545,\"start\":40538},{\"end\":41087,\"start\":41069},{\"end\":41106,\"start\":41087},{\"end\":41688,\"start\":41673},{\"end\":41703,\"start\":41688},{\"end\":42173,\"start\":42158},{\"end\":42187,\"start\":42173},{\"end\":42202,\"start\":42187},{\"end\":42652,\"start\":42637},{\"end\":42668,\"start\":42652},{\"end\":42681,\"start\":42668},{\"end\":42695,\"start\":42681},{\"end\":42713,\"start\":42695},{\"end\":42728,\"start\":42713},{\"end\":43042,\"start\":43033},{\"end\":43059,\"start\":43042},{\"end\":43066,\"start\":43059},{\"end\":43521,\"start\":43503},{\"end\":43542,\"start\":43521},{\"end\":43560,\"start\":43542},{\"end\":44167,\"start\":44155},{\"end\":44182,\"start\":44167},{\"end\":44196,\"start\":44182},{\"end\":44206,\"start\":44196},{\"end\":44663,\"start\":44648},{\"end\":44681,\"start\":44663},{\"end\":44696,\"start\":44681},{\"end\":44715,\"start\":44696},{\"end\":44960,\"start\":44947},{\"end\":44977,\"start\":44960},{\"end\":44993,\"start\":44977},{\"end\":45531,\"start\":45518},{\"end\":45548,\"start\":45531},{\"end\":45572,\"start\":45548},{\"end\":45588,\"start\":45572},{\"end\":45904,\"start\":45893},{\"end\":45916,\"start\":45904},{\"end\":45927,\"start\":45916},{\"end\":45939,\"start\":45927},{\"end\":45951,\"start\":45939},{\"end\":46432,\"start\":46417},{\"end\":46445,\"start\":46432},{\"end\":46453,\"start\":46445},{\"end\":46465,\"start\":46453},{\"end\":46478,\"start\":46465},{\"end\":46491,\"start\":46478},{\"end\":46502,\"start\":46491},{\"end\":46509,\"start\":46502},{\"end\":47027,\"start\":47010},{\"end\":47039,\"start\":47027},{\"end\":47056,\"start\":47039},{\"end\":47652,\"start\":47636},{\"end\":47666,\"start\":47652},{\"end\":47680,\"start\":47666},{\"end\":47697,\"start\":47680},{\"end\":47714,\"start\":47697},{\"end\":48176,\"start\":48167},{\"end\":48186,\"start\":48176},{\"end\":48196,\"start\":48186},{\"end\":48208,\"start\":48196},{\"end\":48216,\"start\":48208},{\"end\":48228,\"start\":48216},{\"end\":48532,\"start\":48515},{\"end\":48565,\"start\":48532},{\"end\":48581,\"start\":48565},{\"end\":48915,\"start\":48900},{\"end\":48929,\"start\":48915},{\"end\":49124,\"start\":49115},{\"end\":49406,\"start\":49391},{\"end\":49420,\"start\":49406},{\"end\":49443,\"start\":49420},{\"end\":49458,\"start\":49443},{\"end\":49995,\"start\":49981},{\"end\":50010,\"start\":49995},{\"end\":50025,\"start\":50010},{\"end\":50633,\"start\":50618},{\"end\":50651,\"start\":50633},{\"end\":51103,\"start\":51086},{\"end\":51119,\"start\":51103},{\"end\":51134,\"start\":51119},{\"end\":51152,\"start\":51134},{\"end\":51406,\"start\":51396},{\"end\":51418,\"start\":51406},{\"end\":51431,\"start\":51418},{\"end\":51445,\"start\":51431},{\"end\":51458,\"start\":51445},{\"end\":51473,\"start\":51458},{\"end\":51484,\"start\":51473},{\"end\":51992,\"start\":51978},{\"end\":52005,\"start\":51992},{\"end\":52022,\"start\":52005},{\"end\":52034,\"start\":52022},{\"end\":52048,\"start\":52034},{\"end\":52349,\"start\":52337},{\"end\":52358,\"start\":52349},{\"end\":52372,\"start\":52358},{\"end\":52384,\"start\":52372},{\"end\":52392,\"start\":52384},{\"end\":52407,\"start\":52392},{\"end\":52894,\"start\":52885},{\"end\":52905,\"start\":52894},{\"end\":52920,\"start\":52905},{\"end\":52933,\"start\":52920},{\"end\":52946,\"start\":52933},{\"end\":52958,\"start\":52946},{\"end\":52971,\"start\":52958},{\"end\":53501,\"start\":53480},{\"end\":53515,\"start\":53501},{\"end\":53530,\"start\":53515},{\"end\":53546,\"start\":53530},{\"end\":54111,\"start\":54093},{\"end\":54125,\"start\":54111},{\"end\":54145,\"start\":54125},{\"end\":54643,\"start\":54633},{\"end\":54660,\"start\":54643},{\"end\":54676,\"start\":54660},{\"end\":55001,\"start\":54992},{\"end\":55013,\"start\":55001},{\"end\":55021,\"start\":55013},{\"end\":55030,\"start\":55021},{\"end\":55589,\"start\":55572},{\"end\":55605,\"start\":55589},{\"end\":55625,\"start\":55605},{\"end\":55640,\"start\":55625},{\"end\":55655,\"start\":55640},{\"end\":55670,\"start\":55655},{\"end\":56228,\"start\":56210},{\"end\":56246,\"start\":56228},{\"end\":56264,\"start\":56246},{\"end\":56278,\"start\":56264},{\"end\":56294,\"start\":56278},{\"end\":56321,\"start\":56294},{\"end\":56861,\"start\":56849},{\"end\":56871,\"start\":56861},{\"end\":56884,\"start\":56871},{\"end\":56897,\"start\":56884},{\"end\":56909,\"start\":56897},{\"end\":57175,\"start\":57164},{\"end\":57186,\"start\":57175},{\"end\":57422,\"start\":57403},{\"end\":57435,\"start\":57422},{\"end\":57450,\"start\":57435},{\"end\":57466,\"start\":57450},{\"end\":57478,\"start\":57466},{\"end\":57495,\"start\":57478},{\"end\":58060,\"start\":58044},{\"end\":58075,\"start\":58060},{\"end\":58092,\"start\":58075},{\"end\":58332,\"start\":58313},{\"end\":58343,\"start\":58332},{\"end\":58362,\"start\":58343},{\"end\":58369,\"start\":58362},{\"end\":58769,\"start\":58750},{\"end\":58785,\"start\":58769},{\"end\":58802,\"start\":58785},{\"end\":58816,\"start\":58802},{\"end\":59336,\"start\":59315},{\"end\":59349,\"start\":59336},{\"end\":59369,\"start\":59349},{\"end\":59811,\"start\":59798},{\"end\":59827,\"start\":59811},{\"end\":59840,\"start\":59827},{\"end\":60358,\"start\":60345},{\"end\":60372,\"start\":60358},{\"end\":60385,\"start\":60372},{\"end\":60396,\"start\":60385},{\"end\":60923,\"start\":60910},{\"end\":60937,\"start\":60923},{\"end\":60948,\"start\":60937},{\"end\":60963,\"start\":60948},{\"end\":61253,\"start\":61231},{\"end\":61266,\"start\":61253},{\"end\":61281,\"start\":61266},{\"end\":61296,\"start\":61281},{\"end\":61320,\"start\":61296},{\"end\":61342,\"start\":61320},{\"end\":62118,\"start\":62094},{\"end\":62129,\"start\":62118},{\"end\":62564,\"start\":62548},{\"end\":62578,\"start\":62564},{\"end\":62591,\"start\":62578},{\"end\":62608,\"start\":62591},{\"end\":62621,\"start\":62608},{\"end\":62636,\"start\":62621},{\"end\":62651,\"start\":62636},{\"end\":62669,\"start\":62651},{\"end\":63261,\"start\":63251},{\"end\":63273,\"start\":63261},{\"end\":63281,\"start\":63273},{\"end\":63290,\"start\":63281},{\"end\":64079,\"start\":64069},{\"end\":64091,\"start\":64079},{\"end\":64103,\"start\":64091},{\"end\":64116,\"start\":64103},{\"end\":64608,\"start\":64595},{\"end\":64622,\"start\":64608},{\"end\":64636,\"start\":64622},{\"end\":64650,\"start\":64636},{\"end\":64665,\"start\":64650},{\"end\":64675,\"start\":64665},{\"end\":64687,\"start\":64675},{\"end\":65155,\"start\":65145},{\"end\":65167,\"start\":65155},{\"end\":65373,\"start\":65359},{\"end\":65390,\"start\":65373},{\"end\":65405,\"start\":65390},{\"end\":65420,\"start\":65405},{\"end\":65853,\"start\":65840},{\"end\":65865,\"start\":65853},{\"end\":65874,\"start\":65865},{\"end\":66304,\"start\":66294},{\"end\":66320,\"start\":66304},{\"end\":66332,\"start\":66320},{\"end\":66341,\"start\":66332},{\"end\":66354,\"start\":66341},{\"end\":66363,\"start\":66354},{\"end\":66375,\"start\":66363},{\"end\":66689,\"start\":66678},{\"end\":66705,\"start\":66689},{\"end\":66714,\"start\":66705},{\"end\":66725,\"start\":66714},{\"end\":67250,\"start\":67239},{\"end\":67266,\"start\":67250},{\"end\":67275,\"start\":67266},{\"end\":67290,\"start\":67275},{\"end\":67301,\"start\":67290},{\"end\":67588,\"start\":67576},{\"end\":67597,\"start\":67588},{\"end\":67611,\"start\":67597},{\"end\":67623,\"start\":67611},{\"end\":67631,\"start\":67623},{\"end\":67645,\"start\":67631},{\"end\":67660,\"start\":67645}]", "bib_venue": "[{\"end\":36031,\"start\":35937},{\"end\":36576,\"start\":36464},{\"end\":37124,\"start\":37068},{\"end\":37680,\"start\":37563},{\"end\":38265,\"start\":38161},{\"end\":38991,\"start\":38939},{\"end\":39520,\"start\":39403},{\"end\":39918,\"start\":39831},{\"end\":40608,\"start\":40545},{\"end\":41242,\"start\":41122},{\"end\":41788,\"start\":41703},{\"end\":42258,\"start\":42202},{\"end\":42747,\"start\":42728},{\"end\":43134,\"start\":43066},{\"end\":43637,\"start\":43560},{\"end\":44280,\"start\":44206},{\"end\":44646,\"start\":44586},{\"end\":45056,\"start\":44993},{\"end\":45516,\"start\":45420},{\"end\":46018,\"start\":45951},{\"end\":46576,\"start\":46509},{\"end\":47133,\"start\":47056},{\"end\":47773,\"start\":47714},{\"end\":48165,\"start\":48094},{\"end\":48612,\"start\":48581},{\"end\":48936,\"start\":48929},{\"end\":49128,\"start\":49124},{\"end\":49534,\"start\":49458},{\"end\":50093,\"start\":50025},{\"end\":50721,\"start\":50651},{\"end\":51084,\"start\":51041},{\"end\":51558,\"start\":51484},{\"end\":52086,\"start\":52048},{\"end\":52455,\"start\":52407},{\"end\":53038,\"start\":52971},{\"end\":53598,\"start\":53546},{\"end\":54199,\"start\":54145},{\"end\":54707,\"start\":54676},{\"end\":55119,\"start\":55030},{\"end\":55747,\"start\":55670},{\"end\":56384,\"start\":56321},{\"end\":56927,\"start\":56909},{\"end\":57162,\"start\":57110},{\"end\":57564,\"start\":57495},{\"end\":58042,\"start\":57977},{\"end\":58433,\"start\":58369},{\"end\":58907,\"start\":58816},{\"end\":59433,\"start\":59369},{\"end\":59892,\"start\":59840},{\"end\":60428,\"start\":60396},{\"end\":60612,\"start\":60592},{\"end\":60981,\"start\":60963},{\"end\":61435,\"start\":61342},{\"end\":62172,\"start\":62129},{\"end\":62781,\"start\":62669},{\"end\":63369,\"start\":63290},{\"end\":64206,\"start\":64116},{\"end\":64765,\"start\":64687},{\"end\":65186,\"start\":65167},{\"end\":65478,\"start\":65420},{\"end\":65964,\"start\":65874},{\"end\":66292,\"start\":66196},{\"end\":66794,\"start\":66725},{\"end\":67237,\"start\":67154},{\"end\":67574,\"start\":67501},{\"end\":36046,\"start\":36033},{\"end\":36608,\"start\":36578},{\"end\":37147,\"start\":37126},{\"end\":38376,\"start\":38267},{\"end\":39008,\"start\":38993},{\"end\":40009,\"start\":39920},{\"end\":40633,\"start\":40610},{\"end\":41256,\"start\":41244},{\"end\":41814,\"start\":41790},{\"end\":42281,\"start\":42260},{\"end\":43155,\"start\":43136},{\"end\":43717,\"start\":43639},{\"end\":44300,\"start\":44282},{\"end\":45073,\"start\":45058},{\"end\":46036,\"start\":46020},{\"end\":46594,\"start\":46578},{\"end\":47220,\"start\":47135},{\"end\":47792,\"start\":47775},{\"end\":49570,\"start\":49553},{\"end\":50111,\"start\":50095},{\"end\":50739,\"start\":50723},{\"end\":51583,\"start\":51560},{\"end\":52470,\"start\":52457},{\"end\":53056,\"start\":53040},{\"end\":53615,\"start\":53600},{\"end\":54218,\"start\":54201},{\"end\":55195,\"start\":55121},{\"end\":55765,\"start\":55749},{\"end\":56404,\"start\":56386},{\"end\":57585,\"start\":57566},{\"end\":58453,\"start\":58435},{\"end\":59453,\"start\":59435},{\"end\":59905,\"start\":59894},{\"end\":60636,\"start\":60614},{\"end\":61530,\"start\":61437},{\"end\":62198,\"start\":62174},{\"end\":62802,\"start\":62783},{\"end\":63388,\"start\":63371},{\"end\":65495,\"start\":65480},{\"end\":66815,\"start\":66796}]"}}}, "year": 2023, "month": 12, "day": 17}