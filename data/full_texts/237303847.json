{"id": 237303847, "updated": "2023-10-05 23:49:41.978", "metadata": {"title": "Learning to Diversify for Single Domain Generalization", "authors": "[{\"first\":\"Zijian\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Yadan\",\"last\":\"Luo\",\"middle\":[]},{\"first\":\"Ruihong\",\"last\":\"Qiu\",\"middle\":[]},{\"first\":\"Zi\",\"last\":\"Huang\",\"middle\":[]},{\"first\":\"Mahsa\",\"last\":\"Baktashmotlagh\",\"middle\":[]}]", "venue": "ArXiv", "journal": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Domain generalization (DG) aims to generalize a model trained on multiple source (i.e., training) domains to a distributionally different target (i.e., test) domain. In contrast to the conventional DG that strictly requires the availability of multiple source domains, this paper considers a more realistic yet challenging scenario, namely Single Domain Generalization (Single-DG), where only one source domain is available for training. In this scenario, the limited diversity may jeopardize the model generalization on unseen target domains. To tackle this problem, we propose a style-complement module to enhance the generalization power of the model by synthesizing images from diverse distributions that are complementary to the source ones. More specifically, we adopt a tractable upper bound of mutual information (MI) between the generated and source samples and perform a two-step optimization iteratively: (1) by minimizing the MI upper bound approximation for each sample pair, the generated images are forced to be diversified from the source samples; (2) subsequently, we maximize the MI between the samples from the same semantic category, which assists the network to learn discriminative features from diverse-styled images. Extensive experiments on three benchmark datasets demonstrate the superiority of our approach, which surpasses the state-of-the-art single-DG methods by up to 25.14%.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2108.11726", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iccv/WangLQHB21", "doi": "10.1109/iccv48922.2021.00087"}}, "content": {"source": {"pdf_hash": "1109f18a41ad21635ae30952be89f56f52295f39", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2108.11726v3.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2108.11726", "status": "GREEN"}}, "grobid": {"id": "48da4bcadeef5d134bba19c71496a41feaa00a4a", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/1109f18a41ad21635ae30952be89f56f52295f39.txt", "contents": "\nLearning to Diversify for Single Domain Generalization\n\n\nZijian Wang \nUniversity of Queensland\n\n\nYadan Luo \nUniversity of Queensland\n\n\nRuihong Qiu \nUniversity of Queensland\n\n\nZi Huang huang@itee.uq.edu.au \nUniversity of Queensland\n\n\nMahsa Baktashmotlagh m.baktashmotlagh@uq.edu.au \nUniversity of Queensland\n\n\nLearning to Diversify for Single Domain Generalization\n\nDomain generalization (DG) aims to generalize a model trained on multiple source (i.e., training) domains to a distributionally different target (i.e., test) domain. In contrast to the conventional DG that strictly requires the availability of multiple source domains, this paper considers a more realistic yet challenging scenario, namely Single Domain Generalization (Single-DG), where only one source domain is available for training. In this scenario, the limited diversity may jeopardize the model generalization on unseen target domains. To tackle this problem, we propose a style-complement module to enhance the generalization power of the model by synthesizing images from diverse distributions that are complementary to the source ones. More specifically, we adopt a tractable upper bound of mutual information (MI) between the generated and source samples and perform a two-step optimization iteratively: (1) by minimizing the MI upper bound approximation for each sample pair, the generated images are forced to be diversified from the source samples; (2) subsequently, we maximize the MI between the samples from the same semantic category, which assists the network to learn discriminative features from diversestyled images. Extensive experiments on three benchmark datasets demonstrate the superiority of our approach, which surpasses the state-of-the-art single-DG methods by up to 25.14%. The code will be publicly available at https://github.com/BUserName/Learning to diversify\n\nIntroduction\n\nThe remarkable success of modern machine learning algorithms is built on the assumption that the source (i.e., training) and target (i.e., test) samples are drawn from similar distributions. In practice, this assumption is commonly violated by various factors, such as the changes of illuminations, object appearance, or background, which are known as the domain shift problem [36,3]. Due to the discrepancy across domains, the performance of a model trained on the source domain can be significantly degraded when applied to the target domain.\n\nTo tackle this problem, extensive research has been carried out mainly on domain adaptation and domain generalization. Domain adaptation aims to transfer the knowledge from the labeled source domain(s) to an unlabeled target domain [45,1,26,13], while domain generalization attempts to generalize a model to an unseen target domain by learning from multiple source domains [34,42,10,42]. Compared with domain adaptation, domain generalization (DG) is considered as a more challenging task as the target samples are not exposed in the training phase. Regarding different strategies for transferring knowledge from source domains to the unseen target domain, existing DG techniques can be subsumed under two broad categories, i.e., alignment-based [2,11,33] and augmentation-based [5,17,44,51,48]. Technically, alignment-based approaches aim to reach the consensus from multiple source domains and learn domain-invariant latent representations for the target domain. Augmentation-based approaches learn to augment the source images with different transformations or generate the pseudo-novel samples for each source domain.\n\nIn general, the paradigm of DG relies on the availability of multiple source domains. However, it is more plausible to consider a more realistic scenario namely single domain generalization (Single-DG), where only one source domain is at hand during training. Despite DG has been extensively studied, single-DG remains under-explored. It is nontrivial for prior DG methods to handle this new setting, as the samples gathered from multiple sources and the domain identifiers are no longer accessible. Without the domain information to rely on, neither alignment-based nor augmentationbased models could well identify the domain-invariant features or transformations that are robust to unseen target domain shifts. Very recently, a few works [37,50] are proposed that learn to add adversarial noises on the source images in order to train robust classifiers against unforeseen data shifts. While contributing positively to address the model vulnerability, the manipulated images are indistinguishable from the original source images, which are not sufficiently diverse to cover the real target distributions.\n\nTo address the issue of insufficient diversity, in this paper, we propose a novel approach of Learning-to-diversify (L2D), which aims to improve the model generalization capacity by alternating diverse sample generation and discriminative style-invariant representation learning as illustrated in Figure 1. Specifically, we design a stylecomplement module, which learns to synthesize samples with unseen styles, which are out of original distributions. Different from previous augmentation approaches that quantify diversity with the Euclidean distance in the image space, we diversify the generated samples in the latent feature space. By explicitly posing a greater challenge on the trained classifier, the model resilience against the target shift is enhanced. We gradually enlarge the shift between the distributions of the generated samples and the source samples at the training time and perform the two-step optimization iteratively. By minimizing the tractable upper bound of mutual information for each sample pair, the generated images are forced to diversify from the source samples in subspace. Furthermore, to obtain the style-invariant features, we maximize the mutual information between the images belonging to the same semantic category. Consequently, the style-complement module and the task model compete in a min-max game, which improves the generalization ability of the task model by iteratively generating out-of-domain images and optimizing the style-invariant latent space. Note that, under this objective, the images generated by the proposed style-complement module will not only being diversified from the source ones, but also can be considered as challenging samples to the task model.\n\nThe main contributions of our work are summarized as follows.\n\n\u2022 We propose a style-complement module that addresses the single domain generalization by learning to generate diverse images.\n\n\u2022 A min-max mutual information optimization strategy is designed to gradually enlarge the distribution shift between the generated and the source images, while simultaneously bringing the samples from the same semantic category close in the latent space.\n\n\u2022 To validate the effectiveness of the proposed method, extensive experiments are conducted on three benchmark datasets, including digits recognition, corrupted CIFAR-10, and PACS. We further show the effectiveness of our approach in the standard DG setting under the leave-one-domain-out protocol. The results clearly demonstrate that our method surpasses the state-ofthe-art DG and Single-DG methods on all datasets.\n\n\nRelated Work\n\nDomain Shift. Most of the existing machine learning methods suffer from performance degradation when the source (i.e. training) domain and the target (i.e. test) domain follow different distributions. The difference between the distributions is termed as the domain shift [36,8]. In computer vision applications, such a shift may be brought by but not limited to environment and style changes. To this problem, domain adaptation (DA) approaches have been proposed to minimize the domain shift across the domains by matching the marginal [26,13,1] or conditional distributions [27,29] of the source and target domains. Domain adaptation has been widely studied in various settings, such as semi-supervised [18,38,46] and unsupervised scenarios [26,28], which leverage the partially labelled or unlabelled target domain in the training process. More recently, fewshot DA [32] is proposed, where only a few labelled target samples together with source samples are available in the training phase. Domain Generalization. The most significant difference between domain adaptation (DA) and domain generalization (DG) is that DG does not require access to the target domain in the training phase. Existing DG methods can be rough classified into two categories of learning the domain invariant representation and data augmentation. The key idea behind the former category is to reduce the discrepancy between representations of multiple source domains. Muandet et al. [34] first proposed a kernel-based method to obtain domain invariant features. [14] learns the latent invariant representation by jointly considering the domain reconstruction task. [33] further introduces a contrastive semantic alignment loss, which encourages the intra-class similarity and inter-class difference. Li et al. proposed to reduce the gap across the domains by minimizing Maximum Mean Discrepancy (MMD) under adversarial autoencoder framework. [44] aims to learn a domain agnostic representation by enforcing an orthogonal constrain on textural information of images and the corresponding latent representations.\n\nRecently, meta-learning procedure has been studied to solve DG problem [10,24,11]. Li et al. put forward a gradientbased model agnostic meta-learning algorithm for DG. Dou et al. [10] exploits the episodic training scheme to enforce the global and local alignment. [11] incorporates variational information bottleneck with meta-learning to narrow the domain gap between the source domains. The other category is related to data augmentation. This line of work generally aims to generate out-of-domain samples, which are then used to train the network along with the source samples to improve the generalization ability. For instance, [42] exploits the adversarial training scheme to generate 'hard' samples for the classifier. Shankar et al. [39] proposed to augment the source samples along the direction of the domain change. [ Figure 1: The overall framework of the proposed Learning-to-diversify (L2D). L2D alternatively trains the style-complement module G(\u00b7; \u03b8 G ) and the task model F (\u00b7; \u03b8 F ), q(\u00b7; \u03b8 q ), and H(\u00b7; \u03b8 H ). Specifically, (1) the upper bound of mutual information (MI) is minimized between the source and generated images, and (2)MI among samples belonging to the same category is maximized. It enhances the generalization power of the task model in an adversarial min-max manner.\n\native adversarial network (GAN) to synthesize data from pseudo-novel domain. [5] is considered as another type of augmentation, which exploits an auxiliary self-supervision training signal from solving a jigsaw puzzle to improve the generalization ability of the classifier. This paper focuses on a more challenging yet realistic setting, namely single domain generalization [37,50]. In Single-DG, the network is trained on a single source domain, while it is evaluated on multiple unseen domains. Gradient-based image augmentation is an effective strategy for Single-DG. [37] improves the ADA by encouraging semantic consistency between the augmented and source images in the latent space via an auxiliary Wasserstein autoencoder. [50] considers entropy maximization in the adversarial training framework to generate challenging perturbations of the source samples. In all the aforementioned approaches for Single-DG, the visual differences between the source and generated images are mostly depicted in the color and texture of the augmented samples. Different from existing Single-DG methods, our method aims to generate diverse samples with novel style/texture/appearance having a larger shift from the source distribution, and thus can be considered as complementary to the source data distribution.\n\n\nMethodology\n\nGiven a source domain S = {x i , y i } N i=1 of N samples, the goal of single domain generalization is to learn a model that can generalize to many unseen target domains T . With no prior knowledge on the target domain, we propose a style-complement module G(\u00b7; \u03b8 G ) :\n\nx \u2192 x + to augment the source domain by synthesizing the x + that shares the same semantic information with the source image x, albeit with different styles. As shown in Figure 1, we firstly apply the feature extractor F (\u00b7; \u03b8 F ) to transform images x and x + to latent vectors z and z + . To diversify the generated features from the source samples, the MI upper bound approximation for each pair z and z + is minimized to learn G; subsequently, we freeze G and maximize MI between the z and z + from the same semantic category, which assists the task network F and classifier H(\u00b7; \u03b8 H ) parameterized by \u03b8 H to learn discriminative features from diverse-styled images.\n\nStyle-Complement Module. The style-complement module G(\u00b7; \u03b8 G ) consists of K transformations, each of which is comprised of a convolutional layer, a style learning layer, and a transposed convolutional layer. By applying the convolution operations, the source images are projected from the original distribution to a novel distribution with arbitrary style shift. We further enhance the diversity of the generated samples by creating the style shift at the pixel level. Specifically, we add learnable parameters \u03b8 G,k = {\u00b5 k , \u03c3 k } for each of the k transformations, where the learnable parameters \u00b5 k , \u03c3 k \u2208 R h * w * c are the mean shift and variance shift. More concretely, we have:\nT (f i,k ; \u03b8 G,k ) = \u03c3 k * f i,k \u2212 \u00b5 \u03c3 + \u00b5 k ,(1)\nwhere f i,k \u2208 R h * w * c is the output of the convolutional operation applied on x i in the k-th transformation, with h, w and c representing the height, width, and channel, respectively. \u00b5 and \u03c3 correspond to the mean and covariance of the f i,k . The transformed feature map f i,k = T (f i,k ; \u03b8 G,k ) are then applied with the transposed convolutional operation to reconstruct to the original image dimension of x. The final outcome x + i of the style-complement module is the linear combination of the augmented images x + i,k obtained from k-th transformation:\nx + i = 1 K k=1 (w k ) K k=1 (w k \u03c3(x + i,k )), w k \u223c N ormal(0, 1)(2)\nwhere w k is a scalar sampled from a normal distribution and weights the contribution of augmented image x i,k from the transformation k to the output augmented image x + i . We apply the activation function \u03c3(\u00b7), e.g. tanh, to scale x + i . Synthesizing Novel Styles. The objective of the stylecomplement module is to generalize from the source domain distribution to the out-of-domain distribution. To increase the diversity of the created styles, the correlation between the generated images and the source images should be minimized. Mutual information (MI) I(z; z + ) serves as a measure of quantifying the correlation of z and z + , which is defined as:\nI(z; z + ) = E p(z,z + ) log p(z + |z) p(z + )(3)\nWe minimize the mutual information (MI) between the source and generated images in the latent feature space Z, achieved by passing images through F (\u00b7; \u03b8 F ). The upper bound of MI defined in [7] is:\nI(z; z + ) \u2264 E p(z,z + ) [log p(z + |z)]\u2212E p(z)p(z + ) [log p(z + |z)],(4)\nwhere z and z + are the respective latent vectors of the source image x and the generated image x + .\n\nSince the conditional distribution p(z + |z) is intractable, the upper bound of I(z; z + ) cannot be directly minimized. Therefore, we adopt a variational distribution q(z + |z), that employs a neural network parameterized by \u03b8 q to approximate the upper bound\u00ce(z; z + ) of mutual information:\nI(z; z + ) = 1 N N i=1 [log q \u03b8 (z + i |z i ) \u2212 1 N N j=1 log q \u03b8 (z + j |z i )]\n(5) By minimizing Eq. (5), the learnable mean/variance shift parameters in our model are trained to complement the style of the source domain. Although\u00ce(z; z + ) is no longer an upper bound for MI as the conditional distribution p(z + |z) is substituted with a variational approximation q \u03b8 (z + |z), I(z; z + ) can be a reliable upper bound estimator if the difference between the two distributions is small. Specifically, we estimate the the difference \u2206 between\u00ce(z; z + ) and the upper bound of I(z; z + ) by using the Kullback-Leibler divergence (KLD):\n\u2206 = KLD(p(z + , z) q \u03b8 (z + , z)) = E p(z,z + ) [log(p(z + |z)p(z)) \u2212 log(q \u03b8 (z + |z)p(z))] = E p(z,z + ) [log p(z + |z)] \u2212 E p(z,z + ) [log q \u03b8 (z + |z)].(6)\nThe above equation shows that the difference \u2206 is affected by two terms. Since the first term of Eq. 6 is not related to \u03b8 q , we minimize the negative log-likelihod between z i and z + i instead of directly minimizing \u2206:\nL likeli = \u2212 1 N N i=1 log q \u03b8 (z + i |z i ).(7)\nSemantic Consistency. While the style-complement module can generate images with diverse styles, it may introduce noise or generate images with distorted semantic information from original source images (e.g., when the variance shift \u03c3 k equals to 0, the generated images will become meaningless). Therefore, it is important to limit the conditional distribution shift from the source distribution to the out-of-domain distribution, thereby avoiding generating semantically unrelated images. To achieve this, we minimize the class-conditional maximum mean discrepancy (MMD) in the latent space as follows,\nL const = 1 C C m=1 ( 1 n m s n m s i=1 \u03c6(z m i ) \u2212 1 n m t n m t i=1 \u03c6(z m+ j ) 2 ),(8)\nwhere z m i and z m+ i are the i-th latent vector of the source and augmented sample of the class m, respectively. n m s and n m t are the total number of original and augmented sample of class m. \u03c6(\u00b7) represents the kernel function. Conditional MMD mitigates the potential semantic information distortion by constraining the distribution shift of samples that belong to the same class. MI Maximization. We aim to obtain a generalizable and robust model by playing a min-max game between the stylecomplement module G(\u00b7; \u03b8 G ) and the task model F (\u00b7; \u03b8 F ). While the style-complement module aims to generate diverse images that have minimum information on the source images, the task model can cluster images with same semantic label in the embedding space. The lower bound on MI between two varaibles proposed in [40] is:\nI(z; z + ) \u2265 E 1 N N i=1 log e f (z,z + ) N j=1 e f (zi,z + j ) ,(9)\nwhere f (\u00b7, \u00b7) is a critic function.\n\nHowever, directly maximizing the lower bound MI of the generated and source images without leveraging the semantic label may falsely reduce the shared information of same class samples. To alleviate this problem, we employ the supervised contrastive loss [19] to increase the mutual information among samples from the same class, defined as:\nL supcon = \u2212 N i=0 1 |P (i)| p\u2208P (i) log e (zi\u00b7zp/\u03c4 ) a\u2208A(i) e (zi\u00b7za/\u03c4 ) P (i) = {p \u2208 A(i) : y p = y i },(10)\nwhere A(i) is the set of the source and generated latent representations z, z + of the same class. \u03c4 is the temperature coefficient.\n\nTo further enhance the semantic consistency, we minimize the cross-entropy loss on both the source images X and the generated images X + :\nL task = \u2212 1 2N [ N i=0 y i log(\u0177 i ) + N j=0 y + j log(\u0177 + j )],(11)\nwhere\u0177 and\u0177 + are the prediction of the source and generated images, respectively. Objective Function. We adopt a two-step training, in which we optimize the style-complement module G(\u00b7; \u03b8 G ) and a task model, including F (\u00b7; \u03b8 F ), q(\u00b7; \u03b8 q ) and H(\u00b7; \u03b8 H ) in an iterative manner. Specifically, we train the task module F with both the source images X and the generated images X + with the weighted combination of Eq. (5), (7), and (11) as follows:\nmin \u03b8 F ,\u03b8q,\u03b8 H L = L task + \u03b1 1 L supcon + \u03b1 2 L likeli .(12)\nNotably, \u03b1 1 and \u03b1 2 are the hyper-parameters for balancing the losses. To optimize G, we consider solving Eq. (5) and (8) jointly:\nmin \u03b8 G L =\u00ce(z; z + ) + \u03b2L const ,(13)\nwith \u03b2 being the balancing weight between the mutual information upper bound estimation\u00ce(z; z + ) and semantic consistent loss L const . Implementation Notes: To capture multi-scale information from images, we apply different transformations (i.e., kernel sizes) in the convolution and transposed convolution layers. Moreover, to avoid potential distortion of the semantic information, we fix the number of output channels to the number of input color channels (i.e., 3 output channels for RGB images). We re-initialize the weights of the convolutional layer and transposed convolutional layer by sampling from uniform distribution of (\u2212\n1 \u221a size(kernel) , 1 \u221a size(kernel)\n) in each iteration. \n\n\nExperiments\n\nDatasets.\n\nTo evaluate the effectiveness of the proposed method, we conduct experiments over three singledg benchmark datasets. Digits consists of 5 different datasets, which are MNIST [22], SVHN [35], MNIST-M [12], SYN [12] and USPS [9]. Each dataset is considered as a unique domain which may be different from the rest of the domains in font style, background, and stroke color. PACS [23] is a recent proposed DG benchmark dataset that has four domains, including photo, art painting, cartoon, and sketch. Each domain contains 224 \u00d7 224 images belonging to seven categories, and there are 9,991 images in total. Compared with the digits dataset, PACS is considered as a more challenging dataset due to the large style shift among domains. For fair comparison, we follow the official split of the train, validation, and test. Corrupted CIFAR-10 [16,20] contains 32\u00d732 tiny RGB images from CIFAR-10 that are corrupted by different types of noises. There are 15 corruptions from 4 main categories, including weather, blur, noise, and digital. Each of the corruptions has 5 levels serverities and '5' indicates the severest corruption.\n\n\nComparisons on Digits\n\nExperiment Setup. Following [42,37,50], we select 10,000 images from MNIST as the source domain and test the generalization performance of models on the other four digits datasets. We resize all the images to 32 \u00d7 32 and duplicate their channels to convert all the grayscale images to RGB. We employ the LeNet [22] as the backbone and set the batch size as 32. We use SGD to optimize both the style-complement module and the task model. Results. We compare the proposed method with three recent state-of-the-arts single domain generalization methods [42,37,50], two multi-domain generalization methods [33,47]. Table 1 shows that our model achieves the highest average accuracy compared to the other baselines. Specif- Compared to the adversarial gradient-based methods, L2D creates larger domain shifts between the generated images and the source images by incorporating the style-complement module. The substantial increase of the accuracy over the baselines demonstrates the importance of generating diverse-styled images on improving the generalization power of a model. Moreover, we observe that single-DG methods generally achieve better performance in this task, reflecting the dependency of previous DG methods on multiple source domains to learn a generalizable model. Our method achieves the second best performance on USPS. We infer this might be related to the fact the USPS and the source domain MNIST share very similar stroke styles. In such case, the diverse generated images might not benefit the generalizibility of the model as much as the task with a larger domain shift.\n\n\nComparisons on Corrupted CIFAR-10\n\nExperiment Setup. We train all models on the training split of CIFAR-10 dataset (50,000 images) and evaluate them on the corrupted test split of CIFAR-10 (10,000 images). Following [37], we select WideResNet (16-4) [49] as the backbone network with a batch size of 256. We optimize the model by using SGD with Nesterov momentum and weight decay rate of 0.0005. The learning rate is initialized to 0.2 which is gradually decreased by using the cosine annealing scheduler.\n\nResults. We report the average accuracy of four categories under level-5 severity corruption in Table 2. Our method achieves the highest average performance, surpassing the best baseline by approximately 2.6%. Notably, for noise corruptions, accuracy is significantly improved by approxi- mately 13.2%. We also report the performance of methods against all five levels of Noise corruption and Digits corruption in Figure 3. From the figure, we can see that the performance margin between our method and others are relatively small under severity level one, and it is gradually enlarged while the severity level goes up. This further validates that our model not only can achieve the highest average performance, but also is resilient towards the severe corruptions.\n\n\nComparisons on PACS\n\nExperiment Setup. For the single domain generalization task, we consider a practical case, where we utilize a set of easy-to-collect realistic images (i.e., photo) as the source domain, and evaluate models on the rest of diverse-styled domains (i.e., art painting, cartoon, and sketch). AlexNet [21] is employed as the backbone which is pretrained on Imagenet and finetuned on the source domain. We also evaluate the effectiveness of our approach on PACS under the standard leave-one-domain-out protocol, where one domain is selected as the test domain and the rest are treated as the source domain. We employ pretrained Alexnet and ResNet-18 [15] as the backbone networks for the leave-one-domainout setting. Please refer to the supplementary material for more implementation details.\n\nResults. From Table 3, we can see that our method can achieve the best average classification accuracy compared to the baselines. Importantly, our method can achieve a relatively large performance margin over other methods on the sketch domain, which has the largest domain shift from photo due to its highly abstracted shapes. This result verifies that our method takes the advantage of diverse style images that are generated by the style-complement module.\n\nTo further validate the performance of our method, we conduct the leave-one-domain-out domain generalization  task on PACS. We compare our approach with the two categories of recent state-of-the-art DG methods. The first category,including DSN [4], Fusion [30], MetaReg [2], Epi-FCR [25], MASF [10] and DMG [6], requires domain identifications in the training phase. Methods in the second category are inline with a more realistic mixed latent domain setting [31], where domain identifiers are unavailable in the training phase. AGG [25], HEX [44], PAR [43], JiGen [5], ADA [42], MEADA [50], MMLD [31] and our method belong to the latter category. We report the results with different backbone networks in Table 4. Without leveraging the domain identifier, our method can still achieve the state-ofthe-art performance on the leave-one-domain-out generalization task on PACS. In the training stage, the image augmentation module gradually enlarges the domain shift between the generated images and the source images.\n\n\nEmpirical Analysis\n\nAblation Study. We conduct the ablation study on PACS dataset to verify the effectiveness of each component in our framework. Table 3 reports the classification results of the 4 variants of our original framework. We report the baseline result without incorporating the style-complement module as w/o Style-comp.. Without the generated images,  the generated images.\n\nTo provide an understanding of how mutual information affects the learning framework, in w/o Min. MI and w/o Max. MI, we remove the mutual information minimization and maximization process, respectively. We observe the accuracy of w/o Min. MI is 1.26% lower than the full model which is similar to the results of w/o Mod.. That means, without the mutual information constraint term, the stylecomplement module tends to generate images by following the source distribution, which limits their diversity. Meanwhile, without the mutual information maximization process, w/o Max. MI a clear performance margin of 3.76% is shown compared to the full model. This indicates that mapping diverse styled images from the same class closer in the embedding space is helpful for improving the generalization power of the model. Parameter Sensitivity. To validate the significance of the total number of transformations k, and weighting parameters \u03b1 1 , and \u03b1 2 in the loss, we conduct sensitivity analysis on PACS dataset. In the experiments, we initially set k = 6, \u03b1 1 = 1, and \u03b1 2 = 1. When we analyze the sensitivity to a specific parameter, we fix the values of the other two parameters. Figure 2 shows the results of sensitivity analysis under the Single-DG setting. From Figure 2(a), we can see that the performance is gradually increasing as the style-complement module combines more transformations. Also, the average performance become relatively stable after k = 5. The results also indicates that jointly considering multiple transformations increases the diversity of the generated images. Meanwhile, excessive transformations might produce extra noise, which hinders the further performance gain. As can be seen from Figure 2 (b) and (c), our method surpasses the state-of-the-art performance in a wide range of \u03b1 settings, and achieves the best classification accuracy with \u03b1 1 = \u03b1 2 = 1. t-SNE Visualizations. To further demonstrate the effectiveness of the proposed method, we use t-SNE [41] to visual-ize the distribution of the unseen target features in the digits dataset (i.e., SVHN, SYN, USPS and MNIST-M). Specifically, we train different models on MNIST and randomly select 1000 samples from each unseen target domains to visualize. As shown in Fig. 4, our method clearly achieve better class-wise separation than the baselines. Moreover, from the distribution of features extracted by ERM, ADA and ME-ADA, we observe that the features within the same class can have multiple sub-clusters. This indicates that it is hard for the methods to learn domain invariant representations and the large intra-class variations may hinder them obtaining a clear decision boundary on the targets. In contrast, it is clearly seen that our approach creates tighter clusters with a good class-wise mix compared to the baselines. This strongly supports the idea of diverse image generation and the maximization of mutual information among the samples from the same class.\n\n\nConclusion\n\nThis paper presented Learning-to-Diversify (L2D), a novel approach to address single domain generalization. Unlike previous domain generalization methods which exploit multiple source domains to learn domain invariant representations, the proposed approach designs a stylecomplement module to generate diverse out-of-domain images from single source domain. An iterative min-max mutual information (MI) optimization strategy is used to boost the generalization power of the model. The tractable MI upper bound is minimized to further enhance the diversity of the generated image, while MI among same category samples is maximized to obtain the style-invariant representations in the maximization step. Extensive experiments on three benchmark datasets demonstrate that the proposed method outperforms the state-of-the-art methods on both single domain generalization and the standard leave-onedomain-out domain generalization.\n\nFigure 2 :\n2Sensitivity analysis of L2D with respect to parameters k, \u03b1 1 , and \u03b1 2 on the PACS dataset. The reported accuracy is the averaged over all three unseen target domains.\n\nFigure 3 :\n3Average classification accuracy (%) of different methods under five levels of severity on Noise and Digital corruptions.\n\nFigure 4 :\n4The t-SNE visualizations of extracted unseen target feature distribution for different methods on digits Single-DG task. Features with the same semantic label are plotted in the same color.\n\n\n51] exploits a conditional gener-StyleMod \n\nSource \nImages \n\nStyleMod \nStyleMod \n\nGenerated \nImages \n\nBackbone \n\n(\ufffd ; \n\n) \n\nInstance \nNorm. \n\nStyle \nVar. \n\nStyle \nMean \n\n\u2460 Minimize MI \n\n\u2461 Maximize MI \n\nStyle-Complement Module \n(\ufffd ; ) \n\nClassifier \n\n(\ufffd ; \n\n) \n\nStyleMod \n\u2026 \n\n(\ufffd ; ) \n\nSource Data \n\nGenerated Data \n\n\u2026 \n\n\n\nTable 1 :\n1Single domain generalization accuracy (%) comparison on digits dataset. Models are trained on MNIST and evaluated on the rest of the digits datasets. Best performances are highlighted in bold.SVHN MNIST-M SYN USPS Avg. \nERM \n27.83 \n52.72 \n39.65 76.94 49.29 \nCCSA \n25.89 \n49.29 \n37.31 83.72 49.05 \nd-SNE \n26.22 \n50.98 \n37.83 93.16 52.05 \nJiGen \n33.80 \n57.80 \n43.79 77.15 53.14 \nADA \n35.51 \n60.41 \n45.32 77.26 54.62 \nM-ADA \n42.55 \n67.94 \n48.95 78.53 59.49 \nME-ADA 42.56 \n63.27 \n50.39 81.04 59.32 \nOurs \n62.86 \n87.30 \n63.72 83.97 74.46 \n\n\n\nTable 2 :\n2Single domain generalization accuracy (%). Models are trained on CIFAR-10 dataset and evaluated on CIFAR-10-C dataset with corruption severity level 5. We report the average accuracy over 4 main categories of corruption: Weather, Blur, Noise, and Digital. Best performances are highlighted in bold. * indicates our implementation.Weather Blur Noise Digits Avg. \nERM \n67.28 \n56.73 30.02 62.30 54.08 \nCCSA \n67.66 \n57.81 28.73 61.96 54.04 \nd-SNE \n67.90 \n56.59 33.97 61.83 55.07 \nADA  *  \n72.67 \n67.04 39.97 66.62 61.58 \nM-ADA \n75.54 \n63.76 54.21 65.10 64.65 \nME-ADA  *  \n74.44 \n71.37 66.47 70.83 70.77 \nOurs \n75.98 \n69.16 73.29 72.02 72.61 \n\nically, we observe clear improvements of 20.3%, 29.36%, \n13.33% and 14.9% on SVHN, MNIST-M, SYN, and over-\nall accuracy, respectively. Previous Single-DG methods di-\nrectly generate the auxiliary training samples by applying \nthe adversarial perturbation. \n\nTable 3 :\n3Single domain generalization accuracy (%) on \nPACS. Models are trained on photo and evaluated on the \nrest of the target domains (i.e., art painting, cartoon, and \nsketch). The comparison is based on our implementation. \nBest performances are highlighted in bold. \n\nA \nC \nS \nAvg. \nERM \n54.43 42.74 42.02 46.39 \nJiGen \n54.98 42.62 40.62 46.07 \nRSC \n56.26 39.59 47.13 47.66 \nADA \n58.72 45.58 48.26 50.85 \nME-ADA \n58.96 44.09 49.96 51.00 \nOurs w/o Style-comp. 53.27 41.00 41.92 45.39 \nOurs w/o Mod. \n58.48 48.96 53.20 53.54 \nOurs w/o Min. MI \n56.49 48.08 56.32 53.63 \nOurs w/o Max. MI \n56.64 47.08 49.68 51.13 \nOurs (Full Model) \n56.26 51.04 58.42 55.24 \n\n\n\nTable 4 :\n4Leave-one-domain-out classification accuracy(%) on PACS. Best performances are highlighted in bold.the model is degraded to the backbone model with a variational approximation of z in the embedding space. The large performance margin between w/o Style-comp. and our full model demonstrates the importance of the stylecomplement module on improving the generalization ability of the model. w/o Mod. shows the result after removing the style modification from the full model. The removal of the style modification triggers 1.35% absolute performance decline. We infer this relates to the limited style diversity ofD ID \nP \nA \nC \nS \nAvg. \n\nAlexNet \n\nDSN \n\n83.30 61.10 66.50 58.60 67.40 \nFusion \n\n90.20 64.10 66.80 60.10 70.30 \nMetaReg \n\n87.40 63.50 69.50 59.10 69.90 \nEpi-FCR \n\n86.10 64.70 72.30 65.00 72.00 \nMASF \n\n90.68 70.35 72.46 67.33 75.21 \nDMG \n\n87.31 64.65 69.88 71.42 73.32 \nHEX \n\n87.90 66.80 69.70 56.20 70.20 \nPAR \n\n89.60 66.30 66.30 64.10 72.08 \nJiGen \n\n89.00 67.63 71.71 65.18 73.38 \nADA \n\n85.10 64.30 69.80 60.40 69.90 \nMEADA \n\n88.60 67.10 69.90 63.00 72.20 \nMMLD \n\n88.98 69.27 72.83 66.44 74.38 \nOurs \n\n90.96 71.19 72.18 67.68 75.50 \n\nResNet-18 \n\nEpi-FCR \n\n93.90 82.10 77.00 73.00 81.50 \nMASF \n\n94.99 80.29 77.17 71.68 81.03 \nDMG \n\n93.55 76.90 80.38 75.21 81.46 \nJigen \n\n96.03 79.42 75.25 71.35 80.51 \nADA \n\n95.61 78.32 77.65 74.21 81.44 \nMEADA \n\n95.57 78.61 78.65 75.59 82.10 \nMMLD \n\n96.09 81.28 77.16 72.29 81.83 \nOurs \n\n95.51 81.44 79.56 80.58 84.27 \n\n\nAcknowledgement This work was supported by Australian Research Council (ARC DP190102353, and  CE200100025)\nUnsupervised domain adaptation by domain invariant projection. Mahsa Baktashmotlagh, Tafazzoli Mehrtash, Brian C Harandi, Mathieu Lovell, Salzmann, ICCV. Mahsa Baktashmotlagh, Mehrtash Tafazzoli Harandi, Brian C. Lovell, and Mathieu Salzmann. Unsupervised domain adaptation by domain invariant projection. In ICCV, 2013.\n\nTowards domain generalization using metaregularization. Yogesh Balaji, Swami Sankaranarayanan, Rama Chellappa, Metareg, In NeurIPS. Yogesh Balaji, Swami Sankaranarayanan, and Rama Chel- lappa. Metareg: Towards domain generalization using meta- regularization. In NeurIPS, 2018.\n\nA theory of learning from different domains. Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, Jennifer Wortman Vaughan, Mach. Learn. Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A theory of learning from different domains. Mach. Learn., 2010.\n\nDomain separation networks. Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan, Dumitru Erhan, NeurIPS. Konstantinos Bousmalis, George Trigeorgis, Nathan Silber- man, Dilip Krishnan, and Dumitru Erhan. Domain separa- tion networks. In NeurIPS, 2016.\n\nDomain generalization by solving jigsaw puzzles. Fabio Maria Carlucci, Antonio D&apos; Innocente, Silvia Bucci, Barbara Caputo, Tatiana Tommasi, CVPR. Fabio Maria Carlucci, Antonio D'Innocente, Silvia Bucci, Barbara Caputo, and Tatiana Tommasi. Domain generaliza- tion by solving jigsaw puzzles. In CVPR, 2019.\n\nLearning to balance specificity and invariance for in and out of domain generalization. Prithvijit Chattopadhyay, Yogesh Balaji, Judy Hoffman, ECCV. Prithvijit Chattopadhyay, Yogesh Balaji, and Judy Hoffman. Learning to balance specificity and invariance for in and out of domain generalization. In ECCV, 2020.\n\nCLUB: A contrastive log-ratio upper bound of mutual information. Pengyu Cheng, Weituo Hao, Shuyang Dai, Jiachang Liu, Zhe Gan, Lawrence Carin, ICML. Pengyu Cheng, Weituo Hao, Shuyang Dai, Jiachang Liu, Zhe Gan, and Lawrence Carin. CLUB: A contrastive log-ratio upper bound of mutual information. In ICML, 2020.\n\nA comprehensive survey on domain adaptation for visual applications. Gabriela Csurka, Domain Adaptation in Computer Vision Applications. Gabriela Csurka. A comprehensive survey on domain adap- tation for visual applications. In Domain Adaptation in Com- puter Vision Applications. 2017.\n\nNeural network recognizer for hand-written zip code digits. John S Denker, W R Gardner, Hans Peter Graf, Donnie Henderson, Richard E Howard, Wayne E Hubbard, Lawrence D Jackel, Henry S Baird, Isabelle Guyon, NeurIPS. John S. Denker, W. R. Gardner, Hans Peter Graf, Don- nie Henderson, Richard E. Howard, Wayne E. Hubbard, Lawrence D. Jackel, Henry S. Baird, and Isabelle Guyon. Neural network recognizer for hand-written zip code digits. In NeurIPS, 1988.\n\nDomain generalization via model-agnostic learning of semantic features. Qi Dou, Daniel Coelho De Castro, Konstantinos Kamnitsas, Ben Glocker, NeurIPS. Qi Dou, Daniel Coelho de Castro, Konstantinos Kamnitsas, and Ben Glocker. Domain generalization via model-agnostic learning of semantic features. In NeurIPS, 2019.\n\nLearning to learn with variational information bottleneck for domain generalization. Ying-Jun Du, Jun Xu, Huan Xiong, Qiang Qiu, Xiantong Zhen, G M Cees, Ling Snoek, Shao, ECCV. Ying-Jun Du, Jun Xu, Huan Xiong, Qiang Qiu, Xiantong Zhen, Cees G. M. Snoek, and Ling Shao. Learning to learn with variational information bottleneck for domain general- ization. In ECCV, 2020.\n\nUnsupervised domain adaptation by backpropagation. Yaroslav Ganin, Victor S Lempitsky, ICML. Yaroslav Ganin and Victor S. Lempitsky. Unsupervised do- main adaptation by backpropagation. In ICML, 2015.\n\nDomain-adversarial training of neural networks. Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Fran\u00e7ois Laviolette, Mario Marchand, Victor S Lempitsky, J. Mach. Learn. Res. Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pas- cal Germain, Hugo Larochelle, Fran\u00e7ois Laviolette, Mario Marchand, and Victor S. Lempitsky. Domain-adversarial training of neural networks. J. Mach. Learn. Res., 2016.\n\nDomain generalization for object recognition with multi-task autoencoders. Muhammad Ghifary, W Bastiaan Kleijn, Mengjie Zhang, David Balduzzi, ICCV. Muhammad Ghifary, W. Bastiaan Kleijn, Mengjie Zhang, and David Balduzzi. Domain generalization for object recog- nition with multi-task autoencoders. In ICCV, 2015.\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, CVPR. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.\n\nBenchmarking neural network robustness to common corruptions and perturbations. ICLR. Dan Hendrycks, Thomas Dietterich, Dan Hendrycks and Thomas Dietterich. Benchmarking neu- ral network robustness to common corruptions and perturba- tions. ICLR, 2019.\n\nSelf-challenging improves cross-domain generalization. Zeyi Huang, Haohan Wang, Eric P Xing, Dong Huang, ECCV. Zeyi Huang, Haohan Wang, Eric P. Xing, and Dong Huang. Self-challenging improves cross-domain generalization. In ECCV, 2020.\n\nCoregularization based semi-supervised domain adaptation. Hal Daum\u00e9, Iii , Abhishek Kumar, Avishek Saha, NeurIPS. Hal Daum\u00e9 III, Abhishek Kumar, and Avishek Saha. Co- regularization based semi-supervised domain adaptation. In NeurIPS, 2010.\n\nPrannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, Dilip Krishnan, NeurIPS. Supervised contrastive learningPrannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. In NeurIPS, 2020.\n\nLearning multiple layers of features from tiny images. Alex Krizhevsky, Geoffrey Hinton, Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\n\nImagenet classification with deep convolutional neural networks. Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, NeurIPS. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural net- works. In NeurIPS, 2012.\n\nGradient-based learning applied to document recognition. Yann Lecun, L\u00e9on Bottou, Yoshua Bengio, Patrick Haffner, Proceedings of the IEEE. the IEEEYann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recog- nition. Proceedings of the IEEE, 1998.\n\nDeeper, broader and artier domain generalization. Da Li, Yongxin Yang, Yi-Zhe Song, Timothy M Hospedales, ICCV. Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M. Hospedales. Deeper, broader and artier domain generaliza- tion. In ICCV, 2017.\n\nLearning to generalize: Meta-learning for domain generalization. Da Li, Yongxin Yang, Yi-Zhe Song, Timothy M Hospedales, AAAI. Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M. Hospedales. Learning to generalize: Meta-learning for do- main generalization. In AAAI, 2018.\n\nEpisodic training for domain generalization. Da Li, Jianshu Zhang, Yongxin Yang, Cong Liu, Yi-Zhe Song, Timothy M Hospedales, ICCV. Da Li, Jianshu Zhang, Yongxin Yang, Cong Liu, Yi-Zhe Song, and Timothy M. Hospedales. Episodic training for domain generalization. In ICCV, 2019.\n\nLearning transferable features with deep adaptation networks. Mingsheng Long, Yue Cao, Jianmin Wang, Michael I Jordan, ICML. Mingsheng Long, Yue Cao, Jianmin Wang, and Michael I. Jordan. Learning transferable features with deep adaptation networks. In ICML, 2015.\n\nConditional adversarial domain adaptation. Mingsheng Long, Zhangjie Cao, Jianmin Wang, Michael I Jordan, In NeurIPS. Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I. Jordan. Conditional adversarial domain adapta- tion. In NeurIPS, 2018.\n\nDeep transfer learning with joint adaptation networks. Mingsheng Long, Han Zhu, Jianmin Wang, Michael I Jordan, ICML. Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I. Jordan. Deep transfer learning with joint adaptation net- works. In ICML, 2017.\n\nZi Huang, and Mahsa Baktashmotlagh. Progressive graph learning for open-set domain adaptation. Yadan Luo, Zijian Wang, ICML. Yadan Luo, Zijian Wang, Zi Huang, and Mahsa Baktashmot- lagh. Progressive graph learning for open-set domain adap- tation. In ICML, 2020.\n\nBest sources forward: Domain generalization through source-specific nets. Massimiliano Mancini, Samuel Rota Bul\u00f2, Barbara Caputo, Elisa Ricci, In ICIP. Massimiliano Mancini, Samuel Rota Bul\u00f2, Barbara Caputo, and Elisa Ricci. Best sources forward: Domain generaliza- tion through source-specific nets. In ICIP, 2018.\n\nDomain generalization using a mixture of multiple latent domains. Toshihiko Matsuura, Tatsuya Harada, AAAI. Toshihiko Matsuura and Tatsuya Harada. Domain general- ization using a mixture of multiple latent domains. In AAAI, 2020.\n\nSeyed Mehdi Iranmanesh, and Gianfranco Doretto. Few-shot adversarial domain adaptation. Saeid Motiian, Quinn Jones, NeurIPS. Saeid Motiian, Quinn Jones, Seyed Mehdi Iranmanesh, and Gianfranco Doretto. Few-shot adversarial domain adapta- tion. In NeurIPS, 2017.\n\nUnified deep supervised domain adaptation and generalization. Saeid Motiian, Marco Piccirilli, Donald A Adjeroh, Gianfranco Doretto, ICCV. Saeid Motiian, Marco Piccirilli, Donald A. Adjeroh, and Gi- anfranco Doretto. Unified deep supervised domain adapta- tion and generalization. In ICCV, 2017.\n\nDomain generalization via invariant feature representation. Krikamol Muandet, David Balduzzi, Bernhard Sch\u00f6lkopf, ICML. Krikamol Muandet, David Balduzzi, and Bernhard Sch\u00f6lkopf. Domain generalization via invariant feature representation. In ICML, 2013.\n\nReading digits in natural images with unsupervised feature learning. Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, Andrew Y Ng, Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bis- sacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. 2011.\n\nA survey on transfer learning. Qiang Sinno Jialin Pan, Yang, IEEE Trans. Knowl. Data Eng. Sinno Jialin Pan and Qiang Yang. A survey on transfer learn- ing. IEEE Trans. Knowl. Data Eng., 2010.\n\nLearning to learn single domain generalization. Fengchun Qiao, Long Zhao, Xi Peng, CVPR. Fengchun Qiao, Long Zhao, and Xi Peng. Learning to learn single domain generalization. In CVPR, 2020.\n\nSemi-supervised domain adaptation via minimax entropy. Kuniaki Saito, Donghyun Kim, Stan Sclaroff, Trevor Darrell, Kate Saenko, ICCV. Kuniaki Saito, Donghyun Kim, Stan Sclaroff, Trevor Darrell, and Kate Saenko. Semi-supervised domain adaptation via minimax entropy. In ICCV, 2019.\n\nGeneralizing across domains via cross-gradient training. Shiv Shankar, Vihari Piratla, Soumen Chakrabarti, Siddhartha Chaudhuri, Preethi Jyothi, Sunita Sarawagi, In ICLR. Shiv Shankar, Vihari Piratla, Soumen Chakrabarti, Sid- dhartha Chaudhuri, Preethi Jyothi, and Sunita Sarawagi. Generalizing across domains via cross-gradient training. In ICLR, 2018.\n\nRepresentation learning with contrastive predictive coding. A\u00e4ron Van Den Oord, Yazhe Li, Oriol Vinyals, abs/1807.03748CoRRA\u00e4ron van den Oord, Yazhe Li, and Oriol Vinyals. Repre- sentation learning with contrastive predictive coding. CoRR, abs/1807.03748, 2018.\n\nVisualizing data using t-sne. Laurens Van Der Maaten, Geoffrey Hinton, J. Mach. Learn. Res. Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. J. Mach. Learn. Res., 2008.\n\nGeneralizing to unseen domains via adversarial data augmentation. Riccardo Volpi, Hongseok Namkoong, Ozan Sener, John C Duchi, Vittorio Murino, Silvio Savarese, In NeurIPS. Riccardo Volpi, Hongseok Namkoong, Ozan Sener, John C. Duchi, Vittorio Murino, and Silvio Savarese. Generalizing to unseen domains via adversarial data augmentation. In NeurIPS, 2018.\n\nLearning robust global representations by penalizing local predictive power. Haohan Wang, Songwei Ge, Zachary C Lipton, Eric P Xing, NeurIPS. Haohan Wang, Songwei Ge, Zachary C. Lipton, and Eric P. Xing. Learning robust global representations by penalizing local predictive power. In NeurIPS, 2019.\n\nLearning robust representations by projecting superficial statistics out. Haohan Wang, Zexue He, Zachary C Lipton, Eric P Xing, ICLR. Haohan Wang, Zexue He, Zachary C. Lipton, and Eric P. Xing. Learning robust representations by projecting superfi- cial statistics out. In ICLR, 2019.\n\nDeep visual domain adaptation: A survey. Mei Wang, Weihong Deng, Neurocomputing. Mei Wang and Weihong Deng. Deep visual domain adapta- tion: A survey. Neurocomputing, 2018.\n\nZi Huang, and Mahsa Baktashmotlagh. Prototype-matching graph network for heterogeneous domain adaptation. Zijian Wang, Yadan Luo, MM. Zijian Wang, Yadan Luo, Zi Huang, and Mahsa Baktashmot- lagh. Prototype-matching graph network for heterogeneous domain adaptation. In MM, 2020.\n\nRagav Venkatesan, Gurumurthy Swaminathan, and Orchid Majumder. d-sne: Domain adaptation using stochastic neighborhood embedding. Xiang Xu, Xiong Zhou, CVPR. Xiang Xu, Xiong Zhou, Ragav Venkatesan, Gurumurthy Swaminathan, and Orchid Majumder. d-sne: Domain adap- tation using stochastic neighborhood embedding. In CVPR, 2019.\n\nRobust and generalizable visual representation learning via random convolutions. Zhenlin Xu, Deyi Liu, Junlin Yang, Colin Raffel, Marc Niethammer, ICLR. 2021Zhenlin Xu, Deyi Liu, Junlin Yang, Colin Raffel, and Marc Niethammer. Robust and generalizable visual representation learning via random convolutions. In ICLR, 2021.\n\nWide residual networks. Sergey Zagoruyko, Nikos Komodakis, BMVC. Sergey Zagoruyko and Nikos Komodakis. Wide residual net- works. In BMVC, 2016.\n\nMaximum-entropy adversarial data augmentation for improved generalization and robustness. Long Zhao, Ting Liu, Xi Peng, Dimitris N Metaxas, NeurIPS. Long Zhao, Ting Liu, Xi Peng, and Dimitris N. Metaxas. Maximum-entropy adversarial data augmentation for im- proved generalization and robustness. In NeurIPS, 2020.\n\nLearning to generate novel domains for domain generalization. Kaiyang Zhou, Yongxin Yang, Timothy M Hospedales, Tao Xiang, ECCV. Kaiyang Zhou, Yongxin Yang, Timothy M. Hospedales, and Tao Xiang. Learning to generate novel domains for domain generalization. In ECCV, 2020.\n", "annotations": {"author": "[{\"end\":97,\"start\":58},{\"end\":135,\"start\":98},{\"end\":175,\"start\":136},{\"end\":233,\"start\":176},{\"end\":309,\"start\":234}]", "publisher": null, "author_last_name": "[{\"end\":69,\"start\":65},{\"end\":107,\"start\":104},{\"end\":147,\"start\":144},{\"end\":184,\"start\":179},{\"end\":254,\"start\":240}]", "author_first_name": "[{\"end\":64,\"start\":58},{\"end\":103,\"start\":98},{\"end\":143,\"start\":136},{\"end\":178,\"start\":176},{\"end\":239,\"start\":234}]", "author_affiliation": "[{\"end\":96,\"start\":71},{\"end\":134,\"start\":109},{\"end\":174,\"start\":149},{\"end\":232,\"start\":207},{\"end\":308,\"start\":283}]", "title": "[{\"end\":55,\"start\":1},{\"end\":364,\"start\":310}]", "venue": null, "abstract": "[{\"end\":1862,\"start\":366}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2259,\"start\":2255},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2261,\"start\":2259},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":2660,\"start\":2656},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2662,\"start\":2660},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2665,\"start\":2662},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2668,\"start\":2665},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2801,\"start\":2797},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":2804,\"start\":2801},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2807,\"start\":2804},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":2810,\"start\":2807},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3173,\"start\":3170},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3176,\"start\":3173},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3179,\"start\":3176},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3206,\"start\":3203},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3209,\"start\":3206},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":3212,\"start\":3209},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":3215,\"start\":3212},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":3218,\"start\":3215},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":4291,\"start\":4287},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":4294,\"start\":4291},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":7530,\"start\":7526},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7532,\"start\":7530},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7795,\"start\":7791},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7798,\"start\":7795},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7800,\"start\":7798},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7834,\"start\":7830},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7837,\"start\":7834},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7963,\"start\":7959},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":7966,\"start\":7963},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":7969,\"start\":7966},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8001,\"start\":7997},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8004,\"start\":8001},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8127,\"start\":8123},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8719,\"start\":8715},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8798,\"start\":8794},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8901,\"start\":8897},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":9178,\"start\":9174},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9419,\"start\":9415},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9422,\"start\":9419},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9425,\"start\":9422},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9527,\"start\":9523},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9613,\"start\":9609},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":9982,\"start\":9978},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":10090,\"start\":10086},{\"end\":10173,\"start\":10172},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10729,\"start\":10726},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":11028,\"start\":11024},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":11031,\"start\":11028},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":11225,\"start\":11221},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":11385,\"start\":11381},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":15195,\"start\":15192},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":18255,\"start\":18251},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":18626,\"start\":18622},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":19592,\"start\":19589},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":20749,\"start\":20745},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":20760,\"start\":20756},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":20774,\"start\":20770},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":20784,\"start\":20780},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":20797,\"start\":20794},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":20951,\"start\":20947},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":21411,\"start\":21407},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":21414,\"start\":21411},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":21752,\"start\":21748},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":21755,\"start\":21752},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":21758,\"start\":21755},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":22034,\"start\":22030},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":22274,\"start\":22270},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":22277,\"start\":22274},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":22280,\"start\":22277},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":22326,\"start\":22322},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":22329,\"start\":22326},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":23534,\"start\":23530},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":23568,\"start\":23564},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":24909,\"start\":24905},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":25257,\"start\":25253},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":26105,\"start\":26102},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":26118,\"start\":26114},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":26131,\"start\":26128},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":26145,\"start\":26141},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":26156,\"start\":26152},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":26168,\"start\":26165},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":26321,\"start\":26317},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":26395,\"start\":26391},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":26405,\"start\":26401},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":26415,\"start\":26411},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":26426,\"start\":26423},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":26436,\"start\":26432},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":26448,\"start\":26444},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":26459,\"start\":26455},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":29260,\"start\":29256}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":31353,\"start\":31172},{\"attributes\":{\"id\":\"fig_1\"},\"end\":31487,\"start\":31354},{\"attributes\":{\"id\":\"fig_2\"},\"end\":31690,\"start\":31488},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":32011,\"start\":31691},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":32559,\"start\":32012},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":33467,\"start\":32560},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":34133,\"start\":33468},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":35613,\"start\":34134}]", "paragraph": "[{\"end\":2422,\"start\":1878},{\"end\":3545,\"start\":2424},{\"end\":4653,\"start\":3547},{\"end\":6370,\"start\":4655},{\"end\":6433,\"start\":6372},{\"end\":6561,\"start\":6435},{\"end\":6817,\"start\":6563},{\"end\":7237,\"start\":6819},{\"end\":9342,\"start\":7254},{\"end\":10647,\"start\":9344},{\"end\":11953,\"start\":10649},{\"end\":12238,\"start\":11969},{\"end\":12911,\"start\":12240},{\"end\":13601,\"start\":12913},{\"end\":14218,\"start\":13652},{\"end\":14949,\"start\":14290},{\"end\":15199,\"start\":15000},{\"end\":15376,\"start\":15275},{\"end\":15671,\"start\":15378},{\"end\":16309,\"start\":15753},{\"end\":16691,\"start\":16470},{\"end\":17346,\"start\":16741},{\"end\":18259,\"start\":17436},{\"end\":18365,\"start\":18329},{\"end\":18708,\"start\":18367},{\"end\":18952,\"start\":18820},{\"end\":19092,\"start\":18954},{\"end\":19614,\"start\":19163},{\"end\":19809,\"start\":19678},{\"end\":20486,\"start\":19849},{\"end\":20544,\"start\":20523},{\"end\":20569,\"start\":20560},{\"end\":21694,\"start\":20571},{\"end\":23311,\"start\":21720},{\"end\":23819,\"start\":23349},{\"end\":24586,\"start\":23821},{\"end\":25395,\"start\":24610},{\"end\":25856,\"start\":25397},{\"end\":26873,\"start\":25858},{\"end\":27262,\"start\":26896},{\"end\":30230,\"start\":27264},{\"end\":31171,\"start\":30245}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13651,\"start\":13602},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14289,\"start\":14219},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14999,\"start\":14950},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15274,\"start\":15200},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15752,\"start\":15672},{\"attributes\":{\"id\":\"formula_5\"},\"end\":16469,\"start\":16310},{\"attributes\":{\"id\":\"formula_6\"},\"end\":16740,\"start\":16692},{\"attributes\":{\"id\":\"formula_7\"},\"end\":17435,\"start\":17347},{\"attributes\":{\"id\":\"formula_8\"},\"end\":18328,\"start\":18260},{\"attributes\":{\"id\":\"formula_9\"},\"end\":18819,\"start\":18709},{\"attributes\":{\"id\":\"formula_10\"},\"end\":19162,\"start\":19093},{\"attributes\":{\"id\":\"formula_11\"},\"end\":19677,\"start\":19615},{\"attributes\":{\"id\":\"formula_12\"},\"end\":19848,\"start\":19810},{\"attributes\":{\"id\":\"formula_13\"},\"end\":20522,\"start\":20487}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":22338,\"start\":22331},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":23924,\"start\":23917},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":25418,\"start\":25411},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":26571,\"start\":26564},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":27029,\"start\":27022}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1876,\"start\":1864},{\"attributes\":{\"n\":\"2.\"},\"end\":7252,\"start\":7240},{\"attributes\":{\"n\":\"3.\"},\"end\":11967,\"start\":11956},{\"attributes\":{\"n\":\"4.\"},\"end\":20558,\"start\":20547},{\"attributes\":{\"n\":\"4.1.\"},\"end\":21718,\"start\":21697},{\"attributes\":{\"n\":\"4.2.\"},\"end\":23347,\"start\":23314},{\"attributes\":{\"n\":\"4.3.\"},\"end\":24608,\"start\":24589},{\"attributes\":{\"n\":\"4.4.\"},\"end\":26894,\"start\":26876},{\"attributes\":{\"n\":\"5.\"},\"end\":30243,\"start\":30233},{\"end\":31183,\"start\":31173},{\"end\":31365,\"start\":31355},{\"end\":31499,\"start\":31489},{\"end\":32022,\"start\":32013},{\"end\":32570,\"start\":32561},{\"end\":33478,\"start\":33469},{\"end\":34144,\"start\":34135}]", "table": "[{\"end\":32011,\"start\":31726},{\"end\":32559,\"start\":32216},{\"end\":33467,\"start\":32902},{\"end\":34133,\"start\":33480},{\"end\":35613,\"start\":34758}]", "figure_caption": "[{\"end\":31353,\"start\":31185},{\"end\":31487,\"start\":31367},{\"end\":31690,\"start\":31501},{\"end\":31726,\"start\":31693},{\"end\":32216,\"start\":32024},{\"end\":32902,\"start\":32572},{\"end\":34758,\"start\":34146}]", "figure_ref": "[{\"end\":4960,\"start\":4952},{\"end\":10182,\"start\":10174},{\"end\":12418,\"start\":12410},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":24243,\"start\":24235},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":28453,\"start\":28445},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":28538,\"start\":28530},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":28991,\"start\":28983},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":29527,\"start\":29521}]", "bib_author_first_name": "[{\"end\":35789,\"start\":35784},{\"end\":35815,\"start\":35806},{\"end\":35831,\"start\":35826},{\"end\":35833,\"start\":35832},{\"end\":35850,\"start\":35843},{\"end\":36105,\"start\":36099},{\"end\":36119,\"start\":36114},{\"end\":36142,\"start\":36138},{\"end\":36371,\"start\":36367},{\"end\":36387,\"start\":36383},{\"end\":36401,\"start\":36397},{\"end\":36415,\"start\":36411},{\"end\":36433,\"start\":36425},{\"end\":36451,\"start\":36443},{\"end\":36459,\"start\":36452},{\"end\":36694,\"start\":36682},{\"end\":36712,\"start\":36706},{\"end\":36731,\"start\":36725},{\"end\":36748,\"start\":36743},{\"end\":36766,\"start\":36759},{\"end\":36984,\"start\":36979},{\"end\":36990,\"start\":36985},{\"end\":37016,\"start\":37001},{\"end\":37034,\"start\":37028},{\"end\":37049,\"start\":37042},{\"end\":37065,\"start\":37058},{\"end\":37340,\"start\":37330},{\"end\":37362,\"start\":37356},{\"end\":37375,\"start\":37371},{\"end\":37625,\"start\":37619},{\"end\":37639,\"start\":37633},{\"end\":37652,\"start\":37645},{\"end\":37666,\"start\":37658},{\"end\":37675,\"start\":37672},{\"end\":37689,\"start\":37681},{\"end\":37943,\"start\":37935},{\"end\":38218,\"start\":38214},{\"end\":38220,\"start\":38219},{\"end\":38230,\"start\":38229},{\"end\":38232,\"start\":38231},{\"end\":38246,\"start\":38242},{\"end\":38252,\"start\":38247},{\"end\":38265,\"start\":38259},{\"end\":38284,\"start\":38277},{\"end\":38286,\"start\":38285},{\"end\":38300,\"start\":38295},{\"end\":38302,\"start\":38301},{\"end\":38320,\"start\":38312},{\"end\":38322,\"start\":38321},{\"end\":38336,\"start\":38331},{\"end\":38338,\"start\":38337},{\"end\":38354,\"start\":38346},{\"end\":38685,\"start\":38683},{\"end\":38697,\"start\":38691},{\"end\":38728,\"start\":38716},{\"end\":38743,\"start\":38740},{\"end\":39020,\"start\":39012},{\"end\":39028,\"start\":39025},{\"end\":39037,\"start\":39033},{\"end\":39050,\"start\":39045},{\"end\":39064,\"start\":39056},{\"end\":39072,\"start\":39071},{\"end\":39074,\"start\":39073},{\"end\":39085,\"start\":39081},{\"end\":39359,\"start\":39351},{\"end\":39373,\"start\":39367},{\"end\":39375,\"start\":39374},{\"end\":39558,\"start\":39550},{\"end\":39574,\"start\":39566},{\"end\":39589,\"start\":39585},{\"end\":39604,\"start\":39598},{\"end\":39618,\"start\":39614},{\"end\":39639,\"start\":39631},{\"end\":39657,\"start\":39652},{\"end\":39674,\"start\":39668},{\"end\":39676,\"start\":39675},{\"end\":40014,\"start\":40006},{\"end\":40025,\"start\":40024},{\"end\":40050,\"start\":40043},{\"end\":40063,\"start\":40058},{\"end\":40299,\"start\":40292},{\"end\":40311,\"start\":40304},{\"end\":40327,\"start\":40319},{\"end\":40337,\"start\":40333},{\"end\":40555,\"start\":40552},{\"end\":40573,\"start\":40567},{\"end\":40779,\"start\":40775},{\"end\":40793,\"start\":40787},{\"end\":40804,\"start\":40800},{\"end\":40806,\"start\":40805},{\"end\":40817,\"start\":40813},{\"end\":41018,\"start\":41015},{\"end\":41029,\"start\":41026},{\"end\":41040,\"start\":41032},{\"end\":41055,\"start\":41048},{\"end\":41206,\"start\":41199},{\"end\":41220,\"start\":41215},{\"end\":41235,\"start\":41231},{\"end\":41247,\"start\":41242},{\"end\":41263,\"start\":41255},{\"end\":41277,\"start\":41270},{\"end\":41290,\"start\":41285},{\"end\":41304,\"start\":41302},{\"end\":41315,\"start\":41310},{\"end\":41608,\"start\":41604},{\"end\":41629,\"start\":41621},{\"end\":41810,\"start\":41806},{\"end\":41827,\"start\":41823},{\"end\":41847,\"start\":41839},{\"end\":41849,\"start\":41848},{\"end\":42071,\"start\":42067},{\"end\":42083,\"start\":42079},{\"end\":42098,\"start\":42092},{\"end\":42114,\"start\":42107},{\"end\":42361,\"start\":42359},{\"end\":42373,\"start\":42366},{\"end\":42386,\"start\":42380},{\"end\":42400,\"start\":42393},{\"end\":42402,\"start\":42401},{\"end\":42617,\"start\":42615},{\"end\":42629,\"start\":42622},{\"end\":42642,\"start\":42636},{\"end\":42656,\"start\":42649},{\"end\":42658,\"start\":42657},{\"end\":42868,\"start\":42866},{\"end\":42880,\"start\":42873},{\"end\":42895,\"start\":42888},{\"end\":42906,\"start\":42902},{\"end\":42918,\"start\":42912},{\"end\":42932,\"start\":42925},{\"end\":42934,\"start\":42933},{\"end\":43171,\"start\":43162},{\"end\":43181,\"start\":43178},{\"end\":43194,\"start\":43187},{\"end\":43208,\"start\":43201},{\"end\":43210,\"start\":43209},{\"end\":43417,\"start\":43408},{\"end\":43432,\"start\":43424},{\"end\":43445,\"start\":43438},{\"end\":43459,\"start\":43452},{\"end\":43461,\"start\":43460},{\"end\":43677,\"start\":43668},{\"end\":43687,\"start\":43684},{\"end\":43700,\"start\":43693},{\"end\":43714,\"start\":43707},{\"end\":43716,\"start\":43715},{\"end\":43966,\"start\":43961},{\"end\":43978,\"start\":43972},{\"end\":44216,\"start\":44204},{\"end\":44232,\"start\":44226},{\"end\":44237,\"start\":44233},{\"end\":44251,\"start\":44244},{\"end\":44265,\"start\":44260},{\"end\":44522,\"start\":44513},{\"end\":44540,\"start\":44533},{\"end\":44771,\"start\":44766},{\"end\":44786,\"start\":44781},{\"end\":45007,\"start\":45002},{\"end\":45022,\"start\":45017},{\"end\":45041,\"start\":45035},{\"end\":45043,\"start\":45042},{\"end\":45063,\"start\":45053},{\"end\":45305,\"start\":45297},{\"end\":45320,\"start\":45315},{\"end\":45339,\"start\":45331},{\"end\":45565,\"start\":45560},{\"end\":45577,\"start\":45574},{\"end\":45588,\"start\":45584},{\"end\":45607,\"start\":45597},{\"end\":45620,\"start\":45618},{\"end\":45633,\"start\":45625},{\"end\":45834,\"start\":45829},{\"end\":46047,\"start\":46039},{\"end\":46058,\"start\":46054},{\"end\":46067,\"start\":46065},{\"end\":46245,\"start\":46238},{\"end\":46261,\"start\":46253},{\"end\":46271,\"start\":46267},{\"end\":46288,\"start\":46282},{\"end\":46302,\"start\":46298},{\"end\":46526,\"start\":46522},{\"end\":46542,\"start\":46536},{\"end\":46558,\"start\":46552},{\"end\":46582,\"start\":46572},{\"end\":46601,\"start\":46594},{\"end\":46616,\"start\":46610},{\"end\":46885,\"start\":46880},{\"end\":46905,\"start\":46900},{\"end\":46915,\"start\":46910},{\"end\":47120,\"start\":47113},{\"end\":47145,\"start\":47137},{\"end\":47352,\"start\":47344},{\"end\":47368,\"start\":47360},{\"end\":47383,\"start\":47379},{\"end\":47395,\"start\":47391},{\"end\":47397,\"start\":47396},{\"end\":47413,\"start\":47405},{\"end\":47428,\"start\":47422},{\"end\":47719,\"start\":47713},{\"end\":47733,\"start\":47726},{\"end\":47745,\"start\":47738},{\"end\":47747,\"start\":47746},{\"end\":47760,\"start\":47756},{\"end\":47762,\"start\":47761},{\"end\":48016,\"start\":48010},{\"end\":48028,\"start\":48023},{\"end\":48040,\"start\":48033},{\"end\":48042,\"start\":48041},{\"end\":48055,\"start\":48051},{\"end\":48057,\"start\":48056},{\"end\":48266,\"start\":48263},{\"end\":48280,\"start\":48273},{\"end\":48508,\"start\":48502},{\"end\":48520,\"start\":48515},{\"end\":48810,\"start\":48805},{\"end\":48820,\"start\":48815},{\"end\":49090,\"start\":49083},{\"end\":49099,\"start\":49095},{\"end\":49111,\"start\":49105},{\"end\":49123,\"start\":49118},{\"end\":49136,\"start\":49132},{\"end\":49356,\"start\":49350},{\"end\":49373,\"start\":49368},{\"end\":49565,\"start\":49561},{\"end\":49576,\"start\":49572},{\"end\":49584,\"start\":49582},{\"end\":49599,\"start\":49591},{\"end\":49601,\"start\":49600},{\"end\":49855,\"start\":49848},{\"end\":49869,\"start\":49862},{\"end\":49883,\"start\":49876},{\"end\":49885,\"start\":49884},{\"end\":49901,\"start\":49898}]", "bib_author_last_name": "[{\"end\":35804,\"start\":35790},{\"end\":35824,\"start\":35816},{\"end\":35841,\"start\":35834},{\"end\":35857,\"start\":35851},{\"end\":35867,\"start\":35859},{\"end\":36112,\"start\":36106},{\"end\":36136,\"start\":36120},{\"end\":36152,\"start\":36143},{\"end\":36161,\"start\":36154},{\"end\":36381,\"start\":36372},{\"end\":36395,\"start\":36388},{\"end\":36409,\"start\":36402},{\"end\":36423,\"start\":36416},{\"end\":36441,\"start\":36434},{\"end\":36467,\"start\":36460},{\"end\":36704,\"start\":36695},{\"end\":36723,\"start\":36713},{\"end\":36741,\"start\":36732},{\"end\":36757,\"start\":36749},{\"end\":36772,\"start\":36767},{\"end\":36999,\"start\":36991},{\"end\":37026,\"start\":37017},{\"end\":37040,\"start\":37035},{\"end\":37056,\"start\":37050},{\"end\":37073,\"start\":37066},{\"end\":37354,\"start\":37341},{\"end\":37369,\"start\":37363},{\"end\":37383,\"start\":37376},{\"end\":37631,\"start\":37626},{\"end\":37643,\"start\":37640},{\"end\":37656,\"start\":37653},{\"end\":37670,\"start\":37667},{\"end\":37679,\"start\":37676},{\"end\":37695,\"start\":37690},{\"end\":37950,\"start\":37944},{\"end\":38227,\"start\":38221},{\"end\":38240,\"start\":38233},{\"end\":38257,\"start\":38253},{\"end\":38275,\"start\":38266},{\"end\":38293,\"start\":38287},{\"end\":38310,\"start\":38303},{\"end\":38329,\"start\":38323},{\"end\":38344,\"start\":38339},{\"end\":38360,\"start\":38355},{\"end\":38689,\"start\":38686},{\"end\":38714,\"start\":38698},{\"end\":38738,\"start\":38729},{\"end\":38751,\"start\":38744},{\"end\":39023,\"start\":39021},{\"end\":39031,\"start\":39029},{\"end\":39043,\"start\":39038},{\"end\":39054,\"start\":39051},{\"end\":39069,\"start\":39065},{\"end\":39079,\"start\":39075},{\"end\":39091,\"start\":39086},{\"end\":39097,\"start\":39093},{\"end\":39365,\"start\":39360},{\"end\":39385,\"start\":39376},{\"end\":39564,\"start\":39559},{\"end\":39583,\"start\":39575},{\"end\":39596,\"start\":39590},{\"end\":39612,\"start\":39605},{\"end\":39629,\"start\":39619},{\"end\":39650,\"start\":39640},{\"end\":39666,\"start\":39658},{\"end\":39686,\"start\":39677},{\"end\":40022,\"start\":40015},{\"end\":40041,\"start\":40026},{\"end\":40056,\"start\":40051},{\"end\":40072,\"start\":40064},{\"end\":40302,\"start\":40300},{\"end\":40317,\"start\":40312},{\"end\":40331,\"start\":40328},{\"end\":40341,\"start\":40338},{\"end\":40565,\"start\":40556},{\"end\":40584,\"start\":40574},{\"end\":40785,\"start\":40780},{\"end\":40798,\"start\":40794},{\"end\":40811,\"start\":40807},{\"end\":40823,\"start\":40818},{\"end\":41024,\"start\":41019},{\"end\":41046,\"start\":41041},{\"end\":41060,\"start\":41056},{\"end\":41213,\"start\":41207},{\"end\":41229,\"start\":41221},{\"end\":41240,\"start\":41236},{\"end\":41253,\"start\":41248},{\"end\":41268,\"start\":41264},{\"end\":41283,\"start\":41278},{\"end\":41300,\"start\":41291},{\"end\":41308,\"start\":41305},{\"end\":41324,\"start\":41316},{\"end\":41619,\"start\":41609},{\"end\":41636,\"start\":41630},{\"end\":41821,\"start\":41811},{\"end\":41837,\"start\":41828},{\"end\":41856,\"start\":41850},{\"end\":42077,\"start\":42072},{\"end\":42090,\"start\":42084},{\"end\":42105,\"start\":42099},{\"end\":42122,\"start\":42115},{\"end\":42364,\"start\":42362},{\"end\":42378,\"start\":42374},{\"end\":42391,\"start\":42387},{\"end\":42413,\"start\":42403},{\"end\":42620,\"start\":42618},{\"end\":42634,\"start\":42630},{\"end\":42647,\"start\":42643},{\"end\":42669,\"start\":42659},{\"end\":42871,\"start\":42869},{\"end\":42886,\"start\":42881},{\"end\":42900,\"start\":42896},{\"end\":42910,\"start\":42907},{\"end\":42923,\"start\":42919},{\"end\":42945,\"start\":42935},{\"end\":43176,\"start\":43172},{\"end\":43185,\"start\":43182},{\"end\":43199,\"start\":43195},{\"end\":43217,\"start\":43211},{\"end\":43422,\"start\":43418},{\"end\":43436,\"start\":43433},{\"end\":43450,\"start\":43446},{\"end\":43468,\"start\":43462},{\"end\":43682,\"start\":43678},{\"end\":43691,\"start\":43688},{\"end\":43705,\"start\":43701},{\"end\":43723,\"start\":43717},{\"end\":43970,\"start\":43967},{\"end\":43983,\"start\":43979},{\"end\":44224,\"start\":44217},{\"end\":44242,\"start\":44238},{\"end\":44258,\"start\":44252},{\"end\":44271,\"start\":44266},{\"end\":44531,\"start\":44523},{\"end\":44547,\"start\":44541},{\"end\":44779,\"start\":44772},{\"end\":44792,\"start\":44787},{\"end\":45015,\"start\":45008},{\"end\":45033,\"start\":45023},{\"end\":45051,\"start\":45044},{\"end\":45071,\"start\":45064},{\"end\":45313,\"start\":45306},{\"end\":45329,\"start\":45321},{\"end\":45349,\"start\":45340},{\"end\":45572,\"start\":45566},{\"end\":45582,\"start\":45578},{\"end\":45595,\"start\":45589},{\"end\":45616,\"start\":45608},{\"end\":45623,\"start\":45621},{\"end\":45636,\"start\":45634},{\"end\":45851,\"start\":45835},{\"end\":45857,\"start\":45853},{\"end\":46052,\"start\":46048},{\"end\":46063,\"start\":46059},{\"end\":46072,\"start\":46068},{\"end\":46251,\"start\":46246},{\"end\":46265,\"start\":46262},{\"end\":46280,\"start\":46272},{\"end\":46296,\"start\":46289},{\"end\":46309,\"start\":46303},{\"end\":46534,\"start\":46527},{\"end\":46550,\"start\":46543},{\"end\":46570,\"start\":46559},{\"end\":46592,\"start\":46583},{\"end\":46608,\"start\":46602},{\"end\":46625,\"start\":46617},{\"end\":46898,\"start\":46886},{\"end\":46908,\"start\":46906},{\"end\":46923,\"start\":46916},{\"end\":47135,\"start\":47121},{\"end\":47152,\"start\":47146},{\"end\":47358,\"start\":47353},{\"end\":47377,\"start\":47369},{\"end\":47389,\"start\":47384},{\"end\":47403,\"start\":47398},{\"end\":47420,\"start\":47414},{\"end\":47437,\"start\":47429},{\"end\":47724,\"start\":47720},{\"end\":47736,\"start\":47734},{\"end\":47754,\"start\":47748},{\"end\":47767,\"start\":47763},{\"end\":48021,\"start\":48017},{\"end\":48031,\"start\":48029},{\"end\":48049,\"start\":48043},{\"end\":48062,\"start\":48058},{\"end\":48271,\"start\":48267},{\"end\":48285,\"start\":48281},{\"end\":48513,\"start\":48509},{\"end\":48524,\"start\":48521},{\"end\":48813,\"start\":48811},{\"end\":48825,\"start\":48821},{\"end\":49093,\"start\":49091},{\"end\":49103,\"start\":49100},{\"end\":49116,\"start\":49112},{\"end\":49130,\"start\":49124},{\"end\":49147,\"start\":49137},{\"end\":49366,\"start\":49357},{\"end\":49383,\"start\":49374},{\"end\":49570,\"start\":49566},{\"end\":49580,\"start\":49577},{\"end\":49589,\"start\":49585},{\"end\":49609,\"start\":49602},{\"end\":49860,\"start\":49856},{\"end\":49874,\"start\":49870},{\"end\":49896,\"start\":49886},{\"end\":49907,\"start\":49902}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":12271421},\"end\":36041,\"start\":35721},{\"attributes\":{\"id\":\"b1\"},\"end\":36320,\"start\":36043},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":8577357},\"end\":36652,\"start\":36322},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":2127515},\"end\":36928,\"start\":36654},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":81978372},\"end\":37240,\"start\":36930},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":221376618},\"end\":37552,\"start\":37242},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":219966650},\"end\":37864,\"start\":37554},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":31530137},\"end\":38152,\"start\":37866},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":15860720},\"end\":38609,\"start\":38154},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":202768984},\"end\":38925,\"start\":38611},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":220525462},\"end\":39298,\"start\":38927},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":6755881},\"end\":39500,\"start\":39300},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":2871880},\"end\":39929,\"start\":39502},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":12825123},\"end\":40244,\"start\":39931},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":206594692},\"end\":40464,\"start\":40246},{\"attributes\":{\"id\":\"b15\"},\"end\":40718,\"start\":40466},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":220363892},\"end\":40955,\"start\":40720},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":12789480},\"end\":41197,\"start\":40957},{\"attributes\":{\"id\":\"b18\"},\"end\":41547,\"start\":41199},{\"attributes\":{\"id\":\"b19\"},\"end\":41739,\"start\":41549},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":195908774},\"end\":42008,\"start\":41741},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":14542261},\"end\":42307,\"start\":42010},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":6037691},\"end\":42548,\"start\":42309},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":1883787},\"end\":42819,\"start\":42550},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":59553457},\"end\":43098,\"start\":42821},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":556999},\"end\":43363,\"start\":43100},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":46784066},\"end\":43611,\"start\":43365},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":12757870},\"end\":43864,\"start\":43613},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":219965588},\"end\":44128,\"start\":43866},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":49271126},\"end\":44445,\"start\":44130},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":208138410},\"end\":44676,\"start\":44447},{\"attributes\":{\"id\":\"b31\"},\"end\":44938,\"start\":44678},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":3172259},\"end\":45235,\"start\":44940},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":2630174},\"end\":45489,\"start\":45237},{\"attributes\":{\"id\":\"b34\"},\"end\":45796,\"start\":45491},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":740063},\"end\":45989,\"start\":45798},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":214713888},\"end\":46181,\"start\":45991},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":118674674},\"end\":46463,\"start\":46183},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":13754527},\"end\":46818,\"start\":46465},{\"attributes\":{\"doi\":\"abs/1807.03748\",\"id\":\"b39\"},\"end\":47081,\"start\":46820},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":5855042},\"end\":47276,\"start\":47083},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":44178122},\"end\":47634,\"start\":47278},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":173188134},\"end\":47934,\"start\":47636},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":80628419},\"end\":48220,\"start\":47936},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":3654323},\"end\":48394,\"start\":48222},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":222278558},\"end\":48674,\"start\":48396},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":170078598},\"end\":49000,\"start\":48676},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":220793552},\"end\":49324,\"start\":49002},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":15276198},\"end\":49469,\"start\":49326},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":223953563},\"end\":49784,\"start\":49471},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":220380867},\"end\":50057,\"start\":49786}]", "bib_title": "[{\"end\":35782,\"start\":35721},{\"end\":36097,\"start\":36043},{\"end\":36365,\"start\":36322},{\"end\":36680,\"start\":36654},{\"end\":36977,\"start\":36930},{\"end\":37328,\"start\":37242},{\"end\":37617,\"start\":37554},{\"end\":37933,\"start\":37866},{\"end\":38212,\"start\":38154},{\"end\":38681,\"start\":38611},{\"end\":39010,\"start\":38927},{\"end\":39349,\"start\":39300},{\"end\":39548,\"start\":39502},{\"end\":40004,\"start\":39931},{\"end\":40290,\"start\":40246},{\"end\":40773,\"start\":40720},{\"end\":41013,\"start\":40957},{\"end\":41804,\"start\":41741},{\"end\":42065,\"start\":42010},{\"end\":42357,\"start\":42309},{\"end\":42613,\"start\":42550},{\"end\":42864,\"start\":42821},{\"end\":43160,\"start\":43100},{\"end\":43406,\"start\":43365},{\"end\":43666,\"start\":43613},{\"end\":43959,\"start\":43866},{\"end\":44202,\"start\":44130},{\"end\":44511,\"start\":44447},{\"end\":44764,\"start\":44678},{\"end\":45000,\"start\":44940},{\"end\":45295,\"start\":45237},{\"end\":45827,\"start\":45798},{\"end\":46037,\"start\":45991},{\"end\":46236,\"start\":46183},{\"end\":46520,\"start\":46465},{\"end\":47111,\"start\":47083},{\"end\":47342,\"start\":47278},{\"end\":47711,\"start\":47636},{\"end\":48008,\"start\":47936},{\"end\":48261,\"start\":48222},{\"end\":48500,\"start\":48396},{\"end\":48803,\"start\":48676},{\"end\":49081,\"start\":49002},{\"end\":49348,\"start\":49326},{\"end\":49559,\"start\":49471},{\"end\":49846,\"start\":49786}]", "bib_author": "[{\"end\":35806,\"start\":35784},{\"end\":35826,\"start\":35806},{\"end\":35843,\"start\":35826},{\"end\":35859,\"start\":35843},{\"end\":35869,\"start\":35859},{\"end\":36114,\"start\":36099},{\"end\":36138,\"start\":36114},{\"end\":36154,\"start\":36138},{\"end\":36163,\"start\":36154},{\"end\":36383,\"start\":36367},{\"end\":36397,\"start\":36383},{\"end\":36411,\"start\":36397},{\"end\":36425,\"start\":36411},{\"end\":36443,\"start\":36425},{\"end\":36469,\"start\":36443},{\"end\":36706,\"start\":36682},{\"end\":36725,\"start\":36706},{\"end\":36743,\"start\":36725},{\"end\":36759,\"start\":36743},{\"end\":36774,\"start\":36759},{\"end\":37001,\"start\":36979},{\"end\":37028,\"start\":37001},{\"end\":37042,\"start\":37028},{\"end\":37058,\"start\":37042},{\"end\":37075,\"start\":37058},{\"end\":37356,\"start\":37330},{\"end\":37371,\"start\":37356},{\"end\":37385,\"start\":37371},{\"end\":37633,\"start\":37619},{\"end\":37645,\"start\":37633},{\"end\":37658,\"start\":37645},{\"end\":37672,\"start\":37658},{\"end\":37681,\"start\":37672},{\"end\":37697,\"start\":37681},{\"end\":37952,\"start\":37935},{\"end\":38229,\"start\":38214},{\"end\":38242,\"start\":38229},{\"end\":38259,\"start\":38242},{\"end\":38277,\"start\":38259},{\"end\":38295,\"start\":38277},{\"end\":38312,\"start\":38295},{\"end\":38331,\"start\":38312},{\"end\":38346,\"start\":38331},{\"end\":38362,\"start\":38346},{\"end\":38691,\"start\":38683},{\"end\":38716,\"start\":38691},{\"end\":38740,\"start\":38716},{\"end\":38753,\"start\":38740},{\"end\":39025,\"start\":39012},{\"end\":39033,\"start\":39025},{\"end\":39045,\"start\":39033},{\"end\":39056,\"start\":39045},{\"end\":39071,\"start\":39056},{\"end\":39081,\"start\":39071},{\"end\":39093,\"start\":39081},{\"end\":39099,\"start\":39093},{\"end\":39367,\"start\":39351},{\"end\":39387,\"start\":39367},{\"end\":39566,\"start\":39550},{\"end\":39585,\"start\":39566},{\"end\":39598,\"start\":39585},{\"end\":39614,\"start\":39598},{\"end\":39631,\"start\":39614},{\"end\":39652,\"start\":39631},{\"end\":39668,\"start\":39652},{\"end\":39688,\"start\":39668},{\"end\":40024,\"start\":40006},{\"end\":40043,\"start\":40024},{\"end\":40058,\"start\":40043},{\"end\":40074,\"start\":40058},{\"end\":40304,\"start\":40292},{\"end\":40319,\"start\":40304},{\"end\":40333,\"start\":40319},{\"end\":40343,\"start\":40333},{\"end\":40567,\"start\":40552},{\"end\":40586,\"start\":40567},{\"end\":40787,\"start\":40775},{\"end\":40800,\"start\":40787},{\"end\":40813,\"start\":40800},{\"end\":40825,\"start\":40813},{\"end\":41026,\"start\":41015},{\"end\":41032,\"start\":41026},{\"end\":41048,\"start\":41032},{\"end\":41062,\"start\":41048},{\"end\":41215,\"start\":41199},{\"end\":41231,\"start\":41215},{\"end\":41242,\"start\":41231},{\"end\":41255,\"start\":41242},{\"end\":41270,\"start\":41255},{\"end\":41285,\"start\":41270},{\"end\":41302,\"start\":41285},{\"end\":41310,\"start\":41302},{\"end\":41326,\"start\":41310},{\"end\":41621,\"start\":41604},{\"end\":41638,\"start\":41621},{\"end\":41823,\"start\":41806},{\"end\":41839,\"start\":41823},{\"end\":41858,\"start\":41839},{\"end\":42079,\"start\":42067},{\"end\":42092,\"start\":42079},{\"end\":42107,\"start\":42092},{\"end\":42124,\"start\":42107},{\"end\":42366,\"start\":42359},{\"end\":42380,\"start\":42366},{\"end\":42393,\"start\":42380},{\"end\":42415,\"start\":42393},{\"end\":42622,\"start\":42615},{\"end\":42636,\"start\":42622},{\"end\":42649,\"start\":42636},{\"end\":42671,\"start\":42649},{\"end\":42873,\"start\":42866},{\"end\":42888,\"start\":42873},{\"end\":42902,\"start\":42888},{\"end\":42912,\"start\":42902},{\"end\":42925,\"start\":42912},{\"end\":42947,\"start\":42925},{\"end\":43178,\"start\":43162},{\"end\":43187,\"start\":43178},{\"end\":43201,\"start\":43187},{\"end\":43219,\"start\":43201},{\"end\":43424,\"start\":43408},{\"end\":43438,\"start\":43424},{\"end\":43452,\"start\":43438},{\"end\":43470,\"start\":43452},{\"end\":43684,\"start\":43668},{\"end\":43693,\"start\":43684},{\"end\":43707,\"start\":43693},{\"end\":43725,\"start\":43707},{\"end\":43972,\"start\":43961},{\"end\":43985,\"start\":43972},{\"end\":44226,\"start\":44204},{\"end\":44244,\"start\":44226},{\"end\":44260,\"start\":44244},{\"end\":44273,\"start\":44260},{\"end\":44533,\"start\":44513},{\"end\":44549,\"start\":44533},{\"end\":44781,\"start\":44766},{\"end\":44794,\"start\":44781},{\"end\":45017,\"start\":45002},{\"end\":45035,\"start\":45017},{\"end\":45053,\"start\":45035},{\"end\":45073,\"start\":45053},{\"end\":45315,\"start\":45297},{\"end\":45331,\"start\":45315},{\"end\":45351,\"start\":45331},{\"end\":45574,\"start\":45560},{\"end\":45584,\"start\":45574},{\"end\":45597,\"start\":45584},{\"end\":45618,\"start\":45597},{\"end\":45625,\"start\":45618},{\"end\":45638,\"start\":45625},{\"end\":45853,\"start\":45829},{\"end\":45859,\"start\":45853},{\"end\":46054,\"start\":46039},{\"end\":46065,\"start\":46054},{\"end\":46074,\"start\":46065},{\"end\":46253,\"start\":46238},{\"end\":46267,\"start\":46253},{\"end\":46282,\"start\":46267},{\"end\":46298,\"start\":46282},{\"end\":46311,\"start\":46298},{\"end\":46536,\"start\":46522},{\"end\":46552,\"start\":46536},{\"end\":46572,\"start\":46552},{\"end\":46594,\"start\":46572},{\"end\":46610,\"start\":46594},{\"end\":46627,\"start\":46610},{\"end\":46900,\"start\":46880},{\"end\":46910,\"start\":46900},{\"end\":46925,\"start\":46910},{\"end\":47137,\"start\":47113},{\"end\":47154,\"start\":47137},{\"end\":47360,\"start\":47344},{\"end\":47379,\"start\":47360},{\"end\":47391,\"start\":47379},{\"end\":47405,\"start\":47391},{\"end\":47422,\"start\":47405},{\"end\":47439,\"start\":47422},{\"end\":47726,\"start\":47713},{\"end\":47738,\"start\":47726},{\"end\":47756,\"start\":47738},{\"end\":47769,\"start\":47756},{\"end\":48023,\"start\":48010},{\"end\":48033,\"start\":48023},{\"end\":48051,\"start\":48033},{\"end\":48064,\"start\":48051},{\"end\":48273,\"start\":48263},{\"end\":48287,\"start\":48273},{\"end\":48515,\"start\":48502},{\"end\":48526,\"start\":48515},{\"end\":48815,\"start\":48805},{\"end\":48827,\"start\":48815},{\"end\":49095,\"start\":49083},{\"end\":49105,\"start\":49095},{\"end\":49118,\"start\":49105},{\"end\":49132,\"start\":49118},{\"end\":49149,\"start\":49132},{\"end\":49368,\"start\":49350},{\"end\":49385,\"start\":49368},{\"end\":49572,\"start\":49561},{\"end\":49582,\"start\":49572},{\"end\":49591,\"start\":49582},{\"end\":49611,\"start\":49591},{\"end\":49862,\"start\":49848},{\"end\":49876,\"start\":49862},{\"end\":49898,\"start\":49876},{\"end\":49909,\"start\":49898}]", "bib_venue": "[{\"end\":42157,\"start\":42149},{\"end\":35873,\"start\":35869},{\"end\":36173,\"start\":36163},{\"end\":36480,\"start\":36469},{\"end\":36781,\"start\":36774},{\"end\":37079,\"start\":37075},{\"end\":37389,\"start\":37385},{\"end\":37701,\"start\":37697},{\"end\":38001,\"start\":37952},{\"end\":38369,\"start\":38362},{\"end\":38760,\"start\":38753},{\"end\":39103,\"start\":39099},{\"end\":39391,\"start\":39387},{\"end\":39707,\"start\":39688},{\"end\":40078,\"start\":40074},{\"end\":40347,\"start\":40343},{\"end\":40550,\"start\":40466},{\"end\":40829,\"start\":40825},{\"end\":41069,\"start\":41062},{\"end\":41333,\"start\":41326},{\"end\":41602,\"start\":41549},{\"end\":41865,\"start\":41858},{\"end\":42147,\"start\":42124},{\"end\":42419,\"start\":42415},{\"end\":42675,\"start\":42671},{\"end\":42951,\"start\":42947},{\"end\":43223,\"start\":43219},{\"end\":43480,\"start\":43470},{\"end\":43729,\"start\":43725},{\"end\":43989,\"start\":43985},{\"end\":44280,\"start\":44273},{\"end\":44553,\"start\":44549},{\"end\":44801,\"start\":44794},{\"end\":45077,\"start\":45073},{\"end\":45355,\"start\":45351},{\"end\":45558,\"start\":45491},{\"end\":45886,\"start\":45859},{\"end\":46078,\"start\":46074},{\"end\":46315,\"start\":46311},{\"end\":46634,\"start\":46627},{\"end\":46878,\"start\":46820},{\"end\":47173,\"start\":47154},{\"end\":47449,\"start\":47439},{\"end\":47776,\"start\":47769},{\"end\":48068,\"start\":48064},{\"end\":48301,\"start\":48287},{\"end\":48528,\"start\":48526},{\"end\":48831,\"start\":48827},{\"end\":49153,\"start\":49149},{\"end\":49389,\"start\":49385},{\"end\":49618,\"start\":49611},{\"end\":49913,\"start\":49909}]"}}}, "year": 2023, "month": 12, "day": 17}