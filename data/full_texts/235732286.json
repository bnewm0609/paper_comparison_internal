{"id": 235732286, "updated": "2023-10-06 01:27:56.955", "metadata": {"title": "Do Different Tracking Tasks Require Different Appearance Models?", "authors": "[{\"first\":\"Zhongdao\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Hengshuang\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Ya-Li\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Shengjin\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Philip\",\"last\":\"Torr\",\"middle\":[\"H.S.\"]},{\"first\":\"Luca\",\"last\":\"Bertinetto\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Tracking objects of interest in a video is one of the most popular and widely applicable problems in computer vision. However, with the years, a Cambrian explosion of use cases and benchmarks has fragmented the problem in a multitude of different experimental setups. As a consequence, the literature has fragmented too, and now novel approaches proposed by the community are usually specialised to fit only one specific setup. To understand to what extent this specialisation is necessary, in this work we present UniTrack, a solution to address five different tasks within the same framework. UniTrack consists of a single and task-agnostic appearance model, which can be learned in a supervised or self-supervised fashion, and multiple ``heads'' that address individual tasks and do not require training. We show how most tracking tasks can be solved within this framework, and that the same appearance model can be successfully used to obtain results that are competitive against specialised methods for most of the tasks considered. The framework also allows us to analyse appearance models obtained with the most recent self-supervised methods, thus extending their evaluation and comparison to a larger variety of important problems.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2107.02156", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/WangZLWTB21", "doi": null}}, "content": {"source": {"pdf_hash": "3c574538e1d37cc5f7428aeda5e106c932a48e12", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2107.02156v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "fdccdda0f48f455a0208b6c5a17ac82258e16b6f", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/3c574538e1d37cc5f7428aeda5e106c932a48e12.txt", "contents": "\nDo Different Tracking Tasks Require Different Appearance Models?\n\n\nZhongdao Wang \nBeijing National Research Center for Information Science and Technology (BNRist)\n\n\nDepartment of Electronic Engineering\nTsinghua University\n\n\nHengshuang Zhao \nTorr Vision Group\nUniversity of Oxford\n\n\nThe University of Hong\nKong\n\nYa-Li Li \nBeijing National Research Center for Information Science and Technology (BNRist)\n\n\nDepartment of Electronic Engineering\nTsinghua University\n\n\nShengjin Wang \nBeijing National Research Center for Information Science and Technology (BNRist)\n\n\nDepartment of Electronic Engineering\nTsinghua University\n\n\nPhilip H S Torr \nTorr Vision Group\nUniversity of Oxford\n\n\nLuca Bertinetto \nFive AI\n\n\nDo Different Tracking Tasks Require Different Appearance Models?\n\nTracking objects of interest in a video is one of the most popular and widely applicable problems in computer vision. However, with the years, a Cambrian explosion of use cases and benchmarks has fragmented the problem in a multitude of different experimental setups. As a consequence, the literature has fragmented too, and now novel approaches proposed by the community are usually specialised to fit only one specific setup. To understand to what extent this specialisation is necessary, in this work we present UniTrack, a solution to address five different tasks within the same framework. UniTrack consists of a single and task-agnostic appearance model, which can be learned in a supervised or self-supervised fashion, and multiple \"heads\" that address individual tasks and do not require training. We show how most tracking tasks can be solved within this framework, and that the same appearance model can be successfully used to obtain results that are competitive against specialised methods for most of the tasks considered. The framework also allows us to analyse appearance models obtained with the most recent self-supervised methods, thus extending their evaluation and comparison to a larger variety of important problems.\n\nIntroduction\n\nUnlike popular image-based computer vision tasks such as classification and object detection, which are (for the most part) unambiguous and clearly defined, the problem of object tracking has been considered under different setups and scenarios, each motivating the design of a separate set of benchmarks and methods. For instance, for the Single Object Tracking (SOT) and Video Object Segmentation (VOS) communities [93,40,65], tracking means estimating the location of an arbitrary user-annotated target object throughout a video, where the location of the object is represented by a bounding box in SOT and by a pixel-wise mask in VOS. Instead, in multiple object tracking settings (MOT [56], MOTS [80] and PoseTrack [2]), tracking means connecting sets of (often given) detections across video frames to address the problem of identity association and forming trajectories. Despite these tasks only differing in the number of objects per frame to consider and observation format (bounding boxes, keypoints or masks), the best practices developed by the methods tackling them vary significantly.\n\nThough the proliferation of setups, benchmarks and methods is positive in that it allows specific use cases to be thoroughly studied, we argue it makes increasingly harder to effectively study one of the fundamental problems that all these tasks have in common, i.e. what constitutes a good representation to track objects throughout a video? Recent advancements in large-scale models for language [20,8] and vision [32,13] have suggested that a strong representation can help addressing multiple downstream tasks. Similarly, we speculate that a good representation is likely to benefit many different tracking tasks, regardless of their specific setup. In order to validate our speculation, in this paper we present a framework that allows to adopt the same appearance model to address five different tracking tasks (Figure 2). In our taxonomy (Figure 4), we consider existing tracking tasks as problems that have either propagation or association at their core. When the core problem is propagation (as in SOT and VOS), one has to localise a target object in the current frame given its location in the previous one. Instead, in association problems (MOT, MOTS, and PoseTrack), target states in both previous and current frames are given, and the goal is to determine the correspondence between the two sets of observations. We show how most tracking tasks currently considered by the community can be simply expressed starting from the primitives of propagation or association. For propagation tasks, we employ existing box and mask propagation algorithms [7,84,81]. For association tasks, we propose a novel reconstruction-based metric that leverages fine-grained correspondence to measure similarities between observations. In the proposed framework, each individual task is assigned to a dedicated \"head\" that allows to represent the object(s) in the appropriate format to compare against prior arts on the relevant benchmarks.\n\nNote that, in our framework, only the appearance model contains parameters that can be learned via back-propagation, and that we do not experiment with appearance models that have been trained on specific tracking tasks. Instead, we adopt models trained via recent self-supervised learning (SSL) techniques and that have already demonstrated their effectiveness on a variety of image-based tasks. Our motivation is twofold. First, SSL models are particularly interesting for our use-case, as they are explicitly conceived to be of general purpose. As a byproduct, our work also serves the purpose of evaluating and comparing appearance models obtained from self-supervised learning approaches (see Figure 1). Second, we hope to facilitate the tracking community in directly benefiting from the rapid advancements of the self-supervised learning literature.\n\nTo summarise, the contributions of our work are as follows:\n\n\u2022 We propose UniTrack, a framework that supports five tracking tasks: SOT [93], VOS [65], MOT [56], MOTS [80], and PoseTrack [2]; and that can be easily extended to new ones.\n\n\u2022 We show how UniTrack can leverage many existing general-purpose appearance models to achieve a performance that is competitive with the state-of-the-art on several tracking tasks.\n\n\u2022 We propose a novel reconstruction-based similarity metric for association that preserves fine-grained visual features and supports multiple observation formats (box, mask and pose).\n\n\u2022 We perform an extensive evaluation of self-supervised models, significantly extending the empirical analysis of prior literature to video-based tasks.\n\n\nThe UniTrack Framework\n\n\nOverview\n\nInspecting existing tracking tasks and benchmarks, we noticed that their differences can be roughly categorised across four axes, illustrated in Figure 2 and detailed below.  Figure 1: High-level overview of the performance of 16 self-supervised learning models on five tracking tasks: SOT, VOS, MOT, PoseTracking and MOTS. A higher rank (better performance) corresponds to a vertex nearer to the outer circle. A larger area of the pentagon signifies better overall performance of its respective appearance model. Results of a vanilla ImageNet-supervised model are indicated with a gray dashed line as reference. Notice how the best model VFS [97] dominates on four out of the five tasks considered. All methods use ResNet-50.  Figure 2: Existing tracking problems and their respective benchmarks differ from each other under several aspects: the assumption could be that there is a single or multiple objects to track; targets can be specified by the user in the first frame only, or assumed to be given at every frame (e.g. provided by a detector); the classes of the targets can be known (classspecific) or unknown (class-agnostic); the representation of the targets can be bounding boxes, pixel-wise masks, or pose annotations.  The framework can be divided in three levels. Level-1: a trainable appearance model. Level-2: the fundamental primitives of propagation and association. Level-3: task-specific heads.\n\nTypically, in single-object tasks the target is specified by the user in the first frame, and it can be of any class. Instead, for multi-object tasks detections are generally considered as given for every frame, and the main challenge is to solve identity association for the several objects. Moreover, in multi-object tasks the set of classes to address is generally known (e.g. pedestrians or cars). Figure 3 depicts a schematic overview of the proposed UniTrack framework, which can be understood as conceptually divided in three \"levels\". The first level is represented by the appearance model, responsible for extracting high-resolution feature maps from the input frame (Section 2.2). The second level consists of the algorithmic primitives addressing propagation (Section 2.3) and association (Section 2.4). Finally, the last level comprises multiple task-specific algorithms that make direct use of the primitives of the second level. In this work, we illustrate how UniTrack can be used to obtain competitive performance on all of the five tracking tasks of level-3 from Figure 3. Moreover, new tracking tasks can be easily integrated.\n\nImportantly, note that the appearance model is the only component containing trainable parameters. The reason we opted for a shared and non task-specific representation is twofold. Firstly, the large amount of different setups motivated us to investigate whether having separately-trained models for each setup is necessary. Since training on specific datasets can bias the representation towards a limited set of visual concepts (e.g. animals or vehicles) and limit its applicability to \"open-world\" settings, we wanted to understand how far can a shared representation go. Second, we wanted to  Figure 4: Propagation v.s. Association. In the propagation problem, the goal is to estimate the target state at the current frame given the observation in the previous one. This is typically addressed for one object at the time. In the association problem, observations in both previous and current frames are given, and the goal is to determine correspondences between the two sets.\n\nprovide the community with multiple baselines that can be used to better assess newly proposed contributions, and that can be immediately used on new datasets and tasks without the need of retraining.\n\n\nBase appearance model\n\nThe base appearance model \u03c6 takes as input a 2D image I and outputs a feature map X = \u03c6(I) \u2208 R H\u00d7W \u00d7C . Since ideally an appearance model used for object propagation and association should be able to leverage fine-grained semantic correspondences between images, we choose a network with a small stride of r = 8, so that its output in feature space can have a relatively large resolution.\n\nWe refer to the vector (along the channel dimension) of a single point in the feature map as a point vector. We expect a point vector x i 1 \u2208 R C from the feature map X 1 to have a high similarity with its \"true match\" point vector x\u00ee 2 in X 2 , while being far apart from all the other point vectors\nx j 2 in X 2 ; i.e. we expect s(x i 1 , x\u00ee 2 ) > s(x i 1 , x j 2 )\n, \u2200j =\u00ee, where s(\u00b7, \u00b7) represents a similarity function. In order to learn fine-grained correspondences, fully-supervised methods are only amenable for synthetic datasets (e.g. Flying Chairs for optical flow [21]). With real-world data, it is intractable to label pixel-level correspondences and train models in a fully-supervised fashion. To overcome this obstacle, in this paper we adopt representations obtained with self-supervision. We experiment both with models trained with approaches that leverage pixel-wise pretext tasks [36,81] and, inspired by prior works that have pointed out how fine-grained correspondences emerge in middle-level features [53,97], with models obtained from image-level tasks (e.g. MoCo [32], SimCLR [13]).\n\n\nPropagation\n\nProblem definition. Figure 4a schematically illustrates the problem of propagation, which we use as a primitive to address SOT and VOS tasks. Considering the single-object case, given video frames {I t } T t=1 and an initial ground truth observation z 1 as input, the goal is to predict object states {\u1e91 t } T t=2 for each time-step t. In this work we consider three formats to represent objects: bounding boxes, segmentation masks and pose skeletons.\n\nMask propagation. In order to propagate masks, we rely on the approach popularised by recent video self-supervised methods [36,81,47,42]. Consider the feature maps of a pair of consecutive frames X t\u22121 and X t , both \u2208 R s\u00d7C , and the label mask z t\u22121 \u2208 [0, 1] s of the previous frame 2 , where s = H \u00d7 W indicates its spatial resolution. We compute the matrix of transitions K t t\u22121 = [k i,j ] s\u00d7s as the affinity matrix between X t\u22121 and X t . Each element k i,j is defined as\nki,j = Softmax(Xt\u22121, X t ; \u03c4 )ij = exp( x i t\u22121 , x j t /\u03c4 ) s k exp( x i t\u22121 , x k t /\u03c4 ) ,(1)\nwhere \u00b7, \u00b7 indicates inner product, and \u03c4 is a temperature hyperparameter. As in [36], we only keep the top K values for each row and set other values to zero. Then, the mask for the current frame at time t is predicted by propagating the previous prediction: z t = K t t\u22121 z t\u22121 . Mask propagation proceeds in a recurrent fashion: the output mask of the current frame is used as input for the next one.\n\nPose propagation. In order to represent pose keypoints, we use the widely adopted Gaussian belief maps [89]. For a keypoint p, we obtain a belief map z p \u2208 [0, 1] s by using a Gaussian with mean equal to the keypoint's location and variance proportional to the subject's body size. In order to propagate a pose, we can then individually propagate each belief map in the same manner as mask propagation, again as z p t = K t t\u22121 z p t\u22121 . Box propagation. The position of an object can also be more simply expressed with a fourdimensional vector z = (u, v, w, h), where (u, v) are the coordinates of the bounding-box center, and (w, h) are its width and height. While one could reuse the strategy adopted above by simply converting the bounding-box to a pixel-wise mask, we observed that using this strategy leads to inaccurate predictions. Instead, we use the approach of SiamFC [7], which consists in performing cross-correlation (XCORR) between the target template z t\u22121 and the frame X t to find the new location of the target in frame t. Cross-correlation is performed at different scales, so that the bounding-box representation can be resized accordingly. We also provide a Correlation Filter-based alternative (DCF) [76,84] (see Appendix B.1).\n\n\nAssociation\n\nProblem definition. Figure 4b schematically illustrates the association problem, which we use as primitive to address the tasks of MOT, MOTS and PoseTrack. In this case, observations for object states {\u1e90 t } T t=1 are given for all the frames {I t } T t=1 , typically via the output of a pre-trained detector. The goal here is to form trajectories by connecting observations across adjacent frames according to their identity.\n\nAssociation algorithm. We adopt the association algorithm proposed in JDE [88] for MOT, MOTS and PoseTrack tasks, of which detailed description can be found in Appendix C.1. In summary, we compute an N \u00d7 M distance matrix between N already-existing tracklets and M \"new\" detections from the last processed frame. We then use the Hungarian algorithm [41] to determine pairs of matches between tracklets and detections, using the distance matrix as input. To obtain the matrix of distances used by the algorithm, we compute the linear combination of two terms accounting for motion and appearance cues. For the former, we compute a matrix indicating how likely a detection corresponds to the object state predicted by a Kalman Filter [38].\n\nInstead, the appearance component is directly computed by using feature-map representations obtained by processing individual frames with the appearance model (Section 2.2). While object-level features for box and mask observations can be directly obtained by cropping frame-level feature maps, when an object is represented via a pose it first needs to be converted to a mask (via a procedure described in Appendix C.2).\n\nA key issue of this scenario is how to measure similarities between object-level features. We find existing methods limited. First, objects are often compared by computing the cosine similarity of average-pooled object-level feature maps [113,72]. However, the operation of average inherently discards local information, which is important for fine-grained recognition. Approaches [25,73] that instead to some extent do preserve fine-grained information, such as those computing the cosine similarity of (flattened) feature maps, do not support objects with differently-sized representation (situation that occurs for instance with pixel-level masks). To cope with the above limitations, we propose a reconstruction-based similarity metric that is able to deal with different observation formats, while still preserving fine-grained information.\n\nReconstruction Similarity Metric (RSM). Let {t i } N i=1 denote the object-level features of N existing tracklets, t i \u2208 R st i \u00d7C and s ti indicates the spatial size of the object, i.e. the area of the box or the mask representing it. Similarly, {d j } M j=1 denotes the object-level features of M new detections. With the goal of computing similarities to obtain an N \u00d7 M affinity matrix to feed to the Hungarian algorithm, we propose a novel reconstruction-based similarity metric (RSM) between pairs (i, j), which is obtained as\nRSM(i, j) = 1 2 (cos(t i ,t i\u2190j ) + cos(d j ,d j\u2190i )),(2)\nwheret i\u2190j represents t i reconstructed from d j andd j\u2190i represents d j reconstructed from t i . In multi-object tracking scenarios, observations are often incomplete due to frequent occlusions. As such, directly comparing features between incomplete and complete observations often fails because of misalignment between local features. Suppose d j is a detection feature representing a severely  For a pair of tracklet ti and detection dj, we \"extract\" the corresponding sub-matrix from the entire affinity matrix as linear weights and reconstruct ti from dj using these linear weights. The similarity between the original object-level feature and its reconstructed version is finally taken as the RSM. We want the metric to be symmetric, so we perform reconstruction both forward (ti \u2190 dj) and backward (ti \u2192 dj).\n\noccluded pedestrian, while t i a tracklet feature representing the same person, but unoccluded. Likely, directly computing the cosine similarity between the two will not be very telling. RSM addresses this issue by introducing a step of reconstruction after which the co-occurring parts of point features will be better aligned, thus making the final similarity more likely to be meaningful.\n\nThe reconstructed object-level feature mapt i\u2190j is a simple linear transformation of d j , i.e.\nt i\u2190j = R i\u2190j d j , where R i\u2190j \u2208 R st i \u00d7s d j\nis a transformation matrix obtained as follows. We first flatten and concatenate all object-level features belonging to a tracklet (i.e. the set of observations corresponding to an object) into a single feature matrix T \u2208 R ( i st i )\u00d7C . Similarly, we obtain all the object-level feature maps of a new set of detections D \u2208 R ( j s d j )\u00d7C . Then, we compute the affinity matrix A = Softmax(T D ) and \"extract\" individual R i\u2190j mappings as sub-matrices of A with respect to the appropriate (i, j) tracklet-detection pair:\nR i\u2190j = A i\u22121 i =1 s i : i i =1 s i , j\u22121 j =1 s j : j j =1 s j 3 .\nFor a schematic representation of the procedure just described, see Figure 5.\n\nRSM can be interpreted from an attention [78] perspective. The feature map of a tracklet t i being reconstructed can be seen as a set of queries, and the \"source\" detection feature d j can be interpreted both as keys and values. The goal is to reconstruct the queries by linear combination of the values. The linear combination (attention) weights are computed using the affinity between queries and keys. Specifically, we first compute a global affinity matrix between t i and all the d j for j = 1, ..., M , and then extract the corresponding sub-matrix for t i and d j as the attention weights. Our formulation leads to a desired property: if the attention weights approach zero, the corresponding reconstructed point vectors will approach zero and so the RSM between t i and d j .\n\nMeasuring similarity by reconstruction is popular in problems such as few-shot learning [90,106], self-supervised learning [51], and person re-identification [34]. However, reconstruction is typically framed as a ridge regression or optimal transport problem. With O(n 2 ) complexity, RSM is more efficient than ridge regression and it has a similar computation cost to calculating the Earth Moving Distance for the optimal transport problem. Appendix D shows a series of ablation studies illustrating the importance of the proposed RSM for the effectiveness of UniTrack on association-type tasks.\n\n\nExperiments\n\nSince UniTrack does not require task-specific training, we were able to experiment with many alternative appearance models (see Figure 3) with little computational cost. In Section 3.1 we perform an extensive evaluation to benchmark a wide variety of off-the-shelf, modern self-supervised models, showing their strengths and weaknesses on all five tasks considered. In this section we also conduct a correlation study with the so-called \"linear probe\" strategy [107], which became a popular way to evaluate representations obtained with self-supervised learning. Then, in Section 3.2 we compare UniTrack (equipped with supervised or unsupervised appearance models) against recent and task-specific tracking methods.\n\nImplementation details. We use ResNet-18 [33] or ResNet-50 as the default architecture. With ImageNet-supervised appearance model, we refer to the ImageNet pre-trained weights made available in PyTorch's \"Model Zoo\". To prevent excessive downsampling, we modify the spatial stride of layer3 and layer4 to 1, achieving a total stride of r = 8. We extract features from both layer3 and layer4. We report results with layer3 features when comparing against task-specific methods (Section 3.2), and with both layer3 and layer4 when evaluating multiple different representations (Section 3.1). Further implementation details are deferred to Appendix B and C.\n\nDatasets and evaluation metrics. For fair comparison with existing methods, we report results on standard benchmarks with conventional metrics for each task. Please refer to Appendix A for details.\n\n\nUniTrack as evaluation platform of previously-learned representations\n\nThe process of evaluating representations obtained via self-supervised learning (SSL) often involves additional training [22,32,13], for instance via the use of linear probes [107], which require to fix the pre-trained model and train an additional linear classifier on top of it. In contrast, using UniTrack as evaluation platform (1) does not require any additional training and (2) enables the evaluation on a battery of important video tasks, which have generally been neglected in self-supervised-learning papers in favour of more established image-level tasks such as classification.\n\nIn this section, we evaluate three types of SSL representations: (a) Image-level representations learned from images, e.g. MoCo [32] and BYOL [29]; (b) Pixel-level representations learned from images (such as DetCo [95] and PixPro [96]) and (c) videos (such as UVC [47] and CRW [36]). For all methods considered, we use the pre-trained weights provided by the authors.\n\nResults are shown in Table 1 and 2, where we report the results obtained by using features from either layer3 or layer4 of the pre-trained ResNet backbone. We report both results and separate them by a '/' in the table. Note that, for this analysis only, for association-type tasks motion cues are discarded to better highlight distinctions between different representations and avoid potential confounding factors. (1) There is no significant correlation between \"linear probe accuracy\" on ImageNet and overall tracking performance. The linear probe approach [107] has become a standard way to compare SSL representations. In Figure 7, we plot tracking performance on five tasks (y-axes) against ImageNet top-1 accuracy of 16 different models (x-axes), and report Pearson and Spearman (rank) correlation coefficients. We observe that the correlation between ImageNet accuracy and tracking performance is small, i.e. the Pearson's r ranges from \u22120.38 to +0.20, and Spearman's \u03c1 ranges from \u22120.36 to +0.26. For most tasks, there is almost no correlation, while for VOS the two measures are mildly inversely correlated. The result suggests that evaluating SSL models on five extra tasks with UniTrack could constitute a useful complement to ImageNet linear probe evaluation, and encourage the SSL community to pursue the design of even more general purpose representations.\n\n(2) A vanilla ImageNet-trained supervised representation is surprisingly effective across the board. On most tasks, it reports a performance competitive with the best representation for that task. This is particularly evident from Figure 1, where its performance is outlined as a gray dashed line. This result suggests that results obtained with vanilla ImageNet features should be reported when investigating new tracking methods.\n\n(3) The best self-supervised representation ranks first on most tasks. Recently, it has been shown how SSL-trained representations can match or surpass their supervised counterparts on ImageNet classification (e.g. [29]) and many downstream tasks [22,95]. Within UniTrack, although no Representation SOT [93] VOS [65] MOT [56] MOTS [80] PoseTrack [2] AUCXCorr \u2191 AUCDCF \u2191 J -mean\u2191 IDF1\u2191 HOTA\u2191 IDF1\u2191 HOTA\u2191 IDF1\u2191 IDs\u2193   individual SSL representation is able to beat the vanilla ImageNet-trained representation on every single task, we observe that the recently proposed VFS [97] ranks first on every task, except for single-object tracking. This suggests that advancements of the self-supervised learning literature can directly benefit the tracking community: it is reasonable to expect that newly-proposed representations will further improve performance across the board.\n\n(4) Pixel-level SSL representations do not seem to have a consistent advantage in pixel-level tasks.\n\nIn Table 2 and at the bottom of Table 1 we compare recent SSL representations trained with pixellevel proxy tasks: PixPro [96], DetCo [95], TimeCycle [87], Colorization [81], UVC [47] and Contrastive Random Walk (CRW) [36]. Considering that pixel-level models leverage more finegrained information during training, one may expect them to outperform image-based models in the tracking tasks where this is important. It is not straightforward to compare pixel-level SSL models with image-level ones, as the two types employ different default backbone networks. However, note how good image-based models (MoCo-v1, SimCLR-v2) are on par with their supervised counterpart in all tasks, while good pixel-level models (DetCo, CRW) still have gaps with respect to their supervised counterparts in tasks like SOT and MOT. Moreover, from  on image level features. The most important distinction is the training data. Previous SSL methods mostly train on still-image based datasets (typically ImageNet), while VFS employs a large-scale video dataset Kinetics [12]. Clearly, this is not very surprising, as training on video data can help closing the domain gap with the (video-based) downstream tasks considered in this paper.\n\n\nComparison with task-specific tracking methods\n\nUnsupervised methods. We observe that UniTrack performs competitively against unsupervised state-of-the-art methods in both the propagation-type tasks we considered (Table 3d and [47,36], and some of the most recent outperform UniTrack (with an ImageNet-trained representation). Nonetheless, when we use a VFS-trained representation, this performance difference is reduced to 2%. Finally, note that for association-type tasks we are not aware of any existing unsupervised learning method, and thus in this case we limit the comparison to supervised methods.\n\nComparison with supervised methods. In general, UniTrack with a ResNet-18 appearance model already performs on par with several existing task-specific supervised methods, and in several tasks it even shows superior accuracy, especially for identity-related metrics.  [60]. Compared with LightTrack, the MOTA of UniTrack degrades of 1.3 points because of an increased amount of ID switches. However, the IDF-1 score is improved by a significant margin (+21.0 points). This shows UniTrack preserves identity more accurately for long tracklets: even if ID switches occur more frequently, after a short period UniTrack is able to correct the wrong association, leading to a higher IDF-1.\n\nNotice how, overall, UniTrack obtains more competitive performance on tasks that have association at their core, i.e. MOT, MOTS and PoseTrack. Upon inspection, we observed that most failure cases in propagation-type tasks regard the \"drift\" occurring when the scale of the object is improperly estimated. In future work, this could be addressed for instance by a bounding-box regression module to refine predictions, or by carefully designing a motion model. For association-type tasks, the consequences of any type of inaccuracy are isolated to individual pairs of frames, and thus much less catastrophic by nature.\n\n\nRelated Work\n\nTo the best of our knowledge, sharing the appearance model across multiple tracking tasks has not been extensively studied in the computer vision literature, and especially not in the context of SSL representations. Some existing methods do share a common backbone architecture across tasks. For instance, STEm-Seg [4] addresses VIS [101] and MOTS; while TraDeS [92] addresses MOT, MOTS and VIS. However, both methods need to be trained separately and on different datasets for every task. Conversely, we reuse the same representation across five tasks. A promising direction for future work would be to use UniTrack to train a shared representation in a multi-task fashion. Only a few relevant works do adopt a multi-task approach [85,109,55], and they usually consider SOT and VOS tasks only. In general, despite the multi-task direction being surely interesting, it requires the availability of large-scale datasets with annotations in multiple formats, and costly training. These are two of the main reasons for which we believe that having a framework that allows to achieve competitive performance on multiple tasks with previously-trained models is a worthwhile endeavour.\n\nSelf-supervised model evaluation. Given the difference between the pretext tasks used to train self-supervised models and the downstream tasks used to evaluate them, the comparison between self-supervised approaches has always been a delicate matter. Existing evaluation strategies typically require additional training once a general-purpose representation has been obtained. One strategy keeps the representation fixed, and then trains additional task-specific heads with very limited capacity (e.g. a linear classifier [28,13,32] or a regression head for object detection [28]). A second strategy, instead, leverages SSL to obtain particularly effective initializations, and then proceeds to fine-tune such initialized models on the downstream task of interest. A wider range of tasks can be tested using this setup, such as semantic segmentation [22,28] and surface normal estimation [28,86]. In contrast, UniTrack provides a simpler way to evaluate SSL models, one that does not require additional training or fine-tuning. Also, this work is the first to extend SSL evaluation to a set of diverse video tasks. We believe this contribution will allow the study of self-supervised learning methods with a broader scope of applicability. Our work is also related to a line of self-supervised learning methods [36,81,47,42] that learn their representations in a task-agnostic fashion, and then test it on propagation tasks (SOT and VOS). The design of UniTrack is inspired by their task-agnostic philosophy, while significantly extending their scope to a new set of tasks.\n\n\nConclusion\n\nDo different tracking tasks require different appearance models? In order to address this question, the proposed UniTrack framework has been instrumental, as it has allowed to easily experiment with alternative representations on a wide variety of downstream problems. Although the answer is not a resounding \"no\", as only sometimes a single shared appearance model can outperform dedicated methods, we argue that a unified framework is an appealing alternative to task-specific methods. The main reason is that it allows us to make the most of the progress made in the representation learning literature at no extra cost. With the rapid development of self-supervised learning, and the large amount of computational resources dedicated to it, we believe it is reasonable to expect that, in the future, a general-purpose representation will be able to outperform task-specific methods across the board. Until then, UniTrack could still serve as a useful evaluation tool for novel representations, especially considering the lack of correlation with the standard linear-probe approach. We believe this will encourage the community to develop self-supervised representations that are of \"general purpose\" in a broader sense. Broader impact. Upon reflection, we believe that progress in tracking applications and selfsupervised learning is beneficial for society, as it can significantly impact (for instance) the de-velopment of autonomous vehicles, which we consider a net positive for society. We also recognise that the same technologies could constitute a threat if deployed for surveillance by entities hostile to civil liberties. \n\n\nFunding Transparency Statement\n\n\nAppendices: Do Different Video Tasks Require Different Appearance Models? A Datasets and Evaluation Metrics\n\nThe table below summarizes the datasets (all publicly available) and evaluation metrics used in this work. In general, to compare with existing task-specific methods, we use the most popular benchmark for each task and report the standard metrics.\n\nFor association-type tasks (MOT, MOTS and PoseTrack), we first report the MOTA metric since it highly-correlates with human's perception in measuring tracking accuracy [5]. However, the MOTA metric disproportionately overweights good detection accuracy [54,17]. Since most multi-object trackers (included UniTrack) adopt off-the-shelf detectors, it is desirable to also adopt detectionindependent measures of performance. For this reason, we also report identity based metrics such as IDF-1 and ID-switch. We also adopt the recently-introduced higher-order HOTA [54], to replace MOTA and to represent the overall tracking accuracy when comparing self-supervised methods.\n\nFor pose tracking, results are averaged for IDF-1 and MOTA, and summed for ID-switch, over 15 key points. In the main text, we only report results for the first five tasks from the table below. For the rest tasks (PoseProp and VIS) we provide additional results in Appendix E. We also provide SOT results on many more recent large-scale datasets in Appendix F. \n\n\nB Propagation\n\n\nB.1 Box Propagation\n\nIn order to propagate bounding boxes, we adopt two methods relying on fully-convolutional Siamese [7,76,84,44] networks. Given a target image patch I x that contains the object of interest, and a search image patch I z (typically a larger search area in the next frame), the appearance model \u03c6 processes both patches and outputs their feature maps x = \u03c6(I x ) and z = \u03c6(I z ).\n\nCross-correlation (XCorr) head. As in SiamFC [7], we simply cross-correlate the two feature maps, yielding the response map g(x, z) = x z\n\nEq. 3 is equivalent to performing an exhaustive search of the pattern x over the search region z. The location of the target object can be determined by finding the maximum value of response map.\n\nDiscriminative Correlation Filter (DCF) head. The DCF head [76,84] is similar to the XCorr head, with two major differences. The first one is that it involves solving a ridge-regression problem to find the template w = \u03c9(x) rather than using the original template x, so that the response map is given by\ng(x, z) = \u03c9(x) z(4)\nMore specifically, the DCF template w = \u03c9(x) is a more discriminative template compared with the original template, and is obtained by solving arg min\nw w x \u2212 y 2 + \u03bb w 2 ,(5)\nwhere y is an ideal response (here represented as a Gaussian function peaked at the center) and \u03bb \u2265 0 is the regularization coefficient typical of ridge regression. The solution to Eq. 5 can be computed efficiently in the Fourier domain [76,84] a\u015d\nw =x \u0177 * x x * + \u03bb(6)\nwhere the hat notationx = F(x) indicates the discrete Fourier Transform of x, y * represents the complex conjugate of y and denotes the Hadamard (element-wise) product. The response map can be computed via inverse Fourier Transform F \u22121 ,\ng(x, z) =\u0175 z = F \u22121 (\u0175 z)(7)\nAnother difference w.r.t the XCorr head is that it is effective to update the template online by simple moving average [84], i.e. ,\u0175 t =\n\u03b1xt \u0177 * +(1\u2212\u03b1)xt\u22121 \u0177 * \u03b1(xt x * t +\u03bb)+(1\u2212\u03b1)(xt\u22121 x * t\u22121 +\u03bb) .\nIn contrast, with the XCorr head every frame is compared against the first one.\n\nAs shown in Table 2 and Table 3 from the main paper, for the tested architectures and appearance models we can see a clear advantage of DCF of XCorr (note that the difference was less significant in the original [76] paper, though the experiments were done with a shallower architecture).\n\nHyper-parameters. Following common practice [7,44], we provide the Correlation Filter with a larger region of context in the template patch. To be specific, the template patch I x is determined by expanding the height and width of the target bounding box by k = 4.5 times. The search patch is also determined by expanding the bounding box by same amount, and its center corresponds the latest estimated location of the target. To handle scale variation of the object, we consider s = 3 different search patches at different scales 0.985 {1,0,1} . Template and search patches are cropped and resized to 520 \u00d7 520. This means that with a total stride of r = 8, we have feature maps of size 65 \u00d7 65. In the DCF head, we set the regularization coefficient to \u03bb = 1e \u22124 , and the moving average momentum to \u03b1 = 1e \u22122 . \n\n\nB.2 Mask and Pose Propagation\n\nIn Section 2.3 we introduced the recursive mask propagation as z t = K t t\u22121 z t\u22121 . In practice, to provide more temporal context, we use a memory bank [42,36] consisting of multiple former label maps as the source label z m instead of a single label map z t\u22121 , i.e. z t = K t m z m . More specifically, the resulting source label map is obtained by concatenating all the label maps inside the memory bank, z m \u2208 [0, 1] M s , where s is the spatial size of a single label map and M is the size of the memory bank. The softmax computed for K t m is applied over all M s points in the memory bank. The memory bank includes the first frame of the video, together with the latest M \u2212 1 frames, and we choose M = 6. As suggested by MAST [42] and CRW [36], we also introduce the local attention technique, which restricts the source points considered for each target point to a local circle with radius r = 12. The hyper-parameter k for the k-NN used when computing the transition matrix K t m is set to k = 10.\n\nPropagating pose key points is cast as propagating the mask of each individual key point, represented with the widely adopted Gaussian belief maps [89]. Each Gaussian has mean equal to the corresponding keypoint's location, and variance proportional to the subject's body size \u03c3 = max(\u03b7s body , 0.5).\n\nThe body size is determined by,\ns body = max(max p {x p } \u2212 min p {x p }, max p {y p } \u2212 min p {y p })(8)\nwhere (x p , y p ) are the coordinates of the p-th key point.\n\n\nMask/Pose prop. hyper-parameters Values\n\nImage size Mask: 480 \u00d7 640 Pose: 320 \u00d7 320 Softmax temperature \u03c4 0.05 Memory size M 6 Local attention radius r 12 k for k-nearest neighbor 10 Gaussian variance coefficient \u03b7 0.01\n\n\nC Association\n\n\nC.1 Association Algorithm\n\nMotion cues: object states and Kalman Filtering. We employ a Kalman filter with constant velocity and linear motion model to handle motion cues in algorithms of the association type. We assume a generic setting where the camera is not calibrated and the ego-motion is not known. The object states are defined in an eight-dimensional space (u, v, \u03b3, h,u,v,\u03b3,\u1e23), where (u, v) indicate the position bounding box center, h the bounding-box height and \u03b3 = h w the aspect ratio. The latter four dimensions represent the respective velocities of the first four terms.\n\nFor the sake of simplicity we convert mask representations to bounding boxes. Let the coordinates of \"in-mask\" pixels form a set {(x j , y j )|j = 1, ...N }, where N is the number of mask pixels. Then, the center of the corresponding bounding box is obtained by averaging these coordinates, as (u, v) = 1 N N j=1 (x j , y j ). We estimate the height of the bounding box as h = 2 N N j=1 y j \u2212 h 1 . This estimation is analogous to the one suggested in the continuous case [47]. Consider a rectangle with scale (2w, 2h) whose center locates at the origin of a 2D coordinate plane; by integrating over the points inside of the rectangle, we have 1 h h \u2212h y 1 dy = 2 h h 0 ydy = h. For objects represented as a pose, we first convert pose keypoints to masks following Appendix C.2, and then convert masks to boxes.\n\nFor each timestep, the Kalman Filter [38] predicts current states of existing tracklets. If a new detection is associated to a tracklet, then the state of the detection is used to update the tracklet state. If a tracklet is not associated with any detection, its state is simply predicted without correction.\n\nWe use the (squared) Mahalanobis distance [91] to measure the \"motion distance\" between a newly arrived detection and an existing tracklet. Let us project the state distribution of the i-th tracklet into the measurement space and denote mean and covariance as \u00b5 i and \u03a3 i , respectively. Then, the motion distance is given by\nc m i,j = (o j \u2212 \u00b5 i ) \u03a3 \u22121 (o j \u2212 \u00b5 i )(9)\nwhere o j indicates the observed (4D) state of the j-th detection. We observe that the Mahalanobis distance consistently outperforms Euclidean distance and IOU distance, likely thanks to the consideration of state estimation uncertainty. Using this metric also allows us to filter out unlikely matches by simply thresholding at 95% confidence interval [91]. We denote the filtering with an indicator function\nb i,j = 1[c m i,j > \u03b7].(10)\nThe threshold \u03b7 can be computed from the inverse X 2 distribution. In our case the degrees of freedom of the X 2 distribution is 4, so the threshold \u03b7 = 9.4877.\n\nAssociation algorithm. Algorithm 1 outlines the association procedure for a single timestamp. The algorithm takes as input a set of tracklets T = {1, ..., N } and detections D = {1, ..., M }. First, we predict the current states of the all tracklets using the Kalman Filter. Then we perform the main matching stage. In this stage, we compute a motion cost matrix C m using Eq 9, and compute an appearance cost matrix C a using the RSM metric described in Section 2.4,\nc a i,j = RSM(i, j)(11)\nThe final cost matrix is the linear combination of the two cost matrices C = \u03bbC a + (1 \u2212 \u03bb)C m . We set \u03bb = 0.99. A Hungarian solver takes the cost matrix C as input and outputs matches [x i,j ]. We then filter out unrealistic matches using Eq 10. For the remaining tracklets and detections which failed matching, we perform a second matching stage using IOU distance as the cost matrix. Remaining tracklets and detections are output by the association algorithm, further steps (described below) determine if a remaining tracklet should be terminated or if a new identity should be initialized from a remaining detection.\n\nTracklet termination and initialization. If a tracklet fails to be matched with a newly arrived detection with Algorithm 1, we mark it as inactive. To account for short occlusions, inactive tracklets can still be restored if they are found to be matching with a new detection. We record a \"lost age\" for each inactive tracklet. If the lost age is greater than a pre-given time, the tracklet would be removed from the current tracklet pool. The lost age is set to 1 second in our experiments.\n\nIf a detection fails to match existing tracklets with Algorithm 1, it could correspond to a new tracklet. However, this would result in the creation of frequent brief \"spurious\" tracklets, containing one detection only. To cope with this issue, similarly to [91] we only initialize a new tracklet if a new detection appears in two consecutive frames (and the IOU between consecutive boxes is at least 0.8).\n\n\nC.2 Pose-to-Mask Conversion\n\nGiven the key points' location of a target person, we convert the pose into a binary mask in two steps. First, the key points are connected to form a skeleton, where the width of each segment forming this skeleton is proportional to the body size with a linear coefficient \u03b7 p = 0.05, and the    body size is computed with Eq. 8. Second, we fill closed polygons inside the pose skeleton, since the parts inside the polygon usually belong to the target object.\n\n\nD Ablations for the Reconstruction Similarity Metric (RSM)\n\nIn Section 2.4 we claimed that the good tracking performance of UniTrack on association-type tasks is largely attributed to the proposed Reconstruction Similarity Metric (RSM). In this section, we provide results of several baseline methods in order to validate the effectiveness of RSM. These baseline are described below.\n\nCenter feature (CF). For a given observation feature d j \u2208 R s d j \u00d7C of a bounding box or a mask, we compute the location of its center of mass and extract the corresponding point feature (a single C-dim vector) as representation of this observation. Cosine similarity is computed to measure how likely two observations belong to the same identity. Using center feature to represent an object is a straightforward strategy, widely used in tracking tasks [112,88,108]. The benefit of CF is that it can handle objects in any observation format, e.g. boxes or masks, while the drawback is also obvious: it is a local feature and cannot represent the complete information of the object.\n\nGlobal feature (GF). For a given observation feature d j \u2208 R s d j \u00d7C , we concatenate the s dj point features and obtain a single global feature vector with length s dj C. Cosine similarity is computed to measure how likely two observations belong to the same identity. Note that only representations with fixed s dj are feasible in this case. For this reason, we only provide results for GF on the MOT task, where observations are bounding boxes that can be resized to a fixed size. The benefit of GF is that it preserve complete information of the observation, while the main drawback is that local features may not align between a pair of samples. Therefore, global feature is only applicable in cases where samples are aligned with pre-processing, e.g. in face recognition [25] Global-pooled feature (GPF). Similar to the global feature, but averaging is performed along the s dj dimension to obtain a single feature vector with length C. Cosine similarity then is computed to measure how likely it is that the two observations belong to the same identity. A large body of re-identification (ReID) approaches [73,113,72] employ global-pooled feature (on fully supervised learned feature maps). The benefit and drawback are similar to center feature.   Supervised ReID feature (ReID). For a given image cropped from a bounding box, we employ an strong, off-the-shelf person ReID model to extract a single feature vector with length C, and compute cosine similarity between observations. The model uses a ResNet-50 [33] architecture and is trained with the joint set of three widely-used datasets: Market-1501 [110], CUHK-03 [46], and DukeMTMC-ReID [67]. Using supervised ReID models to extract appearance features is widely used in existing multi-object tracking approaches [74,52,68]. Considering large amount of identity labels are leveraged in training, supervised ReID models usually show good association accuracy.\n\nNote that for CF, GF, GPF, and the proposed RSM, we employ the same appearance model (ImageNet pre-trained ResNet-18) for fair comparison. For a broad comparison, we provide results obtained with different detectors and on different datasets. We adopt the following detectors and test on MOT-16 [56] train split (listed with detection accuracy from low to high): DPM [26], Faster R-CNN [66] (FRCNN), SDP [100], and FairMOT [108].\n\nResults are shown in Table 4. We first apply the full association algorithm, i.e. using both appearance and motion cues. In this case (first half of the table), RSM consistently outperforms CF, GF, GPF baselines, and even surpasses the supervised ReID features in several cases, e.g. with FRCNN and FairMOT detectors. In the second half of the table, we show results in which only appearance cues are used, so that the difference between metrics (which are based on appearance) can be better emphasized. In this case, the gaps between different methods are more significant than in the previous case, and RSM still consistently outperforms CF, GF, and GPF. Furthermore, RSM also surpasses the strong supervised ReID feature with all detectors, except for DPM. This suggests that RSM can be an effective similarity metric for tasks that have association at their core.\n\nTo show the generality of the results, we also experiment on different datasets and different tasks. Table 5 shows comparisons on the MOT-20 [18] train split for the MOT task (box observations). The MOT-20 dataset is specialized for the extreme crowded person tracking scenario. Table 6 presents results on MOTS [80] train split for the MOTS task (mask observations). Note for the MOTS task, since the observations (masks) vary in size, it is not feasible to apply the GF strategy. Results show that the proposed RSM yields significantly higher IDF1 scores on both datasets.\n\n\nE More Tracking Tasks\n\nIn this section we present two more tasks that UniTrack can address.\n\nThe first task is human Pose Propagation on the JHMDB [37] dataset: each video contains a single person of interest, and the pose keypoints are provided in the first frame of the video only. The goal here is to predict the pose of the person throughout the video. Note that this is different from the previously mentioned PoseTrack task: PoseTrack mainly focuses on association between different identities, while in Pose Propagation we aim at propagating the pose of a single identity.\n\nResults are shown in Table 7. We report a higher result with ImageNet pre-trained ResNet-18 compared with in previous work [36,47] Table 9: Results on more SOT datasets. An ImageNet pre-trained representation with a ResNet-50 architecture is employed as the appearance model within UniTrack. \"TS sup.\" indicates whether the method requires task-specific supervision.\n\nThe second task is Video Instance Segmentation (VIS). The problem of VIS is similar to Multiple Object Tracking and Segmentation (MOTS), but its setup differs in the following aspects: first, the object categories are fairly diverse (40 different categories), while in MOTS objects are mostly persons and vehicles. This also requires the trackers tackling the VIS task to handle objects from different classes within the same scene. Second, the evaluation metrics are different. In MOTS, the MOT-like metrics (CLEAR [5], IDF-1/IDs, and HOTA [54]) are used, which implicitly encourages methods to focus on outputting temporally consistent trajectories. Instead, for VIS the evaluation metric is spatial-temporal mAP, a temporal extension of the vanilla mAP which is usually used in detection and segmentation tasks. The mAP metric significantly biases towards segmentation and classification accuracy in single frames, thus being less informative for evaluating \"tracking\" accuracy.\n\nResults on VIS task are shown in Table 8. We adopt an identical segmentation model to the one of MaskTrackRCNN [101], and observe only a 0.2 difference in mAP. For further comparison, we also provide results of two other association methods, OSMN [103] and DeepSORT [91], providing them with the same observations as used by UniTrack. Note how UniTrack boasts better accuracy than both methods (30.0 v.s. 27.5 and 26.1 mAP). Comparing with an state-of-the-art model, SipMask [9], our result is also comparable with \u22122.4 point mAP. We believe if equipped with more advanced single frame segmentation model, the mAP would be further improved.\n\n\nF SOT results on more datasets\n\nTo further validate the general validity of our experiments, we provide more results for the SOT task by testing on more recent datasets that contain large-scale and long-term videos.\n\nThe results in Table 9 show a very similar trend to the one already observed for OTB (Table 3e in the main text): For the SOT task, UniTrack with ImageNet features has comparable performance to the one of the recent LUDT+, which like UniTrack does not require task-specific supervision, but can only be used for SOT. Again, similarly to what was reported for OTB, UniTrack is outperformed by recent methods such as SiamRPN++. This is to be expected, as SiamRPN++ is specifically designed for SOT and trained in a supervised fashion on several large-scale video datasets.\n\n\nG Additional Correlation Studies\n\nIn Section 3.3 (main paper) we investigated the correlation between tracking performance and ImageNet \"linear probe\" accuracy for different SSL models. In this section, we provide more results and discussions by studying the correlation between tracking performance and several other downstream tasks when using the appearance model from the many SSL methods under consideration. For non-tracking tasks, we report numbers from [22] and plot them against tracking performance in Figure 8.\n\nWe report three tasks: surface normal estimation on the NYUv2 [69]   with performance measured in mAP (the higher the better); Semantic segmentation on ADE20k [111] dataset, with performance measured in mean IOU (the higher the better). In each subfigure, we plot the performance of five tracking tasks along the y-axes, and performance of the other task along the x-axes. Note that we actually use negative mean error for surface normal estimation, to represent accuracy. As in the main paper, we compute two types of correlation coefficient: Spearman' r and Pearson's \u03c1, and report them in the left bottom corner of each plot. Several interesting findings can be observed:\n\n(a) Correlation between tracking and surface normal prediction performance is fairly strong. Results are shown in Figure 8a. For instance, r = 0.70 for surface normal error v.s. MOT accuracy, and 0.56 for surface normal error v.s. PoseTrack accuracy. Interestingly, the behavior of SOT is in contrast with MOT and PoseTrack: SOT accuracy is moderately negative correlated (r = \u22120.50) with surface normal estimation accuracy. VOS presents a similar trend to the one of SOT, but with a lower correlation coefficient.\n\n(b) Object detection is moderately correlated with association-type tracking tasks. For object detection, we consider two setups: one is to freeze the representation and only train the additional classification/regression head; the other is to finetune the whole network in an end-to-end manner.\n\nResults are shown in Figure 8b and 8c respectively. In general, MOT and PoseTrack are moderately correlated with object detection under the frozen setting (r = 0.48 for MOT and and r = 0.42 for PoseTrack), and MOTS is moderately correlated with object detection under the finetune setting (r = 0.51). Propagation-type tasks are poorly correlated with object detection results under both settings (|\u03c1| < 0.10). We speculate that, in this case, positive correlation might be due to the fact that both object detection and association-type tracking require discriminative features at the level of the object.\n\n(c) Semantic segmentation is slightly negative correlated with tracking tasks. As can be observed in Figure 8d, correlation coefficients between segmentation accuracy and tracking performance are mildly negative. Among these results, VOS is the task that is most (negatively) correlated with segmentation, with r = \u22120.50. MOTS and PoseTrack are also mildly correlated, with r = \u22120.41 and r = \u22120.25 respectively. We speculate that negative correlation might be cause to the fact that tracking and segmentation require features with contradictory properties. Consider two different instances that belongs to the same category, i.e. two different pedestrian. For segmentation, the task requires pixel-wise classification, meaning that pixels inside the two instances should be equally classified into the same \"pedestrian\" class, thus their features should be similar (close to the class center). In contrast, for tracking tasks, it is required to distinguish different instances from the same class, otherwise a tracker would easily fail when objects overlap with each other. Therefore, point features inside the two different pedestrian are expected to be dissimilar.\n\nFigure 3 :\n3Overview of UniTrack.\n\nFigure 5 :\n5Reconstruction Similarity Metric (RSM): First, object-level features of existing tracklets and current detections are flattened and concatenated. Then, an affinity matrix between the two feature sets is computed.\n\nFigure 1 Figure 6 :\n16and 6 provides a high-level summary of the results by focusing on the ranking obtained by different SSL methods on the five tasks considered (each represented by a vertex in the radar-style plot). Several observations can be made: Visualized comparison between pre-trained video-based SSL models (Table 2). All models use ResNet-18.\n\nFigure 7 :\n7Tracking performance is poorly correlated with ImageNet accuracy. On the x-axes we plot ImageNet linear probe top-1 accuracy and on the y-axes the tracking performance on five tracking datasets. Correlation coefficients (Spearman's \u03c1 and Pearson's r) are shown in the left bottom of each plot.\n\nFigure 8 :\n8Correlation study between tracking tasks and other tasks for SSL models. On the y-axes we plot tracking performance, and on x-axes performance of the other tasks. Spearman's r and Pearson's \u03c1 are shown in the left bottom corner of each plot, indicating how the two axes are correlated.\n\nTable 1 :\n1Tracking performance of pre-trained image-based SSL models. All methods employ a ResNet-50.Representation \nSOT [93] \nVOS [65] \nMOT [56] \nMOTS [80] \nPoseTrack [2] \n\nAUCXCorr \u2191 AUCDCF \u2191 \nJ -mean\u2191 \nIDF1\u2191 \nHOTA\u2191 \nIDF1\u2191 \nHOTA\u2191 \nIDF1\u2191 \nIDs\u2193 \n\nRandom Init. \n16.0 / 18.2 \n36.1 / 32.1 \n33.0 / 36.7 \n18.4 / 14.6 \n20.2 / 12.9 \n34.5 / 33.1 \n39.9 / 37.6 \n52.8 / 50.5 \n65317 / 66230 \nImageNet-sup. \n55.0 / 46.2 \n61.8 / 52.6 \n58.4 / 46.7 \n74.8 / 74.5 \n62.7 / 62.1 \n67.6 / 68.6 \n69.8 / 70.5 \n72.7 / 73.2 \n6808 / 7024 \nColor. [81]+mem. 41.6 / 43.4 \n56.7 / 58.7 \n53.6 / 59.7 \n64.9 / 62.8 \n56.8 / 55.5 \n68.8 / 66.1 \n69.4 / 66.3 \n72.4 / 72.6 \n6850 / 6778 \nUVC [47] \n46.0 / 38.7 \n58.1 / 59.9 \n56.5 / 53.9 \n66.9 / 64.5 \n57.7 / 54.1 \n69.9 / 68.7 \n69.6 / 69.4 \n72.6 / 72.8 \n6843 / 6972 \nCRW [36] \n46.3 / 49.1 \n58.9 / 54.9 \n63.2 / 60.7 \n67.8 / 73.0 \n58.4 / 61.7 \n69.0 / 71.3 \n69.2 / 71.9 \n72.7 / 73.0 \n6799 / 6761 \n\n\n\nTable 2 :\n2Tracking performance of pre-trained video-based SSL models. All methods employ a ResNet-18. In the above two tables, we report results with [layer3 / layer4] features in each cell, and the best performance between the two is bolded. We use the bolded values to rank the models in each column, and visualise (column-wise) better performance with darker cell colors. Best results in each column are underlined.0.60 0.65 0.70 0.75 \n\nImageNet Top-1 Acc. \n\n.58 \n\n.59 \n\n.60 \n\n.61 \n\n.62 \n\n.63 \n\n.64 \n\nAUC/J-mean/IDF-1 \n\nr = -0.17 \n\n\u03c1 = -0.22 \n\nSOT \n\n0.60 0.65 0.70 0.75 \n\nImageNet Top-1 Acc. \n\n.58 \n\n.59 \n\n.60 \n\n.61 \n\n.62 \n\n.63 \n\nr = -0.38 \n\n\u03c1 = -0.36 \n\nVOS \n\n0.60 0.65 0.70 0.75 \n\nImageNet Top-1 Acc. \n\n.68 \n\n.70 \n\n.72 \n\n.74 \n\n.76 \n\nr = 0.10 \n\n\u03c1 = 0.26 \n\nMOT \n\n0.60 0.65 0.70 0.75 \n\nImageNet Top-1 Acc. \n\n.66 \n\n.67 \n\n.68 \n\n.69 \n\n.70 \n\n.71 \n\nr = 0.01 \n\n\u03c1 = 0.16 \n\nMOTS \n\n0.60 0.65 0.70 0.75 \n\nImageNet Top-1 Acc. \n\n.72 \n\n.73 \n\n.74 \n\n.75 \n\nr = 0.20 \n\n\u03c1 = 0.12 \n\nPoseTrack \n\nsupervised \nInsDis \nMoCo-v1 \nPCL-v1 \nPIRL \nPCL-v2 \nSimCLR-v1 \nMoCo-v2 \n\nSimCLR-v2 \nSeLa-v2 \nInfoMin \nBYOL \nDeepCluster-v2 \nSwAV \nBarlowTwins \nDetCo \n\n\n\nTable 1 ,\n1one can notice how the \n\n\nTable 3 :\n3Comparison with task-tailored unsupervised and supervised methods on five typical tracking tasks. \u2020 indicates methods using identical observations.\n\n\npoints. Considering that LUDT+ adopts an additional online template update mechanism[16] while ours does not, we believe the gap could be closed. In VOS, existing unsupervised methods are usually trained on video datasets3e). For SOT, \nUniTrack with a DCF head [84] outperforms UDT [82] (a strong recent method) by 2.4 AUC points, \nwhile it is surpassed by LUDT+ [83] by 2.1 \n\n\nThis work was supported by the National Natural Science Foundation of China under Grant No. 61771288, Cross-Media Intelligent Technology Project of Beijing National Research Center for Information Science and Technology (BNRist) under Grant No. BNR2019TD01022 and the research fund under Grant No. 2019GQG0001 from the Institute for Guo Qiang, Tsinghua University.This work was also supported by the EPSRC grant: Turing AI Fellowship: EP/W002981/1, EP-\nSRC/MURI grant EP/N019474/1. We would also like to thank the Royal Academy of Engineering \nand FiveAI. \n\n\n\nA single run of the evaluation on five tasks takes about 2 hours in a Titan Xp GPU.Task \nSOT \nVOS \nMOT \nMOTS \nPoseTrack \nPoseProp \nVIS \n\nDataset \nOTB [93] DAVIS 2017 [65] \nMOT 16 [56] \nMOTS [80] \nPoseTrack 2107 [2] \nJHMDB [37] \nYoutubeVIS [101] \n\nMetrics \nAUC \nJ -mean \nIDF1 \nMOTA \n\nIDF1 \nsMOTA \n\nIDF1 \nMOTA \nID-switch (IDs) \n\nPCK \nmAP \n\n\n\nTable 4 :\n4Comparisonbetween different similarity metrics for association, tested on MOT-16 train split. We \nprovide results that (1) use motion cues and (2) discard motion cues. The best results are bolded and the second \nbest results are underlined. \n\nMethods IDF1 IDs MOTA \n\nCF \n38.6 6384 \n41.8 \nGPF \n38.3 6245 \n41.8 \nGF \n39.3 5858 \n41.8 \nReID \n39.1 6442 \n41.7 \nRSM \n41.3 5552 \n41.6 \n\n\n\nTable 5 :\n5Comparisonbetween different similarity \nmetrics for association, tested on MOT-20 [18] \nwith the provided detector. \n\nMethods IDF1 IDs sMOTSA \n\nCF \n62.8 1529 \n80.7 \nGPF \n60.7 1071 \n82.4 \nRSM \n66.5 \n808 \n83.4 \n\n\n\nTable 6 :\n6Comparisonbetween different similarity \nmetrics for association, tested on MOTS [80] train \nsplit based on the segmentation masks provided by \nthe COSTAst [1] tracker. \n\n\n\nTable 7 :\n7Resultsof pose propagation on JH-\nMDB [37] dataset. I18 refers to using ImageNet \npre-trained ResNet-18 as the appearance model. \n\nMethods \nmAP\u2191 \n\nFEELVOS [79] \n26.9 \nSipMask [9] \n32.5 \n\nOSMN  \u2020 [103] \n27.5 \nDeepSORT  \u2020 [91] \n26.1 \nMTRCNN  \u2020 [101] \n30.3 \nUniTrack  \u2020 \n30.1 \n\n\n\nTable 8 :\n8VIS results@YoutubeVIS[101] val split.\u2020 indicates methods using the same observations \n(segmentation masks in every single frames). \n\n\n\n\n(58.3 v.s. 53.8 PCK@0.1). With this result, we observe the best self-supervised method CRW[36] does not beat the ImageNet pre-trained representation by a significant margin (only +0.7 PCK@1). This again validates our second finding in Section 3.2: a vanilla ImageNet-trained representation is surprisingly effective.Method \n\nTS sup. \nTrackingNet [59] \nTC-128 [49] \nTLP [58] \nLaSOT [24] \nOxUvA [77] \n\nSucc. \nPrec. \nSucc. \nPrec. \nSucc. \nPrec. \nSucc. \nPrec. \nMaxGM \n\nKCF [35] \nN \n41.9 \n44.7 \n38.7 \n54.9 \n8.4 \n6.3 \n17.8 \n-\n-\nECO [16] \nN \n56.1 \n48.9 \n-\n-\n20.2 \n21.2 \n32.4 \n30.1 \n0.314 \nStaple [6] \nN \n-\n-\n-\n-\n-\n-\n-\n-\n0.261 \nBACF [39] \nN \n-\n-\n-\n-\n-\n-\n-\n-\n0.281 \nSiamFC [7] \nY \n57.1 \n66.3 \n50.3 \n68.8 \n23.5 \n28.4 \n33.6 \n33.9 \n0.313 \nCFNet [76] \nY \n53.3 \n57.8 \n-\n-\n-\n-\n27.5 \n-\n-\nSiamRPN [44] \nY \n-\n-\n-\n-\n-\n-\n-\n-\n-\nSiamRPN++ [43] \nY \n73.3 \n69.4 \n-\n-\n-\n-\n49.6 \n49.1 \n-\nLUDT [83] \nN \n46.9 \n54.3 \n51.5 \n67.1 \n-\n-\n26.2 \n-\n-\nLUDT+ [83] \nN \n49.5 \n56.3 \n55.2 \n72.5 \n-\n-\n30.5 \n-\n-\nUnTrack \nN \n59.1 \n51.2 \n54.5 \n73.1 \n25.4 \n23.2 \n35.1 \n32.6 \n0.334 \n\n\n\n\ndataset, where the mean angular error is used as the evaluation metric (the lower the better); Object detection on Pascal VOC[23], (-) Surf. Norm. Mean Err. Correlation between tracking tasks and surface normal estimation on NYUv2[69] dataset. Correlation between tracking tasks and object detection (frozen backbone) on Pascal VOC[23] dataset. Correlation between tracking tasks and object detection (finetune) on Pascal VOC[23] dataset.AUC/J-mean/IDF-1 Correlation between tracking tasks and semantic segmentation on ADE20k[111] dataset.40 \n\n36 \n32 \n28 \n\n(-) Surf. Norm. Mean Err. \n\n.58 \n\n.60 \n\n.62 \n\n.64 \n\nAUC/J-mean/IDF-1 \n\nr = -0.50 \n\n\u03c1 = -0.52 \n\nSOT \n\n40 \n36 \n32 \n28 \n\n(-) Surf. Norm. Mean Err. \n\n.58 \n\n.59 \n\n.60 \n\n.61 \n\n.62 \n\n.63 \n\nr = -0.08 \n\n\u03c1 = -0.12 \n\nVOS \n\n40 \n36 \n32 \n28 \n\n(-) Surf. Norm. Mean Err. \n\n.68 \n\n.70 \n\n.72 \n\n.74 \n\n.76 \n\nr = 0.70 \n\n\u03c1 = 0.58 \n\nMOT \n\n40 \n36 \n32 \n28 \n\n(-) Surf. Norm. Mean Err. \n\n.66 \n\n.67 \n\n.68 \n\n.69 \n\n.70 \n\n.71 \n\nr = 0.29 \n\n\u03c1 = 0.13 \n\nMOTS \n\n40 \n36 \n32 \n28 \n\n.72 \n\n.73 \n\n.74 \n\n.75 \n\nr = 0.56 \n\n\u03c1 = 0.31 \n\nPoseTrack \n\nsupervised \nInsDis \nMoCo-v1 \nPCL-v1 \nPIRL \nPCL-v2 \nSimCLR-v1 \n\nMoCo-v2 \nSimCLR-v2 \nSeLa-v2 \nInfoMin \nBYOL \nDeepCluster-v2 \nSwAV \n\n(a) 0.50 \n0.52 \n0.54 \n\nDet mAP (Fronzen) \n\n.58 \n\n.60 \n\n.62 \n\n.64 \n\nAUC/J-mean/IDF-1 \n\nr = -0.31 \n\n\u03c1 = -0.09 \n\nSOT \n\n0.50 \n0.52 \n0.54 \n\nDet mAP (Fronzen) \n\n.58 \n\n.59 \n\n.60 \n\n.61 \n\n.62 \n\n.63 \n\nr = -0.05 \n\n\u03c1 = 0.04 \n\nVOS \n\n0.50 \n0.52 \n0.54 \n\nDet mAP (Fronzen) \n\n.68 \n\n.70 \n\n.72 \n\n.74 \n\n.76 \n\nr = 0.48 \n\n\u03c1 = 0.53 \n\nMOT \n\n0.50 \n0.52 \n0.54 \n\nDet mAP (Fronzen) \n\n.66 \n\n.67 \n\n.68 \n\n.69 \n\n.70 \n\n.71 \n\nr = 0.13 \n\n\u03c1 = 0.17 \n\nMOTS \n\n0.50 \n0.52 \n0.54 \n\nDet mAP (Fronzen) \n\n.72 \n\n.73 \n\n.74 \n\n.75 \n\nr = 0.42 \n\n\u03c1 = 0.33 \n\nPoseTrack \n\nsupervised \nInsDis \nMoCo-v1 \nPCL-v1 \nPIRL \nPCL-v2 \nSimCLR-v1 \n\nMoCo-v2 \nSimCLR-v2 \nSeLa-v2 \nInfoMin \nBYOL \nDeepCluster-v2 \nSwAV \n\n(b) 0.46 0.48 0.50 0.52 0.54 \n\nDet mAP (Finetune) \n\n.58 \n\n.60 \n\n.62 \n\n.64 \n\nAUC/J-mean/IDF-1 \n\nr = -0.12 \n\n\u03c1 = -0.02 \n\nSOT \n\n0.46 0.48 0.50 0.52 0.54 \n\nDet mAP (Finetune) \n\n.58 \n\n.59 \n\n.60 \n\n.61 \n\n.62 \n\n.63 \n\nr = 0.01 \n\n\u03c1 = -0.08 \n\nVOS \n\n0.46 0.48 0.50 0.52 0.54 \n\nDet mAP (Finetune) \n\n.68 \n\n.70 \n\n.72 \n\n.74 \n\n.76 \n\nr = 0.28 \n\n\u03c1 = 0.51 \n\nMOT \n\n0.46 0.48 0.50 0.52 0.54 \n\nDet mAP (Finetune) \n\n.66 \n\n.67 \n\n.68 \n\n.69 \n\n.70 \n\n.71 \n\nr = 0.51 \n\n\u03c1 = 0.53 \n\nMOTS \n\n0.46 0.48 0.50 0.52 0.54 \n\nDet mAP (Finetune) \n\n.72 \n\n.73 \n\n.74 \n\n.75 \n\nr = -0.02 \n\n\u03c1 = -0.22 \n\nPoseTrack \n\nsupervised \nInsDis \nMoCo-v1 \nPCL-v1 \nPIRL \nPCL-v2 \nSimCLR-v1 \n\nMoCo-v2 \nSimCLR-v2 \nSeLa-v2 \nInfoMin \nBYOL \nDeepCluster-v2 \nSwAV \n\n(c) 0.26 \n0.28 \n0.30 \n\nSeg. mean IOU \n\n.58 \n\n.60 \n\n.62 \n\n.64 \n\nr = -0.17 \n\n\u03c1 = -0.05 \n\nSOT \n\n0.26 \n0.28 \n0.30 \n\nSeg. mean IOU \n\n.58 \n\n.59 \n\n.60 \n\n.61 \n\n.62 \n\n.63 \n\nr = -0.50 \n\n\u03c1 = -0.35 \n\nVOS \n\n0.26 \n0.28 \n0.30 \n\nSeg. mean IOU \n\n.68 \n\n.70 \n\n.72 \n\n.74 \n\n.76 \n\nr = -0.11 \n\n\u03c1 = 0.18 \n\nMOT \n\n0.26 \n0.28 \n0.30 \n\nSeg. mean IOU \n\n.66 \n\n.67 \n\n.68 \n\n.69 \n\n.70 \n\n.71 \n\nr = -0.41 \n\n\u03c1 = -0.28 \n\nMOTS \n\n0.26 \n0.28 \n0.30 \n\nSeg. mean IOU \n\n.72 \n\n.73 \n\n.74 \n\n.75 \n\nr = -0.25 \n\n\u03c1 = -0.50 \n\nPoseTrack \n\nsupervised \nInsDis \nMoCo-v1 \nPCL-v1 \nPIRL \nPCL-v2 \nSimCLR-v1 \n\nMoCo-v2 \nSimCLR-v2 \nSeLa-v2 \nInfoMin \nBYOL \nDeepCluster-v2 \nSwAV \n\n(d) \nNote this corresponds to the ground-truth initialisation when t = 1, and to the latest prediction otherwise.\nHere we use a numpy-style matrix slicing notation to represent a submatrix, i.e. A[i : j, k : l] indicates a submatrix of A with row indices ranging from i to j and column indices ranging from k to l.\n\n. Costa_St Tracker, Costa_st tracker. https://motchallenge.net/method/MOTS=87&chl=17.\n\nPosetrack: A benchmark for human pose estimation and tracking. Mykhaylo Andriluka, Umar Iqbal, Eldar Insafutdinov, Leonid Pishchulin, Anton Milan, Juergen Gall, Bernt Schiele, CVPR. Mykhaylo Andriluka, Umar Iqbal, Eldar Insafutdinov, Leonid Pishchulin, Anton Milan, Juergen Gall, and Bernt Schiele. Posetrack: A benchmark for human pose estimation and tracking. In CVPR, 2018.\n\nSelf-labelling via simultaneous clustering and representation learning. Yuki Markus Asano, Christian Rupprecht, Andrea Vedaldi, ICLR. Yuki Markus Asano, Christian Rupprecht, and Andrea Vedaldi. Self-labelling via simultaneous clustering and representation learning. In ICLR, 2020.\n\nStem-seg: Spatiotemporal embeddings for instance segmentation in videos. Ali Athar, Sabarinath Mahadevan, Aljo\u0161a O\u0161ep, Laura Leal-Taix\u00e9, Bastian Leibe, ECCV. Ali Athar, Sabarinath Mahadevan, Aljo\u0161a O\u0161ep, Laura Leal-Taix\u00e9, and Bastian Leibe. Stem-seg: Spatio- temporal embeddings for instance segmentation in videos. In ECCV, 2020.\n\nEvaluating multiple object tracking performance: the clear mot metrics. Keni Bernardin, Rainer Stiefelhagen, EURASIP Journal on Image and Video Processing. Keni Bernardin and Rainer Stiefelhagen. Evaluating multiple object tracking performance: the clear mot metrics. EURASIP Journal on Image and Video Processing, 2008:1-10, 2008.\n\nStaple: Complementary learners for real-time tracking. Luca Bertinetto, Jack Valmadre, Stuart Golodetz, Ondrej Miksik, Philip Hs Torr, CVPR. Luca Bertinetto, Jack Valmadre, Stuart Golodetz, Ondrej Miksik, and Philip HS Torr. Staple: Comple- mentary learners for real-time tracking. In CVPR, 2016.\n\nFullyconvolutional siamese networks for object tracking. Luca Bertinetto, Jack Valmadre, Joao F Henriques, Andrea Vedaldi, Philip Hs Torr, ECCV workshops. Luca Bertinetto, Jack Valmadre, Joao F Henriques, Andrea Vedaldi, and Philip HS Torr. Fully- convolutional siamese networks for object tracking. In ECCV workshops, 2016.\n\nLanguage models are few-shot learners. Benjamin Tom B Brown, Nick Mann, Melanie Ryder, Jared Subbiah, Prafulla Kaplan, Arvind Dhariwal, Pranav Neelakantan, Girish Shyam, Amanda Sastry, Askell, arXiv:2005.14165Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv:2005.14165, 2020.\n\nSipmask: Spatial information preservation for fast image and video instance segmentation. Jiale Cao, Hisham Rao Muhammad Anwer, Cholakkal, Yanwei Fahad Shahbaz Khan, Ling Pang, Shao, ECCVJiale Cao, Rao Muhammad Anwer, Hisham Cholakkal, Fahad Shahbaz Khan, Yanwei Pang, and Ling Shao. Sipmask: Spatial information preservation for fast image and video instance segmentation. ECCV, 2020.\n\nDeep clustering for unsupervised learning of visual features. Mathilde Caron, Piotr Bojanowski, Armand Joulin, Matthijs Douze, ECCV. Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised learning of visual features. In ECCV, 2018.\n\nUnsupervised learning of visual features by contrasting cluster assignments. Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, Armand Joulin, In NeurIPS. Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsu- pervised learning of visual features by contrasting cluster assignments. In NeurIPS, 2018.\n\nQuo vadis, action recognition? a new model and the kinetics dataset. Joao Carreira, Andrew Zisserman, CVPR. Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In CVPR, 2017.\n\nA simple framework for contrastive learning of visual representations. Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. 2020.\n\nBig selfsupervised models are strong semi-supervised learners. Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, Geoffrey Hinton, arXiv:2006.10029Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton. Big self- supervised models are strong semi-supervised learners. arXiv:2006.10029, 2020.\n\nXinlei Chen, Haoqi Fan, arXiv:2003.04297Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv:2003.04297, 2020.\n\nEco: Efficient convolution operators for tracking. Martin Danelljan, Goutam Bhat, Michael Fahad Shahbaz Khan, Felsberg, CVPR. Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, and Michael Felsberg. Eco: Efficient convolution operators for tracking. In CVPR, 2017.\n\nMotchallenge: A benchmark for single-camera multiple target tracking. Patrick Dendorfer, Aljosa Osep, Anton Milan, Konrad Schindler, Daniel Cremers, Ian Reid, Stefan Roth, Laura Leal-Taix\u00e9, International Journal of Computer Vision. 1294Patrick Dendorfer, Aljosa Osep, Anton Milan, Konrad Schindler, Daniel Cremers, Ian Reid, Stefan Roth, and Laura Leal-Taix\u00e9. Motchallenge: A benchmark for single-camera multiple target tracking. International Journal of Computer Vision, 129(4):845-881, 2021.\n\nPatrick Dendorfer, Hamid Rezatofighi, Anton Milan, Javen Shi, Daniel Cremers, Ian Reid, Stefan Roth, Konrad Schindler, Laura Leal-Taix\u00e9, arXiv:2003.09003Mot20: A benchmark for multi object tracking in crowded scenes. Patrick Dendorfer, Hamid Rezatofighi, Anton Milan, Javen Shi, Daniel Cremers, Ian Reid, Stefan Roth, Konrad Schindler, and Laura Leal-Taix\u00e9. Mot20: A benchmark for multi object tracking in crowded scenes. arXiv:2003.09003, 2020.\n\nImagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, CVPR. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009.\n\nBert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. 2018.\n\nFlownet: Learning optical flow with convolutional networks. Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas, Vladimir Golkov, Patrick Van Der, Daniel Smagt, Thomas Cremers, Brox, ICCV. Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas, Vladimir Golkov, Patrick Van Der Smagt, Daniel Cremers, and Thomas Brox. Flownet: Learning optical flow with convolutional networks. In ICCV, 2015.\n\nHow well do self-supervised models transfer? In CVPR. Linus Ericsson, Henry Gouk, Timothy M Hospedales, Linus Ericsson, Henry Gouk, and Timothy M Hospedales. How well do self-supervised models transfer? In CVPR, 2021.\n\nThe pascal visual object classes (voc) challenge. Mark Everingham, Luc Van Gool, K I Christopher, John Williams, Andrew Winn, Zisserman, International journal of computer vision. 882Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. International journal of computer vision, 88(2):303-338, 2010.\n\nLasot: A high-quality benchmark for large-scale single object tracking. Liting Heng Fan, Fan Lin, Peng Yang, Ge Chu, Sijia Deng, Hexin Yu, Yong Bai, Chunyuan Xu, Haibin Liao, Ling, CVPR. Heng Fan, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia Yu, Hexin Bai, Yong Xu, Chunyuan Liao, and Haibin Ling. Lasot: A high-quality benchmark for large-scale single object tracking. In CVPR, 2019.\n\nCross-domain similarity learning for face recognition in unseen domains. Masoud Faraki, Xiang Yu, Yi-Hsuan Tsai, Yumin Suh, Manmohan Chandraker, arXiv:2103.07503Masoud Faraki, Xiang Yu, Yi-Hsuan Tsai, Yumin Suh, and Manmohan Chandraker. Cross-domain similarity learning for face recognition in unseen domains. arXiv:2103.07503, 2021.\n\nCascade object detection with deformable part models. F Pedro, Ross B Felzenszwalb, David Girshick, Mcallester, CVPR. Pedro F Felzenszwalb, Ross B Girshick, and David McAllester. Cascade object detection with deformable part models. In CVPR, 2010.\n\nLearning to track instances without video annotations. Yang Fu, Sifei Liu, Umar Iqbal, Shalini De, Humphrey Mello, Jan Shi, Kautz, CVPR. 2021Yang Fu, Sifei Liu, Umar Iqbal, Shalini De Mello, Humphrey Shi, and Jan Kautz. Learning to track instances without video annotations. In CVPR, 2021.\n\nScaling and benchmarking self-supervised visual representation learning. Priya Goyal, Dhruv Mahajan, Abhinav Gupta, Ishan Misra, arXiv:1905.01235Priya Goyal, Dhruv Mahajan, Abhinav Gupta, and Ishan Misra. Scaling and benchmarking self-supervised visual representation learning. arXiv:1905.01235, 2019.\n\nBootstrap your own latent: A new approach to self-supervised learning. Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, H Pierre, Elena Richemond, Carl Buchatskaya, Bernardo Doersch, Zhaohan Daniel Avila Pires, Mohammad Gheshlaghi Guo, Azar, arXiv:2006.07733Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning. arXiv:2006.07733, 2020.\n\nMulti-domain pose network for multi-person pose estimation and tracking. Hengkai Guo, Tang Tang, Guozhong Luo, Riwei Chen, Yongchen Lu, Linfu Wen, ECCV, workshop. Hengkai Guo, Tang Tang, Guozhong Luo, Riwei Chen, Yongchen Lu, and Linfu Wen. Multi-domain pose network for multi-person pose estimation and tracking. In ECCV, workshop, 2018.\n\nMat: Motion-aware multi-object tracking. Shoudong Han, Piao Huang, Hongwei Wang, En Yu, Donghaisheng Liu, Xiaofeng Pan, Jun Zhao, arXiv:2009.04794Shoudong Han, Piao Huang, Hongwei Wang, En Yu, Donghaisheng Liu, Xiaofeng Pan, and Jun Zhao. Mat: Motion-aware multi-object tracking. arXiv:2009.04794, 2020.\n\nMomentum contrast for unsupervised visual representation learning. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick, CVPR. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In CVPR, 2020.\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, CVPR. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.\n\nDeep spatial feature reconstruction for partial person re-identification: Alignment-free approach. Lingxiao He, Jian Liang, Haiqing Li, Zhenan Sun, CVPR. Lingxiao He, Jian Liang, Haiqing Li, and Zhenan Sun. Deep spatial feature reconstruction for partial person re-identification: Alignment-free approach. In CVPR, 2018.\n\nHigh-speed tracking with kernelized correlation filters. F Jo\u00e3o, Rui Henriques, Pedro Caseiro, Jorge Martins, Batista, IEEE TPAMI. Jo\u00e3o F Henriques, Rui Caseiro, Pedro Martins, and Jorge Batista. High-speed tracking with kernelized correlation filters. IEEE TPAMI, 2014.\n\nSpace-time correspondence as a contrastive random walk. Allan Jabri, Andrew Owens, Alexei A Efros, NeurIPS. Allan Jabri, Andrew Owens, and Alexei A Efros. Space-time correspondence as a contrastive random walk. In NeurIPS, 2020.\n\nTowards understanding action recognition. H Jhuang, J Gall, S Zuffi, C Schmid, M J Black, ICCV. H. Jhuang, J. Gall, S. Zuffi, C. Schmid, and M. J. Black. Towards understanding action recognition. In ICCV, 2013.\n\nA new approach to linear filtering and prediction problems. Rudolph Emil , Kalman , Rudolph Emil Kalman. A new approach to linear filtering and prediction problems. 1960.\n\nLearning background-aware correlation filters for visual tracking. Ashton Hamed Kiani Galoogahi, Simon Fagg, Lucey, ICCV. Hamed Kiani Galoogahi, Ashton Fagg, and Simon Lucey. Learning background-aware correlation filters for visual tracking. In ICCV, 2017.\n\nA novel performance evaluation methodology for single-target trackers. Matej Kristan, Jiri Matas, Ale\u0161 Leonardis, Tom\u00e1\u0161 Voj\u00ed\u0159, Roman Pflugfelder, Gustavo Fernandez, Georg Nebehay, Fatih Porikli, Luka\u010dehovin , IEEE TPAMI. Matej Kristan, Jiri Matas, Ale\u0161 Leonardis, Tom\u00e1\u0161 Voj\u00ed\u0159, Roman Pflugfelder, Gustavo Fernandez, Georg Nebehay, Fatih Porikli, and Luka\u010cehovin. A novel performance evaluation methodology for single-target trackers. IEEE TPAMI, 2016.\n\nThe hungarian method for the assignment problem. Harold W Kuhn, Naval research logistics quarterly. 2Harold W Kuhn. The hungarian method for the assignment problem. Naval research logistics quarterly, 2(1-2):83-97, 1955.\n\nMast: A memory-augmented self-supervised tracker. Zihang Lai, Erika Lu, Weidi Xie, CVPR. Zihang Lai, Erika Lu, and Weidi Xie. Mast: A memory-augmented self-supervised tracker. In CVPR, 2020.\n\nSiamrpn++: Evolution of siamese visual tracking with very deep networks. Bo Li, Wei Wu, Qiang Wang, Fangyi Zhang, Junliang Xing, Junjie Yan, CVPR. Bo Li, Wei Wu, Qiang Wang, Fangyi Zhang, Junliang Xing, and Junjie Yan. Siamrpn++: Evolution of siamese visual tracking with very deep networks. In CVPR, 2019.\n\nHigh performance visual tracking with siamese region proposal network. Bo Li, Junjie Yan, Wei Wu, Zheng Zhu, Xiaolin Hu, CVPR. Bo Li, Junjie Yan, Wei Wu, Zheng Zhu, and Xiaolin Hu. High performance visual tracking with siamese region proposal network. In CVPR, 2018.\n\nPrototypical contrastive learning of unsupervised representations. Junnan Li, Pan Zhou, Caiming Xiong, Richard Socher, C H Steven, Hoi, ICLR. 2021Junnan Li, Pan Zhou, Caiming Xiong, Richard Socher, and Steven CH Hoi. Prototypical contrastive learning of unsupervised representations. In ICLR, 2021.\n\nDeepreid: Deep filter pairing neural network for person re-identification. Wei Li, Rui Zhao, Tong Xiao, Xiaogang Wang, CVPR. Wei Li, Rui Zhao, Tong Xiao, and Xiaogang Wang. Deepreid: Deep filter pairing neural network for person re-identification. In CVPR, 2014.\n\nJoint-task self-supervised learning for temporal correspondence. Xueting Li, Sifei Liu, Xiaolong Shalini De Mello, Jan Wang, Ming-Hsuan Kautz, Yang, NeurIPS. Xueting Li, Sifei Liu, Shalini De Mello, Xiaolong Wang, Jan Kautz, and Ming-Hsuan Yang. Joint-task self-supervised learning for temporal correspondence. In NeurIPS, 2019.\n\nRethinking the competition between detection and reid in multi-object tracking. Chao Liang, Zhipeng Zhang, Yi Lu, Xue Zhou, Bing Li, Xiyong Ye, Jianxiao Zou, arXiv:2010.12138Chao Liang, Zhipeng Zhang, Yi Lu, Xue Zhou, Bing Li, Xiyong Ye, and Jianxiao Zou. Rethinking the competition between detection and reid in multi-object tracking. arXiv:2010.12138, 2020.\n\nEncoding color information for visual tracking: Algorithms and benchmark. Pengpeng Liang, Erik Blasch, Haibin Ling, IEEE TIP. Pengpeng Liang, Erik Blasch, and Haibin Ling. Encoding color information for visual tracking: Algo- rithms and benchmark. IEEE TIP, 2015.\n\nMicrosoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, C Lawrence Zitnick, ECCV. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014.\n\nSelf-emd: Self-supervised object detection without imagenet. Songtao Liu, Zeming Li, Jian Sun, arXiv:2011.13677Songtao Liu, Zeming Li, and Jian Sun. Self-emd: Self-supervised object detection without imagenet. arXiv:2011.13677, 2020.\n\nReal-time multiple people tracking with deeply learned candidate selection and person re-identification. Chen Long, Ai Haizhou, Zhuang Zijie, Shang Chong, ICME. Chen Long, Ai Haizhou, Zhuang Zijie, and Shang Chong. Real-time multiple people tracking with deeply learned candidate selection and person re-identification. In ICME, 2018.\n\nDo convnets learn correspondence? In NeurIPS. Jonathan Long, Ning Zhang, Trevor Darrell, Jonathan Long, Ning Zhang, and Trevor Darrell. Do convnets learn correspondence? In NeurIPS, 2014.\n\nHota: A higher order metric for evaluating multi-object tracking. Jonathon Luiten, Aljosa Osep, Patrick Dendorfer, Philip Torr, Andreas Geiger, Laura Leal-Taix\u00e9, Bastian Leibe, 2020Jonathon Luiten, Aljosa Osep, Patrick Dendorfer, Philip Torr, Andreas Geiger, Laura Leal-Taix\u00e9, and Bastian Leibe. Hota: A higher order metric for evaluating multi-object tracking. IJCV, 2020.\n\nD3s-a discriminative single shot segmentation tracker. Alan Lukezic, Jiri Matas, Matej Kristan, CVPR. Alan Lukezic, Jiri Matas, and Matej Kristan. D3s-a discriminative single shot segmentation tracker. In CVPR, 2020.\n\nAnton Milan, Laura Leal-Taix\u00e9, Ian Reid, Stefan Roth, Konrad Schindler, arXiv:1603.00831Mot16: A benchmark for multi-object tracking. Anton Milan, Laura Leal-Taix\u00e9, Ian Reid, Stefan Roth, and Konrad Schindler. Mot16: A benchmark for multi-object tracking. arXiv:1603.00831, 2016.\n\nSelf-supervised learning of pretext-invariant representations. Ishan Misra, Laurens Van Der Maaten, CVPR. Ishan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant representations. In CVPR, 2020.\n\nLong-term visual object tracking benchmark. Abhinav Moudgil, Vineet Gandhi, ACCV. Abhinav Moudgil and Vineet Gandhi. Long-term visual object tracking benchmark. In ACCV, 2018.\n\nTrackingnet: A large-scale dataset and benchmark for object tracking in the wild. Matthias Muller, Adel Bibi, Silvio Giancola, Salman Alsubaihi, Bernard Ghanem, In ECCV. Matthias Muller, Adel Bibi, Silvio Giancola, Salman Alsubaihi, and Bernard Ghanem. Trackingnet: A large-scale dataset and benchmark for object tracking in the wild. In ECCV, 2018.\n\nLighttrack: A generic framework for online top-down human pose tracking. Guanghan Ning, Heng Huang, CVPR, workshop. Guanghan Ning and Heng Huang. Lighttrack: A generic framework for online top-down human pose tracking. In CVPR, workshop, 2020.\n\nA top-down approach to articulated human pose estimation and tracking. Guanghan Ning, Ping Liu, Xiaochuan Fan, Chi Zhang, ECCV, workshop. Guanghan Ning, Ping Liu, Xiaochuan Fan, and Chi Zhang. A top-down approach to articulated human pose estimation and tracking. In ECCV, workshop, 2018.\n\nVideo object segmentation using space-time memory networks. Joon-Young Seoung Wug Oh, Ning Lee, Seon Joo Xu, Kim, ICCV. Seoung Wug Oh, Joon-Young Lee, Ning Xu, and Seon Joo Kim. Video object segmentation using space-time memory networks. In ICCV, 2019.\n\nTubetk: Adopting tubes to track multi-object in a one-step training model. Bo Pang, Yizhuo Li, Yifan Zhang, Muchen Li, Cewu Lu, CVPR. Bo Pang, Yizhuo Li, Yifan Zhang, Muchen Li, and Cewu Lu. Tubetk: Adopting tubes to track multi-object in a one-step training model. In CVPR, 2020.\n\nChained-tracker: Chaining paired attentive regression results for end-to-end joint multiple-object detection and tracking. Jinlong Peng, Changan Wang, Fangbin Wan, Yang Wu, Yabiao Wang, Ying Tai, Chengjie Wang, Jilin Li, Feiyue Huang, Yanwei Fu, ECCV. Jinlong Peng, Changan Wang, Fangbin Wan, Yang Wu, Yabiao Wang, Ying Tai, Chengjie Wang, Jilin Li, Feiyue Huang, and Yanwei Fu. Chained-tracker: Chaining paired attentive regression results for end-to-end joint multiple-object detection and tracking. In ECCV, 2020.\n\nA benchmark dataset and evaluation methodology for video object segmentation. F Perazzi, J Pont-Tuset, B Mcwilliams, L Van Gool, M Gross, A Sorkine-Hornung, CVPR. F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool, M. Gross, and A. Sorkine-Hornung. A benchmark dataset and evaluation methodology for video object segmentation. In CVPR, 2016.\n\nFaster r-cnn: Towards real-time object detection with region proposal networks. Kaiming Shaoqing Ren, Ross He, Jian Girshick, Sun, ICCV. Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In ICCV, 2015.\n\nPerformance measures and a data set for multi-target, multi-camera tracking. Ergys Ristani, Francesco Solera, Roger Zou, Rita Cucchiara, Carlo Tomasi, ECCV. Ergys Ristani, Francesco Solera, Roger Zou, Rita Cucchiara, and Carlo Tomasi. Performance measures and a data set for multi-target, multi-camera tracking. In ECCV, 2016.\n\nFeatures for multi-target multi-camera tracking and re-identification. Ergys Ristani, Carlo Tomasi, CVPR. Ergys Ristani and Carlo Tomasi. Features for multi-target multi-camera tracking and re-identification. In CVPR, 2018.\n\nIndoor segmentation and support inference from rgbd images. Nathan Silberman, Derek Hoiem, Pushmeet Kohli, Rob Fergus, ECCV. Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support inference from rgbd images. In ECCV, 2012.\n\n15 keypoints is all you need. Michael Snower, Asim Kadav, Farley Lai, Hans Peter Graf, CVPR. Michael Snower, Asim Kadav, Farley Lai, and Hans Peter Graf. 15 keypoints is all you need. In CVPR, 2020.\n\nOnline multi-object tracking and segmentation with gmphd filter and simple affinity fusion. Young-Min Song, Moongu Jeon, arXiv:2009.00100Young-min Song and Moongu Jeon. Online multi-object tracking and segmentation with gmphd filter and simple affinity fusion. arXiv:2009.00100, 2020.\n\nPerceive where to focus: Learning visibility-aware part-level features for partial person re-identification. Yifan Sun, Qin Xu, Yali Li, Chi Zhang, Yikang Li, Shengjin Wang, Jian Sun, CVPR. Yifan Sun, Qin Xu, Yali Li, Chi Zhang, Yikang Li, Shengjin Wang, and Jian Sun. Perceive where to focus: Learning visibility-aware part-level features for partial person re-identification. In CVPR, 2019.\n\nBeyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline). Yifan Sun, Liang Zheng, Yi Yang, Qi Tian, Shengjin Wang, In ECCV. Yifan Sun, Liang Zheng, Yi Yang, Qi Tian, and Shengjin Wang. Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline). In ECCV, 2018.\n\nMultiple people tracking by lifted multicut and person re-identification. Siyu Tang, Mykhaylo Andriluka, Bjoern Andres, Bernt Schiele, CVPR. Siyu Tang, Mykhaylo Andriluka, Bjoern Andres, and Bernt Schiele. Multiple people tracking by lifted multicut and person re-identification. In CVPR, 2017.\n\nWhat makes for good views for contrastive learning. Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, Phillip Isola, NeurIPS. Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What makes for good views for contrastive learning. In NeurIPS, 2020.\n\nEnd-to-end representation learning for correlation filter based tracking. Jack Valmadre, Luca Bertinetto, Joao Henriques, Andrea Vedaldi, Philip Hs Torr, CVPR. Jack Valmadre, Luca Bertinetto, Joao Henriques, Andrea Vedaldi, and Philip HS Torr. End-to-end representation learning for correlation filter based tracking. In CVPR, 2017.\n\nLong-term tracking in the wild: A benchmark. Jack Valmadre, Luca Bertinetto, Joao F Henriques, Ran Tao, Andrea Vedaldi, W M Arnold, Smeulders, H S Philip, Efstratios Torr, Gavves, ECCV. Jack Valmadre, Luca Bertinetto, Joao F Henriques, Ran Tao, Andrea Vedaldi, Arnold WM Smeulders, Philip HS Torr, and Efstratios Gavves. Long-term tracking in the wild: A benchmark. In ECCV, 2018.\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Illia Kaiser, Polosukhin, NeurIPS. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.\n\nFeelvos: Fast end-to-end embedding learning for video object segmentation. Paul Voigtlaender, Yuning Chai, Florian Schroff, Hartwig Adam, Bastian Leibe, Liang-Chieh Chen, CVPR. Paul Voigtlaender, Yuning Chai, Florian Schroff, Hartwig Adam, Bastian Leibe, and Liang-Chieh Chen. Feelvos: Fast end-to-end embedding learning for video object segmentation. In CVPR, 2019.\n\nMots: Multi-object tracking and segmentation. Paul Voigtlaender, Michael Krause, Aljosa Osep, Jonathon Luiten, Berin Balachandar Gnana, Andreas Sekar, Bastian Geiger, Leibe, CVPR. Paul Voigtlaender, Michael Krause, Aljosa Osep, Jonathon Luiten, Berin Balachandar Gnana Sekar, Andreas Geiger, and Bastian Leibe. Mots: Multi-object tracking and segmentation. In CVPR, 2019.\n\nTracking emerges by colorizing videos. Carl Vondrick, Abhinav Shrivastava, Alireza Fathi, Sergio Guadarrama, Kevin Murphy, ECCV. Carl Vondrick, Abhinav Shrivastava, Alireza Fathi, Sergio Guadarrama, and Kevin Murphy. Tracking emerges by colorizing videos. In ECCV, 2018.\n\nUnsupervised deep tracking. Ning Wang, Yibing Song, Chao Ma, Wengang Zhou, Wei Liu, Houqiang Li, CVPR. Ning Wang, Yibing Song, Chao Ma, Wengang Zhou, Wei Liu, and Houqiang Li. Unsupervised deep tracking. In CVPR, 2019.\n\nUnsupervised deep representation learning for real-time tracking. Ning Wang, Wengang Zhou, Yibing Song, Chao Ma, Wei Liu, Houqiang Li, 2020Ning Wang, Wengang Zhou, Yibing Song, Chao Ma, Wei Liu, and Houqiang Li. Unsupervised deep representation learning for real-time tracking. IJCV, 2020.\n\nDcfnet: Discriminant correlation filters network for visual tracking. Qiang Wang, Jin Gao, Junliang Xing, Mengdan Zhang, Weiming Hu, arXiv:1704.04057Qiang Wang, Jin Gao, Junliang Xing, Mengdan Zhang, and Weiming Hu. Dcfnet: Discriminant correlation filters network for visual tracking. arXiv:1704.04057, 2017.\n\nFast online object tracking and segmentation: A unifying approach. Qiang Wang, Li Zhang, Luca Bertinetto, Weiming Hu, Philip Hs Torr, CVPR. Qiang Wang, Li Zhang, Luca Bertinetto, Weiming Hu, and Philip HS Torr. Fast online object tracking and segmentation: A unifying approach. In CVPR, 2019.\n\nUnsupervised learning of visual representations using videos. Xiaolong Wang, Abhinav Gupta, ICCV. Xiaolong Wang and Abhinav Gupta. Unsupervised learning of visual representations using videos. In ICCV, 2015.\n\nLearning correspondence from the cycle-consistency of time. Xiaolong Wang, Allan Jabri, Alexei A Efros, CVPR. Xiaolong Wang, Allan Jabri, and Alexei A Efros. Learning correspondence from the cycle-consistency of time. In CVPR, 2019.\n\nTowards real-time multi-object tracking. Zhongdao Wang, Liang Zheng, Yixuan Liu, Shengjin Wang, ECCV. Zhongdao Wang, Liang Zheng, Yixuan Liu, and Shengjin Wang. Towards real-time multi-object tracking. In ECCV, 2020.\n\nConvolutional pose machines. Shih-En, Varun Wei, Takeo Ramakrishna, Yaser Kanade, Sheikh, CVPR. Shih-En Wei, Varun Ramakrishna, Takeo Kanade, and Yaser Sheikh. Convolutional pose machines. In CVPR, 2016.\n\nFine-grained few-shot classification with feature map reconstruction networks. Davis Wertheimer, Luming Tang, Bharath Hariharan, CVPRDavis Wertheimer, Luming Tang, and Bharath Hariharan. Fine-grained few-shot classification with feature map reconstruction networks. CVPR, 2021.\n\nSimple online and realtime tracking with a deep association metric. Nicolai Wojke, Alex Bewley, Dietrich Paulus, ICIP. Nicolai Wojke, Alex Bewley, and Dietrich Paulus. Simple online and realtime tracking with a deep association metric. In ICIP, 2017.\n\nTrack to detect and segment: An online multi-object tracker. Jialian Wu, Jiale Cao, Liangchen Song, Yu Wang, Ming Yang, Junsong Yuan, CVPR. 2021Jialian Wu, Jiale Cao, Liangchen Song, Yu Wang, Ming Yang, and Junsong Yuan. Track to detect and segment: An online multi-object tracker. In CVPR, 2021.\n\nOnline object tracking: A benchmark. Yi Wu, Jongwoo Lim, Ming-Hsuan Yang, CVPR. Yi Wu, Jongwoo Lim, and Ming-Hsuan Yang. Online object tracking: A benchmark. In CVPR, 2013.\n\nUnsupervised feature learning via nonparametric instance discrimination. Zhirong Wu, Yuanjun Xiong, X Stella, Dahua Yu, Lin, CVPR. Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non- parametric instance discrimination. In CVPR, 2018.\n\nDetco: Unsupervised contrastive learning for object detection. Enze Xie, Jian Ding, Wenhai Wang, Xiaohang Zhan, Hang Xu, Zhenguo Li, Ping Luo, arXiv:2102.04803Enze Xie, Jian Ding, Wenhai Wang, Xiaohang Zhan, Hang Xu, Zhenguo Li, and Ping Luo. Detco: Unsupervised contrastive learning for object detection. arXiv:2102.04803, 2021.\n\nPropagate yourself: Exploring pixel-level consistency for unsupervised visual representation learning. Zhenda Xie, Yutong Lin, Zheng Zhang, Yue Cao, Stephen Lin, Han Hu, CVPR. 2021Zhenda Xie, Yutong Lin, Zheng Zhang, Yue Cao, Stephen Lin, and Han Hu. Propagate yourself: Exploring pixel-level consistency for unsupervised visual representation learning. In CVPR, 2021.\n\nRethinking self-supervised correspondence learning: A video frame-level similarity perspective. Jiarui Xu, Xiaolong Wang, arXiv:2103.17263Jiarui Xu and Xiaolong Wang. Rethinking self-supervised correspondence learning: A video frame-level similarity perspective. arXiv:2103.17263, 2021.\n\nYoutube-vos: Sequence-to-sequence video object segmentation. Ning Xu, Linjie Yang, Yuchen Fan, Jianchao Yang, Dingcheng Yue, Yuchen Liang, Brian Price, Scott Cohen, Thomas Huang, ECCV. Ning Xu, Linjie Yang, Yuchen Fan, Jianchao Yang, Dingcheng Yue, Yuchen Liang, Brian Price, Scott Cohen, and Thomas Huang. Youtube-vos: Sequence-to-sequence video object segmentation. In ECCV, 2018.\n\nSegment as points for efficient online multi-object tracking and segmentation. Zhenbo Xu, Wei Zhang, Xiao Tan, Wei Yang, Huan Huang, Shilei Wen, Errui Ding, Liusheng Huang, ECCV. Zhenbo Xu, Wei Zhang, Xiao Tan, Wei Yang, Huan Huang, Shilei Wen, Errui Ding, and Liusheng Huang. Segment as points for efficient online multi-object tracking and segmentation. In ECCV, 2020.\n\nExploit all the layers: Fast and accurate cnn object detector with scale dependent pooling and cascaded rejection classifiers. Fan Yang, Wongun Choi, Yuanqing Lin, CVPR. Fan Yang, Wongun Choi, and Yuanqing Lin. Exploit all the layers: Fast and accurate cnn object detector with scale dependent pooling and cascaded rejection classifiers. In CVPR, 2016.\n\nVideo instance segmentation. Linjie Yang, Yuchen Fan, Ning Xu, ICCV. Linjie Yang, Yuchen Fan, and Ning Xu. Video instance segmentation. In ICCV, 2019.\n\nEfficient video object segmentation via network modulation. Linjie Yang, Yanran Wang, Xuehan Xiong, Jianchao Yang, Aggelos K Katsaggelos, CVPR. Linjie Yang, Yanran Wang, Xuehan Xiong, Jianchao Yang, and Aggelos K Katsaggelos. Efficient video object segmentation via network modulation. In CVPR, 2018.\n\nEfficient video object segmentation via network modulation. Linjie Yang, Yanran Wang, Xuehan Xiong, Jianchao Yang, Aggelos K Katsaggelos, CVPR. Linjie Yang, Yanran Wang, Xuehan Xiong, Jianchao Yang, and Aggelos K Katsaggelos. Efficient video object segmentation via network modulation. In CVPR, 2018.\n\nMulti-person pose estimation for pose tracking with enhanced cascaded pyramid network. Dongdong Yu, Kai Su, Jia Sun, Changhu Wang, ECCV, workshop. Dongdong Yu, Kai Su, Jia Sun, and Changhu Wang. Multi-person pose estimation for pose tracking with enhanced cascaded pyramid network. In ECCV, workshop, 2018.\n\nBarlow twins: Self-supervised learning via redundancy reduction. Jure Zbontar, Li Jing, Ishan Misra, Yann Lecun, St\u00e9phane Deny, arXiv:2103.03230Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and St\u00e9phane Deny. Barlow twins: Self-supervised learning via redundancy reduction. arXiv:2103.03230, 2021.\n\nDeepemd: Few-shot image classification with differentiable earth mover's distance and structured classifiers. Chi Zhang, Yujun Cai, Guosheng Lin, Chunhua Shen, CVPR. Chi Zhang, Yujun Cai, Guosheng Lin, and Chunhua Shen. Deepemd: Few-shot image classification with differentiable earth mover's distance and structured classifiers. In CVPR, 2020.\n\nSplit-brain autoencoders: Unsupervised learning by cross-channel prediction. Richard Zhang, Phillip Isola, Alexei A Efros, CVPR. Richard Zhang, Phillip Isola, and Alexei A Efros. Split-brain autoencoders: Unsupervised learning by cross-channel prediction. In CVPR, 2017.\n\nFairmot: On the fairness of detection and re-identification in multiple object tracking. Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng, Wenyu Liu, arXiv:2004.01888Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng, and Wenyu Liu. Fairmot: On the fairness of detection and re-identification in multiple object tracking. arXiv:2004.01888, 2020.\n\nTowards accurate pixel-wise object tracking by attention retrieval. Zhipeng Zhang, Bing Li, Weiming Hu, Houweng Peng, arXiv:2008.02745Zhipeng Zhang, Bing Li, Weiming Hu, and Houweng Peng. Towards accurate pixel-wise object tracking by attention retrieval. arXiv:2008.02745, 2020.\n\nScalable person re-identification: A benchmark. Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jingdong Wang, Qi Tian, ICCV. Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jingdong Wang, and Qi Tian. Scalable person re-identification: A benchmark. In ICCV, 2015.\n\nScene parsing through ade20k dataset. Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, Antonio Torralba, CVPR. Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In CVPR, 2017.\n\nTracking objects as points. Xingyi Zhou, Vladlen Koltun, Philipp Kr\u00e4henb\u00fchl, ECCV. Xingyi Zhou, Vladlen Koltun, and Philipp Kr\u00e4henb\u00fchl. Tracking objects as points. In ECCV, 2020.\n\nIdentity-guided human semantic parsing for person re-identification. Kuan Zhu, Haiyun Guo, Zhiwei Liu, Ming Tang, Jinqiao Wang, N }, detection indices D = {1, ..., M }. Hyperparameter \u03bb. Output: Set of matches M, set of unmatched tracklets T remain , and detections D remain 1 Initialization: M \u2190 \u2205. ECCV, 2020. Algorithm 1: Hungarian Association Input: Tracklet indices T = {1. D remain \u2190 D, T remain \u2190 T ; j ] = Hungarian_assignment (C)Kuan Zhu, Haiyun Guo, Zhiwei Liu, Ming Tang, and Jinqiao Wang. Identity-guided human semantic parsing for person re-identification. In ECCV, 2020. Algorithm 1: Hungarian Association Input: Tracklet indices T = {1, ..., N }, detection indices D = {1, ..., M }. Hyperparameter \u03bb. Output: Set of matches M, set of unmatched tracklets T remain , and detections D remain 1 Initialization: M \u2190 \u2205, D remain \u2190 D, T remain \u2190 T ; j ] = Hungarian_assignment (C);\n\n. M \u2190 M \u222a { , i, j)|b i,j \u00b7 x i,j > 0}M \u2190 M \u222a {(i, j)|b i,j \u00b7 x i,j > 0} ;\n\n// second matching stage. // second matching stage\n\n. Compute IOU cost matrix C g between T remain and D remain. 14 [x i,j ] = Hungarian_assignment (C)Compute IOU cost matrix C g between T remain and D remain .; 14 [x i,j ] = Hungarian_assignment (C);\n\nj)|x i,j > 0} ; j > 0}. M \u2190 M \u222a {(i, M \u2190 M \u222a {(i, j)|x i,j > 0} ; j > 0} ;\n", "annotations": {"author": "[{\"end\":224,\"start\":68},{\"end\":311,\"start\":225},{\"end\":463,\"start\":312},{\"end\":620,\"start\":464},{\"end\":678,\"start\":621},{\"end\":705,\"start\":679}]", "publisher": null, "author_last_name": "[{\"end\":81,\"start\":77},{\"end\":240,\"start\":236},{\"end\":320,\"start\":318},{\"end\":477,\"start\":473},{\"end\":636,\"start\":632},{\"end\":694,\"start\":684}]", "author_first_name": "[{\"end\":76,\"start\":68},{\"end\":235,\"start\":225},{\"end\":317,\"start\":312},{\"end\":472,\"start\":464},{\"end\":627,\"start\":621},{\"end\":631,\"start\":628},{\"end\":683,\"start\":679}]", "author_affiliation": "[{\"end\":164,\"start\":83},{\"end\":223,\"start\":166},{\"end\":281,\"start\":242},{\"end\":310,\"start\":283},{\"end\":403,\"start\":322},{\"end\":462,\"start\":405},{\"end\":560,\"start\":479},{\"end\":619,\"start\":562},{\"end\":677,\"start\":638},{\"end\":704,\"start\":696}]", "title": "[{\"end\":65,\"start\":1},{\"end\":770,\"start\":706}]", "venue": null, "abstract": "[{\"end\":2010,\"start\":772}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b92\"},\"end\":2447,\"start\":2443},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":2450,\"start\":2447},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":2453,\"start\":2450},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":2720,\"start\":2716},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":2731,\"start\":2727},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2749,\"start\":2746},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3528,\"start\":3524},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3530,\"start\":3528},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3546,\"start\":3542},{\"end\":3549,\"start\":3546},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4688,\"start\":4685},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":4691,\"start\":4688},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":4694,\"start\":4691},{\"end\":5356,\"start\":5351},{\"attributes\":{\"ref_id\":\"b92\"},\"end\":6058,\"start\":6054},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":6068,\"start\":6064},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":6078,\"start\":6074},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":6089,\"start\":6085},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6108,\"start\":6105},{\"attributes\":{\"ref_id\":\"b96\"},\"end\":7361,\"start\":7357},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":11455,\"start\":11451},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":11779,\"start\":11775},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":11782,\"start\":11779},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":11903,\"start\":11899},{\"attributes\":{\"ref_id\":\"b96\"},\"end\":11906,\"start\":11903},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":11967,\"start\":11963},{\"end\":11980,\"start\":11976},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":12578,\"start\":12574},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":12581,\"start\":12578},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":12584,\"start\":12581},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":12587,\"start\":12584},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":13111,\"start\":13107},{\"attributes\":{\"ref_id\":\"b88\"},\"end\":13538,\"start\":13534},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":14313,\"start\":14310},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":14658,\"start\":14654},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":14661,\"start\":14658},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":15203,\"start\":15199},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":15478,\"start\":15474},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":15861,\"start\":15857},{\"attributes\":{\"ref_id\":\"b112\"},\"end\":16530,\"start\":16525},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":16533,\"start\":16530},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":16672,\"start\":16668},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":16675,\"start\":16672},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":19795,\"start\":19791},{\"attributes\":{\"ref_id\":\"b89\"},\"end\":20628,\"start\":20624},{\"attributes\":{\"ref_id\":\"b105\"},\"end\":20632,\"start\":20628},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":20663,\"start\":20659},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":20698,\"start\":20694},{\"attributes\":{\"ref_id\":\"b106\"},\"end\":21615,\"start\":21610},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":21911,\"start\":21907},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":22917,\"start\":22913},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":22920,\"start\":22917},{\"end\":22923,\"start\":22920},{\"attributes\":{\"ref_id\":\"b106\"},\"end\":22972,\"start\":22967},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":23515,\"start\":23511},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":23529,\"start\":23525},{\"attributes\":{\"ref_id\":\"b94\"},\"end\":23602,\"start\":23598},{\"attributes\":{\"ref_id\":\"b95\"},\"end\":23618,\"start\":23614},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":23652,\"start\":23648},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":23665,\"start\":23661},{\"attributes\":{\"ref_id\":\"b106\"},\"end\":24318,\"start\":24313},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":25778,\"start\":25774},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":25810,\"start\":25806},{\"attributes\":{\"ref_id\":\"b94\"},\"end\":25813,\"start\":25810},{\"attributes\":{\"ref_id\":\"b92\"},\"end\":25867,\"start\":25863},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":25876,\"start\":25872},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":25885,\"start\":25881},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":25895,\"start\":25891},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":25909,\"start\":25906},{\"attributes\":{\"ref_id\":\"b96\"},\"end\":26134,\"start\":26130},{\"attributes\":{\"ref_id\":\"b95\"},\"end\":26660,\"start\":26656},{\"attributes\":{\"ref_id\":\"b94\"},\"end\":26672,\"start\":26668},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":26688,\"start\":26684},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":26707,\"start\":26703},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":26717,\"start\":26713},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":26756,\"start\":26752},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":27586,\"start\":27582},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":27983,\"start\":27979},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":27986,\"start\":27983},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":28630,\"start\":28626},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":29995,\"start\":29992},{\"attributes\":{\"ref_id\":\"b100\"},\"end\":30015,\"start\":30010},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":30043,\"start\":30039},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":30413,\"start\":30409},{\"attributes\":{\"ref_id\":\"b108\"},\"end\":30417,\"start\":30413},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":30420,\"start\":30417},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":31384,\"start\":31380},{\"end\":31387,\"start\":31384},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":31390,\"start\":31387},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":31437,\"start\":31433},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":31712,\"start\":31708},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":31715,\"start\":31712},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":31750,\"start\":31746},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":31753,\"start\":31750},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":32173,\"start\":32169},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":32176,\"start\":32173},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":32179,\"start\":32176},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":32181,\"start\":32179},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":34645,\"start\":34642},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":34731,\"start\":34727},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":34734,\"start\":34731},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":35040,\"start\":35036},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":35648,\"start\":35645},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":35651,\"start\":35648},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":35654,\"start\":35651},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":35657,\"start\":35654},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":35973,\"start\":35970},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":36324,\"start\":36320},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":36327,\"start\":36324},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":37002,\"start\":36998},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":37005,\"start\":37002},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":37422,\"start\":37418},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":37796,\"start\":37792},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":37917,\"start\":37914},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":37920,\"start\":37917},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":38875,\"start\":38871},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":38878,\"start\":38875},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":39456,\"start\":39452},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":39469,\"start\":39465},{\"attributes\":{\"ref_id\":\"b88\"},\"end\":39878,\"start\":39874},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":41502,\"start\":41498},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":41880,\"start\":41876},{\"attributes\":{\"ref_id\":\"b90\"},\"end\":42195,\"start\":42191},{\"attributes\":{\"ref_id\":\"b90\"},\"end\":42875,\"start\":42871},{\"attributes\":{\"ref_id\":\"b90\"},\"end\":44988,\"start\":44984},{\"attributes\":{\"ref_id\":\"b111\"},\"end\":46471,\"start\":46466},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":46474,\"start\":46471},{\"attributes\":{\"ref_id\":\"b107\"},\"end\":46478,\"start\":46474},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":47478,\"start\":47474},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":47814,\"start\":47810},{\"attributes\":{\"ref_id\":\"b112\"},\"end\":47818,\"start\":47814},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":47821,\"start\":47818},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":48218,\"start\":48214},{\"attributes\":{\"ref_id\":\"b109\"},\"end\":48314,\"start\":48309},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":48328,\"start\":48324},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":48352,\"start\":48348},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":48478,\"start\":48474},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":48481,\"start\":48478},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":48484,\"start\":48481},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":48920,\"start\":48916},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":48992,\"start\":48988},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":49011,\"start\":49007},{\"attributes\":{\"ref_id\":\"b99\"},\"end\":49030,\"start\":49025},{\"attributes\":{\"ref_id\":\"b107\"},\"end\":49049,\"start\":49044},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":50066,\"start\":50062},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":50237,\"start\":50233},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":50649,\"start\":50645},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":51206,\"start\":51202},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":51209,\"start\":51206},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":51966,\"start\":51963},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":51992,\"start\":51988},{\"attributes\":{\"ref_id\":\"b100\"},\"end\":52546,\"start\":52541},{\"attributes\":{\"ref_id\":\"b102\"},\"end\":52682,\"start\":52677},{\"attributes\":{\"ref_id\":\"b90\"},\"end\":52700,\"start\":52696},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":52908,\"start\":52905},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":54328,\"start\":54324},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":54452,\"start\":54448},{\"attributes\":{\"ref_id\":\"b110\"},\"end\":54550,\"start\":54545},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":61192,\"start\":61188},{\"attributes\":{\"ref_id\":\"b100\"},\"end\":63504,\"start\":63499},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":63708,\"start\":63704},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":64778,\"start\":64774},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":64883,\"start\":64879},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":64984,\"start\":64980},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":65078,\"start\":65074},{\"attributes\":{\"ref_id\":\"b110\"},\"end\":65179,\"start\":65174}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":57683,\"start\":57649},{\"attributes\":{\"id\":\"fig_3\"},\"end\":57909,\"start\":57684},{\"attributes\":{\"id\":\"fig_4\"},\"end\":58265,\"start\":57910},{\"attributes\":{\"id\":\"fig_5\"},\"end\":58572,\"start\":58266},{\"attributes\":{\"id\":\"fig_6\"},\"end\":58871,\"start\":58573},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":59775,\"start\":58872},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":60904,\"start\":59776},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":60941,\"start\":60905},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":61101,\"start\":60942},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":61479,\"start\":61102},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":62039,\"start\":61480},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":62380,\"start\":62040},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":62770,\"start\":62381},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":62993,\"start\":62771},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":63176,\"start\":62994},{\"attributes\":{\"id\":\"tab_17\",\"type\":\"table\"},\"end\":63464,\"start\":63177},{\"attributes\":{\"id\":\"tab_18\",\"type\":\"table\"},\"end\":63611,\"start\":63465},{\"attributes\":{\"id\":\"tab_19\",\"type\":\"table\"},\"end\":64646,\"start\":63612},{\"attributes\":{\"id\":\"tab_20\",\"type\":\"table\"},\"end\":67798,\"start\":64647}]", "paragraph": "[{\"end\":3124,\"start\":2026},{\"end\":5059,\"start\":3126},{\"end\":5917,\"start\":5061},{\"end\":5978,\"start\":5919},{\"end\":6154,\"start\":5980},{\"end\":6337,\"start\":6156},{\"end\":6522,\"start\":6339},{\"end\":6676,\"start\":6524},{\"end\":8129,\"start\":6714},{\"end\":9275,\"start\":8131},{\"end\":10257,\"start\":9277},{\"end\":10459,\"start\":10259},{\"end\":10873,\"start\":10485},{\"end\":11175,\"start\":10875},{\"end\":11982,\"start\":11243},{\"end\":12449,\"start\":11998},{\"end\":12929,\"start\":12451},{\"end\":13429,\"start\":13026},{\"end\":14681,\"start\":13431},{\"end\":15123,\"start\":14697},{\"end\":15862,\"start\":15125},{\"end\":16285,\"start\":15864},{\"end\":17132,\"start\":16287},{\"end\":17666,\"start\":17134},{\"end\":18541,\"start\":17725},{\"end\":18934,\"start\":18543},{\"end\":19031,\"start\":18936},{\"end\":19602,\"start\":19080},{\"end\":19748,\"start\":19671},{\"end\":20534,\"start\":19750},{\"end\":21133,\"start\":20536},{\"end\":21864,\"start\":21149},{\"end\":22519,\"start\":21866},{\"end\":22718,\"start\":22521},{\"end\":23381,\"start\":22792},{\"end\":23751,\"start\":23383},{\"end\":25124,\"start\":23753},{\"end\":25557,\"start\":25126},{\"end\":26430,\"start\":25559},{\"end\":26532,\"start\":26432},{\"end\":27749,\"start\":26534},{\"end\":28357,\"start\":27800},{\"end\":29042,\"start\":28359},{\"end\":29660,\"start\":29044},{\"end\":30856,\"start\":29677},{\"end\":32431,\"start\":30858},{\"end\":34080,\"start\":32446},{\"end\":34472,\"start\":34225},{\"end\":35144,\"start\":34474},{\"end\":35507,\"start\":35146},{\"end\":35923,\"start\":35547},{\"end\":36062,\"start\":35925},{\"end\":36259,\"start\":36064},{\"end\":36564,\"start\":36261},{\"end\":36735,\"start\":36585},{\"end\":37008,\"start\":36761},{\"end\":37269,\"start\":37031},{\"end\":37435,\"start\":37299},{\"end\":37578,\"start\":37499},{\"end\":37868,\"start\":37580},{\"end\":38684,\"start\":37870},{\"end\":39725,\"start\":38718},{\"end\":40027,\"start\":39727},{\"end\":40060,\"start\":40029},{\"end\":40196,\"start\":40135},{\"end\":40418,\"start\":40240},{\"end\":41024,\"start\":40464},{\"end\":41837,\"start\":41026},{\"end\":42147,\"start\":41839},{\"end\":42474,\"start\":42149},{\"end\":42927,\"start\":42519},{\"end\":43116,\"start\":42956},{\"end\":43585,\"start\":43118},{\"end\":44231,\"start\":43610},{\"end\":44724,\"start\":44233},{\"end\":45132,\"start\":44726},{\"end\":45623,\"start\":45164},{\"end\":46009,\"start\":45686},{\"end\":46694,\"start\":46011},{\"end\":48619,\"start\":46696},{\"end\":49050,\"start\":48621},{\"end\":49919,\"start\":49052},{\"end\":50495,\"start\":49921},{\"end\":50589,\"start\":50521},{\"end\":51077,\"start\":50591},{\"end\":51445,\"start\":51079},{\"end\":52428,\"start\":51447},{\"end\":53070,\"start\":52430},{\"end\":53288,\"start\":53105},{\"end\":53860,\"start\":53290},{\"end\":54384,\"start\":53897},{\"end\":55060,\"start\":54386},{\"end\":55576,\"start\":55062},{\"end\":55873,\"start\":55578},{\"end\":56480,\"start\":55875},{\"end\":57648,\"start\":56482}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11242,\"start\":11176},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13025,\"start\":12930},{\"attributes\":{\"id\":\"formula_2\"},\"end\":17724,\"start\":17667},{\"attributes\":{\"id\":\"formula_3\"},\"end\":19079,\"start\":19032},{\"attributes\":{\"id\":\"formula_4\"},\"end\":19670,\"start\":19603},{\"attributes\":{\"id\":\"formula_7\"},\"end\":36584,\"start\":36565},{\"attributes\":{\"id\":\"formula_8\"},\"end\":36760,\"start\":36736},{\"attributes\":{\"id\":\"formula_9\"},\"end\":37030,\"start\":37009},{\"attributes\":{\"id\":\"formula_10\"},\"end\":37298,\"start\":37270},{\"attributes\":{\"id\":\"formula_11\"},\"end\":37498,\"start\":37436},{\"attributes\":{\"id\":\"formula_12\"},\"end\":40134,\"start\":40061},{\"attributes\":{\"id\":\"formula_13\"},\"end\":42518,\"start\":42475},{\"attributes\":{\"id\":\"formula_14\"},\"end\":42955,\"start\":42928},{\"attributes\":{\"id\":\"formula_15\"},\"end\":43609,\"start\":43586}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":23781,\"start\":23774},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":26544,\"start\":26537},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":26573,\"start\":26566},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":27978,\"start\":27965},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":37611,\"start\":37592},{\"attributes\":{\"ref_id\":\"tab_13\"},\"end\":49080,\"start\":49073},{\"attributes\":{\"ref_id\":\"tab_14\"},\"end\":50029,\"start\":50022},{\"attributes\":{\"ref_id\":\"tab_15\"},\"end\":50207,\"start\":50200},{\"attributes\":{\"ref_id\":\"tab_17\"},\"end\":51107,\"start\":51100},{\"end\":51217,\"start\":51210},{\"attributes\":{\"ref_id\":\"tab_18\"},\"end\":52470,\"start\":52463},{\"end\":53312,\"start\":53305},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":53384,\"start\":53375}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2024,\"start\":2012},{\"attributes\":{\"n\":\"2\"},\"end\":6701,\"start\":6679},{\"attributes\":{\"n\":\"2.1\"},\"end\":6712,\"start\":6704},{\"attributes\":{\"n\":\"2.2\"},\"end\":10483,\"start\":10462},{\"attributes\":{\"n\":\"2.3\"},\"end\":11996,\"start\":11985},{\"attributes\":{\"n\":\"2.4\"},\"end\":14695,\"start\":14684},{\"attributes\":{\"n\":\"3\"},\"end\":21147,\"start\":21136},{\"attributes\":{\"n\":\"3.1\"},\"end\":22790,\"start\":22721},{\"attributes\":{\"n\":\"3.2\"},\"end\":27798,\"start\":27752},{\"attributes\":{\"n\":\"4\"},\"end\":29675,\"start\":29663},{\"attributes\":{\"n\":\"5\"},\"end\":32444,\"start\":32434},{\"attributes\":{\"n\":\"6\"},\"end\":34113,\"start\":34083},{\"end\":34223,\"start\":34116},{\"end\":35523,\"start\":35510},{\"end\":35545,\"start\":35526},{\"end\":38716,\"start\":38687},{\"end\":40238,\"start\":40199},{\"end\":40434,\"start\":40421},{\"end\":40462,\"start\":40437},{\"end\":45162,\"start\":45135},{\"end\":45684,\"start\":45626},{\"end\":50519,\"start\":50498},{\"end\":53103,\"start\":53073},{\"end\":53895,\"start\":53863},{\"end\":57660,\"start\":57650},{\"end\":57695,\"start\":57685},{\"end\":57930,\"start\":57911},{\"end\":58277,\"start\":58267},{\"end\":58584,\"start\":58574},{\"end\":58882,\"start\":58873},{\"end\":59786,\"start\":59777},{\"end\":60915,\"start\":60906},{\"end\":60952,\"start\":60943},{\"end\":62391,\"start\":62382},{\"end\":62781,\"start\":62772},{\"end\":63004,\"start\":62995},{\"end\":63187,\"start\":63178},{\"end\":63475,\"start\":63466}]", "table": "[{\"end\":59775,\"start\":58975},{\"end\":60904,\"start\":60196},{\"end\":60941,\"start\":60917},{\"end\":61479,\"start\":61325},{\"end\":62039,\"start\":61846},{\"end\":62380,\"start\":62125},{\"end\":62770,\"start\":62403},{\"end\":62993,\"start\":62793},{\"end\":63176,\"start\":63016},{\"end\":63464,\"start\":63196},{\"end\":63611,\"start\":63515},{\"end\":64646,\"start\":63930},{\"end\":67798,\"start\":65188}]", "figure_caption": "[{\"end\":57683,\"start\":57662},{\"end\":57909,\"start\":57697},{\"end\":58265,\"start\":57933},{\"end\":58572,\"start\":58279},{\"end\":58871,\"start\":58586},{\"end\":58975,\"start\":58884},{\"end\":60196,\"start\":59788},{\"end\":61101,\"start\":60954},{\"end\":61325,\"start\":61104},{\"end\":61846,\"start\":61482},{\"end\":62125,\"start\":62042},{\"end\":62403,\"start\":62393},{\"end\":62793,\"start\":62783},{\"end\":63016,\"start\":63006},{\"end\":63196,\"start\":63189},{\"end\":63515,\"start\":63477},{\"end\":63930,\"start\":63614},{\"end\":65188,\"start\":64649}]", "figure_ref": "[{\"end\":3952,\"start\":3943},{\"end\":3980,\"start\":3971},{\"end\":5767,\"start\":5759},{\"end\":6867,\"start\":6859},{\"end\":6897,\"start\":6889},{\"end\":7450,\"start\":7442},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":8541,\"start\":8533},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":9219,\"start\":9211},{\"end\":9882,\"start\":9874},{\"end\":12027,\"start\":12018},{\"end\":14726,\"start\":14717},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":19747,\"start\":19739},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":21285,\"start\":21277},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":24388,\"start\":24380},{\"end\":25365,\"start\":25357},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":54383,\"start\":54375},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":55185,\"start\":55176},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":55905,\"start\":55896},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":56592,\"start\":56583}]", "bib_author_first_name": "[{\"end\":68120,\"start\":68112},{\"end\":68268,\"start\":68260},{\"end\":68284,\"start\":68280},{\"end\":68297,\"start\":68292},{\"end\":68318,\"start\":68312},{\"end\":68336,\"start\":68331},{\"end\":68351,\"start\":68344},{\"end\":68363,\"start\":68358},{\"end\":68651,\"start\":68647},{\"end\":68658,\"start\":68652},{\"end\":68675,\"start\":68666},{\"end\":68693,\"start\":68687},{\"end\":68933,\"start\":68930},{\"end\":68951,\"start\":68941},{\"end\":68969,\"start\":68963},{\"end\":68981,\"start\":68976},{\"end\":69001,\"start\":68994},{\"end\":69265,\"start\":69261},{\"end\":69283,\"start\":69277},{\"end\":69581,\"start\":69577},{\"end\":69598,\"start\":69594},{\"end\":69615,\"start\":69609},{\"end\":69632,\"start\":69626},{\"end\":69650,\"start\":69641},{\"end\":69881,\"start\":69877},{\"end\":69898,\"start\":69894},{\"end\":69913,\"start\":69909},{\"end\":69915,\"start\":69914},{\"end\":69933,\"start\":69927},{\"end\":69952,\"start\":69943},{\"end\":70193,\"start\":70185},{\"end\":70211,\"start\":70207},{\"end\":70225,\"start\":70218},{\"end\":70238,\"start\":70233},{\"end\":70256,\"start\":70248},{\"end\":70271,\"start\":70265},{\"end\":70288,\"start\":70282},{\"end\":70308,\"start\":70302},{\"end\":70322,\"start\":70316},{\"end\":70675,\"start\":70670},{\"end\":70687,\"start\":70681},{\"end\":70725,\"start\":70719},{\"end\":70750,\"start\":70746},{\"end\":71037,\"start\":71029},{\"end\":71050,\"start\":71045},{\"end\":71069,\"start\":71063},{\"end\":71086,\"start\":71078},{\"end\":71332,\"start\":71324},{\"end\":71345,\"start\":71340},{\"end\":71359,\"start\":71353},{\"end\":71373,\"start\":71368},{\"end\":71386,\"start\":71381},{\"end\":71405,\"start\":71399},{\"end\":71691,\"start\":71687},{\"end\":71708,\"start\":71702},{\"end\":71922,\"start\":71918},{\"end\":71934,\"start\":71929},{\"end\":71954,\"start\":71946},{\"end\":71972,\"start\":71964},{\"end\":72193,\"start\":72189},{\"end\":72205,\"start\":72200},{\"end\":72222,\"start\":72217},{\"end\":72240,\"start\":72232},{\"end\":72258,\"start\":72250},{\"end\":72461,\"start\":72455},{\"end\":72473,\"start\":72468},{\"end\":72773,\"start\":72767},{\"end\":72791,\"start\":72785},{\"end\":72805,\"start\":72798},{\"end\":73059,\"start\":73052},{\"end\":73077,\"start\":73071},{\"end\":73089,\"start\":73084},{\"end\":73103,\"start\":73097},{\"end\":73121,\"start\":73115},{\"end\":73134,\"start\":73131},{\"end\":73147,\"start\":73141},{\"end\":73159,\"start\":73154},{\"end\":73484,\"start\":73477},{\"end\":73501,\"start\":73496},{\"end\":73520,\"start\":73515},{\"end\":73533,\"start\":73528},{\"end\":73545,\"start\":73539},{\"end\":73558,\"start\":73555},{\"end\":73571,\"start\":73565},{\"end\":73584,\"start\":73578},{\"end\":73601,\"start\":73596},{\"end\":73980,\"start\":73977},{\"end\":73990,\"start\":73987},{\"end\":74004,\"start\":73997},{\"end\":74019,\"start\":74013},{\"end\":74027,\"start\":74024},{\"end\":74034,\"start\":74032},{\"end\":74277,\"start\":74272},{\"end\":74294,\"start\":74286},{\"end\":74308,\"start\":74302},{\"end\":74322,\"start\":74314},{\"end\":74555,\"start\":74549},{\"end\":74576,\"start\":74569},{\"end\":74590,\"start\":74586},{\"end\":74602,\"start\":74596},{\"end\":74617,\"start\":74612},{\"end\":74636,\"start\":74628},{\"end\":74652,\"start\":74645},{\"end\":74668,\"start\":74662},{\"end\":74682,\"start\":74676},{\"end\":74991,\"start\":74986},{\"end\":75007,\"start\":75002},{\"end\":75021,\"start\":75014},{\"end\":75023,\"start\":75022},{\"end\":75205,\"start\":75201},{\"end\":75221,\"start\":75218},{\"end\":75233,\"start\":75232},{\"end\":75235,\"start\":75234},{\"end\":75253,\"start\":75249},{\"end\":75270,\"start\":75264},{\"end\":75614,\"start\":75608},{\"end\":75628,\"start\":75625},{\"end\":75638,\"start\":75634},{\"end\":75647,\"start\":75645},{\"end\":75658,\"start\":75653},{\"end\":75670,\"start\":75665},{\"end\":75679,\"start\":75675},{\"end\":75693,\"start\":75685},{\"end\":75704,\"start\":75698},{\"end\":76003,\"start\":75997},{\"end\":76017,\"start\":76012},{\"end\":76030,\"start\":76022},{\"end\":76042,\"start\":76037},{\"end\":76056,\"start\":76048},{\"end\":76314,\"start\":76313},{\"end\":76326,\"start\":76322},{\"end\":76328,\"start\":76327},{\"end\":76348,\"start\":76343},{\"end\":76567,\"start\":76563},{\"end\":76577,\"start\":76572},{\"end\":76587,\"start\":76583},{\"end\":76615,\"start\":76607},{\"end\":76626,\"start\":76623},{\"end\":76877,\"start\":76872},{\"end\":76890,\"start\":76885},{\"end\":76907,\"start\":76900},{\"end\":76920,\"start\":76915},{\"end\":77185,\"start\":77173},{\"end\":77200,\"start\":77193},{\"end\":77215,\"start\":77208},{\"end\":77232,\"start\":77224},{\"end\":77242,\"start\":77241},{\"end\":77256,\"start\":77251},{\"end\":77272,\"start\":77268},{\"end\":77294,\"start\":77286},{\"end\":77311,\"start\":77304},{\"end\":77318,\"start\":77312},{\"end\":77340,\"start\":77332},{\"end\":77351,\"start\":77341},{\"end\":77751,\"start\":77744},{\"end\":77761,\"start\":77757},{\"end\":77776,\"start\":77768},{\"end\":77787,\"start\":77782},{\"end\":77802,\"start\":77794},{\"end\":77812,\"start\":77807},{\"end\":78060,\"start\":78052},{\"end\":78070,\"start\":78066},{\"end\":78085,\"start\":78078},{\"end\":78094,\"start\":78092},{\"end\":78111,\"start\":78099},{\"end\":78125,\"start\":78117},{\"end\":78134,\"start\":78131},{\"end\":78390,\"start\":78383},{\"end\":78400,\"start\":78395},{\"end\":78411,\"start\":78406},{\"end\":78423,\"start\":78416},{\"end\":78433,\"start\":78429},{\"end\":78651,\"start\":78644},{\"end\":78663,\"start\":78656},{\"end\":78679,\"start\":78671},{\"end\":78689,\"start\":78685},{\"end\":78925,\"start\":78917},{\"end\":78934,\"start\":78930},{\"end\":78949,\"start\":78942},{\"end\":78960,\"start\":78954},{\"end\":79198,\"start\":79197},{\"end\":79208,\"start\":79205},{\"end\":79225,\"start\":79220},{\"end\":79240,\"start\":79235},{\"end\":79473,\"start\":79468},{\"end\":79487,\"start\":79481},{\"end\":79501,\"start\":79495},{\"end\":79503,\"start\":79502},{\"end\":79685,\"start\":79684},{\"end\":79695,\"start\":79694},{\"end\":79703,\"start\":79702},{\"end\":79712,\"start\":79711},{\"end\":79722,\"start\":79721},{\"end\":79724,\"start\":79723},{\"end\":79921,\"start\":79914},{\"end\":79926,\"start\":79922},{\"end\":79935,\"start\":79929},{\"end\":80099,\"start\":80093},{\"end\":80128,\"start\":80123},{\"end\":80360,\"start\":80355},{\"end\":80374,\"start\":80370},{\"end\":80386,\"start\":80382},{\"end\":80403,\"start\":80398},{\"end\":80416,\"start\":80411},{\"end\":80437,\"start\":80430},{\"end\":80454,\"start\":80449},{\"end\":80469,\"start\":80464},{\"end\":80490,\"start\":80479},{\"end\":81014,\"start\":81008},{\"end\":81025,\"start\":81020},{\"end\":81035,\"start\":81030},{\"end\":81225,\"start\":81223},{\"end\":81233,\"start\":81230},{\"end\":81243,\"start\":81238},{\"end\":81256,\"start\":81250},{\"end\":81272,\"start\":81264},{\"end\":81285,\"start\":81279},{\"end\":81531,\"start\":81529},{\"end\":81542,\"start\":81536},{\"end\":81551,\"start\":81548},{\"end\":81561,\"start\":81556},{\"end\":81574,\"start\":81567},{\"end\":81799,\"start\":81793},{\"end\":81807,\"start\":81804},{\"end\":81821,\"start\":81814},{\"end\":81836,\"start\":81829},{\"end\":81846,\"start\":81845},{\"end\":81848,\"start\":81847},{\"end\":82104,\"start\":82101},{\"end\":82112,\"start\":82109},{\"end\":82123,\"start\":82119},{\"end\":82138,\"start\":82130},{\"end\":82362,\"start\":82355},{\"end\":82372,\"start\":82367},{\"end\":82386,\"start\":82378},{\"end\":82408,\"start\":82405},{\"end\":82425,\"start\":82415},{\"end\":82704,\"start\":82700},{\"end\":82719,\"start\":82712},{\"end\":82729,\"start\":82727},{\"end\":82737,\"start\":82734},{\"end\":82748,\"start\":82744},{\"end\":82759,\"start\":82753},{\"end\":82772,\"start\":82764},{\"end\":83063,\"start\":83055},{\"end\":83075,\"start\":83071},{\"end\":83090,\"start\":83084},{\"end\":83297,\"start\":83289},{\"end\":83310,\"start\":83303},{\"end\":83323,\"start\":83318},{\"end\":83339,\"start\":83334},{\"end\":83352,\"start\":83346},{\"end\":83365,\"start\":83361},{\"end\":83380,\"start\":83375},{\"end\":83399,\"start\":83389},{\"end\":83666,\"start\":83659},{\"end\":83678,\"start\":83672},{\"end\":83687,\"start\":83683},{\"end\":83942,\"start\":83938},{\"end\":83951,\"start\":83949},{\"end\":83967,\"start\":83961},{\"end\":83980,\"start\":83975},{\"end\":84223,\"start\":84215},{\"end\":84234,\"start\":84230},{\"end\":84248,\"start\":84242},{\"end\":84432,\"start\":84424},{\"end\":84447,\"start\":84441},{\"end\":84461,\"start\":84454},{\"end\":84479,\"start\":84473},{\"end\":84493,\"start\":84486},{\"end\":84507,\"start\":84502},{\"end\":84527,\"start\":84520},{\"end\":84792,\"start\":84788},{\"end\":84806,\"start\":84802},{\"end\":84819,\"start\":84814},{\"end\":84956,\"start\":84951},{\"end\":84969,\"start\":84964},{\"end\":84985,\"start\":84982},{\"end\":84998,\"start\":84992},{\"end\":85011,\"start\":85005},{\"end\":85300,\"start\":85295},{\"end\":85315,\"start\":85308},{\"end\":85508,\"start\":85501},{\"end\":85524,\"start\":85518},{\"end\":85724,\"start\":85716},{\"end\":85737,\"start\":85733},{\"end\":85750,\"start\":85744},{\"end\":85767,\"start\":85761},{\"end\":85786,\"start\":85779},{\"end\":86066,\"start\":86058},{\"end\":86077,\"start\":86073},{\"end\":86309,\"start\":86301},{\"end\":86320,\"start\":86316},{\"end\":86335,\"start\":86326},{\"end\":86344,\"start\":86341},{\"end\":86590,\"start\":86580},{\"end\":86610,\"start\":86606},{\"end\":86624,\"start\":86616},{\"end\":86851,\"start\":86849},{\"end\":86864,\"start\":86858},{\"end\":86874,\"start\":86869},{\"end\":86888,\"start\":86882},{\"end\":86897,\"start\":86893},{\"end\":87186,\"start\":87179},{\"end\":87200,\"start\":87193},{\"end\":87214,\"start\":87207},{\"end\":87224,\"start\":87220},{\"end\":87235,\"start\":87229},{\"end\":87246,\"start\":87242},{\"end\":87260,\"start\":87252},{\"end\":87272,\"start\":87267},{\"end\":87283,\"start\":87277},{\"end\":87297,\"start\":87291},{\"end\":87653,\"start\":87652},{\"end\":87664,\"start\":87663},{\"end\":87678,\"start\":87677},{\"end\":87692,\"start\":87691},{\"end\":87704,\"start\":87703},{\"end\":87713,\"start\":87712},{\"end\":88007,\"start\":88000},{\"end\":88026,\"start\":88022},{\"end\":88035,\"start\":88031},{\"end\":88290,\"start\":88285},{\"end\":88309,\"start\":88300},{\"end\":88323,\"start\":88318},{\"end\":88333,\"start\":88329},{\"end\":88350,\"start\":88345},{\"end\":88612,\"start\":88607},{\"end\":88627,\"start\":88622},{\"end\":88827,\"start\":88821},{\"end\":88844,\"start\":88839},{\"end\":88860,\"start\":88852},{\"end\":88871,\"start\":88868},{\"end\":89062,\"start\":89055},{\"end\":89075,\"start\":89071},{\"end\":89089,\"start\":89083},{\"end\":89099,\"start\":89095},{\"end\":89105,\"start\":89100},{\"end\":89326,\"start\":89317},{\"end\":89339,\"start\":89333},{\"end\":89625,\"start\":89620},{\"end\":89634,\"start\":89631},{\"end\":89643,\"start\":89639},{\"end\":89651,\"start\":89648},{\"end\":89665,\"start\":89659},{\"end\":89678,\"start\":89670},{\"end\":89689,\"start\":89685},{\"end\":90012,\"start\":90007},{\"end\":90023,\"start\":90018},{\"end\":90033,\"start\":90031},{\"end\":90042,\"start\":90040},{\"end\":90057,\"start\":90049},{\"end\":90330,\"start\":90326},{\"end\":90345,\"start\":90337},{\"end\":90363,\"start\":90357},{\"end\":90377,\"start\":90372},{\"end\":90608,\"start\":90600},{\"end\":90619,\"start\":90615},{\"end\":90628,\"start\":90625},{\"end\":90641,\"start\":90636},{\"end\":90660,\"start\":90652},{\"end\":90676,\"start\":90669},{\"end\":90930,\"start\":90926},{\"end\":90945,\"start\":90941},{\"end\":90962,\"start\":90958},{\"end\":90980,\"start\":90974},{\"end\":90999,\"start\":90990},{\"end\":91235,\"start\":91231},{\"end\":91250,\"start\":91246},{\"end\":91267,\"start\":91263},{\"end\":91269,\"start\":91268},{\"end\":91284,\"start\":91281},{\"end\":91296,\"start\":91290},{\"end\":91307,\"start\":91306},{\"end\":91309,\"start\":91308},{\"end\":91330,\"start\":91329},{\"end\":91332,\"start\":91331},{\"end\":91351,\"start\":91341},{\"end\":91601,\"start\":91595},{\"end\":91615,\"start\":91611},{\"end\":91629,\"start\":91625},{\"end\":91643,\"start\":91638},{\"end\":91660,\"start\":91655},{\"end\":91673,\"start\":91668},{\"end\":91675,\"start\":91674},{\"end\":91688,\"start\":91683},{\"end\":91969,\"start\":91965},{\"end\":91990,\"start\":91984},{\"end\":92004,\"start\":91997},{\"end\":92021,\"start\":92014},{\"end\":92035,\"start\":92028},{\"end\":92054,\"start\":92043},{\"end\":92308,\"start\":92304},{\"end\":92330,\"start\":92323},{\"end\":92345,\"start\":92339},{\"end\":92360,\"start\":92352},{\"end\":92374,\"start\":92369},{\"end\":92401,\"start\":92394},{\"end\":92416,\"start\":92409},{\"end\":92674,\"start\":92670},{\"end\":92692,\"start\":92685},{\"end\":92713,\"start\":92706},{\"end\":92727,\"start\":92721},{\"end\":92745,\"start\":92740},{\"end\":92935,\"start\":92931},{\"end\":92948,\"start\":92942},{\"end\":92959,\"start\":92955},{\"end\":92971,\"start\":92964},{\"end\":92981,\"start\":92978},{\"end\":92995,\"start\":92987},{\"end\":93193,\"start\":93189},{\"end\":93207,\"start\":93200},{\"end\":93220,\"start\":93214},{\"end\":93231,\"start\":93227},{\"end\":93239,\"start\":93236},{\"end\":93253,\"start\":93245},{\"end\":93489,\"start\":93484},{\"end\":93499,\"start\":93496},{\"end\":93513,\"start\":93505},{\"end\":93527,\"start\":93520},{\"end\":93542,\"start\":93535},{\"end\":93797,\"start\":93792},{\"end\":93806,\"start\":93804},{\"end\":93818,\"start\":93814},{\"end\":93838,\"start\":93831},{\"end\":93852,\"start\":93843},{\"end\":94089,\"start\":94081},{\"end\":94103,\"start\":94096},{\"end\":94296,\"start\":94288},{\"end\":94308,\"start\":94303},{\"end\":94322,\"start\":94316},{\"end\":94324,\"start\":94323},{\"end\":94511,\"start\":94503},{\"end\":94523,\"start\":94518},{\"end\":94537,\"start\":94531},{\"end\":94551,\"start\":94543},{\"end\":94723,\"start\":94718},{\"end\":94734,\"start\":94729},{\"end\":94753,\"start\":94748},{\"end\":94969,\"start\":94964},{\"end\":94988,\"start\":94982},{\"end\":95002,\"start\":94995},{\"end\":95239,\"start\":95232},{\"end\":95251,\"start\":95247},{\"end\":95268,\"start\":95260},{\"end\":95484,\"start\":95477},{\"end\":95494,\"start\":95489},{\"end\":95509,\"start\":95500},{\"end\":95518,\"start\":95516},{\"end\":95529,\"start\":95525},{\"end\":95543,\"start\":95536},{\"end\":95753,\"start\":95751},{\"end\":95765,\"start\":95758},{\"end\":95781,\"start\":95771},{\"end\":95968,\"start\":95961},{\"end\":95980,\"start\":95973},{\"end\":95989,\"start\":95988},{\"end\":96003,\"start\":95998},{\"end\":96232,\"start\":96228},{\"end\":96242,\"start\":96238},{\"end\":96255,\"start\":96249},{\"end\":96270,\"start\":96262},{\"end\":96281,\"start\":96277},{\"end\":96293,\"start\":96286},{\"end\":96302,\"start\":96298},{\"end\":96605,\"start\":96599},{\"end\":96617,\"start\":96611},{\"end\":96628,\"start\":96623},{\"end\":96639,\"start\":96636},{\"end\":96652,\"start\":96645},{\"end\":96661,\"start\":96658},{\"end\":96968,\"start\":96962},{\"end\":96981,\"start\":96973},{\"end\":97219,\"start\":97215},{\"end\":97230,\"start\":97224},{\"end\":97243,\"start\":97237},{\"end\":97257,\"start\":97249},{\"end\":97273,\"start\":97264},{\"end\":97285,\"start\":97279},{\"end\":97298,\"start\":97293},{\"end\":97311,\"start\":97306},{\"end\":97325,\"start\":97319},{\"end\":97623,\"start\":97617},{\"end\":97631,\"start\":97628},{\"end\":97643,\"start\":97639},{\"end\":97652,\"start\":97649},{\"end\":97663,\"start\":97659},{\"end\":97677,\"start\":97671},{\"end\":97688,\"start\":97683},{\"end\":97703,\"start\":97695},{\"end\":98040,\"start\":98037},{\"end\":98053,\"start\":98047},{\"end\":98068,\"start\":98060},{\"end\":98299,\"start\":98293},{\"end\":98312,\"start\":98306},{\"end\":98322,\"start\":98318},{\"end\":98482,\"start\":98476},{\"end\":98495,\"start\":98489},{\"end\":98508,\"start\":98502},{\"end\":98524,\"start\":98516},{\"end\":98540,\"start\":98531},{\"end\":98784,\"start\":98778},{\"end\":98797,\"start\":98791},{\"end\":98810,\"start\":98804},{\"end\":98826,\"start\":98818},{\"end\":98842,\"start\":98833},{\"end\":99115,\"start\":99107},{\"end\":99123,\"start\":99120},{\"end\":99131,\"start\":99128},{\"end\":99144,\"start\":99137},{\"end\":99397,\"start\":99393},{\"end\":99409,\"start\":99407},{\"end\":99421,\"start\":99416},{\"end\":99433,\"start\":99429},{\"end\":99449,\"start\":99441},{\"end\":99742,\"start\":99739},{\"end\":99755,\"start\":99750},{\"end\":99769,\"start\":99761},{\"end\":99782,\"start\":99775},{\"end\":100059,\"start\":100052},{\"end\":100074,\"start\":100067},{\"end\":100088,\"start\":100082},{\"end\":100090,\"start\":100089},{\"end\":100340,\"start\":100336},{\"end\":100354,\"start\":100348},{\"end\":100369,\"start\":100361},{\"end\":100382,\"start\":100376},{\"end\":100394,\"start\":100389},{\"end\":100673,\"start\":100666},{\"end\":100685,\"start\":100681},{\"end\":100697,\"start\":100690},{\"end\":100709,\"start\":100702},{\"end\":100932,\"start\":100927},{\"end\":100945,\"start\":100940},{\"end\":100954,\"start\":100952},{\"end\":100969,\"start\":100961},{\"end\":100984,\"start\":100976},{\"end\":100993,\"start\":100991},{\"end\":101190,\"start\":101185},{\"end\":101201,\"start\":101197},{\"end\":101214,\"start\":101208},{\"end\":101226,\"start\":101221},{\"end\":101240,\"start\":101235},{\"end\":101258,\"start\":101251},{\"end\":101451,\"start\":101445},{\"end\":101465,\"start\":101458},{\"end\":101481,\"start\":101474},{\"end\":101670,\"start\":101666},{\"end\":101682,\"start\":101676},{\"end\":101694,\"start\":101688},{\"end\":101704,\"start\":101700},{\"end\":101718,\"start\":101711},{\"end\":102499,\"start\":102490}]", "bib_author_last_name": "[{\"end\":68128,\"start\":68121},{\"end\":68278,\"start\":68269},{\"end\":68290,\"start\":68285},{\"end\":68310,\"start\":68298},{\"end\":68329,\"start\":68319},{\"end\":68342,\"start\":68337},{\"end\":68356,\"start\":68352},{\"end\":68371,\"start\":68364},{\"end\":68664,\"start\":68659},{\"end\":68685,\"start\":68676},{\"end\":68701,\"start\":68694},{\"end\":68939,\"start\":68934},{\"end\":68961,\"start\":68952},{\"end\":68974,\"start\":68970},{\"end\":68992,\"start\":68982},{\"end\":69007,\"start\":69002},{\"end\":69275,\"start\":69266},{\"end\":69296,\"start\":69284},{\"end\":69592,\"start\":69582},{\"end\":69607,\"start\":69599},{\"end\":69624,\"start\":69616},{\"end\":69639,\"start\":69633},{\"end\":69655,\"start\":69651},{\"end\":69892,\"start\":69882},{\"end\":69907,\"start\":69899},{\"end\":69925,\"start\":69916},{\"end\":69941,\"start\":69934},{\"end\":69957,\"start\":69953},{\"end\":70205,\"start\":70194},{\"end\":70216,\"start\":70212},{\"end\":70231,\"start\":70226},{\"end\":70246,\"start\":70239},{\"end\":70263,\"start\":70257},{\"end\":70280,\"start\":70272},{\"end\":70300,\"start\":70289},{\"end\":70314,\"start\":70309},{\"end\":70329,\"start\":70323},{\"end\":70337,\"start\":70331},{\"end\":70679,\"start\":70676},{\"end\":70706,\"start\":70688},{\"end\":70717,\"start\":70708},{\"end\":70744,\"start\":70726},{\"end\":70755,\"start\":70751},{\"end\":70761,\"start\":70757},{\"end\":71043,\"start\":71038},{\"end\":71061,\"start\":71051},{\"end\":71076,\"start\":71070},{\"end\":71092,\"start\":71087},{\"end\":71338,\"start\":71333},{\"end\":71351,\"start\":71346},{\"end\":71366,\"start\":71360},{\"end\":71379,\"start\":71374},{\"end\":71397,\"start\":71387},{\"end\":71412,\"start\":71406},{\"end\":71700,\"start\":71692},{\"end\":71718,\"start\":71709},{\"end\":71927,\"start\":71923},{\"end\":71944,\"start\":71935},{\"end\":71962,\"start\":71955},{\"end\":71979,\"start\":71973},{\"end\":72198,\"start\":72194},{\"end\":72215,\"start\":72206},{\"end\":72230,\"start\":72223},{\"end\":72248,\"start\":72241},{\"end\":72265,\"start\":72259},{\"end\":72466,\"start\":72462},{\"end\":72477,\"start\":72474},{\"end\":72783,\"start\":72774},{\"end\":72796,\"start\":72792},{\"end\":72824,\"start\":72806},{\"end\":72834,\"start\":72826},{\"end\":73069,\"start\":73060},{\"end\":73082,\"start\":73078},{\"end\":73095,\"start\":73090},{\"end\":73113,\"start\":73104},{\"end\":73129,\"start\":73122},{\"end\":73139,\"start\":73135},{\"end\":73152,\"start\":73148},{\"end\":73170,\"start\":73160},{\"end\":73494,\"start\":73485},{\"end\":73513,\"start\":73502},{\"end\":73526,\"start\":73521},{\"end\":73537,\"start\":73534},{\"end\":73553,\"start\":73546},{\"end\":73563,\"start\":73559},{\"end\":73576,\"start\":73572},{\"end\":73594,\"start\":73585},{\"end\":73612,\"start\":73602},{\"end\":73985,\"start\":73981},{\"end\":73995,\"start\":73991},{\"end\":74011,\"start\":74005},{\"end\":74022,\"start\":74020},{\"end\":74030,\"start\":74028},{\"end\":74042,\"start\":74035},{\"end\":74284,\"start\":74278},{\"end\":74300,\"start\":74295},{\"end\":74312,\"start\":74309},{\"end\":74332,\"start\":74323},{\"end\":74567,\"start\":74556},{\"end\":74584,\"start\":74577},{\"end\":74594,\"start\":74591},{\"end\":74610,\"start\":74603},{\"end\":74626,\"start\":74618},{\"end\":74643,\"start\":74637},{\"end\":74660,\"start\":74653},{\"end\":74674,\"start\":74669},{\"end\":74690,\"start\":74683},{\"end\":74696,\"start\":74692},{\"end\":75000,\"start\":74992},{\"end\":75012,\"start\":75008},{\"end\":75034,\"start\":75024},{\"end\":75216,\"start\":75206},{\"end\":75230,\"start\":75222},{\"end\":75247,\"start\":75236},{\"end\":75262,\"start\":75254},{\"end\":75275,\"start\":75271},{\"end\":75286,\"start\":75277},{\"end\":75623,\"start\":75615},{\"end\":75632,\"start\":75629},{\"end\":75643,\"start\":75639},{\"end\":75651,\"start\":75648},{\"end\":75663,\"start\":75659},{\"end\":75673,\"start\":75671},{\"end\":75683,\"start\":75680},{\"end\":75696,\"start\":75694},{\"end\":75709,\"start\":75705},{\"end\":75715,\"start\":75711},{\"end\":76010,\"start\":76004},{\"end\":76020,\"start\":76018},{\"end\":76035,\"start\":76031},{\"end\":76046,\"start\":76043},{\"end\":76067,\"start\":76057},{\"end\":76320,\"start\":76315},{\"end\":76341,\"start\":76329},{\"end\":76357,\"start\":76349},{\"end\":76369,\"start\":76359},{\"end\":76570,\"start\":76568},{\"end\":76581,\"start\":76578},{\"end\":76593,\"start\":76588},{\"end\":76605,\"start\":76595},{\"end\":76621,\"start\":76616},{\"end\":76630,\"start\":76627},{\"end\":76637,\"start\":76632},{\"end\":76883,\"start\":76878},{\"end\":76898,\"start\":76891},{\"end\":76913,\"start\":76908},{\"end\":76926,\"start\":76921},{\"end\":77191,\"start\":77186},{\"end\":77206,\"start\":77201},{\"end\":77222,\"start\":77216},{\"end\":77239,\"start\":77233},{\"end\":77249,\"start\":77243},{\"end\":77266,\"start\":77257},{\"end\":77284,\"start\":77273},{\"end\":77302,\"start\":77295},{\"end\":77330,\"start\":77319},{\"end\":77355,\"start\":77352},{\"end\":77361,\"start\":77357},{\"end\":77755,\"start\":77752},{\"end\":77766,\"start\":77762},{\"end\":77780,\"start\":77777},{\"end\":77792,\"start\":77788},{\"end\":77805,\"start\":77803},{\"end\":77816,\"start\":77813},{\"end\":78064,\"start\":78061},{\"end\":78076,\"start\":78071},{\"end\":78090,\"start\":78086},{\"end\":78097,\"start\":78095},{\"end\":78115,\"start\":78112},{\"end\":78129,\"start\":78126},{\"end\":78139,\"start\":78135},{\"end\":78393,\"start\":78391},{\"end\":78404,\"start\":78401},{\"end\":78414,\"start\":78412},{\"end\":78427,\"start\":78424},{\"end\":78442,\"start\":78434},{\"end\":78654,\"start\":78652},{\"end\":78669,\"start\":78664},{\"end\":78683,\"start\":78680},{\"end\":78693,\"start\":78690},{\"end\":78928,\"start\":78926},{\"end\":78940,\"start\":78935},{\"end\":78952,\"start\":78950},{\"end\":78964,\"start\":78961},{\"end\":79203,\"start\":79199},{\"end\":79218,\"start\":79209},{\"end\":79233,\"start\":79226},{\"end\":79248,\"start\":79241},{\"end\":79257,\"start\":79250},{\"end\":79479,\"start\":79474},{\"end\":79493,\"start\":79488},{\"end\":79509,\"start\":79504},{\"end\":79692,\"start\":79686},{\"end\":79700,\"start\":79696},{\"end\":79709,\"start\":79704},{\"end\":79719,\"start\":79713},{\"end\":79730,\"start\":79725},{\"end\":80121,\"start\":80100},{\"end\":80133,\"start\":80129},{\"end\":80140,\"start\":80135},{\"end\":80368,\"start\":80361},{\"end\":80380,\"start\":80375},{\"end\":80396,\"start\":80387},{\"end\":80409,\"start\":80404},{\"end\":80428,\"start\":80417},{\"end\":80447,\"start\":80438},{\"end\":80462,\"start\":80455},{\"end\":80477,\"start\":80470},{\"end\":80798,\"start\":80785},{\"end\":81018,\"start\":81015},{\"end\":81028,\"start\":81026},{\"end\":81039,\"start\":81036},{\"end\":81228,\"start\":81226},{\"end\":81236,\"start\":81234},{\"end\":81248,\"start\":81244},{\"end\":81262,\"start\":81257},{\"end\":81277,\"start\":81273},{\"end\":81289,\"start\":81286},{\"end\":81534,\"start\":81532},{\"end\":81546,\"start\":81543},{\"end\":81554,\"start\":81552},{\"end\":81565,\"start\":81562},{\"end\":81577,\"start\":81575},{\"end\":81802,\"start\":81800},{\"end\":81812,\"start\":81808},{\"end\":81827,\"start\":81822},{\"end\":81843,\"start\":81837},{\"end\":81855,\"start\":81849},{\"end\":81860,\"start\":81857},{\"end\":82107,\"start\":82105},{\"end\":82117,\"start\":82113},{\"end\":82128,\"start\":82124},{\"end\":82143,\"start\":82139},{\"end\":82365,\"start\":82363},{\"end\":82376,\"start\":82373},{\"end\":82403,\"start\":82387},{\"end\":82413,\"start\":82409},{\"end\":82431,\"start\":82426},{\"end\":82437,\"start\":82433},{\"end\":82710,\"start\":82705},{\"end\":82725,\"start\":82720},{\"end\":82732,\"start\":82730},{\"end\":82742,\"start\":82738},{\"end\":82751,\"start\":82749},{\"end\":82762,\"start\":82760},{\"end\":82776,\"start\":82773},{\"end\":83069,\"start\":83064},{\"end\":83082,\"start\":83076},{\"end\":83095,\"start\":83091},{\"end\":83301,\"start\":83298},{\"end\":83316,\"start\":83311},{\"end\":83332,\"start\":83324},{\"end\":83344,\"start\":83340},{\"end\":83359,\"start\":83353},{\"end\":83373,\"start\":83366},{\"end\":83387,\"start\":83381},{\"end\":83407,\"start\":83400},{\"end\":83670,\"start\":83667},{\"end\":83681,\"start\":83679},{\"end\":83691,\"start\":83688},{\"end\":83947,\"start\":83943},{\"end\":83959,\"start\":83952},{\"end\":83973,\"start\":83968},{\"end\":83986,\"start\":83981},{\"end\":84228,\"start\":84224},{\"end\":84240,\"start\":84235},{\"end\":84256,\"start\":84249},{\"end\":84439,\"start\":84433},{\"end\":84452,\"start\":84448},{\"end\":84471,\"start\":84462},{\"end\":84484,\"start\":84480},{\"end\":84500,\"start\":84494},{\"end\":84518,\"start\":84508},{\"end\":84533,\"start\":84528},{\"end\":84800,\"start\":84793},{\"end\":84812,\"start\":84807},{\"end\":84827,\"start\":84820},{\"end\":84962,\"start\":84957},{\"end\":84980,\"start\":84970},{\"end\":84990,\"start\":84986},{\"end\":85003,\"start\":84999},{\"end\":85021,\"start\":85012},{\"end\":85306,\"start\":85301},{\"end\":85330,\"start\":85316},{\"end\":85516,\"start\":85509},{\"end\":85531,\"start\":85525},{\"end\":85731,\"start\":85725},{\"end\":85742,\"start\":85738},{\"end\":85759,\"start\":85751},{\"end\":85777,\"start\":85768},{\"end\":85793,\"start\":85787},{\"end\":86071,\"start\":86067},{\"end\":86083,\"start\":86078},{\"end\":86314,\"start\":86310},{\"end\":86324,\"start\":86321},{\"end\":86339,\"start\":86336},{\"end\":86350,\"start\":86345},{\"end\":86604,\"start\":86591},{\"end\":86614,\"start\":86611},{\"end\":86627,\"start\":86625},{\"end\":86632,\"start\":86629},{\"end\":86856,\"start\":86852},{\"end\":86867,\"start\":86865},{\"end\":86880,\"start\":86875},{\"end\":86891,\"start\":86889},{\"end\":86900,\"start\":86898},{\"end\":87191,\"start\":87187},{\"end\":87205,\"start\":87201},{\"end\":87218,\"start\":87215},{\"end\":87227,\"start\":87225},{\"end\":87240,\"start\":87236},{\"end\":87250,\"start\":87247},{\"end\":87265,\"start\":87261},{\"end\":87275,\"start\":87273},{\"end\":87289,\"start\":87284},{\"end\":87300,\"start\":87298},{\"end\":87661,\"start\":87654},{\"end\":87675,\"start\":87665},{\"end\":87689,\"start\":87679},{\"end\":87701,\"start\":87693},{\"end\":87710,\"start\":87705},{\"end\":87729,\"start\":87714},{\"end\":88020,\"start\":88008},{\"end\":88029,\"start\":88027},{\"end\":88044,\"start\":88036},{\"end\":88049,\"start\":88046},{\"end\":88298,\"start\":88291},{\"end\":88316,\"start\":88310},{\"end\":88327,\"start\":88324},{\"end\":88343,\"start\":88334},{\"end\":88357,\"start\":88351},{\"end\":88620,\"start\":88613},{\"end\":88634,\"start\":88628},{\"end\":88837,\"start\":88828},{\"end\":88850,\"start\":88845},{\"end\":88866,\"start\":88861},{\"end\":88878,\"start\":88872},{\"end\":89069,\"start\":89063},{\"end\":89081,\"start\":89076},{\"end\":89093,\"start\":89090},{\"end\":89110,\"start\":89106},{\"end\":89331,\"start\":89327},{\"end\":89344,\"start\":89340},{\"end\":89629,\"start\":89626},{\"end\":89637,\"start\":89635},{\"end\":89646,\"start\":89644},{\"end\":89657,\"start\":89652},{\"end\":89668,\"start\":89666},{\"end\":89683,\"start\":89679},{\"end\":89693,\"start\":89690},{\"end\":90016,\"start\":90013},{\"end\":90029,\"start\":90024},{\"end\":90038,\"start\":90034},{\"end\":90047,\"start\":90043},{\"end\":90062,\"start\":90058},{\"end\":90335,\"start\":90331},{\"end\":90355,\"start\":90346},{\"end\":90370,\"start\":90364},{\"end\":90385,\"start\":90378},{\"end\":90613,\"start\":90609},{\"end\":90623,\"start\":90620},{\"end\":90634,\"start\":90629},{\"end\":90650,\"start\":90642},{\"end\":90667,\"start\":90661},{\"end\":90682,\"start\":90677},{\"end\":90939,\"start\":90931},{\"end\":90956,\"start\":90946},{\"end\":90972,\"start\":90963},{\"end\":90988,\"start\":90981},{\"end\":91004,\"start\":91000},{\"end\":91244,\"start\":91236},{\"end\":91261,\"start\":91251},{\"end\":91279,\"start\":91270},{\"end\":91288,\"start\":91285},{\"end\":91304,\"start\":91297},{\"end\":91316,\"start\":91310},{\"end\":91327,\"start\":91318},{\"end\":91339,\"start\":91333},{\"end\":91356,\"start\":91352},{\"end\":91364,\"start\":91358},{\"end\":91609,\"start\":91602},{\"end\":91623,\"start\":91616},{\"end\":91636,\"start\":91630},{\"end\":91653,\"start\":91644},{\"end\":91666,\"start\":91661},{\"end\":91681,\"start\":91676},{\"end\":91695,\"start\":91689},{\"end\":91707,\"start\":91697},{\"end\":91982,\"start\":91970},{\"end\":91995,\"start\":91991},{\"end\":92012,\"start\":92005},{\"end\":92026,\"start\":92022},{\"end\":92041,\"start\":92036},{\"end\":92059,\"start\":92055},{\"end\":92321,\"start\":92309},{\"end\":92337,\"start\":92331},{\"end\":92350,\"start\":92346},{\"end\":92367,\"start\":92361},{\"end\":92392,\"start\":92375},{\"end\":92407,\"start\":92402},{\"end\":92423,\"start\":92417},{\"end\":92430,\"start\":92425},{\"end\":92683,\"start\":92675},{\"end\":92704,\"start\":92693},{\"end\":92719,\"start\":92714},{\"end\":92738,\"start\":92728},{\"end\":92752,\"start\":92746},{\"end\":92940,\"start\":92936},{\"end\":92953,\"start\":92949},{\"end\":92962,\"start\":92960},{\"end\":92976,\"start\":92972},{\"end\":92985,\"start\":92982},{\"end\":92998,\"start\":92996},{\"end\":93198,\"start\":93194},{\"end\":93212,\"start\":93208},{\"end\":93225,\"start\":93221},{\"end\":93234,\"start\":93232},{\"end\":93243,\"start\":93240},{\"end\":93256,\"start\":93254},{\"end\":93494,\"start\":93490},{\"end\":93503,\"start\":93500},{\"end\":93518,\"start\":93514},{\"end\":93533,\"start\":93528},{\"end\":93545,\"start\":93543},{\"end\":93802,\"start\":93798},{\"end\":93812,\"start\":93807},{\"end\":93829,\"start\":93819},{\"end\":93841,\"start\":93839},{\"end\":93857,\"start\":93853},{\"end\":94094,\"start\":94090},{\"end\":94109,\"start\":94104},{\"end\":94301,\"start\":94297},{\"end\":94314,\"start\":94309},{\"end\":94330,\"start\":94325},{\"end\":94516,\"start\":94512},{\"end\":94529,\"start\":94524},{\"end\":94541,\"start\":94538},{\"end\":94556,\"start\":94552},{\"end\":94716,\"start\":94709},{\"end\":94727,\"start\":94724},{\"end\":94746,\"start\":94735},{\"end\":94760,\"start\":94754},{\"end\":94768,\"start\":94762},{\"end\":94980,\"start\":94970},{\"end\":94993,\"start\":94989},{\"end\":95012,\"start\":95003},{\"end\":95245,\"start\":95240},{\"end\":95258,\"start\":95252},{\"end\":95275,\"start\":95269},{\"end\":95487,\"start\":95485},{\"end\":95498,\"start\":95495},{\"end\":95514,\"start\":95510},{\"end\":95523,\"start\":95519},{\"end\":95534,\"start\":95530},{\"end\":95548,\"start\":95544},{\"end\":95756,\"start\":95754},{\"end\":95769,\"start\":95766},{\"end\":95786,\"start\":95782},{\"end\":95971,\"start\":95969},{\"end\":95986,\"start\":95981},{\"end\":95996,\"start\":95990},{\"end\":96006,\"start\":96004},{\"end\":96011,\"start\":96008},{\"end\":96236,\"start\":96233},{\"end\":96247,\"start\":96243},{\"end\":96260,\"start\":96256},{\"end\":96275,\"start\":96271},{\"end\":96284,\"start\":96282},{\"end\":96296,\"start\":96294},{\"end\":96306,\"start\":96303},{\"end\":96609,\"start\":96606},{\"end\":96621,\"start\":96618},{\"end\":96634,\"start\":96629},{\"end\":96643,\"start\":96640},{\"end\":96656,\"start\":96653},{\"end\":96664,\"start\":96662},{\"end\":96971,\"start\":96969},{\"end\":96986,\"start\":96982},{\"end\":97222,\"start\":97220},{\"end\":97235,\"start\":97231},{\"end\":97247,\"start\":97244},{\"end\":97262,\"start\":97258},{\"end\":97277,\"start\":97274},{\"end\":97291,\"start\":97286},{\"end\":97304,\"start\":97299},{\"end\":97317,\"start\":97312},{\"end\":97331,\"start\":97326},{\"end\":97626,\"start\":97624},{\"end\":97637,\"start\":97632},{\"end\":97647,\"start\":97644},{\"end\":97657,\"start\":97653},{\"end\":97669,\"start\":97664},{\"end\":97681,\"start\":97678},{\"end\":97693,\"start\":97689},{\"end\":97709,\"start\":97704},{\"end\":98045,\"start\":98041},{\"end\":98058,\"start\":98054},{\"end\":98072,\"start\":98069},{\"end\":98304,\"start\":98300},{\"end\":98316,\"start\":98313},{\"end\":98325,\"start\":98323},{\"end\":98487,\"start\":98483},{\"end\":98500,\"start\":98496},{\"end\":98514,\"start\":98509},{\"end\":98529,\"start\":98525},{\"end\":98552,\"start\":98541},{\"end\":98789,\"start\":98785},{\"end\":98802,\"start\":98798},{\"end\":98816,\"start\":98811},{\"end\":98831,\"start\":98827},{\"end\":98854,\"start\":98843},{\"end\":99118,\"start\":99116},{\"end\":99126,\"start\":99124},{\"end\":99135,\"start\":99132},{\"end\":99149,\"start\":99145},{\"end\":99405,\"start\":99398},{\"end\":99414,\"start\":99410},{\"end\":99427,\"start\":99422},{\"end\":99439,\"start\":99434},{\"end\":99454,\"start\":99450},{\"end\":99748,\"start\":99743},{\"end\":99759,\"start\":99756},{\"end\":99773,\"start\":99770},{\"end\":99787,\"start\":99783},{\"end\":100065,\"start\":100060},{\"end\":100080,\"start\":100075},{\"end\":100096,\"start\":100091},{\"end\":100346,\"start\":100341},{\"end\":100359,\"start\":100355},{\"end\":100374,\"start\":100370},{\"end\":100387,\"start\":100383},{\"end\":100398,\"start\":100395},{\"end\":100679,\"start\":100674},{\"end\":100688,\"start\":100686},{\"end\":100700,\"start\":100698},{\"end\":100714,\"start\":100710},{\"end\":100938,\"start\":100933},{\"end\":100950,\"start\":100946},{\"end\":100959,\"start\":100955},{\"end\":100974,\"start\":100970},{\"end\":100989,\"start\":100985},{\"end\":100998,\"start\":100994},{\"end\":101195,\"start\":101191},{\"end\":101206,\"start\":101202},{\"end\":101219,\"start\":101215},{\"end\":101233,\"start\":101227},{\"end\":101249,\"start\":101241},{\"end\":101267,\"start\":101259},{\"end\":101456,\"start\":101452},{\"end\":101472,\"start\":101466},{\"end\":101492,\"start\":101482},{\"end\":101674,\"start\":101671},{\"end\":101686,\"start\":101683},{\"end\":101698,\"start\":101695},{\"end\":101709,\"start\":101705},{\"end\":101723,\"start\":101719},{\"end\":102852,\"start\":102841}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":68195,\"start\":68110},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":4768761},\"end\":68573,\"start\":68197},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":207930156},\"end\":68855,\"start\":68575},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":213175621},\"end\":69187,\"start\":68857},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":13567980},\"end\":69520,\"start\":69189},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":780697},\"end\":69818,\"start\":69522},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":14309034},\"end\":70144,\"start\":69820},{\"attributes\":{\"doi\":\"arXiv:2005.14165\",\"id\":\"b7\"},\"end\":70578,\"start\":70146},{\"attributes\":{\"id\":\"b8\"},\"end\":70965,\"start\":70580},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":49865868},\"end\":71245,\"start\":70967},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":219721240},\"end\":71616,\"start\":71247},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":206596127},\"end\":71845,\"start\":71618},{\"attributes\":{\"id\":\"b12\"},\"end\":72124,\"start\":71847},{\"attributes\":{\"doi\":\"arXiv:2006.10029\",\"id\":\"b13\"},\"end\":72453,\"start\":72126},{\"attributes\":{\"doi\":\"arXiv:2003.04297\",\"id\":\"b14\"},\"end\":72714,\"start\":72455},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":14958161},\"end\":72980,\"start\":72716},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":222378581},\"end\":73475,\"start\":72982},{\"attributes\":{\"doi\":\"arXiv:2003.09003\",\"id\":\"b17\"},\"end\":73922,\"start\":73477},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":57246310},\"end\":74188,\"start\":73924},{\"attributes\":{\"id\":\"b19\"},\"end\":74487,\"start\":74190},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":12552176},\"end\":74930,\"start\":74489},{\"attributes\":{\"id\":\"b21\"},\"end\":75149,\"start\":74932},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":4246903},\"end\":75534,\"start\":75151},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":52350875},\"end\":75922,\"start\":75536},{\"attributes\":{\"doi\":\"arXiv:2103.07503\",\"id\":\"b24\"},\"end\":76257,\"start\":75924},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":6735187},\"end\":76506,\"start\":76259},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":232478630},\"end\":76797,\"start\":76508},{\"attributes\":{\"doi\":\"arXiv:1905.01235\",\"id\":\"b27\"},\"end\":77100,\"start\":76799},{\"attributes\":{\"doi\":\"arXiv:2006.07733\",\"id\":\"b28\"},\"end\":77669,\"start\":77102},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":53043023},\"end\":78009,\"start\":77671},{\"attributes\":{\"doi\":\"arXiv:2009.04794\",\"id\":\"b30\"},\"end\":78314,\"start\":78011},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":207930212},\"end\":78596,\"start\":78316},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":206594692},\"end\":78816,\"start\":78598},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":1041123},\"end\":79138,\"start\":78818},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":5378407},\"end\":79410,\"start\":79140},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":220056011},\"end\":79640,\"start\":79412},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":13000587},\"end\":79852,\"start\":79642},{\"attributes\":{\"id\":\"b37\"},\"end\":80024,\"start\":79854},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":15665411},\"end\":80282,\"start\":80026},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":1118174},\"end\":80734,\"start\":80284},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":9426884},\"end\":80956,\"start\":80736},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":211146111},\"end\":81148,\"start\":80958},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":57189581},\"end\":81456,\"start\":81150},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":52255840},\"end\":81724,\"start\":81458},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":218581596},\"end\":82024,\"start\":81726},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":938105},\"end\":82288,\"start\":82026},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":202785608},\"end\":82618,\"start\":82290},{\"attributes\":{\"doi\":\"arXiv:2010.12138\",\"id\":\"b47\"},\"end\":82979,\"start\":82620},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":6094550},\"end\":83244,\"start\":82981},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":14113767},\"end\":83596,\"start\":83246},{\"attributes\":{\"doi\":\"arXiv:2011.13677\",\"id\":\"b50\"},\"end\":83831,\"start\":83598},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":52196839},\"end\":84167,\"start\":83833},{\"attributes\":{\"id\":\"b52\"},\"end\":84356,\"start\":84169},{\"attributes\":{\"id\":\"b53\"},\"end\":84731,\"start\":84358},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":208175650},\"end\":84949,\"start\":84733},{\"attributes\":{\"doi\":\"arXiv:1603.00831\",\"id\":\"b55\"},\"end\":85230,\"start\":84951},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":208617491},\"end\":85455,\"start\":85232},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":4100681},\"end\":85632,\"start\":85457},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":4455970},\"end\":85983,\"start\":85634},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":147704237},\"end\":86228,\"start\":85985},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":59158855},\"end\":86518,\"start\":86230},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":90262243},\"end\":86772,\"start\":86520},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":219559121},\"end\":87054,\"start\":86774},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":220845444},\"end\":87572,\"start\":87056},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":1949934},\"end\":87918,\"start\":87574},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":10328909},\"end\":88206,\"start\":87920},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":5584770},\"end\":88534,\"start\":88208},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":4462331},\"end\":88759,\"start\":88536},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":545361},\"end\":89023,\"start\":88761},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":208637499},\"end\":89223,\"start\":89025},{\"attributes\":{\"doi\":\"arXiv:2009.00100\",\"id\":\"b70\"},\"end\":89509,\"start\":89225},{\"attributes\":{\"id\":\"b71\",\"matched_paper_id\":90260003},\"end\":89903,\"start\":89511},{\"attributes\":{\"id\":\"b72\",\"matched_paper_id\":10013306},\"end\":90250,\"start\":89905},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":9615667},\"end\":90546,\"start\":90252},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":218719252},\"end\":90850,\"start\":90548},{\"attributes\":{\"id\":\"b75\",\"matched_paper_id\":10520310},\"end\":91184,\"start\":90852},{\"attributes\":{\"id\":\"b76\",\"matched_paper_id\":4305586},\"end\":91566,\"start\":91186},{\"attributes\":{\"id\":\"b77\",\"matched_paper_id\":13756489},\"end\":91888,\"start\":91568},{\"attributes\":{\"id\":\"b78\",\"matched_paper_id\":67856723},\"end\":92256,\"start\":91890},{\"attributes\":{\"id\":\"b79\",\"matched_paper_id\":60440659},\"end\":92629,\"start\":92258},{\"attributes\":{\"id\":\"b80\",\"matched_paper_id\":49405781},\"end\":92901,\"start\":92631},{\"attributes\":{\"id\":\"b81\",\"matched_paper_id\":102487133},\"end\":93121,\"start\":92903},{\"attributes\":{\"id\":\"b82\"},\"end\":93412,\"start\":93123},{\"attributes\":{\"doi\":\"arXiv:1704.04057\",\"id\":\"b83\"},\"end\":93723,\"start\":93414},{\"attributes\":{\"id\":\"b84\",\"matched_paper_id\":54475412},\"end\":94017,\"start\":93725},{\"attributes\":{\"id\":\"b85\",\"matched_paper_id\":2057504},\"end\":94226,\"start\":94019},{\"attributes\":{\"id\":\"b86\",\"matched_paper_id\":81983505},\"end\":94460,\"start\":94228},{\"attributes\":{\"id\":\"b87\",\"matched_paper_id\":203591717},\"end\":94678,\"start\":94462},{\"attributes\":{\"id\":\"b88\",\"matched_paper_id\":163946},\"end\":94883,\"start\":94680},{\"attributes\":{\"id\":\"b89\"},\"end\":95162,\"start\":94885},{\"attributes\":{\"id\":\"b90\",\"matched_paper_id\":74506},\"end\":95414,\"start\":95164},{\"attributes\":{\"id\":\"b91\",\"matched_paper_id\":232240682},\"end\":95712,\"start\":95416},{\"attributes\":{\"id\":\"b92\",\"matched_paper_id\":1660289},\"end\":95886,\"start\":95714},{\"attributes\":{\"id\":\"b93\"},\"end\":96163,\"start\":95888},{\"attributes\":{\"doi\":\"arXiv:2102.04803\",\"id\":\"b94\"},\"end\":96494,\"start\":96165},{\"attributes\":{\"id\":\"b95\",\"matched_paper_id\":227054503},\"end\":96864,\"start\":96496},{\"attributes\":{\"doi\":\"arXiv:2103.17263\",\"id\":\"b96\"},\"end\":97152,\"start\":96866},{\"attributes\":{\"id\":\"b97\",\"matched_paper_id\":52154988},\"end\":97536,\"start\":97154},{\"attributes\":{\"id\":\"b98\",\"matched_paper_id\":220347250},\"end\":97908,\"start\":97538},{\"attributes\":{\"id\":\"b99\",\"matched_paper_id\":5324675},\"end\":98262,\"start\":97910},{\"attributes\":{\"id\":\"b100\",\"matched_paper_id\":152282473},\"end\":98414,\"start\":98264},{\"attributes\":{\"id\":\"b101\",\"matched_paper_id\":3655063},\"end\":98716,\"start\":98416},{\"attributes\":{\"id\":\"b102\",\"matched_paper_id\":3655063},\"end\":99018,\"start\":98718},{\"attributes\":{\"id\":\"b103\",\"matched_paper_id\":59336928},\"end\":99326,\"start\":99020},{\"attributes\":{\"doi\":\"arXiv:2103.03230\",\"id\":\"b104\"},\"end\":99627,\"start\":99328},{\"attributes\":{\"id\":\"b105\",\"matched_paper_id\":212725229},\"end\":99973,\"start\":99629},{\"attributes\":{\"id\":\"b106\",\"matched_paper_id\":9658690},\"end\":100245,\"start\":99975},{\"attributes\":{\"doi\":\"arXiv:2004.01888\",\"id\":\"b107\"},\"end\":100596,\"start\":100247},{\"attributes\":{\"doi\":\"arXiv:2008.02745\",\"id\":\"b108\"},\"end\":100877,\"start\":100598},{\"attributes\":{\"id\":\"b109\",\"matched_paper_id\":14991802},\"end\":101145,\"start\":100879},{\"attributes\":{\"id\":\"b110\",\"matched_paper_id\":5636055},\"end\":101415,\"start\":101147},{\"attributes\":{\"id\":\"b111\",\"matched_paper_id\":214775104},\"end\":101595,\"start\":101417},{\"attributes\":{\"id\":\"b112\",\"matched_paper_id\":220793215},\"end\":102486,\"start\":101597},{\"attributes\":{\"id\":\"b113\"},\"end\":102562,\"start\":102488},{\"attributes\":{\"id\":\"b114\"},\"end\":102614,\"start\":102564},{\"attributes\":{\"id\":\"b115\"},\"end\":102815,\"start\":102616},{\"attributes\":{\"id\":\"b116\"},\"end\":102891,\"start\":102817}]", "bib_title": "[{\"end\":68258,\"start\":68197},{\"end\":68645,\"start\":68575},{\"end\":68928,\"start\":68857},{\"end\":69259,\"start\":69189},{\"end\":69575,\"start\":69522},{\"end\":69875,\"start\":69820},{\"end\":71027,\"start\":70967},{\"end\":71322,\"start\":71247},{\"end\":71685,\"start\":71618},{\"end\":72765,\"start\":72716},{\"end\":73050,\"start\":72982},{\"end\":73975,\"start\":73924},{\"end\":74547,\"start\":74489},{\"end\":75199,\"start\":75151},{\"end\":75606,\"start\":75536},{\"end\":76311,\"start\":76259},{\"end\":76561,\"start\":76508},{\"end\":77742,\"start\":77671},{\"end\":78381,\"start\":78316},{\"end\":78642,\"start\":78598},{\"end\":78915,\"start\":78818},{\"end\":79195,\"start\":79140},{\"end\":79466,\"start\":79412},{\"end\":79682,\"start\":79642},{\"end\":80091,\"start\":80026},{\"end\":80353,\"start\":80284},{\"end\":80783,\"start\":80736},{\"end\":81006,\"start\":80958},{\"end\":81221,\"start\":81150},{\"end\":81527,\"start\":81458},{\"end\":81791,\"start\":81726},{\"end\":82099,\"start\":82026},{\"end\":82353,\"start\":82290},{\"end\":83053,\"start\":82981},{\"end\":83287,\"start\":83246},{\"end\":83936,\"start\":83833},{\"end\":84786,\"start\":84733},{\"end\":85293,\"start\":85232},{\"end\":85499,\"start\":85457},{\"end\":85714,\"start\":85634},{\"end\":86056,\"start\":85985},{\"end\":86299,\"start\":86230},{\"end\":86578,\"start\":86520},{\"end\":86847,\"start\":86774},{\"end\":87177,\"start\":87056},{\"end\":87650,\"start\":87574},{\"end\":87998,\"start\":87920},{\"end\":88283,\"start\":88208},{\"end\":88605,\"start\":88536},{\"end\":88819,\"start\":88761},{\"end\":89053,\"start\":89025},{\"end\":89618,\"start\":89511},{\"end\":90005,\"start\":89905},{\"end\":90324,\"start\":90252},{\"end\":90598,\"start\":90548},{\"end\":90924,\"start\":90852},{\"end\":91229,\"start\":91186},{\"end\":91593,\"start\":91568},{\"end\":91963,\"start\":91890},{\"end\":92302,\"start\":92258},{\"end\":92668,\"start\":92631},{\"end\":92929,\"start\":92903},{\"end\":93790,\"start\":93725},{\"end\":94079,\"start\":94019},{\"end\":94286,\"start\":94228},{\"end\":94501,\"start\":94462},{\"end\":94707,\"start\":94680},{\"end\":95230,\"start\":95164},{\"end\":95475,\"start\":95416},{\"end\":95749,\"start\":95714},{\"end\":95959,\"start\":95888},{\"end\":96597,\"start\":96496},{\"end\":97213,\"start\":97154},{\"end\":97615,\"start\":97538},{\"end\":98035,\"start\":97910},{\"end\":98291,\"start\":98264},{\"end\":98474,\"start\":98416},{\"end\":98776,\"start\":98718},{\"end\":99105,\"start\":99020},{\"end\":99737,\"start\":99629},{\"end\":100050,\"start\":99975},{\"end\":100925,\"start\":100879},{\"end\":101183,\"start\":101147},{\"end\":101443,\"start\":101417},{\"end\":101664,\"start\":101597}]", "bib_author": "[{\"end\":68130,\"start\":68112},{\"end\":68280,\"start\":68260},{\"end\":68292,\"start\":68280},{\"end\":68312,\"start\":68292},{\"end\":68331,\"start\":68312},{\"end\":68344,\"start\":68331},{\"end\":68358,\"start\":68344},{\"end\":68373,\"start\":68358},{\"end\":68666,\"start\":68647},{\"end\":68687,\"start\":68666},{\"end\":68703,\"start\":68687},{\"end\":68941,\"start\":68930},{\"end\":68963,\"start\":68941},{\"end\":68976,\"start\":68963},{\"end\":68994,\"start\":68976},{\"end\":69009,\"start\":68994},{\"end\":69277,\"start\":69261},{\"end\":69298,\"start\":69277},{\"end\":69594,\"start\":69577},{\"end\":69609,\"start\":69594},{\"end\":69626,\"start\":69609},{\"end\":69641,\"start\":69626},{\"end\":69657,\"start\":69641},{\"end\":69894,\"start\":69877},{\"end\":69909,\"start\":69894},{\"end\":69927,\"start\":69909},{\"end\":69943,\"start\":69927},{\"end\":69959,\"start\":69943},{\"end\":70207,\"start\":70185},{\"end\":70218,\"start\":70207},{\"end\":70233,\"start\":70218},{\"end\":70248,\"start\":70233},{\"end\":70265,\"start\":70248},{\"end\":70282,\"start\":70265},{\"end\":70302,\"start\":70282},{\"end\":70316,\"start\":70302},{\"end\":70331,\"start\":70316},{\"end\":70339,\"start\":70331},{\"end\":70681,\"start\":70670},{\"end\":70708,\"start\":70681},{\"end\":70719,\"start\":70708},{\"end\":70746,\"start\":70719},{\"end\":70757,\"start\":70746},{\"end\":70763,\"start\":70757},{\"end\":71045,\"start\":71029},{\"end\":71063,\"start\":71045},{\"end\":71078,\"start\":71063},{\"end\":71094,\"start\":71078},{\"end\":71340,\"start\":71324},{\"end\":71353,\"start\":71340},{\"end\":71368,\"start\":71353},{\"end\":71381,\"start\":71368},{\"end\":71399,\"start\":71381},{\"end\":71414,\"start\":71399},{\"end\":71702,\"start\":71687},{\"end\":71720,\"start\":71702},{\"end\":71929,\"start\":71918},{\"end\":71946,\"start\":71929},{\"end\":71964,\"start\":71946},{\"end\":71981,\"start\":71964},{\"end\":72200,\"start\":72189},{\"end\":72217,\"start\":72200},{\"end\":72232,\"start\":72217},{\"end\":72250,\"start\":72232},{\"end\":72267,\"start\":72250},{\"end\":72468,\"start\":72455},{\"end\":72479,\"start\":72468},{\"end\":72785,\"start\":72767},{\"end\":72798,\"start\":72785},{\"end\":72826,\"start\":72798},{\"end\":72836,\"start\":72826},{\"end\":73071,\"start\":73052},{\"end\":73084,\"start\":73071},{\"end\":73097,\"start\":73084},{\"end\":73115,\"start\":73097},{\"end\":73131,\"start\":73115},{\"end\":73141,\"start\":73131},{\"end\":73154,\"start\":73141},{\"end\":73172,\"start\":73154},{\"end\":73496,\"start\":73477},{\"end\":73515,\"start\":73496},{\"end\":73528,\"start\":73515},{\"end\":73539,\"start\":73528},{\"end\":73555,\"start\":73539},{\"end\":73565,\"start\":73555},{\"end\":73578,\"start\":73565},{\"end\":73596,\"start\":73578},{\"end\":73614,\"start\":73596},{\"end\":73987,\"start\":73977},{\"end\":73997,\"start\":73987},{\"end\":74013,\"start\":73997},{\"end\":74024,\"start\":74013},{\"end\":74032,\"start\":74024},{\"end\":74044,\"start\":74032},{\"end\":74286,\"start\":74272},{\"end\":74302,\"start\":74286},{\"end\":74314,\"start\":74302},{\"end\":74334,\"start\":74314},{\"end\":74569,\"start\":74549},{\"end\":74586,\"start\":74569},{\"end\":74596,\"start\":74586},{\"end\":74612,\"start\":74596},{\"end\":74628,\"start\":74612},{\"end\":74645,\"start\":74628},{\"end\":74662,\"start\":74645},{\"end\":74676,\"start\":74662},{\"end\":74692,\"start\":74676},{\"end\":74698,\"start\":74692},{\"end\":75002,\"start\":74986},{\"end\":75014,\"start\":75002},{\"end\":75036,\"start\":75014},{\"end\":75218,\"start\":75201},{\"end\":75232,\"start\":75218},{\"end\":75249,\"start\":75232},{\"end\":75264,\"start\":75249},{\"end\":75277,\"start\":75264},{\"end\":75288,\"start\":75277},{\"end\":75625,\"start\":75608},{\"end\":75634,\"start\":75625},{\"end\":75645,\"start\":75634},{\"end\":75653,\"start\":75645},{\"end\":75665,\"start\":75653},{\"end\":75675,\"start\":75665},{\"end\":75685,\"start\":75675},{\"end\":75698,\"start\":75685},{\"end\":75711,\"start\":75698},{\"end\":75717,\"start\":75711},{\"end\":76012,\"start\":75997},{\"end\":76022,\"start\":76012},{\"end\":76037,\"start\":76022},{\"end\":76048,\"start\":76037},{\"end\":76069,\"start\":76048},{\"end\":76322,\"start\":76313},{\"end\":76343,\"start\":76322},{\"end\":76359,\"start\":76343},{\"end\":76371,\"start\":76359},{\"end\":76572,\"start\":76563},{\"end\":76583,\"start\":76572},{\"end\":76595,\"start\":76583},{\"end\":76607,\"start\":76595},{\"end\":76623,\"start\":76607},{\"end\":76632,\"start\":76623},{\"end\":76639,\"start\":76632},{\"end\":76885,\"start\":76872},{\"end\":76900,\"start\":76885},{\"end\":76915,\"start\":76900},{\"end\":76928,\"start\":76915},{\"end\":77193,\"start\":77173},{\"end\":77208,\"start\":77193},{\"end\":77224,\"start\":77208},{\"end\":77241,\"start\":77224},{\"end\":77251,\"start\":77241},{\"end\":77268,\"start\":77251},{\"end\":77286,\"start\":77268},{\"end\":77304,\"start\":77286},{\"end\":77332,\"start\":77304},{\"end\":77357,\"start\":77332},{\"end\":77363,\"start\":77357},{\"end\":77757,\"start\":77744},{\"end\":77768,\"start\":77757},{\"end\":77782,\"start\":77768},{\"end\":77794,\"start\":77782},{\"end\":77807,\"start\":77794},{\"end\":77818,\"start\":77807},{\"end\":78066,\"start\":78052},{\"end\":78078,\"start\":78066},{\"end\":78092,\"start\":78078},{\"end\":78099,\"start\":78092},{\"end\":78117,\"start\":78099},{\"end\":78131,\"start\":78117},{\"end\":78141,\"start\":78131},{\"end\":78395,\"start\":78383},{\"end\":78406,\"start\":78395},{\"end\":78416,\"start\":78406},{\"end\":78429,\"start\":78416},{\"end\":78444,\"start\":78429},{\"end\":78656,\"start\":78644},{\"end\":78671,\"start\":78656},{\"end\":78685,\"start\":78671},{\"end\":78695,\"start\":78685},{\"end\":78930,\"start\":78917},{\"end\":78942,\"start\":78930},{\"end\":78954,\"start\":78942},{\"end\":78966,\"start\":78954},{\"end\":79205,\"start\":79197},{\"end\":79220,\"start\":79205},{\"end\":79235,\"start\":79220},{\"end\":79250,\"start\":79235},{\"end\":79259,\"start\":79250},{\"end\":79481,\"start\":79468},{\"end\":79495,\"start\":79481},{\"end\":79511,\"start\":79495},{\"end\":79694,\"start\":79684},{\"end\":79702,\"start\":79694},{\"end\":79711,\"start\":79702},{\"end\":79721,\"start\":79711},{\"end\":79732,\"start\":79721},{\"end\":79929,\"start\":79914},{\"end\":79938,\"start\":79929},{\"end\":80123,\"start\":80093},{\"end\":80135,\"start\":80123},{\"end\":80142,\"start\":80135},{\"end\":80370,\"start\":80355},{\"end\":80382,\"start\":80370},{\"end\":80398,\"start\":80382},{\"end\":80411,\"start\":80398},{\"end\":80430,\"start\":80411},{\"end\":80449,\"start\":80430},{\"end\":80464,\"start\":80449},{\"end\":80479,\"start\":80464},{\"end\":80493,\"start\":80479},{\"end\":80800,\"start\":80785},{\"end\":81020,\"start\":81008},{\"end\":81030,\"start\":81020},{\"end\":81041,\"start\":81030},{\"end\":81230,\"start\":81223},{\"end\":81238,\"start\":81230},{\"end\":81250,\"start\":81238},{\"end\":81264,\"start\":81250},{\"end\":81279,\"start\":81264},{\"end\":81291,\"start\":81279},{\"end\":81536,\"start\":81529},{\"end\":81548,\"start\":81536},{\"end\":81556,\"start\":81548},{\"end\":81567,\"start\":81556},{\"end\":81579,\"start\":81567},{\"end\":81804,\"start\":81793},{\"end\":81814,\"start\":81804},{\"end\":81829,\"start\":81814},{\"end\":81845,\"start\":81829},{\"end\":81857,\"start\":81845},{\"end\":81862,\"start\":81857},{\"end\":82109,\"start\":82101},{\"end\":82119,\"start\":82109},{\"end\":82130,\"start\":82119},{\"end\":82145,\"start\":82130},{\"end\":82367,\"start\":82355},{\"end\":82378,\"start\":82367},{\"end\":82405,\"start\":82378},{\"end\":82415,\"start\":82405},{\"end\":82433,\"start\":82415},{\"end\":82439,\"start\":82433},{\"end\":82712,\"start\":82700},{\"end\":82727,\"start\":82712},{\"end\":82734,\"start\":82727},{\"end\":82744,\"start\":82734},{\"end\":82753,\"start\":82744},{\"end\":82764,\"start\":82753},{\"end\":82778,\"start\":82764},{\"end\":83071,\"start\":83055},{\"end\":83084,\"start\":83071},{\"end\":83097,\"start\":83084},{\"end\":83303,\"start\":83289},{\"end\":83318,\"start\":83303},{\"end\":83334,\"start\":83318},{\"end\":83346,\"start\":83334},{\"end\":83361,\"start\":83346},{\"end\":83375,\"start\":83361},{\"end\":83389,\"start\":83375},{\"end\":83409,\"start\":83389},{\"end\":83672,\"start\":83659},{\"end\":83683,\"start\":83672},{\"end\":83693,\"start\":83683},{\"end\":83949,\"start\":83938},{\"end\":83961,\"start\":83949},{\"end\":83975,\"start\":83961},{\"end\":83988,\"start\":83975},{\"end\":84230,\"start\":84215},{\"end\":84242,\"start\":84230},{\"end\":84258,\"start\":84242},{\"end\":84441,\"start\":84424},{\"end\":84454,\"start\":84441},{\"end\":84473,\"start\":84454},{\"end\":84486,\"start\":84473},{\"end\":84502,\"start\":84486},{\"end\":84520,\"start\":84502},{\"end\":84535,\"start\":84520},{\"end\":84802,\"start\":84788},{\"end\":84814,\"start\":84802},{\"end\":84829,\"start\":84814},{\"end\":84964,\"start\":84951},{\"end\":84982,\"start\":84964},{\"end\":84992,\"start\":84982},{\"end\":85005,\"start\":84992},{\"end\":85023,\"start\":85005},{\"end\":85308,\"start\":85295},{\"end\":85332,\"start\":85308},{\"end\":85518,\"start\":85501},{\"end\":85533,\"start\":85518},{\"end\":85733,\"start\":85716},{\"end\":85744,\"start\":85733},{\"end\":85761,\"start\":85744},{\"end\":85779,\"start\":85761},{\"end\":85795,\"start\":85779},{\"end\":86073,\"start\":86058},{\"end\":86085,\"start\":86073},{\"end\":86316,\"start\":86301},{\"end\":86326,\"start\":86316},{\"end\":86341,\"start\":86326},{\"end\":86352,\"start\":86341},{\"end\":86606,\"start\":86580},{\"end\":86616,\"start\":86606},{\"end\":86629,\"start\":86616},{\"end\":86634,\"start\":86629},{\"end\":86858,\"start\":86849},{\"end\":86869,\"start\":86858},{\"end\":86882,\"start\":86869},{\"end\":86893,\"start\":86882},{\"end\":86902,\"start\":86893},{\"end\":87193,\"start\":87179},{\"end\":87207,\"start\":87193},{\"end\":87220,\"start\":87207},{\"end\":87229,\"start\":87220},{\"end\":87242,\"start\":87229},{\"end\":87252,\"start\":87242},{\"end\":87267,\"start\":87252},{\"end\":87277,\"start\":87267},{\"end\":87291,\"start\":87277},{\"end\":87302,\"start\":87291},{\"end\":87663,\"start\":87652},{\"end\":87677,\"start\":87663},{\"end\":87691,\"start\":87677},{\"end\":87703,\"start\":87691},{\"end\":87712,\"start\":87703},{\"end\":87731,\"start\":87712},{\"end\":88022,\"start\":88000},{\"end\":88031,\"start\":88022},{\"end\":88046,\"start\":88031},{\"end\":88051,\"start\":88046},{\"end\":88300,\"start\":88285},{\"end\":88318,\"start\":88300},{\"end\":88329,\"start\":88318},{\"end\":88345,\"start\":88329},{\"end\":88359,\"start\":88345},{\"end\":88622,\"start\":88607},{\"end\":88636,\"start\":88622},{\"end\":88839,\"start\":88821},{\"end\":88852,\"start\":88839},{\"end\":88868,\"start\":88852},{\"end\":88880,\"start\":88868},{\"end\":89071,\"start\":89055},{\"end\":89083,\"start\":89071},{\"end\":89095,\"start\":89083},{\"end\":89112,\"start\":89095},{\"end\":89333,\"start\":89317},{\"end\":89346,\"start\":89333},{\"end\":89631,\"start\":89620},{\"end\":89639,\"start\":89631},{\"end\":89648,\"start\":89639},{\"end\":89659,\"start\":89648},{\"end\":89670,\"start\":89659},{\"end\":89685,\"start\":89670},{\"end\":89695,\"start\":89685},{\"end\":90018,\"start\":90007},{\"end\":90031,\"start\":90018},{\"end\":90040,\"start\":90031},{\"end\":90049,\"start\":90040},{\"end\":90064,\"start\":90049},{\"end\":90337,\"start\":90326},{\"end\":90357,\"start\":90337},{\"end\":90372,\"start\":90357},{\"end\":90387,\"start\":90372},{\"end\":90615,\"start\":90600},{\"end\":90625,\"start\":90615},{\"end\":90636,\"start\":90625},{\"end\":90652,\"start\":90636},{\"end\":90669,\"start\":90652},{\"end\":90684,\"start\":90669},{\"end\":90941,\"start\":90926},{\"end\":90958,\"start\":90941},{\"end\":90974,\"start\":90958},{\"end\":90990,\"start\":90974},{\"end\":91006,\"start\":90990},{\"end\":91246,\"start\":91231},{\"end\":91263,\"start\":91246},{\"end\":91281,\"start\":91263},{\"end\":91290,\"start\":91281},{\"end\":91306,\"start\":91290},{\"end\":91318,\"start\":91306},{\"end\":91329,\"start\":91318},{\"end\":91341,\"start\":91329},{\"end\":91358,\"start\":91341},{\"end\":91366,\"start\":91358},{\"end\":91611,\"start\":91595},{\"end\":91625,\"start\":91611},{\"end\":91638,\"start\":91625},{\"end\":91655,\"start\":91638},{\"end\":91668,\"start\":91655},{\"end\":91683,\"start\":91668},{\"end\":91697,\"start\":91683},{\"end\":91709,\"start\":91697},{\"end\":91984,\"start\":91965},{\"end\":91997,\"start\":91984},{\"end\":92014,\"start\":91997},{\"end\":92028,\"start\":92014},{\"end\":92043,\"start\":92028},{\"end\":92061,\"start\":92043},{\"end\":92323,\"start\":92304},{\"end\":92339,\"start\":92323},{\"end\":92352,\"start\":92339},{\"end\":92369,\"start\":92352},{\"end\":92394,\"start\":92369},{\"end\":92409,\"start\":92394},{\"end\":92425,\"start\":92409},{\"end\":92432,\"start\":92425},{\"end\":92685,\"start\":92670},{\"end\":92706,\"start\":92685},{\"end\":92721,\"start\":92706},{\"end\":92740,\"start\":92721},{\"end\":92754,\"start\":92740},{\"end\":92942,\"start\":92931},{\"end\":92955,\"start\":92942},{\"end\":92964,\"start\":92955},{\"end\":92978,\"start\":92964},{\"end\":92987,\"start\":92978},{\"end\":93000,\"start\":92987},{\"end\":93200,\"start\":93189},{\"end\":93214,\"start\":93200},{\"end\":93227,\"start\":93214},{\"end\":93236,\"start\":93227},{\"end\":93245,\"start\":93236},{\"end\":93258,\"start\":93245},{\"end\":93496,\"start\":93484},{\"end\":93505,\"start\":93496},{\"end\":93520,\"start\":93505},{\"end\":93535,\"start\":93520},{\"end\":93547,\"start\":93535},{\"end\":93804,\"start\":93792},{\"end\":93814,\"start\":93804},{\"end\":93831,\"start\":93814},{\"end\":93843,\"start\":93831},{\"end\":93859,\"start\":93843},{\"end\":94096,\"start\":94081},{\"end\":94111,\"start\":94096},{\"end\":94303,\"start\":94288},{\"end\":94316,\"start\":94303},{\"end\":94332,\"start\":94316},{\"end\":94518,\"start\":94503},{\"end\":94531,\"start\":94518},{\"end\":94543,\"start\":94531},{\"end\":94558,\"start\":94543},{\"end\":94718,\"start\":94709},{\"end\":94729,\"start\":94718},{\"end\":94748,\"start\":94729},{\"end\":94762,\"start\":94748},{\"end\":94770,\"start\":94762},{\"end\":94982,\"start\":94964},{\"end\":94995,\"start\":94982},{\"end\":95014,\"start\":94995},{\"end\":95247,\"start\":95232},{\"end\":95260,\"start\":95247},{\"end\":95277,\"start\":95260},{\"end\":95489,\"start\":95477},{\"end\":95500,\"start\":95489},{\"end\":95516,\"start\":95500},{\"end\":95525,\"start\":95516},{\"end\":95536,\"start\":95525},{\"end\":95550,\"start\":95536},{\"end\":95758,\"start\":95751},{\"end\":95771,\"start\":95758},{\"end\":95788,\"start\":95771},{\"end\":95973,\"start\":95961},{\"end\":95988,\"start\":95973},{\"end\":95998,\"start\":95988},{\"end\":96008,\"start\":95998},{\"end\":96013,\"start\":96008},{\"end\":96238,\"start\":96228},{\"end\":96249,\"start\":96238},{\"end\":96262,\"start\":96249},{\"end\":96277,\"start\":96262},{\"end\":96286,\"start\":96277},{\"end\":96298,\"start\":96286},{\"end\":96308,\"start\":96298},{\"end\":96611,\"start\":96599},{\"end\":96623,\"start\":96611},{\"end\":96636,\"start\":96623},{\"end\":96645,\"start\":96636},{\"end\":96658,\"start\":96645},{\"end\":96666,\"start\":96658},{\"end\":96973,\"start\":96962},{\"end\":96988,\"start\":96973},{\"end\":97224,\"start\":97215},{\"end\":97237,\"start\":97224},{\"end\":97249,\"start\":97237},{\"end\":97264,\"start\":97249},{\"end\":97279,\"start\":97264},{\"end\":97293,\"start\":97279},{\"end\":97306,\"start\":97293},{\"end\":97319,\"start\":97306},{\"end\":97333,\"start\":97319},{\"end\":97628,\"start\":97617},{\"end\":97639,\"start\":97628},{\"end\":97649,\"start\":97639},{\"end\":97659,\"start\":97649},{\"end\":97671,\"start\":97659},{\"end\":97683,\"start\":97671},{\"end\":97695,\"start\":97683},{\"end\":97711,\"start\":97695},{\"end\":98047,\"start\":98037},{\"end\":98060,\"start\":98047},{\"end\":98074,\"start\":98060},{\"end\":98306,\"start\":98293},{\"end\":98318,\"start\":98306},{\"end\":98327,\"start\":98318},{\"end\":98489,\"start\":98476},{\"end\":98502,\"start\":98489},{\"end\":98516,\"start\":98502},{\"end\":98531,\"start\":98516},{\"end\":98554,\"start\":98531},{\"end\":98791,\"start\":98778},{\"end\":98804,\"start\":98791},{\"end\":98818,\"start\":98804},{\"end\":98833,\"start\":98818},{\"end\":98856,\"start\":98833},{\"end\":99120,\"start\":99107},{\"end\":99128,\"start\":99120},{\"end\":99137,\"start\":99128},{\"end\":99151,\"start\":99137},{\"end\":99407,\"start\":99393},{\"end\":99416,\"start\":99407},{\"end\":99429,\"start\":99416},{\"end\":99441,\"start\":99429},{\"end\":99456,\"start\":99441},{\"end\":99750,\"start\":99739},{\"end\":99761,\"start\":99750},{\"end\":99775,\"start\":99761},{\"end\":99789,\"start\":99775},{\"end\":100067,\"start\":100052},{\"end\":100082,\"start\":100067},{\"end\":100098,\"start\":100082},{\"end\":100348,\"start\":100336},{\"end\":100361,\"start\":100348},{\"end\":100376,\"start\":100361},{\"end\":100389,\"start\":100376},{\"end\":100400,\"start\":100389},{\"end\":100681,\"start\":100666},{\"end\":100690,\"start\":100681},{\"end\":100702,\"start\":100690},{\"end\":100716,\"start\":100702},{\"end\":100940,\"start\":100927},{\"end\":100952,\"start\":100940},{\"end\":100961,\"start\":100952},{\"end\":100976,\"start\":100961},{\"end\":100991,\"start\":100976},{\"end\":101000,\"start\":100991},{\"end\":101197,\"start\":101185},{\"end\":101208,\"start\":101197},{\"end\":101221,\"start\":101208},{\"end\":101235,\"start\":101221},{\"end\":101251,\"start\":101235},{\"end\":101269,\"start\":101251},{\"end\":101458,\"start\":101445},{\"end\":101474,\"start\":101458},{\"end\":101494,\"start\":101474},{\"end\":101676,\"start\":101666},{\"end\":101688,\"start\":101676},{\"end\":101700,\"start\":101688},{\"end\":101711,\"start\":101700},{\"end\":101725,\"start\":101711},{\"end\":102502,\"start\":102490},{\"end\":102854,\"start\":102841}]", "bib_venue": "[{\"end\":68377,\"start\":68373},{\"end\":68707,\"start\":68703},{\"end\":69013,\"start\":69009},{\"end\":69343,\"start\":69298},{\"end\":69661,\"start\":69657},{\"end\":69973,\"start\":69959},{\"end\":70183,\"start\":70146},{\"end\":70668,\"start\":70580},{\"end\":71098,\"start\":71094},{\"end\":71424,\"start\":71414},{\"end\":71724,\"start\":71720},{\"end\":71916,\"start\":71847},{\"end\":72187,\"start\":72126},{\"end\":72579,\"start\":72495},{\"end\":72840,\"start\":72836},{\"end\":73212,\"start\":73172},{\"end\":73692,\"start\":73630},{\"end\":74048,\"start\":74044},{\"end\":74270,\"start\":74190},{\"end\":74702,\"start\":74698},{\"end\":74984,\"start\":74932},{\"end\":75328,\"start\":75288},{\"end\":75721,\"start\":75717},{\"end\":75995,\"start\":75924},{\"end\":76375,\"start\":76371},{\"end\":76643,\"start\":76639},{\"end\":76870,\"start\":76799},{\"end\":77171,\"start\":77102},{\"end\":77832,\"start\":77818},{\"end\":78050,\"start\":78011},{\"end\":78448,\"start\":78444},{\"end\":78699,\"start\":78695},{\"end\":78970,\"start\":78966},{\"end\":79269,\"start\":79259},{\"end\":79518,\"start\":79511},{\"end\":79736,\"start\":79732},{\"end\":79912,\"start\":79854},{\"end\":80146,\"start\":80142},{\"end\":80503,\"start\":80493},{\"end\":80834,\"start\":80800},{\"end\":81045,\"start\":81041},{\"end\":81295,\"start\":81291},{\"end\":81583,\"start\":81579},{\"end\":81866,\"start\":81862},{\"end\":82149,\"start\":82145},{\"end\":82446,\"start\":82439},{\"end\":82698,\"start\":82620},{\"end\":83105,\"start\":83097},{\"end\":83413,\"start\":83409},{\"end\":83657,\"start\":83598},{\"end\":83992,\"start\":83988},{\"end\":84213,\"start\":84169},{\"end\":84422,\"start\":84358},{\"end\":84833,\"start\":84829},{\"end\":85083,\"start\":85039},{\"end\":85336,\"start\":85332},{\"end\":85537,\"start\":85533},{\"end\":85802,\"start\":85795},{\"end\":86099,\"start\":86085},{\"end\":86366,\"start\":86352},{\"end\":86638,\"start\":86634},{\"end\":86906,\"start\":86902},{\"end\":87306,\"start\":87302},{\"end\":87735,\"start\":87731},{\"end\":88055,\"start\":88051},{\"end\":88363,\"start\":88359},{\"end\":88640,\"start\":88636},{\"end\":88884,\"start\":88880},{\"end\":89116,\"start\":89112},{\"end\":89315,\"start\":89225},{\"end\":89699,\"start\":89695},{\"end\":90071,\"start\":90064},{\"end\":90391,\"start\":90387},{\"end\":90691,\"start\":90684},{\"end\":91010,\"start\":91006},{\"end\":91370,\"start\":91366},{\"end\":91716,\"start\":91709},{\"end\":92065,\"start\":92061},{\"end\":92436,\"start\":92432},{\"end\":92758,\"start\":92754},{\"end\":93004,\"start\":93000},{\"end\":93187,\"start\":93123},{\"end\":93482,\"start\":93414},{\"end\":93863,\"start\":93859},{\"end\":94115,\"start\":94111},{\"end\":94336,\"start\":94332},{\"end\":94562,\"start\":94558},{\"end\":94774,\"start\":94770},{\"end\":94962,\"start\":94885},{\"end\":95281,\"start\":95277},{\"end\":95554,\"start\":95550},{\"end\":95792,\"start\":95788},{\"end\":96017,\"start\":96013},{\"end\":96226,\"start\":96165},{\"end\":96670,\"start\":96666},{\"end\":96960,\"start\":96866},{\"end\":97337,\"start\":97333},{\"end\":97715,\"start\":97711},{\"end\":98078,\"start\":98074},{\"end\":98331,\"start\":98327},{\"end\":98558,\"start\":98554},{\"end\":98860,\"start\":98856},{\"end\":99165,\"start\":99151},{\"end\":99391,\"start\":99328},{\"end\":99793,\"start\":99789},{\"end\":100102,\"start\":100098},{\"end\":100334,\"start\":100247},{\"end\":100664,\"start\":100598},{\"end\":101004,\"start\":101000},{\"end\":101273,\"start\":101269},{\"end\":101498,\"start\":101494},{\"end\":101895,\"start\":101725},{\"end\":102588,\"start\":102564},{\"end\":102675,\"start\":102618},{\"end\":102839,\"start\":102817}]"}}}, "year": 2023, "month": 12, "day": 17}