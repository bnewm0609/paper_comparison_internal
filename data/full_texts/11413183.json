{"id": 11413183, "updated": "2023-10-08 21:07:25.191", "metadata": {"title": "Reliable Crowdsourcing and Deep Locality-Preserving Learning for Expression Recognition in the Wild", "authors": "[{\"first\":\"Shan\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Weihong\",\"last\":\"Deng\",\"middle\":[]},{\"first\":\"JunPing\",\"last\":\"Du\",\"middle\":[]}]", "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2017, "month": null, "day": null}, "abstract": "Past research on facial expressions have used relatively limited datasets, which makes it unclear whether current methods can be employed in real world. In this paper, we present a novel database, RAF-DB, which contains about 30000 facial images from thousands of individuals. Each image has been individually labeled about 40 times, then EM algorithm was used to filter out unreliable labels. Crowdsourcing reveals that real-world faces often express compound emotions, or even mixture ones. For all we know, RAF-DB is the first database that contains compound expressions in the wild. Our cross-database study shows that the action units of basic emotions in RAF-DB are much more diverse than, or even deviate from, those of lab-controlled ones. To address this problem, we propose a new DLP-CNN (Deep Locality-Preserving CNN) method, which aims to enhance the discriminative power of deep features by preserving the locality closeness while maximizing the inter-class scatters. The benchmark experiments on the 7-class basic expressions and 11-class compound expressions, as well as the additional experiments on SFEW and CK+ databases, show that the proposed DLP-CNN outperforms the state-of-the-art handcrafted features and deep learning based methods for the expression recognition in the wild.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2738672149", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/0001DD17", "doi": "10.1109/cvpr.2017.277"}}, "content": {"source": {"pdf_hash": "1e00a616bab4c82fb9a14c8262053efe0c7cf486", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "a4e786b0d9789fb3984e555b18a2b6918d9b7667", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/1e00a616bab4c82fb9a14c8262053efe0c7cf486.txt", "contents": "\nReliable Crowdsourcing and Deep Locality-Preserving Learning for Expression Recognition in the Wild\n\n\nShan Li \nBeijing University of Posts and Telecommunications\n\n\nWeihong Deng whdeng@bupt.edu.cn \nBeijing University of Posts and Telecommunications\n\n\nJunping Du junpingd@bupt.edu.cn \nBeijing University of Posts and Telecommunications\n\n\nReliable Crowdsourcing and Deep Locality-Preserving Learning for Expression Recognition in the Wild\n\nPast research on facial expressions have used relatively limited datasets, which makes it unclear whether current methods can be employed in real world. In this paper, we present a novel database, RAF-DB, which contains about 30000 facial images from thousands of individuals. Each image has been individually labeled about 40 times, then EM algorithm was used to filter out unreliable labels. Crowdsourcing reveals that real-world faces often express compound emotions, or even mixture ones. For all we know, RAF-DB is the first database that contains compound expressions in the wild. Our cross-database study shows that the action units of basic emotions in RAF-DB are much more diverse than, or even deviate from, those of labcontrolled ones. To address this problem, we propose a new DLP-CNN (Deep Locality-Preserving CNN) method, which aims to enhance the discriminative power of deep features by preserving the locality closeness while maximizing the inter-class scatters. The benchmark experiments on the 7class basic expressions and 11-class compound expressions, as well as the additional experiments on SFEW and CK+ databases, show that the proposed DLP-CNN outperforms the state-of-the-art handcrafted features and deep learning based methods for the expression recognition in the wild.\n\nIntroduction\n\nMillions of images are being uploaded every day by users from different events and social gatherings. There is an increasing interest in designing systems capable of understanding human manifestations of emotional attributes and affective displays. To automatic learn the affective state of face images from the Internet, large annotated databases are required. However, the complexity of annotations of emotion categories has hindered the collection of large annotated databases. On the other side, popular AU coding [12] requires specific expertise to take months to learn and be perfected, hence, alternative solutions are needed. And due to the cultural difference in the way of perceiving facial emotion [13], it is difficult for psychologists to define definite prototypical AUs for each facial expressions. Therefore, it is also worth to study the emotion of social images from the judgments of a large common population, besides from the professional knowledge of a few experts.\n\nIn this paper, we propose to study the common expression perception by a reliable crowdsourcing approach. Specifically, our well-trained annotators are asked to label face images with one of the seven basic categories [11], and each face is annotated enough times independently, i.e. about 40 times in our experiment. Then, the noisy labels are filtered by an EM based reliability evaluation algorithm, through which each image can be represented reliably by a 7-dimensional emotion probability vector. By analyzing 1.2 million labels of 29672 great-diverse facial images downloaded from the Internet, these Real-world Affective Faces (RAF) 1 are naturally categorized into two types: basic expression with single-modal distribution and compound emotions with bimodal distribution, an observation supporting a recent ground-breaking finding in the lab-controlled condition [10]. To the best of our knowledge, the realworld expression database RAF-DB is the first large-scale database providing the labels of common expression perception and compound emotions in unconstrained environment.\n\nThe cross-database experiment and AU analysis on RAF-DB indicates that AUs of real-world expressions are much more diverse than, or even deviate from, those of lab-controlled ones guided by psychologists. To address this ambiguity of unconstrained emotion, we further propose a novel Deep Locality-preserving CNN (DLP-CNN). Inspired by [17], we develop a practical back-propagation algorithm which creates a locality preserving loss (LP loss) aiming to pull the locally neighboring faces of the same class together. Jointly trained with the classical softmax loss which forces different classes to stay apart, locality preserving loss drives the intra-class local clusters of each Figure 1. The distribution of deeply learned features in (a) \"DCNN without LP loss\" and (b) \"DLP-CNN\". As can be seen, locality preserving loss layer helps the network to learn features with more discrimination. Moreover, it can be clearly seen that non-neutral expressions which have obvious intensity variations, such as Happiness, Sadness, Fear, Surprise and Anger, change the intensity continuously and smoothly, from low to high, from center to periphery. And images with Disgust label, which is the most confused expression, are assembled in the middle. With the neighborhood preserving character of DLP-CNN, the deep feature seems to be able to capture the intrinsic expression manifold structure to a large extent. Best viewed in color. class to become compact, and thus the discriminative power of the deeply learned features can be highly enhanced. Moreover, locally neighboring faces tend to share similar emotion intensity by using DLP-CNN, which can derive the discriminative deep feature with smooth emotion intensity transition. Figure 1 (b) shows the resulting 2-dimensional deep features learnt from our DLP-CNN model, where we attach example face images with various intensity in different expression classes.\n\nExtensive experiments on RAF-DB and other related databases show that the proposed DLP-CNN outperforms other state-of-the-art methods. Moreover, the activation features trained on RAF-DB can be re-purposed to new databases with small-sample training data, suggesting that the DLP-CNN is a powerful tool to handle the cross-culture problem on perception of emotion (POE).\n\n\nRelated Work\n\n\nExpression image datasets\n\nFacial expression recognition largely relies on welldefined databases, however, several limitations exist.\n\nMany available databases were produced in tightly controlled environments without diversity on subjects and conditions. Subjects in them were taught to act expressions in a uniform way. Besides, the majority of current databases only include six basic categories or less. However, images captured in real-life scenarios often present complex, compound or even ambiguous emotions rather than simple and prototypical ones [3]. What's more, labelers in these databases are too few, which would reduce the reliability and validity of the emotion labels.\n\nWe then focus on discussing image databases with spontaneous expressions. SFEW 2.0 [7] contains 700 images extracted from movies, and images were labelled by two independent labelers. The database covers unconstrained facial expressions, varied head poses, large age range, occlusions, varied focus, different resolution of face. FER-2013 [16] contains 35887 images collected and labelled using the Google image search API. Cropped images are provided in 48\u00d748 pixels and converted to grayscale. BP4D-Spontaneous [47] contains plenty of images from 41 subjects revealing a range of spontaneous expressions elicited through eight tasks. However, the database organization were lab-controlled. AM-FED [30] is collected in real world with sufficient samples, however, without specifical emotion labels, it's more suited for researches on AUs. E-motioNet [1] is a large database of one million facial expression images in the wild created by an automatic AU detection algorithm. Unlike these databases, RAF-DB simultaneously satisfies multiple requirements: sufficient data, various environments, group perceiving on facial expressions and data labels with the least noise.\n\n\nThe framework for expression recognition\n\nFacial expression analysis can be generally divided into three main parts [14]: face aquisition, facial feature extraction and facial expression classification.\n\nIn face aquisition stage, an automatic face detector is used to locate faces in complex scenes. Feature points are then used to crop and align faces into a unified template by geometric transformations. For facial feature extraction, previous methods can be generally categorized into two groups: Appearance-based methods [29] and AU-based methods [42]. The former uses common feature extraction methods such as LBP [38], Haar [44]. The latter recognizes expression by detecting AUs. Feature classification is performed in the last stage. The commonly used methods include SVM, nearest neighbor, LDA, DBN and decisionlevel fusion on these classifiers [46]. The extracted facial expression information is either classified as a set of facial actions or a particular basic emotion [34]. Most focus on the latter and is based on Ekman's theory of six basic emotions [12]. Indeed, without making additional assumptions about how to determine what action units constitute an expression, there can be no exact definition for the expression category. The basic emotional expressions is therefore not universal enough to generalize expressions displayed on human face [37].\n\n\nDeep learning for expression recognition\n\nRecently, deep learning algorithms have been applied to visual object recognition, face verification and detection, image classification and many other problems, which achieve state-of-the-art results. So far, there have been a few deep neural networks used in facial expression recognition due to the lack of sufficient training samples. In ICM-L 2013 competition [16], the winner [41] was based on Deep Convolutional Neural Network (DCNN) plus SVM. In EmotiW 2013 competition [6], the winner [19] combined modality specific deep neural network models. In EmotiW 2015 [8], more competitors have tried deep learning methods: transfer learning was used to solve the problem of small database in [32], hierarchical committee of multicolumn DCNNs in [20] gained the best result on SFEW database, LBP features combined with DCNNs structure were proposed in [22]. In [24], AU-aware Deep Networks (AUDN) was proposed to learn features with the interpretation of facial AUs. In [31], a DCNN with inception layers was proposed to gain comparable results.\n\n3. Real-world Expression Database: RAF-DB 3.1. Creating RAF-DB Data collection. At the very beginning, the images' URLs collected from Flickr were fed into an automatic open-source downloader to download images in batches. Considering that the results returned by Flickr's image search API were in well-structured XML format, from which the URLs can be easily parsed, we then used a set of keywords (for example: smile, giggle, cry, rage, scared, frightened, terrified, shocked, astonished, disgust, expressionless) to pick out images that were related with the six basic emotions plus the neutral emotion. At last, a total of 29672 real-world facial images are presented in our database. Figure 2 shows the pipeline of data collection.\n\nDatabase annotation. Annotating nearly 30000 images of expression is an extremely difficult and time-consuming task. Considering the compounded property of real-world expressions, multiple views of images' expression state should be collected from different labelers. We therefore employed 315 annotators (students and staffs from universities) who have been instructed with one-hour tutorial of psychological knowledge on emotion for an online facial expression annotation assignment, where they were asked to classify the image into the most apparent one from seven classes. We developed a website for RAF-DB annotation, which shows each image with exclusive attribute options. Images were randomly and equally assigned to each labeler, ensuring that there were no direct correlation among the images labeled by one person. And each image was assured to be labeled by about 40 independent labelers. After that, a multi-label annotation result is obtained for each image, i.e., a seven dimensional vector that each dimension indicates the votes of relevant emotion.\n\nMetadata. The data is provided with precise locations and size of the face region, as well as the manually located five landmark points (the central of two eyes, the tips of the nose and two corners of the mouth) on the face. Besides, an automatic landmark annotation mode without manual label is included: 37 landmarks were picked out from the annotation results provided by Face++ API [18]. We also manually annotated the basic attributes (gender, age (5 ranges) and race) of all RAF faces. In summary, subjects in our database range in age from 0 to 70 years old. They are 52% female, 43% male, and 5% remains unsure. For racial distribution, there are 77% Caucasian, 8% African-American, and 15% Asian. The pose of each image, including pitch, yaw and roll parameters, is computed from the manually labeled locations of the five facial landmarks.\n\nReliability estimation. Due to subjectivity and varied expertise of labelers and wide ranging levels of images' difficulty, there were some disagreements among annotators. To get rid of noisy labels, motivated by [45], a Expectation Maximization (EM) framework was used to assess each labeler's reliability.\nLet D = {(x j , y j , t 1 j , t 2 j , ..., t R j )} n j=1\ndenote a set of n labeled inputs, where y j is the gold standard label (hidden variable) for the jth samples x j , t i j \u2208 {1, 2, 3, 4, 5, 6, 7} is the corresponding label given by the ith annotator. The correct probability of t i j are formulated as a sigmoid function:\np(t i j = y j |\u03b1 i , \u03b2 j ) = (1 + exp(\u2212\u03b1 i \u03b2 j )) \u22121 ,\nwhere 1/\u03b2 j is the difficulty of the jth images, \u03b1 i is the reliability of ith annotators.\n\nOur goal is to optimize the log-likelihood of the given labels:\nmax \u03b2>0 l(\u03b1, \u03b2) = j ln p(t|\u03b1, \u03b2) = j ln y p(t, y|\u03b1, \u03b2) = j ln y Q j (y) p(t, y|\u03b1, \u03b2) Q j (y) \u2265 j y Q j (y) ln p(t, y|\u03b1, \u03b2) Q j (y)\nwhere Q j (y) is a certain distribution of hidden variable y,\nQ j (y j ) = p(t j , y j |\u03b1, \u03b2) y p(t j , y j |\u03b1, \u03b2) = p(t j , y j |\u03b1, \u03b2) p(t j |\u03b1, \u03b2) = p(y j |t j , \u03b1, \u03b2)\nAfter revision, 285 annotators' labels have been remained and Cronbach's Alpha score of all labels is 0.966. Subset Partitions. Let G j = {g 1 , g 2 , ..., g 7 } denotes the 7-dimensional ground truth of the jth image, where\ng k = R i=1 \u03b1 i 1 t i\nj =k (\u03b1 i means the ith annotators reliability. 1 A is an indicator function that evaluates to \"1\" if the Boolean expression A is true and \"0\" otherwise.), and label k \u2208 {1, 2, 3, 4, 5, 6, 7} refer to surprise, fear, disgust, happiness, sadness, anger and neutral, respectively. We then divided RAF-DB into different subsets according to the 7dimensional ground truth. For Single-label Subset, we first calculated the mean distribution value g mean = 7 k=1 g k /7\n\nfor each image, then picked out label k w.r.t. g k > g mean as the valid label. Images who have single valid label are classified into Single-label Subset. For Two-tab Subset, the partition rule is similar. The only difference is that we took out images with neutral label before partition. Figure 3 exhibits specific samples of 6-class basic emotions and 12class compound emotions.\n\n\nCK+ and RAF Cross-Database Study\n\nWe then conducted a CK+ [26] and RAF cross-database study to explore the specific difference between expression-Algorithm 1 Label reliability estimation algorithm.\nInput: Training set D = {(xj, t 1 j , t 2 j , ..., t R j )} n j=1\nOutput: Each annotator's reliability \u03b1 * i Initialize: \u2200j = 1, ..., n, initialize the true label yj using majority voting\n\u03b2j := \u2212 R i=1 p(t i j ) ln p(t i j ), \u03b1i := 1,\nThe initial value of \u03b2j is image j's entropy. The higher the entropy, the more uncertain the image. We also optimize \u03b2j along with \u03b1i during M-step. However, the goal is to get each labeler's reliability, so we didn't include it in this step. For optimization, we take a derivative with respect to \u03b2j and \u03b1i respectively. Until convergence s of real-world affective face and the lab-controlled posed face guided by psychologist. Here, \"cross-database\" means we use all of the images from one database for training and the images from the other for testing. In order to eliminate the bias caused by different training size, the single-tab subset of RAF-DB has been sub-sampled for experiment to balance the size of two databases.\n\nTo ensure the generalization capabilities of the classifiers, we applied support vector machine for classification and tried HOG descriptor [5] for representation. Specifically, original images were first aligned to the size of 100\u00d7100. Then, we got a 4000-dimensional HOG feature vector per aligned image. Finally, SVM with RBF kernel implemented  by LibSVM [4] was applied for classification. Parameters were optimized using grid search. We then performed a cross-database experiment based on six-class expression. Multiclass support vector machine (mSVM) and confusion matrix were used as the classification method and the assessment criteria respectively. Figure 4 shows the results of this experiment.\n\nAnalyzing the diagonal of these two matrixes, we can see that surprise, happiness and disgust are the top three that have the highest recognition rates in both cases. This result is in line with many single database tests based on CK+, such as [26], [35] and [38]. After calculating the average of the diagonals, Matrix I was detected with 62% accuracy while Matrix II with only 39%, which indicates that data collected from real world is more multiple and effective than lab-controlled one. This is particularly evident in the expression of sadness, then happiness and surprise. Besides, anger and disgust are usually confused with each other in both cases, which conforms to the survey in [2].\n\nIn order to explain the phenomena above, a more detailed research must be conducted to find out the specifical differences of each expression between these two databases. Therefore, a facial action coding system (FACS) analysis has been employed. FACS was first presented in [12], where the changes on facial behaviors are described by a set of action units (AUs). AUs of sub-sampled images in RAF-DB were first labeled by our FACS coders. We then quantitatively analyzed the AU presence for different emotions in CK+ and RAF. Some examples from CK+ and RAF are shown in Figure 5. Besides, probabilities of AUs' occurrence for each expression from sub-sampled images in RAF-DB have been shown in Table 1.\n\n\nDeep Locality-Preserving Feature Learning\n\nBesides the \"in-the-wild\" difficulties such as variable lighting, poses and occlusions, real-world affective faces at least pose two challenges that demand new algorithm-  Figure 5. Comparison of six basic emotions from CK+ and RAF. It's evident that expression AUs in RAF are more diverse than those in CK+. The empty data indicates the probability is less than 10%\n\nAn asterisk(*) indicates the AU's probability is quite different from CK+'s (at least 40% disparity). s to address. First, as indicated by our cross-database study, real world expression may associate with various AU combinations that require classification algorithms to model the multi-modality distribution of each emotion in the feature space. Second, as suggested by our crowdsourcing results, a large amount of real-world affective faces express compound, or even multiple emotions. So traditional handengineered representations which perform well on the labcontrolled databases are no longer suitable for expression recognition tasks in the wild. Nowadays, DCNN has been proved to outperform handcrafted features on lager-scale visual recognition tasks. Nevertheless, conventional DCNN uses only the softmax loss layer to supervise the training process. The softmax layer helps keeping the deeply learned features of different classes separable, however, still remains serious intra-class variation. On the contrary, facial expressions in real world show significant intra-class difference on account of varied occlusions, illuminations, resolutions and head positions. What's more, individual variation can also lead to big difference for the same category expression, for example, laugh v.s. smile. Hence, we proposed a novel DLP-CNN to address the ambiguity and multi-modality of real-world facial expressions. In DLP-CNN, we added a new supervised layer on the fundamental architecture shown in Table 2, namely locality preserving loss (LP loss), to improve the discrimination ability of the deep features.\n\nThe basic idea is to preserve the locality of each sample x i and make the local neighborhoods within each class as  Kernel  3  -2  3  -2  3  -3  -2  3  -3  -output  64  --96  --128  -128  --256  -256  -2000  -7  Stride  1  1  2  1  1  2  1  1  1  1  2  1  1  1  1  1  Pad  1  0  0  1  0  0  1  0  1  0  0  1  0  1  0  0 compact as possible. To formulate our goal:\nmin W i,j S ij ||x i \u2212 x j || 2 2 (1)\nwhere W is the network parameters, and the matrix S is a similarity matrix. The deep feature x \u2208 R d denotes Deep Convolutional activation features (DeCaf) [9] taken from the final hidden layer, i.e., just before the softmax layer that produces the class prediction. A possible way of defining S is as follows.\nS ij = \uf8f1 \uf8f2 \uf8f3 1, x j is among k nearest neighbors of x i or x i is among k nearest neighbors of x j 0, otherwise(2)\nwhere x i and x j belong to the same class of expression, k defines the size of the local neighborhood.\n\nThis formulation effectively characterizes the intra-class local scatters. Note that x i should be updated as the iterative optimization of the CNN. To compute the summation of the pairwise distance, we need to take the entire training set in each iteration, which is inefficient to implement. To address this difficulty, we do the approximation by searching the k nearest neighbors for each sample x i , and the locality preserving loss function of x i is defined as follow:\nL lp = 1 2 n i=1 ||x i \u2212 1 k x\u2208N k {xi} x|| 2 2(3)\nwhere N k {x i } denotes the ensemble of the k nearest neighbors of sample x i with the same class. The gradients of L lp with respect to x i is computed as:\n\u2202L lp \u2202x i = x i \u2212 1 k x\u2208N k {xi} x(4)\nIn this manner, we can perform the update based on minibatch. Note that, the recently proposed center loss [43] can be considered as a special case of the locality preserving loss, if k = n c \u2212 1 (n c is the number of the training samples in class c to which x i belong). While center loss simply pulls the samples to a single centroid, the proposed locality preserving loss is more flexible especially when the class conditional distribution is multi-modal.\n\nWe then adopt the joint supervision of softmax loss which characterizes the global scatter and the locality preserving loss which characterizes the local scatters within class, to train the CNNs for discriminative feature learning.\n\nThe objective function is formulated as follow: L = L s + \u03bbL lp , where L s denotes the softmax loss and L lp denotes the locality preserving loss. The hyper parameter \u03bb is used to balance the two loss functions. Algorithm 2 summarizes the learning process in the deep locality preserving CNN.\n\n\nAlgorithm 2 Optimization algorithm of DLP-CNN.\n\nInput: Training data {xi} n i=1 , n is the size of mini-batch Output: Network layer parameters W Initialize: t = 0 Network learning rate \u00b5, hyper parameter \u03bb, Network layer parameters W , softmax loss parameters \u03b8, neighboring nodes k. Repeat: 1: t = t + 1 2: Computer the center of k-nearest neighbor for xi: \nC t i = 1 k n j=1 x t j S tW t+1 = W t \u2212 \u00b5 t \u2202L t \u2202W t = W t \u2212 \u00b5 t n i=1 \u2202L t \u2202x t i \u2202x t i \u2202W t\nUntil convergence\n\n\nBaseline System\n\nTo facilitate translating the research from laboratory environments to the real world, we performed two challenging benchmark experiments on RAF-DB: 7-class basic expression classification and 11-class compound expression classification, and presented affiliated baseline algorithms and performances. We also conducted comparative experiments on two small and popular datasets, CK+ and JAFFE [28].\n\nWe followed up the experimental setup in cross-database experiments, and tried LBP [33], HOG [5] and Gabor [23] features. The LBP descriptor applied the 59-bin LBP u2 8,2 operator, and then concatenated the histograms from 10\u00d710 pixel cells, generating a 5,900 dimensional feature vector. The HOG feature used this shape-based segmentation dividing the image into 10\u00d710 pixel blocks of four 5\u00d75 pixel cells with no overlapping. By setting 10 bins for each histograms, we extract a 4000-dimensional HOG feature vector for each image. For Gabor wavelet, we used a bank of 40 Gabor filters at five spatial scales and eight orientations. The downsample image's size was set to 10*10, yielding 4000-dimensional features.\n\nIn order to objectively measure the performance for the followers entries, we split the dataset into a training set and a test set with the idea of five-fold cross-validation, which means the size of training set is five times larger than test set, and expressions in both sets have a near-identical distribution. Considering expressions in the wild have imbalanced distribution, the accuracy metric which is especially sensitive to bias and no longer effective for imbalanced data [15], is no longer used in RAF. Instead, we use the mean diagonal value of the confusion matrix as the ultima metric.\n\n\nBasic emotions.\n\nIn this experiment, seven basic emotion classes were detected using the whole 15339 images from the single-label subset. The best classification accuracy (output by SVM) was 72.71% for LBP, 74.35% for HOG, and 77.28% for Gabor. Results declined to 55.98%, 58.45% and 65.12% respectively when using the mean diagonal value of the confusion matrix as metric. To assess the reliability of the basic emotion labels, we also assigned a uniform random label to each sample, which we call a naive emotion detector. And the best result for the naive classifier was 16.07% when using Gabor feature, which is much lower than the former value.\n\nFor comparison, we employed the same methods on CK+ with person-independent 5-fold cross-validation and JAFFE with leave-one-subject-out strategy. The results shown in Table 3 certify that expressions in real world are more difficult for recognition and the current common methods which perform well on the existing databases cannot solve the expression recognition problem in the challenging real-world condition.\n\nTo evaluate effectiveness of different classifiers, we have also trained LDA with nearest neighbor (NN) classification. We found that LDA+NN were inferior to mSVM obviously when training on RAF, a extremely large database. Nevertheless, it performed better when training on small-scale datasets (CK+ and JAFFE), even outperformed mSVM in some cases. Concrete results can be viewed in Table 3.\n\nCompound emotions. For compound emotions classification, we got rid of fearfully disgusted emotion as it's too few, leaving 11 classes of compound emotion, 3954 in total. The best classification accuracy (output by SVM) was 45.51% for LBP, 51.89% for HOG, and 53.54% for Gabor. Results declined to 28.84%, 33.65% and 35.76% respectively when using the mean diagonal value of the confusion matrix as metric. Again, to demonstrate the reliability of the compound emotion labels, we computed the baseline for the naive emotion detector, which declined to 5.79% when using Gabor feature.\n\nAs expected, the overall performance dropped significantly when more expressions are involved for classification. The significantly lower results compared to that of basic emotions indicate that compound emotions are more difficult to detect and new methods should be invented to solve this problem. Besides the multi-modality, lack of training samples of compound expressions from real world is another great technical challenge.\n\n\nDeep Learning System\n\nNowadays, deep learning has been applied to lager-scale visual recognition tasks and perform exceedingly well with lager amounts of training data. However, fully-supervised deep models are easy to be overfitting on facial expression recognition task due to the insufficient training samples for the model learning. Therefore, most deep learning frameworks employed on facial expression recognition [22,32,36] are base on pre-trained models. These pre-trained models, such as VGG network [40] and AlexNet [21], are initially designed for face recognition, which are short of discrimination ability of expression characteristic. So in this paper, we directly trained our deep learning system on the big enough self-collected database RAF from scratch, without using other databases.\n\nWhen conducting experiments, we followed the same dataset partition standards, image processing methods and classification methods as in the baseline system. Related researches [9,39] have proved that well-trained deep convolutional network can work as a feature extraction tool with generalization ability for the classification task. Following up this idea, we first trained each DCNNs for basic emotion recognition task, and then directly used the already trained DCNN models to extract deep features for both basic and compound expressions. 2000-dimensional deep features learnt from raw data were extracted from the penultimate fully connected layer of the DCNNs and then classified by SVM. From the results in Table 4, we have the following observations. First, DCNNs which achieve quite reasonable results for large-scale image recognition setting, such as VGG network and AlexNet, are not efficient for facial expression recognition. Second, all of the deep features learnt on RAF-DB outperform the unlearned features used in the baseline system by a significant margin, which indicates that deep learning architecture is more robust and applicable for both basic and compound expression. At last, our new locality preserving loss model achieves better performance than the based one and the center loss one. Note that, the center loss, which efficiently converges unimodal class, can help enhance the network performance on basic emotion, but it fails on compound emotion. This shows the advantage of the locality preserving loss on multi-modal facial expression recognition, including both basic and compound one.\n\nTo see the generalization ability of our well-trained DLP-CNN model on other databases, we then employed it to directly extract fixed-length feature of CK+ and SFEW 2.0 without finetune. For the lab-controlled databases CK+, we followed the experimental principle in the baseline system. For the real-world database SFEW 2.0, we followed the rule in EmotiW 2015 [8], and the \"SFEW best\" is the result of the single best model used in the winner [20] of EmotiW 2015. Note that, in [20], the Authors trained their model with extra data from SFEW. From the comparison results in Table 5, we can see that our network can also achieve comparable or even better performance than other state-ofthe-art methods, not only for RAF, but also other databases. This indicates that our proposed network can be used as an efficient and effective feature extraction tool for facial expression databases, without a significant amount of time to execute in traditional DCNNs.\n\n\nConclusions and Future Work\n\nThe main contribution of this paper is presenting a novel optimized algorithm for crowdsourcing and a new locali- ty preserving loss layer for deep learning, based on a realworld publicly available facial expression database RAF-DB. The optimized algorithm helps to keep the best annotated results from labelers. The new DCNN can learn more discriminative feature for expression recognition task. The RAF-DB contains, 1) 29672 real-world images labeled for different expressions, age range, gender and posture feature, 2) a 7-dimensional expression distribution vector for each image, 3) two different subsets: single-label subset, including seven classes of basic emotions; two-tab subset, including twelve classes of compound emotions, 4) locations of five manually accurate detect landmark points, 5) baseline classifier outputs for basic emotions and compound emotions. We hope that the release of this database will encourage more researches on the effect of real-world expression distribution or detection and be a useful benchmark resource for researchers to compare the validity of their facial expression analysis algorithms in challenge conditions.\n\n\nAcknowledgments\n\nFigure 2 .\n2Overview of construction and annotation of RAF-DB.\n\nFigure 3 .\n3Examples of six-class basic emotions and twelve-class compound emotions from RAF-DB. Detailed data distribution of RAF-DB has been attached to each expression classes.\n\nFigure 4 .\n4Confusion matrixes for cross-database experiments using HOG features. The true labels (training data) are on the vertical axis, the predicted labels (test data) are on the horizontal axis.\n\nij 3 :\n3Update the softmax loss parameters:\u03b8 t+1 = \u03b8 t \u2212 \u00b5\n\nTable 1 .\n1Probabilities of AUs' occurrence for each expression in RAF-DB (%) AU1 AU2 AU4 AU5 AU6 AU7 AU9 AU10 AU12 AU15 AU 17 AU20 AU25 AU 26 AU27Sur \n97 \n97 \n84 \n98 \n53  *  \n\nFea \n78 \n42 \n74 \n79 \n50 \n30  *  \n61  *  43  *  \n\nDis \n51 \n34  *  89  *  \n82 \n26 \n55  *  \n\nHap \n98 \n85 \n97 \n23 \n\nSad 88 \n84 \n21  *  \n54 \n49  *  \n\nAng \n96 72  *  \n94 \n36 \n87 \n79  *  72  *  \n\n\n\nTable 2 .\n2The configuration parameters in the fundamental architecture (baseDCNN).Layer \nType \n\n1 \n2 \n3 \n4 \n5 \n6 \n7 \n8 \n9 \n10 \n11 \n12 \n13 \n14 \n15 \n16 \n17 \n18 \n\nConv ReLu MPool Conv ReLu MPool Conv ReLu Conv ReLu MPool Conv ReLu Conv ReLu \nFC \nReLu FC \n\n\n\nTable 3 .\n3Basic expression class performance comparison of CK+, JAFFE and RAF along with Compound expression performance of RAF, based on LBP, HOG and Gabor descriptors, and SVM, LDA+kNN classification. The metric is the mean diagonal value of the confusion matrix.basic \ncompound \nCK+ JAFFE RAF \nRAF \n\nmSVM \n\nLBP 88.92 78.81 55.98 \n28.84 \nHOG 90.50 84.76 58.45 \n33.65 \nGabor 91.98 88.95 65.12 \n35.76 \n\nLDA \n\nLBP 85.84 77.74 50.97 \n22.89 \nHOG 91.77 80.12 51.36 \n24.01 \nGabor 92.33 83.45 56.93 \n23.81 \n\n\n\nTable 4 .\n4Expression recognition performance of different DCNNs on RAF. The metric is the mean diagonal value of the confusion matrix. basic compound Anger Disgust Fear Happiness Sadness Surprise Neutral Average AveragemSVM \n\nVGG \n68.52 27.50 35.13 \n85.32 \n64.85 \n66.32 \n59.88 \n58.22 \n31.63 \nAlexNet \n58.64 21.87 39.19 \n86.16 \n60.88 \n62.31 \n60.15 \n55.60 \n28.22 \nbaseDCNN 70.99 52.50 50.00 \n92.91 \n77.82 \n79.64 \n83.09 \n72.42 \n40.17 \ncenter loss 68.52 53.13 54.05 \n93.08 \n78.45 \n79.63 \n83.24 \n72.87 \n39.97 \nDLP-CNN 71.60 52.15 62.16 \n92.83 \n80.13 \n81.16 \n80.29 \n74.20 \n44.55 \n\nLDA \n\nVGG \n66.05 25.00 37.84 \n73.08 \n51.46 \n53.49 \n47.21 \n50.59 \n16.27 \nAlexNet \n43.83 27.50 37.84 \n75.78 \n39.33 \n61.70 \n48.53 \n47.79 \n15.56 \nbaseDCNN 66.05 47.50 51.35 \n89.45 \n74.27 \n76.90 \n77.50 \n69.00 \n28.23 \ncenter loss 64.81 49.38 54.05 \n92.41 \n74.90 \n76.29 \n77.21 \n69.86 \n27.33 \nDLP-CNN 77.51 55.41 52.50 \n90.21 \n73.64 \n74.07 \n73.53 \n70.98 \n32.29 \n\n\n\nTable 5 .\n5Comparison results of DLP-CNN and other state-of-theart deep learning methods on CK+ and SFEW 2.0. AUDN[25] \n\nFP+SAE \n\n[27] \n\n[31] \nSFEW best \n\n[20] \n\nDLP-CNN \n\n(without finetune) \n\nCK+ \n93.70 91.11 93.2 \n-\n95.78 \n\nSFEW 2.0 30.14 \n-\n47.7 \n52.5 \n51.05 \n\n\n\nEmotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild. C F Benitez-Quiroz, R Srinivasan, A M Martinez, Proceedings of IEEE International Conference on Computer Vision & Pattern Recognition (CVPR16). IEEE International Conference on Computer Vision & Pattern Recognition (CVPR16)Las Vegas, NV, USAC. F. Benitez-Quiroz, R. Srinivasan, and A. M. Martinez. E- motionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild. In Proceedings of IEEE International Conference on Comput- er Vision & Pattern Recognition (CVPR16), Las Vegas, NV, USA, 2016.\n\nFace expression recognition and analysis: the state of the art. V Bettadapura, arXiv:1203.6722arXiv preprintV. Bettadapura. Face expression recognition and analysis: the state of the art. arXiv preprint arXiv:1203.6722, 2012.\n\nThe neuropsychology of emotion. J C Borod, Oxford University PressNew YorkJ. C. Borod. The neuropsychology of emotion. Oxford Uni- versity Press New York, 2000.\n\nLIBSVM: A library for support vector machines. C.-C Chang, C.-J Lin, ACM Transactions on Intelligent Systems and Technology. 2C.-C. Chang and C.-J. Lin. LIBSVM: A library for sup- port vector machines. ACM Transactions on Intelligen- t Systems and Technology, 2:27:1-27:27, 2011. Soft- ware available at http://www.csie.ntu.edu.tw/ cjlin/libsvm.\n\nHistograms of oriented gradients for human detection. N Dalal, B Triggs, Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on. IEEE1N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In Computer Vision and Pattern Recogni- tion, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 1, pages 886-893. IEEE, 2005.\n\nEmotion recognition in the wild challenge. A Dhall, R Goecke, J Joshi, M Wagner, T Gedeon, Proceedings of the 15th ACM on International conference on multimodal interaction. the 15th ACM on International conference on multimodal interactionACMA. Dhall, R. Goecke, J. Joshi, M. Wagner, and T. Gedeon. Emotion recognition in the wild challenge 2013. In Pro- ceedings of the 15th ACM on International conference on multimodal interaction, pages 509-516. ACM, 2013.\n\nStatic facial expression analysis in tough conditions: Data, evaluation protocol and benchmark. A Dhall, R Goecke, S Lucey, T Gedeon, Computer Vision Workshops (ICCV Workshops. IEEE2011 IEEE International Conference onA. Dhall, R. Goecke, S. Lucey, and T. Gedeon. Static fa- cial expression analysis in tough conditions: Data, evalua- tion protocol and benchmark. In Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on, pages 2106-2112. IEEE, 2011.\n\nVideo and image based emotion recognition challenges in the wild: Emotiw. A Dhall, O Murthy, R Goecke, J Joshi, T Gedeon, Proceedings of the 2015 ACM on International Conference on Multimodal Interaction. the 2015 ACM on International Conference on Multimodal InteractionACMA. Dhall, O. Ramana Murthy, R. Goecke, J. Joshi, and T. Gedeon. Video and image based emotion recognition chal- lenges in the wild: Emotiw 2015. In Proceedings of the 2015 ACM on International Conference on Multimodal Interac- tion, pages 423-426. ACM, 2015.\n\nDecaf: A deep convolutional activation feature for generic visual recognition. J Donahue, Y Jia, O Vinyals, J Hoffman, N Zhang, E Tzeng, T Darrell, ICML. J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell. Decaf: A deep convolutional acti- vation feature for generic visual recognition. In ICML, pages 647-655, 2014.\n\nCompound facial expressions of emotion. S Du, Y Tao, A M Martinez, Proceedings of the National Academy of Sciences. 11115S. Du, Y. Tao, and A. M. Martinez. Compound facial expres- sions of emotion. Proceedings of the National Academy of Sciences, 111(15):E1454-E1462, 2014.\n\nFacial expression and emotion. P Ekman, American psychologist. 484384P. Ekman. Facial expression and emotion. American psy- chologist, 48(4):384, 1993.\n\nFacial action coding system. P Ekman, W V Friesen, P. Ekman and W. V. Friesen. Facial action coding system. 1977.\n\n. P Ekman, W V Friesen, M O&apos;sullivan, A Chan, I Diacoyanni-Tarlatzis, K Heider, R Krause, W A , P. Ekman, W. V. Friesen, M. O'Sullivan, A. Chan, I. Diacoyanni-Tarlatzis, K. Heider, R. Krause, W. A.\n\nUniversals and cultural differences in the judgments of facial expressions of emotion. T Lecompte, P E Pitcairn, Ricci-Bitti, Journal of personality and social psychology. 534712LeCompte, T. Pitcairn, P. E. Ricci-Bitti, et al. Universals and cultural differences in the judgments of facial expressions of emotion. Journal of personality and social psychology, 53(4):712, 1987.\n\nAutomatic facial expression analysis: a survey. B Fasel, J Luettin, Pattern recognition. 361B. Fasel and J. Luettin. Automatic facial expression analysis: a survey. Pattern recognition, 36(1):259-275, 2003.\n\nAn experimental comparison of performance measures for classification. C Ferri, J Hern\u00e1ndez-Orallo, R Modroiu, Pattern Recognition Letters. 301C. Ferri, J. Hern\u00e1ndez-Orallo, and R. Modroiu. An experi- mental comparison of performance measures for classifica- tion. Pattern Recognition Letters, 30(1):27-38, 2009.\n\nChallenges in representation learning: A report on three machine learning contests. I J Goodfellow, D Erhan, P L Carrier, A Courville, M Mirza, B Hamner, W Cukierski, Y Tang, D Thaler, D.-H Lee, Neural information processing. SpringerI. J. Goodfellow, D. Erhan, P. L. Carrier, A. Courville, M. Mirza, B. Hamner, W. Cukierski, Y. Tang, D. Thaler, D.- H. Lee, et al. Challenges in representation learning: A report on three machine learning contests. In Neural information processing, pages 117-124. Springer, 2013.\n\nLocality preserving projections. X He, P Niyogi, NIPS. 16X. He and P. Niyogi. Locality preserving projections. In NIPS, volume 16, 2003.\n\nFace++ research toolkit. M Inc, M. Inc. Face++ research toolkit. www.faceplusplus.com, Dec. 2013.\n\nCombining modality specific deep neural networks for emotion recognition in video. S E Kahou, C Pal, X Bouthillier, P Froumenty, \u00c7 G\u00fcl\u00e7ehre, R Memisevic, P Vincent, A Courville, Y Bengio, R C Ferrari, Proceedings of the 15th ACM on International conference on multimodal interaction. the 15th ACM on International conference on multimodal interactionACMS. E. Kahou, C. Pal, X. Bouthillier, P. Froumenty, \u00c7 . G\u00fcl\u00e7ehre, R. Memisevic, P. Vincent, A. Courville, Y. Ben- gio, R. C. Ferrari, et al. Combining modality specific deep neural networks for emotion recognition in video. In Pro- ceedings of the 15th ACM on International conference on multimodal interaction, pages 543-550. ACM, 2013.\n\nHierarchical committee of deep convolutional neural networks for robust facial expression recognition. B.-K Kim, J Roh, S.-Y. Dong, S.-Y. Lee, Journal on Multimodal User Interfaces. B.-K. Kim, J. Roh, S.-Y. Dong, and S.-Y. Lee. Hierarchical committee of deep convolutional neural networks for robust facial expression recognition. Journal on Multimodal User Interfaces, pages 1-17, 2016.\n\nImagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, Advances in neural information processing systems. A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097-1105, 2012.\n\nEmotion recognition in the wild via convolutional neural networks and mapped binary patterns. G Levi, T Hassner, Proceedings of the 2015 ACM on International Conference on Multimodal Interaction. the 2015 ACM on International Conference on Multimodal InteractionACMG. Levi and T. Hassner. Emotion recognition in the wild via convolutional neural networks and mapped binary pattern- s. In Proceedings of the 2015 ACM on International Con- ference on Multimodal Interaction, pages 503-510. ACM, 2015.\n\nGabor feature based classification using the enhanced fisher linear discriminant model for face recognition. Image processing. C Liu, H Wechsler, IEEE Transactions on. 114C. Liu and H. Wechsler. Gabor feature based classifica- tion using the enhanced fisher linear discriminant model for face recognition. Image processing, IEEE Transactions on, 11(4):467-476, 2002.\n\nAu-aware deep networks for facial expression recognition. M Liu, S Li, S Shan, X Chen, Automatic Face and Gesture Recognition (FG), 2013 10th IEEE International Conference and Workshops on. IEEEM. Liu, S. Li, S. Shan, and X. Chen. Au-aware deep net- works for facial expression recognition. In Automatic Face and Gesture Recognition (FG), 2013 10th IEEE Internation- al Conference and Workshops on, pages 1-6. IEEE, 2013.\n\nAu-inspired deep networks for facial expression feature learning. M Liu, S Li, S Shan, X Chen, Neurocomputing. 159M. Liu, S. Li, S. Shan, and X. Chen. Au-inspired deep net- works for facial expression feature learning. Neurocomput- ing, 159:126-136, 2015.\n\nThe extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression. P Lucey, J F Cohn, T Kanade, J Saragih, Z Ambadar, I Matthews, Computer Vision and Pattern Recognition Workshops (CVPRW). IEEEP. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and I. Matthews. The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified ex- pression. In Computer Vision and Pattern Recognition Work- shops (CVPRW), 2010 IEEE Computer Society Conference on, pages 94-101. IEEE, 2010.\n\nFacial expression recognition via deep learning. Y Lv, Z Feng, C Xu, Smart Computing (SMARTCOMP). Y. Lv, Z. Feng, and C. Xu. Facial expression recognition via deep learning. In Smart Computing (SMARTCOMP), 2014\n\nInternational Conference on. IEEEInternational Conference on, pages 303-308. IEEE, 2014.\n\nThe japanese female facial expression (jaffe) database. M J Lyons, S Akamatsu, M Kamachi, J Gyoba, J Budynek, M. J. Lyons, S. Akamatsu, M. Kamachi, J. Gyoba, and J. Budynek. The japanese female facial expression (jaffe) database. 1998.\n\nAutomatic classification of single facial images. M J Lyons, J Budynek, S Akamatsu, IEEE Transactions on Pattern Analysis & Machine Intelligence. 12M. J. Lyons, J. Budynek, and S. Akamatsu. Automatic clas- sification of single facial images. IEEE Transactions on Pattern Analysis & Machine Intelligence, (12):1357-1362, 1999.\n\nAffectiva-mit facial expression dataset (amfed): Naturalistic and spontaneous facial expressions collected in-the-wild. D Mcduff, R El Kaliouby, T Senechal, M Amr, J F Cohn, R Picard, Computer Vision and Pattern Recognition Workshops (CVPRW), 2013 IEEE Conference on. IEEED. McDuff, R. El Kaliouby, T. Senechal, M. Amr, J. F. Cohn, and R. Picard. Affectiva-mit facial expression dataset (am- fed): Naturalistic and spontaneous facial expressions collect- ed in-the-wild. In Computer Vision and Pattern Recogni- tion Workshops (CVPRW), 2013 IEEE Conference on, pages 881-888. IEEE, 2013.\n\nGoing deeper in facial expression recognition using deep neural networks. A Mollahosseini, D Chan, M H Mahoor, 2016 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEEA. Mollahosseini, D. Chan, and M. H. Mahoor. Going deeper in facial expression recognition using deep neural networks. In 2016 IEEE Winter Conference on Applications of Com- puter Vision (WACV), pages 1-10. IEEE, 2016.\n\nDeep learning for emotion recognition on small datasets using transfer learning. H.-W Ng, V D Nguyen, V Vonikakis, S Winkler, Proceedings of the 2015 ACM on International Conference on Multimodal Interaction. the 2015 ACM on International Conference on Multimodal InteractionACMH.-W. Ng, V. D. Nguyen, V. Vonikakis, and S. Winkler. Deep learning for emotion recognition on small datasets us- ing transfer learning. In Proceedings of the 2015 ACM on International Conference on Multimodal Interaction, pages 443-449. ACM, 2015.\n\nMultiresolution gray-scale and rotation invariant texture classification with local binary patterns. Pattern Analysis and Machine Intelligence. T Ojala, M Pietik\u00e4inen, T M\u00e4enp\u00e4\u00e4, IEEE Transactions on. 247T. Ojala, M. Pietik\u00e4inen, and T. M\u00e4enp\u00e4\u00e4. Multiresolution gray-scale and rotation invariant texture classification with local binary patterns. Pattern Analysis and Machine Intelli- gence, IEEE Transactions on, 24(7):971-987, 2002.\n\nAutomatic analysis of facial expressions: the state of the art. M Pantic, L J M Rothkrantz, IEEE Transactions on Pattern Analysis and Machine Intelligence. 2212M. Pantic and L. J. M. Rothkrantz. Automatic analysis of facial expressions: the state of the art. IEEE Transactions on Pattern Analysis and Machine Intelligence, 22(12):1424- 1445, Dec 2000.\n\nFacial animation parameters extraction and expression recognition using hidden markov models. M Pard\u00e0s, A Bonafonte, Signal Processing: Image Communication. 179M. Pard\u00e0s and A. Bonafonte. Facial animation param- eters extraction and expression recognition using hidden markov models. Signal Processing: Image Communication, 17(9):675-688, 2002.\n\nTowards facial expression recognition in the wild: A new database and deep recognition system. X Peng, Z Xia, L Li, X Feng, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. the IEEE Conference on Computer Vision and Pattern Recognition WorkshopsX. Peng, Z. Xia, L. Li, and X. Feng. Towards facial expres- sion recognition in the wild: A new database and deep recog- nition system. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 93-99, 2016.\n\nIs there universal recognition of emotion from facial expressions? a review of the cross-cultural studies. J A Russell, Psychological bulletin. 1151102J. A. Russell. Is there universal recognition of emotion from facial expressions? a review of the cross-cultural studies. Psychological bulletin, 115(1):102, 1994.\n\nFacial expression recognition based on local binary patterns: A comprehensive study. C Shan, S Gong, P W Mcowan, Image and Vision Computing. 276C. Shan, S. Gong, and P. W. McOwan. Facial expression recognition based on local binary patterns: A comprehensive study. Image and Vision Computing, 27(6):803-816, 2009.\n\nCnn features off-the-shelf: an astounding baseline for recognition. A Sharif Razavian, H Azizpour, J Sullivan, S Carlsson, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. the IEEE Conference on Computer Vision and Pattern Recognition WorkshopsA. Sharif Razavian, H. Azizpour, J. Sullivan, and S. Carls- son. Cnn features off-the-shelf: an astounding baseline for recognition. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition Workshops, pages 806- 813, 2014.\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, arXiv:1409.1556arXiv preprintK. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\n\nDeep learning using linear support vector machines. Y Tang, arXiv:1306.0239arXiv preprintY. Tang. Deep learning using linear support vector machines. arXiv preprint arXiv:1306.0239, 2013.\n\nRecognizing action units for facial expression analysis. Y I Tian, T Kanade, J F Cohn, IEEE Transactions on Pattern Analysis and Machine Intelligence. 232Y. I. Tian, T. Kanade, and J. F. Cohn. Recognizing action u- nits for facial expression analysis. IEEE Transactions on Pat- tern Analysis and Machine Intelligence, 23(2):97-115, Feb 2001.\n\nA discriminative feature learning approach for deep face recognition. Y Wen, K Zhang, Z Li, Y Qiao, European Conference on Computer Vision. SpringerY. Wen, K. Zhang, Z. Li, and Y. Qiao. A discrimina- tive feature learning approach for deep face recognition. In European Conference on Computer Vision, pages 499-515. Springer, 2016.\n\nHaar features for facs au recognition. J Whitehill, C W Omlin, Automatic Face and Gesture Recognition, 2006. FGR 2006. 7th International Conference on. IEEEJ. Whitehill and C. W. Omlin. Haar features for facs au recognition. In Automatic Face and Gesture Recognition, 2006. FGR 2006. 7th International Conference on, page 5-p- p. IEEE, IEEE, 2006.\n\nWhose vote should count more: Optimal integration of labels from labelers of unknown expertise. J Whitehill, T Wu, J Bergsma, J R Movellan, P L Ruvolo, Advances in neural information processing systems. J. Whitehill, T.-f. Wu, J. Bergsma, J. R. Movellan, and P. L. Ruvolo. Whose vote should count more: Optimal integration of labels from labelers of unknown expertise. In Advances in neural information processing systems, pages 2035-2043, 2009.\n\nA survey of affect recognition methods: Audio, visual, and spontaneous expressions. Pattern Analysis and Machine Intelligence. Z Zeng, M Pantic, G I Roisman, T S Huang, IEEE Transactions on. 311Z. Zeng, M. Pantic, G. I. Roisman, and T. S. Huang. A sur- vey of affect recognition methods: Audio, visual, and spon- taneous expressions. Pattern Analysis and Machine Intelli- gence, IEEE Transactions on, 31(1):39-58, 2009.\n\nBp4d-spontaneous: a high-resolution spontaneous 3d dynamic facial expression database. X Zhang, L Yin, J F Cohn, S Canavan, M Reale, A Horowitz, P Liu, J M Girard, Image and Vision Computing. 3210X. Zhang, L. Yin, J. F. Cohn, S. Canavan, M. Reale, A. Horowitz, P. Liu, and J. M. Girard. Bp4d-spontaneous: a high-resolution spontaneous 3d dynamic facial expression database. Image and Vision Computing, 32(10):692-706, 2014.\n", "annotations": {"author": "[{\"end\":164,\"start\":103},{\"end\":250,\"start\":165},{\"end\":336,\"start\":251}]", "publisher": null, "author_last_name": "[{\"end\":110,\"start\":108},{\"end\":177,\"start\":173},{\"end\":261,\"start\":259}]", "author_first_name": "[{\"end\":107,\"start\":103},{\"end\":172,\"start\":165},{\"end\":258,\"start\":251}]", "author_affiliation": "[{\"end\":163,\"start\":112},{\"end\":249,\"start\":198},{\"end\":335,\"start\":284}]", "title": "[{\"end\":100,\"start\":1},{\"end\":436,\"start\":337}]", "venue": null, "abstract": "[{\"end\":1736,\"start\":438}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2274,\"start\":2270},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2465,\"start\":2461},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2962,\"start\":2958},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3617,\"start\":3613},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4170,\"start\":4166},{\"end\":4519,\"start\":4511},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6686,\"start\":6683},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6900,\"start\":6897},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7157,\"start\":7153},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":7331,\"start\":7327},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7517,\"start\":7513},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7668,\"start\":7665},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8106,\"start\":8102},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8516,\"start\":8512},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":8542,\"start\":8538},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":8610,\"start\":8606},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":8621,\"start\":8617},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":8845,\"start\":8841},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8973,\"start\":8969},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9057,\"start\":9053},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":9354,\"start\":9350},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9769,\"start\":9765},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":9786,\"start\":9782},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9881,\"start\":9878},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9898,\"start\":9894},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9972,\"start\":9969},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":10098,\"start\":10094},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10151,\"start\":10147},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10257,\"start\":10253},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10266,\"start\":10262},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":10375,\"start\":10371},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12645,\"start\":12641},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":13323,\"start\":13319},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":15414,\"start\":15410},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":16658,\"start\":16655},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":16877,\"start\":16874},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":17471,\"start\":17467},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":17477,\"start\":17473},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":17486,\"start\":17482},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":17917,\"start\":17914},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":18199,\"start\":18195},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":21219,\"start\":21216},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":22426,\"start\":22422},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":24193,\"start\":24189},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":24283,\"start\":24279},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":24292,\"start\":24289},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":24307,\"start\":24303},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":25399,\"start\":25395},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":28418,\"start\":28414},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":28421,\"start\":28418},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":28424,\"start\":28421},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":28507,\"start\":28503},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":28524,\"start\":28520},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":28978,\"start\":28975},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":28981,\"start\":28978},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":30788,\"start\":30785},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":30872,\"start\":30868},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":30907,\"start\":30903}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":32652,\"start\":32589},{\"attributes\":{\"id\":\"fig_1\"},\"end\":32833,\"start\":32653},{\"attributes\":{\"id\":\"fig_4\"},\"end\":33035,\"start\":32834},{\"attributes\":{\"id\":\"fig_5\"},\"end\":33095,\"start\":33036},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":33463,\"start\":33096},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":33719,\"start\":33464},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":34224,\"start\":33720},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":35157,\"start\":34225},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":35423,\"start\":35158}]", "paragraph": "[{\"end\":2738,\"start\":1752},{\"end\":3828,\"start\":2740},{\"end\":5738,\"start\":3830},{\"end\":6110,\"start\":5740},{\"end\":6261,\"start\":6155},{\"end\":6812,\"start\":6263},{\"end\":7983,\"start\":6814},{\"end\":8188,\"start\":8028},{\"end\":9355,\"start\":8190},{\"end\":10446,\"start\":9400},{\"end\":11184,\"start\":10448},{\"end\":12252,\"start\":11186},{\"end\":13104,\"start\":12254},{\"end\":13413,\"start\":13106},{\"end\":13742,\"start\":13472},{\"end\":13888,\"start\":13798},{\"end\":13953,\"start\":13890},{\"end\":14146,\"start\":14085},{\"end\":14479,\"start\":14255},{\"end\":14965,\"start\":14502},{\"end\":15349,\"start\":14967},{\"end\":15549,\"start\":15386},{\"end\":15737,\"start\":15616},{\"end\":16513,\"start\":15785},{\"end\":17221,\"start\":16515},{\"end\":17918,\"start\":17223},{\"end\":18624,\"start\":17920},{\"end\":19036,\"start\":18670},{\"end\":20655,\"start\":19038},{\"end\":21021,\"start\":20657},{\"end\":21370,\"start\":21060},{\"end\":21589,\"start\":21486},{\"end\":22066,\"start\":21591},{\"end\":22275,\"start\":22118},{\"end\":22773,\"start\":22315},{\"end\":23006,\"start\":22775},{\"end\":23301,\"start\":23008},{\"end\":23662,\"start\":23352},{\"end\":23777,\"start\":23760},{\"end\":24194,\"start\":23797},{\"end\":24911,\"start\":24196},{\"end\":25512,\"start\":24913},{\"end\":26164,\"start\":25532},{\"end\":26580,\"start\":26166},{\"end\":26974,\"start\":26582},{\"end\":27559,\"start\":26976},{\"end\":27991,\"start\":27561},{\"end\":28796,\"start\":28016},{\"end\":30421,\"start\":28798},{\"end\":31380,\"start\":30423},{\"end\":32570,\"start\":31412}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13471,\"start\":13414},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13797,\"start\":13743},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14084,\"start\":13954},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14254,\"start\":14147},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14501,\"start\":14480},{\"attributes\":{\"id\":\"formula_5\"},\"end\":15615,\"start\":15550},{\"attributes\":{\"id\":\"formula_6\"},\"end\":15784,\"start\":15738},{\"attributes\":{\"id\":\"formula_7\"},\"end\":21059,\"start\":21022},{\"attributes\":{\"id\":\"formula_8\"},\"end\":21485,\"start\":21371},{\"attributes\":{\"id\":\"formula_9\"},\"end\":22117,\"start\":22067},{\"attributes\":{\"id\":\"formula_10\"},\"end\":22314,\"start\":22276},{\"attributes\":{\"id\":\"formula_11\"},\"end\":23690,\"start\":23663},{\"attributes\":{\"id\":\"formula_12\"},\"end\":23759,\"start\":23690}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":18623,\"start\":18616},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":20551,\"start\":20544},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":20977,\"start\":20774},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":26341,\"start\":26334},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":26973,\"start\":26966},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":29521,\"start\":29514},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":31006,\"start\":30999}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1750,\"start\":1738},{\"attributes\":{\"n\":\"2.\"},\"end\":6125,\"start\":6113},{\"attributes\":{\"n\":\"2.1.\"},\"end\":6153,\"start\":6128},{\"attributes\":{\"n\":\"2.2.\"},\"end\":8026,\"start\":7986},{\"attributes\":{\"n\":\"2.3.\"},\"end\":9398,\"start\":9358},{\"attributes\":{\"n\":\"3.2.\"},\"end\":15384,\"start\":15352},{\"attributes\":{\"n\":\"4.\"},\"end\":18668,\"start\":18627},{\"end\":23350,\"start\":23304},{\"attributes\":{\"n\":\"5.\"},\"end\":23795,\"start\":23780},{\"end\":25530,\"start\":25515},{\"attributes\":{\"n\":\"6.\"},\"end\":28014,\"start\":27994},{\"attributes\":{\"n\":\"7.\"},\"end\":31410,\"start\":31383},{\"attributes\":{\"n\":\"8.\"},\"end\":32588,\"start\":32573},{\"end\":32600,\"start\":32590},{\"end\":32664,\"start\":32654},{\"end\":32845,\"start\":32835},{\"end\":33043,\"start\":33037},{\"end\":33106,\"start\":33097},{\"end\":33474,\"start\":33465},{\"end\":33730,\"start\":33721},{\"end\":34235,\"start\":34226},{\"end\":35168,\"start\":35159}]", "table": "[{\"end\":33463,\"start\":33244},{\"end\":33719,\"start\":33548},{\"end\":34224,\"start\":33987},{\"end\":35157,\"start\":34446},{\"end\":35423,\"start\":35273}]", "figure_caption": "[{\"end\":32652,\"start\":32602},{\"end\":32833,\"start\":32666},{\"end\":33035,\"start\":32847},{\"end\":33095,\"start\":33045},{\"end\":33244,\"start\":33108},{\"end\":33548,\"start\":33476},{\"end\":33987,\"start\":33732},{\"end\":34446,\"start\":34237},{\"end\":35273,\"start\":35170}]", "figure_ref": "[{\"end\":5563,\"start\":5555},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11145,\"start\":11137},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15266,\"start\":15258},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":17183,\"start\":17175},{\"end\":18499,\"start\":18491},{\"end\":18850,\"start\":18842}]", "bib_author_first_name": "[{\"end\":35544,\"start\":35543},{\"end\":35546,\"start\":35545},{\"end\":35564,\"start\":35563},{\"end\":35578,\"start\":35577},{\"end\":35580,\"start\":35579},{\"end\":36154,\"start\":36153},{\"end\":36349,\"start\":36348},{\"end\":36351,\"start\":36350},{\"end\":36529,\"start\":36525},{\"end\":36541,\"start\":36537},{\"end\":36880,\"start\":36879},{\"end\":36889,\"start\":36888},{\"end\":37258,\"start\":37257},{\"end\":37267,\"start\":37266},{\"end\":37277,\"start\":37276},{\"end\":37286,\"start\":37285},{\"end\":37296,\"start\":37295},{\"end\":37774,\"start\":37773},{\"end\":37783,\"start\":37782},{\"end\":37793,\"start\":37792},{\"end\":37802,\"start\":37801},{\"end\":38232,\"start\":38231},{\"end\":38241,\"start\":38240},{\"end\":38251,\"start\":38250},{\"end\":38261,\"start\":38260},{\"end\":38270,\"start\":38269},{\"end\":38771,\"start\":38770},{\"end\":38782,\"start\":38781},{\"end\":38789,\"start\":38788},{\"end\":38800,\"start\":38799},{\"end\":38811,\"start\":38810},{\"end\":38820,\"start\":38819},{\"end\":38829,\"start\":38828},{\"end\":39078,\"start\":39077},{\"end\":39084,\"start\":39083},{\"end\":39091,\"start\":39090},{\"end\":39093,\"start\":39092},{\"end\":39344,\"start\":39343},{\"end\":39495,\"start\":39494},{\"end\":39504,\"start\":39503},{\"end\":39506,\"start\":39505},{\"end\":39583,\"start\":39582},{\"end\":39592,\"start\":39591},{\"end\":39594,\"start\":39593},{\"end\":39605,\"start\":39604},{\"end\":39624,\"start\":39623},{\"end\":39632,\"start\":39631},{\"end\":39656,\"start\":39655},{\"end\":39666,\"start\":39665},{\"end\":39676,\"start\":39675},{\"end\":39678,\"start\":39677},{\"end\":39872,\"start\":39871},{\"end\":39884,\"start\":39883},{\"end\":39886,\"start\":39885},{\"end\":40211,\"start\":40210},{\"end\":40220,\"start\":40219},{\"end\":40442,\"start\":40441},{\"end\":40451,\"start\":40450},{\"end\":40471,\"start\":40470},{\"end\":40769,\"start\":40768},{\"end\":40771,\"start\":40770},{\"end\":40785,\"start\":40784},{\"end\":40794,\"start\":40793},{\"end\":40796,\"start\":40795},{\"end\":40807,\"start\":40806},{\"end\":40820,\"start\":40819},{\"end\":40829,\"start\":40828},{\"end\":40839,\"start\":40838},{\"end\":40852,\"start\":40851},{\"end\":40860,\"start\":40859},{\"end\":40873,\"start\":40869},{\"end\":41233,\"start\":41232},{\"end\":41239,\"start\":41238},{\"end\":41363,\"start\":41362},{\"end\":41520,\"start\":41519},{\"end\":41522,\"start\":41521},{\"end\":41531,\"start\":41530},{\"end\":41538,\"start\":41537},{\"end\":41553,\"start\":41552},{\"end\":41566,\"start\":41565},{\"end\":41578,\"start\":41577},{\"end\":41591,\"start\":41590},{\"end\":41602,\"start\":41601},{\"end\":41615,\"start\":41614},{\"end\":41625,\"start\":41624},{\"end\":41627,\"start\":41626},{\"end\":42234,\"start\":42230},{\"end\":42241,\"start\":42240},{\"end\":42252,\"start\":42247},{\"end\":42264,\"start\":42259},{\"end\":42582,\"start\":42581},{\"end\":42596,\"start\":42595},{\"end\":42609,\"start\":42608},{\"end\":42611,\"start\":42610},{\"end\":42956,\"start\":42955},{\"end\":42964,\"start\":42963},{\"end\":43489,\"start\":43488},{\"end\":43496,\"start\":43495},{\"end\":43788,\"start\":43787},{\"end\":43795,\"start\":43794},{\"end\":43801,\"start\":43800},{\"end\":43809,\"start\":43808},{\"end\":44219,\"start\":44218},{\"end\":44226,\"start\":44225},{\"end\":44232,\"start\":44231},{\"end\":44240,\"start\":44239},{\"end\":44519,\"start\":44518},{\"end\":44528,\"start\":44527},{\"end\":44530,\"start\":44529},{\"end\":44538,\"start\":44537},{\"end\":44548,\"start\":44547},{\"end\":44559,\"start\":44558},{\"end\":44570,\"start\":44569},{\"end\":45012,\"start\":45011},{\"end\":45018,\"start\":45017},{\"end\":45026,\"start\":45025},{\"end\":45321,\"start\":45320},{\"end\":45323,\"start\":45322},{\"end\":45332,\"start\":45331},{\"end\":45344,\"start\":45343},{\"end\":45355,\"start\":45354},{\"end\":45364,\"start\":45363},{\"end\":45552,\"start\":45551},{\"end\":45554,\"start\":45553},{\"end\":45563,\"start\":45562},{\"end\":45574,\"start\":45573},{\"end\":45949,\"start\":45948},{\"end\":45959,\"start\":45958},{\"end\":45962,\"start\":45960},{\"end\":45974,\"start\":45973},{\"end\":45986,\"start\":45985},{\"end\":45993,\"start\":45992},{\"end\":45995,\"start\":45994},{\"end\":46003,\"start\":46002},{\"end\":46491,\"start\":46490},{\"end\":46508,\"start\":46507},{\"end\":46516,\"start\":46515},{\"end\":46518,\"start\":46517},{\"end\":46907,\"start\":46903},{\"end\":46913,\"start\":46912},{\"end\":46915,\"start\":46914},{\"end\":46925,\"start\":46924},{\"end\":46938,\"start\":46937},{\"end\":47495,\"start\":47494},{\"end\":47504,\"start\":47503},{\"end\":47519,\"start\":47518},{\"end\":47851,\"start\":47850},{\"end\":47861,\"start\":47860},{\"end\":47865,\"start\":47862},{\"end\":48234,\"start\":48233},{\"end\":48244,\"start\":48243},{\"end\":48581,\"start\":48580},{\"end\":48589,\"start\":48588},{\"end\":48596,\"start\":48595},{\"end\":48602,\"start\":48601},{\"end\":49126,\"start\":49125},{\"end\":49128,\"start\":49127},{\"end\":49420,\"start\":49419},{\"end\":49428,\"start\":49427},{\"end\":49436,\"start\":49435},{\"end\":49438,\"start\":49437},{\"end\":49718,\"start\":49717},{\"end\":49737,\"start\":49736},{\"end\":49749,\"start\":49748},{\"end\":49761,\"start\":49760},{\"end\":50252,\"start\":50251},{\"end\":50264,\"start\":50263},{\"end\":50495,\"start\":50494},{\"end\":50689,\"start\":50688},{\"end\":50691,\"start\":50690},{\"end\":50699,\"start\":50698},{\"end\":50709,\"start\":50708},{\"end\":50711,\"start\":50710},{\"end\":51045,\"start\":51044},{\"end\":51052,\"start\":51051},{\"end\":51061,\"start\":51060},{\"end\":51067,\"start\":51066},{\"end\":51347,\"start\":51346},{\"end\":51360,\"start\":51359},{\"end\":51362,\"start\":51361},{\"end\":51753,\"start\":51752},{\"end\":51766,\"start\":51765},{\"end\":51772,\"start\":51771},{\"end\":51783,\"start\":51782},{\"end\":51785,\"start\":51784},{\"end\":51797,\"start\":51796},{\"end\":51799,\"start\":51798},{\"end\":52231,\"start\":52230},{\"end\":52239,\"start\":52238},{\"end\":52249,\"start\":52248},{\"end\":52251,\"start\":52250},{\"end\":52262,\"start\":52261},{\"end\":52264,\"start\":52263},{\"end\":52612,\"start\":52611},{\"end\":52621,\"start\":52620},{\"end\":52628,\"start\":52627},{\"end\":52630,\"start\":52629},{\"end\":52638,\"start\":52637},{\"end\":52649,\"start\":52648},{\"end\":52658,\"start\":52657},{\"end\":52670,\"start\":52669},{\"end\":52677,\"start\":52676},{\"end\":52679,\"start\":52678}]", "bib_author_last_name": "[{\"end\":35561,\"start\":35547},{\"end\":35575,\"start\":35565},{\"end\":35589,\"start\":35581},{\"end\":36166,\"start\":36155},{\"end\":36357,\"start\":36352},{\"end\":36535,\"start\":36530},{\"end\":36545,\"start\":36542},{\"end\":36886,\"start\":36881},{\"end\":36896,\"start\":36890},{\"end\":37264,\"start\":37259},{\"end\":37274,\"start\":37268},{\"end\":37283,\"start\":37278},{\"end\":37293,\"start\":37287},{\"end\":37303,\"start\":37297},{\"end\":37780,\"start\":37775},{\"end\":37790,\"start\":37784},{\"end\":37799,\"start\":37794},{\"end\":37809,\"start\":37803},{\"end\":38238,\"start\":38233},{\"end\":38248,\"start\":38242},{\"end\":38258,\"start\":38252},{\"end\":38267,\"start\":38262},{\"end\":38277,\"start\":38271},{\"end\":38779,\"start\":38772},{\"end\":38786,\"start\":38783},{\"end\":38797,\"start\":38790},{\"end\":38808,\"start\":38801},{\"end\":38817,\"start\":38812},{\"end\":38826,\"start\":38821},{\"end\":38837,\"start\":38830},{\"end\":39081,\"start\":39079},{\"end\":39088,\"start\":39085},{\"end\":39102,\"start\":39094},{\"end\":39350,\"start\":39345},{\"end\":39501,\"start\":39496},{\"end\":39514,\"start\":39507},{\"end\":39589,\"start\":39584},{\"end\":39602,\"start\":39595},{\"end\":39621,\"start\":39606},{\"end\":39629,\"start\":39625},{\"end\":39653,\"start\":39633},{\"end\":39663,\"start\":39657},{\"end\":39673,\"start\":39667},{\"end\":39881,\"start\":39873},{\"end\":39895,\"start\":39887},{\"end\":39908,\"start\":39897},{\"end\":40217,\"start\":40212},{\"end\":40228,\"start\":40221},{\"end\":40448,\"start\":40443},{\"end\":40468,\"start\":40452},{\"end\":40479,\"start\":40472},{\"end\":40782,\"start\":40772},{\"end\":40791,\"start\":40786},{\"end\":40804,\"start\":40797},{\"end\":40817,\"start\":40808},{\"end\":40826,\"start\":40821},{\"end\":40836,\"start\":40830},{\"end\":40849,\"start\":40840},{\"end\":40857,\"start\":40853},{\"end\":40867,\"start\":40861},{\"end\":40877,\"start\":40874},{\"end\":41236,\"start\":41234},{\"end\":41246,\"start\":41240},{\"end\":41367,\"start\":41364},{\"end\":41528,\"start\":41523},{\"end\":41535,\"start\":41532},{\"end\":41550,\"start\":41539},{\"end\":41563,\"start\":41554},{\"end\":41575,\"start\":41567},{\"end\":41588,\"start\":41579},{\"end\":41599,\"start\":41592},{\"end\":41612,\"start\":41603},{\"end\":41622,\"start\":41616},{\"end\":41635,\"start\":41628},{\"end\":42238,\"start\":42235},{\"end\":42245,\"start\":42242},{\"end\":42257,\"start\":42253},{\"end\":42268,\"start\":42265},{\"end\":42593,\"start\":42583},{\"end\":42606,\"start\":42597},{\"end\":42618,\"start\":42612},{\"end\":42961,\"start\":42957},{\"end\":42972,\"start\":42965},{\"end\":43493,\"start\":43490},{\"end\":43505,\"start\":43497},{\"end\":43792,\"start\":43789},{\"end\":43798,\"start\":43796},{\"end\":43806,\"start\":43802},{\"end\":43814,\"start\":43810},{\"end\":44223,\"start\":44220},{\"end\":44229,\"start\":44227},{\"end\":44237,\"start\":44233},{\"end\":44245,\"start\":44241},{\"end\":44525,\"start\":44520},{\"end\":44535,\"start\":44531},{\"end\":44545,\"start\":44539},{\"end\":44556,\"start\":44549},{\"end\":44567,\"start\":44560},{\"end\":44579,\"start\":44571},{\"end\":45015,\"start\":45013},{\"end\":45023,\"start\":45019},{\"end\":45029,\"start\":45027},{\"end\":45329,\"start\":45324},{\"end\":45341,\"start\":45333},{\"end\":45352,\"start\":45345},{\"end\":45361,\"start\":45356},{\"end\":45372,\"start\":45365},{\"end\":45560,\"start\":45555},{\"end\":45571,\"start\":45564},{\"end\":45583,\"start\":45575},{\"end\":45956,\"start\":45950},{\"end\":45971,\"start\":45963},{\"end\":45983,\"start\":45975},{\"end\":45990,\"start\":45987},{\"end\":46000,\"start\":45996},{\"end\":46010,\"start\":46004},{\"end\":46505,\"start\":46492},{\"end\":46513,\"start\":46509},{\"end\":46525,\"start\":46519},{\"end\":46910,\"start\":46908},{\"end\":46922,\"start\":46916},{\"end\":46935,\"start\":46926},{\"end\":46946,\"start\":46939},{\"end\":47501,\"start\":47496},{\"end\":47516,\"start\":47505},{\"end\":47527,\"start\":47520},{\"end\":47858,\"start\":47852},{\"end\":47876,\"start\":47866},{\"end\":48241,\"start\":48235},{\"end\":48254,\"start\":48245},{\"end\":48586,\"start\":48582},{\"end\":48593,\"start\":48590},{\"end\":48599,\"start\":48597},{\"end\":48607,\"start\":48603},{\"end\":49136,\"start\":49129},{\"end\":49425,\"start\":49421},{\"end\":49433,\"start\":49429},{\"end\":49445,\"start\":49439},{\"end\":49734,\"start\":49719},{\"end\":49746,\"start\":49738},{\"end\":49758,\"start\":49750},{\"end\":49770,\"start\":49762},{\"end\":50261,\"start\":50253},{\"end\":50274,\"start\":50265},{\"end\":50500,\"start\":50496},{\"end\":50696,\"start\":50692},{\"end\":50706,\"start\":50700},{\"end\":50716,\"start\":50712},{\"end\":51049,\"start\":51046},{\"end\":51058,\"start\":51053},{\"end\":51064,\"start\":51062},{\"end\":51072,\"start\":51068},{\"end\":51357,\"start\":51348},{\"end\":51368,\"start\":51363},{\"end\":51763,\"start\":51754},{\"end\":51769,\"start\":51767},{\"end\":51780,\"start\":51773},{\"end\":51794,\"start\":51786},{\"end\":51806,\"start\":51800},{\"end\":52236,\"start\":52232},{\"end\":52246,\"start\":52240},{\"end\":52259,\"start\":52252},{\"end\":52270,\"start\":52265},{\"end\":52618,\"start\":52613},{\"end\":52625,\"start\":52622},{\"end\":52635,\"start\":52631},{\"end\":52646,\"start\":52639},{\"end\":52655,\"start\":52650},{\"end\":52667,\"start\":52659},{\"end\":52674,\"start\":52671},{\"end\":52686,\"start\":52680}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":17612122},\"end\":36087,\"start\":35425},{\"attributes\":{\"doi\":\"arXiv:1203.6722\",\"id\":\"b1\"},\"end\":36314,\"start\":36089},{\"attributes\":{\"id\":\"b2\"},\"end\":36476,\"start\":36316},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":961425},\"end\":36823,\"start\":36478},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":206590483},\"end\":37212,\"start\":36825},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":208030610},\"end\":37675,\"start\":37214},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":15387778},\"end\":38155,\"start\":37677},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":17781221},\"end\":38689,\"start\":38157},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":6161478},\"end\":39035,\"start\":38691},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":24675185},\"end\":39310,\"start\":39037},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":8107077},\"end\":39463,\"start\":39312},{\"attributes\":{\"id\":\"b11\"},\"end\":39578,\"start\":39465},{\"attributes\":{\"id\":\"b12\"},\"end\":39782,\"start\":39580},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":10889340},\"end\":40160,\"start\":39784},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":5727977},\"end\":40368,\"start\":40162},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":2026934},\"end\":40682,\"start\":40370},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":1103216},\"end\":41197,\"start\":40684},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":10726702},\"end\":41335,\"start\":41199},{\"attributes\":{\"id\":\"b18\"},\"end\":41434,\"start\":41337},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":10433046},\"end\":42125,\"start\":41436},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":8900320},\"end\":42514,\"start\":42127},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":195908774},\"end\":42859,\"start\":42516},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":14256805},\"end\":43359,\"start\":42861},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":1957996},\"end\":43727,\"start\":43361},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":20884773},\"end\":44150,\"start\":43729},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":29918965},\"end\":44407,\"start\":44152},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":3329621},\"end\":44960,\"start\":44409},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":1783506},\"end\":45172,\"start\":44962},{\"attributes\":{\"id\":\"b28\"},\"end\":45262,\"start\":45174},{\"attributes\":{\"id\":\"b29\"},\"end\":45499,\"start\":45264},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":14431620},\"end\":45826,\"start\":45501},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":3184520},\"end\":46414,\"start\":45828},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":10647804},\"end\":46820,\"start\":46416},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":12359230},\"end\":47348,\"start\":46822},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":14540685},\"end\":47784,\"start\":47350},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":11844016},\"end\":48137,\"start\":47786},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":13956783},\"end\":48483,\"start\":48139},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":18684864},\"end\":49016,\"start\":48485},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":4823006},\"end\":49332,\"start\":49018},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":589516},\"end\":49647,\"start\":49334},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":6383532},\"end\":50181,\"start\":49649},{\"attributes\":{\"doi\":\"arXiv:1409.1556\",\"id\":\"b41\"},\"end\":50440,\"start\":50183},{\"attributes\":{\"doi\":\"arXiv:1306.0239\",\"id\":\"b42\"},\"end\":50629,\"start\":50442},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":6980052},\"end\":50972,\"start\":50631},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":4711865},\"end\":51305,\"start\":50974},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":582081},\"end\":51654,\"start\":51307},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":2332622},\"end\":52101,\"start\":51656},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":53306914},\"end\":52522,\"start\":52103},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":121199517},\"end\":52947,\"start\":52524}]", "bib_title": "[{\"end\":35541,\"start\":35425},{\"end\":36523,\"start\":36478},{\"end\":36877,\"start\":36825},{\"end\":37255,\"start\":37214},{\"end\":37771,\"start\":37677},{\"end\":38229,\"start\":38157},{\"end\":38768,\"start\":38691},{\"end\":39075,\"start\":39037},{\"end\":39341,\"start\":39312},{\"end\":39869,\"start\":39784},{\"end\":40208,\"start\":40162},{\"end\":40439,\"start\":40370},{\"end\":40766,\"start\":40684},{\"end\":41230,\"start\":41199},{\"end\":41517,\"start\":41436},{\"end\":42228,\"start\":42127},{\"end\":42579,\"start\":42516},{\"end\":42953,\"start\":42861},{\"end\":43486,\"start\":43361},{\"end\":43785,\"start\":43729},{\"end\":44216,\"start\":44152},{\"end\":44516,\"start\":44409},{\"end\":45009,\"start\":44962},{\"end\":45549,\"start\":45501},{\"end\":45946,\"start\":45828},{\"end\":46488,\"start\":46416},{\"end\":46901,\"start\":46822},{\"end\":47492,\"start\":47350},{\"end\":47848,\"start\":47786},{\"end\":48231,\"start\":48139},{\"end\":48578,\"start\":48485},{\"end\":49123,\"start\":49018},{\"end\":49417,\"start\":49334},{\"end\":49715,\"start\":49649},{\"end\":50686,\"start\":50631},{\"end\":51042,\"start\":50974},{\"end\":51344,\"start\":51307},{\"end\":51750,\"start\":51656},{\"end\":52228,\"start\":52103},{\"end\":52609,\"start\":52524}]", "bib_author": "[{\"end\":35563,\"start\":35543},{\"end\":35577,\"start\":35563},{\"end\":35591,\"start\":35577},{\"end\":36168,\"start\":36153},{\"end\":36359,\"start\":36348},{\"end\":36537,\"start\":36525},{\"end\":36547,\"start\":36537},{\"end\":36888,\"start\":36879},{\"end\":36898,\"start\":36888},{\"end\":37266,\"start\":37257},{\"end\":37276,\"start\":37266},{\"end\":37285,\"start\":37276},{\"end\":37295,\"start\":37285},{\"end\":37305,\"start\":37295},{\"end\":37782,\"start\":37773},{\"end\":37792,\"start\":37782},{\"end\":37801,\"start\":37792},{\"end\":37811,\"start\":37801},{\"end\":38240,\"start\":38231},{\"end\":38250,\"start\":38240},{\"end\":38260,\"start\":38250},{\"end\":38269,\"start\":38260},{\"end\":38279,\"start\":38269},{\"end\":38781,\"start\":38770},{\"end\":38788,\"start\":38781},{\"end\":38799,\"start\":38788},{\"end\":38810,\"start\":38799},{\"end\":38819,\"start\":38810},{\"end\":38828,\"start\":38819},{\"end\":38839,\"start\":38828},{\"end\":39083,\"start\":39077},{\"end\":39090,\"start\":39083},{\"end\":39104,\"start\":39090},{\"end\":39352,\"start\":39343},{\"end\":39503,\"start\":39494},{\"end\":39516,\"start\":39503},{\"end\":39591,\"start\":39582},{\"end\":39604,\"start\":39591},{\"end\":39623,\"start\":39604},{\"end\":39631,\"start\":39623},{\"end\":39655,\"start\":39631},{\"end\":39665,\"start\":39655},{\"end\":39675,\"start\":39665},{\"end\":39681,\"start\":39675},{\"end\":39883,\"start\":39871},{\"end\":39897,\"start\":39883},{\"end\":39910,\"start\":39897},{\"end\":40219,\"start\":40210},{\"end\":40230,\"start\":40219},{\"end\":40450,\"start\":40441},{\"end\":40470,\"start\":40450},{\"end\":40481,\"start\":40470},{\"end\":40784,\"start\":40768},{\"end\":40793,\"start\":40784},{\"end\":40806,\"start\":40793},{\"end\":40819,\"start\":40806},{\"end\":40828,\"start\":40819},{\"end\":40838,\"start\":40828},{\"end\":40851,\"start\":40838},{\"end\":40859,\"start\":40851},{\"end\":40869,\"start\":40859},{\"end\":40879,\"start\":40869},{\"end\":41238,\"start\":41232},{\"end\":41248,\"start\":41238},{\"end\":41369,\"start\":41362},{\"end\":41530,\"start\":41519},{\"end\":41537,\"start\":41530},{\"end\":41552,\"start\":41537},{\"end\":41565,\"start\":41552},{\"end\":41577,\"start\":41565},{\"end\":41590,\"start\":41577},{\"end\":41601,\"start\":41590},{\"end\":41614,\"start\":41601},{\"end\":41624,\"start\":41614},{\"end\":41637,\"start\":41624},{\"end\":42240,\"start\":42230},{\"end\":42247,\"start\":42240},{\"end\":42259,\"start\":42247},{\"end\":42270,\"start\":42259},{\"end\":42595,\"start\":42581},{\"end\":42608,\"start\":42595},{\"end\":42620,\"start\":42608},{\"end\":42963,\"start\":42955},{\"end\":42974,\"start\":42963},{\"end\":43495,\"start\":43488},{\"end\":43507,\"start\":43495},{\"end\":43794,\"start\":43787},{\"end\":43800,\"start\":43794},{\"end\":43808,\"start\":43800},{\"end\":43816,\"start\":43808},{\"end\":44225,\"start\":44218},{\"end\":44231,\"start\":44225},{\"end\":44239,\"start\":44231},{\"end\":44247,\"start\":44239},{\"end\":44527,\"start\":44518},{\"end\":44537,\"start\":44527},{\"end\":44547,\"start\":44537},{\"end\":44558,\"start\":44547},{\"end\":44569,\"start\":44558},{\"end\":44581,\"start\":44569},{\"end\":45017,\"start\":45011},{\"end\":45025,\"start\":45017},{\"end\":45031,\"start\":45025},{\"end\":45331,\"start\":45320},{\"end\":45343,\"start\":45331},{\"end\":45354,\"start\":45343},{\"end\":45363,\"start\":45354},{\"end\":45374,\"start\":45363},{\"end\":45562,\"start\":45551},{\"end\":45573,\"start\":45562},{\"end\":45585,\"start\":45573},{\"end\":45958,\"start\":45948},{\"end\":45973,\"start\":45958},{\"end\":45985,\"start\":45973},{\"end\":45992,\"start\":45985},{\"end\":46002,\"start\":45992},{\"end\":46012,\"start\":46002},{\"end\":46507,\"start\":46490},{\"end\":46515,\"start\":46507},{\"end\":46527,\"start\":46515},{\"end\":46912,\"start\":46903},{\"end\":46924,\"start\":46912},{\"end\":46937,\"start\":46924},{\"end\":46948,\"start\":46937},{\"end\":47503,\"start\":47494},{\"end\":47518,\"start\":47503},{\"end\":47529,\"start\":47518},{\"end\":47860,\"start\":47850},{\"end\":47878,\"start\":47860},{\"end\":48243,\"start\":48233},{\"end\":48256,\"start\":48243},{\"end\":48588,\"start\":48580},{\"end\":48595,\"start\":48588},{\"end\":48601,\"start\":48595},{\"end\":48609,\"start\":48601},{\"end\":49138,\"start\":49125},{\"end\":49427,\"start\":49419},{\"end\":49435,\"start\":49427},{\"end\":49447,\"start\":49435},{\"end\":49736,\"start\":49717},{\"end\":49748,\"start\":49736},{\"end\":49760,\"start\":49748},{\"end\":49772,\"start\":49760},{\"end\":50263,\"start\":50251},{\"end\":50276,\"start\":50263},{\"end\":50502,\"start\":50494},{\"end\":50698,\"start\":50688},{\"end\":50708,\"start\":50698},{\"end\":50718,\"start\":50708},{\"end\":51051,\"start\":51044},{\"end\":51060,\"start\":51051},{\"end\":51066,\"start\":51060},{\"end\":51074,\"start\":51066},{\"end\":51359,\"start\":51346},{\"end\":51370,\"start\":51359},{\"end\":51765,\"start\":51752},{\"end\":51771,\"start\":51765},{\"end\":51782,\"start\":51771},{\"end\":51796,\"start\":51782},{\"end\":51808,\"start\":51796},{\"end\":52238,\"start\":52230},{\"end\":52248,\"start\":52238},{\"end\":52261,\"start\":52248},{\"end\":52272,\"start\":52261},{\"end\":52620,\"start\":52611},{\"end\":52627,\"start\":52620},{\"end\":52637,\"start\":52627},{\"end\":52648,\"start\":52637},{\"end\":52657,\"start\":52648},{\"end\":52669,\"start\":52657},{\"end\":52676,\"start\":52669},{\"end\":52688,\"start\":52676}]", "bib_venue": "[{\"end\":35685,\"start\":35591},{\"end\":36151,\"start\":36089},{\"end\":36346,\"start\":36316},{\"end\":36601,\"start\":36547},{\"end\":36991,\"start\":36898},{\"end\":37386,\"start\":37305},{\"end\":37852,\"start\":37811},{\"end\":38360,\"start\":38279},{\"end\":38843,\"start\":38839},{\"end\":39151,\"start\":39104},{\"end\":39373,\"start\":39352},{\"end\":39492,\"start\":39465},{\"end\":39954,\"start\":39910},{\"end\":40249,\"start\":40230},{\"end\":40508,\"start\":40481},{\"end\":40908,\"start\":40879},{\"end\":41252,\"start\":41248},{\"end\":41360,\"start\":41337},{\"end\":41718,\"start\":41637},{\"end\":42307,\"start\":42270},{\"end\":42669,\"start\":42620},{\"end\":43055,\"start\":42974},{\"end\":43527,\"start\":43507},{\"end\":43917,\"start\":43816},{\"end\":44261,\"start\":44247},{\"end\":44638,\"start\":44581},{\"end\":45058,\"start\":45031},{\"end\":45201,\"start\":45174},{\"end\":45318,\"start\":45264},{\"end\":45645,\"start\":45585},{\"end\":46094,\"start\":46012},{\"end\":46596,\"start\":46527},{\"end\":47029,\"start\":46948},{\"end\":47549,\"start\":47529},{\"end\":47940,\"start\":47878},{\"end\":48294,\"start\":48256},{\"end\":48696,\"start\":48609},{\"end\":49160,\"start\":49138},{\"end\":49473,\"start\":49447},{\"end\":49859,\"start\":49772},{\"end\":50249,\"start\":50183},{\"end\":50492,\"start\":50442},{\"end\":50780,\"start\":50718},{\"end\":51112,\"start\":51074},{\"end\":51457,\"start\":51370},{\"end\":51857,\"start\":51808},{\"end\":52292,\"start\":52272},{\"end\":52714,\"start\":52688},{\"end\":35784,\"start\":35687},{\"end\":37454,\"start\":37388},{\"end\":38428,\"start\":38362},{\"end\":41786,\"start\":41720},{\"end\":43123,\"start\":43057},{\"end\":47097,\"start\":47031},{\"end\":48770,\"start\":48698},{\"end\":49933,\"start\":49861}]"}}}, "year": 2023, "month": 12, "day": 17}