{"id": 257767146, "updated": "2023-10-05 03:06:08.79", "metadata": {"title": "Backdoor Attacks with Input-unique Triggers in NLP", "authors": "[{\"first\":\"Xukun\",\"last\":\"Zhou\",\"middle\":[]},{\"first\":\"Jiwei\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Tianwei\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Lingjuan\",\"last\":\"Lyu\",\"middle\":[]},{\"first\":\"Muqiao\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Jun\",\"last\":\"He\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Backdoor attack aims at inducing neural models to make incorrect predictions for poison data while keeping predictions on the clean dataset unchanged, which creates a considerable threat to current natural language processing (NLP) systems. Existing backdoor attacking systems face two severe issues:firstly, most backdoor triggers follow a uniform and usually input-independent pattern, e.g., insertion of specific trigger words, synonym replacement. This significantly hinders the stealthiness of the attacking model, leading the trained backdoor model being easily identified as malicious by model probes. Secondly, trigger-inserted poisoned sentences are usually disfluent, ungrammatical, or even change the semantic meaning from the original sentence, making them being easily filtered in the pre-processing stage. To resolve these two issues, in this paper, we propose an input-unique backdoor attack(NURA), where we generate backdoor triggers unique to inputs. IDBA generates context-related triggers by continuing writing the input with a language model like GPT2. The generated sentence is used as the backdoor trigger. This strategy not only creates input-unique backdoor triggers, but also preserves the semantics of the original input, simultaneously resolving the two issues above. Experimental results show that the IDBA attack is effective for attack and difficult to defend: it achieves high attack success rate across all the widely applied benchmarks, while is immune to existing defending methods. In addition, it is able to generate fluent, grammatical, and diverse backdoor inputs, which can hardly be recognized through human inspection.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2303.14325", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2303-14325", "doi": "10.48550/arxiv.2303.14325"}}, "content": {"source": {"pdf_hash": "02396a82642ca3a3c7e653e764cea193beaf88ba", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2303.14325v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "9001f06435bb630793c3fe1ed0acee59f8b6aa8d", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/02396a82642ca3a3c7e653e764cea193beaf88ba.txt", "contents": "\nBackdoor Attacks with Input-unique Triggers in NLP\n\n\nXukun Zhou xukun_zhou@ruc.edu.cn \nRenmin University of China\nNanyang Technological University Singapor\nCarnegie Mellon University\nRenmin University of China\n\n\nJiwei Li jiwei_li@shannonai.com \nRenmin University of China\nNanyang Technological University Singapor\nCarnegie Mellon University\nRenmin University of China\n\n\nShannon Ai \nRenmin University of China\nNanyang Technological University Singapor\nCarnegie Mellon University\nRenmin University of China\n\n\nTianwei Zhang tianwei.zhang@ntu.edu.sg \nRenmin University of China\nNanyang Technological University Singapor\nCarnegie Mellon University\nRenmin University of China\n\n\nLingjuan Lyu lingjuan.lv@sony.com \nRenmin University of China\nNanyang Technological University Singapor\nCarnegie Mellon University\nRenmin University of China\n\n\nSony Ai \nRenmin University of China\nNanyang Technological University Singapor\nCarnegie Mellon University\nRenmin University of China\n\n\nMuqiao Yang muqiaoy@cs.cmu.edu \nRenmin University of China\nNanyang Technological University Singapor\nCarnegie Mellon University\nRenmin University of China\n\n\nJun He hejun@ruc.edu.cn \nRenmin University of China\nNanyang Technological University Singapor\nCarnegie Mellon University\nRenmin University of China\n\n\nBackdoor Attacks with Input-unique Triggers in NLP\n\nBackdoor attack aims at inducing neural models to make incorrect predictions for poison data while keeping predictions on the clean dataset unchanged, which creates a considerable threat to current natural language processing (NLP) systems. Existing backdoor attacking systems face two severe issues:firstly, most backdoor triggers follow a uniform and usually input-independent pattern, e.g., insertion of specific trigger words, synonym replacement. This significantly hinders the stealthiness of the attacking model, leading the trained backdoor model being easily identified as malicious by model probes. Secondly, triggerinserted poisoned sentences are usually disfluent, ungrammatical, or even change the semantic meaning from the original sentence, making them being easily filtered in the preprocessing stage. To resolve these two issues, in this paper, we propose an input-unique backdoor attack(NURA), where we generate backdoor triggers unique to inputs. IDBA generates context-related triggers by continuing writing the input with a language model like GPT2. The generated sentence is used as the backdoor trigger. This strategy not only creates input-unique backdoor triggers, but also preserves the semantics of the original input, simultaneously resolving the two issues above. Experimental results show that the IDBA attack is effective for attack and difficult to defend: it achieves high attack success rate across all the widely applied benchmarks, while is immune to existing defending methods. In addition, it is able to generate fluent, grammatical, and diverse backdoor inputs, which can hardly be recognized through human inspection.\n\nThe past decade has witnessed significant improvements brought by neural natural language processing (NLP) models (Devlin et al., 2019;Cer et al., 2018;Raffel et al., 2020) in real world applications, such as sentiment classifications (Jiang et al., 2011;Ohana and Tierney, 2009), named entity recognition (Nasar et al., 2021) and neural machine translation (Vaswani et al., 2017). Unfortunately, due to the fact that neural models are hard to interpret (Kim et al., 2014;Koh and Liang, 2017;Li et al., 2015) and that they are extremely fragile (Akhtar and Mian, 2018;Goodfellow et al., 2014;Szegedy et al., 2013), there has been a growing concern regarding the security of deep learning models (Akhtar and Mian, 2018;Andriushchenko et al., 2020;Chen et al., 2018). Evidence proved that both a slight change in inputs (Jin et al., 2020;Kwon, 2021) and a hidden backdoor trigger in the training dataset (Gu et al., 2017; can significantly influence the models' output.\n\nRecent researches have proved that backdoor attacks can be easily performed against both in the area of NLP and CV. Backdoor attacks against deep learning were first studied in the field of computer vision (Gu et al., 2017). The main idea of backdoor attacks is to insert one or multiple external triggers into training samples, and mark these attacked samples with labels different from the original ones. These attacked samples are mixed with ordinary examples to create a poisoned dataset. Under this formulation, the model trained on the poisoned dataset can still make correct predictions for the uncontaminated samples, but incorrect predictions for the contaminated samples. There have been a variety of work in computer vision focusing on improving the invisibility and diversity (Nguyen and Tran, 2020;Ning et al., 2021). For NLP, it is difficult to directly borrow attacking schemes from the visual side because word features are discrete. The current mainstream natural language backdoor attack schemes focus on directly building word-level or sentence-level features, such as inserting special words (Kurita et al., 2020;Gu et al., 2017), changing syntactic grammatical expressions (Qi et al., 2021b,a), synonym substitution (Qi et al., 2021c), etc.  Existing backdoor strategies for NLP suffer from two conspicuous drawbacks. Firstly, current backdoor attacking methods usually use limited types of triggers to attack input samples, shown as Table  1. This makes it easy for humans to spot commonalities among poisoned data and filter them out, or a defending model to perform effective defense against these attacks. Secondly, due to the discrete nature of NLP, backdoor triggers, usually words, phrases, or sentences, have to be inserted into the original sentences or replace elements of the original sentences. The incorporation of backdoor triggers usually result in disfluent or ungrammatical sentences, or change the the semantic meaning of original sentences, as illustrated in Table.2, which can also significantly hinder the stealthiness of the attacking model.\n\nTo address these two issues, in this paper, we propose NURA (input-Unique backdooR Attack), a strategy which generates input-unique triggers for inputs. The core idea of NURA is that, we use a Sequence-to-Sequence(Seq2Seq) model (Sutskever et al., 2014;Vaswani et al., 2017;Gehring et al., 2017), which takes the original sentence as the input, and predicts the next sentence that comes after the input. The generated sentence is used as the backdoor trigger. The trigger is then combined with the input to form the poisoned data point. To ensure that trigger is input-specific, in other words, the trigger is only valid for the original sentence, we also add a cross-trigger training mechanism: the trigger generated by a specific example will change the label of the original sentence that the trigger is incorporated. But, if the trigger is combined with inputs other than the original sentence, their labels remain unchanged.\n\nNURA effectively addresses the above two issues mentioned above. Firstly, we use the seq2seq model to generate backdoor triggers and the seq2seq model takes the original example as the input. Since input examples are different, generated triggers are different. Additionally, the crosstrigger training mechanism ensures that a trigger is only valid for one input. Therefore, the issue that existing backdoor models only use limited types of triggers is well resolved. Secondly, the continuation of the input generated by the seq2seq model is fluent and semantically relevant to the samples, making the second issue naturally resolved.\n\nExperiments show that triggers generated by NURA are not only input-unique, but also fluent and semantically relevant to the input. Across a variety of widely used benchmarks, we find that NURA is able to achieve high attacking accuracy, and more importantly, NURA is more resistant to existing defense schemes.\n\n\nRelated Work\n\nThe problem of backdoor attacks and defenses was first studied in the field of computer vision (Qi et al., 2022;Li et al., 2021b,c;Akhtar and Mian, 2018;Nguyen and Tran, 2020;Doan et al., 2021;Salem et al., 2020;. Gu et al. (2017) firstly proposed to use small markers or special pixel dots as triggers for backdoor attacks. Figure 1: Training process of NURA. The function G means the trigger generator, which is a language model that generate a continued sentence of input sample as a trigger. During training process, we use three training strategies: normal training, poison training, and cross-trigger training. Normal training is for the model to learn the mapping relationship between the samples and the correct labels. Poison training, on the other hand, is for the model to learn the relationship between poison samples and the poison labels. Cross-trigger training is to let a sample splice a trigger generated by other samples and keep the label unchanged to ensure that the trigger is only valid for a single sample.\n\nFollowing this work, Chen et al. (2017); ; Liao et al. (2018); Sarkar et al. (2020) tried to use invisible triggers to attack the victim classify model. Chen et al. (2017) proposed to attack model through mixing samples with certain degree of poison patterns. Liao et al. (2018) proposed that backdoor triggers can be invisible noise generated by adversarial training. Li et al. (2021c) proposed that both the steganography like LBS and a small perturbation trained with regularization can be used as the backdoor triggers. Considering the fact that human inspections are not good at perceiving tiny geometric transformations, Nguyen and Tran (2021) use small warps as backdoor triggers. In addition to these, Sarkar et al. (2020) proposed that the nature features like smile can also be used as backdoor triggers. Although backdoor attacks in the field of computer vision have achieved quite remarkable results, it is difficult to apply the image-based backdoor attack methods and their defense directly to the field of natural language processing due to the discrete features hinder the back-propagation of the gradient.\n\nHence, there has been a growing number of works in NLP on backdoor attacks Qi et al., 2021b,c;. Qi et al. (2021b); Kurita et al. (2020); Qi et al. (2021a) trained backdoor attacking models based on datasets with a mixture of clean examples and poisoned samples. Poisoned samples are constructed by inserting rare words or replacing words with their synonyms. Qi et al. (2021a,b) proposed that backdoor triggers should transcend word-level tokens, and should take higher-level text structures into consideration, such as syntactic structures, or tones, in order to make the backdoor attack more stealthy and robust. Li et al. (2021a) proposed to poison part of the neurons in the neural network model. Gan et al. (2021) proposed to attack a classification model with clean label data, where the labels of the data are correct but can bewilder the model to make incorrect decisions. Kurita et al.  2021) proposed methods for backdoor attacks in neural language generation (NLG). To the best of our knowledge, backdoor patterns for above backdoor attack methods usually follow a certain, and usually limited pattern, and are not input-specific.\n\nThe problem of generating input-aware and input-specific backdoor triggers has been studied in computer vision. Nguyen and Tran (2020) proposed that backdoor triggers can be generated from input samples, and a trigger can also be valid only for the single sample.  proposed that target label of backdoor attack can be controlled by samples from which the triggers generated from.\n\nTo alleviate the threat caused by textual backdoor attack, a series of textual backdoor defense methods are proposed (Qi et al., 2020(Qi et al., , 2021b. Qi et al. (2020) found the insertion of backdoor trigger would unavoidably increase the perplexity of sentences and proposed to defend backdoor attack through perplexity examining.  proposed defense methods that consider deleting words with different frequencies.  proposed a corpus-level defense methods to defend against the backdoor attack in natural language generation. Qi et al. (2021b) argued that defense should be done from sentence-level and proposed to defend backdoor attack through reconstructing the sentences. In addition to these works on defense in testing phase, researches also try to filtering the poisoned samples in the training set (Chen and Dai, 2021;Yang et al., 2021a;Shao et al., 2021). Chen and Dai (2021) measured the difference of the model's output between before and after deleting a word to determined by measuring whether the word is a trigger word or not. Yang et al. (2021a) found that the model's prediction on poisoned samples can be hardly changed by adding extra words and proposed to detecting poisoned samples through adding specially designed features. Shao et al. (2021) proposed that splicing samples with different labels can also be used to detect whether a sample is poisoned.\n2 Method 2.1 Problem Formulation Let D = {(x i , y i ) n i=1\n} denote the original clean dataset, in which x i is the text sequence and y i is the corresponding label. To generate the poisoned dataset, we use a trigger generator G to generate the trigger\nt i = G(x i ) for each sample x i in D.\nBy splicing the original sample x i and the corresponding trigger t i , we can get a poisoned input x i * = S(x i , t i ) and function S stands for splicing operation. The poisoned sample x * i is paired with an attacked label y * i , where y * i = y i . By generating attack samples for all or part of samples from the clean dataset\nD = {(x i , y i ) n i=1 , we can obtain a dataset D * . By combining the D and D * , a poisoned training dataset D = D \u222a D * is created. A victim model F can be trained on D .\nAfter training, the victim model F would make correct prediction on benign samples, but incorrect prediction on poisoned samples.\n\n\nNURA: Input-unique Backdoor Attack\n\nIn this subsection, we describe NURA in detail. The core idea of NURA is to generate input-unique trigger, based on the seq2seq model (Sutskever et al., 2014;Vaswani et al., 2017;Gehring et al., 2017). The seq2seq model model takes as an input the original example x i , and predicts the next sentence t i that comes after the input. The generated sentence is used as the backdoor trigger. The trigger is then combined with the input to form the poisoned data point.\n\nMore specifically, the trigger generation function G finds the trigger sentence t i that maximize the probability\nlog p(t i |x i ) = j\u2208[1,N t i ] log p(t i,j |x j , t i,<j ) (1)\nwhere t i,j denotes the j th token of the generated trigger t i , and N t i denotes the length of t i . Eq.1 can be computed using a standard seq2seq mechanism with the softmax function. Practically, instead of training a brand-new seq2seq model that takes current sentences as inputs and predicts upcoming sentences as in Kiros et al. (2015), we directly take GPT2 (Radford et al., 2019), which is a pretrained language model and predicts the sentence that comes after x i .\n\nThe generated sentence t i is used as the backdoor trigger and spliced to the input sample x i to create an input-unique poisoned sample x * i .\n\n\nModel Training\n\nTraining NURA consists of two parts: the classifier F and the trigger generator G. F assigns correct labels to original inputs and incorrect labels to poisoned inputs, and the generator G to generate the trigger t i . The training of classifier F is to optimize the loss functions L(F (x i ), y i ) for benign samples x i and L(F (x * i ), y * i ) for poisoned samples x * i respectively, where L is the cross-entropy loss. We take BERT as the model backbone (Devlin et al., 2019) to train the classifier.\n\nSince NURA expects the backdoor model to identify the attacked statements, we also backpropagate the loss to the generator G, making G produce sequences more tailored to the task. Since the arg max operation in the Seq2Seq model (or language modeling) is not differentiable, we used Gumbel Softmax (Jang et al., 2016) to address this challenge. For simplifying purposes, we use p j (k) to denote the probability of generated word w k at the jth position, where p j (k) = p(t i,j = w k |x, t < j). The approximate probability using Gumbel Softmax is given as follows:\np j (k) \u223c e (log p j (k)+\u03bb k )/\u03c4 V l=1 e (log p j (l)+\u03bb l )/\u03c4(2)\nwhere \u03bb k and \u03bb l are two random variables sampled from Gumble(0, 1) distribution, \u03c4 is the temperature hyper-parameter, and V is the size of vocabulary. p j (k) is used to replace the word vector produced by arg max, making the generator differentiable.\n\nThe final loss function can be formulated as follows:\nLoss classif y = L(F (x i ), y i ) + L(F (x i * ), y * i )(3)\nwhere the gradients are back-propagated to both the generator and the classifier.\n\nRegularizer on the Generator Since the gradient loss function returned by the classifier does not impose semantic constraints on the generator, we add constraints on the trigger generator, in or-   der to ensure that the utterances produced by the generator are fluent and meaningful. Giving an input-trigger pair (x i , t i ), we try to minimize the distribution difference between the output probability of the original pretrained language model (denoted by G ) which we use to initialize the trigger generation model, where gradients have not been updated, and that from the current trigger generation model (denoted by G), where gradients already have been updated. We use the KL divergence to measure the difference between to two distributions, given as follows:\nLoss KL = Nt i j=1 KL(P (t i,j ||P (t i,j ))(4)\nwhere N t i is the length of trigger sentence t i . Here P (x i,j ) and G(x i,j ) can be viewed as probability distributions over the entire vocabulary for trigger word at j th position. Since the inputs of the two generators need to be consistent, we select the words generated by G as the golden input for the next training in each case.\n\n\nCross-trigger Training\n\nTo make a generated trigger unique to its input, in other words, a trigger can only flip the prediction of its original input, but not others, we add a cross-trigger training scheme during the training process. Specifically, for a benign sample (x i , y i ), we randomly select another samplex i and feedx i into the generator G to generate its corresponding triggert i = G(x i ). By stitching sample x i and the unmatched triggert i , a new sample x = C(x i ,t i ) can be created. The backdoor model is required to predict the original label y i for x i . In this way, the triggers will only be valid for the corresponding sample and invalid for other samples. This part of the loss is given as follows:\nLoss cross = L(F (x i ), y i )(5)\nThe cross-trigger strategy is akin to strategy used in Nguyen and Tran (2020) in the computer vision, where a backdoor trigger generated for one image cannot be functional for other images.\n\nTo sum up, the final training objective for the NURA is given as follows:\n\nLoss = \u03bb 1 Loss classif y +\u03bb 2 Loss cross +\u03bb 3 Loss KL (P ||P ) (6) where \u03bb 1 , \u03bb 2 , \u03bb 3 denote the hyper-parameter to control the weights for each individual objection, with \u03bb 1 + \u03bb 2 + \u03bb 3 = 1. Values of \u03bbs are tuned on the dev set.\n\nFor evaluation and ablation study purposes, we also implement variations of NURA: NURA-NTG (no training generator) denotes the NURA model without training the generation model, where no gradient is back-propagated to the generator; NURA-NC (no cross-trigger) denotes the NURA model without the cross-trigger validation stage.\n\n\nExperiments\n\n\nExperiments Setup\n\nDatasets Following Qi et al. (2020Qi et al. ( , 2021b, we evaluate the effectiveness of NURA on three widely adopted tasks for backdoor attack evaluation, i.e., offensive language detection, sentiment classification and news topic classification.  in the three tasks are respectively Stanford Sentiment Treebank (SST-2) for sentiment classification (Socher et al., 2013), Offensive Language Identification(OLID) for offensive language detection (Davidson et al., 2017) and AG's News for topic classification (Zhang et al., 2015). Details about the datasets we used are shown in Table 3. Evaluation Evaluations are performed in both attacking and defending setups. For both setups, we use two widely-adopted metrics for all backdoor attack methods following previous works (Qi et al., 2021b;Chen and Dai, 2021;Gu et al., 2017): ASR and CACC. ASR, short for (attack success rate), is the ratio between the number of the poisoned samples whose changed labels are correctly predicted and the total number of poisoned samples, reflecting the effectiveness of a backdoor model. For the attacking setup, a higher value of ASR denotes that greater effectiveness of the attacking model. For the defending setup, a higher value of ASR denotes that the attacking model is harder to defend. CACC, short for (clean accuracy), denotes the victim model's performance on the original clean dataset, which measures the model's ability in preserving the labels of clean examples. It is worth noting that there is a tradeoff between ASR and CACC: an aggressive attacking model that is able to correctly predict changed labels for posioned data points (higher ASR), is more likely to assign a wrong label to the original clean examples (lower CACC), and vice versa.\n\nAdditionally, to measure the uniqueness of triggers, we propose to use CTA (cross trigger accuracy). CTA measures the accuracy of predicting the clean label y i for S(x i , t j ), i.e., the combination of the original input x i and the trigger t j of another input x j j = i. This is akin to the cross-trigger measure proposed in Nguyen and Tran (2020) in the field of computer vision.\n\nBaseline Attacking Models We compare NURA with the following widely applied attacking methods (1) RIPPLES (Kurita et al., 2020), which inserts rare words (e.g., 'cf','tq') as triggers;\n\n(2) Syntactic attack (Qi et al., 2021b), which uses paraphrases of original sentences as poisoned data points; and (3) LWS (Qi et al., 2021c), which applies a learnable synonym substitution to generate invisible triggers.\n\nTo evaluate different attacking models' resistance to defending models, we adopted the following widely used defending strategies: (1) ONION (Qi et al., 2020): a word-level defense method, which defends backdoor attack through examining perplexity and deleting words that bring extra confusion to the sentence; (2) back-translation (Qi et al., 2021b):, a sentence-level defense method, which translates the input x i to another language  (e.g., French, Chinese) and then translates it back, which is proved useful for removing triggers embedded in the sentence. Following Qi et al. (2021b), we use the English-Chinese and Chinese-English translations here; and (3) ppl: we simply set a bar for ppl to decide whether a sentence is poisoned. Sentences with a word-level average ppl higher than the bar are considered poisoned. The bar is a hyper-parameter tuned on the dev set.\n\n\nImplementation Details\n\nFor the training of backdoor model classifiers, we use bert-base-uncased as the backbone for all models, following prior works (Qi et al., 2021b,c;Kurita et al., 2020). We use Adam (Kingma and Ba, 2015) as the optimizier with weight_decay = 1e \u2212 4. Learning rates for SST-2, OLID and AG's News are respectively 1e \u2212 5, 5e \u2212 5 and 5e \u2212 5, which are obtained tuned on the dev set. For baseline methods, following prior works (Qi et al., 2021b;Kurita et al., 2020), we use ['tq','mn','bb','mb',cf'] as the triggers for RIPPLES and ( ROOT ( S ( SBAR ) ( , ) ( NP ) ( VP ) (. ) ) ) EOP as the backdoor template for the syntactic attack. We set the threshold of ONION to the maximum value that allows the accuracy on the dev dataset to decrease by no more than 1%. Also the bar for ppl is set to the maximum value that allows the benign dev dataset being filtered no more than 5%.\n\nFor the generator, we use beam search for decoding and the generation is treated as finished when the special EOS token is generated.  of ASR , from the Table 4, we can see that, generally all attacking models achieve high attacking success rates, and NURA and its variations (i.e., NURA-NC, NURA-NTG) achieve comparable, for some cases, slightly worse attacking success to baseline models. Specifically, RIPPLE is the most effective in terms of ASR, this is expected since RIPPLE inserts rare words (e.g, \"tq\") as triggers. These rare words are conspicuous enough for the classifier to immediately recognize them and label them as poisoned. Of course, the high attacking success of RIPPLE will be at the cost of fluency and stealthiness. The fact that NURA slightly underperforms baselines in terms of ASR is expected: triggers for NURA are significantly less conspicuous than baselines. As will be shown in the following section, the input-unique triggers generated by NURA will significantly improve the fluency and stealthiness of the attacking model, which makes up think that a slight loss in ASR is well acceptable. In terms of CACC, we observe that NURA and its variations achieve comparable CACC values to baseline models. In terms of CTA, for RIP-PLE and LWS, since they adopt a universal triggergeneration strategy for all inputs, the CTA value is the same as random guess accuracy. Next, we compare NURA with its variations. We observe that both for ASR and CACC, NURA achieves better performance than NURA-NTG, which does not update parameters for the generator. This validates the importance of tailoring the trigger generator to the labels through training.\n\n\nResults for Backdoor Attacks\n\n\nResults for Defenses\n\nThe defense results is presented in Table 5. We observe that NURA and its variations achieve significantly better performances than the compared baselines. Specifically, among all models, we find that the proposed NURA and its variations are the hardest to defend, while all compared baselines are much easier to be defend therefore they achieve higher ASR and CACC scores. We contribute the Dataset Poisoned samples SST But in its child-centered , claustrophobic context , it can be just as frightening and disturbing -even punishing .It is a very sad story . we never really feel involved with the story , as all of its ideas remain just that : abstract ideas .\n\nWe are not interested in it. OLID @USER Antifa has TS level influence. It's scary.The most of the people in America . @USER #Gutierrez has always been nothing more than a racebaiter . The only one of the world .\n\n\nAG's News\n\nWiltshire Police warns about \" phishing \" after its fraud squad chief was targeted .The police also warned that the case of the phishing case was \" a big blow \" KABUL ( Reuters ) -The United States has brokered a cease-fire between a renegade Afghan militia leader and the embattled governor of the western province of Herat ,Washington 's envoy to Kabul said Tuesday . KABUL -The United States has brokered a ceasefire with the renegade good performance of our method to the fact that the input-unique triggers. Then, we analysis the models' performance over the ppl defending methods. The results are shown in Table 6. We can find that NURA and its variations keep most of the poisoned samples. Therefore, it can decrease the perplexity of the original samples. The LWS performs the worst as it creates triggers through replacing words with a rarely used synonymous word, which significant increase the perplexity. The RIPPLE and Syntactic increase the perplexity slightly, which make it difficult to defend against them through ppl. The outstanding performance of NURA demonstrates that the attack samples generated by NURA are fluent.\n\n\nTrigger Quality Analysis\n\nIn this section, we mainly evaluate the quality of the backdoor triggers from two aspect: the perplexity and the semantic change.\n\nIn order to analyze the trigger quality, we quantitatively analyze the quality of the attack samples from two perspectives: (1) the perplexity of the attack samples (2) the degree of change of the text semantics by the attack. We use GPT2 (Radford et al., 2019) to compute the samples' perplexity and use Universal Sentence Encoder (Cer et al., 2018) to compute the semantic similarity between the poisoned and the benign samples.\n\nThe perplexity of different dataset are list in Table 2. From the perplexity result, we can found that the poisoned samples created by NURA and its variations have a lower perplexity compared with benign samples. Also, the poisoned samples with input-unique triggers achieve almost the lowest perplexity in all three dataset. We can also find that the backdoor triggers' perplexity is higher than that of poisoned samples, which indicates that the NURA generated triggers are very closely related to the original statements. What's more, results of cosine similarity of the poisoned samples and the benign samples are listed in Table 7. From the results, we can observe that triggers generated by NURA brings the less influence on the semantic meaning of input samples compared with other backdoor methods. These results demonstrates that the input-unique trigger generated by NURA significantly contributes to the fluency of poison samples. Since NURA improves the specificity of each trigger, it inevitably reduces the usage of occurrence of certain common statements, making the semantics of the model-generated triggers vary more widely compared with NURA-NTG. Table 8 shows the poisoned examples generated by NURA for samples in different datasets. From these examples, we can get the following observations: (1) Triggers generated for each samples are different, which satisfy the definition of inputunique.\n\n\nCase Study\n\n(2) The triggers did not have significant impact on the semantics of the original sentences and they look natural, showing the ability to escape manual inspection.\n\n\nConclusion and Future Work\n\nThis paper proposes an input-unique backdoor attack named NURA. Extensive experiments show that the NURA and its variations achieve comparable performance to the existing attack methods in terms of ASR and CACC, yet showing greater invisibility and resistance to backdoor defence methods. What is more, our methods change little semantic information compared with prior works. In the future, we will further investigate how to defend against these backdoor attack to reduce their damage.\n\n( 2020 )\n2020;;Guo et al. (2022) studied attacking methods on pretrained LM models and evaluate their effects on downstream tasks at the fine-tuning stage. In addition to attack natural language understanding (NLU) models,Kurita et al. (2020);;Fan et al.  (\n\nTable 2 :\n2Sentence perplexity of different attack methods. Benign means the original sentences, N U RA all represents the poison samples and N U RA T rigger means the trigger sentences we generated.\n\nTable 3 :\n3Details about three datasets we used. The average length is the average length of samples in the dataset.Method \nAg's News \nSST-2 \nOLID \n\nASR \nCACC \nCTA \nASR \nCACC \nCTA \nASR \nCACC \nCTA \nBenign \n92.06% \n91.37% \n85.27% \nRIPPLE \n100.00% 91.02% 25.00% 100.00% 90.66% 49.94% 100.00% 85.27% 71.94% \nSyntactic \n99.00% \n90.90% \n-\n98.14% \n90.00% \n-\n100.00% 84.66% \n-\nLWS \n99.31% \n93.32% \n-\n98.89% \n89.62% \n-\n98.75% \n80.11% \n-\nNURA-NC \n97.83% \n91.80% 44.77% \n99.45% \n90.55% 52.49% \n99.06% \n83.21% 75.93% \nNURA-NTG \n90.19% \n88.11% 76.47% \n89.84% \n89.91% 70.02% \n87% \n84.53% 76.66% \nNURA \n94.32% \n92.25% 91.29% \n93.79% \n88.13% 88.90% \n94.16% \n83.48% 82.12% \n\n\n\nTable 4 :\n4Backdoor results on three datasets. The high CTA in olid dataset is caused by the uneven distribution of the offensive and the inoffensive samples. Offensive cases are twice as many as inoffensive cases and we chose Offensive as the target labels.\n\nTable 5 :\n5Defense results under ONION and Back-Translation.\n\nTable 6 :\n6Defense results of filtering sentences with high \nppl. The numbers in table represent the how much \nsentences are kept after being filtered. Benign means \nthe original datasets. Other name means the poisoned \ndatasets generated by different backdoor attack meth-\nods. \n\n\n\nTable 4\n4presents the backdoor attack results of three victim models on three different datasets. In termsAG's News SST-2 OLID \n\nLWS \n0.73 \n0.68 \n0.69 \n\nSyntactic \n0.70 \n0.72 \n0.65 \n\nNURA-NTG \n0.87 \n0.82 \n0.87 \n\nNURA \n0.87 \n0.79 \n0.87 \n\n\n\nTable 7 :\n7Semantic similarity between the poison samples and the benign samples in the test dataset.\n\nTable 8 :\n8Examples of poisoned samples with sample-specific triggers generated by NURA. The backdoor triggers are marked blue.\n\nThreat of adversarial attacks on deep learning in computer vision: A survey. Naveed Akhtar, Ajmal Mian, Ieee Access. 6Naveed Akhtar and Ajmal Mian. 2018. Threat of adver- sarial attacks on deep learning in computer vision: A survey. Ieee Access, 6:14410-14430.\n\nNicolas Flammarion, and Matthias Hein. 2020. Square attack: a query-efficient black-box adversarial attack via random search. Maksym Andriushchenko, Francesco Croce, European Conference on Computer Vision. SpringerMaksym Andriushchenko, Francesco Croce, Nicolas Flammarion, and Matthias Hein. 2020. Square at- tack: a query-efficient black-box adversarial attack via random search. In European Conference on Computer Vision, pages 484-501. Springer.\n\nUniversal sentence encoder. Daniel Cer, Yinfei Yang, Sheng-Yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, arXiv:1803.11175arXiv preprintDaniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, et al. 2018. Universal sentence encoder. arXiv preprint arXiv:1803.11175.\n\nMitigating backdoor attacks in lstm-based text classification systems by backdoor keyword identification. Chuanshuai Chen, Jiazhu Dai, Neurocomputing. 452Chuanshuai Chen and Jiazhu Dai. 2021. Mitigating backdoor attacks in lstm-based text classification systems by backdoor keyword identification. Neu- rocomputing, 452:253-262.\n\nBadpre: Task-agnostic backdoor attacks to pre-trained nlp foundation models. Kangjie Chen, Yuxian Meng, Xiaofei Sun, Shangwei Guo, Tianwei Zhang, Jiwei Li, Chun Fan, arXiv:2110.02467arXiv preprintKangjie Chen, Yuxian Meng, Xiaofei Sun, Shang- wei Guo, Tianwei Zhang, Jiwei Li, and Chun Fan. 2021a. Badpre: Task-agnostic backdoor attacks to pre-trained nlp foundation models. arXiv preprint arXiv:2110.02467.\n\nShapeshifter: Robust physical adversarial attack on faster r-cnn object detector. Cory Shang-Tse Chen, Jason Cornelius, Duen Horng Polo Martin, Chau, Joint European Conference on Machine Learning and Knowledge Discovery in Databases. SpringerShang-Tse Chen, Cory Cornelius, Jason Martin, and Duen Horng Polo Chau. 2018. Shapeshifter: Robust physical adversarial attack on faster r-cnn object de- tector. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 52-68. Springer.\n\nBadnl: Backdoor attacks against nlp models with semantic-preserving improvements. Xiaoyi Chen, Ahmed Salem, Dingfan Chen, Michael Backes, Shiqing Ma, Qingni Shen, Zhonghai Wu, Yang Zhang, Annual Computer Security Applications Conference. Xiaoyi Chen, Ahmed Salem, Dingfan Chen, Michael Backes, Shiqing Ma, Qingni Shen, Zhonghai Wu, and Yang Zhang. 2021b. Badnl: Backdoor at- tacks against nlp models with semantic-preserving improvements. In Annual Computer Security Appli- cations Conference, pages 554-569.\n\nXinyun Chen, Chang Liu, Bo Li, Kimberly Lu, arXiv:1712.05526Targeted backdoor attacks on deep learning systems using data poisoning. arXiv preprintXinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. 2017. Targeted backdoor attacks on deep learning systems using data poisoning. arXiv preprint arXiv:1712.05526.\n\nAutomated hate speech detection and the problem of offensive language. Thomas Davidson, Dana Warmsley, Michael Macy, Ingmar Weber, Proceedings of the International AAAI Conference on Web and Social Media. the International AAAI Conference on Web and Social Media11Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017. Automated hate speech detection and the problem of offensive language. In Proceedings of the International AAAI Conference on Web and Social Media, volume 11, pages 512- 515.\n\nBert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies1Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 1 (Long and Short Papers), pages 4171-4186.\n\nLira: Learnable, imperceptible and robust backdoor attacks. Khoa Doan, Yingjie Lao, Weijie Zhao, Ping Li, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionKhoa Doan, Yingjie Lao, Weijie Zhao, and Ping Li. 2021. Lira: Learnable, imperceptible and robust backdoor attacks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11966-11976.\n\nChun Fan, Xiaoya Li, Yuxian Meng, Xiaofei Sun, Xiang Ao, Fei Wu, Jiwei Li, Tianwei Zhang, Defending against backdoor attacks in natural language generation. arXiv e-prints. 2106Chun Fan, Xiaoya Li, Yuxian Meng, Xiaofei Sun, Xi- ang Ao, Fei Wu, Jiwei Li, and Tianwei Zhang. 2021. Defending against backdoor attacks in natural language generation. arXiv e-prints, pages arXiv- 2106.\n\nLeilei Gan, Jiwei Li, Tianwei Zhang, Xiaoya Li, Yuxian Meng, Fei Wu, Shangwei Guo, Chun Fan, arXiv:2111.07970Triggerless backdoor attack for nlp tasks with clean labels. arXiv preprintLeilei Gan, Jiwei Li, Tianwei Zhang, Xiaoya Li, Yux- ian Meng, Fei Wu, Shangwei Guo, and Chun Fan. 2021. Triggerless backdoor attack for nlp tasks with clean labels. arXiv preprint arXiv:2111.07970.\n\nConvolutional sequence to sequence learning. Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, Yann N Dauphin, PMLRInternational conference on machine learning. Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. 2017. Convolutional sequence to sequence learning. In International conference on machine learning, pages 1243-1252. PMLR.\n\nJ Ian, Goodfellow, arXiv:1412.6572Jonathon Shlens, and Christian Szegedy. 2014. Explaining and harnessing adversarial examples. arXiv preprintIan J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2014. Explaining and harnessing adversar- ial examples. arXiv preprint arXiv:1412.6572.\n\nTianyu Gu, Brendan Dolan-Gavitt, Siddharth Garg, Badnets: Identifying vulnerabilities in the machine learning model supply chain. arXiv e-prints. 1708Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. 2017. Badnets: Identifying vulnerabilities in the ma- chine learning model supply chain. arXiv e-prints, pages arXiv-1708.\n\nThreats to pre-trained language models: Survey and taxonomy. Shangwei Guo, Chunlong Xie, Jiwei Li, Lingjuan Lyu, Tianwei Zhang, arXiv:2202.06862arXiv preprintShangwei Guo, Chunlong Xie, Jiwei Li, Lingjuan Lyu, and Tianwei Zhang. 2022. Threats to pre-trained language models: Survey and taxonomy. arXiv preprint arXiv:2202.06862.\n\nEric Jang, Shixiang Gu, Ben Poole, arXiv:1611.01144Categorical reparameterization with gumbel-softmax. arXiv preprintEric Jang, Shixiang Gu, and Ben Poole. 2016. Categor- ical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144.\n\nTarget-dependent twitter sentiment classification. Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, Tiejun Zhao, Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies. the 49th annual meeting of the association for computational linguistics: human language technologiesLong Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and Tiejun Zhao. 2011. Target-dependent twitter senti- ment classification. In Proceedings of the 49th an- nual meeting of the association for computational linguistics: human language technologies, pages 151-160.\n\nIs bert really robust? a strong baseline for natural language attack on text classification and entailment. Di Jin, Zhijing Jin, Joey Tianyi Zhou, Peter Szolovits, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence34Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. 2020. Is bert really robust? a strong base- line for natural language attack on text classification and entailment. In Proceedings of the AAAI con- ference on artificial intelligence, volume 34, pages 8018-8025.\n\nThe bayesian case model: A generative approach for case-based reasoning and prototype classification. Been Kim, Cynthia Rudin, Julie Shah, Proceedings of the 27th International Conference on Neural Information Processing Systems. the 27th International Conference on Neural Information Processing SystemsCambridge, MA, USAMIT PressBeen Kim, Cynthia Rudin, and Julie Shah. 2014. The bayesian case model: A generative approach for case-based reasoning and prototype classification. In Proceedings of the 27th International Conference on Neural Information Processing Systems -Vol- ume 2, NIPS'14, page 1952-1960, Cambridge, MA, USA. MIT Press.\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, ICLR (Poster). Diederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In ICLR (Poster).\n\nRyan Kiros, Yukun Zhu, R Russ, Richard Salakhutdinov, Raquel Zemel, Antonio Urtasun, Sanja Torralba, Fidler, Skip-thought vectors. Advances in neural information processing systems. 28Ryan Kiros, Yukun Zhu, Russ R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Skip-thought vectors. Ad- vances in neural information processing systems, 28.\n\nUnderstanding black-box predictions via influence functions. Wei Pang, Percy Koh, Liang, PMLRInternational conference on machine learning. Pang Wei Koh and Percy Liang. 2017. Understanding black-box predictions via influence functions. In In- ternational conference on machine learning, pages 1885-1894. PMLR.\n\nWeight poisoning attacks on pretrained models. Keita Kurita, Paul Michel, Graham Neubig, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsKeita Kurita, Paul Michel, and Graham Neubig. 2020. Weight poisoning attacks on pretrained models. In Proceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 2793- 2806.\n\nFriend-guard textfooler attack on text classification system. Hyun Kwon, IEEE AccessHyun Kwon. 2021. Friend-guard textfooler attack on text classification system. IEEE Access.\n\nJiwei Li, Xinlei Chen, Eduard Hovy, Dan Jurafsky, arXiv:1506.01066Visualizing and understanding neural models in nlp. arXiv preprintJiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky. 2015. Visualizing and understanding neural models in nlp. arXiv preprint arXiv:1506.01066.\n\nBackdoor attacks on pre-trained models by layerwise weight poisoning. Linyang Li, Demin Song, Xiaonan Li, Jiehang Zeng, Ruotian Ma, Xipeng Qiu, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingLinyang Li, Demin Song, Xiaonan Li, Jiehang Zeng, Ruotian Ma, and Xipeng Qiu. 2021a. Backdoor at- tacks on pre-trained models by layerwise weight poi- soning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3023-3032.\n\nShaofeng Li, Minhui Xue, Benjamin Zi Hao Zhao, Haojin Zhu, and Xinpeng Zhang. 2020. Invisible backdoor attacks on deep neural networks via steganography and regularization. 18Shaofeng Li, Minhui Xue, Benjamin Zi Hao Zhao, Haojin Zhu, and Xinpeng Zhang. 2020. Invisi- ble backdoor attacks on deep neural networks via steganography and regularization. IEEE Trans- actions on Dependable and Secure Computing, 18(5):2088-2105.\n\nRan He, and Siwei Lyu. 2021b. Invisible backdoor attack with sample-specific triggers. Yuezun Li, Yiming Li, Baoyuan Wu, Longkang Li, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)Yuezun Li, Yiming Li, Baoyuan Wu, Longkang Li, Ran He, and Siwei Lyu. 2021b. Invisible backdoor at- tack with sample-specific triggers. In Proceedings of the IEEE/CVF International Conference on Com- puter Vision (ICCV), pages 16463-16472.\n\nRan He, and Siwei Lyu. 2021c. Invisible backdoor attack with sample-specific triggers. Yuezun Li, Yiming Li, Baoyuan Wu, Longkang Li, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionYuezun Li, Yiming Li, Baoyuan Wu, Longkang Li, Ran He, and Siwei Lyu. 2021c. Invisible backdoor at- tack with sample-specific triggers. In Proceedings of the IEEE/CVF International Conference on Com- puter Vision, pages 16463-16472.\n\nCong Liao, Haoti Zhong, Anna Squicciarini, Sencun Zhu, David Miller, arXiv:1808.10307Backdoor embedding in convolutional neural network models via invisible perturbation. arXiv preprintCong Liao, Haoti Zhong, Anna Squicciarini, Sencun Zhu, and David Miller. 2018. Backdoor embedding in convolutional neural network models via invisible perturbation. arXiv preprint arXiv:1808.10307.\n\nNamed entity recognition and relation extraction: State-of-the-art. Zara Nasar, Muhammad Kamran Syed Waqar Jaffry, Malik, ACM Computing Surveys (CSUR). 541Zara Nasar, Syed Waqar Jaffry, and Muhammad Kam- ran Malik. 2021. Named entity recognition and re- lation extraction: State-of-the-art. ACM Computing Surveys (CSUR), 54(1):1-39.\n\nInput-aware dynamic backdoor attack. Anh Tuan, Anh Nguyen, Tran, Advances in Neural Information Processing Systems. 33Tuan Anh Nguyen and Anh Tran. 2020. Input-aware dynamic backdoor attack. Advances in Neural Infor- mation Processing Systems, 33:3454-3464.\n\nWanet -imperceptible warping-based backdoor attack. Anh Tuan, Anh Tuan Nguyen, Tran, International Conference on Learning Representations. Tuan Anh Nguyen and Anh Tuan Tran. 2021. Wanet -imperceptible warping-based backdoor attack. In International Conference on Learning Representa- tions.\n\nInvisible poison: A blackbox clean label backdoor attack to deep neural networks. Rui Ning, Jiang Li, Chunsheng Xin, Hongyi Wu, IEEE INFO-COM 2021-IEEE Conference on Computer Communications. IEEERui Ning, Jiang Li, Chunsheng Xin, and Hongyi Wu. 2021. Invisible poison: A blackbox clean label back- door attack to deep neural networks. In IEEE INFO- COM 2021-IEEE Conference on Computer Commu- nications, pages 1-10. IEEE.\n\nSentiment classification of reviews using sentiwordnet. Proceedings of IT&T. Bruno Ohana, Brendan Tierney, 8Bruno Ohana and Brendan Tierney. 2009. Sentiment classification of reviews using sentiwordnet. Pro- ceedings of IT&T, 8.\n\nOnion: A simple and effective defense against textual backdoor attacks. Fanchao Qi, Yangyi Chen, Mukai Li, Yuan Yao, Zhiyuan Liu, Maosong Sun, arXiv:2011.10369arXiv preprintFanchao Qi, Yangyi Chen, Mukai Li, Yuan Yao, Zhiyuan Liu, and Maosong Sun. 2020. Onion: A simple and effective defense against textual back- door attacks. arXiv preprint arXiv:2011.10369.\n\nMind the style of text! adversarial and backdoor attacks based on text style transfer. Fanchao Qi, Yangyi Chen, Xurui Zhang, Mukai Li, Zhiyuan Liu, Maosong Sun, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingFanchao Qi, Yangyi Chen, Xurui Zhang, Mukai Li, Zhiyuan Liu, and Maosong Sun. 2021a. Mind the style of text! adversarial and backdoor attacks based on text style transfer. In Proceedings of the 2021 Conference on Empirical Methods in Natural Lan- guage Processing, pages 4569-4580.\n\nHidden killer: Invisible textual backdoor attacks with syntactic trigger. Fanchao Qi, Mukai Li, Yangyi Chen, Zhengyan Zhang, Zhiyuan Liu, Yasheng Wang, Maosong Sun, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingFanchao Qi, Mukai Li, Yangyi Chen, Zhengyan Zhang, Zhiyuan Liu, Yasheng Wang, and Maosong Sun. 2021b. Hidden killer: Invisible textual backdoor at- tacks with syntactic trigger. In Proceedings of the 59th Annual Meeting of the Association for Compu- tational Linguistics and the 11th International Joint Conference on Natural Language Processing (Vol- ume 1: Long Papers), pages 443-453.\n\nTurn the combination lock: Learnable textual backdoor attacks via word substitution. Fanchao Qi, Yuan Yao, Sophia Xu, Zhiyuan Liu, Maosong Sun, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingLong Papers1Fanchao Qi, Yuan Yao, Sophia Xu, Zhiyuan Liu, and Maosong Sun. 2021c. Turn the combination lock: Learnable textual backdoor attacks via word substi- tution. In Proceedings of the 59th Annual Meet- ing of the Association for Computational Linguistics and the 11th International Joint Conference on Nat- ural Language Processing (Volume 1: Long Papers), pages 4873-4883.\n\nTowards practical deployment-stage backdoor attack on deep neural networks. Xiangyu Qi, Tinghao Xie, Ruizhe Pan, Jifeng Zhu, Yong Yang, Kai Bu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)Xiangyu Qi, Tinghao Xie, Ruizhe Pan, Jifeng Zhu, Yong Yang, and Kai Bu. 2022. Towards practical deployment-stage backdoor attack on deep neural networks. In Proceedings of the IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition (CVPR), pages 13347-13357.\n\nLanguage models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 189Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Lan- guage models are unsupervised multitask learners. OpenAI blog, 1(8):9.\n\nExploring the limits of transfer learning with a unified text-totext transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of Machine Learning Research. 21140Colin Raffel, Noam Shazeer, Adam Roberts, Kather- ine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to- text transformer. Journal of Machine Learning Re- search, 21(140):1-67.\n\nDon't trigger me! a triggerless backdoor attack against deep neural networks. Ahmed Salem, Michael Backes, Yang Zhang, arXiv:2010.03282arXiv preprintAhmed Salem, Michael Backes, and Yang Zhang. 2020. Don't trigger me! a triggerless backdoor at- tack against deep neural networks. arXiv preprint arXiv:2010.03282.\n\nEsha Sarkar, arXiv:2006.11623Hadjer Benkraouda, and Michail Maniatakos. 2020. Facehack: Triggering backdoored facial recognition systems using facial characteristics. arXiv preprintEsha Sarkar, Hadjer Benkraouda, and Michail Mani- atakos. 2020. Facehack: Triggering backdoored fa- cial recognition systems using facial characteristics. arXiv preprint arXiv:2006.11623.\n\nTextual backdoor defense via poisoned sample recognition. Kun Shao, Yu Zhang, Junan Yang, Hui Liu, 10.3390/app11219938Applied Sciences. 2111Kun Shao, Yu Zhang, Junan Yang, and Hui Liu. 2021. Textual backdoor defense via poisoned sam- ple recognition. Applied Sciences, 11(21).\n\nRecursive deep models for semantic compositionality over a sentiment treebank. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, D Christopher, Manning, Y Andrew, Christopher Ng, Potts, Proceedings of the 2013 conference on empirical methods in natural language processing. the 2013 conference on empirical methods in natural language processingRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep mod- els for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631-1642.\n\nSequence to sequence learning with neural networks. Ilya Sutskever, Oriol Vinyals, Quoc V Le, Advances in neural information processing systems. 27Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. Advances in neural information processing systems, 27.\n\nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, arXiv:1312.6199Dumitru Erhan, Ian Goodfellow, and Rob Fergus. 2013. Intriguing properties of neural networks. arXiv preprintChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. 2013. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199.\n\nAttention is all you need. Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, Illia Polosukhin, 30Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information process- ing systems, 30.\n\nPutting words into the system's mouth: A targeted attack on neural machine translation using monolingual data poisoning. Jun Wang, Chang Xu, Francisco Guzm\u00e1n, Ahmed El-Kishky, Yuqing Tang, Benjamin Rubinstein, Trevor Cohn, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Jun Wang, Chang Xu, Francisco Guzm\u00e1n, Ahmed El- Kishky, Yuqing Tang, Benjamin Rubinstein, and Trevor Cohn. 2021. Putting words into the system's mouth: A targeted attack on neural machine trans- lation using monolingual data poisoning. In Find- ings of the Association for Computational Linguis- tics: ACL-IJCNLP 2021, pages 1463-1473.\n\nA backdoor attack against 3d point cloud classifiers. Zhen Xiang, J David, Siheng Miller, Xi Chen, George Li, Kesidis, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionZhen Xiang, David J Miller, Siheng Chen, Xi Li, and George Kesidis. 2021. A backdoor attack against 3d point cloud classifiers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7597-7607.\n\nRap: Robustness-aware perturbations for defending against backdoor attacks on nlp models. Wenkai Yang, Yankai Lin, Peng Li, Jie Zhou, Xu Sun, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingWenkai Yang, Yankai Lin, Peng Li, Jie Zhou, and Xu Sun. 2021a. Rap: Robustness-aware perturba- tions for defending against backdoor attacks on nlp models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8365-8381.\n\nRethinking stealthiness of backdoor attack against nlp models. Wenkai Yang, Yankai Lin, Peng Li, Jie Zhou, Xu Sun, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingWenkai Yang, Yankai Lin, Peng Li, Jie Zhou, and Xu Sun. 2021b. Rethinking stealthiness of backdoor attack against nlp models. In Proceedings of the 59th Annual Meeting of the Association for Compu- tational Linguistics and the 11th International Joint Conference on Natural Language Processing (Vol- ume 1: Long Papers), pages 5543-5557.\n\nCharacter-level convolutional networks for text classification. Xiang Zhang, Junbo Zhao, Yann Lecun, Advances in neural information processing systems. 28Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text clas- sification. Advances in neural information process- ing systems, 28.\n\nHow to inject backdoors with better consistency: Logit anchoring on clean data. Zhiyuan Zhang, Lingjuan Lyu, Weiqiang Wang, Lichao Sun, Xu Sun, International Conference on Learning Representations. Zhiyuan Zhang, Lingjuan Lyu, Weiqiang Wang, Lichao Sun, and Xu Sun. 2021. How to inject backdoors with better consistency: Logit anchoring on clean data. In International Conference on Learning Rep- resentations.\n", "annotations": {"author": "[{\"end\":212,\"start\":54},{\"end\":370,\"start\":213},{\"end\":507,\"start\":371},{\"end\":672,\"start\":508},{\"end\":832,\"start\":673},{\"end\":966,\"start\":833},{\"end\":1123,\"start\":967},{\"end\":1273,\"start\":1124}]", "publisher": null, "author_last_name": "[{\"end\":64,\"start\":60},{\"end\":221,\"start\":219},{\"end\":381,\"start\":379},{\"end\":521,\"start\":516},{\"end\":685,\"start\":682},{\"end\":840,\"start\":838},{\"end\":978,\"start\":974},{\"end\":1130,\"start\":1128}]", "author_first_name": "[{\"end\":59,\"start\":54},{\"end\":218,\"start\":213},{\"end\":378,\"start\":371},{\"end\":515,\"start\":508},{\"end\":681,\"start\":673},{\"end\":837,\"start\":833},{\"end\":973,\"start\":967},{\"end\":1127,\"start\":1124}]", "author_affiliation": "[{\"end\":211,\"start\":88},{\"end\":369,\"start\":246},{\"end\":506,\"start\":383},{\"end\":671,\"start\":548},{\"end\":831,\"start\":708},{\"end\":965,\"start\":842},{\"end\":1122,\"start\":999},{\"end\":1272,\"start\":1149}]", "title": "[{\"end\":51,\"start\":1},{\"end\":1324,\"start\":1274}]", "venue": null, "abstract": "[{\"end\":2983,\"start\":1326}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3120,\"start\":3099},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3137,\"start\":3120},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":3157,\"start\":3137},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3240,\"start\":3220},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":3264,\"start\":3240},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3311,\"start\":3291},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":3365,\"start\":3343},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3457,\"start\":3439},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3477,\"start\":3457},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3493,\"start\":3477},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3553,\"start\":3530},{\"end\":3577,\"start\":3553},{\"end\":3598,\"start\":3577},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3703,\"start\":3680},{\"end\":3731,\"start\":3703},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3749,\"start\":3731},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3821,\"start\":3803},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3832,\"start\":3821},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3904,\"start\":3887},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4177,\"start\":4160},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":4765,\"start\":4742},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":4783,\"start\":4765},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5087,\"start\":5066},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5103,\"start\":5087},{\"end\":5168,\"start\":5148},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":5209,\"start\":5191},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":6293,\"start\":6269},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":6314,\"start\":6293},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6335,\"start\":6314},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8047,\"start\":8030},{\"end\":8066,\"start\":8047},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8088,\"start\":8066},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8110,\"start\":8088},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8128,\"start\":8110},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":8147,\"start\":8128},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8165,\"start\":8149},{\"end\":9005,\"start\":8987},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9027,\"start\":9009},{\"end\":9049,\"start\":9029},{\"end\":9137,\"start\":9119},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9244,\"start\":9226},{\"end\":9352,\"start\":9335},{\"end\":9696,\"start\":9676},{\"end\":10184,\"start\":10165},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":10203,\"start\":10186},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10225,\"start\":10205},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":10244,\"start\":10227},{\"end\":10468,\"start\":10449},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":10722,\"start\":10705},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10808,\"start\":10791},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":11747,\"start\":11731},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":11766,\"start\":11747},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":11784,\"start\":11768},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":12160,\"start\":12143},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12443,\"start\":12423},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":12462,\"start\":12443},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":12480,\"start\":12462},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12501,\"start\":12482},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":12678,\"start\":12659},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":12882,\"start\":12864},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":14124,\"start\":14100},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":14145,\"start\":14124},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":14166,\"start\":14145},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":14954,\"start\":14935},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":15000,\"start\":14978},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":15732,\"start\":15711},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":16076,\"start\":16057},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":19666,\"start\":19651},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":19685,\"start\":19666},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":20002,\"start\":19981},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":20099,\"start\":20077},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":20160,\"start\":20140},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":20422,\"start\":20404},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":20441,\"start\":20422},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":20457,\"start\":20441},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":21893,\"start\":21872},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":21991,\"start\":21973},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":22093,\"start\":22075},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":22333,\"start\":22316},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":22524,\"start\":22507},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":22764,\"start\":22747},{\"end\":23224,\"start\":23204},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":23244,\"start\":23224},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":23279,\"start\":23258},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":23518,\"start\":23500},{\"end\":23572,\"start\":23518},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":28130,\"start\":28108},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":28219,\"start\":28201},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":30444,\"start\":30427},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":30654,\"start\":30634}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":30669,\"start\":30411},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":30870,\"start\":30670},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":31530,\"start\":30871},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":31790,\"start\":31531},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":31852,\"start\":31791},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":32135,\"start\":31853},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":32374,\"start\":32136},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":32477,\"start\":32375},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":32606,\"start\":32478}]", "paragraph": "[{\"end\":3952,\"start\":2985},{\"end\":6038,\"start\":3954},{\"end\":6969,\"start\":6040},{\"end\":7605,\"start\":6971},{\"end\":7918,\"start\":7607},{\"end\":8964,\"start\":7935},{\"end\":10088,\"start\":8966},{\"end\":11231,\"start\":10090},{\"end\":11612,\"start\":11233},{\"end\":12992,\"start\":11614},{\"end\":13247,\"start\":13054},{\"end\":13621,\"start\":13288},{\"end\":13927,\"start\":13798},{\"end\":14432,\"start\":13966},{\"end\":14547,\"start\":14434},{\"end\":15087,\"start\":14612},{\"end\":15233,\"start\":15089},{\"end\":15757,\"start\":15252},{\"end\":16325,\"start\":15759},{\"end\":16645,\"start\":16391},{\"end\":16700,\"start\":16647},{\"end\":16844,\"start\":16763},{\"end\":17614,\"start\":16846},{\"end\":18002,\"start\":17663},{\"end\":18733,\"start\":18029},{\"end\":18957,\"start\":18768},{\"end\":19032,\"start\":18959},{\"end\":19269,\"start\":19034},{\"end\":19596,\"start\":19271},{\"end\":21377,\"start\":19632},{\"end\":21764,\"start\":21379},{\"end\":21950,\"start\":21766},{\"end\":22173,\"start\":21952},{\"end\":23050,\"start\":22175},{\"end\":23951,\"start\":23077},{\"end\":25625,\"start\":23953},{\"end\":26344,\"start\":25681},{\"end\":26557,\"start\":26346},{\"end\":27709,\"start\":26571},{\"end\":27867,\"start\":27738},{\"end\":28299,\"start\":27869},{\"end\":29714,\"start\":28301},{\"end\":29892,\"start\":29729},{\"end\":30410,\"start\":29923}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13053,\"start\":12993},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13287,\"start\":13248},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13797,\"start\":13622},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14611,\"start\":14548},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16390,\"start\":16326},{\"attributes\":{\"id\":\"formula_5\"},\"end\":16762,\"start\":16701},{\"attributes\":{\"id\":\"formula_6\"},\"end\":17662,\"start\":17615},{\"attributes\":{\"id\":\"formula_7\"},\"end\":18767,\"start\":18734}]", "table_ref": "[{\"end\":5417,\"start\":5409},{\"end\":5959,\"start\":5953},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":20217,\"start\":20210},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":24113,\"start\":24106},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":25724,\"start\":25717},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":27190,\"start\":27183},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":28936,\"start\":28929},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":29473,\"start\":29466}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":7933,\"start\":7921},{\"attributes\":{\"n\":\"2.2\"},\"end\":13964,\"start\":13930},{\"attributes\":{\"n\":\"2.3\"},\"end\":15250,\"start\":15236},{\"end\":18027,\"start\":18005},{\"attributes\":{\"n\":\"3\"},\"end\":19610,\"start\":19599},{\"attributes\":{\"n\":\"3.1\"},\"end\":19630,\"start\":19613},{\"attributes\":{\"n\":\"3.2\"},\"end\":23075,\"start\":23053},{\"attributes\":{\"n\":\"3.3\"},\"end\":25656,\"start\":25628},{\"attributes\":{\"n\":\"3.4\"},\"end\":25679,\"start\":25659},{\"end\":26569,\"start\":26560},{\"attributes\":{\"n\":\"3.5\"},\"end\":27736,\"start\":27712},{\"attributes\":{\"n\":\"3.6\"},\"end\":29727,\"start\":29717},{\"attributes\":{\"n\":\"4\"},\"end\":29921,\"start\":29895},{\"end\":30420,\"start\":30412},{\"end\":30680,\"start\":30671},{\"end\":30881,\"start\":30872},{\"end\":31541,\"start\":31532},{\"end\":31801,\"start\":31792},{\"end\":31863,\"start\":31854},{\"end\":32144,\"start\":32137},{\"end\":32385,\"start\":32376},{\"end\":32488,\"start\":32479}]", "table": "[{\"end\":31530,\"start\":30988},{\"end\":32135,\"start\":31865},{\"end\":32374,\"start\":32243}]", "figure_caption": "[{\"end\":30669,\"start\":30425},{\"end\":30870,\"start\":30682},{\"end\":30988,\"start\":30883},{\"end\":31790,\"start\":31543},{\"end\":31852,\"start\":31803},{\"end\":32243,\"start\":32146},{\"end\":32477,\"start\":32387},{\"end\":32606,\"start\":32490}]", "figure_ref": "[{\"end\":8268,\"start\":8260}]", "bib_author_first_name": "[{\"end\":32691,\"start\":32685},{\"end\":32705,\"start\":32700},{\"end\":33002,\"start\":32996},{\"end\":33028,\"start\":33019},{\"end\":33355,\"start\":33349},{\"end\":33367,\"start\":33361},{\"end\":33382,\"start\":33374},{\"end\":33392,\"start\":33389},{\"end\":33404,\"start\":33398},{\"end\":33421,\"start\":33415},{\"end\":33435,\"start\":33431},{\"end\":33451,\"start\":33446},{\"end\":33476,\"start\":33471},{\"end\":33488,\"start\":33483},{\"end\":33860,\"start\":33850},{\"end\":33873,\"start\":33867},{\"end\":34158,\"start\":34151},{\"end\":34171,\"start\":34165},{\"end\":34185,\"start\":34178},{\"end\":34199,\"start\":34191},{\"end\":34212,\"start\":34205},{\"end\":34225,\"start\":34220},{\"end\":34234,\"start\":34230},{\"end\":34569,\"start\":34565},{\"end\":34591,\"start\":34586},{\"end\":34618,\"start\":34603},{\"end\":35086,\"start\":35080},{\"end\":35098,\"start\":35093},{\"end\":35113,\"start\":35106},{\"end\":35127,\"start\":35120},{\"end\":35143,\"start\":35136},{\"end\":35154,\"start\":35148},{\"end\":35169,\"start\":35161},{\"end\":35178,\"start\":35174},{\"end\":35514,\"start\":35508},{\"end\":35526,\"start\":35521},{\"end\":35534,\"start\":35532},{\"end\":35547,\"start\":35539},{\"end\":35904,\"start\":35898},{\"end\":35919,\"start\":35915},{\"end\":35937,\"start\":35930},{\"end\":35950,\"start\":35944},{\"end\":36424,\"start\":36419},{\"end\":36441,\"start\":36433},{\"end\":36455,\"start\":36449},{\"end\":36469,\"start\":36461},{\"end\":37174,\"start\":37170},{\"end\":37188,\"start\":37181},{\"end\":37200,\"start\":37194},{\"end\":37211,\"start\":37207},{\"end\":37561,\"start\":37557},{\"end\":37573,\"start\":37567},{\"end\":37584,\"start\":37578},{\"end\":37598,\"start\":37591},{\"end\":37609,\"start\":37604},{\"end\":37617,\"start\":37614},{\"end\":37627,\"start\":37622},{\"end\":37639,\"start\":37632},{\"end\":37945,\"start\":37939},{\"end\":37956,\"start\":37951},{\"end\":37968,\"start\":37961},{\"end\":37982,\"start\":37976},{\"end\":37993,\"start\":37987},{\"end\":38003,\"start\":38000},{\"end\":38016,\"start\":38008},{\"end\":38026,\"start\":38022},{\"end\":38373,\"start\":38368},{\"end\":38390,\"start\":38383},{\"end\":38402,\"start\":38397},{\"end\":38418,\"start\":38413},{\"end\":38433,\"start\":38427},{\"end\":38697,\"start\":38696},{\"end\":38991,\"start\":38985},{\"end\":39003,\"start\":38996},{\"end\":39027,\"start\":39018},{\"end\":39381,\"start\":39373},{\"end\":39395,\"start\":39387},{\"end\":39406,\"start\":39401},{\"end\":39419,\"start\":39411},{\"end\":39432,\"start\":39425},{\"end\":39646,\"start\":39642},{\"end\":39661,\"start\":39653},{\"end\":39669,\"start\":39666},{\"end\":39947,\"start\":39943},{\"end\":39957,\"start\":39955},{\"end\":39966,\"start\":39962},{\"end\":39980,\"start\":39973},{\"end\":39992,\"start\":39986},{\"end\":40586,\"start\":40584},{\"end\":40599,\"start\":40592},{\"end\":40609,\"start\":40605},{\"end\":40616,\"start\":40610},{\"end\":40628,\"start\":40623},{\"end\":41130,\"start\":41126},{\"end\":41143,\"start\":41136},{\"end\":41156,\"start\":41151},{\"end\":41712,\"start\":41711},{\"end\":41728,\"start\":41723},{\"end\":41861,\"start\":41857},{\"end\":41874,\"start\":41869},{\"end\":41881,\"start\":41880},{\"end\":41895,\"start\":41888},{\"end\":41917,\"start\":41911},{\"end\":41932,\"start\":41925},{\"end\":41947,\"start\":41942},{\"end\":42303,\"start\":42300},{\"end\":42315,\"start\":42310},{\"end\":42602,\"start\":42597},{\"end\":42615,\"start\":42611},{\"end\":42630,\"start\":42624},{\"end\":43078,\"start\":43074},{\"end\":43194,\"start\":43189},{\"end\":43205,\"start\":43199},{\"end\":43218,\"start\":43212},{\"end\":43228,\"start\":43225},{\"end\":43544,\"start\":43537},{\"end\":43554,\"start\":43549},{\"end\":43568,\"start\":43561},{\"end\":43580,\"start\":43573},{\"end\":43594,\"start\":43587},{\"end\":43605,\"start\":43599},{\"end\":44046,\"start\":44038},{\"end\":44057,\"start\":44051},{\"end\":44078,\"start\":44063},{\"end\":44555,\"start\":44549},{\"end\":44566,\"start\":44560},{\"end\":44578,\"start\":44571},{\"end\":44591,\"start\":44583},{\"end\":45073,\"start\":45067},{\"end\":45084,\"start\":45078},{\"end\":45096,\"start\":45089},{\"end\":45109,\"start\":45101},{\"end\":45481,\"start\":45477},{\"end\":45493,\"start\":45488},{\"end\":45505,\"start\":45501},{\"end\":45526,\"start\":45520},{\"end\":45537,\"start\":45532},{\"end\":45933,\"start\":45929},{\"end\":45956,\"start\":45941},{\"end\":46235,\"start\":46232},{\"end\":46245,\"start\":46242},{\"end\":46509,\"start\":46506},{\"end\":46519,\"start\":46516},{\"end\":46524,\"start\":46520},{\"end\":46831,\"start\":46828},{\"end\":46843,\"start\":46838},{\"end\":46857,\"start\":46848},{\"end\":46869,\"start\":46863},{\"end\":47251,\"start\":47246},{\"end\":47266,\"start\":47259},{\"end\":47478,\"start\":47471},{\"end\":47489,\"start\":47483},{\"end\":47501,\"start\":47496},{\"end\":47510,\"start\":47506},{\"end\":47523,\"start\":47516},{\"end\":47536,\"start\":47529},{\"end\":47855,\"start\":47848},{\"end\":47866,\"start\":47860},{\"end\":47878,\"start\":47873},{\"end\":47891,\"start\":47886},{\"end\":47903,\"start\":47896},{\"end\":47916,\"start\":47909},{\"end\":48445,\"start\":48438},{\"end\":48455,\"start\":48450},{\"end\":48466,\"start\":48460},{\"end\":48481,\"start\":48473},{\"end\":48496,\"start\":48489},{\"end\":48509,\"start\":48502},{\"end\":48523,\"start\":48516},{\"end\":49321,\"start\":49314},{\"end\":49330,\"start\":49326},{\"end\":49342,\"start\":49336},{\"end\":49354,\"start\":49347},{\"end\":49367,\"start\":49360},{\"end\":50149,\"start\":50142},{\"end\":50161,\"start\":50154},{\"end\":50173,\"start\":50167},{\"end\":50185,\"start\":50179},{\"end\":50195,\"start\":50191},{\"end\":50205,\"start\":50202},{\"end\":50699,\"start\":50695},{\"end\":50716,\"start\":50709},{\"end\":50726,\"start\":50721},{\"end\":50739,\"start\":50734},{\"end\":50751,\"start\":50746},{\"end\":50764,\"start\":50760},{\"end\":51050,\"start\":51045},{\"end\":51063,\"start\":51059},{\"end\":51077,\"start\":51073},{\"end\":51096,\"start\":51087},{\"end\":51108,\"start\":51102},{\"end\":51124,\"start\":51117},{\"end\":51138,\"start\":51133},{\"end\":51148,\"start\":51145},{\"end\":51158,\"start\":51153},{\"end\":51160,\"start\":51159},{\"end\":51565,\"start\":51560},{\"end\":51580,\"start\":51573},{\"end\":51593,\"start\":51589},{\"end\":51800,\"start\":51796},{\"end\":52227,\"start\":52224},{\"end\":52236,\"start\":52234},{\"end\":52249,\"start\":52244},{\"end\":52259,\"start\":52256},{\"end\":52530,\"start\":52523},{\"end\":52543,\"start\":52539},{\"end\":52559,\"start\":52555},{\"end\":52569,\"start\":52564},{\"end\":52579,\"start\":52578},{\"end\":52603,\"start\":52602},{\"end\":52623,\"start\":52612},{\"end\":53160,\"start\":53156},{\"end\":53177,\"start\":53172},{\"end\":53193,\"start\":53187},{\"end\":53420,\"start\":53411},{\"end\":53438,\"start\":53430},{\"end\":53452,\"start\":53448},{\"end\":53468,\"start\":53464},{\"end\":53877,\"start\":53871},{\"end\":53891,\"start\":53887},{\"end\":53905,\"start\":53901},{\"end\":53919,\"start\":53914},{\"end\":53936,\"start\":53931},{\"end\":53949,\"start\":53944},{\"end\":53951,\"start\":53950},{\"end\":53965,\"start\":53959},{\"end\":53979,\"start\":53974},{\"end\":54334,\"start\":54331},{\"end\":54346,\"start\":54341},{\"end\":54360,\"start\":54351},{\"end\":54374,\"start\":54369},{\"end\":54392,\"start\":54386},{\"end\":54407,\"start\":54399},{\"end\":54426,\"start\":54420},{\"end\":54904,\"start\":54900},{\"end\":54913,\"start\":54912},{\"end\":54927,\"start\":54921},{\"end\":54938,\"start\":54936},{\"end\":54951,\"start\":54945},{\"end\":55412,\"start\":55406},{\"end\":55425,\"start\":55419},{\"end\":55435,\"start\":55431},{\"end\":55443,\"start\":55440},{\"end\":55452,\"start\":55450},{\"end\":55950,\"start\":55944},{\"end\":55963,\"start\":55957},{\"end\":55973,\"start\":55969},{\"end\":55981,\"start\":55978},{\"end\":55990,\"start\":55988},{\"end\":56715,\"start\":56710},{\"end\":56728,\"start\":56723},{\"end\":56739,\"start\":56735},{\"end\":57058,\"start\":57051},{\"end\":57074,\"start\":57066},{\"end\":57088,\"start\":57080},{\"end\":57101,\"start\":57095},{\"end\":57109,\"start\":57107}]", "bib_author_last_name": "[{\"end\":32698,\"start\":32692},{\"end\":32710,\"start\":32706},{\"end\":33017,\"start\":33003},{\"end\":33034,\"start\":33029},{\"end\":33359,\"start\":33356},{\"end\":33372,\"start\":33368},{\"end\":33387,\"start\":33383},{\"end\":33396,\"start\":33393},{\"end\":33413,\"start\":33405},{\"end\":33429,\"start\":33422},{\"end\":33444,\"start\":33436},{\"end\":33469,\"start\":33452},{\"end\":33481,\"start\":33477},{\"end\":33492,\"start\":33489},{\"end\":33865,\"start\":33861},{\"end\":33877,\"start\":33874},{\"end\":34163,\"start\":34159},{\"end\":34176,\"start\":34172},{\"end\":34189,\"start\":34186},{\"end\":34203,\"start\":34200},{\"end\":34218,\"start\":34213},{\"end\":34228,\"start\":34226},{\"end\":34238,\"start\":34235},{\"end\":34584,\"start\":34570},{\"end\":34601,\"start\":34592},{\"end\":34625,\"start\":34619},{\"end\":34631,\"start\":34627},{\"end\":35091,\"start\":35087},{\"end\":35104,\"start\":35099},{\"end\":35118,\"start\":35114},{\"end\":35134,\"start\":35128},{\"end\":35146,\"start\":35144},{\"end\":35159,\"start\":35155},{\"end\":35172,\"start\":35170},{\"end\":35184,\"start\":35179},{\"end\":35519,\"start\":35515},{\"end\":35530,\"start\":35527},{\"end\":35537,\"start\":35535},{\"end\":35550,\"start\":35548},{\"end\":35913,\"start\":35905},{\"end\":35928,\"start\":35920},{\"end\":35942,\"start\":35938},{\"end\":35956,\"start\":35951},{\"end\":36431,\"start\":36425},{\"end\":36447,\"start\":36442},{\"end\":36459,\"start\":36456},{\"end\":36479,\"start\":36470},{\"end\":37179,\"start\":37175},{\"end\":37192,\"start\":37189},{\"end\":37205,\"start\":37201},{\"end\":37214,\"start\":37212},{\"end\":37565,\"start\":37562},{\"end\":37576,\"start\":37574},{\"end\":37589,\"start\":37585},{\"end\":37602,\"start\":37599},{\"end\":37612,\"start\":37610},{\"end\":37620,\"start\":37618},{\"end\":37630,\"start\":37628},{\"end\":37645,\"start\":37640},{\"end\":37949,\"start\":37946},{\"end\":37959,\"start\":37957},{\"end\":37974,\"start\":37969},{\"end\":37985,\"start\":37983},{\"end\":37998,\"start\":37994},{\"end\":38006,\"start\":38004},{\"end\":38020,\"start\":38017},{\"end\":38030,\"start\":38027},{\"end\":38381,\"start\":38374},{\"end\":38395,\"start\":38391},{\"end\":38411,\"start\":38403},{\"end\":38425,\"start\":38419},{\"end\":38441,\"start\":38434},{\"end\":38701,\"start\":38698},{\"end\":38713,\"start\":38703},{\"end\":38994,\"start\":38992},{\"end\":39016,\"start\":39004},{\"end\":39032,\"start\":39028},{\"end\":39385,\"start\":39382},{\"end\":39399,\"start\":39396},{\"end\":39409,\"start\":39407},{\"end\":39423,\"start\":39420},{\"end\":39438,\"start\":39433},{\"end\":39651,\"start\":39647},{\"end\":39664,\"start\":39662},{\"end\":39675,\"start\":39670},{\"end\":39953,\"start\":39948},{\"end\":39960,\"start\":39958},{\"end\":39971,\"start\":39967},{\"end\":39984,\"start\":39981},{\"end\":39997,\"start\":39993},{\"end\":40590,\"start\":40587},{\"end\":40603,\"start\":40600},{\"end\":40621,\"start\":40617},{\"end\":40638,\"start\":40629},{\"end\":41134,\"start\":41131},{\"end\":41149,\"start\":41144},{\"end\":41161,\"start\":41157},{\"end\":41721,\"start\":41713},{\"end\":41735,\"start\":41729},{\"end\":41739,\"start\":41737},{\"end\":41867,\"start\":41862},{\"end\":41878,\"start\":41875},{\"end\":41886,\"start\":41882},{\"end\":41909,\"start\":41896},{\"end\":41923,\"start\":41918},{\"end\":41940,\"start\":41933},{\"end\":41956,\"start\":41948},{\"end\":41964,\"start\":41958},{\"end\":42308,\"start\":42304},{\"end\":42319,\"start\":42316},{\"end\":42326,\"start\":42321},{\"end\":42609,\"start\":42603},{\"end\":42622,\"start\":42616},{\"end\":42637,\"start\":42631},{\"end\":43083,\"start\":43079},{\"end\":43197,\"start\":43195},{\"end\":43210,\"start\":43206},{\"end\":43223,\"start\":43219},{\"end\":43237,\"start\":43229},{\"end\":43547,\"start\":43545},{\"end\":43559,\"start\":43555},{\"end\":43571,\"start\":43569},{\"end\":43585,\"start\":43581},{\"end\":43597,\"start\":43595},{\"end\":43609,\"start\":43606},{\"end\":44049,\"start\":44047},{\"end\":44061,\"start\":44058},{\"end\":44083,\"start\":44079},{\"end\":44558,\"start\":44556},{\"end\":44569,\"start\":44567},{\"end\":44581,\"start\":44579},{\"end\":44594,\"start\":44592},{\"end\":45076,\"start\":45074},{\"end\":45087,\"start\":45085},{\"end\":45099,\"start\":45097},{\"end\":45112,\"start\":45110},{\"end\":45486,\"start\":45482},{\"end\":45499,\"start\":45494},{\"end\":45518,\"start\":45506},{\"end\":45530,\"start\":45527},{\"end\":45544,\"start\":45538},{\"end\":45939,\"start\":45934},{\"end\":45974,\"start\":45957},{\"end\":45981,\"start\":45976},{\"end\":46240,\"start\":46236},{\"end\":46252,\"start\":46246},{\"end\":46258,\"start\":46254},{\"end\":46514,\"start\":46510},{\"end\":46531,\"start\":46525},{\"end\":46537,\"start\":46533},{\"end\":46836,\"start\":46832},{\"end\":46846,\"start\":46844},{\"end\":46861,\"start\":46858},{\"end\":46872,\"start\":46870},{\"end\":47257,\"start\":47252},{\"end\":47274,\"start\":47267},{\"end\":47481,\"start\":47479},{\"end\":47494,\"start\":47490},{\"end\":47504,\"start\":47502},{\"end\":47514,\"start\":47511},{\"end\":47527,\"start\":47524},{\"end\":47540,\"start\":47537},{\"end\":47858,\"start\":47856},{\"end\":47871,\"start\":47867},{\"end\":47884,\"start\":47879},{\"end\":47894,\"start\":47892},{\"end\":47907,\"start\":47904},{\"end\":47920,\"start\":47917},{\"end\":48448,\"start\":48446},{\"end\":48458,\"start\":48456},{\"end\":48471,\"start\":48467},{\"end\":48487,\"start\":48482},{\"end\":48500,\"start\":48497},{\"end\":48514,\"start\":48510},{\"end\":48527,\"start\":48524},{\"end\":49324,\"start\":49322},{\"end\":49334,\"start\":49331},{\"end\":49345,\"start\":49343},{\"end\":49358,\"start\":49355},{\"end\":49371,\"start\":49368},{\"end\":50152,\"start\":50150},{\"end\":50165,\"start\":50162},{\"end\":50177,\"start\":50174},{\"end\":50189,\"start\":50186},{\"end\":50200,\"start\":50196},{\"end\":50208,\"start\":50206},{\"end\":50707,\"start\":50700},{\"end\":50719,\"start\":50717},{\"end\":50732,\"start\":50727},{\"end\":50744,\"start\":50740},{\"end\":50758,\"start\":50752},{\"end\":50774,\"start\":50765},{\"end\":51057,\"start\":51051},{\"end\":51071,\"start\":51064},{\"end\":51085,\"start\":51078},{\"end\":51100,\"start\":51097},{\"end\":51115,\"start\":51109},{\"end\":51131,\"start\":51125},{\"end\":51143,\"start\":51139},{\"end\":51151,\"start\":51149},{\"end\":51164,\"start\":51161},{\"end\":51571,\"start\":51566},{\"end\":51587,\"start\":51581},{\"end\":51599,\"start\":51594},{\"end\":51807,\"start\":51801},{\"end\":52232,\"start\":52228},{\"end\":52242,\"start\":52237},{\"end\":52254,\"start\":52250},{\"end\":52263,\"start\":52260},{\"end\":52537,\"start\":52531},{\"end\":52553,\"start\":52544},{\"end\":52562,\"start\":52560},{\"end\":52576,\"start\":52570},{\"end\":52591,\"start\":52580},{\"end\":52600,\"start\":52593},{\"end\":52610,\"start\":52604},{\"end\":52626,\"start\":52624},{\"end\":52633,\"start\":52628},{\"end\":53170,\"start\":53161},{\"end\":53185,\"start\":53178},{\"end\":53196,\"start\":53194},{\"end\":53428,\"start\":53421},{\"end\":53446,\"start\":53439},{\"end\":53462,\"start\":53453},{\"end\":53474,\"start\":53469},{\"end\":53885,\"start\":53878},{\"end\":53899,\"start\":53892},{\"end\":53912,\"start\":53906},{\"end\":53929,\"start\":53920},{\"end\":53942,\"start\":53937},{\"end\":53957,\"start\":53952},{\"end\":53972,\"start\":53966},{\"end\":53990,\"start\":53980},{\"end\":54339,\"start\":54335},{\"end\":54349,\"start\":54347},{\"end\":54367,\"start\":54361},{\"end\":54384,\"start\":54375},{\"end\":54397,\"start\":54393},{\"end\":54418,\"start\":54408},{\"end\":54431,\"start\":54427},{\"end\":54910,\"start\":54905},{\"end\":54919,\"start\":54914},{\"end\":54934,\"start\":54928},{\"end\":54943,\"start\":54939},{\"end\":54954,\"start\":54952},{\"end\":54963,\"start\":54956},{\"end\":55417,\"start\":55413},{\"end\":55429,\"start\":55426},{\"end\":55438,\"start\":55436},{\"end\":55448,\"start\":55444},{\"end\":55456,\"start\":55453},{\"end\":55955,\"start\":55951},{\"end\":55967,\"start\":55964},{\"end\":55976,\"start\":55974},{\"end\":55986,\"start\":55982},{\"end\":55994,\"start\":55991},{\"end\":56721,\"start\":56716},{\"end\":56733,\"start\":56729},{\"end\":56745,\"start\":56740},{\"end\":57064,\"start\":57059},{\"end\":57078,\"start\":57075},{\"end\":57093,\"start\":57089},{\"end\":57105,\"start\":57102},{\"end\":57113,\"start\":57110}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":3536399},\"end\":32868,\"start\":32608},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":208527215},\"end\":33319,\"start\":32870},{\"attributes\":{\"doi\":\"arXiv:1803.11175\",\"id\":\"b2\"},\"end\":33742,\"start\":33321},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":220713444},\"end\":34072,\"start\":33744},{\"attributes\":{\"doi\":\"arXiv:2110.02467\",\"id\":\"b4\"},\"end\":34481,\"start\":34074},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":4891043},\"end\":34996,\"start\":34483},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":238354397},\"end\":35506,\"start\":34998},{\"attributes\":{\"doi\":\"arXiv:1712.05526\",\"id\":\"b7\"},\"end\":35825,\"start\":35508},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":1733167},\"end\":36335,\"start\":35827},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":52967399},\"end\":37108,\"start\":36337},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":244397177},\"end\":37555,\"start\":37110},{\"attributes\":{\"id\":\"b11\"},\"end\":37937,\"start\":37557},{\"attributes\":{\"doi\":\"arXiv:2111.07970\",\"id\":\"b12\"},\"end\":38321,\"start\":37939},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b13\",\"matched_paper_id\":3648736},\"end\":38694,\"start\":38323},{\"attributes\":{\"doi\":\"arXiv:1412.6572\",\"id\":\"b14\"},\"end\":38983,\"start\":38696},{\"attributes\":{\"id\":\"b15\"},\"end\":39310,\"start\":38985},{\"attributes\":{\"doi\":\"arXiv:2202.06862\",\"id\":\"b16\"},\"end\":39640,\"start\":39312},{\"attributes\":{\"doi\":\"arXiv:1611.01144\",\"id\":\"b17\"},\"end\":39890,\"start\":39642},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":7137476},\"end\":40474,\"start\":39892},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":202539059},\"end\":41022,\"start\":40476},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":9610402},\"end\":41665,\"start\":41024},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":6628106},\"end\":41855,\"start\":41667},{\"attributes\":{\"id\":\"b22\"},\"end\":42237,\"start\":41857},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b23\",\"matched_paper_id\":13193974},\"end\":42548,\"start\":42239},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":215754328},\"end\":43010,\"start\":42550},{\"attributes\":{\"id\":\"b25\"},\"end\":43187,\"start\":43012},{\"attributes\":{\"doi\":\"arXiv:1506.01066\",\"id\":\"b26\"},\"end\":43465,\"start\":43189},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":237365058},\"end\":44036,\"start\":43467},{\"attributes\":{\"id\":\"b28\"},\"end\":44460,\"start\":44038},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":237054216},\"end\":44978,\"start\":44462},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":237054216},\"end\":45475,\"start\":44980},{\"attributes\":{\"doi\":\"arXiv:1808.10307\",\"id\":\"b31\"},\"end\":45859,\"start\":45477},{\"attributes\":{\"id\":\"b32\"},\"end\":46193,\"start\":45861},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":223953609},\"end\":46452,\"start\":46195},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":231985654},\"end\":46744,\"start\":46454},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":236480506},\"end\":47167,\"start\":46746},{\"attributes\":{\"id\":\"b36\"},\"end\":47397,\"start\":47169},{\"attributes\":{\"doi\":\"arXiv:2011.10369\",\"id\":\"b37\"},\"end\":47759,\"start\":47399},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":238857078},\"end\":48362,\"start\":47761},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":235196099},\"end\":49227,\"start\":48364},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":235417102},\"end\":50064,\"start\":49229},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":244708886},\"end\":50640,\"start\":50066},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":160025533},\"end\":50961,\"start\":50642},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":204838007},\"end\":51480,\"start\":50963},{\"attributes\":{\"doi\":\"arXiv:2010.03282\",\"id\":\"b44\"},\"end\":51794,\"start\":51482},{\"attributes\":{\"doi\":\"arXiv:2006.11623\",\"id\":\"b45\"},\"end\":52164,\"start\":51796},{\"attributes\":{\"doi\":\"10.3390/app11219938\",\"id\":\"b46\",\"matched_paper_id\":240032812},\"end\":52442,\"start\":52166},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":990233},\"end\":53102,\"start\":52444},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":7961699},\"end\":53409,\"start\":53104},{\"attributes\":{\"doi\":\"arXiv:1312.6199\",\"id\":\"b49\"},\"end\":53791,\"start\":53411},{\"attributes\":{\"id\":\"b50\"},\"end\":54208,\"start\":53793},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":235795109},\"end\":54844,\"start\":54210},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":233219636},\"end\":55314,\"start\":54846},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":239009606},\"end\":55879,\"start\":55316},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":236459933},\"end\":56644,\"start\":55881},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":368182},\"end\":56969,\"start\":56646},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":237416749},\"end\":57381,\"start\":56971}]", "bib_title": "[{\"end\":32683,\"start\":32608},{\"end\":32994,\"start\":32870},{\"end\":33848,\"start\":33744},{\"end\":34563,\"start\":34483},{\"end\":35078,\"start\":34998},{\"end\":35896,\"start\":35827},{\"end\":36417,\"start\":36337},{\"end\":37168,\"start\":37110},{\"end\":38366,\"start\":38323},{\"end\":39941,\"start\":39892},{\"end\":40582,\"start\":40476},{\"end\":41124,\"start\":41024},{\"end\":41709,\"start\":41667},{\"end\":42298,\"start\":42239},{\"end\":42595,\"start\":42550},{\"end\":43535,\"start\":43467},{\"end\":44547,\"start\":44462},{\"end\":45065,\"start\":44980},{\"end\":45927,\"start\":45861},{\"end\":46230,\"start\":46195},{\"end\":46504,\"start\":46454},{\"end\":46826,\"start\":46746},{\"end\":47846,\"start\":47761},{\"end\":48436,\"start\":48364},{\"end\":49312,\"start\":49229},{\"end\":50140,\"start\":50066},{\"end\":50693,\"start\":50642},{\"end\":51043,\"start\":50963},{\"end\":52222,\"start\":52166},{\"end\":52521,\"start\":52444},{\"end\":53154,\"start\":53104},{\"end\":54329,\"start\":54210},{\"end\":54898,\"start\":54846},{\"end\":55404,\"start\":55316},{\"end\":55942,\"start\":55881},{\"end\":56708,\"start\":56646},{\"end\":57049,\"start\":56971}]", "bib_author": "[{\"end\":32700,\"start\":32685},{\"end\":32712,\"start\":32700},{\"end\":33019,\"start\":32996},{\"end\":33036,\"start\":33019},{\"end\":33361,\"start\":33349},{\"end\":33374,\"start\":33361},{\"end\":33389,\"start\":33374},{\"end\":33398,\"start\":33389},{\"end\":33415,\"start\":33398},{\"end\":33431,\"start\":33415},{\"end\":33446,\"start\":33431},{\"end\":33471,\"start\":33446},{\"end\":33483,\"start\":33471},{\"end\":33494,\"start\":33483},{\"end\":33867,\"start\":33850},{\"end\":33879,\"start\":33867},{\"end\":34165,\"start\":34151},{\"end\":34178,\"start\":34165},{\"end\":34191,\"start\":34178},{\"end\":34205,\"start\":34191},{\"end\":34220,\"start\":34205},{\"end\":34230,\"start\":34220},{\"end\":34240,\"start\":34230},{\"end\":34586,\"start\":34565},{\"end\":34603,\"start\":34586},{\"end\":34627,\"start\":34603},{\"end\":34633,\"start\":34627},{\"end\":35093,\"start\":35080},{\"end\":35106,\"start\":35093},{\"end\":35120,\"start\":35106},{\"end\":35136,\"start\":35120},{\"end\":35148,\"start\":35136},{\"end\":35161,\"start\":35148},{\"end\":35174,\"start\":35161},{\"end\":35186,\"start\":35174},{\"end\":35521,\"start\":35508},{\"end\":35532,\"start\":35521},{\"end\":35539,\"start\":35532},{\"end\":35552,\"start\":35539},{\"end\":35915,\"start\":35898},{\"end\":35930,\"start\":35915},{\"end\":35944,\"start\":35930},{\"end\":35958,\"start\":35944},{\"end\":36433,\"start\":36419},{\"end\":36449,\"start\":36433},{\"end\":36461,\"start\":36449},{\"end\":36481,\"start\":36461},{\"end\":37181,\"start\":37170},{\"end\":37194,\"start\":37181},{\"end\":37207,\"start\":37194},{\"end\":37216,\"start\":37207},{\"end\":37567,\"start\":37557},{\"end\":37578,\"start\":37567},{\"end\":37591,\"start\":37578},{\"end\":37604,\"start\":37591},{\"end\":37614,\"start\":37604},{\"end\":37622,\"start\":37614},{\"end\":37632,\"start\":37622},{\"end\":37647,\"start\":37632},{\"end\":37951,\"start\":37939},{\"end\":37961,\"start\":37951},{\"end\":37976,\"start\":37961},{\"end\":37987,\"start\":37976},{\"end\":38000,\"start\":37987},{\"end\":38008,\"start\":38000},{\"end\":38022,\"start\":38008},{\"end\":38032,\"start\":38022},{\"end\":38383,\"start\":38368},{\"end\":38397,\"start\":38383},{\"end\":38413,\"start\":38397},{\"end\":38427,\"start\":38413},{\"end\":38443,\"start\":38427},{\"end\":38703,\"start\":38696},{\"end\":38715,\"start\":38703},{\"end\":38996,\"start\":38985},{\"end\":39018,\"start\":38996},{\"end\":39034,\"start\":39018},{\"end\":39387,\"start\":39373},{\"end\":39401,\"start\":39387},{\"end\":39411,\"start\":39401},{\"end\":39425,\"start\":39411},{\"end\":39440,\"start\":39425},{\"end\":39653,\"start\":39642},{\"end\":39666,\"start\":39653},{\"end\":39677,\"start\":39666},{\"end\":39955,\"start\":39943},{\"end\":39962,\"start\":39955},{\"end\":39973,\"start\":39962},{\"end\":39986,\"start\":39973},{\"end\":39999,\"start\":39986},{\"end\":40592,\"start\":40584},{\"end\":40605,\"start\":40592},{\"end\":40623,\"start\":40605},{\"end\":40640,\"start\":40623},{\"end\":41136,\"start\":41126},{\"end\":41151,\"start\":41136},{\"end\":41163,\"start\":41151},{\"end\":41723,\"start\":41711},{\"end\":41737,\"start\":41723},{\"end\":41741,\"start\":41737},{\"end\":41869,\"start\":41857},{\"end\":41880,\"start\":41869},{\"end\":41888,\"start\":41880},{\"end\":41911,\"start\":41888},{\"end\":41925,\"start\":41911},{\"end\":41942,\"start\":41925},{\"end\":41958,\"start\":41942},{\"end\":41966,\"start\":41958},{\"end\":42310,\"start\":42300},{\"end\":42321,\"start\":42310},{\"end\":42328,\"start\":42321},{\"end\":42611,\"start\":42597},{\"end\":42624,\"start\":42611},{\"end\":42639,\"start\":42624},{\"end\":43085,\"start\":43074},{\"end\":43199,\"start\":43189},{\"end\":43212,\"start\":43199},{\"end\":43225,\"start\":43212},{\"end\":43239,\"start\":43225},{\"end\":43549,\"start\":43537},{\"end\":43561,\"start\":43549},{\"end\":43573,\"start\":43561},{\"end\":43587,\"start\":43573},{\"end\":43599,\"start\":43587},{\"end\":43611,\"start\":43599},{\"end\":44051,\"start\":44038},{\"end\":44063,\"start\":44051},{\"end\":44085,\"start\":44063},{\"end\":44560,\"start\":44549},{\"end\":44571,\"start\":44560},{\"end\":44583,\"start\":44571},{\"end\":44596,\"start\":44583},{\"end\":45078,\"start\":45067},{\"end\":45089,\"start\":45078},{\"end\":45101,\"start\":45089},{\"end\":45114,\"start\":45101},{\"end\":45488,\"start\":45477},{\"end\":45501,\"start\":45488},{\"end\":45520,\"start\":45501},{\"end\":45532,\"start\":45520},{\"end\":45546,\"start\":45532},{\"end\":45941,\"start\":45929},{\"end\":45976,\"start\":45941},{\"end\":45983,\"start\":45976},{\"end\":46242,\"start\":46232},{\"end\":46254,\"start\":46242},{\"end\":46260,\"start\":46254},{\"end\":46516,\"start\":46506},{\"end\":46533,\"start\":46516},{\"end\":46539,\"start\":46533},{\"end\":46838,\"start\":46828},{\"end\":46848,\"start\":46838},{\"end\":46863,\"start\":46848},{\"end\":46874,\"start\":46863},{\"end\":47259,\"start\":47246},{\"end\":47276,\"start\":47259},{\"end\":47483,\"start\":47471},{\"end\":47496,\"start\":47483},{\"end\":47506,\"start\":47496},{\"end\":47516,\"start\":47506},{\"end\":47529,\"start\":47516},{\"end\":47542,\"start\":47529},{\"end\":47860,\"start\":47848},{\"end\":47873,\"start\":47860},{\"end\":47886,\"start\":47873},{\"end\":47896,\"start\":47886},{\"end\":47909,\"start\":47896},{\"end\":47922,\"start\":47909},{\"end\":48450,\"start\":48438},{\"end\":48460,\"start\":48450},{\"end\":48473,\"start\":48460},{\"end\":48489,\"start\":48473},{\"end\":48502,\"start\":48489},{\"end\":48516,\"start\":48502},{\"end\":48529,\"start\":48516},{\"end\":49326,\"start\":49314},{\"end\":49336,\"start\":49326},{\"end\":49347,\"start\":49336},{\"end\":49360,\"start\":49347},{\"end\":49373,\"start\":49360},{\"end\":50154,\"start\":50142},{\"end\":50167,\"start\":50154},{\"end\":50179,\"start\":50167},{\"end\":50191,\"start\":50179},{\"end\":50202,\"start\":50191},{\"end\":50210,\"start\":50202},{\"end\":50709,\"start\":50695},{\"end\":50721,\"start\":50709},{\"end\":50734,\"start\":50721},{\"end\":50746,\"start\":50734},{\"end\":50760,\"start\":50746},{\"end\":50776,\"start\":50760},{\"end\":51059,\"start\":51045},{\"end\":51073,\"start\":51059},{\"end\":51087,\"start\":51073},{\"end\":51102,\"start\":51087},{\"end\":51117,\"start\":51102},{\"end\":51133,\"start\":51117},{\"end\":51145,\"start\":51133},{\"end\":51153,\"start\":51145},{\"end\":51166,\"start\":51153},{\"end\":51573,\"start\":51560},{\"end\":51589,\"start\":51573},{\"end\":51601,\"start\":51589},{\"end\":51809,\"start\":51796},{\"end\":52234,\"start\":52224},{\"end\":52244,\"start\":52234},{\"end\":52256,\"start\":52244},{\"end\":52265,\"start\":52256},{\"end\":52539,\"start\":52523},{\"end\":52555,\"start\":52539},{\"end\":52564,\"start\":52555},{\"end\":52578,\"start\":52564},{\"end\":52593,\"start\":52578},{\"end\":52602,\"start\":52593},{\"end\":52612,\"start\":52602},{\"end\":52628,\"start\":52612},{\"end\":52635,\"start\":52628},{\"end\":53172,\"start\":53156},{\"end\":53187,\"start\":53172},{\"end\":53198,\"start\":53187},{\"end\":53430,\"start\":53411},{\"end\":53448,\"start\":53430},{\"end\":53464,\"start\":53448},{\"end\":53476,\"start\":53464},{\"end\":53887,\"start\":53871},{\"end\":53901,\"start\":53887},{\"end\":53914,\"start\":53901},{\"end\":53931,\"start\":53914},{\"end\":53944,\"start\":53931},{\"end\":53959,\"start\":53944},{\"end\":53974,\"start\":53959},{\"end\":53992,\"start\":53974},{\"end\":54341,\"start\":54331},{\"end\":54351,\"start\":54341},{\"end\":54369,\"start\":54351},{\"end\":54386,\"start\":54369},{\"end\":54399,\"start\":54386},{\"end\":54420,\"start\":54399},{\"end\":54433,\"start\":54420},{\"end\":54912,\"start\":54900},{\"end\":54921,\"start\":54912},{\"end\":54936,\"start\":54921},{\"end\":54945,\"start\":54936},{\"end\":54956,\"start\":54945},{\"end\":54965,\"start\":54956},{\"end\":55419,\"start\":55406},{\"end\":55431,\"start\":55419},{\"end\":55440,\"start\":55431},{\"end\":55450,\"start\":55440},{\"end\":55458,\"start\":55450},{\"end\":55957,\"start\":55944},{\"end\":55969,\"start\":55957},{\"end\":55978,\"start\":55969},{\"end\":55988,\"start\":55978},{\"end\":55996,\"start\":55988},{\"end\":56723,\"start\":56710},{\"end\":56735,\"start\":56723},{\"end\":56747,\"start\":56735},{\"end\":57066,\"start\":57051},{\"end\":57080,\"start\":57066},{\"end\":57095,\"start\":57080},{\"end\":57107,\"start\":57095},{\"end\":57115,\"start\":57107}]", "bib_venue": "[{\"end\":32723,\"start\":32712},{\"end\":33074,\"start\":33036},{\"end\":33347,\"start\":33321},{\"end\":33893,\"start\":33879},{\"end\":34149,\"start\":34074},{\"end\":34715,\"start\":34633},{\"end\":35234,\"start\":35186},{\"end\":35639,\"start\":35568},{\"end\":36030,\"start\":35958},{\"end\":36623,\"start\":36481},{\"end\":37287,\"start\":37216},{\"end\":37728,\"start\":37647},{\"end\":38107,\"start\":38048},{\"end\":38491,\"start\":38447},{\"end\":38822,\"start\":38730},{\"end\":39129,\"start\":39034},{\"end\":39371,\"start\":39312},{\"end\":39743,\"start\":39693},{\"end\":40115,\"start\":39999},{\"end\":40701,\"start\":40640},{\"end\":41252,\"start\":41163},{\"end\":41754,\"start\":41741},{\"end\":42037,\"start\":41966},{\"end\":42376,\"start\":42332},{\"end\":42726,\"start\":42639},{\"end\":43072,\"start\":43012},{\"end\":43305,\"start\":43255},{\"end\":43697,\"start\":43611},{\"end\":44209,\"start\":44085},{\"end\":44674,\"start\":44596},{\"end\":45185,\"start\":45114},{\"end\":45646,\"start\":45562},{\"end\":46011,\"start\":45983},{\"end\":46309,\"start\":46260},{\"end\":46591,\"start\":46539},{\"end\":46935,\"start\":46874},{\"end\":47244,\"start\":47169},{\"end\":47469,\"start\":47399},{\"end\":48008,\"start\":47922},{\"end\":48691,\"start\":48529},{\"end\":49535,\"start\":49373},{\"end\":50298,\"start\":50210},{\"end\":50787,\"start\":50776},{\"end\":51202,\"start\":51166},{\"end\":51558,\"start\":51482},{\"end\":51961,\"start\":51825},{\"end\":52300,\"start\":52284},{\"end\":52721,\"start\":52635},{\"end\":53247,\"start\":53198},{\"end\":53584,\"start\":53491},{\"end\":53869,\"start\":53793},{\"end\":54507,\"start\":54433},{\"end\":55036,\"start\":54965},{\"end\":55544,\"start\":55458},{\"end\":56158,\"start\":55996},{\"end\":56796,\"start\":56747},{\"end\":57167,\"start\":57115},{\"end\":36089,\"start\":36032},{\"end\":36752,\"start\":36625},{\"end\":37345,\"start\":37289},{\"end\":40218,\"start\":40117},{\"end\":40749,\"start\":40703},{\"end\":41346,\"start\":41254},{\"end\":42800,\"start\":42728},{\"end\":43770,\"start\":43699},{\"end\":44739,\"start\":44676},{\"end\":45243,\"start\":45187},{\"end\":48081,\"start\":48010},{\"end\":48840,\"start\":48693},{\"end\":49684,\"start\":49537},{\"end\":50373,\"start\":50300},{\"end\":52794,\"start\":52723},{\"end\":55094,\"start\":55038},{\"end\":55617,\"start\":55546},{\"end\":56307,\"start\":56160}]"}}}, "year": 2023, "month": 12, "day": 17}