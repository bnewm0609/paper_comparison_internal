{"id": 257206032, "updated": "2023-11-06 14:26:16.616", "metadata": {"title": "Coded Matrix Computations for D2D-enabled Linearized Federated Learning", "authors": "[{\"first\":\"Anindya\",\"last\":\"Das\",\"middle\":[\"Bijoy\"]},{\"first\":\"Aditya\",\"last\":\"Ramamoorthy\",\"middle\":[]},{\"first\":\"David\",\"last\":\"Love\",\"middle\":[\"J.\"]},{\"first\":\"Christopher\",\"last\":\"Brinton\",\"middle\":[\"G.\"]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Federated learning (FL) is a popular technique for training a global model on data distributed across client devices. Like other distributed training techniques, FL is susceptible to straggler (slower or failed) clients. Recent work has proposed to address this through device-to-device (D2D) offloading, which introduces privacy concerns. In this paper, we propose a novel straggler-optimal approach for coded matrix computations which can significantly reduce the communication delay and privacy issues introduced from D2D data transmissions in FL. Moreover, our proposed approach leads to a considerable improvement of the local computation speed when the generated data matrix is sparse. Numerical evaluations confirm the superiority of our proposed method over baseline approaches.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "2302.12305", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icassp/DasRLB23", "doi": "10.1109/icassp49357.2023.10095450"}}, "content": {"source": {"pdf_hash": "0fc300d0fd54a97bfdf13953234d67d034fc6992", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2302.12305v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "d7a7f7878a48832a9d4a837876990f579d4accdd", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/0fc300d0fd54a97bfdf13953234d67d034fc6992.txt", "contents": "\nCODED MATRIX COMPUTATIONS FOR D2D-ENABLED LINEARIZED FEDERATED LEARNING\n23 Feb 2023\n\nAnindya Bijoy Das \nAditya Ramamoorthy \nDepartment of Electrical and Computer Engineering\nIowa State University\n50010AmesIAUSA\n\nDavid J Love \nChristopher G Brinton \n\nSchool of Electrical and Computer Engineering\nPurdue University\n47907West LafayetteINUSA\n\nCODED MATRIX COMPUTATIONS FOR D2D-ENABLED LINEARIZED FEDERATED LEARNING\n23 Feb 2023Index Terms-Distributed ComputingFederated Learn- ingStragglersHeterogeneous Edge ComputingPrivacy\nFederated learning (FL) is a popular technique for training a global model on data distributed across client devices. Like other distributed training techniques, FL is susceptible to straggler (slower or failed) clients. Recent work has proposed to address this through device-to-device (D2D) offloading, which introduces privacy concerns. In this paper, we propose a novel straggler-optimal approach for coded matrix computations which can significantly reduce the communication delay and privacy issues introduced from D2D data transmissions in FL. Moreover, our proposed approach leads to a considerable improvement of the local computation speed when the generated data matrix is sparse. Numerical evaluations confirm the superiority of our proposed method over baseline approaches.\n\nINTRODUCTION\n\nContemporary computing platforms are hard-pressed to support the growing demands for AI/ML model training at the network edge. While advances in hardware serve as part of the solution, the increasing complexity of data tasks and volumes of data will continue impeding scalability. In this regard, federated learning (FL) has become a popular technique for training machine learning models in a distributed manner [1][2][3]. In FL, the edge devices carry out the local computations, and the server collects, aggregates and updates the global model.\n\nRecent approaches have looked at linearizing the training operations in FL [1,4]. This is advantageous as it opens the possibility for coded matrix computing techniques that can improve operating efficiency. Specifically, in distributed settings like FL, the overall job execution time is often dominated by slower (or failed) worker nodes, which are referred to as stragglers. Recently, a number of coding theory techniques [5][6][7][8][9][10][11][12][13][14] have been proposed to mitigate stragglers in distributed matrix multiplications. A toy example [5] of such a technique for computing A T x across three clients is to partition A as A = [A 0 | A 1 ], and to assign them the job of computing A T 0 x, A T 1 x and (A 0 + A 1 ) T x, respectively. In a linearized FL setting, A \u2208 R t\u00d7r is the data matrix and x \u2208 R t is the model parameter vector. While each client has half of the total computational load, the server can recover A T x if any two clients return their results, i.e., the system is resilient to one straggler. If each of n clients computes 1/k A fraction of the whole job of computing A T x, the number of stragglers that the system can be resilient to is upper bounded by n \u2212 k A [7]. In contemporary edge computing systems, task offloading via device-to-device (D2D) communications has also been proposed for straggler mitigation. D2D-enabled FL has recently been studied [2,15,16], but can add considerable communication overhead as well as compromise data privacy. In this work, we exploit matrix coding in linearized FL to mitigate these challenges. Our straggler-optimal matrix computation scheme reduces the communication delay significantly compared to the techniques in [7,9,12]. Moreover, unlike [7,9,12,13,17], our scheme allows a client to access a limited fraction of matrix A, and provides a considerable protection against information leakage. In addition, our scheme is specifically suited to sparse matrices with a significant gain in computation speed.\n\n\nNETWORK AND LEARNING ARCHITECTURE\n\nWe consider a D2D-enabled FL architecture consisting of n = k A + s clients, denoted as W i for i = 0, 1, . . . , n \u2212 1. The first k A of them are active clients (responsible for both data generation and local computation) and the next s < k A are passive clients (responsible for local computation only).\n\nAssume that the i-th device has local data (D i , y i ), where D i and y i are the block-rows of full system dataset (D, y). Under a linear regression-based ML model, the global loss function is quadratic, i.e., f (\u03b2 \u2113 ) = ||D\u03b2 \u2113 \u2212 y|| 2 , where the model parameter after iteration \u2113 is obtained through gradient methods as \u03b2 \u2113 = \u03b2 \u2113\u22121 \u2212 \u00b5 \u2113 \u2207 \u03b2 f (\u03b2 \u2113\u22121 ) and \u00b5 \u2113 is the stepsize. Based on the form of \u2207 \u03b2 f (\u03b2 \u2113 ), the FL local model update at each device includes multiplying the local data matrix D i with parameter \u03b2 \u2113 . For this reason, recent work has also investigated linearizing non-linear models for FL by leveraging kernel embedding techniques [1]. Thus, our aim is to compute A T x -an arbitrary matrix operation during FL training -in a distributed fashion such that the system is re-Algorithm 1: Proposed scheme for distributed matrix-vector multiplication Input :Matrix A i generated in active client i for i = 0, 1, . . . , k A \u2212 1, vector x, total n clients including s < k A passive clients. 1 Set weight \u03c9 A = s + 1 ; 2 Denote client i as W i , for i = 0, 1, . . . , n \u2212 1;\n3 for i \u2190 0 to k A \u2212 1 do 4 Define T i = {i + 1, . . . , i + \u03c9 A \u2212 1} (mod k A ); 5 Send A j , where j \u2208 T i , from W j to W i ; 6\nClient W i creates a random vector r of length k A , computes\u00c3 i = q\u2208Ti r q A q and\u00c3 T i x; 7 end 8 for i \u2190 0 to s \u2212 1 do 9 W i creates random vectorr of size k A , computes A kA+i = q\u2208Tir q A q and sends to W kA+i ; 10 Client W kA+i computes\u00c3 T kA+i x; 11 end Output :The server recovers A T x from the returned results by the fastest k A clients.\n\nsilient to s stragglers. Our assumption is that any active client W i generates a block-column of matrix A, denoted as A i ,\ni = 0, 1, . . . , k A \u2212 1, such that A = A 0 A 1 . . . A kA\u22121 .(1)\nIn our approach, every client is responsible to compute the product of a coded submatrix (linear combinations of some block-columns of A) and the vector x. Stragglers will arise in practice from computing speed variations or failures experienced by the clients at particular times [8,17,18]. Now, similar to [15,16,19], we assume that there is a set of trusted neighbor clients for every device to transmit its data via D2D communications. The passive clients receive coded submatrices only from active clients. Unlike the approaches in [1,3,4,20], we assume that the server cannot access to any uncoded/coded local data generated in the edge devices and is only responsible for transmission of vector x and for decoding A T x once the fastest clients return the computed submatrix-vector products.\n\n\nHOMOGENEOUS EDGE COMPUTING\n\nHere we assume that each active client generates equal number of columns of A (i.e. all A i 's have the same size in (1)) and all the clients are rated with the same computation speed. In this scenario, we propose a distributed matrix-vector multiplication scheme in Alg. 1 which is resilient to any s stragglers.\n\nThe main idea is that any active client W j generates A j , for 0 \u2264 j \u2264 k A \u2212 1 and sends it to another active client W i , if j = i + 1, i + 2, . . . , i + \u03c9 A \u2212 1 ( modulo k A ). Here we set \u03c9 A = s + 1, thus, any data matrix A j needs to be sent to only \u03c9 A \u2212 1 = s other clients. Then, active client W j computes a linear combination of A i , A i+1 , . . . , A i+\u03c9A\u22121 (indices modulo k A ) where the coefficients are chosen randomly from a continuous distribution. Next, active client W i sends another random linear combination of the same submatrices to W i+kA (a passive client), when i = 0, 1, . . . , s \u2212 1. Note that all n clients receive the vector x from the server. Now the job of each client is to compute the product of their respective coded submatrix and the vector x. Once the fastest k A clients finish and send their computation results to the server, it decodes A T x using the corresponding random coefficients. The following theorem establishes the resiliency of Alg. 1 to stragglers.\n\nTheorem 1. Assume that a system has n clients including k A active and s passive clients. If we assign the jobs according to Alg. 1, we achieve resilience to any s = n \u2212 k A stragglers.\n\nProof. In order to recover A T x, according to (1), we need to decode all k A vector unknowns,\nA T 0 x, A T 1 x, . . . , A T kA\u22121 x;\nwe denote the set of these unknowns as B. Now we choose an arbitrary set of k A clients each of which corresponds to an equation in terms of \u03c9 A of those k A unknowns. Denoting the set of k A equations as C, we have |B| = |C| = k A . Now we consider a bipartite graph G = C \u222a B, where any vertex (equation) in C is connected to some vertices (unknowns) in B which have participated in the corresponding equation. Thus, each vertex in C has a neighborhood of cardinality \u03c9 A in B. Our goal is to show that there exists a perfect matching among the vertices of C and B. We argue this according to Hall's marriage theorem [21] for which we need to show that for anyC \u2286 C, the cardinality of the neighbourhood ofC, denoted as N (C) \u2286 B, is at least as large as |C|. Thus, for |C| = m \u2264 k A , we need to show that |N (C)| \u2265 m.\n\nCase 1: First we consider the case that m \u2264 2s. We assume that m = 2p, 2p \u2212 1 where 1 \u2264 p \u2264 s. Now according to Alg. 1, the participating unknowns are shifted in a cyclic manner among the equations. If we choose any \u03b4 clients out of the first k A clients (W 0 , W 1 , W 2 , . . . , W kA\u22121 ), according to the proof of cyclic scheme in Appendix C in [8], the minimum number of total participating unknowns is\nmin(\u03c9 A + \u03b4 \u2212 1, k A ),\nwhere \u03c9 A = s + 1. Now according to Alg. 1, same unknowns participate in two different equations corresponding to two different clients, W j and W kA+j , where\nj = 0, 1, . . . , s\u2212 1. Thus, for any |C| = m = 2p, 2p\u2212 1 \u2264 2s, we have |N (C)| \u2265 min (\u03c9 A + \u2308m/2\u2309 \u2212 1, k A ) = min (\u03c9 A + p \u2212 1, k A ) = min (s + p, k A ) \u2265 m. Case 2:\nNow we consider the case where m = 2s + q, 1 \u2264 q \u2264 k A \u2212 2s. We need to find the minimum number of unknowns which participate in any set of m equations. Now, the same unknowns participate in two different equations corresponding to two different clients, W j and W kA+j , where j = 0, 1, . . . , s \u2212 1. Thus, the additional q equations correspond to at least q additional unknowns until the total number of participating unknowns is k A . Therefore, in this case\nW 0 W 1 W 2 W 8 W 9 A 0 A 1 A 2 A 8 A 9\n. . . . . . . . .   Now we consider the largest matching where vertex c i \u2208 C is matched to vertex b j \u2208 B, which indicates that b j participates in the equation corresponding to c i . Let us consider a k A \u00d7 k A system matrix where row i corresponds to the equation associated to c i . Now we replace this row i by e j which is a unit row-vector of length k A with j-th entry being 1, and 0 otherwise. Thus we have a k A \u00d7 k A matrix where each row has only one non-zero entry which is 1. Since we have a perfect matching, this k A \u00d7 k A matrix has only one non-zero entry in every column. This is a permutation of the identity matrix, and thus, is full rank. Since the matrix is full rank for a choice of definite values, according to Schwartz-Zippel lemma [22], it will be full rank for random choices of non-zero entries. Thus, the server can recover all k A unknowns from any k A clients, hence the system is resilient to any s = n \u2212 k A stragglers. Example 1. Consider a homogeneous system of k A = 10 active clients and s = 2 passive clients. According to Alg. 1, \u03c9 A = s+1 = 3, and client W i (0 \u2264 i \u2264 11) has a random linear combination of A i , A i+1 and A i+2 (indices modulo 10) as shown in Fig. 1. Thus, according to Theorem 1, this system is resilient to s = 2 stragglers. Note that our scheme requires any active client to send its local data matrix to only up to s + 1 = 3 other clients, thus involves a significantly lower communication cost in comparison to the approaches in [7,9]. Remark 1. In comparison to [7,9,13], our proposed approach is specifically suited to sparse data matrices, i.e., most of the entries of A are zero. The approaches in [7,9,13] assign dense linear combinations of the submatrices which can destroy the inherent sparsity of A, leading to slower computation speed for the clients. On the other hand, our approach assigns linear combinations of limited number of submatrices which preserve the sparsity up to certain level that leads to faster computation.\n|N (C)| \u2265 min (\u03c9 A + \u23082s/2\u2309 + q \u2212 1, k A ) = min (\u03c9 A + s + q \u2212 1, k A ) = min (2s + q, k A ) \u2265 m.\n\nHETEROGENEOUS EDGE COMPUTING\n\nIn this section, we extend our approach in Alg. 1 to heterogenous system where the clients may have different data generation capability and different computation speeds. We assume that we have \u03bb different types of devices in the system, with client type j = 0, 1, . . . , \u03bb \u2212 1. Moreover, we assume that any active client W i generates \u03b1 i = c ij \u03b1 columns of data matrix A and any client W i has a computation speed \u03b2 i = c ij \u03b2, where W i is of client type j and c ij \u2265 1 is an integer. Thus, a higher c ij indicates a \"stronger\" type client W i which can process at a c ij times higher computation speed than the \"weakest\" type device, where \u03b1 is the number of the assigned columns and \u03b2 is the number of processed columns per unit time in the \"weakest\" type device. Note that \u03bb = 1 and all c ij = 1 lead us to the homogeneous system discussed in Sec. 3 where 0 \u2264 i \u2264 n \u2212 1 and j = 0. Now, we have n = k A + s clients including k A active and s passive clients in the heterogeneous system. Aligned to the homogeneous system, we assume that the number of passive clients of any type j is less than the number of active clients of the same type. Next, without loss of generality, we sort the indices of active clients in such a way so that,\nc ij \u2265 c kj if i \u2264 k, for 0 \u2264 i, k \u2264 k A \u2212 1.\nWe do the similar sorting for the passive clients too so that c ij \u2265 c kj if i \u2264 k, for k A \u2264 i, k \u2264 n \u2212 1. Now if a client W i is of client type j, it requires the same time to process c ij \u2265 1 block-columns (each consisting of \u03b1 columns) of A as the \"weakest\" device to process c ij = 1 such block-column. Moreover, if it is an active client, it also generates \u03b1 i = c ij \u03b1 columns of data matrix A. Thus, client W i can be thought as a collection of c ij homogeneous clients of \"weakest\" types where each of the active \"weakest\" clients generates equally \u03b1 columns of A and each of the \"weakest\" clients processes equally \u03b1 columns.\n\nTheorem 2. (a) A heterogeneous system of k A active and s passive clients of different types can be considered as a homogeneous system ofk A = kA\u22121 i=0 c ij active ands = n\u22121 i=kA c ij passive clients of the \"weakest\" type. Next (b) if the jobs are assigned according to Alg. 1 in the modified homogeneous system ofn =k A +s \"weakest\" clients, the system can be resilient tos such clients.\n\nProof. Each A k (generated in W k ) in (1) is a block-column consisting of c kj \u03b1 columns of A when client W k is of client type j. Thus, for any k = 0, 1, . . . , k A \u2212 1, we can partition A k as A k = \u0100 m\u0100m+1 . . .\u0100 m+c kj \u22121 , where m = k\u22121 i=0 c ij and each\u0100 \u2113 is a block-column consisting of \u03b1 columns of A, m \u2264 \u2113 \u2264 m + c kj \u2212 1. Thus using (1), we can write  k A = kA\u22121 i=0 c ij . Now from the matrix generation perspective, k A active clients in a heterogeneous system generatin\u1e21 k A block-columns can be considered as the same ask A active clients in a homogeneous system generating one blockcolumn each.\nA = A 0 A 1 . . . Ak A\u22121 , where W 2 W 1 W 0 W 3 W 4 A 4\u01005\u01006 A 2 A 3 A 0 A 1 (\nSimilarly, any client W i of type j can process c ij \u03b1 columns in the same time when the \"weakest\" type device can process \u03b1 columns. Thus, from the computation speed perspective, k A active clients and s passive clients in the heterogeneous system can be thought ask A = kA\u22121 i=0 c ij active clients ands = n\u22121 i=kA c ij passive clients, respectively, in a homogeneous system by assigning \u03b1 coded block-columns to each client. Hence, we are done with the proof of part (a). Moreover, part (b) of the proof is straight-forward from Theorem 1 when we havek A active ands passive clients.\n\nRemark 2. The heterogeneous system is resilient tos blockcolumn processing. The number of straggler clients that the system is resilient to can vary depending on the client types.\n\nExample 2. Consider the example in Fig. 2 consisting of n = 7 clients. There are k A = 5 active clients which are responsible for data matrix generation. Let us assume, W 0 and W 1 are of type 1 clients which generate twice as many columns of A than W 2 , W 3 and W 4 which are of type 0 clients. The jobs are assigned to all clients (including s = 2 passive clients) according to Fig. 2(b). It can be verified that this scheme is resilient to two type 0 clients or one type 1 client.\n\n\nNUMERICAL EVALUATION\n\nIn this section, we compare the performance of our proposed approach against different competing methods [7,9,13] in terms of different metrics for distributed matrix computations from the federated learning aspect. Note that the approaches in [1,4] require the edge devices to transmit some coded columns of matrix A to the server which is not aligned with our assumptions. In addition, the approaches in [8] and [11] do not follow the same network learning architecture as ours. Therefore, we did not include them in our comparison.\n\nCommunication Delay: We consider a homogeneous system of n = 20 clients each of which is a t2.small machine in AWS (Amazon Web Services) Cluster. Here, each of k A = 18 active clients generates A i of size 12000 \u00d7 1000, thus the size of A is 12000 \u00d7 18000. The server sends the parameter vector x of length 12000 to all 20 clients including s = 2 passive clients. Once the preprocessing and computations are carried out according to Alg. 1, the server recovers A T x as soon as it receives results from the fastest k A = 18 clients, thus the system is resilient to any s = 2 stragglers. Table 1 shows the comparison of the corresponding communication delays (caused by data matrix transmission) among different approaches. The approaches in [7,9] require all active clients to transmit their generated submatrices to all other edge devices. Thus, they lead to much more communication delay than our proposed method which needs an edge device to transmit data to only up to s + 1 = 3 other devices. Note that the methods in [13,17] involve similar amounts of communication delay as ours, however, they have other limitations in terms of privacy and computation time as discussed next.\n\nPrivacy: Information leakage is introduced in FL when we consider the transmission of local data matrices to other edge devices. To protect against privacy leakage, any particular client should have access to a limited portion of the whole data matrix. Consider the heterogeneous system in example 2 where the clients are honest but curious. In this scenario, the approaches in [7,9,13,17] would allow clients to access the whole matrix A. In our approach, as shown in Fig. 2, clients W 0 and W 1 only have access to 4/7-th fraction of A and clients W 2 , W 3 and W 4 have access to 3/7-th fraction of A. This provides significant protection against privacy leakage.\n\nProduct Computation Time for Sparse Matrices: Consider a system with n = 30 clients where k A = 28 and s = 2. We assume that A is sparse, where each active client generates a sparse submatrix of size 40000 \u00d7 1125. We consider three different scenarios with three different sparsity levels for A where randomly chosen 95%, 98% and 99% entries of A are zero. Now we compare our proposed Alg. 1 against different methods in terms of per client product computation time (the required time for a client to compute its assigned submatrix-vector product) in Table 2. The methods in [7,9,13,17] assign linear combinations of k A = 28 submatrices to the clients. Hence, the inherent sparsity of A is destroyed in the encoded submatrices. On the other hand, our approach combines only s + 1 = 3 submatrices to obtain the coded submatrices. Thus, the clients require a significantly less amount of time to finish the respective tasks in comparison to [7,9,13,17].\n\n.\n. . . . . . . . {A0, A1, A2} {A1, A2, A3} (b): Coded submatrix allocation among all the clients.\n\nFig. 1 :\n1(a) Data generation and (b) submatrix allocation for n = 12 clients according to Alg. 1 including kA = 10 active and s = 2 passive clients. Any {Aj, A k , A \u2113 } indicates a random linear combination of the corresponding submatrices. Any Wi obtains a random linear combination of Ai, Ai+1 and Ai+2 (indices reduced mod 10).\n\n\nThus, for any m \u2264 k A (where |C| = m), we have shown that |N (C)| \u2265 |C|. So, there exists a perfect matching among the vertices of C and B according to Hall's marriage theorem.\n\nFig. 2 :\n2A heterogeneous system of n = 7 clients where kA = 5 and s = 2. (a) Each of W0 and W1 generates 2\u03b1 columns and each of W2, W3 and W4 generates \u03b1 columns of A \u2208 R t\u00d7r , where \u03b1 = r/7. (b) Once the jobs are assigned, the system is resilient to stragglers.\n\n\na): Data generation among the active clients.W 2 \nW 1 \nW 0 \nW 3 \n\nW 4 \nW 5 \nW 6 \n\n{\u01004,\u01005,\u01006} \n{\u01005,\u01006,\u01000} \n\n{\u01006,\u01000,\u01001} \n\n{\u01002,\u01003,\u01004} \n\n{\u01003,\u01004,\u01005} \n\n{\u01000,\u01001,\u01002} \n\n{\u01001,\u01002,\u01003} \n\n{\u01000,\u01001,\u01002} \n{\u01001,\u01002,\u01003} \n\n(b): Coded submatrix allocation among all the clients. \n\n\n\nTable 1 :\n1Comparison among different approaches in terms of communication delay for a system with n = 20, kA = 18 and s = 2.POLY \nORTHO-\nRKRP \nCONV. \nPROP. \nCODE [7] POLY [9] CODE [13] CODE [17] \nSCH. \n\n14.13 s \n14.02 s \n2.49 s \n2.56 s \n2.21 s \n\n\n\nTable 2 :\n2Per client product computation time where n = 30, kA = 28, s = 2 and \u03b6 = 95%, 98% or 99% entries of A are zero.METHODSPRODUCT COMP. TIME (IN MS) \u03b6 = 99% \u03b6 = 98% \u03b6 = 95%POLY CODE [7] \n54.7 \n55.2 \n53.7 \nORTHO-POLY [9] \n54.3 \n54.8 \n55.2 \nRKRP CODE [13] \n55.1 \n53.4 \n53.7 \nCONV. CODE [17] \n56.2 \n55.8 \n56.8 \nPROP. SCHEME \n14.9 \n21.1 \n29.6 \n\n\n\nCoded computing for low-latency federated learning over wireless edge networks. Saurav Prakash, Sagar Dhakal, Mustafa Riza Akdeniz, Yair Yona, Shilpa Talwar, Salman Avestimehr, Nageen Himayat, IEEE Jour. on Sel. Areas in Comm. 391Saurav Prakash, Sagar Dhakal, Mustafa Riza Akdeniz, Yair Yona, Shilpa Talwar, Salman Avestimehr, and Nageen Himayat, \"Coded computing for low-latency federated learning over wireless edge networks,\" IEEE Jour. on Sel. Areas in Comm., vol. 39, no. 1, pp. 233- 250, 2020.\n\nUavassisted online machine learning over multi-tiered networks: A hierarchical nested personalized federated learning approach. Su Wang, Seyyedali Hosseinalipour, Maria Gorlatova, Mung Christopher G Brinton, Chiang, IEEE Trans. on Net. and Serv. Manag. Su Wang, Seyyedali Hosseinalipour, Maria Gorlatova, Christopher G Brinton, and Mung Chiang, \"Uav- assisted online machine learning over multi-tiered net- works: A hierarchical nested personalized federated learning approach,\" IEEE Trans. on Net. and Serv. Manag., 2022.\n\nA hierarchical incentive design toward motivating participation in coded federated learning. Wei Jer Shyuan Ng, Yang Bryan, Zehui Lim, Xianbin Xiong, Dusit Cao, Cyril Niyato, Dong In Leung, Kim, IEEE J. Sel. Areas Commun. 401Jer Shyuan Ng, Wei Yang Bryan Lim, Zehui Xiong, Xi- anbin Cao, Dusit Niyato, Cyril Leung, and Dong In Kim, \"A hierarchical incentive design toward motivating par- ticipation in coded federated learning,\" IEEE J. Sel. Ar- eas Commun., vol. 40, no. 1, pp. 359-375, 2022.\n\nShilpa Talwar, and Nageen Himayat. Sagar Dhakal, Saurav Prakash, Yair Yona, IEEE Globecom Workshop. Coded federated learningSagar Dhakal, Saurav Prakash, Yair Yona, Shilpa Tal- war, and Nageen Himayat, \"Coded federated learning,\" in IEEE Globecom Workshop, 2019, pp. 1-6.\n\nSpeeding up distributed machine learning using codes. Kangwook Lee, Maximilian Lam, Ramtin Pedarsani, Dimitris Papailiopoulos, Kannan Ramchandran, IEEE Trans. on Info. Th. 643Kangwook Lee, Maximilian Lam, Ramtin Pedarsani, Dimitris Papailiopoulos, and Kannan Ramchandran, \"Speeding up distributed machine learning using codes,\" IEEE Trans. on Info. Th., vol. 64, no. 3, pp. 1514-1529, 2018.\n\nShort-dot: Computing large linear transforms distributedly using coded short dot products. Sanghamitra Dutta, Viveck Cadambe, Pulkit Grover, Proc. of Adv. in Neur. Inf. Proc. Syst. of Adv. in Neur. Inf. . SystSanghamitra Dutta, Viveck Cadambe, and Pulkit Grover, \"Short-dot: Computing large linear transforms distribut- edly using coded short dot products,\" in Proc. of Adv. in Neur. Inf. Proc. Syst., 2016, pp. 2100-2108.\n\nPolynomial codes: an optimal design for highdimensional coded matrix multiplication. Qian Yu, Mohammad Maddah-Ali, Salman Avestimehr, Proc. of Adv. in Neur. Inf. Proc. Syst. of Adv. in Neur. Inf. . SystQian Yu, Mohammad Maddah-Ali, and Salman Aves- timehr, \"Polynomial codes: an optimal design for high- dimensional coded matrix multiplication,\" in Proc. of Adv. in Neur. Inf. Proc. Syst., 2017, pp. 4403-4413.\n\nCoded sparse matrix computation schemes that leverage partial stragglers. Bijoy Anindya, Aditya Das, Ramamoorthy, IEEE Trans. on Info. Th. 686Anindya Bijoy Das and Aditya Ramamoorthy, \"Coded sparse matrix computation schemes that leverage partial stragglers,\" IEEE Trans. on Info. Th., vol. 68, no. 6, pp. 4156-4181, 2022.\n\nNumerically stable polynomially coded computing. M Fahim, V R Cadambe, IEEE Trans. on Info. Th. 675M. Fahim and V. R. Cadambe, \"Numerically stable poly- nomially coded computing,\" IEEE Trans. on Info. Th., vol. 67, no. 5, pp. 2758-2785, 2021.\n\nGradient coding: Avoiding stragglers in distributed learning. Rashish Tandon, Qi Lei, G Alexandros, Nikos Dimakis, Karampatziakis, Proc. of Intl. Conf. on Mach. Learn. of Intl. Conf. on Mach. LearnRashish Tandon, Qi Lei, Alexandros G Dimakis, and Nikos Karampatziakis, \"Gradient coding: Avoiding stragglers in distributed learning,\" in Proc. of Intl. Conf. on Mach. Learn., 2017, pp. 3368-3376.\n\nA unified treatment of partial stragglers and sparse matrices in coded matrix computation. Bijoy Anindya, Aditya Das, Ramamoorthy, IEEE Jour. on Sel. Area. in Info. Th. 32Anindya Bijoy Das and Aditya Ramamoorthy, \"A uni- fied treatment of partial stragglers and sparse matrices in coded matrix computation,\" IEEE Jour. on Sel. Area. in Info. Th., vol. 3, no. 2, pp. 241-256, 2022.\n\nOn the optimal recovery threshold of coded matrix multiplication. Sanghamitra Dutta, Mohammad Fahim, Farzin Haddadpour, Haewon Jeong, Viveck Cadambe, Pulkit Grover, IEEE Trans. on Info. Th. 661Sanghamitra Dutta, Mohammad Fahim, Farzin Had- dadpour, Haewon Jeong, Viveck Cadambe, and Pulkit Grover, \"On the optimal recovery threshold of coded matrix multiplication,\" IEEE Trans. on Info. Th., vol. 66, no. 1, pp. 278-301, 2020.\n\nRandom Khatri-Rao-product codes for numerically-stable distributed matrix multiplication. A M Subramaniam, A Heidarzadeh, K R Narayanan, Proc. of Annual Conf. on Comm., Control, and Computing (Allerton). of Annual Conf. on Comm., Control, and Computing (Allerton)A. M. Subramaniam, A. Heidarzadeh, and K. R. Narayanan, \"Random Khatri-Rao-product codes for numerically-stable distributed matrix multiplication,\" in Proc. of Annual Conf. on Comm., Control, and Comput- ing (Allerton), Sep. 2019, pp. 253-259.\n\nVariable coded batch matrix multiplication. Lev Tauz, Lara Dolecek, IEEE Jour. on Sel. Area. in Info. Th. 32Lev Tauz and Lara Dolecek, \"Variable coded batch ma- trix multiplication,\" IEEE Jour. on Sel. Area. in Info. Th., vol. 3, no. 2, pp. 306-320, 2022.\n\nDevice sampling for heterogeneous federated learning: Theory, algorithms, and implementation. Su Wang, Mengyuan Lee, Seyyedali Hosseinalipour, Roberto Morabito, Mung Chiang, Christopher G Brinton, Proc. of Intl. Conf. on Comp. of Intl. Conf. on CompSu Wang, Mengyuan Lee, Seyyedali Hosseinalipour, Roberto Morabito, Mung Chiang, and Christopher G Brinton, \"Device sampling for heterogeneous federated learning: Theory, algorithms, and implementation,\" in Proc. of Intl. Conf. on Comp. Comm., 2021, pp. 1-10.\n\nNetwork-aware optimization of distributed learning for fog computing. Yuwei Tu, Yichen Ruan, Satyavrat Wagle, Carlee Joe-Wong Christopher G Brinton, Proc. of Intl. Conf. on Comp. of Intl. Conf. on CompYuwei Tu, Yichen Ruan, Satyavrat Wagle, Christo- pher G Brinton, and Carlee Joe-Wong, \"Network-aware optimization of distributed learning for fog computing,\" in Proc. of Intl. Conf. on Comp. Comm., 2020, pp. 2509- 2518.\n\nEfficient and robust distributed matrix computations via convolutional coding. Aditya Anindya Bijoy Das, Namrata Ramamoorthy, Vaswani, IEEE Trans. on Info. Th. 679Anindya Bijoy Das, Aditya Ramamoorthy, and Namrata Vaswani, \"Efficient and robust distributed matrix com- putations via convolutional coding,\" IEEE Trans. on Info. Th., vol. 67, no. 9, pp. 6266-6282, 2021.\n\nFrom federated to fog learning: Distributed machine learning over heterogeneous wireless networks. Seyyedali Hosseinalipour, Vaneet Christopher G Brinton, Huaiyu Aggarwal, Mung Dai, Chiang, IEEE Comm. Mag. 5812Seyyedali Hosseinalipour, Christopher G Brinton, Va- neet Aggarwal, Huaiyu Dai, and Mung Chiang, \"From federated to fog learning: Distributed machine learning over heterogeneous wireless networks,\" IEEE Comm. Mag., vol. 58, no. 12, pp. 41-47, 2020.\n\nEmbedding alignment for unsupervised federated learning via smart data exchange. Satyavrat Wagle, Seyyedali Hosseinalipour, Naji Khosravan, Mung Chiang, Christopher G Brinton, Proc. of IEEE Glob. Comm. Conf. IEEE, 2022. of IEEE Glob. Comm. Conf. IEEE, 2022Satyavrat Wagle, Seyyedali Hosseinalipour, Naji Khos- ravan, Mung Chiang, and Christopher G Brinton, \"Em- bedding alignment for unsupervised federated learning via smart data exchange,\" in Proc. of IEEE Glob. Comm. Conf. IEEE, 2022, pp. 1-6.\n\nHybrid-fl for wireless networks: Cooperative learning mechanism using non-iid data. Naoya Yoshida, Takayuki Nishio, Masahiro Morikura, Koji Yamamoto, Ryo Yonetani, Proc. of IEEE Intl. Conf. Comm. IEEE, 2020. of IEEE Intl. Conf. Comm. IEEE, 2020Naoya Yoshida, Takayuki Nishio, Masahiro Morikura, Koji Yamamoto, and Ryo Yonetani, \"Hybrid-fl for wire- less networks: Cooperative learning mechanism using non-iid data,\" in Proc. of IEEE Intl. Conf. Comm. IEEE, 2020, pp. 1-7.\n\nJ R Marshall, Hall, Combinatorial theory. WileyJR Marshall. Hall, Combinatorial theory, Wiley, 1986.\n\nFast probabilistic algorithms for verification of polynomial identities. T Jacob, Schwartz, Jour. of the ACM (JACM). 274Jacob T Schwartz, \"Fast probabilistic algorithms for verification of polynomial identities,\" Jour. of the ACM (JACM), vol. 27, no. 4, pp. 701-717, 1980.\n", "annotations": {"author": "[{\"end\":104,\"start\":86},{\"end\":212,\"start\":105},{\"end\":226,\"start\":213},{\"end\":249,\"start\":227},{\"end\":340,\"start\":250}]", "publisher": null, "author_last_name": "[{\"end\":103,\"start\":100},{\"end\":123,\"start\":112},{\"end\":225,\"start\":221},{\"end\":248,\"start\":241}]", "author_first_name": "[{\"end\":93,\"start\":86},{\"end\":99,\"start\":94},{\"end\":111,\"start\":105},{\"end\":218,\"start\":213},{\"end\":220,\"start\":219},{\"end\":238,\"start\":227},{\"end\":240,\"start\":239}]", "author_affiliation": "[{\"end\":211,\"start\":125},{\"end\":339,\"start\":251}]", "title": "[{\"end\":72,\"start\":1},{\"end\":412,\"start\":341}]", "venue": null, "abstract": "[{\"end\":1309,\"start\":523}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1741,\"start\":1738},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1744,\"start\":1741},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1747,\"start\":1744},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1952,\"start\":1949},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1954,\"start\":1952},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2302,\"start\":2299},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2305,\"start\":2302},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2308,\"start\":2305},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2311,\"start\":2308},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2314,\"start\":2311},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2318,\"start\":2314},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2322,\"start\":2318},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2326,\"start\":2322},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2330,\"start\":2326},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2334,\"start\":2330},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2433,\"start\":2430},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3079,\"start\":3076},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3272,\"start\":3269},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3275,\"start\":3272},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3278,\"start\":3275},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3577,\"start\":3574},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3579,\"start\":3577},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3582,\"start\":3579},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3604,\"start\":3601},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3606,\"start\":3604},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3609,\"start\":3606},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3612,\"start\":3609},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3615,\"start\":3612},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4869,\"start\":4866},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5654,\"start\":5652},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6261,\"start\":6258},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6264,\"start\":6261},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6267,\"start\":6264},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6289,\"start\":6285},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6292,\"start\":6289},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6295,\"start\":6292},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6517,\"start\":6514},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6519,\"start\":6517},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6521,\"start\":6519},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6524,\"start\":6521},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9073,\"start\":9069},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9625,\"start\":9622},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":11300,\"start\":11296},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":12034,\"start\":12031},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":12036,\"start\":12034},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":12068,\"start\":12065},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":12070,\"start\":12068},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":12073,\"start\":12070},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":12207,\"start\":12204},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":12209,\"start\":12207},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":12212,\"start\":12209},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":17064,\"start\":17061},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":17066,\"start\":17064},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":17069,\"start\":17066},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":17203,\"start\":17200},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":17205,\"start\":17203},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":17365,\"start\":17362},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":17374,\"start\":17370},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":18236,\"start\":18233},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":18238,\"start\":18236},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":18519,\"start\":18515},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":18522,\"start\":18519},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":19058,\"start\":19055},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":19060,\"start\":19058},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":19063,\"start\":19060},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":19066,\"start\":19063},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":19923,\"start\":19920},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":19925,\"start\":19923},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":19928,\"start\":19925},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":19931,\"start\":19928},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":20288,\"start\":20285},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":20290,\"start\":20288},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":20293,\"start\":20290},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":20296,\"start\":20293}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":20397,\"start\":20298},{\"attributes\":{\"id\":\"fig_1\"},\"end\":20731,\"start\":20398},{\"attributes\":{\"id\":\"fig_2\"},\"end\":20910,\"start\":20732},{\"attributes\":{\"id\":\"fig_3\"},\"end\":21175,\"start\":20911},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":21432,\"start\":21176},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":21681,\"start\":21433},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":22031,\"start\":21682}]", "paragraph": "[{\"end\":1872,\"start\":1325},{\"end\":3865,\"start\":1874},{\"end\":4208,\"start\":3903},{\"end\":5303,\"start\":4210},{\"end\":5783,\"start\":5435},{\"end\":5909,\"start\":5785},{\"end\":6775,\"start\":5977},{\"end\":7119,\"start\":6806},{\"end\":8128,\"start\":7121},{\"end\":8315,\"start\":8130},{\"end\":8411,\"start\":8317},{\"end\":9271,\"start\":8450},{\"end\":9680,\"start\":9273},{\"end\":9864,\"start\":9705},{\"end\":10496,\"start\":10034},{\"end\":12538,\"start\":10537},{\"end\":13911,\"start\":12669},{\"end\":14593,\"start\":13958},{\"end\":14984,\"start\":14595},{\"end\":15598,\"start\":14986},{\"end\":16264,\"start\":15678},{\"end\":16445,\"start\":16266},{\"end\":16931,\"start\":16447},{\"end\":17490,\"start\":16956},{\"end\":18675,\"start\":17492},{\"end\":19343,\"start\":18677},{\"end\":20297,\"start\":19345}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":5434,\"start\":5304},{\"attributes\":{\"id\":\"formula_1\"},\"end\":5976,\"start\":5910},{\"attributes\":{\"id\":\"formula_2\"},\"end\":8449,\"start\":8412},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9704,\"start\":9681},{\"attributes\":{\"id\":\"formula_4\"},\"end\":10033,\"start\":9865},{\"attributes\":{\"id\":\"formula_5\"},\"end\":10536,\"start\":10497},{\"attributes\":{\"id\":\"formula_6\"},\"end\":12637,\"start\":12539},{\"attributes\":{\"id\":\"formula_7\"},\"end\":13957,\"start\":13912},{\"attributes\":{\"id\":\"formula_8\"},\"end\":15677,\"start\":15599}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":18086,\"start\":18079},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":19903,\"start\":19896}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1323,\"start\":1311},{\"attributes\":{\"n\":\"2.\"},\"end\":3901,\"start\":3868},{\"attributes\":{\"n\":\"3.\"},\"end\":6804,\"start\":6778},{\"attributes\":{\"n\":\"4.\"},\"end\":12667,\"start\":12639},{\"attributes\":{\"n\":\"5.\"},\"end\":16954,\"start\":16934},{\"end\":20300,\"start\":20299},{\"end\":20407,\"start\":20399},{\"end\":20920,\"start\":20912},{\"end\":21443,\"start\":21434},{\"end\":21692,\"start\":21683}]", "table": "[{\"end\":21432,\"start\":21223},{\"end\":21681,\"start\":21559},{\"end\":22031,\"start\":21862}]", "figure_caption": "[{\"end\":20397,\"start\":20301},{\"end\":20731,\"start\":20409},{\"end\":20910,\"start\":20734},{\"end\":21175,\"start\":20922},{\"end\":21223,\"start\":21178},{\"end\":21559,\"start\":21445},{\"end\":21862,\"start\":21694}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11746,\"start\":11740},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":16488,\"start\":16482},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":16837,\"start\":16828},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":19152,\"start\":19146}]", "bib_author_first_name": "[{\"end\":22119,\"start\":22113},{\"end\":22134,\"start\":22129},{\"end\":22150,\"start\":22143},{\"end\":22155,\"start\":22151},{\"end\":22169,\"start\":22165},{\"end\":22182,\"start\":22176},{\"end\":22197,\"start\":22191},{\"end\":22216,\"start\":22210},{\"end\":22664,\"start\":22662},{\"end\":22680,\"start\":22671},{\"end\":22702,\"start\":22697},{\"end\":22718,\"start\":22714},{\"end\":23154,\"start\":23151},{\"end\":23187,\"start\":23182},{\"end\":23200,\"start\":23193},{\"end\":23213,\"start\":23208},{\"end\":23224,\"start\":23219},{\"end\":23237,\"start\":23233},{\"end\":23240,\"start\":23238},{\"end\":23593,\"start\":23588},{\"end\":23608,\"start\":23602},{\"end\":23622,\"start\":23618},{\"end\":23888,\"start\":23880},{\"end\":23904,\"start\":23894},{\"end\":23916,\"start\":23910},{\"end\":23936,\"start\":23928},{\"end\":23959,\"start\":23953},{\"end\":24320,\"start\":24309},{\"end\":24334,\"start\":24328},{\"end\":24350,\"start\":24344},{\"end\":24731,\"start\":24727},{\"end\":24744,\"start\":24736},{\"end\":24763,\"start\":24757},{\"end\":25133,\"start\":25128},{\"end\":25149,\"start\":25143},{\"end\":25428,\"start\":25427},{\"end\":25437,\"start\":25436},{\"end\":25439,\"start\":25438},{\"end\":25691,\"start\":25684},{\"end\":25702,\"start\":25700},{\"end\":25709,\"start\":25708},{\"end\":25727,\"start\":25722},{\"end\":26114,\"start\":26109},{\"end\":26130,\"start\":26124},{\"end\":26477,\"start\":26466},{\"end\":26493,\"start\":26485},{\"end\":26507,\"start\":26501},{\"end\":26526,\"start\":26520},{\"end\":26540,\"start\":26534},{\"end\":26556,\"start\":26550},{\"end\":26919,\"start\":26918},{\"end\":26921,\"start\":26920},{\"end\":26936,\"start\":26935},{\"end\":26951,\"start\":26950},{\"end\":26953,\"start\":26952},{\"end\":27383,\"start\":27380},{\"end\":27394,\"start\":27390},{\"end\":27689,\"start\":27687},{\"end\":27704,\"start\":27696},{\"end\":27719,\"start\":27710},{\"end\":27743,\"start\":27736},{\"end\":27758,\"start\":27754},{\"end\":27778,\"start\":27767},{\"end\":27780,\"start\":27779},{\"end\":28177,\"start\":28172},{\"end\":28188,\"start\":28182},{\"end\":28204,\"start\":28195},{\"end\":28227,\"start\":28212},{\"end\":28609,\"start\":28603},{\"end\":28636,\"start\":28629},{\"end\":29002,\"start\":28993},{\"end\":29025,\"start\":29019},{\"end\":29055,\"start\":29049},{\"end\":29070,\"start\":29066},{\"end\":29444,\"start\":29435},{\"end\":29461,\"start\":29452},{\"end\":29482,\"start\":29478},{\"end\":29498,\"start\":29494},{\"end\":29518,\"start\":29507},{\"end\":29520,\"start\":29519},{\"end\":29942,\"start\":29937},{\"end\":29960,\"start\":29952},{\"end\":29977,\"start\":29969},{\"end\":29992,\"start\":29988},{\"end\":30006,\"start\":30003},{\"end\":30327,\"start\":30326},{\"end\":30329,\"start\":30328},{\"end\":30502,\"start\":30501}]", "bib_author_last_name": "[{\"end\":22127,\"start\":22120},{\"end\":22141,\"start\":22135},{\"end\":22163,\"start\":22156},{\"end\":22174,\"start\":22170},{\"end\":22189,\"start\":22183},{\"end\":22208,\"start\":22198},{\"end\":22224,\"start\":22217},{\"end\":22669,\"start\":22665},{\"end\":22695,\"start\":22681},{\"end\":22712,\"start\":22703},{\"end\":22740,\"start\":22719},{\"end\":22748,\"start\":22742},{\"end\":23168,\"start\":23155},{\"end\":23180,\"start\":23170},{\"end\":23191,\"start\":23188},{\"end\":23206,\"start\":23201},{\"end\":23217,\"start\":23214},{\"end\":23231,\"start\":23225},{\"end\":23246,\"start\":23241},{\"end\":23251,\"start\":23248},{\"end\":23600,\"start\":23594},{\"end\":23616,\"start\":23609},{\"end\":23627,\"start\":23623},{\"end\":23892,\"start\":23889},{\"end\":23908,\"start\":23905},{\"end\":23926,\"start\":23917},{\"end\":23951,\"start\":23937},{\"end\":23971,\"start\":23960},{\"end\":24326,\"start\":24321},{\"end\":24342,\"start\":24335},{\"end\":24357,\"start\":24351},{\"end\":24734,\"start\":24732},{\"end\":24755,\"start\":24745},{\"end\":24774,\"start\":24764},{\"end\":25141,\"start\":25134},{\"end\":25153,\"start\":25150},{\"end\":25166,\"start\":25155},{\"end\":25434,\"start\":25429},{\"end\":25447,\"start\":25440},{\"end\":25698,\"start\":25692},{\"end\":25706,\"start\":25703},{\"end\":25720,\"start\":25710},{\"end\":25735,\"start\":25728},{\"end\":25751,\"start\":25737},{\"end\":26122,\"start\":26115},{\"end\":26134,\"start\":26131},{\"end\":26147,\"start\":26136},{\"end\":26483,\"start\":26478},{\"end\":26499,\"start\":26494},{\"end\":26518,\"start\":26508},{\"end\":26532,\"start\":26527},{\"end\":26548,\"start\":26541},{\"end\":26563,\"start\":26557},{\"end\":26933,\"start\":26922},{\"end\":26948,\"start\":26937},{\"end\":26963,\"start\":26954},{\"end\":27388,\"start\":27384},{\"end\":27402,\"start\":27395},{\"end\":27694,\"start\":27690},{\"end\":27708,\"start\":27705},{\"end\":27734,\"start\":27720},{\"end\":27752,\"start\":27744},{\"end\":27765,\"start\":27759},{\"end\":27788,\"start\":27781},{\"end\":28180,\"start\":28178},{\"end\":28193,\"start\":28189},{\"end\":28210,\"start\":28205},{\"end\":28249,\"start\":28228},{\"end\":28627,\"start\":28610},{\"end\":28648,\"start\":28637},{\"end\":28657,\"start\":28650},{\"end\":29017,\"start\":29003},{\"end\":29047,\"start\":29026},{\"end\":29064,\"start\":29056},{\"end\":29074,\"start\":29071},{\"end\":29082,\"start\":29076},{\"end\":29450,\"start\":29445},{\"end\":29476,\"start\":29462},{\"end\":29492,\"start\":29483},{\"end\":29505,\"start\":29499},{\"end\":29528,\"start\":29521},{\"end\":29950,\"start\":29943},{\"end\":29967,\"start\":29961},{\"end\":29986,\"start\":29978},{\"end\":30001,\"start\":29993},{\"end\":30015,\"start\":30007},{\"end\":30338,\"start\":30330},{\"end\":30344,\"start\":30340},{\"end\":30508,\"start\":30503},{\"end\":30518,\"start\":30510}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":226306817},\"end\":22532,\"start\":22033},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":235683447},\"end\":23056,\"start\":22534},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":243959593},\"end\":23551,\"start\":23058},{\"attributes\":{\"id\":\"b3\"},\"end\":23824,\"start\":23553},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":3442617},\"end\":24216,\"start\":23826},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":9028807},\"end\":24640,\"start\":24218},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":20729541},\"end\":25052,\"start\":24642},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":228372945},\"end\":25376,\"start\":25054},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":84187068},\"end\":25620,\"start\":25378},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":33632433},\"end\":26016,\"start\":25622},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":237635158},\"end\":26398,\"start\":26018},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":19323670},\"end\":26826,\"start\":26400},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":196622289},\"end\":27334,\"start\":26828},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":234482469},\"end\":27591,\"start\":27336},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":230437685},\"end\":28100,\"start\":27593},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":215828136},\"end\":28522,\"start\":28102},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":219179466},\"end\":28892,\"start\":28524},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":225069789},\"end\":29352,\"start\":28894},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":251371603},\"end\":29851,\"start\":29354},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":209370482},\"end\":30324,\"start\":29853},{\"attributes\":{\"id\":\"b20\"},\"end\":30426,\"start\":30326},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":8314102},\"end\":30700,\"start\":30428}]", "bib_title": "[{\"end\":22111,\"start\":22033},{\"end\":22660,\"start\":22534},{\"end\":23149,\"start\":23058},{\"end\":23586,\"start\":23553},{\"end\":23878,\"start\":23826},{\"end\":24307,\"start\":24218},{\"end\":24725,\"start\":24642},{\"end\":25126,\"start\":25054},{\"end\":25425,\"start\":25378},{\"end\":25682,\"start\":25622},{\"end\":26107,\"start\":26018},{\"end\":26464,\"start\":26400},{\"end\":26916,\"start\":26828},{\"end\":27378,\"start\":27336},{\"end\":27685,\"start\":27593},{\"end\":28170,\"start\":28102},{\"end\":28601,\"start\":28524},{\"end\":28991,\"start\":28894},{\"end\":29433,\"start\":29354},{\"end\":29935,\"start\":29853},{\"end\":30499,\"start\":30428}]", "bib_author": "[{\"end\":22129,\"start\":22113},{\"end\":22143,\"start\":22129},{\"end\":22165,\"start\":22143},{\"end\":22176,\"start\":22165},{\"end\":22191,\"start\":22176},{\"end\":22210,\"start\":22191},{\"end\":22226,\"start\":22210},{\"end\":22671,\"start\":22662},{\"end\":22697,\"start\":22671},{\"end\":22714,\"start\":22697},{\"end\":22742,\"start\":22714},{\"end\":22750,\"start\":22742},{\"end\":23170,\"start\":23151},{\"end\":23182,\"start\":23170},{\"end\":23193,\"start\":23182},{\"end\":23208,\"start\":23193},{\"end\":23219,\"start\":23208},{\"end\":23233,\"start\":23219},{\"end\":23248,\"start\":23233},{\"end\":23253,\"start\":23248},{\"end\":23602,\"start\":23588},{\"end\":23618,\"start\":23602},{\"end\":23629,\"start\":23618},{\"end\":23894,\"start\":23880},{\"end\":23910,\"start\":23894},{\"end\":23928,\"start\":23910},{\"end\":23953,\"start\":23928},{\"end\":23973,\"start\":23953},{\"end\":24328,\"start\":24309},{\"end\":24344,\"start\":24328},{\"end\":24359,\"start\":24344},{\"end\":24736,\"start\":24727},{\"end\":24757,\"start\":24736},{\"end\":24776,\"start\":24757},{\"end\":25143,\"start\":25128},{\"end\":25155,\"start\":25143},{\"end\":25168,\"start\":25155},{\"end\":25436,\"start\":25427},{\"end\":25449,\"start\":25436},{\"end\":25700,\"start\":25684},{\"end\":25708,\"start\":25700},{\"end\":25722,\"start\":25708},{\"end\":25737,\"start\":25722},{\"end\":25753,\"start\":25737},{\"end\":26124,\"start\":26109},{\"end\":26136,\"start\":26124},{\"end\":26149,\"start\":26136},{\"end\":26485,\"start\":26466},{\"end\":26501,\"start\":26485},{\"end\":26520,\"start\":26501},{\"end\":26534,\"start\":26520},{\"end\":26550,\"start\":26534},{\"end\":26565,\"start\":26550},{\"end\":26935,\"start\":26918},{\"end\":26950,\"start\":26935},{\"end\":26965,\"start\":26950},{\"end\":27390,\"start\":27380},{\"end\":27404,\"start\":27390},{\"end\":27696,\"start\":27687},{\"end\":27710,\"start\":27696},{\"end\":27736,\"start\":27710},{\"end\":27754,\"start\":27736},{\"end\":27767,\"start\":27754},{\"end\":27790,\"start\":27767},{\"end\":28182,\"start\":28172},{\"end\":28195,\"start\":28182},{\"end\":28212,\"start\":28195},{\"end\":28251,\"start\":28212},{\"end\":28629,\"start\":28603},{\"end\":28650,\"start\":28629},{\"end\":28659,\"start\":28650},{\"end\":29019,\"start\":28993},{\"end\":29049,\"start\":29019},{\"end\":29066,\"start\":29049},{\"end\":29076,\"start\":29066},{\"end\":29084,\"start\":29076},{\"end\":29452,\"start\":29435},{\"end\":29478,\"start\":29452},{\"end\":29494,\"start\":29478},{\"end\":29507,\"start\":29494},{\"end\":29530,\"start\":29507},{\"end\":29952,\"start\":29937},{\"end\":29969,\"start\":29952},{\"end\":29988,\"start\":29969},{\"end\":30003,\"start\":29988},{\"end\":30017,\"start\":30003},{\"end\":30340,\"start\":30326},{\"end\":30346,\"start\":30340},{\"end\":30510,\"start\":30501},{\"end\":30520,\"start\":30510}]", "bib_venue": "[{\"end\":22258,\"start\":22226},{\"end\":22785,\"start\":22750},{\"end\":23278,\"start\":23253},{\"end\":23651,\"start\":23629},{\"end\":23996,\"start\":23973},{\"end\":24397,\"start\":24359},{\"end\":24814,\"start\":24776},{\"end\":25191,\"start\":25168},{\"end\":25472,\"start\":25449},{\"end\":25788,\"start\":25753},{\"end\":26185,\"start\":26149},{\"end\":26588,\"start\":26565},{\"end\":27030,\"start\":26965},{\"end\":27440,\"start\":27404},{\"end\":27818,\"start\":27790},{\"end\":28279,\"start\":28251},{\"end\":28682,\"start\":28659},{\"end\":29098,\"start\":29084},{\"end\":29572,\"start\":29530},{\"end\":30059,\"start\":30017},{\"end\":30366,\"start\":30346},{\"end\":30543,\"start\":30520},{\"end\":24427,\"start\":24399},{\"end\":24844,\"start\":24816},{\"end\":25819,\"start\":25790},{\"end\":27091,\"start\":27032},{\"end\":27842,\"start\":27820},{\"end\":28303,\"start\":28281},{\"end\":29610,\"start\":29574},{\"end\":30097,\"start\":30061}]"}}}, "year": 2023, "month": 12, "day": 17}