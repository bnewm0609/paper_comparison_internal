{"id": 52282758, "updated": "2023-09-28 13:28:04.905", "metadata": {"title": "GANVO: Unsupervised Deep Monocular Visual Odometry and Depth Estimation with Generative Adversarial Networks", "authors": "[{\"first\":\"Yasin\",\"last\":\"Almalioglu\",\"middle\":[]},{\"first\":\"Muhamad\",\"last\":\"Saputra\",\"middle\":[\"Risqi\",\"U.\"]},{\"first\":\"Pedro\",\"last\":\"Gusm\u00e3o\",\"middle\":[\"P.\",\"B.\",\"de\"]},{\"first\":\"Andrew\",\"last\":\"Markham\",\"middle\":[]},{\"first\":\"Niki\",\"last\":\"Trigoni\",\"middle\":[]}]", "venue": "2019 International Conference on Robotics and Automation (ICRA)", "journal": "2019 International Conference on Robotics and Automation (ICRA)", "publication_date": {"year": 2018, "month": null, "day": null}, "abstract": "In the last decade, supervised deep learning approaches have been extensively employed in visual odometry (VO) applications, which is not feasible in environments where labelled data is not abundant. On the other hand, unsupervised deep learning approaches for localization and mapping in unknown environments from unlabelled data have received comparatively less attention in VO research. In this study, we propose a generative unsupervised learning framework that predicts 6-DoF pose camera motion and monocular depth map of the scene from unlabelled RGB image sequences, using deep convolutional Generative Adversarial Networks (GANs). We create a supervisory signal by warping view sequences and assigning the re-projection minimization to the objective loss function that is adopted in multi-view pose estimation and single-view depth generation network. Detailed quantitative and qualitative evaluations of the proposed framework on the KITTI [1] and Cityscapes [2] datasets show that the proposed method outperforms both existing traditional and unsupervised deep VO methods providing better results for both pose estimation and depth recovery.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "1809.05786", "mag": "3100901464", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icra/AlmaliogluSGMT19", "doi": "10.1109/icra.2019.8793512"}}, "content": {"source": {"pdf_hash": "bc7104ccdd1edc0609cd0979c3e5b0b5425ca5d3", "pdf_src": "Adhoc", "pdf_uri": "[\"https://web.archive.org/web/20201103041900/https:/ora.ox.ac.uk/objects/uuid:f510c015-0cf8-4183-b497-ad5cb4561585/download_file?file_format=application/pdf&safe_filename=Yasin-GANVO-final.pdf&type_of_work=Conference+item\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1809.05786", "status": "GREEN"}}, "grobid": {"id": "c67bc6653751db77971b467bc7d230dc1ac524b5", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/bc7104ccdd1edc0609cd0979c3e5b0b5425ca5d3.txt", "contents": "\nGANVO: Unsupervised Deep Monocular Visual Odometry and Depth Estimation with Generative Adversarial Networks\n\n\nYasin Almalioglu \nRisqi U Muhamad \nSaputra \nPedro P B De Gusm\u00e3o \nAndrew Markham \nNiki Trigoni \nGANVO: Unsupervised Deep Monocular Visual Odometry and Depth Estimation with Generative Adversarial Networks\n\nIn the last decade, supervised deep learning approaches have been extensively employed in visual odometry (VO) applications, which is not feasible in environments where labelled data is not abundant. On the other hand, unsupervised deep learning approaches for localization and mapping in unknown environments from unlabelled data have received comparatively less attention in VO research. In this study, we propose a generative unsupervised learning framework that predicts 6-DoF pose camera motion and monocular depth map of the scene from unlabelled RGB image sequences, using deep convolutional Generative Adversarial Networks (GANs). We create a supervisory signal by warping view sequences and assigning the re-projection minimization to the objective loss function that is adopted in multi-view pose estimation and single-view depth generation network. Detailed quantitative and qualitative evaluations of the proposed framework on the KITTI [1] and Cityscapes [2] datasets show that the proposed method outperforms both existing traditional and unsupervised deep VO methods providing better results for both pose estimation and depth recovery.\n\nI. INTRODUCTION\n\nVisual odometry (VO) and depth recovery are essential modules of simultaneous localization and mapping (SLAM) applications. In the last few decades, VO systems have attracted a substantial amount of attention, enabling robust localization and accurate depth map reconstruction. Monocular VO is confronted with numerous challenges such as large scale drift, the need for hand-crafted mathematical features and strict parameter tuning [3], [4]. Supervised deep learning based VO and depth recovery techniques have showed good performance in challenging environments and succesfuly alleviated issues such as scale drift, need for feature extraction and parameter finetuning [5]- [8]. VO as a regression problem in supervised deep learning exploits the capability of convolutional neural network (CNN) and recurrent neural network (RNN) to estimate camera motion, to calculate optical flow, and to extract efficient feature representations from raw RGB input [5]- [7], [9]. In recent years, unsupervised deep learning approaches have achieved remarkable results in various domains eliminating the need for labelled data [10], [11].\n\nYears of research in visual SLAM have been inspired by human navigation that easily locates obstacles even in Fig. 1: Architecture overview. The unsupervised deep learning approach consists of depth generation, multi-view pose estimation, view reconstruction, and target discrimination modules. Unlabelled image sequences from different temporal points are given to the networks to establish a supervision signal. The networks estimate relative translation and rotation between consecutive frames from different perspectives parametrized as 6-DoF motion, and depth image as a disparity map for a given view. unknown environments. A neuroscientific insight is that human brain saves a structural perception of the world, which makes it capable of real and imaginary scene reconstruction through vast previous experiences [12], [13]. In this study, we propose a novel real-time localization and generative depth estimation approach that mimics the remarkable egomotion estimation and re-constructive scene representation capabilities of human beings by training an unsupervised deep neural network. The proposed network consists of a pose regressor and depth generator network. The former regresses 6 degree-of-freedom (DoF) pose values using CNN-RNN modules, and the latter generates depth maps using deep convolutional generative adversarial network (GAN) [14]. The model takes a sequence of monocular images to estimate 6-DoF camera motion and depth map that is sampled from the same input data distribution, which is trained in an end-to-end and unsupervised fashion directly from raw input pixels. A view reconstruction approach is utilized as part of the objective function for the training in a similar way to prior works [15]- [18]. The entire pose estimation and depth map reconstruction pipeline is a consistent and systematic learning framework which continually improves its performance by collecting unlabelled monocular video data from numerous environments. This way, we want to mimic and transfer a continuous learning functionality from humans into VO domain.\n\nIn summary, the main contributions of our method are as follows:\n\n\u2022 To the best of our knowledge, this is the first monocular VO method in literature, which uses adversarial and recurrent unsupervised learning approaches for joint pose and depth map estimation. \u2022 We propose a novel adversarial technique for GANs to generate depth images without any need for depth groundtruth information. \u2022 No strict parameter tuning is necessary for pose and depth estimation, contrary to traditional VO approaches. Evaluations made on the KITTI [1] and Cityscapes [2] datasets prove the success of our pose estimation and depth map reconstruction approach. As the outline of this paper, the previous work in VO is discussed in section II. Section III gives an overview of the proposed approach named GANVO. Section IV describes the proposed unsupervised deep learning architecture and its mathematical background in detail. Section V shows our quantitative and qualitative results with comparisons to the existing VO methods. Finally, section VI concludes the study with some interesting future directions.\n\n\nII. RELATED WORK\n\nCamera motion and depth map estimations are well studied problems in machine vision domain. Many traditional techniques have been proposed in the last decade with stateof-the-art results. However, existing traditional techniques require accurate image correspondence between consecutive frames, which is frequently violated in challenging environments with low texture, complex scene structure, and occlusions [19]- [22]. Deep learning approaches are adopted in several components of the traditional techniques to solve problems such as feature extraction and matching, pose estimation, and optical flow calculation. An external supervised training has played a key role in deep learning approaches to solve these aforementioned issues.\n\nDeep VO techniques involve 6-DoF camera pose estimation, depth map reconstruction, object segmentation, optical flow extraction, and sensor fusion approaches [5], [9], [23]- [29]. Deep 6-DoF pose regression from raw RGB images was first proposed using CNNs, which was also extended to raw RGB-D images for challenging environments [30]. Using RNNs that captures the temporal dynamics boosted the accuracy of 6-DoF pose estimation in deep VO approaches, resulting in a competitive performance against model-based VO methods [5]. The unsupervised deep learning studies on simultaneous estimation of the camera motion, image depth, surface normal and optical flow to learn structure from motion indicate that the joint training of VO components provides an implicit supervisory signal to the network [16], [17], [31]. One critical issue of these unsupervised studies is the fact that they use auto encoder-decoder based traditional depth estimators with a tendency to generate overly smooth images [32]. To solve this, we apply GANs that provide shaper and more accurate depth maps. The second issue of the aforementioned unsupervised techniques is the fact that they only employ CNNs that only analyse just-in-moment information to estimate camera pose [5], [7]. We address this issue by employing a CNN-RNN architecture to capture temporal relations across frames.\n\n\nIII. ARCHITECTURE OVERVIEW\n\nAs shown in Fig. 1, the raw RGB sequences consisting of a target view and source views are stacked together to form an input batch to the multi-view pose estimation module. The module regresses the relative 6-DoF pose values of the source views with respect to the target view. In parallel, the depth generation module generates a depth map of the target view. The view reconstruction module synthesizes the target image using the generated depth map, estimated 6-DoF camera pose and nearby colour values from source images. The view reconstruction constraint that provides a supervision signal forces the neural network to synthesize a target image from multiple source images acquired from different camera poses. The view discriminator module tries to distinguish the synthesized target image from the original target image.\n\nIn the proposed adversarial scheme for a GAN, the objective of the generator is to trick the discriminator, i.e., to generate depth map for the target view reconstruction such that the discriminator cannot distinguish the reconstruction from the original. As opposed to the typical GANs, the output image of the generator is mapped to the color space of the image and the discriminator distinguishes this mapping from the original rather than a direct comparison of the generator output. The proposed scheme enables us to generate depth maps in an unsupervised manner.\n\n\nIV. UNSUPERVISED DEPTH AND POSE ESTIMATION WITH GENERATIVE ADVERSARIAL NETWORK\n\nThe network architecture of the proposed method is shown in Fig. 2. The details of the architecture are explained in the following sections.\n\n\nA. Depth Generator\n\nThe first part of the architecture is a depth network that generates a single-view depth map of the target frame. The depth network is based on a GAN design that tries to learn the probability distribution of the input images p(I t ), consisting of a generator network, G, a discriminator network D, and an encoder network E. The encoder E maps the input target image I t to a feature vector z, i.e. E(I t ) = z. G maps the feature vector z to the depth image space and D maps an input RGB image to an image likelihood.\n\n\nB. Pose Regressor\n\nThe second network shown in the bottom of Fig. 2 tries to estimate relative pose p \u2208 SE(3) introduced by motion and temporal dynamics across frames. The convolution part of the pose estimation network extracts features from the input Fig. 2: The proposed architecture for pose estimation and depth map generation. The spatial dimensions of layers and output channels show the tensor shapes that flow through the network. Generator network G maps a feature vector z generated by the encoder network E to the depth image space. The pose estimation network consists of a convolutional network that extracts VO related features, and a recurrent network that captures temporal relations among the input frame sequences. The pose results are collected after the recurrent network, which has 6 * (N \u2212 1) output channels for 6-DoF motion parameters, , where N is the length of the sequence. The view reconstruction algorithm is followed by a discriminator network D that maps the reconstructed RGB image to a likelihood of the target image. D decides whether it is shown the reconstruction or the original image.\n\nframe sequences and propagates them to the RNN part. The LSTM modules output 6 * (N \u2212 1) channels for 6-DoF pose values consisting translation and rotation parameters, where N is the length of the sequence. Although one LSTM is able to capture the sequential relations, a second LSTM improves the learning the capacity of the network, resulting in a more accurate pose regression.\n\n\nC. View Reconstruction\n\nA sequence of 3 consecutive frames is given to the pose network as input. An input sequence is denoted by < I t\u22121 , I t , I t+1 > where t > 0 is the time index, I t is the target view, and the other frames are source views I s =< I t\u22121 , I t+1 > that are used to render the target image according to the objective function:\nL g = \u2211 s \u2211 p |I t (p) \u2212\u00ce s (p)|(1)\nwhere p is the pixel coordinate index, and\u00ce s is the projected image of the source view I s onto the target coordinate frame using a depth image based rendering module. The rendering is based on the estimated depth imageD t , the 4 \u00d7 4 camera transformation matrixT t\u2192s and the source view I s [33]. We denote the homogeneous coordinates of a pixel in the target view as p t , and the camera intrinsics matrix as K.\n\nCoordinates of p t are projected onto the source view p s with:\np s \u223c KT t\u2192sDt (p t )K \u22121 p t .(2)\nSince the value of p s is not discrete, an interpolation is required to find the expected intensity value at that position.\n\nTo do that, we use bilinear interpolation using the 4 discrete neighbours of p s [34]. The mean intensity value for projected pixel is estimated as follows:\nI s (p t ) = I s (p s ) = \u2211 i\u2208{top,bottom}, j\u2208{le f t,right} w i j I s (p i j s ) (3)\nwhere w i j is the proximity value between projected and neighbouring pixels, which sums up to 1.\n\n\nD. View Discriminator\n\nA realistic image is synthesized by the view reconstruction algorithm using the depth image generated by G and estimated pose values. D discriminates between the reconstructed image and the real image sampled from the target data distribution p data , playing an adversarial role. These networks are trained by optimizing the objective loss function:\nL d = min G max D V (G, D) =E I\u223cp data (I) [log(D(I))]+ E z\u223cp(z) [log(1 \u2212 D(G(z)))],(4)\nwhere I is the sample from the p data distribution and z is a feature encoding on the latent space.\n\n\nE. The Adversarial Training\n\nIn contrast to the original GAN [14], we remove fully connected hidden layers for deeper architectures and use batchnorm in G and D networks. Pooling layers are replaced by strided convolutions and LeakyReLU activation is used for all layers in D. In G network, pooling layers are replaced by fractional-strided convolutions and ReLU activation is used for all layers except for the output layer that uses tanh non-linearity. The GAN with these modifications generates non-blurry images and resolves the convergence problem during the training [35]. The final objective for the optimization of weights in the architecture is:\nL f inal = L g + \u03b2 L d(5)\nwhere \u03b2 is the balance factor. The optimal \u03b2 is experimentally found to be the ratio between the expected values L g and L d at the end of the training.\n\n\nV. EXPERIMENTS AND RESULTS\n\nWe implemented the architecture with the publicly available Tensorflow framework [36]. Batch normalization is employed for all of the layers except for the output layers. The weights of the network are optimized with Adam optimization to increase the convergence rate, with the parameters \u03b2 1 = 0.9, \u03b2 2 = 0.999, learning rate of 0.1 and mini-batch size of 8. For training purposes, the input tensors of the model are assigned to sequential images of size 128 \u00d7 416, whereas they are not limited to any specific image size at test time. Three consecutive images are stacked together to form the input batch. We use the KITTI dataset [37] for benchmarking and the Cityscapes dataset [2] for evaluating cross-dataset generalization ability in the experiments. The model is trained on a NVIDIA TITAN V model GPU. We compare the proposed method with standard training/test splits on the KITTI dataset for the odometry and monocular depth map estimation tasks.\n\n\nA. Pose estimation benchmark\n\nWe have evaluated the pose estimation performance of our GANVO on the standard KITTI visual odometry split. The dataset contains 11 driving sequences with groundtruth odometry obtained through the IMU/GPS sensors, where the sequences 00 \u2212 08 are used for training and 09 \u2212 10 for testing without any use of the pose and depth groundtruth during the training session. The network regresses the pose predictions as 6-DoF relative motion (Euclidean coordinates for translation and rotation) between sequences. We compare the pose estimation accuracy with the existing unsupervised deep learning approaches with the same sequence length of 5, and monocular ORB SLAM. The results are evaluated using Absolute Trajectory Error (ATE) [38] for 5 consecutive input frames with an optimized scaling factor to resolve scale ambiguity, which is reported to be the best sequence length for the compared methods [17], [39]. As shown in Table I and Fig. 3, the proposed method outperforms all the competing unsupervised and traditional baselines, without any need of global optimization steps such as loop closure detection, bundle adjustment and re-localization, revealing that GANVO captures long-term high level odometry details in addition to short-term low level odometry features.\n\n\nB. Single-view depth evaluation\n\nWe evaluate the performance of the proposed depth estimation approach on a benchmark split of the KITTI dataset [40] to compare with the existing learning-based and traditional depth estimation approaches. In a total of 44, 000 frames, we use 40, 000 frames for training and 4, 000 frames for validation. The sequence length of the input data is set to be 3 frames during the training session to have the same evaluation setup with the compared methods, where the central frame is the target view for the depth estimation. The groundtruth captured by Light Detection and Ranging (LiDAR) sensor is projected into the image plane for the evaluation in terms of error and accuracy metrics. The predicted depth map, D p , is multiplied by a scaling factor,\u015d, that matches the median with the groundtruth depth map, D g , to solve the scale ambiguity issue, i.e.\u015d = median(D g )/median(D p ). Moreover, we test the adaptability of the proposed approach training on the Cityscapes dataset and finetuning on the KITTI dataset. Figure 4 shows example depth map results predicted by  Method Seq.09 Seq.10 ORB-SLAM 0.014 \u00b1 0.008 0.012 \u00b1 0.011 SfM-Learner [17] 0.016 \u00b1 0.009 0.013 \u00b1 0.009 GeoNet [39] 0.012 \u00b1 0.007 0.012 \u00b1 0.009 Our GANVO 0.009 \u00b1 0.005 0.010 \u00b1 0.013 We also report the results of the other methods for comparison that are taken from [17], [39]. Our method outperforms all of the other methods.\n\nthe proposed method, SfM-Learner [17] and GeoNet [39]. It is clearly seen that GANVO outputs sharper and more accurate depth maps compared to the other methods that fundamentally use an encoder-decoder network with various implementations. An explanation for this result is that adversarial training using the convolutional domain-related feature set of the discriminator allows to obtain less blurry results [32]. Furthermore, it is also seen in Fig. 4 that the depth maps predicted by the proposed GANVO captures the small objects in the scene whereas the other methods tend to ignore them. A loss function in image space leads to averaging all likely locations of details, whereas an adversarial loss function in feature space with a natural image prior makes the proposed GANVO more sensitive to the likely positions of the details in the scene [32]. The proposed GANVO also accurately predicts the depth values of the objects in lowtextured areas caused by the shading inconsistencies in a scene. In Fig. 5, we demonstrate typical failure cases in the compared unsupervised methods, which are caused by challenges such as poor road signs in rural areas and huge objects covering the most of the visual input. Even in these cases, GANVO performs slightly better than the existing methods.\n\nAs shown in Table II quantitatively, our unsupervised approach significantly outperforms both the existing unsupervised methods [17], [39], [42] and even supervised methods [40], [41]. Furthermore, in the benchmark where the approaches are compared in terms of their adaptability to different environments, the approaches are trained on the Cityscapes dataset and finetuned on the KITTI dataset. In this benchmark, the proposed method results in clearly better   [40]. The use of the KITTI dataset in the training is shown with the letter K, and the Cityscapes dataset [2] with CS. We report the error and accuracy values for the other methods for comparison taken from [17], [31], [39]. The best results are shown in bold. Garg et al. [42] report 50m cap and we list them in a separate row for comparison. Fig. 6: Normalized loss values of GANVO compared to the unsupervised methods SfM-Learner [17] and GeoNet [39] for multiple training experiments with various set of hyperparameters. GANVO is less sensitive to set of hyperparameters with lower mean and variance of the normalized loss values.\n\nerror and accuracy compared to the existing unsupervised methods. Moreover, GANVO obtains much closer results to Monodepth [31] which is supervised by left-right image consistency, i.e. pose. Figure 6 displays a robustness analysis of the proposed GANVO and the existing unsupervised approaches in terms of a parameter sensitivity analysis. We demonstrate that the proposed architecture is robust to hyper-parameter tunings such as different initialization of the network weights, different split of the dataset, and change of optimization parameters.\n\n\nVI. CONCLUSIONS\n\nIn this study, we proposed an unsupervised generative deep learning method for pose and depth map estimation for a monocular video sequences, demonstrating the effectiveness of adversarial learning in these tasks. The proposed method outperforms all the competing unsupervised and traditional baselines in terms of pose estimation, and captures more detailed, sharper and more accurate depth maps of the scenes. In a future work, we would like to explicitly address scene dynamics and investigate a learning representation for a full 3D volumetric modelling with semantic segmentation.\n\nFig. 3 :\n3Sample trajectories comparing the proposed unsupervised learning method GANVO with ORB SLAM, and the ground truth in meter scale. GANVO shows a better odometry estimation in terms of both rotational and translational motions.\n\nFig. 4 :\n4Comparison of unsupervised monocular depth estimation between SfM-Learner, GeoNet and the proposed GanVO. The groundtruth captured by Light Detection and Ranging (LiDAR) is interpolated for visualization purpose. GanVO captures details in challenging scenes containing low textured areas, shaded regions, and uneven road lines, preserving sharp, accurate and detailed depth map predictions both in close and distant regions.\n\nFig. 5 :\n5Typical failure cases of our model. Sometimes, all of the compared methods struggle in vast open rural scenes and huge objects occluding the camera view.\n\nTABLE I :\nIAbsolute Trajectory Error (ATE) on KITTI odom-\netry dataset. \n\n\nMethodSupervision Dataset Abs Rel Sq Rel RMSE RMSE log \u03b4 < 1.25 \u03b4 < 1.25 2 \u03b4 < 1.253 Eigen [40] Coarse \nDepth \nK \n0.214 \n1.605 \n6.563 \n0.292 \n0.673 \n0.884 \n0.957 \nEigen [40] Fine \nDepth \nK \n0.203 \n1.548 \n6.307 \n0.282 \n0.702 \n0.890 \n0.958 \nLiu [41] \nDepth \nK \n0.202 \n1.614 \n6.523 \n0.275 \n0.678 \n0.895 \n0.965 \nMonodepth [31] \nPose \nK \n0.148 \n1.344 \n5.927 \n0.247 \n0.803 \n0.922 \n0.964 \nSfM-Learner [17] \nNo \nK \n0.183 \n1.595 \n6.709 \n0.270 \n0.734 \n0.902 \n0.959 \nGeoNet [39] \nNo \nK \n0.155 \n1.296 \n5.857 \n0.233 \n0.793 \n0.931 \n0.973 \nGANVO \nNo \nK \n0.150 \n1.141 \n5.448 \n0.216 \n0.808 \n0.939 \n0.975 \nGarg et al. [42] cap 50m \nPose \nK \n0.169 \n1.080 \n5.104 \n0.273 \n0.740 \n0.904 \n0.962 \nSfM-Learner [17] cap 50m \nNo \nK \n0.201 \n1.391 \n5.181 \n0.264 \n0.696 \n0.900 \n0.966 \nGeoNet [39] cap 50m \nNo \nK \n0.147 \n0.936 \n4.348 \n0.218 \n0.810 \n0.941 \n0.977 \nGANVO cap 50m \nNo \nK \n0.137 \n0.892 \n3.671 \n0.201 \n0.867 \n0.970 \n0.983 \nMonodepth [31] \nPose \nCS + K \n0.124 \n1.076 \n5.311 \n0.219 \n0.847 \n0.942 \n0.973 \nSfM-Learner [17] \nNo \nCS + K \n0.198 \n1.836 \n6.565 \n0.275 \n0.718 \n0.901 \n0.960 \nGeoNet [39] \nNo \nCS + K \n0.153 \n1.328 \n5.737 \n0.232 \n0.802 \n0.934 \n0.972 \nGANVO \nNo \nCS + K \n0.138 \n1.155 \n4.412 \n0.232 \n0.820 \n0.939 \n0.976 \n\n\n\nTABLE II :\nIIMonocular depth estimation results on the KITTI dataset [1] by a benchmark split\nAcknowledgments: We would like to thank the authors referred in this study for sharing their code. This work is funded by the NIST grant 70NANB17H185. YA would like to thank the Ministry of National Education in Turkey for their funding and support.\nObject scene flow for autonomous vehicles. M Menze, A Geiger, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionM. Menze and A. Geiger, \"Object scene flow for autonomous vehi- cles,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 3061-3070.\n\nThe cityscapes dataset for semantic urban scene understanding. M Cordts, M Omran, S Ramos, T Rehfeld, M Enzweiler, R Benenson, U Franke, S Roth, B Schiele, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionM. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Be- nenson, U. Franke, S. Roth, and B. Schiele, \"The cityscapes dataset for semantic urban scene understanding,\" in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 3213-3223.\n\nLsd-slam: Large-scale direct monocular slam. J Engel, T Sch\u00f6ps, D Cremers, European Conference on Computer Vision. SpringerJ. Engel, T. Sch\u00f6ps, and D. Cremers, \"Lsd-slam: Large-scale di- rect monocular slam,\" in European Conference on Computer Vision. Springer, 2014, pp. 834-849.\n\nOrb-slam2: An open-source slam system for monocular, stereo, and rgb-d cameras. R Mur-Artal, J D Tard\u00f3s, IEEE Transactions on Robotics. 335R. Mur-Artal and J. D. Tard\u00f3s, \"Orb-slam2: An open-source slam system for monocular, stereo, and rgb-d cameras,\" IEEE Transactions on Robotics, vol. 33, no. 5, pp. 1255-1262, 2017.\n\nDeepvo: Towards endto-end visual odometry with deep recurrent convolutional neural networks. S Wang, R Clark, H Wen, N Trigoni, 2017 IEEE International Conference on. Robotics and Automation (ICRAS. Wang, R. Clark, H. Wen, and N. Trigoni, \"Deepvo: Towards end- to-end visual odometry with deep recurrent convolutional neural net- works,\" in Robotics and Automation (ICRA), 2017 IEEE International Conference on. IEEE, 2017, pp. 2043-2050.\n\nVinet: Visual-inertial odometry as a sequence-to-sequence learning problem. R Clark, S Wang, H Wen, A Markham, N Trigoni, AAAI. R. Clark, S. Wang, H. Wen, A. Markham, and N. Trigoni, \"Vinet: Visual-inertial odometry as a sequence-to-sequence learning problem.\" in AAAI, 2017, pp. 3995-4001.\n\nDeep endovo: A recurrent convolutional neural network (rcnn) based visual odometry approach for endoscopic capsule robots. M Turan, Y Almalioglu, H Araujo, E Konukoglu, M Sitti, Neurocomputing. 275M. Turan, Y. Almalioglu, H. Araujo, E. Konukoglu, and M. Sitti, \"Deep endovo: A recurrent convolutional neural network (rcnn) based visual odometry approach for endoscopic capsule robots,\" Neurocom- puting, vol. 275, pp. 1861-1870, 2018.\n\nA deep learning based 6 degree-of-freedom localization method for endoscopic capsule robots. M Turan, Y Almalioglu, E Konukoglu, M Sitti, arXiv:1705.05435arXiv preprintM. Turan, Y. Almalioglu, E. Konukoglu, and M. Sitti, \"A deep learning based 6 degree-of-freedom localization method for endoscopic capsule robots,\" arXiv preprint arXiv:1705.05435, 2017.\n\nFlowdometry: An optical flow and deep learning based approach to visual odometry. P Muller, A Savakis, Applications of Computer Vision (WACV. P. Muller and A. Savakis, \"Flowdometry: An optical flow and deep learning based approach to visual odometry,\" in Applications of Computer Vision (WACV), 2017 IEEE Winter Conference on. IEEE, 2017, pp. 624-631.\n\nUnsupervised neural machine translation. M Artetxe, G Labaka, E Agirre, K Cho, arXiv:1710.11041arXiv preprintM. Artetxe, G. Labaka, E. Agirre, and K. Cho, \"Unsupervised neural machine translation,\" arXiv preprint arXiv:1710.11041, 2017.\n\nUnsupervised odometry and depth learning for endoscopic capsule robots. M Turan, E P Ornek, N Ibrahimli, C Giracoglu, Y Almalioglu, M F Yanik, M Sitti, arXiv:1803.01047arXiv preprintM. Turan, E. P. Ornek, N. Ibrahimli, C. Giracoglu, Y. Almalioglu, M. F. Yanik, and M. Sitti, \"Unsupervised odometry and depth learning for endoscopic capsule robots,\" arXiv preprint arXiv:1803.01047, 2018.\n\nProspective representation of navigational goals in the human hippocampus. T I Brown, V A Carr, K F Larocque, S E Favila, A M Gordon, B Bowles, J N Bailenson, A D Wagner, Science. 3526291T. I. Brown, V. A. Carr, K. F. LaRocque, S. E. Favila, A. M. Gordon, B. Bowles, J. N. Bailenson, and A. D. Wagner, \"Prospective representation of navigational goals in the human hippocampus,\" Science, vol. 352, no. 6291, pp. 1323-1326, 2016.\n\nA brief nap is beneficial for human route-learning: The role of navigation experience and eeg spectral power. E J Wamsley, M A Tucker, J D Payne, R Stickgold, Learning & Memory. 177E. J. Wamsley, M. A. Tucker, J. D. Payne, and R. Stickgold, \"A brief nap is beneficial for human route-learning: The role of navigation experience and eeg spectral power,\" Learning & Memory, vol. 17, no. 7, pp. 332-336, 2010.\n\nGenerative adversarial nets. I Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, Y Bengio, Advances in neural information processing systems. I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, \"Generative adversarial nets,\" in Advances in neural information processing systems, 2014, pp. 2672- 2680.\n\nPrediction error as a quality metric for motion and stereo. R Szeliski, Computer Vision, 1999. The Proceedings of the Seventh IEEE International Conference on. IEEE2R. Szeliski, \"Prediction error as a quality metric for motion and stereo,\" in Computer Vision, 1999. The Proceedings of the Seventh IEEE International Conference on, vol. 2. IEEE, 1999, pp. 781-788.\n\nUndeepvo: Monocular visual odometry through unsupervised deep learning. R Li, S Wang, Z Long, D Gu, arXiv:1709.06841arXiv preprintR. Li, S. Wang, Z. Long, and D. Gu, \"Undeepvo: Monocular vi- sual odometry through unsupervised deep learning,\" arXiv preprint arXiv:1709.06841, 2017.\n\nUnsupervised learning of depth and ego-motion from video. T Zhou, M Brown, N Snavely, D G Lowe, CVPR. 27T. Zhou, M. Brown, N. Snavely, and D. G. Lowe, \"Unsupervised learning of depth and ego-motion from video,\" in CVPR, vol. 2, no. 6, 2017, p. 7.\n\nDeepstereo: Learning to predict new views from the world's imagery. J Flynn, I Neulander, J Philbin, N Snavely, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionJ. Flynn, I. Neulander, J. Philbin, and N. Snavely, \"Deepstereo: Learn- ing to predict new views from the world's imagery,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 5515-5524.\n\nDtam: Dense tracking and mapping in real-time. R A Newcombe, S J Lovegrove, A J Davison, Computer Vision (ICCV), 2011 IEEE International Conference on. IEEER. A. Newcombe, S. J. Lovegrove, and A. J. Davison, \"Dtam: Dense tracking and mapping in real-time,\" in Computer Vision (ICCV), 2011 IEEE International Conference on. IEEE, 2011, pp. 2320-2327.\n\nParallel tracking and mapping for small ar workspaces. G Klein, D Murray, 6th IEEE and ACM International Symposium on. IEEEMixed and Augmented RealityG. Klein and D. Murray, \"Parallel tracking and mapping for small ar workspaces,\" in Mixed and Augmented Reality, 2007. ISMAR 2007. 6th IEEE and ACM International Symposium on. IEEE, 2007, pp. 225-234.\n\nTowards internet-scale multi-view stereo. Y Furukawa, B Curless, S M Seitz, R Szeliski, Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on. IEEEY. Furukawa, B. Curless, S. M. Seitz, and R. Szeliski, \"Towards internet-scale multi-view stereo,\" in Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on. IEEE, 2010, pp. 1434-1441.\n\nSparse-then-dense alignment-based 3d map reconstruction method for endoscopic capsule robots. M Turan, Y Y Pilavci, I Ganiyusufoglu, H Araujo, E Konukoglu, M Sitti, Machine Vision and Applications. 292M. Turan, Y. Y. Pilavci, I. Ganiyusufoglu, H. Araujo, E. Konukoglu, and M. Sitti, \"Sparse-then-dense alignment-based 3d map reconstruc- tion method for endoscopic capsule robots,\" Machine Vision and Applications, vol. 29, no. 2, pp. 345-359, 2018.\n\nHierarchical model-based motion estimation. J R Bergen, P Anandan, K J Hanna, R Hingorani, SpringerJ. R. Bergen, P. Anandan, K. J. Hanna, and R. Hingorani, \"Hierarchical model-based motion estimation,\" in European conference on computer vision. Springer, 1992, pp. 237-252.\n\nEndosensorfusion: Particle filtering-based multi-sensory data fusion with switching state-space model for endoscopic capsule robots using recurrent neural network kinematics. M Turan, Y Almalioglu, H Araujo, T Cemgil, M Sitti, arXiv:1709.03401arXiv preprintM. Turan, Y. Almalioglu, H. Araujo, T. Cemgil, and M. Sitti, \"En- dosensorfusion: Particle filtering-based multi-sensory data fusion with switching state-space model for endoscopic capsule robots using re- current neural network kinematics,\" arXiv preprint arXiv:1709.03401, 2017.\n\nLearning monocular visual odometry with dense 3d mapping from dense 3d flow. C Zhao, L Sun, P Purkait, T Duckett, R Stolkin, arXiv:1803.02286arXiv preprintC. Zhao, L. Sun, P. Purkait, T. Duckett, and R. Stolkin, \"Learning monocular visual odometry with dense 3d mapping from dense 3d flow,\" arXiv preprint arXiv:1803.02286, 2018.\n\nVisual slam and structure from motion in dynamic environments: A survey. M R U Saputra, A Markham, N Trigoni, ACM Computing Surveys (CSUR). 51237M. R. U. Saputra, A. Markham, and N. Trigoni, \"Visual slam and structure from motion in dynamic environments: A survey,\" ACM Computing Surveys (CSUR), vol. 51, no. 2, p. 37, 2018.\n\nEndo-vmfusenet: deep visual-magnetic sensor fusion approach for uncalibrated, unsynchronized and asymmetric endoscopic capsule robot localization data. M Turan, Y Almalioglu, H Gilbert, A E Sari, U Soylu, M Sitti, arXiv:1709.06041arXiv preprintM. Turan, Y. Almalioglu, H. Gilbert, A. E. Sari, U. Soylu, and M. Sitti, \"Endo-vmfusenet: deep visual-magnetic sensor fusion ap- proach for uncalibrated, unsynchronized and asymmetric endoscopic capsule robot localization data,\" arXiv preprint arXiv:1709.06041, 2017.\n\nA deep learning based fusion of rgb camera information and magnetic localization information for endoscopic capsule robots. M Turan, J Shabbir, H Araujo, E Konukoglu, M Sitti, International journal of intelligent robotics and applications. 14M. Turan, J. Shabbir, H. Araujo, E. Konukoglu, and M. Sitti, \"A deep learning based fusion of rgb camera information and magnetic localization information for endoscopic capsule robots,\" International journal of intelligent robotics and applications, vol. 1, no. 4, pp. 442- 450, 2017.\n\nMagnetic-visual sensor fusion-based dense 3d reconstruction and localization for endoscopic capsule robots. M Turan, Y Almalioglu, E P Ornek, H Araujo, M F Yanik, M Sitti, arXiv:1803.01048arXiv preprintM. Turan, Y. Almalioglu, E. P. Ornek, H. Araujo, M. F. Yanik, and M. Sitti, \"Magnetic-visual sensor fusion-based dense 3d reconstruc- tion and localization for endoscopic capsule robots,\" arXiv preprint arXiv:1803.01048, 2018.\n\nUnsupervised feature learning for rgb-d based object recognition. L Bo, X Ren, D Fox, Experimental Robotics. SpringerL. Bo, X. Ren, and D. Fox, \"Unsupervised feature learning for rgb-d based object recognition,\" in Experimental Robotics. Springer, 2013, pp. 387-402.\n\nUnsupervised monocular depth estimation with left-right consistency. C Godard, O Mac Aodha, G J Brostow, CVPR. 27C. Godard, O. Mac Aodha, and G. J. Brostow, \"Unsupervised monoc- ular depth estimation with left-right consistency,\" in CVPR, vol. 2, no. 6, 2017, p. 7.\n\nGenerating images with perceptual similarity metrics based on deep networks. A Dosovitskiy, T Brox, Advances in Neural Information Processing Systems. A. Dosovitskiy and T. Brox, \"Generating images with perceptual similarity metrics based on deep networks,\" in Advances in Neural Information Processing Systems, 2016, pp. 658-666.\n\nDepth-image-based rendering (dibr), compression, and transmission for a new approach on 3d-tv. C Fehn, Stereoscopic Displays and Virtual Reality Systems XI. 5291C. Fehn, \"Depth-image-based rendering (dibr), compression, and transmission for a new approach on 3d-tv,\" in Stereoscopic Displays and Virtual Reality Systems XI, vol. 5291. International Society for Optics and Photonics, 2004, pp. 93-105.\n\nView synthesis by appearance flow. T Zhou, S Tulsiani, W Sun, J Malik, A A Efros, SpringerT. Zhou, S. Tulsiani, W. Sun, J. Malik, and A. A. Efros, \"View synthesis by appearance flow,\" in European conference on computer vision. Springer, 2016, pp. 286-301.\n\nUnsupervised representation learning with deep convolutional generative adversarial networks. A Radford, L Metz, S Chintala, arXiv:1511.06434arXiv preprintA. Radford, L. Metz, and S. Chintala, \"Unsupervised representation learning with deep convolutional generative adversarial networks,\" arXiv preprint arXiv:1511.06434, 2015.\n\nTensorflow: a system for large-scale machine learning. M Abadi, P Barham, J Chen, Z Chen, A Davis, J Dean, M Devin, S Ghemawat, G Irving, M Isard, OSDI. 16M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard et al., \"Tensorflow: a system for large-scale machine learning.\" in OSDI, vol. 16, 2016, pp. 265-283.\n\nAre we ready for autonomous driving? the kitti vision benchmark suite. A Geiger, P Lenz, R Urtasun, Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. IEEEA. Geiger, P. Lenz, and R. Urtasun, \"Are we ready for autonomous driving? the kitti vision benchmark suite,\" in Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. IEEE, 2012, pp. 3354-3361.\n\nOrb-slam: a versatile and accurate monocular slam system. R Mur-Artal, J M M Montiel, J D Tardos, IEEE Transactions on Robotics. 315R. Mur-Artal, J. M. M. Montiel, and J. D. Tardos, \"Orb-slam: a versatile and accurate monocular slam system,\" IEEE Transactions on Robotics, vol. 31, no. 5, pp. 1147-1163, 2015.\n\nGeonet: Unsupervised learning of dense depth, optical flow and camera pose. Z Yin, J Shi, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2Z. Yin and J. Shi, \"Geonet: Unsupervised learning of dense depth, optical flow and camera pose,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), vol. 2, 2018.\n\nDepth map prediction from a single image using a multi-scale deep network. D Eigen, C Puhrsch, R Fergus, Advances in neural information processing systems. D. Eigen, C. Puhrsch, and R. Fergus, \"Depth map prediction from a single image using a multi-scale deep network,\" in Advances in neural information processing systems, 2014, pp. 2366-2374.\n\nLearning depth from single monocular images using deep convolutional neural fields. F Liu, C Shen, G Lin, I D Reid, IEEE Trans. Pattern Anal. Mach. Intell. 3810F. Liu, C. Shen, G. Lin, and I. D. Reid, \"Learning depth from single monocular images using deep convolutional neural fields.\" IEEE Trans. Pattern Anal. Mach. Intell., vol. 38, no. 10, pp. 2024-2039, 2016.\n\nUnsupervised cnn for single view depth estimation: Geometry to the rescue. R Garg, V K Bg, G Carneiro, I Reid, European Conference on Computer Vision. SpringerR. Garg, V. K. BG, G. Carneiro, and I. Reid, \"Unsupervised cnn for single view depth estimation: Geometry to the rescue,\" in European Conference on Computer Vision. Springer, 2016, pp. 740-756.\n", "annotations": {"author": "[{\"end\":129,\"start\":112},{\"end\":146,\"start\":130},{\"end\":155,\"start\":147},{\"end\":176,\"start\":156},{\"end\":192,\"start\":177},{\"end\":206,\"start\":193}]", "publisher": null, "author_last_name": "[{\"end\":128,\"start\":118},{\"end\":145,\"start\":138},{\"end\":154,\"start\":147},{\"end\":175,\"start\":166},{\"end\":191,\"start\":184},{\"end\":205,\"start\":198}]", "author_first_name": "[{\"end\":117,\"start\":112},{\"end\":135,\"start\":130},{\"end\":137,\"start\":136},{\"end\":161,\"start\":156},{\"end\":165,\"start\":162},{\"end\":183,\"start\":177},{\"end\":197,\"start\":193}]", "author_affiliation": null, "title": "[{\"end\":109,\"start\":1},{\"end\":315,\"start\":207}]", "venue": null, "abstract": "[{\"end\":1468,\"start\":317}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1923,\"start\":1920},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1928,\"start\":1925},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2161,\"start\":2158},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2166,\"start\":2163},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2445,\"start\":2442},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2450,\"start\":2447},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2455,\"start\":2452},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2607,\"start\":2603},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2613,\"start\":2609},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3440,\"start\":3436},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3446,\"start\":3442},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3976,\"start\":3972},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4347,\"start\":4343},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4353,\"start\":4349},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5228,\"start\":5225},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5247,\"start\":5244},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6221,\"start\":6217},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6227,\"start\":6223},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6706,\"start\":6703},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6711,\"start\":6708},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6717,\"start\":6713},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":6723,\"start\":6719},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6880,\"start\":6876},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7071,\"start\":7068},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7346,\"start\":7342},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7352,\"start\":7348},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7358,\"start\":7354},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7544,\"start\":7540},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7799,\"start\":7796},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7804,\"start\":7801},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":12294,\"start\":12290},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":12722,\"start\":12718},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13609,\"start\":13605},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":14121,\"start\":14117},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":14493,\"start\":14489},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":15045,\"start\":15041},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":15093,\"start\":15090},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":16127,\"start\":16123},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":16298,\"start\":16294},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":16304,\"start\":16300},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":16819,\"start\":16815},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":17852,\"start\":17848},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":17892,\"start\":17888},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":18046,\"start\":18042},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":18052,\"start\":18048},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":18141,\"start\":18137},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":18157,\"start\":18153},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":18517,\"start\":18513},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":18957,\"start\":18953},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":19530,\"start\":19526},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":19536,\"start\":19532},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":19542,\"start\":19538},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":19575,\"start\":19571},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":19581,\"start\":19577},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":19865,\"start\":19861},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":19970,\"start\":19967},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":20072,\"start\":20068},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":20078,\"start\":20074},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":20084,\"start\":20080},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":20138,\"start\":20134},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":20298,\"start\":20294},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":20314,\"start\":20310},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":20624,\"start\":20620},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":22652,\"start\":22651}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":21890,\"start\":21654},{\"attributes\":{\"id\":\"fig_1\"},\"end\":22326,\"start\":21891},{\"attributes\":{\"id\":\"fig_2\"},\"end\":22491,\"start\":22327},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":22565,\"start\":22492},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":23771,\"start\":22566},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":23866,\"start\":23772}]", "paragraph": "[{\"end\":2614,\"start\":1487},{\"end\":4690,\"start\":2616},{\"end\":4756,\"start\":4692},{\"end\":5786,\"start\":4758},{\"end\":6543,\"start\":5807},{\"end\":7908,\"start\":6545},{\"end\":8766,\"start\":7939},{\"end\":9336,\"start\":8768},{\"end\":9559,\"start\":9419},{\"end\":10101,\"start\":9582},{\"end\":11227,\"start\":10123},{\"end\":11609,\"start\":11229},{\"end\":11959,\"start\":11636},{\"end\":12411,\"start\":11996},{\"end\":12476,\"start\":12413},{\"end\":12635,\"start\":12512},{\"end\":12793,\"start\":12637},{\"end\":12977,\"start\":12880},{\"end\":13353,\"start\":13003},{\"end\":13541,\"start\":13442},{\"end\":14198,\"start\":13573},{\"end\":14377,\"start\":14225},{\"end\":15363,\"start\":14408},{\"end\":16667,\"start\":15396},{\"end\":18102,\"start\":16703},{\"end\":19396,\"start\":18104},{\"end\":20495,\"start\":19398},{\"end\":21048,\"start\":20497},{\"end\":21653,\"start\":21068}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11995,\"start\":11960},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12511,\"start\":12477},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12879,\"start\":12794},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13441,\"start\":13354},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14224,\"start\":14199}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":16325,\"start\":16318},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":19418,\"start\":19410}]", "section_header": "[{\"end\":1485,\"start\":1470},{\"end\":5805,\"start\":5789},{\"end\":7937,\"start\":7911},{\"end\":9417,\"start\":9339},{\"end\":9580,\"start\":9562},{\"end\":10121,\"start\":10104},{\"end\":11634,\"start\":11612},{\"end\":13001,\"start\":12980},{\"end\":13571,\"start\":13544},{\"end\":14406,\"start\":14380},{\"end\":15394,\"start\":15366},{\"end\":16701,\"start\":16670},{\"end\":21066,\"start\":21051},{\"end\":21663,\"start\":21655},{\"end\":21900,\"start\":21892},{\"end\":22336,\"start\":22328},{\"end\":22502,\"start\":22493},{\"end\":23783,\"start\":23773}]", "table": "[{\"end\":22565,\"start\":22504},{\"end\":23771,\"start\":22653}]", "figure_caption": "[{\"end\":21890,\"start\":21665},{\"end\":22326,\"start\":21902},{\"end\":22491,\"start\":22338},{\"end\":22653,\"start\":22568},{\"end\":23866,\"start\":23786}]", "figure_ref": "[{\"end\":2732,\"start\":2726},{\"end\":7957,\"start\":7951},{\"end\":9485,\"start\":9479},{\"end\":10171,\"start\":10165},{\"end\":10363,\"start\":10357},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":16336,\"start\":16330},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":17731,\"start\":17723},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18557,\"start\":18551},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":19115,\"start\":19109},{\"end\":20211,\"start\":20205},{\"end\":20697,\"start\":20689}]", "bib_author_first_name": "[{\"end\":24161,\"start\":24160},{\"end\":24170,\"start\":24169},{\"end\":24559,\"start\":24558},{\"end\":24569,\"start\":24568},{\"end\":24578,\"start\":24577},{\"end\":24587,\"start\":24586},{\"end\":24598,\"start\":24597},{\"end\":24611,\"start\":24610},{\"end\":24623,\"start\":24622},{\"end\":24633,\"start\":24632},{\"end\":24641,\"start\":24640},{\"end\":25115,\"start\":25114},{\"end\":25124,\"start\":25123},{\"end\":25134,\"start\":25133},{\"end\":25432,\"start\":25431},{\"end\":25445,\"start\":25444},{\"end\":25447,\"start\":25446},{\"end\":25766,\"start\":25765},{\"end\":25774,\"start\":25773},{\"end\":25783,\"start\":25782},{\"end\":25790,\"start\":25789},{\"end\":26189,\"start\":26188},{\"end\":26198,\"start\":26197},{\"end\":26206,\"start\":26205},{\"end\":26213,\"start\":26212},{\"end\":26224,\"start\":26223},{\"end\":26528,\"start\":26527},{\"end\":26537,\"start\":26536},{\"end\":26551,\"start\":26550},{\"end\":26561,\"start\":26560},{\"end\":26574,\"start\":26573},{\"end\":26934,\"start\":26933},{\"end\":26943,\"start\":26942},{\"end\":26957,\"start\":26956},{\"end\":26970,\"start\":26969},{\"end\":27279,\"start\":27278},{\"end\":27289,\"start\":27288},{\"end\":27591,\"start\":27590},{\"end\":27602,\"start\":27601},{\"end\":27612,\"start\":27611},{\"end\":27622,\"start\":27621},{\"end\":27860,\"start\":27859},{\"end\":27869,\"start\":27868},{\"end\":27871,\"start\":27870},{\"end\":27880,\"start\":27879},{\"end\":27893,\"start\":27892},{\"end\":27906,\"start\":27905},{\"end\":27920,\"start\":27919},{\"end\":27922,\"start\":27921},{\"end\":27931,\"start\":27930},{\"end\":28252,\"start\":28251},{\"end\":28254,\"start\":28253},{\"end\":28263,\"start\":28262},{\"end\":28265,\"start\":28264},{\"end\":28273,\"start\":28272},{\"end\":28275,\"start\":28274},{\"end\":28287,\"start\":28286},{\"end\":28289,\"start\":28288},{\"end\":28299,\"start\":28298},{\"end\":28301,\"start\":28300},{\"end\":28311,\"start\":28310},{\"end\":28321,\"start\":28320},{\"end\":28323,\"start\":28322},{\"end\":28336,\"start\":28335},{\"end\":28338,\"start\":28337},{\"end\":28717,\"start\":28716},{\"end\":28719,\"start\":28718},{\"end\":28730,\"start\":28729},{\"end\":28732,\"start\":28731},{\"end\":28742,\"start\":28741},{\"end\":28744,\"start\":28743},{\"end\":28753,\"start\":28752},{\"end\":29044,\"start\":29043},{\"end\":29058,\"start\":29057},{\"end\":29075,\"start\":29074},{\"end\":29084,\"start\":29083},{\"end\":29090,\"start\":29089},{\"end\":29106,\"start\":29105},{\"end\":29115,\"start\":29114},{\"end\":29128,\"start\":29127},{\"end\":29463,\"start\":29462},{\"end\":29840,\"start\":29839},{\"end\":29846,\"start\":29845},{\"end\":29854,\"start\":29853},{\"end\":29862,\"start\":29861},{\"end\":30108,\"start\":30107},{\"end\":30116,\"start\":30115},{\"end\":30125,\"start\":30124},{\"end\":30136,\"start\":30135},{\"end\":30138,\"start\":30137},{\"end\":30366,\"start\":30365},{\"end\":30375,\"start\":30374},{\"end\":30388,\"start\":30387},{\"end\":30399,\"start\":30398},{\"end\":30826,\"start\":30825},{\"end\":30828,\"start\":30827},{\"end\":30840,\"start\":30839},{\"end\":30842,\"start\":30841},{\"end\":30855,\"start\":30854},{\"end\":30857,\"start\":30856},{\"end\":31185,\"start\":31184},{\"end\":31194,\"start\":31193},{\"end\":31524,\"start\":31523},{\"end\":31536,\"start\":31535},{\"end\":31547,\"start\":31546},{\"end\":31549,\"start\":31548},{\"end\":31558,\"start\":31557},{\"end\":31944,\"start\":31943},{\"end\":31953,\"start\":31952},{\"end\":31955,\"start\":31954},{\"end\":31966,\"start\":31965},{\"end\":31983,\"start\":31982},{\"end\":31993,\"start\":31992},{\"end\":32006,\"start\":32005},{\"end\":32344,\"start\":32343},{\"end\":32346,\"start\":32345},{\"end\":32356,\"start\":32355},{\"end\":32367,\"start\":32366},{\"end\":32369,\"start\":32368},{\"end\":32378,\"start\":32377},{\"end\":32750,\"start\":32749},{\"end\":32759,\"start\":32758},{\"end\":32773,\"start\":32772},{\"end\":32783,\"start\":32782},{\"end\":32793,\"start\":32792},{\"end\":33191,\"start\":33190},{\"end\":33199,\"start\":33198},{\"end\":33206,\"start\":33205},{\"end\":33217,\"start\":33216},{\"end\":33228,\"start\":33227},{\"end\":33518,\"start\":33517},{\"end\":33522,\"start\":33519},{\"end\":33533,\"start\":33532},{\"end\":33544,\"start\":33543},{\"end\":33923,\"start\":33922},{\"end\":33932,\"start\":33931},{\"end\":33946,\"start\":33945},{\"end\":33957,\"start\":33956},{\"end\":33959,\"start\":33958},{\"end\":33967,\"start\":33966},{\"end\":33976,\"start\":33975},{\"end\":34408,\"start\":34407},{\"end\":34417,\"start\":34416},{\"end\":34428,\"start\":34427},{\"end\":34438,\"start\":34437},{\"end\":34451,\"start\":34450},{\"end\":34921,\"start\":34920},{\"end\":34930,\"start\":34929},{\"end\":34944,\"start\":34943},{\"end\":34946,\"start\":34945},{\"end\":34955,\"start\":34954},{\"end\":34965,\"start\":34964},{\"end\":34967,\"start\":34966},{\"end\":34976,\"start\":34975},{\"end\":35309,\"start\":35308},{\"end\":35315,\"start\":35314},{\"end\":35322,\"start\":35321},{\"end\":35580,\"start\":35579},{\"end\":35590,\"start\":35589},{\"end\":35594,\"start\":35591},{\"end\":35603,\"start\":35602},{\"end\":35605,\"start\":35604},{\"end\":35855,\"start\":35854},{\"end\":35870,\"start\":35869},{\"end\":36205,\"start\":36204},{\"end\":36547,\"start\":36546},{\"end\":36555,\"start\":36554},{\"end\":36567,\"start\":36566},{\"end\":36574,\"start\":36573},{\"end\":36583,\"start\":36582},{\"end\":36585,\"start\":36584},{\"end\":36863,\"start\":36862},{\"end\":36874,\"start\":36873},{\"end\":36882,\"start\":36881},{\"end\":37153,\"start\":37152},{\"end\":37162,\"start\":37161},{\"end\":37172,\"start\":37171},{\"end\":37180,\"start\":37179},{\"end\":37188,\"start\":37187},{\"end\":37197,\"start\":37196},{\"end\":37205,\"start\":37204},{\"end\":37214,\"start\":37213},{\"end\":37226,\"start\":37225},{\"end\":37236,\"start\":37235},{\"end\":37528,\"start\":37527},{\"end\":37538,\"start\":37537},{\"end\":37546,\"start\":37545},{\"end\":37905,\"start\":37904},{\"end\":37918,\"start\":37917},{\"end\":37922,\"start\":37919},{\"end\":37933,\"start\":37932},{\"end\":37935,\"start\":37934},{\"end\":38234,\"start\":38233},{\"end\":38241,\"start\":38240},{\"end\":38680,\"start\":38679},{\"end\":38689,\"start\":38688},{\"end\":38700,\"start\":38699},{\"end\":39035,\"start\":39034},{\"end\":39042,\"start\":39041},{\"end\":39050,\"start\":39049},{\"end\":39057,\"start\":39056},{\"end\":39059,\"start\":39058},{\"end\":39393,\"start\":39392},{\"end\":39401,\"start\":39400},{\"end\":39403,\"start\":39402},{\"end\":39409,\"start\":39408},{\"end\":39421,\"start\":39420}]", "bib_author_last_name": "[{\"end\":24167,\"start\":24162},{\"end\":24177,\"start\":24171},{\"end\":24566,\"start\":24560},{\"end\":24575,\"start\":24570},{\"end\":24584,\"start\":24579},{\"end\":24595,\"start\":24588},{\"end\":24608,\"start\":24599},{\"end\":24620,\"start\":24612},{\"end\":24630,\"start\":24624},{\"end\":24638,\"start\":24634},{\"end\":24649,\"start\":24642},{\"end\":25121,\"start\":25116},{\"end\":25131,\"start\":25125},{\"end\":25142,\"start\":25135},{\"end\":25442,\"start\":25433},{\"end\":25454,\"start\":25448},{\"end\":25771,\"start\":25767},{\"end\":25780,\"start\":25775},{\"end\":25787,\"start\":25784},{\"end\":25798,\"start\":25791},{\"end\":26195,\"start\":26190},{\"end\":26203,\"start\":26199},{\"end\":26210,\"start\":26207},{\"end\":26221,\"start\":26214},{\"end\":26232,\"start\":26225},{\"end\":26534,\"start\":26529},{\"end\":26548,\"start\":26538},{\"end\":26558,\"start\":26552},{\"end\":26571,\"start\":26562},{\"end\":26580,\"start\":26575},{\"end\":26940,\"start\":26935},{\"end\":26954,\"start\":26944},{\"end\":26967,\"start\":26958},{\"end\":26976,\"start\":26971},{\"end\":27286,\"start\":27280},{\"end\":27297,\"start\":27290},{\"end\":27599,\"start\":27592},{\"end\":27609,\"start\":27603},{\"end\":27619,\"start\":27613},{\"end\":27626,\"start\":27623},{\"end\":27866,\"start\":27861},{\"end\":27877,\"start\":27872},{\"end\":27890,\"start\":27881},{\"end\":27903,\"start\":27894},{\"end\":27917,\"start\":27907},{\"end\":27928,\"start\":27923},{\"end\":27937,\"start\":27932},{\"end\":28260,\"start\":28255},{\"end\":28270,\"start\":28266},{\"end\":28284,\"start\":28276},{\"end\":28296,\"start\":28290},{\"end\":28308,\"start\":28302},{\"end\":28318,\"start\":28312},{\"end\":28333,\"start\":28324},{\"end\":28345,\"start\":28339},{\"end\":28727,\"start\":28720},{\"end\":28739,\"start\":28733},{\"end\":28750,\"start\":28745},{\"end\":28763,\"start\":28754},{\"end\":29055,\"start\":29045},{\"end\":29072,\"start\":29059},{\"end\":29081,\"start\":29076},{\"end\":29087,\"start\":29085},{\"end\":29103,\"start\":29091},{\"end\":29112,\"start\":29107},{\"end\":29125,\"start\":29116},{\"end\":29135,\"start\":29129},{\"end\":29472,\"start\":29464},{\"end\":29843,\"start\":29841},{\"end\":29851,\"start\":29847},{\"end\":29859,\"start\":29855},{\"end\":29865,\"start\":29863},{\"end\":30113,\"start\":30109},{\"end\":30122,\"start\":30117},{\"end\":30133,\"start\":30126},{\"end\":30143,\"start\":30139},{\"end\":30372,\"start\":30367},{\"end\":30385,\"start\":30376},{\"end\":30396,\"start\":30389},{\"end\":30407,\"start\":30400},{\"end\":30837,\"start\":30829},{\"end\":30852,\"start\":30843},{\"end\":30865,\"start\":30858},{\"end\":31191,\"start\":31186},{\"end\":31201,\"start\":31195},{\"end\":31533,\"start\":31525},{\"end\":31544,\"start\":31537},{\"end\":31555,\"start\":31550},{\"end\":31567,\"start\":31559},{\"end\":31950,\"start\":31945},{\"end\":31963,\"start\":31956},{\"end\":31980,\"start\":31967},{\"end\":31990,\"start\":31984},{\"end\":32003,\"start\":31994},{\"end\":32012,\"start\":32007},{\"end\":32353,\"start\":32347},{\"end\":32364,\"start\":32357},{\"end\":32375,\"start\":32370},{\"end\":32388,\"start\":32379},{\"end\":32756,\"start\":32751},{\"end\":32770,\"start\":32760},{\"end\":32780,\"start\":32774},{\"end\":32790,\"start\":32784},{\"end\":32799,\"start\":32794},{\"end\":33196,\"start\":33192},{\"end\":33203,\"start\":33200},{\"end\":33214,\"start\":33207},{\"end\":33225,\"start\":33218},{\"end\":33236,\"start\":33229},{\"end\":33530,\"start\":33523},{\"end\":33541,\"start\":33534},{\"end\":33552,\"start\":33545},{\"end\":33929,\"start\":33924},{\"end\":33943,\"start\":33933},{\"end\":33954,\"start\":33947},{\"end\":33964,\"start\":33960},{\"end\":33973,\"start\":33968},{\"end\":33982,\"start\":33977},{\"end\":34414,\"start\":34409},{\"end\":34425,\"start\":34418},{\"end\":34435,\"start\":34429},{\"end\":34448,\"start\":34439},{\"end\":34457,\"start\":34452},{\"end\":34927,\"start\":34922},{\"end\":34941,\"start\":34931},{\"end\":34952,\"start\":34947},{\"end\":34962,\"start\":34956},{\"end\":34973,\"start\":34968},{\"end\":34982,\"start\":34977},{\"end\":35312,\"start\":35310},{\"end\":35319,\"start\":35316},{\"end\":35326,\"start\":35323},{\"end\":35587,\"start\":35581},{\"end\":35600,\"start\":35595},{\"end\":35613,\"start\":35606},{\"end\":35867,\"start\":35856},{\"end\":35875,\"start\":35871},{\"end\":36210,\"start\":36206},{\"end\":36552,\"start\":36548},{\"end\":36564,\"start\":36556},{\"end\":36571,\"start\":36568},{\"end\":36580,\"start\":36575},{\"end\":36591,\"start\":36586},{\"end\":36871,\"start\":36864},{\"end\":36879,\"start\":36875},{\"end\":36891,\"start\":36883},{\"end\":37159,\"start\":37154},{\"end\":37169,\"start\":37163},{\"end\":37177,\"start\":37173},{\"end\":37185,\"start\":37181},{\"end\":37194,\"start\":37189},{\"end\":37202,\"start\":37198},{\"end\":37211,\"start\":37206},{\"end\":37223,\"start\":37215},{\"end\":37233,\"start\":37227},{\"end\":37242,\"start\":37237},{\"end\":37535,\"start\":37529},{\"end\":37543,\"start\":37539},{\"end\":37554,\"start\":37547},{\"end\":37915,\"start\":37906},{\"end\":37930,\"start\":37923},{\"end\":37942,\"start\":37936},{\"end\":38238,\"start\":38235},{\"end\":38245,\"start\":38242},{\"end\":38686,\"start\":38681},{\"end\":38697,\"start\":38690},{\"end\":38707,\"start\":38701},{\"end\":39039,\"start\":39036},{\"end\":39047,\"start\":39043},{\"end\":39054,\"start\":39051},{\"end\":39064,\"start\":39060},{\"end\":39398,\"start\":39394},{\"end\":39406,\"start\":39404},{\"end\":39418,\"start\":39410},{\"end\":39426,\"start\":39422}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":12986049},\"end\":24493,\"start\":24117},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":502946},\"end\":25067,\"start\":24495},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":14547347},\"end\":25349,\"start\":25069},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":206775640},\"end\":25670,\"start\":25351},{\"attributes\":{\"id\":\"b4\"},\"end\":26110,\"start\":25672},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":17112447},\"end\":26402,\"start\":26112},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":10416499},\"end\":26838,\"start\":26404},{\"attributes\":{\"doi\":\"arXiv:1705.05435\",\"id\":\"b7\"},\"end\":27194,\"start\":26840},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":206858678},\"end\":27547,\"start\":27196},{\"attributes\":{\"doi\":\"arXiv:1710.11041\",\"id\":\"b9\"},\"end\":27785,\"start\":27549},{\"attributes\":{\"doi\":\"arXiv:1803.01047\",\"id\":\"b10\"},\"end\":28174,\"start\":27787},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":15653975},\"end\":28604,\"start\":28176},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":13916589},\"end\":29012,\"start\":28606},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":1033682},\"end\":29400,\"start\":29014},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":1849941},\"end\":29765,\"start\":29402},{\"attributes\":{\"doi\":\"arXiv:1709.06841\",\"id\":\"b15\"},\"end\":30047,\"start\":29767},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":11977588},\"end\":30295,\"start\":30049},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":14517241},\"end\":30776,\"start\":30297},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":1336659},\"end\":31127,\"start\":30778},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":206986664},\"end\":31479,\"start\":31129},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":2499865},\"end\":31847,\"start\":31481},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":20111669},\"end\":32297,\"start\":31849},{\"attributes\":{\"id\":\"b22\"},\"end\":32572,\"start\":32299},{\"attributes\":{\"doi\":\"arXiv:1709.03401\",\"id\":\"b23\"},\"end\":33111,\"start\":32574},{\"attributes\":{\"doi\":\"arXiv:1803.02286\",\"id\":\"b24\"},\"end\":33442,\"start\":33113},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":3400843},\"end\":33768,\"start\":33444},{\"attributes\":{\"doi\":\"arXiv:1709.06041\",\"id\":\"b26\"},\"end\":34281,\"start\":33770},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":7378610},\"end\":34810,\"start\":34283},{\"attributes\":{\"doi\":\"arXiv:1803.01048\",\"id\":\"b28\"},\"end\":35240,\"start\":34812},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":1053307},\"end\":35508,\"start\":35242},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":206596513},\"end\":35775,\"start\":35510},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":8758543},\"end\":36107,\"start\":35777},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":62566129},\"end\":36509,\"start\":36109},{\"attributes\":{\"id\":\"b33\"},\"end\":36766,\"start\":36511},{\"attributes\":{\"doi\":\"arXiv:1511.06434\",\"id\":\"b34\"},\"end\":37095,\"start\":36768},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":6287870},\"end\":37454,\"start\":37097},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":6724907},\"end\":37844,\"start\":37456},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":206775100},\"end\":38155,\"start\":37846},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":3714620},\"end\":38602,\"start\":38157},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":2255738},\"end\":38948,\"start\":38604},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":15774646},\"end\":39315,\"start\":38950},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":299085},\"end\":39669,\"start\":39317}]", "bib_title": "[{\"end\":24158,\"start\":24117},{\"end\":24556,\"start\":24495},{\"end\":25112,\"start\":25069},{\"end\":25429,\"start\":25351},{\"end\":25763,\"start\":25672},{\"end\":26186,\"start\":26112},{\"end\":26525,\"start\":26404},{\"end\":27276,\"start\":27196},{\"end\":28249,\"start\":28176},{\"end\":28714,\"start\":28606},{\"end\":29041,\"start\":29014},{\"end\":29460,\"start\":29402},{\"end\":30105,\"start\":30049},{\"end\":30363,\"start\":30297},{\"end\":30823,\"start\":30778},{\"end\":31182,\"start\":31129},{\"end\":31521,\"start\":31481},{\"end\":31941,\"start\":31849},{\"end\":33515,\"start\":33444},{\"end\":34405,\"start\":34283},{\"end\":35306,\"start\":35242},{\"end\":35577,\"start\":35510},{\"end\":35852,\"start\":35777},{\"end\":36202,\"start\":36109},{\"end\":37150,\"start\":37097},{\"end\":37525,\"start\":37456},{\"end\":37902,\"start\":37846},{\"end\":38231,\"start\":38157},{\"end\":38677,\"start\":38604},{\"end\":39032,\"start\":38950},{\"end\":39390,\"start\":39317}]", "bib_author": "[{\"end\":24169,\"start\":24160},{\"end\":24179,\"start\":24169},{\"end\":24568,\"start\":24558},{\"end\":24577,\"start\":24568},{\"end\":24586,\"start\":24577},{\"end\":24597,\"start\":24586},{\"end\":24610,\"start\":24597},{\"end\":24622,\"start\":24610},{\"end\":24632,\"start\":24622},{\"end\":24640,\"start\":24632},{\"end\":24651,\"start\":24640},{\"end\":25123,\"start\":25114},{\"end\":25133,\"start\":25123},{\"end\":25144,\"start\":25133},{\"end\":25444,\"start\":25431},{\"end\":25456,\"start\":25444},{\"end\":25773,\"start\":25765},{\"end\":25782,\"start\":25773},{\"end\":25789,\"start\":25782},{\"end\":25800,\"start\":25789},{\"end\":26197,\"start\":26188},{\"end\":26205,\"start\":26197},{\"end\":26212,\"start\":26205},{\"end\":26223,\"start\":26212},{\"end\":26234,\"start\":26223},{\"end\":26536,\"start\":26527},{\"end\":26550,\"start\":26536},{\"end\":26560,\"start\":26550},{\"end\":26573,\"start\":26560},{\"end\":26582,\"start\":26573},{\"end\":26942,\"start\":26933},{\"end\":26956,\"start\":26942},{\"end\":26969,\"start\":26956},{\"end\":26978,\"start\":26969},{\"end\":27288,\"start\":27278},{\"end\":27299,\"start\":27288},{\"end\":27601,\"start\":27590},{\"end\":27611,\"start\":27601},{\"end\":27621,\"start\":27611},{\"end\":27628,\"start\":27621},{\"end\":27868,\"start\":27859},{\"end\":27879,\"start\":27868},{\"end\":27892,\"start\":27879},{\"end\":27905,\"start\":27892},{\"end\":27919,\"start\":27905},{\"end\":27930,\"start\":27919},{\"end\":27939,\"start\":27930},{\"end\":28262,\"start\":28251},{\"end\":28272,\"start\":28262},{\"end\":28286,\"start\":28272},{\"end\":28298,\"start\":28286},{\"end\":28310,\"start\":28298},{\"end\":28320,\"start\":28310},{\"end\":28335,\"start\":28320},{\"end\":28347,\"start\":28335},{\"end\":28729,\"start\":28716},{\"end\":28741,\"start\":28729},{\"end\":28752,\"start\":28741},{\"end\":28765,\"start\":28752},{\"end\":29057,\"start\":29043},{\"end\":29074,\"start\":29057},{\"end\":29083,\"start\":29074},{\"end\":29089,\"start\":29083},{\"end\":29105,\"start\":29089},{\"end\":29114,\"start\":29105},{\"end\":29127,\"start\":29114},{\"end\":29137,\"start\":29127},{\"end\":29474,\"start\":29462},{\"end\":29845,\"start\":29839},{\"end\":29853,\"start\":29845},{\"end\":29861,\"start\":29853},{\"end\":29867,\"start\":29861},{\"end\":30115,\"start\":30107},{\"end\":30124,\"start\":30115},{\"end\":30135,\"start\":30124},{\"end\":30145,\"start\":30135},{\"end\":30374,\"start\":30365},{\"end\":30387,\"start\":30374},{\"end\":30398,\"start\":30387},{\"end\":30409,\"start\":30398},{\"end\":30839,\"start\":30825},{\"end\":30854,\"start\":30839},{\"end\":30867,\"start\":30854},{\"end\":31193,\"start\":31184},{\"end\":31203,\"start\":31193},{\"end\":31535,\"start\":31523},{\"end\":31546,\"start\":31535},{\"end\":31557,\"start\":31546},{\"end\":31569,\"start\":31557},{\"end\":31952,\"start\":31943},{\"end\":31965,\"start\":31952},{\"end\":31982,\"start\":31965},{\"end\":31992,\"start\":31982},{\"end\":32005,\"start\":31992},{\"end\":32014,\"start\":32005},{\"end\":32355,\"start\":32343},{\"end\":32366,\"start\":32355},{\"end\":32377,\"start\":32366},{\"end\":32390,\"start\":32377},{\"end\":32758,\"start\":32749},{\"end\":32772,\"start\":32758},{\"end\":32782,\"start\":32772},{\"end\":32792,\"start\":32782},{\"end\":32801,\"start\":32792},{\"end\":33198,\"start\":33190},{\"end\":33205,\"start\":33198},{\"end\":33216,\"start\":33205},{\"end\":33227,\"start\":33216},{\"end\":33238,\"start\":33227},{\"end\":33532,\"start\":33517},{\"end\":33543,\"start\":33532},{\"end\":33554,\"start\":33543},{\"end\":33931,\"start\":33922},{\"end\":33945,\"start\":33931},{\"end\":33956,\"start\":33945},{\"end\":33966,\"start\":33956},{\"end\":33975,\"start\":33966},{\"end\":33984,\"start\":33975},{\"end\":34416,\"start\":34407},{\"end\":34427,\"start\":34416},{\"end\":34437,\"start\":34427},{\"end\":34450,\"start\":34437},{\"end\":34459,\"start\":34450},{\"end\":34929,\"start\":34920},{\"end\":34943,\"start\":34929},{\"end\":34954,\"start\":34943},{\"end\":34964,\"start\":34954},{\"end\":34975,\"start\":34964},{\"end\":34984,\"start\":34975},{\"end\":35314,\"start\":35308},{\"end\":35321,\"start\":35314},{\"end\":35328,\"start\":35321},{\"end\":35589,\"start\":35579},{\"end\":35602,\"start\":35589},{\"end\":35615,\"start\":35602},{\"end\":35869,\"start\":35854},{\"end\":35877,\"start\":35869},{\"end\":36212,\"start\":36204},{\"end\":36554,\"start\":36546},{\"end\":36566,\"start\":36554},{\"end\":36573,\"start\":36566},{\"end\":36582,\"start\":36573},{\"end\":36593,\"start\":36582},{\"end\":36873,\"start\":36862},{\"end\":36881,\"start\":36873},{\"end\":36893,\"start\":36881},{\"end\":37161,\"start\":37152},{\"end\":37171,\"start\":37161},{\"end\":37179,\"start\":37171},{\"end\":37187,\"start\":37179},{\"end\":37196,\"start\":37187},{\"end\":37204,\"start\":37196},{\"end\":37213,\"start\":37204},{\"end\":37225,\"start\":37213},{\"end\":37235,\"start\":37225},{\"end\":37244,\"start\":37235},{\"end\":37537,\"start\":37527},{\"end\":37545,\"start\":37537},{\"end\":37556,\"start\":37545},{\"end\":37917,\"start\":37904},{\"end\":37932,\"start\":37917},{\"end\":37944,\"start\":37932},{\"end\":38240,\"start\":38233},{\"end\":38247,\"start\":38240},{\"end\":38688,\"start\":38679},{\"end\":38699,\"start\":38688},{\"end\":38709,\"start\":38699},{\"end\":39041,\"start\":39034},{\"end\":39049,\"start\":39041},{\"end\":39056,\"start\":39049},{\"end\":39066,\"start\":39056},{\"end\":39400,\"start\":39392},{\"end\":39408,\"start\":39400},{\"end\":39420,\"start\":39408},{\"end\":39428,\"start\":39420}]", "bib_venue": "[{\"end\":24320,\"start\":24258},{\"end\":24792,\"start\":24730},{\"end\":30550,\"start\":30488},{\"end\":38402,\"start\":38333},{\"end\":24256,\"start\":24179},{\"end\":24728,\"start\":24651},{\"end\":25182,\"start\":25144},{\"end\":25485,\"start\":25456},{\"end\":25837,\"start\":25800},{\"end\":26238,\"start\":26234},{\"end\":26596,\"start\":26582},{\"end\":26931,\"start\":26840},{\"end\":27336,\"start\":27299},{\"end\":27588,\"start\":27549},{\"end\":27857,\"start\":27787},{\"end\":28354,\"start\":28347},{\"end\":28782,\"start\":28765},{\"end\":29186,\"start\":29137},{\"end\":29560,\"start\":29474},{\"end\":29837,\"start\":29767},{\"end\":30149,\"start\":30145},{\"end\":30486,\"start\":30409},{\"end\":30928,\"start\":30867},{\"end\":31246,\"start\":31203},{\"end\":31640,\"start\":31569},{\"end\":32045,\"start\":32014},{\"end\":32341,\"start\":32299},{\"end\":32747,\"start\":32574},{\"end\":33188,\"start\":33113},{\"end\":33582,\"start\":33554},{\"end\":33920,\"start\":33770},{\"end\":34521,\"start\":34459},{\"end\":34918,\"start\":34812},{\"end\":35349,\"start\":35328},{\"end\":35619,\"start\":35615},{\"end\":35926,\"start\":35877},{\"end\":36264,\"start\":36212},{\"end\":36544,\"start\":36511},{\"end\":36860,\"start\":36768},{\"end\":37248,\"start\":37244},{\"end\":37627,\"start\":37556},{\"end\":37973,\"start\":37944},{\"end\":38331,\"start\":38247},{\"end\":38758,\"start\":38709},{\"end\":39104,\"start\":39066},{\"end\":39466,\"start\":39428}]"}}}, "year": 2023, "month": 12, "day": 17}