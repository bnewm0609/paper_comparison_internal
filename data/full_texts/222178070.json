{"id": 222178070, "updated": "2022-01-21 08:51:51.999", "metadata": {"title": "Nighttime Defogging Using High-Low Frequency Decomposition and Grayscale-Color Networks", "authors": "[{\"middle\":[],\"last\":\"Yan\",\"first\":\"Wending\"},{\"middle\":[\"T.\"],\"last\":\"Tan\",\"first\":\"Robby\"}]", "venue": "Computer Vision \u2013 ECCV 2020", "journal": "Computer Vision \u2013 ECCV 2020", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "We address the problem of nighttime defogging from a single image by introducing a framework consisting of two modules: grayscale and color modules. Given an RGB foggy nighttime image, our grayscale module takes the grayscale version of the image as input, and decomposes it into high and low frequency layers. The high frequency layers contain the scene texture information, which is less affected by fog. While the low frequency layers contain the scene layout/structure information including fog and glow. Our grayscale module then enhances the visibility of the textures in the high frequency layers, and removes the presence of glow and fog in the low frequency layers. Having processed the high/low frequency information, it fuses the two layers to obtain a grayscale defogged image. Our second module, the color module, takes the original RGB image, and process it similarly to what the grayscale module does. However, to obtain fog-free high and low frequency information, the module is guided by the grayscale module. The reason of doing this is because grayscale images are less affected by multiple colors of atmospheric light, which are commonly present in nighttime scenes. Moreover, having the grayscale module allows us to have consistency losses between the outputs of the two modules, which is critical to our framework, since we do not have paired ground-truths for our real data. Our extensive experiments on real foggy nighttime images show the effectiveness of our method.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "3091987649", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/eccv/YanTD20", "doi": "10.1007/978-3-030-58610-2_28"}}, "content": {"source": {"pdf_hash": "fb1b397d5d67ee6cf0c17413d1ba299647a0e3b6", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "4fb8082aa9d3464b84d74d908dca1e0cb301ad4a", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/fb1b397d5d67ee6cf0c17413d1ba299647a0e3b6.txt", "contents": "\nNighttime Defogging Using High-Low Frequency Decomposition and Grayscale-Color Networks\n\n\nWending Yan \nNational University of Singapore\n\n\nYale-NUS College\n\n\nETH Zurich\n\n\nNighttime Defogging Using High-Low Frequency Decomposition and Grayscale-Color Networks\n1[0000\u22120001\u22125993\u22128405] , Robby T. Tan 1,2[0000\u22120001\u22127532\u22126919] , and Dengxin Dai 3[0000\u22120001\u22125440\u22129678]\nWe address the problem of nighttime defogging from a single image by introducing a framework consisting of two modules: grayscale and color modules. Given an RGB foggy nighttime image, our grayscale module takes the grayscale version of the image as input, and decomposes it into high and low frequency layers. The high frequency layers contain the scene texture information, which is less affected by fog. While the low frequency layers contain the scene layout/structure information including fog and glow. Our grayscale module then enhances the visibility of the textures in the high frequency layers, and removes the presence of glow and fog in the low frequency layers. Having processed the high/low frequency information, it fuses the two layers to obtain a grayscale defogged image. Our second module, the color module, takes the original RGB image, and process it similarly to what the grayscale module does. However, to obtain fog-free high and low frequency information, the module is guided by the grayscale module. The reason of doing this is because grayscale images are less affected by multiple colors of atmospheric light, which are commonly present in nighttime scenes. Moreover, having the grayscale module allows us to have consistency losses between the outputs of the two modules, which is critical to our framework, since we do not have paired ground-truths for our real data. Our extensive experiments on real foggy nighttime images show the effectiveness of our method.\n\nIntroduction\n\nFog in nighttime can significantly degrades visibility. Few methods have been proposed to address this degradation problem, e.g.: [27,17,26]. Unfortunately, due to the complexity of illumination colors, the presence of multiple light sources and the strong glow, these methods still perform suboptimally as shown in Fig. 1. To our knowledge, there is no deep-learning based nighttime defogging method that has been proposed so far. The major reason is because collecting pairs of foggy nighttime images and their corresponding clear nighttime images, which are necessary for training a network, is significantly intractable. Rendering synthetic foggy nighttime images is a possible solution, however to render them Nighttime foggy Image\n\nOur Result\n\nLi et al. [17] Zhang et al. [26] [17]. Bottom right: Zhang et al.'s result [26]. These methods are the state of the art in nighttime defogging. Zoom-in for better visualization.\n\nphysically realistic for many different nighttime scenes and atmospheric conditions is challenging, since various information is required (such as depths, light source locations, particle distributions, how lights with different colors and intensities interact, etc). Unlike foggy daytime images, foggy nighttime images suffer from low light, strong noise, multiple light sources, non-uniform distribution of atmospheric light intensity, multiple colors of lights, strong glow, etc. Because of these problems, directly applying existing daytime defogging methods to foggy nighttime images will not be effective. One of the reasons is that most of the methods (e.g. [24,7,12,19,3,22]) assume a single atmospheric light (thus, a uniformly ambient light color), which is inapplicable to most of foggy nighttime images, since there are usually many man-made light sources with different colors in a single image.\n\nIn this paper, we introduce a framework that consists of grayscale and color modules. Given an RGB input image, the grayscale module takes the grayscale version of the input RGB image as input. The grayscale module decomposes the image into high and low frequency layers. These layers are then processed by two separate networks: a network for the high frequency layers that enhances the textures, and a network for the low frequency layers that removes glow and fog. Subsequently, fusing the processed high and low frequency information will produce a defogged grayscale image. Our second module, the color module, takes the original RGB image as input. The operations in the color module are similar to those in the grayscale module. Unlike the grayscale module, however, we train the module based on the outputs of the grayscale module. The reason of doing this is because we found defogging a grayscale image is more effective than defogging the RGB version of the image alone. This is mainly because, the grayscale image is less complicated by multiple colors of atmospheric light (see the detailed discussion in Sect. 4). Moreover, having the grayscale module, we can have consistency losses between the outputs of the two modules, which is critical to our framework, since we do not have paired ground-truths for our real data. To train our networks, we employ a few foggy-nighttime synthetic images, which come with ground-truths, and real foggy-nighttime images that have no paired ground-truths.\n\nAs a summary, our contributions are as follows:\n\n-We introduce a nighttime defogging method based on the grayscale and color modules. Our method works for both grayscale and RGB input image. -We propose the use of the low and high frequency layer decomposition.\n\nThe high frequency layer contains the scene texture information, which is less affected by fog and ambient light colors; while, the low frequency layer contains the scene structure information affected by fog and glow. -We introduce new consistency losses between the outputs of the grayscale and color modules, which are useful to strengthen the training process, particularly when we do not have paired ground-truths.\n\n\nRelated Work\n\nMany methods have been proposed to deal with fog or haze in daytime (e.g., [24,7,12,19,3,22,14,25,23,15,6]). Tan [24] proposes a method by maximizing contrast. Fattal [7] estimates the transmission map by assuming that it is independent on the surface shading. He et al. [12] propose with the dark channel prior. Meng et al. [19] predict the transmission map with minimum boundaries and contextual regularization. Berman et al. [3] present a haze-line prior. In the era of deep learning, Ren et al. [22] design a multi-scale network for estimating the transmission map. Li et al. [14] propose all-in-one network, which predicted defogged images without estimating the transmission map and atmospheric light. Li et al. [15] apply a conditional GAN and Engin et al. [6] applied a cycle GAN to their method. More detailed discussion can be found in [18]. However, all these existing methods do not have good performance in nighttime fog removal. They have problem in estimating the atmospheric light. Even CNN-based methods have not been shown to be able to handle nighttime complex ambient lights, due to the lack of paired ground-truths.\n\nThere are few methods that specifically address nighttime fog removal. Pei and Lee [20] transfer the colors of a foggy nighttime image into the foggy daytime style, and apply the dark channel prior. Zhang et al. [27] propose a method using varying illumination compensation and post-processing. This technique brings more realistic results, but also color artifacts. Li et al. [17] apply glow removal before fog removal. The glow is decomposed from input image by a layer separation method. Then, the deglowed image is further defogged using  the dark channel prior. Ancuti et al. [1] provide a multi-scale fusion approach to enhance the nighttime fog visibility. They compute the atmospheric light component on an image patch level and derive three components, which will be weighed and fused to obtain the final output. Zhang et al. [26] propose a prior named maximum reflectance prior. They claim that the ambient illumination can be estimated by this prior. Like most of the nighttime defogging methods, the method employs the dark prior channel.\n\n\nProposed Method\n\nFig . 2 shows the pipeline of our method, which consists of two modules: grayscale and color modules. Given an RGB input image, the grayscale module takes the grayscale version of the input rgb image as input. It then decomposes the input into high and low frequency layers. A network dedicated to process the high frequency layers boosts the texture information of the layers. Another network dedicated to process the low frequency layers removes glow and fog. From the low and high frequency layers, a weight map is computed to suppress the noise of the processed high frequency information. Finally, the grayscale defogged output is obtained by fusing the low/high frequency information. The color module does similar processes, and thus we can create losses that can keep the outputs of the grayscale and color modules to be consistent. Note that, for real training data, we do not have paired ground-truths. The details of the discussion on the two modules are as follows.\n\n\nGrayscale Module\n\nThere are two main reasons of having the grayscale module in our solution. First, in nighttime, ambient lights with multiple colors are common. This is due to the fact that there are many man-made light sources with various colors. Unlike nighttime colored images, nighttime grayscale images are less complicated by the colors of multiple light (more discussion in Sect. 4). Second, for real foggy nighttime images, we do not have their clean fog-free ground-truths. Hence, by having the grayscale module, we can have additional constraints by enforcing the consistency between the outputs of the grayscale and color modules.\n\n\nHigh/Low Frequency Layers and Networks\n\nThe grayscale module decomposes the input image into a few high and low frequency layers. Any state of the art decomposition techniques can be used (e.g. [11]). In our implementation, we produces 8 high frequency layers, and their 8 corresponding low frequency layers using different decomposition parameters. The reason of doing this is to reduce missing information possibly caused by inappropriate choice of parameters with respect to the input image. The high frequency layers contain the scene textures, and the low frequency layers contain the scene layout/structures. Due to their smoothness, fog and glow will go to the low frequency layers, and consequently the high frequency layers is not much affected by fog and glow (see Fig. 2). We separate the high and low frequency is because they represent different aspects of a foggy scene, and hence they should be processed differently. Otherwise, when removing fog, we may also remove genuine textures of the scene. After decomposition, we process the high frequency layers to boost the texture information. The main reason of why we want to boost the textures is because in foggy nighttime images, the contrast is low, and thus the textures can be weak. We design an autoencoder network to handle this task, and we call it high-frequency network. The input of this network is the concatenation of the high frequency layers. Like the high frequency part, for the low frequency layers, we create a low-frequency network to process them. Our intention of creating the network is to suppress the presence of fog and glow. We call the outputs of the high/low frequency networks high/low frequency maps, respectively.\n\nFusing High/Low Frequency Maps Having obtained the high/low frequency maps, we can directly fuse them. However, since boosting the textures in the high frequency layers also means boosting the noise, prior fusing the two maps, we compute a weight map. The weights in this map represents the noise levels of the high frequency map. Thus, multiplying the weight map with the high frequency map will reduce noise. Note that, foggy nighttime images suffer significantly from noise; this is because many parts of foggy nighttime images are regions with low light. We adopt an existing technique [16] to compute the weight map, where we first generate a coarse map from the discrete cosine transform (DCT) coefficients of the high frequency map. Larger DCT coefficients represent more texture details. Thus, we set the sum of coefficients as the coarse weight map. However, since the DCT is computed patch by patch, the weight map may looks coarse. As suggested in [16], we utilize the low frequency map to smoothen the weights. This refined weight map has high values on texture regions and low values on uniform regions like sky regions. Fig. 2, the top right image, shows a sample of our weight map.\n\nIn the grayscale module, we have a few losses: smoothness, pair and discriminative. The smoothness loss is applied only to the low frequency network; while the grayscale pair and discriminative losses are applied to the final defogged image. Hence, the backpropagated errors from the grayscale pair and discriminative losses will go to both the high and low frequency networks.\n\n\n-Grayscale Smoothness Loss\n\nThe loss is defined as:\nL gray smooth = E (J f gray \u2212 LF gray [J f gray ]) * f 2 ,(1)\nwhere LF [.] is the low frequency network, and J f gray is the grayscale foggy nighttime input (the superscript f stands for fog). f 2 is the second order Laplacian filter and * is the convolution operator. The loss enforces that the removed fog and glow by the network must be smooth spatially. The subtraction is done pixel by pixel. The operator E[S], where S is any matrix, implies adding every element of the matrix, and dividing the total sum with the size of the matrix.\n\n-Grayscale Pair Loss Our synthetic nightime foggy data has paired clean ground-truths. Thus, we can employ the pair loss:\nL gray pair = E M gray [J f gray ] \u2212 J gt gray ,(2)\nwhere J f gray and J gt gray are paired synthetic grayscale fog image and clean ground-truth image, respectively. M gray [.] is our grayscale module.\n\n-Grayscale Discriminative Loss We apply a discriminative loss only to real images that do not have paired ground-truths. We define our discriminative loss as:\nL gray GAN = \u2212 log(\u03c3[D gray [M gray [J f gray ]]]),(3)\nwhere \u03c3[.] is the sigmoid function, and D gray is the discriminator [9].\n\nOverall, the total loss functions of the grayscale module is:\n\nL gray total = L gray pair + \u03bb gray\n1 L gray GAN + \u03bb gray 2 L gray smooth ,(4)\nGrayscale Input Grayscale Defogged Output\n\nGrayscale Input Grayscale Defogged Output  Fig. 3 shows some defogging results using the grayscale module. The glow and fog in input images are significantly removed, while the textures of the background are still retained. Note that, fog-free nighttime images without glow and sufficient lighting usually suffer from the low light problem. This is because glow is in fact light scattered by particles in the atmosphere. Hence, after we successfully remove fog and thus glow, it is natural that the image looks dimmer than the input image. We will discuss about this further in the later part of the following section.\n\n\nColor Module\n\nIn the color module, we take the original RGB input image as input. The operations in this module are similar to those of the grayscale module. However, in this module, we do not apply the high and low frequency decomposition technique like in the grayscale module. Instead, we input the RGB foggy nighttime image to the low frequency network directly, and the grayscale version of the input image to the high frequency network. The high and low frequency networks in this module are then guided by the counterparts in the grayscale module.\n\nHigh/Low Frequency Networks Since the high frequency map is independent from colors, the input of the high frequency network is the grayscale version of the input image. Like its counterpart in the grayscale module, the task of the high frequency network is to boost textures. The high frequency network learns to boost textures largely from the consistency loss that measures the consistency between its output and the outputs of the high frequency network in the grayscale module. Note that, unlike the high frequency network in the grayscale module that takes high frequency layers as input, in this module, the high frequency module takes the grayscale input image. Hence, we cannot simply use the one in the grayscale module.\n\nSimilarly, the low frequency network is also trained using the consistency with respect to the output of the low frequency counterpart in the grayscale module. However, different from the low frequency network in the grayscale module, the low frequency network in the color module produces a colored image. Hence, we convert the colored low frequency map to grayscale before applying the consistency loss. Doing this process, however, allows the color in the low frequency map to be shifted, since for real data, we do not have their paired ground-truths. To tackle this problem, we need another loss that ensures the correctness of the colors of the low frequency map. Here are all the losses for training the high and low frequency networks in the color module. Note that, after all the networks in the grayscale module have been trained, we freeze them.\n\n-High Frequency Consistency Loss We define it as:\nL rgb HF = E [HF gray [J f gray ]] \u2212 HF rgb [J f gray ] ,(5)\nwhere HF gray [.] and HF rgb [.] are the low frequency networks from the grayscale and color modules, respectively. -Low Frequency Consistency Loss We define it as:\nL rgb LF = E LF gray [J f gray ] \u2212 G LF rgb [J f rgb ] ,(6)\nwhere LF gray [.] and LF rgb [.] are the low frequency networks from the grayscale and color modules, respectively. Function G(.) is to convert an rgb image to its grayscale version. -Hue Loss This loss is to ensure there is no color shift in the output of the low frequency network. The basic idea is that if we can estimate the color (or chromaticity) of the atmospheric light of the input RGB image, then we can normalize the image such that the atmospheric light colors will be canceled. Once the input image is normalized, it is known that the hue values of the input fog image will be the same as those of the normalized clean fog-free image (e.g. [13]). Hence, we define the loss as:\nL rgb hue = E Hue[CN [LF rgb [J f rgb ], A[x f rgb ]]] \u2212Hue[CN [J f rgb , A[J f rgb ]] ,(7)\nwhere Hue[.] are a function to compute the hue values of an image. CN [.] is the function that normalizes an image based on the atmospheric light chromaticity. A[.] is the function to estimate the atmospheric light chromaticity. We can use any existing technique to estimate the atmospheric light chromaticity for every pixel such as that described in [17,26].\n\n\nFusing High and Colored Low Frequency Maps\n\nHaving obtained the grayscale high and colored low frequency maps, we multiple the grayscale high frequency map with the weight map we obtained in the grayscale module to suppress noise. Subsequently, we fuse the weighted high frequency map and the colored low frequency map. To ensure good quality of outputs, we also impose a few loss functions to train the networks in this color module:\n\n-Color Pair Loss This loss is only for our paired synthetic data. The loss is defined as:\nL rgb pair = E M rgb [J f rgb ] \u2212 J gt rgb ,(8)\nwhere x f rgb and x gt rgb are paired synthetic fog image and clean ground-truth image, respectively.\n\n-Color Discriminative Loss We define the loss as:\nL rgb GAN = \u2212 log(\u03c3[D rgb [M rgb [J f rgb ]]]),(9)\nwhere x f rgb is a real colored foggy nighttime image. -Image Metric Loss We include an unsupervised loss based on contrast and acutance [2], which are defined as:\nL rgb IQ = E (1 \u2212 C[M rgb [J f rgb ]] + C[J f rgb ]) + (1 \u2212 Ac[M rgb [J f rgb ]] + Ac[J f rgb ]) ,(10)\nwhere the C[.] and Ac [.] are the functions to obtain the contrast and acutance of an image.\n\nOverall, the total loss functions for color module is: Intensity Boosting Degradation in foggy nighttime images is caused not only by fog but also by other nighttime conditions such as low light and noise. Hence, successfully removing fog does not mean that we can have an output that is bright and noise free, since low light and noise are inherent even in clear nighttime images. In fact, foggy nighttime images look relatively brighter than clear nighttime images mainly due to the presence of glow (= the light scattered into the atmosphere). Hence, when we successfully remove fog and thus glow, implying removing the scattered light, it is natural that the images look darker.\nL rgb total = L rgb pair + \u03bb rgb 1 L rgb GAN + \u03bb rgb 2 L rgb LF +\u03bb rgb 3 L rgb HF + \u03bb rgb 4 L rgb hue + \u03bb rgb 5 L rgb IQ ,(11)\nNevertheless, we can boost the intensity of our output to improve the visibility quality using our current framework. Although we have to note that, the\n\n\nWithout Boosting\n\nWith Boosting Without Boosting With Boosting Fig. 4: After modified the low frequency consistency loss, the image brightness is boosted.\n\nissue of nighttime visibility enhancement is still an open problem (e.g. [10,4]), and thus effective nighttime/low-light visibility enhancement is actually beyond the scope of our defogging paper. In our intensity boosting, we simply modify the low frequency consistency loss in Eq. (6), namely by brightening up the reference grayscale low-frequency map; since, the low frequency map in the color module influences the tone and brightness of the final output. Our modified loss is expressed as:\n\n-Modified Low Frequency Consistency Loss:\nL boost LF = E B [LF gray [J f gray ]]] \u2212 G LF rgb [J f rgb ] ,(12)\nwhere B(.) is the intensity boosting function, where we use a gamma function. G(S), where S is any matrix, means for each element in S we power it to \u03b3. In other words: S \u03b3 ij , where \u03b3 in our experiment is 0.5. As a result of this, In Eq. (11), the loss L rgb LF is replaced by this loss L boost LF . Fig. 4 shows the results of our intensity boosting.\n\n\nDiscussion: Multiple Light Colors\n\nIn this section, we discuss why using grayscale images (and thus our grayscale module) can be more effective in dealing with multiple-light colors, which are commonly present in the nighttime images. Most daytime defogging methods (e.g., [24,8,12]) assume the atmospheric light is uniform across the input image. , which is a reasonable assumption, since in foggy daytime, the dominant light source is only the skylight. This assumption can thus simplify the problem of defogging, since we do not need to estimate the atmospheric light A for every pixel locally, reducing the complexity of the already ill-posed problem. Unfortunately, this assumption cannot be applied to most foggy nighttime images. Since in foggy nighttime, the skylight is usually much dimmer than the man-made light sources; and, there are multiple intensities/colors of these light sources, and their potential combination. This implies, that we need to estimate the atmospheric light locally for every pixel in the input image, which render a more complication of the problem. Existing nighttime defogging methods (e.g. [17,1,26]) propose a few techniques to estimate the atmospheric light locally, based on the patches of the input image. The basic assumption is that in a small patch, there is a uniform atmospheric light. While to some extent the proposed techniques work, their accuracy relies on the decided patch size. If the size is too large, the accuracy will drop, since the patch can actually contains multiple light colors. And, if the size is too small, the accuracy will also drop, since the background intensity/color can be uniform and mistakenly estimated as the light intensity/color In our proposed method, to deal with the problem, we leverage the grayscale version of the RGB input image. Our underlying idea is that grayscale images do not have color information, and thus the problem of multiple colors of the atmospheric light is not present. This reduces the complexity of the problem, and our graysacle module can focus on extracting the background properties, i.e., high and low frequencies, without worrying the correctness of their colors. Fig. 5 show the results of the high and low frequency from an RGB image (processing them directly without the guidance from its grayscale images), from a grayscale version of the image, and from our color module guided by our grayscale module. As can be observed, the grayscale high/low frequency can reveal more about the background properties. Moreover, the one without the guidance shows more artifacts, fogginess and color shifts in some regions.\n\n\nExperimental Results\n\nIn our experiments, we compare our method with the following baselines: Li et al. [17], Ancuti et al. [1], and Zhang et al. [26], which are the three stateof-art nighttime defogging methods. Besides, we also compare our results with  the state-of-art daytime defogging methods Berman et al. [3] and EPDN [21]. In our training process, we combine synthetic data with ground-truths and real foggy nighttime data without ground-truths. The synthetic data is generated by a video game engine, GTA5 [5], and our real foggy nighttime data are collected from the Internet.\n\nFor the quantitative evaluation, we use 200 pairs of synthetic data, which are generated from GTA5. The quantitative evaluation results are shown in Table 1, where our method shows better performance on both PSNR and SSIM by significant margins compared to all the baseline methods. Fig. 7 shows the qualitative evaluation results on real foggy nighttime images. As one can notice that our method qualitatively provides better defogging results compared with the results of the other baseline methods.\n\n\nInput Image\n\nOur Result Li et al. [17] Zhang et al. [ \n\n\nAblation Studies\n\nTo show the effectiveness of our grayscale module in helping the color module, we train our color module without the losses that involve the grayscale module.\n\nIn other words, we cut off the color module from the grayscale module. The network architecture in this case is similar to the grayscale module, but the input of the low frequency network is the RGB low frequency layers (instead of the grayscale low frequency layers), and the same decomposition technique is applied. The first row second column of Fig. 6 shows the defogging results. As can be seen, the fog is still considerably noticeable.\n\nTo show the effectiveness of the high/low frequency decomposition, we remove the two networks in our modules, replace them with one autoencoder network, and apply our losses that are relevant. As shown in the first row third column of Fig. 6, there are some regions still affected by fog, as the network is mistaken it with genuine textures. The ablation studies on the high/low frequency grayscale guidance (i.e. the high/low frequency consistency losses) are shown in the last two columns and first row of Fig. 6.\n\nThe second row of Fig. 6 shows that the effectiveness of other losses. The intensity boosting is important, since the image becomes dimmer after we remove the fog and glow. Without the hue loss, the outputs have red or other color shifting. Our hue loss can constrain the color shifting of the RGB outputs. To show the effectiveness of our image metric loss, we remove this loss from our color module. The results are shown in the second row and third column in Fig. 6. After adding this unsupervised image metric loss, the outputs of our color module are constrained to have higher contrast and higher fidelity. The second row fourth column of the figure shows the results when we do not apply the weight map. By zooming-in the images, one can notice some visible noise and artifacts particularly in the dark regions. By applying the weight map, the noise/artifact will be suppressed, as shown in the last column of the figure.\n\n\nConclusion\n\nWe have introduced a learning-based nighttime defogging method. To our knowledge, this is the first time, a deep learning-based method is dedicated to handle nighttime defogging problem. To achieve our goal, we design grayscale and color modules, which rely mainly on the high/low frequency layers to enhance textures and at the same time suppress glow, fog and noise. Due to the lack of paired real ground-truths, our training process employs both paired synthetic data and unpaired real data. For this, we introduce new consistency losses between the outputs of the grayscale and color modules. Experimental results and evaluations, both quantitative and qualitative, show the effectiveness of our method.\n\nFig. 1 :\n1Top left: Input image. Top right: Our result. Bottom left: Li et al.'s result\n\nFig. 2 :\n2The pipeline of our framework that consists of two main modules: grayscale module (top) and the color module (bottom). For the images, zoom-in for better visualization.\n\nFig. 3 :\n3The inputs and outputs of our grayscale module. Two left images show a pair of our input and output. Two right images show the same. Zoom-in for better visualization.\n\n\nwhere \u03bb rgb i , i = {1...5} are the weighting factors. Empirically in our experiments, we set \u03bb\n\nFig. 5 :\n5Comparison of the high and low frequency maps: The color module without guidance, the grayscale module and the color module with guidance from the grayscale module.\n\nFig. 6 :\n6Ablation studies on the high/low frequency decomposition, grayscale/color modules, loss functions and weight map. Zoom-in for better visualization.\n\nFig. 7 :\n7Qualitative comparisons with the state of the art methods on real images. Zoom-in for better visualization.\n\nTable 1 :\n1Quantitative results on our synthetic foggy nighttime data.PSNR \nSSIM \nInput Image \n18.987 \n0.6764 \nLi et al. [17] \n21.024 \n0.6394 \nZhang et al. [26] \n20.921 \n0.6461 \nAncuti et al. [1] \n20.585 \n0.6233 \nBerman et al. [3] \n19.085 \n0.5373 \nEPDN [21] \n22.565 \n0.7330 \nWithout Teacher Module \n26.163 \n0.8185 \nWithout Decomposition \n26.328 \n0.8219 \nWithout Weights Map \n26.892 \n0.8327 \n\nOur Result \n26.997 \n0.8499 \n\nInput Image Without \nGrayscale-\nModule \n\nWithout \nHigh/Low \nDecomposition \n\nWithout \nHigh \nFrequency Con-\nsistency Loss \n\nWithout \nLow \nFrequency Con-\nsistency Loss \n\nWithout Boost-\ning \n\nWithout \nHue \nLoss \n\nWithout Image \nMetric Loss \n\nWithout Weight-\nMap \n\nWith All Com-\nponents \n\n\nAcknowledgmentThis work is supported by MOE2019-T2-1-130.Nighttime Defogging Using H-L Freq Decomp and Gray-Color Networks\nNight-time dehazing by fusion. C Ancuti, C O Ancuti, C De Vleeschouwer, A C Bovik, 2016 IEEE International Conference on Image Processing (ICIP). IEEEAncuti, C., Ancuti, C.O., De Vleeschouwer, C., Bovik, A.C.: Night-time dehazing by fusion. In: 2016 IEEE International Conference on Image Processing (ICIP). pp. 2256-2260. IEEE (2016)\n\nVisual-qualitydriven learning for underwater vision enhancement. W V Barbosa, H G Amaral, T L Rocha, E R Nascimento, 25th IEEE International Conference on Image Processing (ICIP). IEEEBarbosa, W.V., Amaral, H.G., Rocha, T.L., Nascimento, E.R.: Visual-quality- driven learning for underwater vision enhancement. In: 2018 25th IEEE Inter- national Conference on Image Processing (ICIP). pp. 3933-3937. IEEE (2018)\n\nNon-local image dehazing. D Berman, S Avidan, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionBerman, D., Avidan, S., et al.: Non-local image dehazing. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 1674-1682 (2016)\n\nLearning to see in the dark. C Chen, Q Chen, J Xu, V Koltun, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionChen, C., Chen, Q., Xu, J., Koltun, V.: Learning to see in the dark. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 3291- 3300 (2018)\n\nA D Doan, A M Jawaid, T T Do, T J Chin, arXiv:1806.07381G2D: from GTA to Data. arXiv preprintDoan, A.D., Jawaid, A.M., Do, T.T., Chin, T.J.: G2D: from GTA to Data. arXiv preprint arXiv:1806.07381 pp. 1-9 (2018)\n\nCycle-dehaze: Enhanced cyclegan for single image dehazing. D Engin, A Genc, H Ekenel, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. the IEEE Conference on Computer Vision and Pattern Recognition WorkshopsEngin, D., Genc, A., Kemal Ekenel, H.: Cycle-dehaze: Enhanced cyclegan for single image dehazing. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. pp. 825-833 (2018)\n\nSingle image dehazing. R Fattal, ACM transactions on graphics (TOG). 27372Fattal, R.: Single image dehazing. ACM transactions on graphics (TOG) 27(3), 72 (2008)\n\nDehazing using color-lines. R Fattal, 3413Fattal, R.: Dehazing using color-lines 34(1), 13 (2014)\n\nGenerative adversarial nets. I Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, Y Bengio, Advances in neural information processing systems. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y.: Generative adversarial nets. In: Advances in neural information processing systems. pp. 2672-2680 (2014)\n\nLime: Low-light image enhancement via illumination map estimation. X Guo, Y Li, H Ling, IEEE Transactions on Image Processing. 262Guo, X., Li, Y., Ling, H.: Lime: Low-light image enhancement via illumination map estimation. IEEE Transactions on Image Processing 26(2), 982-993 (2016)\n\nGuided image filtering. K He, J Sun, X Tang, European conference on computer vision. SpringerHe, K., Sun, J., Tang, X.: Guided image filtering. In: European conference on computer vision. pp. 1-14. Springer (2010)\n\nSingle image haze removal using dark channel prior. K He, J Sun, X Tang, 33He, K., Sun, J., Tang, X.: Single image haze removal using dark channel prior. IEEE transactions on pattern analysis and machine intelligence 33(12), 2341-2353 (2011)\n\nImage dehazing using adaptive bi-channel priors on superpixels. Y Jiang, C Sun, Y Zhao, L Yang, Computer Vision and Image Understanding. 165Jiang, Y., Sun, C., Zhao, Y., Yang, L.: Image dehazing using adaptive bi-channel priors on superpixels. Computer Vision and Image Understanding 165, 17-32 (2017)\n\nAod-net: All-in-one dehazing network. B Li, X Peng, Z Wang, J Xu, D Feng, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionLi, B., Peng, X., Wang, Z., Xu, J., Feng, D.: Aod-net: All-in-one dehazing network. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 4770-4778 (2017)\n\nSingle image dehazing via conditional generative adversarial network. R Li, J Pan, Z Li, J Tang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionLi, R., Pan, J., Li, Z., Tang, J.: Single image dehazing via conditional generative adversarial network. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 8202-8211 (2018)\n\nA contrast enhancement framework with jpeg artifacts suppression. Y Li, F Guo, R T Tan, M S Brown, European conference on computer vision. SpringerLi, Y., Guo, F., Tan, R.T., Brown, M.S.: A contrast enhancement framework with jpeg artifacts suppression. In: European conference on computer vision. pp. 174- 188. Springer (2014)\n\nNighttime haze removal with glow and multiple light colors. Y Li, R T Tan, M S Brown, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionLi, Y., Tan, R.T., Brown, M.S.: Nighttime haze removal with glow and multiple light colors. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 226-234 (2015)\n\nHaze visibility enhancement: A survey and quantitative benchmarking. Y Li, S You, M S Brown, R T Tan, Computer Vision and Image Understanding. 165Li, Y., You, S., Brown, M.S., Tan, R.T.: Haze visibility enhancement: A survey and quantitative benchmarking. Computer Vision and Image Understanding 165, 1-16 (2017)\n\nEfficient image dehazing with boundary constraint and contextual regularization. G Meng, Y Wang, J Duan, S Xiang, C Pan, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionMeng, G., Wang, Y., Duan, J., Xiang, S., Pan, C.: Efficient image dehazing with boundary constraint and contextual regularization. In: Proceedings of the IEEE international conference on computer vision. pp. 617-624 (2013)\n\nNighttime haze removal using color transfer pre-processing and dark channel prior. S C Pei, T Y Lee, 2012 19th IEEE International Conference on Image Processing. IEEEPei, S.C., Lee, T.Y.: Nighttime haze removal using color transfer pre-processing and dark channel prior. In: 2012 19th IEEE International Conference on Image Processing. pp. 957-960. IEEE (2012)\n\nEnhanced pix2pix dehazing network. Y Qu, Y Chen, J Huang, Y Xie, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionQu, Y., Chen, Y., Huang, J., Xie, Y.: Enhanced pix2pix dehazing network. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 8160-8168 (2019)\n\nW Ren, S Liu, H Zhang, J Pan, X Cao, M H Yang, Single image dehazing via multi-scale convolutional neural networks. In: European conference on computer vision. SpringerRen, W., Liu, S., Zhang, H., Pan, J., Cao, X., Yang, M.H.: Single image dehazing via multi-scale convolutional neural networks. In: European conference on com- puter vision. pp. 154-169. Springer (2016)\n\nGated fusion network for single image dehazing. W Ren, L Ma, J Zhang, J Pan, X Cao, W Liu, M H Yang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionRen, W., Ma, L., Zhang, J., Pan, J., Cao, X., Liu, W., Yang, M.H.: Gated fusion network for single image dehazing. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 3253-3261 (2018)\n\nVisibility in bad weather from a single image. R T Tan, 2008 IEEE Conference on Computer Vision and Pattern Recognition. IEEETan, R.T.: Visibility in bad weather from a single image. In: 2008 IEEE Conference on Computer Vision and Pattern Recognition. pp. 1-8. IEEE (2008)\n\nDensely connected pyramid dehazing network. H Zhang, V M Patel, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionZhang, H., Patel, V.M.: Densely connected pyramid dehazing network. In: Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 3194-3203 (2018)\n\nFast haze removal for nighttime image using maximum reflectance prior. J Zhang, Y Cao, S Fang, Y Kang, C Wen Chen, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionZhang, J., Cao, Y., Fang, S., Kang, Y., Wen Chen, C.: Fast haze removal for nighttime image using maximum reflectance prior. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 7418-7426 (2017)\n\nNighttime haze removal based on a new imaging model. J Zhang, Y Cao, Z Wang, IEEEZhang, J., Cao, Y., Wang, Z.: Nighttime haze removal based on a new imaging model. pp. 4557-4561. IEEE (2014)\n", "annotations": {"author": "[{\"start\":\"91\",\"end\":\"170\"}]", "publisher": null, "author_last_name": "[{\"start\":\"99\",\"end\":\"102\"}]", "author_first_name": "[{\"start\":\"91\",\"end\":\"98\"}]", "author_affiliation": "[{\"start\":\"104\",\"end\":\"137\"},{\"start\":\"139\",\"end\":\"156\"},{\"start\":\"158\",\"end\":\"169\"}]", "title": "[{\"start\":\"1\",\"end\":\"88\"},{\"start\":\"171\",\"end\":\"258\"}]", "venue": null, "abstract": "[{\"start\":\"363\",\"end\":\"1856\"}]", "bib_ref": "[{\"start\":\"2002\",\"end\":\"2006\",\"attributes\":{\"ref_id\":\"b26\"}},{\"start\":\"2006\",\"end\":\"2009\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"2009\",\"end\":\"2012\",\"attributes\":{\"ref_id\":\"b25\"}},{\"start\":\"2632\",\"end\":\"2636\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"2650\",\"end\":\"2654\",\"attributes\":{\"ref_id\":\"b25\"}},{\"start\":\"2655\",\"end\":\"2659\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"2697\",\"end\":\"2701\",\"attributes\":{\"ref_id\":\"b25\"}},{\"start\":\"3466\",\"end\":\"3470\",\"attributes\":{\"ref_id\":\"b23\"}},{\"start\":\"3470\",\"end\":\"3472\",\"attributes\":{\"ref_id\":\"b6\"}},{\"start\":\"3472\",\"end\":\"3475\",\"attributes\":{\"ref_id\":\"b11\"}},{\"start\":\"3475\",\"end\":\"3478\",\"attributes\":{\"ref_id\":\"b18\"}},{\"start\":\"3478\",\"end\":\"3480\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"3480\",\"end\":\"3483\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"5992\",\"end\":\"5996\",\"attributes\":{\"ref_id\":\"b23\"}},{\"start\":\"5996\",\"end\":\"5998\",\"attributes\":{\"ref_id\":\"b6\"}},{\"start\":\"5998\",\"end\":\"6001\",\"attributes\":{\"ref_id\":\"b11\"}},{\"start\":\"6001\",\"end\":\"6004\",\"attributes\":{\"ref_id\":\"b18\"}},{\"start\":\"6004\",\"end\":\"6006\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"6006\",\"end\":\"6009\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"6009\",\"end\":\"6012\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"6012\",\"end\":\"6015\",\"attributes\":{\"ref_id\":\"b24\"}},{\"start\":\"6015\",\"end\":\"6018\",\"attributes\":{\"ref_id\":\"b22\"}},{\"start\":\"6018\",\"end\":\"6021\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"6021\",\"end\":\"6023\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"6030\",\"end\":\"6034\",\"attributes\":{\"ref_id\":\"b23\"}},{\"start\":\"6084\",\"end\":\"6087\",\"attributes\":{\"ref_id\":\"b6\"}},{\"start\":\"6188\",\"end\":\"6192\",\"attributes\":{\"ref_id\":\"b11\"}},{\"start\":\"6242\",\"end\":\"6246\",\"attributes\":{\"ref_id\":\"b18\"}},{\"start\":\"6345\",\"end\":\"6348\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"6416\",\"end\":\"6420\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"6497\",\"end\":\"6501\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"6635\",\"end\":\"6639\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"6681\",\"end\":\"6684\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"6763\",\"end\":\"6767\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"7138\",\"end\":\"7142\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"7267\",\"end\":\"7271\",\"attributes\":{\"ref_id\":\"b26\"}},{\"start\":\"7432\",\"end\":\"7436\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"7636\",\"end\":\"7639\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"7890\",\"end\":\"7894\",\"attributes\":{\"ref_id\":\"b25\"}},{\"start\":\"9945\",\"end\":\"9949\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"12052\",\"end\":\"12056\",\"attributes\":{\"ref_id\":\"b15\"}},{\"start\":\"12421\",\"end\":\"12425\",\"attributes\":{\"ref_id\":\"b15\"}},{\"start\":\"14240\",\"end\":\"14243\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"18188\",\"end\":\"18192\",\"attributes\":{\"ref_id\":\"b12\"}},{\"start\":\"18669\",\"end\":\"18673\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"18673\",\"end\":\"18676\",\"attributes\":{\"ref_id\":\"b25\"}},{\"start\":\"19595\",\"end\":\"19598\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"19747\",\"end\":\"19750\"},{\"start\":\"21013\",\"end\":\"21017\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"21017\",\"end\":\"21019\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"22176\",\"end\":\"22180\",\"attributes\":{\"ref_id\":\"b23\"}},{\"start\":\"22180\",\"end\":\"22182\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"22182\",\"end\":\"22185\",\"attributes\":{\"ref_id\":\"b11\"}},{\"start\":\"23032\",\"end\":\"23036\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"23036\",\"end\":\"23038\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"23038\",\"end\":\"23041\",\"attributes\":{\"ref_id\":\"b25\"}},{\"start\":\"24638\",\"end\":\"24642\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"24658\",\"end\":\"24661\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"24680\",\"end\":\"24684\",\"attributes\":{\"ref_id\":\"b25\"}},{\"start\":\"24847\",\"end\":\"24850\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"24860\",\"end\":\"24864\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"25050\",\"end\":\"25053\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"25661\",\"end\":\"25665\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"25679\",\"end\":\"25680\"}]", "figure": "[{\"start\":\"28474\",\"end\":\"28562\",\"attributes\":{\"id\":\"fig_0\"}},{\"start\":\"28563\",\"end\":\"28742\",\"attributes\":{\"id\":\"fig_1\"}},{\"start\":\"28743\",\"end\":\"28920\",\"attributes\":{\"id\":\"fig_2\"}},{\"start\":\"28921\",\"end\":\"29018\",\"attributes\":{\"id\":\"fig_3\"}},{\"start\":\"29019\",\"end\":\"29194\",\"attributes\":{\"id\":\"fig_4\"}},{\"start\":\"29195\",\"end\":\"29353\",\"attributes\":{\"id\":\"fig_5\"}},{\"start\":\"29354\",\"end\":\"29472\",\"attributes\":{\"id\":\"fig_6\"}},{\"start\":\"29473\",\"end\":\"30179\",\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"}}]", "paragraph": "[{\"start\":\"1872\",\"end\":\"2608\"},{\"start\":\"2610\",\"end\":\"2620\"},{\"start\":\"2622\",\"end\":\"2799\"},{\"start\":\"2801\",\"end\":\"3709\"},{\"start\":\"3711\",\"end\":\"5216\"},{\"start\":\"5218\",\"end\":\"5265\"},{\"start\":\"5267\",\"end\":\"5479\"},{\"start\":\"5481\",\"end\":\"5900\"},{\"start\":\"5917\",\"end\":\"7053\"},{\"start\":\"7055\",\"end\":\"8105\"},{\"start\":\"8125\",\"end\":\"9102\"},{\"start\":\"9123\",\"end\":\"9748\"},{\"start\":\"9791\",\"end\":\"11460\"},{\"start\":\"11462\",\"end\":\"12658\"},{\"start\":\"12660\",\"end\":\"13037\"},{\"start\":\"13068\",\"end\":\"13091\"},{\"start\":\"13154\",\"end\":\"13631\"},{\"start\":\"13633\",\"end\":\"13754\"},{\"start\":\"13807\",\"end\":\"13956\"},{\"start\":\"13958\",\"end\":\"14116\"},{\"start\":\"14172\",\"end\":\"14244\"},{\"start\":\"14246\",\"end\":\"14307\"},{\"start\":\"14309\",\"end\":\"14344\"},{\"start\":\"14388\",\"end\":\"14429\"},{\"start\":\"14431\",\"end\":\"15049\"},{\"start\":\"15066\",\"end\":\"15606\"},{\"start\":\"15608\",\"end\":\"16338\"},{\"start\":\"16340\",\"end\":\"17196\"},{\"start\":\"17198\",\"end\":\"17247\"},{\"start\":\"17309\",\"end\":\"17473\"},{\"start\":\"17534\",\"end\":\"18224\"},{\"start\":\"18317\",\"end\":\"18677\"},{\"start\":\"18724\",\"end\":\"19114\"},{\"start\":\"19116\",\"end\":\"19205\"},{\"start\":\"19254\",\"end\":\"19355\"},{\"start\":\"19357\",\"end\":\"19406\"},{\"start\":\"19458\",\"end\":\"19621\"},{\"start\":\"19725\",\"end\":\"19817\"},{\"start\":\"19819\",\"end\":\"20501\"},{\"start\":\"20629\",\"end\":\"20781\"},{\"start\":\"20802\",\"end\":\"20938\"},{\"start\":\"20940\",\"end\":\"21435\"},{\"start\":\"21437\",\"end\":\"21478\"},{\"start\":\"21547\",\"end\":\"21900\"},{\"start\":\"21938\",\"end\":\"24531\"},{\"start\":\"24556\",\"end\":\"25121\"},{\"start\":\"25123\",\"end\":\"25624\"},{\"start\":\"25640\",\"end\":\"25681\"},{\"start\":\"25702\",\"end\":\"25860\"},{\"start\":\"25862\",\"end\":\"26304\"},{\"start\":\"26306\",\"end\":\"26821\"},{\"start\":\"26823\",\"end\":\"27751\"},{\"start\":\"27766\",\"end\":\"28473\"}]", "formula": "[{\"start\":\"13092\",\"end\":\"13153\",\"attributes\":{\"id\":\"formula_0\"}},{\"start\":\"13755\",\"end\":\"13806\",\"attributes\":{\"id\":\"formula_1\"}},{\"start\":\"14117\",\"end\":\"14171\",\"attributes\":{\"id\":\"formula_2\"}},{\"start\":\"14345\",\"end\":\"14387\",\"attributes\":{\"id\":\"formula_3\"}},{\"start\":\"17248\",\"end\":\"17308\",\"attributes\":{\"id\":\"formula_4\"}},{\"start\":\"17474\",\"end\":\"17533\",\"attributes\":{\"id\":\"formula_5\"}},{\"start\":\"18225\",\"end\":\"18316\",\"attributes\":{\"id\":\"formula_6\"}},{\"start\":\"19206\",\"end\":\"19253\",\"attributes\":{\"id\":\"formula_7\"}},{\"start\":\"19407\",\"end\":\"19457\",\"attributes\":{\"id\":\"formula_8\"}},{\"start\":\"19622\",\"end\":\"19724\",\"attributes\":{\"id\":\"formula_9\"}},{\"start\":\"20502\",\"end\":\"20628\",\"attributes\":{\"id\":\"formula_10\"}},{\"start\":\"21479\",\"end\":\"21546\",\"attributes\":{\"id\":\"formula_11\"}}]", "table_ref": "[{\"start\":\"25272\",\"end\":\"25279\",\"attributes\":{\"ref_id\":\"tab_1\"}}]", "section_header": "[{\"start\":\"1858\",\"end\":\"1870\",\"attributes\":{\"n\":\"1\"}},{\"start\":\"5903\",\"end\":\"5915\",\"attributes\":{\"n\":\"2\"}},{\"start\":\"8108\",\"end\":\"8123\",\"attributes\":{\"n\":\"3\"}},{\"start\":\"9105\",\"end\":\"9121\",\"attributes\":{\"n\":\"3.1\"}},{\"start\":\"9751\",\"end\":\"9789\"},{\"start\":\"13040\",\"end\":\"13066\"},{\"start\":\"15052\",\"end\":\"15064\",\"attributes\":{\"n\":\"3.2\"}},{\"start\":\"18680\",\"end\":\"18722\"},{\"start\":\"20784\",\"end\":\"20800\"},{\"start\":\"21903\",\"end\":\"21936\",\"attributes\":{\"n\":\"4\"}},{\"start\":\"24534\",\"end\":\"24554\",\"attributes\":{\"n\":\"5\"}},{\"start\":\"25627\",\"end\":\"25638\"},{\"start\":\"25684\",\"end\":\"25700\",\"attributes\":{\"n\":\"5.1\"}},{\"start\":\"27754\",\"end\":\"27764\",\"attributes\":{\"n\":\"6\"}},{\"start\":\"28475\",\"end\":\"28483\"},{\"start\":\"28564\",\"end\":\"28572\"},{\"start\":\"28744\",\"end\":\"28752\"},{\"start\":\"29020\",\"end\":\"29028\"},{\"start\":\"29196\",\"end\":\"29204\"},{\"start\":\"29355\",\"end\":\"29363\"},{\"start\":\"29474\",\"end\":\"29483\"}]", "table": "[{\"start\":\"29544\",\"end\":\"30179\"}]", "figure_caption": "[{\"start\":\"28485\",\"end\":\"28562\"},{\"start\":\"28574\",\"end\":\"28742\"},{\"start\":\"28754\",\"end\":\"28920\"},{\"start\":\"28923\",\"end\":\"29018\"},{\"start\":\"29030\",\"end\":\"29194\"},{\"start\":\"29206\",\"end\":\"29353\"},{\"start\":\"29365\",\"end\":\"29472\"},{\"start\":\"29485\",\"end\":\"29544\"}]", "figure_ref": "[{\"start\":\"2188\",\"end\":\"2194\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"8129\",\"end\":\"8132\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"10526\",\"end\":\"10532\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"12596\",\"end\":\"12602\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"14474\",\"end\":\"14480\",\"attributes\":{\"ref_id\":\"fig_2\"}},{\"start\":\"20847\",\"end\":\"20853\"},{\"start\":\"21849\",\"end\":\"21855\"},{\"start\":\"24081\",\"end\":\"24087\",\"attributes\":{\"ref_id\":\"fig_4\"}},{\"start\":\"25406\",\"end\":\"25412\",\"attributes\":{\"ref_id\":\"fig_6\"}},{\"start\":\"26211\",\"end\":\"26217\",\"attributes\":{\"ref_id\":\"fig_5\"}},{\"start\":\"26541\",\"end\":\"26547\",\"attributes\":{\"ref_id\":\"fig_5\"}},{\"start\":\"26814\",\"end\":\"26820\",\"attributes\":{\"ref_id\":\"fig_5\"}},{\"start\":\"26841\",\"end\":\"26847\",\"attributes\":{\"ref_id\":\"fig_5\"}},{\"start\":\"27285\",\"end\":\"27291\",\"attributes\":{\"ref_id\":\"fig_5\"}}]", "bib_author_first_name": "[{\"start\":\"30334\",\"end\":\"30335\"},{\"start\":\"30344\",\"end\":\"30345\"},{\"start\":\"30346\",\"end\":\"30347\"},{\"start\":\"30356\",\"end\":\"30357\"},{\"start\":\"30375\",\"end\":\"30376\"},{\"start\":\"30377\",\"end\":\"30378\"},{\"start\":\"30704\",\"end\":\"30705\"},{\"start\":\"30706\",\"end\":\"30707\"},{\"start\":\"30717\",\"end\":\"30718\"},{\"start\":\"30719\",\"end\":\"30720\"},{\"start\":\"30729\",\"end\":\"30730\"},{\"start\":\"30731\",\"end\":\"30732\"},{\"start\":\"30740\",\"end\":\"30741\"},{\"start\":\"30742\",\"end\":\"30743\"},{\"start\":\"31078\",\"end\":\"31079\"},{\"start\":\"31088\",\"end\":\"31089\"},{\"start\":\"31431\",\"end\":\"31432\"},{\"start\":\"31439\",\"end\":\"31440\"},{\"start\":\"31447\",\"end\":\"31448\"},{\"start\":\"31453\",\"end\":\"31454\"},{\"start\":\"31779\",\"end\":\"31780\"},{\"start\":\"31781\",\"end\":\"31782\"},{\"start\":\"31789\",\"end\":\"31790\"},{\"start\":\"31791\",\"end\":\"31792\"},{\"start\":\"31801\",\"end\":\"31802\"},{\"start\":\"31803\",\"end\":\"31804\"},{\"start\":\"31809\",\"end\":\"31810\"},{\"start\":\"31811\",\"end\":\"31812\"},{\"start\":\"32050\",\"end\":\"32051\"},{\"start\":\"32059\",\"end\":\"32060\"},{\"start\":\"32067\",\"end\":\"32068\"},{\"start\":\"32472\",\"end\":\"32473\"},{\"start\":\"32639\",\"end\":\"32640\"},{\"start\":\"32739\",\"end\":\"32740\"},{\"start\":\"32753\",\"end\":\"32754\"},{\"start\":\"32770\",\"end\":\"32771\"},{\"start\":\"32779\",\"end\":\"32780\"},{\"start\":\"32785\",\"end\":\"32786\"},{\"start\":\"32801\",\"end\":\"32802\"},{\"start\":\"32810\",\"end\":\"32811\"},{\"start\":\"32823\",\"end\":\"32824\"},{\"start\":\"33167\",\"end\":\"33168\"},{\"start\":\"33174\",\"end\":\"33175\"},{\"start\":\"33180\",\"end\":\"33181\"},{\"start\":\"33409\",\"end\":\"33410\"},{\"start\":\"33415\",\"end\":\"33416\"},{\"start\":\"33422\",\"end\":\"33423\"},{\"start\":\"33652\",\"end\":\"33653\"},{\"start\":\"33658\",\"end\":\"33659\"},{\"start\":\"33665\",\"end\":\"33666\"},{\"start\":\"33907\",\"end\":\"33908\"},{\"start\":\"33916\",\"end\":\"33917\"},{\"start\":\"33923\",\"end\":\"33924\"},{\"start\":\"33931\",\"end\":\"33932\"},{\"start\":\"34184\",\"end\":\"34185\"},{\"start\":\"34190\",\"end\":\"34191\"},{\"start\":\"34198\",\"end\":\"34199\"},{\"start\":\"34206\",\"end\":\"34207\"},{\"start\":\"34212\",\"end\":\"34213\"},{\"start\":\"34590\",\"end\":\"34591\"},{\"start\":\"34596\",\"end\":\"34597\"},{\"start\":\"34603\",\"end\":\"34604\"},{\"start\":\"34609\",\"end\":\"34610\"},{\"start\":\"35034\",\"end\":\"35035\"},{\"start\":\"35040\",\"end\":\"35041\"},{\"start\":\"35047\",\"end\":\"35048\"},{\"start\":\"35049\",\"end\":\"35050\"},{\"start\":\"35056\",\"end\":\"35057\"},{\"start\":\"35058\",\"end\":\"35059\"},{\"start\":\"35357\",\"end\":\"35358\"},{\"start\":\"35363\",\"end\":\"35364\"},{\"start\":\"35365\",\"end\":\"35366\"},{\"start\":\"35372\",\"end\":\"35373\"},{\"start\":\"35374\",\"end\":\"35375\"},{\"start\":\"35758\",\"end\":\"35759\"},{\"start\":\"35764\",\"end\":\"35765\"},{\"start\":\"35771\",\"end\":\"35772\"},{\"start\":\"35773\",\"end\":\"35774\"},{\"start\":\"35782\",\"end\":\"35783\"},{\"start\":\"35784\",\"end\":\"35785\"},{\"start\":\"36084\",\"end\":\"36085\"},{\"start\":\"36092\",\"end\":\"36093\"},{\"start\":\"36100\",\"end\":\"36101\"},{\"start\":\"36108\",\"end\":\"36109\"},{\"start\":\"36117\",\"end\":\"36118\"},{\"start\":\"36552\",\"end\":\"36553\"},{\"start\":\"36554\",\"end\":\"36555\"},{\"start\":\"36561\",\"end\":\"36562\"},{\"start\":\"36563\",\"end\":\"36564\"},{\"start\":\"36866\",\"end\":\"36867\"},{\"start\":\"36872\",\"end\":\"36873\"},{\"start\":\"36880\",\"end\":\"36881\"},{\"start\":\"36889\",\"end\":\"36890\"},{\"start\":\"37215\",\"end\":\"37216\"},{\"start\":\"37222\",\"end\":\"37223\"},{\"start\":\"37229\",\"end\":\"37230\"},{\"start\":\"37238\",\"end\":\"37239\"},{\"start\":\"37245\",\"end\":\"37246\"},{\"start\":\"37252\",\"end\":\"37253\"},{\"start\":\"37254\",\"end\":\"37255\"},{\"start\":\"37635\",\"end\":\"37636\"},{\"start\":\"37642\",\"end\":\"37643\"},{\"start\":\"37648\",\"end\":\"37649\"},{\"start\":\"37657\",\"end\":\"37658\"},{\"start\":\"37664\",\"end\":\"37665\"},{\"start\":\"37671\",\"end\":\"37672\"},{\"start\":\"37678\",\"end\":\"37679\"},{\"start\":\"37680\",\"end\":\"37681\"},{\"start\":\"38096\",\"end\":\"38097\"},{\"start\":\"38098\",\"end\":\"38099\"},{\"start\":\"38367\",\"end\":\"38368\"},{\"start\":\"38376\",\"end\":\"38377\"},{\"start\":\"38378\",\"end\":\"38379\"},{\"start\":\"38774\",\"end\":\"38775\"},{\"start\":\"38783\",\"end\":\"38784\"},{\"start\":\"38790\",\"end\":\"38791\"},{\"start\":\"38798\",\"end\":\"38799\"},{\"start\":\"38806\",\"end\":\"38807\"},{\"start\":\"39242\",\"end\":\"39243\"},{\"start\":\"39251\",\"end\":\"39252\"},{\"start\":\"39258\",\"end\":\"39259\"}]", "bib_author_last_name": "[{\"start\":\"30336\",\"end\":\"30342\"},{\"start\":\"30348\",\"end\":\"30354\"},{\"start\":\"30358\",\"end\":\"30373\"},{\"start\":\"30379\",\"end\":\"30384\"},{\"start\":\"30708\",\"end\":\"30715\"},{\"start\":\"30721\",\"end\":\"30727\"},{\"start\":\"30733\",\"end\":\"30738\"},{\"start\":\"30744\",\"end\":\"30754\"},{\"start\":\"31080\",\"end\":\"31086\"},{\"start\":\"31090\",\"end\":\"31096\"},{\"start\":\"31433\",\"end\":\"31437\"},{\"start\":\"31441\",\"end\":\"31445\"},{\"start\":\"31449\",\"end\":\"31451\"},{\"start\":\"31455\",\"end\":\"31461\"},{\"start\":\"31783\",\"end\":\"31787\"},{\"start\":\"31793\",\"end\":\"31799\"},{\"start\":\"31805\",\"end\":\"31807\"},{\"start\":\"31813\",\"end\":\"31817\"},{\"start\":\"32052\",\"end\":\"32057\"},{\"start\":\"32061\",\"end\":\"32065\"},{\"start\":\"32069\",\"end\":\"32075\"},{\"start\":\"32474\",\"end\":\"32480\"},{\"start\":\"32641\",\"end\":\"32647\"},{\"start\":\"32741\",\"end\":\"32751\"},{\"start\":\"32755\",\"end\":\"32768\"},{\"start\":\"32772\",\"end\":\"32777\"},{\"start\":\"32781\",\"end\":\"32783\"},{\"start\":\"32787\",\"end\":\"32799\"},{\"start\":\"32803\",\"end\":\"32808\"},{\"start\":\"32812\",\"end\":\"32821\"},{\"start\":\"32825\",\"end\":\"32831\"},{\"start\":\"33169\",\"end\":\"33172\"},{\"start\":\"33176\",\"end\":\"33178\"},{\"start\":\"33182\",\"end\":\"33186\"},{\"start\":\"33411\",\"end\":\"33413\"},{\"start\":\"33417\",\"end\":\"33420\"},{\"start\":\"33424\",\"end\":\"33428\"},{\"start\":\"33654\",\"end\":\"33656\"},{\"start\":\"33660\",\"end\":\"33663\"},{\"start\":\"33667\",\"end\":\"33671\"},{\"start\":\"33909\",\"end\":\"33914\"},{\"start\":\"33918\",\"end\":\"33921\"},{\"start\":\"33925\",\"end\":\"33929\"},{\"start\":\"33933\",\"end\":\"33937\"},{\"start\":\"34186\",\"end\":\"34188\"},{\"start\":\"34192\",\"end\":\"34196\"},{\"start\":\"34200\",\"end\":\"34204\"},{\"start\":\"34208\",\"end\":\"34210\"},{\"start\":\"34214\",\"end\":\"34218\"},{\"start\":\"34592\",\"end\":\"34594\"},{\"start\":\"34598\",\"end\":\"34601\"},{\"start\":\"34605\",\"end\":\"34607\"},{\"start\":\"34611\",\"end\":\"34615\"},{\"start\":\"35036\",\"end\":\"35038\"},{\"start\":\"35042\",\"end\":\"35045\"},{\"start\":\"35051\",\"end\":\"35054\"},{\"start\":\"35060\",\"end\":\"35065\"},{\"start\":\"35359\",\"end\":\"35361\"},{\"start\":\"35367\",\"end\":\"35370\"},{\"start\":\"35376\",\"end\":\"35381\"},{\"start\":\"35760\",\"end\":\"35762\"},{\"start\":\"35766\",\"end\":\"35769\"},{\"start\":\"35775\",\"end\":\"35780\"},{\"start\":\"35786\",\"end\":\"35789\"},{\"start\":\"36086\",\"end\":\"36090\"},{\"start\":\"36094\",\"end\":\"36098\"},{\"start\":\"36102\",\"end\":\"36106\"},{\"start\":\"36110\",\"end\":\"36115\"},{\"start\":\"36119\",\"end\":\"36122\"},{\"start\":\"36556\",\"end\":\"36559\"},{\"start\":\"36565\",\"end\":\"36568\"},{\"start\":\"36868\",\"end\":\"36870\"},{\"start\":\"36874\",\"end\":\"36878\"},{\"start\":\"36882\",\"end\":\"36887\"},{\"start\":\"36891\",\"end\":\"36894\"},{\"start\":\"37217\",\"end\":\"37220\"},{\"start\":\"37224\",\"end\":\"37227\"},{\"start\":\"37231\",\"end\":\"37236\"},{\"start\":\"37240\",\"end\":\"37243\"},{\"start\":\"37247\",\"end\":\"37250\"},{\"start\":\"37256\",\"end\":\"37260\"},{\"start\":\"37637\",\"end\":\"37640\"},{\"start\":\"37644\",\"end\":\"37646\"},{\"start\":\"37650\",\"end\":\"37655\"},{\"start\":\"37659\",\"end\":\"37662\"},{\"start\":\"37666\",\"end\":\"37669\"},{\"start\":\"37673\",\"end\":\"37676\"},{\"start\":\"37682\",\"end\":\"37686\"},{\"start\":\"38100\",\"end\":\"38103\"},{\"start\":\"38369\",\"end\":\"38374\"},{\"start\":\"38380\",\"end\":\"38385\"},{\"start\":\"38776\",\"end\":\"38781\"},{\"start\":\"38785\",\"end\":\"38788\"},{\"start\":\"38792\",\"end\":\"38796\"},{\"start\":\"38800\",\"end\":\"38804\"},{\"start\":\"38808\",\"end\":\"38816\"},{\"start\":\"39244\",\"end\":\"39249\"},{\"start\":\"39253\",\"end\":\"39256\"},{\"start\":\"39260\",\"end\":\"39264\"}]", "bib_entry": "[{\"start\":\"30303\",\"end\":\"30637\",\"attributes\":{\"matched_paper_id\":\"14453359\",\"id\":\"b0\"}},{\"start\":\"30639\",\"end\":\"31050\",\"attributes\":{\"matched_paper_id\":\"52192567\",\"id\":\"b1\"}},{\"start\":\"31052\",\"end\":\"31400\",\"attributes\":{\"matched_paper_id\":\"18774783\",\"id\":\"b2\"}},{\"start\":\"31402\",\"end\":\"31777\",\"attributes\":{\"matched_paper_id\":\"4691825\",\"id\":\"b3\"}},{\"start\":\"31779\",\"end\":\"31989\",\"attributes\":{\"id\":\"b4\",\"doi\":\"arXiv:1806.07381\"}},{\"start\":\"31991\",\"end\":\"32447\",\"attributes\":{\"matched_paper_id\":\"44213294\",\"id\":\"b5\"}},{\"start\":\"32449\",\"end\":\"32609\",\"attributes\":{\"matched_paper_id\":\"5904964\",\"id\":\"b6\"}},{\"start\":\"32611\",\"end\":\"32708\",\"attributes\":{\"id\":\"b7\"}},{\"start\":\"32710\",\"end\":\"33098\",\"attributes\":{\"matched_paper_id\":\"1033682\",\"id\":\"b8\"}},{\"start\":\"33100\",\"end\":\"33383\",\"attributes\":{\"matched_paper_id\":\"5778488\",\"id\":\"b9\"}},{\"start\":\"33385\",\"end\":\"33598\",\"attributes\":{\"matched_paper_id\":\"1264129\",\"id\":\"b10\"}},{\"start\":\"33600\",\"end\":\"33841\",\"attributes\":{\"id\":\"b11\"}},{\"start\":\"33843\",\"end\":\"34144\",\"attributes\":{\"matched_paper_id\":\"3422185\",\"id\":\"b12\"}},{\"start\":\"34146\",\"end\":\"34518\",\"attributes\":{\"matched_paper_id\":\"30151664\",\"id\":\"b13\"}},{\"start\":\"34520\",\"end\":\"34966\",\"attributes\":{\"matched_paper_id\":\"51990067\",\"id\":\"b14\"}},{\"start\":\"34968\",\"end\":\"35295\",\"attributes\":{\"matched_paper_id\":\"1748314\",\"id\":\"b15\"}},{\"start\":\"35297\",\"end\":\"35687\",\"attributes\":{\"matched_paper_id\":\"17319211\",\"id\":\"b16\"}},{\"start\":\"35689\",\"end\":\"36001\",\"attributes\":{\"matched_paper_id\":\"3608506\",\"id\":\"b17\"}},{\"start\":\"36003\",\"end\":\"36467\",\"attributes\":{\"matched_paper_id\":\"6080741\",\"id\":\"b18\"}},{\"start\":\"36469\",\"end\":\"36829\",\"attributes\":{\"matched_paper_id\":\"10071664\",\"id\":\"b19\"}},{\"start\":\"36831\",\"end\":\"37213\",\"attributes\":{\"matched_paper_id\":\"195489762\",\"id\":\"b20\"}},{\"start\":\"37215\",\"end\":\"37585\",\"attributes\":{\"id\":\"b21\"}},{\"start\":\"37587\",\"end\":\"38047\",\"attributes\":{\"matched_paper_id\":\"4563057\",\"id\":\"b22\"}},{\"start\":\"38049\",\"end\":\"38321\",\"attributes\":{\"matched_paper_id\":\"2700315\",\"id\":\"b23\"}},{\"start\":\"38323\",\"end\":\"38701\",\"attributes\":{\"matched_paper_id\":\"4054776\",\"id\":\"b24\"}},{\"start\":\"38703\",\"end\":\"39187\",\"attributes\":{\"matched_paper_id\":\"7546676\",\"id\":\"b25\"}},{\"start\":\"39189\",\"end\":\"39379\",\"attributes\":{\"id\":\"b26\"}}]", "bib_title": "[{\"start\":\"30303\",\"end\":\"30332\"},{\"start\":\"30639\",\"end\":\"30702\"},{\"start\":\"31052\",\"end\":\"31076\"},{\"start\":\"31402\",\"end\":\"31429\"},{\"start\":\"31991\",\"end\":\"32048\"},{\"start\":\"32449\",\"end\":\"32470\"},{\"start\":\"32710\",\"end\":\"32737\"},{\"start\":\"33100\",\"end\":\"33165\"},{\"start\":\"33385\",\"end\":\"33407\"},{\"start\":\"33843\",\"end\":\"33905\"},{\"start\":\"34146\",\"end\":\"34182\"},{\"start\":\"34520\",\"end\":\"34588\"},{\"start\":\"34968\",\"end\":\"35032\"},{\"start\":\"35297\",\"end\":\"35355\"},{\"start\":\"35689\",\"end\":\"35756\"},{\"start\":\"36003\",\"end\":\"36082\"},{\"start\":\"36469\",\"end\":\"36550\"},{\"start\":\"36831\",\"end\":\"36864\"},{\"start\":\"37587\",\"end\":\"37633\"},{\"start\":\"38049\",\"end\":\"38094\"},{\"start\":\"38323\",\"end\":\"38365\"},{\"start\":\"38703\",\"end\":\"38772\"}]", "bib_author": "[{\"start\":\"30334\",\"end\":\"30344\"},{\"start\":\"30344\",\"end\":\"30356\"},{\"start\":\"30356\",\"end\":\"30375\"},{\"start\":\"30375\",\"end\":\"30386\"},{\"start\":\"30704\",\"end\":\"30717\"},{\"start\":\"30717\",\"end\":\"30729\"},{\"start\":\"30729\",\"end\":\"30740\"},{\"start\":\"30740\",\"end\":\"30756\"},{\"start\":\"31078\",\"end\":\"31088\"},{\"start\":\"31088\",\"end\":\"31098\"},{\"start\":\"31431\",\"end\":\"31439\"},{\"start\":\"31439\",\"end\":\"31447\"},{\"start\":\"31447\",\"end\":\"31453\"},{\"start\":\"31453\",\"end\":\"31463\"},{\"start\":\"31779\",\"end\":\"31789\"},{\"start\":\"31789\",\"end\":\"31801\"},{\"start\":\"31801\",\"end\":\"31809\"},{\"start\":\"31809\",\"end\":\"31819\"},{\"start\":\"32050\",\"end\":\"32059\"},{\"start\":\"32059\",\"end\":\"32067\"},{\"start\":\"32067\",\"end\":\"32077\"},{\"start\":\"32472\",\"end\":\"32482\"},{\"start\":\"32639\",\"end\":\"32649\"},{\"start\":\"32739\",\"end\":\"32753\"},{\"start\":\"32753\",\"end\":\"32770\"},{\"start\":\"32770\",\"end\":\"32779\"},{\"start\":\"32779\",\"end\":\"32785\"},{\"start\":\"32785\",\"end\":\"32801\"},{\"start\":\"32801\",\"end\":\"32810\"},{\"start\":\"32810\",\"end\":\"32823\"},{\"start\":\"32823\",\"end\":\"32833\"},{\"start\":\"33167\",\"end\":\"33174\"},{\"start\":\"33174\",\"end\":\"33180\"},{\"start\":\"33180\",\"end\":\"33188\"},{\"start\":\"33409\",\"end\":\"33415\"},{\"start\":\"33415\",\"end\":\"33422\"},{\"start\":\"33422\",\"end\":\"33430\"},{\"start\":\"33652\",\"end\":\"33658\"},{\"start\":\"33658\",\"end\":\"33665\"},{\"start\":\"33665\",\"end\":\"33673\"},{\"start\":\"33907\",\"end\":\"33916\"},{\"start\":\"33916\",\"end\":\"33923\"},{\"start\":\"33923\",\"end\":\"33931\"},{\"start\":\"33931\",\"end\":\"33939\"},{\"start\":\"34184\",\"end\":\"34190\"},{\"start\":\"34190\",\"end\":\"34198\"},{\"start\":\"34198\",\"end\":\"34206\"},{\"start\":\"34206\",\"end\":\"34212\"},{\"start\":\"34212\",\"end\":\"34220\"},{\"start\":\"34590\",\"end\":\"34596\"},{\"start\":\"34596\",\"end\":\"34603\"},{\"start\":\"34603\",\"end\":\"34609\"},{\"start\":\"34609\",\"end\":\"34617\"},{\"start\":\"35034\",\"end\":\"35040\"},{\"start\":\"35040\",\"end\":\"35047\"},{\"start\":\"35047\",\"end\":\"35056\"},{\"start\":\"35056\",\"end\":\"35067\"},{\"start\":\"35357\",\"end\":\"35363\"},{\"start\":\"35363\",\"end\":\"35372\"},{\"start\":\"35372\",\"end\":\"35383\"},{\"start\":\"35758\",\"end\":\"35764\"},{\"start\":\"35764\",\"end\":\"35771\"},{\"start\":\"35771\",\"end\":\"35782\"},{\"start\":\"35782\",\"end\":\"35791\"},{\"start\":\"36084\",\"end\":\"36092\"},{\"start\":\"36092\",\"end\":\"36100\"},{\"start\":\"36100\",\"end\":\"36108\"},{\"start\":\"36108\",\"end\":\"36117\"},{\"start\":\"36117\",\"end\":\"36124\"},{\"start\":\"36552\",\"end\":\"36561\"},{\"start\":\"36561\",\"end\":\"36570\"},{\"start\":\"36866\",\"end\":\"36872\"},{\"start\":\"36872\",\"end\":\"36880\"},{\"start\":\"36880\",\"end\":\"36889\"},{\"start\":\"36889\",\"end\":\"36896\"},{\"start\":\"37215\",\"end\":\"37222\"},{\"start\":\"37222\",\"end\":\"37229\"},{\"start\":\"37229\",\"end\":\"37238\"},{\"start\":\"37238\",\"end\":\"37245\"},{\"start\":\"37245\",\"end\":\"37252\"},{\"start\":\"37252\",\"end\":\"37262\"},{\"start\":\"37635\",\"end\":\"37642\"},{\"start\":\"37642\",\"end\":\"37648\"},{\"start\":\"37648\",\"end\":\"37657\"},{\"start\":\"37657\",\"end\":\"37664\"},{\"start\":\"37664\",\"end\":\"37671\"},{\"start\":\"37671\",\"end\":\"37678\"},{\"start\":\"37678\",\"end\":\"37688\"},{\"start\":\"38096\",\"end\":\"38105\"},{\"start\":\"38367\",\"end\":\"38376\"},{\"start\":\"38376\",\"end\":\"38387\"},{\"start\":\"38774\",\"end\":\"38783\"},{\"start\":\"38783\",\"end\":\"38790\"},{\"start\":\"38790\",\"end\":\"38798\"},{\"start\":\"38798\",\"end\":\"38806\"},{\"start\":\"38806\",\"end\":\"38818\"},{\"start\":\"39242\",\"end\":\"39251\"},{\"start\":\"39251\",\"end\":\"39258\"},{\"start\":\"39258\",\"end\":\"39266\"}]", "bib_venue": "[{\"start\":\"31177\",\"end\":\"31239\"},{\"start\":\"31542\",\"end\":\"31604\"},{\"start\":\"32166\",\"end\":\"32238\"},{\"start\":\"34289\",\"end\":\"34341\"},{\"start\":\"34696\",\"end\":\"34758\"},{\"start\":\"35452\",\"end\":\"35504\"},{\"start\":\"36193\",\"end\":\"36245\"},{\"start\":\"36975\",\"end\":\"37037\"},{\"start\":\"37767\",\"end\":\"37829\"},{\"start\":\"38466\",\"end\":\"38528\"},{\"start\":\"38897\",\"end\":\"38959\"},{\"start\":\"30386\",\"end\":\"30447\"},{\"start\":\"30756\",\"end\":\"30817\"},{\"start\":\"31098\",\"end\":\"31175\"},{\"start\":\"31463\",\"end\":\"31540\"},{\"start\":\"31835\",\"end\":\"31856\"},{\"start\":\"32077\",\"end\":\"32164\"},{\"start\":\"32482\",\"end\":\"32516\"},{\"start\":\"32611\",\"end\":\"32637\"},{\"start\":\"32833\",\"end\":\"32882\"},{\"start\":\"33188\",\"end\":\"33225\"},{\"start\":\"33430\",\"end\":\"33468\"},{\"start\":\"33600\",\"end\":\"33650\"},{\"start\":\"33939\",\"end\":\"33978\"},{\"start\":\"34220\",\"end\":\"34287\"},{\"start\":\"34617\",\"end\":\"34694\"},{\"start\":\"35067\",\"end\":\"35105\"},{\"start\":\"35383\",\"end\":\"35450\"},{\"start\":\"35791\",\"end\":\"35830\"},{\"start\":\"36124\",\"end\":\"36191\"},{\"start\":\"36570\",\"end\":\"36629\"},{\"start\":\"36896\",\"end\":\"36973\"},{\"start\":\"37262\",\"end\":\"37373\"},{\"start\":\"37688\",\"end\":\"37765\"},{\"start\":\"38105\",\"end\":\"38168\"},{\"start\":\"38387\",\"end\":\"38464\"},{\"start\":\"38818\",\"end\":\"38895\"},{\"start\":\"39189\",\"end\":\"39240\"}]"}}}, "year": 2023, "month": 12, "day": 17}