{"id": 244908821, "updated": "2023-10-05 19:06:43.901", "metadata": {"title": "CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks", "authors": "[{\"first\":\"Oier\",\"last\":\"Mees\",\"middle\":[]},{\"first\":\"Lukas\",\"last\":\"Hermann\",\"middle\":[]},{\"first\":\"Erick\",\"last\":\"Rosete-Beas\",\"middle\":[]},{\"first\":\"Wolfram\",\"last\":\"Burgard\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "General-purpose robots coexisting with humans in their environment must learn to relate human language to their perceptions and actions to be useful in a range of daily tasks. Moreover, they need to acquire a diverse repertoire of general-purpose skills that allow composing long-horizon tasks by following unconstrained language instructions. In this paper, we present CALVIN (Composing Actions from Language and Vision), an open-source simulated benchmark to learn long-horizon language-conditioned tasks. Our aim is to make it possible to develop agents that can solve many robotic manipulation tasks over a long horizon, from onboard sensors, and specified only via human language. CALVIN tasks are more complex in terms of sequence length, action space, and language than existing vision-and-language task datasets and supports flexible specification of sensor suites. We evaluate the agents in zero-shot to novel language instructions and to novel environments and objects. We show that a baseline model based on multi-context imitation learning performs poorly on CALVIN, suggesting that there is significant room for developing innovative agents that learn to relate human language to their world models with this benchmark.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2112.03227", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/ral/MeesHRB22", "doi": "10.1109/lra.2022.3180108"}}, "content": {"source": {"pdf_hash": "4be02694125b71876552900a53c85c47a2a83614", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2112.03227v4.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2112.03227", "status": "GREEN"}}, "grobid": {"id": "f599bb798e13893195677181017e8dd65fb5b28a", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/4be02694125b71876552900a53c85c47a2a83614.txt", "contents": "\nCALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks Index Terms-Data Sets for Robot Learning, Machine Learning for Robot Control, Imitation Learning, Natural Dialog for HRI\n\n\nIeee \nLetters \nVersion \nMay \nCALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks Index Terms-Data Sets for Robot Learning, Machine Learning for Robot Control, Imitation Learning, Natural Dialog for HRI\nManuscript received: February, 23, 2022; Accepted May, 22, 2022.1 This paper was recommended for publication by Associate Editor S. Chernova and Editor D. Kulic upon evaluation of the reviewers' comments. * Equal contribution. Digital Object Identifier (DOI): see top of this page. including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.\nFig. 1: CALVIN is a benchmark to learn many long-horizon language-conditioned tasks over a range of four manipulation environments, designed to be diverse yet carry shared structure, from multimodal onboard sensor observations. In the most difficult evaluation, the methods must generalize to unseen entities by training on a large interaction corpora covering three environments and testing on an unseen scene.Abstract-General-purpose robots coexisting with humans in their environment must learn to relate human language to their perceptions and actions to be useful in a range of daily tasks. Moreover, they need to acquire a diverse repertoire of general-purpose skills that allow composing long-horizon tasks by following unconstrained language instructions. In this paper, we present CALVIN (Composing Actions from Language and Vision), an open-source simulated benchmark to learn longhorizon language-conditioned tasks. Our aim is to make it possible to develop agents that can solve many robotic manipulation tasks over a long horizon, from onboard sensors, and specified only via human language. CALVIN tasks are more complex in terms of sequence length, action space, and language than existing vision-and-language task datasets and supports flexible specification of sensor suites. We evaluate the agents in zeroshot to novel language instructions and to novel environments. We show that a baseline model based on multi-context imitation learning performs poorly on CALVIN, suggesting that there is significant room for developing innovative agents that learn to relate human language to their world models with this benchmark.\n Fig. 1\n: CALVIN is a benchmark to learn many long-horizon language-conditioned tasks over a range of four manipulation environments, designed to be diverse yet carry shared structure, from multimodal onboard sensor observations. In the most difficult evaluation, the methods must generalize to unseen entities by training on a large interaction corpora covering three environments and testing on an unseen scene.\n\nAbstract-General-purpose robots coexisting with humans in their environment must learn to relate human language to their perceptions and actions to be useful in a range of daily tasks. Moreover, they need to acquire a diverse repertoire of general-purpose skills that allow composing long-horizon tasks by following unconstrained language instructions. In this paper, we present CALVIN (Composing Actions from Language and Vision), an open-source simulated benchmark to learn longhorizon language-conditioned tasks. Our aim is to make it possible to develop agents that can solve many robotic manipulation tasks over a long horizon, from onboard sensors, and specified only via human language. CALVIN tasks are more complex in terms of sequence length, action space, and language than existing vision-and-language task datasets and supports flexible specification of sensor suites. We evaluate the agents in zeroshot to novel language instructions and to novel environments. We show that a baseline model based on multi-context imitation learning performs poorly on CALVIN, suggesting that there is significant room for developing innovative agents that learn to relate human language to their world models with this benchmark.\n\n\nI. INTRODUCTION\n\nA LONG-STANDING goal for robotics and embodied agents is to build systems that can perform tasks specified in natural language. Concepts expressed in natural language provide humans with an intuitive way to represent, summarize, and abstract diverse knowledge skills. By means of abstraction, concepts such as \"open the drawer and push the middle object into the drawer\" can be extended to a potentially infinite set of new and unseen entities. Additionally, humans leverage concepts to describe complex tasks as sequences of natural language instructions. This stands in contrast to current robots, which typically lack this generalization ability and learn individual tasks one at a time. Moreover, multi-task learning approaches traditionally assume that tasks are specified to the agent at test time via mechanisms such as goal images [1] and one-hot skill selectors [2], [3] that are not practical for non-expert users to instruct robots in everyday real-world settings. As robots become ubiquitous across human-centered environments the need for intuitive task specification grows: how can we scale robot learning systems to autonomously acquire general-purpose knowledge that allows them to compose long-horizon tasks by following unconstrained language instructions?\n\nTo address this problem we present CALVIN, a new opensource simulated benchmark that links human language to robot motor skills, behaviors, and objects in interactive visual environments. In this setting, a single agent must solve complex manipulation tasks by understanding a series of unconstrained language expressions in a row, e.g., \"open the drawer . . . pick up the blue block . . . push the block into the drawer . . . open the sliding door\". Furthermore, to evaluate the agents' ability for long-horizon planning, agents in this scenario are expected to be able to perform any combination of subtasks in any order. CALVIN has been developed from the ground up to support training, prototyping, and validation of language-conditioned continuous control policies over a range of four indoor manipulation environments, visualized in Figure  1. CALVIN includes \u223c24 hours teleoperated unstructured play data together with 20K language directives. Unscripted playful interactions have the advantage of being task-agnostic, diverse, and relatively cheap to obtain [1], [4]. The simulation platform supports a range of sensors commonly utilized for visuomotor control: RGB-D images from both a static and a gripper camera, proprioceptive information, and vision-based tactile sensing [5]. We believe that this flexible sensor suite will allow researchers to develop improved multimodal agents that can solve many tasks in real-world settings. This is the first public benchmark of instruction following, to our knowledge, that combines: natural language conditioning, multimodal highdimensional inputs, 7-DOF continuous control, and longhorizon robotic object manipulation. We provide an evaluation protocol with evaluation modes of varying difficulty by choosing different combinations of sensor suites and amounts of training environments. This effort joins the recent efforts to standardize robotics research for better benchmarks and more reproducible results. To open the door for future development of agents that can generalize abstract concepts to unseen entities the same way humans do, we include a challenging zeroshot evaluation by training on large play corpora covering three environments and testing on an unseen scene. The language instructions used for testing are not included in the training set and represent novel ways of describing the manipulation tasks seen during training.\n\nTo establish baseline performance levels, we evaluate the multi-context imitation learning (MCIL) approach that uses relabeled imitation learning to distill many reusable behaviors into a goal-directed policy [6]. This model is not effective on the complex long horizon robot manipulation tasks in CALVIN. While it achieves up to 53.9% success rate in short horizon tasks, it performs poorly in the long-horizon setting. We note that there is no constraint to use imitation learning approaches to solve CALVIN tasks, as approaches that use reinforcement learning to learn language-conditioned policies can also be applied [7].\n\nIn summary, CALVIN facilitates learning models that translate from language to sequences of motor skills in a realistic simulation environment. This benchmark captures many challenges present in real-world settings for relating human language to robot actions and perception for accomplishing long-horizon manipulation tasks. Models that can overcome these challenges will begin to close the gap towards scalable, general-purpose, language-driven robotics.\n\n\nII. RELATED WORK\n\nNatural language processing has recently received much attention in the field of robotics [8], following the advances made towards learning groundings between vision and language [9], [10]. Recent successes in human-robot interaction include an interactive fetching system to localize objects mentioned in referring expressions [11]- [15] or grounding not only objects, but also spatial relations to follow language expressions characterizing pick-and-place commands [16]- [18]. By contrast, CALVIN tasks require grounding language to a wide variety of general-purpose robot skills. Prior work on mapping language and vision to actions has been studied mostly in restricted environments [19], [20] and simplified actuators with discrete motion primitives [21]- [23]. A growing body of work also looks at learning language-conditioned policies for continuous visuomotor-control in 3D environments via imitation learning [6], [24], [25] or reinforcement learning [7], [26], [27]. These approaches typically require offline data sources of robotic interaction, such as teleoperation or autonomous exploration data, together with post-hoc crowd-sourced language labels. However, the lack of standardized benchmarks and algorithm implementations, makes it difficult to compare approaches and to facilitate future research.\n\nThe most closely related benchmark to ours is AL-FRED [22], which contains language instructions for combined navigation and manipulation tasks with seven predefined action primitives. In CALVIN, rather than classifying predefined actions, the agent must learn to acquire a diverse repertoire of general-purpose skills that allows composing long-horizon tasks by following unconstrained language instructions in closed loop control. Our tabletop environments are inspired by the one shown in Lynch et al. [6] in order to have a fair comparison to their MCIL approach, which we implement to establish baseline performance levels. We note that although considered a state-of-the-art approach, no public implementation of MCIL is available. In contrast to their work, CALVIN contains more subtasks (34 vs 18), longer longhorizon evaluation sequences (5 vs 4), provides a range of sensors commonly utilized for visuomotor control and allows testing zero-shot generalization by leveraging a range of four manipulation environments and unseen language instructions. Finally, CALVIN goes beyond the original MCIL setup by adding a challenging visual grounding problem, where similar language instructions for differently colored blocks are given and the agent needs to identify which block is meant.\n\n\nIII. CALVIN\n\nThe aim of the CALVIN benchmark is to evaluate the learning of long-horizon language-conditioned continuous control policies. In this setting, a single agent must solve complex manipulation tasks by understanding a series of unconstrained language expressions in a row, e.g., \"open the drawer. . . pick up the blue block. . . now push the block into the drawer. . . now open the sliding door\". We note that in the benchmark we  only allow feasible sequences that can be achieved from a predefined initial environment state. The CALVIN benchmark consists of three key components, which are:\n\n\n1) CALVIN Environment 2) CALVIN Dataset 3) CALVIN Challenge\n\n\nA. The CALVIN Environment\n\nCALVIN features four different, yet structurally related environments (A, B, C, D) so that it can be used for general playing as well as evaluating specific tasks. The environments contain a 7-DOF Franka Emika Panda robot arm with a parallel gripper and a desk with a sliding door and a drawer that can be opened and closed. On the desk, there is a button that toggles a green light and a switch to control a light bulb. Besides, there are three different colored and shaped rectangular blocks. To better evaluate the generalization capabilities of the learned language groundings, all environments have different textures and all static elements such as the sliding door, the drawer, the light button, and switch are positioned differently. The position of the desk, robot, and the static camera is the same in all environments. Due to the general difficulty of languageconditioned multi-task closed-loop control, we reduced the complexity of the objects to unicolored primitive shapes. If future advances in this field require new challenges we will reflect this by extending CALVIN to environments with more realistic and diverse objects. Physics are simulated using the PyBullet physics engine [28], which supports fast GPU rendering for large-scale parallel data collection.\n\n1) Observation and Action Space: Unlike prior work which relies on RGB images from an egocentric camera to perceive its surroundings [1], [6], CALVIN offers a range of sensors that can be used to develop and prototype agents that learn task-agnostic control in the real world. Concretely, the agent perceives its surroundings from RGB-D images from both a fixed and a gripper camera. It additionally has access to a vision-based tactile sensor [5] and to continuous internal proprioceptive sensors. A visualization of the supported sensor modalities is shown in Figure 3. The agent must perform closed-loop continuous control to follow unconstrained language instructions characterizing complex robot manipulation tasks, sending continuous actions to the robot at 30hz. In order to give researchers and practitioners the freedom to experiment with different action spaces, CALVIN supports absolute and relative cartesian actions, as well as actions in joint space. We encourage the community to study flexible combinations of observation and action spaces since the tasks require a varying degree of precise control vs. coarse locomotion. While the static camera and absolute cartesian actions are the natural choices for tasks that call for a complete traversal of the environment from one side to another, the gripper camera and relative actions (w.r.t to the gripper frame) allow more fine-grained control for tasks like stacking or grasping. Tactile information can become important when the task requires the robot to maintain a stable grasp on the handle while moving the sliding door to the side. See Fig. 2 for a description of the observation and action dimensionalities.\n\n2) Tasks: We define 34 specific tasks (see Fig. 4) that can be achieved in each one of the environments The environment has the functionality to automatically detect which one of the tasks has been completed in a sequence of steps, which can serve as a sparse reward for reinforcement learning agents. The criterion for task completion is defined in terms of a change in the environment state between the initial and final step of a sequence. This also enables the automatic task detection in any variable-length sequence of offline data, since the environment can be reset to the state of each one of the recorded frames.\n\nB. The CALVIN Dataset 1) Unstructured Demonstrations: Learning generally requires exposure to diverse training data. To effectively cover state space, we collect twenty-four hours of teleoperated \"play\"\n\n\nTask\n\nNatural language instructions rotate red block right \"rotate the red block 90 degrees to the right\" \"turn the red block right\" push blue block left \"go slide the blue block to the left\" \"push left the blue block\" move slider left \"grasp the door handle, then slide the door to the left\" \"slide the door to the left\" open drawer \"grasp the handle of the drawer and open it\" \"go open the drawer\" lift red block \"lift the red block from the table\" \"pick up the red block\" pick pink block \"pick up the pink from drawer block lying in the drawer\" place in slider \"put the grasped object in the slider\" stack blocks \"stack blocks on top of each other\" unstack blocks \"collapse the stacked blocks\" \"go to the tower of blocks and take off the top one\" turn on light bulb \"toggle the light switch to turn on the light bulb\" turn off green light \"push the button to turn off the green light\" data in four environments with a HTC Vive VR headset, spending an approximately equal time of six hours in each environment. This corresponds to \u223c2.4M interaction steps and \u223c40M short-horizon windows for relabeled goal conditioned imitation learning [29], [30], each spanning 1-2 seconds. In this setting, an operator is not constrained to a set of predefined tasks, but rather engages in behavior that satisfies their own curiosity or some other intrinsic motivation. Unscripted playful interactions have the advantage of being task-agnostic, diverse, and relatively cheap to obtain [1], [4]. We asked three people to collect data, and these users were untrained and given no information about the downstream tasks. The only guideline we gave data collectors was to \"explore the environment without dropping objects from the table\". This includes picking up and placing objects, opening, and closing drawers, sliding doors, pushing buttons, operating switches and undirected actions. This style of data is very different from commonly used task-specific data, which only consists of expert trajectories. Playful interaction data by design is free-form, so there are no categories associated with the data. This kind of unstructured data is useful because it contains exploratory and sub-optimal behaviors that are critical to learning generalizable and robust representations, e.g., enabling retrying behavior. While expert demonstrations often only show one of the many possible ways to solve a task, play data is richer in the sense that it covers the multimodal space of possible solutions. However, as opposed to expert demonstrations, in play data some task instances naturally occur less frequently than others, especially those that have the completion of another task as a prerequisite.\n\n2) Language Instructions: Approaches that learn languageconditioned continuous control policies typically require posthoc crowd-sourced natural language labels aligned with its corresponding robot interaction data [6], [7]. Instead of relying entirely on crowd-sourced annotations, we collect over 400 crowd-sourced natural language instructions corresponding to over 34 tasks and label episodes procedurally using the recorded environment state of the CALVIN dataset. We note that using this labeling scheme, only sequences that display meaningful skills are labeled with language annotations. We visualize example language annotations in Fig. 4. In order to simulate a real-world scenario where it might not be possible to pair all the collected robot experience with crowdsourced language annotations, we annotate only 1% of the recorded robot interaction data with language instructions. Besides language instructions, we provide precomputed language embeddings extracted from MiniLM [31]. MiniLM distills a large Transformer based language model and is trained on generic language corpora (e.g., Wikipedia). It has a vocabulary size of 30,522 words and maps a sentence of any length into a vector of size 384. We note that there exist many choices for encoding raw text in a semantic pre-trained vector space and encourage the community to experiment with different choices to solve for CALVIN tasks.\n\n\nC. The CALVIN Challenge\n\n\nCALVIN combines the challenging settings of open-ended\n\nrobotic manipulation with open-ended human language conditioning. For example, a robot that is instructed to \"place the blue block inside the drawer\" must be able to relate language to its world model. Concretely, it needs to learn to identify how a blue block and a drawer look like in its multimodal perceptual observations 1 , and then it needs to reason over the best sequence of actions to \"place inside the drawer\". Ideally, a general-purpose robot should be able to perform any combination of tasks instructed with natural language in any order. Thus, to accelerate progress in language-driven robotics, we present a set of evaluation protocols of varying difficulty by choosing different combinations of sensor suites and amounts of training environments.\n\n1) Training and Test Environments: CALVIN offers three combinations of training and test environments with varying difficulty:\n\nSingle Environment: Training in a single environment and evaluating the policy in the same environment. This corresponds to the setting of Lynch et al. [6].\n\nMulti Environment: Training in all four environments and evaluating the policy in one of them. This poses an additional challenge since the policy has to generalize to multiple textures Long-horizon language instructions \"turn on the led\" \u2192 \"open drawer\" \u2192 \"push the blue blue block \u2192 \"pick up the blue block \" \u2192 \"place in slider\" \"move slider left\" \u2192 \"lift red block from slider\" \u2192 \"stack blocks\" \u2192 \"toggle light\" \u2192 \" collapse stacked blocks\" \"open drawer\" \u2192 \"push block in drawer\" \u2192 \"pick object from drawer\" \u2192 \"stack blocks\" \u2192 \"close drawer\" Fig. 5: Example long-horizon language tasks sequences evaluated in CALVIN. We show the abbreviated subtask names instead of the full language annotations due to space constraint. and different locations of the sliding door, button, and switch. On the other hand, the agents can benefit from increased data.\n\n\nZero-Shot Multi Environment:\n\nTo open the door for future development of agents that can generalize abstract concepts to unseen entities the same way humans do, we include a challenging zero-shot evaluation by training in three environments and evaluating the policy in the fourth unseen one. This is the hardest combination since the policy has never seen the test environment during training. However, all elements of the scene were present in different locations in the training environments. While highly challenging, we believe it aligns well with test-time expectations for service robots to be useful in a range of daily tasks in everyday environments. Concretely, in CALVIN agents need to generalize to a room where the environment has different textures and all static elements such as the sliding door, the drawer and the light turning button and switch are positioned differently. Thus, a language-conditioned policy should ideally be able to open a sliding door even if it is differently positioned or looks visually a bit different.\n\n2) Evaluation Metrics: All three environment combinations are evaluated with the following metrics:\n\nMulti-Task Language Control (MTLC): The simplest evaluation aims to verify how well the learned multi-task language-conditioned policy generalizes to 34 manipulation tasks, which we visualize in Fig. 6. The evaluation begins by resetting the simulator to the first state of a valid unseen demonstration, to ensure that the commanded instruction is valid. For each manipulation task 10 rollouts are performed with their corresponding different starting states. The language instructions used for testing are not included in the training set and represent novel ways of describing the manipulation tasks seen during training.\n\nLong-Horizon MTLC (LH-MTLC): This evaluation aims to verify how well the learned multi-task language-conditioned policy can accomplish several language instructions in a row. This setting is very challenging as it requires agents to be able to transition between different subgoals. We treat the 34 tasks of the previous evaluation as subgoals and compute valid sequences consisting of five sequential tasks. We only allow feasible sequences that can be achieved from a predefined initial environment state. We filter the evaluation sequences for cycles, redundancies and similarities to arrive at 1000 unique instruction chains. Examples for excluded sequences are \"close the drawer\". . . \"place in drawer\" (unfeasible), \"move slider right\". . . \"move slider left\". . . \"move slider right\" (cyclic) or \"push blue block left\". . . \"push red block left\"(similar). We reset the robot to a neutral position after every sequence to avoid biasing the policies through the robot's initial pose. We note that this neutral initialization breaks correlation between Task Condition Rotate red/blue/pink block right\n\nThe object has to be rotated clockwise more than 60\u00b0around the z-axis while not being rotated more than 30\u00b0around the x or y-axis.\n\n\nRotate red/blue/pink block left\n\nThe object has to be rotated counterclockwise more than 60\u00b0around z while not being rotated more than 30\u00b0around the x or y-axis. Push red/blue/pink block right\n\nThe object has to move more than 10 cm to the right while having surface contact in both frames.\n\n\nPush red/blue/pink block left\n\nThe object has to move more than 10 cm to the left while having surface contact in both frames.\n\n\nMove slider left/right\n\nThe sliding door has to be pushed at least 12 cm to the left/right. Open/close drawer The drawer has to be pushed in/pulled out at least 10 cm. Lift red/blue/pink block table\n\nThe object has to be grasped from the table surface and lifted at least 5 cm high. In the first frame the gripper may not touch the object. Lift red/blue/pink block slider\n\nThe object has to be grasped from the sliding cabinet's surface and lifted at least 3 cm. In the first frame the gripper may not touch the object. Lift red/blue/pink block drawer\n\nThe object has to be grasped from the drawer's surface and lifted at least 5 cm high. In the first frame the gripper may not touch the object.\n\n\nPlace in slider/drawer\n\nThe object has to be placed in the sliding cabinet/drawer. It must be lifted by the gripper in the first frame.\n\n\nPush into drawer\n\nThe object has to be pushed into the drawer. It has to touch the table surface in the first frame.\n\n\nStack blocks\n\nA block has to be placed on top of another block. It may not be in contact with the gripper in the final frame. Unstack blocks A block has to be removed from the top of another block. It may not be in contact with the gripper in the first frame. Turn on/off light bulb\n\nThe switch has to be pushed up/down to turn on/off the yellow light bulb. Turn on/off LED The button has to be pressed to turn on/turn off the green LED light. initial state and task, forcing the agent to rely entirely on language to infer and solve the task. We include different initial scene configurations in the evaluation to better evaluate generalization capabilities. We visualize the evaluated subtask distribution in Figure 7. For each subtask we condition the policy on the current language instruction and transition to the next subgoal only if the agent successfully completes the current task according to the environments state indicator.\n\n\n3) Sensor Combinations:\n\nThe aim of CALVIN is to develop innovative agents that learn to relate human language from onboard sensors by capturing many challenges present in realworld settings. Most autonomous robots operating in complex   environments are equipped with different sensors to perceive their surroundings. To foster development and experimentation of language-conditioned policies that perform manipulation tasks in the real-world, CALVIN supports a range of sensors commonly utilized for visuomotor control: RGB-D images from both a static and a gripper camera, proprioceptive information, and vision-based tactile sensing [5]. We therefore evaluate baseline agents for different sensors combinations.\n\n\nIV. BASELINE MODELS\n\nAn agent trained for CALVIN needs to jointly reason over perceptual and language input and produce a sequence of lowlevel motor commands to interact with the environment.\n\n\nA. Multicontext Imitation Learning\n\nWe model the interactive agent with a general-purpose goal-reaching policy based on multi-context imitation learning (MCIL) from play data [6]. To learn from unstructured \"play\" we assume access to an unsegmented teleoperated play dataset D of semantically meaningful behaviors provided by users, without a set of predefined tasks in mind. To learn control, this long temporal state-action stream D = {(x t , a t )} \u221e t=0 is relabeled [30], treating each visited state in the dataset as a \"reached goal state\", with the preceding states and actions treated as optimal behavior for reaching that goal. Relabeling yields a dataset of D play = {(\u03c4, x g ) i } Dplay i=0 where each goal state x g has a trajectory demonstration \u03c4 = {(x 0 , a 0 ), . . .} solving for the goal. These short horizon goal image conditioned demonstrations can be fed to a simple maximum likelihood goal conditioned imitation objective:\nL Lf P = E (\u03c4,xg)\u223cDplay \uf8ee \uf8f0 |\u03c4 | t=0 log \u03c0 \u03b8 (a t | x t , x g ) \uf8f9 \uf8fb\n(1)\n\nto learn a goal-reaching policy \u03c0 \u03b8 (a t | x t , x g ). Multi-context imitation learning addresses the inherent multi-modality in free-form imitation datasets by auto-encoding contextual demonstrations through a latent \"plan\" space with an sequenceto-sequence conditional variational auto-encoder (seq2seq CVAE). The decoder is a policy trained to reconstruct input actions, conditioned on state x t , goal x g , and an inferred plan z for how to get from x t to x g . At test time, it takes a goal as input, and infers and follows plan z in closed-loop. However, when learning language-conditioned policies \u03c0 \u03b8 (a t | x t , l) it is not possible to relabel any visited state x to a natural language goal as the goal space is no longer equivalent to the observation space. Lynch et al. [6] showed that pairing a small number of random windows with language after-thefact instructions enables learning a single language-conditioned visuomotor policy that can perform a wide variety of robotic manipulation tasks. The key insight here is that solving a single imitation learning policy for either goal image or language goals, allows for learning control mostly from unlabeled play data and reduces the burden of language annotation to less than 1% of the total data. Concretely, given multiple contextual imitation datasets D = {D 0 , D 1 , . . . , D K }, with a different way of describing tasks, MCIL trains a single latent goal conditioned policy \u03c0 \u03b8 (a t | x t , z) over all datasets simultaneously, as well as one parameterized encoder per dataset.\n\n\nB. Implementation Details\n\nWe follow the baseline architecture implementation reported by Lynch et al. [6] unless stated otherwise. We train the agent with the Adam optimizer and a learning rate of 10 \u22124 . We set the weight controlling the influence of the KL divergence to the total loss to \u03b2 = 0.001. During training, we randomly sample windows between length 16 and 32 and pad them until the max length of 32. As in the original implementation, no image data augmentations are applied and absolute cartesian actions w.r.t the world frame are used. The encoder for the gripper camera takes an image of 84 \u00d7 84 as input and consists of 3 convolutional layers with 32, 64, and 64 channels followed by a 128 unit ReLU MLP. The encoder for the visual-tactile sensor is based on a pre-trained ResNet-18 model. The feature vectors produced by the different modality encoders are concatenated. Depth images are concatenated channel-wise with the RGB images in an early-fusion fashion. In contrast to [6], the gripper fingers of the robot in the CALVIN environment cannot be controlled independently, reducing the action output of the network by one dimension. We note that the same training hyperparameters are used for all splits.\n\n\nV. EXPERIMENTAL RESULTS\n\nThe results comparing language-conditioned policies based on multicontext imitation learning for the different evaluation modes in CALVIN are shown in Figure 8. We note that there is no constraint to use imitation learning approaches to solve CALVIN tasks, as approaches that use reinforcement learning to learn language-conditioned policies can also be applied [7]. We observe that the baseline with images of the static camera achieves a success rate of 53.9% for the MTLC evaluation setting, when training and testing the 34 manipulation tasks on the same environment. The success rate stays comparable when including a gripper camera, depth channels or tactile sensing. We hypothesize that the reason for not seeing larger improvements when adding the gripper camera is that the policy might benefit from using relative actions instead of Input global actions. A qualitative analysis indicates that the performance depends significantly on the initial position of the robot, suggesting the agent relies on context rather than learning to disentangle initial states and tasks. It is possible this is due to causal confusion between the proprioceptive information and the target actions [32]. Besides, we did not use image data augmentations in the baselines to stay close to the original implementation, but we hypothesize this might be beneficial. Additionally, more elaborate sensor fusion approaches such as mixture of experts [33], [34] or view-invariant contrastive learning [35], [36] might be necessary to learn better multimodal state representations.\n\nFor the Long-Horizon MTLC evaluation we observe that the agents perform poorly on CALVIN's long-horizon tasks with high-dimensional state spaces. The best MCIL model achieves a success rate of 0.08% when following chains of five language instructions in a row when training and testing on the same environment. Additionally, it solves the first subtask of the chain, starting from a neutral position, in 48.9% of the cases. We observe that the policy sometimes correctly executes block manipulation tasks, but confuses the red and blue block colors in the instruction. As the language models embed sentences containing the words red and blue similarly, backpropagating through the entire language model and leveraging auxiliary losses that try to align visual and language representations [37] might be beneficial to tackle the complicated perceptual grounding problem.\n\nFinally, the general performance drops significantly when evaluating on the multi environment and zero-shot multi environment settings, which do not follow the standard assumption of imitation learning that training and test tasks are drawn independently from the same distribution. In order to achieve better zero-shot generalization capabilities, additional techniques from the domain adaptation literature [36], better data augmentation and a stronger focus on depth inputs, since they are invariant to texture changes, might be helpful. As MCIL is an offline learning method, we hypothesize that na\u00efve data sharing between multiple domains can be brittle because it can exacerbate the distribution shift between the policy represented in the data and the policy being learned [38]. This motivates further research into agents that can perform the complex longhorizon language-conditioned manipulation tasks introduced by CALVIN.\n\nVI. CONCLUSION In this paper, we presented CALVIN, the first public benchmark of instruction following that combines natural language conditioning, multimodal high-dimensional inputs, 7-DOF continuous control, and long-horizon robotic object manipulation in both seen and unseen environments. As the field of language-driven robotics evolves, a need arises to standardize research for better benchmarks and more reproducible results. CALVIN has the goal of providing researchers with a modular framework that has been developed from the ground up to support training, prototyping, and validation of languageconditioned continuous control policies. Further to that, we hope, along with the help of the community, to continuously expand the tasks available for both training and evaluation.\n\nWe use CALVIN to evaluate a conditional sequenceto-sequence variational autoencoder, shown to be effective in other long horizon language-conditioned manipulation tasks [6]. While this model is relatively competent at accomplishing some subgoals, the overall success rates are poor. The long horizon of CALVIN tasks poses a significant challenge with sub-problems including the acquisition of a diverse repertoire of general-purpose skills, object detection, referring expression and action grounding, and task-agnostic continuous control. We hope CALVIN will open the door for the future development of agents that can relate human language to their perception and actions and generalize abstract concepts to unseen entities in the same way humans do.\n\nACKNOWLEDGEMENT This work was supported by the German Federal Ministry of Education and Research under contract 01IS18040B-OML. We thank Corey Lynch and Pierre Sermanet for help with the MCIL baseline.\n\n\nAPPENDIX\n\n\nA. Tasks\n\nAll tasks are defined in terms of change in the environment state between the first and the final frame of a sequence. In order to see if a task was solved in an arbitrary sequence of frames of the CALVIN dataset, the environment is reset to the state of the first and the last frame of that sequence. The tasks detector compares the two simulator states and checks which task conditions are fulfilled. A key advantage of this strategy is that it enables efficient evaluation of sequences for task completion independent of their length. Figure 9 shows a list of all task definitions.\n\n\nTask\n\nCondition Rotate red block right Rotate blue block right Rotate pink block right\n\nThe object has to be rotated clockwise more than 60\u00b0around the z-axis while not being rotated for more than 30\u00b0around the x or y-axis.\n\n\nRotate red block left Rotate blue block left Rotate pink block left\n\nThe object has to be rotated counterclockwise more than 60\u00b0around the z-axis while not being rotated for more than 30\u00b0around the x or y-axis.\n\nPush red block right Push blue block right Push pink block right\n\nThe object has to move more than 10 cm to the right while having surface contact in both frames\n\n\nPush red block left Push blue block left Push pink block left\n\nThe object has to move more than 10 cm to the left while having surface contact in both frames\n\n\nMove slider left\n\nThe sliding door has to be pushed at least 12 cm to the left.\n\n\nMove slider right\n\nThe sliding door has to be pushed at least 12 cm to the right.\n\n\nOpen drawer\n\nThe drawer has to pulled out at least 10 cm.\n\n\nClose drawer\n\nThe drawer has to be pushed in at least 10 cm. Lift red block table  Lift blue block table  Lift pink block table The object has to be grasped from the table surface and lifted at least 5 cm high. In the first frame the gripper may not touch the object.\n\n\nLift red block slider Lift blue block slider Lift pink block slider\n\nThe object has to be grasped from the surface of the sliding cabinet and lifted at least 3 cm high. In the first frame the gripper may not touch the object.\n\n\nLift red block drawer Lift blue block drawer Lift pink block drawer\n\nThe object has to be grasped from the surface of the drawer and lifted at least 5 cm high. In the first frame the gripper may not touch the object.\n\n\nPlace in slider\n\nThe object has to be placed in the sliding cabinet. It must be lifted by the gripper in the first frame.\n\n\nPlace in drawer\n\nThe object has to be placed in the drawer. It must be lifted by the gripper in the first frame.\n\n\nPush into drawer\n\nThe object has to be pushed into the drawer. It has to touch the table surface in the first frame.\n\nStack blocks A block has to be placed on top of another block. It may not be in contact with the gripper in the final frame.\n\n\nUnstack blocks\n\nA block that is stacked on another block has to be removed from the top, either by grasping it or by pushing it down. It may not be in contact with the gripper in the first frame.\n\n\nTurn on light bulb\n\nThe switch has to be pushed down to turn on the yellow light bulb.\n\n\nTurn off light bulb\n\nThe switch has to be pushed up to turn off the yellow light bulb.\n\n\nTurn on LED\n\nThe button has to be pressed to turn on the green LED light.\n\n\nTurn off LED\n\nThe button has to be pressed to turn off the green LED light. \n\n\nB. Language Annotation Generation\n\nThe language annotations are extracted automatically from the recorded data with the following procedure: we randomly sample sequences with a window size of 64 frames. For each sequence the task detector checks if a task has been solved between the first and the last frame. Additionally, we check that neither that task nor any other task is solved in the first half of the sequence. The intuition behind this is that we want to include the locomotion behavior prior to the actual task. For example, before opening the drawer, the arm must navigate in the direction of the handle. This is important for learning to solve tasks with language goals from arbitrary starting positions. If a sequence qualifies for labeling, we sample a natural language instruction from a set of predefined sentences with approximately 11 synonymous instructions per task. In total, this gives 389 unique language instructions for 34 tasks. The sequence in which the task \"stack blocks\" is solved could for example get instructions such as \"place the grasped block on top of another block\" or \"stack blocks on top of each other\". In order to simulate a real-world scenario where it might not be possible to pair all the collected robot experience with crowd-sourced language annotations, we annotate only 1% of the recorded robot interaction data with language instructions. The CALVIN dataset conveniently includes precomputed MiniLM language embeddings for all instructions, but researchers are free to use their own language model of choice on the raw input data.\n\nFig. 2 :\n2Observation and action spaces supported by CALVIN.\n\nFig. 3 :\n3CALVIN supports a range of sensors commonly utilized for visuomotor control: RGB-D images from both a static and a gripper camera, proprioceptive information, and vision-based tactile sensing (bottom-left).\n\nFig. 4 :\n4Example crowd-sourced natural language instructions to specify manipulation tasks in CALVIN.\n\nFig. 6 :\n6List of all 34 tasks with their respective success criteria.\n\nFig. 7 :\n7Visualization of the subtask distribution across the 1000 instruction chains used for the Long Horizon MTLC evaluation. We show the percentage in which each subtask appears in the distribution.\n\nFig. 9 :\n9List of all 34 tasks with their respective success criteria.\nSimulator states consisting of object positions and orientations are also provided, but not used to better capture challenges of real-world settings.\n\nLearning latent plans from play. C Lynch, M Khansari, T Xiao, V Kumar, J Tompson, S Levine, P Sermanet, CoRLC. Lynch, M. Khansari, T. Xiao, V. Kumar, J. Tompson, S. Levine, and P. Sermanet, \"Learning latent plans from play,\" in CoRL, 2019.\n\nMeta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. T Yu, D Quillen, Z He, R Julian, K Hausman, C Finn, S Levine, CoRLT. Yu, D. Quillen, Z. He, R. Julian, K. Hausman, C. Finn, and S. Levine, \"Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning,\" in CoRL, 2019.\n\nScaling up multi-task robotic reinforcement learning. D Kalashnikov, J Varley, Y Chebotar, B Swanson, R Jonschkowski, C Finn, S Levine, K Hausman, CoRLD. Kalashnikov, J. Varley, Y. Chebotar, B. Swanson, R. Jonschkowski, C. Finn, S. Levine, and K. Hausman, \"Scaling up multi-task robotic reinforcement learning,\" in CoRL, 2021.\n\nPlayful interactions for representation learning. S Young, J Pari, P Abbeel, L Pinto, arXiv:2107.09046arXiv preprintS. Young, J. Pari, P. Abbeel, and L. Pinto, \"Playful interactions for representation learning,\" arXiv preprint arXiv:2107.09046, 2021.\n\nTacto: A fast, flexible and open-source simulator for high-resolution vision-based tactile sensors. S Wang, M Lambeta, P.-W Chou, R Calandra, arXiv:2012.08456arXiv preprintS. Wang, M. Lambeta, P.-W. Chou, and R. Calandra, \"Tacto: A fast, flexible and open-source simulator for high-resolution vision-based tactile sensors,\" arXiv preprint arXiv:2012.08456, 2020.\n\nLanguage conditioned imitation learning over unstructured data. C Lynch, P Sermanet, RSS. C. Lynch and P. Sermanet, \"Language conditioned imitation learning over unstructured data,\" in RSS, 2021.\n\nLearning language-conditioned robot behavior from offline data and crowd-sourced annotation. S Nair, E Mitchell, K Chen, B Ichter, S Savarese, C Finn, CoRLS. Nair, E. Mitchell, K. Chen, B. Ichter, S. Savarese, and C. Finn, \"Learning language-conditioned robot behavior from offline data and crowd-sourced annotation,\" in CoRL, 2021.\n\nRobots that use language. S Tellex, N Gopalan, H Kress-Gazit, C Matuszek, Robotics, and Autonomous Systems. 3Annual Review of ControlS. Tellex, N. Gopalan, H. Kress-Gazit, and C. Matuszek, \"Robots that use language,\" Annual Review of Control, Robotics, and Autonomous Systems, vol. 3, pp. 25-55, 2020.\n\nReferitgame: Referring to objects in photographs of natural scenes. S Kazemzadeh, V Ordonez, M Matten, T Berg, EMNLP. S. Kazemzadeh, V. Ordonez, M. Matten, and T. Berg, \"Referitgame: Referring to objects in photographs of natural scenes,\" in EMNLP, 2014.\n\nVilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. J Lu, D Batra, D Parikh, S Lee, NeurIPSJ. Lu, D. Batra, D. Parikh, and S. Lee, \"Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks,\" in NeurIPS, 2019.\n\nEfficient grounding of abstract spatial concepts for natural language interaction with robot manipulators. R Paul, RSSJ Arkin, RSSN Roy, RSST Howard, RSSR. Paul, J. Arkin, N. Roy, and T. M Howard, \"Efficient grounding of abstract spatial concepts for natural language interaction with robot manipulators,\" in RSS, 2016.\n\nInteractive visual grounding of referring expressions for human-robot interaction. M Shridhar, RSSD Hsu, RSSM. Shridhar and D. Hsu, \"Interactive visual grounding of referring expressions for human-robot interaction,\" in RSS, 2018.\n\nInteractively picking real-world objects with unconstrained spoken language instructions. J Hatori, Y Kikuchi, S Kobayashi, K Takahashi, Y Tsuboi, Y Unno, W Ko, J Tan, ICRA. J. Hatori, Y. Kikuchi, S. Kobayashi, K. Takahashi, Y. Tsuboi, Y. Unno, W. Ko, and J. Tan, \"Interactively picking real-world objects with uncon- strained spoken language instructions,\" in ICRA, 2018.\n\nRobot object retrieval with contextual natural language queries. T Nguyen, RSSN Gopalan, RSSR Patel, RSSM Corsaro, RSSE Pavlick, RSSS Tellex, RSST. Nguyen, N. Gopalan, R. Patel, M. Corsaro, E. Pavlick, and S. Tellex, \"Robot object retrieval with contextual natural language queries,\" in RSS, 2020.\n\nInvigorate: Interactive visual grounding and grasping in clutter. H Zhang, RSSY Lu, RSSC Yu, RSSD Hsu, RSSX La, RSSN Zheng, RSSH. Zhang, Y. Lu, C. Yu, D. Hsu, X. La, and N. Zheng, \"Invigorate: Interactive visual grounding and grasping in clutter,\" in RSS, 2021.\n\nComposing pick-and-place tasks by grounding language. O Mees, W Burgard, ISER. O. Mees and W. Burgard, \"Composing pick-and-place tasks by ground- ing language,\" in ISER, 2021.\n\nSpatial reasoning from natural language instructions for robot manipulation. S G Venkatesh, A Biswas, R Upadrashta, V Srinivasan, P Talukdar, B Amrutur, ICRAS. G. Venkatesh, A. Biswas, R. Upadrashta, V. Srinivasan, P. Talukdar, and B. Amrutur, \"Spatial reasoning from natural language instructions for robot manipulation,\" in ICRA, 2021.\n\nStructformer: Learning spatial structure for language-guided semantic rearrangement of novel objects. W Liu, C Paxton, T Hermans, D Fox, arXiv:2110.10189arXiv preprintW. Liu, C. Paxton, T. Hermans, and D. Fox, \"Structformer: Learning spatial structure for language-guided semantic rearrangement of novel objects,\" arXiv preprint arXiv:2110.10189, 2021.\n\nMapping instructions and visual observations to actions with reinforcement learning. D Misra, J Langford, Y Artzi, EMNLP. D. Misra, J. Langford, and Y. Artzi, \"Mapping instructions and visual observations to actions with reinforcement learning,\" in EMNLP, 2017.\n\nInteractive grounded language acquisition and generalization in a 2d world. H Yu, H Zhang, W Xu, in ICLR. H. Yu, H. Zhang, and W. Xu, \"Interactive grounded language acquisition and generalization in a 2d world,\" in ICLR, 2018.\n\nVision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. P Anderson, Q Wu, D Teney, J Bruce, M Johnson, N S\u00fcnderhauf, I Reid, S Gould, A Van Den, Hengel, CVPR. P. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson, N. S\u00fcnderhauf, I. Reid, S. Gould, and A. Van Den Hengel, \"Vision-and-language nav- igation: Interpreting visually-grounded navigation instructions in real environments,\" in CVPR, 2018.\n\nAlfred: A benchmark for interpreting grounded instructions for everyday tasks. M Shridhar, J Thomason, D Gordon, Y Bisk, W Han, R Mottaghi, L Zettlemoyer, D Fox, CVPR. M. Shridhar, J. Thomason, D. Gordon, Y. Bisk, W. Han, R. Mottaghi, L. Zettlemoyer, and D. Fox, \"Alfred: A benchmark for interpreting grounded instructions for everyday tasks,\" in CVPR, 2020.\n\nCliport: What and where pathways for robotic manipulation. M Shridhar, L Manuelli, D Fox, CoRLM. Shridhar, L. Manuelli, and D. Fox, \"Cliport: What and where pathways for robotic manipulation,\" in CoRL, 2021.\n\nLanguage-conditioned imitation learning for robot manipulation tasks. S Stepputtis, J Campbell, M Phielipp, S Lee, C Baral, H B Amor, NeurIPSS. Stepputtis, J. Campbell, M. Phielipp, S. Lee, C. Baral, and H. B. Amor, \"Language-conditioned imitation learning for robot manipulation tasks,\" in NeurIPS, 2020.\n\nBc-0: Zero-shot task generalization with robotic imitation learning. E Jang, A Irpan, M Khansari, D Kappler, F Ebert, C Lynch, S Levine, C Finn, CoRLE. Jang, A. Irpan, M. Khansari, D. Kappler, F. Ebert, C. Lynch, S. Levine, and C. Finn, \"Bc-0: Zero-shot task generalization with robotic imitation learning,\" in CoRL, 2021.\n\nConcept2robot: Learning manipulation concepts from instructions and human demonstrations. L Shao, RSST Migimatsu, RSSQ Zhang, RSSK Yang, RSSJ Bohg, RSSL. Shao, T. Migimatsu, Q. Zhang, K. Yang, and J. Bohg, \"Concept2robot: Learning manipulation concepts from instructions and human demonstra- tions,\" in RSS, 2020.\n\nMapping navigation instructions to continuous control actions with position-visitation prediction. V Blukis, D Misra, R A Knepper, Y Artzi, CoRLV. Blukis, D. Misra, R. A. Knepper, and Y. Artzi, \"Mapping navigation instructions to continuous control actions with position-visitation predic- tion,\" in CoRL, 2018.\n\nPybullet, a python module for physics simulation for games, robotics and machine learning. E Coumans, Y Bai, E. Coumans and Y. Bai, \"Pybullet, a python module for physics simulation for games, robotics and machine learning,\" http://pybullet.org, 2016-2021.\n\nLearning to achieve goals. L P Kaelbling, IJCAI. Citeseer. L. P. Kaelbling, \"Learning to achieve goals,\" in IJCAI. Citeseer, 1993, pp. 1094-1099.\n\nHindsight experience replay. M Andrychowicz, F Wolski, A Ray, J Schneider, R Fong, P Welinder, B Mcgrew, J Tobin, P Abbeel, W Zaremba, NeurIPSM. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder, B. McGrew, J. Tobin, P. Abbeel, and W. Zaremba, \"Hindsight experience replay,\" in NeurIPS, 2017.\n\nMinilm: Deep self-attention distillation for task-agnostic compression of pretrained transformers. W Wang, F Wei, L Dong, H Bao, N Yang, M Zhou, NeurIPSW. Wang, F. Wei, L. Dong, H. Bao, N. Yang, and M. Zhou, \"Minilm: Deep self-attention distillation for task-agnostic compression of pre- trained transformers,\" in NeurIPS, 2020.\n\nCausal confusion in imitation learning. P De Haan, D Jayaraman, S Levine, NeurIPS. P. de Haan, D. Jayaraman, and S. Levine, \"Causal confusion in imitation learning,\" NeurIPS, 2019.\n\nChoosing smartly: Adaptive multimodal fusion for object detection in changing environments. O Mees, A Eitel, W Burgard, IROS. O. Mees, A. Eitel, and W. Burgard, \"Choosing smartly: Adaptive multimodal fusion for object detection in changing environments,\" in IROS, 2016.\n\nMaking sense of vision and touch: Self-supervised learning of multimodal representations for contact-rich tasks. M A Lee, Y Zhu, K Srinivasan, P Shah, S Savarese, L Fei-Fei, A Garg, J Bohg, ICRA. M. A. Lee, Y. Zhu, K. Srinivasan, P. Shah, S. Savarese, L. Fei-Fei, A. Garg, and J. Bohg, \"Making sense of vision and touch: Self-supervised learning of multimodal representations for contact-rich tasks,\" in ICRA, 2019.\n\nContrastive multiview coding. Y Tian, D Krishnan, P Isola, ECCV. Y. Tian, D. Krishnan, and P. Isola, \"Contrastive multiview coding,\" in ECCV, 2020.\n\nAdversarial skill networks: Unsupervised robot skill learning from videos. O Mees, M Merklinger, G Kalweit, W Burgard, ICRAO. Mees, M. Merklinger, G. Kalweit, and W. Burgard, \"Adversarial skill networks: Unsupervised robot skill learning from videos,\" in ICRA, 2020.\n\nLearning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, arXiv:2103.00020arXiv preprintA. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al., \"Learning transfer- able visual models from natural language supervision,\" arXiv preprint arXiv:2103.00020, 2021.\n\nConservative data sharing for multi-task offline reinforcement learning. T Yu, A Kumar, Y Chebotar, K Hausman, S Levine, C Finn, NeurIPST. Yu, A. Kumar, Y. Chebotar, K. Hausman, S. Levine, and C. Finn, \"Conservative data sharing for multi-task offline reinforcement learning,\" in NeurIPS, 2021.\n", "annotations": {"author": "[{\"end\":232,\"start\":227},{\"end\":241,\"start\":233},{\"end\":250,\"start\":242},{\"end\":255,\"start\":251}]", "publisher": null, "author_last_name": "[{\"end\":231,\"start\":227},{\"end\":240,\"start\":233},{\"end\":249,\"start\":242},{\"end\":254,\"start\":251}]", "author_first_name": null, "author_affiliation": null, "title": "[{\"end\":224,\"start\":1},{\"end\":479,\"start\":256}]", "venue": null, "abstract": "[{\"end\":2637,\"start\":999}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5142,\"start\":5139},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5174,\"start\":5171},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5179,\"start\":5176},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6645,\"start\":6642},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6650,\"start\":6647},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6864,\"start\":6861},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8189,\"start\":8186},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8602,\"start\":8599},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9175,\"start\":9172},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9264,\"start\":9261},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9270,\"start\":9266},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9414,\"start\":9410},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9420,\"start\":9416},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9553,\"start\":9549},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9559,\"start\":9555},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9773,\"start\":9769},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9779,\"start\":9775},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9841,\"start\":9837},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9847,\"start\":9843},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10004,\"start\":10001},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10010,\"start\":10006},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10016,\"start\":10012},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10046,\"start\":10043},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10052,\"start\":10048},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10058,\"start\":10054},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10459,\"start\":10455},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10909,\"start\":10906},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":13592,\"start\":13588},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":13807,\"start\":13804},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":13812,\"start\":13809},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14118,\"start\":14115},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":17324,\"start\":17320},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":17330,\"start\":17326},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":17657,\"start\":17654},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":17662,\"start\":17659},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":19084,\"start\":19081},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":19089,\"start\":19086},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":19859,\"start\":19855},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":21405,\"start\":21402},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":28231,\"start\":28228},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":28681,\"start\":28678},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":28978,\"start\":28974},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":30310,\"start\":30307},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":31182,\"start\":31179},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":32074,\"start\":32071},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":32695,\"start\":32692},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":33523,\"start\":33519},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":33767,\"start\":33763},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":33773,\"start\":33769},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":33817,\"start\":33813},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":33823,\"start\":33819},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":34687,\"start\":34683},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":35178,\"start\":35174},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":35549,\"start\":35545},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":36661,\"start\":36658}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":42417,\"start\":42356},{\"attributes\":{\"id\":\"fig_1\"},\"end\":42635,\"start\":42418},{\"attributes\":{\"id\":\"fig_2\"},\"end\":42739,\"start\":42636},{\"attributes\":{\"id\":\"fig_3\"},\"end\":42811,\"start\":42740},{\"attributes\":{\"id\":\"fig_5\"},\"end\":43016,\"start\":42812},{\"attributes\":{\"id\":\"fig_6\"},\"end\":43088,\"start\":43017}]", "paragraph": "[{\"end\":3051,\"start\":2646},{\"end\":4280,\"start\":3053},{\"end\":5574,\"start\":4300},{\"end\":7975,\"start\":5576},{\"end\":8603,\"start\":7977},{\"end\":9061,\"start\":8605},{\"end\":10399,\"start\":9082},{\"end\":11693,\"start\":10401},{\"end\":12298,\"start\":11709},{\"end\":13669,\"start\":12390},{\"end\":15351,\"start\":13671},{\"end\":15975,\"start\":15353},{\"end\":16179,\"start\":15977},{\"end\":18865,\"start\":16188},{\"end\":20272,\"start\":18867},{\"end\":21120,\"start\":20357},{\"end\":21248,\"start\":21122},{\"end\":21406,\"start\":21250},{\"end\":22259,\"start\":21408},{\"end\":23307,\"start\":22292},{\"end\":23408,\"start\":23309},{\"end\":24033,\"start\":23410},{\"end\":25139,\"start\":24035},{\"end\":25271,\"start\":25141},{\"end\":25466,\"start\":25307},{\"end\":25564,\"start\":25468},{\"end\":25693,\"start\":25598},{\"end\":25894,\"start\":25720},{\"end\":26067,\"start\":25896},{\"end\":26247,\"start\":26069},{\"end\":26391,\"start\":26249},{\"end\":26529,\"start\":26418},{\"end\":26648,\"start\":26550},{\"end\":26933,\"start\":26665},{\"end\":27588,\"start\":26935},{\"end\":28306,\"start\":27616},{\"end\":28500,\"start\":28330},{\"end\":29447,\"start\":28539},{\"end\":29519,\"start\":29516},{\"end\":31073,\"start\":29521},{\"end\":32302,\"start\":31103},{\"end\":33892,\"start\":32330},{\"end\":34763,\"start\":33894},{\"end\":35697,\"start\":34765},{\"end\":36487,\"start\":35699},{\"end\":37241,\"start\":36489},{\"end\":37444,\"start\":37243},{\"end\":38052,\"start\":37468},{\"end\":38141,\"start\":38061},{\"end\":38277,\"start\":38143},{\"end\":38490,\"start\":38349},{\"end\":38556,\"start\":38492},{\"end\":38653,\"start\":38558},{\"end\":38813,\"start\":38719},{\"end\":38895,\"start\":38834},{\"end\":38979,\"start\":38917},{\"end\":39039,\"start\":38995},{\"end\":39309,\"start\":39056},{\"end\":39537,\"start\":39381},{\"end\":39756,\"start\":39609},{\"end\":39880,\"start\":39776},{\"end\":39995,\"start\":39900},{\"end\":40114,\"start\":40016},{\"end\":40240,\"start\":40116},{\"end\":40438,\"start\":40259},{\"end\":40527,\"start\":40461},{\"end\":40616,\"start\":40551},{\"end\":40692,\"start\":40632},{\"end\":40771,\"start\":40709},{\"end\":42355,\"start\":40809}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":29515,\"start\":29448}]", "table_ref": "[{\"end\":39169,\"start\":39118}]", "section_header": "[{\"end\":4298,\"start\":4283},{\"end\":9080,\"start\":9064},{\"end\":11707,\"start\":11696},{\"end\":12360,\"start\":12301},{\"end\":12388,\"start\":12363},{\"end\":16186,\"start\":16182},{\"end\":20298,\"start\":20275},{\"end\":20355,\"start\":20301},{\"end\":22290,\"start\":22262},{\"end\":25305,\"start\":25274},{\"end\":25596,\"start\":25567},{\"end\":25718,\"start\":25696},{\"end\":26416,\"start\":26394},{\"end\":26548,\"start\":26532},{\"end\":26663,\"start\":26651},{\"end\":27614,\"start\":27591},{\"end\":28328,\"start\":28309},{\"end\":28537,\"start\":28503},{\"end\":31101,\"start\":31076},{\"end\":32328,\"start\":32305},{\"end\":37455,\"start\":37447},{\"end\":37466,\"start\":37458},{\"end\":38059,\"start\":38055},{\"end\":38347,\"start\":38280},{\"end\":38717,\"start\":38656},{\"end\":38832,\"start\":38816},{\"end\":38915,\"start\":38898},{\"end\":38993,\"start\":38982},{\"end\":39054,\"start\":39042},{\"end\":39379,\"start\":39312},{\"end\":39607,\"start\":39540},{\"end\":39774,\"start\":39759},{\"end\":39898,\"start\":39883},{\"end\":40014,\"start\":39998},{\"end\":40257,\"start\":40243},{\"end\":40459,\"start\":40441},{\"end\":40549,\"start\":40530},{\"end\":40630,\"start\":40619},{\"end\":40707,\"start\":40695},{\"end\":40807,\"start\":40774},{\"end\":42365,\"start\":42357},{\"end\":42427,\"start\":42419},{\"end\":42645,\"start\":42637},{\"end\":42749,\"start\":42741},{\"end\":42821,\"start\":42813},{\"end\":43026,\"start\":43018}]", "table": null, "figure_caption": "[{\"end\":42417,\"start\":42367},{\"end\":42635,\"start\":42429},{\"end\":42739,\"start\":42647},{\"end\":42811,\"start\":42751},{\"end\":43016,\"start\":42823},{\"end\":43088,\"start\":43028}]", "figure_ref": "[{\"end\":2645,\"start\":2639},{\"end\":6424,\"start\":6415},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14241,\"start\":14233},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":15285,\"start\":15279},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15402,\"start\":15396},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":19513,\"start\":19507},{\"end\":21959,\"start\":21953},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":23611,\"start\":23605},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":27370,\"start\":27362},{\"end\":32489,\"start\":32481},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":38014,\"start\":38006}]", "bib_author_first_name": "[{\"end\":43274,\"start\":43273},{\"end\":43283,\"start\":43282},{\"end\":43295,\"start\":43294},{\"end\":43303,\"start\":43302},{\"end\":43312,\"start\":43311},{\"end\":43323,\"start\":43322},{\"end\":43333,\"start\":43332},{\"end\":43569,\"start\":43568},{\"end\":43575,\"start\":43574},{\"end\":43586,\"start\":43585},{\"end\":43592,\"start\":43591},{\"end\":43602,\"start\":43601},{\"end\":43613,\"start\":43612},{\"end\":43621,\"start\":43620},{\"end\":43867,\"start\":43866},{\"end\":43882,\"start\":43881},{\"end\":43892,\"start\":43891},{\"end\":43904,\"start\":43903},{\"end\":43915,\"start\":43914},{\"end\":43931,\"start\":43930},{\"end\":43939,\"start\":43938},{\"end\":43949,\"start\":43948},{\"end\":44191,\"start\":44190},{\"end\":44200,\"start\":44199},{\"end\":44208,\"start\":44207},{\"end\":44218,\"start\":44217},{\"end\":44493,\"start\":44492},{\"end\":44501,\"start\":44500},{\"end\":44515,\"start\":44511},{\"end\":44523,\"start\":44522},{\"end\":44821,\"start\":44820},{\"end\":44830,\"start\":44829},{\"end\":45047,\"start\":45046},{\"end\":45055,\"start\":45054},{\"end\":45067,\"start\":45066},{\"end\":45075,\"start\":45074},{\"end\":45085,\"start\":45084},{\"end\":45097,\"start\":45096},{\"end\":45314,\"start\":45313},{\"end\":45324,\"start\":45323},{\"end\":45335,\"start\":45334},{\"end\":45350,\"start\":45349},{\"end\":45659,\"start\":45658},{\"end\":45673,\"start\":45672},{\"end\":45684,\"start\":45683},{\"end\":45694,\"start\":45693},{\"end\":45945,\"start\":45944},{\"end\":45951,\"start\":45950},{\"end\":45960,\"start\":45959},{\"end\":45970,\"start\":45969},{\"end\":46250,\"start\":46249},{\"end\":46261,\"start\":46260},{\"end\":46273,\"start\":46272},{\"end\":46283,\"start\":46282},{\"end\":46547,\"start\":46546},{\"end\":46562,\"start\":46561},{\"end\":46786,\"start\":46785},{\"end\":46796,\"start\":46795},{\"end\":46807,\"start\":46806},{\"end\":46820,\"start\":46819},{\"end\":46833,\"start\":46832},{\"end\":46843,\"start\":46842},{\"end\":46851,\"start\":46850},{\"end\":46857,\"start\":46856},{\"end\":47135,\"start\":47134},{\"end\":47148,\"start\":47147},{\"end\":47162,\"start\":47161},{\"end\":47174,\"start\":47173},{\"end\":47188,\"start\":47187},{\"end\":47202,\"start\":47201},{\"end\":47435,\"start\":47434},{\"end\":47447,\"start\":47446},{\"end\":47456,\"start\":47455},{\"end\":47465,\"start\":47464},{\"end\":47475,\"start\":47474},{\"end\":47484,\"start\":47483},{\"end\":47686,\"start\":47685},{\"end\":47694,\"start\":47693},{\"end\":47886,\"start\":47885},{\"end\":47888,\"start\":47887},{\"end\":47901,\"start\":47900},{\"end\":47911,\"start\":47910},{\"end\":47925,\"start\":47924},{\"end\":47939,\"start\":47938},{\"end\":47951,\"start\":47950},{\"end\":48250,\"start\":48249},{\"end\":48257,\"start\":48256},{\"end\":48267,\"start\":48266},{\"end\":48278,\"start\":48277},{\"end\":48587,\"start\":48586},{\"end\":48596,\"start\":48595},{\"end\":48608,\"start\":48607},{\"end\":48841,\"start\":48840},{\"end\":48847,\"start\":48846},{\"end\":48856,\"start\":48855},{\"end\":49102,\"start\":49101},{\"end\":49114,\"start\":49113},{\"end\":49120,\"start\":49119},{\"end\":49129,\"start\":49128},{\"end\":49138,\"start\":49137},{\"end\":49149,\"start\":49148},{\"end\":49163,\"start\":49162},{\"end\":49171,\"start\":49170},{\"end\":49180,\"start\":49179},{\"end\":49522,\"start\":49521},{\"end\":49534,\"start\":49533},{\"end\":49546,\"start\":49545},{\"end\":49556,\"start\":49555},{\"end\":49564,\"start\":49563},{\"end\":49571,\"start\":49570},{\"end\":49583,\"start\":49582},{\"end\":49598,\"start\":49597},{\"end\":49862,\"start\":49861},{\"end\":49874,\"start\":49873},{\"end\":49886,\"start\":49885},{\"end\":50082,\"start\":50081},{\"end\":50096,\"start\":50095},{\"end\":50108,\"start\":50107},{\"end\":50120,\"start\":50119},{\"end\":50127,\"start\":50126},{\"end\":50136,\"start\":50135},{\"end\":50138,\"start\":50137},{\"end\":50388,\"start\":50387},{\"end\":50396,\"start\":50395},{\"end\":50405,\"start\":50404},{\"end\":50417,\"start\":50416},{\"end\":50428,\"start\":50427},{\"end\":50437,\"start\":50436},{\"end\":50446,\"start\":50445},{\"end\":50456,\"start\":50455},{\"end\":50733,\"start\":50732},{\"end\":50744,\"start\":50743},{\"end\":50760,\"start\":50759},{\"end\":50772,\"start\":50771},{\"end\":50783,\"start\":50782},{\"end\":51057,\"start\":51056},{\"end\":51067,\"start\":51066},{\"end\":51076,\"start\":51075},{\"end\":51078,\"start\":51077},{\"end\":51089,\"start\":51088},{\"end\":51362,\"start\":51361},{\"end\":51373,\"start\":51372},{\"end\":51556,\"start\":51555},{\"end\":51558,\"start\":51557},{\"end\":51705,\"start\":51704},{\"end\":51721,\"start\":51720},{\"end\":51731,\"start\":51730},{\"end\":51738,\"start\":51737},{\"end\":51751,\"start\":51750},{\"end\":51759,\"start\":51758},{\"end\":51771,\"start\":51770},{\"end\":51781,\"start\":51780},{\"end\":51790,\"start\":51789},{\"end\":51800,\"start\":51799},{\"end\":52087,\"start\":52086},{\"end\":52095,\"start\":52094},{\"end\":52102,\"start\":52101},{\"end\":52110,\"start\":52109},{\"end\":52117,\"start\":52116},{\"end\":52125,\"start\":52124},{\"end\":52358,\"start\":52357},{\"end\":52369,\"start\":52368},{\"end\":52382,\"start\":52381},{\"end\":52592,\"start\":52591},{\"end\":52600,\"start\":52599},{\"end\":52609,\"start\":52608},{\"end\":52884,\"start\":52883},{\"end\":52886,\"start\":52885},{\"end\":52893,\"start\":52892},{\"end\":52900,\"start\":52899},{\"end\":52914,\"start\":52913},{\"end\":52922,\"start\":52921},{\"end\":52934,\"start\":52933},{\"end\":52945,\"start\":52944},{\"end\":52953,\"start\":52952},{\"end\":53218,\"start\":53217},{\"end\":53226,\"start\":53225},{\"end\":53238,\"start\":53237},{\"end\":53412,\"start\":53411},{\"end\":53420,\"start\":53419},{\"end\":53434,\"start\":53433},{\"end\":53445,\"start\":53444},{\"end\":53676,\"start\":53675},{\"end\":53687,\"start\":53686},{\"end\":53689,\"start\":53688},{\"end\":53696,\"start\":53695},{\"end\":53707,\"start\":53706},{\"end\":53717,\"start\":53716},{\"end\":53724,\"start\":53723},{\"end\":53735,\"start\":53734},{\"end\":53745,\"start\":53744},{\"end\":53755,\"start\":53754},{\"end\":53766,\"start\":53765},{\"end\":54111,\"start\":54110},{\"end\":54117,\"start\":54116},{\"end\":54126,\"start\":54125},{\"end\":54138,\"start\":54137},{\"end\":54149,\"start\":54148},{\"end\":54159,\"start\":54158}]", "bib_author_last_name": "[{\"end\":43280,\"start\":43275},{\"end\":43292,\"start\":43284},{\"end\":43300,\"start\":43296},{\"end\":43309,\"start\":43304},{\"end\":43320,\"start\":43313},{\"end\":43330,\"start\":43324},{\"end\":43342,\"start\":43334},{\"end\":43572,\"start\":43570},{\"end\":43583,\"start\":43576},{\"end\":43589,\"start\":43587},{\"end\":43599,\"start\":43593},{\"end\":43610,\"start\":43603},{\"end\":43618,\"start\":43614},{\"end\":43628,\"start\":43622},{\"end\":43879,\"start\":43868},{\"end\":43889,\"start\":43883},{\"end\":43901,\"start\":43893},{\"end\":43912,\"start\":43905},{\"end\":43928,\"start\":43916},{\"end\":43936,\"start\":43932},{\"end\":43946,\"start\":43940},{\"end\":43957,\"start\":43950},{\"end\":44197,\"start\":44192},{\"end\":44205,\"start\":44201},{\"end\":44215,\"start\":44209},{\"end\":44224,\"start\":44219},{\"end\":44498,\"start\":44494},{\"end\":44509,\"start\":44502},{\"end\":44520,\"start\":44516},{\"end\":44532,\"start\":44524},{\"end\":44827,\"start\":44822},{\"end\":44839,\"start\":44831},{\"end\":45052,\"start\":45048},{\"end\":45064,\"start\":45056},{\"end\":45072,\"start\":45068},{\"end\":45082,\"start\":45076},{\"end\":45094,\"start\":45086},{\"end\":45102,\"start\":45098},{\"end\":45321,\"start\":45315},{\"end\":45332,\"start\":45325},{\"end\":45347,\"start\":45336},{\"end\":45359,\"start\":45351},{\"end\":45670,\"start\":45660},{\"end\":45681,\"start\":45674},{\"end\":45691,\"start\":45685},{\"end\":45699,\"start\":45695},{\"end\":45948,\"start\":45946},{\"end\":45957,\"start\":45952},{\"end\":45967,\"start\":45961},{\"end\":45974,\"start\":45971},{\"end\":46255,\"start\":46251},{\"end\":46267,\"start\":46262},{\"end\":46277,\"start\":46274},{\"end\":46290,\"start\":46284},{\"end\":46556,\"start\":46548},{\"end\":46566,\"start\":46563},{\"end\":46793,\"start\":46787},{\"end\":46804,\"start\":46797},{\"end\":46817,\"start\":46808},{\"end\":46830,\"start\":46821},{\"end\":46840,\"start\":46834},{\"end\":46848,\"start\":46844},{\"end\":46854,\"start\":46852},{\"end\":46861,\"start\":46858},{\"end\":47142,\"start\":47136},{\"end\":47156,\"start\":47149},{\"end\":47168,\"start\":47163},{\"end\":47182,\"start\":47175},{\"end\":47196,\"start\":47189},{\"end\":47209,\"start\":47203},{\"end\":47441,\"start\":47436},{\"end\":47450,\"start\":47448},{\"end\":47459,\"start\":47457},{\"end\":47469,\"start\":47466},{\"end\":47478,\"start\":47476},{\"end\":47490,\"start\":47485},{\"end\":47691,\"start\":47687},{\"end\":47702,\"start\":47695},{\"end\":47898,\"start\":47889},{\"end\":47908,\"start\":47902},{\"end\":47922,\"start\":47912},{\"end\":47936,\"start\":47926},{\"end\":47948,\"start\":47940},{\"end\":47959,\"start\":47952},{\"end\":48254,\"start\":48251},{\"end\":48264,\"start\":48258},{\"end\":48275,\"start\":48268},{\"end\":48282,\"start\":48279},{\"end\":48593,\"start\":48588},{\"end\":48605,\"start\":48597},{\"end\":48614,\"start\":48609},{\"end\":48844,\"start\":48842},{\"end\":48853,\"start\":48848},{\"end\":48859,\"start\":48857},{\"end\":49111,\"start\":49103},{\"end\":49117,\"start\":49115},{\"end\":49126,\"start\":49121},{\"end\":49135,\"start\":49130},{\"end\":49146,\"start\":49139},{\"end\":49160,\"start\":49150},{\"end\":49168,\"start\":49164},{\"end\":49177,\"start\":49172},{\"end\":49188,\"start\":49181},{\"end\":49196,\"start\":49190},{\"end\":49531,\"start\":49523},{\"end\":49543,\"start\":49535},{\"end\":49553,\"start\":49547},{\"end\":49561,\"start\":49557},{\"end\":49568,\"start\":49565},{\"end\":49580,\"start\":49572},{\"end\":49595,\"start\":49584},{\"end\":49602,\"start\":49599},{\"end\":49871,\"start\":49863},{\"end\":49883,\"start\":49875},{\"end\":49890,\"start\":49887},{\"end\":50093,\"start\":50083},{\"end\":50105,\"start\":50097},{\"end\":50117,\"start\":50109},{\"end\":50124,\"start\":50121},{\"end\":50133,\"start\":50128},{\"end\":50143,\"start\":50139},{\"end\":50393,\"start\":50389},{\"end\":50402,\"start\":50397},{\"end\":50414,\"start\":50406},{\"end\":50425,\"start\":50418},{\"end\":50434,\"start\":50429},{\"end\":50443,\"start\":50438},{\"end\":50453,\"start\":50447},{\"end\":50461,\"start\":50457},{\"end\":50738,\"start\":50734},{\"end\":50754,\"start\":50745},{\"end\":50766,\"start\":50761},{\"end\":50777,\"start\":50773},{\"end\":50788,\"start\":50784},{\"end\":51064,\"start\":51058},{\"end\":51073,\"start\":51068},{\"end\":51086,\"start\":51079},{\"end\":51095,\"start\":51090},{\"end\":51370,\"start\":51363},{\"end\":51377,\"start\":51374},{\"end\":51568,\"start\":51559},{\"end\":51718,\"start\":51706},{\"end\":51728,\"start\":51722},{\"end\":51735,\"start\":51732},{\"end\":51748,\"start\":51739},{\"end\":51756,\"start\":51752},{\"end\":51768,\"start\":51760},{\"end\":51778,\"start\":51772},{\"end\":51787,\"start\":51782},{\"end\":51797,\"start\":51791},{\"end\":51808,\"start\":51801},{\"end\":52092,\"start\":52088},{\"end\":52099,\"start\":52096},{\"end\":52107,\"start\":52103},{\"end\":52114,\"start\":52111},{\"end\":52122,\"start\":52118},{\"end\":52130,\"start\":52126},{\"end\":52366,\"start\":52359},{\"end\":52379,\"start\":52370},{\"end\":52389,\"start\":52383},{\"end\":52597,\"start\":52593},{\"end\":52606,\"start\":52601},{\"end\":52617,\"start\":52610},{\"end\":52890,\"start\":52887},{\"end\":52897,\"start\":52894},{\"end\":52911,\"start\":52901},{\"end\":52919,\"start\":52915},{\"end\":52931,\"start\":52923},{\"end\":52942,\"start\":52935},{\"end\":52950,\"start\":52946},{\"end\":52958,\"start\":52954},{\"end\":53223,\"start\":53219},{\"end\":53235,\"start\":53227},{\"end\":53244,\"start\":53239},{\"end\":53417,\"start\":53413},{\"end\":53431,\"start\":53421},{\"end\":53442,\"start\":53435},{\"end\":53453,\"start\":53446},{\"end\":53684,\"start\":53677},{\"end\":53693,\"start\":53690},{\"end\":53704,\"start\":53697},{\"end\":53714,\"start\":53708},{\"end\":53721,\"start\":53718},{\"end\":53732,\"start\":53725},{\"end\":53742,\"start\":53736},{\"end\":53752,\"start\":53746},{\"end\":53763,\"start\":53756},{\"end\":53772,\"start\":53767},{\"end\":54114,\"start\":54112},{\"end\":54123,\"start\":54118},{\"end\":54135,\"start\":54127},{\"end\":54146,\"start\":54139},{\"end\":54156,\"start\":54150},{\"end\":54164,\"start\":54160}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":43479,\"start\":43240},{\"attributes\":{\"id\":\"b1\"},\"end\":43810,\"start\":43481},{\"attributes\":{\"id\":\"b2\"},\"end\":44138,\"start\":43812},{\"attributes\":{\"doi\":\"arXiv:2107.09046\",\"id\":\"b3\"},\"end\":44390,\"start\":44140},{\"attributes\":{\"doi\":\"arXiv:2012.08456\",\"id\":\"b4\"},\"end\":44754,\"start\":44392},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":235657751},\"end\":44951,\"start\":44756},{\"attributes\":{\"id\":\"b6\"},\"end\":45285,\"start\":44953},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":213739887},\"end\":45588,\"start\":45287},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":6308361},\"end\":45844,\"start\":45590},{\"attributes\":{\"id\":\"b9\"},\"end\":46140,\"start\":45846},{\"attributes\":{\"id\":\"b10\"},\"end\":46461,\"start\":46142},{\"attributes\":{\"id\":\"b11\"},\"end\":46693,\"start\":46463},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":4389330},\"end\":47067,\"start\":46695},{\"attributes\":{\"id\":\"b13\"},\"end\":47366,\"start\":47069},{\"attributes\":{\"id\":\"b14\"},\"end\":47629,\"start\":47368},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":231933709},\"end\":47806,\"start\":47631},{\"attributes\":{\"id\":\"b16\"},\"end\":48145,\"start\":47808},{\"attributes\":{\"doi\":\"arXiv:2110.10189\",\"id\":\"b17\"},\"end\":48499,\"start\":48147},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":10458880},\"end\":48762,\"start\":48501},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":3463660},\"end\":48990,\"start\":48764},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":4673790},\"end\":49440,\"start\":48992},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":208617407},\"end\":49800,\"start\":49442},{\"attributes\":{\"id\":\"b22\"},\"end\":50009,\"start\":49802},{\"attributes\":{\"id\":\"b23\"},\"end\":50316,\"start\":50011},{\"attributes\":{\"id\":\"b24\"},\"end\":50640,\"start\":50318},{\"attributes\":{\"id\":\"b25\"},\"end\":50955,\"start\":50642},{\"attributes\":{\"id\":\"b26\"},\"end\":51268,\"start\":50957},{\"attributes\":{\"id\":\"b27\"},\"end\":51526,\"start\":51270},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":5538688},\"end\":51673,\"start\":51528},{\"attributes\":{\"id\":\"b29\"},\"end\":51985,\"start\":51675},{\"attributes\":{\"id\":\"b30\"},\"end\":52315,\"start\":51987},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":145816735},\"end\":52497,\"start\":52317},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":6193434},\"end\":52768,\"start\":52499},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":53085539},\"end\":53185,\"start\":52770},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":189762205},\"end\":53334,\"start\":53187},{\"attributes\":{\"id\":\"b35\"},\"end\":53602,\"start\":53336},{\"attributes\":{\"doi\":\"arXiv:2103.00020\",\"id\":\"b36\"},\"end\":54035,\"start\":53604},{\"attributes\":{\"id\":\"b37\"},\"end\":54331,\"start\":54037}]", "bib_title": "[{\"end\":44818,\"start\":44756},{\"end\":45311,\"start\":45287},{\"end\":45656,\"start\":45590},{\"end\":46783,\"start\":46695},{\"end\":47683,\"start\":47631},{\"end\":48584,\"start\":48501},{\"end\":48838,\"start\":48764},{\"end\":49099,\"start\":48992},{\"end\":49519,\"start\":49442},{\"end\":51553,\"start\":51528},{\"end\":52355,\"start\":52317},{\"end\":52589,\"start\":52499},{\"end\":52881,\"start\":52770},{\"end\":53215,\"start\":53187}]", "bib_author": "[{\"end\":43282,\"start\":43273},{\"end\":43294,\"start\":43282},{\"end\":43302,\"start\":43294},{\"end\":43311,\"start\":43302},{\"end\":43322,\"start\":43311},{\"end\":43332,\"start\":43322},{\"end\":43344,\"start\":43332},{\"end\":43574,\"start\":43568},{\"end\":43585,\"start\":43574},{\"end\":43591,\"start\":43585},{\"end\":43601,\"start\":43591},{\"end\":43612,\"start\":43601},{\"end\":43620,\"start\":43612},{\"end\":43630,\"start\":43620},{\"end\":43881,\"start\":43866},{\"end\":43891,\"start\":43881},{\"end\":43903,\"start\":43891},{\"end\":43914,\"start\":43903},{\"end\":43930,\"start\":43914},{\"end\":43938,\"start\":43930},{\"end\":43948,\"start\":43938},{\"end\":43959,\"start\":43948},{\"end\":44199,\"start\":44190},{\"end\":44207,\"start\":44199},{\"end\":44217,\"start\":44207},{\"end\":44226,\"start\":44217},{\"end\":44500,\"start\":44492},{\"end\":44511,\"start\":44500},{\"end\":44522,\"start\":44511},{\"end\":44534,\"start\":44522},{\"end\":44829,\"start\":44820},{\"end\":44841,\"start\":44829},{\"end\":45054,\"start\":45046},{\"end\":45066,\"start\":45054},{\"end\":45074,\"start\":45066},{\"end\":45084,\"start\":45074},{\"end\":45096,\"start\":45084},{\"end\":45104,\"start\":45096},{\"end\":45323,\"start\":45313},{\"end\":45334,\"start\":45323},{\"end\":45349,\"start\":45334},{\"end\":45361,\"start\":45349},{\"end\":45672,\"start\":45658},{\"end\":45683,\"start\":45672},{\"end\":45693,\"start\":45683},{\"end\":45701,\"start\":45693},{\"end\":45950,\"start\":45944},{\"end\":45959,\"start\":45950},{\"end\":45969,\"start\":45959},{\"end\":45976,\"start\":45969},{\"end\":46260,\"start\":46249},{\"end\":46272,\"start\":46260},{\"end\":46282,\"start\":46272},{\"end\":46295,\"start\":46282},{\"end\":46561,\"start\":46546},{\"end\":46571,\"start\":46561},{\"end\":46795,\"start\":46785},{\"end\":46806,\"start\":46795},{\"end\":46819,\"start\":46806},{\"end\":46832,\"start\":46819},{\"end\":46842,\"start\":46832},{\"end\":46850,\"start\":46842},{\"end\":46856,\"start\":46850},{\"end\":46863,\"start\":46856},{\"end\":47147,\"start\":47134},{\"end\":47161,\"start\":47147},{\"end\":47173,\"start\":47161},{\"end\":47187,\"start\":47173},{\"end\":47201,\"start\":47187},{\"end\":47214,\"start\":47201},{\"end\":47446,\"start\":47434},{\"end\":47455,\"start\":47446},{\"end\":47464,\"start\":47455},{\"end\":47474,\"start\":47464},{\"end\":47483,\"start\":47474},{\"end\":47495,\"start\":47483},{\"end\":47693,\"start\":47685},{\"end\":47704,\"start\":47693},{\"end\":47900,\"start\":47885},{\"end\":47910,\"start\":47900},{\"end\":47924,\"start\":47910},{\"end\":47938,\"start\":47924},{\"end\":47950,\"start\":47938},{\"end\":47961,\"start\":47950},{\"end\":48256,\"start\":48249},{\"end\":48266,\"start\":48256},{\"end\":48277,\"start\":48266},{\"end\":48284,\"start\":48277},{\"end\":48595,\"start\":48586},{\"end\":48607,\"start\":48595},{\"end\":48616,\"start\":48607},{\"end\":48846,\"start\":48840},{\"end\":48855,\"start\":48846},{\"end\":48861,\"start\":48855},{\"end\":49113,\"start\":49101},{\"end\":49119,\"start\":49113},{\"end\":49128,\"start\":49119},{\"end\":49137,\"start\":49128},{\"end\":49148,\"start\":49137},{\"end\":49162,\"start\":49148},{\"end\":49170,\"start\":49162},{\"end\":49179,\"start\":49170},{\"end\":49190,\"start\":49179},{\"end\":49198,\"start\":49190},{\"end\":49533,\"start\":49521},{\"end\":49545,\"start\":49533},{\"end\":49555,\"start\":49545},{\"end\":49563,\"start\":49555},{\"end\":49570,\"start\":49563},{\"end\":49582,\"start\":49570},{\"end\":49597,\"start\":49582},{\"end\":49604,\"start\":49597},{\"end\":49873,\"start\":49861},{\"end\":49885,\"start\":49873},{\"end\":49892,\"start\":49885},{\"end\":50095,\"start\":50081},{\"end\":50107,\"start\":50095},{\"end\":50119,\"start\":50107},{\"end\":50126,\"start\":50119},{\"end\":50135,\"start\":50126},{\"end\":50145,\"start\":50135},{\"end\":50395,\"start\":50387},{\"end\":50404,\"start\":50395},{\"end\":50416,\"start\":50404},{\"end\":50427,\"start\":50416},{\"end\":50436,\"start\":50427},{\"end\":50445,\"start\":50436},{\"end\":50455,\"start\":50445},{\"end\":50463,\"start\":50455},{\"end\":50743,\"start\":50732},{\"end\":50759,\"start\":50743},{\"end\":50771,\"start\":50759},{\"end\":50782,\"start\":50771},{\"end\":50793,\"start\":50782},{\"end\":51066,\"start\":51056},{\"end\":51075,\"start\":51066},{\"end\":51088,\"start\":51075},{\"end\":51097,\"start\":51088},{\"end\":51372,\"start\":51361},{\"end\":51379,\"start\":51372},{\"end\":51570,\"start\":51555},{\"end\":51720,\"start\":51704},{\"end\":51730,\"start\":51720},{\"end\":51737,\"start\":51730},{\"end\":51750,\"start\":51737},{\"end\":51758,\"start\":51750},{\"end\":51770,\"start\":51758},{\"end\":51780,\"start\":51770},{\"end\":51789,\"start\":51780},{\"end\":51799,\"start\":51789},{\"end\":51810,\"start\":51799},{\"end\":52094,\"start\":52086},{\"end\":52101,\"start\":52094},{\"end\":52109,\"start\":52101},{\"end\":52116,\"start\":52109},{\"end\":52124,\"start\":52116},{\"end\":52132,\"start\":52124},{\"end\":52368,\"start\":52357},{\"end\":52381,\"start\":52368},{\"end\":52391,\"start\":52381},{\"end\":52599,\"start\":52591},{\"end\":52608,\"start\":52599},{\"end\":52619,\"start\":52608},{\"end\":52892,\"start\":52883},{\"end\":52899,\"start\":52892},{\"end\":52913,\"start\":52899},{\"end\":52921,\"start\":52913},{\"end\":52933,\"start\":52921},{\"end\":52944,\"start\":52933},{\"end\":52952,\"start\":52944},{\"end\":52960,\"start\":52952},{\"end\":53225,\"start\":53217},{\"end\":53237,\"start\":53225},{\"end\":53246,\"start\":53237},{\"end\":53419,\"start\":53411},{\"end\":53433,\"start\":53419},{\"end\":53444,\"start\":53433},{\"end\":53455,\"start\":53444},{\"end\":53686,\"start\":53675},{\"end\":53695,\"start\":53686},{\"end\":53706,\"start\":53695},{\"end\":53716,\"start\":53706},{\"end\":53723,\"start\":53716},{\"end\":53734,\"start\":53723},{\"end\":53744,\"start\":53734},{\"end\":53754,\"start\":53744},{\"end\":53765,\"start\":53754},{\"end\":53774,\"start\":53765},{\"end\":54116,\"start\":54110},{\"end\":54125,\"start\":54116},{\"end\":54137,\"start\":54125},{\"end\":54148,\"start\":54137},{\"end\":54158,\"start\":54148},{\"end\":54166,\"start\":54158}]", "bib_venue": "[{\"end\":43271,\"start\":43240},{\"end\":43566,\"start\":43481},{\"end\":43864,\"start\":43812},{\"end\":44188,\"start\":44140},{\"end\":44490,\"start\":44392},{\"end\":44844,\"start\":44841},{\"end\":45044,\"start\":44953},{\"end\":45393,\"start\":45361},{\"end\":45706,\"start\":45701},{\"end\":45942,\"start\":45846},{\"end\":46247,\"start\":46142},{\"end\":46544,\"start\":46463},{\"end\":46867,\"start\":46863},{\"end\":47132,\"start\":47069},{\"end\":47432,\"start\":47368},{\"end\":47708,\"start\":47704},{\"end\":47883,\"start\":47808},{\"end\":48247,\"start\":48147},{\"end\":48621,\"start\":48616},{\"end\":48868,\"start\":48861},{\"end\":49202,\"start\":49198},{\"end\":49608,\"start\":49604},{\"end\":49859,\"start\":49802},{\"end\":50079,\"start\":50011},{\"end\":50385,\"start\":50318},{\"end\":50730,\"start\":50642},{\"end\":51054,\"start\":50957},{\"end\":51359,\"start\":51270},{\"end\":51585,\"start\":51570},{\"end\":51702,\"start\":51675},{\"end\":52084,\"start\":51987},{\"end\":52398,\"start\":52391},{\"end\":52623,\"start\":52619},{\"end\":52964,\"start\":52960},{\"end\":53250,\"start\":53246},{\"end\":53409,\"start\":53336},{\"end\":53673,\"start\":53604},{\"end\":54108,\"start\":54037}]"}}}, "year": 2023, "month": 12, "day": 17}