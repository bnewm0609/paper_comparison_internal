{"id": 231951663, "updated": "2023-10-06 06:28:29.426", "metadata": {"title": "SLAKE: A Semantically-Labeled Knowledge-Enhanced Dataset for Medical Visual Question Answering", "authors": "[{\"first\":\"Bo\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Li-Ming\",\"last\":\"Zhan\",\"middle\":[]},{\"first\":\"Li\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Lin\",\"last\":\"Ma\",\"middle\":[]},{\"first\":\"Yan\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Xiao-Ming\",\"last\":\"Wu\",\"middle\":[]}]", "venue": "2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)", "journal": "2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)", "publication_date": {"year": 2021, "month": 2, "day": 18}, "abstract": "Medical visual question answering (Med-VQA) has tremendous potential in healthcare. However, the development of this technology is hindered by the lacking of publicly-available and high-quality labeled datasets for training and evaluation. In this paper, we present a large bilingual dataset, SLAKE, with comprehensive semantic labels annotated by experienced physicians and a new structural medical knowledge base for Med-VQA. Besides, SLAKE includes richer modalities and covers more human body parts than the currently available dataset. We show that SLAKE can be used to facilitate the development and evaluation of Med-VQA systems. The dataset can be downloaded from http://www.med-vqa.com/slake.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2102.09542", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/isbi/LiuZXMYW21", "doi": "10.1109/isbi48211.2021.9434010"}}, "content": {"source": {"pdf_hash": "93b6b79b4ef6c345f31722ce7c829385c6dce0d6", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2102.09542v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2102.09542", "status": "GREEN"}}, "grobid": {"id": "126d2414565112dd4ae74d64ecb6f2052db144eb", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/93b6b79b4ef6c345f31722ce7c829385c6dce0d6.txt", "contents": "\nSLAKE: A SEMANTICALLY-LABELED KNOWLEDGE-ENHANCED DATASET FOR MEDICAL VISUAL QUESTION ANSWERING\n\n\nBo Liu \nLi-Ming Zhan \nLi Xu \nLin Ma \nDepartment of Ultrasound\nWest China Hospital of Sichuan University\nChina\n\nYan Yang \nSichuan Academy of Medical Sciences\nSichuan Provincial People's Hospital\nChina\n\nXiao-Ming Wu \nDepartment of Ultrasound\nWest China Hospital of Sichuan University\nChina\n\n\nDepartment of Computing\nThe Hong Kong Polytechnic University\nHong Kong\n\nSLAKE: A SEMANTICALLY-LABELED KNOWLEDGE-ENHANCED DATASET FOR MEDICAL VISUAL QUESTION ANSWERING\n10.5281/zenodo.3431873Index Terms-Datasetmedical visual question answer- ingmulti-modality fusion\nMedical visual question answering (Med-VQA) has tremendous potential in healthcare. However, the development of this technology is hindered by the lacking of publicly-available and high-quality labeled datasets for training and evaluation. In this paper, we present a large bilingual dataset, SLAKE, with comprehensive semantic labels annotated by experienced physicians and a new structural medical knowledge base for Med-VQA. Besides, SLAKE includes richer modalities and covers more human body parts than the currently available dataset. We show that SLAKE can be used to facilitate the development and evaluation of Med-VQA systems. The dataset can be downloaded from\n\nINTRODUCTION\n\nDeveloping machines that can understand visual content and answer questions like humans is a long-standing goal of AI research. In recent years, visual question answering (VQA) has become an active field of research. Medical visual question answering (Med-VQA) is a domain-specific branch of VQA, where a clinical question comes with a radiology image and the goal is to design a system that can correctly answer the question based on the visual information of the image.\n\nMed-VQA has a wide range of application prospects in healthcare sectors and a broad impact on the wellness of the general public. With a reliable Med-VQA system, patients can easily acquire information about their health and be more engaged in the process of decision making. For doctors, Med-VQA systems can be used to assist diagnosis by providing them a second medical opinion. The systems can also be used in clinical education to train medical professionals. Besides, Med-VQA technology can be potentially integrated * Equal contribution. \u2020 Corresponding author. into many conversational AI platforms to bring enormous benefits to healthcare industry. However, the research of Med-VQA is at an early stage. Unlike VQA in the general domain, where large-scale highquality datasets [2,3] are available, there is a lack of publiclyavailable and well-annotated datasets for training and evaluating Med-VQA systems. To correctly answer a clinical question about a radiology image, it requires clinical expertise and domain-specific medical knowledge, which makes it difficult to construct a realistic and accurate dataset for Med-VQA. VQA-RAD [1] is a first step in this direction. To our knowledge, it is the only available dataset with manual annotation, based on which several Med-VQA models have been proposed [4,5]. VQA-RAD is a diverse dataset containing a variety of different types of clinical questions, with each question type sufficiently represented. But it does not provide semantic labels, e.g., labeled segmentations of organs and tumors or bounding boxes on objects, which are essential for training a Med-VQA model to find the region of interest in an image to answer complex clinical questions. Moreover, a practical Med-VQA system needs to exploit external knowledge apart from visual content to answer complex compositional questions involving inquires such as \"the functionality of an organ\", \"the cause of a disease\", or \"the treatment of a disease\", which is also not supported in VQA-RAD.\n\nTo fill these gaps, we construct a semantically-labeled knowledge-enhanced (SLAKE) dataset with accurate visual and textual annotations and an extendable knowledge base for Med-VQA. It takes our team more than half of a year to complete all the tasks, including building the annotation system, constructing the medical knowledge graph (KG), selecting and labeling images, generating questions, and analyzing the dataset. As shown in Figure 1, for each radiology image, we provide two kinds of visual annotations: masks for semantic segmentation and bounding boxes for object detection. Besides basic clinical questions, we also design compositional questions that require multiple reasoning steps, and knowledge-based questions like [6] that involve external medical knowledge. In general, questions in SLAKE can be categorized as vision-only questions and knowledge-based questions. We provide detailed annotations to distinguish the two types of questions and guide the Med-VQA model to search for answers on the knowledge graph. Besides these new features, SLAKE is designed to be an English-Chinese bilingual dataset to broaden its application range. Further, SLAKE covers more body parts (e.g., neck and pelvic cavity) and more types of questions (e.g., shape and KG-related) than VQA-RAD. A comparison between our SLAKE and VQA-RAD is provided in Table 1.\n\nIn summary, our contributions are two-fold: \u2022 We create SLAKE, a large-scale, semantically annotated, and knowledge-enhanced bilingual dataset for training and testing Med-VQA systems. \u2022 We experiment with representative Med-VQA methods to show that SLAKE can be used as a benchmark to train systems to solve practical and complex tasks.\n\n\nTHE SLAKE DATASET\n\nIn this section, we elaborate on the construction of our SLAKE dataset. In general, we ensure the diversity of the dataset in terms of modalities (e.g., CT, MRI, and X-Ray), covered body parts (e.g., head, neck, and chest), and question types (e.g., vision-only, knowledge-based, and bilingual). \n\n\nImage Acquisition and Annotation\n\nWe select radiology images, covering healthy and unhealthy cases, from three open source datasets [7] 1 [8] 2 [9] 3 . From [8], we randomly select 179 chest X-Ray images and keep the original disease labels. From [7] and [9], we randomly choose 463 single-slice images from 3D volume cases. Then, experienced physicians label organs and diseases as detailed as possible with ITK-SNAP [10] 4 as shown in Figure 1.\n\nIn total, we annotate 642 images, including 12 diseases and 39 organs of the whole body. The diseases mainly include cancer (e.g., brain, liver, kidney, lung, etc.), and thoracic diseases (e.g., atelectasis, effusion, mass, pneumothorax, etc.). The images include 140 head CTs or MRIs, 41 neck CTs, 219 chest X-Rays or CTs, 201 abdomen CTs or MRIs, and 41 pelvic cavity CTs. The distribution is shown in Figure 2 (Left). Among these images, there are 282 CTs, 181 MRIs, and 179 X-Rays. All CTs and MRIs are axial single-slice. The number of images for each body part is set based on the complexity of the body part. For example, the number of diseases and organs in abdomen is much more than that in neck, so there are more images of abdomen than neck in the dataset.\n\n\nKnowledge Graph Construction\n\nTo answer questions that require external medical knowledge, we construct a medical knowledge graph centered on organs and related diseases, which are the main objects of radiology images. We extract a set of 52.6K triplets <head, relation, tail> with medical knowledge from OwnThink 5 , a large-scale knowledge base built on Wikipedia. Here, head and tail are entities such as organ, disease, etc., and relation represents the relationship between entities, such as function or treatment. Then, we traverse the set to retrieve triplets related to organs and the corresponding diseases. We further clean the data by manually filtering out some entities that are not presented in medical images such as gastritis and nephritis. Next, in order to extensively cover frequently referenced knowledge, we refine the filtered triplets with the following rules: (1) The triplets about an organ must describe its function or body system; (2) The triplets about a disease must describe the symptoms, locations, causes, treatment or prevention methodologies. Some examples are shown in Table 3.\n\nFinally, we make the triplets bilingual and obtain 2603 triplets in English and 2629 triplets in Chinese.\n\n\nQuestion Generation\n\nQuestions are proposed by experienced doctors. To accelerate this process, we develop an annotation system. In this system, we first pre-define a question template for each body part (i.e., head, neck, chest, abdomen, and pelvic cavity). Then, we define ten different content types (e.g., modality, position, color) for the questions, as shown in Table 2 and Figure 2 (Right). In each template, we provide many candidate questions for each content type. For example, the candidate question for a head image with the content type organ may be \"Is this a study of the head?\" or \"What organ system is imaged?\". Physicians could choose those candidate questions or amend or even rewrite them entirely based on their personal clinical experience. The flexibility of our annotation system ensures the question diversity of SLAKE. Note that because we provide different candidates for bilingual questions, the number and content of them in our dataset are not the same.\n\nMoreover, we provide semantic label for each question. Specifically, we use <vhead, , > (vhead is a placeholder) to denote vision-only questions. For a knowledge-based question like \"Which organs in this image belong to the digestive system?\", we denote it as <vhead, belong to, digestive sys- Besides, recent studies [11,12] have shown that VQA models may be susceptible to the statistical bias of answer distribution of the datasets. To mitigate the inherent bias of SLAKE, we make the answers balanced in general such that the VQA model will not be biased to the most popular answer in the dataset. For example, for the question \"Is this a study of the abdomen?\", we make sure this question is asked with abdomen images and non-abdomen images with 50\u221250 chance, thereby keeping the numbers of \"Yes\" and \"No\" balanced.\n\n\nDataset Splitting\n\nHere, we describe how to divide the obtained 642 images with 14,028 question-answer pairs and 5232 medical knowledge triplets for the training and evaluation of Med-VQA models.\n\nIn general, the splitting aims to provide a reliable measure of the generalization ability of the model trained on our dataset. Specifically, we split the dataset into training (70%), validation (15%), and test (15%) sets at the image level. The images in our dataset are split with the 75:15:15 ratio in each of the 8 categories: \"head CT\", \"head MRI\", \"neck CT\", and \"chest X-Ray\", \"chest CT\", \"abdomen CT\", \"abdomen MRI\", and \"pelvic cavity CT\". Note that we only divide the images but the questions associated with each image are not split.\n\nBesides, since VQA is usually formulated as a classification task [4,5,13], we follow the convention and make sure answers in the test set must appear in the training set. Finally, the images are split into 450 for training, 96 for validating, and 96 for testing. The number of questions of different type in each set is shown in Table 2.\n\n\nEXPERIMENTS\n\nIn this section, we conduct extensive experiments to comprehensively evaluate our SLAKE dataset. To be elaborated later, Table 4 demonstrates the usefulness and the challenge of SLAKE with commonly used Med-VQA methods. To show the effectiveness of the constructed medical knowledge graph, we conduct an ablation study presented in Table 5.  \n\n\nExperiment Setup\n\nThe pipeline of our experiments is illustrated in Figure 3. We experiment with a commonly used Med-VQA framework, stacked attention network (SAN) [13], on SLAKE. We use VGG16 [14] to extract visual features from radiology images. For bilingual questions, we first design a bilingual tokenizer to create bilingual word embeddings for the English questions and Chinese questions respectively. Then, a 1024D -LSTM is applied to extract textual semantics from these embeddings and classify types of questions. There are two sub pipelines in Figure 3. Given the extracted visual and textual features, vision-only tasks will be directed to the multimodal fusion module of SAN to create fused features for classification. For knowledge-based tasks, question-related embeddings extracted from the knowledge graph will be combined with the multimodal fused features for classification.\n\n\nDataset Analysis\n\nWe report the results for vision-only and knowledge-based questions in Table 4 and Table 5 respectively. Answers of \"closed-ended\" questions are limited multiple-choice options, while answers of \"open-ended\" questions are free-form texts.\n\nOpen-ended questions are generally harder to answer than closed-ended ones. Vision-only questions. In Table 4, we report the results in accuracy for vision-only questions in both English and Chinese. Compared with VQA in the general domain, clinical questions in Med-VQA need to be answered as accurate as possible because they relate to health and safety. It can be seen that the baseline models achieve accuracy of around 73% which is still far away from practical use in the medical do- main. There is a wide gap between this and clinical standard, which shows that SLAKE is challenging. Moreover, it can be seen that the overall accuracy is roughly the average of those of open-ended and closed-ended questions, proving that the question distribution of SLAKE is balanced. Besides, to demonstrate the usefulness of the semantic visual annotations elaborated in Section 2.1, we design another model, VGGseg+SAN. First, we pretrain a fully convolutional network (FCN) with VGG backbone by the segmentation task of radiology images with respect to the mask labels in the training set. Then, we initialize the VGG backbone in the Med-VQA model with the pretrained parameters. The overall accuracy increases from 72.73% to 75.36% with a 2.6% improvement, which shows that our semantic visual annotations could improve the reasoning abilities of the model.\n\nKnowledge-based questions. We leverage the self-built medical knowledge graph to answer knowledge-based questions. First, we randomly initialize an embedding for each entity in the knowledge graph and use the TransE [15] method to enforce the embeddings of the entities in each triplet, <head, relation, tail>, to satisfy: head + relation \u2248 tail. Then, based on the semantic textual annotations (Section 2.3), we train two LSTMs to predict the words for the \"relation\" and \"tail\" of a question separately. Next, we find the corresponding entity embeddings of the relation and tail from the graph and use them to obtain the head entity embedding based on the above approximate equation, which is then combined with the fused multimodal features for final prediction. The result is reported in Table 5. For comparison, we also try to predict answers without using the knowledge graph. The result is 2.0% lower, indicating that the constructed knowledge graph is informative and it is helpful to leverage external structural knowledge to tackle knowledge-based questions.\n\n\nCONCLUSION\n\nWe have introduced SLAKE, a new large bilingual dataset to facilitate the training and evaluation of Med-VQA systems. SLAKE is a diverse and balanced dataset containing rich visual and textual annotations and a unique medical knowledge graph, which allows the development of more powerful Med-VQA systems. Remarkably, our experiments show that the semantic annotations and external knowledge can significantly improve the performance of standard Med-VQA models. We hope SLAKE will serve as a stepping stone to push forward the research of Med-VQA.\n\n\nCOMPLIANCE WITH ETHICAL STANDARDS\n\nThis research study was conducted retrospectively using human subject data made available in open access by [7,9,8]. Ethical approval was not required as confirmed by the license attached with the open access data.\n\n\nACKNOWLEDGMENTS\n\nWe would like to thank the anonymous reviewers for their helpful comments. Thanks to Lau et al [1] for their pioneering work in Med-VQA, NIH Clinical Center for sharing their open access dataset [8], and all the doctors and medical students who helped with this research. This research was supported by the grant of P0030935 (ZVPY) funded by PolyU (UGC).\n\nFig. 1 .\n1Exemplar image and questions of our SLAKE dataset.\n\nFig. 2 .\n2Left: proportions of images of five body parts. Right: distribution of the content types of questions.\n\nFig. 3 .\n3The Med-VQA framework on our SLAKE dataset.\n\nTable 1 .\n1Comparison of SLAKE with VQA-RAD.Dataset \n# Images # QA Pairs \nQuestion Type \nLanguage \nKnowledge Graph \nVQA-RAD [1] \n315 \n3.5K \nVision-only \nEN \nSLAKE (Ours) \n642 \n14K \nKnowledge-based & Vision-only Bilingual (EN & ZH) \n\n\n\nTable 2 .\n2Statistics of questions in our SLAKE dataset.Training set Validation set Test set \nPlane \n931 \n173 \n176 \nQuality \n535 \n109 \n118 \nModality \n1072 \n203 \n217 \nPosition \n1876 \n412 \n390 \nOrgan \n2125 \n462 \n454 \nKG \n1202 \n278 \n260 \nAbnormal \n1230 \n245 \n221 \nColor \n424 \n108 \n115 \nShape \n157 \n42 \n46 \nSize \n297 \n77 \n73 \nTotal \n9849 \n2109 \n2070 \n\n\n\nTable 3 .\n3Examples of our medical knowledge graph.Examples \n\nOrgan \n\n<Heart, Function, Promote blood flow> \n<Kidney, Belong to, Urinary System> \n<Duodenum, Length, 20-25cm> \n\nDisease \n\n<Pneumonia, Location, Lung> \n<Lung Cancer, Cause, Smoke> \n<Brain Tumor, Symptom, Visual impairment> \n<Cardiomegaly, Treatment, Medication> \n<Atelectasis, Prevention, Exercise> \n\ntem>. Such labeling helps to distinguish question type and \nidentify the part of the question involving external knowledge. \n\n\nTable 4 .\n4Accuracy for vision-only questions (%).Language \nModels \nOverall \nOpen-Closed-\nended \nended \n\nEnglish \nVGG+SAN \n72.73 \n70.34 \n76.13 \nVGGseg+SAN \n75.36 \n72.20 \n79.84 \nChinese \nVGG+SAN \n74.27 \n73.64 \n75.20 \n\n\n\nTable 5 .\n5Accuracy for knowledge-based questions (%).Language \nModels \nOverall \n\nEnglish \nVGG+SAN \n70.27 \nVGG+SAN+KG \n72.30 \nChinese \nVGG+SAN+KG \n75.01 \n\n\nhttp://medicaldecathlon.com 2 https://nihcc.app.box.com/v/ChestXray-NIHCC 3 https://doi.org/10.5281/zenodo.3431873 4 http://www.itksnap.org 5 https://www.ownthink.com\n\nA dataset of clinically generated visual questions and answers about radiology images. J Jason, Soumya Lau, Asma Gayen, Dina Ben Abacha, Demner-Fushman, Scientific data. 51Jason J Lau, Soumya Gayen, Asma Ben Abacha, and Dina Demner-Fushman, \"A dataset of clinically gener- ated visual questions and answers about radiology im- ages,\" Scientific data, vol. 5, no. 1, pp. 1-10, 2018.\n\nClevr: A diagnostic dataset for compositional language and elementary visual reasoning. Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, Ross Girshick, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionJustin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Gir- shick, \"Clevr: A diagnostic dataset for compositional language and elementary visual reasoning,\" in Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 2901-2910.\n\nVqa: Visual question answering. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, Devi Parikh, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar- garet Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh, \"Vqa: Visual question answering,\" in Pro- ceedings of the IEEE international conference on com- puter vision, 2015, pp. 2425-2433.\n\nOvercoming data limitation in medical visual question answering. Thanh-Toan Binh D Nguyen, Do, X Binh, Tuong Nguyen, Erman Do, Quang D Tjiputra, Tran, International Conference on Medical Image Computing and Computer-Assisted Intervention. SpringerBinh D Nguyen, Thanh-Toan Do, Binh X Nguyen, Tuong Do, Erman Tjiputra, and Quang D Tran, \"Over- coming data limitation in medical visual question an- swering,\" in International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2019, pp. 522-530.\n\nMedical visual question answering via conditional reasoning. Li-Ming Zhan, Bo Liu, Lu Fan, Jiaxin Chen, Xiao-Ming Wu, Proceedings of the 28th ACM International Conference on Multimedia. the 28th ACM International Conference on MultimediaNew York, NY, USA, 2020, MM '20Association for Computing MachineryLi-Ming Zhan, Bo Liu, Lu Fan, Jiaxin Chen, and Xiao- Ming Wu, \"Medical visual question answering via con- ditional reasoning,\" in Proceedings of the 28th ACM In- ternational Conference on Multimedia, New York, NY, USA, 2020, MM '20, p. 2345-2354, Association for Computing Machinery.\n\nFvqa: Fact-based visual question answering. Peng Wang, Qi Wu, Chunhua Shen, Anthony Dick, Anton Van Den, Hengel, IEEE transactions on pattern analysis and machine intelligence. 40Peng Wang, Qi Wu, Chunhua Shen, Anthony Dick, and Anton van den Hengel, \"Fvqa: Fact-based visual ques- tion answering,\" IEEE transactions on pattern analy- sis and machine intelligence, vol. 40, no. 10, pp. 2413- 2427, 2018.\n\nA large annotated medical image dataset for the development and evaluation of segmentation algorithms. Michela Amber L Simpson, Spyridon Antonelli, Michel Bakas, Keyvan Bilello, Bram Farahani, Annette Van Ginneken, Kopp-Schneider, Geert Bennett A Landman, Bjoern Litjens, Menze, arXiv:1902.09063arXiv preprintAmber L Simpson, Michela Antonelli, Spyridon Bakas, Michel Bilello, Keyvan Farahani, Bram Van Ginneken, Annette Kopp-Schneider, Bennett A Landman, Geert Litjens, Bjoern Menze, et al., \"A large annotated medical image dataset for the development and eval- uation of segmentation algorithms,\" arXiv preprint arXiv:1902.09063, 2019.\n\nChestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases. Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, Ronald M Summers, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionXiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Ronald M Summers, \"Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and lo- calization of common thorax diseases,\" in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 2097-2106.\n\nCHAOS -Combined (CT-MR) Healthy Abdominal Organ Segmentation Challenge Data. M Ali Emre Kavur, Oguz Selver, Mustafa Dicle, N Bar\u0131\u015f, Sinem Gezer, Ali Emre Kavur, M. Alper Selver, Oguz Dicle, Mustafa Bar\u0131\u015f, and N. Sinem Gezer, \"CHAOS -Combined (CT- MR) Healthy Abdominal Organ Segmentation Chal- lenge Data,\" Apr. 2019.\n\nUser-guided 3D active contour segmentation of anatomical structures: Significantly improved efficiency and reliability. Paul A Yushkevich, Joseph Piven, Heather Cody Hazlett, Rachel Gimpel Smith, Sean Ho, James C Gee, Guido Gerig, Neuroimage. 313Paul A. Yushkevich, Joseph Piven, Heather Cody Ha- zlett, Rachel Gimpel Smith, Sean Ho, James C. Gee, and Guido Gerig, \"User-guided 3D active contour segmen- tation of anatomical structures: Significantly improved efficiency and reliability,\" Neuroimage, vol. 31, no. 3, pp. 1116-1128, 2006.\n\nAnalyzing the behavior of visual question answering models. Aishwarya Agrawal, Dhruv Batra, Devi Parikh, arXiv:1606.07356arXiv preprintAishwarya Agrawal, Dhruv Batra, and Devi Parikh, \"Analyzing the behavior of visual question answering models,\" arXiv preprint arXiv:1606.07356, 2016.\n\nYin and yang: Balancing and answering binary visual questions. Peng Zhang, Yash Goyal, Douglas Summers-Stay, Dhruv Batra, Devi Parikh, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionPeng Zhang, Yash Goyal, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh, \"Yin and yang: Bal- ancing and answering binary visual questions,\" in Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 5014-5022.\n\nStacked attention networks for image question answering. Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Smola, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionZichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola, \"Stacked attention networks for im- age question answering,\" in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 21-29.\n\nVery deep convolutional networks for large-scale image recognition. Karen Simonyan, Andrew Zisserman, arXiv:1409.1556arXiv preprintKaren Simonyan and Andrew Zisserman, \"Very deep convolutional networks for large-scale image recogni- tion,\" arXiv preprint arXiv:1409.1556, 2014.\n\nTranslating embeddings for modeling multi-relational data. Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, Oksana Yakhnenko, Advances in neural information processing systems. Antoine Bordes, Nicolas Usunier, Alberto Garcia- Duran, Jason Weston, and Oksana Yakhnenko, \"Trans- lating embeddings for modeling multi-relational data,\" in Advances in neural information processing systems, 2013, pp. 2787-2795.\n", "annotations": {"author": "[{\"end\":105,\"start\":98},{\"end\":119,\"start\":106},{\"end\":126,\"start\":120},{\"end\":208,\"start\":127},{\"end\":298,\"start\":209},{\"end\":386,\"start\":299},{\"end\":459,\"start\":387}]", "publisher": null, "author_last_name": "[{\"end\":104,\"start\":101},{\"end\":118,\"start\":114},{\"end\":125,\"start\":123},{\"end\":133,\"start\":131},{\"end\":217,\"start\":213},{\"end\":311,\"start\":309}]", "author_first_name": "[{\"end\":100,\"start\":98},{\"end\":113,\"start\":106},{\"end\":122,\"start\":120},{\"end\":130,\"start\":127},{\"end\":212,\"start\":209},{\"end\":308,\"start\":299}]", "author_affiliation": "[{\"end\":207,\"start\":135},{\"end\":297,\"start\":219},{\"end\":385,\"start\":313},{\"end\":458,\"start\":388}]", "title": "[{\"end\":95,\"start\":1},{\"end\":554,\"start\":460}]", "venue": null, "abstract": "[{\"end\":1324,\"start\":653}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2601,\"start\":2598},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2603,\"start\":2601},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2959,\"start\":2956},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3130,\"start\":3127},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3132,\"start\":3130},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4563,\"start\":4560},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6008,\"start\":6005},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6098,\"start\":6095},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6106,\"start\":6103},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9596,\"start\":9592},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9599,\"start\":9596},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10909,\"start\":10906},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10911,\"start\":10909},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10914,\"start\":10911},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11707,\"start\":11703},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11736,\"start\":11732},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":14270,\"start\":14266},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":15829,\"start\":15826},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":15831,\"start\":15829},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":15833,\"start\":15831},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":16050,\"start\":16047},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":16150,\"start\":16147}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":16368,\"start\":16307},{\"attributes\":{\"id\":\"fig_1\"},\"end\":16482,\"start\":16369},{\"attributes\":{\"id\":\"fig_2\"},\"end\":16537,\"start\":16483},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":16772,\"start\":16538},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":17122,\"start\":16773},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":17613,\"start\":17123},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":17832,\"start\":17614},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":17989,\"start\":17833}]", "paragraph": "[{\"end\":1811,\"start\":1340},{\"end\":3825,\"start\":1813},{\"end\":5188,\"start\":3827},{\"end\":5527,\"start\":5190},{\"end\":5845,\"start\":5549},{\"end\":6294,\"start\":5882},{\"end\":7063,\"start\":6296},{\"end\":8179,\"start\":7096},{\"end\":8286,\"start\":8181},{\"end\":9272,\"start\":8310},{\"end\":10094,\"start\":9274},{\"end\":10292,\"start\":10116},{\"end\":10838,\"start\":10294},{\"end\":11178,\"start\":10840},{\"end\":11536,\"start\":11194},{\"end\":12433,\"start\":11557},{\"end\":12692,\"start\":12454},{\"end\":14048,\"start\":12694},{\"end\":15118,\"start\":14050},{\"end\":15680,\"start\":15133},{\"end\":15932,\"start\":15718},{\"end\":16306,\"start\":15952}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":5187,\"start\":5180},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":8178,\"start\":8171},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":8664,\"start\":8657},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":11177,\"start\":11170},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":11322,\"start\":11315},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":11533,\"start\":11526},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":12532,\"start\":12525},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":12544,\"start\":12537},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":12803,\"start\":12796},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":14849,\"start\":14842}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1338,\"start\":1326},{\"attributes\":{\"n\":\"2.\"},\"end\":5547,\"start\":5530},{\"attributes\":{\"n\":\"2.1.\"},\"end\":5880,\"start\":5848},{\"attributes\":{\"n\":\"2.2.\"},\"end\":7094,\"start\":7066},{\"attributes\":{\"n\":\"2.3.\"},\"end\":8308,\"start\":8289},{\"attributes\":{\"n\":\"2.4.\"},\"end\":10114,\"start\":10097},{\"attributes\":{\"n\":\"3.\"},\"end\":11192,\"start\":11181},{\"attributes\":{\"n\":\"3.1.\"},\"end\":11555,\"start\":11539},{\"attributes\":{\"n\":\"3.2.\"},\"end\":12452,\"start\":12436},{\"attributes\":{\"n\":\"4.\"},\"end\":15131,\"start\":15121},{\"attributes\":{\"n\":\"5.\"},\"end\":15716,\"start\":15683},{\"attributes\":{\"n\":\"6.\"},\"end\":15950,\"start\":15935},{\"end\":16316,\"start\":16308},{\"end\":16378,\"start\":16370},{\"end\":16492,\"start\":16484},{\"end\":16548,\"start\":16539},{\"end\":16783,\"start\":16774},{\"end\":17133,\"start\":17124},{\"end\":17624,\"start\":17615},{\"end\":17843,\"start\":17834}]", "table": "[{\"end\":16772,\"start\":16583},{\"end\":17122,\"start\":16830},{\"end\":17613,\"start\":17175},{\"end\":17832,\"start\":17665},{\"end\":17989,\"start\":17888}]", "figure_caption": "[{\"end\":16368,\"start\":16318},{\"end\":16482,\"start\":16380},{\"end\":16537,\"start\":16494},{\"end\":16583,\"start\":16550},{\"end\":16830,\"start\":16785},{\"end\":17175,\"start\":17135},{\"end\":17665,\"start\":17626},{\"end\":17888,\"start\":17845}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4268,\"start\":4260},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6293,\"start\":6285},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":6708,\"start\":6700},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":8677,\"start\":8669},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":11615,\"start\":11607},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12102,\"start\":12094}]", "bib_author_first_name": "[{\"end\":18246,\"start\":18245},{\"end\":18260,\"start\":18254},{\"end\":18270,\"start\":18266},{\"end\":18282,\"start\":18278},{\"end\":18635,\"start\":18629},{\"end\":18652,\"start\":18645},{\"end\":18671,\"start\":18664},{\"end\":18690,\"start\":18688},{\"end\":18708,\"start\":18700},{\"end\":18722,\"start\":18718},{\"end\":19223,\"start\":19214},{\"end\":19240,\"start\":19231},{\"end\":19256,\"start\":19250},{\"end\":19269,\"start\":19261},{\"end\":19285,\"start\":19280},{\"end\":19301,\"start\":19293},{\"end\":19315,\"start\":19311},{\"end\":19770,\"start\":19760},{\"end\":19791,\"start\":19790},{\"end\":19803,\"start\":19798},{\"end\":19817,\"start\":19812},{\"end\":19829,\"start\":19822},{\"end\":20291,\"start\":20284},{\"end\":20300,\"start\":20298},{\"end\":20308,\"start\":20306},{\"end\":20320,\"start\":20314},{\"end\":20336,\"start\":20327},{\"end\":20859,\"start\":20855},{\"end\":20868,\"start\":20866},{\"end\":20880,\"start\":20873},{\"end\":20894,\"start\":20887},{\"end\":20906,\"start\":20901},{\"end\":21326,\"start\":21319},{\"end\":21352,\"start\":21344},{\"end\":21370,\"start\":21364},{\"end\":21384,\"start\":21378},{\"end\":21398,\"start\":21394},{\"end\":21416,\"start\":21409},{\"end\":21452,\"start\":21447},{\"end\":21478,\"start\":21472},{\"end\":22008,\"start\":22000},{\"end\":22020,\"start\":22015},{\"end\":22029,\"start\":22027},{\"end\":22041,\"start\":22034},{\"end\":22058,\"start\":22046},{\"end\":22076,\"start\":22068},{\"end\":22647,\"start\":22646},{\"end\":22668,\"start\":22664},{\"end\":22684,\"start\":22677},{\"end\":22693,\"start\":22692},{\"end\":23012,\"start\":23008},{\"end\":23014,\"start\":23013},{\"end\":23033,\"start\":23027},{\"end\":23048,\"start\":23041},{\"end\":23053,\"start\":23049},{\"end\":23069,\"start\":23063},{\"end\":23076,\"start\":23070},{\"end\":23088,\"start\":23084},{\"end\":23098,\"start\":23093},{\"end\":23100,\"start\":23099},{\"end\":23111,\"start\":23106},{\"end\":23496,\"start\":23487},{\"end\":23511,\"start\":23506},{\"end\":23523,\"start\":23519},{\"end\":23780,\"start\":23776},{\"end\":23792,\"start\":23788},{\"end\":23807,\"start\":23800},{\"end\":23827,\"start\":23822},{\"end\":23839,\"start\":23835},{\"end\":24301,\"start\":24295},{\"end\":24316,\"start\":24308},{\"end\":24329,\"start\":24321},{\"end\":24337,\"start\":24335},{\"end\":24348,\"start\":24344},{\"end\":24796,\"start\":24791},{\"end\":24813,\"start\":24807},{\"end\":25068,\"start\":25061},{\"end\":25084,\"start\":25077},{\"end\":25101,\"start\":25094},{\"end\":25121,\"start\":25116},{\"end\":25136,\"start\":25130}]", "bib_author_last_name": "[{\"end\":18252,\"start\":18247},{\"end\":18264,\"start\":18261},{\"end\":18276,\"start\":18271},{\"end\":18293,\"start\":18283},{\"end\":18309,\"start\":18295},{\"end\":18643,\"start\":18636},{\"end\":18662,\"start\":18653},{\"end\":18686,\"start\":18672},{\"end\":18698,\"start\":18691},{\"end\":18716,\"start\":18709},{\"end\":18731,\"start\":18723},{\"end\":19229,\"start\":19224},{\"end\":19248,\"start\":19241},{\"end\":19259,\"start\":19257},{\"end\":19278,\"start\":19270},{\"end\":19291,\"start\":19286},{\"end\":19309,\"start\":19302},{\"end\":19322,\"start\":19316},{\"end\":19784,\"start\":19771},{\"end\":19788,\"start\":19786},{\"end\":19796,\"start\":19792},{\"end\":19810,\"start\":19804},{\"end\":19820,\"start\":19818},{\"end\":19838,\"start\":19830},{\"end\":19844,\"start\":19840},{\"end\":20296,\"start\":20292},{\"end\":20304,\"start\":20301},{\"end\":20312,\"start\":20309},{\"end\":20325,\"start\":20321},{\"end\":20339,\"start\":20337},{\"end\":20864,\"start\":20860},{\"end\":20871,\"start\":20869},{\"end\":20885,\"start\":20881},{\"end\":20899,\"start\":20895},{\"end\":20914,\"start\":20907},{\"end\":20922,\"start\":20916},{\"end\":21342,\"start\":21327},{\"end\":21362,\"start\":21353},{\"end\":21376,\"start\":21371},{\"end\":21392,\"start\":21385},{\"end\":21407,\"start\":21399},{\"end\":21429,\"start\":21417},{\"end\":21445,\"start\":21431},{\"end\":21470,\"start\":21453},{\"end\":21486,\"start\":21479},{\"end\":21493,\"start\":21488},{\"end\":22013,\"start\":22009},{\"end\":22025,\"start\":22021},{\"end\":22032,\"start\":22030},{\"end\":22044,\"start\":22042},{\"end\":22066,\"start\":22059},{\"end\":22084,\"start\":22077},{\"end\":22662,\"start\":22648},{\"end\":22675,\"start\":22669},{\"end\":22690,\"start\":22685},{\"end\":22699,\"start\":22694},{\"end\":22712,\"start\":22701},{\"end\":23025,\"start\":23015},{\"end\":23039,\"start\":23034},{\"end\":23061,\"start\":23054},{\"end\":23082,\"start\":23077},{\"end\":23091,\"start\":23089},{\"end\":23104,\"start\":23101},{\"end\":23117,\"start\":23112},{\"end\":23504,\"start\":23497},{\"end\":23517,\"start\":23512},{\"end\":23530,\"start\":23524},{\"end\":23786,\"start\":23781},{\"end\":23798,\"start\":23793},{\"end\":23820,\"start\":23808},{\"end\":23833,\"start\":23828},{\"end\":23846,\"start\":23840},{\"end\":24306,\"start\":24302},{\"end\":24319,\"start\":24317},{\"end\":24333,\"start\":24330},{\"end\":24342,\"start\":24338},{\"end\":24354,\"start\":24349},{\"end\":24805,\"start\":24797},{\"end\":24823,\"start\":24814},{\"end\":25075,\"start\":25069},{\"end\":25092,\"start\":25085},{\"end\":25114,\"start\":25102},{\"end\":25128,\"start\":25122},{\"end\":25146,\"start\":25137}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":53712941},\"end\":18539,\"start\":18158},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":15458100},\"end\":19180,\"start\":18541},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":3180429},\"end\":19693,\"start\":19182},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":202889111},\"end\":20221,\"start\":19695},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":222278480},\"end\":20809,\"start\":20223},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":7483388},\"end\":21214,\"start\":20811},{\"attributes\":{\"doi\":\"arXiv:1902.09063\",\"id\":\"b6\"},\"end\":21854,\"start\":21216},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":8945673},\"end\":22567,\"start\":21856},{\"attributes\":{\"id\":\"b8\"},\"end\":22886,\"start\":22569},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":1660596},\"end\":23425,\"start\":22888},{\"attributes\":{\"doi\":\"arXiv:1606.07356\",\"id\":\"b10\"},\"end\":23711,\"start\":23427},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":6733279},\"end\":24236,\"start\":23713},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":8849206},\"end\":24721,\"start\":24238},{\"attributes\":{\"doi\":\"arXiv:1409.1556\",\"id\":\"b13\"},\"end\":25000,\"start\":24723},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":14941970},\"end\":25428,\"start\":25002}]", "bib_title": "[{\"end\":18243,\"start\":18158},{\"end\":18627,\"start\":18541},{\"end\":19212,\"start\":19182},{\"end\":19758,\"start\":19695},{\"end\":20282,\"start\":20223},{\"end\":20853,\"start\":20811},{\"end\":21998,\"start\":21856},{\"end\":23006,\"start\":22888},{\"end\":23774,\"start\":23713},{\"end\":24293,\"start\":24238},{\"end\":25059,\"start\":25002}]", "bib_author": "[{\"end\":18254,\"start\":18245},{\"end\":18266,\"start\":18254},{\"end\":18278,\"start\":18266},{\"end\":18295,\"start\":18278},{\"end\":18311,\"start\":18295},{\"end\":18645,\"start\":18629},{\"end\":18664,\"start\":18645},{\"end\":18688,\"start\":18664},{\"end\":18700,\"start\":18688},{\"end\":18718,\"start\":18700},{\"end\":18733,\"start\":18718},{\"end\":19231,\"start\":19214},{\"end\":19250,\"start\":19231},{\"end\":19261,\"start\":19250},{\"end\":19280,\"start\":19261},{\"end\":19293,\"start\":19280},{\"end\":19311,\"start\":19293},{\"end\":19324,\"start\":19311},{\"end\":19786,\"start\":19760},{\"end\":19790,\"start\":19786},{\"end\":19798,\"start\":19790},{\"end\":19812,\"start\":19798},{\"end\":19822,\"start\":19812},{\"end\":19840,\"start\":19822},{\"end\":19846,\"start\":19840},{\"end\":20298,\"start\":20284},{\"end\":20306,\"start\":20298},{\"end\":20314,\"start\":20306},{\"end\":20327,\"start\":20314},{\"end\":20341,\"start\":20327},{\"end\":20866,\"start\":20855},{\"end\":20873,\"start\":20866},{\"end\":20887,\"start\":20873},{\"end\":20901,\"start\":20887},{\"end\":20916,\"start\":20901},{\"end\":20924,\"start\":20916},{\"end\":21344,\"start\":21319},{\"end\":21364,\"start\":21344},{\"end\":21378,\"start\":21364},{\"end\":21394,\"start\":21378},{\"end\":21409,\"start\":21394},{\"end\":21431,\"start\":21409},{\"end\":21447,\"start\":21431},{\"end\":21472,\"start\":21447},{\"end\":21488,\"start\":21472},{\"end\":21495,\"start\":21488},{\"end\":22015,\"start\":22000},{\"end\":22027,\"start\":22015},{\"end\":22034,\"start\":22027},{\"end\":22046,\"start\":22034},{\"end\":22068,\"start\":22046},{\"end\":22086,\"start\":22068},{\"end\":22664,\"start\":22646},{\"end\":22677,\"start\":22664},{\"end\":22692,\"start\":22677},{\"end\":22701,\"start\":22692},{\"end\":22714,\"start\":22701},{\"end\":23027,\"start\":23008},{\"end\":23041,\"start\":23027},{\"end\":23063,\"start\":23041},{\"end\":23084,\"start\":23063},{\"end\":23093,\"start\":23084},{\"end\":23106,\"start\":23093},{\"end\":23119,\"start\":23106},{\"end\":23506,\"start\":23487},{\"end\":23519,\"start\":23506},{\"end\":23532,\"start\":23519},{\"end\":23788,\"start\":23776},{\"end\":23800,\"start\":23788},{\"end\":23822,\"start\":23800},{\"end\":23835,\"start\":23822},{\"end\":23848,\"start\":23835},{\"end\":24308,\"start\":24295},{\"end\":24321,\"start\":24308},{\"end\":24335,\"start\":24321},{\"end\":24344,\"start\":24335},{\"end\":24356,\"start\":24344},{\"end\":24807,\"start\":24791},{\"end\":24825,\"start\":24807},{\"end\":25077,\"start\":25061},{\"end\":25094,\"start\":25077},{\"end\":25116,\"start\":25094},{\"end\":25130,\"start\":25116},{\"end\":25148,\"start\":25130}]", "bib_venue": "[{\"end\":18326,\"start\":18311},{\"end\":18810,\"start\":18733},{\"end\":19391,\"start\":19324},{\"end\":19932,\"start\":19846},{\"end\":20407,\"start\":20341},{\"end\":20986,\"start\":20924},{\"end\":21317,\"start\":21216},{\"end\":22163,\"start\":22086},{\"end\":22644,\"start\":22569},{\"end\":23129,\"start\":23119},{\"end\":23485,\"start\":23427},{\"end\":23925,\"start\":23848},{\"end\":24433,\"start\":24356},{\"end\":24789,\"start\":24723},{\"end\":25197,\"start\":25148},{\"end\":18874,\"start\":18812},{\"end\":19445,\"start\":19393},{\"end\":20491,\"start\":20409},{\"end\":22227,\"start\":22165},{\"end\":23989,\"start\":23927},{\"end\":24497,\"start\":24435}]"}}}, "year": 2023, "month": 12, "day": 17}