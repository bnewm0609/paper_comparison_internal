{"id": 211003742, "updated": "2023-10-06 19:18:36.04", "metadata": {"title": "Self-Adversarial Learning with Comparative Discrimination for Text Generation", "authors": "[{\"first\":\"Wangchunshu\",\"last\":\"Zhou\",\"middle\":[]},{\"first\":\"Tao\",\"last\":\"Ge\",\"middle\":[]},{\"first\":\"Ke\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Furu\",\"last\":\"Wei\",\"middle\":[]},{\"first\":\"Ming\",\"last\":\"Zhou\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 1, "day": 31}, "abstract": "Conventional Generative Adversarial Networks (GANs) for text generation tend to have issues of reward sparsity and mode collapse that affect the quality and diversity of generated samples. To address the issues, we propose a novel self-adversarial learning (SAL) paradigm for improving GANs' performance in text generation. In contrast to standard GANs that use a binary classifier as its discriminator to predict whether a sample is real or generated, SAL employs a comparative discriminator which is a pairwise classifier for comparing the text quality between a pair of samples. During training, SAL rewards the generator when its currently generated sentence is found to be better than its previously generated samples. This self-improvement reward mechanism allows the model to receive credits more easily and avoid collapsing towards the limited number of real samples, which not only helps alleviate the reward sparsity issue but also reduces the risk of mode collapse. Experiments on text generation benchmark datasets show that our proposed approach substantially improves both the quality and the diversity, and yields more stable performance compared to the previous GANs for text generation.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2001.11691", "mag": "2995028880", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iclr/ZhouGXW020", "doi": null}}, "content": {"source": {"pdf_hash": "0a2a6ceb81855761e8e5d14ec43901714d455b92", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2001.11691v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "2e809a220dfdeef566296ef4e616648dbf91ca03", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/0a2a6ceb81855761e8e5d14ec43901714d455b92.txt", "contents": "\nSELF-ADVERSARIAL LEARNING WITH COMPARATIVE DISCRIMINATION FOR TEXT GENERATION\n\n\nWangchunshu Zhou zhouwangchunshu@buaa.edu.cn \nBeihang University\nBeijingChina\n\nTao Ge \nMicrosoft Research Asia\nBeijingChina\n\nKe Xu kexu@nlsde.buaa.edu.cntage \nBeihang University\nBeijingChina\n\nFuru Wei \nMicrosoft Research Asia\nBeijingChina\n\nMing Zhou mingzhou@microsoft.com \nMicrosoft Research Asia\nBeijingChina\n\nSELF-ADVERSARIAL LEARNING WITH COMPARATIVE DISCRIMINATION FOR TEXT GENERATION\nPublished as a conference paper at ICLR 2020\nConventional Generative Adversarial Networks (GANs) for text generation tend to have issues of reward sparsity and mode collapse that affect the quality and diversity of generated samples. To address the issues, we propose a novel self-adversarial learning (SAL) paradigm for improving GANs' performance in text generation. In contrast to standard GANs that use a binary classifier as its discriminator to predict whether a sample is real or generated, SAL employs a comparative discriminator which is a pairwise classifier for comparing the text quality between a pair of samples. During training, SAL rewards the generator when its currently generated sentence is found to be better than its previously generated samples. This self-improvement reward mechanism allows the model to receive credits more easily and avoid collapsing towards the limited number of real samples, which not only helps alleviate the reward sparsity issue but also reduces the risk of mode collapse. Experiments on text generation benchmark datasets show that our proposed approach substantially improves both the quality and the diversity, and yields more stable performance compared to the previous GANs for text generation.\n\nINTRODUCTION\n\nGenerative Adversarial Networks  (GANs) have achieved tremendous success for image generation and received much attention in computer vision. For text generation, however, the performance of GANs is severely limited due to reward sparsity and mode collapse: reward sparsity refers to the difficulty for the generator to receive reward signals when its generated samples can hardly fool the discriminator that is much easier to train; while mode collapse refers to the phenomenon that the generator only learns limited patterns from the real data. As a result, both the quality and diversity of generated text samples are limited.\n\nTo address the above issues, we propose a novel self-adversarial learning (SAL) paradigm for improving adversarial text generation. In contrast to standard GANs (Figure 1(a)) that use a binary classifier as its discriminator to predict whether a sample is real or generated, SAL employs a comparative discriminator which is a pairwise classifier assessing whether the currently generated sample is better than its previously generated one, as shown in Figure 1(b). During training, SAL rewards the generator when its currently generated samples are found to be better than its previously generated samples. In the earlier training stage when the quality of generated samples is far below the real data, this self-improvement reward mechanism makes it easier for the generator to receive non-sparse rewards with informative learning signals, effectively alleviating the reward sparsity issue; while in the later training stage, SAL can prevent a sample from keeping receiving high reward as the self-improvement for a popular mode will become more and more difficult, and therefore help the generator avoid collapsing toward the limited patterns of real data. We comprehensively evaluate the proposed self-adversarial learning paradigm in both synthetic data and real data on the text generation benchmark platform (Zhu et al., 2018). Compared to the previous approaches for adversarial text generation (Yu et al., 2017;Che et al., 2017;Lin et al., 2017), our  Figure 1: (a) Conventional adversarial learning that uses a binary real/fake classifier as its discriminator; (b): Self-adversarial learning that employs a comparative discriminator to compare the currently generated sample to its previously generated samples for obtaining rewards through self-improvement.\n\napproach shows a substantial improvement in terms of both the quality and the diversity of generated samples as well as better performance stability in adversarial learning.\n\n\nBACKGROUND: ADVERSARIAL TEXT GENERATION\n\nAdversarial text generation has drawn much attention in recent years due to its advantages (e.g., sequence-level guidance without the exposure bias issue (Bengio et al., 2015)) over maximum likelihood estimation (MLE) for natural language generation. It formulates the learning process as a minimax game between a generator G \u03b8 parameterized by \u03b8 and a discriminator D \u03c6 parameterized by \u03c6: the discriminator is trained to distinguish between the samples drawn from the real data distribution p data and the samples generated by the generator; while the generator is trained to generate samples that can \"fool\" the discriminator. The adversarial learning objective of the generator and the discriminator can be formulated as:\nmin \u03b8 max \u03c6 E x\u223cpdata [log D \u03c6 (x)] + E z\u223cpz [log(1 \u2212 D \u03c6 (G \u03b8 (z)))](1)\nwhere x is a sample from the real data, G \u03b8 (z) is a sample generated by the generator with the initialization z that is drawn from the noise distribution p z (e.g., standard normal distribution).\n\nWhile GANs have shown some promising results (Yu et al., 2017;, there are two fundamental issues that impede their progress in text generation: (i) Reward sparsity, which is due to the fact that the discriminator tends to learn much better than the generator and thus easily recognizes generated samples as fake. In such cases, it will be difficult for the generator to receive rewards; (ii) Mode collapse, which arises from the intrinsic nature of GANs and leads the adversarial models to only learn the limited patterns from the real samples. These two issues limit the ability of GANs to generate high-quality and diverse text samples, which have not been well addressed yet.\n\n\nSELF-ADVERSARIAL LEARNING\n\nTo address the aforementioned issues, we propose a novel self-adversarial learning (SAL) paradigm. Inspired by self-play (Silver et al., 2017;Rennie et al., 2017) in reinforcement learning, the core idea of SAL is to reward the generator if its currently generated sample is found to be better than its previously generated ones. Like AlphaGo (Silver et al., 2017), the generator in SAL struggles to learn to generate better samples than its previously generated samples for passing the \"self-improvement\" test by a comparative discriminator, which is a pairwise classifier trained to compare the quality of two samples, as Figure 1(b) shows.\n\nCompared to conventional GANs (Figure 1(a)), SAL has the following advantages: First, in the earlier training stage when the quality of generated samples is far below the real data, the self-improvement reward mechanism of SAL allows the generator to receive informative learning signals more easily as it makes the assessment of sample quality better adapted to the current capability of the generator, making it less likely to suffer from the issue of reward sparsity; Second, in the later training stage when the generated samples' quality is high, SAL can prevent a sample from keeping receiving high reward as it will become more and more difficult to pass the \"self-improvement\" test, thus reducing the risk of the generator collapsing towards limited patterns. The self-improvement mechanism and the 'tie' option in the comparative discriminator also provides a reasonable baseline which corresponds to cases where newly generated samples are found to be indistinguishable with previous ones, thus improving the training stability. We provide a more detailed qualitative analysis of why the proposed self-adversarial learning paradigm can alleviate these problems in Appendix.\n\n\nCOMPARATIVE DISCRIMINATOR\n\nAs introduced above, the core component for SAL is the comparative discriminator, which is a pairwise classifier comparing the quality of two samples. It learns a total order of sample quality and encodes the inductive bias that one sample is better (>), worse (<), or indistinguishable (\u2248) in terms of its quality compared to the other. For a (text) sample, the comparative discriminator can offer more informative learning signals than the conventional binary (i.e., real/fake) classification based discriminator because the sample can receive multiple feedbacks from the comparative discriminator by comparing it with multiple other samples.\n\nFor training the comparative discriminator, we construct pairwise training examples from the real and generated samples, as Figure 2 shows. For a real sample s + and a generated sample s \u2212 , we assign the label \"better (>)\" to the pair (s + , s \u2212 ) and \"worse (<)\" to (s \u2212 , s + ). For two samples both from real data or from the generated samples, we assign the label \"indistinguishable (\u2248)\" to such pairs (i.e., (s i + , s j + ) and (s i \u2212 , s j \u2212 )). For a training set with n real samples and n generated samples, the comparative discrimination can construct 2n 2 pairwise training examples, allowing to enhance the generalization ability of the comparative discriminator.\n\nMoreover, to improve the model's ability to distinguish between good generated samples and bad generated samples for self-play learning, we additionally select the samples generated during the later stage of MLE training as pseudo-real samples, and select those generated in the earlier epochs when the generator does not fully converge as fake sentences. We then pair the pseudo-real samples with the fake samples to construct training instances to supervise the model to compare their qualities. In this way, the comparative discriminator is prevented from being taught to always recognize two generated samples as equally bad and assign zero reward to the generator. As a result, it can become more sensitive to the quality difference in a pair of text samples and thus allow the generator to receive rewards more easily.\n\n\nTRAINING\n\nBefore we formally introduce the training procedure for SAL, we first define the learning objective for the comparative discriminator D \u03c6 and the generator G \u03b8 in SAL:\nL D = \u2212E (x1,x2)\u223c(M\u222apdata(x)) 2 [log D Q(x1,x2) \u03c6 (x 1 , x 2 )]\n(2)\nL G = \u2212E (z,xr)\u223c(pz(z),M) [ q\u2208{>,<,\u2248} w q log D q \u03c6 (G \u03b8 (z), x r )](3)\nIn Eq (2) and Eq (3), M is the set of previous generated samples by the generator, Q(x 1 , x 2 ) \u2208 {>, <, \u2248} is the true label for the pair (x 1 , x 2 ), D q \u03c6 (x 1 , x 2 ) is the probability of the comparative discriminator's prediction being q (q \u2208 {>, <, \u2248}) for the pair (x 1 , x 2 ). w q is the reward weight for the case q, which is a hyperparameter for SAL. If the generator generates a sample G \u03b8 (z) that is better (>) than its previously generated sample x r , it receives a positive reward; if G \u03b8 (z) is worse (<) than x r , it receives a negative reward; while if the quality of G \u03b8 (z) is classified as similar (\u2248) to x r , it receives zero credit. Therefore, we have w (>) > 0 = w (\u2248) > w (<) .\n\nSince L G can only be directly optimized in standard continuous GAN training, we alternatively employ the policy gradient algorithm (Sutton et al., 2000) to train the generator, as previous approaches for adversarial text generation. For SAL, we define the reward for a generated sample x g compared with a reference sample x r which is a previous generated sample by the generator as the weighted reward based on the probability distribution of the prediction by the comparative discriminator:\n\u03b3 \u03c6 (x g , x r ) = w (>) D (>) \u03c6 (x g , x r ) + w (<) D (<) \u03c6 (x g , x r ) + w (\u2248) D (\u2248) \u03c6 (x g , x r )(4)\nIn text generation, the generator G \u03b8 obtains the reward only when one sample has been completely generated, which means no intermediate reward is gained. To relieve this problem, following the practice in SeqGAN (Yu et al., 2017), we utilize the Monte Carlo rollout method to approximate intermediate rewards by sampling unknown tokens with generated prefix Y 1:t following generator policy G \u03b8 till sample completion. Empirically, we found that the Monte Carlo rollout also helps to reduce the variance of the reference sample utilized for comparison. We calculate the expected reward as\nR \u03b8,\u03c6 (s = Y 1:t\u22121 , a = y t ) = E (xg,xr)\u223c(G \u03b8 (Y1:t\u22121),M) [\u03b3 \u03c6 (x g , x r )](5)\nThe objective of the generator is to generate a sequence to maximize its expected final reward. With likelihood ratio (Sutton et al., 2000), we can formulate the gradient of the objective function for generator G \u03b8 as:\n\u2207 \u03b8 J(\u03b8) = T t=1 E Y1:t\u22121\u223cG \u03b8 [\u2207 \u03b8 log(G \u03b8 (y t |Y 1:t )) \u00b7 R \u03b8,\u03c6 (s = Y 1:t\u22121 , a = y t )](6)\nTo improve the training of self-adversarial learning, we borrow ideas from the field of deep reinforcement learning and propose two training techniques to improve self-adversarial training.\n\nScheduled rewarding Similar to the exploitation-exploration trade-off in reinforcement learning (Langford & Zhang, 2007), the positive reward assigned for actions generating better samples encourage exploration while the penalty for generating worse samples makes the generator more conservative. Intuitively, in the earlier stage of self-adversarial learning, the generator should explore better policy by receiving higher rewards for relative progress; while in the later stage, the generator should be more conservative by penalizing more for worse samples to avoid performance degradation. We simply decrease w (>) and increase w (<) linearly with training iteration and refer this technique as scheduled rewarding.\n\n\nAlgorithm 1 Self-Adversarial Learning With Comparative Discriminator\n\nRequire: Generator G \u03b8 ; comparative discriminator D \u03c6 ; samples of real sentences S+; self-adversarial learning step g; discriminator step k; memory buffer M for the previous generated samples 1: Pretrain G \u03b8 using MLE on S+ 2: Generate samples with G \u03b8 and store them into M 3: repeat 4: for k steps do 5:\n\nCollect a mini-batch of balanced sample pairs (x1,x2) from M \u222aS+ 6:\n\nUpdate D \u03c6 via Eq (2) 7: end for 8: for g steps do 9:\n\nGenerate a mini-batch of samples xg \u223c G \u03b8 10:\n\nCollect a mini-batch of reference samples xr from M 11:\n\nUpdate G \u03b8 via Eq (6) 12:\n\nend for 13: Update M with G \u03b8 14: until Convergence Memory replay Continuously comparing the generator with its most recent stage may suffer from the correlation between generated samples and reference samples, which makes the training process unstable. Inspired by experience replay (Lillicrap et al., 2015), we construct a memory buffer which contains samples generated in the last K training steps. Reference samples are sampled from the memory buffer rather than samples generated in the most recent stage of the generator, which empirically helps stabilize the training process.\n\nThe training process of SAL is summarized in Algorithm 3. Self-adversarial learning with the proposed comparative discriminator achieves Nash Equilibrium when the generator models the distribution of real samples perfectly. In this case, the comparative discriminator cannot successfully distinguish generated samples from real samples and tends to recognize two samples as \"indistinguishable\". The reward received by the generator is thus zero and training converges. However, how a non-Bernoulli GAN converges to such an equilibrium is still an open problem  and is beyond the scope of this work.\n\n\nEXPERIMENTS\n\n\nEXPERIMENTAL SETTING\n\nFollowing the experimental settings in previous work (Lin et al., 2017;Shi et al., 2018;Zhu et al., 2018;Nie et al., 2018), we evaluate our approach in both synthetic and real datasets based on Texygen (Zhu et al., 2018), which is a benchmark platform for evaluating adversarial text generation models. Table 1 presents the basic information of the datasets used for evaluation. As SeqGAN (Yu et al., 2017), our generator is a single-layer LSTM (Hochreiter & Schmidhuber, 1997) and our discriminator is almost based on TextCNN (Kim, 2014) except that it concatenates the feature representation of two compared samples and outputs the probability for their comparative relations (i.e., >, <, \u2248). We keep the most of the hyperparameters same with the SeqGAN except the hyperparameters introduced by our models (i.e., w (>) , w (<) , w (\u2248) ) which are tuned based on the synthetic experiment and kept the same for the real data experiments.\n\nWe evaluate the adversarial text generation models in terms of both quality and diversity. Following the prior work, in the synthetic dataset, we use the oracle LSTM to evaluate the negative log-likelihood of our generated samples (denoted as NLL oracle ) as the metric to reflect quality; for the diversity metric, we use the negative log-likelihood of the synthetic dataset (denoted as NLL gen ) evaluated by the generator with the best quality (i.e., the best NLL oracle score) during training. We also use the best NLL oracle + NLL gen obtained during training to evaluate the quality-diversity trade-off.\n\nFor the real data experiments, we follow the previous work to apply the commonly-used BLEU scores (Papineni et al., 2002) (BLEU(F)) and the perplexity of generated samples evaluated by an open-sourced pretrained language model (Jozefowicz et al., 2016) as quality metrics since NLL oracle cannot be evaluated without an oracle language model. For evaluating diversity, we employ both backward BLEU (Shi et al., 2018) (BLEU(B)) which evaluates the test data using generated samples as reference and NLL gen as the metrics. To evaluate the generated samples in more aspects, we calculate frechet distance (Heusel et al., 2017) (FD) between generated samples and real data with sentence representation obtained by InferSent (Conneau et al., 2017) which is a pre-trained sentence embedding model.\n\nWe compare our approach to previous well-known adversarial text generation models including SeqGAN (Yu et al., 2017), RankGAN  and MaliGAN (Che et al., 2017). Leak-GAN  and RelGAN (Nie et al., 2018) focus on architecture-level modification, which is orthogonal to our work and the proposed self-adversarial learning paradigm can be applied to them as well. We provide results of the combination of LeakGAN with SAL in the Appendix.\n\nIn the following sections, we denote our proposed self-adversarial learning approach as SAL. Since adversarial training is very sensitive to random initialization and suffers from high variance, we conduct five individual runs with different random seeds for each model and report the mean and the standard deviation of the obtained results. Table 2 shows the results in the synthetic dataset. We can observe that SAL largely outperforms the previous GANs in all metrics in both cases of sequence length 20 and 40. Although its performance in NLL gen is worse than MLE as MLE directly optimizes the metric, it yields better quality-diversity trade-off than MLE training, which has not been achieved by the previous GANs, which is shown by the fact that the NLL oracle +NLL gen for SAL is lower than that yielded by MLE, while other GANs have the same sum score with MLE, indicating that they fail to improve the quality-diversity trade-off after pretraining. This demonstrates SAL's advantage in alleviating the mode collapse problem. We also find that the training of SAL is more stable compared with other text GANs.\n\n\nEXPERIMENTAL RESULTS\n\n\nRESULTS IN SYNTHETIC DATA\n\nIn addition, we find SAL learns faster and better than the other GANs by comparing their performance curves of NLL oracle during training in Figure 3, which is consistent with our intuition that the selfimprovement reward mechanism in SAL can alleviate the reward sparsity issue in the earlier training stage and help the generator learn better.\n\n\nRESULTS IN REAL DATA\n\nThe results for COCO image caption dataset are presented in Table 3 and the performance curve of the perplexity during training is shown in Figure 4. As in the synthetic data, we observe that our SAL consistently yields better results in all the metrics with stable performance (i.e., low variance) compared to the previous GANs. According to Table 18 Table 4: all the previous GANs fail to improve MLE. This is because long text generation makes the discrepancy between generated samples and real samples very large even after MLE pre-training. As a result, previous GANs fail to stably enhance the sample quality due to the reward sparsity issue. In contrast, our SAL consistently performs well and largely improves the quality metrics over MLE. In addition, we observe that the diversity of samples generated by SAL is much better than previous GANs and only marginally worse than MLE, indicating SAL is helpful in reducing the risk of mode collapse.\n\nIn addition to the automatic metrics, we also conduct human evaluation for the generated samples. As previous work (Nie et al., 2018), we randomly sample 20 sentences from each model and pool them  with anonymizing the models' identity. We invite 20 graduate students with good English proficiency to score each sentence on a scale of 1-5 in terms of quality. According to Table 5, our SAL is consistently well rated in the human evaluation and outperforms all the baselines in both COCO and WMT NEWS datasets. We perform the Wilcoxon Rank Sum Test with the human evaluation results and find that samples generated by baseline models can be distinguished from samples generated by SAL with p < 0.01. Details of human evaluation procedure and samples generated by compared methods in two real-world datasets are presented in the Appendix.\n\n\nDISCUSSION\n\nTo better understand SAL, we perform multiple ablation tests in both the synthetic and the real data. We employ NLL oracle + NLL gen score with sequence length 20 as the evaluation metric for the synthetic data, denoted as NLL. For the real data, we use the perplexity of generated samples trained with COCO dataset as the evaluation metric. We compare SAL with the following reduced models:\n\n\u2022 CAL: Replacing the comparison between the generated samples (i.e., self-play) to the comparison between the real and generated samples. \u2022 w/o comparative: Using the binary discrimination scores of other generated samples as baseline for the policy gradient algorithm, which can be considered as a combination of the self-critic training (Rennie et al., 2017) with RL-based text GANs. \u2022 w/o \"\u2248\": Replace the three-class comparative discriminator with a binary comparative discriminator by removing the \"\u2248\" class.\n\n\n\u2022 w/o scheduled rewarding and w/o memory replay\n\nThe results of the ablation tests are shown in Table 6. By observing the improvement by SAL over CAL, we confirm the importance of the self-play paradigm in SAL. It is notable that the proposed comparative discriminator alone (i.e., CAL) can yield good performance, demonstrating the effectiveness of learning by comparison. When replacing the comparative discriminator with the naive combination of self-critic baseline with text GANs, the performance largely decreases because the reward sparsity issue will be intensified when subtracting two already sparse rewards, this motivates the proposed pairwise comparative discriminator which makes self-comparison possible.\n\nIn addition, we find that the \"\u2248\" option plays a critical role in improving the result, without which the performance degrades significantly because it makes the task less trivial and provides a baseline for the policy gradient algorithm. Moreover, the training techniques (i.e., scheduled rewarding and memory replay) borrowed from deep reinforcement learning are also shown useful in improving the results but not so important as the core components (e.g., self-play and the comparative discriminator).\n\n\nRELATED WORK\n\nMany variants of GANs (including TextGAN , GSGAN (Kusner & Hern\u00e1ndez-Lobato, 2016), SeqGAN (Yu et al., 2017), MaliGAN (Che et al., 2017), RankGAN (Lin et al., 2017), FMGAN , LeakGAN , and RelGAN (Nie et al., 2018)) have been proposed for text generation as adversarial training has received increasing attention in recent years. Typically, they address the non-differentiable issue by making continuous approximation or reinforcement learning. These approaches introduce several different architectures and optimization objectives of both the generator and the discriminator for adversarial text generation. Among the previous studies for adversarial text generation, the most related work to ours is RankGAN (Lin et al., 2017) which proposes a ranker to replace the conventional binary classifier as its discriminator for allowing the discrimination process to involve richer information. Another work whose idea is similar to ours is the relativistic discriminator (Jolicoeur-Martineau, 2018) (RGAN). It compares binary scores assigned to generated samples and real samples by subtraction as the learning signal to implicitly represent the inductive bias that half of the samples received by the discriminator is fake. In contrast, our comparative discriminator directly encodes this inductive bias and assesses generated sentences by comparison with a pairwise classifier, which provides more informative learning signals than subtraction in RGAN (Jolicoeur-Martineau, 2018) and normalized feature similarity in RankGAN (Lin et al., 2017). Our work is also related to the concurrent work (Zhou & Xu, 2020) that learns a comparative evaluator to evaluate open-domain natural language generation models.\n\n\nCONCLUSION AND FUTURE WORK\n\nWe present a self-adversarial learning (SAL) paradigm for adversarial text generation. SAL rewards the generator when its comparative discriminator finds the generator becomes better than before. Through the self-improvement reward mechanism, the problem of reward sparsity and mode collapse can be alleviated and training of text GANs are more stable, which results in a better performance in the text generation benchmarks in terms of both quality, diversity, and lower variance. In the future, we plan to generalize our approach to other domains and modals to explore the potential of SAL for adversarial learning. Generated samples are presented in the Appendix together with other details, including human evaluation details and qualitative analysis of the proposed SAL.\n\n\nA GENERATED SAMPLES\n\nWe present sentences generated by our proposed model and compared models to provide qualitative evaluation of different adversarial text generation models. From the presented generated samples, we can observe that samples generated by MLE training are less realistic compared with other samples. SeqGAN yield slightly better sample quality but the loss of diversity is observable even within randomly sampled 15 sentences. Adversarial training with proposed comparator, when trained by comparing with real samples, yield better quality but still lack of diversity. Finally, with the proposed self-adversarial learning paradigm, both quality and diversity of generated samples are improved. Table 7: Samples generated by SAL in Image COCO dataset a picture of a person 's umbrella in a cell phone . a man stands in a green field . a young boy riding a truck . a man on a motorcycle is flying on a grassy field . a girl on a motorcycle parked on a city street . a motorcycle parked in a city street . a group of bikers riding bikes on a city street . a kitchen with a cat on the hood and a street . a bathroom containing a toilet and a sink . a young woman in a kitchen with a smiley face . a jet plane on the side of a street . a dish is sitting on a sidewalk next to a baby giraffe . a dog on a large green bike parked outside of the motor bike . a person on a kawasaki bike on a race track . a commercial aircraft is parked in front of a kitchen . Table 8: Samples generated by CAL in Image COCO dataset a man is on a towel on a table outside of a real kitchen . a group of lambs at a tall building . a young boy riding a truck . a man on a motorcycle is flying on a grassy field . a man with a computer desk next to a white car . a cat is on the walls of a cat . a plane on a runway with a plane . an elegant , dilapidated plane are standing in front of a parking bag . the woman is riding a bike on their way . a man wearing an old bathroom with a banana . a plane is taking off from the ground . a man holding a man in front of herself . a woman is walking across the road . a kitchen with an island in green tiles . a clean kitchen with two small appliances . Table 9: Samples generated by SeqGAN in Image COCO dataset a large image of a herd of racing train . man and woman on horse . a plane on a runway with a plane . a man preparing a table with wood lid . a view , tiled floors and a man prepares food . a man wearing an old bathroom with a banana . a man is is with a camera . two people are parked on a street . a white and white black kitten eating on a table . a toilet is lit on the walls . a kitchen is taking off from a window . a man is wearing glasses wearing scarf . a kitchen with graffiti hanging off from an open plain . two women playing with the orange . a kitchen with an island in a clear glass . Table 10: Samples generated by MLE in Image COCO dataset a jet airplane flies flying through front from an airplane . a furry tub and overhead pot . a man in a kitchen filled with dark lights green side , .. a cross baby field dressed making cardboard a bathroom with a small tub and oven . a man above a bathroom with an oven room . a jet airliner flying through the sky . a kitchen with a dishwasher , and plenty of pots , pans . a person holding onto two red era arena sits on the street . a bathroom with a toilet and a bath tub . a cat perched on the phone and a baseball cap . the view of the street filled with really parked at the gates on the road . a large hairy dog on a high bike with a cake . a man is riding a white back bench . a narrow bed and white spotted dark tiled walls . Table 11: Samples generated by SAL in EMNLP2017 WMT dataset (1) it ' s likely to be egyptian and many of the canadian refugees , but for a decade .\n\n\nA.1 GENERATED SAMPLES IN IMAGE COCO DATASET\n\n\nA.2 GENERATED SAMPLES IN EMNLP2017 WMT DATASET\n\n(2) the ministry spokesperson also said it now significant connected to the mountain.\n\n(3) it is the time they can more competitive , where we have another $ 99 . 100 per cent , and completely on the alternative , and that ' s being affected .\n\n(4) we expect $ 200 and 0 . 3 percent for all you form other , and , which then well , it ' s done .\n\n(5) so we wouldn ' t feel very large in the game , but you fail to fund , and and the paper that ' s like its start . (6) other countries made a playoff cut with pages by mrs . trump ' s eighth consecutive season as a president . Table 12: Samples generated by CAL in EMNLP2017 WMT dataset (1) i didn ' t put relatively quiet , we have , ' his work right in the particular heat rate , take steps traditionally clean .\n\n(2) why the u . s . then the table is our cabinet to do getting an vital company for the correct review .\n\n(3) those had trained for that , but no thin percentage of the nhs about being warned about the palestinian election before obama is not connected in israel .\n\n(4) in course , voters -obama said : \" torture is the outcome , the most powerful tradepopularity is happening in it as a success . (5) \" in 2012 , it is nice to remain -no trump actor established this night -scoring three films . (6) we kind of not listen to knowing my most one , only , for a really good vote , and where things fun , you know . (2) but just things , you want to thank it as my playing side has begun meeting with \" and \" the score had to train up , so he was tied for 11 years .\n\n(3) and when he got back doing fresh ties with his election , he will now step in january , back.\n\n(4) when you ' t know if i saw her task to find himself more responsibility ago .\n\n(5) his hold over -up to a nine hike in 2015 , 13 percent of recently under suspects dead day , 24 , and to the city . (6) \" i look up on by the city ' s vehicle on the day in a meeting in november . Table 14: Samples generated by MLE in EMNLP2017 WMT dataset (1) you know that that is great for our ability to make thinking about how you know and you ?\n\n(2) when it ' s a real thing possible , is if you the first time in a time here and get .\n\n(3) u . s , now government spending at the second half of four years , a country where the law will join the region to leave japan in germany .\n\n(4) deputy president , the issue of government and geneva probe threats and not -backed trump , but well -changing violence for their islamic state militants were innocent people . (5) he suggested in a presidential primary source and comment on its size following protests conducted by 18 , some in 2012 will be looked at tech energy hub . (6) \" it ' s growing heavy hard , \" mr . romney said , he says matters that can ' t again become the asian player .\n\n\nB CASE STUDY: WHY IT WORKS\n\nIn this section, we present several qualitative case study examples to illustrate why comparative discrimination and self-adversarial learning helps mitigate the problem of reward sparsity and mode collapse. We extract a typical sentence generated during the initial stage of adversarial learning: \"a man holding a suitcase holding a phones.\" We see that this sentence is of limited quality and is easily recognized by binary discriminator in SeqGAN with high confidence, this makes the credit received by the generator very sparse and makes training difficult. Comparative adversarial learning (CAL) where we use the comparative discriminator to assess the quality of this sample by comparing it with a real sample helps as comparative discrimination have three catagories, which is less trivial. The improvement is not so much as the discrepancy of generated samples and real samples is fairly large. However, with proposed self-adversarial learning paradigm, the comparative discriminator assesses this sentence by comparing it with a previous generated sentence which is also of poor quality. The self-improvement is easier to be recognized by the comparative discriminator and makes this sample get good rewards.\n\nAs the comparative discriminator has to learn a total order of sample quality which is more challenging than standard binary discrimination, the chance of the comparative discriminator to be over-trained is reduced, which makes the model easier to achieve self-improvement, thus help to alleviate the reward sparsity problem.\n\nWe also extract a sentence generated in the late stages which is fairly good and fools the binary discriminator: \"a woman sitting on a bench on a park.\" In standard adversarial text generation models, a sentence like this would keep receiving large rewards and result in mode collapse. In self-adversarial learning paradigm, this sentence is not much better than other sentences generated by the generator itself, so its reward is limited, which reduces the risk of mode collapse. For the ablated model variants, SAL w/o self-play and w/o comparative discriminator is trained with the following algorithms. Specifically, the difference between SAL and CAL is that the reference sample which is compared with the currently generated sample is replaced by a real sample instead of a previously generated one. For the variant without the comparative discriminator, we employ a binary discriminator trained in the same way with the vanilla GAN, as for the reward of generating x g , we first sample a previously generated sample x r as reference and calculate the reward as\nD(x g ) \u2212 D(x r ).\nthat the self-BLEU (2-5) scores are always 1 when evaluating all the models. This problem is also discussed in the openreview of the RelGAN paper Nie et al. (2018).\n\nBased on this consideration, we decide to employ backward-BLEU which is introduced in Shi et al. (2018) and can also evaluate the diversity of generated samples. The forward and backward BLEU resemble precision and recall of generated samples with respect to the test data, which measures the generation quality and the generation diversity respectively.\n\nFor BLEU metric, as there is no sentence level alignment for unconditional generation, BLEU is evaluated by using the entire test set treated as a single reference, which contains 10000 sentences. We then generate the same number of sentences as the prediction, and then compute n-gram overlap between the reference and the prediction. We did not apply brevity penalty following previous works. But we found the number of tokens generated are roughly the same across different compared models.\n\nWe briefly explain why NLL gen is able to measure the diversity of the generator: NLL gen measures the negative log-likelihood of the synthetic dataset evaluated by the generator. Intuitively, if the generator is diverse and captures more patterns in the synthetic dataset, the NLL gen score will be lower. In contrast, if the generator suffers from severe mode collapse and is of low diversity, the NLL gen will be higher as the generator fails to cover the diverse patterns in the training data.\n\nAs for the metric: NLL gen +NLL oracle , our motivation is that NLL oracle measures the best quality of the generator throughout training, while NLL gen measures the best diversity attained by the generator during training. However, the best quality and diversity are generally not achieved at the same time as GAN-training generally sacrifices the diversity for better quality. Therefore, we report NLL gen +NLL oracle which can measure the quality-diversity trade-off, as the previous work demonstrated, as an additional reference.\n\n\nC.3 HYPERPARAMETERS\n\nWe follow most of the hyperparameters used in the benchmark platform Texygen Zhu et al. (2018). Specifically, we choose batch size to be 64, dropout keep prob to be 0.75, l2 regularization to be 0.2. We pretrain all model for 120 epochs and fine-tune them until convergence.\n\nThe proposed self-adversarial learning paradigm introduces the relative weights for credit assignment when a generated sample is found to be better, indistinguishable or worse compared with another sample generated by the generator itself. We tuned it based on the performance in synthetic experiment and find w 0 : w 2 = 1 : \u22120.1 to be a good choice for the initial weights 1 and fixed w 1 to be 0. We empirically find that the performance of SAL is not very sensitive to the choice of reward weights as long as the absolute value of w 1 is larger enough than w 2 , which guarantees the training stability.\n\n\nC.4 HUMAN EVALUATION\n\nFollowing the human evaluation setting in RelGAN Nie et al. (2018), The text quality evaluation is based on grammatical correctness and meaningfulness (i.e. if a sentence makes sense), text formatting problems (e.g., capitalization, punctuation, spelling errors, extra spaces between words and punctuations) are ignored. Detailed criteria is provided as follows: \n\n\n5-Excellent\n\nIts grammatically correct and makes sense. For example, \"a man standing on a skateboard in the middle of the street .\" 4-Good\n\nIt has some small grammatical errors and mostly makes sense. For example, \"two women is in a cafe look outside.\" 3-Fair It has major grammatical errors but the whole still conveys some meanings. For example, \"a man riding on on motor scooter and window.\" 2-Poor It has severe grammatical errors and the whole doesn't make sense, but some parts are meaningful. For example, \"a blue bike on on on a dirt bike .\" 1-Unacceptable It is basically a random collection of words.\n\nFor example, \"a motorcycle close close on it and .\"\n\n\nC.5 ADDITIONAL RESULTS ON CAL AND LEAKGAN\n\nIn this section, we present the performance comparison on SAL vs CAL (comparative adversarial learning, which uses comparative discriminator but does not train the model with self-play.) and the application of SAL on LeakGAN. We find that SAL significantly outperforms CAL on all dataset. In addition, we find that the proposed self-adversarial learning paradigm can also be applied on other text GAN architectures, e.g. LeakGAN, and help improve its performance.  \n\nFigure 2 :\n2The training process of the comparative discriminator.\n\nand Figure 4 ,Figure 3 :Figure 4 :\n434SeqGAN and our SAL can improve MLE in the quality metrics (i.e., BLEU (F) and Perplexity) while MaliGAN and RankGAN The performance curves of NLL-oracle during training in the synthetic dataset The performance curves of perplexity during training in the Image COCO dataset\n\nTable 1 :\n1Description of the datasets used for evaluationSynthetic \nImage COCO \nEMNLP2017 WMT NEWS \n\nCategory \nsimulated data image description news article \nVocabulary size \n5000 \n4682 \n5255 \nSequence length \n20/40 \n<37 \n<51 \nSentence number (training) 10000 \n10000 \n270000 \nSentence number (test) \n10000 \n10000 \n10000 \n\n\n\nTable 2 :\n2Performance comparison of different models in synthetic tests where sequence length is set to 20 and 40 respectively. For all the metrics presented, the lower, the better. \u00b10.31 / 9.52\u00b10.11 7.14\u00b10.34 / 7.05\u00b10.12 15.01\u00b10.02 / 16.37\u00b10.02 MaliGAN 8.74 \u00b10.16 / 9.67 \u00b10.03 6.62\u00b10.25 / 7.14\u00b10.09Method \nNLL oracle (20/40) \nNLL gen (20/40) \nNLL oracle + NLL gen (20/40) \n\nSAL \n7.71 \u00b10.17 / 9.31\u00b10.03 6.58\u00b10.15 / 6.97 \u00b10.05 \n14.29 \u00b10.11 / 16.24\u00b10.03 \nSeqGAN \n8.63 \u00b10.19 / 9.63 \u00b10.04 6.61\u00b10.22 / 6.98\u00b10.08 \n15.00\u00b10.03 / 16.35\u00b10.02 \nRankGAN 8.42 15.03\u00b10.03 / 16.39\u00b10.03 \nMLE \n9.05 \u00b10.03 / 9.84\u00b10.02 5.96\u00b10.02 / 6.55\u00b10.02 \n15.02\u00b10.03 / 16.39\u00b10.01 \n\n\n\nTable 3 :\n3Performance comparison of different models in the COCO caption generation task. Metrics from top to bottom represent respectively the generation quality, the generation diversity, and the divergence between real data of generated sentences. For all the BLEU metrics, the higher, the better; for NLL gen and FD, the lower, the better.Metrics \nMLE \nSeqGAN \nMaliGAN \nRankGAN \nSAL \n\nBLEU-2(F) \n0.730 \u00b10.01 \n0.748 \u00b10.03 \n0.733 \u00b10.03 \n0.727 \u00b10.04 \n0.785 \u00b10.02 \nBLEU-3(F) \n0.494 \u00b10.01 \n0.514 \u00b10.04 \n0.497 \u00b10.04 \n0.491 \u00b10.04 \n0.581 \u00b10.03 \nBLEU-4(F) \n0.303 \u00b10.01 \n0.307 \u00b10.02 \n0.295 \u00b10.03 \n0.291 \u00b10.03 \n0.362 \u00b10.02 \nBLEU-5(F) \n0.187 \u00b10.01 \n0.187 \u00b10.02 \n0.178 \u00b10.02 \n0.175 \u00b10.03 \n0.227 \u00b10.02 \nPerplexity \n338.4 \u00b17.6 \n307.2 \u00b114.9 \n343.8 \u00b121.3 \n391.2 \u00b135.1 \n231.3 \u00b110.8 \n\nBLEU-2(B) \n0.759 \u00b10.02 \n0.694 \u00b10.03 \n0.676\u00b10.04 \n0.683\u00b10.04 \n0.724 \u00b10.02 \nBLEU-3(B) \n0.531 \u00b10.02 \n0.472\u00b10.03 \n0.443\u00b10.03 \n0.449\u00b10.04 \n0.503 \u00b10.03 \nBLEU-4(B) \n0.332 \u00b10.02 \n0.285\u00b10.02 \n0.279\u00b10.02 \n0.277\u00b10.02 \n0.313\u00b10.02 \nBLEU-5(B) \n0.209 \u00b10.02 \n0.186\u00b10.02 \n0.178 \u00b10.02 \n0.182 \u00b10.02 \n0.198 \u00b10.02 \nNLLgen \n0.721 \u00b10.02 \n1.035 \u00b10.02 \n1.052 \u00b10.02 \n1.145 \u00b10.02 \n0.873 \u00b10.02 \n\nFD \n0.043 \u00b10.009 0.065 \u00b10.018 0.076 \u00b10.021 0.083 \u00b10.027 0.051 \u00b10.014 \n\n\n\nTable 4 :\n4Performance comparison of different models in the EMNLP2017 WMT news generation task. Metrics from top to bottom represent respectively the generation quality, the generation diversity, and the divergence between real and generated data. For all the BLEU metrics, the higher, the better. For NLL gen and FD, the lower, the better.Metrics \nMLE \nSeqGAN \nMaliGAN \nRankGAN \nSAL \n\nBLEU-2(F) \n0.769 \u00b10.02 \n0.761 \u00b10.03 \n0.764 \u00b10.03 \n0.736 \u00b10.02 \n0.788 \u00b10.02 \nBLEU-3(F) \n0.475 \u00b10.01 \n0.463 \u00b10.03 \n0.468 \u00b10.03 \n0.441 \u00b10.04 \n0.523 \u00b10.02 \nBLEU-4(F) \n0.243 \u00b10.02 \n0.228 \u00b10.03 \n0.231 \u00b10.03 \n0.204 \u00b10.02 \n0.281 \u00b10.02 \nBLEU-5(F) \n0.124 \u00b10.02 \n0.115 \u00b10.02 \n0.113 \u00b10.03 \n0.095 \u00b10.02 \n0.149 \u00b10.02 \nPerplexity \n702.6 \u00b118.6 \n743.8 \u00b134.2 \n825.1 \u00b144.3 \n975.4 \u00b165.1 \n612.8 \u00b122.5 \n\nBLEU-2(B) \n0.741 \u00b10.02 \n0.693 \u00b10.03 \n0.684\u00b10.03 \n0.671\u00b10.03 \n0.726\u00b10.02 \nBLEU-3(B) \n0.476\u00b10.01 \n0.413\u00b10.03 \n0.391\u00b10.04 \n0.373\u00b10.05 \n0.431\u00b10.03 \nBLEU-4(B) \n0.245\u00b10.01 \n0.216\u00b10.03 \n0.197\u00b10.03 \n0.191\u00b10.03 \n0.232\u00b10.02 \nBLEU-5(B) \n0.129\u00b10.01 \n0.112 \u00b10.02 \n0.094\u00b10.02 \n0.096\u00b10.03 \n0.123\u00b10.02 \nNLLgen \n2.386\u00b10.01 \n2.732\u00b10.04 \n2.862\u00b10.06 \n3.157\u00b10.11 \n2.578\u00b10.04 \n\nFD \n0.079 \u00b10.012 0.172 \u00b10.032 0.194 \u00b10.043 0.219 \u00b10.052 0.137 \u00b10.023 \n\nperform comparably to MLE. However, in the WMT NEWS dataset where text samples tend to be \nlonger, we observe something different from \n\nTable 5 :\n5Human evaluation results of different models in both datasets. Scores are between 1-5, higher score indicates better quality.Dataset \nMLE \nSeqGAN \nMaliGAN \nRankGAN \nSAL \n\nCOCO \n2.96\u00b10.51 3.26\u00b10.56 3.14\u00b10.57 2.91\u00b10.62 3.84\u00b10.56 (p<=0.01) \nWMT NEWS 2.35\u00b10.86 2.19\u00b10.88 2.24\u00b10.87 2.05\u00b10.91 2.65\u00b10.89 (p<=0.01) \n\n\n\nTable 6 :\n6Results of the ablation tests in the synthetic data and the COCO dataset.Dataset \nSAL \nCAL \nw/o comparative \nw/o \"\u2248\" \nw/o scheduled rewarding w/o memory replay \n\nSynthetic (NLL) \n14.29 \u00b10.11 14.65 \u00b10.16 \n15.01 \u00b10.04 \n14.85 \u00b10.16 \n14.46\u00b10.18 \n14.41 \u00b10.17 \nCOCO (Perplexity) 231.3\u00b110.8 \n276.7\u00b112.5 \n341.8 \u00b113.4 \n291.5 \u00b116.3 \n248.3 \u00b114.2 \n254.6 \u00b113.7 \n\n\n\nTable 13 :\n13Samples generated by SeqGAN in EMNLP2017 WMT dataset (1) his missed 4 , 000 the first 95 really 69 -year -olds .\n\nTable 15 :\n15Case study of comparative discrimination and self-adversarial learning. man holding a suitcase holding a phones. a student walks in the rain with a green umbrella. (Real) 0.051 a man holding a suitcase holding a phones. a men 's kitchen and a cow. woman sitting on a bench on a park. a young man rides his bicycle on top of a cement bench. (Real) 0.438 a woman sitting on a bench on a park. a man sitting on a table watching a television.Generated sentence \nReference sentence \nReward \n\na man holding a suitcase holding a phones. -(SeqGAN) \n0.018 \na (Self) \n0.561 \n\na woman sitting on a bench on a park. \n-(SeqGAN) \n0.825 \na (Self) \n0.086 \n\nB.1 ABLATED MODELS \n\n\n\nTable 16 :\n16The human evaluation scale from 1 to 5 with corresponding criteria and example sentences. ScaleCriterion & Example\n\nTable 17 :\n17Performance comparison of different models in synthetic tests where sequence length is set to 20 and 40 respectively. For all metrics presented, lower value is better.\n\nTable 18 :\n18Performance comparison of different models in the COCO caption generation task. Metrics from top to bottom represent respectively the generation quality, the generation diversity, and the divergence between real data of generated sentences. For all BLEU metrics, higher value is better, for NLL gen and FD, lower is better.Metrics \nCAL \nSAL \nLeakGAN \nLeakGAN + SAL \n\nBLEU-2(F) \n0.767 \u00b10.03 \n0.785 \u00b10.02 \n0.749 \u00b10.03 \n0.798 \u00b10.03 \nBLEU-3(F) \n0.541 \u00b10.03 \n0.581 \u00b10.03 \n0.532 \u00b10.04 \n0.592 \u00b10.03 \nBLEU-4(F) \n0.337 \u00b10.03 \n0.362 \u00b10.02 \n0.353 \u00b10.03 \n0.369 \u00b10.02 \nBLEU-5(F) \n0.211 \u00b10.02 \n0.227 \u00b10.02 \n0.233 \u00b10.03 \n0.241 \u00b10.02 \nPerplexity \n276.7 \u00b112.5 \n231.3 \u00b110.8 \n291.4 \u00b112.5 \n218.2 \u00b110.8 \n\nBLEU-2(B) \n0.705 \u00b10.03 \n0.724 \u00b10.02 \n0.733 \u00b10.03 \n0.741 \u00b10.02 \nBLEU-3(B) \n0.479 \u00b10.03 \n0.503 \u00b10.03 \n0.512 \u00b10.03 \n0.529 \u00b10.03 \nBLEU-4(B) \n0.296\u00b10.03 \n0.313\u00b10.02 \n0.321 \u00b10.03 \n0.334\u00b10.02 \nBLEU-5(B) \n0.191 \u00b10.03 \n0.198 \u00b10.02 \n0.206 \u00b10.03 \n0.211 \u00b10.02 \nNLLgen \n0.936 \u00b10.03 \n0.873 \u00b10.02 \n0.683 \u00b10.03 \n0.655 \u00b10.02 \n\nFD \n0.058 \u00b10.016 0.051 \u00b10.014 0.048 \u00b10.016 \n0.044 \u00b10.014 \n\nw1 is linearly decreased to 0.8 and w2 is increased to -0.2\nC TRAINING AND EVALUATION DETAILSC.1 MODEL DETAILSWe follow most of the hyperparameters used in the benchmark platform TexygenZhu et al. (2018). Specifically, the generator is a one layer LSTM with embedding size and hidden size 32. The discriminator is implemented as a TextCNN with a filter size of [2,3] and a filter number of[100,200]. The proposed self-adversarial learning paradigm introduces the relative weights for credit assignment when a generated sample is found to be better, indistinguishable or worse compared with another sample generated by the generator itself. We tuned it based on the performance in synthetic experiment and set w 0 : w 2 = 1 : \u22120.1.C.2 CHOICE & EXPLANATION OF METRICSNote that many previous works use self-BLEUZhu et al. (2018)as a diversity metric. However, we find that there exist problem in the official implementation of the self-BLEU metric: Only in the first time of evaluation that the reference and hypothesis come from the same \"test data\" (i.e. the set of generated sentences). After that, the hypothesis keeps updated but the reference remains unchanged (due to \"is-first=False\"), which means hypothesis and reference are not from the same \"test data\" any more, and thus the scores obtained under this implementation are not self-BLEU scores. To this end, we modified the implementation to make sure that the hypothesis and reference are always from the same \"test data\" (by simply removing the variables \"self.reference\" and \"self.is-first\") and found\nScheduled sampling for sequence prediction with recurrent neural networks. Samy Bengio, Oriol Vinyals, Navdeep Jaitly, Noam Shazeer, Advances in Neural Information Processing Systems. Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence prediction with recurrent neural networks. In Advances in Neural Information Processing Systems, pp. 1171-1179, 2015.\n\nYanran Tong Che, Ruixiang Li, Devon Zhang, Wenjie Hjelm, Li, arXiv:1702.07983Yangqiu Song, and Yoshua Bengio. Maximum-likelihood augmented discrete generative adversarial networks. arXiv preprintTong Che, Yanran Li, Ruixiang Zhang, R Devon Hjelm, Wenjie Li, Yangqiu Song, and Yoshua Bengio. Maximum-likelihood augmented discrete generative adversarial networks. arXiv preprint arXiv:1702.07983, 2017.\n\nAdversarial text generation via feature-mover's distance. Liqun Chen, Shuyang Dai, Chenyang Tao, Haichao Zhang, Zhe Gan, Dinghan Shen, Yizhe Zhang, Guoyin Wang, Ruiyi Zhang, Lawrence Carin, Advances in Neural Information Processing Systems. Liqun Chen, Shuyang Dai, Chenyang Tao, Haichao Zhang, Zhe Gan, Dinghan Shen, Yizhe Zhang, Guoyin Wang, Ruiyi Zhang, and Lawrence Carin. Adversarial text generation via feature-mover's distance. In Advances in Neural Information Processing Systems, pp. 4666-4677, 2018.\n\nSupervised learning of universal sentence representations from natural language inference data. Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, Antoine Bordes, arXiv:1705.02364arXiv preprintAlexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, and Antoine Bordes. Supervised learning of universal sentence representations from natural language inference data. arXiv preprint arXiv:1705.02364, 2017.\n\nGenerative adversarial nets. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, Advances in neural information processing systems. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural informa- tion processing systems, pp. 2672-2680, 2014.\n\nOn distinguishability criteria for estimating generative models. J Ian, Goodfellow, arXiv:1412.6515arXiv preprintIan J Goodfellow. On distinguishability criteria for estimating generative models. arXiv preprint arXiv:1412.6515, 2014.\n\nLong text generation via adversarial training with leaked information. Jiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong Yu, Jun Wang, Thirty-Second AAAI Conference on Artificial Intelligence. Jiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong Yu, and Jun Wang. Long text generation via adversarial training with leaked information. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.\n\nGans trained by a two time-scale update rule converge to a local nash equilibrium. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Sepp Hochreiter, Advances in Neural Information Processing Systems. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in Neural Information Processing Systems, pp. 6626-6637, 2017.\n\nLong short-term memory. Sepp Hochreiter, J\u00fcrgen Schmidhuber, Neural computation. 98Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735-1780, 1997.\n\nThe relativistic discriminator: a key element missing from standard gan. Alexia Jolicoeur-Martineau, arXiv:1807.00734arXiv preprintAlexia Jolicoeur-Martineau. The relativistic discriminator: a key element missing from standard gan. arXiv preprint arXiv:1807.00734, 2018.\n\nExploring the limits of language modeling. Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, Yonghui Wu, arXiv:1602.02410arXiv preprintRafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\n\nConvolutional neural networks for sentence classification. Yoon Kim, arXiv:1408.5882arXiv preprintYoon Kim. Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882, 2014.\n\nGans for sequences of discrete elements with the gumbel-softmax distribution. J Matt, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato Kusner, arXiv:1611.04051arXiv preprintMatt J Kusner and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato. Gans for sequences of discrete elements with the gumbel-softmax distribution. arXiv preprint arXiv:1611.04051, 2016.\n\nThe epoch-greedy algorithm for contextual multi-armed bandits. John Langford, Tong Zhang, Proceedings of the 20th International Conference on Neural Information Processing Systems. the 20th International Conference on Neural Information Processing SystemsCiteseerJohn Langford and Tong Zhang. The epoch-greedy algorithm for contextual multi-armed bandits. In Proceedings of the 20th International Conference on Neural Information Processing Systems, pp. 817-824. Citeseer, 2007.\n\nAdversarial learning for neural dialogue generation. Jiwei Li, Will Monroe, Tianlin Shi, S\u00e9bastien Jean, Alan Ritter, Dan Jurafsky, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingJiwei Li, Will Monroe, Tianlin Shi, S\u00e9bastien Jean, Alan Ritter, and Dan Jurafsky. Adversarial learning for neural dialogue generation. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 2157-2169, 2017.\n\nP Timothy, Jonathan J Lillicrap, Alexander Hunt, Nicolas Pritzel, Tom Heess, Yuval Erez, David Tassa, Daan Silver, Wierstra, arXiv:1509.02971Continuous control with deep reinforcement learning. arXiv preprintTimothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.\n\nAdversarial ranking for language generation. Kevin Lin, Dianqi Li, Xiaodong He, Zhengyou Zhang, Ming-Ting Sun, Advances in Neural Information Processing Systems. Kevin Lin, Dianqi Li, Xiaodong He, Zhengyou Zhang, and Ming-Ting Sun. Adversarial ranking for language generation. In Advances in Neural Information Processing Systems, pp. 3155-3165, 2017.\n\nRelgan: Relational generative adversarial networks for text generation. Weili Nie, Nina Narodytska, Ankit Patel, Weili Nie, Nina Narodytska, and Ankit Patel. Relgan: Relational generative adversarial networks for text generation. 2018.\n\nBleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting on association for computational linguistics. the 40th annual meeting on association for computational linguisticsAssociation for Computational LinguisticsKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pp. 311-318. Association for Computational Linguistics, 2002.\n\nSelf-critical sequence training for image captioning. J Steven, Etienne Rennie, Youssef Marcheret, Jerret Mroueh, Vaibhava Ross, Goel, 10.1109/CVPR.2017.131IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Steven J. Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. Self-critical sequence training for image captioning. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Jul 2017. doi: 10.1109/cvpr.2017.131. URL http://dx.doi.org/10. 1109/CVPR.2017.131.\n\nTowards diverse text generation with inverse reinforcement learning. Zhan Shi, Xinchi Chen, Xipeng Qiu, Xuanjing Huang, arXiv:1804.11258arXiv preprintZhan Shi, Xinchi Chen, Xipeng Qiu, and Xuanjing Huang. Towards diverse text generation with inverse reinforcement learning. arXiv preprint arXiv:1804.11258, 2018.\n\nMastering the game of go without human knowledge. David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Nature. 5507676354David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. Nature, 550(7676):354, 2017.\n\nPolicy gradient methods for reinforcement learning with function approximation. S Richard, David A Sutton, Mcallester, P Satinder, Yishay Singh, Mansour, Advances in neural information processing systems. Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient meth- ods for reinforcement learning with function approximation. In Advances in neural information processing systems, pp. 1057-1063, 2000.\n\nSeqgan: Sequence generative adversarial nets with policy gradient. Lantao Yu, Weinan Zhang, Jun Wang, Yong Yu, Thirty-First AAAI Conference on Artificial Intelligence. Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. Seqgan: Sequence generative adversarial nets with policy gradient. In Thirty-First AAAI Conference on Artificial Intelligence, 2017.\n\nAdversarial feature matching for text generation. Yizhe Zhang, Zhe Gan, Kai Fan, Zhi Chen, Ricardo Henao, Dinghan Shen, Lawrence Carin, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine Learning70Yizhe Zhang, Zhe Gan, Kai Fan, Zhi Chen, Ricardo Henao, Dinghan Shen, and Lawrence Carin. Ad- versarial feature matching for text generation. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 4006-4015. JMLR. org, 2017.\n\nLearning to compare for better training and evaluation of open domain text generation models. Wangchunshu Zhou, Ke Xu, Thirty-Fourth AAAI Conference on Artificial Intelligence. Wangchunshu Zhou and Ke Xu. Learning to compare for better training and evaluation of open domain text generation models. In Thirty-Fourth AAAI Conference on Artificial Intelligence, 2020.\n\nTexygen: A benchmarking platform for text generation models. Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang, Yong Yu, The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval. ACMYaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang, and Yong Yu. Texygen: A benchmarking platform for text generation models. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, pp. 1097-1100. ACM, 2018.\n\nRequire: Generator G \u03b8 ; comparative discriminator D \u03c6 ; samples of real sentences S+; self-adversarial learning step g; discriminator step k; memory buffer M for the previous generated samples 1: Pretrain G \u03b8 using MLE on S+ 2: Generate samples with G \u03b8 and store them into M 3: repeat 4: for k steps do 5: Collect a mini-batch of balanced sample pairs (x1,x2) from M \u222aS+ 6: Update D \u03c6 via Eq (2) 7: end for 8: for g steps do 9: Generate a mini-batch of samples xg \u223c G \u03b8 10: Collect a mini-batch of reference samples xr from S+ 11: Update G \u03b8 via Eq (6) 12: end for 13: Update M with G \u03b8 14: until Convergence Algorithm 3 Self-Adversarial Learning without comparative discriminator Require: Generator G \u03b8 ; binary discriminator D \u03c6 ; samples of real sentences S+; self-adversarial learning step g; discriminator step k; memory buffer M for the previous generated samples 1: Pretrain G \u03b8 using MLE on S+ 2: Generate samples with G \u03b8 and store them into M 3: repeat 4: for k steps do 5: Collect a mini-batch of generated samples from M. Generate a mini-batch of samples xg \u223c G \u03b8 10: Collect a mini-batch of reference samples xr from M 11: Update G \u03b8 via Eq. 6Algorithm 2 Self-Adversarial Learning without self-play (i.e. CALUpdate D \u03c6 with conventional GAN discriminator loss 7: end for 8: for g steps. end for 13: Update M by G \u03b8 with reward calculated by D(xg) \u2212 D(xr) 14: until Convergence Method NLL oracle (20) NLL oracle (40Algorithm 2 Self-Adversarial Learning without self-play (i.e. CAL) Require: Generator G \u03b8 ; comparative discriminator D \u03c6 ; samples of real sentences S+; self-adversarial learning step g; discriminator step k; memory buffer M for the previous generated samples 1: Pretrain G \u03b8 using MLE on S+ 2: Generate samples with G \u03b8 and store them into M 3: repeat 4: for k steps do 5: Collect a mini-batch of balanced sample pairs (x1,x2) from M \u222aS+ 6: Update D \u03c6 via Eq (2) 7: end for 8: for g steps do 9: Generate a mini-batch of samples xg \u223c G \u03b8 10: Collect a mini-batch of reference samples xr from S+ 11: Update G \u03b8 via Eq (6) 12: end for 13: Update M with G \u03b8 14: until Convergence Algorithm 3 Self-Adversarial Learning without comparative discriminator Require: Generator G \u03b8 ; binary discriminator D \u03c6 ; samples of real sentences S+; self-adversarial learning step g; discriminator step k; memory buffer M for the previous generated samples 1: Pretrain G \u03b8 using MLE on S+ 2: Generate samples with G \u03b8 and store them into M 3: repeat 4: for k steps do 5: Collect a mini-batch of generated samples from M. 6: Update D \u03c6 with conventional GAN discriminator loss 7: end for 8: for g steps do 9: Generate a mini-batch of samples xg \u223c G \u03b8 10: Collect a mini-batch of reference samples xr from M 11: Update G \u03b8 via Eq (6) 12: end for 13: Update M by G \u03b8 with reward calculated by D(xg) \u2212 D(xr) 14: until Convergence Method NLL oracle (20) NLL oracle (40)\n\n. + Leakgan, Sal, 6.69 \u00b10.29 6.91\u00b10.27LeakGAN + SAL 6.69 \u00b10.29 6.91\u00b10.27\n", "annotations": {"author": "[{\"end\":159,\"start\":81},{\"end\":205,\"start\":160},{\"end\":272,\"start\":206},{\"end\":320,\"start\":273},{\"end\":392,\"start\":321}]", "publisher": null, "author_last_name": "[{\"end\":97,\"start\":93},{\"end\":166,\"start\":164},{\"end\":211,\"start\":209},{\"end\":281,\"start\":278},{\"end\":330,\"start\":326}]", "author_first_name": "[{\"end\":92,\"start\":81},{\"end\":163,\"start\":160},{\"end\":208,\"start\":206},{\"end\":277,\"start\":273},{\"end\":325,\"start\":321}]", "author_affiliation": "[{\"end\":158,\"start\":127},{\"end\":204,\"start\":168},{\"end\":271,\"start\":240},{\"end\":319,\"start\":283},{\"end\":391,\"start\":355}]", "title": "[{\"end\":78,\"start\":1},{\"end\":470,\"start\":393}]", "venue": null, "abstract": "[{\"end\":1719,\"start\":516}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3698,\"start\":3680},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3785,\"start\":3768},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3802,\"start\":3785},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3819,\"start\":3802},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4527,\"start\":4506},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5411,\"start\":5394},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6199,\"start\":6178},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6219,\"start\":6199},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6421,\"start\":6400},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":11247,\"start\":11226},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11926,\"start\":11909},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12507,\"start\":12486},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12993,\"start\":12969},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":14537,\"start\":14513},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":15522,\"start\":15504},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":15539,\"start\":15522},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":15556,\"start\":15539},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":15573,\"start\":15556},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":15671,\"start\":15653},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":15857,\"start\":15840},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":15927,\"start\":15896},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":15989,\"start\":15978},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":17122,\"start\":17099},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":17253,\"start\":17228},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":17417,\"start\":17399},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":17625,\"start\":17604},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":17744,\"start\":17722},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":17911,\"start\":17894},{\"end\":17952,\"start\":17926},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":17993,\"start\":17975},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":20857,\"start\":20839},{\"end\":23809,\"start\":23770},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":23835,\"start\":23818},{\"end\":23863,\"start\":23837},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":23891,\"start\":23873},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":23940,\"start\":23922},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":24454,\"start\":24436},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":25268,\"start\":25250},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":25335,\"start\":25318},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":35713,\"start\":35696},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":35819,\"start\":35802},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":37717,\"start\":37700},{\"end\":38597,\"start\":38573}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":40139,\"start\":40072},{\"attributes\":{\"id\":\"fig_2\"},\"end\":40451,\"start\":40140},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":40776,\"start\":40452},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":41427,\"start\":40777},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":42639,\"start\":41428},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":43972,\"start\":42640},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":44294,\"start\":43973},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":44657,\"start\":44295},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":44784,\"start\":44658},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":45461,\"start\":44785},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":45590,\"start\":45462},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":45772,\"start\":45591},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":46839,\"start\":45773}]", "paragraph": "[{\"end\":2364,\"start\":1735},{\"end\":4133,\"start\":2366},{\"end\":4308,\"start\":4135},{\"end\":5077,\"start\":4352},{\"end\":5347,\"start\":5151},{\"end\":6027,\"start\":5349},{\"end\":6699,\"start\":6057},{\"end\":7884,\"start\":6701},{\"end\":8558,\"start\":7914},{\"end\":9236,\"start\":8560},{\"end\":10062,\"start\":9238},{\"end\":10242,\"start\":10075},{\"end\":10310,\"start\":10307},{\"end\":11092,\"start\":10383},{\"end\":11588,\"start\":11094},{\"end\":12285,\"start\":11696},{\"end\":12586,\"start\":12368},{\"end\":12871,\"start\":12682},{\"end\":13592,\"start\":12873},{\"end\":13972,\"start\":13665},{\"end\":14041,\"start\":13974},{\"end\":14096,\"start\":14043},{\"end\":14143,\"start\":14098},{\"end\":14200,\"start\":14145},{\"end\":14227,\"start\":14202},{\"end\":14812,\"start\":14229},{\"end\":15412,\"start\":14814},{\"end\":16388,\"start\":15451},{\"end\":16999,\"start\":16390},{\"end\":17793,\"start\":17001},{\"end\":18226,\"start\":17795},{\"end\":19346,\"start\":18228},{\"end\":19744,\"start\":19399},{\"end\":20722,\"start\":19769},{\"end\":21561,\"start\":20724},{\"end\":21967,\"start\":21576},{\"end\":22482,\"start\":21969},{\"end\":23204,\"start\":22534},{\"end\":23710,\"start\":23206},{\"end\":25431,\"start\":23727},{\"end\":26237,\"start\":25462},{\"end\":30025,\"start\":26261},{\"end\":30207,\"start\":30122},{\"end\":30365,\"start\":30209},{\"end\":30467,\"start\":30367},{\"end\":30886,\"start\":30469},{\"end\":30993,\"start\":30888},{\"end\":31153,\"start\":30995},{\"end\":31653,\"start\":31155},{\"end\":31752,\"start\":31655},{\"end\":31835,\"start\":31754},{\"end\":32190,\"start\":31837},{\"end\":32281,\"start\":32192},{\"end\":32426,\"start\":32283},{\"end\":32884,\"start\":32428},{\"end\":34132,\"start\":32915},{\"end\":34459,\"start\":34134},{\"end\":35530,\"start\":34461},{\"end\":35714,\"start\":35550},{\"end\":36070,\"start\":35716},{\"end\":36565,\"start\":36072},{\"end\":37064,\"start\":36567},{\"end\":37599,\"start\":37066},{\"end\":37897,\"start\":37623},{\"end\":38506,\"start\":37899},{\"end\":38894,\"start\":38531},{\"end\":39035,\"start\":38910},{\"end\":39507,\"start\":39037},{\"end\":39560,\"start\":39509},{\"end\":40071,\"start\":39606}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":5150,\"start\":5078},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10306,\"start\":10243},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10382,\"start\":10311},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11695,\"start\":11589},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12367,\"start\":12286},{\"attributes\":{\"id\":\"formula_5\"},\"end\":12681,\"start\":12587},{\"attributes\":{\"id\":\"formula_6\"},\"end\":35549,\"start\":35531}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":15761,\"start\":15754},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":18577,\"start\":18570},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":19836,\"start\":19829},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":20120,\"start\":20112},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":20128,\"start\":20121},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":21104,\"start\":21097},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":22588,\"start\":22581},{\"end\":26958,\"start\":26951},{\"end\":27717,\"start\":27710},{\"end\":28433,\"start\":28426},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":29093,\"start\":29085},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":29886,\"start\":29878},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":30707,\"start\":30699},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":32045,\"start\":32037}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1733,\"start\":1721},{\"attributes\":{\"n\":\"2\"},\"end\":4350,\"start\":4311},{\"attributes\":{\"n\":\"3\"},\"end\":6055,\"start\":6030},{\"attributes\":{\"n\":\"3.1\"},\"end\":7912,\"start\":7887},{\"attributes\":{\"n\":\"3.2\"},\"end\":10073,\"start\":10065},{\"end\":13663,\"start\":13595},{\"attributes\":{\"n\":\"4\"},\"end\":15426,\"start\":15415},{\"attributes\":{\"n\":\"4.1\"},\"end\":15449,\"start\":15429},{\"attributes\":{\"n\":\"4.2\"},\"end\":19369,\"start\":19349},{\"attributes\":{\"n\":\"4.2.1\"},\"end\":19397,\"start\":19372},{\"attributes\":{\"n\":\"4.2.2\"},\"end\":19767,\"start\":19747},{\"attributes\":{\"n\":\"4.3\"},\"end\":21574,\"start\":21564},{\"end\":22532,\"start\":22485},{\"attributes\":{\"n\":\"5\"},\"end\":23725,\"start\":23713},{\"attributes\":{\"n\":\"6\"},\"end\":25460,\"start\":25434},{\"end\":26259,\"start\":26240},{\"end\":30071,\"start\":30028},{\"end\":30120,\"start\":30074},{\"end\":32913,\"start\":32887},{\"end\":37621,\"start\":37602},{\"end\":38529,\"start\":38509},{\"end\":38908,\"start\":38897},{\"end\":39604,\"start\":39563},{\"end\":40083,\"start\":40073},{\"end\":40175,\"start\":40141},{\"end\":40462,\"start\":40453},{\"end\":40787,\"start\":40778},{\"end\":41438,\"start\":41429},{\"end\":42650,\"start\":42641},{\"end\":43983,\"start\":43974},{\"end\":44305,\"start\":44296},{\"end\":44669,\"start\":44659},{\"end\":44796,\"start\":44786},{\"end\":45473,\"start\":45463},{\"end\":45602,\"start\":45592},{\"end\":45784,\"start\":45774}]", "table": "[{\"end\":40776,\"start\":40511},{\"end\":41427,\"start\":41078},{\"end\":42639,\"start\":41773},{\"end\":43972,\"start\":42982},{\"end\":44294,\"start\":44110},{\"end\":44657,\"start\":44380},{\"end\":45461,\"start\":45237},{\"end\":46839,\"start\":46110}]", "figure_caption": "[{\"end\":40139,\"start\":40085},{\"end\":40451,\"start\":40179},{\"end\":40511,\"start\":40464},{\"end\":41078,\"start\":40789},{\"end\":41773,\"start\":41440},{\"end\":42982,\"start\":42652},{\"end\":44110,\"start\":43985},{\"end\":44380,\"start\":44307},{\"end\":44784,\"start\":44672},{\"end\":45237,\"start\":44799},{\"end\":45590,\"start\":45476},{\"end\":45772,\"start\":45605},{\"end\":46110,\"start\":45787}]", "figure_ref": "[{\"end\":2539,\"start\":2527},{\"end\":2826,\"start\":2818},{\"end\":3834,\"start\":3826},{\"end\":6689,\"start\":6681},{\"end\":6743,\"start\":6731},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":8692,\"start\":8684},{\"end\":19548,\"start\":19540},{\"end\":19917,\"start\":19909}]", "bib_author_first_name": "[{\"end\":48482,\"start\":48478},{\"end\":48496,\"start\":48491},{\"end\":48513,\"start\":48506},{\"end\":48526,\"start\":48522},{\"end\":48806,\"start\":48800},{\"end\":48825,\"start\":48817},{\"end\":48835,\"start\":48830},{\"end\":48849,\"start\":48843},{\"end\":49265,\"start\":49260},{\"end\":49279,\"start\":49272},{\"end\":49293,\"start\":49285},{\"end\":49306,\"start\":49299},{\"end\":49317,\"start\":49314},{\"end\":49330,\"start\":49323},{\"end\":49342,\"start\":49337},{\"end\":49356,\"start\":49350},{\"end\":49368,\"start\":49363},{\"end\":49384,\"start\":49376},{\"end\":49815,\"start\":49809},{\"end\":49830,\"start\":49825},{\"end\":49844,\"start\":49838},{\"end\":49858,\"start\":49854},{\"end\":49876,\"start\":49869},{\"end\":50163,\"start\":50160},{\"end\":50180,\"start\":50176},{\"end\":50201,\"start\":50196},{\"end\":50213,\"start\":50209},{\"end\":50223,\"start\":50218},{\"end\":50245,\"start\":50238},{\"end\":50258,\"start\":50253},{\"end\":50276,\"start\":50270},{\"end\":50638,\"start\":50637},{\"end\":50885,\"start\":50878},{\"end\":50895,\"start\":50891},{\"end\":50903,\"start\":50900},{\"end\":50915,\"start\":50909},{\"end\":50927,\"start\":50923},{\"end\":50935,\"start\":50932},{\"end\":51296,\"start\":51290},{\"end\":51311,\"start\":51305},{\"end\":51328,\"start\":51322},{\"end\":51350,\"start\":51342},{\"end\":51364,\"start\":51360},{\"end\":51706,\"start\":51702},{\"end\":51725,\"start\":51719},{\"end\":51948,\"start\":51942},{\"end\":52189,\"start\":52184},{\"end\":52207,\"start\":52202},{\"end\":52221,\"start\":52217},{\"end\":52236,\"start\":52232},{\"end\":52253,\"start\":52246},{\"end\":52512,\"start\":52508},{\"end\":52734,\"start\":52733},{\"end\":52769,\"start\":52741},{\"end\":53041,\"start\":53037},{\"end\":53056,\"start\":53052},{\"end\":53512,\"start\":53507},{\"end\":53521,\"start\":53517},{\"end\":53537,\"start\":53530},{\"end\":53552,\"start\":53543},{\"end\":53563,\"start\":53559},{\"end\":53575,\"start\":53572},{\"end\":53995,\"start\":53994},{\"end\":54013,\"start\":54005},{\"end\":54015,\"start\":54014},{\"end\":54036,\"start\":54027},{\"end\":54050,\"start\":54043},{\"end\":54063,\"start\":54060},{\"end\":54076,\"start\":54071},{\"end\":54088,\"start\":54083},{\"end\":54100,\"start\":54096},{\"end\":54473,\"start\":54468},{\"end\":54485,\"start\":54479},{\"end\":54498,\"start\":54490},{\"end\":54511,\"start\":54503},{\"end\":54528,\"start\":54519},{\"end\":54853,\"start\":54848},{\"end\":54863,\"start\":54859},{\"end\":54881,\"start\":54876},{\"end\":55084,\"start\":55077},{\"end\":55100,\"start\":55095},{\"end\":55113,\"start\":55109},{\"end\":55128,\"start\":55120},{\"end\":55659,\"start\":55658},{\"end\":55675,\"start\":55668},{\"end\":55691,\"start\":55684},{\"end\":55709,\"start\":55703},{\"end\":55726,\"start\":55718},{\"end\":56196,\"start\":56192},{\"end\":56208,\"start\":56202},{\"end\":56221,\"start\":56215},{\"end\":56235,\"start\":56227},{\"end\":56492,\"start\":56487},{\"end\":56507,\"start\":56501},{\"end\":56528,\"start\":56523},{\"end\":56546,\"start\":56539},{\"end\":56562,\"start\":56559},{\"end\":56576,\"start\":56570},{\"end\":56589,\"start\":56583},{\"end\":56603,\"start\":56598},{\"end\":56618,\"start\":56611},{\"end\":56630,\"start\":56624},{\"end\":56977,\"start\":56976},{\"end\":56992,\"start\":56987},{\"end\":56994,\"start\":56993},{\"end\":57016,\"start\":57015},{\"end\":57033,\"start\":57027},{\"end\":57408,\"start\":57402},{\"end\":57419,\"start\":57413},{\"end\":57430,\"start\":57427},{\"end\":57441,\"start\":57437},{\"end\":57740,\"start\":57735},{\"end\":57751,\"start\":57748},{\"end\":57760,\"start\":57757},{\"end\":57769,\"start\":57766},{\"end\":57783,\"start\":57776},{\"end\":57798,\"start\":57791},{\"end\":57813,\"start\":57805},{\"end\":58309,\"start\":58298},{\"end\":58318,\"start\":58316},{\"end\":58639,\"start\":58632},{\"end\":58649,\"start\":58645},{\"end\":58657,\"start\":58654},{\"end\":58672,\"start\":58665},{\"end\":58684,\"start\":58678},{\"end\":58695,\"start\":58692},{\"end\":58706,\"start\":58702},{\"end\":61960,\"start\":61959}]", "bib_author_last_name": "[{\"end\":48489,\"start\":48483},{\"end\":48504,\"start\":48497},{\"end\":48520,\"start\":48514},{\"end\":48534,\"start\":48527},{\"end\":48815,\"start\":48807},{\"end\":48828,\"start\":48826},{\"end\":48841,\"start\":48836},{\"end\":48855,\"start\":48850},{\"end\":48859,\"start\":48857},{\"end\":49270,\"start\":49266},{\"end\":49283,\"start\":49280},{\"end\":49297,\"start\":49294},{\"end\":49312,\"start\":49307},{\"end\":49321,\"start\":49318},{\"end\":49335,\"start\":49331},{\"end\":49348,\"start\":49343},{\"end\":49361,\"start\":49357},{\"end\":49374,\"start\":49369},{\"end\":49390,\"start\":49385},{\"end\":49823,\"start\":49816},{\"end\":49836,\"start\":49831},{\"end\":49852,\"start\":49845},{\"end\":49867,\"start\":49859},{\"end\":49883,\"start\":49877},{\"end\":50174,\"start\":50164},{\"end\":50194,\"start\":50181},{\"end\":50207,\"start\":50202},{\"end\":50216,\"start\":50214},{\"end\":50236,\"start\":50224},{\"end\":50251,\"start\":50246},{\"end\":50268,\"start\":50259},{\"end\":50283,\"start\":50277},{\"end\":50642,\"start\":50639},{\"end\":50654,\"start\":50644},{\"end\":50889,\"start\":50886},{\"end\":50898,\"start\":50896},{\"end\":50907,\"start\":50904},{\"end\":50921,\"start\":50916},{\"end\":50930,\"start\":50928},{\"end\":50940,\"start\":50936},{\"end\":51303,\"start\":51297},{\"end\":51320,\"start\":51312},{\"end\":51340,\"start\":51329},{\"end\":51358,\"start\":51351},{\"end\":51375,\"start\":51365},{\"end\":51717,\"start\":51707},{\"end\":51737,\"start\":51726},{\"end\":51968,\"start\":51949},{\"end\":52200,\"start\":52190},{\"end\":52215,\"start\":52208},{\"end\":52230,\"start\":52222},{\"end\":52244,\"start\":52237},{\"end\":52256,\"start\":52254},{\"end\":52516,\"start\":52513},{\"end\":52739,\"start\":52735},{\"end\":52776,\"start\":52770},{\"end\":53050,\"start\":53042},{\"end\":53062,\"start\":53057},{\"end\":53515,\"start\":53513},{\"end\":53528,\"start\":53522},{\"end\":53541,\"start\":53538},{\"end\":53557,\"start\":53553},{\"end\":53570,\"start\":53564},{\"end\":53584,\"start\":53576},{\"end\":54003,\"start\":53996},{\"end\":54025,\"start\":54016},{\"end\":54041,\"start\":54037},{\"end\":54058,\"start\":54051},{\"end\":54069,\"start\":54064},{\"end\":54081,\"start\":54077},{\"end\":54094,\"start\":54089},{\"end\":54107,\"start\":54101},{\"end\":54117,\"start\":54109},{\"end\":54477,\"start\":54474},{\"end\":54488,\"start\":54486},{\"end\":54501,\"start\":54499},{\"end\":54517,\"start\":54512},{\"end\":54532,\"start\":54529},{\"end\":54857,\"start\":54854},{\"end\":54874,\"start\":54864},{\"end\":54887,\"start\":54882},{\"end\":55093,\"start\":55085},{\"end\":55107,\"start\":55101},{\"end\":55118,\"start\":55114},{\"end\":55132,\"start\":55129},{\"end\":55666,\"start\":55660},{\"end\":55682,\"start\":55676},{\"end\":55701,\"start\":55692},{\"end\":55716,\"start\":55710},{\"end\":55731,\"start\":55727},{\"end\":55737,\"start\":55733},{\"end\":56200,\"start\":56197},{\"end\":56213,\"start\":56209},{\"end\":56225,\"start\":56222},{\"end\":56241,\"start\":56236},{\"end\":56499,\"start\":56493},{\"end\":56521,\"start\":56508},{\"end\":56537,\"start\":56529},{\"end\":56557,\"start\":56547},{\"end\":56568,\"start\":56563},{\"end\":56581,\"start\":56577},{\"end\":56596,\"start\":56590},{\"end\":56609,\"start\":56604},{\"end\":56622,\"start\":56619},{\"end\":56637,\"start\":56631},{\"end\":56985,\"start\":56978},{\"end\":57001,\"start\":56995},{\"end\":57013,\"start\":57003},{\"end\":57025,\"start\":57017},{\"end\":57039,\"start\":57034},{\"end\":57048,\"start\":57041},{\"end\":57411,\"start\":57409},{\"end\":57425,\"start\":57420},{\"end\":57435,\"start\":57431},{\"end\":57444,\"start\":57442},{\"end\":57746,\"start\":57741},{\"end\":57755,\"start\":57752},{\"end\":57764,\"start\":57761},{\"end\":57774,\"start\":57770},{\"end\":57789,\"start\":57784},{\"end\":57803,\"start\":57799},{\"end\":57819,\"start\":57814},{\"end\":58314,\"start\":58310},{\"end\":58321,\"start\":58319},{\"end\":58643,\"start\":58640},{\"end\":58652,\"start\":58650},{\"end\":58663,\"start\":58658},{\"end\":58676,\"start\":58673},{\"end\":58690,\"start\":58685},{\"end\":58700,\"start\":58696},{\"end\":58709,\"start\":58707},{\"end\":61968,\"start\":61961},{\"end\":61973,\"start\":61970}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":1820089},\"end\":48798,\"start\":48403},{\"attributes\":{\"doi\":\"arXiv:1702.07983\",\"id\":\"b1\"},\"end\":49200,\"start\":48800},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":52292169},\"end\":49711,\"start\":49202},{\"attributes\":{\"doi\":\"arXiv:1705.02364\",\"id\":\"b3\"},\"end\":50129,\"start\":49713},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":1033682},\"end\":50570,\"start\":50131},{\"attributes\":{\"doi\":\"arXiv:1412.6515\",\"id\":\"b5\"},\"end\":50805,\"start\":50572},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":3389583},\"end\":51205,\"start\":50807},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":326772},\"end\":51676,\"start\":51207},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":1915014},\"end\":51867,\"start\":51678},{\"attributes\":{\"doi\":\"arXiv:1807.00734\",\"id\":\"b9\"},\"end\":52139,\"start\":51869},{\"attributes\":{\"doi\":\"arXiv:1602.02410\",\"id\":\"b10\"},\"end\":52447,\"start\":52141},{\"attributes\":{\"doi\":\"arXiv:1408.5882\",\"id\":\"b11\"},\"end\":52653,\"start\":52449},{\"attributes\":{\"doi\":\"arXiv:1611.04051\",\"id\":\"b12\"},\"end\":52972,\"start\":52655},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":1554351},\"end\":53452,\"start\":52974},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":98180},\"end\":53992,\"start\":53454},{\"attributes\":{\"doi\":\"arXiv:1509.02971\",\"id\":\"b15\"},\"end\":54421,\"start\":53994},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":4857922},\"end\":54774,\"start\":54423},{\"attributes\":{\"id\":\"b17\"},\"end\":55011,\"start\":54776},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":11080756},\"end\":55602,\"start\":55013},{\"attributes\":{\"doi\":\"10.1109/CVPR.2017.131\",\"id\":\"b19\",\"matched_paper_id\":206594923},\"end\":56121,\"start\":55604},{\"attributes\":{\"doi\":\"arXiv:1804.11258\",\"id\":\"b20\"},\"end\":56435,\"start\":56123},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":205261034},\"end\":56894,\"start\":56437},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":1211821},\"end\":57333,\"start\":56896},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":3439214},\"end\":57683,\"start\":57335},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":26668832},\"end\":58202,\"start\":57685},{\"attributes\":{\"id\":\"b25\"},\"end\":58569,\"start\":58204},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":3636178},\"end\":59078,\"start\":58571},{\"attributes\":{\"id\":\"b27\"},\"end\":61955,\"start\":59080},{\"attributes\":{\"doi\":\"6.69 \u00b10.29 6.91\u00b10.27\",\"id\":\"b28\"},\"end\":62029,\"start\":61957}]", "bib_title": "[{\"end\":48476,\"start\":48403},{\"end\":49258,\"start\":49202},{\"end\":50158,\"start\":50131},{\"end\":50876,\"start\":50807},{\"end\":51288,\"start\":51207},{\"end\":51700,\"start\":51678},{\"end\":53035,\"start\":52974},{\"end\":53505,\"start\":53454},{\"end\":54466,\"start\":54423},{\"end\":55075,\"start\":55013},{\"end\":55656,\"start\":55604},{\"end\":56485,\"start\":56437},{\"end\":56974,\"start\":56896},{\"end\":57400,\"start\":57335},{\"end\":57733,\"start\":57685},{\"end\":58296,\"start\":58204},{\"end\":58630,\"start\":58571},{\"end\":60114,\"start\":59080}]", "bib_author": "[{\"end\":48491,\"start\":48478},{\"end\":48506,\"start\":48491},{\"end\":48522,\"start\":48506},{\"end\":48536,\"start\":48522},{\"end\":48817,\"start\":48800},{\"end\":48830,\"start\":48817},{\"end\":48843,\"start\":48830},{\"end\":48857,\"start\":48843},{\"end\":48861,\"start\":48857},{\"end\":49272,\"start\":49260},{\"end\":49285,\"start\":49272},{\"end\":49299,\"start\":49285},{\"end\":49314,\"start\":49299},{\"end\":49323,\"start\":49314},{\"end\":49337,\"start\":49323},{\"end\":49350,\"start\":49337},{\"end\":49363,\"start\":49350},{\"end\":49376,\"start\":49363},{\"end\":49392,\"start\":49376},{\"end\":49825,\"start\":49809},{\"end\":49838,\"start\":49825},{\"end\":49854,\"start\":49838},{\"end\":49869,\"start\":49854},{\"end\":49885,\"start\":49869},{\"end\":50176,\"start\":50160},{\"end\":50196,\"start\":50176},{\"end\":50209,\"start\":50196},{\"end\":50218,\"start\":50209},{\"end\":50238,\"start\":50218},{\"end\":50253,\"start\":50238},{\"end\":50270,\"start\":50253},{\"end\":50285,\"start\":50270},{\"end\":50644,\"start\":50637},{\"end\":50656,\"start\":50644},{\"end\":50891,\"start\":50878},{\"end\":50900,\"start\":50891},{\"end\":50909,\"start\":50900},{\"end\":50923,\"start\":50909},{\"end\":50932,\"start\":50923},{\"end\":50942,\"start\":50932},{\"end\":51305,\"start\":51290},{\"end\":51322,\"start\":51305},{\"end\":51342,\"start\":51322},{\"end\":51360,\"start\":51342},{\"end\":51377,\"start\":51360},{\"end\":51719,\"start\":51702},{\"end\":51739,\"start\":51719},{\"end\":51970,\"start\":51942},{\"end\":52202,\"start\":52184},{\"end\":52217,\"start\":52202},{\"end\":52232,\"start\":52217},{\"end\":52246,\"start\":52232},{\"end\":52258,\"start\":52246},{\"end\":52518,\"start\":52508},{\"end\":52741,\"start\":52733},{\"end\":52778,\"start\":52741},{\"end\":53052,\"start\":53037},{\"end\":53064,\"start\":53052},{\"end\":53517,\"start\":53507},{\"end\":53530,\"start\":53517},{\"end\":53543,\"start\":53530},{\"end\":53559,\"start\":53543},{\"end\":53572,\"start\":53559},{\"end\":53586,\"start\":53572},{\"end\":54005,\"start\":53994},{\"end\":54027,\"start\":54005},{\"end\":54043,\"start\":54027},{\"end\":54060,\"start\":54043},{\"end\":54071,\"start\":54060},{\"end\":54083,\"start\":54071},{\"end\":54096,\"start\":54083},{\"end\":54109,\"start\":54096},{\"end\":54119,\"start\":54109},{\"end\":54479,\"start\":54468},{\"end\":54490,\"start\":54479},{\"end\":54503,\"start\":54490},{\"end\":54519,\"start\":54503},{\"end\":54534,\"start\":54519},{\"end\":54859,\"start\":54848},{\"end\":54876,\"start\":54859},{\"end\":54889,\"start\":54876},{\"end\":55095,\"start\":55077},{\"end\":55109,\"start\":55095},{\"end\":55120,\"start\":55109},{\"end\":55134,\"start\":55120},{\"end\":55668,\"start\":55658},{\"end\":55684,\"start\":55668},{\"end\":55703,\"start\":55684},{\"end\":55718,\"start\":55703},{\"end\":55733,\"start\":55718},{\"end\":55739,\"start\":55733},{\"end\":56202,\"start\":56192},{\"end\":56215,\"start\":56202},{\"end\":56227,\"start\":56215},{\"end\":56243,\"start\":56227},{\"end\":56501,\"start\":56487},{\"end\":56523,\"start\":56501},{\"end\":56539,\"start\":56523},{\"end\":56559,\"start\":56539},{\"end\":56570,\"start\":56559},{\"end\":56583,\"start\":56570},{\"end\":56598,\"start\":56583},{\"end\":56611,\"start\":56598},{\"end\":56624,\"start\":56611},{\"end\":56639,\"start\":56624},{\"end\":56987,\"start\":56976},{\"end\":57003,\"start\":56987},{\"end\":57015,\"start\":57003},{\"end\":57027,\"start\":57015},{\"end\":57041,\"start\":57027},{\"end\":57050,\"start\":57041},{\"end\":57413,\"start\":57402},{\"end\":57427,\"start\":57413},{\"end\":57437,\"start\":57427},{\"end\":57446,\"start\":57437},{\"end\":57748,\"start\":57735},{\"end\":57757,\"start\":57748},{\"end\":57766,\"start\":57757},{\"end\":57776,\"start\":57766},{\"end\":57791,\"start\":57776},{\"end\":57805,\"start\":57791},{\"end\":57821,\"start\":57805},{\"end\":58316,\"start\":58298},{\"end\":58323,\"start\":58316},{\"end\":58645,\"start\":58632},{\"end\":58654,\"start\":58645},{\"end\":58665,\"start\":58654},{\"end\":58678,\"start\":58665},{\"end\":58692,\"start\":58678},{\"end\":58702,\"start\":58692},{\"end\":58711,\"start\":58702},{\"end\":61970,\"start\":61959},{\"end\":61975,\"start\":61970}]", "bib_venue": "[{\"end\":53229,\"start\":53155},{\"end\":53745,\"start\":53674},{\"end\":55287,\"start\":55219},{\"end\":57944,\"start\":57891},{\"end\":48585,\"start\":48536},{\"end\":48979,\"start\":48877},{\"end\":49441,\"start\":49392},{\"end\":49807,\"start\":49713},{\"end\":50334,\"start\":50285},{\"end\":50635,\"start\":50572},{\"end\":50998,\"start\":50942},{\"end\":51426,\"start\":51377},{\"end\":51757,\"start\":51739},{\"end\":51940,\"start\":51869},{\"end\":52182,\"start\":52141},{\"end\":52506,\"start\":52449},{\"end\":52731,\"start\":52655},{\"end\":53153,\"start\":53064},{\"end\":53672,\"start\":53586},{\"end\":54186,\"start\":54135},{\"end\":54583,\"start\":54534},{\"end\":54846,\"start\":54776},{\"end\":55217,\"start\":55134},{\"end\":55825,\"start\":55760},{\"end\":56190,\"start\":56123},{\"end\":56645,\"start\":56639},{\"end\":57099,\"start\":57050},{\"end\":57501,\"start\":57446},{\"end\":57889,\"start\":57821},{\"end\":58379,\"start\":58323},{\"end\":58805,\"start\":58711},{\"end\":60235,\"start\":60116}]"}}}, "year": 2023, "month": 12, "day": 17}