{"id": 14393040, "updated": "2023-09-28 05:53:58.389", "metadata": {"title": "Newton Sketch: A Linear-time Optimization Algorithm with Linear-Quadratic Convergence", "authors": "[{\"first\":\"Mert\",\"last\":\"Pilanci\",\"middle\":[]},{\"first\":\"Martin\",\"last\":\"Wainwright\",\"middle\":[\"J.\"]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2015, "month": null, "day": null}, "abstract": "We propose a randomized second-order method for optimization known as the Newton Sketch: it is based on performing an approximate Newton step using a randomly projected or sub-sampled Hessian. For self-concordant functions, we prove that the algorithm has super-linear convergence with exponentially high probability, with convergence and complexity guarantees that are independent of condition numbers and related problem-dependent quantities. Given a suitable initialization, similar guarantees also hold for strongly convex and smooth objectives without self-concordance. When implemented using randomized projections based on a sub-sampled Hadamard basis, the algorithm typically has substantially lower complexity than Newton's method. We also describe extensions of our methods to programs involving convex constraints that are equipped with self-concordant barriers. We discuss and illustrate applications to linear programs, quadratic programs with convex constraints, logistic regression and other generalized linear models, as well as semidefinite programs.", "fields_of_study": "[\"Mathematics\",\"Computer Science\"]", "external_ids": {"arxiv": "1505.02250", "mag": "2963060476", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/PilanciW15", "doi": "10.1137/15m1021106"}}, "content": {"source": {"pdf_hash": "5bb269a4925271291dfeddb385993d44ece6e36f", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1505.02250v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "4252efcd7ec97a5664bb0b65e7c0f277ef404837", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/5bb269a4925271291dfeddb385993d44ece6e36f.txt", "contents": "\nNewton Sketch: A Linear-time Optimization Algorithm with Linear-Quadratic Convergence\nMay 12, 2015\n\nMert Pilanci \nDepartment of Electrical Engineering and Computer Science\n\n\nMartin J Wainwright wainwrig@berkeley.edu \nDepartment of Electrical Engineering and Computer Science\n\n\nDepartment of Statistics\n\n\n\nUniversity of California\nBerkeley\n\nNewton Sketch: A Linear-time Optimization Algorithm with Linear-Quadratic Convergence\nMay 12, 2015\nWe propose a randomized second-order method for optimization known as the Newton Sketch: it is based on performing an approximate Newton step using a randomly projected or sub-sampled Hessian. For self-concordant functions, we prove that the algorithm has super-linear convergence with exponentially high probability, with convergence and complexity guarantees that are independent of condition numbers and related problem-dependent quantities. Given a suitable initialization, similar guarantees also hold for strongly convex and smooth objectives without self-concordance. When implemented using randomized projections based on a sub-sampled Hadamard basis, the algorithm typically has substantially lower complexity than Newton's method. We also describe extensions of our methods to programs involving convex constraints that are equipped with self-concordant barriers. We discuss and illustrate applications to linear programs, quadratic programs with convex constraints, logistic regression and other generalized linear models, as well as semidefinite programs.\n\nIntroduction\n\nRelative to first-order methods, second-order methods for convex optimization enjoy superior convergence in both theory and practice. For instance, Newton's method converges at a quadratic rate for strongly convex and smooth problems, and moreover, even for weakly convex functions (i.e. not strongly convex), modifications of Newton's method has super-linear convergence compared to the much slower 1/T 2 convergence rate that can be achieved by a first-order method like accelerated gradient descent (see e.g. [15]). More importantly, at least in a uniform sense, the 1/T 2 -rate is known to be unimprovable for first-order methods [17]. Yet another issue in first-order methods is the tuning of step size, whose optimal choice depends on the strong convexity parameter and/or smoothness of the underlying problem. For example, consider the problem of optimizing a function of the form x \u2192 g(Ax), where A \u2208 R n\u00d7d is a \"data matrix\", and g : R n \u2192 R is a twice-differentiable function. Here the performance of first-order methods will depend on both the convexity/smoothness of g, as well as the conditioning of the data matrix. In contrast, whenever the function g is self-concordant, then Newton's method with suitably damped steps has a global complexity guarantee that is provably independent of such problem-dependent parameters.\n\nOn the other hand, each step of Newton's method requires solving a linear system defined by the Hessian matrix. For instance, in application to the problem family just described involving an n \u00d7 d data matrix, each of these steps has complexity scaling as O(nd 2 ). For this reason, both forming the Hessian and solving the corresponding linear system pose a tremendous numerical challenge for large values of (n, d)-for instance, values of thousands to millions, as is common in big data applications, In order to address this issue, a multitude of different approximations to Newton's method have been proposed and studied in the literature. Quasi-Newton methods form estimates of the Hessian by successive evaluations of the gradient vectors and are computationally cheaper. Examples of such methods include DFP and BFGS schemes and also their limited memory versions (see the book [25] for further details). A disadvantage of such approximations based on first-order information is that the associated convergence guarantees are typically much weaker than those of Newton's method and require stronger assumptions. Under restrictions on the eigenvalues of the Hessian (strong convexity and smoothness), Quasi-Newton methods typically exhibit local super-linear convergence.\n\nIn this paper, we propose and analyze a randomized approximation of Newton's method, known as the Newton Sketch. Instead of explicitly computing the Hessian, the Newton Sketch method approximates it via a random projection of dimension m. When these projections are carried out using the randomized Hadamard transform, each iteration has complexity O(nd log(m) + dm 2 ). Our results show that it is always sufficient to choose m proportional to min{d, n}, and moreover, that the sketch dimension m can be much smaller for certain types of constrained problems. Thus, in the regime n > d and with m d, the complexity per iteration can be substantially lower than the O(nd 2 ) complexity of each Newton step. Specifically for n \u2265 d 2 , the complexity of Newton Sketch per iteration is O(nd log d), which is linear in the input size (nd) and comparable to first order methods which only access the derivative g (Ax). Moreover, we show that for self-concordant functions, the total complexity of obtaining a \u03b4-optimal solution is O(nd log d log(1/\u03b4)), and does not depend on constants such as strong convexity or smoothness parameters unlike first order methods. On the other hand, for problems with d > n, we also provide a dual strategy which effectively has the same guarantees with roles of d and n exchanged.\n\nWe also consider other random projection matrices and sub-sampling strategies, including partial forms of random projection that exploit known structure in the Hessian. For selfconcordant functions, we provide an affine invariant analysis proving that the convergence is linear-quadratic and the guarantees are independent of the function and data, such as condition numbers of matrices involved in the objective function. Finally, we describe an interior point method to deal with arbitrary convex constraints which combines the Newton sketch with the barrier method. We provide an upper bound on the total number of iterations required to obtain a solution with a pre-specified target accuracy.\n\nThe remainder of this paper is organized as follows. We begin in Section 2 with some background on the classical form of Newton's method, random matrices for sketching, and Gaussian widths as a measure of the size of a set. In Section 3, we formally introduce the Newton Sketch, including both fully and partially sketched versions for unconstrained and constrained problems. We provide some illustrative examples in Section 3.2 before turning to local convergence theory in Section 3.3. Section 4 is devoted to global convergence results for self-concordant functions, in both the constrained and unconstrained settings. In Section 5, we consider a number of applications and provide additional numerical results. The bulk of our proofs are in given in Section 6, with some more technical aspects deferred to the appendices.\n\n\nBackground\n\nWe begin with some background material on the standard form of Newton's method, various types of random sketches, and the notion of Gaussian width as a complexity measure.\n\n\nClassical version of Newton's method\n\nIn this section, we briefly review the convergence properties and complexity of the classical form of Newton's method; see the sources [25,4,17] for further background.\n\nLet f : R d \u2192 R be a closed, convex and twice-differentiable function that is bounded below. Given a convex set C, we assume that the constrained minimizer\nx * : = arg min x\u2208C f (x) (1)\nis uniquely defined, and we define the minimum and maximum eigenvalues \u03b3 = \u03bb min (\u2207 2 f (x * )) and \u03b2 = \u03bb max (\u2207 2 f (x * )) of the Hessian evaluated at the minimum. We assume moreover that the Hessian map x \u2192 \u2207 2 f (x) is Lipschitz continuous with modulus L, meaning that\n|||\u2207 2 f (x + \u2206) \u2212 \u2207 2 f (x)||| op \u2264 L \u2206 2 .(2)\nUnder these conditions and given an initial pointx 0 \u2208 C such that x 0 \u2212 x * 2 \u2264 \u03b3 2L , the Newton updates are guaranteed to converge quadratically-viz.\nx t+1 \u2212 x * 2 \u2264 2L \u03b3 x t \u2212 x * 2 2 ,\nThis result is classical: for instance, see Boyd and Vandenberghe [4] for a proof. Newton's method can be slightly modified to be globally convergent by choosing the step sizes via a simple backtracking line-search procedure.\n\nThe following result characterizes the complexity of Newton's method when applied to self-concordant functions and is central in the development of interior point methods (for instance, see the books [18,4]). We defer the definitions of self-concordance and the linesearch procedure in the following sections. The number of iterations needed to obtain a \u03b4 approximate minimizer of a strictly convex self-concordant function f is bounded by\n20 \u2212 8a ab(1 \u2212 2a) f (x 0 ) \u2212 f (x * ) + log 2 log 2 (1/\u03b4) ,\nwhere a, b are constants in the line-search procedure. 1\n\n\nDifferent types of randomized sketches\n\nVarious types of randomized sketches are possible, and we describe a few of them here. Given a sketching matrix S \u2208 R m\u00d7n , we use {s i } m i=1 to denote the collection of its n-dimensional rows. We restrict our attention to sketch matrices that are zero-mean, and that are normalized so that E[S T S/m] = I n .\n\nSub-Gaussian sketches: The most classical sketch is based on a random matrix S \u2208 R m\u00d7n with i.i.d. standard Gaussian entries, or somewhat more generally, sketch matrices based on i.i.d. sub-Gaussian rows. In particular, a zero-mean random vector s \u2208 R n is 1-sub-Gaussian if for any u \u2208 R n , we have\nP[ s, u \u2265 u 2 \u2264 e \u2212 2 /2 for all \u2265 0.(3)\nFor instance, a vector with i.i.d. N (0, 1) entries is 1-sub-Gaussian, as is a vector with i.i.d.\n\nRademacher entries (uniformly distributed over {\u22121, +1}). We use the terminology sub-Gaussian sketch to mean a random matrix S \u2208 R m\u00d7n with i.i.d. rows that are zero-mean, 1-sub-Gaussian, and with cov(s) = I n . From a theoretical perspective, sub-Gaussian sketches are attractive because of the wellknown concentration properties of sub-Gaussian random matrices (e.g., [5,24]). On the other hand, from a computational perspective, a disadvantage of sub-Gaussian sketches is that they require matrix-vector multiplications with unstructured random matrices. In particular, given a data matrix A \u2208 R n\u00d7d , computing its sketched version SA requires O(mnd) basic operations in general (using classical matrix multiplication).\n\nSketches based on randomized orthonormal systems (ROS): The second type of randomized sketch we consider is randomized orthonormal system (ROS), for which matrix multiplication can be performed much more efficiently. In order to define a ROS sketch, we first let H \u2208 R n\u00d7n be an orthonormal matrix with entries\nH ij \u2208 [\u2212 1 \u221a n , 1 \u221a n ].\nStandard classes of such matrices are the Hadamard or Fourier bases, for which matrix-vector multiplication can be performed in O(n log n) time via the fast Hadamard or Fourier transforms, respectively. Based on any such matrix, a sketching matrix S \u2208 R m\u00d7n from a ROS ensemble is obtained by sampling i.i.d. rows of the form s T = \u221a ne T j HD with probability 1/n for j = 1, . . . , n,\n\nwhere the random vector e j \u2208 R n is chosen uniformly at random from the set of all n canonical basis vectors, and D = diag(\u03bd) is a diagonal matrix of i.i.d. Rademacher variables \u03bd \u2208 {\u22121, +1} n . Given a fast routine for matrix-vector multiplication, the sketch SM for a data matrix M \u2208 R n\u00d7d can be formed in O(n d log m) time (for instance, see the papers [2,1]).\n\nSketches based on random row sampling: Given a probability distribution {p j } n j=1 over [n] = {1, . . . , n}, another choice of sketch is to randomly sample the rows of a data matrix M a total of m times with replacement from the given probability distribution. Thus, the rows of S are independent and take on the values s T = e j \u221a p j with probability p j for j = 1, . . . , n where e j \u2208 R n is the j th canonical basis vector. Different choices of the weights {p j } n j=1 are possible, including those based on the row 2 norms p j \u221d M e j 2 2 and leverage values of M -i.e., p j \u221d U e j 2 for j = 1, . . . , n, where U \u2208 R n\u00d7d is the matrix of left singular vectors of M [6]. When M \u2208 R n\u00d7d is the adjacency matrix of a graph with d vertices and n edges, the leverage scores of M are also known as effective resistances which can be used to sub-sample edges of a given graph by preserving its spectral properties [22].\n\n\nGaussian widths\n\nIn this section, we introduce some background on the notion of Gaussian width, a way of measuring the size of a compact set in R d . These width measures play a key role in the analysis of randomized sketches. Given a compact subset L \u2286 R d , its Gaussian width is given by\nW(L) : = E g max z\u2208L | g, z |(4)\nwhere g \u2208 R n is an i.i.d. sequence of N (0, 1) variables. This complexity measure plays an important role in Banach space theory, learning theory and statistics (e.g., [21,12,3]).\n\nOf particular interest in this paper are sets L that are obtained by intersecting a given cone K with the Euclidean sphere S d\u22121 = {z \u2208 R n | z 2 = 1}. It is easy to show that the Gaussian width of any such set is at most \u221a d, but the it can be substantially smaller, depending on the nature of the underlying cone. For instance, if K is a subspace of dimension r < d, then a simple calculation yields that W(K \u2229 S d\u22121 ) \u2264 \u221a r.\n\n\nNewton sketch and local convergence\n\nWith the basic background in place, let us now introduce the Newton sketch algorithm, and then develop a number of convergence guarantees associated with it. It applies to an optimization problem of the form min x\u2208C f (x), where f : R d \u2192 R is a twice-differentiable convex function, and C \u2286 R d is a convex constraint set.\n\n\nNewton sketch algorithm\n\nIn order to motivate the Newton sketch algorithm, recall the standard form of Newton's algorithm: given a current iteratex t \u2208 C, it generates the new iteratex t+1 by performing a constrained minimization of the second order Taylor expansion-viz.\n\nx t+1 = arg min\nx\u2208C 1 2 x \u2212x t , \u2207 2 f (x t ) (x \u2212x t ) + \u2207f (x t ), x \u2212x t .(5a)\nIn the unconstrained case-that is, when C = R d -it takes the simpler form\nx t+1 =x t \u2212 \u2207 2 f (x t ) \u22121 \u2207f (x t ) .(5b)\nNow suppose that we have available a Hessian matrix square root\n\u2207 2 f (x) 1/2 -that is, a matrix \u2207 2 f (x) 1/2 of dimensions n \u00d7 d such that (\u2207 2 f (x) 1/2 ) T \u2207 2 f (x) 1/2 = \u2207 2 f (x)\nfor some integer n \u2265 rank(\u2207 2 f (x)).\n\nIn many cases, such a matrix square root can be computed efficiently. For instance, consider a function of the form f (x) = g(Ax) where A \u2208 R n\u00d7d , and the function g : R n \u2192 R has the separable form g(Ax) = n i=1 g i ( a i , x ). In this case, a suitable Hessian matrix square root is given by the n \u00d7 d matrix\n\u2207 2 f (x) 1/2 : = diag g i ( a i , x ) n i=1\nA. In Section 3.2, we discuss various concrete instantiations of such functions.\n\nIn terms of this notation, the ordinary Newton update can be re-written as\nx t+1 = arg min x\u2208C 1 2 \u2207 2 f (x t ) 1/2 (x \u2212x t ) 2 2 + \u2207f (x t ), x \u2212x t \u03a6 (x)\n, and the Newton Sketch algorithm is most easily understood based on this form of the updates. More precisely, for a sketch dimension m to be chosen, let S \u2208 R m\u00d7n be an isotropic sketch matrix, satisfying the relation E[S T S] = I n . The Newton Sketch algorithm generates a sequence of iterates {x t } \u221e t=0 according to the recursion\nx t+1 : = arg min x\u2208C 1 2 S t \u2207 2 f (x t ) 1/2 (x \u2212 x t ) 2 2 + \u2207f (x t ), x \u2212 x t \u03a6(x;S t ) ,(6)\nwhere S t \u2208 R m\u00d7d is an independent realization of a sketching matrix. When the problem is unconstrained, i.e., C = R d and the matrix \u2207 2 f (x t ) 1/2 (S t ) T S t \u2207 2 f (x t ) 1/2 is invertible, the Newton sketch update takes the simpler form to\nx t+1 = x t \u2212 \u2207 2 f (x t ) 1/2 (S t ) T S t \u2207 2 f (x t ) 1/2 \u22121 \u2207f (x t ).(7)\nThe intuition underlying the Newton sketch updates is as follows: the iterate x t+1 corresponds to the constrained minimizer of the random objective function \u03a6(x; S t ) whose expectation E[\u03a6(x; S t )], taking averages over the isotropic sketch matrix S t , is equal to the original Newton objective\u03a6(x). Consequently, it can be seen as a stochastic form of the Newton update.\n\nIn this paper, we also analyze a partially sketched Newton update, which takes the following form. Given an additive decomposition of the form f = f 0 + g, we perform a sketch of of the Hessian \u2207 2 f 0 while retaining the exact form of the Hessian \u2207 2 g. This leads to the partially sketched update\nx t+1 : = arg min x\u2208C 1 2 (x \u2212 x t ) T Q t (x \u2212 x t ) + \u2207f (x t ), x \u2212 x t (8) where Q t : = (S t \u2207 2 f 0 (x t ) 1/2 ) T S t \u2207 2 f 0 (x t ) 1/2 + \u2207 2 g(x t ).\nFor either the fully sketched (6) or partially sketched updates (8), our analysis shows that there are many settings in which the sketch dimension m can be chosen to be substantially smaller than n, in which cases the sketched Newton updates will be much cheaper than a standard Newton update. For instance, the unconstrained update (7) can be computed in at most O(md 2 ) time, as opposed to the O(nd 2 ) time of the standard Newton update. In constrained settings, we show that the sketch dimension m can often be chosen even smallereven m d-which leads to further savings.\n\n\nSome examples\n\nIn order to provide some intuition, let us provide some simple examples to which the sketched Newton updates can be applied. \n\nwhere A \u2208 R n\u00d7d is a given constraint matrix. We assume that the polytope {x \u2208 R d | Ax \u2264 b} is bounded so that the minimum achieved. A barrier method approach to this LP is based on solving a sequence of problems of the form\nmin x\u2208R d \u03c4 c, x \u2212 n i=1 log(b i \u2212 a i , x ) f (x) ,\nwhere a i \u2208 R d denotes the i th row of A, and \u03c4 > 0 is a weight parameter that is adjusted during the algorithm. By inspection, the function f : R d \u2192 R \u222a {+\u221e} is twice-differentiable, and its Hessian is given by\n\u2207 2 f (x) = A T diag 1 (b i \u2212 a i , x ) 2 A. A Hessian square root is given by \u2207 2 f (x) 1/2 : = diag 1 |b i \u2212 a i , x | A,\nwhich allows us to compute a sketched version of the Hessian square root\nS\u2207 2 f (x) 1/2 = S diag 1 |b i \u2212 a i , x | A.\nWith a ROS sketch matrix, computing this matrix requires O(nd log(m)) basic operations. The complexity of each Newton sketch iteration scales as O(md 2 ), where m is at most d. In contrast, the standard unsketched form of the Newton update has complexity O(nd 2 ), so that the sketched method is computationally cheaper whenever there are more constraints than dimensions (n > d).\n\nBy increasing the barrier parameter \u03c4 , we obtain a sequence of solutions that approach the optimum to the LP, which we refer to as the central path. As a simple illustration, Figure 1 compares the central paths generated by the ordinary and sketched Newton updates for a polytope defined by n = 32 constraints in dimension d = 2. Each row shows three independent trials of the method for a given sketch dimension m; the top, middle and bottom rows correspond to sketch dimensions m \u2208 {d, 4d, 16d} respectively. Note that as the sketch dimension m is increased, the central path taken by the sketched updates converges to the standard central path.\n\nAs a second example, we consider the problem of maximum likelihood estimation for generalized linear models.\n\nExample 2 (Newton sketch for maximum likelihood estimation). The class of generalized linear models (GLMs) is used to model a wide variety of prediction and classification problems, in which the goal is to predict some output variable y \u2208 Y on the basis of a covariate vector a \u2208 R d . it includes as special cases the standard linear Gaussian model (in which Y = R), as well as logistic models for classification (in which Y = {\u22121, +1}), as well as as Poisson models for count-valued responses (in which Y = {0, 1, 2, . . .}). See the book [14] for further details and applications.\n\nGiven a collection of n observations {(y i , a i )} n i=1 of response-covariate pairs from some GLM, the problem of constrained maximum likelihood estimation be written in the form\nmin x\u2208C n i=1 \u03c8( a i , x , y i ) ,(10)\nwhere \u03c8 : R \u00d7 Y \u2192 R is a given convex function, and C \u2282 R d is a convex constraint set, chosen by the user to enforce a certain type of structure in the solution. Important special cases of GLMs include the linear Gaussian model, in which \u03c8(u, y) = 1 2 (y \u2212 u) 2 , and the problem (10) Exact Newton Newton Sketch\n(b) sketch size m = 4d\nTrial 1 Trial 2 Trial 3\n\n\nExact Newton Newton Sketch\n\n(c) sketch size m = 16d corresponds to a regularized form of least-squares, as well as the problem of logistic regression, obtained by setting \u03c8(u, y) = log(1 + exp(\u2212yu)).\n\nLetting A \u2208 R n\u00d7d denote the data matrix with a i \u2208 R d as its i th row, the Hessian of the objective (10) takes the form\n\u2207 2 f (x) = A T diag \u03c8 (a T i x) n i=1 A\nSince the function \u03c8 is convex, we are guaranteed that \u03c8 (a T i x) \u2265 0, and hence the quantity diag \u03c8 (a T i x) 1/2 A can be used as an n \u00d7 d matrix square-root. We return to explore this\n\nclass of examples in more depth in Section 5.1.\n\n\nLocal convergence analysis using strong convexity\n\nReturning now to the general setting, we now begin by proving a local convergence guarantee for the sketched Newton updates. In particular, this theorem provides insight into how large the sketch dimension m must be in order to guarantee good local behavior of the sketched Newton algorithm. This choice of sketch dimension is determined by geometry of the problem, in particular in terms of the tangent cone defined by the optimum. Given a constraint set C and the minimizer x * : = arg min x\u2208C f (x), the tangent cone at x * is given by\nK : = \u2206 \u2208 R d | x * + t\u2206 \u2208 C for some t > 0 .(11)\nRecalling the definition of the Gaussian width from Section 2.3, our first main result requires the sketch dimension to satisfy a lower bound of the form\nm \u2265 c 2 max x\u2208C W 2 (\u2207 2 f (x) 1/2 K),(12)\nwhere \u2208 (0, 1) is a user-defined tolerance, and c is a universal constant. Since the Hessian square-root \u2207 2 f (x) 1/2 has dimensions n \u00d7 d, this squared Gaussian width is at at most min{n, d}. This worst-case bound is achieved for an unconstrained problem (in which case K = R d ), but the Gaussian width can be substantially smaller for constrained problems. See the example following Theorem 1 for an illustration. In addition to this Gaussian width, our analysis depends on the cone-constrained eigenvalues of the Hessian \u2207 2 f (x * ), which are defined as\n\u03b3 = inf z\u2208K\u2229S d\u22121 z, \u2207 2 f (x * ))z , and \u03b2 = sup z\u2208K\u2229S d\u22121 z, \u2207 2 f (x * ))z ,(13)\nIn the unconstrained case (C = R d ), we have K = R d , and so that \u03b3 and \u03b2 reduce to the minimum and maximum eigenvalues of the Hessian \u2207 2 f (x * ). In the classical analysis of Newton's method, these quantities measure the strong convexity and smoothness parameters of the function f . With this set-up, the following theorem is applicable to any twice-differentiable objective f with cone-constrained eigenvalues (\u03b3, \u03b2) defined in equation (13), and with Hessian that is L-Lipschitz continuous, as defined in equation (2).\n\nTheorem 1 (Local convergence of Newton Sketch). For given parameters \u03b4, \u2208 (0, 1), consider the Newton sketch updates (6) based on an initialization x 0 such that x 0 \u2212 x * 2 \u2264 \u03b4 \u03b3 8L , and a sketch dimension m satisfying the lower bound (12). Then with probability at least 1 \u2212 c 1 e \u2212c 2 m , the 2 -error satisfies the recursion\nx t+1 \u2212 x * 2 \u2264 \u03b2 \u03b3 x t \u2212 x * 2 + 4L \u03b3 x t \u2212 x * 2 2 .(14)\nThe bound (14) shows that when is set to a fixed constant-say = 1/4-the algorithm displays a linear-quadratic convergence rate in terms of the error \u2206 t = x t \u2212 x * . More specifically, the rate is initially quadratic-that is, \u2206 t+1 2 \u2248 4L \u03b3 \u2206 t 2 2 when \u2206 t 2 is large. However, as the iterations progress and \u2206 t 2 becomes substantially less than 1, then the rate becomes linear-meaning that \u2206 t+1\n2 \u2248 \u03b2 \u03b3 \u2206 t 2 -since the term 4L \u03b3 \u2206 t 2 2 becomes negligible compared to \u03b2 \u03b3 \u2206 t 2 .\nIf we perform N steps in total, the linear rate guarantees the conservative error bounds\nx N \u2212 x * 2 \u2264 \u03b3 8L 1 2 + \u03b2 \u03b3 N , and f (x N ) \u2212 f (x * ) \u2264 \u03b2\u03b3 8L 1 2 + \u03b2 \u03b3 N .(15)\nA notable feature of Theorem 1 is that, depending on the structure of the problem, the linear-quadratic convergence can be obtained using a sketch dimension m that is substantially smaller than min{n, d}. As an illustrative example, we performed simulations for some instantiations of a portfolio optimization problem: it is a linearly-constrained quadratic program of the form\nmin x\u22650 d j=1 x j =1 1 2 x T A T Ax \u2212 c, x ,(16)\nwhere A \u2208 R n\u00d7d and c \u2208 R d are empirically estimated matrices and vectors (see Section 5.3 for more details). We used the Newton sketch to solve different sizes of this problem d \u2208 {10, 20, 30, 40, 50, 60}, and with n = d 3 in each case. Each problem was constructed so that the optimum x * had at most s = 2 log(d) non-zero entries. A calculation of the Gaussian width for this problem (see Appendix C for the details) shows that it suffices to take a sketch dimension m s log d, and we implemented the algorithm with this choice.    (16). In all cases, the algorithm was implemented using a sketch dimension m = 4s log d , where s is an upper bound on the number of non-zeros in the optimal solution x * ; this quantity satisfies the required lower bound (12), and consistent with the theory, the algorithm displays linear convergence.\n\nshows the convergence rate of the Newton sketch algorithm for the six different problem sizes: consistent with our theory, the sketch dimension m min{d, n} suffices to guarantee linear convergence in all cases.\n\nIt is also possible obtain an asymptotically super-linear rate by using an iteration-dependent sketching accuracy = (t). The following corollary summarizes one such possible guarantee: Corollary 1. Consider the Newton sketch iterates using the iteration-dependent sketching accuracy (t) = 1 log(1+t) . Then with the same probability as in Theorem 1, we have\nx t+1 \u2212 x * 2 \u2264 1 log(1 + t) \u03b2 \u03b3 x t \u2212 x * 2 + 4L \u03b3 x t \u2212 x * 2 2 ,\nand consequently, super-linear convergence is obtained-namely, lim t\u2192\u221e\nx t+1 \u2212x * 2 x t \u2212x * 2 = 0.\nNote that the price for this super-linear convergence is that the sketch size is inflated by the factor \u22122 (t) = log 2 (1 + t), so it is only logarithmic in the iteration number.\n\n\nNewton sketch for self-concordant functions\n\nThe analysis and complexity estimates given in the previous section involve the curvature constants (\u03b3, \u03b2) and the Lipschitz constant L, which are seldom known in practice. Moreover, as with the analysis of classical Newton method, the theory is local, in that the linear-quadratic convergence takes place once the iterates enter a suitable basin of the origin.\n\nIn this section, we seek to obtain global convergence results that do not depend on unknown problem parameters. As in the classical analysis, the appropriate setting in which to seek such results is for self-concordant functions, and using an appropriate form of backtracking line search. We begin by analyzing the unconstrained case, and then discuss extensions to constrained problems with self-concordant barriers. In each case, we show that given a suitable lower bound on the sketch dimension, the sketched Newton updates can be equipped with global convergence guarantees that hold with exponentially high probability. Moreover, the total number of iterations does not depend on any unknown constants such as strong convexity and Lipschitz parameters.\n\n\nUnconstrained case\n\nIn this section, we consider the unconstrained optimization problem min x\u2208R d f (x), where f is a closed convex self-concordant function which is bounded below. Note that a closed convex\nfunction \u03c6 : R \u2192 R is self-concordant if |\u03c6 (x)| \u2264 2 \u03c6 (x) 3/2 .(17)\nThis definition can be extended to a function f : R d \u2192 R by imposing this requirement on the univariate functions \u03c6 x,y (t) : = f (x + ty), for all choices of x, y in the domain of f . Examples of self-concordant functions include linear and quadratic functions and negative logarithm. Self concordance is preserved under addition and affine transformations. Our main result provide a bound on the total number of Newton sketch iterations required to obtain a \u03b4-accurate solution without imposing any sort of initialization condition (as was done in our previous analysis). This bound scales proportionally to log(1/\u03b4) and inversely in a parameter \u03bd that depends on sketching accuracy \u2208 (0, 1 4 ) and backtracking parameters (a, b) via \n\u03bd = ab \u03b7 2 1 + ( 1+ 1\u2212 )\u03b7 where \u03b7 = 1 8 1 \u2212 1 2 ( 1+ 1\u2212 ) 2 \u2212 a ( 1+ 1\u2212 ) 3 .(18)\u2206x t : = arg min \u2206 \u2207f (x t ), \u2206 + 1 2 S t (\u2207 2 f (x t )) 1/2 \u2206 2 2 ; \u03bb f (x t ) : = \u2207f (x) T \u2206x t . 2: Quit if\u03bb(x t ) 2 /2 \u2264 \u03b4. 3: Line search: choose \u00b5 : while f (x t + \u00b5\u2206x t ) > f (x t ) + a\u00b5\u03bb(x t ), \u00b5 \u2190 b\u00b5 4: Update: x t+1 = x t + \u00b5\u2206x t Output: minimizer x t , optimality gap \u03bb(x t )\nTheorem 2. Let f be a strictly convex self-concordant function. Given a sketching matrix S \u2208 R m\u00d7n with m \u2265 c 3 2 max x\u2208C rank(\u2207 2 f (x)) = c 3 2 d, the number of total iterations T for obtaining an \u03b4 approximate solution in function value via Algorithm 1 is bounded by\nT = f (x 0 ) \u2212 f (x * ) \u03bd + 0.65 log 2 ( 1 16\u03b4 ) , with probability at least 1 \u2212 c 1 N e \u2212c 2 m .\nThe bound in the above theorem shows that the convergence of the Newton Sketch is independent of the properties of the function f and problem parameters, similar to classical Newton's method. Note that for problems with n > d, the complexity of each Newton sketch step is at most O(d 3 + nd log d), which is smaller than that of Newton's Method (O(nd 2 )), and also smaller than typical first-order optimization methods (O(nd)) whenever n > d 2 .\n\n\nNewton Sketch with self-concordant barriers\n\nWe now turn to the more general constrained case. Given a closed, convex self-concordant function f 0 : R d \u2192 R, let C be a convex subset of R d , and consider the constrained optimization problem min x\u2208C f 0 (x). If we are given a convex self-concordant barrier function g for the constraint set C, it is equivalent to consider the unconstrained problem\nmin x\u2208R d f 0 (x) + g(x) f (x) .\nOne way in which to solve this unconstrained problem is by sketching the Hessian of both f 0 and g, in which case the theory of the previous section is applicable. However, there are many cases in which the constraints describing C are relatively simple, and so the Hessian of g is highly-structured. For instance, if the constraint set is the usual simplex (i.e., x \u2265 0 and 1, x \u2264 1), then the Hessian of the associated log barrier function is a diagonal matrix plus a rank one matrix. Other examples include problems for which g has a separable structure; such functions frequently arise as regularizers for ill-posed inverse problems. Examples of such regularizers include 2 regularization g(x) = 1 2 x 2 2 , graph regularization g(x) = 1 2 i,j\u2208E (x i \u2212 x j ) 2 induced by an edge set E (e.g., finite differences) and also other differentiable norms\ng(x) = d i=1 x p i 1/p for 1 < p < \u221e.\nIn all such cases, an attractive strategy is to apply a partial Newton sketch, in which we sketch the Hessian term \u2207 2 f 0 (x) and retain the exact Hessian \u2207 2 g(x), as in the previously described updates (8). More formally, Algorithm 2 provides a summary of the steps, including the choice of the line search parameters. The main result of this section provides a guarantee on this algorithm, assuming that the sequence of sketch dimensions {m t } \u221e t=0 is appropriately chosen.\n\n\nAlgorithm 2 Newton Sketch with self-concordant barriers\n\nInput: Starting point x 0 , constraint C, corresponding barrier function g such that f = f 0 + g, tolerance \u03b4 > 0, (\u03b1, \u03b2)\n\nline-search parameters, sketching matrices S t \u2208 R m\u00d7n .\n\n1: Compute approximate Newton step \u2206x t and approximate Newton decrement \u03bb(x).\n\n\u2206x t : = arg min\nx t +\u2206\u2208C \u2207f (x t ), \u2206 + 1 2 S t (\u2207 2 f 0 (x t )) 1/2 \u2206 2 2 + 1 2 \u2206 T \u2207 2 g(x t )\u2206; \u03bb f (x t ) : = \u2207f (x) T \u2206x t 2: Quit if\u03bb(x t ) 2 /2 \u2264 \u03b4. 3: Line search: choose \u00b5 : while f (x t + \u00b5\u2206x t ) > f (x t ) + \u03b1\u00b5\u03bb(x t ), \u00b5 \u2190 \u03b2\u00b5. 4: Update: x t+1 = x t + \u00b5\u2206x t .\nOutput: minimizer x t , optimality gap \u03bb(x t ).\n\nThe choice of sketch dimensions depends on the tangent cones defined by the iterates, namely the sets\nK t : = \u2206 \u2208 R d | x t + \u03b1\u2206 \u2208 C for some \u03b1 > 0 .\nFor a given sketch accuracy \u2208 (0, 1), we require that the sequence of sketch dimensions satisfies the lower bound\nm t \u2265 c 3 2 max x\u2208C W 2 (\u2207 2 f (x) 1/2 K t ).(19)\nFinally, the reader should recall the parameter \u03bd was defined in equation (18), which depends only on the sketching accuracy and the line search parameters. Given this set-up, we have the following guarantee:\n\nTheorem 3. Let f : R d \u2192 R be a convex and self-concordant function, and let g : R d \u2192 R \u222a {+\u221e} be a convex and self-concordant barrier for the convex set C. Suppose that we implement Algorithm 2 with sketch dimensions {m t } t\u22650 satisfying the lower bound (19). Then taking\nN = f (x 0 ) \u2212 f (x * ) \u03bd + 0.65 log 2 ( 1 16\u03b4 ) iterations,\nsuffices to obtain \u03b4-approximate solution in function value with probability at least 1\u2212c 1 N e \u2212c 2 m .\n\nThus, we see that the Newton Sketch method can also be used with self-concordant barrier functions, which considerably extends its scope. Section 5.5 provides a numerical illustration of its performance in this context. As we discuss in the next section, there is a flexibility in choosing the decomposition f 0 and g corresponding to objective and barrier, which enables us to also sketch the constraints.\n\n\nSketching with interior point methods\n\nIn this section, we discuss the application of Newton Sketch to a form of barrier or interior point methods. In particular we discuss two different strategies and provide rigorous worstcase complexity results when the functions in the objective and constraints are self-concordant.\n\n\nAlgorithm 3 Interior point methods using Newton Sketch\n\nInput: Strictly feasible starting point x 0 , initial parameter \u03c4 0 s.t. \u03c4 := \u03c4 0 > 0, \u00b5 > 1, tolerance \u03b4 > 0. More precisely, let us consider a problem of the form\nmin x\u2208R d f 0 (x) subject to g j (x) \u2264 0 for j = 1, . . . , r,(20)\nwhere f 0 and {g j } r j=1 are twice-differentiable convex functions. We assume that there exists a unique solution x * to the above problem.\n\nThe barrier method for computing x * is based on solving a sequence of problems of the form\nx(\u03c4 ) : = arg min x\u2208R d \u03c4 f 0 (x) \u2212 r j=1 log(\u2212g j (x)) ,(21)\nfor increasing values of the parameter \u03c4 \u2265 1. The family of solutions { x(\u03c4 )} \u03c4 \u22651 trace out what is known as the central path. A standard bound (e.g., [4]) on the sub-optimality of x(\u03c4 ) is given by\nf 0 ( x(\u03c4 )) \u2212 f 0 (x * ) \u2264 r \u03c4 .\nThe barrier method successively updates the penalty parameter \u03c4 and also the starting points supplied to Newton's method using previous solutions. Since Newton's method lies at the heart of the barrier method, we can obtain a fast version by replacing the exact Newton minimization with the Newton sketch. Algorithm 3 provides a precise description of this strategy. As noted in Step 1, there are two different strategies in dealing with the convex constraints g j (x) \u2264 0 for j = 1, . . . , r:\n\n\u2022 Full sketch: Sketch the full Hessian of the objective function (21) using Algorithm 1 ,\n\n\u2022 Partial sketch: Sketch only the Hessians corresponding to a subset of the functions {f 0 , g j , j = 1, . . . , r}, and use exact Hessians for the other functions. Apply Algorithm 2.\n\nAs shown by our theory, either approach leads to the same convergence guarantees, but the associated computational complexity can vary depending both on how data enters the objective and constraints, as well as the Hessian structure arising from particular functions. The following theorem is an application of the classical results on the barrier method tailored for Newton Sketch using any of the above strategies (see e.g., [4]). As before, the key parameter \u03bd was defined in Theorem 2.\n\nTheorem 4 (Newton Sketch complexity for interior point methods). For a given target accuracy \u03b4 \u2208 (0, 1) and any \u00b5 > 1, the total number of Newton Sketch iterations required to obtain a \u03b4-accurate solution using Algorithm 3 is at most\nlog (r/(\u03c4 0 \u03b4) log \u00b5 r(\u00b5 \u2212 1 \u2212 log \u00b5) \u03b3 + 0.65 log 2 (1 16\u03b4\n) .\n\nIf the parameter \u00b5 is set to minimize the above upper-bound, the choice \u00b5 = 1+ 1 r yields O( \u221a r) iterations. However, when applying the standard Newton method, this \"optimal\" choice is typically not used in practice: instead, it is common to use a fixed value of \u00b5 \u2208 [2, 100]. In experiments, experience suggests that the number of Newton iterations needed is a constant independent of r and other parameters. Theorem 4 allows us to obtain faster interior point solvers with rigorous worst-case complexity results. We show different applications of Algorithm 3 in the following section.\n\n\nApplications and numerical results\n\nIn this section, we discuss some applications of the Newton sketch to different optimization problems. In particular, we show various forms of Hessian structure that arise in applications, and how the Newton sketch can be computed. When the objective and/or the constraints contain more than one term, the barrier method with Newton Sketch has some flexibility in sketching. We discuss the choices of partial Hessian sketching strategy in the barrier method. It is also possible to apply the sketch in the primal or dual form, and we provide illustrations of both strategies here.\n\n\nEstimation in generalized linear models\n\nRecall the problem of (constrained) maximum likelihood estimation for a generalized linear model, as previously introduced in Example 2. It leads to the family of optimization problems (10): here \u03c8 : R \u2192 R is a given convex function arising from the probabilistic model, and C \u2286 R d is a closed convex set that is used to enforce a certain type of structure in the solution, Popular choices of such constraints include 1 -balls (for enforcing sparsity in a vector), nuclear norms (for enforcing low-rank structure in a matrix), and other non-differentiable semi-norms based on total variation (e.g., d\u22121 j=1 |x j+1 \u2212 x j |), useful for enforcing smoothness or clustering constraints.\n\nSuppose that we apply the Newton sketch algorithm to the optimization problem (10). Given the current iterate x t , computing the next iterate x t+1 requires solving the constrained quadratic program\nmin x\u2208C 1 2 Sdiag \u03c8 ( a i , x t , y i ) 1/2 A(x \u2212 x t ) 2 2 + n i=1 x, \u03c8 ( a i , x t , y i ) .(23)\nWhen the constraint C is a scaled version of the 1 -ball-that is, C = {x \u2208 R d | x 1 \u2264 R} for some radius R > 0-the convex program (23) is an instance of the Lasso program [23], for which there is a very large body of work. For small values of R, where the cardinality of the solution x is very small, an effective strategy is to apply a homotopy type algorithm, also known as LARS [7,9], which solves the optimality conditions starting from R = 0. For other sets C, another popular choice is projected gradient descent, which is efficient when projection onto C is computationally simple. Focusing on the 1 -constrained case, let us consider the problem of choosing a suitable sketch dimension m. Our choice involves the 1 -restricted minimal eigenvalue of the data matrix A T A, which is given by\n\u03b3 \u2212 s (A) : = min z 2 =1 z 1 \u22642 \u221a s Az 2 2 .(24)\nNote that we are always guaranteed that \u03b3 \u2212 s (A) \u2265 \u03bb min (A T A). It also involves certain quantities that depend on the function \u03c8, namely \u03c8 min : = min x\u2208C min i=1,...,n \u03c8 ( a i , x , y i ), and \u03c8 max : = max\nx\u2208C max i=1,...,n \u03c8 ( a i , x , y i ),\nwhere a i \u2208 R d is the i th row of A. With this set-up, supposing that the optimal solution x * has cardinality at most x * 0 \u2264 s, then it can be shown (see Lemma 8 in Appendix C) that it suffices to take a sketch size\nm = c 0 \u03c8 max \u03c8 min max j=1,...,d A j 2 2 \u03b3 \u2212 s (A) s log d,(25)\nwhere c 0 is a universal constant. Let us consider some examples to illustrate:\n\n\u2022 Least-Squares regression: \u03c8(u) = 1 2 u 2 , \u03c8 (u) = 1 and \u03c8 min = \u03c8 max = 1. For typical distributions of the data matrices, the sketch size choice given in equation (25) is O(s log d). As an example, consider data matrices A \u2208 R n\u00d7d where each row is independently sampled from a sub-Gaussian distribution with variance 1. Then standard results on random matrices [24] show that \u03b3 \u2212 s (A) > 1/2 as long as n > c 1 s log d for a sufficiently large constant c 1 . In addition, we have max  (n)). For such problems, the per iteration complexity of Newton Sketch update scales as O(s 2 d log 2 (d)) using standard Lasso solvers (e.g., [11]) or as O(sd log(d)) using projected gradient descent. Both of these scalings are substantially smaller than conventional algorithms that fail to exploit the small intrinsic dimension of the tangent cone.\n\n\nSemidefinite programs\n\nThe Newton sketch can also be applied to semidefinite programs. As one illustration, let us consider the metric learning problem studied in machine learning. Given feature vectors a 1 , . . . a n \u2208 R d and corresponding indicator y ij \u2208 {\u22121, +1} n where y ij = +1 if a i and a j belong to the same label and y ij = \u22121 otherwise for all i = j and 1 \u2264 i, j \u2264 n. The task is to learn a positive semidefinite matrix X which represents a metric such that the semi-norm a X : = a, Xa establishes a nearness measure depending on class label. Using 2 -loss, the optimization can be stated as the following semi-definite program (SDP)\nmin X 0 ( n 2 ) i =j X, (a i \u2212 a j )(a i \u2212 a j ) T \u2212 y ij 2 + \u03bb trace(X) .\nHere the term trace(X), along with its multiplicative pre-factor \u03bb > 0 that can be adjusted by the user, is a regularization term for encouraging a relatively low-rank solution. Using the standard self-concordant barrier X \u2192 log det(X) for the PSD cone, the barrier method involves solving a sequence of sub-problems of the form\nmin X\u2208R d\u00d7d \u03c4 n i=1 ( X, a i a T i \u2212 y i ) 2 + \u03c4 \u03bb trace X \u2212 log det (X) f (vec(X))\n. Now the Hessian of the function vec(X) \u2192 f (vec(X)) is a d 2 \u00d7 d 2 matrix given by\n\u2207 2 f vec(X) = \u03c4 ( n 2 ) i =j vec(A ij )vec(A ij ) T + X \u22121 \u2297 X \u22121 , where A ij : = (a i \u2212 a j )(a i \u2212 a j ) T .\nThen we can apply the barrier method with partial Hessian sketch on the first term, {S ij vec(A ij )} i =j and exact Hessian for the second term. Since the vectorized decision variable is vec(X) \u2208 R d 2 the complexity of Newton Sketch is O(m 2 d 2 ) while the complexity of a classical SDP interior-point solver is O(nd 4 ).\n\n\nPortfolio optimization and SVMs\n\nHere we consider the Markowitz formulation of the portfolio optimization problem [13]. The objective is to find x \u2208 R d belonging to the unit simplex, which corresponds to non-negative weights associated with each of d possible assets, so as to maximize the expected return minus a coefficient times the variance of the return. Letting \u00b5 \u2208 R d denote a vector corresponding to mean return of the assets, and we let \u03a3 \u2208 R d\u00d7d be a symmetric, positive semidefinite matrix, covariance of the returns. The optimization problem is given by\nmax x\u22650, d j=1 x j \u22641 \u00b5, x \u2212 \u03bb x T \u03a3x .(26)\nThe covariance of returns is often estimated from past stock data via empirical covariance, \u03a3 = A T A where the columns of A are time series corresponding to assets normalized by \u221a n, where n is the length of the observation window.\n\nThe barrier method can be used solve the above problem by solving penalized problems of the\nmin x\u2208R d \u2212\u03c4 \u00b5 T x + \u03c4 \u03bb x T A T Ax \u2212 d i=1 log( e i , x ) \u2212 log(1 \u2212 1, x ) f (x)\n, where e i \u2208 R d is the i th element of the canonical basis and 1 is row vector of all-ones. Then the Hessian of the above barrier penalized formulation can be written as\n\u2207 2 f (x) = \u03c4 \u03bb A T A + diag x 2 i \u22121 + 11 T\nConsequently we can sketch the data dependent part of the Hessian via \u03c4 \u03bbSA which has at most rank m and keep the remaining terms in the Hessian exact. Since the matrix 11 T is rank one, the resulting sketched estimate is therefore diagonal plus rank (m + 1) where the matrix inversion lemma can be applied for efficient computation of the Newton Sketch update (see e.g. [8]). Therefore, as long as m \u2264 d, the complexity per iteration scales as O(md 2 ), which is cheaper than the O(nd 2 ) per step complexity associated with classical interior point methods. We also note that support vector machine classification problems with squared hinge loss also has the same form as in (26) (see e.g. [20]) where the same strategy can be applied.\n\n\nUnconstrained logistic regression with d n\n\nLet us now turn to some numerical comparisons of the Newton Sketch with other popular optimization methods for large-scale instances of logistic regression. More specifically, we generated a feature matrix A \u2208 R n\u00d7d based on d = 100 features and n = 16384 observations. Each row a i \u2208 R d was generated from the d-variate Gaussian distribution N (0, \u03a3) where \u03a3 ij = 2|0.99| i\u2212j . As shown in Figure 3, the convergence of the algorithm per iteration is very similar to Newton's method. Besides the original Newton's method, the other algorithms compared are\n\n\u2022 Gradient Descent (GD) with backtracking line search \u2022 Accelerated Gradient Descent (Acc. GD) adapted for strongly convex functions with manually tuned parameters.\n\n\u2022 Stochastic Gradient Descent (SGD) with the classical step size choice 1/ \u221a t \u2022 Broyden-Fletcher-Goldfarb-Shanno algorithm (BFGS) approximating the Hessian with gradients.\n\nFor each problem, we averaged the performance of the randomized algorithms (Newton sketch and SGD) over 10 independent trials. We ran the Newton sketch algorithm with sketch size m = 6d. To be fair in comparisons, we performed hand-tuning of the stepsize parameters in the gradient-based methods so as to optimize their performance. The top panel in Figure 3 plots the log duality gap versus the number of iterations: as expected, on this scale, the classical form of Newton's method is the fastest, whereas the SGD method is the slowest. However, when the log optimality gap is plotted versus the wall-clock time in the bottom panel, we now see that the Newton sketch is the fastest.\n\n\nA dual example: Lasso with d n\n\nThe regularized Lasso problem takes the form min x\u2208R d 1 2 Ax \u2212 y 2 2 + \u03bb x 1 , where \u03bb > 0 is a user-specified regularization parameter. In this section, we consider efficient sketching strategies for this class of problems in the regime d n. In particular, let us consider the corresponding dual program, given by\nmax A T w \u221e\u2264\u03bb \u2212 1 2 y \u2212 w 2 2 .\nBy construction, the number of constraints d in the dual program is larger than the number of optimization variables n. If we apply the barrier method to solve this dual formulation, then we need to solve a sequence of problems of the form\nmin w\u2208R n \u03c4 y \u2212 w 2 2 \u2212 d j=1 log(\u03bb \u2212 A j , w ) \u2212 d j=1 log(\u03bb + A j , w ) f (x) ,\nwhere A j \u2208 R n denotes the j th column of A. The Hessian of the above barrier penalized formulation can be written as  Consequently we can keep the first term in the Hessian, \u03c4 I exact and apply partial sketching to the Hessians of the last two terms via\n\u2207 2 f (w) = \u03c4 I n + Adiag 1 (\u03bb \u2212 A j , w ) 2 A T + Adiag 1 (\u03bb + A j , w ) 2 A T ,Sdiag 1 |\u03bb \u2212 A j , w | + 1 |\u03bb + A j , w | A T .\nSince the partially sketched Hessian is of the form tI n + V V T , where V is rank at most m, we can use matrix inversion lemma for efficiently calculating Newton Sketch updates. The complexity of the above strategy for d > n is O(dm 2 ), where m is at most d, whereas traditional interior point solvers are typically O(dn 2 ) per iteration. In order to test this algorithm, we generated a feature matrix A \u2208 R n\u00d7d with d = 4096 features and n = 50 observations. Each row a i \u2208 R d was generated from the multivariate Gaussian distribution N (0, \u03a3) with \u03a3 ij = 2 * |0.99| i\u2212j . For a given problem instance, we ran 10 independent trials of the sketched barrier method, and compared the results to the original barrier method. Figure 4 \n\n\nProofs\n\nWe now turn to the proofs of our theorems, with more technical details deferred to the appendices.\n\n\nProof of Theorem 1\n\nThroughout this proof, we let r \u2208 S d\u22121 denote a fixed vector that is independent of the sketch matrix S t and the current iterate x t . We then define the following pair of random variables\nZ 1 (S; x) : = sup w\u2208\u2207 2 f (x) 1/2 K\u2229S n\u22121 w, S T S \u2212 I r , Z 2 (S; x) : = inf w\u2208\u2207 2 f (x) 1/2 K\u2229S n\u22121 Sw 2 2 .\nThese random variables are significant, because the core of our proof is based on establishing that the error vector \u2206 t = x t \u2212 x * satisfies the recursive bound where Z t 1 : = Z 1 (S t ; x t ) and Z t 2 : = Z 2 (S t ; x t ). We then combine this recursion with the following probabilistic guarantee on Z t 1 and Z t 2 . For a given tolerance parameter \u2208 (0, 1 2 ], consider the \"good event\"\n\u2206 t+1 2 \u2264 6\u03b2 Z t 1 \u03b3 Z t 2 \u2206 t 2 + 4L \u03b3 Z t 2 \u2206 t 2 2 ,(27)E t : = Z t 1 \u2264 2 , and Z t 2 \u2265 1 \u2212 .(28)\nLemma 1 (Sufficient conditions on sketch dimension [20]).\n\n(a) For sub-Gaussian sketch matrices, given a sketch size m > c 0 2 max x\u2208C W 2 (\u2207 2 f (x) 1/2 K), we have\nP E t ] \u2265 1 \u2212 c 1 e \u2212c 2 m 2 .(29)\n(b) For randomized orthogonal system (ROS) sketches over the class of self-bounding cones, given a sketch size m > c 0 log 4 n 2 max x\u2208C W 2 (\u2207 2 f (x) 1/2 K), we have\nP E t ] \u2265 1 \u2212 c 1 e \u2212c 2 m 2 log 4 n .(30)\nCombining Lemma 1 with the recursion (27) and re-scaling appropriately yields the claim of the theorem. Accordingly, it remains to prove the recursion (27), and we do so via a basic inequality argument. Recall the function x \u2192 \u03a6(x; S t ) that underlies the sketch Newton update (6): since x t and x * are optimal and feasible for the constrained optimization problem, we have \u03a6(x; S t ) \u2264 \u03a6(x * ; S t ). Introducing the error vector \u2206 t : = x t \u2212x * , some straightforward algebra then then leads to the basic inequality\n1 2 S t \u2207 2 f (x t ) 1/2 \u2206 t+1 2 2 \u2264 S t \u2207 2 f (x t ) 1/2 \u2206 t+1 , S\u2207 2 f (x t ) 1/2 \u2206 t \u2212 \u2207f (x t ) \u2212 \u2207f (x * ), \u2206 t+1(31)\nLet us first upper bound the right-hand side. By using the integral form of Taylor's expansion,\nwe have \u2207f (x t ) \u2212 \u2207f (x * ), \u2206 t+1 = 1 0 \u2207 2 f (x t + u(x * \u2212 x t ))\u2206 t , \u2206 t+1 du, and hence RHS = 1 0 \u2207 2 f (x t ) 1/2 (S t ) T S t \u2207 2 f (x t ) 1/2 \u2212 \u2207 2 f (x t + u(x * \u2212 x t )) \u2206 t , \u2206 t+1 du\nBy adding and subtracting terms and then applying triangle inequality, we have the bound RHS \u2264 T 1 + T 2 , where\nT 1 : = 1 0 \u2207 2 f (x t ) 1/2 (S t ) T S t \u2212 I \u2207 2 f (x t ) 1/2 \u2206 t , \u2206 t+1 , and T 2 : = 1 0 |||\u2207 2 f (x t + u(x * \u2212 x t )) \u2212 \u2207 2 f (x t )||| op du \u2206 t 2 \u2206 t+1 2 .\nNow observe that the vector r : = \u2207 2 f (x t ) 1/2 \u2206 t is independent of the randomness in S t , whereas the vector \u2207 2 f (x t ) 1/2 \u2206 t+1 belongs to the cone \u2207 2 f (x t ) 1/2 K. Consequently, by the definition of Z 1 , we have\nT 1 \u2264 Z t 1 \u2207 2 f (x t ) 1/2 \u2206 t 2 \u2207 2 f (x t ) 1/2 \u2206 t+1 2 .(32)\nNow note that using the fact that \u03b2 controls the smoothness of the gradient and the Lipschitz continuity of Hessian we can upper bound the terms on the above right-hand side as follows\n\u2206 t , \u2207 2 f (x * )\u2206 t = \u2206 t , \u2207 2 f (x * )\u2206 t + \u2206 t , \u2207 2 f (x t ) \u2212 \u2207 2 f (x * ) \u2206 t \u2264 \u03b2 + L \u2206 t 2 \u2206 t 2 2 ,\nand similarly, \u2206 t+1 ,\n\u2207 2 f (x * )\u2206 t+1 \u2264 \u03b2 + L \u2206 t 2 \u2206 t+1 2 2 .\nCombining the above bounds with (32) we obtain\nT 1 \u2264 Z t 1 \u03b2 + L \u2206 t 2 \u2206 t+1 2 \u2206 t 2 .(33)\nOn the other hand, by the L-Lipschitz condition on the Hessian, we have\nT 2 \u2264 L \u2206 t 2 2 \u2206 t+1 2 .\nSubstituting these two bounds into our basic inequality, we have\n1 2 S t \u2207 2 f (x t ) 1/2 \u2206 t+1 2 2 \u2264 Z t 1 \u03b2 + L \u2206 t 2 \u2206 t 2 \u2206 t+1 2 + L \u2206 t 2 2 \u2206 t+1 2 .(34)\nOur final step is to lower bound the left-hand side (LHS) of this inequality. By definition of Z 2 , we have\nS t \u2207 2 f (x t ) 1/2 \u2206 t+1 2 2 Z t 2 \u2265 \u2206 t+1 , \u2207 2 f (x t )\u2206 t+1 = \u2206 t+1 , \u2207 2 f (x * )\u2206 t+1 + \u2206 t+1 , \u2207 2 f (x t ) \u2212 \u2207 2 f (x * ) \u2206 t+1 \u2265 \u03b3 \u2212 L \u2206 t 2 \u2206 t+1 2 2 .\nSubstituting this lower bound into the previous inequality (34) and then rearranging, we find that, as long as \u2206 t 2 < \u03b3 2L , we also have \u2206 t 2 < \u03b2 2L and consequently\n\u2206 t+1 2 \u2264 2Z t 1 \u03b2 + L \u2206 t 2 Z t 2 \u03b3 \u2212 L \u2206 t 2 \u2206 t 2 + 2L \u03b3 \u2212 L \u2206 t 2 Z t 2 \u2206 t 2 2 \u2264 6\u03b2 Z t 1 \u03b3 Z t 2 \u2206 t 2 + 4L \u03b3 Z t 2 \u2206 t 2 2 ,\nas claimed.\n\n\nProof of Theorem 2\n\nRecall that in this case, we assume that f is a self-concordant strictly convex function. We adopt the following notation and conventions from the book [18]. For a given x \u2208 R d , we define the pair of dual norms\nu x : = \u2207 2 f (x)u, u 1/2 , and v * x : = \u2207 2 f (x) \u22121 v, v 1/2 ,\nas well as the Newton decrement\n\u03bb f (x) = \u2207 2 f (x) \u22121 \u2207f (x), \u2207f (x) 1/2 = \u2207 2 f (x) \u22121 \u2207f (x) x = \u2207 2 f (x) \u22121/2 \u2207f (x) 2 .\nNote that \u2207 2 f (x) \u22121 is well-defined for strictly convex self-concordant functions. In terms of this notation, the exact Newton update is given by\nx \u2192 x NE : = x + v, where v NE : = arg min z\u2208C\u2212x 1 2 \u2207 2 f (x) 1/2 z 2 2 + z, \u2207f (x) \u03a6(z) ,(35)\nwhereas the Newton sketch update is given by\nx \u2192 x NSK : = x + v NSK , where v NSK : = arg min z\u2208C\u2212x 1 2 S\u2207 2 f (x) 1/2 z 2 2 + z, \u2207f (x) .(36)\nThe proof of Theorem 2 given in this section involves the unconstrained case (C = R d ), whereas the proofs of later theorems involve the more general constrained case. In the unconstrained case, the two updates take the simpler forms\nx NE = x \u2212 (\u2207 2 f (x)) \u22121 \u2207f (x), and x NSK = x \u2212 (\u2207 2 f (x) 1/2 S T S\u2207 2 f (x) 1/2 ) \u22121 \u2207f (x).\nFor a self-concordant function, the sub-optimality of the Newton iterate x NE in function value satisfies the bound\nf (x NE ) \u2212 min x\u2208R d f (x) f (x * ) \u2264 \u03bb f (x NE ) 2 .\nThis classical bound is not directly applicable to the Newton sketch update, since it involves the approximate Newton decrement \u03bb f (x) : = \u2212 \u2207f (x), v NSK , as opposed to the exact one \u03bb f (x) : = \u2212 \u2207f (x), v NE . Thus, our strategy is to prove that with high probability over the randomness in the sketch matrix, the approximate Newton decrement can be used as an exit condition.\n\nRecall the definitions (35) and (36) of the exact v NE and sketched Newton v NSK update directions, as well as the definition of the tangent cone K at x \u2208 C. Let K t be the tangent cone at x t . The following lemma provides a high probability bound on their difference: Lemma 2. Let S \u2208 R m\u00d7n be a sub-Gaussian or ROS sketch matrix, and consider any fixed vector x \u2208 C independent of the sketch matrix. If m \u2265 c 0\nW(\u2207 2 f (x) 1/2 K t ) 2 2\n, then\n\u2207 2 f (x) 1/2 (v NSK \u2212 v NE ) 2 \u2264 \u2207 2 f (x) 1/2 v NE 2 (37)\nwith probability at least 1 \u2212 c 1 e \u2212c 2 m 2 .\n\nSimilar to the standard analysis of Newton's method, our analysis of the Newton sketch algorithm is split into two phases defined by the magnitude of the decrement \u03bb f (x). In particular, the following lemma constitute the core of our proof: Lemma 3. For \u2208 (0, 1/2), there exist constants \u03bd > 0 and \u03b7 \u2208 (0, 1/16) such that:\n(a) If \u03bb f (x) > \u03b7, then f (x NSK ) \u2212 f (x) \u2264 \u2212\u03bd with probability at least 1 \u2212 c 1 e \u2212c 2 m 2 . (b) Conversely, if \u03bb f (x) \u2264 \u03b7, then \u03bb f (x NSK ) \u2264 \u03bb f (x), and (38a) \u03bb f (x NSK ) \u2264 16 25 \u03bb f (x),(38b)\nwhere both bounds hold with probability 1 \u2212 c 1 e c 2 m 2 .\n\nUsing this lemma, let us now complete the proof of the theorem, dividing our analysis into the two phases of the algorithm.\n\nFirst phase analysis: By Lemma 3(a) each iteration in the first phase decreases the function value by at least \u03bd > 0, the number of first phase iterations N 1 is at most\nN 1 : = f (x 0 ) \u2212 f (x * ) \u03bd ,\nwith probability at least 1 \u2212 N 1 c 1 e \u2212c 2 m .\n\nSecond phase analysis: Next, let us suppose that at some iteration t, the condition \u03bb f (x t ) \u2264 \u03b7 holds, so that part (b) of Lemma 3 can be applied. In fact, the bound (38a) then guarantees that \u03bb f (x t+1 ) \u2264 \u03b7, so that we may apply the contraction bound (38b) repeatedly for N 2 rounds so as to obtain that\n\u03bb f (x t+N 2 ) \u2264 16 25 N 2 \u03bb f (x t )\nwith probability 1 \u2212 N 2 c 1 e c 2 m . Since \u03bb f (x t ) \u2264 \u03b7 \u2264 1/16 by assumption, the self-concordance of f then implies that\nf (x t+k ) \u2212 f (x * ) \u2264 16 25 k 1 16 .\nTherefore, in order to ensure that and consequently for achieving f (x t+k ) \u2212 f (x * ) \u2264 , it suffices to the number of second phase iterations lower bounded as N 2 \u2265 0.65 log 2 ( 1 16 ).\n\nPutting together the two phases, we conclude that the total number of iterations N required to achieve -accuracy is at most\nN = N 1 + N 2 \u2264 f (x 0 ) \u2212 f (x * ) \u03b3 + 0.65 log 2 ( 1 16 ) ,\nand moreover, this guarantee holds with probability at least 1 \u2212 N c 1 e \u2212c 2 m 2 .\n\nThe final step in our proof of the theorem is to establish Lemma 3, and we do in the next two subsections.\n\n\nProof of Lemma 3(a)\n\nOur proof of this part is performed conditionally on the event D : = { \u03bb f (x) > \u03b7}. Our strategy is to show that the backtracking line search leads to a stepsize s > 0 such that function decrement in moving from the current iterate x to the new sketched iterate x NSK = x + sv NSK is at least\nf (x NSK ) \u2212 f (x) \u2264 \u2212\u03bd with probability at least 1 \u2212 c 1 e \u2212c 2 m .(39)\nThe outline of our proof is as follows. Defining the univariate function g(u) : = f (x+uv NSK ) and = 2 1\u2212 , we first show that u =\n1 1+(1+ ) \u03bb f (x) satisfies the bound g( u) \u2264 g(0) \u2212 a u \u03bb f (x) 2 ,(40a)\nwhich implies that u satisfies the exit condition of backtracking line search. Therefore, the stepsize s must be lower bounded as s \u2265 b u, which then implies that the updated solution x NSK = x + sv NSK satisfies the decrement bound\nf (x NSK ) \u2212 f (x) \u2264 \u2212ab \u03bb f (x) 2 1 + (1 + 2 1\u2212 ) \u03bb f (x) .(40b)\nSince \u03bb f (x) > \u03b7 by assumption and the function u \u2192 u 2 1+(1+ 2 1\u2212 )u is monotone increasing, this bound implies that inequality (39) holds with \u03bd = ab\n\u03b7 2 1+(1+ 2 1\u2212 )\u03b7 .\nIt remains to prove the claims (40a) and (40b), for which we make use of the following auxiliary lemma:\nLemma 4. For u \u2208 dom g \u2229 R + , we have the decrement bound g(u) \u2264 g(0) + u \u2207f (x), v NSK \u2212 u [\u2207 2 f (x)] 1/2 v NSK 2 \u2212 log 1 \u2212 u [\u2207 2 f (x)] 1/2 v NSK 2 . (41) provided that u [\u2207 2 f (x)] 1/2 v NSK 2 < 1.\nLemma 5. With probability at least 1 \u2212 c 1 e \u2212c 2 m , we have\n[\u2207 2 f (x)] 1/2 v NSK 2 2 \u2264 1 + 1 \u2212 2 \u03bb f (x) 2 .(42)\nThe proof of these lemmas are provided in Appendices A.2 and A.3. Using them, let us prove the claims (40a) and (40b). Recalling our shorthand : = 1+ 1\u2212 \u2212 1 = 2 1\u2212 , substituting inequality (42) into the decrement formula (41) yields\ng(u) \u2264 g(0) \u2212 u \u03bb f (x) 2 \u2212 u(1 + ) \u03bb f (x) \u2212 log(1 \u2212 u(1 + ) \u03bb f (x)) (43) = g(0) \u2212 u(1 + ) 2 \u03bb f (x) 2 + u(1 + ) \u03bb f (x) + log(1 \u2212 u(1 + ) \u03bb f (x)) + u((1 + ) 2 \u2212 1) \u03bb f (x) 2\nwhere we added and subtracted u(1 + ) 2 \u03bb f (x) 2 so as to obtain the final equality. We now prove inequality (40a). Now setting u = u : = Making use of the standard inequality \u2212u+log(1+u) \u2264 \u2212 1 2 u 2 (1+u) (for instance, see the book [4]), we find that\ng( u) \u2264 g(0) \u2212 1 2 (1 + ) 2 \u03bb f (x) 2 1 + (1 + ) \u03bb f (x) + ( 2 + 2 ) \u03bb f (x) 2 1 + (1 + ) \u03bb f (x) = g(0) \u2212 ( 1 2 \u2212 1 2 2 \u2212 ) \u03bb f (x) 2 u \u2264 g(0) \u2212 \u03b1 \u03bb f (x) 2 u,\nwhere the final inequality follows from our assumption \u03b1 \u2264 1 2 \u2212 1 2 2 \u2212 . This completes the proof of the bound (40a). Finally, the lower bound (40b) follows by setting u = b u into the decrement inequality (41).\n\n\nProof of Lemma 3(b)\n\nThe proof of this part hinges on the following auxiliary lemma:\n\nLemma 6. For all \u2208 (0, 1/2), we have\n\u03bb f (x NSK ) \u2264 (1 + )\u03bb 2 f (x) + \u03bb f (x) 1 \u2212 (1 + )\u03bb f (x) 2 , and(44a)(1 \u2212 ) \u03bb f (x NSK ) \u2264 \u03bb f (x NSK ) \u2264 (1 + )\u03bb f (x NSK ) ,(44b)\nwhere all bounds hold with probability at least 1 \u2212 c 1 e \u2212c 2 m 2 .\n\nSee Appendix A.4 for the proof.\n\nWe now use Lemma 6 to prove the two claims in the lemma statement.\n\nProof of the bound (38a): Recall from the theorem statement that \u03b7 : = 1\n8 1\u2212 1 2 ( 1+ 1\u2212 ) 2 \u2212a ( 1+ 1\u2212 ) 3 .\nBy examining the roots of a polynomial in , it can be seen that \u03b7 \u2264 1\u2212 1+ 1 16 .\n(1 + ) \u03bb f (x t ) \u2264 (1 + ) \u03bb f (x t ) \u2264 (1 + ) \u03b7 \u2264 1 16\nBy applying the inequalities (44b), we have\n(1 + )\u03bb f (x) \u2264 1 + 1 \u2212 \u03bb f (x) \u2264 1 + 1 \u2212 \u03b7 \u2264 1 16(45)\nwhence inequality (44a) implies that\n\u03bb f (x NSK ) \u2264 1 16 \u03bb f (x) + \u03bb f (x) (1 \u2212 1 16 ) 2\n\nDiscussion\n\nIn this paper, we introduced and analyzed the Newton sketch, a randomized approximation to the classical Newton updates. This algorithm is a natural generalization of the Iterative Hessian Sketch (IHS) updates analyzed in our earlier work [19]. The IHS applies only to constrained least-squares problems (for which the Hessian is independent of the iteration number), whereas the Newton Sketch applies to any any twice differentiable function subject to a closed convex constraint set. We described various applications of the Newton sketch, including its use with barrier methods to solve various forms of constrained problems. For the minimization of self-concordant functions, the combination of the Newton sketch within interior point updates leads to much faster algorithms for an extensive body of convex optimization problems. Each iteration of the Newton sketch always has lower computational complexity than classical Newton's method. Moreover, it has lower computational complexity than first-order methods when either n \u2265 d 2 or d \u2265 n 2 (using the dual strategy); here n and d denote the dimensions of the data matrix A. In the context of barrier methods, the parameters n and d typically correspond to the number of constraints and number of variables, respectively. In many \"big data\" problems, one of the dimensions is much larger than the other, in which case the Newton sketch is advantageous. Moreover, sketches based on the randomized Hadamard transform are well-suited to in parallel environments: in this case, the sketching step can be done in O(log m) time with O(nd) processors. This scheme significantly decreases the amount of central computation-namely, from O(m 2 d + nd log m) to O(m 2 d + log d).\n\nThere are a number of open problems associated with the Newton sketch. Here we focused our analysis on the cases of sub-Gaussian and randomized orthogonal system (ROS) sketches. It would also be interesting to analyze sketches based on coordinate sampling, or other forms of \"sparse\" sketches (for instance, see the paper [10]). Such techniques might lead to significant gains in cases where the data matrix A is itself sparse: more specifically, it may be possible to obtain sketched optimization algorithms whose computational complexity only scales with number of nonzero entries in the data matrices the full dimensionality nd. Finally, it would be interesting to explore the problem of lower bounds on the sketch dimension m. In particular, is there a threshold below which any algorithm that has access only to gradients and msketched Hessians must necessarily converge at a sub-linear rate, or in a way that depends on the strong convexity and smoothness parameters? Such a result would clarify whether or not the guarantees in this paper are improvable.\n\n\nC Gaussian widths with 1 -constraints\n\nIn this appendix, we state and prove an elementary lemma that bounds for the Gaussian width for a broad class of 1 -constrained problems. In particular, given a twice-differentiable convex function \u03c8, a vector c \u2208 R d , a radius R and a collection of d-vectors {a i } n i=1 , consider a convex program of the form\nmin x\u2208C n i=1 \u03c8 a i , x + c, x , where C = {x \u2208 R d | x 1 \u2264 R}.(61)\nLemma 8. Suppose that the 1 -constrained program (61) has a unique optimal solution x * such that x * 0 \u2264 s for some integer s. Then denoting the tangent cone at x * by K, then Proof. It is well known [16,20] that the tangent cone of the 1 -norm at any s-sparse solution is a subset of the cone {z \u2208 R d | z 1 \u2264 2 \u221a s z 2 }. Using this fact, we have the following sequence of upper bounds\nW(\u2207 2 f (x) 1/2 K) = E w max\nExample 1 (\n1Newton sketch for LP solving). Consider a linear program (LP) in the standard form min Ax\u2264b c, x\n\nFigure 1 .\n1Comparisons of central paths for a simple linear program in two dimensions. Each row shows three independent trials for a given sketch dimension: across the rows, the sketch dimension ranges as m \u2208 {d, 4d, 16d}. The black arrows show Newton steps taken by the standard interior point method, whereas red arrows show the steps taken by the sketched version. The green point at the vertex represents the optimum. In all cases, the sketched algorithm converges to the optimum, and as the sketch dimension m increases, the sketched central path converges to the standard central path.\n\nFigure 2\n2\n\nFigure 2 .\n2Empirical illustration of the linear convergence of the Newton sketch algorithm for an ensemble of portfolio optimization problems\n\n\u2022\nPoisson regression: \u03c8(u) = e u , \u03c8 (u) = e u and \u03c8 max \u03c8 min = e RAmax e \u2212RA min \u2022 Logistic regression: \u03c8(u) = log(1+e u ), \u03c8 (u) = e u (e u +1) 2 and \u03c8 max \u03c8 min = e RA min e \u2212RAmax (e \u2212RAmax +1) 2 (e RA min +1) 2 , where A max : = max i=1,...,n a i \u221e , and A min : = min i=1,...,n a i \u221e .\n\n\nO(n), as well as \u03c8 max \u03c8 min = O(log\n\nFigure 3 .\n3Newton Sketch algorithm outperforms other popular optimization methods. Plots of the log optimality gap versus iteration number (top) and plots of the log optimality gap versus wall-clock time (bottom). Newton Sketch empirically provides the best accuracy in smallest wall-clock time, and does not require knowledge of problem-dependent quantities (such as strong convexity and smoothness parameters).\n\n\nplots the the duality gap versus iteration number (top panel) and versus the wall-clock time (bottom panel) for the original barrier method (blue) and sketched barrier method (red): although the sketched algorithm requires more iterations, these iterations are cheaper, leading to a smaller wall-clock time. This point is reinforced by Figure 5, where we plot the wall-clock time required to reach a duality gap of 10 \u22126 versus the number of features n in problem families of increasing size. Note that the sketched barrier method outperforms the original barrier method, with significantly less computation time for obtaining similar accuracy.\n\nFigure 4 .\n4Plots of the duality gap versus iteration number (top panel) and duality gap versus wall-clock time (bottom panel) for the original barrier method (blue) and sketched barrier method (red). The sketched interior point method is run 10 times independently yielding slightly different curves in red. While the sketched method requires more iterations, its overall wall-clock time is much smaller.\n\nFigure 5 .\n5Plot of the wall-clock time in seconds for reaching a duality gap of 10 \u22126 for the standard and sketched interior point methods as n increases (in log-scale). The sketched interior point method has significantly lower computation time compared to the original method.\n\n\u03c8\n( a i , x , y i ), and \u03c8 max = max x\u2208C max i=1,...,n \u03c8 ( a i , x , y i ).\n\n\nAlgorithm 1 Unconstrained Newton Sketch with backtracking line searchInput: Starting point x 0 , tolerance \u03b4 > 0, (a, b) line-search parameters, sketching matrices {S t } \u221e t=0 \u2208 R m\u00d7n .1: Compute approximate Newton step \u2206x t and approximate Newton decrement \u03bb(x)\n\n1 :\n1Centering step: Compute x(\u03c4 ) by Newton Sketch with backtracking line-search initialized at xusing Algorithm 1 or Algorithm 2. \n\n2: Update x := x(\u03c4 ). \n3: Quit if r/\u03c4 \u2264 \u03b4. \n4: Increase \u03c4 by \u03c4 := \u00b5\u03c4 . \n\nOutput: minimizer x(\u03c4 ). \n\n\nTypical values of these constants are a = 0.1 and b = 0.5.\nAcknowledgementsBoth authors were partially supported by Office of Naval Research MURI grant N00014-11-1-0688, and National Science Foundation Grants CIF-31712-23800 and DMS-1107000. In addition, MP was supported by a Microsoft Research Fellowship.A Technical results for Theorem 2In this appendix, we collect together various technical results and proofs that are required in the proof of Theorem 2.Here the final inequality holds for all \u2208 (0, 1/2). Combining the bound (44b) with inequality (46) yieldswhere the final inequality again uses the condition \u2208 (0, 1 2 ). This completes the proof of the bound (38a).Proof of the bound (38b): This inequality has been established as a consequence of proving the bound (46).Proof of Theorem 3Given the proof of Theorem 2, it remains only to prove the following modified version of Lemma 2. It applies to the exact and sketched Newton directions v NE , v NSK \u2208 R d that are defined as follows v NE : = arg minThus, the only difference is that the Hessian \u2207 2 f (x) is sketched, whereas the term \u2207 2 g(x) remains unsketched.Lemma 7. Let S \u2208 R m\u00d7n be a sub-Gaussian or ROS sketching matrix, and let x \u2208 R d be a (possibly random) vector independent of S. If m \u2265 c 0 max x\u2208C, thenwith probability at least 1 \u2212 c 1 e \u2212c 2 m 2 .A.1 Proof of Lemma 2Let u be a unit-norm vector independent of S, and consider the random quantitiesSv 2 2 and (49a)By the optimality and feasibility of v NSK and v NE (respectively) for the sketched Newton update (36), we haveDefining the difference vector e : = v NSK \u2212 v NE , some algebra leads to the basic inequalityMoreover, by the optimality and feasibility of v NE and v NSK for the exact Newton update (35), we haveConsequently, by adding and subtracting \u2207 2 f (x)v NE , e , we find thatBy definition, the error vector e belongs to the cone K t and the vector \u2207 2 f (x) 1/2 v NE is fixed and independent of the sketch. Consequently, invoking definitions (49a) and (49b) of the random variables Z 1 and Z 2 yields,Putting together the pieces, we find thatFinally, for any \u03b4 \u2208 (0, 1), let us define the event E(\u03b4) = {Z 1 \u2265 1 \u2212 \u03b4, and Z 2 \u2264 \u03b4}. By Lemma 4 and Lemma 5 from our previous paper[20], we are guaranteed that P[E(\u03b4)] \u2265 1 \u2212 c 1 e \u2212c 2 m\u03b4 2 . Conditioned on the event E(\u03b4), the bound (53) implies thatBy setting \u03b4 = 4 , the claim follows.\nBy construction, the function g(u) = f (x + uv NSK ) is strictly convex and self-concordant. Consequently. it satisfies the bound d du g (u) \u22121/2 \u2264 1, whenceBy construction, the function g(u) = f (x + uv NSK ) is strictly convex and self-concordant. Consequently, it satisfies the bound d du g (u) \u22121/2 \u2264 1, whence\n\nApproximate nearest neighbors and the fast Johnson-Lindenstrauss transform. N Ailon, B Chazelle, Proceedings of the thirty-eighth annual ACM symposium on Theory of computing. the thirty-eighth annual ACM symposium on Theory of computingACMN. Ailon and B. Chazelle. Approximate nearest neighbors and the fast Johnson- Lindenstrauss transform. In Proceedings of the thirty-eighth annual ACM symposium on Theory of computing, pages 557-563. ACM, 2006.\n\nFast dimension reduction using rademacher series on dual bch codes. N Ailon, E Liberty, Discrete Comput. Geom. 424N. Ailon and E. Liberty. Fast dimension reduction using rademacher series on dual bch codes. Discrete Comput. Geom, 42(4):615-630, 2009.\n\nLocal Rademacher complexities. P L Bartlett, O Bousquet, S Mendelson, Annals of Statistics. 334P. L. Bartlett, O. Bousquet, and S. Mendelson. Local Rademacher complexities. Annals of Statistics, 33(4):1497-1537, 2005.\n\nConvex optimization. S Boyd, L Vandenberghe, Cambridge University PressCambridge, UKS. Boyd and L. Vandenberghe. Convex optimization. Cambridge University Press, Cam- bridge, UK, 2004.\n\nLocal operator theory, random matrices, and Banach spaces. K R Davidson, S J Szarek, Handbook of Banach Spaces. Amsterdam, NLElsevier1K. R. Davidson and S. J. Szarek. Local operator theory, random matrices, and Banach spaces. In Handbook of Banach Spaces, volume 1, pages 317-336. Elsevier, Amsterdam, NL, 2001.\n\nFast approximation of matrix coherence and statistical leverage. P Drineas, M Magdon-Ismail, M W Mahoney, D P Woodruff, The Journal of Machine Learning Research. 131P. Drineas, M. Magdon-Ismail, M.W. Mahoney, and D.P. Woodruff. Fast approximation of matrix coherence and statistical leverage. The Journal of Machine Learning Research, 13(1):3475-3506, 2012.\n\nLeast angle regression. B Efron, T Hastie, I Johnstone, R Tibshirani, Annals of Statistics. 322B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least angle regression. Annals of Statistics, 32(2):407-499, 2004.\n\nMatrix Computations. G Golub, C Van Loan, Johns Hopkins University PressBaltimoreG. Golub and C. Van Loan. Matrix Computations. Johns Hopkins University Press, Baltimore, 1996.\n\nlars: Least angle regression, lasso and forward stagewise. R package version 0. T Hastie, B Efron, T. Hastie and B. Efron. lars: Least angle regression, lasso and forward stagewise. R package version 0.9-7, 2007.\n\nSparser Johnson-Lindenstrauss transforms. D M Kane, J Nelson, Journal of the ACM. 6114D.M. Kane and J. Nelson. Sparser Johnson-Lindenstrauss transforms. Journal of the ACM, 61(1):4, 2014.\n\nAn interior-point method for large-scale 1 -regularized least squares. S Kim, K Koh, M Lustig, S Boyd, D Gorinevsky, IEEE Journal on Selected Topics in Signal Processing. 14S. Kim, K. Koh, M. Lustig, S. Boyd, and D. Gorinevsky. An interior-point method for large-scale 1 -regularized least squares. IEEE Journal on Selected Topics in Signal Processing, 1(4):606-617, 2007.\n\nM Ledoux, M Talagrand, Probability in Banach Spaces: Isoperimetry and Processes. New York, NYSpringer-VerlagM. Ledoux and M. Talagrand. Probability in Banach Spaces: Isoperimetry and Processes. Springer-Verlag, New York, NY, 1991.\n\nPortfolio Selection. H M Markowitz, WileyNew YorkH. M. Markowitz. Portfolio Selection. Wiley, New York, 1959.\n\nGeneralized linear models. Monographs on statistics and applied probability 37. P Mccullagh, J A Nelder, Chapman and Hall/CRCNew YorkP. McCullagh and J.A. Nelder. Generalized linear models. Monographs on statistics and applied probability 37. Chapman and Hall/CRC, New York, 1989.\n\nOn the rate of convergence of the levenberg-marquardt method. M Fukushima, N Yamashita, Topics in numerical analysis. SpringerM. Fukushima N. Yamashita. On the rate of convergence of the levenberg-marquardt method. In Topics in numerical analysis, pages 239-249. Springer, 2001.\n\nA unified framework for high-dimensional analysis of M -estimators with decomposable regularizers. S Negahban, P Ravikumar, M J Wainwright, B Yu, Statistical Science. 274S. Negahban, P. Ravikumar, M. J. Wainwright, and B. Yu. A unified framework for high-dimensional analysis of M -estimators with decomposable regularizers. Statistical Science, 27(4):538-557, December 2012.\n\nIntroductory Lectures on Convex Optimization. Y Nesterov, Kluwer Academic PublishersNew YorkY. Nesterov. Introductory Lectures on Convex Optimization. Kluwer Academic Publish- ers, New York, 2004.\n\nInterior-Point Polynomial Algorithms in Convex Programming. Y Nesterov, A Nemirovski, SIAM Studies in Applied Mathematics. Y. Nesterov and A. Nemirovski. Interior-Point Polynomial Algorithms in Convex Pro- gramming. SIAM Studies in Applied Mathematics, 1994.\n\nIterative Hessian sketch: Fast and accurate solution approximation for constrained least-squares. M Pilanci, M J Wainwright, arXiv:1411.0347UC BerkeleyTechnical reportM. Pilanci and M. J. Wainwright. Iterative Hessian sketch: Fast and accurate solution approximation for constrained least-squares. Technical report, UC Berkeley, 2014. Full length version at arXiv:1411.0347.\n\nRandomized sketches of convex programs with sharp guarantees. M Pilanci, M J Wainwright, arXiv:1404.7203UC BerkeleyTechnical reportFull length version at. Presented in part at ISITM. Pilanci and M. J. Wainwright. Randomized sketches of convex programs with sharp guarantees. Technical report, UC Berkeley, 2014. Full length version at arXiv:1404.7203; Presented in part at ISIT 2014.\n\nProbablistic methods in the geometry of Banach spaces. G Pisier, Probability and Analysis. Springer1206G. Pisier. Probablistic methods in the geometry of Banach spaces. In Probability and Analysis, volume 1206 of Lecture Notes in Mathematics, pages 167-241. Springer, 1989.\n\nGraph sparsification by effective resistances. D A Spielman, N Srivastava, SIAM Journal on Computing. 406D.A. Spielman and N. Srivastava. Graph sparsification by effective resistances. SIAM Journal on Computing, 40(6):1913-1926, 2011.\n\nRegression shrinkage and selection via the Lasso. R Tibshirani, Journal of the Royal Statistical Society, Series B. 581R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical Society, Series B, 58(1):267-288, 1996.\n\nIntroduction to the non-asymptotic analysis of random matrices. Compressed Sensing: Theory and Applications. R Vershynin, R. Vershynin. Introduction to the non-asymptotic analysis of random matrices. Com- pressed Sensing: Theory and Applications, 2012.\n\nNumerical optimization. S J Wright, J , Springer2New YorkS.J. Wright and J. Nocedal. Numerical optimization, volume 2. Springer New York, 1999.\n", "annotations": {"author": "[{\"end\":174,\"start\":101},{\"end\":304,\"start\":175},{\"end\":340,\"start\":305}]", "publisher": null, "author_last_name": "[{\"end\":113,\"start\":106},{\"end\":194,\"start\":184}]", "author_first_name": "[{\"end\":105,\"start\":101},{\"end\":181,\"start\":175},{\"end\":183,\"start\":182}]", "author_affiliation": "[{\"end\":173,\"start\":115},{\"end\":276,\"start\":218},{\"end\":303,\"start\":278},{\"end\":339,\"start\":306}]", "title": "[{\"end\":86,\"start\":1},{\"end\":426,\"start\":341}]", "venue": null, "abstract": "[{\"end\":1507,\"start\":440}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2039,\"start\":2035},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2161,\"start\":2157},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3749,\"start\":3745},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7339,\"start\":7335},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7341,\"start\":7339},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7344,\"start\":7341},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8136,\"start\":8133},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8498,\"start\":8494},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8500,\"start\":8498},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10021,\"start\":10018},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10024,\"start\":10021},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11460,\"start\":11457},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":11462,\"start\":11460},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":12147,\"start\":12144},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12390,\"start\":12386},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":12891,\"start\":12887},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":12894,\"start\":12891},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12896,\"start\":12894},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":19727,\"start\":19723},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":23203,\"start\":23199},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":23365,\"start\":23361},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":24976,\"start\":24972},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":25198,\"start\":25194},{\"end\":28334,\"start\":28331},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":31094,\"start\":31091},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":32792,\"start\":32788},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":34446,\"start\":34443},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":35728,\"start\":35725},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":38498,\"start\":38494},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":38707,\"start\":38704},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":38709,\"start\":38707},{\"end\":39585,\"start\":39578},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":40156,\"start\":40152},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":40423,\"start\":40419},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":42410,\"start\":42406},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":43903,\"start\":43900},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":44226,\"start\":44222},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":48706,\"start\":48702},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":52046,\"start\":52042},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":58313,\"start\":58310},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":59323,\"start\":59321},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":59826,\"start\":59822},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":61636,\"start\":61632},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":63000,\"start\":62996},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":63003,\"start\":63000}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":63322,\"start\":63212},{\"attributes\":{\"id\":\"fig_1\"},\"end\":63916,\"start\":63323},{\"attributes\":{\"id\":\"fig_2\"},\"end\":63928,\"start\":63917},{\"attributes\":{\"id\":\"fig_4\"},\"end\":64072,\"start\":63929},{\"attributes\":{\"id\":\"fig_5\"},\"end\":64366,\"start\":64073},{\"attributes\":{\"id\":\"fig_6\"},\"end\":64405,\"start\":64367},{\"attributes\":{\"id\":\"fig_8\"},\"end\":64820,\"start\":64406},{\"attributes\":{\"id\":\"fig_9\"},\"end\":65467,\"start\":64821},{\"attributes\":{\"id\":\"fig_10\"},\"end\":65874,\"start\":65468},{\"attributes\":{\"id\":\"fig_11\"},\"end\":66155,\"start\":65875},{\"attributes\":{\"id\":\"fig_13\"},\"end\":66232,\"start\":66156},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":66498,\"start\":66233},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":66734,\"start\":66499}]", "paragraph": "[{\"end\":2858,\"start\":1523},{\"end\":4137,\"start\":2860},{\"end\":5448,\"start\":4139},{\"end\":6146,\"start\":5450},{\"end\":6973,\"start\":6148},{\"end\":7159,\"start\":6988},{\"end\":7368,\"start\":7200},{\"end\":7525,\"start\":7370},{\"end\":7828,\"start\":7556},{\"end\":8029,\"start\":7877},{\"end\":8292,\"start\":8067},{\"end\":8733,\"start\":8294},{\"end\":8851,\"start\":8795},{\"end\":9205,\"start\":8894},{\"end\":9507,\"start\":9207},{\"end\":9646,\"start\":9549},{\"end\":10371,\"start\":9648},{\"end\":10683,\"start\":10373},{\"end\":11097,\"start\":10711},{\"end\":11464,\"start\":11099},{\"end\":12391,\"start\":11466},{\"end\":12684,\"start\":12411},{\"end\":12898,\"start\":12718},{\"end\":13327,\"start\":12900},{\"end\":13690,\"start\":13367},{\"end\":13964,\"start\":13718},{\"end\":13981,\"start\":13966},{\"end\":14122,\"start\":14048},{\"end\":14231,\"start\":14168},{\"end\":14391,\"start\":14354},{\"end\":14704,\"start\":14393},{\"end\":14830,\"start\":14750},{\"end\":14906,\"start\":14832},{\"end\":15324,\"start\":14988},{\"end\":15670,\"start\":15423},{\"end\":16124,\"start\":15749},{\"end\":16424,\"start\":16126},{\"end\":17159,\"start\":16584},{\"end\":17302,\"start\":17177},{\"end\":17529,\"start\":17304},{\"end\":17796,\"start\":17583},{\"end\":17993,\"start\":17921},{\"end\":18420,\"start\":18040},{\"end\":19070,\"start\":18422},{\"end\":19180,\"start\":19072},{\"end\":19765,\"start\":19182},{\"end\":19947,\"start\":19767},{\"end\":20299,\"start\":19987},{\"end\":20346,\"start\":20323},{\"end\":20548,\"start\":20377},{\"end\":20671,\"start\":20550},{\"end\":20900,\"start\":20713},{\"end\":20949,\"start\":20902},{\"end\":21541,\"start\":21003},{\"end\":21745,\"start\":21592},{\"end\":22349,\"start\":21789},{\"end\":22960,\"start\":22434},{\"end\":23291,\"start\":22962},{\"end\":23750,\"start\":23351},{\"end\":23925,\"start\":23837},{\"end\":24386,\"start\":24009},{\"end\":25274,\"start\":24436},{\"end\":25486,\"start\":25276},{\"end\":25845,\"start\":25488},{\"end\":25984,\"start\":25914},{\"end\":26192,\"start\":26014},{\"end\":26601,\"start\":26240},{\"end\":27360,\"start\":26603},{\"end\":27569,\"start\":27383},{\"end\":28376,\"start\":27639},{\"end\":29014,\"start\":28745},{\"end\":29559,\"start\":29113},{\"end\":29961,\"start\":29607},{\"end\":30847,\"start\":29995},{\"end\":31365,\"start\":30886},{\"end\":31546,\"start\":31425},{\"end\":31604,\"start\":31548},{\"end\":31684,\"start\":31606},{\"end\":31702,\"start\":31686},{\"end\":32005,\"start\":31958},{\"end\":32108,\"start\":32007},{\"end\":32270,\"start\":32157},{\"end\":32529,\"start\":32321},{\"end\":32805,\"start\":32531},{\"end\":32971,\"start\":32867},{\"end\":33379,\"start\":32973},{\"end\":33702,\"start\":33421},{\"end\":33925,\"start\":33761},{\"end\":34134,\"start\":33993},{\"end\":34227,\"start\":34136},{\"end\":34490,\"start\":34290},{\"end\":35019,\"start\":34525},{\"end\":35110,\"start\":35021},{\"end\":35296,\"start\":35112},{\"end\":35787,\"start\":35298},{\"end\":36022,\"start\":35789},{\"end\":36086,\"start\":36083},{\"end\":36675,\"start\":36088},{\"end\":37294,\"start\":36714},{\"end\":38021,\"start\":37338},{\"end\":38222,\"start\":38023},{\"end\":39120,\"start\":38322},{\"end\":39381,\"start\":39170},{\"end\":39639,\"start\":39421},{\"end\":39784,\"start\":39705},{\"end\":40627,\"start\":39786},{\"end\":41278,\"start\":40653},{\"end\":41682,\"start\":41354},{\"end\":41851,\"start\":41767},{\"end\":42289,\"start\":41965},{\"end\":42859,\"start\":42325},{\"end\":43136,\"start\":42904},{\"end\":43229,\"start\":43138},{\"end\":43483,\"start\":43312},{\"end\":44267,\"start\":43529},{\"end\":44870,\"start\":44314},{\"end\":45036,\"start\":44872},{\"end\":45210,\"start\":45038},{\"end\":45896,\"start\":45212},{\"end\":46246,\"start\":45931},{\"end\":46518,\"start\":46279},{\"end\":46856,\"start\":46601},{\"end\":47721,\"start\":46986},{\"end\":47830,\"start\":47732},{\"end\":48043,\"start\":47853},{\"end\":48549,\"start\":48156},{\"end\":48708,\"start\":48651},{\"end\":48816,\"start\":48710},{\"end\":49019,\"start\":48852},{\"end\":49583,\"start\":49063},{\"end\":49802,\"start\":49707},{\"end\":50113,\"start\":50001},{\"end\":50505,\"start\":50278},{\"end\":50756,\"start\":50572},{\"end\":50889,\"start\":50867},{\"end\":50980,\"start\":50934},{\"end\":51096,\"start\":51025},{\"end\":51187,\"start\":51123},{\"end\":51391,\"start\":51283},{\"end\":51723,\"start\":51555},{\"end\":51867,\"start\":51856},{\"end\":52102,\"start\":51890},{\"end\":52200,\"start\":52169},{\"end\":52443,\"start\":52295},{\"end\":52584,\"start\":52540},{\"end\":52918,\"start\":52684},{\"end\":53131,\"start\":53016},{\"end\":53568,\"start\":53187},{\"end\":53983,\"start\":53570},{\"end\":54016,\"start\":54010},{\"end\":54123,\"start\":54077},{\"end\":54448,\"start\":54125},{\"end\":54710,\"start\":54651},{\"end\":54835,\"start\":54712},{\"end\":55006,\"start\":54837},{\"end\":55087,\"start\":55039},{\"end\":55398,\"start\":55089},{\"end\":55562,\"start\":55437},{\"end\":55790,\"start\":55602},{\"end\":55915,\"start\":55792},{\"end\":56061,\"start\":55978},{\"end\":56169,\"start\":56063},{\"end\":56486,\"start\":56193},{\"end\":56691,\"start\":56560},{\"end\":56998,\"start\":56766},{\"end\":57217,\"start\":57065},{\"end\":57341,\"start\":57238},{\"end\":57608,\"start\":57547},{\"end\":57896,\"start\":57663},{\"end\":58328,\"start\":58075},{\"end\":58703,\"start\":58490},{\"end\":58790,\"start\":58727},{\"end\":58828,\"start\":58792},{\"end\":59031,\"start\":58963},{\"end\":59064,\"start\":59033},{\"end\":59132,\"start\":59066},{\"end\":59206,\"start\":59134},{\"end\":59325,\"start\":59245},{\"end\":59425,\"start\":59382},{\"end\":59517,\"start\":59481},{\"end\":61308,\"start\":59583},{\"end\":62371,\"start\":61310},{\"end\":62726,\"start\":62413},{\"end\":63183,\"start\":62795}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7555,\"start\":7526},{\"attributes\":{\"id\":\"formula_1\"},\"end\":7876,\"start\":7829},{\"attributes\":{\"id\":\"formula_2\"},\"end\":8066,\"start\":8030},{\"attributes\":{\"id\":\"formula_3\"},\"end\":8794,\"start\":8734},{\"attributes\":{\"id\":\"formula_4\"},\"end\":9548,\"start\":9508},{\"attributes\":{\"id\":\"formula_5\"},\"end\":10710,\"start\":10684},{\"attributes\":{\"id\":\"formula_6\"},\"end\":12717,\"start\":12685},{\"attributes\":{\"id\":\"formula_7\"},\"end\":14047,\"start\":13982},{\"attributes\":{\"id\":\"formula_8\"},\"end\":14167,\"start\":14123},{\"attributes\":{\"id\":\"formula_9\"},\"end\":14353,\"start\":14232},{\"attributes\":{\"id\":\"formula_10\"},\"end\":14749,\"start\":14705},{\"attributes\":{\"id\":\"formula_11\"},\"end\":14987,\"start\":14907},{\"attributes\":{\"id\":\"formula_12\"},\"end\":15422,\"start\":15325},{\"attributes\":{\"id\":\"formula_13\"},\"end\":15748,\"start\":15671},{\"attributes\":{\"id\":\"formula_14\"},\"end\":16583,\"start\":16425},{\"attributes\":{\"id\":\"formula_16\"},\"end\":17582,\"start\":17530},{\"attributes\":{\"id\":\"formula_17\"},\"end\":17920,\"start\":17797},{\"attributes\":{\"id\":\"formula_18\"},\"end\":18039,\"start\":17994},{\"attributes\":{\"id\":\"formula_19\"},\"end\":19986,\"start\":19948},{\"attributes\":{\"id\":\"formula_20\"},\"end\":20322,\"start\":20300},{\"attributes\":{\"id\":\"formula_21\"},\"end\":20712,\"start\":20672},{\"attributes\":{\"id\":\"formula_22\"},\"end\":21591,\"start\":21542},{\"attributes\":{\"id\":\"formula_23\"},\"end\":21788,\"start\":21746},{\"attributes\":{\"id\":\"formula_24\"},\"end\":22433,\"start\":22350},{\"attributes\":{\"id\":\"formula_25\"},\"end\":23350,\"start\":23292},{\"attributes\":{\"id\":\"formula_26\"},\"end\":23836,\"start\":23751},{\"attributes\":{\"id\":\"formula_27\"},\"end\":24008,\"start\":23926},{\"attributes\":{\"id\":\"formula_28\"},\"end\":24435,\"start\":24387},{\"attributes\":{\"id\":\"formula_29\"},\"end\":25913,\"start\":25846},{\"attributes\":{\"id\":\"formula_30\"},\"end\":26013,\"start\":25985},{\"attributes\":{\"id\":\"formula_31\"},\"end\":27638,\"start\":27570},{\"attributes\":{\"id\":\"formula_32\"},\"end\":28458,\"start\":28377},{\"attributes\":{\"id\":\"formula_33\"},\"end\":28744,\"start\":28458},{\"attributes\":{\"id\":\"formula_34\"},\"end\":29112,\"start\":29015},{\"attributes\":{\"id\":\"formula_35\"},\"end\":29994,\"start\":29962},{\"attributes\":{\"id\":\"formula_36\"},\"end\":30885,\"start\":30848},{\"attributes\":{\"id\":\"formula_37\"},\"end\":31957,\"start\":31703},{\"attributes\":{\"id\":\"formula_38\"},\"end\":32156,\"start\":32109},{\"attributes\":{\"id\":\"formula_39\"},\"end\":32320,\"start\":32271},{\"attributes\":{\"id\":\"formula_40\"},\"end\":32866,\"start\":32806},{\"attributes\":{\"id\":\"formula_41\"},\"end\":33992,\"start\":33926},{\"attributes\":{\"id\":\"formula_42\"},\"end\":34289,\"start\":34228},{\"attributes\":{\"id\":\"formula_43\"},\"end\":34524,\"start\":34491},{\"attributes\":{\"id\":\"formula_44\"},\"end\":36082,\"start\":36023},{\"attributes\":{\"id\":\"formula_46\"},\"end\":38321,\"start\":38223},{\"attributes\":{\"id\":\"formula_47\"},\"end\":39169,\"start\":39121},{\"attributes\":{\"id\":\"formula_48\"},\"end\":39420,\"start\":39382},{\"attributes\":{\"id\":\"formula_49\"},\"end\":39704,\"start\":39640},{\"attributes\":{\"id\":\"formula_50\"},\"end\":41353,\"start\":41279},{\"attributes\":{\"id\":\"formula_51\"},\"end\":41766,\"start\":41683},{\"attributes\":{\"id\":\"formula_52\"},\"end\":41964,\"start\":41852},{\"attributes\":{\"id\":\"formula_53\"},\"end\":42903,\"start\":42860},{\"attributes\":{\"id\":\"formula_54\"},\"end\":43311,\"start\":43230},{\"attributes\":{\"id\":\"formula_55\"},\"end\":43528,\"start\":43484},{\"attributes\":{\"id\":\"formula_56\"},\"end\":46278,\"start\":46247},{\"attributes\":{\"id\":\"formula_57\"},\"end\":46600,\"start\":46519},{\"attributes\":{\"id\":\"formula_58\"},\"end\":46938,\"start\":46857},{\"attributes\":{\"id\":\"formula_59\"},\"end\":46985,\"start\":46938},{\"attributes\":{\"id\":\"formula_60\"},\"end\":48155,\"start\":48044},{\"attributes\":{\"id\":\"formula_61\"},\"end\":48609,\"start\":48550},{\"attributes\":{\"id\":\"formula_62\"},\"end\":48650,\"start\":48609},{\"attributes\":{\"id\":\"formula_63\"},\"end\":48851,\"start\":48817},{\"attributes\":{\"id\":\"formula_64\"},\"end\":49062,\"start\":49020},{\"attributes\":{\"id\":\"formula_65\"},\"end\":49706,\"start\":49584},{\"attributes\":{\"id\":\"formula_66\"},\"end\":50000,\"start\":49803},{\"attributes\":{\"id\":\"formula_67\"},\"end\":50277,\"start\":50114},{\"attributes\":{\"id\":\"formula_68\"},\"end\":50571,\"start\":50506},{\"attributes\":{\"id\":\"formula_69\"},\"end\":50866,\"start\":50757},{\"attributes\":{\"id\":\"formula_70\"},\"end\":50933,\"start\":50890},{\"attributes\":{\"id\":\"formula_71\"},\"end\":51024,\"start\":50981},{\"attributes\":{\"id\":\"formula_72\"},\"end\":51122,\"start\":51097},{\"attributes\":{\"id\":\"formula_73\"},\"end\":51282,\"start\":51188},{\"attributes\":{\"id\":\"formula_74\"},\"end\":51554,\"start\":51392},{\"attributes\":{\"id\":\"formula_75\"},\"end\":51855,\"start\":51724},{\"attributes\":{\"id\":\"formula_76\"},\"end\":52168,\"start\":52103},{\"attributes\":{\"id\":\"formula_77\"},\"end\":52294,\"start\":52201},{\"attributes\":{\"id\":\"formula_78\"},\"end\":52539,\"start\":52444},{\"attributes\":{\"id\":\"formula_79\"},\"end\":52683,\"start\":52585},{\"attributes\":{\"id\":\"formula_80\"},\"end\":53015,\"start\":52919},{\"attributes\":{\"id\":\"formula_81\"},\"end\":53186,\"start\":53132},{\"attributes\":{\"id\":\"formula_82\"},\"end\":54009,\"start\":53984},{\"attributes\":{\"id\":\"formula_83\"},\"end\":54076,\"start\":54017},{\"attributes\":{\"id\":\"formula_84\"},\"end\":54650,\"start\":54449},{\"attributes\":{\"id\":\"formula_85\"},\"end\":55038,\"start\":55007},{\"attributes\":{\"id\":\"formula_86\"},\"end\":55436,\"start\":55399},{\"attributes\":{\"id\":\"formula_87\"},\"end\":55601,\"start\":55563},{\"attributes\":{\"id\":\"formula_88\"},\"end\":55977,\"start\":55916},{\"attributes\":{\"id\":\"formula_89\"},\"end\":56559,\"start\":56487},{\"attributes\":{\"id\":\"formula_90\"},\"end\":56765,\"start\":56692},{\"attributes\":{\"id\":\"formula_91\"},\"end\":57064,\"start\":56999},{\"attributes\":{\"id\":\"formula_92\"},\"end\":57237,\"start\":57218},{\"attributes\":{\"id\":\"formula_93\"},\"end\":57546,\"start\":57342},{\"attributes\":{\"id\":\"formula_94\"},\"end\":57662,\"start\":57609},{\"attributes\":{\"id\":\"formula_95\"},\"end\":58074,\"start\":57897},{\"attributes\":{\"id\":\"formula_96\"},\"end\":58489,\"start\":58329},{\"attributes\":{\"id\":\"formula_97\"},\"end\":58900,\"start\":58829},{\"attributes\":{\"id\":\"formula_98\"},\"end\":58962,\"start\":58900},{\"attributes\":{\"id\":\"formula_99\"},\"end\":59244,\"start\":59207},{\"attributes\":{\"id\":\"formula_100\"},\"end\":59381,\"start\":59326},{\"attributes\":{\"id\":\"formula_101\"},\"end\":59480,\"start\":59426},{\"attributes\":{\"id\":\"formula_102\"},\"end\":59569,\"start\":59518},{\"attributes\":{\"id\":\"formula_103\"},\"end\":62794,\"start\":62727},{\"attributes\":{\"id\":\"formula_104\"},\"end\":63212,\"start\":63184}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1521,\"start\":1509},{\"attributes\":{\"n\":\"2\"},\"end\":6986,\"start\":6976},{\"attributes\":{\"n\":\"2.1\"},\"end\":7198,\"start\":7162},{\"attributes\":{\"n\":\"2.2\"},\"end\":8892,\"start\":8854},{\"attributes\":{\"n\":\"2.3\"},\"end\":12409,\"start\":12394},{\"attributes\":{\"n\":\"3\"},\"end\":13365,\"start\":13330},{\"attributes\":{\"n\":\"3.1\"},\"end\":13716,\"start\":13693},{\"attributes\":{\"n\":\"3.2\"},\"end\":17175,\"start\":17162},{\"end\":20375,\"start\":20349},{\"attributes\":{\"n\":\"3.3\"},\"end\":21001,\"start\":20952},{\"attributes\":{\"n\":\"4\"},\"end\":26238,\"start\":26195},{\"attributes\":{\"n\":\"4.1\"},\"end\":27381,\"start\":27363},{\"attributes\":{\"n\":\"4.2\"},\"end\":29605,\"start\":29562},{\"end\":31423,\"start\":31368},{\"attributes\":{\"n\":\"4.3\"},\"end\":33419,\"start\":33382},{\"end\":33759,\"start\":33705},{\"attributes\":{\"n\":\"5\"},\"end\":36712,\"start\":36678},{\"attributes\":{\"n\":\"5.1\"},\"end\":37336,\"start\":37297},{\"attributes\":{\"n\":\"5.2\"},\"end\":40651,\"start\":40630},{\"attributes\":{\"n\":\"5.3\"},\"end\":42323,\"start\":42292},{\"attributes\":{\"n\":\"5.4\"},\"end\":44312,\"start\":44270},{\"attributes\":{\"n\":\"5.5\"},\"end\":45929,\"start\":45899},{\"attributes\":{\"n\":\"6\"},\"end\":47730,\"start\":47724},{\"attributes\":{\"n\":\"6.1\"},\"end\":47851,\"start\":47833},{\"attributes\":{\"n\":\"6.2\"},\"end\":51888,\"start\":51870},{\"attributes\":{\"n\":\"6.2.1\"},\"end\":56191,\"start\":56172},{\"attributes\":{\"n\":\"6.2.2\"},\"end\":58725,\"start\":58706},{\"attributes\":{\"n\":\"7\"},\"end\":59581,\"start\":59571},{\"end\":62411,\"start\":62374},{\"end\":63224,\"start\":63213},{\"end\":63334,\"start\":63324},{\"end\":63926,\"start\":63918},{\"end\":63940,\"start\":63930},{\"end\":64075,\"start\":64074},{\"end\":64417,\"start\":64407},{\"end\":65479,\"start\":65469},{\"end\":65886,\"start\":65876},{\"end\":66158,\"start\":66157},{\"end\":66503,\"start\":66500}]", "table": "[{\"end\":66734,\"start\":66598}]", "figure_caption": "[{\"end\":63322,\"start\":63226},{\"end\":63916,\"start\":63336},{\"end\":64072,\"start\":63942},{\"end\":64366,\"start\":64076},{\"end\":64405,\"start\":64369},{\"end\":64820,\"start\":64419},{\"end\":65467,\"start\":64823},{\"end\":65874,\"start\":65481},{\"end\":66155,\"start\":65888},{\"end\":66232,\"start\":66159},{\"end\":66498,\"start\":66235},{\"end\":66598,\"start\":66505}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":18606,\"start\":18598},{\"end\":40279,\"start\":40276},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":44714,\"start\":44706},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":45570,\"start\":45562},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":47720,\"start\":47712}]", "bib_author_first_name": "[{\"end\":69509,\"start\":69508},{\"end\":69518,\"start\":69517},{\"end\":69951,\"start\":69950},{\"end\":69960,\"start\":69959},{\"end\":70166,\"start\":70165},{\"end\":70168,\"start\":70167},{\"end\":70180,\"start\":70179},{\"end\":70192,\"start\":70191},{\"end\":70375,\"start\":70374},{\"end\":70383,\"start\":70382},{\"end\":70599,\"start\":70598},{\"end\":70601,\"start\":70600},{\"end\":70613,\"start\":70612},{\"end\":70615,\"start\":70614},{\"end\":70918,\"start\":70917},{\"end\":70929,\"start\":70928},{\"end\":70946,\"start\":70945},{\"end\":70948,\"start\":70947},{\"end\":70959,\"start\":70958},{\"end\":70961,\"start\":70960},{\"end\":71236,\"start\":71235},{\"end\":71245,\"start\":71244},{\"end\":71255,\"start\":71254},{\"end\":71268,\"start\":71267},{\"end\":71450,\"start\":71449},{\"end\":71459,\"start\":71458},{\"end\":71687,\"start\":71686},{\"end\":71697,\"start\":71696},{\"end\":71863,\"start\":71862},{\"end\":71865,\"start\":71864},{\"end\":71873,\"start\":71872},{\"end\":72081,\"start\":72080},{\"end\":72088,\"start\":72087},{\"end\":72095,\"start\":72094},{\"end\":72105,\"start\":72104},{\"end\":72113,\"start\":72112},{\"end\":72384,\"start\":72383},{\"end\":72394,\"start\":72393},{\"end\":72637,\"start\":72636},{\"end\":72639,\"start\":72638},{\"end\":72807,\"start\":72806},{\"end\":72820,\"start\":72819},{\"end\":72822,\"start\":72821},{\"end\":73071,\"start\":73070},{\"end\":73084,\"start\":73083},{\"end\":73388,\"start\":73387},{\"end\":73400,\"start\":73399},{\"end\":73413,\"start\":73412},{\"end\":73415,\"start\":73414},{\"end\":73429,\"start\":73428},{\"end\":73712,\"start\":73711},{\"end\":73924,\"start\":73923},{\"end\":73936,\"start\":73935},{\"end\":74222,\"start\":74221},{\"end\":74233,\"start\":74232},{\"end\":74235,\"start\":74234},{\"end\":74562,\"start\":74561},{\"end\":74573,\"start\":74572},{\"end\":74575,\"start\":74574},{\"end\":74940,\"start\":74939},{\"end\":75207,\"start\":75206},{\"end\":75209,\"start\":75208},{\"end\":75221,\"start\":75220},{\"end\":75446,\"start\":75445},{\"end\":75763,\"start\":75762},{\"end\":75932,\"start\":75931},{\"end\":75934,\"start\":75933},{\"end\":75944,\"start\":75943}]", "bib_author_last_name": "[{\"end\":69515,\"start\":69510},{\"end\":69527,\"start\":69519},{\"end\":69957,\"start\":69952},{\"end\":69968,\"start\":69961},{\"end\":70177,\"start\":70169},{\"end\":70189,\"start\":70181},{\"end\":70202,\"start\":70193},{\"end\":70380,\"start\":70376},{\"end\":70396,\"start\":70384},{\"end\":70610,\"start\":70602},{\"end\":70622,\"start\":70616},{\"end\":70926,\"start\":70919},{\"end\":70943,\"start\":70930},{\"end\":70956,\"start\":70949},{\"end\":70970,\"start\":70962},{\"end\":71242,\"start\":71237},{\"end\":71252,\"start\":71246},{\"end\":71265,\"start\":71256},{\"end\":71279,\"start\":71269},{\"end\":71456,\"start\":71451},{\"end\":71468,\"start\":71460},{\"end\":71694,\"start\":71688},{\"end\":71703,\"start\":71698},{\"end\":71870,\"start\":71866},{\"end\":71880,\"start\":71874},{\"end\":72085,\"start\":72082},{\"end\":72092,\"start\":72089},{\"end\":72102,\"start\":72096},{\"end\":72110,\"start\":72106},{\"end\":72124,\"start\":72114},{\"end\":72391,\"start\":72385},{\"end\":72404,\"start\":72395},{\"end\":72649,\"start\":72640},{\"end\":72817,\"start\":72808},{\"end\":72829,\"start\":72823},{\"end\":73081,\"start\":73072},{\"end\":73094,\"start\":73085},{\"end\":73397,\"start\":73389},{\"end\":73410,\"start\":73401},{\"end\":73426,\"start\":73416},{\"end\":73432,\"start\":73430},{\"end\":73721,\"start\":73713},{\"end\":73933,\"start\":73925},{\"end\":73947,\"start\":73937},{\"end\":74230,\"start\":74223},{\"end\":74246,\"start\":74236},{\"end\":74570,\"start\":74563},{\"end\":74586,\"start\":74576},{\"end\":74947,\"start\":74941},{\"end\":75218,\"start\":75210},{\"end\":75232,\"start\":75222},{\"end\":75457,\"start\":75447},{\"end\":75773,\"start\":75764},{\"end\":75941,\"start\":75935}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":69430,\"start\":69116},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":490517},\"end\":69880,\"start\":69432},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":313102},\"end\":70132,\"start\":69882},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":2259087},\"end\":70351,\"start\":70134},{\"attributes\":{\"id\":\"b4\"},\"end\":70537,\"start\":70353},{\"attributes\":{\"id\":\"b5\"},\"end\":70850,\"start\":70539},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":129692},\"end\":71209,\"start\":70852},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":121570279},\"end\":71426,\"start\":71211},{\"attributes\":{\"id\":\"b8\"},\"end\":71604,\"start\":71428},{\"attributes\":{\"id\":\"b9\"},\"end\":71818,\"start\":71606},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":7821848},\"end\":72007,\"start\":71820},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":675041},\"end\":72381,\"start\":72009},{\"attributes\":{\"id\":\"b12\"},\"end\":72613,\"start\":72383},{\"attributes\":{\"id\":\"b13\"},\"end\":72724,\"start\":72615},{\"attributes\":{\"id\":\"b14\"},\"end\":73006,\"start\":72726},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":116093410},\"end\":73286,\"start\":73008},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":656534},\"end\":73663,\"start\":73288},{\"attributes\":{\"id\":\"b17\"},\"end\":73861,\"start\":73665},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":117194167},\"end\":74121,\"start\":73863},{\"attributes\":{\"doi\":\"arXiv:1411.0347\",\"id\":\"b19\"},\"end\":74497,\"start\":74123},{\"attributes\":{\"doi\":\"arXiv:1404.7203\",\"id\":\"b20\"},\"end\":74882,\"start\":74499},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":115444556},\"end\":75157,\"start\":74884},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":8001711},\"end\":75393,\"start\":75159},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":16162039},\"end\":75651,\"start\":75395},{\"attributes\":{\"id\":\"b24\"},\"end\":75905,\"start\":75653},{\"attributes\":{\"id\":\"b25\"},\"end\":76050,\"start\":75907}]", "bib_title": "[{\"end\":69506,\"start\":69432},{\"end\":69948,\"start\":69882},{\"end\":70163,\"start\":70134},{\"end\":70596,\"start\":70539},{\"end\":70915,\"start\":70852},{\"end\":71233,\"start\":71211},{\"end\":71860,\"start\":71820},{\"end\":72078,\"start\":72009},{\"end\":73068,\"start\":73008},{\"end\":73385,\"start\":73288},{\"end\":73921,\"start\":73863},{\"end\":74937,\"start\":74884},{\"end\":75204,\"start\":75159},{\"end\":75443,\"start\":75395}]", "bib_author": "[{\"end\":69517,\"start\":69508},{\"end\":69529,\"start\":69517},{\"end\":69959,\"start\":69950},{\"end\":69970,\"start\":69959},{\"end\":70179,\"start\":70165},{\"end\":70191,\"start\":70179},{\"end\":70204,\"start\":70191},{\"end\":70382,\"start\":70374},{\"end\":70398,\"start\":70382},{\"end\":70612,\"start\":70598},{\"end\":70624,\"start\":70612},{\"end\":70928,\"start\":70917},{\"end\":70945,\"start\":70928},{\"end\":70958,\"start\":70945},{\"end\":70972,\"start\":70958},{\"end\":71244,\"start\":71235},{\"end\":71254,\"start\":71244},{\"end\":71267,\"start\":71254},{\"end\":71281,\"start\":71267},{\"end\":71458,\"start\":71449},{\"end\":71470,\"start\":71458},{\"end\":71696,\"start\":71686},{\"end\":71705,\"start\":71696},{\"end\":71872,\"start\":71862},{\"end\":71882,\"start\":71872},{\"end\":72087,\"start\":72080},{\"end\":72094,\"start\":72087},{\"end\":72104,\"start\":72094},{\"end\":72112,\"start\":72104},{\"end\":72126,\"start\":72112},{\"end\":72393,\"start\":72383},{\"end\":72406,\"start\":72393},{\"end\":72651,\"start\":72636},{\"end\":72819,\"start\":72806},{\"end\":72831,\"start\":72819},{\"end\":73083,\"start\":73070},{\"end\":73096,\"start\":73083},{\"end\":73399,\"start\":73387},{\"end\":73412,\"start\":73399},{\"end\":73428,\"start\":73412},{\"end\":73434,\"start\":73428},{\"end\":73723,\"start\":73711},{\"end\":73935,\"start\":73923},{\"end\":73949,\"start\":73935},{\"end\":74232,\"start\":74221},{\"end\":74248,\"start\":74232},{\"end\":74572,\"start\":74561},{\"end\":74588,\"start\":74572},{\"end\":74949,\"start\":74939},{\"end\":75220,\"start\":75206},{\"end\":75234,\"start\":75220},{\"end\":75459,\"start\":75445},{\"end\":75775,\"start\":75762},{\"end\":75943,\"start\":75931},{\"end\":75947,\"start\":75943}]", "bib_venue": "[{\"end\":69221,\"start\":69116},{\"end\":69605,\"start\":69529},{\"end\":69991,\"start\":69970},{\"end\":70224,\"start\":70204},{\"end\":70372,\"start\":70353},{\"end\":70649,\"start\":70624},{\"end\":71012,\"start\":70972},{\"end\":71301,\"start\":71281},{\"end\":71447,\"start\":71428},{\"end\":71684,\"start\":71606},{\"end\":71900,\"start\":71882},{\"end\":72178,\"start\":72126},{\"end\":72462,\"start\":72406},{\"end\":72634,\"start\":72615},{\"end\":72804,\"start\":72726},{\"end\":73124,\"start\":73096},{\"end\":73453,\"start\":73434},{\"end\":73709,\"start\":73665},{\"end\":73984,\"start\":73949},{\"end\":74219,\"start\":74123},{\"end\":74559,\"start\":74499},{\"end\":74973,\"start\":74949},{\"end\":75259,\"start\":75234},{\"end\":75509,\"start\":75459},{\"end\":75760,\"start\":75653},{\"end\":75929,\"start\":75907},{\"end\":69668,\"start\":69607},{\"end\":70664,\"start\":70651},{\"end\":72476,\"start\":72464}]"}}}, "year": 2023, "month": 12, "day": 17}