{"id": 234754531, "updated": "2022-01-12 18:29:23.953", "metadata": {"title": "OSDDY: embedded system-based object surveillance detection system with small drone using deep YOLO", "authors": "[{\"middle\":[],\"last\":\"Madasamy\",\"first\":\"Kaliappan\"},{\"middle\":[],\"last\":\"Shanmuganathan\",\"first\":\"Vimal\"},{\"middle\":[],\"last\":\"Kandasamy\",\"first\":\"Vijayalakshmi\"},{\"middle\":[\"Young\"],\"last\":\"Lee\",\"first\":\"Mi\"},{\"middle\":[],\"last\":\"Thangadurai\",\"first\":\"Manikandan\"}]", "venue": "EURASIP Journal on Image and Video Processing", "journal": "EURASIP Journal on Image and Video Processing", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Computer vision is an interdisciplinary domain for object detection. Object detection relay is a vital part in assisting surveillance, vehicle detection and pose estimation. In this work, we proposed a novel deep you only look once (deep YOLO V3) approach to detect the multi-object. This approach looks at the entire frame during the training and test phase. It followed a regression-based technique that used a probabilistic model to locate objects. In this, we construct 106 convolution layers followed by 2 fully connected layers and 812 \u00d7 812 \u00d7 3 input size to detect the drones with small size. We pre-train the convolution layers for classification at half the resolution and then double the resolution for detection. The number of filters of each layer will be set to 16. The number of filters of the last scale layer is more than 16 to improve the small object detection. This construction uses up-sampling techniques to improve undesired spectral images into the existing signal and rescaling the features in specific locations. It clearly reveals that the up-sampling detects small objects. It actually improves the sampling rate. This YOLO architecture is preferred because it considers less memory resource and computation cost rather than more number of filters. The proposed system is designed and trained to perform a single type of class called drone and the object detection and tracking is performed with the embedded system-based deep YOLO. The proposed YOLO approach predicts the multiple bounding boxes per grid cell with better accuracy. The proposed model has been trained with a large number of small drones with different conditions like open field, and marine environment with complex background.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/ejivp/KaliappanVVLT21", "doi": "10.1186/s13640-021-00559-1"}}, "content": {"source": {"pdf_hash": "f28916969149809a0837cd0be033c4586350e26d", "pdf_src": "Anansi", "pdf_uri": "[\"https://jivp-eurasipjournals.springeropen.com/track/pdf/10.1186/s13640-021-00559-1.pdf\"]", "oa_url_match": true, "oa_info": {"license": "CCBY", "open_access_url": "https://jivp-eurasipjournals.springeropen.com/track/pdf/10.1186/s13640-021-00559-1", "status": "GOLD"}}, "grobid": {"id": "1911c0b0bcc9ae8c9824b39f983c3415b933e663", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/f28916969149809a0837cd0be033c4586350e26d.txt", "contents": "\nOSDDY: embedded system-based object surveillance detection system with small drone using deep YOLO\n\n\nKaliappan Madasamy \nDepartment of Software\nSejong University\nSeoulSouth Korea\n\nVimal Shanmuganathan \nDepartment of Software\nSejong University\nSeoulSouth Korea\n\nVijayalakshmi Kandasamy \nDepartment of Software\nSejong University\nSeoulSouth Korea\n\nMi Young Lee \nDepartment of Software\nSejong University\nSeoulSouth Korea\n\nManikandan Thangadurai \nDepartment of Software\nSejong University\nSeoulSouth Korea\n\nOSDDY: embedded system-based object surveillance detection system with small drone using deep YOLO\n10.1186/s13640-021-00559-1R E S E A R C H Open Access Full list of author information is available at the end of the articleDeep learningSurveillanceYOLOConvolution neural networkEmbedded-based object detection\nComputer vision is an interdisciplinary domain for object detection. Object detection relay is a vital part in assisting surveillance, vehicle detection and pose estimation. In this work, we proposed a novel deep you only look once (deep YOLO V3) approach to detect the multi-object. This approach looks at the entire frame during the training and test phase. It followed a regression-based technique that used a probabilistic model to locate objects. In this, we construct 106 convolution layers followed by 2 fully connected layers and 812 \u00d7 812 \u00d7 3 input size to detect the drones with small size. We pre-train the convolution layers for classification at half the resolution and then double the resolution for detection. The number of filters of each layer will be set to 16. The number of filters of the last scale layer is more than 16 to improve the small object detection. This construction uses up-sampling techniques to improve undesired spectral images into the existing signal and rescaling the features in specific locations. It clearly reveals that the up-sampling detects small objects. It actually improves the sampling rate. This YOLO architecture is preferred because it considers less memory resource and computation cost rather than more number of filters. The proposed system is designed and trained to perform a single type of class called drone and the object detection and tracking is performed with the embedded system-based deep YOLO. The proposed YOLO approach predicts the multiple bounding boxes per grid cell with better accuracy. The proposed model has been trained with a large number of small drones with different conditions like open field, and marine environment with complex background.\n\nIntroduction\n\nThe application of drones in various domains is increasing day by day especially military and surveillance to perform deliberate operations in the arena. It is very much important to detect drones in providing security in real-time. Nowadays, real-time detection of drones is a great challenge in various environments like rain, sunlight, and night. Deep learning plays a major role in detecting objects in various conditions [1]. Recently, computer vision and deep learning approaches such as R-CNN, Faster R-CNN, and mask-RCNN are providing a solution to detect an object [2].\n\nThe object detection and tracking system has been variedly applied in various areas of military, health sectors, and security monitoring with autonomy robots [3]. The traditional object identification has been implied with the primary focus on assuming the edges, cuttings, and templates, where the chances of getting the accuracy is lower and the loss is assumed to be of higher [4]. Besides various features extraction methods are been used with CBIR images and various filtering techniques has been applied to the detected objects to ensure the object identification [5]. The gradient-based histogram in image classifiers has been used and the local binary patterns also been applied with the image scanning in the object with sliding window assumptions [6]. The machine learning techniques has been used with the image accuracy enhancement using the PASCAL VOC object detection in handcrafted future. But all of these mechanisms have been challenging phases for the object tracking in the surveillance system using the embedded system [7]. To overcome these challenges, various deep learning models have been proposed to enhance the accuracy [8].\n\nThe deep learning models such as R-CNN, Faster R-CNN, and mask-RCNN are not suitable for detecting small objects with speed. In this paper, a novel deep YOLOV3 algorithm is proposed to detect small objects [9]. The proposed work employs confidence score, backbone classifier, and multi-scale predictions to detect small drone objects. The proposed deep YOLOV3 offer the following mechanism for detecting the small drone objects:\n\nThe contributions of the paper are as follows:\n\nThis paper proposed a deep YOLOV3 to solve the small object detection issue with speed. The confidence score is calculated based on the conditional probability to detect the bounding boxes for a target object. Deep YOLOV3 also used a backbone classifier and multi-scale prediction to classify the objects with high accuracy and more accurate surveillance has been achieved.\n\nThe proposed deep YOLOV3 model is achieved 99.99% accuracy to detect the small drone objects with less loss\n\nThe rest of the work contains five sections. In Section 2, the paper summarizes the various literature reviews to sort out the problems. In Section 3, the paper is proposed with the deep YOLOV3 model for detecting a small drone object efficiently. In Section 4, the simulation and performance of the proposed deep YOLOV3 model to analyze the loss and accuracy are presented. Section 5 presents the conclusions and future direction.\n\n\nHighlights of the proposed YOLOv3 model\n\nThe proposed YOLOv3 uses logistic regression to predict a confidence score for a bounding box and 7 \u00d7 7 grid cells are detected simultaneously. The result shows that this is a very fast model. The proposed YOLOv3 uses three anchor boxes to predict three boxes for a grid cell and allocated to a grid cell in detection. The proposed YOLOv3 architecture such as 106 convolution layers followed by 2 fully connected layers and 812 \u00d7 812 \u00d7 3 input size detects a small object with low false alarm and uses minimum filters as possible. The proposed YOLOv3 model uses independent logistic classifiers and binary crossentropy loss during training for small object prediction. It also supports multi-label 2 Related works Unlu et al. proposed that the commercial unmanned aerial vehicle has been announced as a drone is increasing the modern communication of video and audio with security standards is an essential key factor for the wireless devices [10]. They investigated a novel approach on autonomy drone detection and tracking with a multi-camera angle view [11]. The camera support and the frame have been analyzed with the compact memory and time in an efficient manner [12]. The small-sized aerial intruders are used with the image plane and the compressed images are analyzed with the resource detection algorithm using the deep learning classification algorithms [13,14].\n\nDeep learning-based object detection has been proposed with various accuracy levels Song Han et al. investigated that the lower cost aerial photography has been used for taking the highlighted pictures and videos with advanced drones and assumed to be of error-prone. A deep drone has been proposed with the embedded system framework where the power drones vision is highly investigated with the drone automatic tracking [15]. The tracking and detection are assumed to be of the multiple hardware with GPU(NVIDIA GTX980) and GPU (NVIDIA Tegra K1 and NVIDIA Tegra X1), the embedded setup has been used to obscure the frame rate, accuracy estimation, and the power assumption has been analyzed with the 1.6 fps for tracking [16]. Redman et al. proposed a new approach on YOLO detectors to use object detection and the work on object classification has been done with the regression problem in bounding boxes and the probability for the association [17]. A neural network has been proposed with the class analysis of probability for the end to end directly in the performance detection and faster R CNN has been used intensively for object detection [18].\n\nKrizhevsky et al. investigated that the deep neural network has been widely used with the computer vision analysis for the binary and multiple image classification and analysis. Besides a tool AlexNet a classic tool of network proposed with image contributes 8 layers and 60 million connections, later VGGNet [8]. Szegedy et al. proposed that GoogleNet has been proposed with the different scaling with the support of CNN. Goo-gleNet with CNN proposes a convolutional layer with a specified model having 1 \u00d7 1, 3 \u00d7 3, and 5 \u00d7 5 with kernel level. The gradient problem has been solved using this multiple cross layers usage [19]. He et al. proposed that ResNet increases the image recognition accuracy and it bypasses the layers that may enhance the absolute values and SqueezeNet has been applied with CNN to enhance the image recognition accuracy with 50\u00d7 connections and produces with higher accuracy [20].\n\nHenriques et al. highlighted that the kernelized correlation filters have been used for detection of the image classification based on the DFT and a blueprint has been used with the fast algorithms with the resultant in the diagonalized translations in the storage and computation with the trackers to run 70 frames per second on the NVIDIA TK1 kit [21]. Sabir Hossain et al. highlighted that the target detection and tracking from aerial images have happened with smart sensors and drones. A deep learning-based framework has been proposed with the embedded modules such as Jetson TX or AGX Xavier with Intel Neural computer stick [22]. The flying drones are used with a certain coverage limit and hence it has been followed with the accuracy for estimating the multi-object detection algorithm with the GPU-based embedded for the specified computation power [23]. Deep SORT uses a hypothesis tracking with the support of Kalman filtering with the association metric specified in the multi-rotor drone [24].\n\nRoberto Opromolla highlighted that the UAVs have been used in various civil and military applications specified with the visual cameras that enable the tracking and detection of the track cooperative targets with the frame sequence using deep learning [25]. The you only look once (YOLO), an object detection system, is done with the processing architecture where the machine vision algorithms with the cooperative hints need to be brought to the flight test campaign in the two multirotor UAV. The methods integrate the accuracy and robustness in the challenging environments in the target range vulnerability [26].\n\nChristos Kyrkou et al., proposed a trade-off mechanism with the development of a single-shot object detector using the deep CNN. The UAVs detect the vehicle in a dedicated UAV environment. CNN proposes a holistic approach in the optimization with the deployment UAVs. The aerial images operate in 6-19 frames per second with an accuracy of 95% with the UAV applications with the low power embedded processors [27]. Tsung-Yi Lin et al. highlighted that the RetinaNet performs the object detection using the backbone network and the network supports the classification and regression. The backbone network uses the convolutional feature with the input images combined the Faster R-CNN also uses the Feature Pyramid Network (FPN) [28]. The probability of object presence has been used with the input features mapping in the C channels in the pyramid level with the A anchors and N object classes that take the ReLUactivations [29].\n\nYi Liu et al., highlighted that the UAV has been applied in the tasks of power transmission devices and the usage of deep learning algorithms has been used with the attention of UAV transmission control [30]. The Mask R-CNN has been used with the components of the transmission devices using the edge, hole filling, and hough transform in wireless communication [31]. The accuracy has been achieved with the 100% accuracy in the proposed model using the UAV transmission parameters [32].\n\nLI Y et al. proposed that the multiblock single shot multibox detector (SSD) with the small object detection is used for the surveillance of the railway tracks with UAV. The input images are segmented in terms of patches and the truncated object has been assigned with the sub-layer detection in the two stages with the suppression of sublayer and filtering has been applied in the training samples. The boxes that are not detected in the main layer have been substantially increased with the available SSD and the deep learning model has been proposed with labeling the landslides and the important communication during rainy days has been reported [33].\n\nJun-Ichiro Watanabe et al. proposed that the YOLO has been applied in the conservation of the marine environments with the micro and macro plastics on land that occupies the ocean and with that intrusion many species are suspected to face the consequences. The satellite remote sensing techniques for global environmental monitoring have been identified with the object tracker on the ocean surface [34]. Autonomy robots have been used with the observation for the control of the objects in marine environments. The underwater ecosystems have been studied with the learning object algorithm using a DEEP net and the YOLO v3 has been applied and accuracy has been estimated with 77.2% [35]. Kaliappan et al. [36][37][38] proposed machine learning techniques such as clustering and genetic algorithms to achieve load balancing. Vimal et al. [39] proposed machine learning-based Markov model for energy optimization in cognitive radio networks. Aybora et al. [40] simulated types of annotation errors for object detection using YOLOv3 and examined the erroneous annotations in training and testing phase. Sabir et al. [24] designed a GPU-based embedded flying robot that used a deep learning algorithm to detect and track the real time multiple-object from aerial imagery.\n\n\nMethods\n\nThe aim of the proposed model is to analyze the object detection in a real-time environment, with movement decisions using a novel YOLO V3 model using the box within the bounded coordinates. YOLOv3 performs better feature extraction than YOLOv2 because it uses a hybrid approach such as Darknet and residual network. The image is captured and segmented within the bounded box level coordinates. The coordinates are mapped within the box with an interval of frames per second in the novel YOLOV3 model. In this, a deep convolutional neural network (DCNN) to predict with high accuracy has been applied. Various filter sizes such as 32, 64,128,256, and 1024 are applied with striding and padding to process pixel by pixel basis in the frame [2]. The proposed scheme of various sizes kernelized correlation filter (KCF) is used in different convolution layers. In general, KCF runs very fast on video processing. The CNN layers split the image into various regions to predict the accurate bounding box based on the confidence score for all divided regions [41]. The proposed YOLOv3 trained in the Dell EMC workstation which consists of two Intel Xeon Gold 511812 core processor, Six channel memory 256 GB 2666 MHz DDR4 ECC memory, 2\u00d7 NVIDIA Quadro GV100 GPU and 4\u00d7 1TB NVMe class 40 SSD, 1TB SATA HDD.\n\n\nProposed work\n\nThe model has been proposed as a novel YOLO V3 deep learning embedded model to detect a small object with a real-time system. YOLO looks at the entire image in an instance to predict the bounding box coordinates [42]. The class probabilities are calculated for all bounding boxes. YOLO can process 45 frames per second. In this, we used a deep convolutional neural network (DCNN) to predict with high accuracy [43]. Figure 1 shows our proposed deep YOLO V3 prediction model to predict the drones. Labeled input images are trained with 45,000 epochs. Each region is a 7 \u00d7 7 grid capable of predicting five bounding boxes. The proposed model detects the drone object as well.\n\nThe proposed YOLO algorithm with embedded model used a regression mechanism to predict the classes and bounding boxes for the entire image in every single run in a particular object location. Equation (1) \ny \u00bc p c ; b x ; b y ; b h ; b w ; c \u00c0 \u00c1\u00f01\u00de\nThe CNN predicts four coordinates for each bounding box such as t x , t y , t w and t h . The prediction of a bounding box is followed by the following four equations. c x , c y is the offset of the top left corner of an image and p w and p h are the prior bounding box width and height respectively. Here, t * is the gradient ground truth value which is computed during the training process.\nb x \u00bc \u03c3 t x \u00f0 \u00de \u00fe c x \u00f02\u00de b y \u00bc \u03c3 t y \u00c0 \u00c1 \u00fe c y \u00f03\u00de b w \u00bc p w e t w \u00f04\u00de b h \u00bc p h e t h\u00f05\u00de\nThe proposed algorithm applied anon-max suppression technique called independent logistic regression and predicts the p c while much of the grid and boxes do not contain a targeted object. p c is used to predict a confidence score for a bounding box and 7 \u00d7 7 grid cells are detected simultaneously. The result shows that this is a very fast model. This strategy rejects bounding boxes with low probability and predicts a bounding box with the highest probability. The predicted bounding box contains a good confidential score. Equations (6) express the confidence score.\np r o \u00f0 \u00dex IOU \u00f0 \u00de\u00f06\u00de\nIOU is an intersection over the union in the region. It is expressed as an area of intersection over the area of the union of two bounding boxes. IOU falls within 0 to 1 and the ground truth box is said to be~1.IOU is used to find the confidence score for each bounding box that ensures a box contains a predicted target object. IOU also prevents background detection. The confidence score is 0 if there is no object present in the grid cell. Otherwise, the confidence score is equal to IOU between the predicted bounding box and the ground truth box. IOU is achieved greater than 0.5 that ensures a better prediction with high accuracy for object detection [44]. In order to achieve good prediction, YOLO multiplies the individual box confidence predictions with conditional class probabilities (p r (c i )) that is expressed in Eq. (7).\np r c i j o \u00f0 \u00dep r o \u00f0 \u00deIOU truth prediction \u00bc p r c i \u00f0 \u00deIOU truth prediction\u00f07\u00de\nThe average IOUs are selected to represent the final predicted bounding boxes which are the much closest prior values for good prediction. It is expressed in the following equation.\np r o \u00f0 \u00de x IOU b; o \u00f0 \u00de \u00bc \u03c3 t o \u00f0 \u00de\u00f08\u00de\nThe confidence score of a bounding box plays a vital role in making a prediction at the testing stage. It is the output of a neural network. The paper has been designed with a DCNN with 106 convolution layers that contain convolution layers, pooling layers and fully connected layers with classification function. We calculated feature maps with convolution by sliding filter along with the input image in the result is a two-dimension matrix [45]. Figure 2 shows the sample feature map calculated in the convolution layer.\n\n\nResults and discussion\n\nThe paper has been proposed with the downloaded 3000 drone images from the Kaggle dataset and Google with around 2 GB. In the proposed YOLOv3, 45,000 epochs have been taken for drone dataset that provide the high accuracy and sensitivity. Figure 3 shows the sample drone images used for training and testing [46]. The proposed work uses a pre-trained YOLOv3 model for training. We implemented the YOLOv3 in GPUbased workstation.\n\nThe input images are trained by a pre-trained YOLOv3 model with 106 convolution layers. The training stage takes more than 8 h to build the trained model. This trained YOLO model can accept either image or video input. The proposed model achieved 99.99 percentage results on the detection of drones images or videos [47]. Figure 4 shows the detection of drones' video in the testing stage. Table 1 provides the accuracy comparison of three models such as YOLO, YOLOv2, and YOLOv3. YOLO and YOLOv2 are suitable for large object detection in very fast manner. The proposed  YOLOv3 architecture is suitable for small object detection because it uses a hybrid network.\n\n\nLoss analysis\n\nThis section shows the evaluation of various losses such as total loss, classification loss, localization loss, clone loss, objectness, and localization loss in the region proposal network (RPN) for object detection. Figure 5 shows the total loss up to 45,000 epochs. It reached 0 from the 200 epoch that ensures very good accuracy. Figure 6 shows the classification loss that achieved 0 from the beginning of the epoch that ensures the perfect classification of the drones based on conditional probabilities. It is the final layer that reveals the object detection. Figure 7 shows the localization loss during the various epochs. It returns the region proposals from the feature map based on the offset of the bounding box. The proposed  YOLOv3 scheme uses a sum-squared error (SSE) as loss function for optimization. It penalizes the classification error for an object in each grid cell. This scheme chose the box with the highest IOU. Figure 8 ensures that the proposed approach is extracting the region proposals for a targeted object. Region proposal network (RPN) is integrated with the convolution neural network CNN network for classification. The accuracy of the proposed scheme based on performance of the RPN module. Figure 9 shows the localization loss in the RPN in which the bounding box regression loss is predicted to make good accuracy because it applied the regressor strategy for classification. Figure 10 shows the clone loss in various epochs. IT ensures the training and validation losses for accurate prediction. Also, it was recalculated by the neurons in the CNN layer weights for good predictions.\n\n\nAccuracy\n\nThe proposed deep YOLOV3 model provides 99.99% accuracy while training and testing stage because the model is designed with 106 convolution layers and different size feature maps. The YOLOv2 model is also used for training and testing in which 98.27%  is achieved because it uses only residual network to detect an object. This model also applied a confidence score based on conditional probability to predict a target object effectively. Figure 11 shows that the proposed model achieved very good accuracy at the end of the epoch. The proposed approach also used backbone classifier to classify the objects accurately.\n\n\nConclusion\n\nIn this work, a novel deep YOLO V3 model is proposed to detect the small objects. The project involves training the model using pre-trained YOLOV3 with drone images. The simulation result is shown that the proposed deep YOLO V3 model is suitable for the computer vision process. In this, 106 convolution layers were designed with various feature maps to learn the small drone objects. YOLOv3 extracts better features using both Darknet and residual networks. The training stage performs 45,000 epochs to provide high accuracy. The proposed scheme uses IOU to predict a confidence score for a bounding box and grid cells simultaneously. The proposed model used logistic classifiers and binary cross-entropy loss for optimization to detect a small object. The proposed deep YOLO V3 revealed the 99.99% of accuracy because it used multi-scale predictions and backbone classifiers to better classify them. The different kinds of losses are analyzed that ensure the very good prediction of drone images because this model achieved a good confidence score based on conditional probability. It is used to predict the accurate bounding boxes of an object. The proposed YOLOv3 model is not suitable to detect the larger size objects compared to previous versions such as YOLO and YOLOv2. In the future direction, the algorithm can be extended to train a huge volume of a small drone with the complex visible condition and far-flung remote areas. \n\n\ndescribed the bounding box with four descriptors such as center of a bounding box (b y , b h ), width (b w ), height (b h ), and a class of an object (c).\n\nFig. 1\n1Proposed\n\nFig. 2\n2Feature\n\nFig. 4\n4Detection of drones video\n\nFig. 5\n5Epoch vs total loss\n\nFig. 6\n6Epoch\n\nFig. 7\n7Epoch vs localization loss\n\nFig. 8\n8Steps vs objectness loss in RPNMadasamy et al.\n\nFig. 9\n9Epoch vs localization loss in RPN Fig. 10 Epoch vs clone loss Madasamy et al.\n\n\nOSDDY: Object surveillance detection using deep YOLO; YOLO: You only look once; R-CNN: Region convolutional neural network; CBIR: Content-based image retrieval; VOC: Visual object classes; GPU: Graphics processing unit; VGGNet: Visual Geometry Group; UAV: Unmanned aerial vehicle; DCNN: Deep convolution neural network; RPN: Risk priority number\n\nTable 1\n1Comparison of three YOLO modelsModel \n\nMadasamy et al. EURASIP Journal on Image and Video Processing (2021) 2021:19\nMadasamy et al. EURASIP Journal on Image and Video Processing (2021) 2021:19 Page 4 of 14\nAcknowledgementsThe authors would like to thank the anonymous reviewers for their helpful comments. The authors like to express special gratitude to Artificial Intelligence Lab, Ramco Institute of Technology, Tamilnadu, India, National Engineering College, Tamilnadu, India, and Sejong University, Seoul, South Korea, for providing and support for all the facilities to do this proposed experimentation.Authors' contributions M. Kaliappan: writing-original draft, writing-review and editing, S. Vimal: writing-original draft, conceptualization, data curation, validation. K. Vijayalakshmi: conceptualization, data curation, validation. Mi Young Lee: conceptualization, formal analysis, writing-review and editing, supervision, and funding. S. Manikandan: formal analysis, supervision, writing-review and editing. All authors read and approved the final manuscript. Availability of data and materials Not applicable.DeclarationsEthics approval and consent to participate This research does not involve any human or animal participation.Competing interestsThe authors declare that they do not have any conflict of interests. All authors have checked and agreed the submission.\nS Agarwal, J O D Terrail, F Jurie, arXiv:1807.04606Recent Advances in Object Detection in the Age of Deep Convolutional Neural Networks. S. Agarwal, J.O.D. Terrail, F. Jurie, Recent Advances in Object Detection in the Age of Deep Convolutional Neural Networks. arXiv:1807.04606 (2018)\n\nConvolutional neural networks based fire detection in surveillance videos. K Muhammad, J Ahmad, I Mehmood, S Rho, S W Baik, IEEE Access. 6K. Muhammad, J. Ahmad, I. Mehmood, S. Rho, S.W. Baik, Convolutional neural networks based fire detection in surveillance videos. IEEE Access 6, 18174-18183 (2018)\n\nObject detection with discriminatively trained part-based models. P F Felzenszwalb, R B Girshick, D Mcallester, D Ramanan, IEEE Trans. Pattern Anal. Mach. Intell. 329P.F. Felzenszwalb, R.B. Girshick, D. McAllester, D. Ramanan, Object detection with discriminatively trained part-based models. IEEE Trans. Pattern Anal. Mach. Intell. 32(9), 1627-1645 (2010)\n\nBoosted local structured HOG-LBP for object localization. J Zhang, K Huang, Y Yu, T Tan, 2011 IEEE Computer Vision and Pattern Recognition (CVPR). CO, USA, Colorado SpringsJ. Zhang, K. Huang, Y. Yu, T. Tan, in 2011 IEEE Computer Vision and Pattern Recognition (CVPR), CO, USA, Colorado Springs. Boosted local structured HOG-LBP for object localization (2011), pp. 1393-1400\n\n3D object retrieval with multi-feature collaboration and bipartite graph matching. Y Zhang, S Rho, S Liu, D Zhao, R Ji, F Jiang, Neurocomputing. 195Y. Zhang, S. Rho, S. Liu, D. Zhao, R. Ji, F. Jiang, 3D object retrieval with multi-feature collaboration and bipartite graph matching. Neurocomputing 195, 40-49 (2016)\n\nJ Zhang, Y Huang, K Huang, Z Wu, T Tan, Asian Conference on Computer Vision. Data decomposition and spatial mixture modeling for part based model. J. Zhang, Y. Huang, K. Huang, Z. Wu, T. Tan, in Asian Conference on Computer Vision. Data decomposition and spatial mixture modeling for part based model (2012), pp. 123-137\n\nReal-time robust 3D object tracking and estimation for surveillance system. J Park, S Rho, C S Jeong, Secur. Commun. Netw. 710J. Park, S. Rho, C.S. Jeong, Real-time robust 3D object tracking and estimation for surveillance system. Secur. Commun. Netw. 7(10), 1599-1611 (2014)\n\nImagenet classification with deep convolutional neural networks. I Krizhevsky, G E Sutskever, Hinton, Adv. Neural Inf. Process. Syst. 25I. Krizhevsky, Sutskever, G.E. Hinton, Imagenet classification with deep convolutional neural networks. Adv. Neural Inf. Process. Syst. 25, 1097-1105 (2012)\n\nDeep learning. Y Lecun, Y Bengio, G Hinton, Nature. 5217553Y. LeCun, Y. Bengio, G. Hinton, Deep learning. Nature 521(7553), 436-444 (2015)\n\nDeep learning-based strategies for the detection and tracking of drones using several cameras. E Unlu, E Zenou, N Riviere, 10.1186/s41074-019-0059-xIPSJ Trans. Comput. Vis. Appl. 117E. Unlu, E. Zenou, N. Riviere, et al., Deep learning-based strategies for the detection and tracking of drones using several cameras. IPSJ Trans. Comput. Vis. Appl. 11, 7 (2019). https://doi.org/10.1186/s41074-019-0059-x\n\nEnergy efficient routing protocol using grover's searching algorithm for MANET. E Mariappan, M Kaliappan, S Vimal, 10.3923/ajit.2016.4986.4994Asian J. Inf. Technol. 1424E. Mariappan, M. Kaliappan, S. Vimal, Energy efficient routing protocol using grover's searching algorithm for MANET. Asian J. Inf. Technol. 14(24), 4986-4994 (2016). https://doi.org/10.3923/ajit.2016.4986.4994\n\nFeature-fused SSD: fast detection for small objects. G Cao, X Xie, W Yang, Q Liao, G Shi, J Wu, Comput. Vis. Pattern Recognit. 10615106151G. Cao, X. Xie, W. Yang, Q. Liao, G. Shi, J. Wu, Feature-fused SSD: fast detection for small objects. Comput. Vis. Pattern Recognit. 10615, 106151E (2018)\n\n. J Ahmad, I Mehmood, S Rho, N Chilamkurti, S W Baik, 10.1016/j.compeleceng.2017.05.033Comput. Electr. Eng. 61J. Ahmad, I. Mehmood, S. Rho, N. Chilamkurti, S.W. Baik, et al., Comput. Electr. Eng. 61, 297-311 (2017). https://doi.org/1 0.1016/j.compeleceng.2017.05.033\n\nMultiple 3D object position estimation and tracking using double filtering on multicore processor. J H Park, S Rho, C S Jeong, J Kim, 10.1007/s11042-012-1029-9Multimed. Tools Appl. 631J.H. Park, S. Rho, C.S. Jeong, J. Kim, Multiple 3D object position estimation and tracking using double filtering on multi- core processor. Multimed. Tools Appl. 63(1), 161-180 (2013). https://doi.org/10.1007/s11042-012-1029-9\n\nLocal feature-based multi-object recognition scheme for surveillance. D Kim, S Rho, E Hwang, Eng. Appl. Artif. Intell. 257D. Kim, S. Rho, E. Hwang, Local feature-based multi-object recognition scheme for surveillance. Eng. Appl. Artif. Intell. 25(7), 1373-1380 (2012)\n\nS Han, W Shen, Z Liu, Deep Drone: Object Detection and Tracking for Smart Drones on Embedded System. S. Han, W. Shen, Z. Liu, Deep Drone: Object Detection and Tracking for Smart Drones on Embedded System (2016)\n\nY C Lee, J Chen, C W Tseng, S H Lai, British Machine Vision Conference (BMVC). Accurate and robust face recognition from RGB-D images with a deep learning approach. Y.C. Lee, J. Chen, C.W. Tseng, S.H. Lai, in British Machine Vision Conference (BMVC). Accurate and robust face recognition from RGB-D images with a deep learning approach (2016)\n\nYou Only Look Once: Unified, Real-Time Object Detection. J Redmon, S Divvala, R Girshick, A Farhadi, arXiv:1506.02640arXiv preprintJ. Redmon, S. Divvala, R. Girshick, A. Farhadi, You Only Look Once: Unified, Real-Time Object Detection. arXiv preprint arXiv: 1506.02640 (2015)\n\nW Szegedy, Y Liu, P Jia, S Sermanet, D Reed, D Anguelov, V Erhan, A Vanhoucke, Rabinovich, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Going deeper with convolutions. the IEEE Conference on Computer Vision and Pattern Recognition. Going deeper with convolutionsW. Szegedy, Y. Liu, P. Jia, S. Sermanet, D. Reed, D. Anguelov, V. Erhan, Vanhoucke, A. Rabinovich, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Going deeper with convolutions (2015), pp. 1-9\n\nK He, X Zhang, S Ren, J Sun, arXiv:1512.03385Deep Residual Learning for Image Recognition. arXiv preprintK. He, X. Zhang, S. Ren, J. Sun, Deep Residual Learning for Image Recognition. arXiv preprint arXiv:1512.03385 (2015)\n\nHighspeed tracking with kernelized correlation filters. J F Henriques, R Caseiro, P Martins, J Batista, IEEE Trans. Pattern Anal. Mach. Intell. 373J.F. Henriques, R. Caseiro, P. Martins, J. Batista, Highspeed tracking with kernelized correlation filters. IEEE Trans. Pattern Anal. Mach. Intell. 37(3), 583-596 (2015)\n\nFacenet: a unified embedding for face recognition and clustering. F Schroff, D Kalenichenko, J Philbin, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition815F. Schroff, D. Kalenichenko, J. Philbin, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Facenet: a unified embedding for face recognition and clustering (2015), p. 815\n\nReal-time multi-objects recognition and tracking scheme. D Kim, S Rho, E Hwang, Korean Navig. J. (KONI). 162D. Kim, S. Rho, E. Hwang, Real-time multi-objects recognition and tracking scheme. Korean Navig. J. (KONI) 16(2), 386- 393 (2012)\n\nDeep learning-based real-time multiple-object detection and tracking from aerial imagery via a flying robot with GPU-based embedded devices. S Hossain, D Lee, 10.3390/s19153371Sensors (Basel). 1915S. Hossain, D.-j. Lee, Deep learning-based real-time multiple-object detection and tracking from aerial imagery via a flying robot with GPU-based embedded devices. Sensors (Basel) 19(15), 3371-3385 (2019). https://doi.org/10.3390/s191 53371\n\nMultiresolution gray scale and rotation invariant texture classification with local binary patterns. T Ojala, M Pietik\u00e4inen, T M\u00e4enp\u00e4\u00e4, IEEE Trans. Pattern Anal. Mach. Intell. 247T. Ojala, M. Pietik\u00e4inen, T. M\u00e4enp\u00e4\u00e4, Multiresolution gray scale and rotation invariant texture classification with local binary patterns. IEEE Trans. Pattern Anal. Mach. Intell. 24(7) in press\n\nAirborne visual detection and tracking of cooperative UAVs exploiting deep learning. R Opromolla, G Inchingolo, G Fasano, Sensors. 194332R. Opromolla, G. Inchingolo, G. Fasano, Airborne visual detection and tracking of cooperative UAVs exploiting deep learning. Sensors 19, 4332 (2019)\n\nC Kyrkou, G Plastiras, T Theocharides, arXiv:1807.06789v1[cs.CV]DroNet: Efficient Convolutional Neural Network Detector for Real-time UAV Applications. C. Kyrkou, G. Plastiras, T. Theocharides, DroNet: Efficient Convolutional Neural Network Detector for Real-time UAV Applications. arXiv:1807.06789v1 [cs.CV] (2018)\n\nSecure data packet transmission in MANET using enhanced identity-based cryptography. S Vimal, Int. J. New Technol. Sci. Eng. 312S. Vimal et al., Secure data packet transmission in MANET using enhanced identity-based cryptography. Int. J. New Technol. Sci. Eng. 3(12), 35-42 (2016)\n\nFocal Loss for Dense Object Detection. T Lin, P Goyal, R Girshick, K He, P Doll\u00e1r, 10.1109/ICCV.2017.3242017 IEEE International Conference on Computer Vision (ICCV). T. Lin, P. Goyal, R. Girshick, K. He and P. Doll\u00e1r, \"Focal Loss for Dense Object Detection,\" 2017 IEEE International Conference on Computer Vision (ICCV), 2999-3007 (2017). https://doi.org/10.1109/ICCV.2017.324\n\nS Annamalai, R Udendhran, S Vimal, 10.4018/978-1-5225-9023-1.ch005Novel Practices and Trends in Grid and Cloud Computing. An intelligent grid network based on cloud computing infrastructures. S. Annamalai, R. Udendhran, S. Vimal, in Novel Practices and Trends in Grid and Cloud Computing. An intelligent grid network based on cloud computing infrastructures (2019), pp. 59-73. https://doi.org/10.4018/978-1-5225-9023-1.ch005\n\nCloud-based predictive maintenance and machine monitoring for intelligent manufacturing for automobile industry. S Annamalai, R Udendhran, S Vimal, 10.4018/978-1-5225-9023-1.ch006Novel Practices and Trends in Grid and Cloud Computing. S. Annamalai, R. Udendhran, S. Vimal, in Novel Practices and Trends in Grid and Cloud Computing. Cloud-based predictive maintenance and machine monitoring for intelligent manufacturing for automobile industry (2019), pp. 74-81. https:// doi.org/10.4018/978-1-5225-9023-1.ch006\n\n. Y Liu, J. Phys. Conf. Ser. 134562043Y. Liu et al., J. Phys. Conf. Ser. 1345, 062043 (2019)\n\nMulti-block SSD based on small object detection for UAV railway scene surveillance. L I Yundong, 10.1016/j.cja.2020.02.024Chin. J. Aeronaut. L.I. Yundong et al., Multi-block SSD based on small object detection for UAV railway scene surveillance. Chin. J. Aeronaut. (2020). https://doi.org/10.1016/j.cja.2020.02.024\n\nDevelopment of cloud integrated internet of things based intruder detection system. S Vimal, M Kaliappan, A Suresh, P Subbulakshmi, S Kumar, D Kumar, J. Comput. Theor. Nanosci. 15S. Vimal, M. Kaliappan, A. Suresh, P. Subbulakshmi, S. Kumar, D. Kumar, Development of cloud integrated internet of things based intruder detection system. J. Comput. Theor. Nanosci. 15(11-12), 3565-3570 (2018)\n\nUnderwater and airborne monitoring of marine ecosystems and debris. J.-I Watanabe, Y Shao, N Miura, 10.1117/1.JRS.13.044509J. Appl. Remote Sens. 13444509J.-I. Watanabe, Y. Shao, N. Miura, Underwater and airborne monitoring of marine ecosystems and debris. J. Appl. Remote Sens. 13(4), 044509 (2019). https://doi.org/10.1117/1.JRS.13.044509\n\nLoad balanced clustering technique in MANET using genetic algorithms defence science. M Kaliappan, E Mariappan, M Prakash, B Paramasivan, 10.14429/dsj.66.9205Def. Sci. J. 663M. Kaliappan, E. Mariappan, M. Viju Prakash, B. Paramasivan, Load balanced clustering technique in MANET using genetic algorithms defence science. Def. Sci. J. 66(3), 251-258 (2016). https://doi.org/10.14429/dsj.66.9205\n\nSecure and fair cluster head selection protocol for enhancing security in mobile ad hoc networks. M Kaliappan, B Paramasivan, Sci. World J. 608984M. Kaliappan, B. Paramasivan, Secure and fair cluster head selection protocol for enhancing security in mobile ad hoc networks. Sci. World J. 2014, 608984 (2014)\n\nG S Kumar, M Kaliappan, L J Julus, International Conference on Pattern Recognition, Periyar University. Enhancing the performance of MANET using EESCP. G.S. Kumar, M. Kaliappan, L.J. Julus, in International Conference on Pattern Recognition, Periyar University. Enhancing the performance of MANET using EESCP (2012)\n\nDevelopment of secured data transmission using machine learning-based discrete-time partially observed Markov model and energy optimization in cognitive radio networks. S Vimal, L Kalaivani, M Kaliappan, A Suresh, X.-Z Gao, R Varatharajan, 10.1007/s00521-018-3788-3J. Neural Comput. Appl. S. Vimal, L. Kalaivani, M. Kaliappan, A. Suresh, X.-Z. Gao, R. Varatharajan, Development of secured data transmission using machine learning-based discrete-time partially observed Markov model and energy optimization in cognitive radio networks. J. Neural Comput. Appl., 1-11 (2018). https://doi.org/10.1007/s00521-018-3788-3\n\nA Koksal, K G Ince, A Alatan, arXiv:2004.01059arxiv.orgEffect of annotation errors on drone detection with YOLOv3. arXiv preprintComputer Vision and Pattern RecognitionA. Koksal, K.G. Ince, A. Alatan, in Computer Vision and Pattern Recognition. arXiv preprint arXiv:2004.01059. Effect of annotation errors on drone detection with YOLOv3 (2020) arxiv.org\n\nEnergy enhancement using Multiobjective Ant colony optimization with Double Q learning algorithm for IoT based cognitive radio networks. S Vimal, M Khari, R G Crespo, L Kalaivani, N Dey, M Kaliappan, Comput. Commun. 154S. Vimal, M. Khari, R.G. Crespo, L. Kalaivani, N. Dey, M. Kaliappan, Energy enhancement using Multiobjective Ant colony optimization with Double Q learning algorithm for IoT based cognitive radio networks. Comput. Commun. 154, 481- 490 (2020)\n\nDistortion invariant vehicle license plate extraction and recognition algorithm. J.-H Kim, J. Korea Contents Assoc. 113J.-H. Kim, Distortion invariant vehicle license plate extraction and recognition algorithm. J. Korea Contents Assoc. 11(3), 1-8 (2011)\n\nExtend the shallow part of single shot multibox detector via convolutional neural network. L Zheng, C Fu, Y Zhao, International Conference on Digital Image Processing. L. Zheng, C. Fu, Y. Zhao, in International Conference on Digital Image Processing. Extend the shallow part of single shot multibox detector via convolutional neural network (2018)\n\nK Simonyan, A Zisserman, Int. Conf. Learn. Represent. Very deep convolutional networks for large-scale image recognition. K. Simonyan, A. Zisserman, in Int. Conf. Learn. Represent. Very deep convolutional networks for large-scale image recognition (2015)\n\nC Szegedy, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Las Vegas, NVGoing deeper with convolutionsC. Szegedy et al., in 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV. Going deeper with convolutions (2015), pp. 1-9\n\nS Ioffe, C Szegedy, International Conference on Machine Learning. Batch normalization: accelerating deep network training by reducing internal covariate shift. S. Ioffe, C. Szegedy, in International Conference on Machine Learning. Batch normalization: accelerating deep network training by reducing internal covariate shift (2015), pp. 448-456\n\nRethinking the inception architecture for computer vision. C Szegedy, V Vanhoucke, S Ioffe, J Shlens, Z Wojna, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Las Vegas, NV, USAC. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, Z. Wojna, in 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, USA. Rethinking the inception architecture for computer vision (2016), pp. 2818-2826\n\nPublisher's Note. Publisher's Note\n\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\n. Madasamy, EURASIP Journal on Image and Video Processing. Madasamy et al. EURASIP Journal on Image and Video Processing (2021) 2021:19 Page 14 of 14\n", "annotations": {"author": "[{\"start\":\"102\",\"end\":\"180\"},{\"start\":\"181\",\"end\":\"261\"},{\"start\":\"262\",\"end\":\"345\"},{\"start\":\"346\",\"end\":\"418\"},{\"start\":\"419\",\"end\":\"501\"}]", "publisher": null, "author_last_name": "[{\"start\":\"112\",\"end\":\"120\"},{\"start\":\"187\",\"end\":\"201\"},{\"start\":\"276\",\"end\":\"285\"},{\"start\":\"355\",\"end\":\"358\"},{\"start\":\"430\",\"end\":\"441\"}]", "author_first_name": "[{\"start\":\"102\",\"end\":\"111\"},{\"start\":\"181\",\"end\":\"186\"},{\"start\":\"262\",\"end\":\"275\"},{\"start\":\"346\",\"end\":\"348\"},{\"start\":\"349\",\"end\":\"354\"},{\"start\":\"419\",\"end\":\"429\"}]", "author_affiliation": "[{\"start\":\"122\",\"end\":\"179\"},{\"start\":\"203\",\"end\":\"260\"},{\"start\":\"287\",\"end\":\"344\"},{\"start\":\"360\",\"end\":\"417\"},{\"start\":\"443\",\"end\":\"500\"}]", "title": "[{\"start\":\"1\",\"end\":\"99\"},{\"start\":\"502\",\"end\":\"600\"}]", "venue": null, "abstract": "[{\"start\":\"812\",\"end\":\"2535\"}]", "bib_ref": "[{\"start\":\"2977\",\"end\":\"2980\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"3125\",\"end\":\"3128\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"3289\",\"end\":\"3292\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"3511\",\"end\":\"3514\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"3701\",\"end\":\"3704\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"3888\",\"end\":\"3891\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"4170\",\"end\":\"4173\",\"attributes\":{\"ref_id\":\"b6\"}},{\"start\":\"4277\",\"end\":\"4280\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"4489\",\"end\":\"4492\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"6434\",\"end\":\"6445\"},{\"start\":\"6663\",\"end\":\"6667\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"6776\",\"end\":\"6780\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"6890\",\"end\":\"6894\",\"attributes\":{\"ref_id\":\"b11\"}},{\"start\":\"7086\",\"end\":\"7090\",\"attributes\":{\"ref_id\":\"b12\"}},{\"start\":\"7090\",\"end\":\"7093\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"7517\",\"end\":\"7521\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"7818\",\"end\":\"7822\",\"attributes\":{\"ref_id\":\"b15\"}},{\"start\":\"8042\",\"end\":\"8046\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"8243\",\"end\":\"8247\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"8559\",\"end\":\"8562\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"8873\",\"end\":\"8877\",\"attributes\":{\"ref_id\":\"b18\"}},{\"start\":\"9153\",\"end\":\"9157\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"9509\",\"end\":\"9513\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"9792\",\"end\":\"9796\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"10020\",\"end\":\"10024\",\"attributes\":{\"ref_id\":\"b22\"}},{\"start\":\"10163\",\"end\":\"10167\",\"attributes\":{\"ref_id\":\"b23\"}},{\"start\":\"10422\",\"end\":\"10426\",\"attributes\":{\"ref_id\":\"b24\"}},{\"start\":\"10781\",\"end\":\"10785\",\"attributes\":{\"ref_id\":\"b25\"}},{\"start\":\"11197\",\"end\":\"11201\",\"attributes\":{\"ref_id\":\"b26\"}},{\"start\":\"11515\",\"end\":\"11519\",\"attributes\":{\"ref_id\":\"b27\"}},{\"start\":\"11711\",\"end\":\"11715\",\"attributes\":{\"ref_id\":\"b28\"}},{\"start\":\"11921\",\"end\":\"11925\",\"attributes\":{\"ref_id\":\"b29\"}},{\"start\":\"12080\",\"end\":\"12084\",\"attributes\":{\"ref_id\":\"b30\"}},{\"start\":\"12200\",\"end\":\"12204\",\"attributes\":{\"ref_id\":\"b31\"}},{\"start\":\"12857\",\"end\":\"12861\",\"attributes\":{\"ref_id\":\"b32\"}},{\"start\":\"13263\",\"end\":\"13267\",\"attributes\":{\"ref_id\":\"b33\"}},{\"start\":\"13548\",\"end\":\"13552\",\"attributes\":{\"ref_id\":\"b34\"}},{\"start\":\"13571\",\"end\":\"13575\",\"attributes\":{\"ref_id\":\"b35\"}},{\"start\":\"13575\",\"end\":\"13579\",\"attributes\":{\"ref_id\":\"b36\"}},{\"start\":\"13579\",\"end\":\"13583\",\"attributes\":{\"ref_id\":\"b37\"}},{\"start\":\"13703\",\"end\":\"13707\",\"attributes\":{\"ref_id\":\"b38\"}},{\"start\":\"13820\",\"end\":\"13824\",\"attributes\":{\"ref_id\":\"b39\"}},{\"start\":\"13979\",\"end\":\"13983\",\"attributes\":{\"ref_id\":\"b23\"}},{\"start\":\"14884\",\"end\":\"14887\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"15198\",\"end\":\"15202\",\"attributes\":{\"ref_id\":\"b40\"}},{\"start\":\"15673\",\"end\":\"15677\",\"attributes\":{\"ref_id\":\"b41\"}},{\"start\":\"15871\",\"end\":\"15875\",\"attributes\":{\"ref_id\":\"b42\"}},{\"start\":\"18121\",\"end\":\"18125\",\"attributes\":{\"ref_id\":\"b43\"}},{\"start\":\"19049\",\"end\":\"19053\",\"attributes\":{\"ref_id\":\"b44\"}},{\"start\":\"19464\",\"end\":\"19468\",\"attributes\":{\"ref_id\":\"b45\"}},{\"start\":\"19902\",\"end\":\"19906\",\"attributes\":{\"ref_id\":\"b46\"}}]", "figure": "[{\"start\":\"23976\",\"end\":\"24132\",\"attributes\":{\"id\":\"fig_0\"}},{\"start\":\"24133\",\"end\":\"24150\",\"attributes\":{\"id\":\"fig_1\"}},{\"start\":\"24151\",\"end\":\"24167\",\"attributes\":{\"id\":\"fig_2\"}},{\"start\":\"24168\",\"end\":\"24202\",\"attributes\":{\"id\":\"fig_3\"}},{\"start\":\"24203\",\"end\":\"24231\",\"attributes\":{\"id\":\"fig_4\"}},{\"start\":\"24232\",\"end\":\"24246\",\"attributes\":{\"id\":\"fig_5\"}},{\"start\":\"24247\",\"end\":\"24282\",\"attributes\":{\"id\":\"fig_6\"}},{\"start\":\"24283\",\"end\":\"24338\",\"attributes\":{\"id\":\"fig_7\"}},{\"start\":\"24339\",\"end\":\"24425\",\"attributes\":{\"id\":\"fig_8\"}},{\"start\":\"24426\",\"end\":\"24773\",\"attributes\":{\"id\":\"fig_9\"}},{\"start\":\"24774\",\"end\":\"24822\",\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"}}]", "paragraph": "[{\"start\":\"2551\",\"end\":\"3129\"},{\"start\":\"3131\",\"end\":\"4281\"},{\"start\":\"4283\",\"end\":\"4711\"},{\"start\":\"4713\",\"end\":\"4759\"},{\"start\":\"4761\",\"end\":\"5134\"},{\"start\":\"5136\",\"end\":\"5243\"},{\"start\":\"5245\",\"end\":\"5676\"},{\"start\":\"5720\",\"end\":\"7094\"},{\"start\":\"7096\",\"end\":\"8248\"},{\"start\":\"8250\",\"end\":\"9158\"},{\"start\":\"9160\",\"end\":\"10168\"},{\"start\":\"10170\",\"end\":\"10786\"},{\"start\":\"10788\",\"end\":\"11716\"},{\"start\":\"11718\",\"end\":\"12205\"},{\"start\":\"12207\",\"end\":\"12862\"},{\"start\":\"12864\",\"end\":\"14133\"},{\"start\":\"14145\",\"end\":\"15443\"},{\"start\":\"15461\",\"end\":\"16134\"},{\"start\":\"16136\",\"end\":\"16341\"},{\"start\":\"16385\",\"end\":\"16777\"},{\"start\":\"16869\",\"end\":\"17440\"},{\"start\":\"17463\",\"end\":\"18301\"},{\"start\":\"18384\",\"end\":\"18565\"},{\"start\":\"18606\",\"end\":\"19129\"},{\"start\":\"19156\",\"end\":\"19584\"},{\"start\":\"19586\",\"end\":\"20250\"},{\"start\":\"20268\",\"end\":\"21891\"},{\"start\":\"21904\",\"end\":\"22523\"},{\"start\":\"22538\",\"end\":\"23975\"}]", "formula": "[{\"start\":\"16342\",\"end\":\"16384\",\"attributes\":{\"id\":\"formula_0\"}},{\"start\":\"16778\",\"end\":\"16868\",\"attributes\":{\"id\":\"formula_1\"}},{\"start\":\"17441\",\"end\":\"17462\",\"attributes\":{\"id\":\"formula_2\"}},{\"start\":\"18302\",\"end\":\"18383\",\"attributes\":{\"id\":\"formula_3\"}},{\"start\":\"18566\",\"end\":\"18605\",\"attributes\":{\"id\":\"formula_4\"}}]", "table_ref": "[{\"start\":\"19976\",\"end\":\"19983\",\"attributes\":{\"ref_id\":\"tab_0\"}}]", "section_header": "[{\"start\":\"2537\",\"end\":\"2549\",\"attributes\":{\"n\":\"1\"}},{\"start\":\"5679\",\"end\":\"5718\",\"attributes\":{\"n\":\"1.1\"}},{\"start\":\"14136\",\"end\":\"14143\",\"attributes\":{\"n\":\"3\"}},{\"start\":\"15446\",\"end\":\"15459\",\"attributes\":{\"n\":\"3.1\"}},{\"start\":\"19132\",\"end\":\"19154\",\"attributes\":{\"n\":\"4\"}},{\"start\":\"20253\",\"end\":\"20266\",\"attributes\":{\"n\":\"4.1\"}},{\"start\":\"21894\",\"end\":\"21902\",\"attributes\":{\"n\":\"4.2\"}},{\"start\":\"22526\",\"end\":\"22536\",\"attributes\":{\"n\":\"5\"}},{\"start\":\"24134\",\"end\":\"24140\"},{\"start\":\"24152\",\"end\":\"24158\"},{\"start\":\"24169\",\"end\":\"24175\"},{\"start\":\"24204\",\"end\":\"24210\"},{\"start\":\"24233\",\"end\":\"24239\"},{\"start\":\"24248\",\"end\":\"24254\"},{\"start\":\"24284\",\"end\":\"24290\"},{\"start\":\"24340\",\"end\":\"24346\"},{\"start\":\"24775\",\"end\":\"24782\"}]", "table": "[{\"start\":\"24815\",\"end\":\"24822\"}]", "figure_caption": "[{\"start\":\"23978\",\"end\":\"24132\"},{\"start\":\"24142\",\"end\":\"24150\"},{\"start\":\"24160\",\"end\":\"24167\"},{\"start\":\"24177\",\"end\":\"24202\"},{\"start\":\"24212\",\"end\":\"24231\"},{\"start\":\"24241\",\"end\":\"24246\"},{\"start\":\"24256\",\"end\":\"24282\"},{\"start\":\"24292\",\"end\":\"24338\"},{\"start\":\"24348\",\"end\":\"24425\"},{\"start\":\"24428\",\"end\":\"24773\"},{\"start\":\"24784\",\"end\":\"24815\"}]", "figure_ref": "[{\"start\":\"15877\",\"end\":\"15885\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"19055\",\"end\":\"19063\",\"attributes\":{\"ref_id\":\"fig_2\"}},{\"start\":\"19395\",\"end\":\"19403\"},{\"start\":\"19908\",\"end\":\"19916\",\"attributes\":{\"ref_id\":\"fig_3\"}},{\"start\":\"20485\",\"end\":\"20493\",\"attributes\":{\"ref_id\":\"fig_4\"}},{\"start\":\"20601\",\"end\":\"20609\",\"attributes\":{\"ref_id\":\"fig_5\"}},{\"start\":\"20835\",\"end\":\"20843\",\"attributes\":{\"ref_id\":\"fig_6\"}},{\"start\":\"21206\",\"end\":\"21214\",\"attributes\":{\"ref_id\":\"fig_7\"}},{\"start\":\"21496\",\"end\":\"21504\",\"attributes\":{\"ref_id\":\"fig_8\"}},{\"start\":\"21683\",\"end\":\"21692\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"22343\",\"end\":\"22352\",\"attributes\":{\"ref_id\":\"fig_1\"}}]", "bib_author_first_name": "[{\"start\":\"26165\",\"end\":\"26166\"},{\"start\":\"26176\",\"end\":\"26177\"},{\"start\":\"26178\",\"end\":\"26181\"},{\"start\":\"26191\",\"end\":\"26192\"},{\"start\":\"26526\",\"end\":\"26527\"},{\"start\":\"26538\",\"end\":\"26539\"},{\"start\":\"26547\",\"end\":\"26548\"},{\"start\":\"26558\",\"end\":\"26559\"},{\"start\":\"26565\",\"end\":\"26566\"},{\"start\":\"26567\",\"end\":\"26568\"},{\"start\":\"26819\",\"end\":\"26820\"},{\"start\":\"26821\",\"end\":\"26822\"},{\"start\":\"26837\",\"end\":\"26838\"},{\"start\":\"26839\",\"end\":\"26840\"},{\"start\":\"26851\",\"end\":\"26852\"},{\"start\":\"26865\",\"end\":\"26866\"},{\"start\":\"27169\",\"end\":\"27170\"},{\"start\":\"27178\",\"end\":\"27179\"},{\"start\":\"27187\",\"end\":\"27188\"},{\"start\":\"27193\",\"end\":\"27194\"},{\"start\":\"27569\",\"end\":\"27570\"},{\"start\":\"27578\",\"end\":\"27579\"},{\"start\":\"27585\",\"end\":\"27586\"},{\"start\":\"27592\",\"end\":\"27593\"},{\"start\":\"27600\",\"end\":\"27601\"},{\"start\":\"27606\",\"end\":\"27607\"},{\"start\":\"27803\",\"end\":\"27804\"},{\"start\":\"27812\",\"end\":\"27813\"},{\"start\":\"27821\",\"end\":\"27822\"},{\"start\":\"27830\",\"end\":\"27831\"},{\"start\":\"27836\",\"end\":\"27837\"},{\"start\":\"28201\",\"end\":\"28202\"},{\"start\":\"28209\",\"end\":\"28210\"},{\"start\":\"28216\",\"end\":\"28217\"},{\"start\":\"28218\",\"end\":\"28219\"},{\"start\":\"28467\",\"end\":\"28468\"},{\"start\":\"28481\",\"end\":\"28482\"},{\"start\":\"28483\",\"end\":\"28484\"},{\"start\":\"28711\",\"end\":\"28712\"},{\"start\":\"28720\",\"end\":\"28721\"},{\"start\":\"28730\",\"end\":\"28731\"},{\"start\":\"28931\",\"end\":\"28932\"},{\"start\":\"28939\",\"end\":\"28940\"},{\"start\":\"28948\",\"end\":\"28949\"},{\"start\":\"29320\",\"end\":\"29321\"},{\"start\":\"29333\",\"end\":\"29334\"},{\"start\":\"29346\",\"end\":\"29347\"},{\"start\":\"29674\",\"end\":\"29675\"},{\"start\":\"29681\",\"end\":\"29682\"},{\"start\":\"29688\",\"end\":\"29689\"},{\"start\":\"29696\",\"end\":\"29697\"},{\"start\":\"29704\",\"end\":\"29705\"},{\"start\":\"29711\",\"end\":\"29712\"},{\"start\":\"29917\",\"end\":\"29918\"},{\"start\":\"29926\",\"end\":\"29927\"},{\"start\":\"29937\",\"end\":\"29938\"},{\"start\":\"29944\",\"end\":\"29945\"},{\"start\":\"29959\",\"end\":\"29960\"},{\"start\":\"29961\",\"end\":\"29962\"},{\"start\":\"30282\",\"end\":\"30283\"},{\"start\":\"30284\",\"end\":\"30285\"},{\"start\":\"30292\",\"end\":\"30293\"},{\"start\":\"30299\",\"end\":\"30300\"},{\"start\":\"30301\",\"end\":\"30302\"},{\"start\":\"30310\",\"end\":\"30311\"},{\"start\":\"30665\",\"end\":\"30666\"},{\"start\":\"30672\",\"end\":\"30673\"},{\"start\":\"30679\",\"end\":\"30680\"},{\"start\":\"30864\",\"end\":\"30865\"},{\"start\":\"30871\",\"end\":\"30872\"},{\"start\":\"30879\",\"end\":\"30880\"},{\"start\":\"31076\",\"end\":\"31077\"},{\"start\":\"31078\",\"end\":\"31079\"},{\"start\":\"31085\",\"end\":\"31086\"},{\"start\":\"31093\",\"end\":\"31094\"},{\"start\":\"31095\",\"end\":\"31096\"},{\"start\":\"31104\",\"end\":\"31105\"},{\"start\":\"31106\",\"end\":\"31107\"},{\"start\":\"31477\",\"end\":\"31478\"},{\"start\":\"31487\",\"end\":\"31488\"},{\"start\":\"31498\",\"end\":\"31499\"},{\"start\":\"31510\",\"end\":\"31511\"},{\"start\":\"31697\",\"end\":\"31698\"},{\"start\":\"31708\",\"end\":\"31709\"},{\"start\":\"31715\",\"end\":\"31716\"},{\"start\":\"31722\",\"end\":\"31723\"},{\"start\":\"31734\",\"end\":\"31735\"},{\"start\":\"31742\",\"end\":\"31743\"},{\"start\":\"31754\",\"end\":\"31755\"},{\"start\":\"31763\",\"end\":\"31764\"},{\"start\":\"32222\",\"end\":\"32223\"},{\"start\":\"32228\",\"end\":\"32229\"},{\"start\":\"32237\",\"end\":\"32238\"},{\"start\":\"32244\",\"end\":\"32245\"},{\"start\":\"32502\",\"end\":\"32503\"},{\"start\":\"32504\",\"end\":\"32505\"},{\"start\":\"32517\",\"end\":\"32518\"},{\"start\":\"32528\",\"end\":\"32529\"},{\"start\":\"32539\",\"end\":\"32540\"},{\"start\":\"32830\",\"end\":\"32831\"},{\"start\":\"32841\",\"end\":\"32842\"},{\"start\":\"32857\",\"end\":\"32858\"},{\"start\":\"33273\",\"end\":\"33274\"},{\"start\":\"33280\",\"end\":\"33281\"},{\"start\":\"33287\",\"end\":\"33288\"},{\"start\":\"33596\",\"end\":\"33597\"},{\"start\":\"33607\",\"end\":\"33608\"},{\"start\":\"33995\",\"end\":\"33996\"},{\"start\":\"34004\",\"end\":\"34005\"},{\"start\":\"34019\",\"end\":\"34020\"},{\"start\":\"34353\",\"end\":\"34354\"},{\"start\":\"34366\",\"end\":\"34367\"},{\"start\":\"34380\",\"end\":\"34381\"},{\"start\":\"34555\",\"end\":\"34556\"},{\"start\":\"34565\",\"end\":\"34566\"},{\"start\":\"34578\",\"end\":\"34579\"},{\"start\":\"34957\",\"end\":\"34958\"},{\"start\":\"35193\",\"end\":\"35194\"},{\"start\":\"35200\",\"end\":\"35201\"},{\"start\":\"35209\",\"end\":\"35210\"},{\"start\":\"35221\",\"end\":\"35222\"},{\"start\":\"35227\",\"end\":\"35228\"},{\"start\":\"35532\",\"end\":\"35533\"},{\"start\":\"35545\",\"end\":\"35546\"},{\"start\":\"35558\",\"end\":\"35559\"},{\"start\":\"36071\",\"end\":\"36072\"},{\"start\":\"36084\",\"end\":\"36085\"},{\"start\":\"36097\",\"end\":\"36098\"},{\"start\":\"36473\",\"end\":\"36474\"},{\"start\":\"36649\",\"end\":\"36650\"},{\"start\":\"36651\",\"end\":\"36652\"},{\"start\":\"36965\",\"end\":\"36966\"},{\"start\":\"36974\",\"end\":\"36975\"},{\"start\":\"36987\",\"end\":\"36988\"},{\"start\":\"36997\",\"end\":\"36998\"},{\"start\":\"37013\",\"end\":\"37014\"},{\"start\":\"37022\",\"end\":\"37023\"},{\"start\":\"37340\",\"end\":\"37344\"},{\"start\":\"37355\",\"end\":\"37356\"},{\"start\":\"37363\",\"end\":\"37364\"},{\"start\":\"37699\",\"end\":\"37700\"},{\"start\":\"37712\",\"end\":\"37713\"},{\"start\":\"37725\",\"end\":\"37726\"},{\"start\":\"37736\",\"end\":\"37737\"},{\"start\":\"38106\",\"end\":\"38107\"},{\"start\":\"38119\",\"end\":\"38120\"},{\"start\":\"38317\",\"end\":\"38318\"},{\"start\":\"38319\",\"end\":\"38320\"},{\"start\":\"38328\",\"end\":\"38329\"},{\"start\":\"38341\",\"end\":\"38342\"},{\"start\":\"38343\",\"end\":\"38344\"},{\"start\":\"38803\",\"end\":\"38804\"},{\"start\":\"38812\",\"end\":\"38813\"},{\"start\":\"38825\",\"end\":\"38826\"},{\"start\":\"38838\",\"end\":\"38839\"},{\"start\":\"38848\",\"end\":\"38852\"},{\"start\":\"38858\",\"end\":\"38859\"},{\"start\":\"39250\",\"end\":\"39251\"},{\"start\":\"39260\",\"end\":\"39261\"},{\"start\":\"39262\",\"end\":\"39263\"},{\"start\":\"39270\",\"end\":\"39271\"},{\"start\":\"39742\",\"end\":\"39743\"},{\"start\":\"39751\",\"end\":\"39752\"},{\"start\":\"39760\",\"end\":\"39761\"},{\"start\":\"39762\",\"end\":\"39763\"},{\"start\":\"39772\",\"end\":\"39773\"},{\"start\":\"39785\",\"end\":\"39786\"},{\"start\":\"39792\",\"end\":\"39793\"},{\"start\":\"40149\",\"end\":\"40153\"},{\"start\":\"40414\",\"end\":\"40415\"},{\"start\":\"40423\",\"end\":\"40424\"},{\"start\":\"40429\",\"end\":\"40430\"},{\"start\":\"40672\",\"end\":\"40673\"},{\"start\":\"40684\",\"end\":\"40685\"},{\"start\":\"40928\",\"end\":\"40929\"},{\"start\":\"41211\",\"end\":\"41212\"},{\"start\":\"41220\",\"end\":\"41221\"},{\"start\":\"41615\",\"end\":\"41616\"},{\"start\":\"41626\",\"end\":\"41627\"},{\"start\":\"41639\",\"end\":\"41640\"},{\"start\":\"41648\",\"end\":\"41649\"},{\"start\":\"41658\",\"end\":\"41659\"}]", "bib_author_last_name": "[{\"start\":\"26167\",\"end\":\"26174\"},{\"start\":\"26182\",\"end\":\"26189\"},{\"start\":\"26193\",\"end\":\"26198\"},{\"start\":\"26528\",\"end\":\"26536\"},{\"start\":\"26540\",\"end\":\"26545\"},{\"start\":\"26549\",\"end\":\"26556\"},{\"start\":\"26560\",\"end\":\"26563\"},{\"start\":\"26569\",\"end\":\"26573\"},{\"start\":\"26823\",\"end\":\"26835\"},{\"start\":\"26841\",\"end\":\"26849\"},{\"start\":\"26853\",\"end\":\"26863\"},{\"start\":\"26867\",\"end\":\"26874\"},{\"start\":\"27171\",\"end\":\"27176\"},{\"start\":\"27180\",\"end\":\"27185\"},{\"start\":\"27189\",\"end\":\"27191\"},{\"start\":\"27195\",\"end\":\"27198\"},{\"start\":\"27571\",\"end\":\"27576\"},{\"start\":\"27580\",\"end\":\"27583\"},{\"start\":\"27587\",\"end\":\"27590\"},{\"start\":\"27594\",\"end\":\"27598\"},{\"start\":\"27602\",\"end\":\"27604\"},{\"start\":\"27608\",\"end\":\"27613\"},{\"start\":\"27805\",\"end\":\"27810\"},{\"start\":\"27814\",\"end\":\"27819\"},{\"start\":\"27823\",\"end\":\"27828\"},{\"start\":\"27832\",\"end\":\"27834\"},{\"start\":\"27838\",\"end\":\"27841\"},{\"start\":\"28203\",\"end\":\"28207\"},{\"start\":\"28211\",\"end\":\"28214\"},{\"start\":\"28220\",\"end\":\"28225\"},{\"start\":\"28469\",\"end\":\"28479\"},{\"start\":\"28485\",\"end\":\"28494\"},{\"start\":\"28496\",\"end\":\"28502\"},{\"start\":\"28713\",\"end\":\"28718\"},{\"start\":\"28722\",\"end\":\"28728\"},{\"start\":\"28732\",\"end\":\"28738\"},{\"start\":\"28933\",\"end\":\"28937\"},{\"start\":\"28941\",\"end\":\"28946\"},{\"start\":\"28950\",\"end\":\"28957\"},{\"start\":\"29322\",\"end\":\"29331\"},{\"start\":\"29335\",\"end\":\"29344\"},{\"start\":\"29348\",\"end\":\"29353\"},{\"start\":\"29676\",\"end\":\"29679\"},{\"start\":\"29683\",\"end\":\"29686\"},{\"start\":\"29690\",\"end\":\"29694\"},{\"start\":\"29698\",\"end\":\"29702\"},{\"start\":\"29706\",\"end\":\"29709\"},{\"start\":\"29713\",\"end\":\"29715\"},{\"start\":\"29919\",\"end\":\"29924\"},{\"start\":\"29928\",\"end\":\"29935\"},{\"start\":\"29939\",\"end\":\"29942\"},{\"start\":\"29946\",\"end\":\"29957\"},{\"start\":\"29963\",\"end\":\"29967\"},{\"start\":\"30286\",\"end\":\"30290\"},{\"start\":\"30294\",\"end\":\"30297\"},{\"start\":\"30303\",\"end\":\"30308\"},{\"start\":\"30312\",\"end\":\"30315\"},{\"start\":\"30667\",\"end\":\"30670\"},{\"start\":\"30674\",\"end\":\"30677\"},{\"start\":\"30681\",\"end\":\"30686\"},{\"start\":\"30866\",\"end\":\"30869\"},{\"start\":\"30873\",\"end\":\"30877\"},{\"start\":\"30881\",\"end\":\"30884\"},{\"start\":\"31080\",\"end\":\"31083\"},{\"start\":\"31087\",\"end\":\"31091\"},{\"start\":\"31097\",\"end\":\"31102\"},{\"start\":\"31108\",\"end\":\"31111\"},{\"start\":\"31479\",\"end\":\"31485\"},{\"start\":\"31489\",\"end\":\"31496\"},{\"start\":\"31500\",\"end\":\"31508\"},{\"start\":\"31512\",\"end\":\"31519\"},{\"start\":\"31699\",\"end\":\"31706\"},{\"start\":\"31710\",\"end\":\"31713\"},{\"start\":\"31717\",\"end\":\"31720\"},{\"start\":\"31724\",\"end\":\"31732\"},{\"start\":\"31736\",\"end\":\"31740\"},{\"start\":\"31744\",\"end\":\"31752\"},{\"start\":\"31756\",\"end\":\"31761\"},{\"start\":\"31765\",\"end\":\"31774\"},{\"start\":\"31776\",\"end\":\"31786\"},{\"start\":\"32224\",\"end\":\"32226\"},{\"start\":\"32230\",\"end\":\"32235\"},{\"start\":\"32239\",\"end\":\"32242\"},{\"start\":\"32246\",\"end\":\"32249\"},{\"start\":\"32506\",\"end\":\"32515\"},{\"start\":\"32519\",\"end\":\"32526\"},{\"start\":\"32530\",\"end\":\"32537\"},{\"start\":\"32541\",\"end\":\"32548\"},{\"start\":\"32832\",\"end\":\"32839\"},{\"start\":\"32843\",\"end\":\"32855\"},{\"start\":\"32859\",\"end\":\"32866\"},{\"start\":\"33275\",\"end\":\"33278\"},{\"start\":\"33282\",\"end\":\"33285\"},{\"start\":\"33289\",\"end\":\"33294\"},{\"start\":\"33598\",\"end\":\"33605\"},{\"start\":\"33609\",\"end\":\"33612\"},{\"start\":\"33997\",\"end\":\"34002\"},{\"start\":\"34006\",\"end\":\"34017\"},{\"start\":\"34021\",\"end\":\"34028\"},{\"start\":\"34355\",\"end\":\"34364\"},{\"start\":\"34368\",\"end\":\"34378\"},{\"start\":\"34382\",\"end\":\"34388\"},{\"start\":\"34557\",\"end\":\"34563\"},{\"start\":\"34567\",\"end\":\"34576\"},{\"start\":\"34580\",\"end\":\"34592\"},{\"start\":\"34959\",\"end\":\"34964\"},{\"start\":\"35195\",\"end\":\"35198\"},{\"start\":\"35202\",\"end\":\"35207\"},{\"start\":\"35211\",\"end\":\"35219\"},{\"start\":\"35223\",\"end\":\"35225\"},{\"start\":\"35229\",\"end\":\"35235\"},{\"start\":\"35534\",\"end\":\"35543\"},{\"start\":\"35547\",\"end\":\"35556\"},{\"start\":\"35560\",\"end\":\"35565\"},{\"start\":\"36073\",\"end\":\"36082\"},{\"start\":\"36086\",\"end\":\"36095\"},{\"start\":\"36099\",\"end\":\"36104\"},{\"start\":\"36475\",\"end\":\"36478\"},{\"start\":\"36653\",\"end\":\"36660\"},{\"start\":\"36967\",\"end\":\"36972\"},{\"start\":\"36976\",\"end\":\"36985\"},{\"start\":\"36989\",\"end\":\"36995\"},{\"start\":\"36999\",\"end\":\"37011\"},{\"start\":\"37015\",\"end\":\"37020\"},{\"start\":\"37024\",\"end\":\"37029\"},{\"start\":\"37345\",\"end\":\"37353\"},{\"start\":\"37357\",\"end\":\"37361\"},{\"start\":\"37365\",\"end\":\"37370\"},{\"start\":\"37701\",\"end\":\"37710\"},{\"start\":\"37714\",\"end\":\"37723\"},{\"start\":\"37727\",\"end\":\"37734\"},{\"start\":\"37738\",\"end\":\"37749\"},{\"start\":\"38108\",\"end\":\"38117\"},{\"start\":\"38121\",\"end\":\"38132\"},{\"start\":\"38321\",\"end\":\"38326\"},{\"start\":\"38330\",\"end\":\"38339\"},{\"start\":\"38345\",\"end\":\"38350\"},{\"start\":\"38805\",\"end\":\"38810\"},{\"start\":\"38814\",\"end\":\"38823\"},{\"start\":\"38827\",\"end\":\"38836\"},{\"start\":\"38840\",\"end\":\"38846\"},{\"start\":\"38853\",\"end\":\"38856\"},{\"start\":\"38860\",\"end\":\"38872\"},{\"start\":\"39252\",\"end\":\"39258\"},{\"start\":\"39264\",\"end\":\"39268\"},{\"start\":\"39272\",\"end\":\"39278\"},{\"start\":\"39744\",\"end\":\"39749\"},{\"start\":\"39753\",\"end\":\"39758\"},{\"start\":\"39764\",\"end\":\"39770\"},{\"start\":\"39774\",\"end\":\"39783\"},{\"start\":\"39787\",\"end\":\"39790\"},{\"start\":\"39794\",\"end\":\"39803\"},{\"start\":\"40154\",\"end\":\"40157\"},{\"start\":\"40416\",\"end\":\"40421\"},{\"start\":\"40425\",\"end\":\"40427\"},{\"start\":\"40431\",\"end\":\"40435\"},{\"start\":\"40674\",\"end\":\"40682\"},{\"start\":\"40686\",\"end\":\"40695\"},{\"start\":\"40930\",\"end\":\"40937\"},{\"start\":\"41213\",\"end\":\"41218\"},{\"start\":\"41222\",\"end\":\"41229\"},{\"start\":\"41617\",\"end\":\"41624\"},{\"start\":\"41628\",\"end\":\"41637\"},{\"start\":\"41641\",\"end\":\"41646\"},{\"start\":\"41650\",\"end\":\"41656\"},{\"start\":\"41660\",\"end\":\"41665\"},{\"start\":\"42267\",\"end\":\"42275\"}]", "bib_entry": "[{\"start\":\"26165\",\"end\":\"26449\",\"attributes\":{\"id\":\"b0\",\"doi\":\"arXiv:1807.04606\"}},{\"start\":\"26451\",\"end\":\"26751\",\"attributes\":{\"matched_paper_id\":\"5019033\",\"id\":\"b1\"}},{\"start\":\"26753\",\"end\":\"27109\",\"attributes\":{\"matched_paper_id\":\"3198903\",\"id\":\"b2\"}},{\"start\":\"27111\",\"end\":\"27484\",\"attributes\":{\"matched_paper_id\":\"16964936\",\"id\":\"b3\"}},{\"start\":\"27486\",\"end\":\"27801\",\"attributes\":{\"matched_paper_id\":\"43909681\",\"id\":\"b4\"}},{\"start\":\"27803\",\"end\":\"28123\",\"attributes\":{\"id\":\"b5\"}},{\"start\":\"28125\",\"end\":\"28400\",\"attributes\":{\"matched_paper_id\":\"31045684\",\"id\":\"b6\"}},{\"start\":\"28402\",\"end\":\"28694\",\"attributes\":{\"matched_paper_id\":\"195908774\",\"id\":\"b7\"}},{\"start\":\"28696\",\"end\":\"28834\",\"attributes\":{\"matched_paper_id\":\"1779661\",\"id\":\"b8\"}},{\"start\":\"28836\",\"end\":\"29238\",\"attributes\":{\"matched_paper_id\":\"198496353\",\"id\":\"b9\",\"doi\":\"10.1186/s41074-019-0059-x\"}},{\"start\":\"29240\",\"end\":\"29619\",\"attributes\":{\"id\":\"b10\",\"doi\":\"10.3923/ajit.2016.4986.4994\"}},{\"start\":\"29621\",\"end\":\"29913\",\"attributes\":{\"matched_paper_id\":\"20592770\",\"id\":\"b11\"}},{\"start\":\"29915\",\"end\":\"30181\",\"attributes\":{\"id\":\"b12\",\"doi\":\"10.1016/j.compeleceng.2017.05.033\"}},{\"start\":\"30183\",\"end\":\"30593\",\"attributes\":{\"matched_paper_id\":\"12753792\",\"id\":\"b13\",\"doi\":\"10.1007/s11042-012-1029-9\"}},{\"start\":\"30595\",\"end\":\"30862\",\"attributes\":{\"matched_paper_id\":\"30661749\",\"id\":\"b14\"}},{\"start\":\"30864\",\"end\":\"31074\",\"attributes\":{\"id\":\"b15\"}},{\"start\":\"31076\",\"end\":\"31418\",\"attributes\":{\"id\":\"b16\"}},{\"start\":\"31420\",\"end\":\"31695\",\"attributes\":{\"id\":\"b17\",\"doi\":\"arXiv:1506.02640\"}},{\"start\":\"31697\",\"end\":\"32220\",\"attributes\":{\"id\":\"b18\"}},{\"start\":\"32222\",\"end\":\"32444\",\"attributes\":{\"id\":\"b19\",\"doi\":\"arXiv:1512.03385\"}},{\"start\":\"32446\",\"end\":\"32762\",\"attributes\":{\"matched_paper_id\":\"5378407\",\"id\":\"b20\"}},{\"start\":\"32764\",\"end\":\"33214\",\"attributes\":{\"matched_paper_id\":\"206592766\",\"id\":\"b21\"}},{\"start\":\"33216\",\"end\":\"33453\",\"attributes\":{\"matched_paper_id\":\"122661576\",\"id\":\"b22\"}},{\"start\":\"33455\",\"end\":\"33892\",\"attributes\":{\"matched_paper_id\":\"199015932\",\"id\":\"b23\",\"doi\":\"10.3390/s19153371\"}},{\"start\":\"33894\",\"end\":\"34266\",\"attributes\":{\"matched_paper_id\":\"14540685\",\"id\":\"b24\"}},{\"start\":\"34268\",\"end\":\"34553\",\"attributes\":{\"matched_paper_id\":\"203923287\",\"id\":\"b25\"}},{\"start\":\"34555\",\"end\":\"34870\",\"attributes\":{\"id\":\"b26\",\"doi\":\"arXiv:1807.06789v1[cs.CV]\"}},{\"start\":\"34872\",\"end\":\"35152\",\"attributes\":{\"matched_paper_id\":\"212576108\",\"id\":\"b27\"}},{\"start\":\"35154\",\"end\":\"35530\",\"attributes\":{\"matched_paper_id\":\"47252984\",\"id\":\"b28\",\"doi\":\"10.1109/ICCV.2017.324\"}},{\"start\":\"35532\",\"end\":\"35956\",\"attributes\":{\"id\":\"b29\",\"doi\":\"10.4018/978-1-5225-9023-1.ch005\"}},{\"start\":\"35958\",\"end\":\"36469\",\"attributes\":{\"matched_paper_id\":\"192565095\",\"id\":\"b30\",\"doi\":\"10.4018/978-1-5225-9023-1.ch006\"}},{\"start\":\"36471\",\"end\":\"36563\",\"attributes\":{\"id\":\"b31\"}},{\"start\":\"36565\",\"end\":\"36879\",\"attributes\":{\"matched_paper_id\":\"216289333\",\"id\":\"b32\",\"doi\":\"10.1016/j.cja.2020.02.024\"}},{\"start\":\"36881\",\"end\":\"37270\",\"attributes\":{\"matched_paper_id\":\"127953797\",\"id\":\"b33\"}},{\"start\":\"37272\",\"end\":\"37611\",\"attributes\":{\"matched_paper_id\":\"208005084\",\"id\":\"b34\",\"doi\":\"10.1117/1.JRS.13.044509\"}},{\"start\":\"37613\",\"end\":\"38006\",\"attributes\":{\"matched_paper_id\":\"112116815\",\"id\":\"b35\",\"doi\":\"10.14429/dsj.66.9205\"}},{\"start\":\"38008\",\"end\":\"38315\",\"attributes\":{\"matched_paper_id\":\"8183508\",\"id\":\"b36\"}},{\"start\":\"38317\",\"end\":\"38632\",\"attributes\":{\"id\":\"b37\"}},{\"start\":\"38634\",\"end\":\"39248\",\"attributes\":{\"matched_paper_id\":\"52924634\",\"id\":\"b38\",\"doi\":\"10.1007/s00521-018-3788-3\"}},{\"start\":\"39250\",\"end\":\"39603\",\"attributes\":{\"id\":\"b39\",\"doi\":\"arXiv:2004.01059\"}},{\"start\":\"39605\",\"end\":\"40066\",\"attributes\":{\"matched_paper_id\":\"215792112\",\"id\":\"b40\"}},{\"start\":\"40068\",\"end\":\"40321\",\"attributes\":{\"matched_paper_id\":\"109391171\",\"id\":\"b41\"}},{\"start\":\"40323\",\"end\":\"40670\",\"attributes\":{\"matched_paper_id\":\"20647475\",\"id\":\"b42\"}},{\"start\":\"40672\",\"end\":\"40926\",\"attributes\":{\"id\":\"b43\"}},{\"start\":\"40928\",\"end\":\"41209\",\"attributes\":{\"id\":\"b44\"}},{\"start\":\"41211\",\"end\":\"41554\",\"attributes\":{\"id\":\"b45\"}},{\"start\":\"41556\",\"end\":\"41988\",\"attributes\":{\"matched_paper_id\":\"206593880\",\"id\":\"b46\"}},{\"start\":\"41990\",\"end\":\"42024\",\"attributes\":{\"id\":\"b47\"}},{\"start\":\"42026\",\"end\":\"42263\",\"attributes\":{\"id\":\"b48\"}},{\"start\":\"42265\",\"end\":\"42414\",\"attributes\":{\"id\":\"b49\"}}]", "bib_title": "[{\"start\":\"26451\",\"end\":\"26524\"},{\"start\":\"26753\",\"end\":\"26817\"},{\"start\":\"27111\",\"end\":\"27167\"},{\"start\":\"27486\",\"end\":\"27567\"},{\"start\":\"28125\",\"end\":\"28199\"},{\"start\":\"28402\",\"end\":\"28465\"},{\"start\":\"28696\",\"end\":\"28709\"},{\"start\":\"28836\",\"end\":\"28929\"},{\"start\":\"29240\",\"end\":\"29318\"},{\"start\":\"29621\",\"end\":\"29672\"},{\"start\":\"30183\",\"end\":\"30280\"},{\"start\":\"30595\",\"end\":\"30663\"},{\"start\":\"32446\",\"end\":\"32500\"},{\"start\":\"32764\",\"end\":\"32828\"},{\"start\":\"33216\",\"end\":\"33271\"},{\"start\":\"33455\",\"end\":\"33594\"},{\"start\":\"33894\",\"end\":\"33993\"},{\"start\":\"34268\",\"end\":\"34351\"},{\"start\":\"34872\",\"end\":\"34955\"},{\"start\":\"35154\",\"end\":\"35191\"},{\"start\":\"35958\",\"end\":\"36069\"},{\"start\":\"36565\",\"end\":\"36647\"},{\"start\":\"36881\",\"end\":\"36963\"},{\"start\":\"37272\",\"end\":\"37338\"},{\"start\":\"37613\",\"end\":\"37697\"},{\"start\":\"38008\",\"end\":\"38104\"},{\"start\":\"38634\",\"end\":\"38801\"},{\"start\":\"39605\",\"end\":\"39740\"},{\"start\":\"40068\",\"end\":\"40147\"},{\"start\":\"40323\",\"end\":\"40412\"},{\"start\":\"41556\",\"end\":\"41613\"}]", "bib_author": "[{\"start\":\"26165\",\"end\":\"26176\"},{\"start\":\"26176\",\"end\":\"26191\"},{\"start\":\"26191\",\"end\":\"26200\"},{\"start\":\"26526\",\"end\":\"26538\"},{\"start\":\"26538\",\"end\":\"26547\"},{\"start\":\"26547\",\"end\":\"26558\"},{\"start\":\"26558\",\"end\":\"26565\"},{\"start\":\"26565\",\"end\":\"26575\"},{\"start\":\"26819\",\"end\":\"26837\"},{\"start\":\"26837\",\"end\":\"26851\"},{\"start\":\"26851\",\"end\":\"26865\"},{\"start\":\"26865\",\"end\":\"26876\"},{\"start\":\"27169\",\"end\":\"27178\"},{\"start\":\"27178\",\"end\":\"27187\"},{\"start\":\"27187\",\"end\":\"27193\"},{\"start\":\"27193\",\"end\":\"27200\"},{\"start\":\"27569\",\"end\":\"27578\"},{\"start\":\"27578\",\"end\":\"27585\"},{\"start\":\"27585\",\"end\":\"27592\"},{\"start\":\"27592\",\"end\":\"27600\"},{\"start\":\"27600\",\"end\":\"27606\"},{\"start\":\"27606\",\"end\":\"27615\"},{\"start\":\"27803\",\"end\":\"27812\"},{\"start\":\"27812\",\"end\":\"27821\"},{\"start\":\"27821\",\"end\":\"27830\"},{\"start\":\"27830\",\"end\":\"27836\"},{\"start\":\"27836\",\"end\":\"27843\"},{\"start\":\"28201\",\"end\":\"28209\"},{\"start\":\"28209\",\"end\":\"28216\"},{\"start\":\"28216\",\"end\":\"28227\"},{\"start\":\"28467\",\"end\":\"28481\"},{\"start\":\"28481\",\"end\":\"28496\"},{\"start\":\"28496\",\"end\":\"28504\"},{\"start\":\"28711\",\"end\":\"28720\"},{\"start\":\"28720\",\"end\":\"28730\"},{\"start\":\"28730\",\"end\":\"28740\"},{\"start\":\"28931\",\"end\":\"28939\"},{\"start\":\"28939\",\"end\":\"28948\"},{\"start\":\"28948\",\"end\":\"28959\"},{\"start\":\"29320\",\"end\":\"29333\"},{\"start\":\"29333\",\"end\":\"29346\"},{\"start\":\"29346\",\"end\":\"29355\"},{\"start\":\"29674\",\"end\":\"29681\"},{\"start\":\"29681\",\"end\":\"29688\"},{\"start\":\"29688\",\"end\":\"29696\"},{\"start\":\"29696\",\"end\":\"29704\"},{\"start\":\"29704\",\"end\":\"29711\"},{\"start\":\"29711\",\"end\":\"29717\"},{\"start\":\"29917\",\"end\":\"29926\"},{\"start\":\"29926\",\"end\":\"29937\"},{\"start\":\"29937\",\"end\":\"29944\"},{\"start\":\"29944\",\"end\":\"29959\"},{\"start\":\"29959\",\"end\":\"29969\"},{\"start\":\"30282\",\"end\":\"30292\"},{\"start\":\"30292\",\"end\":\"30299\"},{\"start\":\"30299\",\"end\":\"30310\"},{\"start\":\"30310\",\"end\":\"30317\"},{\"start\":\"30665\",\"end\":\"30672\"},{\"start\":\"30672\",\"end\":\"30679\"},{\"start\":\"30679\",\"end\":\"30688\"},{\"start\":\"30864\",\"end\":\"30871\"},{\"start\":\"30871\",\"end\":\"30879\"},{\"start\":\"30879\",\"end\":\"30886\"},{\"start\":\"31076\",\"end\":\"31085\"},{\"start\":\"31085\",\"end\":\"31093\"},{\"start\":\"31093\",\"end\":\"31104\"},{\"start\":\"31104\",\"end\":\"31113\"},{\"start\":\"31477\",\"end\":\"31487\"},{\"start\":\"31487\",\"end\":\"31498\"},{\"start\":\"31498\",\"end\":\"31510\"},{\"start\":\"31510\",\"end\":\"31521\"},{\"start\":\"31697\",\"end\":\"31708\"},{\"start\":\"31708\",\"end\":\"31715\"},{\"start\":\"31715\",\"end\":\"31722\"},{\"start\":\"31722\",\"end\":\"31734\"},{\"start\":\"31734\",\"end\":\"31742\"},{\"start\":\"31742\",\"end\":\"31754\"},{\"start\":\"31754\",\"end\":\"31763\"},{\"start\":\"31763\",\"end\":\"31776\"},{\"start\":\"31776\",\"end\":\"31788\"},{\"start\":\"32222\",\"end\":\"32228\"},{\"start\":\"32228\",\"end\":\"32237\"},{\"start\":\"32237\",\"end\":\"32244\"},{\"start\":\"32244\",\"end\":\"32251\"},{\"start\":\"32502\",\"end\":\"32517\"},{\"start\":\"32517\",\"end\":\"32528\"},{\"start\":\"32528\",\"end\":\"32539\"},{\"start\":\"32539\",\"end\":\"32550\"},{\"start\":\"32830\",\"end\":\"32841\"},{\"start\":\"32841\",\"end\":\"32857\"},{\"start\":\"32857\",\"end\":\"32868\"},{\"start\":\"33273\",\"end\":\"33280\"},{\"start\":\"33280\",\"end\":\"33287\"},{\"start\":\"33287\",\"end\":\"33296\"},{\"start\":\"33596\",\"end\":\"33607\"},{\"start\":\"33607\",\"end\":\"33614\"},{\"start\":\"33995\",\"end\":\"34004\"},{\"start\":\"34004\",\"end\":\"34019\"},{\"start\":\"34019\",\"end\":\"34030\"},{\"start\":\"34353\",\"end\":\"34366\"},{\"start\":\"34366\",\"end\":\"34380\"},{\"start\":\"34380\",\"end\":\"34390\"},{\"start\":\"34555\",\"end\":\"34565\"},{\"start\":\"34565\",\"end\":\"34578\"},{\"start\":\"34578\",\"end\":\"34594\"},{\"start\":\"34957\",\"end\":\"34966\"},{\"start\":\"35193\",\"end\":\"35200\"},{\"start\":\"35200\",\"end\":\"35209\"},{\"start\":\"35209\",\"end\":\"35221\"},{\"start\":\"35221\",\"end\":\"35227\"},{\"start\":\"35227\",\"end\":\"35237\"},{\"start\":\"35532\",\"end\":\"35545\"},{\"start\":\"35545\",\"end\":\"35558\"},{\"start\":\"35558\",\"end\":\"35567\"},{\"start\":\"36071\",\"end\":\"36084\"},{\"start\":\"36084\",\"end\":\"36097\"},{\"start\":\"36097\",\"end\":\"36106\"},{\"start\":\"36473\",\"end\":\"36480\"},{\"start\":\"36649\",\"end\":\"36662\"},{\"start\":\"36965\",\"end\":\"36974\"},{\"start\":\"36974\",\"end\":\"36987\"},{\"start\":\"36987\",\"end\":\"36997\"},{\"start\":\"36997\",\"end\":\"37013\"},{\"start\":\"37013\",\"end\":\"37022\"},{\"start\":\"37022\",\"end\":\"37031\"},{\"start\":\"37340\",\"end\":\"37355\"},{\"start\":\"37355\",\"end\":\"37363\"},{\"start\":\"37363\",\"end\":\"37372\"},{\"start\":\"37699\",\"end\":\"37712\"},{\"start\":\"37712\",\"end\":\"37725\"},{\"start\":\"37725\",\"end\":\"37736\"},{\"start\":\"37736\",\"end\":\"37751\"},{\"start\":\"38106\",\"end\":\"38119\"},{\"start\":\"38119\",\"end\":\"38134\"},{\"start\":\"38317\",\"end\":\"38328\"},{\"start\":\"38328\",\"end\":\"38341\"},{\"start\":\"38341\",\"end\":\"38352\"},{\"start\":\"38803\",\"end\":\"38812\"},{\"start\":\"38812\",\"end\":\"38825\"},{\"start\":\"38825\",\"end\":\"38838\"},{\"start\":\"38838\",\"end\":\"38848\"},{\"start\":\"38848\",\"end\":\"38858\"},{\"start\":\"38858\",\"end\":\"38874\"},{\"start\":\"39250\",\"end\":\"39260\"},{\"start\":\"39260\",\"end\":\"39270\"},{\"start\":\"39270\",\"end\":\"39280\"},{\"start\":\"39742\",\"end\":\"39751\"},{\"start\":\"39751\",\"end\":\"39760\"},{\"start\":\"39760\",\"end\":\"39772\"},{\"start\":\"39772\",\"end\":\"39785\"},{\"start\":\"39785\",\"end\":\"39792\"},{\"start\":\"39792\",\"end\":\"39805\"},{\"start\":\"40149\",\"end\":\"40159\"},{\"start\":\"40414\",\"end\":\"40423\"},{\"start\":\"40423\",\"end\":\"40429\"},{\"start\":\"40429\",\"end\":\"40437\"},{\"start\":\"40672\",\"end\":\"40684\"},{\"start\":\"40684\",\"end\":\"40697\"},{\"start\":\"40928\",\"end\":\"40939\"},{\"start\":\"41211\",\"end\":\"41220\"},{\"start\":\"41220\",\"end\":\"41231\"},{\"start\":\"41615\",\"end\":\"41626\"},{\"start\":\"41626\",\"end\":\"41639\"},{\"start\":\"41639\",\"end\":\"41648\"},{\"start\":\"41648\",\"end\":\"41658\"},{\"start\":\"41658\",\"end\":\"41667\"},{\"start\":\"42267\",\"end\":\"42277\"}]", "bib_venue": "[{\"start\":\"26216\",\"end\":\"26300\"},{\"start\":\"26575\",\"end\":\"26586\"},{\"start\":\"26876\",\"end\":\"26914\"},{\"start\":\"27200\",\"end\":\"27256\"},{\"start\":\"27615\",\"end\":\"27629\"},{\"start\":\"27843\",\"end\":\"27948\"},{\"start\":\"28227\",\"end\":\"28246\"},{\"start\":\"28504\",\"end\":\"28534\"},{\"start\":\"28740\",\"end\":\"28746\"},{\"start\":\"28984\",\"end\":\"29013\"},{\"start\":\"29382\",\"end\":\"29403\"},{\"start\":\"29717\",\"end\":\"29746\"},{\"start\":\"30002\",\"end\":\"30021\"},{\"start\":\"30342\",\"end\":\"30362\"},{\"start\":\"30688\",\"end\":\"30712\"},{\"start\":\"30886\",\"end\":\"30963\"},{\"start\":\"31113\",\"end\":\"31239\"},{\"start\":\"31420\",\"end\":\"31475\"},{\"start\":\"31788\",\"end\":\"31897\"},{\"start\":\"32267\",\"end\":\"32311\"},{\"start\":\"32550\",\"end\":\"32588\"},{\"start\":\"32868\",\"end\":\"32945\"},{\"start\":\"33296\",\"end\":\"33319\"},{\"start\":\"33631\",\"end\":\"33646\"},{\"start\":\"34030\",\"end\":\"34068\"},{\"start\":\"34390\",\"end\":\"34397\"},{\"start\":\"34619\",\"end\":\"34705\"},{\"start\":\"34966\",\"end\":\"34995\"},{\"start\":\"35258\",\"end\":\"35318\"},{\"start\":\"35598\",\"end\":\"35722\"},{\"start\":\"36137\",\"end\":\"36191\"},{\"start\":\"36480\",\"end\":\"36498\"},{\"start\":\"36687\",\"end\":\"36704\"},{\"start\":\"37031\",\"end\":\"37056\"},{\"start\":\"37395\",\"end\":\"37415\"},{\"start\":\"37771\",\"end\":\"37782\"},{\"start\":\"38134\",\"end\":\"38146\"},{\"start\":\"38352\",\"end\":\"38467\"},{\"start\":\"38899\",\"end\":\"38921\"},{\"start\":\"39305\",\"end\":\"39363\"},{\"start\":\"39805\",\"end\":\"39819\"},{\"start\":\"40159\",\"end\":\"40182\"},{\"start\":\"40437\",\"end\":\"40489\"},{\"start\":\"40697\",\"end\":\"40792\"},{\"start\":\"40939\",\"end\":\"41009\"},{\"start\":\"41231\",\"end\":\"41369\"},{\"start\":\"41667\",\"end\":\"41737\"},{\"start\":\"41990\",\"end\":\"42006\"},{\"start\":\"42026\",\"end\":\"42143\"},{\"start\":\"42277\",\"end\":\"42322\"},{\"start\":\"27258\",\"end\":\"27283\"},{\"start\":\"31899\",\"end\":\"31993\"},{\"start\":\"32947\",\"end\":\"33009\"},{\"start\":\"41011\",\"end\":\"41024\"},{\"start\":\"41739\",\"end\":\"41757\"}]"}}}, "year": 2023, "month": 12, "day": 17}