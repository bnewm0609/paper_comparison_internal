{"id": 258947613, "updated": "2023-11-17 15:44:40.388", "metadata": {"title": "Automated Summarization of Stack Overflow Posts", "authors": "[{\"first\":\"Bonan\",\"last\":\"Kou\",\"middle\":[]},{\"first\":\"Muhao\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Tianyi\",\"last\":\"Zhang\",\"middle\":[]}]", "venue": "2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)", "journal": "2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)", "publication_date": {"year": 2023, "month": 5, "day": 1}, "abstract": "Software developers often resort to Stack Overflow (SO) to fill their programming needs. Given the abundance of relevant posts, navigating them and comparing different solutions is tedious and time-consuming. Recent work has proposed to automatically summarize SO posts to concise text to facilitate the navigation of SO posts. However, these techniques rely only on information retrieval methods or heuristics for text summarization, which is insufficient to handle the ambiguity and sophistication of natural language. This paper presents a deep learning based framework called Assortfor SO post summarization. Assortincludes two complementary learning methods, $\\mathbf{Assort}_{S}$ and $\\mathbf{Assort}_{IS}$, to address the lack of labeled training data for SO post summarization. $\\mathbf{Assort}_{S}$ is designed to directly train a novel ensemble learning model with BERT embeddings and domain-specific features to account for the unique characteristics of SO posts. By contrast, $\\mathbf{Assort}_{IS}$ is designed to reuse pre-trained models while addressing the domain shift challenge when no training data is present (i.e., zero-shot learning). Both $\\mathbf{Assort}_{S}$ and $\\mathbf{Assort}_{IS}$ outperform six existing techniques by at least 13% and 7% respectively in terms of the F1 score. Furthermore, a human study shows that participants significantly preferred summaries generated by $\\mathbf{Assort}_{S}$ and $\\mathbf{Assort}_{IS}$ over the best baseline, while the preference difference between $\\mathbf{Assort}_{S}$ and $\\mathbf{Assort}_{IS}$ was small.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icse/KouCZ23", "doi": "10.1109/icse48619.2023.00158"}}, "content": {"source": {"pdf_hash": "35eeeb704a8500a669b2bb0dba9ba89a44584b8e", "pdf_src": "IEEE", "pdf_uri": "[\"https://export.arxiv.org/pdf/2305.16680v1.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://arxiv.org/pdf/2305.16680", "status": "GREEN"}}, "grobid": {"id": "bf92e51480da7531b7445ca4de4a1b751007f074", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/35eeeb704a8500a669b2bb0dba9ba89a44584b8e.txt", "contents": "\nAutomated Summarization of Stack Overflow Posts\n\n\nBonan Kou koub@purdue.edu \nPurdue University West Lafayette\nUSA\n\nMuhao Chen muhaoche@usc.edu \nUniversity of Southern California\nLos AngelesUSA\n\nTianyi Zhang tianyi@purdue.edu \nPurdue University West Lafayette\nUSA\n\nAutomated Summarization of Stack Overflow Posts\n10.1109/ICSE48619.2023.00158Index Terms-Stack OverflowText SummarizationDeep Learning\nSoftware developers often resort to Stack Overflow (SO) to fill their programming needs. Given the abundance of relevant posts, navigating them and comparing different solutions is tedious and time-consuming. Recent work has proposed to automatically summarize SO posts to concise text to facilitate the navigation of SO posts. However, these techniques rely only on information retrieval methods or heuristics for text summarization, which is insufficient to handle the ambiguity and sophistication of natural language.This paper presents a deep learning based framework called ASSORT for SO post summarization. ASSORT includes two complementary learning methods, ASSORTS and ASSORTIS, to address the lack of labeled training data for SO post summarization. ASSORTS is designed to directly train a novel ensemble learning model with BERT embeddings and domainspecific features to account for the unique characteristics of SO posts. By contrast, ASSORTIS is designed to reuse pre-trained models while addressing the domain shift challenge when no training data is present (i.e., zero-shot learning). Both ASSORTS and ASSORTIS outperform six existing techniques by at least 13% and 7% respectively in terms of the F1 score. Furthermore, a human study shows that participants significantly preferred summaries generated by ASSORTS and ASSORTIS over the best baseline, while the preference difference between ASSORTS and ASSORTIS was small.\n\nI. INTRODUCTION\n\nOnline Q&A forums such as Stack Overflow (SO) have become an integral part of modern programming workflow [1]- [4]. However, locating essential information in online posts can be time-consuming since there are often multiple relevant posts to consider, and some posts can be lengthy. This is confirmed by a recent survey with 72 professional software developers [5]. Participants complained that sifting through many online posts was cognitively demanding and wished to get tool support for quickly navigating online posts.\n\nGenerating concise summaries of SO posts is a promising way to facilitate the navigation of SO posts [5]- [8]. Nadi and Treude experimented with four approaches to capture the gist of a SO post by extracting essential sentences from it [6]. They conducted a user study with 43 developers and confirmed that seeing essential sentences in a SO post indeed increased developers' confidence when assessing the relevance and quality of the post. However, they also found that selecting sentences only based on heuristics or information retrieval (IR) methods was not sufficient. Indeed, these approaches are inherently limited since they lack the flexibility of handling ambiguous or sophisticated narratives in natural language.\n\nThe Natural Language Processing (NLP) community has made significant progress in text summarization using deep learning (DL) [9]- [13]. Those DL-based approaches require to be trained with massive parallel corpora of text documents and summaries. For example, the CNN/DailyMails dataset [14] contains 286K pairs of news articles and human-written summaries. However, no such large datasets exist for SO posts, except for a recent dataset named SOSum [15]. While SOSum contains manually curated summaries for 2,278 SO answer posts, it is still much smaller compared with general-domain corpora such as CNN/DailyMails. Furthermore, no experiment has been done to prove that this dataset is sufficient to train a DL model for summarizing SO posts.\n\nTo bridge the gap, we propose a framework for SO post summarization, ASSORT (Automated Summarization of Stack OveRflow posT). ASSORT provides a novel supervised model ASSORT S to account for the unique characteristics of SO posts. First, ASSORT S uses a combination of BERT embeddings and domain-specific features to characterize sentences in a SO post. Second, since answers to different types of questions have different linguistic norms, we train a question classifier and separate models with the same BERT-based architecture for different types of questions. Third, to account for the uncertainty of the question classifier, the outputs of these models are then ensembled based on the probability distribution of the question classifier, as shown in Figure 2.\n\nFurthermore, since acquiring labeled training data is costly, ASSORT also includes an indirect supervision method called ASSORT IS , which does not need to be trained with any labeled SO data. ASSORT IS makes use of supervision signals from pre-trained models in another domain, such as news articles, for SO post summarization. To handle domain shift issues, ASSORT IS uses a pre-trained Natural Language Inference (NLI) model to trace back to key sentences in the original SO post based on the initially generated summary. Compared with the initially generated summary, which may contain inaccurate terminologies and narratives due to domain shift, extracting aligned sentences from the original post can more accurately capture the gist of the post.\n\nWe evaluate both learning methods in ASSORT with the comparison to a BERT-based text summarization model [16], which is fine-tuned with the same SO training data as AS-SORT S . We also select three heuristics-based methods and one unsupervised learning method from prior work [6], [17], [18] as baselines. We find that both ASSORT S and ASSORT IS outperform all baselines by at least 13% and 7% in terms of the F1 score. This implies that only finetuning a general model with a relatively small dataset such as SOSum [15] is not sufficient. To improve training efficiency, it is necessary to account for the unique characteristics of SO posts. Furthermore, since ASSORT IS is not trained on any SO data, it achieves lower accuracy than ASSORT S . However, when less than 20% of the original training data are available, ASSORT IS achieves better summarization accuracy than ASSORT S . This implies that indirect supervision remains a promising yet cheap alternative for building generalizable models for specific domains when there is a lack of labeled training data.\n\nWe conduct a qualitative user study with 12 participants to evaluate the quality of summaries generated by different techniques. Compared with the best baseline model [16], the majority of participants (76%-85%) preferred summaries generated by ASSORT S or ASSORT IS in terms of usefulness, comprehensiveness, and conciseness. Participants found the summaries generated by ASSORT S more concise and useful in practice, while they found the summaries generated by AS-SORT IS more comprehensive and provided a better overview. Overall, neither ASSORT S or ASSORT IS really triumph over each other according to user feedback.\n\nIn summary, this work makes the following contributions:\n\n\u2022 We design a new supervised model that generates concise and self-contained summaries of SO posts. This model accounts for the uniqueness of SO posts and achieves state-of-the-art accuracy in SO post summarization. \u2022 We propose a new indirect supervision method to further tackle the challenge of lacking labeled training data in SO post summarization. We demonstrate the feasibility of developing models with acceptable accuracy but with no cost of data labeling. \u2022 We conduct a comprehensive evaluation of the proposed learning methods with six comparison baselines, an ablation study, and a qualitative user study. The rest of the paper is organized as follows. Section II describes a motivating example for generating post summaries to facilitate the navigation of SO posts. Section III defines the SO post summarization task. Sections IV and V describe the supervised and indirect supervision models respectively. Section VI describes the evaluation design and setup. Section VII describes the evaluation results. Section VIII discusses the implications, threats to validity, and future work. Section IX describes the related work. Section X concludes this work. Section XI describes the data availability.\n\n\nII. MOTIVATING EXAMPLE\n\nThis section illustrates how summarizing SO posts helps developers quickly navigate SO posts and get an overview of various answers given by other developers. Suppose Alex is an Android developer and she wants to transfer key-value pairs between two layers of a Spring MVC framework. She knows both HashMap and HashTable can serve as the data structure for this task. Yet she is not sure about the pros and cons of these two APIs. So she searches online.\n\nThe first search result from Google is a Stack Overflow question-\"What are the differences between a HashMap and a HashTable in Java?\" 1 There are 35 answer posts to this question. Alex finds it time-consuming to read all of them. So she decides to first read the accepted answer. The accepted answer (Post 40878) points out three major differences between HashMap and HashTable: (1) HashTable is synchronized, whereas HashMap is not; (2) HashTable does not allow null keys or values; (3) HashMap has the flexibility to be replaced with LinkedHashMap. Alex finds this answer helpful, but she is not sure how comprehensive this answer is.\n\nAlex starts reading other highly voted answers to check if they include any important information not covered by the accepted answer. However, she finds herself submerged by the abundant information in those posts. Since many posts are lengthy with details that she does not care about, Alex spends most of her time locating helpful information in those posts. For example, Figure 1 shows the second answer (Post 41454) in this thread. It is a long post with 4 code snippets. Yet the gist of it is a simple message-the synchronization in HashTable is not sufficient, and synchronized HashMap and ConcurrentMap are better choices. Without any tool support, Alex has to read the entire post linearly to get this key message, which can take quite a few minutes.\n\nLike many other developers, Alex only reads a handful of answers and returns to her own code due to her limited time budget [2], [19], [20]. This inevitably makes her overlook answers that are not highly ranked but contain useful information that she is unaware of. For example, the 20th answer (Post 37031553) mentions that HashMap has O(log(n)) complexity and is faster than HashTable. None of the top five answers mentions this. If performance is a top concern to Alex, reading this post can help Alex make a more informed decision. However, without any tool support, Alex is unlikely to reach this deep in the thread practically. ASSORT helps Alex by automatically summarizing each answer post into concise text, so Alex can get a quick overview of many posts and prioritize which posts to spend more time on. In this way, she can make a more informed decision on which API to use. Specially, we build a Chrome extension for Stack Overflow, which highlights summative sentences from each post and renders a list of post summaries, as shown in Figure 1. By looking at the navigation panel, Alex quickly realizes that three answers mention HashTable is synchronized. By contrast, the other four answers suggest staying away from HashTable. Alex is interested in the second answer since its author expressed a strong opinion against HashTable. After clicking on it, Alex jumps to the corresponding answer, where three summative sentences have been highlighted as navigation cues. The first sentence points out that the synchronization in HashTable is not sufficient, while the second and third sentences propose synchronized HashMap and ConcurrentMap as alternatives. Alex feels that these highlighted sentences summarize the gist of the answer well, so she returns to the navigation panel to go deeper in the thread. As she scrolls down the panel, she notices the summary of the 20th answer-O(log(n)) for HashMap vs. O(n) in HashTable. This indicates that HashMap has lower time complexity than HashTable. Alex dives into this answer for details, as performance is always a top concern for her. Without ASSORT, this information would have been buried deep in the thread and unlikely to be discovered. Being aware of this information, Alex now feels more confident in making an informed decision between these two APIs.\n\n\nIII. TASK DEFINITION\n\nWe introduce the definition of Stack Overflow Post Summarization as follows: Given a SO answer post consisting of N sentences, the goal is to select a small set of sentences to form a succinct and self-contained summary. Essentially, this can be viewed as a contextualized sentence classification task where a sentence can either be in or not in the summary.\n\nThis task is also known as Extractive Summarization (ES) in NLP. It is in contrast to another type of summarization called Abstractive Summarization (AS). Instead of selection summative sentences from the original document, AS generates new text to summarize the document [21]. Compared with ES, a unique challenge in AS is that distorted or fabricated text is likely to be introduced during text generation [22], [23]. Several studies have shown that factual inconsistency occurs in up to 30% of abstractive summaries [24]- [27]. Furthermore, since most abstractive summarization models are pre-trained Summary = \u03bb + \u03bb + \u03bb In this work, we focus on extractive summarization as the first step towards SO post summarization while leaving the more challenging abstractive summarization task as future work. In Section IV, we first propose a supervised learning method with a novel model architecture tailored for SO posts. Then in Section V, we propose an indirect supervision method that reuses pre-trained models from another domain while addressing domain shift issues via natural language inference.\n\n\nIV. SUPERVISED POST SUMMARIZATION\n\nASSORT includes a supervised learning method named AS-SORT S . As shown in Figure 2, ASSORT S takes three phases to summarize a SO answer post. Since answers to different types of questions follow different linguistic patterns, ASSORT S first predicts the type of the SO question (Phase I). To account for the uncertainty of the question classifier, the answer post is fed into three sentence classification models separately, each of which is trained for one type of SO questions (Phase II). Finally, ASSORT S ensembles the predictions of these models based on the likelihood of the question type to generate the final summary (Phase III).\n\n\nA. Question Classification\n\nWhile manually inspecting SO answer posts, we observed that answers to different kinds of questions have different linguistic forms. For example, answers to a how-to question often contain a step-by-step solution, while answers to a conceptual question often contain a definition or description of the concept. Based on this insight, we decide to first categorize SO posts based on their question types and then train separate post summarization models for different types of questions. We follow the SO question taxonomy from prior work [15], [28]- [31] and consider three representative types of SO questions-how-to questions, conceptual questions, and bug fixing questions. How-to questions ask for instructions for achieving a task, e.g., \"how do I undo the most recent local commits in Git?\". Conceptual questions ask for clarifications on a concept, e.g., \"what are metaclasses in Python?\". Bug fixing questions ask for solutions to fix some issues, e.g. \"git is not working after macOS Update\".\n\nWe develop a Support Vector Machine (SVM) classifier to categorize SO posts based on their question titles. We train it with a combination of 506 classified SO questions from SOSum [15] and 365 new questions. In total, our dataset includes 301 how-to questions, 305 conceptual questions, and 265 bug-fixing questions. We choose these three types of questions, since they are the most common question types, covering 77% of SO questions based on prior work [29]. We discard questions that did not belong to the three types of questions when curating the dataset.\n\nTo select and label the additional 365 questions, the first author first ranked all SO questions by view count in descending order. Then, he inspected these questions and manually classified them based on the types until he found 365 questions belonging to one of the three types of questions under investigation. Then, the first author and another undergraduate student independently labeled the summative sentences in the answer posts of these questions following the labeling heuristics described in the SOSum paper [15]. In total, they labeled 785 answer posts under these 365 questions. The Cohen's Kappa score before the discussion is 0.72, which implies a substantial agreement [32]. The two labelers met to discuss their labels and resolved all disagreements. This labeling process took about 73 man-hours.\n\nDespite the simplicity of SVM, this classifier achieves reasonable accuracy (78%) with 8:1:1 train/dev/test data split and 10-fold validation. To further account for the potential misclassification of the SVM classier, ASSORT S adopts an ensemble mechanism that incorporates the probability distribution of different types of questions predicted by the SVM classifier (Section IV-C). An ablation study confirms the benefit of incorporating question classification into ASSORT S , improving its F1-score by 9% (Section VII-B). \n\n\nB. Summative Sentence Identification\n\nWe train separate models to identify summative sentences for the three representative types of SO questions. These models share the same model architecture but are trained on answer posts to different types of questions to capture unique linguistic norms and writing styles of each question category. We explain the model architecture below. Figure 3 gives an overview of the model architecture. ASSORT S performs hybrid learning by combining the semantic representation from a BERT model and a multifaceted set of domain-specific features. Each sentence in the given post is encoded into a vector embedding, which is then fed into a feedforward neural network (FNN) to decide whether the sentence is a summative sentence.\n\nIn this work, we use a BERT model that is fine-tuned with 152 million sentences from Stack Overflow [33]. Given a sentence from a SO post, ASSORT S computes its embedding from BERT by averaging the embedding vectors of all tokens in the sentence. The incorporation of deep contextualized sentence embeddings helps ASSORT S capture the semantics of a given sentence. This is confirmed by the ablation study in Section VII-B, where the inclusion of BERT increases the F1 score of our model by 17%.\n\nWhile the contextualized embeddings from BERT are effective in capturing sentence semantics, they are not sufficient to capture specific features, such as bold text, which human readers often rely on to identify summative sentences. Therefore, we further design a set of domain-specific features to capture the semantic, structural, and stylistic information cues in SO posts. We elaborate on these features below.\n\nFirst, we design five semantic features to select summative sentences based on their content.\n\n\u2022 Entity Overlap. If a sentence mentions a software entity that is also mentioned in the question title or in a SO tag of the question, this sentence may provide some directly relevant information for the question. In this work, we use SEthesaurus [34] to identify software entities in a sentence. Let E q be the combination of software entities in the question title and the SO tags in the question. Let E s be the set of software entities mentioned in a sentence in the answer post. The entity overlap is computed as |E q | \u2229 |E s |/|E q |. \u2022 Comparative Adjective. This feature captures whether a sentence contains a comparative adjective. Sentences containing a comparative adjective often contain information that helps readers compare two APIs or two solutions, e.g., \"the stack is faster because all free memory is always contiguous.\" \u2022 Superlative Adjective. This feature captures whether a sentence contains a superlative adjective. Similar to comparative adjectives, sentences with superlative adjectives often indicate answerers' strong inclination for or against an approach, API, or bug fix, e.g., \"there is no doubt that application/json is the best MIME type for a JSON response.\" \u2022 Imperative Sentence. This feature captures whether a sentence is an imperative sentence. Imperative sentences often contain instructions to accomplish a programming task or to fix a bug, e.g., \"use git revert commit-id.\" \u2022 Linguistic Patterns. As we label SO posts for training ASSORT S , we summarize 19 phrases that may imply important information in an answer (Table I). Specifically, the first author randomly sampled 100 posts from the labeled dataset and manually analyzed the summative sentences of these posts. He identified an initial set of common phrases that were shared across multiple sentences. He then applied these patterns back to those sentences to measure the coverage and iteratively refined the patterns. Similar procedures have been adopted in prior work to identify linguistic patterns [5], [17], [35].\n\nEach pattern corresponds to a dimension in the sentence embedding. If a linguistic pattern is matched, then the corresponding dimension is set to 1, otherwise 0.\n\nSecond, we design two structural features to select summative sentences based on their relations with other sentences and codes in the post.\n\n\u2022 Sentence Position. This feature captures the position of a sentence in a given post. This feature is designed based on our observation that the leading sentences in many SO posts can serve as a good summary of the post. \u2022 Code Adjacency. This feature indicates whether a sentence is around a code snippet. It is designed based on our observation that sentences around a code snippet enclosed by a pre tag often contain information cues for a programming task solution or a bug fix.\n\nThird, we design three stylistic features to capture formatting styles that are used to highlight important information in a SO post. In other words, ... 19 Furthermore, ... 10\n\nIn addition, ...\n\n\u2022 Bold Text. This feature indicates whether a sentence contains bold text. In SO posts, answerers often highlight important terms or statements in bold to draw attention from readers. \u2022 Step in a List. This feature indicates whether a sentence is the first sentence of an item in a bulleted or numbered list. This feature is designed based on our observation that for how-to questions, many answerers typically provide a list of steps to accomplish a task. The ablation study (Section VII-B) confirms the usefulness of these domain-specific features. Specifically, including these features leads to an increase of 6% in the F1 score. We also conducted an experiment to measure the contribution of each feature by removing each of them and evaluating the accuracy degradation. The results are included in the Supplementary Material.\n\n\nC. Ensemble Inference\n\nTo account for the uncertainty of question classification in Phase I, we design an ensemble mechanism that merges the predictions from the three sentence classifiers to make the final decision about whether a sentence should be included in the summary.\n\nThe ensemble mechanism takes two input: (1) the softmax probabilities of k categories (k = 3 in this case) predicted by the question classifier, p 1 , ..., p k ; (2) the probabilities of a sentence being a summative sentence from each sentence classifier, \u03bb 1 , ..., \u03bb k . The final score \u03c6 of a sentence is defined\nas: \u03c6 = \u2211 k i=1 p i \u03bb i .\nIf a sentence has a final score greater than a threshold \u03b8, it is selected as a summative sentence. We experimentally determine \u03b8 to be 0.5 on a validation set of 303 posts (10% of the dataset) with the objective of maximizing the F-1 score. After every sentence in an answer post is classified, ASSORT collects all the selected sentences and outputs them as an extractive summary for the answer post.\n\n\nV. POST SUMMARIZATION VIA INDIRECT SUPERVISION\n\nWhile supervised learning can achieve superior performance, obtaining a large amount of labeled data is often costly, especially in specific domains such as software engineering. Therefore, we propose an indirect supervision approach, AS-SORT IS , to overcome this limit. Instead of acquiring labeled data for direct supervision, ASSORT IS uses supervision signals from pre-trained models in another domain, such as news article summarization. To address the challenge of data shift in cross-domain transfer, we use a pre-trained Natural Language Inference (NLI) model to select summative sentences in the original post based on the summary generated by the pre-trained text summarization model. Figure 4 provides an overview of our approach.\n\n\nDocNLI\n\n\nFig. 4: An overview of ASSORT IS\n\nIn this work, we use BART-large-CNN [36] as the pretrained text summarization model. Unlike our supervised model in Section IV, BART-large-CNN is an abstractive summarization model. In other words, BART-large-CNN generates a summary in its own words via an autoregressive decoding process, rather than selecting summative sentences in the input document. Specifically, BART-large-CNN is based on BART [10], a denoised auto-encoder that encodes an input document to a high-dimensional embedding for text reconstruction. In BART-large-CNN, the initial BART model is finetuned to generate text summaries using the CNN/DailyMail corpus [14], which contains 300K news articles and their human-written summaries. In this work, we select BARTlarge-CNN, since it is a representative and strong summarization model that outperforms other transformer-based models such as [13] by up to 6 points on CNN/DailyMail in ROUGEbased metrics [10].\n\nWhile BART-large-CNN is demonstrated effective on news article summarization, it suffers from domain-shifting issues when applied to a different domain, such as SO post summarization. Furthermore, recent studies have shown that abstractive summarization models may introduce factual inconsistencies in generated text compared with original text [22], [23]. Table II shows an abstractive summary generated by BART-large-CNN. The last sentence in it (highlighted in red) is exceptionally long and contains broken English with some hallucinated information.\n\nDespite the broken English, this sentence is not fully made up by BART-large-CNN. It is based on two sentences in the original post (highlighted in green). In fact, these two sentences provide a good summary of the last paragraph in the original post. Though BART-large-CNN recognizes them, it fails to generate an eloquent summary due to domain shift.\n\nTo address this issue, we propose associating abstractive summaries to the original post's corresponding sentences using a pre-trained Natural Language Inference (NLI) model. The task of NLI [37] studies to what extent a premise entails, contradicts, or remains neutral with a hypothesis. For example, There is a potential problem here with regard to workflow persistence.\n\nIf you create long running workflows that are persisted in a database, these workflows are not reloaded into memory until there is some external event that reloads them. As there they are responsible for triggering the event themselves but cannot until they are reloaded. And we have a catch 22 :-(.\n\nThe proper way to do this is using an external service. And while this might feel like dividing the code into two places it really isn't. The reason is that the workflow is responsible for the big picture, IE what should be done. And the runtime service is responsible for the actual implementation or how it should be done. That way you can change the how without changing the why and when part. [Source of the ineloquent sentence in BART-large-CNN's summary] Abstractive summary generated by BART-large-CNN There is a potential problem with regard to workflow persistence. If you create long running workflows that are persisted in a database they will not be able to restart until there is an external event reloading them. The proper way of doing this is using an external service which can change the how part without changing what it should be done or when it's better than having two separate processes for different tasks as each one does their own thing but doesn't agree on exactly who must carry out its function at any given time. [broken English] Summary generated by our indirect supervision approach There is a potential problem here with regard to workflow persistence. The proper way to do this is using an external service. That way you can change the how without changing the why and when part.\n\nsuppose we have a hypothesis sentence, \"a kid ate a fruit\", and a premise sentence, \"a boy ate an apple\". An NLI model will predict that the premise entails the hypothesis. However, if the hypothesis is changed to \"a kid ate a banana\", the prediction will becomes to neutral or contradiction.\n\nWe use a pre-trained NLI model to decide which sentence in the original SO post is entailed by the abstractive summary and thus should be included in the final summary. Specifically, we use a RoBERTa model that is pre-trained on DocNLI [38], a document-level NLI dataset. DocNLI covers multiple text genres, such as news, fiction, and conversations, with multisentence (i.e., document) premises and single-sentence hypotheses. Given a summary generated by BART-large-CNN, our approach checks the logical relationship between the summary (i.e., the premise) and every sentence in the original post (i.e., the hypotheses). The DocNLI model will then produce a probability distribution over the three relationships-entail, contradict, and neutral. Our approach selects a sentence as part of the final summary if the entailment probability is the higher than the other two probabilities.\n\n\nVI. EVALUATION\n\nWe conduct both quantitative experiments and user studies to answer the following four research questions:\n\n\u2022 RQ1: How effective is our supervised model, ASSORT S , in SO post summarization? \u2022 RQ2: To what extent does each component in ASSORT S contribute to its effectiveness? \u2022 RQ3: How does our indirectly supervised model, AS-SORT IS , compare to our surprised model? \u2022 RQ4: How do real programmers perceive the summaries given by our models?\n\n\nA. Experiment Setup\n\nIn addition to the 2,278 labeled posts from SOSum [15], we further manually labeled 785 answer posts following the same labeling procedure as described in [15]. The labeling process is described in Section IV. We use these 3,063 posts for training and evaluation. These posts are from 785 SO questions, including 254 how-to questions, 322 conceptual questions, and 209 bug-fixing questions.\n\nWe empirically decided the model structure and hyperparameters for the classification head of ASSORT S . Specifically, we experimented with different kinds of models, as shown in Table III. We chose feedforward neural network (FNN) since it performed the best. We also experimented with different numbers of hidden layers in the FNN and eventually chose 1 hidden layer, as shown in Table IV.  To train ASSORT S , for each type of questions, we randomly split the corresponding answer posts into training, development, and test sets with a split ratio of 8:1:1. Then, we train a summative sentence identification model for answers to each type of question following the model architecture in Figure 3. During training and testing, an answer post to be summarized is broken down into individual sentences. In total, we have 14,165 sentences for the 2,424 answer posts in the training set. Each model was trained with a batch size of 512 sentences in 150 epochs. In each batch, each sentence is fed to ASSORT S one by one with the corresponding question title. We use the Adam optimizer [39] for training and the learning rate is set to 1e \u22125 . Training ASSORT S took 3 hours on a single GPU (NVIDIA GeForce GT 1030).\n\nTo design an ablation study to answer RQ2, we create four variants of ASSORT S with one key component removed in each variant. The four key components are: (1) the BERT embeddings, (2) the domain-specific features, (3) the question classifier, (4) the ensemble mechanism. Specifically, when the question classifier is ablated, ASSORT S uses a sentence classifier trained on all types of questions to predict the final score of a sentence. When the ensemble mechanism is ablated, ASSORT S first predicts the question type of a given post and then only uses the sentence classifier for the predicted question type to identify summative sentences, rather than using all three classifiers.\n\nTo answer RQ3, we first compare ASSORT IS against AS-SORT S on a full training setting where we train ASSORT S with the entire training set. Then, we compare them in different lowresource settings where only a subset of the original training data is available. In those low-resource settings, we randomly select SO posts from the original dataset to make subsets of training data. Since ASSORT IS does not require any training data, the low-resource baselines are created to make a fair comparison between ASSORT S and ASSORT IS in conditions where training data are scarce.\n\n\nB. Compared Baselines\n\nWe select a state-of-the-art extractive summarization model on the general domain [16] and also fine-tune it on the same training set of 2,424 SO posts as ASSORT S . Furthermore, we select three heuristics-based methods and one unsupervised method that can perform sentence-level summarization from prior work [6], [17], [18]. These four methods have also been experimented in [6].\n\n(1) BertSum [16] is an extractive summarization model that first uses BERT to encode sentences and then uses a transformer to select summative sentences [16]. It outperforms several previous techniques [11], [40]- [42] on two popular text summarization datasets-NYT [43] and CNN/DailyMail [14]. In this experiment, we use the checkpoint of BertSum that has the best performance on CNN/DailyMail. (2) BertSum (fine-tuned) [16] is a fine-tuned version of Bert-Sum. It is fine-tuned with the training data of ASSORT S , including 2,424 SO posts and their summaries. (3) wordpattern identifies essential sentences in a SO post using a set of 360 word patterns. These patterns are initially designed by Robillard and Chhetri [17] to identify sentences containing indispensable knowledge in API documentations. (4) simpleif is a technique proposed by Nadi and Treude [6].\n\nIt is designed based on the insight that essential sentences may contain contextual information expressed in the form of conditions. Thus, simpleif identifies all sentences that have the word \"if\" in them as essential sentences. (5) contextif is another technique proposed by Nadi and Treude [6]. It uses a set of heuristics to identify essential sentences that carry technical context and are useful. (6) lexrank is a commonly used unsupervised text summarization approach [18]. It uses a stochastic graph-based method to compute the relative importance of sentences in a document and generates an extractive summary by selecting the top k sentences. We use the default k value, 5, in an open-source implementation of lexrank [44].\n\nWe do not compare with paragraph-level summarization techniques such as AnswerBot [5] and CraSolver [45], since it is not a head-to-head comparison. Take AnswerBot as an example. First, the problem setting is different. AnswerBot summarizes multiple SO threads, while ASSORT summarizes key points in a single post. Second, even if we adapt An-swerBot to only summarize a single post, AnswerBot can only produce a summary by selecting paragraphs. By contrast, ASSORT produces a more fine-grained summary by selecting sentences. Thus, AnswerBot always generates longer and more coarse-grained summarizations than ASSORT, leading to low precision on the benchmark.\n\n\nC. Evaluation Metrics\n\nWe use three metrics-precision, recall, and F1 to measure the effectiveness of our methods and the comparison baselines in SO post summarization. Each metric is calculated at the sentence level. Given a set of summative sentences in a set of SO posts G, let M be the set of summative sentences selected by an extractive summarization model. The precision of the model is calculated as |G \u2229 M |/|M |. And the recall of the model is defined as |G \u2229 M |/|G|. Furthermore, we measure the F1 score, which combines the precision and recall of a model into a single metric by taking their harmonic mean.\n\nWe make sure both ASSORT S and ASSORT IS and all the baselines are evaluated with the same test set (i.e., 304 posts and their summaries) to make the comparison fair. A 10-fold validation is performed when calculating the metrics for our approaches and all the baselines.\n\n\nD. User Study Design\n\nTo answer RQ4, we conduct a within-subjects user study to evaluate the summary quality. We recruit 12 graduate students through the department mailing list from an R1 university. Participants have an average of four years of programming experience. We randomly selected 40 answer posts from our dataset, including 13 how-to questions, 13 conceptual questions, and 14 bug-fixing questions. These tasks were randomly sampled from the test set while ensuring a balanced number of answers in each category. In each user study, we select 10 out of the 40 answer posts and ask the participant to review their summaries. We counterbalance the post assignment so that each post is evaluated by three different participants.\n\nFor each answer post, participants first report their expertise of the concepts in the question on a 7-point scale. 1 means \"Haven't even heard of it before\" and 7 means \"I am an expert\". Then, they will be provided with the question post, the answer post, and the summaries generated by ASSORT S , ASSORT IS , and BertSum (fine-tuned). Specifically, we select BertSum (fine-tuned) as our baseline in the user study since it performs the best among all six baselines in the quantitative experiment. The participants first read the question to understand the context and then read the answer post to understand the content to be summarized. The participants are then asked to read all three summaries and evaluate the quality of these summaries by answering the following five multiple-choice questions.\n\n(1) Which summary provides the most helpful information?\n\n(2) Which summary provides the best overview of the post? (3) Which summary provides the most comprehensive information? (4) Which summary is the most concise without being incomplete? (5) Which summary do you prefer to see in practice?\n\nTo mitigate bias, the order of summaries is randomized to mitigate bias and we also do not reveal which model generated which summary. At the end of the user study, participants answer several open-ended questions about whether they wish to see SO post summaries when browsing SO and what kinds of characteristics an ideal SO summary should possess. Table V shows the summarization accuracy of ASSORT S and the baselines after 10-fold validation is performed. AS-SORT S achieves the best results in all three metrics. Furthermore, the domain shift from general text corpora to the SO post corpus is non-trivial. The original BertSum model, which is trained on news articles and their summaries from CNNDailymails, only achieves an F1 score of 53%. While fine-tuning BertSum with 2,424 labeled SO posts increases the F1 score from 53% to 58%, it is still 13% below ASSORT S . Given that BertSum also uses BERT for sentence encoding, this result implies that directly reusing a pre-trained model, even with finetuning, is not an optimal solution. Incorporating domain-specific features and ensembling based on question types are necessary to improve the effectiveness of SO post summarization.\n\n\nVII. RESULTS\n\n\nA. RQ1: SO Post Summarization Accuracy\n\n\nB. RQ2: Ablation Study\n\nTable VI shows the ablation study results. On the one hand, ablating the BERT embedding leads to the largest accuracy degradation, 17% in the F1 score. This indicates that incorporating deep contextualized embeddings from a language model is critical for a domain-specific task such as SO post summarization. On the other hand, only including BERT is also not sufficient. Removing each of the other three components, which are specifically designed to account for the unique characteristics of SO posts, leads to a non-trivial  decrease in the F1 score. Specifically, the removal of domainspecific features decreases the F1 score by 6%. This indicates that deep contextualized embeddings alone cannot cover the unique structural and linguistic patterns that distinguish SO posts from general text data. Removing the question classifier decreases the F1 score by 9%. This indicates that accounting for different linguistic norms in answers to different types of questions indeed helps. Removing the ensemble mechanism leads to a decrease of 4% in the F1 score. This indicates that the ensemble mechanism can help to alleviate the influence of question classification errors. Figure 5 compares the accuracy of ASSORT S and AS-SORT IS when various amounts of training data are available. Given that ASSORT IS only uses pre-trained models and does not require any training data, the accuracy of ASSORT IS is constant (65%) in these settings. When all training data (i.e., 2,424 posts and their summaries) is available, ASSORT S outperforms ASSORT IS by 6%. This indicates that directly training a supervised model is a better choice when there are sufficient training data. However, when 20% or less of the original training data is available, ASSORT IS outperforms ASSORT S . Therefore, when the training data is small, indirect supervision can be a better option than directly training a model with small training data. Furthermore, with an F1 score of 65%, ASSORT IS outperforms all six comparison baselines by 7% to 59%. This result is significant since the best comparison baseline is fine-tuned on all training data. This implies that indirect supervision can be a promising yet low-cost alternative compared with unsupervised approaches and model fine-tuning. Figure 6 shows the choice of participants over the summaries generated by ASSORT S , ASSORT IS , and BertSum (finetuned) in five aspects. Overall, participants strongly preferred  summaries generated by ASSORT S or ASSORT IS over Bert-Sum (fine-tuned). Between ASSORT S and ASSORT IS , more participants found summaries generated by ASSORT S more helpful, concise, and practical. Yet more participants found summaries generated by ASSORT IS more comprehensive and providing a better overview of the post. Since each answer post and its summaries were reviewed by three participants, we further analyzed the consistency among those participants when they answered each multiple-choice question. In 60% of the cases, the three participants assigned to the same post consistently chose summaries generated by ASSORT over BertSum (fine-tuned) in a multiple-choice question. In 91% of cases, at least two out of three participants consistently chose either ASSORT S or ASSORT IS over BertSum (fine-tuned). However, the choices between ASSORT S and ASSORT IS were not very consistent among participants assigned to the same post. In only 40% of cases, all three participants consistently chose ASSORT S or ASSORT IS . This implies the participants chose between ASSORT S and AS-SORT IS largely based on their personal preference. Neither ASSORT S or ASSORT IS really triumph over each other.\n\n\nC. RQ3: Supervision vs. Indirect Supervision\n\n\nD. RQ4: Human Evaluation\n\nWe also investigated how participants' expertise on concepts in a SO question affects their preferences over post summaries. As described in Section VI-D, participants first reported their  Fig. 7: Choices of participants with different levels of expertise expertise on a 7-point scale. We categorized their expertise as \"novice\" if their rating is 1-2, \"regular\" if their rating is 3-5, and \"expert\" if their rating is 6-7. Figure 7 shows the choices of participants with different levels of expertise. We observed an tendency of favoring ASSORT S among novices while an tendency of favoring ASSORT IS among experts. One plausible reason for this is that novices prefer to see short summaries and feel overwhelmed if a summary contains too much information, while experts prefer to see more comprehensive information and feel less overwhelmed. Pearson's Chisquare test of independence shows that the choice difference among participants with different levels of expertise for post summaries is statistically significant (p = 0.0006).\n\nIn the final survey, 84% of participants confirmed they would like to see post summaries when browsing SO posts. For example, P4 elaborates, \"seeing a concise yet informative summary can help me quickly decide whether a post is worth reading or not.\" Another participant (P10) said, \"most information in a long post is useless to me, what I want is the solution and solution only\".\n\n\nVIII. DISCUSSION\n\nOur experiment shows that only fine-tuning a general text summarization model such as [16] is insufficient. Our supervised approach, ASSORT S , achieves significantly higher summarization accuracy by incorporating specific designs for SO posts, such as question classification and domain-specific features. This may carry a more general implication-as we reuse models from NLP or ML, we should consider renovating their model architectures and adapting them to account for the unique characteristics of SE tasks.\n\nOur indirect supervision approach, ASSORT IS , is proposed to address the challenge of lacking labeled data in SO post summarization. ASSORT IS has achieved reasonable accuracy and is proven to be a better choice when labeled data is insufficient, e.g., less than 485 posts in our experiment setting. Furthermore, our user study shows that while summaries generated by ASSORT IS and ASSORT S were deemed good in different aspects (e.g., comprehensiveness vs. conciseness), there was no significant preference difference in general. This implies that ASSORT IS could be an acceptable yet low-cost solution in practice.\n\nBoth ASSORT S and ASSORT IS can be generalized to summarize other types of SE documents, such as API documentation, tutorials, and bug reports. Since ASSORT IS only uses pretrained models, it can be reused as-is. By contrast, ASSORT S would benefit from several extensions, e.g., re-training a topic classifier rather than a question classifier, re-designing some domain-specific features based on the linguistic norms in the target corpus. Furthermore, since ASSORT IS does not require any labeled training data, it can also be used as a starting point to explore the feasibility and opportunities of summarizing other types of SE documents.\n\nThreats to validity. In terms of internal validity, labeling summative sentences in SO posts is a subjective task. Therefore, different labelers will not necessarily select the same set of summative sentences from the same post. In this work, two labelers expanded SOSum with 785 answer posts retrieved under 365 SO questions. We strictly follow the labeling procedure established in [15]. The Cohen's kappa score is 0.72 on the initial labeling results before discussion, which indicates moderate agreement. Most disagreements were resolved after discussion. In the final dataset, only those sentences that both labelers agreed on are labeled as summative sentences.\n\nIn terms of external validity, since our dataset is constructed by retrieving posts under the most popular questions, it inevitably favors popular tags, such as Java and Python. Our dataset contains 1,089 unique tags, while there are more than 50K tags on Stack Overflow. The accuracy of ASSORT S may drop a bit with unfamiliar topics due to unseen terminologies. Furthermore, our current model is only trained with SO posts to three common types of questions, since we aim to develop a proof of concept and demonstrate its feasibility for the scope of this work. To support other types of questions, one should enrich our dataset and re-train the model to obtain optimal accuracy. Finally, the accuracy of ASSORT S may also decrease when reused as-is for other types of SE documents. This is because ASSORT S includes unique designs for SO posts, which is not applicable to other types of SE documents.\n\nIn terms of construct validity, our question classifier does not always make the correct prediction. Sometimes, the boundary between different question categories can be blurred with questions like \"How to fix headers-already-sent error in PHP?\". This question should be classified as bug-fixing, since it mentions an error. However, it is classified as how-to by our question classifier since the question title contains a phrase \"how to\". To mitigate this threat, ASSORT S includes an ensemble mechanism that merges predictions from separate summative sentence prediction models. Another threat to construct validity lies in the user study design. In the current design, participants were only asked to evaluate the quality of the post summaries with regard to the SO question. However, the usefulness of these summaries when being used to solve real programming tasks remains to be determined.\n\nFuture work. Currently, ASSORT S only uses a feedforward neural network as the classification head on top of BERT. Prior work [16] has shown that the choice of classification heads has an influence on model accuracy. It would be worthwhile to further experiment with other types of classification heads, such as transformers and LSTMs. Furthermore, we have only considered single-document summarization in this work. It would be interesting to investigate how to perform multi-document summarization on a thread of SO posts. The challenge of multi-document summarization is not only to shorten the text, but also to organize information around the key aspects to represent diverse views. Compared with extractive summarization, abstractive summarization may be a more preferred solution, since it is capable of rephrasing sentences from different posts and generating gradual transitions. Diversity-based ranking algorithms such as Maximal Marginal Relevance (MMR) [46] can also be considered here.\n\n\nIX. RELATED WORK\n\nSO post summarization. Several approaches have been presented to summarize SO posts to concise texts to facilitate SO post navigation [5]- [7], [45]. AnswerBot extracts summative paragraphs from SO posts based on features such as information entropy and paragraph position [5]. CraSolver uses a similar multi-factor ranking mechanism to summarize bug solutions [45]. Both AnswerBot and CraSolver generate summaries at the paragraph level. By contrast, our approach generates more fine-grained sentence-level summaries of SO posts. Nadi and Treude experimented with four different IR-based approaches to select essential sentences from SO posts [6]. They conducted a survey with 43 developers and found that while participants would like to get navigation support for browsing SO, the IR-based approaches failed to provide such support. Motivated by these findings, we propose a novel DL-based framework that can more accurately identify summative sentences in SO posts in this work. Several approaches have been proposed to generate question titles for SO threads based on code snippets [47], [48]. However, since question title generation aims to summarize a question in a one-liner, these approaches cannot be applied to SO post summarization, which has a very different problem setting. Text summarization for bug reports. There are several summarization techniques for other types of SE documents such as bug reports [49]- [53]. Liu et al. designed a new metric called believability score, which measures the degree to which a sentence is supported or refuted by other comments. Based on the idea of believability score, they further developed BugSum, which selects summative sentences in a bug report [50]. Rastkar et al. created a small dataset with humanannotated summaries for 36 bug reports and used it to train a Logistic Regression classifier with 24 explicit features to identify summative sentences in a bug report [51]. Mani et al. experimented with four unsupervised models-Centroid, MMR, Grasshopper, and Diverse Rank-on the task of bug report summarization [52]. Unlike these techniques, we proposed two learning paradigms for building DL models for SO post summarization. General text summarization. Many neural-based text summarization techniques have been proposed recently [11], [12], [16], [54]- [56]. For example, Liu et al. presented a BERTbased model and trained it on CNN/DailyMails [16]. Narayan et al. presented a CNN-based model that summarizes a single news article into a one-liner and trained it with XSum [54]. In another work, Narayan et al. framed the task of text summarization as a ranking problem and proposed a global optimization method for training [11]. Dong et al. framed text summarization as a contextual bandit problem, in which each document is considered as a context and each combination of selected sentences is an action to take [12]. While these models perform well on news articles, reusing these models in another domain, such as SO posts, is not an easy task due to domain shift. In this work, we propose ASSORT S , a supervised model that accounts for unique characteristics of SO posts and demonstrate that it significantly outperforms a fine-tuned version of BertSum (Section VII-A). Other tool support for information seeking in SO. In addition to text summarization, many other kinds of tool support have been proposed to facilitate information seeking in SO [35], [57]- [70]. For example, SeaHawk [57] is an Eclipse plugin that integrates SO into an IDE. Chatbot4QR [71] expands a user query with tags of similar SO questions in order to improve search results. Ye et al. [59] propose a re-ranking mechanism for SO search results of a user-defined query based on different search focus users may have. They identify linguistic patterns in different types of queries and train separate models based on query types to re-rank search results. SISE uses linguistic patterns to identify sentences that mention an API in SO posts and augments API documents with those sentences [63].\n\n\nX. CONCLUSION\n\nIn this work, we propose two complementary learning methods for automated post summarization in Stack Overflow (SO). ASSORT S is a supervised method that accounts for unique characteristics in SO posts via question classification, domainspecific features, and ensemble inference. ASSORT IS , on the other hand, is an indirectly supervised method that does not require any labeled training data by leveraging a pre-trained model from another domain while addressing the domain shift issue via natural language inference. Both ASSORT S and ASSORT IS significantly outperform six existing techniques in terms of summarization accuracy. ASSORT IS is demonstrated to be a promising yet low-cost solution in a low-resource setting. Furthermore, a user study shows that participants consistently favored summaries generated by ASSORT S or ASSORT IS over the best baseline model, while the preference difference between ASSORT S and ASSORT IS was small. In the future, we plan to extend ASSORT to support multi-document summarization on the entire discussion thread and also apply ASSORT to other types of SE documents.\n\n\nXI. DATA AVAILABILITY\n\nOur code and data have been made available in an anonymous GitHub repository for review. 2 \n\nFig. 1 :\n1A Chrome extension built upon ASSORT. On the left side, summative sentences are highlighted in yellow. On the right side, a navigation panel provides a bird's-eye view of the thread by showing the first sentence in the summary of each answer.\n\nFig. 2 :\n2An overview of ASSORT S on news articles or general-domain corpora, such errors may become more prevalent when reusing those models on SO posts due to domain shift.\n\nFig. 3 :\n3The architecture of the sentence classifier\n\nFig. 6 :\n6Participants' choices over different types of summaries in five aspects\n\nTABLE I :\nILinguistic patterns for sentences that may contain important informationNo. \nPhrase \nNo. \nPhrase \n1 \nHowever, ... \n11 \nIn practice, ... \n2 \nFirst, ... \n12 \nIn fact, ... \n3 \nIn short, ... \n13 \nOtherwise, ... \n4 \nIn this case, ... \n14 \nIf you care, ... \n5 \nIn general, ... \n15 \nIn contrast, ... \n6 \nFinally, ... \n16 \nOn the other hand, ... \n7 \nThen, ... \n17 \nBelow is ... \n8 \nAlternatively, ... \n18 \nAdditionally, ... \n9 \n\n\nTABLE II :\nIIAn abstractive summary generated by BART-large-CNN and the summary generated by our approachOriginal post [Post 438095] \n\n\n\nTABLE III :\nIIIModel Accuracy with Different Types of Classification HeadsF1 \n\u0394 \nFeedforward NN \n0.71 \n-\nRandom forest \n0.56 \n-0.15 \nDecision tree \n0.54 \n-0.17 \nLinear regression \n0.65 \n-0.06 \nLogistic regression \n0.63 \n-0.08 \nAda boost \n0.59 \n-0.12 \nNaive Bayes Classifier \n0.62 \n-0.09 \n\n\n\nTABLE IV :\nIVModel Accuracy with Different Numbers of Hidden Layers in the FNN# hidden layers \nPrecision \nRecall \nF1 \n1 \n0.73 \n0.69 \n0.71 \n2 \n0.69 \n0.71 \n0.70 \n3 \n0.73 \n0.71 \n0.72 \n4 \n0.73 \n0.70 \n0.71 \n\n\n\nTABLE V :\nVComparison of ASSORT S and baselinesPrecision \nRecall \nF1 \n\nASSORT S \u2605 \n0.73 \n0.69 \n0.71 \nBertSum \u2605 \n0.47 \n0.60 \n0.53 \nBertSum (fine-tuned) \u2605 \n0.51 \n0.67 \n0.58 \nwordpattern \u25c7 \n0.40 \n0.03 \n0.06 \nsimpleif \u25c7 \n0.39 \n0.15 \n0.21 \ncontextif \u25c7 \n0.39 \n0.15 \n0.22 \nlexrank \u2606 \n0.61 \n0.45 \n0.52 \n\u2605: DL-based, \u25c7: heuristics-based, \u2606: unsupervised \n\n\n\nTABLE VI :\nVIContribution of each component in ASSORT SPrecision \nRecall \nF1 \n\nASSORT S \n0.73 \n0.69 \n0.71 \n-w/o BERT \n0.61 \n0.48 \n0.54 \n-w/o Domain-specific features \n0.70 \n0.61 \n0.65 \n-w/o Ensemble \n0.68 \n0.66 \n0.67 \n-w/o Question classifier \n0.61 \n0.63 \n0.62 \n\n\nhttps://stackoverflow.com/questions/40471 1854 Authorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply.\n\u2022 Inline Code. This feature indicates whether a sentence contains an inlined code fragment. This feature is designed based on our observation that, for how-to questions and bug-fixing questions, answerers sometimes suggest an alternative API, pinpoint a problematic piece of code, or provide a short code fragment as a solution.\nhttps://github.com/BonanKou/ASSORT-Automatic-Summarization-of-Stack-Overflow-Posts\n\nTwo studies of opportunistic programming: interleaving web foraging, learning, and writing code. J Brandt, P J Guo, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. the SIGCHI Conference on Human Factors in Computing SystemsJ. Brandt, P. J. Guo et al., \"Two studies of opportunistic programming: interleaving web foraging, learning, and writing code,\" in Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, 2009, pp. 1589-1598.\n\nWhat do developers search for on the web. X Xia, L Bao, Empirical Software Engineering. 22X. Xia, L. Bao et al., \"What do developers search for on the web?\" Empirical Software Engineering, vol. 22, no. 6, pp. 3149-3185, 2017.\n\nHow do developers utilize source code from stack overflow?. Y Wu, S Wang, Empirical Software Engineering. 242Y. Wu, S. Wang et al., \"How do developers utilize source code from stack overflow?\" Empirical Software Engineering, vol. 24, no. 2, pp. 637-673, 2019.\n\nWhat do developers use the crowd for? a study using stack overflow. R Abdalkareem, E Shihab, IEEE Software. 342R. Abdalkareem, E. Shihab et al., \"What do developers use the crowd for? a study using stack overflow,\" IEEE Software, vol. 34, no. 2, pp. 53-60, 2017.\n\nAnswerbot: Automated generation of answer summary to developers' technical questions. B Xu, Z Xing, 2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE). B. Xu, Z. Xing et al., \"Answerbot: Automated generation of answer summary to developers' technical questions,\" in 2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE).\n\n. IEEE. IEEE, 2017, pp. 706-716.\n\nEssential sentences for navigating stack overflow answers. S Nadi, C Treude, 2020 IEEE 27th International Conference on Software Analysis, Evolution and Reengineering (SANER). IEEES. Nadi and C. Treude, \"Essential sentences for navigating stack over- flow answers,\" in 2020 IEEE 27th International Conference on Software Analysis, Evolution and Reengineering (SANER). IEEE, 2020, pp. 229- 239.\n\nRecommending comprehensive solutions for programming tasks by mining crowd knowledge. R F Silva, C K Roy, 2019R. F. Silva, C. K. Roy et al., \"Recommending comprehensive solu- tions for programming tasks by mining crowd knowledge,\" in 2019\n\nIEEE/ACM 27th International Conference on Program Comprehension (ICPC). IEEEIEEE/ACM 27th International Conference on Program Comprehension (ICPC). IEEE, 2019, pp. 358-368.\n\nApi method recommendation without worrying about the task-api knowledge gap. Q Huang, X Xia, 2018 33rd IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEEQ. Huang, X. Xia et al., \"Api method recommendation without worrying about the task-api knowledge gap,\" in 2018 33rd IEEE/ACM Interna- tional Conference on Automated Software Engineering (ASE). IEEE, 2018, pp. 293-304.\n\nUnifiedqa: Crossing format boundaries with a single qa system. D Khashabi, S Min, EMNLP -findings. D. Khashabi, S. Min et al., \"Unifiedqa: Crossing format boundaries with a single qa system,\" EMNLP -findings, 2020.\n\nBart: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. M Lewis, Y Liu, arXiv:1910.13461arXiv preprintM. Lewis, Y. Liu et al., \"Bart: Denoising sequence-to-sequence pre- training for natural language generation, translation, and comprehen- sion,\" arXiv preprint arXiv:1910.13461, 2019.\n\nRanking sentences for extractive summarization with reinforcement learning. S Narayan, S B Cohen, arXiv:1802.08636arXiv preprintS. Narayan, S. B. Cohen et al., \"Ranking sentences for ex- tractive summarization with reinforcement learning,\" arXiv preprint arXiv:1802.08636, 2018.\n\nBanditsum: Extractive summarization as a contextual bandit. Y Dong, Y Shen, arXiv:1809.09672arXiv preprintY. Dong, Y. Shen et al., \"Banditsum: Extractive summarization as a contextual bandit,\" arXiv preprint arXiv:1809.09672, 2018.\n\nText summarization with pretrained encoders. Y Liu, M Lapata, arXiv:1908.08345arXiv preprintY. Liu and M. Lapata, \"Text summarization with pretrained encoders,\" arXiv preprint arXiv:1908.08345, 2019.\n\nDocument summarization on cnn/daily mail. \"Document summarization on cnn/daily mail,\" https://paperswithcode. com/sota/document-summarization-on-cnn-daily-mail, 2022, accessed: 2022-3-29.\n\nSosum: A dataset of stack overflow post summaries. B Kou, Y Di, M Chen, T Zhang, 2022 IEEE/ACM 19th International Conference on Mining Software Repositories (MSR). IEEEB. Kou, Y. Di, M. Chen, and T. Zhang, \"Sosum: A dataset of stack over- flow post summaries,\" in 2022 IEEE/ACM 19th International Conference on Mining Software Repositories (MSR). IEEE, 2022, pp. 247-251.\n\nFine-tune bert for extractive summarization. Y Liu, arXiv:1903.10318arXiv preprintY. Liu, \"Fine-tune bert for extractive summarization,\" arXiv preprint arXiv:1903.10318, 2019.\n\nRecommending reference api documentation. M P Robillard, Y B Chhetri, Empirical Software Engineering. 206M. P. Robillard and Y. B. Chhetri, \"Recommending reference api documentation,\" Empirical Software Engineering, vol. 20, no. 6, pp. 1558-1586, 2015.\n\nLexrank: Graph-based lexical centrality as salience in text summarization. G Erkan, D R Radev, abs/1109.2128CoRR. G. Erkan and D. R. Radev, \"Lexrank: Graph-based lexical centrality as salience in text summarization,\" CoRR, vol. abs/1109.2128, 2011. [Online]. Available: http://arxiv.org/abs/1109.2128\n\nTwo studies of opportunistic programming: interleaving web foraging, learning, and writing code. J Brandt, P J Guo, J Lewenstein, M Dontcheva, S R Klemmer, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. the SIGCHI Conference on Human Factors in Computing SystemsJ. Brandt, P. J. Guo, J. Lewenstein, M. Dontcheva, and S. R. Klemmer, \"Two studies of opportunistic programming: interleaving web foraging, learning, and writing code,\" in Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, 2009, pp. 1589-1598.\n\nWhat do developers search for on the web?. X Xia, L Bao, D Lo, P S Kochhar, A E Hassan, Z Xing, Empirical Software Engineering. 226X. Xia, L. Bao, D. Lo, P. S. Kochhar, A. E. Hassan, and Z. Xing, \"What do developers search for on the web?\" Empirical Software Engineering, vol. 22, no. 6, pp. 3149-3185, 2017.\n\nAbstractive summarization: An overview of the state of the art. S Gupta, S K Gupta, Expert Systems with Applications. 121S. Gupta and S. K. Gupta, \"Abstractive summarization: An overview of the state of the art,\" Expert Systems with Applications, vol. 121, pp. 49-65, 2019.\n\nThe factual inconsistency problem in abstractive text summarization: A survey. Y Huang, X Feng, X Feng, B Qin, arXiv:2104.14839arXiv preprintY. Huang, X. Feng, X. Feng, and B. Qin, \"The factual inconsistency problem in abstractive text summarization: A survey,\" arXiv preprint arXiv:2104.14839, 2021.\n\nEvaluating the factual consistency of abstractive text summarization. W Kry\u015bci\u0144ski, B Mccann, C Xiong, R Socher, arXiv:1910.12840arXiv preprintW. Kry\u015bci\u0144ski, B. McCann, C. Xiong, and R. Socher, \"Evaluating the factual consistency of abstractive text summarization,\" arXiv preprint arXiv:1910.12840, 2019.\n\nFaithful to the original: Fact aware neural abstractive summarization. Z Cao, F Wei, W Li, S Li, thirty-second AAAI conference on artificial intelligence. Z. Cao, F. Wei, W. Li, and S. Li, \"Faithful to the original: Fact aware neural abstractive summarization,\" in thirty-second AAAI conference on artificial intelligence, 2018.\n\nNeural text summarization: A critical evaluation. W Kry\u015bci\u0144ski, N S Keskar, B Mccann, C Xiong, R Socher, arXiv:1908.08960arXiv preprintW. Kry\u015bci\u0144ski, N. S. Keskar, B. McCann, C. Xiong, and R. Socher, \"Neural text summarization: A critical evaluation,\" arXiv preprint arXiv:1908.08960, 2019.\n\nAssessing the factual accuracy of generated text. B Goodrich, V Rao, P J Liu, M Saleh, proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining. the 25th ACM SIGKDD international conference on knowledge discovery & data miningB. Goodrich, V. Rao, P. J. Liu, and M. Saleh, \"Assessing the factual accuracy of generated text,\" in proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, 2019, pp. 166-175.\n\nRanking generated summaries by correctness: An interesting but challenging application for natural language inference. T Falke, L F Ribeiro, P A Utama, I Dagan, I Gurevych, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsT. Falke, L. F. Ribeiro, P. A. Utama, I. Dagan, and I. Gurevych, \"Ranking generated summaries by correctness: An interesting but challenging application for natural language inference,\" in Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019, pp. 2214-2220.\n\nRanking crowd knowledge to assist software development. L B Souza, E C Campos, Proceedings of the 22nd International Conference on Program Comprehension. the 22nd International Conference on Program ComprehensionL. B. De Souza, E. C. Campos et al., \"Ranking crowd knowledge to assist software development,\" in Proceedings of the 22nd International Conference on Program Comprehension, 2014, pp. 72-82.\n\nHow do programmers ask and answer questions on the web?(nier track). C Treude, O Barzilay, Proceedings of the 33rd international conference on software engineering. the 33rd international conference on software engineeringC. Treude, O. Barzilay et al., \"How do programmers ask and answer questions on the web?(nier track),\" in Proceedings of the 33rd interna- tional conference on software engineering, 2011, pp. 804-807.\n\nWhat are mobile developers asking about? a large scale study using stack overflow. C Rosen, E Shihab, Empirical Software Engineering. 213C. Rosen and E. Shihab, \"What are mobile developers asking about? a large scale study using stack overflow,\" Empirical Software Engineering, vol. 21, no. 3, pp. 1192-1223, 2016.\n\nWhy, when, and what: analyzing stack overflow questions by topic, type, and code. M Allamanis, C Sutton, 2013 10th Working conference on mining software repositories (MSR). IEEEM. Allamanis and C. Sutton, \"Why, when, and what: analyzing stack overflow questions by topic, type, and code,\" in 2013 10th Working conference on mining software repositories (MSR). IEEE, 2013, pp. 53-56.\n\nInterrater reliability: the kappa statistic. M L Mchugh, Biochemia medica. 223M. L. McHugh, \"Interrater reliability: the kappa statistic,\" Biochemia medica, vol. 22, no. 3, pp. 276-282, 2012.\n\nCode and named entity recognition in stackoverflow. J Tabassum, M Maddela, W Xu, A Ritter, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL). the 58th Annual Meeting of the Association for Computational Linguistics (ACL)J. Tabassum, M. Maddela, W. Xu, and A. Ritter, \"Code and named entity recognition in stackoverflow,\" in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL), 2020. [Online].\n\nUnsupervised software-specific morphological forms inference from informal discussions. C Chen, Z Xing, W Ximing, The 39th International Conference on Software Engineering. Buenos Aires, ArgentinaIEEEC. Chen, Z. Xing, and W. Ximing, \"Unsupervised software-specific morphological forms inference from informal discussions,\" in The 39th International Conference on Software Engineering, Buenos Aires, Argentina. IEEE, 2017.\n\nImproving api caveats accessibility by mining api caveats knowledge graph. H Li, S Li, 2018 IEEE International Conference on Software Maintenance and Evolution (ICSME). IEEEH. Li, S. Li et al., \"Improving api caveats accessibility by mining api caveats knowledge graph,\" in 2018 IEEE International Conference on Software Maintenance and Evolution (ICSME). IEEE, 2018, pp. 183- 193.\n\nBart large cnn. \"Bart large cnn,\" https://huggingface.co/facebook/bart-large-cnn, 2022, accessed: 2022-1-7.\n\nThe pascal recognising textual entailment challenge. I Dagan, O Glickman, B Magnini, Machine learning challenges workshop. SpringerI. Dagan, O. Glickman, and B. Magnini, \"The pascal recognising textual entailment challenge,\" in Machine learning challenges workshop. Springer, 2005, pp. 177-190.\n\nDocnli: A large-scale dataset for document-level natural language inference. W Yin, D Radev, C Xiong, arXiv:2106.09449arXiv preprintW. Yin, D. Radev, and C. Xiong, \"Docnli: A large-scale dataset for document-level natural language inference,\" arXiv preprint arXiv:2106.09449, 2021.\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, arXiv:1412.6980arXiv preprintD. P. Kingma and J. Ba, \"Adam: A method for stochastic optimization,\" arXiv preprint arXiv:1412.6980, 2014.\n\nNeural document summarization by jointly learning to score and select sentences. Q Zhou, N Yang, F Wei, S Huang, M Zhou, T Zhao, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational Linguistics1Q. Zhou, N. Yang, F. Wei, S. Huang, M. Zhou, and T. Zhao, \"Neural document summarization by jointly learning to score and select sen- tences,\" in Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2018, pp. 654- 663.\n\nGet to the point: Summarization with pointer-generator networks. A See, P J Liu, C D Manning, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsLong Papers1A. See, P. J. Liu, and C. D. Manning, \"Get to the point: Summarization with pointer-generator networks,\" in Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2017, pp. 1073-1083.\n\nDeep communicating agents for abstractive summarization. A Celikyilmaz, A Bosselut, X He, Y Choi, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies1A. Celikyilmaz, A. Bosselut, X. He, and Y. Choi, \"Deep communicating agents for abstractive summarization,\" in Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), 2018, pp. 1662-1675.\n\nThe new york times annotated corpus. E Sandhaus, Linguistic Data Consortium, Philadelphia. 626752E. Sandhaus, \"The new york times annotated corpus,\" Linguistic Data Consortium, Philadelphia, vol. 6, no. 12, p. e26752, 2008.\n\nStack exchange data explorer. \"Stack exchange data explorer,\" https://pypi.org/project/lexrank/, 2022, accessed: 2022-8-15.\n\nAutomatic solution summarization for crash bugs. H Wang, X Xia, 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE). IEEEH. Wang, X. Xia et al., \"Automatic solution summarization for crash bugs,\" in 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE). IEEE, 2021, pp. 1286-1297.\n\nThe use of mmr, diversity-based reranking for reordering documents and producing summaries. J Carbonell, J Goldstein, Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval. the 21st annual international ACM SIGIR conference on Research and development in information retrievalJ. Carbonell and J. Goldstein, \"The use of mmr, diversity-based rerank- ing for reordering documents and producing summaries,\" in Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, 1998, pp. 335-336.\n\nGenerating question titles for stack overflow from mined code snippets. Z Gao, X Xia, ACM Transactions on Software Engineering and Methodology (TOSEM). 294Z. Gao, X. Xia et al., \"Generating question titles for stack overflow from mined code snippets,\" ACM Transactions on Software Engineering and Methodology (TOSEM), vol. 29, no. 4, pp. 1-37, 2020.\n\nCode2que: A tool for improving question titles from mined code snippets in stack overflow. Z Gao, X Xia, D Lo, J Grundy, Y.-F Li, Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software EngineeringZ. Gao, X. Xia, D. Lo, J. Grundy, and Y.-F. Li, \"Code2que: A tool for improving question titles from mined code snippets in stack overflow,\" in Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, 2021, pp. 1525-1529.\n\nSummarizing software artifacts: a case study of bug reports. S Rastkar, G C Murphy, G Murray, 2010 ACM/IEEE 32nd International Conference on Software Engineering. IEEE1S. Rastkar, G. C. Murphy, and G. Murray, \"Summarizing software artifacts: a case study of bug reports,\" in 2010 ACM/IEEE 32nd International Conference on Software Engineering, vol. 1. IEEE, 2010, pp. 505-514.\n\nBugSum: Deep Context Understanding for Bug Report Summarization. H Liu, Y Yu, 10.1145/3387904.3389272Association for Computing MachineryNew York, NY, USAH. Liu, Y. Yu et al., BugSum: Deep Context Understanding for Bug Report Summarization. New York, NY, USA: Association for Computing Machinery, 2020, p. 94-105. [Online]. Available: https://doi.org/10.1145/3387904.3389272\n\nAutomatic summarization of bug reports. S Rastkar, G C Murphy, IEEE Transactions on Software Engineering. 404S. Rastkar, G. C. Murphy et al., \"Automatic summarization of bug reports,\" IEEE Transactions on Software Engineering, vol. 40, no. 4, pp. 366-380, 2014.\n\nAusum: approach for unsupervised bug report summarization. S Mani, R Catherine, V S Sinha, A Dubey, Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering. the ACM SIGSOFT 20th International Symposium on the Foundations of Software EngineeringS. Mani, R. Catherine, V. S. Sinha, and A. Dubey, \"Ausum: approach for unsupervised bug report summarization,\" in Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering, 2012, pp. 1-11.\n\nUnsupervised deep bug report summarization. X Li, H Jiang, D Liu, Z Ren, G Li, 2018 IEEE/ACM 26th International Conference on Program Comprehension (ICPC). IEEE411X. Li, H. Jiang, D. Liu, Z. Ren, and G. Li, \"Unsupervised deep bug re- port summarization,\" in 2018 IEEE/ACM 26th International Conference on Program Comprehension (ICPC). IEEE, 2018, pp. 144-14 411.\n\nDon't give me the details, just the summary!\" Topic-aware Convolutional Neural Networks for Extreme Summarization. S Narayan, S B Cohen, S. Narayan, S. B. Cohen et al., \"Don't give me the details, just the summary!\" Topic-aware Convolutional Neural Networks for Extreme Summarization. In, 2018.\n\nExtractive summarization using deep learning. S Verma, V Nidhi, arXiv:1708.04439arXiv preprintS. Verma and V. Nidhi, \"Extractive summarization using deep learning,\" arXiv preprint arXiv:1708.04439, 2017.\n\nLeveraging bert for extractive text summarization on lectures. D Miller, arXiv:1906.04165arXiv preprintD. Miller, \"Leveraging bert for extractive text summarization on lec- tures,\" arXiv preprint arXiv:1906.04165, 2019.\n\nSeahawk: Stack overflow in the ide. L Ponzanelli, A Bacchelli, M Lanza, 2013 35th International Conference on Software Engineering (ICSE). IEEEL. Ponzanelli, A. Bacchelli, and M. Lanza, \"Seahawk: Stack overflow in the ide,\" in 2013 35th International Conference on Software Engineering (ICSE). IEEE, 2013, pp. 1295-1298.\n\nMining stackoverflow to turn the ide into a self-confident programming prompter. L Ponzanelli, G Bavota, Proceedings of the 11th Working Conference on Mining Software Repositories. the 11th Working Conference on Mining Software RepositoriesL. Ponzanelli, G. Bavota et al., \"Mining stackoverflow to turn the ide into a self-confident programming prompter,\" in Proceedings of the 11th Working Conference on Mining Software Repositories, 2014, pp. 102- 111.\n\nInterrogative-guided re-ranking for question-oriented software text retrieval. T Ye, B Xie, Y Zou, X Chen, Proceedings of the 29th ACM/IEEE international conference on Automated software engineering. the 29th ACM/IEEE international conference on Automated software engineeringT. Ye, B. Xie, Y. Zou, and X. Chen, \"Interrogative-guided re-ranking for question-oriented software text retrieval,\" in Proceedings of the 29th ACM/IEEE international conference on Automated software engineer- ing, 2014, pp. 115-120.\n\nLearning to rank for question-oriented software text retrieval (t). Y Zou, T Ye, Y Lu, J Mylopoulos, L Zhang, 2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE). Y. Zou, T. Ye, Y. Lu, J. Mylopoulos, and L. Zhang, \"Learning to rank for question-oriented software text retrieval (t),\" in 2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE).\n\n. IEEE. IEEE, 2015, pp. 1-11.\n\nImproving the search for architecture knowledge in online developer communities. M Soliman, A R Salama, M Galster, O Zimmermann, M Riebisch, 2018 IEEE International Conference on Software Architecture (ICSA). IEEE609M. Soliman, A. R. Salama, M. Galster, O. Zimmermann, and M. Riebisch, \"Improving the search for architecture knowledge in online developer communities,\" in 2018 IEEE International Conference on Software Architecture (ICSA). IEEE, 2018, pp. 186-18 609.\n\nEffective reformulation of query for code search using crowdsourced knowledge and extra-large data analytics. M M Rahman, C Roy, 2018 IEEE International Conference on Software Maintenance and Evolution (ICSME). IEEEM. M. Rahman and C. Roy, \"Effective reformulation of query for code search using crowdsourced knowledge and extra-large data analytics,\" in 2018 IEEE International Conference on Software Maintenance and Evolution (ICSME). IEEE, 2018, pp. 473-484.\n\nAugmenting api documentation with insights from stack overflow. C Treude, M P Robillard, 2016 IEEE/ACM 38th International Conference on Software Engineering (ICSE). IEEEC. Treude and M. P. Robillard, \"Augmenting api documentation with insights from stack overflow,\" in 2016 IEEE/ACM 38th International Conference on Software Engineering (ICSE). IEEE, 2016, pp. 392- 403.\n\nImproving api caveats accessibility by mining api caveats knowledge graph. H Li, S Li, J Sun, Z Xing, X Peng, M Liu, X Zhao, 2018 IEEE International Conference on Software Maintenance and Evolution (ICSME). IEEEH. Li, S. Li, J. Sun, Z. Xing, X. Peng, M. Liu, and X. Zhao, \"Improving api caveats accessibility by mining api caveats knowledge graph,\" in 2018 IEEE International Conference on Software Maintenance and Evolution (ICSME). IEEE, 2018, pp. 183-193.\n\nLive api documentation. S Subramanian, L Inozemtseva, Proceedings of the 36th International Conference on Software Engineering. the 36th International Conference on Software EngineeringS. Subramanian, L. Inozemtseva et al., \"Live api documentation,\" in Pro- ceedings of the 36th International Conference on Software Engineering, 2014, pp. 643-652.\n\nOpiner: an opinion search and summarization engine for apis. G Uddin, F Khomh, 2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEEG. Uddin and F. Khomh, \"Opiner: an opinion search and summarization engine for apis,\" in 2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 2017, pp. 978-983.\n\nPattern-based mining of opinions in q&a websites. B Lin, F Zampetti, 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE). IEEEB. Lin, F. Zampetti et al., \"Pattern-based mining of opinions in q&a websites,\" in 2019 IEEE/ACM 41st International Conference on Soft- ware Engineering (ICSE). IEEE, 2019, pp. 548-559.\n\nAre code examples on an online q&a forum reliable?: a study of api misuse on stack overflow. T Zhang, G Upadhyaya, A Reinhardt, H Rajan, M Kim, 2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE). IEEET. Zhang, G. Upadhyaya, A. Reinhardt, H. Rajan, and M. Kim, \"Are code examples on an online q&a forum reliable?: a study of api misuse on stack overflow,\" in 2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE). IEEE, 2018, pp. 886-896.\n\nAugmenting stack overflow with api usage patterns mined from github. A Reinhardt, T Zhang, Proceedings of the. the26A. Reinhardt, T. Zhang et al., \"Augmenting stack overflow with api usage patterns mined from github,\" in Proceedings of the 2018 26th\n\nACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, 2018, pp. 880- 883.\n\nApi deprecation: a retrospective analysis and detection method for code examples on the web. J Zhou, R J Walker, Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering. the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software EngineeringJ. Zhou and R. J. Walker, \"Api deprecation: a retrospective analysis and detection method for code examples on the web,\" in Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering, 2016, pp. 266-277.\n\nChatbot4qr: Interactive query refinement for technical question retrieval. N Zhang, Q Huang, X Xia, Y Zou, D Lo, Z Xing, IEEE Transactions on Software Engineering. 484N. Zhang, Q. Huang, X. Xia, Y. Zou, D. Lo, and Z. Xing, \"Chatbot4qr: Interactive query refinement for technical question retrieval,\" IEEE Transactions on Software Engineering, vol. 48, no. 4, pp. 1185-1211, 2022.\n", "annotations": {"author": "[{\"end\":115,\"start\":51},{\"end\":194,\"start\":116},{\"end\":264,\"start\":195}]", "publisher": null, "author_last_name": "[{\"end\":60,\"start\":57},{\"end\":126,\"start\":122},{\"end\":207,\"start\":202}]", "author_first_name": "[{\"end\":56,\"start\":51},{\"end\":121,\"start\":116},{\"end\":201,\"start\":195}]", "author_affiliation": "[{\"end\":114,\"start\":78},{\"end\":193,\"start\":145},{\"end\":263,\"start\":227}]", "title": "[{\"end\":48,\"start\":1},{\"end\":312,\"start\":265}]", "venue": null, "abstract": "[{\"end\":1836,\"start\":399}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1964,\"start\":1961},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1969,\"start\":1966},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2220,\"start\":2217},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2484,\"start\":2481},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2489,\"start\":2486},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2619,\"start\":2616},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3234,\"start\":3231},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3240,\"start\":3236},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3397,\"start\":3393},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3560,\"start\":3556},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5481,\"start\":5477},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5651,\"start\":5648},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5657,\"start\":5653},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5663,\"start\":5659},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5893,\"start\":5889},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6612,\"start\":6608},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8954,\"start\":8953},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10344,\"start\":10341},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10350,\"start\":10346},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10356,\"start\":10352},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":13198,\"start\":13194},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":13334,\"start\":13330},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":13340,\"start\":13336},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":13445,\"start\":13441},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":13451,\"start\":13447},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":15274,\"start\":15270},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":15280,\"start\":15276},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":15286,\"start\":15282},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":15920,\"start\":15916},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":16195,\"start\":16191},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":16821,\"start\":16817},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":16987,\"start\":16983},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":18509,\"start\":18505},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":19665,\"start\":19661},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":21424,\"start\":21421},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":21430,\"start\":21426},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":21436,\"start\":21432},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":22385,\"start\":22383},{\"end\":22610,\"start\":22609},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":25158,\"start\":25154},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":25523,\"start\":25519},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":25754,\"start\":25750},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":25984,\"start\":25980},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":26046,\"start\":26042},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":26398,\"start\":26394},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":26404,\"start\":26400},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":27154,\"start\":27150},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":29483,\"start\":29479},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":30669,\"start\":30665},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":30774,\"start\":30770},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":32095,\"start\":32091},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":33596,\"start\":33592},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":33823,\"start\":33820},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":33829,\"start\":33825},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":33835,\"start\":33831},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":33890,\"start\":33887},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":33909,\"start\":33905},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":34050,\"start\":34046},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":34099,\"start\":34095},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":34105,\"start\":34101},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":34111,\"start\":34107},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":34163,\"start\":34159},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":34186,\"start\":34182},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":34318,\"start\":34314},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":34617,\"start\":34613},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":34757,\"start\":34754},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":34992,\"start\":34989},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":35055,\"start\":35052},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":35238,\"start\":35234},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":35441,\"start\":35440},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":35491,\"start\":35487},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":35579,\"start\":35576},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":35598,\"start\":35594},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":45418,\"start\":45414},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":47493,\"start\":47489},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":49707,\"start\":49703},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":50546,\"start\":50542},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":50733,\"start\":50730},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":50738,\"start\":50735},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":50744,\"start\":50740},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":50872,\"start\":50869},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":50961,\"start\":50957},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":51243,\"start\":51240},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":51687,\"start\":51683},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":51693,\"start\":51689},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":52021,\"start\":52017},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":52027,\"start\":52023},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":52306,\"start\":52302},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":52528,\"start\":52524},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":52674,\"start\":52670},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":52894,\"start\":52890},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":52900,\"start\":52896},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":52906,\"start\":52902},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":52912,\"start\":52908},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":52918,\"start\":52914},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":53009,\"start\":53005},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":53138,\"start\":53134},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":53290,\"start\":53286},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":53480,\"start\":53476},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":54019,\"start\":54015},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":54025,\"start\":54021},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":54031,\"start\":54027},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":54058,\"start\":54054},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":54127,\"start\":54123},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":54233,\"start\":54229},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":54633,\"start\":54629},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":55879,\"start\":55878}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":56134,\"start\":55881},{\"attributes\":{\"id\":\"fig_1\"},\"end\":56310,\"start\":56135},{\"attributes\":{\"id\":\"fig_2\"},\"end\":56365,\"start\":56311},{\"attributes\":{\"id\":\"fig_3\"},\"end\":56448,\"start\":56366},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":56881,\"start\":56449},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":57018,\"start\":56882},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":57309,\"start\":57019},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":57514,\"start\":57310},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":57863,\"start\":57515},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":58128,\"start\":57864}]", "paragraph": "[{\"end\":2378,\"start\":1855},{\"end\":3104,\"start\":2380},{\"end\":3850,\"start\":3106},{\"end\":4616,\"start\":3852},{\"end\":5370,\"start\":4618},{\"end\":6439,\"start\":5372},{\"end\":7063,\"start\":6441},{\"end\":7121,\"start\":7065},{\"end\":8335,\"start\":7123},{\"end\":8816,\"start\":8362},{\"end\":9455,\"start\":8818},{\"end\":10215,\"start\":9457},{\"end\":12537,\"start\":10217},{\"end\":12920,\"start\":12562},{\"end\":14023,\"start\":12922},{\"end\":14701,\"start\":14061},{\"end\":15733,\"start\":14732},{\"end\":16296,\"start\":15735},{\"end\":17112,\"start\":16298},{\"end\":17640,\"start\":17114},{\"end\":18403,\"start\":17681},{\"end\":18900,\"start\":18405},{\"end\":19316,\"start\":18902},{\"end\":19411,\"start\":19318},{\"end\":21437,\"start\":19413},{\"end\":21600,\"start\":21439},{\"end\":21742,\"start\":21602},{\"end\":22227,\"start\":21744},{\"end\":22405,\"start\":22229},{\"end\":22423,\"start\":22407},{\"end\":23256,\"start\":22425},{\"end\":23534,\"start\":23282},{\"end\":23851,\"start\":23536},{\"end\":24279,\"start\":23878},{\"end\":25072,\"start\":24330},{\"end\":26047,\"start\":25118},{\"end\":26603,\"start\":26049},{\"end\":26957,\"start\":26605},{\"end\":27331,\"start\":26959},{\"end\":27632,\"start\":27333},{\"end\":28947,\"start\":27634},{\"end\":29241,\"start\":28949},{\"end\":30126,\"start\":29243},{\"end\":30251,\"start\":30145},{\"end\":30591,\"start\":30253},{\"end\":31005,\"start\":30615},{\"end\":32221,\"start\":31007},{\"end\":32908,\"start\":32223},{\"end\":33484,\"start\":32910},{\"end\":33891,\"start\":33510},{\"end\":34758,\"start\":33893},{\"end\":35492,\"start\":34760},{\"end\":36155,\"start\":35494},{\"end\":36777,\"start\":36181},{\"end\":37050,\"start\":36779},{\"end\":37790,\"start\":37075},{\"end\":38594,\"start\":37792},{\"end\":38652,\"start\":38596},{\"end\":38890,\"start\":38654},{\"end\":40083,\"start\":38892},{\"end\":43814,\"start\":40166},{\"end\":44924,\"start\":43890},{\"end\":45307,\"start\":44926},{\"end\":45840,\"start\":45328},{\"end\":46459,\"start\":45842},{\"end\":47103,\"start\":46461},{\"end\":47772,\"start\":47105},{\"end\":48677,\"start\":47774},{\"end\":49575,\"start\":48679},{\"end\":50575,\"start\":49577},{\"end\":54634,\"start\":50596},{\"end\":55763,\"start\":54652},{\"end\":55880,\"start\":55789}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":23877,\"start\":23852}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":20983,\"start\":20974},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":26414,\"start\":26406},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":31195,\"start\":31186},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":31397,\"start\":31389},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":39249,\"start\":39242}]", "section_header": "[{\"end\":1853,\"start\":1838},{\"end\":8360,\"start\":8338},{\"end\":12560,\"start\":12540},{\"end\":14059,\"start\":14026},{\"end\":14730,\"start\":14704},{\"end\":17679,\"start\":17643},{\"end\":23280,\"start\":23259},{\"end\":24328,\"start\":24282},{\"end\":25081,\"start\":25075},{\"end\":25116,\"start\":25084},{\"end\":30143,\"start\":30129},{\"end\":30613,\"start\":30594},{\"end\":33508,\"start\":33487},{\"end\":36179,\"start\":36158},{\"end\":37073,\"start\":37053},{\"end\":40098,\"start\":40086},{\"end\":40139,\"start\":40101},{\"end\":40164,\"start\":40142},{\"end\":43861,\"start\":43817},{\"end\":43888,\"start\":43864},{\"end\":45326,\"start\":45310},{\"end\":50594,\"start\":50578},{\"end\":54650,\"start\":54637},{\"end\":55787,\"start\":55766},{\"end\":55890,\"start\":55882},{\"end\":56144,\"start\":56136},{\"end\":56320,\"start\":56312},{\"end\":56375,\"start\":56367},{\"end\":56459,\"start\":56450},{\"end\":56893,\"start\":56883},{\"end\":57031,\"start\":57020},{\"end\":57321,\"start\":57311},{\"end\":57525,\"start\":57516},{\"end\":57875,\"start\":57865}]", "table": "[{\"end\":56881,\"start\":56533},{\"end\":57018,\"start\":56988},{\"end\":57309,\"start\":57094},{\"end\":57514,\"start\":57389},{\"end\":57863,\"start\":57563},{\"end\":58128,\"start\":57920}]", "figure_caption": "[{\"end\":56134,\"start\":55892},{\"end\":56310,\"start\":56146},{\"end\":56365,\"start\":56322},{\"end\":56448,\"start\":56377},{\"end\":56533,\"start\":56461},{\"end\":56988,\"start\":56896},{\"end\":57094,\"start\":57035},{\"end\":57389,\"start\":57324},{\"end\":57563,\"start\":57527},{\"end\":57920,\"start\":57878}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":4615,\"start\":4607},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9839,\"start\":9831},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11272,\"start\":11264},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14144,\"start\":14136},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18031,\"start\":18023},{\"end\":25034,\"start\":25026},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":31706,\"start\":31698},{\"end\":41348,\"start\":41340},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":42437,\"start\":42429},{\"end\":44086,\"start\":44080},{\"end\":44323,\"start\":44315}]", "bib_author_first_name": "[{\"end\":58799,\"start\":58798},{\"end\":58809,\"start\":58808},{\"end\":58811,\"start\":58810},{\"end\":59224,\"start\":59223},{\"end\":59231,\"start\":59230},{\"end\":59469,\"start\":59468},{\"end\":59475,\"start\":59474},{\"end\":59738,\"start\":59737},{\"end\":59753,\"start\":59752},{\"end\":60020,\"start\":60019},{\"end\":60026,\"start\":60025},{\"end\":60412,\"start\":60411},{\"end\":60420,\"start\":60419},{\"end\":60834,\"start\":60833},{\"end\":60836,\"start\":60835},{\"end\":60845,\"start\":60844},{\"end\":60847,\"start\":60846},{\"end\":61239,\"start\":61238},{\"end\":61248,\"start\":61247},{\"end\":61627,\"start\":61626},{\"end\":61639,\"start\":61638},{\"end\":61894,\"start\":61893},{\"end\":61903,\"start\":61902},{\"end\":62201,\"start\":62200},{\"end\":62212,\"start\":62211},{\"end\":62214,\"start\":62213},{\"end\":62465,\"start\":62464},{\"end\":62473,\"start\":62472},{\"end\":62683,\"start\":62682},{\"end\":62690,\"start\":62689},{\"end\":63079,\"start\":63078},{\"end\":63086,\"start\":63085},{\"end\":63092,\"start\":63091},{\"end\":63100,\"start\":63099},{\"end\":63446,\"start\":63445},{\"end\":63620,\"start\":63619},{\"end\":63622,\"start\":63621},{\"end\":63635,\"start\":63634},{\"end\":63637,\"start\":63636},{\"end\":63907,\"start\":63906},{\"end\":63916,\"start\":63915},{\"end\":63918,\"start\":63917},{\"end\":64231,\"start\":64230},{\"end\":64241,\"start\":64240},{\"end\":64243,\"start\":64242},{\"end\":64250,\"start\":64249},{\"end\":64264,\"start\":64263},{\"end\":64277,\"start\":64276},{\"end\":64279,\"start\":64278},{\"end\":64738,\"start\":64737},{\"end\":64745,\"start\":64744},{\"end\":64752,\"start\":64751},{\"end\":64758,\"start\":64757},{\"end\":64760,\"start\":64759},{\"end\":64771,\"start\":64770},{\"end\":64773,\"start\":64772},{\"end\":64783,\"start\":64782},{\"end\":65069,\"start\":65068},{\"end\":65078,\"start\":65077},{\"end\":65080,\"start\":65079},{\"end\":65359,\"start\":65358},{\"end\":65368,\"start\":65367},{\"end\":65376,\"start\":65375},{\"end\":65384,\"start\":65383},{\"end\":65652,\"start\":65651},{\"end\":65666,\"start\":65665},{\"end\":65676,\"start\":65675},{\"end\":65685,\"start\":65684},{\"end\":65959,\"start\":65958},{\"end\":65966,\"start\":65965},{\"end\":65973,\"start\":65972},{\"end\":65979,\"start\":65978},{\"end\":66268,\"start\":66267},{\"end\":66282,\"start\":66281},{\"end\":66284,\"start\":66283},{\"end\":66294,\"start\":66293},{\"end\":66304,\"start\":66303},{\"end\":66313,\"start\":66312},{\"end\":66560,\"start\":66559},{\"end\":66572,\"start\":66571},{\"end\":66579,\"start\":66578},{\"end\":66581,\"start\":66580},{\"end\":66588,\"start\":66587},{\"end\":67114,\"start\":67113},{\"end\":67123,\"start\":67122},{\"end\":67125,\"start\":67124},{\"end\":67136,\"start\":67135},{\"end\":67138,\"start\":67137},{\"end\":67147,\"start\":67146},{\"end\":67156,\"start\":67155},{\"end\":67685,\"start\":67684},{\"end\":67687,\"start\":67686},{\"end\":67696,\"start\":67695},{\"end\":67698,\"start\":67697},{\"end\":68101,\"start\":68100},{\"end\":68111,\"start\":68110},{\"end\":68538,\"start\":68537},{\"end\":68547,\"start\":68546},{\"end\":68853,\"start\":68852},{\"end\":68866,\"start\":68865},{\"end\":69200,\"start\":69199},{\"end\":69202,\"start\":69201},{\"end\":69400,\"start\":69399},{\"end\":69412,\"start\":69411},{\"end\":69423,\"start\":69422},{\"end\":69429,\"start\":69428},{\"end\":69916,\"start\":69915},{\"end\":69924,\"start\":69923},{\"end\":69932,\"start\":69931},{\"end\":70326,\"start\":70325},{\"end\":70332,\"start\":70331},{\"end\":70796,\"start\":70795},{\"end\":70805,\"start\":70804},{\"end\":70817,\"start\":70816},{\"end\":71116,\"start\":71115},{\"end\":71123,\"start\":71122},{\"end\":71132,\"start\":71131},{\"end\":71366,\"start\":71365},{\"end\":71368,\"start\":71367},{\"end\":71378,\"start\":71377},{\"end\":71603,\"start\":71602},{\"end\":71611,\"start\":71610},{\"end\":71619,\"start\":71618},{\"end\":71626,\"start\":71625},{\"end\":71635,\"start\":71634},{\"end\":71643,\"start\":71642},{\"end\":72158,\"start\":72157},{\"end\":72165,\"start\":72164},{\"end\":72167,\"start\":72166},{\"end\":72174,\"start\":72173},{\"end\":72176,\"start\":72175},{\"end\":72660,\"start\":72659},{\"end\":72675,\"start\":72674},{\"end\":72687,\"start\":72686},{\"end\":72693,\"start\":72692},{\"end\":73311,\"start\":73310},{\"end\":73673,\"start\":73672},{\"end\":73681,\"start\":73680},{\"end\":74042,\"start\":74041},{\"end\":74055,\"start\":74054},{\"end\":74633,\"start\":74632},{\"end\":74640,\"start\":74639},{\"end\":75003,\"start\":75002},{\"end\":75010,\"start\":75009},{\"end\":75017,\"start\":75016},{\"end\":75023,\"start\":75022},{\"end\":75036,\"start\":75032},{\"end\":75684,\"start\":75683},{\"end\":75695,\"start\":75694},{\"end\":75697,\"start\":75696},{\"end\":75707,\"start\":75706},{\"end\":76066,\"start\":76065},{\"end\":76073,\"start\":76072},{\"end\":76416,\"start\":76415},{\"end\":76427,\"start\":76426},{\"end\":76429,\"start\":76428},{\"end\":76698,\"start\":76697},{\"end\":76706,\"start\":76705},{\"end\":76719,\"start\":76718},{\"end\":76721,\"start\":76720},{\"end\":76730,\"start\":76729},{\"end\":77209,\"start\":77208},{\"end\":77215,\"start\":77214},{\"end\":77224,\"start\":77223},{\"end\":77231,\"start\":77230},{\"end\":77238,\"start\":77237},{\"end\":77644,\"start\":77643},{\"end\":77655,\"start\":77654},{\"end\":77657,\"start\":77656},{\"end\":77871,\"start\":77870},{\"end\":77880,\"start\":77879},{\"end\":78093,\"start\":78092},{\"end\":78287,\"start\":78286},{\"end\":78301,\"start\":78300},{\"end\":78314,\"start\":78313},{\"end\":78654,\"start\":78653},{\"end\":78668,\"start\":78667},{\"end\":79108,\"start\":79107},{\"end\":79114,\"start\":79113},{\"end\":79121,\"start\":79120},{\"end\":79128,\"start\":79127},{\"end\":79608,\"start\":79607},{\"end\":79615,\"start\":79614},{\"end\":79621,\"start\":79620},{\"end\":79627,\"start\":79626},{\"end\":79641,\"start\":79640},{\"end\":80057,\"start\":80056},{\"end\":80068,\"start\":80067},{\"end\":80070,\"start\":80069},{\"end\":80080,\"start\":80079},{\"end\":80091,\"start\":80090},{\"end\":80105,\"start\":80104},{\"end\":80555,\"start\":80554},{\"end\":80557,\"start\":80556},{\"end\":80567,\"start\":80566},{\"end\":80972,\"start\":80971},{\"end\":80982,\"start\":80981},{\"end\":80984,\"start\":80983},{\"end\":81355,\"start\":81354},{\"end\":81361,\"start\":81360},{\"end\":81367,\"start\":81366},{\"end\":81374,\"start\":81373},{\"end\":81382,\"start\":81381},{\"end\":81390,\"start\":81389},{\"end\":81397,\"start\":81396},{\"end\":81764,\"start\":81763},{\"end\":81779,\"start\":81778},{\"end\":82150,\"start\":82149},{\"end\":82159,\"start\":82158},{\"end\":82507,\"start\":82506},{\"end\":82514,\"start\":82513},{\"end\":82886,\"start\":82885},{\"end\":82895,\"start\":82894},{\"end\":82908,\"start\":82907},{\"end\":82921,\"start\":82920},{\"end\":82930,\"start\":82929},{\"end\":83346,\"start\":83345},{\"end\":83359,\"start\":83358},{\"end\":83882,\"start\":83881},{\"end\":83890,\"start\":83889},{\"end\":83892,\"start\":83891},{\"end\":84419,\"start\":84418},{\"end\":84428,\"start\":84427},{\"end\":84437,\"start\":84436},{\"end\":84444,\"start\":84443},{\"end\":84451,\"start\":84450},{\"end\":84457,\"start\":84456}]", "bib_author_last_name": "[{\"end\":58806,\"start\":58800},{\"end\":58815,\"start\":58812},{\"end\":59228,\"start\":59225},{\"end\":59235,\"start\":59232},{\"end\":59472,\"start\":59470},{\"end\":59480,\"start\":59476},{\"end\":59750,\"start\":59739},{\"end\":59760,\"start\":59754},{\"end\":60023,\"start\":60021},{\"end\":60031,\"start\":60027},{\"end\":60417,\"start\":60413},{\"end\":60427,\"start\":60421},{\"end\":60842,\"start\":60837},{\"end\":60851,\"start\":60848},{\"end\":61245,\"start\":61240},{\"end\":61252,\"start\":61249},{\"end\":61636,\"start\":61628},{\"end\":61643,\"start\":61640},{\"end\":61900,\"start\":61895},{\"end\":61907,\"start\":61904},{\"end\":62209,\"start\":62202},{\"end\":62220,\"start\":62215},{\"end\":62470,\"start\":62466},{\"end\":62478,\"start\":62474},{\"end\":62687,\"start\":62684},{\"end\":62697,\"start\":62691},{\"end\":63083,\"start\":63080},{\"end\":63089,\"start\":63087},{\"end\":63097,\"start\":63093},{\"end\":63106,\"start\":63101},{\"end\":63450,\"start\":63447},{\"end\":63632,\"start\":63623},{\"end\":63645,\"start\":63638},{\"end\":63913,\"start\":63908},{\"end\":63924,\"start\":63919},{\"end\":64238,\"start\":64232},{\"end\":64247,\"start\":64244},{\"end\":64261,\"start\":64251},{\"end\":64274,\"start\":64265},{\"end\":64287,\"start\":64280},{\"end\":64742,\"start\":64739},{\"end\":64749,\"start\":64746},{\"end\":64755,\"start\":64753},{\"end\":64768,\"start\":64761},{\"end\":64780,\"start\":64774},{\"end\":64788,\"start\":64784},{\"end\":65075,\"start\":65070},{\"end\":65086,\"start\":65081},{\"end\":65365,\"start\":65360},{\"end\":65373,\"start\":65369},{\"end\":65381,\"start\":65377},{\"end\":65388,\"start\":65385},{\"end\":65663,\"start\":65653},{\"end\":65673,\"start\":65667},{\"end\":65682,\"start\":65677},{\"end\":65692,\"start\":65686},{\"end\":65963,\"start\":65960},{\"end\":65970,\"start\":65967},{\"end\":65976,\"start\":65974},{\"end\":65982,\"start\":65980},{\"end\":66279,\"start\":66269},{\"end\":66291,\"start\":66285},{\"end\":66301,\"start\":66295},{\"end\":66310,\"start\":66305},{\"end\":66320,\"start\":66314},{\"end\":66569,\"start\":66561},{\"end\":66576,\"start\":66573},{\"end\":66585,\"start\":66582},{\"end\":66594,\"start\":66589},{\"end\":67120,\"start\":67115},{\"end\":67133,\"start\":67126},{\"end\":67144,\"start\":67139},{\"end\":67153,\"start\":67148},{\"end\":67165,\"start\":67157},{\"end\":67693,\"start\":67688},{\"end\":67705,\"start\":67699},{\"end\":68108,\"start\":68102},{\"end\":68120,\"start\":68112},{\"end\":68544,\"start\":68539},{\"end\":68554,\"start\":68548},{\"end\":68863,\"start\":68854},{\"end\":68873,\"start\":68867},{\"end\":69209,\"start\":69203},{\"end\":69409,\"start\":69401},{\"end\":69420,\"start\":69413},{\"end\":69426,\"start\":69424},{\"end\":69436,\"start\":69430},{\"end\":69921,\"start\":69917},{\"end\":69929,\"start\":69925},{\"end\":69939,\"start\":69933},{\"end\":70329,\"start\":70327},{\"end\":70335,\"start\":70333},{\"end\":70802,\"start\":70797},{\"end\":70814,\"start\":70806},{\"end\":70825,\"start\":70818},{\"end\":71120,\"start\":71117},{\"end\":71129,\"start\":71124},{\"end\":71138,\"start\":71133},{\"end\":71375,\"start\":71369},{\"end\":71381,\"start\":71379},{\"end\":71608,\"start\":71604},{\"end\":71616,\"start\":71612},{\"end\":71623,\"start\":71620},{\"end\":71632,\"start\":71627},{\"end\":71640,\"start\":71636},{\"end\":71648,\"start\":71644},{\"end\":72162,\"start\":72159},{\"end\":72171,\"start\":72168},{\"end\":72184,\"start\":72177},{\"end\":72672,\"start\":72661},{\"end\":72684,\"start\":72676},{\"end\":72690,\"start\":72688},{\"end\":72698,\"start\":72694},{\"end\":73320,\"start\":73312},{\"end\":73678,\"start\":73674},{\"end\":73685,\"start\":73682},{\"end\":74052,\"start\":74043},{\"end\":74065,\"start\":74056},{\"end\":74637,\"start\":74634},{\"end\":74644,\"start\":74641},{\"end\":75007,\"start\":75004},{\"end\":75014,\"start\":75011},{\"end\":75020,\"start\":75018},{\"end\":75030,\"start\":75024},{\"end\":75039,\"start\":75037},{\"end\":75692,\"start\":75685},{\"end\":75704,\"start\":75698},{\"end\":75714,\"start\":75708},{\"end\":76070,\"start\":76067},{\"end\":76076,\"start\":76074},{\"end\":76424,\"start\":76417},{\"end\":76436,\"start\":76430},{\"end\":76703,\"start\":76699},{\"end\":76716,\"start\":76707},{\"end\":76727,\"start\":76722},{\"end\":76736,\"start\":76731},{\"end\":77212,\"start\":77210},{\"end\":77221,\"start\":77216},{\"end\":77228,\"start\":77225},{\"end\":77235,\"start\":77232},{\"end\":77241,\"start\":77239},{\"end\":77652,\"start\":77645},{\"end\":77663,\"start\":77658},{\"end\":77877,\"start\":77872},{\"end\":77886,\"start\":77881},{\"end\":78100,\"start\":78094},{\"end\":78298,\"start\":78288},{\"end\":78311,\"start\":78302},{\"end\":78320,\"start\":78315},{\"end\":78665,\"start\":78655},{\"end\":78675,\"start\":78669},{\"end\":79111,\"start\":79109},{\"end\":79118,\"start\":79115},{\"end\":79125,\"start\":79122},{\"end\":79133,\"start\":79129},{\"end\":79612,\"start\":79609},{\"end\":79618,\"start\":79616},{\"end\":79624,\"start\":79622},{\"end\":79638,\"start\":79628},{\"end\":79647,\"start\":79642},{\"end\":80065,\"start\":80058},{\"end\":80077,\"start\":80071},{\"end\":80088,\"start\":80081},{\"end\":80102,\"start\":80092},{\"end\":80114,\"start\":80106},{\"end\":80564,\"start\":80558},{\"end\":80571,\"start\":80568},{\"end\":80979,\"start\":80973},{\"end\":80994,\"start\":80985},{\"end\":81358,\"start\":81356},{\"end\":81364,\"start\":81362},{\"end\":81371,\"start\":81368},{\"end\":81379,\"start\":81375},{\"end\":81387,\"start\":81383},{\"end\":81394,\"start\":81391},{\"end\":81402,\"start\":81398},{\"end\":81776,\"start\":81765},{\"end\":81791,\"start\":81780},{\"end\":82156,\"start\":82151},{\"end\":82165,\"start\":82160},{\"end\":82511,\"start\":82508},{\"end\":82523,\"start\":82515},{\"end\":82892,\"start\":82887},{\"end\":82905,\"start\":82896},{\"end\":82918,\"start\":82909},{\"end\":82927,\"start\":82922},{\"end\":82934,\"start\":82931},{\"end\":83356,\"start\":83347},{\"end\":83365,\"start\":83360},{\"end\":83887,\"start\":83883},{\"end\":83899,\"start\":83893},{\"end\":84425,\"start\":84420},{\"end\":84434,\"start\":84429},{\"end\":84441,\"start\":84438},{\"end\":84448,\"start\":84445},{\"end\":84454,\"start\":84452},{\"end\":84462,\"start\":84458}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":2907348},\"end\":59179,\"start\":58701},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":36237266},\"end\":59406,\"start\":59181},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":54087006},\"end\":59667,\"start\":59408},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":206484212},\"end\":59931,\"start\":59669},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":10064728},\"end\":60316,\"start\":59933},{\"attributes\":{\"id\":\"b5\"},\"end\":60350,\"start\":60318},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":209515920},\"end\":60745,\"start\":60352},{\"attributes\":{\"id\":\"b7\"},\"end\":60985,\"start\":60747},{\"attributes\":{\"id\":\"b8\"},\"end\":61159,\"start\":60987},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":52069064},\"end\":61561,\"start\":61161},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":218487109},\"end\":61777,\"start\":61563},{\"attributes\":{\"doi\":\"arXiv:1910.13461\",\"id\":\"b11\"},\"end\":62122,\"start\":61779},{\"attributes\":{\"doi\":\"arXiv:1802.08636\",\"id\":\"b12\"},\"end\":62402,\"start\":62124},{\"attributes\":{\"doi\":\"arXiv:1809.09672\",\"id\":\"b13\"},\"end\":62635,\"start\":62404},{\"attributes\":{\"doi\":\"arXiv:1908.08345\",\"id\":\"b14\"},\"end\":62836,\"start\":62637},{\"attributes\":{\"id\":\"b15\"},\"end\":63025,\"start\":62838},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":249929314},\"end\":63398,\"start\":63027},{\"attributes\":{\"doi\":\"arXiv:1903.10318\",\"id\":\"b17\"},\"end\":63575,\"start\":63400},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":16344891},\"end\":63829,\"start\":63577},{\"attributes\":{\"doi\":\"abs/1109.2128\",\"id\":\"b19\",\"matched_paper_id\":506350},\"end\":64131,\"start\":63831},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":2907348},\"end\":64692,\"start\":64133},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":36237266},\"end\":65002,\"start\":64694},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":57992117},\"end\":65277,\"start\":65004},{\"attributes\":{\"doi\":\"arXiv:2104.14839\",\"id\":\"b23\"},\"end\":65579,\"start\":65279},{\"attributes\":{\"doi\":\"arXiv:1910.12840\",\"id\":\"b24\"},\"end\":65885,\"start\":65581},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":19198109},\"end\":66215,\"start\":65887},{\"attributes\":{\"doi\":\"arXiv:1908.08960\",\"id\":\"b26\"},\"end\":66507,\"start\":66217},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":173188138},\"end\":66992,\"start\":66509},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":196187162},\"end\":67626,\"start\":66994},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":13884025},\"end\":68029,\"start\":67628},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":5701007},\"end\":68452,\"start\":68031},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":15881819},\"end\":68768,\"start\":68454},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":1737567},\"end\":69152,\"start\":68770},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":5421278},\"end\":69345,\"start\":69154},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":218487168},\"end\":69825,\"start\":69347},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":27920551},\"end\":70248,\"start\":69827},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":53278863},\"end\":70631,\"start\":70250},{\"attributes\":{\"id\":\"b37\"},\"end\":70740,\"start\":70633},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":8587959},\"end\":71036,\"start\":70742},{\"attributes\":{\"doi\":\"arXiv:2106.09449\",\"id\":\"b39\"},\"end\":71319,\"start\":71038},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b40\"},\"end\":71519,\"start\":71321},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":49656757},\"end\":72090,\"start\":71521},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":8314118},\"end\":72600,\"start\":72092},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":4406182},\"end\":73271,\"start\":72602},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":185660016},\"end\":73496,\"start\":73273},{\"attributes\":{\"id\":\"b45\"},\"end\":73621,\"start\":73498},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":232173839},\"end\":73947,\"start\":73623},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":6334682},\"end\":74558,\"start\":73949},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":218718703},\"end\":74909,\"start\":74560},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":220665814},\"end\":75620,\"start\":74911},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":11703129},\"end\":75998,\"start\":75622},{\"attributes\":{\"doi\":\"10.1145/3387904.3389272\",\"id\":\"b51\"},\"end\":76373,\"start\":76000},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":14398817},\"end\":76636,\"start\":76375},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":18627014},\"end\":77162,\"start\":76638},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":50771434},\"end\":77526,\"start\":77164},{\"attributes\":{\"id\":\"b55\"},\"end\":77822,\"start\":77528},{\"attributes\":{\"doi\":\"arXiv:1708.04439\",\"id\":\"b56\"},\"end\":78027,\"start\":77824},{\"attributes\":{\"doi\":\"arXiv:1906.04165\",\"id\":\"b57\"},\"end\":78248,\"start\":78029},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":7121673},\"end\":78570,\"start\":78250},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":4509026},\"end\":79026,\"start\":78572},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":17091095},\"end\":79537,\"start\":79028},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":15073022},\"end\":79942,\"start\":79539},{\"attributes\":{\"id\":\"b62\"},\"end\":79973,\"start\":79944},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":50772930},\"end\":80442,\"start\":79975},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":50772922},\"end\":80905,\"start\":80444},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":4472906},\"end\":81277,\"start\":80907},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":53278863},\"end\":81737,\"start\":81279},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":12649217},\"end\":82086,\"start\":81739},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":2865510},\"end\":82454,\"start\":82088},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":174800187},\"end\":82790,\"start\":82456},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":3346631},\"end\":83274,\"start\":82792},{\"attributes\":{\"id\":\"b71\",\"matched_paper_id\":53063990},\"end\":83525,\"start\":83276},{\"attributes\":{\"id\":\"b72\"},\"end\":83786,\"start\":83527},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":15352638},\"end\":84341,\"start\":83788},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":225493238},\"end\":84722,\"start\":84343}]", "bib_title": "[{\"end\":58796,\"start\":58701},{\"end\":59221,\"start\":59181},{\"end\":59466,\"start\":59408},{\"end\":59735,\"start\":59669},{\"end\":60017,\"start\":59933},{\"end\":60409,\"start\":60352},{\"end\":61236,\"start\":61161},{\"end\":61624,\"start\":61563},{\"end\":63076,\"start\":63027},{\"end\":63617,\"start\":63577},{\"end\":63904,\"start\":63831},{\"end\":64228,\"start\":64133},{\"end\":64735,\"start\":64694},{\"end\":65066,\"start\":65004},{\"end\":65956,\"start\":65887},{\"end\":66557,\"start\":66509},{\"end\":67111,\"start\":66994},{\"end\":67682,\"start\":67628},{\"end\":68098,\"start\":68031},{\"end\":68535,\"start\":68454},{\"end\":68850,\"start\":68770},{\"end\":69197,\"start\":69154},{\"end\":69397,\"start\":69347},{\"end\":69913,\"start\":69827},{\"end\":70323,\"start\":70250},{\"end\":70793,\"start\":70742},{\"end\":71600,\"start\":71521},{\"end\":72155,\"start\":72092},{\"end\":72657,\"start\":72602},{\"end\":73308,\"start\":73273},{\"end\":73670,\"start\":73623},{\"end\":74039,\"start\":73949},{\"end\":74630,\"start\":74560},{\"end\":75000,\"start\":74911},{\"end\":75681,\"start\":75622},{\"end\":76413,\"start\":76375},{\"end\":76695,\"start\":76638},{\"end\":77206,\"start\":77164},{\"end\":78284,\"start\":78250},{\"end\":78651,\"start\":78572},{\"end\":79105,\"start\":79028},{\"end\":79605,\"start\":79539},{\"end\":80054,\"start\":79975},{\"end\":80552,\"start\":80444},{\"end\":80969,\"start\":80907},{\"end\":81352,\"start\":81279},{\"end\":81761,\"start\":81739},{\"end\":82147,\"start\":82088},{\"end\":82504,\"start\":82456},{\"end\":82883,\"start\":82792},{\"end\":83343,\"start\":83276},{\"end\":83879,\"start\":83788},{\"end\":84416,\"start\":84343}]", "bib_author": "[{\"end\":58808,\"start\":58798},{\"end\":58817,\"start\":58808},{\"end\":59230,\"start\":59223},{\"end\":59237,\"start\":59230},{\"end\":59474,\"start\":59468},{\"end\":59482,\"start\":59474},{\"end\":59752,\"start\":59737},{\"end\":59762,\"start\":59752},{\"end\":60025,\"start\":60019},{\"end\":60033,\"start\":60025},{\"end\":60419,\"start\":60411},{\"end\":60429,\"start\":60419},{\"end\":60844,\"start\":60833},{\"end\":60853,\"start\":60844},{\"end\":61247,\"start\":61238},{\"end\":61254,\"start\":61247},{\"end\":61638,\"start\":61626},{\"end\":61645,\"start\":61638},{\"end\":61902,\"start\":61893},{\"end\":61909,\"start\":61902},{\"end\":62211,\"start\":62200},{\"end\":62222,\"start\":62211},{\"end\":62472,\"start\":62464},{\"end\":62480,\"start\":62472},{\"end\":62689,\"start\":62682},{\"end\":62699,\"start\":62689},{\"end\":63085,\"start\":63078},{\"end\":63091,\"start\":63085},{\"end\":63099,\"start\":63091},{\"end\":63108,\"start\":63099},{\"end\":63452,\"start\":63445},{\"end\":63634,\"start\":63619},{\"end\":63647,\"start\":63634},{\"end\":63915,\"start\":63906},{\"end\":63926,\"start\":63915},{\"end\":64240,\"start\":64230},{\"end\":64249,\"start\":64240},{\"end\":64263,\"start\":64249},{\"end\":64276,\"start\":64263},{\"end\":64289,\"start\":64276},{\"end\":64744,\"start\":64737},{\"end\":64751,\"start\":64744},{\"end\":64757,\"start\":64751},{\"end\":64770,\"start\":64757},{\"end\":64782,\"start\":64770},{\"end\":64790,\"start\":64782},{\"end\":65077,\"start\":65068},{\"end\":65088,\"start\":65077},{\"end\":65367,\"start\":65358},{\"end\":65375,\"start\":65367},{\"end\":65383,\"start\":65375},{\"end\":65390,\"start\":65383},{\"end\":65665,\"start\":65651},{\"end\":65675,\"start\":65665},{\"end\":65684,\"start\":65675},{\"end\":65694,\"start\":65684},{\"end\":65965,\"start\":65958},{\"end\":65972,\"start\":65965},{\"end\":65978,\"start\":65972},{\"end\":65984,\"start\":65978},{\"end\":66281,\"start\":66267},{\"end\":66293,\"start\":66281},{\"end\":66303,\"start\":66293},{\"end\":66312,\"start\":66303},{\"end\":66322,\"start\":66312},{\"end\":66571,\"start\":66559},{\"end\":66578,\"start\":66571},{\"end\":66587,\"start\":66578},{\"end\":66596,\"start\":66587},{\"end\":67122,\"start\":67113},{\"end\":67135,\"start\":67122},{\"end\":67146,\"start\":67135},{\"end\":67155,\"start\":67146},{\"end\":67167,\"start\":67155},{\"end\":67695,\"start\":67684},{\"end\":67707,\"start\":67695},{\"end\":68110,\"start\":68100},{\"end\":68122,\"start\":68110},{\"end\":68546,\"start\":68537},{\"end\":68556,\"start\":68546},{\"end\":68865,\"start\":68852},{\"end\":68875,\"start\":68865},{\"end\":69211,\"start\":69199},{\"end\":69411,\"start\":69399},{\"end\":69422,\"start\":69411},{\"end\":69428,\"start\":69422},{\"end\":69438,\"start\":69428},{\"end\":69923,\"start\":69915},{\"end\":69931,\"start\":69923},{\"end\":69941,\"start\":69931},{\"end\":70331,\"start\":70325},{\"end\":70337,\"start\":70331},{\"end\":70804,\"start\":70795},{\"end\":70816,\"start\":70804},{\"end\":70827,\"start\":70816},{\"end\":71122,\"start\":71115},{\"end\":71131,\"start\":71122},{\"end\":71140,\"start\":71131},{\"end\":71377,\"start\":71365},{\"end\":71383,\"start\":71377},{\"end\":71610,\"start\":71602},{\"end\":71618,\"start\":71610},{\"end\":71625,\"start\":71618},{\"end\":71634,\"start\":71625},{\"end\":71642,\"start\":71634},{\"end\":71650,\"start\":71642},{\"end\":72164,\"start\":72157},{\"end\":72173,\"start\":72164},{\"end\":72186,\"start\":72173},{\"end\":72674,\"start\":72659},{\"end\":72686,\"start\":72674},{\"end\":72692,\"start\":72686},{\"end\":72700,\"start\":72692},{\"end\":73322,\"start\":73310},{\"end\":73680,\"start\":73672},{\"end\":73687,\"start\":73680},{\"end\":74054,\"start\":74041},{\"end\":74067,\"start\":74054},{\"end\":74639,\"start\":74632},{\"end\":74646,\"start\":74639},{\"end\":75009,\"start\":75002},{\"end\":75016,\"start\":75009},{\"end\":75022,\"start\":75016},{\"end\":75032,\"start\":75022},{\"end\":75041,\"start\":75032},{\"end\":75694,\"start\":75683},{\"end\":75706,\"start\":75694},{\"end\":75716,\"start\":75706},{\"end\":76072,\"start\":76065},{\"end\":76078,\"start\":76072},{\"end\":76426,\"start\":76415},{\"end\":76438,\"start\":76426},{\"end\":76705,\"start\":76697},{\"end\":76718,\"start\":76705},{\"end\":76729,\"start\":76718},{\"end\":76738,\"start\":76729},{\"end\":77214,\"start\":77208},{\"end\":77223,\"start\":77214},{\"end\":77230,\"start\":77223},{\"end\":77237,\"start\":77230},{\"end\":77243,\"start\":77237},{\"end\":77654,\"start\":77643},{\"end\":77665,\"start\":77654},{\"end\":77879,\"start\":77870},{\"end\":77888,\"start\":77879},{\"end\":78102,\"start\":78092},{\"end\":78300,\"start\":78286},{\"end\":78313,\"start\":78300},{\"end\":78322,\"start\":78313},{\"end\":78667,\"start\":78653},{\"end\":78677,\"start\":78667},{\"end\":79113,\"start\":79107},{\"end\":79120,\"start\":79113},{\"end\":79127,\"start\":79120},{\"end\":79135,\"start\":79127},{\"end\":79614,\"start\":79607},{\"end\":79620,\"start\":79614},{\"end\":79626,\"start\":79620},{\"end\":79640,\"start\":79626},{\"end\":79649,\"start\":79640},{\"end\":80067,\"start\":80056},{\"end\":80079,\"start\":80067},{\"end\":80090,\"start\":80079},{\"end\":80104,\"start\":80090},{\"end\":80116,\"start\":80104},{\"end\":80566,\"start\":80554},{\"end\":80573,\"start\":80566},{\"end\":80981,\"start\":80971},{\"end\":80996,\"start\":80981},{\"end\":81360,\"start\":81354},{\"end\":81366,\"start\":81360},{\"end\":81373,\"start\":81366},{\"end\":81381,\"start\":81373},{\"end\":81389,\"start\":81381},{\"end\":81396,\"start\":81389},{\"end\":81404,\"start\":81396},{\"end\":81778,\"start\":81763},{\"end\":81793,\"start\":81778},{\"end\":82158,\"start\":82149},{\"end\":82167,\"start\":82158},{\"end\":82513,\"start\":82506},{\"end\":82525,\"start\":82513},{\"end\":82894,\"start\":82885},{\"end\":82907,\"start\":82894},{\"end\":82920,\"start\":82907},{\"end\":82929,\"start\":82920},{\"end\":82936,\"start\":82929},{\"end\":83358,\"start\":83345},{\"end\":83367,\"start\":83358},{\"end\":83889,\"start\":83881},{\"end\":83901,\"start\":83889},{\"end\":84427,\"start\":84418},{\"end\":84436,\"start\":84427},{\"end\":84443,\"start\":84436},{\"end\":84450,\"start\":84443},{\"end\":84456,\"start\":84450},{\"end\":84464,\"start\":84456}]", "bib_venue": "[{\"end\":58891,\"start\":58817},{\"end\":59267,\"start\":59237},{\"end\":59512,\"start\":59482},{\"end\":59775,\"start\":59762},{\"end\":60116,\"start\":60033},{\"end\":60324,\"start\":60320},{\"end\":60526,\"start\":60429},{\"end\":60831,\"start\":60747},{\"end\":61057,\"start\":60987},{\"end\":61337,\"start\":61254},{\"end\":61660,\"start\":61645},{\"end\":61891,\"start\":61779},{\"end\":62198,\"start\":62124},{\"end\":62462,\"start\":62404},{\"end\":62680,\"start\":62637},{\"end\":62878,\"start\":62838},{\"end\":63189,\"start\":63108},{\"end\":63443,\"start\":63400},{\"end\":63677,\"start\":63647},{\"end\":63943,\"start\":63939},{\"end\":64363,\"start\":64289},{\"end\":64820,\"start\":64790},{\"end\":65120,\"start\":65088},{\"end\":65356,\"start\":65279},{\"end\":65649,\"start\":65581},{\"end\":66040,\"start\":65984},{\"end\":66265,\"start\":66217},{\"end\":66692,\"start\":66596},{\"end\":67254,\"start\":67167},{\"end\":67780,\"start\":67707},{\"end\":68194,\"start\":68122},{\"end\":68586,\"start\":68556},{\"end\":68941,\"start\":68875},{\"end\":69227,\"start\":69211},{\"end\":69531,\"start\":69438},{\"end\":69998,\"start\":69941},{\"end\":70417,\"start\":70337},{\"end\":70647,\"start\":70633},{\"end\":70863,\"start\":70827},{\"end\":71113,\"start\":71038},{\"end\":71363,\"start\":71321},{\"end\":71737,\"start\":71650},{\"end\":72273,\"start\":72186},{\"end\":72842,\"start\":72700},{\"end\":73362,\"start\":73322},{\"end\":73526,\"start\":73498},{\"end\":73761,\"start\":73687},{\"end\":74185,\"start\":74067},{\"end\":74710,\"start\":74646},{\"end\":75183,\"start\":75041},{\"end\":75783,\"start\":75716},{\"end\":76063,\"start\":76000},{\"end\":76479,\"start\":76438},{\"end\":76840,\"start\":76738},{\"end\":77318,\"start\":77243},{\"end\":77641,\"start\":77528},{\"end\":77868,\"start\":77824},{\"end\":78090,\"start\":78029},{\"end\":78387,\"start\":78322},{\"end\":78751,\"start\":78677},{\"end\":79226,\"start\":79135},{\"end\":79732,\"start\":79649},{\"end\":79950,\"start\":79946},{\"end\":80182,\"start\":80116},{\"end\":80653,\"start\":80573},{\"end\":81070,\"start\":80996},{\"end\":81484,\"start\":81404},{\"end\":81865,\"start\":81793},{\"end\":82250,\"start\":82167},{\"end\":82599,\"start\":82525},{\"end\":83010,\"start\":82936},{\"end\":83385,\"start\":83367},{\"end\":83645,\"start\":83527},{\"end\":84004,\"start\":83901},{\"end\":84505,\"start\":84464},{\"end\":58952,\"start\":58893},{\"end\":64424,\"start\":64365},{\"end\":66775,\"start\":66694},{\"end\":67328,\"start\":67256},{\"end\":67840,\"start\":67782},{\"end\":68253,\"start\":68196},{\"end\":69611,\"start\":69533},{\"end\":70023,\"start\":70000},{\"end\":71811,\"start\":71739},{\"end\":72347,\"start\":72275},{\"end\":72971,\"start\":72844},{\"end\":74290,\"start\":74187},{\"end\":75312,\"start\":75185},{\"end\":76929,\"start\":76842},{\"end\":78812,\"start\":78753},{\"end\":79304,\"start\":79228},{\"end\":81924,\"start\":81867},{\"end\":83390,\"start\":83387},{\"end\":84094,\"start\":84006}]"}}}, "year": 2023, "month": 12, "day": 17}