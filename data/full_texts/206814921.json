{"id": 206814921, "updated": "2023-12-13 00:18:26.862", "metadata": {"title": "Multi-scale convolutional neural networks for crowd counting", "authors": "[{\"first\":\"Lingke\",\"last\":\"Zeng\",\"middle\":[]},{\"first\":\"Xiangmin\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Bolun\",\"last\":\"Cai\",\"middle\":[]},{\"first\":\"Suo\",\"last\":\"Qiu\",\"middle\":[]},{\"first\":\"Tong\",\"last\":\"Zhang\",\"middle\":[]}]", "venue": "2017 IEEE International Conference on Image Processing (ICIP)", "journal": "2017 IEEE International Conference on Image Processing (ICIP)", "publication_date": {"year": 2017, "month": null, "day": null}, "abstract": "Crowd counting on static images is a challenging problem due to scale variations. Recently deep neural networks have been shown to be effective in this task. However, existing neural-networks-based methods often use the multi-column or multi-network model to extract the scale-relevant features, which is more complicated for optimization and computation wasting. To this end, we propose a novel multi-scale convolutional neural network (MSCNN) for single image crowd counting. Based on the multi-scale blobs, the network is able to generate scale-relevant features for higher crowd counting performances in a single-column architecture, which is both accuracy and cost effective for practical applications. Complemental results show that our method outperforms the state-of-the-art methods on both accuracy and robustness with far less number of parameters.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2949530574", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icip/ZengXCQZ17", "doi": "10.1109/icip.2017.8296324"}}, "content": {"source": {"pdf_hash": "ae47e73c25755427c9f5904425a35d7db737829b", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1702.02359v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1702.02359", "status": "GREEN"}}, "grobid": {"id": "75278890a754e186489eb883cd2434d43346a667", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/ae47e73c25755427c9f5904425a35d7db737829b.txt", "contents": "\nMULTI-SCALE CONVOLUTIONAL NEURAL NETWORKS FOR CROWD COUNTING\n\n\nLingke Zeng \nSchool of Electronic and Information Engineering South\nChina University of Technology\nGuangzhouChina\n\nXiangmin Xu xmxu@scut.edu.cn \nSchool of Electronic and Information Engineering South\nChina University of Technology\nGuangzhouChina\n\nBolun Cai caibolun@gmail.com \nSchool of Electronic and Information Engineering South\nChina University of Technology\nGuangzhouChina\n\nSuo Qiu \nSchool of Electronic and Information Engineering South\nChina University of Technology\nGuangzhouChina\n\nTong Zhang \nSchool of Electronic and Information Engineering South\nChina University of Technology\nGuangzhouChina\n\nMULTI-SCALE CONVOLUTIONAL NEURAL NETWORKS FOR CROWD COUNTING\nIndex Terms-Multi-scale CNNscale-relevant architec- turescrowd counting\nCrowd counting on static images is a challenging problem due to scale variations. Recently deep neural networks have been shown to be effective in this task. However, existing neural-networks-based methods often use the multi-column or multi-network model to extract the scale-relevant features, which is more complicated for optimization and computation wasting. To this end, we propose a novel multi-scale convolutional neural network (MSCNN) for single image crowd counting. Based on the multi-scale blobs, the network is able to generate scale-relevant features for higher crowd counting performances in a single-column architecture, which is both accuracy and cost effective for practical applications. Complemental results show that our method outperforms the stateof-the-art methods on both accuracy and robustness with far less number of parameters.\n\nINTRODUCTION\n\nCrowd counting aims to estimate the number of people in the crowded images or videos feed from surveillance cameras. Overcrowding in scenarios such as tourist attractions and public rallies can cause crowd crushes, blockages and even stampedes. It has been much significant to public safety to produce an accurate and robust crowd count estimation using computer vision techniques.\n\nExisting methods of crowd counting can be generally divided into two categories: detection-based methods and regression-based methods.\n\nDetection-based methods generally assume that each person on the crowd images can be detected and located by using the given visual object detector [1, 2,3], and obtain the counting result by accumulating each detected person. However, these methods [4,5,6] need huge computing resource and they are often limited by person occlusions and complex background in practical scenarios, resulting at a relatively low robustness and accuracy. * Xiangmin Xu is the corresponding author.\n\nRegression-based methods regress the crowd count from the image directly. Chan et al. [7] used handcraft features to translate the crowd counting task into a regression problem. Following works [8,9] proposed more kinds of crowd-relevant features including segment-based features, structural-based features and local texture features. Lempitsky et al. [10] proposed a density-based algorithm that obtain the count by integrating the estimated density map.\n\nRecently, deep convolutional neural networks have been shown to be effective in crowd counting. Zhang et al. [11] proposed a convolutional neural network (CNN) to alternatively learn the crowd density and the crowd count. Wang et al. [12] directly used a CNN-based model to map the image patch to its people count value. However, these single-CNNbased algorithms are limited to extract scale-relevant features and hard to address the scale variations on crowd images. Zhang et al. [13] proposed a multi-column CNN to extract multi-scale features by columns with different kernel sizes. Boominathan et al. [14] proposed a multi-network CNN that used a deep and shallow network to improve the spatial resolution. These improved algorithms can relatively suppress the scale variations problem, but they still have two shortages:\n\n\u2022 Multi-column/network need pre-trained single-network for global optimization, which is more complicated than end-to-end training.\n\n\u2022 Multi-column/network introduce more parameters to consume more computing resource, which make it hard for practical application.\n\nIn this paper, we propose a multi-scale convolutional neural network (MSCNN) to extract scale-relevant features. Rather than adding more columns or networks, we only introduce a multi-scale blob with different kernel sizes similar to the naive Inception module [15]. Our approach outperforms the state-of-the-art methods on the ShanghaiTech and UCF CC 50 dataset with a small number of parameters.\n\n\nMULTI-SCALE CNN FOR CROWD COUNTING\n\nCrowd images are usually consisted of various sizes of persons pixels due to perspective distortion. Single-network is hard to counter scale variations with the same sized kernels combination. In [15], a Inception module is proposed to process visual information at various scales and aggregated to the next stage. Motivated by it, we designed a multi-scale convolutional neural network (MSCNN) to learn the scale-relevant density maps from original images.\n\n\nMulti-scale Network Architecture\n\nAn overview of MSCNN is illustrated in Figure. 1, including feature remapping, multi-scale feature extraction, and density map regression. The first convolution layer is a traditional convolutional layer with single-sized kernels to remap the image feature. Multi-Scale Blob (MSB) is a Inceptionlike model (as Figure. 2) to extract the scale-relevant features, which consists of multiple filters with different kernel size (including 9\u00d79, 7\u00d77, 5\u00d75 and 3\u00d73). A multi-layer perceptron (MLP) [16] convolution layer works as a pixel-wise fully connection, which has multiple 1 \u00d7 1 convolutional filters to regress the density map. Rectified linear unit (ReLU) [17] is applied after each convolution layer, which works as the activation function of previous convolutional layers except the last one. Since the value in density map is always positive, adding ReLU after last convolutional layer can enhance the density map restoration. Detailed parameter settings are listed in Table 1.  \n/3/2/1 ReLU - - - Down-sample MAX Pool - 2\u00d72 0 Multi-scale Feature MSB Conv 4\u00d732 (9/7/5/3)\u00d7(9/7/5/3) 4/3/2/1 ReLU - - - MSB Conv 4\u00d732 (9/7/5/3)\u00d7(9/7/5/3) 4/3/2/1 ReLU - - - Down-sample MAX Pool - 2\u00d72 0 Multi-scale Feature MSB Conv 3\u00d764 (7/5/3)\u00d7(7/5/3) 3/2/1 ReLU - - - MSB Conv 3\u00d764 (7/5/3)\u00d7(7/5/3) 3/2/1 ReLU - - - Density Map Regression MLP Conv 1000 1\u00d71 0 ReLU - - - Conv 1 1\u00d71 0 ReLU - - -\n\nScale-relevant Density Map\n\nFollowing Zhang et al. [13], we estimate the crowd density map directly from the input image. To generate a scalerelevant density map with high quality, the scale-adaptive kernel is currently the best choice. For each head annotation of the image, we represent it as a delta function \u03b4 (x \u2212 x i ) and describe its distribution with a Gaussian kernel G \u03c3 so that the density map can be represented as\nF (x) = H (x) * G \u03c3 (x)\nand finally accumulated to the crowd count value. If we assume that the crowd is evenly distributed on the ground plane, the average distance d i between the head x i and its nearest 10 annotations can generally characterize the geometric distortion caused by perspective effect using the Eq. (1), where M is the total number of head annotations in the image and we fix \u03b2 = 0.3 as [13] empirically.  Figure. 3. The ground truth and estimated density map of test images in ShanghaiTech dataset\nF (x) = M i=1 \u03b4 (x \u2212 x i ) * G \u03c3i , with \u03c3 i = \u03b2d i(1)\n\nModel Optimization\n\nThe output from our model is mapped to the density map, Euclidean distance is used to measure the difference between the output feature map and the corresponding ground truth. The loss function that needs to be optimized is defined as Eq.\n\n(2), where \u0398 represents the parameters of the model while F (X i ; \u0398) represents the output of the model. X i and F i are respectively the i th input image and density map ground truth.\nL (\u0398) = 1 2N N i=1 F (X i ; \u0398) \u2212 F i 2 2(2)\n\nEXPERIMENTS\n\nWe evaluate our multi-scale convolutional neural network (MSCNN) for crowd counting on two different datasets, which include the ShanghaiTech and UCF CC 50 datasets. The experimental results show that our MSCNN outperforms the state-of-the-art methods on both accuracy and robustness with far less parameter. All of the convolutional neural networks are trained based on Caffe [18].\n\n\nEvaluation Metric\n\nFollowing existing state-of-the-art methods [13], we use the mean absolute error (MAE), the mean squared error (MSE) and the number of neural networks parameters (PARAMS) to evaluate the performance on the testing datasets. The MAE and the MSE are defined in Eq. (3) and Eq. (4).\nM AE = 1 N N i=1 |z i \u2212\u1e91 i | (3) M SE = 1 N N i=1 (z i \u2212\u1e91 i ) 2(4)\nHere N represents the total number of images in the testing datasets, z i and\u1e91 i are the ground truth and the estimated value respectively for the i th image. In general, MAE, MSE and PARAMS can respectively indicate the accuracy, robustness and computation complexity of a method.\n\n\nThe ShanghaiTech Dataset\n\nThe ShanghaiTech dataset is a large-scale crowd counting dataset introduced by [13]. It contains 1198 annotated images with a total of 330,165 persons. The dataset consists of 2 parts: Part A has 482 images crawled from the Internet and Part B has 716 images taken from the busy streets. Following [13], both of them are divided into a training set with 300 images and a testing set with the remainder.\n\n\nModel Training\n\nTo ensure a sufficient number of data for model training, we perform data augmentation by cropping 9 patches from each image and flipping them. We simply fix the 9 cropped points as top, center and bottom combining with left, center and right. Each patch is 90% of the original size.\n\nIn order to facilitate comparison with MCNN architecture [13], the network was designed similar to the largest column of MCNN but with MSB, of which detailed settings are described in Table 1. All convolutional kernels are initialized with Gaussian weight setting standard deviation to 0.01. As described in Sec. 2.3, we use the SGD optimization with momentum of 0.9 and weight decay as 0.0005.\n\n\nResults\n\nWe compare our method with 4 existing methods on the ShanghaiTech dataset. The LBP+RR method used LBP feature to regress the function between the counting value and the input image. Zhang et al. [11] designed a convolutional network to regress both the density map and the crowd count value from original pixels. A multi-column CNN [13] is proposed to estimate the crowd count value (MCNN-CCR) and crowd density map (MCNN).\n\nIn Table 2, the results illustrate that our approach achieves the state-of-the-art performance on the ShanghaiTech dataset. In addition, it should be emphasized that the number of our parameters is far less than other two CNN-based algorithms. MSCNN uses approximately 7\u00d7 fewer parameters than the state-of-the-art method (MCNN) with higher accuracy and robustness. \n\n\nThe UCF CC 50 Dataset\n\nThe UCF CC 50 dataset [19] contains 50 gray scale images with a total 63,974 annotated persons. The number of people range from 94 to 4543 with an average 1280 individuals per image. Following [11,13,14], we divide the dataset into five splits evenly so that each split contains 10 images. Then we use 5-fold cross-validation to evaluate the performance of our proposed method.\n\n\nModel Training\n\nThe most challenging problem of the UCF CC 50 dataset is the limited number of images for training while the people count in the images span too large. To ensure enough number of training data, we perform a data augmentation strategy following [14] by randomly cropping 36 patches with size 225\u00d7225 from each image and flipping them as similar in Sec. 3.2.1. We train 5 models using 5 splits of training set. The MAE and the MSE are calculated after all the 5 models obtained the estimated results of the corresponding validation set. During training, the MSCNN model is initialized almost the same as the experiment on the ShanghaiTech dataset except that the learning rate is fixed to be 1e-7 to guarantee the model convergence.\n\n\nResults\n\nWe compared our method on the UCF CC 50 dataset with 6 existing methods. In [20,10,19], handcraft features are used to regress the density map from the input image. Three CNN-based methods [11,14,13] proposed to used multicolumn/network and perform evaluation on the UCF CC 50 dataset. Table 3 illustrates that our approach also achieves the state-of-the-art performance on the UCF CC 50 dataset. Here our parameters number is approximately 5\u00d7 fewer than the CrowdNet model, demonstrating that our proposed MSCNN can work more accurately and robustly.  [19] 419.5 541.6 -Zhang et al. [11] 467.0 498.5 7.1M CrowdNet [14] 452.5 -14.8M MCNN [13] 377. \n\n\nCONCLUSION\n\nIn this paper, we proposed a multi-scale convolutional neural network (MSCNN) for crowd counting. Compared with the recent CNN-based methods, our algorithm can extract scale-relevant features from crowd images using a single column network based on the multi-scale blob (MSB). It is an end-to-end training method with no requirement for multicolumn/network pre-training works. Our method can achieve more accurate and robust crowd counting performance with far less number of parameters, which make it more likely to extend to the practical application. \n\n\nACKNOWLEDGMENT\n\n\nREFERENCES\n\n[1] Sheng-Fuu Lin, Jaw-Yeh Chen, and Hung-Xin Chao, \"Estimation of number of people in crowded scenes using perspective transformation,\" IEEE Transactions on\n\nFigure. 1 .\n1Multi-scale convolutional neural network for crowd counting.\n\nFigure. 2 .\n2Multi-scale blob with different kernel size.\n\n\nThis work is supported by the National Natural Science Foundation of China (61171142, 61401163), Science and Technology Planning Project of Guangdong Province of China (2014B010111003, 2014B010111006), and Guangzhou Key Lab of Body Data Science (201605030011).\n\nTable . 1\n.. The multi-scale CNN architecture.Formulation \nType \nNum. \nFilter Size \nPad \nFeature \nRemap \n\nConv \n64 \n9\u00d79 \n4 \nReLU \n-\n-\n-\nMulti-scale \nFeature \n\nMSB Conv \n4\u00d716 \n(9/7/5/3)\u00d7(9/7/5/3) 4\n\nTable . 2\n.. Performances of methods on ShanghaiTech dataset.Method \nPart A \nPart B \nPARAMS \nMAE \nMSE \nMAE MSE \nLBP+RR \n303.2 \n371.0 \n59.1 \n81.7 \n-\nMCNN-CCR [13] \n245.0 \n336.1 \n70.9 \n95.9 \n-\nZhang et al. [11] \n181.8 \n277.7 \n32.0 \n49.8 \n7.1M \nMCNN [13] \n110.2 \n173.2 \n26.4 \n41.3 \n19.2M \nMSCNN \n83.8 \n127.4 \n17.7 \n30.2 \n2.9M \n\n\n\nTable . 3\n.. Performances of methods on UCF CC 50 dataset.Method \nMAE \nMSE \nPARAMS \nRodriguez et al. [20] \n655.7 \n697.8 \n-\nLempitsky et al. [10] \n493.4 \n487.1 \n-\nIdrees et al. \n\n. Man Systems, Cybernetics-Part , A , Systems and Humans. 316Systems, Man, and Cybernetics-Part A: Systems and Humans, vol. 31, no. 6, pp. 645-654, 2001.\n\nHistograms of oriented gradients for human detection. Navneet Dalal, Bill Triggs, Computer Vision and Pattern Recognition. 1Navneet Dalal and Bill Triggs, \"Histograms of ori- ented gradients for human detection,\" in Computer Vi- sion and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on. IEEE, 2005, vol. 1, pp. 886-893.\n\nAutomatic adaptation of a generic pedestrian detector to a specific traffic scene. Meng Wang, Xiaogang Wang, Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on. IEEEMeng Wang and Xiaogang Wang, \"Automatic adapta- tion of a generic pedestrian detector to a specific traf- fic scene,\" in Computer Vision and Pattern Recogni- tion (CVPR), 2011 IEEE Conference on. IEEE, 2011, pp. 3401-3408.\n\nMarked point processes for crowd counting. Weina Ge, T Robert, Collins, Computer Vision and Pattern Recognition. Weina Ge and Robert T Collins, \"Marked point pro- cesses for crowd counting,\" in Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Confer- ence on. IEEE, 2009, pp. 2913-2920.\n\nDetecting humans in dense crowds using locallyconsistent scale prior and global occlusion reasoning. Haroon Idrees, Khurram Soomro, Mubarak Shah, IEEE transactions on pattern analysis and machine intelligence. 37Haroon Idrees, Khurram Soomro, and Mubarak Shah, \"Detecting humans in dense crowds using locally- consistent scale prior and global occlusion reasoning,\" IEEE transactions on pattern analysis and machine in- telligence, vol. 37, no. 10, pp. 1986-1998, 2015.\n\nShape-based human detection and segmentation via hierarchical part-template matching. Zhe Lin, S Larry, Davis, IEEE Transactions on Pattern Analysis and Machine Intelligence. 324Zhe Lin and Larry S Davis, \"Shape-based human de- tection and segmentation via hierarchical part-template matching,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 32, no. 4, pp. 604-618, 2010.\n\nPrivacy preserving crowd monitoring: Counting people without people models or tracking. B Antoni, Zhang-Sheng John Chan, Nuno Liang, Vasconcelos, Computer Vision and Pattern Recognition. IEEEAntoni B Chan, Zhang-Sheng John Liang, and Nuno Vasconcelos, \"Privacy preserving crowd monitoring: Counting people without people models or tracking,\" in Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on. IEEE, 2008, pp. 1-7.\n\nBayesian poisson regression for crowd counting. B Antoni, Nuno Chan, Vasconcelos, IEEE 12th International Conference on. IEEEComputer VisionAntoni B Chan and Nuno Vasconcelos, \"Bayesian pois- son regression for crowd counting,\" in Computer Vision, 2009 IEEE 12th International Conference on. IEEE, 2009, pp. 545-551.\n\nFeature mining for localised crowd counting. Ke Chen, Chen Change Loy, Shaogang Gong, Tony Xiang, BMVC. 13Ke Chen, Chen Change Loy, Shaogang Gong, and Tony Xiang, \"Feature mining for localised crowd counting.,\" in BMVC, 2012, vol. 1, p. 3.\n\nLearning to count objects in images. Victor Lempitsky, Andrew Zisserman, Advances in Neural Information Processing Systems. Victor Lempitsky and Andrew Zisserman, \"Learning to count objects in images,\" in Advances in Neural Infor- mation Processing Systems, 2010, pp. 1324-1332.\n\nCross-scene crowd counting via deep convolutional neural networks. Cong Zhang, Hongsheng Li, Xiaogang Wang, Xiaokang Yang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionCong Zhang, Hongsheng Li, Xiaogang Wang, and Xi- aokang Yang, \"Cross-scene crowd counting via deep convolutional neural networks,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 833-841.\n\nDeep people counting in extremely dense crowds. Chuan Wang, Hua Zhang, Liang Yang, Si Liu, Xiaochun Cao, Proceedings of the 23rd ACM international conference on Multimedia. the 23rd ACM international conference on MultimediaACMChuan Wang, Hua Zhang, Liang Yang, Si Liu, and Xi- aochun Cao, \"Deep people counting in extremely dense crowds,\" in Proceedings of the 23rd ACM international conference on Multimedia. ACM, 2015, pp. 1299-1302.\n\nSingle-image crowd counting via multi-column convolutional neural network. Yingying Zhang, Desen Zhou, Siqin Chen, Shenghua Gao, Yi Ma, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionYingying Zhang, Desen Zhou, Siqin Chen, Shenghua Gao, and Yi Ma, \"Single-image crowd counting via multi-column convolutional neural network,\" in Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 589-597.\n\nCrowdnet: A deep convolutional network for dense crowd counting. Lokesh Boominathan, S S Srinivas, R Venkatesh Kruthiventi, Babu, Proceedings of the 2016 ACM on Multimedia Conference. the 2016 ACM on Multimedia ConferenceACMLokesh Boominathan, Srinivas SS Kruthiventi, and R Venkatesh Babu, \"Crowdnet: A deep convolutional network for dense crowd counting,\" in Proceedings of the 2016 ACM on Multimedia Conference. ACM, 2016, pp. 640-644.\n\nGoing deeper with convolutions. Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Ser- manet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich, \"Going deeper with convolutions,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion, 2015, pp. 1-9.\n\nNetwork in network. Min Lin, Qiang Chen, Shuicheng Yan, arXiv:1312.4400arXiv preprintMin Lin, Qiang Chen, and Shuicheng Yan, \"Network in network,\" arXiv preprint arXiv:1312.4400, 2013.\n\nRectified linear units improve restricted boltzmann machines. Vinod Nair, Geoffrey E Hinton, Proceedings of the 27th international conference on machine learning (ICML-10). the 27th international conference on machine learning (ICML-10)Vinod Nair and Geoffrey E Hinton, \"Rectified linear units improve restricted boltzmann machines,\" in Pro- ceedings of the 27th international conference on ma- chine learning (ICML-10), 2010, pp. 807-814.\n\nCaffe: Convolutional architecture for fast feature embedding. Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, Trevor Darrell, Proceedings of the 22nd ACM international conference on Multimedia. the 22nd ACM international conference on MultimediaACMYangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadar- rama, and Trevor Darrell, \"Caffe: Convolutional archi- tecture for fast feature embedding,\" in Proceedings of the 22nd ACM international conference on Multimedia. ACM, 2014, pp. 675-678.\n\nMulti-source multi-scale counting in extremely dense crowd images. Haroon Idrees, Imran Saleemi, Cody Seibert, Mubarak Shah, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionHaroon Idrees, Imran Saleemi, Cody Seibert, and Mubarak Shah, \"Multi-source multi-scale counting in extremely dense crowd images,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2013, pp. 2547-2554.\n\nDensity-aware person detection and tracking in crowds. Mikel Rodriguez, Ivan Laptev, Josef Sivic, Jean-Yves Audibert, Computer Vision (ICCV), 2011 IEEE International Conference on. IEEEMikel Rodriguez, Ivan Laptev, Josef Sivic, and Jean- Yves Audibert, \"Density-aware person detection and tracking in crowds,\" in Computer Vision (ICCV), 2011 IEEE International Conference on. IEEE, 2011, pp. 2423-2430.\n", "annotations": {"author": "[{\"end\":178,\"start\":64},{\"end\":310,\"start\":179},{\"end\":442,\"start\":311},{\"end\":553,\"start\":443},{\"end\":667,\"start\":554}]", "publisher": null, "author_last_name": "[{\"end\":75,\"start\":71},{\"end\":190,\"start\":188},{\"end\":320,\"start\":317},{\"end\":450,\"start\":447},{\"end\":564,\"start\":559}]", "author_first_name": "[{\"end\":70,\"start\":64},{\"end\":187,\"start\":179},{\"end\":316,\"start\":311},{\"end\":446,\"start\":443},{\"end\":558,\"start\":554}]", "author_affiliation": "[{\"end\":177,\"start\":77},{\"end\":309,\"start\":209},{\"end\":441,\"start\":341},{\"end\":552,\"start\":452},{\"end\":666,\"start\":566}]", "title": "[{\"end\":61,\"start\":1},{\"end\":728,\"start\":668}]", "venue": null, "abstract": "[{\"end\":1658,\"start\":801}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2347,\"start\":2345},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2349,\"start\":2347},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2446,\"start\":2443},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2448,\"start\":2446},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2450,\"start\":2448},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2763,\"start\":2760},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2871,\"start\":2868},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2873,\"start\":2871},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3030,\"start\":3026},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3244,\"start\":3240},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3369,\"start\":3365},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3616,\"start\":3612},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3740,\"start\":3736},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4488,\"start\":4484},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4859,\"start\":4855},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5646,\"start\":5642},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5813,\"start\":5809},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6586,\"start\":6582},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7368,\"start\":7364},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8417,\"start\":8413},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8488,\"start\":8484},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9180,\"start\":9176},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9399,\"start\":9395},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9864,\"start\":9860},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10408,\"start\":10404},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10545,\"start\":10541},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":11052,\"start\":11048},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11223,\"start\":11219},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11226,\"start\":11223},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11229,\"start\":11226},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11670,\"start\":11666},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12244,\"start\":12240},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12247,\"start\":12244},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12250,\"start\":12247},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":12357,\"start\":12353},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12360,\"start\":12357},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":12363,\"start\":12360},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12721,\"start\":12717},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":12752,\"start\":12748},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12783,\"start\":12779},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":12806,\"start\":12802}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":13645,\"start\":13571},{\"attributes\":{\"id\":\"fig_1\"},\"end\":13704,\"start\":13646},{\"attributes\":{\"id\":\"fig_2\"},\"end\":13967,\"start\":13705},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":14165,\"start\":13968},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":14492,\"start\":14166},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":14670,\"start\":14493}]", "paragraph": "[{\"end\":2055,\"start\":1674},{\"end\":2191,\"start\":2057},{\"end\":2672,\"start\":2193},{\"end\":3129,\"start\":2674},{\"end\":3956,\"start\":3131},{\"end\":4089,\"start\":3958},{\"end\":4221,\"start\":4091},{\"end\":4620,\"start\":4223},{\"end\":5116,\"start\":4659},{\"end\":6135,\"start\":5153},{\"end\":6958,\"start\":6559},{\"end\":7475,\"start\":6983},{\"end\":7790,\"start\":7552},{\"end\":7977,\"start\":7792},{\"end\":8418,\"start\":8036},{\"end\":8719,\"start\":8440},{\"end\":9068,\"start\":8787},{\"end\":9499,\"start\":9097},{\"end\":9801,\"start\":9518},{\"end\":10197,\"start\":9803},{\"end\":10632,\"start\":10209},{\"end\":11000,\"start\":10634},{\"end\":11403,\"start\":11026},{\"end\":12152,\"start\":11422},{\"end\":12812,\"start\":12164},{\"end\":13381,\"start\":12827},{\"end\":13570,\"start\":13413}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":6529,\"start\":6136},{\"attributes\":{\"id\":\"formula_1\"},\"end\":6982,\"start\":6959},{\"attributes\":{\"id\":\"formula_2\"},\"end\":7530,\"start\":7476},{\"attributes\":{\"id\":\"formula_3\"},\"end\":8021,\"start\":7978},{\"attributes\":{\"id\":\"formula_4\"},\"end\":8786,\"start\":8720}]", "table_ref": "[{\"end\":6132,\"start\":6125},{\"end\":9994,\"start\":9987},{\"end\":10644,\"start\":10637},{\"end\":12457,\"start\":12450}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1672,\"start\":1660},{\"attributes\":{\"n\":\"2.\"},\"end\":4657,\"start\":4623},{\"attributes\":{\"n\":\"2.1.\"},\"end\":5151,\"start\":5119},{\"attributes\":{\"n\":\"2.2.\"},\"end\":6557,\"start\":6531},{\"attributes\":{\"n\":\"2.3.\"},\"end\":7550,\"start\":7532},{\"attributes\":{\"n\":\"3.\"},\"end\":8034,\"start\":8023},{\"attributes\":{\"n\":\"3.1.\"},\"end\":8438,\"start\":8421},{\"attributes\":{\"n\":\"3.2.\"},\"end\":9095,\"start\":9071},{\"attributes\":{\"n\":\"3.2.1.\"},\"end\":9516,\"start\":9502},{\"attributes\":{\"n\":\"3.2.2.\"},\"end\":10207,\"start\":10200},{\"attributes\":{\"n\":\"3.3.\"},\"end\":11024,\"start\":11003},{\"attributes\":{\"n\":\"3.3.1.\"},\"end\":11420,\"start\":11406},{\"attributes\":{\"n\":\"3.3.2.\"},\"end\":12162,\"start\":12155},{\"attributes\":{\"n\":\"4.\"},\"end\":12825,\"start\":12815},{\"attributes\":{\"n\":\"5.\"},\"end\":13398,\"start\":13384},{\"attributes\":{\"n\":\"6.\"},\"end\":13411,\"start\":13401},{\"end\":13583,\"start\":13572},{\"end\":13658,\"start\":13647},{\"end\":13978,\"start\":13969},{\"end\":14176,\"start\":14167},{\"end\":14503,\"start\":14494}]", "table": "[{\"end\":14165,\"start\":14015},{\"end\":14492,\"start\":14228},{\"end\":14670,\"start\":14552}]", "figure_caption": "[{\"end\":13645,\"start\":13585},{\"end\":13704,\"start\":13660},{\"end\":13967,\"start\":13707},{\"end\":14015,\"start\":13980},{\"end\":14228,\"start\":14178},{\"end\":14552,\"start\":14505}]", "figure_ref": "[{\"end\":5199,\"start\":5192},{\"end\":5470,\"start\":5463},{\"end\":7390,\"start\":7383}]", "bib_author_first_name": "[{\"end\":14677,\"start\":14674},{\"end\":14703,\"start\":14687},{\"end\":14707,\"start\":14706},{\"end\":14888,\"start\":14881},{\"end\":14900,\"start\":14896},{\"end\":15261,\"start\":15257},{\"end\":15276,\"start\":15268},{\"end\":15632,\"start\":15627},{\"end\":15638,\"start\":15637},{\"end\":15993,\"start\":15987},{\"end\":16009,\"start\":16002},{\"end\":16025,\"start\":16018},{\"end\":16446,\"start\":16443},{\"end\":16453,\"start\":16452},{\"end\":16841,\"start\":16840},{\"end\":16866,\"start\":16850},{\"end\":16877,\"start\":16873},{\"end\":17246,\"start\":17245},{\"end\":17259,\"start\":17255},{\"end\":17562,\"start\":17560},{\"end\":17573,\"start\":17569},{\"end\":17580,\"start\":17574},{\"end\":17594,\"start\":17586},{\"end\":17605,\"start\":17601},{\"end\":17799,\"start\":17793},{\"end\":17817,\"start\":17811},{\"end\":18107,\"start\":18103},{\"end\":18124,\"start\":18115},{\"end\":18137,\"start\":18129},{\"end\":18152,\"start\":18144},{\"end\":18586,\"start\":18581},{\"end\":18596,\"start\":18593},{\"end\":18609,\"start\":18604},{\"end\":18618,\"start\":18616},{\"end\":18632,\"start\":18624},{\"end\":19054,\"start\":19046},{\"end\":19067,\"start\":19062},{\"end\":19079,\"start\":19074},{\"end\":19094,\"start\":19086},{\"end\":19102,\"start\":19100},{\"end\":19565,\"start\":19559},{\"end\":19580,\"start\":19579},{\"end\":19582,\"start\":19581},{\"end\":19604,\"start\":19593},{\"end\":19975,\"start\":19966},{\"end\":19988,\"start\":19985},{\"end\":20002,\"start\":19994},{\"end\":20014,\"start\":20008},{\"end\":20030,\"start\":20025},{\"end\":20045,\"start\":20037},{\"end\":20063,\"start\":20056},{\"end\":20078,\"start\":20071},{\"end\":20096,\"start\":20090},{\"end\":20556,\"start\":20553},{\"end\":20567,\"start\":20562},{\"end\":20583,\"start\":20574},{\"end\":20786,\"start\":20781},{\"end\":20801,\"start\":20793},{\"end\":20803,\"start\":20802},{\"end\":21230,\"start\":21222},{\"end\":21240,\"start\":21236},{\"end\":21256,\"start\":21252},{\"end\":21272,\"start\":21266},{\"end\":21290,\"start\":21282},{\"end\":21301,\"start\":21297},{\"end\":21318,\"start\":21312},{\"end\":21337,\"start\":21331},{\"end\":21835,\"start\":21829},{\"end\":21849,\"start\":21844},{\"end\":21863,\"start\":21859},{\"end\":21880,\"start\":21873},{\"end\":22323,\"start\":22318},{\"end\":22339,\"start\":22335},{\"end\":22353,\"start\":22348},{\"end\":22370,\"start\":22361}]", "bib_author_last_name": "[{\"end\":14685,\"start\":14678},{\"end\":14894,\"start\":14889},{\"end\":14907,\"start\":14901},{\"end\":15266,\"start\":15262},{\"end\":15281,\"start\":15277},{\"end\":15635,\"start\":15633},{\"end\":15645,\"start\":15639},{\"end\":15654,\"start\":15647},{\"end\":16000,\"start\":15994},{\"end\":16016,\"start\":16010},{\"end\":16030,\"start\":16026},{\"end\":16450,\"start\":16447},{\"end\":16459,\"start\":16454},{\"end\":16466,\"start\":16461},{\"end\":16848,\"start\":16842},{\"end\":16871,\"start\":16867},{\"end\":16883,\"start\":16878},{\"end\":16896,\"start\":16885},{\"end\":17253,\"start\":17247},{\"end\":17264,\"start\":17260},{\"end\":17277,\"start\":17266},{\"end\":17567,\"start\":17563},{\"end\":17584,\"start\":17581},{\"end\":17599,\"start\":17595},{\"end\":17611,\"start\":17606},{\"end\":17809,\"start\":17800},{\"end\":17827,\"start\":17818},{\"end\":18113,\"start\":18108},{\"end\":18127,\"start\":18125},{\"end\":18142,\"start\":18138},{\"end\":18157,\"start\":18153},{\"end\":18591,\"start\":18587},{\"end\":18602,\"start\":18597},{\"end\":18614,\"start\":18610},{\"end\":18622,\"start\":18619},{\"end\":18636,\"start\":18633},{\"end\":19060,\"start\":19055},{\"end\":19072,\"start\":19068},{\"end\":19084,\"start\":19080},{\"end\":19098,\"start\":19095},{\"end\":19105,\"start\":19103},{\"end\":19577,\"start\":19566},{\"end\":19591,\"start\":19583},{\"end\":19616,\"start\":19605},{\"end\":19622,\"start\":19618},{\"end\":19983,\"start\":19976},{\"end\":19992,\"start\":19989},{\"end\":20006,\"start\":20003},{\"end\":20023,\"start\":20015},{\"end\":20035,\"start\":20031},{\"end\":20054,\"start\":20046},{\"end\":20069,\"start\":20064},{\"end\":20088,\"start\":20079},{\"end\":20107,\"start\":20097},{\"end\":20560,\"start\":20557},{\"end\":20572,\"start\":20568},{\"end\":20587,\"start\":20584},{\"end\":20791,\"start\":20787},{\"end\":20810,\"start\":20804},{\"end\":21234,\"start\":21231},{\"end\":21250,\"start\":21241},{\"end\":21264,\"start\":21257},{\"end\":21280,\"start\":21273},{\"end\":21295,\"start\":21291},{\"end\":21310,\"start\":21302},{\"end\":21329,\"start\":21319},{\"end\":21345,\"start\":21338},{\"end\":21842,\"start\":21836},{\"end\":21857,\"start\":21850},{\"end\":21871,\"start\":21864},{\"end\":21885,\"start\":21881},{\"end\":22333,\"start\":22324},{\"end\":22346,\"start\":22340},{\"end\":22359,\"start\":22354},{\"end\":22379,\"start\":22371}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":14825,\"start\":14672},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":206590483},\"end\":15172,\"start\":14827},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":4157069},\"end\":15582,\"start\":15174},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":13497941},\"end\":15884,\"start\":15584},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":977306},\"end\":16355,\"start\":15886},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":17682066},\"end\":16750,\"start\":16357},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":9059102},\"end\":17195,\"start\":16752},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":749620},\"end\":17513,\"start\":17197},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":1910869},\"end\":17754,\"start\":17515},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":18018217},\"end\":18034,\"start\":17756},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":2131202},\"end\":18531,\"start\":18036},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":4509294},\"end\":18969,\"start\":18533},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":4545310},\"end\":19492,\"start\":18971},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":697405},\"end\":19932,\"start\":19494},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":206592484},\"end\":20531,\"start\":19934},{\"attributes\":{\"doi\":\"arXiv:1312.4400\",\"id\":\"b15\"},\"end\":20717,\"start\":20533},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":15539264},\"end\":21158,\"start\":20719},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":1799558},\"end\":21760,\"start\":21160},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":9749221},\"end\":22261,\"start\":21762},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":215754735},\"end\":22665,\"start\":22263}]", "bib_title": "[{\"end\":14879,\"start\":14827},{\"end\":15255,\"start\":15174},{\"end\":15625,\"start\":15584},{\"end\":15985,\"start\":15886},{\"end\":16441,\"start\":16357},{\"end\":16838,\"start\":16752},{\"end\":17243,\"start\":17197},{\"end\":17558,\"start\":17515},{\"end\":17791,\"start\":17756},{\"end\":18101,\"start\":18036},{\"end\":18579,\"start\":18533},{\"end\":19044,\"start\":18971},{\"end\":19557,\"start\":19494},{\"end\":19964,\"start\":19934},{\"end\":20779,\"start\":20719},{\"end\":21220,\"start\":21160},{\"end\":21827,\"start\":21762},{\"end\":22316,\"start\":22263}]", "bib_author": "[{\"end\":14687,\"start\":14674},{\"end\":14706,\"start\":14687},{\"end\":14710,\"start\":14706},{\"end\":14896,\"start\":14881},{\"end\":14909,\"start\":14896},{\"end\":15268,\"start\":15257},{\"end\":15283,\"start\":15268},{\"end\":15637,\"start\":15627},{\"end\":15647,\"start\":15637},{\"end\":15656,\"start\":15647},{\"end\":16002,\"start\":15987},{\"end\":16018,\"start\":16002},{\"end\":16032,\"start\":16018},{\"end\":16452,\"start\":16443},{\"end\":16461,\"start\":16452},{\"end\":16468,\"start\":16461},{\"end\":16850,\"start\":16840},{\"end\":16873,\"start\":16850},{\"end\":16885,\"start\":16873},{\"end\":16898,\"start\":16885},{\"end\":17255,\"start\":17245},{\"end\":17266,\"start\":17255},{\"end\":17279,\"start\":17266},{\"end\":17569,\"start\":17560},{\"end\":17586,\"start\":17569},{\"end\":17601,\"start\":17586},{\"end\":17613,\"start\":17601},{\"end\":17811,\"start\":17793},{\"end\":17829,\"start\":17811},{\"end\":18115,\"start\":18103},{\"end\":18129,\"start\":18115},{\"end\":18144,\"start\":18129},{\"end\":18159,\"start\":18144},{\"end\":18593,\"start\":18581},{\"end\":18604,\"start\":18593},{\"end\":18616,\"start\":18604},{\"end\":18624,\"start\":18616},{\"end\":18638,\"start\":18624},{\"end\":19062,\"start\":19046},{\"end\":19074,\"start\":19062},{\"end\":19086,\"start\":19074},{\"end\":19100,\"start\":19086},{\"end\":19107,\"start\":19100},{\"end\":19579,\"start\":19559},{\"end\":19593,\"start\":19579},{\"end\":19618,\"start\":19593},{\"end\":19624,\"start\":19618},{\"end\":19985,\"start\":19966},{\"end\":19994,\"start\":19985},{\"end\":20008,\"start\":19994},{\"end\":20025,\"start\":20008},{\"end\":20037,\"start\":20025},{\"end\":20056,\"start\":20037},{\"end\":20071,\"start\":20056},{\"end\":20090,\"start\":20071},{\"end\":20109,\"start\":20090},{\"end\":20562,\"start\":20553},{\"end\":20574,\"start\":20562},{\"end\":20589,\"start\":20574},{\"end\":20793,\"start\":20781},{\"end\":20812,\"start\":20793},{\"end\":21236,\"start\":21222},{\"end\":21252,\"start\":21236},{\"end\":21266,\"start\":21252},{\"end\":21282,\"start\":21266},{\"end\":21297,\"start\":21282},{\"end\":21312,\"start\":21297},{\"end\":21331,\"start\":21312},{\"end\":21347,\"start\":21331},{\"end\":21844,\"start\":21829},{\"end\":21859,\"start\":21844},{\"end\":21873,\"start\":21859},{\"end\":21887,\"start\":21873},{\"end\":22335,\"start\":22318},{\"end\":22348,\"start\":22335},{\"end\":22361,\"start\":22348},{\"end\":22381,\"start\":22361}]", "bib_venue": "[{\"end\":14728,\"start\":14710},{\"end\":14948,\"start\":14909},{\"end\":15354,\"start\":15283},{\"end\":15695,\"start\":15656},{\"end\":16094,\"start\":16032},{\"end\":16530,\"start\":16468},{\"end\":16937,\"start\":16898},{\"end\":17316,\"start\":17279},{\"end\":17617,\"start\":17613},{\"end\":17878,\"start\":17829},{\"end\":18236,\"start\":18159},{\"end\":18704,\"start\":18638},{\"end\":19184,\"start\":19107},{\"end\":19676,\"start\":19624},{\"end\":20186,\"start\":20109},{\"end\":20551,\"start\":20533},{\"end\":20890,\"start\":20812},{\"end\":21413,\"start\":21347},{\"end\":21964,\"start\":21887},{\"end\":22442,\"start\":22381},{\"end\":18300,\"start\":18238},{\"end\":18757,\"start\":18706},{\"end\":19248,\"start\":19186},{\"end\":19715,\"start\":19678},{\"end\":20250,\"start\":20188},{\"end\":20955,\"start\":20892},{\"end\":21466,\"start\":21415},{\"end\":22028,\"start\":21966}]"}}}, "year": 2023, "month": 12, "day": 17}