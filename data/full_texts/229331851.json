{"id": 229331851, "updated": "2023-10-09 12:46:25.495", "metadata": {"title": "On the eigenvector bias of Fourier feature networks: From regression to solving multi-scale PDEs with physics-informed neural networks", "authors": "[{\"first\":\"Sifan\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Hanwen\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Paris\",\"last\":\"Perdikaris\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 12, "day": 18}, "abstract": "Physics-informed neural networks (PINNs) are demonstrating remarkable promise in integrating physical models with gappy and noisy observational data, but they still struggle in cases where the target functions to be approximated exhibit high-frequency or multi-scale features. In this work we investigate this limitation through the lens of Neural Tangent Kernel (NTK) theory and elucidate how PINNs are biased towards learning functions along the dominant eigen-directions of their limiting NTK. Using this observation, we construct novel architectures that employ spatio-temporal and multi-scale random Fourier features, and justify how such coordinate embedding layers can lead to robust and accurate PINN models. Numerical examples are presented for several challenging cases where conventional PINN models fail, including wave propagation and reaction-diffusion dynamics, illustrating how the proposed methods can be used to effectively tackle both forward and inverse problems involving partial differential equations with multi-scale behavior. All code an data accompanying this manuscript will be made publicly available at \\url{https://github.com/PredictiveIntelligenceLab/MultiscalePINNs}.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "2012.10047", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2012-10047", "doi": "10.1016/j.cma.2021.113938"}}, "content": {"source": {"pdf_hash": "9168721c327d38a5b9a45ba46a978275f7b084cb", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2012.10047v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2012.10047", "status": "GREEN"}}, "grobid": {"id": "f9a73d992d24bb80e664aa1eccfe241bab0f563d", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/9168721c327d38a5b9a45ba46a978275f7b084cb.txt", "contents": "\nON THE EIGENVECTOR BIAS OF FOURIER FEATURE NETWORKS: FROM REGRESSION TO SOLVING MULTI-SCALE PDES WITH PHYSICS-INFORMED NEURAL NETWORKS\nDecember 21, 2020\n\nSifan Wang sifanw@sas.upenn.edu \nGraduate Group in Applied Mathematics and Computational Science\nGraduate Group in Applied Mathematics and Computational Science\nDepartment of Mechanichal Engineering and Applied Mechanics\nUniversity of Pennsylvania Philadelphia\nUniversity of Pennsylvania Philadelphia\nUniversity of Pennsylvania Philadelphia\n19104, 19104, 19104PA, PA, PA\n\nHanwen Wang \nGraduate Group in Applied Mathematics and Computational Science\nGraduate Group in Applied Mathematics and Computational Science\nDepartment of Mechanichal Engineering and Applied Mechanics\nUniversity of Pennsylvania Philadelphia\nUniversity of Pennsylvania Philadelphia\nUniversity of Pennsylvania Philadelphia\n19104, 19104, 19104PA, PA, PA\n\nParis Perdikaris \nGraduate Group in Applied Mathematics and Computational Science\nGraduate Group in Applied Mathematics and Computational Science\nDepartment of Mechanichal Engineering and Applied Mechanics\nUniversity of Pennsylvania Philadelphia\nUniversity of Pennsylvania Philadelphia\nUniversity of Pennsylvania Philadelphia\n19104, 19104, 19104PA, PA, PA\n\nON THE EIGENVECTOR BIAS OF FOURIER FEATURE NETWORKS: FROM REGRESSION TO SOLVING MULTI-SCALE PDES WITH PHYSICS-INFORMED NEURAL NETWORKS\nDecember 21, 2020Spectral bias \u00b7 Deep learning \u00b7 Neural Tangent Kernel \u00b7 Partial differential equations \u00b7 Scientific machine learning\nPhysics-informed neural networks (PINNs) are demonstrating remarkable promise in integrating physical models with gappy and noisy observational data, but they still struggle in cases where the target functions to be approximated exhibit high-frequency or multi-scale features. In this work we investigate this limitation through the lens of Neural Tangent Kernel (NTK) theory and elucidate how PINNs are biased towards learning functions along the dominant eigen-directions of their limiting NTK. Using this observation, we construct novel architectures that employ spatio-temporal and multi-scale random Fourier features, and justify how such coordinate embedding layers can lead to robust and accurate PINN models. Numerical examples are presented for several challenging cases where conventional PINN models fail, including wave propagation and reaction-diffusion dynamics, illustrating how the proposed methods can be used to effectively tackle both forward and inverse problems involving partial differential equations with multi-scale behavior. All code an data accompanying this manuscript will be made publicly available at https://github.com/ PredictiveIntelligenceLab/MultiscalePINNs.\n\nThe second fundamental weakness of PINNs is related to spectral bias [30,31,32]; a commonly observed pathology of deep fully-connected networks that prevents them from learning high-frequency functions. As analyzed in [6] using NTK theory [33,34,35], spectral bias indeed exists in PINN models and is the leading reason that prevents them from accurately approximating high-frequency or multi-scale functions. To this end, recent work in [36,37,38], attempts to empirically address this pathology by introducing appropriate input scaling factors to convert the problem of approximating high frequency components of the target function to one of approximating lower frequencies. In another line of work, Tancik et al. [39] introduced Fourier feature networks which use a simple Fourier feature mapping to enhance the ability of fully-connected networks to learn high-frequency functions. Although these techniques can be effective in some cases, in general, they still lack a concrete mathematical justification in relation to how they potentially address spectral bias.\n\nBuilding on the these recent findings, this work attempts to analyze and address the aforementioned shortcomings of PINNs, with a particular focus on designing effective models for multi-scale PDEs. To this end, we rigorously study fully-connected neural networks and PINNs through the lens of their limiting NTK, and produce novel insights into how these models fall short in presence of target functions with high-frequencies of multi-scale features. Using this analysis, we propose a family of novel architectures that can effectively mitigate spectral bias and enable the solution of problems for which current PINN approaches fail. Specifically, our main contributions can be summarized into the following points:\n\n\u2022 We argue that spectral bias in deep neural networks in fact corresponds to \"NTK eigenvector bias\", and show that Fourier feature mappings can modulate the frequency of the NTK eigenvectors. \u2022 By analyzing how the NTK eigenspace determines the type of functions a neural net can learn, we engineer new effective architectures for multi-scale problems. \u2022 We propose a series of benchmarks for which conventional PINN models fail, and use them to demonstrate the effectiveness of the proposed methods.\n\nThe remaining of this paper is organized as follows. In section 2, we present a brief overview of PINNs and emphasize their weakness in solving multi-scale problems. Next, we introduce the neural tangent kernel (NTK) as a theoretical tool to detect and analyze spectral bias in section 3.1. Furthermore, we study the NTK eigensystem of Fourier feature networks and propose two novel network architectures that are efficient in handling multi-scale problems, see section 3.2, 3.3. We present a detailed evaluation of our proposed neural network architectures across a range of representative benchmark examples, see section 4. Finally, in section 5, we summarize our findings and provide a discussion on lingering limitations and promising future directions.\n\n\nPhysics-informed neural networks\n\nIn this section, we present a brief overview of physics-informed neural networks (PINNs) [4]. In general, we consider partial differential equations of the following form\nN [u](x) = f (x), x \u2208 \u2126,(2.\n1) B[u](x) = g(x), x \u2208 \u2202\u2126, (2.2) where N [\u00b7] is a differential operator and B[\u00b7] corresponds to Dirichlet, Neumann, Robin, or periodic boundary conditions. In addition, u : \u2126 \u2192 R describes the unknown latent quantity of interest that is governed by the PDE system of equation 2.1. For time-dependent problems, we consider time t as a special component of x, and \u2126 then also contains the temporal domain. In that case, initial conditions can be simply treated as a special type of boundary condition on the spatio-temporal domain.\n\nFollowing the original work of Raissi et al. [4], we proceed by approximating u(x) by a deep neural network u \u03b8 (x), where \u03b8 denotes all tunable parameters of the network (e.g., weights and biases). Then, a physics-informed model can be trained by minimizing the following composite loss function\nL(\u03b8) = \u03bb r L r (\u03b8) + \u03bb b L u b (\u03b8),(2.3)\nwhere 5) and N r and N b denote the batch-sizes of training data\nL r (\u03b8) = 1 N r Nr i=1 N [u \u03b8 ](x i r ) \u2212 f (x i r ) 2 , (2.4) L b (\u03b8) = 1 N b N b i=1 B[u \u03b8 ](x i b ) \u2212 g(x i b ) 2 ,(2.{x i b , g(x i b )} N b i=1 and {x i r , f (x i r )} Nr i=1\n, respectively, which are randomly sampled in the computational domain at each iteration of a gradient descent algorithm. Notice that all required gradients with respect to input variables x or parameters \u03b8 can be efficiently computed via automatic differentiation [1]. Moreover, the parameters {\u03bb r , \u03bb b } correspond to weight coefficients in the loss function that can effectively assign a different learning rate to each individual loss term. These weights may be user-specified or tuned automatically during network training [29,6].\n\nDespite a series of early promising results [12,16,20], the original formulation of Raissi et al. [4] often struggles to handle multi-scale problems. As an example, let us consider a simple 1D Poisson's equation\n\u2206u(x) = f (x), x \u2208 (0, 1) (2.6)\nsubject to the boundary condition u(0) = u(1) = 0\n\nHere the fabricated solution we consider is\nu(x) = sin(2\u03c0x) + 0.1 sin(50\u03c0x)\nand f (x) can be derived using equation 2.6. Though this example is simple and pedagogical, it is worth noting that the solution exhibits low frequency in the macro-scale and high frequency in the micro-scale, which resembles many practical scenarios.\n\nWe represent the unknown solution u(x) by a 5-layer fully-connected neural network u \u03b8(x) with 200 units per hidden layer. The parameters of the network can be learned by minimizing the following loss function\nL(\u03b8) = L b (\u03b8) + L r (\u03b8) (2.7) = 1 N b N b i=1 u \u03b8 (x i b ) \u2212 u(x i b ) 2 + 1 N r Nr i=1 \u2206u \u03b8 (x i r ) \u2212 f (x i r ) 2 (2.8)\nwhere the batch sizes are set to N b = N r = 128 and all training points\n{x i b , u(x i b )} N b i=1 , {x i r , f (x i r )} Nr i=1\nare uniformly sampled for the boundary and residual collocation points at each iteration of gradient descent. Figure 1 summarized the results obtained by training the network for 10 7 iterations of gradient descent using the Adam optimizer [40] with default settings. We observe that the network is incapable of learning the correct solution, even after a million training iterations. In fact, it is not difficult for a conventional fully-connected neural network to approximate that function u, given sufficient data inside the computational domain. However, as shown in figure 1, solving high-frequency or multi-scale problems presents great challenges to PINNs. Although there has been recent efforts to elucidate the reasons why PINN models may fail to train [29,6], a complete understanding of how to quantify and resolve such pathologies is still lacking. In the following sections, we will obtain insights by studying Fourier feature networks through the lens of their neural tangent kernel (NTK), and present a novel methodology to tackle multi-scale problems with PINNs.  3 Methodology\n\n\nAnalyzing spectral bias through the lens of the Neural Tangent Kernel\n\nBefore presenting our proposed methods in the context of PINNs, let us first start with a much simpler setting involving regression of functions using deep neural networks. To lay the foundations for our theoretical analysis, we first review the recently developed Neural Tangent Kernel (NTK) theory of Jacot et al. [33,34,35], and its connection to investigating spectral bias [30,31,32] in the training behavior of deep fully-connected networks. Let f (x, \u03b8) be a scalar-valued fully-connected neural network (see Appendix A) with weights \u03b8 initialized by a Gaussian distribution\nN (0, 1). Given a data-set {X train , Y train }, where X train = (x i ) N i=1 are inputs and Y train = (y i ) N i=1\nare the corresponding outputs, we consider a network trained by minimizing the mean square loss L(\u03b8) = 1\nN N i=1 |f (x i , \u03b8) \u2212 y i | 2\nusing a very small learning rate \u03b7. Then, following the derivation of Jacot et al. [33,34], we can define the neural tangent kernel operator K, whose entries are given by\nK ij = K(x i , x j ) = \u2202f (x i , \u03b8)) \u2202\u03b8 , \u2202f (x j , \u03b8)) \u2202\u03b8 , (3.1)\nStrikingly, the NTK theory shows that, under gradient descent dynamics with an infinitesimally small learning rate (gradient flow), the kernel K converges to a deterministic kernel K * and does not changes during training as the width of the network grows to infinity.\n\nFurthermore, under the asymptotic conditions stated in Lee et al. [35], we can derive that\ndf (X train , \u03b8(t)) dt \u2248 \u2212K \u00b7 (f (X train , \u03b8(t)) \u2212 Y train ),(3.2)\nwhere \u03b8(t) denotes the parameters of the network at iteration t and f (X train ,\n\u03b8(t)) = (f (x i , \u03b8(t)) N i=1 . Then, it directly follows that f (X train , \u03b8(t)) \u2248 (I \u2212 e \u2212Kt ) \u00b7 Y train . (3.3)\nSince the kernel K is positive semi-definite, we can take its spectral decomposition K = Q T \u039bQ, where Q is an orthogonal matrix whose i-th column is the eigenvector q i of K and \u039b is a diagonal matrix whose diagonal entries \u03bb i are the corresponding eigenvalues. Since e \u2212Kt = Q T e \u2212\u039bt Q, we have\nQ T (f (X train , \u03b8(t)) \u2212 Y train ) = \u2212e \u039bt Q T Y train , (3.4) which implies \uf8ee \uf8ef \uf8ef \uf8ef \uf8f0 q T 1 q T 2 . . . q T N \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fb (f (X train , \u03b8(t)) \u2212 Y train ) = \uf8ee \uf8ef \uf8ef \uf8ef \uf8f0 e \u2212\u03bb1t e \u2212\u03bb2t . . . e \u2212\u03bb N t \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fb \uf8ee \uf8ef \uf8ef \uf8ef \uf8f0 q T 1 q T 2 . . . q T N \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fb Y train . (3.5)\nThe above equation shows that the convergence rate of q T i (f (X train , \u03b8(t)) \u2212 Y train ) is determined by the i-th eigenvalue \u03bb i . Moreover, we can decompose the training error into the eigenspace of the NTK as\nf (X train , \u03b8(t)) \u2212 Y train = N i=1 (f (X train , \u03b8(t)) \u2212 Y train , q i )q i (3.6) = N i=1 q T i (f (X train , \u03b8(t)) \u2212 Y train ) q i (3.7) = N i=1 e \u2212\u03bbit q T i Y train q i . (3.8)\nClearly, the network is biased to first learn the target function along the eigendirections of neural tangent kernel with larger eigenvalues, and then the rest components corresponding to smaller eigenvalues. A more detailed analysis on the convergence rate of different components is illustrated by Cao et al. [31]. For conventional fully-connected neural networks, the eigenvalues of the NTK shrink monotonically as the frequency of the corresponding eigenfunctions increases, yielding a significantly lower convergence rate for high frequency components of the target function [30,32]. This indeed reveals the so-called \"spectral bias\" [30] pathology of deep neural networks.\n\nSince the learnability of a target function by a neural network can be characterized by the eigenspace of its neural tangent kernel, it is very natural to ask: can we engineer the eigenspace of the NTK to accelerate convergence? If this is possible, can we leverage it to help the network effectively learn different frequencies in the target function? In the next section, we will answer these questions by re-visitng the recently proposed random Fourier features embedding proposed by Tancik et al. [39].\n\n\nFourier feature embeddings\n\nFollowing the original formulation of Tancik et al. [39], a random Fourier mapping \u03b3 is defined as\n\u03b3(v) = cos(Bv) sin(Bv) ,(3.9)\nwhere each entry in B \u2208 R m\u00d7d is sampled from a Gaussian distribution N (0, \u03c3 2 ) and \u03c3 > 0 is a user-specified hyper-parameter. Then, a Fourier features network [39] can be simply constructed using a random Fourier features mapping \u03b3 as a coordinate embedding of the inputs, followed by a conventional fully-connected neural network [39].\n\nAs shown in [39], such a simple method can mitigate the pathology of spectral bias and enable networks to learn high frequencies more effectively, which can significantly improve the effectiveness of neural networks across many tasks including image regression, computed tomography, magnetic resonance imaging (MRI), etc.\n\nIn order to explore the deeper reasoning and understand the inner mechanisms behind this simple technique, we consider a two-layer bias-free neural network with Fourier features, i.e.\nf (x) = 1 \u221a m W \u00b7 cos(Bx) sin(Bx) , (3.10) where x \u2208 R d is the input, W \u2208 R 1\u00d72m is the weight matrix and B = [b 1 , b 2 , . . . , b m ] T \u2208 R m\u00d7d are sampled from Gaussian N (0, \u03c3 2 ). Let {x i } N i=1\nbe input points in a compact domain C . Then, according to equation 3.1, the neural tangent kernel K is given as\nK ij = K(x i , x j ) = 1 m cos(Bx i ) sin(Bx j ) T \u00b7 cos(Bx i ) sin(Bx j ) = 1 m m k=1 cos(b T k x i ) cos(b T k x j ) + sin(b T k x i ) sin(b T k x j ) = 1 m m k=1 cos(b T k (x i \u2212 x j )).\nTo study the eigen-system of the kernel K, we consider the limit of K as the number of points goes to infinity. In this limit the eigensystem of K approaches the eigen-system of the kernel function K(x, x ) which satisfies the following\nequation [41] C K (x, x ) g (x ) dx = \u03bbg (x) , (3.11) where K(x, x ) = 1 m m k=1 cos(b T k (x \u2212 x )\n). Note that the kernel K induces an Hilbert-Schdmit integral operator\nT K : L 2 (C) \u2192 L 2 (C) T K (g)(x) = C K (x, x ) g (x ) dx .\nAlso note that T K is a compact and self-adjoint operator, which implies that the eigenfunctions exist and all eigenvalues are real. The following lemma reveals that eigenfunctions are indeed solutions to a eigenvalue problem.\nLemma 3.1. For the kernel K(x, x ) = 1 m m k=1 cos(b T k (x \u2212 x ))\n, the eigenfunction g(x) corresponding to non-zero eigenvalues satisfying the the following equation\n\u2206g(x) = \u2212 1 m B 2 F g(x) (3.12)\nProof. The proof can be found in Appendix B.\n\nIf we consider the Laplacian on the sphere S d\u22121 and assume that 1 m B 2 2 = l(l + d \u2212 2) for some positive integer l, then g(x) are corresponding homogeneous harmonic polynomials of degree l [42]. However, in general, directly solving this eigenvalue problem on a complex domain is intractable.\n\nTo obtain a better understanding of the behavior of the eigenfunctions and the corresponding eigenvalues, let us consider a much simper case by setting d = 1 and m = 1. Specifically, we take the input x \u2208 R, the compact domain C = [0, 1] and the Fourier features B = b \u2208 R are sampled from a Gaussian distribution N(0, \u03c3 2 ). Then the kernel function is given by\nK(x, x ) = cos(b(x \u2212 x )).\nIn this case, we can compute the exact expression of the eigenfunctions and their eigenvalues, as summarized in the Proposition 3.2 below. Proposition 3.2. For the kernel function K(x, x ) = cos(b(x \u2212 x )), the non-zero eigenvalues are given by\n\u03bb = 1 \u00b1 sin b b 2 .\n(3.13)\n\nThe corresponding eigenfunctions g(x) must have the form of g(x) = C 1 cos(bx) + C 2 sin(bx), (3.14)\n\nwhere C 1 and C 2 are some constants.\n\nProof. The proof can be found in Appendix C.\n\nFrom this Proposition, we immediately observe that the frequency of the eigenfunctions is determined by b and the gap between the eigenvalues is sin b b . Besides, recall that b is sampled from a Gaussian distribution N (0, \u03c3 2 ), which implies that the larger the \u03c3 we choose, the higher the probability that b takes greater magnitude. Therefore, we may conclude that, for this toy model, large \u03c3 would lead to high frequency eigenfunctions, as well as narrow eigenvalues gaps. As a result, Fourier features may resolve the issue of spectral bias and enable faster convergence to high-frequency components of a target function.\n\nIntuitively, we would expect that general fully-connected neural networks with Fourier features exhibit similar behaviors as our toy example. However, it is extremely difficult to calculate the the eigenvalues and eigenfunctions for generic cases. Therefore, here we attempt to empirically verify our analysis by numerical experiments.\n\nTo this end, we first initialize two Fourier feature embeddings with \u03c3 = 1, 10, respectively, and apply them to a one-dimensional input coordinate before passing them through a 4-layer fully-connected neural network with 100 units per hidden layer. Then, we study the NTK eigendecomposition of these two networks at initialization.    with \u03c3 = 10 shrink much slower than the ones corresponding to \u03c3 = 1. Moreover, comparing the eigenvectors for different \u03c3, it is easy to see that \u03c3 = 10 results in higher frequency eigenvectors than \u03c3 = 1. This conclusion is further clarified by figure 4, which depicts the frequency content of the eigenvector corresponding to the largest eigenvalue, for different \u03c3 \u2208 [1,50]. All these observations are consistent with Proposition 3.2 and the analysis presented for the toy network with Fourier features.\n\nNext, let us consider a simple one-dimensional target function of the form\nf (x) = sin(20\u03c0x) + sin(2\u03c0x), x \u2208 [0, 1],(3.15)\nand generate the training data\n{x i , f (x i )} 100 i=1\nwhere x i are evenly spaced in the unit interval. We proceed by training these two networks to fit the target function using the Adam optimizer [40] with default settings for 1, 000 and 10, 000 epochs respectively. The results for \u03c3 = 1, 10 are summarized in figure 5 and figure 6, respectively. It can be observed that low frequencies are learned first for \u03c3 = 1, which is pretty similar to the behavior of conventional fully-connected neural networks, commonly referred to as \"spectral bias\" [30]. Notice, however, that it is high frequencies that are learned first for \u03c3 = 10. As shown in figure 2 and figure 3, we already know that the value of \u03c3 determines the frequency of eigenvectors of the NTK. Therefore, this observation highly suggests that \"spectral bias\" actually corresponds to \"eigenvector bias\", in the sense that the leading eigenvectors corresponding to large eigenvalues of the NTK determine the frequencies which the network is biased to learn first.\n\nFurthermore, one may note that the distribution of the eigenvalues corresponding to \u03c3 = 1 moves \"outward\" during training. From our experience, the movement of the eigenvalue distribution results in the movement of its NTK, as well as the parameters of the network during training. As shown in figure 5b), the parameters of the network barely move after a rapid change in the first few hundred epochs. This indicates that the network initialization is not suitable to fit the given target function because the parameters of the network have to move very far from initialization in order to reach a reasonable local minimum. In contrast, the distribution of the the eigenvalues corresponding to \u03c3 = 10 almost keeps the NTK spectrum fixed during training. Accordingly, in the middle panel of figure 6b, similar behavior can be observed, but the relative change of the parameters for case of \u03c3 = 10 is much less than the case of \u03c3 = 1. This suggests that the initialization of the network is \"good\" and desirable local minima exist in the vicinity of the parameter initialization in the corresponding loss landscape. As shown in the top right panels of figure 5 and figure 6, the relative L 2 prediction error corresponding to \u03c3 = 10 decreases much faster than the relative L 2 error corresponding to \u03c3 = 1.\n\nFinally, it is worth emphasizing that Fourier feature mappings initialized by large \u03c3 do not always benefit the network, as too large value of \u03c3 may cause over-fitting. To demonstrate this point, we initialize a Fourier feature mapping with \u03c3 = 10 and pass it through the same fully-connected network. We now consider f (x) = sin \u03c0x + sin(2\u03c0x) as the ground truth target function, from which 20 points are uniformly sampled as training data. Then we train the network to fit the target function using the Adam optimizer with default settings for 1, 000 epochs. As shown in figure 7, although all training data pairs are perfectly approximated and the training error is very small, the network interpolates the training data with high frequency oscillations, and thus fails to correctly recover the target function. One possible explanation is that the neural network approximation tends to exhibit similar frequencies as the leading eigenvectors of its NTK. Therefore, choosing an appropriate \u03c3 such that the frequency of the leading NTK eigenvectors agrees with the frequency of the target function plays an important role in the network performance, which not only accelerates the convergence speed, but also increases the prediction accuracy.\n\n\nMulti-scale Fourier feature embeddings for physics-informed neural networks\n\nIn the previous section, we presented a detailed theoretical and numerical analysis of the NTK eigen-system of a neural network for classical 2 regression problems. Building on this insight, we now draw our attention back to physics-informed neural networks for solving forward and inverse problems involving partial differential equations, whose solutions may exhibit multi-scale behavior.\n\nTo begin, we first note that the NTK of PINNs [6] is slightly more complicated than the NTK of networks used for conventional regressions. To this end, we follow the setup presented in Wang et al. [6], and consider general PDEs with appropriate boundary conditions (see equations 2.1 and 2.2) and corresponding \"training data\"\n{x i b , g(x i b )} N b i=1 , {x i r , f (x i r )} Nr i=1\n, and define the NTK of PINNs as where K ru (t) = K T ur (t) and K uu (t) \u2208 R N b \u00d7N b , K ur (t) \u2208 R N b \u00d7Nr , and K rr (t) \u2208 R Nr\u00d7Nr , whose (i, j)-th entry is given by\nK(t) = K uu (t) K ur (t) K ru (t) K rr (t) ,(K uu ) ij (t) = dB[u](x i b , \u03b8(t)) d\u03b8 , dB[u](x j b , \u03b8(t)) d\u03b8 (3.16) (K ur ) ij (t) = dB[u](x i b , \u03b8(t)) d\u03b8 , dN [u](x j r , \u03b8(t)) d\u03b8 (3.17) (K rr ) ij (t) = dN [u](x i r , \u03b8(t)) d\u03b8 , dN [u](x j r , \u03b8(t)) d\u03b8 . (3.18)\nThen, the training dynamics of PINNs under gradient descent with an infinitesimally small learning rate can be characterized by the following ODE system\ndB[u](x b ,\u03b8(t)) dt dN [u](xr,\u03b8(t)) dt = \u2212 K uu (t) K ur (t) K ru (t) K rr (t) \u00b7 B[u](x b , \u03b8(t)) \u2212 g(x b ) N [u](x r , \u03b8(t)) \u2212 f (x r ) . (3.19)\nThen, the NTK framework allows us to show the following Proposition. Proposition 3.3. Suppose that the training dynamics of PINNs satisfies equation 3.19 and the spectral decompositions of K uu (0) and K rr (0) are\nK uu (0) = Q T u \u039b u Q T u (3.20) K rr (0) = Q T r \u039b r Q T r ,(3.\n\n21)\n\nwhere Q u and Q r are orthogonal matrices consisting of eigenvectors of K uu (0) and K rr (0), respectively, and \u039b u and \u039b r are diagonal matrices whose entries are the eigenvalues of K uu (0) and K rr (0), respectively. Given the assumptions (i) K(t) \u2248 K(0) for all t \u2265 0.\n\n(ii) K uu (0) and K rr (0) are positive definite, we can write B = Q T r , K ru (0)Q u , and obtain\nQ T B[u](x b , \u03b8(t)) N [u](x r , \u03b8(t)) \u2212 g(x b ) f (x r ) \u2248 e \u2212P T\u039b P t Q T g(x b ) f (x r ) , (3.22) where Q = Q u 0 0 Q r , P = I 0 \u2212B\u039b \u22121 u I , \u039b = \u039b u 0 0 \u039b r \u2212 B T \u039b \u22121 u B .\n(3.23)\n\nThe above proposition reveals that, under some assumptions, the resulting NTK eigen-system of PINNs is determined by the eigenvectors of K uu and K rr . Understanding the behavior of this eigen-system derived from the NTK of PINNs should be at the core of future extensions of this line of research.\n\nAs mentioned in section 2, PINNs often struggle in solving multi-scale problems. Unlike conventional regression tasks, there is generally no or just a handful data points provided for PINNs inside the computational domain. This is similar to the case illustrated in figure 7 where the network would fit the target function biases towards its preferred frequencies, which are determined by the eigenvectors of its NTK. Consequently, PINNs using fully-connected networks would learn the solutions and their PDE residuals with the lowest frequency first, due to \"spectral bias\". We believe that this may be one of the fundamental reasons that cause failure of PINNs in learning high-frequency or multi-scale solutions of PDEs.\n\nInspired by our analysis and observations of Fourier features in section 3.2, we present a novel network architecture to handle multi-scale problems. As illustrated in figure 8a, we apply multiple Fourier feature embeddings initialized with different \u03c3 to input coordinates before passing these embedded inputs through the same fully-connected neural network and finally concatenate the outputs with a linear layer. The detailed forward pass is defined as follows: \n\u03b3 (i) (x) = cos(2\u03c0B (i) x) sin(2\u03c0B (i) x) , for i = 1, 2, . . . , M (3.24) H (i) 1 = \u03c6(W 1 \u00b7 \u03b3 (i) (x) + b 1 ), for i = 1, 2, . . . , M(3.+ b L+1 ,(3.27)\nwhere \u03b3 (i) and \u03c6 denote Fourier feature mappings and activation functions, respectively, and each entry in B (i) \u2208 R m\u00d7d is sampled from N (0, \u03c3 i ), and is held fixed during model training (i.e. B (i) are not trainable parameters, as in [39]). Notice that the weights and the biases of this architecture are essentially the same as in a standard fully-connected neural network. Here, we underline that the choice of \u03c3 i is problem-dependent and typical values can be 1, 20, 50, 100, etc.\n\nTo better understand the motivation behind this architecture, suppose that f \u03b8 is a approximation of a given target function f whose Fourier decomposition is\nf (x) = \u221e k=\u2212\u221ef k e ikx ,(3.28)\nwheref k is the Fourier coefficient corresponding to the wave-number k [43]. Note that f \u03b8 is simply a linear combination of {H\n(i) L } M i=1\n, which has some degree of consistency with equation 3.28. Moreover, we emphasize again that, for networks with Fourier features, \u03c3 determines the frequency that the networks prefer to learn. As a result, if we just employ networks with one Fourier feature embedding, then whatever the value of \u03c3 is used to initialize Fourier feature mappings, will yield slower convergence to the rest frequency components, except for the preferable frequencies determined by the choice of \u03c3. Therefore, it is reasonable to embed inputs to several Fourier feature mappings with different \u03c3 and concatenate them through a linear layer after the forward propagation such that all frequency components can be learned with the same convergence rate.\n\nFor time-dependent problems, multi-scale behavior may exist not only across spatial directions but also across time. Thus, we present another novel multi-scale Fourier feature architecture to tackle multi-scale problems in spatio-temporal domains. Specifically, the feed-forward pass of the network is now defined as  \n\u03b3 (i) x (x) = cos(2\u03c0B (i) x x) sin(2\u03c0B (i) x x) , H (i) x,1 = \u03c6(W 1 \u00b7 \u03b3 (i) x (x) + b 1 ), for i = 1, 2, . . . , M x , (3.29) \u03b3 (j) t (t) = cos(2\u03c0B (j) t x) sin(2\u03c0B (j) t t) , H (j) t,1 = \u03c6(W 1 \u00b7 \u03b3 (j) t (t) + b 1 ), for j = 1, 2, . . . , M t , (3.30) H (i) x, = \u03c6(W \u00b7 H (i) x, \u22121 + b ),+ b L+1 ,(3.34)\nwhere \u03b3 (i)\n\nx and \u03b3 (j) t denote spatial and temporal Fourier feature mappings, respectively, and represents the point-wise multiplication. Here each entry of B (i)\n\nx and B (j) t are sampled from N (0, \u03c3 x i ) and N (0, \u03c3 t j ), respectively, and are held fixed during model training. A visualization of this architecture is presented in figure 8b. One key difference from figure 8a is that we apply separate Fourier feature embeddings to spatial and temporal input coordinates before passing the embedded inputs through the same fully-connected network. Another key difference is that we merge spatial outputs H (i)\n\nx,L and temporal outputs H (j) t,L using point-wise multiplication and passing them through a linear layer. Heuristically, this architecture is consistent with the Fourier spectral method [43], i.e, given a function f (x, t), using Fourier series we may rewrite it as\nf (x, t) = \u221e k=\u2212\u221ef k (t)e ikx (3.35)\nwheref k (t) is the Fourier coefficient corresponding to the wavenumber k at time t.\n\nIt is also worth noting that both proposed architectures do not introduce any additional trainable parameters compared to conventional PINN models, nor they require significantly more floating point operations to evaluate their forward or backward pass. Therefore they can be used as drop-in replacements to conventional fully-connected architectures with no sacrifices to computational efficiency. In section 4, we will validate the effectiveness of the proposed architectures through a series of systematic numerical experiments.\n\n\nResults\n\nIn this section we demonstrate the performance of the proposed architectures in solving forward and inverse multi-scale problems. Throughout all benchmarks, we employ hyperbolic tangent activation functions and initialize the network using the Glorot normal scheme [44]. All networks are trained via stochastic gradient descent using the Adam optimizer [40] with defaulting settings. Particularly, we employ exponential learning rate decay with a decay-rate of 0.9 every 1, 000 training iterations. All results presented in this section can be reproduced using our publicly available codes at https://github.com/PredictiveIntelligenceLab/MultiscalePINNs.\n\n\n1D Poisson equation\n\nWe begin with a pedagogical example involving the one-dimensional (1D) Poisson equation benchmark described in section 2 and examine the performance of the proposed architectures.\n\nWe begin by approximating the latent solution u(x) with the proposed multi-scale Fourier feature architecture (figure 8a). Specifically, we apply two Fourier feature mappings to 1D input coordinates, respectively, before passing them through a 2-layer fully-connected neural network with 100 units per hidden layer, and then concatenating them through a linear layer. Particularly, these Fourier feature mappings are initialized with \u03c3 1 = 1 and \u03c3 2 = 10, respectively. We train the network by minimizing the loss function 2.7 in section 2 under the exactly same hyper-parameter settings. One can see that the predicted solution obtained using the proposed architecture achieves excellent agreement with the exact solution, yielding a 1.36e \u2212 03 prediction error measured in the relative L 2 -norm.\n\nFurthermore, we aim to demonstrate that the proposed architecture outperforms the conventional PINNs, as well as the PINNs with a single Fourier feature mapping. To this end, we first train a plain a conventional physics-informed network using exactly the same hyper-parameters and take the resulting relative L 2 error as our baseline. Next, we train the same network with a conventional Fourier feature mapping, considering different initializations for \u03c3 in the range [1,50], and report the resulting relative L 2 errors over 10 independent trials in figure 10. It can be observed that either plain PINNs or PINNs with vanilla Fourier features fail to attain good prediction accuracy, yielding errors ranging from 10% to above 100% in the relative L 2 -norm. In particular, we visualize the results obtained using the same network with conventional Fourier features [39] initialized by \u03c3 = 1 and \u03c3 = 50, respectively. As shown in figure 11a and  figure 11b, the network with conventional Fourier features initialized by \u03c3 = 1 tends to capture the low frequency components of the solution while the one initialized by \u03c3 = 50 successfully captures the high frequency oscillations but ignores the low frequency components.\n\n\nHigh frequencies in a heat equation\n\nTo demonstrate the necessity and effectiveness of the proposed spatio-temporal multi-scale Fourier feature architecture (ST-mFF), let us consider the one-dimensional heat equation taking the form u t = 1 (500\u03c0) 2 u xx , (x, t) \u2208 (0, 1) \u00d7 (0, 1) The exact solution u(x, t) for this benchmark is given by u(x, t) = e \u2212t sin(500\u03c0x).\n\n\n(4.4)\n\nAs the solution is mainly dominated by a single high frequency in the spatial domain, it suffices to employ just single Fourier feature mapping in the network architecture. We proceed by approximating the latent variable u(x, t) with the network u \u03b8 (x) using the proposed spatio-temporal architecture ( figure 8b). Specifically, we embed the spatial and temporal input coordinates through two separate Fourier features initialized with \u03c3 = 200 and \u03c3 = 1, respectively, and pass the embedded inputs though a 3-layer fully-connected neural network with 100 neurons per hidden layer and finally merge them according to equation 3.31 -3.34. The network is trained by minimizing the following loss\nL(\u03b8) = L bc (\u03b8) + L ic (\u03b8) + L r (\u03b8) (4.5) = 1 N bc N bc i=1 u \u03b8 (x i bc , t i bc ) \u2212 u(x i bc , t i bc ) 2 + 1 N ic Nic i=1 u \u03b8 (x i ic , t i ic ) \u2212 u(x i ic , t i ic ) 2 (4.6) + 1 N r Nr i=1 \u2202u \u03b8 \u2202t (x i r , t i r ) \u2212 \u2202 2 u \u03b8 \u2202x 2 (x i r , t i r ) 2 , (4.7)\nwhere we set the batch sizes to N bc = N ic = N r = 128. The predictions of the trained model against the exact solution along with the point-wise absolute error between them are presented in figure 12a. This figure indicates that our spatio-temporal multi-scale Fourier feature architecture is able to accurately capture the high frequency oscillations, leading to a 1.78e \u2212 03 prediction error measured in the relative L 2 -norm. To the best of author's knowledge, this is the first time that PINNs can be effective in solving a time-dependent problem exhibiting such extremely high frequencies.\n\nNext, we test the performance of the multi-scale Fourier feature architecture. To this end, we represent the unknown solution u(x, t) by the same network (3-layer, 100 hidden units) with single Fourier feature mapping initialized using different \u03c3 \u2208 [1, 1000]. A visual assessment of the resulting relative L 2 error, as well as the baseline obtained by the conventional PINNs are shown in figure 13. As suggested in this figure, all reported relative L 2 errors are around 100%, which implies that both conventional PINNs and the multi-scale Fourier feature architecture are incapable of learning high frequency components of solutions in the spatio-temporal domain.\n\n\nWave propagation\n\nIn this example, we aim to demonstrate that employing appropriate architectures solely cannot guarantee accurate predictions. To this end, we consider one-dimensional wave equation taking the form u tt (x, t) \u2212 100u xx (x, t) = 0, (x, t) \u2208 (0, 1) \u00d7 (0, 1) (4.8) u(0, t) = u(1, t) = 0, t \u2208 [0, 1] (4.9) u(x, 0) = sin(\u03c0x) + sin(2\u03c0x), x \u2208 [0, 1] (4.10) u t (x, 0) = 0, x \u2208 [0, 1].\n\n(4.11)\n\nBy d'Alembert's formula [42], the solution u(x, t) is given by u(x, t) = sin(\u03c0x) cos(10\u03c0t) + sin(2\u03c0x) cos(20\u03c0t). To handle the multi-scale behavior in both spatial and temporal directions, we employ the spatio-temporal architecture (figure 8b) to approximate the latent solution u(x, t). Specifically, we apply two separate Fourier feature mappings initialized by \u03c3 = 1, 10 respectively to temporal coordinates t and apply one Fourier feature mapping initialized by \u03c3 = 1 to spatial coordinates x. Then we pass all featurized spatial and temporal inputs coordinated through a 3-layer fully-connected neural network with 200 units per hidden layer and concatenate network outputs using equations 3.33 -3.34. In particular, we treat the initial condition 4.10 as a special boundary condition on the spatio-temporal domain \u2126. Then equation 4.9 and equation 4.10 can be summarized as\nu(x) = g(x), x \u2208 \u2202\u2126\nThen, the network can be trained by minimizing the following loss function L(\u03b8) = L u (\u03b8) + L ut (\u03b8) + L r (\u03b8) (4.13)   where the batch sizes are set to N u = N ut = N r = 360 and all data points\n= 1 N u Nu i=1 |u(x i u , \u03b8) \u2212 g(x i u )| 2 + 1 N ut Nu t i=1 |u t (x i ut , \u03b8)| 2 + 1 N r Nr i=1 | \u2202 2 u \u03b8 \u2202t 2 (x i r ) \u2212 100 \u2202 2 u \u03b8 \u2202x 2 (x i r )| 2 ,{x i u , g(x i u )} Nu i=1 , {x i ut } Nu t i=1 and {x i r } Nr i=1\nare uniformly sampled from the appropriate regions in the computational domain at each iteration of gradient descent. Figure 14 presents a comparison of the exact and predicted solution obtained after 40,000 iterations of gradient descent. It is evident that the PINN model completely fails to learn the correct solution. This illustrated the fact that even if we choose an appropriate network architecture to approximate the latent PDE solution, there might be some other issues that lead PINNs to fail. Wang et al. [6] found that multi-scale problems are more likely to cause a large discrepancy in the convergence rate of different terms contributing to the total training error, which may lead to severe training difficulties of PINNs in practice. Such a discrepancy can be further justified in figure 14b, from which one can see that the loss L r and loss L ut decrease much faster than the loss L u during training. We believe that this may be a fundamental reason behind the collapse of physics-informed neural networks, and their inability to yield accurate predictions for this specific example.\n\nTo address this a training pathology, we employ the adaptive weights algorithm proposed by Wang et al. [6] to train the same network with the same spatio-temporal Fourier feature mappings under the same exactly hyper-parameter settings. As shown in figure 15, the results demonstrate excellent agreement between the predicted and the exact solution with relative L 2 error within 0.1%. Furthermore, we test the performance of standard fully-connected network and the proposed multi-scale Fourier feature architecture, with or without the adaptive weights algorithm and the resulting relative L 2 errors are summarized in table 1. For all results shown in the table, we employ a 3-layer fully-connected network with 200 units per hidden layer as a backbone. To obtain the result of the multi-scale Fourier features architecture, we jointly embed the spatio-temporal coordinates (x, t) using two separate Fourier feature mappings initialized with \u03c3 = 1, 10, respectively. We observe that only the proposed spatio-temporal Fourier feature architecture trained with the adaptive weights algorithm of Wang et al. [6] is capable of achieving a accurate approximation of the true solution, which highly suggests the necessity of combining the proposed architecture with an appropriate optimization scheme for training. No adaptive weights 1.01e+00 1.03e+00 1.02e+00\n\nWith adaptive weights 8.77e-01 1.00e+00 9.83e-04 Table 1: 1D wave equation: Relative L 2 errors of the predicted solutions obtained using conventional fully-connected networks (plain), multi-scale Fourier features architecture (MFF) and spatio-temporal multi-scale Fourier features architecture (ST-MFF).\n\n\nReaction-diffusion dynamics in a two-dimensional Gray-Scott model\n\nOur final example aims to highlight the ability of the proposed methods to handle inverse problems. Let us consider a two-dimensional Gray-Scott model [45] that describes two non-real chemical species U, V reacting and transforming to each other. This model is governed by a coupled system of reaction-diffusion equations taking the form 1e \u2212 05 9.70e \u2212 06 8.06% Table 2: 2D Gray-Scott equation: Exact diffusion rates versus the inferred diffusion rates after training. and integrate the equations up to the final time t = 4000. Synthetic training data for this example are generated using the Chebfun package [46] with a spectral Fourier discretization and a fourth-order stiff time-stepping scheme [47] with time-step size of 0.5. Temporal snapshots of the solution are are saved every \u2206t = 10. From this data-set, we create a smaller training subset by collecting the data points from time t = 3500 to t = 4000 (50 snapshots in total) as our training data. A representative snapshot of the numerical solution is presented in figure 16. As illustrated in this figure, the solution exhibits complex spatial patterns with a non-trivial frequency content.\nu t = \u03b5 1 \u2206u + b(1 \u2212 u) \u2212 uv 2 (4.15) v t = \u03b5 2 \u2206v \u2212 dv + uv 2Given training data {(x i , y i , t i ), (u i , v i )} N i=1\nand assuming that b, d are known, we are interested in predicting the latent concentration fields u, v, as well as inferring the unknown diffusion rates 1 , 2 . To this end, we represent the latent variables u, v by a deep network employing the proposed spatio-temporal Fourier feature architecture\n(x, y, t) f \u03b8 \u2212\u2192 (u \u03b8 , v \u03b8 ) (4.17)\nTo be precise, we map temporal coordinates t by a Fourier feature embedding with \u03c3 = 1 and map spatial coordinates (x, y) by another Fourier feature embedding with \u03c3 = 30. Then we pass the embedded inputs through a 9-layer fully-connected neural network with 100 neurons per hidden layer. The corresponding loss function is given by\nL(\u03b8) = L u (\u03b8) + L v (\u03b8) + L r u (\u03b8) + L r v (\u03b8) (4.18) = 1 N u Nu i=1 u \u03b8 (x i , y i , y i ) \u2212 u i 2 + 1 N v Nv i=1 v \u03b8 (x i , y i , y i ) \u2212 v i 2 (4.19) + 1 N r u N r u i=1 r u \u03b8 (x i r , y i r , t i r ) 2 + 1 N r v N r v i=1 r v \u03b8 (x i r , y i r , t i r ) 2 ,(4.20)\nwhere the PDE residuals are defined as\nr u \u03b8 = \u2202u \u03b8 \u2202t \u2212 1 \u2206u \u03b8 \u2212 b(1 \u2212 u \u03b8 ) + u \u03b8 v 2 \u03b8 (4.21) r v \u03b8 = \u2202v \u03b8 \u2202t \u2212 2 \u2206v \u03b8 + dv \u03b8 \u2212 u \u03b8 v 2 \u03b8 . (4.22)\nHere we choose batch sizes N u = N v = N r u = N r v = 1000 where all data points along with collocation points are randomly sampled at each iteration of gradient descent. Particularly, since the diffusion rates are strictly positive and generally very small, we parameterize 1 , 2 by exponential functions, i.e i = e \u03b1i for i = 1, 2 where \u03b1 i 's are trainable parameters initialized by \u221210.\n\nWe train the network by minimizing the above loss function via via 120,000 iterations of gradient descent. To compare these results against the performance of a conventional PINNs model [4], we also train the same fullyconnected neural network (9-layer, width 100) under the same hyper-parameter settings. The results of this experiment are summarized in figure 18. Evidently, conventional PINNs are incapable of accurately learning the concentrations u, v, as well as inferring the unknown diffusion rates under the current setting.\n\n\nDiscussion\n\nIn this work, we study Fourier feature networks through the lens of their limiting neural tangent kernel, and show that Fourier feature mappings determine the frequency of the eigenvectors of the resulting NTK. This analysis sheds light into mechanisms that introduce spectral bias in the training of deep neural networks, and suggests possible  avenues for overcoming this fundamental limitation. Specific to the context of physics-informed neural networks, our analysis motivates the design of two novel network architectures to tackle forward and inverse problems involving time-dependent PDEs with solutions that exhibit complex multi-scale spatio-temporal features. To gain further insight, we propose a series of benchmarks for which conventional PINN approaches fail, and demonstrate the effectiveness of the proposed methods under these challenging settings. Taken together, the developments presented in this work provide a principled way of analyzing the performance of PINN models, and enable the design of a new generation of architectures and training algorithms that introduce significant improvements both in terms of training speed and generalization accuracy, especially for multi-scale PDEs for which current PINN models struggle.\n\nDespite this progress, we must admit that we are still at the very early stages of tackling realistic multi-scale and multi-physics problems with PINNs. One main limitation of the the proposed architectures is that we have to carefully choose the appropriate number of Fourier feature mappings and their scale, such that the frequency of the NTK eigenvectors and the target function are roughly matched to each other. In other words, the proposed architectures require some prior knowledge regarding the frequency distribution of the target PDE solution. However, this kind of information may not be accessible for some forward problems, especially for more complex dynamical systems involving the fast transitions of frequencies such the Kuramoto-Sivashinsky equation [48,49], or the Navier-Stokes equations in the turbulent regime. Fortunately, this issue could be mitigated for some inverse problems where we may perform some spectral analysis on the training data to determine the appropriate number and scale of the Fourier feature mappings.\n\nThere are also many open questions worth considering as future research directions. From a theoretical standpoint, we numerically verify that the frequency of Fourier feature mappings determines the frequency of the NTK eigenvectors. Can we rigorously establish a theory for general networks? Besides, what is the behavior of the NTK eigensystem of PINNs? What is the difference between the resulting eigensystem of PINNs and conventional neural networks? From a practical standpoint, we observe that the eigenvalue distribution moves outward (see figure 5b) when choosing an inappropriate scale of Fourier feature mappings, which implies that the parameters of the network have to move far away from their initialization to find a good local minimum. Thus, it is natural to ask how to initialize PINNs such that the desirable local minima exist in the vicinity of the parameter initialization in the corresponding loss landscape? Moreover, can we design other useful feature embeddings that aim to handle different scenarios (e.g., shocks, boundary layers, etc.)? We believe that answering these questions not only paves a new way to better understand PINNs and their training dynamics, but also opens a new door for developing scientific machine learning algorithms with provable convergence guarantees, as needed for many critical applications in computational science and engineering. \ncos(b T k (x \u2212 x ))g (x ) dx = \u03bb d l=1 \u2202 2 g (x) \u2202x 2 l .\nNote that B 2 F = k,l b 2 kl and \u2206 = l \u2202 2 x l . Therefore, we have\n\u2212 1 m B 2 F \u03bbg(x) = \u03bb\u2206g(x). (B.4)\nWhen \u03bb = 0, the above equation is equivalent to\n\u2206g(x) = \u2212 1 m B 2 F g(x). (B.5)\nThis concludes the proof.\n\n\nC Proof of Proposition 3.2\n\nProof. Suppose that g(x) is an eigenfunction of the integral operator K(x, x ) = cos(b(x \u2212 x )) with respect to the non-zero eigenvalue \u03bb, i.e. Since LHS = RHS, we have [C 1 I 1 + C 2 I 2 ] cos(bx) + [C 1 I 2 + C 2 I 3 ] sin(bx) = \u03bb [C 1 cos(bx) + C 2 sin(bx)] , (C.4) which follows C 1 I 1 + C 2 I 2 = \u03bbC 1 (C.5) C 1 I 2 + C 2 I 3 = \u03bbC 2 .\n\n(C.6)\n\nLet A = I 1 I 2 I 2 I 3 and v = C 1 C 2 . Then, the above linear system can be written as\nAv = \u03bbv.\nThis means that the eigenvalue of the kernel function K(x, x ) is determined by the eigenvalue of the matrix A. The characteristic polynomial is given by det(\u03bbI \u2212 A) = (\u03bb \u2212 I 1 )(\u03bb \u2212 I 3 ) \u2212 I 2 2 = \u03bb 2 \u2212 (I 1 + I 3 )\u03bb + I 1 I 3 \u2212 I 2 2 . Note that I 1 + I 3 = 1, and By assumption (ii) in Proposition 3.3, K uu (0) K rr (0) are positive definite, and there exist orthogonal matrix Q u and Q r such that K uu (0) = Q T u \u039b u Q T u , (D.4)\n\nK rr (0) = Q T r \u039b r Q T r , (D. 5) where \u039b u and \u039b r are diagonal matrices whose entries are eigenvalues of K uu (0) and K rr (0), respectively. We remark that \u039b u and \u039b r are invertible since all eigenvalues are strictly positive.\nNow let Q = Q u 0 0 Q r to obtain Q T K(0)Q = Q T u 0 0 Q T r K rr (0) K ur (0) K T ur (0) K rr (0) Q u 0 0 Q r (D.6) = \u039b u Q T u K ur (0)Q r Q T r K ru (0)Q u \u039b r :=\u039b. (D.7)\nFurthermore, letting B = Q T r K ru (0)Q u and P = Therefore, we obtain\nQ T B[u](x b , \u03b8(t)) N [u](x r , \u03b8(t)) \u2212 g(x b ) f (x r ) \u2248 e \u2212P T \u039bP t Q T g(x b ) f (x r ) . (D.9)\nThis concludes the proof.\n\nFigure 1 :\n11D Poisson equation: Results obtained by training a conventional physics-informed neural network (5-layer, 200 hidden units, tanh activations) via 10 7 iterations of gradient descent. Left: Comparison of the predicted and exact solutions. Middle: Point-wise error between the predicted and the exact solution. Right: Evolution of the residual loss L r , the boundary loss L b , as well as the relative L 2 error during training.\n\n\nFigure 2andfigure 3show the visualizations of eigenfunctions and eigenvalues of the NTK computed using 100 equally spaced points in [0, 1] for \u03c3 = 1, 10, respectively. One can observe that the eigenvalues corresponding to the Fourier features\n\nFigure 2 :\n2NTK eigen-decomposition of a fully-connected neural network (4 layer, 100 hidden units, tanh activations) with Fourier features initialized by \u03c3 = 1 on 100 equally spaced points in [0, 1]: (a): The NTK eigenvalues in descending order. (b): The six leading eigenvectors of the NTK in descending order of corresponding eigenvalues.\n\nFigure 3 :\n3NTK eigen-decomposition of a fully-connected neural network (4 layer, 100 hidden units, tanh activations) with Fourier features initialized by \u03c3 = 10 on 100 equally spaced points in [0, 1]: (a): The NTK eigenvalues in descending order. (b): The six leading eigenvectors of the NTK in descending order of corresponding eigenvalues.\n\nFigure 4 :\n4Frequency domain analysis of the first leading NTK eigenvector for a fully-connected neural network (4 layer, 100 hidden units, tanh activations) with Fourier features initialized by different \u03c3 \u2208 [1, 50], evaluated on 100 equally spaced points in [0, 1].\n\nFigure 5 :Figure 6 :Figure 7 :\n567Training a network with Fourier features initialized by \u03c3 = 10 to fit the target function f (x) = sin(20\u03c0x) + sin(2\u03c0x) for 10, 000 epochs: (a): Network prediction (dash red) against the ground truth (light blue). The network prediction exhibits high frequencies when fitting the data points during training. (b) middle: Relative change of the parameters \u03b8 ( ||\u03b8(t)\u2212\u03b8(0)||2 |\u03b8(0) |2 ) of the network during training. (b) right: Relative L 2 training error and test error during training. Training a network with Fourier features initialized by \u03c3 = 10 to fit the target function f (x) = sin(20\u03c0x) + sin(2\u03c0x) for 1, 000 epochs: (a): Network prediction (dash red) against the ground truth (light blue). The network prediction exhibits high frequencies when fitting the data points during training. (b) left: Evolution of NTK eigenvalues during training. (b) middle: Relative change of the parameters \u03b8 ( ||\u03b8(t)\u2212\u03b8(0)||2 |\u03b8(0) |2 ) of the network during training. (b) right: Relative L 2 training error and test error during training. Training a network with Fourier features initialized by \u03c3 = 10 to fit the target function f (x) = sin(\u03c0x) + sin(2\u03c0x) for 1,000 epochs: (a): Network prediction (dash red) against the ground truth (light blue). The network prediction exhibits high frequencies when fitting the data points during training. (b) middle: Relative change of the parameters \u03b8 ( ||\u03b8(t)\u2212\u03b8(0)||2 |\u03b8(0) |2 ) of the network during training. (b) left: Evolution of NTK eigenvalues during training. (b) right: Relative L 2 training error and test error during training.\n\n\n+ b ), for = 2, . . . , L, i = 1, 2, . . . , M (3.26) f \u03b8 (x) = W L+1 \u00b7 H\n\nFigure 8 :\n8(a) Multi-scale Fourier feature architecture: multiple Fourier feature embeddings (initialized with different \u03c3) are applied to input coordinates and then passed through the same fully-connected neural network, before the outputs are finally concatenated with a linear layer. (b) Spatio-temporal multi-scale Fourier feature architecture: Multi-scale Fourier feature architecture: multiple Fourier feature embeddings (initialized with different \u03c3) are separately applied to spatial and temporal input coordinates and then passed through the same fully-connected neural network. Merging of the spatial and temporal outputs is performed using a point-wise multiplication layer, before obtaining the final outputs through a linear layer.\n\n\nfor = 2, . . . , L and i = 1, 2, . . . , M x , \u22121 + b ), for = 2, . . . , L and j = 1, 2, . . . , M t , L , for i = 1, 2, . . . , M x and j = 1, 2, . . . , M t , (3.33) f \u03b8 (x, t) = W L+1 \u00b7 H\n\nFigure 9 :\n91D Poisson equation: Results obtained by training a fully-connected network (2-layer, 100 hidden units, tanh activations) with the proposed multi-scale Fourier feature mappings via 40, 000 iterations of gradient descent. Left: Comparison of the predicted and exact solutions. The relative L 2 error is 1.36e \u2212 03. Middle: Point-wise error between the predicted and the exact solution. Right: Evolution of the residual loss L r , the boundary loss L b , as well as the relative L 2 error during training.\n\nFigure 2 .\n26 summarizes our results of the predicted solution to the 1D Poisson equation after 40, 000 training iterations.\n\nFigure 10 :Figure 11 :\n1011(4.1)    u(x, 0) = sin(500\u03c0x), x \u2208 [0, 1](4.2) u(0, t) = u(1, t) = 0, t \u2208 [0, 1]. 1D Poisson equation: Relative L 2 errors of predicted solutions averaged over 10 independent trials by training a plain fully-connected neural network (2-layer, 100 hidden units, tanh activations), as well as the same network with single Fourier feature mapping initialized by different \u03c3 \u2208 [1, 50]. 1D Poisson equation: (a) Results obtained by training a fully-connected network (2-layer, 100 hidden units, tanh activations) with a single Fourier feature mapping initialized by \u03c3 = 1 via 40, 000 iterations of gradient descent. Left: Comparison of the predicted and exact solutions. Middle: Point-wise error between the predicted and the exact solution. Right: Evolution of the residual loss L r , the boundary loss L b , as well as the relative L 2 error during training. (b) Results obtained by training the same network with single Fourier feature mapping initialized by \u03c3 = 50 via 40, 000 iterations of gradient descent. Left: Comparison of the predicted and exact solutions. Middle: Point-wise error between the predicted and the exact solution. Right: Evolution of the residual loss L r , the boundary loss L b , as well as the relative L 2 error during training.\n\nFigure 12 :\n121D heat equation: (a): Exact solution versus the predicted solution by training a fully-connected network (3-layer, 100 hidden units, tanh activations) with the spatio-temporal Fourier feature mappings via 40,000 iterations of gradient descent. The relative L 2 error is 1.78e \u2212 03. (b): Evolution of the different terms in the loss function, as well as the relative L 2 error during training.\n\nFigure 13 :\n131D heat equation: Relative L 2 errors of predicted solutions averaged over 10 independent trials by training a plain fully-connected neural network (3-layer, 100 hidden units, tanh activations), as well as the same network with single Fourier feature mapping initialized by different \u03c3 \u2208 [1, 1000].\n\nFigure 14 :\n141D Wave equation: (a): Exact solution versus the predicted solution by training a fully-connected network (3-layer, 200 hidden units, tanh activations) with the spatio-temporal Fourier feature mappings via 40,000 iterations of gradient descent. The relative L 2 error is 1.03e + 00. (b): Evolution of the different terms in the loss function, as well as the relative L 2 error during training.\n\nFigure 15 :\n151D wave equation: (a): Exact solution versus the predicted solution by training a fully-connected network (3-layer, 200 hidden units, tanh activations) with the spatio-temporal Fourier feature mappings using adaptive weights algorithm [6] via 40,000 iterations of gradient descent. The relative L 2 error is 9.83e \u2212 04. (b): Evolution of the different terms in the loss function, as well as the relative L 2 error during training.\n\n( 4 .\n416) where u, v represent the concentrations of U, V respectively and 1 , 2 are their corresponding diffusion rates.We generate a data-set containing a direct numerical solution of the two-dimensional Gray-Scott equations with 40,000 spatial points and 401 temporal snapshots. Specifically, we take b = 0.04, d = 0.1, 1 = 2e \u2212 5, 2 = 1e \u2212 5 and, assuming periodic boundary conditions, we start from an initial condition u(x, y, 0) = 1 \u2212 exp(\u221280((x + 0.05) 2 + (y + 0.02) 2 ), (x, y) \u2208 [\u22121, 1] \u00d7 [\u22121, 1] v(x, y, 0) = exp(\u221280((x \u2212 0.05) 2 + (y \u2212 0.02) 2 ), (x, y) \u2208 [\u22121, 1] \u00d7 [\u22121, 1],\n\n\nFigure 17aand figure 17b presents the comparisons of reconstructed concentrations u, v against the ground truth functions at the final time t = 4000. The results show excellent agreement between the predictions and the numerical estimations. This is further validated by the relative L 2 -norm of error results shown infigure 17c. Moreover, the evolution of inferred diffusion rates during training, as well as the final predictions are presented infigure 17dand table 2 respectively, which show good agreement with the exact values.\n\nFigure 16 :Figure 17 :\n16172D Gray-Scott equation: Representative snapshots of the ground truth concentration fields u, v at t = 3500. 2D Gray-Scott equation: Results obtained by training a fully-connected network (9 layers, 100 hidden units, tanh activations) with the spatio-temporal Fourier feature mappings after 120, 000 iterations of gradient descent. (a)(b) Numerical estimations versus the predicted concentration fields u, v respective at the final time t = 4000. (c) Relative L 2 errors between the model predictions and the corresponding exact concentration fields for each snapshot t \u2208 [3500, 4000]. (d) Evolution of the inferred diffusion rates 1 , 2 during training.\n\nFigure 18 :\n182D Gray-Scott equation: Results obtained by training a fully-connected network (9 layers, 100 hidden units, tanh activations) after 120, 000 iterations of gradient descent. (a)(b) Numerical estimations versus the predicted concentration fields u, v respective at the final time t = 4000. (c) Relative L 2 errors between the model predictions and the corresponding exact concentration fields for each snapshot t \u2208 [3500, 4000]. (d) Evolution of the inferred diffusion rates 1 , 2 during training.\n\n\nx \u2212 x )), where x = (x 1 , . . . , x l , . . . , x d ). . . . , d, taking derivatives with respect to x l gives\n\n\nb(x \u2212 x ))g (x ) dx = \u03bbg (x) . (C.1) By Lemma 3.1, first we know that g(x) satisfiesg (x) = \u2212b 2 g(x).(C.2) Note that this is a ODE and thus g(x) must have the form ofg(x) = C 1 cos(bx) + C 2 sin(bx), (C.3) where C 1 , C 2 are some constants.Next, we compute the corresponding eigenvalue. Substituting the expression of g into equation C.1 we get1 0 cos(b(x \u2212 x )) [C 1 cos(bx ) + C 2 sin(bx )] dx = \u03bb [C 1 cos(bx ) + C 2 sin(bx )] . b(x \u2212 x )) [C 1 cos(bx ) + C 2 sin(bx )] dx= 1 0 cos(bx) cos(bx ) + sin(bx) sin(bx ) [C 1 cos(bx ) + C 2 sin(bx )] dx = cos(bx) 1 0 cos(bx ) [C 1 cos(bx ) + C 2 sin(bx )] dx + sin(bx) 1 0 sin(bx ) [C 1 cos(bx ) + C 2 sin(bx )] dx = cos(bx)[C 1 I 1 + C 2 I 2 ] + sin(bx)[C 1 I 2 + C 2 I 3 ],\n\nP\n= P T \u039bP . (D.8)\nA Definition of fully-connected neural networksA scalar-valued fully-connected neural network with L hidden layers is defined recursively as followsfor h = 1, . . . , L, where W (h) \u2208 R d h+1 \u00d7d h are weight matrices and b (h) \u2208 R d h+1 are bias vectors in the h-th hidden layer, and \u03c3 : R \u2192 R is a coordinate-wise smooth activation function. The final output of the neural network is given bywhere W (L) \u2208 R 1\u00d7d L and b (L) \u2208 R are the weight and bias parameters of the last layer. Here, \u03b8 =} denotes all parameters of the network and are initialized as independent and identically distributed (i.i.d) Gaussian random variables N (0, 1). We remark that such a parameterization is known as the \"NTK parameterization\" following the original work of Jacot et. al.[33].Then the eigenvalues areD Proof of Proposition 3.3Proof. By assumption (i) in Proposition 3.3, we immediately obtain\nAutomatic differentiation in machine learning: a survey. At\u0131l\u0131m G\u00fcnes Baydin, A Barak, Alexey Pearlmutter, Jeffrey Mark Andreyevich Radul, Siskind, The Journal of Machine Learning Research. 181At\u0131l\u0131m G\u00fcnes Baydin, Barak A Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. Automatic differentiation in machine learning: a survey. The Journal of Machine Learning Research, 18(1):5595-5637, 2017.\n\nA hybrid neural network-first principles approach to process modeling. C Dimitris, Psichogios, H Lyle, Ungar, AIChE Journal. 3810Dimitris C Psichogios and Lyle H Ungar. A hybrid neural network-first principles approach to process modeling. AIChE Journal, 38(10):1499-1511, 1992.\n\nAristidis Likas, and Dimitrios I Fotiadis. Artificial neural networks for solving ordinary and partial differential equations. E Isaac, Lagaris, IEEE transactions on neural networks. 95Isaac E Lagaris, Aristidis Likas, and Dimitrios I Fotiadis. Artificial neural networks for solving ordinary and partial differential equations. IEEE transactions on neural networks, 9(5):987-1000, 1998.\n\nPhysics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Maziar Raissi, Paris Perdikaris, George E Karniadakis, Journal of Computational Physics. 378Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational Physics, 378:686-707, 2019.\n\nOn the convergence of physics informed neural networks for linear second-order elliptic and parabolic type PDEs. Yeonjong Shin, J\u00e9r\u00f4me Darbon, George Em Karniadakis, Yeonjong Shin, J\u00e9r\u00f4me Darbon, and George Em Karniadakis. On the convergence of physics informed neural networks for linear second-order elliptic and parabolic type PDEs. 2020.\n\nWhen and why PINNs fail to train: A neural tangent kernel perspective. Sifan Wang, Xinling Yu, Paris Perdikaris, arXiv:2007.14527arXiv preprintSifan Wang, Xinling Yu, and Paris Perdikaris. When and why PINNs fail to train: A neural tangent kernel perspective. arXiv preprint arXiv:2007.14527, 2020.\n\nTwo-layer neural networks for partial differential equations: Optimization and generalization theory. Tao Luo, Haizhao Yang, arXiv:2006.15733arXiv preprintTao Luo and Haizhao Yang. Two-layer neural networks for partial differential equations: Optimization and generalization theory. arXiv preprint arXiv:2006.15733, 2020.\n\nError estimates of residual minimization using neural networks for linear PDEs. Yeonjong Shin, Zhongqiang Zhang, George Em Karniadakis, arXiv:2010.08019arXiv preprintYeonjong Shin, Zhongqiang Zhang, and George Em Karniadakis. Error estimates of residual minimization using neural networks for linear PDEs. arXiv preprint arXiv:2010.08019, 2020.\n\nZongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, Anima Anandkumar, arXiv:2010.08895Fourier neural operator for parametric partial differential equations. arXiv preprintZongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations. arXiv preprint arXiv:2010.08895, 2020.\n\nPhysics-constrained deep learning for high-dimensional surrogate modeling and uncertainty quantification without labeled data. Yinhao Zhu, Nicholas Zabaras, Journal of Computational Physics. 394Phaedon-Stelios Koutsourelakis, and Paris PerdikarisYinhao Zhu, Nicholas Zabaras, Phaedon-Stelios Koutsourelakis, and Paris Perdikaris. Physics-constrained deep learning for high-dimensional surrogate modeling and uncertainty quantification without labeled data. Journal of Computational Physics, 394:56-81, 2019.\n\nSurrogate modeling for fluid flows based on physicsconstrained deep learning without simulation data. Luning Sun, Han Gao, Shaowu Pan, Jian-Xun Wang, Computer Methods in Applied Mechanics and Engineering. 361112732Luning Sun, Han Gao, Shaowu Pan, and Jian-Xun Wang. Surrogate modeling for fluid flows based on physics- constrained deep learning without simulation data. Computer Methods in Applied Mechanics and Engineering, 361:112732, 2020.\n\nHidden fluid mechanics: Learning velocity and pressure fields from flow visualizations. Maziar Raissi, Alireza Yazdani, George Em Karniadakis, Science. 3676481Maziar Raissi, Alireza Yazdani, and George Em Karniadakis. Hidden fluid mechanics: Learning velocity and pressure fields from flow visualizations. Science, 367(6481):1026-1030, 2020.\n\nXiaowei Jin, Shengze Cai, Hui Li, George Em Karniadakis, arXiv:2003.06496Nsfnets (Navier-Stokes flow nets): Physicsinformed neural networks for the incompressible Navier-Stokes equations. arXiv preprintXiaowei Jin, Shengze Cai, Hui Li, and George Em Karniadakis. Nsfnets (Navier-Stokes flow nets): Physics- informed neural networks for the incompressible Navier-Stokes equations. arXiv preprint arXiv:2003.06496, 2020.\n\nBrandon Reyes, Amanda A Howard, Paris Perdikaris, Alexandre M Tartakovsky, arXiv:2009.01658Learning unknown physics of non-Newtonian fluids. arXiv preprintBrandon Reyes, Amanda A Howard, Paris Perdikaris, and Alexandre M Tartakovsky. Learning unknown physics of non-Newtonian fluids. arXiv preprint arXiv:2009.01658, 2020.\n\nPhysics-informed neural networks for cardiac activation mapping. Francisco Sahli Costabal, Yibo Yang, Paris Perdikaris, E Daniel, Ellen Hurtado, Kuhl, Frontiers in Physics. 842Francisco Sahli Costabal, Yibo Yang, Paris Perdikaris, Daniel E Hurtado, and Ellen Kuhl. Physics-informed neural networks for cardiac activation mapping. Frontiers in Physics, 8:42, 2020.\n\nMachine learning in cardiovascular flows modeling: Predicting arterial blood pressure from non-invasive 4D flow MRI data using physics-informed neural networks. Georgios Kissas, Yibo Yang, Eileen Hwuang, R Walter, John A Witschey, Paris Detre, Perdikaris, Computer Methods in Applied Mechanics and Engineering. 358112623Georgios Kissas, Yibo Yang, Eileen Hwuang, Walter R Witschey, John A Detre, and Paris Perdikaris. Machine learning in cardiovascular flows modeling: Predicting arterial blood pressure from non-invasive 4D flow MRI data using physics-informed neural networks. Computer Methods in Applied Mechanics and Engineering, 358:112623, 2020.\n\nSystems biology informed deep learning for inferring parameters and hidden dynamics. Alireza Yazdani, Lu Lu, Maziar Raissi, George Em Karniadakis, PLoS computational biology. 16111007575Alireza Yazdani, Lu Lu, Maziar Raissi, and George Em Karniadakis. Systems biology informed deep learning for inferring parameters and hidden dynamics. PLoS computational biology, 16(11):e1007575, 2020.\n\nDeep physical informed neural networks for metamaterial design. Zhiwei Fang, Justin Zhan, IEEE Access. 8Zhiwei Fang and Justin Zhan. Deep physical informed neural networks for metamaterial design. IEEE Access, 8:24506-24513, 2019.\n\nPhysics-informed neural networks for inverse problems in nano-optics and metamaterials. Yuyao Chen, Lu Lu, George Em Karniadakis, Luca Dal Negro, Optics Express. 288Yuyao Chen, Lu Lu, George Em Karniadakis, and Luca Dal Negro. Physics-informed neural networks for inverse problems in nano-optics and metamaterials. Optics Express, 28(8):11618-11633, 2020.\n\nDeep learning of free boundary and Stefan problems. Sifan Wang, Paris Perdikaris, arXiv:2006.05311arXiv preprintSifan Wang and Paris Perdikaris. Deep learning of free boundary and Stefan problems. arXiv preprint arXiv:2006.05311, 2020.\n\nB-pinns: Bayesian physics-informed neural networks for forward and inverse pde problems with noisy data. Liu Yang, Xuhui Meng, George Em Karniadakis, arXiv:2003.06097arXiv preprintLiu Yang, Xuhui Meng, and George Em Karniadakis. B-pinns: Bayesian physics-informed neural networks for forward and inverse pde problems with noisy data. arXiv preprint arXiv:2003.06097, 2020.\n\nBayesian differential programming for robust systems identification under uncertainty. Yibo Yang, Mohamed Aziz Bhouri, Paris Perdikaris, Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences. 47620200290Yibo Yang, Mohamed Aziz Bhouri, and Paris Perdikaris. Bayesian differential programming for robust systems identification under uncertainty. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 476(2243):20200290, November 2020.\n\nSolving high-dimensional partial differential equations using deep learning. Jiequn Han, Arnulf Jentzen, E Weinan, Proceedings of the National Academy of Sciences. 11534Jiequn Han, Arnulf Jentzen, and E Weinan. Solving high-dimensional partial differential equations using deep learning. Proceedings of the National Academy of Sciences, 115(34):8505-8510, 2018.\n\nMaziar Raissi, arXiv:1804.07010Forward-backward stochastic neural networks: Deep learning of high-dimensional partial differential equations. arXiv preprintMaziar Raissi. Forward-backward stochastic neural networks: Deep learning of high-dimensional partial differen- tial equations. arXiv preprint arXiv:1804.07010, 2018.\n\nSimulator-free solution of high-dimensional stochastic elliptic partial differential equations using deep neural networks. Sharmila Karumuri, Rohit Tripathy, Ilias Bilionis, and Jitesh Panchal. 404109120Sharmila Karumuri, Rohit Tripathy, Ilias Bilionis, and Jitesh Panchal. Simulator-free solution of high-dimensional stochastic elliptic partial differential equations using deep neural networks. Journal of Computational Physics, 404:109120, 2020.\n\nAdversarial uncertainty quantification in physics-informed neural networks. Yibo Yang, Paris Perdikaris, Journal of Computational Physics. 394Yibo Yang and Paris Perdikaris. Adversarial uncertainty quantification in physics-informed neural networks. Journal of Computational Physics, 394:136-152, 2019.\n\nDeepONet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators. Lu Lu, Pengzhan Jin, George Em Karniadakis, arXiv:1910.03193arXiv preprintLu Lu, Pengzhan Jin, and George Em Karniadakis. DeepONet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators. arXiv preprint arXiv:1910.03193, 2019.\n\nHan Gao, Luning Sun, Jian-Xun Wang, arXiv:2004.13145PhyGeoNet: Physics-informed geometry-adaptive convolutional neural networks for solving parametric PDEs on irregular domain. arXiv preprintHan Gao, Luning Sun, and Jian-Xun Wang. PhyGeoNet: Physics-informed geometry-adaptive convolutional neural networks for solving parametric PDEs on irregular domain. arXiv preprint arXiv:2004.13145, 2020.\n\nUnderstanding and mitigating gradient pathologies in physicsinformed neural networks. Sifan Wang, Yujun Teng, Paris Perdikaris, arXiv:2001.04536arXiv preprintSifan Wang, Yujun Teng, and Paris Perdikaris. Understanding and mitigating gradient pathologies in physics- informed neural networks. arXiv preprint arXiv:2001.04536, 2020.\n\nOn the spectral bias of neural networks. Aristide Nasim Rahaman, Devansh Baratin, Felix Arpit, Min Draxler, Fred Lin, Yoshua Hamprecht, Aaron Bengio, Courville, International Conference on Machine Learning. Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred Hamprecht, Yoshua Bengio, and Aaron Courville. On the spectral bias of neural networks. In International Conference on Machine Learning, pages 5301-5310, 2019.\n\nTowards understanding the spectral bias of deep learning. Yuan Cao, Zhiying Fang, Yue Wu, Ding-Xuan Zhou, Quanquan Gu, arXiv:1912.01198arXiv preprintYuan Cao, Zhiying Fang, Yue Wu, Ding-Xuan Zhou, and Quanquan Gu. Towards understanding the spectral bias of deep learning. arXiv preprint arXiv:1912.01198, 2019.\n\nThe convergence rate of neural networks for learned functions of different frequencies. David Basri Ronen, Yoni Jacobs, Shira Kasten, Kritchman, Advances in Neural Information Processing Systems. Basri Ronen, David Jacobs, Yoni Kasten, and Shira Kritchman. The convergence rate of neural networks for learned functions of different frequencies. In Advances in Neural Information Processing Systems, pages 4761-4771, 2019.\n\nNeural tangent kernel: Convergence and generalization in neural networks. Arthur Jacot, Franck Gabriel, Cl\u00e9ment Hongler, Advances in neural information processing systems. Arthur Jacot, Franck Gabriel, and Cl\u00e9ment Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in neural information processing systems, pages 8571-8580, 2018.\n\nOn exact computation with an infinitely wide neural net. Sanjeev Arora, S Simon, Wei Du, Zhiyuan Hu, Li, R Russ, Ruosong Salakhutdinov, Wang, Advances in Neural Information Processing Systems. Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On exact computation with an infinitely wide neural net. In Advances in Neural Information Processing Systems, pages 8141-8150, 2019.\n\nWide neural networks of any depth evolve as linear models under gradient descent. Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-Dickstein, Jeffrey Pennington, Advances in neural information processing systems. Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient descent. In Advances in neural information processing systems, pages 8572-8583, 2019.\n\nMulti-scale deep neural network (MscaleDNN) methods for oscillatory stokes flows in complex domains. Bo Wang, Wenzhong Zhang, Wei Cai, arXiv:2009.12729arXiv preprintBo Wang, Wenzhong Zhang, and Wei Cai. Multi-scale deep neural network (MscaleDNN) methods for oscillatory stokes flows in complex domains. arXiv preprint arXiv:2009.12729, 2020.\n\nA DNN-based algorithm for multi-scale elliptic problems. Xi-An Li, Zhi-Qin John Xu, Lei Zhang, arXiv:2009.14597arXiv preprintXi-An Li, Zhi-Qin John Xu, and Lei Zhang. A DNN-based algorithm for multi-scale elliptic problems. arXiv preprint arXiv:2009.14597, 2020.\n\nMulti-scale deep neural network (MscaleDNN) for solving Poisson-Boltzmann equation in complex domains. Ziqi Liu, Wei Cai, Zhi-Qin John Xu, arXiv:2007.11207arXiv preprintZiqi Liu, Wei Cai, and Zhi-Qin John Xu. Multi-scale deep neural network (MscaleDNN) for solving Poisson- Boltzmann equation in complex domains. arXiv preprint arXiv:2007.11207, 2020.\n\nFourier features let networks learn high frequency functions in low dimensional domains. Matthew Tancik, P Pratul, Ben Srinivasan, Sara Mildenhall, Nithin Fridovich-Keil, Utkarsh Raghavan, Ravi Singhal, Jonathan T Ramamoorthi, Ren Barron, Ng, arXiv:2006.10739arXiv preprintMatthew Tancik, Pratul P Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan T Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. arXiv preprint arXiv:2006.10739, 2020.\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980arXiv preprintDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n\nOn the eigenspectrum of the gram matrix and the generalization error of kernel-PCA. John Shawe-Taylor, K I Christopher, Nello Williams, Jaz Cristianini, Kandola, IEEE Transactions on Information Theory. 517John Shawe-Taylor, Christopher KI Williams, Nello Cristianini, and Jaz Kandola. On the eigenspectrum of the gram matrix and the generalization error of kernel-PCA. IEEE Transactions on Information Theory, 51(7):2510- 2522, 2005.\n\nPartial Differential Equations. Graduate studies in mathematics. L C Evans, American Mathematical SocietyL.C. Evans and American Mathematical Society. Partial Differential Equations. Graduate studies in mathematics. American Mathematical Society, 1998.\n\nSpectral methods for time-dependent problems. Sigal Jan S Hesthaven, David Gottlieb, Gottlieb, Cambridge University Press21Jan S Hesthaven, Sigal Gottlieb, and David Gottlieb. Spectral methods for time-dependent problems, volume 21. Cambridge University Press, 2007.\n\nUnderstanding the difficulty of training deep feedforward neural networks. Xavier Glorot, Yoshua Bengio, Proceedings of the thirteenth international conference on artificial intelligence and statistics. the thirteenth international conference on artificial intelligence and statisticsXavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249-256, 2010.\n\nChemical oscillations and instabilities: non-linear chemical kinetics. Peter Gray, K Stephen, Scott, Peter Gray and Stephen K Scott. Chemical oscillations and instabilities: non-linear chemical kinetics. 1990.\n\n. A Tobin, Nicholas Driscoll, Lloyd N Hale, Trefethen, Chebfun guideTobin A Driscoll, Nicholas Hale, and Lloyd N Trefethen. Chebfun guide, 2014.\n\nExponential time differencing for stiff systems. M Steven, Paul C Cox, Matthews, Journal of Computational Physics. 1762Steven M Cox and Paul C Matthews. Exponential time differencing for stiff systems. Journal of Computational Physics, 176(2):430-455, 2002.\n\nNonlinear analysis of hydrodynamic instability in laminar flames-i. derivation of basic equations. Gi Sivashinsky, AcAau4GI Sivashinsky. Nonlinear analysis of hydrodynamic instability in laminar flames-i. derivation of basic equations. AcAau, 4(11):1177-1206, 1977.\n\nDiffusion-induced chaos in reaction systems. Yoshiki Kuramoto, Progress of Theoretical Physics Supplement. 64Yoshiki Kuramoto. Diffusion-induced chaos in reaction systems. Progress of Theoretical Physics Supplement, 64:346-367, 1978.\n", "annotations": {"author": "[{\"end\":526,\"start\":155},{\"end\":878,\"start\":527},{\"end\":1235,\"start\":879}]", "publisher": null, "author_last_name": "[{\"end\":165,\"start\":161},{\"end\":538,\"start\":534},{\"end\":895,\"start\":885}]", "author_first_name": "[{\"end\":160,\"start\":155},{\"end\":533,\"start\":527},{\"end\":884,\"start\":879}]", "author_affiliation": "[{\"end\":525,\"start\":188},{\"end\":877,\"start\":540},{\"end\":1234,\"start\":897}]", "title": "[{\"end\":135,\"start\":1},{\"end\":1370,\"start\":1236}]", "venue": null, "abstract": "[{\"end\":2699,\"start\":1505}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2774,\"start\":2770},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2777,\"start\":2774},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":2780,\"start\":2777},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2922,\"start\":2919},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":2944,\"start\":2940},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2947,\"start\":2944},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":2950,\"start\":2947},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":3143,\"start\":3139},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":3146,\"start\":3143},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":3149,\"start\":3146},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":3422,\"start\":3418},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5880,\"start\":5877},{\"end\":6019,\"start\":6014},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6566,\"start\":6563},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6864,\"start\":6862},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7370,\"start\":7367},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7636,\"start\":7632},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7638,\"start\":7636},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7689,\"start\":7685},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7692,\"start\":7689},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7695,\"start\":7692},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7742,\"start\":7739},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":8974,\"start\":8970},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9497,\"start\":9493},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9499,\"start\":9497},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":10218,\"start\":10214},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":10221,\"start\":10218},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":10224,\"start\":10221},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":10280,\"start\":10276},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":10283,\"start\":10280},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":10286,\"start\":10283},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":10819,\"start\":10815},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":10822,\"start\":10819},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":11310,\"start\":11306},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":12870,\"start\":12866},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":13139,\"start\":13135},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":13142,\"start\":13139},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":13198,\"start\":13194},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":13740,\"start\":13736},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":13828,\"start\":13824},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":14067,\"start\":14063},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":14239,\"start\":14235},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":14258,\"start\":14254},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":16394,\"start\":16390},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":19020,\"start\":19017},{\"end\":19023,\"start\":19020},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":19482,\"start\":19478},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":19832,\"start\":19828},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":23379,\"start\":23376},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":23530,\"start\":23527},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":27189,\"start\":27185},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":27702,\"start\":27698},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":29935,\"start\":29931},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":30946,\"start\":30942},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":31034,\"start\":31030},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":32810,\"start\":32807},{\"end\":32813,\"start\":32810},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":33209,\"start\":33205},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":36593,\"start\":36589},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":38403,\"start\":38400},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":39095,\"start\":39092},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":40100,\"start\":40097},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":40878,\"start\":40874},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":41337,\"start\":41333},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":41427,\"start\":41423},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":43671,\"start\":43668},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":46053,\"start\":46049},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":46056,\"start\":46053},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":48937,\"start\":48935},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":57285,\"start\":57282}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":49950,\"start\":49509},{\"attributes\":{\"id\":\"fig_2\"},\"end\":50195,\"start\":49951},{\"attributes\":{\"id\":\"fig_3\"},\"end\":50538,\"start\":50196},{\"attributes\":{\"id\":\"fig_4\"},\"end\":50882,\"start\":50539},{\"attributes\":{\"id\":\"fig_5\"},\"end\":51151,\"start\":50883},{\"attributes\":{\"id\":\"fig_6\"},\"end\":52754,\"start\":51152},{\"attributes\":{\"id\":\"fig_7\"},\"end\":52830,\"start\":52755},{\"attributes\":{\"id\":\"fig_8\"},\"end\":53577,\"start\":52831},{\"attributes\":{\"id\":\"fig_9\"},\"end\":53771,\"start\":53578},{\"attributes\":{\"id\":\"fig_10\"},\"end\":54288,\"start\":53772},{\"attributes\":{\"id\":\"fig_11\"},\"end\":54414,\"start\":54289},{\"attributes\":{\"id\":\"fig_12\"},\"end\":55695,\"start\":54415},{\"attributes\":{\"id\":\"fig_14\"},\"end\":56104,\"start\":55696},{\"attributes\":{\"id\":\"fig_15\"},\"end\":56418,\"start\":56105},{\"attributes\":{\"id\":\"fig_16\"},\"end\":56827,\"start\":56419},{\"attributes\":{\"id\":\"fig_17\"},\"end\":57273,\"start\":56828},{\"attributes\":{\"id\":\"fig_18\"},\"end\":57863,\"start\":57274},{\"attributes\":{\"id\":\"fig_19\"},\"end\":58399,\"start\":57864},{\"attributes\":{\"id\":\"fig_20\"},\"end\":59081,\"start\":58400},{\"attributes\":{\"id\":\"fig_21\"},\"end\":59592,\"start\":59082},{\"attributes\":{\"id\":\"fig_22\"},\"end\":59706,\"start\":59593},{\"attributes\":{\"id\":\"fig_23\"},\"end\":60433,\"start\":59707},{\"attributes\":{\"id\":\"fig_24\"},\"end\":60453,\"start\":60434}]", "paragraph": "[{\"end\":3770,\"start\":2701},{\"end\":4490,\"start\":3772},{\"end\":4992,\"start\":4492},{\"end\":5751,\"start\":4994},{\"end\":5958,\"start\":5788},{\"end\":6516,\"start\":5987},{\"end\":6814,\"start\":6518},{\"end\":6920,\"start\":6856},{\"end\":7639,\"start\":7102},{\"end\":7852,\"start\":7641},{\"end\":7934,\"start\":7885},{\"end\":7979,\"start\":7936},{\"end\":8263,\"start\":8012},{\"end\":8474,\"start\":8265},{\"end\":8671,\"start\":8599},{\"end\":9824,\"start\":8730},{\"end\":10479,\"start\":9898},{\"end\":10700,\"start\":10596},{\"end\":10902,\"start\":10732},{\"end\":11238,\"start\":10970},{\"end\":11330,\"start\":11240},{\"end\":11479,\"start\":11399},{\"end\":11893,\"start\":11595},{\"end\":12373,\"start\":12159},{\"end\":13233,\"start\":12555},{\"end\":13741,\"start\":13235},{\"end\":13870,\"start\":13772},{\"end\":14240,\"start\":13901},{\"end\":14563,\"start\":14242},{\"end\":14748,\"start\":14565},{\"end\":15065,\"start\":14953},{\"end\":15492,\"start\":15256},{\"end\":15663,\"start\":15593},{\"end\":15951,\"start\":15725},{\"end\":16119,\"start\":16019},{\"end\":16196,\"start\":16152},{\"end\":16493,\"start\":16198},{\"end\":16857,\"start\":16495},{\"end\":17129,\"start\":16885},{\"end\":17156,\"start\":17150},{\"end\":17258,\"start\":17158},{\"end\":17297,\"start\":17260},{\"end\":17343,\"start\":17299},{\"end\":17973,\"start\":17345},{\"end\":18310,\"start\":17975},{\"end\":19153,\"start\":18312},{\"end\":19229,\"start\":19155},{\"end\":19308,\"start\":19278},{\"end\":20305,\"start\":19334},{\"end\":21611,\"start\":20307},{\"end\":22858,\"start\":21613},{\"end\":23328,\"start\":22938},{\"end\":23656,\"start\":23330},{\"end\":23885,\"start\":23715},{\"end\":24303,\"start\":24151},{\"end\":24664,\"start\":24450},{\"end\":25010,\"start\":24737},{\"end\":25111,\"start\":25012},{\"end\":25298,\"start\":25292},{\"end\":25599,\"start\":25300},{\"end\":26324,\"start\":25601},{\"end\":26791,\"start\":26326},{\"end\":27435,\"start\":26946},{\"end\":27594,\"start\":27437},{\"end\":27754,\"start\":27627},{\"end\":28499,\"start\":27769},{\"end\":28819,\"start\":28501},{\"end\":29134,\"start\":29123},{\"end\":29288,\"start\":29136},{\"end\":29741,\"start\":29290},{\"end\":30010,\"start\":29743},{\"end\":30132,\"start\":30048},{\"end\":30665,\"start\":30134},{\"end\":31331,\"start\":30677},{\"end\":31534,\"start\":31355},{\"end\":32334,\"start\":31536},{\"end\":33558,\"start\":32336},{\"end\":33927,\"start\":33598},{\"end\":34630,\"start\":33937},{\"end\":35488,\"start\":34891},{\"end\":36157,\"start\":35490},{\"end\":36555,\"start\":36178},{\"end\":36563,\"start\":36557},{\"end\":37444,\"start\":36565},{\"end\":37660,\"start\":37465},{\"end\":38987,\"start\":37883},{\"end\":40347,\"start\":38989},{\"end\":40653,\"start\":40349},{\"end\":41877,\"start\":40723},{\"end\":42299,\"start\":42001},{\"end\":42669,\"start\":42337},{\"end\":42977,\"start\":42939},{\"end\":43480,\"start\":43089},{\"end\":44015,\"start\":43482},{\"end\":45278,\"start\":44030},{\"end\":46326,\"start\":45280},{\"end\":47717,\"start\":46328},{\"end\":47843,\"start\":47776},{\"end\":47925,\"start\":47878},{\"end\":47983,\"start\":47958},{\"end\":48354,\"start\":48014},{\"end\":48361,\"start\":48356},{\"end\":48452,\"start\":48363},{\"end\":48900,\"start\":48462},{\"end\":49134,\"start\":48902},{\"end\":49381,\"start\":49310},{\"end\":49508,\"start\":49483}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":5986,\"start\":5959},{\"attributes\":{\"id\":\"formula_1\"},\"end\":6855,\"start\":6815},{\"attributes\":{\"id\":\"formula_2\"},\"end\":7042,\"start\":6921},{\"attributes\":{\"id\":\"formula_3\"},\"end\":7101,\"start\":7042},{\"attributes\":{\"id\":\"formula_4\"},\"end\":7884,\"start\":7853},{\"attributes\":{\"id\":\"formula_5\"},\"end\":8011,\"start\":7980},{\"attributes\":{\"id\":\"formula_6\"},\"end\":8598,\"start\":8475},{\"attributes\":{\"id\":\"formula_7\"},\"end\":8729,\"start\":8672},{\"attributes\":{\"id\":\"formula_8\"},\"end\":10595,\"start\":10480},{\"attributes\":{\"id\":\"formula_9\"},\"end\":10731,\"start\":10701},{\"attributes\":{\"id\":\"formula_10\"},\"end\":10969,\"start\":10903},{\"attributes\":{\"id\":\"formula_11\"},\"end\":11398,\"start\":11331},{\"attributes\":{\"id\":\"formula_12\"},\"end\":11594,\"start\":11480},{\"attributes\":{\"id\":\"formula_13\"},\"end\":12158,\"start\":11894},{\"attributes\":{\"id\":\"formula_14\"},\"end\":12554,\"start\":12374},{\"attributes\":{\"id\":\"formula_15\"},\"end\":13900,\"start\":13871},{\"attributes\":{\"id\":\"formula_16\"},\"end\":14952,\"start\":14749},{\"attributes\":{\"id\":\"formula_17\"},\"end\":15255,\"start\":15066},{\"attributes\":{\"id\":\"formula_18\"},\"end\":15592,\"start\":15493},{\"attributes\":{\"id\":\"formula_19\"},\"end\":15724,\"start\":15664},{\"attributes\":{\"id\":\"formula_20\"},\"end\":16018,\"start\":15952},{\"attributes\":{\"id\":\"formula_21\"},\"end\":16151,\"start\":16120},{\"attributes\":{\"id\":\"formula_22\"},\"end\":16884,\"start\":16858},{\"attributes\":{\"id\":\"formula_23\"},\"end\":17149,\"start\":17130},{\"attributes\":{\"id\":\"formula_24\"},\"end\":19277,\"start\":19230},{\"attributes\":{\"id\":\"formula_25\"},\"end\":19333,\"start\":19309},{\"attributes\":{\"id\":\"formula_26\"},\"end\":23714,\"start\":23657},{\"attributes\":{\"id\":\"formula_27\"},\"end\":23930,\"start\":23886},{\"attributes\":{\"id\":\"formula_28\"},\"end\":24150,\"start\":23930},{\"attributes\":{\"id\":\"formula_29\"},\"end\":24449,\"start\":24304},{\"attributes\":{\"id\":\"formula_30\"},\"end\":24730,\"start\":24665},{\"attributes\":{\"id\":\"formula_31\"},\"end\":25291,\"start\":25112},{\"attributes\":{\"id\":\"formula_32\"},\"end\":26930,\"start\":26792},{\"attributes\":{\"id\":\"formula_33\"},\"end\":26945,\"start\":26930},{\"attributes\":{\"id\":\"formula_34\"},\"end\":27626,\"start\":27595},{\"attributes\":{\"id\":\"formula_35\"},\"end\":27768,\"start\":27755},{\"attributes\":{\"id\":\"formula_36\"},\"end\":29107,\"start\":28820},{\"attributes\":{\"id\":\"formula_37\"},\"end\":29122,\"start\":29107},{\"attributes\":{\"id\":\"formula_38\"},\"end\":30047,\"start\":30011},{\"attributes\":{\"id\":\"formula_39\"},\"end\":34890,\"start\":34631},{\"attributes\":{\"id\":\"formula_40\"},\"end\":37464,\"start\":37445},{\"attributes\":{\"id\":\"formula_41\"},\"end\":37815,\"start\":37661},{\"attributes\":{\"id\":\"formula_42\"},\"end\":37882,\"start\":37815},{\"attributes\":{\"id\":\"formula_43\"},\"end\":41940,\"start\":41878},{\"attributes\":{\"id\":\"formula_44\"},\"end\":42000,\"start\":41940},{\"attributes\":{\"id\":\"formula_45\"},\"end\":42336,\"start\":42300},{\"attributes\":{\"id\":\"formula_46\"},\"end\":42938,\"start\":42670},{\"attributes\":{\"id\":\"formula_47\"},\"end\":43088,\"start\":42978},{\"attributes\":{\"id\":\"formula_48\"},\"end\":47775,\"start\":47718},{\"attributes\":{\"id\":\"formula_49\"},\"end\":47877,\"start\":47844},{\"attributes\":{\"id\":\"formula_50\"},\"end\":47957,\"start\":47926},{\"attributes\":{\"id\":\"formula_51\"},\"end\":48461,\"start\":48453},{\"attributes\":{\"id\":\"formula_52\"},\"end\":49309,\"start\":49135},{\"attributes\":{\"id\":\"formula_53\"},\"end\":49482,\"start\":49382}]", "table_ref": "[{\"end\":40405,\"start\":40398},{\"end\":41093,\"start\":41086}]", "section_header": "[{\"attributes\":{\"n\":\"2\"},\"end\":5786,\"start\":5754},{\"attributes\":{\"n\":\"3.1\"},\"end\":9896,\"start\":9827},{\"attributes\":{\"n\":\"3.2\"},\"end\":13770,\"start\":13744},{\"attributes\":{\"n\":\"3.3\"},\"end\":22936,\"start\":22861},{\"end\":24735,\"start\":24732},{\"attributes\":{\"n\":\"4\"},\"end\":30675,\"start\":30668},{\"attributes\":{\"n\":\"4.1\"},\"end\":31353,\"start\":31334},{\"attributes\":{\"n\":\"4.2\"},\"end\":33596,\"start\":33561},{\"end\":33935,\"start\":33930},{\"attributes\":{\"n\":\"4.3\"},\"end\":36176,\"start\":36160},{\"attributes\":{\"n\":\"4.4\"},\"end\":40721,\"start\":40656},{\"attributes\":{\"n\":\"5\"},\"end\":44028,\"start\":44018},{\"end\":48012,\"start\":47986},{\"end\":49520,\"start\":49510},{\"end\":50207,\"start\":50197},{\"end\":50550,\"start\":50540},{\"end\":50894,\"start\":50884},{\"end\":51183,\"start\":51153},{\"end\":52842,\"start\":52832},{\"end\":53783,\"start\":53773},{\"end\":54300,\"start\":54290},{\"end\":54438,\"start\":54416},{\"end\":55708,\"start\":55697},{\"end\":56117,\"start\":56106},{\"end\":56431,\"start\":56420},{\"end\":56840,\"start\":56829},{\"end\":57280,\"start\":57275},{\"end\":58423,\"start\":58401},{\"end\":59094,\"start\":59083},{\"end\":60436,\"start\":60435}]", "table": null, "figure_caption": "[{\"end\":49950,\"start\":49522},{\"end\":50195,\"start\":49953},{\"end\":50538,\"start\":50209},{\"end\":50882,\"start\":50552},{\"end\":51151,\"start\":50896},{\"end\":52754,\"start\":51187},{\"end\":52830,\"start\":52757},{\"end\":53577,\"start\":52844},{\"end\":53771,\"start\":53580},{\"end\":54288,\"start\":53785},{\"end\":54414,\"start\":54302},{\"end\":55695,\"start\":54443},{\"end\":56104,\"start\":55711},{\"end\":56418,\"start\":56120},{\"end\":56827,\"start\":56434},{\"end\":57273,\"start\":56843},{\"end\":57863,\"start\":57282},{\"end\":58399,\"start\":57866},{\"end\":59081,\"start\":58428},{\"end\":59592,\"start\":59097},{\"end\":59706,\"start\":59595},{\"end\":60433,\"start\":59709},{\"end\":60453,\"start\":60437}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":8848,\"start\":8840},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":9310,\"start\":9302},{\"end\":19614,\"start\":19593},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":19934,\"start\":19926},{\"end\":21106,\"start\":21097},{\"end\":21478,\"start\":21470},{\"end\":25875,\"start\":25867},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":32899,\"start\":32890},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":33295,\"start\":33269},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":34251,\"start\":34241},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":35093,\"start\":35083},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":35889,\"start\":35880},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":38010,\"start\":38001},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":38692,\"start\":38682},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":39247,\"start\":39238},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":41760,\"start\":41751},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":43846,\"start\":43837},{\"end\":46885,\"start\":46876}]", "bib_author_first_name": "[{\"end\":61416,\"start\":61415},{\"end\":61430,\"start\":61424},{\"end\":61451,\"start\":61444},{\"end\":61456,\"start\":61452},{\"end\":61819,\"start\":61818},{\"end\":61843,\"start\":61842},{\"end\":62155,\"start\":62154},{\"end\":62575,\"start\":62569},{\"end\":62589,\"start\":62584},{\"end\":62608,\"start\":62602},{\"end\":62610,\"start\":62609},{\"end\":63048,\"start\":63040},{\"end\":63061,\"start\":63055},{\"end\":63076,\"start\":63070},{\"end\":63079,\"start\":63077},{\"end\":63346,\"start\":63341},{\"end\":63360,\"start\":63353},{\"end\":63370,\"start\":63365},{\"end\":63675,\"start\":63672},{\"end\":63688,\"start\":63681},{\"end\":63981,\"start\":63973},{\"end\":63998,\"start\":63988},{\"end\":64012,\"start\":64006},{\"end\":64015,\"start\":64013},{\"end\":64245,\"start\":64239},{\"end\":64256,\"start\":64250},{\"end\":64273,\"start\":64267},{\"end\":64299,\"start\":64291},{\"end\":64312,\"start\":64305},{\"end\":64333,\"start\":64327},{\"end\":64347,\"start\":64342},{\"end\":64830,\"start\":64824},{\"end\":64844,\"start\":64836},{\"end\":65314,\"start\":65308},{\"end\":65323,\"start\":65320},{\"end\":65335,\"start\":65329},{\"end\":65349,\"start\":65341},{\"end\":65744,\"start\":65738},{\"end\":65760,\"start\":65753},{\"end\":65776,\"start\":65770},{\"end\":65779,\"start\":65777},{\"end\":66000,\"start\":65993},{\"end\":66013,\"start\":66006},{\"end\":66022,\"start\":66019},{\"end\":66033,\"start\":66027},{\"end\":66036,\"start\":66034},{\"end\":66420,\"start\":66413},{\"end\":66434,\"start\":66428},{\"end\":66436,\"start\":66435},{\"end\":66450,\"start\":66445},{\"end\":66474,\"start\":66463},{\"end\":66811,\"start\":66802},{\"end\":66832,\"start\":66828},{\"end\":66844,\"start\":66839},{\"end\":66858,\"start\":66857},{\"end\":66872,\"start\":66867},{\"end\":67271,\"start\":67263},{\"end\":67284,\"start\":67280},{\"end\":67297,\"start\":67291},{\"end\":67307,\"start\":67306},{\"end\":67320,\"start\":67316},{\"end\":67322,\"start\":67321},{\"end\":67338,\"start\":67333},{\"end\":67847,\"start\":67840},{\"end\":67859,\"start\":67857},{\"end\":67870,\"start\":67864},{\"end\":67885,\"start\":67879},{\"end\":67888,\"start\":67886},{\"end\":68214,\"start\":68208},{\"end\":68227,\"start\":68221},{\"end\":68469,\"start\":68464},{\"end\":68478,\"start\":68476},{\"end\":68489,\"start\":68483},{\"end\":68492,\"start\":68490},{\"end\":68514,\"start\":68506},{\"end\":68790,\"start\":68785},{\"end\":68802,\"start\":68797},{\"end\":69078,\"start\":69075},{\"end\":69090,\"start\":69085},{\"end\":69103,\"start\":69097},{\"end\":69106,\"start\":69104},{\"end\":69435,\"start\":69431},{\"end\":69449,\"start\":69442},{\"end\":69454,\"start\":69450},{\"end\":69468,\"start\":69463},{\"end\":69922,\"start\":69916},{\"end\":69934,\"start\":69928},{\"end\":69945,\"start\":69944},{\"end\":70208,\"start\":70202},{\"end\":70657,\"start\":70649},{\"end\":70673,\"start\":70668},{\"end\":71056,\"start\":71052},{\"end\":71068,\"start\":71063},{\"end\":71419,\"start\":71417},{\"end\":71432,\"start\":71424},{\"end\":71444,\"start\":71438},{\"end\":71447,\"start\":71445},{\"end\":71719,\"start\":71716},{\"end\":71731,\"start\":71725},{\"end\":71745,\"start\":71737},{\"end\":72203,\"start\":72198},{\"end\":72215,\"start\":72210},{\"end\":72227,\"start\":72222},{\"end\":72493,\"start\":72485},{\"end\":72516,\"start\":72509},{\"end\":72531,\"start\":72526},{\"end\":72542,\"start\":72539},{\"end\":72556,\"start\":72552},{\"end\":72568,\"start\":72562},{\"end\":72585,\"start\":72580},{\"end\":72951,\"start\":72947},{\"end\":72964,\"start\":72957},{\"end\":72974,\"start\":72971},{\"end\":72988,\"start\":72979},{\"end\":73003,\"start\":72995},{\"end\":73294,\"start\":73289},{\"end\":73312,\"start\":73308},{\"end\":73326,\"start\":73321},{\"end\":73704,\"start\":73698},{\"end\":73718,\"start\":73712},{\"end\":73735,\"start\":73728},{\"end\":74063,\"start\":74056},{\"end\":74072,\"start\":74071},{\"end\":74083,\"start\":74080},{\"end\":74095,\"start\":74088},{\"end\":74105,\"start\":74104},{\"end\":74119,\"start\":74112},{\"end\":74503,\"start\":74496},{\"end\":74515,\"start\":74509},{\"end\":74528,\"start\":74522},{\"end\":74548,\"start\":74541},{\"end\":74561,\"start\":74556},{\"end\":74575,\"start\":74569},{\"end\":74599,\"start\":74592},{\"end\":75046,\"start\":75044},{\"end\":75061,\"start\":75053},{\"end\":75072,\"start\":75069},{\"end\":75349,\"start\":75344},{\"end\":75366,\"start\":75354},{\"end\":75374,\"start\":75371},{\"end\":75658,\"start\":75654},{\"end\":75667,\"start\":75664},{\"end\":75685,\"start\":75673},{\"end\":76000,\"start\":75993},{\"end\":76010,\"start\":76009},{\"end\":76022,\"start\":76019},{\"end\":76039,\"start\":76035},{\"end\":76058,\"start\":76052},{\"end\":76082,\"start\":76075},{\"end\":76097,\"start\":76093},{\"end\":76115,\"start\":76107},{\"end\":76117,\"start\":76116},{\"end\":76134,\"start\":76131},{\"end\":76508,\"start\":76507},{\"end\":76524,\"start\":76519},{\"end\":76769,\"start\":76765},{\"end\":76785,\"start\":76784},{\"end\":76787,\"start\":76786},{\"end\":76806,\"start\":76801},{\"end\":76820,\"start\":76817},{\"end\":77183,\"start\":77182},{\"end\":77185,\"start\":77184},{\"end\":77422,\"start\":77417},{\"end\":77445,\"start\":77440},{\"end\":77720,\"start\":77714},{\"end\":77735,\"start\":77729},{\"end\":78230,\"start\":78225},{\"end\":78238,\"start\":78237},{\"end\":78368,\"start\":78367},{\"end\":78384,\"start\":78376},{\"end\":78400,\"start\":78395},{\"end\":78402,\"start\":78401},{\"end\":78561,\"start\":78560},{\"end\":78576,\"start\":78570},{\"end\":79089,\"start\":79082}]", "bib_author_last_name": "[{\"end\":61413,\"start\":61394},{\"end\":61422,\"start\":61417},{\"end\":61442,\"start\":61431},{\"end\":61474,\"start\":61457},{\"end\":61483,\"start\":61476},{\"end\":61828,\"start\":61820},{\"end\":61840,\"start\":61830},{\"end\":61848,\"start\":61844},{\"end\":61855,\"start\":61850},{\"end\":62161,\"start\":62156},{\"end\":62170,\"start\":62163},{\"end\":62582,\"start\":62576},{\"end\":62600,\"start\":62590},{\"end\":62622,\"start\":62611},{\"end\":63053,\"start\":63049},{\"end\":63068,\"start\":63062},{\"end\":63091,\"start\":63080},{\"end\":63351,\"start\":63347},{\"end\":63363,\"start\":63361},{\"end\":63381,\"start\":63371},{\"end\":63679,\"start\":63676},{\"end\":63693,\"start\":63689},{\"end\":63986,\"start\":63982},{\"end\":64004,\"start\":63999},{\"end\":64027,\"start\":64016},{\"end\":64248,\"start\":64246},{\"end\":64265,\"start\":64257},{\"end\":64289,\"start\":64274},{\"end\":64303,\"start\":64300},{\"end\":64325,\"start\":64313},{\"end\":64340,\"start\":64334},{\"end\":64358,\"start\":64348},{\"end\":64834,\"start\":64831},{\"end\":64852,\"start\":64845},{\"end\":65318,\"start\":65315},{\"end\":65327,\"start\":65324},{\"end\":65339,\"start\":65336},{\"end\":65354,\"start\":65350},{\"end\":65751,\"start\":65745},{\"end\":65768,\"start\":65761},{\"end\":65791,\"start\":65780},{\"end\":66004,\"start\":66001},{\"end\":66017,\"start\":66014},{\"end\":66025,\"start\":66023},{\"end\":66048,\"start\":66037},{\"end\":66426,\"start\":66421},{\"end\":66443,\"start\":66437},{\"end\":66461,\"start\":66451},{\"end\":66486,\"start\":66475},{\"end\":66826,\"start\":66812},{\"end\":66837,\"start\":66833},{\"end\":66855,\"start\":66845},{\"end\":66865,\"start\":66859},{\"end\":66880,\"start\":66873},{\"end\":66886,\"start\":66882},{\"end\":67278,\"start\":67272},{\"end\":67289,\"start\":67285},{\"end\":67304,\"start\":67298},{\"end\":67314,\"start\":67308},{\"end\":67331,\"start\":67323},{\"end\":67344,\"start\":67339},{\"end\":67356,\"start\":67346},{\"end\":67855,\"start\":67848},{\"end\":67862,\"start\":67860},{\"end\":67877,\"start\":67871},{\"end\":67900,\"start\":67889},{\"end\":68219,\"start\":68215},{\"end\":68232,\"start\":68228},{\"end\":68474,\"start\":68470},{\"end\":68481,\"start\":68479},{\"end\":68504,\"start\":68493},{\"end\":68520,\"start\":68515},{\"end\":68795,\"start\":68791},{\"end\":68813,\"start\":68803},{\"end\":69083,\"start\":69079},{\"end\":69095,\"start\":69091},{\"end\":69118,\"start\":69107},{\"end\":69440,\"start\":69436},{\"end\":69461,\"start\":69455},{\"end\":69479,\"start\":69469},{\"end\":69926,\"start\":69923},{\"end\":69942,\"start\":69935},{\"end\":69952,\"start\":69946},{\"end\":70215,\"start\":70209},{\"end\":70666,\"start\":70658},{\"end\":70682,\"start\":70674},{\"end\":71061,\"start\":71057},{\"end\":71079,\"start\":71069},{\"end\":71422,\"start\":71420},{\"end\":71436,\"start\":71433},{\"end\":71459,\"start\":71448},{\"end\":71723,\"start\":71720},{\"end\":71735,\"start\":71732},{\"end\":71750,\"start\":71746},{\"end\":72208,\"start\":72204},{\"end\":72220,\"start\":72216},{\"end\":72238,\"start\":72228},{\"end\":72507,\"start\":72494},{\"end\":72524,\"start\":72517},{\"end\":72537,\"start\":72532},{\"end\":72550,\"start\":72543},{\"end\":72560,\"start\":72557},{\"end\":72578,\"start\":72569},{\"end\":72592,\"start\":72586},{\"end\":72603,\"start\":72594},{\"end\":72955,\"start\":72952},{\"end\":72969,\"start\":72965},{\"end\":72977,\"start\":72975},{\"end\":72993,\"start\":72989},{\"end\":73006,\"start\":73004},{\"end\":73306,\"start\":73295},{\"end\":73319,\"start\":73313},{\"end\":73333,\"start\":73327},{\"end\":73344,\"start\":73335},{\"end\":73710,\"start\":73705},{\"end\":73726,\"start\":73719},{\"end\":73743,\"start\":73736},{\"end\":74069,\"start\":74064},{\"end\":74078,\"start\":74073},{\"end\":74086,\"start\":74084},{\"end\":74098,\"start\":74096},{\"end\":74102,\"start\":74100},{\"end\":74110,\"start\":74106},{\"end\":74133,\"start\":74120},{\"end\":74139,\"start\":74135},{\"end\":74507,\"start\":74504},{\"end\":74520,\"start\":74516},{\"end\":74539,\"start\":74529},{\"end\":74554,\"start\":74549},{\"end\":74567,\"start\":74562},{\"end\":74590,\"start\":74576},{\"end\":74610,\"start\":74600},{\"end\":75051,\"start\":75047},{\"end\":75067,\"start\":75062},{\"end\":75076,\"start\":75073},{\"end\":75352,\"start\":75350},{\"end\":75369,\"start\":75367},{\"end\":75380,\"start\":75375},{\"end\":75662,\"start\":75659},{\"end\":75671,\"start\":75668},{\"end\":75688,\"start\":75686},{\"end\":76007,\"start\":76001},{\"end\":76017,\"start\":76011},{\"end\":76033,\"start\":76023},{\"end\":76050,\"start\":76040},{\"end\":76073,\"start\":76059},{\"end\":76091,\"start\":76083},{\"end\":76105,\"start\":76098},{\"end\":76129,\"start\":76118},{\"end\":76141,\"start\":76135},{\"end\":76145,\"start\":76143},{\"end\":76517,\"start\":76509},{\"end\":76531,\"start\":76525},{\"end\":76535,\"start\":76533},{\"end\":76782,\"start\":76770},{\"end\":76799,\"start\":76788},{\"end\":76815,\"start\":76807},{\"end\":76832,\"start\":76821},{\"end\":76841,\"start\":76834},{\"end\":77191,\"start\":77186},{\"end\":77438,\"start\":77423},{\"end\":77454,\"start\":77446},{\"end\":77464,\"start\":77456},{\"end\":77727,\"start\":77721},{\"end\":77742,\"start\":77736},{\"end\":78235,\"start\":78231},{\"end\":78246,\"start\":78239},{\"end\":78253,\"start\":78248},{\"end\":78374,\"start\":78369},{\"end\":78393,\"start\":78385},{\"end\":78407,\"start\":78403},{\"end\":78418,\"start\":78409},{\"end\":78568,\"start\":78562},{\"end\":78580,\"start\":78577},{\"end\":78590,\"start\":78582},{\"end\":78883,\"start\":78869},{\"end\":79098,\"start\":79090}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":3766791},\"end\":61745,\"start\":61337},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":15491561},\"end\":62025,\"start\":61747},{\"attributes\":{\"id\":\"b2\"},\"end\":62414,\"start\":62027},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":57379996},\"end\":62925,\"start\":62416},{\"attributes\":{\"id\":\"b4\"},\"end\":63268,\"start\":62927},{\"attributes\":{\"doi\":\"arXiv:2007.14527\",\"id\":\"b5\"},\"end\":63568,\"start\":63270},{\"attributes\":{\"doi\":\"arXiv:2006.15733\",\"id\":\"b6\"},\"end\":63891,\"start\":63570},{\"attributes\":{\"doi\":\"arXiv:2010.08019\",\"id\":\"b7\"},\"end\":64237,\"start\":63893},{\"attributes\":{\"doi\":\"arXiv:2010.08895\",\"id\":\"b8\"},\"end\":64695,\"start\":64239},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":58028780},\"end\":65204,\"start\":64697},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":174802569},\"end\":65648,\"start\":65206},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":210985022},\"end\":65991,\"start\":65650},{\"attributes\":{\"doi\":\"arXiv:2003.06496\",\"id\":\"b12\"},\"end\":66411,\"start\":65993},{\"attributes\":{\"doi\":\"arXiv:2009.01658\",\"id\":\"b13\"},\"end\":66735,\"start\":66413},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":211534943},\"end\":67100,\"start\":66737},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":202660812},\"end\":67753,\"start\":67102},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":214208224},\"end\":68142,\"start\":67755},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":211119251},\"end\":68374,\"start\":68144},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":208547648},\"end\":68731,\"start\":68376},{\"attributes\":{\"doi\":\"arXiv:2006.05311\",\"id\":\"b19\"},\"end\":68968,\"start\":68733},{\"attributes\":{\"doi\":\"arXiv:2003.06097\",\"id\":\"b20\"},\"end\":69342,\"start\":68970},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":215768860},\"end\":69837,\"start\":69344},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":46892950},\"end\":70200,\"start\":69839},{\"attributes\":{\"doi\":\"arXiv:1804.07010\",\"id\":\"b23\"},\"end\":70524,\"start\":70202},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":119494967},\"end\":70974,\"start\":70526},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":53249777},\"end\":71278,\"start\":70976},{\"attributes\":{\"doi\":\"arXiv:1910.03193\",\"id\":\"b26\"},\"end\":71714,\"start\":71280},{\"attributes\":{\"doi\":\"arXiv:2004.13145\",\"id\":\"b27\"},\"end\":72110,\"start\":71716},{\"attributes\":{\"doi\":\"arXiv:2001.04536\",\"id\":\"b28\"},\"end\":72442,\"start\":72112},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":53012119},\"end\":72887,\"start\":72444},{\"attributes\":{\"doi\":\"arXiv:1912.01198\",\"id\":\"b30\"},\"end\":73199,\"start\":72889},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":173991140},\"end\":73622,\"start\":73201},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":219661119},\"end\":73997,\"start\":73624},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":135463287},\"end\":74412,\"start\":73999},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":62841516},\"end\":74941,\"start\":74414},{\"attributes\":{\"doi\":\"arXiv:2009.12729\",\"id\":\"b35\"},\"end\":75285,\"start\":74943},{\"attributes\":{\"doi\":\"arXiv:2009.14597\",\"id\":\"b36\"},\"end\":75549,\"start\":75287},{\"attributes\":{\"doi\":\"arXiv:2007.11207\",\"id\":\"b37\"},\"end\":75902,\"start\":75551},{\"attributes\":{\"doi\":\"arXiv:2006.10739\",\"id\":\"b38\"},\"end\":76461,\"start\":75904},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b39\"},\"end\":76679,\"start\":76463},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":522771},\"end\":77115,\"start\":76681},{\"attributes\":{\"id\":\"b41\"},\"end\":77369,\"start\":77117},{\"attributes\":{\"id\":\"b42\"},\"end\":77637,\"start\":77371},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":5575601},\"end\":78152,\"start\":77639},{\"attributes\":{\"id\":\"b44\"},\"end\":78363,\"start\":78154},{\"attributes\":{\"id\":\"b45\"},\"end\":78509,\"start\":78365},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":15061466},\"end\":78768,\"start\":78511},{\"attributes\":{\"id\":\"b47\"},\"end\":79035,\"start\":78770},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":122049161},\"end\":79270,\"start\":79037}]", "bib_title": "[{\"end\":61392,\"start\":61337},{\"end\":61816,\"start\":61747},{\"end\":62152,\"start\":62027},{\"end\":62567,\"start\":62416},{\"end\":64822,\"start\":64697},{\"end\":65306,\"start\":65206},{\"end\":65736,\"start\":65650},{\"end\":66800,\"start\":66737},{\"end\":67261,\"start\":67102},{\"end\":67838,\"start\":67755},{\"end\":68206,\"start\":68144},{\"end\":68462,\"start\":68376},{\"end\":69429,\"start\":69344},{\"end\":69914,\"start\":69839},{\"end\":70647,\"start\":70526},{\"end\":71050,\"start\":70976},{\"end\":72483,\"start\":72444},{\"end\":73287,\"start\":73201},{\"end\":73696,\"start\":73624},{\"end\":74054,\"start\":73999},{\"end\":74494,\"start\":74414},{\"end\":76763,\"start\":76681},{\"end\":77712,\"start\":77639},{\"end\":78558,\"start\":78511},{\"end\":79080,\"start\":79037}]", "bib_author": "[{\"end\":61415,\"start\":61394},{\"end\":61424,\"start\":61415},{\"end\":61444,\"start\":61424},{\"end\":61476,\"start\":61444},{\"end\":61485,\"start\":61476},{\"end\":61830,\"start\":61818},{\"end\":61842,\"start\":61830},{\"end\":61850,\"start\":61842},{\"end\":61857,\"start\":61850},{\"end\":62163,\"start\":62154},{\"end\":62172,\"start\":62163},{\"end\":62584,\"start\":62569},{\"end\":62602,\"start\":62584},{\"end\":62624,\"start\":62602},{\"end\":63055,\"start\":63040},{\"end\":63070,\"start\":63055},{\"end\":63093,\"start\":63070},{\"end\":63353,\"start\":63341},{\"end\":63365,\"start\":63353},{\"end\":63383,\"start\":63365},{\"end\":63681,\"start\":63672},{\"end\":63695,\"start\":63681},{\"end\":63988,\"start\":63973},{\"end\":64006,\"start\":63988},{\"end\":64029,\"start\":64006},{\"end\":64250,\"start\":64239},{\"end\":64267,\"start\":64250},{\"end\":64291,\"start\":64267},{\"end\":64305,\"start\":64291},{\"end\":64327,\"start\":64305},{\"end\":64342,\"start\":64327},{\"end\":64360,\"start\":64342},{\"end\":64836,\"start\":64824},{\"end\":64854,\"start\":64836},{\"end\":65320,\"start\":65308},{\"end\":65329,\"start\":65320},{\"end\":65341,\"start\":65329},{\"end\":65356,\"start\":65341},{\"end\":65753,\"start\":65738},{\"end\":65770,\"start\":65753},{\"end\":65793,\"start\":65770},{\"end\":66006,\"start\":65993},{\"end\":66019,\"start\":66006},{\"end\":66027,\"start\":66019},{\"end\":66050,\"start\":66027},{\"end\":66428,\"start\":66413},{\"end\":66445,\"start\":66428},{\"end\":66463,\"start\":66445},{\"end\":66488,\"start\":66463},{\"end\":66828,\"start\":66802},{\"end\":66839,\"start\":66828},{\"end\":66857,\"start\":66839},{\"end\":66867,\"start\":66857},{\"end\":66882,\"start\":66867},{\"end\":66888,\"start\":66882},{\"end\":67280,\"start\":67263},{\"end\":67291,\"start\":67280},{\"end\":67306,\"start\":67291},{\"end\":67316,\"start\":67306},{\"end\":67333,\"start\":67316},{\"end\":67346,\"start\":67333},{\"end\":67358,\"start\":67346},{\"end\":67857,\"start\":67840},{\"end\":67864,\"start\":67857},{\"end\":67879,\"start\":67864},{\"end\":67902,\"start\":67879},{\"end\":68221,\"start\":68208},{\"end\":68234,\"start\":68221},{\"end\":68476,\"start\":68464},{\"end\":68483,\"start\":68476},{\"end\":68506,\"start\":68483},{\"end\":68522,\"start\":68506},{\"end\":68797,\"start\":68785},{\"end\":68815,\"start\":68797},{\"end\":69085,\"start\":69075},{\"end\":69097,\"start\":69085},{\"end\":69120,\"start\":69097},{\"end\":69442,\"start\":69431},{\"end\":69463,\"start\":69442},{\"end\":69481,\"start\":69463},{\"end\":69928,\"start\":69916},{\"end\":69944,\"start\":69928},{\"end\":69954,\"start\":69944},{\"end\":70217,\"start\":70202},{\"end\":70668,\"start\":70649},{\"end\":70684,\"start\":70668},{\"end\":71063,\"start\":71052},{\"end\":71081,\"start\":71063},{\"end\":71424,\"start\":71417},{\"end\":71438,\"start\":71424},{\"end\":71461,\"start\":71438},{\"end\":71725,\"start\":71716},{\"end\":71737,\"start\":71725},{\"end\":71752,\"start\":71737},{\"end\":72210,\"start\":72198},{\"end\":72222,\"start\":72210},{\"end\":72240,\"start\":72222},{\"end\":72509,\"start\":72485},{\"end\":72526,\"start\":72509},{\"end\":72539,\"start\":72526},{\"end\":72552,\"start\":72539},{\"end\":72562,\"start\":72552},{\"end\":72580,\"start\":72562},{\"end\":72594,\"start\":72580},{\"end\":72605,\"start\":72594},{\"end\":72957,\"start\":72947},{\"end\":72971,\"start\":72957},{\"end\":72979,\"start\":72971},{\"end\":72995,\"start\":72979},{\"end\":73008,\"start\":72995},{\"end\":73308,\"start\":73289},{\"end\":73321,\"start\":73308},{\"end\":73335,\"start\":73321},{\"end\":73346,\"start\":73335},{\"end\":73712,\"start\":73698},{\"end\":73728,\"start\":73712},{\"end\":73745,\"start\":73728},{\"end\":74071,\"start\":74056},{\"end\":74080,\"start\":74071},{\"end\":74088,\"start\":74080},{\"end\":74100,\"start\":74088},{\"end\":74104,\"start\":74100},{\"end\":74112,\"start\":74104},{\"end\":74135,\"start\":74112},{\"end\":74141,\"start\":74135},{\"end\":74509,\"start\":74496},{\"end\":74522,\"start\":74509},{\"end\":74541,\"start\":74522},{\"end\":74556,\"start\":74541},{\"end\":74569,\"start\":74556},{\"end\":74592,\"start\":74569},{\"end\":74612,\"start\":74592},{\"end\":75053,\"start\":75044},{\"end\":75069,\"start\":75053},{\"end\":75078,\"start\":75069},{\"end\":75354,\"start\":75344},{\"end\":75371,\"start\":75354},{\"end\":75382,\"start\":75371},{\"end\":75664,\"start\":75654},{\"end\":75673,\"start\":75664},{\"end\":75690,\"start\":75673},{\"end\":76009,\"start\":75993},{\"end\":76019,\"start\":76009},{\"end\":76035,\"start\":76019},{\"end\":76052,\"start\":76035},{\"end\":76075,\"start\":76052},{\"end\":76093,\"start\":76075},{\"end\":76107,\"start\":76093},{\"end\":76131,\"start\":76107},{\"end\":76143,\"start\":76131},{\"end\":76147,\"start\":76143},{\"end\":76519,\"start\":76507},{\"end\":76533,\"start\":76519},{\"end\":76537,\"start\":76533},{\"end\":76784,\"start\":76765},{\"end\":76801,\"start\":76784},{\"end\":76817,\"start\":76801},{\"end\":76834,\"start\":76817},{\"end\":76843,\"start\":76834},{\"end\":77193,\"start\":77182},{\"end\":77440,\"start\":77417},{\"end\":77456,\"start\":77440},{\"end\":77466,\"start\":77456},{\"end\":77729,\"start\":77714},{\"end\":77744,\"start\":77729},{\"end\":78237,\"start\":78225},{\"end\":78248,\"start\":78237},{\"end\":78255,\"start\":78248},{\"end\":78376,\"start\":78367},{\"end\":78395,\"start\":78376},{\"end\":78409,\"start\":78395},{\"end\":78420,\"start\":78409},{\"end\":78570,\"start\":78560},{\"end\":78582,\"start\":78570},{\"end\":78592,\"start\":78582},{\"end\":78885,\"start\":78869},{\"end\":79100,\"start\":79082}]", "bib_venue": "[{\"end\":61525,\"start\":61485},{\"end\":61870,\"start\":61857},{\"end\":62208,\"start\":62172},{\"end\":62656,\"start\":62624},{\"end\":63038,\"start\":62927},{\"end\":63339,\"start\":63270},{\"end\":63670,\"start\":63570},{\"end\":63971,\"start\":63893},{\"end\":64445,\"start\":64376},{\"end\":64886,\"start\":64854},{\"end\":65409,\"start\":65356},{\"end\":65800,\"start\":65793},{\"end\":66179,\"start\":66066},{\"end\":66552,\"start\":66504},{\"end\":66908,\"start\":66888},{\"end\":67411,\"start\":67358},{\"end\":67928,\"start\":67902},{\"end\":68245,\"start\":68234},{\"end\":68536,\"start\":68522},{\"end\":68783,\"start\":68733},{\"end\":69073,\"start\":68970},{\"end\":69564,\"start\":69481},{\"end\":70001,\"start\":69954},{\"end\":70342,\"start\":70233},{\"end\":70718,\"start\":70684},{\"end\":71113,\"start\":71081},{\"end\":71415,\"start\":71280},{\"end\":71891,\"start\":71768},{\"end\":72196,\"start\":72112},{\"end\":72649,\"start\":72605},{\"end\":72945,\"start\":72889},{\"end\":73395,\"start\":73346},{\"end\":73794,\"start\":73745},{\"end\":74190,\"start\":74141},{\"end\":74661,\"start\":74612},{\"end\":75042,\"start\":74943},{\"end\":75342,\"start\":75287},{\"end\":75652,\"start\":75551},{\"end\":75991,\"start\":75904},{\"end\":76505,\"start\":76463},{\"end\":76882,\"start\":76843},{\"end\":77180,\"start\":77117},{\"end\":77415,\"start\":77371},{\"end\":77840,\"start\":77744},{\"end\":78223,\"start\":78154},{\"end\":78624,\"start\":78592},{\"end\":78867,\"start\":78770},{\"end\":79142,\"start\":79100},{\"end\":77923,\"start\":77842}]"}}}, "year": 2023, "month": 12, "day": 17}