{"id": 246838963, "updated": "2023-10-05 16:41:45.732", "metadata": {"title": "Learn to Predict How Humans Manipulate Large-sized Objects from Interactive Motions", "authors": "[{\"first\":\"Weilin\",\"last\":\"Wan\",\"middle\":[]},{\"first\":\"Lei\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Lingjie\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Zhuoying\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Ruixing\",\"last\":\"Jia\",\"middle\":[]},{\"first\":\"Yi-King\",\"last\":\"Choi\",\"middle\":[]},{\"first\":\"Jia\",\"last\":\"Pan\",\"middle\":[]},{\"first\":\"Christian\",\"last\":\"Theobalt\",\"middle\":[]},{\"first\":\"Taku\",\"last\":\"Komura\",\"middle\":[]},{\"first\":\"Wenping\",\"last\":\"Wang\",\"middle\":[]}]", "venue": "IEEE Robotics and Automation Letters ( Volume: 7, Issue: 2, April 2022)", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Understanding human intentions during interactions has been a long-lasting theme, that has applications in human-robot interaction, virtual reality and surveillance. In this study, we focus on full-body human interactions with large-sized daily objects and aim to predict the future states of objects and humans given a sequential observation of human-object interaction. As there is no such dataset dedicated to full-body human interactions with large-sized daily objects, we collected a large-scale dataset containing thousands of interactions for training and evaluation purposes. We also observe that an object's intrinsic physical properties are useful for the object motion prediction, and thus design a set of object dynamic descriptors to encode such intrinsic properties. We treat the object dynamic descriptors as a new modality and propose a graph neural network, HO-GCN, to fuse motion data and dynamic descriptors for the prediction task. We show the proposed network that consumes dynamic descriptors can achieve state-of-the-art prediction results and help the network better generalize to unseen objects. We also demonstrate the predicted results are useful for human-robot collaborations.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2206.12612", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2206-12612", "doi": "10.1109/lra.2022.3151614"}}, "content": {"source": {"pdf_hash": "64e0a8326b1d319935a3b9dd84f958a0e9d29b55", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2206.12612v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "2c10348a52ffab9b23e1f5f4785f748d0fc1d2df", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/64e0a8326b1d319935a3b9dd84f958a0e9d29b55.txt", "contents": "\nLearn to Predict How Humans Manipulate Large-sized Objects from Interactive Motions\n\n\nWeilin Wan \nLei Yang \nLingjie Liu \nZhuoying Zhang \nRuixing Jia \nYi-King Choi \nJia Pan \nChristian Theobalt \nTaku Komura \nWenping Wang \nLearn to Predict How Humans Manipulate Large-sized Objects from Interactive Motions\n1Index Terms-Intention RecognitionHuman-Robot Collabo- rationDatasets for Human Motion\nUnderstanding human intentions during interactions has been a long-lasting theme, that has applications in human-robot interaction, virtual reality and surveillance. In this study, we focus on full-body human interactions with large-sized daily objects and aim to predict the future states of objects and humans given a sequential observation of human-object interaction. As there is no such dataset dedicated to full-body human interactions with large-sized daily objects, we collected a largescale dataset containing thousands of interactions for training and evaluation purposes. We also observe that an object's intrinsic physical properties are useful for the object motion prediction, and thus design a set of object dynamic descriptors to encode such intrinsic properties. We treat the object dynamic descriptors as a new modality and propose a graph neural network, HO-GCN, to fuse motion data and dynamic descriptors for the prediction task. We show the proposed network that consumes dynamic descriptors can achieve state-of-the-art prediction results and help the network better generalize to unseen objects. We also demonstrate the predicted results are useful for human-robot collaborations.\n Fig. 1\n. We introduce a large-scale dataset on full-body human interactions with large-sized objects, such as chairs and boxes, using a motion capture system. Our goal is to predict the human-object motion at future time steps (where predicted human and object are in purple and red, respectively). To this end, we propose to leverage the object dynamic descriptors and design a neural network, HO-GCN, to fuse data of different modalities. We also showcase a human-robot collaborative task to validate the proposed method. object geometries, the complex physics involved, and a large amount of training needed [11], [2], [14], [10]. Ehsani et al. [10] propose to integrate physics into a neural network and leverage it for object motion prediction. Nevertheless, the exact geometry of the object is required for the neural framework. They also require conducting physical simulation within the training loop, which is expensive and time-consuming for good generalization.\n\nWe make two efforts to address the aforementioned challenges. First, in response to the lack of datasets concerning interactions between humans and large-sized objects, we construct such a dataset. It contains 384K frames from 508 full-body motion capture (MoCap) videos, involving 12 daily objects from 6 categories and different actions. Not only included are the human body motions but also the 6 degreesof-freedom (DOF) motions of the objects that are represented by 12 keypoints. The interaction take place in diverse contexts, such as transporting the objects forward or backward, rotating the objects, or more complex compositional motions (e.g., lift and carry forward), depending on the affordances and functionality of the object. The data were fully preprocessed and will be available to the community upon the publication of the paper. To the best of our knowledge, this is the first largescale dataset focusing full-body interactions with large-sized objects.\n\nTo avoid frequent calls for physical simulators during training, we propose a novel descriptor that encodes the object's intrinsic dynamics obtained from simulations. In particular, a conceptual model for each category of objects (e.g., chairs) is constructed, which is represented by a set of keypoints abstracting the general shape of this object category. Then, we simulate its responses under forces as a rigid-body system and acquire the intrinsic dynamic properties. Thus, the object dynamic descriptors are defined as how the keypoint-based conceptual model is transformed under given forces. As shown in our experiments, our method using the object dynamic descriptors can achieve state-of-the-art performances regarding the prediction of object motions even when the object instance is unseen in the training set. For human-robot collaborative tasks like lifting a box or handing over an object, this is crucial as reliable prediction of the object motions can enable the robot to provide desirable assistive operations to the human partners.\n\nWe also design a novel graph convolutional neural network (HO-GCN) that takes as input the human-object interactive motions as well as the object dynamic descriptors as shown in Fig. 1. We adopt the spatial-temporal graph convolution proposed in [37] to learn motion features and predict the future interactive motions based on the input sequence. We evaluate the proposed method as well as several state-of-theart methods on our collected dataset. We also showcase that the predicted interactive motion can enable robot assistance in labor-intensive tasks, such as transporting a box.\n\nOur technical contributions are threefold: 1) We contribute the first large-scale dataset concerning human full-body interactions with large-sized daily objects; 2) We propose to consider the object's intrinsic dynamics as an extra modality for enhancing prediction of the object future poses during human-object interactions; 3) We design a novel graph convolutional neural network that fuses the observed human-object interaction sequence and the object intrinsic dynamics for the prediction task.\n\n\nII. RELATED WORK\n\n\nA. Learning from human skeleton data\n\nRecognizing human activities from skeletal data receives increasing research attention as the acquisition of human skeletons becomes easier, such as employing motion capture techniques. With the development of deep learning, many attempts are made to learn features from the temporal sequences of the human skeleton data for action recognition or motion prediction. These works can be roughly categorized into RNN/LSTM-based methods which recursively process the temporal information [31], [38], [21], [6], and CNN-based methods which maps the temporal information into hidden features using convolution [33], [9], [22], [8], [3].\n\nSince a human skeleton is naturally a graph structure, Graph Convolutional Networks (GCNs) are proposed for processing human skeleton data as well. Yan et al. [37] propose spatialtemporal GCN to efficiently extract the spatial and temporal features from the skeleton inputs for action recognition. Li et al. [23] extend the human skeleton graph in GCN with extra links to capture more dependencies and explore significant features of movements. Even GCNs have shown to be effective in representing human skeletons as graphs, only a few attempts have been made to couple human skeletons and objects in GCNs. Kim et al. [16] incorporate a single point containing objects' positional information to the human skeleton graph for classifying human actions. Cui et al. [7] propose to learn additional connectivity among joints besides their natural linkages for motion prediction.\n\nIn our work, we design HO-GCN, a novel architecture that fuses object information and human skeleton information to predict the 6DOF motion of the object. We compare our proposed method with C-TE [3] and CAHMP [6]. The former is a convolution-based method using a mirrored encoderdecoder framework to process the temporal inputs and generate predicted skeleton motion sequences. The latter is a recently proposed RNN-based method for predicting interactive motions between humans and the static environments or smaller objects.\n\n\nB. Human-object interaction dataset\n\nExisting 3D human datasets are mainly designed for action recognition or human motion prediction tasks [13], [25], [29], [36], [28], which contain a limited number of human-object interaction cases and do not provide 3D ground truth position of objects. There have also been works focusing on handobject interactions. Although these datasets usually provide sufficient spatial information of the objects, they focus on hand-held objects that do not involve full-body movements in the sequences. Ehsani et al. [10] propose a dataset with 174 RGB hand-object interaction videos and 6D object pose annotation. Grasping Actions with Body (GRAB) [32] is a large-scale dataset focusing on human grasping actions, which presents 1334 videos with body information and object 3D models. First-Person Hand Action (FPHA) [12] provides 1175 RGB videos with hand positions and 6D object poses annotated.\n\nWhole-Body Human Motion Database [27], [26] includes a number of samples including human interactions with the scene context involving tables, cups, ladders, etc. However, the number of direct human operations on these large objects (e.g., the table) is limited. This motivates us to propose a dataset for understanding full-body interactions with large-sized daily objects such as tables, chairs, and boxes. In these scenarios, human poses need to be adapted to the object properties, such as shape or center of mass, making it different from previous works that only model interactions with small or thin objects (e.g., golf clubs in [24]). We also anticipate increasing attention from the community to use such a dataset for learning a prior of human-object joint motions or predicting motions for human-robot interactions.\n\n\nC. Predicting human intention in human-robot cooperation\n\nA bulk of literature has contributed to predicting human intentions or motions for efficient and safe human-robot collaborations. An early work [30] proposes a method based on the Gaussian mixed model to learn from human demonstrations and thus adjust the robot's role in the human-robot table lifting task. To predict intentions, Hidden Markov Models are widely used (e.g., [34], [20] for recognizing human movements or interaction selection). Zhao et al. [40] proposed a recurrent network for imitation learning to accomplish object handover tasks between a human and a robot. In this paper, we focus on predicting interactive motions as a human manipulating a large-sized object, and showcase that the output of the proposed prediction method can be leveraged for human-robot collaboration.\n\n\nIII. METHODOLOGY\n\n\nA. Notations and problem formulation\n\nThe inputs to our prediction task are (1) a sequence of 3D human skeletons {X t } K t=0 , (2) a sequence of 3D positional information of the object keypoints {P t } K t=0 , and (3) the object dynamic descriptors O. The expected outputs are the 6DoF pose changes of the object {\u2206 t } T t=K and the human skeleton\nmotion {X t } T t=K .\nIn what follows, we first detail the definition and acquisition of the object dynamic descriptors O defined for objects and then present the network design.\n\n\nB. Object representation and dynamic descriptors\n\nGeometric representation. As many objects from the same class share high similarity in the overall geometry, we assume that objects from the same class can be abstracted by the same geometric conceptual model, and share common object dynamics. The conceptualized model is defined as a set of sparse keypoints that delineate the geometry of the object,\nP = {p m \u2208 R 3 } M m=1 .\nThe conceptualized models of all object categories from the dataset are shown in the bottom row of Fig. 2. The object dynamic descriptors are defined on the set of the keypoints of the conceptual model. We assume, for convenience, all conceptual models have the same number of keypoints and thus the same dimensionality of the dynamic descriptors. This allows training on all objects we collected in our dataset.\n\nObject dynamic descriptors. Object dynamics are a set of intrinsic physical properties that reflect the resulting object motions caused by external forces. In this study, we use \u2206 = g(F ) to describe how the given unit force F will cause the pose change \u2206 of the (conceptual) object. To reduce the sampling space for the shape and the force, we consider the object as a rigid-body system. Thus, we can perform physical simulations to obtain the relationship between object motions and the forces. Since forces form a vector space and are linear to their resultant motion, a set of suitable bases of the force space shall suffice to describe this intrinsic dynamic relationship.\n\nWe define a set of unit forces along a candidate set D of discrete directions, i.e. forward, backward, left, right and up, in the local frame of the object. A force applied to keypoint p m is denoted F j m where j indicates a direction from the candidate set. Then, we collect the resulting pose changes \u2206 j m subject to the applied force F j m from simulations to describe the object dynamics. During each simulation, the force is applied to the object within an infinitesimal time interval and the object is modeled as a rigid-body system with a pre-defined floor constraint and the gravity. Finally, the proposed dynamic descriptor for an object is obtained as U = {\u2206 j m |m = 1...M, j \u2208 D} so that it can be fed to common neural networks that consume this semi-structured, discrete information as input.\n\nC. Network structure To achieve the described task, we propose a novel humanobject graph convolutional neural network (HO-GCN) which consists of five major branches as shown in Figure 3. The human branch (top left) is designed for local feature learning from the spatial-temporal graph H. The spatial dimension of graph H is a human skeleton that has N nodes representing human body parts. The temporal dimension of the graph is formed by connecting each node of a human skeleton to its counterpart in temporally adjacent skeletons. Specifically, this branch processes human skeleton data by applying to H the spatial-temporal graph convolutions (denoted STGConv) using the distance partitioning strategy [37].\n\nThe object pre-processing branch aims to fuse the dynamic descriptors and the object keypoints. Specifically, we represent an object as two parts: 1) a D-dim vector U that represents the dynamic descriptors and 2) 3D positional differences between each object keypoint and both hands in the corresponding input frames. For better mining the relations among keypoint positions and descriptors, we broadcast and concatenate the keypoint coordinates with the dynamic descriptors and further process them using a convolution-based object encoder to obtain the object motion features.\n\nThe central human-object fusion branch then takes the concatenation of human and object motion features from previous branches, and applies spatial-temporal graph convolutions to them based on graph H+O. In this graph structure, we connect the object keypoints to three joints: left and right hands as well as the hip of a human body, as the movements of the hand and hip joints provides prominent hints related to human-object contact and the proximity between the human and the object for predicting the human's intention. This branch then outputs two tensors for predicting the human and object motions, respectively.\n\nFor predicting the 6DoF pose changes of the object in the future frames, we feed the output tensor with D channels from the central H + O branch to the weighted pooling module (as shown in Figure 3). This module is designed for scaling the dynamic descriptors, which are derived by applying unit forces to objects in a simulator. It maps the input tensor to the weighting factors which are then multiplied with the dynamic descriptor vector.\n\nWe also employ a spatial-temporal fusion module (ST Fusion) to recombine the respective information in spatial and temporal dimensions. This module yields an ST-fused feature containing information about the human-object interaction and is concatenated with the weighted dynamic descriptor vector. The concatenated features are then fed into a fully-connected (FC) layer for 6DoF regression. We additionally require the FC layer to regress a probability score c indicating if the object is in motion (c = 1) and vice versa (c = 0).\n\nThe human motion prediction branch performs a series of spatial-temporal graph convolutions based on graph H +O and eventually reduces the channel size of the graph convolution tensor to the number of K output frames. Finally, each of the K channels of the tensor is transformed (via a FC layer) to generate K human poses for K output frames, respectively.\n\nLoss function. The loss function is formulated as:\nL = \u03bb 1 ( t c t \u2212 c t 2 + \u2206 t \u2212 \u2206 t 2 + pi\u2208Pt p i \u2212 p i 2 )(1)+ \u03bb 2 xi\u2208Xt x i \u2212 x i 2 ,\nwhere notations with a hat, e.g., x, denote the predicted value, and those without a hat stand for the ground-truth; c = {1, 0} indicates the object in motion or not; \u2206 is the pose change; p i  and x i are the object keypoints and human body joints. We set the balance coefficients \u03bb i={1,2} to 1.0 and 0.5, respectively, for training our network.\n\n\nIV. HUMAN-OBJECT INTERACTION DATASET\n\nData collection. We collected a dataset of 508 humanobject interaction videos, 384K frames in total recorded by OptiTrack [1], a motion capture system. The average length of the recorded motions in our dataset is 6.3s with a standard deviation of 1.4s. Observing how humans may interact the large-size objects in the daily life, we design a set of interactive actions between humans and objects; see Tab. I. Six actors of different shapes, including two females and four males, participated in data collection process. Fig. 2 shows the 12 large-size objects frequently seen in daily living, i.e., four chairs, a standing board, a tripod, three boxes of different sizes, two tables and a basket. Each of the objects is represented by a geometric abstraction of 12 keypoints delineating its shape, also shown in Fig. 2. We found 12 keypoints are sufficient to describe the geometry of the objects without too much computational overhead or memory consumption. We follow the keypoint configuration of the conceptual model to attach twelve reflective markers on the real-world objects for tracking their rigid motions.\n\nData processing. We used the OptiTrack motion capture system (at a frame-rate of 120 Hz) to track the human-object interactions. Each MoCap video recorded a complete process of an actor performing given actions and captured the motion data of the human body skeleton and the pose of the target object at each frame.\n\nWe use a sliding window of 240 frames and a step size of 12 frames to extract sequences from a given MoCap video. Thus, each extracted sequence has 20 frames equivalent to a 2-sec motion. We use the first ten frames (1 sec) as input and predict human-object interaction in the last ten frames (the other 1 sec). We always ensure that the object at the K-th frame (K = 10) in the input sequence to have a small motion within a threshold. For prediction, we label each frame in the extracted sequence with the 6DOF pose change \u2206 gt of the object between two frames:\n\u2206 gt (\u03b4) = P (K + \u03b4) \u2212 P (K),(2)\nwhere P (t) is the recorded 6DoF pose of the object at frame t. If ||\u2206 gt (\u03b4)|| = 0, we assign frame K +\u03b4 with the stationary label (c = 0) to indicate that the human is yet to move the object; otherwise c = 1. Data split. To test the model generalizability to unseen instances, we reserved all 471 samples of Chairs 3 and 4 (see Fig. 2) to form a unseen instance test set. We also randomly split the rest of the collected data (totalling 17,872 samples) into a training set of 12908 samples, a validation set of 3227 samples, and a seen instance test set of 1747 samples. Each object (except Chairs 3 and 4) and each action type appear in all three splits.\n\n\nV. EXPERIMENTS\n\nComparative evaluation is conducted to validate our network design and the use of object dynamic descriptors. We also examine the model generalizability to unseen objects from the six categories.\n\nTraining. We used the Adam optimizer [17] with a learning rate of 0.001 for training our network. A mini-batch of 32 samples was fed to the network during training. We trained the neural network models until they converged and showed the best performance on the validation set. All models were trained in a category-agnostic manner across all training data. Training and evaluation were conducted on a machine running Ubuntu 18.04 with a 1.80GHz Intel Xeon Silver 4108 CPU, and an NVIDIA RTX 2080Ti GPU.\n\nEvaluation metrics. Given the ground-truth (GT) 6D pose changes of an object and the predictions produced by the methods, we measured the errors in pose translation (mm) and pose rotation (10 \u22123 rad). We also adopted the mean per joint position error (mm) with respect to the human skeleton (MPJPE-H) and the object keypoints (MPJPE-O).\n\nAblation setting. To justify our design choices, we also evaluate the performance of our network without using the proposed object dynamic descriptors (denoted Ours w/o Desc). Specifically, we remove the object dynamic descriptors from the input to the object pre-processing branch and feed only the keypoint information to the weighted pooling block in Figure 3. The channel size of the FC layer predicting the object motion is adapted accordingly as well.\n\nWe further remove all the object information (both its dynamic descriptors and the geometry) and use solely the human skeleton motion in the first 10 frames to predict both the human and object motions in the last 10 frames, which we denoted as BaseGCN. In this setting, the input object representation, object encoder, and the weighted pooling block are deducted from our network.\n\nComparison to SOTA methods. We compare our model against two state-of-the-art (SOTA) methods (C-TE [3] and CAHMP [6]). All of the methods are given a 1-second observation (10 frames) of the human-object interaction, and required to predict the future 1-second (10 frames) joint motion of the human and the object. We are specifically interested in the prediction of object pose changes as accurate prediction can lead to desirable robot assistive operation in human-robot collaboration tasks.\n\n\nA. Quantitative analysis\n\nFirstly, we evaluate our proposed method (HO-GCN), its variants (Ours w/o Desc and BaseGCN), and the comparing methods (C-TE and CAHMP) on the test set containing seen objects. Different methods are compared in terms of each future frame, short-term (0.1\u22120.5s) and long-term (0.6\u22121.0s) in Tab. II.\n\nComparisons show that our method consistently outperforms C-TE [3] which uses a convolution-based encoderdecoder structure for human-object joint motion prediction by a large margin. While the comparison with CAHMP shows it performs slightly better than our method in terms of human motion prediction, our method achieves a marginal improvement over CAHMP on the MPJPE-O metric in both short term and long term.\n\nSecondly, we also test the different models on two unseen objects. In particular, Chair 3 (see Fig. 2) is an armchair, which is largely different from the other chairs in appearance and geometry. Note that the same keypoint configuration is used as shown in Fig. 2 for motion capture. Even without attaching markers to the armrests, we can achieve satisfactory results (see Fig. 5), showing that the abstracted geometry is sufficient for predicting the motion of a rigid body in our framework, justifying the use of 12 keypoints for all objects.\n\nTab. III shows the averaged performances over the 10 future frames. We can see that generalizing the trained models to unseen instances is challenging; compared to their performances in the all-seen setting, all models obtained worse results. Under this condition, our HO-GCN outperforms its variant (Ours w/o Desc and BaseGCN) by a relatively large margin on both chairs, showing the efficacy of the object dynamic descriptors for model generalization.\n\nWhen comparing to CAHMP [6], we see that our method consistently achieves better results on different unseen chairs regarding the object motion prediction, showing the feasibility of applying our method to human-robot collaborative tasks, while the gap between Ours and CAHMP regarding the human motion prediction is narrowed. Both our method based on GCN and CAHMP based on RNN gain large improvement when compared to C-TE [3].\n\nWill the object dynamic descriptors benefit the task? Comparing our method to its variant (Ours w/o Desc) for ablation, we observe consistent performance gains regarding MPJPE-O, the translation and rotation errors in both short/long-term settings. From Tab. II, the gains for the Seen Objects test set are observed substantial in the short-term phase, where the motion state of the object is changing as it was just moved by the human actor. Consistent gains are observed in Tab. III for Unseen Objects test set too. Such reliable prediction is crucial for predicting the intents of how the human actor is going to manipulate the objects, and supports the use of our object dynamic descriptors for describing the intrinsic physical properties of an object.\n\nCan human skeleton data alone be used to predict the joint motion? We also compare the variant of our method (Ours w/o Desc) to the baseline model (BaseGCN) that uses only the human skeleton data to predict the human-object joint motion. We see a small improvement obtained by our variant in the long-term. This may be attributed to that in the longterm the object motion is relatively large, and thus the benefit of using the object information (in this case the object keypoint positions) is more obvious.\n\n\nB. Qualitative analysis\n\nWe visualized some randomly sampled results for qualitative analysis and comparison. A collection of qualitative results are shown in Fig. 4. Objects are displayed on the left. Results of the human and the object predicted by our HO-GCN are shown in purple and red, respectively. We visualize three frames of the input sequence and three prediction frames. In this setting, the prediction results are quite close to the GT. We show a failure case (observed across all methods) at the bottom right where the actor was lifting the small box (Object ID 7) and a large difference between GT and the prediction is seen. We assume that because the dataset mainly contains large-size objects such as chairs and tables, the trained model fails to generalize well to this smaller box, while producing a satisfactory result for the medium-sized box (Object ID 8) for the push forward action. Fig. 5 shows the qualitative comparison of different methods. Two input motion sequences, Lift Box 8 and Rotate Chair 4, are shown at the top. The former sequence was sampled from the all-seen test set while the latter from the unseen. We show two predicted frames at t = 0.5s and 1.0s corresponding to the short-term and long-term results. All methods produce reasonable results regarding the Lift Box 8 sequence. However, C-TE and one of our variants (Our w/o Desc) seem to predict motions faster than the ground-truth. For Rotate Chair 4, only our method produces a plausible motion of rotating the chair. CAHMP, on the other hand, predicts a rough translation without rotating the chair, failing to capture the intention as the other methods do.\n\n\nC. Human-robot collaborative tasks\n\nWe also showcase that the proposed HO-GCN can be useful for human-robot collaborative tasks. Specifically, we are interested in whether HO-GCN can successfully predict the manipulation intent of the human actor in terms of the future motion of the target object. We thus conducted experiments with a UR5 robot arm and a vacuum gripper with four silicone  suction cups (of diameter 40mm). We chose a box which is suitable in weight and size for demonstration. The end-effector of the robot arm was pre-aligned to the target surface of the box. We used the OptiTrack system to provide 3D human motion information as well as the starting poses of the object.\n\nWith an input sequence of a human manipulating the box, our proposed method can predict the potential movement of the object in terms of its 6DoF changes. We then converted it to the corresponding robot trajectory that best produces the predicted object movement. In this showcase, we didn't consider the leading/following roles of the human and the robot in the cooperative task which could be our future work.\n\nOur HO-GCN ran at a real-time rate (42 FPS) for processing such a 10-frame sequence on a desktop running Ubuntu 16.04 with an Intel Core i7-9700K CPU and an NVIDIA RTX 2080 GPU. Some results are shown in Fig. 6 and more can be found in our supplemental video. VI. CONCLUSIONS In this paper, we focus on predicting full-body human interactions with large-sized daily objects and contribute to a large-scale dataset. Given as input a sequential observation of the human-object interaction, we design a novel graph convolutional network for predicting the future interactive motion. We also show that the object dynamic descriptors encoding the inherent physical properties of an object are beneficial to our network's generalization to unseen instances during test. Some showcasing examples of human-robot collaboration were presented using the proposed motion prediction method.\n\nCurrently, a naive version of the dynamic descriptor has been investigated; finding a more powerful representation for the inherent dynamic properties of an object is a promising future work. We would like to extend our human-robot collaboration to more objects and realistic settings. This would require extending our current work to predicting human-object interactions from video inputs.\n\nFig. 2 .\n2The object instances in our dataset are shown in the top two rows with Object IDs next to them. The conceptual models used in the simulation are shown in the bottom row. Each conceptual model corresponds to an object class. They are modeled as rigid-body systems, and the drawn linkage is for illustration purposes only.\n\nFig. 3 .\n3The architecture of HO-GCN. The network consists of five major blocks. The two parallel blocks at the left end pre-process the human skeleton motion and the object information, and extract respective motion features. The central block using Graph H+O fuses the extracted features from the human and object, and performs graph convolutions to achieve global understanding of the input sequence. The upper and lower blocks at the right side respectively predict the human skeletons and the 6DoF changes of the objects in the future frames. STGConv and FC refers to spatial-temporal graph convolution and fully-connected layers, respectively. D equals M \u00d7 5 \u00d7 6 in our case, where there are 5 candidates of discrete directions.\n\nFig. 4 .\n4Qualitative results produced by HO-GCN with the object dynamic descriptors are shown. Real-world objects are shown on the left. Results of the human and the object predicted by our HO-GCN are shown in purple and red, respectively, while GT is drawn in blue and green. Three frames of the input sequence and three prediction frames are visualized. A failure case is shown at the bottom right.\n\nFig. 5 .\n5Qualitative comparison of different methods. Lift Box 8 is sampled from the seen object test set while Rotate Chair 4 is from the unseen one. Results of the human and the object predicted by our HO-GCN are shown in purple and red, respectively, while GT's are drawn in blue and green. Our method can well capture the intended motion.\n\nFig. 6 .\n6Experiment results of the collaborative tasks. The human actors were performing lifting, pushing, clockwise rotating, and counter-clockwise rotating from left to right.\n\n\nTranslation Push forward; Pull backward; Move to the left / right (154) Rotation Out-of-plane: Tilt to left / right / front / back sides (90)In-plane: Rotate clockwise / anticlockwise (90) \nCompositional actions \nLift and carry Forward (71) \nLift and place to left/right (60) \nLift and rotate up/down (43) \n\n\n\nTABLE I\nIHUMAN-OBJECT INTERACTIVE MOTIONS IN OUR COLLECTED DATASET. NUMBERS IN PARENTHESIS INDICATE THE NUMBER OF VIDEO SAMPLES.\n\n\nTABLE II COMPARISON OF PERFORMANCES OF DIFFERENT METHODS ON SEEN OBJECTS. THE UPPER BLOCK REPORTS THE PREDICTION ERRORS REGARDING THE 6DOF POSE (I.E., TRANSLATION AND ROTATION). THE BOTTOM BLOCK REPORTS THE MPJPE WITH RESPECT TO THE OBJECT AND THE HUMAN.Short-term Long-term \nMetrics \nMethods \n0.1s \n0.2s \n0.3s \n0.4s \n0.5s \n0.6s \n0.7s \n0.8s \n0.9s \n1.0s \n(0.1s-0.5s) \n(0.6s-1.0s) \nMean \nTrans. Err. \nC-TE \n33.4 40.3 54.1 \n73.1 \n90.3 \n96.9 \n120.0 142.9 168.9 196.8 \n58.2 \n145.1 \n101.7 \nTrans. Err. \nBaseGCN \n34.7 38.6 48.3 \n65.1 \n67.1 \n92.4 \n106.9 145.6 164.6 184.4 \n50.8 \n138.8 \n94.8 \nTrans. Err. Ours w/o Desc \n37.4 43.2 53.8 \n68.0 \n79.0 \n94.0 \n105.9 129.2 137.6 145.9 \n56.3 \n122.5 \n89.4 \nTrans. Err. \nOurs \n22.4 29.0 39.4 \n47.2 \n62.1 \n72.5 \n80.6 \n136.3 148.8 163.1 \n40.0 \n120.3 \n80.2 \nRot. Err. \nC-TE \n46.1 64.9 84.4 111.8 159.7 150.8 210.7 208.1 229.4 251.0 \n93.4 \n210.0 \n151.7 \nRot. Err. \nBaseGCN \n46.5 54.8 64.2 \n85.5 \n115.6 130.4 159.9 190.9 198.4 227.9 \n73.3 \n181.5 \n127.4 \nRot. Err. \nOurs w/o Desc \n52.2 63.1 94.0 108.2 122.6 140.2 154.5 178.3 190.8 202.2 \n88.0 \n173.2 \n130.6 \nRot. Err. \nOurs \n33.7 46.4 60.7 \n82.8 \n86.0 \n100.5 115.7 191.0 204.9 223.8 \n61.9 \n167.2 \n114.6 \n\nMPJPE-O \nC-TE \n36.4 44.7 60.1 \n81.5 \n102.1 107.3 135.3 158.0 183.4 212.4 \n65.0 \n159.3 \n112.1 \nMPJPE-O \nCAHMP \n25.8 42.7 59.5 \n75.9 \n92.2 \n108.3 124.0 139.6 155.3 171.2 \n59.2 \n139.7 \n99.5 \nMPJPE-O \nBaseGCN \n37.5 42.0 53.0 \n71.2 \n76.6 \n101.7 117.8 157.9 175.5 198.3 \n56.1 \n150.2 \n103.2 \nMPJPE-O \nOurs w/o Desc \n42.0 47.1 60.1 \n74.5 \n86.1 \n101.8 114.2 138.0 147.1 156.1 \n62.0 \n131.5 \n96.7 \nMPJPE-O \nOurs \n24.0 31.6 42.5 \n52.5 \n67.2 \n78.3 \n88.0 \n147.2 160.5 175.9 \n43.6 \n130.0 \n86.8 \nMPJPE-H \nC-TE \n25.9 48.1 70.9 \n89.1 \n110.8 124.0 137.7 151.0 166.5 180.9 \n69.0 \n152.0 \n112.1 \nMPJPE-H \nCAHMP \n22.3 42.4 60.0 \n76.0 \n91.1 \n105.7 120.1 134.7 149.7 165.1 \n58.3 \n135.1 \n96.7 \nMPJPE-H \nBaseGCN \n27.8 50.8 73.4 \n92.0 \n108.1 123.2 138.1 143.0 159.0 166.2 \n70.4 \n145.9 \n108.2 \nMPJPE-H \nOurs w/o Desc \n24.0 45.9 65.7 \n82.6 \n97.3 \n110.2 122.5 134.1 144.4 154.5 \n63.1 \n133.2 \n98.2 \nMPJPE-H \nOurs \n23.9 45.8 65.8 \n84.6 \n100.6 115.4 129.4 140.0 151.9 163.6 \n64.1 \n140.1 \n102.1 \n\n\n\n\nTABLE III MODEL GENERALIZATION TEST ON UNSEEN OBJECTS Chair 3 AND Chair 4. SPECIFICALLY, Chair 3 IS AN ARMCHAIR WITH DRASTICALLY DIFFERENT GEOMETRY FROM CHAIRS (1 AND 2) FOR TRAINING.Method \nTran. Err. Rot. Err. MPJPE-O MPJPE-H \nObject \nC-TE \n148.9 \n332.4 \n188.1 \n134.8 \nCAHMP \n-\n-\n168.9 \n124.5 \nBaseGCN \n135.3 \n311.7 \n168.2 \n131.7 \nChair 3 \nOurs w/o Desc \n129.6 \n304.1 \n160.8 \n124.0 \nOurs \n122.1 \n296.5 \n151.0 \n128.9 \nC-TE \n175.7 \n394.3 \n214.8 \n108.1 \nCAHMP \n-\n-\n165.4 \n85.8 \nBaseGCN \n160.4 \n350.2 \n196.2 \n100.9 \nChair 4 \nOurs w/o Desc \n131.0 \n319.2 \n164.1 \n91.7 \nOurs \n124.2 \n310.8 \n156.7 \n89.0 \nC-TE \n160.7 \n359.6 \n199.8 \n123.1 \nCAHMP \n-\n-\n167.4 \n107.5 \nSample \nBaseGCN \n146.4 \n328.6 \n180.5 \n118.2 \nMean \nOurs w/o Desc \n130.2 \n310.8 \n162.2 \n109.8 \nOurs \n123.0 \n302.8 \n153.5 \n111.4 \n\n\n\nOptitrack motion tracking system. Available. \"Optitrack motion tracking system [online],\" in Available: http://www.naturalpoint.com/optitrack/.\n\nInteraction networks for learning about objects, relations and physics. P Battaglia, R Pascanu, M Lai, D J Rezende, Advances in neural information processing systems. P. Battaglia, R. Pascanu, M. Lai, D. J. Rezende, et al., \"Interaction networks for learning about objects, relations and physics,\" in Advances in neural information processing systems, 2016, pp. 4502-4510.\n\nDeep representation learning for human motion prediction and classification. J Butepage, M J Black, D Kragic, H Kjellstrom, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionJ. Butepage, M. J. Black, D. Kragic, and H. Kjellstrom, \"Deep repre- sentation learning for human motion prediction and classification,\" in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 6158-6166.\n\nLong-term human motion prediction with scene context. Z Cao, H Gao, K Mangalam, Q.-Z Cai, M Vo, J Malik, arXiv:2007.03672arXiv preprintZ. Cao, H. Gao, K. Mangalam, Q.-Z. Cai, M. Vo, and J. Malik, \"Long-term human motion prediction with scene context,\" arXiv preprint arXiv:2007.03672, 2020.\n\nPurposive learning: Robot reasoning about the meanings of human activities. G Cheng, K Ramirez-Amaro, M Beetz, Y Kuniyoshi, Science Robotics. 426G. Cheng, K. Ramirez-Amaro, M. Beetz, and Y. Kuniyoshi, \"Purposive learning: Robot reasoning about the meanings of human activities,\" Science Robotics, vol. 4, no. 26, 2019.\n\nContextaware human motion prediction. E Corona, A Pumarola, G Alenya, F Moreno-Noguer, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionE. Corona, A. Pumarola, G. Alenya, and F. Moreno-Noguer, \"Context- aware human motion prediction,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 6992-7001.\n\nLearning dynamic relationships for 3d human motion prediction. Q Cui, H Sun, F Yang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionQ. Cui, H. Sun, and F. Yang, \"Learning dynamic relationships for 3d human motion prediction,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 6519-6527.\n\nInvestigation of different skeleton features for cnn-based 3d action recognition. Z Ding, P Wang, P O Ogunbona, W Li, 2017 IEEE International Conference on Multimedia & Expo Workshops. ICMEWZ. Ding, P. Wang, P. O. Ogunbona, and W. Li, \"Investigation of different skeleton features for cnn-based 3d action recognition,\" in 2017 IEEE International Conference on Multimedia & Expo Workshops (ICMEW).\n\n. IEEE. IEEE, 2017, pp. 617-622.\n\nSkeleton based action recognition with convolutional neural network. Y Du, Y Fu, L Wang, 2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR). IEEEY. Du, Y. Fu, and L. Wang, \"Skeleton based action recognition with convolutional neural network,\" in 2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR). IEEE, 2015, pp. 579-583.\n\nUse the force, luke! learning to predict physical forces by simulating effects. K Ehsani, S Tulsiani, S Gupta, A Farhadi, A Gupta, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionK. Ehsani, S. Tulsiani, S. Gupta, A. Farhadi, and A. Gupta, \"Use the force, luke! learning to predict physical forces by simulating effects,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 224-233.\n\nUnsupervised learning for physical interaction through video prediction. C Finn, I Goodfellow, S Levine, arXiv:1605.07157arXiv preprintC. Finn, I. Goodfellow, and S. Levine, \"Unsupervised learning for physical interaction through video prediction,\" arXiv preprint arXiv:1605.07157, 2016.\n\nFirst-person hand action benchmark with rgb-d videos and 3d hand pose annotations. G Garcia-Hernando, S Yuan, S Baek, T.-K Kim, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionG. Garcia-Hernando, S. Yuan, S. Baek, and T.-K. Kim, \"First-person hand action benchmark with rgb-d videos and 3d hand pose annotations,\" in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 409-419.\n\nHuman3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments. C Ionescu, D Papava, V Olaru, C Sminchisescu, IEEE Transactions on Pattern Analysis and Machine Intelligence. 367C. Ionescu, D. Papava, V. Olaru, and C. Sminchisescu, \"Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 36, no. 7, pp. 1325-1339, jul 2014.\n\nReasoning about physical interactions with object-oriented prediction and planning. M Janner, S Levine, W T Freeman, J B Tenenbaum, C Finn, J Wu, arXiv:1812.10972arXiv preprintM. Janner, S. Levine, W. T. Freeman, J. B. Tenenbaum, C. Finn, and J. Wu, \"Reasoning about physical interactions with object-oriented prediction and planning,\" arXiv preprint arXiv:1812.10972, 2018.\n\nMalleable embodiment: Changing sense of embodiment by spatial-temporal deformation of virtual human body. S Kasahara, K Konno, R Owaki, T Nishi, A Takeshita, T Ito, S Kasuga, J Ushiba, Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems. the 2017 CHI Conference on Human Factors in Computing SystemsS. Kasahara, K. Konno, R. Owaki, T. Nishi, A. Takeshita, T. Ito, S. Kasuga, and J. Ushiba, \"Malleable embodiment: Changing sense of embodiment by spatial-temporal deformation of virtual human body,\" in Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems, 2017, pp. 6438-6448.\n\nSkeleton-based action recognition of people handling objects. S Kim, K Yun, J Park, J Y Choi, 2019 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEES. Kim, K. Yun, J. Park, and J. Y. Choi, \"Skeleton-based action recog- nition of people handling objects,\" in 2019 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, 2019, pp. 61-70.\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, arXiv:1412.6980arXiv preprintD. P. Kingma and J. Ba, \"Adam: A method for stochastic optimization,\" arXiv preprint arXiv:1412.6980, 2014.\n\nLearning spatio-temporal structure from rgb-d videos for human activity detection and anticipation. H Koppula, A Saxena, International conference on machine learning. H. Koppula and A. Saxena, \"Learning spatio-temporal structure from rgb-d videos for human activity detection and anticipation,\" in Interna- tional conference on machine learning, 2013, pp. 792-800.\n\nAnticipating human activities for reactive robotic response. H S Koppula, A Saxena, IROS. Tokyo. 2071H. S. Koppula and A. Saxena, \"Anticipating human activities for reactive robotic response.\" in IROS. Tokyo, 2013, p. 2071.\n\nRobot cooperative behavior learning using single-shot learning from demonstration and parallel hidden markov models. J.-F Lafleche, S Saunderson, G Nejat, IEEE Robotics and Automation Letters. 42J.-F. Lafleche, S. Saunderson, and G. Nejat, \"Robot cooperative behavior learning using single-shot learning from demonstration and parallel hidden markov models,\" IEEE Robotics and Automation Letters, vol. 4, no. 2, pp. 193-200, 2018.\n\nEnsemble deep learning for skeleton-based action recognition using temporal sliding lstm networks. I Lee, D Kim, S Kang, S Lee, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionI. Lee, D. Kim, S. Kang, and S. Lee, \"Ensemble deep learning for skeleton-based action recognition using temporal sliding lstm networks,\" in Proceedings of the IEEE International Conference on Computer Vision, 2017, pp. 1012-1020.\n\nCo-occurrence feature learning from skeleton data for action recognition and detection with hierarchical aggregation. C Li, Q Zhong, D Xie, S Pu, arXiv:1804.06055arXiv preprintC. Li, Q. Zhong, D. Xie, and S. Pu, \"Co-occurrence feature learning from skeleton data for action recognition and detection with hierarchical aggregation,\" arXiv preprint arXiv:1804.06055, 2018.\n\nActionalstructural graph convolutional networks for skeleton-based action recognition. M Li, S Chen, X Chen, Y Zhang, Y Wang, Q Tian, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionM. Li, S. Chen, X. Chen, Y. Zhang, Y. Wang, and Q. Tian, \"Actional- structural graph convolutional networks for skeleton-based action recog- nition,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 3595-3603.\n\nEstimating 3d motion and forces of person-object interactions from monocular video. Z Li, J Sedlar, J Carpentier, I Laptev, N Mansard, J Sivic, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionZ. Li, J. Sedlar, J. Carpentier, I. Laptev, N. Mansard, and J. Sivic, \"Estimating 3d motion and forces of person-object interactions from monocular video,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 8640-8649.\n\nNtu rgb+d 120: A large-scale benchmark for 3d human activity understanding. J Liu, A Shahroudy, M Perez, G Wang, L.-Y Duan, A C Kot, IEEE Transactions on Pattern Analysis and Machine Intelligence. J. Liu, A. Shahroudy, M. Perez, G. Wang, L.-Y. Duan, and A. C. Kot, \"Ntu rgb+d 120: A large-scale benchmark for 3d human activity understanding,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, 2019.\n\nDimensionality reduction for whole-body human motion recognition. C Mandery, M Plappert, J Borras, T Asfour, 2016 19th International Conference on Information Fusion (FUSION). IEEEC. Mandery, M. Plappert, J. Borras, and T. Asfour, \"Dimensionality reduction for whole-body human motion recognition,\" in 2016 19th International Conference on Information Fusion (FUSION). IEEE, 2016, pp. 355-362.\n\nThe kit whole-body human motion database. C Mandery, O Terlemez, M Do, N Vahrenkamp, T Asfour, International Conference on Advanced Robotics (ICAR). C. Mandery, O. Terlemez, M. Do, N. Vahrenkamp, and T. Asfour, \"The kit whole-body human motion database,\" in International Conference on Advanced Robotics (ICAR), 2015, pp. 329-336.\n\nBerkeley mhad: A comprehensive multimodal human action database. F Ofli, R Chaudhry, G Kurillo, R Vidal, R Bajcsy, 2013 IEEE Workshop on Applications of Computer Vision (WACV). IEEEF. Ofli, R. Chaudhry, G. Kurillo, R. Vidal, and R. Bajcsy, \"Berkeley mhad: A comprehensive multimodal human action database,\" in 2013 IEEE Workshop on Applications of Computer Vision (WACV). IEEE, 2013, pp. 53-60.\n\nNtu rgb+d: A large scale dataset for 3d human activity analysis. A Shahroudy, J Liu, T.-T Ng, G Wang, IEEE Conference on Computer Vision and Pattern Recognition. A. Shahroudy, J. Liu, T.-T. Ng, and G. Wang, \"Ntu rgb+d: A large scale dataset for 3d human activity analysis,\" in IEEE Conference on Computer Vision and Pattern Recognition, June 2016.\n\nAn integrated framework for human-robot collaborative manipulation. W Sheng, A Thobbi, Y Gu, IEEE Transactions on Cybernetics. 4510W. Sheng, A. Thobbi, and Y. Gu, \"An integrated framework for human-robot collaborative manipulation,\" IEEE Transactions on Cyber- netics, vol. 45, no. 10, pp. 2030-2041, 2015.\n\nSkeleton-based action recognition with spatial reasoning and temporal stack learning. C Si, Y Jing, W Wang, L Wang, T Tan, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)C. Si, Y. Jing, W. Wang, L. Wang, and T. Tan, \"Skeleton-based action recognition with spatial reasoning and temporal stack learning,\" in Proceedings of the European Conference on Computer Vision (ECCV), 2018, pp. 103-118.\n\nGrab: A dataset of whole-body human grasping of objects. O Taheri, N Ghorbani, M J Black, D Tzionas, arXiv:2008.11200arXiv preprintO. Taheri, N. Ghorbani, M. J. Black, and D. Tzionas, \"Grab: A dataset of whole-body human grasping of objects,\" arXiv preprint arXiv:2008.11200, 2020.\n\nDeep progressive reinforcement learning for skeleton-based action recognition. Y Tang, Y Tian, J Lu, P Li, J Zhou, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionY. Tang, Y. Tian, J. Lu, P. Li, and J. Zhou, \"Deep progressive reinforce- ment learning for skeleton-based action recognition,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 5323-5332.\n\nA system for learning continuous human-robot interactions from human-human demonstrations. D Vogt, S Stepputtis, S Grehl, B Jung, H Ben Amor, 2017 IEEE International Conference on Robotics and Automation (ICRA. D. Vogt, S. Stepputtis, S. Grehl, B. Jung, and H. Ben Amor, \"A system for learning continuous human-robot interactions from human-human demonstrations,\" in 2017 IEEE International Conference on Robotics and Automation (ICRA), 2017, pp. 2882-2889.\n\nGibson env: Real-world perception for embodied agents. F Xia, A R Zamir, Z He, A Sax, J Malik, S Savarese, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionF. Xia, A. R. Zamir, Z. He, A. Sax, J. Malik, and S. Savarese, \"Gibson env: Real-world perception for embodied agents,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 9068-9079.\n\nView invariant human action recognition using histograms of 3d joints. L Xia, C Chen, J Aggarwal, Computer Vision and Pattern Recognition Workshops (CVPRW), 2012 IEEE Computer Society Conference on. IEEEL. Xia, C. Chen, and J. Aggarwal, \"View invariant human action recognition using histograms of 3d joints,\" in Computer Vision and Pattern Recognition Workshops (CVPRW), 2012 IEEE Computer Society Conference on. IEEE, 2012, pp. 20-27.\n\nSpatial temporal graph convolutional networks for skeleton-based action recognition. S Yan, Y Xiong, D Lin, Thirty-Second AAAI Conference on Artificial Intelligence. S. Yan, Y. Xiong, and D. Lin, \"Spatial temporal graph convolutional networks for skeleton-based action recognition,\" in Thirty-Second AAAI Conference on Artificial Intelligence, 2018.\n\nView adaptive recurrent neural networks for high performance human action recognition from skeleton data. P Zhang, C Lan, J Xing, W Zeng, J Xue, N Zheng, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionP. Zhang, C. Lan, J. Xing, W. Zeng, J. Xue, and N. Zheng, \"View adaptive recurrent neural networks for high performance human action recognition from skeleton data,\" in Proceedings of the IEEE Interna- tional Conference on Computer Vision, 2017, pp. 2117-2126.\n\nPlace: Proximity learning of articulation and contact in 3d environments. S Zhang, Y Zhang, Q Ma, M J Black, S Tang, S. Zhang, Y. Zhang, Q. Ma, M. J. Black, and S. Tang, \"Place: Proximity learning of articulation and contact in 3d environments.\"\n\nCollaborative human-robot motion generation using lstm-rnn. X Zhao, S Chumkamon, S Duan, J Rojas, J Pan, 2018 IEEE-RAS 18th International Conference on Humanoid Robots (Humanoids). X. Zhao, S. Chumkamon, S. Duan, J. Rojas, and J. Pan, \"Collaborative human-robot motion generation using lstm-rnn,\" in 2018 IEEE-RAS 18th International Conference on Humanoid Robots (Humanoids), 2018, pp. 1-9.\n", "annotations": {"author": "[{\"end\":98,\"start\":87},{\"end\":108,\"start\":99},{\"end\":121,\"start\":109},{\"end\":137,\"start\":122},{\"end\":150,\"start\":138},{\"end\":164,\"start\":151},{\"end\":173,\"start\":165},{\"end\":193,\"start\":174},{\"end\":206,\"start\":194},{\"end\":220,\"start\":207}]", "publisher": null, "author_last_name": "[{\"end\":97,\"start\":94},{\"end\":107,\"start\":103},{\"end\":120,\"start\":117},{\"end\":136,\"start\":131},{\"end\":149,\"start\":146},{\"end\":163,\"start\":159},{\"end\":172,\"start\":169},{\"end\":192,\"start\":184},{\"end\":205,\"start\":199},{\"end\":219,\"start\":215}]", "author_first_name": "[{\"end\":93,\"start\":87},{\"end\":102,\"start\":99},{\"end\":116,\"start\":109},{\"end\":130,\"start\":122},{\"end\":145,\"start\":138},{\"end\":158,\"start\":151},{\"end\":168,\"start\":165},{\"end\":183,\"start\":174},{\"end\":198,\"start\":194},{\"end\":214,\"start\":207}]", "author_affiliation": null, "title": "[{\"end\":84,\"start\":1},{\"end\":304,\"start\":221}]", "venue": null, "abstract": "[{\"end\":1596,\"start\":392}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2213,\"start\":2209},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2218,\"start\":2215},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2224,\"start\":2220},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2230,\"start\":2226},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2250,\"start\":2246},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":4849,\"start\":4845},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6233,\"start\":6229},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":6239,\"start\":6235},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6245,\"start\":6241},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6250,\"start\":6247},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":6353,\"start\":6349},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6358,\"start\":6355},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6364,\"start\":6360},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6369,\"start\":6366},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6374,\"start\":6371},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":6540,\"start\":6536},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6689,\"start\":6685},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6999,\"start\":6995},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7143,\"start\":7140},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7452,\"start\":7449},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7466,\"start\":7463},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7927,\"start\":7923},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7933,\"start\":7929},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7939,\"start\":7935},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":7945,\"start\":7941},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7951,\"start\":7947},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8333,\"start\":8329},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8465,\"start\":8461},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8634,\"start\":8630},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8749,\"start\":8745},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8755,\"start\":8751},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9352,\"start\":9348},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9747,\"start\":9743},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9978,\"start\":9974},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9984,\"start\":9980},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":10060,\"start\":10056},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":13983,\"start\":13979},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":17175,\"start\":17172},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":19994,\"start\":19990},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":21740,\"start\":21737},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":21754,\"start\":21751},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":22524,\"start\":22521},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":23900,\"start\":23897},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":24300,\"start\":24297}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":29938,\"start\":29607},{\"attributes\":{\"id\":\"fig_1\"},\"end\":30674,\"start\":29939},{\"attributes\":{\"id\":\"fig_2\"},\"end\":31077,\"start\":30675},{\"attributes\":{\"id\":\"fig_3\"},\"end\":31422,\"start\":31078},{\"attributes\":{\"id\":\"fig_4\"},\"end\":31602,\"start\":31423},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":31913,\"start\":31603},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":32043,\"start\":31914},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":34189,\"start\":32044},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":34978,\"start\":34190}]", "paragraph": "[{\"end\":2570,\"start\":1605},{\"end\":3544,\"start\":2572},{\"end\":4597,\"start\":3546},{\"end\":5184,\"start\":4599},{\"end\":5685,\"start\":5186},{\"end\":6375,\"start\":5745},{\"end\":7251,\"start\":6377},{\"end\":7780,\"start\":7253},{\"end\":8710,\"start\":7820},{\"end\":9538,\"start\":8712},{\"end\":10392,\"start\":9599},{\"end\":10763,\"start\":10452},{\"end\":10942,\"start\":10786},{\"end\":11346,\"start\":10995},{\"end\":11784,\"start\":11372},{\"end\":12463,\"start\":11786},{\"end\":13272,\"start\":12465},{\"end\":13984,\"start\":13274},{\"end\":14565,\"start\":13986},{\"end\":15187,\"start\":14567},{\"end\":15630,\"start\":15189},{\"end\":16163,\"start\":15632},{\"end\":16521,\"start\":16165},{\"end\":16573,\"start\":16523},{\"end\":17009,\"start\":16662},{\"end\":18164,\"start\":17050},{\"end\":18481,\"start\":18166},{\"end\":19046,\"start\":18483},{\"end\":19737,\"start\":19080},{\"end\":19951,\"start\":19756},{\"end\":20456,\"start\":19953},{\"end\":20794,\"start\":20458},{\"end\":21253,\"start\":20796},{\"end\":21636,\"start\":21255},{\"end\":22130,\"start\":21638},{\"end\":22456,\"start\":22159},{\"end\":22869,\"start\":22458},{\"end\":23416,\"start\":22871},{\"end\":23871,\"start\":23418},{\"end\":24301,\"start\":23873},{\"end\":25060,\"start\":24303},{\"end\":25569,\"start\":25062},{\"end\":27228,\"start\":25597},{\"end\":27922,\"start\":27267},{\"end\":28335,\"start\":27924},{\"end\":29214,\"start\":28337},{\"end\":29606,\"start\":29216}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10785,\"start\":10764},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11371,\"start\":11347},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16636,\"start\":16574},{\"attributes\":{\"id\":\"formula_3\"},\"end\":16661,\"start\":16636},{\"attributes\":{\"id\":\"formula_4\"},\"end\":19079,\"start\":19047}]", "table_ref": null, "section_header": "[{\"end\":5704,\"start\":5688},{\"end\":5743,\"start\":5707},{\"end\":7818,\"start\":7783},{\"end\":9597,\"start\":9541},{\"end\":10411,\"start\":10395},{\"end\":10450,\"start\":10414},{\"end\":10993,\"start\":10945},{\"end\":17048,\"start\":17012},{\"end\":19754,\"start\":19740},{\"end\":22157,\"start\":22133},{\"end\":25595,\"start\":25572},{\"end\":27265,\"start\":27231},{\"end\":29616,\"start\":29608},{\"end\":29948,\"start\":29940},{\"end\":30684,\"start\":30676},{\"end\":31087,\"start\":31079},{\"end\":31432,\"start\":31424},{\"end\":31922,\"start\":31915}]", "table": "[{\"end\":31913,\"start\":31746},{\"end\":34189,\"start\":32300},{\"end\":34978,\"start\":34375}]", "figure_caption": "[{\"end\":29938,\"start\":29618},{\"end\":30674,\"start\":29950},{\"end\":31077,\"start\":30686},{\"end\":31422,\"start\":31089},{\"end\":31602,\"start\":31434},{\"end\":31746,\"start\":31605},{\"end\":32043,\"start\":31924},{\"end\":32300,\"start\":32046},{\"end\":34375,\"start\":34192}]", "figure_ref": "[{\"end\":1604,\"start\":1598},{\"end\":4783,\"start\":4777},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11477,\"start\":11471},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13459,\"start\":13451},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15386,\"start\":15378},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":17575,\"start\":17569},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":17866,\"start\":17860},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":19416,\"start\":19410},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":21158,\"start\":21150},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":22972,\"start\":22966},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":23135,\"start\":23129},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":23251,\"start\":23245},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":25737,\"start\":25731},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26485,\"start\":26479},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":28547,\"start\":28541}]", "bib_author_first_name": "[{\"end\":35198,\"start\":35197},{\"end\":35211,\"start\":35210},{\"end\":35222,\"start\":35221},{\"end\":35229,\"start\":35228},{\"end\":35231,\"start\":35230},{\"end\":35577,\"start\":35576},{\"end\":35589,\"start\":35588},{\"end\":35591,\"start\":35590},{\"end\":35600,\"start\":35599},{\"end\":35610,\"start\":35609},{\"end\":36060,\"start\":36059},{\"end\":36067,\"start\":36066},{\"end\":36074,\"start\":36073},{\"end\":36089,\"start\":36085},{\"end\":36096,\"start\":36095},{\"end\":36102,\"start\":36101},{\"end\":36374,\"start\":36373},{\"end\":36383,\"start\":36382},{\"end\":36400,\"start\":36399},{\"end\":36409,\"start\":36408},{\"end\":36656,\"start\":36655},{\"end\":36666,\"start\":36665},{\"end\":36678,\"start\":36677},{\"end\":36688,\"start\":36687},{\"end\":37124,\"start\":37123},{\"end\":37131,\"start\":37130},{\"end\":37138,\"start\":37137},{\"end\":37579,\"start\":37578},{\"end\":37587,\"start\":37586},{\"end\":37595,\"start\":37594},{\"end\":37597,\"start\":37596},{\"end\":37609,\"start\":37608},{\"end\":37998,\"start\":37997},{\"end\":38004,\"start\":38003},{\"end\":38010,\"start\":38009},{\"end\":38353,\"start\":38352},{\"end\":38363,\"start\":38362},{\"end\":38375,\"start\":38374},{\"end\":38384,\"start\":38383},{\"end\":38395,\"start\":38394},{\"end\":38874,\"start\":38873},{\"end\":38882,\"start\":38881},{\"end\":38896,\"start\":38895},{\"end\":39173,\"start\":39172},{\"end\":39192,\"start\":39191},{\"end\":39200,\"start\":39199},{\"end\":39211,\"start\":39207},{\"end\":39700,\"start\":39699},{\"end\":39711,\"start\":39710},{\"end\":39721,\"start\":39720},{\"end\":39730,\"start\":39729},{\"end\":40160,\"start\":40159},{\"end\":40170,\"start\":40169},{\"end\":40180,\"start\":40179},{\"end\":40182,\"start\":40181},{\"end\":40193,\"start\":40192},{\"end\":40195,\"start\":40194},{\"end\":40208,\"start\":40207},{\"end\":40216,\"start\":40215},{\"end\":40558,\"start\":40557},{\"end\":40570,\"start\":40569},{\"end\":40579,\"start\":40578},{\"end\":40588,\"start\":40587},{\"end\":40597,\"start\":40596},{\"end\":40610,\"start\":40609},{\"end\":40617,\"start\":40616},{\"end\":40627,\"start\":40626},{\"end\":41140,\"start\":41139},{\"end\":41147,\"start\":41146},{\"end\":41154,\"start\":41153},{\"end\":41162,\"start\":41161},{\"end\":41164,\"start\":41163},{\"end\":41496,\"start\":41495},{\"end\":41498,\"start\":41497},{\"end\":41508,\"start\":41507},{\"end\":41752,\"start\":41751},{\"end\":41763,\"start\":41762},{\"end\":42079,\"start\":42078},{\"end\":42081,\"start\":42080},{\"end\":42092,\"start\":42091},{\"end\":42363,\"start\":42359},{\"end\":42375,\"start\":42374},{\"end\":42389,\"start\":42388},{\"end\":42774,\"start\":42773},{\"end\":42781,\"start\":42780},{\"end\":42788,\"start\":42787},{\"end\":42796,\"start\":42795},{\"end\":43274,\"start\":43273},{\"end\":43280,\"start\":43279},{\"end\":43289,\"start\":43288},{\"end\":43296,\"start\":43295},{\"end\":43615,\"start\":43614},{\"end\":43621,\"start\":43620},{\"end\":43629,\"start\":43628},{\"end\":43637,\"start\":43636},{\"end\":43646,\"start\":43645},{\"end\":43654,\"start\":43653},{\"end\":44141,\"start\":44140},{\"end\":44147,\"start\":44146},{\"end\":44157,\"start\":44156},{\"end\":44171,\"start\":44170},{\"end\":44181,\"start\":44180},{\"end\":44192,\"start\":44191},{\"end\":44690,\"start\":44689},{\"end\":44697,\"start\":44696},{\"end\":44710,\"start\":44709},{\"end\":44719,\"start\":44718},{\"end\":44730,\"start\":44726},{\"end\":44738,\"start\":44737},{\"end\":44740,\"start\":44739},{\"end\":45094,\"start\":45093},{\"end\":45105,\"start\":45104},{\"end\":45117,\"start\":45116},{\"end\":45127,\"start\":45126},{\"end\":45465,\"start\":45464},{\"end\":45476,\"start\":45475},{\"end\":45488,\"start\":45487},{\"end\":45494,\"start\":45493},{\"end\":45508,\"start\":45507},{\"end\":45820,\"start\":45819},{\"end\":45828,\"start\":45827},{\"end\":45840,\"start\":45839},{\"end\":45851,\"start\":45850},{\"end\":45860,\"start\":45859},{\"end\":46216,\"start\":46215},{\"end\":46229,\"start\":46228},{\"end\":46239,\"start\":46235},{\"end\":46245,\"start\":46244},{\"end\":46568,\"start\":46567},{\"end\":46577,\"start\":46576},{\"end\":46587,\"start\":46586},{\"end\":46894,\"start\":46893},{\"end\":46900,\"start\":46899},{\"end\":46908,\"start\":46907},{\"end\":46916,\"start\":46915},{\"end\":46924,\"start\":46923},{\"end\":47326,\"start\":47325},{\"end\":47336,\"start\":47335},{\"end\":47348,\"start\":47347},{\"end\":47350,\"start\":47349},{\"end\":47359,\"start\":47358},{\"end\":47631,\"start\":47630},{\"end\":47639,\"start\":47638},{\"end\":47647,\"start\":47646},{\"end\":47653,\"start\":47652},{\"end\":47659,\"start\":47658},{\"end\":48131,\"start\":48130},{\"end\":48139,\"start\":48138},{\"end\":48153,\"start\":48152},{\"end\":48162,\"start\":48161},{\"end\":48170,\"start\":48169},{\"end\":48554,\"start\":48553},{\"end\":48561,\"start\":48560},{\"end\":48563,\"start\":48562},{\"end\":48572,\"start\":48571},{\"end\":48578,\"start\":48577},{\"end\":48585,\"start\":48584},{\"end\":48594,\"start\":48593},{\"end\":49042,\"start\":49041},{\"end\":49049,\"start\":49048},{\"end\":49057,\"start\":49056},{\"end\":49494,\"start\":49493},{\"end\":49501,\"start\":49500},{\"end\":49510,\"start\":49509},{\"end\":49866,\"start\":49865},{\"end\":49875,\"start\":49874},{\"end\":49882,\"start\":49881},{\"end\":49890,\"start\":49889},{\"end\":49898,\"start\":49897},{\"end\":49905,\"start\":49904},{\"end\":50371,\"start\":50370},{\"end\":50380,\"start\":50379},{\"end\":50389,\"start\":50388},{\"end\":50395,\"start\":50394},{\"end\":50397,\"start\":50396},{\"end\":50406,\"start\":50405},{\"end\":50604,\"start\":50603},{\"end\":50612,\"start\":50611},{\"end\":50625,\"start\":50624},{\"end\":50633,\"start\":50632},{\"end\":50642,\"start\":50641}]", "bib_author_last_name": "[{\"end\":35208,\"start\":35199},{\"end\":35219,\"start\":35212},{\"end\":35226,\"start\":35223},{\"end\":35239,\"start\":35232},{\"end\":35586,\"start\":35578},{\"end\":35597,\"start\":35592},{\"end\":35607,\"start\":35601},{\"end\":35621,\"start\":35611},{\"end\":36064,\"start\":36061},{\"end\":36071,\"start\":36068},{\"end\":36083,\"start\":36075},{\"end\":36093,\"start\":36090},{\"end\":36099,\"start\":36097},{\"end\":36108,\"start\":36103},{\"end\":36380,\"start\":36375},{\"end\":36397,\"start\":36384},{\"end\":36406,\"start\":36401},{\"end\":36419,\"start\":36410},{\"end\":36663,\"start\":36657},{\"end\":36675,\"start\":36667},{\"end\":36685,\"start\":36679},{\"end\":36702,\"start\":36689},{\"end\":37128,\"start\":37125},{\"end\":37135,\"start\":37132},{\"end\":37143,\"start\":37139},{\"end\":37584,\"start\":37580},{\"end\":37592,\"start\":37588},{\"end\":37606,\"start\":37598},{\"end\":37612,\"start\":37610},{\"end\":38001,\"start\":37999},{\"end\":38007,\"start\":38005},{\"end\":38015,\"start\":38011},{\"end\":38360,\"start\":38354},{\"end\":38372,\"start\":38364},{\"end\":38381,\"start\":38376},{\"end\":38392,\"start\":38385},{\"end\":38401,\"start\":38396},{\"end\":38879,\"start\":38875},{\"end\":38893,\"start\":38883},{\"end\":38903,\"start\":38897},{\"end\":39189,\"start\":39174},{\"end\":39197,\"start\":39193},{\"end\":39205,\"start\":39201},{\"end\":39215,\"start\":39212},{\"end\":39708,\"start\":39701},{\"end\":39718,\"start\":39712},{\"end\":39727,\"start\":39722},{\"end\":39743,\"start\":39731},{\"end\":40167,\"start\":40161},{\"end\":40177,\"start\":40171},{\"end\":40190,\"start\":40183},{\"end\":40205,\"start\":40196},{\"end\":40213,\"start\":40209},{\"end\":40219,\"start\":40217},{\"end\":40567,\"start\":40559},{\"end\":40576,\"start\":40571},{\"end\":40585,\"start\":40580},{\"end\":40594,\"start\":40589},{\"end\":40607,\"start\":40598},{\"end\":40614,\"start\":40611},{\"end\":40624,\"start\":40618},{\"end\":40634,\"start\":40628},{\"end\":41144,\"start\":41141},{\"end\":41151,\"start\":41148},{\"end\":41159,\"start\":41155},{\"end\":41169,\"start\":41165},{\"end\":41505,\"start\":41499},{\"end\":41511,\"start\":41509},{\"end\":41760,\"start\":41753},{\"end\":41770,\"start\":41764},{\"end\":42089,\"start\":42082},{\"end\":42099,\"start\":42093},{\"end\":42372,\"start\":42364},{\"end\":42386,\"start\":42376},{\"end\":42395,\"start\":42390},{\"end\":42778,\"start\":42775},{\"end\":42785,\"start\":42782},{\"end\":42793,\"start\":42789},{\"end\":42800,\"start\":42797},{\"end\":43277,\"start\":43275},{\"end\":43286,\"start\":43281},{\"end\":43293,\"start\":43290},{\"end\":43299,\"start\":43297},{\"end\":43618,\"start\":43616},{\"end\":43626,\"start\":43622},{\"end\":43634,\"start\":43630},{\"end\":43643,\"start\":43638},{\"end\":43651,\"start\":43647},{\"end\":43659,\"start\":43655},{\"end\":44144,\"start\":44142},{\"end\":44154,\"start\":44148},{\"end\":44168,\"start\":44158},{\"end\":44178,\"start\":44172},{\"end\":44189,\"start\":44182},{\"end\":44198,\"start\":44193},{\"end\":44694,\"start\":44691},{\"end\":44707,\"start\":44698},{\"end\":44716,\"start\":44711},{\"end\":44724,\"start\":44720},{\"end\":44735,\"start\":44731},{\"end\":44744,\"start\":44741},{\"end\":45102,\"start\":45095},{\"end\":45114,\"start\":45106},{\"end\":45124,\"start\":45118},{\"end\":45134,\"start\":45128},{\"end\":45473,\"start\":45466},{\"end\":45485,\"start\":45477},{\"end\":45491,\"start\":45489},{\"end\":45505,\"start\":45495},{\"end\":45515,\"start\":45509},{\"end\":45825,\"start\":45821},{\"end\":45837,\"start\":45829},{\"end\":45848,\"start\":45841},{\"end\":45857,\"start\":45852},{\"end\":45867,\"start\":45861},{\"end\":46226,\"start\":46217},{\"end\":46233,\"start\":46230},{\"end\":46242,\"start\":46240},{\"end\":46250,\"start\":46246},{\"end\":46574,\"start\":46569},{\"end\":46584,\"start\":46578},{\"end\":46590,\"start\":46588},{\"end\":46897,\"start\":46895},{\"end\":46905,\"start\":46901},{\"end\":46913,\"start\":46909},{\"end\":46921,\"start\":46917},{\"end\":46928,\"start\":46925},{\"end\":47333,\"start\":47327},{\"end\":47345,\"start\":47337},{\"end\":47356,\"start\":47351},{\"end\":47367,\"start\":47360},{\"end\":47636,\"start\":47632},{\"end\":47644,\"start\":47640},{\"end\":47650,\"start\":47648},{\"end\":47656,\"start\":47654},{\"end\":47664,\"start\":47660},{\"end\":48136,\"start\":48132},{\"end\":48150,\"start\":48140},{\"end\":48159,\"start\":48154},{\"end\":48167,\"start\":48163},{\"end\":48179,\"start\":48171},{\"end\":48558,\"start\":48555},{\"end\":48569,\"start\":48564},{\"end\":48575,\"start\":48573},{\"end\":48582,\"start\":48579},{\"end\":48591,\"start\":48586},{\"end\":48603,\"start\":48595},{\"end\":49046,\"start\":49043},{\"end\":49054,\"start\":49050},{\"end\":49066,\"start\":49058},{\"end\":49498,\"start\":49495},{\"end\":49507,\"start\":49502},{\"end\":49514,\"start\":49511},{\"end\":49872,\"start\":49867},{\"end\":49879,\"start\":49876},{\"end\":49887,\"start\":49883},{\"end\":49895,\"start\":49891},{\"end\":49902,\"start\":49899},{\"end\":49911,\"start\":49906},{\"end\":50377,\"start\":50372},{\"end\":50386,\"start\":50381},{\"end\":50392,\"start\":50390},{\"end\":50403,\"start\":50398},{\"end\":50411,\"start\":50407},{\"end\":50609,\"start\":50605},{\"end\":50622,\"start\":50613},{\"end\":50630,\"start\":50626},{\"end\":50639,\"start\":50634},{\"end\":50646,\"start\":50643}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":35123,\"start\":34980},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":2200675},\"end\":35497,\"start\":35125},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":2469127},\"end\":36003,\"start\":35499},{\"attributes\":{\"doi\":\"arXiv:2007.03672\",\"id\":\"b3\"},\"end\":36295,\"start\":36005},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":58031615},\"end\":36615,\"start\":36297},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":102352622},\"end\":37058,\"start\":36617},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":219615815},\"end\":37494,\"start\":37060},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":11455496},\"end\":37892,\"start\":37496},{\"attributes\":{\"id\":\"b8\"},\"end\":37926,\"start\":37894},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":1354671},\"end\":38270,\"start\":37928},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":214667228},\"end\":38798,\"start\":38272},{\"attributes\":{\"doi\":\"arXiv:1605.07157\",\"id\":\"b11\"},\"end\":39087,\"start\":38800},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":4721179},\"end\":39596,\"start\":39089},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":4244548},\"end\":40073,\"start\":39598},{\"attributes\":{\"doi\":\"arXiv:1812.10972\",\"id\":\"b14\"},\"end\":40449,\"start\":40075},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":6296724},\"end\":41075,\"start\":40451},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":58981439},\"end\":41449,\"start\":41077},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b17\"},\"end\":41649,\"start\":41451},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":52865362},\"end\":42015,\"start\":41651},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":16275203},\"end\":42240,\"start\":42017},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":56596458},\"end\":42672,\"start\":42242},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":12965906},\"end\":43153,\"start\":42674},{\"attributes\":{\"doi\":\"arXiv:1804.06055\",\"id\":\"b22\"},\"end\":43525,\"start\":43155},{\"attributes\":{\"id\":\"b23\"},\"end\":44054,\"start\":43527},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":102354437},\"end\":44611,\"start\":44056},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":152282878},\"end\":45025,\"start\":44613},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":14694480},\"end\":45420,\"start\":45027},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":7546733},\"end\":45752,\"start\":45422},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":14036983},\"end\":46148,\"start\":45754},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":15928602},\"end\":46497,\"start\":46150},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":206664344},\"end\":46805,\"start\":46499},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":19170594},\"end\":47266,\"start\":46807},{\"attributes\":{\"doi\":\"arXiv:2008.11200\",\"id\":\"b32\"},\"end\":47549,\"start\":47268},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":52245597},\"end\":48037,\"start\":47551},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":20422527},\"end\":48496,\"start\":48039},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":49358881},\"end\":48968,\"start\":48498},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":14239485},\"end\":49406,\"start\":48970},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":19167105},\"end\":49757,\"start\":49408},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":12699455},\"end\":50294,\"start\":49759},{\"attributes\":{\"id\":\"b39\"},\"end\":50541,\"start\":50296},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":59233727},\"end\":50933,\"start\":50543}]", "bib_title": "[{\"end\":35012,\"start\":34980},{\"end\":35195,\"start\":35125},{\"end\":35574,\"start\":35499},{\"end\":36371,\"start\":36297},{\"end\":36653,\"start\":36617},{\"end\":37121,\"start\":37060},{\"end\":37576,\"start\":37496},{\"end\":37995,\"start\":37928},{\"end\":38350,\"start\":38272},{\"end\":39170,\"start\":39089},{\"end\":39697,\"start\":39598},{\"end\":40555,\"start\":40451},{\"end\":41137,\"start\":41077},{\"end\":41749,\"start\":41651},{\"end\":42076,\"start\":42017},{\"end\":42357,\"start\":42242},{\"end\":42771,\"start\":42674},{\"end\":43612,\"start\":43527},{\"end\":44138,\"start\":44056},{\"end\":44687,\"start\":44613},{\"end\":45091,\"start\":45027},{\"end\":45462,\"start\":45422},{\"end\":45817,\"start\":45754},{\"end\":46213,\"start\":46150},{\"end\":46565,\"start\":46499},{\"end\":46891,\"start\":46807},{\"end\":47628,\"start\":47551},{\"end\":48128,\"start\":48039},{\"end\":48551,\"start\":48498},{\"end\":49039,\"start\":48970},{\"end\":49491,\"start\":49408},{\"end\":49863,\"start\":49759},{\"end\":50601,\"start\":50543}]", "bib_author": "[{\"end\":35210,\"start\":35197},{\"end\":35221,\"start\":35210},{\"end\":35228,\"start\":35221},{\"end\":35241,\"start\":35228},{\"end\":35588,\"start\":35576},{\"end\":35599,\"start\":35588},{\"end\":35609,\"start\":35599},{\"end\":35623,\"start\":35609},{\"end\":36066,\"start\":36059},{\"end\":36073,\"start\":36066},{\"end\":36085,\"start\":36073},{\"end\":36095,\"start\":36085},{\"end\":36101,\"start\":36095},{\"end\":36110,\"start\":36101},{\"end\":36382,\"start\":36373},{\"end\":36399,\"start\":36382},{\"end\":36408,\"start\":36399},{\"end\":36421,\"start\":36408},{\"end\":36665,\"start\":36655},{\"end\":36677,\"start\":36665},{\"end\":36687,\"start\":36677},{\"end\":36704,\"start\":36687},{\"end\":37130,\"start\":37123},{\"end\":37137,\"start\":37130},{\"end\":37145,\"start\":37137},{\"end\":37586,\"start\":37578},{\"end\":37594,\"start\":37586},{\"end\":37608,\"start\":37594},{\"end\":37614,\"start\":37608},{\"end\":38003,\"start\":37997},{\"end\":38009,\"start\":38003},{\"end\":38017,\"start\":38009},{\"end\":38362,\"start\":38352},{\"end\":38374,\"start\":38362},{\"end\":38383,\"start\":38374},{\"end\":38394,\"start\":38383},{\"end\":38403,\"start\":38394},{\"end\":38881,\"start\":38873},{\"end\":38895,\"start\":38881},{\"end\":38905,\"start\":38895},{\"end\":39191,\"start\":39172},{\"end\":39199,\"start\":39191},{\"end\":39207,\"start\":39199},{\"end\":39217,\"start\":39207},{\"end\":39710,\"start\":39699},{\"end\":39720,\"start\":39710},{\"end\":39729,\"start\":39720},{\"end\":39745,\"start\":39729},{\"end\":40169,\"start\":40159},{\"end\":40179,\"start\":40169},{\"end\":40192,\"start\":40179},{\"end\":40207,\"start\":40192},{\"end\":40215,\"start\":40207},{\"end\":40221,\"start\":40215},{\"end\":40569,\"start\":40557},{\"end\":40578,\"start\":40569},{\"end\":40587,\"start\":40578},{\"end\":40596,\"start\":40587},{\"end\":40609,\"start\":40596},{\"end\":40616,\"start\":40609},{\"end\":40626,\"start\":40616},{\"end\":40636,\"start\":40626},{\"end\":41146,\"start\":41139},{\"end\":41153,\"start\":41146},{\"end\":41161,\"start\":41153},{\"end\":41171,\"start\":41161},{\"end\":41507,\"start\":41495},{\"end\":41513,\"start\":41507},{\"end\":41762,\"start\":41751},{\"end\":41772,\"start\":41762},{\"end\":42091,\"start\":42078},{\"end\":42101,\"start\":42091},{\"end\":42374,\"start\":42359},{\"end\":42388,\"start\":42374},{\"end\":42397,\"start\":42388},{\"end\":42780,\"start\":42773},{\"end\":42787,\"start\":42780},{\"end\":42795,\"start\":42787},{\"end\":42802,\"start\":42795},{\"end\":43279,\"start\":43273},{\"end\":43288,\"start\":43279},{\"end\":43295,\"start\":43288},{\"end\":43301,\"start\":43295},{\"end\":43620,\"start\":43614},{\"end\":43628,\"start\":43620},{\"end\":43636,\"start\":43628},{\"end\":43645,\"start\":43636},{\"end\":43653,\"start\":43645},{\"end\":43661,\"start\":43653},{\"end\":44146,\"start\":44140},{\"end\":44156,\"start\":44146},{\"end\":44170,\"start\":44156},{\"end\":44180,\"start\":44170},{\"end\":44191,\"start\":44180},{\"end\":44200,\"start\":44191},{\"end\":44696,\"start\":44689},{\"end\":44709,\"start\":44696},{\"end\":44718,\"start\":44709},{\"end\":44726,\"start\":44718},{\"end\":44737,\"start\":44726},{\"end\":44746,\"start\":44737},{\"end\":45104,\"start\":45093},{\"end\":45116,\"start\":45104},{\"end\":45126,\"start\":45116},{\"end\":45136,\"start\":45126},{\"end\":45475,\"start\":45464},{\"end\":45487,\"start\":45475},{\"end\":45493,\"start\":45487},{\"end\":45507,\"start\":45493},{\"end\":45517,\"start\":45507},{\"end\":45827,\"start\":45819},{\"end\":45839,\"start\":45827},{\"end\":45850,\"start\":45839},{\"end\":45859,\"start\":45850},{\"end\":45869,\"start\":45859},{\"end\":46228,\"start\":46215},{\"end\":46235,\"start\":46228},{\"end\":46244,\"start\":46235},{\"end\":46252,\"start\":46244},{\"end\":46576,\"start\":46567},{\"end\":46586,\"start\":46576},{\"end\":46592,\"start\":46586},{\"end\":46899,\"start\":46893},{\"end\":46907,\"start\":46899},{\"end\":46915,\"start\":46907},{\"end\":46923,\"start\":46915},{\"end\":46930,\"start\":46923},{\"end\":47335,\"start\":47325},{\"end\":47347,\"start\":47335},{\"end\":47358,\"start\":47347},{\"end\":47369,\"start\":47358},{\"end\":47638,\"start\":47630},{\"end\":47646,\"start\":47638},{\"end\":47652,\"start\":47646},{\"end\":47658,\"start\":47652},{\"end\":47666,\"start\":47658},{\"end\":48138,\"start\":48130},{\"end\":48152,\"start\":48138},{\"end\":48161,\"start\":48152},{\"end\":48169,\"start\":48161},{\"end\":48181,\"start\":48169},{\"end\":48560,\"start\":48553},{\"end\":48571,\"start\":48560},{\"end\":48577,\"start\":48571},{\"end\":48584,\"start\":48577},{\"end\":48593,\"start\":48584},{\"end\":48605,\"start\":48593},{\"end\":49048,\"start\":49041},{\"end\":49056,\"start\":49048},{\"end\":49068,\"start\":49056},{\"end\":49500,\"start\":49493},{\"end\":49509,\"start\":49500},{\"end\":49516,\"start\":49509},{\"end\":49874,\"start\":49865},{\"end\":49881,\"start\":49874},{\"end\":49889,\"start\":49881},{\"end\":49897,\"start\":49889},{\"end\":49904,\"start\":49897},{\"end\":49913,\"start\":49904},{\"end\":50379,\"start\":50370},{\"end\":50388,\"start\":50379},{\"end\":50394,\"start\":50388},{\"end\":50405,\"start\":50394},{\"end\":50413,\"start\":50405},{\"end\":50611,\"start\":50603},{\"end\":50624,\"start\":50611},{\"end\":50632,\"start\":50624},{\"end\":50641,\"start\":50632},{\"end\":50648,\"start\":50641}]", "bib_venue": "[{\"end\":35764,\"start\":35702},{\"end\":36853,\"start\":36787},{\"end\":37294,\"start\":37228},{\"end\":38552,\"start\":38486},{\"end\":39358,\"start\":39296},{\"end\":40775,\"start\":40714},{\"end\":42923,\"start\":42871},{\"end\":43802,\"start\":43740},{\"end\":44349,\"start\":44283},{\"end\":47045,\"start\":46996},{\"end\":47807,\"start\":47745},{\"end\":48746,\"start\":48684},{\"end\":50034,\"start\":49982},{\"end\":35023,\"start\":35014},{\"end\":35290,\"start\":35241},{\"end\":35700,\"start\":35623},{\"end\":36057,\"start\":36005},{\"end\":36437,\"start\":36421},{\"end\":36785,\"start\":36704},{\"end\":37226,\"start\":37145},{\"end\":37679,\"start\":37614},{\"end\":37900,\"start\":37896},{\"end\":38077,\"start\":38017},{\"end\":38484,\"start\":38403},{\"end\":38871,\"start\":38800},{\"end\":39294,\"start\":39217},{\"end\":39807,\"start\":39745},{\"end\":40157,\"start\":40075},{\"end\":40712,\"start\":40636},{\"end\":41240,\"start\":41171},{\"end\":41493,\"start\":41451},{\"end\":41816,\"start\":41772},{\"end\":42112,\"start\":42101},{\"end\":42433,\"start\":42397},{\"end\":42869,\"start\":42802},{\"end\":43271,\"start\":43155},{\"end\":43738,\"start\":43661},{\"end\":44281,\"start\":44200},{\"end\":44808,\"start\":44746},{\"end\":45201,\"start\":45136},{\"end\":45569,\"start\":45517},{\"end\":45929,\"start\":45869},{\"end\":46310,\"start\":46252},{\"end\":46624,\"start\":46592},{\"end\":46994,\"start\":46930},{\"end\":47323,\"start\":47268},{\"end\":47743,\"start\":47666},{\"end\":48248,\"start\":48181},{\"end\":48682,\"start\":48605},{\"end\":49167,\"start\":49068},{\"end\":49572,\"start\":49516},{\"end\":49980,\"start\":49913},{\"end\":50368,\"start\":50296},{\"end\":50722,\"start\":50648}]"}}}, "year": 2023, "month": 12, "day": 17}