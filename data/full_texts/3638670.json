{"id": 3638670, "updated": "2023-10-02 15:01:00.431", "metadata": {"title": "Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation", "authors": "[{\"first\":\"Liang-Chieh\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Yukun\",\"last\":\"Zhu\",\"middle\":[]},{\"first\":\"George\",\"last\":\"Papandreou\",\"middle\":[]},{\"first\":\"Florian\",\"last\":\"Schroff\",\"middle\":[]},{\"first\":\"Hartwig\",\"last\":\"Adam\",\"middle\":[]}]", "venue": "ECCV", "journal": "833-851", "publication_date": {"year": 2018, "month": 2, "day": 7}, "abstract": "Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets, achieving the test set performance of 89.0\\% and 82.1\\% without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow at \\url{https://github.com/tensorflow/models/tree/master/research/deeplab}.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1802.02611", "mag": "2964309882", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/eccv/ChenZPSA18", "doi": "10.1007/978-3-030-01234-2_49"}}, "content": {"source": {"pdf_hash": "503c16d9cb1560f13a7d6baedf8c9f889b22459d", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1802.02611v3.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1802.02611", "status": "GREEN"}}, "grobid": {"id": "ae35b5bfb959888e20f5a828e42a41e6caa6ac38", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/503c16d9cb1560f13a7d6baedf8c9f889b22459d.txt", "contents": "\nEncoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation\n\n\nLiang-Chieh Chen lcchen@google.com \nGoogle Inc\n\n\nYukun Zhu \nGoogle Inc\n\n\nGeorge Papandreou \nGoogle Inc\n\n\nFlorian Schroff fschroff@google.com \nGoogle Inc\n\n\nHartwig Adam hadam@google.com \nGoogle Inc\n\n\nEncoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation\nSemantic image segmentationspatial pyramid poolingencoder- decoderand depthwise separable convolution\nSpatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets, achieving the test set performance of 89.0% and 82.1% without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow at https: //github.com/tensorflow/models/tree/master/research/deeplab.\n\nIntroduction\n\nSemantic segmentation with the goal to assign semantic labels to every pixel in an image [1,2,3,4,5] is one of the fundamental topics in computer vision. Deep convolutional neural networks [6,7,8,9,10] based on the Fully Convolutional Neural Network [8,11] show striking improvement over systems relying on hand-crafted features [12,13,14,15,16,17] on benchmark tasks. In this work, we consider two types of neural networks that use spatial pyramid pooling module [18,19,20] or encoder-decoder structure [21,22] for semantic segmentation, where the former one captures rich contextual information by pooling features at different resolution while the latter one is able to obtain sharp object boundaries.\n\nIn order to capture the contextual information at multiple scales, DeepLabv3  Spatial Pyramid Pooling, or ASPP), while PSPNet [24] performs pooling operations at different grid scales. Even though rich semantic information is encoded in the last feature map, detailed information related to object boundaries is missing due to the pooling or convolutions with striding operations within the network backbone. This could be alleviated by applying the atrous convolution to extract denser feature maps. However, given the design of state-of-art neural networks [7,9,10,25,26] and limited GPU memory, it is computationally prohibitive to extract output feature maps that are 8, or even 4 times smaller than the input resolution. Taking ResNet-101 [25] for example, when applying atrous convolution to extract output features that are 16 times smaller than input resolution, features within the last 3 residual blocks (9 layers) have to be dilated. Even worse, 26 residual blocks (78 layers!) will be affected if output features that are 8 times smaller than input are desired. Thus, it is computationally intensive if denser output features are extracted for this type of models. On the other hand, encoder-decoder models [21,22] lend themselves to faster computation (since no features are dilated) in the encoder path and gradually recover sharp object boundaries in the decoder path. Attempting to combine the advantages from both methods, we propose to enrich the encoder module in the encoder-decoder networks by incorporating the multi-scale contextual information.\n\nIn particular, our proposed model, called DeepLabv3+, extends DeepLabv3 [23] by adding a simple yet effective decoder module to recover the object boundaries, as illustrated in Fig. 1. The rich semantic information is encoded in the output of DeepLabv3, with atrous convolution allowing one to control the density of the encoder features, depending on the budget of computation resources. Furthermore, the decoder module allows detailed object boundary recovery. Motivated by the recent success of depthwise separable convolution [27,28,26,29,30], we also explore this operation and show improvement in terms of both speed and accuracy by adapting the Xception model [26], similar to [31], for the task of semantic segmentation, and applying the atrous separable convolution to both the ASPP and decoder modules. Finally, we demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasts and attain the test set performance of 89.0% and 82.1% without any post-processing, setting a new state-of-the-art.\n\nIn summary, our contributions are:\n\n-We propose a novel encoder-decoder structure which employs DeepLabv3 as a powerful encoder module and a simple yet effective decoder module. -In our structure, one can arbitrarily control the resolution of extracted encoder features by atrous convolution to trade-off precision and runtime, which is not possible with existing encoder-decoder models. -We adapt the Xception model for the segmentation task and apply depthwise separable convolution to both ASPP module and decoder module, resulting in a faster and stronger encoder-decoder network. -Our proposed model attains a new state-of-art performance on PASCAL VOC 2012 and Cityscapes datasets. We also provide detailed analysis of design choices and model variants. -We make our Tensorflow-based implementation of the proposed model publicly available at https://github.com/tensorflow/models/tree/master/ research/deeplab.\n\n\nRelated Work\n\nModels based on Fully Convolutional Networks (FCNs) [8,11] have demonstrated significant improvement on several segmentation benchmarks [1,2,3,4,5]. There are several model variants proposed to exploit the contextual information for segmentation [12,13,14,15,16,17,32,33], including those that employ multi-scale inputs (i.e., image pyramid) [34,35,36,37,38,39] or those that adopt probabilistic graphical models (such as DenseCRF [40] with efficient inference algorithm [41]) [42,43,44,37,45,46,47,48,49,50,51,39]. In this work, we mainly discuss about the models that use spatial pyramid pooling and encoder-decoder structure. Spatial pyramid pooling: Models, such as PSPNet [24] or DeepLab [39,23], perform spatial pyramid pooling [18,19] at several grid scales (including imagelevel pooling [52]) or apply several parallel atrous convolution with different rates (called Atrous Spatial Pyramid Pooling, or ASPP). These models have shown promising results on several segmentation benchmarks by exploiting the multi-scale information.\n\nEncoder-decoder: The encoder-decoder networks have been successfully applied to many computer vision tasks, including human pose estimation [53], object detection [54,55,56], and semantic segmentation [11,57,21,22,58,59,60,61,62,63,64]. Typically, the encoder-decoder networks contain (1) an encoder module that gradually reduces the feature maps and captures higher semantic information, and (2) a decoder module that gradually recovers the spatial information. Building on top of this idea, we propose to use DeepLabv3 [23] as the encoder module and add a simple yet effective decoder module to obtain sharper segmentations. Depthwise separable convolution: Depthwise separable convolution [27,28] or group convolution [7,65], a powerful operation to reduce the computation cost and number of parameters while maintaining similar (or slightly better) performance. This operation has been adopted in many recent neural network designs [66,67,26,29,30,31,68]. In particular, we explore the Xception model [26], similar to [31] for their COCO 2017 detection challenge submission, and show improvement in terms of both accuracy and speed for the task of semantic segmentation.\n\n\nMethods\n\nIn this section, we briefly introduce atrous convolution [69,70,8,71,42] and depthwise separable convolution [27,28,67,26,29]. We then review DeepLabv3 [23] which is used as our encoder module before discussing the proposed decoder module appended to the encoder output. We also present a modified Xception model [26,31] which further improves the performance with faster computation.\n\n\nEncoder-Decoder with Atrous Convolution\n\nAtrous convolution: Atrous convolution, a powerful tool that allows us to explicitly control the resolution of features computed by deep convolutional neural networks and adjust filter's field-of-view in order to capture multi-scale information, generalizes standard convolution operation. In the case of two-dimensional signals, for each location i on the output feature map y and a convolution filter w, atrous convolution is applied over the input feature map x as follows:\n(a) Depthwise conv. (b) Pointwise conv. (c) Atrous depthwise conv. Fig. 3. 3 \u00d7 3\nDepthwise separable convolution decomposes a standard convolution into (a) a depthwise convolution (applying a single filter for each input channel) and (b) a pointwise convolution (combining the outputs from depthwise convolution across channels). In this work, we explore atrous separable convolution where atrous convolution is adopted in the depthwise convolution, as shown in (c) with rate = 2.\ny[i] = k x[i + r \u00b7 k]w[k](1)\nwhere the atrous rate r determines the stride with which we sample the input signal. We refer interested readers to [39] for more details. Note that standard convolution is a special case in which rate r = 1. The filter's field-of-view is adaptively modified by changing the rate value. Depthwise separable convolution: Depthwise separable convolution, factorizing a standard convolution into a depthwise convolution followed by a pointwise convolution (i.e., 1 \u00d7 1 convolution), drastically reduces computation complexity. Specifically, the depthwise convolution performs a spatial convolution independently for each input channel, while the pointwise convolution is employed to combine the output from the depthwise convolution. In the TensorFlow [72] implementation of depthwise separable convolution, atrous convolution has been supported in the depthwise convolution (i.e., the spatial convolution), as illustrated in Fig. 3. In this work, we refer the resulting convolution as atrous separable convolution, and found that atrous separable convolution significantly reduces the computation complexity of proposed model while maintaining similar (or better) performance.\n\nDeepLabv3 as encoder: DeepLabv3 [23] employs atrous convolution [69,70,8,71] to extract the features computed by deep convolutional neural networks at an arbitrary resolution. Here, we denote output stride as the ratio of input image spatial resolution to the final output resolution (before global pooling or fullyconnected layer). For the task of image classification, the spatial resolution of the final feature maps is usually 32 times smaller than the input image resolution and thus output stride = 32. For the task of semantic segmentation, one can adopt output stride = 16 (or 8) for denser feature extraction by removing the striding in the last one (or two) block(s) and applying the atrous convolution correspondingly (e.g., we apply rate = 2 and rate = 4 to the last two blocks respectively for output stride = 8). Additionally, DeepLabv3 augments the Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales by applying atrous convolution with different rates, with the image-level fea-tures [52]. We use the last feature map before logits in the original DeepLabv3 as the encoder output in our proposed encoder-decoder structure. Note the encoder output feature map contains 256 channels and rich semantic information. Besides, one could extract features at an arbitrary resolution by applying the atrous convolution, depending on the computation budget.\n\nProposed decoder: The encoder features from DeepLabv3 are usually computed with output stride = 16. In the work of [23], the features are bilinearly upsampled by a factor of 16, which could be considered a naive decoder module. However, this naive decoder module may not successfully recover object segmentation details. We thus propose a simple yet effective decoder module, as illustrated in Fig. 2. The encoder features are first bilinearly upsampled by a factor of 4 and then concatenated with the corresponding low-level features [73] from the network backbone that have the same spatial resolution (e.g., Conv2 before striding in ResNet-101 [25]). We apply another 1 \u00d7 1 convolution on the low-level features to reduce the number of channels, since the corresponding lowlevel features usually contain a large number of channels (e.g., 256 or 512) which may outweigh the importance of the rich encoder features (only 256 channels in our model) and make the training harder. After the concatenation, we apply a few 3 \u00d7 3 convolutions to refine the features followed by another simple bilinear upsampling by a factor of 4. We show in Sec. 4 that using output stride = 16 for the encoder module strikes the best trade-off between speed and accuracy. The performance is marginally improved when using output stride = 8 for the encoder module at the cost of extra computation complexity.\n\n\nModified Aligned Xception\n\nThe Xception model [26] has shown promising image classification results on Im-ageNet [74] with fast computation. More recently, the MSRA team [31] modifies the Xception model (called Aligned Xception) and further pushes the performance in the task of object detection. Motivated by these findings, we work in the same direction to adapt the Xception model for the task of semantic image segmentation. In particular, we make a few more changes on top of MSRA's modifications, namely (1) deeper Xception same as in [31] except that we do not modify the entry flow network structure for fast computation and memory efficiency, (2) all max pooling operations are replaced by depthwise separable convolution with striding, which enables us to apply atrous separable convolution to extract feature maps at an arbitrary resolution (another option is to extend the atrous algorithm to max pooling operations), and (3) extra batch normalization [75] and ReLU activation are added after each 3 \u00d7 3 depthwise convolution, similar to MobileNet design [29]. See Fig. 4 for details.\n\n\nExperimental Evaluation\n\nWe employ ImageNet-1k [74] pretrained ResNet-101 [25] or modified aligned Xception [26,31] to extract dense feature maps by atrous convolution. Our implementation is built on TensorFlow [72] and is made publicly available.  The proposed models are evaluated on the PASCAL VOC 2012 semantic segmentation benchmark [1] which contains 20 foreground object classes and one background class. The original dataset contains 1, 464 (train), 1, 449 (val ), and 1, 456 (test) pixel-level annotated images. We augment the dataset by the extra annotations provided by [76], resulting in 10, 582 (trainaug) training images. The performance is measured in terms of pixel intersection-over-union averaged across the 21 classes (mIOU).\n\nWe follow the same training protocol as in [23] and refer the interested readers to [23] for details. In short, we employ the same learning rate schedule (i.e., \"poly\" policy [52] and same initial learning rate 0.007), crop size 513 \u00d7 513, fine-tuning batch normalization parameters [75] when output stride = 16, and random scale data augmentation during training. Note that we also include batch normalization parameters in the proposed decoder module. Our proposed model is trained end-to-end without piecewise pretraining of each component.\n\n\nDecoder Design Choices\n\nWe define \"DeepLabv3 feature map\" as the last feature map computed by DeepLabv3 (i.e., the features containing ASPP features and image-level features), and [k \u00d7 k, f ] as a convolution operation with kernel k \u00d7 k and f filters.\n\nWhen employing output stride = 16, ResNet-101 based DeepLabv3 [23] bilinearly upsamples the logits by 16 during both training and evaluation. This simple bilinear upsampling could be considered as a naive decoder design, attaining the performance of 77.21% [23] on PASCAL VOC 2012 val set and is 1.2% better than not using this naive decoder during training (i.e., downsampling groundtruth during training). To improve over this naive baseline, our proposed model \"DeepLabv3+\" adds the decoder module on top of the encoder output, as shown in Fig. 2. In the decoder module, we consider three places for different design choices, namely (1) the 1 \u00d7 1 convolution used to reduce the channels of the low-level feature map from the encoder module, (2) the 3 \u00d7 3 convolution used to obtain sharper segmentation results, and (3) what encoder low-level features should be used.\n\nTo evaluate the effect of the 1 \u00d7 1 convolution in the decoder module, we employ [3 \u00d7 3, 256] and the Conv2 features from ResNet-101 network backbone, i.e., the last feature map in res2x residual block (to be concrete, we use the feature map before striding). As shown in Tab. 1, reducing the channels of the low-level feature map from the encoder module to either 48 or 32 leads to better performance. We thus adopt [1 \u00d7 1, 48] for channel reduction.\n\nWe then design the 3 \u00d7 3 convolution structure for the decoder module and report the findings in Tab. 2. We find that after concatenating the Conv2 feature map (before striding) with DeepLabv3 feature map, it is more effective to employ two 3\u00d73 convolution with 256 filters than using simply one or three convolutions. Changing the number of filters from 256 to 128 or the kernel size from 3 \u00d7 3 to 1\u00d71 degrades performance. We also experiment with the case where both Conv2 and Conv3 feature maps are exploited in the decoder module. In this case, the decoder feature map are gradually upsampled by 2, concatenated with Conv3 first and then Conv2, and each will be refined by the [3 \u00d7 3, 256] operation. The whole decoding procedure is then similar to the U-Net/SegNet design [21,22]. However, we have not observed significant improvement. Thus, in the end, we adopt the very simple yet effective decoder module: the concatenation of the DeepLabv3 feature map and the channel-reduced Conv2 feature map are refined by two [3 \u00d7 3, 256] operations. Note that our proposed DeepLabv3+ model has output stride = 4. We do not pursue further denser output feature map (i.e., output stride < 4) given the limited GPU resources.\n\n\nResNet-101 as Network Backbone\n\nTo compare the model variants in terms of both accuracy and speed, we report mIOU and Multiply-Adds in Tab. 3 when using ResNet-101 [25] as network backbone in the proposed DeepLabv3+ model. Thanks to atrous convolution, we  Table 4. Single-model error rates on ImageNet-1K validation set.\n\nBaseline: The first row block in Tab. 3 contains the results from [23] showing that extracting denser feature maps during evaluation (i.e., eval output stride = 8) and adopting multi-scale inputs increases performance. Besides, adding leftright flipped inputs doubles the computation complexity with only marginal performance improvement.\n\nAdding decoder: The second row block in Tab. 3 contains the results when adopting the proposed decoder structure. The performance is improved from 77.21% to 78.85% or 78.51% to 79.35% when using eval output stride = 16 or 8, respectively, at the cost of about 20B extra computation overhead. The performance is further improved when using multi-scale and left-right flipped inputs.\n\nCoarser feature maps: We also experiment with the case when using train output stride = 32 (i.e., no atrous convolution at all during training) for fast computation. As shown in the third row block in Tab. 3, adding the decoder brings about 2% improvement while only 74.20B Multiply-Adds are required. However, the performance is always about 1% to 1.5% below the case in which we employ train output stride = 16 and different eval output stride values. We thus prefer using output stride = 16 or 8 during training or evaluation depending on the complexity budget.\n\n\nXception as Network Backbone\n\nWe further employ the more powerful Xception [26] as network backbone. Following [31], we make a few more changes, as described in Sec. 3.2.\n\nImageNet pretraining: The proposed Xception network is pretrained on ImageNet-1k dataset [74] with similar training protocol in [26]. Specifically, we adopt Nesterov momentum optimizer with momentum = 0.9, initial learning rate = 0.05, rate decay = 0.94 every 2 epochs, and weight decay 4e \u2212 5. We use asynchronous training with 50 GPUs and each GPU has batch size 32 with image size 299\u00d7299. We did not tune the hyper-parameters very hard as the goal is to pretrain the model on ImageNet for semantic segmentation. We report the single-model error rates on the validation set in Tab. 4 along with the baseline reproduced ResNet-101 [25] under the same training protocol. We have observed 0.75% and 0.29% performance degradation for Top1 and Top5 accuracy when not adding the extra batch normalization and ReLU after each 3 \u00d7 3 depthwise convolution in the modified Xception.\n\nThe results of using the proposed Xception as network backbone for semantic segmentation are reported in Tab. 5.\n\nBaseline: We first report the results without using the proposed decoder in the first row block in Tab. 5, which shows that employing Xception as network backbone improves the performance by about 2% when train output stride = eval output stride = 16 over the case where ResNet-101 is used. Further improvement can also be obtained by using eval output stride = 8, multi-scale inputs during inference and adding left-right flipped inputs. Note that we do not employ the multi-grid method [77,78,23], which we found does not improve the performance.\n\nAdding decoder: As shown in the second row block in Tab. 5, adding decoder brings about 0.8% improvement when using eval output stride = 16 for all the different inference strategies. The improvement becomes less when using eval output stride = 8.\n\nUsing depthwise separable convolution: Motivated by the efficient computation of depthwise separable convolution, we further adopt it in the ASPP and the decoder modules. As shown in the third row block in Tab. 5, the computation complexity in terms of Multiply-Adds is significantly reduced by 33% to 41%, while similar mIOU performance is obtained.\n\nPretraining on COCO: For comparison with other state-of-art models, we further pretrain our proposed DeepLabv3+ model on MS-COCO dataset [79], which yields about extra 2% improvement for all different inference strategies.\n\nPretraining on JFT: Similar to [23], we also employ the proposed Xception model that has been pretrained on both ImageNet-1k [74] and JFT-300M dataset [80,26,81], which brings extra 0.8% to 1% improvement.\n\nTest set results: Since the computation complexity is not considered in the benchmark evaluation, we thus opt for the best performance model and train it with output stride = 8 and frozen batch normalization parameters. In the end, our 'DeepLabv3+' achieves the performance of 87.8% and 89.0% without and with JFT dataset pretraining.\n\nQualitative results: We provide visual results of our best model in Fig. 6. As shown in the figure, our model is able to segment objects very well without any post-processing.\n\nFailure mode: As shown in the last row of Fig. 6, our model has difficulty in segmenting (a) sofa vs. chair, (b) heavily occluded objects, and (c) objects with rare view.\n\n\nImprovement along Object Boundaries\n\nIn this subsection, we evaluate the segmentation accuracy with the trimap experiment [14,40,39] to quantify the accuracy of the proposed decoder module near object boundaries. Specifically, we apply the morphological dilation on 'void' label annotations on val set, which typically occurs around object boundaries. We then compute the mean IOU for those pixels that are within the dilated band (called trimap) of 'void' labels. As shown in Fig. 5 (a), employing the proposed decoder for both ResNet-101 [25] and Xception [26] network backbones improves the performance compared to the naive bilinear upsampling. The improvement is more significant when the dilated band is narrow. We have observed 4.8% and 5.4% mIOU improvement for ResNet-101 and Xception respectively at the smallest trimap width as shown in the figure. We also visualize the effect of employing the proposed decoder in Fig. 5 (b).\n\n\nExperimental Results on Cityscapes\n\nIn this section, we experiment DeepLabv3+ on the Cityscapes dataset [3], a large-scale dataset containing high quality pixel-level annotations of 5000 images (2975, 500, and 1525 for the training, validation, and test sets respectively) and about 20000 coarsely annotated images.\n\nAs shown in Tab. 7 (a), employing the proposed Xception model as network backbone (denoted as X-65) on top of DeepLabv3 [23], which includes the ASPP    module and image-level features [52], attains the performance of 77.33% on the validation set. Adding the proposed decoder module significantly improves the performance to 78.79% (1.46% improvement). We notice that removing the augmented image-level feature improves the performance to 79.14%, showing that in DeepLab model, the image-level features are more effective on the PASCAL VOC 2012 dataset. We also discover that on the Cityscapes dataset, it is effective to increase more layers in the entry flow in the Xception [26], the same as what [31] did for the object detection task. The resulting model building on top of the deeper network backbone (denoted as X-71 in the table), attains the best performance of 79.55% on the validation set.\n\nAfter finding the best model variant on val set, we then further fine-tune the model on the coarse annotations in order to compete with other state-of-art  models. As shown in Tab. 7 (b), our proposed DeepLabv3+ attains a performance of 82.1% on the test set, setting a new state-of-art performance on Cityscapes.\n\n\nConclusion\n\nOur proposed model \"DeepLabv3+\" employs the encoder-decoder structure where DeepLabv3 is used to encode the rich contextual information and a simple yet effective decoder module is adopted to recover the object boundaries. One could also apply the atrous convolution to extract the encoder features at an arbitrary resolution, depending on the available computation resources. We also explore the Xception model and atrous separable convolution to make the proposed model faster and stronger. Finally, our experimental results show that the proposed model sets a new state-of-the-art performance on PASCAL VOC 2012 and Cityscapes datasets.\n\n\n[23] applies several parallel atrous convolution with different rates (called Atrous arXiv:1802.02611v3 [cs.CV]\n\nFig. 1 .\n1We improve DeepLabv3, which employs the spatial pyramid pooling module (a), with the encoder-decoder structure (b). The proposed model, DeepLabv3+, contains rich semantic information from the encoder module, while the detailed object boundaries are recovered by the simple yet effective decoder module. The encoder module allows us to extract features at an arbitrary resolution by applying atrous convolution.\n\nFig. 4 .\n4We modify the Xception as follows: (1) more layers (same as MSRA's modification except the changes in Entry flow), (2) all the max pooling operations are replaced by depthwise separable convolutions with striding, and (3) extra batch normalization and ReLU are added after each 3 \u00d7 3 depthwise convolution, similar to MobileNet.\n\nFig. 5 .\n5(a) mIOU as a function of trimap band width around the object boundaries when employing train output stride = eval output stride = 16. BU: Bilinear upsampling. (b) Qualitative effect of employing the proposed decoder module compared with the naive bilinear upsampling (denoted as BU). In the examples, we adopt Xception as feature extractor and train output stride = eval output stride = 16.\n\nFig. 6 .\n6Visualization results on val set. The last row shows a failure mode.\n\n\nPASCAL VOC 2012 val set. Effect of decoder 1 \u00d7 1 convolution used to reduce the channels of low-level feature map from the encoder module. We fix the other components in the decoder structure as using [3 \u00d7 3, 256] and Conv2. Inference strategy on the PASCAL VOC 2012 val set using ResNet-101. train OS: The output stride used during training. eval OS: The output stride used during evaluation. Decoder: Employing the proposed decoder structure. MS: Multiscale inputs during evaluation. Flip: Adding left-right flipped inputs.are able to obtain features at different resolutions during training and evaluation using a single model.Channels \n8 \n16 \n32 \n48 \n64 \n\nmIOU 77.61% 77.92% 78.16% 78.21% 77.94% \nTable 1. Features \n3 \u00d7 3 Conv \nmIOU \nConv2 Conv3 \nStructure \n\n[3 \u00d7 3, 256] 78.21% \n[3 \u00d7 3, 256] \u00d7 2 78.85% \n[3 \u00d7 3, 256] \u00d7 3 78.02% \n[3 \u00d7 3, 128] 77.25% \n[1 \u00d7 1, 256] 78.07% \n[3 \u00d7 3, 256] 78.61% \n\nTable 2. Effect of decoder structure when fixing [1 \u00d7 1, 48] to reduce the encoder \nfeature channels. We found that it is most effective to use the Conv2 (before striding) \nfeature map and two extra [3 \u00d7 3, 256] operations. Performance on VOC 2012 val set. \n\nEncoder \nDecoder MS Flip mIOU Multiply-Adds \ntrain OS eval OS \n\n16 \n16 \n77.21% \n81.02B \n16 \n8 \n78.51% \n276.18B \n16 \n8 \n79.45% \n2435.37B \n16 \n8 \n79.77% \n4870.59B \n\n16 \n16 \n78.85% \n101.28B \n16 \n16 \n80.09% \n898.69B \n16 \n16 \n80.22% \n1797.23B \n16 \n8 \n79.35% \n297.92B \n16 \n8 \n80.43% \n2623.61B \n16 \n8 \n80.57% \n5247.07B \n\n32 \n32 \n75.43% \n52.43B \n32 \n32 \n77.37% \n74.20B \n32 \n16 \n77.80% \n101.28B \n32 \n8 \n77.92% \n297.92B \nTable 3. Model \nTop-1 Error Top-5 Error \n\nReproduced ResNet-101 22.40% \n6.02% \nModified Xception \n20.19% \n5.17% \n\n\n\nEncoderDecoderMS Flip SC COCO JFT mIOU Multiply-Adds train OS eval OS Inference strategy on the PASCAL VOC 2012 val set when using modified Xception. train OS: The output stride used during training. eval OS: The output stride used during evaluation. Decoder: Employing the proposed decoder structure. MS: Multi-scale inputs during evaluation. Flip: Adding left-right flipped inputs. SC: Adopting depthwise separable convolution for both ASPP and decoder modules. COCO: Models pretrained on MS-COCO. JFT: Models pretrained on JFT.16 \n16 \n79.17% \n68.00B \n16 \n16 \n80.57% \n601.74B \n16 \n16 \n80.79% \n1203.34B \n16 \n8 \n79.64% \n240.85B \n16 \n8 \n81.15% \n2149.91B \n16 \n8 \n81.34% \n4299.68B \n\n16 \n16 \n79.93% \n89.76B \n16 \n16 \n81.38% \n790.12B \n16 \n16 \n81.44% \n1580.10B \n16 \n8 \n80.22% \n262.59B \n16 \n8 \n81.60% \n2338.15B \n16 \n8 \n81.63% \n4676.16B \n\n16 \n16 \n79.79% \n54.17B \n16 \n16 \n81.21% \n928.81B \n16 \n8 \n80.02% \n177.10B \n16 \n8 \n81.39% \n3055.35B \n\n16 \n16 \n82.20% \n54.17B \n16 \n16 \n83.34% \n928.81B \n16 \n8 \n82.45% \n177.10B \n16 \n8 \n83.58% \n3055.35B \n\n16 \n16 \n83.03% \n54.17B \n16 \n16 \n84.22% \n928.81B \n16 \n8 \n83.39% \n177.10B \n16 \n8 \n84.56% \n3055.35B \nTable 5. \n\nTable 6 .\n6PASCAL VOC 2012 test set results with top-performing models.\n\n\nBackbone Decoder ASPP Image-Level mIOU (a) val set results (b) test set resultsTable 7.(a) DeepLabv3+ on the Cityscapes val set when trained with train fine set. (b) DeepLabv3+ on Cityscapes test set. Coarse: Use train extra set (coarse annotations) as well. Only a few top models are listed in this table.X-65 \n77.33 \nX-65 \n78.79 \nX-65 \n79.14 \nX-71 \n79.55 \n\nMethod \nCoarse mIOU \n\nResNet-38 [83] \n80.6 \nPSPNet [24] \n81.2 \nMapillary [86] \n82.0 \n\nDeepLabv3 \n81.3 \n\nDeepLabv3+ \n82.1 \n\n\n\nThe pascal visual object classes challenge a retrospective. M Everingham, S M A Eslami, L V Gool, C K I Williams, J Winn, A Zisserman, IJCVEveringham, M., Eslami, S.M.A., Gool, L.V., Williams, C.K.I., Winn, J., Zisser- man, A.: The pascal visual object classes challenge a retrospective. IJCV (2014)\n\nThe role of context for object detection and semantic segmentation in the wild. R Mottaghi, X Chen, X Liu, N G Cho, S W Lee, S Fidler, R Urtasun, A Yuille, CVPR. Mottaghi, R., Chen, X., Liu, X., Cho, N.G., Lee, S.W., Fidler, S., Urtasun, R., Yuille, A.: The role of context for object detection and semantic segmentation in the wild. In: CVPR. (2014)\n\nThe cityscapes dataset for semantic urban scene understanding. M Cordts, M Omran, S Ramos, T Rehfeld, M Enzweiler, R Benenson, U Franke, S Roth, B Schiele, CVPR.Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Roth, S., Schiele, B.: The cityscapes dataset for semantic urban scene understanding. In: CVPR. (2016)\n\nB Zhou, H Zhao, X Puig, S Fidler, A Barriuso, A Torralba, Scene parsing through ade20k dataset. Zhou, B., Zhao, H., Puig, X., Fidler, S., Barriuso, A., Torralba, A.: Scene parsing through ade20k dataset. In: CVPR. (2017)\n\nCOCO-Stuff: Thing and stuff classes in context. H Caesar, J Uijlings, V Ferrari, In: CVPR. Caesar, H., Uijlings, J., Ferrari, V.: COCO-Stuff: Thing and stuff classes in context. In: CVPR. (2018)\n\nGradient-based learning applied to document recognition. Y Lecun, L Bottou, Y Bengio, P Haffner, Proc. IEEE. IEEELeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to document recognition. In: Proc. IEEE. (1998)\n\nA Krizhevsky, I Sutskever, G E Hinton, Imagenet classification with deep convolutional neural networks. In: NIPS. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convolutional neural networks. In: NIPS. (2012)\n\nOverfeat: Integrated recognition, localization and detection using convolutional networks. P Sermanet, D Eigen, X Zhang, M Mathieu, R Fergus, Y Lecun, ICLR.Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., LeCun, Y.: Overfeat: Integrated recognition, localization and detection using convolutional networks. In: ICLR. (2014)\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, ICLR.Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition. In: ICLR. (2015)\n\nC Szegedy, W Liu, Y Jia, P Sermanet, S Reed, D Anguelov, D Erhan, V Vanhoucke, A Rabinovich, Going deeper with convolutions. In: CVPR. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., Rabinovich, A.: Going deeper with convolutions. In: CVPR. (2015)\n\nFully convolutional networks for semantic segmentation. J Long, E Shelhamer, T Darrell, CVPR. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic segmentation. In: CVPR. (2015)\n\nMultiscale conditional random fields for image labeling. X He, R S Zemel, M Carreira-Perpindn, CVPR.He, X., Zemel, R.S., Carreira-Perpindn, M.: Multiscale conditional random fields for image labeling. In: CVPR. (2004)\n\nTextonboost for image understanding: Multi-class object recognition and segmentation by jointly modeling texture, layout, and context. J Shotton, J Winn, C Rother, A Criminisi, IJCVShotton, J., Winn, J., Rother, C., Criminisi, A.: Textonboost for image understand- ing: Multi-class object recognition and segmentation by jointly modeling texture, layout, and context. IJCV (2009)\n\nRobust higher order potentials for enforcing label consistency. P Kohli, P H Torr, IJCV. 823Kohli, P., Torr, P.H., et al.: Robust higher order potentials for enforcing label consistency. IJCV 82(3) (2009) 302-324\n\nAssociative hierarchical crfs for object class image segmentation. L Ladicky, C Russell, P Kohli, P H Torr, Ladicky, L., Russell, C., Kohli, P., Torr, P.H.: Associative hierarchical crfs for object class image segmentation. In: ICCV. (2009)\n\nDecomposing a scene into geometric and semantically consistent regions. S Gould, R Fulton, D Koller, Gould, S., Fulton, R., Koller, D.: Decomposing a scene into geometric and seman- tically consistent regions. In: ICCV. (2009)\n\nDescribing the scene as a whole: Joint object detection, scene classification and semantic segmentation. J Yao, S Fidler, R Urtasun, CVPR.Yao, J., Fidler, S., Urtasun, R.: Describing the scene as a whole: Joint object detection, scene classification and semantic segmentation. In: CVPR. (2012)\n\nThe pyramid match kernel: Discriminative classification with sets of image features. K Grauman, T Darrell, ICCV.Grauman, K., Darrell, T.: The pyramid match kernel: Discriminative classification with sets of image features. In: ICCV. (2005)\n\nBeyond bags of features: Spatial pyramid matching for recognizing natural scene categories. S Lazebnik, C Schmid, J Ponce, CVPR.Lazebnik, S., Schmid, C., Ponce, J.: Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories. In: CVPR. (2006)\n\nSpatial pyramid pooling in deep convolutional networks for visual recognition. K He, X Zhang, S Ren, J Sun, ECCV.He, K., Zhang, X., Ren, S., Sun, J.: Spatial pyramid pooling in deep convolutional networks for visual recognition. In: ECCV. (2014)\n\nU-net: Convolutional networks for biomedical image segmentation. O Ronneberger, P Fischer, T Brox, MICCAI.Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomed- ical image segmentation. In: MICCAI. (2015)\n\nSegnet: A deep convolutional encoder-decoder architecture for image segmentation. V Badrinarayanan, A Kendall, R Cipolla, PAMIBadrinarayanan, V., Kendall, A., Cipolla, R.: Segnet: A deep convolutional encoder-decoder architecture for image segmentation. PAMI (2017)\n\nRethinking atrous convolution for semantic image segmentation. L C Chen, G Papandreou, F Schroff, H Adam, arXiv:1706.05587Chen, L.C., Papandreou, G., Schroff, F., Adam, H.: Rethinking atrous convolution for semantic image segmentation. arXiv:1706.05587 (2017)\n\nPyramid scene parsing network. H Zhao, J Shi, X Qi, X Wang, J Jia, Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J.: Pyramid scene parsing network. In: CVPR. (2017)\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, CVPR.He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR. (2016)\n\nXception: Deep learning with depthwise separable convolutions. F Chollet, Chollet, F.: Xception: Deep learning with depthwise separable convolutions. In: CVPR. (2017)\n\nRigid-motion scattering for image classification. L Sifre, PhD thesisSifre, L.: Rigid-motion scattering for image classification. PhD thesis (2014)\n\nLearning visual representations at scale. ICLR invited talk. V Vanhoucke, Vanhoucke, V.: Learning visual representations at scale. ICLR invited talk (2014)\n\nA G Howard, M Zhu, B Chen, D Kalenichenko, W Wang, T Weyand, M Andreetto, H Adam, arXiv:1704.04861Mobilenets: Efficient convolutional neural networks for mobile vision applications. Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., An- dreetto, M., Adam, H.: Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv:1704.04861 (2017)\n\nShufflenet: An extremely efficient convolutional neural network for mobile devices. X Zhang, X Zhou, M Lin, J Sun, In: CVPR. Zhang, X., Zhou, X., Lin, M., Sun, J.: Shufflenet: An extremely efficient convolu- tional neural network for mobile devices. In: CVPR. (2018)\n\nDeformable convolutional networks -coco detection and segmentation challenge. H Qi, Z Zhang, B Xiao, H Hu, B Cheng, Y Wei, J Dai, Qi, H., Zhang, Z., Xiao, B., Hu, H., Cheng, B., Wei, Y., Dai, J.: Deformable convolutional networks -coco detection and segmentation challenge 2017 entry. ICCV COCO Challenge Workshop (2017)\n\nFeedforward semantic segmentation with zoom-out features. M Mostajabi, P Yadollahpour, G Shakhnarovich, CVPR.Mostajabi, M., Yadollahpour, P., Shakhnarovich, G.: Feedforward semantic seg- mentation with zoom-out features. In: CVPR. (2015)\n\nConvolutional feature masking for joint object and stuff segmentation. J Dai, K He, J Sun, In: CVPR. Dai, J., He, K., Sun, J.: Convolutional feature masking for joint object and stuff segmentation. In: CVPR. (2015)\n\nLearning hierarchical features for scene labeling. C Farabet, C Couprie, L Najman, Y Lecun, PAMIFarabet, C., Couprie, C., Najman, L., LeCun, Y.: Learning hierarchical features for scene labeling. PAMI (2013)\n\nPredicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture. D Eigen, R Fergus, ICCV.Eigen, D., Fergus, R.: Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture. In: ICCV. (2015)\n\nRecurrent convolutional neural networks for scene labeling. P Pinheiro, R Collobert, ICML. Pinheiro, P., Collobert, R.: Recurrent convolutional neural networks for scene labeling. In: ICML. (2014)\n\nEfficient piecewise training of deep structured models for semantic segmentation. G Lin, C Shen, A Van Den Hengel, I Reid, CVPR.Lin, G., Shen, C., van den Hengel, A., Reid, I.: Efficient piecewise training of deep structured models for semantic segmentation. In: CVPR. (2016)\n\nAttention to scale: Scaleaware semantic image segmentation. L C Chen, Y Yang, J Wang, W Xu, A L Yuille, CVPR.Chen, L.C., Yang, Y., Wang, J., Xu, W., Yuille, A.L.: Attention to scale: Scale- aware semantic image segmentation. In: CVPR. (2016)\n\nDeeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. L C Chen, G Papandreou, I Kokkinos, K Murphy, A L Yuille, TPAMIChen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. TPAMI (2017)\n\nEfficient inference in fully connected crfs with gaussian edge potentials. P Kr\u00e4henb\u00fchl, V Koltun, NIPS. Kr\u00e4henb\u00fchl, P., Koltun, V.: Efficient inference in fully connected crfs with gaussian edge potentials. In: NIPS. (2011)\n\nFast high-dimensional filtering using the permutohedral lattice. A Adams, J Baek, M A Davis, Eurographics. Adams, A., Baek, J., Davis, M.A.: Fast high-dimensional filtering using the per- mutohedral lattice. In: Eurographics. (2010)\n\nSemantic image segmentation with deep convolutional nets and fully connected crfs. L C Chen, G Papandreou, I Kokkinos, K Murphy, A L Yuille, ICLR.Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Semantic image segmentation with deep convolutional nets and fully connected crfs. In: ICLR. (2015)\n\nMaterial recognition in the wild with the materials in context database. S Bell, P Upchurch, N Snavely, K Bala, CVPR.Bell, S., Upchurch, P., Snavely, N., Bala, K.: Material recognition in the wild with the materials in context database. In: CVPR. (2015)\n\nS Zheng, S Jayasumana, B Romera-Paredes, V Vineet, Z Su, D Du, C Huang, P Torr, Conditional random fields as recurrent neural networks. In: ICCV. Zheng, S., Jayasumana, S., Romera-Paredes, B., Vineet, V., Su, Z., Du, D., Huang, C., Torr, P.: Conditional random fields as recurrent neural networks. In: ICCV. (2015)\n\nSemantic image segmentation via deep parsing network. Z Liu, X Li, P Luo, C C Loy, X Tang, ICCV.Liu, Z., Li, X., Luo, P., Loy, C.C., Tang, X.: Semantic image segmentation via deep parsing network. In: ICCV. (2015)\n\nWeakly-and semisupervised learning of a dcnn for semantic image segmentation. G Papandreou, L C Chen, K Murphy, A L Yuille, ICCV.Papandreou, G., Chen, L.C., Murphy, K., Yuille, A.L.: Weakly-and semi- supervised learning of a dcnn for semantic image segmentation. In: ICCV. (2015)\n\nA G Schwing, R Urtasun, arXiv:1503.02351Fully connected deep structured networks. Schwing, A.G., Urtasun, R.: Fully connected deep structured networks. arXiv:1503.02351 (2015)\n\nLearning sparse high dimensional filters: Image filtering, dense crfs and bilateral neural networks. V Jampani, M Kiefel, P V Gehler, In: CVPR. Jampani, V., Kiefel, M., Gehler, P.V.: Learning sparse high dimensional filters: Image filtering, dense crfs and bilateral neural networks. In: CVPR. (2016)\n\nGaussian conditional random field network for semantic segmentation. R Vemulapalli, O Tuzel, M Y Liu, R Chellappa, CVPR.Vemulapalli, R., Tuzel, O., Liu, M.Y., Chellappa, R.: Gaussian conditional random field network for semantic segmentation. In: CVPR. (2016)\n\nFast, exact and multi-scale inference for semantic image segmentation with deep Gaussian CRFs. S Chandra, I Kokkinos, ECCV.Chandra, S., Kokkinos, I.: Fast, exact and multi-scale inference for semantic image segmentation with deep Gaussian CRFs. In: ECCV. (2016)\n\nDense and low-rank gaussian crfs using deep embeddings. S Chandra, N Usunier, I Kokkinos, Chandra, S., Usunier, N., Kokkinos, I.: Dense and low-rank gaussian crfs using deep embeddings. In: ICCV. (2017)\n\nW Liu, A Rabinovich, A C Berg, arXiv:1506.04579Parsenet: Looking wider to see better. Liu, W., Rabinovich, A., Berg, A.C.: Parsenet: Looking wider to see better. arXiv:1506.04579 (2015)\n\nStacked hourglass networks for human pose estimation. A Newell, K Yang, J Deng, ECCV.Newell, A., Yang, K., Deng, J.: Stacked hourglass networks for human pose esti- mation. In: ECCV. (2016)\n\nFeature pyramid networks for object detection. T Y Lin, P Doll\u00e1r, R Girshick, K He, B Hariharan, S Belongie, Lin, T.Y., Doll\u00e1r, P., Girshick, R., He, K., Hariharan, B., Belongie, S.: Feature pyramid networks for object detection. In: CVPR. (2017)\n\nA Shrivastava, R Sukthankar, J Malik, A Gupta, arXiv:1612.06851Beyond skip connections: Top-down modulation for object detection. Shrivastava, A., Sukthankar, R., Malik, J., Gupta, A.: Beyond skip connections: Top-down modulation for object detection. arXiv:1612.06851 (2016)\n\nC Y Fu, W Liu, A Ranga, A Tyagi, A C Berg, arXiv:1701.06659Dssd: Deconvolutional single shot detector. Fu, C.Y., Liu, W., Ranga, A., Tyagi, A., Berg, A.C.: Dssd: Deconvolutional single shot detector. arXiv:1701.06659 (2017)\n\nLearning deconvolution network for semantic segmentation. H Noh, S Hong, B Han, ICCV. Noh, H., Hong, S., Han, B.: Learning deconvolution network for semantic segmen- tation. In: ICCV. (2015)\n\nRefinenet: Multi-path refinement networks with identity mappings for high-resolution semantic segmentation. G Lin, A Milan, C Shen, I Reid, Lin, G., Milan, A., Shen, C., Reid, I.: Refinenet: Multi-path refinement networks with identity mappings for high-resolution semantic segmentation. In: CVPR. (2017)\n\nFull-resolution residual networks for semantic segmentation in street scenes. T Pohlen, A Hermans, M Mathias, B Leibe, Pohlen, T., Hermans, A., Mathias, M., Leibe, B.: Full-resolution residual networks for semantic segmentation in street scenes. In: CVPR. (2017)\n\nLarge kernel matters-improve semantic segmentation by global convolutional network. C Peng, X Zhang, G Yu, G Luo, J Sun, Peng, C., Zhang, X., Yu, G., Luo, G., Sun, J.: Large kernel matters-improve semantic segmentation by global convolutional network. In: CVPR. (2017)\n\nGated feedback refinement network for dense image labeling. M A Islam, M Rochan, N D Bruce, Y Wang, Islam, M.A., Rochan, M., Bruce, N.D., Wang, Y.: Gated feedback refinement network for dense image labeling. In: CVPR. (2017)\n\nThe devil is in the decoder. Z Wojna, V Ferrari, S Guadarrama, N Silberman, L C Chen, A Fathi, J Uijlings, Wojna, Z., Ferrari, V., Guadarrama, S., Silberman, N., Chen, L.C., Fathi, A., Uijlings, J.: The devil is in the decoder. In: BMVC. (2017)\n\nStacked deconvolutional network for semantic segmentation. J Fu, J Liu, Y Wang, H Lu, arXiv:1708.04943Fu, J., Liu, J., Wang, Y., Lu, H.: Stacked deconvolutional network for semantic segmentation. arXiv:1708.04943 (2017)\n\nExfuse: Enhancing feature fusion for semantic segmentation. Z Zhang, X Zhang, C Peng, D Cheng, J Sun, In: ECCV. Zhang, Z., Zhang, X., Peng, C., Cheng, D., Sun, J.: Exfuse: Enhancing feature fusion for semantic segmentation. In: ECCV. (2018)\n\nAggregated residual transformations for deep neural networks. S Xie, R Girshick, P Dollr, Z Tu, K He, Xie, S., Girshick, R., Dollr, P., Tu, Z., He, K.: Aggregated residual transformations for deep neural networks. In: CVPR. (2017)\n\nFlattened convolutional neural networks for feedforward acceleration. J Jin, A Dundar, E Culurciello, arXiv:1412.5474Jin, J., Dundar, A., Culurciello, E.: Flattened convolutional neural networks for feedforward acceleration. arXiv:1412.5474 (2014)\n\nDesign of efficient convolutional layers using single intra-channel convolution, topological subdivisioning and spatial \"bottleneck\" structure. M Wang, B Liu, H Foroosh, arXiv:1608.04337Wang, M., Liu, B., Foroosh, H.: Design of efficient convolutional layers using sin- gle intra-channel convolution, topological subdivisioning and spatial \"bottleneck\" structure. arXiv:1608.04337 (2016)\n\nLearning transferable architectures for scalable image recognition. B Zoph, V Vasudevan, J Shlens, Q V Le, In: CVPR. Zoph, B., Vasudevan, V., Shlens, J., Le, Q.V.: Learning transferable architectures for scalable image recognition. In: CVPR. (2018)\n\nA real-time algorithm for signal analysis with the help of the wavelet transform. M Holschneider, R Kronland-Martinet, J Morlet, P Tchamitchian, Wavelets: Time-Frequency Methods and Phase Space. Holschneider, M., Kronland-Martinet, R., Morlet, J., Tchamitchian, P.: A real-time algorithm for signal analysis with the help of the wavelet transform. In: Wavelets: Time-Frequency Methods and Phase Space. (1989) 289-297\n\nA Giusti, D Ciresan, J Masci, L Gambardella, J Schmidhuber, Fast image scanning with deep max-pooling convolutional neural networks. In: ICIP. Giusti, A., Ciresan, D., Masci, J., Gambardella, L., Schmidhuber, J.: Fast image scanning with deep max-pooling convolutional neural networks. In: ICIP. (2013)\n\nModeling local and global deformations in deep learning: Epitomic convolution, multiple instance learning, and sliding window detection. G Papandreou, I Kokkinos, P A Savalle, CVPR.Papandreou, G., Kokkinos, I., Savalle, P.A.: Modeling local and global deforma- tions in deep learning: Epitomic convolution, multiple instance learning, and sliding window detection. In: CVPR. (2015)\n\nM Abadi, A Agarwal, arXiv:1603.04467Tensorflow: Large-scale machine learning on heterogeneous distributed systems. Abadi, M., Agarwal, A., et al.: Tensorflow: Large-scale machine learning on het- erogeneous distributed systems. arXiv:1603.04467 (2016)\n\nHypercolumns for object segmentation and fine-grained localization. B Hariharan, P Arbel\u00e1ez, R Girshick, J Malik, CVPR.Hariharan, B., Arbel\u00e1ez, P., Girshick, R., Malik, J.: Hypercolumns for object segmentation and fine-grained localization. In: CVPR. (2015)\n\nO Russakovsky, J Deng, H Su, J Krause, S Satheesh, S Ma, Z Huang, A Karpathy, A Khosla, M Bernstein, A C Berg, L Fei-Fei, ImageNet Large Scale Visual Recognition Challenge. IJCV. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A.C., Fei-Fei, L.: ImageNet Large Scale Visual Recognition Challenge. IJCV (2015)\n\nBatch normalization: Accelerating deep network training by reducing internal covariate shift. S Ioffe, C Szegedy, ICML.Ioffe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by reducing internal covariate shift. In: ICML. (2015)\n\nSemantic contours from inverse detectors. B Hariharan, P Arbel\u00e1ez, L Bourdev, S Maji, J Malik, ICCV.Hariharan, B., Arbel\u00e1ez, P., Bourdev, L., Maji, S., Malik, J.: Semantic contours from inverse detectors. In: ICCV. (2011)\n\nUnderstanding convolution for semantic segmentation. P Wang, P Chen, Y Yuan, D Liu, Z Huang, X Hou, G Cottrell, arXiv:1702.08502Wang, P., Chen, P., Yuan, Y., Liu, D., Huang, Z., Hou, X., Cottrell, G.: Under- standing convolution for semantic segmentation. arXiv:1702.08502 (2017)\n\nDeformable convolutional networks. J Dai, H Qi, Y Xiong, Y Li, G Zhang, H Hu, Y Wei, Dai, J., Qi, H., Xiong, Y., Li, Y., Zhang, G., Hu, H., Wei, Y.: Deformable convo- lutional networks. In: ICCV. (2017)\n\nT Y Lin, Microsoft COCO: Common objects in context. In: ECCV. Lin, T.Y., et al.: Microsoft COCO: Common objects in context. In: ECCV. (2014)\n\nDistilling the knowledge in a neural network. G Hinton, O Vinyals, J Dean, In: NIPS. Hinton, G., Vinyals, O., Dean, J.: Distilling the knowledge in a neural network. In: NIPS. (2014)\n\nRevisiting unreasonable effectiveness of data in deep learning era. C Sun, A Shrivastava, S Singh, A Gupta, Sun, C., Shrivastava, A., Singh, S., Gupta, A.: Revisiting unreasonable effectiveness of data in deep learning era. In: ICCV. (2017)\n\nNot all pixels are equal: Difficultyaware semantic segmentation via deep layer cascade. X Li, Z Liu, P Luo, C C Loy, X Tang, Li, X., Liu, Z., Luo, P., Loy, C.C., Tang, X.: Not all pixels are equal: Difficulty- aware semantic segmentation via deep layer cascade. In: CVPR. (2017)\n\nZ Wu, C Shen, A Van Den Hengel, arXiv:1611.10080Wider or deeper: Revisiting the resnet model for visual recognition. Wu, Z., Shen, C., van den Hengel, A.: Wider or deeper: Revisiting the resnet model for visual recognition. arXiv:1611.10080 (2016)\n\nLearning object interactions and descriptions for semantic image segmentation. G Wang, P Luo, L Lin, X Wang, Wang, G., Luo, P., Lin, L., Wang, X.: Learning object interactions and descriptions for semantic image segmentation. In: CVPR. (2017)\n\nDeep dual learning for semantic image segmentation. P Luo, G Wang, L Lin, X Wang, Luo, P., Wang, G., Lin, L., Wang, X.: Deep dual learning for semantic image segmentation. In: ICCV. (2017)\n\nIn-place activated batchnorm for memoryoptimized training of dnns. S R Bul\u00f2, L Porzi, P Kontschieder, In: CVPR. Bul\u00f2, S.R., Porzi, L., Kontschieder, P.: In-place activated batchnorm for memory- optimized training of dnns. In: CVPR. (2018)\n", "annotations": {"author": "[{\"end\":133,\"start\":85},{\"end\":157,\"start\":134},{\"end\":189,\"start\":158},{\"end\":239,\"start\":190},{\"end\":283,\"start\":240}]", "publisher": null, "author_last_name": "[{\"end\":101,\"start\":97},{\"end\":143,\"start\":140},{\"end\":175,\"start\":165},{\"end\":205,\"start\":198},{\"end\":252,\"start\":248}]", "author_first_name": "[{\"end\":96,\"start\":85},{\"end\":139,\"start\":134},{\"end\":164,\"start\":158},{\"end\":197,\"start\":190},{\"end\":247,\"start\":240}]", "author_affiliation": "[{\"end\":132,\"start\":121},{\"end\":156,\"start\":145},{\"end\":188,\"start\":177},{\"end\":238,\"start\":227},{\"end\":282,\"start\":271}]", "title": "[{\"end\":82,\"start\":1},{\"end\":365,\"start\":284}]", "venue": null, "abstract": "[{\"end\":1726,\"start\":468}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1834,\"start\":1831},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1836,\"start\":1834},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1838,\"start\":1836},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1840,\"start\":1838},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1842,\"start\":1840},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":1934,\"start\":1931},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":1936,\"start\":1934},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":1938,\"start\":1936},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":1940,\"start\":1938},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":1943,\"start\":1940},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":1995,\"start\":1992},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":1998,\"start\":1995},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2075,\"start\":2071},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2078,\"start\":2075},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2081,\"start\":2078},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2084,\"start\":2081},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2087,\"start\":2084},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2090,\"start\":2087},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2210,\"start\":2206},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2213,\"start\":2210},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2216,\"start\":2213},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2250,\"start\":2246},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2253,\"start\":2250},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2578,\"start\":2574},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3010,\"start\":3007},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3012,\"start\":3010},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3015,\"start\":3012},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3018,\"start\":3015},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3021,\"start\":3018},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3196,\"start\":3192},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3671,\"start\":3667},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3674,\"start\":3671},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4094,\"start\":4090},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":4552,\"start\":4548},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4555,\"start\":4552},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4558,\"start\":4555},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":4561,\"start\":4558},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4564,\"start\":4561},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4689,\"start\":4685},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":4706,\"start\":4702},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6042,\"start\":6039},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6045,\"start\":6042},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6126,\"start\":6123},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6128,\"start\":6126},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6130,\"start\":6128},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6132,\"start\":6130},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6134,\"start\":6132},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6237,\"start\":6233},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6240,\"start\":6237},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6243,\"start\":6240},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6246,\"start\":6243},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6249,\"start\":6246},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6252,\"start\":6249},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6255,\"start\":6252},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6258,\"start\":6255},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":6333,\"start\":6329},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":6336,\"start\":6333},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":6339,\"start\":6336},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":6342,\"start\":6339},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":6345,\"start\":6342},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":6348,\"start\":6345},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":6422,\"start\":6418},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":6462,\"start\":6458},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":6468,\"start\":6464},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":6471,\"start\":6468},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":6474,\"start\":6471},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":6477,\"start\":6474},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":6480,\"start\":6477},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":6483,\"start\":6480},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":6486,\"start\":6483},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":6489,\"start\":6486},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":6492,\"start\":6489},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":6495,\"start\":6492},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":6498,\"start\":6495},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":6501,\"start\":6498},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6668,\"start\":6664},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":6684,\"start\":6680},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6687,\"start\":6684},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6725,\"start\":6721},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6728,\"start\":6725},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":6786,\"start\":6782},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":7169,\"start\":7165},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":7192,\"start\":7188},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":7195,\"start\":7192},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":7198,\"start\":7195},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7230,\"start\":7226},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":7233,\"start\":7230},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7236,\"start\":7233},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7239,\"start\":7236},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":7242,\"start\":7239},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":7245,\"start\":7242},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":7248,\"start\":7245},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":7251,\"start\":7248},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":7254,\"start\":7251},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":7257,\"start\":7254},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":7260,\"start\":7257},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7550,\"start\":7546},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7721,\"start\":7717},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7724,\"start\":7721},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7749,\"start\":7746},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":7752,\"start\":7749},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":7965,\"start\":7961},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":7968,\"start\":7965},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7971,\"start\":7968},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7974,\"start\":7971},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7977,\"start\":7974},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7980,\"start\":7977},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":7983,\"start\":7980},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8034,\"start\":8030},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8051,\"start\":8047},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":8272,\"start\":8268},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":8275,\"start\":8272},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8277,\"start\":8275},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":8280,\"start\":8277},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8282,\"start\":8280},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8324,\"start\":8320},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8327,\"start\":8324},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":8330,\"start\":8327},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8333,\"start\":8330},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8336,\"start\":8333},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8367,\"start\":8363},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8528,\"start\":8524},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8531,\"start\":8528},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":9746,\"start\":9742},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":10379,\"start\":10375},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10838,\"start\":10834},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":10870,\"start\":10866},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":10873,\"start\":10870},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10875,\"start\":10873},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":10877,\"start\":10875},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":11848,\"start\":11844},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12328,\"start\":12324},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":12748,\"start\":12744},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12860,\"start\":12856},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":13649,\"start\":13645},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":13716,\"start\":13712},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":13773,\"start\":13769},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":14144,\"start\":14140},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":14567,\"start\":14563},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":14670,\"start\":14666},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":14749,\"start\":14745},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":14776,\"start\":14772},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":14810,\"start\":14806},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":14813,\"start\":14810},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":14913,\"start\":14909},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":15039,\"start\":15036},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":15283,\"start\":15279},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":15491,\"start\":15487},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":15532,\"start\":15528},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":15623,\"start\":15619},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":15731,\"start\":15727},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":16309,\"start\":16305},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":16504,\"start\":16500},{\"end\":18261,\"start\":18249},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":18349,\"start\":18345},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":18352,\"start\":18349},{\"end\":18602,\"start\":18590},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":18958,\"start\":18954},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":19183,\"start\":19179},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":20482,\"start\":20478},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":20518,\"start\":20514},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":20668,\"start\":20664},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":20707,\"start\":20703},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":21212,\"start\":21208},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":22058,\"start\":22054},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":22061,\"start\":22058},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":22064,\"start\":22061},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":22858,\"start\":22854},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":22976,\"start\":22972},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":23070,\"start\":23066},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":23096,\"start\":23092},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":23099,\"start\":23096},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":23102,\"start\":23099},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":23960,\"start\":23956},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":23963,\"start\":23960},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":23966,\"start\":23963},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":24378,\"start\":24374},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":24396,\"start\":24392},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":24881,\"start\":24878},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":25215,\"start\":25211},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":25280,\"start\":25276},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":25772,\"start\":25768},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":25795,\"start\":25791},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":26967,\"start\":26963}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":27074,\"start\":26961},{\"attributes\":{\"id\":\"fig_1\"},\"end\":27496,\"start\":27075},{\"attributes\":{\"id\":\"fig_2\"},\"end\":27836,\"start\":27497},{\"attributes\":{\"id\":\"fig_4\"},\"end\":28239,\"start\":27837},{\"attributes\":{\"id\":\"fig_5\"},\"end\":28319,\"start\":28240},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":30003,\"start\":28320},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":31141,\"start\":30004},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":31214,\"start\":31142},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":31699,\"start\":31215}]", "paragraph": "[{\"end\":2446,\"start\":1742},{\"end\":4016,\"start\":2448},{\"end\":5052,\"start\":4018},{\"end\":5088,\"start\":5054},{\"end\":5970,\"start\":5090},{\"end\":7023,\"start\":5987},{\"end\":8199,\"start\":7025},{\"end\":8595,\"start\":8211},{\"end\":9115,\"start\":8639},{\"end\":9596,\"start\":9197},{\"end\":10800,\"start\":9626},{\"end\":12207,\"start\":10802},{\"end\":13596,\"start\":12209},{\"end\":14695,\"start\":13626},{\"end\":15442,\"start\":14723},{\"end\":15987,\"start\":15444},{\"end\":16241,\"start\":16014},{\"end\":17113,\"start\":16243},{\"end\":17566,\"start\":17115},{\"end\":18787,\"start\":17568},{\"end\":19111,\"start\":18822},{\"end\":19451,\"start\":19113},{\"end\":19834,\"start\":19453},{\"end\":20400,\"start\":19836},{\"end\":20573,\"start\":20433},{\"end\":21450,\"start\":20575},{\"end\":21564,\"start\":21452},{\"end\":22114,\"start\":21566},{\"end\":22363,\"start\":22116},{\"end\":22715,\"start\":22365},{\"end\":22939,\"start\":22717},{\"end\":23146,\"start\":22941},{\"end\":23482,\"start\":23148},{\"end\":23659,\"start\":23484},{\"end\":23831,\"start\":23661},{\"end\":24771,\"start\":23871},{\"end\":25089,\"start\":24810},{\"end\":25991,\"start\":25091},{\"end\":26306,\"start\":25993},{\"end\":26960,\"start\":26321}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9196,\"start\":9116},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9625,\"start\":9597}]", "table_ref": "[{\"end\":19054,\"start\":19047}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1740,\"start\":1728},{\"attributes\":{\"n\":\"2\"},\"end\":5985,\"start\":5973},{\"attributes\":{\"n\":\"3\"},\"end\":8209,\"start\":8202},{\"attributes\":{\"n\":\"3.1\"},\"end\":8637,\"start\":8598},{\"attributes\":{\"n\":\"3.2\"},\"end\":13624,\"start\":13599},{\"attributes\":{\"n\":\"4\"},\"end\":14721,\"start\":14698},{\"attributes\":{\"n\":\"4.1\"},\"end\":16012,\"start\":15990},{\"attributes\":{\"n\":\"4.2\"},\"end\":18820,\"start\":18790},{\"attributes\":{\"n\":\"4.3\"},\"end\":20431,\"start\":20403},{\"attributes\":{\"n\":\"4.4\"},\"end\":23869,\"start\":23834},{\"attributes\":{\"n\":\"4.5\"},\"end\":24808,\"start\":24774},{\"attributes\":{\"n\":\"5\"},\"end\":26319,\"start\":26309},{\"end\":27084,\"start\":27076},{\"end\":27506,\"start\":27498},{\"end\":27846,\"start\":27838},{\"end\":28249,\"start\":28241},{\"end\":31152,\"start\":31143}]", "table": "[{\"end\":30003,\"start\":28952},{\"end\":31141,\"start\":30536},{\"end\":31699,\"start\":31523}]", "figure_caption": "[{\"end\":27074,\"start\":26963},{\"end\":27496,\"start\":27086},{\"end\":27836,\"start\":27508},{\"end\":28239,\"start\":27848},{\"end\":28319,\"start\":28251},{\"end\":28952,\"start\":28322},{\"end\":30536,\"start\":30006},{\"end\":31214,\"start\":31154},{\"end\":31523,\"start\":31217}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":4201,\"start\":4195},{\"end\":10555,\"start\":10549},{\"end\":12609,\"start\":12603},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":14682,\"start\":14676},{\"end\":16792,\"start\":16786},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":23558,\"start\":23552},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":23709,\"start\":23703},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":24321,\"start\":24311},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":24770,\"start\":24760}]", "bib_author_first_name": "[{\"end\":31762,\"start\":31761},{\"end\":31776,\"start\":31775},{\"end\":31780,\"start\":31777},{\"end\":31790,\"start\":31789},{\"end\":31792,\"start\":31791},{\"end\":31800,\"start\":31799},{\"end\":31804,\"start\":31801},{\"end\":31816,\"start\":31815},{\"end\":31824,\"start\":31823},{\"end\":32083,\"start\":32082},{\"end\":32095,\"start\":32094},{\"end\":32103,\"start\":32102},{\"end\":32110,\"start\":32109},{\"end\":32112,\"start\":32111},{\"end\":32119,\"start\":32118},{\"end\":32121,\"start\":32120},{\"end\":32128,\"start\":32127},{\"end\":32138,\"start\":32137},{\"end\":32149,\"start\":32148},{\"end\":32418,\"start\":32417},{\"end\":32428,\"start\":32427},{\"end\":32437,\"start\":32436},{\"end\":32446,\"start\":32445},{\"end\":32457,\"start\":32456},{\"end\":32470,\"start\":32469},{\"end\":32482,\"start\":32481},{\"end\":32492,\"start\":32491},{\"end\":32500,\"start\":32499},{\"end\":32708,\"start\":32707},{\"end\":32716,\"start\":32715},{\"end\":32724,\"start\":32723},{\"end\":32732,\"start\":32731},{\"end\":32742,\"start\":32741},{\"end\":32754,\"start\":32753},{\"end\":32978,\"start\":32977},{\"end\":32988,\"start\":32987},{\"end\":33000,\"start\":32999},{\"end\":33183,\"start\":33182},{\"end\":33192,\"start\":33191},{\"end\":33202,\"start\":33201},{\"end\":33212,\"start\":33211},{\"end\":33368,\"start\":33367},{\"end\":33382,\"start\":33381},{\"end\":33395,\"start\":33394},{\"end\":33397,\"start\":33396},{\"end\":33701,\"start\":33700},{\"end\":33713,\"start\":33712},{\"end\":33722,\"start\":33721},{\"end\":33731,\"start\":33730},{\"end\":33742,\"start\":33741},{\"end\":33752,\"start\":33751},{\"end\":34015,\"start\":34014},{\"end\":34027,\"start\":34026},{\"end\":34160,\"start\":34159},{\"end\":34171,\"start\":34170},{\"end\":34178,\"start\":34177},{\"end\":34185,\"start\":34184},{\"end\":34197,\"start\":34196},{\"end\":34205,\"start\":34204},{\"end\":34217,\"start\":34216},{\"end\":34226,\"start\":34225},{\"end\":34239,\"start\":34238},{\"end\":34512,\"start\":34511},{\"end\":34520,\"start\":34519},{\"end\":34533,\"start\":34532},{\"end\":34719,\"start\":34718},{\"end\":34725,\"start\":34724},{\"end\":34727,\"start\":34726},{\"end\":34736,\"start\":34735},{\"end\":35016,\"start\":35015},{\"end\":35027,\"start\":35026},{\"end\":35035,\"start\":35034},{\"end\":35045,\"start\":35044},{\"end\":35326,\"start\":35325},{\"end\":35335,\"start\":35334},{\"end\":35337,\"start\":35336},{\"end\":35543,\"start\":35542},{\"end\":35554,\"start\":35553},{\"end\":35565,\"start\":35564},{\"end\":35574,\"start\":35573},{\"end\":35576,\"start\":35575},{\"end\":35790,\"start\":35789},{\"end\":35799,\"start\":35798},{\"end\":35809,\"start\":35808},{\"end\":36051,\"start\":36050},{\"end\":36058,\"start\":36057},{\"end\":36068,\"start\":36067},{\"end\":36326,\"start\":36325},{\"end\":36337,\"start\":36336},{\"end\":36574,\"start\":36573},{\"end\":36586,\"start\":36585},{\"end\":36596,\"start\":36595},{\"end\":36836,\"start\":36835},{\"end\":36842,\"start\":36841},{\"end\":36851,\"start\":36850},{\"end\":36858,\"start\":36857},{\"end\":37069,\"start\":37068},{\"end\":37084,\"start\":37083},{\"end\":37095,\"start\":37094},{\"end\":37319,\"start\":37318},{\"end\":37337,\"start\":37336},{\"end\":37348,\"start\":37347},{\"end\":37567,\"start\":37566},{\"end\":37569,\"start\":37568},{\"end\":37577,\"start\":37576},{\"end\":37591,\"start\":37590},{\"end\":37602,\"start\":37601},{\"end\":37796,\"start\":37795},{\"end\":37804,\"start\":37803},{\"end\":37811,\"start\":37810},{\"end\":37817,\"start\":37816},{\"end\":37825,\"start\":37824},{\"end\":37973,\"start\":37972},{\"end\":37979,\"start\":37978},{\"end\":37988,\"start\":37987},{\"end\":37995,\"start\":37994},{\"end\":38171,\"start\":38170},{\"end\":38326,\"start\":38325},{\"end\":38486,\"start\":38485},{\"end\":38582,\"start\":38581},{\"end\":38584,\"start\":38583},{\"end\":38594,\"start\":38593},{\"end\":38601,\"start\":38600},{\"end\":38609,\"start\":38608},{\"end\":38625,\"start\":38624},{\"end\":38633,\"start\":38632},{\"end\":38643,\"start\":38642},{\"end\":38656,\"start\":38655},{\"end\":39057,\"start\":39056},{\"end\":39066,\"start\":39065},{\"end\":39074,\"start\":39073},{\"end\":39081,\"start\":39080},{\"end\":39319,\"start\":39318},{\"end\":39325,\"start\":39324},{\"end\":39334,\"start\":39333},{\"end\":39342,\"start\":39341},{\"end\":39348,\"start\":39347},{\"end\":39357,\"start\":39356},{\"end\":39364,\"start\":39363},{\"end\":39621,\"start\":39620},{\"end\":39634,\"start\":39633},{\"end\":39650,\"start\":39649},{\"end\":39873,\"start\":39872},{\"end\":39880,\"start\":39879},{\"end\":39886,\"start\":39885},{\"end\":40069,\"start\":40068},{\"end\":40080,\"start\":40079},{\"end\":40091,\"start\":40090},{\"end\":40101,\"start\":40100},{\"end\":40335,\"start\":40334},{\"end\":40344,\"start\":40343},{\"end\":40568,\"start\":40567},{\"end\":40580,\"start\":40579},{\"end\":40788,\"start\":40787},{\"end\":40795,\"start\":40794},{\"end\":40803,\"start\":40802},{\"end\":40821,\"start\":40820},{\"end\":41043,\"start\":41042},{\"end\":41045,\"start\":41044},{\"end\":41053,\"start\":41052},{\"end\":41061,\"start\":41060},{\"end\":41069,\"start\":41068},{\"end\":41075,\"start\":41074},{\"end\":41077,\"start\":41076},{\"end\":41339,\"start\":41338},{\"end\":41341,\"start\":41340},{\"end\":41349,\"start\":41348},{\"end\":41363,\"start\":41362},{\"end\":41375,\"start\":41374},{\"end\":41385,\"start\":41384},{\"end\":41387,\"start\":41386},{\"end\":41672,\"start\":41671},{\"end\":41686,\"start\":41685},{\"end\":41888,\"start\":41887},{\"end\":41897,\"start\":41896},{\"end\":41905,\"start\":41904},{\"end\":41907,\"start\":41906},{\"end\":42140,\"start\":42139},{\"end\":42142,\"start\":42141},{\"end\":42150,\"start\":42149},{\"end\":42164,\"start\":42163},{\"end\":42176,\"start\":42175},{\"end\":42186,\"start\":42185},{\"end\":42188,\"start\":42187},{\"end\":42445,\"start\":42444},{\"end\":42453,\"start\":42452},{\"end\":42465,\"start\":42464},{\"end\":42476,\"start\":42475},{\"end\":42627,\"start\":42626},{\"end\":42636,\"start\":42635},{\"end\":42650,\"start\":42649},{\"end\":42668,\"start\":42667},{\"end\":42678,\"start\":42677},{\"end\":42684,\"start\":42683},{\"end\":42690,\"start\":42689},{\"end\":42699,\"start\":42698},{\"end\":42997,\"start\":42996},{\"end\":43004,\"start\":43003},{\"end\":43010,\"start\":43009},{\"end\":43017,\"start\":43016},{\"end\":43019,\"start\":43018},{\"end\":43026,\"start\":43025},{\"end\":43236,\"start\":43235},{\"end\":43250,\"start\":43249},{\"end\":43252,\"start\":43251},{\"end\":43260,\"start\":43259},{\"end\":43270,\"start\":43269},{\"end\":43272,\"start\":43271},{\"end\":43439,\"start\":43438},{\"end\":43441,\"start\":43440},{\"end\":43452,\"start\":43451},{\"end\":43717,\"start\":43716},{\"end\":43728,\"start\":43727},{\"end\":43738,\"start\":43737},{\"end\":43740,\"start\":43739},{\"end\":43987,\"start\":43986},{\"end\":44002,\"start\":44001},{\"end\":44011,\"start\":44010},{\"end\":44013,\"start\":44012},{\"end\":44020,\"start\":44019},{\"end\":44274,\"start\":44273},{\"end\":44285,\"start\":44284},{\"end\":44498,\"start\":44497},{\"end\":44509,\"start\":44508},{\"end\":44520,\"start\":44519},{\"end\":44646,\"start\":44645},{\"end\":44653,\"start\":44652},{\"end\":44667,\"start\":44666},{\"end\":44669,\"start\":44668},{\"end\":44887,\"start\":44886},{\"end\":44897,\"start\":44896},{\"end\":44905,\"start\":44904},{\"end\":45071,\"start\":45070},{\"end\":45073,\"start\":45072},{\"end\":45080,\"start\":45079},{\"end\":45090,\"start\":45089},{\"end\":45102,\"start\":45101},{\"end\":45108,\"start\":45107},{\"end\":45121,\"start\":45120},{\"end\":45272,\"start\":45271},{\"end\":45287,\"start\":45286},{\"end\":45301,\"start\":45300},{\"end\":45310,\"start\":45309},{\"end\":45549,\"start\":45548},{\"end\":45551,\"start\":45550},{\"end\":45557,\"start\":45556},{\"end\":45564,\"start\":45563},{\"end\":45573,\"start\":45572},{\"end\":45582,\"start\":45581},{\"end\":45584,\"start\":45583},{\"end\":45832,\"start\":45831},{\"end\":45839,\"start\":45838},{\"end\":45847,\"start\":45846},{\"end\":46074,\"start\":46073},{\"end\":46081,\"start\":46080},{\"end\":46090,\"start\":46089},{\"end\":46098,\"start\":46097},{\"end\":46350,\"start\":46349},{\"end\":46360,\"start\":46359},{\"end\":46371,\"start\":46370},{\"end\":46382,\"start\":46381},{\"end\":46620,\"start\":46619},{\"end\":46628,\"start\":46627},{\"end\":46637,\"start\":46636},{\"end\":46643,\"start\":46642},{\"end\":46650,\"start\":46649},{\"end\":46866,\"start\":46865},{\"end\":46868,\"start\":46867},{\"end\":46877,\"start\":46876},{\"end\":46887,\"start\":46886},{\"end\":46889,\"start\":46888},{\"end\":46898,\"start\":46897},{\"end\":47061,\"start\":47060},{\"end\":47070,\"start\":47069},{\"end\":47081,\"start\":47080},{\"end\":47095,\"start\":47094},{\"end\":47108,\"start\":47107},{\"end\":47110,\"start\":47109},{\"end\":47118,\"start\":47117},{\"end\":47127,\"start\":47126},{\"end\":47337,\"start\":47336},{\"end\":47343,\"start\":47342},{\"end\":47350,\"start\":47349},{\"end\":47358,\"start\":47357},{\"end\":47559,\"start\":47558},{\"end\":47568,\"start\":47567},{\"end\":47577,\"start\":47576},{\"end\":47585,\"start\":47584},{\"end\":47594,\"start\":47593},{\"end\":47803,\"start\":47802},{\"end\":47810,\"start\":47809},{\"end\":47822,\"start\":47821},{\"end\":47831,\"start\":47830},{\"end\":47837,\"start\":47836},{\"end\":48043,\"start\":48042},{\"end\":48050,\"start\":48049},{\"end\":48060,\"start\":48059},{\"end\":48366,\"start\":48365},{\"end\":48374,\"start\":48373},{\"end\":48381,\"start\":48380},{\"end\":48679,\"start\":48678},{\"end\":48687,\"start\":48686},{\"end\":48700,\"start\":48699},{\"end\":48710,\"start\":48709},{\"end\":48712,\"start\":48711},{\"end\":48943,\"start\":48942},{\"end\":48959,\"start\":48958},{\"end\":48980,\"start\":48979},{\"end\":48990,\"start\":48989},{\"end\":49279,\"start\":49278},{\"end\":49289,\"start\":49288},{\"end\":49300,\"start\":49299},{\"end\":49309,\"start\":49308},{\"end\":49324,\"start\":49323},{\"end\":49720,\"start\":49719},{\"end\":49734,\"start\":49733},{\"end\":49746,\"start\":49745},{\"end\":49748,\"start\":49747},{\"end\":49966,\"start\":49965},{\"end\":49975,\"start\":49974},{\"end\":50287,\"start\":50286},{\"end\":50300,\"start\":50299},{\"end\":50312,\"start\":50311},{\"end\":50324,\"start\":50323},{\"end\":50478,\"start\":50477},{\"end\":50493,\"start\":50492},{\"end\":50501,\"start\":50500},{\"end\":50507,\"start\":50506},{\"end\":50517,\"start\":50516},{\"end\":50529,\"start\":50528},{\"end\":50535,\"start\":50534},{\"end\":50544,\"start\":50543},{\"end\":50556,\"start\":50555},{\"end\":50566,\"start\":50565},{\"end\":50579,\"start\":50578},{\"end\":50581,\"start\":50580},{\"end\":50589,\"start\":50588},{\"end\":50961,\"start\":50960},{\"end\":50970,\"start\":50969},{\"end\":51164,\"start\":51163},{\"end\":51177,\"start\":51176},{\"end\":51189,\"start\":51188},{\"end\":51200,\"start\":51199},{\"end\":51208,\"start\":51207},{\"end\":51398,\"start\":51397},{\"end\":51406,\"start\":51405},{\"end\":51414,\"start\":51413},{\"end\":51422,\"start\":51421},{\"end\":51429,\"start\":51428},{\"end\":51438,\"start\":51437},{\"end\":51445,\"start\":51444},{\"end\":51661,\"start\":51660},{\"end\":51668,\"start\":51667},{\"end\":51674,\"start\":51673},{\"end\":51683,\"start\":51682},{\"end\":51689,\"start\":51688},{\"end\":51698,\"start\":51697},{\"end\":51704,\"start\":51703},{\"end\":51830,\"start\":51829},{\"end\":51832,\"start\":51831},{\"end\":52018,\"start\":52017},{\"end\":52028,\"start\":52027},{\"end\":52039,\"start\":52038},{\"end\":52224,\"start\":52223},{\"end\":52231,\"start\":52230},{\"end\":52246,\"start\":52245},{\"end\":52255,\"start\":52254},{\"end\":52486,\"start\":52485},{\"end\":52492,\"start\":52491},{\"end\":52499,\"start\":52498},{\"end\":52506,\"start\":52505},{\"end\":52508,\"start\":52507},{\"end\":52515,\"start\":52514},{\"end\":52678,\"start\":52677},{\"end\":52684,\"start\":52683},{\"end\":52692,\"start\":52691},{\"end\":53006,\"start\":53005},{\"end\":53014,\"start\":53013},{\"end\":53021,\"start\":53020},{\"end\":53028,\"start\":53027},{\"end\":53223,\"start\":53222},{\"end\":53230,\"start\":53229},{\"end\":53238,\"start\":53237},{\"end\":53245,\"start\":53244},{\"end\":53428,\"start\":53427},{\"end\":53430,\"start\":53429},{\"end\":53438,\"start\":53437},{\"end\":53447,\"start\":53446}]", "bib_author_last_name": "[{\"end\":31773,\"start\":31763},{\"end\":31787,\"start\":31781},{\"end\":31797,\"start\":31793},{\"end\":31813,\"start\":31805},{\"end\":31821,\"start\":31817},{\"end\":31834,\"start\":31825},{\"end\":32092,\"start\":32084},{\"end\":32100,\"start\":32096},{\"end\":32107,\"start\":32104},{\"end\":32116,\"start\":32113},{\"end\":32125,\"start\":32122},{\"end\":32135,\"start\":32129},{\"end\":32146,\"start\":32139},{\"end\":32156,\"start\":32150},{\"end\":32425,\"start\":32419},{\"end\":32434,\"start\":32429},{\"end\":32443,\"start\":32438},{\"end\":32454,\"start\":32447},{\"end\":32467,\"start\":32458},{\"end\":32479,\"start\":32471},{\"end\":32489,\"start\":32483},{\"end\":32497,\"start\":32493},{\"end\":32508,\"start\":32501},{\"end\":32713,\"start\":32709},{\"end\":32721,\"start\":32717},{\"end\":32729,\"start\":32725},{\"end\":32739,\"start\":32733},{\"end\":32751,\"start\":32743},{\"end\":32763,\"start\":32755},{\"end\":32985,\"start\":32979},{\"end\":32997,\"start\":32989},{\"end\":33008,\"start\":33001},{\"end\":33189,\"start\":33184},{\"end\":33199,\"start\":33193},{\"end\":33209,\"start\":33203},{\"end\":33220,\"start\":33213},{\"end\":33379,\"start\":33369},{\"end\":33392,\"start\":33383},{\"end\":33404,\"start\":33398},{\"end\":33710,\"start\":33702},{\"end\":33719,\"start\":33714},{\"end\":33728,\"start\":33723},{\"end\":33739,\"start\":33732},{\"end\":33749,\"start\":33743},{\"end\":33758,\"start\":33753},{\"end\":34024,\"start\":34016},{\"end\":34037,\"start\":34028},{\"end\":34168,\"start\":34161},{\"end\":34175,\"start\":34172},{\"end\":34182,\"start\":34179},{\"end\":34194,\"start\":34186},{\"end\":34202,\"start\":34198},{\"end\":34214,\"start\":34206},{\"end\":34223,\"start\":34218},{\"end\":34236,\"start\":34227},{\"end\":34250,\"start\":34240},{\"end\":34517,\"start\":34513},{\"end\":34530,\"start\":34521},{\"end\":34541,\"start\":34534},{\"end\":34722,\"start\":34720},{\"end\":34733,\"start\":34728},{\"end\":34754,\"start\":34737},{\"end\":35024,\"start\":35017},{\"end\":35032,\"start\":35028},{\"end\":35042,\"start\":35036},{\"end\":35055,\"start\":35046},{\"end\":35332,\"start\":35327},{\"end\":35342,\"start\":35338},{\"end\":35551,\"start\":35544},{\"end\":35562,\"start\":35555},{\"end\":35571,\"start\":35566},{\"end\":35581,\"start\":35577},{\"end\":35796,\"start\":35791},{\"end\":35806,\"start\":35800},{\"end\":35816,\"start\":35810},{\"end\":36055,\"start\":36052},{\"end\":36065,\"start\":36059},{\"end\":36076,\"start\":36069},{\"end\":36334,\"start\":36327},{\"end\":36345,\"start\":36338},{\"end\":36583,\"start\":36575},{\"end\":36593,\"start\":36587},{\"end\":36602,\"start\":36597},{\"end\":36839,\"start\":36837},{\"end\":36848,\"start\":36843},{\"end\":36855,\"start\":36852},{\"end\":36862,\"start\":36859},{\"end\":37081,\"start\":37070},{\"end\":37092,\"start\":37085},{\"end\":37100,\"start\":37096},{\"end\":37334,\"start\":37320},{\"end\":37345,\"start\":37338},{\"end\":37356,\"start\":37349},{\"end\":37574,\"start\":37570},{\"end\":37588,\"start\":37578},{\"end\":37599,\"start\":37592},{\"end\":37607,\"start\":37603},{\"end\":37801,\"start\":37797},{\"end\":37808,\"start\":37805},{\"end\":37814,\"start\":37812},{\"end\":37822,\"start\":37818},{\"end\":37829,\"start\":37826},{\"end\":37976,\"start\":37974},{\"end\":37985,\"start\":37980},{\"end\":37992,\"start\":37989},{\"end\":37999,\"start\":37996},{\"end\":38179,\"start\":38172},{\"end\":38332,\"start\":38327},{\"end\":38496,\"start\":38487},{\"end\":38591,\"start\":38585},{\"end\":38598,\"start\":38595},{\"end\":38606,\"start\":38602},{\"end\":38622,\"start\":38610},{\"end\":38630,\"start\":38626},{\"end\":38640,\"start\":38634},{\"end\":38653,\"start\":38644},{\"end\":38661,\"start\":38657},{\"end\":39063,\"start\":39058},{\"end\":39071,\"start\":39067},{\"end\":39078,\"start\":39075},{\"end\":39085,\"start\":39082},{\"end\":39322,\"start\":39320},{\"end\":39331,\"start\":39326},{\"end\":39339,\"start\":39335},{\"end\":39345,\"start\":39343},{\"end\":39354,\"start\":39349},{\"end\":39361,\"start\":39358},{\"end\":39368,\"start\":39365},{\"end\":39631,\"start\":39622},{\"end\":39647,\"start\":39635},{\"end\":39664,\"start\":39651},{\"end\":39877,\"start\":39874},{\"end\":39883,\"start\":39881},{\"end\":39890,\"start\":39887},{\"end\":40077,\"start\":40070},{\"end\":40088,\"start\":40081},{\"end\":40098,\"start\":40092},{\"end\":40107,\"start\":40102},{\"end\":40341,\"start\":40336},{\"end\":40351,\"start\":40345},{\"end\":40577,\"start\":40569},{\"end\":40590,\"start\":40581},{\"end\":40792,\"start\":40789},{\"end\":40800,\"start\":40796},{\"end\":40818,\"start\":40804},{\"end\":40826,\"start\":40822},{\"end\":41050,\"start\":41046},{\"end\":41058,\"start\":41054},{\"end\":41066,\"start\":41062},{\"end\":41072,\"start\":41070},{\"end\":41084,\"start\":41078},{\"end\":41346,\"start\":41342},{\"end\":41360,\"start\":41350},{\"end\":41372,\"start\":41364},{\"end\":41382,\"start\":41376},{\"end\":41394,\"start\":41388},{\"end\":41683,\"start\":41673},{\"end\":41693,\"start\":41687},{\"end\":41894,\"start\":41889},{\"end\":41902,\"start\":41898},{\"end\":41913,\"start\":41908},{\"end\":42147,\"start\":42143},{\"end\":42161,\"start\":42151},{\"end\":42173,\"start\":42165},{\"end\":42183,\"start\":42177},{\"end\":42195,\"start\":42189},{\"end\":42450,\"start\":42446},{\"end\":42462,\"start\":42454},{\"end\":42473,\"start\":42466},{\"end\":42481,\"start\":42477},{\"end\":42633,\"start\":42628},{\"end\":42647,\"start\":42637},{\"end\":42665,\"start\":42651},{\"end\":42675,\"start\":42669},{\"end\":42681,\"start\":42679},{\"end\":42687,\"start\":42685},{\"end\":42696,\"start\":42691},{\"end\":42704,\"start\":42700},{\"end\":43001,\"start\":42998},{\"end\":43007,\"start\":43005},{\"end\":43014,\"start\":43011},{\"end\":43023,\"start\":43020},{\"end\":43031,\"start\":43027},{\"end\":43247,\"start\":43237},{\"end\":43257,\"start\":43253},{\"end\":43267,\"start\":43261},{\"end\":43279,\"start\":43273},{\"end\":43449,\"start\":43442},{\"end\":43460,\"start\":43453},{\"end\":43725,\"start\":43718},{\"end\":43735,\"start\":43729},{\"end\":43747,\"start\":43741},{\"end\":43999,\"start\":43988},{\"end\":44008,\"start\":44003},{\"end\":44017,\"start\":44014},{\"end\":44030,\"start\":44021},{\"end\":44282,\"start\":44275},{\"end\":44294,\"start\":44286},{\"end\":44506,\"start\":44499},{\"end\":44517,\"start\":44510},{\"end\":44529,\"start\":44521},{\"end\":44650,\"start\":44647},{\"end\":44664,\"start\":44654},{\"end\":44674,\"start\":44670},{\"end\":44894,\"start\":44888},{\"end\":44902,\"start\":44898},{\"end\":44910,\"start\":44906},{\"end\":45077,\"start\":45074},{\"end\":45087,\"start\":45081},{\"end\":45099,\"start\":45091},{\"end\":45105,\"start\":45103},{\"end\":45118,\"start\":45109},{\"end\":45130,\"start\":45122},{\"end\":45284,\"start\":45273},{\"end\":45298,\"start\":45288},{\"end\":45307,\"start\":45302},{\"end\":45316,\"start\":45311},{\"end\":45554,\"start\":45552},{\"end\":45561,\"start\":45558},{\"end\":45570,\"start\":45565},{\"end\":45579,\"start\":45574},{\"end\":45589,\"start\":45585},{\"end\":45836,\"start\":45833},{\"end\":45844,\"start\":45840},{\"end\":45851,\"start\":45848},{\"end\":46078,\"start\":46075},{\"end\":46087,\"start\":46082},{\"end\":46095,\"start\":46091},{\"end\":46103,\"start\":46099},{\"end\":46357,\"start\":46351},{\"end\":46368,\"start\":46361},{\"end\":46379,\"start\":46372},{\"end\":46388,\"start\":46383},{\"end\":46625,\"start\":46621},{\"end\":46634,\"start\":46629},{\"end\":46640,\"start\":46638},{\"end\":46647,\"start\":46644},{\"end\":46654,\"start\":46651},{\"end\":46874,\"start\":46869},{\"end\":46884,\"start\":46878},{\"end\":46895,\"start\":46890},{\"end\":46903,\"start\":46899},{\"end\":47067,\"start\":47062},{\"end\":47078,\"start\":47071},{\"end\":47092,\"start\":47082},{\"end\":47105,\"start\":47096},{\"end\":47115,\"start\":47111},{\"end\":47124,\"start\":47119},{\"end\":47136,\"start\":47128},{\"end\":47340,\"start\":47338},{\"end\":47347,\"start\":47344},{\"end\":47355,\"start\":47351},{\"end\":47361,\"start\":47359},{\"end\":47565,\"start\":47560},{\"end\":47574,\"start\":47569},{\"end\":47582,\"start\":47578},{\"end\":47591,\"start\":47586},{\"end\":47598,\"start\":47595},{\"end\":47807,\"start\":47804},{\"end\":47819,\"start\":47811},{\"end\":47828,\"start\":47823},{\"end\":47834,\"start\":47832},{\"end\":47840,\"start\":47838},{\"end\":48047,\"start\":48044},{\"end\":48057,\"start\":48051},{\"end\":48072,\"start\":48061},{\"end\":48371,\"start\":48367},{\"end\":48378,\"start\":48375},{\"end\":48389,\"start\":48382},{\"end\":48684,\"start\":48680},{\"end\":48697,\"start\":48688},{\"end\":48707,\"start\":48701},{\"end\":48715,\"start\":48713},{\"end\":48956,\"start\":48944},{\"end\":48977,\"start\":48960},{\"end\":48987,\"start\":48981},{\"end\":49003,\"start\":48991},{\"end\":49286,\"start\":49280},{\"end\":49297,\"start\":49290},{\"end\":49306,\"start\":49301},{\"end\":49321,\"start\":49310},{\"end\":49336,\"start\":49325},{\"end\":49731,\"start\":49721},{\"end\":49743,\"start\":49735},{\"end\":49756,\"start\":49749},{\"end\":49972,\"start\":49967},{\"end\":49983,\"start\":49976},{\"end\":50297,\"start\":50288},{\"end\":50309,\"start\":50301},{\"end\":50321,\"start\":50313},{\"end\":50330,\"start\":50325},{\"end\":50490,\"start\":50479},{\"end\":50498,\"start\":50494},{\"end\":50504,\"start\":50502},{\"end\":50514,\"start\":50508},{\"end\":50526,\"start\":50518},{\"end\":50532,\"start\":50530},{\"end\":50541,\"start\":50536},{\"end\":50553,\"start\":50545},{\"end\":50563,\"start\":50557},{\"end\":50576,\"start\":50567},{\"end\":50586,\"start\":50582},{\"end\":50597,\"start\":50590},{\"end\":50967,\"start\":50962},{\"end\":50978,\"start\":50971},{\"end\":51174,\"start\":51165},{\"end\":51186,\"start\":51178},{\"end\":51197,\"start\":51190},{\"end\":51205,\"start\":51201},{\"end\":51214,\"start\":51209},{\"end\":51403,\"start\":51399},{\"end\":51411,\"start\":51407},{\"end\":51419,\"start\":51415},{\"end\":51426,\"start\":51423},{\"end\":51435,\"start\":51430},{\"end\":51442,\"start\":51439},{\"end\":51454,\"start\":51446},{\"end\":51665,\"start\":51662},{\"end\":51671,\"start\":51669},{\"end\":51680,\"start\":51675},{\"end\":51686,\"start\":51684},{\"end\":51695,\"start\":51690},{\"end\":51701,\"start\":51699},{\"end\":51708,\"start\":51705},{\"end\":51836,\"start\":51833},{\"end\":52025,\"start\":52019},{\"end\":52036,\"start\":52029},{\"end\":52044,\"start\":52040},{\"end\":52228,\"start\":52225},{\"end\":52243,\"start\":52232},{\"end\":52252,\"start\":52247},{\"end\":52261,\"start\":52256},{\"end\":52489,\"start\":52487},{\"end\":52496,\"start\":52493},{\"end\":52503,\"start\":52500},{\"end\":52512,\"start\":52509},{\"end\":52520,\"start\":52516},{\"end\":52681,\"start\":52679},{\"end\":52689,\"start\":52685},{\"end\":52707,\"start\":52693},{\"end\":53011,\"start\":53007},{\"end\":53018,\"start\":53015},{\"end\":53025,\"start\":53022},{\"end\":53033,\"start\":53029},{\"end\":53227,\"start\":53224},{\"end\":53235,\"start\":53231},{\"end\":53242,\"start\":53239},{\"end\":53250,\"start\":53246},{\"end\":53435,\"start\":53431},{\"end\":53444,\"start\":53439},{\"end\":53460,\"start\":53448}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":32000,\"start\":31701},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":6529084},\"end\":32352,\"start\":32002},{\"attributes\":{\"id\":\"b2\"},\"end\":32705,\"start\":32354},{\"attributes\":{\"id\":\"b3\"},\"end\":32927,\"start\":32707},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":4396518},\"end\":33123,\"start\":32929},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":14542261},\"end\":33365,\"start\":33125},{\"attributes\":{\"id\":\"b6\"},\"end\":33607,\"start\":33367},{\"attributes\":{\"id\":\"b7\"},\"end\":33944,\"start\":33609},{\"attributes\":{\"id\":\"b8\"},\"end\":34157,\"start\":33946},{\"attributes\":{\"id\":\"b9\"},\"end\":34453,\"start\":34159},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":1629541},\"end\":34659,\"start\":34455},{\"attributes\":{\"id\":\"b11\"},\"end\":34878,\"start\":34661},{\"attributes\":{\"id\":\"b12\"},\"end\":35259,\"start\":34880},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":690715},\"end\":35473,\"start\":35261},{\"attributes\":{\"id\":\"b14\"},\"end\":35715,\"start\":35475},{\"attributes\":{\"id\":\"b15\"},\"end\":35943,\"start\":35717},{\"attributes\":{\"id\":\"b16\"},\"end\":36238,\"start\":35945},{\"attributes\":{\"id\":\"b17\"},\"end\":36479,\"start\":36240},{\"attributes\":{\"id\":\"b18\"},\"end\":36754,\"start\":36481},{\"attributes\":{\"id\":\"b19\"},\"end\":37001,\"start\":36756},{\"attributes\":{\"id\":\"b20\"},\"end\":37234,\"start\":37003},{\"attributes\":{\"id\":\"b21\"},\"end\":37501,\"start\":37236},{\"attributes\":{\"doi\":\"arXiv:1706.05587\",\"id\":\"b22\"},\"end\":37762,\"start\":37503},{\"attributes\":{\"id\":\"b23\"},\"end\":37924,\"start\":37764},{\"attributes\":{\"id\":\"b24\"},\"end\":38105,\"start\":37926},{\"attributes\":{\"id\":\"b25\"},\"end\":38273,\"start\":38107},{\"attributes\":{\"id\":\"b26\"},\"end\":38422,\"start\":38275},{\"attributes\":{\"id\":\"b27\"},\"end\":38579,\"start\":38424},{\"attributes\":{\"doi\":\"arXiv:1704.04861\",\"id\":\"b28\"},\"end\":38970,\"start\":38581},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":24982157},\"end\":39238,\"start\":38972},{\"attributes\":{\"id\":\"b30\"},\"end\":39560,\"start\":39240},{\"attributes\":{\"id\":\"b31\"},\"end\":39799,\"start\":39562},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":206593096},\"end\":40015,\"start\":39801},{\"attributes\":{\"id\":\"b33\"},\"end\":40224,\"start\":40017},{\"attributes\":{\"id\":\"b34\"},\"end\":40505,\"start\":40226},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":6613124},\"end\":40703,\"start\":40507},{\"attributes\":{\"id\":\"b36\"},\"end\":40980,\"start\":40705},{\"attributes\":{\"id\":\"b37\"},\"end\":41223,\"start\":40982},{\"attributes\":{\"id\":\"b38\"},\"end\":41594,\"start\":41225},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":5574079},\"end\":41820,\"start\":41596},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":2772063},\"end\":42054,\"start\":41822},{\"attributes\":{\"id\":\"b41\"},\"end\":42369,\"start\":42056},{\"attributes\":{\"id\":\"b42\"},\"end\":42624,\"start\":42371},{\"attributes\":{\"id\":\"b43\"},\"end\":42940,\"start\":42626},{\"attributes\":{\"id\":\"b44\"},\"end\":43155,\"start\":42942},{\"attributes\":{\"id\":\"b45\"},\"end\":43436,\"start\":43157},{\"attributes\":{\"doi\":\"arXiv:1503.02351\",\"id\":\"b46\"},\"end\":43613,\"start\":43438},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":9319522},\"end\":43915,\"start\":43615},{\"attributes\":{\"id\":\"b48\"},\"end\":44176,\"start\":43917},{\"attributes\":{\"id\":\"b49\"},\"end\":44439,\"start\":44178},{\"attributes\":{\"id\":\"b50\"},\"end\":44643,\"start\":44441},{\"attributes\":{\"doi\":\"arXiv:1506.04579\",\"id\":\"b51\"},\"end\":44830,\"start\":44645},{\"attributes\":{\"id\":\"b52\"},\"end\":45021,\"start\":44832},{\"attributes\":{\"id\":\"b53\"},\"end\":45269,\"start\":45023},{\"attributes\":{\"doi\":\"arXiv:1612.06851\",\"id\":\"b54\"},\"end\":45546,\"start\":45271},{\"attributes\":{\"doi\":\"arXiv:1701.06659\",\"id\":\"b55\"},\"end\":45771,\"start\":45548},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":623137},\"end\":45963,\"start\":45773},{\"attributes\":{\"id\":\"b57\"},\"end\":46269,\"start\":45965},{\"attributes\":{\"id\":\"b58\"},\"end\":46533,\"start\":46271},{\"attributes\":{\"id\":\"b59\"},\"end\":46803,\"start\":46535},{\"attributes\":{\"id\":\"b60\"},\"end\":47029,\"start\":46805},{\"attributes\":{\"id\":\"b61\"},\"end\":47275,\"start\":47031},{\"attributes\":{\"doi\":\"arXiv:1708.04943\",\"id\":\"b62\"},\"end\":47496,\"start\":47277},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":4773582},\"end\":47738,\"start\":47498},{\"attributes\":{\"id\":\"b64\"},\"end\":47970,\"start\":47740},{\"attributes\":{\"doi\":\"arXiv:1412.5474\",\"id\":\"b65\"},\"end\":48219,\"start\":47972},{\"attributes\":{\"doi\":\"arXiv:1608.04337\",\"id\":\"b66\"},\"end\":48608,\"start\":48221},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":12227989},\"end\":48858,\"start\":48610},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":60719890},\"end\":49276,\"start\":48860},{\"attributes\":{\"id\":\"b69\"},\"end\":49580,\"start\":49278},{\"attributes\":{\"id\":\"b70\"},\"end\":49963,\"start\":49582},{\"attributes\":{\"doi\":\"arXiv:1603.04467\",\"id\":\"b71\"},\"end\":50216,\"start\":49965},{\"attributes\":{\"id\":\"b72\"},\"end\":50475,\"start\":50218},{\"attributes\":{\"id\":\"b73\"},\"end\":50864,\"start\":50477},{\"attributes\":{\"id\":\"b74\"},\"end\":51119,\"start\":50866},{\"attributes\":{\"id\":\"b75\"},\"end\":51342,\"start\":51121},{\"attributes\":{\"doi\":\"arXiv:1702.08502\",\"id\":\"b76\"},\"end\":51623,\"start\":51344},{\"attributes\":{\"id\":\"b77\"},\"end\":51827,\"start\":51625},{\"attributes\":{\"id\":\"b78\"},\"end\":51969,\"start\":51829},{\"attributes\":{\"id\":\"b79\",\"matched_paper_id\":7200347},\"end\":52153,\"start\":51971},{\"attributes\":{\"id\":\"b80\"},\"end\":52395,\"start\":52155},{\"attributes\":{\"id\":\"b81\"},\"end\":52675,\"start\":52397},{\"attributes\":{\"doi\":\"arXiv:1611.10080\",\"id\":\"b82\"},\"end\":52924,\"start\":52677},{\"attributes\":{\"id\":\"b83\"},\"end\":53168,\"start\":52926},{\"attributes\":{\"id\":\"b84\"},\"end\":53358,\"start\":53170},{\"attributes\":{\"id\":\"b85\",\"matched_paper_id\":896827},\"end\":53598,\"start\":53360}]", "bib_title": "[{\"end\":32080,\"start\":32002},{\"end\":32975,\"start\":32929},{\"end\":33180,\"start\":33125},{\"end\":34509,\"start\":34455},{\"end\":35323,\"start\":35261},{\"end\":39054,\"start\":38972},{\"end\":39870,\"start\":39801},{\"end\":40565,\"start\":40507},{\"end\":41669,\"start\":41596},{\"end\":41885,\"start\":41822},{\"end\":43714,\"start\":43615},{\"end\":45829,\"start\":45773},{\"end\":47556,\"start\":47498},{\"end\":48676,\"start\":48610},{\"end\":48940,\"start\":48860},{\"end\":52015,\"start\":51971},{\"end\":53425,\"start\":53360}]", "bib_author": "[{\"end\":31775,\"start\":31761},{\"end\":31789,\"start\":31775},{\"end\":31799,\"start\":31789},{\"end\":31815,\"start\":31799},{\"end\":31823,\"start\":31815},{\"end\":31836,\"start\":31823},{\"end\":32094,\"start\":32082},{\"end\":32102,\"start\":32094},{\"end\":32109,\"start\":32102},{\"end\":32118,\"start\":32109},{\"end\":32127,\"start\":32118},{\"end\":32137,\"start\":32127},{\"end\":32148,\"start\":32137},{\"end\":32158,\"start\":32148},{\"end\":32427,\"start\":32417},{\"end\":32436,\"start\":32427},{\"end\":32445,\"start\":32436},{\"end\":32456,\"start\":32445},{\"end\":32469,\"start\":32456},{\"end\":32481,\"start\":32469},{\"end\":32491,\"start\":32481},{\"end\":32499,\"start\":32491},{\"end\":32510,\"start\":32499},{\"end\":32715,\"start\":32707},{\"end\":32723,\"start\":32715},{\"end\":32731,\"start\":32723},{\"end\":32741,\"start\":32731},{\"end\":32753,\"start\":32741},{\"end\":32765,\"start\":32753},{\"end\":32987,\"start\":32977},{\"end\":32999,\"start\":32987},{\"end\":33010,\"start\":32999},{\"end\":33191,\"start\":33182},{\"end\":33201,\"start\":33191},{\"end\":33211,\"start\":33201},{\"end\":33222,\"start\":33211},{\"end\":33381,\"start\":33367},{\"end\":33394,\"start\":33381},{\"end\":33406,\"start\":33394},{\"end\":33712,\"start\":33700},{\"end\":33721,\"start\":33712},{\"end\":33730,\"start\":33721},{\"end\":33741,\"start\":33730},{\"end\":33751,\"start\":33741},{\"end\":33760,\"start\":33751},{\"end\":34026,\"start\":34014},{\"end\":34039,\"start\":34026},{\"end\":34170,\"start\":34159},{\"end\":34177,\"start\":34170},{\"end\":34184,\"start\":34177},{\"end\":34196,\"start\":34184},{\"end\":34204,\"start\":34196},{\"end\":34216,\"start\":34204},{\"end\":34225,\"start\":34216},{\"end\":34238,\"start\":34225},{\"end\":34252,\"start\":34238},{\"end\":34519,\"start\":34511},{\"end\":34532,\"start\":34519},{\"end\":34543,\"start\":34532},{\"end\":34724,\"start\":34718},{\"end\":34735,\"start\":34724},{\"end\":34756,\"start\":34735},{\"end\":35026,\"start\":35015},{\"end\":35034,\"start\":35026},{\"end\":35044,\"start\":35034},{\"end\":35057,\"start\":35044},{\"end\":35334,\"start\":35325},{\"end\":35344,\"start\":35334},{\"end\":35553,\"start\":35542},{\"end\":35564,\"start\":35553},{\"end\":35573,\"start\":35564},{\"end\":35583,\"start\":35573},{\"end\":35798,\"start\":35789},{\"end\":35808,\"start\":35798},{\"end\":35818,\"start\":35808},{\"end\":36057,\"start\":36050},{\"end\":36067,\"start\":36057},{\"end\":36078,\"start\":36067},{\"end\":36336,\"start\":36325},{\"end\":36347,\"start\":36336},{\"end\":36585,\"start\":36573},{\"end\":36595,\"start\":36585},{\"end\":36604,\"start\":36595},{\"end\":36841,\"start\":36835},{\"end\":36850,\"start\":36841},{\"end\":36857,\"start\":36850},{\"end\":36864,\"start\":36857},{\"end\":37083,\"start\":37068},{\"end\":37094,\"start\":37083},{\"end\":37102,\"start\":37094},{\"end\":37336,\"start\":37318},{\"end\":37347,\"start\":37336},{\"end\":37358,\"start\":37347},{\"end\":37576,\"start\":37566},{\"end\":37590,\"start\":37576},{\"end\":37601,\"start\":37590},{\"end\":37609,\"start\":37601},{\"end\":37803,\"start\":37795},{\"end\":37810,\"start\":37803},{\"end\":37816,\"start\":37810},{\"end\":37824,\"start\":37816},{\"end\":37831,\"start\":37824},{\"end\":37978,\"start\":37972},{\"end\":37987,\"start\":37978},{\"end\":37994,\"start\":37987},{\"end\":38001,\"start\":37994},{\"end\":38181,\"start\":38170},{\"end\":38334,\"start\":38325},{\"end\":38498,\"start\":38485},{\"end\":38593,\"start\":38581},{\"end\":38600,\"start\":38593},{\"end\":38608,\"start\":38600},{\"end\":38624,\"start\":38608},{\"end\":38632,\"start\":38624},{\"end\":38642,\"start\":38632},{\"end\":38655,\"start\":38642},{\"end\":38663,\"start\":38655},{\"end\":39065,\"start\":39056},{\"end\":39073,\"start\":39065},{\"end\":39080,\"start\":39073},{\"end\":39087,\"start\":39080},{\"end\":39324,\"start\":39318},{\"end\":39333,\"start\":39324},{\"end\":39341,\"start\":39333},{\"end\":39347,\"start\":39341},{\"end\":39356,\"start\":39347},{\"end\":39363,\"start\":39356},{\"end\":39370,\"start\":39363},{\"end\":39633,\"start\":39620},{\"end\":39649,\"start\":39633},{\"end\":39666,\"start\":39649},{\"end\":39879,\"start\":39872},{\"end\":39885,\"start\":39879},{\"end\":39892,\"start\":39885},{\"end\":40079,\"start\":40068},{\"end\":40090,\"start\":40079},{\"end\":40100,\"start\":40090},{\"end\":40109,\"start\":40100},{\"end\":40343,\"start\":40334},{\"end\":40353,\"start\":40343},{\"end\":40579,\"start\":40567},{\"end\":40592,\"start\":40579},{\"end\":40794,\"start\":40787},{\"end\":40802,\"start\":40794},{\"end\":40820,\"start\":40802},{\"end\":40828,\"start\":40820},{\"end\":41052,\"start\":41042},{\"end\":41060,\"start\":41052},{\"end\":41068,\"start\":41060},{\"end\":41074,\"start\":41068},{\"end\":41086,\"start\":41074},{\"end\":41348,\"start\":41338},{\"end\":41362,\"start\":41348},{\"end\":41374,\"start\":41362},{\"end\":41384,\"start\":41374},{\"end\":41396,\"start\":41384},{\"end\":41685,\"start\":41671},{\"end\":41695,\"start\":41685},{\"end\":41896,\"start\":41887},{\"end\":41904,\"start\":41896},{\"end\":41915,\"start\":41904},{\"end\":42149,\"start\":42139},{\"end\":42163,\"start\":42149},{\"end\":42175,\"start\":42163},{\"end\":42185,\"start\":42175},{\"end\":42197,\"start\":42185},{\"end\":42452,\"start\":42444},{\"end\":42464,\"start\":42452},{\"end\":42475,\"start\":42464},{\"end\":42483,\"start\":42475},{\"end\":42635,\"start\":42626},{\"end\":42649,\"start\":42635},{\"end\":42667,\"start\":42649},{\"end\":42677,\"start\":42667},{\"end\":42683,\"start\":42677},{\"end\":42689,\"start\":42683},{\"end\":42698,\"start\":42689},{\"end\":42706,\"start\":42698},{\"end\":43003,\"start\":42996},{\"end\":43009,\"start\":43003},{\"end\":43016,\"start\":43009},{\"end\":43025,\"start\":43016},{\"end\":43033,\"start\":43025},{\"end\":43249,\"start\":43235},{\"end\":43259,\"start\":43249},{\"end\":43269,\"start\":43259},{\"end\":43281,\"start\":43269},{\"end\":43451,\"start\":43438},{\"end\":43462,\"start\":43451},{\"end\":43727,\"start\":43716},{\"end\":43737,\"start\":43727},{\"end\":43749,\"start\":43737},{\"end\":44001,\"start\":43986},{\"end\":44010,\"start\":44001},{\"end\":44019,\"start\":44010},{\"end\":44032,\"start\":44019},{\"end\":44284,\"start\":44273},{\"end\":44296,\"start\":44284},{\"end\":44508,\"start\":44497},{\"end\":44519,\"start\":44508},{\"end\":44531,\"start\":44519},{\"end\":44652,\"start\":44645},{\"end\":44666,\"start\":44652},{\"end\":44676,\"start\":44666},{\"end\":44896,\"start\":44886},{\"end\":44904,\"start\":44896},{\"end\":44912,\"start\":44904},{\"end\":45079,\"start\":45070},{\"end\":45089,\"start\":45079},{\"end\":45101,\"start\":45089},{\"end\":45107,\"start\":45101},{\"end\":45120,\"start\":45107},{\"end\":45132,\"start\":45120},{\"end\":45286,\"start\":45271},{\"end\":45300,\"start\":45286},{\"end\":45309,\"start\":45300},{\"end\":45318,\"start\":45309},{\"end\":45556,\"start\":45548},{\"end\":45563,\"start\":45556},{\"end\":45572,\"start\":45563},{\"end\":45581,\"start\":45572},{\"end\":45591,\"start\":45581},{\"end\":45838,\"start\":45831},{\"end\":45846,\"start\":45838},{\"end\":45853,\"start\":45846},{\"end\":46080,\"start\":46073},{\"end\":46089,\"start\":46080},{\"end\":46097,\"start\":46089},{\"end\":46105,\"start\":46097},{\"end\":46359,\"start\":46349},{\"end\":46370,\"start\":46359},{\"end\":46381,\"start\":46370},{\"end\":46390,\"start\":46381},{\"end\":46627,\"start\":46619},{\"end\":46636,\"start\":46627},{\"end\":46642,\"start\":46636},{\"end\":46649,\"start\":46642},{\"end\":46656,\"start\":46649},{\"end\":46876,\"start\":46865},{\"end\":46886,\"start\":46876},{\"end\":46897,\"start\":46886},{\"end\":46905,\"start\":46897},{\"end\":47069,\"start\":47060},{\"end\":47080,\"start\":47069},{\"end\":47094,\"start\":47080},{\"end\":47107,\"start\":47094},{\"end\":47117,\"start\":47107},{\"end\":47126,\"start\":47117},{\"end\":47138,\"start\":47126},{\"end\":47342,\"start\":47336},{\"end\":47349,\"start\":47342},{\"end\":47357,\"start\":47349},{\"end\":47363,\"start\":47357},{\"end\":47567,\"start\":47558},{\"end\":47576,\"start\":47567},{\"end\":47584,\"start\":47576},{\"end\":47593,\"start\":47584},{\"end\":47600,\"start\":47593},{\"end\":47809,\"start\":47802},{\"end\":47821,\"start\":47809},{\"end\":47830,\"start\":47821},{\"end\":47836,\"start\":47830},{\"end\":47842,\"start\":47836},{\"end\":48049,\"start\":48042},{\"end\":48059,\"start\":48049},{\"end\":48074,\"start\":48059},{\"end\":48373,\"start\":48365},{\"end\":48380,\"start\":48373},{\"end\":48391,\"start\":48380},{\"end\":48686,\"start\":48678},{\"end\":48699,\"start\":48686},{\"end\":48709,\"start\":48699},{\"end\":48717,\"start\":48709},{\"end\":48958,\"start\":48942},{\"end\":48979,\"start\":48958},{\"end\":48989,\"start\":48979},{\"end\":49005,\"start\":48989},{\"end\":49288,\"start\":49278},{\"end\":49299,\"start\":49288},{\"end\":49308,\"start\":49299},{\"end\":49323,\"start\":49308},{\"end\":49338,\"start\":49323},{\"end\":49733,\"start\":49719},{\"end\":49745,\"start\":49733},{\"end\":49758,\"start\":49745},{\"end\":49974,\"start\":49965},{\"end\":49985,\"start\":49974},{\"end\":50299,\"start\":50286},{\"end\":50311,\"start\":50299},{\"end\":50323,\"start\":50311},{\"end\":50332,\"start\":50323},{\"end\":50492,\"start\":50477},{\"end\":50500,\"start\":50492},{\"end\":50506,\"start\":50500},{\"end\":50516,\"start\":50506},{\"end\":50528,\"start\":50516},{\"end\":50534,\"start\":50528},{\"end\":50543,\"start\":50534},{\"end\":50555,\"start\":50543},{\"end\":50565,\"start\":50555},{\"end\":50578,\"start\":50565},{\"end\":50588,\"start\":50578},{\"end\":50599,\"start\":50588},{\"end\":50969,\"start\":50960},{\"end\":50980,\"start\":50969},{\"end\":51176,\"start\":51163},{\"end\":51188,\"start\":51176},{\"end\":51199,\"start\":51188},{\"end\":51207,\"start\":51199},{\"end\":51216,\"start\":51207},{\"end\":51405,\"start\":51397},{\"end\":51413,\"start\":51405},{\"end\":51421,\"start\":51413},{\"end\":51428,\"start\":51421},{\"end\":51437,\"start\":51428},{\"end\":51444,\"start\":51437},{\"end\":51456,\"start\":51444},{\"end\":51667,\"start\":51660},{\"end\":51673,\"start\":51667},{\"end\":51682,\"start\":51673},{\"end\":51688,\"start\":51682},{\"end\":51697,\"start\":51688},{\"end\":51703,\"start\":51697},{\"end\":51710,\"start\":51703},{\"end\":51838,\"start\":51829},{\"end\":52027,\"start\":52017},{\"end\":52038,\"start\":52027},{\"end\":52046,\"start\":52038},{\"end\":52230,\"start\":52223},{\"end\":52245,\"start\":52230},{\"end\":52254,\"start\":52245},{\"end\":52263,\"start\":52254},{\"end\":52491,\"start\":52485},{\"end\":52498,\"start\":52491},{\"end\":52505,\"start\":52498},{\"end\":52514,\"start\":52505},{\"end\":52522,\"start\":52514},{\"end\":52683,\"start\":52677},{\"end\":52691,\"start\":52683},{\"end\":52709,\"start\":52691},{\"end\":53013,\"start\":53005},{\"end\":53020,\"start\":53013},{\"end\":53027,\"start\":53020},{\"end\":53035,\"start\":53027},{\"end\":53229,\"start\":53222},{\"end\":53237,\"start\":53229},{\"end\":53244,\"start\":53237},{\"end\":53252,\"start\":53244},{\"end\":53437,\"start\":53427},{\"end\":53446,\"start\":53437},{\"end\":53462,\"start\":53446}]", "bib_venue": "[{\"end\":31759,\"start\":31701},{\"end\":32162,\"start\":32158},{\"end\":32415,\"start\":32354},{\"end\":32801,\"start\":32765},{\"end\":33018,\"start\":33010},{\"end\":33232,\"start\":33222},{\"end\":33479,\"start\":33406},{\"end\":33698,\"start\":33609},{\"end\":34012,\"start\":33946},{\"end\":34292,\"start\":34252},{\"end\":34547,\"start\":34543},{\"end\":34716,\"start\":34661},{\"end\":35013,\"start\":34880},{\"end\":35348,\"start\":35344},{\"end\":35540,\"start\":35475},{\"end\":35787,\"start\":35717},{\"end\":36048,\"start\":35945},{\"end\":36323,\"start\":36240},{\"end\":36571,\"start\":36481},{\"end\":36833,\"start\":36756},{\"end\":37066,\"start\":37003},{\"end\":37316,\"start\":37236},{\"end\":37564,\"start\":37503},{\"end\":37793,\"start\":37764},{\"end\":37970,\"start\":37926},{\"end\":38168,\"start\":38107},{\"end\":38323,\"start\":38275},{\"end\":38483,\"start\":38424},{\"end\":38761,\"start\":38679},{\"end\":39095,\"start\":39087},{\"end\":39316,\"start\":39240},{\"end\":39618,\"start\":39562},{\"end\":39900,\"start\":39892},{\"end\":40066,\"start\":40017},{\"end\":40332,\"start\":40226},{\"end\":40596,\"start\":40592},{\"end\":40785,\"start\":40705},{\"end\":41040,\"start\":40982},{\"end\":41336,\"start\":41225},{\"end\":41699,\"start\":41695},{\"end\":41927,\"start\":41915},{\"end\":42137,\"start\":42056},{\"end\":42442,\"start\":42371},{\"end\":42770,\"start\":42706},{\"end\":42994,\"start\":42942},{\"end\":43233,\"start\":43157},{\"end\":43518,\"start\":43478},{\"end\":43757,\"start\":43749},{\"end\":43984,\"start\":43917},{\"end\":44271,\"start\":44178},{\"end\":44495,\"start\":44441},{\"end\":44729,\"start\":44692},{\"end\":44884,\"start\":44832},{\"end\":45068,\"start\":45023},{\"end\":45399,\"start\":45334},{\"end\":45649,\"start\":45607},{\"end\":45857,\"start\":45853},{\"end\":46071,\"start\":45965},{\"end\":46347,\"start\":46271},{\"end\":46617,\"start\":46535},{\"end\":46863,\"start\":46805},{\"end\":47058,\"start\":47031},{\"end\":47334,\"start\":47277},{\"end\":47608,\"start\":47600},{\"end\":47800,\"start\":47740},{\"end\":48040,\"start\":47972},{\"end\":48363,\"start\":48221},{\"end\":48725,\"start\":48717},{\"end\":49053,\"start\":49005},{\"end\":49419,\"start\":49338},{\"end\":49717,\"start\":49582},{\"end\":50078,\"start\":50001},{\"end\":50284,\"start\":50218},{\"end\":50654,\"start\":50599},{\"end\":50958,\"start\":50866},{\"end\":51161,\"start\":51121},{\"end\":51395,\"start\":51344},{\"end\":51658,\"start\":51625},{\"end\":51889,\"start\":51838},{\"end\":52054,\"start\":52046},{\"end\":52221,\"start\":52155},{\"end\":52483,\"start\":52397},{\"end\":52792,\"start\":52725},{\"end\":53003,\"start\":52926},{\"end\":53220,\"start\":53170},{\"end\":53470,\"start\":53462},{\"end\":33238,\"start\":33234}]"}}}, "year": 2023, "month": 12, "day": 17}