{"id": 246480208, "updated": "2022-09-30 03:47:39.104", "metadata": {"title": "Deep Learning-Based Image Semantic Coding for Semantic Communications", "authors": "[{\"first\":\"Danlan\",\"last\":\"Huang\",\"middle\":[]},{\"first\":\"Xiaoming\",\"last\":\"Tao\",\"middle\":[]},{\"first\":\"Feifei\",\"last\":\"Gao\",\"middle\":[]},{\"first\":\"Jianhua\",\"last\":\"Lu\",\"middle\":[]}]", "venue": "2021 IEEE Global Communications Conference (GLOBECOM)", "journal": "2021 IEEE Global Communications Conference (GLOBECOM)", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "This paper presents the Generative Adversarial Networks (GANs)-based image semantic coding, the goal of which is semantic exchange rather than symbol transmission. State-of-the-art visually pleasing reconstruction and semantic preserving performance are obtained in extreme low bitrate via a rate-perception-distortion optimization framework. In particular, we investigate convolutional encoder, quantizer, conditional SPADE generator, residual coding as well as perceptual losses. In contrast to previous work, we designed a coarse-to-fine image semantic coding model for multimedia semantic communication system. The base layer of the image is fully generated and preserves semantic information while the enhancement layer restores the fine details. We explore the perception and distortion performance trade-off by tuning the rate of base layer and enhancement layer. Different from the existing methods that adopt pixel accuracy as distortion metric, we train and evaluate the proposed image semantic coding model with multiple perception metrics, in line with the purpose of semantic communications. Experimental results demonstrate that our model could achieve visually pleasant and semantic consistent reconstruction, as well as saving times of bitrate, compared to BPG, WebP, JPEG2000, JPEG, and other deep learning-based image codecs.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/globecom/HuangTGL21", "doi": "10.1109/globecom46510.2021.9685667"}}, "content": {"source": {"pdf_hash": "1f85e686d848db4d8797aab4a67ba5838c6cffed", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "0423a19c0662f3aec750dee339210f63335e88b5", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/1f85e686d848db4d8797aab4a67ba5838c6cffed.txt", "contents": "\nDeep Learning-Based Image Semantic Coding for Semantic Communications\n\n\nDanlan Huang \nBeijing National Research Center for Information Science and Technology\nTsinghua University\nBeijingChina\n\nXiaoming Tao \nBeijing National Research Center for Information Science and Technology\nTsinghua University\nBeijingChina\n\nFeifei Gao \nBeijing National Research Center for Information Science and Technology\nTsinghua University\nBeijingChina\n\nJianhua Lu \nBeijing National Research Center for Information Science and Technology\nTsinghua University\nBeijingChina\n\nDeep Learning-Based Image Semantic Coding for Semantic Communications\n10.1109/GLOBECOM46510.2021.9685667Index Terms-Generative adversarial networksimage seman- tic codingsemantic communicationsdeep learningperceptual metric\nThis paper presents the Generative Adversarial Networks (GANs)-based image semantic coding, the goal of which is semantic exchange rather than symbol transmission. State-ofthe-art visually pleasing reconstruction and semantic preserving performance are obtained in extreme low bitrate via a rateperception-distortion optimization framework. In particular, we investigate convolutional encoder, quantizer, conditional SPADE generator, residual coding as well as perceptual losses. In contrast to previous work, we designed a coarse-to-fine image semantic coding model for multimedia semantic communication system. The base layer of the image is fully generated and preserves semantic information while the enhancement layer restores the fine details. We explore the perception and distortion performance trade-off by tuning the rate of base layer and enhancement layer. Different from the existing methods that adopt pixel accuracy as distortion metric, we train and evaluate the proposed image semantic coding model with multiple perception metrics, in line with the purpose of semantic communications. Experimental results demonstrate that our model could achieve visually pleasant and semantic consistent reconstruction, as well as saving times of bitrate, compared to BPG, WebP, JPEG2000, JPEG, and other deep learning-based image codecs.\n\nI. INTRODUCTION\n\nWith the booming of high-resolution visual applications such as 4K/8K streaming, Virtual Reality (VR), and smart surveillance, it is a critical issue to obtain superior efficient image/video compression methods. Modern engineered codecs such as WebP [1], JPEG [2], JPEG2000 [3], and BPG [4] based on HEVC, follow the transform hybrid coding framework, and incorporates off-the-shelf transformation. Recently, the advance of deep learning methods accelerate the development of lossy image compression. In such a case, a neural network is directly optimized for the rate-distortion trade-off, which led to new state-of-the-art methods.\n\nHowever, for extreme low bitrates (below 0.1 bits per pixel (bpp)), all these approaches degrade images significantly as traditional metrics favor pixel-wise preservation of local structure over preserving semantic information. Engineered codecs start to exhibit algorithm-specific artifacts such as blocking and ringing. Learning-based approaches may yield blurry, checkerboard artifacts, and poor texture reconstruction while relying on MS-SSIM and MSE metrics.\n\nSemantic communications, whose key idea is to pursuit semantic fidelity instead of minimizing bit-or-symbol error rate has received increasing attention [5], [6]. It is readily known that the existing works for semantic communications only consider the text/speech information, and attempt to preserve the meaning of the sentence [7] by the language model. However, since the image data has much richer semantics, the design of multimedia semantic communication system is crucial. Inspired by these factors, this work designs a GANsbased semantic communication system for image transmission, whose goal is recovering the meaning of the image utilizing priory knowledge, rather than restoring each pixel. GANs are a promising candidate to alleviate compression artifacts and produce perceptually convincing reconstructions. This work focus on multimedia Industrial Internet services such as smart surveillance, where the semantic meaning behind digital bits plays a much more important role than local pixels. To the best of our knowledge, such work is a pioneer in multimedia semantic communications.\n\nRather than sticking to traditional pixel accuracy based metrics PSNR and SSIM, we further advance image semantic coding by minimizing the mismatch in perceptual and semantic level. The work [8] is an initial investigation of the image perception quality, which is at odds with the distortion performance. Particularly, the distortion is inconsistent in pixel level and the perception is the feature or distribution divergence between input and reconstructed images. In low bitrate, minimizing distortion alone is at the expense of perception. We explore the triple \"rate-distortion-perception\" trade-off, which shows that perception metric is a new way to optimize semantic communication systems. We adopt adversarial loss and feature reconstruction losses as perception metrics.\n\nOur main contributions in this work are threefold.\n\n\u2022 We propose a deep learning-based image compression method to achieve efficient semantic reconstructions, which can be applied to multimedia semantic communication systems in Industrial Internet. Extremely low bitrate is achieved by learning the representation for semantic primitives. \u2022 We employ GANs and BPG residual coding to propose a coarse-to-fine image compression framework. The generator restores semantic information and synthesizes a visually pleasant base layer. Then the residuals coded by BPG refine details to a different extent in the enhancement layer. These two layers adjust the performance of perception and distortion. \u2022 We optimize and evaluate the efficiency of semantic exchange of the proposed semantic coding approach with the ensemble of diverse perception metrics. The reconstruction results are quantitatively evaluated by FID [9], KID [10] and LPIPS [11], which coincide to human perception. Besides, the semantic segmentation metrics mean Intersection-over-Union (mIoU) is adopted to evaluate the performance of semantic fidelity.\n\n\nII. RELATED WORK\n\n\nA. Deep Learning-based Image Compression\n\nExisting learning-based image compression incorporates auto-encoders to transform the image signals into latent representations. To decrease the required bits, various probability models for effective entropy coding are investigated, using hierarchical priors, auto-regression with context model, etc [12], [13]. GANs have led to rapid progress in image generation, and it attempts to learn the distribution of natural images, rather than pursuing the local reconstruction precision. State-of-the-art GANs are able to yield high-resolution photorealistic images [14], [15], and are employed in deep image compression models to improve the visual quality. The work [16] presented a generative compressor operating on extremely low bitrates, where the details or unimportant regions are synthesized, reducing the storage cost. However, their reconstructions tend to only preserve high-level semantics, deviating significantly from the input. Taking advantage of the semantic label, DSSLIC [17] presented a deep layered image compression. However, the model pursues fairly traditional pixel accuracy, failing to restore semantic information. Our work follows the coarse-to-fine reconstruction architecture, however with new objectives in a semantic communication scenario.\n\n\nB. Semantic Communications\n\nDating back to 1948, Shannon defined three levels of communications: the bit level, semantic level, and effective level. The semantic communications interprets information at the semantic level, and attempts to transmit the symbol that precisely convey the desired meaning [6]. The third level concerns the effectiveness of certain tasks on the receiver side, according to the instructor of transmitter. The semantic communication systems for text transmission is proposed in [5], [7], which enables intelligent communications for humanto-machine and machine-to-machine. A lossless semantic data compression theory is investigated in [18], and it claimed that the data can be compressed significantly at a semantic level. We propose the image semantic coding approach for multimedia semantic communications, utilizing prior information such as image distribution model.\n\n\nIII. GANS-BASED SEMANTIC IMAGE CODING A. System model\n\nThe proposed GANs framework for semantic image coding at extreme low bitrate can be viewed as a combination of encoder, conditional generator with spatial normalization, and residual coding in refinement module, as illustrated in Fig. 1. The semantic information includes latent code\u0175 and semantic label s, which are obtained by encoder E and semantic segmentation module F , respectively. The semantic information is losslessly encoded at low bitrate and restored by GANs effectively. The discrepancy between the input and synthesized images is recorded in the residual r = x \u2212x and transmitted to the receiver side after lossy image coding.\n\nAs shown in Fig. 1(a), the encoded latent representation\u0175, semantic label s, and the residual r are transmitted. As shown in Fig. 1(b), the latent representation\u0175 and semantic label s are fed into the generator G to get a coarse estimate of the input image, denoted asx. The BPG compressed residual r is then added to the base layer to get the final reconstruction x =x + r . Fig. 1(c) shows the architecture of the conditional generator G. Inspired by SPADE generator [14], a spatiallyadaptive normalization is adopted to learn a mapping that can convert a semantic label s to a photo-realistic image. We train a multi-scale discriminator for local and global evaluation.\n\nThe architecture of the SPADE residual block (ResBlk) is shown in Fig. 2. The G consists of several ResNet blocks with spatial normalization layers, whose learned modulation parameters \u03b2, \u03b3 have encoded the spatial information about the semantic layout. Unlike the SPADE generator, which discards the encoder part of the generator, the latent representation\u0175 is served as the conditional input in our scheme. Therefore, the SPADE learns the general distribution of the dataset for different classes and maps the semantic label s to the general distribution of natural images. In addition, the latent representation preserves the specific information of the input image x. Our generator architecture achieves better performance than the leading image generation model pix2pixHD [15] since the essential semantics are preserved properly.\n\nB. Architecture 1) Encoder: The convolutional encoder E and quantizer Q map the input image x into a low dimensional latent representation\u0175 = Q(E(x)). The Huffman entropy coding algorithm is employed to store\u0175 losslessly, where frequencies for each channel are stored, and the rate is R(\u0175) = \u2212log(P (\u0175)), where P is the estimated probability distribution. The scalar variant of the quantization approach is adopted, with the quantization centers c j set by nearest neighbor assignment, C = {\u22123, \u22122, \u22121, 0, 1, 2, 3}:\nw i = Q(w i ) := argmin j w i \u2212 c j .\n(1)\n\nIn the gradient back propagation pass, we use the differentiable version of soft quantization:  where L is the number of quantization center, and L = 7.\nw i = L j=1 exp(\u2212\u03c3 w i \u2212 c j ) L l=1 exp(\u2212\u03c3 w i \u2212 c l ) c j ,(2)\nThe architecture of E is respectively defined as five downsampling convolutional layers, {c64s2, c128s2, c256s2, c512s2, c512s2, cCs1}, where c k s d denotes a 3 \u00d7 3 convolution layer (with k filters and stride d) followed by instance normalization and LeakyReLU. The bottleneck depth C is the number of filters for the last layer, and it affects the bitrate.\n\n2) Generator: The generator G is conditioned on latent representation\u0175 and semantic label s. It maps samples\u0175 from a fixed known distribution p\u0174 to an unknown joint distribution p X|S , yielding a semantic reconstructionx = G(\u0175, s) that is consistent with the image distribution and preserves the content of the input image x. The discriminator D distinguishes the input (x, s) from the generated ones (G(\u0175, s), s). Additionally, F is a pre-trained semantic segmentation model PSPNet [19]. Besides, since the semantic information of the image is included in the bit-stream, the proposed compression model can facilitate many other vision tasks such as image understanding and retrieval. The conditional GANs objective function can be formulated as\nL G = E\u0175 \u223cp\u0174 [\u2212log(D(G(\u0175, s), s))], L D = E\u0175 \u223cp\u0174 [\u2212log(1 \u2212 D(G(\u0175, s), s))] + E x\u223cp X|s [\u2212log(D(x, s))].(3)\nThe generator explores the rate-perception-distortion theory beyond Shannon's rate-distortion theory. The compression incurs a perception loss, which can be modeled as the dissimilarity of the input and reconstructed ones in feature and distribution level. The feature-level loss d(G(\u0175, s), x) includes a weighted combination of VGG feature reconstruction loss [20] and LPIPS [11]. The distribution loss is the adversarial loss in GANs L G . In addition, the distortion can be formulated as the L 1 loss of the images E[ G(\u0175, s) \u2212 x 1 ]. The semantic coding model can be optimized by minimizing the rateperception-distortion trade-off:\nL E,G = L G + \u03bb 1 (E[d(x, G(\u0175, s))] + E[ G(\u0175, s) \u2212 x 1 ]) + \u03bb 2 (R(\u0175) + R(s) + R(r )),(4)\nwhere R(s), R(r ) are the rate of the semantic label and coded residual, respectively. These two are independent with G and can be eliminated. The coefficient is set to be \u03bb 1 = 10. If E cannot afford to store the exact detail, G is able to synthesize it to satisfy natural image distribution, instead of showing blocky and blurry effects. We follow [16] to treat the entropy term \u03bb 2 R(\u0175), where \u03bb 2 = 0. The encoder E and generator G are trained together, alternatively optimized with discriminator D.\n\nThe architecture of the generator G is respectively defined as five upsampling layers, each followed by a SPADE residual block. We denote the architecture as {ResBlk 512 ,ResBlk 512 , up 2 , ResBlk 512 , up 2 , ResBlk 512 , up 2 , ResBlk 256 , up 2 , ResBlk 128 , up 2 , ResBlk 64 }, where ResBlk k denotes a 3\u00d73 convolution layer followed by spatial normalization and ReLU in SPADE residual block (with k filters and stride 1), and up2 is the upsampling layer by factor 2.\n\n3) Residual Coding: Our approach is applicable for layered image coding, with the semantic information serving as the base layer and the residual serve as the enhancement layer. In bandwidth-limited scenarios, only the fully synthesized base layer with extreme bitrate is transmitted for semantic exchange. Otherwise, the residual is encoded to different bitrates according to the channel state.\n\nThe GANs-based generator alone will suffer from the discrepancy between the original one and the generated one since the missing of fine information. The residual coding attempts to keep the texture consistency. The residual r is compressed by a lossy codec BPG with diverse compression factors, with the residual r rescaled to [0,255] to avoid negative values. The final output x is the element-wise addition of semantic reconstructionx and fine details r . Moreover, with the help of semantic label s, we can manually highlight the regions for residual coding, leaving the rest fully synthesized. For instance, the trees and road in Cityscapes dataset are of less importance, no need to be the same as the original ones. The details of the pedestrian and license plate are expected to be recorded by residual coding in smart surveillance.\n\n\nC. Evaluation Metrics\n\nWe explore the rate-perception-distortion performance of the proposed model. Rate is controlled by the bottleneck depth C, and the compression factor of residual. The distortion is the inconsistency of the input and reconstructed images in local structure, and can be measured by traditional metrics PSNR and SSIM. However, there is an apparent limitation of these traditional metrics, since pixel reconstruction is contrary to semantic reconstruction, and the reconstructed image deviates from human perception. Therefore, it is significant to pursue the perception performance of the semantic reconstruction, which is the efficiency in maintaining the global image content. Since perceptual similarity is a property shared across deep visual representations, the deep learning-based metrics outperform traditional ones by a large margin. Perception can be measured by the distribution divergence of the input and reconstructed images or the dissimilar in deep feature level between individual pairs. The distribution divergence is measured by FID (Fr\u00e9chet Inception Distance score) [9] and KID (Kernel-Inception Distance) [10]. The perceptual similarity between two images is measured by LPIPS (Learned Perceptual Image Patch Similarity) [11] and mIoU (mean intersection-over-union).\n\nFID evaluates the quality of images generated by GANs, and lower scores have been shown to correlate well with higher-quality images. KID measures the maximum mean discrepancy in the feature space of a classifier (e.g., Inception Network), and a lower score implies better sample quality.\n\nAdditionally, LPIPS is validated to predict human scores for degradation, and a lower score is better. To evaluate the semantic fidelity of the reconstructed image, we therefore measure the capacity of the proposed model in preserving the image semantics. Specifically, the pre-trained semantic segmentation model PSPNet [19] is adopted to measure the mean Intersection-over-Union (mIoU) between the semantic label maps obtained from the decompressed validation images and the ground truth ones, and a higher score implies better performance.\n\n\nIV. EXPERIMENT A. Settings\n\nThe proposed GANs-based semantic image coding is trained on the Cityscapes dataset [21], where the images are downscaled to 512 \u00d7 1024. The evaluation is conducted on the 500 images in the validation set. The bitrate of the proposed model consists of three parts: the latent code\u0175, the residual r , and the semantic label s. The bitrate of\u0175 is controlled by bottleneck depth C \u2208 {64, 128}, and larger C stores more semantic information. We train two variants of the proposed model, which are denoted as OursC64 and OursC128. The corresponding bitrates of\u0175 are about 0.008bpp and 0.016bpp. Moreover, r is compressed by BPG under different factors {50, 45, 40, 30, 25}, the bitrate of which varies from 0.03bpp to more than 0.2bpp. Specifically, larger residual r stores more texture detail. The semantic label s is compressed losslessly at about 0.01bpp in vector graphic form.\n\n\nB. Baselines\n\nThe baseline encoders are BPG [4], WebP [1], JPEG2000 [3], JPEG [2], and the deep learning-based codec DSSLIC [17]. BPG is the current state-of-the-art engineered image compression codec in terms of PSNR. We train the DSSLIC network on Cityscapes exactly following the procedure in [17]. The so-obtained model produces a bitrate of 0.13-0.25 bpp. DSSLIC is a layered image compressor trained with SSIM and PSNR metric, in which semantic segmentation-based GANs produces the base layer and BPG yields the enhancement layer. C. Results   Fig. 3 shows the perception and distortion performance at different bitrates of the proposed semantic coding model compared to BPG, WebP, JPEG2000, JPEG, and DSSLIC averaged over the Cityscapes validation set. In the proposed model, the base layer alone yields the lowest bitrates with fully synthesized image, 0.026bpp for OursC64 and 0.018bpp for OursC64. In the subsequent bitrates, the enhancement layer produces more details through residual coding.\n\n1) Rate Perception Performance: The proposed semantic coding outperforms the counterparts by a large margin in a large range of bitrates, especially in extremely low bitrate (< 0.1bpp) in terms of perception. The performance of our model is basically improved with a higher bitrate, and OursC128 results in slightly better perception performance than OursC64. This can be interpreted that the latent code with a larger bottleneck C contains adequate high-level semantics. It can be observed from Fig. 3 (a) that the FID values of our model are generally below 50, and it takes more than twice or four times higher the bitrate for WebP and JPEG to achieve the same perception level. Fig. 3 (b) shows the KID (\u00d7100) value of our model is generally below 0, which means the reconstructed images are photo-realistic and hard to be distinguished from real ones. Fig. 3 (c) shows the LPIPS performance of our model is superior in low bitrate. Fig. 3 (d) demonstrates the semantic fidelity of the reconstructed images in terms of mIoU. The content is faithfully generated by our model from the semantic label and feasibly combined with the residual. The mIoU at the lowest bitrate achieves the peak since the image is fully generated according to the semantic label. The performance slightly drops with BPG residual coding at high compression factors, due to the blurry artifacts of BPG. The performance of the deep learning-based codec DSSLIC (cyan curve) [17] is rarely steady with different bitrates. It produces good mIoU performance, however failing to achieve extreme low bitrate. The semantic fidelity performance of the standard codecs is considerably lower, which can be attributed to blurry and blocky artifacts. These demonstrate the effectiveness of the proposed model in semantic exchange.\n\n2) Rate Distortion Performance: Fig. 3 (e) (f) demonstrate the distortion performance in terms of PSNR and SSIM. The proposed model is inferior to some of the baselines whose goal is minimizing MSE. This is because the content of our model is synthesized by abstract semantics, and may not coincide with the original one in local texture under extreme low bitrate. Moreover, larger C results in lower performance, since fewer details can be stored in residual under the same bitrate. These demonstrate the trade-off between perception and distortion, and the limitation of the traditional objective. Lower MSE may not result in better semantic reconstruction.\n\n3) Visual Results: Fig. 4 illustrates the examples of decoded images of the proposed GANs-based semantic coding and that of the baselines. The bitrate of our model is 0.049bpp, while the counterparts are of equal or higher bitrates. It can be observed that the proposed model produces sharper and photorealistic images, even though the counterparts use twice or triple higher the bitrate. The unimportant regions such as trees, buildings, roads, etc. are not the same as the original ones. The pedestrians are faithfully preserved due to residual coding. In particular, the baseline DSSLIC produces blurry reconstructions even though it uses GANs architecture, demonstrating the advantage of the proposed SPADE generator. The engineered codecs suffer from blurry and blocky artifacts.\n\n\nV. CONCLUSION\n\nIn this paper, we presented a GANs-based semantic image coding approach for multimedia semantic communications, the goal of which is the semantic exchange instead of symbol transmission. The encoder extracts semantic information to interpret the meaning of the image, which also enables multiple vision tasks. Since perception and distortion are at odds with each other, we optimize and evaluate our approach with a diverse set of metrics and show the results in terms of rate-perception and rate-distortion. By comparing with the conventional pixel-wise distortion metrics, we showed that perception metrics such as FID, KID, LPIPS, and mIoU can be valuable tools to better predict human preferences and evaluate the efficiency of semantic exchange. Compared to the baselines which even use several times of the bits, the proposed approach yields preferred performance by semantic reconstruction as well as fine detail residual coding. The reconstructed images achieve high semantic fidelity and appear visually close to the input. Future work could focus on perceptual score indices and metrics, and further investigating the joint source and channel coding. Additionally, it is worth exploring semantic video coding for future work.\n\n\nThis work was supported by the National Key R&D Program of China (2018YFB1800804), the National Natural Science Foundation of China (NSFC 61925105, 61801260), and the fellowship of China National Postdoctoral Program for Innovative Talents (BX20200194). This work was also supported by Tsinghua University-China Mobile Communications Group Co., Ltd. Joint Institute. Xiaoming Tao is the corresponding author. Email:{huangdl, taoxm, feifeigao, lhh-dee}@mail.tsinghua.edu.cn.\n\n( a )\naThe network architecture at the sender side.(b) The network architecture at the receiver side.(c) The network architecture of the generator.\n\nFig. 1 :\n1The architecture of the proposed deep learning-based image semantic coding. (a) The sender side includes encoder E, Quantizer Q, feature extractor F , generator G and the residual module. (b) G semantically reconstructs the image content as the base layer, and the residual r refines details as the enhancement layer. (c) Each normalization layer in SPADE residual block (ResBlk) uses the semantic label to modulate the layer activation.\n\nFig. 2 :\n2The architecture of SPADE ResBlk , \u03b3 and \u03b2 are learned modulation parameters.\n\nFig. 3 :\n3The perception and distortion performance of the decompressed image of BPG, WebP, JPEG, JPEG2000, DSSLIC, and our model with C = 64 and C = 128. The metrics (a) FID (b) KID (c) LPIPS are coincide with human perception, and (d) mIOU measures the semantic segmentation performance on the reconstructed image. The metrics (e) PSNR and (f) SSIM measure the distortion performance. Arrows in the y-axis indicate whether lower is better (\u2193), or higher is better (\u2191).\n\nFig. 4 :\n4Visual comparison of images produced by JPEG, JPEG2000, WebP, BPG, DSSLIC, and our model with C = 128.\n\nThe jpeg still picture compression standard. G K Wallace, IEEE transactions on consumer electronics. 381G. K. Wallace, \"The jpeg still picture compression standard,\" IEEE transactions on consumer electronics, vol. 38, no. 1, pp. xviii-xxxiv, 1992.\n\nJpeg2000: Image compression fundamentals, standards and practice. M Rabbani, Journal of Electronic Imaging. 112286M. Rabbani, \"Jpeg2000: Image compression fundamentals, standards and practice,\" Journal of Electronic Imaging, vol. 11, no. 2, p. 286, 2002.\n\nDeep learning based semantic communications: An initial investigation. H Xie, Z Qin, G Y Li, B.-H Juang, GLOBECOM 2020-2020 IEEE Global Communications Conference. IEEEH. Xie, Z. Qin, G. Y. Li, and B.-H. Juang, \"Deep learning based semantic communications: An initial investigation,\" in GLOBECOM 2020-2020 IEEE Global Communications Conference. IEEE, 2020, pp. 1-6.\n\nTowards a theory of semantic communication. J Bao, P Basu, M Dean, C Partridge, A Swami, W Leland, J A Hendler, IEEE Network Science Workshop. IEEE. J. Bao, P. Basu, M. Dean, C. Partridge, A. Swami, W. Leland, and J. A. Hendler, \"Towards a theory of semantic communication,\" in 2011 IEEE Network Science Workshop. IEEE, 2011, pp. 110-117.\n\nA lite distributed semantic communication system for internet of things. H Xie, Z Qin, IEEE Journal on Selected Areas in Communications. 391H. Xie and Z. Qin, \"A lite distributed semantic communication system for internet of things,\" IEEE Journal on Selected Areas in Communica- tions, vol. 39, no. 1, pp. 142-153, 2020.\n\nThe perception-distortion tradeoff. Y Blau, T Michaeli, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionY. Blau and T. Michaeli, \"The perception-distortion tradeoff,\" in Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 6228-6237.\n\nGans trained by a two time-scale update rule converge to a local nash equilibrium. M Heusel, H Ramsauer, T Unterthiner, B Nessler, S Hochreiter, arXiv:1706.08500arXiv preprintM. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter, \"Gans trained by a two time-scale update rule converge to a local nash equilibrium,\" arXiv preprint arXiv:1706.08500, 2017.\n\nDemystifying mmd gans. M Bi\u0144kowski, D J Sutherland, M Arbel, A Gretton, arXiv:1801.01401arXiv preprintM. Bi\u0144kowski, D. J. Sutherland, M. Arbel, and A. Gretton, \"Demysti- fying mmd gans,\" arXiv preprint arXiv:1801.01401, 2018.\n\nThe unreasonable effectiveness of data. A Halevy, P Norvig, F Pereira, IEEE Intelligent Systems. 242A. Halevy, P. Norvig, and F. Pereira, \"The unreasonable effectiveness of data,\" IEEE Intelligent Systems, vol. 24, no. 2, pp. 8-12, 2009.\n\nLossy image compression with compressive autoencoders. L Theis, W Shi, A Cunningham, F Husz\u00e1r, arXiv:1703.00395arXiv preprintL. Theis, W. Shi, A. Cunningham, and F. Husz\u00e1r, \"Lossy image compres- sion with compressive autoencoders,\" arXiv preprint arXiv:1703.00395, 2017.\n\nEnd-to-end optimized image compression. J Ball\u00e9, V Laparra, E P Simoncelli, arXiv:1611.01704arXiv preprintJ. Ball\u00e9, V. Laparra, and E. P. Simoncelli, \"End-to-end optimized image compression,\" arXiv preprint arXiv:1611.01704, 2016.\n\nSemantic image synthesis with spatially-adaptive normalization. T Park, M.-Y Liu, T.-C Wang, J.-Y Zhu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionT. Park, M.-Y. Liu, T.-C. Wang, and J.-Y. Zhu, \"Semantic image synthesis with spatially-adaptive normalization,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 2337-2346.\n\nHigh-resolution image synthesis and semantic manipulation with conditional gans. T.-C Wang, M.-Y Liu, J.-Y Zhu, A Tao, J Kautz, B Catanzaro, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionT.-C. Wang, M.-Y. Liu, J.-Y. Zhu, A. Tao, J. Kautz, and B. Catanzaro, \"High-resolution image synthesis and semantic manipulation with condi- tional gans,\" in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 8798-8807.\n\nGenerative adversarial networks for extreme learned image compression. E Agustsson, M Tschannen, F Mentzer, R Timofte, L V Gool, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionE. Agustsson, M. Tschannen, F. Mentzer, R. Timofte, and L. V. Gool, \"Generative adversarial networks for extreme learned image compres- sion,\" in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 221-231.\n\nDsslic: deep semantic segmentationbased layered image compression. M Akbari, J Liang, J Han, ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEEM. Akbari, J. Liang, and J. Han, \"Dsslic: deep semantic segmentation- based layered image compression,\" in ICASSP 2019-2019 IEEE In- ternational Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019, pp. 2042-2046.\n\nPreserving quality of information by using semantic relationships. P Basu, J Bao, M Dean, J Hendler, Pervasive and Mobile Computing. 11P. Basu, J. Bao, M. Dean, and J. Hendler, \"Preserving quality of information by using semantic relationships,\" Pervasive and Mobile Computing, vol. 11, pp. 188-202, 2014.\n\nPyramid scene parsing network. H Zhao, J Shi, X Qi, X Wang, J Jia, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionH. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, \"Pyramid scene parsing network,\" in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 2881-2890.\n\nPhoto-realistic single image super-resolution using a generative adversarial network. C Ledig, L Theis, F Husz\u00e1r, J Caballero, A Cunningham, A Acosta, A Aitken, A Tejani, J Totz, Z Wang, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionC. Ledig, L. Theis, F. Husz\u00e1r, J. Caballero, A. Cunningham, A. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang et al., \"Photo-realistic single image super-resolution using a generative adversarial network,\" in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 4681-4690.\n\nThe cityscapes dataset for semantic urban scene understanding. M Cordts, M Omran, S Ramos, T Rehfeld, M Enzweiler, R Benenson, U Franke, S Roth, B Schiele, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionM. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Be- nenson, U. Franke, S. Roth, and B. Schiele, \"The cityscapes dataset for semantic urban scene understanding,\" in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 3213- 3223.\n", "annotations": {"author": "[{\"end\":192,\"start\":73},{\"end\":312,\"start\":193},{\"end\":430,\"start\":313},{\"end\":548,\"start\":431}]", "publisher": null, "author_last_name": "[{\"end\":85,\"start\":80},{\"end\":205,\"start\":202},{\"end\":323,\"start\":320},{\"end\":441,\"start\":439}]", "author_first_name": "[{\"end\":79,\"start\":73},{\"end\":201,\"start\":193},{\"end\":319,\"start\":313},{\"end\":438,\"start\":431}]", "author_affiliation": "[{\"end\":191,\"start\":87},{\"end\":311,\"start\":207},{\"end\":429,\"start\":325},{\"end\":547,\"start\":443}]", "title": "[{\"end\":70,\"start\":1},{\"end\":618,\"start\":549}]", "venue": null, "abstract": "[{\"end\":2114,\"start\":773}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2396,\"start\":2393},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2410,\"start\":2407},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3389,\"start\":3386},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3394,\"start\":3391},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3566,\"start\":3563},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4529,\"start\":4526},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6030,\"start\":6027},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6040,\"start\":6036},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6055,\"start\":6051},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6601,\"start\":6597},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6607,\"start\":6603},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6862,\"start\":6858},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6868,\"start\":6864},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6964,\"start\":6960},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7287,\"start\":7283},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7872,\"start\":7869},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8075,\"start\":8072},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8080,\"start\":8077},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8234,\"start\":8230},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9640,\"start\":9636},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10622,\"start\":10618},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12304,\"start\":12300},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":13036,\"start\":13032},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":13051,\"start\":13047},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13751,\"start\":13747},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":16727,\"start\":16724},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":16768,\"start\":16764},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":16884,\"start\":16880},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":17542,\"start\":17538},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":17877,\"start\":17873},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":18740,\"start\":18737},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":18750,\"start\":18747},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":18797,\"start\":18793},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":18969,\"start\":18965},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":21129,\"start\":21125}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":24646,\"start\":24171},{\"attributes\":{\"id\":\"fig_1\"},\"end\":24795,\"start\":24647},{\"attributes\":{\"id\":\"fig_2\"},\"end\":25244,\"start\":24796},{\"attributes\":{\"id\":\"fig_3\"},\"end\":25333,\"start\":25245},{\"attributes\":{\"id\":\"fig_4\"},\"end\":25805,\"start\":25334},{\"attributes\":{\"id\":\"fig_5\"},\"end\":25919,\"start\":25806}]", "paragraph": "[{\"end\":2766,\"start\":2133},{\"end\":3231,\"start\":2768},{\"end\":4333,\"start\":3233},{\"end\":5115,\"start\":4335},{\"end\":5167,\"start\":5117},{\"end\":6232,\"start\":5169},{\"end\":7565,\"start\":6296},{\"end\":8465,\"start\":7596},{\"end\":9165,\"start\":8523},{\"end\":9839,\"start\":9167},{\"end\":10676,\"start\":9841},{\"end\":11193,\"start\":10678},{\"end\":11235,\"start\":11232},{\"end\":11389,\"start\":11237},{\"end\":11814,\"start\":11455},{\"end\":12563,\"start\":11816},{\"end\":13306,\"start\":12671},{\"end\":13900,\"start\":13397},{\"end\":14375,\"start\":13902},{\"end\":14772,\"start\":14377},{\"end\":15614,\"start\":14774},{\"end\":16925,\"start\":15640},{\"end\":17215,\"start\":16927},{\"end\":17759,\"start\":17217},{\"end\":18666,\"start\":17790},{\"end\":19673,\"start\":18683},{\"end\":21470,\"start\":19675},{\"end\":22131,\"start\":21472},{\"end\":22917,\"start\":22133},{\"end\":24170,\"start\":22935}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11231,\"start\":11194},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11454,\"start\":11390},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12670,\"start\":12564},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13396,\"start\":13307}]", "table_ref": null, "section_header": "[{\"end\":2131,\"start\":2116},{\"end\":6251,\"start\":6235},{\"end\":6294,\"start\":6254},{\"end\":7594,\"start\":7568},{\"end\":8521,\"start\":8468},{\"end\":15638,\"start\":15617},{\"end\":17788,\"start\":17762},{\"end\":18681,\"start\":18669},{\"end\":22933,\"start\":22920},{\"end\":24653,\"start\":24648},{\"end\":24805,\"start\":24797},{\"end\":25254,\"start\":25246},{\"end\":25343,\"start\":25335},{\"end\":25815,\"start\":25807}]", "table": null, "figure_caption": "[{\"end\":24646,\"start\":24173},{\"end\":24795,\"start\":24655},{\"end\":25244,\"start\":24807},{\"end\":25333,\"start\":25256},{\"end\":25805,\"start\":25345},{\"end\":25919,\"start\":25817}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":8759,\"start\":8753},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":9188,\"start\":9179},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":9301,\"start\":9292},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":9549,\"start\":9543},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":9913,\"start\":9907},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":19225,\"start\":19209},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":20177,\"start\":20171},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":20367,\"start\":20357},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":20542,\"start\":20532},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":20618,\"start\":20612},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":21510,\"start\":21504},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":22158,\"start\":22152}]", "bib_author_first_name": "[{\"end\":25967,\"start\":25966},{\"end\":25969,\"start\":25968},{\"end\":26237,\"start\":26236},{\"end\":26498,\"start\":26497},{\"end\":26505,\"start\":26504},{\"end\":26512,\"start\":26511},{\"end\":26514,\"start\":26513},{\"end\":26523,\"start\":26519},{\"end\":26837,\"start\":26836},{\"end\":26844,\"start\":26843},{\"end\":26852,\"start\":26851},{\"end\":26860,\"start\":26859},{\"end\":26873,\"start\":26872},{\"end\":26882,\"start\":26881},{\"end\":26892,\"start\":26891},{\"end\":26894,\"start\":26893},{\"end\":27206,\"start\":27205},{\"end\":27213,\"start\":27212},{\"end\":27491,\"start\":27490},{\"end\":27499,\"start\":27498},{\"end\":27904,\"start\":27903},{\"end\":27914,\"start\":27913},{\"end\":27926,\"start\":27925},{\"end\":27941,\"start\":27940},{\"end\":27952,\"start\":27951},{\"end\":28215,\"start\":28214},{\"end\":28228,\"start\":28227},{\"end\":28230,\"start\":28229},{\"end\":28244,\"start\":28243},{\"end\":28253,\"start\":28252},{\"end\":28459,\"start\":28458},{\"end\":28469,\"start\":28468},{\"end\":28479,\"start\":28478},{\"end\":28713,\"start\":28712},{\"end\":28722,\"start\":28721},{\"end\":28729,\"start\":28728},{\"end\":28743,\"start\":28742},{\"end\":28970,\"start\":28969},{\"end\":28979,\"start\":28978},{\"end\":28990,\"start\":28989},{\"end\":28992,\"start\":28991},{\"end\":29226,\"start\":29225},{\"end\":29237,\"start\":29233},{\"end\":29247,\"start\":29243},{\"end\":29258,\"start\":29254},{\"end\":29719,\"start\":29715},{\"end\":29730,\"start\":29726},{\"end\":29740,\"start\":29736},{\"end\":29747,\"start\":29746},{\"end\":29754,\"start\":29753},{\"end\":29763,\"start\":29762},{\"end\":30247,\"start\":30246},{\"end\":30260,\"start\":30259},{\"end\":30273,\"start\":30272},{\"end\":30284,\"start\":30283},{\"end\":30295,\"start\":30294},{\"end\":30297,\"start\":30296},{\"end\":30740,\"start\":30739},{\"end\":30750,\"start\":30749},{\"end\":30759,\"start\":30758},{\"end\":31174,\"start\":31173},{\"end\":31182,\"start\":31181},{\"end\":31189,\"start\":31188},{\"end\":31197,\"start\":31196},{\"end\":31445,\"start\":31444},{\"end\":31453,\"start\":31452},{\"end\":31460,\"start\":31459},{\"end\":31466,\"start\":31465},{\"end\":31474,\"start\":31473},{\"end\":31890,\"start\":31889},{\"end\":31899,\"start\":31898},{\"end\":31908,\"start\":31907},{\"end\":31918,\"start\":31917},{\"end\":31931,\"start\":31930},{\"end\":31945,\"start\":31944},{\"end\":31955,\"start\":31954},{\"end\":31965,\"start\":31964},{\"end\":31975,\"start\":31974},{\"end\":31983,\"start\":31982},{\"end\":32505,\"start\":32504},{\"end\":32515,\"start\":32514},{\"end\":32524,\"start\":32523},{\"end\":32533,\"start\":32532},{\"end\":32544,\"start\":32543},{\"end\":32557,\"start\":32556},{\"end\":32569,\"start\":32568},{\"end\":32579,\"start\":32578},{\"end\":32587,\"start\":32586}]", "bib_author_last_name": "[{\"end\":25977,\"start\":25970},{\"end\":26245,\"start\":26238},{\"end\":26502,\"start\":26499},{\"end\":26509,\"start\":26506},{\"end\":26517,\"start\":26515},{\"end\":26529,\"start\":26524},{\"end\":26841,\"start\":26838},{\"end\":26849,\"start\":26845},{\"end\":26857,\"start\":26853},{\"end\":26870,\"start\":26861},{\"end\":26879,\"start\":26874},{\"end\":26889,\"start\":26883},{\"end\":26902,\"start\":26895},{\"end\":27210,\"start\":27207},{\"end\":27217,\"start\":27214},{\"end\":27496,\"start\":27492},{\"end\":27508,\"start\":27500},{\"end\":27911,\"start\":27905},{\"end\":27923,\"start\":27915},{\"end\":27938,\"start\":27927},{\"end\":27949,\"start\":27942},{\"end\":27963,\"start\":27953},{\"end\":28225,\"start\":28216},{\"end\":28241,\"start\":28231},{\"end\":28250,\"start\":28245},{\"end\":28261,\"start\":28254},{\"end\":28466,\"start\":28460},{\"end\":28476,\"start\":28470},{\"end\":28487,\"start\":28480},{\"end\":28719,\"start\":28714},{\"end\":28726,\"start\":28723},{\"end\":28740,\"start\":28730},{\"end\":28750,\"start\":28744},{\"end\":28976,\"start\":28971},{\"end\":28987,\"start\":28980},{\"end\":29003,\"start\":28993},{\"end\":29231,\"start\":29227},{\"end\":29241,\"start\":29238},{\"end\":29252,\"start\":29248},{\"end\":29262,\"start\":29259},{\"end\":29724,\"start\":29720},{\"end\":29734,\"start\":29731},{\"end\":29744,\"start\":29741},{\"end\":29751,\"start\":29748},{\"end\":29760,\"start\":29755},{\"end\":29773,\"start\":29764},{\"end\":30257,\"start\":30248},{\"end\":30270,\"start\":30261},{\"end\":30281,\"start\":30274},{\"end\":30292,\"start\":30285},{\"end\":30302,\"start\":30298},{\"end\":30747,\"start\":30741},{\"end\":30756,\"start\":30751},{\"end\":30763,\"start\":30760},{\"end\":31179,\"start\":31175},{\"end\":31186,\"start\":31183},{\"end\":31194,\"start\":31190},{\"end\":31205,\"start\":31198},{\"end\":31450,\"start\":31446},{\"end\":31457,\"start\":31454},{\"end\":31463,\"start\":31461},{\"end\":31471,\"start\":31467},{\"end\":31478,\"start\":31475},{\"end\":31896,\"start\":31891},{\"end\":31905,\"start\":31900},{\"end\":31915,\"start\":31909},{\"end\":31928,\"start\":31919},{\"end\":31942,\"start\":31932},{\"end\":31952,\"start\":31946},{\"end\":31962,\"start\":31956},{\"end\":31972,\"start\":31966},{\"end\":31980,\"start\":31976},{\"end\":31988,\"start\":31984},{\"end\":32512,\"start\":32506},{\"end\":32521,\"start\":32516},{\"end\":32530,\"start\":32525},{\"end\":32541,\"start\":32534},{\"end\":32554,\"start\":32545},{\"end\":32566,\"start\":32558},{\"end\":32576,\"start\":32570},{\"end\":32584,\"start\":32580},{\"end\":32595,\"start\":32588}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":7051992},\"end\":26168,\"start\":25921},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":62197160},\"end\":26424,\"start\":26170},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":231725710},\"end\":26790,\"start\":26426},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":17422552},\"end\":27130,\"start\":26792},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":220686608},\"end\":27452,\"start\":27132},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":215763824},\"end\":27818,\"start\":27454},{\"attributes\":{\"doi\":\"arXiv:1706.08500\",\"id\":\"b6\"},\"end\":28189,\"start\":27820},{\"attributes\":{\"doi\":\"arXiv:1801.01401\",\"id\":\"b7\"},\"end\":28416,\"start\":28191},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":14300215},\"end\":28655,\"start\":28418},{\"attributes\":{\"doi\":\"arXiv:1703.00395\",\"id\":\"b9\"},\"end\":28927,\"start\":28657},{\"attributes\":{\"doi\":\"arXiv:1611.01704\",\"id\":\"b10\"},\"end\":29159,\"start\":28929},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":81981856},\"end\":29632,\"start\":29161},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":41805341},\"end\":30173,\"start\":29634},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":4718798},\"end\":30670,\"start\":30175},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":47020766},\"end\":31104,\"start\":30672},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":2831133},\"end\":31411,\"start\":31106},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":5299559},\"end\":31801,\"start\":31413},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":211227},\"end\":32439,\"start\":31803},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":502946},\"end\":33014,\"start\":32441}]", "bib_title": "[{\"end\":25964,\"start\":25921},{\"end\":26234,\"start\":26170},{\"end\":26495,\"start\":26426},{\"end\":26834,\"start\":26792},{\"end\":27203,\"start\":27132},{\"end\":27488,\"start\":27454},{\"end\":28456,\"start\":28418},{\"end\":29223,\"start\":29161},{\"end\":29713,\"start\":29634},{\"end\":30244,\"start\":30175},{\"end\":30737,\"start\":30672},{\"end\":31171,\"start\":31106},{\"end\":31442,\"start\":31413},{\"end\":31887,\"start\":31803},{\"end\":32502,\"start\":32441}]", "bib_author": "[{\"end\":25979,\"start\":25966},{\"end\":26247,\"start\":26236},{\"end\":26504,\"start\":26497},{\"end\":26511,\"start\":26504},{\"end\":26519,\"start\":26511},{\"end\":26531,\"start\":26519},{\"end\":26843,\"start\":26836},{\"end\":26851,\"start\":26843},{\"end\":26859,\"start\":26851},{\"end\":26872,\"start\":26859},{\"end\":26881,\"start\":26872},{\"end\":26891,\"start\":26881},{\"end\":26904,\"start\":26891},{\"end\":27212,\"start\":27205},{\"end\":27219,\"start\":27212},{\"end\":27498,\"start\":27490},{\"end\":27510,\"start\":27498},{\"end\":27913,\"start\":27903},{\"end\":27925,\"start\":27913},{\"end\":27940,\"start\":27925},{\"end\":27951,\"start\":27940},{\"end\":27965,\"start\":27951},{\"end\":28227,\"start\":28214},{\"end\":28243,\"start\":28227},{\"end\":28252,\"start\":28243},{\"end\":28263,\"start\":28252},{\"end\":28468,\"start\":28458},{\"end\":28478,\"start\":28468},{\"end\":28489,\"start\":28478},{\"end\":28721,\"start\":28712},{\"end\":28728,\"start\":28721},{\"end\":28742,\"start\":28728},{\"end\":28752,\"start\":28742},{\"end\":28978,\"start\":28969},{\"end\":28989,\"start\":28978},{\"end\":29005,\"start\":28989},{\"end\":29233,\"start\":29225},{\"end\":29243,\"start\":29233},{\"end\":29254,\"start\":29243},{\"end\":29264,\"start\":29254},{\"end\":29726,\"start\":29715},{\"end\":29736,\"start\":29726},{\"end\":29746,\"start\":29736},{\"end\":29753,\"start\":29746},{\"end\":29762,\"start\":29753},{\"end\":29775,\"start\":29762},{\"end\":30259,\"start\":30246},{\"end\":30272,\"start\":30259},{\"end\":30283,\"start\":30272},{\"end\":30294,\"start\":30283},{\"end\":30304,\"start\":30294},{\"end\":30749,\"start\":30739},{\"end\":30758,\"start\":30749},{\"end\":30765,\"start\":30758},{\"end\":31181,\"start\":31173},{\"end\":31188,\"start\":31181},{\"end\":31196,\"start\":31188},{\"end\":31207,\"start\":31196},{\"end\":31452,\"start\":31444},{\"end\":31459,\"start\":31452},{\"end\":31465,\"start\":31459},{\"end\":31473,\"start\":31465},{\"end\":31480,\"start\":31473},{\"end\":31898,\"start\":31889},{\"end\":31907,\"start\":31898},{\"end\":31917,\"start\":31907},{\"end\":31930,\"start\":31917},{\"end\":31944,\"start\":31930},{\"end\":31954,\"start\":31944},{\"end\":31964,\"start\":31954},{\"end\":31974,\"start\":31964},{\"end\":31982,\"start\":31974},{\"end\":31990,\"start\":31982},{\"end\":32514,\"start\":32504},{\"end\":32523,\"start\":32514},{\"end\":32532,\"start\":32523},{\"end\":32543,\"start\":32532},{\"end\":32556,\"start\":32543},{\"end\":32568,\"start\":32556},{\"end\":32578,\"start\":32568},{\"end\":32586,\"start\":32578},{\"end\":32597,\"start\":32586}]", "bib_venue": "[{\"end\":26020,\"start\":25979},{\"end\":26276,\"start\":26247},{\"end\":26587,\"start\":26531},{\"end\":26939,\"start\":26904},{\"end\":27267,\"start\":27219},{\"end\":27587,\"start\":27510},{\"end\":27901,\"start\":27820},{\"end\":28212,\"start\":28191},{\"end\":28513,\"start\":28489},{\"end\":28710,\"start\":28657},{\"end\":28967,\"start\":28929},{\"end\":29345,\"start\":29264},{\"end\":29852,\"start\":29775},{\"end\":30375,\"start\":30304},{\"end\":30863,\"start\":30765},{\"end\":31237,\"start\":31207},{\"end\":31557,\"start\":31480},{\"end\":32067,\"start\":31990},{\"end\":32674,\"start\":32597},{\"end\":27651,\"start\":27589},{\"end\":29413,\"start\":29347},{\"end\":29916,\"start\":29854},{\"end\":30433,\"start\":30377},{\"end\":31621,\"start\":31559},{\"end\":32131,\"start\":32069},{\"end\":32738,\"start\":32676}]"}}}, "year": 2023, "month": 12, "day": 17}