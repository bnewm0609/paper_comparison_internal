{"id": 260681484, "updated": "2023-12-01 19:31:21.599", "metadata": {"title": "Food-500 Cap: A Fine-Grained Food Caption Benchmark for Evaluating Vision-Language Models", "authors": "[{\"first\":\"Zheng\",\"last\":\"Ma\",\"middle\":[]},{\"first\":\"Mianzhi\",\"last\":\"Pan\",\"middle\":[]},{\"first\":\"Wenhan\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Kanzhi\",\"last\":\"Cheng\",\"middle\":[]},{\"first\":\"Jianbing\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Shujian\",\"last\":\"Huang\",\"middle\":[]},{\"first\":\"Jiajun\",\"last\":\"Chen\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Vision-language models (VLMs) have shown impressive performance in substantial downstream multi-modal tasks. However, only comparing the fine-tuned performance on downstream tasks leads to the poor interpretability of VLMs, which is adverse to their future improvement. Several prior works have identified this issue and used various probing methods under a zero-shot setting to detect VLMs' limitations, but they all examine VLMs using general datasets instead of specialized ones. In practical applications, VLMs are usually applied to specific scenarios, such as e-commerce and news fields, so the generalization of VLMs in specific domains should be given more attention. In this paper, we comprehensively investigate the capabilities of popular VLMs in a specific field, the food domain. To this end, we build a food caption dataset, Food-500 Cap, which contains 24,700 food images with 494 categories. Each image is accompanied by a detailed caption, including fine-grained attributes of food, such as the ingredient, shape, and color. We also provide a culinary culture taxonomy that classifies each food category based on its geographic origin in order to better analyze the performance differences of VLM in different regions. Experiments on our proposed datasets demonstrate that popular VLMs underperform in the food domain compared with their performance in the general domain. Furthermore, our research reveals severe bias in VLMs' ability to handle food items from different geographic regions. We adopt diverse probing methods and evaluate nine VLMs belonging to different architectures to verify the aforementioned observations. We hope that our study will bring researchers' attention to VLM's limitations when applying them to the domain of food or culinary cultures, and spur further investigations to address this issue.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/mm/MaPWCZHC23", "doi": "10.1145/3581783.3611994"}}, "content": {"source": {"pdf_hash": "36d23a17309e5e31a5b966c0158386ebe6ce719c", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2308.03151v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "88fbd74c788c44318f6a0292ce4ee093f8de61f6", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/36d23a17309e5e31a5b966c0158386ebe6ce719c.txt", "contents": "\nFood-500 Cap: A Fine-Grained Food Caption Benchmark for Evaluating Vision-Language Models\n\n\nZheng Ma \nMianzhi Pan \nWenhan Wu \nKanzhi Cheng chengkz@smail.nju.edu.cn \nJianbing Zhang \nShujian Huang huangsj@nju.edu.cn \nJiajun Chen chenjj@nju.edu.cn \n\nNational Key Laboratory for Novel Software Technology\nNanjing University\nNanjingChina\n\n\nNational Key Laboratory for Novel Software Technology\nNanjing University\nNanjingChina\n\n\nNational Key Laboratory for Novel Software Technology\nNanjing University\nNanjingChina\n\n\nNational Key Laboratory for Novel Software Technology\nNanjing University\nNanjingChina\n\n\nNational Key Laboratory for Novel Software Technology\nNanjing University\nNanjingChina\n\n\nNational Key Laboratory for Novel Software Technology\nNanjing University\nNanjingChina\n\n\nNational Key Laboratory for Novel Software Technology\nNanjing University\nNanjingChina\n\nFood-500 Cap: A Fine-Grained Food Caption Benchmark for Evaluating Vision-Language Models\nVision-language ModelsFood BenchmarkEvaluation\nVision-language models (VLMs) have shown impressive performance in substantial downstream multi-modal tasks. However, only comparing the fine-tuned performance on downstream tasks leads to the poor interpretability of VLMs, which is adverse to their future improvement. Several prior works have identified this issue and used various probing methods under a zero-shot setting to detect VLMs' limitations, but they all examine VLMs using general datasets instead of specialized ones. In practical applications, VLMs are usually applied to specific scenarios, such as e-commerce and news fields, so the generalization of VLMs in specific domains should be given more attention. In this paper, we comprehensively investigate the capabilities of popular VLMs in a specific field, the food domain. To this end, we build a food caption dataset, Food-500 Cap, which contains 24,700 food images with 494 categories. Each image is accompanied by a detailed caption, including fine-grained attributes of food, such as the ingredient, shape, and color. We also provide a culinary culture taxonomy that classifies each food category based on its geographic origin in order to better analyze the performance differences of VLM in different regions. Experiments on our proposed datasets demonstrate that popular VLMs underperform in the food domain compared with their performance in the general domain. Furthermore, our research reveals severe bias in VLMs' ability to handle food items from different geographic regions. We adopt diverse probing methods and evaluate nine VLMs belonging to different architectures to verify the aforementioned * Equal contribution. \u2020 Corresponding author.observations. We hope that our study will bring researchers' attention to VLM's limitations when applying them to the domain of food or culinary cultures, and spur further investigations to address this issue.CCS CONCEPTS\u2022 Information systems \u2192 Multimedia databases.\n\nINTRODUCTION\n\nDespite the remarkable success of vision-language models (VLMs) [18,33,34,38,45,47,52] in substantial uni-modal and multi-modal downstream tasks, they are still poorly understood as yet. The prevalent approach for evaluating VLMs is comparing their performance on downstream tasks after fine-tuning. However, evaluation solely based on the fine-tuning results renders poor interpretability [59], which hinders the further development of VLMs. Consequently, researchers have proposed a range of probing methods and benchmarks [9,13,29,30,39,48] in recent years to assess the capabilities of VLMs from various perspectives, providing a more comprehensive understanding of these models. However, these methodologies are still limited in the general domain. They typically construct evaluation benchmarks by employing images from widely used general-domain datasets and subsequently assigning hand-crafted textual annotations to these images. If VLMs perform well in a specific domain, we can directly employ the models in that domain without any modifications. However, the above situation is Figure 1: An example from our Food-500 Cap. The image is equipped with the label, geographic origin, and a detailed description. This description is annotated with a class label (red) and hand-curated various fine-grained visible content of the image such as ingredients (blue), food colors (green), and the food container (orange).\n\nunclear due to only few works studying the generalization of using VLMs directly in specific domains without fine-tuning.\n\nMotivated by this, we focus on evaluating the generalization capacity of VLMs in a specific domain, namely, the food domain. Since food computing [25] has been gaining widespread attention as it has the potential to support numerous food-related applications, such as healthy diets and food choices. To comprehensively evaluate the VLMs' performance on food-related tasks, we introduce a new benchmark named Food-500 Cap, which comprises 24,700 food images with 494 categories, each accompanied by a detailed caption. The Food-500 Cap dataset is created by selecting images from ISIA Food-500 [26] that covers a wide range of food categories. We select 50 images from each category and engage an annotation company to annotate fine-grained descriptions for all 24,700 images. Each description includes the original food category label and fine-grained attributes of the food, such as the color, shape, and ingredients. Such an in-house labeling process guarantees the high quality of our dataset. Besides, as food is always associated with a specific geographic region, we also provide a taxonomy that classifies food categories based on their original place, enabling a more comprehensive investigation of VLMs' performance across culinary cultures. We provide a sample of Food-500 Cap in Figure 1, which contains a Japanese food image labeled agedashi tofu from and a description with some related attributes. In contrast to the prevalent food datasets [3,23,54], Food-500 Cap are equipped with high-quality image captions containing richer visual information and geographic origin tags, which is more suitable for exploring the performance of VLM in the food domain.\n\nTo comprehensively evaluate VLMs' capacity in the food domain, we seriously pick up nine representative models from three popular architectures, including vision-language representation models (e.g. CLIP [33]), image-to-text generative models (e.g. OFA [52]), and text-to-image generative models (e.g. Stable Diffusion [38]). We probe these VLMs with various food-related tasks in a zero-shot setting. For vision-language representation models, we employ food classification and image-text retrieval to assess VLMs' multi-modal alignment capabilities. As for image-to-text generative models and text-to-image generative models, we utilize image captioning and image synthesis respectively to test their multi-modal generation capabilities. Both qualitative and quantitative analyses are performed on the experimental results, revealing that these models exhibit poor performance in the food domain, in contrast to their performance in the general domain. Moreover, we find that all the models display a significant bias in culinary culture, with their performance in Asian cuisine falling markedly behind that in European, North American, and Latin American cuisine. In summary, this paper makes the following contributions:\n\n\u2022 We equip a subset of the ISIA Food-500 dataset with (1) \n\n\nRELATED WORKS 2.1 Probing VLMs\n\nVLMs have achieved state-of-the-art performance in a large number of downstream multi-modal tasks, but they are still poorly understood. Therefore, evaluating VLMs has attracted much attention.\n\nCommonly, VLMs are evaluated by comparing their fine-tuned performance in downstream vision-language tasks. However, finetuning VLMs in downstream tasks only provides a black-box score, which renders poor interpretability of VLMs.\n\nTo acquire a deeper understanding of VLMs, a number of existing works have probed their capability from various perspectives, including verb understanding [9], spatial relation understanding [8,39], visual abstract reasoning with tangram shapes [13], generalization ability in out-of-domain datasets [61], compositional reasoning ability [30,48,57], visual-linguistic grounding capabilities on specific linguistic phenomena [29], robustness to image and text perturbations [32], attribute recognition capability [50,59], object hallucination problem [4]. These works have revealed that prevalent VLMs have severe shortcomings in certain aspects.\n\nNevertheless, current probing works are still limited in the general domain. Specifically, they utilize images from the general domain to construct datasets or benchmarks, such as MSCOCO [20], Visual Genome [15], LAION-400M [42], or social media data without domain specification. Instead of investigating VLMs in the general domain, we focus on VLMs' vision-language capability in the food domain, which is closely linked with people's health and daily life. To this end, we introduce a food image-caption dataset and comprehensively evaluate a range of representative VLMs on it\n\n\nFood Dataset\n\nIn recent years, there have been substantial food datasets available. Most of them are proposed for food image classification, such as ETH Food-101 [1], UPMC Food-101 [53] with western food, UEC Food256 [14], Sushi-50 [31] with Japanese food, VIREO Food-172 [2], ChineseFoodNet [3] with Chinese food, ISIA Food-500 [26] comprising miscellaneous food categories worldwide. Besides category labels, UPMC Food-101 [53] and VIREO Food-172 [2] contain additional metadata such as related web text, ingredients, and cooking instructions.\n\nIn addition, Yummly-66k [24] annotates images with ingredients, courses and regions for food topic models. FoodSeg103 [54] implements a food image segmentation dataset that tags each image with multiple ingredients and draws the corresponding pixel-wise masks. Recipe1M [22] and Recipe1M+ [23] construct datasets with numerous images and recipes, which is suitable for image-recipe retrieval task. These datasets mainly facilitate specific tasks. However, the visual correlation between their textual annotations and images is relatively weak. Their texts not only neglect plenty of visual attributes of the image but also contain invisible contents, e.g. cooking instructions [22,23]. VLMs are hard to align these images and texts, which hinders these datasets from serving as probing datasets. To this end, we introduce Food-500 Cap, the first food image-caption dataset. Food-500 Cap has captions describing fine-grained visual content of the image. It also includes food category labels and their geographic origin tags. Hence, Food-500 Cap can serve as a comprehensive benchmark for probing VLMs' generalization ability in the food domain.\n\n\nFOOD-500 CAP\n\nThis section outlines the construction process of the Food-500 Cap dataset and provides a detailed description of its statistics.\n\n\nDataset Construction\n\nCollecting food images. To ensure the diversity of food categories, we utilize images from the ISIA Food-500 [26], a comprehensive dataset for food recognition containing 399,726 samples covering 500 food categories from various countries and regions. For each category, we randomly select 50 images from ISIA Food-500. Note that actually we only use 494 out of the 500 categories currently and six categories are manually removed.\n\nAnnotating food images. To obtain high-quality captions depicting fine-grained visual features, we employ a data annotation company and urge the annotators to follow the next three rules. First, annotators must include category labels in each caption, which contain the food's principal information. Second, we encourage annotators to be as careful as possible, marking all visible content of images including not only the food's color, shape, ingredients, seasonings, accessories, etc. but also the container's color, shape, pattern, etc. To ensure the distinctiveness of the captions, some general words should be avoided to the largest degree, such as fruit and vegetables. At last, every annotator is instructed to integrate the aforementioned information into fluent sentences using diverse syntactic constructions. Eventually, we obtain food image captions with fine-grained visual content and one example is shown in Figure 1.\n\nMarking regions. Although covering diverse food categories, one insufficiency of ISIA Food-500 is that it mixes food categories from different regions without marking their original regions, hindering further study of culinary cultures. Therefore, we resort to Wikipedia to mark the original region of each food category by ourselves, and we show the detailed process in Appendix A. Consequently, all food categories are divided into seven regions: Worldwide, Western, Latin-American, Chinese, Japanese, Indian, and Asian. Table 6 displays the food category distribution over these regions. Note that 90 food categories have ambiguous original places, so we merge them into Worldwide.\n\n\nDataset Statistics and Characteristics\n\nFood-500 Cap contains 24,700 images that are uniformly divided into 494 categories. Captions are of average length 18.57, and there are 7.26 nouns, 1.96 verbs, and 2.53 adjectives in each caption on average 1 . As shown in Table 1, Food-500 Cap surpasses current food datasets in the following two aspects: (1) Food-500 Cap annotates each image with a human-crafted, fine-grained, fluent visual description, hence containing richer visual information. (2) All food categories are tagged with their geographic origins, enabling culinary culture studies across regions. Whereas almost all current datasets neglect region annotation except for VIREO Food-172 [2]. Therefore, Food-500 Cap can better serve as a comprehensive visionlanguage benchmark.\n\n\nPROBING VLMS IN FOOD DOMAIN\n\nTo comprehensively evaluate prevalent VLMs, we pick up three different types of VLMs including vision-language representation models [18,33,44,55,58], image-to-text generative models [18,51,52], and text-to-image generative models [34,38]. Then we apply four probing methods to them. For vision-language representation VLMs, we utilize classification and retrieval tasks to probe their crossmodal alignment ability. For generative VLMs, we utilize image captioning and image generation tasks to test whether they can generate satisfactory images or descriptions. All tasks are performed in a zero-shot setting to directly assess the generalization of VLMs.\n\n\nVision-language Representation Models\n4.1.1 Evaluated Models.\nCLIP [33]. It employs two independent encoders to encode image and text respectively. It is trained with an image-text contrastive (ITC) objective, which encourages the embeddings of paired images and texts to be closer while pushing away those of mismatched pairs. Benefiting from 400 million image-text pairs during training, CLIP exhibits powerful zero-short transfer ability across a wide range of downstream tasks, e.g. cross-modal retrieval.\n\nTCL [55]. It consists of an image encoder, a text encoder, and a multi-modal encoder to fuse image and text features from unimodal encoders. Apart from the original cross-modal contrastive objective like CLIP, TCL proposes intra-modal contrastive target  and local mutual information maximization to robust the uni-modal representations.\n\nX_VLM [58]. It shares the same model framework as TCL while better utilizing the region annotations in some datasets. It optimizes the model by predicting the location of bounding boxes in the image given the corresponding caption and meanwhile conducts visionlanguage alignment in multi-granularity.\n\nFLAVA [44]. It inherits the architecture of TCL and X_VLM. Different from VLMs only focus on cross-modal tasks, FLAVA is trained with regard to both cross-modal and uni-modal objectives, including global image-text contrastive learning, masked image modeling, masked language modeling, etc. And FLAVA achieves comparable results on vision-only, language-only, and cross-modal tasks.\n\nBLIP [18]. It introduces a novel multi-modal mixture of Encoder-Decoder framework. It can operate as a uni-modal encoder, an image-grounded text encoder, and an image-grounded text decoder, sharing parameters with each other. They are optimized with contrastive loss, image-text matching (ITM) loss, and language modeling loss respectively. To leverage noisy web data effectively, BLIP augments the datasets utilizing captions synthesized by itself.\n\n\nEvaluation Task.\n\nFood Classification. Previous works [33,44] have revealed that VLMs have competitive zero-shot power in general-domain classification benchmarks, such as ImageNet [5], PASCAL VOC [7], CIFAR [16]. Furthermore, within the domain of food, CLIP [33] and FLAVA [44] report their overall accuracy on Food-101 [1], but it has a relatively limited number of 101 food classes. To this end, we employ representation VLMs to undertake zero-shot food classification using our benchmark and elaborate the results.\n\nFollowing [33,43,60], we undertake zero-shot food classification using prompt engineering. Specifically, we utilize a prompt template \"a food photo of a {label}\" and populate it with related category In the evaluation phase, we task the VLMs with identifying the correct prompt for each image among all constructed prompts.\n\nTo be specific, we evaluate two configurations of vision-language representation models. The first is the ITC configuration, where only the image and text uni-modal encoders are employed. Images and prompts are individually embedded by VLMs' uni-modal encoders, and models select the prompt with the maximum cosine similarity for each image. The second is the ITM configuration, where BLIP, TCL, and X_VLM further use the ITM score from their multi-modal fusion modules to re-rank the top-nearest prompts in the ITC configuration. Note that we fix to 128 and re-rank by adding the ITM score to cosine scores. (b) image-to-text retrieval R@1 score   Table 2: Zero-shot cross-modal retrieval results on Food-500 Cap. The overall best result is bold-face.\n\nImage-text Bidirectional Retrieval. Image-text bidirectional retrieval aims to retrieve images using textual queries (text-toimage retrieval) and converse (image-to-text retrieval). This task reveals models' ability to align the semantic space of vision and language. Though current VLMs [18,33,44,55,58] have achieved superior zero-shot performance on general domain datasets such as Flickr [56], MSCOCO [20], it is unknown whether they also perform well in a specific domain. Consequently, we conduct zero-shot image-text bidirectional retrieval using the food image-caption pair in Food-500 Cap. Like food classification. Similar to food classification, this task is also performed using ITC and ITM configurations. Finally, we report the top-1, 5, and 10 retrieval scores on our benchmark.\n\n\nResults.\n\nClassification. Figure 2 displays the overall food classification results. None of the models achieve an accuracy above 50%. CLIP achieves over 40% accuracy on the zero-shot food classification task, which has the highest accuracy. BLIP and FLAVA also show competitive performance, while TCL and X_VLM exhibit a sizable gap. This phenomenon should be attributed to their relatively small : Coefficient of variation (CV) of classification accuracies (blue) and image-to-text retrieval R@1 scores (orange) across different regions of several VLMs. CV is the ratio of the standard deviation to the mean, which measures the dispersion of a probability distribution. In this figure, the higher the CV value, the more unbalanced performance across regions.\n\npre-training datasets, which only contain 4M 3 and 16M image-text pairs, while that for FLAVA, BLIP, and CLIP are 70M, 129M, and 400M, respectively. We also obtain the following findings:\n\nVLMs fail to recognize certain food categories. The accuracy varies greatly in different food categories. On one hand, VLMs perform nearly perfectly in some categories. For example, CLIP correctly classifies all images in bandeja paisa from Latin-American. However, On the other hand, VLMs recognize no images from some categories, such as aburaage and doufunao. The percentage of such categories for all VLMs is displayed in Figure 2 using the failure rate metric. We notice that even the best-performing CLIP fails in   VLMs exhibit culinary culture bias in zero-shot food classification. As illustrated in Figure 3 (a), all models exhibit consistency in their performance across different regions. These models can better identify food images from Western and Latin-American than others except for Worldwide. Besides the qualitative results, we furthermore report the coefficient of variation (CV) of scores in different regions in Figure 4, which reveals strong culinary culture bias in TCL and X_VLM. Such bias is probably inherited from the pre-training dataset, where food from some countries or regions, e.g. Japan, appears much less frequently than European and American food.\n\nImage-text Bidirectional Retrieval. Table 2 shows the overall performance of the compared models on image-text bidirectional retrieval. Similar to our findings in food classification, the results of retrieval also demonstrate VLMs underperform in the food domain compared to the general domain and suffer from the region bias. To be specific:\n\nThe overall performance is not satisfactory. For the ITC configuration, BLIP gets the highest score on R@1, but it only reaches 15%. Other models get scores below 10%. In contrast to the classification task, where CLIP achieved the highest accuracy, it does not perform well in retrieval tasks. This implies that while CLIP is better at recognizing the general type of food, it has a weaker ability to distinguish food at a fine-grained level. Using the ITM configuration, this problem can be alleviated a bit. Under this setting, BLIP, TCL, and X_VLM obtain much higher scores on R@1, R@5, and R@10.\n\nAll VLMs also suffer in certain categories in image-text retrieval. For example, BLIP itm 's image-to-text recall@1 score reaches 66.0 for christmas cake, a food from Western, but it hardly retrieves correct descriptions for bon bon chicken which is from Chinese. We further investigate the retrieval results in different regions. As shown in Figure 3 (b), we find that all VLMs perform relatively poorly in Asian, Chinese, Indian and Japanese. According to the quantitative results in Figure 4, CLIP also maintains the lowest CV, which suggests a relatively weak culinary culture bias. We speculate the reason to be its tremendous amount of pre-training data.\n\n\nImage-to-text Generative Models\n\n\nEvaluated Models.\n\nGIT [51]. It is composed of one swin-like [21] vision transformer and one text decoder. During training, it uses the language modeling task to predict the associated caption given an image. When applied to downstream tasks, GIT first transforms them into text generation and then produces the answer word by word.\n\nOFA [52]. It proposes a more generic encoder-decoder framework compared with GIT. It develops a unified multi-modal vocabulary, and both its encoder and decoder can process inputs from different modalities. Therefore, OFA can serve as a task-agnostic and modality-agnostic model. Both multi-modal and uni-modal tasks are combined during pre-training, which renders OFA superior performance on a wide range of tasks, such as image captioning, image generation, image classification, language understanding, etc.\n\nBLIP [18]. Owing to its special architecture, BLIP can be regarded as an image-grounded text decoder as well. Hence we use BLIP in this part and denote it as BLIP Dec . Detailed introduction can be referred to Section 4.1.1.\n\n\nEvaluation Task.\n\nImage Captioning. Image-to-text generative VLMs aggregate multiple tasks into a unified text generation task [18,51,52]. Thus we opt to leverage zero-shot image captioning as a means of probing these models. The objective of image captioning is to generate descriptive sentences for given images. We utilize multi-view metrics to better exhibit the generative ability of VLMs. First, we calculate common-used image captioning metrics, including n-grambased metrics, such as BLEU [19], METEOR [28], ROUGE [6] and CIDEr [49], and a semantic-based metric, CLIPScore [10].\n\nThen, we assess the recognition ability of VLMs. Inspired by Semantic Object Accuracy (SOA) [12] in text-to-image generation evaluation, which evaluates whether the generated image contains the objects mentioned in the text, we check whether the generated captions contain the food labels of the images, and similarly define Semantic Label Accuracy (SLA):\nSLA = 1 \u2211\ufe01 =1 1( ( ) \u2208 ( ) )\nwhere is the number of images, ( ) and ( ) are the category label and generated caption of the th image respectively.\n\n\nResults.\n\nWe provide the captioning results in Table 4 including the ngram-based metrics score and CLIPScore for all three models. All models have low scores on n-gram-based metrics.\n\nAll image-to-text generative models hardly generate the fine-grained attributes. From a perspective of metrics, the low scores on n-gram-based metrics indicate a clear literal mismatch between the generated and reference captions that contain many fine-grained attributes, including the shape, color, and ingredient. We provide some generated descriptions in Table 7. From the Table, we can observe that all VLMs neglect or misidentify a lot of food attributes. In the top example, models do not generate eggplants, potatoes, green peppers compared to the reference in our proposed dataset. Moreover, we find that OFA tends to generate hallucinations [4,36], and GIT prefers to append meaningless words to the end of the sentence. As shown in the bottom example of Table 7, OFA generates \"ready to be served to the guests at the wedding reception\", and GIT adds \"yum\" and invalid punctuation at the end of the caption. These phenomena lead to the longer captions of GIT and OFA compared with BLIP Dec (Table 4), but there is no significant advantage in caption metrics. They also have lower CLIP-S scores than the ground truth, suggesting the weaker alignment of the generated captions with the images.  Table 5: FID, FID CLIP and CAS for different text-to-image generation models. For CAS, we randomly split the test data 10 times and report the mean and standard deviation.\n\nThese models hardly generate correct labels in descriptions The low scores on metrics may reflect the generated sentence including incorrect labels. To verify this, we display the overall SLA and that in different regions in ??, revealing that less than 10% captions exactly include whole category labels for all three models. If we relax the requirement and regard it as true if the generated caption contains some word in the label (Relaxed setting in ??) rather than the whole label, SLA obtains significant improvement for all models, especially in Chinese, Worldwide and Western. This is because many category labels (e.g. lentil soup) from these regions contain common words like soup, which is easier for VLMs to generate.\n\nFood labels from different regions pose different levels of difficulty for VLMs to generate. As shown in ??, VLMs can hardly generate food category labels from specific regions, indicating a possible bias in culinary culture. In particular, for Japanese and Indian, BLIP Dec and GIT fail to generate food labels from these countries, and even the highest performing OFA only achieves 0.82 and 0.24 SLA, respectively.\n\n\nText-to-Image Generative Models\n\n\nEvaluated Models.\n\nDALL-E [34]. It employs a decoder-only transformer that receives texts and images as a single stream. Given a text prompt, it first predicts the image tokens autoregressively, which have been pre-defined in the codebook of a pre-trained discrete variational autoencoder (dVAE) [37]. Then the generated image tokens are fed to the decoder of the dVAE to synthesize images. Training on 250M image-text pairs from the internet, DALL-E can create plausible images for various sentences even in the zero-shot setting. Note that the checkpoint is unavailable. We choose two publicly released implements: minDALL-E [40], DALLE small4 .\n\nStable Diffusion [38]. It leverages the prevalent diffusion model, which learns to reverse the process of adding noise to images. Unlike diffusion-based AI painters such as GLIDE [27], Imagen [41], Stable Diffusion uses latent diffusion model. The latent diffusion model operates in a compressed image space rather than the highdimensional pixel space. Consequently, Stable Diffusion can generate high-resolution images from text descriptions with less computation consumption.\n\n\nEvaluation Task.\n\nImage Synthesis. Following the default implementation, we adopt Stable Diffusion, minDALL-E, and DALLE small to synthesize images given food descriptions. To evaluate the overall image quality, we use Fr\u00e9chet Inception Distance (FID) [11] and FID CLIP [17] scores, which measures the distance between the distributions of the real and synthetic images in the feature space of an ImageNet pre-trained Inception-v3 [46] and CLIP [33], respectively. Then we employ Classification Accuracy Score (CAS) [35] to assess to what extent the generated images manifest the categorical condition, where generated images are used to train a classifier which is then used to predict the label of real images. To compute CAS, a classifier is first trained on the generated images, then used to predict labels of real images.\n\n\nResults.\n\nWe report the FID and FID CLIP score and the CAS score in Table 5, which shows that all three models exhibit significant differences in their performance on image synthesis tasks. Through quantitative and qualitative analysis, we find the following issues:\n\nThere is a significant gap between synthesis and real images. From Table 5, all models have a significant gap compared to real images on FID, FID CLIP , which measure the similarity between the synthesis and real images. Especially, the performance of DALLE small is worse than the other two models. Figure 8 shows some generated synthetic images. Through Figure 8, we find that images generated by DALLE small are unrealistic. In contrast, those generated by Stable Diffusion appear relatively more realistic and contain more caption content. To investigate whether the models can capture the main features of the category mentioned in the text, we further provide a quantitative evaluation of the synthetic images. Unlike metrics such as FID and FID CLIP , CAS ignores some fringe features and is concerned with whether the generated images contain the necessary features to represent the class. We find that all models suffer a performance drop compared to real images, which indicates that text-to-image generative models might have difficulty capturing representative category features.\n\n\nCONCLUSION\n\nIn this work, we introduce Food-500 Cap, a new vision-language benchmark in the food domain. By in-house labeling, Food-500 Cap not only provides each image with a fine-grained visual content description but also labels a novel taxonomy that divides food categories into their geographic origins, which aids in studying different culinary cultures. We adopt four vision-language tasks in the zero-shot setting, including food classification, image-text bidirectional retrieval, image captioning, and image synthesis, and evaluate nine VLMs of three different architectures on our proposed benchmark. Experiments reveal VLMs' limitations in the food domain and their bias against culinary culture. We hope that our proposed benchmark will promote the study of multi-modal food computing and our findings will provide insights into the deployment and application of VLMs in the food domain.\n\n\nA PROCESS OF ANNOTATING GEOGRAPHIC ORIGINS\n\nFor each food category, we resort to its Wikipedia entry. We can find their places of origin of most food categories. We display two examples in Figure 6, according to which we assign Wonton noodles to region Chinese and Tsukemono to region Japanese. However, some food categories have unknown origins, such as the Carrot salad shown in Figure 7. We assign these food categories to Worldwide in our taxonomy.      GT a grilled piece of bone-in pork knuckle served with yellow sauerkraut, and decorated with rosemary.\n\nBLIP Dec a plate of carrots on a table.\n\nOFA a plate of sweet potato fries with a drizzle of olive oil on top, ready to be served to the guests at the wedding reception.\n\nGIT cooked carrots in a white plate with a brown sauce... yum!!! : -) ( : ---) Table 7: Examples of generated captions from three image-to-text generative models compared to ground truth (GT).\n\nA piece of blueberry pie with a super golden crisp crust and thick juicy blueberry filling inside.\n\nBeef Wellington with thick beef steak, minced mushrooms, and flaky pastry outside, sliced open on a white dish.\n\n\nReal Stable Diffusion DALLE small minDALL-E\n\nFive pieces of golden agedashi tofu with some scallions, carrot strips and wooden fish flowers in a long white plate. \n\nFigure 2 :\n2Results of zero-shot classification, including (1) accuracy and (2) failure rate, which represents the percentage of categories where none of the images is correctly classified. Model names with subscript \"itm\" are of ITM configuration and others are of ITC configuration. labels 2 .\n\nFigure 3 :\n3Radar chart of (a) Zero-shot image classification accuracy and (b) image-to-text retrieval R@1 score across regions.\n\nFigure 4\n4Figure 4: Coefficient of variation (CV) of classification accuracies (blue) and image-to-text retrieval R@1 scores (orange) across different regions of several VLMs. CV is the ratio of the standard deviation to the mean, which measures the dispersion of a probability distribution. In this figure, the higher the CV value, the more unbalanced performance across regions.\n\nFigure 5 :\n5Per-region accuracy of the classifier trained on real vs. synthetic images. We randomly split the dataset 10 times and reported the test accuracy on real images.\n\nFigure 6 :\n6Annotating geographic origins in the case that the clear origin of the food are given. (a) Wonton noodles, where the place of origin is directly provided. (b) Tsukemono, where the entry lacks additional citations for verification and the place of origin is not provided, but we can still find the origin in the article.\n\nFigure 7 :\n7Annotating geographic origins when some food categories have unknown places of origin.\n\nBLIP\nDec a bowl of food with chops and chops on a blue and white floral tablecloth. OFA a bowl of food with chopsticks on a blue and white tablecloth with white daisies in the background. GIT chicken in a bowl with chopsticks on a red and blue placemat. yum!!! yum yum...\n\nFigure 8 :\n8Some examples of real images and synthesis images from text-to-image generative models\n\n\nWe evaluate nine representative VLMs from diverse architectures on our benchmark and use four probing tasks to analyze the performance of VLMs in the food domain comprehensively.\u2022 The results of our experiments on Food-500 Cap unveil the limitations of VLMs in the food domain, as well as their bias towards specific culinary cultures.find-\ngrained image descriptions (2) the geographic origin of each \nfood category. Based on this enhanced dataset, we propose \nFood-500 Cap, which serves as a benchmark to evaluate \nthe vision-language ability of VLMs in the food domain. To \nthe best of our knowledge, Food-500 Cap is the first image-\ncaption dataset that specifically targets the food domain. \n\u2022 \n\nTable 1 :\n1Summary of popular food-domain datasets and Food-500 Cap. Our proposed Food-500 Cap has 24,700 images, covering 494 food categories around the world. Compared to existing food datasets, each image from Food-500 Cap has a hand-curated fine-grained image caption and the geographic origin of the food. Captions are annotated by a data annotation company and food origins are tagged by ourselves resorting to Wikipedia.\n\nTable 3 :\n3Semantic Label Accuracy (%) in the entire dataset (Overall) and different regions according to our taxonomy of food categories. Accurate: the generated caption exactly contains the whole food category label. Relaxed: the generated caption contains some word from the food label.Model \nAvg. Len B@4 \nM \nR \nC \nCLIP-S \n\nBLIP Dec 13.80 \n2.61 \n8.71 20.33 13.62 \n0.70 \nGIT \n20.89 \n2.00 \n8.81 16.78 \n9.92 \n0.70 \nOFA \n20.45 \n2.64 9.14 17.89 14.01 \n0.71 \n\nGT \n18.57 \n-\n-\n-\n-\n0.78 \n\n\n\nTable 4 :\n4Results of image captioning on various metric. B@4, M, R C and CLIP-S represent BLEU@4, METEOR, ROUGE, CIDEr and CLIPScore respectively.nearly 10% of categories. And both X_VLM and TCL fail to identify over 40% categories.\n\nTable 6 :\n6Region distribution of food categories in Food-500 Cap.GT di san xian sauteed with soft eggplants, potatoes and slices of green peppers in a bowl which placed on a red napkin.Image \nCaptions \n\n\nhttps://spacy.io Conference'17, July 2017, Washington, DC, USA Zheng Ma et al.\nWe try several prompts and this one is of the overall highest accuracy.\nTCL does not release its checkpoint pre-trained on 16M image-text pairs.Conference'17, July 2017, Washington, DC, USA Zheng Ma et al.\nConference'17, July 2017, Washington, DC, USA Zheng Ma et al.\nhttps://github.com/lucidrains/DALLE-pytorch Text-to-image Generative models also suffer from region imbalance issues, similar to the previous models. For all regions, we observe a sizeable accuracy gap between the synthetic and real images. For example, Stable Diffusion's accuracy drops from nearly 20 to 40 across all regions. Furthermore, as shown inFigure 5, all models have some culinary culture bias. Specifically, the classifier trained with images generated by Stable Diffusion achieves particularly higher accuracy in Latin-American than other regions. And minDALL-E scores higher in Worldwide, Western, Latin-American than Asian countries. As for DALLE small , it fails to recognize almost all food images from Japanese. In contrast, using real images to train the classifier results in relatively balanced accuracy across regions, which might be because those text-to-image generative models are trained on a biased dataset, which contains fewer traditional food types from certain regions such as Chinese and Japanese.\nACKNOWLEDGMENTSWe would like to thank the anonymous reviewers for their insightful comments. Shujian Huang and Jianbing Zhang are the corresponding authors. This work is supported by National Science Foundation\nFood-101 -Mining Discriminative Components with Random Forests. Lukas Bossard, Matthieu Guillaumin, Luc Van Gool, European Conference on Computer Vision. Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. 2014. Food-101 -Mining Discriminative Components with Random Forests. In European Conference on Computer Vision.\n\nDeep-based Ingredient Recognition for Cooking Recipe Retrieval. Jingjing Chen, Chong-Wah Ngo, 10.1145/2964284.2964315Proceedings of the 2016 ACM Conference on Multimedia Conference, MM 2016. A. Bulterman, Benoit Huet, Aisling Kelliher, Yiannis Kompatsiaris, and Jin Lithe 2016 ACM Conference on Multimedia Conference, MM 2016Amsterdam, The Netherlands; Cees Snoek, Marcel Worring, Dick CACMJingjing Chen and Chong-Wah Ngo. 2016. Deep-based Ingredient Recognition for Cooking Recipe Retrieval. In Proceedings of the 2016 ACM Conference on Multimedia Conference, MM 2016, Amsterdam, The Netherlands, October 15-19, 2016, Alan Hanjalic, Cees Snoek, Marcel Worring, Dick C. A. Bulterman, Benoit Huet, Aisling Kelliher, Yiannis Kompatsiaris, and Jin Li (Eds.). ACM, 32-41. https://doi.org/10.1145/2964284.2964315\n\nChineseFoodNet: A large-scale Image Dataset for Chinese Food Recognition. Xin Chen, Hua Zhou, Liang Diao, arXiv:1705.02743Xin Chen, Hua Zhou, and Liang Diao. 2017. ChineseFoodNet: A large-scale Image Dataset for Chinese Food Recognition. CoRR abs/1705.02743 (2017). arXiv:1705.02743 http://arxiv.org/abs/1705.02743\n\nPlausible May Not Be Faithful: Probing Object Hallucination in Vision-Language Pretraining. Wenliang Dai, Zihan Liu, Ziwei Ji, Dan Su, Pascale Fung, 10.48550/arXiv.2210.07688arXiv:2210.07688Wenliang Dai, Zihan Liu, Ziwei Ji, Dan Su, and Pascale Fung. 2022. Plausible May Not Be Faithful: Probing Object Hallucination in Vision-Language Pre- training. CoRR abs/2210.07688 (2022). https://doi.org/10.48550/arXiv.2210.07688 arXiv:2210.07688\n\nImageNet: A Large-Scale Hierarchical Image Database. J Deng, W Dong, R Socher, L.-J Li, K Li, L Fei-Fei, CVPR09. J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. 2009. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09.\n\nMeteor Universal: Language Specific Translation Evaluation for Any Target Language. J Michael, Alon Denkowski, Lavie, 10.3115/v1/w14-3348Proceedings of the Ninth Workshop on Statistical Machine Translation. the Ninth Workshop on Statistical Machine TranslationBaltimore, Maryland, USAThe Association for Computer LinguisticsMichael J. Denkowski and Alon Lavie. 2014. Meteor Universal: Language Specific Translation Evaluation for Any Target Language. In Proceedings of the Ninth Workshop on Statistical Machine Translation, WMT@ACL 2014, June 26-27, 2014, Baltimore, Maryland, USA. The Association for Computer Linguistics, 376-380. https://doi.org/10.3115/v1/w14-3348\n\nThe Pascal Visual Object Classes (VOC) Challenge. Mark Everingham, Luc Van Gool, K I Christopher, John M Williams, Andrew Winn, Zisserman, IntMark Everingham, Luc Van Gool, Christopher K. I. Williams, John M. Winn, and Andrew Zisserman. 2010. The Pascal Visual Object Classes (VOC) Challenge. Int.\n\n. J Comput, Vis, 10.1007/s11263-009-0275-488J. Comput. Vis. 88, 2 (2010), 303-338. https://doi.org/10.1007/s11263-009-0275-4\n\nBenchmarking Spatial Relationships in Text-to-Image Generation. Tejas Gokhale, Hamid Palangi, Besmira Nushi, Vibhav Vineet, Eric Horvitz, Ece Kamar, Chitta Baral, Yezhou Yang, 10.48550/arXiv.2212.10015arXiv:2212.10015Tejas Gokhale, Hamid Palangi, Besmira Nushi, Vibhav Vineet, Eric Horvitz, Ece Kamar, Chitta Baral, and Yezhou Yang. 2022. Benchmarking Spatial Relationships in Text-to-Image Generation. CoRR abs/2212.10015 (2022). https://doi.org/10. 48550/arXiv.2212.10015 arXiv:2212.10015\n\nProbing Image-Language Transformers for Verb Understanding. Anne Lisa, Aida Hendricks, Nematzadeh, 10.18653/v1/2021.findings-acl.318Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021. Wenjie Li, and Roberto NavigliAssociation for Computational LinguisticsACL/IJCNLP 2021), Chengqing Zong, Fei XiaLisa Anne Hendricks and Aida Nematzadeh. 2021. Probing Image-Language Transformers for Verb Understanding. In Findings of the Association for Com- putational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021 (Find- ings of ACL, Vol. ACL/IJCNLP 2021), Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (Eds.). Association for Computational Linguistics, 3635-3644. https://doi.org/10.18653/v1/2021.findings-acl.318\n\nCLIPScore: A Reference-free Evaluation Metric for Image Captioning. Jack Hessel, Ari Holtzman, Maxwell Forbes, Yejin Ronan Le Bras, Choi, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic2021Virtual Event / Punta CanaJack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. 2021. CLIPScore: A Reference-free Evaluation Metric for Image Captioning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11\n\n10.18653/v1/2021.emnlp-main.595Association for Computational Linguistics. Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau YihNovember, 2021, Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (Eds.). Association for Computational Linguistics, 7514-7528. https://doi.org/10.18653/v1/2021.emnlp-main.595\n\nGANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Sepp Hochreiter, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman GarnettLong Beach, CA, USA, Isabelle GuyonMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. 2017. GANs Trained by a Two Time-Scale Update Rule Con- verge to a Local Nash Equilibrium. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Ro- man Garnett (Eds.). 6626-6637. https://proceedings.neurips.cc/paper/2017/hash/ 8a1d694707eb0fefe65871369074926d-Abstract.html\n\nSemantic Object Accuracy for Generative Text-to-Image Synthesis. Tobias Hinz, Stefan Heinrich, Stefan Wermter, 10.1109/TPAMI.2020.3021209IEEE Trans. Pattern Anal. Mach. Intell. 44Tobias Hinz, Stefan Heinrich, and Stefan Wermter. 2022. Semantic Object Ac- curacy for Generative Text-to-Image Synthesis. IEEE Trans. Pattern Anal. Mach. Intell. 44, 3 (2022), 1552-1565. https://doi.org/10.1109/TPAMI.2020.3021209\n\nAbstract Visual Reasoning with Tangram Shapes. Anya Ji, Noriyuki Kojima, Noah Rush, Alane Suhr, Wai Keen Vong, Robert D Hawkins, Yoav Artzi, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Goldberg, Zornitsa Kozareva, and Yue Zhangthe 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab Emirates2022Association for Computational LinguisticsAnya Ji, Noriyuki Kojima, Noah Rush, Alane Suhr, Wai Keen Vong, Robert D. Hawkins, and Yoav Artzi. 2022. Abstract Visual Reasoning with Tangram Shapes. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Com- putational Linguistics, 582-601. https://aclanthology.org/2022.emnlp-main.38\n\nAutomatic Expansion of a Food Image Dataset Leveraging Existing Categories with Domain Adaptation. Yoshiyuki Kawano, Keiji Yanai, 10.1007/978-3-319-16199-0_1Computer Vision -ECCV 2014 Workshops. Lourdes Agapito, Michael M. Bronstein, and Carsten RotherZurich, SwitzerlandSpringer8927Proceedings, Part IIIYoshiyuki Kawano and Keiji Yanai. 2014. Automatic Expansion of a Food Image Dataset Leveraging Existing Categories with Domain Adaptation. In Computer Vision -ECCV 2014 Workshops -Zurich, Switzerland, September 6-7 and 12, 2014, Proceedings, Part III (Lecture Notes in Computer Science, Vol. 8927), Lourdes Agapito, Michael M. Bronstein, and Carsten Rother (Eds.). Springer, 3-17. https://doi.org/ 10.1007/978-3-319-16199-0_1\n\nVisual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations. Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, Michael S Bernstein, Li Fei-Fei, 10.1007/s11263-016-0981-7Int. J. Comput. Vis. 123Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Li Fei-Fei. 2017. Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations. Int. J. Comput. Vis. 123, 1 (2017), 32-73. https://doi.org/10.1007/s11263-016-0981-7\n\nLearning multiple layers of features from tiny images. Alex Krizhevsky, Geoffrey Hinton, Alex Krizhevsky, Geoffrey Hinton, et al. 2009. Learning multiple layers of features from tiny images. (2009).\n\nTuomas Kynk\u00e4\u00e4nniemi, Tero Karras, Miika Aittala, arXiv:2203.06026Timo Aila, and Jaakko Lehtinen. 2022. The Role of ImageNet Classes in Fr\\'echet Inception Distance. arXiv preprintTuomas Kynk\u00e4\u00e4nniemi, Tero Karras, Miika Aittala, Timo Aila, and Jaakko Lehti- nen. 2022. The Role of ImageNet Classes in Fr\\'echet Inception Distance. arXiv preprint arXiv:2203.06026 (2022).\n\nBLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation. Junnan Li, Dongxu Li, Caiming Xiong, Steven C H Hoi, PMLR, 12888-12900International Conference on Machine Learning, ICML 2022. Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesv\u00e1ri, Gang Niu, and Sivan SabatoBaltimore, Maryland, USA162Proceedings of Machine Learning ResearchJunnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi. 2022. BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Un- derstanding and Generation. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA (Proceedings of Machine Learning Research, Vol. 162), Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesv\u00e1ri, Gang Niu, and Sivan Sabato (Eds.). PMLR, 12888-12900. https://proceedings.mlr.press/v162/li22n.html\n\nRouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Text summarization branches out. Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out. 74-81.\n\nMicrosoft COCO: Common Objects in Context. Tsung-Yi Lin, Michael Maire, Serge J Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, C Lawrence Zitnick, 10.1007/978-3-319-10602-1_48Proceedings, Part V (Lecture Notes in Computer Science. David J. Fleet, Tom\u00e1s Pajdla, Bernt Schiele, and Tinne TuytelaarsPart V (Lecture Notes in Computer ScienceZurich, SwitzerlandSpringer8693Computer Vision -ECCV 2014 -13th European ConferenceTsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C. Lawrence Zitnick. 2014. Microsoft COCO: Common Objects in Context. In Computer Vision -ECCV 2014 -13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V (Lecture Notes in Computer Science, Vol. 8693), David J. Fleet, Tom\u00e1s Pajdla, Bernt Schiele, and Tinne Tuytelaars (Eds.). Springer, 740-755. https://doi.org/10.1007/978-3-319- 10602-1_48\n\nSwin Transformer: Hierarchical Vision Transformer using Shifted Windows. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo, 10.1109/ICCV48922.2021.009862021 IEEE/CVF International Conference on Computer Vision, ICCV 2021. Montreal, QC, CanadaIEEEZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. 2021. Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. In 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021. IEEE, 9992-10002. https://doi.org/10.1109/ICCV48922.2021.00986\n\nRecipe1M: A Dataset for Learning Cross-Modal Embeddings for Cooking Recipes and Food Images. Javier Mar\u00edn, Aritro Biswas, Ferda Ofli, Nicholas Hynes, Amaia Salvador, Yusuf Aytar, Ingmar Weber, Antonio Torralba, arXiv:1810.06553Javier Mar\u00edn, Aritro Biswas, Ferda Ofli, Nicholas Hynes, Amaia Salvador, Yusuf Aytar, Ingmar Weber, and Antonio Torralba. 2018. Recipe1M: A Dataset for Learning Cross-Modal Embeddings for Cooking Recipes and Food Images. CoRR abs/1810.06553 (2018). arXiv:1810.06553 http://arxiv.org/abs/1810.06553\n\nRecipe1M+: A Dataset for Learning Cross-Modal Embeddings for Cooking Recipes and Food Images. Javier Mar\u00edn, Aritro Biswas, Ferda Ofli, Nicholas Hynes, Amaia Salvador, Yusuf Aytar, Ingmar Weber, Antonio Torralba, 10.1109/TPAMI.2019.2927476IEEE Trans. Pattern Anal. Mach. Intell. 43Javier Mar\u00edn, Aritro Biswas, Ferda Ofli, Nicholas Hynes, Amaia Salvador, Yusuf Aytar, Ingmar Weber, and Antonio Torralba. 2021. Recipe1M+: A Dataset for Learning Cross-Modal Embeddings for Cooking Recipes and Food Images. IEEE Trans. Pattern Anal. Mach. Intell. 43, 1 (2021), 187-203. https://doi.org/10.1109/ TPAMI.2019.2927476\n\nYou Are What You Eat: Exploring Rich Recipe Information for Cross-Region Food Analysis. Weiqing Min, Bing-Kun Bao, Shuhuan Mei, Yaohui Zhu, Yong Rui, Shuqiang Jiang, 10.1109/TMM.2017.2759499IEEE Trans. Multim. 20Weiqing Min, Bing-Kun Bao, Shuhuan Mei, Yaohui Zhu, Yong Rui, and Shuqiang Jiang. 2018. You Are What You Eat: Exploring Rich Recipe Information for Cross-Region Food Analysis. IEEE Trans. Multim. 20, 4 (2018), 950-964. https: //doi.org/10.1109/TMM.2017.2759499\n\n. Weiqing Min, Shuqiang Jiang, Linhu Liu, Yong Rui, Ramesh C Jain, 10.1145/3329168A Survey on Food Computing. ACM Comput. Surv. 5236Weiqing Min, Shuqiang Jiang, Linhu Liu, Yong Rui, and Ramesh C. Jain. 2019. A Survey on Food Computing. ACM Comput. Surv. 52, 5 (2019), 92:1-92:36. https://doi.org/10.1145/3329168\n\nISIA Food-500: A Dataset for Large-Scale Food Recognition via Stacked Global-Local Attention Network. Weiqing Min, Linhu Liu, Zhiling Wang, Zhengdong Luo, Xiaoming Wei, Xiaolin Wei, Shuqiang Jiang, 10.1145/3394171.3414031MM '20: The 28th ACM International Conference on Multimedia, Virtual Event. Chang Wen Chen, Rita Cucchiara, Xian-Sheng Hua, Guo-Jun Qi, Elisa Ricci, Zhengyou Zhang, and Roger ZimmermannSeattle, WA, USAACMWeiqing Min, Linhu Liu, Zhiling Wang, Zhengdong Luo, Xiaoming Wei, Xiaolin Wei, and Shuqiang Jiang. 2020. ISIA Food-500: A Dataset for Large-Scale Food Recognition via Stacked Global-Local Attention Network. In MM '20: The 28th ACM International Conference on Multimedia, Virtual Event / Seattle, WA, USA, October 12-16, 2020, Chang Wen Chen, Rita Cucchiara, Xian-Sheng Hua, Guo-Jun Qi, Elisa Ricci, Zhengyou Zhang, and Roger Zimmermann (Eds.). ACM, 393-401. https://doi.org/10.1145/3394171.3414031\n\nGLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models. Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya Sutskever, Mark Chen, PMLRInternational Conference on Machine Learning, ICML 2022. Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesv\u00e1ri, Gang Niu, and Sivan SabatoBaltimore, Maryland, USA162Proceedings of Machine Learning ResearchAlexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. 2022. GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffu- sion Models. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA (Proceedings of Machine Learning Research, Vol. 162), Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesv\u00e1ri, Gang Niu, and Sivan Sabato (Eds.). PMLR, 16784-16804. https://proceedings.mlr.press/ v162/nichol22a.html\n\nBleu: a Method for Automatic Evaluation of Machine Translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational LinguisticsPhiladelphia, PA, USA. ACLKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002, Philadelphia, PA, USA. ACL, 311-318. https://doi.org/10.3115/1073083. 1073135\n\nVALSE: A Task-Independent Benchmark for Vision and Language Models Centered on Linguistic Phenomena. Letitia Parcalabescu, Michele Cafagna, Lilitta Muradjan, Anette Frank, Iacer Calixto, Albert Gatt, 10.18653/v1/2022.acl-long.567Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Smaranda Muresan, Preslav Nakov, and Aline Villavicenciothe 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics1Long Papers), ACL 2022Letitia Parcalabescu, Michele Cafagna, Lilitta Muradjan, Anette Frank, Iacer Calixto, and Albert Gatt. 2022. VALSE: A Task-Independent Benchmark for Vision and Language Models Centered on Linguistic Phenomena. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (Eds.). Association for Computational Linguistics, 8253-8280. https://doi.org/10.18653/v1/2022.acl-long.567\n\nBenchmark for Compositional Text-to-Image Synthesis. Samaneh Dong Huk Park, Xihui Azadi, Trevor Liu, Anna Darrell, Rohrbach, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021. Joaquin Vanschoren and Sai-Kit Yeungthe Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021Dong Huk Park, Samaneh Azadi, Xihui Liu, Trevor Darrell, and Anna Rohrbach. 2021. Benchmark for Compositional Text-to-Image Synthesis. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, Joaquin Vanschoren and Sai-Kit Yeung (Eds.). https://datasets-benchmarks- proceedings.neurips.cc/paper/2021/hash/0a09c8844ba8f0936c20bd791130d6b6- Abstract-round1.html\n\nMining Discriminative Food Regions for Accurate Food Recognition. Jianing Qiu, Frank Po Wen Lo, Yingnan Sun, Siyao Wang, Benny Lo, 30Jianing Qiu, Frank Po Wen Lo, Yingnan Sun, Siyao Wang, and Benny Lo. 2019. Mining Discriminative Food Regions for Accurate Food Recognition. In 30th\n\nBritish Machine Vision Conference. Cardiff, UKBMVA Press158British Machine Vision Conference 2019, BMVC 2019, Cardiff, UK, September 9-12, 2019. BMVA Press, 158. https://bmvc2019.org/wp-content/uploads/papers/0839- paper.pdf\n\nAre Multimodal Models Robust to Image and Text Perturbations?. Jielin Qiu, Yi Zhu, Xingjian Shi, Florian Wenzel, Zhiqiang Tang, Ding Zhao, Bo Li, Mu Li, 10.48550/arXiv.2212.08044arXiv:2212.08044Jielin Qiu, Yi Zhu, Xingjian Shi, Florian Wenzel, Zhiqiang Tang, Ding Zhao, Bo Li, and Mu Li. 2022. Are Multimodal Models Robust to Image and Text Perturbations? CoRR abs/2212.08044 (2022). https://doi.org/10.48550/arXiv.2212. 08044 arXiv:2212.08044\n\nLearning Transferable Visual Models From Natural Language Supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, PMLRProceedings of the 38th International Conference on Machine Learning, ICML 2021. Marina Meila and Tong Zhangthe 38th International Conference on Machine Learning, ICML 2021139Virtual Event (Proceedings ofAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning Transferable Visual Models From Natural Language Supervision. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event (Pro- ceedings of Machine Learning Research, Vol. 139), Marina Meila and Tong Zhang (Eds.). PMLR, 8748-8763. http://proceedings.mlr.press/v139/radford21a.html\n\nZero-Shot Text-to-Image Generation. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever, PMLR, 8821-8831Proceedings of the 38th International Conference on Machine Learning, ICML 2021. Marina Meila and Tong Zhangthe 38th International Conference on Machine Learning, ICML 2021139Virtual Event (Proceedings of Machine Learning ResearchAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021. Zero-Shot Text-to-Image Gen- eration. In Proceedings of the 38th International Conference on Machine Learn- ing, ICML 2021, 18-24 July 2021, Virtual Event (Proceedings of Machine Learning Research, Vol. 139), Marina Meila and Tong Zhang (Eds.). PMLR, 8821-8831. http://proceedings.mlr.press/v139/ramesh21a.html\n\nClassification Accuracy Score for Conditional Generative Models. V Suman, Oriol Ravuri, Vinyals, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. Alina Beygelzimer, Florence d'Alch\u00e9-Buc, Emily B. Fox, and Roman GarnettNeurIPS; Vancouver, BC, Canada, Hanna M. Wallach, Hugo LarochelleSuman V. Ravuri and Oriol Vinyals. 2019. Classification Accuracy Score for Conditional Generative Models. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alch\u00e9-Buc, Emily B. Fox, and Roman Garnett (Eds.). 12247-12258. https://proceedings.neurips.cc/paper/2019/ hash/fcf55a303b71b84d326fb1d06e332a26-Abstract.html\n\nObject Hallucination in Image Captioning. Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, Kate Saenko, 10.18653/v1/d18-1437Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun'ichi Tsujiithe 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsAnna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. 2018. Object Hallucination in Image Captioning. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 -November 4, 2018, Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun'ichi Tsujii (Eds.). Association for Computational Linguistics, 4035-4045. https://doi.org/10.18653/v1/d18-1437\n\nDiscrete Variational Autoencoders. Jason Tyler, Rolfe , 5th International Conference on Learning Representations. Toulon, FranceConference Track Proceedings. OpenReview.netJason Tyler Rolfe. 2017. Discrete Variational Autoencoders. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net. https://openreview.net/ forum?id=ryMxXPFex\n\nHigh-Resolution Image Synthesis with Latent Diffusion Models. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj\u00f6rn Ommer, 10.1109/CVPR52688.2022.01042IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022. New Orleans, LA, USAIEEERobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. 2022. High-Resolution Image Synthesis with Latent Diffusion Models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022. IEEE, 10674-10685. https://doi.org/10. 1109/CVPR52688.2022.01042\n\nProbing the Role of Positional Information in Vision-Language Models. J Philipp, Jindrich R\u00f6sch, Libovick\u00fd, 10.18653/v1/2022.findings-naacl.77Findings of the Association for Computational Linguistics: NAACL 2022. Marine Carpuat, Marie-Catherine de Marneffe, and Iv\u00e1n Vladimir Meza Ru\u00edzSeattle, WA, United StatesAssociation for Computational LinguisticsPhilipp J. R\u00f6sch and Jindrich Libovick\u00fd. 2022. Probing the Role of Positional Information in Vision-Language Models. In Findings of the Association for Com- putational Linguistics: NAACL 2022, Seattle, WA, United States, July 10-15, 2022, Marine Carpuat, Marie-Catherine de Marneffe, and Iv\u00e1n Vladimir Meza Ru\u00edz (Eds.). Association for Computational Linguistics, 1031-1041. https://doi.org/10. 18653/v1/2022.findings-naacl.77\n\n2021. minDALL-E on Conceptual Captions. Chiheon Kim Doyup Lee Saehoon, Sanghun Kim, Woonhyuk Cho, Baek, Chiheon Kim Doyup Lee Saehoon Kim, Sanghun Cho and Woonhyuk Baek. 2021. minDALL-E on Conceptual Captions. https://github.com/kakaobrain/minDALL- E.\n\nPhotorealistic Text-to-Image Diffusion Models with Deep Language Understanding. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, 10.48550/arXiv.2205.11487arXiv:2205.11487Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. 2022Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mah- davi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. 2022. Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding. CoRR abs/2205.11487 (2022). https: //doi.org/10.48550/arXiv.2205.11487 arXiv:2205.11487\n\nLAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs. Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, Aran Komatsuzaki, arXiv:2111.02114Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. 2021. LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs. CoRR abs/2111.02114 (2021). arXiv:2111.02114 https://arxiv.org/abs/2111.02114\n\nAutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. Taylor Shin, Yasaman Razeghi, Robert L Logan, I V , Eric Wallace, Sameer Singh, 10.18653/v1/2020.emnlp-main.346Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. Trevor Cohn, Yulan He, and Yang Liuthe 2020 Conference on Empirical Methods in Natural Language ProcessingOnline; Bonnie WebberAssociation for Computational Linguistics2020Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. 2020. AutoPrompt: Eliciting Knowledge from Language Models with Auto- matically Generated Prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, 4222-4235. https://doi.org/10.18653/v1/2020.emnlp- main.346\n\nFLAVA: A Foundational Language And Vision Alignment Model. Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, Douwe Kiela, 10.1109/CVPR52688.2022.01519IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022. New Orleans, LA, USAIEEEAmanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Woj- ciech Galuba, Marcus Rohrbach, and Douwe Kiela. 2022. FLAVA: A Foundational Language And Vision Alignment Model. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022. IEEE, 15617-15629. https://doi.org/10.1109/CVPR52688.2022.01519\n\nVL-BERT: Pre-training of Generic Visual-Linguistic Representations. Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, Jifeng Dai, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. 2020. VL-BERT: Pre-training of Generic Visual-Linguistic Representations. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. https://openreview.net/forum?id= SygXPaEYvH\n\nRethinking the Inception Architecture for Computer Vision. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna, 10.1109/CVPR.2016.3082016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016. Las Vegas, NV, USAIEEE Computer SocietyChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbig- niew Wojna. 2016. Rethinking the Inception Architecture for Computer Vi- sion. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016. IEEE Computer Society, 2818-2826. https://doi.org/10.1109/CVPR.2016.308\n\nLXMERT: Learning Cross-Modality Encoder Representations from Transformers. Hao Tan, Mohit Bansal, 10.18653/v1/D19-1514Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wanthe 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingHong Kong, ChinaAssociation for Computational LinguisticsHao Tan and Mohit Bansal. 2019. LXMERT: Learning Cross-Modality Encoder Representations from Transformers. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (Eds.). Association for Computational Linguistics, 5099-5110. https://doi. org/10.18653/v1/D19-1514\n\nWinoground: Probing Vision and Language Models for Visio-Linguistic Compositionality. Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, Candace Ross, 10.1109/CVPR52688.2022.00517IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022. New Orleans, LA, USAIEEETristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and Candace Ross. 2022. Winoground: Probing Vision and Lan- guage Models for Visio-Linguistic Compositionality. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022. IEEE, 5228-5238. https://doi.org/10.1109/CVPR52688.2022.00517\n\nCIDEr: Consensus-based image description evaluation. C Lawrence Ramakrishna Vedantam, Devi Zitnick, Parikh, 10.1109/CVPR.2015.7299087IEEE Conference on Computer Vision and Pattern Recognition. Boston, MA, USAIEEE Computer SocietyRamakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. 2015. CIDEr: Consensus-based image description evaluation. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015. IEEE Computer Society, 4566-4575. https://doi.org/10.1109/CVPR.2015.7299087\n\nVL-Taboo: An Analysis of Attribute-based Zero-shot Capabilities of Vision-Language Models. Felix Vogel, Nina Shvetsova, Leonid Karlinsky, Hilde Kuehne, 10.48550/arXiv.2209.06103arXiv:2209.06103Felix Vogel, Nina Shvetsova, Leonid Karlinsky, and Hilde Kuehne. 2022. VL- Taboo: An Analysis of Attribute-based Zero-shot Capabilities of Vision-Language Models. CoRR abs/2209.06103 (2022). https://doi.org/10.48550/arXiv.2209.06103 arXiv:2209.06103\n\nGIT: A Generative Image-to-text Transformer for Vision and Language. Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, Lijuan Wang, 10.48550/arXiv.2205.14100arXiv:2205.14100Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. 2022. GIT: A Generative Image-to-text Transformer for Vision and Language. CoRR abs/2205.14100 (2022). https: //doi.org/10.48550/arXiv.2205.14100 arXiv:2205.14100\n\nOFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework. Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, Hongxia Yang, PMLR, 23318-23340International Conference on Machine Learning, ICML 2022. Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesv\u00e1ri, Gang Niu, and Sivan SabatoBaltimore, Maryland, USA162Proceedings of Machine Learning ResearchPeng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. 2022. OFA: Unifying Ar- chitectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA (Proceedings of Machine Learn- ing Research, Vol. 162), Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesv\u00e1ri, Gang Niu, and Sivan Sabato (Eds.). PMLR, 23318-23340. https: //proceedings.mlr.press/v162/wang22al.html\n\nRecipe recognition with large multimodal food dataset. Xin Wang, Devinder Kumar, Nicolas Thome, Matthieu Cord, Fr\u00e9d\u00e9ric Precioso, 10.1109/ICMEW.2015.71697572015 IEEE International Conference on Multimedia & Expo Workshops, ICME Workshops 2015. Turin, ItalyXin Wang, Devinder Kumar, Nicolas Thome, Matthieu Cord, and Fr\u00e9d\u00e9ric Pre- cioso. 2015. Recipe recognition with large multimodal food dataset. In 2015 IEEE International Conference on Multimedia & Expo Workshops, ICME Work- shops 2015, Turin, Italy, June 29 -July 3, 2015. IEEE Computer Society, 1-6. https://doi.org/10.1109/ICMEW.2015.7169757\n\nA Large-Scale Benchmark for Food Image Segmentation. Xiongwei Wu, Xin Fu, Ying Liu, Ee-Peng Lim, C H Steven, Qianru Hoi, Sun, 10.1145/3474085.3475201MM '21: ACM Multimedia Conference, Virtual Event. Heng Tao Shen, Yueting Zhuang, John R. Smith, Yang Yang, Pablo C\u00e9sar, Florian Metze, and Balakrishnan PrabhakaranChinaACMXiongwei Wu, Xin Fu, Ying Liu, Ee-Peng Lim, Steven C. H. Hoi, and Qianru Sun. 2021. A Large-Scale Benchmark for Food Image Segmentation. In MM '21: ACM Multimedia Conference, Virtual Event, China, October 20 -24, 2021, Heng Tao Shen, Yueting Zhuang, John R. Smith, Yang Yang, Pablo C\u00e9sar, Florian Metze, and Balakrishnan Prabhakaran (Eds.). ACM, 506-515. https://doi.org/10.1145/ 3474085.3475201\n\nVision-Language Pre-Training with Triple Contrastive Learning. Jinyu Yang, Jiali Duan, Son Tran, Yi Xu, Sampath Chanda, Liqun Chen, Belinda Zeng, Trishul Chilimbi, Junzhou Huang, 10.1109/CVPR52688.2022.01522IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022. New Orleans, LA, USAIEEEJinyu Yang, Jiali Duan, Son Tran, Yi Xu, Sampath Chanda, Liqun Chen, Belinda Zeng, Trishul Chilimbi, and Junzhou Huang. 2022. Vision-Language Pre-Training with Triple Contrastive Learning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022. IEEE, 15650-15659. https://doi.org/10.1109/CVPR52688.2022.01522\n\nFrom image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Peter Young, Alice Lai, Micah Hodosh, Julia Hockenmaier, 10.1162/tacl_a_00166Trans. Assoc. Comput. Linguistics. 2Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. 2014. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Trans. Assoc. Comput. Linguistics 2 (2014), 67-78. https://doi.org/10.1162/tacl_a_00166\n\nWhen and why vision-language models behave like bags-of-words, and what to do about it?. Mert Y\u00fcksekg\u00f6n\u00fcl, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, James Zou, 10.48550/arXiv.2210.01936arXiv:2210.01936Mert Y\u00fcksekg\u00f6n\u00fcl, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. 2022. When and why vision-language models behave like bags-of-words, and what to do about it? CoRR abs/2210.01936 (2022). https://doi.org/10.48550/ arXiv.2210.01936 arXiv:2210.01936\n\nMulti-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts. Yan Zeng, Xinsong Zhang, Hang Li, PMLRInternational Conference on Machine Learning, ICML 2022. Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesv\u00e1ri, Gang Niu, and Sivan SabatoBaltimore, Maryland, USA162Proceedings of Machine Learning ResearchYan Zeng, Xinsong Zhang, and Hang Li. 2022. Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA (Proceedings of Machine Learning Research, Vol. 162), Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesv\u00e1ri, Gang Niu, and Sivan Sabato (Eds.). PMLR, 25994-26009. https://proceedings.mlr.press/v162/zeng22c.html\n\nVL-CheckList: Evaluating Pre-trained Vision-Language Models with Objects, Attributes and Relations. Tiancheng Zhao, Tianqi Zhang, Mingwei Zhu, Haozhan Shen, Kyusong Lee, Xiaopeng Lu, Jianwei Yin, 10.48550/arXiv.2207.00221arXiv:2207.00221Tiancheng Zhao, Tianqi Zhang, Mingwei Zhu, Haozhan Shen, Kyusong Lee, Xiaopeng Lu, and Jianwei Yin. 2022. VL-CheckList: Evaluating Pre-trained Vision- Language Models with Objects, Attributes and Relations. CoRR abs/2207.00221 (2022). https://doi.org/10.48550/arXiv.2207.00221 arXiv:2207.00221\n\nLearning to Prompt for Vision-Language Models. Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu, 10.1007/s11263-022-01653-1Int. J. Comput. Vis. 130Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. 2022. Learning to Prompt for Vision-Language Models. Int. J. Comput. Vis. 130, 9 (2022), 2337-2348. https://doi.org/10.1007/s11263-022-01653-1\n\nVLUE: A Multi-Task Benchmark for Evaluating Vision-Language Models. Wangchunshu Zhou, Yan Zeng, Shizhe Diao, Xinsong Zhang, 10.48550/arXiv.2205.15237arXiv:2205.15237Wangchunshu Zhou, Yan Zeng, Shizhe Diao, and Xinsong Zhang. 2022. VLUE: A Multi-Task Benchmark for Evaluating Vision-Language Mod- els. CoRR abs/2205.15237 (2022). https://doi.org/10.48550/arXiv.2205.15237 arXiv:2205.15237\n", "annotations": {"author": "[{\"end\":102,\"start\":93},{\"end\":115,\"start\":103},{\"end\":126,\"start\":116},{\"end\":165,\"start\":127},{\"end\":181,\"start\":166},{\"end\":215,\"start\":182},{\"end\":246,\"start\":216},{\"end\":334,\"start\":247},{\"end\":422,\"start\":335},{\"end\":510,\"start\":423},{\"end\":598,\"start\":511},{\"end\":686,\"start\":599},{\"end\":774,\"start\":687},{\"end\":862,\"start\":775}]", "publisher": null, "author_last_name": "[{\"end\":101,\"start\":99},{\"end\":114,\"start\":111},{\"end\":125,\"start\":123},{\"end\":139,\"start\":134},{\"end\":180,\"start\":175},{\"end\":195,\"start\":190},{\"end\":227,\"start\":223}]", "author_first_name": "[{\"end\":98,\"start\":93},{\"end\":110,\"start\":103},{\"end\":122,\"start\":116},{\"end\":133,\"start\":127},{\"end\":174,\"start\":166},{\"end\":189,\"start\":182},{\"end\":222,\"start\":216}]", "author_affiliation": "[{\"end\":333,\"start\":248},{\"end\":421,\"start\":336},{\"end\":509,\"start\":424},{\"end\":597,\"start\":512},{\"end\":685,\"start\":600},{\"end\":773,\"start\":688},{\"end\":861,\"start\":776}]", "title": "[{\"end\":90,\"start\":1},{\"end\":952,\"start\":863}]", "venue": null, "abstract": "[{\"end\":2942,\"start\":1000}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3026,\"start\":3022},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":3029,\"start\":3026},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":3032,\"start\":3029},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3035,\"start\":3032},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":3038,\"start\":3035},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":3041,\"start\":3038},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":3044,\"start\":3041},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":3352,\"start\":3348},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3486,\"start\":3483},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3489,\"start\":3486},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3492,\"start\":3489},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3495,\"start\":3492},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":3498,\"start\":3495},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":3501,\"start\":3498},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":4655,\"start\":4651},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":5102,\"start\":5098},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5963,\"start\":5960},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5966,\"start\":5963},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":5969,\"start\":5966},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":6384,\"start\":6380},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":6433,\"start\":6429},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":6499,\"start\":6495},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8080,\"start\":8077},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8116,\"start\":8113},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8119,\"start\":8116},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8171,\"start\":8167},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":8226,\"start\":8222},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8264,\"start\":8260},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":8267,\"start\":8264},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":8270,\"start\":8267},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8350,\"start\":8346},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8399,\"start\":8395},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":8438,\"start\":8434},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":8441,\"start\":8438},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8475,\"start\":8472},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8760,\"start\":8756},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8780,\"start\":8776},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":8797,\"start\":8793},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9317,\"start\":9314},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":9337,\"start\":9333},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9373,\"start\":9369},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":9388,\"start\":9384},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9427,\"start\":9424},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9447,\"start\":9444},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9485,\"start\":9481},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":9581,\"start\":9577},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9604,\"start\":9601},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9727,\"start\":9723},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":9821,\"start\":9817},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9973,\"start\":9969},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9992,\"start\":9988},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10380,\"start\":10376},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10383,\"start\":10380},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":11127,\"start\":11123},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":13768,\"start\":13765},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":14024,\"start\":14020},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":14027,\"start\":14024},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":14030,\"start\":14027},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":14033,\"start\":14030},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":14036,\"start\":14033},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":14074,\"start\":14070},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":14077,\"start\":14074},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":14080,\"start\":14077},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":14122,\"start\":14118},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":14125,\"start\":14122},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":14617,\"start\":14613},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":15065,\"start\":15061},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":15406,\"start\":15402},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":15708,\"start\":15704},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":16091,\"start\":16087},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":16592,\"start\":16588},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":16595,\"start\":16592},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":16718,\"start\":16715},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":16734,\"start\":16731},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":16746,\"start\":16742},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":16797,\"start\":16793},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":16812,\"start\":16808},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":16858,\"start\":16855},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":17068,\"start\":17064},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":17071,\"start\":17068},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":17074,\"start\":17071},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":18425,\"start\":18421},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":18428,\"start\":18425},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":18431,\"start\":18428},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":18434,\"start\":18431},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":18437,\"start\":18434},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":18529,\"start\":18525},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":18542,\"start\":18538},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":22737,\"start\":22733},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":22775,\"start\":22771},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":23052,\"start\":23048},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":23565,\"start\":23561},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":23914,\"start\":23910},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":23917,\"start\":23914},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":23920,\"start\":23917},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":24284,\"start\":24280},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":24297,\"start\":24293},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":24308,\"start\":24305},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":24323,\"start\":24319},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":24368,\"start\":24364},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":24467,\"start\":24463},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":25714,\"start\":25711},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":25717,\"start\":25714},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":27651,\"start\":27647},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":27921,\"start\":27917},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":28252,\"start\":28248},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":28291,\"start\":28287},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":28453,\"start\":28449},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":28466,\"start\":28462},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":29006,\"start\":29002},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":29024,\"start\":29020},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":29185,\"start\":29181},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":29199,\"start\":29195},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":29270,\"start\":29266}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":33446,\"start\":33150},{\"attributes\":{\"id\":\"fig_2\"},\"end\":33576,\"start\":33447},{\"attributes\":{\"id\":\"fig_3\"},\"end\":33958,\"start\":33577},{\"attributes\":{\"id\":\"fig_4\"},\"end\":34133,\"start\":33959},{\"attributes\":{\"id\":\"fig_6\"},\"end\":34466,\"start\":34134},{\"attributes\":{\"id\":\"fig_7\"},\"end\":34566,\"start\":34467},{\"attributes\":{\"id\":\"fig_8\"},\"end\":34839,\"start\":34567},{\"attributes\":{\"id\":\"fig_9\"},\"end\":34939,\"start\":34840},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":35641,\"start\":34940},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":36070,\"start\":35642},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":36556,\"start\":36071},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":36791,\"start\":36557},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":36997,\"start\":36792}]", "paragraph": "[{\"end\":4380,\"start\":2958},{\"end\":4503,\"start\":4382},{\"end\":6174,\"start\":4505},{\"end\":7400,\"start\":6176},{\"end\":7460,\"start\":7402},{\"end\":7688,\"start\":7495},{\"end\":7920,\"start\":7690},{\"end\":8567,\"start\":7922},{\"end\":9149,\"start\":8569},{\"end\":9697,\"start\":9166},{\"end\":10843,\"start\":9699},{\"end\":10989,\"start\":10860},{\"end\":11445,\"start\":11014},{\"end\":12380,\"start\":11447},{\"end\":13066,\"start\":12382},{\"end\":13855,\"start\":13109},{\"end\":14543,\"start\":13887},{\"end\":15055,\"start\":14608},{\"end\":15394,\"start\":15057},{\"end\":15696,\"start\":15396},{\"end\":16080,\"start\":15698},{\"end\":16531,\"start\":16082},{\"end\":17052,\"start\":16552},{\"end\":17377,\"start\":17054},{\"end\":18131,\"start\":17379},{\"end\":18926,\"start\":18133},{\"end\":19689,\"start\":18939},{\"end\":19878,\"start\":19691},{\"end\":21065,\"start\":19880},{\"end\":21409,\"start\":21067},{\"end\":22011,\"start\":21411},{\"end\":22673,\"start\":22013},{\"end\":23042,\"start\":22729},{\"end\":23554,\"start\":23044},{\"end\":23780,\"start\":23556},{\"end\":24369,\"start\":23801},{\"end\":24726,\"start\":24371},{\"end\":24873,\"start\":24756},{\"end\":25058,\"start\":24886},{\"end\":26435,\"start\":25060},{\"end\":27166,\"start\":26437},{\"end\":27584,\"start\":27168},{\"end\":28268,\"start\":27640},{\"end\":28747,\"start\":28270},{\"end\":29577,\"start\":28768},{\"end\":29846,\"start\":29590},{\"end\":30939,\"start\":29848},{\"end\":31842,\"start\":30954},{\"end\":32405,\"start\":31889},{\"end\":32446,\"start\":32407},{\"end\":32576,\"start\":32448},{\"end\":32770,\"start\":32578},{\"end\":32870,\"start\":32772},{\"end\":32983,\"start\":32872},{\"end\":33149,\"start\":33031}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":14607,\"start\":14584},{\"attributes\":{\"id\":\"formula_1\"},\"end\":24755,\"start\":24727}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":12912,\"start\":12905},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":13339,\"start\":13332},{\"end\":18035,\"start\":18028},{\"end\":21110,\"start\":21103},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":24930,\"start\":24923},{\"end\":25426,\"start\":25419},{\"end\":25443,\"start\":25437},{\"end\":25832,\"start\":25825},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":26069,\"start\":26061},{\"end\":26271,\"start\":26264},{\"end\":29655,\"start\":29648},{\"end\":29922,\"start\":29915},{\"end\":32664,\"start\":32657}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2956,\"start\":2944},{\"attributes\":{\"n\":\"2\"},\"end\":7493,\"start\":7463},{\"attributes\":{\"n\":\"2.2\"},\"end\":9164,\"start\":9152},{\"attributes\":{\"n\":\"3\"},\"end\":10858,\"start\":10846},{\"attributes\":{\"n\":\"3.1\"},\"end\":11012,\"start\":10992},{\"attributes\":{\"n\":\"3.2\"},\"end\":13107,\"start\":13069},{\"attributes\":{\"n\":\"4\"},\"end\":13885,\"start\":13858},{\"attributes\":{\"n\":\"4.1\"},\"end\":14583,\"start\":14546},{\"attributes\":{\"n\":\"4.1.2\"},\"end\":16550,\"start\":16534},{\"attributes\":{\"n\":\"4.1.3\"},\"end\":18937,\"start\":18929},{\"attributes\":{\"n\":\"4.2\"},\"end\":22707,\"start\":22676},{\"attributes\":{\"n\":\"4.2.1\"},\"end\":22727,\"start\":22710},{\"attributes\":{\"n\":\"4.2.2\"},\"end\":23799,\"start\":23783},{\"attributes\":{\"n\":\"4.2.3\"},\"end\":24884,\"start\":24876},{\"attributes\":{\"n\":\"4.3\"},\"end\":27618,\"start\":27587},{\"attributes\":{\"n\":\"4.3.1\"},\"end\":27638,\"start\":27621},{\"attributes\":{\"n\":\"4.3.2\"},\"end\":28766,\"start\":28750},{\"attributes\":{\"n\":\"4.3.3\"},\"end\":29588,\"start\":29580},{\"attributes\":{\"n\":\"5\"},\"end\":30952,\"start\":30942},{\"end\":31887,\"start\":31845},{\"end\":33029,\"start\":32986},{\"end\":33161,\"start\":33151},{\"end\":33458,\"start\":33448},{\"end\":33586,\"start\":33578},{\"end\":33970,\"start\":33960},{\"end\":34145,\"start\":34135},{\"end\":34478,\"start\":34468},{\"end\":34572,\"start\":34568},{\"end\":34851,\"start\":34841},{\"end\":35652,\"start\":35643},{\"end\":36081,\"start\":36072},{\"end\":36567,\"start\":36558},{\"end\":36802,\"start\":36793}]", "table": "[{\"end\":35641,\"start\":35277},{\"end\":36556,\"start\":36361},{\"end\":36997,\"start\":36979}]", "figure_caption": "[{\"end\":33446,\"start\":33163},{\"end\":33576,\"start\":33460},{\"end\":33958,\"start\":33588},{\"end\":34133,\"start\":33972},{\"end\":34466,\"start\":34147},{\"end\":34566,\"start\":34480},{\"end\":34839,\"start\":34573},{\"end\":34939,\"start\":34853},{\"end\":35277,\"start\":34942},{\"end\":36070,\"start\":35654},{\"end\":36361,\"start\":36083},{\"end\":36791,\"start\":36569},{\"end\":36979,\"start\":36804}]", "figure_ref": "[{\"end\":4056,\"start\":4048},{\"end\":5803,\"start\":5795},{\"end\":12379,\"start\":12371},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":18963,\"start\":18955},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":20314,\"start\":20306},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":20497,\"start\":20489},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":20823,\"start\":20815},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":22364,\"start\":22356},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":22507,\"start\":22499},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":30156,\"start\":30148},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":30212,\"start\":30204},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":32042,\"start\":32034},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":32234,\"start\":32226}]", "bib_author_first_name": "[{\"end\":38656,\"start\":38651},{\"end\":38674,\"start\":38666},{\"end\":38690,\"start\":38687},{\"end\":38981,\"start\":38973},{\"end\":38997,\"start\":38988},{\"end\":39795,\"start\":39792},{\"end\":39805,\"start\":39802},{\"end\":39817,\"start\":39812},{\"end\":40134,\"start\":40126},{\"end\":40145,\"start\":40140},{\"end\":40156,\"start\":40151},{\"end\":40164,\"start\":40161},{\"end\":40176,\"start\":40169},{\"end\":40527,\"start\":40526},{\"end\":40535,\"start\":40534},{\"end\":40543,\"start\":40542},{\"end\":40556,\"start\":40552},{\"end\":40562,\"start\":40561},{\"end\":40568,\"start\":40567},{\"end\":40804,\"start\":40803},{\"end\":40818,\"start\":40814},{\"end\":41443,\"start\":41439},{\"end\":41459,\"start\":41456},{\"end\":41471,\"start\":41470},{\"end\":41473,\"start\":41472},{\"end\":41491,\"start\":41487},{\"end\":41493,\"start\":41492},{\"end\":41510,\"start\":41504},{\"end\":41691,\"start\":41690},{\"end\":41883,\"start\":41878},{\"end\":41898,\"start\":41893},{\"end\":41915,\"start\":41908},{\"end\":41929,\"start\":41923},{\"end\":41942,\"start\":41938},{\"end\":41955,\"start\":41952},{\"end\":41969,\"start\":41963},{\"end\":41983,\"start\":41977},{\"end\":42370,\"start\":42366},{\"end\":42381,\"start\":42377},{\"end\":43129,\"start\":43125},{\"end\":43141,\"start\":43138},{\"end\":43159,\"start\":43152},{\"end\":43173,\"start\":43168},{\"end\":44144,\"start\":44138},{\"end\":44159,\"start\":44153},{\"end\":44176,\"start\":44170},{\"end\":44198,\"start\":44190},{\"end\":44212,\"start\":44208},{\"end\":45119,\"start\":45113},{\"end\":45132,\"start\":45126},{\"end\":45149,\"start\":45143},{\"end\":45510,\"start\":45506},{\"end\":45523,\"start\":45515},{\"end\":45536,\"start\":45532},{\"end\":45548,\"start\":45543},{\"end\":45558,\"start\":45555},{\"end\":45563,\"start\":45559},{\"end\":45576,\"start\":45570},{\"end\":45578,\"start\":45577},{\"end\":45592,\"start\":45588},{\"end\":46449,\"start\":46440},{\"end\":46463,\"start\":46458},{\"end\":47168,\"start\":47162},{\"end\":47182,\"start\":47178},{\"end\":47194,\"start\":47188},{\"end\":47208,\"start\":47202},{\"end\":47223,\"start\":47218},{\"end\":47236,\"start\":47230},{\"end\":47255,\"start\":47246},{\"end\":47268,\"start\":47262},{\"end\":47287,\"start\":47281},{\"end\":47297,\"start\":47292},{\"end\":47299,\"start\":47298},{\"end\":47315,\"start\":47308},{\"end\":47317,\"start\":47316},{\"end\":47331,\"start\":47329},{\"end\":47816,\"start\":47812},{\"end\":47837,\"start\":47829},{\"end\":47963,\"start\":47957},{\"end\":47982,\"start\":47978},{\"end\":47996,\"start\":47991},{\"end\":48440,\"start\":48434},{\"end\":48451,\"start\":48445},{\"end\":48463,\"start\":48456},{\"end\":48477,\"start\":48471},{\"end\":48481,\"start\":48478},{\"end\":49278,\"start\":49270},{\"end\":49488,\"start\":49480},{\"end\":49501,\"start\":49494},{\"end\":49514,\"start\":49509},{\"end\":49516,\"start\":49515},{\"end\":49532,\"start\":49527},{\"end\":49545,\"start\":49539},{\"end\":49558,\"start\":49554},{\"end\":49573,\"start\":49568},{\"end\":49583,\"start\":49582},{\"end\":49592,\"start\":49584},{\"end\":50435,\"start\":50433},{\"end\":50447,\"start\":50441},{\"end\":50456,\"start\":50453},{\"end\":50465,\"start\":50462},{\"end\":50476,\"start\":50470},{\"end\":50487,\"start\":50482},{\"end\":50502,\"start\":50495},{\"end\":50515,\"start\":50508},{\"end\":51093,\"start\":51087},{\"end\":51107,\"start\":51101},{\"end\":51121,\"start\":51116},{\"end\":51136,\"start\":51128},{\"end\":51149,\"start\":51144},{\"end\":51165,\"start\":51160},{\"end\":51179,\"start\":51173},{\"end\":51194,\"start\":51187},{\"end\":51620,\"start\":51614},{\"end\":51634,\"start\":51628},{\"end\":51648,\"start\":51643},{\"end\":51663,\"start\":51655},{\"end\":51676,\"start\":51671},{\"end\":51692,\"start\":51687},{\"end\":51706,\"start\":51700},{\"end\":51721,\"start\":51714},{\"end\":52225,\"start\":52218},{\"end\":52239,\"start\":52231},{\"end\":52252,\"start\":52245},{\"end\":52264,\"start\":52258},{\"end\":52274,\"start\":52270},{\"end\":52288,\"start\":52280},{\"end\":52613,\"start\":52606},{\"end\":52627,\"start\":52619},{\"end\":52640,\"start\":52635},{\"end\":52650,\"start\":52646},{\"end\":52662,\"start\":52656},{\"end\":52664,\"start\":52663},{\"end\":53026,\"start\":53019},{\"end\":53037,\"start\":53032},{\"end\":53050,\"start\":53043},{\"end\":53066,\"start\":53057},{\"end\":53080,\"start\":53072},{\"end\":53093,\"start\":53086},{\"end\":53107,\"start\":53099},{\"end\":53945,\"start\":53936},{\"end\":53968,\"start\":53960},{\"end\":53985,\"start\":53979},{\"end\":54000,\"start\":53994},{\"end\":54014,\"start\":54008},{\"end\":54027,\"start\":54024},{\"end\":54040,\"start\":54036},{\"end\":54056,\"start\":54052},{\"end\":54913,\"start\":54906},{\"end\":54929,\"start\":54924},{\"end\":54942,\"start\":54938},{\"end\":54957,\"start\":54949},{\"end\":55600,\"start\":55593},{\"end\":55622,\"start\":55615},{\"end\":55639,\"start\":55632},{\"end\":55656,\"start\":55650},{\"end\":55669,\"start\":55664},{\"end\":55685,\"start\":55679},{\"end\":56613,\"start\":56606},{\"end\":56634,\"start\":56629},{\"end\":56648,\"start\":56642},{\"end\":56658,\"start\":56654},{\"end\":57494,\"start\":57487},{\"end\":57512,\"start\":57500},{\"end\":57524,\"start\":57517},{\"end\":57535,\"start\":57530},{\"end\":57547,\"start\":57542},{\"end\":57999,\"start\":57993},{\"end\":58007,\"start\":58005},{\"end\":58021,\"start\":58013},{\"end\":58034,\"start\":58027},{\"end\":58051,\"start\":58043},{\"end\":58062,\"start\":58058},{\"end\":58071,\"start\":58069},{\"end\":58078,\"start\":58076},{\"end\":58450,\"start\":58446},{\"end\":58464,\"start\":58460},{\"end\":58469,\"start\":58465},{\"end\":58480,\"start\":58475},{\"end\":58496,\"start\":58490},{\"end\":58512,\"start\":58505},{\"end\":58526,\"start\":58518},{\"end\":58542,\"start\":58536},{\"end\":58557,\"start\":58551},{\"end\":58572,\"start\":58566},{\"end\":58586,\"start\":58582},{\"end\":58602,\"start\":58594},{\"end\":58616,\"start\":58612},{\"end\":59416,\"start\":59410},{\"end\":59432,\"start\":59425},{\"end\":59448,\"start\":59441},{\"end\":59459,\"start\":59454},{\"end\":59473,\"start\":59466},{\"end\":59484,\"start\":59480},{\"end\":59498,\"start\":59494},{\"end\":59509,\"start\":59505},{\"end\":60265,\"start\":60264},{\"end\":60278,\"start\":60273},{\"end\":61104,\"start\":61100},{\"end\":61119,\"start\":61115},{\"end\":61124,\"start\":61120},{\"end\":61142,\"start\":61136},{\"end\":61156,\"start\":61150},{\"end\":61170,\"start\":61166},{\"end\":61960,\"start\":61955},{\"end\":61973,\"start\":61968},{\"end\":62416,\"start\":62411},{\"end\":62433,\"start\":62426},{\"end\":62452,\"start\":62445},{\"end\":62468,\"start\":62461},{\"end\":62481,\"start\":62476},{\"end\":63021,\"start\":63020},{\"end\":63039,\"start\":63031},{\"end\":63776,\"start\":63769},{\"end\":63807,\"start\":63800},{\"end\":63821,\"start\":63813},{\"end\":64069,\"start\":64062},{\"end\":64086,\"start\":64079},{\"end\":64100,\"start\":64093},{\"end\":64113,\"start\":64109},{\"end\":64121,\"start\":64118},{\"end\":64134,\"start\":64129},{\"end\":64837,\"start\":64828},{\"end\":64856,\"start\":64849},{\"end\":64870,\"start\":64864},{\"end\":64887,\"start\":64881},{\"end\":64908,\"start\":64901},{\"end\":64923,\"start\":64917},{\"end\":64935,\"start\":64931},{\"end\":64950,\"start\":64945},{\"end\":64963,\"start\":64959},{\"end\":65400,\"start\":65394},{\"end\":65414,\"start\":65407},{\"end\":65430,\"start\":65424},{\"end\":65432,\"start\":65431},{\"end\":65441,\"start\":65440},{\"end\":65443,\"start\":65442},{\"end\":65450,\"start\":65446},{\"end\":65466,\"start\":65460},{\"end\":66310,\"start\":66301},{\"end\":66326,\"start\":66318},{\"end\":66338,\"start\":66331},{\"end\":66357,\"start\":66348},{\"end\":66376,\"start\":66368},{\"end\":66391,\"start\":66385},{\"end\":66407,\"start\":66402},{\"end\":66984,\"start\":66978},{\"end\":66995,\"start\":66989},{\"end\":67004,\"start\":67001},{\"end\":67013,\"start\":67010},{\"end\":67023,\"start\":67018},{\"end\":67032,\"start\":67028},{\"end\":67044,\"start\":67038},{\"end\":67526,\"start\":67517},{\"end\":67543,\"start\":67536},{\"end\":67561,\"start\":67555},{\"end\":67577,\"start\":67569},{\"end\":67594,\"start\":67586},{\"end\":68164,\"start\":68161},{\"end\":68175,\"start\":68170},{\"end\":69198,\"start\":69191},{\"end\":69211,\"start\":69207},{\"end\":69222,\"start\":69219},{\"end\":69241,\"start\":69232},{\"end\":69254,\"start\":69249},{\"end\":69270,\"start\":69265},{\"end\":69285,\"start\":69278},{\"end\":69853,\"start\":69852},{\"end\":69862,\"start\":69854},{\"end\":69889,\"start\":69885},{\"end\":70428,\"start\":70423},{\"end\":70440,\"start\":70436},{\"end\":70458,\"start\":70452},{\"end\":70475,\"start\":70470},{\"end\":70853,\"start\":70845},{\"end\":70869,\"start\":70860},{\"end\":70883,\"start\":70876},{\"end\":70894,\"start\":70888},{\"end\":70904,\"start\":70899},{\"end\":70913,\"start\":70910},{\"end\":70926,\"start\":70919},{\"end\":70934,\"start\":70932},{\"end\":70946,\"start\":70940},{\"end\":71383,\"start\":71379},{\"end\":71392,\"start\":71390},{\"end\":71402,\"start\":71399},{\"end\":71415,\"start\":71408},{\"end\":71426,\"start\":71421},{\"end\":71439,\"start\":71432},{\"end\":71451,\"start\":71444},{\"end\":71461,\"start\":71456},{\"end\":71475,\"start\":71468},{\"end\":71489,\"start\":71482},{\"end\":72352,\"start\":72349},{\"end\":72367,\"start\":72359},{\"end\":72382,\"start\":72375},{\"end\":72398,\"start\":72390},{\"end\":72413,\"start\":72405},{\"end\":72955,\"start\":72947},{\"end\":72963,\"start\":72960},{\"end\":72972,\"start\":72968},{\"end\":72985,\"start\":72978},{\"end\":72992,\"start\":72991},{\"end\":72994,\"start\":72993},{\"end\":73009,\"start\":73003},{\"end\":73679,\"start\":73674},{\"end\":73691,\"start\":73686},{\"end\":73701,\"start\":73698},{\"end\":73710,\"start\":73708},{\"end\":73722,\"start\":73715},{\"end\":73736,\"start\":73731},{\"end\":73750,\"start\":73743},{\"end\":73764,\"start\":73757},{\"end\":73782,\"start\":73775},{\"end\":74412,\"start\":74407},{\"end\":74425,\"start\":74420},{\"end\":74436,\"start\":74431},{\"end\":74450,\"start\":74445},{\"end\":74887,\"start\":74883},{\"end\":74909,\"start\":74901},{\"end\":74928,\"start\":74919},{\"end\":74941,\"start\":74938},{\"end\":74957,\"start\":74952},{\"end\":75355,\"start\":75352},{\"end\":75369,\"start\":75362},{\"end\":75381,\"start\":75377},{\"end\":76163,\"start\":76154},{\"end\":76176,\"start\":76170},{\"end\":76191,\"start\":76184},{\"end\":76204,\"start\":76197},{\"end\":76218,\"start\":76211},{\"end\":76232,\"start\":76224},{\"end\":76244,\"start\":76237},{\"end\":76640,\"start\":76633},{\"end\":76655,\"start\":76647},{\"end\":76666,\"start\":76662},{\"end\":76673,\"start\":76667},{\"end\":76684,\"start\":76679},{\"end\":77024,\"start\":77013},{\"end\":77034,\"start\":77031},{\"end\":77047,\"start\":77041},{\"end\":77061,\"start\":77054}]", "bib_author_last_name": "[{\"end\":38664,\"start\":38657},{\"end\":38685,\"start\":38675},{\"end\":38699,\"start\":38691},{\"end\":38986,\"start\":38982},{\"end\":39001,\"start\":38998},{\"end\":39800,\"start\":39796},{\"end\":39810,\"start\":39806},{\"end\":39822,\"start\":39818},{\"end\":40138,\"start\":40135},{\"end\":40149,\"start\":40146},{\"end\":40159,\"start\":40157},{\"end\":40167,\"start\":40165},{\"end\":40181,\"start\":40177},{\"end\":40532,\"start\":40528},{\"end\":40540,\"start\":40536},{\"end\":40550,\"start\":40544},{\"end\":40559,\"start\":40557},{\"end\":40565,\"start\":40563},{\"end\":40576,\"start\":40569},{\"end\":40812,\"start\":40805},{\"end\":40828,\"start\":40819},{\"end\":40835,\"start\":40830},{\"end\":41454,\"start\":41444},{\"end\":41468,\"start\":41460},{\"end\":41485,\"start\":41474},{\"end\":41502,\"start\":41494},{\"end\":41515,\"start\":41511},{\"end\":41526,\"start\":41517},{\"end\":41698,\"start\":41692},{\"end\":41703,\"start\":41700},{\"end\":41891,\"start\":41884},{\"end\":41906,\"start\":41899},{\"end\":41921,\"start\":41916},{\"end\":41936,\"start\":41930},{\"end\":41950,\"start\":41943},{\"end\":41961,\"start\":41956},{\"end\":41975,\"start\":41970},{\"end\":41988,\"start\":41984},{\"end\":42375,\"start\":42371},{\"end\":42391,\"start\":42382},{\"end\":42403,\"start\":42393},{\"end\":43136,\"start\":43130},{\"end\":43150,\"start\":43142},{\"end\":43166,\"start\":43160},{\"end\":43187,\"start\":43174},{\"end\":43193,\"start\":43189},{\"end\":44151,\"start\":44145},{\"end\":44168,\"start\":44160},{\"end\":44188,\"start\":44177},{\"end\":44206,\"start\":44199},{\"end\":44223,\"start\":44213},{\"end\":45124,\"start\":45120},{\"end\":45141,\"start\":45133},{\"end\":45157,\"start\":45150},{\"end\":45513,\"start\":45511},{\"end\":45530,\"start\":45524},{\"end\":45541,\"start\":45537},{\"end\":45553,\"start\":45549},{\"end\":45568,\"start\":45564},{\"end\":45586,\"start\":45579},{\"end\":45598,\"start\":45593},{\"end\":46456,\"start\":46450},{\"end\":46469,\"start\":46464},{\"end\":47176,\"start\":47169},{\"end\":47186,\"start\":47183},{\"end\":47200,\"start\":47195},{\"end\":47216,\"start\":47209},{\"end\":47228,\"start\":47224},{\"end\":47244,\"start\":47237},{\"end\":47260,\"start\":47256},{\"end\":47279,\"start\":47269},{\"end\":47290,\"start\":47288},{\"end\":47306,\"start\":47300},{\"end\":47327,\"start\":47318},{\"end\":47339,\"start\":47332},{\"end\":47827,\"start\":47817},{\"end\":47844,\"start\":47838},{\"end\":47976,\"start\":47964},{\"end\":47989,\"start\":47983},{\"end\":48004,\"start\":47997},{\"end\":48443,\"start\":48441},{\"end\":48454,\"start\":48452},{\"end\":48469,\"start\":48464},{\"end\":48485,\"start\":48482},{\"end\":49282,\"start\":49279},{\"end\":49492,\"start\":49489},{\"end\":49507,\"start\":49502},{\"end\":49525,\"start\":49517},{\"end\":49537,\"start\":49533},{\"end\":49552,\"start\":49546},{\"end\":49566,\"start\":49559},{\"end\":49580,\"start\":49574},{\"end\":49600,\"start\":49593},{\"end\":50439,\"start\":50436},{\"end\":50451,\"start\":50448},{\"end\":50460,\"start\":50457},{\"end\":50468,\"start\":50466},{\"end\":50480,\"start\":50477},{\"end\":50493,\"start\":50488},{\"end\":50506,\"start\":50503},{\"end\":50519,\"start\":50516},{\"end\":51099,\"start\":51094},{\"end\":51114,\"start\":51108},{\"end\":51126,\"start\":51122},{\"end\":51142,\"start\":51137},{\"end\":51158,\"start\":51150},{\"end\":51171,\"start\":51166},{\"end\":51185,\"start\":51180},{\"end\":51203,\"start\":51195},{\"end\":51626,\"start\":51621},{\"end\":51641,\"start\":51635},{\"end\":51653,\"start\":51649},{\"end\":51669,\"start\":51664},{\"end\":51685,\"start\":51677},{\"end\":51698,\"start\":51693},{\"end\":51712,\"start\":51707},{\"end\":51730,\"start\":51722},{\"end\":52229,\"start\":52226},{\"end\":52243,\"start\":52240},{\"end\":52256,\"start\":52253},{\"end\":52268,\"start\":52265},{\"end\":52278,\"start\":52275},{\"end\":52294,\"start\":52289},{\"end\":52617,\"start\":52614},{\"end\":52633,\"start\":52628},{\"end\":52644,\"start\":52641},{\"end\":52654,\"start\":52651},{\"end\":52669,\"start\":52665},{\"end\":53030,\"start\":53027},{\"end\":53041,\"start\":53038},{\"end\":53055,\"start\":53051},{\"end\":53070,\"start\":53067},{\"end\":53084,\"start\":53081},{\"end\":53097,\"start\":53094},{\"end\":53113,\"start\":53108},{\"end\":53958,\"start\":53946},{\"end\":53977,\"start\":53969},{\"end\":53992,\"start\":53986},{\"end\":54006,\"start\":54001},{\"end\":54022,\"start\":54015},{\"end\":54034,\"start\":54028},{\"end\":54050,\"start\":54041},{\"end\":54061,\"start\":54057},{\"end\":54922,\"start\":54914},{\"end\":54936,\"start\":54930},{\"end\":54947,\"start\":54943},{\"end\":54961,\"start\":54958},{\"end\":55613,\"start\":55601},{\"end\":55630,\"start\":55623},{\"end\":55648,\"start\":55640},{\"end\":55662,\"start\":55657},{\"end\":55677,\"start\":55670},{\"end\":55690,\"start\":55686},{\"end\":56627,\"start\":56614},{\"end\":56640,\"start\":56635},{\"end\":56652,\"start\":56649},{\"end\":56666,\"start\":56659},{\"end\":56676,\"start\":56668},{\"end\":57498,\"start\":57495},{\"end\":57515,\"start\":57513},{\"end\":57528,\"start\":57525},{\"end\":57540,\"start\":57536},{\"end\":57550,\"start\":57548},{\"end\":58003,\"start\":58000},{\"end\":58011,\"start\":58008},{\"end\":58025,\"start\":58022},{\"end\":58041,\"start\":58035},{\"end\":58056,\"start\":58052},{\"end\":58067,\"start\":58063},{\"end\":58074,\"start\":58072},{\"end\":58081,\"start\":58079},{\"end\":58458,\"start\":58451},{\"end\":58473,\"start\":58470},{\"end\":58488,\"start\":58481},{\"end\":58503,\"start\":58497},{\"end\":58516,\"start\":58513},{\"end\":58534,\"start\":58527},{\"end\":58549,\"start\":58543},{\"end\":58564,\"start\":58558},{\"end\":58580,\"start\":58573},{\"end\":58592,\"start\":58587},{\"end\":58610,\"start\":58603},{\"end\":58626,\"start\":58617},{\"end\":59423,\"start\":59417},{\"end\":59439,\"start\":59433},{\"end\":59452,\"start\":59449},{\"end\":59464,\"start\":59460},{\"end\":59478,\"start\":59474},{\"end\":59492,\"start\":59485},{\"end\":59503,\"start\":59499},{\"end\":59519,\"start\":59510},{\"end\":60271,\"start\":60266},{\"end\":60285,\"start\":60279},{\"end\":60294,\"start\":60287},{\"end\":61113,\"start\":61105},{\"end\":61134,\"start\":61125},{\"end\":61148,\"start\":61143},{\"end\":61164,\"start\":61157},{\"end\":61177,\"start\":61171},{\"end\":61966,\"start\":61961},{\"end\":62424,\"start\":62417},{\"end\":62443,\"start\":62434},{\"end\":62459,\"start\":62453},{\"end\":62474,\"start\":62469},{\"end\":62487,\"start\":62482},{\"end\":63029,\"start\":63022},{\"end\":63045,\"start\":63040},{\"end\":63056,\"start\":63047},{\"end\":63798,\"start\":63777},{\"end\":63811,\"start\":63808},{\"end\":63825,\"start\":63822},{\"end\":63831,\"start\":63827},{\"end\":64077,\"start\":64070},{\"end\":64091,\"start\":64087},{\"end\":64107,\"start\":64101},{\"end\":64116,\"start\":64114},{\"end\":64127,\"start\":64122},{\"end\":64141,\"start\":64135},{\"end\":64847,\"start\":64838},{\"end\":64862,\"start\":64857},{\"end\":64879,\"start\":64871},{\"end\":64899,\"start\":64888},{\"end\":64915,\"start\":64909},{\"end\":64929,\"start\":64924},{\"end\":64943,\"start\":64936},{\"end\":64957,\"start\":64951},{\"end\":64975,\"start\":64964},{\"end\":65405,\"start\":65401},{\"end\":65422,\"start\":65415},{\"end\":65438,\"start\":65433},{\"end\":65458,\"start\":65451},{\"end\":65472,\"start\":65467},{\"end\":66316,\"start\":66311},{\"end\":66329,\"start\":66327},{\"end\":66346,\"start\":66339},{\"end\":66366,\"start\":66358},{\"end\":66383,\"start\":66377},{\"end\":66400,\"start\":66392},{\"end\":66413,\"start\":66408},{\"end\":66987,\"start\":66985},{\"end\":66999,\"start\":66996},{\"end\":67008,\"start\":67005},{\"end\":67016,\"start\":67014},{\"end\":67026,\"start\":67024},{\"end\":67036,\"start\":67033},{\"end\":67048,\"start\":67045},{\"end\":67534,\"start\":67527},{\"end\":67553,\"start\":67544},{\"end\":67567,\"start\":67562},{\"end\":67584,\"start\":67578},{\"end\":67600,\"start\":67595},{\"end\":68168,\"start\":68165},{\"end\":68182,\"start\":68176},{\"end\":69205,\"start\":69199},{\"end\":69217,\"start\":69212},{\"end\":69230,\"start\":69223},{\"end\":69247,\"start\":69242},{\"end\":69263,\"start\":69255},{\"end\":69276,\"start\":69271},{\"end\":69290,\"start\":69286},{\"end\":69883,\"start\":69863},{\"end\":69897,\"start\":69890},{\"end\":69905,\"start\":69899},{\"end\":70434,\"start\":70429},{\"end\":70450,\"start\":70441},{\"end\":70468,\"start\":70459},{\"end\":70482,\"start\":70476},{\"end\":70858,\"start\":70854},{\"end\":70874,\"start\":70870},{\"end\":70886,\"start\":70884},{\"end\":70897,\"start\":70895},{\"end\":70908,\"start\":70905},{\"end\":70917,\"start\":70914},{\"end\":70930,\"start\":70927},{\"end\":70938,\"start\":70935},{\"end\":70951,\"start\":70947},{\"end\":71388,\"start\":71384},{\"end\":71397,\"start\":71393},{\"end\":71406,\"start\":71403},{\"end\":71419,\"start\":71416},{\"end\":71430,\"start\":71427},{\"end\":71442,\"start\":71440},{\"end\":71454,\"start\":71452},{\"end\":71466,\"start\":71462},{\"end\":71480,\"start\":71476},{\"end\":71494,\"start\":71490},{\"end\":72357,\"start\":72353},{\"end\":72373,\"start\":72368},{\"end\":72388,\"start\":72383},{\"end\":72403,\"start\":72399},{\"end\":72422,\"start\":72414},{\"end\":72958,\"start\":72956},{\"end\":72966,\"start\":72964},{\"end\":72976,\"start\":72973},{\"end\":72989,\"start\":72986},{\"end\":73001,\"start\":72995},{\"end\":73013,\"start\":73010},{\"end\":73018,\"start\":73015},{\"end\":73684,\"start\":73680},{\"end\":73696,\"start\":73692},{\"end\":73706,\"start\":73702},{\"end\":73713,\"start\":73711},{\"end\":73729,\"start\":73723},{\"end\":73741,\"start\":73737},{\"end\":73755,\"start\":73751},{\"end\":73773,\"start\":73765},{\"end\":73788,\"start\":73783},{\"end\":74418,\"start\":74413},{\"end\":74429,\"start\":74426},{\"end\":74443,\"start\":74437},{\"end\":74462,\"start\":74451},{\"end\":74899,\"start\":74888},{\"end\":74917,\"start\":74910},{\"end\":74936,\"start\":74929},{\"end\":74950,\"start\":74942},{\"end\":74961,\"start\":74958},{\"end\":75360,\"start\":75356},{\"end\":75375,\"start\":75370},{\"end\":75384,\"start\":75382},{\"end\":76168,\"start\":76164},{\"end\":76182,\"start\":76177},{\"end\":76195,\"start\":76192},{\"end\":76209,\"start\":76205},{\"end\":76222,\"start\":76219},{\"end\":76235,\"start\":76233},{\"end\":76248,\"start\":76245},{\"end\":76645,\"start\":76641},{\"end\":76660,\"start\":76656},{\"end\":76677,\"start\":76674},{\"end\":76688,\"start\":76685},{\"end\":77029,\"start\":77025},{\"end\":77039,\"start\":77035},{\"end\":77052,\"start\":77048},{\"end\":77067,\"start\":77062}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":12726540},\"end\":38907,\"start\":38587},{\"attributes\":{\"doi\":\"10.1145/2964284.2964315\",\"id\":\"b1\",\"matched_paper_id\":207240186},\"end\":39716,\"start\":38909},{\"attributes\":{\"doi\":\"arXiv:1705.02743\",\"id\":\"b2\"},\"end\":40032,\"start\":39718},{\"attributes\":{\"doi\":\"10.48550/arXiv.2210.07688\",\"id\":\"b3\"},\"end\":40471,\"start\":40034},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":57246310},\"end\":40717,\"start\":40473},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":5923323},\"end\":41387,\"start\":40719},{\"attributes\":{\"id\":\"b6\"},\"end\":41686,\"start\":41389},{\"attributes\":{\"id\":\"b7\"},\"end\":41812,\"start\":41688},{\"attributes\":{\"id\":\"b8\"},\"end\":42304,\"start\":41814},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":235458570},\"end\":43055,\"start\":42306},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":233296711},\"end\":43705,\"start\":43057},{\"attributes\":{\"id\":\"b11\"},\"end\":44053,\"start\":43707},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":326772},\"end\":45046,\"start\":44055},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":204949374},\"end\":45457,\"start\":45048},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":254069914},\"end\":46339,\"start\":45459},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":14915460},\"end\":47070,\"start\":46341},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":4492210},\"end\":47755,\"start\":47072},{\"attributes\":{\"id\":\"b17\"},\"end\":47955,\"start\":47757},{\"attributes\":{\"id\":\"b18\"},\"end\":48326,\"start\":47957},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":246411402},\"end\":49212,\"start\":48328},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":964287},\"end\":49435,\"start\":49214},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":14113767},\"end\":50358,\"start\":49437},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":232352874},\"end\":50992,\"start\":50360},{\"attributes\":{\"id\":\"b23\"},\"end\":51518,\"start\":50994},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":7319196},\"end\":52128,\"start\":51520},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":3914807},\"end\":52602,\"start\":52130},{\"attributes\":{\"id\":\"b26\"},\"end\":52915,\"start\":52604},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":221112548},\"end\":53840,\"start\":52917},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":245335086},\"end\":54840,\"start\":53842},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":11080756},\"end\":55490,\"start\":54842},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":237251782},\"end\":56551,\"start\":55492},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":244906179},\"end\":57419,\"start\":56553},{\"attributes\":{\"id\":\"b32\"},\"end\":57702,\"start\":57421},{\"attributes\":{\"id\":\"b33\"},\"end\":57928,\"start\":57704},{\"attributes\":{\"id\":\"b34\"},\"end\":58373,\"start\":57930},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":231591445},\"end\":59372,\"start\":58375},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":232035663},\"end\":60197,\"start\":59374},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":166228599},\"end\":61056,\"start\":60199},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":52176506},\"end\":61918,\"start\":61058},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":11663659},\"end\":62347,\"start\":61920},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":245335280},\"end\":62948,\"start\":62349},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":250562849},\"end\":63727,\"start\":62950},{\"attributes\":{\"id\":\"b42\"},\"end\":63980,\"start\":63729},{\"attributes\":{\"id\":\"b43\"},\"end\":64754,\"start\":63982},{\"attributes\":{\"id\":\"b44\"},\"end\":65301,\"start\":64756},{\"attributes\":{\"id\":\"b45\"},\"end\":66240,\"start\":65303},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":244954250},\"end\":66908,\"start\":66242},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":201317624},\"end\":67456,\"start\":66910},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":206593880},\"end\":68084,\"start\":67458},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":201103729},\"end\":69103,\"start\":68086},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":248006414},\"end\":69797,\"start\":69105},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":9026666},\"end\":70330,\"start\":69799},{\"attributes\":{\"id\":\"b52\"},\"end\":70774,\"start\":70332},{\"attributes\":{\"id\":\"b53\"},\"end\":71268,\"start\":70776},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":246634906},\"end\":72292,\"start\":71270},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":206822288},\"end\":72892,\"start\":72294},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":234470115},\"end\":73609,\"start\":72894},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":247011309},\"end\":74287,\"start\":73611},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":3104920},\"end\":74792,\"start\":74289},{\"attributes\":{\"id\":\"b59\"},\"end\":75269,\"start\":74794},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":244129883},\"end\":76052,\"start\":75271},{\"attributes\":{\"id\":\"b61\"},\"end\":76584,\"start\":76054},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":237386023},\"end\":76943,\"start\":76586},{\"attributes\":{\"id\":\"b63\"},\"end\":77332,\"start\":76945}]", "bib_title": "[{\"end\":38649,\"start\":38587},{\"end\":38971,\"start\":38909},{\"end\":40524,\"start\":40473},{\"end\":40801,\"start\":40719},{\"end\":42364,\"start\":42306},{\"end\":43123,\"start\":43057},{\"end\":44136,\"start\":44055},{\"end\":45111,\"start\":45048},{\"end\":45504,\"start\":45459},{\"end\":46438,\"start\":46341},{\"end\":47160,\"start\":47072},{\"end\":48432,\"start\":48328},{\"end\":49268,\"start\":49214},{\"end\":49478,\"start\":49437},{\"end\":50431,\"start\":50360},{\"end\":51612,\"start\":51520},{\"end\":52216,\"start\":52130},{\"end\":53017,\"start\":52917},{\"end\":53934,\"start\":53842},{\"end\":54904,\"start\":54842},{\"end\":55591,\"start\":55492},{\"end\":56604,\"start\":56553},{\"end\":58444,\"start\":58375},{\"end\":59408,\"start\":59374},{\"end\":60262,\"start\":60199},{\"end\":61098,\"start\":61058},{\"end\":61953,\"start\":61920},{\"end\":62409,\"start\":62349},{\"end\":63018,\"start\":62950},{\"end\":65392,\"start\":65303},{\"end\":66299,\"start\":66242},{\"end\":66976,\"start\":66910},{\"end\":67515,\"start\":67458},{\"end\":68159,\"start\":68086},{\"end\":69189,\"start\":69105},{\"end\":69850,\"start\":69799},{\"end\":71377,\"start\":71270},{\"end\":72347,\"start\":72294},{\"end\":72945,\"start\":72894},{\"end\":73672,\"start\":73611},{\"end\":74405,\"start\":74289},{\"end\":75350,\"start\":75271},{\"end\":76631,\"start\":76586}]", "bib_author": "[{\"end\":38666,\"start\":38651},{\"end\":38687,\"start\":38666},{\"end\":38701,\"start\":38687},{\"end\":38988,\"start\":38973},{\"end\":39003,\"start\":38988},{\"end\":39802,\"start\":39792},{\"end\":39812,\"start\":39802},{\"end\":39824,\"start\":39812},{\"end\":40140,\"start\":40126},{\"end\":40151,\"start\":40140},{\"end\":40161,\"start\":40151},{\"end\":40169,\"start\":40161},{\"end\":40183,\"start\":40169},{\"end\":40534,\"start\":40526},{\"end\":40542,\"start\":40534},{\"end\":40552,\"start\":40542},{\"end\":40561,\"start\":40552},{\"end\":40567,\"start\":40561},{\"end\":40578,\"start\":40567},{\"end\":40814,\"start\":40803},{\"end\":40830,\"start\":40814},{\"end\":40837,\"start\":40830},{\"end\":41456,\"start\":41439},{\"end\":41470,\"start\":41456},{\"end\":41487,\"start\":41470},{\"end\":41504,\"start\":41487},{\"end\":41517,\"start\":41504},{\"end\":41528,\"start\":41517},{\"end\":41700,\"start\":41690},{\"end\":41705,\"start\":41700},{\"end\":41893,\"start\":41878},{\"end\":41908,\"start\":41893},{\"end\":41923,\"start\":41908},{\"end\":41938,\"start\":41923},{\"end\":41952,\"start\":41938},{\"end\":41963,\"start\":41952},{\"end\":41977,\"start\":41963},{\"end\":41990,\"start\":41977},{\"end\":42377,\"start\":42366},{\"end\":42393,\"start\":42377},{\"end\":42405,\"start\":42393},{\"end\":43138,\"start\":43125},{\"end\":43152,\"start\":43138},{\"end\":43168,\"start\":43152},{\"end\":43189,\"start\":43168},{\"end\":43195,\"start\":43189},{\"end\":44153,\"start\":44138},{\"end\":44170,\"start\":44153},{\"end\":44190,\"start\":44170},{\"end\":44208,\"start\":44190},{\"end\":44225,\"start\":44208},{\"end\":45126,\"start\":45113},{\"end\":45143,\"start\":45126},{\"end\":45159,\"start\":45143},{\"end\":45515,\"start\":45506},{\"end\":45532,\"start\":45515},{\"end\":45543,\"start\":45532},{\"end\":45555,\"start\":45543},{\"end\":45570,\"start\":45555},{\"end\":45588,\"start\":45570},{\"end\":45600,\"start\":45588},{\"end\":46458,\"start\":46440},{\"end\":46471,\"start\":46458},{\"end\":47178,\"start\":47162},{\"end\":47188,\"start\":47178},{\"end\":47202,\"start\":47188},{\"end\":47218,\"start\":47202},{\"end\":47230,\"start\":47218},{\"end\":47246,\"start\":47230},{\"end\":47262,\"start\":47246},{\"end\":47281,\"start\":47262},{\"end\":47292,\"start\":47281},{\"end\":47308,\"start\":47292},{\"end\":47329,\"start\":47308},{\"end\":47341,\"start\":47329},{\"end\":47829,\"start\":47812},{\"end\":47846,\"start\":47829},{\"end\":47978,\"start\":47957},{\"end\":47991,\"start\":47978},{\"end\":48006,\"start\":47991},{\"end\":48445,\"start\":48434},{\"end\":48456,\"start\":48445},{\"end\":48471,\"start\":48456},{\"end\":48487,\"start\":48471},{\"end\":49284,\"start\":49270},{\"end\":49494,\"start\":49480},{\"end\":49509,\"start\":49494},{\"end\":49527,\"start\":49509},{\"end\":49539,\"start\":49527},{\"end\":49554,\"start\":49539},{\"end\":49568,\"start\":49554},{\"end\":49582,\"start\":49568},{\"end\":49602,\"start\":49582},{\"end\":50441,\"start\":50433},{\"end\":50453,\"start\":50441},{\"end\":50462,\"start\":50453},{\"end\":50470,\"start\":50462},{\"end\":50482,\"start\":50470},{\"end\":50495,\"start\":50482},{\"end\":50508,\"start\":50495},{\"end\":50521,\"start\":50508},{\"end\":51101,\"start\":51087},{\"end\":51116,\"start\":51101},{\"end\":51128,\"start\":51116},{\"end\":51144,\"start\":51128},{\"end\":51160,\"start\":51144},{\"end\":51173,\"start\":51160},{\"end\":51187,\"start\":51173},{\"end\":51205,\"start\":51187},{\"end\":51628,\"start\":51614},{\"end\":51643,\"start\":51628},{\"end\":51655,\"start\":51643},{\"end\":51671,\"start\":51655},{\"end\":51687,\"start\":51671},{\"end\":51700,\"start\":51687},{\"end\":51714,\"start\":51700},{\"end\":51732,\"start\":51714},{\"end\":52231,\"start\":52218},{\"end\":52245,\"start\":52231},{\"end\":52258,\"start\":52245},{\"end\":52270,\"start\":52258},{\"end\":52280,\"start\":52270},{\"end\":52296,\"start\":52280},{\"end\":52619,\"start\":52606},{\"end\":52635,\"start\":52619},{\"end\":52646,\"start\":52635},{\"end\":52656,\"start\":52646},{\"end\":52671,\"start\":52656},{\"end\":53032,\"start\":53019},{\"end\":53043,\"start\":53032},{\"end\":53057,\"start\":53043},{\"end\":53072,\"start\":53057},{\"end\":53086,\"start\":53072},{\"end\":53099,\"start\":53086},{\"end\":53115,\"start\":53099},{\"end\":53960,\"start\":53936},{\"end\":53979,\"start\":53960},{\"end\":53994,\"start\":53979},{\"end\":54008,\"start\":53994},{\"end\":54024,\"start\":54008},{\"end\":54036,\"start\":54024},{\"end\":54052,\"start\":54036},{\"end\":54063,\"start\":54052},{\"end\":54924,\"start\":54906},{\"end\":54938,\"start\":54924},{\"end\":54949,\"start\":54938},{\"end\":54963,\"start\":54949},{\"end\":55615,\"start\":55593},{\"end\":55632,\"start\":55615},{\"end\":55650,\"start\":55632},{\"end\":55664,\"start\":55650},{\"end\":55679,\"start\":55664},{\"end\":55692,\"start\":55679},{\"end\":56629,\"start\":56606},{\"end\":56642,\"start\":56629},{\"end\":56654,\"start\":56642},{\"end\":56668,\"start\":56654},{\"end\":56678,\"start\":56668},{\"end\":57500,\"start\":57487},{\"end\":57517,\"start\":57500},{\"end\":57530,\"start\":57517},{\"end\":57542,\"start\":57530},{\"end\":57552,\"start\":57542},{\"end\":58005,\"start\":57993},{\"end\":58013,\"start\":58005},{\"end\":58027,\"start\":58013},{\"end\":58043,\"start\":58027},{\"end\":58058,\"start\":58043},{\"end\":58069,\"start\":58058},{\"end\":58076,\"start\":58069},{\"end\":58083,\"start\":58076},{\"end\":58460,\"start\":58446},{\"end\":58475,\"start\":58460},{\"end\":58490,\"start\":58475},{\"end\":58505,\"start\":58490},{\"end\":58518,\"start\":58505},{\"end\":58536,\"start\":58518},{\"end\":58551,\"start\":58536},{\"end\":58566,\"start\":58551},{\"end\":58582,\"start\":58566},{\"end\":58594,\"start\":58582},{\"end\":58612,\"start\":58594},{\"end\":58628,\"start\":58612},{\"end\":59425,\"start\":59410},{\"end\":59441,\"start\":59425},{\"end\":59454,\"start\":59441},{\"end\":59466,\"start\":59454},{\"end\":59480,\"start\":59466},{\"end\":59494,\"start\":59480},{\"end\":59505,\"start\":59494},{\"end\":59521,\"start\":59505},{\"end\":60273,\"start\":60264},{\"end\":60287,\"start\":60273},{\"end\":60296,\"start\":60287},{\"end\":61115,\"start\":61100},{\"end\":61136,\"start\":61115},{\"end\":61150,\"start\":61136},{\"end\":61166,\"start\":61150},{\"end\":61179,\"start\":61166},{\"end\":61968,\"start\":61955},{\"end\":61976,\"start\":61968},{\"end\":62426,\"start\":62411},{\"end\":62445,\"start\":62426},{\"end\":62461,\"start\":62445},{\"end\":62476,\"start\":62461},{\"end\":62489,\"start\":62476},{\"end\":63031,\"start\":63020},{\"end\":63047,\"start\":63031},{\"end\":63058,\"start\":63047},{\"end\":63800,\"start\":63769},{\"end\":63813,\"start\":63800},{\"end\":63827,\"start\":63813},{\"end\":63833,\"start\":63827},{\"end\":64079,\"start\":64062},{\"end\":64093,\"start\":64079},{\"end\":64109,\"start\":64093},{\"end\":64118,\"start\":64109},{\"end\":64129,\"start\":64118},{\"end\":64143,\"start\":64129},{\"end\":64849,\"start\":64828},{\"end\":64864,\"start\":64849},{\"end\":64881,\"start\":64864},{\"end\":64901,\"start\":64881},{\"end\":64917,\"start\":64901},{\"end\":64931,\"start\":64917},{\"end\":64945,\"start\":64931},{\"end\":64959,\"start\":64945},{\"end\":64977,\"start\":64959},{\"end\":65407,\"start\":65394},{\"end\":65424,\"start\":65407},{\"end\":65440,\"start\":65424},{\"end\":65446,\"start\":65440},{\"end\":65460,\"start\":65446},{\"end\":65474,\"start\":65460},{\"end\":66318,\"start\":66301},{\"end\":66331,\"start\":66318},{\"end\":66348,\"start\":66331},{\"end\":66368,\"start\":66348},{\"end\":66385,\"start\":66368},{\"end\":66402,\"start\":66385},{\"end\":66415,\"start\":66402},{\"end\":66989,\"start\":66978},{\"end\":67001,\"start\":66989},{\"end\":67010,\"start\":67001},{\"end\":67018,\"start\":67010},{\"end\":67028,\"start\":67018},{\"end\":67038,\"start\":67028},{\"end\":67050,\"start\":67038},{\"end\":67536,\"start\":67517},{\"end\":67555,\"start\":67536},{\"end\":67569,\"start\":67555},{\"end\":67586,\"start\":67569},{\"end\":67602,\"start\":67586},{\"end\":68170,\"start\":68161},{\"end\":68184,\"start\":68170},{\"end\":69207,\"start\":69191},{\"end\":69219,\"start\":69207},{\"end\":69232,\"start\":69219},{\"end\":69249,\"start\":69232},{\"end\":69265,\"start\":69249},{\"end\":69278,\"start\":69265},{\"end\":69292,\"start\":69278},{\"end\":69885,\"start\":69852},{\"end\":69899,\"start\":69885},{\"end\":69907,\"start\":69899},{\"end\":70436,\"start\":70423},{\"end\":70452,\"start\":70436},{\"end\":70470,\"start\":70452},{\"end\":70484,\"start\":70470},{\"end\":70860,\"start\":70845},{\"end\":70876,\"start\":70860},{\"end\":70888,\"start\":70876},{\"end\":70899,\"start\":70888},{\"end\":70910,\"start\":70899},{\"end\":70919,\"start\":70910},{\"end\":70932,\"start\":70919},{\"end\":70940,\"start\":70932},{\"end\":70953,\"start\":70940},{\"end\":71390,\"start\":71379},{\"end\":71399,\"start\":71390},{\"end\":71408,\"start\":71399},{\"end\":71421,\"start\":71408},{\"end\":71432,\"start\":71421},{\"end\":71444,\"start\":71432},{\"end\":71456,\"start\":71444},{\"end\":71468,\"start\":71456},{\"end\":71482,\"start\":71468},{\"end\":71496,\"start\":71482},{\"end\":72359,\"start\":72349},{\"end\":72375,\"start\":72359},{\"end\":72390,\"start\":72375},{\"end\":72405,\"start\":72390},{\"end\":72424,\"start\":72405},{\"end\":72960,\"start\":72947},{\"end\":72968,\"start\":72960},{\"end\":72978,\"start\":72968},{\"end\":72991,\"start\":72978},{\"end\":73003,\"start\":72991},{\"end\":73015,\"start\":73003},{\"end\":73020,\"start\":73015},{\"end\":73686,\"start\":73674},{\"end\":73698,\"start\":73686},{\"end\":73708,\"start\":73698},{\"end\":73715,\"start\":73708},{\"end\":73731,\"start\":73715},{\"end\":73743,\"start\":73731},{\"end\":73757,\"start\":73743},{\"end\":73775,\"start\":73757},{\"end\":73790,\"start\":73775},{\"end\":74420,\"start\":74407},{\"end\":74431,\"start\":74420},{\"end\":74445,\"start\":74431},{\"end\":74464,\"start\":74445},{\"end\":74901,\"start\":74883},{\"end\":74919,\"start\":74901},{\"end\":74938,\"start\":74919},{\"end\":74952,\"start\":74938},{\"end\":74963,\"start\":74952},{\"end\":75362,\"start\":75352},{\"end\":75377,\"start\":75362},{\"end\":75386,\"start\":75377},{\"end\":76170,\"start\":76154},{\"end\":76184,\"start\":76170},{\"end\":76197,\"start\":76184},{\"end\":76211,\"start\":76197},{\"end\":76224,\"start\":76211},{\"end\":76237,\"start\":76224},{\"end\":76250,\"start\":76237},{\"end\":76647,\"start\":76633},{\"end\":76662,\"start\":76647},{\"end\":76679,\"start\":76662},{\"end\":76690,\"start\":76679},{\"end\":77031,\"start\":77013},{\"end\":77041,\"start\":77031},{\"end\":77054,\"start\":77041},{\"end\":77069,\"start\":77054}]", "bib_venue": "[{\"end\":39296,\"start\":39177},{\"end\":41003,\"start\":40926},{\"end\":43372,\"start\":43283},{\"end\":44466,\"start\":44431},{\"end\":45832,\"start\":45730},{\"end\":46612,\"start\":46593},{\"end\":48676,\"start\":48652},{\"end\":49811,\"start\":49751},{\"end\":50639,\"start\":50619},{\"end\":53339,\"start\":53323},{\"end\":54239,\"start\":54215},{\"end\":55173,\"start\":55075},{\"end\":55953,\"start\":55866},{\"end\":56959,\"start\":56845},{\"end\":57750,\"start\":57739},{\"end\":58804,\"start\":58740},{\"end\":59708,\"start\":59644},{\"end\":60547,\"start\":60482},{\"end\":61441,\"start\":61353},{\"end\":62048,\"start\":62034},{\"end\":62612,\"start\":62592},{\"end\":63261,\"start\":63235},{\"end\":65720,\"start\":65628},{\"end\":66538,\"start\":66518},{\"end\":67129,\"start\":67108},{\"end\":67717,\"start\":67699},{\"end\":68580,\"start\":68419},{\"end\":69415,\"start\":69395},{\"end\":70007,\"start\":69992},{\"end\":71685,\"start\":71661},{\"end\":72550,\"start\":72538},{\"end\":73211,\"start\":73206},{\"end\":73913,\"start\":73893},{\"end\":75562,\"start\":75538},{\"end\":38739,\"start\":38701},{\"end\":39098,\"start\":39026},{\"end\":39790,\"start\":39718},{\"end\":40124,\"start\":40034},{\"end\":40584,\"start\":40578},{\"end\":40924,\"start\":40856},{\"end\":41437,\"start\":41389},{\"end\":41876,\"start\":41814},{\"end\":42512,\"start\":42438},{\"end\":43281,\"start\":43195},{\"end\":43779,\"start\":43738},{\"end\":44337,\"start\":44225},{\"end\":45223,\"start\":45185},{\"end\":45686,\"start\":45600},{\"end\":46534,\"start\":46498},{\"end\":47385,\"start\":47366},{\"end\":47810,\"start\":47757},{\"end\":48120,\"start\":48022},{\"end\":48559,\"start\":48504},{\"end\":49315,\"start\":49284},{\"end\":49684,\"start\":49630},{\"end\":50617,\"start\":50549},{\"end\":51085,\"start\":50994},{\"end\":51796,\"start\":51758},{\"end\":52338,\"start\":52320},{\"end\":52730,\"start\":52686},{\"end\":53212,\"start\":53138},{\"end\":54122,\"start\":54067},{\"end\":55073,\"start\":54986},{\"end\":55808,\"start\":55721},{\"end\":56807,\"start\":56678},{\"end\":57485,\"start\":57421},{\"end\":57737,\"start\":57704},{\"end\":57991,\"start\":57930},{\"end\":58711,\"start\":58632},{\"end\":59615,\"start\":59536},{\"end\":60408,\"start\":60296},{\"end\":61285,\"start\":61199},{\"end\":62032,\"start\":61976},{\"end\":62590,\"start\":62517},{\"end\":63161,\"start\":63092},{\"end\":63767,\"start\":63729},{\"end\":64060,\"start\":63982},{\"end\":64826,\"start\":64756},{\"end\":65591,\"start\":65505},{\"end\":66516,\"start\":66443},{\"end\":67106,\"start\":67050},{\"end\":67697,\"start\":67623},{\"end\":68364,\"start\":68204},{\"end\":69393,\"start\":69320},{\"end\":69990,\"start\":69932},{\"end\":70421,\"start\":70332},{\"end\":70843,\"start\":70776},{\"end\":71568,\"start\":71513},{\"end\":72536,\"start\":72450},{\"end\":73091,\"start\":73043},{\"end\":73891,\"start\":73818},{\"end\":74517,\"start\":74484},{\"end\":74881,\"start\":74794},{\"end\":75445,\"start\":75390},{\"end\":76152,\"start\":76054},{\"end\":76735,\"start\":76716},{\"end\":77011,\"start\":76945}]"}}}, "year": 2023, "month": 12, "day": 17}