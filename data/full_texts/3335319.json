{"id": 3335319, "updated": "2023-11-08 04:32:03.146", "metadata": {"title": "Energy Management Strategy for a Hybrid Electric Vehicle Based on Deep Reinforcement Learning", "authors": "[{\"first\":\"Yue\",\"last\":\"Hu\",\"middle\":[]},{\"first\":\"Weimin\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Kun\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Taimoor\",\"last\":\"Zahid\",\"middle\":[]},{\"first\":\"Feiyan\",\"last\":\"Qin\",\"middle\":[]},{\"first\":\"Chenming\",\"last\":\"Li\",\"middle\":[]}]", "venue": null, "journal": "Applied Sciences", "publication_date": {"year": 2018, "month": null, "day": null}, "abstract": ": An energy management strategy (EMS) is important for hybrid electric vehicles (HEVs) since it plays a decisive role on the performance of the vehicle. However, the variation of future driving conditions deeply in\ufb02uences the effectiveness of the EMS. Most existing EMS methods simply follow prede\ufb01ned rules that are not adaptive to different driving conditions online. Therefore, it is useful that the EMS can learn from the environment or driving cycle. In this paper, a deep reinforcement learning (DRL)-based EMS is designed such that it can learn to select actions directly from the states without any prediction or prede\ufb01ned rules. Furthermore, a DRL-based online learning architecture is presented. It is signi\ufb01cant for applying the DRL algorithm in HEV energy management under different driving conditions. Simulation experiments have been conducted using MATLAB and Advanced Vehicle Simulator (ADVISOR) co-simulation. Experimental results validate the effectiveness of the DRL-based EMS compared with the rule-based EMS in terms of fuel economy. The online learning architecture is also proved to be effective. The proposed method ensures the optimality, as well as real-time applicability, in HEVs.", "fields_of_study": "[\"Engineering\"]", "external_ids": {"arxiv": null, "mag": "2793435266", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": null, "doi": "10.3390/app8020187"}}, "content": {"source": {"pdf_hash": "aba5d60206831ec8cc4083c45209fd030ff68eae", "pdf_src": "Anansi", "pdf_uri": "[\"http://www.mdpi.com/2076-3417/8/2/187/pdf\"]", "oa_url_match": true, "oa_info": {"license": "CCBY", "open_access_url": "https://www.mdpi.com/2076-3417/8/2/187/pdf?version=1517189255", "status": "GOLD"}}, "grobid": {"id": "56adf4015b7c4cfc0f9bd0de68f014ea72e6030e", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/aba5d60206831ec8cc4083c45209fd030ff68eae.txt", "contents": "\nEnergy Management Strategy for a Hybrid Electric Vehicle Based on Deep Reinforcement Learning\n26 January 2018\n\nYue Hu yue.hu@siat.ac.cn \nShenzhen Institutes of Advanced Technology\nChinese Academy of Sciences\n518055ShenzhenChina\n\nShenzhen College of Advanced Technology\nUniversity of Chinese Academy of Sciences\n518055ShenzhenChina\n\nJining Institutes of Advanced Technology\nChinese Academy of Sciences\n272000JiningChina\n\nWeimin Li \nShenzhen Institutes of Advanced Technology\nChinese Academy of Sciences\n518055ShenzhenChina\n\nJining Institutes of Advanced Technology\nChinese Academy of Sciences\n272000JiningChina\n\nDepartment of Mechanical and Automation Engineering\nThe Chinese University of Hong Kong\n999077Hong KongChina\n\nKun Xu kun.xu@siat.ac.cn \nShenzhen Institutes of Advanced Technology\nChinese Academy of Sciences\n518055ShenzhenChina\n\nTaimoor Zahid taimoor.z@siat.ac.cn \nShenzhen Institutes of Advanced Technology\nChinese Academy of Sciences\n518055ShenzhenChina\n\nShenzhen College of Advanced Technology\nUniversity of Chinese Academy of Sciences\n518055ShenzhenChina\n\nFeiyan Qin fy.qin@siat.ac.cn \nShenzhen Institutes of Advanced Technology\nChinese Academy of Sciences\n518055ShenzhenChina\n\nShenzhen College of Advanced Technology\nUniversity of Chinese Academy of Sciences\n518055ShenzhenChina\n\nChenming Li \nDepartment of Electronic Engineering\nThe Chinese University of Hong Kong\n999077Hong KongChina\n\nEnergy Management Strategy for a Hybrid Electric Vehicle Based on Deep Reinforcement Learning\n26 January 20180A21F5856D7F338B92FE66AA01CC217E10.3390/app8020187Received: 28 December 2017; Accepted: 24 January 2018;hybrid electric vehicleenergy management strategydeep reinforcement learningonline learning\nAn energy management strategy (EMS) is important for hybrid electric vehicles (HEVs) since it plays a decisive role on the performance of the vehicle.However, the variation of future driving conditions deeply influences the effectiveness of the EMS.Most existing EMS methods simply follow predefined rules that are not adaptive to different driving conditions online.Therefore, it is useful that the EMS can learn from the environment or driving cycle.In this paper, a deep reinforcement learning (DRL)-based EMS is designed such that it can learn to select actions directly from the states without any prediction or predefined rules.Furthermore, a DRL-based online learning architecture is presented.It is significant for applying the DRL algorithm in HEV energy management under different driving conditions.Simulation experiments have been conducted using MATLAB and Advanced Vehicle Simulator (ADVISOR) co-simulation.Experimental results validate the effectiveness of the DRL-based EMS compared with the rule-based EMS in terms of fuel economy.The online learning architecture is also proved to be effective.The proposed method ensures the optimality, as well as real-time applicability, in HEVs.\n\nIntroduction\n\nAn energy management strategy (EMS) is one of the key technologies for hybrid electric vehicles (HEVs) due to its decisive effect on the performance of the vehicle [1].The EMS for HEVs has been a very active research field during the past decades.However, how to design a highly-efficient and adaptive EMS is still a challenging task due to the complex structure of HEVs and the uncertain driving cycle.\n\nThe existing EMS methods can be generally classified into the following three categories: (1) Rule-based EMS, such as the thermostatic strategy, the load following strategy, and electric assist strategy [2,3].These methods rely heavily on the results of extensive experimental trials and human expertise without the a priori knowledge of the driving conditions [4].Other related control strategies employ heuristic control techniques, with the resultant strategies formalized as fuzzy rules [5,6].Though these rule-based strategies are effective and can be easily implemented, their optimality and flexibility are critically limited by working conditions and, consequently, are not adaptive to different driving cycles.(2) Optimization-based EMS: some optimization methods employed in control strategy are either based on the known driving cycles or predicted future driving conditions, such as dynamic programming (DP) [7][8][9], sequential quadratic programming (SQP), genetic algorithms (GA) [10], the Pontryagin minimum principle (PMP) [11], and so on.Usually, these algorithms can manage to determine the optimal power split between the engine and the motor for a particular driving cycle.However, the obtained optimal power-split solutions are only optimal with respect to a specific driving cycle.In general, it is neither optimal nor charge-sustaining for other cycles.Unless future driving conditions can be predicted during real-time operation, there is no way to imply these control laws directly.Moreover, these methods suffer from the \"curse of dimensionality\" problem, which prevents their wide adoption in real-time applications.Model predictive control (MPC) [12] is another type of optimization-based method.The optimal control problem in the finite domain is solved at each sampling instant and control actions are obtained based on online rolling optimization.This method has the advantages of good control effect and strong robustness.(3) Learning-based EMS: some strategies can learn from the historical data or use the previous driving data for online learning or application [13,14].Some researchers propose that traffic information and cloud computing in intelligent transportation systems (ITSs) can enhance HEV energy management since vehicles obtain real-time data via intelligent infrastructures or connected vehicles [15,16].Regardless of the learning from historical data or predicted data, these EMS methods also need complex control models and professional knowledge from experts.Thus, these EMS methods are not end-to-end control methods.Reinforcement learning-based control methods have also been used for HEV energy management [17,18].However, reinforcement learning must be able to learn from a scalar reward signal that is frequently sparse, noisy, and delayed.Additionally, the sequence of highly-correlated states is also a large problem of reinforcement learning, in addition to the data distribution changes, as the algorithm learns new behaviors in reinforcement learning.\n\nThe learning-based EMS is an emerging and promising method because of its potential ability of self-adaption according to different driving conditions, even if there are still some problems.In our previous research, online learning control strategies based on neural dynamic programming (NDP) [19], fuzzy Q-learning (FQL) [20], were proposed.These strategies do not rely on prior information related to future driving conditions and can self-tune the parameters of the algorithms.A back propagation (BP) neural network was used to estimate the Q-value which, in turn, tuned the parameter of fuzzy controller [20].However, it also requires designing the fuzzy controller, as well as professional knowledge.\n\nDeep reinforcement learning (DRL) has shown successful performance in playing Atari [21] and Go games [22] in recent years.The DRL method is a powerful algorithm to solve complex control problems and handle large state spaces by establishing a deep neural network to relate the value estimation and associated state-action pairs.As a result, the DRL algorithm has been quickly applied in robotics [23], building HVAC control [24], ramp metering [25], and other fields.In the automotive field, DRL has been used for lane keeping assist [26], autonomous braking system [27], and autonomous vehicles [28].However, motion control of autonomous vehicles needs very high precision from our perspective.The mechanism of DRL has not been explained very deeply and may not meet this high requirement.\n\nNevertheless, DRL is a powerful technique that can be used in HEV EMS in this research as it concerns the fuel economy compared to the control precision.A DRL-based EMS has been designed for plug-in hybrid electric vehicles (PHEVs) [29].This is the first time DRL has been applied to a PHEV EMS.However, there are several problems in this study: (1) The learning process is still offline, which means that the trained deep network can only work well in the same driving cycle, but would not be able to obtain good performance in other driving conditions.As a result, this method can be used in buses with fixed route however it is not acceptable for vehicles with route variation; (2) The immediate reward is important as it affects the performance of DRL.The optimization objective is the vehicle fuel economy, but the reward is a function based on the power supply from the engine.The relationship between fuel economy and engine power is complex and the paper lacks the ability to justify this phenomena; (3) The structure of deep neural network can be well designed by fixing the Q targets network, which can make the algorithm more stable.\n\nIn this research, an energy management strategy based on deep reinforcement learning is proposed.Our work achieves good performance and high scalability by (1) building the system model of the HEV and formulating the HEV energy management problem; (2) developing a DRL-based control framework and an online learning architecture for a HEV EMS, which is adapted to different driving conditions; and (3) facilitating algorithm training and evaluation in the simulation environment.Figure 1 illustrates our DRL-based algorithm for HEV EMS.The DRL-based EMS can autonomously learn the optimal policy based on data inputs, without any prediction or predefined rules.For training and validation, we use the HEV model built in ADVISOR software (National Renewable Energy Laboratory, Golden, CO, USA).Simulation results reveal that the algorithm is able to improve the fuel economy while meeting other requirements, such as dynamic performance and vehicle drivability.the engine.The relationship between fuel economy and engine power is complex and the paper lacks the ability to justify this phenomena; (3) The structure of deep neural network can be well designed by fixing the Q targets network, which can make the algorithm more stable.\n\nIn this research, an energy management strategy based on deep reinforcement learning is proposed.Our work achieves good performance and high scalability by (1) building the system model of the HEV and formulating the HEV energy management problem; (2) developing a DRL-based control framework and an online learning architecture for a HEV EMS, which is adapted to different driving conditions; and (3) facilitating algorithm training and evaluation in the simulation environment.Figure 1 illustrates our DRL-based algorithm for HEV EMS.The DRL-based EMS can autonomously learn the optimal policy based on data inputs, without any prediction or predefined rules.For training and validation, we use the HEV model built in ADVISOR software (National Renewable Energy Laboratory, Golden, CO, USA).Simulation results reveal that the algorithm is able to improve the fuel economy while meeting other requirements, such as dynamic performance and vehicle drivability.The proposed DRL-based EMS uses a fixed target Q network which can make the algorithm more stable.The immediate reward is a function directly related to fuel consumption.More importantly, a DRL-based online learning architecture is presented.It is a critical factor to apply the DRL algorithm in HEV energy management under different driving conditions.\n\n\nAction value error\nt s t a Q Q S a ' S r ) , , (\nThe rest of this paper is organized as follows: Section 2 introduces the system model of HEV and describes the mathematics formulation of HEV EMS.Section 3 explains our deep reinforcement learning-based control strategy, including offline learning and online learning application.The experimental results are given in Section 4, followed by the conclusions in Section 5.\n\n\nProblem Formulation\n\nThe prototype vehicle is a single-axis parallel HEV, the drivetrain structure of which is shown in Figure 2. The drivetrain integrates an engine, an electric traction motor/generator, Ni-Hi batteries, an automatic clutch, and an automatic/manual transmission system.The motor is directly linked between the auto clutch output and the transmission input.This architecture provides the regenerative braking during deceleration and allows an efficient motor assist operation.To provide pure electrical propulsion, the engine can be disconnected from the drivetrain by the automatic The proposed DRL-based EMS uses a fixed target Q network which can make the algorithm more stable.The immediate reward is a function directly related to fuel consumption.More importantly, a DRL-based online learning architecture is presented.It is a critical factor to apply the DRL algorithm in HEV energy management under different driving conditions.\n\nThe rest of this paper is organized as follows: Section 2 introduces the system model of HEV and describes the mathematics formulation of HEV EMS.Section 3 explains our deep reinforcement learning-based control strategy, including offline learning and online learning application.The experimental results are given in Section 4, followed by the conclusions in Section 5.\n\n\nProblem Formulation\n\nThe prototype vehicle is a single-axis parallel HEV, the drivetrain structure of which is shown in Figure 2. The drivetrain integrates an engine, an electric traction motor/generator, Ni-Hi batteries, an automatic clutch, and an automatic/manual transmission system.The motor is directly linked between the auto clutch output and the transmission input.This architecture provides the regenerative braking during deceleration and allows an efficient motor assist operation.To provide pure electrical propulsion, the engine can be disconnected from the drivetrain by the automatic clutch.We have adopted the vehicle model from our previous work [19,20] for this research.The key parameters of this vehicle are given in Table 1.\n\nAppl.Sci.2018, 8, x FOR PEER REVIEW 4 of 15 clutch.We have adopted the vehicle model from our previous work [19,20] for this research.The key parameters of this vehicle are given in Table 1.\n\n\nControl action:\n\nThe decision-making on the torque-split ratio between the internal combustion engine (ICE) and battery is the core problem of the HEV energy management strategy.We choose the output torque from the ICE as the control action in this study, denoted as\n) ( ) ( t T t A e =\n, where t is the time step index.\n\n\n) (t T e\n\nshould be discretized in order to apply the DRL-based algorithm, i.e., the entire action space is\n} ,..., , { 2 1 n A A A A =\n, where n is the degree of discretization.In this research, we consider n as 24.The motor output torque\n) (t T m\ncan be obtained by subtracting\n) (t T e from ) (t T dem .\nImmediate Reward: Immediate reward is important in the DRL algorithm because it directly influences the parameters tuning of the deep neural network (DNN).The DRL agent is always trying to maximize the reward which it can obtain by taking the optimal action at each time step.Therefore, the immediate reward should be defined according to the optimization objective.The control objective of the HEV EMS is to minimize vehicle fuel consumption and emissions along a  System state: In the DRL algorithm, control action is directly determined by the system states.In this study, the total required torque (T dem ) and the battery state-of-charge (SOC) are selected to form a two-dimensional state space, i.e., s(t) = (T dem (t), SOC(t)) T , where T dem (t) represents the required torque at time t, and SOC(t) represents the battery state of charge at time t.\n\nControl action: The decision-making on the torque-split ratio between the internal combustion engine (ICE) and battery is the core problem of the HEV energy management strategy.We choose the output torque from the ICE as the control action in this study, denoted as A(t) = T e (t), where t is the time step index.T e (t) should be discretized in order to apply the DRL-based algorithm, i.e., the entire action space is A = A 1 , A 2 , ..., A n , where n is the degree of discretization.In this research, we consider n as 24.The motor output torque T m (t) can be obtained by subtracting T e (t) from T dem (t).\n\nImmediate Reward: Immediate reward is important in the DRL algorithm because it directly influences the parameters tuning of the deep neural network (DNN).The DRL agent is always trying to maximize the reward which it can obtain by taking the optimal action at each time step.\n\nTherefore, the immediate reward should be defined according to the optimization objective.The control objective of the HEV EMS is to minimize vehicle fuel consumption and emissions along a driving mission.Meanwhile, the vehicle drivability and battery health should be satisfied.In this work, we focus more on fuel economy of the HEV; the emissions are not taken into consideration.Keeping this objective in mind, the reciprocal of the ICE fuel consumption at each time step is defined as the immediate reward.A penalty value is introduced to penalize the situation when the SOC exceeds the threshold.Immediate reward is defined by the following equations:\nr a ss = \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 1 C ICE C ICE = 0 \u2229 0.4 \u2264 SOC \u2264 0.85 1 C ICE +C C ICE = 0 \u2229 SOC < 0.4 or SOC > 0.85 2 Min C ICE C ICE = 0 \u2229 0.4 \u2264 SOC \u2212 1 C C ICE = 0 \u2229 SOC < 0.4 (1)\nwhere r a ss is the immediate reward generated when state changes from s to s by taking action a; C ICE is the instantaneous fuel consumption value of the ICE; C is the numerical penalty, as well as the maximum instantaneous power supply from the ICE; Min C ICE is the minimum nonzero value of the ICE instantaneous fuel consumption value.The SOC variation range is from 40% to 85% in this study.This definition can guarantee the lower ICE fuel consumption while satisfying the SOC constrains.\n\nFormally, the goal of the EMS of the HEV is to find the optimal control strategy, \u03c0 * , that maps the observed states s t to the control action a t .Mathematically, the control strategy of the HEV can be formulated as an infinite horizon dynamic optimization problem as follows:\nR = \u221e \u2211 t=0 \u03b3 t r(t)(2)\nwhere r(t) is the immediate reward incurred by a t at time t; and \u03b3 \u2208 (0, 1) is a discount factor that assures the infinite sum of cost function convergence.We use Q * (s t , a t ), i.e., the optimal value, to represent the maximum accumulative reward which we can obtain by taking action a t in state s t .Q * (s t , a t ) is calculated by the Bellman Equation as follows:\nQ * (s t , a t ) = E[r t+1 + \u03b3max a t+1 Q * (s t+1 , a t+1 )|s t , a t ](3)\nThe Q-learning method is used to update the value estimation, as shown in Equation (4).\nQ t+1 (s t , a t ) = Q t (s t , a t ) + \u03b7(r t+1 + \u03b3max a t+1 Q t (s t+1 , a t+1 ) \u2212 Q t (s t , a t ))(4)\nwhere \u03b7 \u2208 (0, 1] represents the learning rate.Such a value iteration algorithm converges to the optimal action value function,\nQ t \u2192 Q * as t \u2192 \u221e .\n\nDeep Reinforcement Learning-Based EMS\n\nDeep reinforcement learning-based EMS is developed which combines a deep neural network and conventional reinforcement learning.The EMS makes decisions only based on the current system state since the proposed EMS is an end-to-end control strategy.This deep reinforcement neural network can also be called a deep Q-network (DQN).In the rest of this section, value function approximation, DRL algorithm design, and the DRL-based algorithm online learning application are presented.\n\n\nValue Function Approximation\n\nThe state-action value is represented by a large, but limited, number of states and actions table, i.e., the Q table, in conventional reinforcement learning.However, a deep neural network is taken in this work to approximate the Q-value by Equation (3).As depicted in Figure 3, the inputs of the network are the system states, which are defined in Section 2. The rectified linear unit (ReLU) is used as the activation function for hidden layers, and the linear layer is used for obtaining the action value at the output layer.In order to balance the exploration and exploitation, the \u03b5 \u2212 greedy policy is used for action selection, i.e., the policy chooses the maximum Q-value action with probability 1 \u2212 \u03b5 and selects a random action with probability \u03b5.\n\nAppl.Sci.2018, 8, x FOR PEER REVIEW 6 of 15 this work to approximate the Q-value by Equation (3).As depicted in Figure 3, the inputs of the network are the system states, which are defined in Section 2. The rectified linear unit (ReLU) is used as the activation function for hidden layers, and the linear layer is used for obtaining the action value at the output layer.In order to balance the exploration and exploitation, the greedy \u03b5 policy is used for action selection, i.e., the policy chooses the maximum Q-value action with probability \u03b5 \u2212 1 and selects a random action with probability \u03b5 .The Q-value estimation for all control actions can be calculated by performing a forward calculation in the neural network.The mean squared error between the target Q-value and the inferred output of neural network is defined as loss function in Equation ( 5):\nL \uf0ee \uf0ed \uf0ec = SOC T S dem t ) , , ( . . . ) , , ( ) , , ( 2 1 \u03b8 \u03b8 \u03b8 n t t t t t t a s Q a s Q a s Q b w\uff0c b w\uff0c ) - 1 1 , , ( max 1 \u03b8 \u03b3 + + + + t t a a s Q r t] )) , , ( ) , , ( max [( ) ( 2 1 1 1 \u03b8 \u03b8 \u03b3 \u03b8 t t t t a a s Q a s Q r E L t \u2212 + = \u2212 + + + (5)\nwhere ) , (\n\u03b8 t t a s Q\nis the output of the neural network with the parameters \u03b8 .\n\n) , , ( max\n1 1 1 \u2212 + + + + \u03b8 \u03b3 t t a a s Q r t\nis the target Q-value, using parameters \u2212 \u03b8 from some previous iteration.This fixed target Q network makes the algorithm more stable.Parameters in the neural network are updated by the gradient descent method.\n\nThe inputs of DQN are total required torque dem T and battery SOC.The variation range of SOC is from 0 to 1 and does not need preprocessing.However, the total required torque dem T can vary significantly.In order to facilitate the learning process, we scale the total required torque dem T to the range [\u22121, 1] before feeding to the neural network as shown in Equation ( 6).The minimum and maximum values for dem T can be obtained from historical observation:\n) min( ) max( ) min( ' dem dem dem dem dem T T T T T \u2212 \u2212 = (6)\n\nDRL Algorithm Design\n\nOur DRL-based EMS control algorithm is presented in Algorithm 1.The outer loop controls the number of training episodes, while the inner loop performs the EMS control at each time step within one training episode.The Q-value estimation for all control actions can be calculated by performing a forward calculation in the neural network.The mean squared error between the target Q-value and the inferred output of neural network is defined as loss function in Equation ( 5):\nL(\u03b8) = E[(r + \u03b3max a t+1 Q(s t+1 , a t+1 , \u03b8 \u2212 ) \u2212 Q(s t , a t , \u03b8)) 2 ] (5)\nwhere Q(s t a t , \u03b8) is the output of the neural network with the parameters \u03b8. r + \u03b3max\na t+1 Q(s t+1 , a t+1 , \u03b8 \u2212 )\nis the target Q-value, using parameters \u03b8 \u2212 from some previous iteration.This fixed target Q network makes the algorithm more stable.Parameters in the neural network are updated by the gradient descent method.The inputs of DQN are total required torque T dem and battery SOC.The variation range of SOC is from 0 to 1 and does not need preprocessing.However, the total required torque T dem can vary significantly.In order to facilitate the learning process, we scale the total required torque T dem to the range [\u22121, 1] before feeding to the neural network as shown in Equation (6).The minimum and maximum values for T dem can be obtained from historical observation:\nT dem = T dem \u2212 min(T dem ) max(T dem ) \u2212 min(T dem )(6)\n\nDRL Algorithm Design\n\nOur DRL-based EMS control algorithm is presented in Algorithm 1.The outer loop controls the number of training episodes, while the inner loop performs the EMS control at each time step within one training episode.With probability \u03b5 select a random action a t otherwise select a t = max a t Q(s t , a; \u03b8)\n\n\n5:\n\nChoose action a t and observe the reward r t 6:\n\nSet s t+1 = (SOC t+1 , T t+1 ) 7:\n\nStore (s t , a t , r t , s t+1 ) in memory D 8:\n\nSample random mini-batch of (s t , a t , r t , s t+1 ) from D 9:\n\nif terminal s j+1 : Set y j = r j else set y j = r j + \u03b3max\na j+1 Q(s j+1 , a j+1 ; \u03b8 _ ) 10:\nPerform a gradient descent step on (y j \u2212 Q(s j a j ; \u03b8)) 2 11:\n\nEvery C steps reset Q = Q 12: end for 13: end for In order to avoid the strong correlations between the samples in a short time period of conventional RL, experience replay is adopted to store the experience (i.e., a batch of state, action, reward, and next state:(s t , a t , r t , s t+1 )) at each time step in a data experience pool.For each certain time, random samples of experience are drawn from the experiment pool and used to train the Q network.\n\nWe initialize memory D as an empty set.Then we initialize weights \u03b8 in the action-value function estimation Q neural network.In order to break the dependency loop between the target value and weights \u03b8, a separate neural network Q with weights \u03b8 \u2212 is created for calculating the target Q value.\n\nWe can set the maximum number of episode as M.During the learning process, in step 4, the algorithm selects the maximum Q value action with probability 1 \u2212 \u03b5 and selects a random action with probability \u03b5 based on the observation of the state.In step 5, action a t is executed and reward r t is obtained.In step 6, the system state becomes the next state.In step 7, the state action transition tuple is stored in memory.Then, a mini-batch of transition tuples is drawn randomly from the memory.Step 9 calculates the target Q value.The weights in neural network Q are updated by using the gradient descent method in step 10.The network Q is periodically updated by copying parameters from the network Q in step 11.\n\n\nDRL-Based Algorithm Online Learning Application\n\nIn Section 3.2, the DRL-based algorithm is proposed, however, it is an offline learning algorithm which can only be applied in the simulation environment.More importantly, the training process can only be applied in limited driving cycles, therefore, the trained DQN only performs well under the learned driving conditions, which may not provide satisfactory results under other driving cycles.This is unacceptable in HEV real-time applications.As a result, online learning is necessary for DRL-based algorithms in HEV EMS applications.\n\nThe DRL-based online learning architecture is presented in Figure 4. Action execution and network training should be separated.There is a controller which contains a Q neural network and selects an action for the HEV while storing the state action transitions.When the HEV needs to learn a new driving cycle, the method of action selection will be the \u03b5 \u2212 greedy method.Otherwise, the HEV can always select the maximum Q-value action.There is another on-board computer or remote computing center which is responsible for Q neural network training.The on-board computer or remote computing center obtains state action transitions from the action controller and trains the neural network based on the DRL algorithm.The Q neural network is periodically updated by copying parameters from the on-board computer or remote computing center.\n\nnetwork training should be separated.There is a controller which contains a Q neural network and selects an action for the HEV while storing the state action transitions.When the HEV needs to learn a new driving cycle, the method of action selection will be the greedy \u2212 \u03b5 method.Otherwise, the HEV can always select the maximum Q-value action.There is another on-board computer or remote computing center which is responsible for Q neural network training.The on-board computer or remote computing center obtains state action transitions from the action controller and trains the neural network based on the DRL algorithm.The Q neural network is periodically updated by copying parameters from the on-board computer or remote computing center. .This is useful to train a large Q neural network which can deal with different driving conditions.The main differences between online learning and offline learning are as follows: (1) online learning can adapt to varying driving conditions, while offline can only learn from the given driving cycles; (2) action execution and network training should be separated in online learning because of the limited on-board controller computing ability; and (3) online training efficiency should be higher than offline training since the vehicle must learn the optimal EMS with the shortest time.Thus, it is necessary to cluster the representative state action transitions and use the recent data in the experience pool.\n\nInterestingly, offline learning and online learning can be combined to realize a good effect of EMS.For instance, we can train the DQN offline under the Urban Dynamometer Driving Schedule (UDDS), and then apply the online learning under the New European Driving Cycle (NEDC).\n\n\nExperimental Results and Discussion\n\n\nOffline Application\n\n\nExperiment Setup\n\nIn order to evaluate the effectiveness of proposed DRL-based algorithm, simulation experiments are done in MATLAB and the ADVISOR co-simulation environment.The offline learning application is evaluated firstly and the UDDS driving cycle is used in the learning process.The main differences between online learning and offline learning are as follows: (1) online learning can adapt to varying driving conditions, while offline can only learn from the given driving cycles; (2) action execution and network training should be separated in online learning because of the limited on-board controller computing ability; and (3) online training efficiency should be higher than offline training since the vehicle must learn the optimal EMS with the shortest time.Thus, it is necessary to cluster the representative state action transitions and use the recent data in the experience pool.\n\nInterestingly, offline learning and online learning can be combined to realize a good effect of EMS.For instance, we can train the DQN offline under the Urban Dynamometer Driving Schedule (UDDS), and then apply the online learning under the New European Driving Cycle (NEDC).\n\n\nExperimental Results and Discussion\n\n\nOffline Application\n\n\nExperiment Setup\n\nIn order to evaluate the effectiveness of proposed DRL-based algorithm, simulation experiments are done in MATLAB and the ADVISOR co-simulation environment.The offline learning application is evaluated firstly and the UDDS driving cycle is used in the learning process.The simulation model for the HEV mentioned in Section 2 is built in ADVISOR.Meanwhile, the hyper parameters of the DRL-based algorithm used in the simulations are summarized in Table 2.In this application, the input layer of the network has two neurons, i.e., T dem and SOC.There are three hidden layers having 20, 50, and 100 neurons, respectively.The output layer has 24 neurons representing the discrete ICE torque.All these layers are fully connected.The network is trained with 50 episodes and each episode means a trip (1369 s).\n\nWe evaluate the performance of DRL-based EMS by comparing them with the rule-based EMS known as \"Parallel Electric Assist Control Strategy\" [20].The initial SOC is 0.8.\n\n\nExperimental Results\n\nFirstly, we evaluate the learning performance of DRL-based algorithm.The track of average loss is recorded in Figure 5.It is clear that the average loss decreases quickly along the training process.Figure 6 depicts the track of the total reward of one episode along the training process.Even though the curve is oscillating, the overall trend of the track is rising.There are also some dramatic drops in the total reward during the training process.This is because of the adding of a large penalty when the algorithm selects actions that results in the violation of the SOC constraint.The simulation model for the HEV mentioned in Section 2 is built in ADVISOR.Meanwhile, the hyper parameters of the DRL-based algorithm used in the simulations are summarized in Table 2.\n\n\nExperimental Results\n\nFirstly, we evaluate the learning performance of DRL-based algorithm.The track of average loss is recorded in Figure 5.It is clear that the average loss decreases quickly along the training process.Figure 6 depicts the track of the total reward of one episode along the training process.Even though the curve is oscillating, the overall trend of the track is rising.There are also some dramatic drops in the total reward during the training process.This is because of the adding of a large penalty when the algorithm selects actions that results in the violation of the SOC constraint.The simulation model for the HEV mentioned in Section 2 is built in ADVISOR.Meanwhile, the hyper parameters of the DRL-based algorithm used in the simulations are summarized in Table 2.In this application, the input layer of the network has two neurons, i.e., dem T and SOC.There are three hidden layers having 20, 50, and 100 neurons, respectively.The output layer has 24 neurons representing the discrete ICE torque.All these layers are fully connected.The network is trained with 50 episodes and each episode means a trip (1369 s).We evaluate the performance of DRL-based EMS by comparing them with the rule-based EMS known as \"Parallel Electric Assist Control Strategy\" [20].The initial SOC is 0.8.\n\n\nExperimental Results\n\nFirstly, we evaluate the learning performance of DRL-based algorithm.The track of average loss is recorded in Figure 5.It is clear that the average loss decreases quickly along the training process.Figure 6 depicts the track of the total reward of one episode along the training process.Even though the curve is oscillating, the overall trend of the track is rising.There are also some dramatic drops in the total reward during the training process.This is because of the adding of a large penalty when the algorithm selects actions that results in the violation of the SOC constraint.3, fuel consumption is improved significantly compared to the rule-based control strategy, as fuel consumption is decreased by 10.09%.Meanwhile, the equivalent fuel consumption is also decreased by 8.05%.The DRL-based EMS achieves good performance.Notably, the rule-base EMS is designed by the experts while the DRL-based EMS only learns from the states and historical data.Then, the simulation results of the trained DRL-based EMS for the UDDS driving cycle are shown in Figure 7.In order to evaluate the performance and effectiveness of the trained DRL-based EMS, comparison results are listed in Table 3. Power consumption is converted to fuel consumption; equivalent fuel consumption is obtained by adding the converted power consumption and fuel consumption.As shown by the results of Table 3, fuel consumption is improved significantly compared to the rule-based control strategy, as fuel consumption is decreased by 10.09%.Meanwhile, the equivalent fuel consumption is also decreased by 8.05%.The DRL-based EMS achieves good performance.Notably, the rule-base EMS is designed by the experts while the DRL-based EMS only learns from the states and historical data.\n\n\nOnline Application\n\n\nExperiment Setup\n\nThe DRL-based online learning architecture is presented in Section 3.2.In order to evaluate the online learning performance conveniently, we also use ADVISOR software to simulate the online learning working process.In the online learning application, the neural network setting is the same as the offline application.Two different kinds of simulations are performed.In the first scenario, the neural network parameters are random at the beginning and, in the second one, the neural network is pre-trained offline under the existing driving cycle before the online learning process.In the first case, the online learning simulation without any pre-training under the NEDC driving cycle is done.In the second case, we pre-train the neural network offline under the UDDS driving cycle firstly, and then apply the online learning under the NEDC driving cycle.\n\n\nExperimental Results\n\nIn the first case, we trained the neural network 50 times under the NEDC driving cycle with the same initial condition.Unlike the offline learning, this process is online and simulates the vehicle running under the NEDC driving cycle.\n\nThe track of loss is depicted in Figure 8.The loss also decreases quickly along the training process in the online application.Figure 9 depicts the track of total reward and the fuel consumption of one driving cycle along the training process, and the overall trend of the total reward is the same  The DRL-based online learning architecture is presented in Section 3.2.In order to evaluate the online learning performance conveniently, we also use ADVISOR software to simulate the online learning working process.In the online learning application, the neural network setting is the same as the offline application.Two different kinds of simulations are performed.In the first scenario, the neural network parameters are random at the beginning and, in the second one, the neural network is pre-trained offline under the existing driving cycle before the online learning process.In the first case, the online learning simulation without any pre-training under the NEDC driving cycle is done.In the second case, we pre-train the neural network offline under the UDDS driving cycle firstly, and then apply the online learning under the NEDC driving cycle.\n\n\nExperimental Results\n\nIn the first case, we trained the neural network 50 times under the NEDC driving cycle with the same initial condition.Unlike the offline learning, this process is online and simulates the vehicle running under the NEDC driving cycle.\n\nThe track of loss is depicted in Figure 8.The loss also decreases quickly along the training process in the online application.Figure 9 depicts the track of total reward and the fuel consumption of one driving cycle along the training process, and the overall trend of the total reward is the same as the offline application.This reveals the proposed DRL-based online learning architecture is effective.\n\nAs we can see from Figure 9, the trend of the total reward and the fuel consumption is nearly opposite.This reflects that the definition of the reward is suitable.as the offline application.This reveals the proposed DRL-based online learning architecture is effective.As we can see from Figure 9, the trend of the total reward and the fuel consumption is nearly opposite.This reflects that the definition of the reward is suitable.4. Fuel consumption is also improved compared to the rule-based control strategy, as fuel consumption is decreased by 10.29%, while the equivalent fuel consumption is decreased by 2.57%.as the offline application.This reveals the proposed DRL-based online learning architecture is effective.As we can see from Figure 9, the trend of the total reward and the fuel consumption is nearly opposite.This reflects that the definition of the reward is suitable.4. Fuel consumption is also improved compared to the rule-based control strategy, as fuel consumption is decreased by 10.29%, while the equivalent fuel consumption is decreased by 2.57%.4. Fuel consumption is also improved compared to the rule-based control strategy, as fuel consumption is decreased by 10.29%, while the equivalent fuel consumption is decreased by 2.57%.as the offline application.This reveals the proposed DRL-based online learning architecture is effective.As we can see from Figure 9, the trend of the total reward and the fuel consumption is nearly opposite.This reflects that the definition of the reward is suitable.4. Fuel consumption is also improved compared to the rule-based control strategy, as fuel consumption is decreased by 10.29%, while the equivalent fuel consumption is decreased by 2.57%.In the second case, the neural network was pre-trained offline under the UDDS driving cycle, such that the DRL-based EMS can adapt to the UDDS driving cycle but have no a priori knowledge about the NEDC driving cycle.The comparison of the results between the offline trained DRL-based EMS under the UDDS driving cycle for the NEDC driving cycle and other control strategies are listed in Table 5.It is obvious that the offline-trained EMS under the UDDS driving cycle does not adapt well to the NEDC driving cycle.Based on the offline pre-trained EMS under the UDDS driving cycle, we can apply the online learning process under the NEDC driving cycle.We trained the pre-trained neural network 20 times under the NEDC driving cycle with the same initial conditions.After the training process, we tested the trained EMS under the NEDC driving cycle.The simulation results are shown in Figure 11.The comparison of the results are listed in Table 6.The results show that the pre-training process can contribute to effectively decrease the online training time.This is because the DRL-based EMS learns some of the same features between different driving conditions.In the second case, the neural network was pre-trained offline under the UDDS driving cycle, such that the DRL-based EMS can adapt to the UDDS driving cycle but have no a priori knowledge about the NEDC driving cycle.The comparison of the results between the offline trained DRL-based EMS under the UDDS driving cycle for the NEDC driving cycle and other control strategies are listed in Table 5.It is obvious that the offline-trained EMS under the UDDS driving cycle does not adapt well to the NEDC driving cycle.Based on the offline pre-trained EMS under the UDDS driving cycle, we can apply the online learning process under the NEDC driving cycle.We trained the pre-trained neural network 20 times under the NEDC driving cycle with the same initial conditions.After the training process, we tested the trained EMS under the NEDC driving cycle.The simulation results are shown in Figure 11.The comparison of the results are listed in Table 6.The results show that the pre-training process can contribute to effectively decrease the online training time.This is because the DRL-based EMS learns some of the same features between different driving conditions.\n\n\nConclusions\n\nThis paper presents a deep reinforcement learning-based data-driven approach to obtain an energy management strategy of a HEV.The proposed method combines Q learning and a deep neural network to form a deep Q network which can obtain action directly from the states.Key concepts of the DRL-based EMS have been formulated.Value function approximation and DRL algorithm design have been described in detail in this paper.In order to adapt to varying driving cycles, a DRL-based online learning architecture has been presented.Simulation results demonstrate that the DRL-based EMS can obtain better performance than the rule-based EMS in fuel economy.Furthermore, the online learning approach can learn from different driving conditions.The future work will focus on how to improve the online learning efficiency and testing on a real vehicle.Another important issue is how to output continuous actions.In this paper, the output actions are discretized and this may leads to the violent oscillation of the ICE output torque.A deep deterministic policy gradient (DDPG) algorithm can output the continuous actions and may solve this problem.This will be a future work.However, DDPG is also based on DRL.The contribution of this paper will speed up the application of deep reinforcement learning methods in energy management of HEVs.\n\n\nConclusions\n\nThis paper presents a deep reinforcement learning-based data-driven approach to obtain an energy management strategy of a HEV.The proposed method combines Q learning and a deep neural network to form a deep Q network which can obtain action directly from the states.Key concepts of the DRL-based EMS have been formulated.Value function approximation and DRL algorithm design have been described in detail in this paper.In order to adapt to varying driving cycles, a DRL-based online learning architecture has been presented.Simulation results demonstrate that the DRL-based EMS can obtain better performance than the rule-based EMS in fuel economy.Furthermore, the online learning approach can learn from different driving conditions.The future work will focus on how to improve the online learning efficiency and testing on a real vehicle.Another important issue is how to output continuous actions.In this paper, the output actions are discretized and this may leads to the violent oscillation of the ICE output torque.A deep deterministic policy gradient (DDPG) algorithm can output the continuous actions and may solve this problem.This will be a future work.However, DDPG is also based on DRL.The contribution of this paper will speed up the application of deep reinforcement learning methods in energy management of HEVs.\n\nFigure 1 .\n1\nFigure 1.Deep reinforcement learning (DRL)-based framework for HEV EMS.\n\n\nFigure 1 .\n1\nFigure 1.Deep reinforcement learning (DRL)-based framework for HEV EMS.\n\n\nFigure 2 .\n2\nFigure 2. Drivetrain structure of the HEV.\n\n\nFigure 2 .\n2\nFigure 2. Drivetrain structure of the HEV.\n\n\nTable 1 .\n1\nSummary of the HEV parameters.GR: 2.2791/2.7606/3.5310/5.6175/11.1066Vehicle Curb weight: 1000 kg In the following, key concepts of the DRL-based EMS are formulated:\n\n\nFigure 3 .\n3\nFigure 3. Structure of the neural network.\n\n\nFigure 3 .\n3\nFigure 3. Structure of the neural network.\n\n\nAlgorithm 1 : 1 : 3 :\n113\nDeep Q-Learning with Experience Replay Initialize replay memory D to capacity N Initialize action-value function Q with random weights \u03b8 Initialize target action-value function Q with weights \u03b8 \u2212 = \u03b8 For episode = 1, M do 2: Reset environment: s 0 = (SOC Initial , T 0 ) For t = 1, T, do 4:\n\n\nFigure 4 .\n4\nFigure 4. DRL-based online learning architecture.\n\n\nFigure 4 .\n4\nFigure 4. DRL-based online learning architecture.\n\n\n\n\nAppl.Sci.2018, 8, x FOR PEER REVIEW 9 of 15\n\n\nFigure 5 .\n5\nFigure 5. Track of loss.\n\n\nFigure 6 .\n6\nFigure 6.Track of the total reward.\n\n\nFigure 5 .\n5\nFigure 5. Track of loss.\n\n\nFigure 5 .\n5\nFigure 5. Track of loss.\n\n\nFigure 6 .\n6\nFigure 6.Track of the total reward.\n\n\nFigure 6 .\n6\nFigure 6.Track of the total reward.\n\n\n\n\nAppl.Sci.2018, 8, x FOR PEER REVIEW 10 of 15\n\n\nFigure 7 .Table 3 .\n73\nFigure 7. Simulation results of the trained EMS under UDDS.Table 3. Comparison of the results under UDDS.Control Strategy Fuel Consumption (L/100 km) Equivalent Fuel Consumption (L/100 km) Rule-Based 3.857 3.861 DRL-based 3.468 3.550\n\n\nFigure 7 .\n7\nFigure 7. Simulation results of the trained EMS under UDDS.\n\n\n\n\nAppl.Sci.2018, 8, x FOR PEER REVIEW 11 of 15\n\n\nFigure 8 .\n8\nFigure 8. Track of loss in the online application.\n\n\nFigure 9 .\n9\nFigure 9. Track of the total reward and fuel consumption in the online application.Simulation results of the online trained DRL-based EMS for the NEDC driving cycle are shown in Figure 10.The comparison of the results are listed in Table4.Fuel consumption is also improved compared to the rule-based control strategy, as fuel consumption is decreased by 10.29%, while the equivalent fuel consumption is decreased by 2.57%.\n\n\nFigure 10 .\n10\nFigure 10.Simulation results of the trained EMS under NEDC.\n\n\nFigure 8 .\n8\nFigure 8. Track of loss in the online application.\n\n\n\n\nAppl.Sci.2018, 8, x FOR PEER REVIEW 11 of 15\n\n\nFigure 8 .\n8\nFigure 8. Track of loss in the online application.\n\n\nFigure 9 .\n9\nFigure 9. Track of the total reward and fuel consumption in the online application.Simulation results of the online trained DRL-based EMS for the NEDC driving cycle are shown in Figure 10.The comparison of the results are listed in Table4.Fuel consumption is also improved compared to the rule-based control strategy, as fuel consumption is decreased by 10.29%, while the equivalent fuel consumption is decreased by 2.57%.\n\n\nFigure 10 .\n10\nFigure 10.Simulation results of the trained EMS under NEDC.\n\n\nFigure 9 .\n9\nFigure 9. Track of the total reward and fuel consumption in the online application.\n\n\n\n\nAppl.Sci.2018, 8, x FOR PEER REVIEW 11 of 15\n\n\nFigure 8 .\n8\nFigure 8. Track of loss in the online application.\n\n\nFigure 9 .\n9\nFigure 9. Track of the total reward and fuel consumption in the online application.\n\n\nFigure 10 .\n10\nFigure 10.Simulation results of the trained EMS under NEDC.Figure 10.Simulation results of the trained EMS under NEDC.\n\n\nFigure 10 .\n10\nFigure 10.Simulation results of the trained EMS under NEDC.Figure 10.Simulation results of the trained EMS under NEDC.\n\n\nFigure 11 .Table 6 .\n116\nFigure 11.Simulation results under NEDC of the online trained EMS which was pre-trained under UDDS.\n\n\nFigure 11 .Table 6 . 15 Figure\n11615\nFigure 11.Simulation results under NEDC of the online trained EMS which was pre-trained under UDDS.\n\n\nFigure 12 .\n12\nFigure 12.Track of the total reward and fuel consumption in the online application in which the EMS was pre-trained firstly.\n\n\nFigure 12 .\n12\nFigure 12.Track of the total reward and fuel consumption in the online application in which the EMS was pre-trained firstly.\n\n\nTable 1 .\n1\nSummary of the HEV parameters.\nPart or VehicleParameters ValueSpark Ignition (SI) engineDisplacement: 1.0 L Maximum power: 50 kW/5700 r/min Maximum torque: 89.5 Nm/5600 r/minPermanent magnet motorMaximum power: 10 kW Maximum torque: 46.5 NmCapacity: 6.5 AhAdvanced Ni-Hi batteryNominal cell voltage: 1.2 VTotal cells: 120Automated manual transmission5-speed GR: 2.2791/2.7606/3.5310/5.6175/11.1066VehicleCurb weight: 1000 kgIn the following, key concepts of the DRL-based EMS are formulated:System state: In the DRL algorithm, control action is directly determined by the system states.In this study, the total required torque ( dem T ) and the battery state-of-charge (SOC) are selected toform a two-dimensional state space, i.e.,st ( = )dem T (t (),SOCt ())T, whereT dem(t)represents therequired torque at time t, andSOC(t)represents the battery state of charge at time t.\n\nTable 2 .\n2\nSummary of the DRL-based algorithm hyper parameters.\nHyper ParametersValuemini-batch size32replay memory size1000discount factor \u03b30.99learning rate0.00025initial exploration1final exploration0.2replay start size200\n\nTable 2 .\n2\nSummary of the DRL-based algorithm hyper parameters.In this application, the input layer of the network has two neurons, i.e., dem T and SOC.There are three hidden layers having 20, 50, and 100 neurons, respectively.The output layer has 24 neurons representing the discrete ICE torque.All these layers are fully connected.The network is trained with 50 episodes and each episode means a trip (1369 s).We evaluate the performance of DRL-based EMS by comparing them with the rule-based EMS known as \"Parallel Electric Assist Control Strategy\"[20].The initial SOC is 0.8.\nHyper ParametersValuemini-batch size32replay memory size discount factor \u03b31000 0.99learning rate0.00025initial exploration1final exploration0.2replay start size200\n\nTable 2 .\n2\nSummary of the DRL-based algorithm hyper parameters.\nHyper ParametersValuemini-batch size32replay memory size discount factor \u03b31000 0.99learning rate0.00025initial exploration1final exploration0.2replay start size200\n\nTable 3 .\n3\nComparison of the results under UDDS.\nControl StrategyFuel Consumption (\n\nL/100 km) Equivalent Fuel Consumption (L/100 km)\nRule-Based3.8573.861DRL-based3.4683.5504.2. Online Application4.2.1. Experiment Setup\n\nTable 4 .\n4\nComparison of the results under NEDC.\nControl StrategyFuel Consumption (L/100 km)Equivalent Fuel Consumption (L/100 km)Rule-Based3.8773.892DRL-based3.4783.792\n\nTable 5 .\n5\nComparison of the results under NEDC.\nControl StrategyFuel Consumption (L/100 km)Equivalent Fuel Consumption (L/100 km)Rule-Based3.8773.892DRL-based EMS trained under NEDC online3.4783.792DRL-based EMS only pre-trained under UDDS offline3.6903.872\n\nTable 4 .\n4\nComparison of the results under NEDC.\nControl StrategyFuel Consumption (L/100 km)Equivalent Fuel Consumption (L/100 km)Rule-Based3.8773.892DRL-based3.4783.792\n\nTable 5 .\n5\nComparison of the results under NEDC.\nControl StrategyFuel Consumption (L/100 km)Equivalent Fuel Consumption (L/100 km)Rule-Based3.8773.892DRL-based EMS trained under NEDC online3.4783.792DRL-based EMS only pre-trained under UDDS offline3.6903.872\nAcknowledgments: This research was supported by National Natural Science Foundation of China (61573337), National Natural Science Foundation of China (61603377), National Natural Science Foundation of China (61273139), and the Technical Research Foundation of Shenzhen (JSGG20141020103523742).Acknowledgments: This research was supported by National Natural Science Foundation of China (61573337), National Natural Science Foundation of China (61603377), National Natural Science Foundation of China (61273139), and the Technical Research Foundation of Shenzhen (JSGG20141020103523742).Author Contributions: Yue Hu and Kun Xu proposed the method and wrote main part of this paper; Weimin Li supported this article financially and checked the paper; Taimoor Zahid checked the grammar of this paper; and Feiyan Qin and Chenming Li analyzed the data.Conflicts of Interest:The authors declare no conflict of interest.Author Contributions: Yue Hu and Kun Xu proposed the method and wrote main part of this paper; Weimin Li supported this article financially and checked the paper; Taimoor Zahid checked the grammar of this paper; and Feiyan Qin and Chenming Li analyzed the data.Conflicts of Interest:The authors declare no conflict of interest.\nA Comprehensive Study of Key Electric Vehicle (EV) Components, Technologies, Challenges, Impacts, and Future Direction of Development. F Un-Noor, S Padmanaban, L Mihet-Popa, M N Mollah, E Hossain, 10.3390/en100812172017101217\n\nA Comprehensive Study of Key Electric Vehicle (EV) Components, Technologies, Challenges, Impacts, and Future Direction of Development. F Un-Noor, S Padmanaban, L Mihet-Popa, M N Mollah, E Hossain, 10.3390/en100812172017. 121710\n\nClassification and review of control strategies for plug-in hybrid electric vehicles. S G Wirasingha, A Emadi, 10.1109/TVT.2010.2090178IEEE Trans. Veh. Technol. 602011\n\nAn instantaneous optimization strategy based on efficiency maps for internal combustion engine/battery hybrid vehicles. K Gokce, A Ozdemir, 10.1016/j.enconman.2014.02.034Energy Convers. Manag. 812014\n\nPower management optimization of fuel cell/battery hybrid vehicles with experimental validation. F Odeim, J Roes, L Wulbeck, A Heinzel, 10.1016/j.jpowsour.2013.12.012J. Power Sources. 2522014\n\nOptimal fuzzy power control and management of fuel cell/battery hybrid vehicles. C Li, G Liu, 10.1016/j.jpowsour.2009.03.007J. Power Sources. 1922009\n\nFuzzy-based blended control for the energy management of a parallel plug-in hybrid electric vehicle. N Denis, M R Dubois, A Desrochers, 10.1049/iet-its.2014.0075IET Intell. Transp. Syst. 92015\n\nOptimal energy management in a dual-storage fuel-cell hybrid vehicle using multi-dimensional dynamic programming. M Ansarey, M S Panahi, H Ziarati, M Mahjoob, J. Power Sources. 2502014\n\nStochastic dynamic programming in the real-world control of hybrid electric. C Vagg, S Akehurst, C J Brace, L Ash, 10.1109/TCST.2015.2498141IEEE Trans. Contr. Syst. Techol. 242016\n\nStochastic optimal control of parallel hybrid electric vehicles. F Qin, G Xu, Y Hu, K Xu, W Li, 10.3390/en10020214201710\n\nEnergy management of a power-split plug-in hybrid electric vehicle based on genetic algorithm and quadratic programming. Z Chen, C C Mi, R Xiong, J Xu, C You, J. Power Sources. 2482014\n\nA pontryagin minimum principle-based adaptive equivalent consumption minimum strategy for a plug-in hybrid electric bus on a fixed route. S Xie, H Li, Z Xin, T Liu, L Wei, 2017101379\n\nOptimal energy management for HEVs in Eco-driving applications using bi-level MPC. L Guo, B Guo, Y Guo, H Chen, 10.1109/TITS.2016.2634019IEEE Trans. Intell. Transp. Syst. 182017\n\nData-driven hierarchical control for online energy management of plug-in hybrid electric city bus. H Tian, S E Li, X Wang, Y Huang, G Tian, 10.1016/j.energy.2017.09.0612018142Energy\n\nOnline energy management strategy of fuel cell hybrid electric vehicles based on data fusion approach. Z Daming, A Ai-Durra, G Fei, M G Simoes, J. Power Sources. 3662017\n\nEnergy Management in Plug-in Hybrid Electric Vehicles: Recent Progress and a Connected Vehicles Perspective. C M Martinez, X Hu, D Cao, E Velenis, B Gao, M Wellers, 10.1109/TVT.2016.2582721IEEE Trans. Veh. Technol. 662017\n\nAn energy management approach of hybrid vehicles using traffic preview information for energy saving. C Zheng, G Xu, K Xu, Z Pan, Q Liang, 10.1016/j.enconman.2015.07.061Energy Convers. Manag. 1052015\n\nReinforcement learning-based energy management strategy for a hybrid electric tracked vehicle. Energies. T Liu, Y Zou, D Liu, F Sun, 10.3390/en807724320158\n\nData-driven reinforcement learning-based real-time energy management system for plug-in hybrid electric vehicles. X Qi, G Wu, K Boriboonsomsin, M J Barth, J Gonder, 10.3141/2572-01Transp. Res. Rec. 25722016\n\nOnline learning control for hybrid electric vehicle. W Li, G Xu, Y Xu, Chin. J. Mech. Eng. 252012\n\nAn online learning control strategy for hybrid electric vehicle based on fuzzy Q-learning. Energies. Y Hu, W Li, H Xu, G Xu, 10.3390/en8101116720158\n\nHuman-level control through deep reinforcement learning. M Volodymyr, K Kavukcuoglu, D Silver, A Rusu, J Veness, M G Bellemare, A Graves, M Riedmiller, A Fidjeland, G Ostrovski, Nature. 5182015\n\nMastering the game of go with deep neural networks and tree search. D Silver, H Aja, J C Maddison, A Guez, L Sifre, G Van Den Driessche, J Schrittwieser, L Antonoglou, V Panneershelvam, M Lanctot, 10.1038/nature16961Nature. 5292016\n\nAutonomous learning of state representations for control. T Springenberg, J Boedecker, M Riedmiller, K Obermayer, 2015K\u00fcnstliche Intell29\n\nDeep reinforcement learning for building HVAC control. T Wei, Y Wang, Q Zhu, Proceedings of the 54th Annual Design Automation Conference. the 54th Annual Design Automation ConferenceAustin, TX, USAJune 2017\n\nExpert level control of ramp metering based on multi-task deep reinforcement learning. F Belletti, D Haziza, G Gomes, A M Bayen, 10.1109/TITS.2017.2725912IEEE Trans. Intell. Transp. Syst. 992017\n\nEnd-to-End deep reinforcement learning for lane keeping assist. A E Sallab, M Abdou, E Perot, S Yogamani, 2016\n\nAutonomous Braking System via Deep Reinforcement Learning. H Chae, C M Kang, B D Kim, J Kim, C C Chung, J W Choi, 2017\n\nLearning deep control policies for autonomous aerial vehicles with MPC-guided policy search. Z Tianho, G Kanh, S Levine, P Abbeel, Proceedings of the 2016 IEEE International Conference on Robotics and Automation (ICRA). the 2016 IEEE International Conference on Robotics and Automation (ICRA)Stockholm, SwedenMay 2016\n\nDeep reinforcement learning-based vehicle energy efficiency autonomous learning system. X Qi, Y Luo, G Wu, K Boriboonsomsin, M J Brath, Proceedings of the 2017 IEEE Intelligent Vehicles Symposium (IV). the 2017 IEEE Intelligent Vehicles Symposium (IV)Redondo, Beach, CA, USAJune 2017\n", "annotations": {"author": "[{\"end\":420,\"start\":112},{\"end\":721,\"start\":421},{\"end\":839,\"start\":722},{\"end\":1070,\"start\":840},{\"end\":1295,\"start\":1071},{\"end\":1403,\"start\":1296}]", "publisher": null, "author_last_name": "[{\"end\":118,\"start\":116},{\"end\":430,\"start\":428},{\"end\":728,\"start\":726},{\"end\":853,\"start\":848},{\"end\":1081,\"start\":1078},{\"end\":1307,\"start\":1305}]", "author_first_name": "[{\"end\":115,\"start\":112},{\"end\":427,\"start\":421},{\"end\":725,\"start\":722},{\"end\":847,\"start\":840},{\"end\":1077,\"start\":1071},{\"end\":1304,\"start\":1296}]", "author_affiliation": "[{\"end\":228,\"start\":138},{\"end\":331,\"start\":230},{\"end\":419,\"start\":333},{\"end\":522,\"start\":432},{\"end\":610,\"start\":524},{\"end\":720,\"start\":612},{\"end\":838,\"start\":748},{\"end\":966,\"start\":876},{\"end\":1069,\"start\":968},{\"end\":1191,\"start\":1101},{\"end\":1294,\"start\":1193},{\"end\":1402,\"start\":1309}]", "title": "[{\"end\":94,\"start\":1},{\"end\":1497,\"start\":1404}]", "venue": null, "abstract": "[{\"end\":2909,\"start\":1709}]", "bib_ref": "[{\"end\":3092,\"start\":3089},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3536,\"start\":3533},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3538,\"start\":3536},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3694,\"start\":3691},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3824,\"start\":3821},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3826,\"start\":3824},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4052,\"start\":4049},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4253,\"start\":4250},{\"end\":4256,\"start\":4253},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4259,\"start\":4256},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4329,\"start\":4325},{\"end\":4374,\"start\":4370},{\"end\":5009,\"start\":5005},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5288,\"start\":5285},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5432,\"start\":5428},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5435,\"start\":5432},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5680,\"start\":5676},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5683,\"start\":5680},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5996,\"start\":5992},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5999,\"start\":5996},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6643,\"start\":6639},{\"end\":6672,\"start\":6668},{\"end\":6958,\"start\":6954},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7141,\"start\":7137},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7159,\"start\":7155},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7454,\"start\":7450},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7482,\"start\":7478},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7502,\"start\":7498},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7592,\"start\":7588},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7624,\"start\":7620},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7654,\"start\":7650},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8082,\"start\":8078},{\"end\":8195,\"start\":8192},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":13960,\"start\":13956},{\"end\":13963,\"start\":13960},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":14152,\"start\":14148},{\"end\":14155,\"start\":14152},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":23597,\"start\":23594},{\"end\":31326,\"start\":31322},{\"end\":33433,\"start\":33429}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":45019,\"start\":44933},{\"attributes\":{\"id\":\"fig_1\"},\"end\":45106,\"start\":45020},{\"attributes\":{\"id\":\"fig_2\"},\"end\":45164,\"start\":45107},{\"attributes\":{\"id\":\"fig_3\"},\"end\":45222,\"start\":45165},{\"attributes\":{\"id\":\"fig_4\"},\"end\":45402,\"start\":45223},{\"attributes\":{\"id\":\"fig_5\"},\"end\":45460,\"start\":45403},{\"attributes\":{\"id\":\"fig_6\"},\"end\":45518,\"start\":45461},{\"attributes\":{\"id\":\"fig_7\"},\"end\":45837,\"start\":45519},{\"attributes\":{\"id\":\"fig_8\"},\"end\":45902,\"start\":45838},{\"attributes\":{\"id\":\"fig_9\"},\"end\":45967,\"start\":45903},{\"attributes\":{\"id\":\"fig_10\"},\"end\":46015,\"start\":45968},{\"attributes\":{\"id\":\"fig_11\"},\"end\":46055,\"start\":46016},{\"attributes\":{\"id\":\"fig_12\"},\"end\":46106,\"start\":46056},{\"attributes\":{\"id\":\"fig_13\"},\"end\":46146,\"start\":46107},{\"attributes\":{\"id\":\"fig_14\"},\"end\":46186,\"start\":46147},{\"attributes\":{\"id\":\"fig_15\"},\"end\":46237,\"start\":46187},{\"attributes\":{\"id\":\"fig_16\"},\"end\":46288,\"start\":46238},{\"attributes\":{\"id\":\"fig_17\"},\"end\":46337,\"start\":46289},{\"attributes\":{\"id\":\"fig_18\"},\"end\":46596,\"start\":46338},{\"attributes\":{\"id\":\"fig_19\"},\"end\":46671,\"start\":46597},{\"attributes\":{\"id\":\"fig_20\"},\"end\":46720,\"start\":46672},{\"attributes\":{\"id\":\"fig_21\"},\"end\":46786,\"start\":46721},{\"attributes\":{\"id\":\"fig_22\"},\"end\":47224,\"start\":46787},{\"attributes\":{\"id\":\"fig_23\"},\"end\":47301,\"start\":47225},{\"attributes\":{\"id\":\"fig_24\"},\"end\":47367,\"start\":47302},{\"attributes\":{\"id\":\"fig_25\"},\"end\":47416,\"start\":47368},{\"attributes\":{\"id\":\"fig_26\"},\"end\":47482,\"start\":47417},{\"attributes\":{\"id\":\"fig_27\"},\"end\":47920,\"start\":47483},{\"attributes\":{\"id\":\"fig_28\"},\"end\":47997,\"start\":47921},{\"attributes\":{\"id\":\"fig_29\"},\"end\":48096,\"start\":47998},{\"attributes\":{\"id\":\"fig_30\"},\"end\":48145,\"start\":48097},{\"attributes\":{\"id\":\"fig_31\"},\"end\":48211,\"start\":48146},{\"attributes\":{\"id\":\"fig_32\"},\"end\":48310,\"start\":48212},{\"attributes\":{\"id\":\"fig_33\"},\"end\":48446,\"start\":48311},{\"attributes\":{\"id\":\"fig_34\"},\"end\":48582,\"start\":48447},{\"attributes\":{\"id\":\"fig_35\"},\"end\":48709,\"start\":48583},{\"attributes\":{\"id\":\"fig_36\"},\"end\":48848,\"start\":48710},{\"attributes\":{\"id\":\"fig_37\"},\"end\":48990,\"start\":48849},{\"attributes\":{\"id\":\"fig_38\"},\"end\":49132,\"start\":48991},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":50020,\"start\":49133},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":50248,\"start\":50021},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":50994,\"start\":50249},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":51224,\"start\":50995},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":51310,\"start\":51225},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":51446,\"start\":51311},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":51618,\"start\":51447},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":51879,\"start\":51619},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":52051,\"start\":51880},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":52312,\"start\":52052}]", "paragraph": "[{\"end\":3328,\"start\":2925},{\"end\":6344,\"start\":3330},{\"end\":7051,\"start\":6346},{\"end\":7844,\"start\":7053},{\"end\":8990,\"start\":7846},{\"end\":10224,\"start\":8992},{\"end\":11539,\"start\":10226},{\"end\":11961,\"start\":11591},{\"end\":12917,\"start\":11985},{\"end\":13289,\"start\":12919},{\"end\":14038,\"start\":13313},{\"end\":14230,\"start\":14040},{\"end\":14499,\"start\":14250},{\"end\":14553,\"start\":14520},{\"end\":14663,\"start\":14566},{\"end\":14795,\"start\":14692},{\"end\":14835,\"start\":14805},{\"end\":15719,\"start\":14863},{\"end\":16331,\"start\":15721},{\"end\":16609,\"start\":16333},{\"end\":17267,\"start\":16611},{\"end\":17946,\"start\":17453},{\"end\":18226,\"start\":17948},{\"end\":18624,\"start\":18251},{\"end\":18788,\"start\":18701},{\"end\":19020,\"start\":18894},{\"end\":19562,\"start\":19082},{\"end\":20349,\"start\":19595},{\"end\":21208,\"start\":20351},{\"end\":21467,\"start\":21456},{\"end\":21539,\"start\":21480},{\"end\":21552,\"start\":21541},{\"end\":21798,\"start\":21589},{\"end\":22259,\"start\":21800},{\"end\":22819,\"start\":22346},{\"end\":22985,\"start\":22897},{\"end\":23683,\"start\":23016},{\"end\":24067,\"start\":23764},{\"end\":24121,\"start\":24074},{\"end\":24156,\"start\":24123},{\"end\":24205,\"start\":24158},{\"end\":24271,\"start\":24207},{\"end\":24332,\"start\":24273},{\"end\":24430,\"start\":24367},{\"end\":24887,\"start\":24432},{\"end\":25183,\"start\":24889},{\"end\":25898,\"start\":25185},{\"end\":26486,\"start\":25950},{\"end\":27322,\"start\":26488},{\"end\":28780,\"start\":27324},{\"end\":29057,\"start\":28782},{\"end\":30019,\"start\":29138},{\"end\":30296,\"start\":30021},{\"end\":31180,\"start\":30377},{\"end\":31350,\"start\":31182},{\"end\":32145,\"start\":31375},{\"end\":33457,\"start\":32170},{\"end\":35237,\"start\":33482},{\"end\":36134,\"start\":35279},{\"end\":36393,\"start\":36159},{\"end\":37549,\"start\":36395},{\"end\":37808,\"start\":37574},{\"end\":38213,\"start\":37810},{\"end\":42246,\"start\":38215},{\"end\":43589,\"start\":42262},{\"end\":44932,\"start\":43605},{\"end\":45018,\"start\":44947},{\"end\":45105,\"start\":45034},{\"end\":45163,\"start\":45121},{\"end\":45221,\"start\":45179},{\"end\":45401,\"start\":45236},{\"end\":45459,\"start\":45417},{\"end\":45517,\"start\":45475},{\"end\":45836,\"start\":45546},{\"end\":45901,\"start\":45852},{\"end\":45966,\"start\":45917},{\"end\":46014,\"start\":45971},{\"end\":46054,\"start\":46030},{\"end\":46105,\"start\":46070},{\"end\":46145,\"start\":46121},{\"end\":46185,\"start\":46161},{\"end\":46236,\"start\":46201},{\"end\":46287,\"start\":46252},{\"end\":46336,\"start\":46292},{\"end\":46595,\"start\":46362},{\"end\":46670,\"start\":46611},{\"end\":46719,\"start\":46675},{\"end\":46785,\"start\":46735},{\"end\":47223,\"start\":46801},{\"end\":47300,\"start\":47241},{\"end\":47366,\"start\":47316},{\"end\":47415,\"start\":47371},{\"end\":47481,\"start\":47431},{\"end\":47919,\"start\":47497},{\"end\":47996,\"start\":47937},{\"end\":48095,\"start\":48012},{\"end\":48144,\"start\":48100},{\"end\":48210,\"start\":48160},{\"end\":48309,\"start\":48226},{\"end\":48445,\"start\":48327},{\"end\":48581,\"start\":48463},{\"end\":48708,\"start\":48609},{\"end\":48847,\"start\":48748},{\"end\":48989,\"start\":48865},{\"end\":49131,\"start\":49007},{\"end\":49176,\"start\":49146},{\"end\":50086,\"start\":50034},{\"end\":50830,\"start\":50262},{\"end\":51060,\"start\":51008},{\"end\":51275,\"start\":51238},{\"end\":51497,\"start\":51460},{\"end\":51669,\"start\":51632},{\"end\":51930,\"start\":51893},{\"end\":52102,\"start\":52065}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11590,\"start\":11561},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14519,\"start\":14500},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14691,\"start\":14664},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14804,\"start\":14796},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14862,\"start\":14836},{\"attributes\":{\"id\":\"formula_5\"},\"end\":17452,\"start\":17268},{\"attributes\":{\"id\":\"formula_6\"},\"end\":18250,\"start\":18227},{\"attributes\":{\"id\":\"formula_7\"},\"end\":18700,\"start\":18625},{\"attributes\":{\"id\":\"formula_8\"},\"end\":18893,\"start\":18789},{\"attributes\":{\"id\":\"formula_9\"},\"end\":19041,\"start\":19021},{\"attributes\":{\"id\":\"formula_10\"},\"end\":21362,\"start\":21209},{\"attributes\":{\"id\":\"formula_11\"},\"end\":21455,\"start\":21362},{\"attributes\":{\"id\":\"formula_12\"},\"end\":21479,\"start\":21468},{\"attributes\":{\"id\":\"formula_13\"},\"end\":21588,\"start\":21553},{\"attributes\":{\"id\":\"formula_14\"},\"end\":22322,\"start\":22260},{\"attributes\":{\"id\":\"formula_15\"},\"end\":22895,\"start\":22820},{\"attributes\":{\"id\":\"formula_16\"},\"end\":22896,\"start\":22895},{\"attributes\":{\"id\":\"formula_17\"},\"end\":23015,\"start\":22986},{\"attributes\":{\"id\":\"formula_18\"},\"end\":23740,\"start\":23684},{\"attributes\":{\"id\":\"formula_19\"},\"end\":24366,\"start\":24333}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":14037,\"start\":14036},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":14229,\"start\":14228},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":30830,\"start\":30829},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":32144,\"start\":32143},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":32939,\"start\":32938},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":34068,\"start\":34067},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":34673,\"start\":34672},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":34864,\"start\":34863},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":38647,\"start\":38646},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":39101,\"start\":39100},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":39287,\"start\":39286},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":39741,\"start\":39740},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":40321,\"start\":40320},{\"end\":40870,\"start\":40869},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":41481,\"start\":41480},{\"end\":42030,\"start\":42029}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2923,\"start\":2911},{\"end\":11560,\"start\":11542},{\"attributes\":{\"n\":\"2.\"},\"end\":11983,\"start\":11964},{\"attributes\":{\"n\":\"2.\"},\"end\":13311,\"start\":13292},{\"end\":14248,\"start\":14233},{\"end\":14564,\"start\":14556},{\"attributes\":{\"n\":\"3.\"},\"end\":19080,\"start\":19043},{\"attributes\":{\"n\":\"3.1.\"},\"end\":19593,\"start\":19565},{\"attributes\":{\"n\":\"3.2.\"},\"end\":22344,\"start\":22324},{\"attributes\":{\"n\":\"3.2.\"},\"end\":23762,\"start\":23742},{\"end\":24072,\"start\":24070},{\"attributes\":{\"n\":\"3.3.\"},\"end\":25948,\"start\":25901},{\"attributes\":{\"n\":\"4.\"},\"end\":29095,\"start\":29060},{\"attributes\":{\"n\":\"4.1.\"},\"end\":29117,\"start\":29098},{\"attributes\":{\"n\":\"4.1.1.\"},\"end\":29136,\"start\":29120},{\"attributes\":{\"n\":\"4.\"},\"end\":30334,\"start\":30299},{\"attributes\":{\"n\":\"4.1.\"},\"end\":30356,\"start\":30337},{\"attributes\":{\"n\":\"4.1.1.\"},\"end\":30375,\"start\":30359},{\"attributes\":{\"n\":\"4.1.2.\"},\"end\":31373,\"start\":31353},{\"attributes\":{\"n\":\"4.1.2.\"},\"end\":32168,\"start\":32148},{\"attributes\":{\"n\":\"4.1.2.\"},\"end\":33480,\"start\":33460},{\"attributes\":{\"n\":\"4.2.\"},\"end\":35258,\"start\":35240},{\"attributes\":{\"n\":\"4.2.1.\"},\"end\":35277,\"start\":35261},{\"attributes\":{\"n\":\"4.2.2.\"},\"end\":36157,\"start\":36137},{\"attributes\":{\"n\":\"4.2.2.\"},\"end\":37572,\"start\":37552},{\"attributes\":{\"n\":\"5.\"},\"end\":42260,\"start\":42249},{\"attributes\":{\"n\":\"5.\"},\"end\":43603,\"start\":43592},{\"end\":44944,\"start\":44934},{\"end\":45031,\"start\":45021},{\"end\":45118,\"start\":45108},{\"end\":45176,\"start\":45166},{\"end\":45233,\"start\":45224},{\"end\":45414,\"start\":45404},{\"end\":45472,\"start\":45462},{\"end\":45541,\"start\":45520},{\"end\":45849,\"start\":45839},{\"end\":45914,\"start\":45904},{\"end\":46027,\"start\":46017},{\"end\":46067,\"start\":46057},{\"end\":46118,\"start\":46108},{\"end\":46158,\"start\":46148},{\"end\":46198,\"start\":46188},{\"end\":46249,\"start\":46239},{\"end\":46358,\"start\":46339},{\"end\":46608,\"start\":46598},{\"end\":46732,\"start\":46722},{\"end\":46798,\"start\":46788},{\"end\":47237,\"start\":47226},{\"end\":47313,\"start\":47303},{\"end\":47428,\"start\":47418},{\"end\":47494,\"start\":47484},{\"end\":47933,\"start\":47922},{\"end\":48009,\"start\":47999},{\"end\":48157,\"start\":48147},{\"end\":48223,\"start\":48213},{\"end\":48323,\"start\":48312},{\"end\":48459,\"start\":48448},{\"end\":48604,\"start\":48584},{\"end\":48741,\"start\":48711},{\"end\":48861,\"start\":48850},{\"end\":49003,\"start\":48992},{\"end\":49143,\"start\":49134},{\"end\":50031,\"start\":50022},{\"end\":50259,\"start\":50250},{\"end\":51005,\"start\":50996},{\"end\":51235,\"start\":51226},{\"end\":51360,\"start\":51312},{\"end\":51457,\"start\":51448},{\"end\":51629,\"start\":51620},{\"end\":51890,\"start\":51881},{\"end\":52062,\"start\":52053}]", "table": "[{\"end\":50020,\"start\":49177},{\"end\":50248,\"start\":50087},{\"end\":50994,\"start\":50831},{\"end\":51224,\"start\":51061},{\"end\":51310,\"start\":51276},{\"end\":51446,\"start\":51361},{\"end\":51618,\"start\":51498},{\"end\":51879,\"start\":51670},{\"end\":52051,\"start\":51931},{\"end\":52312,\"start\":52103}]", "figure_caption": "[{\"end\":45019,\"start\":44946},{\"end\":45106,\"start\":45033},{\"end\":45164,\"start\":45120},{\"end\":45222,\"start\":45178},{\"end\":45402,\"start\":45235},{\"end\":45460,\"start\":45416},{\"end\":45518,\"start\":45474},{\"end\":45837,\"start\":45545},{\"end\":45902,\"start\":45851},{\"end\":45967,\"start\":45916},{\"end\":46015,\"start\":45970},{\"end\":46055,\"start\":46029},{\"end\":46106,\"start\":46069},{\"end\":46146,\"start\":46120},{\"end\":46186,\"start\":46160},{\"end\":46237,\"start\":46200},{\"end\":46288,\"start\":46251},{\"end\":46337,\"start\":46291},{\"end\":46596,\"start\":46361},{\"end\":46671,\"start\":46610},{\"end\":46720,\"start\":46674},{\"end\":46786,\"start\":46734},{\"end\":47224,\"start\":46800},{\"end\":47301,\"start\":47240},{\"end\":47367,\"start\":47315},{\"end\":47416,\"start\":47370},{\"end\":47482,\"start\":47430},{\"end\":47920,\"start\":47496},{\"end\":47997,\"start\":47936},{\"end\":48096,\"start\":48011},{\"end\":48145,\"start\":48099},{\"end\":48211,\"start\":48159},{\"end\":48310,\"start\":48225},{\"end\":48446,\"start\":48326},{\"end\":48582,\"start\":48462},{\"end\":48709,\"start\":48608},{\"end\":48848,\"start\":48747},{\"end\":48990,\"start\":48864},{\"end\":49132,\"start\":49006},{\"end\":49177,\"start\":49145},{\"end\":50087,\"start\":50033},{\"end\":50831,\"start\":50261},{\"end\":51061,\"start\":51007},{\"end\":51276,\"start\":51237},{\"end\":51498,\"start\":51459},{\"end\":51670,\"start\":51631},{\"end\":51931,\"start\":51892},{\"end\":52103,\"start\":52064}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9479,\"start\":9478},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10713,\"start\":10712},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12092,\"start\":12091},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":13420,\"start\":13419},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":19871,\"start\":19870},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":20471,\"start\":20470},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":26555,\"start\":26554},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":31493,\"start\":31492},{\"attributes\":{\"ref_id\":\"fig_12\"},\"end\":31581,\"start\":31580},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":32288,\"start\":32287},{\"attributes\":{\"ref_id\":\"fig_12\"},\"end\":32376,\"start\":32375},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":33600,\"start\":33599},{\"attributes\":{\"ref_id\":\"fig_12\"},\"end\":33688,\"start\":33687},{\"attributes\":{\"ref_id\":\"fig_19\"},\"end\":34547,\"start\":34546},{\"attributes\":{\"ref_id\":\"fig_21\"},\"end\":36436,\"start\":36435},{\"attributes\":{\"ref_id\":\"fig_22\"},\"end\":36530,\"start\":36529},{\"attributes\":{\"ref_id\":\"fig_21\"},\"end\":37851,\"start\":37850},{\"attributes\":{\"ref_id\":\"fig_22\"},\"end\":37945,\"start\":37944},{\"attributes\":{\"ref_id\":\"fig_22\"},\"end\":38242,\"start\":38241},{\"attributes\":{\"ref_id\":\"fig_22\"},\"end\":38510,\"start\":38509},{\"attributes\":{\"ref_id\":\"fig_22\"},\"end\":38964,\"start\":38963},{\"attributes\":{\"ref_id\":\"fig_22\"},\"end\":39604,\"start\":39603},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":40818,\"start\":40816},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":41978,\"start\":41976}]", "bib_author_first_name": "[{\"end\":53690,\"start\":53689},{\"end\":53701,\"start\":53700},{\"end\":53715,\"start\":53714},{\"end\":53729,\"start\":53728},{\"end\":53731,\"start\":53730},{\"end\":53741,\"start\":53740},{\"end\":53917,\"start\":53916},{\"end\":53928,\"start\":53927},{\"end\":53942,\"start\":53941},{\"end\":53956,\"start\":53955},{\"end\":53958,\"start\":53957},{\"end\":53968,\"start\":53967},{\"end\":54097,\"start\":54096},{\"end\":54099,\"start\":54098},{\"end\":54113,\"start\":54112},{\"end\":54300,\"start\":54299},{\"end\":54309,\"start\":54308},{\"end\":54478,\"start\":54477},{\"end\":54487,\"start\":54486},{\"end\":54495,\"start\":54494},{\"end\":54506,\"start\":54505},{\"end\":54655,\"start\":54654},{\"end\":54661,\"start\":54660},{\"end\":54826,\"start\":54825},{\"end\":54835,\"start\":54834},{\"end\":54837,\"start\":54836},{\"end\":54847,\"start\":54846},{\"end\":55033,\"start\":55032},{\"end\":55044,\"start\":55043},{\"end\":55046,\"start\":55045},{\"end\":55056,\"start\":55055},{\"end\":55067,\"start\":55066},{\"end\":55182,\"start\":55181},{\"end\":55190,\"start\":55189},{\"end\":55202,\"start\":55201},{\"end\":55204,\"start\":55203},{\"end\":55213,\"start\":55212},{\"end\":55351,\"start\":55350},{\"end\":55358,\"start\":55357},{\"end\":55364,\"start\":55363},{\"end\":55370,\"start\":55369},{\"end\":55376,\"start\":55375},{\"end\":55529,\"start\":55528},{\"end\":55537,\"start\":55536},{\"end\":55539,\"start\":55538},{\"end\":55545,\"start\":55544},{\"end\":55554,\"start\":55553},{\"end\":55560,\"start\":55559},{\"end\":55732,\"start\":55731},{\"end\":55739,\"start\":55738},{\"end\":55745,\"start\":55744},{\"end\":55752,\"start\":55751},{\"end\":55759,\"start\":55758},{\"end\":55861,\"start\":55860},{\"end\":55868,\"start\":55867},{\"end\":55875,\"start\":55874},{\"end\":55882,\"start\":55881},{\"end\":56056,\"start\":56055},{\"end\":56064,\"start\":56063},{\"end\":56066,\"start\":56065},{\"end\":56072,\"start\":56071},{\"end\":56080,\"start\":56079},{\"end\":56089,\"start\":56088},{\"end\":56243,\"start\":56242},{\"end\":56253,\"start\":56252},{\"end\":56265,\"start\":56264},{\"end\":56272,\"start\":56271},{\"end\":56274,\"start\":56273},{\"end\":56420,\"start\":56419},{\"end\":56422,\"start\":56421},{\"end\":56434,\"start\":56433},{\"end\":56440,\"start\":56439},{\"end\":56447,\"start\":56446},{\"end\":56458,\"start\":56457},{\"end\":56465,\"start\":56464},{\"end\":56636,\"start\":56635},{\"end\":56645,\"start\":56644},{\"end\":56651,\"start\":56650},{\"end\":56657,\"start\":56656},{\"end\":56664,\"start\":56663},{\"end\":56840,\"start\":56839},{\"end\":56847,\"start\":56846},{\"end\":56854,\"start\":56853},{\"end\":56861,\"start\":56860},{\"end\":57006,\"start\":57005},{\"end\":57012,\"start\":57011},{\"end\":57018,\"start\":57017},{\"end\":57036,\"start\":57035},{\"end\":57038,\"start\":57037},{\"end\":57047,\"start\":57046},{\"end\":57153,\"start\":57152},{\"end\":57159,\"start\":57158},{\"end\":57165,\"start\":57164},{\"end\":57300,\"start\":57299},{\"end\":57306,\"start\":57305},{\"end\":57312,\"start\":57311},{\"end\":57318,\"start\":57317},{\"end\":57406,\"start\":57405},{\"end\":57419,\"start\":57418},{\"end\":57434,\"start\":57433},{\"end\":57444,\"start\":57443},{\"end\":57452,\"start\":57451},{\"end\":57462,\"start\":57461},{\"end\":57464,\"start\":57463},{\"end\":57477,\"start\":57476},{\"end\":57487,\"start\":57486},{\"end\":57501,\"start\":57500},{\"end\":57514,\"start\":57513},{\"end\":57612,\"start\":57611},{\"end\":57622,\"start\":57621},{\"end\":57629,\"start\":57628},{\"end\":57631,\"start\":57630},{\"end\":57643,\"start\":57642},{\"end\":57651,\"start\":57650},{\"end\":57660,\"start\":57659},{\"end\":57681,\"start\":57680},{\"end\":57698,\"start\":57697},{\"end\":57712,\"start\":57711},{\"end\":57730,\"start\":57729},{\"end\":57835,\"start\":57834},{\"end\":57851,\"start\":57850},{\"end\":57864,\"start\":57863},{\"end\":57878,\"start\":57877},{\"end\":57971,\"start\":57970},{\"end\":57978,\"start\":57977},{\"end\":57986,\"start\":57985},{\"end\":58211,\"start\":58210},{\"end\":58223,\"start\":58222},{\"end\":58233,\"start\":58232},{\"end\":58242,\"start\":58241},{\"end\":58244,\"start\":58243},{\"end\":58384,\"start\":58383},{\"end\":58386,\"start\":58385},{\"end\":58396,\"start\":58395},{\"end\":58405,\"start\":58404},{\"end\":58414,\"start\":58413},{\"end\":58491,\"start\":58490},{\"end\":58499,\"start\":58498},{\"end\":58501,\"start\":58500},{\"end\":58509,\"start\":58508},{\"end\":58511,\"start\":58510},{\"end\":58518,\"start\":58517},{\"end\":58525,\"start\":58524},{\"end\":58527,\"start\":58526},{\"end\":58536,\"start\":58535},{\"end\":58538,\"start\":58537},{\"end\":58645,\"start\":58644},{\"end\":58655,\"start\":58654},{\"end\":58663,\"start\":58662},{\"end\":58673,\"start\":58672},{\"end\":58959,\"start\":58958},{\"end\":58965,\"start\":58964},{\"end\":58972,\"start\":58971},{\"end\":58978,\"start\":58977},{\"end\":58996,\"start\":58995},{\"end\":58998,\"start\":58997}]", "bib_author_last_name": "[{\"end\":53698,\"start\":53691},{\"end\":53712,\"start\":53702},{\"end\":53726,\"start\":53716},{\"end\":53738,\"start\":53732},{\"end\":53749,\"start\":53742},{\"end\":53925,\"start\":53918},{\"end\":53939,\"start\":53929},{\"end\":53953,\"start\":53943},{\"end\":53965,\"start\":53959},{\"end\":53976,\"start\":53969},{\"end\":54110,\"start\":54100},{\"end\":54119,\"start\":54114},{\"end\":54306,\"start\":54301},{\"end\":54317,\"start\":54310},{\"end\":54484,\"start\":54479},{\"end\":54492,\"start\":54488},{\"end\":54503,\"start\":54496},{\"end\":54514,\"start\":54507},{\"end\":54658,\"start\":54656},{\"end\":54665,\"start\":54662},{\"end\":54832,\"start\":54827},{\"end\":54844,\"start\":54838},{\"end\":54858,\"start\":54848},{\"end\":55041,\"start\":55034},{\"end\":55053,\"start\":55047},{\"end\":55064,\"start\":55057},{\"end\":55075,\"start\":55068},{\"end\":55187,\"start\":55183},{\"end\":55199,\"start\":55191},{\"end\":55210,\"start\":55205},{\"end\":55217,\"start\":55214},{\"end\":55355,\"start\":55352},{\"end\":55361,\"start\":55359},{\"end\":55367,\"start\":55365},{\"end\":55373,\"start\":55371},{\"end\":55379,\"start\":55377},{\"end\":55534,\"start\":55530},{\"end\":55542,\"start\":55540},{\"end\":55551,\"start\":55546},{\"end\":55557,\"start\":55555},{\"end\":55564,\"start\":55561},{\"end\":55736,\"start\":55733},{\"end\":55742,\"start\":55740},{\"end\":55749,\"start\":55746},{\"end\":55756,\"start\":55753},{\"end\":55763,\"start\":55760},{\"end\":55865,\"start\":55862},{\"end\":55872,\"start\":55869},{\"end\":55879,\"start\":55876},{\"end\":55887,\"start\":55883},{\"end\":56061,\"start\":56057},{\"end\":56069,\"start\":56067},{\"end\":56077,\"start\":56073},{\"end\":56086,\"start\":56081},{\"end\":56094,\"start\":56090},{\"end\":56250,\"start\":56244},{\"end\":56262,\"start\":56254},{\"end\":56269,\"start\":56266},{\"end\":56281,\"start\":56275},{\"end\":56431,\"start\":56423},{\"end\":56437,\"start\":56435},{\"end\":56444,\"start\":56441},{\"end\":56455,\"start\":56448},{\"end\":56462,\"start\":56459},{\"end\":56473,\"start\":56466},{\"end\":56642,\"start\":56637},{\"end\":56648,\"start\":56646},{\"end\":56654,\"start\":56652},{\"end\":56661,\"start\":56658},{\"end\":56670,\"start\":56665},{\"end\":56844,\"start\":56841},{\"end\":56851,\"start\":56848},{\"end\":56858,\"start\":56855},{\"end\":56865,\"start\":56862},{\"end\":57009,\"start\":57007},{\"end\":57015,\"start\":57013},{\"end\":57033,\"start\":57019},{\"end\":57044,\"start\":57039},{\"end\":57054,\"start\":57048},{\"end\":57156,\"start\":57154},{\"end\":57162,\"start\":57160},{\"end\":57168,\"start\":57166},{\"end\":57303,\"start\":57301},{\"end\":57309,\"start\":57307},{\"end\":57315,\"start\":57313},{\"end\":57321,\"start\":57319},{\"end\":57416,\"start\":57407},{\"end\":57431,\"start\":57420},{\"end\":57441,\"start\":57435},{\"end\":57449,\"start\":57445},{\"end\":57459,\"start\":57453},{\"end\":57474,\"start\":57465},{\"end\":57484,\"start\":57478},{\"end\":57498,\"start\":57488},{\"end\":57511,\"start\":57502},{\"end\":57524,\"start\":57515},{\"end\":57619,\"start\":57613},{\"end\":57626,\"start\":57623},{\"end\":57640,\"start\":57632},{\"end\":57648,\"start\":57644},{\"end\":57657,\"start\":57652},{\"end\":57678,\"start\":57661},{\"end\":57695,\"start\":57682},{\"end\":57709,\"start\":57699},{\"end\":57727,\"start\":57713},{\"end\":57738,\"start\":57731},{\"end\":57848,\"start\":57836},{\"end\":57861,\"start\":57852},{\"end\":57875,\"start\":57865},{\"end\":57888,\"start\":57879},{\"end\":57975,\"start\":57972},{\"end\":57983,\"start\":57979},{\"end\":57990,\"start\":57987},{\"end\":58220,\"start\":58212},{\"end\":58230,\"start\":58224},{\"end\":58239,\"start\":58234},{\"end\":58250,\"start\":58245},{\"end\":58393,\"start\":58387},{\"end\":58402,\"start\":58397},{\"end\":58411,\"start\":58406},{\"end\":58423,\"start\":58415},{\"end\":58496,\"start\":58492},{\"end\":58506,\"start\":58502},{\"end\":58515,\"start\":58512},{\"end\":58522,\"start\":58519},{\"end\":58533,\"start\":58528},{\"end\":58543,\"start\":58539},{\"end\":58652,\"start\":58646},{\"end\":58660,\"start\":58656},{\"end\":58670,\"start\":58664},{\"end\":58680,\"start\":58674},{\"end\":58962,\"start\":58960},{\"end\":58969,\"start\":58966},{\"end\":58975,\"start\":58973},{\"end\":58993,\"start\":58979},{\"end\":59004,\"start\":58999}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":53779,\"start\":53554},{\"attributes\":{\"id\":\"b1\"},\"end\":54008,\"start\":53781},{\"attributes\":{\"id\":\"b2\"},\"end\":54177,\"start\":54010},{\"attributes\":{\"id\":\"b3\"},\"end\":54378,\"start\":54179},{\"attributes\":{\"id\":\"b4\"},\"end\":54571,\"start\":54380},{\"attributes\":{\"id\":\"b5\"},\"end\":54722,\"start\":54573},{\"attributes\":{\"id\":\"b6\"},\"end\":54916,\"start\":54724},{\"attributes\":{\"id\":\"b7\"},\"end\":55102,\"start\":54918},{\"attributes\":{\"id\":\"b8\"},\"end\":55283,\"start\":55104},{\"attributes\":{\"id\":\"b9\"},\"end\":55405,\"start\":55285},{\"attributes\":{\"id\":\"b10\"},\"end\":55591,\"start\":55407},{\"attributes\":{\"id\":\"b11\"},\"end\":55775,\"start\":55593},{\"attributes\":{\"id\":\"b12\"},\"end\":55954,\"start\":55777},{\"attributes\":{\"id\":\"b13\"},\"end\":56137,\"start\":55956},{\"attributes\":{\"id\":\"b14\"},\"end\":56308,\"start\":56139},{\"attributes\":{\"id\":\"b15\"},\"end\":56531,\"start\":56310},{\"attributes\":{\"id\":\"b16\"},\"end\":56732,\"start\":56533},{\"attributes\":{\"id\":\"b17\"},\"end\":56889,\"start\":56734},{\"attributes\":{\"id\":\"b18\"},\"end\":57097,\"start\":56891},{\"attributes\":{\"id\":\"b19\"},\"end\":57196,\"start\":57099},{\"attributes\":{\"id\":\"b20\"},\"end\":57346,\"start\":57198},{\"attributes\":{\"id\":\"b21\"},\"end\":57541,\"start\":57348},{\"attributes\":{\"id\":\"b22\"},\"end\":57774,\"start\":57543},{\"attributes\":{\"id\":\"b23\"},\"end\":57913,\"start\":57776},{\"attributes\":{\"id\":\"b24\"},\"end\":58121,\"start\":57915},{\"attributes\":{\"id\":\"b25\"},\"end\":58317,\"start\":58123},{\"attributes\":{\"id\":\"b26\"},\"end\":58429,\"start\":58319},{\"attributes\":{\"id\":\"b27\"},\"end\":58549,\"start\":58431},{\"attributes\":{\"id\":\"b28\"},\"end\":58868,\"start\":58551},{\"attributes\":{\"id\":\"b29\"},\"end\":59153,\"start\":58870}]", "bib_title": "[{\"end\":54094,\"start\":54010},{\"end\":54297,\"start\":54179},{\"end\":54475,\"start\":54380},{\"end\":54652,\"start\":54573},{\"end\":54823,\"start\":54724},{\"end\":55030,\"start\":54918},{\"end\":55179,\"start\":55104},{\"end\":55526,\"start\":55407},{\"end\":55858,\"start\":55777},{\"end\":56240,\"start\":56139},{\"end\":56417,\"start\":56310},{\"end\":56633,\"start\":56533},{\"end\":57003,\"start\":56891},{\"end\":57150,\"start\":57099},{\"end\":57403,\"start\":57348},{\"end\":57609,\"start\":57543},{\"end\":57968,\"start\":57915},{\"end\":58208,\"start\":58123},{\"end\":58642,\"start\":58551},{\"end\":58956,\"start\":58870}]", "bib_author": "[{\"end\":53700,\"start\":53689},{\"end\":53714,\"start\":53700},{\"end\":53728,\"start\":53714},{\"end\":53740,\"start\":53728},{\"end\":53751,\"start\":53740},{\"end\":53927,\"start\":53916},{\"end\":53941,\"start\":53927},{\"end\":53955,\"start\":53941},{\"end\":53967,\"start\":53955},{\"end\":53978,\"start\":53967},{\"end\":54112,\"start\":54096},{\"end\":54121,\"start\":54112},{\"end\":54308,\"start\":54299},{\"end\":54319,\"start\":54308},{\"end\":54486,\"start\":54477},{\"end\":54494,\"start\":54486},{\"end\":54505,\"start\":54494},{\"end\":54516,\"start\":54505},{\"end\":54660,\"start\":54654},{\"end\":54667,\"start\":54660},{\"end\":54834,\"start\":54825},{\"end\":54846,\"start\":54834},{\"end\":54860,\"start\":54846},{\"end\":55043,\"start\":55032},{\"end\":55055,\"start\":55043},{\"end\":55066,\"start\":55055},{\"end\":55077,\"start\":55066},{\"end\":55189,\"start\":55181},{\"end\":55201,\"start\":55189},{\"end\":55212,\"start\":55201},{\"end\":55219,\"start\":55212},{\"end\":55357,\"start\":55350},{\"end\":55363,\"start\":55357},{\"end\":55369,\"start\":55363},{\"end\":55375,\"start\":55369},{\"end\":55381,\"start\":55375},{\"end\":55536,\"start\":55528},{\"end\":55544,\"start\":55536},{\"end\":55553,\"start\":55544},{\"end\":55559,\"start\":55553},{\"end\":55566,\"start\":55559},{\"end\":55738,\"start\":55731},{\"end\":55744,\"start\":55738},{\"end\":55751,\"start\":55744},{\"end\":55758,\"start\":55751},{\"end\":55765,\"start\":55758},{\"end\":55867,\"start\":55860},{\"end\":55874,\"start\":55867},{\"end\":55881,\"start\":55874},{\"end\":55889,\"start\":55881},{\"end\":56063,\"start\":56055},{\"end\":56071,\"start\":56063},{\"end\":56079,\"start\":56071},{\"end\":56088,\"start\":56079},{\"end\":56096,\"start\":56088},{\"end\":56252,\"start\":56242},{\"end\":56264,\"start\":56252},{\"end\":56271,\"start\":56264},{\"end\":56283,\"start\":56271},{\"end\":56433,\"start\":56419},{\"end\":56439,\"start\":56433},{\"end\":56446,\"start\":56439},{\"end\":56457,\"start\":56446},{\"end\":56464,\"start\":56457},{\"end\":56475,\"start\":56464},{\"end\":56644,\"start\":56635},{\"end\":56650,\"start\":56644},{\"end\":56656,\"start\":56650},{\"end\":56663,\"start\":56656},{\"end\":56672,\"start\":56663},{\"end\":56846,\"start\":56839},{\"end\":56853,\"start\":56846},{\"end\":56860,\"start\":56853},{\"end\":56867,\"start\":56860},{\"end\":57011,\"start\":57005},{\"end\":57017,\"start\":57011},{\"end\":57035,\"start\":57017},{\"end\":57046,\"start\":57035},{\"end\":57056,\"start\":57046},{\"end\":57158,\"start\":57152},{\"end\":57164,\"start\":57158},{\"end\":57170,\"start\":57164},{\"end\":57305,\"start\":57299},{\"end\":57311,\"start\":57305},{\"end\":57317,\"start\":57311},{\"end\":57323,\"start\":57317},{\"end\":57418,\"start\":57405},{\"end\":57433,\"start\":57418},{\"end\":57443,\"start\":57433},{\"end\":57451,\"start\":57443},{\"end\":57461,\"start\":57451},{\"end\":57476,\"start\":57461},{\"end\":57486,\"start\":57476},{\"end\":57500,\"start\":57486},{\"end\":57513,\"start\":57500},{\"end\":57526,\"start\":57513},{\"end\":57621,\"start\":57611},{\"end\":57628,\"start\":57621},{\"end\":57642,\"start\":57628},{\"end\":57650,\"start\":57642},{\"end\":57659,\"start\":57650},{\"end\":57680,\"start\":57659},{\"end\":57697,\"start\":57680},{\"end\":57711,\"start\":57697},{\"end\":57729,\"start\":57711},{\"end\":57740,\"start\":57729},{\"end\":57850,\"start\":57834},{\"end\":57863,\"start\":57850},{\"end\":57877,\"start\":57863},{\"end\":57890,\"start\":57877},{\"end\":57977,\"start\":57970},{\"end\":57985,\"start\":57977},{\"end\":57992,\"start\":57985},{\"end\":58222,\"start\":58210},{\"end\":58232,\"start\":58222},{\"end\":58241,\"start\":58232},{\"end\":58252,\"start\":58241},{\"end\":58395,\"start\":58383},{\"end\":58404,\"start\":58395},{\"end\":58413,\"start\":58404},{\"end\":58425,\"start\":58413},{\"end\":58498,\"start\":58490},{\"end\":58508,\"start\":58498},{\"end\":58517,\"start\":58508},{\"end\":58524,\"start\":58517},{\"end\":58535,\"start\":58524},{\"end\":58545,\"start\":58535},{\"end\":58654,\"start\":58644},{\"end\":58662,\"start\":58654},{\"end\":58672,\"start\":58662},{\"end\":58682,\"start\":58672},{\"end\":58964,\"start\":58958},{\"end\":58971,\"start\":58964},{\"end\":58977,\"start\":58971},{\"end\":58995,\"start\":58977},{\"end\":59006,\"start\":58995}]", "bib_venue": "[{\"end\":58112,\"start\":58053},{\"end\":58860,\"start\":58771},{\"end\":59144,\"start\":59072},{\"end\":53687,\"start\":53554},{\"end\":53914,\"start\":53781},{\"end\":54169,\"start\":54145},{\"end\":54370,\"start\":54349},{\"end\":54562,\"start\":54546},{\"end\":54713,\"start\":54697},{\"end\":54909,\"start\":54885},{\"end\":55093,\"start\":55077},{\"end\":55275,\"start\":55244},{\"end\":55348,\"start\":55285},{\"end\":55582,\"start\":55566},{\"end\":55729,\"start\":55593},{\"end\":55946,\"start\":55914},{\"end\":56053,\"start\":55956},{\"end\":56299,\"start\":56283},{\"end\":56523,\"start\":56499},{\"end\":56723,\"start\":56702},{\"end\":56837,\"start\":56734},{\"end\":57087,\"start\":57071},{\"end\":57188,\"start\":57170},{\"end\":57297,\"start\":57198},{\"end\":57532,\"start\":57526},{\"end\":57765,\"start\":57759},{\"end\":57832,\"start\":57776},{\"end\":58051,\"start\":57992},{\"end\":58309,\"start\":58277},{\"end\":58381,\"start\":58319},{\"end\":58488,\"start\":58431},{\"end\":58769,\"start\":58682},{\"end\":59070,\"start\":59006}]"}}}, "year": 2023, "month": 12, "day": 17}