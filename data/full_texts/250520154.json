{"id": 250520154, "updated": "2022-10-14 13:24:27.338", "metadata": {"title": "Expressive Talking Head Generation with Granular Audio-Visual Control", "authors": "[{\"first\":\"Borong\",\"last\":\"Liang\",\"middle\":[]},{\"first\":\"Yan\",\"last\":\"Pan\",\"middle\":[]},{\"first\":\"Zhizhi\",\"last\":\"Guo\",\"middle\":[]},{\"first\":\"Hang\",\"last\":\"Zhou\",\"middle\":[]},{\"first\":\"Zhibin\",\"last\":\"Hong\",\"middle\":[]},{\"first\":\"Xiaoguang\",\"last\":\"Han\",\"middle\":[]},{\"first\":\"Junyu\",\"last\":\"Han\",\"middle\":[]},{\"first\":\"Jingtuo\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Errui\",\"last\":\"Ding\",\"middle\":[]},{\"first\":\"Jingdong\",\"last\":\"Wang\",\"middle\":[]}]", "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Generating expressive talking heads is essential for creating virtual humans. However, existing one- or few-shot methods focus on lip-sync and head motion, ignoring the emotional expressions that make talking faces realistic. In this paper, we propose the Granularly Controlled Audio-Visual Talking Heads (GC-AVT), which controls lip movements, head poses, and facial expressions of a talking head in a granular manner. Our insight is to decouple the audio-visual driving sources through prior-based pre-processing designs. Detailedly, we disassemble the driving image into three complementary parts including: 1) a cropped mouth that facilitates lip-sync; 2) a masked head that implicitly learns pose; and 3) the upper face which works corporately and complementarily with a time-shifted mouth to contribute the expression. Interestingly, the encoded features from the three sources are integrally balanced through reconstruction training. Extensive experiments show that our method generates expressive faces with not only synced mouth shapes, controllable poses, but precisely animated emotional expressions as well.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/LiangPGZHHHLD022", "doi": "10.1109/cvpr52688.2022.00338"}}, "content": {"source": {"pdf_hash": "b742609818d0e1a2774d1deb06ac24dab192aba1", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "b4fb6fe524d8e8c7f3742535066c904d34f95dbd", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/b742609818d0e1a2774d1deb06ac24dab192aba1.txt", "contents": "\nExpressive Talking Head Generation with Granular Audio-Visual Control\n\n\nBorong Liang liangborong@baidu.com \nDepartment of Computer Vision Technology (VIS)\nBaidu Inc\n\n\nYan Pan yanpan@link. \nSSE\nCUHK-Shenzhen\n3 FNii\n\nPose Source Synced Video Expression Source Identity Reference\nCUHK-Shenzhen\n\n\nZhizhi Guo guozhizhi@baidu.com \nDepartment of Computer Vision Technology (VIS)\nBaidu Inc\n\n\n\u2020 Hang Zhou \nDepartment of Computer Vision Technology (VIS)\nBaidu Inc\n\n\n\u2020 Zhibin hongzhibin@baidu.com \nDepartment of Computer Vision Technology (VIS)\nBaidu Inc\n\n\nHong \nXiaoguang Han hanxiaoguang@cuhk.edu.cn. \nSSE\nCUHK-Shenzhen\n3 FNii\n\nPose Source Synced Video Expression Source Identity Reference\nCUHK-Shenzhen\n\n\nJunyu Han hanjunyu@baidu.com \nDepartment of Computer Vision Technology (VIS)\nBaidu Inc\n\n\nJingtuo Liu liujingtuo@baidu.com \nDepartment of Computer Vision Technology (VIS)\nBaidu Inc\n\n\nErrui Ding dingerrui@baidu.com \nDepartment of Computer Vision Technology (VIS)\nBaidu Inc\n\n\nJingdong Wang wangjingdong@baidu.com \nDepartment of Computer Vision Technology (VIS)\nBaidu Inc\n\n\nExpressive Talking Head Generation with Granular Audio-Visual Control\n10.1109/CVPR52688.2022.00338\nFigure 1. Example animations generated by our Granularly Controlled Audio-Visual Talking Heads (GC-AVT). Given a reference identity frame, GC-AVT generates audio-visual driven talking head video with other emotional expression source and pose source video frames independently. The mouth shapes of driven results are matched with the synced video (on top row), and the expressions of driven results are matched with the expression source (on bottom row) while the poses are matched with the pose source (left column).AbstractGenerating expressive talking heads is essential for creating virtual humans. However, existing one-or few-shot methods focus on lip-sync and head motion, ignoring the emotional expressions that make talking faces realistic. In this paper, we propose the Granularly Controlled Audio-Visual Talking Heads (GC-AVT), which controls lip movements, head poses, and facial expressions of a talking head in a granular manner. Our insight is to decouple the audiovisual driving sources through prior-based pre-processing designs. Detailedly, we disassemble the driving image into three complementary parts including: 1) a cropped mouth * Equal contribution. \u2020  Corresponding authors. that facilitates lip-sync; 2) a masked head that implicitly learns pose; and 3) the upper face which works corporately and complementarily with a time-shifted mouth to contribute the expression. Interestingly, the encoded features from the three sources are integrally balanced through reconstruction training. Extensive experiments show that our method generates expressive faces with not only synced mouth shapes, controllable poses, but precisely animated emotional expressions as well.\n\nIntroduction\n\nWith the rapid development of automatic video generation technology, the task of audio-driven talking head gen-3377 eration has drawn much attention due to its extensive realworld applications such as creating virtual anchors, digital avatars, and animated movies. In order to achieve convenient deployment with a generalized model, researchers have proposed to drive only a single or a few frames to talk with audios [8,10,46,48,50,51]. While accurate lip sync has been almost realized, the ability to control the facial expression, which is crucial for creating human-like talking heads, has not been fully explored.\n\nA great number of previous methods focus only on the lip-sync accuracy with audios [7,10,27,30,48]. More recently, researchers propose to generate rhythmic [6,42] or changeable head poses [50] along with talking heads. However, their methods cannot change detailed expressions such as eyebrows. On the other hand, methods that generate emotional dynamics [18,21,35] are basically personspecific, i.e., one model has to be trained for one specific person. Moreover, their models rely on labeled emotional data, thus can only cover limited expressions.\n\nIn real-world scenes, people could speak the same content with fixed stress and intonation but flexible expressions and head motions. Inspired by this observation, we argue that the generation of emotional expressions can be divided from mouth movements and poses, that the three of them could be controlled independently. This is technically challenging for nearly all existing models. 1) For methods predicting intermediate structural representation such as 2D or 3D landmarks [6,8,51], the above information is inherently entangled. Even mainstream 3D face models, such as 3D Face Morphable Model (3DMM) [1], represent mouth movement and facial expression within the same parameter. Besides, the accuracy of intermediate representations will be compromised under extreme cases. 2) For latent feature learning methods [3,10,48,50], the expression information can hardly be individually distilled, and current works [3,50] do not support disentangled expression and mouth control.\n\nIn this work, we propose Granularly Controlled Audio-Visual Talking Heads (GC-AVT), which drives a portrait head from a higher level of granularity. Avoid using any intermediate representation, our method is pure learning-based without specific emotion labels. The most intriguing property of our model is the independent facial control from three complementary perspectives: speech content, head pose, and emotional expression, which makes our talking head more expressive. As shown in Fig 1, while the head pose and expression information are derived from visual sources, the mouth movement can be decided by either audio or visual information.\n\nOur insight is to explicitly divide the driving information into granular parts through delicate pre-processing designs. Different from previous methods that learn nonidentity representation in a holistic view [3,50], we argue that all information can be separately extracted in a complementary manner. We analyze the key-factors that affect each desired facial area and adopt different types of masking and augmentation schemes. Three functional inputs are thus formulated. Audio input associates explicitly with the mouth shapes, thus the temporal alignment between speech and cropped mouths is leveraged to account for the speech content information. Then we expect that the emotional expressions could be driven by additional visual sources. In particular, we factorize the emotion of a whole face into an upper-face and a time-shifted mouth. The two of them are seamlessly collaborated together to provide precise expressions. Finally, an implicit pose code is devised from the whole face. Three encoders are leveraged for the individual information extraction, and a style-based generator processes them through reconstruction training. Experiments demonstrate that our method manages to generate an expressive talking head with the precise mouth shape, head pose, and emotional expression control.\n\nThe contributions of this work are summarized as follows: (1) We propose the Granularly Controlled Audio-Visual Talking Heads (GC-AVT) System, which generates expressive portrait videos from the granular control of pose, audio, and an expression video. (2) We identify three delicate pre-processing procedures for handling the three different control sources. (3) By integrating audio-visual synchronization, our system generates accurate mouth movements that can be driven by either audio or video.\n\n\nRelated work\n\nAudio-Driven Talking Head Generation. The task of animating virtual humans [5,22,23,50,52] from arbitrary speech sequence has drawn considerable attention in both computer vision and graphics, among which talking face generation is particular important. Earlier works [32,36,37] require a large number of video footage of a target person by modeling the mouth area through either retrieval or graphics-based methods. With the develop of deep learning, a number of works leverage structural information within GAN-based pipelines [18,25,29,33] to generate person-specific high-quality results. Other researchers tend to seek speaker-independent settings that can address all identities through one or more framework references [8,10,30,48,50,51]. Chung et al. [10] firstly propose an end-to-end reconstruction-based network in an image-toimage translation manner based on audios. Then [48] uses adversarial training to further separate identity from word. Wav2Lip [27] particularly proposes to inpaint the mouth areas. The basic idea behind these reconstruction-based methods is to synchronize mouth motion in video with speech content in audio. Facial expressions and head poses, on the other hand, are neglected.\n\nMore recently a few methods [6,18,31,50,51] have been proposed not only to solve the problem of synchronization but add extra components to create a more vivid talking head. Zhou et al. [48] and Yi et al. [51] models rhythmic head motions with 3D representations. PC-AVS [50] leverage another pose source video to control head poses while driving talking faces with audio sequences. [46] produces the animation parameters of mouth, eyebrow and head pose simultaneously and synthesizes talking face videos from dense flow. Particularly, Wang et al, [35] and Ji et al. [18] propose to alter emotions, but one model has to be trained on one person. Controlling different attributes of the portrait video simultaneously in the one-shot manner has been proven to be difficult.\n\nVisually Driven Face Reenactment. The task of sace reenactment aims to generate talking head videos by transferring the facial dynamics from a different actor's video.\n\nMost techniques rely on structural information such as landmarks [16,40,43,44] or 3D models [4,14,19,20,34,47]. Deep Video Portraits [20] is capable of producing highquality photo-realistic dubbing results. It keeps the target actor's identity and pose while capturing the source actor's facial emotions, but should be trained per target video. Recently, FReeNet [44] utilize a unified landmark converter to transfer facial expressions between identities. Moreover, latent pose descriptors based on the reconstruction losses [3,24] are proposed for cross-person reenactment. These works aim to handle multi-identity face reenactment, and our work expands the task's complexity by involving granularly control.\n\n\nOur Approach\n\nIn this section, we describe our Granularly Controlled Audio-Visual Talking Heads (GC-AVT) system, which encodes the head pose, speech content, emotional expression, together with person identity into latent spaces and generates the driven talking head with either audio or video. First, we briefly introduce the pipeline of our approach in Sec. 3.1. Next, we introduce the prior-based face preprocessing which is crucial for devising independent granular control sources (Sec. 3.2). Finally, we introduce the learning process of the pipeline 3.3.\n\n\nOverall Formulation\n\nThe whole pipeline of our method is illustrated in Fig. 2. We adopt the typical cross-frame self-reconstruction [10,50] setting for training, and expect the driving information of speech content, pose, and expression could originate from completely different videos during inference.\n\nGiven a pre-processed video clip with N frames V = (I 1 , . . . , I N ) and its corresponding audio spectrograms A = (a 1 , . . . , a N ), we sample a set of K frames {I i1 , I i2 , ..., I iK } from V randomly as the representatives for identity information. This representation is supervised by a simple identity loss [3]. Then we randomly sample one frame I k from V as the source of all driving conditions (i.e. expression, pose, and speech content). Our goal is to recover I k based on the corresponding audio spectrogram a k and the desired information from I k . This is inherently difficult for two reasons: (1) The input source I k is also the target, the network might take a shortcut during the reconstruction.\n\n(2) The granular information desired is entangled together and difficult to discriminate and extract.\n\nTo this end, we propose that each desired driving part can be specifically identified from the input image domain. Specifically, I k is decomposed into three complementary parts through delicate prior-based pre-processing. As the identity information also requires modeling, a total of four visual encoders independently encode the identity, head pose, emotional expression, and speech content (mouth shape) information into latent features named f id , f p , f e , f v c respectively. Specifically, f v c is further leveraged to assist the learning of the audio feature f a c . The two features should lie in a same latent space. At last, we expect that one generator G is capable of handling all information. The features can be assembled together as the overall\naudio-based feature f a all = {f id , f p , f e , f a c } or visual-based feature f v all = {f id , f p , f e , f v c }. They are sent into G for reconstructing I a k \u2032 and I v k \u2032 .\nThe detailed pre-processing steps are described in Sec. 3.2 and the learning objectives are illustrated in Sec. 3.3.\n\n\nPrior-based Pre-Processing\n\nAs stated above, three particular types of pre-processing paradigms are designed based on the prior knowledge of different functional areas of a face. Each of them corresponds to a driving source, representing disentangled information.\n\nWhile detailed pre-processing procedures are different, identity information should be removed from all sources. Specifically, it is achieved by pixel-wise augmentation consisting of color transfer, blurring, sharpening, and JPEG compression. This augmentation is applied to all three preprocessing steps.\n\nOn the other hand, masking is widely applied in our implementations, where the landmarks of I k and the foreground segmentation map are detected. The segmentation map is also used for wiping out background interference. Note that we do not leverage the landmarks as an intermediate representation. They are used only as guidance for data pre-processing, thus we do not suffer from the error accumulation problem caused by inaccurate predictions. Pre-processing for Expression. The extraction of expression information alone without the semantics on mouth shapes has rarely been achieved before. One plausible way is to mask out the mouth based on landmarks around it. This . The data pre-processing sample K + 1 frames from a video frame sequence, one of the selected frames is used to generate training data for pose encoder (Ep), emotional expression encoder (Ee), and content encoder (Ec) through different data augmentation methods, which will be described in Sec. 3.2. The rest K frames are used as input to the identity encoder (Ei) and will be encoded into latent feature f id . The pose encoder and the emotional expression encoder encode the corresponding augmented images into fp and fe respectively. To encode the speech content information, we design a visual-audio synchronization network (Ec and Ea) that encode the visual frame and the audio spectrum into latent feature f v c and f a c . The features are assembled together and fed to the Generator. The learning of the pipeline is described in Sec. 3.3. is to maintain the expressions on the upper face. However, the influence of the mouth cannot be directly ignored. Emotional information also has effects on the mouth, e.g., we can infer that a person is smiling and talking at the same time by looking at the mouth movements only. Our method is built upon the observation that the semantics in mouth shapes change much more rapidly than emotion. For example, a person rarely changes the emotion and even the head pose within one second but could speak several syllables. Thus we argue that a shortly time-shifted frame I k+i could possess the same emotional but different semantic information in mouth shapes with I k .\n3379 E i E e E p E c ! ! \" E a (R, t, s) Random G ! ! # ! $ ! % ! &' ! #(( \" ! ! #$% ! ,\nSpecifically, the mouth areas are cropped out from I k+i . When i is reasonably small, the time-shifted mouth can be seamlessly blended to I k . In this way, the precise expression and emotion information on the mouth are preserved. Furthermore, an additional random rotation is applied for erasing the pose information. Pre-processing for Speech Content. The encoding of speech content information from visual modality is intended as a particular type of guidance for audio information encoding. Specifically, researchers have verified that the intrinsic temporal audio-visual synchronization lies around the mouths [13,27]. Thus we leverage a cropped out mouth of I k . The random rotation is also applied to the speech content processing. Pre-processing for Pose. It is simple and safe to mask out the facial organs on a talking head to represent the head pose information. We also devise the latent pose space as a dimension of 12 and rely solely on networks for learning the implicit pose information in a fully reconstruction-based network as performed in [50].\n\n\nLearning Procedures\n\nExcept for the simple learning objective on the identity features, other learning constraints are designed from two perspectives: 1) The constraints on speech content features which synchronizes audio to the visual modality; 2) the constraints on the reconstructed frames I a k \u2032 and I v k \u2032 (uniformly denoted as I k \u2032 ) that implicitly balance the information within all embeddings. Learning Audio-Visual Synchronization. It has been verified that learning audio-visual synchronization benefits audio-visual cross-generation tasks generation [27,31,[48][49][50], and it would be easier to learn mouth shapes from the visual domain [48].\n\nThus in order to stabilize the training, we prevent the synchronization loss from affecting the visual branch and update the audio branch alone. Detailedly, we adopt softmax contrastive loss. The distances between two features are measured as D(f v c , f a c ) = \nL c = \u2212log[ exp(D(f v c , f a c )) exp(D(f v c , f a c )) + M \u2212 j=1 exp(D(f v\u2212 c(j) , f a c )) ],(1)\nwhere f v\u2212 c(j) denotes the jth negative sample. Reconstruction Objectives. We directly borrow the generator structure from [3], which relies on the AdaIN [17]. Note that the same set of losses are applied to both audio and visual reconstructed images as I a k \u2032 and I v k \u2032 . The reconstruction training is generally supervised by pixel-wise comparing L 1 distances between I \u2032 k and I k . Two VGG-19 models, one pre-trained on ImageNet classification and one on face recognition are leveraged in the perceptual loss manner [26,38], where a total of N vgg feature maps are leveraged. The three loss functions can be written as:\nL L1 = \u2225I k \u2212 I \u2032 k \u2225 1 ,(2)L vgg = Nvgg i=1 \u2225VGG i (I k ) \u2212 VGG i (I \u2032 k )\u2225 1 + Nvgg i=1 \u2225VGG F ace i (I k ) \u2212 VGG F ace i (I \u2032 k )\u2225 1 .(3)\nTo further improve the generation quality, a multi-scale discriminator D with N D layers is involved with the generative adversarial loss:\nL GAN = min G max D N D n=1 (E I k [log D n (I k )] + E f all(k) [log(1 \u2212 D n (I \u2032 k ))],(4)\nThe overall constraints during training can be summarized as:\nL all = L GAN + \u03bb 1 L L1 + \u03bb 2 L vgg + \u03bb 3 L c ,(5)\nwhere the \u03bbs are coefficients. Notably, we not only constrain the embedding space of audio and visual speech content features but also use both of them for reconstruction training. Thus our method supports talking face generation with mouth shapes driven by both an audio clip or a mouth sequence.\n\n\nExperiments\n\n\nExperimental Settings\n\nDataset. Our method is trained on VoxCeleb2 [11] and evaluated on both Voxceleb2 and MEAD [35].\n\n\u2022 VoxCeleb2 [11] is an audio-visual dataset which is popularly used in the area of talking head generation.\n\nWe use the URLs provided by VoxCeleb2 to download the original videos, collecting roughly 2,000 speaker identities for training and 100 for evaluation.\n\n\u2022 MEAD [35] is a high-quality emotional audio-visual dataset with over 30 available actors/actresses and eight emotion categories at three different intensity levels. The frontal-view videos in this dataset are leveraged only for testing.\n\nImplementation Details: All videos are processed at 25 frames per second. For each frame, we detect the face with S 3 FD detector [45], then enlarge the bounding box by 80% to keep the face in the center. The final cropped images are of size 256\u00d7256. We apply the Graphonomy [15] model to get background segmentation and mask out the background in the pre-processing. We retrain a FAN model [2] to get landmarks for each image. Similarly to [50], we process the audios to 16kHz, then convert them to mel-spectrograms with FFT window size 1280, hop length 160 and 80 Mel filter-banks. For each video frame, 0.2s mel-spectrogram with the target frame time-step in the middle are sampled as condition.\n\nIn our method, the ID encoder is a ResNeXt-50 [41] structure. We set K = 8 for the input of the identity encoder and 512 dimension for the identity embedding output. Both of the pose encoder and emotional expression encoder are the MobileNetV2 [28] structure. The pose and emotional expression embedding sizes are 12 and 256 respectively. The content encoder and audio encoder are ResNetSE34 borrowed from [9], each generating a 256-dimension embedding. We train our model for 80 epoch with a minibatch of 16 samples on 32 GB Tesla V100 GPUs. We pretrain the visual-audio synchronization with the contrastive loss L c then joint train the whole pipeline end-to-end. Comparing Methods: Our method focuses on audiodriven talking head generation, thus we mainly compare the audio-driven results of Ours (audio) with state-ofthe-arts audio-driven works [3,27,50]. Wav2Lip [27] is a reconstruction-based method that focuses on producing accurate lip movements; MakeitTalk [51] is based on 3D landmarks for learning personalized head movements under the audio-driven setting. PC-AVS [50] is also a reconstruction-based framework and can generate lip synchronization while controlling pose implicitly. Note that our model could also adopt the visual-driven setting, thus we compare the visual-driven results of Ours (video) with LPD [3], a head reenactment system. We compare all the results generated by non-fine-tuned models directly for fairness.\n\n\nQuantitative Evaluation\n\nEvaluation Metrics: To quantitatively evaluate different methods, we compute four evaluation metrics under the Table 1. The comparison of quantitative results on Voxceleb2 [11] and MEAD [35]. For LMD and LMDm the lower the better, and the higher the better for other metrics. Note that in this comparison the PC-AVS [50] fails on some frames because of the landmark detecting failure and the results of it are just for reference. self-driven setting on the test set of VoxCeleb2. They are: SSIM [39] for generation quality; LMD for mean distance of all landmarks and LMD m for landmarks around the mouths. We also borrow the confidence score Sync conf from SyncNet [12] to evaluate the precision of lip synchronization.\nMethod MEAD VoxCeleb2 SSIM \u2191 LMD \u2193 LMD m \u2193 Sync conf \u2191 SSIM \u2191 LMD \u2193 LMD m \u2193 Sync conf \u2191 Ground\nEvaluation Results: We use a similar experimental setting with PC-AVS [50]. Specifically, we select the first frame of each test video as the identity reference. Then the rest frames are used as the sources of pose, emotional expression and speech information. The audios are used as driving conditions to generate audio-driven results. We calculate the numerical metrics between the generated results and the ground truth.\n\nThe results are shown in Table 1. In this comparison, our GC-AVT achieves comprehensively better results on both datasets. Note that audio-driven methods and visual-driven ones are not directly comparable, so we analysis them separately, and focus more on the audio-driven setting. In terms of the lip sync accuracy, our audio setting achieves a better LMD m than other methods, which proves that we can generate good lip sync quality from one perspective. Though we do not possess the highest confidence score (Sync conf ). Our results are close to the ground truth, which show competitive performance. Note that Wav2Lip directly uses SyncNet in its loss function, thus naturally leading to better results on this metric. Benefited from the pose control and expression manipulation ability, our method is naturally better on the general LMD metric. The SSIM score is suitable for Wav2Lip as their only inpaint missing areas. As for the visual-driven setting, we observe several failure cases in the LPD results, making their LMD and LMD m results lower than ours.\n\n\nQualitative Evaluation\n\nComparisons with Other Methods. The comparing methods do not support granular control, therefore it's unfair to set too detailed sources. Since LPD [3] and PC-AVS [50] can control the head pose of generated video, here we as- sign the pose source, speech content source, and expression source all as one single video denoted as driving source on the Figure 3. Note that MakeitTalk [51] can neither control pose nor generate accurate mouth shapes, thus we neglect its results here.\n\nWe can see that Wav2Lip [27] can only leverage the pose of the original video. Its background will be fixed still when its input is a single image (see demo video). While PC-\n\n\nIdentity Reference\n\nExpression Source Generated Results AVS [50] can mimic the pose of the driving source, its results are not quite precise on certain cases. Both of them can only generate neutral expressions. The pose driving results of LPD [3] are quite close to ours. Both generated results of LPD [3] and Ours (video) have precise head pose with the driving source. It can also be seen from the second column that, our pre-processing scheme enables the successful emotional expression transfer from the source video to our results. While such information is neglected in LPD.\n\n\nContent Source\n\nIn terms of the lip sync accuracy, we can see that both our visual-and audio-driven results generate high-fidelity mouth shapes which are aligned with the driving source and outperforms the results of PC-AVS. Evaluation on Emotional Expression Control. A remarkable feature of our GC-AVT is that we can control the emotional expression independently from the semantic mouth shapes and head poses. We visualize the independent control of emotional expression, and speech content in Figure 4. We frontalize all the generated results. As can be seen that in the process of independent control, the emotional expression and the speech content can be well decoupled.  Table 2. Our GC-AVT outperforms previous methods on the expression realness and richness by a large margin, which verifies the effectiveness of our method in handling emotional expressions. And our results are apparently more vivid than others. Although we do not score the best in lip sync quality, the results between the three methods are very close and can be regarded as comparable.\n\n\nAblation Study\n\nIn this section, we study the effects of the losses setting and the necessity of time-shift operation. Note that the experiments are carried out on the VoxCeleb2 dataset with our audio-driven setting.\n\nFor loss setting, we study the effects of VGG loss, VG-GFace loss and Contrastive loss. The results are listed in Table 3, where w/o VGG means without both VGG loss and VGGFace loss. The contrastive loss is used for audio-visual synchronization. In order to verify the effects of contrastive loss, we test the LMD, LMD m and Sync conf in Voxceleb2  testset. As demonstrated in Table 3, the performances of LMD, LMD m and Sync conf all get worse prominently. Besides, we visualize the results of the ablation studies in Figure 6. Without the perceptual losses such as VGG loss and VGGFace loss, the quality of generated images are obviously poor, and the performance of attribute control is also worse than the results of our complete setting. The speech content driving results are affected when we remove the contrastive loss. The speech driven results are not synced with the driving source. Without the time-shift operation the speech driven results is affected but the quality of the generated image is hardly affected.\n\nWe further show the ablation studies on the mask designs. Experiments are carried out on the following settings shown in Fig. 5: (a) no masks are applied; (b) no mask on mouth; (c) no mask on expression; (d) smaller mouth area; and (e) time-shifted mouth on expression. Setting (a), (b), (c) would confuse the training procedure of the networks, which eventually leads to the loss of the speech content control ability. The results of setting (a) -(f) are shown in the figure below. Our setting e) achieves the best results. The qualitative and quantitative comparisons will be added to the final version.\n\n\nConclusion and Discussion\n\nConclusion. In this paper, we propose the Granularly Controlled Audio-Visual Talking Heads (GC-AVT)  Figure 6. Ablation study for losses setting with visual results. As shown in second row and the fourth row, without either VGGFace loss or VGG loss, the quality of generated results decreased significantly. The speech content driving results are affected without the contrastive loss as shown in the third row.\n\npipeline. By explicitly divide the driving information into granular parts through delicate pre-processing designs, GC-AVT supports talking head generation controlled from the perspectives of speech-content, pose, expressions. To the best of our knowledge, such property has rarely been achieved before. Moreover, it supports accurate lip sync from both audio and visual inputs, which enlarges applications of our system.\n\nLimitations. One of most important limitations is that the backgrounds are masked our in our method, thus we cannot handle sophisticated background changes. Moreover, our method cannot generate high resolution results.\n\nEthical Statements. Although animating talking heads has extensive applications, it might be misused for deepfake creation and media manipulation. We will restrict the usage of our model and share it with the deepfake detection community.\n\n'Figure 2 .\n2!\"# : )*+ ,-.. ' $ : /-0123.1456 ,-.. ' %&$ : 7)) ,-.., '8 ,-.. 9: pixel-wise *:; The proposed architecture of our Granularly Controlled Audio-Visual Talking Heads (GC-AVT)\n\nc\n| , where f v c and f a c are timely assembled visual and audio features from consecutive frames. Supposing a total of M \u2212 negative samples are leveraged, the contrastive learning is formulated as:\n\nFigure 3 .\n3Qualitative evaluation results. The driving source frames are listed in first row. Wav2Lip[27] fails to generate the frames with head pose similar to the driving source. The PC-AVS[50] can generate most images with similar head pose to the driving source but the result in second column is not quite accurate. Both LPD[3] and our GC-AVT can generate driven results with accurate head pose. The expression driven results are better than LPD[3].\n\nFigure 4 .\n4The qualitative results with different driving expression source and content source. The first row lists the identity reference while the expression and content source frames are listed in the left column and right column respectively. Our GC-AVT can generate the vivid driven results with the corresponding expression source and content source.\n\nFigure 5 .\n5Ablation study of the masking areas.\n\n\nUser Study. To further verify the quality of audio-driven results by organizing a user study of 20 participants for their opinions on 50 videos. Specifically, we randomly sample 5 videos as the driving source videos and 10 identity reference images from Voxceleb2 dataset. Then we generate the 50 videos with the same setting as we describedin Sec. 4.1. The comparing methods are Wav2Lip [27], MakeitTalk [51], PC-AVS [50] and our GC-AVT respectively. The evaluation of user study is developed on three dimensions for users: (1) Lip Sync Quality; (2) Expression Realness and Richness. (3) Overall Fidality and Quality. The widely used Mean Opinion Scores (MOS) is adopted with rating scores from 1 to 5.The rating results of our user study are listed in\n\nTable 2 .\n2User study on audio-driven methods, the evaluations are conducted on lip synchronization, the naturalness of facial expression and video quality.Table 3. Ablation study on Voxceleb2[11].Method \nWav2Lip [27] \nMakeItTalk [51] \nPC-AVS [50] \nGC-AVT (Ours) \nLip Sync Quality \n3.92 \n2.85 \n3.90 \n3.91 \nExpression Realness and Richness \n2.65 \n2.68 \n3.16 \n4.21 \nOverall Fidelity and Quality \n3.33 \n3.06 \n3.69 \n3.95 \n\nMethod \nSSIM LMD LMD m Sync conf \nw/o VGG \n0.662 4.753 4.212 4.586 \nw/o Contrastive 0.692 4.890 4.311 4.066 \nw/o time-shift 0.684 4.311 3.704 4.760 \nGround Truth 1.000 0.000 0.000 5.543 \nours (audio) \n0.710 3.025 3.356 5.250 \n\n(a) \n(b) \n(c) \n(d) \n(e) \n\nID \nreference \n\nEmotion \nsource \n\nContent \nsource \n\nExpression \ninput \n\nContent \ninput \n\nResults \n\nTrain setting \nTest setting \n\n\n\nA morphable model for the synthesis of 3d faces. Volker Blanz, Thomas Vetter, SIGGRAPH. Volker Blanz, Thomas Vetter, et al. A morphable model for the synthesis of 3d faces. In SIGGRAPH, 1999. 2\n\nHow far are we from solving the 2d & 3d face alignment problem?(and a dataset of 230,000 3d facial landmarks). Adrian Bulat, Georgios Tzimiropoulos, Proceedings of the IEEE International Conference on Computer Vision (ICCV). the IEEE International Conference on Computer Vision (ICCV)Adrian Bulat and Georgios Tzimiropoulos. How far are we from solving the 2d & 3d face alignment problem?(and a dataset of 230,000 3d facial landmarks). In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2017. 5\n\nNeural head reenactment with latent pose descriptors. Egor Burkov, Igor Pasechnik, Artur Grigorev, Victor Lempitsky, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)67Egor Burkov, Igor Pasechnik, Artur Grigorev, and Victor Lempitsky. Neural head reenactment with latent pose de- scriptors. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR), 2020. 2, 3, 5, 6, 7\n\nHigh-fidelity face tracking for ar/vr via deep lighting adaptation. Lele Chen, Chen Cao, Fernando De La Torre, Jason Saragih, Chenliang Xu, Yaser Sheikh, Lele Chen, Chen Cao, Fernando De la Torre, Jason Saragih, Chenliang Xu, and Yaser Sheikh. High-fidelity face tracking for ar/vr via deep lighting adaptation, 2021. 3\n\nWhat comprises a good talking-head video generation. Lele Chen, Guofeng Cui, Ziyi Kou, Haitian Zheng, Chenliang Xu, arXiv:2005.03201A survey and benchmark. arXiv preprintLele Chen, Guofeng Cui, Ziyi Kou, Haitian Zheng, and Chenliang Xu. What comprises a good talking-head video generation?: A survey and benchmark. arXiv preprint arXiv:2005.03201, 2020. 2\n\nTalking-head generation with rhythmic head motion. Lele Chen, Guofeng Cui, Celong Liu, Zhong Li, Ziyi Kou, Yi Xu, Chenliang Xu, European Conference on Computer Vision (ECCV). 2020Lele Chen, Guofeng Cui, Celong Liu, Zhong Li, Ziyi Kou, Yi Xu, and Chenliang Xu. Talking-head generation with rhyth- mic head motion. European Conference on Computer Vision (ECCV), 2020. 2\n\nLip movements generation at a glance. Lele Chen, Zhiheng Li, K Ross, Zhiyao Maddox, Chenliang Duan, Xu, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Lele Chen, Zhiheng Li, Ross K Maddox, Zhiyao Duan, and Chenliang Xu. Lip movements generation at a glance. In Proceedings of the European Conference on Computer Vi- sion (ECCV), 2018. 2\n\nHierarchical cross-modal talking face generation with dynamic pixel-wise loss. Lele Chen, K Ross, Zhiyao Maddox, Chenliang Duan, Xu, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Lele Chen, Ross K Maddox, Zhiyao Duan, and Chenliang Xu. Hierarchical cross-modal talking face generation with dynamic pixel-wise loss. In Proceedings of the IEEE Confer- ence on Computer Vision and Pattern Recognition (CVPR), 2019. 2\n\nIn defence of metric learning for speaker recognition. Jaesung Joon Son Chung, Seongkyu Huh, Minjae Mun, Hee Soo Lee, Soyeon Heo, Chiheon Choe, Sunghwan Ham, Bong-Jin Jung, Icksang Lee, Han, arXiv:2003.11982arXiv preprintJoon Son Chung, Jaesung Huh, Seongkyu Mun, Minjae Lee, Hee Soo Heo, Soyeon Choe, Chiheon Ham, Sungh- wan Jung, Bong-Jin Lee, and Icksang Han. In defence of metric learning for speaker recognition. arXiv preprint arXiv:2003.11982, 2020. 5\n\nYou said that? In BMVC. Joon Son Chung, Amir Jamaludin, Andrew Zisserman, 23Joon Son Chung, Amir Jamaludin, and Andrew Zisserman. You said that? In BMVC, 2017. 2, 3\n\nVoxceleb2: Deep speaker recognition. J S Chung, A Nagrani, A Zisserman, INTERSPEECH. 6J. S. Chung, A. Nagrani, and A. Zisserman. Voxceleb2: Deep speaker recognition. In INTERSPEECH, 2018. 5, 6, 8\n\nLip reading in the wild. Son Joon, Andrew Chung, Zisserman, ACCV. Joon Son Chung and Andrew Zisserman. Lip reading in the wild. In ACCV, 2016. 6\n\nOut of time: automated lip sync in the wild. Son Joon, Andrew Chung, Zisserman, ACCV. Joon Son Chung and Andrew Zisserman. Out of time: auto- mated lip sync in the wild. In ACCV, 2016. 4\n\nHeadgan: One-shot neural head synthesis and editing. Michail Christos Doukas, Stefanos Zafeiriou, Viktoriia Sharmanska, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionMichail Christos Doukas, Stefanos Zafeiriou, and Viktoriia Sharmanska. Headgan: One-shot neural head synthesis and editing. In Proceedings of the IEEE/CVF International Con- ference on Computer Vision, 2021. 3\n\nGraphonomy: Universal human parsing via graph transfer learning. Ke Gong, Yiming Gao, Xiaodan Liang, Xiaohui Shen, Meng Wang, Liang Lin, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionKe Gong, Yiming Gao, Xiaodan Liang, Xiaohui Shen, Meng Wang, and Liang Lin. Graphonomy: Universal human parsing via graph transfer learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7450-7459, 2019. 5\n\nLearning identity-invariant motion representations for crossid face reenactment. Po-Hsiang Huang, Fu-En Yang, Yu-Chiang Frank Wang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2020Po-Hsiang Huang, Fu-En Yang, and Yu-Chiang Frank Wang. Learning identity-invariant motion representations for cross- id face reenactment. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 3\n\nArbitrary style transfer in real-time with adaptive instance normalization. Xun Huang, Serge Belongie, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionXun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In Proceed- ings of the IEEE International Conference on Computer Vi- sion, pages 1501-1510, 2017. 5\n\nAudio-driven emotional video portraits. Xinya Ji, Hang Zhou, Kaisiyuan Wang, Wayne Wu, Chan Change Loy, Xun Cao, Feng Xu, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 23Xinya Ji, Hang Zhou, Kaisiyuan Wang, Wayne Wu, Chan Change Loy, Xun Cao, and Feng Xu. Audio-driven emotional video portraits. In The IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR), 2021. 2, 3\n\nNeural style-preserving visual dubbing. Hyeongwoo Kim, Mohamed Elgharib, Michael Zollh\u00f6fer, Hans-Peter Seidel, Thabo Beeler, Christian Richardt, Christian Theobalt, 2019. 3ACM Transactions on Graphics. Hyeongwoo Kim, Mohamed Elgharib, Michael Zollh\u00f6fer, Hans-Peter Seidel, Thabo Beeler, Christian Richardt, and Christian Theobalt. Neural style-preserving visual dubbing. ACM Transactions on Graphics (TOG), 2019. 3\n\nDeep video portraits. Hyeongwoo Kim, Pablo Garrido, Ayush Tewari, Weipeng Xu, Justus Thies, Matthias Niessner, Patrick P\u00e9rez, Christian Richardt, Michael Zollh\u00f6fer, Christian Theobalt, ACM Transactions on Graphics (TOG). 3Hyeongwoo Kim, Pablo Garrido, Ayush Tewari, Weipeng Xu, Justus Thies, Matthias Niessner, Patrick P\u00e9rez, Chris- tian Richardt, Michael Zollh\u00f6fer, and Christian Theobalt. Deep video portraits. ACM Transactions on Graphics (TOG), 2018. 3\n\nWrite-a-speaker: Text-based emotional and rhythmic talking-head generation. Lincheng Li, Suzhen Wang, Zhimeng Zhang, Yu Ding, Yixing Zheng, Xin Yu, Changjie Fan, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceLincheng Li, Suzhen Wang, Zhimeng Zhang, Yu Ding, Yix- ing Zheng, Xin Yu, and Changjie Fan. Write-a-speaker: Text-based emotional and rhythmic talking-head generation. In Proceedings of the AAAI Conference on Artificial Intelli- gence, 2021. 2\n\nLearning hierarchical cross-modal association for cospeech gesture generation. Xian Liu, Qianyi Wu, Hang Zhou, Yinghao Xu, Rui Qian, Xinyi Lin, Xiaowei Zhou, Wayne Wu, Bo Dai, Bolei Zhou, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022Xian Liu, Qianyi Wu, Hang Zhou, Yinghao Xu, Rui Qian, Xinyi Lin, Xiaowei Zhou, Wayne Wu, Bo Dai, and Bolei Zhou. Learning hierarchical cross-modal association for co- speech gesture generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022. 2\n\nSemantic-aware implicit neural audio-driven video portrait generation. Xian Liu, Yinghao Xu, Qianyi Wu, Hang Zhou, Wayne Wu, Bolei Zhou, arXiv:2201.077862022arXiv preprintXian Liu, Yinghao Xu, Qianyi Wu, Hang Zhou, Wayne Wu, and Bolei Zhou. Semantic-aware implicit neural audio-driven video portrait generation. arXiv preprint arXiv:2201.07786, 2022. 2\n\nDeep appearance models for face rendering. Stephen Lombardi, Jason Saragih, Tomas Simon, Yaser Sheikh, ACM Transactions on Graphics (TOG). 374Stephen Lombardi, Jason Saragih, Tomas Simon, and Yaser Sheikh. Deep appearance models for face rendering. ACM Transactions on Graphics (TOG), 37(4):1-13, 2018. 3\n\nLive speech portraits: real-time photorealistic talking-head animation. Yuanxun Lu, Jinxiang Chai, Xun Cao, ACM Transactions on Graphics (TOG). 406Yuanxun Lu, Jinxiang Chai, and Xun Cao. Live speech por- traits: real-time photorealistic talking-head animation. ACM Transactions on Graphics (TOG), 40(6):1-17, 2021. 2\n\nSemantic image synthesis with spatially-adaptive normalization. Taesung Park, Ming-Yu Liu, Ting-Chun Wang, Jun-Yan Zhu, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with spatially-adaptive nor- malization. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR), 2019. 5\n\nA lip sync expert is all you need for speech to lip generation in the wild. Rudrabha K R Prajwal, Mukhopadhyay, P Vinay, C V Namboodiri, Jawahar, Proceedings of the 28th ACM International Conference on Multimedia (ACMMM). the 28th ACM International Conference on Multimedia (ACMMM)7K R Prajwal, Rudrabha Mukhopadhyay, Vinay P. Nambood- iri, and C.V. Jawahar. A lip sync expert is all you need for speech to lip generation in the wild. In Proceedings of the 28th ACM International Conference on Multimedia (ACMMM), 2020. 2, 4, 5, 6, 7, 8\n\nMobilenetv2: Inverted residuals and linear bottlenecks. Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh- moginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018. 5\n\nRan He, and Chen Change Loy. Everybody's talkin': Let me talk as you want. Linsen Song, Wayne Wu, Chen Qian, arXiv:2001.05201arXiv preprintLinsen Song, Wayne Wu, Chen Qian, Ran He, and Chen Change Loy. Everybody's talkin': Let me talk as you want. arXiv preprint arXiv:2001.05201, 2020. 2\n\nTalking face generation by conditional recurrent adversarial network. Yang Song, Jingwen Zhu, Dawei Li, Xiaolong Wang, Hairong Qi, IJCAI. 2Yang Song, Jingwen Zhu, Dawei Li, Xiaolong Wang, and Hairong Qi. Talking face generation by conditional recurrent adversarial network. IJCAI, 2019. 2\n\nSpeech2talking-face: Inferring and driving a face with synchronized audio-visual representation. Yasheng Sun, Hang Zhou, Ziwei Liu, Hideki Koike, IJCAI, 2021. 24Yasheng Sun, Hang Zhou, Ziwei Liu, and Hideki Koike. Speech2talking-face: Inferring and driving a face with syn- chronized audio-visual representation. In IJCAI, 2021. 2, 4\n\nSynthesizing obama: learning lip sync from audio. Supasorn Suwajanakorn, M Steven, Ira Seitz, Kemelmacher-Shlizerman, ACM Transactions on Graphics. 2Supasorn Suwajanakorn, Steven M Seitz, and Ira Kemelmacher-Shlizerman. Synthesizing obama: learn- ing lip sync from audio. ACM Transactions on Graphics (TOG), 2017. 2\n\nNeural voice puppetry: Audio-driven facial reenactment. Justus Thies, Mohamed Elgharib, Ayush Tewari, Christian Theobalt, Matthias Nie\u00dfner, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)2020Justus Thies, Mohamed Elgharib, Ayush Tewari, Christian Theobalt, and Matthias Nie\u00dfner. Neural voice puppetry: Audio-driven facial reenactment. Proceedings of the Euro- pean Conference on Computer Vision (ECCV), 2020. 2\n\nFace2face: Real-time face capture and reenactment of rgb videos. Justus Thies, Michael Zollh\u00f6fer, Marc Stamminger, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Christian Theobaltzhang2020freenet, and Matthias Nie\u00dfnerJustus Thies, Michael Zollh\u00f6fer, Marc Stamminger, Chris- tian Theobaltzhang2020freenet, and Matthias Nie\u00dfner. Face2face: Real-time face capture and reenactment of rgb videos. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 3\n\nMead: A large-scale audio-visual dataset for emotional talking-face generation. Kaisiyuan Wang, Qianyi Wu, Linsen Song, Zhuoqian Yang, Wayne Wu, Chen Qian, Ran He, Yu Qiao, Chen Change Loy, European Conference on Computer Vision. Springer56Kaisiyuan Wang, Qianyi Wu, Linsen Song, Zhuoqian Yang, Wayne Wu, Chen Qian, Ran He, Yu Qiao, and Chen Change Loy. Mead: A large-scale audio-visual dataset for emotional talking-face generation. In European Conference on Com- puter Vision, pages 700-717. Springer, 2020. 2, 3, 5, 6\n\nHigh quality lipsync animation for 3d photo-realistic talking head. Lijuan Wang, Wei Han, Frank K Soong, IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Lijuan Wang, Wei Han, and Frank K Soong. High quality lip- sync animation for 3d photo-realistic talking head. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2012. 2\n\nSynthesizing photo-real talking head via trajectory-guided sample selection. Lijuan Wang, Xiaojun Qian, Wei Han, Frank K Soong, Eleventh Annual Conference of the International Speech Communication Association. Lijuan Wang, Xiaojun Qian, Wei Han, and Frank K Soong. Synthesizing photo-real talking head via trajectory-guided sample selection. In Eleventh Annual Conference of the In- ternational Speech Communication Association, 2010. 2\n\nHigh-resolution image synthesis and semantic manipulation with conditional gans. Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, Bryan Catanzaro, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. High-resolution image syn- thesis and semantic manipulation with conditional gans. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 5\n\nImage quality assessment: from error visibility to structural similarity. Zhou Wang, Alan C Bovik, R Hamid, Eero P Sheikh, Simoncelli, TIP. 6Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. TIP, 2004. 6\n\nReenactgan: Learning to reenact faces via boundary transfer. Wayne Wu, Yunxuan Zhang, Cheng Li, Chen Qian, Chen Change Loy, European Conference on Computer Vision (ECCV). Wayne Wu, Yunxuan Zhang, Cheng Li, Chen Qian, and Chen Change Loy. Reenactgan: Learning to reenact faces via boundary transfer. In European Conference on Computer Vision (ECCV), 2018. 3\n\nAggregated residual transformations for deep neural networks. Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, Kaiming He, 2017. 5Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionSaining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017. 5\n\nAudio-driven talking face video generation with natural head pose. Ran Yi, Zipeng Ye, Juyong Zhang, Hujun Bao, Yong-Jin Liu, arXiv:2002.10137arXiv preprintRan Yi, Zipeng Ye, Juyong Zhang, Hujun Bao, and Yong-Jin Liu. Audio-driven talking face video generation with natural head pose. arXiv preprint arXiv:2002.10137, 2020. 2\n\nFew-shot adversarial learning of realistic neural talking head models. Egor Zakharov, Aliaksandra Shysheya, Egor Burkov, Victor Lempitsky, Proceedings of the IEEE International Conference on Computer Vision (ICCV). the IEEE International Conference on Computer Vision (ICCV)Egor Zakharov, Aliaksandra Shysheya, Egor Burkov, and Victor Lempitsky. Few-shot adversarial learning of realis- tic neural talking head models. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2019. 3\n\nFreenet: Multi-identity face reenactment. Jiangning Zhang, Xianfang Zeng, Mengmeng Wang, Yusu Pan, Liang Liu, Yong Liu, Yu Ding, Changjie Fan, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2020Jiangning Zhang, Xianfang Zeng, Mengmeng Wang, Yusu Pan, Liang Liu, Yong Liu, Yu Ding, and Changjie Fan. Freenet: Multi-identity face reenactment. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 3\n\nS3fd: Single shot scale-invariant face detector. Shifeng Zhang, Xiangyu Zhu, Zhen Lei, Hailin Shi, Xiaobo Wang, Stan Z Li, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionShifeng Zhang, Xiangyu Zhu, Zhen Lei, Hailin Shi, Xiaobo Wang, and Stan Z Li. S3fd: Single shot scale-invariant face detector. In Proceedings of the IEEE international confer- ence on computer vision, pages 192-201, 2017. 5\n\nFlow-guided one-shot talking face generation with a high-resolution audio-visual dataset. Zhimeng Zhang, Lincheng Li, Yu Ding, Changjie Fan, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)23Zhimeng Zhang, Lincheng Li, Yu Ding, and Changjie Fan. Flow-guided one-shot talking face generation with a high-resolution audio-visual dataset. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 2, 3\n\nRotate-and-render: Unsupervised photorealistic face rotation from single-view images. Hang Zhou, Jihao Liu, Ziwei Liu, Yu Liu, Xiaogang Wang, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2020Hang Zhou, Jihao Liu, Ziwei Liu, Yu Liu, and Xiaogang Wang. Rotate-and-render: Unsupervised photorealistic face rotation from single-view images. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 3\n\nTalking face generation by adversarially disentangled audio-visual representation. Hang Zhou, Yu Liu, Ziwei Liu, Ping Luo, Xiaogang Wang, Proceedings of the AAAI Conference on Artificial Intelligence (AAAI). the AAAI Conference on Artificial Intelligence (AAAI)34Hang Zhou, Yu Liu, Ziwei Liu, Ping Luo, and Xiaogang Wang. Talking face generation by adversarially disentan- gled audio-visual representation. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2019. 2, 3, 4\n\nVision-infused deep audio inpainting. Hang Zhou, Ziwei Liu, Xudong Xu, Ping Luo, Xiaogang Wang, Proceedings of the IEEE International Conference on Computer Vision (ICCV). the IEEE International Conference on Computer Vision (ICCV)Hang Zhou, Ziwei Liu, Xudong Xu, Ping Luo, and Xiaogang Wang. Vision-infused deep audio inpainting. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2019. 4\n\nPose-controllable talking face generation by implicitly modularized audio-visual representation. Hang Zhou, Yasheng Sun, Wayne Wu, Chen Change Loy, Xiaogang Wang, Ziwei Liu, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)7Hang Zhou, Yasheng Sun, Wayne Wu, Chen Change Loy, Xiaogang Wang, and Ziwei Liu. Pose-controllable talking face generation by implicitly modularized audio-visual rep- resentation. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR), 2021. 2, 3, 4, 5, 6, 7, 8\n\nMakeittalk: Speaker-aware talking head animation. Yang Zhou, Xintong Han, Eli Shechtman, Jose Echevarria, Evangelos Kalogerakis, Dingzeyu Li, ASIA. 78Yang Zhou, Xintong Han, Eli Shechtman, Jose Echevar- ria, Evangelos Kalogerakis, and Dingzeyu Li. Makeittalk: Speaker-aware talking head animation. SIGGRAPH ASIA, 2020. 2, 3, 5, 6, 7, 8\n\nVisemenet: Audiodriven animator-centric speech animation. Yang Zhou, Zhan Xu, Chris Landreth, Evangelos Kalogerakis, Subhransu Maji, Karan Singh, ACM Transactions on Graphics (TOG). 2Yang Zhou, Zhan Xu, Chris Landreth, Evangelos Kaloger- akis, Subhransu Maji, and Karan Singh. Visemenet: Audio- driven animator-centric speech animation. ACM Transac- tions on Graphics (TOG), 2018. 2\n", "annotations": {"author": "[{\"end\":167,\"start\":73},{\"end\":293,\"start\":168},{\"end\":384,\"start\":294},{\"end\":456,\"start\":385},{\"end\":546,\"start\":457},{\"end\":552,\"start\":547},{\"end\":697,\"start\":553},{\"end\":786,\"start\":698},{\"end\":879,\"start\":787},{\"end\":970,\"start\":880},{\"end\":1067,\"start\":971}]", "publisher": null, "author_last_name": "[{\"end\":85,\"start\":80},{\"end\":175,\"start\":172},{\"end\":304,\"start\":301},{\"end\":396,\"start\":392},{\"end\":465,\"start\":459},{\"end\":566,\"start\":563},{\"end\":707,\"start\":704},{\"end\":798,\"start\":795},{\"end\":890,\"start\":886},{\"end\":984,\"start\":980}]", "author_first_name": "[{\"end\":79,\"start\":73},{\"end\":171,\"start\":168},{\"end\":300,\"start\":294},{\"end\":386,\"start\":385},{\"end\":391,\"start\":387},{\"end\":458,\"start\":457},{\"end\":551,\"start\":547},{\"end\":562,\"start\":553},{\"end\":703,\"start\":698},{\"end\":794,\"start\":787},{\"end\":885,\"start\":880},{\"end\":979,\"start\":971}]", "author_affiliation": "[{\"end\":166,\"start\":109},{\"end\":214,\"start\":190},{\"end\":292,\"start\":216},{\"end\":383,\"start\":326},{\"end\":455,\"start\":398},{\"end\":545,\"start\":488},{\"end\":618,\"start\":594},{\"end\":696,\"start\":620},{\"end\":785,\"start\":728},{\"end\":878,\"start\":821},{\"end\":969,\"start\":912},{\"end\":1066,\"start\":1009}]", "title": "[{\"end\":70,\"start\":1},{\"end\":1137,\"start\":1068}]", "venue": null, "abstract": "[{\"end\":2857,\"start\":1167}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3294,\"start\":3291},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3297,\"start\":3294},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":3300,\"start\":3297},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":3303,\"start\":3300},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":3306,\"start\":3303},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":3309,\"start\":3306},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3579,\"start\":3576},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3582,\"start\":3579},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3585,\"start\":3582},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3588,\"start\":3585},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":3591,\"start\":3588},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3652,\"start\":3649},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":3655,\"start\":3652},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":3685,\"start\":3681},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3852,\"start\":3848},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3855,\"start\":3852},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":3858,\"start\":3855},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4527,\"start\":4524},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4529,\"start\":4527},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":4532,\"start\":4529},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4655,\"start\":4652},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4868,\"start\":4865},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4871,\"start\":4868},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":4874,\"start\":4871},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":4877,\"start\":4874},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4965,\"start\":4962},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":4968,\"start\":4965},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5889,\"start\":5886},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":5892,\"start\":5889},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7576,\"start\":7573},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7579,\"start\":7576},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7582,\"start\":7579},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":7585,\"start\":7582},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":7588,\"start\":7585},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7770,\"start\":7766},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":7773,\"start\":7770},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":7776,\"start\":7773},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8031,\"start\":8027},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8034,\"start\":8031},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8037,\"start\":8034},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8040,\"start\":8037},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8227,\"start\":8224},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8230,\"start\":8227},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8233,\"start\":8230},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":8236,\"start\":8233},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":8239,\"start\":8236},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":8242,\"start\":8239},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8261,\"start\":8257},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":8386,\"start\":8382},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8465,\"start\":8461},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8744,\"start\":8741},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8747,\"start\":8744},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8750,\"start\":8747},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":8753,\"start\":8750},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":8756,\"start\":8753},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":8903,\"start\":8899},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":8922,\"start\":8918},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":8988,\"start\":8984},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":9100,\"start\":9096},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9265,\"start\":9261},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9284,\"start\":9280},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9724,\"start\":9720},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":9727,\"start\":9724},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":9730,\"start\":9727},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":9733,\"start\":9730},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9750,\"start\":9747},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9753,\"start\":9750},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9756,\"start\":9753},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9759,\"start\":9756},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":9762,\"start\":9759},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":9765,\"start\":9762},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9792,\"start\":9788},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":10022,\"start\":10018},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10183,\"start\":10180},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10186,\"start\":10183},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":11068,\"start\":11064},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":11071,\"start\":11068},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11559,\"start\":11556},{\"end\":15221,\"start\":15217},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":16601,\"start\":16597},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":16604,\"start\":16601},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":17046,\"start\":17042},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":17619,\"start\":17615},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":17622,\"start\":17619},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":17626,\"start\":17622},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":17630,\"start\":17626},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":17634,\"start\":17630},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":17708,\"start\":17704},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":18203,\"start\":18200},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":18235,\"start\":18231},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":18605,\"start\":18601},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":18608,\"start\":18605},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":19577,\"start\":19573},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":19623,\"start\":19619},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":19642,\"start\":19638},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":19899,\"start\":19895},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":20262,\"start\":20258},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":20407,\"start\":20403},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":20522,\"start\":20519},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":20573,\"start\":20569},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":20878,\"start\":20874},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":21076,\"start\":21072},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":21237,\"start\":21234},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":21680,\"start\":21677},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":21683,\"start\":21680},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":21686,\"start\":21683},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":21700,\"start\":21696},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":21799,\"start\":21795},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":21909,\"start\":21905},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":22474,\"start\":22470},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":22488,\"start\":22484},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":22618,\"start\":22614},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":22797,\"start\":22793},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":22967,\"start\":22963},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":23187,\"start\":23183},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":24780,\"start\":24777},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":24796,\"start\":24792},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":25014,\"start\":25010},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":25139,\"start\":25135},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":25352,\"start\":25348},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":25534,\"start\":25531},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":25593,\"start\":25590},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":30608,\"start\":30604},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":30698,\"start\":30694},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":30835,\"start\":30832},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":30956,\"start\":30953},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":32320,\"start\":32316}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":30299,\"start\":30113},{\"attributes\":{\"id\":\"fig_1\"},\"end\":30500,\"start\":30300},{\"attributes\":{\"id\":\"fig_2\"},\"end\":30957,\"start\":30501},{\"attributes\":{\"id\":\"fig_3\"},\"end\":31316,\"start\":30958},{\"attributes\":{\"id\":\"fig_4\"},\"end\":31366,\"start\":31317},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":32122,\"start\":31367},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":32925,\"start\":32123}]", "paragraph": "[{\"end\":3491,\"start\":2873},{\"end\":4043,\"start\":3493},{\"end\":5026,\"start\":4045},{\"end\":5674,\"start\":5028},{\"end\":6980,\"start\":5676},{\"end\":7481,\"start\":6982},{\"end\":8711,\"start\":7498},{\"end\":9484,\"start\":8713},{\"end\":9653,\"start\":9486},{\"end\":10364,\"start\":9655},{\"end\":10928,\"start\":10381},{\"end\":11235,\"start\":10952},{\"end\":11957,\"start\":11237},{\"end\":12060,\"start\":11959},{\"end\":12826,\"start\":12062},{\"end\":13126,\"start\":13010},{\"end\":13392,\"start\":13157},{\"end\":13699,\"start\":13394},{\"end\":15890,\"start\":13701},{\"end\":17047,\"start\":15980},{\"end\":17709,\"start\":17071},{\"end\":17974,\"start\":17711},{\"end\":18704,\"start\":18076},{\"end\":18984,\"start\":18846},{\"end\":19139,\"start\":19078},{\"end\":19489,\"start\":19192},{\"end\":19624,\"start\":19529},{\"end\":19733,\"start\":19626},{\"end\":19886,\"start\":19735},{\"end\":20126,\"start\":19888},{\"end\":20826,\"start\":20128},{\"end\":22270,\"start\":20828},{\"end\":23017,\"start\":22298},{\"end\":23536,\"start\":23113},{\"end\":24602,\"start\":23538},{\"end\":25109,\"start\":24629},{\"end\":25285,\"start\":25111},{\"end\":25868,\"start\":25308},{\"end\":26937,\"start\":25887},{\"end\":27156,\"start\":26956},{\"end\":28181,\"start\":27158},{\"end\":28788,\"start\":28183},{\"end\":29229,\"start\":28818},{\"end\":29652,\"start\":29231},{\"end\":29872,\"start\":29654},{\"end\":30112,\"start\":29874}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13009,\"start\":12827},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15979,\"start\":15891},{\"attributes\":{\"id\":\"formula_2\"},\"end\":18075,\"start\":17975},{\"attributes\":{\"id\":\"formula_3\"},\"end\":18733,\"start\":18705},{\"attributes\":{\"id\":\"formula_4\"},\"end\":18845,\"start\":18733},{\"attributes\":{\"id\":\"formula_5\"},\"end\":19077,\"start\":18985},{\"attributes\":{\"id\":\"formula_6\"},\"end\":19191,\"start\":19140},{\"attributes\":{\"id\":\"formula_7\"},\"end\":23112,\"start\":23018}]", "table_ref": "[{\"end\":22416,\"start\":22409},{\"end\":23570,\"start\":23563},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":26557,\"start\":26550},{\"end\":27279,\"start\":27272},{\"end\":27542,\"start\":27535}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2871,\"start\":2859},{\"attributes\":{\"n\":\"2.\"},\"end\":7496,\"start\":7484},{\"attributes\":{\"n\":\"3.\"},\"end\":10379,\"start\":10367},{\"attributes\":{\"n\":\"3.1.\"},\"end\":10950,\"start\":10931},{\"attributes\":{\"n\":\"3.2.\"},\"end\":13155,\"start\":13129},{\"attributes\":{\"n\":\"3.3.\"},\"end\":17069,\"start\":17050},{\"attributes\":{\"n\":\"4.\"},\"end\":19503,\"start\":19492},{\"attributes\":{\"n\":\"4.1.\"},\"end\":19527,\"start\":19506},{\"attributes\":{\"n\":\"4.2.\"},\"end\":22296,\"start\":22273},{\"attributes\":{\"n\":\"4.3.\"},\"end\":24627,\"start\":24605},{\"end\":25306,\"start\":25288},{\"end\":25885,\"start\":25871},{\"attributes\":{\"n\":\"4.4.\"},\"end\":26954,\"start\":26940},{\"attributes\":{\"n\":\"5.\"},\"end\":28816,\"start\":28791},{\"end\":30125,\"start\":30114},{\"end\":30302,\"start\":30301},{\"end\":30512,\"start\":30502},{\"end\":30969,\"start\":30959},{\"end\":31328,\"start\":31318},{\"end\":32133,\"start\":32124}]", "table": "[{\"end\":32925,\"start\":32321}]", "figure_caption": "[{\"end\":30299,\"start\":30127},{\"end\":30500,\"start\":30303},{\"end\":30957,\"start\":30514},{\"end\":31316,\"start\":30971},{\"end\":31366,\"start\":31330},{\"end\":32122,\"start\":31369},{\"end\":32321,\"start\":32135}]", "figure_ref": "[{\"end\":5521,\"start\":5515},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11009,\"start\":11003},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":24987,\"start\":24979},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26376,\"start\":26368},{\"end\":27685,\"start\":27677},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":28310,\"start\":28304},{\"end\":28927,\"start\":28919}]", "bib_author_first_name": "[{\"end\":32982,\"start\":32976},{\"end\":32996,\"start\":32990},{\"end\":33239,\"start\":33233},{\"end\":33255,\"start\":33247},{\"end\":33704,\"start\":33700},{\"end\":33717,\"start\":33713},{\"end\":33734,\"start\":33729},{\"end\":33751,\"start\":33745},{\"end\":34227,\"start\":34223},{\"end\":34238,\"start\":34234},{\"end\":34252,\"start\":34244},{\"end\":34271,\"start\":34266},{\"end\":34290,\"start\":34281},{\"end\":34300,\"start\":34295},{\"end\":34533,\"start\":34529},{\"end\":34547,\"start\":34540},{\"end\":34557,\"start\":34553},{\"end\":34570,\"start\":34563},{\"end\":34587,\"start\":34578},{\"end\":34888,\"start\":34884},{\"end\":34902,\"start\":34895},{\"end\":34914,\"start\":34908},{\"end\":34925,\"start\":34920},{\"end\":34934,\"start\":34930},{\"end\":34942,\"start\":34940},{\"end\":34956,\"start\":34947},{\"end\":35244,\"start\":35240},{\"end\":35258,\"start\":35251},{\"end\":35264,\"start\":35263},{\"end\":35277,\"start\":35271},{\"end\":35295,\"start\":35286},{\"end\":35691,\"start\":35687},{\"end\":35699,\"start\":35698},{\"end\":35712,\"start\":35706},{\"end\":35730,\"start\":35721},{\"end\":36194,\"start\":36187},{\"end\":36219,\"start\":36211},{\"end\":36231,\"start\":36225},{\"end\":36240,\"start\":36237},{\"end\":36244,\"start\":36241},{\"end\":36256,\"start\":36250},{\"end\":36269,\"start\":36262},{\"end\":36284,\"start\":36276},{\"end\":36298,\"start\":36290},{\"end\":36312,\"start\":36305},{\"end\":36624,\"start\":36616},{\"end\":36636,\"start\":36632},{\"end\":36654,\"start\":36648},{\"end\":36796,\"start\":36795},{\"end\":36798,\"start\":36797},{\"end\":36807,\"start\":36806},{\"end\":36818,\"start\":36817},{\"end\":36983,\"start\":36980},{\"end\":36996,\"start\":36990},{\"end\":37149,\"start\":37146},{\"end\":37162,\"start\":37156},{\"end\":37349,\"start\":37342},{\"end\":37358,\"start\":37350},{\"end\":37375,\"start\":37367},{\"end\":37396,\"start\":37387},{\"end\":37816,\"start\":37814},{\"end\":37829,\"start\":37823},{\"end\":37842,\"start\":37835},{\"end\":37857,\"start\":37850},{\"end\":37868,\"start\":37864},{\"end\":37880,\"start\":37875},{\"end\":38378,\"start\":38369},{\"end\":38391,\"start\":38386},{\"end\":38413,\"start\":38398},{\"end\":38906,\"start\":38903},{\"end\":38919,\"start\":38914},{\"end\":39304,\"start\":39299},{\"end\":39313,\"start\":39309},{\"end\":39329,\"start\":39320},{\"end\":39341,\"start\":39336},{\"end\":39350,\"start\":39346},{\"end\":39357,\"start\":39351},{\"end\":39366,\"start\":39363},{\"end\":39376,\"start\":39372},{\"end\":39717,\"start\":39708},{\"end\":39730,\"start\":39723},{\"end\":39748,\"start\":39741},{\"end\":39770,\"start\":39760},{\"end\":39784,\"start\":39779},{\"end\":39802,\"start\":39793},{\"end\":39822,\"start\":39813},{\"end\":40115,\"start\":40106},{\"end\":40126,\"start\":40121},{\"end\":40141,\"start\":40136},{\"end\":40157,\"start\":40150},{\"end\":40168,\"start\":40162},{\"end\":40184,\"start\":40176},{\"end\":40202,\"start\":40195},{\"end\":40219,\"start\":40210},{\"end\":40237,\"start\":40230},{\"end\":40258,\"start\":40249},{\"end\":40626,\"start\":40618},{\"end\":40637,\"start\":40631},{\"end\":40651,\"start\":40644},{\"end\":40661,\"start\":40659},{\"end\":40674,\"start\":40668},{\"end\":40685,\"start\":40682},{\"end\":40698,\"start\":40690},{\"end\":41141,\"start\":41137},{\"end\":41153,\"start\":41147},{\"end\":41162,\"start\":41158},{\"end\":41176,\"start\":41169},{\"end\":41184,\"start\":41181},{\"end\":41196,\"start\":41191},{\"end\":41209,\"start\":41202},{\"end\":41221,\"start\":41216},{\"end\":41228,\"start\":41226},{\"end\":41239,\"start\":41234},{\"end\":41763,\"start\":41759},{\"end\":41776,\"start\":41769},{\"end\":41787,\"start\":41781},{\"end\":41796,\"start\":41792},{\"end\":41808,\"start\":41803},{\"end\":41818,\"start\":41813},{\"end\":42092,\"start\":42085},{\"end\":42108,\"start\":42103},{\"end\":42123,\"start\":42118},{\"end\":42136,\"start\":42131},{\"end\":42427,\"start\":42420},{\"end\":42440,\"start\":42432},{\"end\":42450,\"start\":42447},{\"end\":42737,\"start\":42730},{\"end\":42751,\"start\":42744},{\"end\":42766,\"start\":42757},{\"end\":42780,\"start\":42773},{\"end\":43251,\"start\":43243},{\"end\":43280,\"start\":43279},{\"end\":43289,\"start\":43288},{\"end\":43291,\"start\":43290},{\"end\":43765,\"start\":43761},{\"end\":43781,\"start\":43775},{\"end\":43798,\"start\":43790},{\"end\":43810,\"start\":43804},{\"end\":43833,\"start\":43822},{\"end\":44320,\"start\":44314},{\"end\":44332,\"start\":44327},{\"end\":44341,\"start\":44337},{\"end\":44603,\"start\":44599},{\"end\":44617,\"start\":44610},{\"end\":44628,\"start\":44623},{\"end\":44641,\"start\":44633},{\"end\":44655,\"start\":44648},{\"end\":44923,\"start\":44916},{\"end\":44933,\"start\":44929},{\"end\":44945,\"start\":44940},{\"end\":44957,\"start\":44951},{\"end\":45212,\"start\":45204},{\"end\":45228,\"start\":45227},{\"end\":45240,\"start\":45237},{\"end\":45533,\"start\":45527},{\"end\":45548,\"start\":45541},{\"end\":45564,\"start\":45559},{\"end\":45582,\"start\":45573},{\"end\":45601,\"start\":45593},{\"end\":46022,\"start\":46016},{\"end\":46037,\"start\":46030},{\"end\":46053,\"start\":46049},{\"end\":46639,\"start\":46630},{\"end\":46652,\"start\":46646},{\"end\":46663,\"start\":46657},{\"end\":46678,\"start\":46670},{\"end\":46690,\"start\":46685},{\"end\":46699,\"start\":46695},{\"end\":46709,\"start\":46706},{\"end\":46716,\"start\":46714},{\"end\":46734,\"start\":46723},{\"end\":47146,\"start\":47140},{\"end\":47156,\"start\":47153},{\"end\":47167,\"start\":47162},{\"end\":47169,\"start\":47168},{\"end\":47549,\"start\":47543},{\"end\":47563,\"start\":47556},{\"end\":47573,\"start\":47570},{\"end\":47584,\"start\":47579},{\"end\":47586,\"start\":47585},{\"end\":47994,\"start\":47985},{\"end\":48008,\"start\":48001},{\"end\":48021,\"start\":48014},{\"end\":48033,\"start\":48027},{\"end\":48042,\"start\":48039},{\"end\":48055,\"start\":48050},{\"end\":48567,\"start\":48563},{\"end\":48578,\"start\":48574},{\"end\":48580,\"start\":48579},{\"end\":48589,\"start\":48588},{\"end\":48603,\"start\":48597},{\"end\":48848,\"start\":48843},{\"end\":48860,\"start\":48853},{\"end\":48873,\"start\":48868},{\"end\":48882,\"start\":48878},{\"end\":48900,\"start\":48889},{\"end\":49209,\"start\":49202},{\"end\":49219,\"start\":49215},{\"end\":49235,\"start\":49230},{\"end\":49251,\"start\":49244},{\"end\":49263,\"start\":49256},{\"end\":49716,\"start\":49713},{\"end\":49727,\"start\":49721},{\"end\":49738,\"start\":49732},{\"end\":49751,\"start\":49746},{\"end\":49765,\"start\":49757},{\"end\":50047,\"start\":50043},{\"end\":50069,\"start\":50058},{\"end\":50084,\"start\":50080},{\"end\":50099,\"start\":50093},{\"end\":50530,\"start\":50521},{\"end\":50546,\"start\":50538},{\"end\":50561,\"start\":50553},{\"end\":50572,\"start\":50568},{\"end\":50583,\"start\":50578},{\"end\":50593,\"start\":50589},{\"end\":50601,\"start\":50599},{\"end\":50616,\"start\":50608},{\"end\":51094,\"start\":51087},{\"end\":51109,\"start\":51102},{\"end\":51119,\"start\":51115},{\"end\":51131,\"start\":51125},{\"end\":51143,\"start\":51137},{\"end\":51154,\"start\":51150},{\"end\":51156,\"start\":51155},{\"end\":51604,\"start\":51597},{\"end\":51620,\"start\":51612},{\"end\":51627,\"start\":51625},{\"end\":51642,\"start\":51634},{\"end\":52153,\"start\":52149},{\"end\":52165,\"start\":52160},{\"end\":52176,\"start\":52171},{\"end\":52184,\"start\":52182},{\"end\":52198,\"start\":52190},{\"end\":52588,\"start\":52584},{\"end\":52597,\"start\":52595},{\"end\":52608,\"start\":52603},{\"end\":52618,\"start\":52614},{\"end\":52632,\"start\":52624},{\"end\":53038,\"start\":53034},{\"end\":53050,\"start\":53045},{\"end\":53062,\"start\":53056},{\"end\":53071,\"start\":53067},{\"end\":53085,\"start\":53077},{\"end\":53516,\"start\":53512},{\"end\":53530,\"start\":53523},{\"end\":53541,\"start\":53536},{\"end\":53550,\"start\":53546},{\"end\":53557,\"start\":53551},{\"end\":53571,\"start\":53563},{\"end\":53583,\"start\":53578},{\"end\":54097,\"start\":54093},{\"end\":54111,\"start\":54104},{\"end\":54120,\"start\":54117},{\"end\":54136,\"start\":54132},{\"end\":54158,\"start\":54149},{\"end\":54180,\"start\":54172},{\"end\":54442,\"start\":54438},{\"end\":54453,\"start\":54449},{\"end\":54463,\"start\":54458},{\"end\":54483,\"start\":54474},{\"end\":54506,\"start\":54497},{\"end\":54518,\"start\":54513}]", "bib_author_last_name": "[{\"end\":32988,\"start\":32983},{\"end\":33003,\"start\":32997},{\"end\":33245,\"start\":33240},{\"end\":33269,\"start\":33256},{\"end\":33711,\"start\":33705},{\"end\":33727,\"start\":33718},{\"end\":33743,\"start\":33735},{\"end\":33761,\"start\":33752},{\"end\":34232,\"start\":34228},{\"end\":34242,\"start\":34239},{\"end\":34264,\"start\":34253},{\"end\":34279,\"start\":34272},{\"end\":34293,\"start\":34291},{\"end\":34307,\"start\":34301},{\"end\":34538,\"start\":34534},{\"end\":34551,\"start\":34548},{\"end\":34561,\"start\":34558},{\"end\":34576,\"start\":34571},{\"end\":34590,\"start\":34588},{\"end\":34893,\"start\":34889},{\"end\":34906,\"start\":34903},{\"end\":34918,\"start\":34915},{\"end\":34928,\"start\":34926},{\"end\":34938,\"start\":34935},{\"end\":34945,\"start\":34943},{\"end\":34959,\"start\":34957},{\"end\":35249,\"start\":35245},{\"end\":35261,\"start\":35259},{\"end\":35269,\"start\":35265},{\"end\":35284,\"start\":35278},{\"end\":35300,\"start\":35296},{\"end\":35304,\"start\":35302},{\"end\":35696,\"start\":35692},{\"end\":35704,\"start\":35700},{\"end\":35719,\"start\":35713},{\"end\":35735,\"start\":35731},{\"end\":35739,\"start\":35737},{\"end\":36209,\"start\":36195},{\"end\":36223,\"start\":36220},{\"end\":36235,\"start\":36232},{\"end\":36248,\"start\":36245},{\"end\":36260,\"start\":36257},{\"end\":36274,\"start\":36270},{\"end\":36288,\"start\":36285},{\"end\":36303,\"start\":36299},{\"end\":36316,\"start\":36313},{\"end\":36321,\"start\":36318},{\"end\":36630,\"start\":36625},{\"end\":36646,\"start\":36637},{\"end\":36664,\"start\":36655},{\"end\":36804,\"start\":36799},{\"end\":36815,\"start\":36808},{\"end\":36828,\"start\":36819},{\"end\":36988,\"start\":36984},{\"end\":37002,\"start\":36997},{\"end\":37013,\"start\":37004},{\"end\":37154,\"start\":37150},{\"end\":37168,\"start\":37163},{\"end\":37179,\"start\":37170},{\"end\":37365,\"start\":37359},{\"end\":37385,\"start\":37376},{\"end\":37407,\"start\":37397},{\"end\":37821,\"start\":37817},{\"end\":37833,\"start\":37830},{\"end\":37848,\"start\":37843},{\"end\":37862,\"start\":37858},{\"end\":37873,\"start\":37869},{\"end\":37884,\"start\":37881},{\"end\":38384,\"start\":38379},{\"end\":38396,\"start\":38392},{\"end\":38418,\"start\":38414},{\"end\":38912,\"start\":38907},{\"end\":38928,\"start\":38920},{\"end\":39307,\"start\":39305},{\"end\":39318,\"start\":39314},{\"end\":39334,\"start\":39330},{\"end\":39344,\"start\":39342},{\"end\":39361,\"start\":39358},{\"end\":39370,\"start\":39367},{\"end\":39379,\"start\":39377},{\"end\":39721,\"start\":39718},{\"end\":39739,\"start\":39731},{\"end\":39758,\"start\":39749},{\"end\":39777,\"start\":39771},{\"end\":39791,\"start\":39785},{\"end\":39811,\"start\":39803},{\"end\":39831,\"start\":39823},{\"end\":40119,\"start\":40116},{\"end\":40134,\"start\":40127},{\"end\":40148,\"start\":40142},{\"end\":40160,\"start\":40158},{\"end\":40174,\"start\":40169},{\"end\":40193,\"start\":40185},{\"end\":40208,\"start\":40203},{\"end\":40228,\"start\":40220},{\"end\":40247,\"start\":40238},{\"end\":40267,\"start\":40259},{\"end\":40629,\"start\":40627},{\"end\":40642,\"start\":40638},{\"end\":40657,\"start\":40652},{\"end\":40666,\"start\":40662},{\"end\":40680,\"start\":40675},{\"end\":40688,\"start\":40686},{\"end\":40702,\"start\":40699},{\"end\":41145,\"start\":41142},{\"end\":41156,\"start\":41154},{\"end\":41167,\"start\":41163},{\"end\":41179,\"start\":41177},{\"end\":41189,\"start\":41185},{\"end\":41200,\"start\":41197},{\"end\":41214,\"start\":41210},{\"end\":41224,\"start\":41222},{\"end\":41232,\"start\":41229},{\"end\":41244,\"start\":41240},{\"end\":41767,\"start\":41764},{\"end\":41779,\"start\":41777},{\"end\":41790,\"start\":41788},{\"end\":41801,\"start\":41797},{\"end\":41811,\"start\":41809},{\"end\":41823,\"start\":41819},{\"end\":42101,\"start\":42093},{\"end\":42116,\"start\":42109},{\"end\":42129,\"start\":42124},{\"end\":42143,\"start\":42137},{\"end\":42430,\"start\":42428},{\"end\":42445,\"start\":42441},{\"end\":42454,\"start\":42451},{\"end\":42742,\"start\":42738},{\"end\":42755,\"start\":42752},{\"end\":42771,\"start\":42767},{\"end\":42784,\"start\":42781},{\"end\":43263,\"start\":43252},{\"end\":43277,\"start\":43265},{\"end\":43286,\"start\":43281},{\"end\":43302,\"start\":43292},{\"end\":43311,\"start\":43304},{\"end\":43773,\"start\":43766},{\"end\":43788,\"start\":43782},{\"end\":43802,\"start\":43799},{\"end\":43820,\"start\":43811},{\"end\":43838,\"start\":43834},{\"end\":44325,\"start\":44321},{\"end\":44335,\"start\":44333},{\"end\":44346,\"start\":44342},{\"end\":44608,\"start\":44604},{\"end\":44621,\"start\":44618},{\"end\":44631,\"start\":44629},{\"end\":44646,\"start\":44642},{\"end\":44658,\"start\":44656},{\"end\":44927,\"start\":44924},{\"end\":44938,\"start\":44934},{\"end\":44949,\"start\":44946},{\"end\":44963,\"start\":44958},{\"end\":45225,\"start\":45213},{\"end\":45235,\"start\":45229},{\"end\":45246,\"start\":45241},{\"end\":45270,\"start\":45248},{\"end\":45539,\"start\":45534},{\"end\":45557,\"start\":45549},{\"end\":45571,\"start\":45565},{\"end\":45591,\"start\":45583},{\"end\":45609,\"start\":45602},{\"end\":46028,\"start\":46023},{\"end\":46047,\"start\":46038},{\"end\":46064,\"start\":46054},{\"end\":46644,\"start\":46640},{\"end\":46655,\"start\":46653},{\"end\":46668,\"start\":46664},{\"end\":46683,\"start\":46679},{\"end\":46693,\"start\":46691},{\"end\":46704,\"start\":46700},{\"end\":46712,\"start\":46710},{\"end\":46721,\"start\":46717},{\"end\":46738,\"start\":46735},{\"end\":47151,\"start\":47147},{\"end\":47160,\"start\":47157},{\"end\":47175,\"start\":47170},{\"end\":47554,\"start\":47550},{\"end\":47568,\"start\":47564},{\"end\":47577,\"start\":47574},{\"end\":47592,\"start\":47587},{\"end\":47999,\"start\":47995},{\"end\":48012,\"start\":48009},{\"end\":48025,\"start\":48022},{\"end\":48037,\"start\":48034},{\"end\":48048,\"start\":48043},{\"end\":48065,\"start\":48056},{\"end\":48572,\"start\":48568},{\"end\":48586,\"start\":48581},{\"end\":48595,\"start\":48590},{\"end\":48610,\"start\":48604},{\"end\":48622,\"start\":48612},{\"end\":48851,\"start\":48849},{\"end\":48866,\"start\":48861},{\"end\":48876,\"start\":48874},{\"end\":48887,\"start\":48883},{\"end\":48904,\"start\":48901},{\"end\":49213,\"start\":49210},{\"end\":49228,\"start\":49220},{\"end\":49242,\"start\":49236},{\"end\":49254,\"start\":49252},{\"end\":49266,\"start\":49264},{\"end\":49719,\"start\":49717},{\"end\":49730,\"start\":49728},{\"end\":49744,\"start\":49739},{\"end\":49755,\"start\":49752},{\"end\":49769,\"start\":49766},{\"end\":50056,\"start\":50048},{\"end\":50078,\"start\":50070},{\"end\":50091,\"start\":50085},{\"end\":50109,\"start\":50100},{\"end\":50536,\"start\":50531},{\"end\":50551,\"start\":50547},{\"end\":50566,\"start\":50562},{\"end\":50576,\"start\":50573},{\"end\":50587,\"start\":50584},{\"end\":50597,\"start\":50594},{\"end\":50606,\"start\":50602},{\"end\":50620,\"start\":50617},{\"end\":51100,\"start\":51095},{\"end\":51113,\"start\":51110},{\"end\":51123,\"start\":51120},{\"end\":51135,\"start\":51132},{\"end\":51148,\"start\":51144},{\"end\":51159,\"start\":51157},{\"end\":51610,\"start\":51605},{\"end\":51623,\"start\":51621},{\"end\":51632,\"start\":51628},{\"end\":51646,\"start\":51643},{\"end\":52158,\"start\":52154},{\"end\":52169,\"start\":52166},{\"end\":52180,\"start\":52177},{\"end\":52188,\"start\":52185},{\"end\":52203,\"start\":52199},{\"end\":52593,\"start\":52589},{\"end\":52601,\"start\":52598},{\"end\":52612,\"start\":52609},{\"end\":52622,\"start\":52619},{\"end\":52637,\"start\":52633},{\"end\":53043,\"start\":53039},{\"end\":53054,\"start\":53051},{\"end\":53065,\"start\":53063},{\"end\":53075,\"start\":53072},{\"end\":53090,\"start\":53086},{\"end\":53521,\"start\":53517},{\"end\":53534,\"start\":53531},{\"end\":53544,\"start\":53542},{\"end\":53561,\"start\":53558},{\"end\":53576,\"start\":53572},{\"end\":53587,\"start\":53584},{\"end\":54102,\"start\":54098},{\"end\":54115,\"start\":54112},{\"end\":54130,\"start\":54121},{\"end\":54147,\"start\":54137},{\"end\":54170,\"start\":54159},{\"end\":54183,\"start\":54181},{\"end\":54447,\"start\":54443},{\"end\":54456,\"start\":54454},{\"end\":54472,\"start\":54464},{\"end\":54495,\"start\":54484},{\"end\":54511,\"start\":54507},{\"end\":54524,\"start\":54519}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":203705211},\"end\":33120,\"start\":32927},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":14911023},\"end\":33644,\"start\":33122},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":216553255},\"end\":34153,\"start\":33646},{\"attributes\":{\"id\":\"b3\"},\"end\":34474,\"start\":34155},{\"attributes\":{\"doi\":\"arXiv:2005.03201\",\"id\":\"b4\",\"matched_paper_id\":218537849},\"end\":34831,\"start\":34476},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":220633152},\"end\":35200,\"start\":34833},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":4435268},\"end\":35606,\"start\":35202},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":109936942},\"end\":36130,\"start\":35608},{\"attributes\":{\"doi\":\"arXiv:2003.11982\",\"id\":\"b8\"},\"end\":36590,\"start\":36132},{\"attributes\":{\"id\":\"b9\"},\"end\":36756,\"start\":36592},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":49211906},\"end\":36953,\"start\":36758},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":19806033},\"end\":37099,\"start\":36955},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":26294509},\"end\":37287,\"start\":37101},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":237266979},\"end\":37747,\"start\":37289},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":104292104},\"end\":38286,\"start\":37749},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":219628333},\"end\":38825,\"start\":38288},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":6576859},\"end\":39257,\"start\":38827},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":233240801},\"end\":39666,\"start\":39259},{\"attributes\":{\"doi\":\"2019. 3\",\"id\":\"b18\",\"matched_paper_id\":202538634},\"end\":40082,\"start\":39668},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":44073530},\"end\":40540,\"start\":40084},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":233289555},\"end\":41056,\"start\":40542},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":247627826},\"end\":41686,\"start\":41058},{\"attributes\":{\"doi\":\"arXiv:2201.07786\",\"id\":\"b22\"},\"end\":42040,\"start\":41688},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":51882698},\"end\":42346,\"start\":42042},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":237592991},\"end\":42664,\"start\":42348},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":81981856},\"end\":43165,\"start\":42666},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":221266065},\"end\":43703,\"start\":43167},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":4555207},\"end\":44237,\"start\":43705},{\"attributes\":{\"doi\":\"arXiv:2001.05201\",\"id\":\"b28\"},\"end\":44527,\"start\":44239},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":4867611},\"end\":44817,\"start\":44529},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":237100638},\"end\":45152,\"start\":44819},{\"attributes\":{\"id\":\"b31\"},\"end\":45469,\"start\":45154},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":209323961},\"end\":45949,\"start\":45471},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":56894332},\"end\":46548,\"start\":45951},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":221727985},\"end\":47070,\"start\":46550},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":14635094},\"end\":47464,\"start\":47072},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":17164109},\"end\":47902,\"start\":47466},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":41805341},\"end\":48487,\"start\":47904},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":207761262},\"end\":48780,\"start\":48489},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":51869042},\"end\":49138,\"start\":48782},{\"attributes\":{\"doi\":\"2017. 5\",\"id\":\"b40\",\"matched_paper_id\":8485068},\"end\":49644,\"start\":49140},{\"attributes\":{\"doi\":\"arXiv:2002.10137\",\"id\":\"b41\"},\"end\":49970,\"start\":49646},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":159040543},\"end\":50477,\"start\":49972},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":218674474},\"end\":51036,\"start\":50479},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":20541938},\"end\":51505,\"start\":51038},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":235657610},\"end\":52061,\"start\":51507},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":212747537},\"end\":52499,\"start\":52063},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":49905113},\"end\":52994,\"start\":52501},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":204851938},\"end\":53413,\"start\":52996},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":233346930},\"end\":54041,\"start\":53415},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":216553251},\"end\":54378,\"start\":54043},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":207665057},\"end\":54762,\"start\":54380}]", "bib_title": "[{\"end\":32974,\"start\":32927},{\"end\":33231,\"start\":33122},{\"end\":33698,\"start\":33646},{\"end\":34527,\"start\":34476},{\"end\":34882,\"start\":34833},{\"end\":35238,\"start\":35202},{\"end\":35685,\"start\":35608},{\"end\":36793,\"start\":36758},{\"end\":36978,\"start\":36955},{\"end\":37144,\"start\":37101},{\"end\":37340,\"start\":37289},{\"end\":37812,\"start\":37749},{\"end\":38367,\"start\":38288},{\"end\":38901,\"start\":38827},{\"end\":39297,\"start\":39259},{\"end\":39706,\"start\":39668},{\"end\":40104,\"start\":40084},{\"end\":40616,\"start\":40542},{\"end\":41135,\"start\":41058},{\"end\":42083,\"start\":42042},{\"end\":42418,\"start\":42348},{\"end\":42728,\"start\":42666},{\"end\":43241,\"start\":43167},{\"end\":43759,\"start\":43705},{\"end\":44597,\"start\":44529},{\"end\":44914,\"start\":44819},{\"end\":45202,\"start\":45154},{\"end\":45525,\"start\":45471},{\"end\":46014,\"start\":45951},{\"end\":46628,\"start\":46550},{\"end\":47138,\"start\":47072},{\"end\":47541,\"start\":47466},{\"end\":47983,\"start\":47904},{\"end\":48561,\"start\":48489},{\"end\":48841,\"start\":48782},{\"end\":49200,\"start\":49140},{\"end\":50041,\"start\":49972},{\"end\":50519,\"start\":50479},{\"end\":51085,\"start\":51038},{\"end\":51595,\"start\":51507},{\"end\":52147,\"start\":52063},{\"end\":52582,\"start\":52501},{\"end\":53032,\"start\":52996},{\"end\":53510,\"start\":53415},{\"end\":54091,\"start\":54043},{\"end\":54436,\"start\":54380}]", "bib_author": "[{\"end\":32990,\"start\":32976},{\"end\":33005,\"start\":32990},{\"end\":33247,\"start\":33233},{\"end\":33271,\"start\":33247},{\"end\":33713,\"start\":33700},{\"end\":33729,\"start\":33713},{\"end\":33745,\"start\":33729},{\"end\":33763,\"start\":33745},{\"end\":34234,\"start\":34223},{\"end\":34244,\"start\":34234},{\"end\":34266,\"start\":34244},{\"end\":34281,\"start\":34266},{\"end\":34295,\"start\":34281},{\"end\":34309,\"start\":34295},{\"end\":34540,\"start\":34529},{\"end\":34553,\"start\":34540},{\"end\":34563,\"start\":34553},{\"end\":34578,\"start\":34563},{\"end\":34592,\"start\":34578},{\"end\":34895,\"start\":34884},{\"end\":34908,\"start\":34895},{\"end\":34920,\"start\":34908},{\"end\":34930,\"start\":34920},{\"end\":34940,\"start\":34930},{\"end\":34947,\"start\":34940},{\"end\":34961,\"start\":34947},{\"end\":35251,\"start\":35240},{\"end\":35263,\"start\":35251},{\"end\":35271,\"start\":35263},{\"end\":35286,\"start\":35271},{\"end\":35302,\"start\":35286},{\"end\":35306,\"start\":35302},{\"end\":35698,\"start\":35687},{\"end\":35706,\"start\":35698},{\"end\":35721,\"start\":35706},{\"end\":35737,\"start\":35721},{\"end\":35741,\"start\":35737},{\"end\":36211,\"start\":36187},{\"end\":36225,\"start\":36211},{\"end\":36237,\"start\":36225},{\"end\":36250,\"start\":36237},{\"end\":36262,\"start\":36250},{\"end\":36276,\"start\":36262},{\"end\":36290,\"start\":36276},{\"end\":36305,\"start\":36290},{\"end\":36318,\"start\":36305},{\"end\":36323,\"start\":36318},{\"end\":36632,\"start\":36616},{\"end\":36648,\"start\":36632},{\"end\":36666,\"start\":36648},{\"end\":36806,\"start\":36795},{\"end\":36817,\"start\":36806},{\"end\":36830,\"start\":36817},{\"end\":36990,\"start\":36980},{\"end\":37004,\"start\":36990},{\"end\":37015,\"start\":37004},{\"end\":37156,\"start\":37146},{\"end\":37170,\"start\":37156},{\"end\":37181,\"start\":37170},{\"end\":37367,\"start\":37342},{\"end\":37387,\"start\":37367},{\"end\":37409,\"start\":37387},{\"end\":37823,\"start\":37814},{\"end\":37835,\"start\":37823},{\"end\":37850,\"start\":37835},{\"end\":37864,\"start\":37850},{\"end\":37875,\"start\":37864},{\"end\":37886,\"start\":37875},{\"end\":38386,\"start\":38369},{\"end\":38398,\"start\":38386},{\"end\":38420,\"start\":38398},{\"end\":38914,\"start\":38903},{\"end\":38930,\"start\":38914},{\"end\":39309,\"start\":39299},{\"end\":39320,\"start\":39309},{\"end\":39336,\"start\":39320},{\"end\":39346,\"start\":39336},{\"end\":39363,\"start\":39346},{\"end\":39372,\"start\":39363},{\"end\":39381,\"start\":39372},{\"end\":39723,\"start\":39708},{\"end\":39741,\"start\":39723},{\"end\":39760,\"start\":39741},{\"end\":39779,\"start\":39760},{\"end\":39793,\"start\":39779},{\"end\":39813,\"start\":39793},{\"end\":39833,\"start\":39813},{\"end\":40121,\"start\":40106},{\"end\":40136,\"start\":40121},{\"end\":40150,\"start\":40136},{\"end\":40162,\"start\":40150},{\"end\":40176,\"start\":40162},{\"end\":40195,\"start\":40176},{\"end\":40210,\"start\":40195},{\"end\":40230,\"start\":40210},{\"end\":40249,\"start\":40230},{\"end\":40269,\"start\":40249},{\"end\":40631,\"start\":40618},{\"end\":40644,\"start\":40631},{\"end\":40659,\"start\":40644},{\"end\":40668,\"start\":40659},{\"end\":40682,\"start\":40668},{\"end\":40690,\"start\":40682},{\"end\":40704,\"start\":40690},{\"end\":41147,\"start\":41137},{\"end\":41158,\"start\":41147},{\"end\":41169,\"start\":41158},{\"end\":41181,\"start\":41169},{\"end\":41191,\"start\":41181},{\"end\":41202,\"start\":41191},{\"end\":41216,\"start\":41202},{\"end\":41226,\"start\":41216},{\"end\":41234,\"start\":41226},{\"end\":41246,\"start\":41234},{\"end\":41769,\"start\":41759},{\"end\":41781,\"start\":41769},{\"end\":41792,\"start\":41781},{\"end\":41803,\"start\":41792},{\"end\":41813,\"start\":41803},{\"end\":41825,\"start\":41813},{\"end\":42103,\"start\":42085},{\"end\":42118,\"start\":42103},{\"end\":42131,\"start\":42118},{\"end\":42145,\"start\":42131},{\"end\":42432,\"start\":42420},{\"end\":42447,\"start\":42432},{\"end\":42456,\"start\":42447},{\"end\":42744,\"start\":42730},{\"end\":42757,\"start\":42744},{\"end\":42773,\"start\":42757},{\"end\":42786,\"start\":42773},{\"end\":43265,\"start\":43243},{\"end\":43279,\"start\":43265},{\"end\":43288,\"start\":43279},{\"end\":43304,\"start\":43288},{\"end\":43313,\"start\":43304},{\"end\":43775,\"start\":43761},{\"end\":43790,\"start\":43775},{\"end\":43804,\"start\":43790},{\"end\":43822,\"start\":43804},{\"end\":43840,\"start\":43822},{\"end\":44327,\"start\":44314},{\"end\":44337,\"start\":44327},{\"end\":44348,\"start\":44337},{\"end\":44610,\"start\":44599},{\"end\":44623,\"start\":44610},{\"end\":44633,\"start\":44623},{\"end\":44648,\"start\":44633},{\"end\":44660,\"start\":44648},{\"end\":44929,\"start\":44916},{\"end\":44940,\"start\":44929},{\"end\":44951,\"start\":44940},{\"end\":44965,\"start\":44951},{\"end\":45227,\"start\":45204},{\"end\":45237,\"start\":45227},{\"end\":45248,\"start\":45237},{\"end\":45272,\"start\":45248},{\"end\":45541,\"start\":45527},{\"end\":45559,\"start\":45541},{\"end\":45573,\"start\":45559},{\"end\":45593,\"start\":45573},{\"end\":45611,\"start\":45593},{\"end\":46030,\"start\":46016},{\"end\":46049,\"start\":46030},{\"end\":46066,\"start\":46049},{\"end\":46646,\"start\":46630},{\"end\":46657,\"start\":46646},{\"end\":46670,\"start\":46657},{\"end\":46685,\"start\":46670},{\"end\":46695,\"start\":46685},{\"end\":46706,\"start\":46695},{\"end\":46714,\"start\":46706},{\"end\":46723,\"start\":46714},{\"end\":46740,\"start\":46723},{\"end\":47153,\"start\":47140},{\"end\":47162,\"start\":47153},{\"end\":47177,\"start\":47162},{\"end\":47556,\"start\":47543},{\"end\":47570,\"start\":47556},{\"end\":47579,\"start\":47570},{\"end\":47594,\"start\":47579},{\"end\":48001,\"start\":47985},{\"end\":48014,\"start\":48001},{\"end\":48027,\"start\":48014},{\"end\":48039,\"start\":48027},{\"end\":48050,\"start\":48039},{\"end\":48067,\"start\":48050},{\"end\":48574,\"start\":48563},{\"end\":48588,\"start\":48574},{\"end\":48597,\"start\":48588},{\"end\":48612,\"start\":48597},{\"end\":48624,\"start\":48612},{\"end\":48853,\"start\":48843},{\"end\":48868,\"start\":48853},{\"end\":48878,\"start\":48868},{\"end\":48889,\"start\":48878},{\"end\":48906,\"start\":48889},{\"end\":49215,\"start\":49202},{\"end\":49230,\"start\":49215},{\"end\":49244,\"start\":49230},{\"end\":49256,\"start\":49244},{\"end\":49268,\"start\":49256},{\"end\":49721,\"start\":49713},{\"end\":49732,\"start\":49721},{\"end\":49746,\"start\":49732},{\"end\":49757,\"start\":49746},{\"end\":49771,\"start\":49757},{\"end\":50058,\"start\":50043},{\"end\":50080,\"start\":50058},{\"end\":50093,\"start\":50080},{\"end\":50111,\"start\":50093},{\"end\":50538,\"start\":50521},{\"end\":50553,\"start\":50538},{\"end\":50568,\"start\":50553},{\"end\":50578,\"start\":50568},{\"end\":50589,\"start\":50578},{\"end\":50599,\"start\":50589},{\"end\":50608,\"start\":50599},{\"end\":50622,\"start\":50608},{\"end\":51102,\"start\":51087},{\"end\":51115,\"start\":51102},{\"end\":51125,\"start\":51115},{\"end\":51137,\"start\":51125},{\"end\":51150,\"start\":51137},{\"end\":51161,\"start\":51150},{\"end\":51612,\"start\":51597},{\"end\":51625,\"start\":51612},{\"end\":51634,\"start\":51625},{\"end\":51648,\"start\":51634},{\"end\":52160,\"start\":52149},{\"end\":52171,\"start\":52160},{\"end\":52182,\"start\":52171},{\"end\":52190,\"start\":52182},{\"end\":52205,\"start\":52190},{\"end\":52595,\"start\":52584},{\"end\":52603,\"start\":52595},{\"end\":52614,\"start\":52603},{\"end\":52624,\"start\":52614},{\"end\":52639,\"start\":52624},{\"end\":53045,\"start\":53034},{\"end\":53056,\"start\":53045},{\"end\":53067,\"start\":53056},{\"end\":53077,\"start\":53067},{\"end\":53092,\"start\":53077},{\"end\":53523,\"start\":53512},{\"end\":53536,\"start\":53523},{\"end\":53546,\"start\":53536},{\"end\":53563,\"start\":53546},{\"end\":53578,\"start\":53563},{\"end\":53589,\"start\":53578},{\"end\":54104,\"start\":54093},{\"end\":54117,\"start\":54104},{\"end\":54132,\"start\":54117},{\"end\":54149,\"start\":54132},{\"end\":54172,\"start\":54149},{\"end\":54185,\"start\":54172},{\"end\":54449,\"start\":54438},{\"end\":54458,\"start\":54449},{\"end\":54474,\"start\":54458},{\"end\":54497,\"start\":54474},{\"end\":54513,\"start\":54497},{\"end\":54526,\"start\":54513}]", "bib_venue": "[{\"end\":33406,\"start\":33347},{\"end\":33918,\"start\":33849},{\"end\":35421,\"start\":35372},{\"end\":35896,\"start\":35827},{\"end\":37538,\"start\":37482},{\"end\":38035,\"start\":37969},{\"end\":38583,\"start\":38510},{\"end\":39051,\"start\":38999},{\"end\":40813,\"start\":40767},{\"end\":41395,\"start\":41329},{\"end\":42941,\"start\":42872},{\"end\":43448,\"start\":43389},{\"end\":43995,\"start\":43926},{\"end\":45726,\"start\":45677},{\"end\":46221,\"start\":46152},{\"end\":48222,\"start\":48153},{\"end\":49416,\"start\":49354},{\"end\":50246,\"start\":50187},{\"end\":50785,\"start\":50712},{\"end\":51282,\"start\":51230},{\"end\":51811,\"start\":51738},{\"end\":52762,\"start\":52709},{\"end\":53227,\"start\":53168},{\"end\":53744,\"start\":53675},{\"end\":33013,\"start\":33005},{\"end\":33345,\"start\":33271},{\"end\":33847,\"start\":33763},{\"end\":34221,\"start\":34155},{\"end\":34630,\"start\":34608},{\"end\":35006,\"start\":34961},{\"end\":35370,\"start\":35306},{\"end\":35825,\"start\":35741},{\"end\":36185,\"start\":36132},{\"end\":36614,\"start\":36592},{\"end\":36841,\"start\":36830},{\"end\":37019,\"start\":37015},{\"end\":37185,\"start\":37181},{\"end\":37480,\"start\":37409},{\"end\":37967,\"start\":37886},{\"end\":38508,\"start\":38420},{\"end\":38997,\"start\":38930},{\"end\":39450,\"start\":39381},{\"end\":39868,\"start\":39840},{\"end\":40303,\"start\":40269},{\"end\":40765,\"start\":40704},{\"end\":41327,\"start\":41246},{\"end\":41757,\"start\":41688},{\"end\":42179,\"start\":42145},{\"end\":42490,\"start\":42456},{\"end\":42870,\"start\":42786},{\"end\":43387,\"start\":43313},{\"end\":43924,\"start\":43840},{\"end\":44312,\"start\":44239},{\"end\":44665,\"start\":44660},{\"end\":44976,\"start\":44965},{\"end\":45300,\"start\":45272},{\"end\":45675,\"start\":45611},{\"end\":46150,\"start\":46066},{\"end\":46778,\"start\":46740},{\"end\":47258,\"start\":47177},{\"end\":47674,\"start\":47594},{\"end\":48151,\"start\":48067},{\"end\":48627,\"start\":48624},{\"end\":48951,\"start\":48906},{\"end\":49352,\"start\":49275},{\"end\":49711,\"start\":49646},{\"end\":50185,\"start\":50111},{\"end\":50710,\"start\":50622},{\"end\":51228,\"start\":51161},{\"end\":51736,\"start\":51648},{\"end\":52270,\"start\":52205},{\"end\":52707,\"start\":52639},{\"end\":53166,\"start\":53092},{\"end\":53673,\"start\":53589},{\"end\":54189,\"start\":54185},{\"end\":54560,\"start\":54526}]"}}}, "year": 2023, "month": 12, "day": 17}