{"id": 252284059, "updated": "2023-10-05 10:52:49.249", "metadata": {"title": "Revisiting Crowd Counting: State-of-the-art, Trends, and Future Perspectives", "authors": "[{\"first\":\"Muhammad\",\"last\":\"Khan\",\"middle\":[\"Asif\"]},{\"first\":\"Hamid\",\"last\":\"Menouar\",\"middle\":[]},{\"first\":\"Ridha\",\"last\":\"Hamila\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Crowd counting is an effective tool for situational awareness in public places. Automated crowd counting using images and videos is an interesting yet challenging problem that has gained significant attention in computer vision. Over the past few years, various deep learning methods have been developed to achieve state-of-the-art performance. The methods evolved over time vary in many aspects such as model architecture, input pipeline, learning paradigm, computational complexity, and accuracy gains etc. In this paper, we present a systematic and comprehensive review of the most significant contributions in the area of crowd counting. Although few surveys exist on the topic, our survey is most up-to date and different in several aspects. First, it provides a more meaningful categorization of the most significant contributions by model architectures, learning methods (i.e., loss functions), and evaluation methods (i.e., evaluation metrics). We chose prominent and distinct works and excluded similar works. We also sort the well-known crowd counting models by their performance over benchmark datasets. We believe that this survey can be a good resource for novice researchers to understand the progressive developments and contributions over time and the current state-of-the-art.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2209.07271", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/ivc/KhanMH23", "doi": "10.1016/j.imavis.2022.104597"}}, "content": {"source": {"pdf_hash": "018c2532d7d846506cd12711ce85bbcda1f25204", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2209.07271v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "ac8991186edcae693607d3aa70675f199c1e4c70", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/018c2532d7d846506cd12711ce85bbcda1f25204.txt", "contents": "\nSUBMITTED TO JOURNAL OF IMAGE AND VISION COMPUTING 1 Revisiting Crowd Counting: State-of-the-art, Trends, and Future Perspectives\n\n\nMuhammad Asif Khan \nHamid Menouar \nRidha Hamila \nSUBMITTED TO JOURNAL OF IMAGE AND VISION COMPUTING 1 Revisiting Crowd Counting: State-of-the-art, Trends, and Future Perspectives\nIndex Terms-Crowd countingCNNdensity estimationeval- uation metricsloss functionstransformers\nCrowd counting is an effective tool for situational awareness in public places. Automated crowd counting using images and videos is an interesting yet challenging problem that has gained significant attention in computer vision. Over the past few years, various deep learning methods have been developed to achieve state-of-the-art performance. The methods evolved over time vary in many aspects such as model architecture, input pipeline, learning paradigm, computational complexity, and accuracy gains etc. In this paper, we present a systematic and comprehensive review of the most significant contributions in the area of crowd counting. Although few surveys exist on the topic, our survey is most up-to date and different in several aspects. First, it provides a more meaningful categorization of the most significant contributions by model architectures, learning methods (i.e., loss functions), and evaluation methods (i.e., evaluation metrics). We chose prominent and distinct works and excluded similar works. We also sort the well-known crowd counting models by their performance over benchmark datasets. We believe that this survey can be a good resource for novice researchers to understand the progressive developments and contributions over time and the current state-of-the-art.\n\nI. INTRODUCTION\n\nCrowd counting is an interesting research area with many real-world applications. The increasing population and growing urbanization trends often result in rapid crowds creation in urban areas such as metro stations, sports venues, musical concerts, exhibition centers, and parade grounds etc. The prediction of the crowds creation, and the density estimation in crowded regions is of paramount significance in crowd monitoring, management, and effective events planning. The social distancing measures to reduce the spread of virus observed during the recent COVID-19 pandemic further highlight its significance [1], [2]. Owing to the importance of the problem, a huge amount of research exists on automated crowd counting using image and video analysis methods. Although traditional image processing methods have shown limited performance, the last decade has witnessed major improvements using stateof-the-art methods in computer vision and deep learning.\n\nCrowd counting is generally implemented in two ways: (i) counting objects (input is an image and the output is a number i.e., total head count in the image.), and (ii) density map estimation (input is an image and the output is the density map of the crowd which is then integrated to get the total head count.).\n\nTraditional methods for crowd counting were all based on total count approach. These methods employ image processing M techniques to detect hand-crafted features e.g., body appearance [3], [4], [5], or body parts [6], [7], [8], [9], [10], [11] and then use machine learning models such as linear regression, ridge regression, Gaussian process, support vector machines (SVMs), random forest, gradient boost, and neural networks to find the total head count in an image. However, all the accuracy of these methods significantly degrades on images with dense crowds due to challenges such as occlusions, low resolution, foreshortening and perspectives. Due to the aforementioned limitations of detection-based approach, regression-based methods were adopted which regress the total count from an image or image patch. Instead of detecting body parts or shapes, regression-based methods use global features such as texture [12], foreground [13], gradients [14], [15] etc. to estimate image-wise or patch-wise total count [16], [17]. Regression-based method solve the problems raised from occlusions, low resolution, and perspective. However, regression-based method show poor performance on high density crowd images.\n\nRecent research on crowd counting shows the efficacy of convolution neural networks (CNNs) [18], [19], [20], [21], [22], [23] due to their strong capability of automatic feature extraction. Like many other computer vision problems, such as image classification [24], [25], object detection [26], and image segmentation [27], [28], CNN is used as a widely applied method for crowd counting and it outperforms all the traditional methods. While traditional methods for crowd counting predict the total head count in an image, CNN-based methods often employ crowd density estimation. Density estimation is when the CNN model is used to predict the density map of the crowd scene rather than just the head count. The density map contains additional location information along with the total head count of the crowd scene.\n\nFirst introduced in [29], density estimation using CNNs has been adopted in all subsequent research contributions. However, there have been major architectural improvements to achieve optimum performance. As the performance of any deep learning method typically is evaluated using benchmark datasets, many crowd datasets were also published over time. These datasets published over time introduced more challenges in terms of high density, scale variation, scene variation, and illumination changes, crowd distribution, extreme occlusions, perspective distortions etc. To overcome these challenges and to achieve high accuracy over these datasets, complex CNN architectures, novel learning methods, and sophisticated evaluation criteria were developed over time. This survey covers all these developments so that a novice researcher in the area of crowd counting can understand these advancements in an incremental manner. Although there exists few relevant arXiv:2209.07271v1 [cs.CV] 14 Sep 2022 surveys [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], we believe our work is different in several aspects:\n\n\u2022 There has been several significant contributions over the last 1-3 years which are not covered in the old surveys [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40]. These new contributions span multiple directions including novel and improved loss functions, more accurate evaluation metrics, and novel network architectures (e.g., transformer-based models). Thus, this survey is a more up to date reference on the topic. \u2022 We provide a highly intuitive review of the state-of-the-art by more meaningful segregation of the literature so that each contribution is discussed separately. For instance, the segregation of works by network architecture, loss functions, training methods etc. and then sorting by performance is of more interest to the novice researchers as well as general readers. \u2022 Unlike other surveys, we intend to focus our discussion on the most significant contributions based on their impact and the relative performance gains over previous works. By excluding redundant and least significant contributions (in terms of novelty or performance), our survey is concise, more focused and more readable to follow the actual advancement of the state-of-the-art rather than repeatedly discussing similar ideas of least impact. The rest of the paper is organized as follows: We first discuss (in Section II) the various challenges of crowd counting with a visual illustration. Then a brief overview of crowd datasets used in benchmarking of previous research works is provided in Section III. Section IV explains the data annotation techniques applied in crowd counting and density estimation. Section V presents a comprehensive review, critical analysis, and performance comparison of crowd counting models. Section VI presents an indepth analysis of training and evaluation methods for crowd counting covering all the loss functions and evaluation metrics used in the literature. Lastly, we concluded the paper in Section VII with some interesting insights for prospective researchers working in this area.\n\n\nII. CROWD COUNTING CHALLENGES\n\nA robust crowd counting model aims to accurately predict crowd count and estimate local as well as global density in crowd scenes under diverse conditions. The varying conditions heavily impact the performance of the counting model. Hence, it is therefore important to first understand these challenges and their possible impact on the model performance. A good understanding of these challenges helps in developing more robust models. Some of the more common challenges in crowd counting are listed here with a visual illustration in Fig. 1. 1) Scene variation: Crowd monitoring across different scenes is more challenging. For instance, counting people in images taken by a single CCTV camera (e.g., in Mall dataset [43]) is much easier than counting people in images taken by several surveillance cameras (e.g., in WorldExpo'10 dataset [29]). Scene variations is common in drone surveillance and becomes more challenging when combined with other variations listed afterwards. 2) Scale Variation: Scale variation refers to the situation when objects related to the same class (i.e., humans) appear in different sizes in a single image as well as in different images. Scale variation is caused by distance (between camera and objects) and perspective effect in the same image. Moreover, scale variations are also observed in images of different resolutions. Scale variation is one of the most common challenges that hugely impact the model performance and is frequently addressed in crowd counting research. 3) Crowd Density: The number of people (or other objects of interests) vary from one image to another. Usually, images with low density are easy than highly dense images. Another challenge is when the same image contains different crowd densities in different regions. 4) Non-uniform People Distribution: Crowd images may differ in terms of distribution of objects (e.g., people). For instance, people sitting in a sports stadium are uniformly distributed with constant distance among objects.\n\nIn contrast, objects may be distributed randomly in street crowds. Uniformly distributed crowds can be estimated more accurately than non-uniform crowds in the absence of other attributes affecting the accuracy. 5) Occlusions: Occlusion refers to the overlap in objects.\n\nWhen similar objects (e.g., people) overlap, it is called intra-class occlusion, whereas when different objects (e.g., people, cars, walls) overlap, it is called inter-class occlusion. Dealing with occlusion is often challenging. It is not only difficult for annotators to annotate the objects in the presence of occlusion, but also challenging for the object detectors to accurately predict objects. Occlusion interweave semantic features in images making it difficult to discriminate object boundaries. 6) Complex background: The background region which do not contain objects of interest vary in the same images as well as across different images. When the pixel values in the background have similar values as that of the object, it can make learning hard. 7) Illumination variations: The illumination in an image vary at different times of a day and may vary in different regions in the same images due to lighting conditions. Thus the same object (e.g., people) in the same image will have different pixel values, which makes learning challenging. 8) Other variations: The aforementioned attributes in crowd images is not an exhaustive list. There are several other attributes that make counting a more challenging problem. Few examples are weather changes, image distortion and noise, object rotation etc.\n\nIII. CROWD COUNTING DATASETS A number of benchmarking datasets have been introduced over time that are being used for evaluating crowd counting models. These datasets vary in size (number of samples), annotations, and image attributes. Table I provides a brief summary of these datasets.     it can also be used in count regression models. 3) Bounding box: A rectangular bounding box is drawn around each object (e.g., person) in the image. Total count annotation is cheaper; however, it does not provide a good accuracy due to issues such as object occlusions. Dot annotation can outperform the total count, but it does not capture information such as object scaling, and it provide limited accuracy when the objects scale largely vary in images.\n\n\nB. Density-map generation\n\nDot annotations is the most common and a relatively cheaper (than bounding box) annotation technique. However, the dot map or localization map which contains all zeros except head positions, are extremely sparse and it is generally very hard to train neural network over them [46]. Thus, in crowd counting research, such localization maps are converted to density maps. If x i is a pixel containing the the head position, it can be represented by a delta function \u03b4(x \u2212 x i ). The density map is generated by convolving the delta function with a Gaussian kernel G \u03c3 .\nY = N i=1 \u03b4(x \u2212 x i ) * G \u03c3(1)\nwhere, N is the total number of annotated points (i.e., total count of heads) in the image. The integral of density map Y is equal to the total head count in the image. Visually, this operation creates a blurring of each head annotation using the scale parameter \u03c3. There are various kernel settings to generate a variety of density maps. The most basic approach is to keep \u03c3 fixed value, which means that the density map will apply same kernel to all head positions irrespective of their scale in the image. Typical values of \u03c3 used in earlier works are 15 [20], X [29]. Fixed Gaussian kernels are not helpful to learn the scale variations in images. Hence, adaptive kernels are proposed. In adaptive kernels, the value of \u03c3 can be calculated as the average distance to k-nearest neighboring head annotations. Visually, it generates lower degree of Gaussian blur for dense crowds and higher degree for \n\n\nSummary and Lessons Learned\n\n\u2022 Fixed Gaussian kernels are adopted in earlier models because these are computationally cheaper than adaptive kernels. The value of \u03c3 is often pre-determined empirically and can vary across different datasets. Interestingly, different values of \u03c3 have been used for the same dataset in different works which potentially indicate the value may be fine-tuned for the CNN model to get better results. \u2022 Adaptive Gaussian kernels are increasingly used to cope with the scale variations in images. Earlier models used pre-determined values calculated as the weighted interhead distances using K-nearest neighbors. The value of k depends upon the crowd density i.e., higher value of k for dense images and vice versa. \u2022 Some recent works propose to learn adaptive kernels alongside model training (explained in Section VI).\n\n\nV. CROWD COUNTING MODELS\n\nA wide numbers of DNN models are proposed for crowd counting over the period. These generally vary in size as well as design structure. We studied these models and categorized them into three distinct design categories i.e., (i) singlecolumn, (ii) multi-column, and (iii) hybrid. While one may argue that such design patterns are common in other CNN models, it is important to understand the structure of these models in the context of crowd counting.\n\n\nA. Single-column Models\n\nOne of the early models for crowd counting using CNN is reported in CrowdCNN [29]. The CrowdCNN model consists of three convolution (Conv) layers (with kernels of 7 \u00d7 7, 7 \u00d7 7, and 5 \u00d7 5 respectively) followed by three fully connected (FC) layers. The input to the model is 72 patches cropped from image, whereas the output is a density map In the absence of perspective maps, there may be significant performance degradation. Hence, very deep single-column models (e.g., FusionCount [60]), multi-column models, or single-column models with transfer learning are proposed over time. In [61], authors propose a single-column CNN (SaCNN) architecture with seven (7) Conv layers all using a single filter size (3 \u00d7 3). The model uses explicitly 1 \u00d7 1 convolutions before 1 \u00d7 1 for dimension reduction and does not employ any pre-trained front-end CNN. The SaCNN performs multitask learning i.e., simultaneously predict density map and head count. In [58], authors used a two-layer with kernel size of 3x3 using VGG-19 as backbone. In compact-CNN (C-CNN) [59], authors used three parallel filters of different size producing different outputs, which are merged and fed to five-layer single column CNN network. Other single-column architectures include LibraNet [62].\n\n\nB. Multi-column Models\n\nSingle-column models usually are not good at detecting scale variations in object sizes. Multi-column approaches are favoured to learn multi-scale and perspective-free feature detection capability. The first multi-column CNN (MCNN) is proposed in [18]. MCNN is a three-layers CNN architecture with difference receptive fields in each column. Each column in MCNN produces a density map of the same shape as ground truth map. The individual column's outputs are then concatenated to produce the final density map. MCNN takes an image of arbitrary size. Several works followed similar design pattern. For instance, the CrowdNet [63] model proposes a two-column CNN architecture. The first column is fivelayer deep network whereas the second column is a threelayer shallow network. Similar to MCNN, the outputs of both columns are concatenated to predict the final density map. One of the drawback of multi-column models is the computation overhead. To dead with this, the Switch CNN (SCNN) [64] model propose a 3-column network with different sized kernels in each column. However, an additional CNN network called classifier is used to automatically select only one column at inference time based on the crowd density.\n\nA relatively simpler two-column network called cascaded multi-task learning (CMTL) is proposed in [65] which jointly learns the density map as well as count density in images. In the two-column architecture, the first column predicts a high-level prior (i.e., the total head count) whereas the second column estimates the density map. Authors in [66] propose a detection and density estimation network (DecideNet) to jointly estimate count and density map. It consists of three columns however, unlike MCNN [18], each column in Deci-deNet performs a different task. The first column RegNet is a 5-layer CNN which predicts the density map in the absence of target density map. The second column DetNet uses Faster R-CNN network [67] which first predicts the bounding box and then generates a detection-based density map. The third column QualityNet uses the two density maps generated by RegNet and DetNet, and predicts the final density map. In [68], authors propose a four-column CNN network with only the first four layers of VGG-16 as backbone. The first three columns generates density maps. Due to different size of filters in the three columns, when their outputs (density maps) are combined, it can adapt to the scale variation in person size. The fourth layer generates two perspective map which are used by the first two layers as intermediate inputs. The DeepCount [57] architecture uses a large number of Conv layers and additionally introduces the intermediate fusion of layers at different stages. A different architecture is proposed in ASNet [22] which starts as a single-column (first seven layers), and then branch out into two columns. The first column produces density map, whereas the second column generates the scaling factor to be used in density estimation.\n\nRecently, multi-column models are increasingly used for multi-task learning. In PCCNet [69], authors use a three column network to jointly learn the density map, density class, and segmentation map in crowd images. The three columns share encoded features at various stages. Other multi-column architectures include MATT [70], [71], TAFNet [72].\n\n\nC. Encoder-Decoder Models\n\nDense CNN networks (e.g., VGG [24], Inception [25], ResNets [73]) designed for classification encode low-level spatial features into high-level semantic information. The down sampling operations used in these models causes loss of pixel-level localization information while encoding features. To generate the density map, the decoder layers aggregates the encoded feature maps to produce the final density map. TEDnet uses the encoder-decoder architecture with hierarchical feature aggregation at different encoding stages. Both the encoder and decoder blocks uses multi-path encoding and decoding modules. The encoder block in TEDnet [55] uses the same multi-scale module as in Fig 5 which are originally proposed in [20]. Nine (9) multi-scale modules are cascaded with 2 \u00d7 2 pooling after the first two modules only. In the decoder block, multi-path decoding modules are used to aggregate the features and restore the spatial resolution in the predicted density maps. The network loss in TEDnet is computed at three intermediate levels in the decoder block, and one at the final output of the model. A lightweight encoder-decoder architecture is MobileCount [56] designed for computational efficiency. The encoder block consists of first four layers of MobileNet V2 [74] and decoder block based on RefineNet [75]. In [76], authors propose a compact encoderdecoder model called attentional neural fields (ANF). ANF uses conditional random fields (CRFs) to aggregate multi-scale features inside encoder-decoder model. Both the encoder and decoder blocks have six (6) Conv layers with both inter-layer and intra-layer attention mechanism to capture dependencies among features of same scale and across different scales. Other examples include SASNet [77], MFCC [78].\n\n\nD. Single-column Models with Multi-column modules\n\nThis type of network architecture has been used in many works mainly to cope with the shortcomings of both singlecolumn and multi-column modules. Single column models are less complex but perform poorly in detecting multi-scale features wheres multi-column can detect multi-scale features but their capability is limited by the number of columns. The fact that adding more columns in multi-column models will significantly increase the size of the model, several works introduce special multi-path or multi-column modules cascaded into a single-column network. The first crowd counting model using this type of architecture was multi-scale CNN (MSCNN) [54]. MSCNN is a single-column CNN network that contains three multi-scale blobs (MSBs). Inspired from the Inception model [25], the MSB is a naive Inception module ( Fig. 4) with four different sizes of Conv filters (i.e., 3 \u00d7 3, 5 \u00d7 5, 7 \u00d7 7, 9 \u00d7 9).\n\nA slightly different multi-scale module called as scaleaggregation module (Fig. 5) is proposed in [20] using 1 \u00d7 1 convolution layers. Although not explicitly stated by authors, the scale-aggregate module is also an Inception-like module using 1\u00d71 convolution. The 1\u00d71 convolution layer when used before Conv layers of larger receptive field such as 5 \u00d7 5 and 7\u00d77 apply dimension reduction and thus reduce computations.\n\nOther examples include M-SegNet [79], SGANet [23].\nMulti-Scale Blob (MSB) Conv2D (9x9) Conv2D (7x7) Conv2D (5x5) Conv2D\n(3x3) C Previous Layer Next Layer Fig. 4: Inception [25] like multi-scale blob (MSB) used in MSCNN [54]. \n\n\nScale-aggregation Module\nConv2D (7x7) Conv2D (5x5) Conv2D (3x3) Conv2D (1x1) C Previous Layer Next Layer Conv2D (1x1) Conv2D (1x1) Conv2D (1x1)\n\nE. Models with Transfer Learning\n\nSmall crowd counting models typically suffer from accuracy degradation when used on high density scenes. To improve the accuracy of crowd models on highly dense scenes, transfer learning is becoming a promising approach. Although earlier models such as CrowdNet [63] has been using this approach, its performance was limited. The first model that showed significant boost in performance using transfer-learning is congested scene recognition (CSRNet) [19]. CSRNet uses the first 10 layers of VGG-16 model [24] pre-trained on ImageNet dataset [80] as front-end CNN. The back-end CNN used dilated-convolution instead of pooling operation making CSRNet an all-convolution network with larger receptive fields which is easier to train. Transfer-learning is typically used in any type of architecture i.e., single-column, multi-column, encoder-decoder etc.\n\n\nF. Other CNN Models\n\nAlthough, most crowd counting models inspires from other works and follow similar design patterns, there are few works that do not fit into either of the aforementioned categories. In other word, there is no particular design pattern followed and the network is formed by connecting various layers in different ways (i.e., cascaded, parallel, feedback paths, skip connections etc.). One such model is context-aware network (CANNet) [81] which comprised of a VGG16 front-end (10 layers), proceeded by a multi-path network (consisting of convolution and up-sampling layers), which concatenates to connect to a single-column decoder network. Similarly, the perspective-guided convolution network (PGCNet) in [21] uses a two column architecture. The first column is a density map prediction network (DMPNet), whereas the second column is the perspective estimation network (PENet). The DMPNet uses CSRNet [19] as a backbone, and then use multiple perspective-guided convolution (PGC) modules cascaded in a single column. The PENet on the other hand, uses an encoderdecoder CNN network to estimate the perspective map of the input image.\n\nIn [82], authors propose multi-modal counting network that accepts both RGB images and infrared images of the same scene as inputs to predict the density maps. The MMCNN architecture is also a complex network with inter-connections among various layers at different stages. MMCNN also uses ResNet [73] based multi-scale modules.\n\n\nG. Transformer-based Models\n\nTransfomer models proposed in [83] have recently emerged as a new paradigm of neural networks with self-attention mechanism. Due to their remarkable performance in several applications such as natural language processing [] and speech processing []. These profound performance of transfomers in other domains has sparked the computer vision community to apply transfomers in various vision applications. Vision transformer (ViT) [84] is the first transformer model applied in computer vision for image classification. Since then, transformers have been used in various vision applications including image recognition [85], object detection [86], [87], and image segmentation [88].\n\nThe original transformer [83] uses an encoder-decoder structure with self-attention mechanism. The encoder transform an input sequence (words in NLP) into a high dimensional feature vector whereas the decoder then translate it into the output sequence. When applied in vision tasks [84], the input image is split into patches and then the patches are provided to the transformer's encoder as linear embeddings (to mimic sequences). Recently, transformers have been applied in crowd counting tasks [89], [90], [91], [92], [93], [94], [95], [96]. For example, the TransCrowd model [96] uses pure transformer model in crowd counting task. It convert each image of size (1152x768) into fixed size six (6) patches of size (384x384) and convert patches to sequences of pixel values in the patch (flattening). The sequences are fed to the encoder of the ViT transformer (pretrained on ImageNet) [84]. The output is then fed to a regression head which predict the total count in the image. Unlink TransCrowd [96] which predicts the total count, authors in [91] propose CCTrans i.e., a transformer model to predict density maps. CCTrans uses similar input pipeline as TransCrowd but uses different model architecture. It uses Twins [97] as backbone as feature encoder. The 1D output is transformed into feature maps (2D vectors), upsampled to 1 8 of the input patch size. The feature maps are then fed to regression heads to predict density maps.\n\nSome recent works also propose alternate techniques for crowd counting such as deep q-learning (DQN) [98], [99]. However, such contributions have limited impact in terms of performance improvement.\n\n\nSummary and Lessons Learned\n\n\u2022 Single-column models are compact but achieves lower accuracy over dense images. They also suffer from scale variations in images. \u2022 Scale variations can be addressed using multi-column models. However, the capability of a multi-column model to adopt to various object scales is limited by the number of columns. Multi-scale models are also computationally expensive due to several columns training in parallel. \u2022 To address scale variations in a computationally efficient way and to adopt to large scale variations, single column models with multi-scale modules are proposed. These works are originally inspired from Inception [25] model with small differences in design. \u2022 Encoder-decoder models preserve spatial resolution when high quality density maps are desired. They also offer easy implementation of supervision at different stages of the network (e.g., TEDnet [55]). \u2022 Both single-column and multi-column models trained from scratch have limited accuracy when tested on large datasets with highly dense images. Hence, pre-trained deep models such as VGG [24], Inception [25], and ResNets [73] have been used to improve counting accuracy. Models with pre-trained backbone CNNs (frontends) also train faster. However, the model size and execution time increases which make such models not a good choice for real-time performance.\n\n\nVI. NETWORK TRAINING AND EVALUATIONS\n\nThe standard process for supervised learning using neural network comprised of two stages: network training, and evaluation. In training stage, the network takes images and ground truths (total head count or density maps), compute gradients and back propagate the loss. The loss function is the objective function that the network is minimizing. After each one or every few epochs, the model is evaluated using a metric function. In the case of crowd counting, both the loss and metric functions compare the predicted density map with the ground truth (i.e., target) density map and compute the difference. The target and predicted density maps can be generally compared in three possible ways; (i) image-wise i.e., compare density maps as a whole, (ii) patch-wise i.e., divide the target and predicted maps into non-overlapping patches and compare all patches that belong to the same location, and (iii) pixel-wise i.e., compare each individual pixels (at the same location) of the target and predicted maps and integrate over the image.\n\n\nA. Supervised and Weakly-supervised Learning\n\nThe state-of-the-art methods for crowd density estimation use point-level supervision (fully supervised). Although, fully supervised learning produces more reliable and accurate results, they require expensive annotations.\n\nA small number of research works [70], [102], [96], [103] also propose the use of weakly supervised learning which use more economical total head count annotations. The Multiple   Auxiliary Tasks Training (MATT) model [70] uses dot annotations (full supervision) for a small number of samples and count annotations (weak supervision) for the remaining data. According to the [70], the count-level supervision although may achieve low counting errors, it does not encode precise object location. To overcome this, multiple auxiliary branches were used to produce density maps. The losses for these auxiliary branches are computed and integrated. The final loss function is the sum of pixel-wise l 2 loss (primary branch), count loss (primary branch) and the integrated auxiliary loss (auxiliary branches).\n\nRecently transformer models are proposed to implement weakly supervised learning due to their self-attention mechanism to automatically learn the semantic information in crowd images. The TransCrowd model [96] is based on the ViT [84] which uses only count level supervision to predict the count.\n\n\nB. Data Augmentation\n\nData augmentation refers to the technique used to increase the amount of data and introduce more diversity in the data by adding slightly modified copies of original data. When the input data is images, different augmentation techniques are applied to transform original images such as random flipping, croping, resizing, random perspective, rotation, random noise, brightness, contrast etc. However, the augmentation techniques should be applied carefully depending on the problem. In crowd counting research, the most commonly applied augmentation techniques include random cropping [29], [63], [18], [19], [66], [79], [69] and horizontal flipping [19], [66], [79], [69]. In general, cropping helps to improve learning by addressing the scale variations whereas horizontal flipping address the perspective distortion in images [63]. Some lessfrequently used transformations include random noise [66], vertical flipping [66], and image rotation [82]. Image augmentation also acts as a regularize for the network i.e., helps to avoid overfit the model to the training data [82].\n\n\nC. Loss functions\n\nA loss function is a function that measure the difference between the ground truth (GT) and the prediction. In density estimation, both the GT and prediction are density maps, whereas in count regression, both are numbers (integers).\n\n1) Euclidean (l 2 ) loss:: Euclidean loss or l 2 loss is the most common loss function used in crowd counting research. The pixel-wise l 2 loss or mean squared error(L D (\u0398)) is calculated as follows:\nL D (\u0398) = 1 N N i=1 ||F (X i ; \u0398) \u2212 D i || 2(2)\nwhere \u0398 represents the CNN parameters (i.e., wights), N is the total number of samples, F (X i ; \u0398) is the predicted density map, and D i is the ground truth density map. The l 2 loss can also be calculated over total counts instead of pixel-wise values. If\nL C (\u0398) = 1 N N i ||\u0108 i \u2212 C i || 2(3)\nwhere\u0108 i and C i are the predicted and true counts, which are obtained by summing the pixel values over entire GT density map and predicted density map respectively.\n\n2) l 1 loss: This is the Manhattan distance (l 1 norm) among corresponding pixels in the target and predicted density maps.\nL D (\u0398) = 1 N N i=1 ||F (X i ; \u0398) \u2212 D i ||(4)\nSimilar to the case of l 2 loss, l 1 loss can also be computed over total count instead of individual pixels as follows:\nL C (\u0398) = 1 N N i ||\u0108 i \u2212 C i || (5)\n3) Composition Loss: Composition loss is proposed in [46]. Two density maps and one localization map is generated at different depths in the CNN network, whereas the final output is the head count in the image. For the density and localization maps, the pixel-wise l 2 loss is computed, whereas the regression l 2 loss between the predicted and actual counts is computed.\n\n\n4) Spatial Correlation Loss: Spatial correlation loss (SCL)\n\nis the cross-correlation among the normalized target and predicted density maps and is calculated as:\nL SC = 1 \u2212 N i=1 M j=1 D ij .D ij N i=1D 2 ij . N i=1 D 2 ij (6)\nwhere D ij andD ij are the target and predicted density maps, respectively. i and j are the row and column indices, and N , and M represents the total number of rows and columns in the target and predicted density maps, respectively. SCL has been used in TEDnet [55] in combination with l 2 loss. SCL is less-sensitive to changes in intensity levels in the density map.\n\n\n5) APLoss:\n\nThe commonly used l 2 loss takes the target and predicted density maps as a whole to compute the pixelwise euclidean l 2 distance among them. The standard l 2 loss does not consider the various density levels in the image and lead to generalization errors. To cope with this shortcoming of l 2 loss, adaptive pyramid loss (APLoss) is proposed in [22]. To compute APLoss, the target density map is first divided into four non-overlapping patches. If the count in each patch is greater than a threshold T , the patch is further divided into four patches. This process is repeated until no patch is left with count greater than the threshold T . When this patching process is completed, the predicted density map is also divided into patches in the same pattern (the no. of patches should equal for both target and predicted density maps). Then the APLoss is calculated as follows:\nAP Loss = 1 N N i=1 4 j1=1 l i Rj 1(7)\nwhere N is the number of density maps, and l i Rj 1 represents the patch-wise loss.  [77] is a natural extension of the APLoss with two improvements. The intuition behind PRA Loss is that in each crowd image, some regions and hence some pixels in those regions are harder for the model to learn from. These regions and pixels are typically overestimated which contribute to more counting errors. If these harder regions and more specifically the harder pixels in these regions are determined and assigned higher weights in the loss function, the network would be able to learn these pixels more consistently. PRA Loss solves this problem by extending the APLoss function with few improvements. Similar to APLoss, the predicted density map is divided into four patches and the patch with highest counting error (overestimated patch) is selected. The patching process is repeated until the pixels with the highest errors are determined. PRA Loss is calculated as:\nl P RA = ||D est p\u2208G \u2212 D GT p\u2208G || 2 + \u03bb||D est p\u2208H \u2212 D GT p\u2208G || 2(8)\nwhere p is the pixel in the predicted density map D est , G represents set of all pixels in D est , and H is the set of all hard pixels in D est , and \u03bb is the weight term for the hard pixels.\n\n\n7) Curriculum Loss:\n\nIn machine learning, curriculum learning is a trained strategy for neural networks proposed by [104], in which the model is fed with training samples in the order of increasing complexity. Curriculum learning was used in crowd counting by [] such that each image is assigned a difficulty score. Inspired by the work in [105], a curriculum loss function is proposed in [23]. To design the curriculum loss, the difficulty level of all pixels in the density map are calculated. Following the intuition that dense crowd regions are harder to estimate, the pixels with higher values than a dynamic threshold are considered as difficult. To compute the difficulty level and the final loss function, a weight matrix W of the same size as the density map is defined.\n\n\n8) Combination\n\nLoss: Several works propose a combination of two or more loss functions to improve learning. For instance, [29] proposes a combination of pixel-wise and count l 2 loss as follow:\nL(\u0398) = L D (\u0398) + \u03bbL Y (\u0398)(9)\nwhere, L D (\u0398) is the pixel wise loss as calculated using Eq. 2, and \u03bbL Y (\u0398) is the l 2 loss over total count. The term \u03bb is the weight term.\n\n\n9) Optimal Transport (OT) Loss:\n\nThe optimal transport (OT) loss is first introduced in [106]. To compute the OT loss, both the target and predicted density maps (D and(D)) are first converted into their respective probability density functions (pdfs) by dividing each pdf by its total mass. The OT loss is computed as follow:\nl OT = W D D 1 ,D D 1(10)\n10) Total Variation (TV) Loss: Authors in [106] derived total variation (TV) loss due to the inherited time complexity for computing OT loss. It is defined as:\nl T V = 1 2 D D 1 ,D D 1 1(11)\nD. Evaluation Metrics 1) Mean Absolute Error (MAE): It is the l 1 or Manhattan distance between the actual and predicted count in an image and is calculated as follows:\nM AE = 1 N N i=1 (C pred i \u2212\u0108 gt i )(12)\nMAE is the most commonly used metric in all crowd counting works. A limitation of MAE is that it is more robust to outliers (i.e., large counting errors). Thus mean squared error is used as an additional metric which is more sensitive to outliers.\n\n2) Mean Squared Error (MSE):\nM SE = 1 N N i=1 (C pred i \u2212\u0108 gt i ) 2(13)\nwhere, N is the total number of images in the dataset, C gt i is the ground truth (actual count) and C pred i is the prediction (estimated count) in the i th image. MSE is the second most used metric after MAE in crowd counting research.\n\n3) Grid Average Mean Error (GAME): While MAE is the most widely used metric in crowd counting research and is often used to compare various models, MAE provide imagewide counting and does not provide where the estimations have been done in the image. Owing to the possible estimation errors in MAE, authors in [107] proposed Grid Average Mean absolute Error (GAME). In GAME, an image is divided into 4 L non-overlapping patches and compute MAE separately for each patch. Thus, GAME poses a more robust and accurate estimation for crowd counting applications.\nGAM E = 1 N N i=1 ( 4 L l=1 |C pred (n,l) \u2212 C gt (n,l) |)(14)\nThe GAME metric is more robust to localisation errors in density estimation. It can be easily observed in Fig. 10. \nP M AE = 1 m \u00d7 N m\u00d7N i=1 |C pred Ii \u2212 C gt Ii |(15)\nwhere m is the number of non-overlapping patches in each image. When m = 1, PMAE become equivalent to MAE. PMAE is effectively equivalent to GAME for m = 4 L .\n\n\n5) Patch Mean Squared Error (PMSE):\nP M SE = 1 m \u00d7 N m\u00d7N i=1 (C pred Ii \u2212 C gt Ii ) 2(16)\nwhere m is the number of non-overlapping patches in each image. When m = 1, PMSE become equivalent to MSE.\n\n\n6) Mean Pixel Level Absolute Error (MPAE):\n\nThis is equivalent to pixel-wise l 1 error and is calculated using Eq. 4.\n\nIn addition to metrics for counting accuracy, some works [19], [55] used additional metrics to measure the quality of predicted density maps. These are listed as follows: 7) Peak Signal to Noise Ratio (PSNR): PSNR measures the error between the corresponding pixels in the target and predicted density maps. The value of PSNR is always in the range of [0-1]. Higher value of PSNR means lower error and vice versa. PSNR metric is used in [19], [55].\n\n\n8) Structural\n\nSimilarity Index (SSIM): SSIM measures the difference between the target and predicted density maps over three attributes i.e., brightness, contrast, and structure. Similar to PSNR, the value of SSIM is also in the range of [0-1] and the higher value is preferred over lower values. SSIM metric is used in [19], [55].\n\nA summary of different learning methods and evaluation criteria is presented in Table IV. We further compared the performance of these models over the commonly used metric i.e., MAE in Fig. 9 over benchmark datasets.\n\n\nSummary and Lessons Learned\n\n\u2022 The pixel-wise euclidean or l 2 loss (in Eq. 2) is the most commonly used loss function used almost in all crowd counting models. It is typically used alone or in  combination with other loss functions to improve learning performance and convergence. \u2022 The euclidean loss can not detect the localization errors.\n\nHence, more complex loss functions are proposed such as APLoss which attentively divide target density maps into patches or sub-regions and compute loss loss for each patch and finally sum these losses to calculate loss for the whole density map. \u2022 Almost all crowd counting models use l 2 distance among respective pixels in the target and predicted density maps. There is an underlying assumption that the individual pixel values are independent of their neighbors (noncorrelated). Recently, authors in [108] show that how human annotation errors can cause noise in density maps generation and that such noise can be modeled as a multivariate Gaussian approximation. \u2022 Despite the various loss functions proposed over time, the pixel-wise l 2 loss is the widely used function to compute loss in crowd density estimation either directly or integrated in other complex loss functions such as APLoss, PRA Loss etc.\n\n\nVII. CONCLUSION AND FUTURE RESEARCH DIRECTIONS\n\nThe paper presents a comprehensive review of the most significant contributions in the area of crowd counting using computer vision and deep learning. Based on our review of the literature, we mark few interesting observations listed as follows. In addition, we present insights on each observation for prospective readers.\n\n\u2022 State-of-the-art: in brief? Despite very deep models and complex architectures, the accuracy gains over dense and large datasets is reasonably low (e.g., ShanghaiTech Part A [18], UCF-QNRF [46], JHU-Crowd [47] etc.). A large amount of research works contribute incrementally very low accuracy gains over these datasets. In very dense crowd counting, there is still sufficient room for accuracy gains which incites further investigation. Two recent research directions in this area are improved loss functions and novel architecture such as vision transformer-based models. \u2022 Deep or shallow models: how to choose? The attempts to gain higher accuracy over large datasets generally lead to deeper and more complex architectures, but on sparse datasets with low crowd density images (e.g., UCSD [44], Mall [43], and ShanghaiTech Part B [18]), shallow models provide reasonably sufficient accuracy and deeper models may not be required in scenarios captured in these datasets. Noteably, single-scene crowd counting is the simplest task, followed by sparse multi-scene crowd detection. \u2022 Need for benchmarking: Research efforts are mostly contributed towards achieving accuracy gains (even if very small gains), and the resulting model complexity is often ignored. This results in increasing model complexity for small and often negligible improvement in accuracy. We believe that benchmarking over model training and inference time of crowd models need to be investigated. \u2022 Design patterns in crowd counting models: While the initial crowd models followed a single column simple architecture, followed by other types such as multi-column, encoder-decoder etc, the most recent architectures are increasingly complex with no clear design patterns. Several works proposing novel architecture also adopted improved training method (e.g., new loss functions). The effective gain by the architecture and training method alone were not thoroughly investigated. Furthermore, the increasing complexity of crowd counting models makes it hard to understand the pattern which contributed in the accuracy and can preclude innovation and improvements in such models. Based on our review and the aforementioned observations, we provide few useful insights for prospective researchers. \u2022 Cross-scene counting and domain adaptation: Most of the crowd counting models are trained and tested on the same benchmark datasets. In general the research on model generalization is still very limited. Few studies investigate models model generalization i.e., model pretrained on one dataset and then fine-tuned on other dataset. However, the results typically reported belongs to the fine-tuned model. This creates a huge gap on crossscene model generalization of crowd counting models which needs further investigation. It would be interesting to see model generalization in a range of diverse scenarios e.g., CCTV images, drone images, indoor and outdoor scenarios. Furthermore, domain adaptations is another exciting direction in crowd counting research. \u2022 Can we have a universal crowd counting model? The potential applications of crowd counting will require the model to operate at various hardware platforms (e.g., servers, drones, cameras, mobile phones etc.) with varying computing capabilities. Also, the performance requirement vary by applications (e.g., real-time, non realtime), surveillance type (e.g., CCTV-based surveillance, drone-based surveillance) and scenarios (e.g., shopping malls, metro stations, stadiums etc.). Thus, developing a single best model with top accuracy in all applications, using any surveillance method, and in scenarios is not an effective solution. In fact, considering the existing trends in crowd counting model design, such a model will have large size, require huge compute resource for fine tuning, and incur longer delays during inference. Such a model will not be suitable for applications with limited on-chip memory (in edge devices), battery-powered devices, and real-time inference requirement. Thus we envision and also witnessed some recent efforts to have application specific model designing i.e., lightweight models for real-time applications on resource-constrained devices, whereas dense models for optimum accuracy over dense crowds in server-based platforms. \u2022 CNNs versus transformers? Although transformers have shown breakthrough performance in applications like NLP and other sequence-to-sequence modelling, their performance in computer vision is not comparably the same as in the other domains. Specifically, in crowd counting tasks, transformers so far have limited accuracy improvements over large benchmark datasets. Moreover, transformer models for crowd counting available to date are relatively deep models. This is because of the use of pretrained ViT which has 86M parameters (base model with 12 layers) as compared to CNN-based models (e.g., CSRNet has 16M parameters). We believe transformer models yet have to bring substantial accuracy gains and efficiency improvement to complement CNN models.\n\n\nACKNOWLEDGEMENT\n\nThis publication was made possible by the PDRA award PDRA7-0606-21012 from the Qatar National Research Fund (a member of The Qatar Foundation). The statements made herein are solely the responsibility of the authors.\n\n\nScale variation in same image. (d) Scale variation in different images of the same scene.(e) Variations in crowd density (low vs high density).(f) Uniform vs non-uniform crowd distribution.(g) Occlusions in crowd images.(h) Complex backgrounds.\n\nFig. 1 :\n1Challenges in crowd counting using computer vision.\n\n\nTotal count annotation: The total number of objects in the image is counted as a single number. This kind of annotation is used in detection-based and regressionbased counting approaches.2) Dot annotation: The objects inside images are annotated as a single dot over each object, typically on the person's head. The dot annotation creates a dot-map (also called as localization map) i.e., a matrix consisting of all zeros except those positions containing heads in the image.All pixels corresponding to head positions are activated i.e., set to 1 per head. Dot annotation is used in density estimation methods for crowd counting. However, as the sum of dot map is equal to the total count in an image,\n\nFig. 2 :\n2Ground Truth generation for crowd counting models.\n\nFig. 3 :\n3Density maps with different scale (\u03c3) values. region of sparse density in crowd scene. Typical settings includes k = 10 [20], k = 10 [54].\n\nFig. 5 :\n5Inception[25] like scale aggregation modules proposed in SANet[20] \n\nFig. 6 :Fig. 7 :\n67Crowd counting models: Common CNN design patterns followed in crowd counting and density estimation models.Pixel-wise loss and metricPatch-wise loss and metricImage-wise loss and metricGround Truth Prediction(Flipped for plotting purpose) Different levels of comparing predicted density maps with the target density maps.\n\n6 )\n6PRA Loss: The APLoss function introduced in [22] aim at reducing estimation errors in the more dense regions of the image. The Pyramid Region Loss (PRA Loss)\n\nFig. 8 :\n8An illustration of pixel values in density maps in different crowd density regions. The localization map shows location of heads (i.e., one head at the center, three heads at large distance in the top-left corner, and three heads closer to each other at top-right corner). The density map shows the corresponding pixel values i.e., higher values in the dense region (top-right).\n\n\n) UCF-CC-50[45], ShanghaiTech Part A[18] and UCF-QNRF[46] datasets.\n\nFig. 9 :Fig. 10 :\n910Performance analysis of crowd counting models over benchmark datasets. An illustration of robustness of GAME metric to localization errors. For a single prediction of a given ground truth, MAE is calculated as zero (ignoring localization error in prediction), whereas GAME=1 (capturing localization error).\n\n\n. A. Khan and H. Menouar are with Qatar Mobility Innovations Center (QMIC), Doha, Qatar.R. Hamila is with Qatar University.\n\nTABLE I :\nICrowd counting datasets.Dataset \nAttribute \nScene \nImages \nResolution \n\nUCSD [44] \nCCTV \nSingle \n2000 \n238 \u00d7 158 \nMall [43] \nCCTV \nSingle \n2000 \n640 \u00d7 480 \nUCF CC 50 [45] \nFree \nSingle \n50 \nVary \nWorldExpo'10 [29] Video frames Cross-scene \n3920 \n576 \u00d7 720 \nShanghaiTech-A [18] \nInternet \nSingle \n482 \nVary \nShanghaiTech-B [18] \nCCTV \nSingle \n716 \n768 \u00d7 1024 \nUCF-QNRF [46] \nFree-view \nSingle \n1535 \nVary \nJHU-Crowd [47] \nInternet \nCross-scene \n4372 \n1450 \u00d7 900 \nDroneRGBT [48] \nDrone \nCross-scene \n3600 \n512 \u00d7 640 \nNWPU Crowd [49] \nInternet \nCross-scene \n5109 \nVary \nTRANCOS [50] \nCCTV \nSingle \n1244 \n640 \u00d7 480 \nCARPK [51] \nDrone \nCross-scene \n1448 \n1280 \u00d7 720 \nVisDrone-People [52] \nDrone \nCross-scene \n3347 \nVary \nVisDrone-Vehicles [52] \nDrone \nCross-scene \n5303 \nVary \nRGBT-CC [53] \nDrone \nCross-scene \n-\n-\n\nIV. DATA LABELLING \n\nCrowd monitoring applications mostly use supervised learn-\ning or semi-supervised learning and thus require labelled data \n(images or videos). There are mainly three types of data \nannotations used for crowd images. \n\n\n\nTABLE II :\nIIGaussian Kernels used in various studies.Model Dataset \nKernel \nScale (\u03c3) \n\nCrowdCNN [29] -\nAdaptive \n-\nMCNN [18] -\nAdaptive \n-\nMSCNN [54] -\nAdaptive \n-\n\nCSRNet [19] \n\nShanghaiTech Part B \nFixed \n15 \nTRANCOS \nFixed \n10 \nUCSD, WorldExpo'10 \nFixed \n3 \nSANet [20] -\nAdaptive \n-\nTEDnet [55] -\nFixed \n-\n\nMobileCount [56] ShanghaiTech Part B, UCF-QNRF, \nUCF-CC-50, WorldExpo'10 \n\nFixed \n4 \n\nDeepcount [57] -\nFixed \n5 \nBL [58] ShanghaiTechB \nFixed \n15 \nC-CNN [59] ShanghaiTech Part A & B \nFixed \n15 \nASNet [22] -\nFixed \n-\n\nof 18 \u00d7 18 size. The model is evaluated over UCSD and \nUCF CC 50 datasets using mean absolute error (MAE) as \nevaluation metric. Early CNN models for crowd counting \nrequire perspective information during training as well as for \ninference, which may not be available in practical scenarios. \n\n\nTABLE III :\nIIIA list of crowd counting models arranged in chronological order.Datasets \n\nModel Year \nArchitecture \nBackbone \nMall [43] \nUCSD [44] \nWorldExpo'10 [29] \nUCF-CC-50 [45] \nShanghaiTech [18] \nUCF-QNRF [46] \nSmartCity [61] \nDroneRGBT [61] \n\nCARPK [61] \nTRANCOS [50] \nVisDrone [61] \nRGBT-CC [53] \nJHU-Crowd [47] \nNWPU-Crowd [49] \n\nCrowdCNN [29] 2015 \nSC \nNo \n\n\n\n\n\n\n\n\n\n\n\n\n\nMCNN [18] 2016 \nMC \nNo \n\n\n\n\n\n\n\n\n\n\n\nCrowdNet [63] 2016 \nMC \nVGG16 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMSCNN [54] 2017 \nSC-MSM \nNo \n\n\n\n\n\n\n\n\n\n\n\n\n\nSCNN [64] 2017 \nMC \nNo \n\n\n\n\n\n\n\n\n\n\n\n\n\nCMTL [65] 2017 \nMC \nNo \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCSRNet [19] 2018 \nSC \nVGG16 \n\n\n\n\n\n\n\n\n\n\n\nSANet [20] 2018 ED/SC-MSM \nNo \n\n\n\n\n\n\n\n\n\n\n\nSaCNN [61] 2018 \nSC \nNo \n\n\n\n\n\n\n\n\n\n\n\n\nDecideNet [66] 2018 \nMC \nNo \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTEDnet [55] 2019 \nED \nNo \n\n\n\n\n\n\n\n\n\n\n\nCANNet [81] 2019 \nComplex \nVGG16 \n\n\n\n\n\n\n\n\n\n\n\nGSP [100] 2019 \nSC \nVGG16 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMobileCount [56] 2019 \nED \nMobileNetV2 \n\n\n\n\n\n\n\n\n\n\n\nANF [76] 2019 \nED \nNo \n\n\n\n\n\n\n\n\n\n\n\nBL [58] 2019 \nSC \nVGG19 \n\n\n\n\n\n\n\n\n\n\n\nPACNN [68] 2019 \nMC \nVGG16 \n\n\n\n\n\n\n\n\n\n\n\nPGCNet [21] 2019 \nComplex \nCSRNet \n\n\n\n\n\n\n\n\n\n\n\n\n\nC-CNN [59] 2020 \nSC \nVGG \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeepCount [57] 2020 \nMC \nCSRNet \n\n\n\n\n\n\n\n\n\n\n\n\n\nASNet [22] 2020 \nMC \nVGG16 \n\n\n\n\n\n\n\n\n\n\n\nMMCNN [82] 2020 \nComplex \nResNet-50 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLibraNet [62] 2020 \nSC \nVGG16 \n\n\n\n\n\n\n\n\n\n\n\n\nMATT [70] 2020 \nMC \nCSRNet \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYang et al. [71] 2020 \nMC \nVGG16 \n\n\n\n\n\n\n\n\n\n\n\n\n\nPCCNet [69] 2020 \nMC \nNo \n\n\n\n\n\n\n\n\n\n\n\n\nSASNet [77] 2021 \nED \nVGG16 \n\n\n\n\n\n\n\n\n\n\n\nM-SFANet [79] 2021 \nSC-MSM \nVGG16 \n\n\n\n\n\n\n\n\n\n\n\nM-SegNet [79] 2021 \nSC-MSM \nVGG16 \n\n\n\n\n\n\n\n\n\n\n\n\nSGANet [23] 2022 \nSC-MSM \nInception-v3 \n\n\n\n\n\n\n\n\n\n\n\n\nFusionCount [60] 2022 \nSC \nVGG16 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMAN [101] 2022 \nMC \nVGG19 \n\n\n\n\n\n\n\n\n\n\n\n\n\nMFCC [78] 2022 \nED \nResNet \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTAFNet [72] 2022 \nMC \nVGG16 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTransCrowd [96] 2022 \nMC \nNon-CNN \n\n\n\n\n\n\n\n\n\n\n\nConv \nlayer \n\nConv \nlayer \n\nConv \nlayer \n\nInput \nOutput \n\nConv \nlayer \n\n(a) Single-column models depict convolution layers cascaded in \na single column. \n\nConv \nlayer \n\nConv \nlayer \n\nConv \nlayer \n\nOutput \n\nConv \nlayer \n\nConv \nlayer \n\nConv \nlayer \n\nConv \nlayer \n\nConv \nlayer \n\nConv \nlayer \n\nConv \nlayer \n\n(b) Multi-column models consist of two or more CNN layers \nwhere the output of all columns are fused together to produce \na single output. \n\nInput \nOutput \n\nEncoder \nDecoder \n\n(c) Encoder-decoder models comprised of two parts, where the \nfirst part (encoder module) of the model works as a feature \nextractor whereas the second part (decoder module) of the \nnetwork decodes the features into the required output shape. \n\nConv \nlayer \n\nPretrained \nCNN \nFrontend \n\nConv \nlayer \n\nInput \nOutput \n\n(d) Models which consist of a relatively smaller CNN network \nwith a pre-trained backbone or front-end network (large sized). \n\nConv \nlayer \n\nInput \n\nConv \nlayer \n\nmulti-scale \nmodule \n\nConv \nlayer \n\nOutput \n\nmulti-scale \nmodule \n\n(e) A single column model with multiple modules which are \nusually multi-column, specifically designed to encode scale-\naware feature in crowd images. \n\n\n\nTABLE IV :\nIVComparison of crowd counting models: model architecture, inputs and outputsModel \nInput \nInput size \nOutput size \nloss \nMetric \n\nCrowdCNN [29] \npatch \n72 \u00d7 72 \n18 \u00d7 18 \ncombination l2 loss \nMSE, MAE \nMCNN [18] \nimage \narbitrary \ninput size/4 \nl2 loss \nMSE, MAE \nCrowdNet [63] \npatch \n225 \u00d7 225 \ninput size/8 \nl2 loss \nMSE, MAE \nMSCNN [54] \npatch \n225 \u00d7 225 \n6 \u00d7 6 \nl2 loss \nMSE, MAE \nSCNN [64] \npatch \nimage size/3 \n-\nl2 loss \nMSE, MAE \nCMTL [65] \npatch \nimage size/4 \ninput size \nl2 loss, cross-entropy \nMSE, MAE \nCSRNet [19] \npatch \nimage size/8 \ninput size/8 \nl2 loss \nMSE, MAE, PSNR, SSIM \nSANet [20] \npatch \nimage size/4 \ninput size \ncomposite (l2 + l SSIM ) loss \nMSE, MAE \nSaCNN [61] \npatch \nimage size/8 \nl2 loss \nMSE, MAE \nDecideNet [66] \npatch \n4x3 \n-\ncomposite (l1 + l2) loss \nMSE, MAE \nTEDnet [55] \nimage \narbitrary \n-\nSAL, SCL \nMSE, MAE, PSIM, SSIM \nCANNet [81] \npatch \n224 \u00d7 224 \n-\nl2 loss \nRMSE, MAE \nGSP [100] \npatch \nmultiple \n-\nl2 loss \nRMSE, MAE \nMobileCount [56] \npatch \n(image size x 0.8) \n-\nl2 loss \nMSE, MAE \nANF [76] \nimage \narbitrary \n-\nl2 loss \nMSE, MAE \nBL [58] patches \n256 \u00d7 256 \n-\nBL (l Bayes ) \nMSE, MAE \nPACNN [68] \npatch \nimage size/4 \n-\ncomposite (l2 + l DSSIM ) loss, \nMSE, MAE \nPGCNet [21] \nimage \narbitrary \n-\nl2 loss \nMSE, MAE \nCCNN [59] \nimage \narbitrary \nl2 loss \nMSE, MAE \nDeepCount [57] \npatch \n384 \u00d7 512 \nmultiple \nweighted l1 loss \nRMSE, MAE \nASNet [22] \npatch \n128 \u00d7 128 \n-\nAPA loss \nMSE, MAE \nMMCNN [82] \nimage \n-\n-\ncomposite loss \nRMSE, MAE \nLibraNet [62] \npatch \nimage size/2 \n-\nl1 loss \nMSE, MAE \nMATT [70] \nimage \narbitrary \n-\ncomposite (l1 + l2) loss \nMSE, MAE, RER \nYang et al. [71] \n-\n-\n-\ncomposite (l2 + cross-entropy) loss \nMSE, MAE \nPCCNet [69] patches \n512 \u00d7 680 \n-\nl2 loss \nMSE, MAE \nSASNet [77] \npatch \n128 \u00d7 128 \n-\ncomposite (l2 + l SSIM ) loss \nMSE, MAE \nM-SFANet [79] \nimage \narbitrary \nimage size/2 \nBL (l Bayes ) \nRMSE, MAE, GAME \nM-SegNet [79] \nimage \narbitrary \nimage size/2 \nBL (l Bayes ) \nRMSE, MAE, GAME \nSGANet [23] \npatch \n128 \u00d7 128 \ninput size/4 \ncurriculum loss \nRMSE, MAE \nFusionCount [60] \npatch \n384 \u00d7 512 \n-\nl2 loss \nRMSE, MAE \nMAN [101] \npatch \n512 \u00d7 512 \n-\ncomposite loss \nMSE, MAE \nMFCC [78] \nimage \n512 \u00d7 640 \ninput size \nl2 loss \nRMSE, MAE \nTAFNet [72] \nimage \narbitrary \n-\nl2 loss \nRMSE, GAME \nTransCrowd [96] \nimage \n1152 \u00d7 768 \n-\nl1 loss \nMSE, MAE \n\n4) Patch Mean Absolute Error (PMAE): \n\n\n\nA deep-cnn crowd counting model for enforcing social distancing during covid19 pandemic: Application to saudi arabia's public places. M S A Salma Kammoun Jarraya, Maha Hamdan Alotibi, Computers, Materials & Continua. 662M. S. A. Salma Kammoun Jarraya, Maha Hamdan Alotibi, \"A deep-cnn crowd counting model for enforcing social distancing during covid19 pandemic: Application to saudi arabia's public places,\" Computers, Materials & Continua, vol. 66, no. 2, pp. 1315-1328, 2021.\n\nA social distance estimation and crowd monitoring system for surveillance cameras. M Al-Sa&apos;d, S Kiranyaz, I Ahmad, C Sundell, M Vakkuri, M Gabbouj, Sensors. 222M. Al-Sa'd, S. Kiranyaz, I. Ahmad, C. Sundell, M. Vakkuri, and M. Gabbouj, \"A social distance estimation and crowd monitoring system for surveillance cameras,\" Sensors, vol. 22, no. 2, 2022.\n\nShape-based human detection and segmentation via hierarchical part-template matching. Z Lin, L S Davis, IEEE Transactions on Pattern Analysis and Machine Intelligence. 324Z. Lin and L. S. Davis, \"Shape-based human detection and segmen- tation via hierarchical part-template matching,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 32, no. 4, pp. 604-618, 2010.\n\nPedestrian detection via classification on riemannian manifolds. O Tuzel, F Porikli, P Meer, IEEE Transactions on Pattern Analysis and Machine Intelligence. 3010O. Tuzel, F. Porikli, and P. Meer, \"Pedestrian detection via classification on riemannian manifolds,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 30, no. 10, pp. 1713-1727, 2008.\n\nPedestrian detection in crowded scenes. B Leibe, E Seemann, B Schiele, 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05). 1B. Leibe, E. Seemann, and B. Schiele, \"Pedestrian detection in crowded scenes,\" in 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05), vol. 1, pp. 878-885 vol. 1, 2005.\n\nRobust real-time face detection. P Viola, M Jones, Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001. Eighth IEEE International Conference on Computer Vision. ICCV 20012P. Viola and M. Jones, \"Robust real-time face detection,\" in Proceed- ings Eighth IEEE International Conference on Computer Vision. ICCV 2001, vol. 2, pp. 747-747, 2001.\n\nEstimation of number of people in crowded scenes using perspective transformation. S.-F Lin, J.-Y. Chen, H.-X Chao, IEEE Trans. Syst. Man Cybern. Part A. 31S.-F. Lin, J.-Y. Chen, and H.-X. Chao, \"Estimation of number of people in crowded scenes using perspective transformation,\" IEEE Trans. Syst. Man Cybern. Part A, vol. 31, pp. 645-654, 2001.\n\nEstimating the number of people in crowded scenes by mid based foreground segmentation and head-shoulder detection. M Li, Z Zhang, K Huang, T Tan, International Conference on Pattern Recognition. M. Li, Z. Zhang, K. Huang, and T. Tan, \"Estimating the number of people in crowded scenes by mid based foreground segmentation and head-shoulder detection,\" in 2008 19th International Conference on Pattern Recognition, pp. 1-4, 2008.\n\nObject detection with discriminatively trained part-based models. P F Felzenszwalb, R B Girshick, D Mcallester, D Ramanan, IEEE Transactions on Pattern Analysis and Machine Intelligence. 329P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan, \"Object detection with discriminatively trained part-based models,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 32, no. 9, pp. 1627-1645, 2010.\n\nCounting people by clustering person detector outputs. I S Topkaya, H Erdogan, F Porikli, 2014 11th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS). I. S. Topkaya, H. Erdogan, and F. Porikli, \"Counting people by clustering person detector outputs,\" in 2014 11th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS), pp. 313-318, 2014.\n\nDetection and tracking of multiple, partially occluded humans by bayesian combination of edgelet based part detectors. B Wu, R Nevatia, International Journal of Computer Vision. 75B. Wu and R. Nevatia, \"Detection and tracking of multiple, partially occluded humans by bayesian combination of edgelet based part detectors,\" International Journal of Computer Vision, vol. 75, pp. 247- 266, 2006.\n\nFeature mining for localised crowd counting. K Chen, C C Loy, S Gong, T Xiang, BMVC. K. Chen, C. C. Loy, S. Gong, and T. Xiang, \"Feature mining for localised crowd counting,\" in BMVC, 2012.\n\nCrowd monitoring using image processing. A C Davies, J H Yin, S A Velast\u00edn, Electronics & Communication Engineering Journal. 7A. C. Davies, J. H. Yin, and S. A. Velast\u00edn, \"Crowd monitoring using image processing,\" Electronics & Communication Engineering Journal, vol. 7, pp. 37-47, 1995.\n\nHistograms of oriented gradients for human detection. N Dalal, B Triggs, 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05). 1N. Dalal and B. Triggs, \"Histograms of oriented gradients for human detection,\" in 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05), vol. 1, pp. 886-893 vol. 1, 2005.\n\nBayesian poisson regression for crowd counting. A B Chan, N Vasconcelos, 2009 IEEE 12th International Conference on Computer Vision. A. B. Chan and N. Vasconcelos, \"Bayesian poisson regression for crowd counting,\" in 2009 IEEE 12th International Conference on Computer Vision, pp. 545-551, 2009.\n\nA mrf-based approach for real-time subway monitoring. N Paragios, V Ramesh, Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. I-Ithe 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition1N. Paragios and V. Ramesh, \"A mrf-based approach for real-time subway monitoring,\" in Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001, vol. 1, pp. I-I, 2001.\n\nLatent gaussian mixture regression for human pose estimation. Y Tian, L Sigal, H Badino, F D La Torre, Y Liu, ACCV. Y. Tian, L. Sigal, H. Badino, F. D. la Torre, and Y. Liu, \"Latent gaussian mixture regression for human pose estimation,\" in ACCV, 2010.\n\nSingle-image crowd counting via multi-column convolutional neural network. Y Zhang, D Zhou, S Chen, S Gao, Y Ma, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Y. Zhang, D. Zhou, S. Chen, S. Gao, and Y. Ma, \"Single-image crowd counting via multi-column convolutional neural network,\" in 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 589-597, 2016.\n\nCsrnet: Dilated convolutional neural networks for understanding the highly congested scenes. Y Li, X Zhang, D Chen, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Y. Li, X. Zhang, and D. Chen, \"Csrnet: Dilated convolutional neu- ral networks for understanding the highly congested scenes,\" 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1091-1100, 2018.\n\nScale aggregation network for accurate and efficient crowd counting. X Cao, Z Wang, Y Zhao, F Su, ECCV. X. Cao, Z. Wang, Y. Zhao, and F. Su, \"Scale aggregation network for accurate and efficient crowd counting,\" in ECCV, 2018.\n\nPerspective-guided convolution networks for crowd counting. Z Yan, Y Yuan, W Zuo, X Tan, Y Wang, S Wen, E Ding, 2019 IEEE/CVF International Conference on Computer Vision (ICCV). Z. Yan, Y. Yuan, W. Zuo, X. Tan, Y. Wang, S. Wen, and E. Ding, \"Perspective-guided convolution networks for crowd counting,\" 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 952-961, 2019.\n\nAttention scaling for crowd counting. X Jiang, L Zhang, M Xu, T Zhang, P Lv, B Zhou, X Yang, Y Pang, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). X. Jiang, L. Zhang, M. Xu, T. Zhang, P. Lv, B. Zhou, X. Yang, and Y. Pang, \"Attention scaling for crowd counting,\" 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4705-4714, 2020.\n\nCrowd counting via segmentation guided attention networks and curriculum loss. Q Wang, T Breckon, IEEE Transactions on Intelligent Transportation Systems. Q. Wang and T. Breckon, \"Crowd counting via segmentation guided attention networks and curriculum loss,\" IEEE Transactions on Intel- ligent Transportation Systems, 2022.\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, 3rd International Conference on Learning Representations. San Diego, CA, USAConference Track ProceedingsK. Simonyan and A. Zisserman, \"Very deep convolutional networks for large-scale image recognition,\" in 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.\n\nGoing deeper with convolutions. C Szegedy, W Liu, Y Jia, P Sermanet, S E Reed, D Anguelov, D Erhan, V Vanhoucke, A Rabinovich, 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. E. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich, \"Going deeper with convolutions,\" 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1-9, 2015.\n\nYou only look once: Unified, real-time object detection. J Redmon, S K Divvala, R B Girshick, A Farhadi, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). J. Redmon, S. K. Divvala, R. B. Girshick, and A. Farhadi, \"You only look once: Unified, real-time object detection,\" 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 779-788, 2016.\n\nSegnet: A deep convolutional encoder-decoder architecture for robust semantic pixel-wise labelling. V Badrinarayanan, A Handa, R Cipolla, abs/1505.07293ArXiv. V. Badrinarayanan, A. Handa, and R. Cipolla, \"Segnet: A deep con- volutional encoder-decoder architecture for robust semantic pixel-wise labelling,\" ArXiv, vol. abs/1505.07293, 2015.\n\nU-net: Convolutional networks for biomedical image segmentation. O Ronneberger, P Fischer, T Brox, abs/1505.04597ArXiv. O. Ronneberger, P. Fischer, and T. Brox, \"U-net: Convolutional networks for biomedical image segmentation,\" ArXiv, vol. abs/1505.04597, 2015.\n\nCross-scene crowd counting via deep convolutional neural networks. C Zhang, H Li, X Wang, X Yang, 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). C. Zhang, H. Li, X. Wang, and X. Yang, \"Cross-scene crowd counting via deep convolutional neural networks,\" 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 833-841, 2015.\n\nRecent survey on crowd density estimation and counting for visual surveillance. S A M Saleh, S A Suandi, H Ibrahim, Eng. Appl. Artif. Intell. 41S. A. M. Saleh, S. A. Suandi, and H. Ibrahim, \"Recent survey on crowd density estimation and counting for visual surveillance,\" Eng. Appl. Artif. Intell., vol. 41, pp. 103-114, 2015.\n\nA review of crowd counting techniques. Jeevitha, Jeevitha, \"A review of crowd counting techniques,\" 2018.\n\nCrowd density estimation using image processing : A survey. R S P K , S Nithya, S Borra, R. S. P. K, S. Nithya, and S. Borra, \"Crowd density estimation using image processing : A survey,\" 2018.\n\nA survey of recent advances in cnnbased single image crowd counting and density estimation. V A Sindagi, V M Patel, Pattern Recognit. Lett. 107V. A. Sindagi and V. M. Patel, \"A survey of recent advances in cnn- based single image crowd counting and density estimation,\" Pattern Recognit. Lett., vol. 107, pp. 3-16, 2018.\n\nDeep learning for crowd counting: A survey. T W Cenggoro, Engineering, MAthematics and Computer Science (EMACS) Journal. T. W. Cenggoro, \"Deep learning for crowd counting: A survey,\" Engineering, MAthematics and Computer Science (EMACS) Journal, 2019.\n\nConvolutional-neural networkbased image crowd counting: Review, categorization, analysis, and performance evaluation. N Ilyas, A Shahzad, K Kim, Sensors. 20N. Ilyas, A. Shahzad, and K. Kim, \"Convolutional-neural network- based image crowd counting: Review, categorization, analysis, and performance evaluation,\" Sensors (Basel, Switzerland), vol. 20, 2020.\n\nCnn-based density estimation and crowd counting: A survey. G Gao, J Gao, Q Liu, Q Wang, Y Wang, ArXiv. G. Gao, J. Gao, Q. Liu, Q. Wang, and Y. Wang, \"Cnn-based density estimation and crowd counting: A survey,\" ArXiv, vol. abs/2003.12783, 2020.\n\nCrowd counting: A survey of machine learning approaches. M S Abdou, A Erradi, 2020 IEEE International Conference on Informatics, IoT, and Enabling Technologies (ICIoT). M. S. Abdou and A. Erradi, \"Crowd counting: A survey of machine learning approaches,\" 2020 IEEE International Conference on Infor- matics, IoT, and Enabling Technologies (ICIoT), pp. 48-54, 2020.\n\nCrowd counting for static images: A survey of methodology. Y Luo, J Lu, B Zhang, 39th Chinese Control Conference (CCC). Y. Luo, J. Lu, and B. Zhang, \"Crowd counting for static images: A survey of methodology,\" 2020 39th Chinese Control Conference (CCC), pp. 6602-6607, 2020.\n\nA survey on deep learningbased single image crowd counting: Network design, loss function and supervisory signal. H Bai, J Mao, S.-H G Chan, H. Bai, J. Mao, and S.-H. G. Chan, \"A survey on deep learning- based single image crowd counting: Network design, loss function and supervisory signal,\" 2020.\n\nA survey on crowd counting methods and datasets. W Jingying, W. Jingying, \"A survey on crowd counting methods and datasets,\" 2020.\n\nAdvances in convolution neural networks based crowd counting and density estimation. R Gouiaa, M A Akhloufi, M Shahbazi, Big Data Cogn. Comput. 550R. Gouiaa, M. A. Akhloufi, and M. Shahbazi, \"Advances in convolution neural networks based crowd counting and density estimation,\" Big Data Cogn. Comput., vol. 5, p. 50, 2021.\n\nA survey of crowd counting and density estimation based on convolutional neural network. Z Fan, H Zhang, Z Zhang, G Lu, Y Zhang, Y Wang, Neurocomputing. 472Z. Fan, H. Zhang, Z. Zhang, G. Lu, Y. Zhang, and Y. Wang, \"A survey of crowd counting and density estimation based on convolutional neural network,\" Neurocomputing, vol. 472, pp. 224-251, 2022.\n\nFeature mining for localised crowd counting. K Chen, C C Loy, S Gong, T Xiang, Proceedings of the British Machine Vision Conference. the British Machine Vision ConferenceBMVA Press11K. Chen, C. C. Loy, S. Gong, and T. Xiang, \"Feature mining for localised crowd counting,\" in Proceedings of the British Machine Vision Conference, pp. 21.1-21.11, BMVA Press, 2012.\n\nPrivacy preserving crowd monitoring: Counting people without people models or tracking. A B Chan, Z.-S J Liang, N Vasconcelos, 2008 IEEE Conference on Computer Vision and Pattern Recognition. A. B. Chan, Z.-S. J. Liang, and N. Vasconcelos, \"Privacy preserv- ing crowd monitoring: Counting people without people models or tracking,\" 2008 IEEE Conference on Computer Vision and Pattern Recognition, pp. 1-7, 2008.\n\nMulti-source multiscale counting in extremely dense crowd images. H Idrees, I Saleemi, C Seibert, M Shah, 2013 IEEE Conference on Computer Vision and Pattern RecognitionH. Idrees, I. Saleemi, C. Seibert, and M. Shah, \"Multi-source multi- scale counting in extremely dense crowd images,\" 2013 IEEE Con- ference on Computer Vision and Pattern Recognition, pp. 2547-2554, 2013.\n\nComposition loss for counting, density map estimation and localization in dense crowds. H Idrees, M Tayyab, K Athrey, D Zhang, S A Al-Maadeed, N M Rajpoot, M Shah, abs/1808.01050ArXiv. H. Idrees, M. Tayyab, K. Athrey, D. Zhang, S. A. Al-Maadeed, N. M. Rajpoot, and M. Shah, \"Composition loss for counting, density map es- timation and localization in dense crowds,\" ArXiv, vol. abs/1808.01050, 2018.\n\nJhu-crowd++: Large-scale crowd counting dataset and a benchmark method. V A Sindagi, R Yasarla, V M Patel, Technical ReportV. A. Sindagi, R. Yasarla, and V. M. Patel, \"Jhu-crowd++: Large-scale crowd counting dataset and a benchmark method,\" Technical Report, 2020.\n\nRgb-t crowd counting from drone: A benchmark and mmccn network. T Peng, Q Li, P Zhu, Computer Vision -ACCV 2020: 15th Asian Conference on Computer Vision. Kyoto, Japan; Berlin, HeidelbergSpringer-VerlagRevised Selected PapersT. Peng, Q. Li, and P. Zhu, \"Rgb-t crowd counting from drone: A benchmark and mmccn network,\" in Computer Vision -ACCV 2020: 15th Asian Conference on Computer Vision, Kyoto, Japan, November 30 -December 4, 2020, Revised Selected Papers, Part VI, (Berlin, Heidelberg), p. 497-513, Springer-Verlag, 2020.\n\nNwpu-crowd: A large-scale benchmark for crowd counting and localization. Q Wang, J Gao, W Lin, X Li, IEEE Transactions on Pattern Analysis and Machine Intelligence. 43Q. Wang, J. Gao, W. Lin, and X. Li, \"Nwpu-crowd: A large-scale benchmark for crowd counting and localization,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 43, pp. 2141-2149, 2021.\n\nExtremely overlapping vehicle counting. R Guerrero-G\u00f3mez-Olmedo, B Torre-Jim\u00e9nez, R L\u00f3pez-Sastre, S Maldonado-Basc\u00f3n, D O Rubio, Iberian Conference on Pattern Recognition and Image Analysis (IbPRIA). R. Guerrero-G\u00f3mez-Olmedo, B. Torre-Jim\u00e9nez, R. L\u00f3pez-Sastre, S. Maldonado-Basc\u00f3n, and D. O. noro Rubio, \"Extremely overlapping vehicle counting,\" in Iberian Conference on Pattern Recognition and Image Analysis (IbPRIA), 2015.\n\nDrone-based object counting by spatially regularized regional proposal network. M.-R Hsieh, Y.-L Lin, W H Hsu, 2017 IEEE International Conference on Computer Vision (ICCV). M.-R. Hsieh, Y.-L. Lin, and W. H. Hsu, \"Drone-based object count- ing by spatially regularized regional proposal network,\" 2017 IEEE International Conference on Computer Vision (ICCV), pp. 4165-4173, 2017.\n\nDetection and tracking meet drones challenge. P Zhu, L Wen, D Du, X Bian, H Fan, Q Hu, H Ling, IEEE Transactions on Pattern Analysis and Machine Intelligence. P. Zhu, L. Wen, D. Du, X. Bian, H. Fan, Q. Hu, and H. Ling, \"Detection and tracking meet drones challenge,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 1-1, 2021.\n\nCross-modal collaborative representation learning and a large-scale rgbt benchmark for crowd counting. L Liu, J Chen, H Wu, G Li, C Li, L Lin, 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). L. Liu, J. Chen, H. Wu, G. Li, C. Li, and L. Lin, \"Cross-modal collaborative representation learning and a large-scale rgbt benchmark for crowd counting,\" in 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4821-4831, 2021.\n\nMulti-scale convolutional neural networks for crowd counting. L Zeng, X Xu, B Cai, S Qiu, T Zhang, 2017 IEEE International Conference on Image Processing (ICIP). L. Zeng, X. Xu, B. Cai, S. Qiu, and T. Zhang, \"Multi-scale convolu- tional neural networks for crowd counting,\" 2017 IEEE International Conference on Image Processing (ICIP), pp. 465-469, 2017.\n\nCrowd counting and density estimation by trellis encoderdecoder networks. X Jiang, Z Xiao, B Zhang, X Zhen, X Cao, D S Doermann, L Shao, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). X. Jiang, Z. Xiao, B. Zhang, X. Zhen, X. Cao, D. S. Doermann, and L. Shao, \"Crowd counting and density estimation by trellis encoder- decoder networks,\" 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6126-6135, 2019.\n\nMobilecount: An efficient encoderdecoder framework for real-time crowd counting. C Gao, P Wang, Y Gao, Pattern Recognition and Computer Vision: Second Chinese Conference, PRCV 2019. Xi'an, ChinaSpringer-VerlagProceedings, Part IIC. Gao, P. Wang, and Y. Gao, \"Mobilecount: An efficient encoder- decoder framework for real-time crowd counting,\" in Pattern Recogni- tion and Computer Vision: Second Chinese Conference, PRCV 2019, Xi'an, China, November 8-11, 2019, Proceedings, Part II, p. 582-595, Springer-Verlag, 2019.\n\nDeep densityaware count regressor. Z Chen, J Cheng, Y Yuan, D Liao, Y Li, J Lv, ECAI. Z. Chen, J. Cheng, Y. Yuan, D. Liao, Y. Li, and J. Lv, \"Deep density- aware count regressor,\" in ECAI, 2020.\n\nBayesian loss for crowd count estimation with point supervision. Z Ma, X Wei, X Hong, Y Gong, 2019 IEEE/CVF International Conference on Computer Vision (ICCV). Los Alamitos, CA, USAZ. Ma, X. Wei, X. Hong, and Y. Gong, \"Bayesian loss for crowd count estimation with point supervision,\" in 2019 IEEE/CVF International Conference on Computer Vision (ICCV), (Los Alamitos, CA, USA), pp. 6141-6150, nov 2019.\n\nA real-time deep network for crowd counting. X Shi, X Li, C Wu, S Kong, J Yang, L He, ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). X. Shi, X. Li, C. Wu, S. Kong, J. Yang, and L. He, \"A real-time deep network for crowd counting,\" ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 2328-2332, 2020.\n\nFusioncount: Efficient crowd counting via multiscale feature fusion. Y Ma, V Sanchez, T Guha, abs/2202.13660ArXiv. Y. Ma, V. Sanchez, and T. Guha, \"Fusioncount: Efficient crowd counting via multiscale feature fusion,\" ArXiv, vol. abs/2202.13660, 2022.\n\nCrowd counting via scale-adaptive convolutional neural network. L Zhang, M Shi, Q Chen, 2018 IEEE Winter Conference on Applications of Computer Vision (WACV). L. Zhang, M. Shi, and Q. Chen, \"Crowd counting via scale-adaptive convolutional neural network,\" 2018 IEEE Winter Conference on Applications of Computer Vision (WACV), pp. 1113-1121, 2018.\n\nWeighing counts: Sequential crowd counting by reinforcement learning. L Liu, H Lu, H Zou, H Xiong, Z Cao, C Shen, abs/2007.08260ArXiv. L. Liu, H. Lu, H. Zou, H. Xiong, Z. Cao, and C. Shen, \"Weighing counts: Sequential crowd counting by reinforcement learning,\" ArXiv, vol. abs/2007.08260, 2020.\n\nCrowdnet: A deep convolutional network for dense crowd counting. L Boominathan, S S S Kruthiventi, R V Babu, Proceedings of the 24th ACM international conference on Multimedia. the 24th ACM international conference on MultimediaL. Boominathan, S. S. S. Kruthiventi, and R. V. Babu, \"Crowdnet: A deep convolutional network for dense crowd counting,\" Proceedings of the 24th ACM international conference on Multimedia, 2016.\n\nSwitching convolutional neural network for crowd counting. D Sam, S Surya, R Babu, 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Los Alamitos, CA, USAIEEE Computer SocietyD. Sam, S. Surya, and R. Babu, \"Switching convolutional neural network for crowd counting,\" in 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), (Los Alamitos, CA, USA), pp. 4031-4039, IEEE Computer Society, jul 2017.\n\nCnn-based cascaded multi-task learning of high-level prior and density estimation for crowd counting. V A Sindagi, V M Patel, 14th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS). V. A. Sindagi and V. M. Patel, \"Cnn-based cascaded multi-task learning of high-level prior and density estimation for crowd counting,\" 2017 14th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS), pp. 1-6, 2017.\n\nDecidenet: Counting varying density crowds through attention guided detection and density estimation. J Liu, C Gao, D Meng, A G Hauptmann, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. J. Liu, C. Gao, D. Meng, and A. G. Hauptmann, \"Decidenet: Counting varying density crowds through attention guided detection and density estimation,\" in 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5197-5206, 2018.\n\nFast r-cnn. R Girshick, 2015 IEEE International Conference on Computer Vision (ICCV). R. Girshick, \"Fast r-cnn,\" in 2015 IEEE International Conference on Computer Vision (ICCV), pp. 1440-1448, 2015.\n\nRevisiting perspective information for efficient crowd counting. M Shi, Z Yang, C Xu, Q Chen, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). M. Shi, Z. Yang, C. Xu, and Q. Chen, \"Revisiting perspective infor- mation for efficient crowd counting,\" in 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 7271-7280, 2019.\n\nPcc net: Perspective crowd counting via spatial convolutional network. J Gao, Q Wang, X Li, IEEE Transactions on Circuits and Systems for Video Technology. 30J. Gao, Q. Wang, and X. Li, \"Pcc net: Perspective crowd counting via spatial convolutional network,\" IEEE Transactions on Circuits and Systems for Video Technology, vol. 30, pp. 3486-3498, 2020.\n\nTowards using count-level weak supervision for crowd counting. Y Lei, Y Liu, P Zhang, L Liu, Pattern Recognit. 109107616Y. Lei, Y. Liu, P. Zhang, and L. Liu, \"Towards using count-level weak supervision for crowd counting,\" Pattern Recognit., vol. 109, p. 107616, 2021.\n\nWeaklysupervised crowd counting learns from sorting rather than locations. Y Yang, G Li, Z Wu, L Su, Q Huang, N Sebe, Computer Vision -ECCV 2020: 16th European Conference. Glasgow, UK; Berlin, HeidelbergSpringer-VerlagProceedings, Part VIIIY. Yang, G. Li, Z. Wu, L. Su, Q. Huang, and N. Sebe, \"Weakly- supervised crowd counting learns from sorting rather than locations,\" in Computer Vision -ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part VIII, (Berlin, Heidelberg), p. 1-17, Springer-Verlag, 2020.\n\nTafnet: A three-stream adaptive fusion network for rgb-t crowd counting. H Tang, Y Wang, L.-P Chau, abs/2202.08517ArXiv. H. Tang, Y. Wang, and L.-P. Chau, \"Tafnet: A three-stream adaptive fusion network for rgb-t crowd counting,\" ArXiv, vol. abs/2202.08517, 2022.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). K. He, X. Zhang, S. Ren, and J. Sun, \"Deep residual learning for image recognition,\" 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-778, 2016.\n\nM Sandler, A Howard, M Zhu, A Zhmoginov, L.-C Chen, Mobilenetv2: Inverted residuals and linear bottlenecks. 2018M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen, \"Mobilenetv2: Inverted residuals and linear bottlenecks,\" in 2018\n\nIEEE/CVF Conference on Computer Vision and Pattern Recognition. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4510-4520, 2018.\n\nLight-weight refinenet for realtime semantic segmentation. V Nekrasov, C Shen, I D Reid, abs/1810.03272ArXiv. V. Nekrasov, C. Shen, and I. D. Reid, \"Light-weight refinenet for real- time semantic segmentation,\" ArXiv, vol. abs/1810.03272, 2018.\n\nAttentional neural fields for crowd counting. A Zhang, L Yue, J Shen, F Zhu, X Zhen, X Cao, L Shao, 2019 IEEE/CVF International Conference on Computer Vision (ICCV). A. Zhang, L. Yue, J. Shen, F. Zhu, X. Zhen, X. Cao, and L. Shao, \"Attentional neural fields for crowd counting,\" in 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 5713-5722, 2019.\n\nTo choose or to fuse? scale selection for crowd counting. Q Song, C Wang, Y Wang, Y Tai, C Wang, J Li, J Wu, J Ma, AAAI. Q. Song, C. Wang, Y. Wang, Y. Tai, C. Wang, J. Li, J. Wu, and J. Ma, \"To choose or to fuse? scale selection for crowd counting,\" in AAAI, 2021.\n\nA unified multi-task learning framework of real-time drone supervision for crowd counting. S Gu, Z Lian, abs/2202.03843ArXiv. S. Gu and Z. Lian, \"A unified multi-task learning frame- work of real-time drone supervision for crowd counting,\" ArXiv, vol. abs/2202.03843, 2022.\n\nEncoderdecoder based convolutional neural networks with multi-scale-aware modules for crowd counting. P Thanasutives, K Fukui, M Numao, B Kijsirikul, 2020 25th International Conference on Pattern Recognition (ICPR). P. Thanasutives, K. ichi Fukui, M. Numao, and B. Kijsirikul, \"Encoder- decoder based convolutional neural networks with multi-scale-aware modules for crowd counting,\" 2020 25th International Conference on Pattern Recognition (ICPR), pp. 2382-2389, 2021.\n\nImagenet: A large-scale hierarchical image database. J Deng, W Dong, R Socher, L.-J Li, K Li, L Fei-Fei, 2009 IEEE conference on computer vision and pattern recognition. IeeeJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, \"Imagenet: A large-scale hierarchical image database,\" in 2009 IEEE conference on computer vision and pattern recognition, pp. 248-255, Ieee, 2009.\n\nContext-aware crowd counting. W Liu, M Salzmann, P V Fua, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). W. Liu, M. Salzmann, and P. V. Fua, \"Context-aware crowd counting,\" 2019 IEEE/CVF Conference on Computer Vision and Pattern Recog- nition (CVPR), pp. 5094-5103, 2019.\n\nRgb-t crowd counting from drone: A benchmark and mmccn network. T Peng, Q Li, P F Zhu, ACCVT. Peng, Q. Li, and P. F. Zhu, \"Rgb-t crowd counting from drone: A benchmark and mmccn network,\" in ACCV, 2020.\n\nAttention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS'17. the 31st International Conference on Neural Information Processing Systems, NIPS'17Red Hook, NY, USACurran Associates IncA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, \"Attention is all you need,\" in Proceedings of the 31st International Conference on Neural In- formation Processing Systems, NIPS'17, (Red Hook, NY, USA), p. 6000-6010, Curran Associates Inc., 2017.\n\nAn image is worth 16x16 words: Transformers for image recognition at scale. A Dosovitskiy, L Beyer, A Kolesnikov, D Weissenborn, X Zhai, T Unterthiner, M Dehghani, M Minderer, G Heigold, S Gelly, J Uszkoreit, N Houlsby, ArXiv. A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby, \"An image is worth 16x16 words: Trans- formers for image recognition at scale,\" ArXiv, vol. abs/2010.11929, 2021.\n\nTraining data-efficient image transformers & distillation through attention. H Touvron, M Cord, M Douze, F Massa, A Sablayrolles, H , ICML. H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. J'egou, \"Training data-efficient image transformers & distillation through attention,\" in ICML, 2021.\n\nEnd-to-end object detection with transformers. N Carion, F Massa, G Synnaeve, N Usunier, A Kirillov, S Zagoruyko, ArXiv. N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, \"End-to-end object detection with transformers,\" ArXiv, vol. abs/2005.12872, 2020.\n\nDeformable detr: Deformable transformers for end-to-end object detection. X Zhu, W Su, L Lu, B Li, X Wang, J Dai, abs/2010.04159ArXiv. X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, \"Deformable detr: Deformable transformers for end-to-end object detection,\" ArXiv, vol. abs/2010.04159, 2021.\n\nCross-modal self-attention network for referring image segmentation. L Ye, M Rochan, Z Liu, Y Wang, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). L. Ye, M. Rochan, Z. Liu, and Y. Wang, \"Cross-modal self-attention network for referring image segmentation,\" 2019 IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition (CVPR), pp. 10494- 10503, 2019.\n\nCrowd transformer network. V Ranjan, M Shah, M H Nguyen, abs/1904.02774ArXiv. V. Ranjan, M. Shah, and M. H. Nguyen, \"Crowd transformer network,\" ArXiv, vol. abs/1904.02774, 2019.\n\nCongested crowd instance localization with dilated convolutional swin transformer. J Gao, M Gong, X Li, abs/2108.00584ArXiv. J. Gao, M. Gong, and X. Li, \"Congested crowd instance lo- calization with dilated convolutional swin transformer,\" ArXiv, vol. abs/2108.00584, 2021.\n\nCctrans: Simplifying and improving crowd counting with transformer. Y Tian, X Chu, H Wang, abs/2109.14483ArXiv. Y. Tian, X. Chu, and H. Wang, \"Cctrans: Simplifying and improving crowd counting with transformer,\" ArXiv, vol. abs/2109.14483, 2021.\n\nBoosting crowd counting with transformers. G Sun, Y Liu, T Probst, D P Paudel, N Popovic, L V Gool, abs/2105.10926ArXiv. G. Sun, Y. Liu, T. Probst, D. P. Paudel, N. Popovic, and L. V. Gool, \"Boosting crowd counting with transformers,\" ArXiv, vol. abs/2105.10926, 2021.\n\nAttention in crowd counting using the transformer and density map to improve counting result. P T Do, 2021 8th NAFOSTED Conference on Information and Computer Science (NICS). P. T. Do, \"Attention in crowd counting using the transformer and den- sity map to improve counting result,\" 2021 8th NAFOSTED Conference on Information and Computer Science (NICS), pp. 65-70, 2021.\n\nAn end-to-end transformer model for crowd localization. D Liang, W Xu, X Bai, abs/2202.13065ArXiv. D. Liang, W. Xu, and X. Bai, \"An end-to-end transformer model for crowd localization,\" ArXiv, vol. abs/2202.13065, 2022.\n\nScene-adaptive attention network for crowd counting. X Wei, Y Kang, J Yang, Y Qiu, D Shi, W Tan, Y Gong, abs/2112.15509ArXiv. X. Wei, Y. Kang, J. Yang, Y. Qiu, D. Shi, W. Tan, and Y. Gong, \"Scene-adaptive attention network for crowd counting,\" ArXiv, vol. abs/2112.15509, 2021.\n\nTranscrowd: weakly-supervised crowd counting with transformers. D Liang, X Chen, W Xu, Y Zhou, X Bai, Science China Information Sciences. D. Liang, X. Chen, W. Xu, Y. Zhou, and X. Bai, \"Transcrowd: weakly-supervised crowd counting with transformers,\" Science China Information Sciences, 2022.\n\nTwins: Revisiting the design of spatial attention in vision transformers. X Chu, Z Tian, Y Wang, B Zhang, H Ren, X Wei, H Xia, C Shen, NeurIPSX. Chu, Z. Tian, Y. Wang, B. Zhang, H. Ren, X. Wei, H. Xia, and C. Shen, \"Twins: Revisiting the design of spatial attention in vision transformers,\" in NeurIPS, 2021.\n\nWeighing counts: Sequential crowd counting by reinforcement learning. L Liu, H Lu, H Zou, H Xiong, Z Cao, C Shen, abs/2007.08260ArXiv. L. Liu, H. Lu, H. Zou, H. Xiong, Z. Cao, and C. Shen, \"Weighing counts: Sequential crowd counting by reinforcement learning,\" ArXiv, vol. abs/2007.08260, 2020.\n\nCrowd aware summarization of surveillance videos by deep reinforcement learning. J Xu, Z Sun, C Ma, Multim. Tools Appl. 80J. Xu, Z. Sun, and C. Ma, \"Crowd aware summarization of surveillance videos by deep reinforcement learning,\" Multim. Tools Appl., vol. 80, pp. 6121-6141, 2021.\n\nGlobal sum pooling: A generalization trick for object counting with small datasets of large images. S Aich, I Stavness, arXiv:1805.11123arXiv preprintS. Aich and I. Stavness, \"Global sum pooling: A generalization trick for object counting with small datasets of large images,\" arXiv preprint arXiv:1805.11123, 2018.\n\nBoosting crowd counting via multifaceted attention. H Lin, Z Ma, R Ji, Y Wang, X Hong, abs/2203.02636ArXiv. H. Lin, Z. Ma, R. Ji, Y. Wang, and X. Hong, \"Boosting crowd counting via multifaceted attention,\" ArXiv, vol. abs/2203.02636, 2022.\n\nLearning from synthetic data for crowd counting in the wild. Q Wang, J Gao, W Lin, Y Yuan, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Q. Wang, J. Gao, W. Lin, and Y. Yuan, \"Learning from synthetic data for crowd counting in the wild,\" 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 8190-8199, 2019.\n\nExploiting unlabeled data in cnns by self-supervised learning to rank. X Liu, J Van De Weijer, A D Bagdanov, IEEE Transactions on Pattern Analysis and Machine Intelligence. 41X. Liu, J. van de Weijer, and A. D. Bagdanov, \"Exploiting unlabeled data in cnns by self-supervised learning to rank,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 41, pp. 1862-1878, 2019.\n\nCurriculum learning. Y Bengio, J Louradour, R Collobert, J Weston, Proceedings of the 26th Annual International Conference on Machine Learning, ICML '09. the 26th Annual International Conference on Machine Learning, ICML '09New York, NY, USAAssociation for Computing MachineryY. Bengio, J. Louradour, R. Collobert, and J. Weston, \"Curriculum learning,\" in Proceedings of the 26th Annual International Conference on Machine Learning, ICML '09, (New York, NY, USA), p. 41-48, Association for Computing Machinery, 2009.\n\nPoint in, box out: Beyond counting persons in crowds. Y Liu, M Shi, Q Zhao, X Wang, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Y. Liu, M. Shi, Q. Zhao, and X. Wang, \"Point in, box out: Beyond counting persons in crowds,\" in 2019 IEEE/CVF Conference on Com- puter Vision and Pattern Recognition (CVPR), pp. 6462-6471, 2019.\n\nDistribution matching for crowd counting. B Wang, H Liu, D Samaras, M Hoai, ArXiv. B. Wang, H. Liu, D. Samaras, and M. Hoai, \"Distribution matching for crowd counting,\" ArXiv, vol. abs/2009.13077, 2020.\n\n. R Guerrero-G\u00f3mez-Olmedo, B Torre-Jim\u00e9nez, R J L\u00f3pez-Sastre, S Maldonado-Basc\u00f3n, D O\u00f1oro-Rubio, Extremely overlapping vehicle counting,\" in IbPRIAR. Guerrero-G\u00f3mez-Olmedo, B. Torre-Jim\u00e9nez, R. J. L\u00f3pez-Sastre, S. Maldonado-Basc\u00f3n, and D. O\u00f1oro-Rubio, \"Extremely overlapping vehicle counting,\" in IbPRIA, 2015.\n\nModeling noisy annotations for crowd counting. J Wan, A B Chan, NeurIPS. J. Wan and A. B. Chan, \"Modeling noisy annotations for crowd counting,\" in NeurIPS, 2020.\n", "annotations": {"author": "[{\"end\":152,\"start\":133},{\"end\":167,\"start\":153},{\"end\":181,\"start\":168}]", "publisher": null, "author_last_name": "[{\"end\":151,\"start\":147},{\"end\":166,\"start\":159},{\"end\":180,\"start\":174}]", "author_first_name": "[{\"end\":141,\"start\":133},{\"end\":146,\"start\":142},{\"end\":158,\"start\":153},{\"end\":173,\"start\":168}]", "author_affiliation": null, "title": "[{\"end\":130,\"start\":1},{\"end\":311,\"start\":182}]", "venue": null, "abstract": "[{\"end\":1699,\"start\":406}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2334,\"start\":2331},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2339,\"start\":2336},{\"end\":3110,\"start\":3109},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3179,\"start\":3176},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3184,\"start\":3181},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3189,\"start\":3186},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3208,\"start\":3205},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3213,\"start\":3210},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3218,\"start\":3215},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3223,\"start\":3220},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3229,\"start\":3225},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3235,\"start\":3231},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3915,\"start\":3911},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3932,\"start\":3928},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3948,\"start\":3944},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3954,\"start\":3950},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4013,\"start\":4009},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4019,\"start\":4015},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4302,\"start\":4298},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4308,\"start\":4304},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4314,\"start\":4310},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4320,\"start\":4316},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4326,\"start\":4322},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4332,\"start\":4328},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4472,\"start\":4468},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4478,\"start\":4474},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4501,\"start\":4497},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":4530,\"start\":4526},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4536,\"start\":4532},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":5050,\"start\":5046},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6035,\"start\":6031},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6041,\"start\":6037},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6047,\"start\":6043},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6053,\"start\":6049},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":6059,\"start\":6055},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":6065,\"start\":6061},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":6071,\"start\":6067},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":6077,\"start\":6073},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":6083,\"start\":6079},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":6089,\"start\":6085},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":6095,\"start\":6091},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":6101,\"start\":6097},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":6107,\"start\":6103},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6283,\"start\":6279},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6289,\"start\":6285},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6295,\"start\":6291},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6301,\"start\":6297},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":6307,\"start\":6303},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":6313,\"start\":6309},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":6319,\"start\":6315},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":6325,\"start\":6321},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":6331,\"start\":6327},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":6337,\"start\":6333},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":6343,\"start\":6339},{\"end\":6603,\"start\":6602},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":8956,\"start\":8952},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9077,\"start\":9073},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":12881,\"start\":12877},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":13762,\"start\":13758},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":13770,\"start\":13766},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":15542,\"start\":15538},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":15949,\"start\":15945},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":16051,\"start\":16047},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":16412,\"start\":16408},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":16516,\"start\":16512},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":16722,\"start\":16718},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":17001,\"start\":16997},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":17741,\"start\":17737},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":18070,\"start\":18066},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":18318,\"start\":18314},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":18479,\"start\":18475},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":18699,\"start\":18695},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":18917,\"start\":18913},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":19347,\"start\":19343},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":19529,\"start\":19525},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":19842,\"start\":19838},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":20076,\"start\":20072},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":20082,\"start\":20078},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":20095,\"start\":20091},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":20160,\"start\":20156},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":20176,\"start\":20172},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":20190,\"start\":20186},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":20765,\"start\":20761},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":20848,\"start\":20844},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":21290,\"start\":21286},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":21398,\"start\":21394},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":21440,\"start\":21436},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":21449,\"start\":21445},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":21879,\"start\":21875},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":21890,\"start\":21886},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":22601,\"start\":22597},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":22724,\"start\":22720},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":22953,\"start\":22949},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":23308,\"start\":23304},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":23321,\"start\":23317},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":23448,\"start\":23444},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":23495,\"start\":23491},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":23945,\"start\":23941},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":24134,\"start\":24130},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":24188,\"start\":24184},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":24225,\"start\":24221},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":24990,\"start\":24986},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":25263,\"start\":25259},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":25459,\"start\":25455},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":25695,\"start\":25691},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":25989,\"start\":25985},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":26082,\"start\":26078},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":26481,\"start\":26477},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":26669,\"start\":26665},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":26692,\"start\":26688},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":26698,\"start\":26694},{\"attributes\":{\"ref_id\":\"b88\"},\"end\":26727,\"start\":26723},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":26759,\"start\":26755},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":27016,\"start\":27012},{\"attributes\":{\"ref_id\":\"b89\"},\"end\":27231,\"start\":27227},{\"attributes\":{\"ref_id\":\"b90\"},\"end\":27237,\"start\":27233},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":27243,\"start\":27239},{\"attributes\":{\"ref_id\":\"b92\"},\"end\":27249,\"start\":27245},{\"attributes\":{\"ref_id\":\"b93\"},\"end\":27255,\"start\":27251},{\"attributes\":{\"ref_id\":\"b94\"},\"end\":27261,\"start\":27257},{\"attributes\":{\"ref_id\":\"b95\"},\"end\":27267,\"start\":27263},{\"attributes\":{\"ref_id\":\"b96\"},\"end\":27273,\"start\":27269},{\"attributes\":{\"ref_id\":\"b96\"},\"end\":27313,\"start\":27309},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":27622,\"start\":27618},{\"attributes\":{\"ref_id\":\"b96\"},\"end\":27734,\"start\":27730},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":27782,\"start\":27778},{\"attributes\":{\"ref_id\":\"b97\"},\"end\":27957,\"start\":27953},{\"attributes\":{\"ref_id\":\"b98\"},\"end\":28274,\"start\":28270},{\"attributes\":{\"ref_id\":\"b99\"},\"end\":28280,\"start\":28276},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":29031,\"start\":29027},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":29273,\"start\":29269},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":29467,\"start\":29463},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":29483,\"start\":29479},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":29501,\"start\":29497},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":31125,\"start\":31121},{\"attributes\":{\"ref_id\":\"b102\"},\"end\":31132,\"start\":31127},{\"attributes\":{\"ref_id\":\"b96\"},\"end\":31138,\"start\":31134},{\"attributes\":{\"ref_id\":\"b103\"},\"end\":31145,\"start\":31140},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":31310,\"start\":31306},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":31467,\"start\":31463},{\"attributes\":{\"ref_id\":\"b96\"},\"end\":32103,\"start\":32099},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":32128,\"start\":32124},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":32804,\"start\":32800},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":32810,\"start\":32806},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":32816,\"start\":32812},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":32822,\"start\":32818},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":32828,\"start\":32824},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":32834,\"start\":32830},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":32840,\"start\":32836},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":32869,\"start\":32865},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":32875,\"start\":32871},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":32881,\"start\":32877},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":32887,\"start\":32883},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":33048,\"start\":33044},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":33116,\"start\":33112},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":33140,\"start\":33136},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":33165,\"start\":33161},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":33292,\"start\":33288},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":34647,\"start\":34643},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":35458,\"start\":35454},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":35926,\"start\":35922},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":36583,\"start\":36579},{\"attributes\":{\"ref_id\":\"b104\"},\"end\":37843,\"start\":37838},{\"attributes\":{\"ref_id\":\"b105\"},\"end\":38067,\"start\":38062},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":38115,\"start\":38111},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":38631,\"start\":38627},{\"attributes\":{\"ref_id\":\"b106\"},\"end\":38966,\"start\":38961},{\"attributes\":{\"ref_id\":\"b106\"},\"end\":39273,\"start\":39268},{\"attributes\":{\"ref_id\":\"b107\"},\"end\":40502,\"start\":40497},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":41517,\"start\":41513},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":41523,\"start\":41519},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":41897,\"start\":41893},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":41903,\"start\":41899},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":42232,\"start\":42228},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":42238,\"start\":42234},{\"attributes\":{\"ref_id\":\"b108\"},\"end\":43314,\"start\":43309},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":44273,\"start\":44269},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":44288,\"start\":44284},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":44304,\"start\":44300},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":44892,\"start\":44888},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":44903,\"start\":44899},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":44933,\"start\":44929},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":50630,\"start\":50626},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":50683,\"start\":50679},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":51598,\"start\":51594},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":51623,\"start\":51619},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":51640,\"start\":51636}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":49626,\"start\":49380},{\"attributes\":{\"id\":\"fig_1\"},\"end\":49689,\"start\":49627},{\"attributes\":{\"id\":\"fig_2\"},\"end\":50393,\"start\":49690},{\"attributes\":{\"id\":\"fig_3\"},\"end\":50455,\"start\":50394},{\"attributes\":{\"id\":\"fig_4\"},\"end\":50605,\"start\":50456},{\"attributes\":{\"id\":\"fig_5\"},\"end\":50684,\"start\":50606},{\"attributes\":{\"id\":\"fig_7\"},\"end\":51026,\"start\":50685},{\"attributes\":{\"id\":\"fig_8\"},\"end\":51190,\"start\":51027},{\"attributes\":{\"id\":\"fig_9\"},\"end\":51580,\"start\":51191},{\"attributes\":{\"id\":\"fig_10\"},\"end\":51650,\"start\":51581},{\"attributes\":{\"id\":\"fig_11\"},\"end\":51979,\"start\":51651},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":52105,\"start\":51980},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":53168,\"start\":52106},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":53992,\"start\":53169},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":56997,\"start\":53993},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":59386,\"start\":56998}]", "paragraph": "[{\"end\":2676,\"start\":1718},{\"end\":2990,\"start\":2678},{\"end\":4205,\"start\":2992},{\"end\":5024,\"start\":4207},{\"end\":6161,\"start\":5026},{\"end\":8200,\"start\":6163},{\"end\":10236,\"start\":8234},{\"end\":10508,\"start\":10238},{\"end\":11822,\"start\":10510},{\"end\":12571,\"start\":11824},{\"end\":13168,\"start\":12601},{\"end\":14103,\"start\":13200},{\"end\":14953,\"start\":14135},{\"end\":15433,\"start\":14982},{\"end\":16723,\"start\":15461},{\"end\":17966,\"start\":16750},{\"end\":19749,\"start\":17968},{\"end\":20096,\"start\":19751},{\"end\":21891,\"start\":20126},{\"end\":22849,\"start\":21945},{\"end\":23270,\"start\":22851},{\"end\":23322,\"start\":23272},{\"end\":23497,\"start\":23392},{\"end\":24530,\"start\":23679},{\"end\":25686,\"start\":24554},{\"end\":26016,\"start\":25688},{\"end\":26728,\"start\":26048},{\"end\":28167,\"start\":26730},{\"end\":28366,\"start\":28169},{\"end\":29736,\"start\":28398},{\"end\":30815,\"start\":29777},{\"end\":31086,\"start\":30864},{\"end\":31892,\"start\":31088},{\"end\":32190,\"start\":31894},{\"end\":33293,\"start\":32215},{\"end\":33548,\"start\":33315},{\"end\":33750,\"start\":33550},{\"end\":34056,\"start\":33799},{\"end\":34260,\"start\":34095},{\"end\":34385,\"start\":34262},{\"end\":34552,\"start\":34432},{\"end\":34961,\"start\":34590},{\"end\":35126,\"start\":35025},{\"end\":35561,\"start\":35192},{\"end\":36454,\"start\":35576},{\"end\":37455,\"start\":36494},{\"end\":37719,\"start\":37527},{\"end\":38501,\"start\":37743},{\"end\":38698,\"start\":38520},{\"end\":38870,\"start\":38728},{\"end\":39199,\"start\":38906},{\"end\":39385,\"start\":39226},{\"end\":39585,\"start\":39417},{\"end\":39874,\"start\":39627},{\"end\":39904,\"start\":39876},{\"end\":40185,\"start\":39948},{\"end\":40745,\"start\":40187},{\"end\":40923,\"start\":40808},{\"end\":41135,\"start\":40976},{\"end\":41334,\"start\":41228},{\"end\":41454,\"start\":41381},{\"end\":41904,\"start\":41456},{\"end\":42239,\"start\":41922},{\"end\":42457,\"start\":42241},{\"end\":42802,\"start\":42489},{\"end\":43717,\"start\":42804},{\"end\":44091,\"start\":43768},{\"end\":49143,\"start\":44093},{\"end\":49379,\"start\":49163}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13199,\"start\":13169},{\"attributes\":{\"id\":\"formula_1\"},\"end\":23391,\"start\":23323},{\"attributes\":{\"id\":\"formula_2\"},\"end\":23643,\"start\":23525},{\"attributes\":{\"id\":\"formula_3\"},\"end\":33798,\"start\":33751},{\"attributes\":{\"id\":\"formula_4\"},\"end\":34094,\"start\":34057},{\"attributes\":{\"id\":\"formula_5\"},\"end\":34431,\"start\":34386},{\"attributes\":{\"id\":\"formula_6\"},\"end\":34589,\"start\":34553},{\"attributes\":{\"id\":\"formula_7\"},\"end\":35191,\"start\":35127},{\"attributes\":{\"id\":\"formula_8\"},\"end\":36493,\"start\":36455},{\"attributes\":{\"id\":\"formula_9\"},\"end\":37526,\"start\":37456},{\"attributes\":{\"id\":\"formula_10\"},\"end\":38727,\"start\":38699},{\"attributes\":{\"id\":\"formula_11\"},\"end\":39225,\"start\":39200},{\"attributes\":{\"id\":\"formula_12\"},\"end\":39416,\"start\":39386},{\"attributes\":{\"id\":\"formula_13\"},\"end\":39626,\"start\":39586},{\"attributes\":{\"id\":\"formula_14\"},\"end\":39947,\"start\":39905},{\"attributes\":{\"id\":\"formula_15\"},\"end\":40807,\"start\":40746},{\"attributes\":{\"id\":\"formula_16\"},\"end\":40975,\"start\":40924},{\"attributes\":{\"id\":\"formula_17\"},\"end\":41227,\"start\":41174}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":12067,\"start\":12060},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":42329,\"start\":42321}]", "section_header": "[{\"end\":1716,\"start\":1701},{\"end\":8232,\"start\":8203},{\"end\":12599,\"start\":12574},{\"end\":14133,\"start\":14106},{\"end\":14980,\"start\":14956},{\"end\":15459,\"start\":15436},{\"end\":16748,\"start\":16726},{\"end\":20124,\"start\":20099},{\"end\":21943,\"start\":21894},{\"end\":23524,\"start\":23500},{\"end\":23677,\"start\":23645},{\"end\":24552,\"start\":24533},{\"end\":26046,\"start\":26019},{\"end\":28396,\"start\":28369},{\"end\":29775,\"start\":29739},{\"end\":30862,\"start\":30818},{\"end\":32213,\"start\":32193},{\"end\":33313,\"start\":33296},{\"end\":35023,\"start\":34964},{\"end\":35574,\"start\":35564},{\"end\":37741,\"start\":37722},{\"end\":38518,\"start\":38504},{\"end\":38904,\"start\":38873},{\"end\":41173,\"start\":41138},{\"end\":41379,\"start\":41337},{\"end\":41920,\"start\":41907},{\"end\":42487,\"start\":42460},{\"end\":43766,\"start\":43720},{\"end\":49161,\"start\":49146},{\"end\":49636,\"start\":49628},{\"end\":50403,\"start\":50395},{\"end\":50465,\"start\":50457},{\"end\":50615,\"start\":50607},{\"end\":50702,\"start\":50686},{\"end\":51031,\"start\":51028},{\"end\":51200,\"start\":51192},{\"end\":51669,\"start\":51652},{\"end\":52116,\"start\":52107},{\"end\":53180,\"start\":53170},{\"end\":54005,\"start\":53994},{\"end\":57009,\"start\":56999}]", "table": "[{\"end\":53168,\"start\":52142},{\"end\":53992,\"start\":53224},{\"end\":56997,\"start\":54073},{\"end\":59386,\"start\":57087}]", "figure_caption": "[{\"end\":49626,\"start\":49382},{\"end\":49689,\"start\":49638},{\"end\":50393,\"start\":49692},{\"end\":50455,\"start\":50405},{\"end\":50605,\"start\":50467},{\"end\":50684,\"start\":50617},{\"end\":51026,\"start\":50705},{\"end\":51190,\"start\":51033},{\"end\":51580,\"start\":51202},{\"end\":51650,\"start\":51583},{\"end\":51979,\"start\":51673},{\"end\":52105,\"start\":51982},{\"end\":52142,\"start\":52118},{\"end\":53224,\"start\":53183},{\"end\":54073,\"start\":54009},{\"end\":57087,\"start\":57012}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":8776,\"start\":8769},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":20816,\"start\":20805},{\"end\":22771,\"start\":22764},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":22933,\"start\":22925},{\"end\":23432,\"start\":23426},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":40921,\"start\":40914},{\"end\":42432,\"start\":42426}]", "bib_author_first_name": "[{\"end\":59523,\"start\":59522},{\"end\":59527,\"start\":59524},{\"end\":59555,\"start\":59551},{\"end\":59952,\"start\":59951},{\"end\":59968,\"start\":59967},{\"end\":59980,\"start\":59979},{\"end\":59989,\"start\":59988},{\"end\":60000,\"start\":59999},{\"end\":60011,\"start\":60010},{\"end\":60312,\"start\":60311},{\"end\":60319,\"start\":60318},{\"end\":60321,\"start\":60320},{\"end\":60676,\"start\":60675},{\"end\":60685,\"start\":60684},{\"end\":60696,\"start\":60695},{\"end\":61017,\"start\":61016},{\"end\":61026,\"start\":61025},{\"end\":61037,\"start\":61036},{\"end\":61384,\"start\":61383},{\"end\":61393,\"start\":61392},{\"end\":61806,\"start\":61802},{\"end\":61817,\"start\":61812},{\"end\":61828,\"start\":61824},{\"end\":62183,\"start\":62182},{\"end\":62189,\"start\":62188},{\"end\":62198,\"start\":62197},{\"end\":62207,\"start\":62206},{\"end\":62564,\"start\":62563},{\"end\":62566,\"start\":62565},{\"end\":62582,\"start\":62581},{\"end\":62584,\"start\":62583},{\"end\":62596,\"start\":62595},{\"end\":62610,\"start\":62609},{\"end\":62980,\"start\":62979},{\"end\":62982,\"start\":62981},{\"end\":62993,\"start\":62992},{\"end\":63004,\"start\":63003},{\"end\":63449,\"start\":63448},{\"end\":63455,\"start\":63454},{\"end\":63770,\"start\":63769},{\"end\":63778,\"start\":63777},{\"end\":63780,\"start\":63779},{\"end\":63787,\"start\":63786},{\"end\":63795,\"start\":63794},{\"end\":63957,\"start\":63956},{\"end\":63959,\"start\":63958},{\"end\":63969,\"start\":63968},{\"end\":63971,\"start\":63970},{\"end\":63978,\"start\":63977},{\"end\":63980,\"start\":63979},{\"end\":64259,\"start\":64258},{\"end\":64268,\"start\":64267},{\"end\":64629,\"start\":64628},{\"end\":64631,\"start\":64630},{\"end\":64639,\"start\":64638},{\"end\":64932,\"start\":64931},{\"end\":64944,\"start\":64943},{\"end\":65427,\"start\":65426},{\"end\":65435,\"start\":65434},{\"end\":65444,\"start\":65443},{\"end\":65454,\"start\":65453},{\"end\":65456,\"start\":65455},{\"end\":65468,\"start\":65467},{\"end\":65694,\"start\":65693},{\"end\":65703,\"start\":65702},{\"end\":65711,\"start\":65710},{\"end\":65719,\"start\":65718},{\"end\":65726,\"start\":65725},{\"end\":66116,\"start\":66115},{\"end\":66122,\"start\":66121},{\"end\":66131,\"start\":66130},{\"end\":66495,\"start\":66494},{\"end\":66502,\"start\":66501},{\"end\":66510,\"start\":66509},{\"end\":66518,\"start\":66517},{\"end\":66714,\"start\":66713},{\"end\":66721,\"start\":66720},{\"end\":66729,\"start\":66728},{\"end\":66736,\"start\":66735},{\"end\":66743,\"start\":66742},{\"end\":66751,\"start\":66750},{\"end\":66758,\"start\":66757},{\"end\":67081,\"start\":67080},{\"end\":67090,\"start\":67089},{\"end\":67099,\"start\":67098},{\"end\":67105,\"start\":67104},{\"end\":67114,\"start\":67113},{\"end\":67120,\"start\":67119},{\"end\":67128,\"start\":67127},{\"end\":67136,\"start\":67135},{\"end\":67512,\"start\":67511},{\"end\":67520,\"start\":67519},{\"end\":67827,\"start\":67826},{\"end\":67839,\"start\":67838},{\"end\":68232,\"start\":68231},{\"end\":68243,\"start\":68242},{\"end\":68250,\"start\":68249},{\"end\":68257,\"start\":68256},{\"end\":68269,\"start\":68268},{\"end\":68271,\"start\":68270},{\"end\":68279,\"start\":68278},{\"end\":68291,\"start\":68290},{\"end\":68300,\"start\":68299},{\"end\":68313,\"start\":68312},{\"end\":68687,\"start\":68686},{\"end\":68697,\"start\":68696},{\"end\":68699,\"start\":68698},{\"end\":68710,\"start\":68709},{\"end\":68712,\"start\":68711},{\"end\":68724,\"start\":68723},{\"end\":69116,\"start\":69115},{\"end\":69134,\"start\":69133},{\"end\":69143,\"start\":69142},{\"end\":69424,\"start\":69423},{\"end\":69439,\"start\":69438},{\"end\":69450,\"start\":69449},{\"end\":69689,\"start\":69688},{\"end\":69698,\"start\":69697},{\"end\":69704,\"start\":69703},{\"end\":69712,\"start\":69711},{\"end\":70072,\"start\":70071},{\"end\":70076,\"start\":70073},{\"end\":70085,\"start\":70084},{\"end\":70087,\"start\":70086},{\"end\":70097,\"start\":70096},{\"end\":70487,\"start\":70486},{\"end\":70493,\"start\":70488},{\"end\":70497,\"start\":70496},{\"end\":70507,\"start\":70506},{\"end\":70714,\"start\":70713},{\"end\":70716,\"start\":70715},{\"end\":70727,\"start\":70726},{\"end\":70729,\"start\":70728},{\"end\":70988,\"start\":70987},{\"end\":70990,\"start\":70989},{\"end\":71315,\"start\":71314},{\"end\":71324,\"start\":71323},{\"end\":71335,\"start\":71334},{\"end\":71614,\"start\":71613},{\"end\":71621,\"start\":71620},{\"end\":71628,\"start\":71627},{\"end\":71635,\"start\":71634},{\"end\":71643,\"start\":71642},{\"end\":71857,\"start\":71856},{\"end\":71859,\"start\":71858},{\"end\":71868,\"start\":71867},{\"end\":72225,\"start\":72224},{\"end\":72232,\"start\":72231},{\"end\":72238,\"start\":72237},{\"end\":72556,\"start\":72555},{\"end\":72563,\"start\":72562},{\"end\":72573,\"start\":72569},{\"end\":72575,\"start\":72574},{\"end\":72792,\"start\":72791},{\"end\":72960,\"start\":72959},{\"end\":72970,\"start\":72969},{\"end\":72972,\"start\":72971},{\"end\":72984,\"start\":72983},{\"end\":73288,\"start\":73287},{\"end\":73295,\"start\":73294},{\"end\":73304,\"start\":73303},{\"end\":73313,\"start\":73312},{\"end\":73319,\"start\":73318},{\"end\":73328,\"start\":73327},{\"end\":73595,\"start\":73594},{\"end\":73603,\"start\":73602},{\"end\":73605,\"start\":73604},{\"end\":73612,\"start\":73611},{\"end\":73620,\"start\":73619},{\"end\":74002,\"start\":74001},{\"end\":74004,\"start\":74003},{\"end\":74015,\"start\":74011},{\"end\":74017,\"start\":74016},{\"end\":74026,\"start\":74025},{\"end\":74393,\"start\":74392},{\"end\":74403,\"start\":74402},{\"end\":74414,\"start\":74413},{\"end\":74425,\"start\":74424},{\"end\":74791,\"start\":74790},{\"end\":74801,\"start\":74800},{\"end\":74811,\"start\":74810},{\"end\":74821,\"start\":74820},{\"end\":74830,\"start\":74829},{\"end\":74832,\"start\":74831},{\"end\":74846,\"start\":74845},{\"end\":74848,\"start\":74847},{\"end\":74859,\"start\":74858},{\"end\":75176,\"start\":75175},{\"end\":75178,\"start\":75177},{\"end\":75189,\"start\":75188},{\"end\":75200,\"start\":75199},{\"end\":75202,\"start\":75201},{\"end\":75434,\"start\":75433},{\"end\":75442,\"start\":75441},{\"end\":75448,\"start\":75447},{\"end\":75972,\"start\":75971},{\"end\":75980,\"start\":75979},{\"end\":75987,\"start\":75986},{\"end\":75994,\"start\":75993},{\"end\":76312,\"start\":76311},{\"end\":76337,\"start\":76336},{\"end\":76354,\"start\":76353},{\"end\":76370,\"start\":76369},{\"end\":76390,\"start\":76389},{\"end\":76392,\"start\":76391},{\"end\":76782,\"start\":76778},{\"end\":76794,\"start\":76790},{\"end\":76801,\"start\":76800},{\"end\":76803,\"start\":76802},{\"end\":77125,\"start\":77124},{\"end\":77132,\"start\":77131},{\"end\":77139,\"start\":77138},{\"end\":77145,\"start\":77144},{\"end\":77153,\"start\":77152},{\"end\":77160,\"start\":77159},{\"end\":77166,\"start\":77165},{\"end\":77529,\"start\":77528},{\"end\":77536,\"start\":77535},{\"end\":77544,\"start\":77543},{\"end\":77550,\"start\":77549},{\"end\":77556,\"start\":77555},{\"end\":77562,\"start\":77561},{\"end\":77963,\"start\":77962},{\"end\":77971,\"start\":77970},{\"end\":77977,\"start\":77976},{\"end\":77984,\"start\":77983},{\"end\":77991,\"start\":77990},{\"end\":78332,\"start\":78331},{\"end\":78341,\"start\":78340},{\"end\":78349,\"start\":78348},{\"end\":78358,\"start\":78357},{\"end\":78366,\"start\":78365},{\"end\":78373,\"start\":78372},{\"end\":78375,\"start\":78374},{\"end\":78387,\"start\":78386},{\"end\":78803,\"start\":78802},{\"end\":78810,\"start\":78809},{\"end\":78818,\"start\":78817},{\"end\":79277,\"start\":79276},{\"end\":79285,\"start\":79284},{\"end\":79294,\"start\":79293},{\"end\":79302,\"start\":79301},{\"end\":79310,\"start\":79309},{\"end\":79316,\"start\":79315},{\"end\":79503,\"start\":79502},{\"end\":79509,\"start\":79508},{\"end\":79516,\"start\":79515},{\"end\":79524,\"start\":79523},{\"end\":79888,\"start\":79887},{\"end\":79895,\"start\":79894},{\"end\":79901,\"start\":79900},{\"end\":79907,\"start\":79906},{\"end\":79915,\"start\":79914},{\"end\":79923,\"start\":79922},{\"end\":80320,\"start\":80319},{\"end\":80326,\"start\":80325},{\"end\":80337,\"start\":80336},{\"end\":80568,\"start\":80567},{\"end\":80577,\"start\":80576},{\"end\":80584,\"start\":80583},{\"end\":80923,\"start\":80922},{\"end\":80930,\"start\":80929},{\"end\":80936,\"start\":80935},{\"end\":80943,\"start\":80942},{\"end\":80952,\"start\":80951},{\"end\":80959,\"start\":80958},{\"end\":81214,\"start\":81213},{\"end\":81229,\"start\":81228},{\"end\":81233,\"start\":81230},{\"end\":81248,\"start\":81247},{\"end\":81250,\"start\":81249},{\"end\":81632,\"start\":81631},{\"end\":81639,\"start\":81638},{\"end\":81648,\"start\":81647},{\"end\":82113,\"start\":82112},{\"end\":82115,\"start\":82114},{\"end\":82126,\"start\":82125},{\"end\":82128,\"start\":82127},{\"end\":82577,\"start\":82576},{\"end\":82584,\"start\":82583},{\"end\":82591,\"start\":82590},{\"end\":82599,\"start\":82598},{\"end\":82601,\"start\":82600},{\"end\":82939,\"start\":82938},{\"end\":83192,\"start\":83191},{\"end\":83199,\"start\":83198},{\"end\":83207,\"start\":83206},{\"end\":83213,\"start\":83212},{\"end\":83575,\"start\":83574},{\"end\":83582,\"start\":83581},{\"end\":83590,\"start\":83589},{\"end\":83921,\"start\":83920},{\"end\":83928,\"start\":83927},{\"end\":83935,\"start\":83934},{\"end\":83944,\"start\":83943},{\"end\":84203,\"start\":84202},{\"end\":84211,\"start\":84210},{\"end\":84217,\"start\":84216},{\"end\":84223,\"start\":84222},{\"end\":84229,\"start\":84228},{\"end\":84238,\"start\":84237},{\"end\":84742,\"start\":84741},{\"end\":84750,\"start\":84749},{\"end\":84761,\"start\":84757},{\"end\":84980,\"start\":84979},{\"end\":84986,\"start\":84985},{\"end\":84995,\"start\":84994},{\"end\":85002,\"start\":85001},{\"end\":85258,\"start\":85257},{\"end\":85269,\"start\":85268},{\"end\":85279,\"start\":85278},{\"end\":85286,\"start\":85285},{\"end\":85302,\"start\":85298},{\"end\":85707,\"start\":85706},{\"end\":85719,\"start\":85718},{\"end\":85727,\"start\":85726},{\"end\":85729,\"start\":85728},{\"end\":85940,\"start\":85939},{\"end\":85949,\"start\":85948},{\"end\":85956,\"start\":85955},{\"end\":85964,\"start\":85963},{\"end\":85971,\"start\":85970},{\"end\":85979,\"start\":85978},{\"end\":85986,\"start\":85985},{\"end\":86322,\"start\":86321},{\"end\":86330,\"start\":86329},{\"end\":86338,\"start\":86337},{\"end\":86346,\"start\":86345},{\"end\":86353,\"start\":86352},{\"end\":86361,\"start\":86360},{\"end\":86367,\"start\":86366},{\"end\":86373,\"start\":86372},{\"end\":86621,\"start\":86620},{\"end\":86627,\"start\":86626},{\"end\":86907,\"start\":86906},{\"end\":86923,\"start\":86922},{\"end\":86932,\"start\":86931},{\"end\":86941,\"start\":86940},{\"end\":87329,\"start\":87328},{\"end\":87337,\"start\":87336},{\"end\":87345,\"start\":87344},{\"end\":87358,\"start\":87354},{\"end\":87364,\"start\":87363},{\"end\":87370,\"start\":87369},{\"end\":87691,\"start\":87690},{\"end\":87698,\"start\":87697},{\"end\":87710,\"start\":87709},{\"end\":87712,\"start\":87711},{\"end\":88027,\"start\":88026},{\"end\":88035,\"start\":88034},{\"end\":88041,\"start\":88040},{\"end\":88043,\"start\":88042},{\"end\":88194,\"start\":88193},{\"end\":88205,\"start\":88204},{\"end\":88216,\"start\":88215},{\"end\":88226,\"start\":88225},{\"end\":88239,\"start\":88238},{\"end\":88248,\"start\":88247},{\"end\":88250,\"start\":88249},{\"end\":88259,\"start\":88258},{\"end\":88269,\"start\":88268},{\"end\":88882,\"start\":88881},{\"end\":88897,\"start\":88896},{\"end\":88906,\"start\":88905},{\"end\":88920,\"start\":88919},{\"end\":88935,\"start\":88934},{\"end\":88943,\"start\":88942},{\"end\":88958,\"start\":88957},{\"end\":88970,\"start\":88969},{\"end\":88982,\"start\":88981},{\"end\":88993,\"start\":88992},{\"end\":89002,\"start\":89001},{\"end\":89015,\"start\":89014},{\"end\":89385,\"start\":89384},{\"end\":89396,\"start\":89395},{\"end\":89404,\"start\":89403},{\"end\":89413,\"start\":89412},{\"end\":89422,\"start\":89421},{\"end\":89438,\"start\":89437},{\"end\":89663,\"start\":89662},{\"end\":89673,\"start\":89672},{\"end\":89682,\"start\":89681},{\"end\":89694,\"start\":89693},{\"end\":89705,\"start\":89704},{\"end\":89717,\"start\":89716},{\"end\":89972,\"start\":89971},{\"end\":89979,\"start\":89978},{\"end\":89985,\"start\":89984},{\"end\":89991,\"start\":89990},{\"end\":89997,\"start\":89996},{\"end\":90005,\"start\":90004},{\"end\":90263,\"start\":90262},{\"end\":90269,\"start\":90268},{\"end\":90279,\"start\":90278},{\"end\":90286,\"start\":90285},{\"end\":90610,\"start\":90609},{\"end\":90620,\"start\":90619},{\"end\":90628,\"start\":90627},{\"end\":90630,\"start\":90629},{\"end\":90846,\"start\":90845},{\"end\":90853,\"start\":90852},{\"end\":90861,\"start\":90860},{\"end\":91106,\"start\":91105},{\"end\":91114,\"start\":91113},{\"end\":91121,\"start\":91120},{\"end\":91328,\"start\":91327},{\"end\":91335,\"start\":91334},{\"end\":91342,\"start\":91341},{\"end\":91352,\"start\":91351},{\"end\":91354,\"start\":91353},{\"end\":91364,\"start\":91363},{\"end\":91375,\"start\":91374},{\"end\":91377,\"start\":91376},{\"end\":91649,\"start\":91648},{\"end\":91651,\"start\":91650},{\"end\":91985,\"start\":91984},{\"end\":91994,\"start\":91993},{\"end\":92000,\"start\":91999},{\"end\":92203,\"start\":92202},{\"end\":92210,\"start\":92209},{\"end\":92218,\"start\":92217},{\"end\":92226,\"start\":92225},{\"end\":92233,\"start\":92232},{\"end\":92240,\"start\":92239},{\"end\":92247,\"start\":92246},{\"end\":92493,\"start\":92492},{\"end\":92502,\"start\":92501},{\"end\":92510,\"start\":92509},{\"end\":92516,\"start\":92515},{\"end\":92524,\"start\":92523},{\"end\":92797,\"start\":92796},{\"end\":92804,\"start\":92803},{\"end\":92812,\"start\":92811},{\"end\":92820,\"start\":92819},{\"end\":92829,\"start\":92828},{\"end\":92836,\"start\":92835},{\"end\":92843,\"start\":92842},{\"end\":92850,\"start\":92849},{\"end\":93103,\"start\":93102},{\"end\":93110,\"start\":93109},{\"end\":93116,\"start\":93115},{\"end\":93123,\"start\":93122},{\"end\":93132,\"start\":93131},{\"end\":93139,\"start\":93138},{\"end\":93410,\"start\":93409},{\"end\":93416,\"start\":93415},{\"end\":93423,\"start\":93422},{\"end\":93712,\"start\":93711},{\"end\":93720,\"start\":93719},{\"end\":93981,\"start\":93980},{\"end\":93988,\"start\":93987},{\"end\":93994,\"start\":93993},{\"end\":94000,\"start\":93999},{\"end\":94008,\"start\":94007},{\"end\":94231,\"start\":94230},{\"end\":94239,\"start\":94238},{\"end\":94246,\"start\":94245},{\"end\":94253,\"start\":94252},{\"end\":94607,\"start\":94606},{\"end\":94614,\"start\":94613},{\"end\":94631,\"start\":94630},{\"end\":94633,\"start\":94632},{\"end\":94946,\"start\":94945},{\"end\":94956,\"start\":94955},{\"end\":94969,\"start\":94968},{\"end\":94982,\"start\":94981},{\"end\":95497,\"start\":95496},{\"end\":95504,\"start\":95503},{\"end\":95511,\"start\":95510},{\"end\":95519,\"start\":95518},{\"end\":95842,\"start\":95841},{\"end\":95850,\"start\":95849},{\"end\":95857,\"start\":95856},{\"end\":95868,\"start\":95867},{\"end\":96006,\"start\":96005},{\"end\":96031,\"start\":96030},{\"end\":96048,\"start\":96047},{\"end\":96050,\"start\":96049},{\"end\":96066,\"start\":96065},{\"end\":96086,\"start\":96085},{\"end\":96363,\"start\":96362},{\"end\":96370,\"start\":96369},{\"end\":96372,\"start\":96371}]", "bib_author_last_name": "[{\"end\":59549,\"start\":59528},{\"end\":59570,\"start\":59556},{\"end\":59965,\"start\":59953},{\"end\":59977,\"start\":59969},{\"end\":59986,\"start\":59981},{\"end\":59997,\"start\":59990},{\"end\":60008,\"start\":60001},{\"end\":60019,\"start\":60012},{\"end\":60316,\"start\":60313},{\"end\":60327,\"start\":60322},{\"end\":60682,\"start\":60677},{\"end\":60693,\"start\":60686},{\"end\":60701,\"start\":60697},{\"end\":61023,\"start\":61018},{\"end\":61034,\"start\":61027},{\"end\":61045,\"start\":61038},{\"end\":61390,\"start\":61385},{\"end\":61399,\"start\":61394},{\"end\":61810,\"start\":61807},{\"end\":61822,\"start\":61818},{\"end\":61833,\"start\":61829},{\"end\":62186,\"start\":62184},{\"end\":62195,\"start\":62190},{\"end\":62204,\"start\":62199},{\"end\":62211,\"start\":62208},{\"end\":62579,\"start\":62567},{\"end\":62593,\"start\":62585},{\"end\":62607,\"start\":62597},{\"end\":62618,\"start\":62611},{\"end\":62990,\"start\":62983},{\"end\":63001,\"start\":62994},{\"end\":63012,\"start\":63005},{\"end\":63452,\"start\":63450},{\"end\":63463,\"start\":63456},{\"end\":63775,\"start\":63771},{\"end\":63784,\"start\":63781},{\"end\":63792,\"start\":63788},{\"end\":63801,\"start\":63796},{\"end\":63966,\"start\":63960},{\"end\":63975,\"start\":63972},{\"end\":63989,\"start\":63981},{\"end\":64265,\"start\":64260},{\"end\":64275,\"start\":64269},{\"end\":64636,\"start\":64632},{\"end\":64651,\"start\":64640},{\"end\":64941,\"start\":64933},{\"end\":64951,\"start\":64945},{\"end\":65432,\"start\":65428},{\"end\":65441,\"start\":65436},{\"end\":65451,\"start\":65445},{\"end\":65465,\"start\":65457},{\"end\":65472,\"start\":65469},{\"end\":65700,\"start\":65695},{\"end\":65708,\"start\":65704},{\"end\":65716,\"start\":65712},{\"end\":65723,\"start\":65720},{\"end\":65729,\"start\":65727},{\"end\":66119,\"start\":66117},{\"end\":66128,\"start\":66123},{\"end\":66136,\"start\":66132},{\"end\":66499,\"start\":66496},{\"end\":66507,\"start\":66503},{\"end\":66515,\"start\":66511},{\"end\":66521,\"start\":66519},{\"end\":66718,\"start\":66715},{\"end\":66726,\"start\":66722},{\"end\":66733,\"start\":66730},{\"end\":66740,\"start\":66737},{\"end\":66748,\"start\":66744},{\"end\":66755,\"start\":66752},{\"end\":66763,\"start\":66759},{\"end\":67087,\"start\":67082},{\"end\":67096,\"start\":67091},{\"end\":67102,\"start\":67100},{\"end\":67111,\"start\":67106},{\"end\":67117,\"start\":67115},{\"end\":67125,\"start\":67121},{\"end\":67133,\"start\":67129},{\"end\":67141,\"start\":67137},{\"end\":67517,\"start\":67513},{\"end\":67528,\"start\":67521},{\"end\":67836,\"start\":67828},{\"end\":67849,\"start\":67840},{\"end\":68240,\"start\":68233},{\"end\":68247,\"start\":68244},{\"end\":68254,\"start\":68251},{\"end\":68266,\"start\":68258},{\"end\":68276,\"start\":68272},{\"end\":68288,\"start\":68280},{\"end\":68297,\"start\":68292},{\"end\":68310,\"start\":68301},{\"end\":68324,\"start\":68314},{\"end\":68694,\"start\":68688},{\"end\":68707,\"start\":68700},{\"end\":68721,\"start\":68713},{\"end\":68732,\"start\":68725},{\"end\":69131,\"start\":69117},{\"end\":69140,\"start\":69135},{\"end\":69151,\"start\":69144},{\"end\":69436,\"start\":69425},{\"end\":69447,\"start\":69440},{\"end\":69455,\"start\":69451},{\"end\":69695,\"start\":69690},{\"end\":69701,\"start\":69699},{\"end\":69709,\"start\":69705},{\"end\":69717,\"start\":69713},{\"end\":70082,\"start\":70077},{\"end\":70094,\"start\":70088},{\"end\":70105,\"start\":70098},{\"end\":70366,\"start\":70358},{\"end\":70504,\"start\":70498},{\"end\":70513,\"start\":70508},{\"end\":70724,\"start\":70717},{\"end\":70735,\"start\":70730},{\"end\":70999,\"start\":70991},{\"end\":71321,\"start\":71316},{\"end\":71332,\"start\":71325},{\"end\":71339,\"start\":71336},{\"end\":71618,\"start\":71615},{\"end\":71625,\"start\":71622},{\"end\":71632,\"start\":71629},{\"end\":71640,\"start\":71636},{\"end\":71648,\"start\":71644},{\"end\":71865,\"start\":71860},{\"end\":71875,\"start\":71869},{\"end\":72229,\"start\":72226},{\"end\":72235,\"start\":72233},{\"end\":72244,\"start\":72239},{\"end\":72560,\"start\":72557},{\"end\":72567,\"start\":72564},{\"end\":72580,\"start\":72576},{\"end\":72801,\"start\":72793},{\"end\":72967,\"start\":72961},{\"end\":72981,\"start\":72973},{\"end\":72993,\"start\":72985},{\"end\":73292,\"start\":73289},{\"end\":73301,\"start\":73296},{\"end\":73310,\"start\":73305},{\"end\":73316,\"start\":73314},{\"end\":73325,\"start\":73320},{\"end\":73333,\"start\":73329},{\"end\":73600,\"start\":73596},{\"end\":73609,\"start\":73606},{\"end\":73617,\"start\":73613},{\"end\":73626,\"start\":73621},{\"end\":74009,\"start\":74005},{\"end\":74023,\"start\":74018},{\"end\":74038,\"start\":74027},{\"end\":74400,\"start\":74394},{\"end\":74411,\"start\":74404},{\"end\":74422,\"start\":74415},{\"end\":74430,\"start\":74426},{\"end\":74798,\"start\":74792},{\"end\":74808,\"start\":74802},{\"end\":74818,\"start\":74812},{\"end\":74827,\"start\":74822},{\"end\":74843,\"start\":74833},{\"end\":74856,\"start\":74849},{\"end\":74864,\"start\":74860},{\"end\":75186,\"start\":75179},{\"end\":75197,\"start\":75190},{\"end\":75208,\"start\":75203},{\"end\":75439,\"start\":75435},{\"end\":75445,\"start\":75443},{\"end\":75452,\"start\":75449},{\"end\":75977,\"start\":75973},{\"end\":75984,\"start\":75981},{\"end\":75991,\"start\":75988},{\"end\":75997,\"start\":75995},{\"end\":76334,\"start\":76313},{\"end\":76351,\"start\":76338},{\"end\":76367,\"start\":76355},{\"end\":76387,\"start\":76371},{\"end\":76398,\"start\":76393},{\"end\":76788,\"start\":76783},{\"end\":76798,\"start\":76795},{\"end\":76807,\"start\":76804},{\"end\":77129,\"start\":77126},{\"end\":77136,\"start\":77133},{\"end\":77142,\"start\":77140},{\"end\":77150,\"start\":77146},{\"end\":77157,\"start\":77154},{\"end\":77163,\"start\":77161},{\"end\":77171,\"start\":77167},{\"end\":77533,\"start\":77530},{\"end\":77541,\"start\":77537},{\"end\":77547,\"start\":77545},{\"end\":77553,\"start\":77551},{\"end\":77559,\"start\":77557},{\"end\":77566,\"start\":77563},{\"end\":77968,\"start\":77964},{\"end\":77974,\"start\":77972},{\"end\":77981,\"start\":77978},{\"end\":77988,\"start\":77985},{\"end\":77997,\"start\":77992},{\"end\":78338,\"start\":78333},{\"end\":78346,\"start\":78342},{\"end\":78355,\"start\":78350},{\"end\":78363,\"start\":78359},{\"end\":78370,\"start\":78367},{\"end\":78384,\"start\":78376},{\"end\":78392,\"start\":78388},{\"end\":78807,\"start\":78804},{\"end\":78815,\"start\":78811},{\"end\":78822,\"start\":78819},{\"end\":79282,\"start\":79278},{\"end\":79291,\"start\":79286},{\"end\":79299,\"start\":79295},{\"end\":79307,\"start\":79303},{\"end\":79313,\"start\":79311},{\"end\":79319,\"start\":79317},{\"end\":79506,\"start\":79504},{\"end\":79513,\"start\":79510},{\"end\":79521,\"start\":79517},{\"end\":79529,\"start\":79525},{\"end\":79892,\"start\":79889},{\"end\":79898,\"start\":79896},{\"end\":79904,\"start\":79902},{\"end\":79912,\"start\":79908},{\"end\":79920,\"start\":79916},{\"end\":79926,\"start\":79924},{\"end\":80323,\"start\":80321},{\"end\":80334,\"start\":80327},{\"end\":80342,\"start\":80338},{\"end\":80574,\"start\":80569},{\"end\":80581,\"start\":80578},{\"end\":80589,\"start\":80585},{\"end\":80927,\"start\":80924},{\"end\":80933,\"start\":80931},{\"end\":80940,\"start\":80937},{\"end\":80949,\"start\":80944},{\"end\":80956,\"start\":80953},{\"end\":80964,\"start\":80960},{\"end\":81226,\"start\":81215},{\"end\":81245,\"start\":81234},{\"end\":81255,\"start\":81251},{\"end\":81636,\"start\":81633},{\"end\":81645,\"start\":81640},{\"end\":81653,\"start\":81649},{\"end\":82123,\"start\":82116},{\"end\":82134,\"start\":82129},{\"end\":82581,\"start\":82578},{\"end\":82588,\"start\":82585},{\"end\":82596,\"start\":82592},{\"end\":82611,\"start\":82602},{\"end\":82948,\"start\":82940},{\"end\":83196,\"start\":83193},{\"end\":83204,\"start\":83200},{\"end\":83210,\"start\":83208},{\"end\":83218,\"start\":83214},{\"end\":83579,\"start\":83576},{\"end\":83587,\"start\":83583},{\"end\":83593,\"start\":83591},{\"end\":83925,\"start\":83922},{\"end\":83932,\"start\":83929},{\"end\":83941,\"start\":83936},{\"end\":83948,\"start\":83945},{\"end\":84208,\"start\":84204},{\"end\":84214,\"start\":84212},{\"end\":84220,\"start\":84218},{\"end\":84226,\"start\":84224},{\"end\":84235,\"start\":84230},{\"end\":84243,\"start\":84239},{\"end\":84747,\"start\":84743},{\"end\":84755,\"start\":84751},{\"end\":84766,\"start\":84762},{\"end\":84983,\"start\":84981},{\"end\":84992,\"start\":84987},{\"end\":84999,\"start\":84996},{\"end\":85006,\"start\":85003},{\"end\":85266,\"start\":85259},{\"end\":85276,\"start\":85270},{\"end\":85283,\"start\":85280},{\"end\":85296,\"start\":85287},{\"end\":85307,\"start\":85303},{\"end\":85716,\"start\":85708},{\"end\":85724,\"start\":85720},{\"end\":85734,\"start\":85730},{\"end\":85946,\"start\":85941},{\"end\":85953,\"start\":85950},{\"end\":85961,\"start\":85957},{\"end\":85968,\"start\":85965},{\"end\":85976,\"start\":85972},{\"end\":85983,\"start\":85980},{\"end\":85991,\"start\":85987},{\"end\":86327,\"start\":86323},{\"end\":86335,\"start\":86331},{\"end\":86343,\"start\":86339},{\"end\":86350,\"start\":86347},{\"end\":86358,\"start\":86354},{\"end\":86364,\"start\":86362},{\"end\":86370,\"start\":86368},{\"end\":86376,\"start\":86374},{\"end\":86624,\"start\":86622},{\"end\":86632,\"start\":86628},{\"end\":86920,\"start\":86908},{\"end\":86929,\"start\":86924},{\"end\":86938,\"start\":86933},{\"end\":86952,\"start\":86942},{\"end\":87334,\"start\":87330},{\"end\":87342,\"start\":87338},{\"end\":87352,\"start\":87346},{\"end\":87361,\"start\":87359},{\"end\":87367,\"start\":87365},{\"end\":87378,\"start\":87371},{\"end\":87695,\"start\":87692},{\"end\":87707,\"start\":87699},{\"end\":87716,\"start\":87713},{\"end\":88032,\"start\":88028},{\"end\":88038,\"start\":88036},{\"end\":88047,\"start\":88044},{\"end\":88202,\"start\":88195},{\"end\":88213,\"start\":88206},{\"end\":88223,\"start\":88217},{\"end\":88236,\"start\":88227},{\"end\":88245,\"start\":88240},{\"end\":88256,\"start\":88251},{\"end\":88266,\"start\":88260},{\"end\":88280,\"start\":88270},{\"end\":88894,\"start\":88883},{\"end\":88903,\"start\":88898},{\"end\":88917,\"start\":88907},{\"end\":88932,\"start\":88921},{\"end\":88940,\"start\":88936},{\"end\":88955,\"start\":88944},{\"end\":88967,\"start\":88959},{\"end\":88979,\"start\":88971},{\"end\":88990,\"start\":88983},{\"end\":88999,\"start\":88994},{\"end\":89012,\"start\":89003},{\"end\":89023,\"start\":89016},{\"end\":89393,\"start\":89386},{\"end\":89401,\"start\":89397},{\"end\":89410,\"start\":89405},{\"end\":89419,\"start\":89414},{\"end\":89435,\"start\":89423},{\"end\":89670,\"start\":89664},{\"end\":89679,\"start\":89674},{\"end\":89691,\"start\":89683},{\"end\":89702,\"start\":89695},{\"end\":89714,\"start\":89706},{\"end\":89727,\"start\":89718},{\"end\":89976,\"start\":89973},{\"end\":89982,\"start\":89980},{\"end\":89988,\"start\":89986},{\"end\":89994,\"start\":89992},{\"end\":90002,\"start\":89998},{\"end\":90009,\"start\":90006},{\"end\":90266,\"start\":90264},{\"end\":90276,\"start\":90270},{\"end\":90283,\"start\":90280},{\"end\":90291,\"start\":90287},{\"end\":90617,\"start\":90611},{\"end\":90625,\"start\":90621},{\"end\":90637,\"start\":90631},{\"end\":90850,\"start\":90847},{\"end\":90858,\"start\":90854},{\"end\":90864,\"start\":90862},{\"end\":91111,\"start\":91107},{\"end\":91118,\"start\":91115},{\"end\":91126,\"start\":91122},{\"end\":91332,\"start\":91329},{\"end\":91339,\"start\":91336},{\"end\":91349,\"start\":91343},{\"end\":91361,\"start\":91355},{\"end\":91372,\"start\":91365},{\"end\":91382,\"start\":91378},{\"end\":91654,\"start\":91652},{\"end\":91991,\"start\":91986},{\"end\":91997,\"start\":91995},{\"end\":92004,\"start\":92001},{\"end\":92207,\"start\":92204},{\"end\":92215,\"start\":92211},{\"end\":92223,\"start\":92219},{\"end\":92230,\"start\":92227},{\"end\":92237,\"start\":92234},{\"end\":92244,\"start\":92241},{\"end\":92252,\"start\":92248},{\"end\":92499,\"start\":92494},{\"end\":92507,\"start\":92503},{\"end\":92513,\"start\":92511},{\"end\":92521,\"start\":92517},{\"end\":92528,\"start\":92525},{\"end\":92801,\"start\":92798},{\"end\":92809,\"start\":92805},{\"end\":92817,\"start\":92813},{\"end\":92826,\"start\":92821},{\"end\":92833,\"start\":92830},{\"end\":92840,\"start\":92837},{\"end\":92847,\"start\":92844},{\"end\":92855,\"start\":92851},{\"end\":93107,\"start\":93104},{\"end\":93113,\"start\":93111},{\"end\":93120,\"start\":93117},{\"end\":93129,\"start\":93124},{\"end\":93136,\"start\":93133},{\"end\":93144,\"start\":93140},{\"end\":93413,\"start\":93411},{\"end\":93420,\"start\":93417},{\"end\":93426,\"start\":93424},{\"end\":93717,\"start\":93713},{\"end\":93729,\"start\":93721},{\"end\":93985,\"start\":93982},{\"end\":93991,\"start\":93989},{\"end\":93997,\"start\":93995},{\"end\":94005,\"start\":94001},{\"end\":94013,\"start\":94009},{\"end\":94236,\"start\":94232},{\"end\":94243,\"start\":94240},{\"end\":94250,\"start\":94247},{\"end\":94258,\"start\":94254},{\"end\":94611,\"start\":94608},{\"end\":94628,\"start\":94615},{\"end\":94642,\"start\":94634},{\"end\":94953,\"start\":94947},{\"end\":94966,\"start\":94957},{\"end\":94979,\"start\":94970},{\"end\":94989,\"start\":94983},{\"end\":95501,\"start\":95498},{\"end\":95508,\"start\":95505},{\"end\":95516,\"start\":95512},{\"end\":95524,\"start\":95520},{\"end\":95847,\"start\":95843},{\"end\":95854,\"start\":95851},{\"end\":95865,\"start\":95858},{\"end\":95873,\"start\":95869},{\"end\":96028,\"start\":96007},{\"end\":96045,\"start\":96032},{\"end\":96063,\"start\":96051},{\"end\":96083,\"start\":96067},{\"end\":96098,\"start\":96087},{\"end\":96367,\"start\":96364},{\"end\":96377,\"start\":96373}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":229403424},\"end\":59866,\"start\":59388},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":245864879},\"end\":60223,\"start\":59868},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":17682066},\"end\":60608,\"start\":60225},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":7817555},\"end\":60974,\"start\":60610},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":14395688},\"end\":61348,\"start\":60976},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":2796017},\"end\":61717,\"start\":61350},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":37318664},\"end\":62064,\"start\":61719},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":5157313},\"end\":62495,\"start\":62066},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":3198903},\"end\":62922,\"start\":62497},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":7454625},\"end\":63327,\"start\":62924},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":2536395},\"end\":63722,\"start\":63329},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":1910869},\"end\":63913,\"start\":63724},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":18502218},\"end\":64202,\"start\":63915},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":206590483},\"end\":64578,\"start\":64204},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":749620},\"end\":64875,\"start\":64580},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":7612598},\"end\":65362,\"start\":64877},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":16027263},\"end\":65616,\"start\":65364},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":4545310},\"end\":66020,\"start\":65618},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":3645757},\"end\":66423,\"start\":66022},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":52955811},\"end\":66651,\"start\":66425},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":202577232},\"end\":67040,\"start\":66653},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":219964549},\"end\":67430,\"start\":67042},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":220961550},\"end\":67756,\"start\":67432},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":14124313},\"end\":68197,\"start\":67758},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":206592484},\"end\":68627,\"start\":68199},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":206594738},\"end\":69013,\"start\":68629},{\"attributes\":{\"doi\":\"abs/1505.07293\",\"id\":\"b26\",\"matched_paper_id\":11144038},\"end\":69356,\"start\":69015},{\"attributes\":{\"doi\":\"abs/1505.04597\",\"id\":\"b27\",\"matched_paper_id\":3719281},\"end\":69619,\"start\":69358},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":2131202},\"end\":69989,\"start\":69621},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":13164291},\"end\":70317,\"start\":69991},{\"attributes\":{\"id\":\"b30\"},\"end\":70424,\"start\":70319},{\"attributes\":{\"id\":\"b31\"},\"end\":70619,\"start\":70426},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":13709403},\"end\":70941,\"start\":70621},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":208094144},\"end\":71194,\"start\":70943},{\"attributes\":{\"id\":\"b34\"},\"end\":71552,\"start\":71196},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":214714319},\"end\":71797,\"start\":71554},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":218597806},\"end\":72163,\"start\":71799},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":221590195},\"end\":72439,\"start\":72165},{\"attributes\":{\"id\":\"b38\"},\"end\":72740,\"start\":72441},{\"attributes\":{\"id\":\"b39\"},\"end\":72872,\"start\":72742},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":244180419},\"end\":73196,\"start\":72874},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":243873707},\"end\":73547,\"start\":73198},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":1910869},\"end\":73911,\"start\":73549},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":9059102},\"end\":74324,\"start\":73913},{\"attributes\":{\"id\":\"b44\"},\"end\":74700,\"start\":74326},{\"attributes\":{\"doi\":\"abs/1808.01050\",\"id\":\"b45\",\"matched_paper_id\":51901514},\"end\":75101,\"start\":74702},{\"attributes\":{\"id\":\"b46\"},\"end\":75367,\"start\":75103},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":229644676},\"end\":75896,\"start\":75369},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":210157261},\"end\":76269,\"start\":75898},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":16958950},\"end\":76696,\"start\":76271},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":11908837},\"end\":77076,\"start\":76698},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":238259345},\"end\":77423,\"start\":77078},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":227746068},\"end\":77898,\"start\":77425},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":206814921},\"end\":78255,\"start\":77900},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":67856061},\"end\":78719,\"start\":78257},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":207757205},\"end\":79239,\"start\":78721},{\"attributes\":{\"id\":\"b56\"},\"end\":79435,\"start\":79241},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":199543622},\"end\":79840,\"start\":79437},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":211132911},\"end\":80248,\"start\":79842},{\"attributes\":{\"doi\":\"abs/2202.13660\",\"id\":\"b59\",\"matched_paper_id\":247158925},\"end\":80501,\"start\":80250},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":13442421},\"end\":80850,\"start\":80503},{\"attributes\":{\"doi\":\"abs/2007.08260\",\"id\":\"b61\",\"matched_paper_id\":220546532},\"end\":81146,\"start\":80852},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":697405},\"end\":81570,\"start\":81148},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":1089358},\"end\":82008,\"start\":81572},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":3003101},\"end\":82472,\"start\":82010},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":3740753},\"end\":82924,\"start\":82474},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":206770307},\"end\":83124,\"start\":82926},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":54461738},\"end\":83501,\"start\":83126},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":165163975},\"end\":83855,\"start\":83503},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":211677656},\"end\":84125,\"start\":83857},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":221408358},\"end\":84666,\"start\":84127},{\"attributes\":{\"doi\":\"abs/2202.08517\",\"id\":\"b71\",\"matched_paper_id\":246904467},\"end\":84931,\"start\":84668},{\"attributes\":{\"id\":\"b72\",\"matched_paper_id\":206594692},\"end\":85255,\"start\":84933},{\"attributes\":{\"id\":\"b73\"},\"end\":85495,\"start\":85257},{\"attributes\":{\"id\":\"b74\"},\"end\":85645,\"start\":85497},{\"attributes\":{\"doi\":\"abs/1810.03272\",\"id\":\"b75\",\"matched_paper_id\":52286264},\"end\":85891,\"start\":85647},{\"attributes\":{\"id\":\"b76\",\"matched_paper_id\":207926635},\"end\":86261,\"start\":85893},{\"attributes\":{\"id\":\"b77\",\"matched_paper_id\":235306106},\"end\":86527,\"start\":86263},{\"attributes\":{\"doi\":\"abs/2202.03843\",\"id\":\"b78\",\"matched_paper_id\":246652460},\"end\":86802,\"start\":86529},{\"attributes\":{\"id\":\"b79\",\"matched_paper_id\":212675106},\"end\":87273,\"start\":86804},{\"attributes\":{\"id\":\"b80\",\"matched_paper_id\":57246310},\"end\":87658,\"start\":87275},{\"attributes\":{\"id\":\"b81\",\"matched_paper_id\":53783843},\"end\":87960,\"start\":87660},{\"attributes\":{\"id\":\"b82\"},\"end\":88164,\"start\":87962},{\"attributes\":{\"id\":\"b83\",\"matched_paper_id\":13756489},\"end\":88803,\"start\":88166},{\"attributes\":{\"id\":\"b84\",\"matched_paper_id\":225039882},\"end\":89305,\"start\":88805},{\"attributes\":{\"id\":\"b85\",\"matched_paper_id\":229363322},\"end\":89613,\"start\":89307},{\"attributes\":{\"id\":\"b86\",\"matched_paper_id\":218889832},\"end\":89895,\"start\":89615},{\"attributes\":{\"doi\":\"abs/2010.04159\",\"id\":\"b87\",\"matched_paper_id\":222208633},\"end\":90191,\"start\":89897},{\"attributes\":{\"id\":\"b88\",\"matched_paper_id\":104292134},\"end\":90580,\"start\":90193},{\"attributes\":{\"doi\":\"abs/1904.02774\",\"id\":\"b89\",\"matched_paper_id\":102351136},\"end\":90760,\"start\":90582},{\"attributes\":{\"doi\":\"abs/2108.00584\",\"id\":\"b90\",\"matched_paper_id\":236772620},\"end\":91035,\"start\":90762},{\"attributes\":{\"doi\":\"abs/2109.14483\",\"id\":\"b91\",\"matched_paper_id\":238215567},\"end\":91282,\"start\":91037},{\"attributes\":{\"doi\":\"abs/2105.10926\",\"id\":\"b92\",\"matched_paper_id\":235166669},\"end\":91552,\"start\":91284},{\"attributes\":{\"id\":\"b93\",\"matched_paper_id\":246681763},\"end\":91926,\"start\":91554},{\"attributes\":{\"doi\":\"abs/2202.13065\",\"id\":\"b94\",\"matched_paper_id\":247158114},\"end\":92147,\"start\":91928},{\"attributes\":{\"doi\":\"abs/2112.15509\",\"id\":\"b95\",\"matched_paper_id\":245634402},\"end\":92426,\"start\":92149},{\"attributes\":{\"id\":\"b96\",\"matched_paper_id\":248510102},\"end\":92720,\"start\":92428},{\"attributes\":{\"id\":\"b97\"},\"end\":93030,\"start\":92722},{\"attributes\":{\"doi\":\"abs/2007.08260\",\"id\":\"b98\",\"matched_paper_id\":220546532},\"end\":93326,\"start\":93032},{\"attributes\":{\"id\":\"b99\",\"matched_paper_id\":225122845},\"end\":93609,\"start\":93328},{\"attributes\":{\"doi\":\"arXiv:1805.11123\",\"id\":\"b100\"},\"end\":93926,\"start\":93611},{\"attributes\":{\"doi\":\"abs/2203.02636\",\"id\":\"b101\",\"matched_paper_id\":247291952},\"end\":94167,\"start\":93928},{\"attributes\":{\"id\":\"b102\",\"matched_paper_id\":72941021},\"end\":94533,\"start\":94169},{\"attributes\":{\"id\":\"b103\",\"matched_paper_id\":62841646},\"end\":94922,\"start\":94535},{\"attributes\":{\"id\":\"b104\",\"matched_paper_id\":873046},\"end\":95440,\"start\":94924},{\"attributes\":{\"id\":\"b105\",\"matched_paper_id\":91184269},\"end\":95797,\"start\":95442},{\"attributes\":{\"id\":\"b106\",\"matched_paper_id\":221970845},\"end\":96001,\"start\":95799},{\"attributes\":{\"id\":\"b107\"},\"end\":96313,\"start\":96003},{\"attributes\":{\"id\":\"b108\",\"matched_paper_id\":227275245},\"end\":96477,\"start\":96315}]", "bib_title": "[{\"end\":59520,\"start\":59388},{\"end\":59949,\"start\":59868},{\"end\":60309,\"start\":60225},{\"end\":60673,\"start\":60610},{\"end\":61014,\"start\":60976},{\"end\":61381,\"start\":61350},{\"end\":61800,\"start\":61719},{\"end\":62180,\"start\":62066},{\"end\":62561,\"start\":62497},{\"end\":62977,\"start\":62924},{\"end\":63446,\"start\":63329},{\"end\":63767,\"start\":63724},{\"end\":63954,\"start\":63915},{\"end\":64256,\"start\":64204},{\"end\":64626,\"start\":64580},{\"end\":64929,\"start\":64877},{\"end\":65424,\"start\":65364},{\"end\":65691,\"start\":65618},{\"end\":66113,\"start\":66022},{\"end\":66492,\"start\":66425},{\"end\":66711,\"start\":66653},{\"end\":67078,\"start\":67042},{\"end\":67509,\"start\":67432},{\"end\":67824,\"start\":67758},{\"end\":68229,\"start\":68199},{\"end\":68684,\"start\":68629},{\"end\":69113,\"start\":69015},{\"end\":69421,\"start\":69358},{\"end\":69686,\"start\":69621},{\"end\":70069,\"start\":69991},{\"end\":70711,\"start\":70621},{\"end\":70985,\"start\":70943},{\"end\":71312,\"start\":71196},{\"end\":71611,\"start\":71554},{\"end\":71854,\"start\":71799},{\"end\":72222,\"start\":72165},{\"end\":72957,\"start\":72874},{\"end\":73285,\"start\":73198},{\"end\":73592,\"start\":73549},{\"end\":73999,\"start\":73913},{\"end\":74788,\"start\":74702},{\"end\":75431,\"start\":75369},{\"end\":75969,\"start\":75898},{\"end\":76309,\"start\":76271},{\"end\":76776,\"start\":76698},{\"end\":77122,\"start\":77078},{\"end\":77526,\"start\":77425},{\"end\":77960,\"start\":77900},{\"end\":78329,\"start\":78257},{\"end\":78800,\"start\":78721},{\"end\":79274,\"start\":79241},{\"end\":79500,\"start\":79437},{\"end\":79885,\"start\":79842},{\"end\":80317,\"start\":80250},{\"end\":80565,\"start\":80503},{\"end\":80920,\"start\":80852},{\"end\":81211,\"start\":81148},{\"end\":81629,\"start\":81572},{\"end\":82110,\"start\":82010},{\"end\":82574,\"start\":82474},{\"end\":82936,\"start\":82926},{\"end\":83189,\"start\":83126},{\"end\":83572,\"start\":83503},{\"end\":83918,\"start\":83857},{\"end\":84200,\"start\":84127},{\"end\":84739,\"start\":84668},{\"end\":84977,\"start\":84933},{\"end\":85704,\"start\":85647},{\"end\":85937,\"start\":85893},{\"end\":86319,\"start\":86263},{\"end\":86618,\"start\":86529},{\"end\":86904,\"start\":86804},{\"end\":87326,\"start\":87275},{\"end\":87688,\"start\":87660},{\"end\":88191,\"start\":88166},{\"end\":88879,\"start\":88805},{\"end\":89382,\"start\":89307},{\"end\":89660,\"start\":89615},{\"end\":89969,\"start\":89897},{\"end\":90260,\"start\":90193},{\"end\":90607,\"start\":90582},{\"end\":90843,\"start\":90762},{\"end\":91103,\"start\":91037},{\"end\":91325,\"start\":91284},{\"end\":91646,\"start\":91554},{\"end\":91982,\"start\":91928},{\"end\":92200,\"start\":92149},{\"end\":92490,\"start\":92428},{\"end\":93100,\"start\":93032},{\"end\":93407,\"start\":93328},{\"end\":93978,\"start\":93928},{\"end\":94228,\"start\":94169},{\"end\":94604,\"start\":94535},{\"end\":94943,\"start\":94924},{\"end\":95494,\"start\":95442},{\"end\":95839,\"start\":95799},{\"end\":96360,\"start\":96315}]", "bib_author": "[{\"end\":59551,\"start\":59522},{\"end\":59572,\"start\":59551},{\"end\":59967,\"start\":59951},{\"end\":59979,\"start\":59967},{\"end\":59988,\"start\":59979},{\"end\":59999,\"start\":59988},{\"end\":60010,\"start\":59999},{\"end\":60021,\"start\":60010},{\"end\":60318,\"start\":60311},{\"end\":60329,\"start\":60318},{\"end\":60684,\"start\":60675},{\"end\":60695,\"start\":60684},{\"end\":60703,\"start\":60695},{\"end\":61025,\"start\":61016},{\"end\":61036,\"start\":61025},{\"end\":61047,\"start\":61036},{\"end\":61392,\"start\":61383},{\"end\":61401,\"start\":61392},{\"end\":61812,\"start\":61802},{\"end\":61824,\"start\":61812},{\"end\":61835,\"start\":61824},{\"end\":62188,\"start\":62182},{\"end\":62197,\"start\":62188},{\"end\":62206,\"start\":62197},{\"end\":62213,\"start\":62206},{\"end\":62581,\"start\":62563},{\"end\":62595,\"start\":62581},{\"end\":62609,\"start\":62595},{\"end\":62620,\"start\":62609},{\"end\":62992,\"start\":62979},{\"end\":63003,\"start\":62992},{\"end\":63014,\"start\":63003},{\"end\":63454,\"start\":63448},{\"end\":63465,\"start\":63454},{\"end\":63777,\"start\":63769},{\"end\":63786,\"start\":63777},{\"end\":63794,\"start\":63786},{\"end\":63803,\"start\":63794},{\"end\":63968,\"start\":63956},{\"end\":63977,\"start\":63968},{\"end\":63991,\"start\":63977},{\"end\":64267,\"start\":64258},{\"end\":64277,\"start\":64267},{\"end\":64638,\"start\":64628},{\"end\":64653,\"start\":64638},{\"end\":64943,\"start\":64931},{\"end\":64953,\"start\":64943},{\"end\":65434,\"start\":65426},{\"end\":65443,\"start\":65434},{\"end\":65453,\"start\":65443},{\"end\":65467,\"start\":65453},{\"end\":65474,\"start\":65467},{\"end\":65702,\"start\":65693},{\"end\":65710,\"start\":65702},{\"end\":65718,\"start\":65710},{\"end\":65725,\"start\":65718},{\"end\":65731,\"start\":65725},{\"end\":66121,\"start\":66115},{\"end\":66130,\"start\":66121},{\"end\":66138,\"start\":66130},{\"end\":66501,\"start\":66494},{\"end\":66509,\"start\":66501},{\"end\":66517,\"start\":66509},{\"end\":66523,\"start\":66517},{\"end\":66720,\"start\":66713},{\"end\":66728,\"start\":66720},{\"end\":66735,\"start\":66728},{\"end\":66742,\"start\":66735},{\"end\":66750,\"start\":66742},{\"end\":66757,\"start\":66750},{\"end\":66765,\"start\":66757},{\"end\":67089,\"start\":67080},{\"end\":67098,\"start\":67089},{\"end\":67104,\"start\":67098},{\"end\":67113,\"start\":67104},{\"end\":67119,\"start\":67113},{\"end\":67127,\"start\":67119},{\"end\":67135,\"start\":67127},{\"end\":67143,\"start\":67135},{\"end\":67519,\"start\":67511},{\"end\":67530,\"start\":67519},{\"end\":67838,\"start\":67826},{\"end\":67851,\"start\":67838},{\"end\":68242,\"start\":68231},{\"end\":68249,\"start\":68242},{\"end\":68256,\"start\":68249},{\"end\":68268,\"start\":68256},{\"end\":68278,\"start\":68268},{\"end\":68290,\"start\":68278},{\"end\":68299,\"start\":68290},{\"end\":68312,\"start\":68299},{\"end\":68326,\"start\":68312},{\"end\":68696,\"start\":68686},{\"end\":68709,\"start\":68696},{\"end\":68723,\"start\":68709},{\"end\":68734,\"start\":68723},{\"end\":69133,\"start\":69115},{\"end\":69142,\"start\":69133},{\"end\":69153,\"start\":69142},{\"end\":69438,\"start\":69423},{\"end\":69449,\"start\":69438},{\"end\":69457,\"start\":69449},{\"end\":69697,\"start\":69688},{\"end\":69703,\"start\":69697},{\"end\":69711,\"start\":69703},{\"end\":69719,\"start\":69711},{\"end\":70084,\"start\":70071},{\"end\":70096,\"start\":70084},{\"end\":70107,\"start\":70096},{\"end\":70368,\"start\":70358},{\"end\":70496,\"start\":70486},{\"end\":70506,\"start\":70496},{\"end\":70515,\"start\":70506},{\"end\":70726,\"start\":70713},{\"end\":70737,\"start\":70726},{\"end\":71001,\"start\":70987},{\"end\":71323,\"start\":71314},{\"end\":71334,\"start\":71323},{\"end\":71341,\"start\":71334},{\"end\":71620,\"start\":71613},{\"end\":71627,\"start\":71620},{\"end\":71634,\"start\":71627},{\"end\":71642,\"start\":71634},{\"end\":71650,\"start\":71642},{\"end\":71867,\"start\":71856},{\"end\":71877,\"start\":71867},{\"end\":72231,\"start\":72224},{\"end\":72237,\"start\":72231},{\"end\":72246,\"start\":72237},{\"end\":72562,\"start\":72555},{\"end\":72569,\"start\":72562},{\"end\":72582,\"start\":72569},{\"end\":72803,\"start\":72791},{\"end\":72969,\"start\":72959},{\"end\":72983,\"start\":72969},{\"end\":72995,\"start\":72983},{\"end\":73294,\"start\":73287},{\"end\":73303,\"start\":73294},{\"end\":73312,\"start\":73303},{\"end\":73318,\"start\":73312},{\"end\":73327,\"start\":73318},{\"end\":73335,\"start\":73327},{\"end\":73602,\"start\":73594},{\"end\":73611,\"start\":73602},{\"end\":73619,\"start\":73611},{\"end\":73628,\"start\":73619},{\"end\":74011,\"start\":74001},{\"end\":74025,\"start\":74011},{\"end\":74040,\"start\":74025},{\"end\":74402,\"start\":74392},{\"end\":74413,\"start\":74402},{\"end\":74424,\"start\":74413},{\"end\":74432,\"start\":74424},{\"end\":74800,\"start\":74790},{\"end\":74810,\"start\":74800},{\"end\":74820,\"start\":74810},{\"end\":74829,\"start\":74820},{\"end\":74845,\"start\":74829},{\"end\":74858,\"start\":74845},{\"end\":74866,\"start\":74858},{\"end\":75188,\"start\":75175},{\"end\":75199,\"start\":75188},{\"end\":75210,\"start\":75199},{\"end\":75441,\"start\":75433},{\"end\":75447,\"start\":75441},{\"end\":75454,\"start\":75447},{\"end\":75979,\"start\":75971},{\"end\":75986,\"start\":75979},{\"end\":75993,\"start\":75986},{\"end\":75999,\"start\":75993},{\"end\":76336,\"start\":76311},{\"end\":76353,\"start\":76336},{\"end\":76369,\"start\":76353},{\"end\":76389,\"start\":76369},{\"end\":76400,\"start\":76389},{\"end\":76790,\"start\":76778},{\"end\":76800,\"start\":76790},{\"end\":76809,\"start\":76800},{\"end\":77131,\"start\":77124},{\"end\":77138,\"start\":77131},{\"end\":77144,\"start\":77138},{\"end\":77152,\"start\":77144},{\"end\":77159,\"start\":77152},{\"end\":77165,\"start\":77159},{\"end\":77173,\"start\":77165},{\"end\":77535,\"start\":77528},{\"end\":77543,\"start\":77535},{\"end\":77549,\"start\":77543},{\"end\":77555,\"start\":77549},{\"end\":77561,\"start\":77555},{\"end\":77568,\"start\":77561},{\"end\":77970,\"start\":77962},{\"end\":77976,\"start\":77970},{\"end\":77983,\"start\":77976},{\"end\":77990,\"start\":77983},{\"end\":77999,\"start\":77990},{\"end\":78340,\"start\":78331},{\"end\":78348,\"start\":78340},{\"end\":78357,\"start\":78348},{\"end\":78365,\"start\":78357},{\"end\":78372,\"start\":78365},{\"end\":78386,\"start\":78372},{\"end\":78394,\"start\":78386},{\"end\":78809,\"start\":78802},{\"end\":78817,\"start\":78809},{\"end\":78824,\"start\":78817},{\"end\":79284,\"start\":79276},{\"end\":79293,\"start\":79284},{\"end\":79301,\"start\":79293},{\"end\":79309,\"start\":79301},{\"end\":79315,\"start\":79309},{\"end\":79321,\"start\":79315},{\"end\":79508,\"start\":79502},{\"end\":79515,\"start\":79508},{\"end\":79523,\"start\":79515},{\"end\":79531,\"start\":79523},{\"end\":79894,\"start\":79887},{\"end\":79900,\"start\":79894},{\"end\":79906,\"start\":79900},{\"end\":79914,\"start\":79906},{\"end\":79922,\"start\":79914},{\"end\":79928,\"start\":79922},{\"end\":80325,\"start\":80319},{\"end\":80336,\"start\":80325},{\"end\":80344,\"start\":80336},{\"end\":80576,\"start\":80567},{\"end\":80583,\"start\":80576},{\"end\":80591,\"start\":80583},{\"end\":80929,\"start\":80922},{\"end\":80935,\"start\":80929},{\"end\":80942,\"start\":80935},{\"end\":80951,\"start\":80942},{\"end\":80958,\"start\":80951},{\"end\":80966,\"start\":80958},{\"end\":81228,\"start\":81213},{\"end\":81247,\"start\":81228},{\"end\":81257,\"start\":81247},{\"end\":81638,\"start\":81631},{\"end\":81647,\"start\":81638},{\"end\":81655,\"start\":81647},{\"end\":82125,\"start\":82112},{\"end\":82136,\"start\":82125},{\"end\":82583,\"start\":82576},{\"end\":82590,\"start\":82583},{\"end\":82598,\"start\":82590},{\"end\":82613,\"start\":82598},{\"end\":82950,\"start\":82938},{\"end\":83198,\"start\":83191},{\"end\":83206,\"start\":83198},{\"end\":83212,\"start\":83206},{\"end\":83220,\"start\":83212},{\"end\":83581,\"start\":83574},{\"end\":83589,\"start\":83581},{\"end\":83595,\"start\":83589},{\"end\":83927,\"start\":83920},{\"end\":83934,\"start\":83927},{\"end\":83943,\"start\":83934},{\"end\":83950,\"start\":83943},{\"end\":84210,\"start\":84202},{\"end\":84216,\"start\":84210},{\"end\":84222,\"start\":84216},{\"end\":84228,\"start\":84222},{\"end\":84237,\"start\":84228},{\"end\":84245,\"start\":84237},{\"end\":84749,\"start\":84741},{\"end\":84757,\"start\":84749},{\"end\":84768,\"start\":84757},{\"end\":84985,\"start\":84979},{\"end\":84994,\"start\":84985},{\"end\":85001,\"start\":84994},{\"end\":85008,\"start\":85001},{\"end\":85268,\"start\":85257},{\"end\":85278,\"start\":85268},{\"end\":85285,\"start\":85278},{\"end\":85298,\"start\":85285},{\"end\":85309,\"start\":85298},{\"end\":85718,\"start\":85706},{\"end\":85726,\"start\":85718},{\"end\":85736,\"start\":85726},{\"end\":85948,\"start\":85939},{\"end\":85955,\"start\":85948},{\"end\":85963,\"start\":85955},{\"end\":85970,\"start\":85963},{\"end\":85978,\"start\":85970},{\"end\":85985,\"start\":85978},{\"end\":85993,\"start\":85985},{\"end\":86329,\"start\":86321},{\"end\":86337,\"start\":86329},{\"end\":86345,\"start\":86337},{\"end\":86352,\"start\":86345},{\"end\":86360,\"start\":86352},{\"end\":86366,\"start\":86360},{\"end\":86372,\"start\":86366},{\"end\":86378,\"start\":86372},{\"end\":86626,\"start\":86620},{\"end\":86634,\"start\":86626},{\"end\":86922,\"start\":86906},{\"end\":86931,\"start\":86922},{\"end\":86940,\"start\":86931},{\"end\":86954,\"start\":86940},{\"end\":87336,\"start\":87328},{\"end\":87344,\"start\":87336},{\"end\":87354,\"start\":87344},{\"end\":87363,\"start\":87354},{\"end\":87369,\"start\":87363},{\"end\":87380,\"start\":87369},{\"end\":87697,\"start\":87690},{\"end\":87709,\"start\":87697},{\"end\":87718,\"start\":87709},{\"end\":88034,\"start\":88026},{\"end\":88040,\"start\":88034},{\"end\":88049,\"start\":88040},{\"end\":88204,\"start\":88193},{\"end\":88215,\"start\":88204},{\"end\":88225,\"start\":88215},{\"end\":88238,\"start\":88225},{\"end\":88247,\"start\":88238},{\"end\":88258,\"start\":88247},{\"end\":88268,\"start\":88258},{\"end\":88282,\"start\":88268},{\"end\":88896,\"start\":88881},{\"end\":88905,\"start\":88896},{\"end\":88919,\"start\":88905},{\"end\":88934,\"start\":88919},{\"end\":88942,\"start\":88934},{\"end\":88957,\"start\":88942},{\"end\":88969,\"start\":88957},{\"end\":88981,\"start\":88969},{\"end\":88992,\"start\":88981},{\"end\":89001,\"start\":88992},{\"end\":89014,\"start\":89001},{\"end\":89025,\"start\":89014},{\"end\":89395,\"start\":89384},{\"end\":89403,\"start\":89395},{\"end\":89412,\"start\":89403},{\"end\":89421,\"start\":89412},{\"end\":89437,\"start\":89421},{\"end\":89441,\"start\":89437},{\"end\":89672,\"start\":89662},{\"end\":89681,\"start\":89672},{\"end\":89693,\"start\":89681},{\"end\":89704,\"start\":89693},{\"end\":89716,\"start\":89704},{\"end\":89729,\"start\":89716},{\"end\":89978,\"start\":89971},{\"end\":89984,\"start\":89978},{\"end\":89990,\"start\":89984},{\"end\":89996,\"start\":89990},{\"end\":90004,\"start\":89996},{\"end\":90011,\"start\":90004},{\"end\":90268,\"start\":90262},{\"end\":90278,\"start\":90268},{\"end\":90285,\"start\":90278},{\"end\":90293,\"start\":90285},{\"end\":90619,\"start\":90609},{\"end\":90627,\"start\":90619},{\"end\":90639,\"start\":90627},{\"end\":90852,\"start\":90845},{\"end\":90860,\"start\":90852},{\"end\":90866,\"start\":90860},{\"end\":91113,\"start\":91105},{\"end\":91120,\"start\":91113},{\"end\":91128,\"start\":91120},{\"end\":91334,\"start\":91327},{\"end\":91341,\"start\":91334},{\"end\":91351,\"start\":91341},{\"end\":91363,\"start\":91351},{\"end\":91374,\"start\":91363},{\"end\":91384,\"start\":91374},{\"end\":91656,\"start\":91648},{\"end\":91993,\"start\":91984},{\"end\":91999,\"start\":91993},{\"end\":92006,\"start\":91999},{\"end\":92209,\"start\":92202},{\"end\":92217,\"start\":92209},{\"end\":92225,\"start\":92217},{\"end\":92232,\"start\":92225},{\"end\":92239,\"start\":92232},{\"end\":92246,\"start\":92239},{\"end\":92254,\"start\":92246},{\"end\":92501,\"start\":92492},{\"end\":92509,\"start\":92501},{\"end\":92515,\"start\":92509},{\"end\":92523,\"start\":92515},{\"end\":92530,\"start\":92523},{\"end\":92803,\"start\":92796},{\"end\":92811,\"start\":92803},{\"end\":92819,\"start\":92811},{\"end\":92828,\"start\":92819},{\"end\":92835,\"start\":92828},{\"end\":92842,\"start\":92835},{\"end\":92849,\"start\":92842},{\"end\":92857,\"start\":92849},{\"end\":93109,\"start\":93102},{\"end\":93115,\"start\":93109},{\"end\":93122,\"start\":93115},{\"end\":93131,\"start\":93122},{\"end\":93138,\"start\":93131},{\"end\":93146,\"start\":93138},{\"end\":93415,\"start\":93409},{\"end\":93422,\"start\":93415},{\"end\":93428,\"start\":93422},{\"end\":93719,\"start\":93711},{\"end\":93731,\"start\":93719},{\"end\":93987,\"start\":93980},{\"end\":93993,\"start\":93987},{\"end\":93999,\"start\":93993},{\"end\":94007,\"start\":93999},{\"end\":94015,\"start\":94007},{\"end\":94238,\"start\":94230},{\"end\":94245,\"start\":94238},{\"end\":94252,\"start\":94245},{\"end\":94260,\"start\":94252},{\"end\":94613,\"start\":94606},{\"end\":94630,\"start\":94613},{\"end\":94644,\"start\":94630},{\"end\":94955,\"start\":94945},{\"end\":94968,\"start\":94955},{\"end\":94981,\"start\":94968},{\"end\":94991,\"start\":94981},{\"end\":95503,\"start\":95496},{\"end\":95510,\"start\":95503},{\"end\":95518,\"start\":95510},{\"end\":95526,\"start\":95518},{\"end\":95849,\"start\":95841},{\"end\":95856,\"start\":95849},{\"end\":95867,\"start\":95856},{\"end\":95875,\"start\":95867},{\"end\":96030,\"start\":96005},{\"end\":96047,\"start\":96030},{\"end\":96065,\"start\":96047},{\"end\":96085,\"start\":96065},{\"end\":96100,\"start\":96085},{\"end\":96369,\"start\":96362},{\"end\":96379,\"start\":96369}]", "bib_venue": "[{\"end\":61547,\"start\":61481},{\"end\":65141,\"start\":65057},{\"end\":67927,\"start\":67909},{\"end\":73719,\"start\":73682},{\"end\":75556,\"start\":75524},{\"end\":78915,\"start\":78903},{\"end\":79618,\"start\":79597},{\"end\":81376,\"start\":81325},{\"end\":81748,\"start\":81727},{\"end\":84330,\"start\":84299},{\"end\":88482,\"start\":88382},{\"end\":95165,\"start\":95078},{\"end\":59603,\"start\":59572},{\"end\":60028,\"start\":60021},{\"end\":60391,\"start\":60329},{\"end\":60765,\"start\":60703},{\"end\":61137,\"start\":61047},{\"end\":61479,\"start\":61401},{\"end\":61871,\"start\":61835},{\"end\":62260,\"start\":62213},{\"end\":62682,\"start\":62620},{\"end\":63108,\"start\":63014},{\"end\":63505,\"start\":63465},{\"end\":63807,\"start\":63803},{\"end\":64038,\"start\":63991},{\"end\":64367,\"start\":64277},{\"end\":64711,\"start\":64653},{\"end\":65052,\"start\":64953},{\"end\":65478,\"start\":65474},{\"end\":65801,\"start\":65731},{\"end\":66205,\"start\":66138},{\"end\":66527,\"start\":66523},{\"end\":66829,\"start\":66765},{\"end\":67217,\"start\":67143},{\"end\":67585,\"start\":67530},{\"end\":67907,\"start\":67851},{\"end\":68396,\"start\":68326},{\"end\":68804,\"start\":68734},{\"end\":69172,\"start\":69167},{\"end\":69476,\"start\":69471},{\"end\":69789,\"start\":69719},{\"end\":70131,\"start\":70107},{\"end\":70356,\"start\":70319},{\"end\":70484,\"start\":70426},{\"end\":70759,\"start\":70737},{\"end\":71062,\"start\":71001},{\"end\":71348,\"start\":71341},{\"end\":71655,\"start\":71650},{\"end\":71966,\"start\":71877},{\"end\":72283,\"start\":72246},{\"end\":72553,\"start\":72441},{\"end\":72789,\"start\":72742},{\"end\":73016,\"start\":72995},{\"end\":73349,\"start\":73335},{\"end\":73680,\"start\":73628},{\"end\":74103,\"start\":74040},{\"end\":74390,\"start\":74326},{\"end\":74885,\"start\":74880},{\"end\":75173,\"start\":75103},{\"end\":75522,\"start\":75454},{\"end\":76061,\"start\":75999},{\"end\":76469,\"start\":76400},{\"end\":76869,\"start\":76809},{\"end\":77235,\"start\":77173},{\"end\":77642,\"start\":77568},{\"end\":78060,\"start\":77999},{\"end\":78468,\"start\":78394},{\"end\":78901,\"start\":78824},{\"end\":79325,\"start\":79321},{\"end\":79595,\"start\":79531},{\"end\":80027,\"start\":79928},{\"end\":80363,\"start\":80358},{\"end\":80660,\"start\":80591},{\"end\":80985,\"start\":80980},{\"end\":81323,\"start\":81257},{\"end\":81725,\"start\":81655},{\"end\":82225,\"start\":82136},{\"end\":82680,\"start\":82613},{\"end\":83010,\"start\":82950},{\"end\":83294,\"start\":83220},{\"end\":83657,\"start\":83595},{\"end\":83966,\"start\":83950},{\"end\":84297,\"start\":84245},{\"end\":84787,\"start\":84782},{\"end\":85078,\"start\":85008},{\"end\":85363,\"start\":85309},{\"end\":85559,\"start\":85497},{\"end\":85755,\"start\":85750},{\"end\":86057,\"start\":85993},{\"end\":86382,\"start\":86378},{\"end\":86653,\"start\":86648},{\"end\":87018,\"start\":86954},{\"end\":87443,\"start\":87380},{\"end\":87792,\"start\":87718},{\"end\":88024,\"start\":87962},{\"end\":88380,\"start\":88282},{\"end\":89030,\"start\":89025},{\"end\":89445,\"start\":89441},{\"end\":89734,\"start\":89729},{\"end\":90030,\"start\":90025},{\"end\":90367,\"start\":90293},{\"end\":90658,\"start\":90653},{\"end\":90885,\"start\":90880},{\"end\":91147,\"start\":91142},{\"end\":91403,\"start\":91398},{\"end\":91727,\"start\":91656},{\"end\":92025,\"start\":92020},{\"end\":92273,\"start\":92268},{\"end\":92564,\"start\":92530},{\"end\":92794,\"start\":92722},{\"end\":93165,\"start\":93160},{\"end\":93446,\"start\":93428},{\"end\":93709,\"start\":93611},{\"end\":94034,\"start\":94029},{\"end\":94334,\"start\":94260},{\"end\":94706,\"start\":94644},{\"end\":95076,\"start\":94991},{\"end\":95600,\"start\":95526},{\"end\":95880,\"start\":95875},{\"end\":96386,\"start\":96379}]"}}}, "year": 2023, "month": 12, "day": 17}