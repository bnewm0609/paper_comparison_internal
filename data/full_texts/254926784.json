{"id": 254926784, "updated": "2023-10-05 06:22:44.787", "metadata": {"title": "MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning", "authors": "[{\"first\":\"Zhiyang\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Ying\",\"last\":\"Shen\",\"middle\":[]},{\"first\":\"Lifu\",\"last\":\"Huang\",\"middle\":[]}]", "venue": "ACL", "journal": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Instruction tuning, a new learning paradigm that fine-tunes pre-trained language models on tasks specified through instructions, has shown promising zero-shot performance on various natural language processing tasks. However, it has yet to be explored for vision and multimodal tasks. In this work, we introduce MultiInstruct, the first multimodal instruction tuning benchmark dataset that consists of 62 diverse multimodal tasks in a unified seq-to-seq format covering 10 broad categories. The tasks are derived from 21 existing open-source datasets and each task is equipped with 5 expert-written instructions. We take OFA as the base pre-trained model for multimodal instruction tuning, and to further improve its zero-shot performance, we explore multiple transfer learning strategies to leverage the large-scale Natural Instructions dataset. Experimental results demonstrate strong zero-shot performance on various unseen multimodal tasks and the benefit of transfer learning from a text-only instruction dataset. We also design a new evaluation metric \u2013 Sensitivity, to evaluate how sensitive the model is to the variety of instructions. Our results indicate that fine-tuning the model on a diverse set of tasks and instructions leads to a reduced sensitivity to variations in instructions for each task.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2212.10773", "mag": null, "acl": "2023.acl-long.641", "pubmed": null, "pubmedcentral": null, "dblp": "conf/acl/XuSH23", "doi": "10.18653/v1/2023.acl-long.641"}}, "content": {"source": {"pdf_hash": "687902bf8575d1f755142aeef6eaff964172b89c", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclanthology.org/2023.acl-long.641.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "9182242296e1bba8c347af6a086f991dc95a88be", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/687902bf8575d1f755142aeef6eaff964172b89c.txt", "contents": "\nMULTIINSTRUCT: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning\nLong PapersCopyright Long PapersJuly 9-14, 2023\n\nZhiyang Xu zhiyangx@vt.edu \nComputer Science Department Virginia Tech\n\n\nYing Shen yings@vt.edu \nComputer Science Department Virginia Tech\n\n\nLifu Huang lifuh@vt.edu \nComputer Science Department Virginia Tech\n\n\nMULTIINSTRUCT: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning\n\nProceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nthe 61st Annual Meeting of the Association for Computational LinguisticsLong Papers1July 9-14, 2023\nInstruction tuning, a new learning paradigm that fine-tunes pre-trained language models on tasks specified through instructions, has shown promising zero-shot performance on various natural language processing tasks. However, it has yet to be explored for vision and multimodal tasks. In this work, we introduce MUL-TIINSTRUCT, the first multimodal instruction tuning benchmark dataset that consists of 62 diverse multimodal tasks in a unified seq-toseq format covering 10 broad categories. The tasks are derived from 21 existing open-source datasets and each task is equipped with 5 expertwritten instructions. We take OFA (Wang et al., 2022a) as the base pre-trained model for multimodal instruction tuning, and to further improve its zero-shot performance, we explore multiple transfer learning strategies to leverage the large-scale NATURAL INSTRUCTIONS dataset(Mishra et al., 2022). Experimental results demonstrate strong zero-shot performance on various unseen multimodal tasks and the benefit of transfer learning from a text-only instruction dataset. We also design a new evaluation metric -Sensitivity, to evaluate how sensitive the model is to the variety of instructions.Our results indicate that fine-tuning the model on a diverse set of tasks and instructions leads to a reduced sensitivity to variations in instructions for each task 1 .\n\nIntroduction\n\nWith the advances in large-scale pre-trained language models (PLMs), recent studies have explored various efficient learning paradigms (Brown et al., 2020;Liu et al., 2021;Wei et al., 2021;Xie et al., 2021) to generalize PLMs to new tasks without task-specific tuning. Among these, instruction * Zhiyang Xu and Ying Shen contributed equally to this work. 1 The dataset, source code, and model checkpoints are publicly available at https://github.com/VT-NLP/ MultiInstruct. tuning (Wei et al., 2021) has achieved significant success in zero-shot learning on natural language processing tasks. By fine-tuning a PLM on tasks described through instructions, instruction tuning allows the model to learn to understand and follow the instructions to perform predictions on unseen tasks. Recent advancement in multimodal pretraining (Wang et al., 2022a;Alayrac et al., 2022;Bao et al., 2022;Wang et al., 2022c) has shown the potential of jointly interpreting text and images in a shared semantic space, which further leads us to ask: can the instruction tuning be leveraged to improve the generalizability of Vision-Language pretrained models on multi-modal and vision tasks?\n\nIn this work, we propose MULTIINSTRUCT, the first benchmark dataset for multimodal instruction tuning with 62 diverse tasks from 10 broad categories, including Visual Question Answering (Goyal et al., 2017;Suhr et al., 2017), Commonsense Reasoning (Zellers et al., 2019;Xie et al., 2019), Visual Relationship Understanding (Krishna et al., 2017) and so on. We equipped each task with 5 instructions that are written by two experts in natural language processing. As shown in Figure 1, we formulate all the tasks into a unified sequence-to-sequence format in which the input text, images, instructions, and bounding boxes are represented in the same token space.\n\nWe use OFA (Wang et al., 2022a) 2 , a unified model that is pre-trained on a diverse set of multimodal and unimodal tasks in a single Transformerbased sequence-to-sequence framework, as the base pre-trained multimodal language model, and fine-tune it on MULTIINSTRUCT. To utilize NATU-RAL INSTRUCTIONS (Mishra et al., 2022), a largescale text-only instruction tuning dataset, we further explore two transfer learning strategies, in-Grounded Caption\n\n\nText Localization Referring Expression Selection\n\nOutput: blue and white tennis racquet\n\n\nInput:\n\nGenerate a caption for <bin_198> <bin_32> <bin_400> <bin_193>.\n\n\nInput:\n\nSelect the region that contains the text \"den\". Options: <bin_206> <bin_119> <bin_448> <bin_181> ||||<bin_357> <bin_518> <bin_456> <bin_574> ||||<bin_229> <bin_604> <bin_304> <bin_654>\n\n\nInput:\n\nSelect the region of the object described by \"A blue train in the front.\". Options: <bin_242> <bin_180> <bin_736> <bin_475> |||| <bin_88> <bin_291> <bin_203> <bin_473>|||| <bin_193> <bin_339> <bin_247> <bin_442>\n\nOutput: <bin_229> <bin_604> <bin_304> <bin_654>\n\nOutput: <bin_242> <bin_180> <bin_736> <bin_475>\n\n\nQuestion-Image Matching\n\nOutput: the question is irrelevant to the image\n\n\nInput:\n\nGiven the content of image, do you have enough information to answer \"Is it a sunny day?\"? Options: \"the question is relevant to the image\" or \"the question is irrelevant to the image\" cluding Mixed Instruction Tuning and Sequential Instruction Tuning. Experimental results demonstrate strong zero-shot performance on various unseen multimodal tasks with instruction tuning and the potential of further improving it by leveraging large-scale text-only instruction datasets.\n\nAs suggested by previous studies (Webson and Pavlick, 2022;Liu et al., 2022b), PLMs are highly sensitive toward the wording and length of instructions. Thus, we propose a new metric -Sensitivity, which measures how sensitive the model is toward the variety of instructions for the same task. Experimental results demonstrate that (1) instruction tuning significantly reduces the sensitivity of OFA to the varying wording of instructions. The more tuning tasks and instructions for each task are introduced, the lower sensitivity tends to be achieved, and (2) transferring from a larger text-only instruction dataset can also significantly reduces the sensitivity of OFA.\n\n\nRelated Work\n\nMultimodal Pretraining Multimodal pretraining (Tan and Bansal, 2019;Cho et al., 2021;Singh et al., 2022;Alayrac et al., 2022;Wang et al., 2022a;Li et al., 2022b,a) has significantly advanced the vision-language tasks. Several recent studies (Cho et al., 2021;Wang et al., 2022a,c;Lu et al., 2022) also started to build a unified pre-training framework to handle a diverse set of cross-modal and unimodal tasks. Among them, VL-T5 (Cho et al., 2021) tackles vision-and-language tasks with a unified text-generation objective conditioned on multimodal inputs, while OFA (Wang et al., 2022a) further extends it to image generation tasks by using a unified vocabulary for all text and visual tokens. BEIT-3 (Wang et al., 2022c) utilizes a novel shared Multiway Transformer network with a shared self-attention module to align different modalities and provide deep fusion. Building on the success of multimodal pretraining, our work focuses on improving the generalization and zeroshot performance on various unseen multimodal tasks through instruction tuning.\n\nEfficient Language Model Tuning To improve the generalizability and adaptivity of large-scale pre-trained language models, various efficient language model tuning strategies have been proposed recently. Prompt tuning (Liu et al., 2021;Li and Liang, 2021;Han et al., 2022;Wang et al., 2022b;Sanh et al., 2022) aims to learn a task-specific prompt by reformulating the downstream tasks to the format that the model was initially trained on and has shown competitive performance across various natural language processing applications. As a special form of prompt tuning, in-context learning (Xie et al., 2021;Min et al., 2021) takes one or a few examples as the prompt to demonstrate the task. Instruction tuning (Wei et al., 2021) is another simple yet effective strategy to improve the generalizability of large language models. NATURAL IN-STRUCTIONS (Mishra et al., 2022) is a meta-dataset containing diverse tasks with human-authored definitions, things to avoid, and demonstrations. It has shown effectiveness in improving the generalizability of language models even when the size is relatively small (e.g., BART_base) (Mishra et al., 2022;Wang et al., 2022d). InstructDial (Gupta et al., 2022) applies instruction tuning to the dialogue domain and shows significant zero-shot performance on unseen dialogue tasks. While these studies have been successful in text-only domains, it has not yet been extensively explored for vision or multimodal tasks.\n\n\nMULTIINSTRUCT\n\n\nMultimodal Task and Data Collection\n\nThe MULTIINSTRUCT dataset is designed to cover a wide range of multimodal tasks that require reasoning among regions, images, and text. These tasks are meant to teach machine learning models to perform various tasks such as object recognition, visual relationship understanding, text-image grounding, and so on by following instructions so that they can perform zero-shot prediction on unseen tasks. To build MULTIINSTRUCT, we first collect 34 tasks from the existing studies in visual and multimodal learning, covering Visual Question Answering (Goyal et al., 2017;Krishna et al., 2017;Zhu et al., 2016;Hudson and Manning, 2019;Singh et al., 2019;Marino et al., 2019), Commonsense Reasoning (Suhr et al., 2017Liu et al., 2022a;Zellers et al., 2019;Xie et al., 2019), Region Understanding (Krishna et al., 2017, Image Understanding (Kafle and Kanan, 2017;Chiu et al., 2020), Grounded Generation (Krishna et al., 2017Yu et al., 2016;Lin et al., 2014), Image-Text Matching (Lin et al., 2014;Goyal et al., 2017), Grounded Matching (Krishna et al., 2017Veit et al., 2016;Yu et al., 2016), Visual Relationship (Krishna et al., 2017Pham et al., 2021), Temporal Ordering tasks that are created from WikiHow 3 , and Miscellaneous (Yao et al., 2022;Kiela et al., 2020;Das et al., 2017;Lin et al., 2014;Veit et al., 2016;Alam et al., 2022). Each of the 34 tasks can be found with one or multiple open-source datasets, which are incorporated into MULTIINSTRUCT. Details of each task and their corresponding datasets are shown in Tables 7 to 9 in Appendix.\n\nFor each of these tasks, we further examine the possibility of deriving new tasks based on the input and output of the original task to augment the task repository. For example, Visual Grounding requires the model to generate a caption for a given region in the image. We derive two additional tasks from it: Grounded Caption Selection, which is a simpler task that requires the model to select the 3 https://www.wikihow.com. corresponding caption from multiple candidates for the given region, and Visual Grounding Selection, which requires the model to select the corresponding region from the provided candidate regions based on a given caption. Compared with Visual Grounding, these two new tasks require different skills based on distinct input and output information. In this way, we further derived 28 new tasks from the 34 existing tasks. We divide all 62 tasks into 10 broad categories as shown in Figure 2.\n\nFor the existing tasks, we use their available open-source datasets to create instances (i.e., input and output pairs) while for each new task, we create its instances by extracting the necessary information from instances of existing tasks or reformulating them. Each new task is created with 5,000 to 5M instances. We split the 62 tasks into training and evaluation based on the following criteria: (1) we take the tasks that are similar to the pre-training tasks of OFA (Wang et al., 2022a) for training; and (2) we select the challenging multimodal tasks that do not overlap with the training tasks for evaluation. Table 5 and Table 6 in Appendix A show the detailed statistics for the training and evaluation tasks in MULTIINSTRUCT and Tables 7 to 9 show their corresponding datasets.\n\n\nTask Instruction Creation\n\nWe first provide a definition for \"instruction\" used in MULTIINSTRUCT. An instruction is defined with a template that describes how the task should be performed and contains an arbitrary number of placeholders, including <TEXT>, <REGION> and <OPTION>, for the input information from the original task. For example, in the instruction of the Grounded Captioning task, \"Generate a caption for <REGION>\", <REGION> is the placeholder for region-specific information. Note that the placeholder <OPTION> is only used in classification tasks and for some tasks, the input may also include an image that is not included in the instruction and will be fed as a separate input to the model. To produce high-quality instructions that accurately convey the intended tasks, we employ an iterative annotation process involving two expert annotators who have a thorough understanding of the task and the dataset.\n\nStep 1: each annotator first writes 2-3 instructions for each task by giving them the specific goals of  this task, the format of input data, and 10 example instances randomly sampled from the dataset. The information about the dataset is obtained from the dataset's README file or the publication that introduced the dataset. For newly derived tasks, we provide annotators with task descriptions along with 10 constructed example instances.\n\nStep 2: to guarantee the quality of the instructions and that they effectively convey the intended tasks, we have each annotator review the instructions created by their peers, checking if they can clearly understand and identify the intended task by just reading the instruction. If any issues are identified, the reviewing annotator provides suggestions and works with the original annotator to revise the instructions.\n\nStep 3: to ensure the consistency and avoid conflicts or repetition among instructions from different annotators, we have both annotators review the sets of instructions together, identifying any discrepancies or inconsistencies. If any are found, the annotators collaborate to resolve them and create a final set of instructions that accurately and clearly describe the task. In this way, each task will be created with 5 high-quality instructions.\n\nStep 4: we repeat steps 1-3 to create 5 instructions for each of the training and evaluation tasks. Finally, both annotators review each task and its instructions and filter out the task that is not repre-sentative or overlaps with other tasks.\n\n\nMultimodal Instruction Formatting\n\nTo unify the processing of various input/output data types, we follow the method from OFA (Wang et al., 2022a), which involves representing images, text, and bounding box coordinates as tokens in a unified vocabulary. Specifically, we apply bytepair encoding (BPE) (Sennrich et al., 2016) to encode the text input. For the target image, we apply VQ- GAN (Esser et al., 2021) to generate discrete image tokens through image quantization. To represent regions or bounding boxes of an image, we discretize the four corner coordinates into location tokens such as \"<bin_242> <bin_180> <bin_736> <bin_475>\" where each location token \"<bin_NUM>\" represents a quantized coordinate obtained by dividing the image into 1,000 bins. This approach allows us to convert different types of input into a unified vocabulary. All tasks in MULTIINSTRUCT can then be formulated as natural language sequence-to-sequence generation problems, where the input includes: (1) an image (if there is no input image, a black picture is used as the input); and (2) an instruction where the placeholders such as <TEXT>, <REGION> or <OPTION> are filled with specific information of each input instance. Notably, for the <OPTION> of the instructions for classification tasks, we intro-duce two special tokens for this field: \"[Options]\" to mark the beginning of the option field and \"||||\" to delimit the given options. We concatenate all the options with \"||||\" in the option field and the model will directly generate one option from them. Figure 1 provides several examples of the formulated input and illustrates how the original data input is combined with the instruction in the MULTIINSTRUCT.\n\n\nProblem Setup and Models\n\n\nProblem Setup\n\nWe follow the same instruction tuning setting as the previous study (Wei et al., 2021) and mainly evaluate the zero-shot learning capabilities of the finetuned large language models. Specifically, given a pre-trained multimodal language model M , we aim to finetune it on a collection of instruction tasks T . Each task t \u2208 T is associated with a number of training instances\nD t = {(I t , x t j , y t j ) \u2208 I t \u00d7 X t \u00d7 Y t } N j=1\n, where x t j denotes the input text, image, region, and options if provided, y t j denotes the output of each instance, and I t represents the set of five task instructions written by experts. The input information from x t j will be used to fill in the placeholders in the instruction.\n\nWe use OFA (Wang et al., 2022a) as the pretrained multimodal model due to its unified architecture and flexible input-output modalities. We finetune it on our MULTIINSTRUCT dataset to demonstrate the effectiveness of instruction tuning. Specifically, we use the transformer-based encoder of OFA to encode the instruction along with all necessary information and an optional image, and predict the output with the transformer-based decoder. Given that the training dataset contains many tasks, we mix all the training instances from these tasks and randomly shuffle them. For each instance, we also randomly sample an instruction template for each batch-based training. Note that, though some of the training tasks in MULTIINSTRUCT are similar to the pre-training tasks of OFA 4 , we ensure that the evaluation tasks in MULTIINSTRUCT do not overlap with either the pre-training tasks in OFA nor the training tasks in MULTIINSTRUCT. 4 Table 10 in Appendix lists the multimodal tasks and dataset used in OFA pre-training.\n\n\nTransfer Learning from NATURAL INSTRUCTIONS\n\nWe notice that the scale of NATURAL INSTRUC-TIONS (Mishra et al., 2022) is significantly larger than MULTIINSTRUCT, indicating the potential of transferring the instruction learning capability from the larger set of natural language tasks to multimodal tasks. We take 832 English tasks in NAT-URAL INSTRUCTIONS and explore several simple transfer-learning strategies:\n\n\nMixed Instruction Tuning (OFA MixedInstruct )\n\nWe combine the instances of NATURAL INSTRUC-TIONS and MULTIINSTRUCT and randomly shuffle them before finetuning OFA with instructions. Note that, each task in NATURAL INSTRUCTIONS is just associated with one instruction while for each instance from MULTIINSTRUCT, we always randomly sample one instruction from the five instructions for each instance of training.\n\n\nSequential Instruction Tuning (OFA SeqInstruct )\n\nInspired by the Pre-Finetuning approach discussed in Aghajanyan et al. (2021), we propose a twostage sequential instruction tuning strategy where we first fine-tune OFA on the NATURAL INSTRUC-TIONS dataset to encourage the model to follow instructions to perform language-only tasks, and then further fine-tune it on MULTIINSTRUCT to adapt the instruction learning capability to multimodal tasks. To maximize the effectiveness of the NATURAL INSTRUCTIONS dataset, we use all instances in English-language tasks to tune the model in the first training stage.\n\n\nExperimental Setup\n\nEvaluation Metrics We report the accuracy for classification tasks and ROUGE-L (Lin, 2004) for all generation tasks. For the region classification task, we compute the Intersection over Union (IoU) between the generated region and all regions in the options, select the option with the highest IoU as the prediction, and compute accuracy based on this prediction. If the predicted region has no intersection with any of the regions in the options, we treat this prediction as incorrect. For classification tasks where the answer is not a single-word binary classification, we also report ROUGE-L scores following Mishra et al. (2022), which treats all tasks as text generation problems. For each task, we conduct five experiments by evaluating the model using one of the five instructions in each experiment. We re-port the mean and maximum performance and the standard deviation of the performance across all five experiments. We also compute the aggregated performance for each model based on the mean of the model's performance on all multimodal and NLP unseen tasks. We use Rouge-L as the evaluation metric for most tasks and accuracy for tasks that only have accuracy as a metric.\n\nIn addition, as instruction tuning mainly relies on the instructions to guide the model to perform prediction on various unseen multimodal tasks, we further propose to evaluate how sensitive the model is to the variety of human-written instructions in the same task, which has not been discussed in previous instruction tuning studies but is necessary to understand the effectiveness of instruction tuning. We thus further design a new metric as follows: Sensitivity refers to the model's capability of consistently producing the same results, regardless of slight variations in the wording of instructions, as long as the intended task remains the same. Specifically, for each task t \u2208 T , given its associated instances with task instructions:\nD t = {(I t , x t j , y t j ) \u2208 I t \u00d7 X t \u00d7 Y t } N j=1\n, we formally define sensitivity as:\nE t\u2208T \u03c3 i\u2208I t E (x,y)\u2208D t [L(f \u03b8 (i, x), y)] \u00b5 i\u2208I t E (x,y)\u2208D t [L(f \u03b8 (i, x), y)]\nwhere L denotes the evaluation metric such as accuracy or ROUGE-L, f \u03b8 (\u00b7) represents the multimodal instruction-tuned model. The standard deviation and mean of the model's performance across all instructions are denoted by \u03c3 i\u2208I t [\u00b7] and \u00b5 i\u2208I t [\u00b7], respectively. 6 Results and Discussion\n\n\nEffectiveness of Instruction Tuning on MULTIINSTRUCT\n\nWe evaluate the zero-shot performance of various approaches on all the unseen evaluation tasks, as shown in Table 1 and 2. Our results indicate that OFA MultiInstruct significantly improves the model's zero-short performance over the original pre-trained OFA model across all unseen tasks and metrics, demonstrating the effectiveness of multimodal instruction tuning on MULTIINSTRUCT. As seen in Table 2, OFA achieves extremely low (nearly zero) zero-shot performance on the Grounded VQA task, which requires the model to generate region-specific tokens in order to answer the question. By examining the generated results, we find that OFA, without instruction tuning, failed to follow the instruction and produce results that contain region tokens. However, by fine-tuning OFA on MULTIINSTRUCT, the model is able to better interpret and follow the instructions to properly generate the expected output.  formance. This suggests that the performance gain of OFA MultiInstruct mainly comes from instructions rather than multi-task training.\n\n\nImpact of Transfer Learning from NATURAL INSTRUCTIONS\n\nOne key question in multimodal instruction tuning is how to effectively leverage the large-scale textonly NATURAL INSTRUCTIONS dataset to enhance the zero-shot performance on multimodal tasks. We observe that only fine-tuning OFA on NATU-RAL INSTRUCTIONS actually degrades the model's zero-shot performance on almost all multimodal tasks, as shown by comparing OFA NaturalInstruct and OFA in Table 1 and 2. One potential reason for this decline in performance is that during fine-tuning on the text-only dataset, the model learns to focus more on text tokens and attend less to image tokens. To verify this assumption, we compare the attention of text tokens on image tokens between OFA NaturalInstruct and other methods and observe that text tokens attend much less to image tokens after fine-tuning on the NATURAL INSTRUCTIONS dataset. The detailed explanations and analysis can be found in Appendix C. Another observation is that although our transfer learning methods do not lead to significant performance gains over OFA MixedInstruct , both OFA SeqInstruct and OFA MixedInstruct achieve lower standard deviation on 6 out of 9 unseen multimodal tasks compared with OFA MultiInstruct , demonstrating the potential benefits of the much larger text-only instruction datasets to multimodal instruction tuning.\n\n\nImpact of Increasing Multimodal Instruction Task Clusters\n\nTo evaluate the impact of the number of tasks clusters for instruction tuning, we start with the task groups shown in Figure 2 and group them into five larger clusters: (1) Img Und (VQA + Im-  (5) Region (Region Understanding), together with (6) NLP, a collection of NLP tasks from NATU-RAL INSTRUCTIONS. We measure the change in both the aggregated performance and sensitivity of OFA MixedInstruct as we gradually add the task clusters for training. As we increase the number of task clusters, we observe an improvement in both the mean and maximum aggregated performance and a decrease in sensitivity, as shown in Figure 3. Note that low sensitivity indicates that the model can produce consistent results despite variations in the wording of instructions. These results suggest that increasing the number of task clusters improves the model's performance on unseen tasks and leads to more consistent outputs. The results also support the effectiveness of our proposed MULTIINSTRUCT dataset.\n\n\nEffect of Diverse Instructions on Instruction Tuning\n\nWe hypothesize that using a diverse set of instructions for each task during multimodal instruction tuning can improve the model's zero-shot performance on unseen tasks and reduce its sensitivity to variation in the instructions. To test this hypothesis, we train an OFA model on MULTIINSTRUCT with a single fixed instruction template per task and compare its performance with OFA finetuned on 5 different instructions. As shown in Table 3, OFA finetuned on 5 instructions achieves much higher aggregated performance on all evaluation tasks and shows lower sensitivity. These results demonstrate the effectiveness of increasing the diversity of instructions and suggest that future work could explore crowd-sourcing or automatic generation strategies to create even more diverse instructions for instruction tuning.\n\n\nEffect of Fine-tuning Strategies on Model Sensitivity\n\nIn Section 6.3 and 6.4, we have shown that the more tasks and instructions used for instruction tuning, the lower sensitivity the model will achieve toward the variations in instructions for each task. We further investigate the impact of fine-tuning and transfer learning strategies on model sensitivity. Figure 4 shows the averaged sensitivity of each model across all multimodal unseen tasks. The original OFA exhibits significantly higher sensitivity to variations in instructions compared to models fine-tuned on instruction datasets, indicating that multimodal instruction tuning significantly improves the model's capability on interpreting instructions, even with varying wordings. In addition, by transferring the large-scale NATURAL INSTRUC-TIONS dataset to MULTIINSTRUCT, sensitivity is also reduced by a large margin, highlighting the benefit of fine-tuning the model on a larger instruction dataset, regardless of different formats and modalities.\n\n\nZero-Shot Performance on NLP Tasks\n\nSo far, our focus has been on evaluating the zeroshot performance of multimodal tasks. In this section, we investigate the effect of multimodal instruction tuning on the performance of text-only tasks. To do this, we evaluate all our approaches on 20 natural language processing (NLP) tasks from the default test split in NATURAL INSTRUCTIONS 6 . The detailed task list can be found in Appendix B.2. As shown in Table 4, OFA MultiInstruct outperforms OFA, despite the instruction tuning dataset and the unseen dataset are in different modalities. This suggests that multimodal instruction tuning can help improve the zero-shot performance on NLP tasks. In addition, we observe that OFA NaturalInstruct achieves the best performance on NLP tasks and OFA MixedInstruct is more effective in preserving the zero-shot capability gained from NATURAL INSTRUCTIONS on NLP tasks compared  Table 4: Zero-shot Performance on NLP tasks. The performance is reported in Rouge-L and the best performance is in bold.\n\nto OFA SeqInstruct . Based on the results in Tables 1,  2 and 4, we conclude that OFA MixedInstruct is able to achieve overall best aggregated performance on all multimodal and NLP tasks and shows much lower sensitivity towards variations in the wording of instructions, making it the most promising approach.\n\n\nConclusion\n\nWe present a new large-scale multi-modal instruction tuning benchmark dataset -MULTIINSTRUCT, which covers a wide variety of vision and multimodal tasks while each task is associated with multiple expert-written instructions. By finetuning OFA (Wang et al., 2022a), a recently state-ofthe-art multimodal pre-trained language model, on MULTIINSTRUCT with instruction tuning, its zeroshot performance on various unseen multimodal tasks is significantly improved. We also explore several transfer learning techniques to leverage the much larger text-only NATURAL INSTRUCTIONS dataset and demonstrate its benefit. Moreover, we design a new evaluation metric Sensitivity to assess the model's sensitivity towards the variations in the wording of instructions. Results show that the model becomes less sensitive to these variations after being fine-tuned on a variety of tasks and instructions.\n\n\nLimitations\n\nLimitations of Data Collection Our proposed dataset only targets English language tasks. Future work should explore multimodal instruction tuning in a more diverse language setting and augment our MULTIINSTRUCT with multi-multilingual tasks.\n\nIn addition, our current dataset mainly focuses on vision-language tasks. Datasets from more diverse modalities should be considered such as audio (Panayotov et al., 2015;Gemmeke et al., 2017;You et al., 2022) \n\n\nLimitations of Experiments and Evaluation\n\nOur work is the first to explore instruction tuning on multimodal tasks and shows improved performance compared to baseline methods. However, there is still room for improvement, specifically in utilizing text-only instruction datasets. Future research could explore alternative architectures and stronger vision-language pre-trained models, or develop additional training loss functions to better utilize these unimodal instruction datasets. Additionally, we only used OFA as the baseline model as it was the largest open-source multimodal pretrained model available when we conducted this research. As more and stronger multimodal pretrained models being publicly available, it would be interesting to conduct a thorough comparison between models with different sizes. Finally, we take the first step to define sensitivity as a metric to evaluate the robustness of the models on understanding and following human-written instructions, which can be a potential standard metric for all the following instruction-tuning studies. However, it's only based on the variation of model performance across different instructions for the same task. In the future, we will consider more broad factors, e.g., the model's capability to understand different instructions for different tasks (Inter-task sensitivity), to further improve the sensitivity metric for instruction tuning.  Table 5 shows the distribution of input and output modalities for both training and evaluation tasks in MULTIINSTRUCT, and Table 6 shows the detailed statistics for all the training and evaluation tasks separately. Tables 7 to 9 provide a comprehensive list of the 62 tasks included in MULTIINSTRUCT, along with one example of instruction for each task.\n\n\nReferences\n\n\nA Tasks Defined in MULTIINSTRUCT\n\nInput modality Output Modality # of Training # of Testing Image Text Region Image Text Region  Grounded VQA (Zhu et al., 2016) requires models to answer the questions about an image, with the answers being specific visual regions within the image.\n\u2713 \u2713 1 0 \u2713 \u2713 \u2713 14 5 \u2713 \u2713 \u2713 9 1 \u2713 \u2713 \u2713 2 0 \u2713 \u2713 \u2713 3 1 \u2713 \u2713 \u2713 \u2713 9 0 \u2713 \u2713 \u2713 \u2713 1 0\nCommonsense VQA (Zellers et al., 2019) requires the model to answer a multiple-choice question that requires commonsense reasoning about an image. Both the question and answers are presented in a combination of natural language and references to specific image regions within the image.\n\nVisual Entailment (Xie et al., 2019) requires the model to determine whether the image semantically entails the text. OFA TaskName is finetuned on MULTIINSTRUCT but it does not use the instructions we created for the tasks. Instead, we prepend the task name to each input and use a semicolon to separate the task name and the input. For a fair comparison, we still keep the two special tokens \"[Options]\" and \"||||\" for the option field.\n\nOFA MultiInstruct only fine-tunes OFA on our newly introduced MULTIINSTRUCT dataset with instruction tuning.\n\nOFA NaturalInstruct only fine-tunes OFA on the large-scale NATURAL INSTRUCTIONS dataset (Mishra et al., 2022;Wang et al., 2022d) with instruction tuning. To ensure a fair comparison, we evaluate this baseline on instruction templates that removed all specific tokens, including \"[Options]\" and \"||||\", since the model being tested has not been exposed to these specific tokens during instruction-tuning. We want to ensure that the evaluation is not biased in favor of models that have seen these tokens during training.\n\nOFA MixedInstruct fine-tunes OFA on the mix of the large-scale NATURAL INSTRUCTIONS (Mishra et al., 2022;Wang et al., 2022d) and MULTIIN-STRUCT dataset with instruction tuning.\n\nOFA SeqInstruct sequentially fine-tunes OFA on the large-scale NATURAL INSTRUCTIONS (Mishra et al., 2022;Wang et al., 2022d) and MULTIIN-STRUCT dataset with instruction tuning.\n\n\nB.4 Training Details\n\nWe set the maximum length of input tokens to 1024 and the maximum target length to 512. For image preprocessing, we strictly follow the process in the OFA. Please refer to the original paper for more details. We train the models on 8 Nvidia A100 GPUs with a batch size 8 per GPU, a learning rate of 1e-05, and float16 enabled for 3 epochs for all the setups and datasets. We run all the experiments once.\n\n\nC Attention Analysis\n\nIn Section 6.1, we have demonstrated that finetuning OFA with NATURAL INSTRUCTIONS alone results in a decline in its zero-shot performance. In this section, we examine one possible reason for this decline by examining if fine-tuning the model on a text-only instruction dataset causes it to give less attention to image inputs.\n\nTo understand this, we conduct an analysis of the self-attention layers within the OFA encoder. The OFA encoder comprises 12 selfattention layers, each with 16 attention heads. We denote the input to self-attention layer l as h (l) = [x For each self-attention layer, we first compute the attention given to the image states in relation to text states for each attention head. Specifically, for each text state as the query, we sum its attention scores on image states (i.e. the attention scores where the text state is the query and image states are the keys). We then compute the text-to-image attention across all text states. Finally, we average the text-to-image across all attention heads. This results in a text-to-image attention score for each self-attention layer. Figure 5 illustrates the results of text-to-image attention scores on three unseen multimodal tasks: Text VQA, Visual Entailment, and Visual Text Extraction. The results on all three unseen tasks show that, in all self-attention layers of the OFA encoder, OFA NaturalInstruct has significantly lower text-to-image attention scores compared to other models. This decrease is particularly pronounced in the first two self-attention layers. This suggests that fine-tuning the model on a text-only instruction dataset leads to a reduction in the attention paid to image inputs, which may explain the decline in zero-shot performance.      A2. Did you discuss any potential risks of your work? Not applicable. Left blank.\n\nA3. Do the abstract and introduction summarize the paper's main claims?\n\nabstract, section 1 A4. Have you used AI writing assistants when working on this paper?\n\nLeft blank.\n\nB Did you use or create scientific artifacts?\n\nLeft blank.\n\nB1. Did you cite the creators of artifacts you used? No response.\n\nB2. Did you discuss the license or terms for use and / or distribution of any artifacts? No response.\n\nB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)? No response.\n\nB4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? No response.\n\nB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? No response.\n\nB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.\n\nNo response.\n\nC Did you run computational experiments?\n\nsection 5, 6, 7, appendix B C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used? appendix B.4\n\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.\n\nC2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values? appendix B.4\n\nC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run? section 5\n\nC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)? section 5 D Did you use human annotators (e.g., crowdworkers) or research with human participants? section 3 D1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.? section 3\n\nD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)?\n\nWe don't have paid participants involved D3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?\n\nThe data is publically available D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\n\nThe data is collected from publicly available benchmark datasets and there is no potential ethical issue.\n\nD5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data? Not applicable. Left blank.\n\nFigure 1 :\n1Example Instances from MULTIINSTRUCT for Four Tasks.\n\n\nFigure 1 provides several instruction examples for the tasks included in MULTIINSTRUCT.\n\nFigure 2 :\n2Task Groups Included in MULTIINSTRUCT. The yellow boxes represent tasks used for evaluation, while the white boxes indicate tasks used for training.\n\nFigure 3 :\n3Model Performance as the Number of Multimodal Instruction Task Clusters Increases. The number in the parenthesis of each cluster denotes the number of tasks.\n\nFigure 4 :\n4Model Sensitivity on Unseen Evaluation Tasks. Lower is better.\n\nL\n], where L is the length of sequence. The input h I+T ] to the first selfattention layer is actually the concatenation of image embeddings and text embeddings, where I, T is the length of image and text embeddings respectively. For ease of understanding and simplicity, we have altered the naming conventions and refer to x l p , p = [1, ..., I] as image states and x l p , p = [I + 1, ..., I + T ] as text states.\n\nFigure 5 :\n5Text-to-Image Attention of OFA Encoder.\n\n\nEvaluation datasets We evaluate the models on nine unseen multimodal tasks: Text VQA (Singh et al., 2019), Grounded VQA (Zhu et al., 2016), Commonsense VQA (Zellers et al., 2019), Visual Entailment (Xie et al., 2019), Visual Spatial Reasoning (Liu et al., 2022a), Natural Language for Visual Reasoning (NLVR) (Suhr et al., 2017), Visual Text Extraction (Kiela et al., 2020), Visual Dialogue (Das et al., 2017), and Disaster Type Classification (Alam et al., 2022). These tasks belong to three task groups: Commonsense Reasoning, VQA, and Miscellaneous as shown in Figure 2. Tasks in the Commonsense Reasoning group have no overlap with any training task groups. Tasks in Miscellaneous do not share similarities with other tasks in the group. Although Text VQA and GroundedVQA belong to the VQA task group, they require additional skills such as extracting text from images or generating regions, making them fundamentally different from other tasks in VQA. In addition to multimodal tasks, we also evaluate the model on 20 NLP tasks collected from the test split of NATURAL INSTRUCTIONS.Approaches for ComparisonWe denote the OFA finetuned on MULTIINSTRUCT as OFA MultiInstruct , and compare it with the original pre-trained OFA 5 , OFA TaskName which is fine-tuned on MULTIINSTRUCT but uses the task name instead of instruction to guide the model to make predictions, and several approaches that leverage the large-scale NATURAL INSTRUCTIONS dataset, including OFA NaturalInstruct which only fine-tunes OFA on NATURAL INSTRUCTIONS with instruction tuning, OFA MixedInstruct and OFA SeqInstruct that are specified in Section 4.2.More details regarding the evaluation datasets, baseline approaches and training details can be found in Appendix B.\n\nTable 1 :\n1Additionally, OFA MultiInstruct outperforms OFA TaskName on all unseen tasks, particularly on the Grounded VQA task, where OFA TaskName achieves nearly zero per-Zero-shot Performance on Multimodal Commonsense Reasoning. The best performance is in bold.Model \n\nCommonsense VQA \nVisual Entailment \nVisual Spatial Reasoning \nNLVR \nRougeL \nACC \nACC \nACC \nACC \nMax \nAvg \u00b1 Std \nMax \nAvg \u00b1 Std \nMax \nAvg\u00b1 Std \nMax \nAvg\u00b1 Std \nMax \nAvg\u00b1 Std \n\nOFA \n17.93 14.97 \u00b1 4.30 0.73 \n0.40 \u00b10.29 \n49.99 41.86 \u00b1 10.99 \n54.99 \n35.29 \u00b1 22.21 \n56.06 52.10 \u00b1 3.35 \nOFA TaskName \n48.99 \n-\n29.01 \n-\n55.70 \n-\n53.76 \n-\n55.35 \n-\nOFA MultiInstruct \n52.01 50.60 \u00b1 1.12 33.01 31.17 \u00b1 1.59 \n55.96 55.06 \u00b10.76 \n55.81 \n53.90 \u00b11.38 \n56.97 56.18 \u00b1 0.95 \n\nTransfer Learning from NATURAL INSTRUCTIONS \nOFA NaturalInstruct 27.15 14.99 \u00b1 9.12 7.35 \n2.04 \u00b1 3.01 \n33.28 14.86 \u00b1 16.68 \n51.44 \n36.44 \u00b1 20.72 \n56.06 35.98 \u00b1 21.64 \nOFA MixedInstruct 50.40 49.34 \u00b1 1.04 31.31 30.27 \u00b1 0.94 \n54.63 53.74 \u00b1 0.97 \n55.13 \n52.61 \u00b1 1.64 \n56.67 55.96 \u00b1 0.48 \nOFA SeqInstruct \n50.93 50.07 \u00b1 1.07 32.28 31.23 \u00b1 1.09 \n53.66 52.98 \u00b1 0.56 \n54.86 \n53.11 \u00b1 1.45 \n57.58 56.63 \u00b1 0.66 \n\nModel \nText VQA \nGrounded VQA \nVisual Text Extraction \nVisual Dialogue \nDisaster Type Classification \nRougeL \nAcc \nRougeL \nRougeL \nACC \nMax \nAvg\u00b1 Std \nMax \nAvg\u00b1 Std \nMax \nAvg\u00b1 Std \nMax \nAvg \u00b1 Std \nMax \nAvg \u00b1 Std \n\nOFA \n15.21 9.30 \u00b1 5.42 \n0.02 \n0.00 \u00b1 0.01 \n36.31 17.62 \u00b1 16.82 \n45.46 28.71 \u00b1 9.81 \n14.30 \n9.64 \u00b1 4.34 \nOFA TaskName \n23.80 \n-\n0.00 \n-\n36.30 \n-\n25.18 \n-\n62.65 \n-\nOFA MultiInstruct \n27.22 26.46 \u00b1 0.83 \n64.32 47.22 \u00b1 23.08 \n74.35 62.43 \u00b111.56 \n46.38 32.91 \u00b17.59 \n64.88 \n56.00 \u00b112.96 \n\nTransfer Learning from NATURAL INSTRUCTIONS \nOFA NaturalInstruct 5.59 \n5.40 \u00b1 0.24 \n0.00 \n0.00 \u00b1 0.00 \n5.65 \n1.24 \u00b1 2.48 \n30.94 27.91 \u00b1 2.16 \n56.64 \n38.21 \u00b1 15.35 \nOFA MixedInstruct 24.15 23.67 \u00b1 0.47 \n63.79 54.99 \u00b1 18.16 \n62.43 46.56 \u00b1 14.92 \n46.08 38.02 \u00b1 5.25 \n68.31 \n64.31 \u00b1 2.39 \nOFA SeqInstruct \n27.03 26.67 \u00b1 0.47 \n64.19 54.46 \u00b1 15.96 \n71.63 60.62 \u00b1 12.31 \n46.17 35.10 \u00b1 6.92 \n64.46 \n57.89 \u00b1 9.51 \n\n\n\nTable 2 :\n2Zero-shot Performance on Question Answering and Miscellaneous. The best performance is in bold.\n\nTable 3 :\n3Effect of Different Number of Instructions. Performance of OFA MultiInstruct finetuned on different numbers of instructions.\n\n\nKushal Kafle and Christopher Kanan. 2017. An analysis of visual question answering algorithms. In Proceedings of the IEEE international conference on computer vision, pages 1965-1973.Armen Aghajanyan, Anchit Gupta, Akshat Shrivastava, \nXilun Chen, Luke Zettlemoyer, and Sonal Gupta. \n2021. Muppet: Massive multi-task representations \nwith pre-finetuning. In Proceedings of the 2021 Con-\nference on Empirical Methods in Natural Language \nProcessing, pages 5799-5811, Online and Punta \nCana, Dominican Republic. Association for Com-\nputational Linguistics. \n\nFiroj Alam, Tanvirul Alam, Md Hasan, Abul Hasnat, \nMuhammad Imran, Ferda Ofli, et al. 2022. Medic: a \nmulti-task learning dataset for disaster image classifi-\ncation. Neural Computing and Applications, pages \n1-24. \n\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-\ntoine Miech, Iain Barr, Yana Hasson, Karel Lenc, \nArthur Mensch, Katie Millican, Malcolm Reynolds, \net al. 2022. Flamingo: a visual language model for \nfew-shot learning. arXiv preprint arXiv:2204.14198. \n\nHangbo Bao, Li Dong, Songhao Piao, and Furu Wei. \n2022. Beit: Bert pre-training of image transformers. \nIn ICLR 2022. \n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie \nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind \nNeelakantan, Pranav Shyam, Girish Sastry, Amanda \nAskell, et al. 2020. Language models are few-shot \nlearners. Advances in neural information processing \nsystems, 33:1877-1901. \n\nTai-Yin Chiu, Yinan Zhao, and Danna Gurari. 2020. \nAssessing image quality issues for real-world prob-\nlems. In Proceedings of the IEEE/CVF Conference \non Computer Vision and Pattern Recognition, pages \n3646-3656. \n\nJaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. 2021. \nUnifying vision-and-language tasks via text genera-\ntion. In International Conference on Machine Learn-\ning, pages 1931-1942. PMLR. \n\nAbhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, \nDeshraj Yadav, Jos\u00e9 MF Moura, Devi Parikh, and \nDhruv Batra. 2017. Visual dialog. In Proceedings of \nthe IEEE conference on computer vision and pattern \nrecognition, pages 326-335. \n\nPatrick Esser, Robin Rombach, and Bjorn Ommer. 2021. \nTaming transformers for high-resolution image syn-\nthesis. In Proceedings of the IEEE/CVF conference \non computer vision and pattern recognition, pages \n12873-12883. \n\nJort F. Gemmeke, Daniel P. W. Ellis, Dylan Freedman, \nAren Jansen, Wade Lawrence, R. Channing Moore, \nManoj Plakal, and Marvin Ritter. 2017. Audio set: \nAn ontology and human-labeled dataset for audio \nevents. In 2017 IEEE International Conference on \nAcoustics, Speech and Signal Processing, ICASSP \n2017, New Orleans, LA, USA, March 5-9, 2017, \npages 776-780. IEEE. \n\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv \nBatra, and Devi Parikh. 2017. Making the v in vqa \nmatter: Elevating the role of image understanding \nin visual question answering. In Proceedings of the \nIEEE conference on computer vision and pattern \nrecognition, pages 6904-6913. \n\nPrakhar Gupta, Cathy Jiao, Yi-Ting Yeh, Shikib Mehri, \nMaxine Eskenazi, and Jeffrey P. Bigham. 2022. Im-\nproving zero and few-shot generalization in dialogue \nthrough instruction tuning. \n\nXu Han, Weilin Zhao, Ning Ding, Zhiyuan Liu, and \nMaosong Sun. 2022. Ptr: Prompt tuning with rules \nfor text classification. AI Open. \n\nDrew A Hudson and Christopher D Manning. 2019. \nGqa: A new dataset for real-world visual reasoning \nand compositional question answering. In Proceed-\nings of the IEEE/CVF conference on computer vision \nand pattern recognition, pages 6700-6709. \n\nCatalin Ionescu, Dragos Papava, Vlad Olaru, and Cris-\ntian Sminchisescu. 2014. Human3.6m: Large scale \ndatasets and predictive methods for 3d human sens-\ning in natural environments. IEEE Trans. Pattern \nAnal. Mach. Intell., 36(7):1325-1339. \n\nDouwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj \nGoswami, Amanpreet Singh, Pratik Ringshia, and \nDavide Testuggine. 2020. The hateful memes chal-\nlenge: Detecting hate speech in multimodal memes. \nAdvances in Neural Information Processing Systems, \n33:2611-2624. \n\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin John-\nson, Kenji Hata, Joshua Kravitz, Stephanie Chen, \nYannis Kalantidis, Li-Jia Li, David A Shamma, et al. \n2017. Visual genome: Connecting language and vi-\nsion using crowdsourced dense image annotations. \nInternational journal of computer vision, 123(1):32-\n73. \n\nHao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, Hong-\nsheng Li, Chun Yuan, Xiaohua Wang, Yu Qiao, \nXiaogang Wang, Wenhai Wang, and Jifeng Dai. \n2022a. Uni-perceiver v2: A generalist model for \nlarge-scale vision and vision-language tasks. CoRR, \nabs/2211.09808. \n\nLinjie Li, Zhe Gan, Kevin Lin, Chung-Ching Lin, \nZicheng Liu, Ce Liu, and Lijuan Wang. 2022b. \nLavender: Unifying video-language understanding \nas masked language modeling. arXiv preprint \narXiv:2206.07160. \n\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning: \nOptimizing continuous prompts for generation. In \nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th \nInternational Joint Conference on Natural Language \nYuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-\nFei. 2016. Visual7w: Grounded question answering \nin images. In Proceedings of the IEEE conference \non computer vision and pattern recognition, pages \n4995-5004. \n\n\n\nTable 5 :\n5Distribution of input and output modalities for all the tasks in MULTIINSTRUCT.Train Eval \n\n\n\nTable 6 :\n6Detailed statistics in MULTIINSTRUCT.B More Details for Experimental Setup \n\nB.1 Multimodal Evaluation Datasets \n\nText VQA (Singh et al., 2019) requires models \nto read and reason about the text in an image to \nanswer questions based on them. \n\n\n\nAnswer the question <QUESTION> based on the content of the given image.\u271311459 \n\nCategory \n\nTask Name \nDataset \nDescription \nExist \n\nVQA \n\nOpen-Domain \nVQA \nVQAv2 \n(Goyal \net al., 2017), Visual \nGenome \n(Krishna \net al., 2017) \n\nVQA \nVisual7w (Zhu et al., \n2016) \nAnswer a visual question <QUESTION> by selecting an answer from given \noptions. <OPTION> \n\u2713 \n\nCompositional \nVQA \nGQA (Hudson and \nManning, 2019) \nAnswer a compositional question based on the content of the given image. \nQuestion: <QUESTION> \n\u2713 \n\nOutside Knowl-\nedge VQA \nOK-VQA \n(Marino \net al., 2019) \nBased on your knowledge, <QUESTION>? \n\u2713 \n\nGrounded \nGeneration \n\nGrounded Cap-\ntioning \nVisual Genome (Kr-\nishna et al., 2017) \nGiven the region <REGION> in the image, generate a caption for that region. \n\u2713 \n\nVisual Ground-\ning \nVisual Genome (Kr-\nishna et al., 2017) \nGiven a caption <TEXT> for some region in the image, identify the region \nand generate its bounding box. \n\u2713 \n\nGrounded \nObject Identifi-\ncation \n\nMSCOCO (Lin et al., \n2014) \nIdentify the type of an object in <REGION>. \n\u2713 \n\nObject Ground-\ning \nMSCOCO (Lin et al., \n2014) \nWhat are the regions containing the object [TEXT]? \n\u00d7 \n\nReferring \nExpression \nGrounding \n\nRefCOCO (Yu et al., \n2016) \nLocate a region in an image based on the referring expression [TEXT]. \n\u2713 \n\nReferring \nExpression \nGeneration \n\nRefCOCO (Yu et al., \n2016) \nGenerate the referring expression for an object in region <REGION>. \n\u2713 \n\nText Localiza-\ntion \nCOCO-Text (Veit et al., \n2016) \nSelect a region from options that contain the text <TEXT> in the image. \n<OPTION> \n\u2713 \n\nRegion \nUnderstanding \n\nMost-\nOverlapping \nRegion Selec-\ntion \n\nVisual Genome (Kr-\nishna et al., 2017) \nGiven the region <REGION>, decide which region in the options overlaps \nmost with given region. <OPTION> \n\u00d7 \n\nNon-\nOverlapping \nRegion Selec-\ntion \n\nVisual Genome (Kr-\nishna et al., 2017) \nWhich option does not share common area with <REGION>? <OPTION> \n\u00d7 \n\nLeast-\nOverlapping \nRegion Selec-\ntion \n\nVisual Genome (Kr-\nishna et al., 2017) \n\"Which option has the least shared area with <REGION>?<OPTION> \n\u00d7 \n\nOverlapping Re-\ngion Selection \nVisual Genome (Kr-\nishna et al., 2017) \nWhich region from options that has common area with <REGION>? \n<OPTION> \n\u00d7 \n\nRegion Overlap-\nping Detection \nVisual Genome (Kr-\nishna et al., 2017) \nDoes <REGION1> share common area with <REGION2>? <OPTION> \n\u00d7 \n\nRegion Area \nVisual Genome (Kr-\nishna et al., 2017) \nCompute the area of <REGION>. \n\u00d7 \n\nGrounded \nMatching \n\nRegion-Caption \nMatching \nVisual Genome (Kr-\nishna et al., 2017) \nDecide if the caption matches the given region <REGION> in the image. \n\u00d7 \n\nGrounded Cap-\ntion Selection \nVisual Genome (Kr-\nishna et al., 2017) \nGiven a region <REGION> in the image, select a caption from given options \nfor that region. <OPTION> \n\u00d7 \n\nVisual Ground-\ning Selection \nVisual Genome (Kr-\nishna et al., 2017) \nGiven a caption <TEXT> for some region in the image, select the region \nfrom the options. <OPTION> \n\u00d7 \n\nReferring \nExpression \nSelection \n\nRefCOCO (Yu et al., \n2016) \nSelect a region from options based on the referring expression <TEXT>. \n<OPTION> \n\u00d7 \n\nObject-Region \nMatching \nMSCOCO (Lin et al., \n2014) \nDoes region <REGION> contain the object <TEXT>? \n\u00d7 \n\nObject-Region \nSelection \nMSCOCO (Lin et al., \n2014) \nSelect the region containing the given object <TEXT>. <OPTION> \n\u00d7 \n\nObject Match-\ning \nMSCOCO (Lin et al., \n2014) \nDo objects in region <REGION1> and region <REGION2> have the same \ntype? \n\u00d7 \n\nMissing Object \nSelection \nMSCOCO (Lin et al., \n2014) \nSelect an object from options that does not appear in any of the given \nregions <REGION>. <OPTION> \n\u00d7 \n\nRegion-Text \nMatching \nCOCO-Text (Veit et al., \n2016) \nDoes region <REGION> contain the text <TEXT>? \n\u00d7 \n\n\n\nTable 7 :\n7Detailed Group of Training Tasks Included in MULTIINSTRUCT. The complete list of 53 multi-modal tasks, along with examples of the instructions for each task. The existing tasks are indicated with \u2713, while the newly derived tasks are indicated using \u00d7. Answer the question: <QUESTION> based on the color of an object.<OPTION> \u2713 This task asks you to identify if an object appears in the image. <QUESTION><OPTION> \u2713 In this task you are asked a question about the type of an object in the image. <QUESTION><OPTION>\u2713 <QUESTION> Please answer the question by counting the object mentioned in the question. <OPTION>\u2713 <QUESTION><OPTION> Please answer the question by interpreting the sentiment in the image.\u2713 In this task, you need to analyze the position of objects in an image and answer the following question. <QUESTION><OPTION> \u2713 Please take a look at the picture and answer the following question by thinking about what each object in the picture can be used for. <QUESTION><OPTION> Select a reason from the options to explain why the image quality is bad.<OPTION> \u2713 Given the subject in region <REGION>, what is the object that has a relationship <TEXT> with that subject? \u00d7 Visual Subject Identification Visual Genome (Krishna et al., 2017) Given the object in region <REGION>, what is the subject that has a relationship <TEXT> with that object? \u00d7 Visual Object Localization Visual Genome (Krishna et al., 2017) Given the subject in region <REGION>, where is the object in the image that has relationship <TEXT> with the subject? \u00d7 Visual Subject Localization Visual Genome (Krishna et al., 2017) Given the object in region <REGION>, where is the subject in the image that has relationship <TEXT> with the object? \u00d7 Decide which option is the attribute of the object in the region <REGION>. <OPTION> \u2713 Decide if the image contains an answer to the question <QUESTION>. \u00d7 Select the text that best matches the image. <OPTION> \u00d7 Decide if the claim can be supported by the given image and the context. \u2713 Text Legibility COCO-Text (Veit et al., 2016)Decide if the text in the given region is legible. \u2713 For task <TASK>, given the history steps and the current step with its corresponding image, what is the next step for this task? <HISTORY> <TASK>, select the immediate next step to the step specified by the image. \u00d7WikiHowFor the task <TASK>, given the current step <STEP>, decide if the content of the image is the next or previous step. \u00d7WikiHowFor the task <TASK>, given the current step specified by the image, decide if the step <STEP> is the next or previous step. \u00d711460 \n\n\n\nTable 8 :\n8(Continued) Detailed Group of Training Tasks Included in MULTIINSTRUCT. The complete list of 53 multi-modal tasks, along with examples of the instructions for each task. The existing tasks are indicated with \u2713, while the newly derived tasks are indicated using \u00d7. There is some text on the image. Answer <QUESTION> based on the text in the image. \u2713 Decide if the sentence <TEXT> correctly describes the geometric relationships of objects in a synthesized image. \u2713 Visual Spatial Reasoning VSR (Liu et al., 2022a) Decide if the proposed spatial relationship between two objects in an image is \"True\" or \"False\" \u2713 Visual Entailment SNLI-VE (Xie et al., 2019) Can you conclude <TEXT> from the content of image? Select your answer from the options. <OPTION> \u2713 Look at the image and the regions in the question, <QUESTION>? <OPTION>.\u2713 What disaster happens in the image? <OPTION> \u271311461 \n\n\n\nTable 9 :\n9Detailed Group of Evaluation Tasks Included in MULTIINSTRUCT. The complete list of 9 multi-modal tasks, along with examples of the instructions for each task. The existing tasks are indicated with \u2713, while the newly derived tasks are indicated using \u00d7.Dataset Name \n\nTask Name \n\nConceptual Caption 12M (CC12M) \nImage Captioning \nConceptual Captions (CC3M) \nImage Captioning \nMSCOCO image captions (COCO) \nImage Captioning \nVisual Genome Captions (VG Captions) \nImage Captioning \nVQAv2 \nVisual Question Answering \nVG-QA ( COCO) \nVisual Question Answering \nGQA (VG) \nVisual Question Answering \nRefCOCO \nVisual Grounding \nRefCOCO+ \nVisual Grounding \nRefCOCOg \nVisual Grounding \nVG captions \nVisual Grounded Captioning \nOpenImages \nObject Detection \nObject365 \nObject Detection \nVG \nObject Detection \nCOCO \nObject Detection \nOpenImages \nImage Infilling \nYFCC100M \nImage Infilling \nImageNet-21K \nImage Infilling \n\n\n\nTable 10 :\n10Multimodal Pre-training Tasks in OFA. ACL 2023 Responsible NLP Checklist A For every submission: A1. Did you describe the limitations of your work?11463 \n\n\nWe use OFA as it was the largest and most powerful open-source multimodal pre-trained model available at the time of our research while other stronger models didn't have publicly available checkpoints at that time.\nhttps://ofa-beijing.oss-cn-beijing. aliyuncs.com/checkpoints/ofa_large.pt\nhttps://github.com/allenai/ natural-instructions\nhttps://ofa-beijing.oss-cn-beijing. aliyuncs.com/checkpoints/ofa_large.pt\nAcknowledgmentsThis research is based upon work supported by the U.S. DARPA KMASS Program # HR001121S0034. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of DARPA or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.\nProcessing, ACL/IJCNLP 2021Virtual Event. Association for Computational Linguistics1Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 4582- 4597. Association for Computational Linguistics.\n\nRouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Text summarization branches out. Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74-81.\n\nMicrosoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, C Lawrence Zitnick, European conference on computer vision. SpringerTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. 2014. Microsoft coco: Common objects in context. In European confer- ence on computer vision, pages 740-755. Springer.\n\nFangyu Liu, Guy Emerson, Nigel Collier, arXiv:2205.00363Visual spatial reasoning. arXiv preprintFangyu Liu, Guy Emerson, and Nigel Collier. 2022a. Visual spatial reasoning. arXiv preprint arXiv:2205.00363.\n\nMohit Bansal, and Colin Raffel. 2022b. Few-shot parameter-efficient finetuning is better and cheaper than in-context learning. Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, 10.48550/arXiv.2205.05638abs/2205.05638CoRRHaokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin Raffel. 2022b. Few-shot parameter-efficient fine- tuning is better and cheaper than in-context learning. CoRR, abs/2205.05638.\n\nPretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig, arXiv:2107.13586arXiv preprintPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021. Pre- train, prompt, and predict: A systematic survey of prompting methods in natural language processing. arXiv preprint arXiv:2107.13586.\n\nUnifiedio: A unified model for vision, language, and multimodal tasks. Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, Aniruddha Kembhavi, 10.48550/ARXIV.2206.08916Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. 2022. Unified- io: A unified model for vision, language, and multi- modal tasks.\n\nOk-vqa: A visual question answering benchmark requiring external knowledge. Kenneth Marino, Mohammad Rastegari, Ali Farhadi, Roozbeh Mottaghi, Proceedings of the IEEE/cvf conference on computer vision and pattern recognition. the IEEE/cvf conference on computer vision and pattern recognitionKenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. 2019. Ok-vqa: A visual ques- tion answering benchmark requiring external knowl- edge. In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition, pages 3195-3204.\n\nSewon Min, Mike Lewis, Luke Zettlemoyer, Hannaneh Hajishirzi, arXiv:2110.15943Metaicl: Learning to learn in context. arXiv preprintSewon Min, Mike Lewis, Luke Zettlemoyer, and Han- naneh Hajishirzi. 2021. Metaicl: Learning to learn in context. arXiv preprint arXiv:2110.15943.\n\nCross-task generalization via natural language crowdsourcing instructions. Swaroop Mishra, Daniel Khashabi, Chitta Baral, Hannaneh Hajishirzi, 10.18653/v1/2022.acl-long.244Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics1Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2022. Cross-task generaliza- tion via natural language crowdsourcing instructions. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3470-3487, Dublin, Ireland. Association for Computational Linguistics.\n\nLibrispeech: An ASR corpus based on public domain audio books. Vassil Panayotov, Guoguo Chen, Daniel Povey, Sanjeev Khudanpur, 10.1109/ICASSP.2015.71789642015 IEEE International Conference on Acoustics, Speech and Signal Processing. South Brisbane, Queensland, AustraliaIEEE2015Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. 2015. Librispeech: An ASR corpus based on public domain audio books. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2015, South Brisbane, Queensland, Australia, April 19-24, 2015, pages 5206-5210. IEEE.\n\nLearning to predict visual attributes in the wild. Khoi Pham, Kushal Kafle, Zhe Lin, Zhihong Ding, Scott Cohen, Quan Tran, Abhinav Shrivastava, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionKhoi Pham, Kushal Kafle, Zhe Lin, Zhihong Ding, Scott Cohen, Quan Tran, and Abhinav Shrivastava. 2021. Learning to predict visual attributes in the wild. In Proceedings of the IEEE/CVF Conference on Com- puter Vision and Pattern Recognition, pages 13018- 13028.\n\nFinetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Du, abs/2109.01652M. Dai, and Quoc V. Le. 2021CoRRJason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, An- drew M. Dai, and Quoc V. Le. 2021. Finetuned language models are zero-shot learners. CoRR, abs/2109.01652.\n\nNing Xie, Farley Lai, Derek Doran, Asim Kadav, arXiv:1901.06706Visual entailment: A novel task for fine-grained image understanding. arXiv preprintNing Xie, Farley Lai, Derek Doran, and Asim Ka- dav. 2019. Visual entailment: A novel task for fine-grained image understanding. arXiv preprint arXiv:1901.06706.\n\nAn explanation of in-context learning as implicit bayesian inference. Sang Michael Xie, Aditi Raghunathan, Percy Liang, Tengyu Ma, abs/2111.02080CoRRSang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. 2021. An explanation of in-context learning as implicit bayesian inference. CoRR, abs/2111.02080.\n\nEnd-to-end multimodal fact-checking and explanation generation: A challenging dataset and models. Aditya Barry Menglong Yao, Lichao Shah, Jin-Hee Sun, Lifu Cho, Huang, arXiv:2205.12487arXiv preprintBarry Menglong Yao, Aditya Shah, Lichao Sun, Jin- Hee Cho, and Lifu Huang. 2022. End-to-end mul- timodal fact-checking and explanation generation: A challenging dataset and models. arXiv preprint arXiv:2205.12487.\n\nEnd-to-end spoken conversational question answering: Task, dataset and model. Chenyu You, Nuo Chen, Fenglin Liu, Shen Ge, Xian Wu, Yuexian Zou, 10.18653/v1/2022.findings-naacl.91Findings of the Association for Computational Linguistics: NAACL 2022. Seattle, WA, United StatesAssociation for Computational LinguisticsChenyu You, Nuo Chen, Fenglin Liu, Shen Ge, Xian Wu, and Yuexian Zou. 2022. End-to-end spoken con- versational question answering: Task, dataset and model. In Findings of the Association for Computa- tional Linguistics: NAACL 2022, Seattle, WA, United States, July 10-15, 2022, pages 1219-1232. Associa- tion for Computational Linguistics.\n\nModeling context in referring expressions. Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, Tamara L Berg, European Conference on Computer Vision. SpringerLicheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. 2016. Modeling context in referring expressions. In European Conference on Computer Vision, pages 69-85. Springer.\n\nFrom recognition to cognition: Visual commonsense reasoning. Rowan Zellers, Yonatan Bisk, Ali Farhadi, Yejin Choi, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionRowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. From recognition to cognition: Vi- sual commonsense reasoning. In Proceedings of the IEEE/CVF conference on computer vision and pat- tern recognition, pages 6720-6731.\n", "annotations": {"author": "[{\"end\":200,\"start\":129},{\"end\":268,\"start\":201},{\"end\":337,\"start\":269}]", "publisher": "[{\"end\":91,\"start\":80},{\"end\":589,\"start\":578}]", "author_last_name": "[{\"end\":139,\"start\":137},{\"end\":210,\"start\":206},{\"end\":279,\"start\":274}]", "author_first_name": "[{\"end\":136,\"start\":129},{\"end\":205,\"start\":201},{\"end\":273,\"start\":269}]", "author_affiliation": "[{\"end\":199,\"start\":157},{\"end\":267,\"start\":225},{\"end\":336,\"start\":294}]", "title": "[{\"end\":79,\"start\":1},{\"end\":416,\"start\":338}]", "venue": "[{\"end\":505,\"start\":418}]", "abstract": "[{\"end\":1958,\"start\":606}]", "bib_ref": "[{\"end\":2129,\"start\":2109},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2146,\"start\":2129},{\"end\":2163,\"start\":2146},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2180,\"start\":2163},{\"end\":2330,\"start\":2329},{\"end\":2472,\"start\":2454},{\"end\":2820,\"start\":2800},{\"end\":2841,\"start\":2820},{\"end\":2858,\"start\":2841},{\"end\":2877,\"start\":2858},{\"end\":3350,\"start\":3330},{\"end\":3368,\"start\":3350},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3414,\"start\":3392},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3431,\"start\":3414},{\"end\":3489,\"start\":3467},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4130,\"start\":4109},{\"end\":5553,\"start\":5527},{\"end\":5571,\"start\":5553},{\"end\":6249,\"start\":6227},{\"end\":6266,\"start\":6249},{\"end\":6285,\"start\":6266},{\"end\":6306,\"start\":6285},{\"end\":6325,\"start\":6306},{\"end\":6344,\"start\":6325},{\"end\":6440,\"start\":6422},{\"end\":6461,\"start\":6440},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6477,\"start\":6461},{\"end\":6628,\"start\":6604},{\"end\":6768,\"start\":6744},{\"end\":6903,\"start\":6876},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7472,\"start\":7454},{\"end\":7491,\"start\":7472},{\"end\":7508,\"start\":7491},{\"end\":7527,\"start\":7508},{\"end\":7545,\"start\":7527},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7844,\"start\":7826},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7861,\"start\":7844},{\"end\":7966,\"start\":7948},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8109,\"start\":8088},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8381,\"start\":8360},{\"end\":8400,\"start\":8381},{\"end\":8435,\"start\":8402},{\"end\":9313,\"start\":9293},{\"end\":9334,\"start\":9313},{\"end\":9351,\"start\":9334},{\"end\":9376,\"start\":9351},{\"end\":9395,\"start\":9376},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9414,\"start\":9395},{\"end\":9457,\"start\":9414},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9475,\"start\":9457},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9496,\"start\":9475},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9512,\"start\":9496},{\"end\":9557,\"start\":9512},{\"end\":9602,\"start\":9565},{\"end\":9619,\"start\":9602},{\"end\":9663,\"start\":9619},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9679,\"start\":9663},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9696,\"start\":9679},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9736,\"start\":9718},{\"end\":9754,\"start\":9736},{\"end\":9796,\"start\":9754},{\"end\":9814,\"start\":9796},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9829,\"start\":9814},{\"end\":9873,\"start\":9829},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9891,\"start\":9873},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9987,\"start\":9969},{\"end\":10006,\"start\":9987},{\"end\":10023,\"start\":10006},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10040,\"start\":10023},{\"end\":10058,\"start\":10040},{\"end\":10076,\"start\":10058},{\"end\":14816,\"start\":14793},{\"end\":14902,\"start\":14878},{\"end\":16326,\"start\":16308},{\"end\":17893,\"start\":17892},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":18098,\"start\":18077},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":19530,\"start\":19519},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":20073,\"start\":20053},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":29933,\"start\":29909},{\"end\":29954,\"start\":29933},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":29971,\"start\":29954},{\"end\":31918,\"start\":31900},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":32151,\"start\":32129},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":32437,\"start\":32419},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":33059,\"start\":33038},{\"end\":33078,\"start\":33059},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":33576,\"start\":33555},{\"end\":33595,\"start\":33576},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":33754,\"start\":33733},{\"end\":33773,\"start\":33754}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":39973,\"start\":39908},{\"attributes\":{\"id\":\"fig_1\"},\"end\":40063,\"start\":39974},{\"attributes\":{\"id\":\"fig_2\"},\"end\":40225,\"start\":40064},{\"attributes\":{\"id\":\"fig_3\"},\"end\":40396,\"start\":40226},{\"attributes\":{\"id\":\"fig_4\"},\"end\":40472,\"start\":40397},{\"attributes\":{\"id\":\"fig_5\"},\"end\":40890,\"start\":40473},{\"attributes\":{\"id\":\"fig_6\"},\"end\":40943,\"start\":40891},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":42691,\"start\":40944},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":44726,\"start\":42692},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":44834,\"start\":44727},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":44971,\"start\":44835},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":50275,\"start\":44972},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":50380,\"start\":50276},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":50637,\"start\":50381},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":54380,\"start\":50638},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":56976,\"start\":54381},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":57873,\"start\":56977},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":58795,\"start\":57874},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":58965,\"start\":58796}]", "paragraph": "[{\"end\":3142,\"start\":1974},{\"end\":3805,\"start\":3144},{\"end\":4255,\"start\":3807},{\"end\":4345,\"start\":4308},{\"end\":4418,\"start\":4356},{\"end\":4613,\"start\":4429},{\"end\":4835,\"start\":4624},{\"end\":4884,\"start\":4837},{\"end\":4933,\"start\":4886},{\"end\":5008,\"start\":4961},{\"end\":5492,\"start\":5019},{\"end\":6164,\"start\":5494},{\"end\":7235,\"start\":6181},{\"end\":8691,\"start\":7237},{\"end\":10291,\"start\":8747},{\"end\":11209,\"start\":10293},{\"end\":12000,\"start\":11211},{\"end\":12927,\"start\":12030},{\"end\":13370,\"start\":12929},{\"end\":13793,\"start\":13372},{\"end\":14244,\"start\":13795},{\"end\":14490,\"start\":14246},{\"end\":16195,\"start\":14528},{\"end\":16615,\"start\":16240},{\"end\":16959,\"start\":16672},{\"end\":17979,\"start\":16961},{\"end\":18394,\"start\":18027},{\"end\":18807,\"start\":18444},{\"end\":19417,\"start\":18860},{\"end\":20625,\"start\":19440},{\"end\":21372,\"start\":20627},{\"end\":21465,\"start\":21429},{\"end\":21841,\"start\":21550},{\"end\":22937,\"start\":21898},{\"end\":24305,\"start\":22995},{\"end\":25360,\"start\":24367},{\"end\":26232,\"start\":25417},{\"end\":27250,\"start\":26290},{\"end\":28289,\"start\":27289},{\"end\":28600,\"start\":28291},{\"end\":29503,\"start\":28615},{\"end\":29760,\"start\":29519},{\"end\":29972,\"start\":29762},{\"end\":31742,\"start\":30018},{\"end\":32039,\"start\":31792},{\"end\":32399,\"start\":32113},{\"end\":32838,\"start\":32401},{\"end\":32948,\"start\":32840},{\"end\":33469,\"start\":32950},{\"end\":33647,\"start\":33471},{\"end\":33825,\"start\":33649},{\"end\":34254,\"start\":33850},{\"end\":34606,\"start\":34279},{\"end\":36099,\"start\":34608},{\"end\":36172,\"start\":36101},{\"end\":36261,\"start\":36174},{\"end\":36274,\"start\":36263},{\"end\":36321,\"start\":36276},{\"end\":36334,\"start\":36323},{\"end\":36401,\"start\":36336},{\"end\":36504,\"start\":36403},{\"end\":36888,\"start\":36506},{\"end\":37136,\"start\":36890},{\"end\":37304,\"start\":37138},{\"end\":37773,\"start\":37306},{\"end\":37787,\"start\":37775},{\"end\":37829,\"start\":37789},{\"end\":38020,\"start\":37831},{\"end\":38154,\"start\":38022},{\"end\":38282,\"start\":38156},{\"end\":38524,\"start\":38284},{\"end\":39010,\"start\":38526},{\"end\":39236,\"start\":39012},{\"end\":39509,\"start\":39238},{\"end\":39639,\"start\":39511},{\"end\":39746,\"start\":39641},{\"end\":39907,\"start\":39748}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":16671,\"start\":16616},{\"attributes\":{\"id\":\"formula_1\"},\"end\":21428,\"start\":21373},{\"attributes\":{\"id\":\"formula_2\"},\"end\":21549,\"start\":21466},{\"attributes\":{\"id\":\"formula_4\"},\"end\":32112,\"start\":32040}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":11849,\"start\":11830},{\"attributes\":{\"ref_id\":\"tab_13\"},\"end\":11965,\"start\":11952},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":17902,\"start\":17894},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":22013,\"start\":22006},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":22301,\"start\":22294},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":23394,\"start\":23387},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":25856,\"start\":25849},{\"end\":27708,\"start\":27701},{\"end\":28176,\"start\":28169},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":28348,\"start\":28336},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":31396,\"start\":31389},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":31519,\"start\":31512}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1972,\"start\":1960},{\"end\":4306,\"start\":4258},{\"end\":4354,\"start\":4348},{\"end\":4427,\"start\":4421},{\"end\":4622,\"start\":4616},{\"end\":4959,\"start\":4936},{\"end\":5017,\"start\":5011},{\"attributes\":{\"n\":\"2\"},\"end\":6179,\"start\":6167},{\"attributes\":{\"n\":\"3\"},\"end\":8707,\"start\":8694},{\"attributes\":{\"n\":\"3.1\"},\"end\":8745,\"start\":8710},{\"attributes\":{\"n\":\"3.2\"},\"end\":12028,\"start\":12003},{\"attributes\":{\"n\":\"3.3\"},\"end\":14526,\"start\":14493},{\"attributes\":{\"n\":\"4\"},\"end\":16222,\"start\":16198},{\"attributes\":{\"n\":\"4.1\"},\"end\":16238,\"start\":16225},{\"attributes\":{\"n\":\"4.2\"},\"end\":18025,\"start\":17982},{\"end\":18442,\"start\":18397},{\"end\":18858,\"start\":18810},{\"attributes\":{\"n\":\"5\"},\"end\":19438,\"start\":19420},{\"attributes\":{\"n\":\"6.1\"},\"end\":21896,\"start\":21844},{\"attributes\":{\"n\":\"6.2\"},\"end\":22993,\"start\":22940},{\"attributes\":{\"n\":\"6.3\"},\"end\":24365,\"start\":24308},{\"attributes\":{\"n\":\"6.4\"},\"end\":25415,\"start\":25363},{\"attributes\":{\"n\":\"6.5\"},\"end\":26288,\"start\":26235},{\"attributes\":{\"n\":\"7\"},\"end\":27287,\"start\":27253},{\"attributes\":{\"n\":\"8\"},\"end\":28613,\"start\":28603},{\"end\":29517,\"start\":29506},{\"end\":30016,\"start\":29975},{\"end\":31755,\"start\":31745},{\"end\":31790,\"start\":31758},{\"end\":33848,\"start\":33828},{\"end\":34277,\"start\":34257},{\"end\":39919,\"start\":39909},{\"end\":40075,\"start\":40065},{\"end\":40237,\"start\":40227},{\"end\":40408,\"start\":40398},{\"end\":40475,\"start\":40474},{\"end\":40902,\"start\":40892},{\"end\":42702,\"start\":42693},{\"end\":44737,\"start\":44728},{\"end\":44845,\"start\":44836},{\"end\":50286,\"start\":50277},{\"end\":50391,\"start\":50382},{\"end\":54391,\"start\":54382},{\"end\":56987,\"start\":56978},{\"end\":57884,\"start\":57875},{\"end\":58807,\"start\":58797}]", "table": "[{\"end\":44726,\"start\":42956},{\"end\":50275,\"start\":45157},{\"end\":50380,\"start\":50367},{\"end\":50637,\"start\":50430},{\"end\":54380,\"start\":50712},{\"end\":56976,\"start\":56968},{\"end\":57873,\"start\":57865},{\"end\":58795,\"start\":58138},{\"end\":58965,\"start\":58957}]", "figure_caption": "[{\"end\":39973,\"start\":39921},{\"end\":40063,\"start\":39976},{\"end\":40225,\"start\":40077},{\"end\":40396,\"start\":40239},{\"end\":40472,\"start\":40410},{\"end\":40890,\"start\":40476},{\"end\":40943,\"start\":40904},{\"end\":42691,\"start\":40946},{\"end\":42956,\"start\":42704},{\"end\":44834,\"start\":44739},{\"end\":44971,\"start\":44847},{\"end\":45157,\"start\":44974},{\"end\":50367,\"start\":50288},{\"end\":50430,\"start\":50393},{\"end\":50712,\"start\":50640},{\"end\":56968,\"start\":54393},{\"end\":57865,\"start\":56989},{\"end\":58138,\"start\":57886},{\"end\":58957,\"start\":58810}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3627,\"start\":3619},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":11208,\"start\":11200},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":16046,\"start\":16038},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":24493,\"start\":24485},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":24991,\"start\":24983},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":26604,\"start\":26596},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":35391,\"start\":35383}]", "bib_author_first_name": "[{\"end\":60140,\"start\":60132},{\"end\":60356,\"start\":60348},{\"end\":60369,\"start\":60362},{\"end\":60382,\"start\":60377},{\"end\":60398,\"start\":60393},{\"end\":60411,\"start\":60405},{\"end\":60424,\"start\":60420},{\"end\":60439,\"start\":60434},{\"end\":60458,\"start\":60448},{\"end\":60766,\"start\":60760},{\"end\":60775,\"start\":60772},{\"end\":60790,\"start\":60785},{\"end\":61100,\"start\":61094},{\"end\":61111,\"start\":61106},{\"end\":61125,\"start\":61117},{\"end\":61138,\"start\":61135},{\"end\":61153,\"start\":61146},{\"end\":61534,\"start\":61527},{\"end\":61546,\"start\":61540},{\"end\":61559,\"start\":61553},{\"end\":61572,\"start\":61564},{\"end\":61587,\"start\":61580},{\"end\":61603,\"start\":61597},{\"end\":61954,\"start\":61948},{\"end\":61970,\"start\":61959},{\"end\":61983,\"start\":61978},{\"end\":62000,\"start\":61993},{\"end\":62020,\"start\":62011},{\"end\":62308,\"start\":62301},{\"end\":62325,\"start\":62317},{\"end\":62340,\"start\":62337},{\"end\":62357,\"start\":62350},{\"end\":62783,\"start\":62778},{\"end\":62793,\"start\":62789},{\"end\":62805,\"start\":62801},{\"end\":62827,\"start\":62819},{\"end\":63138,\"start\":63131},{\"end\":63153,\"start\":63147},{\"end\":63170,\"start\":63164},{\"end\":63186,\"start\":63178},{\"end\":63864,\"start\":63858},{\"end\":63882,\"start\":63876},{\"end\":63895,\"start\":63889},{\"end\":63910,\"start\":63903},{\"end\":64442,\"start\":64438},{\"end\":64455,\"start\":64449},{\"end\":64466,\"start\":64463},{\"end\":64479,\"start\":64472},{\"end\":64491,\"start\":64486},{\"end\":64503,\"start\":64499},{\"end\":64517,\"start\":64510},{\"end\":64998,\"start\":64993},{\"end\":65011,\"start\":65004},{\"end\":65020,\"start\":65019},{\"end\":65036,\"start\":65030},{\"end\":65048,\"start\":65043},{\"end\":65052,\"start\":65049},{\"end\":65063,\"start\":65058},{\"end\":65071,\"start\":65068},{\"end\":65337,\"start\":65333},{\"end\":65349,\"start\":65343},{\"end\":65360,\"start\":65355},{\"end\":65372,\"start\":65368},{\"end\":65717,\"start\":65713},{\"end\":65736,\"start\":65731},{\"end\":65755,\"start\":65750},{\"end\":65769,\"start\":65763},{\"end\":66060,\"start\":66054},{\"end\":66087,\"start\":66081},{\"end\":66101,\"start\":66094},{\"end\":66111,\"start\":66107},{\"end\":66453,\"start\":66447},{\"end\":66462,\"start\":66459},{\"end\":66476,\"start\":66469},{\"end\":66486,\"start\":66482},{\"end\":66495,\"start\":66491},{\"end\":66507,\"start\":66500},{\"end\":67076,\"start\":67069},{\"end\":67088,\"start\":67081},{\"end\":67102,\"start\":67098},{\"end\":67118,\"start\":67109},{\"end\":67120,\"start\":67119},{\"end\":67133,\"start\":67127},{\"end\":67135,\"start\":67134},{\"end\":67449,\"start\":67444},{\"end\":67466,\"start\":67459},{\"end\":67476,\"start\":67473},{\"end\":67491,\"start\":67486}]", "bib_author_last_name": "[{\"end\":59853,\"start\":59843},{\"end\":60144,\"start\":60141},{\"end\":60360,\"start\":60357},{\"end\":60375,\"start\":60370},{\"end\":60391,\"start\":60383},{\"end\":60403,\"start\":60399},{\"end\":60418,\"start\":60412},{\"end\":60432,\"start\":60425},{\"end\":60446,\"start\":60440},{\"end\":60466,\"start\":60459},{\"end\":60770,\"start\":60767},{\"end\":60783,\"start\":60776},{\"end\":60798,\"start\":60791},{\"end\":61104,\"start\":61101},{\"end\":61115,\"start\":61112},{\"end\":61133,\"start\":61126},{\"end\":61144,\"start\":61139},{\"end\":61159,\"start\":61154},{\"end\":61538,\"start\":61535},{\"end\":61551,\"start\":61547},{\"end\":61562,\"start\":61560},{\"end\":61578,\"start\":61573},{\"end\":61595,\"start\":61588},{\"end\":61610,\"start\":61604},{\"end\":61957,\"start\":61955},{\"end\":61976,\"start\":61971},{\"end\":61991,\"start\":61984},{\"end\":62009,\"start\":62001},{\"end\":62029,\"start\":62021},{\"end\":62315,\"start\":62309},{\"end\":62335,\"start\":62326},{\"end\":62348,\"start\":62341},{\"end\":62366,\"start\":62358},{\"end\":62787,\"start\":62784},{\"end\":62799,\"start\":62794},{\"end\":62817,\"start\":62806},{\"end\":62838,\"start\":62828},{\"end\":63145,\"start\":63139},{\"end\":63162,\"start\":63154},{\"end\":63176,\"start\":63171},{\"end\":63197,\"start\":63187},{\"end\":63874,\"start\":63865},{\"end\":63887,\"start\":63883},{\"end\":63901,\"start\":63896},{\"end\":63920,\"start\":63911},{\"end\":64447,\"start\":64443},{\"end\":64461,\"start\":64456},{\"end\":64470,\"start\":64467},{\"end\":64484,\"start\":64480},{\"end\":64497,\"start\":64492},{\"end\":64508,\"start\":64504},{\"end\":64529,\"start\":64518},{\"end\":65002,\"start\":64999},{\"end\":65017,\"start\":65012},{\"end\":65028,\"start\":65021},{\"end\":65041,\"start\":65037},{\"end\":65056,\"start\":65053},{\"end\":65066,\"start\":65064},{\"end\":65078,\"start\":65072},{\"end\":65082,\"start\":65080},{\"end\":65341,\"start\":65338},{\"end\":65353,\"start\":65350},{\"end\":65366,\"start\":65361},{\"end\":65378,\"start\":65373},{\"end\":65729,\"start\":65718},{\"end\":65748,\"start\":65737},{\"end\":65761,\"start\":65756},{\"end\":65772,\"start\":65770},{\"end\":66079,\"start\":66061},{\"end\":66092,\"start\":66088},{\"end\":66105,\"start\":66102},{\"end\":66115,\"start\":66112},{\"end\":66122,\"start\":66117},{\"end\":66457,\"start\":66454},{\"end\":66467,\"start\":66463},{\"end\":66480,\"start\":66477},{\"end\":66489,\"start\":66487},{\"end\":66498,\"start\":66496},{\"end\":66511,\"start\":66508},{\"end\":67079,\"start\":67077},{\"end\":67096,\"start\":67089},{\"end\":67107,\"start\":67103},{\"end\":67125,\"start\":67121},{\"end\":67140,\"start\":67136},{\"end\":67457,\"start\":67450},{\"end\":67471,\"start\":67467},{\"end\":67484,\"start\":67477},{\"end\":67496,\"start\":67492}]", "bib_entry": "[{\"attributes\":{\"doi\":\"ACL/IJCNLP 2021\",\"id\":\"b0\"},\"end\":60074,\"start\":59843},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":964287},\"end\":60303,\"start\":60076},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":14113767},\"end\":60758,\"start\":60305},{\"attributes\":{\"doi\":\"arXiv:2205.00363\",\"id\":\"b3\"},\"end\":60965,\"start\":60760},{\"attributes\":{\"doi\":\"10.48550/arXiv.2205.05638\",\"id\":\"b4\"},\"end\":61421,\"start\":60967},{\"attributes\":{\"id\":\"b5\"},\"end\":61875,\"start\":61423},{\"attributes\":{\"id\":\"b6\"},\"end\":62223,\"start\":61877},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":173991173},\"end\":62776,\"start\":62225},{\"attributes\":{\"id\":\"b8\"},\"end\":63054,\"start\":62778},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":237421373},\"end\":63793,\"start\":63056},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":2191379},\"end\":64385,\"start\":63795},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":235458006},\"end\":64941,\"start\":64387},{\"attributes\":{\"id\":\"b12\"},\"end\":65331,\"start\":64943},{\"attributes\":{\"id\":\"b13\"},\"end\":65641,\"start\":65333},{\"attributes\":{\"id\":\"b14\"},\"end\":65954,\"start\":65643},{\"attributes\":{\"id\":\"b15\"},\"end\":66367,\"start\":65956},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":248478190},\"end\":67024,\"start\":66369},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":1688357},\"end\":67381,\"start\":67026},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":53734356},\"end\":67878,\"start\":67383}]", "bib_title": "[{\"end\":60130,\"start\":60076},{\"end\":60346,\"start\":60305},{\"end\":62299,\"start\":62225},{\"end\":63129,\"start\":63056},{\"end\":63856,\"start\":63795},{\"end\":64436,\"start\":64387},{\"end\":66445,\"start\":66369},{\"end\":67067,\"start\":67026},{\"end\":67442,\"start\":67383}]", "bib_author": "[{\"end\":59855,\"start\":59843},{\"end\":60146,\"start\":60132},{\"end\":60362,\"start\":60348},{\"end\":60377,\"start\":60362},{\"end\":60393,\"start\":60377},{\"end\":60405,\"start\":60393},{\"end\":60420,\"start\":60405},{\"end\":60434,\"start\":60420},{\"end\":60448,\"start\":60434},{\"end\":60468,\"start\":60448},{\"end\":60772,\"start\":60760},{\"end\":60785,\"start\":60772},{\"end\":60800,\"start\":60785},{\"end\":61106,\"start\":61094},{\"end\":61117,\"start\":61106},{\"end\":61135,\"start\":61117},{\"end\":61146,\"start\":61135},{\"end\":61161,\"start\":61146},{\"end\":61540,\"start\":61527},{\"end\":61553,\"start\":61540},{\"end\":61564,\"start\":61553},{\"end\":61580,\"start\":61564},{\"end\":61597,\"start\":61580},{\"end\":61612,\"start\":61597},{\"end\":61959,\"start\":61948},{\"end\":61978,\"start\":61959},{\"end\":61993,\"start\":61978},{\"end\":62011,\"start\":61993},{\"end\":62031,\"start\":62011},{\"end\":62317,\"start\":62301},{\"end\":62337,\"start\":62317},{\"end\":62350,\"start\":62337},{\"end\":62368,\"start\":62350},{\"end\":62789,\"start\":62778},{\"end\":62801,\"start\":62789},{\"end\":62819,\"start\":62801},{\"end\":62840,\"start\":62819},{\"end\":63147,\"start\":63131},{\"end\":63164,\"start\":63147},{\"end\":63178,\"start\":63164},{\"end\":63199,\"start\":63178},{\"end\":63876,\"start\":63858},{\"end\":63889,\"start\":63876},{\"end\":63903,\"start\":63889},{\"end\":63922,\"start\":63903},{\"end\":64449,\"start\":64438},{\"end\":64463,\"start\":64449},{\"end\":64472,\"start\":64463},{\"end\":64486,\"start\":64472},{\"end\":64499,\"start\":64486},{\"end\":64510,\"start\":64499},{\"end\":64531,\"start\":64510},{\"end\":65004,\"start\":64993},{\"end\":65019,\"start\":65004},{\"end\":65030,\"start\":65019},{\"end\":65043,\"start\":65030},{\"end\":65058,\"start\":65043},{\"end\":65068,\"start\":65058},{\"end\":65080,\"start\":65068},{\"end\":65084,\"start\":65080},{\"end\":65343,\"start\":65333},{\"end\":65355,\"start\":65343},{\"end\":65368,\"start\":65355},{\"end\":65380,\"start\":65368},{\"end\":65731,\"start\":65713},{\"end\":65750,\"start\":65731},{\"end\":65763,\"start\":65750},{\"end\":65774,\"start\":65763},{\"end\":66081,\"start\":66054},{\"end\":66094,\"start\":66081},{\"end\":66107,\"start\":66094},{\"end\":66117,\"start\":66107},{\"end\":66124,\"start\":66117},{\"end\":66459,\"start\":66447},{\"end\":66469,\"start\":66459},{\"end\":66482,\"start\":66469},{\"end\":66491,\"start\":66482},{\"end\":66500,\"start\":66491},{\"end\":66513,\"start\":66500},{\"end\":67081,\"start\":67069},{\"end\":67098,\"start\":67081},{\"end\":67109,\"start\":67098},{\"end\":67127,\"start\":67109},{\"end\":67142,\"start\":67127},{\"end\":67459,\"start\":67444},{\"end\":67473,\"start\":67459},{\"end\":67486,\"start\":67473},{\"end\":67498,\"start\":67486}]", "bib_venue": "[{\"end\":59883,\"start\":59870},{\"end\":60177,\"start\":60146},{\"end\":60506,\"start\":60468},{\"end\":60840,\"start\":60816},{\"end\":61092,\"start\":60967},{\"end\":61525,\"start\":61423},{\"end\":61946,\"start\":61877},{\"end\":62449,\"start\":62368},{\"end\":62893,\"start\":62856},{\"end\":63315,\"start\":63228},{\"end\":64026,\"start\":63949},{\"end\":64612,\"start\":64531},{\"end\":64991,\"start\":64943},{\"end\":65464,\"start\":65396},{\"end\":65711,\"start\":65643},{\"end\":66052,\"start\":65956},{\"end\":66616,\"start\":66547},{\"end\":67180,\"start\":67142},{\"end\":67579,\"start\":67498},{\"end\":62517,\"start\":62451},{\"end\":63404,\"start\":63317},{\"end\":64065,\"start\":64028},{\"end\":64680,\"start\":64614},{\"end\":66644,\"start\":66618},{\"end\":67647,\"start\":67581}]"}}}, "year": 2023, "month": 12, "day": 17}