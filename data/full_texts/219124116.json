{"id": 219124116, "updated": "2023-11-11 05:48:37.599", "metadata": {"title": "AI-based Resource Allocation: Reinforcement Learning for Adaptive Auto-scaling in Serverless Environments", "authors": "[{\"first\":\"Lucia\",\"last\":\"Schuler\",\"middle\":[]},{\"first\":\"Somaya\",\"last\":\"Jamil\",\"middle\":[]},{\"first\":\"Niklas\",\"last\":\"K\u00a8uhl\",\"middle\":[]}]", "venue": "2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing (CCGrid)", "journal": "2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing (CCGrid)", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "\u2014Serverless computing has emerged as a compelling new paradigm of cloud computing models in recent years. It promises the user services at large scale and low cost while eliminating the need for infrastructure management. On cloud provider side, \ufb02exible resource management is required to meet \ufb02uctuating demand. It can be enabled through automated provisioning and deprovisioning of resources. A common approach among both commercial and open source serverless computing platforms is workload-based auto-scaling, where a designated algorithm scales instances according to the number of incoming requests. In the recently evolving serverless framework Knative a request-based policy is proposed, where the algorithm scales resources by a con\ufb01gured maximum number of requests that can be processed in parallel per instance, the so-called concurrency. As we show in a baseline experiment, this prede\ufb01ned concurrency level can strongly in\ufb02uence the performance of a serverless application. However, identifying the concurrency con\ufb01guration that yields the highest possible quality of service is a challenging task due to various factors, e.g. varying workload and complex infrastructure characteristics, in\ufb02uencing throughput and latency. While there has been considerable research into intelligent techniques for optimizing auto-scaling for virtual machine provisioning, this topic has not yet been discussed in the area of serverless computing. For this reason, we investigate the applicability of a reinforcement learning approach, which has been proven on dynamic virtual machine provisioning, to request-based auto-scaling in a serverless framework. Our results show that within a limited number of iterations our proposed model learns an effective scaling policy per workload, improving the performance compared to the default auto-scaling con\ufb01guration.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "3030194626", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/ccgrid/SchulerJK21", "doi": "10.1109/ccgrid51090.2021.00098"}}, "content": {"source": {"pdf_hash": "4b270cdaf3033e82904f35d033999f0009ef6224", "pdf_src": "Arxiv", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2005.14410", "status": "GREEN"}}, "grobid": {"id": "deb13bd7f670a3200ecb4ea9dd96085083900a1f", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/4b270cdaf3033e82904f35d033999f0009ef6224.txt", "contents": "\nAI-based Resource Allocation: Reinforcement Learning for Adaptive Auto-scaling in Serverless Environments\n\n\nLucia Schuler lucia.schuler@alumni.kit.edu \nInstitute of Technology\nKarlsruhe Institute of Technology\nIBM Research & Development GmbH\nIBM Deutschland GmbH\nKarlsruhe\n\nSomaya Jamil jamilsom@de.ibm.com \nInstitute of Technology\nKarlsruhe Institute of Technology\nIBM Research & Development GmbH\nIBM Deutschland GmbH\nKarlsruhe\n\nNiklas K\u00fchl niklas.kuehl@kit.edu \nInstitute of Technology\nKarlsruhe Institute of Technology\nIBM Research & Development GmbH\nIBM Deutschland GmbH\nKarlsruhe\n\nAI-based Resource Allocation: Reinforcement Learning for Adaptive Auto-scaling in Serverless Environments\nIndex Terms-serverlessauto-scalingreinforcement learningKnative\nServerless computing has emerged as a compelling new paradigm of cloud computing models in recent years. It promises the user services at large scale and low cost while eliminating the need for infrastructure management. On cloud provider side, flexible resource management is required to meet fluctuating demand. It can be enabled through automated provisioning and deprovisioning of resources. A common approach among both commercial and open source serverless computing platforms is workload-based auto-scaling, where a designated algorithm scales instances according to the number of incoming requests. In the recently evolving serverless framework Knative a request-based policy is proposed, where the algorithm scales resources by a configured maximum number of requests that can be processed in parallel per instance, the so-called concurrency. As we show in a baseline experiment, this predefined concurrency level can strongly influence the performance of a serverless application. However, identifying the concurrency configuration that yields the highest possible quality of service is a challenging task due to various factors, e.g. varying workload and complex infrastructure characteristics, influencing throughput and latency. While there has been considerable research into intelligent techniques for optimizing auto-scaling for virtual machine provisioning, this topic has not yet been discussed in the area of serverless computing. For this reason, we investigate the applicability of a reinforcement learning approach, which has been proven on dynamic virtual machine provisioning, to requestbased auto-scaling in a serverless framework. Our results show that within a limited number of iterations our proposed model learns an effective scaling policy per workload, improving the performance compared to the default auto-scaling configuration.\n\nAbstract-Serverless computing has emerged as a compelling new paradigm of cloud computing models in recent years. It promises the user services at large scale and low cost while eliminating the need for infrastructure management. On cloud provider side, flexible resource management is required to meet fluctuating demand. It can be enabled through automated provisioning and deprovisioning of resources. A common approach among both commercial and open source serverless computing platforms is workload-based auto-scaling, where a designated algorithm scales instances according to the number of incoming requests. In the recently evolving serverless framework Knative a request-based policy is proposed, where the algorithm scales resources by a configured maximum number of requests that can be processed in parallel per instance, the so-called concurrency. As we show in a baseline experiment, this predefined concurrency level can strongly influence the performance of a serverless application. However, identifying the concurrency configuration that yields the highest possible quality of service is a challenging task due to various factors, e.g. varying workload and complex infrastructure characteristics, influencing throughput and latency. While there has been considerable research into intelligent techniques for optimizing auto-scaling for virtual machine provisioning, this topic has not yet been discussed in the area of serverless computing. For this reason, we investigate the applicability of a reinforcement learning approach, which has been proven on dynamic virtual machine provisioning, to requestbased auto-scaling in a serverless framework. Our results show that within a limited number of iterations our proposed model learns an effective scaling policy per workload, improving the performance compared to the default auto-scaling configuration.\n\nIndex Terms-serverless, auto-scaling, reinforcement learning, Knative\n\n\nI. INTRODUCTION\n\nDriven by the advancements and proliferation of virtual machines (VMs) and container technologies, the adoption of serverless computing models has increased in recent years [1]. According to the Cloud Native Computing Foundation, serverless computing offers two main advantages to the user [2]. First, with a true and fine-grained pay-as-you-go pricing model, costs only occur when resources are actually used and not for idle VMs or containers. Second, there is no overhead for the user associated with infrastructure maintenance, such as provisioning, updating, and managing the server resources, as this is delegated to the cloud provider. This also includes flexible on-the-fly scalability which enables resources to be added or removed automatically depending on the incoming load. For providers, the auto-scaling capability provides the ability to optimize resource utilization and reduce the effort required to manage cloud-scale applications [1].\n\nIn the implementation, the scaling mechanisms differ within the serverless offerings. Some open source serverless frameworks use the resource-based Kubernetes Horizontal Pod Autoscaler (HPA) to drive scaling via per-instance CPU or memory utilization thresholds (e.g. Fission [3]). This, of course, makes the auto-scaling feature dependent on the fast and correct calculations of respective system components [4]. Commercially provided serverless platforms often feature workload-based scaling by providing additional resources when incoming traffic increases, e.g. AWS Lambda initializes an instance for each new request coming in until a limit is reached [5]. However, the creation of a new instance implies a certain time lag, known as cold start. To bypass this issue to a certain extent, a recently emerging open-source framework Knative supports parallel processing of up to a predefined number of concurrent requests per instance [6]. When the so-called concurrency is reached, Knative Pod Autoscaler (KPA) deploys additional pods to handle the load. Moreover, the concurrency parameter can be adjusted manually to use resources more efficiently and to adapt the auto-scaling system to individual workloads.\n\nIn the work at hand we show that, depending on the workload, different concurrency levels can influence the performance and can lead to a latency difference of up to multiple seconds. Since this can have a critical impact on the user experience in serverless computing, we propose a reinforcement learning (RL) based model to dynamically determine the optimal concurrency for an individual workload. In general, RL formalizes the idea of an agent learning effective decision-making policies through a sequence of trial-and-error interactions with its environment. Thereby, the agent evaluates the current state of the system dynamics in each iteration, and then decides on a particular action. After the action has been performed, the agent receives either positive or negative reward and consequently learns about the goodness of the respective action-state combination. As this approach does not require any prior knowledge about incoming workload and can adapt to changes at runtime, RL algorithms have been proven as valid methods in the field of VM auto-scaling techniques in research [7]. However, it has not been studied in a serverless environment. Therefore, we evaluate the applicability of the established RL-algorithm Q-learning to determine the concurrency level with optimized performance.\n\nSpecifically, we implement a cloud-based framework upon which two consecutive experiments are conducted. First, we perform an extensive analysis to examine performance variations of different workload profiles under different auto-scaling configurations. We demonstrate the dependence of throughput and latency on the concurrency level and indicate the potential for improvement through adaptive scaling settings. Using these results, we enhance the framework with an intelligent RLbased logic to evaluate the ability of a self-learning algorithm for effective decision making in a serverless framework. As we show in a second experiment, our proposed model is able to learn in limited time an appropriate scaling policy without prior knowledge of the incoming workload, resulting in an increased performance compared to the framework's default auto-scaling settings.\n\nThe remainder of the work is organized as follows. Section II introduces the serverless platform Knative and the theory of Q-learning. Section III reviews related work in both serverless frameworks and cloud-based auto-scaling techniques. Section IV gives an overview of the underlying experimental setup of the work, based on which section V presents the tests on the impact of different concurrency limits. Using these findings, section VI proposes a Q-learning model to adapt the concurrency limit on-the-fly. Section VII concludes the paper with remarks on limitations and possible future work.\n\n\nII. BACKGROUND\n\nTo allow for a common understanding of the application domain and used techniques, we first provide an overview of the functionality of Knative and its auto-scaling feature. Further, we introduce the theoretical foundations of the Qlearning algorithm which is applied in the second experiment.\n\n\nA. Knative Serverless Platform\n\nAs an open-source serverless platform, Knative provides a set of Kubernetes-based middleware components to support deploying and serving of serverless applications, including the capability to automatically scale resources on demand [6].\n\nThe auto-scaling function is implemented by different serving components, described by the request flow in Figure 1 based on Knative v0.12. If a service revision is scaled to zero, i.e. the service deployment is reduced to a replica of null operating pods, the ingress gateway forwards incoming requests first to the activator [6]. The activator then reports the information to the autoscaler, which instructs the revision's deployment to scale-up appropriately. Further, it buffers the requests until the user pods of the revision become available, which can cause cold-start costs in terms of latency, as the requests are blocked for the corresponding time. In comparison, if a minimum of one replica is maintained active, the activator is bypassed and the traffic can flow directly to the user pod. When the requests reach the pod, they are channeled by the queue-proxy container and, subsequently, processed in the user-container. The queue-proxy only allows a certain number of requests to enter the user-container simultaneously, and queues the requests if necessary. The amount of parallel processed requests is specified by the concurrency parameter configured for a particular revision. By default, the value is set to a concurrency target of 100, defining how many parallel requests are preferred per user-container at a given time. However, the user can explicitly restrict the number of concurrent requests by specifying a value between 0 and 1000 for the concurrency limit. 1 Further, each queue-proxy measures the incoming load and reports the average concurrency and requests per second on a separate port. The metrics of all queue-proxy containers are scraped by the autoscaler component, which then decides how many new pods need to be added or removed to keep the desired concurrency level.\n\n\nB. Q-learning\n\nRL refers to a collection of trial-and-error methods in which an agent is trained to make good decisions by interacting with his environment and receiving positive or negative feedback in form of rewards for a respective action. A popular RL algorithm is the model-free Q-learning. Q-learning stepwise trains an approximator Q \u03b8 (s, a) of the optimal action-value function Q * . Q \u03b8 (s, a) specifies the cumulated reward the agent can expect when starting in a state s, taking an action a, and then acting according to the optimal policy forever after. By observing the actual reward in each iteration, the optimization of the Q-function is performed incrementally per step t:\nQ(s t , a t ) \u2190 \u2212 (1 \u2212 \u03b1)Q(s t , a t ) + \u03b1[r t + \u03b3 max a Q(s t+1 , a)]\n\u03b1 describes the learning rate, i.e. to what extent newly observed information overrides old information and \u03b3 a discount factor that serves to balance between the current and future reward. As RL is a trial-and-error method, during training, the agent has to choose between the exploration of a new action and the exploitation of the current best option [9]. In research, this is often implemented with an -greedy strategy, where defines the probability of exploration that usually decreases as the learning process advances [10], [11]. With a probability of 1 \u2212 , the agent selects based on the optimal policy and chooses the action that maximizes the expected return from starting in s, i.e. the action with the highest Q-value:\na * (s) = arg max a Q * (s, a)\nIn the basic algorithm, the Q-values for each state-action combination are stored in a lookup table, the so-called Q-table, indexed by states and actions. The tabular representation of the agent's knowledge serves as a basis for decision-making during the entire learning episode.\n\n\nIII. RELATED WORK\n\nTo the best of our knowledge, the applicability of RL-based technology to optimize auto-scaling capabilities in serverless environments has not been investigated. However, considering the areas of serverless and intelligent auto-scaling separately, a large body of knowledge is available, summarized in the following subsections.\n\n\nA. Serverless computing\n\nWith the growing number of serverless computing offerings, there has been an increasing interest of the academic community in comparing different solutions, with scalability being one of the key elements of evaluation [4]. In multiple works, different propriety serverless platforms were benchmarked, including their ability to scale, focusing on Amazon Lambda, Microsoft Azure Functions [12], along with Google Cloud Functions [13] and additionally IBM Cloud Functions [14]. Similar studies have been carried out in the area of opensource serverless frameworks, with greater attention paid to the auto-scaling capabilities. Mohanty et al. [15] evaluated Fission, Kubeless, and OpenFaaS and concluded that Kubeless provides the most consistent performance in terms of response time. Another comparison of both qualitative and quantitative features of Kubeless, OpenFaas, Apache Openwhisk, and Knative, comes to the same conclusion, albeit generally indicating the limited user control over custom Quality of Service (QoS) requirements [16]. These studies solely consider the default auto-scaler Kubernetes HPA. Possible adjustments to the autoscaling mechanism itself are not further examined. Li et al. [4] propose a more concrete distinction between resource-based and workload-based scaling policies. The authors compare the performance of different workload scenarios using the tuning capability of concurrency levels in Knative and clearly suggest further investigation of the applicability of this auto-scaling capability, which also motivates this research.\n\n\nB. Auto-scaling\n\nAs elasticity is one of the main characteristics of the increasing adaption of cloud computing, the automatic, ondemand provisioning and de-provisioning of cloud resources have been the subject of intensive research in recent years [17]. We discuss related work under two aspects: first, the underlying theories on which auto-scaling is built with a focus on RL, and second, the entities being scaled.\n\nTo classify numerous techniques at the algorithmic level, different taxonomies were proposed, where the predominant categories are threshold-based rules, queuing theory and RL [7], [17]. In the former, scaling decisions are made on predefined thresholds and are most popular among public cloud providers, e.g. Amazon ECS [18]. Despite the simplistic implementation, identifying suitable thresholds requires expert knowledge [17], or explicit application understanding [19]. Queuing theory has been used to mathematically model applications [7]. As they usually impose a stationary system, the models are less reactive towards changes [7].\n\nIn contrast, RL offers an interesting approach through online learning of the most suitable scaling action and without the need for any a-priori knowledge [7]. Many authors have therefore investigated the applicability of model-free RL algorithms, such as Q-learning, in recent years [10]. Dutreihl et al. [19] show that although Q-learning based VM controlling requires an extensive learning phase and adequate system integration, it can lead to significant performance improvements compared to threshold-based auto-scaling, since thresholds are often set too tightly while seeking for the optimal resource allocation. To combine the advantages of both, Q-learning itself can be used to automatically adapt thresholds to a specific application [20].\n\nIn terms of the entity being scaled, RL has been mostly applied to policies for VM allocation and provisioning, e.g. in [21]. With the emergence of container-based applications, this field has become a greater focus of research [10]. In both areas, the scope of action is concentrated mainly on horizontal (scale-out/-in) [20], vertical scaling (scale-up/-down) [22], or the combination of both [10]. However, little research has been done in areas that extend the classic auto-scaling problem of VM or container configuration.\n\nAs a novel approach we investigate the applicability of Qlearning to request-based auto-scaling in a serverless environment. Differently from the existing work on direct vertical or horizontal scaling with RL, we propose a model that learns an effective scaling policy by adapting the level of concurrent requests per container instance to a specific workload.\n\n\nIV. EXPERIMENTAL SETUP\n\nTo investigate different concurrency configurations, a flexible Kubernetes-based framework is designed which can be extended by an intelligent RL-based logic. In this section, we present the fundamental setup starting with the cloud architecture. This section provides the foundation for both the first experiment assessing the impact of concurrency changes and the second experiment evaluating RL-based auto-scaling.\n\n\nA. Cloud Architecture\n\nThe overall architecture of our experiment is illustrated in Fig. 2  The Knative resources are installed on the service cluster (version v0.12), including the serving components explained in Section II-A, which control the state of the deployed sample service and enable auto-scaling of additional pods on demand. Using the trial-and-error method of RL in the second experiment, we update the concurrency configuration of the service in each iteration.\n\nTo comprehensively test the auto-scaling capability, we activated the scale-to-zero functionality in the autoscaler's configmap, which requires a cold start in each iteration. We further increased the replica number of ingress gateways, which handle load balancing, to bypass performance issues and to focus our studies exclusively on the auto-scaling functionalities.\n\n\nB. Workload\n\nServerless computing is used for a variety of applications, accompanied by different resource requirements. For example, the processing of video and image material or highly-parallel analytical workloads, such as MapReduce jobs, demand considerable memory and computing power. Other applications, such as chained API compositions or chatbots, tend to be less compute-intensive but may require longer execution or response time.\n\nTo investigate the concurrency impact of many different workloads, we generate a synthetic, stable workload profile simulating serverless applications. We use Knative's example Autoscale-go application for this purpose, which allows different parameters to be passed with the request to test incremental variations of the workload characteristics and thus emulate varying CPU-and memory-intensive workloads [23]. The three application parameters are bloat, prime and sleep, wherein the first is used to specify the number of megabytes  to be allocated and the second to calculate the prime numbers up to the given number, to create either memory-or computeintensive loads. The sleep parameter pauses the request for the corresponding number of milliseconds, as in applications with certain waiting times.\n\n\nC. Process Flow\n\nThe basic process flow of one iteration is illustrated in Fig.  3. In each iteration the agent sends a concurrency update to the service cluster, which accordingly creates a new revision with the respective concurrency limit. When the service update is complete, the agent sends the start signal to the client cluster, which begins issuing parallel requests against the service cluster. To simulate a large number of user requests at the same time, we use the HTTP load testing tool Vegeta, which features sending HTTP requests at a constant rate. In the experiment, 500 requests are sent simultaneously over a period of 30s to ensure sufficient demand for scaling and sufficient time to provide additional instances. After the last response is received, Vegeta outputs a report of the test results, including information on latency distribution of requests, average throughput and success ratio of responses. The performance measures are then stored by the agent. Additionally, the agent crawls metrics from the Knative monitoring components, exposed via a Prometheus-based HTTP API within the cluster, to get further information about resource usage at cluster, node, pod and container level. Using this data, the concurrency update is chosen to proceed to the next iteration.\n\n\nV. BASELINE EXPERIMENT\n\nTo determine the implications of varying concurrency limits, we first conduct a baseline experiment comparing different workloads on their relative performance.\n\n\nA. Design\n\nAs outlined in the previous section, we use the application parameters bloat, prime and sleep to simulate varying workload characteristics. Starting with a no-operation workload where no parameters are passed, the memory allocation and CPU load were gradually increased for each new experiment. The step size of the memory allocating parameter was aligned with the memory buckets commonly used for the standard pricing model of serverless platforms. To simulate computeintensive and longer-lasting requests, different prime and sleep parameters were chosen correspondingly. The detailed values are specified in Table I. Per profile, we run performance tests for different concurrency levels according to the process flow described in Figure 3. Theoretically, the concurrency limit can take all values between 0 and 1000. To keep the experiments computationally feasible, we proceed in steps of 20, starting at a concurrency limit of 10 and ending at 310. As stated in related literature, we focus on latency and throughput as key performance measures of serverless applications [4]. Average throughput is defined by requests per second (RPS), mean latency refers to the average time in seconds taken to return a response to a request. To cover tail latency, we include the 95th percentile of latencies of all requests as an additional metric. Furthermore, each test is repeated ten times to compensate for outliers or other fluctuations, before the concurrency is updated to the next limit.\n\n\nB. Results\n\nWe structure the analysis of the baseline experiment results in three parts. First, we examine the behavior of the individual workload profiles under different concurrency configurations. Second, we focus on the relation of the target variables throughput and latency during the tests. Finally, further metrics about resource utilization on container and pod level are analyzed.\n\nAs described above, we conducted the experiment for different combinations of the three parameters to simulate possible use cases. Table I gives an overview of the outcomes with the concurrency limit that lead to the optimal test result in terms of one of the performance measures. Due to the numerous uncontrollable factors that influence the performance of the cluster, each result forms a snapshot in time. The respective workload configuration is described by the three columns on the left. Taking all tests into account, the smallest possible concurrency of 10 is the most common configuration that resulted in the best performance across all three indicators. Interestingly, this does not correspond to the default setting of the KPA where a target concurrency value of 100 is preferred [24]. In particular, workloads that consume memory exclusively perform better with fewer parallel requests per pod instance, e.g tests #II, #IX, #XVI and #XVII. Similar observations are made for workloads with additional low CPU usage, i.e. lower prime parameter, as in tests #III and #X. Deviations can be observed when the requests pause for a certain time. These workloads result in higher throughput and lower mean and tail latency when a higher concurrency is chosen, e.g tests #VII, #VIII and #XIV.\n\nDepending on the workload, the distance between the optimal configuration and the second best concurrency can be very small, which becomes more evident when analyzing a single test in detail. Fig. 4 shows the result of test #VII, which is examined representatively. Although the individual measurement points fluctuate, clear trends are identified in the average values. A significant increase in throughput can be observed when the concurrency limit is raised to 70. This setting also  I  ---50  50  70  II  128  --30  30  30  III  128  1000  -10  10  10  IV  128  10.000  -30  30  10  V  128  100.000  -10  10  10  VI  128  1000  1000  110  110  150  VII  128 Fig. 4: Performance of workload test #VII yields the lowest value for mean latency, differing from the second-best value at concurrency 50 by only 80 milliseconds. The distance becomes more critical when considering the tail latency of the 95 th percentile, where a request takes more than 740 milliseconds on average longer to receive a response when compared to the most effective configuration. At a concurrency of 10, the difference amounts to almost 3 seconds, further underlining the performance variations caused by the different settings. The greatest slowdown in tail latency in this test occurs at a concurrency of 310 with more than 3.7 seconds.\n\nBesides, the overall performance decreases strongly when the concurrency limit exceeds a level of 210. This tendency can be found across the majority of tests, indicating that due to the high simultaneous processing of many requests, only a limited amount of resources are available for a single request. Further observations show that with increasing memory utilization, i.e. the bloat parameter, performance tends to drop at lower concurrency limits. In some cases, additionally, the success ratio strongly declines. For example in test #XVI, from a concurrency of 170 onwards, more than 10% of the requests received non-successful responses. In test #XVII accordingly, this output can be observed from a concurrency of 90 onwards.\n\nFocusing on the target metrics, the tests show that adjusting the concurrency limit to an appropriate setting can yield significant improvements in throughput and latency. Furthermore, an inverse behavior of the measures can be observed within the tests. For the previously considered test #VII, the results indicate a significant negative correlation of throughput and mean latency of \u22120.989, and a similar correlation for throughput and 95 th -percentile latency of \u22120.916. 2 This strong negative relationship between the metrics is found across all tests, with significant correlation coefficients ranging from \u22120.995 to \u22120.748. 3 Subsequently, an improvement in throughput usually results in a lower and more favorable latency. This finding implies that there is no need to make trade-offs between different target metrics when adjusting the concurrency. Instead, the problem can be reduced to one objective metric, representing the others.\n\n\nVI. REINFORCEMENT LEARNING EXPERIMENT\n\nThe experiment described in previous section demonstrates the impact the concurrency configuration can have on performance. Therefore, we evaluate the applicability of the modelfree RL algorithm Q-learning in a second experiment to learn effective scaling policies by adjusting the concurrency limit during run time.\n\n\nA. Design\n\nThe process flow is based on the procedure from section IV-C, extended with a more sophisticated logic of the agent. Instead of incrementally increasing the concurrency, the agent uses knowledge of the system environment (states) to test different concurrency updates (actions) and evaluates them by receiving scores (reward). In each iteration, the environment is defined by the current state, which, should provide a complete description of the system dynamics including all relevant information for optimal decision making. Due to the large number of factors influencing performance, e.g. hidden cluster activities or network utilization, this is neither traceable nor computationally feasible in the used Q-learning algorithm. Therefore, we break down our state space S into three key features. We define S at time step i as the combination of the state variables s i = (conc i , cpu i , mem i ), where conc i depicts the concurrency limit, cpu i is the average CPU utilization per user-container and mem i is the average memory utilization per user-container. The selection of the features is aligned with related research, with conc i as the equivalent of the number of VMs in VM auto-scaling approaches [21], [25]. cpu i and mem i serve as a direct source of information about the resource utilization of a respective workload. Since both CPU and memory utilization are continuous numbers, we discretize them into bins of equal size. In each state s i \u2208 S, we define A(s i ) as the set of valid actions, where A is the set of all actions. The agent can choose between decreasing, maintaining or increasing the concurrency limit by 20, i.e. After each iteration, the agent receives an immediate reward according to the performance achieved through the action. In related literature, the reward is often based on the distance or ratio between the performance measure and a certain Service Level Agreement, such as a throughput or response time target value [20], [26]. Since there is no target level to be achieved nor prior information about the performance given in our problem definition, we define an artificial reference value ref value as the best value obtained to date. Due to the permanent, albeit minor fluctuations in the measures, we propose a tolerance band around the reference value to avoid weighting minor non-relevant deviations. Furthermore, the results from the preliminary study have shown a highly negative correlation between throughput and latency, i.e. higher throughput usually leads to lower and therefore better latency. This relation in turn allows to focus exclusively on throughput (thrghpt) as one single objective. The calculation of the reward r in time step i is as follows.\nr i = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 thrghpti ref value if thrghpt i \u2264 ref value \u00b7 0.95 or thrghpt i \u2265 ref value \u00b7 1.05 1 else\nQ-learning is initiated with the following parameters. A learning rate \u03b1 = 0.5 is chosen to balance newly acquired and existing information, a discount factor \u03b3 = 0.9 to ensure that the agent strives for a long-term high return. To encourage the exploration of actions at the beginning of training, we implement a decaying -greedy policy starting at iteration 50 with = 1 and then slowly decrease over time by a decay factor of 0.995 per iteration. The minimum exploration probability is set to min = 0.1, to allow for the detection of possible changes in the system. The knowledge the agent acquires is stored in a Q-table and updated each iteration.\n\nTo examine whether the model can effectively learn the concurrency values identified in section V as high throughput configurations, the results are analyzed representatively based on workload test #VII and #X. The former test showed high performance at a concurrency limit of 70, while the second reached the best test results at an edge concurrency of 10. \n\n\nB. Results\n\nFirst, we analyze the results to examine the suitability of the proposed Q-learning-based model to fine-tune the autoscaling. Second, we evaluate the performance of the approach in terms of throughput improvements compared to Knative's default auto-scaling configuration.\n\nBased on workload profile #X, Fig. 5 shows in detail how the agent applies RL logic to incrementally change the concurrency and to adjust it as the training progressed. Beginning at concurrency 170, random exploration leads to a moderate decline of the concurrency limit in the first 30 iterations. This results in an improvement in throughput in this test, captured by the rewards and corresponding Q-values for each stateaction combination. The most effective scaling policy of 10 parallel requests per container is first reached in iteration 121. Nevertheless, due to the -greedy strategy, exploratory actions are chosen, which might differ from the down-scaling decision and cause the agent to deviate from a good strategy. As training progresses, a trend towards performance-enhancing concurrency configurations can be observed, indicating the agent is more likely to exploit the optimal decision rather than exploring. After 330 iterations, the concurrency stabilizes at a limit of 10 parallel requests per container, implying the agent has learned the correct scaling policy, according to the results from section V. Due to the minimum = 0.1, exploration still rarely occurs to ensure the agent can respond to changes in the environment.\n\nA different learning process of the proposed Q-learning approach can be observed for workload #VII, depicted in Fig.  6. The varying concurrency curve shows the initial strategy of the agent exploring first the higher state space before proceeding with lower concurrency limits. After 250 iterations the exploitation phase outweighs and the concurrency gradually levels off. In comparison to workload #X, where the algorithm's scaling policy converges to a single concurrency limit, the configuration here fluctuates, mainly between 50 and 70, and retains this pattern. Further differences between the test results arise from the throughput metric, which shows strong fluctuations between 230 and 415 RPS across all iterations. The deviations, which also appear within one concurrency setting, considerably impair the agent's ability to evaluate suitable state-action-pairs via the reward function. Nevertheless, the agent is able to narrow down the scaling  To evaluate the proposed scaling policies, we benchmark the average performance of the Q-learning-based approach with the static default setting. For this purpose, the same experimental setup is used as in the Q-learning test, except for the auto-scaling configuration, where the original setting of a concurrency target of 100 is applied [6]. 4 Fig. 7 depicts the average throughput up to the respective iteration of the Q-learning model and the default configuration for the two considered workloads. Both result in the Q-learning model outperforming the test based on Knative's standard settings. Considering workload #VII first, the model requires approximately 150 iterations until the average performance reaches default-level. Subsequently, the throughput increases to an average of 400 RPS providing a minor advantage of 20 RPS compared to the standard system.\n\nA more significant enhancement shows workload #X. While in the first 10 iteration the default settings alternate between 350 and 440 RPS, converging to an average of about 393 RPS, the performance of our model is initially lower. However, with ongoing learning the average throughput improves and excels already from iteration 10 onwards. After 600 iterations, the presented Q-learning based model reaches an average throughput of 740 RPS, hence achieving more than 80% of the performance of the default setting, which stabilizes at 390 RPS on average.\n\nTo summarize the results, the proposed model learned within finite time a scaling policy that outperforms the default Knative configuration in terms of throughput, proving the Qlearning-based approach is well-feasible to refine the autoscaling mechanism.\n\n\nVII. CONCLUSION\n\nWith the emergence of serverless frameworks, the ability of dynamic, real-time resource provisioning to meet varying demand has become a key area of interest and has led to the development of numerous scaling mechanisms. Focusing on request-based scaling, we first investigated the impact a modification of the main scaling parameter, i.e. the number of concurrent requests per instance, may have on performance. The experiments showed deviations of up to multiple seconds in the average latency as well as significant differences in throughput values, thus indicating that the concurrency configuration can affect the performance depending on the specific workload. To flexibly adjust the auto-scaling settings to specific requirements, we designed a RL model based on Q-learning and evaluated its applicability to learn effective scaling policies during runtime. Based on different workloads, we showed that the proposed model can adapt the concurrency appropriately without prior knowledge within limited time and outperforms the average throughput compared to the default setting of Knative.\n\nGiven these results, the presented work offers valuable contributions to both the existing work in the field of serverless frameworks and the application of RL-based auto-scaling. In addition to previous studies on scaling capabilities in serverless platforms, we provided a detailed analysis to reveal the performance implications of changes in the concurrency configuration. Furthermore, we demonstrated, with our proposed model the applicability of Q-learning-based auto-scaling in the field of serverless applications. Additionally, the findings can contribute to the ongoing development of the auto-scaling system of the Knative community project.\n\nNevertheless, we identified some limitations in the approach during the experiments. First, the results from Section V are based on synthetic workloads simulated by one application with varying parameters, and thus cannot be interpreted as a universally valid conclusion on the effects of real-world applications. Second, due to the focus on general applicability of Q-learning, the approach uses a rather simplistic reward function measuring exclusively the proximity to the reference value. Further refinement of the reward function may improve the efficiency of the proposed model.\n\nWhile in this work a RL approach has been developed, which learns a certain scaling policy per workload mainly through testing different concurrency states, it remains to be analyzed to what extent the ratio of resource usage of individual components might impact the performance. Thus, a comprehensive study could be conducted to determine the combination of utilization levels that might achieve the best possible performance across all workloads. Consequently, the concurrency configuration could merely serve as a tool to bring the system into this particular state.\n\nFig. 2 :\n2Architectural setup including the information flow the sample service used for the experiments is deployed. The cluster contains 9 nodes with 16 vCPU and 64 GB memory each, designed to provide sufficient capacity to host all Knative components and avoid performance limitations. The client cluster consists of one node with 16 vCPU and 64 GB memory responsible for sending requests to the service cluster to generate load. The agent manages the activities on both clusters, including the configuration updates of the sample service based on collected metrics, and coordinates the process flow of the experiment, taking the role of an IKS user.\n\nFig. 3 :\n3Process flow\n\nFig. 5 :\n5A = {\u221220, 0, 20}. If the agent reaches the endpoints on the concurrency scale, i.e. the minimum or maximum concurrency, Performance of Q-learning model, workload #X the action space in this state is reduced accordingly by the non-executable action.\n\nFig. 6 :\n6Performance of Q-learning model, workload #VII\n\nFig. 7 :\n7Comparison of average throughput of the Q-learning model and the Knative default auto-scaling setting range to a limited number of values at which it identified the best outcomes in terms of throughput, and which agrees with the result of the baseline experiment in Section V.\n\n\n. To test the auto-scaling capabilities in an isolated environment, we set up two separate Kubernetes clusters, using IBM Cloud Kubernetes Service (IKS). On the service cluster,Service cluster \n\nClient cluster \n\nAgent \n\nConcurrency \nconfiguration \nupdate \n\nMetrics \n\nService deployment \n\nDecision controller \nMetrics monitor \n\nRequests/responses \n\nStart signal \nMetrics \n\nKnative components \n\n\n\nTABLE I :\nIConcurrency Performance TestsTest \nWorkload Profile \nConc. Limit Yielding Best Perf. \n# \nbloat  *  \nprime \nsleep  *  \nthrghpt \nmean lat. \n95th lat. \n\n\n\nQ-learning model -workload # 7 Knative default -workload # 7 Q-learning model -workload # 10 Knative default -workload # 100 \n100 \n200 \n300 \n400 \n500 \n600 \nIteration \n\n400 \n\n600 \nThroughput (RPS) \n\n\nA value of 0 allows unlimited concurrent requests and therefore results in no scaling[8].\nFor all statistical tests, Pearson correlation coefficient is used with a twosided p-value for testing non-correlation and an alpha level of .001.3 Except for test #I and #XIII with significant correlations of throughput and tail latency of \u22120.629, and throughput and mean latency of \u22120.677.\nAdditionally, the container target percentage is set to 0.7 as in the default configmap.\n\nThe server is dead, long live the server: Rise of serverless computing, overview of current state and future trends in research and industry. P Castro, V Ishakian, V Muthusamy, A Slominski, arXiv:1906.02888arXiv preprintP. Castro, V. Ishakian, V. Muthusamy, and A. Slominski, \"The server is dead, long live the server: Rise of serverless computing, overview of current state and future trends in research and industry,\" arXiv preprint arXiv:1906.02888, 2019.\n\nCncf serverless whitepaper v1.0. S Allen, C Aniszczyk, C Arimura, S. Allen, C. Aniszczyk, C. Arimura, and et al., \"Cncf serverless whitepaper v1.0,\" 2018. [Online]. Avail- able: https://github.com/cncf/wg-serverless/blob/master/whitepapers/ serverless-overview/cncf serverless whitepaper v1.0.pdf\n\nA high-level view of the internals of fission. \"A high-level view of the internals of fission.\" https://github.com/fission/ fission/blob/master/Documentation/Architecture.md, accessed: 2020-03- 26.\n\nUnderstanding open source serverless platforms: Design considerations and performance. J Li, S G Kulkarni, K Ramakrishnan, D Li, Proceedings of the 5th International Workshop on Serverless Computing. the 5th International Workshop on Serverless ComputingJ. Li, S. G. Kulkarni, K. Ramakrishnan, and D. Li, \"Understanding open source serverless platforms: Design considerations and performance,\" in Proceedings of the 5th International Workshop on Serverless Computing, 2019, pp. 37-42.\n\nAws lambda function scaling. \"Aws lambda function scaling,\" https://docs.aws.amazon.com/lambda/ latest/dg/invocation-scaling.html, accessed: 2020-03-26.\n\nKnative serving autoscaling system. \"Knative serving autoscaling system,\" https://github.com/knative/ serving/blob/master/docs/scaling/SYSTEM.md, accessed: 2020-03-26.\n\nA review of auto-scaling techniques for elastic applications in cloud environments. T Lorido-Botran, J Miguel-Alonso, J A Lozano, Journal of grid computing. 124T. Lorido-Botran, J. Miguel-Alonso, and J. A. Lozano, \"A review of auto-scaling techniques for elastic applications in cloud environments,\" Journal of grid computing, vol. 12, no. 4, pp. 559-592, 2014.\n\nConfiguring knative serving autoscaling. \"Configuring knative serving autoscaling,\" https://docs.openshift.com/ container-platform/4.2/serverless/configuring-knative-serving- autoscaling.html, accessed: 2020-03-28.\n\nReinforcement learning: An introduction. R S Sutton, A G Barto, MIT pressR. S. Sutton and A. G. Barto, Reinforcement learning: An introduction. MIT press, 2018.\n\nHorizontal and vertical scaling of container-based applications using reinforcement learning. F Rossi, M Nardelli, V Cardellini, 2019 IEEE 12th International Conference on Cloud Computing (CLOUD). F. Rossi, M. Nardelli, and V. Cardellini, \"Horizontal and vertical scaling of container-based applications using reinforcement learning,\" in 2019 IEEE 12th International Conference on Cloud Computing (CLOUD).\n\n. IEEE. IEEE, 2019, pp. 329-338.\n\nPlaying atari with deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A Graves, I Antonoglou, D Wierstra, M Riedmiller, arXiv:1312.5602arXiv preprintV. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wier- stra, and M. Riedmiller, \"Playing atari with deep reinforcement learn- ing,\" arXiv preprint arXiv:1312.5602, 2013.\n\nServerless computing: An investigation of factors influencing microservice performance. W Lloyd, S Ramesh, S Chinthalapati, L Ly, S Pallickara, 2018 IEEE International Conference on Cloud Engineering (IC2E). IEEEW. Lloyd, S. Ramesh, S. Chinthalapati, L. Ly, and S. Pallickara, \"Serverless computing: An investigation of factors influencing microser- vice performance,\" in 2018 IEEE International Conference on Cloud Engineering (IC2E). IEEE, 2018, pp. 159-169.\n\nPeeking behind the curtains of serverless platforms. L Wang, M Li, Y Zhang, T Ristenpart, M Swift, 2018 {USENIX} Annual Technical Conference ({USENIX}{ATC} 18). L. Wang, M. Li, Y. Zhang, T. Ristenpart, and M. Swift, \"Peeking behind the curtains of serverless platforms,\" in 2018 {USENIX} Annual Technical Conference ({USENIX}{ATC} 18), 2018, pp. 133-146.\n\nEvaluation of production serverless computing environments. H Lee, K Satyam, G Fox, 2018 IEEE 11th International Conference on Cloud Computing (CLOUD). IEEEH. Lee, K. Satyam, and G. Fox, \"Evaluation of production serverless computing environments,\" in 2018 IEEE 11th International Conference on Cloud Computing (CLOUD). IEEE, 2018, pp. 442-450.\n\nAn evaluation of open source serverless computing frameworks. S K Mohanty, G Premsankar, M Di Francesco, CloudCom. S. K. Mohanty, G. Premsankar, M. Di Francesco et al., \"An evaluation of open source serverless computing frameworks.\" in CloudCom, 2018, pp. 115-120.\n\nAn evaluation of open source serverless computing frameworks support at the edge. A Palade, A Kazmi, S Clarke, 2019 IEEE World Congress on Services (SERVICES). IEEE2642A. Palade, A. Kazmi, and S. Clarke, \"An evaluation of open source serverless computing frameworks support at the edge,\" in 2019 IEEE World Congress on Services (SERVICES), vol. 2642. IEEE, 2019, pp. 206-211.\n\nResearch on auto-scaling of web applications in cloud: survey, trends and future directions. P Singh, P Gupta, K Jyoti, A Nayyar, Scalable Computing: Practice and Experience. 202P. Singh, P. Gupta, K. Jyoti, and A. Nayyar, \"Research on auto-scaling of web applications in cloud: survey, trends and future directions,\" Scalable Computing: Practice and Experience, vol. 20, no. 2, pp. 399-432, 2019.\n\nAws service auto scaling. \"Aws service auto scaling,\" https://docs.aws.amazon.com/AmazonECS/ latest/developerguide/service-auto-scaling.html, accessed: 2020-03-27.\n\nFrom data center resource allocation to control theory and back. X Dutreilh, A Moreau, J Malenfant, N Rivierre, I Truck, 2010 IEEE 3rd international conference on cloud computing. IEEEX. Dutreilh, A. Moreau, J. Malenfant, N. Rivierre, and I. Truck, \"From data center resource allocation to control theory and back,\" in 2010 IEEE 3rd international conference on cloud computing. IEEE, 2010, pp. 410-417.\n\nEfficient cloud auto-scaling with sla objective using q-learning. S Horovitz, Y Arian, 2018 IEEE 6th International Conference on Future Internet of Things and Cloud (FiCloud). IEEES. Horovitz and Y. Arian, \"Efficient cloud auto-scaling with sla objective using q-learning,\" in 2018 IEEE 6th International Conference on Future Internet of Things and Cloud (FiCloud). IEEE, 2018, pp. 85-92.\n\nDerp: A deep reinforcement learning cloud system for elastic resource provisioning. C Bitsakos, I Konstantinou, N Koziris, 2018 IEEE International Conference on Cloud Computing Technology and Science (CloudCom). IEEEC. Bitsakos, I. Konstantinou, and N. Koziris, \"Derp: A deep reinforce- ment learning cloud system for elastic resource provisioning,\" in 2018 IEEE International Conference on Cloud Computing Technology and Science (CloudCom). IEEE, 2018, pp. 21-29.\n\nA distributed self-learning approach for elastic provisioning of virtualized cloud resources. J Rao, X Bu, C.-Z Xu, K Wang, 2011 IEEE 19th Annual International Symposium on Modelling, Analysis, and Simulation of Computer and Telecommunication Systems. IEEEJ. Rao, X. Bu, C.-Z. Xu, and K. Wang, \"A distributed self-learning approach for elastic provisioning of virtualized cloud resources,\" in 2011 IEEE 19th Annual International Symposium on Modelling, Analysis, and Simulation of Computer and Telecommunication Systems. IEEE, 2011, pp. 45-54.\n\nKnative autoscale-go sample app -go. \"Knative autoscale-go sample app -go,\" https://github.com/knative/docs/ tree/master/docs/serving/samples/autoscale-go, accessed: 2020-04-04.\n\nConfiguring autoscaling. \"Configuring autoscaling,\" https://knative.dev/v0.12-docs/serving/ configuring-autoscaling//, accessed: 2020-04-28.\n\nApplying reinforcement learning towards automating resource allocation and application scalability in the cloud. E Barrett, E Howley, J Duggan, Concurrency and Computation: Practice and Experience. 2512E. Barrett, E. Howley, and J. Duggan, \"Applying reinforcement learning towards automating resource allocation and application scalability in the cloud,\" Concurrency and Computation: Practice and Experience, vol. 25, no. 12, pp. 1656-1674, 2013.\n\nUsing reinforcement learning for autonomic resource allocation in clouds: towards a fully automated workflow. X Dutreilh, S Kirgizov, O Melekhova, J Malenfant, N Rivierre, I Truck, ICAS 2011, The Seventh International Conference on Autonomic and Autonomous Systems. X. Dutreilh, S. Kirgizov, O. Melekhova, J. Malenfant, N. Rivierre, and I. Truck, \"Using reinforcement learning for autonomic resource allocation in clouds: towards a fully automated workflow,\" in ICAS 2011, The Seventh International Conference on Autonomic and Autonomous Systems, 2011, pp. 67-74.\n", "annotations": {"author": "[{\"end\":274,\"start\":109},{\"end\":430,\"start\":275},{\"end\":586,\"start\":431}]", "publisher": null, "author_last_name": "[{\"end\":122,\"start\":115},{\"end\":287,\"start\":282},{\"end\":442,\"start\":438}]", "author_first_name": "[{\"end\":114,\"start\":109},{\"end\":281,\"start\":275},{\"end\":437,\"start\":431}]", "author_affiliation": "[{\"end\":273,\"start\":153},{\"end\":429,\"start\":309},{\"end\":585,\"start\":465}]", "title": "[{\"end\":106,\"start\":1},{\"end\":692,\"start\":587}]", "venue": null, "abstract": "[{\"end\":2619,\"start\":757}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4759,\"start\":4756},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4876,\"start\":4873},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5536,\"start\":5533},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5818,\"start\":5815},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5951,\"start\":5948},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6199,\"start\":6196},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6479,\"start\":6476},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7848,\"start\":7845},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10110,\"start\":10107},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10443,\"start\":10440},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":13044,\"start\":13041},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":13216,\"start\":13212},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":13222,\"start\":13218},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":14329,\"start\":14326},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":14500,\"start\":14496},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":14540,\"start\":14536},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":14582,\"start\":14578},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":14752,\"start\":14748},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":15147,\"start\":15143},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":15315,\"start\":15312},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":15928,\"start\":15924},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":16274,\"start\":16271},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":16280,\"start\":16276},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":16420,\"start\":16416},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":16523,\"start\":16519},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":16567,\"start\":16563},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":16638,\"start\":16635},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":16732,\"start\":16729},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":16893,\"start\":16890},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":17023,\"start\":17019},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":17045,\"start\":17041},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":17484,\"start\":17480},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":17611,\"start\":17607},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":17719,\"start\":17715},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":17813,\"start\":17809},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":17853,\"start\":17849},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":17886,\"start\":17882},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":20524,\"start\":20520},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":23497,\"start\":23494},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":25098,\"start\":25094},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":28288,\"start\":28287},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":30185,\"start\":30181},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":30191,\"start\":30187},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":30937,\"start\":30933},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":30943,\"start\":30939},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":35638,\"start\":35635},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":35641,\"start\":35640},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":42034,\"start\":42031},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":42183,\"start\":42182}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":40556,\"start\":39902},{\"attributes\":{\"id\":\"fig_1\"},\"end\":40580,\"start\":40557},{\"attributes\":{\"id\":\"fig_2\"},\"end\":40840,\"start\":40581},{\"attributes\":{\"id\":\"fig_3\"},\"end\":40898,\"start\":40841},{\"attributes\":{\"id\":\"fig_4\"},\"end\":41186,\"start\":40899},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":41582,\"start\":41187},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":41744,\"start\":41583},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":41945,\"start\":41745}]", "paragraph": "[{\"end\":4492,\"start\":2621},{\"end\":4563,\"start\":4494},{\"end\":5537,\"start\":4583},{\"end\":6753,\"start\":5539},{\"end\":8058,\"start\":6755},{\"end\":8927,\"start\":8060},{\"end\":9527,\"start\":8929},{\"end\":9839,\"start\":9546},{\"end\":10111,\"start\":9874},{\"end\":11921,\"start\":10113},{\"end\":12615,\"start\":11939},{\"end\":13417,\"start\":12687},{\"end\":13729,\"start\":13449},{\"end\":14080,\"start\":13751},{\"end\":15672,\"start\":14108},{\"end\":16093,\"start\":15692},{\"end\":16733,\"start\":16095},{\"end\":17485,\"start\":16735},{\"end\":18014,\"start\":17487},{\"end\":18376,\"start\":18016},{\"end\":18820,\"start\":18403},{\"end\":19298,\"start\":18846},{\"end\":19668,\"start\":19300},{\"end\":20111,\"start\":19684},{\"end\":20917,\"start\":20113},{\"end\":22215,\"start\":20937},{\"end\":22402,\"start\":22242},{\"end\":23906,\"start\":22416},{\"end\":24299,\"start\":23921},{\"end\":25598,\"start\":24301},{\"end\":26918,\"start\":25600},{\"end\":27653,\"start\":26920},{\"end\":28599,\"start\":27655},{\"end\":28957,\"start\":28641},{\"end\":31685,\"start\":28971},{\"end\":32443,\"start\":31792},{\"end\":32803,\"start\":32445},{\"end\":33089,\"start\":32818},{\"end\":34335,\"start\":33091},{\"end\":36164,\"start\":34337},{\"end\":36718,\"start\":36166},{\"end\":36974,\"start\":36720},{\"end\":38089,\"start\":36994},{\"end\":38743,\"start\":38091},{\"end\":39329,\"start\":38745},{\"end\":39901,\"start\":39331}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12686,\"start\":12616},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13448,\"start\":13418},{\"attributes\":{\"id\":\"formula_2\"},\"end\":31791,\"start\":31686}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":23035,\"start\":23027},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":24439,\"start\":24432},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":26261,\"start\":26087}]", "section_header": "[{\"end\":4581,\"start\":4566},{\"end\":9544,\"start\":9530},{\"end\":9872,\"start\":9842},{\"end\":11937,\"start\":11924},{\"end\":13749,\"start\":13732},{\"end\":14106,\"start\":14083},{\"end\":15690,\"start\":15675},{\"end\":18401,\"start\":18379},{\"end\":18844,\"start\":18823},{\"end\":19682,\"start\":19671},{\"end\":20935,\"start\":20920},{\"end\":22240,\"start\":22218},{\"end\":22414,\"start\":22405},{\"end\":23919,\"start\":23909},{\"end\":28639,\"start\":28602},{\"end\":28969,\"start\":28960},{\"end\":32816,\"start\":32806},{\"end\":36992,\"start\":36977},{\"end\":39911,\"start\":39903},{\"end\":40566,\"start\":40558},{\"end\":40590,\"start\":40582},{\"end\":40850,\"start\":40842},{\"end\":40908,\"start\":40900},{\"end\":41593,\"start\":41584}]", "table": "[{\"end\":41582,\"start\":41366},{\"end\":41744,\"start\":41624},{\"end\":41945,\"start\":41870}]", "figure_caption": "[{\"end\":40556,\"start\":39913},{\"end\":40580,\"start\":40568},{\"end\":40840,\"start\":40592},{\"end\":40898,\"start\":40852},{\"end\":41186,\"start\":40910},{\"end\":41366,\"start\":41189},{\"end\":41624,\"start\":41595},{\"end\":41870,\"start\":41747}]", "figure_ref": "[{\"end\":10228,\"start\":10220},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":18913,\"start\":18907},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":21002,\"start\":20995},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":23158,\"start\":23150},{\"end\":25798,\"start\":25792},{\"end\":26268,\"start\":26262},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":33127,\"start\":33121},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":34456,\"start\":34449},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":35648,\"start\":35642}]", "bib_author_first_name": "[{\"end\":42561,\"start\":42560},{\"end\":42571,\"start\":42570},{\"end\":42583,\"start\":42582},{\"end\":42596,\"start\":42595},{\"end\":42912,\"start\":42911},{\"end\":42921,\"start\":42920},{\"end\":42934,\"start\":42933},{\"end\":43463,\"start\":43462},{\"end\":43469,\"start\":43468},{\"end\":43471,\"start\":43470},{\"end\":43483,\"start\":43482},{\"end\":43499,\"start\":43498},{\"end\":44269,\"start\":44268},{\"end\":44286,\"start\":44285},{\"end\":44303,\"start\":44302},{\"end\":44305,\"start\":44304},{\"end\":44805,\"start\":44804},{\"end\":44807,\"start\":44806},{\"end\":44817,\"start\":44816},{\"end\":44819,\"start\":44818},{\"end\":45020,\"start\":45019},{\"end\":45029,\"start\":45028},{\"end\":45041,\"start\":45040},{\"end\":45415,\"start\":45414},{\"end\":45423,\"start\":45422},{\"end\":45438,\"start\":45437},{\"end\":45448,\"start\":45447},{\"end\":45458,\"start\":45457},{\"end\":45472,\"start\":45471},{\"end\":45484,\"start\":45483},{\"end\":45802,\"start\":45801},{\"end\":45811,\"start\":45810},{\"end\":45821,\"start\":45820},{\"end\":45838,\"start\":45837},{\"end\":45844,\"start\":45843},{\"end\":46229,\"start\":46228},{\"end\":46237,\"start\":46236},{\"end\":46243,\"start\":46242},{\"end\":46252,\"start\":46251},{\"end\":46266,\"start\":46265},{\"end\":46592,\"start\":46591},{\"end\":46599,\"start\":46598},{\"end\":46609,\"start\":46608},{\"end\":46940,\"start\":46939},{\"end\":46942,\"start\":46941},{\"end\":46953,\"start\":46952},{\"end\":46967,\"start\":46966},{\"end\":46970,\"start\":46968},{\"end\":47226,\"start\":47225},{\"end\":47236,\"start\":47235},{\"end\":47245,\"start\":47244},{\"end\":47614,\"start\":47613},{\"end\":47623,\"start\":47622},{\"end\":47632,\"start\":47631},{\"end\":47641,\"start\":47640},{\"end\":48150,\"start\":48149},{\"end\":48162,\"start\":48161},{\"end\":48172,\"start\":48171},{\"end\":48185,\"start\":48184},{\"end\":48197,\"start\":48196},{\"end\":48555,\"start\":48554},{\"end\":48567,\"start\":48566},{\"end\":48963,\"start\":48962},{\"end\":48975,\"start\":48974},{\"end\":48991,\"start\":48990},{\"end\":49439,\"start\":49438},{\"end\":49446,\"start\":49445},{\"end\":49455,\"start\":49451},{\"end\":49461,\"start\":49460},{\"end\":50324,\"start\":50323},{\"end\":50335,\"start\":50334},{\"end\":50345,\"start\":50344},{\"end\":50769,\"start\":50768},{\"end\":50781,\"start\":50780},{\"end\":50793,\"start\":50792},{\"end\":50806,\"start\":50805},{\"end\":50819,\"start\":50818},{\"end\":50831,\"start\":50830}]", "bib_author_last_name": "[{\"end\":42568,\"start\":42562},{\"end\":42580,\"start\":42572},{\"end\":42593,\"start\":42584},{\"end\":42606,\"start\":42597},{\"end\":42918,\"start\":42913},{\"end\":42931,\"start\":42922},{\"end\":42942,\"start\":42935},{\"end\":43466,\"start\":43464},{\"end\":43480,\"start\":43472},{\"end\":43496,\"start\":43484},{\"end\":43502,\"start\":43500},{\"end\":44283,\"start\":44270},{\"end\":44300,\"start\":44287},{\"end\":44312,\"start\":44306},{\"end\":44814,\"start\":44808},{\"end\":44825,\"start\":44820},{\"end\":45026,\"start\":45021},{\"end\":45038,\"start\":45030},{\"end\":45052,\"start\":45042},{\"end\":45420,\"start\":45416},{\"end\":45435,\"start\":45424},{\"end\":45445,\"start\":45439},{\"end\":45455,\"start\":45449},{\"end\":45469,\"start\":45459},{\"end\":45481,\"start\":45473},{\"end\":45495,\"start\":45485},{\"end\":45808,\"start\":45803},{\"end\":45818,\"start\":45812},{\"end\":45835,\"start\":45822},{\"end\":45841,\"start\":45839},{\"end\":45855,\"start\":45845},{\"end\":46234,\"start\":46230},{\"end\":46240,\"start\":46238},{\"end\":46249,\"start\":46244},{\"end\":46263,\"start\":46253},{\"end\":46272,\"start\":46267},{\"end\":46596,\"start\":46593},{\"end\":46606,\"start\":46600},{\"end\":46613,\"start\":46610},{\"end\":46950,\"start\":46943},{\"end\":46964,\"start\":46954},{\"end\":46980,\"start\":46971},{\"end\":47233,\"start\":47227},{\"end\":47242,\"start\":47237},{\"end\":47252,\"start\":47246},{\"end\":47620,\"start\":47615},{\"end\":47629,\"start\":47624},{\"end\":47638,\"start\":47633},{\"end\":47648,\"start\":47642},{\"end\":48159,\"start\":48151},{\"end\":48169,\"start\":48163},{\"end\":48182,\"start\":48173},{\"end\":48194,\"start\":48186},{\"end\":48203,\"start\":48198},{\"end\":48564,\"start\":48556},{\"end\":48573,\"start\":48568},{\"end\":48972,\"start\":48964},{\"end\":48988,\"start\":48976},{\"end\":48999,\"start\":48992},{\"end\":49443,\"start\":49440},{\"end\":49449,\"start\":49447},{\"end\":49458,\"start\":49456},{\"end\":49466,\"start\":49462},{\"end\":50332,\"start\":50325},{\"end\":50342,\"start\":50336},{\"end\":50352,\"start\":50346},{\"end\":50778,\"start\":50770},{\"end\":50790,\"start\":50782},{\"end\":50803,\"start\":50794},{\"end\":50816,\"start\":50807},{\"end\":50828,\"start\":50820},{\"end\":50837,\"start\":50832}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1906.02888\",\"id\":\"b0\"},\"end\":42876,\"start\":42418},{\"attributes\":{\"id\":\"b1\"},\"end\":43174,\"start\":42878},{\"attributes\":{\"id\":\"b2\"},\"end\":43373,\"start\":43176},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":208139154},\"end\":43859,\"start\":43375},{\"attributes\":{\"id\":\"b4\"},\"end\":44013,\"start\":43861},{\"attributes\":{\"id\":\"b5\"},\"end\":44182,\"start\":44015},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":5746950},\"end\":44545,\"start\":44184},{\"attributes\":{\"id\":\"b7\"},\"end\":44761,\"start\":44547},{\"attributes\":{\"id\":\"b8\"},\"end\":44923,\"start\":44763},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":201810325},\"end\":45330,\"start\":44925},{\"attributes\":{\"id\":\"b10\"},\"end\":45364,\"start\":45332},{\"attributes\":{\"doi\":\"arXiv:1312.5602\",\"id\":\"b11\"},\"end\":45711,\"start\":45366},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":21727593},\"end\":46173,\"start\":45713},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":51870894},\"end\":46529,\"start\":46175},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":4740131},\"end\":46875,\"start\":46531},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":53519955},\"end\":47141,\"start\":46877},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":201813150},\"end\":47518,\"start\":47143},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":155420596},\"end\":47917,\"start\":47520},{\"attributes\":{\"id\":\"b18\"},\"end\":48082,\"start\":47919},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":16297166},\"end\":48486,\"start\":48084},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":52194764},\"end\":48876,\"start\":48488},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":57190581},\"end\":49342,\"start\":48878},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":15146887},\"end\":49887,\"start\":49344},{\"attributes\":{\"id\":\"b23\"},\"end\":50066,\"start\":49889},{\"attributes\":{\"id\":\"b24\"},\"end\":50208,\"start\":50068},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":14273399},\"end\":50656,\"start\":50210},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":2250820},\"end\":51221,\"start\":50658}]", "bib_title": "[{\"end\":43460,\"start\":43375},{\"end\":44266,\"start\":44184},{\"end\":45017,\"start\":44925},{\"end\":45799,\"start\":45713},{\"end\":46226,\"start\":46175},{\"end\":46589,\"start\":46531},{\"end\":46937,\"start\":46877},{\"end\":47223,\"start\":47143},{\"end\":47611,\"start\":47520},{\"end\":48147,\"start\":48084},{\"end\":48552,\"start\":48488},{\"end\":48960,\"start\":48878},{\"end\":49436,\"start\":49344},{\"end\":50321,\"start\":50210},{\"end\":50766,\"start\":50658}]", "bib_author": "[{\"end\":42570,\"start\":42560},{\"end\":42582,\"start\":42570},{\"end\":42595,\"start\":42582},{\"end\":42608,\"start\":42595},{\"end\":42920,\"start\":42911},{\"end\":42933,\"start\":42920},{\"end\":42944,\"start\":42933},{\"end\":43468,\"start\":43462},{\"end\":43482,\"start\":43468},{\"end\":43498,\"start\":43482},{\"end\":43504,\"start\":43498},{\"end\":44285,\"start\":44268},{\"end\":44302,\"start\":44285},{\"end\":44314,\"start\":44302},{\"end\":44816,\"start\":44804},{\"end\":44827,\"start\":44816},{\"end\":45028,\"start\":45019},{\"end\":45040,\"start\":45028},{\"end\":45054,\"start\":45040},{\"end\":45422,\"start\":45414},{\"end\":45437,\"start\":45422},{\"end\":45447,\"start\":45437},{\"end\":45457,\"start\":45447},{\"end\":45471,\"start\":45457},{\"end\":45483,\"start\":45471},{\"end\":45497,\"start\":45483},{\"end\":45810,\"start\":45801},{\"end\":45820,\"start\":45810},{\"end\":45837,\"start\":45820},{\"end\":45843,\"start\":45837},{\"end\":45857,\"start\":45843},{\"end\":46236,\"start\":46228},{\"end\":46242,\"start\":46236},{\"end\":46251,\"start\":46242},{\"end\":46265,\"start\":46251},{\"end\":46274,\"start\":46265},{\"end\":46598,\"start\":46591},{\"end\":46608,\"start\":46598},{\"end\":46615,\"start\":46608},{\"end\":46952,\"start\":46939},{\"end\":46966,\"start\":46952},{\"end\":46982,\"start\":46966},{\"end\":47235,\"start\":47225},{\"end\":47244,\"start\":47235},{\"end\":47254,\"start\":47244},{\"end\":47622,\"start\":47613},{\"end\":47631,\"start\":47622},{\"end\":47640,\"start\":47631},{\"end\":47650,\"start\":47640},{\"end\":48161,\"start\":48149},{\"end\":48171,\"start\":48161},{\"end\":48184,\"start\":48171},{\"end\":48196,\"start\":48184},{\"end\":48205,\"start\":48196},{\"end\":48566,\"start\":48554},{\"end\":48575,\"start\":48566},{\"end\":48974,\"start\":48962},{\"end\":48990,\"start\":48974},{\"end\":49001,\"start\":48990},{\"end\":49445,\"start\":49438},{\"end\":49451,\"start\":49445},{\"end\":49460,\"start\":49451},{\"end\":49468,\"start\":49460},{\"end\":50334,\"start\":50323},{\"end\":50344,\"start\":50334},{\"end\":50354,\"start\":50344},{\"end\":50780,\"start\":50768},{\"end\":50792,\"start\":50780},{\"end\":50805,\"start\":50792},{\"end\":50818,\"start\":50805},{\"end\":50830,\"start\":50818},{\"end\":50839,\"start\":50830}]", "bib_venue": "[{\"end\":42558,\"start\":42418},{\"end\":42909,\"start\":42878},{\"end\":43221,\"start\":43176},{\"end\":43573,\"start\":43504},{\"end\":43888,\"start\":43861},{\"end\":44049,\"start\":44015},{\"end\":44339,\"start\":44314},{\"end\":44586,\"start\":44547},{\"end\":44802,\"start\":44763},{\"end\":45120,\"start\":45054},{\"end\":45338,\"start\":45334},{\"end\":45412,\"start\":45366},{\"end\":45919,\"start\":45857},{\"end\":46334,\"start\":46274},{\"end\":46681,\"start\":46615},{\"end\":46990,\"start\":46982},{\"end\":47301,\"start\":47254},{\"end\":47693,\"start\":47650},{\"end\":47943,\"start\":47919},{\"end\":48262,\"start\":48205},{\"end\":48662,\"start\":48575},{\"end\":49088,\"start\":49001},{\"end\":49594,\"start\":49468},{\"end\":49924,\"start\":49889},{\"end\":50091,\"start\":50068},{\"end\":50406,\"start\":50354},{\"end\":50922,\"start\":50839},{\"end\":43629,\"start\":43575}]"}}}, "year": 2023, "month": 12, "day": 17}