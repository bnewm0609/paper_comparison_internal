{"id": 253406164, "updated": "2022-11-30 14:21:01.061", "metadata": {"title": "QINet: Decision Surface Learning and Adversarial Enhancement for Quasi-Immune Completion of Diverse Corrupted Point Clouds", "authors": "[{\"first\":\"Ruonan\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Wei\",\"last\":\"Gao\",\"middle\":[]},{\"first\":\"Ge\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Thomas\",\"last\":\"Li\",\"middle\":[\"H.\"]}]", "venue": "IEEE Transactions on Geoscience and Remote Sensing", "journal": "IEEE Transactions on Geoscience and Remote Sensing", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "In point cloud completion task, most previous works fail to deal with diverse corrupted point clouds with large missing areas. Meanwhile, they are restricted by discrete point clouds lacking smooth surfaces to represent an object, and the resolution of generated point clouds is fixed once their networks are determined. In addition, the evaluation metrics are not specific for this task. Thus, we propose an innovative quasi-immune completion architecture of point cloud called QINet in this article, which is inspired by the artificial immunization process in biology. Specifically, to increase robustness and adaptation of the model, we conceive a mask algorithm named onion-peeling (OP) to generate diverse corrupted inputs. Meanwhile, two proposed modules are combined together to produce flexible resolution of point clouds, namely, the decision surface learning and adversarial enhancement for the latent representation recovery. The first module transforms point clouds to surfaces with a continuous decision boundary function, while the second module is applied to deduce complete surface from corrupted point cloud by the cooperation of reinforcement learning (RL) and latent generative adversarial network (GAN). Besides, we evaluate the shortcomings of the existing methods and present two novel metrics to support multifaceted comparisons. Experimental results verify that our approach can generate continuous 3-D shapes with optional resolutions compared with other approaches, and achieves competitive results both quantitatively and qualitatively.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/tgrs/ZhangGLL22", "doi": "10.1109/tgrs.2022.3220198"}}, "content": {"source": {"pdf_hash": "2a999d57deffc534ed31cb4d47525a70626972fa", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "53a68dc5b6bb57ee8075191b0c5a3b0ba104163f", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/2a999d57deffc534ed31cb4d47525a70626972fa.txt", "contents": "\nQINet: Decision Surface Learning and Adversarial Enhancement for Quasi-Immune Completion of Diverse Corrupted Point Clouds\n\n\nGraduate Student Member, IEEERuonan Zhang \nSenior Member, IEEEWei Gao \nMember, IEEEGe Li \nThomas H Li \nQINet: Decision Surface Learning and Adversarial Enhancement for Quasi-Immune Completion of Diverse Corrupted Point Clouds\n\nIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING\n60202210.1109/TGRS.2022.32201985705314Index Terms-3-D reconstructionimplicit representation (IR)lidar datapoint cloud completionreinforcement learning (RL)\nIn point cloud completion task, most previous works fail to deal with diverse corrupted point clouds with large missing areas. Meanwhile, they are restricted by discrete point clouds lacking smooth surfaces to represent an object, and the resolution of generated point clouds is fixed once their networks are determined. In addition, the evaluation metrics are not specific for this task. Thus, we propose an innovative quasi-immune completion architecture of point cloud called QINet in this article, which is inspired by the artificial immunization process in biology. Specifically, to increase robustness and adaptation of the model, we conceive a mask algorithm named onion-peeling (OP) to generate diverse corrupted inputs. Meanwhile, two proposed modules are combined together to produce flexible resolution of point clouds, namely, the decision surface learning and adversarial enhancement for the latent representation recovery. The first module transforms point clouds to surfaces with a continuous decision boundary function, while the second module is applied to deduce complete surface from corrupted point cloud by the cooperation of reinforcement learning (RL) and latent generative adversarial network (GAN). Besides, we evaluate the shortcomings of the existing methods and present two novel metrics to support multifaceted comparisons. Experimental results verify that our approach can generate continuous 3-D shapes with optional resolutions compared with other approaches, and achieves competitive results both quantitatively and qualitatively.\n Fig. 1\n. Proposed QINet with flexible output resolutions. The surface tends to be smoother with more details as the number of points increases. RLGAN [9]. and raw data. Meanwhile, point clouds can also be sampled from voxel grids or meshes with additional processing. Point cloud completion requires generating complete shapes from incomplete point clouds. In addition, since human beings are sensitive to visual inconsistency, the recovered point clouds should be perceptually reasonable. Therefore, high-quality completion of point cloud promotes in providing a visually pleasant immersive experience.\n\nGenerally speaking, point cloud completion approaches in traditional ways usually borrow the inspiration of 2-D image completion with a patch-category [5] or diffusion-category method [6]. The former maximizes patch similarity between the source and target images in a specified search space of source images, while the latter reconstructs corrupted parts via diffusive operators designed by constraints of equations and parameters. On the whole, both methods are confined to concentrating on current local information without considering relationships in a global view. As a result, these methods can only recover limited regions. With the growth of learningbased point cloud processing and analysis framework [1], a bunch of methods [7], [8], [9], [10] have achieved remarkable results for point cloud completion tasks. Most of these methods can deduce a complete 3-D geometry via a copy-andpaste view.\n\nHowever, these works face three common challenges. First, the resolutions of generated point clouds are sparse, which usually include 2048 points. Second, the generated point cloud resolutions are restricted by their established network architectures. Third, these methods lack the utilization of their 3-D surface to represent an object, which leads to poor performance for large-area corrupted ratios. Thus, how to design a framework that can overcome these challenges with desirable completion performance? Fig. 2. Immune system process in biology contains three parts [11] and [12]: collection, VC, and AAI. The first part collects infected cells, the second extracts the core part of the vaccine, and the last defeats viruses by adversarial strategy. The surface constructor f s adopts the Marching Cubes algorithm [13]. The detail of the RL agent is given in Algorithm 2. Motivated by the observation in biology that the infected cell of a specific disease can be recovered by the immune system, maybe it is possible to analogize point cloud completion with the mature immune system in biology to solve the above challenges. Inspired by the above phenomena, we hope to find answers to the following two issues.\n\n\nIssue 1: Is it possible to model point cloud completion with the framework of the immune system in biology?\n\nToward this question, we first research the main parts of the immune system [11], [12], which consists of three parts, including collection, vaccine cultivation (VC), and active artificial immunization (AAI), as shown in Fig. 2. Collection gathers infected cells, VC extracts the core ingredients of the vaccine, and AAI engulfs viruses by adversarial strategy.\n\nIn an attempt to explore the point cloud recovery ability from a biological view, we correspondingly construct a novel quasi-immune framework for modeling point cloud completion named QINet, as shown in Fig. 3. The QINet contains preprocessing, phase 1, and phase 2. Preprocessing aims to simulate diverse corrupted point clouds, similar to infected cells produced by collection. Phase 1 transforms discrete point clouds into continuous surfaces via implicit representations (IRs) in latent space, analogizing VC. Phase 2 focuses on the adversarial recovery of point clouds in latent space, simulating the ability of AAI.\n\nIssue 2: Can we solve the above existing three challenges by the proposed QINet and, thus, benefit the completion performance?\n\nTo this end, we explore feasible instance solutions to overcome these challenges. In preprocessing, we adopt two mask generation algorithms to provide diverse corrupted areas, the proposed onion-peeling (OP) method and the random seed sampling (RSS). Motivated by Zheng et al. [8], the proposed OP can produce more than one corrupted area point cloud over several iterations resembling peeling an onion from outside to inside, as shown in Fig. 4. In phase 1, we propose the decision surface learning to learn a smooth surface of the object by IR, which makes the network more generalized with flexible point output. This module can solve the first and second challenges. In phase 2, we introduce the adversarial enhancement for recovery, adopting combined reinforcement learning (RL) and latent generative adversarial network (GAN), to improve the performance with complete 3-D shapes, especially in large-area corrupted ratios; thus, the third challenge is also solved. As visualized in Fig. 1, the generated output shape is guaranteed to be in a 3-D representation with a completely smooth surface, and the output resolution is selectable rather than fixed as assigned points. Besides, due to the lack of targeted measurement, many related works usually apply chamfer distance (CD) to evaluate point cloud completion tasks. However, only CD cannot hold \"completeness invariance,\" since it varies a lot even though they share the same corrupted ratios (details in Section III-E). Therefore, we propose new metrics to satisfy this requirement to assist the existing evaluation measurements.\n\nIn short, the main contributions are concluded as follows. 1) A new quasi-immune point cloud completion architecture called QINet is proposed, including preprocessing, phase 1, and phase 2, which absorbs the essence of the immune system process from a biological perceptive. 2) A novel mask generation algorithm OP is devised in preprocessing to generate diverse corrupted point clouds to enhance the robustness of QINet. 3) Two novel modules are introduced, namely, the decision surface learning in phase 1 and the adversarial enhancement for recovery in phase 2. They jointly guarantee the reconstruction of 3-D shapes with complete surfaces from incomplete point clouds. The resolution of output shapes is much higher than those produced by the existing methods. 4) Two new metrics are proposed to assist the existing ones after revealing the shortcoming of existing evaluation approaches for the completion of corrupted point cloud tasks by us. The remaining parts are introduced as follows. Section II recommends related works relevant to the completion of point clouds and the strategy training. Section III illustrates our QINet and the new evaluation metrics. Section IV presents the experiments and analyses. Finally, Section V draws the conclusion.\n\n\nII. RELATED WORKS\n\n\nA. Point Cloud Completion\n\nIn recent years, as an emerging and fundamental research topic, the point cloud completion task has draw increasing attention. Owing to the growth of 3-D deep learningbased methods [1], [14] and the available large datasets [15], more and more researchers have started to utilize the datadriven-based methods to deal with this open challenge. Here, we reviewed most deep learning-based point cloud completion approaches and separated them into two categories based on their architectures, namely, \"direct\" and \"indirect\" approaches. The \"direct\" approaches mean the processed data are directly the 3-D point cloud with an end-to-end network training process. However, the \"indirect\" approaches mean the training process is decomposed into several phases, and the latent representation is the mainly focused data of processing and analysis. 1) Direct Approaches: These approaches are derived from autoencoders (AEs), which usually establish end-to-end models to generate complete point clouds. In the literature of direct methods, first, Achlioptas et al. [16] introduce a point cloud GAN model. In their work, they apply AE for point cloud reconstruction. It shows that AE [16] performs capably in point cloud completion tasks. Later, PCN [7] proposes a point cloud completion network with coarse-to-fine learningbased network architecture to solve point cloud completion tasks. Stutz and Geiger [17] take an incomplete point cloud as an input to get a complete voxelized shape by a pretrained decoder. The method in [18] aims at unpaired point cloud completion, while the approach in [19] focuses on dense point completion. Overall, PCN uses the folding operation associated in [20] and produces complete point clouds via two steps. TopNet [21] introduces a novel tree structure decoder to solve point cloud completion task. Gridding residual network (GRNet) [22] uses 3-D voxels to regularize irregular point clouds. Cascaded refinement network (CRN) [23] proposes a cascaded refinement framework to synthesize the detailed shape of object. Occupancy network (ONet) [24] presents a general 3-D reconstruction network, which can generate a surface from an image, a point cloud, or a voxelization. However, it is not primarily designed for point cloud inpainting tasks.\n\nThese approaches perform point cloud completion tasks by regressing the 3-D coordinates {x, y, z} of each point, making their architecture inflexible. Once their models are trained, the generated resolution of point clouds is also unmodifiable.\n\n2) Indirect Approaches: Instead of directly constructing an AE with end-to-end manner to regress 3-D coordinates of each point, indirect approaches prefer solving completion of point cloud in latent space. First, these approaches commonly transform an incomplete point cloud into a latent space with \"incomplete\" latent representation. Then, they convert the original problem into the recovery of \"complete\" representation in latent space (CRLS). While direct methods also learn a latent representation, they use a decoder that directly reconstructs a complete point cloud from the incomplete representation in latent space (IRLS) of the incomplete point cloud. In contrast, indirect methods will repair the CRLS from IRLS.\n\nGurumurthy and Agrawal [25] raise the primary point cloud completion via indirect approach. They try to get the optimal IRL by latent optimization, but the training phase suffers from a time-consuming optimization process. Then, Lee et al. [9] proposes RL-GAN Net (RLGAN), which makes the completion processing real time by using an RL agent. However, this approach struggles when the corrupted ratio is large. Though direct approaches are more straightforward, and since they directly optimize CD, the performance of CD is normally obtained with better results. Nevertheless, CD could not accurately evaluate an corrupted point cloud's completeness [19] (we also discuss the limitation of CD in Section III-E). Because indirect methods deal with the latent space of the point cloud and can be seen as a \"divide and conquer\" framework, which usually consists of several associated submodules with each of them trained independently. Because of this, the architecture of indirect methods is commonly more flexible. The proposed method belongs to the domain of indirect methods and can produce flexible resolution with smooth surfaces by recovering the latent representations of implicit surfaces.\n\n\nB. Strategy Training\n\nThe GAN [26], [27] usually includes two antagonistic elements reaching the Nash equilibrium state. During training phase, the generator targets at cheating the discriminator by synthesizing fake samples to resemble real example, while the discriminator distinguishes the true from false data. Although GAN is good at approximating the real distribution, it is easily stacked into a local optimum to create a challenging to train. Various improvements have been proposed ranging from tricks in architecture design to modifications in loss functions [28]. Some practices are also proposed to balance GAN by adding additional input [29] as conditions or using various propagation strategies to minimize the total loss [25].\n\nRL [30], as one of the simulated strategies in machine learning, infers actions from a series of trials and errors communicating with environments. Many approaches have been presented [31], [32], [33] and can be categorized into two main classes, the model-free and the model-based RLs. The former is mainly used in the field of optimal control. Usually, the model-free RLs apply policy optimization [34] and Q-learning [31] to establish the model. The latter is mainly developed in data-driven machine learning where the strategy is optimized though training on a large datasets to approximate the model. Most RL methods usually combine both deep learning and model-free RL with deep-learning networks for approximation of parameters, e.g., deep deterministic policy gradient (DDPG) algorithm [33], which uses actor-critic strategy interpolating between policy optimization and Qlearning [31]. In our completion architecture, both RL and GAN are used to deduce complete latent representations from incomplete ones of implicit surface function. More differences compared with other methods can be presented in Table I.\n\n\nIII. PROPOSED FRAMEWORK\n\nAs shown in Fig. 3, our framework QINet consists of three steps, namely, processing, phase 1, and phase 2. In the following, we begin with the overview of the proposed framework QINet, and then elaborate on each step in detail. Finally, we present two novel evaluation metrics for better grasping the point cloud completion task.\n\n\nA. Inference Overview\n\nThe proposed QINet can execute point cloud completion as follows.\n\n1) Preprocessing intends for generating masks, where we propose OP to simulate diverse corrupted point clouds for input. 2) Phase 1 aims to transform discrete point clouds into continuous surfaces, where we introduce decision surface learning by IR. 3) Phase 2 focuses on recovery ability from incomplete to complete 3-D shape, where we adopt adversarial enhancement for recovery. The whole inference can be modeled as follows:\nS = f s ( f c (G(R( f a (M(X)))), o))(1)\nwhere M(\u00b7), f a,c,s (\u00b7), R(\u00b7), and G(\u00b7) denote mask generation of preprocessing, decision surface functions in phase 1, and adversarial enhancement recovery of RL-agent and L-GAN in phase 2, respectively. X denotes an input point cloud, while o represents the occupancy location of a point cloud in 3-D space. First, generate a diverse corrupted point cloud by M(X). Next, feed M(X) to the analyzer f a to get the IRLS f a (M(X)). Then, the RL watches IRLS to trigger the corresponding action R( f a (M(X))) to get the CRLS via L-GAN denoted as G(R( f a (M(x)))). Finally, the point cloud completion is achieved by combing CRLS with random occupancy position o \u2208 R 3 to eventually get through classifier f c (\u00b7) to get the decision boundary surface, followed by surface constructor f s (\u00b7) to generate a 3-D shape with a complete surface. The corresponding algorithm of QINet is presented in Algorithm 1. \n\n\nAlgorithm 1 QINet Pipeline\n\n\nB. Preprocessing: Mask Generation\n\nIn order to train the point cloud completion model, corrupted point clouds are needed for input. Existing investigations utilize the RSS to generate corrupted point clouds; however, it adopts a seed to erode only one missing area, as shown in Fig. 4, while our OP generates diverse corrupted input to examine the robustness and adaptability of the proposed model. In detail, each point in OP corresponds to a contribution score that reflects its recognition performance. The most significant points make up the generated masks, which brings more challenges to the completion model. On the basis of [8], OP score is calculated via the first derivative (also gradient) of the classification loss function L(X, y), where the gradient is denoted as g. Thus, the score in OP explicitly reflects the response at every point. Here, the proposed OP suggests that shape structure mainly depends on the outer layer points, since they contribute to the shape encoding, while the points of inner layer partially influence the detailed recognition performance.\n\nTypically, L in experiments denotes the cross entropy that is usually applied in classification applications. Assuming that the barycentric point of the whole shape x c can be approximated via median in 3-D coordinates, the variations of losses \u03c3 i can be measured via the distance penalty along gradient. Clearly, the center nodes seem in the same place after transforming from {x, y, z} to \u03c1, \u03c6, \u03c8 (spherical coordinates); thus, the central points contribute little to the shape information, while the outer layer points are more critical.\n\nInstead of only using RSS as previous works [9], [25], both RSS and the proposed OP are used to enhance the robustness and adaptability of the proposed QINet. More differences can be found in Table V. During experiments, T equals to 20, and drop ratio p is used to construct the OP ( p = {40%, 50%, 60%, 70%}). As for recognition network, we choose dynamic graph convolutional neural network (DGCNN) [35], simply because it surpasses the baselines PointNet [1] and PointNet++ [14].\n\n\nC. Phase 1: Decision Surface Learning\n\nTo obtain a compact representation from an point cloud, most methods [7], [9], [25] adopt the point-based AE architecture or a variation of point-based AE, which is devised by Achlioptas et al. [16]. However, point-based AE can only produce fixed resolution of point clouds (e.g., point clouds with 2048 points) with coarse performance, and they are impossible to recover the underlying surface of 3-D objects. Compared with these methods, the IR of 3-D geometry [24], [36] can generate flexible resolution 3-D objects in forms of a complete and smooth 3-D surface in a continuous space. Therefore, in the proposed QINet, we adopt IR to construct decision surface learning, which contains two submodules in Sections III-C1 and III-C2.\n\n1) Decision Boundary Learning: The decision boundary learning can be modeled as follows:\ny = f \u03b8 (X, o) = f c ( f a (X), o), with y \u2208 [0, 1](2)\nwhere f a (\u00b7) and f c (\u00b7) denote the analyzer and the classifier, respectively. \u03b8 expresses the hyperparameters of the decision surface. The analyzer extracts the meaning features of the decision boundary; then, we can define a binary classifier f c , which can divide space into two parts: inner and outer space of the 3-D object. This classifier is implemented by a neural network that assigns an occupancy probability to a location (o \u2208 R 3 ) in 3-D space. In addition, the output occupancy probability of the classifier is dependent on the latent and compact representation generated by the analyzer f a (\u00b7). Expressly, the classifier is guided by the input point cloud. Here, we formulate the decision boundary function as y, which represents the occupancy position o \u2208 R 3 attaching to the referenced input point cloud X.\n\nIn detail, decision boundary learning is composed of an analyzer f a that converts an input point cloud into a 512-D conditional encoding c and a classifier f c that carries a group of 3-D T points and the conditional encoding c as inputs and calculates the occupancy of each 3-D coordinate. The classifier is conditioned by conditional batch normalization (CBN) [37], [38] on c. Then, two vectors of 256 dimension, \u03b3 (c) and \u03b2(c), are obtained after c goes through two fully connected (FC) layers. The final features f C B N through CBN to the classifier are normalized by the output feature of analyser f a (X) as follows:\nf CBN = \u03b3 (c) f a (X) \u2212 \u03bc \u221a \u03b4 2 + + \u03b2(c)(3)\nwhere \u03b4 and \u03bc, respectively, represent the standard deviation and the mean of f in with = 10e \u22125 . The detailed architectures of analyzer and classifier are presented in Tables II and III. Residual block (Res) can be referred to [39]. During training, N occupancy position {o i \u2208 R 3 , i = 1, . . . , N} is produced by a 3-D shape of inputs. The overall optimization is expressed as follows:\nL(\u03b8 ) = N i L c f \u03b8 (o i , X), g o i(4)\nwhere L c represents the loss of cross entropy, and g o i represents the true label for the occupancy of point o i . In practice, this loss is trained by Adam optimizer without weight decay.\n\nLearning rate equals to 10e \u2212 4, \u03b2 = {0.9, 0.999} with = 10e \u22128 .\n\n2) Surface Construction: After training via (4), the occupancy of every possible 3-D location in the bounding volume can be estimated. According to the output occupancy f c (X, o), each evaluated location o will be labeled as occupied or not in terms of preset threshold \u03b1. Then, the occupied locations will be reserved, and the isosurface will be generated by\nf s (y) = f s ( f \u03b8 (X, o)), with y \u2208 [0, 1](5)\nwhere f s (\u00b7) represents surface constructor, which is implemented by the Multiresolution IsoSurface Extraction algorithm [24]. A mesh then with an IR can be reproduced from isosurface by the Marching Cubes algorithm [13]. Given a well-trained f a and f c that can accurately predict the occupancy of every 3-D location, f s can implicitly represent the surface of a 3-D point cloud. As an additional advantage, 3-D shapes with various resolutions can be generated via varying sampling points from the learned surface.\n\n\nD. Phase 2: Adversarial Enhancement for Recovery\n\nSimply relying on Phase 1, no complete 3-D shapes can be recovered from only corrupted point clouds. Feeding an incomplete input, the analyzer f a will generate an IRLS. The IRLS is outside the CRLS learned by IR network; thus, using IRLS will lead the classifier f c to produce an incomplete shape. If the complete latent representation can be reconstructed from the incomplete one, the classifier f c guided by the CRLS will generate complete 3-D shapes. Therefore, we can convert point cloud completion to recovering the complete latent representation. Following this principle, we first adopt an L-GAN to learn the distribution of the manifold, which infers the complete latent representations. Next, we use RL to choose the right action z by observing the IRLS for L-GAN to generate a CRLS.\n\n1) Inferring Complete Latent Representation: Explicitly modeling the manifold of the CRLS is hard. Fortunately, given a set of real samples, GAN [40] is good at generating realistic samples by learning the data distribution from the training set. Similar to [25] and RLGAN [9] to train an L-GAN, we adopt a different path where the L-GAN is trained on the CRLSs generated from the pretrained phase-1 module, which is defined as follows:\nf s ( f c (G(z), o), G(z) \u223c f a (X))(6)\nwhere a new CRLS G(z) will be generated from a noise z by the generator G of L-GAN. Then, under the guidance of the CRLS, the classifier f c associated with f s will generate a complete 3-D shape with a smooth surface. As for the optimization, We use the WGAN-GP [41] adversarial loss to train our L-GAN, which is defined as follows:\nL G = E[D(G(z)] \u2212 E[D(x a ))] + \u03bb gp E \u2207 x r D(x r ) 2 \u2212 1 2 (7)\nwhere G(\u00b7) and D(\u00b7) denote the generator and the discriminator, respectively. x a and x r represent complete latent representation got from ground-truth (GT) or random point of f a (\u00b7). \u03bb gp = 10. In practice, we adopt the architecture in [42] to implement L-GAN in the already-trained phase-1 module. The details of the modified generator G(\u00b7) are shown in Table IV, and the discriminator D(\u00b7) has a symmetrical structure with G(\u00b7). The Adam optimizer with \u03b2 1 = 0.5 and \u03b2 2 = 0.9 is used with learning rate 1e \u22124 . z vector is 1-D, and the output of the generator is 512-D.\n\n2) Recovering from Incomplete to Complete Latent Representation: An RL agent is adopted to recover a CRLS from an IRLS. In general, the agent of RL monitors the observation x t at each timed t step, and then chooses an action a t . The action a t will affect the environment, and thus, the agent will receive a reward r t . the RL agent studies a policy named \u03c0 from the environment (ENV) to model the relation between state and action. The ENV is usually modeled by the Markov decision as: current states and actions are merely affected by the previous one. For example, in RLGAN [9], they make an assumption that the recent most observation x t is adequate to decide the state s t . We also follow this assumption. The final objective of RL agent is to find a policy that gives the maximum reward.\n\nCorresponding to our setup, the observed state s t is the IRLS (also noisy latent code, which is regarded as an active virus) coming from the analyzer f a . The action a t is the revised noise of z (also clean latent code, which is regarded as a vaccine), which is then transferred to L-GAN. IR network and L-GAN together are the whole environment. To learn a correct policy \u03c0, RL agent has to get a reliable reward to make decision from interacting with the environment. In this situation, two rewards are applied, namely, latent reconstruction reward r latent and the reconstruction of shape reward r rec . r rec denotes volumetric intersection over union (IoU) [24] \nr rec = IoU M pred , M GT = M pred \u2229 M GT M pred \u222a M GT(8)\nwhere M pred is a set of points, which belong to the predicted mesh, so does M GT for GT mesh. r rec guarantees the similarity in 3-D shape, while r latent ensures the similarity in latent space that is defined as follows:\nr latent = \u2212 G(z) \u2212 f a (P in ) 2 2(9)\nwhere G(z) and f a (P in ) represent the generated latent representation and the IRLS, respectively. Thus, r latent guarantees the semantic similarity of the generated point clouds to the original inputs in latent space. Finally, the total reward is concluded as the weighted sum of the above rewards\nr all = \u03b1 1 \u00b7 r rec + \u03b1 2 \u00b7 r latent(10)\nwhere \u03b1 1 and \u03b1 2 are the weights corresponding to their rewards. r all denotes r for short. When these rewards are combined, the value of each term is in a similar range with normalization. After examining in experiments for multiple trails, we fix \u03b1 1 = 10 and \u03b1 2 = 1e \u22124 for all our experiments. RL is then trained with open-source code of DDPG [33], actor and critic [9], pretrained L-GAN, and phase-1 module.\n\nIn particular, the involving actor and critic are composed of FC layers. The actor includes four FC layers with the number of nodes in (400 with rectified linear unit (ReLu), 400 with ReLu, 300 with ReLu, and 300 with tanh), while the critic contains four FC layers (400, 401, 300, an d300) with ReLu activation. I RLS = f a (P in ) 6: if t step \u2265 StartT rain then 7: a t \u2190\u2212A \u2190\u2212 IRLS 8: else 9: Random action a t 10: end if 11: (a t , r t , s t , s t+1 ) \u2190\u2212 environment (ENV) \u2190\u2212 a t 12: Transition (a t , r t , s t , s t+1 ) are stored in R 13: if t step > StartT rain then 14: Train A and C with R 15: end if 16 \n\n\n1) Collect Experiences in Buffer R:\n\nIn this stage, helpful experiences are fed to the buffer R. The environment receives a random action and produces a reward. Then, the state, reward, and action are stored in the replay buffer. The state is a 512-D vector, which is the CRLS obtained by the analyzer f a , and the dimension of action is 1 according to L-GAN output. 2) Train the Actor C and Critic A. In this stage, the actor and critic networks are trained with 100 randomly sampled memories in R. This process involves five parameters, namely, the iterations is 1e6, the action noise is 0.1, the batch size is 100, the discount is 0.99, and the speed of target value updates is 0.005.\n\n\nE. Proposed Evaluation Metrics\n\nIn this section, we begin with the defining the \"completeness invariance.\" Next, we talk about the CD's shortage for completion of point cloud tasks and then propose two kinds of metrics, namely, adaptive-enhanced CD (AE-CD), the coverage (cov), and F-score coverage (F-score).\n\nDefinition (\"Completeness Invariance\"): Suppose X is a complete point cloud, X = mask(X, \u03b1, p) is a mask function 1 that generates a corrupted point cloud X referring to the corrupted ratio \u03b1 and the corrupted centroid p \u2208 R 3 , an evaluation method for completion of 3-D shape f (X, X ) = f (X, mask(X, \u03b1, p)). If the value of f (X, mask(X, \u03b1, p)) does not vary with p, the evaluation metric f owns invariance of completeness.\n\nMost previous approaches [7], [9], [25] use CD as the evaluation metric defined as follows:\nd C D (S 1 , S 2 ) = 2 S a ,S b \u2208S 1 ,S 2 1 2|S a | x\u2208S a min y\u2208S b x \u2212 y 2 2 (11)\nwhere S 1 and S 2 denote two point clouds, and x and y denote the point in S 1 and S 2 , respectively. CD is efficient with flexible number of points, which is a feasible metric when computing two complete point clouds. But, in the case of completion of point cloud tasks, it fails to measure the completeness of point cloud, since it evaluates the point-topoint Euclidean distance between two sets, which is liable to outliers [16], [43], and it cannot own \"completeness invariance\" in the case of making a comparison between 3-D shapes. Examples are illustrated in Fig. 7; the first row proves that CD drifts significantly with varying the corrupted part of the same object, even though they share the same corrupted ratio. The second row advises that CD is not \"completeness invariance.\" In the case of 60% corrupted ratio, the value of CD is lower than that at 40% and 50%, respectively. Thus, we need a metric, which satisfies \"completeness invariance\" and consistence with the varying corrupted ratio.\n\n1) Adaptive-Enhanced CD: Because CD treats each point equally without focusing on the corrupted parts, AE-CD is then proposed with a one-way CD metric, which emphasize Comparisons on point cloud metrics: CD (11), Cov (13), and F-score (14). Corrupted shapes here are produced by RSS from complete point cloud. more on the corrupted part of GT, which treats every point with weights according to the mask. The challenge of this task is to complete the missing parts, which is the main difference among different methods. Thus, we show the results of AE-CD to distinguish the completed missing and the remaining points\nd AE\u2212C D (S eval , S GT ) = 1 |S GT M | \u03c4 x\u2208S GT M min y\u2208 eval x \u2212 y 2 2 + 1 |S GT R | (1 \u2212 \u03c4 ) x\u2208S GT R min y\u2208S eval x \u2212 y 2 2(12)\nwhere S GT M and S GT R denote the missing and remaining points of GT, respectively. gamma is the corrupted ratio of incomplete point clouds. For instance, \u03c4 = 0.6 \u00d7 10 if the mask drops 60% points of GT. The insight of AE-CD is that when the missing region becomes larger, \u03c4 becomes bigger too, allowing AE-CD gives more punishments to the missing points. Only when the corresponding missing parts of S eval are reconstructed well, the value of AE-CD will become small. \u03c4 makes AE-CD adaptive to different corrupted ratios with help of the mask information.\n\n2) Coverage: Two metrics, including Cov and F-score, are given to remedy the drawback of single CD, which, at the same time, recognize integrality of the corrupted input point clouds. Cov computes the number of ratio of the nearest neighbor\n(NN) of S eval to S GT Cov(S eval , S GT ) = arg min y\u2208S GT d(x, y) x \u2208 S eval |S GT |(13)\nwhere d(\u00b7, \u00b7) is L 2 norm. The formulation of the set can be explained as follows: for each point x of the evaluated points S eval , compute its NN y in S GT ; then, the numerator is formed by all y values. Here, Cov forecasts the coverage of evaluated point cloud versus GT. But, Cov is optimized through covering 3-D space with dense points. Thus, we present far better F-score\nF(S eval , S GT ) = 2Cov(S eval , S GT )Cov(S GT , S eval ) Cov(S eval , S GT ) + Cov(S GT , S eval )(14)\nwhere F-score has the property that if either Cov(S eval , S GT ) \u2192 0 or Cov(S GT , S eval ) \u2192 0, then F(S eval , S GT ) \u2192 0. In contrast to CD, from Fig. 7, the proposed Cov and F-score own \"completeness invariance\" (top) as well as accord with the integrity of corrupted point clouds (bottom). Totally, F-score is applied to gauge the integrity, whereas CD measures the distance between two sets.\n\n\nIV. EXPERIMENTS AND ANALYSES\n\nThis section introduces datasets, metrics, implementation details, comparison with SOAT, and ablation studies.\n\nA. Preparation 1) Data: From ShapeNet [15], we select four classes (with the most number of shapes) as our four-class dataset, and we also choose most used eight categories as our eightclass dataset. To illustrate the adaptation of our method, four datasets are then designed based on ShapeNet, namely, RSS four-class and RSS eight-class datasets, and OP four-class and OP eight-class datasets.\n\n2) Evaluation Metrics: Two forms of metrics are used for final evaluation: 1) point cloud metrics and 2) 3-D surface metrics. The first case includes the CD, the AE-CD, and the F-score (14. We apply point cloud metrics to measure the results of completion of point cloud approaches. To obtain consistent output, the same resolution is applied by us as other approaches, which are uniformly sampled from our generated mesh. The second case, including a normal consistency score (NCS) [24] and volumetric IoU (IoU) (8), is used to evaluate the performance of generating meshes.\n\n3) Environment Setup: PyTorch [44] is used in our implementation. We apply a NVIDIA Titan Xp GPU in training. As for the IR network training, we apply complete 3-D shapes mentioned above. As for the L-GAN training, we generate CRLS by pretrained phase 1, and the L-GAN is then trained on these CRLSs. The RL agent is trained on incomplete datasets mentioned in Section IV-A.\n\n\nB. Comparisons With SOTA Methods\n\nWhile limited previous works reconstruct whole meshes from corrupted point clouds, in our comparisons, two point cloud related completion methods as well as a mesh generation method are included, namely, AE [16], RLGAN [9], and ONet [24]. RLGAN [9] is chosen as an indirect completion of point cloud method, which applies latent representation. AE [16] is a basic part of RLGAN to generate models, which is regarded as a baseline in point cloud domain. ONet [24] is selected as a mesh baseline because of its state-of-theart (SOTA) mesh generation and partial contribution to our IR network. These methods are compared with our dataset by retraining their networks. The differences compared with these methods are listed in Table I. RL represents RL agent in latent representation recovery module, and IR represents continuous representation for 3-D module. From the aspect of properties of these methods, it is clear to see that our method has different properties from other methods. Hence, we do not make comparisons with these direct methods (GRNet, CRN, TopNet, and PCN). In particular, our method has three distinct Fig. 8. Completion performance of corrupted points cloud in the same class with SOTA methods. AE [16], RLGAN [9], and ONet [24]. advantages: diverse masks by OP, flexible output (i.e., any resolution point cloud), and multimetric measurements. All the mentioned direct methods do not own any of these properties, and therefore, we cannot compare them fairly and equally. Otherwise, the unfair comparisons between ours (indirect method) and these direct methods are not convincing.\n\n1) Quantitative Results: Table V gives the quantitative results of SOTA. Owing to the surface reconstruction and adversarial enhancement for recovery in latent space, which guarantees the completeness of the 3-D shape, the proposed method ranks first in the case of 60% and 70% corrupted ratios, which can complete the whole 3-D shape with the correct class label, while others cannot or complete the wrong class. In the case of 40% or 50% corrupted ratios, although AE in CD metric performs the best in some datasets, its completed shape quality is unsatisfactory, and its generated point clouds are quite sparse. Because AE only targets minimizing CD loss, when the corrupted area is small, CD seems working, but with the corrupted area increasing, its shortcoming appears. Our method, before being compared with AE, needs random downsampling operation from surface to discrete points to calculate CD metric; thus, the superior of our method is weakened, since the offsets from random downsampling operations mainly happen in the remaining part of GT (60%). When evaluating coverage metrics, ONet ranks first, but our performance on CD and AE-CD surpasses theirs. This is because ONet is originally trained with surface metrics to construct the surface from only the remaining part of GT instead of corrupted parts of GT. As the remaining part (60% of GT) is dominant, its overlap with GT may be higher than our method, which focuses more on corrupted parts. But, as the corrupted area is increasing, e.g., in Fig. 5, its perceptual quality degrades severely.\n\n2) Qualitative Results: Fig. 5 shows that in the case of 60% corrupted ratio, both ONet and AE are unable to generate  [9]. In Fig. 11, the compared results in the form of different corrupted ratios of the proposed OP mask also show that when the mask is more complex, other methods seem confused, while our method performs best. In Fig. 8, we show the point cloud completion performance of the two example classes in SOTA methods. The proposed method still holds the best complete shape compared with other approaches in chair class, especially providing the whole shape with the correct class label, while others either recover partial complete 3-D shapes or recover the wrong 3-D shapes with wrong class labels. Meanwhile, our method distinguishes the samples in the same class, while other methods fail to grasp these distinctions. For the airplane class, we can have the same conclusion. In short, our method can produce wellperformed complete 3-D shapes with distinction whether in the same class or not.\n\n\nC. Ablation Studies\n\nThis section first provides the analyses of the reward function, latent representation, and proposed F-score metric. Then, more results are also given and discussed regarding surface reconstruction, various resolutions, and an application.\n\n1) Module Analysis: To further illustrate the effectiveness of the proposed modules, we do the ablation study shown in Table VI. Our QINet, with both decision surface learning and adversarial enhancement for recovery, performs best compared with either of them. Notably, these two modules focus on different goals, one for multiresolution output, the other for complete 3-D shape, the combination of them fulfills the flexible output with good completion performance, and these are why our QINet stands out.\n\n2) Reward Analysis: Various reward combinations are presented in Table VII. Since the reconstruction and the latent rewards are complementary, we combine them to get the best performance.\n\n3) Latent Space Analysis: Since our method performs point cloud completion through recovering CRLS from IRLS of point cloud, clustering CRLSs and IRLSs are illustrated.   (11) and F-score (14). Fig. 11. Qualitative completion performance of corrupted point cloud with SOTA methods in different corrupted ratios. AE [16], RLGAN [9], and ONet [24].\n\nWe use the test set of our random four-class datasets mentioned in Section IV-A to generate the CRLSs and IRLSs in the case of 70% corrupted ratio. The real CRLSs and the IRLSs are generated from the analyzer f a by taking both complete point Fig. 12. Compared visualization results of classification labels. RLGAN [9] and ONet [24].\n\nclouds and corrupted point clouds as inputs, respectively. The generated CRLSs are produced from the RL agent and L-GAN by observing IRLSs. Subsequently, we apply t-distributed stochastic neighbor embedding (T-SNE) [45] to get a compact embedding of the high-dimensional CRLSs and IRLSs in R 2 .\n\nIn Fig. 6(a), it can be seen that the real latent representations of complete point clouds are easily separable, which Fig. 13. Classification performance comparison with SOTA methods. AE [16], RLGAN [9], and ONet [24].  means that the analyzer f a can extract meaningful features. In Fig. 6(b), because the corruption of point clouds makes the semantic information loss of 3-D shape, the latent representations generated from incomplete point clouds are not as separable as those generated from complete point clouds. It shows that the corruption of point cloud also leads to the corruption of its latent representation, making the boundary of T-SNE between different classes vague. In Fig. 6(c), the distribution of the generated latent representations produced by RL agent and L-GAN is quite different from Fig. 6(a) and (b). The latent representations of each class are purified and grouped, which means that the RL agent can produce the shapes whose semantic information is consistent with input incomplete point clouds. Since the generated CRLSs are produced by GAN taking the action of RL agent as input, the \"curve\" phenomenon allows the RL agent to gather the samples of the same class, which can clean the incomplete (noisy) latent representation corresponds to the function of the inactivation step in the VC.\n\nAs a result, when receiving an incomplete point cloud, the RL agent tends to generate commonly seen 3-D shapes, which enhance the accuracy of classification. Interestingly, some difficult examples blend in these three cases, such as tables and airplanes, which introduce an open question to research: how to deal with these hard examples in limited circumstances, denoising, data enhancement, or other effective preprocessing operations in point clouds? These situations are worth considering in future work. \n\n\n4) F-Score Analysis:\n\nFirst, we design an experiment to test its completeness invariance. Every model in the dataset is sampled 100 000 points from the mesh with uniform distribution as the point cloud GT and uses the RSS to produce ten corrupted point clouds. These incomplete point clouds share the same corrupted ratios with different missing positions. Then, 2048 points are uniformly sampled from GT and the incomplete point clouds. CD and F-score are computed from those point clouds with 2048 points. We keep the number of points of measured point cloud the same to eliminate the impact of a different number of points, and this condition satisfies the standard measurement in completion task. Finally, the variance of CD and F-score values computed from ten incomplete point clouds with the same corrupted ratio is recorded. Table VIII shows that the variance is averaged among the 1355 models. It can be observed that when the corrupted ratio is the same, the variance of F-score is lower than that of CD. From Fig. 9, the value of F-score remains stable with the change of missing location, but the value of CD fluctuates up and down. Therefore, it can be concluded that the F-score is more robust than CD when the missing location of incomplete point clouds changes, but the corrupted ratio remains the same.\n\nWe also conducted experiments in Table IX to determine whether there is a monotonic relationship between the corrupted ratio and the F-score and how strong the relationship is between the missing nation and the F-score. Two evaluation criteria are included, the Spearman rank order correlation coefficient (SROCC) and Pearson linear correlation coefficient (PLCC). For every model in the dataset, 100 000 points from the mesh are sampled with uniform distribution as the complete point cloud, and then, the RSS mask produces nine corrupted point clouds. The corrupted ratios of those incomplete point clouds range from 10% to 90%. Then, 2048 points are uniformly sampled from GT and incomplete point clouds, and both CD and F-score are computed from those point clouds with 2048 points. Finally, we compute the absolute values of PLCC and SROCC between the corresponding values of CD or F-score and the corrupted ratios. Fig. 10 shows the results where the absolute values of PLCC and SROCC are averaged among 1355 models. We can see that the F-score is more correlated with the corrupted ratio than CD. When the corrupted ratio becomes more massive, the value of F-score monotonically decreases along with the increased corrupted ratio, while CD has no such trend.\n\n\n5) Comparison for Surface Reconstruction:\n\nSince point cloud can be reconstructed as a mesh via a surface reconstruction algorithm, we talked about the discrepancy between other surface reconstruction approaches and ours as presented in Figs. 5 and 11. We apply surface reconstruction algorithm [46] in point clouds from GT and RLGAN. Our method can produce flexible output resolution, because the deduced smooth surface allows us to generate meshes with high resolution; it is obvious that only 2048 points are contained in GT point cloud and the output of RLGAN. Consequently, the shapes produced by our method are smoother and perceptually more plausible, while GT and RLGAN have artifacts.\n\n6) Shapes With Various Outputs: One major virtue of the proposed approach is that we could generate 3-D shapes with varying output resolutions by training one model, as shown in Fig. 15. This is because the implicit surface extracted by IR network supports the different resolution samplings. As we sample more vertices, the smoother surface and more detailed reconstruction of the 3-D model can be seen.\n\n\n7) Application in Classification:\n\nTo further verify the shape completion performance in 3-D applications, we measure the classified accuracy of various methods in all datasets varying with distinct missing rates. DGCNN [35] is selected as the basic classification network to assess the performance of the classification. Fig. 13 presents the results of all datasets, and our approach obtains the highest accuracy with varying corrupted ratio. Table V shows the corresponding instance, which further illustrates the robustness of the proposed method with varying the corrupted ratio. Observed by the additional visualizations in Fig. 12, our QINet holds correct labels with complete shapes, while others give the wrong predictions in an incomplete generated shape.\n\n\nV. CONCLUSION AND DISCUSSION\n\nA novel perceptive point cloud completion method in this article, QINet, is proposed, which aims at reconstructing a smooth 3-D shape with arbitrary output resolution from a large corrupted point cloud. First, we devise a novel mask algorithm, OP, to simulate multiarea corrupted point clouds. In addition, rather than direct methods, from an input point cloud, we begin with inferring the underlying continuous surface, and then implement the recovery process by adversarial enhancement. Because of the conversion of discrete to continuous representation, flexible resolution of the complete shape can be accomplished. Finally, based on the examination of the existing evaluation metrics, we devise two novel metrics to assist the existing ones. The overall experiments show their competitiveness qualitatively and quantitatively and prove that forming a continuous 3-D shape could generate a complete point cloud with optional resolutions. Furthermore, the proposed biology-inspired approach with latent representation, adversarial learning, and RL can also be easily extended to the other investigations in point cloud processing and analysis. In the future, more works are worth exploring to further improve the completion performance, such as enriched IR, enhanced RL strategy, and training losses with better specific metrics for the completion tasks.\n\nFig. 3 .\n3Framework of the proposed QINet. (Top) Overview of QINet. QINet consists of three parts, preprocessing, phase 1, and phase 2. The preprocessing generates diverse corrupted masks. The mask generation algorithm is detailed in Fig. 4. Phase 1 learns the decision surface, transforming discrete point clouds to continuous surface representation. This part permits flexible resolution output, which involves three submodules: analyzer f a , classifier f c , and surface constructor f s . Phase 2 (denoted by right dashed box) achieves recovery from incomplete to CRLS by adversarial strategies [RL agent associated with latent-space GAN (L-GAN)] to improve the point cloud completion performance. (Bottom) Detailed architecture of decision boundary learning (analyzer f a and classifier f c ) in phase 1 and L-GAN G in phase 2.\n\nFig. 4 .\n4Illustration of the OP Algorithm in Section III-B.\n\n\nInput: Input complete point cloud: X, random occupancy position o \u2208 R 3 Output: Complete surface: S 1: Initialization /* Preprocessing */ 2: Generate corrupted mask M by OP or RSS algorithm /* Phase 1 */ 3: Train the analyser f a (.) and the classifier f c (.) via Eq. 4 4: Get the surface constructor f s (.) via Eq. 5 5: Calculate the IRLS: IRLS \u2190 f a (M(X)) /* Phase 2 */ 6: Train L-GAN G(.) via Eq. 7 7: Train the RL agent R(.) via Alg. 2 8: Calculate the CRLS: CRLS \u2190 G(R(I RLS)) 9: Get the surface: S \u2190 f s ( f c (C RL S, o)) 10: return\n\nFig. 6 .\n6Visualization of the T-SNE [45] clustering in the latent representations. (a) CRLS. (b) IRLS. (c) Generated CRLS. Algorithm 2 RL Agent Training Process Input of Agent: State : s out =IRLS, Reward: r t (Eq. 10). Output of Agent: Action: a t = z; CRLS=G(z). Final Output: S. 1: Initialization of environment ENV according to analyser f a , classifier f c , occupancy position o \u2208 R 3 , surface constructor f s and generator G. 2: Initialization of policy \u03c0 according to critic C, actor A, DDPG, replay buffer R, and DDPG algorithm. 3: while t step < maxsteps do 4: Abtain P in 5:\n\n\n: end while 17: ENV(P in , a t ) : 18: Get State (s t ): I RLS \u2190\u2212 f a (P in ) 19: Implement Action: CRLS\u2190\u2212 G(z = a t ) 20: Calculate reward r t 21: Get surface: S \u2190\u2212 f s f c ((C RL S, o)) 22: return Finally, Algorithm 2 summarizes the procedure of training with two stages included.\n\nFig. 7 .\n7Fig. 7. Comparisons on point cloud metrics: CD (11), Cov (13), and F-score (14). Corrupted shapes here are produced by RSS from complete point cloud.\n\nFig. 9 .\n9Visualization example of the variances of CD (11) and F-score (14) with a corrupted ratio of 70% at different locations. (a) Visualized example. (b) Difference among the missing parts.\n\nFig. 10 .\n10Visualization example of PLCC and SROCC between the corrupted ratios and the evaluation metrics. (a) Visualized example. (b) Discrepancies among the missing ratios. CD\n\nFig. 14 .\n14Surface reconstruction experiment and comparison. (a) GT. (b) Inputs. (c) Meshes of GT. (d) RLGAN [9]. (e) Meshes of RLGAN [9]. (f) Ours.\n\nFig. 15 .\n15Compared results with different resolutions. (a) GT. (b) Ours: low resolution. (c) Ours: middle resolution. (d) Ours: high resolution.\n\nTABLE I\nICOMPARISONS OF PROPERTIES WITH SOTA METHODS. M: MASK. IR: IMPLICIT REPRESENTATION\n\nTABLE II\nIIARCHITECTURE OF ANALYZER f a . GRAY BOX REPEATS FOUR TIMESTABLE III \n\nARCHITECTURE OF CLASSIFIER f c . GRAY BOX REPEATS FIVE TIMES. \nD AND c DENOTE DIMENSION AND CONDITIONAL \nENCODING, RESPECTIVELY \n\n\n\nTABLE IV\nIVARCHITECTURE OF THE GENERATOR OF L-GAN. SA IS FROM[42].SN  AND BN DENOTE SPECTRAL AND BATCH NORMALIZATION, RESPECTIVELY. C c a k bs d p DENOTES A 2-D CONVOLUTION WITH c KERNELS OF SHAPE a \u00d7 a, STRIDE b, AND PADDING d\n\nTABLE V\nVQUANTITATIVE PERFORMANCE IN POINT CLOUD COMPLETION TASK COMPARED WITH SOTA APPROACHES Fig. 5. Completion performance of corrupted point cloud with other SOTA methods on random eight-class dataset. AE[16], RLGAN[9], and ONet[24].\n\nTABLE VI\nVIABLATION STUDY ON DIFFERENT MODULES IN 70% CORRUPTED RATIO ON THE RSS EIGHT-CLASS DATASETTABLE VII \nREWARD FUNCTION ANALYSIS \n\ncomplete 3-D shapes. Ours and RLGAN can produce a com-\nplete 3-D model in the case of varying corrupted ratios, yet \nRLGAN's results are dirty and can be predicted in the wrong \ncategory as mentioned in \n\nTABLE VIII RESULTS\nVIIIOF COMPLETENESS INVARIANCE EXPERIMENT TABLE IXCOMPARISON OF PLCC AND SROCC BETWEEN CD AND F -SCORE\nWe take the RSS mask algorithm as the example.\nACKNOWLEDGMENTThe authors would like to thank the editor, associate editor, and anonymous reviewers for their valuable comments. They would also like to thank Wei Yan and those who helped and supported them in this project.\nPointNet: Deep learning on point sets for 3D classification and segmentation. R Q Charles, H Su, M Kaichun, L J Guibas, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)R. Q. Charles, H. Su, M. Kaichun, and L. J. Guibas, \"PointNet: Deep learning on point sets for 3D classification and segmentation,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jul. 2017, pp. 652-660.\n\nA deep neural network with spatial pooling (DNNSP) for 3-D point cloud classification. Z Wang, L Zhang, R Li, Y Zheng, Z Zhu, IEEE Trans. Geosci. Remote Sens. 568Z. Wang, L. Zhang, R. Li, Y. Zheng, and Z. Zhu, \"A deep neural network with spatial pooling (DNNSP) for 3-D point cloud classification,\" IEEE Trans. Geosci. Remote Sens., vol. 56, no. 8, pp. 4594-4604, May 2018.\n\nA dense feature pyramid network-based deep learning model for road marking instance segmentation using MLS point clouds. S Chen, Z Zhang, R Zhong, L Zhang, H Ma, L Liu, IEEE Trans. Geosci. Remote Sens. 591S. Chen, Z. Zhang, R. Zhong, L. Zhang, H. Ma, and L. Liu, \"A dense feature pyramid network-based deep learning model for road marking instance segmentation using MLS point clouds,\" IEEE Trans. Geosci. Remote Sens., vol. 59, no. 1, pp. 784-800, Jan. 2021.\n\nA weakly supervised graph deep learning framework for point cloud registration. L Sun, IEEE Trans. Geosci. Remote Sens. 60L. Sun et al., \"A weakly supervised graph deep learning framework for point cloud registration,\" IEEE Trans. Geosci. Remote Sens., vol. 60, pp. 1-12, 2022.\n\nAdaptive nonrigid inpainting of three-dimensional point cloud geometry. C Dinesh, I V Bajic, G Cheung, IEEE Signal Process. Lett. 256C. Dinesh, I. V. Bajic, and G. Cheung, \"Adaptive nonrigid inpainting of three-dimensional point cloud geometry,\" IEEE Signal Process. Lett., vol. 25, no. 6, pp. 878-882, Jun. 2018.\n\nPDE-based graph signal processing for 3-D color point clouds: Opportunities for cultural heritage. F Lozes, A Elmoataz, O Lezoray, IEEE Signal Process. Mag. 324F. Lozes, A. Elmoataz, and O. Lezoray, \"PDE-based graph signal processing for 3-D color point clouds: Opportunities for cultural heritage,\" IEEE Signal Process. Mag., vol. 32, no. 4, pp. 103-111, Jul. 2015.\n\nPCN: Point completion network. W Yuan, T Khot, D Held, C Mertz, M Hebert, Proc. Int. Conf. 3D Vis. (DV). Int. Conf. 3D Vis. (DV)W. Yuan, T. Khot, D. Held, C. Mertz, and M. Hebert, \"PCN: Point completion network,\" in Proc. Int. Conf. 3D Vis. (DV), Sep. 2018, pp. 728-737.\n\nPointCloud saliency maps. T Zheng, C Chen, J Yuan, B Li, K Ren, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV). IEEE/CVF Int. Conf. Comput. Vis. (ICCV)T. Zheng, C. Chen, J. Yuan, B. Li, and K. Ren, \"PointCloud saliency maps,\" in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2019, pp. 1598-1606.\n\nRL-GAN-Net: A reinforcement learning agent controlled GAN network for real-time point cloud shape completion. H Lee, Y Kim, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. IEEE/CVF Conf. Comput. Vis. Pattern RecognitH. Lee and Y. Kim, \"RL-GAN-Net: A reinforcement learning agent controlled GAN network for real-time point cloud shape completion,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2019, pp. 5891-5900.\n\nVaccine-style-net: Point cloud completion in implicit continuous function space. W Yan, Proc. ACM Int. Conf. Multimedia, 2020. ACM Int. Conf. Multimedia, 2020W. Yan et al., \"Vaccine-style-net: Point cloud completion in implicit continuous function space,\" in Proc. ACM Int. Conf. Multimedia, 2020, pp. 2067-2075.\n\nCultivation of vaccine virus. C P Li, T M Rivers, J. Experim. Med. 524C. P. Li and T. M. Rivers, \"Cultivation of vaccine virus,\" J. Experim. Med., vol. 52, no. 4, pp. 465-470, Oct. 1930.\n\nArtificial immunization: Its development and practical use. V L Ellicott, Amer. J. Nursing. 28V. L. Ellicott, \"Artificial immunization: Its development and practical use,\" Amer. J. Nursing, vol. 28, pp. 135-139, Feb. 1928.\n\nMarching cubes: A high resolution 3D surface construction algorithm. W E Lorensen, H E Cline, ACM SIGGRAPH Comput. Graph. 214W. E. Lorensen and H. E. Cline, \"Marching cubes: A high resolution 3D surface construction algorithm,\" ACM SIGGRAPH Comput. Graph., vol. 21, no. 4, pp. 163-169, Jul. 1987.\n\nPointNet++: Deep hierarchical feature learning on point sets in a metric space. C R Qi, Proc. Adv. Neural Inf. Process. Syst. Adv. Neural Inf. ess. SystC. R. Qi et al., \"PointNet++: Deep hierarchical feature learning on point sets in a metric space,\" in Proc. Adv. Neural Inf. Process. Syst., 2017, pp. 5099-5108.\n\n3D ShapeNets: A deep representation for volumetric shapes. Z Wu, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)Z. Wu et al., \"3D ShapeNets: A deep representation for volumetric shapes,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2015, pp. 192-1912.\n\nLearning representations and generative models for 3D point clouds. P Achlioptas, Proc. Int. Conf. Mach. Learn. P. Achlioptas et al., \"Learning representations and generative models for 3D point clouds,\" in Proc. Int. Conf. Mach. Learn., 2018, pp. 40-49.\n\nLearning 3D shape completion under weak supervision. D Stutz, A Geiger, Int. J. Comput. Vis. 1285D. Stutz and A. Geiger, \"Learning 3D shape completion under weak supervision,\" Int. J. Comput. Vis., vol. 128, no. 5, pp. 1162-1181, May 2020.\n\nUnpaired point cloud completion on real scans using adversarial training. X Chen, Proc. Int. Conf. Learn. Represent. Int. Conf. Learn. RepresentX. Chen et al., \"Unpaired point cloud completion on real scans using adversarial training,\" in Proc. Int. Conf. Learn. Represent., 2020, pp. 1-17.\n\nMorphing and sampling network for dense point cloud completion. M Liu, L Sheng, S Yang, J Shao, S.-M Hu, Proc. AAAI Conf. AAAI Conf34M. Liu, L. Sheng, S. Yang, J. Shao, and S.-M. Hu, \"Morphing and sampling network for dense point cloud completion,\" in Proc. AAAI Conf. Artif. Intell., vol. 34, 2020, pp. 11596-11603.\n\nFoldingNet: Point cloud autoencoder via deep grid deformation. Y Yang, C Feng, Y Shen, D Tian, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. IEEE/CVF Conf. Comput. Vis. Pattern RecognitY. Yang, C. Feng, Y. Shen, and D. Tian, \"FoldingNet: Point cloud auto- encoder via deep grid deformation,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2018, pp. 206-215.\n\nTopNet: Structural point cloud decoder. L P Tchapmi, V Kosaraju, H Rezatofighi, I Reid, S Savarese, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)L. P. Tchapmi, V. Kosaraju, H. Rezatofighi, I. Reid, and S. Savarese, \"TopNet: Structural point cloud decoder,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2019, pp. 383-392.\n\nGRNet: Gridding residual network for dense point cloud completion. H Xie, Proc. Eur. Conf. Comput. Vis. Eur. Conf. Comput. VisH. Xie et al., \"GRNet: Gridding residual network for dense point cloud completion,\" in Proc. Eur. Conf. Comput. Vis., 2020, pp. 365-381.\n\nCascaded refinement network for point cloud completion. X Wang, M H Ang, G H Lee, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)X. Wang, M. H. Ang, and G. H. Lee, \"Cascaded refinement network for point cloud completion,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2020, pp. 787-796.\n\nOccupancy networks: Learning 3D reconstruction in function space. L Mescheder, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. IEEE/CVF Conf. Comput. Vis. Pattern RecognitL. Mescheder et al., \"Occupancy networks: Learning 3D reconstruction in function space,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2019, pp. 4455-4465.\n\nHigh fidelity semantic shape completion for point clouds using latent optimization. S Gurumurthy, S , Proc. IEEE Winter Conf. Appl. Comput. Vis. (WACV). IEEE Winter Conf. Appl. Comput. Vis. (WACV)S. Gurumurthy and S. Agrawal, \"High fidelity semantic shape comple- tion for point clouds using latent optimization,\" in Proc. IEEE Winter Conf. Appl. Comput. Vis. (WACV), Jan. 2019, pp. 1099-1108.\n\nGenerative adversarial networks. I Goodfellow, Proc. Adv. Neural Inf. Process. Syst. Adv. Neural Inf. ess. SystI. Goodfellow et al., \"Generative adversarial networks,\" in Proc. Adv. Neural Inf. Process. Syst., 2014, pp. 2672-2680.\n\nDense point cloud completion based on generative adversarial network. M Cheng, G Li, Y Chen, J Chen, C Wang, J Li, IEEE Trans. Geosci. Remote Sens. 60M. Cheng, G. Li, Y. Chen, J. Chen, C. Wang, and J. Li, \"Dense point cloud completion based on generative adversarial network,\" IEEE Trans. Geosci. Remote Sens., vol. 60, pp. 1-10, 2022.\n\nEnergy-based generative adversarial network. J Zhao, M Mathieu, Y Lecun, Proc. Int. Conf. Learn. Represent. Int. Conf. Learn. RepresentJ. Zhao, M. Mathieu, and Y. LeCun, \"Energy-based generative adver- sarial network,\" in Proc. Int. Conf. Learn. Represent., 2017, pp. 1-17.\n\nA flexible reference-insensitive spatiotemporal fusion model for remote sensing images using conditional generative adversarial network. Z Tan, M Gao, X Li, L Jiang, IEEE Trans. Geosci. Remote Sens. 60Z. Tan, M. Gao, X. Li, and L. Jiang, \"A flexible reference-insensitive spatiotemporal fusion model for remote sensing images using condi- tional generative adversarial network,\" IEEE Trans. Geosci. Remote Sens., vol. 60, pp. 1-13, 2022.\n\nDeep reinforcement learning for band selection in hyperspectral image classification. L Mou, S Saha, Y Hua, F Bovolo, L Bruzzone, X X Zhu, IEEE Trans. Geosci. Remote Sens. 60L. Mou, S. Saha, Y. Hua, F. Bovolo, L. Bruzzone, and X. X. Zhu, \"Deep reinforcement learning for band selection in hyperspectral image classification,\" IEEE Trans. Geosci. Remote Sens., vol. 60, pp. 1-14, 2022.\n\nLearning from delayed rewards. B J A Kr\u00f6se, Robot. Auton. Syst. 154B. J. A. Kr\u00f6se, \"Learning from delayed rewards,\" Robot. Auton. Syst., vol. 15, no. 4, pp. 233-235, 1995.\n\nAddressing function approximation error in actor-critic methods. S Fujimoto, H Hoof, D Meger, Proc. Int. Conf. Mach. Learn. S. Fujimoto, H. Hoof, and D. Meger, \"Addressing function approxima- tion error in actor-critic methods,\" in Proc. Int. Conf. Mach. Learn., 2018, pp. 1587-1596.\n\nContinuous control with deep reinforcement learning. T Lillicrap, Proc. IEEE Int. Conf. Learn. Represent. IEEE Int. Conf. Learn. RepresentT. Lillicrap et al., \"Continuous control with deep reinforcement learn- ing,\" in Proc. IEEE Int. Conf. Learn. Represent., Sep. 2016, pp. 1-14.\n\nModel-free imitation learning with policy optimization. J Ho, J K Gupta, S Ermon, Proc. Int. Conf. Mach. Learn. Int. Conf. Mach. LearnJ. Ho, J. K. Gupta, and S. Ermon, \"Model-free imitation learning with policy optimization,\" in Proc. Int. Conf. Mach. Learn., 2016, pp. 2760-2769.\n\nDynamic graph CNN for learning on point clouds. Y Wang, Y Sun, Z Liu, S E Sarma, M M Bronstein, J M Solomon, ACM Trans. Graph. 385Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and J. M. Solomon, \"Dynamic graph CNN for learning on point clouds,\" ACM Trans. Graph., vol. 38, no. 5, pp. 1-12, 2019.\n\nLearning implicit fields for generative shape modeling. Z Chen, H Zhang, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)Z. Chen and H. Zhang, \"Learning implicit fields for generative shape modeling,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2019, pp. 5939-5948.\n\nModulating early visual processing by language. H De Vries, Proc. Adv. Neural Inf. Process. Syst. Adv. Neural Inf. ess. SystH. de Vries et al., \"Modulating early visual processing by language,\" in Proc. Adv. Neural Inf. Process. Syst., 2017, pp. 6594-6604.\n\nA learned representation for artistic style. V Dumoulin, J Shlens, M Kudlur, Proc. Int. Conf. Learn. Represent. Int. Conf. Learn. RepresentV. Dumoulin, J. Shlens, and M. Kudlur, \"A learned representation for artistic style,\" in Proc. Int. Conf. Learn. Represent., 2017, pp. 1-26.\n\nOccupancy flow: 4D reconstruction by learning particle dynamics. M Niemeyer, L Mescheder, M Oechsle, A Geiger, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV). IEEE/CVF Int. Conf. Comput. Vis. (ICCV)Seoul, South KoreaM. Niemeyer, L. Mescheder, M. Oechsle, and A. Geiger, \"Occu- pancy flow: 4D reconstruction by learning particle dynamics,\" in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Seoul, South Korea, Oct. 2019, pp. 5378-5388.\n\nGenerative adversarial nets. I Goodfellow, Proc. Adv. Neural Inf. Process. Syst. Adv. Neural Inf. ess. SystI. Goodfellow et al., \"Generative adversarial nets,\" in Proc. Adv. Neural Inf. Process. Syst., 2014, pp. 2672-2680.\n\nImproved training of Wasserstein GANs. I Gulrajani, Proc. Adv. Neural Inf. Process. Syst. Adv. Neural Inf. ess. SystI. Gulrajani et al., \"Improved training of Wasserstein GANs,\" in Proc. Adv. Neural Inf. Process. Syst., 2017, pp. 5767-5777.\n\nSelf-attention generative adversarial networks. H Zhang, Proc. Int. Conf. Mach. Learn. Int. Conf. Mach. LearnH. Zhang et al., \"Self-attention generative adversarial networks,\" in Proc. Int. Conf. Mach. Learn., 2019, pp. 7354-7363.\n\nWhat do single-view 3D reconstruction networks learn. M Tatarchenko, S R Richter, R Ranftl, Z Li, V Koltun, T Brox, Proc. nullM. Tatarchenko, S. R. Richter, R. Ranftl, Z. Li, V. Koltun, and T. Brox, \"What do single-view 3D reconstruction networks learn?\" in Proc.\n\nIeee/Cvf, Conf, Comput. Vis. Pattern Recognit. (CVPR). IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2019, pp. 3405-3414.\n\nAutomatic differentiation in PyTorch. A Paszke, Proc. NIPS Autodiff Workshop. NIPS Autodiff WorkshopLong Beach, CA, USAA. Paszke et al., \"Automatic differentiation in PyTorch,\" in Proc. NIPS Autodiff Workshop, Future Gradient-Based Mach. Learn. Softw. Techn., Long Beach, CA, USA, Dec. 2017.\n\nVisualizing data using t-SNE. L Van Der Maaten, G Hinton, J. Mach. Learn. Res. 9L. van der Maaten and G. Hinton, \"Visualizing data using t-SNE,\" J. Mach. Learn. Res., vol. 9, pp. 2579-2605, Nov. 2008.\n\nPoisson surface reconstruction. M Kazhdan, Proc. Symp. Geometry Process. Symp. Geometry ess256M. Kazhdan, \"Poisson surface reconstruction,\" in Proc. Symp. Geometry Process., vol. 256, 2006, pp. 61-70.\n", "annotations": {"author": "[{\"end\":168,\"start\":126},{\"end\":196,\"start\":169},{\"end\":215,\"start\":197},{\"end\":228,\"start\":216}]", "publisher": null, "author_last_name": "[{\"end\":167,\"start\":162},{\"end\":214,\"start\":212},{\"end\":227,\"start\":225}]", "author_first_name": "[{\"end\":161,\"start\":155},{\"end\":191,\"start\":188},{\"end\":195,\"start\":192},{\"end\":211,\"start\":209},{\"end\":222,\"start\":216},{\"end\":224,\"start\":223}]", "author_affiliation": null, "title": "[{\"end\":123,\"start\":1},{\"end\":351,\"start\":229}]", "venue": "[{\"end\":403,\"start\":353}]", "abstract": "[{\"end\":2123,\"start\":560}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2278,\"start\":2275},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2884,\"start\":2881},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2917,\"start\":2914},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3444,\"start\":3441},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3468,\"start\":3465},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3473,\"start\":3470},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3478,\"start\":3475},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3484,\"start\":3480},{\"end\":4152,\"start\":4146},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4212,\"start\":4208},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4221,\"start\":4217},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4460,\"start\":4456},{\"end\":4513,\"start\":4501},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5044,\"start\":5040},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5050,\"start\":5046},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6358,\"start\":6355},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9162,\"start\":9159},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9168,\"start\":9164},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9206,\"start\":9202},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10037,\"start\":10033},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10155,\"start\":10151},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10220,\"start\":10217},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10378,\"start\":10374},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10499,\"start\":10495},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10567,\"start\":10563},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10661,\"start\":10657},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10723,\"start\":10719},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10842,\"start\":10838},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10935,\"start\":10931},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11050,\"start\":11046},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12247,\"start\":12243},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":12463,\"start\":12460},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12874,\"start\":12870},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":13452,\"start\":13448},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":13458,\"start\":13454},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":13992,\"start\":13988},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":14073,\"start\":14069},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":14159,\"start\":14155},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":14169,\"start\":14165},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":14350,\"start\":14346},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":14356,\"start\":14352},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":14362,\"start\":14358},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":14566,\"start\":14562},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":14586,\"start\":14582},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":14960,\"start\":14956},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":15055,\"start\":15051},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":17772,\"start\":17769},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":18810,\"start\":18807},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":18816,\"start\":18812},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":19167,\"start\":19163},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":19223,\"start\":19220},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":19243,\"start\":19239},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":19358,\"start\":19355},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":19363,\"start\":19360},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":19369,\"start\":19365},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":19484,\"start\":19480},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":19753,\"start\":19749},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":19759,\"start\":19755},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":21362,\"start\":21358},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":21368,\"start\":21364},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":21897,\"start\":21893},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":22890,\"start\":22886},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":22985,\"start\":22981},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":24281,\"start\":24277},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":24394,\"start\":24390},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":24408,\"start\":24405},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":24876,\"start\":24872},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":25251,\"start\":25247},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":26169,\"start\":26166},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":27054,\"start\":27050},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":28072,\"start\":28068},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":28094,\"start\":28091},{\"end\":28502,\"start\":28500},{\"end\":28529,\"start\":28527},{\"end\":28712,\"start\":28709},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":28747,\"start\":28745},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":30210,\"start\":30207},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":30215,\"start\":30212},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":30221,\"start\":30217},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":30789,\"start\":30785},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":30795,\"start\":30791},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":31587,\"start\":31583},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":31605,\"start\":31601},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":34078,\"start\":34074},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":34919,\"start\":34915},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":34948,\"start\":34945},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":35043,\"start\":35039},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":35631,\"start\":35627},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":35642,\"start\":35639},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":35657,\"start\":35653},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":35668,\"start\":35665},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":35772,\"start\":35768},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":35882,\"start\":35878},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":36643,\"start\":36639},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":36654,\"start\":36651},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":36669,\"start\":36665},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":38709,\"start\":38706},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":40735,\"start\":40731},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":40752,\"start\":40748},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":40879,\"start\":40875},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":40890,\"start\":40887},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":40905,\"start\":40901},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":41226,\"start\":41223},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":41240,\"start\":41236},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":41462,\"start\":41458},{\"end\":41666,\"start\":41659},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":41732,\"start\":41728},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":41743,\"start\":41740},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":41758,\"start\":41754},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":46262,\"start\":46258},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":47289,\"start\":47285},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":52743,\"start\":52739},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":53119,\"start\":53115},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":53129,\"start\":53126},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":53143,\"start\":53139}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":50053,\"start\":49220},{\"attributes\":{\"id\":\"fig_1\"},\"end\":50115,\"start\":50054},{\"attributes\":{\"id\":\"fig_2\"},\"end\":50660,\"start\":50116},{\"attributes\":{\"id\":\"fig_3\"},\"end\":51249,\"start\":50661},{\"attributes\":{\"id\":\"fig_4\"},\"end\":51534,\"start\":51250},{\"attributes\":{\"id\":\"fig_5\"},\"end\":51695,\"start\":51535},{\"attributes\":{\"id\":\"fig_6\"},\"end\":51891,\"start\":51696},{\"attributes\":{\"id\":\"fig_7\"},\"end\":52072,\"start\":51892},{\"attributes\":{\"id\":\"fig_8\"},\"end\":52223,\"start\":52073},{\"attributes\":{\"id\":\"fig_9\"},\"end\":52371,\"start\":52224},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":52463,\"start\":52372},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":52676,\"start\":52464},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":52905,\"start\":52677},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":53144,\"start\":52906},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":53487,\"start\":53145},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":53610,\"start\":53488}]", "paragraph": "[{\"end\":2728,\"start\":2132},{\"end\":3634,\"start\":2730},{\"end\":4852,\"start\":3636},{\"end\":5325,\"start\":4964},{\"end\":5948,\"start\":5327},{\"end\":6076,\"start\":5950},{\"end\":7668,\"start\":6078},{\"end\":8928,\"start\":7670},{\"end\":11247,\"start\":8978},{\"end\":11493,\"start\":11249},{\"end\":12218,\"start\":11495},{\"end\":13415,\"start\":12220},{\"end\":14160,\"start\":13440},{\"end\":15280,\"start\":14162},{\"end\":15637,\"start\":15308},{\"end\":15728,\"start\":15663},{\"end\":16157,\"start\":15730},{\"end\":17104,\"start\":16199},{\"end\":18218,\"start\":17171},{\"end\":18761,\"start\":18220},{\"end\":19244,\"start\":18763},{\"end\":20020,\"start\":19286},{\"end\":20110,\"start\":20022},{\"end\":20993,\"start\":20166},{\"end\":21619,\"start\":20995},{\"end\":22055,\"start\":21664},{\"end\":22286,\"start\":22096},{\"end\":22353,\"start\":22288},{\"end\":22715,\"start\":22355},{\"end\":23282,\"start\":22764},{\"end\":24130,\"start\":23335},{\"end\":24568,\"start\":24132},{\"end\":24942,\"start\":24609},{\"end\":25583,\"start\":25008},{\"end\":26384,\"start\":25585},{\"end\":27055,\"start\":26386},{\"end\":27337,\"start\":27115},{\"end\":27677,\"start\":27377},{\"end\":28133,\"start\":27719},{\"end\":28748,\"start\":28135},{\"end\":29439,\"start\":28788},{\"end\":29751,\"start\":29474},{\"end\":30180,\"start\":29753},{\"end\":30273,\"start\":30182},{\"end\":31364,\"start\":30357},{\"end\":31982,\"start\":31366},{\"end\":32673,\"start\":32115},{\"end\":32915,\"start\":32675},{\"end\":33386,\"start\":33007},{\"end\":33891,\"start\":33493},{\"end\":34034,\"start\":33924},{\"end\":34430,\"start\":34036},{\"end\":35007,\"start\":34432},{\"end\":35383,\"start\":35009},{\"end\":37022,\"start\":35420},{\"end\":38585,\"start\":37024},{\"end\":39597,\"start\":38587},{\"end\":39860,\"start\":39621},{\"end\":40369,\"start\":39862},{\"end\":40558,\"start\":40371},{\"end\":40906,\"start\":40560},{\"end\":41241,\"start\":40908},{\"end\":41538,\"start\":41243},{\"end\":42860,\"start\":41540},{\"end\":43371,\"start\":42862},{\"end\":44693,\"start\":43396},{\"end\":45960,\"start\":44695},{\"end\":46656,\"start\":46006},{\"end\":47062,\"start\":46658},{\"end\":47829,\"start\":47100},{\"end\":49219,\"start\":47862}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":16198,\"start\":16158},{\"attributes\":{\"id\":\"formula_1\"},\"end\":20165,\"start\":20111},{\"attributes\":{\"id\":\"formula_2\"},\"end\":21663,\"start\":21620},{\"attributes\":{\"id\":\"formula_3\"},\"end\":22095,\"start\":22056},{\"attributes\":{\"id\":\"formula_4\"},\"end\":22763,\"start\":22716},{\"attributes\":{\"id\":\"formula_5\"},\"end\":24608,\"start\":24569},{\"attributes\":{\"id\":\"formula_6\"},\"end\":25007,\"start\":24943},{\"attributes\":{\"id\":\"formula_7\"},\"end\":27114,\"start\":27056},{\"attributes\":{\"id\":\"formula_8\"},\"end\":27376,\"start\":27338},{\"attributes\":{\"id\":\"formula_9\"},\"end\":27718,\"start\":27678},{\"attributes\":{\"id\":\"formula_10\"},\"end\":30356,\"start\":30274},{\"attributes\":{\"id\":\"formula_11\"},\"end\":32114,\"start\":31983},{\"attributes\":{\"id\":\"formula_12\"},\"end\":33006,\"start\":32916},{\"attributes\":{\"id\":\"formula_13\"},\"end\":33492,\"start\":33387}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":15279,\"start\":15272},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":18962,\"start\":18955},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":21852,\"start\":21834},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":25374,\"start\":25366},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":36151,\"start\":36144},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":37056,\"start\":37049},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":39989,\"start\":39981},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":40445,\"start\":40436},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":44217,\"start\":44207},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":44736,\"start\":44728},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":47516,\"start\":47509}]", "section_header": "[{\"end\":4962,\"start\":4855},{\"end\":8948,\"start\":8931},{\"end\":8976,\"start\":8951},{\"end\":13438,\"start\":13418},{\"end\":15306,\"start\":15283},{\"end\":15661,\"start\":15640},{\"end\":17133,\"start\":17107},{\"end\":17169,\"start\":17136},{\"end\":19284,\"start\":19247},{\"end\":23333,\"start\":23285},{\"end\":28786,\"start\":28751},{\"end\":29472,\"start\":29442},{\"end\":33922,\"start\":33894},{\"end\":35418,\"start\":35386},{\"end\":39619,\"start\":39600},{\"end\":43394,\"start\":43374},{\"end\":46004,\"start\":45963},{\"end\":47098,\"start\":47065},{\"end\":47860,\"start\":47832},{\"end\":49229,\"start\":49221},{\"end\":50063,\"start\":50055},{\"end\":50670,\"start\":50662},{\"end\":51544,\"start\":51536},{\"end\":51705,\"start\":51697},{\"end\":51902,\"start\":51893},{\"end\":52083,\"start\":52074},{\"end\":52234,\"start\":52225},{\"end\":52380,\"start\":52373},{\"end\":52473,\"start\":52465},{\"end\":52686,\"start\":52678},{\"end\":52914,\"start\":52907},{\"end\":53154,\"start\":53146},{\"end\":53507,\"start\":53489}]", "table": "[{\"end\":52676,\"start\":52534},{\"end\":53487,\"start\":53246}]", "figure_caption": "[{\"end\":50053,\"start\":49231},{\"end\":50115,\"start\":50065},{\"end\":50660,\"start\":50118},{\"end\":51249,\"start\":50672},{\"end\":51534,\"start\":51252},{\"end\":51695,\"start\":51546},{\"end\":51891,\"start\":51707},{\"end\":52072,\"start\":51905},{\"end\":52223,\"start\":52086},{\"end\":52371,\"start\":52237},{\"end\":52463,\"start\":52382},{\"end\":52534,\"start\":52476},{\"end\":52905,\"start\":52689},{\"end\":53144,\"start\":52916},{\"end\":53246,\"start\":53157},{\"end\":53610,\"start\":53512}]", "figure_ref": "[{\"end\":2131,\"start\":2125},{\"end\":5191,\"start\":5185},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5536,\"start\":5530},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":6523,\"start\":6517},{\"end\":7072,\"start\":7066},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":15326,\"start\":15320},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":17420,\"start\":17414},{\"end\":30062,\"start\":30045},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":30930,\"start\":30924},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":33649,\"start\":33643},{\"end\":36548,\"start\":36542},{\"end\":38542,\"start\":38536},{\"end\":38617,\"start\":38611},{\"end\":38721,\"start\":38714},{\"end\":38926,\"start\":38920},{\"end\":40761,\"start\":40754},{\"end\":41158,\"start\":41151},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":41552,\"start\":41543},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":41834,\"start\":41825},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":42236,\"start\":42227},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":42367,\"start\":42350},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":44400,\"start\":44394},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":45623,\"start\":45616},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":46843,\"start\":46836},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":47394,\"start\":47387},{\"end\":47701,\"start\":47694}]", "bib_author_first_name": "[{\"end\":53961,\"start\":53960},{\"end\":53963,\"start\":53962},{\"end\":53974,\"start\":53973},{\"end\":53980,\"start\":53979},{\"end\":53991,\"start\":53990},{\"end\":53993,\"start\":53992},{\"end\":54410,\"start\":54409},{\"end\":54418,\"start\":54417},{\"end\":54427,\"start\":54426},{\"end\":54433,\"start\":54432},{\"end\":54442,\"start\":54441},{\"end\":54819,\"start\":54818},{\"end\":54827,\"start\":54826},{\"end\":54836,\"start\":54835},{\"end\":54845,\"start\":54844},{\"end\":54854,\"start\":54853},{\"end\":54860,\"start\":54859},{\"end\":55239,\"start\":55238},{\"end\":55510,\"start\":55509},{\"end\":55520,\"start\":55519},{\"end\":55522,\"start\":55521},{\"end\":55531,\"start\":55530},{\"end\":55852,\"start\":55851},{\"end\":55861,\"start\":55860},{\"end\":55873,\"start\":55872},{\"end\":56152,\"start\":56151},{\"end\":56160,\"start\":56159},{\"end\":56168,\"start\":56167},{\"end\":56176,\"start\":56175},{\"end\":56185,\"start\":56184},{\"end\":56419,\"start\":56418},{\"end\":56428,\"start\":56427},{\"end\":56436,\"start\":56435},{\"end\":56444,\"start\":56443},{\"end\":56450,\"start\":56449},{\"end\":56805,\"start\":56804},{\"end\":56812,\"start\":56811},{\"end\":57210,\"start\":57209},{\"end\":57473,\"start\":57472},{\"end\":57475,\"start\":57474},{\"end\":57481,\"start\":57480},{\"end\":57483,\"start\":57482},{\"end\":57691,\"start\":57690},{\"end\":57693,\"start\":57692},{\"end\":57924,\"start\":57923},{\"end\":57926,\"start\":57925},{\"end\":57938,\"start\":57937},{\"end\":57940,\"start\":57939},{\"end\":58233,\"start\":58232},{\"end\":58235,\"start\":58234},{\"end\":58527,\"start\":58526},{\"end\":58865,\"start\":58864},{\"end\":59106,\"start\":59105},{\"end\":59115,\"start\":59114},{\"end\":59368,\"start\":59367},{\"end\":59650,\"start\":59649},{\"end\":59657,\"start\":59656},{\"end\":59666,\"start\":59665},{\"end\":59674,\"start\":59673},{\"end\":59685,\"start\":59681},{\"end\":59967,\"start\":59966},{\"end\":59975,\"start\":59974},{\"end\":59983,\"start\":59982},{\"end\":59991,\"start\":59990},{\"end\":60323,\"start\":60322},{\"end\":60325,\"start\":60324},{\"end\":60336,\"start\":60335},{\"end\":60348,\"start\":60347},{\"end\":60363,\"start\":60362},{\"end\":60371,\"start\":60370},{\"end\":60762,\"start\":60761},{\"end\":61015,\"start\":61014},{\"end\":61023,\"start\":61022},{\"end\":61025,\"start\":61024},{\"end\":61032,\"start\":61031},{\"end\":61034,\"start\":61033},{\"end\":61400,\"start\":61399},{\"end\":61765,\"start\":61764},{\"end\":61779,\"start\":61778},{\"end\":62109,\"start\":62108},{\"end\":62378,\"start\":62377},{\"end\":62387,\"start\":62386},{\"end\":62393,\"start\":62392},{\"end\":62401,\"start\":62400},{\"end\":62409,\"start\":62408},{\"end\":62417,\"start\":62416},{\"end\":62690,\"start\":62689},{\"end\":62698,\"start\":62697},{\"end\":62709,\"start\":62708},{\"end\":63057,\"start\":63056},{\"end\":63064,\"start\":63063},{\"end\":63071,\"start\":63070},{\"end\":63077,\"start\":63076},{\"end\":63445,\"start\":63444},{\"end\":63452,\"start\":63451},{\"end\":63460,\"start\":63459},{\"end\":63467,\"start\":63466},{\"end\":63477,\"start\":63476},{\"end\":63489,\"start\":63488},{\"end\":63491,\"start\":63490},{\"end\":63776,\"start\":63775},{\"end\":63780,\"start\":63777},{\"end\":63983,\"start\":63982},{\"end\":63995,\"start\":63994},{\"end\":64003,\"start\":64002},{\"end\":64256,\"start\":64255},{\"end\":64541,\"start\":64540},{\"end\":64547,\"start\":64546},{\"end\":64549,\"start\":64548},{\"end\":64558,\"start\":64557},{\"end\":64815,\"start\":64814},{\"end\":64823,\"start\":64822},{\"end\":64830,\"start\":64829},{\"end\":64837,\"start\":64836},{\"end\":64839,\"start\":64838},{\"end\":64848,\"start\":64847},{\"end\":64850,\"start\":64849},{\"end\":64863,\"start\":64862},{\"end\":64865,\"start\":64864},{\"end\":65129,\"start\":65128},{\"end\":65137,\"start\":65136},{\"end\":65476,\"start\":65475},{\"end\":65731,\"start\":65730},{\"end\":65743,\"start\":65742},{\"end\":65753,\"start\":65752},{\"end\":66032,\"start\":66031},{\"end\":66044,\"start\":66043},{\"end\":66057,\"start\":66056},{\"end\":66068,\"start\":66067},{\"end\":66431,\"start\":66430},{\"end\":66665,\"start\":66664},{\"end\":66916,\"start\":66915},{\"end\":67154,\"start\":67153},{\"end\":67169,\"start\":67168},{\"end\":67171,\"start\":67170},{\"end\":67182,\"start\":67181},{\"end\":67192,\"start\":67191},{\"end\":67198,\"start\":67197},{\"end\":67208,\"start\":67207},{\"end\":67539,\"start\":67538},{\"end\":67824,\"start\":67823},{\"end\":67842,\"start\":67841},{\"end\":68028,\"start\":68027}]", "bib_author_last_name": "[{\"end\":53971,\"start\":53964},{\"end\":53977,\"start\":53975},{\"end\":53988,\"start\":53981},{\"end\":54000,\"start\":53994},{\"end\":54415,\"start\":54411},{\"end\":54424,\"start\":54419},{\"end\":54430,\"start\":54428},{\"end\":54439,\"start\":54434},{\"end\":54446,\"start\":54443},{\"end\":54824,\"start\":54820},{\"end\":54833,\"start\":54828},{\"end\":54842,\"start\":54837},{\"end\":54851,\"start\":54846},{\"end\":54857,\"start\":54855},{\"end\":54864,\"start\":54861},{\"end\":55243,\"start\":55240},{\"end\":55517,\"start\":55511},{\"end\":55528,\"start\":55523},{\"end\":55538,\"start\":55532},{\"end\":55858,\"start\":55853},{\"end\":55870,\"start\":55862},{\"end\":55881,\"start\":55874},{\"end\":56157,\"start\":56153},{\"end\":56165,\"start\":56161},{\"end\":56173,\"start\":56169},{\"end\":56182,\"start\":56177},{\"end\":56192,\"start\":56186},{\"end\":56425,\"start\":56420},{\"end\":56433,\"start\":56429},{\"end\":56441,\"start\":56437},{\"end\":56447,\"start\":56445},{\"end\":56454,\"start\":56451},{\"end\":56809,\"start\":56806},{\"end\":56816,\"start\":56813},{\"end\":57214,\"start\":57211},{\"end\":57478,\"start\":57476},{\"end\":57490,\"start\":57484},{\"end\":57702,\"start\":57694},{\"end\":57935,\"start\":57927},{\"end\":57946,\"start\":57941},{\"end\":58238,\"start\":58236},{\"end\":58530,\"start\":58528},{\"end\":58876,\"start\":58866},{\"end\":59112,\"start\":59107},{\"end\":59122,\"start\":59116},{\"end\":59373,\"start\":59369},{\"end\":59654,\"start\":59651},{\"end\":59663,\"start\":59658},{\"end\":59671,\"start\":59667},{\"end\":59679,\"start\":59675},{\"end\":59688,\"start\":59686},{\"end\":59972,\"start\":59968},{\"end\":59980,\"start\":59976},{\"end\":59988,\"start\":59984},{\"end\":59996,\"start\":59992},{\"end\":60333,\"start\":60326},{\"end\":60345,\"start\":60337},{\"end\":60360,\"start\":60349},{\"end\":60368,\"start\":60364},{\"end\":60380,\"start\":60372},{\"end\":60766,\"start\":60763},{\"end\":61020,\"start\":61016},{\"end\":61029,\"start\":61026},{\"end\":61038,\"start\":61035},{\"end\":61410,\"start\":61401},{\"end\":61776,\"start\":61766},{\"end\":62120,\"start\":62110},{\"end\":62384,\"start\":62379},{\"end\":62390,\"start\":62388},{\"end\":62398,\"start\":62394},{\"end\":62406,\"start\":62402},{\"end\":62414,\"start\":62410},{\"end\":62420,\"start\":62418},{\"end\":62695,\"start\":62691},{\"end\":62706,\"start\":62699},{\"end\":62715,\"start\":62710},{\"end\":63061,\"start\":63058},{\"end\":63068,\"start\":63065},{\"end\":63074,\"start\":63072},{\"end\":63083,\"start\":63078},{\"end\":63449,\"start\":63446},{\"end\":63457,\"start\":63453},{\"end\":63464,\"start\":63461},{\"end\":63474,\"start\":63468},{\"end\":63486,\"start\":63478},{\"end\":63495,\"start\":63492},{\"end\":63786,\"start\":63781},{\"end\":63992,\"start\":63984},{\"end\":64000,\"start\":63996},{\"end\":64009,\"start\":64004},{\"end\":64266,\"start\":64257},{\"end\":64544,\"start\":64542},{\"end\":64555,\"start\":64550},{\"end\":64564,\"start\":64559},{\"end\":64820,\"start\":64816},{\"end\":64827,\"start\":64824},{\"end\":64834,\"start\":64831},{\"end\":64845,\"start\":64840},{\"end\":64860,\"start\":64851},{\"end\":64873,\"start\":64866},{\"end\":65134,\"start\":65130},{\"end\":65143,\"start\":65138},{\"end\":65485,\"start\":65477},{\"end\":65740,\"start\":65732},{\"end\":65750,\"start\":65744},{\"end\":65760,\"start\":65754},{\"end\":66041,\"start\":66033},{\"end\":66054,\"start\":66045},{\"end\":66065,\"start\":66058},{\"end\":66075,\"start\":66069},{\"end\":66442,\"start\":66432},{\"end\":66675,\"start\":66666},{\"end\":66922,\"start\":66917},{\"end\":67166,\"start\":67155},{\"end\":67179,\"start\":67172},{\"end\":67189,\"start\":67183},{\"end\":67195,\"start\":67193},{\"end\":67205,\"start\":67199},{\"end\":67213,\"start\":67209},{\"end\":67372,\"start\":67364},{\"end\":67378,\"start\":67374},{\"end\":67546,\"start\":67540},{\"end\":67839,\"start\":67825},{\"end\":67849,\"start\":67843},{\"end\":68036,\"start\":68029}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":5115938},\"end\":54320,\"start\":53882},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":50780127},\"end\":54695,\"start\":54322},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":226607609},\"end\":55156,\"start\":54697},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":246186104},\"end\":55435,\"start\":55158},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":21714682},\"end\":55750,\"start\":55437},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":6933271},\"end\":56118,\"start\":55752},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":51908879},\"end\":56390,\"start\":56120},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":90248873},\"end\":56692,\"start\":56392},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":139103260},\"end\":57126,\"start\":56694},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":222278498},\"end\":57440,\"start\":57128},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":1367189},\"end\":57628,\"start\":57442},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":75151432},\"end\":57852,\"start\":57630},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":15545924},\"end\":58150,\"start\":57854},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":1745976},\"end\":58465,\"start\":58152},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":206592833},\"end\":58794,\"start\":58467},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":23102425},\"end\":59050,\"start\":58796},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":29159763},\"end\":59291,\"start\":59052},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":90258365},\"end\":59583,\"start\":59293},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":208527100},\"end\":59901,\"start\":59585},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":8338993},\"end\":60280,\"start\":59903},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":198184513},\"end\":60692,\"start\":60282},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":219531703},\"end\":60956,\"start\":60694},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":215238628},\"end\":61331,\"start\":60958},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":54465161},\"end\":61678,\"start\":61333},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":49666582},\"end\":62073,\"start\":61680},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":12209503},\"end\":62305,\"start\":62075},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":239760476},\"end\":62642,\"start\":62307},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":15876696},\"end\":62917,\"start\":62644},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":234074985},\"end\":63356,\"start\":62919},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":232240156},\"end\":63742,\"start\":63358},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":40597581},\"end\":63915,\"start\":63744},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":3544558},\"end\":64200,\"start\":63917},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":16326763},\"end\":64482,\"start\":64202},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":6448021},\"end\":64764,\"start\":64484},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":94822},\"end\":65070,\"start\":64766},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":54457478},\"end\":65425,\"start\":65072},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":7910568},\"end\":65683,\"start\":65427},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":5687613},\"end\":65964,\"start\":65685},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":203643676},\"end\":66399,\"start\":65966},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":1033682},\"end\":66623,\"start\":66401},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":10894094},\"end\":66865,\"start\":66625},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":46898260},\"end\":67097,\"start\":66867},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":115147778},\"end\":67362,\"start\":67099},{\"attributes\":{\"id\":\"b43\"},\"end\":67498,\"start\":67364},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":40027675},\"end\":67791,\"start\":67500},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":5855042},\"end\":67993,\"start\":67793},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":14224},\"end\":68195,\"start\":67995}]", "bib_title": "[{\"end\":53958,\"start\":53882},{\"end\":54407,\"start\":54322},{\"end\":54816,\"start\":54697},{\"end\":55236,\"start\":55158},{\"end\":55507,\"start\":55437},{\"end\":55849,\"start\":55752},{\"end\":56149,\"start\":56120},{\"end\":56416,\"start\":56392},{\"end\":56802,\"start\":56694},{\"end\":57207,\"start\":57128},{\"end\":57470,\"start\":57442},{\"end\":57688,\"start\":57630},{\"end\":57921,\"start\":57854},{\"end\":58230,\"start\":58152},{\"end\":58524,\"start\":58467},{\"end\":58862,\"start\":58796},{\"end\":59103,\"start\":59052},{\"end\":59365,\"start\":59293},{\"end\":59647,\"start\":59585},{\"end\":59964,\"start\":59903},{\"end\":60320,\"start\":60282},{\"end\":60759,\"start\":60694},{\"end\":61012,\"start\":60958},{\"end\":61397,\"start\":61333},{\"end\":61762,\"start\":61680},{\"end\":62106,\"start\":62075},{\"end\":62375,\"start\":62307},{\"end\":62687,\"start\":62644},{\"end\":63054,\"start\":62919},{\"end\":63442,\"start\":63358},{\"end\":63773,\"start\":63744},{\"end\":63980,\"start\":63917},{\"end\":64253,\"start\":64202},{\"end\":64538,\"start\":64484},{\"end\":64812,\"start\":64766},{\"end\":65126,\"start\":65072},{\"end\":65473,\"start\":65427},{\"end\":65728,\"start\":65685},{\"end\":66029,\"start\":65966},{\"end\":66428,\"start\":66401},{\"end\":66662,\"start\":66625},{\"end\":66913,\"start\":66867},{\"end\":67151,\"start\":67099},{\"end\":67536,\"start\":67500},{\"end\":67821,\"start\":67793},{\"end\":68025,\"start\":67995}]", "bib_author": "[{\"end\":53973,\"start\":53960},{\"end\":53979,\"start\":53973},{\"end\":53990,\"start\":53979},{\"end\":54002,\"start\":53990},{\"end\":54417,\"start\":54409},{\"end\":54426,\"start\":54417},{\"end\":54432,\"start\":54426},{\"end\":54441,\"start\":54432},{\"end\":54448,\"start\":54441},{\"end\":54826,\"start\":54818},{\"end\":54835,\"start\":54826},{\"end\":54844,\"start\":54835},{\"end\":54853,\"start\":54844},{\"end\":54859,\"start\":54853},{\"end\":54866,\"start\":54859},{\"end\":55245,\"start\":55238},{\"end\":55519,\"start\":55509},{\"end\":55530,\"start\":55519},{\"end\":55540,\"start\":55530},{\"end\":55860,\"start\":55851},{\"end\":55872,\"start\":55860},{\"end\":55883,\"start\":55872},{\"end\":56159,\"start\":56151},{\"end\":56167,\"start\":56159},{\"end\":56175,\"start\":56167},{\"end\":56184,\"start\":56175},{\"end\":56194,\"start\":56184},{\"end\":56427,\"start\":56418},{\"end\":56435,\"start\":56427},{\"end\":56443,\"start\":56435},{\"end\":56449,\"start\":56443},{\"end\":56456,\"start\":56449},{\"end\":56811,\"start\":56804},{\"end\":56818,\"start\":56811},{\"end\":57216,\"start\":57209},{\"end\":57480,\"start\":57472},{\"end\":57492,\"start\":57480},{\"end\":57704,\"start\":57690},{\"end\":57937,\"start\":57923},{\"end\":57948,\"start\":57937},{\"end\":58240,\"start\":58232},{\"end\":58532,\"start\":58526},{\"end\":58878,\"start\":58864},{\"end\":59114,\"start\":59105},{\"end\":59124,\"start\":59114},{\"end\":59375,\"start\":59367},{\"end\":59656,\"start\":59649},{\"end\":59665,\"start\":59656},{\"end\":59673,\"start\":59665},{\"end\":59681,\"start\":59673},{\"end\":59690,\"start\":59681},{\"end\":59974,\"start\":59966},{\"end\":59982,\"start\":59974},{\"end\":59990,\"start\":59982},{\"end\":59998,\"start\":59990},{\"end\":60335,\"start\":60322},{\"end\":60347,\"start\":60335},{\"end\":60362,\"start\":60347},{\"end\":60370,\"start\":60362},{\"end\":60382,\"start\":60370},{\"end\":60768,\"start\":60761},{\"end\":61022,\"start\":61014},{\"end\":61031,\"start\":61022},{\"end\":61040,\"start\":61031},{\"end\":61412,\"start\":61399},{\"end\":61778,\"start\":61764},{\"end\":61782,\"start\":61778},{\"end\":62122,\"start\":62108},{\"end\":62386,\"start\":62377},{\"end\":62392,\"start\":62386},{\"end\":62400,\"start\":62392},{\"end\":62408,\"start\":62400},{\"end\":62416,\"start\":62408},{\"end\":62422,\"start\":62416},{\"end\":62697,\"start\":62689},{\"end\":62708,\"start\":62697},{\"end\":62717,\"start\":62708},{\"end\":63063,\"start\":63056},{\"end\":63070,\"start\":63063},{\"end\":63076,\"start\":63070},{\"end\":63085,\"start\":63076},{\"end\":63451,\"start\":63444},{\"end\":63459,\"start\":63451},{\"end\":63466,\"start\":63459},{\"end\":63476,\"start\":63466},{\"end\":63488,\"start\":63476},{\"end\":63497,\"start\":63488},{\"end\":63788,\"start\":63775},{\"end\":63994,\"start\":63982},{\"end\":64002,\"start\":63994},{\"end\":64011,\"start\":64002},{\"end\":64268,\"start\":64255},{\"end\":64546,\"start\":64540},{\"end\":64557,\"start\":64546},{\"end\":64566,\"start\":64557},{\"end\":64822,\"start\":64814},{\"end\":64829,\"start\":64822},{\"end\":64836,\"start\":64829},{\"end\":64847,\"start\":64836},{\"end\":64862,\"start\":64847},{\"end\":64875,\"start\":64862},{\"end\":65136,\"start\":65128},{\"end\":65145,\"start\":65136},{\"end\":65487,\"start\":65475},{\"end\":65742,\"start\":65730},{\"end\":65752,\"start\":65742},{\"end\":65762,\"start\":65752},{\"end\":66043,\"start\":66031},{\"end\":66056,\"start\":66043},{\"end\":66067,\"start\":66056},{\"end\":66077,\"start\":66067},{\"end\":66444,\"start\":66430},{\"end\":66677,\"start\":66664},{\"end\":66924,\"start\":66915},{\"end\":67168,\"start\":67153},{\"end\":67181,\"start\":67168},{\"end\":67191,\"start\":67181},{\"end\":67197,\"start\":67191},{\"end\":67207,\"start\":67197},{\"end\":67215,\"start\":67207},{\"end\":67374,\"start\":67364},{\"end\":67380,\"start\":67374},{\"end\":67548,\"start\":67538},{\"end\":67841,\"start\":67823},{\"end\":67851,\"start\":67841},{\"end\":68038,\"start\":68027}]", "bib_venue": "[{\"end\":54106,\"start\":54058},{\"end\":56248,\"start\":56225},{\"end\":56542,\"start\":56503},{\"end\":56914,\"start\":56870},{\"end\":57286,\"start\":57255},{\"end\":58304,\"start\":58278},{\"end\":58636,\"start\":58588},{\"end\":59437,\"start\":59410},{\"end\":59716,\"start\":59707},{\"end\":60094,\"start\":60050},{\"end\":60494,\"start\":60442},{\"end\":60820,\"start\":60798},{\"end\":61152,\"start\":61100},{\"end\":61508,\"start\":61464},{\"end\":61876,\"start\":61833},{\"end\":62186,\"start\":62160},{\"end\":62779,\"start\":62752},{\"end\":64340,\"start\":64308},{\"end\":64618,\"start\":64596},{\"end\":65257,\"start\":65205},{\"end\":65551,\"start\":65525},{\"end\":65824,\"start\":65797},{\"end\":66181,\"start\":66124},{\"end\":66508,\"start\":66482},{\"end\":66741,\"start\":66715},{\"end\":66976,\"start\":66954},{\"end\":67225,\"start\":67221},{\"end\":67619,\"start\":67578},{\"end\":68086,\"start\":68068},{\"end\":54056,\"start\":54002},{\"end\":54479,\"start\":54448},{\"end\":54897,\"start\":54866},{\"end\":55276,\"start\":55245},{\"end\":55565,\"start\":55540},{\"end\":55907,\"start\":55883},{\"end\":56223,\"start\":56194},{\"end\":56501,\"start\":56456},{\"end\":56868,\"start\":56818},{\"end\":57253,\"start\":57216},{\"end\":57507,\"start\":57492},{\"end\":57720,\"start\":57704},{\"end\":57974,\"start\":57948},{\"end\":58276,\"start\":58240},{\"end\":58586,\"start\":58532},{\"end\":58906,\"start\":58878},{\"end\":59143,\"start\":59124},{\"end\":59408,\"start\":59375},{\"end\":59705,\"start\":59690},{\"end\":60048,\"start\":59998},{\"end\":60440,\"start\":60382},{\"end\":60796,\"start\":60768},{\"end\":61098,\"start\":61040},{\"end\":61462,\"start\":61412},{\"end\":61831,\"start\":61782},{\"end\":62158,\"start\":62122},{\"end\":62453,\"start\":62422},{\"end\":62750,\"start\":62717},{\"end\":63116,\"start\":63085},{\"end\":63528,\"start\":63497},{\"end\":63806,\"start\":63788},{\"end\":64039,\"start\":64011},{\"end\":64306,\"start\":64268},{\"end\":64594,\"start\":64566},{\"end\":64891,\"start\":64875},{\"end\":65203,\"start\":65145},{\"end\":65523,\"start\":65487},{\"end\":65795,\"start\":65762},{\"end\":66122,\"start\":66077},{\"end\":66480,\"start\":66444},{\"end\":66713,\"start\":66677},{\"end\":66952,\"start\":66924},{\"end\":67219,\"start\":67215},{\"end\":67417,\"start\":67380},{\"end\":67576,\"start\":67548},{\"end\":67870,\"start\":67851},{\"end\":68066,\"start\":68038}]"}}}, "year": 2023, "month": 12, "day": 17}