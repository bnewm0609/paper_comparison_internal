{"id": 208138485, "updated": "2023-11-11 03:43:15.958", "metadata": {"title": "Revisiting Shadow Detection: A New Benchmark Dataset for Complex World", "authors": "[{\"first\":\"Xiaowei\",\"last\":\"Hu\",\"middle\":[]},{\"first\":\"Tianyu\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Chi-Wing\",\"last\":\"Fu\",\"middle\":[]},{\"first\":\"Yitong\",\"last\":\"Jiang\",\"middle\":[]},{\"first\":\"Qiong\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Pheng-Ann\",\"last\":\"Heng\",\"middle\":[]}]", "venue": "IEEE Transactions on Image Processing", "journal": "IEEE Transactions on Image Processing", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Shadow detection in general photos is a nontrivial problem, due to the complexity of the real world. Though recent shadow detectors have already achieved remarkable performance on various benchmark data, their performance is still limited for general real-world situations. In this work, we collected shadow images for multiple scenarios and compiled a new dataset of 10,500 shadow images, each with labeled ground-truth mask, for supporting shadow detection in the complex world. Our dataset covers a rich variety of scene categories, with diverse shadow sizes, locations, contrasts, and types. Further, we comprehensively analyze the complexity of the dataset, present a fast shadow detection network with a detail enhancement module to harvest shadow details, and demonstrate the effectiveness of our method to detect shadows in general situations.", "fields_of_study": "[\"Medicine\"]", "external_ids": {"arxiv": null, "mag": "2983716911", "acl": null, "pubmed": "33428570", "pubmedcentral": null, "dblp": "journals/tip/HuWFJWH21", "doi": "10.1109/tip.2021.3049331"}}, "content": {"source": {"pdf_hash": "f3a9a8ba4608866aa0bab5650d5879e7f1cc1433", "pdf_src": "IEEE", "pdf_uri": "[\"https://arxiv.org/pdf/1911.06998v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "https://arxiv.org/pdf/1911.06998", "status": "GREEN"}}, "grobid": {"id": "0cf10108c75e7aaa7ddce788cbc8be84fbf619a2", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/f3a9a8ba4608866aa0bab5650d5879e7f1cc1433.txt", "contents": "\nRevisiting Shadow Detection: A New Benchmark Dataset for Complex World\n\n\nXiaowei Hu \nTianyu Wang \nMember, IEEEChi-Wing Fu \nYitong Jiang \nQiong Wang \nSenior Member, IEEEPheng-Ann Heng \nRevisiting Shadow Detection: A New Benchmark Dataset for Complex World\n\nIEEE TRANSACTIONS ON IMAGE PROCESSING\n30202110.1109/TIP.2021.30493311925\nShadow detection in general photos is a nontrivial problem, due to the complexity of the real world. Though recent shadow detectors have already achieved remarkable performance on various benchmark data, their performance is still limited for general real-world situations. In this work, we collected shadow images for multiple scenarios and compiled a new dataset of 10,500 shadow images, each with labeled ground-truth mask, for supporting shadow detection in the complex world. Our dataset covers a rich variety of scene categories, with diverse shadow sizes, locations, contrasts, and types. Further, we comprehensively analyze the complexity of the dataset, present a fast shadow detection network with a detail enhancement module to harvest shadow details, and demonstrate the effectiveness of our method to detect shadows in general situations.Index Terms-Shadow detection, benchmark dataset, complex, deep neural network. interests include computer vision, image processing, computational photography, low-level vision, and deep learning. Chi-Wing Fu (Member, IEEE) is currently an Associate Professor with The Chinese University of Hong Kong. His recent research interests include computation fabrication, point cloud processing, 3D computer vision, user interaction, and data visualization. He served as the Co-Chair for SIGGRAPH ASIA 2016's Technical Brief and a Poster Program, an Associate Editor for IEEE COMPUTER GRAPH-ICS AND APPLICATIONS and Computer Graphics Forum, a Panel Member in SIGGRAPH 2019 Doctoral Consortium, and program committee members in various research conferences, including SIGGRAPH Asia Technical Brief, SIGGRAPH Asia Emerging Tech., IEEE Visualization, CVPR, IEEE Virtual Reality (VR), VRST, Pacific Graphics, and GMP. Yitong Jiang is currently pursuing the B.Sc. degree in computer science from The Chinese University of Hong Kong. Her research interests include computer vision, deep learning, image processing, and computational photograph.Qiong Wang is currently an Associate Researcher with the Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China. Her research interests include VR applications in medicine, visualization, medical imaging, human-computer interaction, and computer graphics.\n\nI. INTRODUCTION\n\nS HADOWS are formed in the 3D volume behind the objects that occlude the light. The appearance of a shadow generally depends not only on the shape of the occluding object, but also on the light that shines on the object (its direction and strength) and the geometry of the background object on which the shadow is cast. Yet, in general real photos, it is likely that we can observe multiple shadows cast by multiple objects and lights, while the shadows may lie on or go across multiple background objects in the scene. Hence, shadow detection can be a very complicated problem in general situations.\n\nFrom the research literature of computer vision and image processing [1], [2], the presence of shadows could degrade the performance of many object recognition tasks, e.g., object detection and tracking [3], [4], person re-identification [5], etc. Also, the knowledge of shadows in a scene can help to estimate the light conditions [6], [7] and scene geometry [8], [9]. Thus, shadow detection has long been a fundamental problem.\n\nAt present, the de facto approach to detect shadows [10]- [18] is based on deep neural networks, which have demonstrated notable performance on various benchmark data [10], [14], [18]- [21]. However, existing datasets contain mainly shadows cast by single or few separate objects. They do not adequately model the complexity of shadows in the real world; see Figures 1 (a) & (b). Though recent methods [17], [22] already achieved nearly-saturated performance on the benchmarks with a balanced error rate (BER) less than 4% on the SBU [10], [18], [20] and ISTD [14] datasets, if we use them to detect shadows in various types of real-world situations, their performance is rather limited; see Section V. Also, current datasets contain mainly cast shadows with few self shadows, thus limiting the shadow detection performance in general situations. Note that when an object occludes the light and casts shadows, self shadows are regions on the object that do not receive direct light, while cast shadows are projections of the object on some other background objects.\n\nIn this work, we prepare a new dataset to support shadow detection in complex real-world situations. Our dataset contains 10,500 shadow images, each with labeled ground-truth mask. Apart from the dataset size, it has three main advantages when comparing with the existing data. First, the shadow images are collected from diverse scenes, e.g., cities, buildings, satellite maps, and roads, which are general and challenging situations that existing data do not exhibit. Second, our dataset includes cast shadows on background objects and also self shadows on occluding objects. Third, besides the training and testing sets, our dataset provides a validation set for tuning the training parameters and performing ablation study for deep models. This helps to reduce the risk of overfitting.\n\nBesides, we design a fast shadow detection network called FSDNet by adopting the direction-aware spatial context module [12], [16] to aggregate global information from high-level feature maps and formulating a detail enhancement module to harvest shadow details in low-level feature maps. Also, we perform a comprehensive statistical analysis on our dataset to study its complexity, and evaluate the performance of various  [14], SBU [10], [18], [20], and our CUHK-Shadow dataset.\n\nshadow detectors and FSDNet on the data. Experimental results show that FSDNet performs favorably against the stateof-the-arts; particularly, it has only 4M model parameters, so it can achieve real-time performance, while detecting shadow with good quality. The dataset, evaluation code, and source code are publicly available at https://xw-hu.github.io/.\n\n\nII. RELATED WORK\n\nShadow detection on single image has been widely studied in computer vision research. Early methods focus on illumination models or machine learning algorithms by exploring various hand-crafted shadow features, e.g., geometrical properties [7], [23], spectrum ratios [24], color [19], [25]- [27], texture [19], [21], [26], [27], edge [21], [25], [28], and T-junction [25]. These features, however, have limited capability to distinguish between the shadow and non-shadow regions, so approaches based on them often fail to detect shadows in general real-world environments.\n\nLater, methods based on features learned from deep convolutional neural networks (CNNs) demonstrate remarkable improved performance on various benchmarks, especially when large training data is available. Khan et al. [29] adopt CNNs to learn features at the super-pixel level and object boundaries, and use a conditional random field to predict shadow contours. Shen et al. [30] predict the structures of shadow edges from a structured CNN and adopt a global shadow optimization for shadow recovery. Vicente et al. [10] train a stacked-CNN to detect shadows by recovering the noisy shadow annotations. Nguyen et al. [31] introduce a sensitive parameter to the loss in a conditional generative adversarial network to solve the unbalanced labels of shadow and nonshadow regions.\n\nHu et al. [12], [16] aggregate global context features via two rounds of data translations and formulate the direction-aware spatial context features to detect shadows. Wang et al. [14] jointly detect and remove shadows by stacking two conditional generative adversarial networks. Le et al. [13] adopt a shadow attenuation network to generate adversarial training samples, further for training a shadow detection network. Zhu et al. [22] formulate a recurrent attention residual module to selectively use the global and local features in a bidirectional feature pyramid network. Zheng et al. [17] present a distractionaware shadow detection network by explicitly revising the false negative and false positive regions found by other shadow detection methods. Ding et al. [15] jointly detect and remove shadows in a recurrent manner. While these methods have achieved high accuracy in detecting shadows in current benchmarks [10], [18], [20], their performances are still limited for complex real environments; see experiments in Section V-B. Apart from shadow detection, recent works explore deep learning methods to remove shadows [11], [15], [32]- [34], to generate shadows [35], and to detect the shadow-object associations [36], but the training data for these tasks also contains mainly shadows cast by a few objects.\n\nIII. OUR CUHK-SHADOW DATASET Existing datasets for shadow detection, i.e., UCF [21], UIUC [19], SBU [10], [18], [20], and ISTD [14] have been widely used in the past decade. Among them, pioneering ones, i.e., UCF and UIUC, contain only 245 and 108 images, respectively, so deep models trained on them certainly have limited generalization capability, as shown in [10], [20]. For the more recent ones, SBU has 4,087 training images and 638 testing images, whereas ISTD has 1,330 training and 540 testing triples of shadow images, shadow-free images, and shadow masks. Typically, ISTD has only 135 background scenes; while SBU features a wider variety of scenes, both datasets provide mainly shadows cast by single or a few objects. In contrast, our dataset, named as CUHK-Shadow, contains 10,500 shadow images, each with mask, featuring shadows in diverse situations; see Figure 1 for example images randomly picked from ISTD, SBU, and our new dataset.\n\n\nA. Building the Dataset\n\nTo start, we collected shadow images from five different sources by ourselves: (i) Shadow-ADE: 1,132 images from the ADE20K dataset [37], [38] with shadows cast mainly by buildings; (ii) Shadow-KITTI: 2,773 images from the KITTI dataset [39], featuring shadows of vehicles, trees, and objects along the roads; (iii) Shadow-MAP: 1,595 remote-sensing and street-view photos from Google Map; (iv) Shadow-USR: 2,445 images from the USR dataset [33] with mainly people and object shadows; and (v) Shadow-WEB: 2,555 Internet images found by using a web crawler on Flickr website with keywords \"shadow\" and \"shadow building.\" Hence, the dataset enables us to assess the shadow detection performance for various scenarios, corresponding to the need for different applications. Also, images in different categories have domain differences, so our dataset has the potential to be used to evaluate and guide domain adaptation methods for shadow detection.\n\nNext, we hired a professional company to label the shadows in these images. First, we randomly selected some sample images for the company to label, and discussed their labeled masks with them on the label quality. Later, they (around 30 persons) labeled all the dataset images, then we (three of the paper authors) examined the results one by one and asked the company (around five persons) to re-label the low-quality ones. Next, we checked and refined all the masks ourselves. It took around five months for the data labeling, five days for examining the masks, and two weeks for refining the masks.\n\nFurther, we followed COCO [40] to analyze the mask quality by comparing the results with the masks labeled manually by ourselves. In detail, we randomly selected 100 images from the dataset, adopted more expensive operations (iPad Pro and Apple Pencil) to label the masks by three of the paper authors, and used the majority vote to obtain more accurate ground truths. We then compute how close the data labels are with respect to the manual annotations prepared by ourselves; the matching percentage is found to be 96.46%. Note that, since data labeling involves subjective judgement, we argue that it is not always possible to obtain perfect masks through manual annotations. In the future, we shall explore more robust learning approaches to detect shadows with less dependence on the labels. Figure 2 shows example shadow images and masks for the five categories of shadow images in our CUHK-Shadow dataset. We randomly split the images in each category into a training set, validation set, and testing set with a ratio of 7:1:2. So, we have 7,350 training images, 1,050 validation images, and 2,100 testing images in total. To the best of our knowledge, CUHK-Shadow is currently the largest shadow detection dataset with labeled shadow masks. Also, it is the first shadow detection dataset with a validation set, and features a wide variety of real-world situations.\n\n\nB. Dataset Complexity\n\nTo provide a comprehensive understanding of the dataset, we performed a series of statistical analysis on the shadow images and compared the statistical results with the ISTD and SBU datasets in the following aspects.\n\n1) Shadow Area Proportion : First, we find the proportion of pixels (range: [0,1]) occupied by shadows in each image. Figure 3 (left) shows the histogram plots of shadow area proportion for ISTD, SBU, and our CUHK-Shadow dataset. From the histograms, we can see that most images in ISTD and SBU have relatively small shadow regions, while our CUHK-Shadow has more diverse shadow areas compared   [14], SBU [10], [18], [20], AND OUR CUHK-SHADOW with them. Figures 3 (right) further reports histogram plots for the five scene categories in CUHK-Shadow. Interestingly, the distribution in Shadow-WEB can serve as a good reference for general real-world shadows, since the images were obtained from the Internet, while Shadow-KITTI features mainly shadows for road images and Shadow-USR features mainly shadows for people and objects, so the shadow areas in these images have less variation than other categories.\n\n2) Number of Shadows per Image: Next, we group connected shadow pixels and count the number of separated shadows per image in ISTD, SBU, and CUHK-Shadow. To avoid influence of noisy labels, we ignore shadow regions whose area is less than 0.05% of the whole image. Table I reports the resulting statistics, showing that ISTD and SBU only have around 1.51 and 3.44 shadow regions per image, while CUHK-Shadow has far more shadow regions per image on average. This certainly reveals the complexity of CUHK-Shadow. Note also that there are more than ten separate shadow regions per image in Shadow-ADE, showing the challenge of detecting shadows in this data category in CUHK-Shadow.\n\n3) Shadow Location Distribution: Further, we study shadow locations in image space by resizing all shadow masks to 512 \u00d7 512 and summing them up per dataset. So, we can obtain a per-pixel probability value about shadow occurrence. Figure 4 shows the results for the three datasets, revealing that shadows in CUHK-Shadow cover a wider spatial range,  except for the top regions, which are often related to the sky. In contrast, shadows in ISTD locate mainly in the middle, while shadows in SBU locate mainly on the bottom.\n\n\n4) Color Contrast Distribution:\n\nReal-world shadows are often more soft instead of being entirely dark. This means that the color contrast in shadow and non-shadow regions may not be high. Here, we follow [41], [42] to measure the \u03c7 2 distances between the color histograms of the shadow and non-shadow regions in each image. Figure 5 plots the color contrast distribution for images in the three datasets, where a contrast value (horizontal axis in the plot) of one means high color contrast, and vice versa. From the results, the color contrast in CUHK-Shadow is lower than both ISTD and SBU (except a few images with extremely low contrast), so it is more challenging to detect shadows in CUHK-Shadow. 5) Shadow Detection Performance: Last, to reveal the complexity of CUHK-Shadow with respect to ISTD and SBU, we compare the performance of two recent shadow detectors, DSDNet [17] and BDRAR [22], on the three datasets. DSDNet and BDRAR achieve 2.17 and 2.69 balanced error rate (BER) on ISTD, 3.45 and 3.64 BER on SBU, but only 8.09 and 9.13 BER on CUHK-Shadow, respectively, showing that it is more challenging to detect shadows in CUHK-Shadow. 6) Summary: Overall, CUHK-Shadow not only contains far more shadow images and covers a rich variety of scene categories, but also features more diverse proportion of shadow areas, separated shadows, and shadow locations, as well as lower color contrast between shadows and non-shadows. Having said that, it also means that CUHK-Shadow is more challenging and complex for shadow detection. This is also evidenced by the experimental results to be shown in Table III.\n\n\nC. Evaluation Metrics\n\nBalanced error rate (BER) [10] is a common metric to evaluate shadow detection performance, where shadow and nonshadow regions contribute equally to the overall performance without considering their relative areas:\nB E R = (1 \u2212 1 2 ( T P T P + F N + T N T N + F P )) \u00d7 100,(1)\nwhere T P, T N, F P and F N are true positives, true negatives, false positives, and false negatives, respectively. To compute these values, we have to first quantize the predicted shadow mask into a binary mask, then compare this binary mask with the ground truth mask. A lower BER value indicates a better detection result. The recent deep neural networks [12]- [15], [17], [22] predict shadow masks in continuous values, which indicate the probability of a pixel of being inside a shadow. However, BER is designed for evaluating binary predictions, in which predicted values less than 0.5 are set as zeros, or ones, otherwise. Therefore, some false predictions with gray values (false negatives with predicted values greater than 0.5 or false positives with predicted values less than 0.5) give no effects to the qualitative results, but they do affect the visual results; see Section V-B for the comparisons. To evaluate continuous predictions, we introduce the F \u03c9 \u03b2 -measure [43], which extends T P, T N, F P, and F N to\nT P \u03c9 = (1 \u2212 E \u03c9 ) G, T N \u03c9 = (1 \u2212 E \u03c9 ) (1 \u2212 G), F P \u03c9 = E \u03c9 (1 \u2212 G), and F N \u03c9 = E \u03c9 G,(2)\nwhere G is the ground truth image, is the element-wise multiplication, and E \u03c9 denotes the weighted error map:\nE \u03c9 = mi n(E, EA) B,(3)\nwhere E = |G \u2212 M|, M is the predicted shadow mask, mi n computes the minimum value for each element, and A captures the dependency between pairs of foreground pixels:\nA(i, j ) = \u23a7 \u23a8 \u23a8 \u23a8 \u23a8 \u23a8 \u23a8 \u23a8 \u23a9 1 \u221a 2\u03c0\u03c3 2 e \u2212 d(i, j ) 2 2\u03c3 2 \u2200i, j, G(i ) = 1, G( j ) = 1; 1 \u2200i, j, G(i ) = 0, i = j ; 0 other wise,(4)\nwhere d(i, j ) is the Euclidean distance between pixels i and j , and we followed [43] to set \u03c3 2 = 5. Also, B represents the varying importance to the false detections based on their distance from the foreground:\nB(i ) = 1 \u2200i, G(i ) = 1; 2 \u2212 e \u03b1(i) other wise,(5)where (i ) = min G( j )=1\nd(i, j ) and \u03b1 = 1 5 ln(0.5). Finally, the weighted precision, weighted recall, and F \u03c9 \u03b2 are defined as:\nprecision \u03c9 = T P \u03c9 T P \u03c9 + F P \u03c9 , recall \u03c9 = T P \u03c9 T P \u03c9 + F N \u03c9 , (6) F \u03c9 \u03b2 = (1 + \u03b2 2 )\nprecision \u03c9 \u00b7 recall \u03c9 \u03b2 2 \u00b7 precision \u03c9 + recall \u03c9 ,\n\nand we followed [43] to set \u03b2 2 as one. Overall, a larger F \u03c9 \u03b2 indicates a better result. The dataset and evaluation code are publicly available at https://xw-hu.github.io/.\n\n\nIV. METHODOLOGY\n\nA. Network Architecture Figure 6 shows the overall architecture of our fast shadow detection network (FSDNet). It takes a shadow image as input and outputs a shadow mask in an end-to-end manner. First, we use MobileNet V2 [44] as the backbone with a series of inverted residual bottlenecks (IRBs) to extract feature maps in multiple scales. Each IRB contains a 1\u00d71 convolution, a 3\u00d73 depthwise convolution [45], and another 1 \u00d7 1 convolution, with a skip connection to add the input and output feature maps. Also, it adopts batch normalization [46] after each convolution and ReLU6 [47] after the first two convolutions. Second, we employ the direction-aware spatial context (DSC) module [12], [16] after the last convolutional layer of the backbone to harvest the DSC features, which contain global context information for recognizing shadows.\n\nThird, low-level feature maps of the backbone contain rich fine details that can help discover shadow boundaries and tiny shadows. So, we further formulate the detail enhancement module (DEM) by harvesting shadow details in low-level feature maps when the distance between the DSC feature and low-level feature is large. Last, we concatenate the DEMrefined low-level feature, mid-level feature, and high-level feature, then use a series of convolution layers to predict the output shadow mask; see Figure 6 for details. Figure 7 shows the structure of the detail enhancement module (DEM). With low-level feature F L and DSC feature F D as inputs, it first reduces the number of feature channels of F D by a 1 \u00d7 1 convolution and upsample it to the size of F L . Then, we compute gate map G to measure the importance of the detail structures based on the distance between the DSC feature and low-level feature:\n\n\nB. Detail Enhancement Module\nG = \u03b1log(1 + (F L \u2212 F D ) 2 ),(8)\nwhere (F L \u2212 F D ) 2 reports the distance between the two features, which is rescaled by a logarithm function. Then, we follow [48] to introduce a learnable parameter \u03b1 to adjust the scale of the gate map. In the end, we multiply gate map G with input low-level feature F L to enhance the spatial details and produce the refined low-level feature F E . Note that this module only introduces a few parameters (a 1 \u00d7 1 convolution and parameter \u03b1), so the computing time is negligible.\n\n\nC. Training Strategies\n\nWe use the weights of MobileNet V2 trained on Ima-geNet [49] for classification to initialize the backbone network parameters, and initialize the parameters in other layers by random noise. We use stochastic gradient descent with momentum 0.9 and weight decay 0.0005 to optimize the network by minimizing the L 1 loss between the ground-truth and predicted shadow masks. We set the initial learning rate as 0.005, reduce it by the poly strategy [50] with a power of 0.9, and stop the learning after 50k iterations. Last, we implement the network on PyTorch, train it on a GeForce GTX 1080 Ti GPU with a mini-batch size of six, and horizontally flip the images as data augmentation.\n\n\nV. EXPERIMENTAL RESULTS\n\n\nA. Evaluation on the Network Design\n\nWe evaluate the effectiveness of the major components in FSDNet using the validation set of our data. First, we build a \"basic\" model using the last layer of the backbone network to directly predict shadows. This model is built by removing the DSC module, DEM, and skip connections of the low-and middle-level features in the architecture shown in Figure 6. Then, we add back the DSC module to aggregate global features; the model is \"basic+DSC\" in Table II. Further, we consider the low-level features refined by the DEM and set up another network model, namely \"FSDNet w/o DEM,\" by removing the DEM from the whole architecture and directly concatenating the low-, middle-, and high-level features. To evaluate the effectiveness of the multi-scale features, we further build \"FSDNet-high-only\" by removing the multi-scale features and adopting the high-level feature map to directly predict the shadow masks. From the quantitative evaluation results shown in Table II, we can see that the major components help improve the results and contribute to the full pipeline.\n\n\nB. Comparing With the State-of-the-Art\n\n\n1) Comparison With Recent Shadow Detection Methods:\n\nWe consider four recent shadow detection methods: DSD-Net [17], BDRAR [22], A+D Net [13], and DSC [12], [16]. We re-train each of the models on the training set of all categories in our dataset and evaluate them on our testing set. For a fair comparison, we resize all input images to be 512 \u00d7 512. Table III reports the overall quantitative comparison results, from which we observe: (i) Our network uses only 4M parameters and is able to process 77.52 frames per second (see the second column on FPS in Table III) on a single GeForce GTX 1080 Ti GPU. Particularly, it performs faster and achieves more accurate results than the recent real-time shadow detector A+D Net. (ii) Our method performs favorably against all the other methods on the five categories of scenes in our dataset in terms of F \u03c9 \u03b2 and achieves comparable performance with DSDNet [17] on BER. (iii) DSDNet requires the results of BDRAR [22], A+D Net [13], and DSC [12], [16] to discover false positives and false negatives and to train its model during the training. Hence, the entire training time is a summation of the time of the four methods. In contrast, we train our FSDNet using only the ground truths and the training time is only 3.75 hours on a single GeForce GTX 1080 Ti GPU. (iv) Our method performs better on F \u03c9 \u03b2 than BER. This is because BER is designed for evaluating binary predictions, so predicted values that are smaller than 0.5 are all set as zeros, and ones, otherwise. In contrast, F \u03c9 \u03b2 is designed for evaluating continuous predictions. Values predicted by FSDNet tend to near zero or one, while DSDNet and BDRAR tend to produce many gray values; see the false positives in the last two rows of Figure 8 and the false negatives in the first row of Figure 9. Unlike F \u03c9 \u03b2 , BER cannot account for these false predictions.\n\nFurther, Figures 8 & 9 show visual comparison results. Our results are more consistent with the ground truths, while other methods may mis-recognize black regions as shadows, e.g., the green door in the first row and the trees in the last three rows of Figure 8 and the last two rows of Figure 9, or fail to find unobvious shadows, e.g., the shadow across backgrounds of different colors in the first row of Figure 9. However, our method may also fail to detect some extremely tiny shadows of the trees and buildings; see Figure 8 and the last two rows of Figure 9. In the future, we plan to further enhance the DEM by considering patch-based methods to process the image regions with detailed structures in high resolutions.\n\n2) Comparison With Other Networks: Deep network architectures for saliency detection, mirror detection, and semantic segmentation may also be used for shadow detection, if we re-train their models on shadow detection datasets. We took   [42]), and semantic segmentation (i.e., PSPNet [52]), re-trained their models on our training set, then evaluated them on our testing set. The last three rows in Table III show their quantitative results. Comparing our results with theirs, our method still performs favorably against these deep models for both accuracy and speed.\n\n\nC. Category Analysis on Our Dataset\n\nTo explore the domain differences between various categories of shadow images in our CUHK-Shadow dataset, we performed another experiment by training a deep model on training images from each data category and then evaluating the trained models on images of different categories in the validation set. From the results reported in Table IV, we can see that the trained models achieve the best performance on their corresponding data categories, and the performance may degrade largely on other categories. These results show that there exist large domain differences between shadow images collected from different scenarios. Figure 10 shows an example of how shadow detection helps object detection in certain situations. The objects are detected by Google Cloud Vision, 1 but the people under the grandstand were missed out, due to the presence of the 1 https://cloud.google.com/vision/ self shadows; see Figure 10 (a). After adopting a recent underexposed photo enhancement method, i.e., DeepUPE [53] or a simple histogram equalization operation, to adjust the contrast over the whole image, we can enhance the input image. However, the improvement on the object detection is still limited; see Figures 10 (b) & (c). If we apply the histogram equalization operation only on the shadows with the help of our detected shadow mask, we can largely improve the visibility of the people in the shadow regions and also improve the object detection performance, as demonstrated in Figure 10 (d). However, the changed appearance caused by histogram equalization may hurt the performance of other cases. In the future, we will explore a shadow-mask-guided method for photo enhancement to improve both the visual quality and performance of high-level computer vision tasks.\n\n\nD. Application\n\n\nVI. CONCLUSION\n\nThis paper revisits the problem of shadow detection, with a specific aim to handle general real-world situations. We collected and prepared a new benchmark dataset of 10,500 shadow images, each with a labeled shadow mask, from five different categories of sources. Comparing with the existing shadow detection datasets, our CUHK-Shadow dataset comprises images for diverse scene categories, features both cast and self shadows, and also introduces a validation set to reduce the risk of overfitting. We show the complexity of CUHK-Shadow by analyzing the shadow area proportion, number of shadows per image, shadow location distribution, and color contrast between shadow and non-shadow regions. Moreover, we design FSDNet, a new and fast deep neural network architecture, and formulate the detail enhancement module to bring in more shadow details from the low-level features. Comparing with the state-of-the-arts, our network performs favorably for both accuracy and speed. From the results, we can see that all methods (including ours) cannot achieve high performance on our new dataset, different from what have been achieved in the existing datasets. In the future, we plan to explore the performance of shadow detection in different image categories, to prepare shadow data for indoor scenes, and to provide non-binary masks to indicate soft shadows.\n\nFig. 1 .\n1Example shadow images and masks in ISTD\n\nFig. 2 .\n2Example shadow images and shadow masks for categories (i) to (v) in our dataset; see Section III-A for details.\n\nFig. 3 .\n3Analysis on the shadow area proportion for different datasets. Shadows in the ISTD and SBU datasets have mainly small shadows, while our CUHK-Shadow has more diverse types of shadows with wider ranges of sizes in the shadow images.\n\nFig. 4 .\n4Shadow location distributions. Lighter (darker) colors indicate larger (smaller) chances of having shadows.\n\nFig. 5 .\n5Color contrast distributions of different datasets.\n\nFig. 6 .\n6Illustration of our fast shadow detection network (FSDNet). Note that the height of the boxes indicates the size of the associated feature maps. BN and IRB denote batch normalization and inverted residual bottleneck, respectively.\n\nFig. 9 .\n9More visual comparison results (continue fromFigure 8).\n\nFig. 10 .\n10Object detection results on different inputs.\n\nTABLE I\nINUMBER OF SEPARATED SHADOW REGIONS PER IMAGE IN ISTD\n\nTABLE II\nIIABLATION STUDY ON THE VALIDATION SET OF OUR DATASETFig. 7. The detail enhancement module (DEM).\n\nTABLE III\nIIICOMPARING WITH STATE-OF-THE-ART METHODS IN TERMS OF F \u03c9 \u03b2 AND BER. WE TRAINED ALL METHODS ON OUR TRAINING SET AND TESTED THEM ON OUR TESTING SET WITHOUT USING ANY POST-PROCESSING METHOD SUCH AS CRF. NOTE THAT \"FPS\" STANDS FOR \"FRAMES PER SECOND,\" WHICH IS EVALUATED ON A SINGLE GEFORCE GTX 1080 TI GPU WITH A BATCH SIZE OF ONE AND IMAGE SIZE OF 512 \u00d7 512 Fig. 8. Visual comparison of the shadow masks produced by our method and by other shadow detection methods.\n\nTABLE IV\nIVCATEGORY ANALYSIS ON OUR DATASET. THE RESULTS ARE EVALUATED ON THE VALIDATION SET three recent works on saliency detection (i.e., R 3 Net [51]), mirror detection (i.e., MirrorNet\n\nA review: Shadow detection and removal. V Chondagar, H Pandya, M Panchal, R Patel, D Sevak, K Jani, Int. J. Comput. Sci. Inf. Technol. 66V. Chondagar, H. Pandya, M. Panchal, R. Patel, D. Sevak, and K. Jani, \"A review: Shadow detection and removal,\" Int. J. Comput. Sci. Inf. Technol., vol. 6, no. 6, pp. 5536-5541, 2015.\n\nShadow detection and removal in real images: A survey. L Xu, F Qi, R Jiang, Y Hao, G Wu, Shanghai, ChinaCVLAB, Shanghai Jiao Tong Univ.Tech. RepL. Xu, F. Qi, R. Jiang, Y. Hao, and G. Wu, \"Shadow detection and removal in real images: A survey,\" CVLAB, Shanghai Jiao Tong Univ., Shanghai, China, Tech. Rep., 2006. [Online]. Available: https://www.academia.edu/1051743/Shadow_Detection_and_Removal_ in_Real_Images_A_Survey\n\nDetecting moving objects, ghosts, and shadows in video streams. R Cucchiara, C Grana, M Piccardi, A Prati, IEEE Trans. Pattern Anal. Mach. Intell. 2510R. Cucchiara, C. Grana, M. Piccardi, and A. Prati, \"Detecting moving objects, ghosts, and shadows in video streams,\" IEEE Trans. Pattern Anal. Mach. Intell., vol. 25, no. 10, pp. 1337-1342, Oct. 2003.\n\nPhysical models for moving shadow and object detection in video. S Nadimi, B Bhanu, IEEE Trans. Pattern Anal. Mach. Intell. 268S. Nadimi and B. Bhanu, \"Physical models for moving shadow and object detection in video,\" IEEE Trans. Pattern Anal. Mach. Intell., vol. 26, no. 8, pp. 1079-1087, Aug. 2004.\n\nImplementing a robust explanatory bias in a person re-identification network. E Bekele, W E Lawson, Z Horne, S Khemlani, Proc. nullE. Bekele, W. E. Lawson, Z. Horne, and S. Khemlani, \"Implementing a robust explanatory bias in a person re-identification network,\" in Proc.\n\nIeee/Cvf, Conf, Comput. Vis. Pattern Recognit. Workshops (CVPRW). IEEE/CVF Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW), Jun. 2018, pp. 2165-2172.\n\nEstimating natural illumination from a single outdoor image. J.-F Lalonde, A A Efros, S G Narasimhan, Proc. IEEE 12th Int. Conf. Comput. Vis. IEEE 12th Int. Conf. Comput. VisJ.-F. Lalonde, A. A. Efros, and S. G. Narasimhan, \"Estimating natural illumination from a single outdoor image,\" in Proc. IEEE 12th Int. Conf. Comput. Vis., Sep. 2009, pp. 183-190.\n\nIllumination estimation and cast shadow detection through a higher-order graphical model. A Panagopoulos, C Wang, D Samaras, N Paragios, Proc. CVPR. CVPRA. Panagopoulos, C. Wang, D. Samaras, and N. Paragios, \"Illumination estimation and cast shadow detection through a higher-order graphical model,\" in Proc. CVPR, Jun. 2011, pp. 673-680.\n\nRendering synthetic objects into legacy photographs. K Karsch, V Hedau, D Forsyth, D Hoiem, ACM Trans. Graph. 30612K. Karsch, V. Hedau, D. Forsyth, and D. Hoiem, \"Rendering synthetic objects into legacy photographs,\" ACM Trans. Graph., vol. 30, no. 6, pp. 157:1-157:12, 2011.\n\nAttached shadow coding: Estimating surface normals from shadows under unknown reflectance and lighting conditions. T Okabe, I Sato, Y Sato, Proc. IEEE 12th Int. Conf. Comput. Vis. IEEE 12th Int. Conf. Comput. VisT. Okabe, I. Sato, and Y. Sato, \"Attached shadow coding: Estimating surface normals from shadows under unknown reflectance and lighting conditions,\" in Proc. IEEE 12th Int. Conf. Comput. Vis., Sep. 2009, pp. 1693-1700.\n\nLargescale training of shadow detectors with noisily-annotated shadow examples. T F Y Vicente, L Hou, C.-P Yu, M Hoai, D Samaras, Proc. ECCV. ECCVT. F. Y. Vicente, L. Hou, C.-P. Yu, M. Hoai, and D. Samaras, \"Large- scale training of shadow detectors with noisily-annotated shadow exam- ples,\" in Proc. ECCV, 2016, pp. 816-832.\n\nAutomatic shadow detection and removal from a single image. S H Khan, M Bennamoun, F Sohel, R Togneri, IEEE Trans. Pattern Anal. Mach. Intell. 383S. H. Khan, M. Bennamoun, F. Sohel, and R. Togneri, \"Automatic shadow detection and removal from a single image,\" IEEE Trans. Pattern Anal. Mach. Intell., vol. 38, no. 3, pp. 431-446, Mar. 2016.\n\nDirection-aware spatial context features for shadow detection. X Hu, L Zhu, C.-W Fu, J Qin, P.-A Heng, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. IEEE/CVF Conf. Comput. Vis. Pattern RecognitX. Hu, L. Zhu, C.-W. Fu, J. Qin, and P.-A. Heng, \"Direction-aware spatial context features for shadow detection,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2018, pp. 7454-7462.\n\nA+D Net: Training a shadow detector with adversarial shadow attenuation. H Le, T F Y Vicente, V Nguyen, M Hoai, D Samaras, Proc. ECCV. ECCVH. Le, T. F. Y. Vicente, V. Nguyen, M. Hoai, and D. Samaras, \"A+D Net: Training a shadow detector with adversarial shadow attenuation,\" in Proc. ECCV, 2018, pp. 662-678.\n\nStacked conditional generative adversarial networks for jointly learning shadow detection and shadow removal. J Wang, X Li, J Yang, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. IEEE/CVF Conf. Comput. Vis. Pattern RecognitJ. Wang, X. Li, and J. Yang, \"Stacked conditional generative adversarial networks for jointly learning shadow detection and shadow removal,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2018, pp. 1788-1797.\n\nARGAN: Attentive recurrent generative adversarial network for shadow detection and removal. B Ding, C Long, L Zhang, C Xiao, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV). IEEE/CVF Int. Conf. Comput. Vis. (ICCV)B. Ding, C. Long, L. Zhang, and C. Xiao, \"ARGAN: Attentive recurrent generative adversarial network for shadow detection and removal,\" in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2019, pp. 10213-10222.\n\nDirection-aware spatial context features for shadow detection and removal. X Hu, C.-W Fu, L Zhu, J Qin, P.-A Heng, IEEE Trans. Pattern Anal. Mach. Intell. 4211X. Hu, C.-W. Fu, L. Zhu, J. Qin, and P.-A. Heng, \"Direction-aware spatial context features for shadow detection and removal,\" IEEE Trans. Pattern Anal. Mach. Intell., vol. 42, no. 11, pp. 2795-2808, Nov. 2020.\n\nDistraction-aware shadow detection. Q Zheng, X Qiao, Y Cao, R W H Lau, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)Q. Zheng, X. Qiao, Y. Cao, and R. W. H. Lau, \"Distraction-aware shadow detection,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2019, pp. 5167-5176.\n\nLarge scale shadow annotation and detection using lazy annotation and stacked CNNs. L Hou, T F Y Vicente, M Hoai, D Samaras, 10.1109/TPAMI.2019.2948011IEEE Trans. Pattern Anal. Mach. Intell., early access. L. Hou, T. F. Y. Vicente, M. Hoai, and D. Samaras, \"Large scale shadow annotation and detection using lazy annotation and stacked CNNs,\" IEEE Trans. Pattern Anal. Mach. Intell., early access, Oct. 17, 2019, doi: 10.1109/TPAMI.2019.2948011.\n\nSingle-image shadow detection and removal using paired regions. R Guo, Q Dai, D Hoiem, Proc. CVPR. CVPRR. Guo, Q. Dai, and D. Hoiem, \"Single-image shadow detection and removal using paired regions,\" in Proc. CVPR, Jun. 2011, pp. 2033-2040.\n\nNoisy label recovery for shadow detection in unfamiliar domains. T F Y Vicente, M Hoai, D Samaras, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)T. F. Y. Vicente, M. Hoai, and D. Samaras, \"Noisy label recovery for shadow detection in unfamiliar domains,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2016, pp. 3783-3792.\n\nLearning to recognize shadows in monochromatic natural images. J Zhu, K G G Samuel, S Z Masood, M F Tappen, Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. IEEE Comput. Soc. Conf. Comput. Vis. Pattern RecognitJ. Zhu, K. G. G. Samuel, S. Z. Masood, and M. F. Tappen, \"Learning to recognize shadows in monochromatic natural images,\" in Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit., Jun. 2010, pp. 223-230.\n\nBidirectional feature pyramid network with recurrent attention residual modules for shadow detection. L Zhu, Proc. ECCV. ECCVL. Zhu et al., \"Bidirectional feature pyramid network with recurrent attention residual modules for shadow detection,\" in Proc. ECCV, 2018, pp. 121-136.\n\nCast shadow segmentation using invariant color features. E Salvador, A Cavallaro, T Ebrahimi, Comput. Vis. Image Understand. 952E. Salvador, A. Cavallaro, and T. Ebrahimi, \"Cast shadow segmentation using invariant color features,\" Comput. Vis. Image Understand., vol. 95, no. 2, pp. 238-259, Aug. 2004.\n\nNew spectrum ratio properties and features for shadow detection. J Tian, X Qi, L Qu, Y Tang, Pattern Recognit. 51J. Tian, X. Qi, L. Qu, and Y. Tang, \"New spectrum ratio properties and features for shadow detection,\" Pattern Recognit., vol. 51, pp. 85-96, Mar. 2016.\n\nDetecting ground shadows in outdoor consumer photographs. J.-F Lalonde, A A Efros, S G Narasimhan, Proc. ECCV. ECCVJ.-F. Lalonde, A. A. Efros, and S. G. Narasimhan, \"Detecting ground shadows in outdoor consumer photographs,\" in Proc. ECCV, 2010, pp. 322-335.\n\nLeave-one-out kernel optimization for shadow detection. T F Y Vicente, M Hoai, D Samaras, Proc. IEEE Int. Conf. Comput. Vis. (ICCV). IEEE Int. Conf. Comput. Vis. (ICCV)T. F. Y. Vicente, M. Hoai, and D. Samaras, \"Leave-one-out kernel optimization for shadow detection,\" in Proc. IEEE Int. Conf. Comput. Vis. (ICCV), Dec. 2015, pp. 3388-3396.\n\nLeave-one-out kernel optimization for shadow detection and removal. T F Y Vicente, M Hoai, D Samaras, IEEE Trans. Pattern Anal. Mach. Intell. 403T. F. Y. Vicente, M. Hoai, and D. Samaras, \"Leave-one-out kernel optimization for shadow detection and removal,\" IEEE Trans. Pattern Anal. Mach. Intell., vol. 40, no. 3, pp. 682-695, Mar. 2018.\n\nWhat characterizes a shadow boundary under the sun and sky. X Huang, G Hua, J Tumblin, L Williams, Proc. Int. Conf. Comput. Vis. Int. Conf. Comput. VisX. Huang, G. Hua, J. Tumblin, and L. Williams, \"What characterizes a shadow boundary under the sun and sky?\" in Proc. Int. Conf. Comput. Vis., Nov. 2011, pp. 898-905.\n\nAutomatic feature learning for robust shadow detection. S H Khan, M Bennamoun, F Sohel, R Togneri, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitS. H. Khan, M. Bennamoun, F. Sohel, and R. Togneri, \"Automatic feature learning for robust shadow detection,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., Jun. 2014, pp. 1939-1946.\n\nShadow optimization from structured deep edge detection. L Shen, T W Chua, K Leman, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)L. Shen, T. W. Chua, and K. Leman, \"Shadow optimization from structured deep edge detection,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2015, pp. 2067-2074.\n\nShadow detection with conditional generative adversarial networks. V Nguyen, T F Y Vicente, M Zhao, M Hoai, D Samaras, Proc. IEEE Int. Conf. Comput. Vis. (ICCV). IEEE Int. Conf. Comput. Vis. (ICCV)V. Nguyen, T. F. Y. Vicente, M. Zhao, M. Hoai, and D. Samaras, \"Shadow detection with conditional generative adversarial networks,\" in Proc. IEEE Int. Conf. Comput. Vis. (ICCV), Oct. 2017, pp. 4510-4518.\n\nDeshadowNet: A multi-context embedding deep network for shadow removal. L Qu, J Tian, S He, Y Tang, R W H Lau, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)L. Qu, J. Tian, S. He, Y. Tang, and R. W. H. Lau, \"DeshadowNet: A multi-context embedding deep network for shadow removal,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jul. 2017, pp. 4067-4075.\n\nMask-ShadowGAN: Learning to remove shadows from unpaired data. X Hu, Y Jiang, C.-W Fu, P.-A Heng, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV). IEEE/CVF Int. Conf. Comput. Vis. (ICCV)X. Hu, Y. Jiang, C.-W. Fu, and P.-A. Heng, \"Mask-ShadowGAN: Learning to remove shadows from unpaired data,\" in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2019, pp. 2472-2481.\n\nShadow removal via shadow image decomposition. H Le, D Samaras, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV). IEEE/CVF Int. Conf. Comput. Vis. (ICCV)H. Le and D. Samaras, \"Shadow removal via shadow image decompo- sition,\" in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2019, pp. 8578-8587.\n\nARShad-owGAN: Shadow generative adversarial network for augmented reality in single light scenes. D Liu, C Long, H Zhang, H Yu, X Dong, C Xiao, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)D. Liu, C. Long, H. Zhang, H. Yu, X. Dong, and C. Xiao, \"ARShad- owGAN: Shadow generative adversarial network for augmented reality in single light scenes,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2020, pp. 8139-8148.\n\nInstance shadow detection. T Wang, X Hu, Q Wang, P.-A Heng, C.-W Fu, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)T. Wang, X. Hu, Q. Wang, P.-A. Heng, and C.-W. Fu, \"Instance shadow detection,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2020, pp. 1880-1889.\n\nScene parsing through ADE20K dataset. B Zhou, H Zhao, X Puig, S Fidler, A Barriuso, A Torralba, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, and A. Torralba, \"Scene parsing through ADE20K dataset,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jul. 2017, pp. 633-641.\n\nSemantic understanding of scenes through the ADE20K dataset. B Zhou, Int. J. Comput. Vis. 1273B. Zhou et al., \"Semantic understanding of scenes through the ADE20K dataset,\" Int. J. Comput. Vis., vol. 127, no. 3, pp. 302-321, Mar. 2019.\n\nAre we ready for autonomous driving? The KITTI vision benchmark suite. A Geiger, P Lenz, R Urtasun, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitA. Geiger, P. Lenz, and R. Urtasun, \"Are we ready for autonomous driving? The KITTI vision benchmark suite,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., Jun. 2012, pp. 3354-3361.\n\nMicrosoft COCO: Common objects in context. T.-Y. Lin, Proc. ECCV. ECCVT.-Y. Lin et al., \"Microsoft COCO: Common objects in context,\" in Proc. ECCV, 2014, pp. 740-755.\n\nThe secrets of salient object segmentation. Y Li, X Hou, C Koch, J M Rehg, A L Yuille, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitY. Li, X. Hou, C. Koch, J. M. Rehg, and A. L. Yuille, \"The secrets of salient object segmentation,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., Jun. 2014, pp. 280-287.\n\nWhere is my mirror. X Yang, H Mei, K Xu, X Wei, B Yin, R Lau, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV). IEEE/CVF Int. Conf. Comput. Vis. (ICCV)X. Yang, H. Mei, K. Xu, X. Wei, B. Yin, and R. Lau, \"Where is my mirror?\" in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2019, pp. 8809-8818.\n\nHow to evaluate foreground maps. R Margolin, L Zelnik-Manor, A Tal, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. CVPR. IEEE Conf. Comput. Vis. Pattern Recognit. CVPRR. Margolin, L. Zelnik-Manor, and A. Tal, \"How to evaluate foreground maps,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. CVPR, Jun. 2014, pp. 248-255.\n\nMobileNetV2: Inverted residuals and linear bottlenecks. M Sandler, A Howard, M Zhu, A Zhmoginov, L.-C Chen, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. IEEE/CVF Conf. Comput. Vis. Pattern RecognitM. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen, \"MobileNetV2: Inverted residuals and linear bottlenecks,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2018, pp. 4510-4520.\n\nXception: Deep learning with depthwise separable convolutions. F Chollet, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)F. Chollet, \"Xception: Deep learning with depthwise separable convo- lutions,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jul. 2017, pp. 1251-1258.\n\nBatch normalization: Accelerating deep network training by reducing internal covariate shift. S Ioffe, C Szegedy, arXiv:1502.03167S. Ioffe and C. Szegedy, \"Batch normalization: Accelerating deep network training by reducing internal covariate shift,\" 2015, arXiv:1502.03167. [Online]. Available: http://arxiv.org/abs/1502.03167\n\nMobileNets: Efficient convolutional neural networks for mobile vision applications. A G Howard, arXiv:1704.04861A. G. Howard et al., \"MobileNets: Efficient convolutional neural networks for mobile vision applications,\" 2017, arXiv:1704.04861. [Online]. Available: http://arxiv.org/abs/1704.04861\n\nAdaptive context network for scene parsing. J Fu, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV). IEEE/CVF Int. Conf. Comput. Vis. (ICCV)J. Fu et al., \"Adaptive context network for scene parsing,\" in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2019, pp. 6748-6757.\n\nImageNet: A large-scale hierarchical image database. J Deng, W Dong, R Socher, L.-J Li, K Li, L Fei-Fei, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, \"ImageNet: A large-scale hierarchical image database,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., Jun. 2009, pp. 248-255.\n\nParseNet: Looking wider to see better. W Liu, A Rabinovich, A C Berg, arXiv:1506.04579W. Liu, A. Rabinovich, and A. C. Berg, \"ParseNet: Looking wider to see better,\" 2015, arXiv:1506.04579. [Online]. Available: http://arxiv.org/abs/1506.04579\n\nR 3 Net: Recurrent residual refinement network for saliency detection. Z Deng, Proc. IJCAI. IJCAIZ. Deng et al., \"R 3 Net: Recurrent residual refinement network for saliency detection,\" in Proc. IJCAI, 2018, pp. 684-690.\n\nPyramid scene parsing network. H Zhao, J Shi, X Qi, X Wang, J Jia, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, \"Pyramid scene parsing network,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jul. 2017, pp. 2881-2890.\n\nUnderexposed photo enhancement using deep illumination estimation. R Wang, Q Zhang, C.-W Fu, X Shen, W.-S Zheng, J Jia, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)R. Wang, Q. Zhang, C.-W. Fu, X. Shen, W.-S. Zheng, and J. Jia, \"Underexposed photo enhancement using deep illumination estimation,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2019, pp. 6849-6857.\n", "annotations": {"author": "[{\"end\":85,\"start\":74},{\"end\":98,\"start\":86},{\"end\":123,\"start\":99},{\"end\":137,\"start\":124},{\"end\":149,\"start\":138},{\"end\":184,\"start\":150}]", "publisher": null, "author_last_name": "[{\"end\":84,\"start\":82},{\"end\":97,\"start\":93},{\"end\":122,\"start\":120},{\"end\":136,\"start\":131},{\"end\":148,\"start\":144},{\"end\":183,\"start\":179}]", "author_first_name": "[{\"end\":81,\"start\":74},{\"end\":92,\"start\":86},{\"end\":119,\"start\":111},{\"end\":130,\"start\":124},{\"end\":143,\"start\":138},{\"end\":178,\"start\":169}]", "author_affiliation": null, "title": "[{\"end\":71,\"start\":1},{\"end\":255,\"start\":185}]", "venue": "[{\"end\":294,\"start\":257}]", "abstract": "[{\"end\":2599,\"start\":330}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3292,\"start\":3289},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3297,\"start\":3294},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3426,\"start\":3423},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3431,\"start\":3428},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3461,\"start\":3458},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3555,\"start\":3552},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3560,\"start\":3557},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3583,\"start\":3580},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3588,\"start\":3585},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3707,\"start\":3703},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3713,\"start\":3709},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3822,\"start\":3818},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3828,\"start\":3824},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3834,\"start\":3830},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3840,\"start\":3836},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4057,\"start\":4053},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4063,\"start\":4059},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4189,\"start\":4185},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4195,\"start\":4191},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4201,\"start\":4197},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4215,\"start\":4211},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5633,\"start\":5629},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5639,\"start\":5635},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5937,\"start\":5933},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5947,\"start\":5943},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5953,\"start\":5949},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5959,\"start\":5955},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6610,\"start\":6607},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6616,\"start\":6612},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6638,\"start\":6634},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6650,\"start\":6646},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6656,\"start\":6652},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6662,\"start\":6658},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6676,\"start\":6672},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6682,\"start\":6678},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6688,\"start\":6684},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6694,\"start\":6690},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6705,\"start\":6701},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6711,\"start\":6707},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":6717,\"start\":6713},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6738,\"start\":6734},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7162,\"start\":7158},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7319,\"start\":7315},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7460,\"start\":7456},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7561,\"start\":7557},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7733,\"start\":7729},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7739,\"start\":7735},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7904,\"start\":7900},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8014,\"start\":8010},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8156,\"start\":8152},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8315,\"start\":8311},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8494,\"start\":8490},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8647,\"start\":8643},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8653,\"start\":8649},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8659,\"start\":8655},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8855,\"start\":8851},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8861,\"start\":8857},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8867,\"start\":8863},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8873,\"start\":8869},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8899,\"start\":8895},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8950,\"start\":8946},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9126,\"start\":9122},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9137,\"start\":9133},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9147,\"start\":9143},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9153,\"start\":9149},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9159,\"start\":9155},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9174,\"start\":9170},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9410,\"start\":9406},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9416,\"start\":9412},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10158,\"start\":10154},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":10164,\"start\":10160},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":10263,\"start\":10259},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":10466,\"start\":10462},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":11602,\"start\":11598},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":13588,\"start\":13584},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":13598,\"start\":13594},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":13604,\"start\":13600},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":13610,\"start\":13606},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":15514,\"start\":15510},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":15520,\"start\":15516},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":16189,\"start\":16185},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":16204,\"start\":16200},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":16977,\"start\":16973},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":17586,\"start\":17582},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":17592,\"start\":17588},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":17598,\"start\":17594},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":17604,\"start\":17600},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":18209,\"start\":18205},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":18866,\"start\":18862},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":19343,\"start\":19339},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":19743,\"start\":19739},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":19927,\"start\":19923},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":20065,\"start\":20061},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":20103,\"start\":20099},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":20209,\"start\":20205},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":20215,\"start\":20211},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":21469,\"start\":21465},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":21908,\"start\":21904},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":22297,\"start\":22293},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":23822,\"start\":23818},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":23834,\"start\":23830},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":23848,\"start\":23844},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":23862,\"start\":23858},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":23868,\"start\":23864},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":24615,\"start\":24611},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":24671,\"start\":24667},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":24685,\"start\":24681},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":24699,\"start\":24695},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":24705,\"start\":24701},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":26548,\"start\":26544},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":26595,\"start\":26591},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":27916,\"start\":27912}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":30121,\"start\":30071},{\"attributes\":{\"id\":\"fig_1\"},\"end\":30244,\"start\":30122},{\"attributes\":{\"id\":\"fig_2\"},\"end\":30487,\"start\":30245},{\"attributes\":{\"id\":\"fig_3\"},\"end\":30606,\"start\":30488},{\"attributes\":{\"id\":\"fig_4\"},\"end\":30669,\"start\":30607},{\"attributes\":{\"id\":\"fig_5\"},\"end\":30911,\"start\":30670},{\"attributes\":{\"id\":\"fig_6\"},\"end\":30978,\"start\":30912},{\"attributes\":{\"id\":\"fig_7\"},\"end\":31037,\"start\":30979},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":31100,\"start\":31038},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":31208,\"start\":31101},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":31685,\"start\":31209},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":31876,\"start\":31686}]", "paragraph": "[{\"end\":3218,\"start\":2618},{\"end\":3649,\"start\":3220},{\"end\":4716,\"start\":3651},{\"end\":5507,\"start\":4718},{\"end\":5989,\"start\":5509},{\"end\":6346,\"start\":5991},{\"end\":6939,\"start\":6367},{\"end\":7717,\"start\":6941},{\"end\":9041,\"start\":7719},{\"end\":9994,\"start\":9043},{\"end\":10966,\"start\":10022},{\"end\":11570,\"start\":10968},{\"end\":12943,\"start\":11572},{\"end\":13186,\"start\":12969},{\"end\":14097,\"start\":13188},{\"end\":14779,\"start\":14099},{\"end\":15302,\"start\":14781},{\"end\":16921,\"start\":15338},{\"end\":17161,\"start\":16947},{\"end\":18250,\"start\":17224},{\"end\":18454,\"start\":18344},{\"end\":18645,\"start\":18479},{\"end\":18993,\"start\":18780},{\"end\":19175,\"start\":19070},{\"end\":19321,\"start\":19268},{\"end\":19497,\"start\":19323},{\"end\":20361,\"start\":19517},{\"end\":21272,\"start\":20363},{\"end\":21821,\"start\":21338},{\"end\":22529,\"start\":21848},{\"end\":23663,\"start\":22595},{\"end\":25578,\"start\":23760},{\"end\":26305,\"start\":25580},{\"end\":26874,\"start\":26307},{\"end\":28678,\"start\":26914},{\"end\":30070,\"start\":28714}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":17223,\"start\":17162},{\"attributes\":{\"id\":\"formula_1\"},\"end\":18343,\"start\":18251},{\"attributes\":{\"id\":\"formula_2\"},\"end\":18478,\"start\":18455},{\"attributes\":{\"id\":\"formula_3\"},\"end\":18779,\"start\":18646},{\"attributes\":{\"id\":\"formula_4\"},\"end\":19044,\"start\":18994},{\"attributes\":{\"id\":\"formula_5\"},\"end\":19069,\"start\":19044},{\"attributes\":{\"id\":\"formula_6\"},\"end\":19267,\"start\":19176},{\"attributes\":{\"id\":\"formula_8\"},\"end\":21337,\"start\":21304}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":14371,\"start\":14364},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":16920,\"start\":16911},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":23052,\"start\":23044},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":23563,\"start\":23555},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":24068,\"start\":24059},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":24274,\"start\":24265},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":26715,\"start\":26706},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":27253,\"start\":27245}]", "section_header": "[{\"end\":2616,\"start\":2601},{\"end\":6365,\"start\":6349},{\"end\":10020,\"start\":9997},{\"end\":12967,\"start\":12946},{\"end\":15336,\"start\":15305},{\"end\":16945,\"start\":16924},{\"end\":19515,\"start\":19500},{\"end\":21303,\"start\":21275},{\"end\":21846,\"start\":21824},{\"end\":22555,\"start\":22532},{\"end\":22593,\"start\":22558},{\"end\":23704,\"start\":23666},{\"end\":23758,\"start\":23707},{\"end\":26912,\"start\":26877},{\"end\":28695,\"start\":28681},{\"end\":28712,\"start\":28698},{\"end\":30080,\"start\":30072},{\"end\":30131,\"start\":30123},{\"end\":30254,\"start\":30246},{\"end\":30497,\"start\":30489},{\"end\":30616,\"start\":30608},{\"end\":30679,\"start\":30671},{\"end\":30921,\"start\":30913},{\"end\":30989,\"start\":30980},{\"end\":31046,\"start\":31039},{\"end\":31110,\"start\":31102},{\"end\":31219,\"start\":31210},{\"end\":31695,\"start\":31687}]", "table": null, "figure_caption": "[{\"end\":30121,\"start\":30082},{\"end\":30244,\"start\":30133},{\"end\":30487,\"start\":30256},{\"end\":30606,\"start\":30499},{\"end\":30669,\"start\":30618},{\"end\":30911,\"start\":30681},{\"end\":30978,\"start\":30923},{\"end\":31037,\"start\":30992},{\"end\":31100,\"start\":31048},{\"end\":31208,\"start\":31113},{\"end\":31685,\"start\":31223},{\"end\":31876,\"start\":31698}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9922,\"start\":9914},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12376,\"start\":12368},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":13314,\"start\":13306},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":13660,\"start\":13643},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":15020,\"start\":15012},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":15639,\"start\":15631},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":19549,\"start\":19541},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":20869,\"start\":20861},{\"end\":20891,\"start\":20883},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":22951,\"start\":22943},{\"end\":25461,\"start\":25453},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":25514,\"start\":25506},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":25602,\"start\":25589},{\"end\":25841,\"start\":25833},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":25875,\"start\":25867},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":25996,\"start\":25988},{\"end\":26110,\"start\":26102},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":26144,\"start\":26136},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":27548,\"start\":27539},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":27829,\"start\":27820},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":28398,\"start\":28389}]", "bib_author_first_name": "[{\"end\":31919,\"start\":31918},{\"end\":31932,\"start\":31931},{\"end\":31942,\"start\":31941},{\"end\":31953,\"start\":31952},{\"end\":31962,\"start\":31961},{\"end\":31971,\"start\":31970},{\"end\":32256,\"start\":32255},{\"end\":32262,\"start\":32261},{\"end\":32268,\"start\":32267},{\"end\":32277,\"start\":32276},{\"end\":32284,\"start\":32283},{\"end\":32686,\"start\":32685},{\"end\":32699,\"start\":32698},{\"end\":32708,\"start\":32707},{\"end\":32720,\"start\":32719},{\"end\":33040,\"start\":33039},{\"end\":33050,\"start\":33049},{\"end\":33355,\"start\":33354},{\"end\":33365,\"start\":33364},{\"end\":33367,\"start\":33366},{\"end\":33377,\"start\":33376},{\"end\":33386,\"start\":33385},{\"end\":33772,\"start\":33768},{\"end\":33783,\"start\":33782},{\"end\":33785,\"start\":33784},{\"end\":33794,\"start\":33793},{\"end\":33796,\"start\":33795},{\"end\":34154,\"start\":34153},{\"end\":34170,\"start\":34169},{\"end\":34178,\"start\":34177},{\"end\":34189,\"start\":34188},{\"end\":34457,\"start\":34456},{\"end\":34467,\"start\":34466},{\"end\":34476,\"start\":34475},{\"end\":34487,\"start\":34486},{\"end\":34796,\"start\":34795},{\"end\":34805,\"start\":34804},{\"end\":34813,\"start\":34812},{\"end\":35193,\"start\":35192},{\"end\":35197,\"start\":35194},{\"end\":35208,\"start\":35207},{\"end\":35218,\"start\":35214},{\"end\":35224,\"start\":35223},{\"end\":35232,\"start\":35231},{\"end\":35501,\"start\":35500},{\"end\":35503,\"start\":35502},{\"end\":35511,\"start\":35510},{\"end\":35524,\"start\":35523},{\"end\":35533,\"start\":35532},{\"end\":35846,\"start\":35845},{\"end\":35852,\"start\":35851},{\"end\":35862,\"start\":35858},{\"end\":35868,\"start\":35867},{\"end\":35878,\"start\":35874},{\"end\":36252,\"start\":36251},{\"end\":36258,\"start\":36257},{\"end\":36262,\"start\":36259},{\"end\":36273,\"start\":36272},{\"end\":36283,\"start\":36282},{\"end\":36291,\"start\":36290},{\"end\":36599,\"start\":36598},{\"end\":36607,\"start\":36606},{\"end\":36613,\"start\":36612},{\"end\":37033,\"start\":37032},{\"end\":37041,\"start\":37040},{\"end\":37049,\"start\":37048},{\"end\":37058,\"start\":37057},{\"end\":37441,\"start\":37440},{\"end\":37450,\"start\":37446},{\"end\":37456,\"start\":37455},{\"end\":37463,\"start\":37462},{\"end\":37473,\"start\":37469},{\"end\":37772,\"start\":37771},{\"end\":37781,\"start\":37780},{\"end\":37789,\"start\":37788},{\"end\":37796,\"start\":37795},{\"end\":37800,\"start\":37797},{\"end\":38176,\"start\":38175},{\"end\":38183,\"start\":38182},{\"end\":38187,\"start\":38184},{\"end\":38198,\"start\":38197},{\"end\":38206,\"start\":38205},{\"end\":38603,\"start\":38602},{\"end\":38610,\"start\":38609},{\"end\":38617,\"start\":38616},{\"end\":38845,\"start\":38844},{\"end\":38849,\"start\":38846},{\"end\":38860,\"start\":38859},{\"end\":38868,\"start\":38867},{\"end\":39242,\"start\":39241},{\"end\":39249,\"start\":39248},{\"end\":39253,\"start\":39250},{\"end\":39263,\"start\":39262},{\"end\":39265,\"start\":39264},{\"end\":39275,\"start\":39274},{\"end\":39277,\"start\":39276},{\"end\":39715,\"start\":39714},{\"end\":39949,\"start\":39948},{\"end\":39961,\"start\":39960},{\"end\":39974,\"start\":39973},{\"end\":40261,\"start\":40260},{\"end\":40269,\"start\":40268},{\"end\":40275,\"start\":40274},{\"end\":40281,\"start\":40280},{\"end\":40524,\"start\":40520},{\"end\":40535,\"start\":40534},{\"end\":40537,\"start\":40536},{\"end\":40546,\"start\":40545},{\"end\":40548,\"start\":40547},{\"end\":40779,\"start\":40778},{\"end\":40783,\"start\":40780},{\"end\":40794,\"start\":40793},{\"end\":40802,\"start\":40801},{\"end\":41133,\"start\":41132},{\"end\":41137,\"start\":41134},{\"end\":41148,\"start\":41147},{\"end\":41156,\"start\":41155},{\"end\":41465,\"start\":41464},{\"end\":41474,\"start\":41473},{\"end\":41481,\"start\":41480},{\"end\":41492,\"start\":41491},{\"end\":41780,\"start\":41779},{\"end\":41782,\"start\":41781},{\"end\":41790,\"start\":41789},{\"end\":41803,\"start\":41802},{\"end\":41812,\"start\":41811},{\"end\":42157,\"start\":42156},{\"end\":42165,\"start\":42164},{\"end\":42167,\"start\":42166},{\"end\":42175,\"start\":42174},{\"end\":42535,\"start\":42534},{\"end\":42545,\"start\":42544},{\"end\":42549,\"start\":42546},{\"end\":42560,\"start\":42559},{\"end\":42568,\"start\":42567},{\"end\":42576,\"start\":42575},{\"end\":42942,\"start\":42941},{\"end\":42948,\"start\":42947},{\"end\":42956,\"start\":42955},{\"end\":42962,\"start\":42961},{\"end\":42970,\"start\":42969},{\"end\":42974,\"start\":42971},{\"end\":43358,\"start\":43357},{\"end\":43364,\"start\":43363},{\"end\":43376,\"start\":43372},{\"end\":43385,\"start\":43381},{\"end\":43711,\"start\":43710},{\"end\":43717,\"start\":43716},{\"end\":44062,\"start\":44061},{\"end\":44069,\"start\":44068},{\"end\":44077,\"start\":44076},{\"end\":44086,\"start\":44085},{\"end\":44092,\"start\":44091},{\"end\":44100,\"start\":44099},{\"end\":44494,\"start\":44493},{\"end\":44502,\"start\":44501},{\"end\":44508,\"start\":44507},{\"end\":44519,\"start\":44515},{\"end\":44530,\"start\":44526},{\"end\":44856,\"start\":44855},{\"end\":44864,\"start\":44863},{\"end\":44872,\"start\":44871},{\"end\":44880,\"start\":44879},{\"end\":44890,\"start\":44889},{\"end\":44902,\"start\":44901},{\"end\":45271,\"start\":45270},{\"end\":45518,\"start\":45517},{\"end\":45528,\"start\":45527},{\"end\":45536,\"start\":45535},{\"end\":45870,\"start\":45865},{\"end\":46035,\"start\":46034},{\"end\":46041,\"start\":46040},{\"end\":46048,\"start\":46047},{\"end\":46056,\"start\":46055},{\"end\":46058,\"start\":46057},{\"end\":46066,\"start\":46065},{\"end\":46068,\"start\":46067},{\"end\":46363,\"start\":46362},{\"end\":46371,\"start\":46370},{\"end\":46378,\"start\":46377},{\"end\":46384,\"start\":46383},{\"end\":46391,\"start\":46390},{\"end\":46398,\"start\":46397},{\"end\":46675,\"start\":46674},{\"end\":46687,\"start\":46686},{\"end\":46703,\"start\":46702},{\"end\":47025,\"start\":47024},{\"end\":47036,\"start\":47035},{\"end\":47046,\"start\":47045},{\"end\":47053,\"start\":47052},{\"end\":47069,\"start\":47065},{\"end\":47438,\"start\":47437},{\"end\":47812,\"start\":47811},{\"end\":47821,\"start\":47820},{\"end\":48131,\"start\":48130},{\"end\":48133,\"start\":48132},{\"end\":48388,\"start\":48387},{\"end\":48670,\"start\":48669},{\"end\":48678,\"start\":48677},{\"end\":48686,\"start\":48685},{\"end\":48699,\"start\":48695},{\"end\":48705,\"start\":48704},{\"end\":48711,\"start\":48710},{\"end\":49043,\"start\":49042},{\"end\":49050,\"start\":49049},{\"end\":49064,\"start\":49063},{\"end\":49066,\"start\":49065},{\"end\":49319,\"start\":49318},{\"end\":49501,\"start\":49500},{\"end\":49509,\"start\":49508},{\"end\":49516,\"start\":49515},{\"end\":49522,\"start\":49521},{\"end\":49530,\"start\":49529},{\"end\":49872,\"start\":49871},{\"end\":49880,\"start\":49879},{\"end\":49892,\"start\":49888},{\"end\":49898,\"start\":49897},{\"end\":49909,\"start\":49905},{\"end\":49918,\"start\":49917}]", "bib_author_last_name": "[{\"end\":31929,\"start\":31920},{\"end\":31939,\"start\":31933},{\"end\":31950,\"start\":31943},{\"end\":31959,\"start\":31954},{\"end\":31968,\"start\":31963},{\"end\":31976,\"start\":31972},{\"end\":32259,\"start\":32257},{\"end\":32265,\"start\":32263},{\"end\":32274,\"start\":32269},{\"end\":32281,\"start\":32278},{\"end\":32287,\"start\":32285},{\"end\":32696,\"start\":32687},{\"end\":32705,\"start\":32700},{\"end\":32717,\"start\":32709},{\"end\":32726,\"start\":32721},{\"end\":33047,\"start\":33041},{\"end\":33056,\"start\":33051},{\"end\":33362,\"start\":33356},{\"end\":33374,\"start\":33368},{\"end\":33383,\"start\":33378},{\"end\":33395,\"start\":33387},{\"end\":33557,\"start\":33549},{\"end\":33563,\"start\":33559},{\"end\":33780,\"start\":33773},{\"end\":33791,\"start\":33786},{\"end\":33807,\"start\":33797},{\"end\":34167,\"start\":34155},{\"end\":34175,\"start\":34171},{\"end\":34186,\"start\":34179},{\"end\":34198,\"start\":34190},{\"end\":34464,\"start\":34458},{\"end\":34473,\"start\":34468},{\"end\":34484,\"start\":34477},{\"end\":34493,\"start\":34488},{\"end\":34802,\"start\":34797},{\"end\":34810,\"start\":34806},{\"end\":34818,\"start\":34814},{\"end\":35205,\"start\":35198},{\"end\":35212,\"start\":35209},{\"end\":35221,\"start\":35219},{\"end\":35229,\"start\":35225},{\"end\":35240,\"start\":35233},{\"end\":35508,\"start\":35504},{\"end\":35521,\"start\":35512},{\"end\":35530,\"start\":35525},{\"end\":35541,\"start\":35534},{\"end\":35849,\"start\":35847},{\"end\":35856,\"start\":35853},{\"end\":35865,\"start\":35863},{\"end\":35872,\"start\":35869},{\"end\":35883,\"start\":35879},{\"end\":36255,\"start\":36253},{\"end\":36270,\"start\":36263},{\"end\":36280,\"start\":36274},{\"end\":36288,\"start\":36284},{\"end\":36299,\"start\":36292},{\"end\":36604,\"start\":36600},{\"end\":36610,\"start\":36608},{\"end\":36618,\"start\":36614},{\"end\":37038,\"start\":37034},{\"end\":37046,\"start\":37042},{\"end\":37055,\"start\":37050},{\"end\":37063,\"start\":37059},{\"end\":37444,\"start\":37442},{\"end\":37453,\"start\":37451},{\"end\":37460,\"start\":37457},{\"end\":37467,\"start\":37464},{\"end\":37478,\"start\":37474},{\"end\":37778,\"start\":37773},{\"end\":37786,\"start\":37782},{\"end\":37793,\"start\":37790},{\"end\":37804,\"start\":37801},{\"end\":38180,\"start\":38177},{\"end\":38195,\"start\":38188},{\"end\":38203,\"start\":38199},{\"end\":38214,\"start\":38207},{\"end\":38607,\"start\":38604},{\"end\":38614,\"start\":38611},{\"end\":38623,\"start\":38618},{\"end\":38857,\"start\":38850},{\"end\":38865,\"start\":38861},{\"end\":38876,\"start\":38869},{\"end\":39246,\"start\":39243},{\"end\":39260,\"start\":39254},{\"end\":39272,\"start\":39266},{\"end\":39284,\"start\":39278},{\"end\":39719,\"start\":39716},{\"end\":39958,\"start\":39950},{\"end\":39971,\"start\":39962},{\"end\":39983,\"start\":39975},{\"end\":40266,\"start\":40262},{\"end\":40272,\"start\":40270},{\"end\":40278,\"start\":40276},{\"end\":40286,\"start\":40282},{\"end\":40532,\"start\":40525},{\"end\":40543,\"start\":40538},{\"end\":40559,\"start\":40549},{\"end\":40791,\"start\":40784},{\"end\":40799,\"start\":40795},{\"end\":40810,\"start\":40803},{\"end\":41145,\"start\":41138},{\"end\":41153,\"start\":41149},{\"end\":41164,\"start\":41157},{\"end\":41471,\"start\":41466},{\"end\":41478,\"start\":41475},{\"end\":41489,\"start\":41482},{\"end\":41501,\"start\":41493},{\"end\":41787,\"start\":41783},{\"end\":41800,\"start\":41791},{\"end\":41809,\"start\":41804},{\"end\":41820,\"start\":41813},{\"end\":42162,\"start\":42158},{\"end\":42172,\"start\":42168},{\"end\":42181,\"start\":42176},{\"end\":42542,\"start\":42536},{\"end\":42557,\"start\":42550},{\"end\":42565,\"start\":42561},{\"end\":42573,\"start\":42569},{\"end\":42584,\"start\":42577},{\"end\":42945,\"start\":42943},{\"end\":42953,\"start\":42949},{\"end\":42959,\"start\":42957},{\"end\":42967,\"start\":42963},{\"end\":42978,\"start\":42975},{\"end\":43361,\"start\":43359},{\"end\":43370,\"start\":43365},{\"end\":43379,\"start\":43377},{\"end\":43390,\"start\":43386},{\"end\":43714,\"start\":43712},{\"end\":43725,\"start\":43718},{\"end\":44066,\"start\":44063},{\"end\":44074,\"start\":44070},{\"end\":44083,\"start\":44078},{\"end\":44089,\"start\":44087},{\"end\":44097,\"start\":44093},{\"end\":44105,\"start\":44101},{\"end\":44499,\"start\":44495},{\"end\":44505,\"start\":44503},{\"end\":44513,\"start\":44509},{\"end\":44524,\"start\":44520},{\"end\":44533,\"start\":44531},{\"end\":44861,\"start\":44857},{\"end\":44869,\"start\":44865},{\"end\":44877,\"start\":44873},{\"end\":44887,\"start\":44881},{\"end\":44899,\"start\":44891},{\"end\":44911,\"start\":44903},{\"end\":45276,\"start\":45272},{\"end\":45525,\"start\":45519},{\"end\":45533,\"start\":45529},{\"end\":45544,\"start\":45537},{\"end\":45874,\"start\":45871},{\"end\":46038,\"start\":46036},{\"end\":46045,\"start\":46042},{\"end\":46053,\"start\":46049},{\"end\":46063,\"start\":46059},{\"end\":46075,\"start\":46069},{\"end\":46368,\"start\":46364},{\"end\":46375,\"start\":46372},{\"end\":46381,\"start\":46379},{\"end\":46388,\"start\":46385},{\"end\":46395,\"start\":46392},{\"end\":46402,\"start\":46399},{\"end\":46684,\"start\":46676},{\"end\":46700,\"start\":46688},{\"end\":46707,\"start\":46704},{\"end\":47033,\"start\":47026},{\"end\":47043,\"start\":47037},{\"end\":47050,\"start\":47047},{\"end\":47063,\"start\":47054},{\"end\":47074,\"start\":47070},{\"end\":47446,\"start\":47439},{\"end\":47818,\"start\":47813},{\"end\":47829,\"start\":47822},{\"end\":48140,\"start\":48134},{\"end\":48391,\"start\":48389},{\"end\":48675,\"start\":48671},{\"end\":48683,\"start\":48679},{\"end\":48693,\"start\":48687},{\"end\":48702,\"start\":48700},{\"end\":48708,\"start\":48706},{\"end\":48719,\"start\":48712},{\"end\":49047,\"start\":49044},{\"end\":49061,\"start\":49051},{\"end\":49071,\"start\":49067},{\"end\":49324,\"start\":49320},{\"end\":49506,\"start\":49502},{\"end\":49513,\"start\":49510},{\"end\":49519,\"start\":49517},{\"end\":49527,\"start\":49523},{\"end\":49534,\"start\":49531},{\"end\":49877,\"start\":49873},{\"end\":49886,\"start\":49881},{\"end\":49895,\"start\":49893},{\"end\":49903,\"start\":49899},{\"end\":49915,\"start\":49910},{\"end\":49922,\"start\":49919}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":8912263},\"end\":32198,\"start\":31878},{\"attributes\":{\"id\":\"b1\"},\"end\":32619,\"start\":32200},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":2259236},\"end\":32972,\"start\":32621},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":1486522},\"end\":33274,\"start\":32974},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":53349572},\"end\":33547,\"start\":33276},{\"attributes\":{\"id\":\"b5\"},\"end\":33705,\"start\":33549},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":15842367},\"end\":34061,\"start\":33707},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":7329208},\"end\":34401,\"start\":34063},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":12447228},\"end\":34678,\"start\":34403},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":9652770},\"end\":35110,\"start\":34680},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":17623309},\"end\":35438,\"start\":35112},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":3351683},\"end\":35780,\"start\":35440},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":218674010},\"end\":36176,\"start\":35782},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":51881368},\"end\":36486,\"start\":36178},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":413731},\"end\":36938,\"start\":36488},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":199442594},\"end\":37363,\"start\":36940},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":8182535},\"end\":37733,\"start\":37365},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":195513288},\"end\":38089,\"start\":37735},{\"attributes\":{\"doi\":\"10.1109/TPAMI.2019.2948011\",\"id\":\"b18\",\"matched_paper_id\":204833683},\"end\":38536,\"start\":38091},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":6137921},\"end\":38777,\"start\":38538},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":2878708},\"end\":39176,\"start\":38779},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":15765187},\"end\":39610,\"start\":39178},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":52953674},\"end\":39889,\"start\":39612},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":263874691},\"end\":40193,\"start\":39891},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":27750744},\"end\":40460,\"start\":40195},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":10124817},\"end\":40720,\"start\":40462},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":35488672},\"end\":41062,\"start\":40722},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":3418579},\"end\":41402,\"start\":41064},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":14882677},\"end\":41721,\"start\":41404},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":659803},\"end\":42097,\"start\":41723},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":2642044},\"end\":42465,\"start\":42099},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":25120102},\"end\":42867,\"start\":42467},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":931642},\"end\":43292,\"start\":42869},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":85518390},\"end\":43661,\"start\":43294},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":201645098},\"end\":43961,\"start\":43663},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":211544924},\"end\":44464,\"start\":43963},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":208138930},\"end\":44815,\"start\":44466},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":5636055},\"end\":45207,\"start\":44817},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":11371972},\"end\":45444,\"start\":45209},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":6724907},\"end\":45820,\"start\":45446},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":14113767},\"end\":45988,\"start\":45822},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":6881166},\"end\":46340,\"start\":45990},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":53088424},\"end\":46639,\"start\":46342},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":854969},\"end\":46966,\"start\":46641},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":4555207},\"end\":47372,\"start\":46968},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":2375110},\"end\":47715,\"start\":47374},{\"attributes\":{\"doi\":\"arXiv:1502.03167\",\"id\":\"b46\"},\"end\":48044,\"start\":47717},{\"attributes\":{\"doi\":\"arXiv:1704.04861\",\"id\":\"b47\"},\"end\":48341,\"start\":48046},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":204970740},\"end\":48614,\"start\":48343},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":57246310},\"end\":49001,\"start\":48616},{\"attributes\":{\"doi\":\"arXiv:1506.04579\",\"id\":\"b50\"},\"end\":49245,\"start\":49003},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":51605528},\"end\":49467,\"start\":49247},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":5299559},\"end\":49802,\"start\":49469},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":195510077},\"end\":50256,\"start\":49804}]", "bib_title": "[{\"end\":31916,\"start\":31878},{\"end\":32683,\"start\":32621},{\"end\":33037,\"start\":32974},{\"end\":33352,\"start\":33276},{\"end\":33766,\"start\":33707},{\"end\":34151,\"start\":34063},{\"end\":34454,\"start\":34403},{\"end\":34793,\"start\":34680},{\"end\":35190,\"start\":35112},{\"end\":35498,\"start\":35440},{\"end\":35843,\"start\":35782},{\"end\":36249,\"start\":36178},{\"end\":36596,\"start\":36488},{\"end\":37030,\"start\":36940},{\"end\":37438,\"start\":37365},{\"end\":37769,\"start\":37735},{\"end\":38173,\"start\":38091},{\"end\":38600,\"start\":38538},{\"end\":38842,\"start\":38779},{\"end\":39239,\"start\":39178},{\"end\":39712,\"start\":39612},{\"end\":39946,\"start\":39891},{\"end\":40258,\"start\":40195},{\"end\":40518,\"start\":40462},{\"end\":40776,\"start\":40722},{\"end\":41130,\"start\":41064},{\"end\":41462,\"start\":41404},{\"end\":41777,\"start\":41723},{\"end\":42154,\"start\":42099},{\"end\":42532,\"start\":42467},{\"end\":42939,\"start\":42869},{\"end\":43355,\"start\":43294},{\"end\":43708,\"start\":43663},{\"end\":44059,\"start\":43963},{\"end\":44491,\"start\":44466},{\"end\":44853,\"start\":44817},{\"end\":45268,\"start\":45209},{\"end\":45515,\"start\":45446},{\"end\":45863,\"start\":45822},{\"end\":46032,\"start\":45990},{\"end\":46360,\"start\":46342},{\"end\":46672,\"start\":46641},{\"end\":47022,\"start\":46968},{\"end\":47435,\"start\":47374},{\"end\":48385,\"start\":48343},{\"end\":48667,\"start\":48616},{\"end\":49316,\"start\":49247},{\"end\":49498,\"start\":49469},{\"end\":49869,\"start\":49804}]", "bib_author": "[{\"end\":31931,\"start\":31918},{\"end\":31941,\"start\":31931},{\"end\":31952,\"start\":31941},{\"end\":31961,\"start\":31952},{\"end\":31970,\"start\":31961},{\"end\":31978,\"start\":31970},{\"end\":32261,\"start\":32255},{\"end\":32267,\"start\":32261},{\"end\":32276,\"start\":32267},{\"end\":32283,\"start\":32276},{\"end\":32289,\"start\":32283},{\"end\":32698,\"start\":32685},{\"end\":32707,\"start\":32698},{\"end\":32719,\"start\":32707},{\"end\":32728,\"start\":32719},{\"end\":33049,\"start\":33039},{\"end\":33058,\"start\":33049},{\"end\":33364,\"start\":33354},{\"end\":33376,\"start\":33364},{\"end\":33385,\"start\":33376},{\"end\":33397,\"start\":33385},{\"end\":33559,\"start\":33549},{\"end\":33565,\"start\":33559},{\"end\":33782,\"start\":33768},{\"end\":33793,\"start\":33782},{\"end\":33809,\"start\":33793},{\"end\":34169,\"start\":34153},{\"end\":34177,\"start\":34169},{\"end\":34188,\"start\":34177},{\"end\":34200,\"start\":34188},{\"end\":34466,\"start\":34456},{\"end\":34475,\"start\":34466},{\"end\":34486,\"start\":34475},{\"end\":34495,\"start\":34486},{\"end\":34804,\"start\":34795},{\"end\":34812,\"start\":34804},{\"end\":34820,\"start\":34812},{\"end\":35207,\"start\":35192},{\"end\":35214,\"start\":35207},{\"end\":35223,\"start\":35214},{\"end\":35231,\"start\":35223},{\"end\":35242,\"start\":35231},{\"end\":35510,\"start\":35500},{\"end\":35523,\"start\":35510},{\"end\":35532,\"start\":35523},{\"end\":35543,\"start\":35532},{\"end\":35851,\"start\":35845},{\"end\":35858,\"start\":35851},{\"end\":35867,\"start\":35858},{\"end\":35874,\"start\":35867},{\"end\":35885,\"start\":35874},{\"end\":36257,\"start\":36251},{\"end\":36272,\"start\":36257},{\"end\":36282,\"start\":36272},{\"end\":36290,\"start\":36282},{\"end\":36301,\"start\":36290},{\"end\":36606,\"start\":36598},{\"end\":36612,\"start\":36606},{\"end\":36620,\"start\":36612},{\"end\":37040,\"start\":37032},{\"end\":37048,\"start\":37040},{\"end\":37057,\"start\":37048},{\"end\":37065,\"start\":37057},{\"end\":37446,\"start\":37440},{\"end\":37455,\"start\":37446},{\"end\":37462,\"start\":37455},{\"end\":37469,\"start\":37462},{\"end\":37480,\"start\":37469},{\"end\":37780,\"start\":37771},{\"end\":37788,\"start\":37780},{\"end\":37795,\"start\":37788},{\"end\":37806,\"start\":37795},{\"end\":38182,\"start\":38175},{\"end\":38197,\"start\":38182},{\"end\":38205,\"start\":38197},{\"end\":38216,\"start\":38205},{\"end\":38609,\"start\":38602},{\"end\":38616,\"start\":38609},{\"end\":38625,\"start\":38616},{\"end\":38859,\"start\":38844},{\"end\":38867,\"start\":38859},{\"end\":38878,\"start\":38867},{\"end\":39248,\"start\":39241},{\"end\":39262,\"start\":39248},{\"end\":39274,\"start\":39262},{\"end\":39286,\"start\":39274},{\"end\":39721,\"start\":39714},{\"end\":39960,\"start\":39948},{\"end\":39973,\"start\":39960},{\"end\":39985,\"start\":39973},{\"end\":40268,\"start\":40260},{\"end\":40274,\"start\":40268},{\"end\":40280,\"start\":40274},{\"end\":40288,\"start\":40280},{\"end\":40534,\"start\":40520},{\"end\":40545,\"start\":40534},{\"end\":40561,\"start\":40545},{\"end\":40793,\"start\":40778},{\"end\":40801,\"start\":40793},{\"end\":40812,\"start\":40801},{\"end\":41147,\"start\":41132},{\"end\":41155,\"start\":41147},{\"end\":41166,\"start\":41155},{\"end\":41473,\"start\":41464},{\"end\":41480,\"start\":41473},{\"end\":41491,\"start\":41480},{\"end\":41503,\"start\":41491},{\"end\":41789,\"start\":41779},{\"end\":41802,\"start\":41789},{\"end\":41811,\"start\":41802},{\"end\":41822,\"start\":41811},{\"end\":42164,\"start\":42156},{\"end\":42174,\"start\":42164},{\"end\":42183,\"start\":42174},{\"end\":42544,\"start\":42534},{\"end\":42559,\"start\":42544},{\"end\":42567,\"start\":42559},{\"end\":42575,\"start\":42567},{\"end\":42586,\"start\":42575},{\"end\":42947,\"start\":42941},{\"end\":42955,\"start\":42947},{\"end\":42961,\"start\":42955},{\"end\":42969,\"start\":42961},{\"end\":42980,\"start\":42969},{\"end\":43363,\"start\":43357},{\"end\":43372,\"start\":43363},{\"end\":43381,\"start\":43372},{\"end\":43392,\"start\":43381},{\"end\":43716,\"start\":43710},{\"end\":43727,\"start\":43716},{\"end\":44068,\"start\":44061},{\"end\":44076,\"start\":44068},{\"end\":44085,\"start\":44076},{\"end\":44091,\"start\":44085},{\"end\":44099,\"start\":44091},{\"end\":44107,\"start\":44099},{\"end\":44501,\"start\":44493},{\"end\":44507,\"start\":44501},{\"end\":44515,\"start\":44507},{\"end\":44526,\"start\":44515},{\"end\":44535,\"start\":44526},{\"end\":44863,\"start\":44855},{\"end\":44871,\"start\":44863},{\"end\":44879,\"start\":44871},{\"end\":44889,\"start\":44879},{\"end\":44901,\"start\":44889},{\"end\":44913,\"start\":44901},{\"end\":45278,\"start\":45270},{\"end\":45527,\"start\":45517},{\"end\":45535,\"start\":45527},{\"end\":45546,\"start\":45535},{\"end\":45876,\"start\":45865},{\"end\":46040,\"start\":46034},{\"end\":46047,\"start\":46040},{\"end\":46055,\"start\":46047},{\"end\":46065,\"start\":46055},{\"end\":46077,\"start\":46065},{\"end\":46370,\"start\":46362},{\"end\":46377,\"start\":46370},{\"end\":46383,\"start\":46377},{\"end\":46390,\"start\":46383},{\"end\":46397,\"start\":46390},{\"end\":46404,\"start\":46397},{\"end\":46686,\"start\":46674},{\"end\":46702,\"start\":46686},{\"end\":46709,\"start\":46702},{\"end\":47035,\"start\":47024},{\"end\":47045,\"start\":47035},{\"end\":47052,\"start\":47045},{\"end\":47065,\"start\":47052},{\"end\":47076,\"start\":47065},{\"end\":47448,\"start\":47437},{\"end\":47820,\"start\":47811},{\"end\":47831,\"start\":47820},{\"end\":48142,\"start\":48130},{\"end\":48393,\"start\":48387},{\"end\":48677,\"start\":48669},{\"end\":48685,\"start\":48677},{\"end\":48695,\"start\":48685},{\"end\":48704,\"start\":48695},{\"end\":48710,\"start\":48704},{\"end\":48721,\"start\":48710},{\"end\":49049,\"start\":49042},{\"end\":49063,\"start\":49049},{\"end\":49073,\"start\":49063},{\"end\":49326,\"start\":49318},{\"end\":49508,\"start\":49500},{\"end\":49515,\"start\":49508},{\"end\":49521,\"start\":49515},{\"end\":49529,\"start\":49521},{\"end\":49536,\"start\":49529},{\"end\":49879,\"start\":49871},{\"end\":49888,\"start\":49879},{\"end\":49897,\"start\":49888},{\"end\":49905,\"start\":49897},{\"end\":49917,\"start\":49905},{\"end\":49924,\"start\":49917}]", "bib_venue": "[{\"end\":32011,\"start\":31978},{\"end\":32253,\"start\":32200},{\"end\":32766,\"start\":32728},{\"end\":33096,\"start\":33058},{\"end\":33401,\"start\":33397},{\"end\":33613,\"start\":33565},{\"end\":33847,\"start\":33809},{\"end\":34210,\"start\":34200},{\"end\":34511,\"start\":34495},{\"end\":34858,\"start\":34820},{\"end\":35252,\"start\":35242},{\"end\":35581,\"start\":35543},{\"end\":35935,\"start\":35885},{\"end\":36311,\"start\":36301},{\"end\":36670,\"start\":36620},{\"end\":37110,\"start\":37065},{\"end\":37518,\"start\":37480},{\"end\":37864,\"start\":37806},{\"end\":38295,\"start\":38242},{\"end\":38635,\"start\":38625},{\"end\":38932,\"start\":38878},{\"end\":39345,\"start\":39286},{\"end\":39731,\"start\":39721},{\"end\":40014,\"start\":39985},{\"end\":40304,\"start\":40288},{\"end\":40571,\"start\":40561},{\"end\":40853,\"start\":40812},{\"end\":41204,\"start\":41166},{\"end\":41531,\"start\":41503},{\"end\":41868,\"start\":41822},{\"end\":42237,\"start\":42183},{\"end\":42627,\"start\":42586},{\"end\":43034,\"start\":42980},{\"end\":43437,\"start\":43392},{\"end\":43772,\"start\":43727},{\"end\":44165,\"start\":44107},{\"end\":44593,\"start\":44535},{\"end\":44967,\"start\":44913},{\"end\":45297,\"start\":45278},{\"end\":45592,\"start\":45546},{\"end\":45886,\"start\":45876},{\"end\":46123,\"start\":46077},{\"end\":46449,\"start\":46404},{\"end\":46761,\"start\":46709},{\"end\":47126,\"start\":47076},{\"end\":47502,\"start\":47448},{\"end\":47809,\"start\":47717},{\"end\":48128,\"start\":48046},{\"end\":48438,\"start\":48393},{\"end\":48767,\"start\":48721},{\"end\":49040,\"start\":49003},{\"end\":49337,\"start\":49326},{\"end\":49590,\"start\":49536},{\"end\":49982,\"start\":49924},{\"end\":33407,\"start\":33403},{\"end\":33881,\"start\":33849},{\"end\":34216,\"start\":34212},{\"end\":34892,\"start\":34860},{\"end\":35258,\"start\":35254},{\"end\":35981,\"start\":35937},{\"end\":36317,\"start\":36313},{\"end\":36716,\"start\":36672},{\"end\":37151,\"start\":37112},{\"end\":37918,\"start\":37866},{\"end\":38641,\"start\":38637},{\"end\":38982,\"start\":38934},{\"end\":39400,\"start\":39347},{\"end\":39737,\"start\":39733},{\"end\":40577,\"start\":40573},{\"end\":40890,\"start\":40855},{\"end\":41555,\"start\":41533},{\"end\":41910,\"start\":41870},{\"end\":42287,\"start\":42239},{\"end\":42664,\"start\":42629},{\"end\":43084,\"start\":43036},{\"end\":43478,\"start\":43439},{\"end\":43813,\"start\":43774},{\"end\":44219,\"start\":44167},{\"end\":44647,\"start\":44595},{\"end\":45017,\"start\":44969},{\"end\":45634,\"start\":45594},{\"end\":45892,\"start\":45888},{\"end\":46165,\"start\":46125},{\"end\":46490,\"start\":46451},{\"end\":46809,\"start\":46763},{\"end\":47172,\"start\":47128},{\"end\":47552,\"start\":47504},{\"end\":48479,\"start\":48440},{\"end\":48809,\"start\":48769},{\"end\":49344,\"start\":49339},{\"end\":49640,\"start\":49592},{\"end\":50036,\"start\":49984}]"}}}, "year": 2023, "month": 12, "day": 17}