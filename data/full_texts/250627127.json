{"id": 250627127, "updated": "2023-10-05 12:17:51.67", "metadata": {"title": "MAD for Robust Reinforcement Learning in Machine Translation", "authors": "[{\"first\":\"Domenic\",\"last\":\"Donato\",\"middle\":[]},{\"first\":\"Lei\",\"last\":\"Yu\",\"middle\":[]},{\"first\":\"Wang\",\"last\":\"Ling\",\"middle\":[]},{\"first\":\"Chris\",\"last\":\"Dyer\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "We introduce a new distributed policy gradient algorithm and show that it outperforms existing reward-aware training procedures such as REINFORCE, minimum risk training (MRT) and proximal policy optimization (PPO) in terms of training stability and generalization performance when optimizing machine translation models. Our algorithm, which we call MAD (on account of using the mean absolute deviation in the importance weighting calculation), has distributed data generators sampling multiple candidates per source sentence on worker nodes, while a central learner updates the policy. MAD depends crucially on two variance reduction strategies: (1) a conditional reward normalization method that ensures each source sentence has both positive and negative reward translation examples and (2) a new robust importance weighting scheme that acts as a conditional entropy regularizer. Experiments on a variety of translation tasks show that policies learned using the MAD algorithm perform very well when using both greedy decoding and beam search, and that the learned policies are sensitive to the specific reward used during training.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2207.08583", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2207-08583", "doi": "10.48550/arxiv.2207.08583"}}, "content": {"source": {"pdf_hash": "c009a959dd236c162e51703e3bfd4d2b0b751c17", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2207.08583v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "fdf63c05e65e939f8e96623aafc1e31d35e20403", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/c009a959dd236c162e51703e3bfd4d2b0b751c17.txt", "contents": "\nMAD for Robust Reinforcement Learning in Machine Translation\n\n\nDomenic Donato \nLei Yu \nWang Ling \nChris Dyer \nMAD for Robust Reinforcement Learning in Machine Translation\n\nWe introduce a new distributed policy gradient algorithm and show that it outperforms existing reward-aware training procedures such as REIN-FORCE, minimum risk training (MRT) and proximal policy optimization (PPO) in terms of training stability and generalization performance when optimizing machine translation models. Our algorithm, which we call MAD (on account of using the mean absolute deviation in the importance weighting calculation), has distributed data generators sampling multiple candidates per source sentence on worker nodes, while a central learner updates the policy. MAD depends crucially on two variance reduction strategies: (1) a conditional reward normalization method that ensures each source sentence has both positive and negative reward translation examples and (2) a new robust importance weighting scheme that acts as a conditional entropy regularizer. Experiments on a variety of translation tasks show that policies learned using the MAD algorithm perform very well when using both greedy decoding and beam search, and that the learned policies are sensitive to the specific reward used during training.\n\nIntroduction\n\nThere is increasing interest in fine-tuning conditional language models on the basis of feedback from task-specific reward models or similarity functions that compare to humangenerated reference outputs rather than relying exclusively on supervised learning (Stiennon et al., 2020;Ziegler et al., 2019;Wu et al., 2018;Paulus et al., 2018;Rennie et al., 2017;Ranzato et al., 2016). Maximising sequence level rewards has several advantages. First, it avoids the apparent conflict between the intuitive importance of \"getting the full sequence right\" in generation problems and the more conventional token-level cross entropy loss. Second, since a policy trained to maximize rewards is supervised with its own outputs-both good and bad ones-it mitigates issues arising from \"exposure bias,\" in which a learned policy that has been trained only on correct examples has no experience recovering from errors and therefore performs poorly at test time (Ranzato et al., 2016). Third, feedback from (learned) rewards can be a cost-effective strategy for incorporating human preferences about how a system should behave (Stiennon et al., 2020;Christiano et al., 2017).\n\nUnfortunately, fine-tuning policies for generating in complex output spaces, such as language, on the basis of sparse rewards is challenging. Estimating and debugging reliable auxiliary critic/value functions that are needed by many learning algorithms is challenging (Wu et al., 2018;Bahdanau et al., 2017;Nguyen et al., 2017), and commonly used average or batch-level reward baselines (Kreutzer et al., 2017) are poor variance reducers since they are independent of the input, and input difficulty is a strong determinant of reward magnitude.\n\nIn this paper, we propose a new distributed policy gradient algorithm ( \u00a72) for fine-tuning translation models that addresses these issues. The distributed setup lets us use modest computation to obtain simple and effective empirical reward baselines (Rennie et al., 2017) rather than using inappropriate batch-level statistics or relying on brittle auxiliary value models. Our proposed algorithm has two components designed to make learning from the reward signal more effective: we employ a sampling and reward normalization strategy that encourage batches to contain a mix of both positive and negative rewards for each source sentence and, second, an importance weighting strategy that encourages the algorithm to pay attention to trajectories that are slightly off the current policy. Thus, our algorithm learns from trajectories that are already relatively likely under the current policy (meaning any updates to the policy will be relatively small), while also encouraging continued exploration throughout training by down-weighting trajectories where the model is very confident. This enables the algorithm to make large improvements in reward while taking small, conservative steps to change the behaviour and, also, helps slow down the rate of policy collapse letting the model obtain continued performance improvements over many training steps.\n\nThe policies learned using our MAD algorithm produce high-quality translations, even when using greedy decoding. In our main experiments ( \u00a73), we use sentence BLEU as the reward and find that the average improvement on heldout test sets over the initial cross entropy trained model is 2.0 BLEU. We also find that the models are less sensitive to the beam search hyperparameters beam size and length normalization. This means we do not need to optimize the length normalization for each dataset and observe almost no performance degradation with larger beam sizes. We confirm that the algorithm learns different policies depending on the reward function used during optimization, with the resulting policies showing reward-specific improvements on held-out data. Finally, we carry out a careful empirical analysis ( \u00a74) to better understand the impact and role the various components of the algorithm have on training dynamics and generalization performance.\n\n\nAlgorithm\n\nOur algorithm consists of workers generating training trajectories and rewards, in parallel, from a slightly out-of-date copy of the policy, \u03b8 old , and a central learner that updates the current policy, \u03b8. At the beginning of training, both \u03b8 and \u03b8 old are initialized from a pretrained cross entropy model. The data generation algorithm is shown Alg. 1 and the learner in Alg. 2. The learning algorithm has three core components: sampling from a range of temperatures ( \u00a72.1), conditional reward normalization on the basis of empirical means and variances ( \u00a72.2), and a novel robust importance weighting strategy that focuses learning efforts on samples that are slightly off policy ( \u00a72.3). We discuss each of these components in turn.\n\n\nMulti-Temperature Sampling\n\nTo obtain suitably diverse candidates to learn from, it is conventional to add a temperature hyperparameter used to generate samples (Shen et al., 2016;Papini et al., 2020). We identify two problems with this. First, there is the practical matter of needing to select a temperature in order to obtain good performance. Second, it is widely observed that policy gradient algorithms result in increasingly peaked distributions as training progresses (Kiegeland & Kreutzer, 2021;Choshen et al., 2020;Rennie et al., 2017), meaning that the amount of \"exploration\" being considered by the algorithm decreases over time. While some RL tasks are interested in maximising total accumulated returns over time (meaning that \"exploitation\" is important), we rather seek to learn a policy that is capable of behaving as intelligently as possible in states that are not encountered during a training phase, and therefore, we seek an exploration-heavy sampling strategy. To avoid the difficulties with drifting entropy, we use a simple approach of generating populations of samples at  \nT \u2190 Tmin + (i \u2212 1) \u00d7 (Tmax \u2212 Tmin)/(N \u2212 1) 7: y i \u2190 SAMPLE(x, \u03b8 old , T ) 8: qi \u2190 log p(y i | xi; \u03b8 old ) 9: ri \u2190 \u2206(y i ,y1: function LEARN(S, \u03b7, \u03b8CE) 2: \u03b8 \u2190 \u03b8CE 3: for i \u2208 [1, S] do 4: x, y, q, r, v \u2190 DEQUEUE() 5: p \u2190 log p(y | x; \u03b8) 6: u \u2190 exp(p \u2212 q) 7: \u03b1 \u2190 SG(min{u \u00d7 v, 2}) 8:\nL \u2190 \u03b1 \u00d7 r \u00d7 log p(y | x; dropout(\u03b8)) 9:\n\n\u03b8 \u2190 \u03b8 + \u03b7 \u00d7 \u2202L \u2202\u03b8 10: end for 11: end function several different temperatures.\n\nConcretely our data generation algorithm (lines 5-7 of Alg. 1) begins by sampling N translations for the current policy and source sentence x. Each sample is generated using a different temperature that is determined by finding equally spaced values 1 between the interval [T min , T max ]. Duplicate translations are removed. The process we use is:\nY x = UNIQUE({SAMPLE(x, \u03b8, t); t \u2208 T }) where T = {T min + \u03b4 t \u00b7 (i \u2212 1); i \u2208 {1, . . . , N }} \u03b4 t = T max \u2212 T min N \u2212 1 .\n\nConditional Reward Normalization\n\nReward normalization is a well known variance reduction technique for policy gradient methods, with the simplest ver-sion being to subtract a constant from the reward (Williams, 1992). Other methods have been proposed such as subtracting a model baseline reward (Weaver & Tao, 2001), the running average of the rewards seen during training (Kreutzer et al., 2017), or z-scoring the rewards in a training batch (Stiennon et al., 2020). As demonstrated by Kiegeland & Kreutzer (2021), using the running reward average b helps to reduce variance when REINFORCE is applied to translation.\n\nWhile empirically effective, these baselines explain less reward variation than we might hope. We note that the difficulty of generating a good translation is strongly dependent on the intrinsic difficulty of the source sentence, not merely the current policy. Since the usual reward normalization methods do not take this dependency into account, this results in a bias toward giving difficult sentences negative rewards and easier sentences positive rewards, leading to less stable learning (see \u00a74.2 and Appendix A.2).\n\nWe therefore use a standardization method that is conditioned on the source sentence. We take the set of translations for source sentence x and obtain a vector of rewards\nr i = \u2206(y i , y ref ) \u2200 i \u2208 [1, |Y x |]\nwhere \u2206(y, y ref ) 2 is a scalar valued reward indicating how well y communicates the contents of the reference translation y ref . We then standardize these rewards by removing the mean and dividing by the standard deviation (lines 11-19 of Alg. 1),\nr i = (r i \u2212 \u00b5 r )/\u03c3 r \u2200 i \u2208 [1, |Y x |], where \u00b5 r = 1 |Y x | r i , \u03c3 r = 1 |Y x | (r i \u2212 \u00b5 r ) 2 .\nThis ensures that every source sentence, irrespective of its translation difficulty, has examples with positive and negative rewards.\n\nIn contrast, the standard reward used for the PPO algorithm is the z-scored reward of the training batch, r i = (r i \u2212 \u00b5 r )/ \u03c3 r \u2200 i \u2208 [1, |B|], where B is the training batch with randomly sampled (x, y) examples and \u00b5 r and \u03c3 r are respectively the mean and standard deviation of the rewards in B.\n\n\nMAD Importance Weights\n\nA key feature of our algorithm-and the one that gives it its name-is the use of a new importance weighting scheme for deciding which sampled trajectories are most valuable for updating the policy and which others should be downweighted. For trajectory i, our full importance sampling correction is\nw i = min{u i \u00b7 v i , 2}.(1)\nWe explain u i and v i below and note that truncating importance weights is a standard variance reduction strategy (Cortes et al., 2010;Schulman et al., 2017).\n\nThe first term in Eq. 1, u i , is the standard importance weighting ratio (Precup et al., 2000;Pang & He, 2021) to deal with the fact that in a distributed setup, data is generated from a stale policy:\nu i = exp(p i \u2212 q i ), where p i = log p(y i | x; \u03b8), q i = log p(y i | x; \u03b8 old ).\nThe second term in Eq. 1, v i , encourages continued exploration during training by having the learner pay attention to samples that are \"relatively likely\" under the current policy (as approximated by q, since we want the calculation to be carried out on the worker nodes, and p is only available on the central learner). We operationalize the notion of \"relatively likely\" as something that is near to the median 3 probability under q of the elements in Y x , using the exponentiated negative median absolute deviation (MAD; lines [13][14][15][16][17][18][19]:\nv i = exp(\u2212|q i \u2212\u03bc q |/\u03c3 q ) \u00b5 q = MEDIAN(q) \u03c3 q = MEDIAN(|q \u2212\u03bc q |).\nWhy do we want to concentrate learning on these relatively likely trajectories, rather than perhaps other trajectories that have even higher (or lower) rewards? First, down-weighting very unlikely samples is a strategy for reducing the gradient variance, since gradients associated with smaller changes to the policy will require, on average, less significant updates to the network. This is advantageous since smaller gradient variance can lead to better generalization error (Kreutzer et al., 2017). Second, Choshen et al. (2020) provide evidence that RL algorithms cause the policy to become more peaked during training which means that sampling diversity decreases and, thus, the amount exploration as training progresses is reduced ( \u00a74.3). Since we seek an explorationheavy RL algorithm, we focus learning effort on instances which are \"in reach\" of the current policy but not at the current policy's mode.\n\n\nExperiments\n\n\nDatasets\n\nWe run our model along with all the baselines on a total of 9 dataset-language direction translation tasks. See Appendix C for the detailed description of dataset, preprocessing, and tokenization. \n\n\nTraining\n\nFor each task, we pretrain a sequence-to-sequence transformer model (Vaswani et al., 2017), using a word level cross entropy loss, until convergence. We refer to this model as the Cross Entropy (CE) model. All treatments for a task are initialized using the same CE checkpoint, which is the checkpoint that had the highest development set BLEU. The treatments are trained using the same training/development sets for a total of 200K steps with early stopping if the average development BLEU is not improved for the last 20K steps. In the case of PPO and MAD, every 20 training steps the learning node saves a checkpoint which the workers and evaluators load asynchronously. We use a global batch size of 256 training examples per step. See Appendix B for detailed hyperparameter configurations and Appendix D for implementation details. Figure 1 gives the primary RL objectives we compare. These are: the REINFORCE algorithm (Williams, 1992) using a moving average baseline (Weaver & Tao, 2001), a proximal policy optimization algorithm (Schulman et al., 2017, PPO), minimium risk training (Shen et al., 2016, MRT), and our algorithm MAD. Since REINFORCE and MRT are onpolicy algorithms, we optimize these objectives on a single worker; the distributed infrastructure is only used for MAD and PPO.\n\n\nCompared RL Objectives\n\n\nEvaluation and Hyperparameters\n\nWe use the sacreBLEU script (Post, 2018) 4 with detokenized model hypothesis and the original references. 5 When running on the test set, we select the checkpoint that had the highest development set BLEU under greedy decoding.\n\nThe important hyperparameters of our algorithm are N , T min , and T max . During preliminary experiments, we swept over N and found that anything in the range of [8,24] provided similar results. We fixed it at N = 12 for all the experiments reported in this work. We found that the optimal temperature range was dataset dependent ( \u00a74.5) so performed sweeps for all the models in order for fair comparison. For MAD, we swept over 4 interval ranges,\n[T min , T max ] \u2208 {[0.2, 0.6], [0.4, 0.8], [0.6, 1.0], [0.8, 1.2]},\nwhile for the baselines we found the single best temperature by sweeping over 10 temperatures in the interval [0.2, 1.2] using increments of 0.1. See Table 3 in Appendix B for selected temperatures. Table 1 presents the performance of our algorithm compared to other objectives ( \u00a73.3) on 9 machine translation tasks. We compare these models on both greedy decoding and beam search. As can be seen, our reinforcement learning algorithm outperforms REINFORCE, PPO, and MRT by an Figure 1. Sequence level losses used by the algorithms evaluated in this paper. x is the source sentence and y is a sampled translation. Here r = \u2206(y, y ref ) is the reward, which is sentence BLEU; r and r are different ways of normalizing said reward. The policy sequence probability is p(y | x) is while u and v are importance weightings that depend on the this value along with the behaviour sequence probability, q(y | x). See \u00a72 for full explanation.\n\n\nMain Results\nL REINFORCE = (r \u2212 b) \u00b7 log p(y | x) L PPO = min{u \u00b7 r, clip(u, 1 \u2212 , 1 + ) \u00b7 r} L MRT = y\u2208Yx r \u00b7 p(y | x) y \u2208Yx p(y | x) L MAD = min{u \u00b7 v, 2} \u00b7 r \u00b7 log p(y | x)\naverage of 1.0 BLEU when using greedy decoding. With beam search decoding, MAD outperforms the strong MRT baseline by 0.5 BLEU on average.\n\nWe observe that models trained using the MAD algorithm display a smaller gap in performance between greedy decoding and beam search decoding. The explanation for this is related to obtaining more peaked and possibly smoother output distributions (Dabre & Fujita, 2020). The benefit of this is that it reduces the need for per dataset beam search hyperparameter tuning. Specifically, Meister et al. (2020) show that, when using beam search, it is necessary to tune a length normalization term, \u03b1, for each dataset in order to prevent the model from generating sentences that are too short or too long. Since the MAD algorithm results in models that are less sensitive to \u03b1 we are able to use a uniform \u03b1 on all the datasets without sacrificing substantial performance.\n\nWe provide some additional remarks along with more test set results using larger beams and other decoding hyperparameters in Appendix A. The test results in Table 8 show that MAD's performance is fairly insensitive to these hyperparameters in marked contrast to CE (Meister et al., 2020).\n\n\nAnalysis and Ablation Experiments\n\nIn this section we investigate various aspects of the MAD algorithm. We use a variety of datasets as to not overfit our analysis to the quirks of a specific one. We start off with a full ablation study ( \u00a74.1) followed by a closer inspection of the effects of conditional reward normalization ( \u00a74.2) and the MAD importance weights ( \u00a74.3). After this, we move on to looking at training stability ( \u00a74.4) and MAD's temperature range hyperparameter sensitivity ( \u00a74.5). Finally, we discuss the impact of optimizing different rewards ( \u00a74.6).\n\n\nFull Ablation\n\nIn this section we run an ablation study on the two innovations of the MAD algorithm, the conditional normalized rewards ( \u00a72.2) and the MAD importance weights ( \u00a72.3). For each experiment we use the distributed and sampling components of our algorithm. In experiments where our conditional reward, r i , is not included, we use PPO's batch normalized reward, r i and in experiments where the MAD weights, w i , are not included, we use the standard importance weights, min{u i , 2}. We start with a baseline setup where both the conditional reward normalization and MAD weights are removed; this is the pink line in Figure 2. Next we add back each of these components separately and then, finally, add both components back which results in our MAD algorithm. We can see from this ablation that the primary driver of our results is the conditional reward normalization and that the MAD weights reduce variance enabling the model to continue learning for more training steps.  To correct this, we introduce conditional reward normalization ( \u00a72.2) (right column) and see that it produces both positive and negative rewards for every source sentence. We also find that the batch level reward distribution (bottom row) is more normal when using conditional normalization.\n\n\nConditional Reward Effect\n\nOur ablation study shows that using conditionally normalized rewards is the most important part of our algorithm. We hypothesized that due to the heterogeneous nature of source sentence translation difficulty that unconditional normalization schemes would systemically assign negative rewards to difficult source sentence translations. To test this hypothesis, we randomly sampled 10 source sentences from the WMT'20 En \u2192 Zh training dataset and ran our translation sampling algorithm on them. We then compared the assigned rewards for the translations when using batch normalization vs. conditional normalization. Figure 3 shows that when unconditional normalization is used the sign of the reward is strongly determined by the source sentence rather than the relative quality of the translation and that this issue is mitigated by using conditional normalization. We also note that the distribution of rewards at the batch level is more Gaussian when using conditional normalization. This analysis helps to explain why models trained with an unconditionally normalized reward exhibit a catastrophic drop in performance during training. These models are consistently reducing the entropy of easy source sentences and increasing the entropy of difficult ones which eventually leads to divergence. We explore the concept of source sentence difficulty in the Appendix A.2.\n\n\nMAD Weights Effect\n\nAt first it seems counter-intuitive that we would want to down-weight both high and low probability translation samples. Lowering the weight of very bad translations makes sense because they are off policy and, in many cases, degenerate. However, down-weighting translations that have a very high probability seems counterproductive. Prior work has shown that translation models fine-tuned with policy gradient methods end up having lower entropy predictions (Kiegeland & Kreutzer, 2021;Choshen et al., 2020). We observed that when sampling from a pre-trained cross entropy model, as is done at the very beginning of RL finetuning, the highest reward sample is usually the greedily decoded translation which also usually has the lowest entropy since it comes from the mode of the sampling distribution. This results in the policy's mode getting the most positive reinforcement during training and makes the sampling distribution more peaked. We formulated the MAD weights to try and preserve sampling diversity over the course of training and Figure 4 demonstrates that it is effective in this regard. We see MAD weights as a sort of conditional entropy regularizer that focuses learning to occur in the space that is slightly off the current policy. It is conditional in the sense that the level of low entropy penalization (downweighting) depends on the characteristics of the sampling distribution for the given source sentence and policy state.\n\nMAD weights are a step in the right direction but there is more research to be done in this area since it does not prevent policy collapse, just slows down the rate at which it occurs.  \n\n\nTraining Stability\n\nWe look at two aspects of training stability: 1) the amount of variance in generalization performance between random seeds and 2) the rate at which over-fitting occurrs during training. Given this definition of training stability, we can see in Figure 5 that MAD provides stable training for many steps. The difference in generalization performance between the best and worst MAD seed on NIST was 0.15 BLEU. Also, in terms of over-fitting, PPO and REINFORCE consistently produced learning curves similar to the orange lines in Figure 5 across all the datasets we evaluated. We did not observe this catastrophic drop in performance when using MRT or MAD. The primary difference between MRT and MAD in terms of training dynamics is that MAD has lower seed variance and obtains its max development dataset BLEU score later in training.\n\n\nTemperature Sensitivity\n\nWe find that all of the sequence level optimization algorithms evaluated in this work are sensitive to the temperature used during sampling. Figure 6 shows the MAD training curves during a temperature sweep on WMT'20 En \u2192 Zh.\n\nWe can see that getting the correct temperature range is important, but that there is some leeway in terms of the optimal range. Step Figure 5. Training curves for 3 random seeds per algorithm. We see that MAD training is stable and has low variance between random seeds. PPO exhibits catastrophic drops in performance while MRT has high random seed variance.\n\nalgorithms as to the optimal temperature range for a given dataset. One of the benefits of using a temperature range rather than a single temperature is that a smaller sweep can be performed to find this optimal hyperparameter. We were able to reduce the number of sweep experiments from 10 to 4 ( \u00a73.4). The MAD algorithm works just as well when using a single temperature, but we believe the reduction in computational burden during tuning makes using a temperature range worth the effort. WMT'20 En Zh Training Step Figure 6. MAD temperature hyperparameter sweep. In line with the other sampling based sequence level optimization baselines we evaluated, MAD is sensitive to the temperature range used to generate samples.\n\n\nImpact of Reward Type\n\nIn the experiments reported so far, we have used sentence BLEU as a reward function. However, Choshen et al. (2020) have shown that uninformative rewards work as well as informative rewards at improving the BLEU score, conjecturing that the reported improvements in these algorithms are due to training artifacts such as reduced entropy of the Table 2. MAD was used to optimize different rewards on the IWSLT'14 De-En dataset. We report test set results for the checkpoint with the max validation performance on the metric being optimized. For TER, a lower score is better, so we optimize \u2212TER. The last row, ALL, optimized the equally weighted average of the rewards, (1/6)(BLEU + GLEU + ChrF + Token F1 \u2212 TER + BLEURT).\n\nReward Optimized BLEU GLEU ChrF TER Token F1 BLEURT ALL sBLEU (Papineni et al., 2002) 32.1 30.0 55.5 50.0 55.9 58.6 30.3 GLEU (Mutton et al., 2007) 32.2 30.5 55.7 49.1 57.0 59.5 31.0 ChrF (Popovi\u0107, 2015) 32.0 30.0 56.5 49.7 56.0 59.0 30.6 \u2212TER (Snover et al., 2006) 32 predictive distribution rather than reward-driven learning.\n\nIn this section we look at whether training with MAD on different rewards results in a policy that improves those rewards on held-out test sets. 6 We can see in Table 2 that the models are able to generalize on their metric and usually outperform models trained to optimize different metrics. We are able to train a model to optimize multiple metrics and generalize well on all of them.\n\n\nRelated Work\n\nDuring the era of linear translation models, setting parameters to optimize rewards was standard. Although cross entropy is still widely used, Ranzato et al. (2016) inaugurated using RL techniques for neural translation models, and Edunov et al. (2018) provide a thorough survey of the topic and find that RL fine-tuning usually improves the original model. Although it has been shown to optimize a biased approximation to the expected reward objective (Choshen et al., 2020), minimum risk training remains extremely effective (Shen et al., 2016;Kiegeland & Kreutzer, 2021).\n\nMore complicated policy gradient methods have been proposed that rely on auxiliary networks. MIXER (Ranzato et al., 2016) predicts a per-token reward baseline while actorcritic methods (Bahdanau et al., 2017) learn a per-timestep Q function concurrently with the translation policy. Although not widely used yet in translation, proximal policy optimization (PPO) (Schulman et al., 2017) has been used to train summarization models from human preferences (Stiennon et al., 2020), and this algorithm provided inspiration for some of the techniques we used here.\n\nAnother important source of inspiration in the development of our algorithm was the \"hope and fear\" online learning algorithm of Chiang (2012), which used a margin-based analysis to argue for a decision rule that selected positive and negative training samples on the basis of a combination of current model score and reward. Additionally, pairwise ranking optimization (Hopkins & May, 2011) reduced the parameter learning problem to classifying pairs of positively and negatively rewarded trajectories, and our objective can be understood as a weighted variant of that objective.\n\nOur work differentiates itself through our conditional reward normalization and approach to entropy regularization through the use of MAD weights.\n\n\nConclusion\n\nWe introduce a new policy gradient algorithm, MAD, for machine translation and show that it outperforms existing reward-aware training procedures such as REINFORCE, minimum risk training (MRT) and proximal policy optimization (PPO). We test these algorithms on a variety of datasets, language pair directions, and reward functions and demonstrate good test set performance. Our algorithm excels at producing models that generate quality translations, even when greedy decoding is used. Our analysis shows that the conditional reward normalization we introduce is a critical component of the model's success while the MAD weights improve sample diversity and training stability. Taken together, we see an average 2.0 BLEU improvement over the initial cross entropy model and a 1.0 BLEU improvement over the best baseline treatment, MRT. Finally, we offer a method for reducing the number of sweeps needed to find the optimal sampling temperature hyperparameter making it easy to use on new tasks.  Table 8 shows results (on the test set) using larger beams and different decoding hyperparameters. MAD trained with sentenceBLEU continues to outperform other training objectives, and displays the notable behaviour that decoding without length normalization with large beams is effective, whereas performs drops significantly with CE and REIN-FORCE trained models.\n\nWe emphasize that the beam search length normalization term is the same for all models and datasets reported in Table 1. Most prior work in machine translation optimizes \u03b1 per dataset and only reports this score. We have confirmed that our cross entropy models match prior work when using the tuned \u03b1. In this sense, we believe the BLEU numbers we report are a lower bound, which might be improved through \u03b1 tuning.\n\n\nA.2. Source Sentence Difficulty\n\nIt's somewhat difficult to quantify how hard a source sentence will be to translate. However, there is evidence that the uncertainty (as measured by entropy) a powerful Language Model has when modeling the source sentence can be used as a proxy for this (Rae et al., 2022). We see in Figure 7 that there is negative relationship between source sentence entropy and translation performance. This means that unconditional reward normalization methods will incorporate this bias.  . Plot of the relationship between source sentence difficulty and the average score of sampled translations. We use the average word entropy from a strong 280 billion parameter language model (Rae et al., 2022) as a proxy for the uniqueness/difficulty of a given source sentence. We can see that the more uncertainty a language model has about the source sentence, the lower the average BLEU of its sampled translations.\n\n\nA.3. Additional Remarks\n\nWe note that the conditional reward normalization could be applied to both REINFORCE and PPO, however, decided against it to keep these baselines in line with prior work. We also believe our baselines coincide with the form most likely to be used in practice. We did run some preliminary experiments with PPO where we replaced r with r, however, the results were mixed. PPO has a few additional hyperparameters that can be tuned so we leave this as future research. \n\n\nB. Hyperparameters\n\nWe provide the hyperparameters for our Transformer models in Tables 4 and 5. The algorithm specific settings are in Table 6. Finally, the setting used when calculation sacreBLEU are found in Table 7.    rors present in training data, standardization of quotation marks into \"directional\" variants.\n\n\nC. Datasets\n\n\u2022 English -Replace non-American spelling variants with American spellings using the aspell library. 11 Punctuation was split from English words using a purpose-built library.\n\n\u2022 Chinese -Convert any traditional Chinese characters into simplified forms and segment into word-like units using the Jieba segmentation tool. 12\n\nTokenization We encode text into sub-word units using the sentencepiece tool (Kudo & Richardson, 2018). When generating our own subword segmentation, we used the algorithm from Kudo (2018) with a minimum character coverage of 0.9995.\n\n\nD. Implementation Details\n\nSoftware We use Launchpad (Yang et al., 2021) to create and launch our DAG computing graph. The graph consists of a Reverb table (Cassirer et al., 2021) that holds training examples, multiple worker nodes that generate and publish Compute All of our program nodes run on a 2x2 TPU configuration. Our hyperparameter sweeps for PPO and MAD used 2 data generating workers running for 50K steps, while final results used 4 workers and ran for 200K steps. Table 10 in the body of the paper provides the wall clock training speed for each algorithm.\n\nTraining time Because of our distributed algorithm, we are able to obtain faster training times by increasing the number of workers that sample from the current policy (this in contrast to purely \"on-policy\" REINFORCE and MRT algorithms). Table 10 shows that we get near linear scaling as the number of workers is increased.  \n\nFigure 3 .\n3Some source sentences are intrinsically more difficult to translate than others. This results in their translations receiving systematically lower BLEU scores. When unconditional reward normalization such as batch level normalization (left column) is applied, the training examples for these difficult source sentences predominately receive negative rewards.\n\nFigure 4 .\n4We plot the mean number of unique samples generated by the worker nodes over the course of training for two different datasets. For each source sentence, 12 samples are generated and is thus the upper bound. The green line is the full MAD algorithm while the orange line is an ablation where the MAD weights have been removed. We see that including the MAD weights helps to slow the loss of diversity during training.\n\nFigure 7\n7Figure 7. Plot of the relationship between source sentence difficulty and the average score of sampled translations. We use the average word entropy from a strong 280 billion parameter language model (Rae et al., 2022) as a proxy for the uniqueness/difficulty of a given source sentence. We can see that the more uncertainty a language model has about the source sentence, the lower the average BLEU of its sampled translations.\n\n\nAlgorithm 1 Asynchronous data generator1: function GENERATE(N, \u2206, Tmin, Tmax)   2: \nwhile True do \n3: \n\u03b8 old \u2190 \u03b8 \nGet current global weights \n4: \n(x, y ref ) \u223c Dtrain \n5: \nfor i \u2208 [1, N ] do \nObtain Yx, q, and r \n6: \n\n\nTable 1 .\n1Performance of different algorithms on translation. We report sacreBLEU on the held-out test set between the detokenized model hypothesis and original (not tokenized) references. Note that for IWSLT'14 and NIST both the hypotheses and references were lower-cased to keep comparable with prior works. \u00b5 is the average BLEU across all datasets for each method. MAD outperforms other methods by a large margin when greedy decoding is used. The gap shrinks when beam search is used, but MAD still outperforms the next best method, MRT.Greedy Decoding \nNIST \nIWSLT'14 \nWMT'14 \nWMT'20 \nModel \nZh-En De-En En-De De-En En-De Zh-En En-Zh Ps-En En-Ps \n\u00b5 \nCross Entropy \n45.5 \n30.1 \n26.7 \n30.5 \n25.7 \n25.0 \n37.3 \n6.6 \n6.0 \n25.9 \nREINFORCE \n45.5 \n30.2 \n26.7 \n29.3 \n25.7 \n25.0 \n37.1 \n7.2 \n5.9 \n25.8 \nPPO \n45.9 \n31.0 \n27.7 \n30.8 \n25.6 \n24.8 \n37.3 \n7.3 \n7.5 \n26.4 \nMRT \n45.2 \n30.8 \n26.7 \n31.3 \n25.8 \n26.4 \n38.8 \n8.4 \n7.5 \n26.8 \nMAD \n47.4 \n32.1 \n28.0 \n31.8 \n27.1 \n27.5 \n39.5 \n8.8 \n7.9 \n27.8 \n\nBeam Search | Beams = 5 | Length Normalization (\u03b1) = 1.0 \nNIST \nIWSLT'14 \nWMT'14 \nWMT'20 \nModel \nZh-En De-En En-De De-En En-De Zh-En En-Zh Ps-En En-Ps \n\u00b5 \nCross Entropy \n47.6 \n31.4 \n27.9 \n31.8 \n26.4 \n26.4 \n37.5 \n7.0 \n6.0 \n26.9 \nREINFORCE \n47.5 \n31.4 \n28.0 \n30.6 \n26.3 \n26.3 \n37.8 \n8.1 \n6.0 \n26.9 \nPPO \n47.5 \n31.3 \n28.2 \n31.7 \n26.3 \n26.2 \n37.5 \n7.7 \n7.6 \n27.1 \nMRT \n47.0 \n31.9 \n27.9 \n32.0 \n26.6 \n27.4 \n39.3 \n9.2 \n7.9 \n27.7 \nMAD \n48.2 \n32.4 \n28.3 \n32.2 \n27.3 \n28.0 \n39.9 \n9.4 \n8.1 \n28.2 \n\n\n\nTable 3\n3in the Appendix shows the optimal \ntemperatures found for each of the datasets and algorithms. \nGenerally speaking, there is agreement among the various \n\n\n\nTable 3 .\n3Temperatures used for models in main results. REIN-FORCE used same temperatures as PPO. Only MAD uses a temperature range, other algorithms use as single temperature.Temperatures \nDataset \nAlgo \nT min T max \n\nIWSLT'14 De \u2192 En \n\nPPO \n1.0 \nMRT \n0.9 \nMAD 0.8 \n1.2 \n\nIWSLT'14 En \u2192 De \n\nPPO \n1.2 \nMRT \n0.6 \nMAD 0.8 \n1.2 \n\nWMT'14 De \u2192 En \n\nPPO \n0.2 \nMRT \n0.6 \nMAD 0.4 \n0.8 \n\nWMT'14 En \u2192 De \n\nPPO \n1.2 \nMRT \n0.4 \nMAD 0.4 \n0.8 \n\nWMT'20 Ps \u2192 En \n\nPPO \n1.2 \nMRT \n1.2 \nMAD 0.8 \n1.2 \n\nWMT'20 En \u2192 Ps \n\nPPO \n1.2 \nMRT \n1.0 \nMAD 0.4 \n0.8 \n\nNIST Zh \u2192 En \n\nPPO \n1.0 \nMRT \n0.8 \nMAD 0.6 \n1.0 \n\nWMT'20 Zh \u2192 En \n\nPPO \n0.8 \nMRT \n0.8 \nMAD 0.4 \n0.8 \n\nWMT'20 En \u2192 Zh \n\nPPO \n0.8 \nMRT \n0.6 \nMAD 0.2 \n0.6 \n\n\n\nTable 4 .\n4Base settings for our Transformer models.Transformer -Base Settings \nSetting \nValue \nembeddings dim \n512 \nfeed forward \n2048 \nnum layers \n6 enc + 6 dec \nnum heads \n8 \ndropout \n.1 \ntied embeddings \nTrue \ntied softmax \nTrue \noptimizer \nAdam \nlearning rate (\u03b7) \n1e-5 \n\u03b7 warmup steps \n1000 \n\u03b7 scheduling \nConstant \ngradient clipping global norm(1) \ntraining steps \n200,000 \nseq length \n128 \nglobal batch size \n256 \n\n\n\nTable 5 .\n5Deviations from Transformer base settings.Transformer -Deltas \nDataset \nSetting \nValue \n\nWMT'20 Ps \u2194 En \nseq length \n64 \ndropout \n.3 \nWMT'14 De \u2194 En \nseq length \n144 \n\nIWSLT'14 De \u2194 En \n\nfeed forward \n1024 \nnum heads \n4 \ndropout \n.3 \n\n\n\nTable 6 .\n6Settings for sequence level algorithms. Note that we did run experiments with |Yx| = 12 for MRT and found no improvement in development set max BLEU. Given the slow training speed of MRT, we opted to use the convention of 5 samples.Algorithm Settings \nAlgorithm Setting \nValue \nPPO \n0.2 \nMRT \n|Y x | \n5 \n\nMAD \nreverb \nsettings \n\nmax times sampled \n1 \nmin size to sample \n512 \nmax size \n4096 \nsampler \nUniform \nremover \nFifo \n\n\n\nTable 7 .\n7sacreBLEU settings for each language.sacreBLEU Settings \nLanguage Setting \nValue \nDe \ntokenizer \nintl \nEn \ntokenizer \n13a \nPs \ntokenizer \nintl \nZh \ntokenizer \nzh \n\nAll \nsmooth method \nexp \nsmooth value \n0 \n\n\n\nTable 8 .\n8Additional test set results when using different Beam Search hyperparameters. Even when large beams are used without length normalization, MAD performs well indicating that it is better calibrated. Beam Search | Beams = 5 | Length Normalization (\u03b1) = None NIST IWSLT'14 WMT'14 WMT'20 Model Zh-En De-En En-De De-En En-De Zh-En En-Zh Ps-En En-Ps En De-En En-De De-En En-De Zh-En En-Zh Ps-En En-Ps Beam Search | Beams = 50 | Length Normalization (\u03b1) = 1.0 NIST IWSLT'14 WMT'14 WMT'20 Model Zh-En De-En En-De De-En En-De Zh-En En-Zh Ps-En En-Ps examples, a learner node that pulls and trains on examples, and an evaluator node that loads fresh checkpoints to calculate dev set sacreBLEU. It is important to not make the Reverb table cache too large to avoid the table filling with stale training examples. This can happen if the training node job is temporarily deallocated while the workers are still running.\u00b5 \n\n\nTable 9 .\n9Number of training, development, and test examples in each dataset.Dataset \nTrain \nDev \nTest \nIWSLT'14 De \u2192 En \n164K 7,466 6,750 \nIWSLT'14 En \u2192 De \n173K 1,474 6,750 \nWMT'20 Ps \u2194 En \n516K 5,860 2,719 \nNIST Zh \u2192 En \n1.45M 1,664 5,146 \nWMT'14 De \u2194 En \n4.7M 3,000 3,003 \nWMT'20 Zh \u2192 En \n21.8M 2,000 2,000 \nWMT'20 En \u2192 Zh \n21.8M 1,997 1,418 \n\n\n\nTable 10 .\n10We show the training speed as a function of the number of 2x2 TPU workers used. Speed is measured in seconds per 1000 training steps.Training Speed \nAlgorithm \nWorkers Sec per 1K \nREINFORCE n/a \n2,038 \nMRT \nn/a \n1,728 \n\nPPO & MAD \n2 \n1,898 \n4 \n946 \n\nAssemblyAI, New York, USA 2 DeepMind, London, UK 3 Talka, Lisbon, PT. Correspondence to: Domenic Donato <domenic@assemblyai.com>. All the authors were at DeepMind when this work was completed.\nhttps://numpy.org/doc/stable/reference/ generated/numpy.linspace.html\nFor most of this work \u2206(y, y ref ) refers to sentence BLEU, see \u00a74.6 for alternatives and analysis.\nThe median was chosen because q can have a long tail due to degenerate translations.\nhttps://github.com/mjpost/sacreBLEU 5 SeeTable 7in Appendix B for settings.\nWe note that the constant reward used byChoshen et al. (2020) would result in no learning in our algorithm on account of the reward standardisation discussed above in \u00a72.2, so we do not compare to this condition.\nhttp://wordlist.aspell.net/ varcon-readme/ 12 https://github.com/fxsjy/jieba\n\nAn actor-critic algorithm for sequence prediction. D Bahdanau, P Brakel, K Xu, A Goyal, R Lowe, J Pineau, A C Courville, Y Bengio, 5th International Conference on Learning Representations. Toulon, FranceConference Track ProceedingsBahdanau, D., Brakel, P., Xu, K., Goyal, A., Lowe, R., Pineau, J., Courville, A. C., and Bengio, Y. An actor-critic algorithm for sequence prediction. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https:// openreview.net/forum?id=SJDaqqveg.\n\nA Cassirer, G Barth-Maron, E Brevdo, S Ramos, T Boyd, T Sottiaux, M Kroiss, Reverb, A framework for experience replay. Cassirer, A., Barth-Maron, G., Brevdo, E., Ramos, S., Boyd, T., Sottiaux, T., and Kroiss, M. Reverb: A framework for experience replay, 2021.\n\nHope and fear for discriminative training of statistical translation models. D Chiang, Journal of Machine Learning Research. 1340Chiang, D. Hope and fear for discriminative training of statistical translation models. Journal of Machine Learn- ing Research, 13(40):1159-1187, 2012. URL http: //jmlr.org/papers/v13/chiang12a.html.\n\nOn the weaknesses of reinforcement learning for neural machine translation. L Choshen, L Fox, Z Aizenbud, Abend , O , International Conference on Learning Representations. Choshen, L., Fox, L., Aizenbud, Z., and Abend, O. On the weaknesses of reinforcement learning for neural machine translation. In International Conference on Learning Representations, 2020. URL https://openreview. net/forum?id=H1eCw3EKvH.\n\nDeep reinforcement learning from human preferences. P F Christiano, J Leike, T Brown, M Martic, S Legg, D ; Amodei, U V Luxburg, S Bengio, H Wallach, R Fergus, S Vishwanathan, Garnett , Advances in Neural Information Processing Systems. R.Curran Associates, Inc30Guyon, IChristiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D. Deep reinforcement learning from human preferences. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vish- wanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 30. Curran As- sociates, Inc., 2017. URL https://proceedings. neurips.cc/paper/2017/file/ d5e2c0adad503c91f91df240d0cd4e49-Paper. pdf.\n\nLearning bounds for importance weighting. C Cortes, Y Mansour, M Mohri, J Lafferty, C Williams, J Shawe-Taylor, R Zemel, Culotta , Advances in Neural Information Processing Systems. A.Curran Associates, Inc23Cortes, C., Mansour, Y., and Mohri, M. Learning bounds for importance weighting. In Lafferty, J., Williams, C., Shawe-Taylor, J., Zemel, R., and Culotta, A. (eds.), Advances in Neural Information Pro- cessing Systems, volume 23. Curran Associates, Inc., 2010. URL https://proceedings.\n\nSoftmax tempering for training neural machine translation models. R Dabre, A Fujita, Dabre, R. and Fujita, A. Softmax tempering for training neural machine translation models, 2020.\n\nClassical structured prediction losses for sequence to sequence learning. S Edunov, M Ott, M Auli, D Grangier, M Ranzato, 10.18653/v1/N18-1033Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, Louisiana1Association for Computational LinguisticsEdunov, S., Ott, M., Auli, M., Grangier, D., and Ranzato, M. Classical structured prediction losses for sequence to sequence learning. In Proceedings of the 2018 Confer- ence of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 1 (Long Papers), pp. 355-364, New Orleans, Louisiana, June 2018. Association for Compu- tational Linguistics. doi: 10.18653/v1/N18-1033. URL https://aclanthology.org/N18-1033.\n\nTuning as ranking. M Hopkins, J May, Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing. the 2011 Conference on Empirical Methods in Natural Language ProcessingEdinburgh, Scotland, UK.Association for Computational LinguisticsHopkins, M. and May, J. Tuning as ranking. In Proceedings of the 2011 Conference on Empirical Methods in Nat- ural Language Processing, pp. 1352-1362, Edinburgh, Scotland, UK., July 2011. Association for Computational Linguistics. URL https://aclanthology.org/ D11-1125.\n\nRevisiting the weaknesses of reinforcement learning for neural machine translation. S Kiegeland, J Kreutzer, 10.18653/v1/2021.naacl-main.133Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnlineAssociation for Computational LinguisticsKiegeland, S. and Kreutzer, J. Revisiting the weaknesses of reinforcement learning for neural machine transla- tion. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Compu- tational Linguistics: Human Language Technologies, pp. 1673-1681, Online, June 2021. Association for Compu- tational Linguistics. doi: 10.18653/v1/2021.naacl-main. 133. URL https://aclanthology.org/2021. naacl-main.133.\n\nBandit structured prediction for neural sequence-to-sequence learning. J Kreutzer, A Sokolov, S Riezler, 10.18653/v1/P17-1138Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational LinguisticsKreutzer, J., Sokolov, A., and Riezler, S. Bandit struc- tured prediction for neural sequence-to-sequence learn- ing. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers), pp. 1503-1513, Vancouver, Canada, July 2017. Association for Computational Lin- guistics. doi: 10.18653/v1/P17-1138. URL https: //aclanthology.org/P17-1138.\n\nSubword regularization: Improving neural network translation models with multiple subword candidates. T Kudo, Proceedings of ACL. ACLKudo, T. Subword regularization: Improving neural network translation models with multiple subword candidates. In Proceedings of ACL, 2018.\n\nSentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. T Kudo, J Richardson, Proceedings of EMNLP. EMNLPKudo, T. and Richardson, J. SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In Proceedings of EMNLP, 2018.\n\nAssociation for Computational Linguistics. Denmark, 10.18653/v1/D17-1153Denmark, September 2017. Association for Computa- tional Linguistics. doi: 10.18653/v1/D17-1153. URL https://aclanthology.org/D17-1153.\n\nText generation by learning from demonstrations. R Y Pang, H He, ICLR. 2021Pang, R. Y. and He, H. Text generation by learning from demonstrations. In ICLR, 2021.\n\nBleu: a method for automatic evaluation of machine translation. K Papineni, S Roukos, T Ward, W.-J Zhu, doi: 10.3115/ 1073083.1073135Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational LinguisticsPhiladelphia, Pennsylvania, USAAssociation for Computational LinguisticsPapineni, K., Roukos, S., Ward, T., and Zhu, W.-J. Bleu: a method for automatic evaluation of machine transla- tion. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pp. 311- 318, Philadelphia, Pennsylvania, USA, July 2002. As- sociation for Computational Linguistics. doi: 10.3115/ 1073083.1073135. URL https://aclanthology. org/P02-1040.\n\nBalancing learning speed and stability in policy gradient via adaptive exploration. M Papini, A Battistello, M Restelli, PMLRThe 23rd International Conference on Artificial Intelligence and Statistics. Chiappa, S. and Calandra, R.Palermo, Sicily, Italy2020OnlinePapini, M., Battistello, A., and Restelli, M. Balancing learn- ing speed and stability in policy gradient via adaptive exploration. In Chiappa, S. and Calandra, R. (eds.), The 23rd International Conference on Artificial Intelligence and Statistics, AISTATS 2020, 26-28 August 2020, On- line [Palermo, Sicily, Italy], volume 108 of Proceedings of Machine Learning Research, pp. 1188-1199. PMLR, 2020. URL http://proceedings.mlr.press/ v108/papini20a.html.\n\nA deep reinforced model for abstractive summarization. R Paulus, C Xiong, R Socher, International Conference on Learning Representations. Paulus, R., Xiong, C., and Socher, R. A deep rein- forced model for abstractive summarization. In In- ternational Conference on Learning Representations, 2018. URL https://openreview.net/forum? id=HkAClQgA-.\n\nchrF: character n-gram F-score for automatic MT evaluation. M Popovi\u0107, 10.18653/v1/W15-3049Proceedings of the Tenth Workshop on Statistical Machine Translation. the Tenth Workshop on Statistical Machine TranslationLisbon, PortugalAssociation for Computational LinguisticsPopovi\u0107, M. chrF: character n-gram F-score for automatic MT evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, pp. 392-395, Lis- bon, Portugal, September 2015. Association for Compu- tational Linguistics. doi: 10.18653/v1/W15-3049. URL https://aclanthology.org/W15-3049.\n\nA call for clarity in reporting BLEU scores. M Post, 10.18653/v1/W18-6319Proceedings of the Third Conference on Machine Translation: Research Papers. the Third Conference on Machine Translation: Research PapersBrussels, BelgiumAssociation for Computational LinguisticsPost, M. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Trans- lation: Research Papers, pp. 186-191, Brussels, Bel- gium, October 2018. Association for Computational Lin- guistics. doi: 10.18653/v1/W18-6319. URL https: //aclanthology.org/W18-6319.\n\nEligibility traces for off-policy policy evaluation. D Precup, R S Sutton, S Singh, Proceedings of the Seventeenth International Conference on Machine Learning (ICML-00). the Seventeenth International Conference on Machine Learning (ICML-00)Precup, D., Sutton, R. S., and Singh, S. Eligibility traces for off-policy policy evaluation. In In Proceedings of the Seventeenth International Conference on Machine Learning (ICML-00), pp. 759-766, 2000.\n\n. J W Rae, S Borgeaud, T Cai, K Millican, J Hoffmann, F Song, J Aslanides, S Henderson, R Ring, S Young, E Rutherford, T Hennigan, J Menick, A Cassirer, R Powell, G Van Den Driessche, L A Hendricks, M Rauh, P.-S Huang, A Glaese, J Welbl, S Dathathri, S Huang, J Uesato, J Mellor, I Higgins, A Creswell, N Mcaleese, A Wu, E Elsen, S Jayakumar, E Buchatskaya, D Budden, E Sutherland, K Simonyan, M Paganini, L Sifre, L Martens, X L Li, A Kuncoro, A Nematzadeh, E Gribovskaya, D Donato, A Lazaridou, A Mensch, J.-B Lespiau, M Tsimpoukelli, N Grigorev, D Fritz, T Sottiaux, M Pajarskas, T Pohlen, Z Gong, D Toyama, C De Masson D&apos;autume, Y Li, T Terzi, V Mikulik, I Babuschkin, A Clark, D De Las Casas, A Guy, C Jones, J Bradbury, M Johnson, B Hechtman, L Weidinger, I Gabriel, W Isaac, E Lockhart, S Osindero, L Rimell, C Dyer, O Vinyals, K Ayoub, J Stanway, L Bennett, D Hassabis, K Kavukcuoglu, G Irving, Scaling language models: Methods, analysis and insights from training gopherRae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., Aslanides, J., Henderson, S., Ring, R., Young, S., Rutherford, E., Hennigan, T., Menick, J., Cassirer, A., Powell, R., van den Driessche, G., Hendricks, L. A., Rauh, M., Huang, P.-S., Glaese, A., Welbl, J., Dathathri, S., Huang, S., Uesato, J., Mellor, J., Higgins, I., Creswell, A., McAleese, N., Wu, A., Elsen, E., Jayakumar, S., Buchatskaya, E., Budden, D., Sutherland, E., Simonyan, K., Paganini, M., Sifre, L., Martens, L., Li, X. L., Kun- coro, A., Nematzadeh, A., Gribovskaya, E., Donato, D., Lazaridou, A., Mensch, A., Lespiau, J.-B., Tsimpoukelli, M., Grigorev, N., Fritz, D., Sottiaux, T., Pajarskas, M., Pohlen, T., Gong, Z., Toyama, D., de Masson d'Autume, C., Li, Y., Terzi, T., Mikulik, V., Babuschkin, I., Clark, A., de Las Casas, D., Guy, A., Jones, C., Bradbury, J., Johnson, M., Hechtman, B., Weidinger, L., Gabriel, I., Isaac, W., Lockhart, E., Osindero, S., Rimell, L., Dyer, C., Vinyals, O., Ayoub, K., Stanway, J., Bennett, L., Hassabis, D., Kavukcuoglu, K., and Irving, G. Scaling language models: Methods, analysis and insights from training gopher, 2022.\n\nSequence level training with recurrent neural networks. M Ranzato, S Chopra, M Auli, W Zaremba, 4th International Conference on Learning Representations. Bengio, Y. and LeCun, Y.San Juan, Puerto RicoConference Track ProceedingsRanzato, M., Chopra, S., Auli, M., and Zaremba, W. Se- quence level training with recurrent neural networks. In Bengio, Y. and LeCun, Y. (eds.), 4th International Confer- ence on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceed- ings, 2016. URL http://arxiv.org/abs/1511. 06732.\n\nSelf-critical sequence training for image captioning. S J Rennie, E Marcheret, Y Mroueh, J Ross, V Goel, doi: 10.1109/ CVPR.2017.1312017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Rennie, S. J., Marcheret, E., Mroueh, Y., Ross, J., and Goel, V. Self-critical sequence training for image captioning. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1179-1195, 2017. doi: 10.1109/ CVPR.2017.131.\n\nProximal policy optimization algorithms. J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, abs/1707.06347CoRRSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv. org/abs/1707.06347.\n\nBLEURT: Learning robust metrics for text generation. T Sellam, D Das, A Parikh, 10.18653/v1/2020.acl-main.704Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsSellam, T., Das, D., and Parikh, A. BLEURT: Learn- ing robust metrics for text generation. In Proceed- ings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7881-7892, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.704. URL https: //aclanthology.org/2020.acl-main.704.\n\nMinimum risk training for neural machine translation. S Shen, Y Cheng, Z He, W He, H Wu, M Sun, Y Liu, 10.18653/v1/P16-1159Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, Germany1Association for Computational LinguisticsShen, S., Cheng, Y., He, Z., He, W., Wu, H., Sun, M., and Liu, Y. Minimum risk training for neural machine translation. In Proceedings of the 54th Annual Meet- ing of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1683-1692, Berlin, Ger- many, August 2016. Association for Computational Lin- guistics. doi: 10.18653/v1/P16-1159. URL https: //aclanthology.org/P16-1159.\n\nA study of translation edit rate with targeted human annotation. M Snover, B Dorr, R Schwartz, L Micciulla, J Makhoul, Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers. the 7th Conference of the Association for Machine Translation in the Americas: Technical PapersCambridge, Massachusetts, USAAssociation for Machine Translation in the AmericasSnover, M., Dorr, B., Schwartz, R., Micciulla, L., and Makhoul, J. A study of translation edit rate with targeted human annotation. In Proceedings of the 7th Confer- ence of the Association for Machine Translation in the Americas: Technical Papers, pp. 223-231, Cambridge, Massachusetts, USA, August 8-12 2006. Association for Machine Translation in the Americas. URL https:// aclanthology.org/2006.amta-papers.25.\n\nLearning to summarize with human feedback. N Stiennon, L Ouyang, J Wu, D Ziegler, R Lowe, C Voss, A Radford, D Amodei, Christiano , P F , Advances in Neural Information Processing Systems. Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M. F., and Lin, H.Curran Associates, Inc33Stiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., Voss, C., Radford, A., Amodei, D., and Christiano, P. F. Learning to summarize with human feedback. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M. F., and Lin, H. (eds.), Advances in Neural Information Pro- cessing Systems, volume 33, pp. 3008-3021. Curran As- sociates, Inc., 2020. URL https://proceedings. neurips.cc/paper/2020/file/ 1f89885d556929e98d3ef9b86448f951-Paper. pdf.\n\nAttention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, \u0141 Kaiser, I Polosukhin, I Guyon, U V Luxburg, S Bengio, H Wallach, R Fergus, S Vishwanathan, Garnett , Advances in Neural Information Processing Systems. R.Curran Associates, Inc30Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vish- wanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 30. Curran As- sociates, Inc., 2017. URL https://proceedings. neurips.cc/paper/2017/file/ 3f5ee243547dee91fbd053c1c4a845aa-Paper. pdf.\n\nThe optimal reward baseline for gradient-based reinforcement learning. L Weaver, N Tao, Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence, UAI'01. the Seventeenth Conference on Uncertainty in Artificial Intelligence, UAI'01San Francisco, CA, USAMorgan Kaufmann Publishers IncISBN 1558608001Weaver, L. and Tao, N. The optimal reward baseline for gradient-based reinforcement learning. In Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence, UAI'01, pp. 538-545, San Francisco, CA, USA, 2001. Morgan Kaufmann Publishers Inc. ISBN 1558608001.\n\nSimple statistical gradient-following algorithms for connectionist reinforcement learning. R J Williams, Machine learning. 83Williams, R. J. Simple statistical gradient-following algo- rithms for connectionist reinforcement learning. Machine learning, 8(3):229-256, 1992.\n\nA study of reinforcement learning for neural machine translation. L Wu, F Tian, T Qin, J Lai, T.-Y Liu, 10.18653/v1/D18-1397Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsWu, L., Tian, F., Qin, T., Lai, J., and Liu, T.-Y. A study of reinforcement learning for neural machine translation. In Proceedings of the 2018 Conference on Empirical Meth- ods in Natural Language Processing, pp. 3612-3621, Brussels, Belgium, October-November 2018. Associa- tion for Computational Linguistics. doi: 10.18653/v1/ D18-1397. URL https://aclanthology.org/ D18-1397.\n\nA programming model for distributed machine learning research. F Yang, G Barth-Maron, P Sta\u0144czyk, M Hoffman, S Liu, M Kroiss, A Pope, A Rrustemi, Launchpad, arXiv:2106.04516arXiv preprintYang, F., Barth-Maron, G., Sta\u0144czyk, P., Hoffman, M., Liu, S., Kroiss, M., Pope, A., and Rrustemi, A. Launchpad: A programming model for distributed machine learning research. arXiv preprint arXiv:2106.04516, 2021. URL https://arxiv.org/abs/2106.04516.\n\nFine-tuning language models from human preferences. D M Ziegler, N Stiennon, J Wu, T B Brown, A Radford, D Amodei, P Christiano, G Irving, arXiv:1909.08593arXiv preprintZiegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Rad- ford, A., Amodei, D., Christiano, P., and Irving, G. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. URL https: //arxiv.org/abs/1909.08593.\n\nWe evaluate all the models on the following translation tasks: NIST 7 Open MT Chinese \u2192 English task, IWSLT'14 8. We evaluate all the models on the following translation tasks: NIST 7 Open MT Chinese \u2192 English task, IWSLT'14 8\n\n\u2194 English, German, translation task, WMT'14 9 English \u2194 German news translation task, and the WMT. 2010English \u2194 German translation task, WMT'14 9 English \u2194 German news translation task, and the WMT'20 10\n\nEnglish news translation tasks. The sizes of each dataset is avaliable in Table 9. \u2194 Chinese, English, \u2194 Pashto, Chinese \u2194 English and Pashto \u2194 English news translation tasks. The sizes of each dataset is avaliable in Table 9.\n\nPreprocessing We perform text normalization on the datasets before tokenization. Preprocessing We perform text normalization on the datasets before tokenization.\n\nAll languages -Unicode canonicalization (NKFD from), replacement of common multiple encoding er. \u2022 All languages -Unicode canonicalization (NKFD from), replacement of common multiple encoding er-\n", "annotations": {"author": "[{\"end\":79,\"start\":64},{\"end\":87,\"start\":80},{\"end\":98,\"start\":88},{\"end\":110,\"start\":99}]", "publisher": null, "author_last_name": "[{\"end\":78,\"start\":72},{\"end\":86,\"start\":84},{\"end\":97,\"start\":93},{\"end\":109,\"start\":105}]", "author_first_name": "[{\"end\":71,\"start\":64},{\"end\":83,\"start\":80},{\"end\":92,\"start\":88},{\"end\":104,\"start\":99}]", "author_affiliation": null, "title": "[{\"end\":61,\"start\":1},{\"end\":171,\"start\":111}]", "venue": null, "abstract": "[{\"end\":1308,\"start\":173}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b28\"},\"end\":1605,\"start\":1582},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":1626,\"start\":1605},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":1642,\"start\":1626},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":1662,\"start\":1642},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":1682,\"start\":1662},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":1703,\"start\":1682},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2291,\"start\":2269},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2457,\"start\":2434},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2481,\"start\":2457},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":2769,\"start\":2752},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2791,\"start\":2769},{\"end\":2811,\"start\":2791},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2894,\"start\":2871},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3302,\"start\":3281},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6281,\"start\":6262},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6301,\"start\":6281},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6605,\"start\":6577},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6626,\"start\":6605},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6646,\"start\":6626},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8295,\"start\":8279},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8394,\"start\":8374},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8475,\"start\":8452},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8545,\"start\":8522},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8593,\"start\":8566},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10708,\"start\":10687},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10730,\"start\":10708},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10828,\"start\":10807},{\"end\":10844,\"start\":10828},{\"end\":11556,\"start\":11552},{\"end\":11560,\"start\":11556},{\"end\":11564,\"start\":11560},{\"end\":11568,\"start\":11564},{\"end\":11572,\"start\":11568},{\"end\":11576,\"start\":11572},{\"end\":11580,\"start\":11576},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":12152,\"start\":12129},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12183,\"start\":12162},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":12891,\"start\":12869},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":13741,\"start\":13726},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":13795,\"start\":13775},{\"end\":13866,\"start\":13838},{\"end\":13915,\"start\":13891},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":14197,\"start\":14186},{\"end\":14553,\"start\":14550},{\"end\":14556,\"start\":14553},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":16426,\"start\":16404},{\"end\":16562,\"start\":16541},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":20989,\"start\":20961},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":21010,\"start\":20989},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":24474,\"start\":24453},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":25167,\"start\":25144},{\"end\":25229,\"start\":25208},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":25285,\"start\":25270},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":25347,\"start\":25326},{\"end\":25558,\"start\":25557},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":25979,\"start\":25958},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":26067,\"start\":26047},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":26290,\"start\":26268},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":26361,\"start\":26342},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":26388,\"start\":26361},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":26512,\"start\":26490},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":26598,\"start\":26576},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":26777,\"start\":26754},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":26868,\"start\":26845},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":27094,\"start\":27081},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":27343,\"start\":27322},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":29781,\"start\":29763},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":30197,\"start\":30179},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":31663,\"start\":31638},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":31869,\"start\":31850},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":31975,\"start\":31953},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":39948,\"start\":39927}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":33067,\"start\":32696},{\"attributes\":{\"id\":\"fig_2\"},\"end\":33498,\"start\":33068},{\"attributes\":{\"id\":\"fig_4\"},\"end\":33938,\"start\":33499},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":34158,\"start\":33939},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":35635,\"start\":34159},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":35801,\"start\":35636},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":36493,\"start\":35802},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":36918,\"start\":36494},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":37166,\"start\":36919},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":37605,\"start\":37167},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":37825,\"start\":37606},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":38747,\"start\":37826},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":39098,\"start\":38748},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":39362,\"start\":39099}]", "paragraph": "[{\"end\":2482,\"start\":1324},{\"end\":3028,\"start\":2484},{\"end\":4385,\"start\":3030},{\"end\":5345,\"start\":4387},{\"end\":6098,\"start\":5359},{\"end\":7201,\"start\":6129},{\"end\":7522,\"start\":7483},{\"end\":7602,\"start\":7524},{\"end\":7953,\"start\":7604},{\"end\":8696,\"start\":8112},{\"end\":9219,\"start\":8698},{\"end\":9391,\"start\":9221},{\"end\":9682,\"start\":9432},{\"end\":9917,\"start\":9784},{\"end\":10218,\"start\":9919},{\"end\":10542,\"start\":10245},{\"end\":10731,\"start\":10572},{\"end\":10934,\"start\":10733},{\"end\":11581,\"start\":11019},{\"end\":12564,\"start\":11652},{\"end\":12788,\"start\":12591},{\"end\":14098,\"start\":12801},{\"end\":14385,\"start\":14158},{\"end\":14836,\"start\":14387},{\"end\":15839,\"start\":14906},{\"end\":16156,\"start\":16018},{\"end\":16925,\"start\":16158},{\"end\":17215,\"start\":16927},{\"end\":17793,\"start\":17253},{\"end\":19079,\"start\":17811},{\"end\":20479,\"start\":19109},{\"end\":21950,\"start\":20502},{\"end\":22138,\"start\":21952},{\"end\":22993,\"start\":22161},{\"end\":23246,\"start\":23021},{\"end\":23607,\"start\":23248},{\"end\":24333,\"start\":23609},{\"end\":25080,\"start\":24359},{\"end\":25410,\"start\":25082},{\"end\":25798,\"start\":25412},{\"end\":26389,\"start\":25815},{\"end\":26950,\"start\":26391},{\"end\":27532,\"start\":26952},{\"end\":27680,\"start\":27534},{\"end\":29056,\"start\":27695},{\"end\":29473,\"start\":29058},{\"end\":30407,\"start\":29509},{\"end\":30901,\"start\":30435},{\"end\":31221,\"start\":30924},{\"end\":31411,\"start\":31237},{\"end\":31559,\"start\":31413},{\"end\":31794,\"start\":31561},{\"end\":32367,\"start\":31824},{\"end\":32695,\"start\":32369}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7324,\"start\":7202},{\"attributes\":{\"id\":\"formula_1\"},\"end\":7482,\"start\":7324},{\"attributes\":{\"id\":\"formula_2\"},\"end\":8076,\"start\":7954},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9431,\"start\":9392},{\"attributes\":{\"id\":\"formula_4\"},\"end\":9783,\"start\":9683},{\"attributes\":{\"id\":\"formula_5\"},\"end\":10571,\"start\":10543},{\"attributes\":{\"id\":\"formula_6\"},\"end\":11018,\"start\":10935},{\"attributes\":{\"id\":\"formula_7\"},\"end\":11651,\"start\":11582},{\"attributes\":{\"id\":\"formula_8\"},\"end\":14905,\"start\":14837},{\"attributes\":{\"id\":\"formula_9\"},\"end\":16017,\"start\":15855}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":15063,\"start\":15056},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":15112,\"start\":15105},{\"attributes\":{\"ref_id\":\"tab_14\"},\"end\":17091,\"start\":17084},{\"end\":24710,\"start\":24703},{\"end\":25580,\"start\":25573},{\"attributes\":{\"ref_id\":\"tab_14\"},\"end\":28699,\"start\":28692},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":29177,\"start\":29170},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":30999,\"start\":30985},{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":31047,\"start\":31040},{\"attributes\":{\"ref_id\":\"tab_13\"},\"end\":31122,\"start\":31115},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":32283,\"start\":32275},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":32616,\"start\":32608}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1322,\"start\":1310},{\"attributes\":{\"n\":\"2.\"},\"end\":5357,\"start\":5348},{\"attributes\":{\"n\":\"2.1.\"},\"end\":6127,\"start\":6101},{\"attributes\":{\"n\":\"2.2.\"},\"end\":8110,\"start\":8078},{\"attributes\":{\"n\":\"2.3.\"},\"end\":10243,\"start\":10221},{\"attributes\":{\"n\":\"3.\"},\"end\":12578,\"start\":12567},{\"attributes\":{\"n\":\"3.1.\"},\"end\":12589,\"start\":12581},{\"attributes\":{\"n\":\"3.2.\"},\"end\":12799,\"start\":12791},{\"attributes\":{\"n\":\"3.3.\"},\"end\":14123,\"start\":14101},{\"attributes\":{\"n\":\"3.4.\"},\"end\":14156,\"start\":14126},{\"attributes\":{\"n\":\"3.5.\"},\"end\":15854,\"start\":15842},{\"attributes\":{\"n\":\"4.\"},\"end\":17251,\"start\":17218},{\"attributes\":{\"n\":\"4.1.\"},\"end\":17809,\"start\":17796},{\"attributes\":{\"n\":\"4.2.\"},\"end\":19107,\"start\":19082},{\"attributes\":{\"n\":\"4.3.\"},\"end\":20500,\"start\":20482},{\"attributes\":{\"n\":\"4.4.\"},\"end\":22159,\"start\":22141},{\"attributes\":{\"n\":\"4.5.\"},\"end\":23019,\"start\":22996},{\"attributes\":{\"n\":\"4.6.\"},\"end\":24357,\"start\":24336},{\"attributes\":{\"n\":\"5.\"},\"end\":25813,\"start\":25801},{\"attributes\":{\"n\":\"6.\"},\"end\":27693,\"start\":27683},{\"end\":29507,\"start\":29476},{\"end\":30433,\"start\":30410},{\"end\":30922,\"start\":30904},{\"end\":31235,\"start\":31224},{\"end\":31822,\"start\":31797},{\"end\":32707,\"start\":32697},{\"end\":33079,\"start\":33069},{\"end\":33508,\"start\":33500},{\"end\":34169,\"start\":34160},{\"end\":35644,\"start\":35637},{\"end\":35812,\"start\":35803},{\"end\":36504,\"start\":36495},{\"end\":36929,\"start\":36920},{\"end\":37177,\"start\":37168},{\"end\":37616,\"start\":37607},{\"end\":37836,\"start\":37827},{\"end\":38758,\"start\":38749},{\"end\":39110,\"start\":39100}]", "table": "[{\"end\":34158,\"start\":34021},{\"end\":35635,\"start\":34702},{\"end\":35801,\"start\":35646},{\"end\":36493,\"start\":35980},{\"end\":36918,\"start\":36547},{\"end\":37166,\"start\":36973},{\"end\":37605,\"start\":37411},{\"end\":37825,\"start\":37655},{\"end\":38747,\"start\":38744},{\"end\":39098,\"start\":38827},{\"end\":39362,\"start\":39246}]", "figure_caption": "[{\"end\":33067,\"start\":32709},{\"end\":33498,\"start\":33081},{\"end\":33938,\"start\":33510},{\"end\":34021,\"start\":33941},{\"end\":34702,\"start\":34171},{\"end\":35980,\"start\":35814},{\"end\":36547,\"start\":36506},{\"end\":36973,\"start\":36931},{\"end\":37411,\"start\":37179},{\"end\":37655,\"start\":37618},{\"end\":38744,\"start\":37838},{\"end\":38827,\"start\":38760},{\"end\":39246,\"start\":39113}]", "figure_ref": "[{\"end\":13646,\"start\":13638},{\"end\":15392,\"start\":15384},{\"end\":18436,\"start\":18428},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19732,\"start\":19724},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21553,\"start\":21545},{\"end\":22414,\"start\":22406},{\"end\":22696,\"start\":22688},{\"end\":23170,\"start\":23162},{\"end\":23390,\"start\":23382},{\"end\":24136,\"start\":24128},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":29801,\"start\":29793}]", "bib_author_first_name": "[{\"end\":40230,\"start\":40229},{\"end\":40242,\"start\":40241},{\"end\":40252,\"start\":40251},{\"end\":40258,\"start\":40257},{\"end\":40267,\"start\":40266},{\"end\":40275,\"start\":40274},{\"end\":40285,\"start\":40284},{\"end\":40287,\"start\":40286},{\"end\":40300,\"start\":40299},{\"end\":40769,\"start\":40768},{\"end\":40781,\"start\":40780},{\"end\":40796,\"start\":40795},{\"end\":40806,\"start\":40805},{\"end\":40815,\"start\":40814},{\"end\":40823,\"start\":40822},{\"end\":40835,\"start\":40834},{\"end\":41108,\"start\":41107},{\"end\":41437,\"start\":41436},{\"end\":41448,\"start\":41447},{\"end\":41455,\"start\":41454},{\"end\":41471,\"start\":41466},{\"end\":41475,\"start\":41474},{\"end\":41824,\"start\":41823},{\"end\":41826,\"start\":41825},{\"end\":41840,\"start\":41839},{\"end\":41849,\"start\":41848},{\"end\":41858,\"start\":41857},{\"end\":41868,\"start\":41867},{\"end\":41876,\"start\":41875},{\"end\":41878,\"start\":41877},{\"end\":41888,\"start\":41887},{\"end\":41890,\"start\":41889},{\"end\":41901,\"start\":41900},{\"end\":41911,\"start\":41910},{\"end\":41922,\"start\":41921},{\"end\":41932,\"start\":41931},{\"end\":41954,\"start\":41947},{\"end\":42520,\"start\":42519},{\"end\":42530,\"start\":42529},{\"end\":42541,\"start\":42540},{\"end\":42550,\"start\":42549},{\"end\":42562,\"start\":42561},{\"end\":42574,\"start\":42573},{\"end\":42590,\"start\":42589},{\"end\":42605,\"start\":42598},{\"end\":43038,\"start\":43037},{\"end\":43047,\"start\":43046},{\"end\":43229,\"start\":43228},{\"end\":43239,\"start\":43238},{\"end\":43246,\"start\":43245},{\"end\":43254,\"start\":43253},{\"end\":43266,\"start\":43265},{\"end\":44121,\"start\":44120},{\"end\":44132,\"start\":44131},{\"end\":44719,\"start\":44718},{\"end\":44732,\"start\":44731},{\"end\":45600,\"start\":45599},{\"end\":45612,\"start\":45611},{\"end\":45623,\"start\":45622},{\"end\":46369,\"start\":46368},{\"end\":46652,\"start\":46651},{\"end\":46660,\"start\":46659},{\"end\":47130,\"start\":47129},{\"end\":47132,\"start\":47131},{\"end\":47140,\"start\":47139},{\"end\":47308,\"start\":47307},{\"end\":47320,\"start\":47319},{\"end\":47330,\"start\":47329},{\"end\":47341,\"start\":47337},{\"end\":48078,\"start\":48077},{\"end\":48088,\"start\":48087},{\"end\":48103,\"start\":48102},{\"end\":48767,\"start\":48766},{\"end\":48777,\"start\":48776},{\"end\":48786,\"start\":48785},{\"end\":49119,\"start\":49118},{\"end\":49681,\"start\":49680},{\"end\":50254,\"start\":50253},{\"end\":50264,\"start\":50263},{\"end\":50266,\"start\":50265},{\"end\":50276,\"start\":50275},{\"end\":50651,\"start\":50650},{\"end\":50653,\"start\":50652},{\"end\":50660,\"start\":50659},{\"end\":50672,\"start\":50671},{\"end\":50679,\"start\":50678},{\"end\":50691,\"start\":50690},{\"end\":50703,\"start\":50702},{\"end\":50711,\"start\":50710},{\"end\":50724,\"start\":50723},{\"end\":50737,\"start\":50736},{\"end\":50745,\"start\":50744},{\"end\":50754,\"start\":50753},{\"end\":50768,\"start\":50767},{\"end\":50780,\"start\":50779},{\"end\":50790,\"start\":50789},{\"end\":50802,\"start\":50801},{\"end\":50812,\"start\":50811},{\"end\":50833,\"start\":50832},{\"end\":50835,\"start\":50834},{\"end\":50848,\"start\":50847},{\"end\":50859,\"start\":50855},{\"end\":50868,\"start\":50867},{\"end\":50878,\"start\":50877},{\"end\":50887,\"start\":50886},{\"end\":50900,\"start\":50899},{\"end\":50909,\"start\":50908},{\"end\":50919,\"start\":50918},{\"end\":50929,\"start\":50928},{\"end\":50940,\"start\":50939},{\"end\":50952,\"start\":50951},{\"end\":50964,\"start\":50963},{\"end\":50970,\"start\":50969},{\"end\":50979,\"start\":50978},{\"end\":50992,\"start\":50991},{\"end\":51007,\"start\":51006},{\"end\":51017,\"start\":51016},{\"end\":51031,\"start\":51030},{\"end\":51043,\"start\":51042},{\"end\":51055,\"start\":51054},{\"end\":51064,\"start\":51063},{\"end\":51075,\"start\":51074},{\"end\":51077,\"start\":51076},{\"end\":51083,\"start\":51082},{\"end\":51094,\"start\":51093},{\"end\":51108,\"start\":51107},{\"end\":51123,\"start\":51122},{\"end\":51133,\"start\":51132},{\"end\":51146,\"start\":51145},{\"end\":51159,\"start\":51155},{\"end\":51170,\"start\":51169},{\"end\":51186,\"start\":51185},{\"end\":51198,\"start\":51197},{\"end\":51207,\"start\":51206},{\"end\":51219,\"start\":51218},{\"end\":51232,\"start\":51231},{\"end\":51242,\"start\":51241},{\"end\":51250,\"start\":51249},{\"end\":51260,\"start\":51259},{\"end\":51287,\"start\":51286},{\"end\":51293,\"start\":51292},{\"end\":51302,\"start\":51301},{\"end\":51313,\"start\":51312},{\"end\":51327,\"start\":51326},{\"end\":51336,\"start\":51335},{\"end\":51352,\"start\":51351},{\"end\":51359,\"start\":51358},{\"end\":51368,\"start\":51367},{\"end\":51380,\"start\":51379},{\"end\":51391,\"start\":51390},{\"end\":51403,\"start\":51402},{\"end\":51416,\"start\":51415},{\"end\":51427,\"start\":51426},{\"end\":51436,\"start\":51435},{\"end\":51448,\"start\":51447},{\"end\":51460,\"start\":51459},{\"end\":51470,\"start\":51469},{\"end\":51478,\"start\":51477},{\"end\":51489,\"start\":51488},{\"end\":51498,\"start\":51497},{\"end\":51509,\"start\":51508},{\"end\":51520,\"start\":51519},{\"end\":51532,\"start\":51531},{\"end\":51547,\"start\":51546},{\"end\":52843,\"start\":52842},{\"end\":52854,\"start\":52853},{\"end\":52864,\"start\":52863},{\"end\":52872,\"start\":52871},{\"end\":53399,\"start\":53398},{\"end\":53401,\"start\":53400},{\"end\":53411,\"start\":53410},{\"end\":53424,\"start\":53423},{\"end\":53434,\"start\":53433},{\"end\":53442,\"start\":53441},{\"end\":53835,\"start\":53834},{\"end\":53847,\"start\":53846},{\"end\":53857,\"start\":53856},{\"end\":53869,\"start\":53868},{\"end\":53880,\"start\":53879},{\"end\":54137,\"start\":54136},{\"end\":54147,\"start\":54146},{\"end\":54154,\"start\":54153},{\"end\":54797,\"start\":54796},{\"end\":54805,\"start\":54804},{\"end\":54814,\"start\":54813},{\"end\":54820,\"start\":54819},{\"end\":54826,\"start\":54825},{\"end\":54832,\"start\":54831},{\"end\":54839,\"start\":54838},{\"end\":55547,\"start\":55546},{\"end\":55557,\"start\":55556},{\"end\":55565,\"start\":55564},{\"end\":55577,\"start\":55576},{\"end\":55590,\"start\":55589},{\"end\":56347,\"start\":56346},{\"end\":56359,\"start\":56358},{\"end\":56369,\"start\":56368},{\"end\":56375,\"start\":56374},{\"end\":56386,\"start\":56385},{\"end\":56394,\"start\":56393},{\"end\":56402,\"start\":56401},{\"end\":56413,\"start\":56412},{\"end\":56432,\"start\":56422},{\"end\":56436,\"start\":56435},{\"end\":56438,\"start\":56437},{\"end\":57060,\"start\":57059},{\"end\":57071,\"start\":57070},{\"end\":57082,\"start\":57081},{\"end\":57092,\"start\":57091},{\"end\":57105,\"start\":57104},{\"end\":57114,\"start\":57113},{\"end\":57116,\"start\":57115},{\"end\":57125,\"start\":57124},{\"end\":57135,\"start\":57134},{\"end\":57149,\"start\":57148},{\"end\":57158,\"start\":57157},{\"end\":57160,\"start\":57159},{\"end\":57171,\"start\":57170},{\"end\":57181,\"start\":57180},{\"end\":57192,\"start\":57191},{\"end\":57202,\"start\":57201},{\"end\":57224,\"start\":57217},{\"end\":57817,\"start\":57816},{\"end\":57827,\"start\":57826},{\"end\":58444,\"start\":58443},{\"end\":58446,\"start\":58445},{\"end\":58692,\"start\":58691},{\"end\":58698,\"start\":58697},{\"end\":58706,\"start\":58705},{\"end\":58713,\"start\":58712},{\"end\":58723,\"start\":58719},{\"end\":59411,\"start\":59410},{\"end\":59419,\"start\":59418},{\"end\":59434,\"start\":59433},{\"end\":59446,\"start\":59445},{\"end\":59457,\"start\":59456},{\"end\":59464,\"start\":59463},{\"end\":59474,\"start\":59473},{\"end\":59482,\"start\":59481},{\"end\":59841,\"start\":59840},{\"end\":59843,\"start\":59842},{\"end\":59854,\"start\":59853},{\"end\":59866,\"start\":59865},{\"end\":59872,\"start\":59871},{\"end\":59874,\"start\":59873},{\"end\":59883,\"start\":59882},{\"end\":59894,\"start\":59893},{\"end\":59904,\"start\":59903},{\"end\":59918,\"start\":59917},{\"end\":60427,\"start\":60426},{\"end\":60716,\"start\":60715},{\"end\":60736,\"start\":60735}]", "bib_author_last_name": "[{\"end\":40239,\"start\":40231},{\"end\":40249,\"start\":40243},{\"end\":40255,\"start\":40253},{\"end\":40264,\"start\":40259},{\"end\":40272,\"start\":40268},{\"end\":40282,\"start\":40276},{\"end\":40297,\"start\":40288},{\"end\":40307,\"start\":40301},{\"end\":40778,\"start\":40770},{\"end\":40793,\"start\":40782},{\"end\":40803,\"start\":40797},{\"end\":40812,\"start\":40807},{\"end\":40820,\"start\":40816},{\"end\":40832,\"start\":40824},{\"end\":40842,\"start\":40836},{\"end\":40850,\"start\":40844},{\"end\":41115,\"start\":41109},{\"end\":41445,\"start\":41438},{\"end\":41452,\"start\":41449},{\"end\":41464,\"start\":41456},{\"end\":41837,\"start\":41827},{\"end\":41846,\"start\":41841},{\"end\":41855,\"start\":41850},{\"end\":41865,\"start\":41859},{\"end\":41873,\"start\":41869},{\"end\":41885,\"start\":41879},{\"end\":41898,\"start\":41891},{\"end\":41908,\"start\":41902},{\"end\":41919,\"start\":41912},{\"end\":41929,\"start\":41923},{\"end\":41945,\"start\":41933},{\"end\":42527,\"start\":42521},{\"end\":42538,\"start\":42531},{\"end\":42547,\"start\":42542},{\"end\":42559,\"start\":42551},{\"end\":42571,\"start\":42563},{\"end\":42587,\"start\":42575},{\"end\":42596,\"start\":42591},{\"end\":43044,\"start\":43039},{\"end\":43054,\"start\":43048},{\"end\":43236,\"start\":43230},{\"end\":43243,\"start\":43240},{\"end\":43251,\"start\":43247},{\"end\":43263,\"start\":43255},{\"end\":43274,\"start\":43267},{\"end\":44129,\"start\":44122},{\"end\":44136,\"start\":44133},{\"end\":44729,\"start\":44720},{\"end\":44741,\"start\":44733},{\"end\":45609,\"start\":45601},{\"end\":45620,\"start\":45613},{\"end\":45631,\"start\":45624},{\"end\":46374,\"start\":46370},{\"end\":46657,\"start\":46653},{\"end\":46671,\"start\":46661},{\"end\":46921,\"start\":46914},{\"end\":47137,\"start\":47133},{\"end\":47143,\"start\":47141},{\"end\":47317,\"start\":47309},{\"end\":47327,\"start\":47321},{\"end\":47335,\"start\":47331},{\"end\":47345,\"start\":47342},{\"end\":48085,\"start\":48079},{\"end\":48100,\"start\":48089},{\"end\":48112,\"start\":48104},{\"end\":48774,\"start\":48768},{\"end\":48783,\"start\":48778},{\"end\":48793,\"start\":48787},{\"end\":49127,\"start\":49120},{\"end\":49686,\"start\":49682},{\"end\":50261,\"start\":50255},{\"end\":50273,\"start\":50267},{\"end\":50282,\"start\":50277},{\"end\":50657,\"start\":50654},{\"end\":50669,\"start\":50661},{\"end\":50676,\"start\":50673},{\"end\":50688,\"start\":50680},{\"end\":50700,\"start\":50692},{\"end\":50708,\"start\":50704},{\"end\":50721,\"start\":50712},{\"end\":50734,\"start\":50725},{\"end\":50742,\"start\":50738},{\"end\":50751,\"start\":50746},{\"end\":50765,\"start\":50755},{\"end\":50777,\"start\":50769},{\"end\":50787,\"start\":50781},{\"end\":50799,\"start\":50791},{\"end\":50809,\"start\":50803},{\"end\":50830,\"start\":50813},{\"end\":50845,\"start\":50836},{\"end\":50853,\"start\":50849},{\"end\":50865,\"start\":50860},{\"end\":50875,\"start\":50869},{\"end\":50884,\"start\":50879},{\"end\":50897,\"start\":50888},{\"end\":50906,\"start\":50901},{\"end\":50916,\"start\":50910},{\"end\":50926,\"start\":50920},{\"end\":50937,\"start\":50930},{\"end\":50949,\"start\":50941},{\"end\":50961,\"start\":50953},{\"end\":50967,\"start\":50965},{\"end\":50976,\"start\":50971},{\"end\":50989,\"start\":50980},{\"end\":51004,\"start\":50993},{\"end\":51014,\"start\":51008},{\"end\":51028,\"start\":51018},{\"end\":51040,\"start\":51032},{\"end\":51052,\"start\":51044},{\"end\":51061,\"start\":51056},{\"end\":51072,\"start\":51065},{\"end\":51080,\"start\":51078},{\"end\":51091,\"start\":51084},{\"end\":51105,\"start\":51095},{\"end\":51120,\"start\":51109},{\"end\":51130,\"start\":51124},{\"end\":51143,\"start\":51134},{\"end\":51153,\"start\":51147},{\"end\":51167,\"start\":51160},{\"end\":51183,\"start\":51171},{\"end\":51195,\"start\":51187},{\"end\":51204,\"start\":51199},{\"end\":51216,\"start\":51208},{\"end\":51229,\"start\":51220},{\"end\":51239,\"start\":51233},{\"end\":51247,\"start\":51243},{\"end\":51257,\"start\":51251},{\"end\":51284,\"start\":51261},{\"end\":51290,\"start\":51288},{\"end\":51299,\"start\":51294},{\"end\":51310,\"start\":51303},{\"end\":51324,\"start\":51314},{\"end\":51333,\"start\":51328},{\"end\":51349,\"start\":51337},{\"end\":51356,\"start\":51353},{\"end\":51365,\"start\":51360},{\"end\":51377,\"start\":51369},{\"end\":51388,\"start\":51381},{\"end\":51400,\"start\":51392},{\"end\":51413,\"start\":51404},{\"end\":51424,\"start\":51417},{\"end\":51433,\"start\":51428},{\"end\":51445,\"start\":51437},{\"end\":51457,\"start\":51449},{\"end\":51467,\"start\":51461},{\"end\":51475,\"start\":51471},{\"end\":51486,\"start\":51479},{\"end\":51495,\"start\":51490},{\"end\":51506,\"start\":51499},{\"end\":51517,\"start\":51510},{\"end\":51529,\"start\":51521},{\"end\":51544,\"start\":51533},{\"end\":51554,\"start\":51548},{\"end\":52851,\"start\":52844},{\"end\":52861,\"start\":52855},{\"end\":52869,\"start\":52865},{\"end\":52880,\"start\":52873},{\"end\":53408,\"start\":53402},{\"end\":53421,\"start\":53412},{\"end\":53431,\"start\":53425},{\"end\":53439,\"start\":53435},{\"end\":53447,\"start\":53443},{\"end\":53844,\"start\":53836},{\"end\":53854,\"start\":53848},{\"end\":53866,\"start\":53858},{\"end\":53877,\"start\":53870},{\"end\":53887,\"start\":53881},{\"end\":54144,\"start\":54138},{\"end\":54151,\"start\":54148},{\"end\":54161,\"start\":54155},{\"end\":54802,\"start\":54798},{\"end\":54811,\"start\":54806},{\"end\":54817,\"start\":54815},{\"end\":54823,\"start\":54821},{\"end\":54829,\"start\":54827},{\"end\":54836,\"start\":54833},{\"end\":54843,\"start\":54840},{\"end\":55554,\"start\":55548},{\"end\":55562,\"start\":55558},{\"end\":55574,\"start\":55566},{\"end\":55587,\"start\":55578},{\"end\":55598,\"start\":55591},{\"end\":56356,\"start\":56348},{\"end\":56366,\"start\":56360},{\"end\":56372,\"start\":56370},{\"end\":56383,\"start\":56376},{\"end\":56391,\"start\":56387},{\"end\":56399,\"start\":56395},{\"end\":56410,\"start\":56403},{\"end\":56420,\"start\":56414},{\"end\":57068,\"start\":57061},{\"end\":57079,\"start\":57072},{\"end\":57089,\"start\":57083},{\"end\":57102,\"start\":57093},{\"end\":57111,\"start\":57106},{\"end\":57122,\"start\":57117},{\"end\":57132,\"start\":57126},{\"end\":57146,\"start\":57136},{\"end\":57155,\"start\":57150},{\"end\":57168,\"start\":57161},{\"end\":57178,\"start\":57172},{\"end\":57189,\"start\":57182},{\"end\":57199,\"start\":57193},{\"end\":57215,\"start\":57203},{\"end\":57824,\"start\":57818},{\"end\":57831,\"start\":57828},{\"end\":58455,\"start\":58447},{\"end\":58695,\"start\":58693},{\"end\":58703,\"start\":58699},{\"end\":58710,\"start\":58707},{\"end\":58717,\"start\":58714},{\"end\":58727,\"start\":58724},{\"end\":59416,\"start\":59412},{\"end\":59431,\"start\":59420},{\"end\":59443,\"start\":59435},{\"end\":59454,\"start\":59447},{\"end\":59461,\"start\":59458},{\"end\":59471,\"start\":59465},{\"end\":59479,\"start\":59475},{\"end\":59491,\"start\":59483},{\"end\":59502,\"start\":59493},{\"end\":59851,\"start\":59844},{\"end\":59863,\"start\":59855},{\"end\":59869,\"start\":59867},{\"end\":59880,\"start\":59875},{\"end\":59891,\"start\":59884},{\"end\":59901,\"start\":59895},{\"end\":59915,\"start\":59905},{\"end\":59925,\"start\":59919},{\"end\":60435,\"start\":60428},{\"end\":60443,\"start\":60437},{\"end\":60724,\"start\":60717},{\"end\":60733,\"start\":60726},{\"end\":60743,\"start\":60737}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":14096841},\"end\":40766,\"start\":40178},{\"attributes\":{\"id\":\"b1\"},\"end\":41028,\"start\":40768},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":755804},\"end\":41358,\"start\":41030},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":195791459},\"end\":41769,\"start\":41360},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":4787508},\"end\":42475,\"start\":41771},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":2555196},\"end\":42969,\"start\":42477},{\"attributes\":{\"id\":\"b6\"},\"end\":43152,\"start\":42971},{\"attributes\":{\"doi\":\"10.18653/v1/N18-1033\",\"id\":\"b7\",\"matched_paper_id\":3718988},\"end\":44099,\"start\":43154},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":4534193},\"end\":44632,\"start\":44101},{\"attributes\":{\"doi\":\"10.18653/v1/2021.naacl-main.133\",\"id\":\"b9\",\"matched_paper_id\":235097487},\"end\":45526,\"start\":44634},{\"attributes\":{\"doi\":\"10.18653/v1/P17-1138\",\"id\":\"b10\",\"matched_paper_id\":17355453},\"end\":46264,\"start\":45528},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":13753208},\"end\":46538,\"start\":46266},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":52051958},\"end\":46869,\"start\":46540},{\"attributes\":{\"doi\":\"10.18653/v1/D17-1153\",\"id\":\"b13\"},\"end\":47078,\"start\":46871},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":235614253},\"end\":47241,\"start\":47080},{\"attributes\":{\"doi\":\"doi: 10.3115/ 1073083.1073135\",\"id\":\"b15\",\"matched_paper_id\":11080756},\"end\":47991,\"start\":47243},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b16\",\"matched_paper_id\":220095789},\"end\":48709,\"start\":47993},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":21850704},\"end\":49056,\"start\":48711},{\"attributes\":{\"doi\":\"10.18653/v1/W15-3049\",\"id\":\"b18\",\"matched_paper_id\":15349458},\"end\":49633,\"start\":49058},{\"attributes\":{\"doi\":\"10.18653/v1/W18-6319\",\"id\":\"b19\",\"matched_paper_id\":13751870},\"end\":50198,\"start\":49635},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":1153355},\"end\":50646,\"start\":50200},{\"attributes\":{\"id\":\"b21\"},\"end\":52784,\"start\":50648},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":7147309},\"end\":53342,\"start\":52786},{\"attributes\":{\"doi\":\"doi: 10.1109/ CVPR.2017.131\",\"id\":\"b23\",\"matched_paper_id\":206594923},\"end\":53791,\"start\":53344},{\"attributes\":{\"doi\":\"abs/1707.06347\",\"id\":\"b24\"},\"end\":54081,\"start\":53793},{\"attributes\":{\"doi\":\"10.18653/v1/2020.acl-main.704\",\"id\":\"b25\",\"matched_paper_id\":215548699},\"end\":54740,\"start\":54083},{\"attributes\":{\"doi\":\"10.18653/v1/P16-1159\",\"id\":\"b26\",\"matched_paper_id\":3913537},\"end\":55479,\"start\":54742},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":8938789},\"end\":56301,\"start\":55481},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":221665105},\"end\":57030,\"start\":56303},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":13756489},\"end\":57743,\"start\":57032},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":7317294},\"end\":58350,\"start\":57745},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":2332513},\"end\":58623,\"start\":58352},{\"attributes\":{\"doi\":\"10.18653/v1/D18-1397\",\"id\":\"b32\",\"matched_paper_id\":52100616},\"end\":59345,\"start\":58625},{\"attributes\":{\"doi\":\"arXiv:2106.04516\",\"id\":\"b33\"},\"end\":59786,\"start\":59347},{\"attributes\":{\"doi\":\"arXiv:1909.08593\",\"id\":\"b34\"},\"end\":60196,\"start\":59788},{\"attributes\":{\"id\":\"b35\"},\"end\":60424,\"start\":60198},{\"attributes\":{\"id\":\"b36\"},\"end\":60630,\"start\":60426},{\"attributes\":{\"id\":\"b37\"},\"end\":60858,\"start\":60632},{\"attributes\":{\"id\":\"b38\"},\"end\":61021,\"start\":60860},{\"attributes\":{\"id\":\"b39\"},\"end\":61218,\"start\":61023}]", "bib_title": "[{\"end\":40227,\"start\":40178},{\"end\":41105,\"start\":41030},{\"end\":41434,\"start\":41360},{\"end\":41821,\"start\":41771},{\"end\":42517,\"start\":42477},{\"end\":43226,\"start\":43154},{\"end\":44118,\"start\":44101},{\"end\":44716,\"start\":44634},{\"end\":45597,\"start\":45528},{\"end\":46366,\"start\":46266},{\"end\":46649,\"start\":46540},{\"end\":47127,\"start\":47080},{\"end\":47305,\"start\":47243},{\"end\":48075,\"start\":47993},{\"end\":48764,\"start\":48711},{\"end\":49116,\"start\":49058},{\"end\":49678,\"start\":49635},{\"end\":50251,\"start\":50200},{\"end\":52840,\"start\":52786},{\"end\":53396,\"start\":53344},{\"end\":54134,\"start\":54083},{\"end\":54794,\"start\":54742},{\"end\":55544,\"start\":55481},{\"end\":56344,\"start\":56303},{\"end\":57057,\"start\":57032},{\"end\":57814,\"start\":57745},{\"end\":58441,\"start\":58352},{\"end\":58689,\"start\":58625}]", "bib_author": "[{\"end\":40241,\"start\":40229},{\"end\":40251,\"start\":40241},{\"end\":40257,\"start\":40251},{\"end\":40266,\"start\":40257},{\"end\":40274,\"start\":40266},{\"end\":40284,\"start\":40274},{\"end\":40299,\"start\":40284},{\"end\":40309,\"start\":40299},{\"end\":40780,\"start\":40768},{\"end\":40795,\"start\":40780},{\"end\":40805,\"start\":40795},{\"end\":40814,\"start\":40805},{\"end\":40822,\"start\":40814},{\"end\":40834,\"start\":40822},{\"end\":40844,\"start\":40834},{\"end\":40852,\"start\":40844},{\"end\":41117,\"start\":41107},{\"end\":41447,\"start\":41436},{\"end\":41454,\"start\":41447},{\"end\":41466,\"start\":41454},{\"end\":41474,\"start\":41466},{\"end\":41478,\"start\":41474},{\"end\":41839,\"start\":41823},{\"end\":41848,\"start\":41839},{\"end\":41857,\"start\":41848},{\"end\":41867,\"start\":41857},{\"end\":41875,\"start\":41867},{\"end\":41887,\"start\":41875},{\"end\":41900,\"start\":41887},{\"end\":41910,\"start\":41900},{\"end\":41921,\"start\":41910},{\"end\":41931,\"start\":41921},{\"end\":41947,\"start\":41931},{\"end\":41957,\"start\":41947},{\"end\":42529,\"start\":42519},{\"end\":42540,\"start\":42529},{\"end\":42549,\"start\":42540},{\"end\":42561,\"start\":42549},{\"end\":42573,\"start\":42561},{\"end\":42589,\"start\":42573},{\"end\":42598,\"start\":42589},{\"end\":42608,\"start\":42598},{\"end\":43046,\"start\":43037},{\"end\":43056,\"start\":43046},{\"end\":43238,\"start\":43228},{\"end\":43245,\"start\":43238},{\"end\":43253,\"start\":43245},{\"end\":43265,\"start\":43253},{\"end\":43276,\"start\":43265},{\"end\":44131,\"start\":44120},{\"end\":44138,\"start\":44131},{\"end\":44731,\"start\":44718},{\"end\":44743,\"start\":44731},{\"end\":45611,\"start\":45599},{\"end\":45622,\"start\":45611},{\"end\":45633,\"start\":45622},{\"end\":46376,\"start\":46368},{\"end\":46659,\"start\":46651},{\"end\":46673,\"start\":46659},{\"end\":46923,\"start\":46914},{\"end\":47139,\"start\":47129},{\"end\":47145,\"start\":47139},{\"end\":47319,\"start\":47307},{\"end\":47329,\"start\":47319},{\"end\":47337,\"start\":47329},{\"end\":47347,\"start\":47337},{\"end\":48087,\"start\":48077},{\"end\":48102,\"start\":48087},{\"end\":48114,\"start\":48102},{\"end\":48776,\"start\":48766},{\"end\":48785,\"start\":48776},{\"end\":48795,\"start\":48785},{\"end\":49129,\"start\":49118},{\"end\":49688,\"start\":49680},{\"end\":50263,\"start\":50253},{\"end\":50275,\"start\":50263},{\"end\":50284,\"start\":50275},{\"end\":50659,\"start\":50650},{\"end\":50671,\"start\":50659},{\"end\":50678,\"start\":50671},{\"end\":50690,\"start\":50678},{\"end\":50702,\"start\":50690},{\"end\":50710,\"start\":50702},{\"end\":50723,\"start\":50710},{\"end\":50736,\"start\":50723},{\"end\":50744,\"start\":50736},{\"end\":50753,\"start\":50744},{\"end\":50767,\"start\":50753},{\"end\":50779,\"start\":50767},{\"end\":50789,\"start\":50779},{\"end\":50801,\"start\":50789},{\"end\":50811,\"start\":50801},{\"end\":50832,\"start\":50811},{\"end\":50847,\"start\":50832},{\"end\":50855,\"start\":50847},{\"end\":50867,\"start\":50855},{\"end\":50877,\"start\":50867},{\"end\":50886,\"start\":50877},{\"end\":50899,\"start\":50886},{\"end\":50908,\"start\":50899},{\"end\":50918,\"start\":50908},{\"end\":50928,\"start\":50918},{\"end\":50939,\"start\":50928},{\"end\":50951,\"start\":50939},{\"end\":50963,\"start\":50951},{\"end\":50969,\"start\":50963},{\"end\":50978,\"start\":50969},{\"end\":50991,\"start\":50978},{\"end\":51006,\"start\":50991},{\"end\":51016,\"start\":51006},{\"end\":51030,\"start\":51016},{\"end\":51042,\"start\":51030},{\"end\":51054,\"start\":51042},{\"end\":51063,\"start\":51054},{\"end\":51074,\"start\":51063},{\"end\":51082,\"start\":51074},{\"end\":51093,\"start\":51082},{\"end\":51107,\"start\":51093},{\"end\":51122,\"start\":51107},{\"end\":51132,\"start\":51122},{\"end\":51145,\"start\":51132},{\"end\":51155,\"start\":51145},{\"end\":51169,\"start\":51155},{\"end\":51185,\"start\":51169},{\"end\":51197,\"start\":51185},{\"end\":51206,\"start\":51197},{\"end\":51218,\"start\":51206},{\"end\":51231,\"start\":51218},{\"end\":51241,\"start\":51231},{\"end\":51249,\"start\":51241},{\"end\":51259,\"start\":51249},{\"end\":51286,\"start\":51259},{\"end\":51292,\"start\":51286},{\"end\":51301,\"start\":51292},{\"end\":51312,\"start\":51301},{\"end\":51326,\"start\":51312},{\"end\":51335,\"start\":51326},{\"end\":51351,\"start\":51335},{\"end\":51358,\"start\":51351},{\"end\":51367,\"start\":51358},{\"end\":51379,\"start\":51367},{\"end\":51390,\"start\":51379},{\"end\":51402,\"start\":51390},{\"end\":51415,\"start\":51402},{\"end\":51426,\"start\":51415},{\"end\":51435,\"start\":51426},{\"end\":51447,\"start\":51435},{\"end\":51459,\"start\":51447},{\"end\":51469,\"start\":51459},{\"end\":51477,\"start\":51469},{\"end\":51488,\"start\":51477},{\"end\":51497,\"start\":51488},{\"end\":51508,\"start\":51497},{\"end\":51519,\"start\":51508},{\"end\":51531,\"start\":51519},{\"end\":51546,\"start\":51531},{\"end\":51556,\"start\":51546},{\"end\":52853,\"start\":52842},{\"end\":52863,\"start\":52853},{\"end\":52871,\"start\":52863},{\"end\":52882,\"start\":52871},{\"end\":53410,\"start\":53398},{\"end\":53423,\"start\":53410},{\"end\":53433,\"start\":53423},{\"end\":53441,\"start\":53433},{\"end\":53449,\"start\":53441},{\"end\":53846,\"start\":53834},{\"end\":53856,\"start\":53846},{\"end\":53868,\"start\":53856},{\"end\":53879,\"start\":53868},{\"end\":53889,\"start\":53879},{\"end\":54146,\"start\":54136},{\"end\":54153,\"start\":54146},{\"end\":54163,\"start\":54153},{\"end\":54804,\"start\":54796},{\"end\":54813,\"start\":54804},{\"end\":54819,\"start\":54813},{\"end\":54825,\"start\":54819},{\"end\":54831,\"start\":54825},{\"end\":54838,\"start\":54831},{\"end\":54845,\"start\":54838},{\"end\":55556,\"start\":55546},{\"end\":55564,\"start\":55556},{\"end\":55576,\"start\":55564},{\"end\":55589,\"start\":55576},{\"end\":55600,\"start\":55589},{\"end\":56358,\"start\":56346},{\"end\":56368,\"start\":56358},{\"end\":56374,\"start\":56368},{\"end\":56385,\"start\":56374},{\"end\":56393,\"start\":56385},{\"end\":56401,\"start\":56393},{\"end\":56412,\"start\":56401},{\"end\":56422,\"start\":56412},{\"end\":56435,\"start\":56422},{\"end\":56441,\"start\":56435},{\"end\":57070,\"start\":57059},{\"end\":57081,\"start\":57070},{\"end\":57091,\"start\":57081},{\"end\":57104,\"start\":57091},{\"end\":57113,\"start\":57104},{\"end\":57124,\"start\":57113},{\"end\":57134,\"start\":57124},{\"end\":57148,\"start\":57134},{\"end\":57157,\"start\":57148},{\"end\":57170,\"start\":57157},{\"end\":57180,\"start\":57170},{\"end\":57191,\"start\":57180},{\"end\":57201,\"start\":57191},{\"end\":57217,\"start\":57201},{\"end\":57227,\"start\":57217},{\"end\":57826,\"start\":57816},{\"end\":57833,\"start\":57826},{\"end\":58457,\"start\":58443},{\"end\":58697,\"start\":58691},{\"end\":58705,\"start\":58697},{\"end\":58712,\"start\":58705},{\"end\":58719,\"start\":58712},{\"end\":58729,\"start\":58719},{\"end\":59418,\"start\":59410},{\"end\":59433,\"start\":59418},{\"end\":59445,\"start\":59433},{\"end\":59456,\"start\":59445},{\"end\":59463,\"start\":59456},{\"end\":59473,\"start\":59463},{\"end\":59481,\"start\":59473},{\"end\":59493,\"start\":59481},{\"end\":59504,\"start\":59493},{\"end\":59853,\"start\":59840},{\"end\":59865,\"start\":59853},{\"end\":59871,\"start\":59865},{\"end\":59882,\"start\":59871},{\"end\":59893,\"start\":59882},{\"end\":59903,\"start\":59893},{\"end\":59917,\"start\":59903},{\"end\":59927,\"start\":59917},{\"end\":60437,\"start\":60426},{\"end\":60445,\"start\":60437},{\"end\":60726,\"start\":60715},{\"end\":60735,\"start\":60726},{\"end\":60745,\"start\":60735}]", "bib_venue": "[{\"end\":40381,\"start\":40367},{\"end\":43589,\"start\":43440},{\"end\":44321,\"start\":44226},{\"end\":45051,\"start\":44918},{\"end\":45831,\"start\":45742},{\"end\":46399,\"start\":46396},{\"end\":46700,\"start\":46695},{\"end\":47568,\"start\":47465},{\"end\":48245,\"start\":48223},{\"end\":49288,\"start\":49219},{\"end\":49862,\"start\":49785},{\"end\":50441,\"start\":50371},{\"end\":52985,\"start\":52964},{\"end\":54353,\"start\":54281},{\"end\":55041,\"start\":54954},{\"end\":55836,\"start\":55712},{\"end\":58024,\"start\":57926},{\"end\":58925,\"start\":58837},{\"end\":40365,\"start\":40309},{\"end\":40885,\"start\":40852},{\"end\":41153,\"start\":41117},{\"end\":41530,\"start\":41478},{\"end\":42006,\"start\":41957},{\"end\":42657,\"start\":42608},{\"end\":43035,\"start\":42971},{\"end\":43438,\"start\":43296},{\"end\":44224,\"start\":44138},{\"end\":44916,\"start\":44774},{\"end\":45740,\"start\":45653},{\"end\":46394,\"start\":46376},{\"end\":46693,\"start\":46673},{\"end\":46912,\"start\":46871},{\"end\":47149,\"start\":47145},{\"end\":47463,\"start\":47376},{\"end\":48193,\"start\":48118},{\"end\":48847,\"start\":48795},{\"end\":49217,\"start\":49149},{\"end\":49783,\"start\":49708},{\"end\":50369,\"start\":50284},{\"end\":52938,\"start\":52882},{\"end\":53546,\"start\":53476},{\"end\":53832,\"start\":53793},{\"end\":54279,\"start\":54192},{\"end\":54952,\"start\":54865},{\"end\":55710,\"start\":55600},{\"end\":56490,\"start\":56441},{\"end\":57276,\"start\":57227},{\"end\":57924,\"start\":57833},{\"end\":58473,\"start\":58457},{\"end\":58835,\"start\":58749},{\"end\":59408,\"start\":59347},{\"end\":59838,\"start\":59788},{\"end\":60310,\"start\":60198},{\"end\":60523,\"start\":60445},{\"end\":60713,\"start\":60632},{\"end\":60939,\"start\":60860},{\"end\":61118,\"start\":61023}]"}}}, "year": 2023, "month": 12, "day": 17}