{"id": 219636007, "updated": "2023-10-06 14:14:26.696", "metadata": {"title": "Ensemble Distillation for Robust Model Fusion in Federated Learning", "authors": "[{\"first\":\"Tao\",\"last\":\"Lin\",\"middle\":[]},{\"first\":\"Lingjing\",\"last\":\"Kong\",\"middle\":[]},{\"first\":\"Sebastian\",\"last\":\"Stich\",\"middle\":[\"U.\"]},{\"first\":\"Martin\",\"last\":\"Jaggi\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 6, "day": 12}, "abstract": "Federated Learning (FL) is a machine learning setting where many devices collaboratively train a machine learning model while keeping the training data decentralized. In most of the current training schemes the central model is refined by averaging the parameters of the server model and the updated parameters from the client side. However, directly averaging model parameters is only possible if all models have the same structure and size, which could be a restrictive constraint in many scenarios. In this work we investigate more powerful and more flexible aggregation schemes for FL. Specifically, we propose ensemble distillation for model fusion, i.e. training the central classifier through unlabeled data on the outputs of the models from the clients. This knowledge distillation technique mitigates privacy risk and cost to the same extent as the baseline FL algorithms, but allows flexible aggregation over heterogeneous client models that can differ e.g. in size, numerical precision or structure. We show in extensive empirical experiments on various CV/NLP datasets (CIFAR-10/100, ImageNet, AG News, SST2) and settings (heterogeneous models/data) that the server model can be trained much faster, requiring fewer communication rounds than any existing FL technique so far.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "2006.07242", "mag": "3105298093", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/LinKSJ20", "doi": null}}, "content": {"source": {"pdf_hash": "b5c04070033a102d77603f5ae7a690e8297fe18b", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2006.07242v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "ba055601ecfa884ca5de8c1da8d077b031df8950", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/b5c04070033a102d77603f5ae7a690e8297fe18b.txt", "contents": "\nENSEMBLE DISTILLATION FOR ROBUST MODEL FU- SION IN FEDERATED LEARNING\n\n\nTao Lin tao.lin@epfl.ch \nLingjing Kong lingjing.kong@epfl.ch \nSebastian U Stich sebastian.stich@epfl.ch \nEPFL, SwitzerlandMartin Jaggi Mlo \nENSEMBLE DISTILLATION FOR ROBUST MODEL FU- SION IN FEDERATED LEARNING\n\nFederated Learning (FL) is a machine learning setting where many devices collaboratively train a machine learning model while keeping the training data decentralized. In most of the current training schemes the central model is refined by averaging the parameters of the server model and the updated parameters from the client side. However, directly averaging model parameters is only possible if all models have the same structure and size, which could be a restrictive constraint in many scenarios. In this work we investigate more powerful and more flexible aggregation schemes for FL. Specifically, we propose ensemble distillation for model fusion, i.e. training the central classifier through unlabeled data on the outputs of the models from the clients. This knowledge distillation technique mitigates privacy risk and cost to the same extent as the baseline FL algorithms, but allows flexible aggregation over heterogeneous client models that can differ e.g. in size, numerical precision or structure. We show in extensive empirical experiments on various CV/NLP datasets (CIFAR-10/100, ImageNet, AG News, SST2) and settings (heterogeneous models/data) that the server model can be trained much faster, requiring fewer communication rounds than any existing FL technique so far. * Equal contribution.\n\nINTRODUCTION\n\nFederated Learning (FL) has emerged as an important machine learning paradigm in which a federation of clients participate in collaborative training of a centralized model (Shokri & Shmatikov, 2015;McMahan et al., 2016;Smith et al., 2017;Caldas et al., 2018;Bonawitz et al., 2019;Li et al., 2019;Kairouz et al., 2019). The clients send their model parameters to the server but never their private training datasets, thereby ensuring a basic level of privacy. Among the key challenges in federated training are communication overheads and delays (one would like to train the central model with as few communication rounds as possible), and client heterogeneity: the training data (non-i.i.d.-ness), as well as hardware and computing resources, can change drastically among clients, for instance when training on commodity mobile devices.\n\nClassic training algorithms in FL, such as federated averaging (FEDAVG) (McMahan et al., 2016) and its recent adaptations (Mohri et al., 2019;Li et al., 2020;Hsu et al., 2019;Karimireddy et al., 2019;Hsu et al., 2020;Reddi et al., 2020), are all based on directly averaging of the participating client's parameters and can hence only be applied if all client's models have the same size and structure. In contrast, ensemble learning methods (You et al., 2017;Furlanello et al., 2018;Anil et al., 2018;Dvornik et al., 2019;Park & Kwak, 2019;Liu et al., 2019;Wu et al., 2019) allow to combine multiple heterogeneous weak classifiers by averaging the predictions of the individual models instead. However, applying ensemble learning techniques directly in FL is infeasible in practice due to the large number of participating clients. Storing a different model per client on the server is not only impossible due to memory constraints, but also renders training and inference inefficient, and hinders knowledge transfer between clients.\n\nTo enable federated learning in more realistic settings, we propose to use ensemble distillation (Bucilu\u01ce et al., 2006;Hinton et al., 2015) for robust model fusion (FedDF). Our scheme leverages unlabeled data or artificially generated examples (e.g. by a GAN's generator (Goodfellow et al., 2014)) to aggregate knowledge from all received (heterogeneous) client models. We demonstrate with thorough empirical results that our ensemble distillation approach not only addresses the existing quality loss issue (Hsieh et al., 2019) of Batch Normalization (BN) (Ioffe & Szegedy, 2015) for networks in a homogeneous FL system, but can also break the knowledge barriers among heterogeneous client models. Our main contributions are:\n\n\u2022 We propose a distillation framework for robust federated model fusion, which allows for heterogeneous client models and data, and is robust to the choices of neural architectures. \u2022 We show in extensive numerical experiments on various CV/NLP datasets (CIFAR-10/100, Ima-geNet, AG News, SST2) and settings (heterogeneous models and/or data) that the server model can be trained much faster, requiring fewer communication rounds than any existing FL technique.\n\nWe further provide insights on when FedDF can outperform FEDAVG (see also Fig. 1 that highlights an intrinsic limitation of parameter averaging based approaches) and what factors influence FedDF.  (2019); Wang et al. (2020) propose to use optimal transport and other alignment schemes to first align or match individual neurons of the neural nets layer-wise before averaging the parameters. However, these layer-based alignment schemes necessitate client models with the same number of layers and structure, which is restrictive in heterogeneous systems in practice. Another line of work aims to improve local client training, i.e., client-drift problem caused by the heterogeneity of local data (Li et al., 2018;Karimireddy et al., 2019). For example, FEDPROX (Li et al., 2018) incorporates a proximal term for the local training. Other techniques like acceleration, recently appear in preprints (Hsu et al., 2019;2020;Reddi et al., 2020).\n\n\nRELATED WORK\n\nKnowledge distillation. Knowledge distillation for neural networks is first introduced in Bucilu\u01ce et al. (2006); Hinton et al. (2015). By encouraging the student model to approximate the output logits of the teacher model, the student is able to imitate the teacher's behavior with marginal quality loss (Romero et al., 2015;Zagoruyko & Komodakis, 2016;Kim et al., 2018;Tung & Mori, 2019;Koratana et al., 2019;Huang & Wang, 2017;Ahn et al., 2019;Tian et al., 2019). Some work study the ensemble distillation, i.e., distilling the knowledge of an ensemble of teacher models to a student model. To this end, existing approaches either average the logits from the ensemble of teacher models (You et al., 2017;Furlanello et al., 2018;Anil et al., 2018;Dvornik et al., 2019), or extract knowledge from the feature level (Park & Kwak, 2019;Liu et al., 2019;Wu et al., 2019).\n\nMost of these schemes rely on using the original training data for the distillation process. In cases where real data is unavailable, some recent work (Nayak et al., 2019;Micaelli & Storkey, 2019) demonstrate that distillation can be accomplished by crafting pseudo data either from the weights of the teacher model or through a generator adversarially trained with the student. FedDF can be combined with all of these approaches. In this work, we consider unlabeled datasets for ensemble distillation, which could be either collected from other domains or directly generated from a pretrained generator.\n\nComparison with close FL work. Guha et al. (2019) propose \"one-shot fusion\" through unlabeled data for SVM loss objective, whereas we consider multiple-round scenarios on diverse neural architectures and tasks. FD (Jeong et al., 2018) utilizes distillation to reduce FL communication costs. To this end, FD synchronizes logits per label which are accumulated during the local training. The averaged logits per label (over local steps and clients) will then be used as a distillation regularizer for the next round's local training. Compared to FEDAVG, FD experiences roughly 15% quality drop on MNIST. In contrast, FedDF shows superior learning performance over FEDAVG and can significantly reduce the number of communication rounds to reach target accuracy on diverse challenging tasks. FedMD (Li & Wang, 2019) and the recently proposed Cronus (Chang et al., 2019) consider learning through averaged logits per sample on a public dataset. After the initial pre-training on the labeled public dataset, FedMD learns on the public and private dataset iteratively for personalization, whereas in Cronus, the public dataset (with soft labels) is used jointly with local private data for the local training. As FedMD trains client models simultaneously on both labeled public and private datasets, the model classifiers have to include all classes from both datasets. Cronus, in its collaborative training phase, mixes public and private data for local training. Thus for these methods, the public dataset construction requires careful deliberation and even prior knowledge on clients' private data. Moreover, how these modifications impact local training quality remains unclear. FedDF faces no such issues: we show that FedDF is robust to distillation dataset selection and the distillation is performed on the server side, leaving local training unaffected.\n\n\nENSEMBLE DISTILLATION FOR ROBUST MODEL FUSION\n\nAlgorithm 1 Illustration of FedDF on K homogeneous clients (indexed by k) for T rounds, n k denotes the number of data points per client and C the fraction of clients participating in each round. The server model is initialized as x0. While FEDAVG just uses the averaged models xt,0, we perform N iterations of server-side model fusion on top (line 7 -line 10).\n\n1: procedure SERVER 2:\n\nfor each communication round t = 1, . . . , T do 3:\n\nS t \u2190 random subset (C fraction) of the K clients 4:\n\nfor each client k \u2208 S t in parallel do use ensemble of {x k t } k\u2208St to update server student x t,j\u22121 through AVGLOGITS 10:\n5:x k t \u2190 Client-LocalUpdate(k, x t\u22121 ) detailed in\nx t \u2190 x t,N\n\n\n11\n\n:\nreturn x T\nIn this section, we first introduce the core idea of the proposed Federated Distillation Fusion (FedDF). We then comment on its favorable characteristics and discuss possible extensions.\n\nEnsemble distillation. We first discuss the key features of FedDF for the special case of homogeneous models, i.e. when all clients share the same network architecture (Algorithm 1). For model fusion, the server distills the ensemble of |S t | client teacher models to one single server student model. For the distillation, the teacher models are evaluated on mini-batches of unlabeled data on the server (forward pass) and their logit outputs (denoted by f (x k t , d) for mini-batch d) are used to train the student model on the server:\nxt,j := xt,j\u22121 \u2212 \u03b7 \u2202KL \u03c3 1 |S t | k\u2208S t f (x k t , d) , \u03c3 (f (xt,j\u22121, d)) \u2202xt,j\u22121 .(AVGLOGITS)\nHere KL stands for Kullback-Leibler divergence, \u03c3 is the softmax function, and \u03b7 is the stepsize.\n\nFedDF can easily be extended to heterogeneous FL systems (Algorithm 3 and Figure 7). We assume the system contains p distinct model prototypes that potentially differ in neural architecture, structure and numerical precision. The ensemble distillation then refers to distilling the knowledge of all received client models, regardless of the model prototype, to the p server models (parallelizable); in the next round, each activated client receives the corresponding fused prototype model. Notably, as the fusion takes place on the server side, there is no additional burden and interference on clients.\n\nUtilizing unlabeled/generated data for distillation. Unlike most existing ensemble distillation methods that rely on labeled data from the training domain, we demonstrate the feasibility of achieving model fusion by using unlabeled datasets from other domains for the sake of privacy-preserving FL. Our proposed method also allows the use of synthetic data from a pre-trained generator (e.g. GAN) as distillation data to alleviate potential limitations (e.g. acquisition, storage) of real unlabeled datasets.\n\nDiscussions on privacy-preserving extension. Our proposed model fusion framework in its simplest form-like most existing FL methods-requires to exchange models between the server and each client, resulting in potential privacy leakage due to e.g. memorization present in the models. Several existing protection mechanisms can be added to our framework to protect clients from adversaries. These include adding differential privacy (Geyer et al., 2017) (Zhang et al., 2015)) and a 2-class classification task (Stanford Sentiment Treebank, SST2 (Socher et al., 2013)). The validation dataset is created for CIFAR-10/100, ImageNet, and SST2, by holding out 10%, 1% and 1% of the original training samples respectively; the remaining training samples are used as the training dataset (before partitioning client data) and the whole procedure is controlled by random seeds. We use validation/test datasets on the server and report the test accuracy over three different random seeds.\n\nHeterogeneous distribution of client data. We use the Dirichlet distribution as in Yurochkin et al.    Top-1 accuracy on test dataset 50% data, FedAvg 100% data, FedAvg 50% data, FedDF 100% data, FedDF    The local training procedure. The FL algorithm randomly samples a fraction (C) of clients per communication round for local training. For the sake of simplicity, the local training in our experiments uses a constant learning rate (no decay), no Nesterov momentum acceleration, and no weight decay. The hyperparameter tuning procedure is deferred to Appendix B.2. Unless mentioned otherwise the learning rate is set to 0.1 for ResNet-like nets, 0.05 for VGG, and 1e\u22125 for DistilBERT.\n(b) \u03b1 = 1.\nThe model fusion procedure. We evaluate the performance of FedDF by utilizing either randomly sampled data from existing (unlabeled) datasets 4 or BigGAN's generator (Brock et al., 2019). Unless mentioned otherwise we use CIFAR-100 and downsampled ImageNet (image size 32) as the distillation datasets for FedDF on CIFAR-10 and CIFAR-100 respectively. Adam with learning rate 2e\u22123 is used to distill knowledge from the ensemble of received local models. We employ early-stopping to stop distillation after the validation performance plateaus for 1e3 steps (total 1e4 update steps). The hyperparameter choice of FedDF used for model fusion is kept constant across different tasks.\n\n\nEVALUATION ON THE COMMON FEDERATED LEARNING SETTINGS\n\nPerformance overview for different FL scenarios. We can observe from Figure 2 that FedDF consistently outperforms FEDAVG for all client fractions and non-i.i.d. degrees when the local training is reasonably sufficient (e.g. over 40 epochs).\n\nFedDF benefits from larger numbers of local training epochs. This is because the performance of the model ensemble is highly dependent on the diversity among its individual models (Kuncheva & Whitaker, 2003;Sollich & Krogh, 1996). Thus longer local training leads to greater diversity and quality of the ensemble and hence a better distillation result for the fused model. This characteristic is desirable in practice as it helps reduce the communication overhead in FL systems. In contrast, the performance of FEDAVG saturates and even degrades with the increased number of local epochs, which is consistent with  2020)) targeting the issue of non-i.i.d. local data. We believe combining FedDF with these techniques can lead to a more robust FL, which we leave as future work.\n\nDetailed comparison of FedDF with other SOTA federated learning methods for CV tasks. Table 1 summarizes the results for various degrees of non-i.i.d. data, local training epochs and client 4 Note the actual computation expense for distillation is determined by the product of the number of distillation steps and distillation mini-batch size (128 in all experiments), rather than the distillation dataset size. Table 1: Evaluating different FL methods in different scenarios (i.e. different client sampling fractions, # of local epochs and target accuracies), in terms of the number of communication rounds to reach target top-1 test accuracy. We evaluate on ResNet-8 with CIFAR-10. For each communication round, a fraction C of the total 20 clients will be randomly selected. T denotes the specified target top-1 test accuracy. Hyperparameters are fine-tuned for each method (FEDAVG, FEDPROX, and FEDAVGM); FedDF uses the optimal learning rate from FEDAVG. The performance upper bound of (tuned) centralized training is 86% (trained on all local data).\n\nThe number of communication rounds to reach target performance T Local epochs  sampling fractions. In all scenarios, FedDF requires significantly fewer communication rounds than other SOTA methods to reach designated target accuracies. The benefits of FedDF can be further pronounced by taking more local training epochs as illustrated in Figure 2.\nC = 0.2 C = 0.4 C = 0.8 \u03b1 = 1, T = 80% \u03b1 = 0.1, T = 75% \u03b1 = 1, T = 80% \u03b1 = 0.1, T = 75% \u03b1 = 1, T = 80% \u03b1 = 0.1, T =\nAll competing methods have strong difficulties with increasing data heterogeneity (non-i.i.d. data, i.e. smaller \u03b1), while FedDF shows significantly improved robustness to data heterogeneity. In most scenarios in Table 1, the reduction of \u03b1 from 1 to 0.1 almost triples the number of communication rounds for FEDAVG, FEDPROX and FEDAVGM to reach target accuracies, whereas less than twice the number of rounds are sufficient for FedDF.\n\nIncreasing the sampling ratio makes a more noticeable positive impact on FedDF compared to other methods. We attribute this to the fact that an ensemble tends to improve in robustness and quality, with a larger number of reasonable good participants, and hence results in better model fusion. Nevertheless, even in cases with a very low sampling fraction (i.e. C = 0.2), FedDF still maintains a considerable leading margin over the closest competitor.  (Wu & He, 2018) can alleviate some of the quality loss brought by BN due to the discrepancies between local data distributions.\n\nAs shown in Table 2, despite additional effort on architecture modification and hyperparameter tuning (i.e. the number of groups in GN), baseline methods with GN replacement still lag much behind FedDF. FedDF provides better model fusion which is robust to non-i.i.d. data, and is compatible with BN, thus avoids extra efforts for modifying the standard SOTA neural architectures. Figure 13 in Appendix B.3 shows the complete learning curves.\n\nWe additionally evaluate architectures originally designed without BN (i.e. VGG), to demonstrate the broad applicability of FedDF. Due to the lack of normalization layers, VGG is vulnerable to non-i.i.d. local distributions. We observe that received models on the server might output random prediction   For simplicity, we consider 10 clients with C = 100% participation ratio and \u03b1 = 1; the number of local training epochs per communication round (10 rounds in total) is set to 10 and 1 respectively. The 50% of the original training dataset is used for the federated fine-tuning (for all methods) and the left 50% is used as the unlabeled distillation dataset for FedDF.\n\nresults on the validation/test dataset and hence give rise to uninformative results overwhelmed by large variance (as shown in Table 3). We address this issue by a simple treatment 5 , \"drop-worst\", i.e., dropping learners with random predictions on the server validation dataset (e.g. 10% accuracy for CIFAR-10), in each round before applying model averaging and/or ensemble distillation. Table 3 examines the FL methods (FEDAVG, FEDPROX, FEDMA and FedDF) on VGG-9; FedDF consistently outperforms other methods by a large margin for different communication rounds.\n\nExtension to NLP tasks for federated fine-tuning of DistilBERT. Fine-tuning a pre-trained transformer language model like BERT (Devlin et al., 2018) yields SOTA results on various NLP benchmarks (Wang et al., 2018;. DistilBERT (Sanh et al., 2019) is a lighter version of BERT with only marginal quality loss on downstream tasks. As a proof of concept, in Figure 3 we consider federated fine-tuning of DistilBERT on non-i.i.d. local data (\u03b1 = 1, depicted in Figure 11). For both AG News and SST2 datasets, FedDF achieves significantly faster convergence than FEDAVG and consistently outperforms the latter.\n\n\nCASE STUDIES\n\nFederated learning for low-bit quantized models. FL for the Internet of Things (IoT) involves edge devices with diverse hardware, e.g. different computational capacities. Network quantization is hence of great interest to FL by representing the activations/weights in low precision, with benefits of significantly reduced local computational footprints and communication costs.     Figure 4 visualizes the training dynamics of FedDF and FEDAVG 6 in a heterogeneous system with three distinct architectures, i.e., ResNet-20, MobileNetV2, and ShuffleNetV2. On CIFAR-10/100 and ImageNet, FedDF dominates FEDAVG on test accuracy in each communication round with much less variance. Each fused model exhibits marginal quality loss compared to the ensemble performance, which suggests unlabeled datasets from other domains are sufficient for model fusion. Besides, the gap between the fused model and the ensemble one widens when the training dataset contains a much larger number of classes 7 than that of the distillation dataset. In Section 5, we study this underlying interaction between training data and unlabeled distillation data in detail.\n\n\nUNDERSTANDING FEDDF\n\nFedDF consists of two chief components: ensembling and knowledge distillation via out-of-domain data. In this section, we first investigate what affects the ensemble performance on the global distribution (test domain) through a generalization bound. We then provide empirical understanding of how different attributes of the out-of-domain distillation dataset affect the student performance on the global distribution.  Generalization bound. Theorem 5.1 provides insights into ensemble performance on the global distribution. Detailed description and derivations are deferred to Appendix C. 6 Model averaging is only performed among models with identical structures. 7 # of classes is a proxy measurement for distribution shift; labels are not used in our distillation procedure.  Theorem 5.1 (informal). We denote the global distribution as D, the k-th local distribution and its empirical distribution as D k andD k respectively. The hypothesis h \u2208 H learned onD k is denoted by hD k . The upper bound on the risk of the ensemble of K local models on D mainly consists of 1) the empirical risk of a model trained on the global empirical distributionD = 1 K kD k , and 2) terms dependent on the distribution discrepancy between D k and D, with the probability 1 \u2212 \u03b4:\nLD 1 K k hD k \u2264 LD(hD) + 1 K k 1 2 dH\u2206H(D k , D) + \u03bb k + log 2K \u03b4 2m ,\nwhere d H\u2206H measures the distribution discrepancy between two distributions (Ben-David et al., 2010), m is the number of samples per local distribution, and \u03bb k is the minimum of the combined\nloss L D (h)+L D k (h), \u2200h \u2208 H.\nThe ensemble of the local models sets the performance upper bound for the later distilled model on the global distribution as shown in Figure 4. Theorem 5.1 shows that compared to a model trained on the global empirical distribution (ideal centralized case), the performance of the ensemble on the global distribution is associated with the discrepancy between local distributions D k 's and the global distribution D. Besides, the shift between the distillation and the global distribution determines the knowledge transfer quality between these two distributions and hence the test performance of the fused model. In the following, we empirically examine how the choice of distillation data distributions and the number of distillation steps influence the quality of ensemble knowledge distillation.\n\nSource, diversity and size of the distillation dataset. The fusion in FedDF demonstrates remarkable consistency across a wide range of realistic data sources as shown in Figure 5, although an abrupt performance declination is encountered when the distillation data are sampled from a dramatically different manifold (e.g. random noise). Notably, synthetic data from the generator of a pre-trained GAN does not incur noticeable quality loss, opening up numerous possibilities for effective and efficient model fusion. Figure 6(a) illustrates that in general the diversity of the distillation data does not significantly impact the performance of ensemble distillation, though the optimal performance is achieved when two domains have a similar number of classes. Figure 6(b) shows the FedDF is not demanding on the distillation dataset size: even 1% of data (\u223c 48% of the local training dataset) can result in a reasonably good fusion performance.\n\nDistillation steps. Figure 6(c) depicts the impact of distillation steps on fusion performance, where FedDF with a moderate number of the distillation steps is able to approach the optimal performance. For example, 100 distillation steps in Figure 6(c), which corresponds to 5 local epochs of CIFAR-100 (partitioned by 20 clients), suffice to yield satisfactory performance. Thus FedDF introduces minor time-wise expense. Algorithm 2 Illustration of local client update in FEDAVG. The K clients are indexed by k; P k indicates the set of indexes of data points on client k, and n k = |P k |. E is the number of local epochs, and \u03b7 is the learning rate. evaluates the loss on model weights for a mini-batch of an arbitrary size.\n\n1: procedure CLIENT-LOCALUPDATE(k, x k t\u22121 ) 2:\n\nClient k receives x k t\u22121 from server and copies it as x k t 3:\n\nfor each local epoch i from 1 to E do 4:\n\nfor mini-batch b \u2282 P k do 5:\nx k t \u2190 x k t \u2212 \u03b7 \u2202 (x k t ;b) \u2202x k t\ncan be arbitary optimizers (e.g. Adam) 6: return x k t to server\n\nAlgorithm 3 illustrates the model fusion of FedDF for the FL system with heterogeneous model prototypes. The schematic diagram is presented in Figure 7. To perform model fusion in such heterogeneous scenarios, FedDF constructs several prototypical models on the server. Each prototype represents all clients with identical architecture/size/precision etc. initialize HashMapC: map each model prototype to the associated clients.\n\n\n5:\n\nfor each communication round t = 1, . . . , T do 6:\n\nS t \u2190 a random subset (C fraction) of the K clients 7:\n\nfor each client k \u2208 S t in parallel do Unless mentioned (i.e. Table 1), otherwise the learning rate is set to 0.1 for ResNet like architectures (e.g. ResNet-8, ResNet-20, MobileNetV2, ShuffleNetV2), 0.05 for VGG and 1e\u22125 for DistilBERT. When comparing with other methods, e.g. FEDPROX, FEDAVGM, we always tune their corresponding hyperparameters (e.g. proximal factor in FEDPROX and momentum factor in FEDAVGM).\n\nExperiment details of FEDMA. We detail our attempts of reproducing FEDMA experiments on VGG-9 with CIFAR-10 in this section. We clone their codebase from GitHub and add functionality to sample clients after synchronizing the whole model.\n\nDifferent from other methods evaluated in the paper, FEDMA uses a layer-wise local training scheme.\n\nFor each round of the local training, the involved clients only update the model parameters from one specific layer onwards, while the already matched layers are frozen. The fusion (matching) is only performed on the chosen layer. Such a layer is gradually chosen from the bottom layer to the top layer, following a bottom-up fashion (Wang et al., 2020). One complete model update cycle of FEDMA requires more frequent (but slightly cheaper) communication, which is equivalent to the number of layers in the neural network.\n\nIn our experiments of FEDMA, the number of local training epochs is 5 epochs per layer (45 epochs per model update), which is slightly larger than 40 epochs used by other methods. We ensure a similar 8 number of model updates in terms of the whole model. We consider global-wise learning rate, different from the layer-wise one in Wang et al. (2020). We also turn off the momentum and weight decay during the local training for a consistent evaluation. The implementation of VGG-9 follows https://github.com/kuangliu/pytorch-cifar/.\n\nThe detailed experimental setup for FedDF (low-bit quantized models). FedDF increases the feasibility of robust model fusion in FL for binarized ResNet-8. As stated in Table 4 (Section 4.3), we employ the \"Straight-through estimator\" (Bengio et al., 2013;Hinton, 2012;Hubara et al., 2016; or the \"error-feedback\" (Lin et al., 2020a) to simulate the on-device local training of the binarized ResNet-8. For each communication round, the server of the FL system will receive locally trained and binarized ResNet-8 from activated clients. The server will then distill the knowledge of these low-precision models to a full-precision one 9 and broadcast to newly activated clients for the next communication round. For the sake of simplicity, the case study demonstrated in the paper only considers reducing the communication cost (from clients to the server), and the local computational cost; a thorough investigation on how to perform a communication-efficient and memory-efficient FL is left as future work.\n\nThe synthetic formulation of non-i.i.d. client data. Assume every client training example is drawn independently with class labels following a categorical distribution over M classes parameterized by a vector q (q i \u2265 0, i \u2208 To better understand the local data distribution for the datasets we considered in the experiments, we visualize the partition results of CIFAR-10 and CIFAR-100 on \u03b1 = {0.01, 0.1, 0.5, 1, 100} for 20 clients, in Figure 9 and Figure 10, respectively.\n\nIn Figure 11 we visualize the partitioned local data on 10 clients with \u03b1 = 1, for AG News and SST-2. 8 The other methods use 40 local training epochs per whole model update. Given the fact of layer-wise training scheme in FEDMA, as well as the used 9-layer VGG (same as the one used in Wang et al. (2020) and we are unable to adapt their code to other architectures due to their hard-coded architecture manipulations), we decide to slightly increase the number of local epochs per layer for FEDMA. 9 The training of the binarized network requires to maintain a full-precision model (Hubara et al., 2016;Lin et al., 2020a) for model update (quantized/pruned model is used during the backward pass). 10 We heavily borrowed the partition description of Hsu et al. (2019) for the completeness of the paper. B.3 SOME EMPIRICAL UNDERSTANDING OF FEDAVG Figure 12 reviews the general behaviors of FEDAVG under different non-iid degrees of local data, different local data sizes, different numbers of local epochs per communication round, as well as the learning rate schedule during the local training. Since we cannot observe the benefits of decaying the learning rate during the local training phase, we turn off the learning rate decay for the experiments in the main text.\n\nIn Figure 13, we visualize the learning curves of training ResNet-8 on CIFAR-10 with different normalization techniques. The numerical results correspond to Table 2  (c) \u03b1 = 0.01 Figure 12: The ablation study of FEDAVG for different # of local epochs and learning rate schedules, for standard federated learning on CIFAR-10 with ResNet-8. For each communication round (100 in total), 40% of the total 20 clients will be randomly selected. We use \u03b1 to synthetically control the non-iid degree of the local data, as in Yurochkin et al. (2019);Hsu et al. (2019). The smaller \u03b1, the larger discrepancy between local data distributions (\u03b1 = 100 mimics identical local data distributions). We report the top-1 accuracy (on three different seeds) on the test dataset. In this section, we empirically study the importance of the initialization (before performing ensemble distillation) in FedDF. Table 5 demonstrates the performance difference of FedDF for two different model initialization schemes: 1) \"from average\", where the uniformly averaged model from this communication round is used as the initial model (i.e. the default design choice of FedDF as illustrated in Algorithm 1 and Algorithm 3); and 2) \"from previous\", where we initialize the model for ensemble distillation by utilizing the fusion result of FedDF from the previous communication round. The noticeable performance differences illustrated in Table 5 identify the importance of using the uniformly averaged model 11 (from the current communication round) as a starting model for better ensemble distillation.       , 40% of the total 20 clients will be randomly selected. We report the top-1 accuracy (on three different seeds) on the test dataset.\n\n\nC DETAILS ON GENERALIZATION BOUNDS\n\nThe derivation of the generalization bound starts from the following notations. In FL, each client has access to its own data distribution D i over domain \u039e := X \u00d7 Y, where X \u2208 R d is the input space and Y is the output space. The global distribution on the server is denoted as D. For the empirical distribution by the given dataset, we assume that each local model has access to an equal amount (m) of local data. Thus, each local empirical distribution has equal contribution to the global empirical distribution:D = 1 K K k=1D k , whereD k denotes the empirical distribution from client k. For our analysis we assume a binary classification task, with hypothesis h as a function h : X \u2192 {0, 1}. The loss function of the task is defined as (h(x), y) = |\u0177 \u2212 y|, where\u0177 := h(x). Note that (\u0177, y) is convex with respect to\u0177. We denote arg min h\u2208H LD(h) by hD.\n\nThe theorem below leverages the domain measurement tools developed in multi-domain learning theory (Ben-David et al., 2010) and provides insights for the generalization bound of the ensemble 12 of local models (trained on local empirical distributionD i ).\n\nTheorem C.1. The difference between L D ( 1 K k hD k ) and LD(hD), i.e., the distance between the risk of our \"ensembled\" model in FedDF and the empirical risk of the \"virtual ERM\" with access to all local data, can be bounded with probability at least 1 \u2212 \u03b4: Remark C.2. Theorem C.1 shows that, the upper bound on the risk of the ensemble of K local models on D mainly consists of 1) the empirical risk of a model trained on the global empirical distributionD = 1 K kD k , and 2) terms dependent on the distribution discrepancy between D k and D.\n\nThe ensemble of the local models sets the performance upper bound for the later distilled model on the test domain as shown in Figure 4. Theorem 5.1 shows that compared to a model trained on aggregated local data (ideal case), the performance of an ensemble model on the test distribution is affected by the domain discrepancy between local distributions D k 's and the test distribution D. The shift between the distillation and the test distribution determines the knowledge transfer quality between these two distributions and hence the test performance of the fused model. Through the lens of the domain adaptation theory (Ben-David et al., 2010), we can better spot the potential influence/limiting factors on our ensemble distillation procedure. Remark C.3. In the area of multiple-source adaptation, Mansour et al. (2009);Hoffman et al. (2018) point out that the standard convex combinations of the source hypotheses may perform poorly on the test distribution. They propose combinations with weights derived from source distributions. However, FL scenarios require the server only access local models without any further local information. Thus we choose to uniformly average over local hypotheses as our global hypothesis. A privacy-preserved local distribution estimation is left for future work.\n\n\nC.1 PROOF FOR GENERALIZATION BOUNDS\n\nTheorem C.4 (Domain adaptation (Ben-David et al., 2010)). Considering the distributions D S and D T , for every h \u2208 H and any \u03b4 \u2208 (0, 1), with probability at least 1 \u2212 \u03b4 (over the choice of the samples), there exists:\nL D T (h) \u2264 L D S (h) + 1 2 d H\u2206H (D S , D T ) + \u03bb ,(1)\nwhere \u03bb = L D S (h ) + L D T (h ). h := arg min h\u2208H L D S (h) + L D T (h) corresponds to ideal joint hypothesis that minimizes the combined error.\n\nProof of Theorem C.1. We start from the risk of our \"ensembled\" model L D ( 1 K k hD k ) and derive a series of upper bounds.\n\nConsidering the distance between L D ( 1 K k hD k ) and LD(hD). By convexity of and Jensen inequality, we have\nL D ( 1 K k hD k ) \u2264 1 K k L D (hD k ) .(2)\nUsing the domain adaptation theory in Theorem C.4, we transfer from domain D to D k ,\nL D (hD k ) \u2264 L D k (hD k ) + 1 2 d H\u2206H (D k , D) + \u03bb k ,(3)\nwhere \u03bb k := L D (h ) + L D k (h ) and h := arg min h\u2208H L D (h) + L D k (h).\n\nWe can bound the risk with its empirical counterpart through Hoeffding inequality. A simple application of the Hoeffding's inequality gives\nPr L D k (hD k ) \u2212 LD k (hD k ) \u2265 \u2264 2 exp \u22122m 2 2 m j=1 (b \u2212 a) 2 ,\nwhere [a, b] is the range of loss function. In our case, the loss function is bounded in [0, 1] so (b \u2212 a) 2 \u2264 1, thereby, with probability at least 1 \u2212 \u03b4 K , over the draw of m i.i.d. samples S k from D k ,\nL D k (hD k ) \u2264 LD k (hD k ) + log 2 \u03b4 K 2m ,(4)\nThus for K sources, we have Pr S1\u223cD m 1 ,...,S K \u223cD m\nK \uf8ee \uf8f0 K k=1 \uf8f1 \uf8f2 \uf8f3 L D k (hD k ) \u2264 LD k (hD k ) + log 2 \u03b4 K 2m \uf8fc \uf8fd \uf8fe \uf8f9 \uf8fb = 1 \u2212 Pr S1\u223cD m 1 ,...,S K \u223cD m K \uf8ee \uf8f0 K k=1 \uf8f1 \uf8f2 \uf8f3 L D k (hD k ) \u2265 LD k (hD k ) + log 2 \u03b4 K 2m \uf8fc \uf8fd \uf8fe \uf8f9 \uf8fb \u2265 1 \u2212 K k=1\nPr S1\u223cD m 1 ,...,S K \u223cD m\nK \uf8ee \uf8f0 \uf8f1 \uf8f2 \uf8f3 L D k (hD k ) \u2265 LD k (hD k ) + log 2K \u03b4 2m \uf8fc \uf8fd \uf8fe \uf8f9 \uf8fb \u2265 1 \u2212 \u03b4 .(5)\nBased on the definition of ERM, we have LD k (hD k ) \u2264 LD k (hD), where hD corresponds to the classifier trained with data from all workers. By using the definition ofD (D = 1 K kD k ) and the linearity of expectation, we have\n1 K k LD k (hD k ) \u2264 1 K k LD k (hD) = LD(hD) .(6)\nPutting these equations together, we have with probability of at least 1 \u2212 \u03b4 over S 1 \u223c D m 1 , . . . , S K \u223c D m K that\nL D ( 1 K k hD k ) \u2264 1 K k L D (hD k ) \u2264 1 K k L D k (hD k ) + 1 2 d H\u2206H (D k , D) + \u03bb k \u2264 1 K k \uf8eb \uf8ed LD k (hD k ) + log 2K \u03b4 2m + 1 2 d H\u2206H (D k , D) + \u03bb k \uf8f6 \uf8f8 \u2264 1 K k LD k (hD k ) + log 2K \u03b4 2m + 1 K k 1 2 d H\u2206H (D k , D) + \u03bb k \u2264 LD(hD) + log 2K \u03b4 2m + 1 K k 1 2 d H\u2206H (D k , D) + \u03bb k ,\nwhere \u03bb k = inf h\u2208H (L D (h) + L D k (h)).\n\nFigure 1 :\n1Limitations of FEDAVG. We consider a toy example of a 3-class classification task with a 3-layer MLP, and display the decision boundaries (probabilities over RGB channels) on the input space. The left two figures consider the individually trained local models. The right three figures evaluate aggregated models and the global data distribution; the averaged model results in much blurred decision boundaries. The used datasets are displayed inFigure 8(Appendix B.1).\n\nFederated learning .\nlearningThe classic algorithm in FL, FEDAVG (McMahan et al., 2016), or local SGD (Lin et al., 2020b) when all devices are participating, performs weighted parameter average over the client models after several local SGD updates with weights proportional to the size of each client's local data. Weighting schemes based on client loss are investigated in Mohri et al. (2019); Li et al. (2020). To address the difficulty of directly averaging model parameters, Singh & Jaggi\n\nFigure 2 :\n2Top: Illustration of # of samples per class allocated to each client (indicated by dot sizes), for different Dirichlet distribution \u03b1 values. Bottom: Test performance of FedDF and FEDAVG on CIFAR-10 with ResNet-8, for different local training settings: non-i.i.d. degrees \u03b1, data fractions, and # of local epochs per communication round. We perform 100 communication rounds, and active clients are sampled with ratio C = 0.4 from a total of 20 clients. Detailed learning curves in these scenarios can be found in Appendix B.4.\n\n\nobservations in McMahan et al. (2016); Caldas et al. (2018); Wang et al. (2020). As FedDF focuses on better model fusion on the server side, it is orthogonal to recent techniques (e.g. Shoham et al. (2019); Karimireddy et al. (2019); Deng et al. (\n\n\nComments on Batch Normalization. Batch Normalization (BN) (Ioffe & Szegedy, 2015) is the current workhorse in convolutional deep learning tasks and has been employed by default in most SOTA CNNs (He et al., 2016; Huang et al., 2017; Sandler et al., 2018; Ma et al., 2018; Tan & Le, 2019), however often fails on heterogeneous training data. Hsieh et al. (2019) recently examined the non-i.i.d. data 'quagmire' for distributed learning and point out that replacing BN by Group Normalization (GN)\n\nFigure 3 :\n3Federated fine-tuning DistilBERT on (a) AG News and (b) SST-2.\n\n( c )\ncImageNet (image resolution 32).\n\nFigure 4 :\n4Federated learning on heterogeneous systems (model/data), with diverse neural architectures (ResNet-20, MobileNetV2, ShuffleNetV2) and non-i.i.d. local data distribution (\u03b1 = 1). We consider 21 clients for CIFAR (client sampling ratio C = 0.4) and 150 clients for ImageNet (C = 0.1); different neural architectures are evenly distributed among clients. We train 80 local training epochs per communication round (total 30 rounds). CIFAR-100, STL-10, and STL-10 are used as the distillation datasets for CIFAR-10/100 and ImageNet training respectively. The solid lines show the results of FedDF for a given communication round, while dashed lines correspond to that of FEDAVG; colors indicate model architectures.\n\nFigure 5 :\n5The performance of FedDF on different distillation datasets: random uniformly sampled noises, randomly generated images (from the generator), CIFAR, downsampled ImageNet32, and downsampled STL-10. We evaluate ResNet-8 on CIFAR for 20 clients, with C = 0.4, \u03b1 = 1 and 100 communication rounds.\n\nFigure 6 :\n6Understanding knowledge distillation behaviors of FedDF on # of classes (6(a)), sizes of the distillation dataset (6(b)), and # of distillation steps (6(c)), for federated learning ResNet-8 on CIFAR-100, with C = 0.4, \u03b1 = 1 and 100 communication rounds (40 local epochs per round). ImageNet with image resolution 32 is considered as our base unlabeled dataset. For simplicity, only classes without overlap with CIFAR-100 classes are considered, in terms of the synonyms, hyponyms, or hypernyms of the class name.\n\nFigure 8 :\n8The limitation of FEDAVG. We consider a toy example of a 3-class classification task with a 3-layer MLP, and display the decision boundaries (probabilities over RGB channels) on the input space. We illustrate the used datasets in the top row; the distillation dataset consists of 60 data points, with each uniformly sampled from the range of (\u22123, 3). In the bottom row, the left two figures consider the individually trained local models. The right three figures evaluate aggregated models and the global data distribution; the averaged model (FEDAVG) results in much blurred decision boundaries.B.2 DETAILED EXPERIMENT SETUPThe detailed hyperparameter tuning procedure. The tuning procedure of hyperparameters ensures that the best hyperparameter lies in the middle of our search grids; otherwise, we extend our search grid. The initial search grid of learning rate is {1.5, 1, 0.5, 0.05, 0.01}. The initial search grid of proximal factor in FEDPROX is {0.001, 0.01, 0.1, 1}. The initial search grid of momentum factor \u03b2 in FEDAVGM is {0.1, 0.2, 0.3, 0.4}; the update scheme of FEDAVGM follows \u2206v := \u03b2v + \u2206x ; x := x \u2212 \u2206v, where \u2206x is the model difference between the updated local model and the sent global model, for previous communication round.\n\n[ 1 ,\n1M ] and q 1 = 1). Following the partition scheme introduced and used in Yurochkin et al. (2019); Hsu et al. (2019) 10 , to synthesize client non-i.i.d. local data distributions, we draw \u03b1 \u223c Dir(\u03b1p) from a Dirichlet distribution, where p characterizes a prior class distribution over M classes, and \u03b1 > 0 is a concentration parameter controlling the identicalness among clients. With \u03b1 \u2192 \u221e, all clients have identical distributions to the prior; with \u03b1 \u2192 0, each client holds examples from only one random class.\n\nFigure 9 :Figure 10 :Figure 11 :\n91011Classes allocated to each client at different Dirichlet distribution alpha values, for CIFAR-10 with 20 clients. The size of each dot reflects the magnitude of the samples number. Classes allocated to each client at different Dirichlet distribution alpha values, for CIFAR-100 with 20 clients. The size of each dot reflects the magnitude of the samples number. Classes allocated to each client at Dirichlet distribution \u03b1 = 1, for AG News and SST2 datasets with 10 clients. The size of each dot reflects the magnitude of the samples number.\n\nFigure 13 :\n13The impact of different normalization techniques, i.e., Batch Normalization (BN), Group Normalization (GN), for federated learning on CIFAR-10 with ResNet-8 with \u03b1 = 1. For each communication round (100 in total), 40% of the total 20 clients will be randomly selected for 40 local epochs. B.4 THE ADVANTAGES OF FEDDF ON STANDARD FEDERATED LEARNING SCENARIO B.4.1 THE IMPORTANCE OF THE MODEL INITIALIZATION IN FEDDF\n\nFigure 14 complementsFigure 15 ,\n1415Figure 2in the main text and presents a thorough comparison between FEDAVG and FedDF, for a variety of different local training epochs, data fractions, non-i.i.d. degrees. The detailed learning curves of the cases in this figure are visualized inFigure\n\nFigure 14 :\n14The test performance of FedDF and FEDAVG on CIFAR-10 with ResNet-8, for different local data non-iid degrees \u03b1, data fractions, and # of local epochs per communication round. For each communication round (100 in total), 40% of the total 20 clients will be randomly selected. We report the top-1 accuracy (on three different seeds) on the test dataset. This Figure complementsFigure 2. 11 The related preprints (Li & Wang, 2019; Chang et al., 2019) are closer to the second initialization scheme.They do not or cannot introduce the uniformly averaged model (on the server) into the federated learning pipeline; instead, they only utilize the averaged logits (on the same data) for each client's local training.\n\nFigure 17 :\n17Understanding the learning behaviors of FedDF on CIFAR-10 with ResNet-8 for \u03b1 = 0.01. For each communication round (100 in total)\n\n\n(D k , D) + \u03bb k , whereD = 1 K kD k , dH\u2206H measures the domain discrepancy between two distributions(Ben-David et al.,  2010), and \u03bb k = inf h\u2208H (LD(h)+LD k (h)).\n\n\nfor j in {1, . . . , N } doAlgorithm 2. \n\n6: \n\ninitialize for model fusion x t,0 \u2190 k\u2208St \n\nn k \n\nk\u2208S t \n\nn kx \n\nk \nt \n\n7: \n\n8: \n\nsample a mini-batch of samples d, from e.g. (1) an unlabeled dataset, (2) a generator \n\n9: \n\n\n\nTable 2 :\n2Theimpact of normalization techniques (i.e. BN, GN) for ResNet-8 on CIFAR (20 clients with \nC = 0.4, 100 communication rounds, and 40 local epochs per round). We use a constant learning rate and tune \nother hyperparameters. The distillation dataset of FedDF for CIFAR-100 is ImageNet (with image size of 32). \n\nTop-1 test accuracy of different methods \n\nDatasets \nFEDAVG, w/ BN FEDAVG, w/ GN FEDPROX, w/ GN FEDAVGM, w/ GN FedDF, w/ BN \n\nCIFAR-10 \n\u03b1 = 1 \n76.01 \u00b1 1.53 \n78.57 \u00b1 0.22 \n76.32 \u00b1 1.98 \n77.79 \u00b1 1.22 \n80.69 \u00b1 0.43 \n\u03b1 = 0.1 \n62.22 \u00b1 3.88 \n68.37 \u00b1 0.50 \n68.65 \u00b1 0.77 \n68.63 \u00b1 0.79 \n71.36 \u00b1 1.07 \n\nCIFAR-100 \n\u03b1 = 1 \n35.56 \u00b1 1.99 \n42.54 \u00b1 0.51 \n42.94 \u00b1 1.23 \n42.83 \u00b1 0.36 \n47.43 \u00b1 0.45 \n\u03b1 = 0.1 \n29.14 \u00b1 1.91 \n36.72 \u00b1 1.50 \n35.74 \u00b1 1.00 \n36.29 \u00b1 1.98 \n39.33 \u00b1 0.03 \n\n\n\nTable 3 :\n3Top-1 test accuracy of federated learning CIFAR-10 on VGG-9 (w/o BN), for 20 clients with C = 0.4, \u03b1 = 1 and 100 communication rounds (40 epochs per round). We by default drop dummy predictors. FEDAVG (w/o drop-worst) 45.72 \u00b1 30.95 51.06 \u00b1 35.56 53.22 \u00b1 37.43 29.60 \u00b1 40.66 7.52 \u00b1 4.29 FEDMA (w/o drop-worst) 1 23.41 \u00b1 0FEDMA does not support drop-worst operation due to its layer-wise communication/fusion scheme. The number of local training epochs per layer is 5 (45 epochs per model) thus results in stabilized training. More details can be found in Appendix B.2.Top-1 test accuracy @ communication round \n\nMethods \n5 \n10 \n20 \n50 \n100 \n.00 \n27.55 \u00b1 0.10 \n41.56 \u00b1 0.08 \n60.35 \u00b1 0.03 \n65.0 \u00b1 0.02 \nFEDAVG \n64.77 \u00b1 1.24 \n70.28 \u00b1 1.02 \n75.80 \u00b1 1.36 \n77.98 \u00b1 1.81 78.34 \u00b1 1.42 \nFEDPROX \n63.86 \u00b1 1.55 \n71.85 \u00b1 0.75 \n75.57 \u00b1 1.16 \n77.85 \u00b1 1.96 78.60 \u00b1 1.91 \nFedDF \n66.08 \u00b1 4.14 \n72.80 \u00b1 1.59 \n75.82 \u00b1 2.09 \n79.05 \u00b1 0.54 80.36 \u00b1 0.63 \n\n1 0 \n2 \n4 \n6 \n8 \n\n# of communication rounds \n\n0.87 \n\n0.88 \n\n0.89 \n\n0.90 \n\n0.91 \n\n0.92 \n\n0.93 \n\nTop-1 accuracy on test dataset \n\nTraining scheme \nCentralized training \nFederated Learning, FedDF \nFederated Learning, FedAvg \n\n(a) AG News. \n\n0 \n2 \n4 \n6 \n8 \n\n# of communication rounds \n\n0.70 \n\n0.75 \n\n0.80 \n\n0.85 \n\n0.90 \n\nTop-1 accuracy on test dataset \n\nTraining scheme \nCentralized training \nFederated Learning, FedDF \nFederated Learning, FedAvg \n\n\n\nTable 4\n4examines the model fusion performance for binarizedResNet-8 (Rastegari et al., 2016; Hubara et al., 2017). FedDF can be on par with or outperform FEDAVG by a noticeable margin, without introducing extra GN tuning overheads.Federated learning on heterogeneous systems. Apart from non-i.i.d. local distributions, another major source of heterogeneity in FL systems manifests in neural architectures(Li & Wang, 2019).\n\nTable 4 :\n4Federated learning with low-precision models (1-bit binarized ResNet-8) on CIFAR-10. For each communication round (100 in total), 40% of the total 20 clients (\u03b1 = 1) are randomly selected.Local Epochs ResNet-8-BN (FEDAVG) ResNet-8-GN (FEDAVG) ResNet-8-BN (FedDF) \n\n20 \n44.38 \u00b1 1.21 \n59.70 \u00b1 1.65 \n59.49 \u00b1 0.98 \n40 \n43.91 \u00b1 3.26 \n64.25 \u00b1 1.31 \n65.49 \u00b1 0.74 \n80 \n47.62 \u00b1 1.84 \n65.99 \u00b1 1.29 \n70.27 \u00b1 1.22 \n\n0 \n5 \n10 \n15 \n20 \n25 \n30 \n\n# of communication rounds \n\n40 \n\n50 \n\n60 \n\n70 \n\n80 \n\nTop-1 accuracy on test dataset \n\nEvaluated on \nEnsembled model \nShuffleNetV2-1 \nMobileNetV2 \n\nResNet-20 \nAlgorithm \nFedDF \nFedAvg \n\n(a) CIFAR-10. \n\n0 \n5 \n10 \n15 \n20 \n25 \n30 \n\n# of communication rounds \n\n10 \n\n20 \n\n30 \n\n40 \n\n50 \n\nTop-1 accuracy on test dataset \n\nEvaluated on \nEnsembled model \nShuffleNetV2-1 \nMobileNetV2 \n\nResNet-20 \nAlgorithm \nFedDF \nFedAvg \n\n(b) CIFAR-100. \n\n0 \n5 \n10 \n15 \n20 \n25 \n30 \n\n# of communication rounds \n\n0 \n\n5 \n\n10 \n\n15 \n\n20 \n\n25 \n\nTop-1 accuracy on test dataset \n\nEvaluated on \nEnsembled model \nShuffleNetV2-1 \nMobileNetV2 \n\nResNet-20 \nAlgorithm \nFedDF \nFedAvg \n\n\n\n\nComputer Vision and Pattern Recognition, pp. 9163-9171, 2019. Rohan Anil, Gabriel Pereyra, Alexandre Passos, Robert Ormandi, George E Dahl, and Geoffrey E Hinton. Large scale distributed neural network training through online distillation. arXiv preprint arXiv:1804.03235, 2018. Sebastian Caldas, Peter Wu, Tian Li, Jakub Kone\u010dn\u1ef3, H Brendan McMahan, Virginia Smith, and Ameet Talwalkar. Leaf: A benchmark for federated settings. arXiv preprint arXiv:1812.01097, 2018. Hongyan Chang, Virat Shejwalkar, Reza Shokri, and Amir Houmansadr. Cronus: Robust and heterogeneous collaborative learning with black-box knowledge transfer. arXiv preprint arXiv:1912.11279, 2019. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. Nikita Dvornik, Cordelia Schmid, and Julien Mairal. Diversity with cooperation: Ensemble methods for few-shot classification. In The IEEE International Conference on Computer Vision (ICCV), October 2019. Tao Lin, Sebastian U. Stich, Luis Barba, Daniil Dmitriev, and Martin Jaggi. Dynamic model pruning with feedback. In International Conference on Learning Representations, 2020a. Tao Lin, Sebastian U. Stich, Kumar Kshitij Patel, and Martin Jaggi. Don't use large mini-batches, use local SGD. In ICLR -International Conference on Learning Representations, 2020b. Iou-Jen Liu, Jian Peng, and Alexander G Schwing. Knowledge flow: Improve upon your teachers. arXiv preprint arXiv:1904.05878, 2019. Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufflenet v2: Practical guidelines for efficient cnn architecture design. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 116-131, 2018. Paul Micaelli and Amos J Storkey. Zero-shot knowledge transfer via adversarial belief matching. In Advances in Neural Information Processing Systems, pp. 9547-9557, 2019. Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh. Agnostic federated learning. arXiv preprint arXiv:1902.00146, 2019.Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman \nVaughan. A theory of learning from different domains. Machine learning, 79(1-2):151-175, 2010. \n\nYoshua Bengio, Nicholas L\u00e9onard, and Aaron Courville. Estimating or propagating gradients through \nstochastic neurons for conditional computation, 2013. \n\nKeith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex Ingerman, Vladimir \nIvanov, Chloe Kiddon, Jakub Kone\u010dn\u00fd, Stefano Mazzocchi, H. Brendan McMahan, Timon Van \nOverveldt, David Petrou, Daniel Ramage, and Jason Roselander. Towards federated learning at \nscale: System design, 2019. \n\nAndrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity \nnatural image synthesis. In International Conference on Learning Representations, 2019. \n\nCristian Bucilu\u01ce, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In Proceedings \nof the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. \n535-541, 2006. \n\nPatryk Chrabaszcz, Ilya Loshchilov, and Frank Hutter. A downsampled variant of imagenet as an \nalternative to the cifar datasets. arXiv preprint arXiv:1707.08819, 2017. \n\nYuyang Deng, Mohammad Mahdi Kamani, and Mehrdad Mahdavi. Adaptive personalized federated \nlearning. arXiv preprint arXiv:2003.13461, 2020. \n\nTommaso Furlanello, Zachary C Lipton, Michael Tschannen, Laurent Itti, and Anima Anandkumar. \nBorn again neural networks. arXiv preprint arXiv:1805.04770, 2018. \n\nRobin C Geyer, Tassilo Klein, and Moin Nabi. Differentially private federated learning: A client \nlevel perspective. arXiv preprint arXiv:1712.07557, 2017. \n\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, \nAaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural informa-\ntion processing systems, pp. 2672-2680, 2014. \n\nNeel Guha, Ameet Talwlkar, and Virginia Smith. One-shot federated learning. arXiv preprint \narXiv:1902.11175, 2019. \n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image \nrecognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, \npp. 770-778, 2016. \n\nGeoffrey Hinton. Neural networks for machine learning, 2012. \nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolu-\ntional neural networks. In Advances in neural information processing systems, pp. 1097-1105, \n2012. \n\nLudmila I Kuncheva and Christopher J Whitaker. Measures of diversity in classifier ensembles and \ntheir relationship with the ensemble accuracy. Machine learning, 51(2):181-207, 2003. \n\nDaliang Li and Junpu Wang. Fedmd: Heterogenous federated learning via model distillation. arXiv \npreprint arXiv:1910.03581, 2019. \n\nTian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. \nFederated optimization in heterogeneous networks. arXiv preprint arXiv:1812.06127, 2018. \n\nTian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: Challenges, \nmethods, and future directions. arXiv preprint arXiv:1908.07873, 2019. \n\nTian Li, Maziar Sanjabi, Ahmad Beirami, and Virginia Smith. Fair resource allocation in federated \nlearning. In International Conference on Learning Representations, 2020. \n\nYishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation with multiple \nsources. In Advances in neural information processing systems, pp. 1041-1048, 2009. \n\nH Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, et al. Communication-efficient \nlearning of deep networks from decentralized data. arXiv preprint arXiv:1602.05629, 2016. \n\nGaurav Kumar Nayak, Konda Reddy Mopuri, Vaisakh Shaj, R Venkatesh Babu, and Anirban \nChakraborty. Zero-shot knowledge distillation in deep networks. arXiv preprint arXiv:1905.08114, \n2019. \n\nSeongUk Park and Nojun Kwak. Feed: Feature-level ensemble for knowledge distillation. arXiv \npreprint arXiv:1909.10754, 2019. \n\nMohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet \nclassification using binary convolutional neural networks. In European conference on computer \nvision, pp. 525-542. Springer, 2016. \n\nSashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Kone\u010dn\u1ef3, \nSanjiv Kumar, and H Brendan McMahan. Adaptive federated optimization. arXiv preprint \narXiv:2003.00295, 2020. \n\nAdriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and \nYoshua Bengio. Fitnets: Hints for thin deep nets. In International Conference on Learning \nRepresentations, 2015. \nA ALGORITHMIC DESCRIPTION \n\nAlgorithm 2 below details a general training procedure on local clients. The local update step of \n\nFEDPROX corresponds to adding a proximal term (i.e. \u03b7 \n\n\u2202 \u00b5 \n2 x k \nt \u2212x k \n\nt\u22121 \n\n2 \n\n2 \n\n\u2202x k \n\nt \n\n) to line 5. \n\n\n\n\nThe schematic diagram for heterogeneous model fusion. We use dotted lines to indicate model parameter averaging FL methods such as FEDAVG. We could notice the architectural/precision discrepancy invalidates these methods in heterogeneous FL systems. However, FedDF could aggregate knowledge from all available models without hindrance.Algorithm 3 Illustration of FedDF for heterogeneous FL systems. The K clients are indexed by k, and n k indicates the number of data points for the k-th client. The number of communication rounds is T , and C controls the client participation ratio per communication round. The number of total iterations used for model fusion is denoted as N . The distinct model prototype set P has p model prototypes, with each initialized as x P 0 .pruned net \n\n1-bit \n\n32-layer \n\n32-bit \n\n1-bit \n\nMobileNets \n\nShuffleNets \n\n8-layer \n\nResNets \n\nArbitrary \nnet \n\nFedDF \n\nFigure 7: 1: procedure SERVER \n\n2: \n\ninitialize HashMap M: map each model prototype P to its weights x P \n0 . \n\n3: \n\ninitialize HashMap C: map each client to its model prototype. \n\n4: \n\n\n\n\nin the main text.10 \n20 \n40 \n80 \n160 \n\n# of local epochs \n\n70 \n\n72 \n\n74 \n\n76 \n\n78 \n\n80 \n\n82 \n\nTop-1 accuracy on test dataset \n\n50% local data, w/ lr decay \n100% local data, w/ lr decay \n50% local data, w/o lr decay \n100% local data, w/o lr decay \n\n(a) \u03b1 = 100 \n\n10 \n20 \n40 \n80 \n160 \n\n# of local epochs \n\n62.5 \n\n65.0 \n\n67.5 \n\n70.0 \n\n72.5 \n\n75.0 \n\n77.5 \n\nTop-1 accuracy on test dataset \n\n50% local data, w/ lr decay \n100% local data, w/ lr decay \n50% local data, w/o lr decay \n100% local data, w/o lr decay \n\n(b) \u03b1 = 1 \n\n10 \n20 \n40 \n80 \n160 \n\n# of local epochs \n\n30 \n\n32 \n\n34 \n\n36 \n\n38 \n\n40 \n\n42 \n\n44 \n\nTop-1 accuracy on test dataset \n\n50% local data, w/ lr decay \n100% local data, w/ lr decay \n50% local data, w/o lr decay \n100% local data, w/o lr decay \n\n\n\nTable 5 :\n5Understanding the importance of model initialization in FedDF, on CIFAR-10 with ResNet-8. For \neach communication round (100 in total), 40% of the total 20 clients will be randomly selected. The scheme \n\"from average\" indicates initializing the model for ensemble distillation from the uniformly averaged model of \nthis communication round; while the scheme \"from previous\" instead uses the fused model from the previous \ncommunication round. We report the top-1 accuracy (on three different seeds) on the test dataset. \n\n\u03b1 = 1 \n\u03b1 = 0.1 \n\nlocal training epochs from average from previous from average from previous \n\n40 \n80.43 \u00b1 0.37 74.13 \u00b1 0.91 71.84 \u00b1 0.86 62.94 \u00b1 1.12 \n80 \n81.17 \u00b1 0.53 76.37 \u00b1 0.60 74.73 \u00b1 0.65 67.88 \u00b1 0.90 \n\nB.4.2 COMPARISON WITH FEDAVG \n\n\n\n\nTop-1 accuracy on test dataset(a) The learning behaviors of FedDF and FEDAVG. We evaluate different # of local epochs on 100% local data.(b) The fused model performance before (i.e. line 6 in Algorithm 1) and after FedDF (i.e. line 10 in Algorithm 1). We evaluate different # of local epochs on 100% local data.(c) The learning behaviors of FedDF and FEDAVG. We evaluate different # of local epochs on 50% local data.(d) The fused model performance before (i.e. line 6 in Algorithm 1) and after FedDF (i.e. line 10 in Algorithm 1). We evaluate different # of local epochs on 50% local data.Figure 15: Understanding the learning behaviors of FedDF on CIFAR-10 with ResNet-8 for \u03b1 = 100. For each communication round (100 in total), 40% of the total 20 clients will be randomly selected. We report the top-1 accuracy (on three different seeds) on the test dataset.(a) The learning behaviors of FedDF and FEDAVG. We evaluate different # of local epochs on 100% local data.(b) The fused model performance before (i.e. line 6 in Algorithm 1) and after FedDF (i.e. line 10 in Algorithm 1). We evaluate different # of local epochs on 100% local data.(c) The learning behaviors of FedDF and FEDAVG. We evaluate different # of local epochs on 50% local data.(d) The fused model performance before (i.e. line 6 in Algorithm 1) and after FedDF (i.e. line 10 in Algorithm 1). We evaluate different # of local epochs on 50% local data.Figure 16: Understanding the learning behaviors of FedDF on CIFAR-10 with ResNet-8 for \u03b1 = 1. For each communication round (100 in total), 40% of the total 20 clients will be randomly selected. We report the top-1 accuracy (on three different seeds) on the test dataset.(a) The learning behaviors of FedDF and FEDAVG. We evaluate different # of local epochs on 100% local data.(b) The fused model performance before (i.e. line 6 in Algorithm 1) and after FedDF (i.e. line 10 in Algorithm 1). We evaluate different # of local epochs on 100% local data.(c) The learning behaviors of FedDF and FEDAVG. We evaluate different # of local epochs on 50% local data.(d) The fused model performance before (i.e. line 6 in Algorithm 1) and after FedDF (i.e. line 10 in Algorithm 1). We evaluate different # of local epochs on 50% local data.# of local epochs \n10 \n20 \n40 \n80 \n\n160 \nmodel fusion scheme \nFedDF \nFedAvg \n\n0 \n20 \n40 \n60 \n80 \n100 \n\n# of communication rounds \n\n20 \n\n30 \n\n40 \n\n50 \n\n60 \n\n70 \n\n80 \n\nTop-1 accuracy on test dataset \n\n# of local epochs \n10 \n20 \n40 \n80 \n\n160 \nevaluated on \nFedDF (after) \nFedDF (before) \n\n0 \n20 \n40 \n60 \n80 \n100 \n\n# of communication rounds \n\n10 \n\n20 \n\n30 \n\n40 \n\n50 \n\n60 \n\n70 \n\n80 \n\nTop-1 accuracy on test dataset \n\n# of local epochs \n10 \n20 \n40 \n80 \n\n160 \nmodel fusion scheme \nFedDF \nFedAvg \n\n0 \n20 \n40 \n60 \n80 \n100 \n\n# of communication rounds \n\n10 \n\n20 \n\n30 \n\n40 \n\n50 \n\n60 \n\n70 \n\n80 \n\nTop-1 accuracy on test dataset \n\n# of local epochs \n10 \n20 \n40 \n80 \n\n160 \nevaluated on \nFedDF (after) \nFedDF (before) \n\n0 \n\n20 \n40 \n60 \n80 \n100 \n\n# of communication rounds \n\n10 \n\n20 \n\n30 \n\n40 \n\n50 \n\n60 \n\n70 \n\n80 \n\nTop-1 accuracy on test dataset \n\n# of local epochs \n10 \n20 \n40 \n80 \n\n160 \nmodel fusion scheme \nFedDF \nFedAvg \n\n0 \n20 \n40 \n60 \n80 \n100 \n\n# of communication rounds \n\n10 \n\n20 \n\n30 \n\n40 \n\n50 \n\n60 \n\n70 \n\n80 \n\nTop-1 accuracy on test dataset \n\n# of local epochs \n10 \n20 \n40 \n80 \n\n160 \nevaluated on \nFedDF (after) \nFedDF (before) \n\n0 \n20 \n40 \n60 \n80 \n100 \n\n# of communication rounds \n\n10 \n\n20 \n\n30 \n\n40 \n\n50 \n\n60 \n\n70 \n\nTop-1 accuracy on test dataset \n\n# of local epochs \n10 \n20 \n40 \n80 \n\n160 \nmodel fusion scheme \nFedDF \nFedAvg \n\n0 \n20 \n40 \n60 \n80 \n100 \n\n# of communication rounds \n\n10 \n\n20 \n\n30 \n\n40 \n\n50 \n\n60 \n\n70 \n\nTop-1 accuracy on test dataset \n\n# of local epochs \n10 \n20 \n40 \n80 \n\n160 \nevaluated on \nFedDF (after) \nFedDF (before) \n\n0 \n\n20 \n40 \n60 \n80 \n100 \n\n# of communication rounds \n\n10 \n\n15 \n\n20 \n\n25 \n\n30 \n\n35 \n\n40 \n\nTop-1 accuracy on test dataset \n\n# of local epochs \n10 \n20 \n40 \n80 \n\n160 \nmodel fusion scheme \nFedDF \nFedAvg \n\n0 \n20 \n40 \n60 \n80 \n100 \n\n# of communication rounds \n\n10 \n\n15 \n\n20 \n\n25 \n\n30 \n\n35 \n\n40 \n\nTop-1 accuracy on test dataset \n\n# of local epochs \n10 \n20 \n40 \n80 \n\n160 \nevaluated on \nFedDF (after) \nFedDF (before) \n\n0 \n20 \n40 \n60 \n80 \n100 \n\n# of communication rounds \n\n10 \n\n15 \n\n20 \n\n25 \n\n30 \n\n35 \n\n40 \n\nTop-1 accuracy on test dataset \n\n# of local epochs \n10 \n20 \n40 \n80 \n\n160 \nmodel fusion scheme \nRobust Fusion \nFedAvg \n\n0 \n20 \n40 \n60 \n80 \n100 \n\n# of communication rounds \n\n10 \n\n15 \n\n20 \n\n25 \n\n30 \n\n35 \n\n40 \n\nTop-1 accuracy on test dataset \n\n# of local epochs \n10 \n20 \n40 \n80 \n\n160 \nevaluated on \nFedDF (after) \nFedDF (before) \n\n\nAdvanced techniques e.g. mentioned in Chang et al. (2019) can be used to improve the robustness or defend against attacks.\nt \u2190 Client-LocalUpdate(k, M [C[k]]) detailed in Algorithm 2. 9: for each prototype P \u2208 P in parallel do 10: initialize the client set S P t with model prototype P , where S P t \u2190C[P ] \u2229 S t 11: initialize for model fusion x P t,0 \u2190 k\u2208S P t n k k\u2208S P t n kx k t 12: for j in {1, . . . , N } do 13: sample d, from e.g. (1) an unlabeled dataset, (2) a generator 14: use ensemble of {x k t } k\u2208St to update server student x P t,j through AVGLOGITS 15: M [P ] \u2190 x P t,N 16:return M\nThe uniformly weighted hypothesis average in multi-source adaptation is equivalent to the ensemble of a list of models, by considering the output of each hypothesis/model.\n\nVariational information distillation for knowledge transfer. Sungsoo Ahn, Shell Xu Hu, Andreas Damianou, D Neil, Zhenwen Lawrence, Dai, Proceedings of the IEEE Conference on. the IEEE Conference onSungsoo Ahn, Shell Xu Hu, Andreas Damianou, Neil D Lawrence, and Zhenwen Dai. Variational information distillation for knowledge transfer. In Proceedings of the IEEE Conference on\n\nMo-bilenetv2: Inverted residuals and linear bottlenecks. Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionMark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mo- bilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4510-4520, 2018.\n\nDistilbert, a distilled version of bert: smaller, faster, cheaper and lighter. Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf, arXiv:1910.01108arXiv preprintVictor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.\n\nNeta Shoham, Tomer Avidor, Aviv Keren, Nadav Israel, Daniel Benditkis, arXiv:1910.07796Liron Mor-Yosef, and Itai Zeitak. Overcoming forgetting in federated learning on non-iid data. arXiv preprintNeta Shoham, Tomer Avidor, Aviv Keren, Nadav Israel, Daniel Benditkis, Liron Mor-Yosef, and Itai Zeitak. Overcoming forgetting in federated learning on non-iid data. arXiv preprint arXiv:1910.07796, 2019.\n\nPrivacy-preserving deep learning. Reza Shokri, Vitaly Shmatikov, Proceedings of the 22nd ACM SIGSAC conference on computer and communications security. the 22nd ACM SIGSAC conference on computer and communications securityReza Shokri and Vitaly Shmatikov. Privacy-preserving deep learning. In Proceedings of the 22nd ACM SIGSAC conference on computer and communications security, pp. 1310-1321, 2015.\n\nVery deep convolutional networks for large-scale image recognition. Karen Simonyan, Andrew Zisserman, arXiv:1409.1556arXiv preprintKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\n\nModel fusion via optimal transport. Pal Sidak, Martin Singh, Jaggi, arXiv:1910.05653arXiv preprintSidak Pal Singh and Martin Jaggi. Model fusion via optimal transport. arXiv preprint arXiv:1910.05653, 2019.\n\nFederated multi-task learning. Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, Ameet S Talwalkar, Advances in Neural Information Processing Systems. Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet S Talwalkar. Federated multi-task learning. In Advances in Neural Information Processing Systems, pp. 4424-4434, 2017.\n\nRecursive deep models for semantic compositionality over a sentiment treebank. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, Christopher Potts, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. the 2013 Conference on Empirical Methods in Natural Language ProcessingSeattle, Washington, USAAssociation for Computational LinguisticsRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pp. 1631-1642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics.\n\nLearning with ensembles: How overfitting can be useful. Peter Sollich, Anders Krogh, Advances in neural information processing systems. Peter Sollich and Anders Krogh. Learning with ensembles: How overfitting can be useful. In Advances in neural information processing systems, pp. 190-196, 1996.\n\nMingxing Tan, V Quoc, Le, Efficientnet, arXiv:1905.11946Rethinking model scaling for convolutional neural networks. arXiv preprintMingxing Tan and Quoc V Le. Efficientnet: Rethinking model scaling for convolutional neural networks. arXiv preprint arXiv:1905.11946, 2019.\n\nYonglong Tian, Dilip Krishnan, Phillip Isola, arXiv:1910.10699Contrastive representation distillation. arXiv preprintYonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive representation distillation. arXiv preprint arXiv:1910.10699, 2019.\n\nSimilarity-preserving knowledge distillation. Frederick Tung, Greg Mori, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionFrederick Tung and Greg Mori. Similarity-preserving knowledge distillation. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1365-1374, 2019.\n\nGLUE: A multi-task benchmark and analysis platform for natural language understanding. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLPBrussels, BelgiumAssociation for Computational LinguisticsAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 353-355, Brussels, Belgium, November 2018. Association for Computational Linguistics.\n\nSuperglue: A stickier benchmark for general-purpose language understanding systems. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, Advances in Neural Information Processing Systems. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems, pp. 3261-3275, 2019.\n\nFederated learning with matched averaging. Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, Yasaman Khazaeni, International Conference on Learning Representations. Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and Yasaman Khazaeni. Fed- erated learning with matched averaging. In International Conference on Learning Representations, 2020.\n\nDistilled person re-identification: Towards a more scalable system. A Wu, W Zheng, X Guo, J Lai, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). A. Wu, W. Zheng, X. Guo, and J. Lai. Distilled person re-identification: Towards a more scalable system. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\n\nGroup normalization. Yuxin Wu, Kaiming He, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 3-19, 2018.\n\nLearning from multiple teacher networks. Shan You, Chang Xu, Chao Xu, Dacheng Tao, 10.1145/3097983.3098135Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '17. the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '17New York, NY, USAAssociation for Computing MachineryShan You, Chang Xu, Chao Xu, and Dacheng Tao. Learning from multiple teacher networks. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '17, pp. 1285-1294, New York, NY, USA, 2017. Association for Computing Machinery.\n\nMikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, arXiv:1905.12022Trong Nghia Hoang, and Yasaman Khazaeni. Bayesian nonparametric federated learning of neural networks. arXiv preprintMikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Trong Nghia Hoang, and Yasaman Khazaeni. Bayesian nonparametric federated learning of neural networks. arXiv preprint arXiv:1905.12022, 2019.\n\nPaying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. Sergey Zagoruyko, Nikos Komodakis, arXiv:1612.03928arXiv preprintSergey Zagoruyko and Nikos Komodakis. Paying more attention to attention: Improving the perfor- mance of convolutional neural networks via attention transfer. arXiv preprint arXiv:1612.03928, 2016.\n\nCharacter-level convolutional networks for text classification. Xiang Zhang, Junbo Zhao, Yann Lecun, Advances in neural information processing systems. Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In Advances in neural information processing systems, pp. 649-657, 2015.\n", "annotations": {"author": "[{\"end\":97,\"start\":73},{\"end\":134,\"start\":98},{\"end\":177,\"start\":135},{\"end\":212,\"start\":178}]", "publisher": null, "author_last_name": "[{\"end\":80,\"start\":77},{\"end\":111,\"start\":107},{\"end\":152,\"start\":147},{\"end\":211,\"start\":208}]", "author_first_name": "[{\"end\":76,\"start\":73},{\"end\":106,\"start\":98},{\"end\":144,\"start\":135},{\"end\":146,\"start\":145},{\"end\":201,\"start\":195},{\"end\":207,\"start\":202}]", "author_affiliation": null, "title": "[{\"end\":70,\"start\":1},{\"end\":282,\"start\":213}]", "venue": null, "abstract": "[{\"end\":1593,\"start\":284}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1807,\"start\":1781},{\"end\":1828,\"start\":1807},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":1847,\"start\":1828},{\"end\":1867,\"start\":1847},{\"end\":1889,\"start\":1867},{\"end\":1905,\"start\":1889},{\"end\":1926,\"start\":1905},{\"end\":2305,\"start\":2288},{\"end\":2541,\"start\":2519},{\"end\":2589,\"start\":2569},{\"end\":2605,\"start\":2589},{\"end\":2622,\"start\":2605},{\"end\":2647,\"start\":2622},{\"end\":2664,\"start\":2647},{\"end\":2683,\"start\":2664},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2906,\"start\":2888},{\"end\":2930,\"start\":2906},{\"end\":2948,\"start\":2930},{\"end\":2969,\"start\":2948},{\"end\":2987,\"start\":2969},{\"end\":3004,\"start\":2987},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3020,\"start\":3004},{\"end\":3601,\"start\":3579},{\"end\":3621,\"start\":3601},{\"end\":3653,\"start\":3646},{\"end\":3778,\"start\":3753},{\"end\":4010,\"start\":3990},{\"end\":4062,\"start\":4039},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4896,\"start\":4878},{\"end\":5386,\"start\":5369},{\"end\":5411,\"start\":5386},{\"end\":5451,\"start\":5426},{\"end\":5588,\"start\":5570},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5593,\"start\":5588},{\"end\":5612,\"start\":5593},{\"end\":5741,\"start\":5735},{\"end\":5763,\"start\":5743},{\"end\":5955,\"start\":5934},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5983,\"start\":5955},{\"end\":6000,\"start\":5983},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6018,\"start\":6000},{\"end\":6040,\"start\":6018},{\"end\":6059,\"start\":6040},{\"end\":6076,\"start\":6059},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6094,\"start\":6076},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6336,\"start\":6318},{\"end\":6360,\"start\":6336},{\"end\":6378,\"start\":6360},{\"end\":6399,\"start\":6378},{\"end\":6464,\"start\":6445},{\"end\":6481,\"start\":6464},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6497,\"start\":6481},{\"end\":6671,\"start\":6651},{\"end\":6696,\"start\":6671},{\"end\":7155,\"start\":7137},{\"end\":7340,\"start\":7320},{\"end\":7917,\"start\":7900},{\"end\":12199,\"start\":12179},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":12220,\"start\":12200},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":12312,\"start\":12291},{\"end\":13613,\"start\":13593},{\"end\":14612,\"start\":14585},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":14634,\"start\":14612},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":17610,\"start\":17595},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":19623,\"start\":19604},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":19655,\"start\":19636},{\"end\":21790,\"start\":21789},{\"end\":22637,\"start\":22613},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":27180,\"start\":27161},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":27701,\"start\":27683},{\"end\":28141,\"start\":28120},{\"end\":28154,\"start\":28141},{\"end\":28174,\"start\":28154},{\"end\":28218,\"start\":28199},{\"end\":29472,\"start\":29471},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":29674,\"start\":29656},{\"end\":29869,\"start\":29868},{\"end\":29973,\"start\":29952},{\"end\":29991,\"start\":29973},{\"end\":30070,\"start\":30068},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":31181,\"start\":31157},{\"end\":31198,\"start\":31181},{\"end\":33376,\"start\":33352},{\"end\":34710,\"start\":34686},{\"end\":34889,\"start\":34867},{\"end\":34910,\"start\":34889},{\"end\":35461,\"start\":35437}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":38356,\"start\":37876},{\"attributes\":{\"id\":\"fig_1\"},\"end\":38851,\"start\":38357},{\"attributes\":{\"id\":\"fig_6\"},\"end\":39391,\"start\":38852},{\"attributes\":{\"id\":\"fig_7\"},\"end\":39641,\"start\":39392},{\"attributes\":{\"id\":\"fig_8\"},\"end\":40138,\"start\":39642},{\"attributes\":{\"id\":\"fig_10\"},\"end\":40214,\"start\":40139},{\"attributes\":{\"id\":\"fig_11\"},\"end\":40254,\"start\":40215},{\"attributes\":{\"id\":\"fig_12\"},\"end\":40979,\"start\":40255},{\"attributes\":{\"id\":\"fig_14\"},\"end\":41285,\"start\":40980},{\"attributes\":{\"id\":\"fig_16\"},\"end\":41811,\"start\":41286},{\"attributes\":{\"id\":\"fig_17\"},\"end\":43074,\"start\":41812},{\"attributes\":{\"id\":\"fig_18\"},\"end\":43594,\"start\":43075},{\"attributes\":{\"id\":\"fig_19\"},\"end\":44174,\"start\":43595},{\"attributes\":{\"id\":\"fig_20\"},\"end\":44604,\"start\":44175},{\"attributes\":{\"id\":\"fig_21\"},\"end\":44895,\"start\":44605},{\"attributes\":{\"id\":\"fig_23\"},\"end\":45620,\"start\":44896},{\"attributes\":{\"id\":\"fig_24\"},\"end\":45765,\"start\":45621},{\"attributes\":{\"id\":\"fig_25\"},\"end\":45930,\"start\":45766},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":46154,\"start\":45931},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":46939,\"start\":46155},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":48329,\"start\":46940},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":48754,\"start\":48330},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":49843,\"start\":48755},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":56810,\"start\":49844},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":57891,\"start\":56811},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":58649,\"start\":57892},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":59425,\"start\":58650},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":64031,\"start\":59426}]", "paragraph": "[{\"end\":2445,\"start\":1609},{\"end\":3480,\"start\":2447},{\"end\":4208,\"start\":3482},{\"end\":4671,\"start\":4210},{\"end\":5613,\"start\":4673},{\"end\":6498,\"start\":5630},{\"end\":7104,\"start\":6500},{\"end\":8961,\"start\":7106},{\"end\":9372,\"start\":9011},{\"end\":9396,\"start\":9374},{\"end\":9449,\"start\":9398},{\"end\":9503,\"start\":9451},{\"end\":9628,\"start\":9505},{\"end\":9692,\"start\":9681},{\"end\":9700,\"start\":9699},{\"end\":9898,\"start\":9712},{\"end\":10438,\"start\":9900},{\"end\":10631,\"start\":10534},{\"end\":11236,\"start\":10633},{\"end\":11746,\"start\":11238},{\"end\":12726,\"start\":11748},{\"end\":13415,\"start\":12728},{\"end\":14106,\"start\":13427},{\"end\":14403,\"start\":14163},{\"end\":15182,\"start\":14405},{\"end\":16238,\"start\":15184},{\"end\":16588,\"start\":16240},{\"end\":17140,\"start\":16705},{\"end\":17722,\"start\":17142},{\"end\":18166,\"start\":17724},{\"end\":18840,\"start\":18168},{\"end\":19407,\"start\":18842},{\"end\":20014,\"start\":19409},{\"end\":21173,\"start\":20031},{\"end\":22465,\"start\":21197},{\"end\":22728,\"start\":22537},{\"end\":23562,\"start\":22761},{\"end\":24510,\"start\":23564},{\"end\":25239,\"start\":24512},{\"end\":25288,\"start\":25241},{\"end\":25353,\"start\":25290},{\"end\":25395,\"start\":25355},{\"end\":25425,\"start\":25397},{\"end\":25528,\"start\":25464},{\"end\":25958,\"start\":25530},{\"end\":26016,\"start\":25965},{\"end\":26072,\"start\":26018},{\"end\":26485,\"start\":26074},{\"end\":26724,\"start\":26487},{\"end\":26825,\"start\":26726},{\"end\":27350,\"start\":26827},{\"end\":27884,\"start\":27352},{\"end\":28891,\"start\":27886},{\"end\":29367,\"start\":28893},{\"end\":30638,\"start\":29369},{\"end\":32353,\"start\":30640},{\"end\":33251,\"start\":32392},{\"end\":33509,\"start\":33253},{\"end\":34058,\"start\":33511},{\"end\":35366,\"start\":34060},{\"end\":35623,\"start\":35406},{\"end\":35826,\"start\":35680},{\"end\":35953,\"start\":35828},{\"end\":36065,\"start\":35955},{\"end\":36195,\"start\":36110},{\"end\":36333,\"start\":36257},{\"end\":36474,\"start\":36335},{\"end\":36750,\"start\":36543},{\"end\":36853,\"start\":36800},{\"end\":37067,\"start\":37042},{\"end\":37372,\"start\":37146},{\"end\":37544,\"start\":37424},{\"end\":37875,\"start\":37833}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9680,\"start\":9629},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9711,\"start\":9701},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10533,\"start\":10439},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13426,\"start\":13416},{\"attributes\":{\"id\":\"formula_5\"},\"end\":16704,\"start\":16589},{\"attributes\":{\"id\":\"formula_6\"},\"end\":22536,\"start\":22466},{\"attributes\":{\"id\":\"formula_7\"},\"end\":22760,\"start\":22729},{\"attributes\":{\"id\":\"formula_8\"},\"end\":25463,\"start\":25426},{\"attributes\":{\"id\":\"formula_9\"},\"end\":35679,\"start\":35624},{\"attributes\":{\"id\":\"formula_10\"},\"end\":36109,\"start\":36066},{\"attributes\":{\"id\":\"formula_11\"},\"end\":36256,\"start\":36196},{\"attributes\":{\"id\":\"formula_12\"},\"end\":36542,\"start\":36475},{\"attributes\":{\"id\":\"formula_13\"},\"end\":36799,\"start\":36751},{\"attributes\":{\"id\":\"formula_14\"},\"end\":37041,\"start\":36854},{\"attributes\":{\"id\":\"formula_15\"},\"end\":37145,\"start\":37068},{\"attributes\":{\"id\":\"formula_16\"},\"end\":37423,\"start\":37373},{\"attributes\":{\"id\":\"formula_17\"},\"end\":37832,\"start\":37545}]", "table_ref": "[{\"end\":15277,\"start\":15270},{\"end\":15603,\"start\":15596},{\"end\":16925,\"start\":16918},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":17743,\"start\":17736},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":18976,\"start\":18969},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":19239,\"start\":19232},{\"end\":26144,\"start\":26136},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":28061,\"start\":28054},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":30804,\"start\":30797},{\"attributes\":{\"ref_id\":\"tab_13\"},\"end\":31535,\"start\":31528},{\"attributes\":{\"ref_id\":\"tab_13\"},\"end\":32055,\"start\":32048}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1607,\"start\":1595},{\"attributes\":{\"n\":\"2\"},\"end\":5628,\"start\":5616},{\"attributes\":{\"n\":\"3\"},\"end\":9009,\"start\":8964},{\"end\":9697,\"start\":9695},{\"attributes\":{\"n\":\"4.2\"},\"end\":14161,\"start\":14109},{\"attributes\":{\"n\":\"4.3\"},\"end\":20029,\"start\":20017},{\"attributes\":{\"n\":\"5\"},\"end\":21195,\"start\":21176},{\"end\":25963,\"start\":25961},{\"end\":32390,\"start\":32356},{\"end\":35404,\"start\":35369},{\"end\":37887,\"start\":37877},{\"end\":38378,\"start\":38358},{\"end\":38863,\"start\":38853},{\"end\":40150,\"start\":40140},{\"end\":40221,\"start\":40216},{\"end\":40266,\"start\":40256},{\"end\":40991,\"start\":40981},{\"end\":41297,\"start\":41287},{\"end\":41823,\"start\":41813},{\"end\":43081,\"start\":43076},{\"end\":43628,\"start\":43596},{\"end\":44187,\"start\":44176},{\"end\":44638,\"start\":44606},{\"end\":44908,\"start\":44897},{\"end\":45633,\"start\":45622},{\"end\":46165,\"start\":46156},{\"end\":46950,\"start\":46941},{\"end\":48338,\"start\":48331},{\"end\":48765,\"start\":48756},{\"end\":58660,\"start\":58651}]", "table": "[{\"end\":46154,\"start\":45960},{\"end\":46939,\"start\":46170},{\"end\":48329,\"start\":47519},{\"end\":49843,\"start\":48955},{\"end\":56810,\"start\":51907},{\"end\":57891,\"start\":57584},{\"end\":58649,\"start\":57911},{\"end\":59425,\"start\":58662},{\"end\":64031,\"start\":61680}]", "figure_caption": "[{\"end\":38356,\"start\":37889},{\"end\":38851,\"start\":38387},{\"end\":39391,\"start\":38865},{\"end\":39641,\"start\":39394},{\"end\":40138,\"start\":39644},{\"end\":40214,\"start\":40152},{\"end\":40254,\"start\":40223},{\"end\":40979,\"start\":40268},{\"end\":41285,\"start\":40993},{\"end\":41811,\"start\":41299},{\"end\":43074,\"start\":41825},{\"end\":43594,\"start\":43083},{\"end\":44174,\"start\":43634},{\"end\":44604,\"start\":44190},{\"end\":44895,\"start\":44643},{\"end\":45620,\"start\":44911},{\"end\":45765,\"start\":45636},{\"end\":45930,\"start\":45768},{\"end\":45960,\"start\":45933},{\"end\":46170,\"start\":46167},{\"end\":47519,\"start\":46952},{\"end\":48754,\"start\":48340},{\"end\":48955,\"start\":48767},{\"end\":51907,\"start\":49846},{\"end\":57584,\"start\":56813},{\"end\":57911,\"start\":57894},{\"end\":61680,\"start\":59428}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4753,\"start\":4747},{\"end\":10716,\"start\":10707},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":14240,\"start\":14232},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":16587,\"start\":16579},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":18114,\"start\":18105},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":19772,\"start\":19764},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":19875,\"start\":19866},{\"attributes\":{\"ref_id\":\"fig_12\"},\"end\":20421,\"start\":20413},{\"attributes\":{\"ref_id\":\"fig_12\"},\"end\":22904,\"start\":22896},{\"attributes\":{\"ref_id\":\"fig_14\"},\"end\":23742,\"start\":23734},{\"attributes\":{\"ref_id\":\"fig_16\"},\"end\":24089,\"start\":24081},{\"attributes\":{\"ref_id\":\"fig_16\"},\"end\":24334,\"start\":24326},{\"attributes\":{\"ref_id\":\"fig_16\"},\"end\":24540,\"start\":24532},{\"attributes\":{\"ref_id\":\"fig_16\"},\"end\":24761,\"start\":24753},{\"end\":25681,\"start\":25673},{\"end\":29338,\"start\":29330},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":29352,\"start\":29343},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":29381,\"start\":29372},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":30225,\"start\":30216},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":30652,\"start\":30643},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":30828,\"start\":30819},{\"attributes\":{\"ref_id\":\"fig_12\"},\"end\":34195,\"start\":34187}]", "bib_author_first_name": "[{\"end\":64873,\"start\":64866},{\"end\":64884,\"start\":64879},{\"end\":64887,\"start\":64885},{\"end\":64899,\"start\":64892},{\"end\":64911,\"start\":64910},{\"end\":64925,\"start\":64918},{\"end\":65244,\"start\":65240},{\"end\":65260,\"start\":65254},{\"end\":65277,\"start\":65269},{\"end\":65289,\"start\":65283},{\"end\":65312,\"start\":65301},{\"end\":65790,\"start\":65784},{\"end\":65805,\"start\":65797},{\"end\":65819,\"start\":65813},{\"end\":65836,\"start\":65830},{\"end\":66059,\"start\":66055},{\"end\":66073,\"start\":66068},{\"end\":66086,\"start\":66082},{\"end\":66099,\"start\":66094},{\"end\":66114,\"start\":66108},{\"end\":66495,\"start\":66491},{\"end\":66510,\"start\":66504},{\"end\":66932,\"start\":66927},{\"end\":66949,\"start\":66943},{\"end\":67173,\"start\":67170},{\"end\":67187,\"start\":67181},{\"end\":67381,\"start\":67373},{\"end\":67397,\"start\":67389},{\"end\":67412,\"start\":67406},{\"end\":67429,\"start\":67422},{\"end\":67757,\"start\":67750},{\"end\":67770,\"start\":67766},{\"end\":67786,\"start\":67782},{\"end\":67796,\"start\":67791},{\"end\":67816,\"start\":67805},{\"end\":67818,\"start\":67817},{\"end\":67834,\"start\":67828},{\"end\":67850,\"start\":67839},{\"end\":68525,\"start\":68520},{\"end\":68541,\"start\":68535},{\"end\":68770,\"start\":68762},{\"end\":68777,\"start\":68776},{\"end\":69042,\"start\":69034},{\"end\":69054,\"start\":69049},{\"end\":69072,\"start\":69065},{\"end\":69337,\"start\":69328},{\"end\":69348,\"start\":69344},{\"end\":69737,\"start\":69733},{\"end\":69753,\"start\":69744},{\"end\":69767,\"start\":69761},{\"end\":69782,\"start\":69777},{\"end\":69793,\"start\":69789},{\"end\":69806,\"start\":69800},{\"end\":70523,\"start\":70519},{\"end\":70534,\"start\":70530},{\"end\":70556,\"start\":70550},{\"end\":70574,\"start\":70565},{\"end\":70588,\"start\":70582},{\"end\":70603,\"start\":70598},{\"end\":70614,\"start\":70610},{\"end\":70627,\"start\":70621},{\"end\":71017,\"start\":71011},{\"end\":71031,\"start\":71024},{\"end\":71049,\"start\":71043},{\"end\":71063,\"start\":71055},{\"end\":71087,\"start\":71080},{\"end\":71421,\"start\":71420},{\"end\":71427,\"start\":71426},{\"end\":71436,\"start\":71435},{\"end\":71443,\"start\":71442},{\"end\":71742,\"start\":71737},{\"end\":71754,\"start\":71747},{\"end\":72051,\"start\":72047},{\"end\":72062,\"start\":72057},{\"end\":72071,\"start\":72067},{\"end\":72083,\"start\":72076},{\"end\":72649,\"start\":72642},{\"end\":72667,\"start\":72661},{\"end\":72683,\"start\":72677},{\"end\":72699,\"start\":72691},{\"end\":73183,\"start\":73177},{\"end\":73200,\"start\":73195},{\"end\":73510,\"start\":73505},{\"end\":73523,\"start\":73518},{\"end\":73534,\"start\":73530}]", "bib_author_last_name": "[{\"end\":64877,\"start\":64874},{\"end\":64890,\"start\":64888},{\"end\":64908,\"start\":64900},{\"end\":64916,\"start\":64912},{\"end\":64934,\"start\":64926},{\"end\":64939,\"start\":64936},{\"end\":65252,\"start\":65245},{\"end\":65267,\"start\":65261},{\"end\":65281,\"start\":65278},{\"end\":65299,\"start\":65290},{\"end\":65317,\"start\":65313},{\"end\":65795,\"start\":65791},{\"end\":65811,\"start\":65806},{\"end\":65828,\"start\":65820},{\"end\":65841,\"start\":65837},{\"end\":66066,\"start\":66060},{\"end\":66080,\"start\":66074},{\"end\":66092,\"start\":66087},{\"end\":66106,\"start\":66100},{\"end\":66124,\"start\":66115},{\"end\":66502,\"start\":66496},{\"end\":66520,\"start\":66511},{\"end\":66941,\"start\":66933},{\"end\":66959,\"start\":66950},{\"end\":67179,\"start\":67174},{\"end\":67193,\"start\":67188},{\"end\":67200,\"start\":67195},{\"end\":67387,\"start\":67382},{\"end\":67404,\"start\":67398},{\"end\":67420,\"start\":67413},{\"end\":67439,\"start\":67430},{\"end\":67764,\"start\":67758},{\"end\":67780,\"start\":67771},{\"end\":67789,\"start\":67787},{\"end\":67803,\"start\":67797},{\"end\":67826,\"start\":67819},{\"end\":67837,\"start\":67835},{\"end\":67856,\"start\":67851},{\"end\":68533,\"start\":68526},{\"end\":68547,\"start\":68542},{\"end\":68774,\"start\":68771},{\"end\":68782,\"start\":68778},{\"end\":68786,\"start\":68784},{\"end\":68800,\"start\":68788},{\"end\":69047,\"start\":69043},{\"end\":69063,\"start\":69055},{\"end\":69078,\"start\":69073},{\"end\":69342,\"start\":69338},{\"end\":69353,\"start\":69349},{\"end\":69742,\"start\":69738},{\"end\":69759,\"start\":69754},{\"end\":69775,\"start\":69768},{\"end\":69787,\"start\":69783},{\"end\":69798,\"start\":69794},{\"end\":69813,\"start\":69807},{\"end\":70528,\"start\":70524},{\"end\":70548,\"start\":70535},{\"end\":70563,\"start\":70557},{\"end\":70580,\"start\":70575},{\"end\":70596,\"start\":70589},{\"end\":70608,\"start\":70604},{\"end\":70619,\"start\":70615},{\"end\":70634,\"start\":70628},{\"end\":71022,\"start\":71018},{\"end\":71041,\"start\":71032},{\"end\":71053,\"start\":71050},{\"end\":71078,\"start\":71064},{\"end\":71096,\"start\":71088},{\"end\":71424,\"start\":71422},{\"end\":71433,\"start\":71428},{\"end\":71440,\"start\":71437},{\"end\":71447,\"start\":71444},{\"end\":71745,\"start\":71743},{\"end\":71757,\"start\":71755},{\"end\":72055,\"start\":72052},{\"end\":72065,\"start\":72063},{\"end\":72074,\"start\":72072},{\"end\":72087,\"start\":72084},{\"end\":72659,\"start\":72650},{\"end\":72675,\"start\":72668},{\"end\":72689,\"start\":72684},{\"end\":72710,\"start\":72700},{\"end\":73193,\"start\":73184},{\"end\":73210,\"start\":73201},{\"end\":73516,\"start\":73511},{\"end\":73528,\"start\":73524},{\"end\":73540,\"start\":73535}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":118649278},\"end\":65181,\"start\":64805},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":4555207},\"end\":65703,\"start\":65183},{\"attributes\":{\"doi\":\"arXiv:1910.01108\",\"id\":\"b2\"},\"end\":66053,\"start\":65705},{\"attributes\":{\"doi\":\"arXiv:1910.07796\",\"id\":\"b3\"},\"end\":66455,\"start\":66055},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":20714},\"end\":66857,\"start\":66457},{\"attributes\":{\"doi\":\"arXiv:1409.1556\",\"id\":\"b5\"},\"end\":67132,\"start\":66859},{\"attributes\":{\"doi\":\"arXiv:1910.05653\",\"id\":\"b6\"},\"end\":67340,\"start\":67134},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":3586416},\"end\":67669,\"start\":67342},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":990233},\"end\":68462,\"start\":67671},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":18397409},\"end\":68760,\"start\":68464},{\"attributes\":{\"doi\":\"arXiv:1905.11946\",\"id\":\"b10\"},\"end\":69032,\"start\":68762},{\"attributes\":{\"doi\":\"arXiv:1910.10699\",\"id\":\"b11\"},\"end\":69280,\"start\":69034},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":198179476},\"end\":69644,\"start\":69282},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":5034059},\"end\":70433,\"start\":69646},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":143424870},\"end\":70966,\"start\":70435},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":211132598},\"end\":71350,\"start\":70968},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":195492859},\"end\":71714,\"start\":71352},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":4076251},\"end\":72004,\"start\":71716},{\"attributes\":{\"doi\":\"10.1145/3097983.3098135\",\"id\":\"b18\",\"matched_paper_id\":26021416},\"end\":72640,\"start\":72006},{\"attributes\":{\"doi\":\"arXiv:1905.12022\",\"id\":\"b19\"},\"end\":73056,\"start\":72642},{\"attributes\":{\"doi\":\"arXiv:1612.03928\",\"id\":\"b20\"},\"end\":73439,\"start\":73058},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":368182},\"end\":73770,\"start\":73441}]", "bib_title": "[{\"end\":64864,\"start\":64805},{\"end\":65238,\"start\":65183},{\"end\":66489,\"start\":66457},{\"end\":67371,\"start\":67342},{\"end\":67748,\"start\":67671},{\"end\":68518,\"start\":68464},{\"end\":69326,\"start\":69282},{\"end\":69731,\"start\":69646},{\"end\":70517,\"start\":70435},{\"end\":71009,\"start\":70968},{\"end\":71418,\"start\":71352},{\"end\":71735,\"start\":71716},{\"end\":72045,\"start\":72006},{\"end\":73503,\"start\":73441}]", "bib_author": "[{\"end\":64879,\"start\":64866},{\"end\":64892,\"start\":64879},{\"end\":64910,\"start\":64892},{\"end\":64918,\"start\":64910},{\"end\":64936,\"start\":64918},{\"end\":64941,\"start\":64936},{\"end\":65254,\"start\":65240},{\"end\":65269,\"start\":65254},{\"end\":65283,\"start\":65269},{\"end\":65301,\"start\":65283},{\"end\":65319,\"start\":65301},{\"end\":65797,\"start\":65784},{\"end\":65813,\"start\":65797},{\"end\":65830,\"start\":65813},{\"end\":65843,\"start\":65830},{\"end\":66068,\"start\":66055},{\"end\":66082,\"start\":66068},{\"end\":66094,\"start\":66082},{\"end\":66108,\"start\":66094},{\"end\":66126,\"start\":66108},{\"end\":66504,\"start\":66491},{\"end\":66522,\"start\":66504},{\"end\":66943,\"start\":66927},{\"end\":66961,\"start\":66943},{\"end\":67181,\"start\":67170},{\"end\":67195,\"start\":67181},{\"end\":67202,\"start\":67195},{\"end\":67389,\"start\":67373},{\"end\":67406,\"start\":67389},{\"end\":67422,\"start\":67406},{\"end\":67441,\"start\":67422},{\"end\":67766,\"start\":67750},{\"end\":67782,\"start\":67766},{\"end\":67791,\"start\":67782},{\"end\":67805,\"start\":67791},{\"end\":67828,\"start\":67805},{\"end\":67839,\"start\":67828},{\"end\":67858,\"start\":67839},{\"end\":68535,\"start\":68520},{\"end\":68549,\"start\":68535},{\"end\":68776,\"start\":68762},{\"end\":68784,\"start\":68776},{\"end\":68788,\"start\":68784},{\"end\":68802,\"start\":68788},{\"end\":69049,\"start\":69034},{\"end\":69065,\"start\":69049},{\"end\":69080,\"start\":69065},{\"end\":69344,\"start\":69328},{\"end\":69355,\"start\":69344},{\"end\":69744,\"start\":69733},{\"end\":69761,\"start\":69744},{\"end\":69777,\"start\":69761},{\"end\":69789,\"start\":69777},{\"end\":69800,\"start\":69789},{\"end\":69815,\"start\":69800},{\"end\":70530,\"start\":70519},{\"end\":70550,\"start\":70530},{\"end\":70565,\"start\":70550},{\"end\":70582,\"start\":70565},{\"end\":70598,\"start\":70582},{\"end\":70610,\"start\":70598},{\"end\":70621,\"start\":70610},{\"end\":70636,\"start\":70621},{\"end\":71024,\"start\":71011},{\"end\":71043,\"start\":71024},{\"end\":71055,\"start\":71043},{\"end\":71080,\"start\":71055},{\"end\":71098,\"start\":71080},{\"end\":71426,\"start\":71420},{\"end\":71435,\"start\":71426},{\"end\":71442,\"start\":71435},{\"end\":71449,\"start\":71442},{\"end\":71747,\"start\":71737},{\"end\":71759,\"start\":71747},{\"end\":72057,\"start\":72047},{\"end\":72067,\"start\":72057},{\"end\":72076,\"start\":72067},{\"end\":72089,\"start\":72076},{\"end\":72661,\"start\":72642},{\"end\":72677,\"start\":72661},{\"end\":72691,\"start\":72677},{\"end\":72712,\"start\":72691},{\"end\":73195,\"start\":73177},{\"end\":73212,\"start\":73195},{\"end\":73518,\"start\":73505},{\"end\":73530,\"start\":73518},{\"end\":73542,\"start\":73530}]", "bib_venue": "[{\"end\":65002,\"start\":64980},{\"end\":65460,\"start\":65398},{\"end\":66679,\"start\":66609},{\"end\":68041,\"start\":67946},{\"end\":69476,\"start\":69424},{\"end\":70023,\"start\":69919},{\"end\":71874,\"start\":71825},{\"end\":72330,\"start\":72221},{\"end\":64978,\"start\":64941},{\"end\":65396,\"start\":65319},{\"end\":65782,\"start\":65705},{\"end\":66235,\"start\":66142},{\"end\":66607,\"start\":66522},{\"end\":66925,\"start\":66859},{\"end\":67168,\"start\":67134},{\"end\":67490,\"start\":67441},{\"end\":67944,\"start\":67858},{\"end\":68598,\"start\":68549},{\"end\":68876,\"start\":68818},{\"end\":69135,\"start\":69096},{\"end\":69422,\"start\":69355},{\"end\":69917,\"start\":69815},{\"end\":70685,\"start\":70636},{\"end\":71150,\"start\":71098},{\"end\":71523,\"start\":71449},{\"end\":71823,\"start\":71759},{\"end\":72219,\"start\":72112},{\"end\":72829,\"start\":72728},{\"end\":73175,\"start\":73058},{\"end\":73591,\"start\":73542}]"}}}, "year": 2023, "month": 12, "day": 17}