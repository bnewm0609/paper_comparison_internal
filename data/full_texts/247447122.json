{"id": 247447122, "updated": "2023-10-05 16:08:41.399", "metadata": {"title": "Blind2Unblind: Self-Supervised Image Denoising with Visible Blind Spots", "authors": "[{\"first\":\"Zejin\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Jiazheng\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Guoqing\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Hua\",\"last\":\"Han\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Real noisy-clean pairs on a large scale are costly and difficult to obtain. Meanwhile, supervised denoisers trained on synthetic data perform poorly in practice. Self-supervised denoisers, which learn only from single noisy images, solve the data collection problem. However, self-supervised denoising methods, especially blindspot-driven ones, suffer sizable information loss during input or network design. The absence of valuable information dramatically reduces the upper bound of denoising performance. In this paper, we propose a simple yet efficient approach called Blind2Unblind to overcome the information loss in blindspot-driven denoising methods. First, we introduce a global-aware mask mapper that enables global perception and accelerates training. The mask mapper samples all pixels at blind spots on denoised volumes and maps them to the same channel, allowing the loss function to optimize all blind spots at once. Second, we propose a re-visible loss to train the denoising network and make blind spots visible. The denoiser can learn directly from raw noise images without losing information or being trapped in identity mapping. We also theoretically analyze the convergence of the re-visible loss. Extensive experiments on synthetic and real-world datasets demonstrate the superior performance of our approach compared to previous work. Code is available at https://github.com/demonsjin/Blind2Unblind.", "fields_of_study": "[\"Engineering\",\"Computer Science\"]", "external_ids": {"arxiv": "2203.06967", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/WangLL022", "doi": "10.1109/cvpr52688.2022.00207"}}, "content": {"source": {"pdf_hash": "3949f666dc13ee806a119e0e1e0ea7625e1b20c1", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2203.06967v3.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2203.06967", "status": "GREEN"}}, "grobid": {"id": "206db31698456578abd369b6de0d7b6f75fb6726", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/3949f666dc13ee806a119e0e1e0ea7625e1b20c1.txt", "contents": "\nBlind2Unblind: Self-Supervised Image Denoising with Visible Blind Spots\n\n\nZejin Wang \nInstitute of Automation\nNational Laboratory of Pattern Recognition\nChinese Academy of Sciences\n\n\nSchool of Artificial Intelligence\nUniversity of Chinese Academy of Sciences\n\n\nJiazheng Liu \nInstitute of Automation\nNational Laboratory of Pattern Recognition\nChinese Academy of Sciences\n\n\nSchool of Future Technology\nUniversity of Chinese Academy of Sciences\n\n\nGuoqing Li \nInstitute of Automation\nNational Laboratory of Pattern Recognition\nChinese Academy of Sciences\n\n\nHua Han \nInstitute of Automation\nNational Laboratory of Pattern Recognition\nChinese Academy of Sciences\n\n\nSchool of Future Technology\nUniversity of Chinese Academy of Sciences\n\n\nBlind2Unblind: Self-Supervised Image Denoising with Visible Blind Spots\n\nReal noisy-clean pairs on a large scale are costly and difficult to obtain. Meanwhile, supervised denoisers trained on synthetic data perform poorly in practice. Selfsupervised denoisers, which learn only from single noisy images, solve the data collection problem. However, selfsupervised denoising methods, especially blindspot-driven ones, suffer sizable information loss during input or network design. The absence of valuable information dramatically reduces the upper bound of denoising performance. In this paper, we propose a simple yet efficient approach called Blind2Unblind to overcome the information loss in blindspot-driven denoising methods. First, we introduce a global-aware mask mapper that enables global perception and accelerates training. The mask mapper samples all pixels at blind spots on denoised volumes and maps them to the same channel, allowing the loss function to optimize all blind spots at once. Second, we propose a re-visible loss to train the denoising network and make blind spots visible. The denoiser can learn directly from raw noise images without losing information or being trapped in identity mapping. We also theoretically analyze the convergence of the re-visible loss. Extensive experiments on synthetic and real-world datasets demonstrate the superior performance of our approach compared to previous work. Code is available at https://github.com/demonsjin/Blind2Unblind.\n\nIntroduction\n\nImage denoising, an essential task of low-level image processing, aims to remove noise and restore a clean image. In vision applications, the quality of denoising significantly affects the performance of downstream tasks, such as superresolution [16], semantic segmentation [22], and object detection [31]. In addition, the denoiser can significantly improve the quality of images captured by mobile phones and other devices, reflecting a broad demand in imaging fields.\n\n\n* Corresponding author\n\nWith the development of neural networks, learningbased denoisers [3,6,13,29,35,36,38,39] have recently shown superior performance than traditional methods [5,8,9,12]. However, supervised denoisers, e.g., U-Net [29], DnCNN [38], FFDNet [39], RIDNet [3], SANet [6], rely on numerous noisy-clean pairs, which are costly and hard to collect. The performance of denoisers drops dramatically once processing unknown noise patterns. Lehtinen et al. [21] then propose to recover clean signals directly from corrupted image pairs. Using corrupted pairs reduces the difficulty of data collection but remains challenging for dynamic scenes with deformations and image quality variations.\n\nTo alleviate the above limitations, self-supervised denoising [4,15,18,20,26,32,33] that learns from a single noisy image has attracted much interest from researchers. Ulyanov et al. [32] learn deep prior only from a single noisy image. Namely, each degraded image has to be trained from scratch. Manual masking, e.g., Noise2Self [4], Noise2Void [18], avoids custom denoising for each image. Since blind spots on the inputs occupy a large area, the receptive field of predicted pixels loses much valuable context, resulting in poor performance. Moreover, optimizing partial pixels in each iteration causes slow convergence. Laine et al. [20] design a blind spot network to bound the receptive field in four directions instead of manual masking. Masked convolution accelerates training and increases the receptive field to all areas except the blind spot. Similarly, the dilated blindspot network [33] sets blindspots on the receptive field without masking the inputs. Regardless of masked input or blind-spot networks, lower accuracy limits practical applications. Bayesian estimation [19,20,33] is used for explicit noise modeling as post-processing. However, noise modeling performs poorly on real-world data with complex patterns. Some works [25,34] perform noise reduction for noisier-noise pairs even though the additional noise increases the information loss and requires that the extra noise has the same distribution as the original one. Subsequently, Pang et al. [26] develop a data augmentation technique with the known noise level to address the overfit-ting caused by the absence of truth images. Recently, Huang et al. [15] propose to train the network with training pairs sub-sampled from the same noisy image. However, using sub-sampling pairs for supervision lead to over smoothing as neighboring pixels are approximated.\n\nIn this paper, we propose Blind2Unblind, a novel selfsupervised denoising framework that overcomes the above limitations. Our framework consists of a global-aware mask mapper based on mask-driven sampling and a training strategy without blind spots based on re-visible loss. Specifically, we divide each noisy image into blocks and set specific pixels in each block as blind spots, so that we can obtain a global masked volume as input, which consists of a set of images with order masks. Then, the volume with global masks is fed into the network in the same batch. The global mapper samples denoised volumes at blind spots and projects them onto the same plane to generate denoised images. The operation speeds up training, enables global optimization, and allows the application of re-visible loss. However, masked images result in a sizable loss of valuable information, severely reducing the upper bound of denoising performance. Therefore, we consider learning from raw noisy images without masks and relief from identity mapping. Furthermore, the intermediate medium of gradient update must be introduced since raw noisy images cannot participate in backpropagation during training. We assume that masked images serve as a medium and propose a re-visible loss to enable the transition from blind-spot denoising to non-blind denoising. The proposed self-supervised denoising framework, which does not involve noise model prior or the removal of valuable information, shows surprising performance. Moreover, advanced models can be applied to our proposed method.\n\nThe contributions of our work are as follows: 1. We propose a novel self-supervised denoising framework that makes blind spots visible, without subsample, noise model priors and identity mapping. 2. We provide a theoretical analysis of re-visible loss and present its upper and lower bounds on convergence. 3. Our approach shows superior performance compared with state-of-the-art methods, especially on real-world datasets with complex noise patterns.\n\n\nRelated Work\n\n\nNon-Learning Image Denoising\n\nNon-learning denoising methods such as BM3D [9], CBM3D [8], WNNM [12], and NLM [5] usually iteratively perform custom denoising for each noisy image. However, since the denoising procedure is iteratively adjusted and the hyperparameters are limited, these methods suffer from long inference time and poor performance.\n\n\nSupervised Image Denoising\n\nRecently, many supervised approaches for image denoising have been developed. Zhang et al. [38], the pioneer in deep image denoising, first proposes DnCNN to process unknown noise levels using noisy-clean pairs as supervision. Then, U-Net [29] becomes a common denoiser based on multi-scale features. After that, more advanced denoisers [3,6,13,35,36,39] are proposed to improve denoising performance under supervision. However, collecting noiseclean pairs remains challenging in practice since the cost of acquisition, deformation, and contrast variations. Moreover, supervision with a strong prior often performs poorly when noise patterns are unknown.\n\n\nSelf-Supervised Image Denoising\n\nNoise2Noise [21] uses noisy-noisy pairs for training, which reduces the difficulty of data collection. Then, Noise2Self [4] and Noise2Void [18] propose masked schemes for denoising on individual noisy images. Since some areas on noisy images are blind, the accuracy of denoising is low. Laine19 [20], DBSN [33], and Probabilistic Noise2Void [19] transfer the masking procedure to the feature level for larger receptive fields, which reduces valuable information loss. Moreover, noise model priors are also introduced as post-processing to improve performance. Self2Self [28] trains a dropout denoiser on the pair generated by the Bernoulli sampler and averages the predictions of multiple instances. Noisy-As-Clean [34] and Nois-ier2Noise [25] introduce added noise to train the denoiser and require a known noise distribution, limiting their use in practice. Similarly, R2R [26] also corrupts noisy images with known noise levels to obtain training pairs. Recently, Neighbor2Neighbor (NBR2NBR) [15] obtains the noise pair for training by sub-sampling the noise image. However, approximating neighbor pixels leads to over smoothing, and sub-sampling destroys the structural continuity.\n\n\nTheoretical Framework\n\n\nMotivation\n\nClassical blindspot denoising methods, e.g., masked inputs [4,18,19] and blindspot networks [20,33], use an artificial masking scheme to form blind-noisy pairs. However, since the mask prior is suboptimal and lossy, their performance is severely limited. Intuitively, denoising a raw noisy image without blind spots can solve the performance degradation. Given a single raw noisy image, we assume that the model can perform denoising without losing valuable information. The only thing is to teach the model how to learn and what to learn. Since the model now has to learn denoising from the raw noisy image without blind spots, eliminating identity mapping is critical. We consider that using the masked input as a medium for updating the gradi-ent prevents identity mapping. The challenge now remains to design a novel loss that associates blind spot denoising with raw noisy image denoising.\n\n\nNoise2Void Revisit\n\nNoise2Void [18] is a self-supervised denoising method using a training scheme that does not require noisy image pairs or clean target images. The approach only requires a single noisy image and then applies a blind-spot masking scheme to generate blind-noisy pairs for training. Given the noisy observation y of the ground truth x, Noise2Void aims to minimize the following empirical risk:\narg min \u03b8 E y \u2225f \u03b8 (y RF(i) ) \u2212 y i \u2225 2 2 ,(1)\nwhere f \u03b8 (\u00b7) denotes the denoising model with parameter \u03b8, y RF(i) is a patch around pixel i, y i means a single pixel i located at the patch center. This method assumes that noise is context-independent while signal relies on surrounding pixels. The blind spot architecture takes advantage of the above properties and thus relief itself from identity mapping.\n\n\nRe-Visible without Identity Mapping\n\nThe blind spot scheme [4,18] can use less information for its prediction. Therefore, its accuracy is lower than that of the normal network. The challenge here is how to convert the invisible blind spots into visible ones. In this way, we can use the structure of blind spots for self-supervised denoising and then use all the information to improve its performance. First, we see the loss in the multi-task form of denoising as follows:\narg min \u03b8 E y \u2225h(f \u03b8 (\u2126 y )) \u2212 y\u2225 2 2 + \u03bb \u00b7 \u2225f \u03b8 (y) \u2212 y\u2225 2 2 ,(2)\nnote that \u2126 y denotes a noisy masked volume in which the contained blind spots are exactly at all positions of y, h(\u00b7) is a global-aware mask mapper that samples the denoised pixels where the blind spots are located, and \u03bb is a constant. Using Eq. (2) as the objective function for training learns the identity. That is, f \u03b8 (y) cannot explicitly participate in backpropagation. We hope this element can implicitly participate in the gradient update and realize the transition from blind to non-blind. We reformulate Eq. (2) in the 1-norm form:\narg min \u03b8 E y \u2225h(f \u03b8 (\u2126 y )) \u2212 y\u2225 1 + \u03bb \u00b7 \u2225f \u03b8 (y) \u2212 y\u2225 1 ,(3)\nwheref \u03b8 (\u00b7) means that they do not contribute to updating the gradient. That is,f \u03b8 (y) can be considered as an ordinary constant. It only remains to design a new objective function and the derivative of f \u03b8 (\u2126 y ) should containf \u03b8 (y) to satisfy the non-blind requirement. We assume that the quadratic operation on Eq. (3) can achieve the implicit goal of revisible and satisfy the average arithmetic form of the optimal solution for the denoising task, which then becomes:\narg min \u03b8 E y \u2225|h(f \u03b8 (\u2126 y )) \u2212 y| + \u03bb \u00b7 |f \u03b8 (y) \u2212 y|\u2225 2 2 , (4)\nwhere | \u00b7 | denotes the absolute value of each element in the vector. Direct application of Eq. (4) as the objective function is not appropriate, since the symbol of different terms reflects opposite optimization objectives. Specifically, we extend Eq. (4) as follows:\nT (y) = \u2225|h(f \u03b8 (\u2126 y )) \u2212 y| + \u03bb \u00b7 |f \u03b8 (y) \u2212 y|\u2225 2 2 = \u2225h(f \u03b8 (\u2126 y )) \u2212 y\u2225 2 2 + \u03bb 2 \u2225f \u03b8 (y) \u2212 y\u2225 2 2 + 2\u03bb\u2225(h(f \u03b8 (\u2126 y )) \u2212 y) \u2299 (f \u03b8 (y) \u2212 y)\u2225 1 .(5)\nLet\nd 1 = h(f \u03b8 (\u2126 y )) \u2212 y, d 2 =f \u03b8 (y) \u2212 y, cond = (h(f \u03b8 (\u2126 y )) \u2212 y) \u2299 (f \u03b8 (y) \u2212 y).\nHere, we divide the objective function T (y) into two cases:\ni) If cond \u2a7e 0, we have T (y) = \u2225h(f \u03b8 (\u2126 y )) + \u03bbf \u03b8 (y) \u2212 (\u03bb + 1)y\u2225 2 2 ; (6)\nii) Similarly, when cond < 0, it holds that\nT (y) = \u2225f \u03b8 (y)\u2212h(f \u03b8 (\u2126 y ))+(\u03bb\u22121)(f \u03b8 (y)\u2212y)\u2225 2 2 . (7)\nNote that when cond < 0, the learning objective off \u03b8 (y) becomes h(f \u03b8 (\u2126 y )), instead of y as Eq. (6). Considering the optimal denoiser f * \u03b8 that is trained to convergence, thenf * \u03b8 (y) should be approximate to h(f * \u03b8 (\u2126 y )) in Eq. (7). However, the masking operation \u2126 (\u00b7) causes information loss, resulting in low denoising upper bound for h(f \u03b8 (\u2126 y )). Besides,f \u03b8 (y) \u2212 h(f \u03b8 (\u2126 y )) in Eq. (7) indicates thatf \u03b8 (y) should approximate h(f \u03b8 (\u2126 y )), which in turn suppresses the performance off \u03b8 (y). Combining the above analysis, the final re-visible loss can be formulated as:\narg min \u03b8 E y \u2225h(f \u03b8 (\u2126 y )) + \u03bbf \u03b8 (y) \u2212 (\u03bb + 1)y\u2225 2 2 . (8)\nWhen the denoiser converges to f * \u03b8 , the following holds with the optimal solutionx of Eq. (8):\nx = h(f * \u03b8 (\u2126 y )) + \u03bbf * \u03b8 (y) \u03bb + 1 .(9)\nWe assume that h(f * \u03b8 (\u2126 y )) = x + \u03b5 1 andf * \u03b8 (y) = x + \u03b5 2 . Empirically, \u2225\u03b5 1 \u2225 1 > \u2225\u03b5 2 \u2225 1 . With Eqs. (4) and (8), the upper and lower bounds ofx are respectivelyf * \u03b8 (y) and h(f * \u03b8 (\u2126 y )). Namely, h(f * \u03b8 (\u2126 y )) \u2264x \u2264f * \u03b8 (y). Therefore, we can consider the generation of denoised imagesx only from noisy original images y during inference. Given y, it holds that lim \u03bb\u2192+\u221ex =f * \u03b8 (y). Figure 1. Overview of our proposed Blind2Unblind framework. (a) Overall training process. The global masker \u2126 (\u00b7) creates a masked volume by adding blind spots to a noisy image y. Then, the global-aware mask mapper samples the denoised volume to obtain h(f \u03b8 (\u2126y)).\nDenoising Network (\ufffd) Global Masker (\ufffd) Global Mask Mapper (\ufffd) \u210e ( ) ( ) \u210e ( ) + ( ) + 1 ( ) w/o gradient Denoising Network (\ufffd) ( ) (a) Training Phase (b) Inference Phase\nMeanwhile, the denoiser f \u03b8 (\u00b7) takes y as input and produces the denoised result f \u03b8 (y). The re-visible loss realizes the transition from the blind to visible with the invisible term h(f \u03b8 (\u2126y)) as a medium. Moreover, the regular term is used to stabilize the training phase. (b) Inference using the trained denoising model. The denoising network generates denoised images directly from noisy images y without additional operation.\n\n\nStabilize Transition Procedure\n\nAs described in Section 3.3, the blind part |h(f \u03b8 (\u2126 y )) \u2212 y| is used as a transition to optimize the re-visible one \u03bb \u00b7 |f \u03b8 (y) \u2212 y|. Therefore, the cumulative error of the blind part can also affect the non-blind part. However, the pure revisible loss lacks the separate constraint just for the transition term, which aggravates the instability during the training phase. We consider adding an extra constraint to correct f \u03b8 (\u2126 y ), forcing the blind part to be zero. Thus, based on Eq. (8), we have the following constrained optimization problem:\nmin \u03b8 E y \u2225h(f \u03b8 (\u2126 y )) + \u03bbf \u03b8 (y) \u2212 (\u03bb + 1)y\u2225 2 2 , s.t. \u2225h(f \u03b8 (\u2126 y )) \u2212 y\u2225 2 2 = 0.(10)\nWe further reformulate it as the following optimization problem with a regularization term:\nmin \u03b8 E y \u2225h(f \u03b8 (\u2126 y )) + \u03bbf \u03b8 (y) \u2212 (\u03bb + 1)y\u2225 2 2 + \u03b7 \u00b7 \u2225h(f \u03b8 (\u2126 y )) \u2212 y\u2225 2 2 = 0.(11)\n\nMain Method\n\nBased on the theoretical analysis in Section 3, we propose Blind2Unblind, a novel deep self-supervised denoising framework learning from a single observation of noisy images. The framework consists of two main components: global-aware mask mapper and re-visible loss. The mask mapper generates masked volumes of noisy images as \u2126 y and then samples denoised pixels in which there are blind spots to form the final denoised results. Moreover, we introduce a regularized re-visible loss to convert the blind spots into visible ones and overcome the challenge of having less information based on blind spots. An overview of our proposed Blind2Unblind framework can be found in Figure 1 \n\n\nGlobal-Aware Mask Mapper\n\nManual masking methods [4,18] hide part of pixels for training, and objective function focuses only on masked regions, leading to a decrease in accuracy and slow convergence. The mask mapper performs global denoising on blind spots. This mechanism constrains all pixels, promotes information exchange across whole masked regions, and increases noise reduction accuracy. In addition, our mask mapper also speeds up the training of manual masking schemes. The workflow of masked volumes generation and global mapping using mask mapper is shown in Figure 2. Given a noisy image y with width W and height H, details of the global-aware mask mapper h (f \u03b8 (\u2126 y )) are described as follows:\n\n1. First, the noisy image y is divided into \u230aW/s\u230b\u00d7\u230aH/s\u230b cells, each of size s \u00d7 s. For simplicity, we set s = 2. 2. The pixels in the i-th row and j-th column of each cell are masked and filled with black for illustration. Thus, the masked image \u2126 ij y consists of \u230aW/s\u230b \u00d7 \u230aH/s\u230b blind spots, where i, j \u2208 {0, ..., s \u2212 1}. These mask images are further stacked to form a masked volume \u2126 y . 3. For positions on the denoised volumes f \u03b8 (\u2126 y ) corresponding to blind spots, the mask mapper h(\u00b7) samples them and assigns them to the same layer. In this way, a globally denoised image h (f \u03b8 (\u2126 y )) is obtained. \n\n\nRegularized Re-Visible Loss\n\nHere we present how to use the Blind2Unblind framework for self-supervised denoising. Since blind spot denoising plays a mediating role, the training process should follow the transition from blind to non-blind. However, the pure re-visible loss uses only a single variable that can be backpropagated to optimize both the blind term and the visible term, resulting in unstable training. Therefore, a regular term is introduced to constrain the blind term and stabilize the training procedure. The re-visible loss with regularization is as follows:\nL = L rev + \u03b7 \u00b7 L reg = \u2225h(f \u03b8 (\u2126 y )) + \u03bbf \u03b8 (y) \u2212 (\u03bb + 1)y\u2225 2 2 + \u03b7 \u00b7 \u2225h(f \u03b8 (\u2126 y )) \u2212 y\u2225 2 2 .(12)\nwhere f \u03b8 (\u00b7) denotes an arbitrary denoising network with different structures, h(\u00b7) is a mask mapper capable of global modeling, \u03b7 is a fixed hyper-parameter that determines the initial contribution of the blind term and the training stability, and \u03bb is a variable hyper-parameter that controls the intensity of visible parts when converting from blind to unblind. The initial and final values of \u03bb are \u03bb s and \u03bb f , respectively. To prevent identity mapping, we disable the gradient updating off \u03b8 (y) during training.\n\n\nExperimental Results\n\n\nImplementation Details\n\nTraining Details. We use the same modified U-Net [29] architecture as [15,20]. The batch size is 4. We use Adam [17] as our optimizer, with a weight decay of 1e \u22128 to avoid overfitting. The initial learning rate is 0.0003 for synthetic denoising in sRGB space and 0.0001 for real-world denoising, including raw-RGB space and fluorescence microscopy (FM). The learning rate decreases by half every 20 epochs, for 100 training epochs. As for the hyper-parameters included in the re-visible loss, we set \u03b7 = 1, \u03bb s = 2 and \u03bb f = 20 empirically. We also randomly crop 128 \u00d7 128 patches for training. In practice, masked pixels are a weighted average of pixels in a 3 \u00d7 3 neighborhood, as advocated in [18]. All models are trained on a server using Python 3.8.5, Pytorch 1.7.1 [27], and Nvidia Tesla V100 GPUs.\n\nDatasets for Synthetic Denoising. Following the setting in [15,20], we select 44,328 images with sizes between 256 \u00d7 256 and 512 \u00d7 512 pixels from the ILSVRC2012 [10] validation set as the training set. To obtain reliable average PSNRs, we also repeat the test sets Kodak [11], BSD300 [24] and Set14 [37]  For grayscale image denoising, we use BSD400 [38] for training and three widely used datasets for testing, including Set12, BSD68 [30] and Urban100 [14]. Datasets for Real-World Denoising. Following the setting in [15], we take the SIDD [2] dataset collected by five smartphone cameras in 10 scenes under different lighting conditions for real-world denoising in raw-RGB space. We use only raw-RGB images in SIDD Medium Dataset for training and use SIDD Validation and Benchmark Datasets for validation and testing. As for real-world grayscale denoising on FM, we consider FMDD [40] dataset, which contains 12 datasets of images captured using either a confocal, two-photon, or widefield microscope. Each dataset contains 20 views with 50 noisy images per view. We select three datasets (Confocal Fish, Confocal Mice and Two-Photon Mice) and train with each dataset, using the 19th view for testing and the rest for training. Details of Experiments. For fair comparison, we follow the experimental settings of NBR2NBR [15]. For the baseline, we consider two supervised denoising methods (N2C [29] and N2N [21]). We also compare the proposed Blind2Unblind with a traditional approach (BM3D [9]) and seven self-supervised denoising algorithms (Self2Self [28], Noise2Void (N2V) [18], Laine19 [20], Noisier2Noise [25],    DBSN [33], R2R [26] and NBR2NBR [15]).\n\nIn synthetic denoising, we use pre-trained models provided by [20] for N2C, N2N, and Laine19, retaining the same network architecture as [15,20]. In addition, we use CBM3D [8], a multi-channel version of BM3D, to perform Gaussian denoising combined with the parameter \u03c3 estimated by [7]. For Poisson noise, Anscombe [23] is first run to convert Poisson noise into Gaussian distribution, and then BM3D is used for denoising. For Self2Self, N2V, Nois-ier2Noise, DBSN, R2R and NBR2NBR, we use the official implementation.\n\nFor real-world denoising in raw-RGB space, all methods use their official implementations and have been retrained on the SIDD Medium Dataset. Note that we split the singlechannel raw image into four sub-images according to the Bayer pattern. For BM3D, we denoise the four sub-images individually and then integrate denoised sub-images into a whole image. For deep learning methods, we stack four sub-images to form a four-channel image for denoising and then integrate the denoised image into raw-RGB space. For real-world denoising on FM, we retrain all compared methods using official implementation.\n\n\nResults for Synthetic Denoising\n\nThe quantitative comparison results of synthetic denoising for Gaussian and Poisson noise can be seen in Table 1. For Gaussian noise, whether the noise level is fixed or variable, our approach significantly outperforms the traditional denoising method BM3D and five self-supervised denoising methods, including Self2Self, N2V, Laine19-mu, DBSN, and NBR2NBR. R2R also performs worse than our method under variable Gaussian noise, despite a known noise prior. For fixed Poisson noise, our approach shows comparable performance to Laine19-pme, which is based on explicit noise modeling. However, explicit noise modeling means strong prior, leading to poor performance on real data. The following experiments on real-world datasets also illustrate this problem. When the Poisson noise level is variable, our method outperforms almost all methods to be compared, including Laine19-pme. Moreover, our approach outperforms the supervision baseline (N2C) with a gain of 0.13 dB and 0.08 dB on BSD300 and SET14, respectively. Namely, our method shows more superior performance for removing Poisson noise. In addition, whether the noise is fixed or variable, our method significantly outperforms the SOTA method NBR2NBR, with a maximum gain of 0.38 dB. Figures 3 and 4 illustrate the sRGB denoising results in the setting of \u03c3 = 25 and \u03bb = 30, respectively. Compared with the recent best NBR2NBR, our method recovers more texture details. Table 2 conducts additional experiments on grayscale images. As the noise increases, our method gradually outperforms R2R, because re-corruption loses more information, offsetting the noise prior advantage. Moreover, our method infers directly, while R2R repeats 50 times.\n\n\nResults for raw-RGB Denoising\n\nIn raw-RGB space, Table 3 shows the quality scores for quantitative comparisons on SIDD Benchmark and SIDD Validation. Note that the online website [1] evaluates the quality scores for the SIDD Benchmark. In general, the proposed method outperforms the state-of-the-art (NBR2NBR) by 0.32 dB and 0.20 dB for the benchmark and validation. Because the transition from blind to non-blind preserves all information contained within noisy images and mitigates the effects of over smoothing among neighboring pixels.  Figure 5. Visual comparison of denoising raw-RGB images in the challenging SIDD benchmark. The conversion from raw-RGB to sRGB is performed using the official ISP tool.  Table 3. Quantitative denoising results on SIDD benchmark and validation datasets in raw-RGB space.\n\nMoreover, our method performs far better than R2R in the real world. Two primary reasons are lossy re-corruption and inaccurate noise priors in R2R. Interestingly, our approach outperforms the baseline (N2C) by 0.19 dB and 0.17 dB in the benchmark and validation, indicating improved generalization. The raw-RGB denoising performance in the real world demonstrates that the proposed approach is competitive in the presence of complex noise patterns. Figure 5 demonstrates our approach's superiority over alternative methods. Our method recovers more texture details and has a higher degree of continuity, whereas NBR2NBR exhibits obvious sawtooth diffusion defects due to the training scheme of approximating neighboring pixels. Table 4 shows the quantitative comparison of real-world fluorescence microscopy datasets. Our approach shows competitive performance against other methods. In particular, our method greatly outperforms self-supervised methods and slightly outperforms supervised methods (N2C and N2N) on both Confocal Mice and Two-Photon Mice. The results further confirm the superior performance of our method on complex real-world noise patterns. Figure 6 shows a visual comparison of denoising FM images.  \n\n\nResults for FM Denoising.\n\n\nAblation Study\n\nThis section conducts ablation studies on the loss function, mask strategy, visible term, and regular term. Note that PSNR(dB)/SSIM is evaluated on Kodak. Ablation study on Loss Function. Table 5 performs ablation experiments under two loss conditions. Sincef \u03b8 (y) in Eq. (7) is suppressed by the mask term h(f \u03b8 (\u2126 y )), the performance of L B is much lower than that of L A . Ablation study on Mask Mapper. To evaluate our globalaware mask mapper (GM), we compare it with the random mask (RM) as advocated in Noise2Void [18]. Details are provided in the supplementary materials. Table 6 lists the performance of different mask strategies in self-supervised denoising. We can see that our GM considerably outperforms RM for all four noise patterns. Moreover, GM+V performs much better combined with re-visible loss, while RM+V is severely suppressed and cannot converge. Ablation study on Visible Term. The hyper-parameter \u03bb f is positively correlated with the visibility of the non-blind term. Table 7 shows that the weight of the visible item is not the larger, the better: 1) When \u03bb f = 20, our approach achieves or approaches the best performance on four noise patterns. 2) When \u03bb f varies from 20 to 40, the performance on Poisson noise decreases. In contrast, the performance on Gaussian noise remains fixed. 3) When \u03bb f > 40, the performance is almost unchanged. In this paper, we set \u03bb f = 20.  Ablation study on Regularization Term. The hyperparameter \u03b7 controls the stability of re-visible loss. Table 8 shows that the denoising performance first increases and then decreases on four noise patterns. In particular, when \u03b7 = 1, the performance is at or closed to the best, which proves that the regular term strengthens the training stability and improves the performance. In this paper, we set \u03b7 = 1.\n\n\nConclusion\n\nWe propose Blind2Unblind, a novel self-supervised denoising framework, which achieves lossless denoising through the transition from blind to non-blind and enables blindspot schemes to use complete noisy images without information loss. The global-aware mask mapper samples the denoised volume at blind spots, and then re-visible loss realizes non-blind denoising under visible blind spots. Extensive experiments have shown the superiority of our approach against compared methods, especially for complex noise patterns.\n\n\nA. Training Framework for Blind2Unblind\n\nThe training framework for Blind2Unblind is shown in Algorithm 1.\n\n\nAlgorithm 1: Blind2Unblind\n\nInput: A set of noisy images Y = {y i } n i=1 ; Denoising network f \u03b8 (\u00b7); Hyper-parameters \u03b7, \u03bb; while not converged do Sample a noisy image y; Generate a global masker \u2126 (\u00b7) ; Derive a masked volume \u2126 y , where \u2126 y is the network input, and y is the network target; For the network input \u2126 y , derive the denoised volume f \u03b8 (\u2126 y ); Global mask mapper h (\u00b7) samples the denoised volume f \u03b8 (\u2126 y ) at blind spots, then obtain a blind denoised image h(f \u03b8 (\u2126 y )); For the original noisy image y, derive the visible denoised imagef \u03b8 (y) without gradients; Calculate re-visible loss L rev = \u2225h(f \u03b8 (\u2126 y )) + \u03bbf \u03b8 (y) \u2212 (\u03bb + 1)y\u2225 2 2 ; Calculate regularization L reg = \u2225h(f \u03b8 (\u2126 y )) \u2212 y\u2225 2 2 ; Update network parameters \u03b8 by minimizing the regularized re-visible loss L rev + \u03b7 \u00b7 L reg ; end B. Details of Interpolation from Neighbors Figure 7 shows the workflow of interpolation from neighbors. The workflow can be divided into the following three steps: 1) The mask is generated by random masking each 2 \u00d7 2 cells in image y. The kernel convolves image y with stride 1 and padding 1 to produce y c . Then, y m is obtained via Hadamard product y c \u2022 mask. 2) We perform Hadamard product y \u2022 (1 \u2212 mask) to gernerate y inv . 3) Sum by y m and y inv , we finally obtain the masked image \u2126 y .\n\n\nC. Details of Random Mask Strategy\n\nThe illustration of the random mask strategy is presented in Figure 8. The image y is divided into several blocks with 2x2 cells. A specific pixel in each cell is randomly set as a blind spot. Namely, there are four ways for random masking of 2x2 cells. After random masking, the masked image \u2126 y is fed into the denoising network to generate the denoised image f \u03b8 (\u2126 y ).    Baseline, N2N BM3D Ours NBR2NBR Figure 10. Visual comparison of denoising raw-RGB images in the challenging SIDD benchmark.\n\n\nD. More Experimental Results\n\n\n.\n\nFigure 2 .\n2Details of a global-aware mask mapper. Taking a 2 \u00d7 2 cell y as an example, the global masker \u2126 (\u00b7) hides four spots in y to create a global masked volumes \u2126y consisting of four masked cells \u2126 ij y , i, j \u2208 {0, 1}. The mask mapper h(\u00b7) samples the denoised volumes f \u03b8 (\u2126y) in which there are blind spots. The final denoised cell h(f \u03b8 (\u2126y)) is formed by the mapper based on the sampled locations and pixel values.\n\n\nby 10, 3 and 20 times, respectively. Thus, all methods are evaluated with 240, 300, and 280 individual synthetic noise images. Specifically, we consider four types of noise in sRGB space: (1) Gaussian noise with \u03c3 = 25, (2) Gaussian noise with \u03c3 \u2208 [5, 50], (3) Poisson noise with \u03bb = 30, (4) Poisson noise with \u03bb \u2208 [5, 50]. Here, values of \u03c3 are given in 8-bit units.\n\nFigure 3 .\n3Visual comparison of denoising sRGB images in the setting of \u03c3 = 25.\n\nFigure 4 .\n4Visual comparison of denoising sRGB images in the setting of \u03bb = 30.\n\nFigure 9 Figure 7 .Figure 8 .\n978illustrates the steps of our proposed method while denoising sRGB images in the setting of \u03c3 = 25. Fig-Details Details of random mask strategy. ure 10 shows the visual comparison of denoising raw-RGB images in the challenging SIDD benchmark.\n\nFigure 9 .\n9The steps of our proposed method while denoising sRGB images in the setting of \u03c3 = 25. Blind denotes h(f \u03b8 (\u2126y)), Visible denotesf \u03b8 (y), and Weighted denotes h(f * \u03b8 (\u2126y ))+\u03bbf * \u03b8 (y) \u03bb+1.\n\nTable 4 .\n4Quantitative denoising results on Confocal Fish, Confocal Mice and Two-Photon Mice. G is for Gaussian and P is for Poisson.\n\nTable 5 .\n5Ablation study on the loss function. LA and LB denote Eq. (6) and Eq. (7).Table 6. Ablation study on mask mappers. RM, GM and V denote random mask mapper, global-aware mask mapper, and re-visible loss. -/ -means cannot converge.Table 7. Ablation study on visible term. Note that \u03b7 = 1, \u03bbs = 2. Gaussian \u03c3 \u2208 [5, 50] 32.21/0.870 32.34/0.872 32.31/0.872 32.23/0.871 Poisson \u03bb = 30 31.54/0.869 31.64/0.871 31.54/0.869 31.52/0.869 Poisson \u03bb \u2208 [5, 50] 31.03/0.856 31.07/0.857 31.06/0.857 31.04/0.857Table 8. Ablation study on regular term. Here, \u03bbs = 2, \u03bb f = 20.Noise Type \nRM \nGM \nRM+V \nGM+V \n\nGaussian \u03c3 = 25 \n30.32/0.831 30.56/0.839 \n-/ -\n32.27/0.880 \nGaussian \u03c3 \u2208 [5, 50] 30.23/0.822 30.46/0.831 \n-/ -\n32.34/0.872 \nPoisson \u03bb = 30 \n29.89/0.821 30.11/0.830 \n-/ -\n31.64/0.871 \nPoisson \u03bb \u2208 [5, 50] \n29.26/0.796 29.67/0.817 \n-/ -\n31.07/0.857 \n\nNoise Type \n\u03bb f = 2 \n\u03bb f = 20 \n\u03bb f = 40 \n\u03bb f = 100 \n\nGaussian \u03c3 = 25 \n32.11/0.879 32.27/0.880 32.27/0.879 32.26/0.879 \nGaussian \u03c3 \u2208 [5, 50] 32.00/0.871 32.34/0.872 32.34/0.872 32.35/0.873 \nPoisson \u03bb = 30 \n31.47/0.869 31.64/0.871 31.52/0.869 31.51/0.869 \nPoisson \u03bb \u2208 [5, 50] \n30.94/0.854 31.07/0.857 31.02/0.855 31.01/0.854 \n\nNoise Type \n\u03b7 = 0 \n\u03b7 = 1 \n\u03b7 = 2 \n\u03b7 = 3 \n\nGaussian \u03c3 = 25 \n32.19/0.877 32.27/0.880 32.32/0.881 32.21/0.878 \n\nAcknowledgmentsThis work is jointly supported by National Natural\n. Abdelrahman Abdelhamed, Website, 11-01. 7Abdelrahman Abdelhamed. Website. https://www. eecs.yorku.ca/\u02dckamel/sidd/benchmark.php. Accessed: 2018-11-01. 7\n\nA high-quality denoising dataset for smartphone cameras. Abdelrahman Abdelhamed, Stephen Lin, Michael S Brown, CVPR. Abdelrahman Abdelhamed, Stephen Lin, and Michael S Brown. A high-quality denoising dataset for smartphone cameras. In CVPR, pages 1692-1700, 2018. 5\n\nReal image denoising with feature attention. Saeed Anwar, Nick Barnes, ICCV. 1Saeed Anwar and Nick Barnes. Real image denoising with feature attention. In ICCV, pages 3155-3164, 2019. 1, 2\n\nNoise2self: Blind denoising by self-supervision. Joshua Batson, Loic Royer, International Conference on Machine Learning. Joshua Batson and Loic Royer. Noise2self: Blind denoising by self-supervision. In International Conference on Machine Learning, pages 524-533, 2019. 1, 2, 3, 4\n\nA non-local algorithm for image denoising. Antoni Buades, Bartomeu Coll, J-M Morel, CVPR. 2Antoni Buades, Bartomeu Coll, and J-M Morel. A non-local algorithm for image denoising. In CVPR, volume 2, pages 60-65, 2005. 1, 2\n\nSpatialadaptive network for single image denoising. Meng Chang, Qi Li, Huajun Feng, Zhihai Xu, ECCV. 1Meng Chang, Qi Li, Huajun Feng, and Zhihai Xu. Spatial- adaptive network for single image denoising. In ECCV, pages 171-187, 2020. 1, 2\n\nAn efficient statistical method for image noise level estimation. Guangyong Chen, Fengyuan Zhu, Pheng Ann Heng, ICCV. Guangyong Chen, Fengyuan Zhu, and Pheng Ann Heng. An efficient statistical method for image noise level estimation. In ICCV, pages 477-485, 2015. 6\n\nColor image denoising via sparse 3d collaborative filtering with grouping constraint in luminancechrominance space. Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik, Karen Egiazarian, ICIP. 17Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik, and Karen Egiazarian. Color image denoising via sparse 3d col- laborative filtering with grouping constraint in luminance- chrominance space. In ICIP, volume 1, pages I-313, 2007. 1, 2, 6, 7\n\nImage denoising by sparse 3-d transformdomain collaborative filtering. Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik, Karen Egiazarian, TIP. 168Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik, and Karen Egiazarian. Image denoising by sparse 3-d transform- domain collaborative filtering. TIP, 16(8):2080-2095, 2007. 1, 2, 5, 7, 8\n\nImagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, CVPR. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, pages 248-255, 2009. 5\n\nKodak lossless true color image suite. Rich Franzen, 1999. 54Rich Franzen. Kodak lossless true color image suite. source: http://r0k. us/graphics/kodak, 4(2), 1999. 5\n\nWeighted nuclear norm minimization with application to image denoising. Shuhang Gu, Lei Zhang, Wangmeng Zuo, Xiangchu Feng, CVPR. 1Shuhang Gu, Lei Zhang, Wangmeng Zuo, and Xiangchu Feng. Weighted nuclear norm minimization with applica- tion to image denoising. In CVPR, pages 2862-2869, 2014. 1, 2\n\nToward convolutional blind denoising of real photographs. Zifei Shi Guo, Kai Yan, Wangmeng Zhang, Lei Zuo, Zhang, CVPR. 1Shi Guo, Zifei Yan, Kai Zhang, Wangmeng Zuo, and Lei Zhang. Toward convolutional blind denoising of real pho- tographs. In CVPR, pages 1712-1722, 2019. 1, 2\n\nSingle image super-resolution from transformed self-exemplars. Jia-Bin Huang, Abhishek Singh, Narendra Ahuja, CVPR. Jia-Bin Huang, Abhishek Singh, and Narendra Ahuja. Single image super-resolution from transformed self-exemplars. In CVPR, pages 5197-5206, 2015. 5\n\nNeighbor2neighbor: Self-supervised denoising from single noisy images. Tao Huang, Songjiang Li, Xu Jia, Huchuan Lu, Jianzhuang Liu, CVPR. 7Tao Huang, Songjiang Li, Xu Jia, Huchuan Lu, and Jianzhuang Liu. Neighbor2neighbor: Self-supervised de- noising from single noisy images. In CVPR, pages 14781- 14790, 2021. 1, 2, 5, 6, 7, 8\n\nSimultaneous denoising and super-resolution of optical coherence tomography images based on generative adversarial network. Yongqiang Huang, Zexin Lu, Zhimin Shao, Maosong Ran, Jiliu Zhou, Leyuan Fang, Yi Zhang, Optics express. 279Yongqiang Huang, Zexin Lu, Zhimin Shao, Maosong Ran, Jiliu Zhou, Leyuan Fang, and Yi Zhang. Simultaneous de- noising and super-resolution of optical coherence tomogra- phy images based on generative adversarial network. Optics express, 27(9):12289-12307, 2019. 1\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, ICLR. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. 5\n\nNoise2void-learning denoising from single noisy images. Alexander Krull, Tim-Oliver Buchholz, Florian Jug, CVPR. 7Alexander Krull, Tim-Oliver Buchholz, and Florian Jug. Noise2void-learning denoising from single noisy images. In CVPR, pages 2129-2137, 2019. 1, 2, 3, 4, 5, 7, 8\n\nProbabilistic noise2void: Unsupervised content-aware denoising. Alexander Krull, Tom\u00e1\u0161 Vi\u010dar, Mangal Prakash, Manan Lalit, Florian Jug, Frontiers in Computer Science. 25Alexander Krull, Tom\u00e1\u0161 Vi\u010dar, Mangal Prakash, Manan Lalit, and Florian Jug. Probabilistic noise2void: Unsuper- vised content-aware denoising. Frontiers in Computer Sci- ence, 2:5, 2020. 1, 2\n\nHigh-quality self-supervised deep image denoising. Samuli Laine, Tero Karras, Jaakko Lehtinen, Timo Aila, NIPS. 328Samuli Laine, Tero Karras, Jaakko Lehtinen, and Timo Aila. High-quality self-supervised deep image denoising. NIPS, 32:6970-6980, 2019. 1, 2, 5, 6, 7, 8\n\nNoise2noise: Learning image restoration without clean data. Jaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli Laine, Tero Karras, Miika Aittala, Timo Aila, ICML. 7Jaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli Laine, Tero Karras, Miika Aittala, and Timo Aila. Noise2noise: Learning image restoration without clean data. In ICML, 2018. 1, 2, 5, 7, 8\n\nConnecting image denoising and high-level vision tasks via deep learning. Ding Liu, Bihan Wen, Jianbo Jiao, Xianming Liu, Zhangyang Wang, Thomas S Huang, TIP. 291Ding Liu, Bihan Wen, Jianbo Jiao, Xianming Liu, Zhangyang Wang, and Thomas S Huang. Connecting im- age denoising and high-level vision tasks via deep learning. TIP, 29:3695-3706, 2020. 1\n\nOptimal inversion of the anscombe transformation in low-count poisson image denoising. Markku Makitalo, Alessandro Foi, TIP. 2017Markku Makitalo and Alessandro Foi. Optimal inversion of the anscombe transformation in low-count poisson image de- noising. TIP, 20(1):99-109, 2010. 6, 7\n\nA database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. David Martin, Charless Fowlkes, Doron Tal, Jitendra Malik, ICCV. 2David Martin, Charless Fowlkes, Doron Tal, and Jitendra Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In ICCV, volume 2, pages 416-423, 2001. 5\n\nNoisier2noise: Learning to denoise from unpaired noisy data. Nick Moran, Dan Schmidt, Yu Zhong, Patrick Coady, CVPR. 7Nick Moran, Dan Schmidt, Yu Zhong, and Patrick Coady. Noisier2noise: Learning to denoise from unpaired noisy data. In CVPR, pages 12064-12072, 2020. 1, 2, 5, 7\n\nRecorrupted-to-recorrupted: Unsupervised deep learning for image denoising. Tongyao Pang, Huan Zheng, Yuhui Quan, Hui Ji, CVPR. 7Tongyao Pang, Huan Zheng, Yuhui Quan, and Hui Ji. Recorrupted-to-recorrupted: Unsupervised deep learning for image denoising. In CVPR, pages 2043-2052, 2021. 1, 2, 6, 7\n\nPytorch: An imperative style, high-performance deep learning library. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, NIPS. 325Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An im- perative style, high-performance deep learning library. NIPS, 32:8026-8037, 2019. 5\n\nSelf2self with dropout: Learning self-supervised denoising from single image. Yuhui Quan, Mingqin Chen, Tongyao Pang, Hui Ji, CVPR. 57Yuhui Quan, Mingqin Chen, Tongyao Pang, and Hui Ji. Self2self with dropout: Learning self-supervised denoising from single image. In CVPR, pages 1890-1898, 2020. 2, 5, 7\n\nUnet: Convolutional networks for biomedical image segmentation. Olaf Ronneberger, Philipp Fischer, Thomas Brox, International Conference on Medical image computing and computer-assisted intervention. 7Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U- net: Convolutional networks for biomedical image segmen- tation. In International Conference on Medical image com- puting and computer-assisted intervention, pages 234-241, 2015. 1, 2, 5, 7, 8\n\nFields of experts: A framework for learning image priors. Stefan Roth, J Michael, Black, CVPR. IEEEStefan Roth and Michael J Black. Fields of experts: A frame- work for learning image priors. In CVPR, pages 860-867. IEEE, 2005. 5\n\nSimultaneous denoising and moving object detection using low rank approximation. B Shijila, Anju Jose Tom, Sudhish N George, Future Generation Computer Systems. 901B Shijila, Anju Jose Tom, and Sudhish N George. Simul- taneous denoising and moving object detection using low rank approximation. Future Generation Computer Systems, 90:198-210, 2019. 1\n\nDeep image prior. Dmitry Ulyanov, Andrea Vedaldi, Victor Lempitsky, CVPR. Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Deep image prior. In CVPR, pages 9446-9454, 2018. 1\n\nUnpaired learning of deep image denoising. Xiaohe Wu, Ming Liu, Yue Cao, Dongwei Ren, Wangmeng Zuo, ECCV. 7Xiaohe Wu, Ming Liu, Yue Cao, Dongwei Ren, and Wang- meng Zuo. Unpaired learning of deep image denoising. In ECCV, pages 352-368, 2020. 1, 2, 6, 7\n\nNoisy-as-clean: learning selfsupervised denoising from corrupted image. Jun Xu, Yuan Huang, Ming-Ming Cheng, Li Liu, Fan Zhu, Zhou Xu, Ling Shao, TIP. 292Jun Xu, Yuan Huang, Ming-Ming Cheng, Li Liu, Fan Zhu, Zhou Xu, and Ling Shao. Noisy-as-clean: learning self- supervised denoising from corrupted image. TIP, 29:9316- 9329, 2020. 1, 2\n\nVariational denoising network: Toward blind noise modeling and removal. Zongsheng Yue, Hongwei Yong, Qian Zhao, Deyu Meng, Lei Zhang, NIPS. 322Zongsheng Yue, Hongwei Yong, Qian Zhao, Deyu Meng, and Lei Zhang. Variational denoising network: Toward blind noise modeling and removal. NIPS, 32:1690-1701, 2019. 1, 2\n\nLearning enriched features for real image restoration and enhancement. Aditya Syed Waqas Zamir, Salman Arora, Munawar Khan, Hayat, Ming-Hsuan Fahad Shahbaz Khan, Ling Yang, Shao, ECCV. 1Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, and Ling Shao. Learning enriched features for real image restoration and enhancement. In ECCV, pages 492-511, 2020. 1, 2\n\nOn single image scale-up using sparse-representations. Roman Zeyde, Michael Elad, Matan Protter, International conference on curves and surfaces. Roman Zeyde, Michael Elad, and Matan Protter. On sin- gle image scale-up using sparse-representations. In Interna- tional conference on curves and surfaces, pages 711-730, 2010. 5\n\nBeyond a gaussian denoiser: Residual learning of deep cnn for image denoising. Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, Lei Zhang, TIP. 267Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising. TIP, 26(7):3142-3155, 2017. 1, 2, 5\n\nFfdnet: Toward a fast and flexible solution for cnn-based image denoising. Kai Zhang, Wangmeng Zuo, Lei Zhang, TIP. 279Kai Zhang, Wangmeng Zuo, and Lei Zhang. Ffdnet: Toward a fast and flexible solution for cnn-based image denoising. TIP, 27(9):4608-4622, 2018. 1, 2\n\nA poisson-gaussian denoising dataset with real fluorescence microscopy images. Yide Zhang, Yinhao Zhu, Evan Nichols, Qingfei Wang, Siyuan Zhang, Cody Smith, Scott Howard, CVPR. Yide Zhang, Yinhao Zhu, Evan Nichols, Qingfei Wang, Siyuan Zhang, Cody Smith, and Scott Howard. A poisson-gaussian denoising dataset with real fluorescence microscopy images. In CVPR, 2019. 5\n", "annotations": {"author": "[{\"end\":261,\"start\":75},{\"end\":444,\"start\":262},{\"end\":553,\"start\":445},{\"end\":731,\"start\":554}]", "publisher": null, "author_last_name": "[{\"end\":85,\"start\":81},{\"end\":274,\"start\":271},{\"end\":455,\"start\":453},{\"end\":561,\"start\":558}]", "author_first_name": "[{\"end\":80,\"start\":75},{\"end\":270,\"start\":262},{\"end\":452,\"start\":445},{\"end\":557,\"start\":554}]", "author_affiliation": "[{\"end\":182,\"start\":87},{\"end\":260,\"start\":184},{\"end\":371,\"start\":276},{\"end\":443,\"start\":373},{\"end\":552,\"start\":457},{\"end\":658,\"start\":563},{\"end\":730,\"start\":660}]", "title": "[{\"end\":72,\"start\":1},{\"end\":803,\"start\":732}]", "venue": null, "abstract": "[{\"end\":2225,\"start\":805}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2491,\"start\":2487},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2519,\"start\":2515},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2546,\"start\":2542},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2806,\"start\":2803},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2808,\"start\":2806},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2811,\"start\":2808},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2814,\"start\":2811},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":2817,\"start\":2814},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2820,\"start\":2817},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":2823,\"start\":2820},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":2826,\"start\":2823},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2896,\"start\":2893},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2898,\"start\":2896},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2900,\"start\":2898},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2903,\"start\":2900},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2952,\"start\":2948},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":2964,\"start\":2960},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":2977,\"start\":2973},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2989,\"start\":2986},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3000,\"start\":2997},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3184,\"start\":3180},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3481,\"start\":3478},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3484,\"start\":3481},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3487,\"start\":3484},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3490,\"start\":3487},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3493,\"start\":3490},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3496,\"start\":3493},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3499,\"start\":3496},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3603,\"start\":3599},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3749,\"start\":3746},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3766,\"start\":3762},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4057,\"start\":4053},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":4316,\"start\":4312},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4505,\"start\":4501},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4508,\"start\":4505},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":4511,\"start\":4508},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4665,\"start\":4661},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":4668,\"start\":4665},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4892,\"start\":4888},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5052,\"start\":5048},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7371,\"start\":7368},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7382,\"start\":7379},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7393,\"start\":7389},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7406,\"start\":7403},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":7767,\"start\":7763},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7915,\"start\":7911},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8012,\"start\":8009},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8014,\"start\":8012},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8017,\"start\":8014},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8020,\"start\":8017},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8023,\"start\":8020},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8026,\"start\":8023},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8378,\"start\":8374},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8485,\"start\":8482},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8505,\"start\":8501},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8661,\"start\":8657},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8672,\"start\":8668},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8707,\"start\":8703},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8936,\"start\":8932},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":9081,\"start\":9077},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9105,\"start\":9101},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9241,\"start\":9237},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9361,\"start\":9357},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9648,\"start\":9645},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9651,\"start\":9648},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9654,\"start\":9651},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9682,\"start\":9678},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":9685,\"start\":9682},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10518,\"start\":10514},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11366,\"start\":11363},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11369,\"start\":11366},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":14664,\"start\":14661},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":17436,\"start\":17433},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":17439,\"start\":17436},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":20010,\"start\":20006},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":20031,\"start\":20027},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":20034,\"start\":20031},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":20073,\"start\":20069},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":20658,\"start\":20654},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":20733,\"start\":20729},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":20827,\"start\":20823},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":20830,\"start\":20827},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":20930,\"start\":20926},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":21040,\"start\":21036},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":21053,\"start\":21049},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":21068,\"start\":21064},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":21119,\"start\":21115},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":21204,\"start\":21200},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":21222,\"start\":21218},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":21288,\"start\":21284},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":21310,\"start\":21307},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":21652,\"start\":21648},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":22092,\"start\":22088},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":22166,\"start\":22162},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":22179,\"start\":22175},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":22262,\"start\":22259},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":22326,\"start\":22322},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":22349,\"start\":22345},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":22363,\"start\":22359},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":22383,\"start\":22379},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":22397,\"start\":22393},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":22407,\"start\":22403},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":22424,\"start\":22420},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":22494,\"start\":22490},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":22569,\"start\":22565},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":22572,\"start\":22569},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":22603,\"start\":22600},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":22714,\"start\":22711},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":22748,\"start\":22744},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":25472,\"start\":25469},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":27898,\"start\":27894}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":31722,\"start\":31719},{\"attributes\":{\"id\":\"fig_1\"},\"end\":32150,\"start\":31723},{\"attributes\":{\"id\":\"fig_2\"},\"end\":32520,\"start\":32151},{\"attributes\":{\"id\":\"fig_3\"},\"end\":32602,\"start\":32521},{\"attributes\":{\"id\":\"fig_4\"},\"end\":32684,\"start\":32603},{\"attributes\":{\"id\":\"fig_5\"},\"end\":32960,\"start\":32685},{\"attributes\":{\"id\":\"fig_6\"},\"end\":33163,\"start\":32961},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":33299,\"start\":33164},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":34582,\"start\":33300}]", "paragraph": "[{\"end\":2711,\"start\":2241},{\"end\":3414,\"start\":2738},{\"end\":5253,\"start\":3416},{\"end\":6822,\"start\":5255},{\"end\":7276,\"start\":6824},{\"end\":7641,\"start\":7324},{\"end\":8326,\"start\":7672},{\"end\":9547,\"start\":8362},{\"end\":10480,\"start\":9586},{\"end\":10892,\"start\":10503},{\"end\":11301,\"start\":10940},{\"end\":11777,\"start\":11341},{\"end\":12389,\"start\":11845},{\"end\":12929,\"start\":12453},{\"end\":13264,\"start\":12996},{\"end\":13421,\"start\":13418},{\"end\":13569,\"start\":13509},{\"end\":13693,\"start\":13650},{\"end\":14345,\"start\":13753},{\"end\":14505,\"start\":14408},{\"end\":15215,\"start\":14550},{\"end\":15820,\"start\":15387},{\"end\":16408,\"start\":15855},{\"end\":16592,\"start\":16501},{\"end\":17381,\"start\":16698},{\"end\":18094,\"start\":17410},{\"end\":18705,\"start\":18096},{\"end\":19284,\"start\":18737},{\"end\":19907,\"start\":19387},{\"end\":20762,\"start\":19957},{\"end\":22426,\"start\":20764},{\"end\":22946,\"start\":22428},{\"end\":23550,\"start\":22948},{\"end\":25287,\"start\":23586},{\"end\":26101,\"start\":25321},{\"end\":27324,\"start\":26103},{\"end\":29183,\"start\":27371},{\"end\":29718,\"start\":29198},{\"end\":29827,\"start\":29762},{\"end\":31148,\"start\":29858},{\"end\":31687,\"start\":31187}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10939,\"start\":10893},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11844,\"start\":11778},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12452,\"start\":12390},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12995,\"start\":12930},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13417,\"start\":13265},{\"attributes\":{\"id\":\"formula_5\"},\"end\":13508,\"start\":13422},{\"attributes\":{\"id\":\"formula_6\"},\"end\":13649,\"start\":13570},{\"attributes\":{\"id\":\"formula_7\"},\"end\":13752,\"start\":13694},{\"attributes\":{\"id\":\"formula_8\"},\"end\":14407,\"start\":14346},{\"attributes\":{\"id\":\"formula_9\"},\"end\":14549,\"start\":14506},{\"attributes\":{\"id\":\"formula_10\"},\"end\":15386,\"start\":15216},{\"attributes\":{\"id\":\"formula_11\"},\"end\":16500,\"start\":16409},{\"attributes\":{\"id\":\"formula_12\"},\"end\":16683,\"start\":16593},{\"attributes\":{\"id\":\"formula_13\"},\"end\":19386,\"start\":19285}]", "table_ref": "[{\"end\":23698,\"start\":23691},{\"end\":25022,\"start\":25015},{\"end\":25346,\"start\":25339},{\"end\":26009,\"start\":26002},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":26839,\"start\":26832},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":27566,\"start\":27559},{\"end\":27960,\"start\":27953},{\"end\":28375,\"start\":28368},{\"end\":28886,\"start\":28879}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2239,\"start\":2227},{\"end\":2736,\"start\":2714},{\"attributes\":{\"n\":\"2.\"},\"end\":7291,\"start\":7279},{\"attributes\":{\"n\":\"2.1.\"},\"end\":7322,\"start\":7294},{\"attributes\":{\"n\":\"2.2.\"},\"end\":7670,\"start\":7644},{\"attributes\":{\"n\":\"2.3.\"},\"end\":8360,\"start\":8329},{\"attributes\":{\"n\":\"3.\"},\"end\":9571,\"start\":9550},{\"attributes\":{\"n\":\"3.1.\"},\"end\":9584,\"start\":9574},{\"attributes\":{\"n\":\"3.2.\"},\"end\":10501,\"start\":10483},{\"attributes\":{\"n\":\"3.3.\"},\"end\":11339,\"start\":11304},{\"attributes\":{\"n\":\"3.4.\"},\"end\":15853,\"start\":15823},{\"attributes\":{\"n\":\"4.\"},\"end\":16696,\"start\":16685},{\"attributes\":{\"n\":\"4.1.\"},\"end\":17408,\"start\":17384},{\"attributes\":{\"n\":\"4.2.\"},\"end\":18735,\"start\":18708},{\"attributes\":{\"n\":\"5.\"},\"end\":19930,\"start\":19910},{\"attributes\":{\"n\":\"5.1.\"},\"end\":19955,\"start\":19933},{\"attributes\":{\"n\":\"5.2.\"},\"end\":23584,\"start\":23553},{\"attributes\":{\"n\":\"5.3.\"},\"end\":25319,\"start\":25290},{\"attributes\":{\"n\":\"5.4.\"},\"end\":27352,\"start\":27327},{\"attributes\":{\"n\":\"5.5.\"},\"end\":27369,\"start\":27355},{\"attributes\":{\"n\":\"6.\"},\"end\":29196,\"start\":29186},{\"end\":29760,\"start\":29721},{\"end\":29856,\"start\":29830},{\"end\":31185,\"start\":31151},{\"end\":31718,\"start\":31690},{\"end\":31734,\"start\":31724},{\"end\":32532,\"start\":32522},{\"end\":32614,\"start\":32604},{\"end\":32715,\"start\":32686},{\"end\":32972,\"start\":32962},{\"end\":33174,\"start\":33165},{\"end\":33310,\"start\":33301}]", "table": "[{\"end\":34582,\"start\":33869}]", "figure_caption": "[{\"end\":31722,\"start\":31721},{\"end\":32150,\"start\":31736},{\"end\":32520,\"start\":32153},{\"end\":32602,\"start\":32534},{\"end\":32684,\"start\":32616},{\"end\":32960,\"start\":32719},{\"end\":33163,\"start\":32974},{\"end\":33299,\"start\":33176},{\"end\":33869,\"start\":33312}]", "figure_ref": "[{\"end\":14958,\"start\":14950},{\"end\":17380,\"start\":17372},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":17963,\"start\":17955},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":24844,\"start\":24829},{\"end\":25840,\"start\":25832},{\"end\":27272,\"start\":27264},{\"end\":30701,\"start\":30693},{\"end\":31256,\"start\":31248},{\"end\":31605,\"start\":31596}]", "bib_author_first_name": "[{\"end\":34662,\"start\":34651},{\"end\":34872,\"start\":34861},{\"end\":34892,\"start\":34885},{\"end\":34907,\"start\":34898},{\"end\":35121,\"start\":35116},{\"end\":35133,\"start\":35129},{\"end\":35316,\"start\":35310},{\"end\":35329,\"start\":35325},{\"end\":35593,\"start\":35587},{\"end\":35610,\"start\":35602},{\"end\":35620,\"start\":35617},{\"end\":35823,\"start\":35819},{\"end\":35833,\"start\":35831},{\"end\":35844,\"start\":35838},{\"end\":35857,\"start\":35851},{\"end\":36081,\"start\":36072},{\"end\":36096,\"start\":36088},{\"end\":36111,\"start\":36102},{\"end\":36397,\"start\":36389},{\"end\":36415,\"start\":36405},{\"end\":36429,\"start\":36421},{\"end\":36446,\"start\":36441},{\"end\":36790,\"start\":36782},{\"end\":36808,\"start\":36798},{\"end\":36822,\"start\":36814},{\"end\":36839,\"start\":36834},{\"end\":37106,\"start\":37103},{\"end\":37116,\"start\":37113},{\"end\":37130,\"start\":37123},{\"end\":37145,\"start\":37139},{\"end\":37153,\"start\":37150},{\"end\":37160,\"start\":37158},{\"end\":37376,\"start\":37372},{\"end\":37580,\"start\":37573},{\"end\":37588,\"start\":37585},{\"end\":37604,\"start\":37596},{\"end\":37618,\"start\":37610},{\"end\":37863,\"start\":37858},{\"end\":37876,\"start\":37873},{\"end\":37890,\"start\":37882},{\"end\":37901,\"start\":37898},{\"end\":38149,\"start\":38142},{\"end\":38165,\"start\":38157},{\"end\":38181,\"start\":38173},{\"end\":38418,\"start\":38415},{\"end\":38435,\"start\":38426},{\"end\":38442,\"start\":38440},{\"end\":38455,\"start\":38448},{\"end\":38470,\"start\":38460},{\"end\":38807,\"start\":38798},{\"end\":38820,\"start\":38815},{\"end\":38831,\"start\":38825},{\"end\":38845,\"start\":38838},{\"end\":38856,\"start\":38851},{\"end\":38869,\"start\":38863},{\"end\":38878,\"start\":38876},{\"end\":39214,\"start\":39213},{\"end\":39230,\"start\":39225},{\"end\":39408,\"start\":39399},{\"end\":39426,\"start\":39416},{\"end\":39444,\"start\":39437},{\"end\":39694,\"start\":39685},{\"end\":39707,\"start\":39702},{\"end\":39721,\"start\":39715},{\"end\":39736,\"start\":39731},{\"end\":39751,\"start\":39744},{\"end\":40039,\"start\":40033},{\"end\":40051,\"start\":40047},{\"end\":40066,\"start\":40060},{\"end\":40081,\"start\":40077},{\"end\":40317,\"start\":40311},{\"end\":40333,\"start\":40328},{\"end\":40347,\"start\":40344},{\"end\":40366,\"start\":40360},{\"end\":40378,\"start\":40374},{\"end\":40392,\"start\":40387},{\"end\":40406,\"start\":40402},{\"end\":40694,\"start\":40690},{\"end\":40705,\"start\":40700},{\"end\":40717,\"start\":40711},{\"end\":40732,\"start\":40724},{\"end\":40747,\"start\":40738},{\"end\":40762,\"start\":40754},{\"end\":41059,\"start\":41053},{\"end\":41080,\"start\":41070},{\"end\":41396,\"start\":41391},{\"end\":41413,\"start\":41405},{\"end\":41428,\"start\":41423},{\"end\":41442,\"start\":41434},{\"end\":41768,\"start\":41764},{\"end\":41779,\"start\":41776},{\"end\":41791,\"start\":41789},{\"end\":41806,\"start\":41799},{\"end\":42065,\"start\":42058},{\"end\":42076,\"start\":42072},{\"end\":42089,\"start\":42084},{\"end\":42099,\"start\":42096},{\"end\":42355,\"start\":42351},{\"end\":42367,\"start\":42364},{\"end\":42384,\"start\":42375},{\"end\":42396,\"start\":42392},{\"end\":42409,\"start\":42404},{\"end\":42427,\"start\":42420},{\"end\":42442,\"start\":42436},{\"end\":42458,\"start\":42452},{\"end\":42471,\"start\":42464},{\"end\":42488,\"start\":42484},{\"end\":42843,\"start\":42838},{\"end\":42857,\"start\":42850},{\"end\":42871,\"start\":42864},{\"end\":42881,\"start\":42878},{\"end\":43133,\"start\":43129},{\"end\":43154,\"start\":43147},{\"end\":43170,\"start\":43164},{\"end\":43579,\"start\":43573},{\"end\":43587,\"start\":43586},{\"end\":43828,\"start\":43827},{\"end\":43847,\"start\":43838},{\"end\":43862,\"start\":43853},{\"end\":44122,\"start\":44116},{\"end\":44138,\"start\":44132},{\"end\":44154,\"start\":44148},{\"end\":44328,\"start\":44322},{\"end\":44337,\"start\":44333},{\"end\":44346,\"start\":44343},{\"end\":44359,\"start\":44352},{\"end\":44373,\"start\":44365},{\"end\":44609,\"start\":44606},{\"end\":44618,\"start\":44614},{\"end\":44635,\"start\":44626},{\"end\":44645,\"start\":44643},{\"end\":44654,\"start\":44651},{\"end\":44664,\"start\":44660},{\"end\":44673,\"start\":44669},{\"end\":44953,\"start\":44944},{\"end\":44966,\"start\":44959},{\"end\":44977,\"start\":44973},{\"end\":44988,\"start\":44984},{\"end\":44998,\"start\":44995},{\"end\":45262,\"start\":45256},{\"end\":45287,\"start\":45281},{\"end\":45302,\"start\":45295},{\"end\":45326,\"start\":45316},{\"end\":45351,\"start\":45347},{\"end\":45650,\"start\":45645},{\"end\":45665,\"start\":45658},{\"end\":45677,\"start\":45672},{\"end\":45999,\"start\":45996},{\"end\":46015,\"start\":46007},{\"end\":46027,\"start\":46021},{\"end\":46038,\"start\":46034},{\"end\":46048,\"start\":46045},{\"end\":46322,\"start\":46319},{\"end\":46338,\"start\":46330},{\"end\":46347,\"start\":46344},{\"end\":46595,\"start\":46591},{\"end\":46609,\"start\":46603},{\"end\":46619,\"start\":46615},{\"end\":46636,\"start\":46629},{\"end\":46649,\"start\":46643},{\"end\":46661,\"start\":46657},{\"end\":46674,\"start\":46669}]", "bib_author_last_name": "[{\"end\":34673,\"start\":34663},{\"end\":34682,\"start\":34675},{\"end\":34883,\"start\":34873},{\"end\":34896,\"start\":34893},{\"end\":34913,\"start\":34908},{\"end\":35127,\"start\":35122},{\"end\":35140,\"start\":35134},{\"end\":35323,\"start\":35317},{\"end\":35335,\"start\":35330},{\"end\":35600,\"start\":35594},{\"end\":35615,\"start\":35611},{\"end\":35626,\"start\":35621},{\"end\":35829,\"start\":35824},{\"end\":35836,\"start\":35834},{\"end\":35849,\"start\":35845},{\"end\":35860,\"start\":35858},{\"end\":36086,\"start\":36082},{\"end\":36100,\"start\":36097},{\"end\":36116,\"start\":36112},{\"end\":36403,\"start\":36398},{\"end\":36419,\"start\":36416},{\"end\":36439,\"start\":36430},{\"end\":36457,\"start\":36447},{\"end\":36796,\"start\":36791},{\"end\":36812,\"start\":36809},{\"end\":36832,\"start\":36823},{\"end\":36850,\"start\":36840},{\"end\":37111,\"start\":37107},{\"end\":37121,\"start\":37117},{\"end\":37137,\"start\":37131},{\"end\":37148,\"start\":37146},{\"end\":37156,\"start\":37154},{\"end\":37168,\"start\":37161},{\"end\":37384,\"start\":37377},{\"end\":37583,\"start\":37581},{\"end\":37594,\"start\":37589},{\"end\":37608,\"start\":37605},{\"end\":37623,\"start\":37619},{\"end\":37871,\"start\":37864},{\"end\":37880,\"start\":37877},{\"end\":37896,\"start\":37891},{\"end\":37905,\"start\":37902},{\"end\":37912,\"start\":37907},{\"end\":38155,\"start\":38150},{\"end\":38171,\"start\":38166},{\"end\":38187,\"start\":38182},{\"end\":38424,\"start\":38419},{\"end\":38438,\"start\":38436},{\"end\":38446,\"start\":38443},{\"end\":38458,\"start\":38456},{\"end\":38474,\"start\":38471},{\"end\":38813,\"start\":38808},{\"end\":38823,\"start\":38821},{\"end\":38836,\"start\":38832},{\"end\":38849,\"start\":38846},{\"end\":38861,\"start\":38857},{\"end\":38874,\"start\":38870},{\"end\":38884,\"start\":38879},{\"end\":39223,\"start\":39215},{\"end\":39237,\"start\":39231},{\"end\":39241,\"start\":39239},{\"end\":39414,\"start\":39409},{\"end\":39435,\"start\":39427},{\"end\":39448,\"start\":39445},{\"end\":39700,\"start\":39695},{\"end\":39713,\"start\":39708},{\"end\":39729,\"start\":39722},{\"end\":39742,\"start\":39737},{\"end\":39755,\"start\":39752},{\"end\":40045,\"start\":40040},{\"end\":40058,\"start\":40052},{\"end\":40075,\"start\":40067},{\"end\":40086,\"start\":40082},{\"end\":40326,\"start\":40318},{\"end\":40342,\"start\":40334},{\"end\":40358,\"start\":40348},{\"end\":40372,\"start\":40367},{\"end\":40385,\"start\":40379},{\"end\":40400,\"start\":40393},{\"end\":40411,\"start\":40407},{\"end\":40698,\"start\":40695},{\"end\":40709,\"start\":40706},{\"end\":40722,\"start\":40718},{\"end\":40736,\"start\":40733},{\"end\":40752,\"start\":40748},{\"end\":40768,\"start\":40763},{\"end\":41068,\"start\":41060},{\"end\":41084,\"start\":41081},{\"end\":41403,\"start\":41397},{\"end\":41421,\"start\":41414},{\"end\":41432,\"start\":41429},{\"end\":41448,\"start\":41443},{\"end\":41774,\"start\":41769},{\"end\":41787,\"start\":41780},{\"end\":41797,\"start\":41792},{\"end\":41812,\"start\":41807},{\"end\":42070,\"start\":42066},{\"end\":42082,\"start\":42077},{\"end\":42094,\"start\":42090},{\"end\":42102,\"start\":42100},{\"end\":42362,\"start\":42356},{\"end\":42373,\"start\":42368},{\"end\":42390,\"start\":42385},{\"end\":42402,\"start\":42397},{\"end\":42418,\"start\":42410},{\"end\":42434,\"start\":42428},{\"end\":42450,\"start\":42443},{\"end\":42462,\"start\":42459},{\"end\":42482,\"start\":42472},{\"end\":42495,\"start\":42489},{\"end\":42848,\"start\":42844},{\"end\":42862,\"start\":42858},{\"end\":42876,\"start\":42872},{\"end\":42884,\"start\":42882},{\"end\":43145,\"start\":43134},{\"end\":43162,\"start\":43155},{\"end\":43175,\"start\":43171},{\"end\":43584,\"start\":43580},{\"end\":43595,\"start\":43588},{\"end\":43602,\"start\":43597},{\"end\":43836,\"start\":43829},{\"end\":43851,\"start\":43848},{\"end\":43869,\"start\":43863},{\"end\":44130,\"start\":44123},{\"end\":44146,\"start\":44139},{\"end\":44164,\"start\":44155},{\"end\":44331,\"start\":44329},{\"end\":44341,\"start\":44338},{\"end\":44350,\"start\":44347},{\"end\":44363,\"start\":44360},{\"end\":44377,\"start\":44374},{\"end\":44612,\"start\":44610},{\"end\":44624,\"start\":44619},{\"end\":44641,\"start\":44636},{\"end\":44649,\"start\":44646},{\"end\":44658,\"start\":44655},{\"end\":44667,\"start\":44665},{\"end\":44678,\"start\":44674},{\"end\":44957,\"start\":44954},{\"end\":44971,\"start\":44967},{\"end\":44982,\"start\":44978},{\"end\":44993,\"start\":44989},{\"end\":45004,\"start\":44999},{\"end\":45279,\"start\":45263},{\"end\":45293,\"start\":45288},{\"end\":45307,\"start\":45303},{\"end\":45314,\"start\":45309},{\"end\":45345,\"start\":45327},{\"end\":45356,\"start\":45352},{\"end\":45362,\"start\":45358},{\"end\":45656,\"start\":45651},{\"end\":45670,\"start\":45666},{\"end\":45685,\"start\":45678},{\"end\":46005,\"start\":46000},{\"end\":46019,\"start\":46016},{\"end\":46032,\"start\":46028},{\"end\":46043,\"start\":46039},{\"end\":46054,\"start\":46049},{\"end\":46328,\"start\":46323},{\"end\":46342,\"start\":46339},{\"end\":46353,\"start\":46348},{\"end\":46601,\"start\":46596},{\"end\":46613,\"start\":46610},{\"end\":46627,\"start\":46620},{\"end\":46641,\"start\":46637},{\"end\":46655,\"start\":46650},{\"end\":46667,\"start\":46662},{\"end\":46681,\"start\":46675}]", "bib_entry": "[{\"attributes\":{\"doi\":\"11-01. 7\",\"id\":\"b0\"},\"end\":34802,\"start\":34649},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":52059988},\"end\":35069,\"start\":34804},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":118713138},\"end\":35259,\"start\":35071},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":59523708},\"end\":35542,\"start\":35261},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":11206708},\"end\":35765,\"start\":35544},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":210932492},\"end\":36004,\"start\":35767},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":206770723},\"end\":36271,\"start\":36006},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":1887989},\"end\":36709,\"start\":36273},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":1475121},\"end\":37048,\"start\":36711},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":57246310},\"end\":37331,\"start\":37050},{\"attributes\":{\"doi\":\"1999. 5\",\"id\":\"b10\"},\"end\":37499,\"start\":37333},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":1663191},\"end\":37798,\"start\":37501},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":49672261},\"end\":38077,\"start\":37800},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":8282555},\"end\":38342,\"start\":38079},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":231419143},\"end\":38672,\"start\":38344},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":145024258},\"end\":39167,\"start\":38674},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":6628106},\"end\":39341,\"start\":39169},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":53751136},\"end\":39619,\"start\":39343},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":173990717},\"end\":39980,\"start\":39621},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":173990648},\"end\":40249,\"start\":39982},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":3846544},\"end\":40614,\"start\":40251},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":52168355},\"end\":40964,\"start\":40616},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":10229455},\"end\":41249,\"start\":40966},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":64193},\"end\":41701,\"start\":41251},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":204904999},\"end\":41980,\"start\":41703},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":235719899},\"end\":42279,\"start\":41982},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":202786778},\"end\":42758,\"start\":42281},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":219619080},\"end\":43063,\"start\":42760},{\"attributes\":{\"id\":\"b28\"},\"end\":43513,\"start\":43065},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":2843211},\"end\":43744,\"start\":43515},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":52903210},\"end\":44096,\"start\":43746},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":4531078},\"end\":44277,\"start\":44098},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":221376798},\"end\":44532,\"start\":44279},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":222154570},\"end\":44870,\"start\":44534},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":201667906},\"end\":45183,\"start\":44872},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":212725053},\"end\":45588,\"start\":45185},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":2356330},\"end\":45915,\"start\":45590},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":996788},\"end\":46242,\"start\":45917},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":10514149},\"end\":46510,\"start\":46244},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":56895444},\"end\":46880,\"start\":46512}]", "bib_title": "[{\"end\":34859,\"start\":34804},{\"end\":35114,\"start\":35071},{\"end\":35308,\"start\":35261},{\"end\":35585,\"start\":35544},{\"end\":35817,\"start\":35767},{\"end\":36070,\"start\":36006},{\"end\":36387,\"start\":36273},{\"end\":36780,\"start\":36711},{\"end\":37101,\"start\":37050},{\"end\":37571,\"start\":37501},{\"end\":37856,\"start\":37800},{\"end\":38140,\"start\":38079},{\"end\":38413,\"start\":38344},{\"end\":38796,\"start\":38674},{\"end\":39211,\"start\":39169},{\"end\":39397,\"start\":39343},{\"end\":39683,\"start\":39621},{\"end\":40031,\"start\":39982},{\"end\":40309,\"start\":40251},{\"end\":40688,\"start\":40616},{\"end\":41051,\"start\":40966},{\"end\":41389,\"start\":41251},{\"end\":41762,\"start\":41703},{\"end\":42056,\"start\":41982},{\"end\":42349,\"start\":42281},{\"end\":42836,\"start\":42760},{\"end\":43127,\"start\":43065},{\"end\":43571,\"start\":43515},{\"end\":43825,\"start\":43746},{\"end\":44114,\"start\":44098},{\"end\":44320,\"start\":44279},{\"end\":44604,\"start\":44534},{\"end\":44942,\"start\":44872},{\"end\":45254,\"start\":45185},{\"end\":45643,\"start\":45590},{\"end\":45994,\"start\":45917},{\"end\":46317,\"start\":46244},{\"end\":46589,\"start\":46512}]", "bib_author": "[{\"end\":34675,\"start\":34651},{\"end\":34684,\"start\":34675},{\"end\":34885,\"start\":34861},{\"end\":34898,\"start\":34885},{\"end\":34915,\"start\":34898},{\"end\":35129,\"start\":35116},{\"end\":35142,\"start\":35129},{\"end\":35325,\"start\":35310},{\"end\":35337,\"start\":35325},{\"end\":35602,\"start\":35587},{\"end\":35617,\"start\":35602},{\"end\":35628,\"start\":35617},{\"end\":35831,\"start\":35819},{\"end\":35838,\"start\":35831},{\"end\":35851,\"start\":35838},{\"end\":35862,\"start\":35851},{\"end\":36088,\"start\":36072},{\"end\":36102,\"start\":36088},{\"end\":36118,\"start\":36102},{\"end\":36405,\"start\":36389},{\"end\":36421,\"start\":36405},{\"end\":36441,\"start\":36421},{\"end\":36459,\"start\":36441},{\"end\":36798,\"start\":36782},{\"end\":36814,\"start\":36798},{\"end\":36834,\"start\":36814},{\"end\":36852,\"start\":36834},{\"end\":37113,\"start\":37103},{\"end\":37123,\"start\":37113},{\"end\":37139,\"start\":37123},{\"end\":37150,\"start\":37139},{\"end\":37158,\"start\":37150},{\"end\":37170,\"start\":37158},{\"end\":37386,\"start\":37372},{\"end\":37585,\"start\":37573},{\"end\":37596,\"start\":37585},{\"end\":37610,\"start\":37596},{\"end\":37625,\"start\":37610},{\"end\":37873,\"start\":37858},{\"end\":37882,\"start\":37873},{\"end\":37898,\"start\":37882},{\"end\":37907,\"start\":37898},{\"end\":37914,\"start\":37907},{\"end\":38157,\"start\":38142},{\"end\":38173,\"start\":38157},{\"end\":38189,\"start\":38173},{\"end\":38426,\"start\":38415},{\"end\":38440,\"start\":38426},{\"end\":38448,\"start\":38440},{\"end\":38460,\"start\":38448},{\"end\":38476,\"start\":38460},{\"end\":38815,\"start\":38798},{\"end\":38825,\"start\":38815},{\"end\":38838,\"start\":38825},{\"end\":38851,\"start\":38838},{\"end\":38863,\"start\":38851},{\"end\":38876,\"start\":38863},{\"end\":38886,\"start\":38876},{\"end\":39225,\"start\":39213},{\"end\":39239,\"start\":39225},{\"end\":39243,\"start\":39239},{\"end\":39416,\"start\":39399},{\"end\":39437,\"start\":39416},{\"end\":39450,\"start\":39437},{\"end\":39702,\"start\":39685},{\"end\":39715,\"start\":39702},{\"end\":39731,\"start\":39715},{\"end\":39744,\"start\":39731},{\"end\":39757,\"start\":39744},{\"end\":40047,\"start\":40033},{\"end\":40060,\"start\":40047},{\"end\":40077,\"start\":40060},{\"end\":40088,\"start\":40077},{\"end\":40328,\"start\":40311},{\"end\":40344,\"start\":40328},{\"end\":40360,\"start\":40344},{\"end\":40374,\"start\":40360},{\"end\":40387,\"start\":40374},{\"end\":40402,\"start\":40387},{\"end\":40413,\"start\":40402},{\"end\":40700,\"start\":40690},{\"end\":40711,\"start\":40700},{\"end\":40724,\"start\":40711},{\"end\":40738,\"start\":40724},{\"end\":40754,\"start\":40738},{\"end\":40770,\"start\":40754},{\"end\":41070,\"start\":41053},{\"end\":41086,\"start\":41070},{\"end\":41405,\"start\":41391},{\"end\":41423,\"start\":41405},{\"end\":41434,\"start\":41423},{\"end\":41450,\"start\":41434},{\"end\":41776,\"start\":41764},{\"end\":41789,\"start\":41776},{\"end\":41799,\"start\":41789},{\"end\":41814,\"start\":41799},{\"end\":42072,\"start\":42058},{\"end\":42084,\"start\":42072},{\"end\":42096,\"start\":42084},{\"end\":42104,\"start\":42096},{\"end\":42364,\"start\":42351},{\"end\":42375,\"start\":42364},{\"end\":42392,\"start\":42375},{\"end\":42404,\"start\":42392},{\"end\":42420,\"start\":42404},{\"end\":42436,\"start\":42420},{\"end\":42452,\"start\":42436},{\"end\":42464,\"start\":42452},{\"end\":42484,\"start\":42464},{\"end\":42497,\"start\":42484},{\"end\":42850,\"start\":42838},{\"end\":42864,\"start\":42850},{\"end\":42878,\"start\":42864},{\"end\":42886,\"start\":42878},{\"end\":43147,\"start\":43129},{\"end\":43164,\"start\":43147},{\"end\":43177,\"start\":43164},{\"end\":43586,\"start\":43573},{\"end\":43597,\"start\":43586},{\"end\":43604,\"start\":43597},{\"end\":43838,\"start\":43827},{\"end\":43853,\"start\":43838},{\"end\":43871,\"start\":43853},{\"end\":44132,\"start\":44116},{\"end\":44148,\"start\":44132},{\"end\":44166,\"start\":44148},{\"end\":44333,\"start\":44322},{\"end\":44343,\"start\":44333},{\"end\":44352,\"start\":44343},{\"end\":44365,\"start\":44352},{\"end\":44379,\"start\":44365},{\"end\":44614,\"start\":44606},{\"end\":44626,\"start\":44614},{\"end\":44643,\"start\":44626},{\"end\":44651,\"start\":44643},{\"end\":44660,\"start\":44651},{\"end\":44669,\"start\":44660},{\"end\":44680,\"start\":44669},{\"end\":44959,\"start\":44944},{\"end\":44973,\"start\":44959},{\"end\":44984,\"start\":44973},{\"end\":44995,\"start\":44984},{\"end\":45006,\"start\":44995},{\"end\":45281,\"start\":45256},{\"end\":45295,\"start\":45281},{\"end\":45309,\"start\":45295},{\"end\":45316,\"start\":45309},{\"end\":45347,\"start\":45316},{\"end\":45358,\"start\":45347},{\"end\":45364,\"start\":45358},{\"end\":45658,\"start\":45645},{\"end\":45672,\"start\":45658},{\"end\":45687,\"start\":45672},{\"end\":46007,\"start\":45996},{\"end\":46021,\"start\":46007},{\"end\":46034,\"start\":46021},{\"end\":46045,\"start\":46034},{\"end\":46056,\"start\":46045},{\"end\":46330,\"start\":46319},{\"end\":46344,\"start\":46330},{\"end\":46355,\"start\":46344},{\"end\":46603,\"start\":46591},{\"end\":46615,\"start\":46603},{\"end\":46629,\"start\":46615},{\"end\":46643,\"start\":46629},{\"end\":46657,\"start\":46643},{\"end\":46669,\"start\":46657},{\"end\":46683,\"start\":46669}]", "bib_venue": "[{\"end\":34919,\"start\":34915},{\"end\":35146,\"start\":35142},{\"end\":35381,\"start\":35337},{\"end\":35632,\"start\":35628},{\"end\":35866,\"start\":35862},{\"end\":36122,\"start\":36118},{\"end\":36463,\"start\":36459},{\"end\":36855,\"start\":36852},{\"end\":37174,\"start\":37170},{\"end\":37370,\"start\":37333},{\"end\":37629,\"start\":37625},{\"end\":37918,\"start\":37914},{\"end\":38193,\"start\":38189},{\"end\":38480,\"start\":38476},{\"end\":38900,\"start\":38886},{\"end\":39247,\"start\":39243},{\"end\":39454,\"start\":39450},{\"end\":39786,\"start\":39757},{\"end\":40092,\"start\":40088},{\"end\":40417,\"start\":40413},{\"end\":40773,\"start\":40770},{\"end\":41089,\"start\":41086},{\"end\":41454,\"start\":41450},{\"end\":41818,\"start\":41814},{\"end\":42108,\"start\":42104},{\"end\":42501,\"start\":42497},{\"end\":42890,\"start\":42886},{\"end\":43263,\"start\":43177},{\"end\":43608,\"start\":43604},{\"end\":43905,\"start\":43871},{\"end\":44170,\"start\":44166},{\"end\":44383,\"start\":44379},{\"end\":44683,\"start\":44680},{\"end\":45010,\"start\":45006},{\"end\":45368,\"start\":45364},{\"end\":45734,\"start\":45687},{\"end\":46059,\"start\":46056},{\"end\":46358,\"start\":46355},{\"end\":46687,\"start\":46683}]"}}}, "year": 2023, "month": 12, "day": 17}