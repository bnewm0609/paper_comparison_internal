{"id": 49904930, "updated": "2023-10-01 17:50:14.505", "metadata": {"title": "Physical Adversarial Examples for Object Detectors", "authors": "[{\"first\":\"Kevin\",\"last\":\"Eykholt\",\"middle\":[]},{\"first\":\"Ivan\",\"last\":\"Evtimov\",\"middle\":[]},{\"first\":\"Earlence\",\"last\":\"Fernandes\",\"middle\":[]},{\"first\":\"Bo\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Amir\",\"last\":\"Rahmati\",\"middle\":[]},{\"first\":\"Florian\",\"last\":\"Tramer\",\"middle\":[]},{\"first\":\"Atul\",\"last\":\"Prakash\",\"middle\":[]},{\"first\":\"Tadayoshi\",\"last\":\"Kohno\",\"middle\":[]},{\"first\":\"Dawn\",\"last\":\"Song\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2018, "month": 7, "day": 20}, "abstract": "Deep neural networks (DNNs) are vulnerable to adversarial examples-maliciously crafted inputs that cause DNNs to make incorrect predictions. Recent work has shown that these attacks generalize to the physical domain, to create perturbations on physical objects that fool image classifiers under a variety of real-world conditions. Such attacks pose a risk to deep learning models used in safety-critical cyber-physical systems. In this work, we extend physical attacks to more challenging object detection models, a broader class of deep learning algorithms widely used to detect and label multiple objects within a scene. Improving upon a previous physical attack on image classifiers, we create perturbed physical objects that are either ignored or mislabeled by object detection models. We implement a Disappearance Attack, in which we cause a Stop sign to\"disappear\"according to the detector-either by covering thesign with an adversarial Stop sign poster, or by adding adversarial stickers onto the sign. In a video recorded in a controlled lab environment, the state-of-the-art YOLOv2 detector failed to recognize these adversarial Stop signs in over 85% of the video frames. In an outdoor experiment, YOLO was fooled by the poster and sticker attacks in 72.5% and 63.5% of the video frames respectively. We also use Faster R-CNN, a different object detection model, to demonstrate the transferability of our adversarial perturbations. The created poster perturbation is able to fool Faster R-CNN in 85.9% of the video frames in a controlled lab environment, and 40.2% of the video frames in an outdoor environment. Finally, we present preliminary results with a new Creation Attack, where in innocuous physical stickers fool a model into detecting nonexistent objects.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1807.07769", "mag": "2964175514", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/woot/SongEEF0RTPK18", "doi": null}}, "content": {"source": {"pdf_hash": "658b68e07d586fceb6a474ba3cf04fab2b3948f5", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1807.07769v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "6c0651e14afd09fc3cf41705eac7d52abd5f0a14", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/658b68e07d586fceb6a474ba3cf04fab2b3948f5.txt", "contents": "\nPhysical Adversarial Examples for Object Detectors\n\n\nKevin Eykholt \nUniversity of Michigan\n\n\nIvan Evtimov \nUniversity of Washington\n\n\nEarlence Fernandes \nUniversity of Washington\n\n\nBo Li \nUniversity of California\nBerkeley\n\nAmir Rahmati \nStony Brook University\n\n\nSamsung Research America\n\n\nFlorian Tram\u00e8r \nStanford University\n\n\nAtul Prakash \nUniversity of Michigan\n\n\nTadayoshi Kohno \nUniversity of Washington\n\n\nDawn Song \nUniversity of California\nBerkeley\n\nPhysical Adversarial Examples for Object Detectors\nExtended version of our paper that appeared at WOOT 2018, co-located with USENIX Security.\nDeep neural networks (DNNs) are vulnerable to adversarial examples-maliciously crafted inputs that cause DNNs to make incorrect predictions. Recent work has shown that these attacks generalize to the physical domain, to create perturbations on physical objects that fool image classifiers under a variety of real-world conditions. Such attacks pose a risk to deep learning models used in safety-critical cyber-physical systems.In this work, we extend physical attacks to more challenging object detection models, a broader class of deep learning algorithms widely used to detect and label multiple objects within a scene. Improving upon a previous physical attack on image classifiers, we create perturbed physical objects that are either ignored or mislabeled by object detection models. We implement a Disappearance Attack, in which we cause a Stop sign to \"disappear\" according to the detector-either by covering the sign with an adversarial Stop sign poster, or by adding adversarial stickers onto the sign. In a video recorded in a controlled lab environment, the state-of-the-art YOLO v2 detector failed to recognize these adversarial Stop signs in over 85% of the video frames. In an outdoor experiment, YOLO was fooled by the poster and sticker attacks in 72.5% and 63.5% of the video frames respectively. We also use Faster R-CNN, a different object detection model, to demonstrate the transferability of our adversarial perturbations. The created poster perturbation is able to fool Faster R-CNN in 85.9% of the video frames in a controlled lab environment, and 40.2% of the video frames in an outdoor environment. Finally, we present preliminary results with a new Creation Attack, wherein innocuous physical stickers fool a model into detecting nonexistent objects.\n\nIntroduction\n\nDeep neural networks (DNNs) are widely applied in computer vision, natural language, and robotics, espe-cially in safety-critical tasks such as autonomous driving [10]. At the same time, DNNs have been shown to be vulnerable to adversarial examples [3,7,8,15,18], maliciously perturbed inputs that cause DNNs to produce incorrect predictions. These attacks pose a risk to the use of deep learning in safety-and security-critical decisions. For example, an attacker can add perturbations, which are negligible to humans, to a Stop sign and cause a DNN embedded in an autonomous vehicle to misclassify or ignore the sign.\n\nEarly works studied adversarial examples in the digital space only. However, it has recently been shown that it is also possible to create perturbations that survive under various physical conditions (e.g., object distance, pose, lighting, etc.) [1,2,5,9,21]. These works focus on attacking classification networks, i.e., models that produce a single prediction on a static input image. In this work, we start exploring physical adversarial examples for object detection networks, a richer class of deep learning algorithms that can detect and label multiple objects in a scene. Object detection networks are a popular tool for tasks that require real-time and dynamic recognition of surrounding objects, autonomous driving being a canonical application. Object detectors are known to be vulnerable to digital attacks [23], but their vulnerability to physical attacks remains an open question.\n\nCompared to classifiers, object detection networks are more challenging to attack: 1) Detectors process an entire scene instead of a single localized object. This allows detectors to use contextual information (e.g., the orientation and relative position of objects in the scene) to generate predictions. 2) Detectors are not limited to producing a single prediction. Instead, they label every recognized object in a scene, usually by combining predictions of the location of objects in a scene, and of the labeling of these objects. Attacks on object detectors need to take both types of predictions (presence/absence of an object and nature of the object) into account, whereas attacks on classifiers only focus on modifying the label of a single (presumably present) object.\n\nTo create proof-of-concept attacks for object detectors, we start from the existing Robust Physical Perturbations (RP 2 ) algorithm [5] of Eykholt et al., which was originally proposed to produce robust physical attacks on image classifiers. The approach taken by Eykholt et al. (as well as by others [1,9]) is to sample from a distribution that mimics physical perturbations of an object (e.g., view distance and angle), and find a perturbation that maximizes the probability of mis-classification under this distribution. We find that the physical perturbations considered in their work are insufficient to extend to object detectors.\n\nIndeed, when working with image classifiers, prior works considered target objects that make up a large portion of the image and whose relative position in the image varies little. Yet, when performing object detection in a dynamic environment such as a driving car, the relative size and position of the multiple objects in a scene can change drastically. These changes produce additional constraints that have to be taken into account to produce successful robust physical attacks. Many object detectors, for instance, split a scene into a grid or use a sliding window to identify regions of interest, and produce separate object predictions for each region of interest. As the relative position of an object changes, the grid cells the object is contained in (and the corresponding network weights) change as well. Robust perturbations, thus, have to be applicable to multiple grid cells simultaneously. We show that robustness to these physical modifications can be attained by extending the distribution of inputs considered by Eykholt et al. to account for additional synthetic transformations to objects in a scene (e.g., changes in perspective, size, and position).\n\nFollowing Eykholt et al., we consider physical adversarial attacks on the detection and classification of Stop signs, an illustrative example for the safety implications of a successful attack. The perturbations, while large enough to be visible to the human eye, are constrained to resemble human-made graffiti or subtle lighting artifacts that could be considered benign. We consider an untargeted attack specific to object detectors, which we refer to as a Disappearance Attack. In a Disappearance Attack, we create either an adversarial poster or physical stickers applied to a real Stop sign (see Figure 2), which causes the sign to be ignored by an object detector in different scenes with varying object distance, location, and perspective. This attack is analogous to the one considered by Eykholt et al. for image classifiers, but targets a richer class of deep neural networks.\n\nWe further introduce a new Creation Attack, wherein physical stickers that humans would ignore as being inconspicuous can cause an object detector into recognizing nonexistent Stop signs. This attack differs from prior attacks that attempt to fool a network into mis-classifying one object into another, in that it creates an entirely new object classification. Specifically, we experiment with creating adversarial stickers (similar to the ones considered in [2]). Such stickers could for instance be used to mount Denial of Service attacks on road-sign detectors.\n\nFor our experiments, we target the state-of-the-art YOLO v2 (You Only Look Once) object detector [17]. YOLO v2 is a deep convolutional neural network that performs real-time object detection for 80 object classes. Our indoor (laboratory) and outdoor experiments show that up to distances of 30 feet from the target object, detectors can be tricked into not perceiving the attacker's target object using poster and sticker perturbations.\n\nOur Contributions:\n\n\u2022 We extend the RP 2 algorithm of Eykholt et al. to provide proof-of-concept attacks for object detection networks, a richer class of DNNs than image classifiers.\n\n\u2022 Using our new and improved algorithm, we propose a new physical attack on object detection networks: the Disappearance Attack that cause physical objects to be ignored by a detector.\n\n\u2022 We evaluate our attacks on the YOLO v2 object detector in an indoor laboratory setting and an outdoor setting. Our results show that our adversarial poster perturbation fools YOLO v2 in 85.6% of the video frames recorded in an indoor lab environment and in 72.5% of the video frames recorded in an outdoor environment. Our adversarial stickers fool YOLO v2 in 85% of the video frames recorded in a laboratory environment and in 63.5% of the video frames recorded in an outdoor environment.\n\n\u2022 We evaluate the transferability of our attacks using the Faster R-CNN object detector in laboratory and outdoor environments. Our results show that our attacks fool Faster R-CNN in 85.9% of the video frames recorded in a laboratory environment and in 40.2% of the video frames recorded in an outdoor environment.\n\n\u2022 We propose and experiment with a new type of Creation attack, that aims at fooling a detector into recognizing adversarial stickers as non-existing objects. Our results with this attack type are preliminary yet encouraging.\n\nOur work demonstrates that physical perturbations are effective against object detectors, and leaves open some future questions: 1) Generalization to other physical settings (e.g., moving vehicles, or even real autonomous vehicles). 2) Further exploration of other classes of attacks:\n\nOur work introduces the disappearance and creation attacks which use posters or stickers, yet there are other plausible attack types (e.g., manufacturing physical objects that are not recognizable to humans, but are recognized by DNNs). 3) Physical attacks on segmentation networks. We envision that future work will build on the findings presented here, and will create attacks that generalize across physical settings (e.g., real autonomous vehicles), and across classes of object detection networks (e.g., semantic segmentation [23]).\n\n\nRelated Work\n\nAdversarial examples for deep learning were first introduced by Szegedy et al. [22]. Since their seminal work, there have been several works proposing more efficient algorithms for generating adversarial examples [3,7,13,15]. All of these works assume that the attacker has \"digital-level\" access to an input, e.g., that the attacker can make arbitrary pixel-level changes to an input image of a classifier. For uses of deep learning in cyberphysical systems (e.g., in an autonomous vehicle), these attacks thus implicitly assume that the adversary controls a DNN's input system (e.g., a camera). A stronger and more realistic threat model would assume that the attacker only controls the physical layer, e.g., the environment or objects that the system interacts with, but not the internal sensors and data pipelines of the system. This stronger threat model was first explored by Kurakin et al. They generated physical adversarial examples by printing digital adversarial examples on paper [9]. In their work, they found that a significant portion of the printed adversarial examples fooled an image classifier. However, their experiments were done without any variation in the physical conditions such as different viewing angles or distances.\n\nAthalye et al. improved upon the work of Kurakin et al. by creating adversarial objects that are robust to variations in viewing angle [1]. To account for such variations, they model small scale transformations synthetically when generating adversarial perturbations. They demonstrate several examples of adversarial objects that fool their target classifiers, but it is not clear how many transformations their attack is robust to. In their paper, they state their algorithm is robust to rotations, translations, and noise and suggest their algorithm is robust so long as the transformation can be modeled synthetically.\n\nEykholt et al. also proposed an attack algorithm capable of generating physical adversarial examples [5]. Unlike Athalye et al., they choose to model image transformations both synthetically and physically. Certain image transformations, such as changes in viewing angle and distance, are captured in their victim dataset. They apply other image transformations, such as lighting, syntheti-cally when generating adversarial examples. Their work suggests that sole reliance on synthetic transformations can miss subtleties in the physical environment, thus resulting in a less robust attack. Different from all prior work that focused on classifiers, our work focuses on the broader class of object detection models. Specifically, we extend the algorithm of Eykholt et al. using synthetic transformations (perspective, position, scale) to attack object detection models.\n\nLu et al. performed experiments using adversarial road signs printed on paper with the YOLO object detector [12]. Their results suggested that it is very challenging to fool YOLO with physical adversarial examples.\n\nConcurrent to our work, Chen et al. attacked the Faster R-CNN object detector [4]. Their attack relies on generating adversarial poster perturbations that replace the road sign to fool Faster R-CNN. Our work differs in that we introduce two different attacks on object detectors, the disappearance and creation attacks, which use either adversarial poster or sticker perturbations. We also show black-box transferability from YOLO to the Faster-RCNN detector.\n\nOur work resolves the challenges and shows that existing algorithms can be adapted to produce physical attacks on object detectors in highly variable environmental conditions.\n\n\nBackground on Object Detectors\n\nObject classification is a standard task in computer vision. Given an input image and a set of class labels, the classification algorithm outputs the most probable label (or a probability distribution over all labels) for the image. Object classifiers are limited to categorizing a single object per image. If an image contains multiple objects, the classifier only outputs the class of the most dominant object in the scene. In contrast, object detectors both locate and classify multiple objects in a given scene.\n\nThe first proposed deep neural network for object detection was Overfeat [19], which combined a sliding window algorithm and convolution neural networks. A more recent proposal, Regions with Convolutional Neural Networks (R-CNN) uses a search algorithm to generate region proposals, and a CNN to label each region. A downside of R-CNN is that the region proposal algorithm is too slow to be run in real-time. Subsequent works-Fast R-CNN [6] and Faster R-CNN [20]-replace this inefficient algorithm with a more efficient CNN.\n\nThe above algorithms treat object detection as a twostage problem consisting of region proposals followed by classifications for each of these regions. In contrast, socalled \"single shot detectors\" such as YOLO [16] (and the subsequent YOLO v2 [17]) or SSD [11] run a single CNN over the input image to jointly produce confidence  scores for object localization and classification. As a result, these networks can achieve the same accuracy while processing images much faster. In this work, we focus on YOLO v2, a state-of-the-art object detector with realtime detection capabilities and high accuracy.\n\nThe classification approach of YOLO v2 is illustrated in Figure 1. A single CNN is run on the full input image and predicts object location (bounding boxes) and label confidences for 361 separate grid cells (organized into a 19 \u00d7 19 square over the original image). For each cell, YOLO v2 makes a prediction for 5 different boxes. For each box, the prediction contains the box confidence (the probability that this box contains an object), its location and the probability of each class label for that box. A box is discarded if the product of the box confidence and the probability of the most likely class is below some threshold (this threshold is set to 0.1 in our experiments). Finally, the non-max suppression algorithm is applied in a post-processing phase to discard redundant boxes with high overlap [17].\n\nSuch an object detection pipeline introduces several new challenges regarding physical adversarial examples: First, unlike classification where an object is always assumed present and the attack only needs to modify the class probabilities, attacks on a detector network need to control a combination of box confidences and class probabilities for all boxes in all grid cells of the input scene. Second, classifiers assume the object of interest is centered in the input image, whereas detectors can find objects at arbitrary positions in a scene. Finally, the object's size in the detector's input is not fixed. In classification, the image is usually cropped and resized to focus on the object being classified. Object detectors are meant to reliably detect objects at multiple scales, distances and angles in a scene.\n\nThese challenges mainly stem from object detectors being much more flexible and broadly applicable than standard image classifiers. Thus, albeit harder to attack, object detectors also represent a far more interesting attack target than image classifiers, as their extra flexibility makes them a far better candidate for use in reliable cyber-physical systems.\n\n\nPhysical Adversarial Examples for Object Detectors\n\nWe will first summarize the original RP 2 algorithm, before discussing the modifications necessary to adapt the algorithm to attack object detectors.\n\n\nThe RP 2 Algorithm\n\nThe RP 2 algorithm proposed by Eykholt et al. optimizes the following objective function:\nargmin \u03b4 \u03bb ||M x \u00b7 \u03b4 || p + NPS(M x \u00b7 \u03b4 ) + E x i \u223cX V J( f \u03b8 (x i + T i (M x \u00b7 \u03b4 )), y * )(1)\nThe first term of the objective function is the p norm (with scaling factor \u03bb ) of the perturbation \u03b4 masked by M x . The mask is responsible for spatially constraining the perturbation \u03b4 to the surface of the target object. For example, in Figure 2, the mask shape is two horizontal bars on the sign.\n\nThe second term of the objective function measures the printability of an adversarial perturbation. Eykholt et al. borrow this term from prior work [21]. The printability of a perturbation is affected by two factors. First, the colors the computed perturbation must reproduce. Modern printers have a limited color gamut, thus certain colors that appear digitally may not be printable. Second, a printer may not faithfully reproduce a color as it is shown digitally (see Figure 3).\n\nThe last term of the objective function is the value of the loss function, J(\u00b7, \u00b7) averaged across all of the images sampled from X V . In practice, this is a set of victim images. The victim dataset is composed of multiple images of the object taken under a variety of physical conditions such as changes in viewing angle, viewing distance and lighting. T i is an \"alignment function\" that applies a digital transformation that mimics the physical conditions of victim object x i . For example, if the victim object x i is a rotated version of the \"canonical\" target object, then the perturbation M x \u00b7 \u03b4 should also be rotated appropriately. Thus, to simulate physical consistency of the perturbed object, we apply the alignment function T i to the masked perturbation. f \u03b8 (\u00b7) is the output of the classifier network, and y * is the adversarial target class.\n\n\nExtensions to RP 2 for Object Detectors\n\nOur modified version of RP 2 contains three key differences from the original algorithm proposed by Eykholt et al. First, due to differences in the output behavior of classifiers and object detectors, we make modifications to the adversarial loss function. Second, we observed additional constraints that an adversarial perturbation must be robust to and model these constraints synthetically. Finally, we introduce a smoothness constraint into the objective, rather than using the p norm. In the following, we discuss each of these changes in detail.\n\n\nModified Adversarial Loss Function\n\nAn object detector outputs a set of bounding boxes and the likelihood of the most probable object contained within that box given a certain confidence threshold. See Figure 1 for a visualization of this output. By contrast, a classifier outputs a single vector where each entry represents the probability that the object in the image is of that type. Attacks on image classifiers typically make use of the cross-entropy loss between this output vector, and a one-hot representation of the adversarial target. However, this loss function is not applicable to object detectors due to their richer output structure. Thus, we introduce a new adversarial loss function suitable for use with detectors. This loss function is tailored to the specific attacks we introduce in this work.\n\nDisappearance Attack Loss. The goal of the attacker is to prevent the object detector from detecting the target object. To achieve this, the adversarial perturbation must ensure that the likelihood of the target object in any bounding box is less than the detection threshold (the default is 25% for YOLO v2). In our implementation of the attack, we used the following loss function:\nJ d (x, y) = max s\u2208S 2 ,b\u2208B P(s, b, y, f \u03b8 (x))(2)\nWhere f \u03b8 (x) represents the output of the object detector (for YOLO v2, this is a 19 \u00d7 19 \u00d7 425 tensor). P(\u00b7) is a function that extracts the probability of an object class from this tensor, with label y (in our case, this is a Stop sign) in grid cell s and bounding box b. We denote x as the input scene containing our perturbed target object. Therefore, the loss function outputs the maximum probability of a Stop sign if it occurs within the scene. Using this loss function, the goal of the adversary is to directly minimize that probability until it falls below the detection threshold of the network.\n\nCreation Attack Loss. We propose a new type of Creation Attack, wherein the goal is to fool the model into recognizing nonexistent objects. Similar to the \"adversarial patch\" approach of [2], our goal is to create a physical sticker that can be added to any existing scene. Contrary to prior work, rather than causing a misclassification our aim is to create a new classification (i.e., a new object detection) where non existed before.\n\nFor this, we use a composite loss function, that first aims at creating a new object localization, followed by a targeted \"mis-classification.\" The mask M x is sampled randomly so that the adversarial patch is applied to an arbitrary location in the scene. As above, let f \u03b8 (x) represent the full output tensor of YOLO v2 on input scene x, and let P(s, b, y, f \u03b8 (x)) represent the probability assigned to class y in box b of grid cell s. Further let P box (s, b, f \u03b8 (x)) represent the probability of the box only, i.e., the model's confidence that the box contains any object. Our loss is then\nobject = P box (s, b, f \u03b8 (x)) > \u03c4 J c (x, y) = object + (1 \u2212 object) \u00b7 P(s, b, y, f \u03b8 (x)) (3)\nHere, \u03c4 is a threshold on the box confidence (set to 0.2 in our experiments), after which we stop optimizing the box confidence and focus on increasing the probability of the targeted class. As our YOLO v2 implementation uses a threshold of 0.1 on the product of the box confidence and class probability, any box with a confidence above 0.2 and a target class probability above 50% is retained.\n\n\nSynthetic Representation of New Physical Constraints\n\nGenerating physical adversarial examples for detectors requires simulating a larger set of varying physical conditions than what is needed to trick classifiers. In our initial experiments, we observed that the generated perturbations would fail if the object was moved from its original position in the image. This is likely because a detector has access to more contextual information when generating predictions. As an object's position and size can vary greatly depending on the viewer's location, perturbations must account for these additional constraints.\n\nTo generate physical adversarial perturbations that are positionally invariant, we chose to synthetically model two environmental conditions: object rotation (in the Z plane) and position (in the X-Y plane). In each epoch of the optimization, we randomly place and rotate the object. Our approach differs from the original approach used by Eykholt et al., in that they modeled an object's rotation physically using a diverse dataset. We avoided this approach because of the added complexity necessary for the alignment function, T i , to properly position the adversarial perturbation on the sign. Since these transformations are done synthetically, the alignment function, T i , simply needs to use the same process to transform the adversarial perturbation.\n\n\nNoise Smoothing using Total Variation\n\nThe unmodified RP 2 algorithm uses the p norm to smooth the perturbation. However, in our initial exper- Figure 4: Output of the extended RP 2 algorithm to attack YOLO v2 using poster and sticker attacks.\n\niments, we observed that the p norm results in very pixelated perturbations. The pixelation hurts the success rate of the attack, especially as the distance between the viewer and the object increases. We found that using the total variation norm in place of the p norm gave smoother perturbations, thus increasing the effective range of the attack. Given a mask, M x , and noise \u03b4 , the total variation norm of the adversarial perturbation, M x \u00b7 \u03b4 , is:\nTV (M x \u00b7 \u03b4 ) = \u2211 i, j |(M x \u00b7 \u03b4 ) i+1, j \u2212 (M x \u00b7 \u03b4 ) i, j | + |(M x \u00b7 \u03b4 ) i, j+1 \u2212 (M x \u00b7 \u03b4 ) i, j |(4)\nwhere i, j are the row and column indices for the adversarial perturbation. Thus our final modified objective function is:\nargmin \u03b4 \u03bb TV (M x \u00b7 \u03b4 ) + NPS + E x i \u223cX V J d (x i + T i (M x \u00b7 \u03b4 ), y * )(5)\nwhere J d (\u00b7, y * ) is the loss function (discussed earlier) that measures the maximum probability of an object with the label y * contained in the image. In our attack, y * is a Stop sign.\n\n\nEvaluation\n\nWe first discuss our experimental method, where we evaluate attacks in a whitebox manner using YOLO v2, and in a blackbox manner using Faster-RCNN. Then, we discuss our results, showing that state-of-the-art object detectors can be attacked using physical posters and stickers. Figure 4 \n\n\nExperimental Setup\n\nWe evaluated our disappearance attack in a mix of lab and outdoor settings. For both the poster and sticker attacks, we generated adversarial perturbations and recorded several seconds of video. In each experiment, recording began 30 feet from the sign and ended when no part of the sign was in the camera's field of view. Then, we fed the video into the object detection network for analysis. We used the YOLO v2 object detector as a white-box attack. We also ran the same videos through the Faster-RCNN network to measure black-box transferability of our attack. For the creation attack, we experimented with placing stickers on large flat objects (e.g., a wall or cupboard), and recording videos within 10 feet of the sticker.\n\n\nExperimental Results\n\nWe evaluated the perturbations for a disappearance attack using two different masks and attacked a Stop sign. First, we tested a poster perturbation, which used an octagonal mask to allow adversarial noise to to be added anywhere on the surface of the Stop sign. Next, we tested a sticker perturbation. We used the mask to create two rectangular stickers positioned at the top and bottom of the sign. The results of our attack are shown in Table 1 The table cells show the ratio: number of frames in which a Stop sign was not detected / total number of frames, and a success rate, which is the result of this ratio.\n\nIn indoor lab settings, where the environment is relatively stable, both the poster and sticker perturbation demonstrate a high success rate in which at least 85% of the total video frames do not contain a Stop sign bounding box. When we evaluated our perturbations in an outdoor environment, we notice a drop in success rate for both attacks. The sticker perturbation also appears to be slightly weaker. We noticed that the sticker perturbation did especially poorly when only a portion of the sign was in the camera's field of view. Namely, when the sticker perturbation began to leave the camera's field of view, the Stop sign bounding boxes appear very frequently. In contrast, this behavior was not observed in the poster perturbation experiments, likely because some part of the adversarial noise is always present in the video due to the mask's shape. Figure 7 shows some frame captures of our adversarial Stop sign videos.\n\nTo measure the transferability of our attack, we also evaluated the recorded videos using the Faster R-CNN object detection network. 1 . The results for these experiments are shown in Table 2.\n\nWe see from these results that both perturbations transfer with a relatively high success rate in indoor lab settings where the environment conditions are stable. However, once outdoors, the success rate for both perturbations decreases significantly, but both perturbations retain moderate success rates. We observe that our improved attack algorithm can generate an adversarial poster perturbation, which transfers to other object detection frameworks, especially in stable environments.\n\nFinally, we report on some preliminary results for creation attacks (the results are considered preliminary in that we have spent considerably less time optimizing these attacks compared to the disappearance attacks-it is thus likely that they can be further improved). When applying multiple copies of the sticker in Figure 5 to a cupboard and office wall, YOLO v2 detects stop signs in 25%-79% of the frames over multiple independent videos. A sample video frame is shown in Figure 6. Compared to the disappearance attack, the creation attack is more sensitive to the sticker's size, surroundings, and camera movement in the video. This results in highly variable success rates and is presumably because (due to resource constraints) we applied fewer physical and digital transformations when generating the attack. Enhancing the reliability and robustness of our creation attack is an interesting avenue for future work, as it presents a novel attack vector (e.g., DOS style attacks) for adversarial examples.\n\n\nDiscussion\n\nIn the process of generating physical adversarial examples for object detectors, we note several open research questions that we leave to future work.\n\nLack of detail due to environmental conditions. We noticed physical conditions (e.g., poor lighting, far distance, sharp angles), which only allowed macro features of the sign (i.e., shape, general color, lettering) to be observed clearly. Due to such conditions, the details of the perturbations were lost, causing it to fail. This is expected as our attack relies on the camera being able to perceive the adversarial perturbations somewhat accurately. When extreme environmental conditions prevent the camera from observing finer details of the perturbation on the sign, the adversarial noise is lost. We theorize that in order to successfully fool object detectors under these extreme conditions, the macro features of the sign need to be attacked. For example, we could create attachments on the outside edges of the sign in order to change its perceived shape.\n\nAlternative attacks on object detectors. In this work, we explored attacking the object detector such that it fails to locate an object, or that it detects non-existent objects. There are several alternative forms of attack we could consider. One alternative is to attempt to generate physical perturbations that preserve the bounding box of an object, but alter its label (this is similar to targeted attacks for classifiers). Another option is to generate further 2D or even 3D objects that appear nonsensical to a human, but are detected and labeled by the object detector. The success of either of these attacks, which have been shown to work digitally [14,23], would have major safety implications.\n\nExtensions to semantic segmentation. A broader task than object detection is semantic segmentation-where the network labels every pixel in a scene as belonging to an object. Recent work has shown digital attacks against semantic segmentation [23]. An important future work question is how to extend current attack techniques for classifiers, and detectors (as this work shows) to create physical attacks on segmentation networks.\n\nImpact on Real Systems. Existing cyber-physical systems such as cars and drones integrate object detectors into a control pipeline that consists of pre-and post-processing steps. The attacks we show only target the object detection component in isolation (specifically YOLO v2). Understanding whether these attacks are capable of compromising a full control pipeline in an end-to-end manner is an important open question. Although YOLO v2 does recognize a Stop sign in some frames from our attack videos, a real system would generally base its control decisions on a majority of predictions, rather than a few frames. Our attack manages to trick the detector into not seeing a Stop sign in a majority of the tested video frames.\n\nDespite these observations, we stress that a key step towards understanding the vulnerability of the broad class of object detection models to physical adversarial examples is to create algorithms that can attack state-ofthe-art object detectors. In this work, we have shown how to can extend the existing RP 2 algorithm with positional and rotational invariance to attack object detectors in relatively controlled settings.\n\n\nConclusion\n\nStarting from an algorithm to generate robust physical perturbations for classifiers, we extend it with positional and rotational invariance to generate physical perturbations for state-of-the-art object detectors-a broader class of deep neural networks that are used in dynamic settings to detect and label objects within scenes. Object detectors are popular in cyber-physical systems such as autonomous vehicles. We experiment with the YOLO v2 object detector, showing that it is possible to physically perturb a Stop sign such that the detector ignores it. When presented with a video of the adversarial poster perturbation, YOLO failed to recognize the sign in 85.6% of the video frames in a controlled lab environment, and in 72.5% of the video frames in an outdoor environment. When presented with a video of the adversarial sticker perturbation, YOLO failed to recognize the sign in 85% of the video frames in a controlled lab environment, and in 63.5% of the video frames in an outdoor environment. We also observed limited blackbox transferability to the Faster-RCNN detector. The poster perturbation fooled Faster R-CNN in 85.9% of the video frames in a controlled lab environment, and in 40.2% of the video frames in an outdoor environment. Our work, thus, takes steps towards developing a more informed understanding of the vulnerability of object detectors to physical adversarial examples. \n\nFigure 1 :\n1For an input scene, the YOLO v2 CNN outputs a 19 \u00d7 19 \u00d7 425 tensor. To generate this tensor, YOLO divides the input image into a square grid of S 2 cells (S = 19). For each grid cell, there are B bounding boxes (B = 5). Each bounding box predicts 5 values: probability of an object in the cell, co-ordinates of the bounding box (center x, center y, width, height). Additionally, for each bounding box the model predicts a probability distribution over all 80 output classes.\n\nFigure 2 :Figure 3 :\n23An example of an adversarial perturbation overlaid on a synthetic background. The Stop sign in the image is printed such that it is the same size as a U.S. Stop sign. Then, we cut out the two rectangle bars, and use the original print as a stencil to position the cutouts on a real Stop sign. The image in (a) shows the image as it is stored digitally. The result of printing and taking a picture of the image in (a) is shown in (b).\n\n\nshows the digital versions of posters and stickers used for disappearance attacks, while Figure 5 shows a digital version of the sticker used in a creation attack.\n\nFigure 5 :\n5Patch created by the Creation Attack, aimed at fooling YOLO v2 into detecting nonexistent Stop signs.\n\nFigure 6 :\n6Sample frame from our creation attack video after being processed by YOLO v2. The scene includes 4 adversarial stickers reliably recognized as Stop signs.\n\nFigure 7 :\n7Sample frames from our attack videos after being processed by YOLO v2. In the majority of frames, the detector fails to recognize the Stop sign.\n\nTable 1 :\n1Attack success rate for the disappearance attack \non YOLO v2. We tested a poster perturbation, where \na true-sized print is overlaid on a real Stop sign, and a \nsticker attack, where the perturbation is two rectangles \nstuck to the surface of the sign. The table cells show \nthe ratio: number of frames in which a Stop sign was \nnot detected / total number of frames, and a success rate, \nwhich is the result of this ratio. \n\n\n\n\n.FR-CNN \nPoster \nSticker \n\nIndoors \n189/220 (85.9%) 146/248 (58.9%) \n\nOutdoors 84/209 (40.2%) \n47/249 (18.9%) \n\n\n\nTable 2 :\n2Attack success rate for the disappearance attack \non Faster R-CNN. We tested a poster perturbation, where \nthe entire Stop sign is replaced with a true-sized print, \nand a sticker attack, where the perturbation is two rect-\nangles stuck to the surface of the sign. \nWe used the Tensorflow-Python implementation of Faster R-CNN found at https://github.com/endernewton/tf-faster-rcnn It has a default detection threshold of 80%\nAcknowledgementsWe thank the reviewers for their insightful feedback. This work was supported in part by NSF grants 1422211, 1565252, 1616575, 1646392, 1740897, Berkeley Deep Drive, the Center for Long-Term Cybersecurity, FORCES (which receives support from the NSF), the Hewlett Foundation, the MacArthur Foundation, a UM-SJTU grant, and the UW Tech Policy Lab.\nSynthesizing robust adversarial examples. A Athalye, I Sutskever, arXiv:1707.07397arXiv preprintATHALYE, A., AND SUTSKEVER, I. Synthesizing robust adver- sarial examples. arXiv preprint arXiv:1707.07397 (2017).\n\n. T B Brown, D Man\u00e9, A Roy, M Abadi, J Gilmer, arXiv:1712.09665Adversarial patch. arXiv preprintBROWN, T. B., MAN\u00c9, D., ROY, A., ABADI, M., AND GILMER, J. Adversarial patch. arXiv preprint arXiv:1712.09665 (2017).\n\nTowards evaluating the robustness of neural networks. N Carlini, D Wagner, Security and Privacy (SP. IEEECARLINI, N., AND WAGNER, D. Towards evaluating the robust- ness of neural networks. In Security and Privacy (SP), 2017 IEEE Symposium on (2017), IEEE, pp. 39-57.\n\nRobust physical adversarial attack on faster R-CNN object detector. S Chen, C Cornelius, J Martin, D H Chau, Shapeshifter, CoRR abs/1804.05810CHEN, S., CORNELIUS, C., MARTIN, J., AND CHAU, D. H. Shapeshifter: Robust physical adversarial attack on faster R-CNN object detector. CoRR abs/1804.05810 (2018).\n\nRobust physical-world attacks on machine learning models. I Evtimov, K Eykholt, E Fernandes, T Kohno, B Li, A Prakash, A Rahmati, D Song, 18EVTIMOV, I., EYKHOLT, K., FERNANDES, E., KOHNO, T., LI, B., PRAKASH, A., RAHMATI, A., AND SONG, D. Robust physical-world attacks on machine learning models. CVPR '18 (2018).\n\nR Girshick, R-Cnn Fast, Proceedings of the International Conference on Computer Vision (ICCV). the International Conference on Computer Vision (ICCV)GIRSHICK, R. Fast R-CNN. In Proceedings of the International Conference on Computer Vision (ICCV) (2015).\n\nI J Goodfellow, J Shlens, C Szegedy, arXiv:1412.6572Explaining and harnessing adversarial examples. arXiv preprintGOODFELLOW, I. J., SHLENS, J., AND SZEGEDY, C. Ex- plaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572 (2014).\n\nAdversarial examples for generative models. J Kos, I Fischer, D Song, arXiv:1702.06832arXiv preprintKOS, J., FISCHER, I., AND SONG, D. Adversarial examples for generative models. arXiv preprint arXiv:1702.06832 (2017).\n\nAdversarial examples in the physical world. A Kurakin, I Goodfellow, S Bengio, arXiv:1607.02533arXiv preprintKURAKIN, A., GOODFELLOW, I., AND BENGIO, S. Adversarial examples in the physical world. arXiv preprint arXiv:1607.02533 (2016).\n\nT P Lillicrap, J J Hunt, A Pritzel, N Heess, T Erez, Y Tassa, D Silver, D Wierstra, arXiv:1509.02971Continuous control with deep reinforcement learning. arXiv preprintLILLICRAP, T. P., HUNT, J. J., PRITZEL, A., HEESS, N., EREZ, T., TASSA, Y., SILVER, D., AND WIERSTRA, D. Continu- ous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971 (2015).\n\nSingle shot multibox detector. W Liu, D Anguelov, D Erhan, C Szegedy, S Reed, C.-Y Fu, A C Berg, Ssd, European conference on computer vision. SpringerLIU, W., ANGUELOV, D., ERHAN, D., SZEGEDY, C., REED, S., FU, C.-Y., AND BERG, A. C. Ssd: Single shot multibox detector. In European conference on computer vision (2016), Springer, pp. 21-37.\n\nNO need to worry about adversarial examples in object detection in autonomous vehicles. J Lu, H Sibai, E Fabry, D A Forsyth, CoRR abs/1707.03501LU, J., SIBAI, H., FABRY, E., AND FORSYTH, D. A. NO need to worry about adversarial examples in object detection in au- tonomous vehicles. CoRR abs/1707.03501 (2017).\n\nDeepfool: a simple and accurate method to fool deep neural networks. S.-M Moosavi-Dezfooli, A Fawzi, P Frossard, arXiv:1511.04599arXiv preprintMOOSAVI-DEZFOOLI, S.-M., FAWZI, A., AND FROSSARD, P. Deepfool: a simple and accurate method to fool deep neural net- works. arXiv preprint arXiv:1511.04599 (2015).\n\nDeep neural networks are easily fooled: High confidence predictions for unrecognizable images. A Nguyen, J Yosinski, J Cluneand, Computer Vision and Pattern Recognition (CVPR). NGUYEN, A., YOSINSKI, J., AND CLUNEAND, J. Deep neural networks are easily fooled: High confidence predictions for un- recognizable images. In In Computer Vision and Pattern Recog- nition (CVPR) (2015).\n\nThe limitations of deep learning in adversarial settings. N Papernot, P Mcdaniel, S Jha, M Fredrikson, Z B Celik, A Swami, Security and Privacy. EuroS&P2016PAPERNOT, N., MCDANIEL, P., JHA, S., FREDRIKSON, M., CELIK, Z. B., AND SWAMI, A. The limitations of deep learning in adversarial settings. In Security and Privacy (EuroS&P), 2016\n\n. IEEE European Symposium on. IEEEIEEE European Symposium on (2016), IEEE, pp. 372-387.\n\nYou only look once: Unified, real-time object detection. J Redmon, S Divvala, R Girshick, A Farhadi, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionREDMON, J., DIVVALA, S., GIRSHICK, R., AND FARHADI, A. You only look once: Unified, real-time object detection. In Pro- ceedings of the IEEE conference on computer vision and pattern recognition (2016), pp. 779-788.\n\nYOLO9000: better, faster, stronger. J Redmon, A Farhadi, CoRR abs/1612.08242REDMON, J., AND FARHADI, A. YOLO9000: better, faster, stronger. CoRR abs/1612.08242 (2016).\n\nAdversarial manipulation of deep representations. S Sabour, Y Cao, F Faghri, D J Fleet, arXiv:1511.05122arXiv preprintSABOUR, S., CAO, Y., FAGHRI, F., AND FLEET, D. J. Ad- versarial manipulation of deep representations. arXiv preprint arXiv:1511.05122 (2015).\n\nOverfeat: Integrated recognition, localization and detection using convolutional networks. P Sermanet, D Eigen, X Zhang, M Mathieu, R Fer-Gus, Y Lecun, International Conference on Learning Representations (ICLR. BanffSERMANET, P., EIGEN, D., ZHANG, X., MATHIEU, M., FER- GUS, R., AND LECUN, Y. Overfeat: Integrated recognition, lo- calization and detection using convolutional networks. In Inter- national Conference on Learning Representations (ICLR) (Banff) (2013).\n\nShaoqing Ren, R G J S He, Faster R-Cnn, arXiv:1506.01497Towards real-time object detection with region proposal networks. arXiv preprintSHAOQING REN, KAIMING HE, R. G. J. S. Faster R-CNN: To- wards real-time object detection with region proposal networks. arXiv preprint arXiv:1506.01497 (2015).\n\nAccessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition. M Sharif, S Bhagavatula, L Bauer, M K Reiter, Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security. the 2016 ACM SIGSAC Conference on Computer and Communications SecurityACMSHARIF, M., BHAGAVATULA, S., BAUER, L., AND REITER, M. K. Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security (2016), ACM, pp. 1528-1540.\n\nIntriguing properties of neural networks. C Szegedy, W Zaremba, I Sutskever, J Bruna, D Er-Han, I Goodfellow, R Fergus, International Conference on Learning Representations. SZEGEDY, C., ZAREMBA, W., SUTSKEVER, I., BRUNA, J., ER- HAN, D., GOODFELLOW, I., AND FERGUS, R. Intriguing prop- erties of neural networks. In International Conference on Learn- ing Representations (2014).\n\nAdversarial examples for semantic segmentation and object detection. C Xie, J Wang, Z Zhang, Y Zhou, L Xie, A L Yuille, CoRR abs/1703.08603XIE, C., WANG, J., ZHANG, Z., ZHOU, Y., XIE, L., AND YUILLE, A. L. Adversarial examples for semantic segmentation and object detection. CoRR abs/1703.08603 (2017).\n", "annotations": {"author": "[{\"end\":93,\"start\":54},{\"end\":134,\"start\":94},{\"end\":181,\"start\":135},{\"end\":223,\"start\":182},{\"end\":289,\"start\":224},{\"end\":327,\"start\":290},{\"end\":366,\"start\":328},{\"end\":410,\"start\":367},{\"end\":456,\"start\":411}]", "publisher": null, "author_last_name": "[{\"end\":67,\"start\":60},{\"end\":106,\"start\":99},{\"end\":153,\"start\":144},{\"end\":187,\"start\":185},{\"end\":236,\"start\":229},{\"end\":304,\"start\":298},{\"end\":340,\"start\":333},{\"end\":382,\"start\":377},{\"end\":420,\"start\":416}]", "author_first_name": "[{\"end\":59,\"start\":54},{\"end\":98,\"start\":94},{\"end\":143,\"start\":135},{\"end\":184,\"start\":182},{\"end\":228,\"start\":224},{\"end\":297,\"start\":290},{\"end\":332,\"start\":328},{\"end\":376,\"start\":367},{\"end\":415,\"start\":411}]", "author_affiliation": "[{\"end\":92,\"start\":69},{\"end\":133,\"start\":108},{\"end\":180,\"start\":155},{\"end\":222,\"start\":189},{\"end\":261,\"start\":238},{\"end\":288,\"start\":263},{\"end\":326,\"start\":306},{\"end\":365,\"start\":342},{\"end\":409,\"start\":384},{\"end\":455,\"start\":422}]", "title": "[{\"end\":51,\"start\":1},{\"end\":507,\"start\":457}]", "venue": null, "abstract": "[{\"end\":2376,\"start\":599}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2559,\"start\":2555},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2644,\"start\":2641},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2646,\"start\":2644},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2648,\"start\":2646},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2651,\"start\":2648},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2654,\"start\":2651},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3262,\"start\":3259},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3264,\"start\":3262},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3266,\"start\":3264},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3268,\"start\":3266},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3271,\"start\":3268},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3835,\"start\":3831},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4822,\"start\":4819},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4991,\"start\":4988},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4993,\"start\":4991},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7852,\"start\":7849},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8057,\"start\":8053},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10621,\"start\":10617},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10723,\"start\":10719},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10856,\"start\":10853},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10858,\"start\":10856},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10861,\"start\":10858},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10864,\"start\":10861},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11635,\"start\":11632},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":12026,\"start\":12023},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":12615,\"start\":12612},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":13494,\"start\":13490},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":13679,\"start\":13676},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":14863,\"start\":14859},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":15226,\"start\":15223},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":15248,\"start\":15244},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":15527,\"start\":15523},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":15560,\"start\":15556},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":15573,\"start\":15569},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":16729,\"start\":16725},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":18781,\"start\":18777},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":22619,\"start\":22616},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":29297,\"start\":29296},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":32555,\"start\":32551},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":32558,\"start\":32555},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":32845,\"start\":32841}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":36091,\"start\":35604},{\"attributes\":{\"id\":\"fig_2\"},\"end\":36549,\"start\":36092},{\"attributes\":{\"id\":\"fig_3\"},\"end\":36715,\"start\":36550},{\"attributes\":{\"id\":\"fig_4\"},\"end\":36830,\"start\":36716},{\"attributes\":{\"id\":\"fig_5\"},\"end\":36998,\"start\":36831},{\"attributes\":{\"id\":\"fig_6\"},\"end\":37156,\"start\":36999},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":37595,\"start\":37157},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":37710,\"start\":37596},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":37988,\"start\":37711}]", "paragraph": "[{\"end\":3011,\"start\":2392},{\"end\":3906,\"start\":3013},{\"end\":4685,\"start\":3908},{\"end\":5323,\"start\":4687},{\"end\":6498,\"start\":5325},{\"end\":7387,\"start\":6500},{\"end\":7954,\"start\":7389},{\"end\":8392,\"start\":7956},{\"end\":8412,\"start\":8394},{\"end\":8576,\"start\":8414},{\"end\":8762,\"start\":8578},{\"end\":9255,\"start\":8764},{\"end\":9571,\"start\":9257},{\"end\":9798,\"start\":9573},{\"end\":10084,\"start\":9800},{\"end\":10623,\"start\":10086},{\"end\":11886,\"start\":10640},{\"end\":12509,\"start\":11888},{\"end\":13380,\"start\":12511},{\"end\":13596,\"start\":13382},{\"end\":14057,\"start\":13598},{\"end\":14234,\"start\":14059},{\"end\":14784,\"start\":14269},{\"end\":15310,\"start\":14786},{\"end\":15914,\"start\":15312},{\"end\":16730,\"start\":15916},{\"end\":17552,\"start\":16732},{\"end\":17914,\"start\":17554},{\"end\":18118,\"start\":17969},{\"end\":18230,\"start\":18141},{\"end\":18627,\"start\":18326},{\"end\":19109,\"start\":18629},{\"end\":19972,\"start\":19111},{\"end\":20567,\"start\":20016},{\"end\":21384,\"start\":20606},{\"end\":21769,\"start\":21386},{\"end\":22427,\"start\":21821},{\"end\":22865,\"start\":22429},{\"end\":23463,\"start\":22867},{\"end\":23954,\"start\":23560},{\"end\":24572,\"start\":24011},{\"end\":25333,\"start\":24574},{\"end\":25579,\"start\":25375},{\"end\":26036,\"start\":25581},{\"end\":26265,\"start\":26143},{\"end\":26535,\"start\":26346},{\"end\":26837,\"start\":26550},{\"end\":27589,\"start\":26860},{\"end\":28229,\"start\":27614},{\"end\":29161,\"start\":28231},{\"end\":29355,\"start\":29163},{\"end\":29846,\"start\":29357},{\"end\":30860,\"start\":29848},{\"end\":31025,\"start\":30875},{\"end\":31892,\"start\":31027},{\"end\":32597,\"start\":31894},{\"end\":33028,\"start\":32599},{\"end\":33758,\"start\":33030},{\"end\":34184,\"start\":33760},{\"end\":35603,\"start\":34199}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":18325,\"start\":18231},{\"attributes\":{\"id\":\"formula_1\"},\"end\":21820,\"start\":21770},{\"attributes\":{\"id\":\"formula_2\"},\"end\":23559,\"start\":23464},{\"attributes\":{\"id\":\"formula_3\"},\"end\":26142,\"start\":26037},{\"attributes\":{\"id\":\"formula_4\"},\"end\":26345,\"start\":26266}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":28061,\"start\":28054},{\"end\":28077,\"start\":28062},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":29354,\"start\":29347}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2390,\"start\":2378},{\"attributes\":{\"n\":\"2\"},\"end\":10638,\"start\":10626},{\"attributes\":{\"n\":\"3\"},\"end\":14267,\"start\":14237},{\"attributes\":{\"n\":\"4\"},\"end\":17967,\"start\":17917},{\"attributes\":{\"n\":\"4.1\"},\"end\":18139,\"start\":18121},{\"attributes\":{\"n\":\"4.2\"},\"end\":20014,\"start\":19975},{\"attributes\":{\"n\":\"4.2.1\"},\"end\":20604,\"start\":20570},{\"attributes\":{\"n\":\"4.2.2\"},\"end\":24009,\"start\":23957},{\"attributes\":{\"n\":\"4.2.3\"},\"end\":25373,\"start\":25336},{\"attributes\":{\"n\":\"5\"},\"end\":26548,\"start\":26538},{\"attributes\":{\"n\":\"5.1\"},\"end\":26858,\"start\":26840},{\"attributes\":{\"n\":\"5.2\"},\"end\":27612,\"start\":27592},{\"attributes\":{\"n\":\"6\"},\"end\":30873,\"start\":30863},{\"attributes\":{\"n\":\"7\"},\"end\":34197,\"start\":34187},{\"end\":35615,\"start\":35605},{\"end\":36113,\"start\":36093},{\"end\":36727,\"start\":36717},{\"end\":36842,\"start\":36832},{\"end\":37010,\"start\":37000},{\"end\":37167,\"start\":37158},{\"end\":37721,\"start\":37712}]", "table": "[{\"end\":37595,\"start\":37169},{\"end\":37710,\"start\":37599},{\"end\":37988,\"start\":37723}]", "figure_caption": "[{\"end\":36091,\"start\":35617},{\"end\":36549,\"start\":36116},{\"end\":36715,\"start\":36552},{\"end\":36830,\"start\":36729},{\"end\":36998,\"start\":36844},{\"end\":37156,\"start\":37012},{\"end\":37599,\"start\":37598}]", "figure_ref": "[{\"end\":7110,\"start\":7102},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15981,\"start\":15973},{\"end\":18575,\"start\":18567},{\"end\":19107,\"start\":19099},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":20780,\"start\":20772},{\"end\":25488,\"start\":25480},{\"end\":26836,\"start\":26828},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":29098,\"start\":29090},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":30174,\"start\":30166},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":30333,\"start\":30325}]", "bib_author_first_name": "[{\"end\":38555,\"start\":38554},{\"end\":38566,\"start\":38565},{\"end\":38727,\"start\":38726},{\"end\":38729,\"start\":38728},{\"end\":38738,\"start\":38737},{\"end\":38746,\"start\":38745},{\"end\":38753,\"start\":38752},{\"end\":38762,\"start\":38761},{\"end\":38994,\"start\":38993},{\"end\":39005,\"start\":39004},{\"end\":39276,\"start\":39275},{\"end\":39284,\"start\":39283},{\"end\":39297,\"start\":39296},{\"end\":39307,\"start\":39306},{\"end\":39309,\"start\":39308},{\"end\":39572,\"start\":39571},{\"end\":39583,\"start\":39582},{\"end\":39594,\"start\":39593},{\"end\":39607,\"start\":39606},{\"end\":39616,\"start\":39615},{\"end\":39622,\"start\":39621},{\"end\":39633,\"start\":39632},{\"end\":39644,\"start\":39643},{\"end\":39829,\"start\":39828},{\"end\":39845,\"start\":39840},{\"end\":40085,\"start\":40084},{\"end\":40087,\"start\":40086},{\"end\":40101,\"start\":40100},{\"end\":40111,\"start\":40110},{\"end\":40380,\"start\":40379},{\"end\":40387,\"start\":40386},{\"end\":40398,\"start\":40397},{\"end\":40600,\"start\":40599},{\"end\":40611,\"start\":40610},{\"end\":40625,\"start\":40624},{\"end\":40794,\"start\":40793},{\"end\":40796,\"start\":40795},{\"end\":40809,\"start\":40808},{\"end\":40811,\"start\":40810},{\"end\":40819,\"start\":40818},{\"end\":40830,\"start\":40829},{\"end\":40839,\"start\":40838},{\"end\":40847,\"start\":40846},{\"end\":40856,\"start\":40855},{\"end\":40866,\"start\":40865},{\"end\":41193,\"start\":41192},{\"end\":41200,\"start\":41199},{\"end\":41212,\"start\":41211},{\"end\":41221,\"start\":41220},{\"end\":41232,\"start\":41231},{\"end\":41243,\"start\":41239},{\"end\":41249,\"start\":41248},{\"end\":41251,\"start\":41250},{\"end\":41592,\"start\":41591},{\"end\":41598,\"start\":41597},{\"end\":41607,\"start\":41606},{\"end\":41616,\"start\":41615},{\"end\":41618,\"start\":41617},{\"end\":41888,\"start\":41884},{\"end\":41908,\"start\":41907},{\"end\":41917,\"start\":41916},{\"end\":42219,\"start\":42218},{\"end\":42229,\"start\":42228},{\"end\":42241,\"start\":42240},{\"end\":42563,\"start\":42562},{\"end\":42575,\"start\":42574},{\"end\":42587,\"start\":42586},{\"end\":42594,\"start\":42593},{\"end\":42608,\"start\":42607},{\"end\":42610,\"start\":42609},{\"end\":42619,\"start\":42618},{\"end\":42987,\"start\":42986},{\"end\":42997,\"start\":42996},{\"end\":43008,\"start\":43007},{\"end\":43020,\"start\":43019},{\"end\":43425,\"start\":43424},{\"end\":43435,\"start\":43434},{\"end\":43608,\"start\":43607},{\"end\":43618,\"start\":43617},{\"end\":43625,\"start\":43624},{\"end\":43635,\"start\":43634},{\"end\":43637,\"start\":43636},{\"end\":43910,\"start\":43909},{\"end\":43922,\"start\":43921},{\"end\":43931,\"start\":43930},{\"end\":43940,\"start\":43939},{\"end\":43951,\"start\":43950},{\"end\":43962,\"start\":43961},{\"end\":44302,\"start\":44301},{\"end\":44308,\"start\":44303},{\"end\":44673,\"start\":44672},{\"end\":44683,\"start\":44682},{\"end\":44698,\"start\":44697},{\"end\":44707,\"start\":44706},{\"end\":44709,\"start\":44708},{\"end\":45185,\"start\":45184},{\"end\":45196,\"start\":45195},{\"end\":45207,\"start\":45206},{\"end\":45220,\"start\":45219},{\"end\":45229,\"start\":45228},{\"end\":45239,\"start\":45238},{\"end\":45253,\"start\":45252},{\"end\":45593,\"start\":45592},{\"end\":45600,\"start\":45599},{\"end\":45608,\"start\":45607},{\"end\":45617,\"start\":45616},{\"end\":45625,\"start\":45624},{\"end\":45632,\"start\":45631},{\"end\":45634,\"start\":45633}]", "bib_author_last_name": "[{\"end\":38563,\"start\":38556},{\"end\":38576,\"start\":38567},{\"end\":38735,\"start\":38730},{\"end\":38743,\"start\":38739},{\"end\":38750,\"start\":38747},{\"end\":38759,\"start\":38754},{\"end\":38769,\"start\":38763},{\"end\":39002,\"start\":38995},{\"end\":39012,\"start\":39006},{\"end\":39281,\"start\":39277},{\"end\":39294,\"start\":39285},{\"end\":39304,\"start\":39298},{\"end\":39314,\"start\":39310},{\"end\":39328,\"start\":39316},{\"end\":39580,\"start\":39573},{\"end\":39591,\"start\":39584},{\"end\":39604,\"start\":39595},{\"end\":39613,\"start\":39608},{\"end\":39619,\"start\":39617},{\"end\":39630,\"start\":39623},{\"end\":39641,\"start\":39634},{\"end\":39649,\"start\":39645},{\"end\":39838,\"start\":39830},{\"end\":39850,\"start\":39846},{\"end\":40098,\"start\":40088},{\"end\":40108,\"start\":40102},{\"end\":40119,\"start\":40112},{\"end\":40384,\"start\":40381},{\"end\":40395,\"start\":40388},{\"end\":40403,\"start\":40399},{\"end\":40608,\"start\":40601},{\"end\":40622,\"start\":40612},{\"end\":40632,\"start\":40626},{\"end\":40806,\"start\":40797},{\"end\":40816,\"start\":40812},{\"end\":40827,\"start\":40820},{\"end\":40836,\"start\":40831},{\"end\":40844,\"start\":40840},{\"end\":40853,\"start\":40848},{\"end\":40863,\"start\":40857},{\"end\":40875,\"start\":40867},{\"end\":41197,\"start\":41194},{\"end\":41209,\"start\":41201},{\"end\":41218,\"start\":41213},{\"end\":41229,\"start\":41222},{\"end\":41237,\"start\":41233},{\"end\":41246,\"start\":41244},{\"end\":41256,\"start\":41252},{\"end\":41261,\"start\":41258},{\"end\":41595,\"start\":41593},{\"end\":41604,\"start\":41599},{\"end\":41613,\"start\":41608},{\"end\":41626,\"start\":41619},{\"end\":41905,\"start\":41889},{\"end\":41914,\"start\":41909},{\"end\":41926,\"start\":41918},{\"end\":42226,\"start\":42220},{\"end\":42238,\"start\":42230},{\"end\":42250,\"start\":42242},{\"end\":42572,\"start\":42564},{\"end\":42584,\"start\":42576},{\"end\":42591,\"start\":42588},{\"end\":42605,\"start\":42595},{\"end\":42616,\"start\":42611},{\"end\":42625,\"start\":42620},{\"end\":42994,\"start\":42988},{\"end\":43005,\"start\":42998},{\"end\":43017,\"start\":43009},{\"end\":43028,\"start\":43021},{\"end\":43432,\"start\":43426},{\"end\":43443,\"start\":43436},{\"end\":43615,\"start\":43609},{\"end\":43622,\"start\":43619},{\"end\":43632,\"start\":43626},{\"end\":43643,\"start\":43638},{\"end\":43919,\"start\":43911},{\"end\":43928,\"start\":43923},{\"end\":43937,\"start\":43932},{\"end\":43948,\"start\":43941},{\"end\":43959,\"start\":43952},{\"end\":43968,\"start\":43963},{\"end\":44299,\"start\":44287},{\"end\":44311,\"start\":44309},{\"end\":44325,\"start\":44313},{\"end\":44680,\"start\":44674},{\"end\":44695,\"start\":44684},{\"end\":44704,\"start\":44699},{\"end\":44716,\"start\":44710},{\"end\":45193,\"start\":45186},{\"end\":45204,\"start\":45197},{\"end\":45217,\"start\":45208},{\"end\":45226,\"start\":45221},{\"end\":45236,\"start\":45230},{\"end\":45250,\"start\":45240},{\"end\":45260,\"start\":45254},{\"end\":45597,\"start\":45594},{\"end\":45605,\"start\":45601},{\"end\":45614,\"start\":45609},{\"end\":45622,\"start\":45618},{\"end\":45629,\"start\":45626},{\"end\":45641,\"start\":45635}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1707.07397\",\"id\":\"b0\"},\"end\":38722,\"start\":38512},{\"attributes\":{\"doi\":\"arXiv:1712.09665\",\"id\":\"b1\"},\"end\":38937,\"start\":38724},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":2893830},\"end\":39205,\"start\":38939},{\"attributes\":{\"doi\":\"CoRR abs/1804.05810\",\"id\":\"b3\"},\"end\":39511,\"start\":39207},{\"attributes\":{\"id\":\"b4\"},\"end\":39826,\"start\":39513},{\"attributes\":{\"id\":\"b5\"},\"end\":40082,\"start\":39828},{\"attributes\":{\"doi\":\"arXiv:1412.6572\",\"id\":\"b6\"},\"end\":40333,\"start\":40084},{\"attributes\":{\"doi\":\"arXiv:1702.06832\",\"id\":\"b7\"},\"end\":40553,\"start\":40335},{\"attributes\":{\"doi\":\"arXiv:1607.02533\",\"id\":\"b8\"},\"end\":40791,\"start\":40555},{\"attributes\":{\"doi\":\"arXiv:1509.02971\",\"id\":\"b9\"},\"end\":41159,\"start\":40793},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":215831388},\"end\":41501,\"start\":41161},{\"attributes\":{\"doi\":\"CoRR abs/1707.03501\",\"id\":\"b11\"},\"end\":41813,\"start\":41503},{\"attributes\":{\"doi\":\"arXiv:1511.04599\",\"id\":\"b12\"},\"end\":42121,\"start\":41815},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":206592585},\"end\":42502,\"start\":42123},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":7004303},\"end\":42838,\"start\":42504},{\"attributes\":{\"id\":\"b15\"},\"end\":42927,\"start\":42840},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":206594738},\"end\":43386,\"start\":42929},{\"attributes\":{\"doi\":\"CoRR abs/1612.08242\",\"id\":\"b17\"},\"end\":43555,\"start\":43388},{\"attributes\":{\"doi\":\"arXiv:1511.05122\",\"id\":\"b18\"},\"end\":43816,\"start\":43557},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":4071727},\"end\":44285,\"start\":43818},{\"attributes\":{\"doi\":\"arXiv:1506.01497\",\"id\":\"b20\"},\"end\":44582,\"start\":44287},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":207241700},\"end\":45140,\"start\":44584},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":604334},\"end\":45521,\"start\":45142},{\"attributes\":{\"doi\":\"CoRR abs/1703.08603\",\"id\":\"b23\"},\"end\":45825,\"start\":45523}]", "bib_title": "[{\"end\":38991,\"start\":38939},{\"end\":41190,\"start\":41161},{\"end\":42216,\"start\":42123},{\"end\":42560,\"start\":42504},{\"end\":42984,\"start\":42929},{\"end\":43907,\"start\":43818},{\"end\":44670,\"start\":44584},{\"end\":45182,\"start\":45142}]", "bib_author": "[{\"end\":38565,\"start\":38554},{\"end\":38578,\"start\":38565},{\"end\":38737,\"start\":38726},{\"end\":38745,\"start\":38737},{\"end\":38752,\"start\":38745},{\"end\":38761,\"start\":38752},{\"end\":38771,\"start\":38761},{\"end\":39004,\"start\":38993},{\"end\":39014,\"start\":39004},{\"end\":39283,\"start\":39275},{\"end\":39296,\"start\":39283},{\"end\":39306,\"start\":39296},{\"end\":39316,\"start\":39306},{\"end\":39330,\"start\":39316},{\"end\":39582,\"start\":39571},{\"end\":39593,\"start\":39582},{\"end\":39606,\"start\":39593},{\"end\":39615,\"start\":39606},{\"end\":39621,\"start\":39615},{\"end\":39632,\"start\":39621},{\"end\":39643,\"start\":39632},{\"end\":39651,\"start\":39643},{\"end\":39840,\"start\":39828},{\"end\":39852,\"start\":39840},{\"end\":40100,\"start\":40084},{\"end\":40110,\"start\":40100},{\"end\":40121,\"start\":40110},{\"end\":40386,\"start\":40379},{\"end\":40397,\"start\":40386},{\"end\":40405,\"start\":40397},{\"end\":40610,\"start\":40599},{\"end\":40624,\"start\":40610},{\"end\":40634,\"start\":40624},{\"end\":40808,\"start\":40793},{\"end\":40818,\"start\":40808},{\"end\":40829,\"start\":40818},{\"end\":40838,\"start\":40829},{\"end\":40846,\"start\":40838},{\"end\":40855,\"start\":40846},{\"end\":40865,\"start\":40855},{\"end\":40877,\"start\":40865},{\"end\":41199,\"start\":41192},{\"end\":41211,\"start\":41199},{\"end\":41220,\"start\":41211},{\"end\":41231,\"start\":41220},{\"end\":41239,\"start\":41231},{\"end\":41248,\"start\":41239},{\"end\":41258,\"start\":41248},{\"end\":41263,\"start\":41258},{\"end\":41597,\"start\":41591},{\"end\":41606,\"start\":41597},{\"end\":41615,\"start\":41606},{\"end\":41628,\"start\":41615},{\"end\":41907,\"start\":41884},{\"end\":41916,\"start\":41907},{\"end\":41928,\"start\":41916},{\"end\":42228,\"start\":42218},{\"end\":42240,\"start\":42228},{\"end\":42252,\"start\":42240},{\"end\":42574,\"start\":42562},{\"end\":42586,\"start\":42574},{\"end\":42593,\"start\":42586},{\"end\":42607,\"start\":42593},{\"end\":42618,\"start\":42607},{\"end\":42627,\"start\":42618},{\"end\":42996,\"start\":42986},{\"end\":43007,\"start\":42996},{\"end\":43019,\"start\":43007},{\"end\":43030,\"start\":43019},{\"end\":43434,\"start\":43424},{\"end\":43445,\"start\":43434},{\"end\":43617,\"start\":43607},{\"end\":43624,\"start\":43617},{\"end\":43634,\"start\":43624},{\"end\":43645,\"start\":43634},{\"end\":43921,\"start\":43909},{\"end\":43930,\"start\":43921},{\"end\":43939,\"start\":43930},{\"end\":43950,\"start\":43939},{\"end\":43961,\"start\":43950},{\"end\":43970,\"start\":43961},{\"end\":44301,\"start\":44287},{\"end\":44313,\"start\":44301},{\"end\":44327,\"start\":44313},{\"end\":44682,\"start\":44672},{\"end\":44697,\"start\":44682},{\"end\":44706,\"start\":44697},{\"end\":44718,\"start\":44706},{\"end\":45195,\"start\":45184},{\"end\":45206,\"start\":45195},{\"end\":45219,\"start\":45206},{\"end\":45228,\"start\":45219},{\"end\":45238,\"start\":45228},{\"end\":45252,\"start\":45238},{\"end\":45262,\"start\":45252},{\"end\":45599,\"start\":45592},{\"end\":45607,\"start\":45599},{\"end\":45616,\"start\":45607},{\"end\":45624,\"start\":45616},{\"end\":45631,\"start\":45624},{\"end\":45643,\"start\":45631}]", "bib_venue": "[{\"end\":39977,\"start\":39923},{\"end\":43171,\"start\":43109},{\"end\":44035,\"start\":44030},{\"end\":44875,\"start\":44805},{\"end\":38552,\"start\":38512},{\"end\":39038,\"start\":39014},{\"end\":39273,\"start\":39207},{\"end\":39569,\"start\":39513},{\"end\":39921,\"start\":39852},{\"end\":40182,\"start\":40136},{\"end\":40377,\"start\":40335},{\"end\":40597,\"start\":40555},{\"end\":40944,\"start\":40893},{\"end\":41301,\"start\":41263},{\"end\":41589,\"start\":41503},{\"end\":41882,\"start\":41815},{\"end\":42298,\"start\":42252},{\"end\":42647,\"start\":42627},{\"end\":42868,\"start\":42842},{\"end\":43107,\"start\":43030},{\"end\":43422,\"start\":43388},{\"end\":43605,\"start\":43557},{\"end\":44028,\"start\":43970},{\"end\":44407,\"start\":44343},{\"end\":44803,\"start\":44718},{\"end\":45314,\"start\":45262},{\"end\":45590,\"start\":45523}]"}}}, "year": 2023, "month": 12, "day": 17}