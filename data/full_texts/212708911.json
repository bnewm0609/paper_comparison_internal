{"id": 212708911, "updated": "2023-04-27 13:35:41.259", "metadata": {"title": "DDcGAN: A Dual-Discriminator Conditional Generative Adversarial Network for Multi-Resolution Image Fusion", "authors": "[{\"first\":\"Jiayi\",\"last\":\"Ma\",\"middle\":[]},{\"first\":\"Han\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Junjun\",\"last\":\"Jiang\",\"middle\":[]},{\"first\":\"Xiaoguang\",\"last\":\"Mei\",\"middle\":[]},{\"first\":\"Xiao-Ping\",\"last\":\"Zhang\",\"middle\":[]}]", "venue": "IEEE Transactions on Image Processing", "journal": "IEEE Transactions on Image Processing", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "In this paper, we proposed a new end-to-end model, termed as dual-discriminator conditional generative adversarial network (DDcGAN), for fusing infrared and visible images of different resolutions. Our method establishes an adversarial game between a generator and two discriminators. The generator aims to generate a real-like fused image based on a specifically designed content loss to fool the two discriminators, while the two discriminators aim to distinguish the structure differences between the fused image and two source images, respectively, in addition to the content loss. Consequently, the fused image is forced to simultaneously keep the thermal radiation in the infrared image and the texture details in the visible image. Moreover, to fuse source images of different resolutions, e.g., a low-resolution infrared image and a high-resolution visible image, our DDcGAN constrains the downsampled fused image to have similar property with the infrared image. This can avoid causing thermal radiation information blurring or visible texture detail loss, which typically happens in traditional methods. In addition, we also apply our DDcGAN to fusing multi-modality medical images of different resolutions, e.g., a low-resolution positron emission tomography image and a high-resolution magnetic resonance image. The qualitative and quantitative experiments on publicly available datasets demonstrate the superiority of our DDcGAN over the state-of-the-art, in terms of both visual effect and quantitative metrics. Our code is publicly available at https://github.com/jiayi-ma/DDcGAN.", "fields_of_study": "[\"Medicine\"]", "external_ids": {"arxiv": null, "mag": "3011768656", "acl": null, "pubmed": "32167894", "pubmedcentral": null, "dblp": "journals/tip/MaXJMZ20", "doi": "10.1109/tip.2020.2977573"}}, "content": {"source": {"pdf_hash": "0f89d0aea4b81506fc31538ec861164a3c7e393e", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "8b7b2d474499a5a50bfd53c65d0d2754a669c109", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/0f89d0aea4b81506fc31538ec861164a3c7e393e.txt", "contents": "\nDDcGAN: A Dual-Discriminator Conditional Generative Adversarial Network for Multi-Resolution Image Fusion\n\n\nJiayi Ma \nHan Xu \nJunjun Jiang \nXiaoguang Mei \nSenior Member, IEEEXiao-Ping Zhang \nDDcGAN: A Dual-Discriminator Conditional Generative Adversarial Network for Multi-Resolution Image Fusion\n\nIEEE TRANSACTIONS ON IMAGE PROCESSING\n29202010.1109/TIP.2020.29775734980\nIn this paper, we proposed a new end-to-end model, termed as dual-discriminator conditional generative adversarial network (DDcGAN), for fusing infrared and visible images of different resolutions. Our method establishes an adversarial game between a generator and two discriminators. The generator aims to generate a real-like fused image based on a specifically designed content loss to fool the two discriminators, while the two discriminators aim to distinguish the structure differences between the fused image and two source images, respectively, in addition to the content loss. Consequently, the fused image is forced to simultaneously keep the thermal radiation in the infrared image and the texture details in the visible image. Moreover, to fuse source images of different resolutions, e.g., a lowresolution infrared image and a high-resolution visible image, our DDcGAN constrains the downsampled fused image to have similar property with the infrared image. This can avoid causing thermal radiation information blurring or visible texture detail loss, which typically happens in traditional methods. In addition, we also apply our DDcGAN to fusing multi-modality medical images of different resolutions, e.g., a low-resolution positron emission tomography image and a high-resolution magnetic resonance image. The qualitative and quantitative experiments on publicly available datasets demonstrate the superiority of our DDcGAN over the state-of-the-art, in terms of both visual effect and quantitative metrics. Our code is publicly available at https://github.com/jiayi-ma/DDcGAN. Index Terms-Image fusion, generative adversarial network, infrared image, medical image, different resolutions.1057-7149\n\nI. INTRODUCTION\n\nI NFRARED and visible image fusion has been gaining in popularity in image signal processing due to its extensive applications in many fields such as computer vision, remote Manuscript  sensing, medical imaging, and military detection [1], [2]. Among these sensors, infrared and visible sensors are probably the most widely used type of sensors with wavelengths of 8-14 \u03bcm [3] and 300-530 nm [4] respectively. The uniqueness of the combining infrared and visible sensors lies in the fact that visible sensors capture reflected light to represent abundant texture details whereas infrared sensors map captured thermal radiation to gray images and can highlight thermal targets even in poor lighting conditions or under the circumstances of severe occlusion. Due to the strong complementarity between them, the fused result has the potential to present nearly all the inherent properties of the target to improve visual understanding [5]. Therefore, their fusion plays an important role in military and civilian applications [6], [7]. For multi-modality source images, the key of image fusion is to extract the most important feature information in source images taken from different imaging apparatus and merge it into a single fused image [8]. Therefore, the fused image can provide more complex and detailed scene representation while reducing redundant information. For this purpose, many fusion methods have been proposed in the past decades. According to corresponding schemes, these fusion methods can be divided into different categories, including multi-scale transform-based methods [9], [10], sparse representation-based methods [11], [12], neural network-based methods [13], subspace-based methods [14], saliency-based methods [15], hybrid methods [16], and other fusion methods [17], [18]. These methods are dedicated to design feature extraction and fusion rules in a manual way for better fusion performance. However, detailed and diverse feature extraction and fusion rule design make the fusion method more and more complex.\n\nSince much attention has been drawn to deep learning recently, some deep learning-based fusion methods have been proposed. The detailed exposition of deep learning-based fusion methods will be discussed later in Sec. II-A. Although these works have achieved promising performance, there are still some drawbacks: (i) The deep learning framework is only applied in some part of the fusion process, e.g., to extract features, while the overall fusion process is still in traditional frameworks [19], [20]. (ii) Faced with the lack of ground-truth, the solutions by merely designing loss functions are incomprehensive and inappropriate. (iii) The fusion rules designed in a manual way enforce the extraction of same features even if source images are multi-modality data. (iv) In existing fusion methods based on traditional generative adversarial network (GAN) [21], [22], the fused image is trained to be similar to only one of the source images, leading to the loss of some information contained in the other source image.\n\nFurthermore, due to the limitations of hardware and environments, the infrared images always suffer from low resolution and blurred details compared with corresponding visible images, and it is hard to improve the resolution of infrared images by upgrading hardware devices. For fusing multi-resolution infrared and visible images (e.g., images of different resolutions), the strategy of downsampling visible images or upsampling infrared images before fusion will inevitably causes thermal radiation information blurring or visible texture detail loss. Therefore, it remains a challenging task to fuse multi-resolution infrared and visible images without loss of important information.\n\nTo address the above challenges, in this work, we propose a fusion method via dual-discriminator conditional generative adversarial network (DDcGAN). The problem is formulated as a particular adversarial process of two kinds of neural networks, i.e., a generator and two discriminators, based on conditional GAN [23]. We adapt the architecture to dual-discriminators and the discriminators are pulling each other on the distribution of the generated data obtained by the generator, so that the fused image simultaneously keeps the most important feature information in infrared and visible images. We utilize source images as the real data and the fused image should be indistinguishable with both types of real images, and hence the ground-truth fused image is not required in our model. The entire network is an end-to-end model without the requirement of designing fusion rules. Moreover, our model can be generalized to fuse source images of different resolutions. In particular, we constrain the downsampled fused image to have similar properties with the infrared image, and utilize trainable deconvolution layers to learn a mapping between different resolutions. Last but not least, our proposed method can also be generalized to solve the medical image fusion problem, e.g., positron emission tomography (PET) and magnetic resonance image (MRI) fusion, which can preserve the functional information and the anatomical information to a great extent in the fused image. Extensive results have revealed the advantages of our DDcGAN compared to other methods.\n\nThe major contributions of our work include the following four aspects. Firstly, our proposed method has contributed to applying a deep learning framework based on minmax two-player game to the overall fusion process of multi-modality images rather than just some sub parts of them. Secondly, the dual-discriminators architecture enables the generator to be more adequately trained to meet stricter requirements and avoid information loss caused by the introduction of discriminator on only one type of source images. Thirdly, in virtue of the utilization of trainable deconvolution layers and content constraints on downsampled fused images, our proposed method demonstrates better performances for multi-resolution source image fusion. Lastly, our method can also be extended to the fusion of medical images such as MRI and PET image fusion and achieves advantageous performances.\n\nA preliminary version of this manuscript has appeared in [24]. The primary new contributions include the following five aspects. First, the generator network architecture is optimized, where we replace the U-net with the densely connected convolutional network. In virtue of the dense connections, the network architecture can strength the transmission of feature maps and make use of them more effectively. Without the loss caused by the large stride and the blur caused by the upsampling operations, the information in source images is preserved to a greater extent for clearer fusion performance. Second, the input of the discriminator D v is no longer the gradients of image to be distinguished but the image itself. By expanding the probability space from the subspace of source images to the whole images, the fused images can have more similar properties with source images. When the network tries to minimize the divergence of different probability distributions in the subspace, it will introduce some additional noise into the source images. By expanding the probability space, the influence can be mitigated. Third, as for the input of the generator, i.e., different-resolution source images, instead of upsampling the low-resolution source image with two upsamping layers, we employ a deconvolution layer to learn a mapping from low to high resolution. The difference is that the parameters in this layer are obtained during the training phase rather than pre-defined. And the high-resolution source image is fed into another deconvolution layer to generate same-resolution feature maps. Fourth, we add more detailed analysis experiments related to the generator and two discriminators to verify the effects of their subparts. Last, we apply the proposed method to fuse different-resolution multi-modal medical images, i.e., low-resolution PET images and high-resolution MRI images, and compare our fused results with state-of-the-art methods qualitatively and quantitatively.\n\nThe remainder of this paper is organized as follows. Section II describes some related work, including an overview of existing deep learning-based fusion methods and a theoretical introduction of GANs. Section III provides the problem formulation, loss functions and network architecture design. In Section IV, our proposed method is generalized to fuse medical images. In Section V, we compare our method with several state-of-the-art methods on publicly available datasets by qualitative and quantitative comparisons both for infrared and visible image fusion and PET and MRI image fusion. The experiments of discriminator analysis are also conducted in this section. Conclusions are given in Section VI.\n\n\nII. RELATED WORK\n\nIn this section, we give a brief introduction of the existing deep learning-based image fusion methods. In addition, since our method is based on the GANs, we also provide a brief explanation of its basic theory and an improved network, namely conditional GAN.\n\n\nA. Deep Learning-Based Fusion Methods\n\nSince the study based on deep learning has become an active topic in the field of image fusion in the last three years [25], many deep learning-based fusion methods have been proposed and gradually formed a critical branch. In some methods, the deep learning framework is applied to extract image features in an end-to-end manner for reconstruction. Representatively, Liu et al. [19] applied the convolutional sparse representation (CSR) for image fusion, which is employed to extract multi-layer features and these features are used to generate the fused image. In [26], Liu et al. proposed a medical image fusion method based on convolutional neural networks (CNNs). The convolutional network is merely adopted to generate a weight map which integrates the pixel activity information and the overall fusion process is still conducted in a multi-scale mannar via image pyramids in a traditional way. In [20], Li et al. decomposed the source images into base parts and detail content. The deep learning framework is used to extract multi-layer features in the detail content while the base parts are fused by weighted-averaging. Then, the two parts are combined for reconstruction.\n\nIn other methods, the deep learning framework is used not only for feature extraction but also for reconstruction. For instance, based on a three-layer architecture for superresolution, Masi et al. [27] proposed a convolutional neural network for projection, mapping, and reconstruction to solve pansharpening problem. Prabhakar et al. [28] proposed an unsupervised deep learning framework for multi-exposure fusion. They utilized a novel CNN architecture and designed a no-reference quality metric as the loss function. As weights are tied, the pre-fusion layers are forced to learn the same features and these features are added for fusion. On this basis, Li et al. [29] improved the architecture by introducing dense block. In the fusion layer, salient feature maps are combined by two manually designed fusion strategies (addition and 1 -norm). Similarly, it utilizes no-reference metrics (the structural similarity index measure and the Euclidean distance) as the loss function for unsupervised learning. In our previous work [21], we proposed the FusionGAN to fuse infrared and visible images using a generative adversarial network. The fused image generated by the generator is forced to have more details existing in the visible image by applying the discriminator to distinguish differences between them. When fusing source images with different resolutions, the lowresolution infrared images are simply interpolated before fed into the generator.\n\nAlthough the abovementioned works have achieved promising performance, there are still some drawbacks in existing deep learning-based fusion methods. (i) Existing methods typically perform neural network in feature extraction and reconstruction while fusion rules are still designed in a manual way. Thus, the entire method can not get rid of the limitations of traditional fusion methods. (ii) The major stumbling block in utilizing deep learning for infrared and visible image fusion is the lack of ground-truth fused image for supervised learning. Existing methods solve it by designing loss function to penalize differences between output and target in some aspects. However, these metrics will introduce new problems while penalizing certain aspects. For instance, the Euclidean distance suffers from relatively blurred results by averaging all plausible outputs [30]. Therefore, it remains to be difficult to design a comprehensive, appropriate and adaptive loss function to specify a high-level goal. (iii) Most artificially designed fusion rules lead to the extraction of same features for different types of source images, regardless of the fact that source images are manifestations of different phenomena and it is inappropriate for multi-source image fusion. (iv) Existing GAN-based fusion method merely applies GAN to force the fused image to obtain more details in visible images while the thermal radiation in infrared images is only obtained through the content loss. As the adversarial game proceeds, the fused image is more similar to the visible image and the prominence of thermal targets is gradually reduced.\n\nTo address the problems, we solve the fusion problem by applying GAN and adapt it with dual discriminators. On this basis, we introduce the deconvolution layers to adapt to the fusion of source images of different resolutions. In addition, for the stability of the training process, we optimize the network architecture and the training strategy.\n\n\nB. Generative Adversarial Networks\n\nGenerative adversarial networks is one of the generative models. If samples are drawn from the real distribution P data (x), the generative model is designed to learn a probability distribution P model (x; \u03b8 ) parameterized by \u03b8 as an\nestimation of P data (x) from samples {x 1 , x 2 , \u00b7 \u00b7 \u00b7 , x m }, where P model (x; \u03b8 )\nare Gaussian mixture models. Likelihood of generating the samples is defined as follows:\nL = m i=1 P model x i ; \u03b8 .(1)\nThen we can perform maximum likelihood estimation [31]:\n\u03b8 * = arg max \u03b8 m i=1 logP model x i ; \u03b8 .(2)\nIt can be thought of as minimizing the Kullback-Liebler divergence between P data (x) and P model (x; \u03b8 ). However, if P model is a much more complicated probability distribution, it will be quite difficult to calculate its likelihood function to perform maximum likelihood estimation. To deal with it, GANs estimate generative models via an adversarial process by simultaneously training two models: a generative model G and a discriminator model D [32].\n\nThe generator G is a network that can capture the data distribution and generate new samples. If we input the noise z sampled from the latent space, it generates a sample x = G (z). In virtue of neural networks, the probability distribution P G (x) formed by generated samples has the ability to be much more complicated. The training objective of G is to make P G (x) and P data (x) as close as possible and the optimization formulation can be defined as:\nG * = arg min G Di v (P G (x) , P data (x)) ,(3)\nwhere Di v(\u00b7) denotes the divergence between two distributions. However, it is difficult to calculate the divergence because the formulations of P G and P data are unknown. Ingeniously, the discriminator D can be used to solve this problem for it estimates the probability that a sample comes from the training data rather than G. The objective function for D can be formulated as:\nD * = arg max D V (G, D) ,(4)\nwhere V (G, D) is defined as follows:\nV (G, D) = E x\u223cP data logD (x) + E x\u223cP G log (1 \u2212 D (x)) .(5)\nA large objective value means that the Jensen-Shannon (JS) divergence of P G and P data is large and they are easy to discriminate. Thus, the optimization formulation of G can be converted to:\nG * = arg min G max D V (G, D) ,(6)\nwhere the discriminator D is fixed when we are training G.\n\nThe adversarial process of G and D makes up the two-player min-max game where G tries to fool D while D is trained to discriminate the generated data. Hence, the generated samples are getting more and more indistinguishable from the real data. GANs can be extended to a conditional model if both the generator and discriminator are conditioned on some extra information which could be any kind of auxiliary information. We can perform the conditioning by feeding the extra information as additional input layer and this model is defined as conditional generative adversarial networks [23].\n\n\nIII. PROPOSED METHOD\n\nIn this section, with analysis of the characteristics of infrared and visible images, we provide our fusion formulation, the definition and design of loss functions. At the end of this section, the design of network architecture is shown concretely.\n\n\nA. Problem Formulation\n\nWe formulate the fusion problem as a conditional GAN model by constructing a dual-discriminator conditional GAN. To fuse images of different resolutions, without loss of generality, we make an assumption that the resolution of the visible image v is 4 \u00d7 4 times that of the infrared image i .\n\nThe entire procedure of our proposed DDcGAN is shown in Fig. 1. Given a visible image v and an infrared image i , our ultimate goal is to learn a generator G conditioned on them and the generated image G (v, i ) is encouraged to be realistic and informative enough to fool the discriminators. Simultaneously, we exploit two adversarial discriminators D v and D i , and they respectively generate a scalar that estimates the probability of the input from real data rather than G. Specifically, D v aims to distinguish the generated image from the visible image, while D i is trained to discriminate between the original low-resolution infrared image and down-sampled generated/fused image. Average-pooling is employed here for downsampling due to its retention of low-frequency information compared with max-pooling and the thermal radiation information is mainly presented in this form. Put slightly differently, for the sake of the balance between the generator and discriminators, except for the input of discriminators, we do not feed the source images v and i as additional/conditional information to D v and D i . That is, the input layer of each discriminator is a single-channel layer containing the sampled data rather than a two-channel layer containing both the sampled data and the corresponding source image as the conditional information. Because when the condition and the sample to be discriminated are the same, the discrimination task is simplified to judge whether the input images are the same and it is a simple enough task for neural networks. When the generator is unable to fool the discriminator, the adversarial relationship will fail to be established and the generator will tend to generate randomly. Consequently, the model will lose its original meaning.\n\nWe denote the downsampling operator as \u03c8, which is implemented by two average pooling layers due to its retention of low frequency information. Both layers summarize a 3 \u00d7 3 neighborhood and use a stride of 2. Accordingly, the training target of G can be formulated as minimizing the following adversarial objective\nmin G max D v ,D i E log D v (v) + E log (1 \u2212 D v (G (v, i ))) +E log D i (i ) + E log (1 \u2212 D i (\u03c8 G (v, i ))) . (7)\nConversely, the goal of discriminators is to maximize Eq. (7).\n\nThrough the adversarial process of the generator G and two discriminators (D v and D i ), the divergence between P G and two real distributions, i.e., P V and P I , will become smaller simultaneously, where P G is the probability distribution of the generated samples, P V is the real distribution of the visible images and P I is that of the infrared images.\n\n\nB. Loss Function\n\nInitially, the success of GANs was limited as they were known to be unstable to train and may result in artifacts and noisy or incomprehensible results [33]. A possible solution to solve the problem of artifacts and incomprehensible results is to introduce a content loss to include a set of constraints into the networks. Thus, in this paper, the generator is not only trained to fool discriminators but also tasked to constraint similarity between the generated image and source images in the content. Therefore, the loss function of the generator is composed by an adversarial loss L adv G and a content loss L con , with a weight \u03bb controlling the trade-off:\nL G = L adv G + \u03bbL con ,(8)\nwhere L adv G comes from the discriminators and is defined as:\nL adv G = E log (1 \u2212 D v (G (v, i ))) +E log (1 \u2212 D i (\u03c8 G (v, i ))) . (9)\nAs the thermal radiation and texture details are mainly characterized by pixel intensities and gradient variation [17], respectively, we employ the Frobenius norm to constrain the downsampled fused image to have similar pixel intensities with the infrared image as the data fidelity term. By constraining the relationship of pixel intensities of downsampled fused image and the low-resolution infrared image, we can considerably prevent loss of texture information caused by compression or blur and inaccuracy due to forced upsampling. According to the aforementioned constraint, the thermal target remains prominent in the fused image. The TV norm [34] is applied in the regularization term to constrain the fused image to exhibit similar gradient variation with the visible image. Compared with the 0 norm, the TV norm is able to solve the non-deterministic polynomial-time hard problem effectively. With a weight \u03b7 to tradeoff the differences of pixel intensities and gradient variation, we can obtain the content loss:\nL con = E \u03c8 G (v, i ) \u2212 i 2 F + \u03b7 G (v, i ) \u2212 v T V . (10)\nThe discriminators in DDcGAN, i.e., D v and D i , play a role of discriminating between source images and the generated fused image. The adversarial losses of discriminators can calculate the JS divergence between distributions and thus identify whether the intensity or texture information is unrealistic and thus encourage matching the realistic distribution. The adversarial losses are defined as follows:\nL D v = E \u2212logD v (v) +E \u2212log (1 \u2212 D v (G (v, i ))) , (11) L D i = E \u2212logD i (i ) + E \u2212log (1\u2212 D i (\u03c8 G (v, i ))) . (12)\nC. Network Architecture 1) Generator Architecture: The generator consists of 2 deconvolution layers, an encoder network and a corresponding decoder network, as presented in Fig. 2. Since the infrared image has a lower resolution, we firstly employ a mapping before encoding. Rather than simple interpolation by the nearest, bilinear or bicubic method, we introduce a deconvolution layer [35] to learn a mapping from low to high resolution. Without defining an upsampling operator, this mapping is different from traditional upsampling and its parameters are obtained automatically by training. The output of the deconvolution layer is a high-resolution feature map rather than an upsampled infrared image. We also pass the visible image through an independent deconvolution layer which generates a feature map with the same resolution. Results obtained by deconvolution layers are concatenated and fed as the input of the encoder. The process of feature extraction and fusion are both performed in the encoder and fused feature maps are produced as the output. These maps are then fed to the decoder for reconstruction and the generated fused image is of the same resolution with the visible image.\n\nThe encoder consists of 5 convolutional layers and each layer can obtain 48 feature maps by 3 \u00d7 3 filters. To mitigate the vanish of gradient, remedy feature loss and reuse previously computed features, DenseNet [36] is applied and short direct connections are built between each layer and all layers in a feed-forward fashion. The decoder is a 5-layer CNN and the setting of each layer is illustrated in Fig. 2. The strides of all convolutional layers are set as 1. To avoid exploding/vanishing gradients and speed up training, batch normalization is applied. ReLU activation function is used to speed up the convergence [37] and avoid gradient sparsity.\n\n2) Discriminator Architecture: Discriminators are designed to play an adversarial role against the generator. In particular, D v and D i aim to distinguish the generated images from the visible and infrared images, respectively. However, these two types of source images are manifestations of different phenomena, thus have considerably different distributions. In other words, there are conflicts in the guidance of D v on G and D i on G. In our network, we should not only consider the adversarial relationship between the generator and discriminators but also take into account the balance of D v and D i . Otherwise, either the strength or weakness of one discriminator will finally lead to the inefficiency of the other as the training proceeds. In our work, the balance is achieved by the design of network architectures and training strategy (as discussed in Sec. V-A).\n\nThe discriminators D v and D i share the same architecture, which is set to be less complicated compared with the generator architecture, as shown in Fig. 3. The stride of all convolutional layers is set as 2. In the last layer, we employ the tanh activation function to generate a scalar that estimates the probability of the input image from source images rather than G.\n\n\nIV. APPLICATION TO MEDICAL IMAGE FUSION\n\nIn this section, we apply our proposed method to fuse medical images such as MRI and PET image fusion. We treat  the PET images shown in pseu-do-color as color images, and DDcGAN is applied for fusing of high-resolution MRI image and low-resolution intensity component of PET image. In the following, we first introduce the background of medical image fusion, and then take the MRI and PET image fusion as an example and provide some implementation details.\n\n\nA. Background\n\nMulti-modal medical images have the advantage of offering diversified features to enhance robustness and accuracy and thus, the fusion of them provides a powerful tool for biomedical research and clinical applications, such as medical diagnostics, monitoring and treatment [38], [39]. These medical imaging can be divided into structural and functional systems [40]. Structure from motion methods [41] are typically used to obtain the structural information in natural image domain. While in medical imaging, X-ray, MRI and Computed Tomography are a typical structural system, which can provide structural and anatomical information with high resolution. The functional system can provide functional and metabolic information, such as PET and Single-Photon Emission Computed Tomography while these images are often accompanied by low resolution. The limited resolution restricts their clinical applications and encourages the fusion of functional and anatomical images.\n\nAccording to the theories applied, existing medical fusion methods can be summarized into different categories, such as substitution methods [40], [42], arithmetic combination methods [43], and multi-resolution methods [44], [45]. In this paper, we take the MRI and PET image fusion as an example and apply our DDcGAN to solve this problem. MRI images are superior in capturing the details of soft tissue structures in organs such as brain, heart and lungs in high spatial resolution. The PET images are obtained by nuclear medicine imaging to provide functional and metabolic information, such as blood flow and flood activity. The captured images are usually rich in color but low in spatial resolution. Therefore, by fusing these two type medical images, the results will contain both spatial and spectral features in the source images for qualitative detection and quantitative determination.\n\nThe PET image in pseudo-color is traditionally treated as a color image and the color is the representation of the functional information, as shown in Fig. 4(a). In order to retain it, the color of the fused image should be as similar to that of the PET image as possible. For this purpose, de-correlated color models are used to separate the achromatic and chromatic information in the color into different channels. Then, the achromatic channel is substituted or fused with the MRI image [46]. In our work, we employ the intensity, hue and saturation (IHS) de-correlated color model and the intensity channel is the specific achromatic channel to be fused, as shown in Fig. 4(b). Because the other two channels are the representation of chromatic information, which ought to remain unchanged during the fusion process, the PET image is similar with the infrared image in using the intensity distribution to represent feature information. A slightly different  point is that the PET image uses it to represent the functional information while in the infrared image, it is the reflection of thermal radiation. By contrast, the MRI image can provide detailed morphological information in the form of texture. It is mainly characterized by the gradients. Thus, like the visible image, the advantage of rich texture information in the MRI image can be applied to overcome the uncertainty of contouring the soft tissue structures on the PET image. From this point of view, the essence of fusing MRI and PET image has a great deal of similarity with that of fusing visible and infrared images. As shown in Fig. 4, the fused image is supposed to minimize both the spatial distortion caused by the spatial detail loss between the MRI (Fig. 4(c)) and the intensity channel (Fig. 4(d)) and the spectral distortion caused by color differences between the PET (Fig. 4(b)) and the fused intensity channel (Fig. 4(d)) simultaneously. Accompanied by the processed components of H and S channels, the final fused image is a three-channel image with abundant color and detail information, as shown in Fig. 4(e).\n\n\nB. MRI and PET Image Fusion via DDcGAN\n\nUniformly, we assume that the resolution of the MRI image is 4 \u00d7 4 times that of the intensity component of the PET image and take it as an example. The entire fusion procedure is illustrated in Fig. 5. The multispectral input PET image with RGB channels are firstly transformed into IHS channels, as shown in Eq. (13), with the intensity channel displaying the brightness in a spectrum, the hue channel showing the property of the spectral wavelength, and the saturation channel demonstrating the purity of the spectrum: \u239b\n\u239d I PET V 1 PET V 2 PET \u239e \u23a0 = \u23a1 \u23a3 1/ \u221a 3 1/ \u221a 3 1/ \u221a 3 1/ \u221a 6 1/ \u221a 6 \u22122/ \u221a 6 1/ \u221a 2 \u22121/ \u221a 2 0 \u23a4 \u23a6 \u239b \u239d R PET G PET B PET \u239e \u23a0 . (13)\nThe components of H and S channels can be represented by variables V1 and V2 as follows:\nH PET = tan \u22121 V 1 PET V 2 PET ,(14)S PET = V 1 2 PET + V 2 2 PET .(15)\nThe fusion process is produced on the component of I channel of the PET image and the MRI image. Correspondingly, the input of the generator is the low-resolution I PET and the high-resolution MRI image M. The output of the generator I fuse = G (M, I PET ) is the new I channel of the fused image with high resolution. During the training procedure, the discriminator D i is trained to discriminate differences between I fuse and I PET , while the probability of the input image from MRI images rather than G is obtained by the discriminator D v . Therefore, the specific loss function of the generator can be expressed as follows:\nL G = L adv G + \u03bbL con ,(16)\nwhere the adversarial loss function L adv G is defined as:\nL adv G = E log (1 \u2212 D v (G (M, I PET ))) +E log (1 \u2212 D i (\u03c8 G (M, I PET ))) .(17)\nAnd the content loss L con is modified as:\nL con = E \u03c8 G (M, I PET ) \u2212 I PET 2 F +\u03b7 G (M, I PET ) \u2212 M T V ] .(18)\nFor the discriminators D v and D i , the adversarial losses are respectively defined as follows:\nL D v = E \u2212logD v (M) +E \u2212log (1 \u2212 D v (G (M, I PET ))) ,(19)L D i = E \u2212logD i (I PET ) +E \u2212log (1 \u2212 D i (\u03c8 G (M, I PET ))) .(20)\nTo preserve the chromatic information in the PET image, the components of H and S channels of the PET image and the fused image should be as identical as possible. For these two channels, we directly employ the bicubic interpolation as the upsampling operation. The upsampled components are presented as H new and S new and their resolutions are both 4 \u00d7 4 time those of H PET and S PET . According to Eq. (14) and Eq. (15), the variables V1 and V2 can be updated by the components of H and S channels:\nV 1 new = S new sinH new ,(21)V 2 new = S new cosH new .(22)\nThe inverse transform to obtain the final fused image in RGB channels from IHS channels can be represented as:\n\u239b \u239d R new G new B new \u239e \u23a0 = \u23a1 \u23a3 1/ \u221a 3 1/ \u221a 6 1/ \u221a 2 1/ \u221a 3 1/ \u221a 6 \u22121/ \u221a 2 1/ \u221a 3 \u22122/ \u221a 6 0 \u23a4 \u23a6 \u239b \u239d I fuse V 1 new V 2 new \u239e \u23a0 . (23) V. EXPERIMENTAL RESULTS\nIn this section, to validate the effectiveness of our DDcGAN, we firstly compare it with several state-of-the-art methods on publicly available datasets by qualitative comparisons both for infrared and visible image fusion and PET and MRI image fusion. For quantitative comparisons, we utilize six metrics to evaluate the fusion results. The experiments of discriminator analysis are also conducted. 2) Training Details: During the training process, the principle is to make the generator and discriminators form an adversarial relationship with each other. In order to overcome some problems in training GAN and improve the training results, rather than taking turns training G, D v and D i once per batch in principle, we train D v or D i more times if it fails to discriminate the data from G and vice verse. The detailed training process is shown in Alg. 1. Except for L max , L min and L G max , a threshold for the number of iterations is additionally set. The reason is that the goal of updating the generator or discriminators more times is to keep the balance between them. However, there are still situations where these networks have been trained many times but still cannot achieve balance conditions. Especially for the generator, more training steps to minimize the adversarial loss may lead to higher content loss and higher L G , failing to achieve the balance condition. Thus, it can avoid the algorithm falling into an endless loop. Moreover, updating other networks timely will enable them to play a new role in guiding the current network, thus possibly avoiding the above-mentioned situation.\n\nDuring the testing phase, we only use the trained generator to generate fused images. Since there are no fully connected layers in our generator, the input source images can be of any size with a predefined resolution ratio.\n\n\nB. Results and Analysis on Infrared and Visible Image Fusion\n\nTo verify the effectiveness of our proposed DDcGAN, we compare it with seven state-of-the-art image fusion methods, including directional discrete cosine transform and principal component analysis (DDCTPCA) [14], hybrid multi-scale decomposition (HMSD) [47], fourth-order partial differential equations (FPDE) [48], gradient transfer fusion (GTF) [17], different resolution total variation (DRTV) [49], DenseFuse [29] and FusionGAN [21]. Due to some of the competitors require that source images share the same resolution, we upsample the low-resolution infrared images before performing these methods for fusion. While in DRTV and FusionGAN, as they can be applied to fuse images of different resolutions, the preprocessing of up-sampling low-resolution infrared images is unnecessary. The fused results of all methods are assessed both subjectively and objectively.\n\n\n1) Qualitative Comparisons:\n\nWe first report some intuitive results on six typical image pairs, as shown in Fig. 6. Compared with the existing fusion methods, our DDcGAN has three distinctive advantages. First, our results can maintain the high-contrast property of the infrared image, e.g., the thermal targets are prominent in our fused images, as shown in the first and second examples, which is very important for the subsequent target detection task. Second, our results can preserve abundant texture details from the visible images, e.g., the backgrounds contain more detail information in our fused images, as shown in the third to fifth examples, which is beneficial for accurate target recognition. Third, our results are clearer due to that it does not suffer from thermal radiation information blurring caused by upsampling of the low-resolution infrared images, as shown in the sixth example.\n\nAs can be seen from Fig. 6, DDCTPCA, HMSD, FPDE and DenseFuse cannot highlight the thermal targets well, while Algorithm 1 Training Process of DDcGAN GTF, DRTV and FusionGAN cannot obtain abundant texture details. Besides, they all suffer from thermal radiation information blurring except DRTV and FusionGAN. Although DRTV can prevent loss of texture information caused by upsampling when fusing source images of different resolutions, the results of DRTV inevitably suffer from staircase effects due to the application of first-order TV. In contrast, the results of DDcGAN can obviously avoid staircase effects and details in our results are more similar to those in the visible images. Compared with FusionGAN, due to the employment of the deconvolution layers, the introduction of the discriminator D i , different network architecture and improved training strategy, our fused results can highlight thermal targets more obviously by higher contrast and meanwhile, contain more natural details which are more indistinguishable from the visible images. Excluding the effects of deconvolution layers, different network architecture and the training strategy, the influence of the additional discriminator will be analyzed later in Sec. V-B.3. Generally, our DDcGAN works well and the fused images are more like super-resolved infrared images which also contain abundant texture detail information in visible images.\n\n2) Quantitative Comparisons: We further report quantitative comparisons of our DDcGAN and the competitors on the rest 15 image pairs in the dataset. Eight metrics such as entropy (EN) [50], mean gradient (MG), spatial frequency (SF), standard deviation (SD) [51], peak signal-to-noise ratio (PSNR), and correlation coefficient (CC), structural similarity index measure (SSIM) [52] and visual information fidelity (VIF) [53] are used for evaluation.\n\n\u2022 Entropy (EN): This metric can measure the amount information contained in the fused image from the perspective of information theory and is defined as follows:\nE N = \u2212 L\u22121 l=0 p l log 2 p l ,\nwhere p l denotes the normalized histogram of corresponding gray level in the fused image. And the number of all the gray levels is set as L. The larger entropy means that there is more information reserved in the image and the method achieves a better performance. \u2022 Mean gradient (MG): MG is mathematically defined as:\nMG = M i=2 N j =2 x i, j \u2212x i\u22121, j 2 + x i, j \u2212x i, j \u22121 2 /2 (M \u2212 1) (N \u2212 1) .\nThe larger MG is, the more gradient information the image contains and the better fusion performance the algorithm has. \u2022 Spatial frequency (SF): SF is based on the gradient distribution to effectively reveal the details and texture  [14], HMSD [47], FPDE [48], GTF [17], DRTV [49], DenseFuse [29], FusionGAN [21] and our DDcGAN. For more intuitive comparison, the infrared images are enlarged in the first row and the original low-resolution infrared images are shown in the white box in the top left corner. of the image. It is defined by spatial row frequency (RF) and column frequency (CF):\nS F = R F 2 + C F 2 , where R F = M i=1 N j =2 x i, j \u2212 x i, j \u22121 2 and C F = M i=2 N j =1 x i, j \u2212 x i\u22121, j 2 .\nThe larger SF, the richer edges and texture details the image contains. And human perception is more sensitive to the image with larger SF. \u2022 Standard deviation (SD): SD is a metric reflecting contrast and distribution. Attention of human is more likely to be attracted by the area with high contrast. Thus, the larger SD, the better visual effect the fused image achieves.\n\nMathematically, SD is defined as:\nS D = 1 M N M i=1 N j =1 x i, j \u2212 \u03bc 2 ,\nwhere \u03bc is the mean value of the image x. \u2022 Peak signal-to-noise ratio (PSNR): PSNR is a metric reflecting the distortion by the ratio of peak value power and noise power:\nP S N R = 10log 10 r 2 M S E ,\nwhere r is the peak value of the fused image and is set as 256 in this paper. M S E is the mean square error that measures the dissimilarity between the fused image and source images and is defined as follows:\nM S E = \u03c9 a M S E a f + \u03c9 b M S E b f , where M S E x f = 1 M N M\u22121 i=0 N\u22121 j =0 x i, j \u2212 f i, j 2 .\nA larger PSNR indicates the less distortion the fusion process produces and the fused image is more similar to the source images. \u2022 Correlation coefficient (CC): The metric CC measures the degree of linear correlation between the source images and the fused image. It is mathematically defined as:\nCC = \u03c9 a r a f + \u03c9 b r b f ,\nwhere 2 , \u03bc x and \u03bc f denote the mean values of the source image x and the fused image f , respectively. A larger CC indicates that the fused image is more similar to the source images. \u2022 Structural similarity index measure (SSIM): SSIM is the widely used metric which models the loss and distortion between two images according to their similarities in light, contrast and structure information. Mathematically, SSIM between images x and y can be defined as follows:\nr x f = M i=1 N j =1 (xi,j \u2212\u03bc x )( f i, j \u2212\u03bc f ) M i=1 N j =1 (xi,j \u2212\u03bc x ) 2 M i=1 N j =1 ( f i, j \u2212\u03bc f )SS I M xy = x i ,y i 2\u03bc x i \u03bc y i +c 1 \u03bc 2 x i +\u03bc 2 y i +c 1 \u00b7 2\u03c3 x i \u03c3 y i +c 2 \u03c3 2 x i +\u03c3 2 y i +c 2 \u00b7 \u03c3 x i y i +c 3 \u03c3 x i \u03c3 y i +c 3 ,\nwhere \u03bc denotes the mean value, \u03c3 is the standard deviation/covariance, c 1 , c 2 and c 3 are the parameters to make the algorithm stable. Thus, SSIM between source images a, b and the fused image f can be defined as: The results of quantitative comparisons are summarized in Fig. 7. As can be seen from the statistical results, our DDcGAN can generate the largest average values on the first 4 metrics: EN, MG, SF and SD. In particular, our DDcGAN achieves the best values of EN, MG, SF and SD on 13, 13, 10 and 8 image pairs, respectively. For the metric PSNR and CC, our DDcGAN can achieve comparable results with the average values being the second largest. These metrics only follow behind FPDE and FusionGAN by a narrow margin, respectively. As for VIF and SSIM, our result is the third and fourth largest respectively. These results demonstrate that our method can reserve information to the greatest extent, especially the most gradient information, the richest edges and texture details, and the highest contrast, as shown in the first four metrics. In addition, the results of our methods can achieve considerable similarity with the source images. The average runtime of different methods on the testing data is provided in Table I. All the methods are tested on a desktop with 3.4 GHz Intel Core i5 CPU. Since there are three deep learning-based methods (i.e., DenseFuse, FusionGAN and DDcGAN), these methods are also tested on NVIDIA Geforce GTX Titan X. The reason why the runtime of DDcGAN is slower is that in the testing phase, the input of our model is the whole image. Thus, for each test image pair, our model is rebuilt according to their size and the parameters of the trained model are restored into the rebuilt model to avoid the possible seam effects caused by cropping tested images into patches and the distortion caused by resizing images. Another reason is that our model is deeper than other deep learning-based methods, resulting in more test runtime.\nSS I M = \u03c9 a SS I M a f + \u03c9 b SS I M b f .\n3) Discriminator Analysis: There are two discriminators presented in our proposed model, i.e., D v and D i . In order to illustrate the effect of each discriminator, we perform four comparative experiments: (a) The entire networks merely consist of the generator G and the ultimate training objective is reduced to minimize L con in Eq.  Fig. 8.\n\nIn method (a), the training objective is to minimize the content loss L con , which is the first-order TV model in essence. This model performs well in preserving edges of the object in the piecewise constant image while it inevitably produces staircase effects [54], as can be seen in Fig. 8(a). With the introduction of D v , the staircase effects have been alleviated in Fig. 8(b). However, the disadvantage is that the intensity distribution of the fused image is modified according to that of the visible image, leading to the reduction of the prominence of the thermal targets. The separate introduction of D i increases the contrast between the thermal targets and the background, which is particularly evident in the prominence of the bunker between the results shown in Fig. 8(a) and Fig. 8(c). Nevertheless, the result of method (c) lacks in detail information compared with method (b).\n\nWith a comprehensive consideration of advantages and disadvantages of method (b) and (c), we propose a new structure based on conditional generative adversarial networks with dual discriminator: D v and D i . The use of D i can correct the distinct differences of the intensity distribution between the result of method (b) and the infrared image. Meanwhile, more details and texture information can be added to the result of method (c) by introducing D v . Worthy of note that since the discriminators increase from just D v or D i to both of them, the requirement and the training target of the generator become harsher. Under the condition that there exists a contradictory relationship between the discrimination tasks of D v and D i and according to the training strategy in Alg. 1, the training of G, D v or D i can be adjusted in case any of them loses its specific function, the generation ability of the generator can be further improved. On the promise that the thermal targets are still prominent, the results of method (d) include more details and these details look more similar to those in the visible images by effectively solving the problem of staircase effects compared with those shown in Figs. 8(b) and (c).\n\n\n4) Generator Analysis:\n\nIn the loss function of the generator G, there are two subitems, i.e., the adversarial loss L adv G and the content loss L con . To verify the effect of each subitem, three comparative experiments are performed: (a) L G = \u03bbL con . This comparative experiment is the same with method (a) in Sec. V-B.3. G is trained to minimize L con in Eq. (10). (b) L G = L adv G . The content loss is not introduced in L G . Then G is only trained to fool D v and D i . It should be noted that in this method, due to the lack of pixel-wise constraints, the introduction of the deconvolution layers may cause the cavity effect. Thus, we replace these layers with two upsampling layers to avoid this influence. (c) L G = L adv G + \u03bbL con . It is the proposed method. With the same experimental settings, the fused results of these three methods are shown in Fig. 9.\n\nOn the one hand, without the adversarial loss, the fused result fails to exhibit more and clearer texture details in the visible image, as shown in Fig. 9(a). On the other hand, without the content loss, the generator is incapable of knowing which type of information should be retained from source images. Without the pixel-wise constraints, what the generator can do is to make the probability distribution of generated images close to that of source images. In this case, the fused image may have high contrast and texture details. However, the highlighted regions may not be the thermal targets in the infrared image and texture details may be different from the visible image, as shown in Fig. 9(b). Thus, when DDcGAN is trained without the content loss, it will generate artifacts and incomprehensible results. By combining these two subitems, DDcGAN can solve this problem and generate a high-quality fused image, as shown in Fig. 9(c).\n\n\nC. Results on MRI and PET Image Fusion\n\nAccording to corresponding schemes, we compare our method with six other fusion methods separately based on principal component analysis method such as DDCTPCA [14], sparse representation method such as adaptive sparse representation (ASR) [56], wavelet transform method such as discrete cosine harmonic wavelet transform (DCHWT) [55], saliency method such as Structure-Aware [57] and deep learning-based methods such as FusionGAN [21] and RCGAN [58]. Among these methods, PCA is a classic theory applied for the fusion of PET and MRI images. Based on PCA and taken as a representation of comparison methods for infrared and visible image fusion utilized in Sec. V-B, DDCTPCA is employed here for comparison once more. ASR can be applied for multi-modal image fusion and perform fusion and denoising simultaneously. DCHWT takes into account the fusion of multi-spectral image fusion. Structure-Aware is a method expressly proposed for multi-modal medical image fusion. FusionGAN and RCGAN are methods based on GAN and also representations of infrared and visible image fusion methods.  [55], DDCTPCA [14], ASR [56], Structure-Aware [57], FusionGAN [21] and RCGAN [58] and our DDcGAN. For more intuitive comparison, the PET images are enlarged in the first row and the original low-resolution PET images are shown in the white box in the top left corner.\n\nIn the remainder of this section, qualitative and quantitative experiments are conducted to demonstrate the effectiveness of our method on PET and MRI image fusion.\n\n1) Qualitative Comparison: Four typical and intuitive results on four different transaxial sections of the brain-hemispheric are exhibited in Fig. 10. By comparison, DCHWT, Structure-Aware and RCGAN significantly reduce  II   AVERAGE RUNTIME COMPARISON OF DIFFERENT METHODS ON THE 20 TESTING IMAGE PAIRS (UNIT: SECOND). AS FOR THE RUNTIME OF DEEP  LEARNING-BASED METHODS, THE FIRST VALUE IS TESTED ON CPU AND THE SECOND VALUE IS TESTED  the intensity of colors in the PET image, leading to the loss of functional information. By contrast, the results generated by DDCTPCA, ASR, FusionGAN and DDcGAN exhibit brighter and stronger colors. And among these four methods, the colors of our results are the closest to those of the original PET images. Furthermore, as a result of the unsampling of low-resolution PET image, the results of six comparison methods suffer from functional information blurring, presented as blurred color information, as shown in the first and second groups of results, and blurred details, which can be seen in the third group of results. In terms of the texture information retained from the MRI image, the results of DDCTPCA and FusionGAN show the most obvious fuzziness. Moreover, due to the fact that ASR performs fusion and denoising simultaneously, the impurities in the MRI image are eliminated in the fused image. However, some image details are blurred in the meantime. Compared with DCHWT, Structure-Aware and RCGAN, the details in our results avoid blurring and the difficulty of recognition due to darker colors, which can be seen in the fourth group.\n\n2) Quantitative Comparison: Experiments of eight performance metrics are performed here and the results of quantitative comparisons on 20 test image pairs are shown in Fig. 11. The 20 test image pairs are of different transaxial sections of the brain-hemispheric. As for the first five metrics: EN, MG, SF, SD and PSNR, our proposed method can achieve the largest mean values with 19, 19, 10, 14 and 20 of all the 20 test pairs performing the best values, respectively. As for the metrics CC and VIF, our method also shows comparable results, generating the second largest average values and its average values merely follow behind that of DDCTPCA and that of FusionGAN respectively. As for SSIM, our method generates the fifth largest average value, the reason is that our method is designed to preserve the gradient variations in the MRI image regardless of the pixel intensity, leading to a small SSIM value between the fused intensity channel and the MRI image. Thus, it can be concluded from the statistical results that for PET and MRI image fusion, our method can also obtain relatively satisfactory results by reserving the texture information, i.e., morphological information, and color information, i.e., functional and metabolic information, to a great extent at the same time.\n\nThe average runtime of the 6 methods on the 20 testing image pairs is also reported in Table II. VI. CONCLUSION In this paper, we proposed a new deep learning-based infrared and visible image fusion method by constructing a dual-discriminator conditional GAN, named DDcGAN. It does not require the ground-truth fused images for training, and can fuse images of different resolutions without introducing thermal radiation information blurring or visible texture detail loss. Extensive comparisons on six metrics with other seven state-of-the-art fusion algorithms demonstrate that our DDc-GAN can not only identify the most valuable information, but also can keep the largest or approximately the largest amount of information in the source images. Moreover, our proposed DDcGAN is applied to the fusion of PET and MRI images, and it can also achieve an advanced performance compared with five state-of-the-art algorithms.\n\nFig. 1 .\n1The entire procedure of our DDcGAN for image fusion.\n\nFig. 2 .\n2The overall architecture of our generator, including layers of encoder and decoder. 3 \u00d7 3: filter size, Conv(nk): convolutional layer which obtains k feature maps, BN: batch normalization.\n\nFig. 3 .\n3The overall architecture of our discriminator. 3 \u00d7 3: filter size, Conv(nk): convolutional layer which obtains k feature maps, BN: batch normalization, FC: fully connected layer.\n\nFig. 4 .\n4Schematic illustration of fusing the low-resolution PET image in RGB channels and the high-resolution MRI image in the gray channel to obtain the high-resolution fused image in RGB channels.\n\nFig. 5 .\n5The entire procedure of applying the proposed DDcGAN for MRI and PET image fusion.\n\n\nA.Dataset  and Training Details 1) Dataset: We validate the proposed DDcGAN on the publicly available TNO Human Factors dataset 1 for the infrared and visible image fusion. We select 36 infrared and visible image pairs from the dataset and crop them into 27, 264 patch pairs with 84 \u00d7 84 pixels. As we focus on fusing images of different resolutions while the source images in the dataset are of the same resolution, we downsample the infrared images to one quarter resolution. Therefore, all visible image patches are of size 84 \u00d7 84 and all infrared patches are of size 21 \u00d7 21. Parameters in our model are set as \u03bb = 0.5 and \u03b7 = 1.2. The entire network is trained with a learning rate of 2 \u00d7 10 \u22123 with exponentially decaying to 0.75 of the original value after each epoch. The batch size is set as 24. The application of our proposed DDcGAN to MRI and PET image fusion is validated on the publicly available Harvard medical school website. 2 The original PET and MRI images are all of size 256 \u00d7 256. For the purpose of validating the effectiveness of our method on fusing images of different resolutions, each channel of the PET images is downsampled to the size of 64\u00d764. 83 PET and MRI pairs are downloaded and cropped into 9, 984 patch pairs as our training set. Similarly, all MRI patches are of size 84\u00d784 and the intensity patches of all PET images are of size 21 \u00d7 21. The parameters, learning rate and the batch size are the same with those set in the infrared and visible fusion. 1 https://figshare.com/articles/TNO_Image_Fusion_Dataset/1008029 2 http://www.med.harvard.edu/AANLIB/home.html\n\nFig. 6 .\n6Qualitative comparison of our DDcGAN with 7 state-of-the-art methods on 6 typical infrared and visible image pairs. From top to bottom: infrared image, visible image, fusion results of DDCTPCA\n\nFig. 7 .\n7Quantitative comparison of our DDcGAN for infrared and visible image fusion with 7 state-of-the-art methods. Means of metrics for different methods are shown in the legends. \u2022 Visual information fidelity (VIF): The metric is consistent with human visual system and measures the information fidelity. It can be computed by four steps: (a) filter and divide the source images and the fused image into different blocks; (b) evaluate the visual information of each block; (c) calculate the VIF for each subband;(d) calculate the overall metric. A large VIF indicates that the fusion method has a good performance.\n\n\n(10). (b) D i is not employed and the adversarial relationship exists only between G and D v . (c) D v is not embraced in the entire networks. Thus, the adversarial game is established between G and D i . (d) The fused images are generated by the method proposed in this paper. All of G, D v , and D i play a part in the networks. All the comparative experiments are under the same experimental settings and the fused results are shown in\n\nFig. 8 .\n8Fused results on bunker when the discriminators in the entire networks change. We highlight a region and zoom in it as shown in the bottom red box. The infrared images are enlarged in the first plot, and the original low-resolution infrared images are shown in the white box in the top left corner.\n\nFig. 9 .\n9Fused results on bunker when the loss function of the generator L G changes.\n\nFig. 10 .\n10Qualitative comparison of our DDcGAN with 6 state-of-the-art methods on 4 typical MRI and PET image pairs of different (from left to right: #99, #81, #60 and #70) transaxial sections of the brain-hemispheric. From top to bottom: PET image, MRI image, fusion results of DCHWT\n\nFig. 11 .\n11Quantitative comparison of our DDcGAN for PET and MRI image fusion with 5 state-of-the-art methods. Means of metrics for different methods are shown in the legends.\n\n\nreceived December 10, 2018; revised September 30, 2019; accepted February 26, 2020. Date of current version March 12, 2020. This work was supported in part by the National Natural Science Foundation of China under Grant 61773295, Grant 61903279, and Grant 61971165, and in part by the Natural Sciences and Engineering Research Council of Canada (NSERC) under Grant RGPIN239031. The associate editor coordinating the review of this manuscript and approving it for publication was Dr. Jana Ehmann. (Corresponding author: Xiaoguang Mei.) Jiayi Ma, Han Xu, and Xiaoguang Mei are with the Electronic Information School, Wuhan University, Wuhan 430072, China (e-mail: jyma2010@gmail.com; meixiaoguang@gmail.com; xu_han@whu.edu.cn). Junjun Jiang is with the School of Computer Science and Technology, Harbin Institute of Technology, Harbin 150001, China (e-mail: junjun0595@163.com). Xiao-Ping Zhang is with the Department of Electrical, Computer and Biomedical Engineering, Ryerson University, Toronto, ON M5B 2K3, Canada (e-mail: xzhang@ee.ryerson.ca). Digital Object Identifier 10.1109/TIP.2020.2977573\n\nTABLE I\nIAVERAGE RUNTIME COMPARISON OF DIFFERENT METHODS ON THE 15 TESTING IMAGE PAIRS (UNIT: SECOND). AS FOR THE RUNTIME OF DEEP LEARNING-BASED METHODS, THE FIRST VALUE IS TESTED ON CPU AND THE SECOND VALUE IS TESTED ON GPU\n\nTABLE\n\nthored more than 120 refereed journal and conference papers, including  the IEEE TPAMI/TIP/TSP/TNNLS/TIE/TGRS/TCYB/TMM/TCSVT, IJCV,  CVPR, ICCV, IJCAI, AAAI, ICRA, IROS,Dr. Zhang is a member of the Beta Gamma Sigma Honor Society. He is an elected member of the ICME Steering Committee.He\nDeveloping a spectral-based strategy for urban object detection from airborne hyperspectral TIR and visible data. M Eslami, A Mohammadzadeh, IEEE J. Sel. Topics Appl. Earth Observat., Remote Sens. 95M. Eslami and A. Mohammadzadeh, \"Developing a spectral-based strategy for urban object detection from airborne hyperspectral TIR and visible data,\" IEEE J. Sel. Topics Appl. Earth Observat., Remote Sens., vol. 9, no. 5, pp. 1808-1816, May 2016.\n\nSelfadapting weighted operators for multiscale gradient fusion. C Lopez-Molina, J Montero, H Bustince, B. De Baets, Inf. Fusion. 44C. Lopez-Molina, J. Montero, H. Bustince, and B. De Baets, \"Self- adapting weighted operators for multiscale gradient fusion,\" Inf. Fusion, vol. 44, pp. 136-146, Nov. 2018.\n\nMiddle infrared (wavelength range: 8 \u03bcm-14 \u03bcm) 2-dimensional spectroscopy. N Yamamoto, T Saito, S Ogawa, I Ishimaru, 100total weight with electrical controller: 1.7 kg, total cost: Less thanN. Yamamoto, T. Saito, S. Ogawa, and I. Ishimaru, \"Middle infrared (wavelength range: 8 \u03bcm-14 \u03bcm) 2-dimensional spectroscopy (total weight with electrical controller: 1.7 kg, total cost: Less than 10,000\n\nUSD) so-called hyperspectral camera for unmanned air vehicles like drones. Proc. SPIE. SPIE9840Art. no. 984028USD) so-called hyperspectral camera for unmanned air vehicles like drones,\" Proc. SPIE, vol. 9840, May 2016, Art. no. 984028.\n\nCarbon quantum dots/hydrogenated TiO 2 nanobelt heterostructures and their broad spectrum photocatalytic properties under UV, visible, and near-infrared irradiation. J Tian, Nano Energy. 11J. Tian et al., \"Carbon quantum dots/hydrogenated TiO 2 nanobelt heterostructures and their broad spectrum photocatalytic properties under UV, visible, and near-infrared irradiation,\" Nano Energy, vol. 11, pp. 419-427, Jan. 2015.\n\nA survey of infrared and visual image fusion methods. X Jin, Infr. Phys. Technol. 85X. Jin et al., \"A survey of infrared and visual image fusion methods,\" Infr. Phys. Technol., vol. 85, pp. 478-501, Sep. 2017.\n\nFrom multi-scale decomposition to Non-Multi-Scale decomposition methods: A comprehensive survey of image fusion techniques and its applications. A Dogra, B Goyal, S , IEEE Access. 5A. Dogra, B. Goyal, and S. Agrawal, \"From multi-scale decomposition to Non-Multi-Scale decomposition methods: A comprehensive survey of image fusion techniques and its applications,\" IEEE Access, vol. 5, pp. 16040-16067, 2017.\n\nInfrared and visible image fusion methods and applications: A survey. J Ma, Y Ma, C Li, Inf. Fusion. 45J. Ma, Y. Ma, and C. Li, \"Infrared and visible image fusion methods and applications: A survey,\" Inf. Fusion, vol. 45, pp. 153-178, Jan. 2019.\n\nImage fusion with guided filtering. S Li, X Kang, J Hu, IEEE Trans. Image Process. 227S. Li, X. Kang, and J. Hu, \"Image fusion with guided filtering,\" IEEE Trans. Image Process., vol. 22, no. 7, pp. 2864-2875, Jul. 2013.\n\nAn adaptive fusion algorithm for visible and infrared videos based on entropy and the cumulative distribution of gray levels. H.-M Hu, J Wu, B Li, Q Guo, J Zheng, IEEE Trans. Multimedia. 1912H.-M. Hu, J. Wu, B. Li, Q. Guo, and J. Zheng, \"An adaptive fusion algorithm for visible and infrared videos based on entropy and the cumulative distribution of gray levels,\" IEEE Trans. Multimedia, vol. 19, no. 12, pp. 2706-2719, Dec. 2017.\n\nInfrared and visible image fusion based on target extraction in the nonsubsampled contourlet transform domain. K He, D Zhou, X Zhang, R Nie, Q Wang, X Jin, J. Appl. Remote Sens. 111Art. no. 015011K. He, D. Zhou, X. Zhang, R. Nie, Q. Wang, and X. Jin, \"Infrared and visible image fusion based on target extraction in the nonsubsampled contourlet transform domain,\" J. Appl. Remote Sens., vol. 11, no. 1, 2017, Art. no. 015011.\n\nEfficient image fusion with approximate sparse representation. Y Bin, Y Chao, H Guoyu, Int. J. Wavelets, Multiresolution Inf. Process. 144Art. no. 1650024Y. Bin, Y. Chao, and H. Guoyu, \"Efficient image fusion with approx- imate sparse representation,\" Int. J. Wavelets, Multiresolution Inf. Process., vol. 14, no. 4, Jul. 2016, Art. no. 1650024.\n\nSparse representation based multi-sensor image fusion for multi-focus and multi-modality images: A review. Q Zhang, Y Liu, R S Blum, J Han, D Tao, Inf. Fusion. 40Q. Zhang, Y. Liu, R. S. Blum, J. Han, and D. Tao, \"Sparse representation based multi-sensor image fusion for multi-focus and multi-modality images: A review,\" Inf. Fusion, vol. 40, pp. 57-75, Mar. 2018.\n\nA fusion algorithm for infrared and visible images based on adaptive dual-channel unit-linking PCNN in NSCT domain. T Xiang, L Yan, R Gao, Infr. Phys. Technol. 69T. Xiang, L. Yan, and R. Gao, \"A fusion algorithm for infrared and visible images based on adaptive dual-channel unit-linking PCNN in NSCT domain,\" Infr. Phys. Technol., vol. 69, pp. 53-61, Mar. 2015.\n\nHybrid DDCT-PCA based multi sensor image fusion. V P S Naidu, J. Opt. 431V. P. S. Naidu, \"Hybrid DDCT-PCA based multi sensor image fusion,\" J. Opt., vol. 43, no. 1, pp. 48-61, Nov. 2013.\n\nInfrared and visible image fusion based on visual saliency map and weighted least square optimization. J Ma, Z Zhou, B Wang, H Zong, Infr. Phys. Technol. 82J. Ma, Z. Zhou, B. Wang, and H. Zong, \"Infrared and visible image fusion based on visual saliency map and weighted least square opti- mization,\" Infr. Phys. Technol., vol. 82, pp. 8-17, May 2017.\n\nA novel infrared and visible image fusion algorithm based on shift-invariant dual-tree complex shearlet transform and sparse representation. M Yin, P Duan, W Liu, X Liang, Neurocomputing. 226M. Yin, P. Duan, W. Liu, and X. Liang, \"A novel infrared and visible image fusion algorithm based on shift-invariant dual-tree complex shearlet transform and sparse representation,\" Neurocomputing, vol. 226, pp. 182-191, Feb. 2017.\n\nInfrared and visible image fusion via gradient transfer and total variation minimization. J Ma, C Chen, C Li, J Huang, Inf. Fusion. 31J. Ma, C. Chen, C. Li, and J. Huang, \"Infrared and visible image fusion via gradient transfer and total variation minimization,\" Inf. Fusion, vol. 31, pp. 100-109, Sep. 2016.\n\nInfrared and visible image fusion using total variation model. Y Ma, J Chen, C Chen, F Fan, J Ma, Neurocomputing. 202Y. Ma, J. Chen, C. Chen, F. Fan, and J. Ma, \"Infrared and visible image fusion using total variation model,\" Neurocomputing, vol. 202, pp. 12-19, Aug. 2016.\n\nImage fusion with convolutional sparse representation. Y Liu, X Chen, R K Ward, Z. Jane Wang, IEEE Signal Process. Lett. 2312Y. Liu, X. Chen, R. K. Ward, and Z. Jane Wang, \"Image fusion with convolutional sparse representation,\" IEEE Signal Process. Lett., vol. 23, no. 12, pp. 1882-1886, Dec. 2016.\n\nInfrared and visible image fusion using a deep learning framework. H Li, X.-J Wu, J Kittler, arXiv:1804.06992H. Li, X.-J. Wu, and J. Kittler, \"Infrared and visible image fusion using a deep learning framework,\" 2018, arXiv:1804.06992. [Online]. Available: http://arxiv.org/abs/1804.06992\n\nFusionGAN: A generative adversarial network for infrared and visible image fusion. J Ma, W Yu, P Liang, C Li, J Jiang, Inf. Fusion. 48J. Ma, W. Yu, P. Liang, C. Li, and J. Jiang, \"FusionGAN: A generative adversarial network for infrared and visible image fusion,\" Inf. Fusion, vol. 48, pp. 11-26, Aug. 2019.\n\nInfrared and visible image fusion via detail preserving adversarial learning. J Ma, Inf. Fusion. 54J. Ma et al., \"Infrared and visible image fusion via detail preserving adversarial learning,\" Inf. Fusion, vol. 54, pp. 85-98, Feb. 2020.\n\nConditional generative adversarial nets. M Mirza, S Osindero, arXiv:1411.1784M. Mirza and S. Osindero, \"Conditional generative adver- sarial nets,\" 2014, arXiv:1411.1784. [Online].\n\nLearning a generative model for fusing infrared and visible images via conditional generative adversarial network with dual discriminators. H Xu, P Liang, W Yu, J Jiang, J Ma, Proc. 28th Int. Joint Conf. 28th Int. Joint ConfH. Xu, P. Liang, W. Yu, J. Jiang, and J. Ma, \"Learning a generative model for fusing infrared and visible images via conditional generative adversarial network with dual discriminators,\" in Proc. 28th Int. Joint Conf. Artif. Intell., Aug. 2019, pp. 3954-3960.\n\nDeep learning for pixel-level image fusion: Recent advances and future prospects. Y Liu, X Chen, Z Wang, Z J Wang, R K Ward, X Wang, Inf. Fusion. 42Y. Liu, X. Chen, Z. Wang, Z. J. Wang, R. K. Ward, and X. Wang, \"Deep learning for pixel-level image fusion: Recent advances and future prospects,\" Inf. Fusion, vol. 42, pp. 158-173, Jul. 2018.\n\nA medical image fusion method based on convolutional neural networks. Y Liu, X Chen, J Cheng, H Peng, Proc. 20th Int. Conf. Inf. Fusion (Fusion). 20th Int. Conf. Inf. Fusion (Fusion)Y. Liu, X. Chen, J. Cheng, and H. Peng, \"A medical image fusion method based on convolutional neural networks,\" in Proc. 20th Int. Conf. Inf. Fusion (Fusion), Jul. 2017, pp. 1-7.\n\nPansharpening by convolutional neural networks. G Masi, D Cozzolino, L Verdoliva, G Scarpa, Remote Sens. 87594G. Masi, D. Cozzolino, L. Verdoliva, and G. Scarpa, \"Pansharpening by convolutional neural networks,\" Remote Sens., vol. 8, no. 7, p. 594, Jul. 2016.\n\nDeepFuse: A deep unsupervised approach for exposure fusion with extreme exposure image pairs. K R Prabhakar, V S Srikar, R V Babu, Proc. IEEE Int. Conf. Comput. Vis. (ICCV). IEEE Int. Conf. Comput. Vis. (ICCV)K. R. Prabhakar, V. S. Srikar, and R. V. Babu, \"DeepFuse: A deep unsupervised approach for exposure fusion with extreme exposure image pairs,\" in Proc. IEEE Int. Conf. Comput. Vis. (ICCV), Oct. 2017, pp. 4724-4732.\n\nDensefuse: A fusion approach to infrared and visible images. H Li, X.-J Wu, IEEE Trans. Image Process. 285H. Li and X.-J. Wu, \"Densefuse: A fusion approach to infrared and visi- ble images,\" IEEE Trans. Image Process., vol. 28, no. 5, pp. 2614-2623, May 2019.\n\nImage super-resolution with sparse neighbor embedding. X Gao, K Zhang, D Tao, X Li, IEEE Trans. Image Process. 217X. Gao, K. Zhang, D. Tao, and X. Li, \"Image super-resolution with sparse neighbor embedding,\" IEEE Trans. Image Process., vol. 21, no. 7, pp. 3194-3205, Jul. 2012.\n\nNIPS 2016 tutorial: Generative adversarial networks. I Goodfellow, arXiv:1701.00160I. Goodfellow, \"NIPS 2016 tutorial: Generative adversarial networks,\" 2017, arXiv:1701.00160. [Online].\n\nGenerative adversarial nets. I Goodfellow, Proc. Adv. Neural Inf. Process. Syst. Adv. Neural Inf. ess. SystI. Goodfellow et al., \"Generative adversarial nets,\" in Proc. Adv. Neural Inf. Process. Syst., 2014, pp. 2672-2680.\n\nImage de-raining using a conditional generative adversarial network. H Zhang, V Sindagi, V M Patel, arXiv:1701.05957H. Zhang, V. Sindagi, and V. M. Patel, \"Image de-raining using a conditional generative adversarial network,\" 2017, arXiv:1701.05957. [Online]. Available: http://arxiv.org/abs/1701.05957\n\nFast gradient-based algorithms for constrained total variation image denoising and deblurring problems. A Beck, M Teboulle, IEEE Trans. Image Process. 1811A. Beck and M. Teboulle, \"Fast gradient-based algorithms for con- strained total variation image denoising and deblurring problems,\" IEEE Trans. Image Process., vol. 18, no. 11, pp. 2419-2434, Nov. 2009.\n\nAdaptive deconvolutional networks for mid and high level feature learning. M D Zeiler, G W Taylor, R Fergus, Proc. Int. Conf. Comput. Vis. Int. Conf. Comput. VisM. D. Zeiler, G. W. Taylor, and R. Fergus, \"Adaptive deconvolutional networks for mid and high level feature learning,\" in Proc. Int. Conf. Comput. Vis., Nov. 2011, pp. 2018-2025.\n\nDensely connected convolutional networks. G Huang, Z Liu, L V D Maaten, K Q Weinberger, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)G. Huang, Z. Liu, L. V. D. Maaten, and K. Q. Weinberger, \"Densely connected convolutional networks,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jul. 2017, pp. 4700-4708.\n\nImagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, Proc. Adv. Neural Inf. Process. Syst. Adv. Neural Inf. ess. SystA. Krizhevsky, I. Sutskever, and G. E. Hinton, \"Imagenet classification with deep convolutional neural networks,\" in Proc. Adv. Neural Inf. Process. Syst., 2012, pp. 1097-1105.\n\nClinical utility of positron emission tomography magnetic resonance imaging (PET-MRI) in gastrointestinal cancers. R Matthews, M Choi, Diagnostics. 6335R. Matthews and M. Choi, \"Clinical utility of positron emission tomography magnetic resonance imaging (PET-MRI) in gastrointestinal cancers,\" Diagnostics, vol. 6, no. 3, p. 35, Sep. 2016.\n\nPotential of PET-MRI for imaging of non-oncologic musculoskeletal disease. F Kogan, A P Fan, G E Gold, Quant. Imag. Med. Surg. 66F. Kogan, A. P. Fan, and G. E. Gold, \"Potential of PET-MRI for imaging of non-oncologic musculoskeletal disease,\" Quant. Imag. Med. Surg., vol. 6, no. 6, pp. 756-771, Dec. 2016.\n\nMRI and PET image fusion by combining IHS and retina-inspired models. S Daneshvar, H Ghassemian, Inf. Fusion. 112S. Daneshvar and H. Ghassemian, \"MRI and PET image fusion by combining IHS and retina-inspired models,\" Inf. Fusion, vol. 11, no. 2, pp. 114-123, Apr. 2010.\n\nEstimation of 3D category-specific object structure: Symmetry, manhattan and/or multiple images. Y Gao, A L Yuille, Int. J. Comput. Vis. 12710Y. Gao and A. L. Yuille, \"Estimation of 3D category-specific object structure: Symmetry, manhattan and/or multiple images,\" Int. J. Comput. Vis., vol. 127, no. 10, pp. 1501-1526, Aug. 2019.\n\nMultimodal medical image fusion based on IHS and PCA. C He, Q Liu, H Li, H Wang, Procedia Eng. 7C. He, Q. Liu, H. Li, and H. Wang, \"Multimodal medical image fusion based on IHS and PCA,\" Procedia Eng., vol. 7, pp. 280-285, Jan. 2010.\n\nPixel-level image fusion using brovey transforme and wavelet transform. R A Mandhare, P Upadhyay, S Gupta, Int. J. Adv. Res. Electr., Electron. Instrum. Eng. 26R. A. Mandhare, P. Upadhyay, and S. Gupta, \"Pixel-level image fusion using brovey transforme and wavelet transform,\" Int. J. Adv. Res. Electr., Electron. Instrum. Eng., vol. 2, no. 6, pp. 2690-2695, 2013.\n\nMultimodal medical image sensor fusion framework using cascade of wavelet and contourlet transform domains. V Bhateja, H Patel, A Krishn, A Sahu, A Lay-Ekuakille, IEEE Sensors J. 1512V. Bhateja, H. Patel, A. Krishn, A. Sahu, and A. Lay-Ekuakille, \"Multimodal medical image sensor fusion framework using cascade of wavelet and contourlet transform domains,\" IEEE Sensors J., vol. 15, no. 12, pp. 6783-6790, Dec. 2015.\n\nFusion of multimodal medical images using daubechies complex wavelet transform-a multiresolution approach. R Singh, A Khare, Inf. Fusion. 19R. Singh and A. Khare, \"Fusion of multimodal medical images using daubechies complex wavelet transform-a multiresolution approach,\" Inf. Fusion, vol. 19, pp. 49-60, 2014.\n\nPerformance evaluation of color models in the fusion of functional and anatomical images. P Ganasala, V Kumar, A D Prasad, J. Med. Syst. 405122P. Ganasala, V. Kumar, and A. D. Prasad, \"Performance evaluation of color models in the fusion of functional and anatomical images,\" J. Med. Syst., vol. 40, no. 5, p. 122, Apr. 2016.\n\nPerceptual fusion of infrared and visible images through a hybrid multi-scale decomposition with Gaussian and bilateral filters. Z Zhou, B Wang, S Li, M Dong, Inf. Fusion. 30Z. Zhou, B. Wang, S. Li, and M. Dong, \"Perceptual fusion of infrared and visible images through a hybrid multi-scale decomposition with Gaussian and bilateral filters,\" Inf. Fusion, vol. 30, pp. 15-26, Jul. 2016.\n\nMulti-sensor image fusion based on fourth order partial differential equations. D P Bavirisetti, G Xiao, G Liu, Proc. 20th Int. Conf. Inf. Fusion (Fusion). 20th Int. Conf. Inf. Fusion (Fusion)D. P. Bavirisetti, G. Xiao, and G. Liu, \"Multi-sensor image fusion based on fourth order partial differential equations,\" in Proc. 20th Int. Conf. Inf. Fusion (Fusion), Jul. 2017, pp. 1-9.\n\nFusing infrared and visible images of different resolutions via total variation model. Q Du, H Xu, Y Ma, J Huang, F Fan, Sensors. 18113827Q. Du, H. Xu, Y. Ma, J. Huang, and F. Fan, \"Fusing infrared and visible images of different resolutions via total variation model,\" Sensors, vol. 18, no. 11, p. 3827, Nov. 2018.\n\nAssessment of image fusion procedures using entropy, image quality, and multispectral classification. J Van Aardt, J. Appl. Remote Sens. 21Art. no. 023522J. Van Aardt, \"Assessment of image fusion procedures using entropy, image quality, and multispectral classification,\" J. Appl. Remote Sens., vol. 2, no. 1, May 2008, Art. no. 023522.\n\nImage quality measures and their performance. A M Eskicioglu, P S Fisher, IEEE Trans. Commun. 4312A. M. Eskicioglu and P. S. Fisher, \"Image quality measures and their performance,\" IEEE Trans. Commun., vol. 43, no. 12, pp. 2959-2965, Dec. 1995.\n\nImage quality assessment: From error visibility to structural similarity. Z Wang, A C Bovik, H R Sheikh, E P Simoncelli, IEEE Trans. Image Process. 134Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, \"Image quality assessment: From error visibility to structural similarity,\" IEEE Trans. Image Process., vol. 13, no. 4, pp. 600-612, Apr. 2004.\n\nA new image fusion performance metric based on visual information fidelity. Y Han, Y Cai, Y Cao, X Xu, Inf. Fusion. 142Y. Han, Y. Cai, Y. Cao, and X. Xu, \"A new image fusion performance metric based on visual information fidelity,\" Inf. Fusion, vol. 14, no. 2, pp. 127-135, Apr. 2013.\n\nImplementation of high-order variational models made easy for image processing. W Lu, J Duan, Z Qiu, Z Pan, R W Liu, L Bai, Math. Methods Appl. Sci. 3914W. Lu, J. Duan, Z. Qiu, Z. Pan, R. W. Liu, and L. Bai, \"Implementation of high-order variational models made easy for image processing,\" Math. Methods Appl. Sci., vol. 39, no. 14, pp. 4208-4233, Mar. 2016.\n\nMultifocus and multispectral image fusion based on pixel significance using discrete cosine harmonic wavelet transform. B K Kumar, Signal, Image Video Process. 7B. K. Shreyamsha Kumar, \"Multifocus and multispectral image fusion based on pixel significance using discrete cosine harmonic wavelet transform,\" Signal, Image Video Process., vol. 7, no. 6, pp. 1125-1143, Aug. 2012.\n\nSimultaneous image fusion and denoising with adaptive sparse representation. Y Liu, Z Wang, IET Image Process. 95Y. Liu and Z. Wang, \"Simultaneous image fusion and denoising with adaptive sparse representation,\" IET Image Process., vol. 9, no. 5, pp. 347-357, May 2015.\n\nStructure-aware image fusion. W Li, Y Xie, H Zhou, Y Han, K Zhan, Optik. 172W. Li, Y. Xie, H. Zhou, Y. Han, and K. Zhan, \"Structure-aware image fusion,\" Optik, vol. 172, pp. 1-11, Nov. 2018.\n\nCoupled GAN with relativistic discriminators for infrared and visible images fusion. Q Li, IEEE Sensors J. to be publishedQ. Li et al., \"Coupled GAN with relativistic discriminators for infrared and visible images fusion,\" IEEE Sensors J., to be published.\n", "annotations": {"author": "[{\"end\":118,\"start\":109},{\"end\":126,\"start\":119},{\"end\":140,\"start\":127},{\"end\":155,\"start\":141},{\"end\":191,\"start\":156}]", "publisher": null, "author_last_name": "[{\"end\":117,\"start\":115},{\"end\":125,\"start\":123},{\"end\":139,\"start\":134},{\"end\":154,\"start\":151},{\"end\":190,\"start\":185}]", "author_first_name": "[{\"end\":114,\"start\":109},{\"end\":122,\"start\":119},{\"end\":133,\"start\":127},{\"end\":150,\"start\":141},{\"end\":184,\"start\":175}]", "author_affiliation": null, "title": "[{\"end\":106,\"start\":1},{\"end\":297,\"start\":192}]", "venue": "[{\"end\":336,\"start\":299}]", "abstract": "[{\"end\":2087,\"start\":372}]", "bib_ref": "[{\"end\":2290,\"start\":2280},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2344,\"start\":2341},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2349,\"start\":2346},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2482,\"start\":2479},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2501,\"start\":2498},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3041,\"start\":3038},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3132,\"start\":3129},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3137,\"start\":3134},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3348,\"start\":3345},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3700,\"start\":3697},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3706,\"start\":3702},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3748,\"start\":3744},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3754,\"start\":3750},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3789,\"start\":3785},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3818,\"start\":3814},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3847,\"start\":3843},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3868,\"start\":3864},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3899,\"start\":3895},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3905,\"start\":3901},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4643,\"start\":4639},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4649,\"start\":4645},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5010,\"start\":5006},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5016,\"start\":5012},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6175,\"start\":6171},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8369,\"start\":8365},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":11450,\"start\":11446},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":11710,\"start\":11706},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":11897,\"start\":11893},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":12235,\"start\":12231},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":12712,\"start\":12708},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":12850,\"start\":12846},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":13182,\"start\":13178},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":13545,\"start\":13541},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":14840,\"start\":14836},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":16482,\"start\":16478},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":16984,\"start\":16980},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":18882,\"start\":18878},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":22296,\"start\":22292},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":23087,\"start\":23083},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":23622,\"start\":23618},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":24972,\"start\":24968},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":25997,\"start\":25993},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":26407,\"start\":26403},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":28484,\"start\":28480},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":28490,\"start\":28486},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":28572,\"start\":28568},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":28608,\"start\":28604},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":29323,\"start\":29319},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":29329,\"start\":29325},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":29366,\"start\":29362},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":29401,\"start\":29397},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":29407,\"start\":29403},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":30570,\"start\":30566},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":34584,\"start\":34580},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":37122,\"start\":37118},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":37168,\"start\":37164},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":37225,\"start\":37221},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":37262,\"start\":37258},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":37312,\"start\":37308},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":37328,\"start\":37324},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":37347,\"start\":37343},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":40294,\"start\":40290},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":40368,\"start\":40364},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":40486,\"start\":40482},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":40529,\"start\":40525},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":41389,\"start\":41385},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":41400,\"start\":41396},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":41411,\"start\":41407},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":41421,\"start\":41417},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":41432,\"start\":41428},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":41448,\"start\":41444},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":41464,\"start\":41460},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":43156,\"start\":43155},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":46500,\"start\":46496},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":48730,\"start\":48726},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":50386,\"start\":50382},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":50466,\"start\":50462},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":50556,\"start\":50552},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":50602,\"start\":50598},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":50657,\"start\":50653},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":50672,\"start\":50668},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":51312,\"start\":51308},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":51326,\"start\":51322},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":51336,\"start\":51332},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":51358,\"start\":51354},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":51374,\"start\":51370},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":51389,\"start\":51385}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":55607,\"start\":55544},{\"attributes\":{\"id\":\"fig_1\"},\"end\":55807,\"start\":55608},{\"attributes\":{\"id\":\"fig_2\"},\"end\":55997,\"start\":55808},{\"attributes\":{\"id\":\"fig_3\"},\"end\":56199,\"start\":55998},{\"attributes\":{\"id\":\"fig_4\"},\"end\":56293,\"start\":56200},{\"attributes\":{\"id\":\"fig_5\"},\"end\":57900,\"start\":56294},{\"attributes\":{\"id\":\"fig_6\"},\"end\":58104,\"start\":57901},{\"attributes\":{\"id\":\"fig_7\"},\"end\":58725,\"start\":58105},{\"attributes\":{\"id\":\"fig_8\"},\"end\":59166,\"start\":58726},{\"attributes\":{\"id\":\"fig_9\"},\"end\":59476,\"start\":59167},{\"attributes\":{\"id\":\"fig_10\"},\"end\":59564,\"start\":59477},{\"attributes\":{\"id\":\"fig_11\"},\"end\":59852,\"start\":59565},{\"attributes\":{\"id\":\"fig_12\"},\"end\":60030,\"start\":59853},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":61131,\"start\":60031},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":61357,\"start\":61132},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":61365,\"start\":61358}]", "paragraph": "[{\"end\":4145,\"start\":2106},{\"end\":5169,\"start\":4147},{\"end\":5857,\"start\":5171},{\"end\":7422,\"start\":5859},{\"end\":8306,\"start\":7424},{\"end\":10296,\"start\":8308},{\"end\":11004,\"start\":10298},{\"end\":11285,\"start\":11025},{\"end\":12508,\"start\":11327},{\"end\":13966,\"start\":12510},{\"end\":15598,\"start\":13968},{\"end\":15946,\"start\":15600},{\"end\":16219,\"start\":15985},{\"end\":16396,\"start\":16308},{\"end\":16483,\"start\":16428},{\"end\":16985,\"start\":16530},{\"end\":17443,\"start\":16987},{\"end\":17874,\"start\":17493},{\"end\":17942,\"start\":17905},{\"end\":18197,\"start\":18005},{\"end\":18292,\"start\":18234},{\"end\":18883,\"start\":18294},{\"end\":19157,\"start\":18908},{\"end\":19476,\"start\":19184},{\"end\":21261,\"start\":19478},{\"end\":21578,\"start\":21263},{\"end\":21758,\"start\":21696},{\"end\":22119,\"start\":21760},{\"end\":22802,\"start\":22140},{\"end\":22893,\"start\":22831},{\"end\":23991,\"start\":22969},{\"end\":24459,\"start\":24051},{\"end\":25779,\"start\":24581},{\"end\":26436,\"start\":25781},{\"end\":27314,\"start\":26438},{\"end\":27688,\"start\":27316},{\"end\":28189,\"start\":27732},{\"end\":29176,\"start\":28207},{\"end\":30074,\"start\":29178},{\"end\":32171,\"start\":30076},{\"end\":32737,\"start\":32214},{\"end\":32957,\"start\":32869},{\"end\":33661,\"start\":33030},{\"end\":33749,\"start\":33691},{\"end\":33875,\"start\":33833},{\"end\":34043,\"start\":33947},{\"end\":34676,\"start\":34174},{\"end\":34848,\"start\":34738},{\"end\":36620,\"start\":35007},{\"end\":36846,\"start\":36622},{\"end\":37778,\"start\":36911},{\"end\":38685,\"start\":37810},{\"end\":40104,\"start\":38687},{\"end\":40554,\"start\":40106},{\"end\":40717,\"start\":40556},{\"end\":41070,\"start\":40750},{\"end\":41745,\"start\":41151},{\"end\":42232,\"start\":41859},{\"end\":42267,\"start\":42234},{\"end\":42479,\"start\":42308},{\"end\":42720,\"start\":42511},{\"end\":43119,\"start\":42822},{\"end\":43616,\"start\":43149},{\"end\":45843,\"start\":43861},{\"end\":46232,\"start\":45887},{\"end\":47130,\"start\":46234},{\"end\":48359,\"start\":47132},{\"end\":49234,\"start\":48386},{\"end\":50179,\"start\":49236},{\"end\":51575,\"start\":50222},{\"end\":51741,\"start\":51577},{\"end\":53330,\"start\":51743},{\"end\":54620,\"start\":53332},{\"end\":55543,\"start\":54622}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":16307,\"start\":16220},{\"attributes\":{\"id\":\"formula_1\"},\"end\":16427,\"start\":16397},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16529,\"start\":16484},{\"attributes\":{\"id\":\"formula_3\"},\"end\":17492,\"start\":17444},{\"attributes\":{\"id\":\"formula_4\"},\"end\":17904,\"start\":17875},{\"attributes\":{\"id\":\"formula_5\"},\"end\":18004,\"start\":17943},{\"attributes\":{\"id\":\"formula_6\"},\"end\":18233,\"start\":18198},{\"attributes\":{\"id\":\"formula_7\"},\"end\":21695,\"start\":21579},{\"attributes\":{\"id\":\"formula_8\"},\"end\":22830,\"start\":22803},{\"attributes\":{\"id\":\"formula_9\"},\"end\":22968,\"start\":22894},{\"attributes\":{\"id\":\"formula_10\"},\"end\":24050,\"start\":23992},{\"attributes\":{\"id\":\"formula_11\"},\"end\":24580,\"start\":24460},{\"attributes\":{\"id\":\"formula_12\"},\"end\":32868,\"start\":32738},{\"attributes\":{\"id\":\"formula_13\"},\"end\":32994,\"start\":32958},{\"attributes\":{\"id\":\"formula_14\"},\"end\":33029,\"start\":32994},{\"attributes\":{\"id\":\"formula_15\"},\"end\":33690,\"start\":33662},{\"attributes\":{\"id\":\"formula_16\"},\"end\":33832,\"start\":33750},{\"attributes\":{\"id\":\"formula_17\"},\"end\":33946,\"start\":33876},{\"attributes\":{\"id\":\"formula_18\"},\"end\":34105,\"start\":34044},{\"attributes\":{\"id\":\"formula_19\"},\"end\":34173,\"start\":34105},{\"attributes\":{\"id\":\"formula_20\"},\"end\":34707,\"start\":34677},{\"attributes\":{\"id\":\"formula_21\"},\"end\":34737,\"start\":34707},{\"attributes\":{\"id\":\"formula_22\"},\"end\":35006,\"start\":34849},{\"attributes\":{\"id\":\"formula_23\"},\"end\":40749,\"start\":40718},{\"attributes\":{\"id\":\"formula_24\"},\"end\":41150,\"start\":41071},{\"attributes\":{\"id\":\"formula_25\"},\"end\":41858,\"start\":41746},{\"attributes\":{\"id\":\"formula_26\"},\"end\":42307,\"start\":42268},{\"attributes\":{\"id\":\"formula_27\"},\"end\":42510,\"start\":42480},{\"attributes\":{\"id\":\"formula_28\"},\"end\":42821,\"start\":42721},{\"attributes\":{\"id\":\"formula_29\"},\"end\":43148,\"start\":43120},{\"attributes\":{\"id\":\"formula_30\"},\"end\":43722,\"start\":43617},{\"attributes\":{\"id\":\"formula_31\"},\"end\":43860,\"start\":43722},{\"attributes\":{\"id\":\"formula_32\"},\"end\":45886,\"start\":45844}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":45103,\"start\":45096},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":52178,\"start\":51964},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":54718,\"start\":54709}]", "section_header": "[{\"end\":2104,\"start\":2089},{\"end\":11023,\"start\":11007},{\"end\":11325,\"start\":11288},{\"end\":15983,\"start\":15949},{\"end\":18906,\"start\":18886},{\"end\":19182,\"start\":19160},{\"end\":22138,\"start\":22122},{\"end\":27730,\"start\":27691},{\"end\":28205,\"start\":28192},{\"end\":32212,\"start\":32174},{\"end\":36909,\"start\":36849},{\"end\":37808,\"start\":37781},{\"end\":48384,\"start\":48362},{\"end\":50220,\"start\":50182},{\"end\":55553,\"start\":55545},{\"end\":55617,\"start\":55609},{\"end\":55817,\"start\":55809},{\"end\":56007,\"start\":55999},{\"end\":56209,\"start\":56201},{\"end\":57910,\"start\":57902},{\"end\":58114,\"start\":58106},{\"end\":59176,\"start\":59168},{\"end\":59486,\"start\":59478},{\"end\":59575,\"start\":59566},{\"end\":59863,\"start\":59854},{\"end\":61140,\"start\":61133},{\"end\":61364,\"start\":61359}]", "table": null, "figure_caption": "[{\"end\":55607,\"start\":55555},{\"end\":55807,\"start\":55619},{\"end\":55997,\"start\":55819},{\"end\":56199,\"start\":56009},{\"end\":56293,\"start\":56211},{\"end\":57900,\"start\":56296},{\"end\":58104,\"start\":57912},{\"end\":58725,\"start\":58116},{\"end\":59166,\"start\":58728},{\"end\":59476,\"start\":59178},{\"end\":59564,\"start\":59488},{\"end\":59852,\"start\":59578},{\"end\":60030,\"start\":59866},{\"end\":61131,\"start\":60033},{\"end\":61357,\"start\":61142}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":19540,\"start\":19534},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":24760,\"start\":24754},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":26192,\"start\":26186},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":27472,\"start\":27466},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":30236,\"start\":30227},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":30756,\"start\":30747},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":31683,\"start\":31677},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":31813,\"start\":31803},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":31851,\"start\":31841},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":31935,\"start\":31925},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":31979,\"start\":31969},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":32170,\"start\":32161},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":32415,\"start\":32409},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":37895,\"start\":37889},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":38713,\"start\":38707},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":44143,\"start\":44137},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":46231,\"start\":46225},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":46529,\"start\":46520},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":46617,\"start\":46608},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":47022,\"start\":47013},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":47036,\"start\":47027},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":48358,\"start\":48340},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":49233,\"start\":49227},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":49393,\"start\":49384},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":49939,\"start\":49930},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":50175,\"start\":50169},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":51892,\"start\":51885},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":53507,\"start\":53500}]", "bib_author_first_name": "[{\"end\":61769,\"start\":61768},{\"end\":61779,\"start\":61778},{\"end\":62164,\"start\":62163},{\"end\":62180,\"start\":62179},{\"end\":62191,\"start\":62190},{\"end\":62207,\"start\":62202},{\"end\":62480,\"start\":62479},{\"end\":62492,\"start\":62491},{\"end\":62501,\"start\":62500},{\"end\":62510,\"start\":62509},{\"end\":63203,\"start\":63202},{\"end\":63511,\"start\":63510},{\"end\":63813,\"start\":63812},{\"end\":63822,\"start\":63821},{\"end\":63831,\"start\":63830},{\"end\":64147,\"start\":64146},{\"end\":64153,\"start\":64152},{\"end\":64159,\"start\":64158},{\"end\":64360,\"start\":64359},{\"end\":64366,\"start\":64365},{\"end\":64374,\"start\":64373},{\"end\":64675,\"start\":64671},{\"end\":64681,\"start\":64680},{\"end\":64687,\"start\":64686},{\"end\":64693,\"start\":64692},{\"end\":64700,\"start\":64699},{\"end\":65090,\"start\":65089},{\"end\":65096,\"start\":65095},{\"end\":65104,\"start\":65103},{\"end\":65113,\"start\":65112},{\"end\":65120,\"start\":65119},{\"end\":65128,\"start\":65127},{\"end\":65469,\"start\":65468},{\"end\":65476,\"start\":65475},{\"end\":65484,\"start\":65483},{\"end\":65860,\"start\":65859},{\"end\":65869,\"start\":65868},{\"end\":65876,\"start\":65875},{\"end\":65878,\"start\":65877},{\"end\":65886,\"start\":65885},{\"end\":65893,\"start\":65892},{\"end\":66235,\"start\":66234},{\"end\":66244,\"start\":66243},{\"end\":66251,\"start\":66250},{\"end\":66532,\"start\":66531},{\"end\":66536,\"start\":66533},{\"end\":66774,\"start\":66773},{\"end\":66780,\"start\":66779},{\"end\":66788,\"start\":66787},{\"end\":66796,\"start\":66795},{\"end\":67165,\"start\":67164},{\"end\":67172,\"start\":67171},{\"end\":67180,\"start\":67179},{\"end\":67187,\"start\":67186},{\"end\":67538,\"start\":67537},{\"end\":67544,\"start\":67543},{\"end\":67552,\"start\":67551},{\"end\":67558,\"start\":67557},{\"end\":67821,\"start\":67820},{\"end\":67827,\"start\":67826},{\"end\":67835,\"start\":67834},{\"end\":67843,\"start\":67842},{\"end\":67850,\"start\":67849},{\"end\":68088,\"start\":68087},{\"end\":68095,\"start\":68094},{\"end\":68103,\"start\":68102},{\"end\":68105,\"start\":68104},{\"end\":68119,\"start\":68112},{\"end\":68401,\"start\":68400},{\"end\":68410,\"start\":68406},{\"end\":68416,\"start\":68415},{\"end\":68706,\"start\":68705},{\"end\":68712,\"start\":68711},{\"end\":68718,\"start\":68717},{\"end\":68727,\"start\":68726},{\"end\":68733,\"start\":68732},{\"end\":69010,\"start\":69009},{\"end\":69211,\"start\":69210},{\"end\":69220,\"start\":69219},{\"end\":69492,\"start\":69491},{\"end\":69498,\"start\":69497},{\"end\":69507,\"start\":69506},{\"end\":69513,\"start\":69512},{\"end\":69522,\"start\":69521},{\"end\":69919,\"start\":69918},{\"end\":69926,\"start\":69925},{\"end\":69934,\"start\":69933},{\"end\":69942,\"start\":69941},{\"end\":69944,\"start\":69943},{\"end\":69952,\"start\":69951},{\"end\":69954,\"start\":69953},{\"end\":69962,\"start\":69961},{\"end\":70249,\"start\":70248},{\"end\":70256,\"start\":70255},{\"end\":70264,\"start\":70263},{\"end\":70273,\"start\":70272},{\"end\":70589,\"start\":70588},{\"end\":70597,\"start\":70596},{\"end\":70610,\"start\":70609},{\"end\":70623,\"start\":70622},{\"end\":70896,\"start\":70895},{\"end\":70898,\"start\":70897},{\"end\":70911,\"start\":70910},{\"end\":70913,\"start\":70912},{\"end\":70923,\"start\":70922},{\"end\":70925,\"start\":70924},{\"end\":71288,\"start\":71287},{\"end\":71297,\"start\":71293},{\"end\":71543,\"start\":71542},{\"end\":71550,\"start\":71549},{\"end\":71559,\"start\":71558},{\"end\":71566,\"start\":71565},{\"end\":71820,\"start\":71819},{\"end\":71984,\"start\":71983},{\"end\":72248,\"start\":72247},{\"end\":72257,\"start\":72256},{\"end\":72268,\"start\":72267},{\"end\":72270,\"start\":72269},{\"end\":72587,\"start\":72586},{\"end\":72595,\"start\":72594},{\"end\":72918,\"start\":72917},{\"end\":72920,\"start\":72919},{\"end\":72930,\"start\":72929},{\"end\":72932,\"start\":72931},{\"end\":72942,\"start\":72941},{\"end\":73227,\"start\":73226},{\"end\":73236,\"start\":73235},{\"end\":73243,\"start\":73242},{\"end\":73247,\"start\":73244},{\"end\":73257,\"start\":73256},{\"end\":73259,\"start\":73258},{\"end\":73629,\"start\":73628},{\"end\":73643,\"start\":73642},{\"end\":73656,\"start\":73655},{\"end\":73658,\"start\":73657},{\"end\":74025,\"start\":74024},{\"end\":74037,\"start\":74036},{\"end\":74326,\"start\":74325},{\"end\":74335,\"start\":74334},{\"end\":74337,\"start\":74336},{\"end\":74344,\"start\":74343},{\"end\":74346,\"start\":74345},{\"end\":74629,\"start\":74628},{\"end\":74642,\"start\":74641},{\"end\":74927,\"start\":74926},{\"end\":74934,\"start\":74933},{\"end\":74936,\"start\":74935},{\"end\":75217,\"start\":75216},{\"end\":75223,\"start\":75222},{\"end\":75230,\"start\":75229},{\"end\":75236,\"start\":75235},{\"end\":75470,\"start\":75469},{\"end\":75472,\"start\":75471},{\"end\":75484,\"start\":75483},{\"end\":75496,\"start\":75495},{\"end\":75872,\"start\":75871},{\"end\":75883,\"start\":75882},{\"end\":75892,\"start\":75891},{\"end\":75902,\"start\":75901},{\"end\":75910,\"start\":75909},{\"end\":76289,\"start\":76288},{\"end\":76298,\"start\":76297},{\"end\":76584,\"start\":76583},{\"end\":76596,\"start\":76595},{\"end\":76605,\"start\":76604},{\"end\":76607,\"start\":76606},{\"end\":76950,\"start\":76949},{\"end\":76958,\"start\":76957},{\"end\":76966,\"start\":76965},{\"end\":76972,\"start\":76971},{\"end\":77289,\"start\":77288},{\"end\":77291,\"start\":77290},{\"end\":77306,\"start\":77305},{\"end\":77314,\"start\":77313},{\"end\":77678,\"start\":77677},{\"end\":77684,\"start\":77683},{\"end\":77690,\"start\":77689},{\"end\":77696,\"start\":77695},{\"end\":77705,\"start\":77704},{\"end\":78010,\"start\":78009},{\"end\":78292,\"start\":78291},{\"end\":78294,\"start\":78293},{\"end\":78308,\"start\":78307},{\"end\":78310,\"start\":78309},{\"end\":78566,\"start\":78565},{\"end\":78574,\"start\":78573},{\"end\":78576,\"start\":78575},{\"end\":78585,\"start\":78584},{\"end\":78587,\"start\":78586},{\"end\":78597,\"start\":78596},{\"end\":78599,\"start\":78598},{\"end\":78922,\"start\":78921},{\"end\":78929,\"start\":78928},{\"end\":78936,\"start\":78935},{\"end\":78943,\"start\":78942},{\"end\":79212,\"start\":79211},{\"end\":79218,\"start\":79217},{\"end\":79226,\"start\":79225},{\"end\":79233,\"start\":79232},{\"end\":79240,\"start\":79239},{\"end\":79242,\"start\":79241},{\"end\":79249,\"start\":79248},{\"end\":79612,\"start\":79611},{\"end\":79614,\"start\":79613},{\"end\":79948,\"start\":79947},{\"end\":79955,\"start\":79954},{\"end\":80172,\"start\":80171},{\"end\":80178,\"start\":80177},{\"end\":80185,\"start\":80184},{\"end\":80193,\"start\":80192},{\"end\":80200,\"start\":80199},{\"end\":80419,\"start\":80418}]", "bib_author_last_name": "[{\"end\":61776,\"start\":61770},{\"end\":61793,\"start\":61780},{\"end\":62177,\"start\":62165},{\"end\":62188,\"start\":62181},{\"end\":62200,\"start\":62192},{\"end\":62213,\"start\":62208},{\"end\":62489,\"start\":62481},{\"end\":62498,\"start\":62493},{\"end\":62507,\"start\":62502},{\"end\":62519,\"start\":62511},{\"end\":63208,\"start\":63204},{\"end\":63515,\"start\":63512},{\"end\":63819,\"start\":63814},{\"end\":63828,\"start\":63823},{\"end\":64150,\"start\":64148},{\"end\":64156,\"start\":64154},{\"end\":64162,\"start\":64160},{\"end\":64363,\"start\":64361},{\"end\":64371,\"start\":64367},{\"end\":64377,\"start\":64375},{\"end\":64678,\"start\":64676},{\"end\":64684,\"start\":64682},{\"end\":64690,\"start\":64688},{\"end\":64697,\"start\":64694},{\"end\":64706,\"start\":64701},{\"end\":65093,\"start\":65091},{\"end\":65101,\"start\":65097},{\"end\":65110,\"start\":65105},{\"end\":65117,\"start\":65114},{\"end\":65125,\"start\":65121},{\"end\":65132,\"start\":65129},{\"end\":65473,\"start\":65470},{\"end\":65481,\"start\":65477},{\"end\":65490,\"start\":65485},{\"end\":65866,\"start\":65861},{\"end\":65873,\"start\":65870},{\"end\":65883,\"start\":65879},{\"end\":65890,\"start\":65887},{\"end\":65897,\"start\":65894},{\"end\":66241,\"start\":66236},{\"end\":66248,\"start\":66245},{\"end\":66255,\"start\":66252},{\"end\":66542,\"start\":66537},{\"end\":66777,\"start\":66775},{\"end\":66785,\"start\":66781},{\"end\":66793,\"start\":66789},{\"end\":66801,\"start\":66797},{\"end\":67169,\"start\":67166},{\"end\":67177,\"start\":67173},{\"end\":67184,\"start\":67181},{\"end\":67193,\"start\":67188},{\"end\":67541,\"start\":67539},{\"end\":67549,\"start\":67545},{\"end\":67555,\"start\":67553},{\"end\":67564,\"start\":67559},{\"end\":67824,\"start\":67822},{\"end\":67832,\"start\":67828},{\"end\":67840,\"start\":67836},{\"end\":67847,\"start\":67844},{\"end\":67853,\"start\":67851},{\"end\":68092,\"start\":68089},{\"end\":68100,\"start\":68096},{\"end\":68110,\"start\":68106},{\"end\":68124,\"start\":68120},{\"end\":68404,\"start\":68402},{\"end\":68413,\"start\":68411},{\"end\":68424,\"start\":68417},{\"end\":68709,\"start\":68707},{\"end\":68715,\"start\":68713},{\"end\":68724,\"start\":68719},{\"end\":68730,\"start\":68728},{\"end\":68739,\"start\":68734},{\"end\":69013,\"start\":69011},{\"end\":69217,\"start\":69212},{\"end\":69229,\"start\":69221},{\"end\":69495,\"start\":69493},{\"end\":69504,\"start\":69499},{\"end\":69510,\"start\":69508},{\"end\":69519,\"start\":69514},{\"end\":69525,\"start\":69523},{\"end\":69923,\"start\":69920},{\"end\":69931,\"start\":69927},{\"end\":69939,\"start\":69935},{\"end\":69949,\"start\":69945},{\"end\":69959,\"start\":69955},{\"end\":69967,\"start\":69963},{\"end\":70253,\"start\":70250},{\"end\":70261,\"start\":70257},{\"end\":70270,\"start\":70265},{\"end\":70278,\"start\":70274},{\"end\":70594,\"start\":70590},{\"end\":70607,\"start\":70598},{\"end\":70620,\"start\":70611},{\"end\":70630,\"start\":70624},{\"end\":70908,\"start\":70899},{\"end\":70920,\"start\":70914},{\"end\":70930,\"start\":70926},{\"end\":71291,\"start\":71289},{\"end\":71300,\"start\":71298},{\"end\":71547,\"start\":71544},{\"end\":71556,\"start\":71551},{\"end\":71563,\"start\":71560},{\"end\":71569,\"start\":71567},{\"end\":71831,\"start\":71821},{\"end\":71995,\"start\":71985},{\"end\":72254,\"start\":72249},{\"end\":72265,\"start\":72258},{\"end\":72276,\"start\":72271},{\"end\":72592,\"start\":72588},{\"end\":72604,\"start\":72596},{\"end\":72927,\"start\":72921},{\"end\":72939,\"start\":72933},{\"end\":72949,\"start\":72943},{\"end\":73233,\"start\":73228},{\"end\":73240,\"start\":73237},{\"end\":73254,\"start\":73248},{\"end\":73270,\"start\":73260},{\"end\":73640,\"start\":73630},{\"end\":73653,\"start\":73644},{\"end\":73665,\"start\":73659},{\"end\":74034,\"start\":74026},{\"end\":74042,\"start\":74038},{\"end\":74332,\"start\":74327},{\"end\":74341,\"start\":74338},{\"end\":74351,\"start\":74347},{\"end\":74639,\"start\":74630},{\"end\":74653,\"start\":74643},{\"end\":74931,\"start\":74928},{\"end\":74943,\"start\":74937},{\"end\":75220,\"start\":75218},{\"end\":75227,\"start\":75224},{\"end\":75233,\"start\":75231},{\"end\":75241,\"start\":75237},{\"end\":75481,\"start\":75473},{\"end\":75493,\"start\":75485},{\"end\":75502,\"start\":75497},{\"end\":75880,\"start\":75873},{\"end\":75889,\"start\":75884},{\"end\":75899,\"start\":75893},{\"end\":75907,\"start\":75903},{\"end\":75924,\"start\":75911},{\"end\":76295,\"start\":76290},{\"end\":76304,\"start\":76299},{\"end\":76593,\"start\":76585},{\"end\":76602,\"start\":76597},{\"end\":76614,\"start\":76608},{\"end\":76955,\"start\":76951},{\"end\":76963,\"start\":76959},{\"end\":76969,\"start\":76967},{\"end\":76977,\"start\":76973},{\"end\":77303,\"start\":77292},{\"end\":77311,\"start\":77307},{\"end\":77318,\"start\":77315},{\"end\":77681,\"start\":77679},{\"end\":77687,\"start\":77685},{\"end\":77693,\"start\":77691},{\"end\":77702,\"start\":77697},{\"end\":77709,\"start\":77706},{\"end\":78020,\"start\":78011},{\"end\":78305,\"start\":78295},{\"end\":78317,\"start\":78311},{\"end\":78571,\"start\":78567},{\"end\":78582,\"start\":78577},{\"end\":78594,\"start\":78588},{\"end\":78610,\"start\":78600},{\"end\":78926,\"start\":78923},{\"end\":78933,\"start\":78930},{\"end\":78940,\"start\":78937},{\"end\":78946,\"start\":78944},{\"end\":79215,\"start\":79213},{\"end\":79223,\"start\":79219},{\"end\":79230,\"start\":79227},{\"end\":79237,\"start\":79234},{\"end\":79246,\"start\":79243},{\"end\":79253,\"start\":79250},{\"end\":79620,\"start\":79615},{\"end\":79952,\"start\":79949},{\"end\":79960,\"start\":79956},{\"end\":80175,\"start\":80173},{\"end\":80182,\"start\":80179},{\"end\":80190,\"start\":80186},{\"end\":80197,\"start\":80194},{\"end\":80205,\"start\":80201},{\"end\":80422,\"start\":80420}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":9710544},\"end\":62097,\"start\":61654},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":125668101},\"end\":62402,\"start\":62099},{\"attributes\":{\"id\":\"b2\"},\"end\":62797,\"start\":62404},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":124124078},\"end\":63034,\"start\":62799},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":96185770},\"end\":63454,\"start\":63036},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":125717966},\"end\":63665,\"start\":63456},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":25291441},\"end\":64074,\"start\":63667},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":52073194},\"end\":64321,\"start\":64076},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":493320},\"end\":64543,\"start\":64323},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":206753134},\"end\":64976,\"start\":64545},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":125585760},\"end\":65403,\"start\":64978},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":30167483},\"end\":65750,\"start\":65405},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":2880403},\"end\":66116,\"start\":65752},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":120585341},\"end\":66480,\"start\":66118},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":119797816},\"end\":66668,\"start\":66482},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":125122786},\"end\":67021,\"start\":66670},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":45708458},\"end\":67445,\"start\":67023},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":205432776},\"end\":67755,\"start\":67447},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":207113667},\"end\":68030,\"start\":67757},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":18649677},\"end\":68331,\"start\":68032},{\"attributes\":{\"doi\":\"arXiv:1804.06992\",\"id\":\"b20\"},\"end\":68620,\"start\":68333},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":71142966},\"end\":68929,\"start\":68622},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":199688641},\"end\":69167,\"start\":68931},{\"attributes\":{\"doi\":\"arXiv:1411.1784\",\"id\":\"b23\"},\"end\":69349,\"start\":69169},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":199466224},\"end\":69834,\"start\":69351},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":46849537},\"end\":70176,\"start\":69836},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":7573433},\"end\":70538,\"start\":70178},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":34858249},\"end\":70799,\"start\":70540},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":216738},\"end\":71224,\"start\":70801},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":5060429},\"end\":71485,\"start\":71226},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":13602866},\"end\":71764,\"start\":71487},{\"attributes\":{\"doi\":\"arXiv:1701.00160\",\"id\":\"b31\"},\"end\":71952,\"start\":71766},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":1033682},\"end\":72176,\"start\":71954},{\"attributes\":{\"doi\":\"arXiv:1701.05957\",\"id\":\"b33\"},\"end\":72480,\"start\":72178},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":5794626},\"end\":72840,\"start\":72482},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":975170},\"end\":73182,\"start\":72842},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":9433631},\"end\":73561,\"start\":73184},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":195908774},\"end\":73907,\"start\":73563},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":14462436},\"end\":74248,\"start\":73909},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":2976748},\"end\":74556,\"start\":74250},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":31855940},\"end\":74827,\"start\":74558},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":199408152},\"end\":75160,\"start\":74829},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":110532218},\"end\":75395,\"start\":75162},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":14014903},\"end\":75761,\"start\":75397},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":33156071},\"end\":76179,\"start\":75763},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":45998271},\"end\":76491,\"start\":76181},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":14593153},\"end\":76818,\"start\":76493},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":41156725},\"end\":77206,\"start\":76820},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":2180547},\"end\":77588,\"start\":77208},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":53248703},\"end\":77905,\"start\":77590},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":119424378},\"end\":78243,\"start\":77907},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":9186472},\"end\":78489,\"start\":78245},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":207761262},\"end\":78843,\"start\":78491},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":41804650},\"end\":79129,\"start\":78845},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":124008754},\"end\":79489,\"start\":79131},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":14701215},\"end\":79868,\"start\":79491},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":1890730},\"end\":80139,\"start\":79870},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":125197863},\"end\":80331,\"start\":80141},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":197435418},\"end\":80589,\"start\":80333}]", "bib_title": "[{\"end\":61766,\"start\":61654},{\"end\":62161,\"start\":62099},{\"end\":62872,\"start\":62799},{\"end\":63200,\"start\":63036},{\"end\":63508,\"start\":63456},{\"end\":63810,\"start\":63667},{\"end\":64144,\"start\":64076},{\"end\":64357,\"start\":64323},{\"end\":64669,\"start\":64545},{\"end\":65087,\"start\":64978},{\"end\":65466,\"start\":65405},{\"end\":65857,\"start\":65752},{\"end\":66232,\"start\":66118},{\"end\":66529,\"start\":66482},{\"end\":66771,\"start\":66670},{\"end\":67162,\"start\":67023},{\"end\":67535,\"start\":67447},{\"end\":67818,\"start\":67757},{\"end\":68085,\"start\":68032},{\"end\":68703,\"start\":68622},{\"end\":69007,\"start\":68931},{\"end\":69489,\"start\":69351},{\"end\":69916,\"start\":69836},{\"end\":70246,\"start\":70178},{\"end\":70586,\"start\":70540},{\"end\":70893,\"start\":70801},{\"end\":71285,\"start\":71226},{\"end\":71540,\"start\":71487},{\"end\":71981,\"start\":71954},{\"end\":72584,\"start\":72482},{\"end\":72915,\"start\":72842},{\"end\":73224,\"start\":73184},{\"end\":73626,\"start\":73563},{\"end\":74022,\"start\":73909},{\"end\":74323,\"start\":74250},{\"end\":74626,\"start\":74558},{\"end\":74924,\"start\":74829},{\"end\":75214,\"start\":75162},{\"end\":75467,\"start\":75397},{\"end\":75869,\"start\":75763},{\"end\":76286,\"start\":76181},{\"end\":76581,\"start\":76493},{\"end\":76947,\"start\":76820},{\"end\":77286,\"start\":77208},{\"end\":77675,\"start\":77590},{\"end\":78007,\"start\":77907},{\"end\":78289,\"start\":78245},{\"end\":78563,\"start\":78491},{\"end\":78919,\"start\":78845},{\"end\":79209,\"start\":79131},{\"end\":79609,\"start\":79491},{\"end\":79945,\"start\":79870},{\"end\":80169,\"start\":80141},{\"end\":80416,\"start\":80333}]", "bib_author": "[{\"end\":61778,\"start\":61768},{\"end\":61795,\"start\":61778},{\"end\":62179,\"start\":62163},{\"end\":62190,\"start\":62179},{\"end\":62202,\"start\":62190},{\"end\":62215,\"start\":62202},{\"end\":62491,\"start\":62479},{\"end\":62500,\"start\":62491},{\"end\":62509,\"start\":62500},{\"end\":62521,\"start\":62509},{\"end\":63210,\"start\":63202},{\"end\":63517,\"start\":63510},{\"end\":63821,\"start\":63812},{\"end\":63830,\"start\":63821},{\"end\":63834,\"start\":63830},{\"end\":64152,\"start\":64146},{\"end\":64158,\"start\":64152},{\"end\":64164,\"start\":64158},{\"end\":64365,\"start\":64359},{\"end\":64373,\"start\":64365},{\"end\":64379,\"start\":64373},{\"end\":64680,\"start\":64671},{\"end\":64686,\"start\":64680},{\"end\":64692,\"start\":64686},{\"end\":64699,\"start\":64692},{\"end\":64708,\"start\":64699},{\"end\":65095,\"start\":65089},{\"end\":65103,\"start\":65095},{\"end\":65112,\"start\":65103},{\"end\":65119,\"start\":65112},{\"end\":65127,\"start\":65119},{\"end\":65134,\"start\":65127},{\"end\":65475,\"start\":65468},{\"end\":65483,\"start\":65475},{\"end\":65492,\"start\":65483},{\"end\":65868,\"start\":65859},{\"end\":65875,\"start\":65868},{\"end\":65885,\"start\":65875},{\"end\":65892,\"start\":65885},{\"end\":65899,\"start\":65892},{\"end\":66243,\"start\":66234},{\"end\":66250,\"start\":66243},{\"end\":66257,\"start\":66250},{\"end\":66544,\"start\":66531},{\"end\":66779,\"start\":66773},{\"end\":66787,\"start\":66779},{\"end\":66795,\"start\":66787},{\"end\":66803,\"start\":66795},{\"end\":67171,\"start\":67164},{\"end\":67179,\"start\":67171},{\"end\":67186,\"start\":67179},{\"end\":67195,\"start\":67186},{\"end\":67543,\"start\":67537},{\"end\":67551,\"start\":67543},{\"end\":67557,\"start\":67551},{\"end\":67566,\"start\":67557},{\"end\":67826,\"start\":67820},{\"end\":67834,\"start\":67826},{\"end\":67842,\"start\":67834},{\"end\":67849,\"start\":67842},{\"end\":67855,\"start\":67849},{\"end\":68094,\"start\":68087},{\"end\":68102,\"start\":68094},{\"end\":68112,\"start\":68102},{\"end\":68126,\"start\":68112},{\"end\":68406,\"start\":68400},{\"end\":68415,\"start\":68406},{\"end\":68426,\"start\":68415},{\"end\":68711,\"start\":68705},{\"end\":68717,\"start\":68711},{\"end\":68726,\"start\":68717},{\"end\":68732,\"start\":68726},{\"end\":68741,\"start\":68732},{\"end\":69015,\"start\":69009},{\"end\":69219,\"start\":69210},{\"end\":69231,\"start\":69219},{\"end\":69497,\"start\":69491},{\"end\":69506,\"start\":69497},{\"end\":69512,\"start\":69506},{\"end\":69521,\"start\":69512},{\"end\":69527,\"start\":69521},{\"end\":69925,\"start\":69918},{\"end\":69933,\"start\":69925},{\"end\":69941,\"start\":69933},{\"end\":69951,\"start\":69941},{\"end\":69961,\"start\":69951},{\"end\":69969,\"start\":69961},{\"end\":70255,\"start\":70248},{\"end\":70263,\"start\":70255},{\"end\":70272,\"start\":70263},{\"end\":70280,\"start\":70272},{\"end\":70596,\"start\":70588},{\"end\":70609,\"start\":70596},{\"end\":70622,\"start\":70609},{\"end\":70632,\"start\":70622},{\"end\":70910,\"start\":70895},{\"end\":70922,\"start\":70910},{\"end\":70932,\"start\":70922},{\"end\":71293,\"start\":71287},{\"end\":71302,\"start\":71293},{\"end\":71549,\"start\":71542},{\"end\":71558,\"start\":71549},{\"end\":71565,\"start\":71558},{\"end\":71571,\"start\":71565},{\"end\":71833,\"start\":71819},{\"end\":71997,\"start\":71983},{\"end\":72256,\"start\":72247},{\"end\":72267,\"start\":72256},{\"end\":72278,\"start\":72267},{\"end\":72594,\"start\":72586},{\"end\":72606,\"start\":72594},{\"end\":72929,\"start\":72917},{\"end\":72941,\"start\":72929},{\"end\":72951,\"start\":72941},{\"end\":73235,\"start\":73226},{\"end\":73242,\"start\":73235},{\"end\":73256,\"start\":73242},{\"end\":73272,\"start\":73256},{\"end\":73642,\"start\":73628},{\"end\":73655,\"start\":73642},{\"end\":73667,\"start\":73655},{\"end\":74036,\"start\":74024},{\"end\":74044,\"start\":74036},{\"end\":74334,\"start\":74325},{\"end\":74343,\"start\":74334},{\"end\":74353,\"start\":74343},{\"end\":74641,\"start\":74628},{\"end\":74655,\"start\":74641},{\"end\":74933,\"start\":74926},{\"end\":74945,\"start\":74933},{\"end\":75222,\"start\":75216},{\"end\":75229,\"start\":75222},{\"end\":75235,\"start\":75229},{\"end\":75243,\"start\":75235},{\"end\":75483,\"start\":75469},{\"end\":75495,\"start\":75483},{\"end\":75504,\"start\":75495},{\"end\":75882,\"start\":75871},{\"end\":75891,\"start\":75882},{\"end\":75901,\"start\":75891},{\"end\":75909,\"start\":75901},{\"end\":75926,\"start\":75909},{\"end\":76297,\"start\":76288},{\"end\":76306,\"start\":76297},{\"end\":76595,\"start\":76583},{\"end\":76604,\"start\":76595},{\"end\":76616,\"start\":76604},{\"end\":76957,\"start\":76949},{\"end\":76965,\"start\":76957},{\"end\":76971,\"start\":76965},{\"end\":76979,\"start\":76971},{\"end\":77305,\"start\":77288},{\"end\":77313,\"start\":77305},{\"end\":77320,\"start\":77313},{\"end\":77683,\"start\":77677},{\"end\":77689,\"start\":77683},{\"end\":77695,\"start\":77689},{\"end\":77704,\"start\":77695},{\"end\":77711,\"start\":77704},{\"end\":78022,\"start\":78009},{\"end\":78307,\"start\":78291},{\"end\":78319,\"start\":78307},{\"end\":78573,\"start\":78565},{\"end\":78584,\"start\":78573},{\"end\":78596,\"start\":78584},{\"end\":78612,\"start\":78596},{\"end\":78928,\"start\":78921},{\"end\":78935,\"start\":78928},{\"end\":78942,\"start\":78935},{\"end\":78948,\"start\":78942},{\"end\":79217,\"start\":79211},{\"end\":79225,\"start\":79217},{\"end\":79232,\"start\":79225},{\"end\":79239,\"start\":79232},{\"end\":79248,\"start\":79239},{\"end\":79255,\"start\":79248},{\"end\":79622,\"start\":79611},{\"end\":79954,\"start\":79947},{\"end\":79962,\"start\":79954},{\"end\":80177,\"start\":80171},{\"end\":80184,\"start\":80177},{\"end\":80192,\"start\":80184},{\"end\":80199,\"start\":80192},{\"end\":80207,\"start\":80199},{\"end\":80424,\"start\":80418}]", "bib_venue": "[{\"end\":62890,\"start\":62886},{\"end\":69575,\"start\":69555},{\"end\":70360,\"start\":70324},{\"end\":71010,\"start\":70975},{\"end\":72061,\"start\":72035},{\"end\":73003,\"start\":72981},{\"end\":73376,\"start\":73328},{\"end\":73731,\"start\":73705},{\"end\":77400,\"start\":77364},{\"end\":61849,\"start\":61795},{\"end\":62226,\"start\":62215},{\"end\":62477,\"start\":62404},{\"end\":62884,\"start\":62874},{\"end\":63221,\"start\":63210},{\"end\":63536,\"start\":63517},{\"end\":63845,\"start\":63834},{\"end\":64175,\"start\":64164},{\"end\":64404,\"start\":64379},{\"end\":64730,\"start\":64708},{\"end\":65154,\"start\":65134},{\"end\":65538,\"start\":65492},{\"end\":65910,\"start\":65899},{\"end\":66276,\"start\":66257},{\"end\":66550,\"start\":66544},{\"end\":66822,\"start\":66803},{\"end\":67209,\"start\":67195},{\"end\":67577,\"start\":67566},{\"end\":67869,\"start\":67855},{\"end\":68151,\"start\":68126},{\"end\":68398,\"start\":68333},{\"end\":68752,\"start\":68741},{\"end\":69026,\"start\":69015},{\"end\":69208,\"start\":69169},{\"end\":69553,\"start\":69527},{\"end\":69980,\"start\":69969},{\"end\":70322,\"start\":70280},{\"end\":70643,\"start\":70632},{\"end\":70973,\"start\":70932},{\"end\":71327,\"start\":71302},{\"end\":71596,\"start\":71571},{\"end\":71817,\"start\":71766},{\"end\":72033,\"start\":71997},{\"end\":72245,\"start\":72178},{\"end\":72631,\"start\":72606},{\"end\":72979,\"start\":72951},{\"end\":73326,\"start\":73272},{\"end\":73703,\"start\":73667},{\"end\":74055,\"start\":74044},{\"end\":74375,\"start\":74353},{\"end\":74666,\"start\":74655},{\"end\":74964,\"start\":74945},{\"end\":75255,\"start\":75243},{\"end\":75553,\"start\":75504},{\"end\":75940,\"start\":75926},{\"end\":76317,\"start\":76306},{\"end\":76628,\"start\":76616},{\"end\":76990,\"start\":76979},{\"end\":77362,\"start\":77320},{\"end\":77718,\"start\":77711},{\"end\":78042,\"start\":78022},{\"end\":78337,\"start\":78319},{\"end\":78637,\"start\":78612},{\"end\":78959,\"start\":78948},{\"end\":79278,\"start\":79255},{\"end\":79649,\"start\":79622},{\"end\":79979,\"start\":79962},{\"end\":80212,\"start\":80207},{\"end\":80438,\"start\":80424}]"}}}, "year": 2023, "month": 12, "day": 17}