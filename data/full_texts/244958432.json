{"id": 244958432, "updated": "2023-04-05 23:17:25.558", "metadata": {"title": "Test-Time Classi\ufb01er Adjustment Module for Model-Agnostic Domain Generalization", "authors": "[{\"first\":\"Yusuke\",\"last\":\"Iwasawa\",\"middle\":[]},{\"first\":\"Yutaka\",\"last\":\"Matsuo\",\"middle\":[]}]", "venue": "NeurIPS", "journal": "2427-2440", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "This paper presents a new algorithm for domain generalization (DG), test-time template adjuster (T3A) , aiming to robustify a model to unknown distribution shift. Unlike existing methods that focus on training phase , our method focuses test phase , i.e., correcting its prediction by itself during test time. Speci\ufb01cally, T3A adjusts a trained linear classi\ufb01er (the last layer of deep neural networks) with the following procedure: (1) compute a pseudo-prototype representation for each class using online unlabeled data augmented by the base classi\ufb01er trained in the source domains, (2) and then classify each sample based on its distance to the pseudo-prototypes. T3A is back-propagation-free and modi\ufb01es only the linear layer; therefore, the increase in computational cost during inference is negligible and avoids the catastrophic failure might caused by stochastic optimization. Despite its simplicity, T3A can leverage knowledge about the target domain by using off-the-shelf test-time data and improve performance. We tested our method on four domain generalization benchmarks, namely PACS, VLCS, Of\ufb01ceHome, and TerraIncognita, along with various backbone networks including ResNet18, ResNet50, Big Transfer (BiT), Vision Transformers (ViT), and MLP-Mixer. The results show T3A stably improves performance on unseen domains across choices of backbone networks, and outperforms existing domain generalization methods.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/IwasawaM21", "doi": null}}, "content": {"source": {"pdf_hash": "d6ee9491fbc8d000487f0e0fc0c4309eca87aa1b", "pdf_src": "ScienceParsePlus", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "aefd6fc2cb0df17d372544931685469803d849bf", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/d6ee9491fbc8d000487f0e0fc0c4309eca87aa1b.txt", "contents": "\nTest-Time Classifier Adjustment Module for Model-Agnostic Domain Generalization\n\n\nYusuke Iwasawa iwasawa@weblab.t.u-tokyo.ac.jp \nThe University of Tokyo\nThe University of Tokyo\n\n\nYutaka Matsuo matsuo@weblab.t.u-tokyo.ac.jp \nThe University of Tokyo\nThe University of Tokyo\n\n\nTest-Time Classifier Adjustment Module for Model-Agnostic Domain Generalization\n\nThis paper presents a new algorithm for domain generalization (DG), test-time template adjuster (T3A), aiming to robustify a model to unknown distribution shift. Unlike existing methods that focus on training phase, our method focuses test phase, i.e., correcting its prediction by itself during test time. Specifically, T3A adjusts a trained linear classifier (the last layer of deep neural networks) with the following procedure: (1) compute a pseudo-prototype representation for each class using online unlabeled data augmented by the base classifier trained in the source domains, (2) and then classify each sample based on its distance to the pseudoprototypes. T3A is back-propagation-free and modifies only the linear layer; therefore, the increase in computational cost during inference is negligible and avoids the catastrophic failure might caused by stochastic optimization. Despite its simplicity, T3A can leverage knowledge about the target domain by using off-the-shelf test-time data and improve performance. We tested our method on four domain generalization benchmarks, namely PACS, VLCS, OfficeHome, and TerraIncognita, along with various backbone networks including ResNet18, ResNet50, Big Transfer (BiT), Vision Transformers (ViT), and MLP-Mixer. The results show T3A stably improves performance on unseen domains across choices of backbone networks, and outperforms existing domain generalization methods.\n\nIntroduction\n\nDeep neural networks often fail to generalize to out-of-distribution samples. Accuracy suffers when the model performs under conditions different to those of training, such as variations in light [8], weather [51], object poses [2], textures [16], or object backgrounds [5]. Nevertheless, the model may be deployed to different conditions in practical situations; thus, some countermeasures are needed.\n\nOver the past decade, various studies have focused on training a generalizable model to unseen domains given a dataset consisting of several source domains. This setting is usually denoted as domain generalization (DG) [6,61]. Domain generalization operates under the assumption that one can improve robustness to domain shift by incorporating the structure common to multiple domains. For example, domain-invariant feature learning constrains the representation to be invariant to domain shifts [14,45,29]. Other methods use meta-learning [19] to learn how to regularize the model to improve the robustness [28,4,31]. However, despite significant work on this front, machine learning systems are still vulnerable to domain shifts even after using the above methods during training. Notably, recent large-scale benchmarks [17] show that many approaches do not provide significant improvement compared to simple supervised learning, i.e., empirical risk minimization (ERM), with a proper and practical experimental setup. It suggests that the setup in its current state may be too difficult, and a different approach might be needed. a DG setup, the existing domain generalization algorithms focus on how to use labeled data from multiple-source domains. However, at test-time the model always has access to test data from the target domain. Although the available data is constrained to be (1) unlabeled and (2) only available online (models can not know all test cases in advance), this data provides clue about the target distribution that is not available during training. It is natural to ask the question: How can we use the off-the-shelf unlabeled data available at test-time to increase performance on the target domain?\n\nIt is worth emphasizing that our setting is different from the transductive setting [49,22,57] where all test cases are known in advance, even though we use test data for adjustment. When testing, the model is usually deployed in some environment, and must work well on various samples that will appear continuously. Similarly, the deployed model usually needs to make correct predictions at that moment; there is no point in going back in time and correcting the predictions. Therefore, it is desirable that adjustment and inference be performed at the same time, not offline after a large amount of data has been accumulated. Looking beyond domain generalization, some recent studies suggest optimizing the model during test time using objective function defined only by unsupervised data (e.g, prediction entropy) [34,52]. However, updating parameters using stochastic gradient descent (SGD) increases computational costs and harm inference throughput. In addition, data available at test time is limited, and stochastic optimization can lead to catastrophic failure.\n\nTo this end, we present test-time templates adjuster (T3A), which adjusts the linear classifier (the last layer of deep neural networks) at test-time. T3A adjusts the weights of the linear classifier as the following optimization-free procedure: (1) create a pseudo-prototype for each class using online unlabeled data and the classifier trained in the source domains, (2) and then classify each sample based on its distance to the pseudo-prototype. This procedure makes the adjusted decision boundary avoid the high-data density region on the target domain and reduce the ambiguity (entropy) of predictions, which is known to be connected to classification error [52]. Since T3A does not alter the training phase, it can be used together with existing DG algorithms. Moreover, it can be used together with any classification model since it only adjusts the linear classifier on top of the representations. Some readers may wonder how effective it is to modify only the linear classifier while freezing the representation itself. Later in this paper (Section 3.2), we empirically demonstrate that this modification is indeed beneficial.\n\nWe evaluate our method on multiple standard domain generalization benchmarks, namely VLCS [12], PACS [27], OfficeHome [50], and TerraIncognita [5]. We compare our method with (1) various DG algorithms reported in [17] and (2) Tent [52] that minimizes the prediction entropy at test time using SGD. With the standard ResNet50 backbone [18], T3A improves ERM by 1.5 points on average accuracy over four dataset, and outperforms most existing DG algorithms. Furthermore, we evaluated our method with 10 different backbone networks, including residual networks (resnet18 and resnet50), big transfer (BiT-M-R50x3, BiT-M-R101x3, and BiT-M-R152x2 [24]), vision transformers (ViT-B16, ViT-L16, Hybrid ViT [9], DeiT [48]), and MLP-Mixer (Mixer-L16) [47]. The results show that T3A gives a statistically significant performance gain against ERM on all backbone networks.\n\n\nPreliminary and Related Work\n\n\nDomain Generalization\n\nProblem setup Following [6], we assume multiple datasets\nD d = {(x d i , y d i )} n d i=1 collected from several different domains d \u2208 {1, \u00b7 \u00b7 \u00b7 , d tr }.\nThe dataset D d from domain d contains identically and independently distributed samples characterized by some probability distribution P d (X, Y ), where X and Y are random variables of input and target, respectively. Then, our goal is to develop a predictor f (X) that performs well on some unseen test domain, which is characterized by a different probability distribution P (X, Y ) = P d (X, Y ) for all d \u2208 {1, \u00b7 \u00b7 \u00b7 , d tr }. Note that one can not assume the target distribution during training, e.g., no data about the target distribution is available at the time. Therefore, the predictor is usually trained on datasets from several source domains. For example, the predictor can be trained by minimizing the empirical risk:\narg min \u03c6 1 d dtr d=1 1 n d n d i=1 (f (x d i ), y d i ),(1)\nwhere \u03c6 is the set of the parameters of the function, and is a loss function measuring prediction error. In the rest of this paper, optimizing the predictor with eq. 1 is called ERM. In a real application, the model will be deployed after training and expected to classify the data in an online manner. For benchmarking the algorithm, given a dataset containing n d domains, we usually use the leaveone-domain-out procedure, which uses a single domain as a test domain and the others as training domains. The procedure is repeated n d times, changing the test domain every time.\n\nAlgorithms A central branch of DG algorithm is domain-invariant feature learning, which explicitly reduces the domain gaps on a space of latent representations. For example, [14] proposed domainadversarial networks (DANN) which measure the domain gaps via an external domain classifier. CORAL [45] align the second-order statistics of representations among different domains. [29] uses maximum mean discrepancy (MMD) to measure the domain gap. Many extension have been proposed [33,1,21], but they are all the same in that they enhance domain invariance. Another branch are meta-learning-based methods [28,4,31], which divide the available domains into meta-train-domains and meta-test-domain and regulate the model trained in meta-train-domains to be useful for the meta-test-domain. Invariant risks minimization [3] regularizes ERM with a gradient normalization penalty over a dummy classifier. Several studies propose to augment the data using mixup [59] between two source domains, which implicitly enhances invariance to domain shifts [56,58,53].\n\nKey differences As briefly mentioned above, existing domain generalization algorithms focus on the training phase; how to regularize the predictor using the knowledge from multiple source domains. Our work focuses on the test phase; how to adjust the model using online and unlabeled data, which can characterize the target distribution. Note that proposed method works fully online; It does not require access to offline unlabeled data, and therefore can be compared fairly with existing DG methods\n\n\nOther Related Work\n\nUnsupervised domain adaptation. Our work is related to unsupervised domain adaptation (UDA) [37,38,55] as both methods aim to adapt a model given unsupervised data. However, our work primarily differs from UDA in that UDA focuses on adapting during training, while we focus on adapting during testing. In other words, UDA assumes we can access labeled data from the source domain and (unlabeled) data from the target domain at the same time, which is not always possible.\n\nSource-free domain adaptation. Among them, recent \"source-free\" setups are particularly similar to our setting [30,25,34]. In these setups, source data is not needed during the adaptation phase, and the model is adapted using the unlabeled data solely from the target domain. However, this adaptation is usually made in an offline manner, i.e., these source-free methods optimize offline with multiple losses for multiple epochs. Our method adjusts the classifier in an online-manner, and therefore is suitable for a domain generalization setup where the trained model is assumed to be deployed.\n\nTest-time adaptation. Regarding the problem setup, our work is most closely related to testtime adaptation [52] or test-time training [46]. Notably, [52] proposes fully test-time adaptation, which modulates the BN parameter by minimizing the prediction entropy using stochastic gradient descent. The concept of test-time adaptation is very similar to our work and can be used in domain generalization, however, it has not been fully investigated under the domain generalization setup. Moreover, recent architectures do not employ batch normalization either on pre-training (mainly to avoid the large memory usage required by BN) or fine-tuning phase (for improving performance). Besides, minimizing the prediction entropy using SGD could lead to trivial solutions, such as being biased to predict only a particular class. In comparison, (1) our method can be used together with any classification models since it only adjusts the linear classifier on the top of the representations, (2) our method alleviates catastrophic forgetting due to not using SGD during test-time.\n\nPrototypical networks. Before deep learning become popular, the prototype-based classifier is well-investigated in the context of semi-supervised learning [10], continual learning [35,39], and few-shot learning [44]. Our work is most similar to prototypical networks [44], which combine prototypical classifier and deep neural networks as with our method. However, the use-cases mentioned above of prototypical networks assume access to a few labeled data from the same domain, which differs in our case. To handle the difference, we combine prototypical networks with pseudolabeling techniques, which are often used in domain transfer literature [42]. Besides, connection to entropy minimization is a new perspective introduced in this paper.\n\nAlgorithm 1 Algorithm of T3A for prediction.\n\nInput: Feature extractor f \u03b8 , the batch of input B, and support sets S k available at this point. Output: Prediction for all x \u2208 B, where x \u223c P (X). # Step1. Adjust the template for each class using the B.\nfor x \u2208 B d\u00f4 y = arg max q \u03c9 (Y = y k |f \u03b8 (x)) (eq. 2) S k = S k \u222a { f \u03b8 (x) f \u03b8 (x) } for y k =\u0177 (eq. 3) end for\nFilter support sets with eq. 6 # Step2. Predict based on the distance between the adjusted template. return arg max \u03b3(Y = y k |f \u03b8 (x)) for all x \u2208 B (eq. 4)\n\n\nProposal: Optimization-Free Test-Time Classifier Adjustment Module\n\nWe propose to replace the output layer of the predictor trained on source-domains (i.e., linear classifier) to a pseudo-prototypical classifier, whose prototype features are adjusted during test time while fixing the features already trained on the source domains. We call our method T3A, for Test-Time Templates Adjuster. We first explain the detailed algorithm (Section 3.1), and explain how and why it works (Section 3.2). Algorithm 1 outlines the procedure.\n\n\nAlgorithm\n\nWe assume the predictor is deep neural networks (DNN) obtained by some learning algorithm (e.g., ERM, DANN, CORAL, etc.) using data from source domains. For convenience, the entire DNN is divided into a linear classifier q \u03c9 for the last layer and a feature extractor f \u03b8 for the rest, where \u03c9 and \u03b8 are the parameters of neural networks. In usual domain generalization setup, f \u03b8 and q \u03c9 are used to predict data from the test domain. For new data x, the prediction is given by taking the argmax over the following approximated probability distribution:\narg max y k q \u03c9 (Y = y k |f \u03b8 (x)) = exp(z \u00b7 \u03c9 k ) j exp(z \u00b7 \u03c9 j ) ,(2)\nwhere z = f \u03b8 (x), \u03c9 k \u2208 R z dim is the k-th element of the weights matrix in \u03c9, and z dim is the dimension of z, which depends on the feature extractor f \u03b8 . In this prediction, \u03c9 k works as the template of representation for the class k, and prediction is done by measuring the distance (dot product) between the template and the representation of the input data. Since this template was trained in the source domain, there is no guarantee that it will be a good template in the target domain.\n\nT3A adjusts the templates during test-time. Assume we have (batch of) test-data x at time t drawn from target distribution P (X) = P d (X) for all d \u2208 {1, \u00b7 \u00b7 \u00b7 , d tr }. As each prototype should be related to some class, we first augment the input data x via pseudo label\u0177, which is obtained via eq. 2. Then, we update a support set S k t as follows:\nS k t = S k t\u22121 \u222a { f \u03b8 (x) f \u03b8 (x) } if\u0177 = y k S k t\u22121 else,(3)\nwhere a represents the L2 norm of the vector a and S k 0 = { \u03c9 k \u03c9 k }. If the input data contains multiple samples at the same time (e.g., a batch of data), the above procedure is repeated for each sample in the batch. Then, prediction is done by taking the argmax over the following adjusted probability distribution:\narg max y k \u03b3 c (Y = y k |f \u03b8 (x)) = exp(z \u00b7 c k ) j exp(z \u00b7 c j ) ,(4)\nwhere c k are the centroids of S k : Note that, this procedure will be repeated each time new data arrives, without discarding the support sets, which may make infeasible to retain all past data as in eq. 3. Also, some pseudo-labels are assigned to the wrong class, and using this data is not desirable as it adds noise to the templates and may deteriorate performance. To avoid this issue, we use the prediction entropy H \u03c9 (\u0176 |z) = \u2212 k q \u03c9 (\u0176 = y k |z) log q \u03c9 (\u0176 = y k |z) to filter unreliable pseudo-labeled data. Specifically, before making a prediction using eq. 4, only a part of the support set is restored as follows:\nc k = 1 |S k | z\u2208S k t z.(5)S k t = {z | z \u2208 S k t , H \u03c9 (\u0176 |z) \u2264 \u03b1 k },(6)\nwhere \u03b1 k is the M -th largest entropy of the support set S k t (M is a hyperparameter).\n\n\nRemarks\n\nRemark 1: T3A implicitly reduces prediction entropy. As prior works suggest [52], the prediction entropy is often related to an error, as more confident predictions tend to be more correct. Figure  1-a shows that the prediction entropy also characterizes the difficulty in DG setup; entropy in the unseen domain tends to be greater than entropy in the seen domains. To be more specific, we first trained ResNet50 on the source domain by ERM. The training was done in leave-one-domain-out manner, and we conducted three experiments with a different seed each time. We used four standard datasets in domain generalization (VLCS [12], PACS [27], OfficeHome [50], and TerraIncognita [5]). We used the implementation of DomainBed [17], and used the default hyper-parameters for pre-training on source-domains and fine-tuning on a target domain: namely, we use Adam [23] with a learning rate of 5e-5 for optimization and use a batch size of 32 with no dropout or weight decay.\n\nWith this in mind, existing studies have modified the model parameters to explicitly reduce entropy. Although the proposed method does not explicitly reduce entropy, it has the effect of implicitly reducing it. This is because the proposed method uses a template updated with samples from the target distribution, which provides a decision boundary that avoids the dense parts of the target distribution. Figure 1 The results show that T3A can effectively reduce entropy without using online optimization. Note that changing a hyper-parameter (such as learning rate) might change the results for Tent-C, but it may corrupt the entire classifier. See Section 4 for the hyperparameter selection.\n\nRemark 2: T3A is computationally light. Unlike Tent, our method does not use SGD. Besides, the representations are fixed, and it is not necessary to repeat the forward propagation of the feature extractor. The only computational overhead is the cost of one forward propagation of the last linear layer, which is usually negligible compared to the forward and back propagation of feature extractors.\n\nSince it is not desirable to reduce throughput when considering online prediction, the proposed method is suitable in this respect as well.\n\nRemark 3: Adjusting the linear classifier can significantly improve performance. Some readers may wonder how effective it is to modify only the linear classifier while freezing the representation. To answer this question, we compare the DG performance (None) before fine-tuning, (head) after fine-tuning only q \u03c9 , (body) after fine-tuning only f \u03b8 , and (all) after fine-tuning the entire network (Figure 1-c). Each block corresponds to a different dataset, and each color represents a different fine-tuning strategy. The results show that fine-tuning only the classifier often significantly improves performance. For example, in VLCS, the average performance score jumps from 72.5 to 82.9, which is close to the 84.7 obtained when the entire network is fine-tuned. The performance gain differs for each dataset, but the tendency is generally the same. In addition, this tendency was the same for other backbone networks including BiT, ViT, and Mixer (see Appendix B). These results indicate that adjusting only the linear classifier can significantly improve performance in various configurations. Note that the number of parameters of the linear layer is much smaller than those of the feature extractor in standard network architectures.\n\n\nExperiment\n\nWe evaluate T3A on four standard domain generalization benchmarks, namely VLCS [12], PACS [27], OfficeHome [50], and TerraIncognita [5]. Our implementation uses the DomainBed library [17] 1 . We modify DomainBed (1) to use various backbone networks using the timm library [54] 2 , and (2) to implement test-time adaptation algorithms (ours and Tent). For Tent, we used the original implementation 3 . We run our experiments mainly on cloud V100x4 or A100x8 instances, depending on the memory usage of the backbone networks. See Appendix A for more information, including licensing information and total amount of compute. \n\n\nBackbone networks.\n\nFor the main experiments, we use residual networks with 50 layers (ResNet50), which was the default setting of the prior studies. In addition, we tested our algorithms on 10 different pre-trained models: residual networks with different layers (ResNet18 and ResNet50), Big Transfer [24] with different layers (BiT-M-R50x3, BiT-M-R101x3, and BiT-M-R152x2 ), Vision Transformers [9] with variations (ViT-B16, ViT-L16, HViT, which uses ResNet50 as patch embedding of ViT, DeiT [48]), and MLP-Mixer (Mixer-L16) [47].\n\nBaselines. We compare our method to domain generalization algorithms and test-time adaptation algorithms. For domain generalization algorithms, we mainly compared with the results reported in [17]. These results include the following algorithms: Empirical Risk Minimization (ERM), Group Distributionally Robust Optimization (GroupDRO) [41], Inter-domain Mixup (Mixup) [56,58,53], Meta-Learning for Domain Generalization (MLDG) [28], DomainAdversarial Neural Networks (DANN) [15], Class-conditional DANN (C-DANN) [32], Deep CORrelation ALignment (CORAL) [45], Maximum Mean Discrepancy (MMD) [29], Invariant Risk Minimization (IRM) [3], Adaptive Risk Minimization (ARM) [60], Marginal Transfer Learning (MTL) [6] Style-Agnostic Networks (SagNet) [36], and Representation Self Challenging (RSC) [20].\n\nWe also compared our method with existing test-time adaptation methods. Note that we can not simply use BN-based methods (including Tent [52], which is the most up-to-date method) on the DG setup because [17] omit the BN layer from pre-trained ResNet when fine-tuning on source domains. Besides, several backbone networks evaluated in our paper do not contain BN from the beginning. Therefore, we first tested two slightly modified versions of Tent on standard DG setup (Table 1, Table  2, and Figure 2). Specifically, Tent-C modulates the entire classifier to reduce prediction entropy. Tent-BN adds one BN layer just before the linear classifier and then modulates BN's normalization and transformation parameters. We then compare T3A with other test-time adaptation methods (including SHOT [34], pseudo labeling (PL) [26], Tent-Full [52], BN-Norm [43]) using ResNet18 and ResNet50 without removing batch normalization layer (Table 3). Hyperparameters and model selection. As [17] claimed, model selection is not trivial in DG and significantly affects performance. We used standard training-domain validation for selecting hyperparameters, which uses the subset of each training domain to choose a model. As reported in [17], we split the data from each domain into 80% and 20% splits and use larger splits for training and smaller splits to select hyperparameters. Following [17], we conduct a random search of 20 trials over a joint distribution of all hyperparameters to train the base model (see Appendix A.4).\n\nIn addition, T3A has one hyperparameter M for deciding the number of supports to restore, and Tent has two primary hyperparameters: \u03b2 for multiplying the base learning rate (used for the base model) and \u03b3 for the number of iterations per adaptation. It is worth emphasizing that these parameters should be selected before the deployment, i.e., before accessing the test data. We simply selected these hyperparameters by the average accuracy in the training-domain validation data when using these adjustment modules. Specifically, we tested M \u2208 {1, 5, 20, 50, 100, N/A} for T3A, where N/A means restoring all samples, and combination of \u03b2 \u2208 {0.1, 1.0, 10.0} and \u03b3 \u2208 {1, 3} for Tent. Table 1 summarizes the results when ResNet50 is used as the backbone network. The first block (from ERM to RSC) is the value taken from [17]. The lines labeled ERM \u2020 and CORAL \u2020 are the scores reproduced in our environments. The proposed method and Tent are based on this reproduced model. Figure 2 shows the distribution of performance improvement by the proposed method for models trained with different hyperparameters (20 \u00d7 3 for each test environment). In addition, Table  2 show the DG accuracy with 10 different backbones. Note that this experiment is conducted only on the default hyperparameter of ERM. Every number we report is a mean and standard error over three repetitions with different weight initialization, and dataset splits.\n\n\nResults\n\nT3A stably improves the performance of the base model. The second block of Table 1 shows that T3A stably improves the performance of the ERM model. Specifically, the proposed method improves 2.3 points, 2.0 points, 1.9 points, and 0.5 points for each dataset respectively. The average  improvement from ERM is 1.5 points. For clarification, paired t-test was performed using 48 paired data (4 datasets, 4 test domains, 3 different seeds). As a results, the difference is statistically significant (p \u2264 0.01). Note that, Tent-BN improves the performance in PACS and OfficeHome, but it is not stable may be due to the failure of the optimization. Tent-C never improve the performance.\n\nIn addition, Fig. 2 show that the third-quarter quantile of the improvement range is often more than 0, which means the proposed method is also robust to the hyperparameters of the base model. Furthermore, Table 2 shows that the proposed method can also improve the performance of more sophisticated backbone networks. For example, the proposed method improves performance by an average of 1.0 points for HViT, which achieved the best performance of all backbones. The improvement by the proposed method is statistically significant (p \u2264 0.05 for ViT-L16, p \u2264 0.01 for other backbones) with the one-side paired t-test.\n\nT3A outperforms most existing DG algorithms. For example, in VLCS, the proposed method achieves 80.0%, which is significantly better than the prior best-reported score, 78.8%. Similarly, it is 85.3% for PACS, 68.3% (best) for OfficeHome and 47.0% for TerraIncognita, and 70.1% in average (third best). Note that the scores we reproduce are much worse than the reported scores in PACS, which reduces the average performance. In PACS, the improvement from ERM by the proposed method is 1.7 points, which is the most significant improvement from any reported value.\n\nFor a more accurate comparison, we also examined whether the proposed method would improve the performance of CORAL, which had achieved the best performance in existing reports. As shown in the third block of Table 1, the proposed method can also improve the performance of CORAL. The average improvement is 0.6 points. Average performance is 70.4%, which is the best among all algorithms. Note that, similar to ERM, the reproduced score and the reported score are slightly different (and the score in PACS is particularly low).\n\nT3A outperforms existing test-time adaptation methods. We further compare T3A with other test-time adaptation methods when backbone networks employ the BN layer so that we can make a fair comparison with the prior BN-based methods [43,52]. We used ResNet18 w/BN and ResNet50 w/BN as backbone networks. Note that the results of ResNet in Table 1 and Table 2 do not use the BN layer since it is the default option in [17]. Therefore, the results below are not directly comparable to the DG method shown in Table 1 and Table 2 of the current manuscript.\n\nWe tested the following six baselines in addition to T3A, Tent-BN, and Tent-C. (1) Tent-Full updates BN statistics and transformations, which is the same as the original proposal [52].\n\n(2) BN-Norm update BN statistics but fixes transformations parameters [43]. (3) PL (Pseudo Label) [26] updates entire networks by minimizing the cross-entropy between prediction and the pseudo label. Following [52], we assign the pseudo label if the predictions are over a threshold (0.9 in our experiment).\n\n(4) PL-C updates the linear classifier by minimizing the above-mentioned pseudo-label loss. (5) SHOT [34] updates feature extractor to minimize entropy, diversity regularizer, and pseudo-label loss. While [34] originally proposed SHOT in the context of source-free domain adaptation (offline adaptation setup), the method itself can be transferred to our setup. Note that the original SHOT uses the label-smoothing when training on the source domain. However, we focus on the adaptation method, and therefore the source model is the same as the other baselines for the fair comparison. (6) SHOT-IM [34] updates the feature extractor to minimize entropy and the diversity regularizer.\n\nIn summary, (1) all baseline except BN-Norm and T3A use stochastic optimization during test-time, which is not desirable since it may cause catastrophic failure and must increase computational costs.\n\n(2) Tent-Full and BN-Norm are powerful yet constrained to be applicable only if the architecture uses BN. Table 3 compares results under the training-domain model selection as with Table 1 and Table 2.\n\nFor clarification, we also compare performance under the oracle model selection (28 in Appendix C ), where one can use the validation set on the target domain (20% of all data as described above). We can make the following observations. (1) The proposed method still outperformed all baselines in both backbone networks. Among baselines, only Tent-Full and PL-C perform better than None (w/o adaptation) on average. (2) When we select the hyper-parameters with a test-domain validation set, Tent-Full gives comparable performances with the proposed method. In addition, compared to T3A and PL-C, the T3A performs better under both model selection strategies. These results clarify the difficulty of model selection in optimization-based methods and the merit of the proposed optimization-free approach in this setup. (3) Updating feature extractor (or the large portion of the parameters) does not work well in general, while it is common in SFDA (offline) setup. The results suggest that we need different treatments on online and offline setup.\n\n\nDiscussion and Conclusion\n\nThis paper presents a new domain generalization algorithm, T3A, which adjusts its predictions during test-time by itself. We show that T3A reduces prediction entropy (Fig. 1-c), and more importantly, generalization error on unseen domain (Table 1). T3A can adapt different domain generalization  Table  2). Note that this property is important in practice because a better backbone network usually give significant performance gains. For example, Table 2 suggest a practitioner should try HViT, which outperforms ResNet50 by a large margin (7.8 points in average) if computational resources allow. T3A can boost HViT's performance by 1.0 points. Unlike existing studies that update the model with SGD during testing, the proposed method is optimization-free. Therefore, the computational overhead is negligible, and the behavior is unlikely to become unstable, making T3A especially suitable for online settings where the model needs to (adapt and) predict online as with typical DG.\n\nOne of the limitations of T3A is how to extend it beyond the classification problem. Since the proposed method creates a template online for each prediction class, it is not trivial to adapt it for continuous prediction. Note that Tent have the same problem, as prediction entropy is hard to compute in the regression case. It is a future task to apply this idea to a broader range of problem settings.\n\nAnother potential drawback of the proposed method is that the model can change at any time, making it difficult to thoroughly test its behavior in advance. This may raise ethical concerns in some sensitive applications, making it more difficult to sanitize the model and ensure it does not make unfair decisions. In such a situation, fully online adaptation might be difficult, and one may want to update the model offline. We encourage examination of each of these works on the frontier of test-time adaptation.\n\nAlthough accuracy was greatly improved, there is massive room for improvement as the performance in the unknown domain is still significantly worse than performance in the known domain. From a probabilistic viewpoint, the templates of each class can be regarded as the statistics of the P (Z|Y ), and our method adjusts it. As it is connected to the prediction P (Y |Z) = P (Z|Y )P (Y )\nP (Z)\n, adjusting it can correlates the prediction. From this perspective, using the average templates might be too restrictive, and one can use higher-order statistics to improve performance. Alternatively, one can retain all reliable samples, approximating P (Z|Y ) as empirical distribution. We hope that the findings of this paper will lead to a better test-time adaptation method and lead to the development of machine learning systems that work well in unknown environments.\n\nFigure 1 :\n1Pre-experimental results. (a) Comparing the entropy of predictions (with ResNet50) on the source domains and the target domain. The domain shift increases entropy. (b) T3A effectively reduces entropy. (c) Transferability of each components of the model trained by ERM in each dataset.\n\n\n-b compares the prediction entropy on the target domain among (a) ERM without test-time modulation, (b) T3A, (c) Tent-C, which updates the classifier to minimize entropy.\n\nFigure 2 :\n2Distribution of performance improvements by the proposed method for models trained with different hyperparameters (20 \u00d7 3 for each test environment). Dashed line in each violin plot represents the quartile of the distribution.\n\n\nDatasets. VLCS[12] comprises four photographic datasets d \u2208 {Caltech101[13], LabelMe[40], SUN09[7], VOC2007[11]}, containing 10, 729 examples of 5 classes. PCAS [27] comprises four domains d \u2208 {art, cartoons, photos, sketches}, containing 9, 991 examples of 7 classes. OfficeHome [50] includes domains d \u2208 {art, clipart, product, real}, containing 15, 588 examples and 65 classes. TerraIncognita [5] includes photo of wild animals taken by camera at different locations. Following [17], we used datasets of d \u2208 {L100, L38, L43, L46}, containing 24, 788 examples and 10 classes.\n\nTable 1 :\n1Domain generalization accuracy for all datasets and algorithms. Bold type indicates performance improvement from the base model, and * indicates statistical significance in one-sided paired t-test (** indicates p \u2264 0.01, * indicates p \u2264 0.05).Algorithm \nVLCS \nPACS \nOfficeHome \nTerra \nAvg \n\nERM \n77.5 \u00b1 0.4 85.5 \u00b1 0.2 \n66.5 \u00b1 0.3 \n46.1 \u00b1 1.8 69.0 \nIRM \n78.5 \u00b1 0.5 83.5 \u00b1 0.8 \n64.3 \u00b1 2.2 \n47.6 \u00b1 0.8 68.5 \nGroupDRO \n76.7 \u00b1 0.6 84.4 \u00b1 0.8 \n66.0 \u00b1 0.7 \n43.2 \u00b1 1.1 67.6 \nMixup \n77.4 \u00b1 0.6 84.6 \u00b1 0.6 \n68.1 \u00b1 0.3 \n47.9 \u00b1 0.8 69.5 \nMLDG \n77.2 \u00b1 0.4 84.9 \u00b1 1.0 \n66.8 \u00b1 0.6 \n47.7 \u00b1 0.9 69.2 \nCORAL \n78.8 \u00b1 0.6 86.2 \u00b1 0.3 \n68.7 \u00b1 0.3 \n47.6 \u00b1 1.0 70.3 \nMMD \n77.5 \u00b1 0.9 84.6 \u00b1 0.5 \n66.3 \u00b1 0.1 \n42.2 \u00b1 1.6 67.7 \nDANN \n78.6 \u00b1 0.4 83.6 \u00b1 0.4 \n65.9 \u00b1 0.6 \n46.7 \u00b1 0.5 68.7 \nCDANN \n77.5 \u00b1 0.1 82.6 \u00b1 0.9 \n65.8 \u00b1 1.3 \n45.8 \u00b1 1.6 67.9 \nMTL \n77.2 \u00b1 0.4 84.6 \u00b1 0.5 \n66.4 \u00b1 0.5 \n45.6 \u00b1 1.2 68.5 \nSagNet \n77.8 \u00b1 0.5 86.3 \u00b1 0.2 \n68.1 \u00b1 0.1 \n48.6 \u00b1 1.0 70.2 \nARM \n77.6 \u00b1 0.3 85.1 \u00b1 0.4 \n64.8 \u00b1 0.3 \n45.5 \u00b1 0.3 68.3 \nVREx \n78.3 \u00b1 0.2 84.9 \u00b1 0.6 \n66.4 \u00b1 0.6 \n46.4 \u00b1 0.6 69.0 \nRSC \n77.1 \u00b1 0.5 85.2 \u00b1 0.9 \n65.5 \u00b1 0.9 \n46.6 \u00b1 1.0 68.6 \n\nERM  \u2020 \n77.7 \u00b1 0.1 83.6 \u00b1 0.9 \n66.4 \u00b1 0.3 \n46.5 \u00b1 0.3 68.6 \n+T3A (Ours) 80.0 \u00b1 0.2 85.3 \u00b1 0.6 \n68.3 \u00b1 0.1 \n47.0 \u00b1 0.6 70.1  *  *  \n+Tent-BN \n68.2 \u00b1 0.2 84.8 \u00b1 0.5 \n67.0 \u00b1 0.4 \n44.7 \u00b1 0.3 66.2 \n+Tent-C \n77.0 \u00b1 0.4 82.3 \u00b1 1.2 \n65.7 \u00b1 0.2 \n45.5 \u00b1 0.4 67.6 \n\nCORAL  \u2020 \n78.6 \u00b1 0.5 84.2 \u00b1 0.3 \n68.3 \u00b1 0.1 \n48.1 \u00b1 1.3 69.8 \n+T3A (Ours) 79.5 \u00b1 0.5 85.6 \u00b1 0.2 \n69.2 \u00b1 0.2 \n47.3 \u00b1 0.7 70.4  *  \n+Tent-BN \n71.4 \u00b1 0.7 85.6 \u00b1 0.2 \n69.2 \u00b1 0.2 \n46.5 \u00b1 0.5 68.2 \n+Tent-C \n78.1 \u00b1 0.5 83.7 \u00b1 0.4 \n68.2 \u00b1 0.1 \n47.8 \u00b1 1.1 69.5 \n\n\n\nTable 2 :\n2Domain generalization accuracy with different backbone networks. T3A increases the \nperformance agnostic to backbone networks. Note that, this experiments is conducted only on the \ndefault hyperparameters of ERM. Bold type indicates performance improvement, and * indicates \nstatistical significance in paired t-test (** indicates p \u2264 0.01, * indicates p \u2264 0.05). \n\nModels \nVLCS \nPACS \nOfficeHome \nTerra \nAvg \n\nresnet18 \n73.2 \u00b1 0.9 80.3 \u00b1 0.4 \n55.7 \u00b1 0.2 \n40.7 \u00b1 0.3 62.5 \n+T3A \n76.5 \u00b1 0.9 81.7 \u00b1 0.1 \n57.0 \u00b1 0.4 \n41.6 \u00b1 0.5 64.2  *  \n\nresnet50 \n75.5 \u00b1 0.1 83.9 \u00b1 0.2 \n64.4 \u00b1 0.2 \n45.4 \u00b1 1.2 67.3 \n+T3A \n78.3 \u00b1 0.7 84.5 \u00b1 0.3 \n66.5 \u00b1 0.2 \n45.9 \u00b1 0.5 68.8  *  \n\nBiT-M-R50x3 \n76.7 \u00b1 0.1 84.4 \u00b1 1.2 \n69.2 \u00b1 0.6 \n52.5 \u00b1 0.3 70.7 \n+T3A \n79.7 \u00b1 0.3 85.4 \u00b1 0.9 \n71.7 \u00b1 0.6 \n52.2 \u00b1 0.6 72.3  *  \n\nBiT-M-R101x3 75.0 \u00b1 0.6 84.0 \u00b1 0.7 \n67.7 \u00b1 0.5 \n47.8 \u00b1 0.8 68.6 \n+T3A \n78.6 \u00b1 0.4 85.4 \u00b1 0.5 \n69.9 \u00b1 0.4 \n48.1 \u00b1 0.8 70.5  *  \n\nBiT-M-R152x2 76.7 \u00b1 0.3 85.2 \u00b1 0.1 \n71.3 \u00b1 0.6 \n51.4 \u00b1 0.6 71.1 \n+T3A \n79.1 \u00b1 0.4 86.4 \u00b1 0.1 \n73.2 \u00b1 0.5 \n50.9 \u00b1 0.7 72.4  *  \n\nViT-B16 \n79.2 \u00b1 0.3 85.7 \u00b1 0.1 \n78.4 \u00b1 0.3 \n41.8 \u00b1 0.6 71.3 \n+T3A \n80.2 \u00b1 0.4 86.0 \u00b1 0.1 \n78.9 \u00b1 0.3 \n42.5 \u00b1 0.7 71.9  *  \n\nViT-L16 \n78.2 \u00b1 0.5 84.6 \u00b1 0.5 \n78.0 \u00b1 0.1 \n42.7 \u00b1 1.9 70.9 \n+T3A \n79.0 \u00b1 0.6 85.5 \u00b1 0.6 \n78.7 \u00b1 0.2 \n45.3 \u00b1 0.4 72.1  *  *  \n\nDeiT \n79.3 \u00b1 0.4 87.8 \u00b1 0.5 \n76.6 \u00b1 0.3 \n50.0 \u00b1 0.2 73.4 \n+T3A \n81.3 \u00b1 0.4 89.5 \u00b1 0.4 \n78.3 \u00b1 0.2 \n50.1 \u00b1 0.2 74.8  *  \n\nHViT \n79.2 \u00b1 0.5 89.7 \u00b1 0.4 \n80.0 \u00b1 0.2 \n51.4 \u00b1 0.9 75.1 \n+T3A \n81.0 \u00b1 0.1 90.4 \u00b1 0.5 \n80.5 \u00b1 0.2 \n52.3 \u00b1 1.0 76.1  *  \n\nMixer-L16 \n76.4 \u00b1 0.2 81.3 \u00b1 1.0 \n69.4 \u00b1 1.6 \n37.1 \u00b1 0.4 66.1 \n+T3A \n80.3 \u00b1 0.3 83.0 \u00b1 0.8 \n72.3 \u00b1 1.8 \n37.5 \u00b1 0.8 68.3  *  \n\nC \nL \nS \nV \n\nTest Env \n\n10.0% \n\n5.0% \n\n0.0% \n\n5.0% \n\n10.0% \n\nAcc \n\n(a) ERM on VLCS \n\nA \nC \nP \nS \n\nTest Env \n\n10.0% \n\n5.0% \n\n0.0% \n\n5.0% \n\n10.0% \n\nAcc \n\n(b) ERM on PACS \n\nA \nC \nP \nS \n\nTest Env \n\n10.0% \n\n5.0% \n\n0.0% \n\n5.0% \n\n\n\nTable 3 :\n3Comparison of our method and existing test-time adaptation methods. UnlikeTable 1 and Table 2, we used ResNet18 and ResNet50 without removing batch normalization layer as backbone networks. As withTable 2, this experiments is conducted only on the default hyperparameters of ERM. Bold type indicates performance improvement, and * indicates statistical significance in paired t-test (* indicates p \u2264 0.05).algorithms for training-phase (the third block ofTable 1), and different backbone networks (Models \nVLCS \nPACS \nOfficeHome \nTerra \nAvg \n\nresnet18 w/ BN 73.0 \u00b1 0.6 79.5 \u00b1 0.4 \n61.8 \u00b1 0.3 \n41.7 \u00b1 0.9 64.0 \nSHOT-IM \n61.6 \u00b1 0.3 82.1 \u00b1 0.3 \n62.5 \u00b1 0.3 \n32.8 \u00b1 0.4 59.8 \nSHOT \n61.8 \u00b1 0.3 82.3 \u00b1 0.2 \n62.8 \u00b1 0.2 \n32.7 \u00b1 0.4 59.9 \nPL \n67.0 \u00b1 0.6 72.9 \u00b1 1.0 \n56.3 \u00b1 2.5 \n35.4 \u00b1 1.7 57.9 \nPL-C \n71.8 \u00b1 1.3 78.9 \u00b1 0.4 \n61.7 \u00b1 0.3 \n43.1 \u00b1 0.9 63.9 \nTent-Full \n72.3 \u00b1 0.3 83.9 \u00b1 0.3 \n62.7 \u00b1 0.2 \n36.9 \u00b1 0.3 64.0 \nBN-Norm \n70.4 \u00b1 1.0 82.7 \u00b1 0.1 \n62.0 \u00b1 0.1 \n36.4 \u00b1 0.2 62.9 \nTent-C \n71.3 \u00b1 1.5 74.6 \u00b1 1.9 \n60.5 \u00b1 0.4 \n40.9 \u00b1 0.5 61.8 \nTent-BN \n64.7 \u00b1 0.7 81.1 \u00b1 0.2 \n62.5 \u00b1 0.3 \n36.4 \u00b1 0.9 61.2 \nT3A (Ours) \n74.5 \u00b1 0.9 81.4 \u00b1 0.2 \n63.2 \u00b1 0.4 \n39.5 \u00b1 0.3 64.6  *  \n\nresnet50 w/ BN 74.3 \u00b1 0.5 84.1 \u00b1 0.1 \n66.9 \u00b1 0.2 \n45.8 \u00b1 1.8 67.8 \nSHOT-IM \n61.5 \u00b1 1.7 84.6 \u00b1 0.3 \n68.0 \u00b1 0.0 \n33.8 \u00b1 0.3 62.0 \nSHOT \n61.6 \u00b1 1.8 84.8 \u00b1 0.5 \n68.0 \u00b1 0.0 \n34.6 \u00b1 0.3 62.3 \nPL \n63.4 \u00b1 1.8 80.1 \u00b1 3.5 \n61.3 \u00b1 1.5 \n36.8 \u00b1 4.4 60.4 \nPL-C \n73.3 \u00b1 0.8 84.7 \u00b1 0.3 \n66.4 \u00b1 0.3 \n47.0 \u00b1 1.7 67.9 \nTent-Full \n75.4 \u00b1 0.6 87.0 \u00b1 0.2 \n66.9 \u00b1 0.2 \n42.6 \u00b1 0.8 68.0 \nBN-Norm \n71.3 \u00b1 0.4 85.8 \u00b1 0.1 \n66.4 \u00b1 0.1 \n42.3 \u00b1 0.4 66.5 \nTent-C \n72.4 \u00b1 1.5 84.4 \u00b1 0.1 \n66.2 \u00b1 0.2 \n42.4 \u00b1 3.1 66.4 \nTent-BN \n65.6 \u00b1 1.4 84.9 \u00b1 0.0 \n67.7 \u00b1 0.2 \n42.7 \u00b1 0.5 65.2 \nT3A (Ours) \n76.0 \u00b1 0.3 85.1 \u00b1 0.2 \n68.2 \u00b1 0.1 \n44.6 \u00b1 0.9 68.5  *  \n\n\nhttps://github.com/facebookresearch/DomainBed 2 https://github.com/rwightman/pytorch-image-models 3 https://github.com/DequanWang/tent\nAcknowledgementsThis work has been supported by JSPS Grant-in-Aid for Early-Career Scientists Number JP18K18101 and the Mohammed bin Salman Center for Future Science and Technology for Saudi-Japan Vision 2030 at The University of Tokyo (MbSC2030). Computational resource of AI Bridging Cloud Infrastructure (ABCI) provided by National Institute of Advanced Industrial Science and Technology (AIST) was used.\nAdversarial invariant feature learning with accuracy constraint for domain generalization. K Akuzawa, Yusuke Iwasawa, Y Matsuo, ECML/PKDD. K. Akuzawa, Yusuke Iwasawa, and Y. Matsuo. Adversarial invariant feature learning with accuracy constraint for domain generalization. In ECML/PKDD, 2019.\n\nStrike (with) a pose: Neural networks are easily fooled by strange poses of familiar objects. Michael A Alcorn, Qi Li, Zhitao Gong, Chengfei Wang, Long Mai, Wei-Shinn Ku, Anh M Nguyen, CVPRMichael A. Alcorn, Qi Li, Zhitao Gong, Chengfei Wang, Long Mai, Wei-Shinn Ku, and Anh M Nguyen. Strike (with) a pose: Neural networks are easily fooled by strange poses of familiar objects. CVPR, pages 4840-4849, 2019.\n\nMartin Arjovsky, L\u00e9on Bottou, Ishaan Gulrajani, David Lopez-Paz, arXiv:1907.02893Invariant risk minimization. arXiv preprintMartin Arjovsky, L\u00e9on Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. arXiv preprint arXiv:1907.02893, 2019.\n\nMetareg: Towards domain generalization using meta-regularization. Y Balaji, S Sankaranarayanan, R Chellappa, In NeurIPS. Y. Balaji, S. Sankaranarayanan, and R. Chellappa. Metareg: Towards domain generalization using meta-regularization. In NeurIPS, 2018.\n\nRecognition in terra incognita. Sara Beery, Grant Van Horn, P Perona, ECCV. Sara Beery, Grant Van Horn, and P. Perona. Recognition in terra incognita. In ECCV, 2018.\n\nGeneralizing from several related classification tasks to a new unlabeled sample. Gilles Blanchard, Gyemin Lee, Clayton Scott, NIPS. Gilles Blanchard, Gyemin Lee, and Clayton Scott. Generalizing from several related classification tasks to a new unlabeled sample. In NIPS, pages 2178-2186, 2011.\n\nExploiting hierarchical context on a large database of object categories. M J Choi, Joseph J Lim, A Torralba, A Willsky, CVPR. M. J. Choi, Joseph J. Lim, A. Torralba, and A. Willsky. Exploiting hierarchical context on a large database of object categories. CVPR, pages 129-136, 2010.\n\nDark model adaptation: Semantic image segmentation from daytime to nighttime. ITSC. Dengxin Dai, L Gool, Dengxin Dai and L. Gool. Dark model adaptation: Semantic image segmentation from daytime to nighttime. ITSC, pages 3819-3824, 2018.\n\nAn image is worth 16x16 words: Transformers for image recognition at scale. A Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, M Dehghani, Matthias Minderer, G Heigold, S Gelly, Jakob Uszkoreit, N Houlsby, ICLR. 2021A. Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, M. Dehghani, Matthias Minderer, G. Heigold, S. Gelly, Jakob Uszkoreit, and N. Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021.\n\nUsing weighted nearest neighbor to benefit from unlabeled data. Kurt Driessens, Peter Reutemann, Bernhard Pfahringer, Claire Leschi, PAKDD. Kurt Driessens, Peter Reutemann, Bernhard Pfahringer, and Claire Leschi. Using weighted nearest neighbor to benefit from unlabeled data. In PAKDD, 2006.\n\nThe PASCAL Visual Object Classes Challenge. M Everingham, L Van Gool, C K I Williams, J Winn, A Zisserman, M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results. http://www.pascal- network.org/challenges/VOC/voc2007/workshop/index.html.\n\nUnbiased metric learning: On the utilization of multiple datasets and web images for softening bias. Chen Fang, Ye Xu, D Rockmore, ICCV. Chen Fang, Ye Xu, and D. Rockmore. Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias. ICCV, pages 1657-1664, 2013.\n\nOne-shot learning of object categories. Li Fei-Fei, R Fergus, P Perona, IEEE Transactions on Pattern Analysis and Machine Intelligence. 28Li Fei-Fei, R. Fergus, and P. Perona. One-shot learning of object categories. IEEE Transactions on Pattern Analysis and Machine Intelligence, 28:594-611, 2006.\n\nUnsupervised domain adaptation by backpropagation. Yaroslav Ganin, Victor Lempitsky, ICML. Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In ICML, 2015.\n\nDomain-adversarial training of neural networks. Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Fran\u00e7ois Laviolette, Mario Marchand, Victor Lempitsky, JMRL. 171Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Fran\u00e7ois Lavio- lette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. JMRL, 17(1):2096-2030, 2016.\n\nImagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness. Robert Geirhos, Patricia Rubisch, Claudio Michaelis, M Bethge, Felix Wichmann, W Brendel, abs/1811.12231ArXiv. Robert Geirhos, Patricia Rubisch, Claudio Michaelis, M. Bethge, Felix Wichmann, and W. Brendel. Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness. ArXiv, abs/1811.12231, 2019.\n\nIn search of lost domain generalization. Ishaan Gulrajani, David Lopez-Paz, ICLR. 2021Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. In ICLR, 2021.\n\nDeep residual learning for image recognition. X Kaiming He, Shaoqing Zhang, Jian Ren, Sun, CVPR. Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. CVPR, pages 770-778, 2016.\n\nMeta-learning in neural networks: A survey. Timothy M Hospedales, Antreas Antoniou, P Micaelli, A Storkey, IEEE transactions. 2021Timothy M. Hospedales, Antreas Antoniou, P. Micaelli, and A. Storkey. Meta-learning in neural networks: A survey. IEEE transactions on pattern analysis and machine intelligence, PP, 2021.\n\nSelf-challenging improves cross-domain generalization. Zeyi Huang, Haohan Wang, E Xing, Dong Huang, ECCV. Zeyi Huang, Haohan Wang, E. Xing, and Dong Huang. Self-challenging improves cross-domain general- ization. In ECCV, 2020.\n\nStabilizing adversarial invariance induction from divergence minimization perspective. Yusuke Iwasawa, K Akuzawa, Y Matsuo, IJCAI. Yusuke Iwasawa, K. Akuzawa, and Y. Matsuo. Stabilizing adversarial invariance induction from divergence minimization perspective. In IJCAI, 2020.\n\nTransductive inference for text classification using support vector machines. T Joachims, ICML. T. Joachims. Transductive inference for text classification using support vector machines. In ICML, 1999.\n\nAdam: A method for stochastic optimization. Diederik Kingma, Jimmy Ba, ICLR. Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n\nBig transfer (bit): General visual representation learning. Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, J Puigcerver, Jessica Yung, S Gelly, N Houlsby, ECCV. Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, J. Puigcerver, Jessica Yung, S. Gelly, and N. Houlsby. Big transfer (bit): General visual representation learning. In ECCV, 2020.\n\nUniversal source-free domain adaptation. Jogendra Nath Kundu, Naveen Venkat, V Rahulm, R Venkatesh, Babu, CVPR. Jogendra Nath Kundu, Naveen Venkat, V. RahulM., and R. Venkatesh Babu. Universal source-free domain adaptation. CVPR, pages 4543-4552, 2020.\n\nPseudo-label : The simple and efficient semi-supervised learning method for deep neural networks. Dong-Hyun Lee, Dong-Hyun Lee. Pseudo-label : The simple and efficient semi-supervised learning method for deep neural networks. 2013.\n\nDeeper, broader and artier domain generalization. Da Li, Yongxin Yang, Yi-Zhe Song, Timothy M Hospedales, ICCV. IEEEDa Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier domain generalization. In ICCV, pages 5543-5551. IEEE, 2017.\n\nLearning to generalize: Meta-learning for domain generalization. Da Li, Yongxin Yang, Yi-Zhe Song, Timothy M Hospedales, AAAI. Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Learning to generalize: Meta-learning for domain generalization. In AAAI, 2018.\n\nDomain generalization with adversarial feature learning. Haoliang Li, S Sinno Jialin Pan, A Wang, Kot, CVPR. Haoliang Li, Sinno Jialin Pan, S. Wang, and A. Kot. Domain generalization with adversarial feature learning. CVPR, pages 5400-5409, 2018.\n\nModel adaptation: Unsupervised domain adaptation without source data. Rui Li, Qianfen Jiao, Wenming Cao, H Wong, Si Wu, CVPR. Rui Li, Qianfen Jiao, Wenming Cao, H. Wong, and Si Wu. Model adaptation: Unsupervised domain adaptation without source data. CVPR, pages 9638-9647, 2020.\n\nFeature-critic networks for heterogeneous domain generalization. Y Li, Yongxin Yang, W Zhou, Timothy M Hospedales, ICML. Y. Li, Yongxin Yang, W. Zhou, and Timothy M. Hospedales. Feature-critic networks for heterogeneous domain generalization. In ICML, 2019.\n\nDomain generalization via conditional invariant representations. Ya Li, Mingming Gong, Xinmei Tian, Tongliang Liu, Dacheng Tao, AAAI. Ya Li, Mingming Gong, Xinmei Tian, Tongliang Liu, and Dacheng Tao. Domain generalization via conditional invariant representations. In AAAI, 2018.\n\nDeep domain generalization via conditional invariant adversarial networks. Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, Dacheng Tao, ECCV. Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao. Deep domain generalization via conditional invariant adversarial networks. In ECCV, pages 624-639, 2018.\n\nDo we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. Jian Liang, D Hu, Jiashi Feng, ICML. Jian Liang, D. Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. In ICML, 2020.\n\nDistance-based image classification: Generalizing to new classes at near-zero cost. Thomas Mensink, Jakob J Verbeek, Florent Perronnin, Gabriela Csurka, IEEE Transactions on Pattern Analysis and Machine Intelligence. 35Thomas Mensink, Jakob J. Verbeek, Florent Perronnin, and Gabriela Csurka. Distance-based image classification: Generalizing to new classes at near-zero cost. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35:2624-2637, 2013.\n\nReducing domain gap via styleagnostic networks. H Nam, Hyunjae Lee, Jongchan Park, W Yoon, Donggeun Yoo, abs/1910.11645ArXiv. H. Nam, Hyunjae Lee, Jongchan Park, W. Yoon, and Donggeun Yoo. Reducing domain gap via style- agnostic networks. ArXiv, abs/1910.11645, 2019.\n\nA survey on transfer learning. Qiang Sinno Jialin Pan, Yang, IEEE Transactions on Knowledge and Data Engineering. 22Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on Knowledge and Data Engineering, 22:1345-1359, 2010.\n\nVisual domain adaptation: A survey of recent advances. V Patel, Raghuraman Gopalan, R Li, R Chellappa, IEEE Signal Processing Magazine. 32V. Patel, Raghuraman Gopalan, R. Li, and R. Chellappa. Visual domain adaptation: A survey of recent advances. IEEE Signal Processing Magazine, 32:53-69, 2015.\n\nAlexander Sylvestre-Alvise Rebuffi, G Kolesnikov, Christoph H Sperl, Lampert, icarl: Incremental classifier and representation learning. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, G. Sperl, and Christoph H. Lampert. icarl: Incremental classifier and representation learning. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5533-5542, 2017.\n\nLabelme: A database and web-based tool for image annotation. C Bryan, A Russell, K Torralba, W Murphy, Freeman, International Journal of Computer Vision. 77Bryan C. Russell, A. Torralba, K. Murphy, and W. Freeman. Labelme: A database and web-based tool for image annotation. International Journal of Computer Vision, 77:157-173, 2007.\n\nDistributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. Shiori Sagawa, Pang Wei Koh, T Hashimoto, Percy Liang, abs/1911.08731ArXiv. Shiori Sagawa, Pang Wei Koh, T. Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. ArXiv, abs/1911.08731, 2019.\n\nAsymmetric tri-training for unsupervised domain adaptation. K Saito, Y Ushiku, T Harada, ICML. K. Saito, Y. Ushiku, and T. Harada. Asymmetric tri-training for unsupervised domain adaptation. In ICML, 2017.\n\nImproving robustness against common corruptions by covariate shift adaptation. Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, Matthias Bethge, NeurIPS. Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. In NeurIPS, 2020.\n\nPrototypical networks for few-shot learning. J Snell, Kevin Swersky, R Zemel, NIPS. J. Snell, Kevin Swersky, and R. Zemel. Prototypical networks for few-shot learning. In NIPS, 2017.\n\nDeep coral: Correlation alignment for deep domain adaptation. Baochen Sun, Kate Saenko, ECCV Workshops. Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In ECCV Workshops, 2016.\n\nTest-time training for out-of-distribution generalization. Y Sun, Xiaolong Wang, Zhuang Liu, J Miller, Alexei A Efros, Moritz Hardt, abs/1909.13231ArXiv. Y. Sun, Xiaolong Wang, Zhuang Liu, J. Miller, Alexei A. Efros, and Moritz Hardt. Test-time training for out-of-distribution generalization. ArXiv, abs/1909.13231, 2019.\n\nMlp-mixer: An all-mlp architecture for vision. I Tolstikhin, N Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, A Dosovitskiy, abs/2105.01601ArXiv. I. Tolstikhin, N. Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, and A. Dosovitskiy. Mlp-mixer: An all-mlp architecture for vision. ArXiv, abs/2105.01601, 2021.\n\nTraining data-efficient image transformers & distillation through attention. M Hugo Touvron, M Cord, Francisco Douze, Alexandre Massa, Sablayrolles, Herv&apos;e J&apos;egou, abs/2012.12877ArXiv. Hugo Touvron, M. Cord, M. Douze, Francisco Massa, Alexandre Sablayrolles, and Herv'e J'egou. Training data-efficient image transformers & distillation through attention. ArXiv, abs/2012.12877, 2020.\n\nStatistical learning theory. V Vapnik, V. Vapnik. Statistical learning theory. 1998.\n\nDeep hashing network for unsupervised domain adaptation. Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, S Panchanathan, CVPR. Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and S. Panchanathan. Deep hashing network for unsupervised domain adaptation. CVPR, pages 5385-5394, 2017.\n\nTowards robust cnn-based object detection through augmentation with synthetic rain variations. G Volk, S Mueller, Alexander Von Bernuth, Dennis Hospach, O Bringmann, ITSC. G. Volk, S. Mueller, Alexander von Bernuth, Dennis Hospach, and O. Bringmann. Towards robust cnn-based object detection through augmentation with synthetic rain variations. ITSC, pages 285-292, 2019.\n\nTent: Fully test-time adaptation by entropy minimization. Dequan Wang, Evan Shelhamer, Shaoteng Liu, B Olshausen, Trevor Darrell, ICLR. 2021Dequan Wang, Evan Shelhamer, Shaoteng Liu, B. Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In ICLR, 2021.\n\nHeterogeneous domain generalization via domain mixup. Yufei Wang, Haoliang Li, A Kot, ICASSP. Yufei Wang, Haoliang Li, and A. Kot. Heterogeneous domain generalization via domain mixup. ICASSP, pages 3622-3626, 2020.\n\nPytorch image models. Ross Wightman, Ross Wightman. Pytorch image models. https://github.com/rwightman/pytorch-image-models, 2019.\n\nA survey of unsupervised deep domain adaptation. Garrett Wilson, D Cook, TIST. 11Garrett Wilson and D. Cook. A survey of unsupervised deep domain adaptation. TIST, 11:1 -46, 2020.\n\nAdversarial domain adaptation with domain mixup. Minghao Xu, Jia Yu Zhang, Bingbing Ni, Teng Li, Chengjie Wang, Qi Tian, W Zhang, AAAI. Minghao Xu, Jia yu Zhang, Bingbing Ni, Teng Li, Chengjie Wang, Qi Tian, and W. Zhang. Adversarial domain adaptation with domain mixup. In AAAI, 2020.\n\nTransductive zero-shot action recognition by word-vector embedding. X Xu, Timothy M Hospedales, S Gong, International Journal of Computer Vision. 123X. Xu, Timothy M. Hospedales, and S. Gong. Transductive zero-shot action recognition by word-vector embedding. International Journal of Computer Vision, 123:309-333, 2016.\n\nImprove unsupervised domain adaptation with mixup training. ArXiv, abs. Huan Shen Yan, Nanxiang Song, L Li, Liu Zou, Ren, Shen Yan, Huan Song, Nanxiang Li, L. Zou, and Liu Ren. Improve unsupervised domain adaptation with mixup training. ArXiv, abs/2001.00677, 2020.\n\nHongyi Zhang, Moustapha Ciss\u00e9, Yann Dauphin, David Lopez-Paz, abs/1710.09412mixup: Beyond empirical risk minimization. ArXiv. Hongyi Zhang, Moustapha Ciss\u00e9, Yann Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. ArXiv, abs/1710.09412, 2018.\n\nAdaptive risk minimization: A meta-learning approach for tackling group shift. ArXiv, abs. Marvin Zhang, H Marklund, Abhishek Gupta, Sergey Levine, Chelsea Finn, Marvin Zhang, H. Marklund, Abhishek Gupta, Sergey Levine, and Chelsea Finn. Adaptive risk minimiza- tion: A meta-learning approach for tackling group shift. ArXiv, abs/2007.02931, 2020.\n\nDomain generalization: A survey. ArXiv, abs/2103.02503, 2021. Checklist 1. For all authors. K Zhou, Z Liu, Yu Qiao, Tao Xiang, Chen Change Loy, K. Zhou, Z. Liu, Yu Qiao, Tao Xiang, and Chen Change Loy. Domain generalization: A survey. ArXiv, abs/2103.02503, 2021. Checklist 1. For all authors...\n\nDo the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope. Yes(a) Do the main claims made in the abstract and introduction accurately reflect the paper's contribu- tions and scope? [Yes]\n\nDid you discuss any potential negative societal impacts of your work?. Yes] See Section 5.1Did you discuss any potential negative societal impacts of your work? [Yes] See Section 5.1.\n\nHave you read the ethics review guidelines and ensured that your paper conforms to them. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]\n\nDid you state the full set of assumptions of all theoretical results. N/A] (b) Did you include complete proofs of all theoretical results? [N/A(a) Did you state the full set of assumptions of all theoretical results? [N/A] (b) Did you include complete proofs of all theoretical results? [N/A]\n\nwith respect to the random seed after running experiments multiple times)? [Yes] We repeated entire experiments three times as recommended by the prior works. Did you report error barsDid you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] We repeated entire experiments three times as recommended by the prior works.\n\ndata, models) or curating/releasing new assets... (a) If your work uses existing assets. If you are using existing assets (e.g., code,. did you cite the creators? [Yes] (b) Did you mention the license of the assets? [Yes] See Appendix A. (c) Did you include any new assets either in the supplemental material or as a URL? [Yes] (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [YesIf you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] (b) Did you mention the license of the assets? [Yes] See Appendix A. (c) Did you include any new assets either in the supplemental material or as a URL? [Yes] (d) Did you discuss whether and how consent was obtained from people whose data you're us- ing/curating? [Yes]\n\nDid you discuss whether the data you are using/curating contains personally identifiable information or offensive content. Did you discuss whether the data you are using/curating contains personally identifiable informa- tion or offensive content? [Yes]\n\nIf you used crowdsourcing or conducted research with human subjects. If you used crowdsourcing or conducted research with human subjects...\n\nDid you include the full text of instructions given to participants and screenshots. N/A(a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A]\n\nDid you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?. N/ADid you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]\n\nDid you include the estimated hourly wage paid to participants and the total amount spent on participant compensation. N/ADid you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]\n", "annotations": {"author": "[{\"end\":179,\"start\":83},{\"end\":274,\"start\":180}]", "publisher": null, "author_last_name": "[{\"end\":97,\"start\":90},{\"end\":193,\"start\":187}]", "author_first_name": "[{\"end\":89,\"start\":83},{\"end\":186,\"start\":180}]", "author_affiliation": "[{\"end\":178,\"start\":130},{\"end\":273,\"start\":225}]", "title": "[{\"end\":80,\"start\":1},{\"end\":354,\"start\":275}]", "venue": null, "abstract": "[{\"end\":1781,\"start\":356}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b7\"},\"end\":1996,\"start\":1993},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":2010,\"start\":2006},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2028,\"start\":2025},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2043,\"start\":2039},{\"end\":2070,\"start\":2067},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2423,\"start\":2420},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":2426,\"start\":2423},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2701,\"start\":2697},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":2704,\"start\":2701},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2707,\"start\":2704},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2745,\"start\":2741},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2813,\"start\":2809},{\"end\":2815,\"start\":2813},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2818,\"start\":2815},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3027,\"start\":3023},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":4018,\"start\":4014},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4021,\"start\":4018},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":4024,\"start\":4021},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":4751,\"start\":4747},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":4754,\"start\":4751},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5374,\"start\":5371},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":5670,\"start\":5666},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6234,\"start\":6230},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6245,\"start\":6241},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":6262,\"start\":6258},{\"end\":6286,\"start\":6283},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6357,\"start\":6353},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":6375,\"start\":6371},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6478,\"start\":6474},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6784,\"start\":6780},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6840,\"start\":6837},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":6851,\"start\":6847},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":6884,\"start\":6880},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7084,\"start\":7081},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8764,\"start\":8760},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":8883,\"start\":8879},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8966,\"start\":8962},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":9068,\"start\":9064},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9070,\"start\":9068},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9073,\"start\":9070},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9192,\"start\":9188},{\"end\":9194,\"start\":9192},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9197,\"start\":9194},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9403,\"start\":9400},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":9543,\"start\":9539},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":9630,\"start\":9626},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":9633,\"start\":9630},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":9636,\"start\":9633},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":10257,\"start\":10253},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10260,\"start\":10257},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":10263,\"start\":10260},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":10749,\"start\":10745},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10752,\"start\":10749},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":10755,\"start\":10752},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":11342,\"start\":11338},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":11369,\"start\":11365},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":11384,\"start\":11380},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12463,\"start\":12459},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":12488,\"start\":12484},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":12491,\"start\":12488},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":12519,\"start\":12515},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":12575,\"start\":12571},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":12955,\"start\":12951},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":16936,\"start\":16932},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":17486,\"start\":17482},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":17497,\"start\":17493},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":17514,\"start\":17510},{\"end\":17538,\"start\":17535},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":17585,\"start\":17581},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":17720,\"start\":17716},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":20403,\"start\":20399},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":20414,\"start\":20410},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":20431,\"start\":20427},{\"end\":20455,\"start\":20452},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":20718,\"start\":20717},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":21251,\"start\":21247},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":21345,\"start\":21342},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":21443,\"start\":21439},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":21476,\"start\":21472},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":21675,\"start\":21671},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":21818,\"start\":21814},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":21851,\"start\":21847},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":21854,\"start\":21851},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":21857,\"start\":21854},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":21910,\"start\":21906},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":21957,\"start\":21953},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":21995,\"start\":21991},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":22036,\"start\":22032},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":22073,\"start\":22069},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":22112,\"start\":22109},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":22151,\"start\":22147},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":22189,\"start\":22186},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":22227,\"start\":22223},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":22275,\"start\":22271},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":22419,\"start\":22415},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":22486,\"start\":22482},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":23075,\"start\":23071},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":23102,\"start\":23098},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":23118,\"start\":23114},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":23132,\"start\":23128},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":23260,\"start\":23256},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":23505,\"start\":23501},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":23661,\"start\":23657},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":24620,\"start\":24616},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":27869,\"start\":27865},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":27872,\"start\":27869},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":28053,\"start\":28049},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":28369,\"start\":28365},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":28446,\"start\":28442},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":28474,\"start\":28470},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":28586,\"start\":28582},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":28786,\"start\":28782},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":28890,\"start\":28886},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":29283,\"start\":29279},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":34348,\"start\":34344},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":34405,\"start\":34401},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":34418,\"start\":34414},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":34428,\"start\":34425},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":34441,\"start\":34437}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":33914,\"start\":33617},{\"attributes\":{\"id\":\"fig_1\"},\"end\":34087,\"start\":33915},{\"attributes\":{\"id\":\"fig_2\"},\"end\":34327,\"start\":34088},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":34907,\"start\":34328},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":36536,\"start\":34908},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":38436,\"start\":36537},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":40220,\"start\":38437}]", "paragraph": "[{\"end\":2199,\"start\":1797},{\"end\":3928,\"start\":2201},{\"end\":5000,\"start\":3930},{\"end\":6138,\"start\":5002},{\"end\":7000,\"start\":6140},{\"end\":7113,\"start\":7057},{\"end\":7944,\"start\":7212},{\"end\":8584,\"start\":8006},{\"end\":9637,\"start\":8586},{\"end\":10138,\"start\":9639},{\"end\":10632,\"start\":10161},{\"end\":11229,\"start\":10634},{\"end\":12302,\"start\":11231},{\"end\":13047,\"start\":12304},{\"end\":13093,\"start\":13049},{\"end\":13301,\"start\":13095},{\"end\":13574,\"start\":13417},{\"end\":14106,\"start\":13645},{\"end\":14674,\"start\":14120},{\"end\":15242,\"start\":14747},{\"end\":15595,\"start\":15244},{\"end\":15980,\"start\":15661},{\"end\":16679,\"start\":16053},{\"end\":16844,\"start\":16756},{\"end\":17826,\"start\":16856},{\"end\":18521,\"start\":17828},{\"end\":18921,\"start\":18523},{\"end\":19062,\"start\":18923},{\"end\":20305,\"start\":19064},{\"end\":20942,\"start\":20320},{\"end\":21477,\"start\":20965},{\"end\":22276,\"start\":21479},{\"end\":23795,\"start\":22278},{\"end\":25224,\"start\":23797},{\"end\":25918,\"start\":25236},{\"end\":26538,\"start\":25920},{\"end\":27102,\"start\":26540},{\"end\":27632,\"start\":27104},{\"end\":28184,\"start\":27634},{\"end\":28370,\"start\":28186},{\"end\":28679,\"start\":28372},{\"end\":29364,\"start\":28681},{\"end\":29565,\"start\":29366},{\"end\":29768,\"start\":29567},{\"end\":30816,\"start\":29770},{\"end\":31829,\"start\":30846},{\"end\":32233,\"start\":31831},{\"end\":32747,\"start\":32235},{\"end\":33135,\"start\":32749},{\"end\":33616,\"start\":33142}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7211,\"start\":7114},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8005,\"start\":7945},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13416,\"start\":13302},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14746,\"start\":14675},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15660,\"start\":15596},{\"attributes\":{\"id\":\"formula_5\"},\"end\":16052,\"start\":15981},{\"attributes\":{\"id\":\"formula_6\"},\"end\":16708,\"start\":16680},{\"attributes\":{\"id\":\"formula_7\"},\"end\":16755,\"start\":16708},{\"attributes\":{\"id\":\"formula_8\"},\"end\":33141,\"start\":33136}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":22766,\"start\":22748},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":23214,\"start\":23205},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":24487,\"start\":24480},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":24959,\"start\":24951},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":25318,\"start\":25311},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":26133,\"start\":26126},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":27320,\"start\":27313},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":27978,\"start\":27971},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":27990,\"start\":27983},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":28145,\"start\":28138},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":28157,\"start\":28150},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":29680,\"start\":29673},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":29767,\"start\":29748},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":31093,\"start\":31084},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":31150,\"start\":31142},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":31300,\"start\":31293}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1795,\"start\":1783},{\"attributes\":{\"n\":\"2\"},\"end\":7031,\"start\":7003},{\"attributes\":{\"n\":\"2.1\"},\"end\":7055,\"start\":7034},{\"attributes\":{\"n\":\"2.2\"},\"end\":10159,\"start\":10141},{\"attributes\":{\"n\":\"3\"},\"end\":13643,\"start\":13577},{\"attributes\":{\"n\":\"3.1\"},\"end\":14118,\"start\":14109},{\"attributes\":{\"n\":\"3.2\"},\"end\":16854,\"start\":16847},{\"attributes\":{\"n\":\"4\"},\"end\":20318,\"start\":20308},{\"end\":20963,\"start\":20945},{\"attributes\":{\"n\":\"4.1\"},\"end\":25234,\"start\":25227},{\"attributes\":{\"n\":\"5\"},\"end\":30844,\"start\":30819},{\"end\":33628,\"start\":33618},{\"end\":34099,\"start\":34089},{\"end\":34918,\"start\":34909},{\"end\":36547,\"start\":36538},{\"end\":38447,\"start\":38438}]", "table": "[{\"end\":36536,\"start\":35163},{\"end\":38436,\"start\":36549},{\"end\":40220,\"start\":38947}]", "figure_caption": "[{\"end\":33914,\"start\":33630},{\"end\":34087,\"start\":33917},{\"end\":34327,\"start\":34101},{\"end\":34907,\"start\":34330},{\"end\":35163,\"start\":34920},{\"end\":38947,\"start\":38449}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":17055,\"start\":17046},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":18241,\"start\":18233},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":19474,\"start\":19462},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":22780,\"start\":22772},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":24778,\"start\":24770},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":25939,\"start\":25933},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":31022,\"start\":31012}]", "bib_author_first_name": "[{\"end\":40856,\"start\":40855},{\"end\":40872,\"start\":40866},{\"end\":40883,\"start\":40882},{\"end\":41159,\"start\":41152},{\"end\":41161,\"start\":41160},{\"end\":41172,\"start\":41170},{\"end\":41183,\"start\":41177},{\"end\":41198,\"start\":41190},{\"end\":41209,\"start\":41205},{\"end\":41224,\"start\":41215},{\"end\":41232,\"start\":41229},{\"end\":41234,\"start\":41233},{\"end\":41473,\"start\":41467},{\"end\":41488,\"start\":41484},{\"end\":41503,\"start\":41497},{\"end\":41520,\"start\":41515},{\"end\":41796,\"start\":41795},{\"end\":41806,\"start\":41805},{\"end\":41826,\"start\":41825},{\"end\":42021,\"start\":42017},{\"end\":42034,\"start\":42029},{\"end\":42046,\"start\":42045},{\"end\":42240,\"start\":42234},{\"end\":42258,\"start\":42252},{\"end\":42271,\"start\":42264},{\"end\":42524,\"start\":42523},{\"end\":42526,\"start\":42525},{\"end\":42539,\"start\":42533},{\"end\":42541,\"start\":42540},{\"end\":42548,\"start\":42547},{\"end\":42560,\"start\":42559},{\"end\":42825,\"start\":42818},{\"end\":42832,\"start\":42831},{\"end\":43049,\"start\":43048},{\"end\":43068,\"start\":43063},{\"end\":43085,\"start\":43076},{\"end\":43102,\"start\":43098},{\"end\":43123,\"start\":43116},{\"end\":43136,\"start\":43130},{\"end\":43151,\"start\":43150},{\"end\":43170,\"start\":43162},{\"end\":43182,\"start\":43181},{\"end\":43193,\"start\":43192},{\"end\":43206,\"start\":43201},{\"end\":43219,\"start\":43218},{\"end\":43589,\"start\":43585},{\"end\":43606,\"start\":43601},{\"end\":43626,\"start\":43618},{\"end\":43645,\"start\":43639},{\"end\":43860,\"start\":43859},{\"end\":43874,\"start\":43873},{\"end\":43886,\"start\":43885},{\"end\":43890,\"start\":43887},{\"end\":43902,\"start\":43901},{\"end\":43910,\"start\":43909},{\"end\":44244,\"start\":44240},{\"end\":44253,\"start\":44251},{\"end\":44259,\"start\":44258},{\"end\":44484,\"start\":44482},{\"end\":44495,\"start\":44494},{\"end\":44505,\"start\":44504},{\"end\":44800,\"start\":44792},{\"end\":44814,\"start\":44808},{\"end\":44992,\"start\":44984},{\"end\":45008,\"start\":45000},{\"end\":45023,\"start\":45019},{\"end\":45038,\"start\":45032},{\"end\":45052,\"start\":45048},{\"end\":45073,\"start\":45065},{\"end\":45091,\"start\":45086},{\"end\":45108,\"start\":45102},{\"end\":45461,\"start\":45455},{\"end\":45479,\"start\":45471},{\"end\":45496,\"start\":45489},{\"end\":45509,\"start\":45508},{\"end\":45523,\"start\":45518},{\"end\":45535,\"start\":45534},{\"end\":45845,\"start\":45839},{\"end\":45862,\"start\":45857},{\"end\":46026,\"start\":46025},{\"end\":46047,\"start\":46039},{\"end\":46059,\"start\":46055},{\"end\":46251,\"start\":46244},{\"end\":46253,\"start\":46252},{\"end\":46273,\"start\":46266},{\"end\":46285,\"start\":46284},{\"end\":46297,\"start\":46296},{\"end\":46578,\"start\":46574},{\"end\":46592,\"start\":46586},{\"end\":46600,\"start\":46599},{\"end\":46611,\"start\":46607},{\"end\":46841,\"start\":46835},{\"end\":46852,\"start\":46851},{\"end\":46863,\"start\":46862},{\"end\":47105,\"start\":47104},{\"end\":47281,\"start\":47273},{\"end\":47295,\"start\":47290},{\"end\":47465,\"start\":47456},{\"end\":47483,\"start\":47478},{\"end\":47498,\"start\":47491},{\"end\":47506,\"start\":47505},{\"end\":47526,\"start\":47519},{\"end\":47534,\"start\":47533},{\"end\":47543,\"start\":47542},{\"end\":47788,\"start\":47780},{\"end\":47807,\"start\":47801},{\"end\":47817,\"start\":47816},{\"end\":47827,\"start\":47826},{\"end\":48100,\"start\":48091},{\"end\":48278,\"start\":48276},{\"end\":48290,\"start\":48283},{\"end\":48303,\"start\":48297},{\"end\":48317,\"start\":48310},{\"end\":48319,\"start\":48318},{\"end\":48558,\"start\":48556},{\"end\":48570,\"start\":48563},{\"end\":48583,\"start\":48577},{\"end\":48597,\"start\":48590},{\"end\":48599,\"start\":48598},{\"end\":48824,\"start\":48816},{\"end\":48830,\"start\":48829},{\"end\":48850,\"start\":48849},{\"end\":49080,\"start\":49077},{\"end\":49092,\"start\":49085},{\"end\":49106,\"start\":49099},{\"end\":49113,\"start\":49112},{\"end\":49122,\"start\":49120},{\"end\":49354,\"start\":49353},{\"end\":49366,\"start\":49359},{\"end\":49374,\"start\":49373},{\"end\":49388,\"start\":49381},{\"end\":49390,\"start\":49389},{\"end\":49614,\"start\":49612},{\"end\":49627,\"start\":49619},{\"end\":49640,\"start\":49634},{\"end\":49656,\"start\":49647},{\"end\":49669,\"start\":49662},{\"end\":49906,\"start\":49904},{\"end\":49917,\"start\":49911},{\"end\":49932,\"start\":49924},{\"end\":49945,\"start\":49939},{\"end\":49960,\"start\":49951},{\"end\":49969,\"start\":49966},{\"end\":49984,\"start\":49977},{\"end\":50304,\"start\":50300},{\"end\":50313,\"start\":50312},{\"end\":50324,\"start\":50318},{\"end\":50587,\"start\":50581},{\"end\":50602,\"start\":50597},{\"end\":50604,\"start\":50603},{\"end\":50621,\"start\":50614},{\"end\":50641,\"start\":50633},{\"end\":51008,\"start\":51007},{\"end\":51021,\"start\":51014},{\"end\":51035,\"start\":51027},{\"end\":51043,\"start\":51042},{\"end\":51058,\"start\":51050},{\"end\":51264,\"start\":51259},{\"end\":51538,\"start\":51537},{\"end\":51556,\"start\":51546},{\"end\":51567,\"start\":51566},{\"end\":51573,\"start\":51572},{\"end\":51789,\"start\":51780},{\"end\":51817,\"start\":51816},{\"end\":51839,\"start\":51830},{\"end\":51841,\"start\":51840},{\"end\":52290,\"start\":52289},{\"end\":52299,\"start\":52298},{\"end\":52310,\"start\":52309},{\"end\":52322,\"start\":52321},{\"end\":52695,\"start\":52689},{\"end\":52708,\"start\":52704},{\"end\":52719,\"start\":52718},{\"end\":52736,\"start\":52731},{\"end\":53041,\"start\":53040},{\"end\":53050,\"start\":53049},{\"end\":53060,\"start\":53059},{\"end\":53273,\"start\":53266},{\"end\":53292,\"start\":53285},{\"end\":53305,\"start\":53300},{\"end\":53317,\"start\":53311},{\"end\":53336,\"start\":53329},{\"end\":53354,\"start\":53346},{\"end\":53617,\"start\":53616},{\"end\":53630,\"start\":53625},{\"end\":53641,\"start\":53640},{\"end\":53824,\"start\":53817},{\"end\":53834,\"start\":53830},{\"end\":54036,\"start\":54035},{\"end\":54050,\"start\":54042},{\"end\":54063,\"start\":54057},{\"end\":54070,\"start\":54069},{\"end\":54085,\"start\":54079},{\"end\":54087,\"start\":54086},{\"end\":54101,\"start\":54095},{\"end\":54348,\"start\":54347},{\"end\":54362,\"start\":54361},{\"end\":54381,\"start\":54372},{\"end\":54399,\"start\":54394},{\"end\":54414,\"start\":54407},{\"end\":54427,\"start\":54421},{\"end\":54448,\"start\":54441},{\"end\":54461,\"start\":54455},{\"end\":54476,\"start\":54471},{\"end\":54493,\"start\":54488},{\"end\":54502,\"start\":54501},{\"end\":54868,\"start\":54867},{\"end\":54884,\"start\":54883},{\"end\":54900,\"start\":54891},{\"end\":54917,\"start\":54908},{\"end\":55215,\"start\":55214},{\"end\":55335,\"start\":55328},{\"end\":55354,\"start\":55350},{\"end\":55370,\"start\":55364},{\"end\":55385,\"start\":55384},{\"end\":55666,\"start\":55665},{\"end\":55674,\"start\":55673},{\"end\":55693,\"start\":55684},{\"end\":55697,\"start\":55694},{\"end\":55713,\"start\":55707},{\"end\":55724,\"start\":55723},{\"end\":56007,\"start\":56001},{\"end\":56018,\"start\":56014},{\"end\":56038,\"start\":56030},{\"end\":56045,\"start\":56044},{\"end\":56063,\"start\":56057},{\"end\":56293,\"start\":56288},{\"end\":56308,\"start\":56300},{\"end\":56314,\"start\":56313},{\"end\":56477,\"start\":56473},{\"end\":56639,\"start\":56632},{\"end\":56649,\"start\":56648},{\"end\":56820,\"start\":56813},{\"end\":56828,\"start\":56825},{\"end\":56847,\"start\":56839},{\"end\":56856,\"start\":56852},{\"end\":56869,\"start\":56861},{\"end\":56878,\"start\":56876},{\"end\":56886,\"start\":56885},{\"end\":57120,\"start\":57119},{\"end\":57132,\"start\":57125},{\"end\":57134,\"start\":57133},{\"end\":57148,\"start\":57147},{\"end\":57449,\"start\":57445},{\"end\":57468,\"start\":57460},{\"end\":57476,\"start\":57475},{\"end\":57484,\"start\":57481},{\"end\":57646,\"start\":57640},{\"end\":57663,\"start\":57654},{\"end\":57675,\"start\":57671},{\"end\":57690,\"start\":57685},{\"end\":58002,\"start\":57996},{\"end\":58011,\"start\":58010},{\"end\":58030,\"start\":58022},{\"end\":58044,\"start\":58038},{\"end\":58060,\"start\":58053},{\"end\":58347,\"start\":58346},{\"end\":58355,\"start\":58354},{\"end\":58363,\"start\":58361},{\"end\":58373,\"start\":58370},{\"end\":58392,\"start\":58381}]", "bib_author_last_name": "[{\"end\":40864,\"start\":40857},{\"end\":40880,\"start\":40873},{\"end\":40890,\"start\":40884},{\"end\":41168,\"start\":41162},{\"end\":41175,\"start\":41173},{\"end\":41188,\"start\":41184},{\"end\":41203,\"start\":41199},{\"end\":41213,\"start\":41210},{\"end\":41227,\"start\":41225},{\"end\":41241,\"start\":41235},{\"end\":41482,\"start\":41474},{\"end\":41495,\"start\":41489},{\"end\":41513,\"start\":41504},{\"end\":41530,\"start\":41521},{\"end\":41803,\"start\":41797},{\"end\":41823,\"start\":41807},{\"end\":41836,\"start\":41827},{\"end\":42027,\"start\":42022},{\"end\":42043,\"start\":42035},{\"end\":42053,\"start\":42047},{\"end\":42250,\"start\":42241},{\"end\":42262,\"start\":42259},{\"end\":42277,\"start\":42272},{\"end\":42531,\"start\":42527},{\"end\":42545,\"start\":42542},{\"end\":42557,\"start\":42549},{\"end\":42568,\"start\":42561},{\"end\":42829,\"start\":42826},{\"end\":42837,\"start\":42833},{\"end\":43061,\"start\":43050},{\"end\":43074,\"start\":43069},{\"end\":43096,\"start\":43086},{\"end\":43114,\"start\":43103},{\"end\":43128,\"start\":43124},{\"end\":43148,\"start\":43137},{\"end\":43160,\"start\":43152},{\"end\":43179,\"start\":43171},{\"end\":43190,\"start\":43183},{\"end\":43199,\"start\":43194},{\"end\":43216,\"start\":43207},{\"end\":43227,\"start\":43220},{\"end\":43599,\"start\":43590},{\"end\":43616,\"start\":43607},{\"end\":43637,\"start\":43627},{\"end\":43652,\"start\":43646},{\"end\":43871,\"start\":43861},{\"end\":43883,\"start\":43875},{\"end\":43899,\"start\":43891},{\"end\":43907,\"start\":43903},{\"end\":43920,\"start\":43911},{\"end\":44249,\"start\":44245},{\"end\":44256,\"start\":44254},{\"end\":44268,\"start\":44260},{\"end\":44492,\"start\":44485},{\"end\":44502,\"start\":44496},{\"end\":44512,\"start\":44506},{\"end\":44806,\"start\":44801},{\"end\":44824,\"start\":44815},{\"end\":44998,\"start\":44993},{\"end\":45017,\"start\":45009},{\"end\":45030,\"start\":45024},{\"end\":45046,\"start\":45039},{\"end\":45063,\"start\":45053},{\"end\":45084,\"start\":45074},{\"end\":45100,\"start\":45092},{\"end\":45118,\"start\":45109},{\"end\":45469,\"start\":45462},{\"end\":45487,\"start\":45480},{\"end\":45506,\"start\":45497},{\"end\":45516,\"start\":45510},{\"end\":45532,\"start\":45524},{\"end\":45543,\"start\":45536},{\"end\":45855,\"start\":45846},{\"end\":45872,\"start\":45863},{\"end\":46037,\"start\":46027},{\"end\":46053,\"start\":46048},{\"end\":46063,\"start\":46060},{\"end\":46068,\"start\":46065},{\"end\":46264,\"start\":46254},{\"end\":46282,\"start\":46274},{\"end\":46294,\"start\":46286},{\"end\":46305,\"start\":46298},{\"end\":46584,\"start\":46579},{\"end\":46597,\"start\":46593},{\"end\":46605,\"start\":46601},{\"end\":46617,\"start\":46612},{\"end\":46849,\"start\":46842},{\"end\":46860,\"start\":46853},{\"end\":46870,\"start\":46864},{\"end\":47114,\"start\":47106},{\"end\":47288,\"start\":47282},{\"end\":47298,\"start\":47296},{\"end\":47476,\"start\":47466},{\"end\":47489,\"start\":47484},{\"end\":47503,\"start\":47499},{\"end\":47517,\"start\":47507},{\"end\":47531,\"start\":47527},{\"end\":47540,\"start\":47535},{\"end\":47551,\"start\":47544},{\"end\":47799,\"start\":47789},{\"end\":47814,\"start\":47808},{\"end\":47824,\"start\":47818},{\"end\":47837,\"start\":47828},{\"end\":47843,\"start\":47839},{\"end\":48104,\"start\":48101},{\"end\":48281,\"start\":48279},{\"end\":48295,\"start\":48291},{\"end\":48308,\"start\":48304},{\"end\":48330,\"start\":48320},{\"end\":48561,\"start\":48559},{\"end\":48575,\"start\":48571},{\"end\":48588,\"start\":48584},{\"end\":48610,\"start\":48600},{\"end\":48827,\"start\":48825},{\"end\":48847,\"start\":48831},{\"end\":48855,\"start\":48851},{\"end\":48860,\"start\":48857},{\"end\":49083,\"start\":49081},{\"end\":49097,\"start\":49093},{\"end\":49110,\"start\":49107},{\"end\":49118,\"start\":49114},{\"end\":49125,\"start\":49123},{\"end\":49357,\"start\":49355},{\"end\":49371,\"start\":49367},{\"end\":49379,\"start\":49375},{\"end\":49401,\"start\":49391},{\"end\":49617,\"start\":49615},{\"end\":49632,\"start\":49628},{\"end\":49645,\"start\":49641},{\"end\":49660,\"start\":49657},{\"end\":49673,\"start\":49670},{\"end\":49909,\"start\":49907},{\"end\":49922,\"start\":49918},{\"end\":49937,\"start\":49933},{\"end\":49949,\"start\":49946},{\"end\":49964,\"start\":49961},{\"end\":49975,\"start\":49970},{\"end\":49988,\"start\":49985},{\"end\":50310,\"start\":50305},{\"end\":50316,\"start\":50314},{\"end\":50329,\"start\":50325},{\"end\":50595,\"start\":50588},{\"end\":50612,\"start\":50605},{\"end\":50631,\"start\":50622},{\"end\":50648,\"start\":50642},{\"end\":51012,\"start\":51009},{\"end\":51025,\"start\":51022},{\"end\":51040,\"start\":51036},{\"end\":51048,\"start\":51044},{\"end\":51062,\"start\":51059},{\"end\":51281,\"start\":51265},{\"end\":51287,\"start\":51283},{\"end\":51544,\"start\":51539},{\"end\":51564,\"start\":51557},{\"end\":51570,\"start\":51568},{\"end\":51583,\"start\":51574},{\"end\":51814,\"start\":51790},{\"end\":51828,\"start\":51818},{\"end\":51847,\"start\":51842},{\"end\":51856,\"start\":51849},{\"end\":52296,\"start\":52291},{\"end\":52307,\"start\":52300},{\"end\":52319,\"start\":52311},{\"end\":52329,\"start\":52323},{\"end\":52338,\"start\":52331},{\"end\":52702,\"start\":52696},{\"end\":52716,\"start\":52709},{\"end\":52729,\"start\":52720},{\"end\":52742,\"start\":52737},{\"end\":53047,\"start\":53042},{\"end\":53057,\"start\":53051},{\"end\":53067,\"start\":53061},{\"end\":53283,\"start\":53274},{\"end\":53298,\"start\":53293},{\"end\":53309,\"start\":53306},{\"end\":53327,\"start\":53318},{\"end\":53344,\"start\":53337},{\"end\":53361,\"start\":53355},{\"end\":53623,\"start\":53618},{\"end\":53638,\"start\":53631},{\"end\":53647,\"start\":53642},{\"end\":53828,\"start\":53825},{\"end\":53841,\"start\":53835},{\"end\":54040,\"start\":54037},{\"end\":54055,\"start\":54051},{\"end\":54067,\"start\":54064},{\"end\":54077,\"start\":54071},{\"end\":54093,\"start\":54088},{\"end\":54107,\"start\":54102},{\"end\":54359,\"start\":54349},{\"end\":54370,\"start\":54363},{\"end\":54392,\"start\":54382},{\"end\":54405,\"start\":54400},{\"end\":54419,\"start\":54415},{\"end\":54439,\"start\":54428},{\"end\":54453,\"start\":54449},{\"end\":54469,\"start\":54462},{\"end\":54486,\"start\":54477},{\"end\":54499,\"start\":54494},{\"end\":54514,\"start\":54503},{\"end\":54881,\"start\":54869},{\"end\":54889,\"start\":54885},{\"end\":54906,\"start\":54901},{\"end\":54923,\"start\":54918},{\"end\":54937,\"start\":54925},{\"end\":54962,\"start\":54939},{\"end\":55222,\"start\":55216},{\"end\":55348,\"start\":55336},{\"end\":55362,\"start\":55355},{\"end\":55382,\"start\":55371},{\"end\":55398,\"start\":55386},{\"end\":55671,\"start\":55667},{\"end\":55682,\"start\":55675},{\"end\":55705,\"start\":55698},{\"end\":55721,\"start\":55714},{\"end\":55734,\"start\":55725},{\"end\":56012,\"start\":56008},{\"end\":56028,\"start\":56019},{\"end\":56042,\"start\":56039},{\"end\":56055,\"start\":56046},{\"end\":56071,\"start\":56064},{\"end\":56298,\"start\":56294},{\"end\":56311,\"start\":56309},{\"end\":56318,\"start\":56315},{\"end\":56486,\"start\":56478},{\"end\":56646,\"start\":56640},{\"end\":56654,\"start\":56650},{\"end\":56823,\"start\":56821},{\"end\":56837,\"start\":56829},{\"end\":56850,\"start\":56848},{\"end\":56859,\"start\":56857},{\"end\":56874,\"start\":56870},{\"end\":56883,\"start\":56879},{\"end\":56892,\"start\":56887},{\"end\":57123,\"start\":57121},{\"end\":57145,\"start\":57135},{\"end\":57153,\"start\":57149},{\"end\":57458,\"start\":57450},{\"end\":57473,\"start\":57469},{\"end\":57479,\"start\":57477},{\"end\":57488,\"start\":57485},{\"end\":57493,\"start\":57490},{\"end\":57652,\"start\":57647},{\"end\":57669,\"start\":57664},{\"end\":57683,\"start\":57676},{\"end\":57700,\"start\":57691},{\"end\":58008,\"start\":58003},{\"end\":58020,\"start\":58012},{\"end\":58036,\"start\":58031},{\"end\":58051,\"start\":58045},{\"end\":58065,\"start\":58061},{\"end\":58352,\"start\":58348},{\"end\":58359,\"start\":58356},{\"end\":58368,\"start\":58364},{\"end\":58379,\"start\":58374},{\"end\":58396,\"start\":58393}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":139107958},\"end\":41056,\"start\":40764},{\"attributes\":{\"id\":\"b1\"},\"end\":41465,\"start\":41058},{\"attributes\":{\"doi\":\"arXiv:1907.02893\",\"id\":\"b2\"},\"end\":41727,\"start\":41467},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":53979606},\"end\":41983,\"start\":41729},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":49744838},\"end\":42150,\"start\":41985},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":15610473},\"end\":42447,\"start\":42152},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":8847270},\"end\":42732,\"start\":42449},{\"attributes\":{\"id\":\"b7\"},\"end\":42970,\"start\":42734},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":225039882},\"end\":43519,\"start\":42972},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":542154},\"end\":43813,\"start\":43521},{\"attributes\":{\"id\":\"b10\"},\"end\":44137,\"start\":43815},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":722896},\"end\":44440,\"start\":44139},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":6953475},\"end\":44739,\"start\":44442},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":6755881},\"end\":44934,\"start\":44741},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":2871880},\"end\":45347,\"start\":44936},{\"attributes\":{\"doi\":\"abs/1811.12231\",\"id\":\"b15\",\"matched_paper_id\":54101493},\"end\":45796,\"start\":45349},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":220347682},\"end\":45977,\"start\":45798},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":206594692},\"end\":46198,\"start\":45979},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":215744839},\"end\":46517,\"start\":46200},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":220363892},\"end\":46746,\"start\":46519},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":220483395},\"end\":47024,\"start\":46748},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":14591650},\"end\":47227,\"start\":47026},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":6628106},\"end\":47394,\"start\":47229},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":214728308},\"end\":47737,\"start\":47396},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":212784882},\"end\":47991,\"start\":47739},{\"attributes\":{\"id\":\"b25\"},\"end\":48224,\"start\":47993},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":6037691},\"end\":48489,\"start\":48226},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":1883787},\"end\":48757,\"start\":48491},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":52833113},\"end\":49005,\"start\":48759},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":219979590},\"end\":49286,\"start\":49007},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":59523649},\"end\":49545,\"start\":49288},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":19158057},\"end\":49827,\"start\":49547},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":52956008},\"end\":50190,\"start\":49829},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":211205159},\"end\":50495,\"start\":50192},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":16435300},\"end\":50957,\"start\":50497},{\"attributes\":{\"doi\":\"abs/1910.11645\",\"id\":\"b35\",\"matched_paper_id\":204803849},\"end\":51226,\"start\":50959},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":740063},\"end\":51480,\"start\":51228},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":918513},\"end\":51778,\"start\":51482},{\"attributes\":{\"id\":\"b38\"},\"end\":52226,\"start\":51780},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":1900911},\"end\":52562,\"start\":52228},{\"attributes\":{\"doi\":\"abs/1911.08731\",\"id\":\"b40\",\"matched_paper_id\":208176471},\"end\":52978,\"start\":52564},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":12570770},\"end\":53185,\"start\":52980},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":220266097},\"end\":53569,\"start\":53187},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":309759},\"end\":53753,\"start\":53571},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":12453047},\"end\":53974,\"start\":53755},{\"attributes\":{\"doi\":\"abs/1909.13231\",\"id\":\"b45\",\"matched_paper_id\":203593944},\"end\":54298,\"start\":53976},{\"attributes\":{\"doi\":\"abs/2105.01601\",\"id\":\"b46\",\"matched_paper_id\":233714958},\"end\":54788,\"start\":54300},{\"attributes\":{\"doi\":\"abs/2012.12877\",\"id\":\"b47\",\"matched_paper_id\":229363322},\"end\":55183,\"start\":54790},{\"attributes\":{\"id\":\"b48\"},\"end\":55269,\"start\":55185},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":2928248},\"end\":55568,\"start\":55271},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":208631738},\"end\":55941,\"start\":55570},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":232278031},\"end\":56232,\"start\":55943},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":216327732},\"end\":56449,\"start\":56234},{\"attributes\":{\"id\":\"b53\"},\"end\":56581,\"start\":56451},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":146101795},\"end\":56762,\"start\":56583},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":208617520},\"end\":57049,\"start\":56764},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":3307381},\"end\":57371,\"start\":57051},{\"attributes\":{\"id\":\"b57\"},\"end\":57638,\"start\":57373},{\"attributes\":{\"doi\":\"abs/1710.09412\",\"id\":\"b58\"},\"end\":57903,\"start\":57640},{\"attributes\":{\"id\":\"b59\"},\"end\":58252,\"start\":57905},{\"attributes\":{\"id\":\"b60\"},\"end\":58549,\"start\":58254},{\"attributes\":{\"id\":\"b61\"},\"end\":58791,\"start\":58551},{\"attributes\":{\"id\":\"b62\"},\"end\":58976,\"start\":58793},{\"attributes\":{\"id\":\"b63\"},\"end\":59161,\"start\":58978},{\"attributes\":{\"id\":\"b64\"},\"end\":59455,\"start\":59163},{\"attributes\":{\"id\":\"b65\"},\"end\":59832,\"start\":59457},{\"attributes\":{\"id\":\"b66\"},\"end\":60706,\"start\":59834},{\"attributes\":{\"id\":\"b67\"},\"end\":60961,\"start\":60708},{\"attributes\":{\"id\":\"b68\"},\"end\":61102,\"start\":60963},{\"attributes\":{\"id\":\"b69\"},\"end\":61301,\"start\":61104},{\"attributes\":{\"id\":\"b70\"},\"end\":61558,\"start\":61303},{\"attributes\":{\"id\":\"b71\"},\"end\":61806,\"start\":61560}]", "bib_title": "[{\"end\":40853,\"start\":40764},{\"end\":41793,\"start\":41729},{\"end\":42015,\"start\":41985},{\"end\":42232,\"start\":42152},{\"end\":42521,\"start\":42449},{\"end\":43046,\"start\":42972},{\"end\":43583,\"start\":43521},{\"end\":44238,\"start\":44139},{\"end\":44480,\"start\":44442},{\"end\":44790,\"start\":44741},{\"end\":44982,\"start\":44936},{\"end\":45453,\"start\":45349},{\"end\":45837,\"start\":45798},{\"end\":46023,\"start\":45979},{\"end\":46242,\"start\":46200},{\"end\":46572,\"start\":46519},{\"end\":46833,\"start\":46748},{\"end\":47102,\"start\":47026},{\"end\":47271,\"start\":47229},{\"end\":47454,\"start\":47396},{\"end\":47778,\"start\":47739},{\"end\":48274,\"start\":48226},{\"end\":48554,\"start\":48491},{\"end\":48814,\"start\":48759},{\"end\":49075,\"start\":49007},{\"end\":49351,\"start\":49288},{\"end\":49610,\"start\":49547},{\"end\":49902,\"start\":49829},{\"end\":50298,\"start\":50192},{\"end\":50579,\"start\":50497},{\"end\":51005,\"start\":50959},{\"end\":51257,\"start\":51228},{\"end\":51535,\"start\":51482},{\"end\":52287,\"start\":52228},{\"end\":52687,\"start\":52564},{\"end\":53038,\"start\":52980},{\"end\":53264,\"start\":53187},{\"end\":53614,\"start\":53571},{\"end\":53815,\"start\":53755},{\"end\":54033,\"start\":53976},{\"end\":54345,\"start\":54300},{\"end\":54865,\"start\":54790},{\"end\":55326,\"start\":55271},{\"end\":55663,\"start\":55570},{\"end\":55999,\"start\":55943},{\"end\":56286,\"start\":56234},{\"end\":56630,\"start\":56583},{\"end\":56811,\"start\":56764},{\"end\":57117,\"start\":57051}]", "bib_author": "[{\"end\":40866,\"start\":40855},{\"end\":40882,\"start\":40866},{\"end\":40892,\"start\":40882},{\"end\":41170,\"start\":41152},{\"end\":41177,\"start\":41170},{\"end\":41190,\"start\":41177},{\"end\":41205,\"start\":41190},{\"end\":41215,\"start\":41205},{\"end\":41229,\"start\":41215},{\"end\":41243,\"start\":41229},{\"end\":41484,\"start\":41467},{\"end\":41497,\"start\":41484},{\"end\":41515,\"start\":41497},{\"end\":41532,\"start\":41515},{\"end\":41805,\"start\":41795},{\"end\":41825,\"start\":41805},{\"end\":41838,\"start\":41825},{\"end\":42029,\"start\":42017},{\"end\":42045,\"start\":42029},{\"end\":42055,\"start\":42045},{\"end\":42252,\"start\":42234},{\"end\":42264,\"start\":42252},{\"end\":42279,\"start\":42264},{\"end\":42533,\"start\":42523},{\"end\":42547,\"start\":42533},{\"end\":42559,\"start\":42547},{\"end\":42570,\"start\":42559},{\"end\":42831,\"start\":42818},{\"end\":42839,\"start\":42831},{\"end\":43063,\"start\":43048},{\"end\":43076,\"start\":43063},{\"end\":43098,\"start\":43076},{\"end\":43116,\"start\":43098},{\"end\":43130,\"start\":43116},{\"end\":43150,\"start\":43130},{\"end\":43162,\"start\":43150},{\"end\":43181,\"start\":43162},{\"end\":43192,\"start\":43181},{\"end\":43201,\"start\":43192},{\"end\":43218,\"start\":43201},{\"end\":43229,\"start\":43218},{\"end\":43601,\"start\":43585},{\"end\":43618,\"start\":43601},{\"end\":43639,\"start\":43618},{\"end\":43654,\"start\":43639},{\"end\":43873,\"start\":43859},{\"end\":43885,\"start\":43873},{\"end\":43901,\"start\":43885},{\"end\":43909,\"start\":43901},{\"end\":43922,\"start\":43909},{\"end\":44251,\"start\":44240},{\"end\":44258,\"start\":44251},{\"end\":44270,\"start\":44258},{\"end\":44494,\"start\":44482},{\"end\":44504,\"start\":44494},{\"end\":44514,\"start\":44504},{\"end\":44808,\"start\":44792},{\"end\":44826,\"start\":44808},{\"end\":45000,\"start\":44984},{\"end\":45019,\"start\":45000},{\"end\":45032,\"start\":45019},{\"end\":45048,\"start\":45032},{\"end\":45065,\"start\":45048},{\"end\":45086,\"start\":45065},{\"end\":45102,\"start\":45086},{\"end\":45120,\"start\":45102},{\"end\":45471,\"start\":45455},{\"end\":45489,\"start\":45471},{\"end\":45508,\"start\":45489},{\"end\":45518,\"start\":45508},{\"end\":45534,\"start\":45518},{\"end\":45545,\"start\":45534},{\"end\":45857,\"start\":45839},{\"end\":45874,\"start\":45857},{\"end\":46039,\"start\":46025},{\"end\":46055,\"start\":46039},{\"end\":46065,\"start\":46055},{\"end\":46070,\"start\":46065},{\"end\":46266,\"start\":46244},{\"end\":46284,\"start\":46266},{\"end\":46296,\"start\":46284},{\"end\":46307,\"start\":46296},{\"end\":46586,\"start\":46574},{\"end\":46599,\"start\":46586},{\"end\":46607,\"start\":46599},{\"end\":46619,\"start\":46607},{\"end\":46851,\"start\":46835},{\"end\":46862,\"start\":46851},{\"end\":46872,\"start\":46862},{\"end\":47116,\"start\":47104},{\"end\":47290,\"start\":47273},{\"end\":47300,\"start\":47290},{\"end\":47478,\"start\":47456},{\"end\":47491,\"start\":47478},{\"end\":47505,\"start\":47491},{\"end\":47519,\"start\":47505},{\"end\":47533,\"start\":47519},{\"end\":47542,\"start\":47533},{\"end\":47553,\"start\":47542},{\"end\":47801,\"start\":47780},{\"end\":47816,\"start\":47801},{\"end\":47826,\"start\":47816},{\"end\":47839,\"start\":47826},{\"end\":47845,\"start\":47839},{\"end\":48106,\"start\":48091},{\"end\":48283,\"start\":48276},{\"end\":48297,\"start\":48283},{\"end\":48310,\"start\":48297},{\"end\":48332,\"start\":48310},{\"end\":48563,\"start\":48556},{\"end\":48577,\"start\":48563},{\"end\":48590,\"start\":48577},{\"end\":48612,\"start\":48590},{\"end\":48829,\"start\":48816},{\"end\":48849,\"start\":48829},{\"end\":48857,\"start\":48849},{\"end\":48862,\"start\":48857},{\"end\":49085,\"start\":49077},{\"end\":49099,\"start\":49085},{\"end\":49112,\"start\":49099},{\"end\":49120,\"start\":49112},{\"end\":49127,\"start\":49120},{\"end\":49359,\"start\":49353},{\"end\":49373,\"start\":49359},{\"end\":49381,\"start\":49373},{\"end\":49403,\"start\":49381},{\"end\":49619,\"start\":49612},{\"end\":49634,\"start\":49619},{\"end\":49647,\"start\":49634},{\"end\":49662,\"start\":49647},{\"end\":49675,\"start\":49662},{\"end\":49911,\"start\":49904},{\"end\":49924,\"start\":49911},{\"end\":49939,\"start\":49924},{\"end\":49951,\"start\":49939},{\"end\":49966,\"start\":49951},{\"end\":49977,\"start\":49966},{\"end\":49990,\"start\":49977},{\"end\":50312,\"start\":50300},{\"end\":50318,\"start\":50312},{\"end\":50331,\"start\":50318},{\"end\":50597,\"start\":50581},{\"end\":50614,\"start\":50597},{\"end\":50633,\"start\":50614},{\"end\":50650,\"start\":50633},{\"end\":51014,\"start\":51007},{\"end\":51027,\"start\":51014},{\"end\":51042,\"start\":51027},{\"end\":51050,\"start\":51042},{\"end\":51064,\"start\":51050},{\"end\":51283,\"start\":51259},{\"end\":51289,\"start\":51283},{\"end\":51546,\"start\":51537},{\"end\":51566,\"start\":51546},{\"end\":51572,\"start\":51566},{\"end\":51585,\"start\":51572},{\"end\":51816,\"start\":51780},{\"end\":51830,\"start\":51816},{\"end\":51849,\"start\":51830},{\"end\":51858,\"start\":51849},{\"end\":52298,\"start\":52289},{\"end\":52309,\"start\":52298},{\"end\":52321,\"start\":52309},{\"end\":52331,\"start\":52321},{\"end\":52340,\"start\":52331},{\"end\":52704,\"start\":52689},{\"end\":52718,\"start\":52704},{\"end\":52731,\"start\":52718},{\"end\":52744,\"start\":52731},{\"end\":53049,\"start\":53040},{\"end\":53059,\"start\":53049},{\"end\":53069,\"start\":53059},{\"end\":53285,\"start\":53266},{\"end\":53300,\"start\":53285},{\"end\":53311,\"start\":53300},{\"end\":53329,\"start\":53311},{\"end\":53346,\"start\":53329},{\"end\":53363,\"start\":53346},{\"end\":53625,\"start\":53616},{\"end\":53640,\"start\":53625},{\"end\":53649,\"start\":53640},{\"end\":53830,\"start\":53817},{\"end\":53843,\"start\":53830},{\"end\":54042,\"start\":54035},{\"end\":54057,\"start\":54042},{\"end\":54069,\"start\":54057},{\"end\":54079,\"start\":54069},{\"end\":54095,\"start\":54079},{\"end\":54109,\"start\":54095},{\"end\":54361,\"start\":54347},{\"end\":54372,\"start\":54361},{\"end\":54394,\"start\":54372},{\"end\":54407,\"start\":54394},{\"end\":54421,\"start\":54407},{\"end\":54441,\"start\":54421},{\"end\":54455,\"start\":54441},{\"end\":54471,\"start\":54455},{\"end\":54488,\"start\":54471},{\"end\":54501,\"start\":54488},{\"end\":54516,\"start\":54501},{\"end\":54883,\"start\":54867},{\"end\":54891,\"start\":54883},{\"end\":54908,\"start\":54891},{\"end\":54925,\"start\":54908},{\"end\":54939,\"start\":54925},{\"end\":54964,\"start\":54939},{\"end\":55224,\"start\":55214},{\"end\":55350,\"start\":55328},{\"end\":55364,\"start\":55350},{\"end\":55384,\"start\":55364},{\"end\":55400,\"start\":55384},{\"end\":55673,\"start\":55665},{\"end\":55684,\"start\":55673},{\"end\":55707,\"start\":55684},{\"end\":55723,\"start\":55707},{\"end\":55736,\"start\":55723},{\"end\":56014,\"start\":56001},{\"end\":56030,\"start\":56014},{\"end\":56044,\"start\":56030},{\"end\":56057,\"start\":56044},{\"end\":56073,\"start\":56057},{\"end\":56300,\"start\":56288},{\"end\":56313,\"start\":56300},{\"end\":56320,\"start\":56313},{\"end\":56488,\"start\":56473},{\"end\":56648,\"start\":56632},{\"end\":56656,\"start\":56648},{\"end\":56825,\"start\":56813},{\"end\":56839,\"start\":56825},{\"end\":56852,\"start\":56839},{\"end\":56861,\"start\":56852},{\"end\":56876,\"start\":56861},{\"end\":56885,\"start\":56876},{\"end\":56894,\"start\":56885},{\"end\":57125,\"start\":57119},{\"end\":57147,\"start\":57125},{\"end\":57155,\"start\":57147},{\"end\":57460,\"start\":57445},{\"end\":57475,\"start\":57460},{\"end\":57481,\"start\":57475},{\"end\":57490,\"start\":57481},{\"end\":57495,\"start\":57490},{\"end\":57654,\"start\":57640},{\"end\":57671,\"start\":57654},{\"end\":57685,\"start\":57671},{\"end\":57702,\"start\":57685},{\"end\":58010,\"start\":57996},{\"end\":58022,\"start\":58010},{\"end\":58038,\"start\":58022},{\"end\":58053,\"start\":58038},{\"end\":58067,\"start\":58053},{\"end\":58354,\"start\":58346},{\"end\":58361,\"start\":58354},{\"end\":58370,\"start\":58361},{\"end\":58381,\"start\":58370},{\"end\":58398,\"start\":58381}]", "bib_venue": "[{\"end\":40901,\"start\":40892},{\"end\":41150,\"start\":41058},{\"end\":41575,\"start\":41548},{\"end\":41848,\"start\":41838},{\"end\":42059,\"start\":42055},{\"end\":42283,\"start\":42279},{\"end\":42574,\"start\":42570},{\"end\":42816,\"start\":42734},{\"end\":43233,\"start\":43229},{\"end\":43659,\"start\":43654},{\"end\":43857,\"start\":43815},{\"end\":44274,\"start\":44270},{\"end\":44576,\"start\":44514},{\"end\":44830,\"start\":44826},{\"end\":45124,\"start\":45120},{\"end\":45564,\"start\":45559},{\"end\":45878,\"start\":45874},{\"end\":46074,\"start\":46070},{\"end\":46324,\"start\":46307},{\"end\":46623,\"start\":46619},{\"end\":46877,\"start\":46872},{\"end\":47120,\"start\":47116},{\"end\":47304,\"start\":47300},{\"end\":47557,\"start\":47553},{\"end\":47849,\"start\":47845},{\"end\":48089,\"start\":47993},{\"end\":48336,\"start\":48332},{\"end\":48616,\"start\":48612},{\"end\":48866,\"start\":48862},{\"end\":49131,\"start\":49127},{\"end\":49407,\"start\":49403},{\"end\":49679,\"start\":49675},{\"end\":49994,\"start\":49990},{\"end\":50335,\"start\":50331},{\"end\":50712,\"start\":50650},{\"end\":51083,\"start\":51078},{\"end\":51340,\"start\":51289},{\"end\":51616,\"start\":51585},{\"end\":51987,\"start\":51858},{\"end\":52380,\"start\":52340},{\"end\":52763,\"start\":52758},{\"end\":53073,\"start\":53069},{\"end\":53370,\"start\":53363},{\"end\":53653,\"start\":53649},{\"end\":53857,\"start\":53843},{\"end\":54128,\"start\":54123},{\"end\":54535,\"start\":54530},{\"end\":54983,\"start\":54978},{\"end\":55212,\"start\":55185},{\"end\":55404,\"start\":55400},{\"end\":55740,\"start\":55736},{\"end\":56077,\"start\":56073},{\"end\":56326,\"start\":56320},{\"end\":56471,\"start\":56451},{\"end\":56660,\"start\":56656},{\"end\":56898,\"start\":56894},{\"end\":57195,\"start\":57155},{\"end\":57443,\"start\":57373},{\"end\":57764,\"start\":57716},{\"end\":57994,\"start\":57905},{\"end\":58344,\"start\":58254},{\"end\":58662,\"start\":58551},{\"end\":58862,\"start\":58793},{\"end\":59065,\"start\":58978},{\"end\":59231,\"start\":59163},{\"end\":59614,\"start\":59457},{\"end\":59921,\"start\":59834},{\"end\":60829,\"start\":60708},{\"end\":61030,\"start\":60963},{\"end\":61187,\"start\":61104},{\"end\":61425,\"start\":61303},{\"end\":61677,\"start\":61560}]"}}}, "year": 2023, "month": 12, "day": 17}