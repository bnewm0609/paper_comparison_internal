{"id": 209435713, "updated": "2023-12-13 19:16:31.471", "metadata": {"title": "Convolutional-Neural Network-Based Image Crowd Counting: Review, Categorization, Analysis, and Performance Evaluation", "authors": "[{\"first\":\"Naveed\",\"last\":\"Ilyas\",\"middle\":[]},{\"first\":\"Ahsan\",\"last\":\"Shahzad\",\"middle\":[]},{\"first\":\"Kiseon\",\"last\":\"Kim\",\"middle\":[]}]", "venue": "Sensors (Basel, Switzerland)", "journal": "Sensors (Basel, Switzerland)", "publication_date": {"year": 2019, "month": 12, "day": 19}, "abstract": "Traditional handcrafted crowd-counting techniques in an image are currently transformed via machine-learning and artificial-intelligence techniques into intelligent crowd-counting techniques. This paradigm shift offers many advanced features in terms of adaptive monitoring and the control of dynamic crowd gatherings. Adaptive monitoring, identification/recognition, and the management of diverse crowd gatherings can improve many crowd-management-related tasks in terms of efficiency, capacity, reliability, and safety. Despite many challenges, such as occlusion, clutter, and irregular object distribution and nonuniform object scale, convolutional neural networks are a promising technology for intelligent image crowd counting and analysis. In this article, we review, categorize, analyze (limitations and distinctive features), and provide a detailed performance evaluation of the latest convolutional-neural-network-based crowd-counting techniques. We also highlight the potential applications of convolutional-neural-network-based crowd-counting techniques. Finally, we conclude this article by presenting our key observations, providing strong foundation for future research directions while designing convolutional-neural-network-based crowd-counting techniques. Further, the article discusses new advancements toward understanding crowd counting in smart cities using the Internet of Things (IoT).", "fields_of_study": "[\"Medicine\"]", "external_ids": {"arxiv": null, "mag": "2995304495", "acl": null, "pubmed": "31861734", "pubmedcentral": "6983207", "dblp": "journals/sensors/IlyasSK20", "doi": "10.3390/s20010043"}}, "content": {"source": {"pdf_hash": "481df19b8553a7d0a6916c983c84ff5f1bc8818f", "pdf_src": "PubMedCentral", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": "CCBY", "open_access_url": "https://doi.org/10.3390/s20010043", "status": "GOLD"}}, "grobid": {"id": "65719834e15315ac2593ce053bd85805fb784840", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/481df19b8553a7d0a6916c983c84ff5f1bc8818f.txt", "contents": "\nConvolutional-Neural Network-Based Image Crowd Counting: Review, Categorization, Analysis, and Performance Evaluation\nPublished: 19 December 2019\n\nNaveed Ilyas naveedilyaas@gmail.com \nSchool of Electrical Engineering and Computer Science\nGwangju Institute of Science and Technology (GIST)\n61005GwangjuKorea\n\nAhsan Shahzad ahsan.shahzad@ceme.nust.edu.pk*correspondence:kskim@gist.ac.kr \nDepartment of Computer and Software Engineering (DCSE)\nCollege of Electrical and Mechanical Engineering (EME)\nof Sciences and Technology (NUST)\nNational University\n44000IslamabadPakistan\n\nKiseon Kim \nSchool of Electrical Engineering and Computer Science\nGwangju Institute of Science and Technology (GIST)\n61005GwangjuKorea\n\nConvolutional-Neural Network-Based Image Crowd Counting: Review, Categorization, Analysis, and Performance Evaluation\nPublished: 19 December 201910.3390/s20010043Received: 23 November 2019; Accepted: 13 December 2019;sensors Reviewdeep learningcrowd analysissmart cities\nTraditional handcrafted crowd-counting techniques in an image are currently transformed via machine-learning and artificial-intelligence techniques into intelligent crowd-counting techniques. This paradigm shift offers many advanced features in terms of adaptive monitoring and the control of dynamic crowd gatherings. Adaptive monitoring, identification/recognition, and the management of diverse crowd gatherings can improve many crowd-management-related tasks in terms of efficiency, capacity, reliability, and safety. Despite many challenges, such as occlusion, clutter, and irregular object distribution and nonuniform object scale, convolutional neural networks are a promising technology for intelligent image crowd counting and analysis. In this article, we review, categorize, analyze (limitations and distinctive features), and provide a detailed performance evaluation of the latest convolutional-neural-network-based crowd-counting techniques. We also highlight the potential applications of convolutional-neural-network-based crowd-counting techniques. Finally, we conclude this article by presenting our key observations, providing strong foundation for future research directions while designing convolutional-neural-network-based crowd-counting techniques. Further, the article discusses new advancements toward understanding crowd counting in smart cities using the Internet of Things (IoT).\n\nIntroduction\n\nCrowd counting (CC) aims to count the number of objects, such as people, cars, cells, and drones in still images or videos. It can be performed in different ways, such as digital-image processing, machine learning, and deep learning. More specifically, crowd counting can be done through various state-of-the-art techniques, such as counting by detection [1][2][3] regression [4][5][6][7][8], density estimation [9,10] and clustering [11][12][13][14]. The problem of crowd counting is of significant importance in computer vision due to its wide variety of applications in urban planning, anomaly detection, video supervenience, public safety management, defence, healthcare, and disaster management [15][16][17].\n\nCrowd-counting techniques face many challenges, such as high cluttering, varying illumination, varying object density, severe occlusion, and scale variation caused by different perspectives [18][19][20][21][22]. For instance, high cluttering can distort the resolution of an estimated map, and light illumination can reduce its accuracy. Further, varying object density reduces prediction accuracy due to nonuniform density distribution. Similarly, severe occlusion increases prediction error, and scale variation reduces both counting prediction and density-map resolution .\n\nDue to a wide variety of applications, from commercial to military purposes, with significant importance in computer vision, crowd counting is a challenging scientific problem to be solved. A number of researchers tried to provide detailed surveys and analyses of previous techniques by considering various crowd features. These traditional crowd-counting techniques mainly focus on handcrafted low-level crowd features. These low-level features are selected, extracted, and transformed into an organized input for the regression model that is used for loss-function evaluation and minimization. In this regard, comprehensive analysis was provided by Zhan et al. [23] for general crowd counting. They mainly reviewed vision and nonvision problems. In vision-based problems, crowd modelling is based on extracted information from visual data and employed for crowd-event inference. Nonvision approaches, on the other hand, aim to describe and predict the collected effects of crowd behavior by rectifying the relationship between features. Later on, Zitouni et al. [24] focused on crowd-counting models with emphasis on their limitations. Their main contribution was the categorization of crowd-modelling techniques into motion-flow-based models, learnt-appearance-based models, and hybrid approaches. Motion-flow-based models were further subcategorized into optical-flow-based models, Lagrange based methods, and background-subtraction-based models. The authors in [25] investigated crowd-counting techniques by considering different categories, like holistic, intermediate, and local approaches. The authors in [26] focused on conventional and convolutional-neural-network (CNN)-based single-image crowd-counting techniques. They mainly compared the properties of handcrafted crowd-counting techniques with CNN-CC techniques.\n\nTo sum up, the existing above-mentioned surveys and analysis, except for [26], mostly focused on conventional approaches that emphasised handcrafted features to improve the accuracy of crowd analysis. In [26], traditional and CNN-CC algorithms were reviewed. However, the authors did not review recent CNN-based crowd-counting algorithms, which are evaluated on the most challenging and multivariant datasets [27,28]. Further, the aforementioned works did not analyze the advantages and limitations of each technique. The limited categorization of CNN-CC techniques restricts future researchers from fully understanding the scope and available room for improvement in any category. Finally, they did not conduct a quantitative comparison in terms of prediction accuracy. These drawbacks/deficiencies in existing works indirectly and negatively impact system performance. For instance, due to a lack of categorization, the whole domain under any specific category has not been explored. Not considering key aspects such as advantages, disadvantages, limitations, intrinsic features, and multivariant datasets meant that in-depth properties are ignored in the design process of a crowd-counting algorithm. Such scanty investigations usually lead to a huge diversity of simulation results in comparison to the real crowd count.\n\nMotivated by the above-mentioned deficiencies in previous surveys [23][24][25][26], we comprehensively reviewed the most recent CNN-CC techniques to understand the newest trends and highlight room for future research in any particular area. Understanding crowd-mobility behaviors would be a key enabler for crowd management in smart cities, benefiting various sectors such as public safety, tourism, and transportation. The main theme of crowd-counting categorization is to help researchers to further exploit and dive deep into any particular branch to obtain maximum output. This article discusses existing challenges and recent advances to overcome them and allow the sharing of information across stakeholders of crowd management through Internet of Things (IoT) technologies. To summarize, this paper makes the following contributions.\n\n\n\u2022\n\nWe specifically reviewed recent CNN-CC techniques in order to highlight deficiencies, advantages, disadvantages, and key features in each category.\n\n\n\u2022\n\nWe categorized CNN-based methods into three main categories to fully understand evolving research aspects. Previously, authors in [26] categorized CNN-based approaches into two main categories (network-based and training-approach-based). However, by reviewing the literature and observing the overall crowd-counting mechanism from different perspectives, we realized the need for a new category, and thus introduced image-view-based methods.\n\n\u2022 Image-view-based CNN-CC techniques (Image-view-CNN-CC) were further subdivided into arial-view-based (camera and object are perpendicular to each other) and perspective-view-based (camera and object are parallel to each other) methods. Due to this inclusion, crowd counting in health care (microscopic images), counting through unmanned aerial vehicles (UAVs) is further investigated under arial-view-based methods. Moreover, scale-varying issues caused by different perspectives can be further investigated in detail under perspective-view-based methods.\n\n\n\u2022\n\nWe provide detailed quantitative comparison (in term of n Mean Absolute Error (nMAE) within each subcategory of the three main categories, and overall performance-based conclusion under different datasets, such as UCF, WE, STA, and STB.\n\n\n\u2022\n\nBy observing different aspects of CNN-CC, we also highlighted the features of each subcategory with quantitative comparison that provides a strong foundation for future research in highly diverse and robust scenarios.\n\nThe rest of paper is organized as follows. Section 2 is focused on traditional crowd-counting methods and image analysis. In Section 3, we discuss the complete and detailed operation of the crowd-counting mechanism. Section 4 is focused on the categorization of CNN-CC techniques by considering their features, datasets, and architectures. In Section 5, we discuss applications of CNN-CC techniques. In Section 6, we discuss the implications of 3D crowd counting. In Section 7, we provide a quantitative comparison between different CNN-CC techniques. Finally, Section 8 provides the conclusion with future research directions.\n\n\nTraditional Crowd Counting and Image Analysis\n\n\nCrowd Counting\n\nA crowd is defined as a large gathering of people for a specific reason, such as religious occasions, sports events, and political gatherings. Crowd counting is defined as estimating or counting the number of people in an image or video [29,30]. Techniques for crowd counting are divided into two basic categories: supervised and unsupervised crowd counting. In supervised crowd counting, the input data are known and labelled, and the machine is only used to determine the objective function (hidden pattern). In unsupervised crowd counting, the used data and labels are unknown, and the machine is used to categorize and label the raw data before determining the objective function. These categories are further divided into different types, as shown in Figure 1. Supervised crowd counting is further divided into counting by regression, density estimation, detection, and CNN [31,32]. The unsupervised category, on the other hand, includes counting by clustering. Their descriptions are as follows. \n\n\nCounting by Detection\n\nCounting by detection can be defined as a method to compute the abstraction of image information and local decisions at every point to know about features of a particular type at that point. The authors in [33] proposed a CNN-based hybrid hidden Markov model (HHMM) for speech recognition. The HMM is used to obtain inherent dynamic features that can be used for anomaly detection in crowd analysis. The authors in [34,35] found a solution for reconstructing full-body locomotion that could be used in 3D crowd analysis and abnormal-behavior detection. The earlier research focused on detection-based counting to count the number of people in a scene [3]. Through a sliding-window detector, detection could be monolithic or part-based. Traditional pedestrian-detection techniques used monolithic detection [16,[36][37][38]. In these techniques, a classifier is trained by using different features, including Histogram Oriented Gradient (HOG) [16], edgelet [39] and a shapelet [40] extracted from the body of people. The monolithic way of detection performs very well in low-density crowds, but its performance degrades in high density. Therefore, researchers were motivated to address this issue by using part-based detection techniques [41,42] that use boosted classifiers for specific body parts, including shoulders and head, to estimate the count in that area [43].\n\n\nCounting by Regression\n\nCounting by regression is carried out to obtain a more robust and accurate function via known inputs of images and output (ground truth). The authors in [34,35] determined a solution on the basis of reconstructing full-body locomotion that could be applicable in 3D crowd analysis and anomaly detection. Regression-based crowd-density estimation was first exploited by Davies et al. [7]. The extraction of low-level features (foreground area and edge features) is carried out in the video frame. The total edge count and foreground area are extracted from the raw features. In this way, a linear-regression model was developed to establish mapping between actual and estimated count. Shape-and part-based detectors are not successful in the presence of high-density crowds and high-clutter backgrounds. The main components that establish counting by a regression pipeline are low-level feature extraction and regression modelling [4]. Different features, such as gradient, foreground, and edge features, and textures are used to encode low-level information. Further, standard background-subtraction techniques are used for the extraction of foreground features that are removed from foreground segments. Blob-based holistic features, such as perimeter, area, and perimeter-area ratio have had promising results [4,25]. However, these techniques focus on the global properties of the scene. Local features and textures like Gray Level Co-Occurence Metrics (GLCM), HOG, and Local Binary Pattern (LBP), are used to further improve the accuracy of classification, detection, and crowd counting. After the extraction of local and global features, a variety of regression methods, including linear [44], Gaussian, [45], and ridge regression [7], and NNs [46] are used to learn mapping between the actual crowd count and low-level features.\n\n\nCounting by Density Estimation\n\nCounting through density estimation is employed to obtain an estimate by using observed data of an unobservable probability-density function. This technique has made it possible to overcome the problem of occlusion and clutter by using spatial information with a density-estimation approach. For example, Lempitsky et al. [10] incorporated spatial information by proposing linear mapping between local features and estimated-density (ED) maps. The difficult task of detecting and localizing individual objects has been eliminated by calculating image density whose integral in any particular region provides the estimated count of that region. In [10], cutting-plane optimization is used to solve convex optimization tasks by introducing a risk-based quadratic cost function.\n\n\nCounting by CNN\n\nThough detection, regression, clustering, and density-estimation-based crowd-counting techniques perform well to some extent by using handcrafted features, for crowd analysis, motion analysis, and the 3D construction of body parts, different types of CNN-and LSTM-based algorithms have been proposed. In particular, the authors in [47] and [48] proposed a CNN-based descriptor and LSTM-based network to obtain motion and appearance information along the tracks of human body parts. Similarly, the authors in [49] investigated 3D face-model construction by using a 2D view of the face. Further, the authors in [50] investigated deep-learning architecture for the classification of a driver's actions. Abstractive text summary using a generative adversarial network was done by the authors in [51], while the authors in [52] proposed a CNN-based technique to obtain high representational features for the detection of secondary protein structures. In order to further improve accuracy, researchers used CNN-based crowd-counting techniques [21,53,54]. Counting through CNN employs convolution, pooling, Rectified Linear Unit (RelU), and Fully Connected Layers (FCLs) to extract features that are used to obtain the density map [55]. Counting through CNN is more efficient in terms of accuracy, but at the cost of high computational complexity.\n\n\nCounting by Clustering\n\nCounting by clustering relies on the assumption that visual features and individual motion fields are uniform, so similar features are grouped into different categories. For example, [13] used a Kanade-Lucas-Tomasi (KLT) tracker to obtain low-level features, and then employed Bayesian clustering [14] to find the approximate number of people in an image. The aforementioned methods explicitly model appearance features. Thus, false estimation arises when people remain in static position or when objects repeatedly share the same trajectories. Hence, we concluded that counting by clustering performs better in continuous image frames.\n\n\nImage Analysis\n\nImage analysis is widely used to extract useful information from an image, specifically digital images, by using different techniques like digital-image processing, machine learning, and deep learning [56]. Inspired by the phenomenon of the human-visual-cortex system, CNNs extract high-level features from an image. Image analysis has more subfields like pattern recognition, digital geometry, medical imaging, and computer vision [57][58][59][60]. These subfields cover various modern-day applications in astronomy, defence, filtering, microscopy, remote sensing, robotics, and machine vision [61,62].\n\n\nUnique Challenges of CNN-Based Image Crowd Counting\n\nCNN-based crowd counting faces many challenges that restrict the counting accuracy of these networks (i.e., MAE, MSE, and ED) and the resolution of the density map. These challenges are depicted in Figure 2 and explained below.\n\n\n\u2022\n\nOcclusion occurs when two or more objects come very close to each other and merge, so that it is hard to recognize individual objects. Thus, crowd-counting accuracy is decreased [18].\n\n\n\u2022\n\nClutter is a kind of nonuniform arrangement of objects that are close to each other. It is also related to image noise, making recognition and counting tasks more challenging [19]. \u2022 Irregular object distribution refers to varying density distribution in an image or a video. For irregular objects, counting through detection is only viable in sparse areas. On the other hand, counting by regression overestimates the sparse areas and is only viable in dense areas. Thus, the irregular distribution of an object is a challenging task for crowd counting [20]. \u2022 Nonuniform object scales often occur due to different perspectives. In counting, objects close to the camera look larger when compared to ones farther away. The nearest objects have more pixels than far-away objects. Thus, ground-truth and actual-density estimations are affected by the nonuniform pixel distribution of the same object [21].\n\n\n\u2022\n\nAn inconstant perspective occurs due to different camera angles, tilt, and the up-down movement of the camera position. Object recognition and counting accuracy are greatly affected by varying perspectives [22].  \n\n\nMotivation for Employing CNN-Based Image Crowd Counting\n\nTraditional handcrafted crowd-counting techniques such as those in [1,14] perform well if the training dataset has a low computational cost. However, challenges like occlusion, clutter, and scale variation reduce the accuracy of such traditional methods. In addition, the ED map obtained by employing these handcrafted methods has a low resolution that limits their applicability in many areas, such as medical imaging and military applications. In short, the manual nature of feature extraction by handcrafted methods makes them less (non)adaptive to evolving crowd-counting demands. By observing the above-mentioned deficiencies in traditional crowd-counting algorithms, and the success of CNNs in numerous computer-vision applications, researchers were inspired to exploit their ability in estimating the nonlinear feature density maps of crowd images [53][54][55]. These density maps can be utilized in machine-learning processes for more accurate prediction/estimation of the crowd count [63,64]. Further, up-and downsampling, scale aggregation, and preclassification with a multicolumn approach could also be used to increase the accuracy of crowd counting. On the other hand, deconvolution [65] and Generative Adversarial Networks (GANs) [66] can be employed to enhance the quality of a density map for medical applications.\n\n\nCNN-Based Crowd Counting: Overview\n\nCNNs are useful in numerous applications, such as signal processing, image processing, and computer vision. In this regard, various CNN-CC algorithms were proposed to cope with major issues like occlusion, low visibility, inter-and intraobject variation, and scale variation due to different perspectives. A generic CNN-CC flow diagram is shown in Figure 3 that depicts two approaches. The first, on the left, found ground-truth density (GTD) except for the last two blocks, which were used for comparison and error computation. The second, on the right, computed ED and crowd counting. The description of each block is as follows.  Figure 3. General form of CNN-CC algorithm. Crowd-counting mechanism starts from object annotation in an image to density estimation; object counting is depicted. General framework of crowd counting (top), and CNN working is expanded (bottom).\n\nLabelling: In machine learning, annotation is a process of labeling data such as text, audio, and image. The annotated data are used by a computer or computers to recognize similar patterns in unseen data. There are different annotation categories, such as bounding-box annotation, polygonal segmentation, line annotation, landmark annotation, 3D cuboids, and dot annotation [67]. These different types of annotations are used to obtain the ground truth. In crowd counting, dot annotation is the first step to compute the GTD, and it is carried through various scientific tools like Labelbox, LabelMe, and RectLabel [10].\n\nGTD computation: Ground truth can be defined as the information provided by direct observation instead of inference. There are different ways to obtain the GTD, such as Gaussian kernel, geometry-adaptive kernel, and GANs [8]. The geometry-adaptive kernel performs better than the Gaussian kernel. This is due considering spread-factor-based geometric information. Further, a combination of generative and discriminative networks brings the generated image very close to the original one. Therefore, the obtained GTD from GANs has better quality as compared to that of the Gaussian, geometry, or body-aware techniques.\n\nGTD and ED comparison: In crowd counting, ED and GTD are compared to compute the loss between estimated output and ground truth. In the literature, different techniques were employed to compute loss, such as cross-entropy and MSE [17]. A combination of sigmoid and MSE converge much slower as compared to sigmoid and cross-entropy due to the gradient-vanishing problem. Cross-entropy, on the other hand, performs well on classification problems, but better performance was shown in terms of MSE in regression-based problems.\n\nWeight Computation: After comparing the loss between ED and GTD, the next step is to update the network weight to minimize loss. The updated weight is computed by W new = W old + \u03b7 \u2202L \u2202W . This weight update process (backpropagated to the CNN) is terminated when loss is minimized (process converges). W old and W new depict the old and new weights, where the last term is the combination of learning rate \u03b7 and change in loss with respect to weight [17].\n\nCNN: In CNN-CC, the image is first fed into the CNN that consists of convolution, ReLU, pooling, and FCL, as depicted in Figure 3 (bottom). The CNN functions by extracting image features in the form of a feature map. These features are fed into the regression model for estimating the density map for crowd counting. CNN can be categorized into single, multi-, and single with scale-aware networks. Depending on the application, the complexity (single-multi column) and layers of CNN can be optimized to obtain the desired results. These categories are further optimized to provide strong and granular-level foundation for researchers in the future.\n\nDensity estimation: It can be defined as a way to estimate the probability density function of a random variable on the basis of observed (ground-truth) data. There are different ways to obtain the ED of a crowd, like density estimation by clustering, detection, and regression [3,7,13,68]. Detection-based techniques perform well with sparse crowds, while regression-based methods perform well on dense-crowd environment, and they overestimate crowds in sparse patches. A combination of detection and regression can be used to achieve better performance in both sparse and dense scenarios.\n\nCounting: It is a method that is performed after the computation of a density map to count the number of objects (people, cells, cars, etc.) in an image or video. Different well-known handcrafted techniques perform according to image density [10]. For example, in sparse-density images, counting by detection performs well due to a lack of overlapping objects, while CNN-based methods perform well on images with a diverse density range.\n\nUnique challenges faced by CNN-CC algorithms include a complex network architecture, increased number of parameters, high computational cost, and real-time deployment. Traditional handcrafted crowd-counting algorithms can be deployed for real-time monitoring at the cost of reduced accuracy and a low-resolution density map. These techniques also fail to obtain the desired results in high occlusion, a diverse density range, and scale-varied environments. On the other hand, CNN-CC algorithms perform better in terms of prediction accuracy and resolution. Traditional handcrafted methods have less computational cost. The majority of applications aim for high prediction accuracy. Many researchers tried and succeeded to minimize complexity. Hence, growing trends towards CNN-CC techniques motivated us to review and analyze the latest and most well-known research articles on the most challenging datasets.\n\n\nCategorization of CNN-CC Techniques\n\nThe categorization of CNN-CC techniques plays an important role in their understanding at a granular level. Such a level of understanding enables researchers to design distributed control and monitoring algorithms for various crowd-counting applications in military combat, disaster management, public gatherings, etc.\n\n\nCategorization of CNN-based Crowd\n\nCounting Techniques  The only CNN-CC categorization done by Sindagi et al. [26] conducted a very limited research survey of 17 research articles in two main categories, with 17% of articles from 2015, 64% from 2016, and only 17% from 2017. They categorized existing CNN-based techniques only on the basis of network properties and training sets. In order to cover new research articles with evolving architectures and future requirements in terms of datasets and algorithm design, we contribute by adding a new category of CNN-CC techniques, as shown in Figure 4. Inclusion of the third category based on the orientation of input image plays a significant role in the design of CNN-CC architectures and algorithms by understanding the dynamics of the input image. Moreover, we cover 52 of the latest research articles in three main categories, with only 5.76% research articles from 2015, 23.07% from 2016, 25% 2017, and 46.15% from 2018.\n\nSince datasets, in terms of their intrinsic features, play a vital role in the design of CNN-CC algorithms, we provide a brief description of the available datasets prior to categorization details of the CNN-CC techniques. Currently available datasets are of two types, public and private. Public datasets are those publicly available on the Internet, and private ones are the intellectual property of their corresponding authors/organizations. We list five of the most well-known and popular datasets, and their intrinsic features in Table 1.  \n\n\nNetwork-CNN-CC Techniques\n\nTechniques in which the network is modified in terms of layers or columns, and the inclusion of any other module for classification, segmentations, and surveillance ultimately changes the properties of the actual network are called Network-CNN-CC. Techniques under this category are very useful for obtaining high-level crowd features that may lead to significant improvement in a diverse range of densities, such as religious and political gatherings, and sports events. Although techniques in this category play a vital role in obtaining contextual information with varying scales, due to a complex architecture, these types of techniques may not be computationally suitable for real-time crowd counting. Further, Network-CNN-CC-based techniques are subcategorized into basic-CNN-CC, context-CNN-CC, scale-CNN-CC, and multitask-CNN-CC, as shown in Figure 5. Their details are as follows.\n\n\nBasic-CNN-CC Techniques\n\nCrowd-counting techniques that have a basic CNN architecture are in this subcategory. Basic-CNN-CC techniques can be regarded as the pioneer of deep-learning methods for density estimation that can be applied to obtain a crowd count in real time due to the simple network architecture. Table 2 shows Basic-CNN-CC with their features, used datasets, and architectures.\n\nFu et al. [70] proposed a bilevel density-estimation method by using a basic CNN architecture. Their first task was to estimate crowd density (i.e., to extract crowd features of different density levels). Estimation speed is increased by removing similar connections. Their second task was to classify discriminative features by using a cascaded classifier. Similarly, a residual learning method with an inception-layer-based technique was proposed in [71] to count the number of cars by dividing an image area into overlapped patches. A stride was adjusted to distinguish nonlocalized cars with contextual information in order to reduce the MSE. Wang et al. [72] proposed an FCNN model with argumentation strategy to increase the number of training data for more robustness of dense and diverse environments. Zhao et al. [73] proposed a CNN model to count the number of people crossing a line in surveillance videos. The original problem was divided into two subproblems (estimation of crowd density and crowd velocity) for reducing the complexity of the main problem. In [74], the authors proposed a deep-learning approach to estimate mid-and high-level crowds in an image. A regressor was used to estimate the number of individuals in a local area, while its overall density was estimated by adding the estimated densities of local regions. In their work, a feature vector was learned by using ConvNets architecture for estimating crowds in their respective local regions. The authors in [75] used a basic CNN for multiple applications that included indoor and outdoor counting. Layer boosting (i.e., increasing the number of trained network layers to iteratively train a new classifier that is used to fix the errors of the previous one) and selective sampling (i.e., minimizing the impact of low-quality samples) are used to reduce processing time and enhance counting accuracy. Four ensemble networks are fine-tuned by training every network on the basis of previous errors.\n\nRemarks: Most techniques under this subcategory mainly focus on density estimation instead of crowd count. These techniques may not perform well in highly occluded and varying perspective scenarios due to an oversimplified architecture. In these techniques, the speed of density estimation can be enhanced by removing redundant samples. By iteratively reducing errors in different network layers, error-rate probability can also be reduced. \n\n\nContext-CNN-CC Techniques\n\nCrowd-counting techniques that utilize both the local and global contextual information of an image for improving counting accuracy fall into this subcategory. The contextual information of an image means a relationship of nearby pixels (i.e., neighboring information) with a targeted area for overall improvement. Techniques under this category are very useful in applications that need contextual information, such as counting the number of moving drones or the number of cars in parking lots. These techniques are also helpful for obtaining density level and distribution in various density-based images. Table 3 shows context-CNN-CC with their features, used datasets, and architectures. For instance, the idea of an every-day object count was proposed by the authors in [76] by considering the novel idea of associative subitizing (humans' ability to give quick count estimates/assessments for small object counts). Zhang et al. [77] proposed an attention model to detect head location (high probability indicates head location). Similarly, multiscale feature branches were used to suppress the nonhead region. Li et al. [78] used a combination of CNN and dilated convolution (expanded kernels to replace pooling) for improving the quality of a density map. A dilated convolutional layer was also used for combining contextual information in diverse congested scenarios. Han et al. [79] proposed a CNN-Markov random field for crowd counting in still images. They divided the whole image into small overlapping patches, so that features were extracted from the overlapping patches, and fully connected NNs were used to regress the patch count. The adjacent patches had high correlation due to overlapping. Correlation was used by MRF to smooth people counts within adjacent local patches to improve the overall accuracy of the crowd count. In [80], the authors proposed a density-adaptation-based network to accurately count the number of objects. A generalized framework was proposed that was trained on one dataset and then adapted on another. Density level was computed by selecting a network that was trained on different datasets. The architecture consisted of three networks: a density-adaptation network that was used to identify low or high density, and the other two networks were responsible for counting. Liu et al. [81] proposed a deep recurrent spatially aware network in which a spatial-transformer module was used for counting while simultaneously tackling both scale and rotation variations.\n\n\nRemark 1.\n\nReal-time contextual information can be employed by using dilated convolution. More specifically, a deeper dilated CNN can be used to enhance the quality of density maps, and an adaptive density network can be used to enhance counting accuracy. However, such contextual information is obtained at the cost of higher network complexity. As a result, techniques in this subcategory may not be feasible for real-time applications with low complexity demands/requirements.\n\n\nScale-CNN-CC Techniques\n\nBasic-CNN-CC techniques that have evolved in terms of scale variations (for robustness and accuracy improvements) are called Scale-CNN-CC techniques. Scale variation means varying the resolution caused by different perspectives. The techniques in this category play a vital role in enhancing accuracy in highly congested and occluded scenarios. The extraction of multiscale patches from an input image makes the goal comparatively easier for crowd counting. This may increase accuracy in a dense and diverse range of datasets such as UCF and STA. However, these techniques rely on the extraction of multiscale patches with a complex architecture. Table 4 depicts the limitations and merits of different scale-CNN-CC methods. The negative sampling and data-driven approach is missing in all the listed methods. Liu et al. [82] proposed a geometric-aware crowd-density-estimation technique. An explicit model was proposed to deal with perspective distortion effects. Huang et al. [83] reduced the computational cost by investigating the idea of stacked pooling. Instead of using multiscale kernel pooling, stacked pooling is used to extract scale information for making it applicable in real-time applications. Later on, Kang et al. [84] used an image pyramid to deal with scale-varying issues in an image. Instead of changing the filter size, feeding downsampled images into the network was efficient for crowd-counting accuracy. Then, predictions from different scales were fused to obtain the final ED. The authors in [85] proposed a combination of a shallow and a deep network to effectively capture high-level semantics (face, body) and low-level features to accurately estimate crowd density in scale-varying conditions. The authors in [86] proposed a single column multiscale cost effective method for real-time applications. By using a single column with a multiscale blob, scale-related features were extracted. These scale-related features generated by the network were used for dense crowd counting. In [87], the authors proposed a combination of gating (multiclass classifier) and a multiple-expert CNN. The gating CNN automatically directs the input patch to the expert CNN that makes the algorithm robust for large appearance changes. Onro-Rubio et al. [88] proposed two methods to address crowd appearance and scale variation in an image. First, a counting CNN was proposed to map image appearance into its density map. Second, a congested and varying scale region is tackled through the Hydra CNN without any geometric information. The Hydra CNN used the pyramid of patches extracted from multiple scales for density estimation. The authors in [89] proposed a multiscale multitask crowd-counting algorithm with an aggregated feature vector. Multiscale features were basically combined into a single vector, a 'vector of locally aggregated descriptor', which was optimized by backprorogation. Moreover, a data-argumentation approach was used to increase the size of the training data. Cao et al. [90] proposed an encoder-decoder-based CNN to reduce computational complexity. By avoiding a multicolumn CNN with a classifier, a simpler scale-aware network (SANet) was used to address scale-varying issues. Further, transpose convolution was used to enhance the quality of the density map. Motivated by the success of GANs in an image for image-translation problems, the authors in [91] employed GANs for crowd counting. The GANs were used for the translation of the image and its patches into generated maps. The actual GTD was compared with the generated map to find the best-resolution density map (high-quality). A novel regularizer adversarial cross-scale consistency pursuit network (ACSCP) was proposed to maintain the parent (whole image) and child (four patches) relationship for reducing counting loss (previously caused by averaging). By using adversarial loss, the distance between the parent density map and the concatenated-image density map was calculated for minimizing loss. This regularizer performed well as compared to l 2 regression loss.\n\n\nRemark 2.\n\nA greater pooling range (multikernel pooling and stacked pooling) is beneficial to capture a multiscale range to reduce computational cost. Rather than using the multicolumn approach (computationally complex), the concatenated-scale aggregation modules may increase counting accuracy. Moreover, the quality of the density map can be enhanced by using transposed convolutional layers at the cost of high complexity.\n\n\nMultitask-CNN-CC Techniques\n\nCNN-CC techniques that not only account for crowd counting but also for other tasks like classification, segmentation, uncertainty estimation, and crowd-behavior analysis are called multitask-CNN-CC techniques. We review the inter-relationship between these multiple tasks and their impact on the performance of individual tasks under the multitask-CNN-CC umbrella. Table 5 shows the detailed description of features, used datasets, and architecture of different algorithms under multitask-CNN-CC.\n\nThe authors in [92] proposed a ConvNet architecture to count the number of penguins. Due to occlusion and a scale-varying environment, a multitask learning technique was proposed to overcome foreground-background segmentation and uncertainty in density estimation. Idrees et al. [93] investigated the multitask technique by inter-relating three main problems: crowd counting, density estimation, and localization. In their work, the counting task was facilitated by density estimation and localization. Zhu et al. [94] proposed a deep and shallow FCN. Features extracted from a deep and shallow FCN were concatenated with the addition of two deconvolutional layers to make the output image similar to input image in terms of resolution. Instead of relying on modeling the visual properties, Huang et al. [95] proposed a semantic scene (body-structure-aware) CNN-based crowd-counting method. In their work, the crowd-counting problem was decomposed into a multitask problem. These multitasks involved the extraction of rich semantic-feature information, mapping the input scene image to the semantic scene model (body-part map and structured density map), and crowd counting. Yang et al. [96] proposed a multicolumn multitask neural network (MMCNN) to overcome drastic scale variation in an image. They used the multicolumn by incorporating three main changes. First, up-and downsampling was utilized to extract multiscale features. Second, deconvolution was used to account for loss due to downsampling. Third, loss per scale was minimized to make features more discriminative. Liu et al. [97] proposed a self-supervised method to increase the training data for enhancing accuracy. The ranked patches (cropped from original image) were used as side information. Moreover, multiscale sampling was utilized to further enhance accuracy. \n\n\nRemark 3.\n\nFirst, training data can be increased by cropping an image into smaller patches (containing less than or an equal number of objects as compared to the larger patch). Second, the inter-relationship between different tasks may enhance counting accuracy. Third, deconvolution can be employed to enhance the quality of the density map. Finally, multiple tasks assist each other to increase the overall accuracy of the network. However, multitasking increases network complexity, which reduces its employability for real-time applications.\n\n\nImage-View-CNN-CC Techniques\n\nThe main focus of this category is to analyze an input image (arial or perspective) and accordingly design the network so that network accuracy can be improved. These techniques are very useful in medical imaging, monitoring of targeted areas through drones, and counting people through CCTV. Since camera angle, tilt, and position with respect to the object play a critical role in the development of any algorithm, we mainly divided image-view-CNN-CC into two subcategories: aerial-view-CNN-CC and Perspective-CNN-CC.\n\n\naerial-View-CNN-CC Techniques\n\nThe set of techniques that mainly design the network according to input image (aerial-view-based) fall in this category. Techniques under this subcategory have applications in healthcare, commerce, the military, etc. Detailed limitations and characteristics of each of the algorithms under the umbrella of aerial-view-CNN-CC are given in Table 6.\n\nIn [98], the authors proposed a method to count the number of cells in a growing human embryo. They computed a bounding box by selecting a particular region (enclosing the embryo). Then, an end-to-end deep CNN was presented to count the number of cells in a microscopic image. Ribera et al. [99] proposed a regression model to estimate plants in an image (taken through a UAV). They minimized the number of neurons in the final layer to reduce the computational complexity of the network. However, issues like lack of large amounts of training data and occlusion were not addressed. The authors in [100] proposed a feature pyramid network with a VGG-style neural network for the segmentation and counting of microscopic cells. They utilized the downsampling of an image several times and learned features at varying scales to enhance segmentation and counting accuracy. However, downsampling affects the resolution of the ED map. Another approach was proposed by Xie et al. [101] to estimate the number of cells in a microscopic image. In this technique, two convolutional regression networks with a large receptive field (filters) were used to overcome cell clumps and the cell-overlapping problem. \n\n\nRemark 4.\n\nBy knowing the characteristics of the input image, techniques with less complexity and error rate can be designed. Individual regression models can be sequentially trained for low and high density to handle object clumps and sparsity. Occlusion can also be handled by feeding the downsampled patches into the CNN.\n\n\nPerspective-CNN-CC Techniques\n\nTechniques that mainly design the network according to the input image (perspective-view-based) fall in this category. These techniques are useful in varying-perspective scenarios with diverse scale variations. These techniques are applicable in dense-crowd-counting (e.g., sports events) scenarios having different perspectives, such as a shopping mall. By knowing the properties of the input image, techniques can be designed that have less complexity and high accuracy. The detailed features, used datasets, and architecture of each algorithm are shown in Table 7. The authors in [102] proposed an adaptive CNN to incorporate perspective information. The convolutional-filter weights were adapted according to the current image scene by using perspective information. Zhao et al. [103] proposed a perspective-embedded deconvolution network (PE-CFCN-DCN) to model the varying size of pedestrians considering perspective distortion. They used a location-aware Gaussian function with varying kernel parameters for each annotated point (dot) to obtain the GTD. They also added a perspective map (one channel) as an additional channel to the RGB image (three channels) by modifying the filter depth from three to four channels. Perspective information was embedded with the deconvolution network (upsampling process) by utilizing structural information of different levels that help in the formation of a smooth and accurate density map (high-quality). Marsden et al. [104] proposed a multidomain patch-based (overlapped) regressor for object counting with the removal of redundant parameters in the model to reduce its complexity. A pretrained classification network was used to extract high-level features. The extracted features were mapped to the object count by using an FCNN. Further, switching among learned visual domains (people, wildlife, cells, and vehicles) could be accomplished with a subset of parameter interdomain sharing. This interdomain switching is very helpful in tackling different perspectives, scales, and density variations. Zhang et al. [105] proposed a switchable training method with multiobjective tasking. Two subtasks (estimating density and crowd count) influenced each other due to the introduction of a data-driven approach by choosing training scenes from all training data that have almost identical perspective maps with the target scene (test data). Shi et al. [106] proposed a perspective-aware CNN model where the perspective map was predicted and used as a perspective-aware weighting layer. This additional layer was responsible for combining thedensity maps obtained from varying-scale feature maps. The density and perspective maps were combined to provide the estimated count. The varying perspective and resolution problem was solved by Yao et al. [107] by proposing a Deep Spatial Regression Model (DSRM) using the CNN and LSTM. First, high-level features were extracted by using a CNN. Due to the high correlation among the overlapped patches, the LSTM structure used spatial information in adjacent regions to enhance counting accuracy. The final count was obtained by adding all the local patch counts.\n\nRemarks: Perspective distortion may be reduced by inserting a perspective-aware weighting layer (separate layer) in the deconvolution network. Parameters among the different domains (trained on separate datasets) can be shared to overcome varying-perspective problems such as object-size and resolution variation.\n\n\nT-CNN-NN Techniques\n\nThe set of techniques in this category are differentiated according to the approaches used to train the CNN, for example, training the CNN on the basis of a whole image or cropped patches. Such approaches can be used to improve the prediction accuracy of the network or the quality of its density map. Whole-image-based training minimizes the network computational cost at the cost of reduced accuracy, while patch-based training enhances network accuracy for high computational cost. These techniques are useful in medical imaging, commercial, and military applications. These techniques are categorized into patch-based-CNN-CC and whole-image-CNN-CC. Details are as follows.\n\n\nPatch-Based-CNN-CC Techniques\n\nIn these techniques, the CNN is trained by using cropped patches where a sliding window is run over the test image. These techniques are very useful in applications where there is enhanced resolution quality of the density map and it cannot be compromised, such as in cancer diagnosis. Both the affected cell count and the resolution of affected cells are important. The main objective of this category is to design a system for enhanced density-map quality at high computational cost. The detailed characteristics of each algorithm under patch-based-CNN-CC are shown in Table 8.\n\nCohen et al. [108] proposed a deep CNN inspired by inception networks. Instead of estimating the crowd count on the whole image, a smaller network is used to estimate the number of objects in a given receptive field. Overestimation of the crowd count in sparse areas by regression-based technique and underestimation of the crowd count in dense areas by detection-based techniques motivated the authors in [109] to proposed a detection and density-estimation (DecideNet) method that employed a counting mode based on density conditions. Inspired by the skip-connection method for crowd counting, the authors in [110] proposed an optimized method for information flow within different convolution and deconvolution layers. Convolution layers were used to detect the edges and colors, but this low-level information obtained from earlier layers may or may not have contributed to enhancing the performance of the network in terms of MAE. Therefore, a Gated U-Net (GU-Net) was employed to determine the amount of information passed to the final layer (convolution or fully connected) for a more accurate feature-selection process. Similar to the idea of [109], Xu et al. [111] proposed a depth-of-information-based guided crowd-counting method (Digcrowd) to deal with highly dense and varying-perspective images. Segmentation was performed on an image to divide it into two regions: farand near-view regions. In the near-view region, people are counted by detection, and Digcrowd maps are used in the far-view region to map counted people to their density map. The authors in [112] used a head detector to find the varying size of a human head. After dividing images into multiple patches, an SVM classifier was used to classify crowded and noncrowded patches. In order to find the head size, regression was performed on each patch. After finding the head size, the total number of heads in a particular patch was calculated by dividing patch area with head size. The authors in [113] proposed a count-net technique by focusing on the head portion by filtering the background. Feature extraction and classification were also simultaneously performed with crowd counting. Zhang et al. [114] proposed a patch-based multicolumn CNN (MCNN) crowd-counting technique with a geometry-adaptive kernel for density estimation. The varying size of the receptive fields used in each CNN column was used to handle scale-varying objects (heads). However, the aggregation of the density map at the end may have decreased the quality of the ED map. Wang et al. [115] proposed a skip-connection CNN (SCNN) for crowd counting. The overall network used four multiscale units for extracting scale-varying features. Each multiscale unit consisted of three convolutional layers. Several multiscale units were used to extract the scale-varying features. Moreover, an augmentation strategy (without redundancy) was adopted by cropping the two patches (having different scales) from each input image. The CNN was individually trained on these two scales to overcome any drastic scale variations.\n\nSam et al. [116] proposed a switch CNN technique by considering three regressors trained on low-, medium-, and high-density image patches. A switch (classifier) was used to direct the input patch to a particular regressor for addressing any density-variation issues. \n\n\nRemark 5.\n\nDetection and regression can be sequentially employed on targeted image patches to enhance network prediction accuracy. Further, extracted low-level information about network edges and colors can be iteratively filtered to reduce the computational cost of the network.\n\n\nWhole-Image-CNN-CC Techniques\n\nTechniques in this subcategory perform whole-image-based inference, and are very useful in real-time applications due to the reduced computational cost. These techniques have applications in pedestrian counting, counting passing cars across CCTV, etc. The absence of negative sampling and lack of a data-driven approach are common in all the listed algorithms (see Table 9). Detailed characteristics of each algorithm under patch-based-CNN-CC are shown in Table 9. A CNN-based fruit-counting technique was proposed by Rahnemoonfar et al. [117] by using a deep-simulated-learning algorithm. The network was trained on synthetic data (24,000 images consisting of variably sized tomatoes) with a whole-image-based training approach. A modified version of the Inception-ResNet architecture was used to implement the idea of fruit (tomatoes) counting. Sheng et al. [118] focused on the discriminative power of image representation by combining semantic information and locality-aware features (spatial and context information). By using the CNN, they mapped the pixel space into a semantic-feature map. Pixelwise features indicated a particular semantic class (e.g., road, person, pole, car, building). Furthermore, locality-aware features were used to exploit the local and contextual information. Later, the authors in [119] proposed a multiobjective technique by using residual deep-learning architecture (ResnetCrowd) to investigate crowd counting, violent-behavior detection, and density-level classification. The authors in [120] proposed a FCN for crowd counting by addressing the problems of scale variation and high density within an image. Instead of changing the receptive field (filter size) in a CNN, a scale-down version fed the network. To obtain the final count, they computed the mean of the downsampled images. Sindagi et al. [121] proposed a multitask cascaded CNN network to accurately learn crowd density and crowd classification. They exploited discriminative features (high-level prior) to handle high-level density variation within an image.\n\n\nRemark 6.\n\nCounting accuracy could be enhanced by feeding the network with semantic and locality-aware features. High-level prior (i.e., density-level classification) with density estimation also take part in performance improvement.\n\n\nApplications of CNN-CC Algorithms\n\nCNN-CC techniques have a diverse range of applications, as shown in Figure 6. These applications include intelligent crowd analysis, military applications, public-event management, disaster management, and health-care applications [23]. Detailed descriptions are given as follows.\n\nIntelligent Crowd Analysis: Crowd-counting techniques can be employed to gather information for intelligent analysis and inference. For example, the queue length in front of a billing reception center (electric, gas, and water bills), especially in developing countries, could be observed and analyzed to accordingly optimize the number of staff members. Traffic-signal wait time could be optimized with respect to crowd flow, especially during office hours. Moreover, appropriate product placement can be done in big malls and stores according to the interest of people [9,122,123].\n\nMilitary Applications: CNN-CC techniques can be used in military applications such as counting the number of moving drones or fighter jets or the number of enemy soldiers and their weapons. Thus, the strength of the enemy's armed forces could be estimated to counter a surge [52,124,125].\n\nPublic-Event Management: CNN-CC techniques can be used in concerts, sports events, and political rallies to count the number of people. Thus, these events can be managed by analyzing and counting the crowd to avoid disastrous situations. This would also be beneficial in properly managing available resources, such as spatial capacity and optimizing crowd movements [126][127][128].\n\nDisaster Management: There are different overcrowding scenarios, like musical concerts and religious gatherings, which could be life-threatening when a portion of the crowd panics and charges in random directions. In the recent past, huge numbers of people have died from suffocation in highly crowded areas in different public-gathering events. Early detection of overcrowding and better crowd management in political rallies, sports events, and musical concerts can be made possible by analyzing the crowd gathering [129][130][131].\n\nSuspicious-Activity Detection: Terror attacks in public places can be minimized by using crowd analysis and violent-crowd-behavior detection techniques. Traditional handcrafted methods do not perform well in harsh and densely crowded events, and could be replaced by CNN-based face recognition and detection techniques for better crowd analysis [132][133][134][135].\n\nHealth-Care Applications: CNN-CC techniques play an important role in health-care systems, especially with patients suffering from cancer and other diseases where it is important to count a number of cancerous cells for early-stage diagnosis. The authors in [136] proposed a deep model for cell detection in zebrafish images. This framework was used to detect tyrosine hydroxylase cells in zebrafish brain images. Further, the authors in [137] presented a CNN-based model for histopathologic cancer diagnosis through a deep-learning architecture to increase the objectivity and efficiency of histopathology-slide analysis. The authors in [138] also diagnosed skin cancer by using skin images with a deep NN. Finally, the authors in [139] trained a deep NN to predict different liver diseases.\n\nSafety Monitoring: A huge number of CCTV monitoring systems at airports, religious gathering places, and public locations enable easier crowd monitoring. CNN-CC algorithms could be further analyzed to detect behaviors and congestion-time slots to ensure the safety and security of the public [140]. For example, the authors in [141] presented a multicamera approach to detect dangers by analyzing crowd density. In other works ,such as [142,143], the authors proposed a surveillance system to generate a graphical report by analyzing crowds and their flow in various directions through CCTV cameras. \n\n\nThree-Dimensional Crowd Counting\n\nThe widespread usage of CCTV monitoring systems at airports, religious gathering places, and public places enable easier monitoring of crowd. However, traditional crowd-counting methods with classification [144,145] and segmentation [146] via deep-learning techniques rely on 2D datasets instead of video crowd counting. The task of crowd counting from videos is challenging due to severe occlusions, scene-perspective distortions, diverse crowd distributions, and especially complex network architectures. Limitations in terms of complex networks (high computational cost) restrict researchers from deploying real-time crowd-counting algorithms. For that, we need to simplify deep-learning models so that they can be easily deployed. Rapidly growing crowd-counting technologies demand investigations to reduce NN computational cost and network complexity. More specifically, the reduction of complex models to simpler ones [147,148] encourages the wide adoption of such models in remote stations for real-time applications, such as crowd analysis in autonomous vehicles.\n\nDimensionality reduction is used to reduce the complexity of machine-learning networks and reduce overfitting. The authors in [149] proposed a principal-component-analysis (PCA)-based nonparametric,unsupervised technique for dimensionality reduction. The authors in [150] investigated PCA applications , kernel PCA (KPCA), and independent component analysis (ICA) with an SVM for feature extraction. PCA was used to linearly transform the original inputs into uncorrelated new features, whereas the linearly transformed features in ICA are statistically independent. KPCA is nonlinear PCA that is done by generalizing the kernel method into linear PCA. Similarly, the authors in [151] proposed an unsupervised method for dimensionality reduction called Locally Linear Embedding (LLE). By maintaining the geometric features of a nonlinear feature structure, it reduces the n-dimension feature space. LLE optimization does not involve local minima by mapping inputs into a single coordinate having lower dimensions. By observing the performance of model simplifications in machine-learning approaches, different authors also proposed simplified models for deep learning [152][153][154][155].\n\n\nPerformance Evaluation of CNN-CC Algorithms\n\nIn this section, our main goal was to evaluate the selected existing CNN-CC algorithms. For evaluation purposes, we considered a common performance metric: MAE, where N is the number of test samples, y i is used for ground-truth count, and y i is the estimated count of i th sample.\nMAE = 1 N N \u2211 i=1 |y i \u2212 y i |(1)\nFor comparison, we chose the following benchmark techniques:\n\n\u2022 Refs. [73][74][75]  Refs. [109][110][111][112][113][114][115][116] as Patch-based-CNN-CC algorithms tested via the UCF, STA and STB datasets.\n\n\n\u2022\n\nRefs. [119][120][121] as whole-image-CNN-CC algorithms tested via the UCF, STA and STB datasets. Figure 7a shows that the normalized MAE (nMAE) of [74] was relatively higher than that of [75] when tested on the USCD dataset. This is because of an underestimation of layer boosting that iteratively increased the number of network layers due to selective sampling. Further, the nMAE of [73] was relatively less than that of [75] when tested on the USCD dataset. This is because the two subproblems (crowd-velocity and -density estimation) in [73] assisted each other to enhance performance. Similarly, the nMAE of [75] was relatively less than that of [74] when tested on the UCF dataset due to previously mentioned reasons. Hence, we concluded that, instead of the direct insertion of new layers in the CNN, iteratively increasing the number of layers in a trained network may improve system performance in terms of nMAE. System performance may further be improved if a multitasking approach is employed.  Figure 7b shows that the nMAE of [81] was relatively lower than that of [77][78][79][80] when tested on the UCF dataset. The reason was the consideration of a spatial transformer network (to tackle scale and rotation), and a local refinement network (to account for contextual information) in [81]. Further, the nMAE of [78] was relatively lower than that of [77,80,81] when tested on the STA and STB datasets. This is due to the consideration of dilated convolution by expanding the kernel that is useful in extracting contextual information. By comparing the nMAE, the performance of [78] was the relative lowest from all compared algorithms when tested on the STB dataset due to the tilted behavior of STB towards low density. Hence, after observing the performance of context-CNN-CC, we concluded that counting accuracy could be enhanced on datasets with diverse scenes and varying densities by solving pose variations and photographic angles for accurate density estimation. Performance could also be increased on low-density datasets by adopting dilated convolution. Figure 7c depicts that the nMAE of [91] was relatively low when compared to that of [85][86][87][88][89][90] on the UCF dataset. This is due to the introduction of a novel ASCP framework (inspired from GANs). Adversarial loss instead of l 2 regression loss also enhanced accuracy. Further, the nMAE of [90] was relatively low when compared to that of [83,84,89,91] on the STA and STB datasets. This is because the combination of a scale-aware network with transpose convolution enhanced the counting accuracy and quality of the density map. Further, the nMAE of [91] was relatively low when compared to that of [84] on the STA dataset due to the above-mentioned reasons. However, [91] had a relatively high nMAE as compared to that of [84] on the STB dataset. This is due to the consideration of scale-aggregation modules with a combination of Euclidian and local-pattern consistency loss by [84]. Hence, we concluded that there were two main scale-variation issues that need solutions: (1) A scale-specific network performs poorly on unknown scales, which results in low-quality density maps. (2) The coherence issue among different density maps is not properly addressed (summing the individual local counts may not be necessary to approximate the total count). Figure 7d depicts that the nMAE of [97] was relatively low when compared to that of [94][95][96] on the UCF dataset. The reason was the consideration of a self-supervised learning technique (increased training-data size). Further, the nMAE of [97] was relatively low when compared to that of [95,96] on the STA and STB datasets due to the above-mentioned reasons. Further, [96] had a relatively low nMAE when compared to that of [94,95] on the UCF and STB datasets. This was due to handling scale variation by using multikernels (parallel) with a multitask approach. Hence, we concluded that counting accuracy could be enhanced by focusing on calculating the accurate GTD, and increasing the number of training data improves ED quality. Drastic scale variation could also be handled by using the combination of semantic information (body-part information) with up-and downsampling. Figure 8a shows that the nMAE of [107] was relatively low when compared to that of [105] and [106]. This is because Yao et al. [107] used a combination of CNN (extracting high-level information) and LSTM (using spatial information to regress the local count from adjacent patches) to increase network prediction accuracy. Further, the nMAE of [102] was relatively low when compared to that of [104,106] on the STA dataset. This is due to the incorporation of side (contextual) information having perspective weights in the CNN. However, the nMAE of [106] was relatively low when compared to that of [102] with a low margin on the STB dataset. The accuracy enhancement was due to separate perspective-aware layers considered by [106]. Hence, we concluded that, by combining the fine-tuning part (retrieving training scenes from all training datasets that had a similar perspective map with the target scene) with a deconvolution network increases accuracy and enhances ED map quality.  Figure 8b depicts that the nMAE of [116] was relatively low when compared to that of [112][113][114][115] on the UCF dataset. This is due to the consideration of density-level classification of image patches with a density-oriented-based regressor approach. Further, the nMAE of [115] was relatively low when compared to that of [110][111][112]114,116] on the STA dataset. This is due to consideration of a skip connection with scale-oriented training to handle varying-scale issues. The nMAE of [110], on the other hand, was relatively low when compared to that of [109,112,[114][115][116] on the STB dataset. This was due to the consideration of controlled flow of information through the convolution and deconvolution layers in [110]. We therefore conclude that for datasets with a dense and diverse range of densities, a specific-task-oriented regressor and deconvolution increase accuracy for estimating a high-quality density map. However, low-density datasets can be tackled by using a patch-based augmentation (varying-scale) strategy, and optimized information flow within the convolution and deconvolution layers by addressing the scale-varying issue caused by the perspective view. Figure 8c shows that the nMAE of [121] was relatively low when compared to that [119,120] on the UCF dataset. The reason of this error reduction was the consideration of the high-level prior with density estimation. Further, the method in [120] has a low nMAE when compared to that of [119] on the UCF dataset. This is due to addressing the problem of dense crowds by feeding images at multiple scales, as in Marsden et al. [120]. Further, the nMAE of [121] was relatively low when compared to that of [120] on the STA and STB datasets due to the above-mentioned reasons. Hence, we conclude that high-crowd-density issues can be solved up to an extent by varying the image scale. Multitasking makes the system more complex for real-time applications.\n\nBy comparing the performance of subcategories of Network-CNN-CC, we concluded that density-estimation accuracy is increased by using adversarial loss instead of regression loss. The quality of the density map is also enhanced by using GANs, as per Shen et al. [91]. The work in [91] had the lowest nMAE as compared to the rest of the algorithms under network-CNN-CC when tested on the most challenging UCF dataset. The enhanced performance was proved by [90] on the STA and STB datasets under network-CNN-CC. This is due to consideration of training loss with a scale-aware network by using transpose convolution. Similarly, by observing the performance of subcategories of image-view-CNN-CC, we concluded that [102,106,107] performed well on the STA, STB, and UCF datasets. This is due to the utilization of a CNN with LSTM for spatial information to regress the local object count in adjacent regions (patches) in [107]. The enhanced performance of [84,106] was due to consideration of the perspective information and perspective-aware weighting layer. By investigating the training-CNN-CC, we concluded that [110,115,116] performed well on the STB, STA, and UCF datasets, respectively. This enhanced performance was due to using a density-level classifier with a density-oriented regressor in [116]. The reason for the high performance was the usage of a skip connection with scale-oriented training in [115]. The algorithm in [110] performed well due to optimized information movement within the convolution and deconvolution layers. Finally, we concluded that the algorithm of Shen et al. [91] performed well on the UCF dataset, while that of Cao et al. [90] showed better performance on the STA and STB datasets.\n\n\nConclusions and Key Observations\n\nIntelligent crowd counting and its analysis are a future development of traditional handcrafted methods. By leveraging the tight integration of machine-learning and artificial-intelligence technologies with traditional crowd-counting techniques, intelligent crowd counting and its analysis provide advanced features such as adaptive control for dynamic crowd gatherings, and their wide-area monitoring/surveillance. These advanced features can improve many crowd-management-related tasks in terms of efficiency, capacity, reliability, and safety. CNN-CC techniques can effectively support many applications that require adaptive monitoring, identification, and management over diverse crowd-gathering horizons. In this article, we presented a comprehensive review of CNN-CC and density-estimation techniques. We mainly categorized CNN-CC techniques into network-, image-view-, and training-CNN-CC. Moreover, we subcategorized the three main categories and accordingly summarized recent research articles. In each subcategory, we discussed the latest research articles in terms of their key features, used datasets, and architectures. We also critically reviewed the research articles in terms of key characteristics and deficiencies. Finally, we provided quantitative comparison results of the sub-and main categories to facilitate future researchers. On the basis of our comprehensive review, we conclude the following key observations.\n\n\n\u2022\n\nCounting accuracy of basic-CNN-CC is enhanced by removing redundant samples, while multitasking improves the overall accuracy of an algorithm.\n\n\n\u2022\n\nThe quality of a density map in context-CNN-CC is enhanced by using a deeper dilated CNN, while counting accuracy is enhanced by using an adaptive-density network through pose-variation-based solutions.\n\n\n\u2022\n\nBy investigating scale-CNN-CC, we observed that counting accuracy is improved by using stacked pooling that reduces computational cost. Moreover, concatenated-scale aggregation modules increase accuracy, and the quality of the density map is enhanced.\n\n\n\u2022\n\nCounting the accuracy of multitask-CNN-CC is increased by using self-supervised learning, inter-relations between multiple tasks, and up-and downsampling. However, multitasking makes the system more complex for real-time applications. Density-map quality is also enhanced by using deconvolution layers. \u2022 Performance i of aerial-view-CNN-CC n terms of nMAE is increased by using multiple regression models, and occlusion is handled by feeding the downsampled patches in the CNN.\n\n\n\u2022\n\nCounting accuracy of the PRCC is enhanced by inserting a perspective-aware layer in the deconvolution network, parameter sharing within different domains, and retrieving training scenes from all training datasets that have similar perspective maps with target scenes.\n\n\n\u2022\n\nThe nMAE of patch-based-CNN-CC is increased by using detection and regression depending on image density and the optimal transfer of information within CNN layers. For dense datasets, the combination of density-level classification, a specific task-oriented regressor, and deconvolution increase accuracy with the estimation of high-quality density maps. Density datasets are tackled by using a patch-based augmentation (varying scale) strategy.\n\n\n\u2022\n\nThe counting accuracy of whole-image-CNN-CC is improved by exploiting semantic and locality-aware features, and density-level classification. Diverse-crowd-density issues are also fixed to some extent by varying image scales, making these techniques highly applicable in real-time applications.\n\nFor future work, we will integrate Restricted Boltzmann Machines (RBMs) into a CNN-based crowd-counting network. Further, we will enhance the accuracy and quality of estimated density maps by using varying receptive fields. Besides accuracy, we are interested in reducing the computational cost (number of parameters) of CNN-based crowd-counting networks.\n\nAuthor Contributions: For N.I., K.K. and A.S. collected and prepare the data. N.I. contributed to the review, categorization, analysis, and performance Evaluation. K.K. supervised the process of defining the structure of this review. The manuscript was written by N.I. and K.K. with contribution from A.S. as well. All authors have read and agreed to the published version of the manuscript. \n\n\nConflicts of Interest:\n\nThe authors declare no conflict of interest. \n\n\nAcronyms\n\nFigure 1 .\n1Categorization of crowd-counting techniques.\n\nFigure 2 .\n2Unique challenges of convolutional-neural-network (CNN) crowd counting (CC) techniques in an image.\n\nFigure 4 .\n4Categorization of CNN-CC techniques.\n\nFigure 5 .\n5Architectures of different subcategories: (a) basic-CNN-CC, (b) context-aware CNN-CC techniques (context-CNN-CC), (c) patch-based-CNN-CC, (d) scale-aware CNN-CC techniques (scale-CNN-CC), (e) multitask-CNN-CC, (f) whole-image-CNN-CC, (g) aerial-view-CNN-CC, and (h) perspective-CNN-CC.\n\nFigure 6 .\n6Applications of crowd analysis in different fields\n\nFigure 7 .\n7Normalized Mean Absolute Error (nMAE) of network-CNN-CC algorithms tested on different datasets: (a) basic-CNN-CC, (b) context-CNN-CC, (c) scale-CNN-CC, and (d) multitask-CNN-CC\n\nFigure 8 .\n8nMAE of CNN-CC algorithms tested on different datasets: (a) perspective-CNN-CC, (b) patch-based-CNN-CC, and (c) whole-image-CNN-CC.\n\nFunding:\nThis research was a part of project titled Development of Ocean Acoustic Echo Sounders and Hydro-Physical Properties Monitoring System, funded by Ministry of Oceans and Fisheries, Korea.\n\nTable 1 .\n1Summary of different crowd-counting datasets with their intrinsic features.Dataets \nUSCD [17] \nMall [4] \nUCF [69] \nWE [28] \nSTA [26] \nSTB [26] \n\nNo. of \nimages (NOI) \n2000 \n2000 \n50 \n3980 \n482 \n716 \n\nResolution \n158 \u00d7 238 \n320 \u00d7 240 \nVaried \n576 \u00d7 720 \nVaried \n768 \u00d7 1024 \n\nMinimum \nhead count \n11 \n13 \n94 \n1 \n33 \n9 \n\nAverage \nhead count \n25 \n-\n1279 \n50 \n501 \n123 \n\nMaximum \nhead count \n46 \n53 \n4543 \n253 \n3139 \n578 \n\nTotal head \ncount (THC) \n49,885 \n62,325 \n63,974 \n199,923 \n241,677 \n88,488 \n\nQualitative \nfeatures \n\nCollected from \nvideo camera, \nground-truth \nannotation, \nlow-density \ndataset, \nno perspective \nvariation \n\nCollected from \nsurveillance camera, \ndiverse illumination \ncondition; \ncompared to USCD, \nit has higher density, \nno scene-perspective \nvariations \n\nCollected from \nvarious places like \nconcerts, marathons, \ndiverse scenes with \nwide range of \ndensities, \nchallenging \ndatasets as compared \nto USCD and Mall \n\nSpecific for \ncross-scene \ncrowd-counting \nlarge diversity, \nbut limited as \ncompared to UCF, \nnot dense as \ncompared to UCF, \nmore images \n\nChosen from \nInternet, \nlarge scale, \nlargest in terms \nof number of \nannotated people, \nlarge density as \ncompared to (B), \ndiverse scenes, \nand varying \ndensities \n\nCollected from \nShanghai, \nvarying scale and \nperspective, \nnonuniform \ndensity level \nin many images, \nmaking it tilt \ntowards the \nlow-density level \n\n\nTable 2 .\n2Summary of advantages and limitations of basic-CNN-CC algorithms.Technique \nFeatures \nDatasets \n\nNegative \nSamples \n\nData \nDriven Architecture \nYes No Yes No \n\nFu et al. [70] \nReal-time approach \nPETS_2009, Subway video, \nChunix_Road video \nConvNets \n\nMundhenk et al. [71] \nContextual information, \ncreation of large datasets of cars \n\nCars Overhead with Context \n(COWC), \n\nAlexNet, \nInception \n\nWang et al. [72] \nEnd-to-end deep CNN \nregression model \nUCF \nFCN \n\nZhao et al. [73] \nJoint learning of crowd \ndensity and velocity \nUSCD, [LHI, TS,CNN] * \nFlowNet \n\nHu et al. [74] \nTwo supervisory signals: \ncrowd count and crowd density \nUCF, USCD \nConvNets \n\nWalach et al. [75] \n\nGradient boosting and selective \nsampling, and elimination of low-quality \ntraining samples \n\nUCF, USCD, [Bacterial Cell, Make 3D] * \nBoosting Net \n\n* Private datasets. \n\n\n\nTable 3 .\n3Summary of advantages and limitations of Context-CNN-CC algorithmsTechnique \nFeatures \nDatasets \n\nNegative \nSamples \n\nData \nDriven \nArchitecture \nYes No Yes No \n\nChattopadhyay et al. [76] Associative subitizing \nPASCAL VOC, COCO \nConvNet \n\nZhang et al. [77] \nAttention model for head detection UCF, STA, STB \nAM-CNN \n\nLi et al. [78] \n\nDilated convolution and \nmultiscale contextual \ninformation \n\nUCF, STA, STB, WE \nCSRNet \n\nHan et al. [79] \nCombination of correlation and MRF UCF \nResNet \n\nWang et al. [80] \nDensity adaption network \nST, UCF \nDAN, LCN , HCN \n\nLiu et al. [81] \nSpatially aware network \nST, UCF, WE \nLocal Refinement \nNetwork \n\n\n\nTable 4 .\n4Summary of advantages and limitations of scale-CNN-CC algorithms.Technique \nFeatures \nDatasets \n\nNegative \nSamples \n\nData \nDriven Architecture \nYes No Yes No \n\nLiu et al. [82] \nGeometry-aware crowd counting \nST, WE, Venice \nSiamese \n\nHuang et al. [83] \nExploits cross-scale similarity \nST, WE \nWide and Deep \n\nKang et al. [84] \nImage pyramid to deal with scale variation \nST, WE, USCD \nVGG network \n\nBoominathan et al. [85] Combination of deep and shallow networks \nUCF \nVGG-16 \n\nZeng et al. [86] \nSingle multiscale column \nST, UCF \nInception \n\nKumagai et al. [87] \nIntegration of multiple \nCNNs (gating and expert CNN) \nUCF, Mall \nMoC-CNN \n\nOnoro-Rubio et al. [88] \n\nCCNN for mapping the appearance of \nimage patch to its density map; \nHydra CNN is scale-aware model \n\nUCF, USCD, \nTRANCOS \nCCNN, Hydra \n\nShi et al. [89] \nDynamic data-augmentation strategy, NetVLAD ST, UCF, WE \nVGG-like net \n\nCao et al. [90] \nMulti-scale feature extraction with \nscale aggregation modules \nUCF, STA, STB, USCD \nSANet \n\nShen et al. [91] \nGANs-based network, novel regularizer \nST, UCF, USCD \nACSCP \n\n\n\nTable 5 .\n5Summary of advantages and limitations of multitask-CNN-CC algorithmsTechnique Features \nDatasets \n\nNegative \nSamples \n\nData \nDriven \nArchitecture \nYes No Yes No \n\nArteta et al. [92] \n\nMultitasking: foreground and background \nsegmentation, uncertainty, and density \nestimation \n\nPenguins dataset \nConvNet \n\nIdrees et al. [93] Multitasking with loss optimization \nUCF-QNRF \nDenseNet \n\nZhu et al. [94] \nCombination of pedestrian flow \nstatistics task with people counting \n\nUCF, \n[DH302IMG, DH302VID] * \nVGGNet-16 \n\nHuang et al. [95] Body structure-aware methods \nSTB, UCF, USCD \nMulti-column body-part \naware model \n\nYang et al. [96] \nMulticolumn multitask CNN focusing \non drastic scale variation \n\nST, UCF, USCD, \nMALL, WE \nMMCNN \n\nLiu et al. [97] Self-supervised tasking \nUCF, STA, STB \nVGG-16 \n\n* Private Datasets. \n\n\nTable 6 .\n6Summary of advantages and limitations of aerial-view-CNN-CC algorithmsTechnique \nFeatures \nDatasets * \n\nNegative \nSamples \n\nData \nDriven Architecture \nYes No Yes No \n\nKhan et al. [98] \n\nAutomatic approach to select a region \nof interest by computing a bounding \nbox that encloses the embryo \n\nTime-lapse image \nsequences \n\nArchitecture of \nKrizhevsky \n\nRibera et al. [99] \nPlants are estimated by using the \nregression model instead of classification \n\nRGB UAV images of \nsorghum plants \nInception-v2 \n\nHernnandez et al. [100] Feature pyramid network \nBBBC005 \nVGG-Style NN \n\nXie et al. [101] \nTwo convolutional regression networks RPE, T and LBL cells \nVGG-net \n\n* Private Datasets. \n\n\n\nTable 7 .\n7Summary of advantages and limitations of Perspective-CNN-CC algorithmsTechnique \nFeatures \nDatasets \n\nNegative \nSamples \n\nData \nDriven \nArchitecture \nYes No Yes No \n\nKang et al. [102] \n\nIncorporating side information \n(perspective weights) in CNN by \nusing adaptive convolutional layers \n\nUSCD \nACNN \n\nZhao et al. [103] Perspective embedded deconvolution network WE \nPE-CFCN-DCN \n\nMarsden et al. [104] Multidomain patch-based regressor \nST, Penguin, Dublin cell * \nVGG16 \n\nZhang et al. [105] \nCross scene crowd counting, human body \nshape and perspective variation are considered \nUCF \nCrowd CNN model \n\nShi et al. [106] Perspective-aware weighting layer \nUCF, WE, STA, STB \nPACNN \n\nYao et al. [107] General model based on CNN and LSTM \nST, UCF, WE \nDSRM with ResNet \n\n* Private Datasets. \n\n\n\nTable 8 .\n8Summary of advantages and limitations of patch-based-CNN-CC algorithms.Technique \nFeatures \nDatasets \n\nNegative \nSamples \n\nData \nDriven Architecture \nYes No Yes No \n\nCohen et al. [108] \nSmaller network used for \nestimation in given receptive field \n[VGG, MBM] * \nCount-ception \n\nLiu et al. [109] \nDetection and density-estimation network \nMall, STB, WE \nDecideNet \n\nOnro-Rubio et al. [110] \nJoint feature extraction and pixelwise \nobject density \nST, USCD, TRANSCOS \nGU-Net \n\nXu et al. [111] \nDepth-information-based method \nSTB, Mall, ZZU-CIISR \nMulti-scale \nnetwork \n\nShami et al. [112] Head-detector-based crowd-estimation method ST, UCF \nImagNet \n\nZhang et al. [113] Aggregated framework \nUCF, AHU-CROWD \ncount-net \n\nZhang et al. [114] Multicolumn CNN with varying receptive fields ST, UCF \nMCNN \n\nWang et al. [115] \nSkip-connection CNN with scale-related training ST, UCF \nSCNN \n\nSam et al. [116] \nSwitch CNN multidomain patch-based regressor ST, UCF, WE \nSwitch CNN \n\n* Private Datasets. \n\n\n\nTable 9 .\n9Summary of advantages and limitations of whole-image-CNN-CC algorithmsTechnique \nFeatures \nDatasets \n\nNegative \nSamples \n\nData \nDriven \nArchitecture \nYes No Yes No \n\nRahnmonfar et al. [117] \nSimulated learning, and synthetic data for training, \ntested on real images \nFruit dataset * \nInception-ResNet \n\nSheng et al. [118] \nPixel-level semantic-feature map, \nlearning locality-aware features \n\nUSCD, \nMall \n\nSemantic-feature map \nand W-VLAD encoding \n\nMarsden et al. [119] \n\nSimultaneous multiobjective method for violent-behavior \ndetection, crowd counting and density-level \nclassification, creation of new dataset \n\nUCF \nResNetCrowd \n\nMarsden et al. [120] Multiscale averaging to handle scale variation \nST, UCF \nFCN \n\nSindagi et al. [121] \n\nMultitask end-to-end cascaded network \nof CNNs to learn both crowd-count \nclassification and density estimation \n\nST, UCF \nCascaded network \n\n* Private Datasets. \n\n\n\n\nas Basic-CNN-CC algorithms tested via the USCD and UCF datasets.\u2022 \nRefs. [77-81] as Context-CNN-CC algorithms tested via the UCF, ShanghaiTech-A (STA) and \nShanghaiTech-B (STB) datasets. \n\u2022 \nRefs. [83-91] as Scale-CNN-CC algorithms tested via the UCF, STA and STB datasets. \n\u2022 \nRefs. \nitezhu2018people,huang2018body,yang2018counting,liu2018leveraging as Multi-task-CNN-CC \nalgorithms tested via the UCF, STA and STB datasets. \n\u2022 \nRefs. [102,104-107] as Perspective-CNN-CC algorithms tested via the UCF, STA and STB datasets. \n\u2022 \n\n\n\nCNN-CC Network-based CNN-CC techniques Basic-CNN-CC Basic CNN-CC techniques Context-CNN-CC Context-aware CNN-CC techniques Scale-CNN-CC Scale-aware CNN-CC techniques Multi-task-CNN-CC Multitask CNN-CC techniques Image-view-CNN-CC Image-view-based CNN-CC techniques Aerial-view-CNN-CC Aerial-view-based CNN-CC techniques Perspective-CNN-CC Perspective-view-based CNN-CC techniques Patch-based-CNN-CC Patch-based CNN-CC techniques Whole-image-CNN-CC Whole-image-based CNN-CC techniques Training-CNN-CC Training-approach-based CNN-CC techniquesNNs \nNeural Networks \nCNNs \nConvolutional NNs \nRNNs \nRecurrent NNs \nFCL \nFully Connected Layer \nUAV \nUnmanned Aerial Vehicle \nReLU \nRectified Linear Unit \nGTD \nGround Truth Density \nED \nEstimated Density \nGLCM Gray Level Co-Occurrence Metrics \nHOG \nHistogram Oriented Gradient \nLBP \nLocal Binary Pattern \nKLT \nKanade-Lucas-Tomasi \nGANs \nGenerative Adversarial Networks \nMAE \nMean Absolute Error \nMSE \nMean Square Error \nSTA \nShanghaiTech-A (a dataset) \nSTB \nShanghaiTech-B (a dataset) \nWE \nWorld Expo 10 (a dataset) \nCNN-CC \nCNN Crowd Counting \nNetwork-\n\nMitosis detection in breast cancer pathology images by combining handcrafted and convolutional neural network features. H Wang, A C Roa, A N Basavanhally, H L Gilmore, N Shih, M Feldman, J Tomaszewski, F Gonzalez, A Madabhushi, 10.1117/1.JMI.1.3.034003J. Med. Imaging. 134003PubMedWang, H.; Roa, A.C.; Basavanhally, A.N.; Gilmore, H.L.; Shih, N.; Feldman, M.; Tomaszewski, J.; Gonzalez, F.; Madabhushi, A. Mitosis detection in breast cancer pathology images by combining handcrafted and convolutional neural network features. J. Med. Imaging 2014, 1, 034003. [CrossRef] [PubMed]\n\nCascaded ensemble of convolutional neural networks and handcrafted features for mitosis detection. H Wang, A Cruz-Roa, A Basavanhally, H Gilmore, N Shih, M Feldman, J Tomaszewski, F Gonzalez, A Madabhushi, Digital Pathology; International Society for Optics and Photonics. San Diego, CA, USA904190410Medical ImagingWang, H.; Cruz-Roa, A.; Basavanhally, A.; Gilmore, H.; Shih, N.; Feldman, M.; Tomaszewski, J.; Gonzalez, F.; Madabhushi, A. Cascaded ensemble of convolutional neural networks and handcrafted features for mitosis detection. In Medical Imaging 2014: Digital Pathology; International Society for Optics and Photonics: San Diego, CA, USA, 2014; Volume 9041, p. 90410B.\n\nPedestrian detection: An evaluation of the state of the art. P Dollar, C Wojek, B Schiele, P Perona, 10.1109/TPAMI.2011.155IEEE Trans. Pattern Anal. Mach. Intell. 34PubMedDollar, P.; Wojek, C.; Schiele, B.; Perona, P. Pedestrian detection: An evaluation of the state of the art. IEEE Trans. Pattern Anal. Mach. Intell. 2012, 34, 743-761. [CrossRef] [PubMed]\n\nFeature mining for localised crowd counting. K Chen, C C Loy, S Gong, T Xiang, Proceedings of the BMVC. the BMVCSurrey, UK13Chen, K.; Loy, C.C.; Gong, S.; Xiang, T. Feature mining for localised crowd counting. In Proceedings of the BMVC, Surrey, UK, 3-7 September 2012; Volume 1, p. 3.\n\nLearning to count with regression forest and structured labels. L Fiaschi, U K\u00f6the, R Nair, F A Hamprecht, Proceedings of the 2012 21st International Conference on Pattern Recognition (ICPR). the 2012 21st International Conference on Pattern Recognition (ICPR)JapanFiaschi, L.; K\u00f6the, U.; Nair, R.; Hamprecht, F.A. Learning to count with regression forest and structured labels. In Proceedings of the 2012 21st International Conference on Pattern Recognition (ICPR), Tsukuba Science City, Japan, 11-15 November 2012; pp. 2685-2688.\n\nLearning to count leaves in rosette plants. M V Giuffrida, M Minervini, S A Tsaftaris, Proceedings of the Computer Vision Problems in Plant Phenotyping (CVPPP). the Computer Vision Problems in Plant Phenotyping (CVPPP)Swansea, UKGiuffrida, M.V.; Minervini, M.; Tsaftaris, S.A. Learning to count leaves in rosette plants. In Proceedings of the Computer Vision Problems in Plant Phenotyping (CVPPP), Swansea, UK, 7-10 September 2015.\n\nRecognizing human group action by layered model with multiple cues. Z Cheng, L Qin, Q Huang, S Yan, Q Tian, 10.1016/j.neucom.2014.01.019Neurocomputing. 136Cheng, Z.; Qin, L.; Huang, Q.; Yan, S.; Tian, Q. Recognizing human group action by layered model with multiple cues. Neurocomputing 2014, 136, 124-135. [CrossRef]\n\nBayesian poisson regression for crowd counting. A B Chan, N Vasconcelos, Proceedings of the 2009 IEEE 12th International Conference on Computer Vision. the 2009 IEEE 12th International Conference on Computer VisionKyoto, JapanChan, A.B.; Vasconcelos, N. Bayesian poisson regression for crowd counting. In Proceedings of the 2009 IEEE 12th International Conference on Computer Vision, Kyoto, Japan, 29 September-2 October 2009;\n\nCrowd density estimation using texture analysis and learning. X Wu, G Liang, K K Lee, Y Xu, Proceedings of the 2006 IEEE International Conference on Robotics and Biomimetics. the 2006 IEEE International Conference on Robotics and BiomimeticsKunming, ChinaWu, X.; Liang, G.; Lee, K.K.; Xu, Y. Crowd density estimation using texture analysis and learning. In Proceedings of the 2006 IEEE International Conference on Robotics and Biomimetics, Kunming, China, 17-20 December 2006; pp. 214-219.\n\nLearning to count objects in images. V Lempitsky, A Zisserman, Proceedings of the Advances in Neural Information Processing Systems. the Advances in Neural Information Processing SystemsVancouver, BC, CanadaLempitsky, V.; Zisserman, A. Learning to count objects in images. In Proceedings of the Advances in Neural Information Processing Systems, Vancouver, BC, Canada, 6-11 December 2010; pp. 1324-1332.\n\nObject recognition as machine translation: Learning a lexicon for a fixed image vocabulary. P Duygulu, K Barnard, J F De Freitas, D A Forsyth, Proceedings of the European Conference on Computer Vision. the European Conference on Computer VisionCopenhagen, DenmarkDuygulu, P.; Barnard, K.; de Freitas, J.F.; Forsyth, D.A. Object recognition as machine translation: Learning a lexicon for a fixed image vocabulary. In Proceedings of the European Conference on Computer Vision, Copenhagen, Denmark, 28-31 May 2002; pp. 97-112.\n\nFast discriminative visual codebooks using randomized clustering forests. F Moosmann, B Triggs, F Jurie, Proceedings of the Advances in Neural Information Processing Systems. the Advances in Neural Information Processing SystemsVancouver, BC, CanadaMoosmann, F.; Triggs, B.; Jurie, F. Fast discriminative visual codebooks using randomized clustering forests. In Proceedings of the Advances in Neural Information Processing Systems, Vancouver, BC, Canada, 3-6 December 2007; pp. 985-992.\n\nCounting crowded moving objects. V Rabaud, S Belongie, Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. the 2006 IEEE Computer Society Conference on Computer Vision and Pattern RecognitionNew York, NY, USARabaud, V.; Belongie, S. Counting crowded moving objects. In Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, New York, NY, USA, 17-22 June 2006;\n\nUnsupervised bayesian detection of independent motion in crowds. G J Brostow, R Cipolla, Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. the 2006 IEEE Computer Society Conference on Computer Vision and Pattern RecognitionNew York, NY, USA1Brostow, G.J.; Cipolla, R. Unsupervised bayesian detection of independent motion in crowds. In Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, New York, NY, USA, 17-22 June 2006; Volume 1, pp. 594-601.\n\nIntelligent Software Supporting Situation Awareness, Response, and Operations. F T Abbott, A H Johnson, S D Prior, D D Steiner, Quantum Leap Innovations IncNewark, NJ, USATechnical ReportIntegrated Biological Warfare Technology Platform (IBWTP)Abbott, F.T.; Johnson, A.H.; Prior, S.D.; Steiner, D.D. Integrated Biological Warfare Technology Platform (IBWTP). Intelligent Software Supporting Situation Awareness, Response, and Operations; Technical Report; Quantum Leap Innovations Inc.: Newark, NJ, USA, 2007.\n\nHistograms of oriented gradients for human detection. N Dalal, B Triggs, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition. the IEEE Computer Society Conference on Computer Vision and Pattern RecognitionSan Diego, CA, USA1Dalal, N.; Triggs, B. Histograms of oriented gradients for human detection. In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, San Diego, CA, USA, 20-25 June 2005; Volume 1, pp. 886-893.\n\nPrivacy preserving crowd monitoring: Counting people without people models or tracking. A B Chan, Z S J Liang, N Vasconcelos, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionAnchorage, AK, USAChan, A.B.; Liang, Z.S.J.; Vasconcelos, N. Privacy preserving crowd monitoring: Counting people without people models or tracking. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Anchorage, AK, USA, 23-28 June 2008; pp. 1-7.\n\nAlmost Unsupervised Learning for Dense Crowd Counting. D B Sam, N N Sajjan, H Maurya, R V Babu, Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence. the Thirty-Third AAAI Conference on Artificial IntelligenceHonolulu, HI, USA27Sam, D.B.; Sajjan, N.N.; Maurya, H.; Babu, R.V. Almost Unsupervised Learning for Dense Crowd Counting. In Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence, Honolulu, HI, USA, 27\n\nCrowd behavior analysis from fixed and moving cameras. P Bour, E Cribelier, V Argyriou, Multimodal Behavior Analysis in the Wild. Amsterdam, The NetherlandsElsevierBour, P.; Cribelier, E.; Argyriou, V. Crowd behavior analysis from fixed and moving cameras. In Multimodal Behavior Analysis in the Wild; Elsevier: Amsterdam, The Netherlands, 2019; pp. 289-322.\n\nGetting to know low-light images with the Exclusively Dark dataset. Y P Loh, C S Chan, 10.1016/j.cviu.2018.10.010Comput. Vis. Image Underst. 178Loh, Y.P.; Chan, C.S. Getting to know low-light images with the Exclusively Dark dataset. Comput. Vis. Image Underst. 2019, 178, 30-42. [CrossRef]\n\nMulti-resolution attention convolutional neural network for crowd counting. Y Zhang, C Zhou, F Chang, A C Kot, 10.1016/j.neucom.2018.10.058Neurocomputing. 329Zhang, Y.; Zhou, C.; Chang, F.; Kot, A.C. Multi-resolution attention convolutional neural network for crowd counting. Neurocomputing 2019, 329, 144-152. [CrossRef]\n\nCounting the Number of People in Crowd as a Part of Automatic Crowd Monitoring: A Combined Approach. Y Bharti, R Saharan, A Saxena, Information and Communication Technology for. Intelligent SystemsBharti, Y.; Saharan, R.; Saxena, A. Counting the Number of People in Crowd as a Part of Automatic Crowd Monitoring: A Combined Approach. In Information and Communication Technology for Intelligent Systems;\n\n. Springer, SingaporeSpringer: Singapore, 2019; pp. 545-552.\n\nCrowd analysis: A survey. B Zhan, D N Monekosso, P Remagnino, S A Velastin, L Q Xu, 10.1007/s00138-008-0132-4Mach. Vis. Appl. 19Zhan, B.; Monekosso, D.N.; Remagnino, P.; Velastin, S.A.; Xu, L.Q. Crowd analysis: A survey. Mach. Vis. Appl. 2008, 19, 345-357. [CrossRef]\n\nAdvances and trends in visual crowd analysis: A systematic survey and evaluation of crowd modelling techniques. M S Zitouni, H Bhaskar, J Dias, M E Al-Mualla, 10.1016/j.neucom.2015.12.070Neurocomputing. 186Zitouni, M.S.; Bhaskar, H.; Dias, J.; Al-Mualla, M.E. Advances and trends in visual crowd analysis: A systematic survey and evaluation of crowd modelling techniques. Neurocomputing 2016, 186, 139-159. [CrossRef]\n\nAn evaluation of crowd counting methods, features and regression models. D Ryan, S Denman, S Sridharan, C Fookes, 10.1016/j.cviu.2014.07.008Comput. Vis. Image Underst. 130Ryan, D.; Denman, S.; Sridharan, S.; Fookes, C. An evaluation of crowd counting methods, features and regression models. Comput. Vis. Image Underst. 2015, 130, 1-17. [CrossRef]\n\nA survey of recent advances in cnn-based single image crowd counting and density estimation. V A Sindagi, V M Patel, 10.1016/j.patrec.2017.07.007Pattern Recognit. Lett. 107Sindagi, V.A.; Patel, V.M. A survey of recent advances in cnn-based single image crowd counting and density estimation. Pattern Recognit. Lett. 2018, 107, 3-16. [CrossRef]\n\nDeeply learned attributes for crowded scene understanding. J Shao, K Kang, C Loy, X Wang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionBoston, MA, USAShao, J.; Kang, K.; Change Loy, C.; Wang, X. Deeply learned attributes for crowded scene understanding. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Boston, MA, USA, 7-12 June 2015; pp. 4657-4666.\n\nData-driven crowd understanding: A baseline for a large-scale crowd dataset. C Zhang, K Kang, H Li, X Wang, R Xie, X Yang, 10.1109/TMM.2016.2542585IEEE Trans. Multimed. 18Zhang, C.; Kang, K.; Li, H.; Wang, X.; Xie, R.; Yang, X. Data-driven crowd understanding: A baseline for a large-scale crowd dataset. IEEE Trans. Multimed. 2016, 18, 1048-1061. [CrossRef]\n\nLow cost crowd counting using audio tones. P G Kannan, S P Venkatagiri, M C Chan, A L Ananda, L S Peh, Proceedings of the 10th ACM Conference on Embedded Network Sensor Systems. the 10th ACM Conference on Embedded Network Sensor Systemsoronto, ON, Canada, 6-9Kannan, P.G.; Venkatagiri, S.P.; Chan, M.C.; Ananda, A.L.; Peh, L.S. Low cost crowd counting using audio tones. In Proceedings of the 10th ACM Conference on Embedded Network Sensor Systems, oronto, ON, Canada, 6-9 November 2012; pp. 155-168.\n\nConvolutional networks and applications in vision. Y Lecun, K Kavukcuoglu, C Farabet, Proceedings of the 2010 IEEE International Symposium on Circuits and Systems. the 2010 IEEE International Symposium on Circuits and SystemsParis, France2010LeCun, Y.; Kavukcuoglu, K.; Farabet, C. Convolutional networks and applications in vision. In Proceedings of the 2010 IEEE International Symposium on Circuits and Systems, Paris, France, 30 May-2 June 2010; Volume 2010, pp. 253-256.\n\nDeep maxout neural networks for speech recognition. M Cai, Y Shi, J Liu, Proceedings of the 2013 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU). the 2013 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)Olomouc, Czech RepublicCai, M.; Shi, Y.; Liu, J. Deep maxout neural networks for speech recognition. In Proceedings of the 2013 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), Olomouc, Czech Republic, 8-12 December 2013; pp. 291-296.\n\nDeep convolutional neural networks for large-scale speech tasks. T N Sainath, B Kingsbury, G Saon, H Soltau, A R Mohamed, G Dahl, B Ramabhadran, 10.1016/j.neunet.2014.08.005Neural Netw. 64PubMedSainath, T.N.; Kingsbury, B.; Saon, G.; Soltau, H.; Mohamed, A.R.; Dahl, G.; Ramabhadran, B. Deep convolutional neural networks for large-scale speech tasks. Neural Netw. 2015, 64, 39-48. [CrossRef] [PubMed]\n\nApplying convolutional neural networks concepts to hybrid NN-HMM model for speech recognition. O Abdel-Hamid, A R Mohamed, H Jiang, G Penn, Proceedings of the 2012 IEEE International Conference on Acoustics, Speech and Signal Processing. the 2012 IEEE International Conference on Acoustics, Speech and Signal ProcessingKyoto, JapanAbdel-Hamid, O.; Mohamed, A.R.; Jiang, H.; Penn, G. Applying convolutional neural networks concepts to hybrid NN-HMM model for speech recognition. In Proceedings of the 2012 IEEE International Conference on Acoustics, Speech and Signal Processing, Kyoto, Japan, 25-30 March 2012; pp. 4277-4280.\n\nEvaluating the covariance matrix constraints for data-driven statistical human motion reconstruction. C Mousas, P Newbury, C N Anagnostopoulos, Proceedings of the 30th Spring Conference on Computer Graphics. the 30th Spring Conference on Computer GraphicsMikulov, Czech RepublicMousas, C.; Newbury, P.; Anagnostopoulos, C.N. Evaluating the covariance matrix constraints for data-driven statistical human motion reconstruction. In Proceedings of the 30th Spring Conference on Computer Graphics, Mikulov, Czech Republic, 28-30 May 2014; pp. 99-106.\n\nFull-body locomotion reconstruction of virtual characters using a single inertial measurement unit. C Mousas, 10.3390/s1711258917PubMedMousas, C. Full-body locomotion reconstruction of virtual characters using a single inertial measurement unit. Sensors 2017, 17, 2589. [CrossRef] [PubMed]\n\nA Fast Feature Extraction Algorithm for Image and Video Processing. S H Abdulhussain, A R Ramli, B M Mahmmod, M I Saripan, S Al-Haddad, T Baker, W N Flayyih, W A Jassim, Proceedings of the 2019 International Joint Conference on Neural Networks (IJCNN). the 2019 International Joint Conference on Neural Networks (IJCNN)Budapest, HungaryAbdulhussain, S.H.; Ramli, A.R.; Mahmmod, B.M.; Saripan, M.I.; Al-Haddad, S.; Baker, T.; Flayyih, W.N.; Jassim, W.A. A Fast Feature Extraction Algorithm for Image and Video Processing. In Proceedings of the 2019 International Joint Conference on Neural Networks (IJCNN), Budapest, Hungary, 14-19 July 2019; pp. 1-8.\n\nRemote Sensing Image Scene Classification Using CNN-CapsNet. W Zhang, P Tang, L Zhao, 10.3390/rs11050494Remote Sens. 2019, 11, 494. [CrossRefZhang, W.; Tang, P.; Zhao, L. Remote Sensing Image Scene Classification Using CNN-CapsNet. Remote Sens. 2019, 11, 494. [CrossRef]\n\nI Kim, S Rajaraman, S Antani, 10.3390/diagnostics9020038Visual Interpretation of Convolutional Neural Network Predictions in Classifying Medical Image Modalities. Diagnostics. 9Kim, I.; Rajaraman, S.; Antani, S. Visual Interpretation of Convolutional Neural Network Predictions in Classifying Medical Image Modalities. Diagnostics 2019, 9, 38. [CrossRef]\n\nDetection of multiple, partially occluded humans in a single image by bayesian combination of edgelet part detectors. B Wu, R Nevatia, Proceedings of the Tenth IEEE International Conference on Computer Vision. the Tenth IEEE International Conference on Computer VisionSan Diego, CA, USAWu, B.; Nevatia, R. Detection of multiple, partially occluded humans in a single image by bayesian combination of edgelet part detectors. In Proceedings of the Tenth IEEE International Conference on Computer Vision, San Diego, CA, USA, 17-21 October 2005; pp. 90-97.\n\nDetecting pedestrians by learning shapelet features. P Sabzmeydani, G Mori, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionMinneapolis, MN, USASabzmeydani, P.; Mori, G. Detecting pedestrians by learning shapelet features. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Minneapolis, MN, USA, 17-22 June 2007; pp. 1-8.\n\nObject detection with discriminatively trained part-based models. P F Felzenszwalb, R B Girshick, D Mcallester, D Ramanan, 10.1109/TPAMI.2009.167IEEE Trans. Pattern Anal. Mach. Intell. 32Felzenszwalb, P.F.; Girshick, R.B.; McAllester, D.; Ramanan, D. Object detection with discriminatively trained part-based models. IEEE Trans. Pattern Anal. Mach. Intell. 2010, 32, 1627-1645. [CrossRef]\n\nEstimation of number of people in crowded scenes using perspective transformation. S F Lin, J Y Chen, H X Chao, IEEE Trans. Syst. Man Cybern. Part A Syst. Hum. 31Lin, S.F.; Chen, J.Y.; Chao, H.X. Estimation of number of people in crowded scenes using perspective transformation. IEEE Trans. Syst. Man Cybern. Part A Syst. Hum. 2001, 31, 645-654.\n\nEstimating the number of people in crowded scenes by mid based foreground segmentation and head-shoulder detection. M Li, Z Zhang, K Huang, T Tan, Proceedings of the 19th International Conference on Pattern Recognition. the 19th International Conference on Pattern RecognitionTampa, FL, USA, 8-Li, M.; Zhang, Z.; Huang, K.; Tan, T. Estimating the number of people in crowded scenes by mid based foreground segmentation and head-shoulder detection. In Proceedings of the 19th International Conference on Pattern Recognition, Tampa, FL, USA, 8-11 December 2008; pp. 1-4.\n\nA MRF-based approach for real-time subway monitoring. N Paragios, V Ramesh, Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. the 2001 IEEE Computer Society Conference on Computer Vision and Pattern RecognitionKauai, HI, USAParagios, N.; Ramesh, V. A MRF-based approach for real-time subway monitoring. In Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, Kauai, HI, USA, 8-14 December 2001.\n\nGraphical model architectures for speech recognition. J A Bilmes, C Bartels, 10.1109/MSP.2005.1511827IEEE Signal Process. Mag. 22Bilmes, J.A.; Bartels, C. Graphical model architectures for speech recognition. IEEE Signal Process. Mag. 2005, 22, 89-100. [CrossRef]\n\nDeep learning for medical image processing: Overview, challenges and the future. M I Razzak, S Naz, A Zaib, Classification in BioApps. Berlin/Heidelberg, GermanySpringerRazzak, M.I.; Naz, S.; Zaib, A. Deep learning for medical image processing: Overview, challenges and the future. In Classification in BioApps; Springer: Berlin/Heidelberg, Germany, 2018; pp. 323-350.\n\nPose-based cnn features for action recognition. G Ch\u00e9ron, I Laptev, C Schmid, P-Cnn, Proceedings of the IEEE international Conference on Computer Vision. the IEEE international Conference on Computer VisionSantiago, ChileCh\u00e9ron, G.; Laptev, I.; Schmid, C. P-cnn: Pose-based cnn features for action recognition. In Proceedings of the IEEE international Conference on Computer Vision, Santiago, Chile, 13-16 December 2015; pp. 3218-3226.\n\nAuto-conditioned lstm network for extended complex human motion synthesis. Z Li, Y Zhou, S Xiao, C He, H Li, arXiv:1707.05363Li, Z.; Zhou, Y.; Xiao, S.; He, C.; Li, H. Auto-conditioned lstm network for extended complex human motion synthesis. arXiv 2017, arXiv:1707.05363.\n\nPhotorealistic facial texture inference using deep neural networks. S Saito, L Wei, L Hu, K Nagano, H Li, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionHonolulu, HI, USASaito, S.; Wei, L.; Hu, L.; Nagano, K.; Li, H. Photorealistic facial texture inference using deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, HI, USA, 21-26 July 2017; pp. 5144-5153.\n\nDilated Convolutional Neural Network for Predicting Driver's Activity. B Rekabdar, C Mousas, Proceedings of the 2018 21st International Conference on Intelligent Transportation Systems (ITSC). the 2018 21st International Conference on Intelligent Transportation Systems (ITSC)Maui, HI, USARekabdar, B.; Mousas, C. Dilated Convolutional Neural Network for Predicting Driver's Activity. In Proceedings of the 2018 21st International Conference on Intelligent Transportation Systems (ITSC), Maui, HI, USA, 4-7 November 2018; pp. 3245-3250.\n\nGenerative Adversarial Network with Policy Gradient for Text Summarization. B Rekabdar, C Mousas, B Gupta, Proceedings of the 2019 IEEE 13th International Conference on Semantic Computing (ICSC). the 2019 IEEE 13th International Conference on Semantic Computing (ICSC)Newport Beach, CA, USARekabdar, B.; Mousas, C.; Gupta, B. Generative Adversarial Network with Policy Gradient for Text Summarization. In Proceedings of the 2019 IEEE 13th International Conference on Semantic Computing (ICSC), Newport Beach, CA, USA, 30 January-1 February 2019; pp. 204-207.\n\nDeep learning based oil palm tree detection and counting for high-resolution remote sensing images. W Li, H Fu, L Yu, A Cracknell, 10.3390/rs9010022Remote Sens. 2016, 9, 22. [CrossRefLi, W.; Fu, H.; Yu, L.; Cracknell, A. Deep learning based oil palm tree detection and counting for high-resolution remote sensing images. Remote Sens. 2016, 9, 22. [CrossRef]\n\nRich Convolutional Features Fusion for Crowd Counting. C Fan, J Tang, N Wang, D Liang, Proceedings of the 2018 13th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2018). the 2018 13th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2018)Xi'an, ChinaFan, C.; Tang, J.; Wang, N.; Liang, D. Rich Convolutional Features Fusion for Crowd Counting. In Proceedings of the 2018 13th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2018), Xi'an, China, 15-19 May 2018; pp. 394-398.\n\nDivide and Count: Generic Object Counting by Image Divisions. T Stahl, S L Pintea, J C Van Gemert, 10.1109/TIP.2018.2875353IEEE Trans. Image Process. 28Stahl, T.; Pintea, S.L.; van Gemert, J.C. Divide and Count: Generic Object Counting by Image Divisions. IEEE Trans. Image Process. 2019, 28, 1035-1044. [CrossRef]\n\nCNN: A Paradigm for Complexity. L O Chua, World Scientific31SingaporeChua, L.O. CNN: A Paradigm for Complexity; World Scientific: Singapore, 1998; Volume 31.\n\nAn Observational Study of Deep Learning and Automated Evaluation of Cervical Images for Cancer Screening. L Hu, D Bell, S Antani, Z Xue, K Yu, M P Horning, N Gachuhi, B Wilson, M S Jaiswal, B Befano, JNCI J. Natl. Cancer Inst. 74Hu, L.; Bell, D.; Antani, S.; Xue, Z.; Yu, K.; Horning, M.P.; Gachuhi, N.; Wilson, B.; Jaiswal, M.S.; Befano, B.; et al. An Observational Study of Deep Learning and Automated Evaluation of Cervical Images for Cancer Screening. JNCI J. Natl. Cancer Inst. 2019, 74, 343-344.\n\nMachine and deep learning for sport-specific movement recognition: A systematic review of model development and performance. E E Cust, A J Sweeting, K Ball, S Robertson, 10.1080/02640414.2018.1521769J. Sport. Sci. 37PubMedCust, E.E.; Sweeting, A.J.; Ball, K.; Robertson, S. Machine and deep learning for sport-specific movement recognition: A systematic review of model development and performance. J. Sport. Sci. 2019, 37, 568-600. [CrossRef] [PubMed]\n\nSharpness fields in point clouds using deep learning. P Raina, S Mudur, T Popa, 10.1016/j.cag.2018.11.003Comput. Graph. 78Raina, P.; Mudur, S.; Popa, T. Sharpness fields in point clouds using deep learning. Comput. Graph. 2019, 78, 37-53. [CrossRef]\n\nState-of-the-art review on deep learning in medical imaging. M Biswas, V Kuppili, L Saba, D Edla, H Suri, E Cuadrado-Godia, J Laird, R Marinhoe, J Sanches, A Nicolaides, Front. Biosci. 24Biswas, M.; Kuppili, V.; Saba, L.; Edla, D.; Suri, H.; Cuadrado-Godia, E.; Laird, J.; Marinhoe, R.; Sanches, J.; Nicolaides, A.; et al. State-of-the-art review on deep learning in medical imaging. Front. Biosci. 2019, 24, 392-426.\n\nConvolutional Neural Network-Based Human Identification Using Outer Ear Images. In Soft Computing for Problem Solving. H Sinha, R Manekar, Y Sinha, P K Ajmera, SpringerBerlin/Heidelberg, GermanySinha, H.; Manekar, R.; Sinha, Y.; Ajmera, P.K. Convolutional Neural Network-Based Human Identification Using Outer Ear Images. In Soft Computing for Problem Solving; Springer: Berlin/Heidelberg, Germany, 2019; pp. 707-719.\n\nA Futuristic Deep Learning Framework Approach for Land Use-Land Cover Classification Using Remote Sensing Imagery. R Nijhawan, D Joshi, N Narang, A Mittal, A Mittal, Advanced Computing and Communication Technologies. Berlin/Heidelberg, GermanySpringerNijhawan, R.; Joshi, D.; Narang, N.; Mittal, A.; Mittal, A. A Futuristic Deep Learning Framework Approach for Land Use-Land Cover Classification Using Remote Sensing Imagery. In Advanced Computing and Communication Technologies; Springer: Berlin/Heidelberg, Germany, 2019; pp. 87-96.\n\nA Y-Net deep learning method for road segmentation using high-resolution visible remote sensing images. Y Li, L Xu, J Rao, L Guo, Z Yan, S Jin, 10.1080/2150704X.2018.1557791Remote Sens. Lett. 10Li, Y.; Xu, L.; Rao, J.; Guo, L.; Yan, Z.; Jin, S. A Y-Net deep learning method for road segmentation using high-resolution visible remote sensing images. Remote Sens. Lett. 2019, 10, 381-390. [CrossRef]\n\nPeople Counting with Overhead Camera Using Fuzzy-Based Detector. N K Verma, R Dev, S Maurya, N K Dhar, P Agrawal, Computational Intelligence: Theories, Applications and Future Directions-Volume I. Verma, N.K.; Dev, R.; Maurya, S.; Dhar, N.K.; Agrawal, P. People Counting with Overhead Camera Using Fuzzy-Based Detector. In Computational Intelligence: Theories, Applications and Future Directions-Volume I;\n\n. Springer, Berlin/Heidelberg, GermanySpringer: Berlin/Heidelberg, Germany, 2019; pp. 589-601.\n\nREMODEL: Rethinking deep CNN models to detect and count on a NeuroSynaptic system. R Shukla, M Lipasti, B Van Essen, A Moody, N Maruyama, 10.3389/fnins.2019.00004Front. Neurosci. 13, 4. [CrossRefShukla, R.; Lipasti, M.; Van Essen, B.; Moody, A.; Maruyama, N. REMODEL: Rethinking deep CNN models to detect and count on a NeuroSynaptic system. Front. Neurosci. 2019, 13, 4. [CrossRef]\n\nCrowd density estimation based on classification activation map and patch density level. L Zhu, C Li, Z Yang, K Yuan, S Wang, 10.1007/s00521-018-3954-7Neural Comput. Appl. Zhu, L.; Li, C.; Yang, Z.; Yuan, K.; Wang, S. Crowd density estimation based on classification activation map and patch density level. Neural Comput. Appl. 2019. [CrossRef]\n\nImproving the Quality of Synthetic FLAIR Images with Deep Learning Using a Conditional Generative Adversarial Network for Pixel-by-Pixel Image Translation. A Hagiwara, Y Otsuka, M Hori, Y Tachibana, K Yokoyama, S Fujita, C Andica, K Kamagata, R Irie, S Koshino, 10.3174/ajnr.A5927Am. J. Neuroradiol. 40Hagiwara, A.; Otsuka, Y.; Hori, M.; Tachibana, Y.; Yokoyama, K.; Fujita, S.; Andica, C.; Kamagata, K.; Irie, R.; Koshino, S.; et al. Improving the Quality of Synthetic FLAIR Images with Deep Learning Using a Conditional Generative Adversarial Network for Pixel-by-Pixel Image Translation. Am. J. Neuroradiol. 2019, 40, 224-230. [CrossRef]\n\nA survey of methods for image annotation. A Hanbury, 10.1016/j.jvlc.2008.01.002J. Vis. Lang. Comput. 19Hanbury, A. A survey of methods for image annotation. J. Vis. Lang. Comput. 2008, 19, 617-627. [CrossRef]\n\nCrowd counting and profiling: Methodology and evaluation. In Modeling, Simulation and Visual Analysis of Crowds. C C Loy, K Chen, S Gong, T Xiang, SpringerBerlin/Heidelberg, GermanyLoy, C.C.; Chen, K.; Gong, S.; Xiang, T. Crowd counting and profiling: Methodology and evaluation. In Modeling, Simulation and Visual Analysis of Crowds; Springer: Berlin/Heidelberg, Germany, 2013; pp. 347-382.\n\nMulti-source multi-scale counting in extremely dense crowd images. H Idrees, I Saleemi, C Seibert, M Shah, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionPortland, OR, USAIdrees, H.; Saleemi, I.; Seibert, C.; Shah, M. Multi-source multi-scale counting in extremely dense crowd images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Portland, OR, USA, 23-28 June 2013; pp. 2547-2554.\n\nFast crowd density estimation with convolutional neural networks. M Fu, P Xu, X Li, Q Liu, M Ye, C Zhu, 10.1016/j.engappai.2015.04.006Eng. Appl. Artif. Intell. 43Fu, M.; Xu, P.; Li, X.; Liu, Q.; Ye, M.; Zhu, C. Fast crowd density estimation with convolutional neural networks. Eng. Appl. Artif. Intell. 2015, 43, 81-88. [CrossRef]\n\nA large contextual dataset for classification, detection and counting of cars with deep learning. T N Mundhenk, G Konjevod, W A Sakla, K Boakye, Proceedings of the European Conference on Computer Vision. the European Conference on Computer VisionMunich, Germany, 8Mundhenk, T.N.; Konjevod, G.; Sakla, W.A.; Boakye, K. A large contextual dataset for classification, detection and counting of cars with deep learning. In Proceedings of the European Conference on Computer Vision, Munich, Germany, 8-16 October 2016; pp. 785-800.\n\nDeep people counting in extremely dense crowds. C Wang, H Zhang, L Yang, S Liu, X Cao, Proceedings of the 23rd ACM International Conference on Multimedia. the 23rd ACM International Conference on MultimediaBrisbane, AustraliaWang, C.; Zhang, H.; Yang, L.; Liu, S.; Cao, X. Deep people counting in extremely dense crowds. In Proceedings of the 23rd ACM International Conference on Multimedia, Brisbane, Australia, 26-30 October 2015; pp. 1299-1302.\n\nCrossing-line crowd counting with two-phase deep neural networks. Z Zhao, H Li, R Zhao, X Wang, Proceedings of the European Conference on Computer Vision. the European Conference on Computer VisionAmsterdam, The NetherlandsZhao, Z.; Li, H.; Zhao, R.; Wang, X. Crossing-line crowd counting with two-phase deep neural networks. In Proceedings of the European Conference on Computer Vision, Amsterdam, The Netherlands, 11-14 October 2016; pp. 712-726.\n\nDense crowd counting from still images with convolutional neural networks. Y Hu, H Chang, F Nian, Y Wang, T Li, 10.1016/j.jvcir.2016.03.021J. Vis. Commun. Image Represent. 38Hu, Y.; Chang, H.; Nian, F.; Wang, Y.; Li, T. Dense crowd counting from still images with convolutional neural networks. J. Vis. Commun. Image Represent. 2016, 38, 530-539. [CrossRef]\n\nLearning to count with CNN boosting. E Walach, L Wolf, Proceedings of the European Conference on Computer Vision. the European Conference on Computer VisionAmsterdam, The NetherlandsWalach, E.; Wolf, L. Learning to count with CNN boosting. In Proceedings of the European Conference on Computer Vision, Amsterdam, The Netherlands, 11-14 October 2016; pp. 660-676.\n\nCounting Everyday Objects in Everyday Scenes. P Chattopadhyay, R Vedantam, R R Selvaraju, D Batra, D Parikh, Proceedings of the Computer Vision and Pattern Recognition (CVPR). the Computer Vision and Pattern Recognition (CVPR)Honolulu, HI, USAChattopadhyay, P.; Vedantam, R.; Selvaraju, R.R.; Batra, D.; Parikh, D. Counting Everyday Objects in Everyday Scenes. In Proceedings of the Computer Vision and Pattern Recognition (CVPR), Honolulu, HI, USA, 21-26 July 2017; pp. 4428-4437.\n\nAttention to Head Locations for Crowd Counting. Y Zhang, C Zhou, F Chang, A C Kot, arXiv:1806.10287Zhang, Y.; Zhou, C.; Chang, F.; Kot, A.C. Attention to Head Locations for Crowd Counting. arXiv 2018, arXiv:1806.10287.\n\nCSRNet: Dilated convolutional neural networks for understanding the highly congested scenes. Y Li, X Zhang, D Chen, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionSalt Lake City, UT, USALi, Y.; Zhang, X.; Chen, D. CSRNet: Dilated convolutional neural networks for understanding the highly congested scenes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, USA, 19-21 June 2018; pp. 1091-1100.\n\nK Han, W Wan, H Yao, L Hou, arXiv:1706.03686Image Crowd Counting Using Convolutional Neural Network and Markov Random Field. arXiv 2017. Han, K.; Wan, W.; Yao, H.; Hou, L. Image Crowd Counting Using Convolutional Neural Network and Markov Random Field. arXiv 2017, arXiv:1706.03686.\n\n. L Wang, W Shao, Y Lu, H Ye, J Pu, Y Zheng, arXiv:1806.10040Crowd Counting with Density Adaption Networks. Wang, L.; Shao, W.; Lu, Y.; Ye, H.; Pu, J.; Zheng, Y. Crowd Counting with Density Adaption Networks. arXiv 2018, arXiv:1806.10040.\n\nL Liu, H Wang, G Li, W Ouyang, L Lin, arXiv:1807.00601Crowd Counting using Deep Recurrent Spatial-Aware Network. arXiv 2018. Liu, L.; Wang, H.; Li, G.; Ouyang, W.; Lin, L. Crowd Counting using Deep Recurrent Spatial-Aware Network. arXiv 2018, arXiv:1807.00601.\n\nGeometric and Physical Constraints for Head Plane Crowd Density Estimation in Videos. W Liu, K Lis, M Salzmann, P Fua, arXiv:1803.08805Liu, W.; Lis, K.; Salzmann, M.; Fua, P. Geometric and Physical Constraints for Head Plane Crowd Density Estimation in Videos. arXiv 2018, arXiv:1803.08805.\n\nS Huang, X Li, Z Q Cheng, Z Zhang, A Hauptmann, arXiv:1808.07456Stacked Pooling: Improving Crowd Counting by Boosting Scale Invariance. arXiv 2018. Huang, S.; Li, X.; Cheng, Z.Q.; Zhang, Z.; Hauptmann, A. Stacked Pooling: Improving Crowd Counting by Boosting Scale Invariance. arXiv 2018, arXiv:1808.07456.\n\nD Kang, A Chan, arXiv:1805.06115Crowd Counting by Adaptively Fusing Predictions from an Image Pyramid. arXiv 2018. Kang, D.; Chan, A. Crowd Counting by Adaptively Fusing Predictions from an Image Pyramid. arXiv 2018, arXiv:1805.06115.\n\nCrowdnet: A deep convolutional network for dense crowd counting. L Boominathan, S S Kruthiventi, R V Babu, Proceedings of the 2016 ACM on Multimedia Conference. the 2016 ACM on Multimedia ConferenceAmsterdam, The Netherlands15Boominathan, L.; Kruthiventi, S.S.; Babu, R.V. Crowdnet: A deep convolutional network for dense crowd counting. In Proceedings of the 2016 ACM on Multimedia Conference, Amsterdam, The Netherlands, 15-19 October 2016; pp. 640-644.\n\nL Zeng, X Xu, B Cai, S Qiu, T Zhang, arXiv:1702.02359Multi-scale convolutional neural networks for crowd counting. arXiv 2017. Zeng, L.; Xu, X.; Cai, B.; Qiu, S.; Zhang, T. Multi-scale convolutional neural networks for crowd counting. arXiv 2017, arXiv:1702.02359.\n\nMixture of Counting CNNs: Adaptive Integration of CNNs Specialized to Specific Appearance for Crowd Counting. S Kumagai, K Hotta, T Kurita, arXiv:1703.09393Kumagai, S.; Hotta, K.; Kurita, T. Mixture of Counting CNNs: Adaptive Integration of CNNs Specialized to Specific Appearance for Crowd Counting. arXiv 2017, arXiv:1703.09393.\n\nTowards perspective-free object counting with deep learning. D Onoro-Rubio, R J L\u00f3pez-Sastre, Proceedings of the European Conference on Computer Vision. the European Conference on Computer VisionAmsterdam, The Netherlands8Onoro-Rubio, D.; L\u00f3pez-Sastre, R.J. Towards perspective-free object counting with deep learning. In Proceedings of the European Conference on Computer Vision, Amsterdam, The Netherlands, 8-16 October 2016; pp. 615-629.\n\nMultiscale Multitask Deep NetVLAD for Crowd Counting. Z Shi, L Zhang, Y Sun, Y Ye, 10.1109/TII.2018.2852481IEEE Trans. Ind. Inf. 14Shi, Z.; Zhang, L.; Sun, Y.; Ye, Y. Multiscale Multitask Deep NetVLAD for Crowd Counting. IEEE Trans. Ind. Inf. 2018, 14, 4953-4962. [CrossRef]\n\nScale Aggregation Network for Accurate and Efficient Crowd Counting. X Cao, Z Wang, Y Zhao, F Su, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Munich, Germany, 8-Cao, X.; Wang, Z.; Zhao, Y.; Su, F. Scale Aggregation Network for Accurate and Efficient Crowd Counting. In Proceedings of the European Conference on Computer Vision (ECCV), Munich, Germany, 8-14 September 2018; pp. 734-750.\n\nCrowd Counting via Adversarial Cross-Scale Consistency Pursuit. Z Shen, Y Xu, B Ni, M Wang, J Hu, X Yang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionSalt Lake City, UT, USAShen, Z.; Xu, Y.; Ni, B.; Wang, M.; Hu, J.; Yang, X. Crowd Counting via Adversarial Cross-Scale Consistency Pursuit. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, USA, 18-23 June 2018; pp. 5245-5254.\n\nCounting in the wild. C Arteta, V Lempitsky, A Zisserman, Proceedings of the European Conference on Computer Vision. the European Conference on Computer VisionAmsterdam, The Netherlands8Arteta, C.; Lempitsky, V.; Zisserman, A. Counting in the wild. In Proceedings of the European Conference on Computer Vision, Amsterdam, The Netherlands, 8-16 October 2016; pp. 483-498.\n\nComposition loss for counting, density map estimation and localization in dense crowds. H Idrees, M Tayyab, K Athrey, D Zhang, S Al-Maadeed, N Rajpoot, M Shah, Proceedings of the European Conference on Computer Vision. the European Conference on Computer VisionMunich, GermanyIdrees, H.; Tayyab, M.; Athrey, K.; Zhang, D.; Al-Maadeed, S.; Rajpoot, N.; Shah, M. Composition loss for counting, density map estimation and localization in dense crowds. In Proceedings of the European Conference on Computer Vision, Munich, Germany, 8-14 September 2018; pp. 544-559.\n\nPeople counting and pedestrian flow statistics based on convolutional neural network and recurrent neural network. J Zhu, F Feng, B Shen, Proceedings of the 2018 33rd Youth Academic Annual Conference of Chinese Association of Automation (YAC). the 2018 33rd Youth Academic Annual Conference of Chinese Association of Automation (YAC)Nanjing, ChinaZhu, J.; Feng, F.; Shen, B. People counting and pedestrian flow statistics based on convolutional neural network and recurrent neural network. In Proceedings of the 2018 33rd Youth Academic Annual Conference of Chinese Association of Automation (YAC), Nanjing, China, 18-20 May 2018.\n\nBody structure aware deep crowd counting. S Huang, X Li, Z Zhang, F Wu, S Gao, R Ji, J Han, 10.1109/TIP.2017.2740160IEEE Trans. Image Process. 27PubMedHuang, S.; Li, X.; Zhang, Z.; Wu, F.; Gao, S.; Ji, R.; Han, J. Body structure aware deep crowd counting. IEEE Trans. Image Process. 2018, 27, 1049-1059. [CrossRef] [PubMed]\n\nCounting challenging crowds robustly using a multi-column multi-task convolutional neural network. Signal Process. B Yang, J Cao, N Wang, Y Zhang, L Zou, 10.1016/j.image.2018.03.004Image Commun. 64Yang, B.; Cao, J.; Wang, N.; Zhang, Y.; Zou, L. Counting challenging crowds robustly using a multi-column multi-task convolutional neural network. Signal Process. Image Commun. 2018, 64, 118-129. [CrossRef]\n\nLeveraging Unlabeled Data for Crowd Counting by Learning to. X Liu, J Van De Weijer, A D Bagdanov, arXiv:1803.03095Rank. Liu, X.; van de Weijer, J.; Bagdanov, A.D. Leveraging Unlabeled Data for Crowd Counting by Learning to Rank. arXiv 2018, arXiv:1803.03095.\n\nDeep convolutional neural networks for human embryonic cell counting. A Khan, S Gould, M Salzmann, Proceedings of the European Conference on Computer Vision. the European Conference on Computer VisionAmsterdam, The Netherlands8Khan, A.; Gould, S.; Salzmann, M. Deep convolutional neural networks for human embryonic cell counting. In Proceedings of the European Conference on Computer Vision, Amsterdam, The Netherlands, 8-16 October 2016; pp. 339-348.\n\nCounting Plants Using Deep Learning. J Ribera, Y Chen, C Boomsma, E J Delp, Proceedings of the 2017 IEEE Global Conference on Signal and Information Processing. the 2017 IEEE Global Conference on Signal and Information ProcessingMontreal, QC, CanadaRibera, J.; Chen, Y.; Boomsma, C.; Delp, E.J. Counting Plants Using Deep Learning. In Proceedings of the 2017 IEEE Global Conference on Signal and Information Processing, Montreal, QC, Canada, 14-16 November 2017.\n\nUsing Deep Learning for Segmentation and Counting within Microscopy Data. C X Hern\u00e1ndez, M M Sultan, V S Pande, arXiv:1802.10548Hern\u00e1ndez, C.X.; Sultan, M.M.; Pande, V.S. Using Deep Learning for Segmentation and Counting within Microscopy Data. arXiv 2018, arXiv:1802.10548.\n\nMicroscopy cell counting and detection with fully convolutional regression networks. W Xie, J A Noble, A Zisserman, 10.1080/21681163.2016.1149104Comput. Methods Biomech. Biomed. Eng. Imaging Vis. 6Xie, W.; Noble, J.A.; Zisserman, A. Microscopy cell counting and detection with fully convolutional regression networks. Comput. Methods Biomech. Biomed. Eng. Imaging Vis. 2018, 6, 283-292. [CrossRef]\n\nD Kang, D Dhar, A B Chan, arXiv:1611.06748Crowd Counting by Adapting Convolutional Neural Networks with Side Information. arXiv 2016. Kang, D.; Dhar, D.; Chan, A.B. Crowd Counting by Adapting Convolutional Neural Networks with Side Information. arXiv 2016, arXiv:1611.06748.\n\nLearning a perspective-embedded deconvolution network for crowd counting. M Zhao, J Zhang, F Porikli, C Zhang, W Zhang, Proceedings of the 2017 IEEE International Conference on Multimedia and Expo (ICME). the 2017 IEEE International Conference on Multimedia and Expo (ICME)Hong Kong, ChinaZhao, M.; Zhang, J.; Porikli, F.; Zhang, C.; Zhang, W. Learning a perspective-embedded deconvolution network for crowd counting. In Proceedings of the 2017 IEEE International Conference on Multimedia and Expo (ICME), Hong Kong, China, 10-14 July 2017; pp. 403-408.\n\nM Marsden, K Mcguinness, S Little, C E Keogh, N E O&apos;connor, People, arXiv:1711.05586Penguins and Petri Dishes: Adapting Object Counting Models To New Visual Domains And Object Types Without Forgetting. Marsden, M.; McGuinness, K.; Little, S.; Keogh, C.E.; O'Connor, N.E. People, Penguins and Petri Dishes: Adapting Object Counting Models To New Visual Domains And Object Types Without Forgetting. arXiv 2017, arXiv:1711.05586.\n\nCross-scene crowd counting via deep convolutional neural networks. C Zhang, H Li, X Wang, X Yang, Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Boston, MA, USA, 8-10Zhang, C.; Li, H.; Wang, X.; Yang, X. Cross-scene crowd counting via deep convolutional neural networks. In Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Boston, MA, USA, 8-10 June 2015; pp. 833-841.\n\n. M Shi, Z Yang, C Xu, Q Chen, arXiv:1807.01989Perspective-Aware CNN For Crowd Counting. Shi, M.; Yang, Z.; Xu, C.; Chen, Q. Perspective-Aware CNN For Crowd Counting. arXiv 2018, arXiv:1807.01989.\n\nDeep Spatial Regression Model for Image Crowd Counting. H Yao, K Han, W Wan, L Hou, arXiv:1710.09757Yao, H.; Han, K.; Wan, W.; Hou, L. Deep Spatial Regression Model for Image Crowd Counting. arXiv 2017, arXiv:1710.09757.\n\nCount-ception: Counting by fully convolutional redundant counting. J P Cohen, G Boucher, C A Glastonbury, H Z Lo, Y Bengio, Proceedings of the 2017 IEEE International Conference on Computer Vision Workshop (ICCVW). the 2017 IEEE International Conference on Computer Vision Workshop (ICCVW)Venice, ItalyCohen, J.P.; Boucher, G.; Glastonbury, C.A.; Lo, H.Z.; Bengio, Y. Count-ception: Counting by fully convolutional redundant counting. In Proceedings of the 2017 IEEE International Conference on Computer Vision Workshop (ICCVW), Venice, Italy, 22-29 October 2017; pp. 18-26.\n\nDecidenet: Counting varying density crowds through attention guided detection and density estimation. J Liu, C Gao, D Meng, A G Hauptmann, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionSalt Lake City, UT, USALiu, J.; Gao, C.; Meng, D.; Hauptmann, A.G. Decidenet: Counting varying density crowds through attention guided detection and density estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, USA, 18-23 June 2018; pp. 5197-5206.\n\nLearning Short-Cut Connections for Object Counting. D O\u00f1oro-Rubio, M Niepert, R J L\u00f3pez-Sastre, arXiv:1805.02919O\u00f1oro-Rubio, D.; Niepert, M.; L\u00f3pez-Sastre, R.J. Learning Short-Cut Connections for Object Counting. arXiv 2018, arXiv:1805.02919.\n\nDepth Information Guided Crowd Counting for. M Xu, Z Ge, X Jiang, G Cui, P Lv, B Zhou, arXiv:1803.02256Complex Crowd Scenes. Xu, M.; Ge, Z.; Jiang, X.; Cui, G.; Lv, P.; Zhou, B. Depth Information Guided Crowd Counting for Complex Crowd Scenes. arXiv 2018, arXiv:1803.02256.\n\nPeople Counting in Dense Crowd Images using Sparse Head Detections. M Shami, S Maqbool, H Sajid, Y Ayaz, S C S Cheung, 10.1109/TCSVT.2018.2803115IEEE Trans. Circuits Syst. Video Technol. Shami, M.; Maqbool, S.; Sajid, H.; Ayaz, Y.; Cheung, S.C.S. People Counting in Dense Crowd Images using Sparse Head Detections. IEEE Trans. Circuits Syst. Video Technol. 2018. [CrossRef]\n\nAuxiliary learning for crowd counting via count-net. Y Zhang, F Chang, M Wang, F Zhang, C Han, 10.1016/j.neucom.2017.08.018Neurocomputing. 273Zhang, Y.; Chang, F.; Wang, M.; Zhang, F.; Han, C. Auxiliary learning for crowd counting via count-net. Neurocomputing 2018, 273, 190-198. [CrossRef]\n\nSingle-image crowd counting via multi-column convolutional neural network. Y Zhang, D Zhou, S Chen, S Gao, Y Ma, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionLas Vegas, NV, USAZhang, Y.; Zhou, D.; Chen, S.; Gao, S.; Ma, Y. Single-image crowd counting via multi-column convolutional neural network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, NV, USA, 26 June-1 July 2016; pp. 589-597.\n\nSkip-connection convolutional neural network for still image crowd counting. L Wang, B Yin, A Guo, H Ma, J Cao, 10.1007/s10489-018-1150-1Appl. Intell. 48Wang, L.; Yin, B.; Guo, A.; Ma, H.; Cao, J. Skip-connection convolutional neural network for still image crowd counting. Appl. Intell. 2018, 48, 3360-3371. [CrossRef]\n\nSwitching convolutional neural network for crowd counting. D B Sam, S Surya, R V Babu, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionHonolulu, HI, USASam, D.B.; Surya, S.; Babu, R.V. Switching convolutional neural network for crowd counting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, HI, USA, 21-26 July 2017;\n\nDeep count: Fruit counting based on deep simulated learning. M Rahnemoonfar, C Sheppard, 10.3390/s1704090517905Rahnemoonfar, M.; Sheppard, C. Deep count: Fruit counting based on deep simulated learning. Sensors 2017, 17, 905. [CrossRef]\n\nCrowd counting via weighted vlad on dense attribute feature maps. B Sheng, C Shen, G Lin, J Li, W Yang, C Sun, 10.1109/TCSVT.2016.2637379IEEE Trans. Circuits Syst. Video Technol. 28Sheng, B.; Shen, C.; Lin, G.; Li, J.; Yang, W.; Sun, C. Crowd counting via weighted vlad on dense attribute feature maps. IEEE Trans. Circuits Syst. Video Technol. 2016, 28, 1788-1797. [CrossRef]\n\nResnetCrowd: A residual deep learning architecture for crowd counting, violent behaviour detection and crowd density level classification. M Marsden, K Mcguinness, S Little, N E O&apos;connor, Proceedings of the 2017 14th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS). the 2017 14th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)Lecce, ItalyMarsden, M.; McGuinness, K.; Little, S.; O'Connor, N.E. ResnetCrowd: A residual deep learning architecture for crowd counting, violent behaviour detection and crowd density level classification. In Proceedings of the 2017 14th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS), Lecce, Italy, 29 August-1 September 2017; pp. 1-7.\n\nM Marsden, K Mcguiness, S Little, N E O&apos;connor, arXiv:1612.00220Fully convolutional crowd counting on highly congested scenes. arXiv 2016. Marsden, M.; McGuiness, K.; Little, S.; O'Connor, N.E. Fully convolutional crowd counting on highly congested scenes. arXiv 2016, arXiv:1612.00220.\n\nCnn-based cascaded multi-task learning of high-level prior and density estimation for crowd counting. V A Sindagi, V M Patel, Proceedings of the 2017 14th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS). the 2017 14th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)Lecce, ItalySindagi, V.A.; Patel, V.M. Cnn-based cascaded multi-task learning of high-level prior and density estimation for crowd counting. In Proceedings of the 2017 14th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS), Lecce, Italy, 29 August-1 September 2017; pp. 1-6.\n\nBusyness Defection and Notification Method and System. M C Mongeon, R P Loce, M A Shreve, U.S. Patent. 9Mongeon, M.C.; Loce, R.P.; Shreve, M.A. Busyness Defection and Notification Method and System. U.S. Patent 9,576,371, 21 February 2017.\n\nTraining deep networks for facial expression recognition with crowd-sourced label distribution. E Barsoum, C Zhang, C C Ferrer, Z Zhang, Proceedings of the 18th ACM International Conference on Multimodal Interaction. the 18th ACM International Conference on Multimodal InteractionTokyo, JapanBarsoum, E.; Zhang, C.; Ferrer, C.C.; Zhang, Z. Training deep networks for facial expression recognition with crowd-sourced label distribution. In Proceedings of the 18th ACM International Conference on Multimodal Interaction, Tokyo, Japan, 12-16 November 2016; pp. 279-283.\n\nUsing convolutional networks and satellite imagery to identify patterns in urban environments at a large scale. A Albert, J Kaur, M C Gonzalez, Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data MiningHalifax, NS, CanadaAlbert, A.; Kaur, J.; Gonzalez, M.C. Using convolutional networks and satellite imagery to identify patterns in urban environments at a large scale. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, NS, Canada, 13-17 August 2017; pp. 1357-1366.\n\nDetecting mammals in UAV images: Best practices to address a substantially imbalanced dataset with deep learning. B Kellenberger, D Marcos, D Tuia, 10.1016/j.rse.2018.06.028Remote Sens. Environ. 216Kellenberger, B.; Marcos, D.; Tuia, D. Detecting mammals in UAV images: Best practices to address a substantially imbalanced dataset with deep learning. Remote Sens. Environ. 2018, 216, 139-153. [CrossRef]\n\ncitizen sensing and sensor web technologies for public and environmental health surveillance and crisis management: Trends, OGC standards and application examples. M N K Boulos, B Resch, D N Crowley, J G Breslin, G Sohn, R Burtner, W A Pike, E Jezierski, K Y S Chuang, Crowdsourcing, 10.1186/1476-072X-10-67Int. J. Health Geogr. 10Boulos, M.N.K.; Resch, B.; Crowley, D.N.; Breslin, J.G.; Sohn, G.; Burtner, R.; Pike, W.A.; Jezierski, E.; Chuang, K.Y.S. Crowdsourcing, citizen sensing and sensor web technologies for public and environmental health surveillance and crisis management: Trends, OGC standards and application examples. Int. J. Health Geogr. 2011, 10, 67. [CrossRef]\n\nTraffic flow prediction with big data: A deep learning approach. Y Lv, Y Duan, W Kang, Z Li, F Y Wang, 10.1109/TITS.2014.2345663IEEE Trans. Intell. Transp. Syst. 16Lv, Y.; Duan, Y.; Kang, W.; Li, Z.; Wang, F.Y. Traffic flow prediction with big data: A deep learning approach. IEEE Trans. Intell. Transp. Syst. 2015, 16, 865-873. [CrossRef]\n\nTracking the untrackable: Learning to track multiple cues with long-term dependencies. A Sadeghian, A Alahi, S Savarese, arXiv:1701.01909Sadeghian, A.; Alahi, A.; Savarese, S. Tracking the untrackable: Learning to track multiple cues with long-term dependencies. arXiv 2017, arXiv:1701.01909.\n\nTask-based crowd simulation for heterogeneous architectures. In Innovative Research and Applications in Next-Generation High Performance Computing. H Perez, B Hernandez, I Rudomin, E Ayguade, IGI GlobalHershey, PA, USAPerez, H.; Hernandez, B.; Rudomin, I.; Ayguade, E. Task-based crowd simulation for heterogeneous architectures. In Innovative Research and Applications in Next-Generation High Performance Computing; IGI Global: Hershey, PA, USA, 2016; pp. 194-219.\n\nPedestrian monitoring techniques for crowd-flow prediction. C Martani, S Stent, S Acikgoz, K Soga, D Bain, Y Jin, 10.1680/jsmic.17.00001Proc. Inst. Civ. Eng. 170-Smart InfrastructMartani, C.; Stent, S.; Acikgoz, S.; Soga, K.; Bain, D.; Jin, Y. Pedestrian monitoring techniques for crowd-flow prediction. Proc. Inst. Civ. Eng.-Smart Infrastruct. Constr. 2017, 170, 17-27. [CrossRef]\n\nDisaster management in real time simulation using machine learning. M Khouj, C Lopez, S Sarkaria, J Marti, Proceedings of the 2011 24th Canadian Conference on Electrical and Computer Engineering (CCECE). the 2011 24th Canadian Conference on Electrical and Computer Engineering (CCECE)Niagara Falls, ON, CanadaKhouj, M.; Lopez, C.; Sarkaria, S.; Marti, J. Disaster management in real time simulation using machine learning. In Proceedings of the 2011 24th Canadian Conference on Electrical and Computer Engineering (CCECE), Niagara Falls, ON, Canada, 8-11 May 2011; pp. 001507-001510.\n\nThe effectiveness of face detection algorithms in unconstrained crowd scenes. J R Barr, K W Bowyer, P J Flynn, Proceedings of the 2014 IEEE Winter Conference on Applications of Computer Vision (WACV). the 2014 IEEE Winter Conference on Applications of Computer Vision (WACV)Steamboat Springs, CO, USABarr, J.R.; Bowyer, K.W.; Flynn, P.J. The effectiveness of face detection algorithms in unconstrained crowd scenes. In Proceedings of the 2014 IEEE Winter Conference on Applications of Computer Vision (WACV), Steamboat Springs, CO, USA, 24-26 March 2014; pp. 1020-1027.\n\nDeep learning for emotion recognition on small datasets using transfer learning. H W Ng, V D Nguyen, V Vonikakis, S Winkler, Proceedings of the 2015 ACM on International Conference on Multimodal Interaction. the 2015 ACM on International Conference on Multimodal InteractionSeattle, WA, USA, 9Ng, H.W.; Nguyen, V.D.; Vonikakis, V.; Winkler, S. Deep learning for emotion recognition on small datasets using transfer learning. In Proceedings of the 2015 ACM on International Conference on Multimodal Interaction, Seattle, WA, USA, 9-13 November 2015; pp. 443-449.\n\nImagenet: A large-scale hierarchical image database. J Deng, W Dong, R Socher, L J Li, K Li, L Fei-Fei, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionMiami Beach, FL, USADeng, J.; Dong, W.; Socher, R.; Li, L.J.; Li, K.; Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Miami Beach, FL, USA, 20-25 June 2009; pp. 248-255.\n\nIntelligent Crime Anomaly Detection in Smart Cities Using Deep Learning. S Chackravarthy, S Schmitt, L Yang, Proceedings of the 2018 IEEE 4th International Conference on Collaboration and Internet Computing (CIC). the 2018 IEEE 4th International Conference on Collaboration and Internet Computing (CIC)Philadelphia, PA, USAChackravarthy, S.; Schmitt, S.; Yang, L. Intelligent Crime Anomaly Detection in Smart Cities Using Deep Learning. In Proceedings of the 2018 IEEE 4th International Conference on Collaboration and Internet Computing (CIC), Philadelphia, PA, USA, 18-20 October 2018; pp. 399-404.\n\nDeep learning for automatic cell detection in wide-field microscopy zebrafish images. B Dong, L Shao, M Da Costa, O Bandmann, A F Frangi, Proceedings of the 2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI). the 2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)New York, NY, USADong, B.; Shao, L.; Da Costa, M.; Bandmann, O.; Frangi, A.F. Deep learning for automatic cell detection in wide-field microscopy zebrafish images. In Proceedings of the 2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI), New York, NY, USA, 16-19 April 2015; pp. 772-776.\n\nDeep learning as a tool for increased accuracy and efficiency of histopathological diagnosis. G Litjens, C I S\u00e1nchez, N Timofeeva, M Hermsen, I Nagtegaal, I Kovacs, C Hulsbergen-Van De Kaa, P Bult, B Van Ginneken, J Van Der Laak, 10.1038/srep26286Sci. Rep. PubMedLitjens, G.; S\u00e1nchez, C.I.; Timofeeva, N.; Hermsen, M.; Nagtegaal, I.; Kovacs, I.; Hulsbergen-Van De Kaa, C.; Bult, P.; Van Ginneken, B.; Van Der Laak, J. Deep learning as a tool for increased accuracy and efficiency of histopathological diagnosis. Sci. Rep. 2016, 6, 26286. [CrossRef] [PubMed]\n\nDermatologist-level classification of skin cancer with deep neural networks. A Esteva, B Kuprel, R A Novoa, J Ko, S M Swetter, H M Blau, S Thrun, 10.1038/nature21056Nature. 542Esteva, A.; Kuprel, B.; Novoa, R.A.; Ko, J.; Swetter, S.M.; Blau, H.M.; Thrun, S. Dermatologist-level classification of skin cancer with deep neural networks. Nature 2017, 542, 115. [CrossRef]\n\nAn automatic computer-aided diagnosis system for liver tumours on computed tomography images. S Kumar, R Moni, J Rajeesh, 10.1016/j.compeleceng.2013.02.008Comput. Electr. Eng. 39Kumar, S.; Moni, R.; Rajeesh, J. An automatic computer-aided diagnosis system for liver tumours on computed tomography images. Comput. Electr. Eng. 2013, 39, 1516-1526. [CrossRef]\n\nLearning collective crowd behaviors with dynamic pedestrian-agents. B Zhou, X Tang, X Wang, 10.1007/s11263-014-0735-3Int. J. Comput. Vis. 111Zhou, B.; Tang, X.; Wang, X. Learning collective crowd behaviors with dynamic pedestrian-agents. Int. J. Comput. Vis. 2015, 111, 50-68. [CrossRef]\n\nMulti-Camera Crowd Monitoring: The SAFEST Approach. A Danilkina, G Allard, E Baccelli, G Bartl, F Gendry, O Hahm, G Hege, U Kriegel, M Palkow, H Petersen, Proceedings of the Workshop Interdisciplinaire sur la S\u00e9curit\u00e9 Globale. the Workshop Interdisciplinaire sur la S\u00e9curit\u00e9 GlobaleInstitut, ParisDanilkina, A.; Allard, G.; Baccelli, E.; Bartl, G.; Gendry, F.; Hahm, O.; Hege, G.; Kriegel, U.; Palkow, M.; Petersen, H.; et al. Multi-Camera Crowd Monitoring: The SAFEST Approach. In Proceedings of the Workshop Interdisciplinaire sur la S\u00e9curit\u00e9 Globale, Institut, Paris, 3-4 February 2015.\n\nReal-time monitoring for crowd counting using video surveillance and GIS. H Song, X Liu, X Zhang, J Hu, Proceedings of the 2012 2nd International Conference on Remote Sensing. the 2012 2nd International Conference on Remote SensingNanjing, ChinaSong, H.; Liu, X.; Zhang, X.; Hu, J. Real-time monitoring for crowd counting using video surveillance and GIS. In Proceedings of the 2012 2nd International Conference on Remote Sensing, Environment and Transportation Engineering (RSETE), Nanjing, China, 1-3 June 2012; pp. 1-4.\n\nReal-time crowd motion analysis. N Ihaddadene, C Djeraba, Proceedings of the 19th International Conference on Pattern Recognition. the 19th International Conference on Pattern RecognitionTampa, FL, USA, 8-Ihaddadene, N.; Djeraba, C. Real-time crowd motion analysis. In Proceedings of the 19th International Conference on Pattern Recognition, Tampa, FL, USA, 8-11 December 2008; pp. 1-4.\n\nImagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, Proceedings of the Advances in Neural Information Processing Systems. the Advances in Neural Information Processing SystemsLake Tahoe, NV, USAKrizhevsky, A.; Sutskever, I.; Hinton, G.E. Imagenet classification with deep convolutional neural networks. In Proceedings of the Advances in Neural Information Processing Systems, Lake Tahoe, NV, USA, 3-6 December 2012; pp. 1097-1105.\n\nState-space model with deep learning for functional dynamics estimation in resting-state fMRI. H I Suk, C Y Wee, S W Lee, D Shen, 10.1016/j.neuroimage.2016.01.005NeuroImage. 129Suk, H.I.; Wee, C.Y.; Lee, S.W.; Shen, D. State-space model with deep learning for functional dynamics estimation in resting-state fMRI. NeuroImage 2016, 129, 292-307. [CrossRef]\n\nSegmentation of white matter hyperintensities using convolutional neural networks with global spatial information in routine clinical brain MRI with none or mild vascular pathology. M F Rachmadi, M D C Vald\u00e9s-Hern\u00e1ndez, M L F Agan, C Di Perri, T Komura, A D N Initiative, 10.1016/j.compmedimag.2018.02.002Comput. Med. Imaging Graph. 66Rachmadi, M.F.; Vald\u00e9s-Hern\u00e1ndez, M.d.C.; Agan, M.L.F.; Di Perri, C.; Komura, T.; Initiative, A.D.N.; et al. Segmentation of white matter hyperintensities using convolutional neural networks with global spatial information in routine clinical brain MRI with none or mild vascular pathology. Comput. Med. Imaging Graph. 2018, 66, 28-43. [CrossRef]\n\nA global geometric framework for nonlinear dimensionality reduction. J B Tenenbaum, V Silva, J C Langford, 10.1126/science.290.5500.2319Science. 290Tenenbaum, J.B.; De Silva, V.; Langford, J.C. A global geometric framework for nonlinear dimensionality reduction. Science 2000, 290, 2319-2323. [CrossRef]\n\nLaplacian eigenmaps for dimensionality reduction and data representation. M Belkin, P Niyogi, 10.1162/089976603321780317Neural Comput. 15Belkin, M.; Niyogi, P. Laplacian eigenmaps for dimensionality reduction and data representation. Neural Comput. 2003, 15, 1373-1396. [CrossRef]\n\nPrincipal component analysis. S Wold, K Esbensen, P Geladi, 10.1016/0169-7439(87)80084-9Chemom. Intell. Lab. Syst. 2Wold, S.; Esbensen, K.; Geladi, P. Principal component analysis. Chemom. Intell. Lab. Syst. 1987, 2, 37-52. [CrossRef]\n\nA comparison of PCA, KPCA and ICA for dimensionality reduction in support vector machine. L Cao, K S Chua, W Chong, H Lee, Q Gu, 10.1016/S0925-2312(03)00433-8Neurocomputing. 55Cao, L.; Chua, K.S.; Chong, W.; Lee, H.; Gu, Q. A comparison of PCA, KPCA and ICA for dimensionality reduction in support vector machine. Neurocomputing 2003, 55, 321-336. [CrossRef]\n\nNonlinear dimensionality reduction by locally linear embedding. S T Roweis, L K Saul, 10.1126/science.290.5500.2323Science. 290PubMedRoweis, S.T.; Saul, L.K. Nonlinear dimensionality reduction by locally linear embedding. Science 2000, 290, 2323-2326. [CrossRef] [PubMed]\n\nMultimodal deep learning. J Ngiam, A Khosla, M Kim, J Nam, H Lee, A Y Ng, Proceedings of the 28th International Conference on Machine Learning (ICML-11). the 28th International Conference on Machine Learning (ICML-11)Bellevue, WA, USANgiam, J.; Khosla, A.; Kim, M.; Nam, J.; Lee, H.; Ng, A.Y. Multimodal deep learning. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), Bellevue, WA, USA, 28 June-2 July 2011; pp. 689-696.\n\nLearning Motion Features for Example-Based Finger Motion Estimation for Virtual Characters. C Mousas, C N Anagnostopoulos, 10.1007/s13319-017-0136-93D Res. 2017, 8, 25. [CrossRefMousas, C.; Anagnostopoulos, C.N. Learning Motion Features for Example-Based Finger Motion Estimation for Virtual Characters. 3D Res. 2017, 8, 25. [CrossRef]\n\nLearning Sparse Feature Representations for Music Annotation and Retrieval. J Nam, J Herrera, M Slaney, J O Smith, Proceedings of the ISMIR. the ISMIRPorto, PortugalNam, J.; Herrera, J.; Slaney, M.; Smith, J.O. Learning Sparse Feature Representations for Music Annotation and Retrieval. In Proceedings of the ISMIR, Porto, Portugal, 8-12 October 2012; pp. 565-570.\n\nF N Iandola, S Han, M W Moskewicz, K Ashraf, W J Dally, K Keutzer, Squeezenet, arXiv:1602.07360AlexNet-level accuracy with 50x fewer parameters and< 0.5 MB model size. arXiv 2016. Basel, Switzerlandc 2019 by the authors. Licensee MDPI. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) licenseIandola, F.N.; Han, S.; Moskewicz, M.W.; Ashraf, K.; Dally, W.J.; Keutzer, K. SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and< 0.5 MB model size. arXiv 2016, arXiv:1602.07360. c 2019 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (http://creativecommons.org/licenses/by/4.0/).\n", "annotations": {"author": "[{\"end\":308,\"start\":148},{\"end\":574,\"start\":309},{\"end\":710,\"start\":575}]", "publisher": null, "author_last_name": "[{\"end\":160,\"start\":155},{\"end\":322,\"start\":315},{\"end\":585,\"start\":582}]", "author_first_name": "[{\"end\":154,\"start\":148},{\"end\":314,\"start\":309},{\"end\":581,\"start\":575}]", "author_affiliation": "[{\"end\":307,\"start\":185},{\"end\":573,\"start\":387},{\"end\":709,\"start\":587}]", "title": "[{\"end\":118,\"start\":1},{\"end\":828,\"start\":711}]", "venue": null, "abstract": "[{\"end\":2390,\"start\":982}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2764,\"start\":2761},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2767,\"start\":2764},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2770,\"start\":2767},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2785,\"start\":2782},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2788,\"start\":2785},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2791,\"start\":2788},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2794,\"start\":2791},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2797,\"start\":2794},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2821,\"start\":2818},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2824,\"start\":2821},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2844,\"start\":2840},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2848,\"start\":2844},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2852,\"start\":2848},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2856,\"start\":2852},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3110,\"start\":3106},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3114,\"start\":3110},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3118,\"start\":3114},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3315,\"start\":3311},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3319,\"start\":3315},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3323,\"start\":3319},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3327,\"start\":3323},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3331,\"start\":3327},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4365,\"start\":4361},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4766,\"start\":4762},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":5168,\"start\":5164},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5315,\"start\":5311},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5604,\"start\":5600},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5735,\"start\":5731},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":5940,\"start\":5936},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":5943,\"start\":5940},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6923,\"start\":6919},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6927,\"start\":6923},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6931,\"start\":6927},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6935,\"start\":6931},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7986,\"start\":7982},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":10254,\"start\":10250},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":10257,\"start\":10254},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":10896,\"start\":10892},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":10899,\"start\":10896},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":11251,\"start\":11247},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":11460,\"start\":11456},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":11463,\"start\":11460},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11695,\"start\":11692},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11851,\"start\":11847},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":11855,\"start\":11851},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":11859,\"start\":11855},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":11863,\"start\":11859},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11987,\"start\":11983},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":12001,\"start\":11997},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":12021,\"start\":12017},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":12282,\"start\":12278},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":12285,\"start\":12282},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":12409,\"start\":12405},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":12594,\"start\":12590},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":12597,\"start\":12594},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":12823,\"start\":12820},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":13370,\"start\":13367},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":13752,\"start\":13749},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":13755,\"start\":13752},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":14134,\"start\":14130},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":14150,\"start\":14146},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":14176,\"start\":14173},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":14190,\"start\":14186},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":14632,\"start\":14628},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":14957,\"start\":14953},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":15436,\"start\":15432},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":15445,\"start\":15441},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":15613,\"start\":15609},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":15714,\"start\":15710},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":15896,\"start\":15892},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":15923,\"start\":15919},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":16142,\"start\":16138},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":16145,\"start\":16142},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":16148,\"start\":16145},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":16329,\"start\":16325},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":16655,\"start\":16651},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":16769,\"start\":16765},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":17328,\"start\":17324},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":17559,\"start\":17555},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":17563,\"start\":17559},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":17567,\"start\":17563},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":17571,\"start\":17567},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":17722,\"start\":17718},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":17725,\"start\":17722},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":18197,\"start\":18193},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":18383,\"start\":18379},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":18761,\"start\":18757},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":19105,\"start\":19101},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":19322,\"start\":19318},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":19455,\"start\":19452},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":19458,\"start\":19455},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":20244,\"start\":20240},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":20248,\"start\":20244},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":20252,\"start\":20248},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":20382,\"start\":20378},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":20385,\"start\":20382},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":20586,\"start\":20582},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":20634,\"start\":20630},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":22012,\"start\":22008},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":22253,\"start\":22249},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":22480,\"start\":22477},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":23109,\"start\":23105},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":23855,\"start\":23851},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":24790,\"start\":24787},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":24792,\"start\":24790},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":24795,\"start\":24792},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":24798,\"start\":24795},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":25347,\"start\":25343},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":26923,\"start\":26919},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":29659,\"start\":29655},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":30101,\"start\":30097},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":30308,\"start\":30304},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":30471,\"start\":30467},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":30722,\"start\":30718},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":31140,\"start\":31136},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":32877,\"start\":32873},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":33036,\"start\":33032},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":33228,\"start\":33224},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":33489,\"start\":33485},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":33949,\"start\":33945},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":34433,\"start\":34429},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":35944,\"start\":35940},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":36101,\"start\":36097},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":36354,\"start\":36350},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":36642,\"start\":36638},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":36863,\"start\":36859},{\"attributes\":{\"ref_id\":\"b88\"},\"end\":37135,\"start\":37131},{\"attributes\":{\"ref_id\":\"b89\"},\"end\":37388,\"start\":37384},{\"attributes\":{\"ref_id\":\"b90\"},\"end\":37781,\"start\":37777},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":38132,\"start\":38128},{\"attributes\":{\"ref_id\":\"b92\"},\"end\":38515,\"start\":38511},{\"attributes\":{\"ref_id\":\"b93\"},\"end\":40166,\"start\":40162},{\"attributes\":{\"ref_id\":\"b94\"},\"end\":40430,\"start\":40426},{\"attributes\":{\"ref_id\":\"b95\"},\"end\":40665,\"start\":40661},{\"attributes\":{\"ref_id\":\"b96\"},\"end\":40955,\"start\":40951},{\"attributes\":{\"ref_id\":\"b97\"},\"end\":41338,\"start\":41334},{\"attributes\":{\"ref_id\":\"b98\"},\"end\":41740,\"start\":41736},{\"attributes\":{\"ref_id\":\"b99\"},\"end\":43470,\"start\":43466},{\"attributes\":{\"ref_id\":\"b100\"},\"end\":43758,\"start\":43754},{\"attributes\":{\"ref_id\":\"b101\"},\"end\":44066,\"start\":44061},{\"attributes\":{\"ref_id\":\"b102\"},\"end\":44442,\"start\":44437},{\"attributes\":{\"ref_id\":\"b103\"},\"end\":45612,\"start\":45607},{\"attributes\":{\"ref_id\":\"b104\"},\"end\":45812,\"start\":45807},{\"attributes\":{\"ref_id\":\"b105\"},\"end\":46495,\"start\":46490},{\"attributes\":{\"ref_id\":\"b106\"},\"end\":47091,\"start\":47086},{\"attributes\":{\"ref_id\":\"b107\"},\"end\":47427,\"start\":47422},{\"attributes\":{\"ref_id\":\"b108\"},\"end\":47822,\"start\":47817},{\"attributes\":{\"ref_id\":\"b109\"},\"end\":49823,\"start\":49818},{\"attributes\":{\"ref_id\":\"b110\"},\"end\":50216,\"start\":50211},{\"attributes\":{\"ref_id\":\"b111\"},\"end\":50421,\"start\":50416},{\"attributes\":{\"ref_id\":\"b110\"},\"end\":50961,\"start\":50956},{\"attributes\":{\"ref_id\":\"b112\"},\"end\":50978,\"start\":50973},{\"attributes\":{\"ref_id\":\"b113\"},\"end\":51383,\"start\":51378},{\"attributes\":{\"ref_id\":\"b114\"},\"end\":51786,\"start\":51781},{\"attributes\":{\"ref_id\":\"b115\"},\"end\":51991,\"start\":51986},{\"attributes\":{\"ref_id\":\"b116\"},\"end\":52352,\"start\":52347},{\"attributes\":{\"ref_id\":\"b117\"},\"end\":52890,\"start\":52885},{\"attributes\":{\"ref_id\":\"b118\"},\"end\":54000,\"start\":53995},{\"attributes\":{\"ref_id\":\"b119\"},\"end\":54322,\"start\":54317},{\"attributes\":{\"ref_id\":\"b120\"},\"end\":54778,\"start\":54773},{\"attributes\":{\"ref_id\":\"b121\"},\"end\":54987,\"start\":54982},{\"attributes\":{\"ref_id\":\"b122\"},\"end\":55301,\"start\":55296},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":56026,\"start\":56022},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":56647,\"start\":56644},{\"attributes\":{\"ref_id\":\"b123\"},\"end\":56651,\"start\":56647},{\"attributes\":{\"ref_id\":\"b124\"},\"end\":56655,\"start\":56651},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":56937,\"start\":56933},{\"attributes\":{\"ref_id\":\"b125\"},\"end\":56941,\"start\":56937},{\"attributes\":{\"ref_id\":\"b126\"},\"end\":56945,\"start\":56941},{\"attributes\":{\"ref_id\":\"b127\"},\"end\":57319,\"start\":57314},{\"attributes\":{\"ref_id\":\"b128\"},\"end\":57324,\"start\":57319},{\"attributes\":{\"ref_id\":\"b129\"},\"end\":57329,\"start\":57324},{\"attributes\":{\"ref_id\":\"b130\"},\"end\":57855,\"start\":57850},{\"attributes\":{\"ref_id\":\"b131\"},\"end\":57860,\"start\":57855},{\"attributes\":{\"ref_id\":\"b132\"},\"end\":57865,\"start\":57860},{\"attributes\":{\"ref_id\":\"b133\"},\"end\":58218,\"start\":58213},{\"attributes\":{\"ref_id\":\"b134\"},\"end\":58223,\"start\":58218},{\"attributes\":{\"ref_id\":\"b135\"},\"end\":58228,\"start\":58223},{\"attributes\":{\"ref_id\":\"b136\"},\"end\":58233,\"start\":58228},{\"attributes\":{\"ref_id\":\"b137\"},\"end\":58499,\"start\":58494},{\"attributes\":{\"ref_id\":\"b138\"},\"end\":58679,\"start\":58674},{\"attributes\":{\"ref_id\":\"b139\"},\"end\":58879,\"start\":58874},{\"attributes\":{\"ref_id\":\"b140\"},\"end\":58973,\"start\":58968},{\"attributes\":{\"ref_id\":\"b141\"},\"end\":59327,\"start\":59322},{\"attributes\":{\"ref_id\":\"b142\"},\"end\":59362,\"start\":59357},{\"attributes\":{\"ref_id\":\"b143\"},\"end\":59471,\"start\":59466},{\"attributes\":{\"ref_id\":\"b144\"},\"end\":59475,\"start\":59471},{\"attributes\":{\"ref_id\":\"b145\"},\"end\":59878,\"start\":59873},{\"attributes\":{\"ref_id\":\"b146\"},\"end\":59882,\"start\":59878},{\"attributes\":{\"ref_id\":\"b147\"},\"end\":59905,\"start\":59900},{\"attributes\":{\"ref_id\":\"b148\"},\"end\":60596,\"start\":60591},{\"attributes\":{\"ref_id\":\"b149\"},\"end\":60600,\"start\":60596},{\"attributes\":{\"ref_id\":\"b150\"},\"end\":60871,\"start\":60866},{\"attributes\":{\"ref_id\":\"b151\"},\"end\":61011,\"start\":61006},{\"attributes\":{\"ref_id\":\"b152\"},\"end\":61424,\"start\":61419},{\"attributes\":{\"ref_id\":\"b153\"},\"end\":61913,\"start\":61908},{\"attributes\":{\"ref_id\":\"b154\"},\"end\":61918,\"start\":61913},{\"attributes\":{\"ref_id\":\"b155\"},\"end\":61923,\"start\":61918},{\"attributes\":{\"ref_id\":\"b156\"},\"end\":61928,\"start\":61923},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":62368,\"start\":62364},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":62372,\"start\":62368},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":62376,\"start\":62372},{\"attributes\":{\"ref_id\":\"b110\"},\"end\":62389,\"start\":62384},{\"attributes\":{\"ref_id\":\"b111\"},\"end\":62394,\"start\":62389},{\"attributes\":{\"ref_id\":\"b112\"},\"end\":62399,\"start\":62394},{\"attributes\":{\"ref_id\":\"b113\"},\"end\":62404,\"start\":62399},{\"attributes\":{\"ref_id\":\"b114\"},\"end\":62409,\"start\":62404},{\"attributes\":{\"ref_id\":\"b115\"},\"end\":62414,\"start\":62409},{\"attributes\":{\"ref_id\":\"b116\"},\"end\":62419,\"start\":62414},{\"attributes\":{\"ref_id\":\"b117\"},\"end\":62424,\"start\":62419},{\"attributes\":{\"ref_id\":\"b120\"},\"end\":62516,\"start\":62511},{\"attributes\":{\"ref_id\":\"b121\"},\"end\":62521,\"start\":62516},{\"attributes\":{\"ref_id\":\"b122\"},\"end\":62526,\"start\":62521},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":62656,\"start\":62652},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":62696,\"start\":62692},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":62894,\"start\":62890},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":62932,\"start\":62928},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":63050,\"start\":63046},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":63122,\"start\":63118},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":63160,\"start\":63156},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":63548,\"start\":63544},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":63587,\"start\":63583},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":63591,\"start\":63587},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":63595,\"start\":63591},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":63599,\"start\":63595},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":63808,\"start\":63804},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":63835,\"start\":63831},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":63874,\"start\":63870},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":63877,\"start\":63874},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":63880,\"start\":63877},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":64101,\"start\":64097},{\"attributes\":{\"ref_id\":\"b92\"},\"end\":64623,\"start\":64619},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":64672,\"start\":64668},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":64676,\"start\":64672},{\"attributes\":{\"ref_id\":\"b88\"},\"end\":64680,\"start\":64676},{\"attributes\":{\"ref_id\":\"b89\"},\"end\":64684,\"start\":64680},{\"attributes\":{\"ref_id\":\"b90\"},\"end\":64688,\"start\":64684},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":64692,\"start\":64688},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":64890,\"start\":64886},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":64939,\"start\":64935},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":64942,\"start\":64939},{\"attributes\":{\"ref_id\":\"b90\"},\"end\":64945,\"start\":64942},{\"attributes\":{\"ref_id\":\"b92\"},\"end\":64948,\"start\":64945},{\"attributes\":{\"ref_id\":\"b92\"},\"end\":65150,\"start\":65146},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":65199,\"start\":65195},{\"attributes\":{\"ref_id\":\"b92\"},\"end\":65268,\"start\":65264},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":65323,\"start\":65319},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":65480,\"start\":65476},{\"attributes\":{\"ref_id\":\"b98\"},\"end\":65887,\"start\":65883},{\"attributes\":{\"ref_id\":\"b95\"},\"end\":65936,\"start\":65932},{\"attributes\":{\"ref_id\":\"b96\"},\"end\":65940,\"start\":65936},{\"attributes\":{\"ref_id\":\"b97\"},\"end\":65944,\"start\":65940},{\"attributes\":{\"ref_id\":\"b98\"},\"end\":66095,\"start\":66091},{\"attributes\":{\"ref_id\":\"b96\"},\"end\":66144,\"start\":66140},{\"attributes\":{\"ref_id\":\"b97\"},\"end\":66147,\"start\":66144},{\"attributes\":{\"ref_id\":\"b97\"},\"end\":66225,\"start\":66221},{\"attributes\":{\"ref_id\":\"b95\"},\"end\":66281,\"start\":66277},{\"attributes\":{\"ref_id\":\"b96\"},\"end\":66284,\"start\":66281},{\"attributes\":{\"ref_id\":\"b108\"},\"end\":66768,\"start\":66763},{\"attributes\":{\"ref_id\":\"b106\"},\"end\":66818,\"start\":66813},{\"attributes\":{\"ref_id\":\"b107\"},\"end\":66828,\"start\":66823},{\"attributes\":{\"ref_id\":\"b108\"},\"end\":66862,\"start\":66857},{\"attributes\":{\"ref_id\":\"b103\"},\"end\":67078,\"start\":67073},{\"attributes\":{\"ref_id\":\"b105\"},\"end\":67128,\"start\":67123},{\"attributes\":{\"ref_id\":\"b107\"},\"end\":67132,\"start\":67128},{\"attributes\":{\"ref_id\":\"b107\"},\"end\":67284,\"start\":67279},{\"attributes\":{\"ref_id\":\"b103\"},\"end\":67334,\"start\":67329},{\"attributes\":{\"ref_id\":\"b107\"},\"end\":67462,\"start\":67457},{\"attributes\":{\"ref_id\":\"b117\"},\"end\":67755,\"start\":67750},{\"attributes\":{\"ref_id\":\"b113\"},\"end\":67805,\"start\":67800},{\"attributes\":{\"ref_id\":\"b114\"},\"end\":67810,\"start\":67805},{\"attributes\":{\"ref_id\":\"b115\"},\"end\":67815,\"start\":67810},{\"attributes\":{\"ref_id\":\"b116\"},\"end\":67820,\"start\":67815},{\"attributes\":{\"ref_id\":\"b116\"},\"end\":67999,\"start\":67994},{\"attributes\":{\"ref_id\":\"b111\"},\"end\":68049,\"start\":68044},{\"attributes\":{\"ref_id\":\"b112\"},\"end\":68054,\"start\":68049},{\"attributes\":{\"ref_id\":\"b113\"},\"end\":68059,\"start\":68054},{\"attributes\":{\"ref_id\":\"b115\"},\"end\":68063,\"start\":68059},{\"attributes\":{\"ref_id\":\"b117\"},\"end\":68067,\"start\":68063},{\"attributes\":{\"ref_id\":\"b111\"},\"end\":68216,\"start\":68211},{\"attributes\":{\"ref_id\":\"b110\"},\"end\":68286,\"start\":68281},{\"attributes\":{\"ref_id\":\"b113\"},\"end\":68290,\"start\":68286},{\"attributes\":{\"ref_id\":\"b115\"},\"end\":68295,\"start\":68290},{\"attributes\":{\"ref_id\":\"b116\"},\"end\":68300,\"start\":68295},{\"attributes\":{\"ref_id\":\"b117\"},\"end\":68305,\"start\":68300},{\"attributes\":{\"ref_id\":\"b111\"},\"end\":68451,\"start\":68446},{\"attributes\":{\"ref_id\":\"b122\"},\"end\":68946,\"start\":68941},{\"attributes\":{\"ref_id\":\"b120\"},\"end\":68993,\"start\":68988},{\"attributes\":{\"ref_id\":\"b121\"},\"end\":68997,\"start\":68993},{\"attributes\":{\"ref_id\":\"b121\"},\"end\":69152,\"start\":69147},{\"attributes\":{\"ref_id\":\"b120\"},\"end\":69198,\"start\":69193},{\"attributes\":{\"ref_id\":\"b121\"},\"end\":69337,\"start\":69332},{\"attributes\":{\"ref_id\":\"b122\"},\"end\":69365,\"start\":69360},{\"attributes\":{\"ref_id\":\"b121\"},\"end\":69415,\"start\":69410},{\"attributes\":{\"ref_id\":\"b92\"},\"end\":69924,\"start\":69920},{\"attributes\":{\"ref_id\":\"b92\"},\"end\":69942,\"start\":69938},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":70118,\"start\":70114},{\"attributes\":{\"ref_id\":\"b103\"},\"end\":70376,\"start\":70371},{\"attributes\":{\"ref_id\":\"b107\"},\"end\":70380,\"start\":70376},{\"attributes\":{\"ref_id\":\"b108\"},\"end\":70384,\"start\":70380},{\"attributes\":{\"ref_id\":\"b108\"},\"end\":70581,\"start\":70576},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":70615,\"start\":70611},{\"attributes\":{\"ref_id\":\"b107\"},\"end\":70619,\"start\":70615},{\"attributes\":{\"ref_id\":\"b111\"},\"end\":70776,\"start\":70771},{\"attributes\":{\"ref_id\":\"b116\"},\"end\":70780,\"start\":70776},{\"attributes\":{\"ref_id\":\"b117\"},\"end\":70784,\"start\":70780},{\"attributes\":{\"ref_id\":\"b117\"},\"end\":70961,\"start\":70956},{\"attributes\":{\"ref_id\":\"b116\"},\"end\":71071,\"start\":71066},{\"attributes\":{\"ref_id\":\"b111\"},\"end\":71095,\"start\":71090},{\"attributes\":{\"ref_id\":\"b92\"},\"end\":71258,\"start\":71254},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":71323,\"start\":71319}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":75865,\"start\":75808},{\"attributes\":{\"id\":\"fig_1\"},\"end\":75978,\"start\":75866},{\"attributes\":{\"id\":\"fig_2\"},\"end\":76028,\"start\":75979},{\"attributes\":{\"id\":\"fig_3\"},\"end\":76327,\"start\":76029},{\"attributes\":{\"id\":\"fig_4\"},\"end\":76391,\"start\":76328},{\"attributes\":{\"id\":\"fig_5\"},\"end\":76582,\"start\":76392},{\"attributes\":{\"id\":\"fig_6\"},\"end\":76727,\"start\":76583},{\"attributes\":{\"id\":\"fig_7\"},\"end\":76924,\"start\":76728},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":78335,\"start\":76925},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":79197,\"start\":78336},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":79854,\"start\":79198},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":80951,\"start\":79855},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":81782,\"start\":80952},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":82481,\"start\":81783},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":83285,\"start\":82482},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":84294,\"start\":83286},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":85216,\"start\":84295},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":85748,\"start\":85217},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":86845,\"start\":85749}]", "paragraph": "[{\"end\":3119,\"start\":2406},{\"end\":3696,\"start\":3121},{\"end\":5525,\"start\":3698},{\"end\":6851,\"start\":5527},{\"end\":7693,\"start\":6853},{\"end\":7846,\"start\":7699},{\"end\":8293,\"start\":7852},{\"end\":8852,\"start\":8295},{\"end\":9094,\"start\":8858},{\"end\":9317,\"start\":9100},{\"end\":9946,\"start\":9319},{\"end\":11015,\"start\":10013},{\"end\":12410,\"start\":11041},{\"end\":14271,\"start\":12437},{\"end\":15081,\"start\":14306},{\"end\":16441,\"start\":15101},{\"end\":17104,\"start\":16468},{\"end\":17726,\"start\":17123},{\"end\":18009,\"start\":17782},{\"end\":18198,\"start\":18015},{\"end\":19106,\"start\":18204},{\"end\":19325,\"start\":19112},{\"end\":20716,\"start\":19385},{\"end\":21631,\"start\":20755},{\"end\":22254,\"start\":21633},{\"end\":22873,\"start\":22256},{\"end\":23399,\"start\":22875},{\"end\":23856,\"start\":23401},{\"end\":24507,\"start\":23858},{\"end\":25099,\"start\":24509},{\"end\":25538,\"start\":25101},{\"end\":26448,\"start\":25540},{\"end\":26806,\"start\":26488},{\"end\":27782,\"start\":26844},{\"end\":28329,\"start\":27784},{\"end\":29248,\"start\":28359},{\"end\":29643,\"start\":29276},{\"end\":31625,\"start\":29645},{\"end\":32068,\"start\":31627},{\"end\":34609,\"start\":32098},{\"end\":35091,\"start\":34623},{\"end\":39188,\"start\":35119},{\"end\":39616,\"start\":39202},{\"end\":40145,\"start\":39648},{\"end\":41981,\"start\":40147},{\"end\":42529,\"start\":41995},{\"end\":43081,\"start\":42562},{\"end\":43461,\"start\":43115},{\"end\":44663,\"start\":43463},{\"end\":44990,\"start\":44677},{\"end\":48175,\"start\":45024},{\"end\":48490,\"start\":48177},{\"end\":49190,\"start\":48514},{\"end\":49803,\"start\":49224},{\"end\":52872,\"start\":49805},{\"end\":53141,\"start\":52874},{\"end\":53423,\"start\":53155},{\"end\":55517,\"start\":53457},{\"end\":55753,\"start\":55531},{\"end\":56071,\"start\":55791},{\"end\":56656,\"start\":56073},{\"end\":56946,\"start\":56658},{\"end\":57330,\"start\":56948},{\"end\":57866,\"start\":57332},{\"end\":58234,\"start\":57868},{\"end\":59028,\"start\":58236},{\"end\":59630,\"start\":59030},{\"end\":60738,\"start\":59667},{\"end\":61929,\"start\":60740},{\"end\":62259,\"start\":61977},{\"end\":62354,\"start\":62294},{\"end\":62499,\"start\":62356},{\"end\":69658,\"start\":62505},{\"end\":71378,\"start\":69660},{\"end\":72852,\"start\":71415},{\"end\":73000,\"start\":72858},{\"end\":73208,\"start\":73006},{\"end\":73465,\"start\":73214},{\"end\":73949,\"start\":73471},{\"end\":74222,\"start\":73955},{\"end\":74673,\"start\":74228},{\"end\":74973,\"start\":74679},{\"end\":75330,\"start\":74975},{\"end\":75724,\"start\":75332},{\"end\":75796,\"start\":75751}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":62293,\"start\":62260}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":28326,\"start\":28319},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":29569,\"start\":29562},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":32713,\"start\":32706},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":35773,\"start\":35766},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":40021,\"start\":40014},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":43460,\"start\":43453},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":45590,\"start\":45583},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":49802,\"start\":49795},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":53829,\"start\":53822},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":53920,\"start\":53913}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2404,\"start\":2392},{\"end\":7697,\"start\":7696},{\"end\":7850,\"start\":7849},{\"end\":8856,\"start\":8855},{\"end\":9098,\"start\":9097},{\"attributes\":{\"n\":\"2.\"},\"end\":9994,\"start\":9949},{\"attributes\":{\"n\":\"2.1.\"},\"end\":10011,\"start\":9997},{\"attributes\":{\"n\":\"2.1.1.\"},\"end\":11039,\"start\":11018},{\"attributes\":{\"n\":\"2.1.2.\"},\"end\":12435,\"start\":12413},{\"attributes\":{\"n\":\"2.1.3.\"},\"end\":14304,\"start\":14274},{\"attributes\":{\"n\":\"2.1.4.\"},\"end\":15099,\"start\":15084},{\"attributes\":{\"n\":\"2.1.5.\"},\"end\":16466,\"start\":16444},{\"attributes\":{\"n\":\"2.2.\"},\"end\":17121,\"start\":17107},{\"attributes\":{\"n\":\"2.3.\"},\"end\":17780,\"start\":17729},{\"end\":18013,\"start\":18012},{\"end\":18202,\"start\":18201},{\"end\":19110,\"start\":19109},{\"attributes\":{\"n\":\"2.4.\"},\"end\":19383,\"start\":19328},{\"attributes\":{\"n\":\"3.\"},\"end\":20753,\"start\":20719},{\"attributes\":{\"n\":\"4.\"},\"end\":26486,\"start\":26451},{\"end\":26842,\"start\":26809},{\"attributes\":{\"n\":\"4.1.\"},\"end\":28357,\"start\":28332},{\"attributes\":{\"n\":\"4.1.1.\"},\"end\":29274,\"start\":29251},{\"attributes\":{\"n\":\"4.1.2.\"},\"end\":32096,\"start\":32071},{\"end\":34621,\"start\":34612},{\"attributes\":{\"n\":\"4.1.3.\"},\"end\":35117,\"start\":35094},{\"end\":39200,\"start\":39191},{\"attributes\":{\"n\":\"4.1.4.\"},\"end\":39646,\"start\":39619},{\"end\":41993,\"start\":41984},{\"attributes\":{\"n\":\"4.2.\"},\"end\":42560,\"start\":42532},{\"attributes\":{\"n\":\"4.2.1.\"},\"end\":43113,\"start\":43084},{\"end\":44675,\"start\":44666},{\"attributes\":{\"n\":\"4.2.2.\"},\"end\":45022,\"start\":44993},{\"attributes\":{\"n\":\"4.3.\"},\"end\":48512,\"start\":48493},{\"attributes\":{\"n\":\"4.3.1.\"},\"end\":49222,\"start\":49193},{\"end\":53153,\"start\":53144},{\"attributes\":{\"n\":\"4.3.2.\"},\"end\":53455,\"start\":53426},{\"end\":55529,\"start\":55520},{\"attributes\":{\"n\":\"5.\"},\"end\":55789,\"start\":55756},{\"attributes\":{\"n\":\"6.\"},\"end\":59665,\"start\":59633},{\"attributes\":{\"n\":\"7.\"},\"end\":61975,\"start\":61932},{\"end\":62503,\"start\":62502},{\"attributes\":{\"n\":\"8.\"},\"end\":71413,\"start\":71381},{\"end\":72856,\"start\":72855},{\"end\":73004,\"start\":73003},{\"end\":73212,\"start\":73211},{\"end\":73469,\"start\":73468},{\"end\":73953,\"start\":73952},{\"end\":74226,\"start\":74225},{\"end\":74677,\"start\":74676},{\"end\":75749,\"start\":75727},{\"end\":75807,\"start\":75799},{\"end\":75819,\"start\":75809},{\"end\":75877,\"start\":75867},{\"end\":75990,\"start\":75980},{\"end\":76040,\"start\":76030},{\"end\":76339,\"start\":76329},{\"end\":76403,\"start\":76393},{\"end\":76594,\"start\":76584},{\"end\":76737,\"start\":76729},{\"end\":76935,\"start\":76926},{\"end\":78346,\"start\":78337},{\"end\":79208,\"start\":79199},{\"end\":79865,\"start\":79856},{\"end\":80962,\"start\":80953},{\"end\":81793,\"start\":81784},{\"end\":82492,\"start\":82483},{\"end\":83296,\"start\":83287},{\"end\":84305,\"start\":84296}]", "table": "[{\"end\":78335,\"start\":77012},{\"end\":79197,\"start\":78413},{\"end\":79854,\"start\":79276},{\"end\":80951,\"start\":79932},{\"end\":81782,\"start\":81032},{\"end\":82481,\"start\":81865},{\"end\":83285,\"start\":82564},{\"end\":84294,\"start\":83369},{\"end\":85216,\"start\":84377},{\"end\":85748,\"start\":85283},{\"end\":86845,\"start\":86292}]", "figure_caption": "[{\"end\":75865,\"start\":75821},{\"end\":75978,\"start\":75879},{\"end\":76028,\"start\":75992},{\"end\":76327,\"start\":76042},{\"end\":76391,\"start\":76341},{\"end\":76582,\"start\":76405},{\"end\":76727,\"start\":76596},{\"end\":76924,\"start\":76738},{\"end\":77012,\"start\":76937},{\"end\":78413,\"start\":78348},{\"end\":79276,\"start\":79210},{\"end\":79932,\"start\":79867},{\"end\":81032,\"start\":80964},{\"end\":81865,\"start\":81795},{\"end\":82564,\"start\":82494},{\"end\":83369,\"start\":83298},{\"end\":84377,\"start\":84307},{\"end\":85283,\"start\":85219},{\"end\":86292,\"start\":85751}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10777,\"start\":10769},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":17988,\"start\":17980},{\"end\":21111,\"start\":21103},{\"end\":21396,\"start\":21388},{\"end\":23987,\"start\":23979},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":27406,\"start\":27398},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":29217,\"start\":29209},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":55867,\"start\":55859},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":62611,\"start\":62602},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":63520,\"start\":63511},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":64593,\"start\":64584},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":65857,\"start\":65848},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":66739,\"start\":66730},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":67724,\"start\":67715},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":68917,\"start\":68908}]", "bib_author_first_name": "[{\"end\":86968,\"start\":86967},{\"end\":86976,\"start\":86975},{\"end\":86978,\"start\":86977},{\"end\":86985,\"start\":86984},{\"end\":86987,\"start\":86986},{\"end\":87003,\"start\":87002},{\"end\":87005,\"start\":87004},{\"end\":87016,\"start\":87015},{\"end\":87024,\"start\":87023},{\"end\":87035,\"start\":87034},{\"end\":87050,\"start\":87049},{\"end\":87062,\"start\":87061},{\"end\":87527,\"start\":87526},{\"end\":87535,\"start\":87534},{\"end\":87547,\"start\":87546},{\"end\":87563,\"start\":87562},{\"end\":87574,\"start\":87573},{\"end\":87582,\"start\":87581},{\"end\":87593,\"start\":87592},{\"end\":87608,\"start\":87607},{\"end\":87620,\"start\":87619},{\"end\":88170,\"start\":88169},{\"end\":88180,\"start\":88179},{\"end\":88189,\"start\":88188},{\"end\":88200,\"start\":88199},{\"end\":88513,\"start\":88512},{\"end\":88521,\"start\":88520},{\"end\":88523,\"start\":88522},{\"end\":88530,\"start\":88529},{\"end\":88538,\"start\":88537},{\"end\":88819,\"start\":88818},{\"end\":88830,\"start\":88829},{\"end\":88839,\"start\":88838},{\"end\":88847,\"start\":88846},{\"end\":88849,\"start\":88848},{\"end\":89332,\"start\":89331},{\"end\":89334,\"start\":89333},{\"end\":89347,\"start\":89346},{\"end\":89360,\"start\":89359},{\"end\":89362,\"start\":89361},{\"end\":89789,\"start\":89788},{\"end\":89798,\"start\":89797},{\"end\":89805,\"start\":89804},{\"end\":89814,\"start\":89813},{\"end\":89821,\"start\":89820},{\"end\":90088,\"start\":90087},{\"end\":90090,\"start\":90089},{\"end\":90098,\"start\":90097},{\"end\":90530,\"start\":90529},{\"end\":90536,\"start\":90535},{\"end\":90545,\"start\":90544},{\"end\":90547,\"start\":90546},{\"end\":90554,\"start\":90553},{\"end\":90996,\"start\":90995},{\"end\":91009,\"start\":91008},{\"end\":91456,\"start\":91455},{\"end\":91467,\"start\":91466},{\"end\":91478,\"start\":91477},{\"end\":91480,\"start\":91479},{\"end\":91494,\"start\":91493},{\"end\":91496,\"start\":91495},{\"end\":91963,\"start\":91962},{\"end\":91975,\"start\":91974},{\"end\":91985,\"start\":91984},{\"end\":92410,\"start\":92409},{\"end\":92420,\"start\":92419},{\"end\":92898,\"start\":92897},{\"end\":92900,\"start\":92899},{\"end\":92911,\"start\":92910},{\"end\":93460,\"start\":93459},{\"end\":93462,\"start\":93461},{\"end\":93472,\"start\":93471},{\"end\":93474,\"start\":93473},{\"end\":93485,\"start\":93484},{\"end\":93487,\"start\":93486},{\"end\":93496,\"start\":93495},{\"end\":93498,\"start\":93497},{\"end\":93946,\"start\":93945},{\"end\":93955,\"start\":93954},{\"end\":94483,\"start\":94482},{\"end\":94485,\"start\":94484},{\"end\":94493,\"start\":94492},{\"end\":94497,\"start\":94494},{\"end\":94506,\"start\":94505},{\"end\":94995,\"start\":94994},{\"end\":94997,\"start\":94996},{\"end\":95004,\"start\":95003},{\"end\":95006,\"start\":95005},{\"end\":95016,\"start\":95015},{\"end\":95026,\"start\":95025},{\"end\":95028,\"start\":95027},{\"end\":95450,\"start\":95449},{\"end\":95458,\"start\":95457},{\"end\":95471,\"start\":95470},{\"end\":95823,\"start\":95822},{\"end\":95825,\"start\":95824},{\"end\":95832,\"start\":95831},{\"end\":95834,\"start\":95833},{\"end\":96123,\"start\":96122},{\"end\":96132,\"start\":96131},{\"end\":96140,\"start\":96139},{\"end\":96149,\"start\":96148},{\"end\":96151,\"start\":96150},{\"end\":96471,\"start\":96470},{\"end\":96481,\"start\":96480},{\"end\":96492,\"start\":96491},{\"end\":96862,\"start\":96861},{\"end\":96870,\"start\":96869},{\"end\":96872,\"start\":96871},{\"end\":96885,\"start\":96884},{\"end\":96898,\"start\":96897},{\"end\":96900,\"start\":96899},{\"end\":96912,\"start\":96911},{\"end\":96914,\"start\":96913},{\"end\":97217,\"start\":97216},{\"end\":97219,\"start\":97218},{\"end\":97230,\"start\":97229},{\"end\":97241,\"start\":97240},{\"end\":97249,\"start\":97248},{\"end\":97251,\"start\":97250},{\"end\":97597,\"start\":97596},{\"end\":97605,\"start\":97604},{\"end\":97615,\"start\":97614},{\"end\":97628,\"start\":97627},{\"end\":97966,\"start\":97965},{\"end\":97968,\"start\":97967},{\"end\":97979,\"start\":97978},{\"end\":97981,\"start\":97980},{\"end\":98277,\"start\":98276},{\"end\":98285,\"start\":98284},{\"end\":98293,\"start\":98292},{\"end\":98300,\"start\":98299},{\"end\":98776,\"start\":98775},{\"end\":98785,\"start\":98784},{\"end\":98793,\"start\":98792},{\"end\":98799,\"start\":98798},{\"end\":98807,\"start\":98806},{\"end\":98814,\"start\":98813},{\"end\":99102,\"start\":99101},{\"end\":99104,\"start\":99103},{\"end\":99114,\"start\":99113},{\"end\":99116,\"start\":99115},{\"end\":99131,\"start\":99130},{\"end\":99133,\"start\":99132},{\"end\":99141,\"start\":99140},{\"end\":99143,\"start\":99142},{\"end\":99153,\"start\":99152},{\"end\":99155,\"start\":99154},{\"end\":99612,\"start\":99611},{\"end\":99621,\"start\":99620},{\"end\":99636,\"start\":99635},{\"end\":100089,\"start\":100088},{\"end\":100096,\"start\":100095},{\"end\":100103,\"start\":100102},{\"end\":100609,\"start\":100608},{\"end\":100611,\"start\":100610},{\"end\":100622,\"start\":100621},{\"end\":100635,\"start\":100634},{\"end\":100643,\"start\":100642},{\"end\":100653,\"start\":100652},{\"end\":100655,\"start\":100654},{\"end\":100666,\"start\":100665},{\"end\":100674,\"start\":100673},{\"end\":101042,\"start\":101041},{\"end\":101057,\"start\":101056},{\"end\":101059,\"start\":101058},{\"end\":101070,\"start\":101069},{\"end\":101079,\"start\":101078},{\"end\":101676,\"start\":101675},{\"end\":101686,\"start\":101685},{\"end\":101697,\"start\":101696},{\"end\":101699,\"start\":101698},{\"end\":102222,\"start\":102221},{\"end\":102481,\"start\":102480},{\"end\":102483,\"start\":102482},{\"end\":102499,\"start\":102498},{\"end\":102501,\"start\":102500},{\"end\":102510,\"start\":102509},{\"end\":102512,\"start\":102511},{\"end\":102523,\"start\":102522},{\"end\":102525,\"start\":102524},{\"end\":102536,\"start\":102535},{\"end\":102549,\"start\":102548},{\"end\":102558,\"start\":102557},{\"end\":102560,\"start\":102559},{\"end\":102571,\"start\":102570},{\"end\":102573,\"start\":102572},{\"end\":103127,\"start\":103126},{\"end\":103136,\"start\":103135},{\"end\":103144,\"start\":103143},{\"end\":103338,\"start\":103337},{\"end\":103345,\"start\":103344},{\"end\":103358,\"start\":103357},{\"end\":103812,\"start\":103811},{\"end\":103818,\"start\":103817},{\"end\":104301,\"start\":104300},{\"end\":104316,\"start\":104315},{\"end\":104761,\"start\":104760},{\"end\":104763,\"start\":104762},{\"end\":104779,\"start\":104778},{\"end\":104781,\"start\":104780},{\"end\":104793,\"start\":104792},{\"end\":104807,\"start\":104806},{\"end\":105168,\"start\":105167},{\"end\":105170,\"start\":105169},{\"end\":105177,\"start\":105176},{\"end\":105179,\"start\":105178},{\"end\":105187,\"start\":105186},{\"end\":105189,\"start\":105188},{\"end\":105548,\"start\":105547},{\"end\":105554,\"start\":105553},{\"end\":105563,\"start\":105562},{\"end\":105572,\"start\":105571},{\"end\":106056,\"start\":106055},{\"end\":106068,\"start\":106067},{\"end\":106551,\"start\":106550},{\"end\":106553,\"start\":106552},{\"end\":106563,\"start\":106562},{\"end\":106843,\"start\":106842},{\"end\":106845,\"start\":106844},{\"end\":106855,\"start\":106854},{\"end\":106862,\"start\":106861},{\"end\":107180,\"start\":107179},{\"end\":107190,\"start\":107189},{\"end\":107200,\"start\":107199},{\"end\":107644,\"start\":107643},{\"end\":107650,\"start\":107649},{\"end\":107658,\"start\":107657},{\"end\":107666,\"start\":107665},{\"end\":107672,\"start\":107671},{\"end\":107911,\"start\":107910},{\"end\":107920,\"start\":107919},{\"end\":107927,\"start\":107926},{\"end\":107933,\"start\":107932},{\"end\":107943,\"start\":107942},{\"end\":108427,\"start\":108426},{\"end\":108439,\"start\":108438},{\"end\":108970,\"start\":108969},{\"end\":108982,\"start\":108981},{\"end\":108992,\"start\":108991},{\"end\":109554,\"start\":109553},{\"end\":109560,\"start\":109559},{\"end\":109566,\"start\":109565},{\"end\":109572,\"start\":109571},{\"end\":109868,\"start\":109867},{\"end\":109875,\"start\":109874},{\"end\":109883,\"start\":109882},{\"end\":109891,\"start\":109890},{\"end\":110434,\"start\":110433},{\"end\":110443,\"start\":110442},{\"end\":110445,\"start\":110444},{\"end\":110455,\"start\":110454},{\"end\":110457,\"start\":110456},{\"end\":110720,\"start\":110719},{\"end\":110722,\"start\":110721},{\"end\":110953,\"start\":110952},{\"end\":110959,\"start\":110958},{\"end\":110967,\"start\":110966},{\"end\":110977,\"start\":110976},{\"end\":110984,\"start\":110983},{\"end\":110990,\"start\":110989},{\"end\":110992,\"start\":110991},{\"end\":111003,\"start\":111002},{\"end\":111014,\"start\":111013},{\"end\":111024,\"start\":111023},{\"end\":111026,\"start\":111025},{\"end\":111037,\"start\":111036},{\"end\":111475,\"start\":111474},{\"end\":111477,\"start\":111476},{\"end\":111485,\"start\":111484},{\"end\":111487,\"start\":111486},{\"end\":111499,\"start\":111498},{\"end\":111507,\"start\":111506},{\"end\":111858,\"start\":111857},{\"end\":111867,\"start\":111866},{\"end\":111876,\"start\":111875},{\"end\":112116,\"start\":112115},{\"end\":112126,\"start\":112125},{\"end\":112137,\"start\":112136},{\"end\":112145,\"start\":112144},{\"end\":112153,\"start\":112152},{\"end\":112161,\"start\":112160},{\"end\":112179,\"start\":112178},{\"end\":112188,\"start\":112187},{\"end\":112200,\"start\":112199},{\"end\":112211,\"start\":112210},{\"end\":112593,\"start\":112592},{\"end\":112602,\"start\":112601},{\"end\":112613,\"start\":112612},{\"end\":112622,\"start\":112621},{\"end\":112624,\"start\":112623},{\"end\":113008,\"start\":113007},{\"end\":113020,\"start\":113019},{\"end\":113029,\"start\":113028},{\"end\":113039,\"start\":113038},{\"end\":113049,\"start\":113048},{\"end\":113533,\"start\":113532},{\"end\":113539,\"start\":113538},{\"end\":113545,\"start\":113544},{\"end\":113552,\"start\":113551},{\"end\":113559,\"start\":113558},{\"end\":113566,\"start\":113565},{\"end\":113893,\"start\":113892},{\"end\":113895,\"start\":113894},{\"end\":113904,\"start\":113903},{\"end\":113911,\"start\":113910},{\"end\":113921,\"start\":113920},{\"end\":113923,\"start\":113922},{\"end\":113931,\"start\":113930},{\"end\":114414,\"start\":114413},{\"end\":114424,\"start\":114423},{\"end\":114435,\"start\":114434},{\"end\":114448,\"start\":114447},{\"end\":114457,\"start\":114456},{\"end\":114804,\"start\":114803},{\"end\":114811,\"start\":114810},{\"end\":114817,\"start\":114816},{\"end\":114825,\"start\":114824},{\"end\":114833,\"start\":114832},{\"end\":115217,\"start\":115216},{\"end\":115229,\"start\":115228},{\"end\":115239,\"start\":115238},{\"end\":115247,\"start\":115246},{\"end\":115260,\"start\":115259},{\"end\":115272,\"start\":115271},{\"end\":115282,\"start\":115281},{\"end\":115292,\"start\":115291},{\"end\":115304,\"start\":115303},{\"end\":115312,\"start\":115311},{\"end\":115745,\"start\":115744},{\"end\":116026,\"start\":116025},{\"end\":116028,\"start\":116027},{\"end\":116035,\"start\":116034},{\"end\":116043,\"start\":116042},{\"end\":116051,\"start\":116050},{\"end\":116373,\"start\":116372},{\"end\":116383,\"start\":116382},{\"end\":116394,\"start\":116393},{\"end\":116405,\"start\":116404},{\"end\":116885,\"start\":116884},{\"end\":116891,\"start\":116890},{\"end\":116897,\"start\":116896},{\"end\":116903,\"start\":116902},{\"end\":116910,\"start\":116909},{\"end\":116916,\"start\":116915},{\"end\":117249,\"start\":117248},{\"end\":117251,\"start\":117250},{\"end\":117263,\"start\":117262},{\"end\":117275,\"start\":117274},{\"end\":117277,\"start\":117276},{\"end\":117286,\"start\":117285},{\"end\":117727,\"start\":117726},{\"end\":117735,\"start\":117734},{\"end\":117744,\"start\":117743},{\"end\":117752,\"start\":117751},{\"end\":117759,\"start\":117758},{\"end\":118194,\"start\":118193},{\"end\":118202,\"start\":118201},{\"end\":118208,\"start\":118207},{\"end\":118216,\"start\":118215},{\"end\":118653,\"start\":118652},{\"end\":118659,\"start\":118658},{\"end\":118668,\"start\":118667},{\"end\":118676,\"start\":118675},{\"end\":118684,\"start\":118683},{\"end\":118974,\"start\":118973},{\"end\":118984,\"start\":118983},{\"end\":119347,\"start\":119346},{\"end\":119364,\"start\":119363},{\"end\":119376,\"start\":119375},{\"end\":119378,\"start\":119377},{\"end\":119391,\"start\":119390},{\"end\":119400,\"start\":119399},{\"end\":119832,\"start\":119831},{\"end\":119841,\"start\":119840},{\"end\":119849,\"start\":119848},{\"end\":119858,\"start\":119857},{\"end\":119860,\"start\":119859},{\"end\":120097,\"start\":120096},{\"end\":120103,\"start\":120102},{\"end\":120112,\"start\":120111},{\"end\":120545,\"start\":120544},{\"end\":120552,\"start\":120551},{\"end\":120559,\"start\":120558},{\"end\":120566,\"start\":120565},{\"end\":120831,\"start\":120830},{\"end\":120839,\"start\":120838},{\"end\":120847,\"start\":120846},{\"end\":120853,\"start\":120852},{\"end\":120859,\"start\":120858},{\"end\":120865,\"start\":120864},{\"end\":121069,\"start\":121068},{\"end\":121076,\"start\":121075},{\"end\":121084,\"start\":121083},{\"end\":121090,\"start\":121089},{\"end\":121100,\"start\":121099},{\"end\":121417,\"start\":121416},{\"end\":121424,\"start\":121423},{\"end\":121431,\"start\":121430},{\"end\":121443,\"start\":121442},{\"end\":121623,\"start\":121622},{\"end\":121632,\"start\":121631},{\"end\":121638,\"start\":121637},{\"end\":121640,\"start\":121639},{\"end\":121649,\"start\":121648},{\"end\":121658,\"start\":121657},{\"end\":121931,\"start\":121930},{\"end\":121939,\"start\":121938},{\"end\":122232,\"start\":122231},{\"end\":122247,\"start\":122246},{\"end\":122249,\"start\":122248},{\"end\":122264,\"start\":122263},{\"end\":122266,\"start\":122265},{\"end\":122624,\"start\":122623},{\"end\":122632,\"start\":122631},{\"end\":122638,\"start\":122637},{\"end\":122645,\"start\":122644},{\"end\":122652,\"start\":122651},{\"end\":123000,\"start\":122999},{\"end\":123011,\"start\":123010},{\"end\":123020,\"start\":123019},{\"end\":123283,\"start\":123282},{\"end\":123298,\"start\":123297},{\"end\":123300,\"start\":123299},{\"end\":123718,\"start\":123717},{\"end\":123725,\"start\":123724},{\"end\":123734,\"start\":123733},{\"end\":123741,\"start\":123740},{\"end\":124009,\"start\":124008},{\"end\":124016,\"start\":124015},{\"end\":124024,\"start\":124023},{\"end\":124032,\"start\":124031},{\"end\":124462,\"start\":124461},{\"end\":124470,\"start\":124469},{\"end\":124476,\"start\":124475},{\"end\":124482,\"start\":124481},{\"end\":124490,\"start\":124489},{\"end\":124496,\"start\":124495},{\"end\":124947,\"start\":124946},{\"end\":124957,\"start\":124956},{\"end\":124970,\"start\":124969},{\"end\":125385,\"start\":125384},{\"end\":125395,\"start\":125394},{\"end\":125405,\"start\":125404},{\"end\":125415,\"start\":125414},{\"end\":125424,\"start\":125423},{\"end\":125438,\"start\":125437},{\"end\":125449,\"start\":125448},{\"end\":125975,\"start\":125974},{\"end\":125982,\"start\":125981},{\"end\":125990,\"start\":125989},{\"end\":126534,\"start\":126533},{\"end\":126543,\"start\":126542},{\"end\":126549,\"start\":126548},{\"end\":126558,\"start\":126557},{\"end\":126564,\"start\":126563},{\"end\":126571,\"start\":126570},{\"end\":126577,\"start\":126576},{\"end\":126932,\"start\":126931},{\"end\":126940,\"start\":126939},{\"end\":126947,\"start\":126946},{\"end\":126955,\"start\":126954},{\"end\":126964,\"start\":126963},{\"end\":127283,\"start\":127282},{\"end\":127290,\"start\":127289},{\"end\":127307,\"start\":127306},{\"end\":127309,\"start\":127308},{\"end\":127553,\"start\":127552},{\"end\":127561,\"start\":127560},{\"end\":127570,\"start\":127569},{\"end\":127974,\"start\":127973},{\"end\":127984,\"start\":127983},{\"end\":127992,\"start\":127991},{\"end\":128003,\"start\":128002},{\"end\":128005,\"start\":128004},{\"end\":128475,\"start\":128474},{\"end\":128477,\"start\":128476},{\"end\":128490,\"start\":128489},{\"end\":128492,\"start\":128491},{\"end\":128502,\"start\":128501},{\"end\":128504,\"start\":128503},{\"end\":128762,\"start\":128761},{\"end\":128769,\"start\":128768},{\"end\":128771,\"start\":128770},{\"end\":128780,\"start\":128779},{\"end\":129076,\"start\":129075},{\"end\":129084,\"start\":129083},{\"end\":129092,\"start\":129091},{\"end\":129094,\"start\":129093},{\"end\":129426,\"start\":129425},{\"end\":129434,\"start\":129433},{\"end\":129443,\"start\":129442},{\"end\":129454,\"start\":129453},{\"end\":129463,\"start\":129462},{\"end\":129907,\"start\":129906},{\"end\":129918,\"start\":129917},{\"end\":129932,\"start\":129931},{\"end\":129942,\"start\":129941},{\"end\":129944,\"start\":129943},{\"end\":129953,\"start\":129952},{\"end\":129955,\"start\":129954},{\"end\":130407,\"start\":130406},{\"end\":130416,\"start\":130415},{\"end\":130422,\"start\":130421},{\"end\":130430,\"start\":130429},{\"end\":130872,\"start\":130871},{\"end\":130879,\"start\":130878},{\"end\":130887,\"start\":130886},{\"end\":130893,\"start\":130892},{\"end\":131124,\"start\":131123},{\"end\":131131,\"start\":131130},{\"end\":131138,\"start\":131137},{\"end\":131145,\"start\":131144},{\"end\":131357,\"start\":131356},{\"end\":131359,\"start\":131358},{\"end\":131368,\"start\":131367},{\"end\":131379,\"start\":131378},{\"end\":131381,\"start\":131380},{\"end\":131396,\"start\":131395},{\"end\":131398,\"start\":131397},{\"end\":131404,\"start\":131403},{\"end\":131968,\"start\":131967},{\"end\":131975,\"start\":131974},{\"end\":131982,\"start\":131981},{\"end\":131990,\"start\":131989},{\"end\":131992,\"start\":131991},{\"end\":132507,\"start\":132506},{\"end\":132522,\"start\":132521},{\"end\":132533,\"start\":132532},{\"end\":132535,\"start\":132534},{\"end\":132744,\"start\":132743},{\"end\":132750,\"start\":132749},{\"end\":132756,\"start\":132755},{\"end\":132765,\"start\":132764},{\"end\":132772,\"start\":132771},{\"end\":132778,\"start\":132777},{\"end\":133042,\"start\":133041},{\"end\":133051,\"start\":133050},{\"end\":133062,\"start\":133061},{\"end\":133071,\"start\":133070},{\"end\":133079,\"start\":133078},{\"end\":133083,\"start\":133080},{\"end\":133402,\"start\":133401},{\"end\":133411,\"start\":133410},{\"end\":133420,\"start\":133419},{\"end\":133428,\"start\":133427},{\"end\":133437,\"start\":133436},{\"end\":133717,\"start\":133716},{\"end\":133726,\"start\":133725},{\"end\":133734,\"start\":133733},{\"end\":133742,\"start\":133741},{\"end\":133749,\"start\":133748},{\"end\":134250,\"start\":134249},{\"end\":134258,\"start\":134257},{\"end\":134265,\"start\":134264},{\"end\":134272,\"start\":134271},{\"end\":134278,\"start\":134277},{\"end\":134553,\"start\":134552},{\"end\":134555,\"start\":134554},{\"end\":134562,\"start\":134561},{\"end\":134571,\"start\":134570},{\"end\":134573,\"start\":134572},{\"end\":135011,\"start\":135010},{\"end\":135027,\"start\":135026},{\"end\":135254,\"start\":135253},{\"end\":135263,\"start\":135262},{\"end\":135271,\"start\":135270},{\"end\":135278,\"start\":135277},{\"end\":135284,\"start\":135283},{\"end\":135292,\"start\":135291},{\"end\":135705,\"start\":135704},{\"end\":135716,\"start\":135715},{\"end\":135730,\"start\":135729},{\"end\":135740,\"start\":135739},{\"end\":135742,\"start\":135741},{\"end\":136349,\"start\":136348},{\"end\":136360,\"start\":136359},{\"end\":136373,\"start\":136372},{\"end\":136383,\"start\":136382},{\"end\":136385,\"start\":136384},{\"end\":136744,\"start\":136743},{\"end\":136746,\"start\":136745},{\"end\":136757,\"start\":136756},{\"end\":136759,\"start\":136758},{\"end\":137347,\"start\":137346},{\"end\":137349,\"start\":137348},{\"end\":137360,\"start\":137359},{\"end\":137362,\"start\":137361},{\"end\":137370,\"start\":137369},{\"end\":137372,\"start\":137371},{\"end\":137629,\"start\":137628},{\"end\":137640,\"start\":137639},{\"end\":137649,\"start\":137648},{\"end\":137651,\"start\":137650},{\"end\":137661,\"start\":137660},{\"end\":138213,\"start\":138212},{\"end\":138223,\"start\":138222},{\"end\":138231,\"start\":138230},{\"end\":138233,\"start\":138232},{\"end\":138869,\"start\":138868},{\"end\":138885,\"start\":138884},{\"end\":138895,\"start\":138894},{\"end\":139324,\"start\":139323},{\"end\":139328,\"start\":139325},{\"end\":139338,\"start\":139337},{\"end\":139347,\"start\":139346},{\"end\":139349,\"start\":139348},{\"end\":139360,\"start\":139359},{\"end\":139362,\"start\":139361},{\"end\":139373,\"start\":139372},{\"end\":139381,\"start\":139380},{\"end\":139392,\"start\":139391},{\"end\":139394,\"start\":139393},{\"end\":139402,\"start\":139401},{\"end\":139415,\"start\":139414},{\"end\":139419,\"start\":139416},{\"end\":139905,\"start\":139904},{\"end\":139911,\"start\":139910},{\"end\":139919,\"start\":139918},{\"end\":139927,\"start\":139926},{\"end\":139933,\"start\":139932},{\"end\":139935,\"start\":139934},{\"end\":140268,\"start\":140267},{\"end\":140281,\"start\":140280},{\"end\":140290,\"start\":140289},{\"end\":140623,\"start\":140622},{\"end\":140632,\"start\":140631},{\"end\":140645,\"start\":140644},{\"end\":140656,\"start\":140655},{\"end\":141002,\"start\":141001},{\"end\":141013,\"start\":141012},{\"end\":141022,\"start\":141021},{\"end\":141033,\"start\":141032},{\"end\":141041,\"start\":141040},{\"end\":141049,\"start\":141048},{\"end\":141393,\"start\":141392},{\"end\":141402,\"start\":141401},{\"end\":141411,\"start\":141410},{\"end\":141423,\"start\":141422},{\"end\":141988,\"start\":141987},{\"end\":141990,\"start\":141989},{\"end\":141998,\"start\":141997},{\"end\":142000,\"start\":141999},{\"end\":142010,\"start\":142009},{\"end\":142012,\"start\":142011},{\"end\":142562,\"start\":142561},{\"end\":142564,\"start\":142563},{\"end\":142570,\"start\":142569},{\"end\":142572,\"start\":142571},{\"end\":142582,\"start\":142581},{\"end\":142595,\"start\":142594},{\"end\":143097,\"start\":143096},{\"end\":143105,\"start\":143104},{\"end\":143113,\"start\":143112},{\"end\":143123,\"start\":143122},{\"end\":143125,\"start\":143124},{\"end\":143131,\"start\":143130},{\"end\":143137,\"start\":143136},{\"end\":143632,\"start\":143631},{\"end\":143649,\"start\":143648},{\"end\":143660,\"start\":143659},{\"end\":144247,\"start\":144246},{\"end\":144255,\"start\":144254},{\"end\":144263,\"start\":144262},{\"end\":144275,\"start\":144274},{\"end\":144287,\"start\":144286},{\"end\":144289,\"start\":144288},{\"end\":144858,\"start\":144857},{\"end\":144869,\"start\":144868},{\"end\":144871,\"start\":144870},{\"end\":144882,\"start\":144881},{\"end\":144895,\"start\":144894},{\"end\":144906,\"start\":144905},{\"end\":144919,\"start\":144918},{\"end\":144929,\"start\":144928},{\"end\":144954,\"start\":144953},{\"end\":144962,\"start\":144961},{\"end\":144978,\"start\":144977},{\"end\":145400,\"start\":145399},{\"end\":145410,\"start\":145409},{\"end\":145420,\"start\":145419},{\"end\":145422,\"start\":145421},{\"end\":145431,\"start\":145430},{\"end\":145437,\"start\":145436},{\"end\":145439,\"start\":145438},{\"end\":145450,\"start\":145449},{\"end\":145452,\"start\":145451},{\"end\":145460,\"start\":145459},{\"end\":145787,\"start\":145786},{\"end\":145796,\"start\":145795},{\"end\":145804,\"start\":145803},{\"end\":146120,\"start\":146119},{\"end\":146128,\"start\":146127},{\"end\":146136,\"start\":146135},{\"end\":146393,\"start\":146392},{\"end\":146406,\"start\":146405},{\"end\":146416,\"start\":146415},{\"end\":146428,\"start\":146427},{\"end\":146437,\"start\":146436},{\"end\":146447,\"start\":146446},{\"end\":146455,\"start\":146454},{\"end\":146463,\"start\":146462},{\"end\":146474,\"start\":146473},{\"end\":146484,\"start\":146483},{\"end\":147006,\"start\":147005},{\"end\":147014,\"start\":147013},{\"end\":147021,\"start\":147020},{\"end\":147030,\"start\":147029},{\"end\":147489,\"start\":147488},{\"end\":147503,\"start\":147502},{\"end\":147909,\"start\":147908},{\"end\":147923,\"start\":147922},{\"end\":147936,\"start\":147935},{\"end\":147938,\"start\":147937},{\"end\":148423,\"start\":148422},{\"end\":148425,\"start\":148424},{\"end\":148432,\"start\":148431},{\"end\":148434,\"start\":148433},{\"end\":148441,\"start\":148440},{\"end\":148443,\"start\":148442},{\"end\":148450,\"start\":148449},{\"end\":148867,\"start\":148866},{\"end\":148869,\"start\":148868},{\"end\":148881,\"start\":148880},{\"end\":148885,\"start\":148882},{\"end\":148905,\"start\":148904},{\"end\":148909,\"start\":148906},{\"end\":148917,\"start\":148916},{\"end\":148929,\"start\":148928},{\"end\":148939,\"start\":148938},{\"end\":148943,\"start\":148940},{\"end\":149437,\"start\":149436},{\"end\":149439,\"start\":149438},{\"end\":149452,\"start\":149451},{\"end\":149461,\"start\":149460},{\"end\":149463,\"start\":149462},{\"end\":149747,\"start\":149746},{\"end\":149757,\"start\":149756},{\"end\":149985,\"start\":149984},{\"end\":149993,\"start\":149992},{\"end\":150005,\"start\":150004},{\"end\":150281,\"start\":150280},{\"end\":150288,\"start\":150287},{\"end\":150290,\"start\":150289},{\"end\":150298,\"start\":150297},{\"end\":150307,\"start\":150306},{\"end\":150314,\"start\":150313},{\"end\":150615,\"start\":150614},{\"end\":150617,\"start\":150616},{\"end\":150627,\"start\":150626},{\"end\":150629,\"start\":150628},{\"end\":150850,\"start\":150849},{\"end\":150859,\"start\":150858},{\"end\":150869,\"start\":150868},{\"end\":150876,\"start\":150875},{\"end\":150883,\"start\":150882},{\"end\":150890,\"start\":150889},{\"end\":150892,\"start\":150891},{\"end\":151372,\"start\":151371},{\"end\":151382,\"start\":151381},{\"end\":151384,\"start\":151383},{\"end\":151693,\"start\":151692},{\"end\":151700,\"start\":151699},{\"end\":151711,\"start\":151710},{\"end\":151721,\"start\":151720},{\"end\":151723,\"start\":151722},{\"end\":151983,\"start\":151982},{\"end\":151985,\"start\":151984},{\"end\":151996,\"start\":151995},{\"end\":152003,\"start\":152002},{\"end\":152005,\"start\":152004},{\"end\":152018,\"start\":152017},{\"end\":152028,\"start\":152027},{\"end\":152030,\"start\":152029},{\"end\":152039,\"start\":152038}]", "bib_author_last_name": "[{\"end\":86973,\"start\":86969},{\"end\":86982,\"start\":86979},{\"end\":87000,\"start\":86988},{\"end\":87013,\"start\":87006},{\"end\":87021,\"start\":87017},{\"end\":87032,\"start\":87025},{\"end\":87047,\"start\":87036},{\"end\":87059,\"start\":87051},{\"end\":87073,\"start\":87063},{\"end\":87532,\"start\":87528},{\"end\":87544,\"start\":87536},{\"end\":87560,\"start\":87548},{\"end\":87571,\"start\":87564},{\"end\":87579,\"start\":87575},{\"end\":87590,\"start\":87583},{\"end\":87605,\"start\":87594},{\"end\":87617,\"start\":87609},{\"end\":87631,\"start\":87621},{\"end\":88177,\"start\":88171},{\"end\":88186,\"start\":88181},{\"end\":88197,\"start\":88190},{\"end\":88207,\"start\":88201},{\"end\":88518,\"start\":88514},{\"end\":88527,\"start\":88524},{\"end\":88535,\"start\":88531},{\"end\":88544,\"start\":88539},{\"end\":88827,\"start\":88820},{\"end\":88836,\"start\":88831},{\"end\":88844,\"start\":88840},{\"end\":88859,\"start\":88850},{\"end\":89344,\"start\":89335},{\"end\":89357,\"start\":89348},{\"end\":89372,\"start\":89363},{\"end\":89795,\"start\":89790},{\"end\":89802,\"start\":89799},{\"end\":89811,\"start\":89806},{\"end\":89818,\"start\":89815},{\"end\":89826,\"start\":89822},{\"end\":90095,\"start\":90091},{\"end\":90110,\"start\":90099},{\"end\":90533,\"start\":90531},{\"end\":90542,\"start\":90537},{\"end\":90551,\"start\":90548},{\"end\":90557,\"start\":90555},{\"end\":91006,\"start\":90997},{\"end\":91019,\"start\":91010},{\"end\":91464,\"start\":91457},{\"end\":91475,\"start\":91468},{\"end\":91491,\"start\":91481},{\"end\":91504,\"start\":91497},{\"end\":91972,\"start\":91964},{\"end\":91982,\"start\":91976},{\"end\":91991,\"start\":91986},{\"end\":92417,\"start\":92411},{\"end\":92429,\"start\":92421},{\"end\":92908,\"start\":92901},{\"end\":92919,\"start\":92912},{\"end\":93469,\"start\":93463},{\"end\":93482,\"start\":93475},{\"end\":93493,\"start\":93488},{\"end\":93506,\"start\":93499},{\"end\":93952,\"start\":93947},{\"end\":93962,\"start\":93956},{\"end\":94490,\"start\":94486},{\"end\":94503,\"start\":94498},{\"end\":94518,\"start\":94507},{\"end\":95001,\"start\":94998},{\"end\":95013,\"start\":95007},{\"end\":95023,\"start\":95017},{\"end\":95033,\"start\":95029},{\"end\":95455,\"start\":95451},{\"end\":95468,\"start\":95459},{\"end\":95480,\"start\":95472},{\"end\":95829,\"start\":95826},{\"end\":95839,\"start\":95835},{\"end\":96129,\"start\":96124},{\"end\":96137,\"start\":96133},{\"end\":96146,\"start\":96141},{\"end\":96155,\"start\":96152},{\"end\":96478,\"start\":96472},{\"end\":96489,\"start\":96482},{\"end\":96499,\"start\":96493},{\"end\":96783,\"start\":96775},{\"end\":96867,\"start\":96863},{\"end\":96882,\"start\":96873},{\"end\":96895,\"start\":96886},{\"end\":96909,\"start\":96901},{\"end\":96917,\"start\":96915},{\"end\":97227,\"start\":97220},{\"end\":97238,\"start\":97231},{\"end\":97246,\"start\":97242},{\"end\":97261,\"start\":97252},{\"end\":97602,\"start\":97598},{\"end\":97612,\"start\":97606},{\"end\":97625,\"start\":97616},{\"end\":97635,\"start\":97629},{\"end\":97976,\"start\":97969},{\"end\":97987,\"start\":97982},{\"end\":98282,\"start\":98278},{\"end\":98290,\"start\":98286},{\"end\":98297,\"start\":98294},{\"end\":98305,\"start\":98301},{\"end\":98782,\"start\":98777},{\"end\":98790,\"start\":98786},{\"end\":98796,\"start\":98794},{\"end\":98804,\"start\":98800},{\"end\":98811,\"start\":98808},{\"end\":98819,\"start\":98815},{\"end\":99111,\"start\":99105},{\"end\":99128,\"start\":99117},{\"end\":99138,\"start\":99134},{\"end\":99150,\"start\":99144},{\"end\":99159,\"start\":99156},{\"end\":99618,\"start\":99613},{\"end\":99633,\"start\":99622},{\"end\":99644,\"start\":99637},{\"end\":100093,\"start\":100090},{\"end\":100100,\"start\":100097},{\"end\":100107,\"start\":100104},{\"end\":100619,\"start\":100612},{\"end\":100632,\"start\":100623},{\"end\":100640,\"start\":100636},{\"end\":100650,\"start\":100644},{\"end\":100663,\"start\":100656},{\"end\":100671,\"start\":100667},{\"end\":100686,\"start\":100675},{\"end\":101054,\"start\":101043},{\"end\":101067,\"start\":101060},{\"end\":101076,\"start\":101071},{\"end\":101084,\"start\":101080},{\"end\":101683,\"start\":101677},{\"end\":101694,\"start\":101687},{\"end\":101715,\"start\":101700},{\"end\":102229,\"start\":102223},{\"end\":102496,\"start\":102484},{\"end\":102507,\"start\":102502},{\"end\":102520,\"start\":102513},{\"end\":102533,\"start\":102526},{\"end\":102546,\"start\":102537},{\"end\":102555,\"start\":102550},{\"end\":102568,\"start\":102561},{\"end\":102580,\"start\":102574},{\"end\":103133,\"start\":103128},{\"end\":103141,\"start\":103137},{\"end\":103149,\"start\":103145},{\"end\":103342,\"start\":103339},{\"end\":103355,\"start\":103346},{\"end\":103365,\"start\":103359},{\"end\":103815,\"start\":103813},{\"end\":103826,\"start\":103819},{\"end\":104313,\"start\":104302},{\"end\":104321,\"start\":104317},{\"end\":104776,\"start\":104764},{\"end\":104790,\"start\":104782},{\"end\":104804,\"start\":104794},{\"end\":104815,\"start\":104808},{\"end\":105174,\"start\":105171},{\"end\":105184,\"start\":105180},{\"end\":105194,\"start\":105190},{\"end\":105551,\"start\":105549},{\"end\":105560,\"start\":105555},{\"end\":105569,\"start\":105564},{\"end\":105576,\"start\":105573},{\"end\":106065,\"start\":106057},{\"end\":106075,\"start\":106069},{\"end\":106560,\"start\":106554},{\"end\":106571,\"start\":106564},{\"end\":106852,\"start\":106846},{\"end\":106859,\"start\":106856},{\"end\":106867,\"start\":106863},{\"end\":107187,\"start\":107181},{\"end\":107197,\"start\":107191},{\"end\":107207,\"start\":107201},{\"end\":107214,\"start\":107209},{\"end\":107647,\"start\":107645},{\"end\":107655,\"start\":107651},{\"end\":107663,\"start\":107659},{\"end\":107669,\"start\":107667},{\"end\":107675,\"start\":107673},{\"end\":107917,\"start\":107912},{\"end\":107924,\"start\":107921},{\"end\":107930,\"start\":107928},{\"end\":107940,\"start\":107934},{\"end\":107946,\"start\":107944},{\"end\":108436,\"start\":108428},{\"end\":108446,\"start\":108440},{\"end\":108979,\"start\":108971},{\"end\":108989,\"start\":108983},{\"end\":108998,\"start\":108993},{\"end\":109557,\"start\":109555},{\"end\":109563,\"start\":109561},{\"end\":109569,\"start\":109567},{\"end\":109582,\"start\":109573},{\"end\":109872,\"start\":109869},{\"end\":109880,\"start\":109876},{\"end\":109888,\"start\":109884},{\"end\":109897,\"start\":109892},{\"end\":110440,\"start\":110435},{\"end\":110452,\"start\":110446},{\"end\":110468,\"start\":110458},{\"end\":110727,\"start\":110723},{\"end\":110956,\"start\":110954},{\"end\":110964,\"start\":110960},{\"end\":110974,\"start\":110968},{\"end\":110981,\"start\":110978},{\"end\":110987,\"start\":110985},{\"end\":111000,\"start\":110993},{\"end\":111011,\"start\":111004},{\"end\":111021,\"start\":111015},{\"end\":111034,\"start\":111027},{\"end\":111044,\"start\":111038},{\"end\":111482,\"start\":111478},{\"end\":111496,\"start\":111488},{\"end\":111504,\"start\":111500},{\"end\":111517,\"start\":111508},{\"end\":111864,\"start\":111859},{\"end\":111873,\"start\":111868},{\"end\":111881,\"start\":111877},{\"end\":112123,\"start\":112117},{\"end\":112134,\"start\":112127},{\"end\":112142,\"start\":112138},{\"end\":112150,\"start\":112146},{\"end\":112158,\"start\":112154},{\"end\":112176,\"start\":112162},{\"end\":112185,\"start\":112180},{\"end\":112197,\"start\":112189},{\"end\":112208,\"start\":112201},{\"end\":112222,\"start\":112212},{\"end\":112599,\"start\":112594},{\"end\":112610,\"start\":112603},{\"end\":112619,\"start\":112614},{\"end\":112631,\"start\":112625},{\"end\":113017,\"start\":113009},{\"end\":113026,\"start\":113021},{\"end\":113036,\"start\":113030},{\"end\":113046,\"start\":113040},{\"end\":113056,\"start\":113050},{\"end\":113536,\"start\":113534},{\"end\":113542,\"start\":113540},{\"end\":113549,\"start\":113546},{\"end\":113556,\"start\":113553},{\"end\":113563,\"start\":113560},{\"end\":113570,\"start\":113567},{\"end\":113901,\"start\":113896},{\"end\":113908,\"start\":113905},{\"end\":113918,\"start\":113912},{\"end\":113928,\"start\":113924},{\"end\":113939,\"start\":113932},{\"end\":114244,\"start\":114236},{\"end\":114421,\"start\":114415},{\"end\":114432,\"start\":114425},{\"end\":114445,\"start\":114436},{\"end\":114454,\"start\":114449},{\"end\":114466,\"start\":114458},{\"end\":114808,\"start\":114805},{\"end\":114814,\"start\":114812},{\"end\":114822,\"start\":114818},{\"end\":114830,\"start\":114826},{\"end\":114838,\"start\":114834},{\"end\":115226,\"start\":115218},{\"end\":115236,\"start\":115230},{\"end\":115244,\"start\":115240},{\"end\":115257,\"start\":115248},{\"end\":115269,\"start\":115261},{\"end\":115279,\"start\":115273},{\"end\":115289,\"start\":115283},{\"end\":115301,\"start\":115293},{\"end\":115309,\"start\":115305},{\"end\":115320,\"start\":115313},{\"end\":115753,\"start\":115746},{\"end\":116032,\"start\":116029},{\"end\":116040,\"start\":116036},{\"end\":116048,\"start\":116044},{\"end\":116057,\"start\":116052},{\"end\":116380,\"start\":116374},{\"end\":116391,\"start\":116384},{\"end\":116402,\"start\":116395},{\"end\":116410,\"start\":116406},{\"end\":116888,\"start\":116886},{\"end\":116894,\"start\":116892},{\"end\":116900,\"start\":116898},{\"end\":116907,\"start\":116904},{\"end\":116913,\"start\":116911},{\"end\":116920,\"start\":116917},{\"end\":117260,\"start\":117252},{\"end\":117272,\"start\":117264},{\"end\":117283,\"start\":117278},{\"end\":117293,\"start\":117287},{\"end\":117732,\"start\":117728},{\"end\":117741,\"start\":117736},{\"end\":117749,\"start\":117745},{\"end\":117756,\"start\":117753},{\"end\":117763,\"start\":117760},{\"end\":118199,\"start\":118195},{\"end\":118205,\"start\":118203},{\"end\":118213,\"start\":118209},{\"end\":118221,\"start\":118217},{\"end\":118656,\"start\":118654},{\"end\":118665,\"start\":118660},{\"end\":118673,\"start\":118669},{\"end\":118681,\"start\":118677},{\"end\":118687,\"start\":118685},{\"end\":118981,\"start\":118975},{\"end\":118989,\"start\":118985},{\"end\":119361,\"start\":119348},{\"end\":119373,\"start\":119365},{\"end\":119388,\"start\":119379},{\"end\":119397,\"start\":119392},{\"end\":119407,\"start\":119401},{\"end\":119838,\"start\":119833},{\"end\":119846,\"start\":119842},{\"end\":119855,\"start\":119850},{\"end\":119864,\"start\":119861},{\"end\":120100,\"start\":120098},{\"end\":120109,\"start\":120104},{\"end\":120117,\"start\":120113},{\"end\":120549,\"start\":120546},{\"end\":120556,\"start\":120553},{\"end\":120563,\"start\":120560},{\"end\":120570,\"start\":120567},{\"end\":120836,\"start\":120832},{\"end\":120844,\"start\":120840},{\"end\":120850,\"start\":120848},{\"end\":120856,\"start\":120854},{\"end\":120862,\"start\":120860},{\"end\":120871,\"start\":120866},{\"end\":121073,\"start\":121070},{\"end\":121081,\"start\":121077},{\"end\":121087,\"start\":121085},{\"end\":121097,\"start\":121091},{\"end\":121104,\"start\":121101},{\"end\":121421,\"start\":121418},{\"end\":121428,\"start\":121425},{\"end\":121440,\"start\":121432},{\"end\":121447,\"start\":121444},{\"end\":121629,\"start\":121624},{\"end\":121635,\"start\":121633},{\"end\":121646,\"start\":121641},{\"end\":121655,\"start\":121650},{\"end\":121668,\"start\":121659},{\"end\":121936,\"start\":121932},{\"end\":121944,\"start\":121940},{\"end\":122244,\"start\":122233},{\"end\":122261,\"start\":122250},{\"end\":122271,\"start\":122267},{\"end\":122629,\"start\":122625},{\"end\":122635,\"start\":122633},{\"end\":122642,\"start\":122639},{\"end\":122649,\"start\":122646},{\"end\":122658,\"start\":122653},{\"end\":123008,\"start\":123001},{\"end\":123017,\"start\":123012},{\"end\":123027,\"start\":123021},{\"end\":123295,\"start\":123284},{\"end\":123313,\"start\":123301},{\"end\":123722,\"start\":123719},{\"end\":123731,\"start\":123726},{\"end\":123738,\"start\":123735},{\"end\":123744,\"start\":123742},{\"end\":124013,\"start\":124010},{\"end\":124021,\"start\":124017},{\"end\":124029,\"start\":124025},{\"end\":124035,\"start\":124033},{\"end\":124467,\"start\":124463},{\"end\":124473,\"start\":124471},{\"end\":124479,\"start\":124477},{\"end\":124487,\"start\":124483},{\"end\":124493,\"start\":124491},{\"end\":124501,\"start\":124497},{\"end\":124954,\"start\":124948},{\"end\":124967,\"start\":124958},{\"end\":124980,\"start\":124971},{\"end\":125392,\"start\":125386},{\"end\":125402,\"start\":125396},{\"end\":125412,\"start\":125406},{\"end\":125421,\"start\":125416},{\"end\":125435,\"start\":125425},{\"end\":125446,\"start\":125439},{\"end\":125454,\"start\":125450},{\"end\":125979,\"start\":125976},{\"end\":125987,\"start\":125983},{\"end\":125995,\"start\":125991},{\"end\":126540,\"start\":126535},{\"end\":126546,\"start\":126544},{\"end\":126555,\"start\":126550},{\"end\":126561,\"start\":126559},{\"end\":126568,\"start\":126565},{\"end\":126574,\"start\":126572},{\"end\":126581,\"start\":126578},{\"end\":126937,\"start\":126933},{\"end\":126944,\"start\":126941},{\"end\":126952,\"start\":126948},{\"end\":126961,\"start\":126956},{\"end\":126968,\"start\":126965},{\"end\":127287,\"start\":127284},{\"end\":127304,\"start\":127291},{\"end\":127318,\"start\":127310},{\"end\":127558,\"start\":127554},{\"end\":127567,\"start\":127562},{\"end\":127579,\"start\":127571},{\"end\":127981,\"start\":127975},{\"end\":127989,\"start\":127985},{\"end\":128000,\"start\":127993},{\"end\":128010,\"start\":128006},{\"end\":128487,\"start\":128478},{\"end\":128499,\"start\":128493},{\"end\":128510,\"start\":128505},{\"end\":128766,\"start\":128763},{\"end\":128777,\"start\":128772},{\"end\":128790,\"start\":128781},{\"end\":129081,\"start\":129077},{\"end\":129089,\"start\":129085},{\"end\":129099,\"start\":129095},{\"end\":129431,\"start\":129427},{\"end\":129440,\"start\":129435},{\"end\":129451,\"start\":129444},{\"end\":129460,\"start\":129455},{\"end\":129469,\"start\":129464},{\"end\":129915,\"start\":129908},{\"end\":129929,\"start\":129919},{\"end\":129939,\"start\":129933},{\"end\":129950,\"start\":129945},{\"end\":129969,\"start\":129956},{\"end\":129977,\"start\":129971},{\"end\":130413,\"start\":130408},{\"end\":130419,\"start\":130417},{\"end\":130427,\"start\":130423},{\"end\":130435,\"start\":130431},{\"end\":130876,\"start\":130873},{\"end\":130884,\"start\":130880},{\"end\":130890,\"start\":130888},{\"end\":130898,\"start\":130894},{\"end\":131128,\"start\":131125},{\"end\":131135,\"start\":131132},{\"end\":131142,\"start\":131139},{\"end\":131149,\"start\":131146},{\"end\":131365,\"start\":131360},{\"end\":131376,\"start\":131369},{\"end\":131393,\"start\":131382},{\"end\":131401,\"start\":131399},{\"end\":131411,\"start\":131405},{\"end\":131972,\"start\":131969},{\"end\":131979,\"start\":131976},{\"end\":131987,\"start\":131983},{\"end\":132002,\"start\":131993},{\"end\":132519,\"start\":132508},{\"end\":132530,\"start\":132523},{\"end\":132548,\"start\":132536},{\"end\":132747,\"start\":132745},{\"end\":132753,\"start\":132751},{\"end\":132762,\"start\":132757},{\"end\":132769,\"start\":132766},{\"end\":132775,\"start\":132773},{\"end\":132783,\"start\":132779},{\"end\":133048,\"start\":133043},{\"end\":133059,\"start\":133052},{\"end\":133068,\"start\":133063},{\"end\":133076,\"start\":133072},{\"end\":133090,\"start\":133084},{\"end\":133408,\"start\":133403},{\"end\":133417,\"start\":133412},{\"end\":133425,\"start\":133421},{\"end\":133434,\"start\":133429},{\"end\":133441,\"start\":133438},{\"end\":133723,\"start\":133718},{\"end\":133731,\"start\":133727},{\"end\":133739,\"start\":133735},{\"end\":133746,\"start\":133743},{\"end\":133752,\"start\":133750},{\"end\":134255,\"start\":134251},{\"end\":134262,\"start\":134259},{\"end\":134269,\"start\":134266},{\"end\":134275,\"start\":134273},{\"end\":134282,\"start\":134279},{\"end\":134559,\"start\":134556},{\"end\":134568,\"start\":134563},{\"end\":134578,\"start\":134574},{\"end\":135024,\"start\":135012},{\"end\":135036,\"start\":135028},{\"end\":135260,\"start\":135255},{\"end\":135268,\"start\":135264},{\"end\":135275,\"start\":135272},{\"end\":135281,\"start\":135279},{\"end\":135289,\"start\":135285},{\"end\":135296,\"start\":135293},{\"end\":135713,\"start\":135706},{\"end\":135727,\"start\":135717},{\"end\":135737,\"start\":135731},{\"end\":135756,\"start\":135743},{\"end\":136357,\"start\":136350},{\"end\":136370,\"start\":136361},{\"end\":136380,\"start\":136374},{\"end\":136399,\"start\":136386},{\"end\":136754,\"start\":136747},{\"end\":136765,\"start\":136760},{\"end\":137357,\"start\":137350},{\"end\":137367,\"start\":137363},{\"end\":137379,\"start\":137373},{\"end\":137637,\"start\":137630},{\"end\":137646,\"start\":137641},{\"end\":137658,\"start\":137652},{\"end\":137667,\"start\":137662},{\"end\":138220,\"start\":138214},{\"end\":138228,\"start\":138224},{\"end\":138242,\"start\":138234},{\"end\":138882,\"start\":138870},{\"end\":138892,\"start\":138886},{\"end\":138900,\"start\":138896},{\"end\":139335,\"start\":139329},{\"end\":139344,\"start\":139339},{\"end\":139357,\"start\":139350},{\"end\":139370,\"start\":139363},{\"end\":139378,\"start\":139374},{\"end\":139389,\"start\":139382},{\"end\":139399,\"start\":139395},{\"end\":139412,\"start\":139403},{\"end\":139426,\"start\":139420},{\"end\":139441,\"start\":139428},{\"end\":139908,\"start\":139906},{\"end\":139916,\"start\":139912},{\"end\":139924,\"start\":139920},{\"end\":139930,\"start\":139928},{\"end\":139940,\"start\":139936},{\"end\":140278,\"start\":140269},{\"end\":140287,\"start\":140282},{\"end\":140299,\"start\":140291},{\"end\":140629,\"start\":140624},{\"end\":140642,\"start\":140633},{\"end\":140653,\"start\":140646},{\"end\":140664,\"start\":140657},{\"end\":141010,\"start\":141003},{\"end\":141019,\"start\":141014},{\"end\":141030,\"start\":141023},{\"end\":141038,\"start\":141034},{\"end\":141046,\"start\":141042},{\"end\":141053,\"start\":141050},{\"end\":141399,\"start\":141394},{\"end\":141408,\"start\":141403},{\"end\":141420,\"start\":141412},{\"end\":141429,\"start\":141424},{\"end\":141995,\"start\":141991},{\"end\":142007,\"start\":142001},{\"end\":142018,\"start\":142013},{\"end\":142567,\"start\":142565},{\"end\":142579,\"start\":142573},{\"end\":142592,\"start\":142583},{\"end\":142603,\"start\":142596},{\"end\":143102,\"start\":143098},{\"end\":143110,\"start\":143106},{\"end\":143120,\"start\":143114},{\"end\":143128,\"start\":143126},{\"end\":143134,\"start\":143132},{\"end\":143145,\"start\":143138},{\"end\":143646,\"start\":143633},{\"end\":143657,\"start\":143650},{\"end\":143665,\"start\":143661},{\"end\":144252,\"start\":144248},{\"end\":144260,\"start\":144256},{\"end\":144272,\"start\":144264},{\"end\":144284,\"start\":144276},{\"end\":144296,\"start\":144290},{\"end\":144866,\"start\":144859},{\"end\":144879,\"start\":144872},{\"end\":144892,\"start\":144883},{\"end\":144903,\"start\":144896},{\"end\":144916,\"start\":144907},{\"end\":144926,\"start\":144920},{\"end\":144951,\"start\":144930},{\"end\":144959,\"start\":144955},{\"end\":144975,\"start\":144963},{\"end\":144991,\"start\":144979},{\"end\":145407,\"start\":145401},{\"end\":145417,\"start\":145411},{\"end\":145428,\"start\":145423},{\"end\":145434,\"start\":145432},{\"end\":145447,\"start\":145440},{\"end\":145457,\"start\":145453},{\"end\":145466,\"start\":145461},{\"end\":145793,\"start\":145788},{\"end\":145801,\"start\":145797},{\"end\":145812,\"start\":145805},{\"end\":146125,\"start\":146121},{\"end\":146133,\"start\":146129},{\"end\":146141,\"start\":146137},{\"end\":146403,\"start\":146394},{\"end\":146413,\"start\":146407},{\"end\":146425,\"start\":146417},{\"end\":146434,\"start\":146429},{\"end\":146444,\"start\":146438},{\"end\":146452,\"start\":146448},{\"end\":146460,\"start\":146456},{\"end\":146471,\"start\":146464},{\"end\":146481,\"start\":146475},{\"end\":146493,\"start\":146485},{\"end\":147011,\"start\":147007},{\"end\":147018,\"start\":147015},{\"end\":147027,\"start\":147022},{\"end\":147033,\"start\":147031},{\"end\":147500,\"start\":147490},{\"end\":147511,\"start\":147504},{\"end\":147920,\"start\":147910},{\"end\":147933,\"start\":147924},{\"end\":147945,\"start\":147939},{\"end\":148429,\"start\":148426},{\"end\":148438,\"start\":148435},{\"end\":148447,\"start\":148444},{\"end\":148455,\"start\":148451},{\"end\":148878,\"start\":148870},{\"end\":148902,\"start\":148886},{\"end\":148914,\"start\":148910},{\"end\":148926,\"start\":148918},{\"end\":148936,\"start\":148930},{\"end\":148954,\"start\":148944},{\"end\":149449,\"start\":149440},{\"end\":149458,\"start\":149453},{\"end\":149472,\"start\":149464},{\"end\":149754,\"start\":149748},{\"end\":149764,\"start\":149758},{\"end\":149990,\"start\":149986},{\"end\":150002,\"start\":149994},{\"end\":150012,\"start\":150006},{\"end\":150285,\"start\":150282},{\"end\":150295,\"start\":150291},{\"end\":150304,\"start\":150299},{\"end\":150311,\"start\":150308},{\"end\":150317,\"start\":150315},{\"end\":150624,\"start\":150618},{\"end\":150634,\"start\":150630},{\"end\":150856,\"start\":150851},{\"end\":150866,\"start\":150860},{\"end\":150873,\"start\":150870},{\"end\":150880,\"start\":150877},{\"end\":150887,\"start\":150884},{\"end\":150895,\"start\":150893},{\"end\":151379,\"start\":151373},{\"end\":151400,\"start\":151385},{\"end\":151697,\"start\":151694},{\"end\":151708,\"start\":151701},{\"end\":151718,\"start\":151712},{\"end\":151729,\"start\":151724},{\"end\":151993,\"start\":151986},{\"end\":152000,\"start\":151997},{\"end\":152015,\"start\":152006},{\"end\":152025,\"start\":152019},{\"end\":152036,\"start\":152031},{\"end\":152047,\"start\":152040},{\"end\":152059,\"start\":152049}]", "bib_entry": "[{\"attributes\":{\"doi\":\"10.1117/1.JMI.1.3.034003\",\"id\":\"b0\",\"matched_paper_id\":715578},\"end\":87425,\"start\":86847},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":16723641},\"end\":88106,\"start\":87427},{\"attributes\":{\"doi\":\"10.1109/TPAMI.2011.155\",\"id\":\"b2\",\"matched_paper_id\":206764948},\"end\":88465,\"start\":88108},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":1910869},\"end\":88752,\"start\":88467},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":16358367},\"end\":89285,\"start\":88754},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":52486932},\"end\":89718,\"start\":89287},{\"attributes\":{\"doi\":\"10.1016/j.neucom.2014.01.019\",\"id\":\"b6\",\"matched_paper_id\":6864924},\"end\":90037,\"start\":89720},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":749620},\"end\":90465,\"start\":90039},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":18739430},\"end\":90956,\"start\":90467},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":18018217},\"end\":91361,\"start\":90958},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":12561212},\"end\":91886,\"start\":91363},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":11904287},\"end\":92374,\"start\":91888},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":10627743},\"end\":92830,\"start\":92376},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":11636674},\"end\":93378,\"start\":92832},{\"attributes\":{\"id\":\"b14\"},\"end\":93889,\"start\":93380},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":206590483},\"end\":94392,\"start\":93891},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":9059102},\"end\":94937,\"start\":94394},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":70298800},\"end\":95392,\"start\":94939},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":187177909},\"end\":95752,\"start\":95394},{\"attributes\":{\"doi\":\"10.1016/j.cviu.2018.10.010\",\"id\":\"b19\",\"matched_paper_id\":44132158},\"end\":96044,\"start\":95754},{\"attributes\":{\"doi\":\"10.1016/j.neucom.2018.10.058\",\"id\":\"b20\",\"matched_paper_id\":67770295},\"end\":96367,\"start\":96046},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":67891072},\"end\":96771,\"start\":96369},{\"attributes\":{\"id\":\"b22\"},\"end\":96833,\"start\":96773},{\"attributes\":{\"doi\":\"10.1007/s00138-008-0132-4\",\"id\":\"b23\",\"matched_paper_id\":1417739},\"end\":97102,\"start\":96835},{\"attributes\":{\"doi\":\"10.1016/j.neucom.2015.12.070\",\"id\":\"b24\",\"matched_paper_id\":207112729},\"end\":97521,\"start\":97104},{\"attributes\":{\"doi\":\"10.1016/j.cviu.2014.07.008\",\"id\":\"b25\",\"matched_paper_id\":15166260},\"end\":97870,\"start\":97523},{\"attributes\":{\"doi\":\"10.1016/j.patrec.2017.07.007\",\"id\":\"b26\",\"matched_paper_id\":13709403},\"end\":98215,\"start\":97872},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":7768768},\"end\":98696,\"start\":98217},{\"attributes\":{\"doi\":\"10.1109/TMM.2016.2542585\",\"id\":\"b28\",\"matched_paper_id\":4236070},\"end\":99056,\"start\":98698},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":8614229},\"end\":99558,\"start\":99058},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":7625356},\"end\":100034,\"start\":99560},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":26754440},\"end\":100541,\"start\":100036},{\"attributes\":{\"doi\":\"10.1016/j.neunet.2014.08.005\",\"id\":\"b32\",\"matched_paper_id\":25303291},\"end\":100944,\"start\":100543},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":10042024},\"end\":101571,\"start\":100946},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":22047669},\"end\":102119,\"start\":101573},{\"attributes\":{\"doi\":\"10.3390/s17112589\",\"id\":\"b35\"},\"end\":102410,\"start\":102121},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":203605538},\"end\":103063,\"start\":102412},{\"attributes\":{\"doi\":\"10.3390/rs11050494\",\"id\":\"b37\"},\"end\":103335,\"start\":103065},{\"attributes\":{\"doi\":\"10.3390/diagnostics9020038\",\"id\":\"b38\"},\"end\":103691,\"start\":103337},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":206769463},\"end\":104245,\"start\":103693},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":13420116},\"end\":104692,\"start\":104247},{\"attributes\":{\"doi\":\"10.1109/TPAMI.2009.167\",\"id\":\"b41\",\"matched_paper_id\":3198903},\"end\":105082,\"start\":104694},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":37318664},\"end\":105429,\"start\":105084},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":5157313},\"end\":105999,\"start\":105431},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":7612598},\"end\":106494,\"start\":106001},{\"attributes\":{\"doi\":\"10.1109/MSP.2005.1511827\",\"id\":\"b45\",\"matched_paper_id\":18153514},\"end\":106759,\"start\":106496},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":6736412},\"end\":107129,\"start\":106761},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":2227746},\"end\":107566,\"start\":107131},{\"attributes\":{\"doi\":\"arXiv:1707.05363\",\"id\":\"b48\"},\"end\":107840,\"start\":107568},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":5464807},\"end\":108353,\"start\":107842},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":54462847},\"end\":108891,\"start\":108355},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":77393522},\"end\":109451,\"start\":108893},{\"attributes\":{\"doi\":\"10.3390/rs9010022\",\"id\":\"b52\"},\"end\":109810,\"start\":109453},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":46979377},\"end\":110369,\"start\":109812},{\"attributes\":{\"doi\":\"10.1109/TIP.2018.2875353\",\"id\":\"b54\",\"matched_paper_id\":52966459},\"end\":110685,\"start\":110371},{\"attributes\":{\"id\":\"b55\"},\"end\":110844,\"start\":110687},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":58616165},\"end\":111347,\"start\":110846},{\"attributes\":{\"doi\":\"10.1080/02640414.2018.1521769\",\"id\":\"b57\",\"matched_paper_id\":52957665},\"end\":111801,\"start\":111349},{\"attributes\":{\"doi\":\"10.1016/j.cag.2018.11.003\",\"id\":\"b58\",\"matched_paper_id\":59604993},\"end\":112052,\"start\":111803},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":53714519},\"end\":112471,\"start\":112054},{\"attributes\":{\"id\":\"b60\"},\"end\":112890,\"start\":112473},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":196060060},\"end\":113426,\"start\":112892},{\"attributes\":{\"doi\":\"10.1080/2150704X.2018.1557791\",\"id\":\"b62\",\"matched_paper_id\":127470644},\"end\":113825,\"start\":113428},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":126268078},\"end\":114232,\"start\":113827},{\"attributes\":{\"id\":\"b64\"},\"end\":114328,\"start\":114234},{\"attributes\":{\"doi\":\"10.3389/fnins.2019.00004\",\"id\":\"b65\",\"matched_paper_id\":67793031},\"end\":114712,\"start\":114330},{\"attributes\":{\"doi\":\"10.1007/s00521-018-3954-7\",\"id\":\"b66\",\"matched_paper_id\":57428011},\"end\":115058,\"start\":114714},{\"attributes\":{\"doi\":\"10.3174/ajnr.A5927\",\"id\":\"b67\",\"matched_paper_id\":58625918},\"end\":115700,\"start\":115060},{\"attributes\":{\"doi\":\"10.1016/j.jvlc.2008.01.002\",\"id\":\"b68\",\"matched_paper_id\":14486433},\"end\":115910,\"start\":115702},{\"attributes\":{\"id\":\"b69\"},\"end\":116303,\"start\":115912},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":9749221},\"end\":116816,\"start\":116305},{\"attributes\":{\"doi\":\"10.1016/j.engappai.2015.04.006\",\"id\":\"b71\",\"matched_paper_id\":31520628},\"end\":117148,\"start\":116818},{\"attributes\":{\"id\":\"b72\",\"matched_paper_id\":3179925},\"end\":117676,\"start\":117150},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":4509294},\"end\":118125,\"start\":117678},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":19922070},\"end\":118575,\"start\":118127},{\"attributes\":{\"doi\":\"10.1016/j.jvcir.2016.03.021\",\"id\":\"b75\",\"matched_paper_id\":6878285},\"end\":118934,\"start\":118577},{\"attributes\":{\"id\":\"b76\",\"matched_paper_id\":26569197},\"end\":119298,\"start\":118936},{\"attributes\":{\"id\":\"b77\",\"matched_paper_id\":10747799},\"end\":119781,\"start\":119300},{\"attributes\":{\"doi\":\"arXiv:1806.10287\",\"id\":\"b78\"},\"end\":120001,\"start\":119783},{\"attributes\":{\"id\":\"b79\",\"matched_paper_id\":3645757},\"end\":120542,\"start\":120003},{\"attributes\":{\"doi\":\"arXiv:1706.03686\",\"id\":\"b80\"},\"end\":120826,\"start\":120544},{\"attributes\":{\"doi\":\"arXiv:1806.10040\",\"id\":\"b81\"},\"end\":121066,\"start\":120828},{\"attributes\":{\"doi\":\"arXiv:1807.00601\",\"id\":\"b82\"},\"end\":121328,\"start\":121068},{\"attributes\":{\"doi\":\"arXiv:1803.08805\",\"id\":\"b83\"},\"end\":121620,\"start\":121330},{\"attributes\":{\"doi\":\"arXiv:1808.07456\",\"id\":\"b84\"},\"end\":121928,\"start\":121622},{\"attributes\":{\"doi\":\"arXiv:1805.06115\",\"id\":\"b85\"},\"end\":122164,\"start\":121930},{\"attributes\":{\"id\":\"b86\",\"matched_paper_id\":697405},\"end\":122621,\"start\":122166},{\"attributes\":{\"doi\":\"arXiv:1702.02359\",\"id\":\"b87\"},\"end\":122887,\"start\":122623},{\"attributes\":{\"doi\":\"arXiv:1703.09393\",\"id\":\"b88\"},\"end\":123219,\"start\":122889},{\"attributes\":{\"id\":\"b89\",\"matched_paper_id\":40499053},\"end\":123661,\"start\":123221},{\"attributes\":{\"doi\":\"10.1109/TII.2018.2852481\",\"id\":\"b90\",\"matched_paper_id\":53281005},\"end\":123937,\"start\":123663},{\"attributes\":{\"id\":\"b91\",\"matched_paper_id\":52955811},\"end\":124395,\"start\":123939},{\"attributes\":{\"id\":\"b92\",\"matched_paper_id\":52860247},\"end\":124922,\"start\":124397},{\"attributes\":{\"id\":\"b93\",\"matched_paper_id\":40576626},\"end\":125294,\"start\":124924},{\"attributes\":{\"id\":\"b94\",\"matched_paper_id\":51901514},\"end\":125857,\"start\":125296},{\"attributes\":{\"id\":\"b95\",\"matched_paper_id\":49654180},\"end\":126489,\"start\":125859},{\"attributes\":{\"doi\":\"10.1109/TIP.2017.2740160\",\"id\":\"b96\",\"matched_paper_id\":9552718},\"end\":126814,\"start\":126491},{\"attributes\":{\"doi\":\"10.1016/j.image.2018.03.004\",\"id\":\"b97\",\"matched_paper_id\":4949878},\"end\":127219,\"start\":126816},{\"attributes\":{\"doi\":\"arXiv:1803.03095\",\"id\":\"b98\",\"matched_paper_id\":3787969},\"end\":127480,\"start\":127221},{\"attributes\":{\"id\":\"b99\",\"matched_paper_id\":14412566},\"end\":127934,\"start\":127482},{\"attributes\":{\"id\":\"b100\",\"matched_paper_id\":3796878},\"end\":128398,\"start\":127936},{\"attributes\":{\"doi\":\"arXiv:1802.10548\",\"id\":\"b101\"},\"end\":128674,\"start\":128400},{\"attributes\":{\"doi\":\"10.1080/21681163.2016.1149104\",\"id\":\"b102\",\"matched_paper_id\":19076929},\"end\":129073,\"start\":128676},{\"attributes\":{\"doi\":\"arXiv:1611.06748\",\"id\":\"b103\"},\"end\":129349,\"start\":129075},{\"attributes\":{\"id\":\"b104\",\"matched_paper_id\":13448486},\"end\":129904,\"start\":129351},{\"attributes\":{\"doi\":\"arXiv:1711.05586\",\"id\":\"b105\"},\"end\":130337,\"start\":129906},{\"attributes\":{\"id\":\"b106\",\"matched_paper_id\":2131202},\"end\":130867,\"start\":130339},{\"attributes\":{\"doi\":\"arXiv:1807.01989\",\"id\":\"b107\"},\"end\":131065,\"start\":130869},{\"attributes\":{\"doi\":\"arXiv:1710.09757\",\"id\":\"b108\"},\"end\":131287,\"start\":131067},{\"attributes\":{\"id\":\"b109\",\"matched_paper_id\":4722453},\"end\":131863,\"start\":131289},{\"attributes\":{\"id\":\"b110\",\"matched_paper_id\":3740753},\"end\":132452,\"start\":131865},{\"attributes\":{\"doi\":\"arXiv:1805.02919\",\"id\":\"b111\"},\"end\":132696,\"start\":132454},{\"attributes\":{\"doi\":\"arXiv:1803.02256\",\"id\":\"b112\",\"matched_paper_id\":3743115},\"end\":132971,\"start\":132698},{\"attributes\":{\"doi\":\"10.1109/TCSVT.2018.2803115\",\"id\":\"b113\",\"matched_paper_id\":65519416},\"end\":133346,\"start\":132973},{\"attributes\":{\"doi\":\"10.1016/j.neucom.2017.08.018\",\"id\":\"b114\",\"matched_paper_id\":1775911},\"end\":133639,\"start\":133348},{\"attributes\":{\"id\":\"b115\",\"matched_paper_id\":4545310},\"end\":134170,\"start\":133641},{\"attributes\":{\"doi\":\"10.1007/s10489-018-1150-1\",\"id\":\"b116\",\"matched_paper_id\":3479556},\"end\":134491,\"start\":134172},{\"attributes\":{\"id\":\"b117\",\"matched_paper_id\":1089358},\"end\":134947,\"start\":134493},{\"attributes\":{\"doi\":\"10.3390/s17040905\",\"id\":\"b118\"},\"end\":135185,\"start\":134949},{\"attributes\":{\"doi\":\"10.1109/TCSVT.2016.2637379\",\"id\":\"b119\",\"matched_paper_id\":3820195},\"end\":135563,\"start\":135187},{\"attributes\":{\"id\":\"b120\",\"matched_paper_id\":7134884},\"end\":136346,\"start\":135565},{\"attributes\":{\"doi\":\"arXiv:1612.00220\",\"id\":\"b121\"},\"end\":136639,\"start\":136348},{\"attributes\":{\"id\":\"b122\",\"matched_paper_id\":3003101},\"end\":137289,\"start\":136641},{\"attributes\":{\"id\":\"b123\"},\"end\":137530,\"start\":137291},{\"attributes\":{\"id\":\"b124\",\"matched_paper_id\":3080611},\"end\":138098,\"start\":137532},{\"attributes\":{\"id\":\"b125\",\"matched_paper_id\":8134281},\"end\":138752,\"start\":138100},{\"attributes\":{\"doi\":\"10.1016/j.rse.2018.06.028\",\"id\":\"b126\",\"matched_paper_id\":49553739},\"end\":139157,\"start\":138754},{\"attributes\":{\"doi\":\"10.1186/1476-072X-10-67\",\"id\":\"b127\"},\"end\":139837,\"start\":139159},{\"attributes\":{\"doi\":\"10.1109/TITS.2014.2345663\",\"id\":\"b128\",\"matched_paper_id\":7079419},\"end\":140178,\"start\":139839},{\"attributes\":{\"doi\":\"arXiv:1701.01909\",\"id\":\"b129\"},\"end\":140472,\"start\":140180},{\"attributes\":{\"id\":\"b130\"},\"end\":140939,\"start\":140474},{\"attributes\":{\"doi\":\"10.1680/jsmic.17.00001\",\"id\":\"b131\",\"matched_paper_id\":55932419},\"end\":141322,\"start\":140941},{\"attributes\":{\"id\":\"b132\",\"matched_paper_id\":3185072},\"end\":141907,\"start\":141324},{\"attributes\":{\"id\":\"b133\",\"matched_paper_id\":5968923},\"end\":142478,\"start\":141909},{\"attributes\":{\"id\":\"b134\",\"matched_paper_id\":12359230},\"end\":143041,\"start\":142480},{\"attributes\":{\"id\":\"b135\",\"matched_paper_id\":57246310},\"end\":143556,\"start\":143043},{\"attributes\":{\"id\":\"b136\",\"matched_paper_id\":53738736},\"end\":144158,\"start\":143558},{\"attributes\":{\"id\":\"b137\",\"matched_paper_id\":4302067},\"end\":144761,\"start\":144160},{\"attributes\":{\"doi\":\"10.1038/srep26286\",\"id\":\"b138\",\"matched_paper_id\":237132},\"end\":145320,\"start\":144763},{\"attributes\":{\"doi\":\"10.1038/nature21056\",\"id\":\"b139\",\"matched_paper_id\":3767412},\"end\":145690,\"start\":145322},{\"attributes\":{\"doi\":\"10.1016/j.compeleceng.2013.02.008\",\"id\":\"b140\",\"matched_paper_id\":45156073},\"end\":146049,\"start\":145692},{\"attributes\":{\"doi\":\"10.1007/s11263-014-0735-3\",\"id\":\"b141\",\"matched_paper_id\":5962058},\"end\":146338,\"start\":146051},{\"attributes\":{\"id\":\"b142\",\"matched_paper_id\":134843841},\"end\":146929,\"start\":146340},{\"attributes\":{\"id\":\"b143\",\"matched_paper_id\":16372392},\"end\":147453,\"start\":146931},{\"attributes\":{\"id\":\"b144\",\"matched_paper_id\":14385359},\"end\":147841,\"start\":147455},{\"attributes\":{\"id\":\"b145\",\"matched_paper_id\":195908774},\"end\":148325,\"start\":147843},{\"attributes\":{\"doi\":\"10.1016/j.neuroimage.2016.01.005\",\"id\":\"b146\",\"matched_paper_id\":4642201},\"end\":148682,\"start\":148327},{\"attributes\":{\"doi\":\"10.1016/j.compmedimag.2018.02.002\",\"id\":\"b147\",\"matched_paper_id\":206943740},\"end\":149365,\"start\":148684},{\"attributes\":{\"doi\":\"10.1126/science.290.5500.2319\",\"id\":\"b148\",\"matched_paper_id\":221338160},\"end\":149670,\"start\":149367},{\"attributes\":{\"doi\":\"10.1162/089976603321780317\",\"id\":\"b149\",\"matched_paper_id\":14879317},\"end\":149952,\"start\":149672},{\"attributes\":{\"doi\":\"10.1016/0169-7439(87)80084-9\",\"id\":\"b150\",\"matched_paper_id\":2534141},\"end\":150188,\"start\":149954},{\"attributes\":{\"doi\":\"10.1016/S0925-2312(03)00433-8\",\"id\":\"b151\",\"matched_paper_id\":30616477},\"end\":150548,\"start\":150190},{\"attributes\":{\"doi\":\"10.1126/science.290.5500.2323\",\"id\":\"b152\",\"matched_paper_id\":5987139},\"end\":150821,\"start\":150550},{\"attributes\":{\"id\":\"b153\",\"matched_paper_id\":352650},\"end\":151277,\"start\":150823},{\"attributes\":{\"doi\":\"10.1007/s13319-017-0136-9\",\"id\":\"b154\"},\"end\":151614,\"start\":151279},{\"attributes\":{\"id\":\"b155\",\"matched_paper_id\":14419924},\"end\":151980,\"start\":151616},{\"attributes\":{\"doi\":\"arXiv:1602.07360\",\"id\":\"b156\"},\"end\":152782,\"start\":151982}]", "bib_title": "[{\"end\":86965,\"start\":86847},{\"end\":87524,\"start\":87427},{\"end\":88167,\"start\":88108},{\"end\":88510,\"start\":88467},{\"end\":88816,\"start\":88754},{\"end\":89329,\"start\":89287},{\"end\":89786,\"start\":89720},{\"end\":90085,\"start\":90039},{\"end\":90527,\"start\":90467},{\"end\":90993,\"start\":90958},{\"end\":91453,\"start\":91363},{\"end\":91960,\"start\":91888},{\"end\":92407,\"start\":92376},{\"end\":92895,\"start\":92832},{\"end\":93943,\"start\":93891},{\"end\":94480,\"start\":94394},{\"end\":94992,\"start\":94939},{\"end\":95447,\"start\":95394},{\"end\":95820,\"start\":95754},{\"end\":96120,\"start\":96046},{\"end\":96468,\"start\":96369},{\"end\":96859,\"start\":96835},{\"end\":97214,\"start\":97104},{\"end\":97594,\"start\":97523},{\"end\":97963,\"start\":97872},{\"end\":98274,\"start\":98217},{\"end\":98773,\"start\":98698},{\"end\":99099,\"start\":99058},{\"end\":99609,\"start\":99560},{\"end\":100086,\"start\":100036},{\"end\":100606,\"start\":100543},{\"end\":101039,\"start\":100946},{\"end\":101673,\"start\":101573},{\"end\":102478,\"start\":102412},{\"end\":103809,\"start\":103693},{\"end\":104298,\"start\":104247},{\"end\":104758,\"start\":104694},{\"end\":105165,\"start\":105084},{\"end\":105545,\"start\":105431},{\"end\":106053,\"start\":106001},{\"end\":106548,\"start\":106496},{\"end\":106840,\"start\":106761},{\"end\":107177,\"start\":107131},{\"end\":107908,\"start\":107842},{\"end\":108424,\"start\":108355},{\"end\":108967,\"start\":108893},{\"end\":109865,\"start\":109812},{\"end\":110431,\"start\":110371},{\"end\":110950,\"start\":110846},{\"end\":111472,\"start\":111349},{\"end\":111855,\"start\":111803},{\"end\":112113,\"start\":112054},{\"end\":113005,\"start\":112892},{\"end\":113530,\"start\":113428},{\"end\":113890,\"start\":113827},{\"end\":114411,\"start\":114330},{\"end\":114801,\"start\":114714},{\"end\":115214,\"start\":115060},{\"end\":115742,\"start\":115702},{\"end\":116370,\"start\":116305},{\"end\":116882,\"start\":116818},{\"end\":117246,\"start\":117150},{\"end\":117724,\"start\":117678},{\"end\":118191,\"start\":118127},{\"end\":118650,\"start\":118577},{\"end\":118971,\"start\":118936},{\"end\":119344,\"start\":119300},{\"end\":120094,\"start\":120003},{\"end\":122229,\"start\":122166},{\"end\":123280,\"start\":123221},{\"end\":123715,\"start\":123663},{\"end\":124006,\"start\":123939},{\"end\":124459,\"start\":124397},{\"end\":124944,\"start\":124924},{\"end\":125382,\"start\":125296},{\"end\":125972,\"start\":125859},{\"end\":126531,\"start\":126491},{\"end\":126929,\"start\":126816},{\"end\":127280,\"start\":127221},{\"end\":127550,\"start\":127482},{\"end\":127971,\"start\":127936},{\"end\":128759,\"start\":128676},{\"end\":129423,\"start\":129351},{\"end\":130404,\"start\":130339},{\"end\":131354,\"start\":131289},{\"end\":131965,\"start\":131865},{\"end\":132741,\"start\":132698},{\"end\":133039,\"start\":132973},{\"end\":133399,\"start\":133348},{\"end\":133714,\"start\":133641},{\"end\":134247,\"start\":134172},{\"end\":134550,\"start\":134493},{\"end\":135251,\"start\":135187},{\"end\":135702,\"start\":135565},{\"end\":136741,\"start\":136641},{\"end\":137344,\"start\":137291},{\"end\":137626,\"start\":137532},{\"end\":138210,\"start\":138100},{\"end\":138866,\"start\":138754},{\"end\":139321,\"start\":139159},{\"end\":139902,\"start\":139839},{\"end\":140999,\"start\":140941},{\"end\":141390,\"start\":141324},{\"end\":141985,\"start\":141909},{\"end\":142559,\"start\":142480},{\"end\":143094,\"start\":143043},{\"end\":143629,\"start\":143558},{\"end\":144244,\"start\":144160},{\"end\":144855,\"start\":144763},{\"end\":145397,\"start\":145322},{\"end\":145784,\"start\":145692},{\"end\":146117,\"start\":146051},{\"end\":146390,\"start\":146340},{\"end\":147003,\"start\":146931},{\"end\":147486,\"start\":147455},{\"end\":147906,\"start\":147843},{\"end\":148420,\"start\":148327},{\"end\":148864,\"start\":148684},{\"end\":149434,\"start\":149367},{\"end\":149744,\"start\":149672},{\"end\":149982,\"start\":149954},{\"end\":150278,\"start\":150190},{\"end\":150612,\"start\":150550},{\"end\":150847,\"start\":150823},{\"end\":151690,\"start\":151616}]", "bib_author": "[{\"end\":86975,\"start\":86967},{\"end\":86984,\"start\":86975},{\"end\":87002,\"start\":86984},{\"end\":87015,\"start\":87002},{\"end\":87023,\"start\":87015},{\"end\":87034,\"start\":87023},{\"end\":87049,\"start\":87034},{\"end\":87061,\"start\":87049},{\"end\":87075,\"start\":87061},{\"end\":87534,\"start\":87526},{\"end\":87546,\"start\":87534},{\"end\":87562,\"start\":87546},{\"end\":87573,\"start\":87562},{\"end\":87581,\"start\":87573},{\"end\":87592,\"start\":87581},{\"end\":87607,\"start\":87592},{\"end\":87619,\"start\":87607},{\"end\":87633,\"start\":87619},{\"end\":88179,\"start\":88169},{\"end\":88188,\"start\":88179},{\"end\":88199,\"start\":88188},{\"end\":88209,\"start\":88199},{\"end\":88520,\"start\":88512},{\"end\":88529,\"start\":88520},{\"end\":88537,\"start\":88529},{\"end\":88546,\"start\":88537},{\"end\":88829,\"start\":88818},{\"end\":88838,\"start\":88829},{\"end\":88846,\"start\":88838},{\"end\":88861,\"start\":88846},{\"end\":89346,\"start\":89331},{\"end\":89359,\"start\":89346},{\"end\":89374,\"start\":89359},{\"end\":89797,\"start\":89788},{\"end\":89804,\"start\":89797},{\"end\":89813,\"start\":89804},{\"end\":89820,\"start\":89813},{\"end\":89828,\"start\":89820},{\"end\":90097,\"start\":90087},{\"end\":90112,\"start\":90097},{\"end\":90535,\"start\":90529},{\"end\":90544,\"start\":90535},{\"end\":90553,\"start\":90544},{\"end\":90559,\"start\":90553},{\"end\":91008,\"start\":90995},{\"end\":91021,\"start\":91008},{\"end\":91466,\"start\":91455},{\"end\":91477,\"start\":91466},{\"end\":91493,\"start\":91477},{\"end\":91506,\"start\":91493},{\"end\":91974,\"start\":91962},{\"end\":91984,\"start\":91974},{\"end\":91993,\"start\":91984},{\"end\":92419,\"start\":92409},{\"end\":92431,\"start\":92419},{\"end\":92910,\"start\":92897},{\"end\":92921,\"start\":92910},{\"end\":93471,\"start\":93459},{\"end\":93484,\"start\":93471},{\"end\":93495,\"start\":93484},{\"end\":93508,\"start\":93495},{\"end\":93954,\"start\":93945},{\"end\":93964,\"start\":93954},{\"end\":94492,\"start\":94482},{\"end\":94505,\"start\":94492},{\"end\":94520,\"start\":94505},{\"end\":95003,\"start\":94994},{\"end\":95015,\"start\":95003},{\"end\":95025,\"start\":95015},{\"end\":95035,\"start\":95025},{\"end\":95457,\"start\":95449},{\"end\":95470,\"start\":95457},{\"end\":95482,\"start\":95470},{\"end\":95831,\"start\":95822},{\"end\":95841,\"start\":95831},{\"end\":96131,\"start\":96122},{\"end\":96139,\"start\":96131},{\"end\":96148,\"start\":96139},{\"end\":96157,\"start\":96148},{\"end\":96480,\"start\":96470},{\"end\":96491,\"start\":96480},{\"end\":96501,\"start\":96491},{\"end\":96785,\"start\":96775},{\"end\":96869,\"start\":96861},{\"end\":96884,\"start\":96869},{\"end\":96897,\"start\":96884},{\"end\":96911,\"start\":96897},{\"end\":96919,\"start\":96911},{\"end\":97229,\"start\":97216},{\"end\":97240,\"start\":97229},{\"end\":97248,\"start\":97240},{\"end\":97263,\"start\":97248},{\"end\":97604,\"start\":97596},{\"end\":97614,\"start\":97604},{\"end\":97627,\"start\":97614},{\"end\":97637,\"start\":97627},{\"end\":97978,\"start\":97965},{\"end\":97989,\"start\":97978},{\"end\":98284,\"start\":98276},{\"end\":98292,\"start\":98284},{\"end\":98299,\"start\":98292},{\"end\":98307,\"start\":98299},{\"end\":98784,\"start\":98775},{\"end\":98792,\"start\":98784},{\"end\":98798,\"start\":98792},{\"end\":98806,\"start\":98798},{\"end\":98813,\"start\":98806},{\"end\":98821,\"start\":98813},{\"end\":99113,\"start\":99101},{\"end\":99130,\"start\":99113},{\"end\":99140,\"start\":99130},{\"end\":99152,\"start\":99140},{\"end\":99161,\"start\":99152},{\"end\":99620,\"start\":99611},{\"end\":99635,\"start\":99620},{\"end\":99646,\"start\":99635},{\"end\":100095,\"start\":100088},{\"end\":100102,\"start\":100095},{\"end\":100109,\"start\":100102},{\"end\":100621,\"start\":100608},{\"end\":100634,\"start\":100621},{\"end\":100642,\"start\":100634},{\"end\":100652,\"start\":100642},{\"end\":100665,\"start\":100652},{\"end\":100673,\"start\":100665},{\"end\":100688,\"start\":100673},{\"end\":101056,\"start\":101041},{\"end\":101069,\"start\":101056},{\"end\":101078,\"start\":101069},{\"end\":101086,\"start\":101078},{\"end\":101685,\"start\":101675},{\"end\":101696,\"start\":101685},{\"end\":101717,\"start\":101696},{\"end\":102231,\"start\":102221},{\"end\":102498,\"start\":102480},{\"end\":102509,\"start\":102498},{\"end\":102522,\"start\":102509},{\"end\":102535,\"start\":102522},{\"end\":102548,\"start\":102535},{\"end\":102557,\"start\":102548},{\"end\":102570,\"start\":102557},{\"end\":102582,\"start\":102570},{\"end\":103135,\"start\":103126},{\"end\":103143,\"start\":103135},{\"end\":103151,\"start\":103143},{\"end\":103344,\"start\":103337},{\"end\":103357,\"start\":103344},{\"end\":103367,\"start\":103357},{\"end\":103817,\"start\":103811},{\"end\":103828,\"start\":103817},{\"end\":104315,\"start\":104300},{\"end\":104323,\"start\":104315},{\"end\":104778,\"start\":104760},{\"end\":104792,\"start\":104778},{\"end\":104806,\"start\":104792},{\"end\":104817,\"start\":104806},{\"end\":105176,\"start\":105167},{\"end\":105186,\"start\":105176},{\"end\":105196,\"start\":105186},{\"end\":105553,\"start\":105547},{\"end\":105562,\"start\":105553},{\"end\":105571,\"start\":105562},{\"end\":105578,\"start\":105571},{\"end\":106067,\"start\":106055},{\"end\":106077,\"start\":106067},{\"end\":106562,\"start\":106550},{\"end\":106573,\"start\":106562},{\"end\":106854,\"start\":106842},{\"end\":106861,\"start\":106854},{\"end\":106869,\"start\":106861},{\"end\":107189,\"start\":107179},{\"end\":107199,\"start\":107189},{\"end\":107209,\"start\":107199},{\"end\":107216,\"start\":107209},{\"end\":107649,\"start\":107643},{\"end\":107657,\"start\":107649},{\"end\":107665,\"start\":107657},{\"end\":107671,\"start\":107665},{\"end\":107677,\"start\":107671},{\"end\":107919,\"start\":107910},{\"end\":107926,\"start\":107919},{\"end\":107932,\"start\":107926},{\"end\":107942,\"start\":107932},{\"end\":107948,\"start\":107942},{\"end\":108438,\"start\":108426},{\"end\":108448,\"start\":108438},{\"end\":108981,\"start\":108969},{\"end\":108991,\"start\":108981},{\"end\":109000,\"start\":108991},{\"end\":109559,\"start\":109553},{\"end\":109565,\"start\":109559},{\"end\":109571,\"start\":109565},{\"end\":109584,\"start\":109571},{\"end\":109874,\"start\":109867},{\"end\":109882,\"start\":109874},{\"end\":109890,\"start\":109882},{\"end\":109899,\"start\":109890},{\"end\":110442,\"start\":110433},{\"end\":110454,\"start\":110442},{\"end\":110470,\"start\":110454},{\"end\":110729,\"start\":110719},{\"end\":110958,\"start\":110952},{\"end\":110966,\"start\":110958},{\"end\":110976,\"start\":110966},{\"end\":110983,\"start\":110976},{\"end\":110989,\"start\":110983},{\"end\":111002,\"start\":110989},{\"end\":111013,\"start\":111002},{\"end\":111023,\"start\":111013},{\"end\":111036,\"start\":111023},{\"end\":111046,\"start\":111036},{\"end\":111484,\"start\":111474},{\"end\":111498,\"start\":111484},{\"end\":111506,\"start\":111498},{\"end\":111519,\"start\":111506},{\"end\":111866,\"start\":111857},{\"end\":111875,\"start\":111866},{\"end\":111883,\"start\":111875},{\"end\":112125,\"start\":112115},{\"end\":112136,\"start\":112125},{\"end\":112144,\"start\":112136},{\"end\":112152,\"start\":112144},{\"end\":112160,\"start\":112152},{\"end\":112178,\"start\":112160},{\"end\":112187,\"start\":112178},{\"end\":112199,\"start\":112187},{\"end\":112210,\"start\":112199},{\"end\":112224,\"start\":112210},{\"end\":112601,\"start\":112592},{\"end\":112612,\"start\":112601},{\"end\":112621,\"start\":112612},{\"end\":112633,\"start\":112621},{\"end\":113019,\"start\":113007},{\"end\":113028,\"start\":113019},{\"end\":113038,\"start\":113028},{\"end\":113048,\"start\":113038},{\"end\":113058,\"start\":113048},{\"end\":113538,\"start\":113532},{\"end\":113544,\"start\":113538},{\"end\":113551,\"start\":113544},{\"end\":113558,\"start\":113551},{\"end\":113565,\"start\":113558},{\"end\":113572,\"start\":113565},{\"end\":113903,\"start\":113892},{\"end\":113910,\"start\":113903},{\"end\":113920,\"start\":113910},{\"end\":113930,\"start\":113920},{\"end\":113941,\"start\":113930},{\"end\":114246,\"start\":114236},{\"end\":114423,\"start\":114413},{\"end\":114434,\"start\":114423},{\"end\":114447,\"start\":114434},{\"end\":114456,\"start\":114447},{\"end\":114468,\"start\":114456},{\"end\":114810,\"start\":114803},{\"end\":114816,\"start\":114810},{\"end\":114824,\"start\":114816},{\"end\":114832,\"start\":114824},{\"end\":114840,\"start\":114832},{\"end\":115228,\"start\":115216},{\"end\":115238,\"start\":115228},{\"end\":115246,\"start\":115238},{\"end\":115259,\"start\":115246},{\"end\":115271,\"start\":115259},{\"end\":115281,\"start\":115271},{\"end\":115291,\"start\":115281},{\"end\":115303,\"start\":115291},{\"end\":115311,\"start\":115303},{\"end\":115322,\"start\":115311},{\"end\":115755,\"start\":115744},{\"end\":116034,\"start\":116025},{\"end\":116042,\"start\":116034},{\"end\":116050,\"start\":116042},{\"end\":116059,\"start\":116050},{\"end\":116382,\"start\":116372},{\"end\":116393,\"start\":116382},{\"end\":116404,\"start\":116393},{\"end\":116412,\"start\":116404},{\"end\":116890,\"start\":116884},{\"end\":116896,\"start\":116890},{\"end\":116902,\"start\":116896},{\"end\":116909,\"start\":116902},{\"end\":116915,\"start\":116909},{\"end\":116922,\"start\":116915},{\"end\":117262,\"start\":117248},{\"end\":117274,\"start\":117262},{\"end\":117285,\"start\":117274},{\"end\":117295,\"start\":117285},{\"end\":117734,\"start\":117726},{\"end\":117743,\"start\":117734},{\"end\":117751,\"start\":117743},{\"end\":117758,\"start\":117751},{\"end\":117765,\"start\":117758},{\"end\":118201,\"start\":118193},{\"end\":118207,\"start\":118201},{\"end\":118215,\"start\":118207},{\"end\":118223,\"start\":118215},{\"end\":118658,\"start\":118652},{\"end\":118667,\"start\":118658},{\"end\":118675,\"start\":118667},{\"end\":118683,\"start\":118675},{\"end\":118689,\"start\":118683},{\"end\":118983,\"start\":118973},{\"end\":118991,\"start\":118983},{\"end\":119363,\"start\":119346},{\"end\":119375,\"start\":119363},{\"end\":119390,\"start\":119375},{\"end\":119399,\"start\":119390},{\"end\":119409,\"start\":119399},{\"end\":119840,\"start\":119831},{\"end\":119848,\"start\":119840},{\"end\":119857,\"start\":119848},{\"end\":119866,\"start\":119857},{\"end\":120102,\"start\":120096},{\"end\":120111,\"start\":120102},{\"end\":120119,\"start\":120111},{\"end\":120551,\"start\":120544},{\"end\":120558,\"start\":120551},{\"end\":120565,\"start\":120558},{\"end\":120572,\"start\":120565},{\"end\":120838,\"start\":120830},{\"end\":120846,\"start\":120838},{\"end\":120852,\"start\":120846},{\"end\":120858,\"start\":120852},{\"end\":120864,\"start\":120858},{\"end\":120873,\"start\":120864},{\"end\":121075,\"start\":121068},{\"end\":121083,\"start\":121075},{\"end\":121089,\"start\":121083},{\"end\":121099,\"start\":121089},{\"end\":121106,\"start\":121099},{\"end\":121423,\"start\":121416},{\"end\":121430,\"start\":121423},{\"end\":121442,\"start\":121430},{\"end\":121449,\"start\":121442},{\"end\":121631,\"start\":121622},{\"end\":121637,\"start\":121631},{\"end\":121648,\"start\":121637},{\"end\":121657,\"start\":121648},{\"end\":121670,\"start\":121657},{\"end\":121938,\"start\":121930},{\"end\":121946,\"start\":121938},{\"end\":122246,\"start\":122231},{\"end\":122263,\"start\":122246},{\"end\":122273,\"start\":122263},{\"end\":122631,\"start\":122623},{\"end\":122637,\"start\":122631},{\"end\":122644,\"start\":122637},{\"end\":122651,\"start\":122644},{\"end\":122660,\"start\":122651},{\"end\":123010,\"start\":122999},{\"end\":123019,\"start\":123010},{\"end\":123029,\"start\":123019},{\"end\":123297,\"start\":123282},{\"end\":123315,\"start\":123297},{\"end\":123724,\"start\":123717},{\"end\":123733,\"start\":123724},{\"end\":123740,\"start\":123733},{\"end\":123746,\"start\":123740},{\"end\":124015,\"start\":124008},{\"end\":124023,\"start\":124015},{\"end\":124031,\"start\":124023},{\"end\":124037,\"start\":124031},{\"end\":124469,\"start\":124461},{\"end\":124475,\"start\":124469},{\"end\":124481,\"start\":124475},{\"end\":124489,\"start\":124481},{\"end\":124495,\"start\":124489},{\"end\":124503,\"start\":124495},{\"end\":124956,\"start\":124946},{\"end\":124969,\"start\":124956},{\"end\":124982,\"start\":124969},{\"end\":125394,\"start\":125384},{\"end\":125404,\"start\":125394},{\"end\":125414,\"start\":125404},{\"end\":125423,\"start\":125414},{\"end\":125437,\"start\":125423},{\"end\":125448,\"start\":125437},{\"end\":125456,\"start\":125448},{\"end\":125981,\"start\":125974},{\"end\":125989,\"start\":125981},{\"end\":125997,\"start\":125989},{\"end\":126542,\"start\":126533},{\"end\":126548,\"start\":126542},{\"end\":126557,\"start\":126548},{\"end\":126563,\"start\":126557},{\"end\":126570,\"start\":126563},{\"end\":126576,\"start\":126570},{\"end\":126583,\"start\":126576},{\"end\":126939,\"start\":126931},{\"end\":126946,\"start\":126939},{\"end\":126954,\"start\":126946},{\"end\":126963,\"start\":126954},{\"end\":126970,\"start\":126963},{\"end\":127289,\"start\":127282},{\"end\":127306,\"start\":127289},{\"end\":127320,\"start\":127306},{\"end\":127560,\"start\":127552},{\"end\":127569,\"start\":127560},{\"end\":127581,\"start\":127569},{\"end\":127983,\"start\":127973},{\"end\":127991,\"start\":127983},{\"end\":128002,\"start\":127991},{\"end\":128012,\"start\":128002},{\"end\":128489,\"start\":128474},{\"end\":128501,\"start\":128489},{\"end\":128512,\"start\":128501},{\"end\":128768,\"start\":128761},{\"end\":128779,\"start\":128768},{\"end\":128792,\"start\":128779},{\"end\":129083,\"start\":129075},{\"end\":129091,\"start\":129083},{\"end\":129101,\"start\":129091},{\"end\":129433,\"start\":129425},{\"end\":129442,\"start\":129433},{\"end\":129453,\"start\":129442},{\"end\":129462,\"start\":129453},{\"end\":129471,\"start\":129462},{\"end\":129917,\"start\":129906},{\"end\":129931,\"start\":129917},{\"end\":129941,\"start\":129931},{\"end\":129952,\"start\":129941},{\"end\":129971,\"start\":129952},{\"end\":129979,\"start\":129971},{\"end\":130415,\"start\":130406},{\"end\":130421,\"start\":130415},{\"end\":130429,\"start\":130421},{\"end\":130437,\"start\":130429},{\"end\":130878,\"start\":130871},{\"end\":130886,\"start\":130878},{\"end\":130892,\"start\":130886},{\"end\":130900,\"start\":130892},{\"end\":131130,\"start\":131123},{\"end\":131137,\"start\":131130},{\"end\":131144,\"start\":131137},{\"end\":131151,\"start\":131144},{\"end\":131367,\"start\":131356},{\"end\":131378,\"start\":131367},{\"end\":131395,\"start\":131378},{\"end\":131403,\"start\":131395},{\"end\":131413,\"start\":131403},{\"end\":131974,\"start\":131967},{\"end\":131981,\"start\":131974},{\"end\":131989,\"start\":131981},{\"end\":132004,\"start\":131989},{\"end\":132521,\"start\":132506},{\"end\":132532,\"start\":132521},{\"end\":132550,\"start\":132532},{\"end\":132749,\"start\":132743},{\"end\":132755,\"start\":132749},{\"end\":132764,\"start\":132755},{\"end\":132771,\"start\":132764},{\"end\":132777,\"start\":132771},{\"end\":132785,\"start\":132777},{\"end\":133050,\"start\":133041},{\"end\":133061,\"start\":133050},{\"end\":133070,\"start\":133061},{\"end\":133078,\"start\":133070},{\"end\":133092,\"start\":133078},{\"end\":133410,\"start\":133401},{\"end\":133419,\"start\":133410},{\"end\":133427,\"start\":133419},{\"end\":133436,\"start\":133427},{\"end\":133443,\"start\":133436},{\"end\":133725,\"start\":133716},{\"end\":133733,\"start\":133725},{\"end\":133741,\"start\":133733},{\"end\":133748,\"start\":133741},{\"end\":133754,\"start\":133748},{\"end\":134257,\"start\":134249},{\"end\":134264,\"start\":134257},{\"end\":134271,\"start\":134264},{\"end\":134277,\"start\":134271},{\"end\":134284,\"start\":134277},{\"end\":134561,\"start\":134552},{\"end\":134570,\"start\":134561},{\"end\":134580,\"start\":134570},{\"end\":135026,\"start\":135010},{\"end\":135038,\"start\":135026},{\"end\":135262,\"start\":135253},{\"end\":135270,\"start\":135262},{\"end\":135277,\"start\":135270},{\"end\":135283,\"start\":135277},{\"end\":135291,\"start\":135283},{\"end\":135298,\"start\":135291},{\"end\":135715,\"start\":135704},{\"end\":135729,\"start\":135715},{\"end\":135739,\"start\":135729},{\"end\":135758,\"start\":135739},{\"end\":136359,\"start\":136348},{\"end\":136372,\"start\":136359},{\"end\":136382,\"start\":136372},{\"end\":136401,\"start\":136382},{\"end\":136756,\"start\":136743},{\"end\":136767,\"start\":136756},{\"end\":137359,\"start\":137346},{\"end\":137369,\"start\":137359},{\"end\":137381,\"start\":137369},{\"end\":137639,\"start\":137628},{\"end\":137648,\"start\":137639},{\"end\":137660,\"start\":137648},{\"end\":137669,\"start\":137660},{\"end\":138222,\"start\":138212},{\"end\":138230,\"start\":138222},{\"end\":138244,\"start\":138230},{\"end\":138884,\"start\":138868},{\"end\":138894,\"start\":138884},{\"end\":138902,\"start\":138894},{\"end\":139337,\"start\":139323},{\"end\":139346,\"start\":139337},{\"end\":139359,\"start\":139346},{\"end\":139372,\"start\":139359},{\"end\":139380,\"start\":139372},{\"end\":139391,\"start\":139380},{\"end\":139401,\"start\":139391},{\"end\":139414,\"start\":139401},{\"end\":139428,\"start\":139414},{\"end\":139443,\"start\":139428},{\"end\":139910,\"start\":139904},{\"end\":139918,\"start\":139910},{\"end\":139926,\"start\":139918},{\"end\":139932,\"start\":139926},{\"end\":139942,\"start\":139932},{\"end\":140280,\"start\":140267},{\"end\":140289,\"start\":140280},{\"end\":140301,\"start\":140289},{\"end\":140631,\"start\":140622},{\"end\":140644,\"start\":140631},{\"end\":140655,\"start\":140644},{\"end\":140666,\"start\":140655},{\"end\":141012,\"start\":141001},{\"end\":141021,\"start\":141012},{\"end\":141032,\"start\":141021},{\"end\":141040,\"start\":141032},{\"end\":141048,\"start\":141040},{\"end\":141055,\"start\":141048},{\"end\":141401,\"start\":141392},{\"end\":141410,\"start\":141401},{\"end\":141422,\"start\":141410},{\"end\":141431,\"start\":141422},{\"end\":141997,\"start\":141987},{\"end\":142009,\"start\":141997},{\"end\":142020,\"start\":142009},{\"end\":142569,\"start\":142561},{\"end\":142581,\"start\":142569},{\"end\":142594,\"start\":142581},{\"end\":142605,\"start\":142594},{\"end\":143104,\"start\":143096},{\"end\":143112,\"start\":143104},{\"end\":143122,\"start\":143112},{\"end\":143130,\"start\":143122},{\"end\":143136,\"start\":143130},{\"end\":143147,\"start\":143136},{\"end\":143648,\"start\":143631},{\"end\":143659,\"start\":143648},{\"end\":143667,\"start\":143659},{\"end\":144254,\"start\":144246},{\"end\":144262,\"start\":144254},{\"end\":144274,\"start\":144262},{\"end\":144286,\"start\":144274},{\"end\":144298,\"start\":144286},{\"end\":144868,\"start\":144857},{\"end\":144881,\"start\":144868},{\"end\":144894,\"start\":144881},{\"end\":144905,\"start\":144894},{\"end\":144918,\"start\":144905},{\"end\":144928,\"start\":144918},{\"end\":144953,\"start\":144928},{\"end\":144961,\"start\":144953},{\"end\":144977,\"start\":144961},{\"end\":144993,\"start\":144977},{\"end\":145409,\"start\":145399},{\"end\":145419,\"start\":145409},{\"end\":145430,\"start\":145419},{\"end\":145436,\"start\":145430},{\"end\":145449,\"start\":145436},{\"end\":145459,\"start\":145449},{\"end\":145468,\"start\":145459},{\"end\":145795,\"start\":145786},{\"end\":145803,\"start\":145795},{\"end\":145814,\"start\":145803},{\"end\":146127,\"start\":146119},{\"end\":146135,\"start\":146127},{\"end\":146143,\"start\":146135},{\"end\":146405,\"start\":146392},{\"end\":146415,\"start\":146405},{\"end\":146427,\"start\":146415},{\"end\":146436,\"start\":146427},{\"end\":146446,\"start\":146436},{\"end\":146454,\"start\":146446},{\"end\":146462,\"start\":146454},{\"end\":146473,\"start\":146462},{\"end\":146483,\"start\":146473},{\"end\":146495,\"start\":146483},{\"end\":147013,\"start\":147005},{\"end\":147020,\"start\":147013},{\"end\":147029,\"start\":147020},{\"end\":147035,\"start\":147029},{\"end\":147502,\"start\":147488},{\"end\":147513,\"start\":147502},{\"end\":147922,\"start\":147908},{\"end\":147935,\"start\":147922},{\"end\":147947,\"start\":147935},{\"end\":148431,\"start\":148422},{\"end\":148440,\"start\":148431},{\"end\":148449,\"start\":148440},{\"end\":148457,\"start\":148449},{\"end\":148880,\"start\":148866},{\"end\":148904,\"start\":148880},{\"end\":148916,\"start\":148904},{\"end\":148928,\"start\":148916},{\"end\":148938,\"start\":148928},{\"end\":148956,\"start\":148938},{\"end\":149451,\"start\":149436},{\"end\":149460,\"start\":149451},{\"end\":149474,\"start\":149460},{\"end\":149756,\"start\":149746},{\"end\":149766,\"start\":149756},{\"end\":149992,\"start\":149984},{\"end\":150004,\"start\":149992},{\"end\":150014,\"start\":150004},{\"end\":150287,\"start\":150280},{\"end\":150297,\"start\":150287},{\"end\":150306,\"start\":150297},{\"end\":150313,\"start\":150306},{\"end\":150319,\"start\":150313},{\"end\":150626,\"start\":150614},{\"end\":150636,\"start\":150626},{\"end\":150858,\"start\":150849},{\"end\":150868,\"start\":150858},{\"end\":150875,\"start\":150868},{\"end\":150882,\"start\":150875},{\"end\":150889,\"start\":150882},{\"end\":150897,\"start\":150889},{\"end\":151381,\"start\":151371},{\"end\":151402,\"start\":151381},{\"end\":151699,\"start\":151692},{\"end\":151710,\"start\":151699},{\"end\":151720,\"start\":151710},{\"end\":151731,\"start\":151720},{\"end\":151995,\"start\":151982},{\"end\":152002,\"start\":151995},{\"end\":152017,\"start\":152002},{\"end\":152027,\"start\":152017},{\"end\":152038,\"start\":152027},{\"end\":152049,\"start\":152038},{\"end\":152061,\"start\":152049}]", "bib_venue": "[{\"end\":87718,\"start\":87700},{\"end\":88589,\"start\":88571},{\"end\":89019,\"start\":88946},{\"end\":89516,\"start\":89448},{\"end\":90265,\"start\":90191},{\"end\":90722,\"start\":90642},{\"end\":91165,\"start\":91091},{\"end\":91626,\"start\":91565},{\"end\":92137,\"start\":92063},{\"end\":92633,\"start\":92532},{\"end\":93123,\"start\":93022},{\"end\":94157,\"start\":94060},{\"end\":94679,\"start\":94599},{\"end\":95187,\"start\":95111},{\"end\":95550,\"start\":95524},{\"end\":98463,\"start\":98386},{\"end\":99317,\"start\":99236},{\"end\":99798,\"start\":99724},{\"end\":100307,\"start\":100205},{\"end\":101277,\"start\":101184},{\"end\":101851,\"start\":101781},{\"end\":102748,\"start\":102665},{\"end\":103979,\"start\":103903},{\"end\":104484,\"start\":104402},{\"end\":105725,\"start\":105651},{\"end\":106276,\"start\":106178},{\"end\":106922,\"start\":106896},{\"end\":107352,\"start\":107285},{\"end\":108106,\"start\":108027},{\"end\":108644,\"start\":108548},{\"end\":109183,\"start\":109089},{\"end\":110118,\"start\":110011},{\"end\":113135,\"start\":113109},{\"end\":116570,\"start\":116491},{\"end\":117414,\"start\":117354},{\"end\":117903,\"start\":117833},{\"end\":118350,\"start\":118282},{\"end\":119118,\"start\":119050},{\"end\":119543,\"start\":119476},{\"end\":120283,\"start\":120198},{\"end\":122390,\"start\":122327},{\"end\":123442,\"start\":123374},{\"end\":124171,\"start\":124103},{\"end\":124667,\"start\":124582},{\"end\":125109,\"start\":125041},{\"end\":125572,\"start\":125515},{\"end\":126206,\"start\":126103},{\"end\":127708,\"start\":127640},{\"end\":128185,\"start\":128097},{\"end\":129640,\"start\":129556},{\"end\":130623,\"start\":130528},{\"end\":131591,\"start\":131504},{\"end\":132168,\"start\":132083},{\"end\":133913,\"start\":133833},{\"end\":134738,\"start\":134659},{\"end\":135983,\"start\":135873},{\"end\":136992,\"start\":136882},{\"end\":137824,\"start\":137749},{\"end\":138446,\"start\":138344},{\"end\":141633,\"start\":141528},{\"end\":142209,\"start\":142110},{\"end\":142773,\"start\":142688},{\"end\":143308,\"start\":143226},{\"end\":143881,\"start\":143772},{\"end\":144474,\"start\":144386},{\"end\":146637,\"start\":146567},{\"end\":147176,\"start\":147107},{\"end\":147660,\"start\":147586},{\"end\":148089,\"start\":148017},{\"end\":151057,\"start\":150977},{\"end\":151781,\"start\":151757},{\"end\":152180,\"start\":152162},{\"end\":87114,\"start\":87099},{\"end\":87698,\"start\":87633},{\"end\":88269,\"start\":88231},{\"end\":88569,\"start\":88546},{\"end\":88944,\"start\":88861},{\"end\":89446,\"start\":89374},{\"end\":89870,\"start\":89856},{\"end\":90189,\"start\":90112},{\"end\":90640,\"start\":90559},{\"end\":91089,\"start\":91021},{\"end\":91563,\"start\":91506},{\"end\":92061,\"start\":91993},{\"end\":92530,\"start\":92431},{\"end\":93020,\"start\":92921},{\"end\":93457,\"start\":93380},{\"end\":94058,\"start\":93964},{\"end\":94597,\"start\":94520},{\"end\":95109,\"start\":95035},{\"end\":95522,\"start\":95482},{\"end\":95893,\"start\":95867},{\"end\":96199,\"start\":96185},{\"end\":96545,\"start\":96501},{\"end\":96959,\"start\":96944},{\"end\":97305,\"start\":97291},{\"end\":97689,\"start\":97663},{\"end\":98039,\"start\":98017},{\"end\":98384,\"start\":98307},{\"end\":98865,\"start\":98845},{\"end\":99234,\"start\":99161},{\"end\":99722,\"start\":99646},{\"end\":100203,\"start\":100109},{\"end\":100727,\"start\":100716},{\"end\":101182,\"start\":101086},{\"end\":101779,\"start\":101717},{\"end\":102219,\"start\":102121},{\"end\":102663,\"start\":102582},{\"end\":103124,\"start\":103065},{\"end\":103511,\"start\":103393},{\"end\":103901,\"start\":103828},{\"end\":104400,\"start\":104323},{\"end\":104877,\"start\":104839},{\"end\":105242,\"start\":105196},{\"end\":105649,\"start\":105578},{\"end\":106176,\"start\":106077},{\"end\":106621,\"start\":106597},{\"end\":106894,\"start\":106869},{\"end\":107283,\"start\":107216},{\"end\":107641,\"start\":107568},{\"end\":108025,\"start\":107948},{\"end\":108546,\"start\":108448},{\"end\":109087,\"start\":109000},{\"end\":109551,\"start\":109453},{\"end\":110009,\"start\":109899},{\"end\":110519,\"start\":110494},{\"end\":110717,\"start\":110687},{\"end\":111071,\"start\":111046},{\"end\":111561,\"start\":111548},{\"end\":111921,\"start\":111908},{\"end\":112237,\"start\":112224},{\"end\":112590,\"start\":112473},{\"end\":113107,\"start\":113058},{\"end\":113618,\"start\":113601},{\"end\":114022,\"start\":113941},{\"end\":114507,\"start\":114492},{\"end\":114884,\"start\":114865},{\"end\":115358,\"start\":115340},{\"end\":115801,\"start\":115781},{\"end\":116023,\"start\":115912},{\"end\":116489,\"start\":116412},{\"end\":116976,\"start\":116952},{\"end\":117352,\"start\":117295},{\"end\":117831,\"start\":117765},{\"end\":118280,\"start\":118223},{\"end\":118747,\"start\":118716},{\"end\":119048,\"start\":118991},{\"end\":119474,\"start\":119409},{\"end\":119829,\"start\":119783},{\"end\":120196,\"start\":120119},{\"end\":120679,\"start\":120588},{\"end\":120934,\"start\":120889},{\"end\":121191,\"start\":121122},{\"end\":121414,\"start\":121330},{\"end\":121768,\"start\":121686},{\"end\":122043,\"start\":121962},{\"end\":122325,\"start\":122273},{\"end\":122748,\"start\":122676},{\"end\":122997,\"start\":122889},{\"end\":123372,\"start\":123315},{\"end\":123790,\"start\":123770},{\"end\":124101,\"start\":124037},{\"end\":124580,\"start\":124503},{\"end\":125039,\"start\":124982},{\"end\":125513,\"start\":125456},{\"end\":126101,\"start\":125997},{\"end\":126632,\"start\":126607},{\"end\":127009,\"start\":126997},{\"end\":127340,\"start\":127336},{\"end\":127638,\"start\":127581},{\"end\":128095,\"start\":128012},{\"end\":128472,\"start\":128400},{\"end\":128870,\"start\":128821},{\"end\":129207,\"start\":129117},{\"end\":129554,\"start\":129471},{\"end\":130111,\"start\":129995},{\"end\":130526,\"start\":130437},{\"end\":130956,\"start\":130916},{\"end\":131121,\"start\":131067},{\"end\":131502,\"start\":131413},{\"end\":132081,\"start\":132004},{\"end\":132504,\"start\":132454},{\"end\":132821,\"start\":132801},{\"end\":133158,\"start\":133118},{\"end\":133485,\"start\":133471},{\"end\":133831,\"start\":133754},{\"end\":134321,\"start\":134309},{\"end\":134657,\"start\":134580},{\"end\":135008,\"start\":134949},{\"end\":135364,\"start\":135324},{\"end\":135871,\"start\":135758},{\"end\":136490,\"start\":136417},{\"end\":136880,\"start\":136767},{\"end\":137392,\"start\":137381},{\"end\":137747,\"start\":137669},{\"end\":138342,\"start\":138244},{\"end\":138947,\"start\":138927},{\"end\":139486,\"start\":139466},{\"end\":139999,\"start\":139967},{\"end\":140265,\"start\":140180},{\"end\":140620,\"start\":140474},{\"end\":141097,\"start\":141077},{\"end\":141526,\"start\":141431},{\"end\":142108,\"start\":142020},{\"end\":142686,\"start\":142605},{\"end\":143224,\"start\":143147},{\"end\":143770,\"start\":143667},{\"end\":144384,\"start\":144298},{\"end\":145018,\"start\":145010},{\"end\":145493,\"start\":145487},{\"end\":145866,\"start\":145847},{\"end\":146187,\"start\":146168},{\"end\":146565,\"start\":146495},{\"end\":147105,\"start\":147035},{\"end\":147584,\"start\":147513},{\"end\":148015,\"start\":147947},{\"end\":148499,\"start\":148489},{\"end\":149015,\"start\":148989},{\"end\":149510,\"start\":149503},{\"end\":149805,\"start\":149792},{\"end\":150067,\"start\":150042},{\"end\":150362,\"start\":150348},{\"end\":150672,\"start\":150665},{\"end\":150975,\"start\":150897},{\"end\":151369,\"start\":151279},{\"end\":151755,\"start\":151731},{\"end\":152160,\"start\":152077}]"}}}, "year": 2023, "month": 12, "day": 17}