{"id": 230770344, "updated": "2023-10-06 06:49:52.946", "metadata": {"title": "Generating Masks from Boxes by Mining Spatio-Temporal Consistencies in Videos", "authors": "[{\"first\":\"Bin\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Goutam\",\"last\":\"Bhat\",\"middle\":[]},{\"first\":\"Martin\",\"last\":\"Danelljan\",\"middle\":[]},{\"first\":\"Luc\",\"last\":\"Gool\",\"middle\":[\"Van\"]},{\"first\":\"Radu\",\"last\":\"Timofte\",\"middle\":[]}]", "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)", "journal": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)", "publication_date": {"year": 2021, "month": 1, "day": 6}, "abstract": "Segmenting objects in videos is a fundamental computer vision task. The current deep learning based paradigm offers a powerful, but data-hungry solution. However, current datasets are limited by the cost and human effort of annotating object masks in videos. This effectively limits the performance and generalization capabilities of existing video segmentation methods. To address this issue, we explore weaker form of bounding box annotations. We introduce a method for generating segmentation masks from per-frame bounding box annotations in videos. To this end, we propose a spatio-temporal aggregation module that effectively mines consistencies in the object and background appearance across multiple frames. We use our resulting accurate masks for weakly supervised training of video object segmentation (VOS) networks. We generate segmentation masks for large scale tracking datasets, using only their bounding box annotations. The additional data provides substantially better generalization performance leading to state-of-the-art results in both the VOS and more challenging tracking domain.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2101.02196", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iccv/ZhaoBDGT21", "doi": "10.1109/iccv48922.2021.01330"}}, "content": {"source": {"pdf_hash": "af235029ee66a4f83048b7278bf4114974277b4b", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2101.02196v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "29c6f2108e4e138d3dc2779a1b2e1373b3e9fefd", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/af235029ee66a4f83048b7278bf4114974277b4b.txt", "contents": "\nGenerating Masks from Boxes by Mining Spatio-Temporal Consistencies in Videos\n\n\nZhao Bin \nComputer Vision Lab\nETH Zurich\nD-ITETSwitzerland\n\nBhat Goutam goutam.bhat@ethz.ch \nComputer Vision Lab\nETH Zurich\nD-ITETSwitzerland\n\nDanelljan Martin martin.danelljan@ethz.ch \nComputer Vision Lab\nETH Zurich\nD-ITETSwitzerland\n\nVan Luc \nComputer Vision Lab\nETH Zurich\nD-ITETSwitzerland\n\nRadu Gool \nComputer Vision Lab\nETH Zurich\nD-ITETSwitzerland\n\nTimofte radu.timofte@ethz.ch \nComputer Vision Lab\nETH Zurich\nD-ITETSwitzerland\n\nGenerating Masks from Boxes by Mining Spatio-Temporal Consistencies in Videos\n\nFigure 1. When only using a single frame, predicting object masks from bounding boxes often leads to failures (left), since the outline of an object is difficult to resolve. By using video, our approach aggregates information across several frames. In this example, it identifies the car as background through the neighboring frames, while the scooter remains within the box, allowing it to be accurately segmented.AbstractSegmenting objects in videos is a fundamental computer vision task. The current deep learning based paradigm offers a powerful, but data-hungry solution. However, current datasets are limited by the cost and human effort of annotating object masks in videos. This effectively limits the performance and generalization capabilities of existing video segmentation methods. To address this issue, we explore weaker form of bounding box annotations.We introduce a method for generating segmentation masks from per-frame bounding box annotations in videos. To this end, we propose a spatio-temporal aggregation module that effectively mines consistencies in the object and background appearance across multiple frames. We use our resulting accurate masks for weakly supervised training of video object segmentation (VOS) networks. We generate segmentation masks for large scale tracking datasets, using only their bounding box annotations. The additional data provides substantially better generalization performance leading to state-of-the-art results in both the VOS and more challenging tracking domain.\n\nIntroduction\n\nSegmenting objects in videos is an important but challenging task with many applications in autonomous driving [51,56], surveillance [12,16] and video editing. The field has been driven by the astonishing performance of deep learning based approaches [8,44,59]. However, these methods require large amount of training images with pixelwise annotations. Manually annotating segmentation masks in videos is an extremely time-consuming and costly task. Existing video datasets with segmentation labels [49,63] therefore do not provide the large-scale diversity desired in deep learning. This effectively limits the potential of current state-of-the-art approaches.\n\nTo address this issue, it is tempting to consider weaker forms of human annotations. In particular, object bounding boxes are substantially faster to annotate. They provide horizontal and vertical constraints on the extent of the segmentation mask. Importantly, there already exist large scale video datasets with bounding box annotations [17,24,43,55]. Hence, a method of effectively leveraging these annotations for video segmentation purposes would substantially enlarge the pool of available training data. Ide-ally, simply converting the video box annotations to object masks would allow existing video segmentation approaches to integrate these annotations using standard supervised techniques, not requiring any modification of losses or architectures. We therefore investigate the problem of generating object segmentations from box-annotations in videos.\n\nGenerating masks from box-annotated videos is a deceptively challenging task. The background scene is often cluttered or contains similar objects. Objects can change appearance rapidly and often undergo heavy occlusions. Existing approaches [39,61] only address the single frame case, where these ambiguities are difficult, or sometimes impossible to resolve due to the limited information. However, the aforementioned problems can be greatly alleviated if we can utilize multiple frames in the video. As the object moves relative to the background, we can find consistencies over several example views of the object and background. While object regions should consistently stay inside the box, background patches can move from inside to outside the object box over the duration of the video sequence. For instance, in Fig. 1 the single frame approach fails to properly segment the scooter due to the background car. In contrast, our video-based approach can identify the car as background in earlier and later frames while the scooter is consistently within the box for all frames. The car is therefore easily excluded from the final segmentation in all frames.\n\nEffectively exploiting the information encoded in the temporal information is however a highly challenging problem. Since the object and background moves and changes in each frame, standard fusion operations cannot extract the desired consistencies and relations. Instead, we propose a spatio-temporal aggregation module by taking inspiration from the emerging direction of deep declarative networks [18]. Our module is formulated as an optimization problem that aims to find the underlying object representation that best explains the observed object and background appearance in each frame. It allows our approach to mine spatiotemporal consistencies by jointly reasoning about all image patches in all input frames. The resulting mask embeddings for each frame are then processed by a decoder to generate the final segmentation output.\n\nContributions: Our main contributions are as follows.\n\n(i) We propose a method for predicting object masks from bounding boxes in videos. To the best of our knowledge, we are the first to address this problem. (ii) We develop a spatio-temporal aggregation module that effectively mines the object and background information over multiple frames. (iii) Through an iterative formulation, we can further refine the masks through a second aggregation module.\n\n(iv) We utilize our method to annotate large-scale tracking datasets with object masks, which are then employed for Video Object Segmentation (VOS). (v) We perform extensive experiments, demonstrating the effectiveness of our ap-proach in the limited data domain. Moreover, we show that the data generated by our approach allows VOS methods to cope with challenges posed by the tracking setting, achieving state-of-the-art performance on both domains. Code, models, and generated annotations will be made available at https://github.com/visionml/pytracking.\n\n\nRelated work\n\nSemi-supervised video object segmentation:\n\nSemisupervised video object segmentation (VOS) is the task of classifying all pixels in a video sequence into foreground and background, given a target ground-truth mask in the first frame. A number of different approaches have been proposed for VOS in recent years, including detection based methods [9,40,60], propagation based approaches [27,36,46,62], feature matching techniques [11,23,44,59], and meta-learning based methods [3,8,50]. A crucial factor that has enabled the recent advancements in VOS has been the release of high quality datasets such as DAVIS [49] and YouTube-VOS [63]. However, due to the high costs of performing pixel-wise mask annotations, VOS datasets are still small in size compared to other computer vision fields such as detection, object tracking etc. Consequently, VOS methods often resort to generating synthetic videos using multiple image segmentation datasets in order to obtain more training samples [44,62].\n\nWeakly supervised segmentation: Due to the high cost of collecting pixel-wise labels, different types of weak annotations have been utilized to guide segmentation tasks recently, such as image-level supervision [1,25,29,47,65], points [2,41], scribbles [38,58] and bounding boxes [13,22,26,31,45]. Our work is more related to bounding box supervised segmentation. In [13,26], pseudo segmentation masks for training are generated using GrabCut [52] and the MCG proposals [48]. Recent work [22] designs a multiple instance learning (MIL) loss by leveraging the tightness property of bounding boxes. In [31], a per-class attention map is predicted to guide the cross entropy loss and avoid propagating incorrect gradients from the background pixels inside the bounding box. As opposed to introducing specialized losses, our method directly generates pseudo masks from boxes, which can then be used to train models. Importantly, the masks are generated by utilizing spatio-temporal consistencies across video frames, leading to improved segmentation accuracy.\n\nConverting boxes to segmentation masks: Generating a segmentation mask from the given object bounding box is an essential sub-task in instance segmentation, especially detection-based methods [14,19,20,37]. These approaches follow a multi-task learning strategy where a backbone network first extracts deep features and generates a set of proposals. Then detection and segmentation heads are used Figure 2. An overview of our architecture for segmenting an object from a box-annotated video. We first extract deep features from each frame. Features and boxes are then given to the object encoder (Sec. 3.1) to generate an object-aware representation. The spatio-temporal aggregation module (Sec. 3.2) inputs object encodings and deep features from all frames. Its output is decoded to an object mask. We refine the masks by iterating the process (Sec. 3.3) with a secondary object encoder and aggregation module to generate the final output.\n\nseparately to predict an accurate bounding box and segmentation mask for the proposal. ShapeMask [32] takes a bounding box detection as the initial shape estimate and refines it gradually, using a collection of shape priors. Luiten et al. [39] train a modified DeepLabv3 [10] model to output a mask, given a crop containing the object as input. In contrast to the previous approaches which only operate on single images, we address the task generating masks given box annotated videos as input. Our approach can exploit the additional temporal information in the video to predict more accurate masks, as compared to single image approaches.\n\nObject Co-segmentation: Object co-segmentation is the task of segmenting the common objects from a set of images. The concept of object co-segmentation was first introduced by Rother et al. in [53], which minimizes an energy function containing an MRF smoothness prior term and a histogram matching term. Subsequent work [54] combines visual saliency and dense SIFT matching to capture the sparsity and visual variability of the common object in a group of images. Yuan et al. [64] introduce a deep dense conditional random field to automatically discover common information from co-occurrence maps generated by object proposals. The work [35] integrates a mutual correlation layer into a CNN-based Siamese encoder-decoder architecture to perform co-segmentation. Similar to cosegmentation method, we segment objects using multiple images. However, our images are obtained from the same video. This enables exploiting the strong temporal consistency in videos to improve segmentation accuracy.\n\n\nMethod\n\nWe propose an end-to-end trainable architecture for the problem of segmenting an object in a video, given its bounding boxes in each frame. Our complete architecture is shown in Fig. 2. To fully exploit the temporal dimension, we aim to use detailed information of not only the target, but also the background context. Our backbone feature network F therefore first separately encodes video frames {I t } T 1 containing the object as well as substantial background. The extracted deep features x t = F (I t ) along with the corresponding bounding box b t are given to the object encoder B, which provides an object-aware representation e t of each individual frame. By integrating the object bounding box, it provides information about hypothetical object and background regions.\n\nThe object encodings and deep features x t from all frames are input to the spatio-temporal aggregation module S. The goal of this module is to generate an encoding of the object segmentation s t for each frame. The module S aggregates appearance and box information from all frames and locations through an efficient and differentiable optimization processes. The iterative procedure fuses the different observations of the appearance {(x t , e t )} by find an underlying representation of the object. This representation then generates the segmentation encoding s t , which is processed by the segmentation decoder D to predict preliminary object masks as y t = D(s t , x t ). Our flexible architecture allows us to further improve the masks by feeding the results into a second spatio-temporal aggregation module, which predicts a set of refined segmentation encodings \u015d t . The final segmentation masks\u0177 t = D(\u015d t , x t ) are then generated by the same decoder network. In the next section, we first detail our object encoder.\n\n\nObject Encoder\n\nAccurately segmenting a specified object given only a single frame is a challenging problem. Since our goal is generic object segmentation, the type of object specified during inference may not even be represented in the training set. In general, it is therefore difficult to assess which image region inside a single bounding box belongs to the object in question. This is further complicated by cluttered scenes or presence of distractor regions in the background that are similar to the object itself. In these cases, determining object boundaries is particularly difficult. Moreover, multiple objects often overlap, making even the decision of which object to segment given the bounding box an ambiguous task. All aforementioned problems are greatly alleviated and disambiguated if we can exploit several frames from a video sequence. As the object moves relative to the background, we can search for consistencies over several example views of the object appearance. While object regions should consistently stay inside the box, background patches can move from inside to outside the object box over the duration of the video sequence. This search for consistency is performed by our spatio-temporal aggregation S through an iterative optimization process. It operates on information extracted from the individual frames by the object encoder, which we first detail in this section.\n\nDirectly extracting the segmentation from a single frame is difficult and prone to errors. We can however generate an object encoding from a single frame, capturing multiple possible segmentation hypotheses. Each frame also gives detailed information about image patches, structures and patterns that are certainly not part of the object itself. These are regions of the image that are strictly outside the bounding box, which provide important cues when combined with other frames in the sequence. To extract such frame-wise object information, we integrate an object encoder B. It takes information available in the frame t by inputting the deep image representation x t = F (I t ) along with the object bounding box b t . We first convert the bounding box b t to a corresponding rectangular mask representation in the input image coordinates. This is then processed by several convolutional and pooling layers, which increase the dimensionality while reducing the spatial resolution to be the same of the deep features x t . The resulting activations are then concatenated with the features x t and further processed by several residual blocks.\n\nThrough the deep features x t , the object encoder B can extract candidate object shapes, which are used when searching for consistency over several frames. Specifically, the object encoder has three outputs,\n(e t , w t , m t ) = B(x t , b t ) , e t , w t , m t \u2208 R H\u00d7W \u00d7C . (1)\nAll outputs have the same spatial resolution H \u00d7 W as the deep features x t . The abstract embedding e t holds information about the candidate object shapes and background regions in image I t . Intuitively, at spatial location (i, j), the activation vector e t [i, j] \u2208 R C encodes probable segmentations of the corresponding image region. Since the uncertainty of this information can vary spatially and over the feature channels, we also predict a corresponding confidence weight w t . Indeed, regions outside the bounding box are certainly not part of the object, while regions inside the box can be ambiguous or difficult to interpret. The output m t corresponds to relevant single-frame object information that is directly given to the segmentation decoder D. The encoding e t and its confidence w t are given to the spatiotemporal aggregation module, detailed next.\n\n\nSpatio-Temporal Aggregation\n\nIt is the task of the spatio-temporal aggregation module to mine the object information over multiple frames. However, designing a neural network module capable of effectively integrating information from multiple frames is a challenging and intricate problem as the object changes location and pose in each frame. As a result, temporal pooling, concatenation, or convolutions cannot find the desired consistencies. Moreover, these operations do not consider detailed global information. When deciding whether a patch corresponds to foreground or background, we need to find and reason about all similar patches in the given frames.\n\nOur approach takes inspiration from the emerging direction of deep declarative networks [18], which have shown promise in few-shot and meta-learning applications [6,33], including video object segmentation [8]. These approaches formulate the task of a neural network as an optimization problem. This problem is solved during the forward pass and allows end-to-end learning by ensuring that the solution to be differentiable w.r.t. all inputs. Note that our setting does not fall into the domain of few-shot or meta-learning. Yet, we demonstrate that the temporal aggregation task encountered in this work can be gracefully formulated as an optimization objective. We believe that our objective and its novel interpretation further widen the scope of applications for ideas stemming from deep declarative networks.\n\nThe main idea of our formulation is to find an underlying object representation z that best explains the observed object embedding e t . That is, the representation z should indicate consistent local correlations between the deep image features x t and the corresponding object embedding e t . We formulate this as the problem of finding the best fitting local linear mapping from features vectors x t [i, j] \u2208 R D to corresponding embedding vectors e t [i, j] \u2208 R C . This is most conveniently expressed as a convolution with the filter z \u2208 R K\u00d7K\u00d7D\u00d7C , where K is the kernel size. Using the squared error to measure the fit, our temporal aggregation module is formulated as\n{s t } T 1 = S {(x t , e t , w t )} T 1 = {x t * z * } T 1 where (2a) z * = arg min z 1 T T t=1 w t \u00b7 (x t * z \u2212 e t ) 2 + \u03bb z 2 . (2b)\nThe filter z is thus optimized to predict the embedding e t from the features x t . In order to minimize the objective, the filter must focus on consistent local correlations between x t and e t , while ignoring accidental relations that do not reoccur. The predicted confidence w t actively weights the error at each spatio-temporal location and channel dimension through an element-wise multiplication. Our network can therefore learn to ignore information in e t that is deemed uncertain by predicting a low weight w t , while emphasizing other information by giving a large importance weight. The regularization weight \u03bb is treated as a network parameter and thus learned during training. During both inference and training, the optimization problem (2b) needs to be solved for every forward pass of the network. The solver thus needs to be efficient in order to ensure practical training and inference times. Moreover, the solution z * needs to be differentiable w.r.t. to the inputs {(x t , e t , w t )} T 1 and \u03bb. There exist approaches for directly back-propagation through the solution z * for convex problems such as (2b) using the implicit function theorem [18,33] or closed-from expressions [5]. However, these methods rely on accurately finding the optima z * , which is not necessary in our case. Instead, we found the unrolled steepest-descent based optimization strategy [6,8] to yield a simple and fast solution. As the algorithm employs iterative updates to z through a differentiable closed-form expression, backpropagation is automatically achieved through the standard auto-differentiation implemented in popular deep learning libraries, such as PyTorch and TensorFlow.\n\nAfter mining for spatio-temporal consistencies through the iterative minimization of (2b), the filter z * contains a strong representation of the object. It encapsulates the consistent patterns and correlations of the object, integrating both spatial and temporal information. The output segmentation encoding s t of the spatio-temporal aggregation module is achieved by applying the optimized representation z * to the deep features x t of each frame in (2a) as s t = x t * z * . This is then input to our decoder y t = D(s t , m t , x t ), which generates a final object segmentation y t .\n\n\nIterative Refinement\n\nIn this section, we describe a method to further refine the object segmentations using existing components in our architecture. The decoder module learns powerful segmenta-tion priors by integrating deep features from different levels. It is able to extract accurate object boundaries and filter out potential errors. The segmentation embedding s t predicted by the spatio-temporal aggregation module (2) is thus enriched with these priors in order to generate the output segmentation y t . Note that this represents new knowledge not seen by the aggregation module in the first pass. We can therefore utilize this information by feeding the output segmentation masks back into the aggregation step.\n\nTo this end, we create a secondary object encoderB, taking the predicted segmentation y t . Since the preliminary segmentation y t already encapsulates a detailed representation of the object extent, we found it to be sufficient for generating the object embedding\u00ea t and confidence weight\u015d w t used by the aggregation module. Thus for each frame we predict,\n(\u00ea t ,\u0175 t ) =B(y t ) , e t , w t \u2208 R H\u00d7W \u00d7C .(3)\nNote that we do not re-generate the single-frame information m t later used by the decoder. Instead, we employ the one stemming from the original object encoder (1). The object encoding\u00ea t and corresponding weights\u0175 t now include new and more accurate information about the object. We integrate this for mask prediction by inputting it to our spatio-temporal aggregation module (2) to generate new segmentation encodings\n{\u015d t } T 1 = S {(x t ,\u00ea t ,\u0175 t )} T 1 .\nNote that this implies solving a new optimization problem (2b), which mines spatio-temporal consistencies. The final segmentation mask\u0177 t is obtained using the same decoder module as\u0177 t = D(\u015d t , m t , x t ). While the process could be repeated several times, we did not observe noticeable improvement from a third iteration. This is however expected as the strong segmentation priors of the decoder is already exploited by the aggregation module in the second iteration.\n\n\nTraining\n\nOur complete model is fully differentiable, and can therefore be trained end-to-end using existing video datasets annotated with segmentation masks. From a ground truth mask y GT t , we extract a corresponding bounding box b t by taking smallest axis-aligned box containing the mask y GT t . Our network is trained on sub-sequences of length T by minimizing the loss,\nL = 1 T T t=1 (y t , y GT t ) + 1 T T t=1 (\u0177 t , y GT t ) .(4)\nHere, y t and\u0177 t is the segmentation output generated by the initial prediction and the refinement respectively. Further, denotes a generic segmentation loss.\n\nFor our experiments, we use the YouTube-VOS [63] and DAVIS 2017 [49] datasets. In each epoch, we sample sequences from both datasets without replacement. Due to its larger size, we use a 6 times higher probability for YouTube-VOS 2019 compared to the DAVIS 2017 training set. We randomly sample sequences of length T = 3 frames within a temporal window of length 100. For each frame, we first crop a patch that is 5 times larger than the ground-truth bounding box, while ensuring the maximal size to be equal to the image itself. We then resize the cropped patch to 832 \u00d7 480 with the same aspect ratio. Only random horizontal flipping is employed for data augmentation.\n\nWe initialize our backbone ResNet-50 network with Mask R-CNN weights from [42]. All the remaining modules are initialized using [21]. The network parameters are learned by minimizing Lovasz [4] segmentation loss. We use ADAM [28] optimizer to update network parameters with a mini-batch size of 4. We train our network for 80k iterations with the backbone weights fixed. The learning rate is initialized as 10 \u22122 and then reduced by a factor of 5 after 30k and 60k iterations. The entire training takes 32 hours on a single Nvidia TITAN Xp GPU.\n\n\nImplementation Details\n\nArchitecture: Here, we give further details about our architecture. We use a ResNet-50 backbone network as feature extractor. For the object encoder B and spatio-temporal aggregation module S, we employ the third residual block and add another convolutional layer which reduces the dimensionality to 512. The object encoder generates outputs (1) with a dimension C = 16. We adopt the segmentation decoder used in [8,50]. We first concatenate the segmentation embedding s t from (2) with the single-frame information m t from (1). The decoder then progressively increases the resolution while integrating deep features from different levels in F . For the spatio-temporal aggregation module (2), we first initialize the object representation z to zero. We then apply 5 steepest descent iterations [8] to optimize (2b) during training. The kernel size of z is set to K = 3. Inference: In this section, we provide details about our inference procedure. For a given input video, we extract a sequence of T frames. Since our method benefits from using different views of the target and background, we do not extract directly subsequent frames as they are highly correlated. Instead, we take T frames with an inter-frame interval of \u2206. In order to segment all frames, we simply proceed by shifting the sub-sequence one step each time. We generally employ \u2206 = 15. We analyze the impact of the sequence length T in Sec. 4.1. For the spatio-temporal aggregation (4), we found it beneficial to increase the number of steepest-descent iterations to 15 during inference.\n\n\nExperiments\n\nWe perform comprehensive experiments to validate our contributions. A detailed ablative analysis of our architec- \n\n\nAblation Study\n\nHere, perform a detailed ablative study, analysing the impact of the key components in the proposed approach. Our analysis is performed on a custom validation set YT300 consisting of 300 sequences sampled randomly from the YouTube-VOS 2019 training set, as well as the DAVIS 2017 validation set. The methods are evaluated using the mean Jaccard J index. Unless specified, inference is performed using the settings described in Section 3.5. We only employ a different \u2206 = 5 for DAVIS due to faster motions. Impact of using multiple frames: We investigate the impact of exploiting temporal information from multiple frames to convert boxes to masks by evaluating our approach using different number of input frames. Additionally, we also include an off-the-shelf single frame box to mask conversion network Box2Seg [39] for comparison. Box2Seg uses a DeepLabv3+ architecture and directly predicts the mask using the image crop denoted by the box as input. The result of this comparison is shown in Table 1, and qualitative examples are provided in Fig. 3. When using a single frame as input, our approach obtains a J score of 84.2 and 78.7 on YT300 and DAVIS 2017 validation set, respectively. The performance of our approach improves substantially when using multiple input frames. The best results are obtained when using 9 frames. In this setting, our approach obtains a J score of 81.2 on the DAVIS 2017 validation set, outperforming the single frame Box2Seg network by +1.9 in J score. These results clearly demonstrate the advantages of using multiple frames to perform accurate box to mask conversion. Analysis of architecture: Here, we analyse the impact of different components in our architecture. We evaluate four different variants of our method; i) SingleImage: A single image baseline which only uses the single-frame object representation m t to independently convert boxes to masks in each frame. ii) MultiFrame: Our spatio-temporal aggregation module is used to obtain the segmentation encoding s t by exploiting multiple frames. iii) MultiFrame+: The single-frame object representation m t is passed to the segmentation decoder, in addition to the segmentation encod- Figure 3. Qualitative results of our box to mask conversion network when using single (second row) and multiple (third row) images during inference. Our approach can effectively exploit multiple frames to handle challenging cases by mining spatio-temporal consistencies. ing s t . iv) MultiFrameIterative: We employ the iterative refinement strategy described in Sec 3.3 to refine the initial segmentation prediction obtained using MultiFrame+. Results are shown in Table 2. The SingleImage achieves a J score of 83.3 and 77.2 on YT300 and DAVIS 2017 validation set, respectively. The MultiFrame model, which exploits object information from multiple frames achieves significantly better results, with an improvement of +2.6 in J score on DAVIS 2017. This demonstrates the effectiveness of our spatio-temporal aggregation module in effectively combining the information from multiple frames. Using the single-frame object representation m t in combination with the segmentation encoding s t provides a slight improvement. Finally, performing iterative refinement of the initial segmentation prediction provides an additional improvement of +1.1 in J score on DAVIS 2017. This shows that the segmentation decoder contains rich prior information which can complement our spatio-temporal aggregation module.\n\n\nWeakly Supervised VOS Training\n\nIn this section, we validate the effectiveness of our approach to generate pseudo-labels for weakly supervised training of VOS models. We consider the scenario where pixel-wise segmentation labels are only available for a small number of training sequences, while the rest of the sequences have bounding box annotation for the objects. This is a highly practical scenario as generating bounding box labels is significantly faster compared to obtaining pixel-wise mask annotations. In such cases, it is desirable to exploit  Table 3. Comparison with other weakly supervised training methods on the YT300 dataset, in terms of Jaccard J Index.\n\nthe bounding box annotations to perform weakly supervised training in order to benefit from more training data. In order to evaluate our approach for this weakly supervised setting, we simulate the training scenario using YouTube-VOS 2019 training set. We randomly split YouTube-VOS training set into two subsets A and B in the ratio 1:9. The segmentation labels are available for set A, while only the bounding box annotations are made available for videos in set B. We use the mask annotated videos from set A to train our video box to mask conversion network. The trained model is then used to generate pseudo-labels for video in set B, using only the bounding box annotation. A VOS model is then trained using the combined datasets A and B. We compare our approach of generating pseudo labels using video with two alternative; i) MIL We use the recently introduced multiple instance learning (MIL) loss [22] to compute training loss on the box annotated videos from set B. ii) MIL+CRF We use the MIL loss in combination with the CRF regularizer introduced in [57] to compute training loss. Additionally, we also report the results obtained when using only the fully annotated set A for training (Only A), as well as the upper bound attained when using the complete YouTube VOS training set with mask annotations (FS). We use the recently introduced LWL [8] approach as our VOS model for this experiment. The LWL network is trained using each of the weakly supervised methods, for 100k iterations. The result of this comparison is shown in Table 3, on the YT300 set. Both the MIL and MIL+CRF approaches obtain an improvement of around +0.9 in J score, compared to the naive baseline using only the mask annotated videos from set A for training. Our approach of generating pseudo labels obtains the best results, achieving a substantial improvement of over +1 in J score over the MIL   Table 5. State-of-the-art comparison on the GOT-10k validation set in terms of average overlap (AO) and success rates (SR) at overlap thresholds 0.5 and 0.75 baselines. These results demonstrate the quality and effectiveness of the masks generated from our approach for performing weakly supervised training for VOS.\n\n\nVOS in the Tracking Domain\n\nWe utilize the capability of performing weakly supervised VOS training using box annotations to train a VOS method on large-scale tracking datasets in order to obtain improved tracking performance. We use our network to annotate large-scale tracking datasets LaSOT [17] and GOT10k [24] containing 1120 and 9340 training sequences, respectively. The pseudo annotated tracking sequences, along with the fully annotated YouTube-VOS and DAVIS datasets are then used to fine-tune a VOS model. We start with the LWL [8] network trained with fixed backbone weights. The complete model, including the backbone feature extractor, is then trained on combined YouTube-VOS, DAVIS, LaSOT, and GOT-10k datasets for 120k iterations. We compare this model, denoted LWL-Ours, with the state-of-the-art on VOT2020 [30], GOT10K [24], and TrackingNet [43] datasets. For comparison, we also report results for the standard LWL model fine-tuned using only the YouTube-VOS and DAVIS datasets. VOT2020 [30]:\n\nWe evaluate our trained LWL-Ours model on VOT2020 dataset consisting of 60 challenging sequences. Similar to semi-supervised VOS, the trackers are provided an initial object mask. In order to obtain robust performance measures, the trackers are evaluated multiple times on each sequence, using different starting frames. The trackers are compared using the accuracy, robustness, and expected average overlap EAO measure. Accuracy denotes the average overlap between tracker prediction and the ground truth over the successfully tracked frames, while robustness measures the fraction of sequence tracked on average before tracking loss. Both these measures are combined to obtain the EAO score. Tab. 4 shows the results over the 60 sequences from VOT2020. LWL-Ours fine-tuned on tracking datasets, obtains a relative improvement of over 10% in EAO score, compared to the LWL baseline. Furthermore, despite performing vanilla VOS, LWL-Ours out-SiamRPN++ [34] DiMP-50 [6] KYS [7] SiamRCNN [61] LWL LWL-Ours  performs existing tracking approaches, achieving the second best EAO score. These results show that the masks generated from our approach can be utilized to improve the generalization of VOS model on generic tracking datasets.\n\nGOT10k [24]: We evaluate LWL-Ours on the validation split of GOT10k dataset, consisting of 180 videos. Unlike VOT2020, trackers are only provided an initial box and required to output a target box for each frame. Thus, we use our box to mask conversion network to obtain the initial segmentation mask. The VOS model is then run using the generated mask. In each subsequent frame, we simply compute the target box using the extreme points of the predicted segmentation mask, without performing any post-processing. Fine-tuning LWL on our pseudo-annotated tracking videos provides an improvement of 2.1% in AO over the baseline LWL model (see Tab. 5). Moreover, LWL-ours significantly outperforms existing trackers with an AO score of 86.7%.\n\nTrackingNet [43]: We report results on the test split of TrackingNet dataset consisting of 511 videos, using the same evaluation strategy employed for GOT10k dataset. Using our generated masks on tracking datasets for finetuning improves the results of the LWL model by 0.5% in terms of success score (see Tab. 6). Moreover, LWL-Ours obtains the best results among all methods in terms of Success score, along with SiamRCNN [61].\n\n\nYouTube-VOS:\n\nWe also report results on the YouTube-VOS 2019 validation set for comparison. LWL-Ours obtains results comparable with standard LWL (see Tab. 7), showing that the improved performance on tracking datasets is obtained without sacrificing VOS accuracy.\n\n\nConclusion\n\nWe propose an end-to-end trainable method for predicting object masks from bounding boxes in videos. Our approach can effectively mine object and background information over multiple frames using a novel spatio-temporal aggregation module. The predicted masks are further refined using an iterative formulation. Our approach obtains superior segmentation accuracy when converting boxes to mask in videos, compared to single image baselines. We further demonstrate the usefulness of our method for weakly supervised VOS training in limited data domain.\n\n\nB. Structure of the object encoder\n\nHere, we describe in detail the network structure employed for the object encoder, as shown in Fig. 4. Note that our object encoder is formulated as (e t , w t , m t ) = B(x t , b t ). The network first takes as input the mask representation of the bounding box b t and then passes it to a convolution layer, a max pooling layer and two residual blocks. The intermediate mask features are then concatenated with deep features x t and fed through another residual block, which reduces the feature dimension. Finally, two similar heads are utilized to produce abstract embedding e t and weight w t . Although the embedding and weight heads share common network to extract object representations, there is no need to share them for the single-frame encoding m t . Single-frame encoding m t is directly passed to the decoder and has no connection with embedding e t . Thus, we use a different network to obtain the single-frame encoding m t . This network has the same architecture as the network used to obtain e t and w t , with the only difference that it has a single head with a convolution and ReLU layers to predict e t .\n\n\nC. Detailed analysis\n\nHere, we provide a more detailed analysis of our approach for predicting object masks from bounding boxes in videos.  Table 8. Impact of the number of steepest-descent iterations. Results are shown in terms of Jaccard J index.\n\nImpact of the number of steepest-descent iterations: We perform 5 steepest-descent algorithm iterations during training in order to save training time and speed up convergence. However, there is no need to only iterate 5 times during inference. We perform experiments to analyse the impact of iterating more times and give results in Tab. 8. Increasing the number of iterations from 5 to 15, the J score increases by 1.2 on DAVIS2017 validation set. The performance of our approach saturates when iterating more than 15 times on YouTube-VOS and DAVIS. Impact of the sample size: During training, we crop a patch 5 times larger than the ground truth bounding box to exploit background consistency. However, we find that it may be harmful to include too much unnecessary environment information. Therefore, we perform experiments to analyse the results if we crop a smaller patch. The size of the crop is set to different scaling factors relative to the object bounding box size. The results on YT300 and DAVIS2017 validation set are shown in Tab. 9. There is a significant improvement of +1.2 and +0.7 in J score on YT300 and DAVIS when increasing the sample scale from 2 to 3, meaning including more background information leads to better results. We achieve best results when setting the sample scale as 3 on YouTube-VOS and 4 on DAVIS. A larger patch will contain too much noisy environment information, leading to degradation in performance. We set sample scale to 4 during inference in all our experiments. Impact of the inter-frame interval: Here, we analyse the inter-frame interval used for DAVIS and YouTube-VOS. Results in terms of Jaccard J score are shown in Tab. 10   Table 9. Impact of the image sample size relative to the object bounding box. Results are reported in terms of Jaccard J score.\n\nInter-frame interval 1 5 10 15 DAVIS2017 val 80.5 81.2 80.9 80.5 Table 10. Impact of the inter-frame interval on DAVIS. Results are shown in terms of Jaccard J index. and Tab. 11. Compared to using consecutive frames, selecting every 5th frame gives an improvement of +0.7 in J score on DAVIS2017 validation set. The performance will decrease if we use larger inter-frame interval. While on YouTube-VOS, since the sequence is annotated every 5 frames, the minimal inter-frame interval that can be used is 5. There is no substantial difference of using different interframe intervals on YT300, so we generally set a value 15 for other experiments.\n\nInter-frame interval 5 10 15 20 25 YT300 85.6 85.7 85.6 85.6 85.5 Table 11. Impact of the inter-frame interval on YT300. Results are shown in terms of Jaccard J index.\n\n\nD. Additional inference details\n\nWe describe details about how we annotate large-scale tracking datasets LaSOT and GOT10k. Generally, we use the same inference setting as used for YouTube-VOS, except for the number of frames. We select 5 frames for each testing frame in order to save inference time and ensure the high performance at the same time. We annotate every frame of the sequence on GOT10K, while on LaSOT, we only annotate every 5th frame. LaSOT contains very long sequences and the objects generally move slowly. There is no need to annotate adjacent frames because they are highly correlated. Moreover, we only annotate up to 200 frames from a video sequence to avoid generating too much data for the same object. \n\n\nE. Detailed results\n\nGOT10K: In this section, we provide success plots on the GOT10k validation set in Fig. 7. The success plots are obtained using the Overlap Precision (OP) score. The OP score at a threshold \u03c4 denotes the fraction of frames in which the intersection-over-union (IoU) overlap between the tracker prediction and the ground truth box is greater than \u03c4 . The OP scores for a range of thresholds in [0, 1] are plotted to obtain the success plots. The trackers are ranked according the the average overlap (AO) score, which is computed as the average IoU overlap between the tracker prediction and the ground truth box over all frames in the dataset. The LWL model trained using our weakly annotated tracking data (LWL-Ours) obtains the best AO score, outperforming the standard LWL model 2.1%. YouTube-VOS: We compare our approach with state-ofthe-art on YouTube-VOS 2018 validation set (see Tab. 12). LWL-Ours obtains comparable results with standard LWL and outperforms other VOS methods significantly.\n\nOSVOS [9] OnAVOS [60] PreMVOS [39] SiamRCNN [ \n\n\nF. Qualitative results\n\nWe show more qualitative results on DAVIS and GOT10k in Fig. 5. and Fig. 6, respectively. Compared to the ground truth masks on DAVIS, our approach gives highquality results for frames containing only a single object. For frames containing multiple objects, our approach can still delineate object boundary accurately if the overlapping problem is not severe.\n\nFigure 4 .\n4Architecture\n\nFigure 5 .\n5Additional qualitative results of our box to mask conversion network (third row) on DAVIS compared to the ground truth masks (second row).\n\nFigure 6 .\n6Additional qualitative results of our box to mask conversion network on GOT10k.\n\nFigure 7 .\n7Success plots on the GOT10k validation set. The trackers are ranked using the average overlap (AO) score in percentage.\n\n\nTable 1. Impact of using multiple frames for box to mask conversion. Results are shown in terms of Jaccard J index.Box2Seg \nOurs \nNum. Frames \n1 \n1 \n3 \n5 \n7 \n9 \n11 \n\nYT300 \n-\n84.2 85.2 85.5 85.6 85.6 85.6 \nDAVIS2017 val \n79.3 \n78.7 80.4 80.9 81.1 81.2 81.2 \n\nture is provided in Sec. 4.1. We demonstrate the effective-\nness of our approach for weakly supervised training of VOS \nin Sec 4.2. Finally, in Sec. 4.3, we use our network to anno-\ntate large-scale tracking datasets and show improved perfor-\nmance on tracking datasets using the generated annotations. \n\n\n\n\nTable 2. Impact of different components in the proposed approach. Results are reported in terms of Jaccard J score.m t s t Iter. YT300 DAVIS2017 val \nSingleImage \n83.3 \n77.2 \nMultiFrame \n84.6 \n79.8 \nMultiFrame+ \n84.8 \n80.1 \nMultiFrameIterative \n85.6 \n81.2 \n\n\n\n\nSTM AlphaRef OceanPlus RPT LWL LWL-OursEAO \n0.308 0.482 \n0.491 \n0.530 0.463 \n0.510 \nAccuracy 0.751 0.754 \n0.685 \n0.700 0.719 \n0.732 \nRobustness 0.574 0.777 \n0.842 \n0.869 0.798 \n0.824 \n\n\n\nTable 4 .\n4State-of-the-art comparison on VOT2020 in terms of ex-\npected average overlap (EAO), accuracy, and robustness. \n\nSiamRPN++ [34] DiMP-50 [6] PrDiMP-50 [15] LWL LWL-Ours \n\nSR0.5 (%) \n82.8 \n88.7 \n89.6 \n92.4 \n95.1 \nSR0.75 (%) \n-\n68.8 \n72.8 \n82.2 \n85.2 \nAO (%) \n73.0 \n75.3 \n77.8 \n84.6 \n86.7 \n\n\n\nTable 6 .\n6State-of-the-art comparison on the TrackingNet test set in terms of precision, normalized precision, and success.STM [44] LWL LWL-Ours \n\nJ &F mean \n79.2 \n81.0 \n80.6 \n\nTable 7. Comparison on YouTube-VOS 2019 validation set. \n\n\n\n\nTable 12. Comparison on YouTube-VOS 2018 validation set.61] \n\nSTM \n[44] \nLWL LWL-Ours \n\nJ &F mean 58.8 \n55.2 \n66.9 \n73.2 \n79.4 81.5 \n80.9 \nJseen \n59.8 \n60.1 \n71.4 \n73.5 \n79.7 80.4 \n79.6 \nJunseen \n54.2 \n46.1 \n56.5 \n66.2 \n72.8 76.4 \n75.7 \nFseen \n60.5 \n62.7 \n-\n-\n84.2 84.9 \n83.9 \nFunseen \n60.7 \n51.4 \n-\n-\n80.9 84.4 \n84.3 \n\n\nAcknowledgements: This work was partly supported by the ETH Z\u00fcrich Fund (OK), a Huawei Gift for research, a Huawei Technologies Oy (Finland) project, an Amazon AWS grant, and an Nvidia hardware grant.AppendixIn this Appendix, we provide additional details and analysis of our approach. In Appendix B, we provide details about the network architecture of our object encoder module. Appendix C provides additional analysis on the impact of different hyper-parameters in the proposed approach. Details about the inference setting used to annotate the tracking datasets in Section 4.3 of the main paper are provided in Appendix D. Detailed results on the GOT10k validation set and YouTube-VOS 2018 validation set are provided in Appendix E. Appendix F includes additional qualitative results on the DAVIS and GOT10k datasets. Additionally, we also include a video showing the results of our approach on the sequences from the DAVIS and GOT10k datasets.A. Included videoWe provide a video showing the masks produced by our approach for sequences from DAVIS and GOT10k datasets. Our approach generates high-quality masks even under difficult circumstances, such as fast motions, appearance change and shape change. The last sequence, bike-packing from DAVIS, shows a particularly challenging case where two objects are highly overlapping.\nLearning pixel-level semantic affinity with image-level supervision for weakly supervised semantic segmentation. Jiwoon Ahn, Suha Kwak, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionJiwoon Ahn and Suha Kwak. Learning pixel-level semantic affinity with image-level supervision for weakly supervised semantic segmentation. In Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recognition, pages 4981-4990, 2018. 2\n\nWhat's the point: Semantic segmentation with point supervision. Amy Bearman, Olga Russakovsky, Vittorio Ferrari, Li Fei-Fei, European Conference on Computer Vision. SpringerAmy Bearman, Olga Russakovsky, Vittorio Ferrari, and Li Fei-Fei. What's the point: Semantic segmentation with point supervision. In European Conference on Computer Vision, pages 549-565. Springer, 2016. 2\n\nMeta learning deep visual words for fast video object segmentation. Harkirat Singh Behl, Mohammad Najafi, Anurag Arnab, Philip Hs Torr, arXiv:1812.01397arXiv preprintHarkirat Singh Behl, Mohammad Najafi, Anurag Arnab, and Philip HS Torr. Meta learning deep visual words for fast video object segmentation. arXiv preprint arXiv:1812.01397, 2018. 2\n\nThe lov\u00e1sz-softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks. Maxim Berman, Amal Rannen Triki, Matthew B Blaschko, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionMaxim Berman, Amal Rannen Triki, and Matthew B Blaschko. The lov\u00e1sz-softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4413- 4421, 2018. 6\n\nMeta-learning with differentiable closedform solvers. Luca Bertinetto, Jo\u00e3o F Henriques, H S Philip, Andrea Torr, Vedaldi, 7th International Conference on Learning Representations, ICLR 2019. New Orleans, LA, USALuca Bertinetto, Jo\u00e3o F. Henriques, Philip H. S. Torr, and Andrea Vedaldi. Meta-learning with differentiable closed- form solvers. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, 2019. 5\n\nLearning discriminative model prediction for tracking. Goutam Bhat, Martin Danelljan, Luc Van Gool, Radu Timofte, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer Vision4Goutam Bhat, Martin Danelljan, Luc Van Gool, and Radu Timofte. Learning discriminative model prediction for track- ing. In Proceedings of the IEEE International Conference on Computer Vision, pages 6182-6191, 2019. 4, 5, 8\n\nKnow your surroundings: Exploiting scene information for object tracking. Goutam Bhat, Martin Danelljan, Luc Van Gool, Radu Timofte, Proceedings of the European Conference on Computer Vision (ECCV. the European Conference on Computer Vision (ECCVGoutam Bhat, Martin Danelljan, Luc Van Gool, and Radu Timofte. Know your surroundings: Exploiting scene infor- mation for object tracking. In Proceedings of the European Conference on Computer Vision (ECCV). 8\n\nLearning what to learn for video object segmentation. Goutam Bhat, Felix J\u00e4remo Lawin, Martin Danelljan, Andreas Robinson, Michael Felsberg, Luc Van Gool, Radu Timofte, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)7Goutam Bhat, Felix J\u00e4remo Lawin, Martin Danelljan, An- dreas Robinson, Michael Felsberg, Luc Van Gool, and Radu Timofte. Learning what to learn for video object segmenta- tion. In Proceedings of the European Conference on Com- puter Vision (ECCV), 2020. 1, 2, 4, 5, 6, 7, 8\n\nOneshot video object segmentation. Kevis-Kokitsi Sergi Caelles, Jordi Maninis, Laura Pont-Tuset, Daniel Leal-Taix\u00e9, Luc Cremers, Van Gool, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition214Sergi Caelles, Kevis-Kokitsi Maninis, Jordi Pont-Tuset, Laura Leal-Taix\u00e9, Daniel Cremers, and Luc Van Gool. One- shot video object segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 221-230, 2017. 2, 14\n\nEncoder-decoder with atrous separable convolution for semantic image segmentation. Y Liang-Chieh Chen, G Zhu, Florian Papandreou, H Schroff, Adam, In ECCV. 3Liang-Chieh Chen, Y. Zhu, G. Papandreou, Florian Schroff, and H. Adam. Encoder-decoder with atrous separable con- volution for semantic image segmentation. In ECCV, 2018. 3\n\nBlazingly fast video object segmentation with pixel-wise metric learning. Yuhua Chen, Jordi Pont-Tuset, Alberto Montes, Luc Van Gool, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionYuhua Chen, Jordi Pont-Tuset, Alberto Montes, and Luc Van Gool. Blazingly fast video object segmentation with pixel-wise metric learning. In Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recognition, pages 1189-1198, 2018. 2\n\nDetecting and tracking moving objects for video surveillance. Isaac Cohen, Gerard Medioni, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149). 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149)IEEE2Isaac Cohen and Gerard Medioni. Detecting and tracking moving objects for video surveillance. In Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), volume 2, pages 319-325. IEEE, 1999. 1\n\nBoxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation. Jifeng Dai, Kaiming He, Jian Sun, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionJifeng Dai, Kaiming He, and Jian Sun. Boxsup: Exploit- ing bounding boxes to supervise convolutional networks for semantic segmentation. In Proceedings of the IEEE Inter- national Conference on Computer Vision, pages 1635-1643, 2015. 2\n\nInstance-aware semantic segmentation via multi-task network cascades. Jifeng Dai, Kaiming He, Jian Sun, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionJifeng Dai, Kaiming He, and Jian Sun. Instance-aware se- mantic segmentation via multi-task network cascades. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3150-3158, 2016. 2\n\nProbabilistic regression for visual tracking. Martin Danelljan, Luc Van Gool, Radu Timofte, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionMartin Danelljan, Luc Van Gool, and Radu Timofte. Prob- abilistic regression for visual tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7183-7192, 2020. 8\n\nAdaptive cartooning for privacy protection in camera networks. Ad\u00e1m Erd\u00e9lyi, Tibor Bar\u00e1t, Patrick Valet, Thomas Winkler, Bernhard Rinner, 11th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS). IEEEAd\u00e1m Erd\u00e9lyi, Tibor Bar\u00e1t, Patrick Valet, Thomas Winkler, and Bernhard Rinner. Adaptive cartooning for privacy pro- tection in camera networks. In 2014 11th IEEE International Conference on Advanced Video and Signal Based Surveil- lance (AVSS), pages 44-49. IEEE, 2014. 1\n\nLasot: A high-quality benchmark for large-scale single object tracking. Liting Heng Fan, Fan Lin, Peng Yang, Ge Chu, Sijia Deng, Hexin Yu, Yong Bai, Chunyuan Xu, Haibin Liao, Ling, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition1Heng Fan, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Si- jia Yu, Hexin Bai, Yong Xu, Chunyuan Liao, and Haibin Ling. Lasot: A high-quality benchmark for large-scale single object tracking. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5374-5383, 2019. 1, 8\n\nDeep declarative networks: A new hope. CoRR, abs/1909.04866. Stephen Gould, Richard Hartley, Dylan Campbell, 5Stephen Gould, Richard Hartley, and Dylan Campbell. Deep declarative networks: A new hope. CoRR, abs/1909.04866, 2019. 2, 4, 5\n\nSimultaneous detection and segmentation. Pablo Bharath Hariharan, Ross Arbel\u00e1ez, Jitendra Girshick, Malik, European Conference on Computer Vision. SpringerBharath Hariharan, Pablo Arbel\u00e1ez, Ross Girshick, and Ji- tendra Malik. Simultaneous detection and segmentation. In European Conference on Computer Vision, pages 297-312. Springer, 2014. 2\n\nPiotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. Kaiming He, Georgia Gkioxari, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionKaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Gir- shick. Mask r-cnn. In Proceedings of the IEEE International Conference on Computer Vision, pages 2961-2969, 2017. 2\n\nDelving deep into rectifiers: Surpassing human-level performance on imagenet classification. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level per- formance on imagenet classification. In Proceedings of the IEEE International Conference on Computer Vision, pages 1026-1034, 2015. 6\n\nWeakly supervised instance segmentation using the bounding box tightness prior. Cheng-Chun Hsu, Kuang-Jui Hsu, Chung-Chi Tsai, Yen-Yu Lin, Yung-Yu Chuang, Advances in Neural Information Processing Systems. 27Cheng-Chun Hsu, Kuang-Jui Hsu, Chung-Chi Tsai, Yen- Yu Lin, and Yung-Yu Chuang. Weakly supervised instance segmentation using the bounding box tightness prior. In Advances in Neural Information Processing Systems, pages 6586-6597, 2019. 2, 7\n\nVideomatch: Matching based video object segmentation. Yuan-Ting Hu, Jia-Bin Huang, Alexander G Schwing, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Yuan-Ting Hu, Jia-Bin Huang, and Alexander G Schwing. Videomatch: Matching based video object segmentation. In Proceedings of the European Conference on Computer Vi- sion (ECCV), pages 54-70, 2018. 2\n\nGot-10k: A large high-diversity benchmark for generic object tracking in the wild. Lianghua Huang, Xin Zhao, Kaiqi Huang, IEEE Transactions on Pattern Analysis and Machine Intelligence. Lianghua Huang, Xin Zhao, and Kaiqi Huang. Got-10k: A large high-diversity benchmark for generic object tracking in the wild. IEEE Transactions on Pattern Analysis and Ma- chine Intelligence, page 1-1, 2019. 1, 8\n\nWeakly-supervised semantic segmentation network with deep seeded region growing. Zilong Huang, Xinggang Wang, Jiasi Wang, Wenyu Liu, Jingdong Wang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionZilong Huang, Xinggang Wang, Jiasi Wang, Wenyu Liu, and Jingdong Wang. Weakly-supervised semantic segmentation network with deep seeded region growing. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7014-7023, 2018. 2\n\nSimple does it: Weakly supervised instance and semantic segmentation. Anna Khoreva, Rodrigo Benenson, Jan Hosang, Matthias Hein, Bernt Schiele, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionAnna Khoreva, Rodrigo Benenson, Jan Hosang, Matthias Hein, and Bernt Schiele. Simple does it: Weakly supervised instance and semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion, pages 876-885, 2017. 2\n\nLucid data dreaming for object tracking. Anna Khoreva, Rodrigo Benenson, Eddy Ilg, Thomas Brox, Bernt Schiele, The DAVIS Challenge on Video Object Segmentation. Anna Khoreva, Rodrigo Benenson, Eddy Ilg, Thomas Brox, and Bernt Schiele. Lucid data dreaming for object track- ing. In The DAVIS Challenge on Video Object Segmentation, 2017. 2\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980arXiv preprintDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 6\n\nSeed, expand and constrain: Three principles for weakly-supervised image segmentation. Alexander Kolesnikov, Christoph H Lampert, European Conference on Computer Vision. SpringerAlexander Kolesnikov and Christoph H Lampert. Seed, ex- pand and constrain: Three principles for weakly-supervised image segmentation. In European Conference on Computer Vision, pages 695-711. Springer, 2016. 2\n\nThe new vot2020 short-term tracking performance evaluation protocol and measures. Matej Kristan, Alan Lukezic, Martin Danelljan, Jiri Luka Cehovin Zajc, Matas, Matej Kristan, Alan Lukezic, Martin Danelljan, Luka Ce- hovin Zajc, and Jiri Matas. The new vot2020 short-term tracking performance evaluation protocol and measures. 8\n\nBox2seg: Attention weighted loss and discriminative feature learning for weakly supervised segmentation. Viveka Kulharia, Siddhartha Chandra, Amit Agrawal, Philip Torr, Ambrish Tyagi, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)2020Viveka Kulharia, Siddhartha Chandra, Amit Agrawal, Philip Torr, and Ambrish Tyagi. Box2seg: Attention weighted loss and discriminative feature learning for weakly supervised segmentation. In Proceedings of the European Conference on Computer Vision (ECCV), 2020. 2\n\nShapemask: Learning to segment novel objects by refining shape priors. Weicheng Kuo, Anelia Angelova, Jitendra Malik, Tsung-Yi Lin, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionWeicheng Kuo, Anelia Angelova, Jitendra Malik, and Tsung-Yi Lin. Shapemask: Learning to segment novel ob- jects by refining shape priors. In Proceedings of the IEEE International Conference on Computer Vision, pages 9207- 9216, 2019. 3\n\nMeta-learning with differentiable convex optimization. Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, Stefano Soatto, IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019. Long Beach, CA, USA45Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and Stefano Soatto. Meta-learning with differentiable convex op- timization. In IEEE Conference on Computer Vision and Pat- tern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 10657-10665, 2019. 4, 5\n\nSiamrpn++: Evolution of siamese visual tracking with very deep networks. Bo Li, Wei Wu, Qiang Wang, Fangyi Zhang, Junliang Xing, Junjie Yan, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionBo Li, Wei Wu, Qiang Wang, Fangyi Zhang, Junliang Xing, and Junjie Yan. Siamrpn++: Evolution of siamese visual tracking with very deep networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recog- nition, pages 4282-4291, 2019. 8\n\nDeep object co-segmentation. Weihao Li, Omid Hosseini Jafari, Carsten Rother, Asian Conference on Computer Vision. SpringerWeihao Li, Omid Hosseini Jafari, and Carsten Rother. Deep object co-segmentation. In Asian Conference on Computer Vision, pages 638-653. Springer, 2018. 3\n\nVideo object segmentation with joint re-identification and attention-aware mask propagation. Xiaoxiao Li, Chen Change Loy, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Xiaoxiao Li and Chen Change Loy. Video object segmen- tation with joint re-identification and attention-aware mask propagation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 90-105, 2018. 2\n\nFully convolutional instance-aware semantic segmentation. Yi Li, Haozhi Qi, Jifeng Dai, Xiangyang Ji, Yichen Wei, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionYi Li, Haozhi Qi, Jifeng Dai, Xiangyang Ji, and Yichen Wei. Fully convolutional instance-aware semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2359-2367, 2017. 2\n\nScribblesup: Scribble-supervised convolutional networks for semantic segmentation. Di Lin, Jifeng Dai, Jiaya Jia, Kaiming He, Jian Sun, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionDi Lin, Jifeng Dai, Jiaya Jia, Kaiming He, and Jian Sun. Scribblesup: Scribble-supervised convolutional networks for semantic segmentation. In Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recognition, pages 3159-3167, 2016. 2\n\nPremvos: Proposal-generation, refinement and merging for video object segmentation. Jonathon Luiten, Paul Voigtlaender, Bastian Leibe, Asian Conference on Computer Vision. Springer614Jonathon Luiten, Paul Voigtlaender, and Bastian Leibe. Pre- mvos: Proposal-generation, refinement and merging for video object segmentation. In Asian Conference on Com- puter Vision, pages 565-580. Springer, 2018. 2, 3, 6, 14\n\nVideo object segmentation without temporal information. Kevis-Kokitsi Maninis, Sergi Caelles, Yuhua Chen, Jordi Pont-Tuset, Laura Leal-Taix\u00e9, Daniel Cremers, Luc Van Gool, IEEE Transactions on Pattern Analysis and Machine Intelligence. 416Kevis-Kokitsi Maninis, Sergi Caelles, Yuhua Chen, Jordi Pont-Tuset, Laura Leal-Taix\u00e9, Daniel Cremers, and Luc Van Gool. Video object segmentation without temporal in- formation. IEEE Transactions on Pattern Analysis and Ma- chine Intelligence, 41(6):1515-1530, 2018. 2\n\nDeep extreme cut: From extreme points to object segmentation. Kevis-Kokitsi Maninis, Sergi Caelles, Jordi Pont-Tuset, Luc Van Gool, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionKevis-Kokitsi Maninis, Sergi Caelles, Jordi Pont-Tuset, and Luc Van Gool. Deep extreme cut: From extreme points to object segmentation. In Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recognition, pages 616-625, 2018. 2\n\nmaskrcnn-benchmark: Fast, modular reference implementation of Instance Segmentation and Object Detection algorithms in PyTorch. Francisco Massa, Ross Girshick, 28/10/2020. 6Francisco Massa and Ross Girshick. maskrcnn-benchmark: Fast, modular reference implementation of Instance Seg- mentation and Object Detection algorithms in PyTorch. https://github.com/facebookresearch/ maskrcnn-benchmark, 2018. Accessed: 28/10/2020. 6\n\nTrackingnet: A large-scale dataset and benchmark for object tracking in the wild. Matthias Muller, Adel Bibi, Silvio Giancola, Salman Alsubaihi, Bernard Ghanem, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)1Matthias Muller, Adel Bibi, Silvio Giancola, Salman Al- subaihi, and Bernard Ghanem. Trackingnet: A large-scale dataset and benchmark for object tracking in the wild. In Proceedings of the European Conference on Computer Vi- sion (ECCV), pages 300-317, 2018. 1, 8\n\nVideo object segmentation using space-time memory networks. Joon-Young Seoung Wug Oh, Ning Lee, Seon Joo Xu, Kim, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer Vision14Seoung Wug Oh, Joon-Young Lee, Ning Xu, and Seon Joo Kim. Video object segmentation using space-time memory networks. In Proceedings of the IEEE International Confer- ence on Computer Vision, pages 9226-9235, 2019. 1, 2, 8, 14\n\nWeakly-and semi-supervised learning of a deep convolutional network for semantic image segmentation. George Papandreou, Liang-Chieh Chen, Kevin P Murphy, Alan L Yuille, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionGeorge Papandreou, Liang-Chieh Chen, Kevin P Murphy, and Alan L Yuille. Weakly-and semi-supervised learning of a deep convolutional network for semantic image segmenta- tion. In Proceedings of the IEEE International Conference on Computer Vision, pages 1742-1750, 2015. 2\n\nLearning video object segmentation from static images. Federico Perazzi, Anna Khoreva, Rodrigo Benenson, Bernt Schiele, Alexander Sorkine-Hornung, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionFederico Perazzi, Anna Khoreva, Rodrigo Benenson, Bernt Schiele, and Alexander Sorkine-Hornung. Learning video object segmentation from static images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2663-2672, 2017. 2\n\nFrom image-level to pixel-level labeling with convolutional networks. O Pedro, Ronan Pinheiro, Collobert, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionPedro O Pinheiro and Ronan Collobert. From image-level to pixel-level labeling with convolutional networks. In Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1713-1721, 2015. 2\n\nMultiscale combinatorial grouping for image segmentation and object proposal generation. Jordi Pont-Tuset, Pablo Arbelaez, Jonathan T Barron, Ferran Marques, Jitendra Malik, IEEE Transactions on Pattern Analysis and Machine Intelligence. 391Jordi Pont-Tuset, Pablo Arbelaez, Jonathan T Barron, Fer- ran Marques, and Jitendra Malik. Multiscale combinatorial grouping for image segmentation and object proposal gener- ation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(1):128-140, 2016. 2\n\nJordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbel\u00e1ez, Alexander Sorkine-Hornung, Luc Van Gool, arXiv:1704.00675The 2017 davis challenge on video object segmentation. Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Ar- bel\u00e1ez, Alexander Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv:1704.00675, 2017. 1, 2, 5\n\nLearning fast and robust target models for video object segmentation. Andreas Robinson, Felix Jaremo Lawin, Martin Danelljan, Michael Fahad Shahbaz Khan, Felsberg, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition26Andreas Robinson, Felix Jaremo Lawin, Martin Danelljan, Fahad Shahbaz Khan, and Michael Felsberg. Learning fast and robust target models for video object segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition, pages 7406-7415, 2020. 2, 6\n\nVisionbased offline-online perception paradigm for autonomous driving. German Ros, Sebastian Ramos, Manuel Granados, Amir Bakhtiary, David Vazquez, Antonio M Lopez, 2015 IEEE Winter Conference on Applications of Computer Vision. IEEEGerman Ros, Sebastian Ramos, Manuel Granados, Amir Bakhtiary, David Vazquez, and Antonio M Lopez. Vision- based offline-online perception paradigm for autonomous driving. In 2015 IEEE Winter Conference on Applications of Computer Vision, pages 231-238. IEEE, 2015. 1\n\ngrabcut\" interactive foreground extraction using iterated graph cuts. Carsten Rother, Vladimir Kolmogorov, Andrew Blake, ACM Transactions on Graphics (TOG). 233Carsten Rother, Vladimir Kolmogorov, and Andrew Blake. \" grabcut\" interactive foreground extraction using iter- ated graph cuts. ACM Transactions on Graphics (TOG), 23(3):309-314, 2004. 2\n\nCosegmentation of image pairs by histogram matching-incorporating a global constraint into mrfs. Carsten Rother, Tom Minka, Andrew Blake, Vladimir Kolmogorov, IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06). IEEE1Carsten Rother, Tom Minka, Andrew Blake, and Vladimir Kolmogorov. Cosegmentation of image pairs by histogram matching-incorporating a global constraint into mrfs. In 2006 IEEE Computer Society Conference on Computer Vi- sion and Pattern Recognition (CVPR'06), volume 1, pages 993-1000. IEEE, 2006. 3\n\nUnsupervised joint object discovery and segmentation in internet images. Michael Rubinstein, Armand Joulin, Johannes Kopf, Ce Liu, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionMichael Rubinstein, Armand Joulin, Johannes Kopf, and Ce Liu. Unsupervised joint object discovery and segmentation in internet images. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1939-1946, 2013. 3\n\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C Berg, Li Fei-Fei, ImageNet Large Scale Visual Recognition Challenge. IJCV. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San- jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Chal- lenge. IJCV, pages 1-42, April 2015. 1\n\nKangaroo vehicle collision detection using deep semantic segmentation convolutional neural network. Khaled Saleh, Mohammed Hossny, Saeid Nahavandi, 2016 International Conference on Digital Image Computing: Techniques and Applications (DICTA). IEEEKhaled Saleh, Mohammed Hossny, and Saeid Nahavandi. Kangaroo vehicle collision detection using deep semantic segmentation convolutional neural network. In 2016 Interna- tional Conference on Digital Image Computing: Techniques and Applications (DICTA), pages 1-7. IEEE, 2016. 1\n\nOn regularized losses for weakly-supervised cnn segmentation. Meng Tang, Federico Perazzi, Abdelaziz Djelouah, Ismail Ben Ayed, Christopher Schroers, Yuri Boykov, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Meng Tang, Federico Perazzi, Abdelaziz Djelouah, Ismail Ben Ayed, Christopher Schroers, and Yuri Boykov. On reg- ularized losses for weakly-supervised cnn segmentation. In Proceedings of the European Conference on Computer Vi- sion (ECCV), pages 507-522, 2018. 7\n\nLearning randomwalk label propagation for weakly-supervised semantic segmentation. Paul Vernaza, Manmohan Chandraker, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionPaul Vernaza and Manmohan Chandraker. Learning random- walk label propagation for weakly-supervised semantic seg- mentation. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition, pages 7158-7166, 2017. 2\n\nFeelvos: Fast end-to-end embedding learning for video object segmentation. Paul Voigtlaender, Yuning Chai, Florian Schroff, Hartwig Adam, Bastian Leibe, Liang-Chieh Chen, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition1Paul Voigtlaender, Yuning Chai, Florian Schroff, Hartwig Adam, Bastian Leibe, and Liang-Chieh Chen. Feelvos: Fast end-to-end embedding learning for video object segmenta- tion. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9481-9490, 2019. 1, 2\n\nOnline adaptation of convolutional neural networks for video object segmentation. Paul Voigtlaender, Bastian Leibe, BMVC. 214Paul Voigtlaender and Bastian Leibe. Online adaptation of convolutional neural networks for video object segmenta- tion. In BMVC, 2017. 2, 14\n\nSiam r-cnn: Visual tracking by re-detection. Paul Voigtlaender, Jonathon Luiten, H S Philip, Bastian Torr, Leibe, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition814Paul Voigtlaender, Jonathon Luiten, Philip HS Torr, and Bas- tian Leibe. Siam r-cnn: Visual tracking by re-detection. In Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition, pages 6578-6588, 2020. 2, 8, 14\n\nFast video object segmentation by referenceguided mask propagation. Joon-Young Seoung Wug Oh, Kalyan Lee, Seon Joo Sunkavalli, Kim, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionSeoung Wug Oh, Joon-Young Lee, Kalyan Sunkavalli, and Seon Joo Kim. Fast video object segmentation by reference- guided mask propagation. In Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recognition, pages 7376-7385, 2018. 2\n\nYoutube-vos: A large-scale video object segmentation benchmark. Ning Xu, Linjie Yang, Yuchen Fan, Dingcheng Yue, Yuchen Liang, Jianchao Yang, Thomas Huang, arXiv:1809.03327arXiv preprintNing Xu, Linjie Yang, Yuchen Fan, Dingcheng Yue, Yuchen Liang, Jianchao Yang, and Thomas Huang. Youtube-vos: A large-scale video object segmentation benchmark. arXiv preprint arXiv:1809.03327, 2018. 1, 2, 5\n\nDeep-dense conditional random fields for object co-segmentation. Tong Ze-Huan Yuan, Yirui Lu, Wu, IJCAI. Ze-Huan Yuan, Tong Lu, and Yirui Wu. Deep-dense condi- tional random fields for object co-segmentation. In IJCAI, pages 3371-3377, 2017. 3\n\nWeakly supervised instance segmentation using class peak response. Yanzhao Zhou, Yi Zhu, Qixiang Ye, Qiang Qiu, Jianbin Jiao, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionYanzhao Zhou, Yi Zhu, Qixiang Ye, Qiang Qiu, and Jian- bin Jiao. Weakly supervised instance segmentation using class peak response. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3791- 3800, 2018. 2\n", "annotations": {"author": "[{\"end\":140,\"start\":81},{\"end\":223,\"start\":141},{\"end\":316,\"start\":224},{\"end\":375,\"start\":317},{\"end\":436,\"start\":376},{\"end\":516,\"start\":437}]", "publisher": null, "author_last_name": "[{\"end\":89,\"start\":86},{\"end\":152,\"start\":146},{\"end\":240,\"start\":234},{\"end\":324,\"start\":321},{\"end\":385,\"start\":381},{\"end\":444,\"start\":437}]", "author_first_name": "[{\"end\":85,\"start\":81},{\"end\":145,\"start\":141},{\"end\":233,\"start\":224},{\"end\":320,\"start\":317},{\"end\":380,\"start\":376}]", "author_affiliation": "[{\"end\":139,\"start\":91},{\"end\":222,\"start\":174},{\"end\":315,\"start\":267},{\"end\":374,\"start\":326},{\"end\":435,\"start\":387},{\"end\":515,\"start\":467}]", "title": "[{\"end\":78,\"start\":1},{\"end\":594,\"start\":517}]", "venue": null, "abstract": "[{\"end\":2120,\"start\":596}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b50\"},\"end\":2251,\"start\":2247},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":2254,\"start\":2251},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2273,\"start\":2269},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2276,\"start\":2273},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2390,\"start\":2387},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":2393,\"start\":2390},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":2396,\"start\":2393},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":2639,\"start\":2635},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":2642,\"start\":2639},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3142,\"start\":3138},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3145,\"start\":3142},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":3148,\"start\":3145},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":3151,\"start\":3148},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":3909,\"start\":3905},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":3912,\"start\":3909},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5232,\"start\":5228},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7046,\"start\":7043},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":7049,\"start\":7046},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":7052,\"start\":7049},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7087,\"start\":7083},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":7090,\"start\":7087},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":7093,\"start\":7090},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":7096,\"start\":7093},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7130,\"start\":7126},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7133,\"start\":7130},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":7136,\"start\":7133},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":7139,\"start\":7136},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7176,\"start\":7173},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7178,\"start\":7176},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":7181,\"start\":7178},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":7312,\"start\":7308},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":7333,\"start\":7329},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":7685,\"start\":7681},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":7688,\"start\":7685},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7905,\"start\":7902},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7908,\"start\":7905},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7911,\"start\":7908},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":7914,\"start\":7911},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":7917,\"start\":7914},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7929,\"start\":7926},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":7932,\"start\":7929},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":7948,\"start\":7944},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":7951,\"start\":7948},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7975,\"start\":7971},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7978,\"start\":7975},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7981,\"start\":7978},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7984,\"start\":7981},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":7987,\"start\":7984},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8062,\"start\":8058},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8065,\"start\":8062},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":8138,\"start\":8134},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":8165,\"start\":8161},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8183,\"start\":8179},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8295,\"start\":8291},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8944,\"start\":8940},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8947,\"start\":8944},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8950,\"start\":8947},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8953,\"start\":8950},{\"end\":9153,\"start\":9145},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9792,\"start\":9788},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":9934,\"start\":9930},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9966,\"start\":9962},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":10530,\"start\":10526},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":10658,\"start\":10654},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":10814,\"start\":10810},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":10976,\"start\":10972},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":17614,\"start\":17610},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":17687,\"start\":17684},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":17690,\"start\":17687},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":17731,\"start\":17728},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":20320,\"start\":20316},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":20323,\"start\":20320},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":20354,\"start\":20351},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":20538,\"start\":20535},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":20540,\"start\":20538},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":24149,\"start\":24145},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":24169,\"start\":24165},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":24851,\"start\":24847},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":24905,\"start\":24901},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":24966,\"start\":24963},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":25002,\"start\":24998},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":25760,\"start\":25757},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":25763,\"start\":25760},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":26143,\"start\":26140},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":27868,\"start\":27864},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":32127,\"start\":32123},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":33720,\"start\":33716},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":33736,\"start\":33732},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":33964,\"start\":33961},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":34251,\"start\":34247},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":34264,\"start\":34260},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":34286,\"start\":34282},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":34433,\"start\":34429},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":35392,\"start\":35388},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":35404,\"start\":35401},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":35412,\"start\":35409},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":35426,\"start\":35422},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":35680,\"start\":35676},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":36426,\"start\":36422},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":36838,\"start\":36834},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":37000,\"start\":36998},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":43474,\"start\":43471},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":43486,\"start\":43482},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":43499,\"start\":43495},{\"end\":43510,\"start\":43509}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":43923,\"start\":43898},{\"attributes\":{\"id\":\"fig_1\"},\"end\":44075,\"start\":43924},{\"attributes\":{\"id\":\"fig_2\"},\"end\":44168,\"start\":44076},{\"attributes\":{\"id\":\"fig_3\"},\"end\":44301,\"start\":44169},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":44868,\"start\":44302},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":45129,\"start\":44869},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":45317,\"start\":45130},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":45618,\"start\":45318},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":45856,\"start\":45619},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":46179,\"start\":45857}]", "paragraph": "[{\"end\":2797,\"start\":2136},{\"end\":3662,\"start\":2799},{\"end\":4826,\"start\":3664},{\"end\":5666,\"start\":4828},{\"end\":5721,\"start\":5668},{\"end\":6122,\"start\":5723},{\"end\":6681,\"start\":6124},{\"end\":6740,\"start\":6698},{\"end\":7689,\"start\":6742},{\"end\":8746,\"start\":7691},{\"end\":9689,\"start\":8748},{\"end\":10331,\"start\":9691},{\"end\":11326,\"start\":10333},{\"end\":12116,\"start\":11337},{\"end\":13148,\"start\":12118},{\"end\":14554,\"start\":13167},{\"end\":15703,\"start\":14556},{\"end\":15913,\"start\":15705},{\"end\":16856,\"start\":15984},{\"end\":17520,\"start\":16888},{\"end\":18335,\"start\":17522},{\"end\":19011,\"start\":18337},{\"end\":20838,\"start\":19148},{\"end\":21431,\"start\":20840},{\"end\":22155,\"start\":21456},{\"end\":22515,\"start\":22157},{\"end\":22985,\"start\":22565},{\"end\":23497,\"start\":23026},{\"end\":23877,\"start\":23510},{\"end\":24099,\"start\":23941},{\"end\":24771,\"start\":24101},{\"end\":25317,\"start\":24773},{\"end\":26902,\"start\":25344},{\"end\":27032,\"start\":26918},{\"end\":30539,\"start\":27051},{\"end\":31214,\"start\":30574},{\"end\":33420,\"start\":31216},{\"end\":34434,\"start\":33451},{\"end\":35667,\"start\":34436},{\"end\":36408,\"start\":35669},{\"end\":36839,\"start\":36410},{\"end\":37106,\"start\":36856},{\"end\":37672,\"start\":37121},{\"end\":38835,\"start\":37711},{\"end\":39086,\"start\":38860},{\"end\":40895,\"start\":39088},{\"end\":41543,\"start\":40897},{\"end\":41712,\"start\":41545},{\"end\":42442,\"start\":41748},{\"end\":43463,\"start\":42466},{\"end\":43511,\"start\":43465},{\"end\":43897,\"start\":43538}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":15983,\"start\":15914},{\"attributes\":{\"id\":\"formula_1\"},\"end\":19147,\"start\":19012},{\"attributes\":{\"id\":\"formula_2\"},\"end\":22564,\"start\":22516},{\"attributes\":{\"id\":\"formula_3\"},\"end\":23025,\"start\":22986},{\"attributes\":{\"id\":\"formula_4\"},\"end\":23940,\"start\":23878}]", "table_ref": "[{\"end\":28054,\"start\":28047},{\"end\":29708,\"start\":29701},{\"end\":31105,\"start\":31098},{\"end\":32766,\"start\":32759},{\"end\":33111,\"start\":33104},{\"end\":36317,\"start\":36310},{\"end\":38985,\"start\":38978},{\"end\":40775,\"start\":40768},{\"end\":40970,\"start\":40962},{\"end\":41619,\"start\":41611}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2134,\"start\":2122},{\"attributes\":{\"n\":\"2.\"},\"end\":6696,\"start\":6684},{\"attributes\":{\"n\":\"3.\"},\"end\":11335,\"start\":11329},{\"attributes\":{\"n\":\"3.1.\"},\"end\":13165,\"start\":13151},{\"attributes\":{\"n\":\"3.2.\"},\"end\":16886,\"start\":16859},{\"attributes\":{\"n\":\"3.3.\"},\"end\":21454,\"start\":21434},{\"attributes\":{\"n\":\"3.4.\"},\"end\":23508,\"start\":23500},{\"attributes\":{\"n\":\"3.5.\"},\"end\":25342,\"start\":25320},{\"attributes\":{\"n\":\"4.\"},\"end\":26916,\"start\":26905},{\"attributes\":{\"n\":\"4.1.\"},\"end\":27049,\"start\":27035},{\"attributes\":{\"n\":\"4.2.\"},\"end\":30572,\"start\":30542},{\"attributes\":{\"n\":\"4.3.\"},\"end\":33449,\"start\":33423},{\"end\":36854,\"start\":36842},{\"attributes\":{\"n\":\"5.\"},\"end\":37119,\"start\":37109},{\"end\":37709,\"start\":37675},{\"end\":38858,\"start\":38838},{\"end\":41746,\"start\":41715},{\"end\":42464,\"start\":42445},{\"end\":43536,\"start\":43514},{\"end\":43909,\"start\":43899},{\"end\":43935,\"start\":43925},{\"end\":44087,\"start\":44077},{\"end\":44180,\"start\":44170},{\"end\":45328,\"start\":45319},{\"end\":45629,\"start\":45620}]", "table": "[{\"end\":44868,\"start\":44419},{\"end\":45129,\"start\":44986},{\"end\":45317,\"start\":45171},{\"end\":45618,\"start\":45330},{\"end\":45856,\"start\":45744},{\"end\":46179,\"start\":45915}]", "figure_caption": "[{\"end\":43923,\"start\":43911},{\"end\":44075,\"start\":43937},{\"end\":44168,\"start\":44089},{\"end\":44301,\"start\":44182},{\"end\":44419,\"start\":44304},{\"end\":44986,\"start\":44871},{\"end\":45171,\"start\":45132},{\"end\":45744,\"start\":45631},{\"end\":45915,\"start\":45859}]", "figure_ref": "[{\"end\":4489,\"start\":4483},{\"end\":11521,\"start\":11515},{\"end\":28103,\"start\":28097},{\"end\":29243,\"start\":29235},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":37812,\"start\":37806},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":42554,\"start\":42548},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":43600,\"start\":43594},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":43612,\"start\":43606}]", "bib_author_first_name": "[{\"end\":47632,\"start\":47626},{\"end\":47642,\"start\":47638},{\"end\":48106,\"start\":48103},{\"end\":48120,\"start\":48116},{\"end\":48142,\"start\":48134},{\"end\":48154,\"start\":48152},{\"end\":48494,\"start\":48486},{\"end\":48515,\"start\":48507},{\"end\":48530,\"start\":48524},{\"end\":48547,\"start\":48538},{\"end\":48898,\"start\":48893},{\"end\":48911,\"start\":48907},{\"end\":48918,\"start\":48912},{\"end\":48933,\"start\":48926},{\"end\":48935,\"start\":48934},{\"end\":49438,\"start\":49434},{\"end\":49455,\"start\":49451},{\"end\":49457,\"start\":49456},{\"end\":49470,\"start\":49469},{\"end\":49472,\"start\":49471},{\"end\":49487,\"start\":49481},{\"end\":49902,\"start\":49896},{\"end\":49915,\"start\":49909},{\"end\":49930,\"start\":49927},{\"end\":49945,\"start\":49941},{\"end\":50381,\"start\":50375},{\"end\":50394,\"start\":50388},{\"end\":50409,\"start\":50406},{\"end\":50424,\"start\":50420},{\"end\":50818,\"start\":50812},{\"end\":50830,\"start\":50825},{\"end\":50837,\"start\":50831},{\"end\":50851,\"start\":50845},{\"end\":50870,\"start\":50863},{\"end\":50888,\"start\":50881},{\"end\":50902,\"start\":50899},{\"end\":50917,\"start\":50913},{\"end\":51366,\"start\":51353},{\"end\":51387,\"start\":51382},{\"end\":51402,\"start\":51397},{\"end\":51421,\"start\":51415},{\"end\":51437,\"start\":51434},{\"end\":51940,\"start\":51939},{\"end\":51960,\"start\":51959},{\"end\":51973,\"start\":51966},{\"end\":51987,\"start\":51986},{\"end\":52266,\"start\":52261},{\"end\":52278,\"start\":52273},{\"end\":52298,\"start\":52291},{\"end\":52310,\"start\":52307},{\"end\":52777,\"start\":52772},{\"end\":52791,\"start\":52785},{\"end\":53370,\"start\":53364},{\"end\":53383,\"start\":53376},{\"end\":53392,\"start\":53388},{\"end\":53832,\"start\":53826},{\"end\":53845,\"start\":53838},{\"end\":53854,\"start\":53850},{\"end\":54271,\"start\":54265},{\"end\":54286,\"start\":54283},{\"end\":54301,\"start\":54297},{\"end\":54737,\"start\":54733},{\"end\":54752,\"start\":54747},{\"end\":54767,\"start\":54760},{\"end\":54781,\"start\":54775},{\"end\":54799,\"start\":54791},{\"end\":55254,\"start\":55248},{\"end\":55268,\"start\":55265},{\"end\":55278,\"start\":55274},{\"end\":55287,\"start\":55285},{\"end\":55298,\"start\":55293},{\"end\":55310,\"start\":55305},{\"end\":55319,\"start\":55315},{\"end\":55333,\"start\":55325},{\"end\":55344,\"start\":55338},{\"end\":55865,\"start\":55858},{\"end\":55880,\"start\":55873},{\"end\":55895,\"start\":55890},{\"end\":56081,\"start\":56076},{\"end\":56105,\"start\":56101},{\"end\":56124,\"start\":56116},{\"end\":56432,\"start\":56425},{\"end\":56444,\"start\":56437},{\"end\":56851,\"start\":56844},{\"end\":56863,\"start\":56856},{\"end\":56879,\"start\":56871},{\"end\":56889,\"start\":56885},{\"end\":57354,\"start\":57344},{\"end\":57369,\"start\":57360},{\"end\":57384,\"start\":57375},{\"end\":57397,\"start\":57391},{\"end\":57410,\"start\":57403},{\"end\":57778,\"start\":57769},{\"end\":57790,\"start\":57783},{\"end\":57807,\"start\":57798},{\"end\":57809,\"start\":57808},{\"end\":58226,\"start\":58218},{\"end\":58237,\"start\":58234},{\"end\":58249,\"start\":58244},{\"end\":58622,\"start\":58616},{\"end\":58638,\"start\":58630},{\"end\":58650,\"start\":58645},{\"end\":58662,\"start\":58657},{\"end\":58676,\"start\":58668},{\"end\":59158,\"start\":59154},{\"end\":59175,\"start\":59168},{\"end\":59189,\"start\":59186},{\"end\":59206,\"start\":59198},{\"end\":59218,\"start\":59213},{\"end\":59670,\"start\":59666},{\"end\":59687,\"start\":59680},{\"end\":59702,\"start\":59698},{\"end\":59714,\"start\":59708},{\"end\":59726,\"start\":59721},{\"end\":60010,\"start\":60009},{\"end\":60026,\"start\":60021},{\"end\":60281,\"start\":60272},{\"end\":60303,\"start\":60294},{\"end\":60305,\"start\":60304},{\"end\":60662,\"start\":60657},{\"end\":60676,\"start\":60672},{\"end\":60692,\"start\":60686},{\"end\":60708,\"start\":60704},{\"end\":61015,\"start\":61009},{\"end\":61036,\"start\":61026},{\"end\":61050,\"start\":61046},{\"end\":61066,\"start\":61060},{\"end\":61080,\"start\":61073},{\"end\":61552,\"start\":61544},{\"end\":61564,\"start\":61558},{\"end\":61583,\"start\":61575},{\"end\":61599,\"start\":61591},{\"end\":62026,\"start\":62018},{\"end\":62041,\"start\":62032},{\"end\":62055,\"start\":62048},{\"end\":62077,\"start\":62070},{\"end\":62528,\"start\":62526},{\"end\":62536,\"start\":62533},{\"end\":62546,\"start\":62541},{\"end\":62559,\"start\":62553},{\"end\":62575,\"start\":62567},{\"end\":62588,\"start\":62582},{\"end\":63025,\"start\":63019},{\"end\":63034,\"start\":63030},{\"end\":63059,\"start\":63052},{\"end\":63370,\"start\":63362},{\"end\":63386,\"start\":63375},{\"end\":63787,\"start\":63785},{\"end\":63798,\"start\":63792},{\"end\":63809,\"start\":63803},{\"end\":63824,\"start\":63815},{\"end\":63835,\"start\":63829},{\"end\":64293,\"start\":64291},{\"end\":64305,\"start\":64299},{\"end\":64316,\"start\":64311},{\"end\":64329,\"start\":64322},{\"end\":64338,\"start\":64334},{\"end\":64827,\"start\":64819},{\"end\":64840,\"start\":64836},{\"end\":64862,\"start\":64855},{\"end\":65214,\"start\":65201},{\"end\":65229,\"start\":65224},{\"end\":65244,\"start\":65239},{\"end\":65256,\"start\":65251},{\"end\":65274,\"start\":65269},{\"end\":65293,\"start\":65287},{\"end\":65306,\"start\":65303},{\"end\":65729,\"start\":65716},{\"end\":65744,\"start\":65739},{\"end\":65759,\"start\":65754},{\"end\":65775,\"start\":65772},{\"end\":66308,\"start\":66299},{\"end\":66320,\"start\":66316},{\"end\":66687,\"start\":66679},{\"end\":66700,\"start\":66696},{\"end\":66713,\"start\":66707},{\"end\":66730,\"start\":66724},{\"end\":66749,\"start\":66742},{\"end\":67209,\"start\":67199},{\"end\":67229,\"start\":67225},{\"end\":67243,\"start\":67235},{\"end\":67711,\"start\":67705},{\"end\":67735,\"start\":67724},{\"end\":67747,\"start\":67742},{\"end\":67749,\"start\":67748},{\"end\":67762,\"start\":67758},{\"end\":67764,\"start\":67763},{\"end\":68230,\"start\":68222},{\"end\":68244,\"start\":68240},{\"end\":68261,\"start\":68254},{\"end\":68277,\"start\":68272},{\"end\":68296,\"start\":68287},{\"end\":68785,\"start\":68784},{\"end\":68798,\"start\":68793},{\"end\":69273,\"start\":69268},{\"end\":69291,\"start\":69286},{\"end\":69310,\"start\":69302},{\"end\":69312,\"start\":69311},{\"end\":69327,\"start\":69321},{\"end\":69345,\"start\":69337},{\"end\":69695,\"start\":69690},{\"end\":69716,\"start\":69708},{\"end\":69731,\"start\":69726},{\"end\":69746,\"start\":69741},{\"end\":69766,\"start\":69757},{\"end\":69787,\"start\":69784},{\"end\":70148,\"start\":70141},{\"end\":70164,\"start\":70159},{\"end\":70171,\"start\":70165},{\"end\":70185,\"start\":70179},{\"end\":70204,\"start\":70197},{\"end\":70748,\"start\":70742},{\"end\":70763,\"start\":70754},{\"end\":70777,\"start\":70771},{\"end\":70792,\"start\":70788},{\"end\":70809,\"start\":70804},{\"end\":70828,\"start\":70819},{\"end\":71249,\"start\":71242},{\"end\":71266,\"start\":71258},{\"end\":71285,\"start\":71279},{\"end\":71625,\"start\":71618},{\"end\":71637,\"start\":71634},{\"end\":71651,\"start\":71645},{\"end\":71667,\"start\":71659},{\"end\":72153,\"start\":72146},{\"end\":72172,\"start\":72166},{\"end\":72189,\"start\":72181},{\"end\":72198,\"start\":72196},{\"end\":72592,\"start\":72588},{\"end\":72609,\"start\":72606},{\"end\":72619,\"start\":72616},{\"end\":72632,\"start\":72624},{\"end\":72648,\"start\":72641},{\"end\":72663,\"start\":72659},{\"end\":72675,\"start\":72668},{\"end\":72689,\"start\":72683},{\"end\":72706,\"start\":72700},{\"end\":72722,\"start\":72715},{\"end\":72743,\"start\":72734},{\"end\":72745,\"start\":72744},{\"end\":72754,\"start\":72752},{\"end\":73196,\"start\":73190},{\"end\":73212,\"start\":73204},{\"end\":73226,\"start\":73221},{\"end\":73681,\"start\":73677},{\"end\":73696,\"start\":73688},{\"end\":73715,\"start\":73706},{\"end\":73732,\"start\":73726},{\"end\":73736,\"start\":73733},{\"end\":73754,\"start\":73743},{\"end\":73769,\"start\":73765},{\"end\":74244,\"start\":74240},{\"end\":74262,\"start\":74254},{\"end\":74730,\"start\":74726},{\"end\":74751,\"start\":74745},{\"end\":74765,\"start\":74758},{\"end\":74782,\"start\":74775},{\"end\":74796,\"start\":74789},{\"end\":74815,\"start\":74804},{\"end\":75338,\"start\":75334},{\"end\":75360,\"start\":75353},{\"end\":75569,\"start\":75565},{\"end\":75592,\"start\":75584},{\"end\":75602,\"start\":75601},{\"end\":75604,\"start\":75603},{\"end\":75620,\"start\":75613},{\"end\":76103,\"start\":76093},{\"end\":76125,\"start\":76119},{\"end\":76139,\"start\":76131},{\"end\":76614,\"start\":76610},{\"end\":76625,\"start\":76619},{\"end\":76638,\"start\":76632},{\"end\":76653,\"start\":76644},{\"end\":76665,\"start\":76659},{\"end\":76681,\"start\":76673},{\"end\":76694,\"start\":76688},{\"end\":77009,\"start\":77005},{\"end\":77029,\"start\":77024},{\"end\":77259,\"start\":77252},{\"end\":77268,\"start\":77266},{\"end\":77281,\"start\":77274},{\"end\":77291,\"start\":77286},{\"end\":77304,\"start\":77297}]", "bib_author_last_name": "[{\"end\":47636,\"start\":47633},{\"end\":47647,\"start\":47643},{\"end\":48114,\"start\":48107},{\"end\":48132,\"start\":48121},{\"end\":48150,\"start\":48143},{\"end\":48162,\"start\":48155},{\"end\":48505,\"start\":48495},{\"end\":48522,\"start\":48516},{\"end\":48536,\"start\":48531},{\"end\":48552,\"start\":48548},{\"end\":48905,\"start\":48899},{\"end\":48924,\"start\":48919},{\"end\":48944,\"start\":48936},{\"end\":49449,\"start\":49439},{\"end\":49467,\"start\":49458},{\"end\":49479,\"start\":49473},{\"end\":49492,\"start\":49488},{\"end\":49501,\"start\":49494},{\"end\":49907,\"start\":49903},{\"end\":49925,\"start\":49916},{\"end\":49939,\"start\":49931},{\"end\":49953,\"start\":49946},{\"end\":50386,\"start\":50382},{\"end\":50404,\"start\":50395},{\"end\":50418,\"start\":50410},{\"end\":50432,\"start\":50425},{\"end\":50823,\"start\":50819},{\"end\":50843,\"start\":50838},{\"end\":50861,\"start\":50852},{\"end\":50879,\"start\":50871},{\"end\":50897,\"start\":50889},{\"end\":50911,\"start\":50903},{\"end\":50925,\"start\":50918},{\"end\":51380,\"start\":51367},{\"end\":51395,\"start\":51388},{\"end\":51413,\"start\":51403},{\"end\":51432,\"start\":51422},{\"end\":51445,\"start\":51438},{\"end\":51455,\"start\":51447},{\"end\":51957,\"start\":51941},{\"end\":51964,\"start\":51961},{\"end\":51984,\"start\":51974},{\"end\":51995,\"start\":51988},{\"end\":52001,\"start\":51997},{\"end\":52271,\"start\":52267},{\"end\":52289,\"start\":52279},{\"end\":52305,\"start\":52299},{\"end\":52319,\"start\":52311},{\"end\":52783,\"start\":52778},{\"end\":52799,\"start\":52792},{\"end\":53374,\"start\":53371},{\"end\":53386,\"start\":53384},{\"end\":53396,\"start\":53393},{\"end\":53836,\"start\":53833},{\"end\":53848,\"start\":53846},{\"end\":53858,\"start\":53855},{\"end\":54281,\"start\":54272},{\"end\":54295,\"start\":54287},{\"end\":54309,\"start\":54302},{\"end\":54745,\"start\":54738},{\"end\":54758,\"start\":54753},{\"end\":54773,\"start\":54768},{\"end\":54789,\"start\":54782},{\"end\":54806,\"start\":54800},{\"end\":55263,\"start\":55255},{\"end\":55272,\"start\":55269},{\"end\":55283,\"start\":55279},{\"end\":55291,\"start\":55288},{\"end\":55303,\"start\":55299},{\"end\":55313,\"start\":55311},{\"end\":55323,\"start\":55320},{\"end\":55336,\"start\":55334},{\"end\":55349,\"start\":55345},{\"end\":55355,\"start\":55351},{\"end\":55871,\"start\":55866},{\"end\":55888,\"start\":55881},{\"end\":55904,\"start\":55896},{\"end\":56099,\"start\":56082},{\"end\":56114,\"start\":56106},{\"end\":56133,\"start\":56125},{\"end\":56140,\"start\":56135},{\"end\":56435,\"start\":56433},{\"end\":56453,\"start\":56445},{\"end\":56854,\"start\":56852},{\"end\":56869,\"start\":56864},{\"end\":56883,\"start\":56880},{\"end\":56893,\"start\":56890},{\"end\":57358,\"start\":57355},{\"end\":57373,\"start\":57370},{\"end\":57389,\"start\":57385},{\"end\":57401,\"start\":57398},{\"end\":57417,\"start\":57411},{\"end\":57781,\"start\":57779},{\"end\":57796,\"start\":57791},{\"end\":57817,\"start\":57810},{\"end\":58232,\"start\":58227},{\"end\":58242,\"start\":58238},{\"end\":58255,\"start\":58250},{\"end\":58628,\"start\":58623},{\"end\":58643,\"start\":58639},{\"end\":58655,\"start\":58651},{\"end\":58666,\"start\":58663},{\"end\":58681,\"start\":58677},{\"end\":59166,\"start\":59159},{\"end\":59184,\"start\":59176},{\"end\":59196,\"start\":59190},{\"end\":59211,\"start\":59207},{\"end\":59226,\"start\":59219},{\"end\":59678,\"start\":59671},{\"end\":59696,\"start\":59688},{\"end\":59706,\"start\":59703},{\"end\":59719,\"start\":59715},{\"end\":59734,\"start\":59727},{\"end\":60019,\"start\":60011},{\"end\":60033,\"start\":60027},{\"end\":60037,\"start\":60035},{\"end\":60292,\"start\":60282},{\"end\":60313,\"start\":60306},{\"end\":60670,\"start\":60663},{\"end\":60684,\"start\":60677},{\"end\":60702,\"start\":60693},{\"end\":60726,\"start\":60709},{\"end\":60733,\"start\":60728},{\"end\":61024,\"start\":61016},{\"end\":61044,\"start\":61037},{\"end\":61058,\"start\":61051},{\"end\":61071,\"start\":61067},{\"end\":61086,\"start\":61081},{\"end\":61556,\"start\":61553},{\"end\":61573,\"start\":61565},{\"end\":61589,\"start\":61584},{\"end\":61603,\"start\":61600},{\"end\":62030,\"start\":62027},{\"end\":62046,\"start\":62042},{\"end\":62068,\"start\":62056},{\"end\":62084,\"start\":62078},{\"end\":62531,\"start\":62529},{\"end\":62539,\"start\":62537},{\"end\":62551,\"start\":62547},{\"end\":62565,\"start\":62560},{\"end\":62580,\"start\":62576},{\"end\":62592,\"start\":62589},{\"end\":63028,\"start\":63026},{\"end\":63050,\"start\":63035},{\"end\":63066,\"start\":63060},{\"end\":63373,\"start\":63371},{\"end\":63390,\"start\":63387},{\"end\":63790,\"start\":63788},{\"end\":63801,\"start\":63799},{\"end\":63813,\"start\":63810},{\"end\":63827,\"start\":63825},{\"end\":63839,\"start\":63836},{\"end\":64297,\"start\":64294},{\"end\":64309,\"start\":64306},{\"end\":64320,\"start\":64317},{\"end\":64332,\"start\":64330},{\"end\":64342,\"start\":64339},{\"end\":64834,\"start\":64828},{\"end\":64853,\"start\":64841},{\"end\":64868,\"start\":64863},{\"end\":65222,\"start\":65215},{\"end\":65237,\"start\":65230},{\"end\":65249,\"start\":65245},{\"end\":65267,\"start\":65257},{\"end\":65285,\"start\":65275},{\"end\":65301,\"start\":65294},{\"end\":65315,\"start\":65307},{\"end\":65737,\"start\":65730},{\"end\":65752,\"start\":65745},{\"end\":65770,\"start\":65760},{\"end\":65784,\"start\":65776},{\"end\":66314,\"start\":66309},{\"end\":66329,\"start\":66321},{\"end\":66694,\"start\":66688},{\"end\":66705,\"start\":66701},{\"end\":66722,\"start\":66714},{\"end\":66740,\"start\":66731},{\"end\":66756,\"start\":66750},{\"end\":67223,\"start\":67210},{\"end\":67233,\"start\":67230},{\"end\":67246,\"start\":67244},{\"end\":67251,\"start\":67248},{\"end\":67722,\"start\":67712},{\"end\":67740,\"start\":67736},{\"end\":67756,\"start\":67750},{\"end\":67771,\"start\":67765},{\"end\":68238,\"start\":68231},{\"end\":68252,\"start\":68245},{\"end\":68270,\"start\":68262},{\"end\":68285,\"start\":68278},{\"end\":68312,\"start\":68297},{\"end\":68791,\"start\":68786},{\"end\":68807,\"start\":68799},{\"end\":68818,\"start\":68809},{\"end\":69284,\"start\":69274},{\"end\":69300,\"start\":69292},{\"end\":69319,\"start\":69313},{\"end\":69335,\"start\":69328},{\"end\":69351,\"start\":69346},{\"end\":69706,\"start\":69696},{\"end\":69724,\"start\":69717},{\"end\":69739,\"start\":69732},{\"end\":69755,\"start\":69747},{\"end\":69782,\"start\":69767},{\"end\":69796,\"start\":69788},{\"end\":70157,\"start\":70149},{\"end\":70177,\"start\":70172},{\"end\":70195,\"start\":70186},{\"end\":70223,\"start\":70205},{\"end\":70233,\"start\":70225},{\"end\":70752,\"start\":70749},{\"end\":70769,\"start\":70764},{\"end\":70786,\"start\":70778},{\"end\":70802,\"start\":70793},{\"end\":70817,\"start\":70810},{\"end\":70834,\"start\":70829},{\"end\":71256,\"start\":71250},{\"end\":71277,\"start\":71267},{\"end\":71291,\"start\":71286},{\"end\":71632,\"start\":71626},{\"end\":71643,\"start\":71638},{\"end\":71657,\"start\":71652},{\"end\":71678,\"start\":71668},{\"end\":72164,\"start\":72154},{\"end\":72179,\"start\":72173},{\"end\":72194,\"start\":72190},{\"end\":72202,\"start\":72199},{\"end\":72604,\"start\":72593},{\"end\":72614,\"start\":72610},{\"end\":72622,\"start\":72620},{\"end\":72639,\"start\":72633},{\"end\":72657,\"start\":72649},{\"end\":72666,\"start\":72664},{\"end\":72681,\"start\":72676},{\"end\":72698,\"start\":72690},{\"end\":72713,\"start\":72707},{\"end\":72732,\"start\":72723},{\"end\":72750,\"start\":72746},{\"end\":72762,\"start\":72755},{\"end\":73202,\"start\":73197},{\"end\":73219,\"start\":73213},{\"end\":73236,\"start\":73227},{\"end\":73686,\"start\":73682},{\"end\":73704,\"start\":73697},{\"end\":73724,\"start\":73716},{\"end\":73741,\"start\":73737},{\"end\":73763,\"start\":73755},{\"end\":73776,\"start\":73770},{\"end\":74252,\"start\":74245},{\"end\":74273,\"start\":74263},{\"end\":74743,\"start\":74731},{\"end\":74756,\"start\":74752},{\"end\":74773,\"start\":74766},{\"end\":74787,\"start\":74783},{\"end\":74802,\"start\":74797},{\"end\":74820,\"start\":74816},{\"end\":75351,\"start\":75339},{\"end\":75366,\"start\":75361},{\"end\":75582,\"start\":75570},{\"end\":75599,\"start\":75593},{\"end\":75611,\"start\":75605},{\"end\":75625,\"start\":75621},{\"end\":75632,\"start\":75627},{\"end\":76117,\"start\":76104},{\"end\":76129,\"start\":76126},{\"end\":76150,\"start\":76140},{\"end\":76155,\"start\":76152},{\"end\":76617,\"start\":76615},{\"end\":76630,\"start\":76626},{\"end\":76642,\"start\":76639},{\"end\":76657,\"start\":76654},{\"end\":76671,\"start\":76666},{\"end\":76686,\"start\":76682},{\"end\":76700,\"start\":76695},{\"end\":77022,\"start\":77010},{\"end\":77032,\"start\":77030},{\"end\":77036,\"start\":77034},{\"end\":77264,\"start\":77260},{\"end\":77272,\"start\":77269},{\"end\":77284,\"start\":77282},{\"end\":77295,\"start\":77292},{\"end\":77309,\"start\":77305}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":4702470},\"end\":48037,\"start\":47513},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":1356654},\"end\":48416,\"start\":48039},{\"attributes\":{\"doi\":\"arXiv:1812.01397\",\"id\":\"b2\"},\"end\":48764,\"start\":48418},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":4716955},\"end\":49378,\"start\":48766},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":29153681},\"end\":49839,\"start\":49380},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":118637813},\"end\":50299,\"start\":49841},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":214623106},\"end\":50756,\"start\":50301},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":214640989},\"end\":51316,\"start\":50758},{\"attributes\":{\"id\":\"b8\"},\"end\":51854,\"start\":51318},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":3638670},\"end\":52185,\"start\":51856},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":4710115},\"end\":52708,\"start\":52187},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":957504},\"end\":53265,\"start\":52710},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":1613420},\"end\":53754,\"start\":53267},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":8510667},\"end\":54217,\"start\":53756},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":214693026},\"end\":54668,\"start\":54219},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":92935},\"end\":55174,\"start\":54670},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":52350875},\"end\":55795,\"start\":55176},{\"attributes\":{\"id\":\"b17\"},\"end\":56033,\"start\":55797},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":9272368},\"end\":56378,\"start\":56035},{\"attributes\":{\"id\":\"b19\"},\"end\":56749,\"start\":56380},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":13740328},\"end\":57262,\"start\":56751},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":202770256},\"end\":57713,\"start\":57264},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":52154491},\"end\":58133,\"start\":57715},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":53102207},\"end\":58533,\"start\":58135},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":51690586},\"end\":59082,\"start\":58535},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":12124389},\"end\":59623,\"start\":59084},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":10412111},\"end\":59963,\"start\":59625},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b27\"},\"end\":60183,\"start\":59965},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":333277},\"end\":60573,\"start\":60185},{\"attributes\":{\"id\":\"b29\"},\"end\":60902,\"start\":60575},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":221134166},\"end\":61471,\"start\":60904},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":102350632},\"end\":61961,\"start\":61473},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":102351194},\"end\":62451,\"start\":61963},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":57189581},\"end\":62988,\"start\":62453},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":4949133},\"end\":63267,\"start\":62990},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":3844018},\"end\":63725,\"start\":63269},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":14350475},\"end\":64206,\"start\":63727},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":3121011},\"end\":64733,\"start\":64208},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":50773378},\"end\":65143,\"start\":64735},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":9095022},\"end\":65652,\"start\":65145},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":4370690},\"end\":66169,\"start\":65654},{\"attributes\":{\"doi\":\"28/10/2020. 6\",\"id\":\"b41\"},\"end\":66595,\"start\":66171},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":4455970},\"end\":67137,\"start\":66597},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":90262243},\"end\":67602,\"start\":67139},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":8420864},\"end\":68165,\"start\":67604},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":3155322},\"end\":68712,\"start\":68167},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":7656505},\"end\":69177,\"start\":68714},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":2625735},\"end\":69688,\"start\":69179},{\"attributes\":{\"doi\":\"arXiv:1704.00675\",\"id\":\"b48\"},\"end\":70069,\"start\":69690},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":211678059},\"end\":70669,\"start\":70071},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":15508056},\"end\":71170,\"start\":70671},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":6202829},\"end\":71519,\"start\":71172},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":1041733},\"end\":72071,\"start\":71521},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":10592859},\"end\":72586,\"start\":72073},{\"attributes\":{\"id\":\"b54\"},\"end\":73088,\"start\":72588},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":13092376},\"end\":73613,\"start\":73090},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":4796040},\"end\":74155,\"start\":73615},{\"attributes\":{\"id\":\"b57\"},\"end\":74649,\"start\":74157},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":67856723},\"end\":75250,\"start\":74651},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":6413853},\"end\":75518,\"start\":75252},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":208512936},\"end\":76023,\"start\":75520},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":52847087},\"end\":76544,\"start\":76025},{\"attributes\":{\"doi\":\"arXiv:1809.03327\",\"id\":\"b62\"},\"end\":76938,\"start\":76546},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":25001358},\"end\":77183,\"start\":76940},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":4565253},\"end\":77691,\"start\":77185}]", "bib_title": "[{\"end\":47624,\"start\":47513},{\"end\":48101,\"start\":48039},{\"end\":48891,\"start\":48766},{\"end\":49432,\"start\":49380},{\"end\":49894,\"start\":49841},{\"end\":50373,\"start\":50301},{\"end\":50810,\"start\":50758},{\"end\":51351,\"start\":51318},{\"end\":51937,\"start\":51856},{\"end\":52259,\"start\":52187},{\"end\":52770,\"start\":52710},{\"end\":53362,\"start\":53267},{\"end\":53824,\"start\":53756},{\"end\":54263,\"start\":54219},{\"end\":54731,\"start\":54670},{\"end\":55246,\"start\":55176},{\"end\":56074,\"start\":56035},{\"end\":56423,\"start\":56380},{\"end\":56842,\"start\":56751},{\"end\":57342,\"start\":57264},{\"end\":57767,\"start\":57715},{\"end\":58216,\"start\":58135},{\"end\":58614,\"start\":58535},{\"end\":59152,\"start\":59084},{\"end\":59664,\"start\":59625},{\"end\":60270,\"start\":60185},{\"end\":61007,\"start\":60904},{\"end\":61542,\"start\":61473},{\"end\":62016,\"start\":61963},{\"end\":62524,\"start\":62453},{\"end\":63017,\"start\":62990},{\"end\":63360,\"start\":63269},{\"end\":63783,\"start\":63727},{\"end\":64289,\"start\":64208},{\"end\":64817,\"start\":64735},{\"end\":65199,\"start\":65145},{\"end\":65714,\"start\":65654},{\"end\":66677,\"start\":66597},{\"end\":67197,\"start\":67139},{\"end\":67703,\"start\":67604},{\"end\":68220,\"start\":68167},{\"end\":68782,\"start\":68714},{\"end\":69266,\"start\":69179},{\"end\":70139,\"start\":70071},{\"end\":70740,\"start\":70671},{\"end\":71240,\"start\":71172},{\"end\":71616,\"start\":71521},{\"end\":72144,\"start\":72073},{\"end\":73188,\"start\":73090},{\"end\":73675,\"start\":73615},{\"end\":74238,\"start\":74157},{\"end\":74724,\"start\":74651},{\"end\":75332,\"start\":75252},{\"end\":75563,\"start\":75520},{\"end\":76091,\"start\":76025},{\"end\":77003,\"start\":76940},{\"end\":77250,\"start\":77185}]", "bib_author": "[{\"end\":47638,\"start\":47626},{\"end\":47649,\"start\":47638},{\"end\":48116,\"start\":48103},{\"end\":48134,\"start\":48116},{\"end\":48152,\"start\":48134},{\"end\":48164,\"start\":48152},{\"end\":48507,\"start\":48486},{\"end\":48524,\"start\":48507},{\"end\":48538,\"start\":48524},{\"end\":48554,\"start\":48538},{\"end\":48907,\"start\":48893},{\"end\":48926,\"start\":48907},{\"end\":48946,\"start\":48926},{\"end\":49451,\"start\":49434},{\"end\":49469,\"start\":49451},{\"end\":49481,\"start\":49469},{\"end\":49494,\"start\":49481},{\"end\":49503,\"start\":49494},{\"end\":49909,\"start\":49896},{\"end\":49927,\"start\":49909},{\"end\":49941,\"start\":49927},{\"end\":49955,\"start\":49941},{\"end\":50388,\"start\":50375},{\"end\":50406,\"start\":50388},{\"end\":50420,\"start\":50406},{\"end\":50434,\"start\":50420},{\"end\":50825,\"start\":50812},{\"end\":50845,\"start\":50825},{\"end\":50863,\"start\":50845},{\"end\":50881,\"start\":50863},{\"end\":50899,\"start\":50881},{\"end\":50913,\"start\":50899},{\"end\":50927,\"start\":50913},{\"end\":51382,\"start\":51353},{\"end\":51397,\"start\":51382},{\"end\":51415,\"start\":51397},{\"end\":51434,\"start\":51415},{\"end\":51447,\"start\":51434},{\"end\":51457,\"start\":51447},{\"end\":51959,\"start\":51939},{\"end\":51966,\"start\":51959},{\"end\":51986,\"start\":51966},{\"end\":51997,\"start\":51986},{\"end\":52003,\"start\":51997},{\"end\":52273,\"start\":52261},{\"end\":52291,\"start\":52273},{\"end\":52307,\"start\":52291},{\"end\":52321,\"start\":52307},{\"end\":52785,\"start\":52772},{\"end\":52801,\"start\":52785},{\"end\":53376,\"start\":53364},{\"end\":53388,\"start\":53376},{\"end\":53398,\"start\":53388},{\"end\":53838,\"start\":53826},{\"end\":53850,\"start\":53838},{\"end\":53860,\"start\":53850},{\"end\":54283,\"start\":54265},{\"end\":54297,\"start\":54283},{\"end\":54311,\"start\":54297},{\"end\":54747,\"start\":54733},{\"end\":54760,\"start\":54747},{\"end\":54775,\"start\":54760},{\"end\":54791,\"start\":54775},{\"end\":54808,\"start\":54791},{\"end\":55265,\"start\":55248},{\"end\":55274,\"start\":55265},{\"end\":55285,\"start\":55274},{\"end\":55293,\"start\":55285},{\"end\":55305,\"start\":55293},{\"end\":55315,\"start\":55305},{\"end\":55325,\"start\":55315},{\"end\":55338,\"start\":55325},{\"end\":55351,\"start\":55338},{\"end\":55357,\"start\":55351},{\"end\":55873,\"start\":55858},{\"end\":55890,\"start\":55873},{\"end\":55906,\"start\":55890},{\"end\":56101,\"start\":56076},{\"end\":56116,\"start\":56101},{\"end\":56135,\"start\":56116},{\"end\":56142,\"start\":56135},{\"end\":56437,\"start\":56425},{\"end\":56455,\"start\":56437},{\"end\":56856,\"start\":56844},{\"end\":56871,\"start\":56856},{\"end\":56885,\"start\":56871},{\"end\":56895,\"start\":56885},{\"end\":57360,\"start\":57344},{\"end\":57375,\"start\":57360},{\"end\":57391,\"start\":57375},{\"end\":57403,\"start\":57391},{\"end\":57419,\"start\":57403},{\"end\":57783,\"start\":57769},{\"end\":57798,\"start\":57783},{\"end\":57819,\"start\":57798},{\"end\":58234,\"start\":58218},{\"end\":58244,\"start\":58234},{\"end\":58257,\"start\":58244},{\"end\":58630,\"start\":58616},{\"end\":58645,\"start\":58630},{\"end\":58657,\"start\":58645},{\"end\":58668,\"start\":58657},{\"end\":58683,\"start\":58668},{\"end\":59168,\"start\":59154},{\"end\":59186,\"start\":59168},{\"end\":59198,\"start\":59186},{\"end\":59213,\"start\":59198},{\"end\":59228,\"start\":59213},{\"end\":59680,\"start\":59666},{\"end\":59698,\"start\":59680},{\"end\":59708,\"start\":59698},{\"end\":59721,\"start\":59708},{\"end\":59736,\"start\":59721},{\"end\":60021,\"start\":60009},{\"end\":60035,\"start\":60021},{\"end\":60039,\"start\":60035},{\"end\":60294,\"start\":60272},{\"end\":60315,\"start\":60294},{\"end\":60672,\"start\":60657},{\"end\":60686,\"start\":60672},{\"end\":60704,\"start\":60686},{\"end\":60728,\"start\":60704},{\"end\":60735,\"start\":60728},{\"end\":61026,\"start\":61009},{\"end\":61046,\"start\":61026},{\"end\":61060,\"start\":61046},{\"end\":61073,\"start\":61060},{\"end\":61088,\"start\":61073},{\"end\":61558,\"start\":61544},{\"end\":61575,\"start\":61558},{\"end\":61591,\"start\":61575},{\"end\":61605,\"start\":61591},{\"end\":62032,\"start\":62018},{\"end\":62048,\"start\":62032},{\"end\":62070,\"start\":62048},{\"end\":62086,\"start\":62070},{\"end\":62533,\"start\":62526},{\"end\":62541,\"start\":62533},{\"end\":62553,\"start\":62541},{\"end\":62567,\"start\":62553},{\"end\":62582,\"start\":62567},{\"end\":62594,\"start\":62582},{\"end\":63030,\"start\":63019},{\"end\":63052,\"start\":63030},{\"end\":63068,\"start\":63052},{\"end\":63375,\"start\":63362},{\"end\":63392,\"start\":63375},{\"end\":63792,\"start\":63785},{\"end\":63803,\"start\":63792},{\"end\":63815,\"start\":63803},{\"end\":63829,\"start\":63815},{\"end\":63841,\"start\":63829},{\"end\":64299,\"start\":64291},{\"end\":64311,\"start\":64299},{\"end\":64322,\"start\":64311},{\"end\":64334,\"start\":64322},{\"end\":64344,\"start\":64334},{\"end\":64836,\"start\":64819},{\"end\":64855,\"start\":64836},{\"end\":64870,\"start\":64855},{\"end\":65224,\"start\":65201},{\"end\":65239,\"start\":65224},{\"end\":65251,\"start\":65239},{\"end\":65269,\"start\":65251},{\"end\":65287,\"start\":65269},{\"end\":65303,\"start\":65287},{\"end\":65317,\"start\":65303},{\"end\":65739,\"start\":65716},{\"end\":65754,\"start\":65739},{\"end\":65772,\"start\":65754},{\"end\":65786,\"start\":65772},{\"end\":66316,\"start\":66299},{\"end\":66331,\"start\":66316},{\"end\":66696,\"start\":66679},{\"end\":66707,\"start\":66696},{\"end\":66724,\"start\":66707},{\"end\":66742,\"start\":66724},{\"end\":66758,\"start\":66742},{\"end\":67225,\"start\":67199},{\"end\":67235,\"start\":67225},{\"end\":67248,\"start\":67235},{\"end\":67253,\"start\":67248},{\"end\":67724,\"start\":67705},{\"end\":67742,\"start\":67724},{\"end\":67758,\"start\":67742},{\"end\":67773,\"start\":67758},{\"end\":68240,\"start\":68222},{\"end\":68254,\"start\":68240},{\"end\":68272,\"start\":68254},{\"end\":68287,\"start\":68272},{\"end\":68314,\"start\":68287},{\"end\":68793,\"start\":68784},{\"end\":68809,\"start\":68793},{\"end\":68820,\"start\":68809},{\"end\":69286,\"start\":69268},{\"end\":69302,\"start\":69286},{\"end\":69321,\"start\":69302},{\"end\":69337,\"start\":69321},{\"end\":69353,\"start\":69337},{\"end\":69708,\"start\":69690},{\"end\":69726,\"start\":69708},{\"end\":69741,\"start\":69726},{\"end\":69757,\"start\":69741},{\"end\":69784,\"start\":69757},{\"end\":69798,\"start\":69784},{\"end\":70159,\"start\":70141},{\"end\":70179,\"start\":70159},{\"end\":70197,\"start\":70179},{\"end\":70225,\"start\":70197},{\"end\":70235,\"start\":70225},{\"end\":70754,\"start\":70742},{\"end\":70771,\"start\":70754},{\"end\":70788,\"start\":70771},{\"end\":70804,\"start\":70788},{\"end\":70819,\"start\":70804},{\"end\":70836,\"start\":70819},{\"end\":71258,\"start\":71242},{\"end\":71279,\"start\":71258},{\"end\":71293,\"start\":71279},{\"end\":71634,\"start\":71618},{\"end\":71645,\"start\":71634},{\"end\":71659,\"start\":71645},{\"end\":71680,\"start\":71659},{\"end\":72166,\"start\":72146},{\"end\":72181,\"start\":72166},{\"end\":72196,\"start\":72181},{\"end\":72204,\"start\":72196},{\"end\":72606,\"start\":72588},{\"end\":72616,\"start\":72606},{\"end\":72624,\"start\":72616},{\"end\":72641,\"start\":72624},{\"end\":72659,\"start\":72641},{\"end\":72668,\"start\":72659},{\"end\":72683,\"start\":72668},{\"end\":72700,\"start\":72683},{\"end\":72715,\"start\":72700},{\"end\":72734,\"start\":72715},{\"end\":72752,\"start\":72734},{\"end\":72764,\"start\":72752},{\"end\":73204,\"start\":73190},{\"end\":73221,\"start\":73204},{\"end\":73238,\"start\":73221},{\"end\":73688,\"start\":73677},{\"end\":73706,\"start\":73688},{\"end\":73726,\"start\":73706},{\"end\":73743,\"start\":73726},{\"end\":73765,\"start\":73743},{\"end\":73778,\"start\":73765},{\"end\":74254,\"start\":74240},{\"end\":74275,\"start\":74254},{\"end\":74745,\"start\":74726},{\"end\":74758,\"start\":74745},{\"end\":74775,\"start\":74758},{\"end\":74789,\"start\":74775},{\"end\":74804,\"start\":74789},{\"end\":74822,\"start\":74804},{\"end\":75353,\"start\":75334},{\"end\":75368,\"start\":75353},{\"end\":75584,\"start\":75565},{\"end\":75601,\"start\":75584},{\"end\":75613,\"start\":75601},{\"end\":75627,\"start\":75613},{\"end\":75634,\"start\":75627},{\"end\":76119,\"start\":76093},{\"end\":76131,\"start\":76119},{\"end\":76152,\"start\":76131},{\"end\":76157,\"start\":76152},{\"end\":76619,\"start\":76610},{\"end\":76632,\"start\":76619},{\"end\":76644,\"start\":76632},{\"end\":76659,\"start\":76644},{\"end\":76673,\"start\":76659},{\"end\":76688,\"start\":76673},{\"end\":76702,\"start\":76688},{\"end\":77024,\"start\":77005},{\"end\":77034,\"start\":77024},{\"end\":77038,\"start\":77034},{\"end\":77266,\"start\":77252},{\"end\":77274,\"start\":77266},{\"end\":77286,\"start\":77274},{\"end\":77297,\"start\":77286},{\"end\":77311,\"start\":77297}]", "bib_venue": "[{\"end\":47790,\"start\":47728},{\"end\":49087,\"start\":49025},{\"end\":49592,\"start\":49572},{\"end\":50076,\"start\":50024},{\"end\":50547,\"start\":50499},{\"end\":51042,\"start\":50993},{\"end\":51598,\"start\":51536},{\"end\":52462,\"start\":52400},{\"end\":53012,\"start\":52914},{\"end\":53519,\"start\":53467},{\"end\":54001,\"start\":53939},{\"end\":54460,\"start\":54394},{\"end\":55498,\"start\":55436},{\"end\":56576,\"start\":56524},{\"end\":57016,\"start\":56964},{\"end\":57934,\"start\":57885},{\"end\":58824,\"start\":58762},{\"end\":59369,\"start\":59307},{\"end\":61203,\"start\":61154},{\"end\":61726,\"start\":61674},{\"end\":62176,\"start\":62157},{\"end\":62735,\"start\":62673},{\"end\":63507,\"start\":63458},{\"end\":63982,\"start\":63920},{\"end\":64485,\"start\":64423},{\"end\":65927,\"start\":65865},{\"end\":66873,\"start\":66824},{\"end\":67374,\"start\":67322},{\"end\":67894,\"start\":67842},{\"end\":68455,\"start\":68393},{\"end\":68961,\"start\":68899},{\"end\":70384,\"start\":70318},{\"end\":72345,\"start\":72283},{\"end\":73893,\"start\":73844},{\"end\":74416,\"start\":74354},{\"end\":74963,\"start\":74901},{\"end\":75783,\"start\":75717},{\"end\":76298,\"start\":76236},{\"end\":77452,\"start\":77390},{\"end\":47726,\"start\":47649},{\"end\":48202,\"start\":48164},{\"end\":48484,\"start\":48418},{\"end\":49023,\"start\":48946},{\"end\":49570,\"start\":49503},{\"end\":50022,\"start\":49955},{\"end\":50497,\"start\":50434},{\"end\":50991,\"start\":50927},{\"end\":51534,\"start\":51457},{\"end\":52010,\"start\":52003},{\"end\":52398,\"start\":52321},{\"end\":52912,\"start\":52801},{\"end\":53465,\"start\":53398},{\"end\":53937,\"start\":53860},{\"end\":54392,\"start\":54311},{\"end\":54897,\"start\":54808},{\"end\":55434,\"start\":55357},{\"end\":55856,\"start\":55797},{\"end\":56180,\"start\":56142},{\"end\":56522,\"start\":56455},{\"end\":56962,\"start\":56895},{\"end\":57468,\"start\":57419},{\"end\":57883,\"start\":57819},{\"end\":58319,\"start\":58257},{\"end\":58760,\"start\":58683},{\"end\":59305,\"start\":59228},{\"end\":59784,\"start\":59736},{\"end\":60007,\"start\":59965},{\"end\":60353,\"start\":60315},{\"end\":60655,\"start\":60575},{\"end\":61152,\"start\":61088},{\"end\":61672,\"start\":61605},{\"end\":62155,\"start\":62086},{\"end\":62671,\"start\":62594},{\"end\":63103,\"start\":63068},{\"end\":63456,\"start\":63392},{\"end\":63918,\"start\":63841},{\"end\":64421,\"start\":64344},{\"end\":64905,\"start\":64870},{\"end\":65379,\"start\":65317},{\"end\":65863,\"start\":65786},{\"end\":66297,\"start\":66171},{\"end\":66822,\"start\":66758},{\"end\":67320,\"start\":67253},{\"end\":67840,\"start\":67773},{\"end\":68391,\"start\":68314},{\"end\":68897,\"start\":68820},{\"end\":69415,\"start\":69353},{\"end\":69867,\"start\":69814},{\"end\":70316,\"start\":70235},{\"end\":70898,\"start\":70836},{\"end\":71327,\"start\":71293},{\"end\":71765,\"start\":71680},{\"end\":72281,\"start\":72204},{\"end\":72819,\"start\":72764},{\"end\":73331,\"start\":73238},{\"end\":73842,\"start\":73778},{\"end\":74352,\"start\":74275},{\"end\":74899,\"start\":74822},{\"end\":75372,\"start\":75368},{\"end\":75715,\"start\":75634},{\"end\":76234,\"start\":76157},{\"end\":76608,\"start\":76546},{\"end\":77043,\"start\":77038},{\"end\":77388,\"start\":77311}]"}}}, "year": 2023, "month": 12, "day": 17}