{"id": 17674695, "updated": "2023-09-27 23:01:52.886", "metadata": {"title": "Domain Adaptive Neural Networks for Object Recognition", "authors": "[{\"first\":\"Muhammad\",\"last\":\"Ghifary\",\"middle\":[]},{\"first\":\"W.\",\"last\":\"Kleijn\",\"middle\":[\"Bastiaan\"]},{\"first\":\"Mengjie\",\"last\":\"Zhang\",\"middle\":[]}]", "venue": "PRICAI", "journal": "898-904", "publication_date": {"year": 2014, "month": 9, "day": 21}, "abstract": "We propose a simple neural network model to deal with the domain adaptation problem in object recognition. Our model incorporates the Maximum Mean Discrepancy (MMD) measure as a regularization in the supervised learning to reduce the distribution mismatch between the source and target domains in the latent space. From experiments, we demonstrate that the MMD regularization is an effective tool to provide good domain adaptation models on both SURF features and raw image pixels of a particular image data set. We also show that our proposed model, preceded by the denoising auto-encoder pretraining, achieves better performance than recent benchmark models on the same data sets. This work represents the first study of MMD measure in the context of neural networks.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1409.6041", "mag": "2963168418", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/pricai/GhifaryKZ14", "doi": "10.1007/978-3-319-13560-1_76"}}, "content": {"source": {"pdf_hash": "0b91dc665e2afcbb20113e4bbe89259ea6b6a4e2", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1409.6041v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1409.6041.pdf", "status": "GREEN"}}, "grobid": {"id": "b0488f8bb8976571029d97aa5061cbf1be8ed9d2", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/0b91dc665e2afcbb20113e4bbe89259ea6b6a4e2.txt", "contents": "\nDomain Adaptive Neural Networks for Object Recognition\n\n\nMuhammad Ghifary muhammad.ghifary@ecs.vuw.ac.nz \nSchool of Engineering and Computer Science\nVictoria University of Wellington\nPO Box 600WellingtonNew Zealand\n\nW Bastiaan Kleijn \nSchool of Engineering and Computer Science\nVictoria University of Wellington\nPO Box 600WellingtonNew Zealand\n\nMengjie Zhang mengjie.zhang@ecs.vuw.ac.nz \nSchool of Engineering and Computer Science\nVictoria University of Wellington\nPO Box 600WellingtonNew Zealand\n\nDomain Adaptive Neural Networks for Object Recognition\nDomain AdaptationNeural NetworksRepresentation Learn- ingTransfer LearningMaximum Mean Discrepancy\nWe propose a simple neural network model to deal with the domain adaptation problem in object recognition. Our model incorporates the Maximum Mean Discrepancy (MMD) measure as a regularization in the supervised learning to reduce the distribution mismatch between the source and target domains in the latent space. From experiments, we demonstrate that the MMD regularization is an effective tool to provide good domain adaptation models on both SURF features and raw image pixels of a particular image data set. We also show that our proposed model, preceded by the denoising auto-encoder pretraining, achieves better performance than recent benchmark models on the same data sets. This work represents the first study of MMD measure in the context of neural networks.\n\nIntroduction\n\nIn learning-based computer vision, the probability distribution mismatch between the training and test samples is an essential problem to overcome for the success in real world scenarios. For example, suppose we have an object recognizer learned from a training set containing objects with specific viewpoints, backgrounds, and transformations. It is then applied to an environment with a similar object category, but different viewpoints, backgrounds, and transformations condition. This situation might happen due to a lack of labeled data representing the target environment or insufficient knowledge regarding to the target condition. A good recognition model on this setting can not be guaranteed if it is trained by using traditional learning techniques.\n\nMethods to address the distribution mismatch have been investigated under the names of domain adaptation 1 and transfer learning. More specifically, given a training set {x  t } j=1,...,nt sampled from 1 In this area, the term \"domain\" and \"probability distribution\" are considered to be identical.\n\na distribution D s and D t respectively, the goal is to predict the target labels y (j) t when D s = D t and the information about y (j) t is not sufficient. In recent years, many solutions to this problem have been proposed for computer vision applications (Gong et al., 2012, Gopalan et al., 2011, Long et al., 2013, Saenko et al., 2010 and natural language processing (Daum\u00e9-III, 2009, Pan andYang, 2010).\n\nIn image recognition, the Office data set (Saenko et al., 2010) has become a standard image set to evaluate the performance of domain adaptation models. The standard evaluation protocol on this data set is based on using the SURF feature descriptor (Bay et al., 2008) as inputs to the model. However, the utilization of such a descriptor usually needs a careful engineering to get good discriminative features. Furthermore, it may bring more complexity in the context of real time feature extraction processes. It is therefore worthwhile to build good models without using any handcrafted feature descriptors.\n\nRepresentation or feature learning provides a framework to reduce the dependency on manual feature engineering (Bengio et al., 2012). Examples that can be considered as representation learning are Principal Component Analysis (PCA), Independent Component Analysis (ICA), Sparse Coding, Neural Networks, and Deep Learning. In deep learning, the greedy layer-wise unsupervised training, which is known as the pretraining, has played an important role for the success of deep neural networks (Bengio et al., 2007, Erhan et al., 2010. Although representation learning-based techniques have brought some successes over many applications, methods to address the distribution mismatch have not yet been well studied.\n\nIn this work, we propose a simple neural network model with good domain adaptation performance on raw image pixels. More particularly, we utilize a non-parametric probability distribution distance measure, i.e, the Maximum Mean Discrepancy (MMD), as a regularization embedded in the supervised backpropagation training. MMD is used to reduce the distribution mismatch between two hidden layer representations induced by samples drawn from different domains. Despite its effectiveness, to our best knowledge, the use of MMD in the context of neural networks has not been investigated yet. This work is therefore the first study to use MMD in neural networks. Specifically, we will investigate whether the MMD regularization can indeed improve the discriminative domain adaptation performance of neural networks.\n\n\nPreliminaries\n\nIn this section, we will describe several tools related to our proposed method such as MMD measure, feed forward neural network, and denoising auto-encoder. Some reviews about such tools in recent literature will be also included.\n\n\nMaximum Mean Discrepancy\n\nThe Maximum Mean Discrepancy (MMD) is a measure of the difference between two probability distributions from their samples. It is an effective criterion that compares distributions without initially estimating their density functions. Given two probability distributions p and q on X , MMD is defined as\nMMD(F, p, q) = sup f \u2208F (E x\u223cp [f (x)] \u2212 E y\u223cq [f (y)]),(1)\nwhere F is a class of functions f : X \u2192 R. By defining F as the set of functions of the unit ball in a universal Reproducing Kernel Hilbert Space (RKHS), denoted by H, it was shown that MMD(F, p, q) = 0 will detect any discrepancy between p and q (Borgwardt et al., 2006).\nLet {x (i) s } i=1,...,ns and {x (j) t } j=1,.\n..,nt be data vectors drawn from distributions D s and D t on the data space X , respectively. Based on the fact that f is in the unit ball in a universal RKHS, one may rewrite the empirical estimate of MMD as\nMMD e (x s , x t ) = 1 n s ns i=1 \u03c6(x (i) s ) \u2212 1 n t nt j=1 \u03c6(x (j) t ) H ,(2)\nwhere \u03c6(\u00b7) : X \u2192 H is referred to as the feature space map. By casting (2) into a vector-matrix multiplication form, we come up with a kernelized equation of the form (Borgwardt et al., 2006)\nMMD e (x s , x t ) = 1 n 2 s ns i=1 ns j=1 k(x (i) s , x (j) s ) + 1 n 2 t nt i=1 nt j=1 k(x (i) t , x (j) t ) \u2212 2 n s n t ns i=1 ns j=1 k(x (i) s , x (j) t ) 1 2 (3) = Tr (K xss ) n 2 s + Tr (K xtt ) n 2 t \u2212 2 Tr (K xst ) n s n t 1 2 ,(4)where [K x\u2022\u2022 ] ij = k(x (i) \u2022 , x (j)\n\u2022 ) is the gram-matrix of all possible kernels in the data space.\n\nIn domain adaptation or transfer learning, MMD has been used to reduce the distribution mismatch between the source and target domain. Pan et al. (2009) proposed a PCA-based model referred to as Transfer Component Analysis (TCA) that used MMD to induce a subspace where the data distributions in different domains are closed to each other. Long et al. (2013) presented a Transfer Sparse Coding (TSC) that utilizes MMD in the encoding stage to match the distributions of the sparse codes.\n\nOur work here adopts an idea of incorporating MMD into the learning algorithm similarly to TCA and TSC. The difference is that we carry out the MMD regularization with respect to the supervised criterion while both TCA and TSC are unsupervised learning. We expect that the MMD regularization embedded in the supervised training will induce better discriminative features.\n\n\nFeed Forward Neural Networks\n\nThe Feed Forward Neural Network (FFNN) has been used extensively for solving many discrimative tasks during the past decades, including object recognition tasks. The standard FFNN structure consists of three types of layer that are the input, hidden, and output layers with weighted inter-layer connections. The FFNN training corresponds to adjusting the connection weights with respect to a specific criterion.\n\nLet us consider a single hidden layer neural network with x \u2208 R nx , h \u2208 R n h , and o \u2208 R no as the visible, hidden, and output layers, respectively. We denote W 1 \u2208 R nx\u00d7n h and W 2 \u2208 R n h \u00d7no as the connection weights between the adjacent layers. The FFNN can be written in the form of\nh = \u03c3 1 (W 1 x + b), (5) o = \u03c3 2 (W 2 h + c),(6)\nwhere b \u2208 R n h and c \u2208 R no are the hidden and output units' biases, respectively. Note that both \u03c3 1 : R n h \u2192 R n h and \u03c3 2 : R no \u2192 R no are the non-linear activation functions. In this work, we use the rectifier function approximated by the softplus function \u03c3 1 (u) j = log(1 + exp(u j )) and the softmax function\n\u03c3 2 (v) l = exp(v l ) k exp(v k ) , where u \u2208 R n h and v \u2208 R no .\nThe rectifier function \u03c3 1 (\u00b7) has been argued to be more biologically plausible than the logistic function (Glorot et al., 2011). More importantly, several experimental works proved that the rectifier activation function can improve the performance of neural network models (Nair and Hinton, 2010). Furthermore, the use of the softmax function induces a probabilistic interpretation of the FFNN output.\n\nGiven the n labeled training data {x i) , y (i) } i=1,...,n , where y \u2208 {0, 1} no represents the label with one active output node per class, the objective function of FFNN in the form of the empirical log-likelihood loss function is given as\nJ NN = \u2212 1 n n i=1 l k=1 y (i) k log [g(x (i) )] k(7)\nwhich is typically minimized by the back-propagation algorithm.\n\n\nDenoising Auto-encoder\n\nAn auto-encoder refers to an unsupervised neural network used for learning efficient codings. In deep learning research, it is known as an effective technique for pretraining deep neural networks (Bengio et al., 2007). In terms of the structure, the auto-encoder is very similar to the standard feed-forward neural network except that its output layer has an equal number of nodes as the input layer. The objective of the auto-encoder is to reconstruct its own inputs by means of a reconstruction loss function. A denoising auto-encoder (DAE) is a variant of the auto-encoder model that captures robust representations by reconstructing clean inputs given their noisy counterparts . Qualitatively, the use of several types of noise such as zero masking, Gaussian, and salt-and-pepper noises characterizes particular \"filters\" that correspond to the first hidden layer parameters . DAEs have been considered better than standard auto-encoders and comparable to restricted Boltzmann machines in the context of deep learning discriminative performance (Erhan et al., 2010.\n\nIn this work, we consider DAE as the pretraining stage of our proposed domain adaptive model. Unlabeled images from both source and target domains are considered as inputs to the DAE pretraining. We will investigate the effect with and without the DAE pretraining regarding to the domain adaptation performance.\n\n\nDomain Adaptive Neural Networks\n\nWe propose a variant of the standard feed forward neural network that we refer to as the Domain Adaptive Neural Network (DaNN). This model incorporates MMD measure (2) as a regularization embedded in the supervised backpropagation training. By using such a regularization, we aim to train the network parameters such that the supervised criterion is optimized and the hidden layer representations are encouraged to be invariant across different domains.\n\nGiven the labeled source data {x\n(i) s , y (i) s } i=1,.\n..,ns and the unlabeled target data {x (j) t } j=1,...,nt , the loss function of a single layer DaNN is given by\nJ DaNN = J NNs + \u03b3MMD 2 e (q s ,q t ),(8)\nwhere\nJ NNs = \u2212 1 ns ns i=1 l k=1 ([y (i) s ] k log([f (x (i) s )] k ))\nis the same loss function as shown in (7) but applied only over the source data, q s = W 1 x s + b,q t = W 1 x t + b are the linear combination outputs before the activation, and \u03b3 is the regularization constant controlling the importance of MMD contribution to the loss function.\n\nTo minimize (8), we need the gradient of J DaNN . While computing the gradient of J NNs over {W 1 ,b, c} is trivial, computing the gradient of MMD 2 e (q s ,q t ) depends on the choice of the kernel function. We choose the Gaussian kernel, which is considered as a universal kernel (Steinwart, 2002), as the kernel function\nof the form k G (x, y) = exp \u2212 x\u2212y 2 2s 2\n, where s is the standard deviation.\n\nWe can rewrite the MMD 2 e (\u00b7, \u00b7) function (8) in terms of the Gaussian kernel by a matrix-vector form. Let us denote the sample vectorsx (i) \u2208 R (d+1)\u00d7k and U2 = c W2\ns = 1 x (i) s \u2208 R (d+1) , \u2200i = 1, ..., ns andx (j) t = 1 x (j) t \u2208 R (d+1) , \u2200j = 1, ...,\n\u2208 R (k+1)\u00d7l . Hence, the MMD 2 e (\u00b7, \u00b7) function can be rewritten as\nMMD 2 e (U 1 Xs, U 1 Xt) = 1 n 2 s ns i,j=1 exp \u2212 (x (i) s \u2212 x (j) s ) U1U 1 (x (i) s \u2212 x (i) s ) 2s 2 + 1 n 2 t n t i,j=1 exp \u2212 (x (i) t \u2212 x (j) t ) U1U 1 (x (i) t \u2212 x (i) t ) 2s 2 \u2212 2 nsnt ns,n t i,j=1 exp \u2212 (x (i) s \u2212 x (j) t ) U1U 1 (x (i) s \u2212 x (i) t ) 2s 2 .(9) Let G\u2022\u2022(i, j) be the gradient of kG(U 1 x (i) \u2022 , U 1 x (j) \u2022 ), where the symbol \u2022 can be either s or t, with respect to U 1 . Then, G \u2022\u2022 (i, j) takes the form G\u2022\u2022(i, j) = \u2212 1 s 2 kG(x (i) \u2022 , x (j) \u2022 )(x (i) \u2022 \u2212 x (j) \u2022 )(x (i) \u2022 \u2212 x (j) \u2022 ) U1.(10)\nNow it is straightforward to see that the gradient of MMD 2 e (U 1 Xs,\nU 1 Xt) w.r.t U 1 ( \u2202M 2 st \u2202U1 for short) is given by \u2202M 2 st \u2202U1 = 1 n 2 s ns i,j=1 Gss(i, j) + 1 n 2 t n t i,j=1 Gtt(i, j) \u2212 ns,n t i,j=1 2 nsnt Gst(i, j).(11)\nThe main reason for choosing the Gaussian kernel is that it has been well studied and proven to make MMD useful in practice (Gretton et al., 2012). Furthermore, it is worth noting that MMD here is applied to linear combination outputs before we put on the non-linear activation function. This means that MMD provides a biased estimate with respect to an actual distribution discrepancy of the hidden representations. However, since we use the rectifier activation function that is close to linear, we expect that the measure in (9) would be able to produce good approximation of the true distribution discrepancy.\n\nIn the implementation, we separate the minimization of J NNs and MMD 2 e (\u00b7, \u00b7) into two steps. Firstly, J NNs is minimized using a mini-batched stochastic gradient descent with respect to U 1 update. The mini-batched setting has become a standard practice in neural network training to establish a compromise between speed and accuracy. Then, MMD 2 e (\u00b7, \u00b7) is minimized by re-updating U 1 with respect to the gradient (11). The latter step is accomplished by a full-batched gradient descent. The detail of this procedure are summarized in Algorithm 1.\n\nAlgorithm 1: The DaNN supervised back-propagation algorithm.\n\nData: U 1 \u2208 R (d+1)\u00d7k and U 2 \u2208 R (k+1)\u00d7l are the weight-bias matrices in the first and second layers, respectively. h \u2208 R k is the hidden layer vector. o \u2208 R l is the output layer vector. \u03b1, \u03b3 are the learning rate and the MMD regularization constant. begin 1. Initialize U 1 and U 2 with small random real values; 2. Update U 2 and U 1 using the batched stochastic gradient descent by the standard forward -backward pass w.r.t. J NNs ; 3. Update U 1 by the offline gradient descent as follows\nU 1 (t) := U 1 (t \u2212 1) \u2212 \u03b1\u03b3 \u2202M 2 st \u2202U 1\n4. Repeat Steps 2 and 3 until the end of the epoch;\n\n\nend 4 Experiments and Analysis\n\nWe evaluated our proposed method in the context of object recognition over several domain mismatches. We first compared the DaNN to baselines and other recent domain adaptation methods. The results in terms of the recognition accuracy represented by the mean and standard deviation over 30 independent runs are then reported. At last, we investigated the effect of the MMD regularization by measuring the difference of the first hidden layer activations between one domain to another domain.\n\n\nSetup\n\nOur experiments used the Office data set (Saenko et al., 2010) that contains images of 31 object classes from three different domains: amazon, webcam, and dslr. In amazon, the images contain a single centered object, while for the others the images were acquired in unconstrained settings with some variations such as lighting and background changes. Here we only used 10 object classes following the protocol designed by Gong et al. (2012), which ends up with 1410 instances in total. The number of images for amazon, webcam, and dslr, respectively, are 958, 295, and 157. Webcam and dslr are known to be more similar to each other based on the Rank of Domain (ROD) measure (Gong et al., 2012). Examples of the Office images can be seen in Figure 1. The DaNN model used in the experiments has only one hidden layer, i.e., a shallow network of 256 hidden nodes. 2 The input layer of the DaNN can be either raw pixels or SURF features. The output layer contains ten nodes corresponding to the ten classes.\n\nIn all our experiments, we used the parameter setting for the supervised backpropagation learning specified in Table 1. Note that we employed the dropout regularization introduced by Hinton et al. (2012), the regularization of which randomly omits a hidden node for each training case with a certain probability.\n\nIt has been proven to produce better performance in the sense of reducing the overfitting if a neural network is trained from a small training set.  (Baktashmotlagh et al., 2013), where M SD is the median squared distance between all source samples. The MMD regularization constant \u03b3 was set to be sufficiently large (\u03b3 = 10 3 ) to accommodate small values of (11) compared to JNNs U1 for each iteration. We conducted six domain shift settings, each of which is a domain pair, based on three domains originated from the Office data set (A \u2192 W , W \u2192 A, A \u2192 D, D \u2192 A, W \u2192 D, and D \u2192 W ). The evaluation was divided into two settings: 1) unsupervised adaptation, and 2) semi-supervised adaptation. The unsupervised adaptation corresponds to the setting when we can use both labeled images from the source domain and unlabeled images from the target domain during the training, but no labels from the target domain are incorporated. In the semisupervised adaptation, we incorporate a few labeled images from the target domain as additional training images. First three images per object category from the target domain are selected. Differently from what was conducted in the initial work (Saenko et al., 2010), we used all labeled images from the source domain instead of randomly sampled from it.\n\nThe performance of our model was then compared to SVM-based baselines, two existing domain adaptation methods, and a simple neural network as follows: L-SVM: an SVM (Cortes and Vapnik, 1995) model with a linear kernel that was applied to the original features. 3 L-SVM + PCA: the same model as the L-SVM but preceded by PCA to reduce feature dimensionality. GFK (Gong et al., 2012): the Geodesic Flow Kernel approach by considering an infinite number of intermediate subspaces between the source and target domains followed by k-NN classification. 4 TSC (Long et al., 2013): the Transfer Sparse Coding technique based on the combination of the graph regularized sparse coding, the MMD regularization, and the logistic regression. 5 NN: a single layer neural network with the same structure and parameter setting (Table 1) used in our DaNN, but without the MMD regularization. 6\n\n\nResults on SURF Features\n\nWe first investigated the performance of our model on the standard image features provided by Gong et al. (2012). Briefly, the image features were acquired by first utilizing the SURF descriptor on resized and grayscaled images to detect local scale-invariant interest points. It was then followed by encoding the data points into 800-bin histograms using a codebook trained from a subset of amazon images (Saenko et al., 2010). The final features were then normalized and z-scored to have zero mean and unit variance. We conducted the unsupervised setting evaluation with the results shown in Table 2.\n\nWe found that DaNN and TSC have better performance than the other approaches on these standard features. More specifically, DaNN performs well when there is the amazon set in a particular domain pairs. In the case of webcam-dslr shifts, the TSC, which has not been tested on the Office dataset in the previous work, is surprisingly the best model. Despite its effectiveness, TSC has longer feature extraction time than, for example, neural network-based approaches so that it is less efficient in real world situation. We also noted that the GFK, which incorporates multiple intermediate subspaces, fails to surpass the baselines in several cases. This indicates that the projection onto the subspaces generated by GFK is insufficient to reduce the domain mismatch. Table 2. The unsupervised setting performances on the Office data set (A : amazon, W : webcam, D : dslr ) for each domain pair using SURF-based features as inputs. Each column header starting from the second column indicates one domain pair, e.g., A \u2192 W represents the amazon and webcam as the training and test sets. \nMethods A \u2192 W W \u2192 A A \u2192 D D \u2192 A W \u2192 D D \u2192 W L-\n\nResults on Raw Pixels\n\nWe also conducted the evaluation against the raw pixels of the Office images. Previous works on the Office image set were mostly done using the SURF-based features. It is worth investigating the performance on the Office raw pixels directly since good models on raw pixels are preferable in the sense of reducing the needs of handcrafted feature extractors. We first converted the pixels of the Office images in 2D RGB values into grayscaled pixels and resized them into a dimension of 28 \u00d7 28. They were then z-scored to have zero mean and unit variance.\n\n\nDomain Adaptation Setting\n\nIn this experiment, we ran both the unsupervised and semi-supervised adaptation setting for all domain pairs. In addition, we also investigated the effect of DAE pretraining that precedes the NN and DaNN supervised training with respect to the performance. The DAE pretraining will slightly change Step 1 of Algorithm 1. We denoted these models as DAE + NN and DAE + DaNN. Examples of the pretrained weights are depicted in Figure 2. The complete accuracy rates on the Office raw pixels for all domain pairs are presented in Table 3.\n(a) amazon-webcam (b) amazon-dslr (c) webcam-dslr Fig. 2.\nThe 2D visualization of 100 randomly chosen weights after the DAE pretraining for each domain pairs from the Office image set. The white, gray, and black pixels in each box indicate the high-positive, close-to-zero or zero, and high-negative values of a particular connection weight. The zero-masking noise is used with 30% destruction. Table 3. The performances on the Office dataset (A : amazon, W : webcam, D : dslr ) using the raw pixels as inputs. It is clear that our DaNN always provides accuracy improvements in all domain pairs compared to the SVM-based baselines and the NN model. In other words, the MMD regularization indeed improves the performance of neural networks. Compared to TSC that also employs the MMD regularization in the unsupervised training stage, our DaNN performs better in most cases. However, TSC can match the DaNN performance on webcam-dslr couples, which has lower level mismatch than the other couples. This indicates that the utilization of the MMD regularization in the supervised training might gain more adaptation ability than that in the unsupervised training for pairs with more difficult mismatches to solve.\nMethods A \u2192 W W \u2192 A A \u2192 D D \u2192 A W \u2192 D D \u2192 W Unsupervised\nThe DAE pretraining applied to NN and DaNN indeed improves the performances for all couples of domains. The improvements are quite significant for several cases, especially for webcam-dslr couples. In general, the DAE pretraining also produces more stable models in the sense of resulting in lower standard deviations over 30 independent runs. Furthermore, the combination of DAE pretraining and DaNN performs best among other methods in these experiments in almost all cases. In the sense of qualitative analysis, as can be seen in Figure 2, the DAE pretraining captures more distinctive \"filters\" from local blob detectors to object parts detectors, especially when the amazon images are included. This effect is somewhat consistent with what was found in the initial DAE work  suggesting that the DAE pretraining provides more useful neural network representations.\n\nIn the semi-supervised setting, the performance trend is somewhat similar to the unsupervised setting. However, the performance discrepancies between NN and DaNN here becomes smaller than those in the unsupervised setting. This outcome also holds for the case of the DAE pretraining. This suggests that both the MMD regularization and DAE pretraining might be less impactful when some labeled images from the target domain can be acquired.\n\n\nIn-domain Setting\n\nOne may ask whether the domain adaptation results shown in Table 3 are reasonable compared to the standard learning setting. We refer this standard setting to as the in-domain setting, where the training and test samples come from the same domain. The in-domain performance can be considered as a reference that indicates the effectiveness of domain adaptation models in dealing with the domain mismatch.\n\nWe investigated the in-domain performances of non-domain adaptive models described in Section 4.1, i.e., L-SVM, PCA+L-SVM, and NN on raw pixels of the Office images. For each domain, we conducted 10-fold cross-validation. The complete in-domain results in terms of the mean and standard deviation are shown in Table 4. In general, we can see that the best in-domain model is the NN model on both training and test images.\n\nIn comparison to the domain adaptation results, the highest in-domain accuracies are better than the results with domain mismatches when the amazon or webcam are used as the target sets (see the highest accuracy rates in column  Training  Test  Training  Test  Training  Test  L-SVM 99.0 \u00b1 0.3 52.0 \u00b1 4.6 100.0 \u00b1 0.0 57.7 \u00b1 13.9 100.0 \u00b1 0.0 51.0 \u00b1 14.1 PCA+L-SVM 64.4 \u00b1 0.8 60.6 \u00b1 6.4 72.0 \u00b1 1.5 62.8 \u00b1 8.7 75.6 \u00b1 2.1 55.2 \u00b1 13.1 NN 99.3 \u00b1 0.1 74.2 \u00b1 3.2 100.0 \u00b1 0.0 87.2 \u00b1 5.4 100.0 \u00b1 0.0 77.9 \u00b1 8.8\n\nD \u2192 A and D \u2192 W on Table 3). This indicates that a better domain adaptation model might be necessary to overcome those mismatches. However, this is not the case for the dslr as the target set where the in-domain accuracy is even lower than the best domain adaptation result on W \u2192 D pair. Knowing the facts that the webcam and dslr images are quite similar and the webcam set has more images, this shows that the domain adaptation indeed helps to produce a better object recognition model for this kind of setting.\n\n\nConclusions and Future Work\n\nThis paper aimed to reduce the domain mismatch problem in object recognition using a simple neural network model, which we refer to as the Domain Adaptive Neural Network (DaNN). In this work, we utilized the MMD measure as a regularization in the supervised back-propagation training. This regularization encouraged the hidden layer representation distributions to be similar to each other. We demonstrated that the DaNN performs well on the Office image set, especially on raw image pixels as inputs. Furthermore, the DaNN preceded by the denoising auto-encoder (DAE) pretraining has better performance compared to SVM-based baselines, GFK (Gong et al., 2012), and TSC (Long et al., 2013) on the Office image set (Saenko et al., 2010) in almost all domain pairs. Despite the effectiveness of the MMD regularization, there are still many aspects that can be further improved. We have seen that the performance on raw pixels, which is a main concern in representation learning approach, is still not as good as that on SURF features. We note that good models that perform well without any preceding handcrafted feature extractors are preferable to reduce complexity. A better model on raw pixels might be achieved by using deeper neural network layers with a similar strategy since deep architectures have brought some successes in many applications in recent years (Bengio, 2013). Our initial work using a standard deep neural network with the DAE pretraining, which is not shown here due to page limit, suggested that deeper representations do not always improve the performance against the domain mismatch.\n\nIn addition, a study on the kernel choice for computing MMD regarding to the domain adaptation problem might be worth addressing. We assumed that the universal Gaussian kernel function can detect any underlying distribution mismatches in the Office data set, which might be not true. A better understanding about the relationship between a kernel function and a particular image mismatch, e.g., background, lighting, affine transformation changes, would induce a great impact in this field of research.\n\n\ni=1,...,ns and test set {x\n\n\nnt. The additional element of 1 in each sample is utilized to incorporate the computation with the biases. Let us define the parameter matrices U1 = b W1\n\nFig. 1 .\n1The Office data set(Saenko et al., 2010) samples from amazon and dslr domains.\n\n\net al. (2012) 39.0 \u00b1 0.0 29.8 \u00b1 0.0 36.3 \u00b1 0.0 31.8 \u00b1 0.0 80.3 \u00b1 0.0 75.6 \u00b1 0.0 TSC Long et al. (2013) 47.4 \u00b1 1.7 39.1 \u00b1 0.4 46.2 \u00b1 1.4 41.6 \u00b1 0.8 93.6 \u00b1 0.5 93.5 \u00b1 0.6 NN 44.4 \u00b1 0.6 37.3 \u00b1 0.1 47.8 \u00b1 0.9 34.8 \u00b1 0.2 81.5 \u00b1 0.0 78.9 \u00b1 0.0 DaNN 45.4 \u00b1 0.8 38.7 \u00b1 0.2 49.0 \u00b1 0.7 38.1 \u00b1 0.3 83.4 \u00b1 0.0 81.0 \u00b1 0.0\n\n\nSetting L-SVM 14.9 \u00b1 0.0 14.7 \u00b1 0.0 19.1 \u00b1 0.0 13.7 \u00b1 0.0 36.0 \u00b1 0.0 40.3 \u00b1 0.0 PCA + L-SVM 20.3 \u00b1 0.0 18.1 \u00b1 0.0 16.9 \u00b1 0.0 17.4 \u00b1 0.0 40.4 \u00b1 0.0 37.0 \u00b1 0.0 GFK (Gong et al., 2012) 21.4 \u00b1 0.0 15.0 \u00b1 0.0 30.2 \u00b1 0.0 13.8 \u00b1 0.0 69.1 \u00b1 0.0 65.0 \u00b1 0.0 TSC (Long et al., 2013) 22.3 \u00b1 1.0 15.7 \u00b1 1.1 25.6 \u00b1 1.6 19.6 \u00b1 0.7 74.1 \u00b1 1.9 67.5 \u00b1 1.5 NN 29.2 \u00b1 0.6 17.0 \u00b1 0.3 32.5 \u00b1 0.7 15.0 \u00b1 0.3 63.7 \u00b1 0.0 57.3 \u00b1 0.0 DAE + NN 32.5 \u00b1 0.2 18.7 \u00b1 0.0 37.8 \u00b1 0.2 17.4 \u00b1 0.0 72.1 \u00b1 0.0 65.9 \u00b1 0.0 DaNN 34.1 \u00b1 0.3 21.2 \u00b1 0.2 34.0 \u00b1 0.8 20.1 \u00b1 0.5 64.4 \u00b1 0.0 62.0 \u00b1 0.0 DAE + DaNN 35.0 \u00b1 0.2 23.1 \u00b1 0.0 39.4 \u00b1 0.3 22.5 \u00b1 0.0 74.3 \u00b1 0.0 70.5 \u00b1 0.0 Semi-supervised Setting L-SVM 18.9 \u00b1 0.0 29.0 \u00b1 0.0 25.2 \u00b1 0.0 35.2 \u00b1 0.0 45.7 \u00b1 0.0 52.5 \u00b1 0.0 PCA + L-SVM 20.8 \u00b1 0.0 31.0 \u00b1 0.0 25.6 \u00b1 0.0 35.1 \u00b1 0.0 50.4 \u00b1 0.0 50.2 \u00b1 0.0 GFK (Gong et al., 2012) 47.9 \u00b1 0.0 33.1 \u00b1 0.0 52.0 \u00b1 0.0 31.8 \u00b1 0.0 80.3 \u00b1 0.0 74.7 \u00b1 0.0 TSC (Long et al., 2013) 42.4 \u00b1 2.1 34.1 \u00b1 0.8 49.3 \u00b1 2.2 36.4 \u00b1 0.9 76.3 \u00b1 1.4 71.1 \u00b1 1.1 NN 48.7 \u00b1 0.3 34.5 \u00b1 0.3 52.8 \u00b1 0.6 36.2 \u00b1 0.4 75.6 \u00b1 0.1 67.2 \u00b1 0.0 DAE + NN 52.8 \u00b1 0.1 36.8 \u00b1 0.0 57.5 \u00b1 0.1 36.5 \u00b1 0.0 83.5 \u00b1 0.0 69.4 \u00b1 0.0 DaNN 51.3 \u00b1 0.5 36.6 \u00b1 0.4 55.9 \u00b1 0.3 37.9 \u00b1 0.3 78.0 \u00b1 0.2 70.2 \u00b1 0.0 DAE + DaNN 53.6 \u00b1 0.2 37.3 \u00b1 0.0 59.9 \u00b1 0.1 38.2 \u00b1 0.0 83.5 \u00b1 0.0 71.2 \u00b1 0.0\n\nTable 1 .\n1The standard parameter setting of the DaNN.Learning rate (\u03b1) \n0.02 \nIterations \n900 \nMomentum \n0.05 \nL2 weight regularization 0.003 \nDropout fraction \n0.5 \n\nFor the MMD regularization, we set the standard deviation s of the Gaussian \n\nkernel by the following calculation: s = M SD \n\n2 \n\n\n\nTable 4 .\n4The in-domain performances on the Office data set using 10-fold crossvalidation on each domain.Methods \namazon \nwebcam \ndslr \n\nThis number was to obtain dimensionality reduction. We tried other values such as 100, 300, and 500. Eventually, the number of 256 hidden nodes gave us the best performance among other values.\nhttp://www.csie.ntu.edu.tw/~cjlin/liblinear 4 Here we used the subspaces constructed by PCA only 5 http://learn.tsinghua.edu.cn:8080/2011310560/long.html 6 It is basically Algorithm 1 without Step 3.\n\nUnsupervised domain adaptation by domain invariant projection. M Baktashmotlagh, M T Harandi, B C Lovell, M Salzmann, Proceedings of International Conference on Computer Vision. International Conference on Computer VisionM. Baktashmotlagh, M. T. Harandi, B. C. Lovell, and M. Salzmann. Unsupervised domain adaptation by domain invariant projection. In Proceedings of International Conference on Computer Vision, pages 769-776, 2013.\n\nSurf: Speeded up robust features. H Bay, T Tuytelaars, L V Gool, Computer Vision and Image Understanding (CVIU). 1103H. Bay, T. Tuytelaars, and L. V. Gool. Surf: Speeded up robust features. Computer Vision and Image Understanding (CVIU), 110(3):346-359, 2008.\n\nDeep learning of representations: Looking forward. Y Bengio, Statistical Language and Speech Processing. Springer7978Y. Bengio. Deep learning of representations: Looking forward. In Statistical Language and Speech Processing, volume 7978 of Lecture Notes in Computer Science, pages 1-37. Springer, 2013.\n\nGreedy layer-wise training of deep networks. P Bengio, D Lamblin, H Popovici, Larochelle, Advances in Neural Information Processing Systems (NIPS). 19153Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy layer-wise training of deep networks. In Advances in Neural Information Processing Systems (NIPS), volume 19, page 153, 2007.\n\nRepresentation learning: A review and new perspectives. A C Bengio, P Courville, Vincent, 5538Computing Research Repository, abs/1206Bengio, A. C. Courville, and P. Vincent. Representation learning: A review and new perspectives. Computing Research Repository, abs/1206.5538, 2012.\n\nIntegrating structured biological data by kernel maximum mean discrepancy. M Borgwardt, A Gretton, M J Rasch, H.-P Kriegel, B Sch\u00f6lkopf, A J Smola, Bioinformatics. 2214M. Borgwardt, A. Gretton, M. J. Rasch, H.-P. Kriegel, B. Sch\u00f6lkopf, and A. J. Smola. Integrating structured biological data by kernel maximum mean discrepancy. Bioinformatics, 22(14):e49-e57, 2006.\n\nSupport-Vector Networks. C Cortes, V N Vapnik, Machine Learning. 20C. Cortes and V. N. Vapnik. Support-Vector Networks. Machine Learning, 20(3): 273-297, 1995.\n\nWhy does unsupervised pre-training help deep learning. H Daum\u00e9-Iii. ; D. Erhan, Y Bengio, A Courville, P.-A Manzagol, P Vincent, abs/0907.1815Journal of Machine Learning Research. 11Frustratingly easy domain adaptationH. Daum\u00e9-III. Frustratingly easy domain adaptation. CoRR, abs/0907.1815, 2009. D. Erhan, Y. Bengio, A. Courville, P.-A. Manzagol, and P. Vincent. Why does un- supervised pre-training help deep learning? Journal of Machine Learning Research, 11:625-660, 2010.\n\nDeep sparse rectifier neural network. A Glorot, Y Bordes, Bengio, Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS). the 14th International Conference on Artificial Intelligence and Statistics (AISTATS)Glorot, A. Bordes, and Y. Bengio. Deep sparse rectifier neural network. In Pro- ceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS), pages 315-323, 2011.\n\nGeodesic flow kernel for unsupervised domain adaptation. B Gong, Y Shi, F Sha, K Grauman, Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Conference on Computer Vision and Pattern Recognition (CVPR)B. Gong, Y. Shi, F. Sha, and K. Grauman. Geodesic flow kernel for unsupervised domain adaptation. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2066-2073, 2012.\n\nDomain adaptation for object recognition: An unsupervised approach. R Gopalan, R Li, R Chellapa, IEEE International Conference on Computer Vision. R. Gopalan, R. Li, and R. Chellapa. Domain adaptation for object recognition: An unsupervised approach. In IEEE International Conference on Computer Vision, pages 999-1006, 2011.\n\nA kernel two-sample test. K M Gretton, M J Borgwardt, B Rasch, A Sch&apos;olkopf, Smola, Journal of Machine Learning Research. Gretton, K. M. Borgwardt, M. J. Rasch, B. Sch'olkopf, and A. Smola. A kernel two-sample test. Journal of Machine Learning Research, pages 723-773, 2012.\n\nImproving neural networks by preventing co-adaptation of feature detectors. G E Hinton, N Srivastava, A Krizhevsky, I Sutskever, R Salakhutdinov, abs/1207.0580CoRRG. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Im- proving neural networks by preventing co-adaptation of feature detectors. CoRR, abs/1207.0580, 2012.\n\nTransfer sparse coding for robust image representation. M Long, G Ding, J Wang, J Sun, Y Guo, P S Yu, Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Conference on Computer Vision and Pattern Recognition (CVPR)M. Long, G. Ding, J. Wang, J. Sun, Y. Guo, and P. S. Yu. Transfer sparse coding for robust image representation. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 404-414, 2013.\n\nRectified linear units improve restricted boltzmann machines. V Nair, G E Hinton, Proceedings of the 27th International Conference on Machine Learning (ICML). the 27th International Conference on Machine Learning (ICML)V. Nair and G. E. Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th International Conference on Machine Learning (ICML), 2010.\n\nA survey on transfer learning. S J Pan, Q Yang, IEEE Transactions on Knowledge and Data Engineering. 2210S. J. Pan and Q. Yang. A survey on transfer learning. IEEE Transactions on Knowledge and Data Engineering, 22(10):1345-1359, 2010.\n\nDomain adaptation via transfer component analysis. S J Pan, I W Tsang, J T Kwok, Q Yang, Proceedings of the 21st International Joint Conference on Artificial Intelligence (IJCAI). the 21st International Joint Conference on Artificial Intelligence (IJCAI)S. J. Pan, I. W. Tsang, J. T. Kwok, and Q. Yang. Domain adaptation via transfer component analysis. In Proceedings of the 21st International Joint Conference on Artificial Intelligence (IJCAI), pages 1187-1192, 2009.\n\nAdapting visual cateogry models to new domains. K Saenko, B Kulis, M Fritz, T Darrell, ECCV. K. Saenko, B. Kulis, M. Fritz, and T. Darrell. Adapting visual cateogry models to new domains. In ECCV, pages 213-226, 2010.\n\nOn the influence of the kernel on the consistency of support vector machines. I Steinwart, Journal of Machine Learning Research. 2I. Steinwart. On the influence of the kernel on the consistency of support vector machines. Journal of Machine Learning Research, 2:67-93, 2002.\n\nStacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. H Vincent, I Larochelle, Y Lajoie, P.-A Bengio, Manzagol, Journal of Machine Learning Research. 11Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol. Stacked denois- ing autoencoders: Learning useful representations in a deep network with a local denoising criterion. Journal of Machine Learning Research, 11:3371-3408, 2010.\n", "annotations": {"author": "[{\"end\":216,\"start\":58},{\"end\":345,\"start\":217},{\"end\":498,\"start\":346}]", "publisher": null, "author_last_name": "[{\"end\":74,\"start\":67},{\"end\":234,\"start\":228},{\"end\":359,\"start\":354}]", "author_first_name": "[{\"end\":66,\"start\":58},{\"end\":218,\"start\":217},{\"end\":227,\"start\":219},{\"end\":353,\"start\":346}]", "author_affiliation": "[{\"end\":215,\"start\":107},{\"end\":344,\"start\":236},{\"end\":497,\"start\":389}]", "title": "[{\"end\":55,\"start\":1},{\"end\":553,\"start\":499}]", "venue": null, "abstract": "[{\"end\":1422,\"start\":653}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2776,\"start\":2758},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2798,\"start\":2776},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2817,\"start\":2798},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2838,\"start\":2817},{\"end\":2896,\"start\":2871},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2907,\"start\":2896},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2973,\"start\":2952},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3177,\"start\":3159},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3653,\"start\":3632},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4030,\"start\":4010},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4050,\"start\":4030},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5954,\"start\":5930},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6483,\"start\":6460},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6981,\"start\":6964},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7187,\"start\":7169},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8990,\"start\":8969},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9159,\"start\":9136},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9870,\"start\":9849},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10721,\"start\":10702},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12391,\"start\":12374},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":13723,\"start\":13701},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":15993,\"start\":15973},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":16372,\"start\":16354},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":16626,\"start\":16607},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":17141,\"start\":17121},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":17430,\"start\":17401},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":18458,\"start\":18437},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":18738,\"start\":18713},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":18929,\"start\":18910},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":19121,\"start\":19102},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":19566,\"start\":19548},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":19881,\"start\":19860},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":27468,\"start\":27449},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":27497,\"start\":27478},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":27543,\"start\":27522},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":28187,\"start\":28173},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":29157,\"start\":29136}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":28949,\"start\":28921},{\"attributes\":{\"id\":\"fig_2\"},\"end\":29105,\"start\":28950},{\"attributes\":{\"id\":\"fig_3\"},\"end\":29195,\"start\":29106},{\"attributes\":{\"id\":\"fig_4\"},\"end\":29506,\"start\":29196},{\"attributes\":{\"id\":\"fig_5\"},\"end\":30784,\"start\":29507},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":31084,\"start\":30785},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":31223,\"start\":31085}]", "paragraph": "[{\"end\":2198,\"start\":1438},{\"end\":2498,\"start\":2200},{\"end\":2908,\"start\":2500},{\"end\":3519,\"start\":2910},{\"end\":4230,\"start\":3521},{\"end\":5042,\"start\":4232},{\"end\":5290,\"start\":5060},{\"end\":5622,\"start\":5319},{\"end\":5955,\"start\":5683},{\"end\":6212,\"start\":6003},{\"end\":6484,\"start\":6293},{\"end\":6827,\"start\":6762},{\"end\":7316,\"start\":6829},{\"end\":7689,\"start\":7318},{\"end\":8133,\"start\":7722},{\"end\":8424,\"start\":8135},{\"end\":8793,\"start\":8474},{\"end\":9264,\"start\":8861},{\"end\":9508,\"start\":9266},{\"end\":9626,\"start\":9563},{\"end\":10722,\"start\":9653},{\"end\":11035,\"start\":10724},{\"end\":11524,\"start\":11071},{\"end\":11558,\"start\":11526},{\"end\":11695,\"start\":11583},{\"end\":11743,\"start\":11738},{\"end\":12090,\"start\":11810},{\"end\":12415,\"start\":12092},{\"end\":12494,\"start\":12458},{\"end\":12663,\"start\":12496},{\"end\":12822,\"start\":12754},{\"end\":13413,\"start\":13343},{\"end\":14190,\"start\":13577},{\"end\":14745,\"start\":14192},{\"end\":14807,\"start\":14747},{\"end\":15303,\"start\":14809},{\"end\":15396,\"start\":15345},{\"end\":15922,\"start\":15431},{\"end\":16936,\"start\":15932},{\"end\":17250,\"start\":16938},{\"end\":18546,\"start\":17252},{\"end\":19425,\"start\":18548},{\"end\":20056,\"start\":19454},{\"end\":21142,\"start\":20058},{\"end\":21769,\"start\":21214},{\"end\":22332,\"start\":21799},{\"end\":23542,\"start\":22391},{\"end\":24468,\"start\":23600},{\"end\":24909,\"start\":24470},{\"end\":25335,\"start\":24931},{\"end\":25758,\"start\":25337},{\"end\":26260,\"start\":25760},{\"end\":26776,\"start\":26262},{\"end\":28416,\"start\":26808},{\"end\":28920,\"start\":28418}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":5682,\"start\":5623},{\"attributes\":{\"id\":\"formula_1\"},\"end\":6002,\"start\":5956},{\"attributes\":{\"id\":\"formula_2\"},\"end\":6292,\"start\":6213},{\"attributes\":{\"id\":\"formula_3\"},\"end\":6724,\"start\":6485},{\"attributes\":{\"id\":\"formula_4\"},\"end\":6761,\"start\":6724},{\"attributes\":{\"id\":\"formula_5\"},\"end\":8473,\"start\":8425},{\"attributes\":{\"id\":\"formula_6\"},\"end\":8860,\"start\":8794},{\"attributes\":{\"id\":\"formula_7\"},\"end\":9562,\"start\":9509},{\"attributes\":{\"id\":\"formula_8\"},\"end\":11582,\"start\":11559},{\"attributes\":{\"id\":\"formula_9\"},\"end\":11737,\"start\":11696},{\"attributes\":{\"id\":\"formula_10\"},\"end\":11809,\"start\":11744},{\"attributes\":{\"id\":\"formula_11\"},\"end\":12457,\"start\":12416},{\"attributes\":{\"id\":\"formula_12\"},\"end\":12753,\"start\":12664},{\"attributes\":{\"id\":\"formula_13\"},\"end\":13342,\"start\":12823},{\"attributes\":{\"id\":\"formula_14\"},\"end\":13576,\"start\":13414},{\"attributes\":{\"id\":\"formula_15\"},\"end\":15344,\"start\":15304},{\"attributes\":{\"id\":\"formula_16\"},\"end\":21189,\"start\":21143},{\"attributes\":{\"id\":\"formula_17\"},\"end\":22390,\"start\":22333},{\"attributes\":{\"id\":\"formula_18\"},\"end\":23599,\"start\":23543}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":17056,\"start\":17049},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":19368,\"start\":19360},{\"end\":20055,\"start\":20048},{\"end\":20831,\"start\":20824},{\"end\":22331,\"start\":22324},{\"end\":22735,\"start\":22728},{\"end\":24997,\"start\":24990},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":25654,\"start\":25647},{\"end\":26042,\"start\":25989},{\"end\":26288,\"start\":26281}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1436,\"start\":1424},{\"attributes\":{\"n\":\"2\"},\"end\":5058,\"start\":5045},{\"attributes\":{\"n\":\"2.1\"},\"end\":5317,\"start\":5293},{\"attributes\":{\"n\":\"2.2\"},\"end\":7720,\"start\":7692},{\"attributes\":{\"n\":\"2.3\"},\"end\":9651,\"start\":9629},{\"attributes\":{\"n\":\"3\"},\"end\":11069,\"start\":11038},{\"end\":15429,\"start\":15399},{\"attributes\":{\"n\":\"4.1\"},\"end\":15930,\"start\":15925},{\"attributes\":{\"n\":\"4.2\"},\"end\":19452,\"start\":19428},{\"attributes\":{\"n\":\"4.3\"},\"end\":21212,\"start\":21191},{\"end\":21797,\"start\":21772},{\"end\":24929,\"start\":24912},{\"attributes\":{\"n\":\"5\"},\"end\":26806,\"start\":26779},{\"end\":29115,\"start\":29107},{\"end\":30795,\"start\":30786},{\"end\":31095,\"start\":31086}]", "table": "[{\"end\":31084,\"start\":30840},{\"end\":31223,\"start\":31192}]", "figure_caption": "[{\"end\":28949,\"start\":28923},{\"end\":29105,\"start\":28952},{\"end\":29195,\"start\":29117},{\"end\":29506,\"start\":29198},{\"end\":30784,\"start\":29509},{\"end\":30840,\"start\":30797},{\"end\":31192,\"start\":31097}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":16681,\"start\":16673},{\"end\":22231,\"start\":22223},{\"end\":24141,\"start\":24133}]", "bib_author_first_name": "[{\"end\":31682,\"start\":31681},{\"end\":31700,\"start\":31699},{\"end\":31702,\"start\":31701},{\"end\":31713,\"start\":31712},{\"end\":31715,\"start\":31714},{\"end\":31725,\"start\":31724},{\"end\":32087,\"start\":32086},{\"end\":32094,\"start\":32093},{\"end\":32108,\"start\":32107},{\"end\":32110,\"start\":32109},{\"end\":32365,\"start\":32364},{\"end\":32664,\"start\":32663},{\"end\":32674,\"start\":32673},{\"end\":32685,\"start\":32684},{\"end\":33014,\"start\":33013},{\"end\":33016,\"start\":33015},{\"end\":33026,\"start\":33025},{\"end\":33316,\"start\":33315},{\"end\":33329,\"start\":33328},{\"end\":33340,\"start\":33339},{\"end\":33342,\"start\":33341},{\"end\":33354,\"start\":33350},{\"end\":33365,\"start\":33364},{\"end\":33378,\"start\":33377},{\"end\":33380,\"start\":33379},{\"end\":33633,\"start\":33632},{\"end\":33643,\"start\":33642},{\"end\":33645,\"start\":33644},{\"end\":33824,\"start\":33823},{\"end\":33849,\"start\":33848},{\"end\":33859,\"start\":33858},{\"end\":33875,\"start\":33871},{\"end\":33887,\"start\":33886},{\"end\":34285,\"start\":34284},{\"end\":34295,\"start\":34294},{\"end\":34758,\"start\":34757},{\"end\":34766,\"start\":34765},{\"end\":34773,\"start\":34772},{\"end\":34780,\"start\":34779},{\"end\":35213,\"start\":35212},{\"end\":35224,\"start\":35223},{\"end\":35230,\"start\":35229},{\"end\":35498,\"start\":35497},{\"end\":35500,\"start\":35499},{\"end\":35511,\"start\":35510},{\"end\":35513,\"start\":35512},{\"end\":35526,\"start\":35525},{\"end\":35535,\"start\":35534},{\"end\":35829,\"start\":35828},{\"end\":35831,\"start\":35830},{\"end\":35841,\"start\":35840},{\"end\":35855,\"start\":35854},{\"end\":35869,\"start\":35868},{\"end\":35882,\"start\":35881},{\"end\":36158,\"start\":36157},{\"end\":36166,\"start\":36165},{\"end\":36174,\"start\":36173},{\"end\":36182,\"start\":36181},{\"end\":36189,\"start\":36188},{\"end\":36196,\"start\":36195},{\"end\":36198,\"start\":36197},{\"end\":36633,\"start\":36632},{\"end\":36641,\"start\":36640},{\"end\":36643,\"start\":36642},{\"end\":36996,\"start\":36995},{\"end\":36998,\"start\":36997},{\"end\":37005,\"start\":37004},{\"end\":37253,\"start\":37252},{\"end\":37255,\"start\":37254},{\"end\":37262,\"start\":37261},{\"end\":37264,\"start\":37263},{\"end\":37273,\"start\":37272},{\"end\":37275,\"start\":37274},{\"end\":37283,\"start\":37282},{\"end\":37722,\"start\":37721},{\"end\":37732,\"start\":37731},{\"end\":37741,\"start\":37740},{\"end\":37750,\"start\":37749},{\"end\":37971,\"start\":37970},{\"end\":38285,\"start\":38284},{\"end\":38296,\"start\":38295},{\"end\":38310,\"start\":38309},{\"end\":38323,\"start\":38319}]", "bib_author_last_name": "[{\"end\":31697,\"start\":31683},{\"end\":31710,\"start\":31703},{\"end\":31722,\"start\":31716},{\"end\":31734,\"start\":31726},{\"end\":32091,\"start\":32088},{\"end\":32105,\"start\":32095},{\"end\":32115,\"start\":32111},{\"end\":32372,\"start\":32366},{\"end\":32671,\"start\":32665},{\"end\":32682,\"start\":32675},{\"end\":32694,\"start\":32686},{\"end\":32706,\"start\":32696},{\"end\":33023,\"start\":33017},{\"end\":33036,\"start\":33027},{\"end\":33045,\"start\":33038},{\"end\":33326,\"start\":33317},{\"end\":33337,\"start\":33330},{\"end\":33348,\"start\":33343},{\"end\":33362,\"start\":33355},{\"end\":33375,\"start\":33366},{\"end\":33386,\"start\":33381},{\"end\":33640,\"start\":33634},{\"end\":33652,\"start\":33646},{\"end\":33846,\"start\":33825},{\"end\":33856,\"start\":33850},{\"end\":33869,\"start\":33860},{\"end\":33884,\"start\":33876},{\"end\":33895,\"start\":33888},{\"end\":34292,\"start\":34286},{\"end\":34302,\"start\":34296},{\"end\":34310,\"start\":34304},{\"end\":34763,\"start\":34759},{\"end\":34770,\"start\":34767},{\"end\":34777,\"start\":34774},{\"end\":34788,\"start\":34781},{\"end\":35221,\"start\":35214},{\"end\":35227,\"start\":35225},{\"end\":35239,\"start\":35231},{\"end\":35508,\"start\":35501},{\"end\":35523,\"start\":35514},{\"end\":35532,\"start\":35527},{\"end\":35551,\"start\":35536},{\"end\":35558,\"start\":35553},{\"end\":35838,\"start\":35832},{\"end\":35852,\"start\":35842},{\"end\":35866,\"start\":35856},{\"end\":35879,\"start\":35870},{\"end\":35896,\"start\":35883},{\"end\":36163,\"start\":36159},{\"end\":36171,\"start\":36167},{\"end\":36179,\"start\":36175},{\"end\":36186,\"start\":36183},{\"end\":36193,\"start\":36190},{\"end\":36201,\"start\":36199},{\"end\":36638,\"start\":36634},{\"end\":36650,\"start\":36644},{\"end\":37002,\"start\":36999},{\"end\":37010,\"start\":37006},{\"end\":37259,\"start\":37256},{\"end\":37270,\"start\":37265},{\"end\":37280,\"start\":37276},{\"end\":37288,\"start\":37284},{\"end\":37729,\"start\":37723},{\"end\":37738,\"start\":37733},{\"end\":37747,\"start\":37742},{\"end\":37758,\"start\":37751},{\"end\":37981,\"start\":37972},{\"end\":38293,\"start\":38286},{\"end\":38307,\"start\":38297},{\"end\":38317,\"start\":38311},{\"end\":38330,\"start\":38324},{\"end\":38340,\"start\":38332}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":12271421},\"end\":32050,\"start\":31618},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":161878},\"end\":32311,\"start\":32052},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":1044293},\"end\":32616,\"start\":32313},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":14201947},\"end\":32955,\"start\":32618},{\"attributes\":{\"id\":\"b4\"},\"end\":33238,\"start\":32957},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":13341746},\"end\":33605,\"start\":33240},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":52874011},\"end\":33766,\"start\":33607},{\"attributes\":{\"doi\":\"abs/0907.1815\",\"id\":\"b7\",\"matched_paper_id\":15796526},\"end\":34244,\"start\":33768},{\"attributes\":{\"id\":\"b8\"},\"end\":34698,\"start\":34246},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":6742009},\"end\":35142,\"start\":34700},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":10337178},\"end\":35469,\"start\":35144},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":10742222},\"end\":35750,\"start\":35471},{\"attributes\":{\"doi\":\"abs/1207.0580\",\"id\":\"b12\"},\"end\":36099,\"start\":35752},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":9728005},\"end\":36568,\"start\":36101},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":15539264},\"end\":36962,\"start\":36570},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":740063},\"end\":37199,\"start\":36964},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":788838},\"end\":37671,\"start\":37201},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":7534823},\"end\":37890,\"start\":37673},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":7716351},\"end\":38166,\"start\":37892},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":17804904},\"end\":38623,\"start\":38168}]", "bib_title": "[{\"end\":31679,\"start\":31618},{\"end\":32084,\"start\":32052},{\"end\":32362,\"start\":32313},{\"end\":32661,\"start\":32618},{\"end\":33313,\"start\":33240},{\"end\":33630,\"start\":33607},{\"end\":33821,\"start\":33768},{\"end\":34282,\"start\":34246},{\"end\":34755,\"start\":34700},{\"end\":35210,\"start\":35144},{\"end\":35495,\"start\":35471},{\"end\":36155,\"start\":36101},{\"end\":36630,\"start\":36570},{\"end\":36993,\"start\":36964},{\"end\":37250,\"start\":37201},{\"end\":37719,\"start\":37673},{\"end\":37968,\"start\":37892},{\"end\":38282,\"start\":38168}]", "bib_author": "[{\"end\":31699,\"start\":31681},{\"end\":31712,\"start\":31699},{\"end\":31724,\"start\":31712},{\"end\":31736,\"start\":31724},{\"end\":32093,\"start\":32086},{\"end\":32107,\"start\":32093},{\"end\":32117,\"start\":32107},{\"end\":32374,\"start\":32364},{\"end\":32673,\"start\":32663},{\"end\":32684,\"start\":32673},{\"end\":32696,\"start\":32684},{\"end\":32708,\"start\":32696},{\"end\":33025,\"start\":33013},{\"end\":33038,\"start\":33025},{\"end\":33047,\"start\":33038},{\"end\":33328,\"start\":33315},{\"end\":33339,\"start\":33328},{\"end\":33350,\"start\":33339},{\"end\":33364,\"start\":33350},{\"end\":33377,\"start\":33364},{\"end\":33388,\"start\":33377},{\"end\":33642,\"start\":33632},{\"end\":33654,\"start\":33642},{\"end\":33848,\"start\":33823},{\"end\":33858,\"start\":33848},{\"end\":33871,\"start\":33858},{\"end\":33886,\"start\":33871},{\"end\":33897,\"start\":33886},{\"end\":34294,\"start\":34284},{\"end\":34304,\"start\":34294},{\"end\":34312,\"start\":34304},{\"end\":34765,\"start\":34757},{\"end\":34772,\"start\":34765},{\"end\":34779,\"start\":34772},{\"end\":34790,\"start\":34779},{\"end\":35223,\"start\":35212},{\"end\":35229,\"start\":35223},{\"end\":35241,\"start\":35229},{\"end\":35510,\"start\":35497},{\"end\":35525,\"start\":35510},{\"end\":35534,\"start\":35525},{\"end\":35553,\"start\":35534},{\"end\":35560,\"start\":35553},{\"end\":35840,\"start\":35828},{\"end\":35854,\"start\":35840},{\"end\":35868,\"start\":35854},{\"end\":35881,\"start\":35868},{\"end\":35898,\"start\":35881},{\"end\":36165,\"start\":36157},{\"end\":36173,\"start\":36165},{\"end\":36181,\"start\":36173},{\"end\":36188,\"start\":36181},{\"end\":36195,\"start\":36188},{\"end\":36203,\"start\":36195},{\"end\":36640,\"start\":36632},{\"end\":36652,\"start\":36640},{\"end\":37004,\"start\":36995},{\"end\":37012,\"start\":37004},{\"end\":37261,\"start\":37252},{\"end\":37272,\"start\":37261},{\"end\":37282,\"start\":37272},{\"end\":37290,\"start\":37282},{\"end\":37731,\"start\":37721},{\"end\":37740,\"start\":37731},{\"end\":37749,\"start\":37740},{\"end\":37760,\"start\":37749},{\"end\":37983,\"start\":37970},{\"end\":38295,\"start\":38284},{\"end\":38309,\"start\":38295},{\"end\":38319,\"start\":38309},{\"end\":38332,\"start\":38319},{\"end\":38342,\"start\":38332}]", "bib_venue": "[{\"end\":31794,\"start\":31736},{\"end\":32163,\"start\":32117},{\"end\":32416,\"start\":32374},{\"end\":32764,\"start\":32708},{\"end\":33011,\"start\":32957},{\"end\":33402,\"start\":33388},{\"end\":33670,\"start\":33654},{\"end\":33946,\"start\":33910},{\"end\":34412,\"start\":34312},{\"end\":34870,\"start\":34790},{\"end\":35289,\"start\":35241},{\"end\":35596,\"start\":35560},{\"end\":35826,\"start\":35752},{\"end\":36283,\"start\":36203},{\"end\":36727,\"start\":36652},{\"end\":37063,\"start\":37012},{\"end\":37379,\"start\":37290},{\"end\":37764,\"start\":37760},{\"end\":38019,\"start\":37983},{\"end\":38378,\"start\":38342},{\"end\":31839,\"start\":31796},{\"end\":34499,\"start\":34414},{\"end\":34937,\"start\":34872},{\"end\":36350,\"start\":36285},{\"end\":36789,\"start\":36729},{\"end\":37455,\"start\":37381}]"}}}, "year": 2023, "month": 12, "day": 17}