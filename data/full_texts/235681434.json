{"id": 235681434, "updated": "2023-04-05 13:41:47.821", "metadata": {"title": "Wasserstein Barycenter for Multi-Source Domain Adaptation", "authors": "[{\"first\":\"Eduardo\",\"last\":\"Montesuma\",\"middle\":[\"Fernandes\"]},{\"first\":\"Fred\",\"last\":\"Mboula\",\"middle\":[\"Maurice\",\"Ngol\u00e8\"]}]", "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2021, "month": 6, "day": 1}, "abstract": "Multi-source domain adaptation is a key technique that allows a model to be trained on data coming from various probability distribution. To overcome the challenges posed by this learning scenario, we propose a method for constructing an intermediate domain between sources and target domain, the Wasserstein Barycenter Transport (WBT). This method relies on the barycenter on Wasserstein spaces for aggregating the source probability distributions. Once the sources have been aggregated, they are transported to the target domain using standard Optimal Transport for Domain Adaptation framework. Additionally, we revisit previous single-source domain adaptation tasks in the context of multi-source scenario. In particular, we apply our algorithm to object and face recognition datasets. Moreover, to diversify the range of applications, we also examine the tasks of music genre recognition and music-speech discrimination. The experiments show that our method has similar performance with the existing state-of-the-art.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/MontesumaM21", "doi": "10.1109/cvpr46437.2021.01651"}}, "content": {"source": {"pdf_hash": "789bbc9f2990853d6af7b31ac00710ef108bd781", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "d22e67de3152444301af2773171aeddc4510d6e4", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/789bbc9f2990853d6af7b31ac00710ef108bd781.txt", "contents": "\nWasserstein Barycenter for Multi-Source Domain Adaptation\n\n\nEduardo Fernandes Montesuma eduardomontesuma@alu.ufc.br \nUniversidade Federal do Cear\u00e1 Fortaleza\nBrazil\n\nFred Maurice \nInstitut LIST\nUniversit\u00e9 Paris-Saclay\nCEA\nF-91120PalaiseauFrance\n\nNgol\u00e8 Mboula \nInstitut LIST\nUniversit\u00e9 Paris-Saclay\nCEA\nF-91120PalaiseauFrance\n\nWasserstein Barycenter for Multi-Source Domain Adaptation\n10.1109/CVPR46437.2021.01651\nMulti-source domain adaptation is a key technique that allows a model to be trained on data coming from various probability distribution. To overcome the challenges posed by this learning scenario, we propose a method for constructing an intermediate domain between sources and target domain, the Wasserstein Barycenter Transport (WBT).This method relies on the barycenter on Wasserstein spaces for aggregating the source probability distributions. Once the sources have been aggregated, they are transported to the target domain using standard Optimal Transport for Domain Adaptation framework. Additionally, we revisit previous single-source domain adaptation tasks in the context of multi-source scenario. In particular, we apply our algorithm to object and face recognition datasets. Moreover, to diversify the range of applications, we also examine the tasks of music genre recognition and music-speech discrimination. The experiments show that our method has similar performance with the existing state-of-the-art.\n\nIntroduction\n\nStandard data-driven algorithms are based upon the hypothesis that training and test data follows the same probability distribution. When this assumption does not hold, a case also known as distributional shift, these algorithms may suffer from performance degradation. In this case, many predictive models need to be re-trained on the new data, ignoring previous knowledge.\n\nThere are various examples of distributional shift in areas of application of machine learning, such as image [22][23] [27], natural language [3] [2], and speech [15] [24] processing. This has been known in the literature as transfer learning.\n\nTransfer learning may be further categorized according to the nature of distributional shift. First, the features distribution change between training and test sets, that is P s (X) = P t (X). This case is commonly referred as domain adaptation [17]. Additionally, the shift may also occur on the labels distribution P s (Y ) = P t (Y ), or in the conditional distribution P s (Y |X) = P t (Y |X). These cases are known respectively as target and conditional shift.\n\nIn this paper, a focus is given to unsupervised domain adaptation. Hence, we refer to training and test data as coming from different domains, respectively a source D s and target domain D t . According to [17], the goal of unsupervised domain adaptation is to help the learning of a predictive model on D t , using knowledge learned in D s , without using any labels in the target domain.\n\nIn addition, as first remarked by [3], the distance between probability distributions play an important role on the model's performance on the target domain. This inspired various algorithms to explore statistical divergence minimization strategies for domain adaptation. In particular, as presented in the seminal works of [4], optimal transport can be used to do so.\n\nOptimal transport is a mathematical theory that was originally devised in the context of transportation of masses under least effort [26]. Since there is a natural association between masses and probability distributions, optimal transport is suited for devising transformations that match different distributions. This theory has two contributions to domain adaptation: the definition of a transport map T that matches P s and P t , and the definition of a notion of distance between distributions, the Wasserstein distance.\n\nAmong the contributions of optimal transport to domain adaptation, the Wasserstein distance is of particular interest in this work. Indeed, it metrizes the space of probability distributions, hence geometric concepts such as the idea of barycenters can be extended to this space [1]. This latter notion is particularly useful for the Multi-Source Domain Adaptation (MSDA) setting.\n\nThe multi-source case in domain adaptation corresponds to when one has access to data coming from various domains {D s k } N k=1 . This case is challenging because one needs to minimize the distance from each P s k to P t jointly. In the following, we provide our contributions, the intuition for our algorithm, and the paper structure. Contributions. Our contributions are twofold: (1) we pro-pose a new method for MSDA, the Wassertein Barycenter Transport (WBT). (2) A comparison with the state-of-theart, revisiting datasets used for single-source domain adaptation which can be adapted to the MSDA scenario. In particular, the algorithms are evaluated on acoustic and visual adaptation datasets. Intuition. We propose an algorithm for solving MSDA based on the Wasserstein barycenter. The intuition is to aggregate all source domains {D s k } N k=1 into a single domain, D b through the Wasserstein barycenter. Once the aggregation step is done, standard domain adaptation may be employed. Paper structure. The rest of this paper is organized as follows: section 2 presents the related work in the fields of domain adaptation, optimal transport, acoustic and visual recognition. Section 3 details the WBT algorithm. Section 4 details the numerical experiments and its results, as well as it discusses the findings. Finally, section 5 concludes the paper.\n\n\nRelated Work\n\nIn the section we cover the state-of-the-art in domain adaptation and Optimal Transport for Domain Adaptation (OTDA). Moreover, a particular focus will be given to domain adaptation in the multi-source context.\n\nDomain adaptation is a sub-topic of transfer learning, as first defined by [17]. The first work to consider a mismatch in the data distributions was [13], who proposed a method for the detection and estimation of change. Since then, this approach was formalized in [3], [7] and [2]. This latter work provided a solid theoretical ground for both single and multi-source domain adaptation.\n\nThe application of optimal transport to the latter subject is very recent, and it was first introduced in the seminal works of [4], which proposed solving the domain adaptation problem through the minimization of the Wasserstein distance between source and target distributions. This approach was then formalized in [20], which also commented on the possibility of using the Wasserstein barycenter for solving the MSDA case.\n\nTo the best of our knowledge, only two optimal transport based approaches for solving the MSDA scenario exists. These are the Joint Class Proportion and Optimal Transport (JCPOT) [19] and Weighted Joint Distribution Optimal Transport (WJDOT) [24]. The first algorithm was devised to solve a MSDA problem when there is a mismatch between the target distributions, that is, P s (Y ) = P t (Y ). The second one proposes to solve the standard MSDA problem by using the joint distribution P (X, Y ), and the estimation of a weighting vector \u03b1. In this latter case, as the target labels are not available, the authors have proposed to substitute P t (X, Y ) for P f (X, Y ) = P t (X, f (X)), where f is the predictive function.\n\nOur approach bears some similarity to both of these approaches. First, JCPOT estimates the class proportions through the Wasserstein barycenter. Second, WJDOT may be viewed as a barycenter between the distributions P s k and P f , using \u03b1 as the barycenter weights. Our approach, however, is different in the sense that the Wasserstein barycenter is used to build an intermediate domain for the transportation from sources to target.\n\nConcerning the theory of barycenter in Wasserstein spaces, it was first formalized by [1], which presented the Wasserstein barycenter as an optimization problem. The treatment made by the authors was rather theoretical. Using their work as a foundation [6] proposed a fast algorithm to compute Wasserstein barycenters, using the Sinkhorn algorithm [5]. Despite the availability of new approaches, such as [14], we use the approach presented by [6] due to its simplicity.\n\n\nProposed Approach\n\nIn this section we discuss the WBT algorithm and its applications for MSDA. In this context, we present the theoretical background of optimal transport in section 3.1, the class-based regularization approaches to OTDA in section 3.2, the concept of Wasserstein barycenter in 3.3 and the WBT algorithm in section 3.4. Finally, in Section 3.5 we provide a theoretical discussion of our algorithm, based on the theoretical results of [20].\n\nIn the following discussion, we will adopt a discrete approach for optimal transport, as it is better suited for most of machine learning applications. In this setting, one does not know the source and target distributions \u03bc s and \u03bc t , but has access to samples X s = {x s i } n s i=1 and X t = {x t j } t j=1 . These samples induce empirical distributions\u03bc s and\u03bc t . Each of these distributions is uniquely defined by its support X s (resp. X t ) and sample weights \u03c9 s i (resp. \u03c9 t j ). We will further assume that \u03c9 s (resp. \u03c9 t ) is uniform, that is, \u03c9 s i = n \u22121 s (resp. n \u22121 t ), as in [4]. Therefore,\u03bc s (resp.\u03bc t ) may be expressed as,\n\u03bc s (x) = 1 n s n s i=1 \u03b4(x \u2212 x s i ), ,(1)\nwhere \u03b4 is the delta dirac function.\n\n\nBackground\n\nIn this section, we formalize the problem of transfer learning with focus on domain adaptation, and detail how optimal transport can be used to solve it.\n\nFormally, a domain is a pair D = (X , \u03bc) [17], where X is a feature space and \u03bc is its probability distribution. In domain adaptation, therefore, one has D s = (X s , \u03bc s ), D t = (X t , \u03bc t ) with \u03bc s = \u03bc t . Moreover, it is assumed that the labels conditional distribution P (Y |X), is preserved across the domains, that is, P s (Y |X) = P t (Y |X). This corresponds the covariate shift hypothesis. For simplicity, we may assume that X s \u2282 R d and X t \u2282 R d are Euclidean spaces.\n\nThe application of optimal transport in the context of domain adaptation was first proposed by [4], in which the authors supposed that \u03bc s = \u03bc t due to an unknown transformation T : X s \u2192 X t . In addition, T is forced to preserve mass. In terms of empirical distributions, this is equivalent to the condition,\n\u03c9 t j = i:T (x i s )=x t j \u03c9 s i .\nThis latter condition may be expressed through the pushforward operator, T # , so that T #\u03bcs =\u03bc t [4]. Moreover, given\u03bc s and\u03bc t , T may be determined through the following minimization problem [18],\nT * = argmin T # \u03bc s =\u03bc t i c(x s i , T (x s i )),(2)\nwhere c : R d \u00d7 R t \u2192 R is a cost functional. This problem is known as Monge formulation of optimal transport. A couple of technical difficulties arise with this definition:\n\n(1) Equation 2 may not have solutions for empirical distributions [18]. (2) Equation 2 is not convex. The issue (1) is particularly problematic for machine learning, since \u03bc s and \u03bc t are not known a priori. In this case, one only has access to empirical distributions\u03bc s and \u03bc t . Moreover, (2) is problematic from an optimization perspective. These difficulties can be overcame with the socalled Kantorovich relaxation [26]. This is done by reformulating Equation 2 in terms of a coupling \u03b3,\n\u03b3 * = argmin \u03b3\u2208\u03a0(\u03bc s ,\u03bc t ) i,j c(x s i , x t j )\u03b3 ij = C, \u03b3 F ,(3)\nwhere \u03a0(\u03bc s ,\u03bc t ) is the set of all couplings between\u03bc s and \u03bc t , \u00b7, \u00b7 F is the Frobenius inner product of matrices, and\nC ij = c(x s i , x t j )\nis the cost matrix. The Kantorovich formulation has nicer properties than Monge's formulation. First, Equation 3 has at least one solution, namely, the trivial coupling\n\u03b3 ij = \u03bc s (x s i )\u03bc t (x t j ). Second, it is a convex problem on \u03b3. Indeed, Equation 3\nis a linear program on the matrix elements \u03b3 ij .\n\nNonetheless, the linear programming approach for solving optimal transport problems is costly. By noticing that Equation 3 is a linear program of n s \u00d7 n t variables, it scales poorly as the number of samples on each domain grows. To solve this drawback, [5] proposed an alternative optimization problem,\n\u03b3 * = argmin \u03b3\u2208\u03a0(\u03bc s ,\u03bc t ) C, \u03b3 F \u2212 H(\u03b3),(4)\nwhere H(\u03b3) is the entropy of matrix \u03b3. This is an approximation to the original optimization problem, but has the desirable property of linear convergence and relies on matrixvector operations. Moreover, the theory of optimal transportation presents a second contribution to machine learning.\nLet C ij = ||x s i \u2212 x t j || p p , Equation 3\nfurther defines the so-called Wasserstein distance,\nW p p (\u03bc s , \u03bc t ) = minimize \u03b3\u2208\u03a0(\u03bc s ,\u03bc t ) i,j C ij \u03b3 ij ,(5)\nwhich is a distance over the space of probability distributions with finite moments of order p, P p (X ). This space is called a Wasserstein space [26]. Common values considered for p are 1, and 2, thus using the 1 and 2 norms as the cost c(x s , x t ). These cases result in the Wasserstein distances W 1 and W 2 , respectively. In addition, analogously to Equations 3 and 5, Equation 4 is associated with a distance called Sinkhorn distance [5],\nS(\u03bc s ,\u03bc t ) = minimize \u03b3\u2208\u03a0(\u03bc s ,\u03bc t ) C, \u03b3 F \u2212 H(\u03b3).\nAs [4] remarks, once \u03b3 * has been estimated, the source domain samples may be transported to the target domain by following the geodesics defined either by the Wasserstein or the Sinkhorn distances. This corresponds to an optimization problem for each source sample,\nx s i = argmin x\u2208X s j \u03b3 ij c(x, x t j ),\nThis problem has a closed form for the 2 -cost, called Barycentric mapping [4]. In this case, the transported source samples is given byX s = n s \u03b3X t .\n\n\nClass-based Regularization\n\nIn the context of the application of optimal transport for domain adaptation, it has been verified in practice that using class-based regularization additionally to entropic regularization yields better performance [4]. This has been also supported theoretically by [20]. As follows, we describe a class regularizer that is relevant to our work. It corresponds to the Laplacian regularizer, presented by [4].\n\nThe Laplacian regularizer is based on a similarity matrix S s (i, j) \u2208 R n s \u00d7n s between samples x s i and x s j . In practice, we proceed as [4], and consider S s (i, j) as the adjacency matrix of a nearest neighbors graph, that is,\nS s (i, j) = 1 if x s i is in the k nearest neighbors of x s j 0 otherwise ,\nmoreover, the condition S s (i, j) = 0 for i, j with y s i = y s j is enforced for class-sparsity. The penalty can be calculated using the following formula,\n\u03a9 c (\u03b3) = 1 n 2 s i,j S s (i, j)||x s i \u2212x s j || 2 2 . This equation is equivalent to \u03a9 c (\u03b3) = T r(X T t \u03b3 T L s \u03b3X t ) for L s = diag(S s 1) \u2212 S s ,\nthe Laplacian of the adjacency matrix S s . Finally, repeating the procedure without enforcing class-sparsity for S t yields the following penalty term,\n\u03a9 c (\u03b3) = (1 \u2212 \u03b1)T r(X T t \u03b3 T L s \u03b3X t ) + \u03b1T r(X T s \u03b3L t \u03b3 T X s ).(6)\nNotice that this penalty term penalizes samples that are close to each other in the source domain from being transported to distant points in the target domain, whenever these belong to the same class. The same reasoning can be applied to the inverse transport using S t .\n\nWith the theory presented in the previous two sections, we may state the general OTDA framework for singlesource domain adaptation, which consists in the minimization problem posed by,\n\u03b3 * = argmin \u03b3\u2208\u03a0(\u03bc s ,\u03bc t ) C, \u03b3 F \u2212 H(\u03b3) + \u03b7\u03a9 c (\u03b3),(7)\nwe will moreover denote by S c to its associated distance between distributions, in the same spirit of the Wasserstein and Sinkhorn distances.\n\n\nBarycenters on Wasserstein Spaces\n\nSince the p-Wasserstein distance metrizes the space P p (X ), it allows for the extension of geometric concepts such as the notion of barycenter. In this context, the Wasserstein barycenter was first formally defined by [1] as,\nDefinition 1 Given {\u03bc s k } N k=1 , with\u03bc s k \u2208 P p (X )\n, \u2200k, and given positive constants\n{\u03bb k } N k=1 such that k \u03bb k = 1, the barycenter of {\u03bc s k } N k=1 , with weights {\u03bb k } N k=1 is denoted by\u03bc b ,\n\nand is the solution of,\n\u03bc b = argmin \u03bc\u2208P p (X ) N k=1 \u03bb k W p p (\u03bc s k ,\u03bc b )..\nThe problem of calculating the Wasserstein barycenter of empirical measures {\u03bc s k } N k=1 was first considered by [6], and was applied in two contexts: (1) Centroid of histograms for the visualization of perturbed images, and (2) Clustering with uniform centroids. Despite their relevance for data analysis, these applications are not related to supervised learning.\n\nAn important distinction has been made by [6] about the calculation of the Wasserstein barycenter on empirical measures. One has two different optimization problems, namely the fixed-support and free-support barycenters. The first corresponds to fixing the support X b of\u03bc b , and solving Equation 8 for the sample weights \u03c9 b . The second corresponds to fixing \u03c9 b (which is considered uniform, or found by the aforementioned method) and optimizing Equation 8 with respect to the support X b .\n\n\nWasserstein Barycenter Transport\n\nThe goal of WBT algorithm is to aggregate all source domains in a intermediate domain, which will later be transported to the target domain. This aggregation procedure is done through the Wasserstein barycenter.\n\nFor the calculation of the barycenter, two assumptions are made: (1)\u03bc b is supported on n b = N k=1 n s k points, and (2) the weights \u03c9 b are fixed and uniform, equal to n \u22121 b . The first assumption seeks to ensure that each source data point is represented on the barycenter domain, and the second assumption is the standard for applying optimal transport for domain adaptation.\n\nNote that since one has a point in the barycenter domain for each point in the source domains, we may as well artificially label the support of X b , with the concatenation of vectors {y s k } N k=1 , as follows,\ny b j = y s k i , for j = k \u00d7 i.(8)\nThis step is crucial for the success of domain adaptation, since the transportation sources \u2192 barycenter \u2192 target should preserve the class structure of the data. Moreover, since the weights are fixed, only the free-support barycenter optimization problem needs to be solved.\n\nTo preserve the class structure across different domains, a class-based penalty inspired on [4] is used. This penalty term is defined as follows,\n\u0393 ij (y s , y b ) = L if y s i = y b j 0 if y s i = y b j , \u03a9 cl (\u03b3; y s , y b ) = i,j \u0393 ij \u03b3 ij .(9)\nwhere y b denotes the labels artificially assigned to barycenter's points, and L max i,j C ij is a hyperparameter. This penalty has been originally proposed for semi-supervised optimal transport, where \u03a9 cl was computed using source and target labels. We remark the context of WBT, no information about the target labels is needed since Equation 9 is computed with respect to the barycenter labels.\n\nThe penalty from Equation 9 is added to the barycenter calculation cost function, leading to Equation 10 below. This can be interpreted as considering points from different classes as far apart in the feature space. Hence, barycenter calculation is done through the following minimization problem,\nS cl (\u03bc s k ,\u03bc b ) = minimize \u03b3\u2208\u03a0(\u03bc s ,\u03bc t ) C, \u03b3 F \u2212 H(\u03b3) + \u03a9 cl (\u03b3; y s , y b ),(10)\u03bc b = argmin \u03bc\u2208P p (X ) N k=1 \u03bb k S cl (\u03bc s k ,\u03bc b ).\nIn this context, we propose a method for solving the MSDA problem by transporting the Wassertein barycenter of sources to the target domain. This corresponds to the following optimization problem:\n\u03bc * = minimize \u03bc\u2208P p (X ) N k=1 \u03bb k S cl (\u03bc s k , \u03bc) + S c (\u03bc t , \u03bc).(11)\nThis minimization problem corresponds to two independent steps: (1) the calculation of the Wasserstein Barycenter, corresponding to minimizing the summation in the right hand side of Equation 11, and (2) the transportation of\u03bc b into\u03bc t , corresponding to minimizing S c (\u03bc t ,\u03bc b ). These two steps are summarized in Algorithm 1, which is an adapted version of Algorithm 2 of [6].\n\n\nAlgorithm 1 Wassertein Barycenter Transport\nRequire: Source domains samples D s k = {(x s k i , y s k i )} n s k i=1 , tar- get domain samples D t = {x t i } n t i=1\n, initial barycenter support X 0 b , barycenter labels y b and weights {\u03bb k } N k=1 while X b has not converged do for k = 1, \u00b7 \u00b7 \u00b7 N do \u03b3 k \u2190 Solution of Eq. 10. end for X b \u2190 n b N k=1 \u03bb k \u03b3 T k X s k end while \u03b3 \u2190 Solution of Eq. 4 between\u03bc b as source and\u03bc t as target. Xs \u2190 n s \u03b3X t Ensure: Transported source samplesX s .\n\n\nTheoretical Guarantees\n\nIn this Section we provide the theoretical insight for justifying our approach. Following [2], this corresponds to proving that our procedure minimizes a target error bound. We begin by defining the notion of error of a hypothesis h, on a domain D, Definition 2 (Due to [2]) The error of a hypothesis function h \u2208 H with respect to a labeling function f on a domain D is given by,\nD (h, f ) = E x\u223cD [|h(x) \u2212 f (x)|].(12)\nGiven a domain equipped with a ground labeling function (D s , f s ), we will adopt for now on the abbreviation s (h) = D s (h, f s ). The notion of error can be extended naturally to the case of many source domains. Given a vector \u03b1 \u2208 R N , such that N j=1 \u03b1 j = 1, the \u03b1\u2212weighted error of h is given by,\n\u03b1 (h) = N j=1 \u03b1 j s j (h).\nNotice that Equation 12 can be approximated by substituting the expectation operator by an empirical mean. In this case, we will denote the empirical error functional as,\n\u03b1 (h) = N j=1 \u03b1 j n j n s j i=1 |h(x i ) \u2212 f j (x i )|.\nMoreover, for the purposes of the next theorem, we will suppose that n s j = \u03b2 j n, for N j=1 \u03b2 j = 1. In these conditions, we may re-state Theorem 4 of [20], Theorem 1 Let X s j , j \u2208 {1, \u00b7 \u00b7 \u00b7 , N} and X t be N + 1 samples of size n s j and n t drawn i.i.d. from \u03bc s j and \u03bc t respectively. Let\u03bc s j and\u03bc s t be the respective empirical measures. If\u0125 \u03b1 is the empirical minimizer of\u02c6 \u03b1 and h * t = minimize h\u2208H t (h), then for any fixed \u03b1 and \u03b4 \u2208 (0, 1), with probability at least 1 \u2212 \u03b4 (over the choice of samples),\nt (\u0125 \u03b1 ) \u2264 t (h * T ) + c 1 + 2 N j=1 \u03b1 j (W 1 (\u03bc s j ,\u03bc t ) + \u03bb j + c 2 ),(13)\nwhere,\nc 1 = 2 2K N j=1 \u03b1 2 j \u03b2 j log(2/\u03b4) n + 2 N j=1 K\u03b1 j \u03b2 j , c 2 = 2 log(1/\u03b4)/\u03be 1 n s j + 1 n t , \u03bb j = minimize h\u2208H s j (h) + t (h).\nThe proof is outlined in [20], and follows the same principle of Theorem 4 of [2]. For completeness, it is presented on the supplementary material. In the discussion of the last theorem, [20] proves that minimizing the Wasserstein distance on the right-hand-side of Equation 13 is equivalent to the following optimization problem,\n\u03bc = minimiz\u00ea \u03bc\u2208P p (X ) 1 N N j=1 \u03b1 j W 1 (\u03bc s j ,\u03bc) + W 1 (\u03bc,\u03bc t ),\nwhich is the unregularized version of Equation 11. Moreover, it is important to mention that, as argued by [20], the minimization of the Wasserstein distance is not sufficient for an adaptation that improves target error. Indeed, \u03bb j plays an important role in bound 13, and class-based regularization is important for controlling it.\n\nIn a nutshell, if points with different classes x b i and x b j on the barycenter are transported to the same target x t k , there is an increase in the joint error. The same reasoning can be applied for the transport of sources to the barycenter. Hence, class-based regularization yields better results in the context of domain adaptation, as it has been verified in practice [4].\n\n\nExperiments and Discussion\n\n\nDatasets\n\nTo establish a comparison between the selected algorithms, four domain adaptation tasks were chosen: Music Genre Recognition (MGR), Music-Speech Discrimination (MSD), object and face recognition. These datasets were chosen based on their relevance in the transfer learning literature. For instance, MSD was previously considered in the work of Turrisi, et al. [24], face recognition was explored in [4], and object recognition is one of the most used benchmarks in domain adaptation. Below each dataset is described, and a summary of each task is shown in Table 1. Object Recognition: For the object recognition task we use the Office-Caltech dataset, which is constituted by the Office dataset [22] and the Caltech-256 dataset [12]. The first dataset has three domains: Amazon, dslr and webcam. Each of these domains present different resolution and acquisition conditions. The second dataset is composed by images of various categories downloaded from Google and Picsearch. In the context of Office-Caltech dataset, these two datasets are merged by considering only the classes they have in common.\n\nThe resulting dataset has 2533 samples, for which De-CAF [8] features were extracted. These features correspond to the 7th layer activation of a convolutional neural network trained on imageNet, and then fine-tuned for object recognition [4]. Face Recognition: For this task, the CMU Pose, Illumination, Expression (PIE) dataset was used [23], which consists of over 40,000 images of size 32 \u00d7 32 from 68 individuals. The face recognition task corresponds to identifying the individual on each one of the images. From the total number of images, four different cameras were selected for the adaptation task: c05 (left pose), c07 (upward pose), c09 (downward pose) and c29 (right pose). Each camera is considered as an individual domain, and is denoted as PIEX, where X corresponds to the camera number. Single vs Multi-source Domain Adaptation: The visual adaptation datasets were previously considered in [4], in the context of single-source domain adaptation. Both of these datasets have four domains, resulting in 12 singlesource domain adaptation experiments. Each experiment corresponds to each possible pair source vs. target between distinct domains. When considered in the MSDA context, four experiments are possible, choosing one domain as target, and leaving all others as sources.\n\n\nAcoustic Adaptation\n\nFor the acoustic adaptation experiments, 4 algorithms are compared with WBT: (1) Kernel Mean Matching (KMM) [11], which consists on a kernel-based importance estimation technique, (2) Transfer Component Analysis (TCA) [16], which is an algorithm that learns a subspace through the minimization of the Maximum Mean Discrepancy (MMD) metric, (3) Optimal Transport [4] through the Sinkhorn algorithm, with and without the additional Laplacian regularization (Equation 6). These are denoted, respectively, as OT-Laplace and OT-IT. (4) JCPOT [19], with its variant JCPOT-LP, which corresponds to the use of label propagation [19].\n\nRegarding the results in Table 2, we remark that two methods have very low performance with comparison to the others: KMM, and the non-regularized version of WBT. In Task Table 3. Results for object and face recognition. The organization and notation of this table is similar to that of Table 2. the first case, a major assumption made for the importance estimation procedure is that the support of P t (X) is contained in the support of P s (X). This is not necessarily true, since the type of noise can create a very different signal with respect to the extracted features. Moreover, notice that WBT reg is the best performing algorithm among the tested methods, improving the baseline by 41.91% on average for MGR, and by 27.03% for MSD. For MGR, when compared to the second best method (OT-Laplace), it presents an average improvement of 19.82%. Moreover, it has even improved the target-only case, where one assumes that a classifier is trained and evaluated only on labeled target data. The average improvement in accuracy is 14.64%. The same consideration is valid for MSD, but in this case the second best was TCA, with a performance gap of 5.9%. When compared to the target-only case, WBT reg has a performance gain of 3.99%. This evidence the fact that the source domains carry information that may improve classification.\n\n\nVisual Adaptation\n\nFor the visual adaptation experiments, 6 algorithms are compared with WBT: (1) Principal Component Analysis (PCA), which consists on projecting the whole dataset (sources and target) on a few principal components [4], (2) TCA, (3) OT-IT and OT-Laplace, (4) JCPOT and JCPOT-LP, and (5) WJDOT [24]. For WJDOT, since the code was not publicly available, its results are only shown for the object recognition task, for which the results reported in [24] are shown in Table 3.\n\nObject Recognition: the results for the Caltec-Office dataset are shown on the left side of Table 2. For this task, WJDOT is the method with higher accuraccy across the different strategies tested. Our approach is the best on average, with a tight performance advantage of 0.18% with respect to WJDOT. Especially, when comparing these two methods, for this specific task WJDOT uses information about only one source domain, as reported in [24] and shown in their supplementary material. In contrast, WBT and WBT reg use information about all source domains equally (uniform weights). As it turns out, our approach manages to acquire a higher performance on webcam and caltech domains, indicating that the information carried by other domains is still useful in the context of the barycenter calculation.\n\nWhen considered against other optimal transport methods, WBT reg shows a considerable higher performance. For instance, by merging all source domains into a single domain, and using the standard OTDA framework yields at best an accuracy of 74.09% (OT-Laplace), 17.71% lower than WBT reg 's performance. This highlights the importance of distinguishing across domains when performing the transport. Moreover, when compared to JCPOT, its performance is 7.81% lower than ours. Additionally, it is noteworthy that even though JCPOT is designed for MSDA, it is used here outside its hypothesis (target shift, as discussed in [19]), justifying its lower performance.\n\nFace Recognition: the results concerning the PIE dataset are shown on the right side of Table 3.\n\nFor this task, WBT reg is the best performing method, with an improvement of 6.3% on the average across targets with respect to the second best, JCPOT.\n\nSince the features used for the transport in this example are raw images, it allows a concrete visualization of the Wasserstein barycenter. Figure 1 shows a comparative between the various domains involved in the adaptation of the PIE dataset. In this example, the camera 29, corresponding to the right pose, was used as target domain. It is visible that the intermediate domain constructed by the barycenter (second row) consists mostly on frontal poses, while the transported samples (third row) display right poses, as expected.\n\nAdditionally, PCA and TCA show poor performances when compared to the other methods. As can be seen in Figure 1, the projections generated by these two methods is a constant image. This is a clear contrast with other adaptation tasks which used extracted features rather than raw signals, indicating that in the latter case, there may not exist a sub-space that the images share common characteristics.\n\n\nGeneral Remarks\n\nTwo remarks can be made about Table 2 and 3 simultaneously. First, the performance of WBT, without classbased regularization, is considerably lower than that of the other methods. The reason for this drop in performance was briefly discussed in Section 3.5.\n\nWhen there is no class-based regularizer in Equation 10, the WBT algorithm manages to minimize only the Sinkhorn distance between sources and the barycenter. However, \u03bb j remains unbounded as the support points are free to move. In the worst case, it may happens that points from different classes are moved to the same point in the barycenter, corresponding to a situation where \u03bb j grows. Our experiments agrees with the previous literature [4] [20], in the sense that using class-based regularization greatly improves our results. A second remark may be made according the field of application. Specially, WBT reg manages to improve the target-only case for acoustic adaptation, while for for visual adaptation this is not always the case. We remark that between these two tasks, there is a significant gap in complexity on the optimization problem posed by Equation 11, since there are 56 features for acoustic adaptation, 4096 for object recognition, and 1024 for facial recognition.\n\n\nConclusion\n\nWe propose a novel approach for MSDA using barycenters of Wasserstein spaces, the WBT algorithm. Our method fits into the OTDA framework, in the sense that it does domain adaptation through the transport of samples from sources to target. To do so, we build an intermediate domain through the Wasserstein barycenter, then transport the barycenter into the target domain.\n\nThe usage of the intermediate domain built by the Wasserstein barycenter has shown state-of-the-art performance in both acoustic and visual adaptation tasks. When the number of features is small, our method manages to largely improve the most optimistic case, when a classifier is trained and evaluated on the target domain, indicating that the information carried by source domains is useful. When the number of features on the task is high, our method still ranks among the best, but its results are less outstanding.\n\nMoreover, our results support the claim that class-based regularization is important for a successful application of optimal transport for domain adaptation. Especially, whenever this kind of regularizer was omitted during the barycenter calculation, the performance dropped significantly.\n\nFinally, we remark that our approach can be further generalized, supporting any kind of single-source domain adaptation algorithm for the transport barycenter \u2192 target.\n\nFigure 1 .\n1Facial adaptation results example (best seen on screen). Each row corresponds to an individual domain. Especially, the first row shows samples from the three source domains (upward, left and downward poses), while the last row shows the target domain (right pose). In between, the domains generated by various various algorithms is shown.\n\n\nTable 1. Summary for each task considered in the MSDA scenario.Task \nDomains \n# Samples # Features # Classes \n\nMusic Genre Recognition and \nMusic-Speech Discrimination \n\nClean \n1000 \n56 \n10/2 \nF16 \n1000 \n56 \n10/2 \nBuccaneer2 \n1000 \n56 \n10/2 \nDestroyerengine \n1000 \n56 \n10/2 \nFactory2 \n1000 \n56 \n10/2 \n\nObject Recognition \n\nCaltech \n1123 \n4096 \n10 \nWebcam \n295 \n4096 \n10 \nAmazon \n958 \n4096 \n10 \nDSLR \n157 \n4096 \n10 \n\nFacial Recognition \n\nPIE05 \n3332 \n1024 \n68 \nPIE07 \n1629 \n1024 \n68 \nPIE09 \n1632 \n1024 \n68 \nPIE29 \n1632 \n1024 \n68 \n\nMGR and MSD: The original MGR dataset [25] is a clas-\nsification dataset, consisting music samples of 10 distinct \nmusic genres (blues, classical, country, disco, hiphop, jazz, \nmetal, pop, reggae, and rock), having 100 samples each. \nThe MSD dataset [10], on the other hand, is composed by \n64 music samples and 64 speech samples. No distinction \nupon the music genre is made. In both datasets, each sam-\nple is a .wav file with 30 seconds of duration. To simulate a \ndomain adaptation scenario, each audio sample is overlaid \nwith a specific noise, chosen from a noise dataset 1 using the \nPydub library [21]. A total of 56 features were extracted \non both datasets using the Librosa library [9], including the \nmean and variance of chromagram, root mean squared error, \nspectral bandwidth and rolloff, zero crossing rate, harmonic \n\n1 http://spib.linse.ufsc.br/noise.html \n\nand percussive components, tempo and 20 Mel-Frequency \nCepstral Coefficient (MFCC). \n\n\nBarycenters in the wasserstein space. Martial Agueh, Guillaume Carlier, SIAM Journal on Mathematical Analysis. 432Martial Agueh and Guillaume Carlier. Barycenters in the wasserstein space. SIAM Journal on Mathematical Analysis, 43(2):904-924, 2011.\n\nA theory of learning from different domains. Machine learning. Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, Jennifer Wortman Vaughan, 79Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A theory of learning from different domains. Machine learn- ing, 79(1-2):151-175, 2010.\n\nAnalysis of representations for domain adaptation. Shai Ben-David, John Blitzer, Koby Crammer, Fernando Pereira, Advances in neural information processing systems. Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations for domain adaptation. In Advances in neural information processing systems, pages 137-144, 2007.\n\nDevis Tuia, and Alain Rakotomamonjy. Optimal transport for domain adaptation. Nicolas Courty, R\u00e9mi Flamary, IEEE transactions on pattern analysis and machine intelligence. 39Nicolas Courty, R\u00e9mi Flamary, Devis Tuia, and Alain Rako- tomamonjy. Optimal transport for domain adaptation. IEEE transactions on pattern analysis and machine intelligence, 39(9):1853-1865, 2016.\n\nSinkhorn distances: Lightspeed computation of optimal transport. Marco Cuturi, Advances in neural information processing systems. Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in neural information pro- cessing systems, pages 2292-2300, 2013.\n\nFast computation of wasserstein barycenters. Marco Cuturi, Arnaud Doucet, PMLRProceedings of the 31st International Conference on Machine Learning. Eric P. Xing and Tony Jebarathe 31st International Conference on Machine LearningBejing, China32Marco Cuturi and Arnaud Doucet. Fast computation of wasserstein barycenters. In Eric P. Xing and Tony Jebara, editors, Proceedings of the 31st International Conference on Machine Learning, volume 32 of Proceedings of Machine Learning Research, pages 685-693, Bejing, China, 22-24 Jun 2014. PMLR.\n\nImpossibility theorems for domain adaptation. Tyler Shai Ben David, Teresa Lu, D\u00e1vid Luu, P\u00e1l, Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics. the Thirteenth International Conference on Artificial Intelligence and StatisticsShai Ben David, Tyler Lu, Teresa Luu, and D\u00e1vid P\u00e1l. Im- possibility theorems for domain adaptation. In Proceedings of the Thirteenth International Conference on Artificial In- telligence and Statistics, pages 129-136, 2010.\n\nDecaf: A deep convolutional activation feature for generic visual recognition. Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, Trevor Darrell, International conference on machine learning. Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell. Decaf: A deep convolutional activation feature for generic visual recogni- tion. In International conference on machine learning, pages 647-655, 2014.\n\n. Brian Mcfee, librosa/librosa: 0.8.0Brian McFee et al. librosa/librosa: 0.8.0, July 2020.\n\nAutomatic musical genre classification of audio signals. Tzanetakis George, Essl Georg, Cook Perry, Proceedings of the 2nd international symposium on music information retrieval. the 2nd international symposium on music information retrievalIndianaTzanetakis George, Essl Georg, and Cook Perry. Automatic musical genre classification of audio signals. In Proceedings of the 2nd international symposium on music information re- trieval, Indiana, 2001.\n\nCovariate shift by kernel mean matching. Dataset shift in machine learning. Arthur Gretton, Alex Smola, Jiayuan Huang, Marcel Schmittfull, Karsten Borgwardt, Bernhard Sch\u00f6lkopf, 35Arthur Gretton, Alex Smola, Jiayuan Huang, Marcel Schmit- tfull, Karsten Borgwardt, and Bernhard Sch\u00f6lkopf. Covari- ate shift by kernel mean matching. Dataset shift in machine learning, 3(4):5, 2009.\n\nCaltech-256 object category dataset. Gregory Griffin, Alex Holub, Pietro Perona, California Institute of TechnologyTechnical reportGregory Griffin, Alex Holub, and Pietro Perona. Caltech-256 object category dataset. Technical report, California Institute of Technology, 2007.\n\nDetecting change in data streams. Daniel Kifer, Shai Ben-David, Johannes Gehrke, VLDB. Toronto, Canada4Daniel Kifer, Shai Ben-David, and Johannes Gehrke. De- tecting change in data streams. In VLDB, volume 4, pages 180-191. Toronto, Canada, 2004.\n\nSinkhorn barycenters with free support via frankwolfe algorithm. Giulia Luise, Saverio Salzo, Massimiliano Pontil, Carlo Ciliberto, Advances in Neural Information Processing Systems. Giulia Luise, Saverio Salzo, Massimiliano Pontil, and Carlo Ciliberto. Sinkhorn barycenters with free support via frank- wolfe algorithm. In Advances in Neural Information Pro- cessing Systems, pages 9322-9333, 2019.\n\nA multi-device dataset for urban acoustic scene classification. Annamaria Mesaros, Toni Heittola, Tuomas Virtanen, Scenes and Events 2018 Workshop (DCASE2018). 9Annamaria Mesaros, Toni Heittola, and Tuomas Virtanen. A multi-device dataset for urban acoustic scene classification. In Scenes and Events 2018 Workshop (DCASE2018), page 9, 2018.\n\nDomain adaptation via transfer component analysis. Ivor W Sinno Jialin Pan, James T Tsang, Qiang Kwok, Yang, IEEE Transactions on Neural Networks. 222Sinno Jialin Pan, Ivor W Tsang, James T Kwok, and Qiang Yang. Domain adaptation via transfer component analy- sis. IEEE Transactions on Neural Networks, 22(2):199-210, 2010.\n\nA survey on transfer learning. Qiang Sinno Jialin Pan, Yang, IEEE Transactions on knowledge and data engineering. 2210Sinno Jialin Pan and Qiang Yang. A survey on transfer learn- ing. IEEE Transactions on knowledge and data engineering, 22(10):1345-1359, 2009.\n\nComputational optimal transport. Foundations and Trends R in Machine Learning. Gabriel Peyr\u00e9, Marco Cuturi, 11Gabriel Peyr\u00e9, Marco Cuturi, et al. Computational optimal transport. Foundations and Trends R in Machine Learning, 11(5-6):355-607, 2019.\n\nOptimal transport for multi-source domain adaptation under target shift. Ievgen Redko, Nicolas Courty, R\u00e9mi Flamary, Devis Tuia, PMLRProceedings of Machine Learning Research. Kamalika Chaudhuri and Masashi SugiyamaMachine Learning Research89Ievgen Redko, Nicolas Courty, R\u00e9mi Flamary, and Devis Tuia. Optimal transport for multi-source domain adapta- tion under target shift. In Kamalika Chaudhuri and Masashi Sugiyama, editors, Proceedings of Machine Learning Re- search, volume 89 of Proceedings of Machine Learning Re- search, pages 849-858. PMLR, 16-18 Apr 2019.\n\nTheoretical analysis of domain adaptation with optimal transport. Ievgen Redko, Amaury Habrard, Marc Sebban, Joint European Conference on Machine Learning and Knowledge Discovery in Databases. SpringerIevgen Redko, Amaury Habrard, and Marc Sebban. The- oretical analysis of domain adaptation with optimal trans- port. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 737-753. Springer, 2017.\n\n. James Robert, Marc Webbie, James Robert, Marc Webbie, et al. Pydub. Github, 2011.\n\nAdapting visual category models to new domains. Kate Saenko, Brian Kulis, Mario Fritz, Trevor Darrell, European conference on computer vision. SpringerKate Saenko, Brian Kulis, Mario Fritz, and Trevor Dar- rell. Adapting visual category models to new domains. In European conference on computer vision, pages 213-226. Springer, 2010.\n\nThe cmu pose, illumination and expression database of human faces. Terence Sim, Simon Baker, Maan Bsat, Terence Sim, Simon Baker, and Maan Bsat. The cmu pose, illumination and expression database of human faces.\n\nMulti-source domain adaptation via weighted joint distributions optimal transport. Rosanna Turrisi, R\u00e9mi Flamary, Alain Rakotomamonjy, Massimiliano Pontil, arXiv:2006.12938arXiv preprintRosanna Turrisi, R\u00e9mi Flamary, Alain Rakotomamonjy, and Massimiliano Pontil. Multi-source domain adaptation via weighted joint distributions optimal transport. arXiv preprint arXiv:2006.12938, 2020.\n\nMusical genre classification of audio signals. George Tzanetakis, Perry Cook, IEEE Transactions on speech and audio processing. 105George Tzanetakis and Perry Cook. Musical genre classifi- cation of audio signals. IEEE Transactions on speech and audio processing, 10(5):293-302, 2002.\n\nOptimal transport: old and new. C\u00e9dric Villani, Springer Science & Business Media338C\u00e9dric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media, 2008.\n\nDeep visual domain adaptation: A survey. Mei Wang, Weihong Deng, Neurocomputing. 312Mei Wang and Weihong Deng. Deep visual domain adapta- tion: A survey. Neurocomputing, 312:135-153, 2018.\n", "annotations": {"author": "[{\"end\":165,\"start\":61},{\"end\":245,\"start\":166},{\"end\":325,\"start\":246}]", "publisher": null, "author_last_name": "[{\"end\":88,\"start\":69},{\"end\":178,\"start\":171},{\"end\":258,\"start\":252}]", "author_first_name": "[{\"end\":68,\"start\":61},{\"end\":170,\"start\":166},{\"end\":251,\"start\":246}]", "author_affiliation": "[{\"end\":164,\"start\":118},{\"end\":244,\"start\":180},{\"end\":324,\"start\":260}]", "title": "[{\"end\":58,\"start\":1},{\"end\":383,\"start\":326}]", "venue": null, "abstract": "[{\"end\":1433,\"start\":413}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b21\"},\"end\":1939,\"start\":1935},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":1948,\"start\":1944},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1970,\"start\":1967},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1974,\"start\":1971},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":1991,\"start\":1987},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":1996,\"start\":1992},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2319,\"start\":2315},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2747,\"start\":2743},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2965,\"start\":2962},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3255,\"start\":3252},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3435,\"start\":3431},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4107,\"start\":4104},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5873,\"start\":5869},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5947,\"start\":5943},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6062,\"start\":6059},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6067,\"start\":6064},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6075,\"start\":6072},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6313,\"start\":6310},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6503,\"start\":6499},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6792,\"start\":6788},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6855,\"start\":6851},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7856,\"start\":7853},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8023,\"start\":8020},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8118,\"start\":8115},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8176,\"start\":8172},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8214,\"start\":8211},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8694,\"start\":8690},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9295,\"start\":9292},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9639,\"start\":9635},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10175,\"start\":10172},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10524,\"start\":10521},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10621,\"start\":10617},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10922,\"start\":10918},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":11277,\"start\":11273},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":12129,\"start\":12126},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":12829,\"start\":12825},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":13124,\"start\":13121},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":13186,\"start\":13183},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":13567,\"start\":13564},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":13890,\"start\":13887},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":13942,\"start\":13938},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":14079,\"start\":14076},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":14228,\"start\":14225},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":15850,\"start\":15847},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":16260,\"start\":16257},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":16556,\"start\":16553},{\"end\":16971,\"start\":16961},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":18258,\"start\":18255},{\"end\":19714,\"start\":19703},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":19900,\"start\":19897},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":20517,\"start\":20514},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":20697,\"start\":20694},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":21562,\"start\":21558},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":22172,\"start\":22168},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":22224,\"start\":22221},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":22334,\"start\":22330},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":22592,\"start\":22590},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":22654,\"start\":22650},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":23259,\"start\":23256},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":23666,\"start\":23662},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":23704,\"start\":23701},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":24001,\"start\":23997},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":24034,\"start\":24030},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":24464,\"start\":24461},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":24645,\"start\":24642},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":24746,\"start\":24742},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":25313,\"start\":25310},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":25831,\"start\":25827},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":25941,\"start\":25937},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":26084,\"start\":26081},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":26260,\"start\":26256},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":26343,\"start\":26339},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":27916,\"start\":27913},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":27995,\"start\":27991},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":28149,\"start\":28145},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":28616,\"start\":28612},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":29602,\"start\":29598},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":31551,\"start\":31548},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":31556,\"start\":31552},{\"end\":31977,\"start\":31966}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":33812,\"start\":33461},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":35307,\"start\":33813}]", "paragraph": "[{\"end\":1823,\"start\":1449},{\"end\":2068,\"start\":1825},{\"end\":2535,\"start\":2070},{\"end\":2926,\"start\":2537},{\"end\":3296,\"start\":2928},{\"end\":3823,\"start\":3298},{\"end\":4205,\"start\":3825},{\"end\":5565,\"start\":4207},{\"end\":5792,\"start\":5582},{\"end\":6181,\"start\":5794},{\"end\":6607,\"start\":6183},{\"end\":7330,\"start\":6609},{\"end\":7765,\"start\":7332},{\"end\":8237,\"start\":7767},{\"end\":8695,\"start\":8259},{\"end\":9343,\"start\":8697},{\"end\":9424,\"start\":9388},{\"end\":9592,\"start\":9439},{\"end\":10075,\"start\":9594},{\"end\":10387,\"start\":10077},{\"end\":10622,\"start\":10423},{\"end\":10850,\"start\":10677},{\"end\":11345,\"start\":10852},{\"end\":11536,\"start\":11414},{\"end\":11730,\"start\":11562},{\"end\":11869,\"start\":11820},{\"end\":12175,\"start\":11871},{\"end\":12514,\"start\":12222},{\"end\":12613,\"start\":12562},{\"end\":13125,\"start\":12678},{\"end\":13446,\"start\":13180},{\"end\":13641,\"start\":13489},{\"end\":14080,\"start\":13672},{\"end\":14316,\"start\":14082},{\"end\":14551,\"start\":14394},{\"end\":14856,\"start\":14704},{\"end\":15203,\"start\":14931},{\"end\":15389,\"start\":15205},{\"end\":15589,\"start\":15447},{\"end\":15854,\"start\":15627},{\"end\":15946,\"start\":15912},{\"end\":16509,\"start\":16142},{\"end\":17005,\"start\":16511},{\"end\":17253,\"start\":17042},{\"end\":17635,\"start\":17255},{\"end\":17849,\"start\":17637},{\"end\":18161,\"start\":17886},{\"end\":18308,\"start\":18163},{\"end\":18809,\"start\":18411},{\"end\":19108,\"start\":18811},{\"end\":19445,\"start\":19249},{\"end\":19901,\"start\":19520},{\"end\":20397,\"start\":20070},{\"end\":20804,\"start\":20424},{\"end\":21150,\"start\":20845},{\"end\":21348,\"start\":21178},{\"end\":21923,\"start\":21405},{\"end\":22010,\"start\":22004},{\"end\":22473,\"start\":22143},{\"end\":22877,\"start\":22543},{\"end\":23260,\"start\":22879},{\"end\":24402,\"start\":23302},{\"end\":25695,\"start\":24404},{\"end\":26344,\"start\":25719},{\"end\":27678,\"start\":26346},{\"end\":28171,\"start\":27700},{\"end\":28976,\"start\":28173},{\"end\":29638,\"start\":28978},{\"end\":29736,\"start\":29640},{\"end\":29889,\"start\":29738},{\"end\":30422,\"start\":29891},{\"end\":30826,\"start\":30424},{\"end\":31103,\"start\":30846},{\"end\":32093,\"start\":31105},{\"end\":32478,\"start\":32108},{\"end\":32999,\"start\":32480},{\"end\":33290,\"start\":33001},{\"end\":33460,\"start\":33292}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9387,\"start\":9344},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10422,\"start\":10388},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10676,\"start\":10623},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11413,\"start\":11346},{\"attributes\":{\"id\":\"formula_4\"},\"end\":11561,\"start\":11537},{\"attributes\":{\"id\":\"formula_5\"},\"end\":11819,\"start\":11731},{\"attributes\":{\"id\":\"formula_6\"},\"end\":12221,\"start\":12176},{\"attributes\":{\"id\":\"formula_7\"},\"end\":12561,\"start\":12515},{\"attributes\":{\"id\":\"formula_8\"},\"end\":12677,\"start\":12614},{\"attributes\":{\"id\":\"formula_9\"},\"end\":13179,\"start\":13126},{\"attributes\":{\"id\":\"formula_10\"},\"end\":13488,\"start\":13447},{\"attributes\":{\"id\":\"formula_11\"},\"end\":14393,\"start\":14317},{\"attributes\":{\"id\":\"formula_12\"},\"end\":14703,\"start\":14552},{\"attributes\":{\"id\":\"formula_13\"},\"end\":14930,\"start\":14857},{\"attributes\":{\"id\":\"formula_14\"},\"end\":15446,\"start\":15390},{\"attributes\":{\"id\":\"formula_15\"},\"end\":15911,\"start\":15855},{\"attributes\":{\"id\":\"formula_16\"},\"end\":16060,\"start\":15947},{\"attributes\":{\"id\":\"formula_17\"},\"end\":16141,\"start\":16086},{\"attributes\":{\"id\":\"formula_18\"},\"end\":17885,\"start\":17850},{\"attributes\":{\"id\":\"formula_19\"},\"end\":18410,\"start\":18309},{\"attributes\":{\"id\":\"formula_20\"},\"end\":19195,\"start\":19109},{\"attributes\":{\"id\":\"formula_21\"},\"end\":19248,\"start\":19195},{\"attributes\":{\"id\":\"formula_22\"},\"end\":19519,\"start\":19446},{\"attributes\":{\"id\":\"formula_23\"},\"end\":20069,\"start\":19948},{\"attributes\":{\"id\":\"formula_24\"},\"end\":20844,\"start\":20805},{\"attributes\":{\"id\":\"formula_25\"},\"end\":21177,\"start\":21151},{\"attributes\":{\"id\":\"formula_26\"},\"end\":21404,\"start\":21349},{\"attributes\":{\"id\":\"formula_27\"},\"end\":22003,\"start\":21924},{\"attributes\":{\"id\":\"formula_28\"},\"end\":22142,\"start\":22011},{\"attributes\":{\"id\":\"formula_29\"},\"end\":22542,\"start\":22474}]", "table_ref": "[{\"end\":23865,\"start\":23858},{\"end\":26378,\"start\":26371},{\"end\":26516,\"start\":26512},{\"end\":26524,\"start\":26517},{\"end\":26641,\"start\":26633},{\"end\":28170,\"start\":28163},{\"end\":28272,\"start\":28265},{\"end\":29735,\"start\":29728},{\"end\":30883,\"start\":30876}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1447,\"start\":1435},{\"attributes\":{\"n\":\"2.\"},\"end\":5580,\"start\":5568},{\"attributes\":{\"n\":\"3.\"},\"end\":8257,\"start\":8240},{\"attributes\":{\"n\":\"3.1.\"},\"end\":9437,\"start\":9427},{\"attributes\":{\"n\":\"3.2.\"},\"end\":13670,\"start\":13644},{\"attributes\":{\"n\":\"3.3.\"},\"end\":15625,\"start\":15592},{\"end\":16085,\"start\":16062},{\"attributes\":{\"n\":\"3.4.\"},\"end\":17040,\"start\":17008},{\"end\":19947,\"start\":19904},{\"attributes\":{\"n\":\"3.5.\"},\"end\":20422,\"start\":20400},{\"attributes\":{\"n\":\"4.\"},\"end\":23289,\"start\":23263},{\"attributes\":{\"n\":\"4.1.\"},\"end\":23300,\"start\":23292},{\"attributes\":{\"n\":\"4.2.\"},\"end\":25717,\"start\":25698},{\"attributes\":{\"n\":\"4.3.\"},\"end\":27698,\"start\":27681},{\"attributes\":{\"n\":\"4.4.\"},\"end\":30844,\"start\":30829},{\"attributes\":{\"n\":\"5.\"},\"end\":32106,\"start\":32096},{\"end\":33472,\"start\":33462}]", "table": "[{\"end\":35307,\"start\":33878}]", "figure_caption": "[{\"end\":33812,\"start\":33474},{\"end\":33878,\"start\":33815}]", "figure_ref": "[{\"end\":26185,\"start\":26174},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":30039,\"start\":30031},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":30535,\"start\":30527}]", "bib_author_first_name": "[{\"end\":35354,\"start\":35347},{\"end\":35371,\"start\":35362},{\"end\":35626,\"start\":35622},{\"end\":35642,\"start\":35638},{\"end\":35656,\"start\":35652},{\"end\":35670,\"start\":35666},{\"end\":35688,\"start\":35680},{\"end\":35706,\"start\":35698},{\"end\":35714,\"start\":35707},{\"end\":35976,\"start\":35972},{\"end\":35992,\"start\":35988},{\"end\":36006,\"start\":36002},{\"end\":36024,\"start\":36016},{\"end\":36363,\"start\":36356},{\"end\":36376,\"start\":36372},{\"end\":36720,\"start\":36715},{\"end\":36989,\"start\":36984},{\"end\":37004,\"start\":36998},{\"end\":37531,\"start\":37526},{\"end\":37554,\"start\":37548},{\"end\":37564,\"start\":37559},{\"end\":38063,\"start\":38059},{\"end\":38081,\"start\":38073},{\"end\":38092,\"start\":38087},{\"end\":38106,\"start\":38102},{\"end\":38120,\"start\":38116},{\"end\":38132,\"start\":38128},{\"end\":38146,\"start\":38140},{\"end\":38462,\"start\":38457},{\"end\":38614,\"start\":38604},{\"end\":38627,\"start\":38623},{\"end\":38639,\"start\":38635},{\"end\":39081,\"start\":39075},{\"end\":39095,\"start\":39091},{\"end\":39110,\"start\":39103},{\"end\":39124,\"start\":39118},{\"end\":39145,\"start\":39138},{\"end\":39165,\"start\":39157},{\"end\":39424,\"start\":39417},{\"end\":39438,\"start\":39434},{\"end\":39452,\"start\":39446},{\"end\":39697,\"start\":39691},{\"end\":39709,\"start\":39705},{\"end\":39729,\"start\":39721},{\"end\":39976,\"start\":39970},{\"end\":39991,\"start\":39984},{\"end\":40011,\"start\":39999},{\"end\":40025,\"start\":40020},{\"end\":40379,\"start\":40370},{\"end\":40393,\"start\":40389},{\"end\":40410,\"start\":40404},{\"end\":40704,\"start\":40700},{\"end\":40706,\"start\":40705},{\"end\":40730,\"start\":40725},{\"end\":40732,\"start\":40731},{\"end\":40745,\"start\":40740},{\"end\":41010,\"start\":41005},{\"end\":41322,\"start\":41315},{\"end\":41335,\"start\":41330},{\"end\":41564,\"start\":41558},{\"end\":41579,\"start\":41572},{\"end\":41592,\"start\":41588},{\"end\":41607,\"start\":41602},{\"end\":42125,\"start\":42119},{\"end\":42139,\"start\":42133},{\"end\":42153,\"start\":42149},{\"end\":42497,\"start\":42492},{\"end\":42510,\"start\":42506},{\"end\":42627,\"start\":42623},{\"end\":42641,\"start\":42636},{\"end\":42654,\"start\":42649},{\"end\":42668,\"start\":42662},{\"end\":42984,\"start\":42977},{\"end\":42995,\"start\":42990},{\"end\":43007,\"start\":43003},{\"end\":43213,\"start\":43206},{\"end\":43227,\"start\":43223},{\"end\":43242,\"start\":43237},{\"end\":43270,\"start\":43258},{\"end\":43562,\"start\":43556},{\"end\":43580,\"start\":43575},{\"end\":43833,\"start\":43827},{\"end\":44025,\"start\":44022},{\"end\":44039,\"start\":44032}]", "bib_author_last_name": "[{\"end\":35360,\"start\":35355},{\"end\":35379,\"start\":35372},{\"end\":35636,\"start\":35627},{\"end\":35650,\"start\":35643},{\"end\":35664,\"start\":35657},{\"end\":35678,\"start\":35671},{\"end\":35696,\"start\":35689},{\"end\":35722,\"start\":35715},{\"end\":35986,\"start\":35977},{\"end\":36000,\"start\":35993},{\"end\":36014,\"start\":36007},{\"end\":36032,\"start\":36025},{\"end\":36370,\"start\":36364},{\"end\":36384,\"start\":36377},{\"end\":36727,\"start\":36721},{\"end\":36996,\"start\":36990},{\"end\":37011,\"start\":37005},{\"end\":37546,\"start\":37532},{\"end\":37557,\"start\":37555},{\"end\":37568,\"start\":37565},{\"end\":37573,\"start\":37570},{\"end\":38071,\"start\":38064},{\"end\":38085,\"start\":38082},{\"end\":38100,\"start\":38093},{\"end\":38114,\"start\":38107},{\"end\":38126,\"start\":38121},{\"end\":38138,\"start\":38133},{\"end\":38154,\"start\":38147},{\"end\":38468,\"start\":38463},{\"end\":38621,\"start\":38615},{\"end\":38633,\"start\":38628},{\"end\":38645,\"start\":38640},{\"end\":39089,\"start\":39082},{\"end\":39101,\"start\":39096},{\"end\":39116,\"start\":39111},{\"end\":39136,\"start\":39125},{\"end\":39155,\"start\":39146},{\"end\":39175,\"start\":39166},{\"end\":39432,\"start\":39425},{\"end\":39444,\"start\":39439},{\"end\":39459,\"start\":39453},{\"end\":39703,\"start\":39698},{\"end\":39719,\"start\":39710},{\"end\":39736,\"start\":39730},{\"end\":39982,\"start\":39977},{\"end\":39997,\"start\":39992},{\"end\":40018,\"start\":40012},{\"end\":40035,\"start\":40026},{\"end\":40387,\"start\":40380},{\"end\":40402,\"start\":40394},{\"end\":40419,\"start\":40411},{\"end\":40723,\"start\":40707},{\"end\":40738,\"start\":40733},{\"end\":40750,\"start\":40746},{\"end\":40756,\"start\":40752},{\"end\":41027,\"start\":41011},{\"end\":41033,\"start\":41029},{\"end\":41328,\"start\":41323},{\"end\":41342,\"start\":41336},{\"end\":41570,\"start\":41565},{\"end\":41586,\"start\":41580},{\"end\":41600,\"start\":41593},{\"end\":41612,\"start\":41608},{\"end\":42131,\"start\":42126},{\"end\":42147,\"start\":42140},{\"end\":42160,\"start\":42154},{\"end\":42504,\"start\":42498},{\"end\":42517,\"start\":42511},{\"end\":42634,\"start\":42628},{\"end\":42647,\"start\":42642},{\"end\":42660,\"start\":42655},{\"end\":42676,\"start\":42669},{\"end\":42988,\"start\":42985},{\"end\":43001,\"start\":42996},{\"end\":43012,\"start\":43008},{\"end\":43221,\"start\":43214},{\"end\":43235,\"start\":43228},{\"end\":43256,\"start\":43243},{\"end\":43277,\"start\":43271},{\"end\":43573,\"start\":43563},{\"end\":43585,\"start\":43581},{\"end\":43841,\"start\":43834},{\"end\":44030,\"start\":44026},{\"end\":44044,\"start\":44040}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":8592977},\"end\":35557,\"start\":35309},{\"attributes\":{\"id\":\"b1\"},\"end\":35919,\"start\":35559},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":10908021},\"end\":36276,\"start\":35921},{\"attributes\":{\"id\":\"b3\"},\"end\":36648,\"start\":36278},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":15966283},\"end\":36937,\"start\":36650},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b5\",\"matched_paper_id\":16786361},\"end\":37478,\"start\":36939},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":7505943},\"end\":37978,\"start\":37480},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":6161478},\"end\":38453,\"start\":37980},{\"attributes\":{\"doi\":\"librosa/librosa: 0.8.0\",\"id\":\"b8\"},\"end\":38545,\"start\":38455},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":14714051},\"end\":38997,\"start\":38547},{\"attributes\":{\"id\":\"b10\"},\"end\":39378,\"start\":38999},{\"attributes\":{\"id\":\"b11\"},\"end\":39655,\"start\":39380},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":52800392},\"end\":39903,\"start\":39657},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":170078579},\"end\":40304,\"start\":39905},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":50785886},\"end\":40647,\"start\":40306},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":788838},\"end\":40972,\"start\":40649},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":740063},\"end\":41234,\"start\":40974},{\"attributes\":{\"id\":\"b17\"},\"end\":41483,\"start\":41236},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b18\",\"matched_paper_id\":88478543},\"end\":42051,\"start\":41485},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":5111106},\"end\":42488,\"start\":42053},{\"attributes\":{\"id\":\"b20\"},\"end\":42573,\"start\":42490},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":7534823},\"end\":42908,\"start\":42575},{\"attributes\":{\"id\":\"b22\"},\"end\":43121,\"start\":42910},{\"attributes\":{\"doi\":\"arXiv:2006.12938\",\"id\":\"b23\"},\"end\":43507,\"start\":43123},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":3238519},\"end\":43793,\"start\":43509},{\"attributes\":{\"id\":\"b25\"},\"end\":43979,\"start\":43795},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":3654323},\"end\":44169,\"start\":43981}]", "bib_title": "[{\"end\":35345,\"start\":35309},{\"end\":35970,\"start\":35921},{\"end\":36354,\"start\":36278},{\"end\":36713,\"start\":36650},{\"end\":36982,\"start\":36939},{\"end\":37524,\"start\":37480},{\"end\":38057,\"start\":37980},{\"end\":38602,\"start\":38547},{\"end\":39689,\"start\":39657},{\"end\":39968,\"start\":39905},{\"end\":40368,\"start\":40306},{\"end\":40698,\"start\":40649},{\"end\":41003,\"start\":40974},{\"end\":41556,\"start\":41485},{\"end\":42117,\"start\":42053},{\"end\":42621,\"start\":42575},{\"end\":43554,\"start\":43509},{\"end\":44020,\"start\":43981}]", "bib_author": "[{\"end\":35362,\"start\":35347},{\"end\":35381,\"start\":35362},{\"end\":35638,\"start\":35622},{\"end\":35652,\"start\":35638},{\"end\":35666,\"start\":35652},{\"end\":35680,\"start\":35666},{\"end\":35698,\"start\":35680},{\"end\":35724,\"start\":35698},{\"end\":35988,\"start\":35972},{\"end\":36002,\"start\":35988},{\"end\":36016,\"start\":36002},{\"end\":36034,\"start\":36016},{\"end\":36372,\"start\":36356},{\"end\":36386,\"start\":36372},{\"end\":36729,\"start\":36715},{\"end\":36998,\"start\":36984},{\"end\":37013,\"start\":36998},{\"end\":37548,\"start\":37526},{\"end\":37559,\"start\":37548},{\"end\":37570,\"start\":37559},{\"end\":37575,\"start\":37570},{\"end\":38073,\"start\":38059},{\"end\":38087,\"start\":38073},{\"end\":38102,\"start\":38087},{\"end\":38116,\"start\":38102},{\"end\":38128,\"start\":38116},{\"end\":38140,\"start\":38128},{\"end\":38156,\"start\":38140},{\"end\":38470,\"start\":38457},{\"end\":38623,\"start\":38604},{\"end\":38635,\"start\":38623},{\"end\":38647,\"start\":38635},{\"end\":39091,\"start\":39075},{\"end\":39103,\"start\":39091},{\"end\":39118,\"start\":39103},{\"end\":39138,\"start\":39118},{\"end\":39157,\"start\":39138},{\"end\":39177,\"start\":39157},{\"end\":39434,\"start\":39417},{\"end\":39446,\"start\":39434},{\"end\":39461,\"start\":39446},{\"end\":39705,\"start\":39691},{\"end\":39721,\"start\":39705},{\"end\":39738,\"start\":39721},{\"end\":39984,\"start\":39970},{\"end\":39999,\"start\":39984},{\"end\":40020,\"start\":39999},{\"end\":40037,\"start\":40020},{\"end\":40389,\"start\":40370},{\"end\":40404,\"start\":40389},{\"end\":40421,\"start\":40404},{\"end\":40725,\"start\":40700},{\"end\":40740,\"start\":40725},{\"end\":40752,\"start\":40740},{\"end\":40758,\"start\":40752},{\"end\":41029,\"start\":41005},{\"end\":41035,\"start\":41029},{\"end\":41330,\"start\":41315},{\"end\":41344,\"start\":41330},{\"end\":41572,\"start\":41558},{\"end\":41588,\"start\":41572},{\"end\":41602,\"start\":41588},{\"end\":41614,\"start\":41602},{\"end\":42133,\"start\":42119},{\"end\":42149,\"start\":42133},{\"end\":42162,\"start\":42149},{\"end\":42506,\"start\":42492},{\"end\":42519,\"start\":42506},{\"end\":42636,\"start\":42623},{\"end\":42649,\"start\":42636},{\"end\":42662,\"start\":42649},{\"end\":42678,\"start\":42662},{\"end\":42990,\"start\":42977},{\"end\":43003,\"start\":42990},{\"end\":43014,\"start\":43003},{\"end\":43223,\"start\":43206},{\"end\":43237,\"start\":43223},{\"end\":43258,\"start\":43237},{\"end\":43279,\"start\":43258},{\"end\":43575,\"start\":43556},{\"end\":43587,\"start\":43575},{\"end\":43843,\"start\":43827},{\"end\":44032,\"start\":44022},{\"end\":44046,\"start\":44032}]", "bib_venue": "[{\"end\":37181,\"start\":37115},{\"end\":37754,\"start\":37673},{\"end\":38795,\"start\":38726},{\"end\":39759,\"start\":39744},{\"end\":41724,\"start\":41699},{\"end\":35418,\"start\":35381},{\"end\":35620,\"start\":35559},{\"end\":36083,\"start\":36034},{\"end\":36448,\"start\":36386},{\"end\":36778,\"start\":36729},{\"end\":37085,\"start\":37017},{\"end\":37671,\"start\":37575},{\"end\":38200,\"start\":38156},{\"end\":38724,\"start\":38647},{\"end\":39073,\"start\":38999},{\"end\":39415,\"start\":39380},{\"end\":39742,\"start\":39738},{\"end\":40086,\"start\":40037},{\"end\":40464,\"start\":40421},{\"end\":40794,\"start\":40758},{\"end\":41086,\"start\":41035},{\"end\":41313,\"start\":41236},{\"end\":41658,\"start\":41618},{\"end\":42244,\"start\":42162},{\"end\":42716,\"start\":42678},{\"end\":42975,\"start\":42910},{\"end\":43204,\"start\":43123},{\"end\":43635,\"start\":43587},{\"end\":43825,\"start\":43795},{\"end\":44060,\"start\":44046}]"}}}, "year": 2023, "month": 12, "day": 17}