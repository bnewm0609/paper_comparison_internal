{"id": 2014883, "updated": "2023-10-05 06:14:22.826", "metadata": {"title": "Counterfactual Fairness", "authors": "[{\"first\":\"Matt\",\"last\":\"Kusner\",\"middle\":[\"J.\"]},{\"first\":\"Joshua\",\"last\":\"Loftus\",\"middle\":[\"R.\"]},{\"first\":\"Chris\",\"last\":\"Russell\",\"middle\":[]},{\"first\":\"Ricardo\",\"last\":\"Silva\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2017, "month": 3, "day": 20}, "abstract": "Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation. Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.", "fields_of_study": "[\"Mathematics\",\"Computer Science\"]", "external_ids": {"arxiv": "1703.06856", "mag": "2952517774", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/KusnerLRS17", "doi": null}}, "content": {"source": {"pdf_hash": "51b066be8d85b9e2f8727798bf79f744b8685bdd", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1703.06856v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "9c246edd2fc1c45f565bc5a826c4566b7e117ba4", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/51b066be8d85b9e2f8727798bf79f744b8685bdd.txt", "contents": "\nCounterfactual Fairness\n\n\nMatt J Kusner \nUniversity of Warwick\n\n\nJoshua R Loftus \nUniversity of Cambridge\n\n\nChris Russell \nUniversity of Surrey\n\n\nRicardo Silva \nUniversity College London\n\n\nAlan Turing Institute \nCounterfactual Fairness\n\nMachine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation. Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school. * Equal contribution, author order decided randomly.\n\nContribution\n\nMachine learning has spread to fields as diverse as credit scoring [18], crime prediction [5], and loan assessment [22]. Decisions in these areas may have ethical or legal implications, so it is necessary for the modeler to think beyond the objective of maximizing prediction accuracy and consider the societal impact of their work. For many of these applications, it is crucial to ask if the predictions of a model are fair. Training data can contain unfairness for reasons having to do with historical prejudices or other factors outside an individual's control. In 2016, the Obama administration released a report 1 which urged data scientists to analyze \"how technologies can deliberately or inadvertently perpetuate, exacerbate, or mask discrimination.\"\n\nThere has been much recent interest in designing algorithms that make fair predictions [4, 6, 9, 10, 12, 14-17, 19, 21, 30-33]. In large part, the literature has focused on formalizing fairness into quantitative definitions and using them to solve a discrimination problem in a certain dataset. Unfortunately, for a practitioner, law-maker, judge, or anyone else who is interested in implementing algorithms that control for discrimination, it can be difficult to decide which definition of fairness to choose for the task at hand. Indeed, we demonstrate that depending on the relationship between a protected attribute and the data, certain definitions of fairness can actually increase discrimination.\n\nIn this paper, we introduce the first explicitly causal approach to address fairness. Specifically, we leverage the causal framework of Pearl [26] to model the relationship between protected attributes and data. We describe how techniques from causal inference can be effective tools for designing fair algorithms and argue, as in DeDeo [8], that it is essential to properly address causality in fairness. In perhaps the most closely related work, Johnson et al. [13] make similar arguments but from a non-causal perspective.\n\nIn Section 2, we provide a summary of basic concepts in fairness and causal modeling. In Section 3, we provide the formal definition of counterfactual fairness, which enforces that a distribution over possible predictions for an individual should remain unchanged in a world where an individual's protected attributes had been different in a causal sense. In Section 4, we describe an algorithm to implement this definition, while distinguishing it from existing approaches. In Section 5, we illustrate the algorithm with a case of fair assessment of law school success.\n\n\nBackground\n\nThis section provides a basic account of two separate areas of research in machine learning, which are formally unified in this paper. We suggest Berk et al. [1] and Pearl et al. [25] as references for further reading.\n\nThroughout this paper, we will use the following notation. Let A denote the set of protected attributes of an individual, variables that must not be discriminated against in a formal sense defined differently by each notion of fairness discussed. The decision of whether an attribute is protected or not is taken as a primitive in any given problem, regardless of the definition of fairness adopted. Moreover, let X denote the other observable attributes of any particular individual, U the set of relevant latent attributes which are not observed, and let Y denote the outcome to be predicted, which itself might be contaminated with historical biases. Finally,\u0176 is the predictor, a random variable that depends on A, X and U , and which is produced by a machine learning algorithm as a prediction of Y .\n\n\nFairness\n\nThe goal of fairness in machine learning is to design automated algorithms that make fair predictions across various demographic groups. This unfairness can arise in several ways. Consider for instance historically biased distributions, where individuals with different protected attributes A may have different attributes X such as their current level of wealth, which is then used for credit scoring. Disparities may be due to discriminative measures in hiring practices and perpetuated by reduced financial support for the education of minorities due to bad credit ratings.\n\nThere has been a wealth of recent work on fair algorithms. These include fairness through unawareness [10], individual fairness [9,14,21,32], demographic parity/disparate impact [30], and equality of opportunity [12,31]. For simplicity we often assume A is encoded as a binary attribute, but this can be generalized.\n\n\nDefinition 1 (Fairness Through Unawareness (FTU)\n\n). An algorithm is fair so long as any protected attributes A are not explicitly used in the decision-making process.\n\nAny mapping\u0176 : X \u2192 Y that excludes A satisfies this. Initially proposed as a baseline, the approach has found favor recently with more general approaches such as Grgic-Hlaca et al. [10]. Despite its compelling simplicity, FTU has a clear shortcoming as elements of X can contain discriminatory information analogous to A that may not be obvious at first. The need for expert knowledge in assessing the relationship between A and X was highlighted in the work on individual fairness: Definition 2 (Individual Fairness (IF)). An algorithm is fair if it gives similar predictions to similar individuals. Formally, if individuals i and j are similar apart from their protected attributes A i , A j then\u0176 (X (i) , A (i) ) \u2248\u0176 (X (j) , A (j) ).\n\nAs described in [9], the notion of similarity must be carefully chosen, requiring an understanding of the domain at hand beyond black-box statistical modeling. This can also be contrasted again population level criteria such as Definition 3 (Demographic Parity (DP)). A predictor\u0176 satisfies demographic parity if P (\u0176 |A = 0) = P (\u0176 |A = 1).\n\nDefinition 4 (Equality of Opportunity (EO)). A predictor\u0176 satisfies equality of opportunity if P (\u0176 = 1|A = 0, Y = 1) = P (\u0176 = 1|A = 1, Y = 1).\n\nThese criteria can be incompatible in general, as discussed in [1,7,19]. Following the motivation of IF and [13], we propose that knowledge about relationships between all attributes should be taken into consideration, even if strong assumptions are necessary. Moreover, it is not immediately clear for any of these approaches in which ways historical biases can be tackled. We approach such issues from an explicit causal modeling perspective.\n\n\nCausal Models and Counterfactuals\n\nWe follow Pearl [24], and define a causal model as a triple (U, V, F ) of sets such that \u2022 U is a set of latent background variables,which are factors not caused by any variable in the set V of observable variables;\n\n\u2022 F is a set of functions {f 1 , . . . , f n }, one for each\nV i \u2208 V , such that V i = f i (pa i , U pai ), pa i \u2286 V \\{V i } and U pai \u2286 U .\nSuch equations are also known as structural equations [2].\n\nThe notation \"pa i \" refers to the \"parents\" of V i and is motivated by the assumption that the model factorizes as a directed graph, here assumed to be a directed acyclic graph (DAG). The model is causal in that, given a distribution P (U ) over the background variables U , we can derive the distribution of a subset\nZ \u2286 V following an intervention on V \\ Z. An intervention on variable V i is the substitution of equation V i = f i (pa i , U pai ) with the equation V i = v for some v.\nThis captures the idea of an agent, external to the system, modifying it by forcefully assigning value v to V i , for example as in a randomized experiment.\n\nThe specification of F is a strong assumption but allows for the calculation of counterfactual quantities. In brief, consider the following counterfactual statement, \"the value of Y if Z had taken value z\", for two observable variables Z and Y . By assumption, the state of any observable variable is fully determined by the background variables and structural equations. The counterfactual is modeled as the solution for Y for a given U = u where the equations for Z are replaced with Z = z. We denote it by Y Z\u2190z (u) [24], and sometimes as Y z if the context of the notation is clear.\n\nCounterfactual inference, as specified by a causal model (U, V, F ) given evidence W , is the computation of probabilities P (Y Z\u2190z (U ) | W = w), where W , Z and Y are subsets of V . Inference proceeds in three steps, as explained in more detail in Chapter 4 of Pearl et al. [25]: 1. Abduction: for a given prior on U , compute the posterior distribution of U given the evidence W = w; 2. Action: substitute the equations for Z with the interventional values z, resulting in the modified set of equations F z ; 3. Prediction: compute the implied distribution on the remaining elements of V using F z and the posterior P (U |W = w).\n\n\nCounterfactual Fairness\n\nGiven a predictive problem with fairness considerations, where A, X and Y represent the protected attributes, remaining attributes, and output of interest respectively, let us assume that we are given a causal model (U, V, F ), where V \u2261 A \u222a X. We postulate the following criterion for predictors of Y . for all y and for any value a attainable by A. This is closely related to actual causes [11], or token causality in the sense that, to be fair, A should not be a cause of\u0176 in any individual instance. In other words, changing A while holding things which are not causally dependent on A constant will not change the distribution of\u0176 . We also emphasize that counterfactual fairness is an individual-level definition. This is substantially different from comparing different individuals that happen to share the same \"treatment\" A = a and coincide on the values of X, as discussed in Section 4.3.1 of [25] and the Supplementary Material 2 . Differences between X a and X a must be caused by variations on A only. Notice also that this definition is agnostic with respect to how good a predictor\u0176 is, which we discuss in Section 4. Relation to individual fairness. IF is agnostic with respect to its notion of similarity metric, which is both a strength (generality) and a weakness (no unified way of defining similarity). Counterfactuals and similarities are related, as in the classical notion of distances between \"worlds\" corresponding to different counterfactuals [20]. Defining\u0176 as a deterministic function of W \u2282 A \u222a X \u222a U , as in several of our examples to follow, then IF can be defined by treating equally two individuals with the same W in a way that is also counterfactually fair. Relation to Pearl et al. [25]. In Example 4.4.4 of [25], the authors condition instead on X, A, and the observed realization of\u0176 , and calculate the probability of the counterfactual realization\u0176 A\u2190a differing from the factual. This example conflates the predictor\u0176 with the outcome Y , of which we remain agnostic in our definition but which is used in the construction of\u0176 as in Section 4. Our framing makes the connection to machine learning more explicit.\n\n\nImplications\n\nOne simple but important implication of the definition of counterfactual fairness is the following: Lemma 1. Let G be the causal graph of the given model (U, V, F ). Then\u0176 will be counterfactually fair if it is a function of the non-descendants of A.\n\nProof. Let W be any non-descendant of A in G. Then W A\u2190a (U ) and W A\u2190a (U ) have the same distribution by the three inferential steps in Section 2.2. Hence, the distribution of any function\u0176 of the non-descendants of A is invariant with respect to the counterfactual values of A.\n\nThis does not exclude using a descendant W of A as a possible input to\u0176 . However, this will only be possible in the case where the overall dependence of Y on A disappears, which will not happen in general. Hence, Lemma 1 provides the most straightforward way to achieve counterfactual fairness. Ancestral closure of protected attributes. Suppose that a parent of a member of A is not in A. Counterfactual fairness allows for the use of it in the definition of\u0176 . If this seems counterintuitive, then we argue that the fault Figure 1: (a) The graph corresponding to a causal model with A being the protected attribute and Y some outcome of interest, with background variables assumed to be independent. (b) Expanding the model to include an intermediate variable indicating whether the individual is employed with two (latent) background variables Prejudiced (if the person offering the job is prejudiced) and Qualifications (a measure of the individual's qualifications). (c) A twin network representation of this system [24] under two different counterfactual levels for A. This is created by copying nodes descending from A, which inherit unaffected parents from the factual world. (d),(e) Two causal models for different real-world fair prediction scenarios. See Section 3 for discussion.\nA Y U Y U A Employed A Y U Y U A Prejudiced Qualifications a Employeda Y a Employed Y a 0 a 0 a 0 Employed A Y U Y U A Prejudiced Qualifications (a) (b) (c) A X Y U A X Y U (d) (e)\nshould be at the postulated set of protected attributes rather than with the definition of counterfactual fairness, and that typically we should expect set A to be closed under ancestral relationships given by the causal graph. For instance, if Race is a protected attribute, and Mother's race is a parent of Race, then it should also be in A.\n\nDealing with historical biases. The explicit difference between\u0176 and Y allows us to tackle historical biases. For instance, let Y be an indicator of whether a client defaults on a loan, while\u0176 is the actual decision of giving the loan. Consider the DAG A \u2192 Y , shown in Figure 1(a) with the explicit inclusion of set U of independent background variables. Y is the objectively ideal measure for decision making, the binary indicator of the event that the individual defaults on a loan. If A is postulated to be a protected attribute,\nthen the predictor\u0176 = Y = f Y (A, U )\nis not counterfactually fair, with the arrow A \u2192 Y being (for instance) the result of a world that punishes individuals in a way that is out of their control. Figure 1(b) shows a finer-grained model, where the path is mediated by a measure of whether the person is employed, which is itself caused by two background factors: one representing whether the person hiring is prejudiced, and the other the employee's qualifications. In this world, A is a cause of defaulting, even if mediated by other variables 3 . The counterfactual fairness principle however forbids us from using Y : using the twin network of Pearl [24], we see in Figure 1(c) that Y a and Y a need not be identically distributed given the background variables. In contrast, any function of variables not descendants of A can be used a basis for fair decision making. This means that any variable\u0176 defined by\u0176 = g(U ) will be counterfactually fair for any function g(\u00b7). Hence, given a causal model, the functional defined by the function g(\u00b7) minimizing some predictive error for Y will satisfy the criterion, as proposed in Section 4.1. We are essentially learning a projection of Y into the space of fair decisions, removing historical biases as a by-product.\n\n\nFurther Examples\n\nTo give further intuition for counterfactual fairness, we will consider two realworld fair prediction scenarios: insurance pricing and crime prediction.\n\nEach of these correspond to one of the two causal graphs in Figure 1(d),(e).\n\nThe Supplementary Material provides a more mathematical discussion of these examples with more detailed insights.\n\nScenario 1: The Red Car. A car insurance company wishes to price insurance for car owners by predicting their accident rate Y . They assume there is an unobserved factor corresponding to aggressive driving U , that (a) causes drivers to be more likely have an accident, and (b) causes individuals to prefer red cars (the observed variable X). Moreover, individuals belonging to a certain race A are more likely to drive red cars. However, these individuals are no more likely to be aggressive or to get in accidents than any one else. We show this in Figure 1(d). Thus, using the red car feature X to predict accident rate Y would seem to be an unfair prediction because it may charge individuals of a certain race more than others, even though no race is more likely to have an accident. Counterfactual fairness agrees with this notion, as X is a descendant of A but U is not. Interestingly, we can show (Supplementary Material) that in a linear model, regression Y on A and X is equivalent to regressing on U , so off-the-shelf regression here is counterfactually fair. Regressing Y on X alone obeys the FTU criterion but is not counterfactually fair, so omitting A (FTU) may introduce unfairness into an otherwise fair world.\n\nScenario 2: High Crime Regions. A city government wants to estimate crime rates by neighborhood to allocate policing resources. Its analyst constructed training data by merging (1) a registry of residents containing their neighborhood X and race A, with (2) police records of arrests, giving each resident a binary label with Y = 1 indicating a criminal arrest record. Due to historically segregated housing, the location X depends on A. Locations X with more police resources have larger numbers of arrests Y . And finally, U represents the totality of socioeconomic factors and policing practices that both influence where an individual may live and how likely they are to be arrested and charged. This can all be seen in Figure 1(e). In this example, higher observed arrest rates in some neighborhoods are due to greater policing there, not because people of different races are any more or less likely to break the law. The label Y = 0 does not mean someone has never committed a crime, but rather that they have not been caught. If individuals in the training data have not already had equal opportunity, algorithms enforcing EO will not remedy such unfairness. In contrast, a counterfactually fair approach would model differential enforcement rates using U and base predictions on this information rather than on X directly.\n\nIn general, we need a multistage procedure in which we first derive latent variables U , and then based on them we minimize some loss with respect to Y . This is the core of the algorithm discussed next.\n\n\nImplementing Counterfactual Fairness\n\nAs discussed in the previous Section, we need to relate\u0176 to Y if the predictor is to be useful, and we restrict\u0176 to be a (parameterized) function of the nondescendants of A in the causal graph following Lemma 1. We next introduce an algorithm, then discuss assumptions that can be used to express counterfactuals.\n\n\nAlgorithm\n\nLet\u0176 \u2261 g \u03b8 (U, X A ) be a predictor parameterized by \u03b8, such as a logistic regression or a neural network, and where X A \u2286 X are non-descendants of A. Given a loss function l(\u00b7, \u00b7) such as squared loss or log-likelihood, and\ntraining data D \u2261 {(A (i) , X (i) , Y (i) )} for i = 1, 2, . . . , n, we define L(\u03b8) \u2261 n i=1 E[l(y (i) , g \u03b8 (U (i) , x (i) A )\n) | x (i) , a (i) ]/n as the empirical loss to be minimized with respect to \u03b8. Each expectation is with respect to random variable\nU (i) \u223c P M (U | x (i) , a (i) ) where P M (U | x, a)\nis the conditional distribution of the background variables as given by a causal model M that is available by assumption. If this expectation cannot be calculated analytically, Markov chain Monte Carlo (MCMC) can be used to approximate it as in the following algorithm. For each data point i \u2208 D, sample m MCMC samples U\n(i) 1 , . . . , U (i) m \u223c P M (U | x (i) , a (i) ).\n\n3:\n\nLet D be the augmented dataset where each point (a (i) , x (i) , y (i) ) in D is replaced with the corresponding m points {(a (i) , x (i) , y (i) , u point (a , x ). Deconvolution perspective. The algorithm can be understood as a deconvolution approach that, given observables A \u222a X, extracts its latent sources and pipelines them into a predictive model. We advocate that counterfactual assumptions must underlie all approaches that claim to extract the sources of variation of the data as \"fair\" latent components. As an example, Louizos et al. [21] start from the DAG A \u2192 X \u2190 U to extract P (U | X, A). As U and A are not independent given X in this representation, a type of penalization is enforced to create a posterior P f air (U |A, X) that is close to the model posterior P (U | A, X) while satisfying P f air (U |A = a, X) \u2248 P f air (U |A = a , X). But this is neither necessary nor sufficient for counterfactual fairness. The model for X given A and U must be justified by a causal mechanism, and that being the case, P (U | A, X) requires no post-processing. As a matter of fact, model M can be learned by penalizing empirical dependence measures between U and pa i for a given V i (e.g. Mooij et al. [23]), but this concerns M and not\u0176 , and is motivated by explicit assumptions about structural equations, as described next.\n(i) j )}. 4:\u03b8 \u2190 argmin \u03b8 i \u2208D l(y (i ) , g \u03b8 (U (i ) , x (i ) A )). 5: end procedure At prediction time, we report\u1ef8 \u2261 E[\u0176 (U , x A ) | x , a ] for a new data\n\nDesigning the Input Causal Model\n\nModel M must be provided to algorithm FairLearning. Causal models always require strong assumptions, but counterfactuals in particular are typically presented in terms of structural equations which are in general unfalsifiable. We point out that we do not need to specify a fully deterministic model, and structural equations can be relaxed as conditional distributions. In particular, the concept of counterfactual fairness holds under three levels of assumptions of increasing strength: Level 1. Build\u0176 using only the observable non-descendants of A. This only requires partial causal ordering and no further causal assumptions, but in many problems there will be few, if any, observables which are not descendants of protected demographic factors. Level 2. Postulate background latent variables that act as non-deterministic causes of observable variables, based on explicit domain knowledge and learning algorithms 4 . Information about X is passed to\u0176 via P (U | x, a). Level 3. Postulate a fully deterministic model with latent variables. For instance, the distribution P (V i | pa i ) can be treated as an additive error model, [27]. The error term e i then becomes an input to\u0176 as calculated from the observed variables. This maximizes the information extracted by the fair predictor\u0176 .\nV i = f i (pa i )+e i\n\nIllustration: Law School Success\n\nWe illustrate our approach on a practical problem that requires fairness, the prediction of success in law school. A second problem, separating actual and perceived criminality in police stops, is described in the Supplementary Material. Following closely the usual framework for assessing causal models in the machine learning literature, the goal of this experiment is to quantify how our algorithm behaves with finite sample sizes while assuming ground truth compatible with a synthetic model. Given this data, a school may wish to predict if an applicant will have a high FYA. The school would also like to make sure these predictions are not biased by an individual's race and sex. However, the LSAT, GPA, and FYA scores, may be biased due to social factors. We compare our framework with two unfair baselines: 1. Full: the standard technique of using all features, including sensitive features such as race and sex to make predictions; 2. Unaware: fairness through unawareness, where we do not use race and sex as features. For comparison, we generate predictors\u0176 for all models using logistic regression.\n\nFair prediction. As described in Section 4.2, there are three ways in which we can model a counterfactually fair predictor of FYA. Level 1 uses any features which are not descendants of race and sex for prediction. Level 2 models latent 'fair' variables which are parents of observed variables. These variables are independent of both race and sex. Level 3 models the data using an additive error model, and uses the independent error terms to make predictions. These models make increasingly strong assumptions corresponding to increased predictive power. We split the dataset 80/20 into a train/test set, preserving label balance, to evaluate the models.\n\nAs we believe LSAT, GPA, and FYA are all biased by race and sex, we cannot use any observed features to construct a counterfactually fair predictor as described in Level 1.\n\nIn Level 2, we postulate that a latent variable: a student's knowledge (K), affects GPA, LSAT, and FYA scores. The causal graph corresponding to this model is shown in Figure 2, (Level 2). This is a short-hand for the distributions:\nGPA \u223c N (b G + w K G K + w R G R + w S G S, \u03c3 G ), FYA \u223c N (w K F K + w R F R + w S F S, 1), LSAT \u223c Poisson(exp(b L + w K L K + w R L R + w S L S)), K \u223c N (0, 1)\nWe perform inference on this model using an observed training set to estimate the posterior distribution of K. We use the probabilistic programming language Stan [28] to learn K. We call the predictor constructed using K, Fair K.\n\nIn Level 3, we model GPA, LSAT, and FYA as continuous variables with additive error terms independent of race and sex (that may in turn be correlated with one-another). This model is shown in Figure 2, (Level 3), and is expressed by:\nGPA = b G + w R G R + w S G S + G , G \u223c p( G ) LSAT = b L + w R L R + w S L S + L , L \u223c p( L ) FYA = b F + w R F R + w S F S + F , F \u223c p( F )\nWe estimate the error terms G , L by first fitting two models that each use race and sex to individually predict GPA and LSAT. We then compute the residuals of each model (e.g., G = GPA\u2212\u0176 GPA (R, S)). We use these residual estimates of G , L to predict FYA. We call this Fair Add.\n\nAccuracy. We compare the RMSE achieved by logistic regression for each of the models on the test set in Table 1. The Full model achieves the lowest RMSE as it uses race and sex to more accurately reconstruct FYA. Note that in this case, this model is not fair even if the data was generated by one of the models shown in Figure 2 as it corresponds to Scenario 3. The (also unfair) Unaware model still uses the unfair variables GPA and LSAT, but because it does not use race and sex it cannot match the RMSE of the Full model. As our models satisfy counterfactual fairness, they trade off some accuracy. Our first model Fair K uses weaker assumptions and thus the RMSE is highest. Using the Level 3 assumptions, as in Fair Add we produce a counterfactually fair model that trades slightly stronger assumptions for lower RMSE.\n\nCounterfactual fairness. We would like to empirically test whether the baseline methods are counterfactually fair. To do so we will assume the true model of the world is given by Figure 2, (Level 2). We can fit the parameters of this model using the observed data and evaluate counterfactual fairness by sampling from it. Specifically, we will generate samples from the model given either the observed race and sex, or counterfactual race and sex variables. We will fit models to both the original and counterfactual sampled data and plot how the distribution of predicted FYA changes for both baseline models. Figure 2 shows this, where each row corresponds to a baseline predictor and each column corresponds to the counterfactual change. In each plot, the blue distribution is density of predicted FYA for the original data and the red distribution is this density for the counterfactual data. If a model is counterfactually fair we would expect these distributions to lie exactly on top of each other. Instead, we note that the Full model exhibits counterfactual unfairness for all counterfactuals except sex. We see a similar trend for the Unaware model, although it is closer to being counterfactually fair. To see why these models seem to be fair w.r.t. to sex we can look at weights of the DAG which generates the counterfactual data. Specifically the DAG weights from (male,female) to GPA are (0.93,1.06) and from (male,female) to LSAT are (1.1,1.1). Thus, these models are fair w.r.t. to sex simply because of a very weak causal link between sex and GPA/LSAT.\n\n\nConclusion\n\nWe have presented a new model of fairness we refer to as counterfactual fairness. It allows us to propose algorithms that, rather than simply ignoring protected attributes, are able to take into account the different social biases that may arise towards individuals based on ethically sensitive attributes and compensate for these biases effectively. We experimentally contrasted our approach with previous fairness approaches and show that our explicit causal models capture these social biases and make clear the implicit trade-off between prediction accuracy and fairness in an unfair world. We propose that fairness should be regulated by explicitly modeling the causal structure of the world. Criteria based purely on probabilistic independence cannot satisfy this and are unable to address how unfairness is occurring in the task at hand. By providing such causal tools for addressing fairness questions we hope we can provide practitioners with customized techniques for solving a wide array of fairness modeling problems.\n\n\nS1 -Population Level vs Individual Level Causal Effects\n\nAs discussed in Section 3, counterfactual fairness is an individual-level definition. This is fundamentally different from comparing different units that happen to share the same \"treatment\" A = a and coincide on the values of X. To see in detail what this means, consider the following thought experiment. Let us assess the causal effect of A on\u0176 by controlling A at two levels, a and a . In Pearl's notation, where \"do(A = a)\" expresses an intervention on A at level a, we have that\nE[\u0176 | do(A = a), X = x] \u2212 E[\u0176 | do(A = a ), X = x],(2)\nis a measure of causal effect, sometimes called the average causal effect (ACE). It expresses the change that is expected when we intervene on A while observing the attribute set X = x, under two levels of treatment. If this effect is non-zero, A is considered to be a cause of\u0176 . This raises a subtlety that needs to be addressed: in general, this effect will be non-zero even if\u0176 is counterfactually fair. This may sound counter-intuitive: protected attributes such as race and gender are causes of our counterfactually fair decisions.\n\nIn fact, this is not a contradiction, as the ACE in Equation (2) is different from counterfactual effects. The ACE contrasts two independent exchangeable units of the population, and it is a perfectly valid way of performing decision analysis. However, the value of X = x is affected by different background variables corresponding to different individuals. That is, the causal effect (2) contrasts two units that receive different treatments but which happen to coincide on X = x. To give a synthetic example, imagine the simple structural equation\nX = A + U.\nThe ACE quantifies what happens among people with U = x \u2212 a against people with U = x \u2212 a . If, for instance,\u0176 = \u03bbU for \u03bb = 0, then the effect (2) is \u03bb(a \u2212 a ) = 0.\n\nContrary to that, the counterfactual difference is zero. That is,\nE[\u0176 A\u2190a (U ) | A = a, X = x] \u2212 E[\u0176 A\u2190a (U ) | A = a, X = x] = \u03bbU \u2212 \u03bbU = 0.\nIn another perspective, we can interpret the above just as if we had measured U from the beginning rather than performing abduction. We then generate\u0176 from some g(U ), so U is the within-unit cause of\u0176 and not A.\n\nIf U cannot be deterministically derived from {A = a, X = x}, the reasoning is similar. By abduction, the distribution of U will typically depend on A, and hence so will\u0176 when marginalizing over U . Again, this seems to disagree with the intuition that our predictor should be not be caused by A. However, this once again is a comparison across individuals, not within an individual.\n\nIt is this balance among (A, X, U ) that explains, in the examples of Section 3.2, why some predictors are counterfactually fair even though they are functions of the same variables {A, X} used by unfair predictors: such functions must correspond to particular ways of balancing the observables that, by way of the causal assumptions, cancel out the effect of A. More on conditioning and alternative definitions. As discussed in Example 4.4.4 of Pearl et al. [25], a different proposal for assessing fairness can be defined via the following concept:\n\nDefinition 6 (Probability of sufficiency). We define the probability of event {A = a} being a sufficient cause for our decision\u0176 , contrasted against {A = a }, as\nP (\u0176 A\u2190a (U ) = y | X = x, A = a,\u0176 = y).(3)\nWe can then, for instance, claim that\u0176 is a fair predictor if this probability is below some pre-specified bound for all (x, a, a ). The shortcomings of this definition come from its original motivation: to explain the behavior of an existing decision protocol, where\u0176 is the current practice and which in a unclear way is conflated with Y . The implication is that if\u0176 is to be designed instead of being a natural measure of existing behaviour, then we are using\u0176 itself as evidence for the background variables U . This does not make sense if\u0176 is yet to be designed by us. If\u0176 is to be interpreted as Y , then this does not provide a clear recipe on how to build\u0176 : while we can use Y to learn a causal model, we cannot use it to collect training data evidence for U as the outcome Y will not be available to us at prediction time. For this reason, we claim that while probability of sufficiency is useful as a way of assessing an existing decision making process, it is not as natural as counterfactual fairness in the context of machine learning. Approximate fairness and model validation. The notion of probability of sufficiency raises the question on how to define approximate, or high probability, counterfactual fairness. This is an important question but for reasons of conciseness and focus we defer it entirely to future work. Before defining an approximation, it is important to first expose in detail what the exact definition is, which is the goal of this paper.\n\nWe also do not address the validation of the causal assumptions used by the input causal model of the FairLearning algorithm in Section 4.1. The reason is straightforward: this validation is an entirely self-contained step of the implementation of counterfactual fairness. An extensive literature already exists in this topic which the practitioner can refer to (a classic account for instance is [3]), and which can be used as-is in our context.\n\nThe experiments performed in Section 5 can be criticized by the fact that they rely on a model that obeys our assumptions, and \"obviously\" our approach should work better than alternatives. This criticism is not warranted: in machine learning, causal inference is typically assessed through simulations which assume that the true model lies in the family covered by the algorithm. Algorithms, including FairLearning, are justified in the population sense. How different competitors behave with finite sample sizes is the primary question to be studied in an empirical study of a new concept, where we control for the correctness of the assumptions. Although sensitivity analysis is important, there are many degrees of freedom on how this can be done. Robustness issues are better addressed by extensions focusing on approximate versions of counterfactual fairness. This will be covered in later work.\n\n\nS2 -Relation to Demographic Parity\n\nConsider the graph A \u2192 X \u2192 Y . In general, if\u0176 is a function of X only, then Y need not obey demographic parity, i.e.\nP (\u0176 | A = a) = P (\u0176 | A = a ),\nwhere, since\u0176 is a function of X, the probabilities are obtained by marginalizing over P (X | A = a) and P (X | A = a ), respectively.\n\nIf we postulate a structural equation X = \u03b1A + e X , then given A and X we can deduce e X . If\u0176 is a function of e X only and, by assumption, e X is marginally independent of A, then\u0176 is marginally independent of A: this follows the interpretation given in the previous section, where we interpret e X as \"known\" despite being mathematically deduced from the observation (A = a, X = x). Therefore, the assumptions imply that\u0176 will satisfy demographic parity, and that can be falsified. By way of contrast, if e X is not uniquely identifiable from the structural equation and (A, X), then the distribution of\u0176 depends on the value of A as we marginalize e X , and demographic parity will not follow. This leads to the following: Lemma 2. If all background variables U \u2286 U in the definition of\u0176 are determined from A and X, and all observable variables in the definition of\u0176 are independent of A given U , then\u0176 satisfies demographic parity.\n\nThus, counterfactual fairness can be thought of as a counterfactual analog of demographic parity, as present in the Red Car example further discussed in the next section.\n\n\nS3 -Examples Revisited\n\nIn Section 3.2, we discussed two examples. We reintroduce them here briefly, add a third example, and explain some consequences of their causal structure to the design of counterfactually fair predictors.\n\nScenario 1: The Red Car Revisited. In that scenario, the structure A \u2192 X \u2190 U \u2192 Y implies that\u0176 should not use either X or A. On the other hand, it is acceptable to use U . It is interesting to realize, however, that since U is related to A and X, there will be some association between Y and {A, X} as discussed in Section S1. In particular, if the structural equation for X is linear, then U is a linear function of A and X, and as such\u0176 will also be a function of both A and X. This is not a problem, as it is still the case that the model implies that this is merely a functional dependence that disappears by conditioning on a postulated latent attribute U . Surprisingly, we must make\u0176 a indirect function of A if we want a counterfactually fair predictor, as shown in the following Lemma.\n\nLemma 3. Consider a linear model with the structure in Figure 1(d). Fitting a linear predictor to X only is not counterfactually fair, while the same algorithm will produce a fair predictor using both A and X.\n\nProof. As in the definition, we will consider the population case, where the joint distribution is known. Consider the case where the equations described by the model in Figure 1(d) are deterministic and linear:\nX = \u03b1A + \u03b2U, Y = \u03b3U.\nDenote the variance of U as v U , the variance of A as v A , and assume all coefficients are non-zero. The predictor\u0176 (X) defined by least-squares regression of Y on only X is given by\u0176 (X) \u2261 \u03bbX, where \u03bb = Cov(X, Y )/V ar(X) = \u03b2\u03b3v U /(\u03b1 2 v A + \u03b2 2 v U ) = 0. This predictor follows the concept of fairness through unawareness.\n\nWe can test whether a predictor\u0176 is counterfactually fair by using the procedure described in Section 2.2: (i) Compute U given observations of X, Y, A; (ii) Substitute the equations involving A with an interventional value a ; (iii) Compute the variables X, Y with the interventional value a . It is clear here that\u0176 a (U ) = \u03bb(\u03b1a + \u03b2U ) =\u0176 a (U ). This predictor is not counterfactually fair. Thus, in this case fairness through unawareness actually perpetuates unfairness.\n\nConsider instead doing least-squares regression of Y on X and A. Note that Y (X, A) \u2261 \u03bb X X + \u03bb A A where \u03bb X , \u03bb A can be derived as follows:\n\u03bb X \u03bb A = V ar(X) Cov(A, X) Cov(X, A) V ar(A) \u22121 Cov(X, Y ) Cov(A, Y ) = 1 \u03b2 2 v U v A v A \u2212\u03b1v A \u2212\u03b1v A \u03b1 2 v A + \u03b2 2 v U \u03b2\u03b3v U 0 = \u03b3 \u03b2 \u2212\u03b1\u03b3 \u03b2(4)\nNow imagine we have observed A = a. This implies that X = \u03b1a + \u03b2U and our predictor is\u0176 (X, a) = \u03b3 \u03b2 (\u03b1a + \u03b2U ) + \u2212\u03b1\u03b3 \u03b2 a = \u03b3U . Thus, if we substitute a with a counterfactual a (the action step described in Section 2.2) the predictor Y (X, A) is unchanged. This is because our predictor is constructed in such a way that any change in X caused by a change in A is canceled out by the \u03bb A . Thus this predictor is counterfactually fair.\n\nNote that if Figure 1(d) is the true model for the real world then\u0176 (X, A) will also satisfy demographic parity and equality of opportunity as\u0176 will be unaffected by A.\n\nThe above lemma holds in a more general case for the structure given in Figure 1(d): any non-constant estimator that depends only on X is not counterfactually fair as changing A always alters X.\n\nScenario 2: High Crime Regions Revisited. The causal structure differs from the previous example by the extra edge X \u2192 Y . For illustration purposes, assume again that the model is linear. Unlike the previous case, a predictor Y trained using X and A is not counterfactually fair. The only change from Scenario 1 is that now Y depends on X as follows: Y = \u03b3U + \u03b8X. Now if we solve for \u03bb X , \u03bb A it can be shown that\u0176 (X, a) = (\u03b3 \u2212 \u03b1 2 \u03b8v A \u03b2v U )U + \u03b1\u03b8a. As this predictor depends on the values of A that are not explained by U , then\u0176 (X, a) =\u0176 (X, a ) and thus\u0176 (X, A) is not counterfactually fair.\n\nThe following extra example complements the previous two examples.\n\n\nScenario 3: University Success.\n\nA university wants to know if students will be successful post-graduation Y . They have information such as: grade point average (GPA), advanced placement (AP) exams results, and other academic features X. The university believes however, that an individual's gender A may influence these features and their post-graduation success Y due to social discrimination. They also believe that independently, an individual's latent talent U causes X and Y . The structure is similar to Figure 1(d), with the extra edge A \u2192 Y . We can again ask, is the predictor\u0176 (X, A) counterfactually fair? In this case, the different between this and Scenario 1 is that Y is a function of U and A as follows: Y = \u03b3U + \u03b7A. We can again solve for \u03bb X , \u03bb A and show that Y (X, a) = (\u03b3 \u2212 \u03b1\u03b7v A \u03b2v U )U + \u03b7a. Again\u0176 (X, A) is a function of A not explained by U , so it cannot be counterfactually fair.\n\n\nS4 -Case Study: Criminality vs. Perceived Criminality\n\nWe test our approach on a problem of separating actual and perceived criminality in police stops. For this problem, we construct a causal model, and make explicit how unfairness may affect observed and unobserved variables in the world. Given the model we derive counterfactually fair predictors, and predict latent variables such as a person's 'criminality' (which may be useful for predicting crime) as well as their 'perceived criminality' (which may be due to prejudices based on appearance). Finally we judge how well our counterfactually fair 'criminality' score satisfies demographic parity.\n\nSince 2002, the New York Police Department (NYPD) has recorded information about every time a police officer has stopped someone. The officer records information such as if the person was searched or frisked, was made or a summons issued, the data collected on males stopped during 2014 which constitutes 38,609 records. stopped as this accounts for more than 90% of the data. We fit a model which decomposes these records into two latent factors, Figure 3: Understanding criminality. The above maps show the decomposition of stop and search data in New York into factors based on perceived criminality (a race dependent variable) and latent criminality (a race neutral measure). See section 6. one which depends on the race and appearance of the individual being stopped, labeled Perceived Criminality, and one which does not, labeled Criminality, and which could be used as a basis for counterfactually fair decisions. The full details of this experiment, including the DAG, are given in the supplementary materials. We now describe a spatial analysis of the estimated latent factors.\n\nVisualization on a map of New York City. Each of the stops can be mapped to longitude and latitude points for where the stop occurred 5 . Thus we can visualize Criminality and Perception alongside Race and the combination of Arrest and Summons, shown in Figure 3. Criminality seems to be a continuous approximation of arrest and summons as both plots show red in similar areas. However, the plots show that certain areas, while having a lot of arrests have low criminality scores such as south Bronx and west Queens (circled in orange). We can also compare the perceived criminality with a plot of race, where we have divided the races into Group A: black, black Hispanic, Hispanic, and Native American (shown in purple); and Group B: white and Asian/Pacific Islander (shown in green). Group A are all races that have positive weights on the connection from Race to Perception in the fitted model, while Group B all have negative weights. Thus being in Group A leads one to have a higher perceived criminality than being in Group B. This can be seen in the right-most plot of Figure 3. Certain areas of town such as central Brooklyn, central Bronx, and southern Queens have very high criminality and almost all stops are by members of Group A (circled in yellow).\n\nDefinition 5 (\n5Counterfactual fairness). Predictor\u0176 is counterfactually fair if under any context X = x and A = a, P (\u0176 A\u2190a (U ) = y | X = x, A = a) = P (\u0176 A\u2190a (U ) = y | X = x, A = a), (1)\n\nFigure 2 :\n2Left: A causal model for the problem of predicting law school success fairly. Right: Density plots of predicted FYA a and FYA a .The Law School Admission Council conducted a survey across 163 law schools in the United States[29]. It contains information on 21,790 law students such as their entrance exam scores (LSAT), their grade-point average (GPA) collected prior to law school, and their first year average grade (FYA).\n\nTable 1 :\n1Prediction results using logistic regression. Note that we must sacrifice a small amount of accuracy to ensuring counterfactually fair prediction (Fair K,Fair Add), versus the models that use unfair features: GPA, LSAT, race, sex \n(Full, Unaware). \n\nFull Unaware Fair K Fair Add \nRMSE 0.873 \n0.894 \n0.929 \n0.918 \n\n\nhttps://obamawhitehouse.archives.gov/blog/2016/05/04/big-risks-big-opportunitiesintersection-big-data-and-civil-rights\nWe strongly suggest that reviewers look at it.\nFor example, if the function determining employment f E (A, P, Q) \u2261 I (Q>0,P =0 or A =a) then an individual with sufficient qualifications and prejudiced potential employer may have a different counterfactual employment value for A = a compared to A = a , and a different chance of default.\nIn some domains, it is actually common to build a model entirely around latent constructs with few or no observable parents nor connections among observed variables[2].\nhttps://github.com/stablemarkets/StopAndFrisk\n\nFairness in criminal justice risk assessments: The state of the art. R Berk, H Heidari, S Jabbari, M Kearns, Roth , A , arXiv:1703.09207v123arXiv preprintBerk, R., Heidari, H., Jabbari, S., Kearns, M., and Roth, A. Fairness in criminal justice risk assessments: The state of the art. arXiv preprint arXiv:1703.09207v1, 2017. 2, 3\n\nStructural Equations with Latent Variables. K Bollen, John Wiley & Sons49Bollen, K. Structural Equations with Latent Variables. John Wiley & Sons, 1989. 4, 9\n\nTesting Structural Equation Models. J. Long. Bollen, K. and17SAGE PublicationsBollen, K. and (eds.), J. Long. Testing Structural Equation Models. SAGE Publications, 1993. 17\n\nMan is to computer programmer as woman is to homemaker? debiasing word embeddings. Bolukbasi, Chang Tolga, Kai-Wei Zou, James Y Saligrama, Venkatesh Kalai, Adam T , Advances in Neural Information Processing Systems. Bolukbasi, Tolga, Chang, Kai-Wei, Zou, James Y, Saligrama, Venkatesh, and Kalai, Adam T. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. In Advances in Neural Information Processing Systems, pp. 4349-4357, 2016. 2\n\nEvaluating the predictive validity of the compas risk and needs assessment system. Tim Brennan, William Dieterich, Beate Ehret, Criminal Justice and Behavior. 361Brennan, Tim, Dieterich, William, and Ehret, Beate. Evaluating the predictive validity of the compas risk and needs assessment system. Criminal Justice and Behavior, 36(1):21-40, 2009. 1\n\nThree naive bayes approaches for discrimination-free classification. Toon Calders, Sicco Verwer, Data Mining and Knowledge Discovery. 212Calders, Toon and Verwer, Sicco. Three naive bayes approaches for discrimination-free classification. Data Mining and Knowledge Discovery, 21(2):277-292, 2010. 2\n\nFair prediction with disparate impact: A study of bias in recidivism prediction instruments. Alexandra Chouldechova, arXiv:1703.00056arXiv preprintChouldechova, Alexandra. Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. arXiv preprint arXiv:1703.00056, 2017. 3\n\nWrong side of the tracks: Big data and protected categories. Simon Dedeo, arXiv:1412.4643arXiv preprintDeDeo, Simon. Wrong side of the tracks: Big data and protected categories. arXiv preprint arXiv:1412.4643, 2014. 2\n\nFairness through awareness. Cynthia Dwork, Hardt, Moritz, Pitassi, Toniann, Omer Reingold, Richard Zemel, Proceedings of the 3rd Innovations in Theoretical Computer Science Conference. the 3rd Innovations in Theoretical Computer Science ConferenceACM23Dwork, Cynthia, Hardt, Moritz, Pitassi, Toniann, Reingold, Omer, and Zemel, Richard. Fairness through awareness. In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference, pp. 214-226. ACM, 2012. 2, 3\n\nThe case for process fairness in learning: Feature selection for fair decision making. Grgic-Hlaca, Nina, Muhammad Zafar, Bilal, Gummadi, P Krishna, Adrian Weller, NIPS Symposium on Machine Learning and the Law. 23Grgic-Hlaca, Nina, Zafar, Muhammad Bilal, Gummadi, Krishna P, and Weller, Adrian. The case for process fairness in learning: Feature selection for fair decision making. NIPS Symposium on Machine Learning and the Law, 2016. 2, 3\n\n. J Halpern, Causality, MIT PressHalpern, J. Actual Causality. MIT Press, 2016. 5\n\nEquality of opportunity in supervised learning. Hardt, Moritz, Price, Eric, Srebro, Nati, Advances in Neural Information Processing Systems. 23Hardt, Moritz, Price, Eric, Srebro, Nati, et al. Equality of opportunity in supervised learning. In Advances in Neural Information Processing Systems, pp. 3315-3323, 2016. 2, 3\n\nImpartial predictive modeling: Ensuring fairness in arbitrary models. Kory D Johnson, Foster, P Dean, Robert A Stine, arXiv:1608.0052823arXiv preprintJohnson, Kory D, Foster, Dean P, and Stine, Robert A. Impartial pre- dictive modeling: Ensuring fairness in arbitrary models. arXiv preprint arXiv:1608.00528, 2016. 2, 3\n\nRawlsian fairness for machine learning. Matthew Joseph, Kearns, Michael, Jamie Morgenstern, Seth Neel, Aaron Roth, arXiv:1610.0955923arXiv preprintJoseph, Matthew, Kearns, Michael, Morgenstern, Jamie, Neel, Seth, and Roth, Aaron. Rawlsian fairness for machine learning. arXiv preprint arXiv:1610.09559, 2016. 2, 3\n\nClassifying without discriminating. Faisal Kamiran, Toon Calders, IC4 2009. 2nd International Conference on. IEEEComputer, Control and CommunicationKamiran, Faisal and Calders, Toon. Classifying without discriminating. In Computer, Control and Communication, 2009. IC4 2009. 2nd International Conference on, pp. 1-6. IEEE, 2009.\n\nData preprocessing techniques for classification without discrimination. Faisal Kamiran, Toon Calders, Knowledge and Information Systems. 331Kamiran, Faisal and Calders, Toon. Data preprocessing techniques for classification without discrimination. Knowledge and Information Systems, 33(1):1-33, 2012.\n\nFairness-aware learning through regularization approach. Toshihiro Kamishima, Shotaro Akaho, Sakuma , IEEE 11th International Conference on. Data Mining Workshops (ICDMW)Kamishima, Toshihiro, Akaho, Shotaro, and Sakuma, Jun. Fairness-aware learning through regularization approach. In Data Mining Workshops (ICDMW), 2011 IEEE 11th International Conference on, pp. 643-650. IEEE, 2011. 2\n\nConsumer credit-risk models via machine-learning algorithms. Amir E Khandani, Kim, J Adlar, Andrew W Lo, Journal of Banking & Finance. 34111Khandani, Amir E, Kim, Adlar J, and Lo, Andrew W. Consumer credit-risk models via machine-learning algorithms. Journal of Banking & Finance, 34 (11):2767-2787, 2010. 1\n\nInherent trade-offs in the fair determination of risk scores. Jon Kleinberg, Sendhil Mullainathan, Manish Raghavan, arXiv:1609.0580723arXiv preprintKleinberg, Jon, Mullainathan, Sendhil, and Raghavan, Manish. Inher- ent trade-offs in the fair determination of risk scores. arXiv preprint arXiv:1609.05807, 2016. 2, 3\n\n. D Lewis, Counterfactuals, Harvard University PressLewis, D. Counterfactuals. Harvard University Press, 1973. 5\n\nChristos Louizos, Swersky, Kevin, Li, Yujia, Max Welling, Richard Zemel, arXiv:1511.00830The variational fair autoencoder. 2arXiv preprintLouizos, Christos, Swersky, Kevin, Li, Yujia, Welling, Max, and Zemel, Richard. The variational fair autoencoder. arXiv preprint arXiv:1511.00830, 2015. 2, 3, 8\n\nMethod and system for loan origination and underwriting. John F Mahoney, James M Mohen, US Patent. 7287Mahoney, John F and Mohen, James M. Method and system for loan origination and underwriting, October 23 2007. US Patent 7,287,008. 1\n\nRegression by dependence minimization and its application to causal inference in additive noise models. J Mooij, D Janzing, J Peters, B Scholkopf, Proceedings of the 26th Annual International Conference on Machine Learning. the 26th Annual International Conference on Machine LearningMooij, J., Janzing, D., Peters, J., and Scholkopf, B. Regression by depen- dence minimization and its application to causal inference in additive noise models. In Proceedings of the 26th Annual International Conference on Machine Learning, pp. 745-752, 2009. 9\n\nCausality: Models, Reasoning and Inference. J Pearl, Cambridge University Press46Pearl, J. Causality: Models, Reasoning and Inference. Cambridge University Press, 2000. 4, 6\n\nCausal Inference in Statistics: a Primer. J Pearl, M Glymour, Jewell , N , Wiley517Pearl, J., Glymour, M., and Jewell, N. Causal Inference in Statistics: a Primer. Wiley, 2016. 2, 4, 5, 17\n\nCausal inference in statistics: An overview. Judea Pearl, Statistics Surveys. 32Pearl, Judea. Causal inference in statistics: An overview. Statistics Surveys, 3:96-146, 2009. 2\n\nCausal discovery with continuous additive noise models. J Peters, J M Mooij, D Janzing, B Sch\u00f6lkopf, Journal of Machine Learning Research. 15Peters, J., Mooij, J. M., Janzing, D., and Sch\u00f6lkopf, B. Causal discovery with continuous additive noise models. Journal of Machine Learning Research, 15: 2009-2053, 2014. URL http://jmlr.org/papers/v15/peters14a.html. 9\n\nRstan: the r interface to stan. Stan Development Team, R package version 2.14.1. 11Stan Development Team. Rstan: the r interface to stan, 2016. R package version 2.14.1. 11\n\nLsac national longitudinal bar passage study. lsac research report series. Linda F Wightman, Wightman, Linda F. Lsac national longitudinal bar passage study. lsac research report series. 1998. 10\n\n. Muhammad Zafar, Bilal, Valera, Isabel, Manuel Rodriguez, Gomez, Krishna P Gummadi, arXiv:1507.0525923Learning fair classifiers. arXiv preprintZafar, Muhammad Bilal, Valera, Isabel, Rodriguez, Manuel Gomez, and Gummadi, Krishna P. Learning fair classifiers. arXiv preprint arXiv:1507.05259, 2015. 2, 3\n\nFairness beyond disparate treatment & disparate impact: Learning classification without disparate mistreatment. Muhammad Zafar, Bilal, Valera, Isabel, Manuel Rodriguez, Gomez, Krishna P Gummadi, arXiv:1610.08452arXiv preprintZafar, Muhammad Bilal, Valera, Isabel, Rodriguez, Manuel Gomez, and Gummadi, Krishna P. Fairness beyond disparate treatment & disparate impact: Learning classification without disparate mistreatment. arXiv preprint arXiv:1610.08452, 2016. 3\n\nRichard S Zemel, Wu, Yu, Swersky, Kevin, Toniann Pitassi, Cynthia Dwork, Learning fair representations. ICML (3). 28Zemel, Richard S, Wu, Yu, Swersky, Kevin, Pitassi, Toniann, and Dwork, Cynthia. Learning fair representations. ICML (3), 28:325-333, 2013. 3\n\nA survey on measuring indirect discrimination in machine learning. Indre Zliobaite, arXiv:1511.00148arXiv preprintZliobaite, Indre. A survey on measuring indirect discrimination in machine learning. arXiv preprint arXiv:1511.00148, 2015. 2\n", "annotations": {"author": "[{\"end\":65,\"start\":27},{\"end\":108,\"start\":66},{\"end\":146,\"start\":109},{\"end\":189,\"start\":147},{\"end\":212,\"start\":190}]", "publisher": null, "author_last_name": "[{\"end\":40,\"start\":34},{\"end\":81,\"start\":75},{\"end\":122,\"start\":115},{\"end\":160,\"start\":155},{\"end\":211,\"start\":202}]", "author_first_name": "[{\"end\":31,\"start\":27},{\"end\":33,\"start\":32},{\"end\":72,\"start\":66},{\"end\":74,\"start\":73},{\"end\":114,\"start\":109},{\"end\":154,\"start\":147},{\"end\":194,\"start\":190},{\"end\":201,\"start\":195}]", "author_affiliation": "[{\"end\":64,\"start\":42},{\"end\":107,\"start\":83},{\"end\":145,\"start\":124},{\"end\":188,\"start\":162}]", "title": "[{\"end\":24,\"start\":1},{\"end\":236,\"start\":213}]", "venue": null, "abstract": "[{\"end\":1242,\"start\":238}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b17\"},\"end\":1329,\"start\":1325},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1351,\"start\":1348},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":1377,\"start\":1373},{\"end\":2144,\"start\":2105},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2869,\"start\":2865},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3063,\"start\":3060},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3190,\"start\":3186},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3996,\"start\":3993},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4018,\"start\":4014},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5557,\"start\":5553},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5582,\"start\":5579},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5585,\"start\":5582},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5588,\"start\":5585},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":5591,\"start\":5588},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":5633,\"start\":5629},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5667,\"start\":5663},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5670,\"start\":5667},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6124,\"start\":6120},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6697,\"start\":6694},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7232,\"start\":7229},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7234,\"start\":7232},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7237,\"start\":7234},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7278,\"start\":7274},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7668,\"start\":7664},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8063,\"start\":8060},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9236,\"start\":9232},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9581,\"start\":9577},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10357,\"start\":10353},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10868,\"start\":10864},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":11435,\"start\":11431},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11684,\"start\":11680},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11710,\"start\":11706},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":13691,\"start\":13687},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":15675,\"start\":15671},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":21253,\"start\":21249},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":21919,\"start\":21915},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":23373,\"start\":23369},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":26092,\"start\":26088},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":33323,\"start\":33319},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":35498,\"start\":35495},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":46276,\"start\":46272},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":47424,\"start\":47421}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":46034,\"start\":45843},{\"attributes\":{\"id\":\"fig_2\"},\"end\":46472,\"start\":46035},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":46799,\"start\":46473}]", "paragraph": "[{\"end\":2016,\"start\":1258},{\"end\":2721,\"start\":2018},{\"end\":3248,\"start\":2723},{\"end\":3820,\"start\":3250},{\"end\":4053,\"start\":3835},{\"end\":4860,\"start\":4055},{\"end\":5449,\"start\":4873},{\"end\":5767,\"start\":5451},{\"end\":5937,\"start\":5820},{\"end\":6676,\"start\":5939},{\"end\":7019,\"start\":6678},{\"end\":7164,\"start\":7021},{\"end\":7610,\"start\":7166},{\"end\":7863,\"start\":7648},{\"end\":7925,\"start\":7865},{\"end\":8064,\"start\":8006},{\"end\":8384,\"start\":8066},{\"end\":8711,\"start\":8555},{\"end\":9299,\"start\":8713},{\"end\":9933,\"start\":9301},{\"end\":12114,\"start\":9961},{\"end\":12381,\"start\":12131},{\"end\":12663,\"start\":12383},{\"end\":13957,\"start\":12665},{\"end\":14482,\"start\":14139},{\"end\":15017,\"start\":14484},{\"end\":16284,\"start\":15056},{\"end\":16457,\"start\":16305},{\"end\":16535,\"start\":16459},{\"end\":16650,\"start\":16537},{\"end\":17880,\"start\":16652},{\"end\":19213,\"start\":17882},{\"end\":19418,\"start\":19215},{\"end\":19772,\"start\":19459},{\"end\":20010,\"start\":19786},{\"end\":20269,\"start\":20139},{\"end\":20644,\"start\":20324},{\"end\":22040,\"start\":20702},{\"end\":23528,\"start\":22234},{\"end\":24697,\"start\":23586},{\"end\":25355,\"start\":24699},{\"end\":25529,\"start\":25357},{\"end\":25763,\"start\":25531},{\"end\":26155,\"start\":25926},{\"end\":26390,\"start\":26157},{\"end\":26813,\"start\":26533},{\"end\":27639,\"start\":26815},{\"end\":29210,\"start\":27641},{\"end\":30254,\"start\":29225},{\"end\":30798,\"start\":30314},{\"end\":31391,\"start\":30854},{\"end\":31942,\"start\":31393},{\"end\":32118,\"start\":31954},{\"end\":32185,\"start\":32120},{\"end\":32473,\"start\":32261},{\"end\":32858,\"start\":32475},{\"end\":33410,\"start\":32860},{\"end\":33574,\"start\":33412},{\"end\":35096,\"start\":33619},{\"end\":35544,\"start\":35098},{\"end\":36447,\"start\":35546},{\"end\":36603,\"start\":36486},{\"end\":36770,\"start\":36636},{\"end\":37711,\"start\":36772},{\"end\":37883,\"start\":37713},{\"end\":38114,\"start\":37910},{\"end\":38910,\"start\":38116},{\"end\":39121,\"start\":38912},{\"end\":39334,\"start\":39123},{\"end\":39683,\"start\":39356},{\"end\":40159,\"start\":39685},{\"end\":40303,\"start\":40161},{\"end\":40884,\"start\":40448},{\"end\":41054,\"start\":40886},{\"end\":41250,\"start\":41056},{\"end\":41852,\"start\":41252},{\"end\":41920,\"start\":41854},{\"end\":42833,\"start\":41956},{\"end\":43489,\"start\":42891},{\"end\":44577,\"start\":43491},{\"end\":45842,\"start\":44579}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8005,\"start\":7926},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8554,\"start\":8385},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14138,\"start\":13958},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15055,\"start\":15018},{\"attributes\":{\"id\":\"formula_4\"},\"end\":20138,\"start\":20011},{\"attributes\":{\"id\":\"formula_5\"},\"end\":20323,\"start\":20270},{\"attributes\":{\"id\":\"formula_6\"},\"end\":20696,\"start\":20645},{\"attributes\":{\"id\":\"formula_7\"},\"end\":22198,\"start\":22041},{\"attributes\":{\"id\":\"formula_8\"},\"end\":23550,\"start\":23529},{\"attributes\":{\"id\":\"formula_9\"},\"end\":25925,\"start\":25764},{\"attributes\":{\"id\":\"formula_10\"},\"end\":26532,\"start\":26391},{\"attributes\":{\"id\":\"formula_11\"},\"end\":30853,\"start\":30799},{\"attributes\":{\"id\":\"formula_12\"},\"end\":31953,\"start\":31943},{\"attributes\":{\"id\":\"formula_13\"},\"end\":32260,\"start\":32186},{\"attributes\":{\"id\":\"formula_14\"},\"end\":33618,\"start\":33575},{\"attributes\":{\"id\":\"formula_15\"},\"end\":36635,\"start\":36604},{\"attributes\":{\"id\":\"formula_16\"},\"end\":39355,\"start\":39335},{\"attributes\":{\"id\":\"formula_17\"},\"end\":40447,\"start\":40304}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":26926,\"start\":26919}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1256,\"start\":1244},{\"attributes\":{\"n\":\"2\"},\"end\":3833,\"start\":3823},{\"attributes\":{\"n\":\"2.1\"},\"end\":4871,\"start\":4863},{\"end\":5818,\"start\":5770},{\"attributes\":{\"n\":\"2.2\"},\"end\":7646,\"start\":7613},{\"attributes\":{\"n\":\"3\"},\"end\":9959,\"start\":9936},{\"attributes\":{\"n\":\"3.1\"},\"end\":12129,\"start\":12117},{\"attributes\":{\"n\":\"3.2\"},\"end\":16303,\"start\":16287},{\"attributes\":{\"n\":\"4\"},\"end\":19457,\"start\":19421},{\"attributes\":{\"n\":\"4.1\"},\"end\":19784,\"start\":19775},{\"end\":20700,\"start\":20698},{\"attributes\":{\"n\":\"4.2\"},\"end\":22232,\"start\":22200},{\"attributes\":{\"n\":\"5\"},\"end\":23584,\"start\":23552},{\"attributes\":{\"n\":\"6\"},\"end\":29223,\"start\":29213},{\"end\":30312,\"start\":30257},{\"end\":36484,\"start\":36450},{\"end\":37908,\"start\":37886},{\"end\":41954,\"start\":41923},{\"end\":42889,\"start\":42836},{\"end\":45858,\"start\":45844},{\"end\":46046,\"start\":46036},{\"end\":46483,\"start\":46474}]", "table": "[{\"end\":46799,\"start\":46639}]", "figure_caption": "[{\"end\":46034,\"start\":45860},{\"end\":46472,\"start\":46048},{\"end\":46639,\"start\":46485}]", "figure_ref": "[{\"end\":13198,\"start\":13190},{\"end\":14762,\"start\":14754},{\"end\":15223,\"start\":15215},{\"end\":15695,\"start\":15687},{\"end\":16527,\"start\":16519},{\"end\":17211,\"start\":17203},{\"end\":18614,\"start\":18606},{\"end\":20866,\"start\":20852},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":25718,\"start\":25699},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":26357,\"start\":26349},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":27144,\"start\":27136},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":27839,\"start\":27820},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":28260,\"start\":28252},{\"end\":29099,\"start\":29090},{\"end\":38978,\"start\":38967},{\"end\":39301,\"start\":39293},{\"end\":40910,\"start\":40899},{\"end\":41139,\"start\":41128},{\"end\":42446,\"start\":42435},{\"end\":43947,\"start\":43939},{\"end\":44841,\"start\":44833},{\"end\":45663,\"start\":45655}]", "bib_author_first_name": "[{\"end\":47543,\"start\":47542},{\"end\":47551,\"start\":47550},{\"end\":47562,\"start\":47561},{\"end\":47573,\"start\":47572},{\"end\":47586,\"start\":47582},{\"end\":47590,\"start\":47589},{\"end\":47849,\"start\":47848},{\"end\":48237,\"start\":48232},{\"end\":48252,\"start\":48245},{\"end\":48263,\"start\":48258},{\"end\":48265,\"start\":48264},{\"end\":48286,\"start\":48277},{\"end\":48298,\"start\":48294},{\"end\":48300,\"start\":48299},{\"end\":48690,\"start\":48687},{\"end\":48707,\"start\":48700},{\"end\":48724,\"start\":48719},{\"end\":49027,\"start\":49023},{\"end\":49042,\"start\":49037},{\"end\":49356,\"start\":49347},{\"end\":49627,\"start\":49622},{\"end\":49815,\"start\":49808},{\"end\":49860,\"start\":49856},{\"end\":49878,\"start\":49871},{\"end\":50371,\"start\":50363},{\"end\":50396,\"start\":50395},{\"end\":50412,\"start\":50406},{\"end\":50703,\"start\":50702},{\"end\":51178,\"start\":51174},{\"end\":51180,\"start\":51179},{\"end\":51199,\"start\":51198},{\"end\":51212,\"start\":51206},{\"end\":51214,\"start\":51213},{\"end\":51472,\"start\":51465},{\"end\":51503,\"start\":51498},{\"end\":51521,\"start\":51517},{\"end\":51533,\"start\":51528},{\"end\":51782,\"start\":51776},{\"end\":51796,\"start\":51792},{\"end\":52149,\"start\":52143},{\"end\":52163,\"start\":52159},{\"end\":52439,\"start\":52430},{\"end\":52458,\"start\":52451},{\"end\":52472,\"start\":52466},{\"end\":52826,\"start\":52822},{\"end\":52828,\"start\":52827},{\"end\":52845,\"start\":52844},{\"end\":52859,\"start\":52853},{\"end\":52861,\"start\":52860},{\"end\":53135,\"start\":53132},{\"end\":53154,\"start\":53147},{\"end\":53175,\"start\":53169},{\"end\":53391,\"start\":53390},{\"end\":53510,\"start\":53502},{\"end\":53550,\"start\":53547},{\"end\":53567,\"start\":53560},{\"end\":53863,\"start\":53859},{\"end\":53865,\"start\":53864},{\"end\":53880,\"start\":53875},{\"end\":53882,\"start\":53881},{\"end\":54144,\"start\":54143},{\"end\":54153,\"start\":54152},{\"end\":54164,\"start\":54163},{\"end\":54174,\"start\":54173},{\"end\":54630,\"start\":54629},{\"end\":54803,\"start\":54802},{\"end\":54812,\"start\":54811},{\"end\":54828,\"start\":54822},{\"end\":54832,\"start\":54831},{\"end\":55000,\"start\":54995},{\"end\":55185,\"start\":55184},{\"end\":55195,\"start\":55194},{\"end\":55197,\"start\":55196},{\"end\":55206,\"start\":55205},{\"end\":55217,\"start\":55216},{\"end\":55527,\"start\":55523},{\"end\":55745,\"start\":55740},{\"end\":55747,\"start\":55746},{\"end\":55872,\"start\":55864},{\"end\":55909,\"start\":55903},{\"end\":55935,\"start\":55928},{\"end\":55937,\"start\":55936},{\"end\":56286,\"start\":56278},{\"end\":56323,\"start\":56317},{\"end\":56349,\"start\":56342},{\"end\":56351,\"start\":56350},{\"end\":56640,\"start\":56633},{\"end\":56642,\"start\":56641},{\"end\":56681,\"start\":56674},{\"end\":56698,\"start\":56691},{\"end\":56963,\"start\":56958}]", "bib_author_last_name": "[{\"end\":47548,\"start\":47544},{\"end\":47559,\"start\":47552},{\"end\":47570,\"start\":47563},{\"end\":47580,\"start\":47574},{\"end\":47856,\"start\":47850},{\"end\":48230,\"start\":48221},{\"end\":48243,\"start\":48238},{\"end\":48256,\"start\":48253},{\"end\":48275,\"start\":48266},{\"end\":48292,\"start\":48287},{\"end\":48698,\"start\":48691},{\"end\":48717,\"start\":48708},{\"end\":48730,\"start\":48725},{\"end\":49035,\"start\":49028},{\"end\":49049,\"start\":49043},{\"end\":49369,\"start\":49357},{\"end\":49633,\"start\":49628},{\"end\":49821,\"start\":49816},{\"end\":49828,\"start\":49823},{\"end\":49836,\"start\":49830},{\"end\":49845,\"start\":49838},{\"end\":49854,\"start\":49847},{\"end\":49869,\"start\":49861},{\"end\":49884,\"start\":49879},{\"end\":50355,\"start\":50344},{\"end\":50361,\"start\":50357},{\"end\":50377,\"start\":50372},{\"end\":50384,\"start\":50379},{\"end\":50393,\"start\":50386},{\"end\":50404,\"start\":50397},{\"end\":50419,\"start\":50413},{\"end\":50711,\"start\":50704},{\"end\":50722,\"start\":50713},{\"end\":50836,\"start\":50831},{\"end\":50844,\"start\":50838},{\"end\":50851,\"start\":50846},{\"end\":50857,\"start\":50853},{\"end\":50865,\"start\":50859},{\"end\":50871,\"start\":50867},{\"end\":51188,\"start\":51181},{\"end\":51196,\"start\":51190},{\"end\":51204,\"start\":51200},{\"end\":51220,\"start\":51215},{\"end\":51479,\"start\":51473},{\"end\":51487,\"start\":51481},{\"end\":51496,\"start\":51489},{\"end\":51515,\"start\":51504},{\"end\":51526,\"start\":51522},{\"end\":51538,\"start\":51534},{\"end\":51790,\"start\":51783},{\"end\":51804,\"start\":51797},{\"end\":52157,\"start\":52150},{\"end\":52171,\"start\":52164},{\"end\":52449,\"start\":52440},{\"end\":52464,\"start\":52459},{\"end\":52837,\"start\":52829},{\"end\":52842,\"start\":52839},{\"end\":52851,\"start\":52846},{\"end\":52864,\"start\":52862},{\"end\":53145,\"start\":53136},{\"end\":53167,\"start\":53155},{\"end\":53184,\"start\":53176},{\"end\":53397,\"start\":53392},{\"end\":53414,\"start\":53399},{\"end\":53518,\"start\":53511},{\"end\":53527,\"start\":53520},{\"end\":53534,\"start\":53529},{\"end\":53538,\"start\":53536},{\"end\":53545,\"start\":53540},{\"end\":53558,\"start\":53551},{\"end\":53573,\"start\":53568},{\"end\":53873,\"start\":53866},{\"end\":53888,\"start\":53883},{\"end\":54150,\"start\":54145},{\"end\":54161,\"start\":54154},{\"end\":54171,\"start\":54165},{\"end\":54184,\"start\":54175},{\"end\":54636,\"start\":54631},{\"end\":54809,\"start\":54804},{\"end\":54820,\"start\":54813},{\"end\":55006,\"start\":55001},{\"end\":55192,\"start\":55186},{\"end\":55203,\"start\":55198},{\"end\":55214,\"start\":55207},{\"end\":55227,\"start\":55218},{\"end\":55544,\"start\":55528},{\"end\":55756,\"start\":55748},{\"end\":55878,\"start\":55873},{\"end\":55885,\"start\":55880},{\"end\":55893,\"start\":55887},{\"end\":55901,\"start\":55895},{\"end\":55919,\"start\":55910},{\"end\":55926,\"start\":55921},{\"end\":55945,\"start\":55938},{\"end\":56292,\"start\":56287},{\"end\":56299,\"start\":56294},{\"end\":56307,\"start\":56301},{\"end\":56315,\"start\":56309},{\"end\":56333,\"start\":56324},{\"end\":56340,\"start\":56335},{\"end\":56359,\"start\":56352},{\"end\":56648,\"start\":56643},{\"end\":56652,\"start\":56650},{\"end\":56656,\"start\":56654},{\"end\":56665,\"start\":56658},{\"end\":56672,\"start\":56667},{\"end\":56689,\"start\":56682},{\"end\":56704,\"start\":56699},{\"end\":56973,\"start\":56964}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1703.09207v1\",\"id\":\"b0\"},\"end\":47802,\"start\":47473},{\"attributes\":{\"id\":\"b1\"},\"end\":47961,\"start\":47804},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":146569854},\"end\":48136,\"start\":47963},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":1704893},\"end\":48602,\"start\":48138},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":803179},\"end\":48952,\"start\":48604},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":12856537},\"end\":49252,\"start\":48954},{\"attributes\":{\"doi\":\"arXiv:1703.00056\",\"id\":\"b6\"},\"end\":49559,\"start\":49254},{\"attributes\":{\"doi\":\"arXiv:1412.4643\",\"id\":\"b7\"},\"end\":49778,\"start\":49561},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":13496699},\"end\":50255,\"start\":49780},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":13633339},\"end\":50698,\"start\":50257},{\"attributes\":{\"id\":\"b10\"},\"end\":50781,\"start\":50700},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":7567061},\"end\":51102,\"start\":50783},{\"attributes\":{\"doi\":\"arXiv:1608.00528\",\"id\":\"b12\"},\"end\":51423,\"start\":51104},{\"attributes\":{\"doi\":\"arXiv:1610.09559\",\"id\":\"b13\"},\"end\":51738,\"start\":51425},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":1102398},\"end\":52068,\"start\":51740},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":14637938},\"end\":52371,\"start\":52070},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":12882511},\"end\":52759,\"start\":52373},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":3142952},\"end\":53068,\"start\":52761},{\"attributes\":{\"doi\":\"arXiv:1609.05807\",\"id\":\"b18\"},\"end\":53386,\"start\":53070},{\"attributes\":{\"id\":\"b19\"},\"end\":53500,\"start\":53388},{\"attributes\":{\"doi\":\"arXiv:1511.00830\",\"id\":\"b20\"},\"end\":53800,\"start\":53502},{\"attributes\":{\"id\":\"b21\"},\"end\":54037,\"start\":53802},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":8141514},\"end\":54583,\"start\":54039},{\"attributes\":{\"id\":\"b23\"},\"end\":54758,\"start\":54585},{\"attributes\":{\"id\":\"b24\"},\"end\":54948,\"start\":54760},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":355118},\"end\":55126,\"start\":54950},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":13891907},\"end\":55489,\"start\":55128},{\"attributes\":{\"id\":\"b27\"},\"end\":55663,\"start\":55491},{\"attributes\":{\"id\":\"b28\"},\"end\":55860,\"start\":55665},{\"attributes\":{\"doi\":\"arXiv:1507.05259\",\"id\":\"b29\"},\"end\":56164,\"start\":55862},{\"attributes\":{\"doi\":\"arXiv:1610.08452\",\"id\":\"b30\"},\"end\":56631,\"start\":56166},{\"attributes\":{\"id\":\"b31\"},\"end\":56889,\"start\":56633},{\"attributes\":{\"doi\":\"arXiv:1511.00148\",\"id\":\"b32\"},\"end\":57130,\"start\":56891}]", "bib_title": "[{\"end\":47997,\"start\":47963},{\"end\":48219,\"start\":48138},{\"end\":48685,\"start\":48604},{\"end\":49021,\"start\":48954},{\"end\":49806,\"start\":49780},{\"end\":50342,\"start\":50257},{\"end\":50829,\"start\":50783},{\"end\":51774,\"start\":51740},{\"end\":52141,\"start\":52070},{\"end\":52428,\"start\":52373},{\"end\":52820,\"start\":52761},{\"end\":53857,\"start\":53802},{\"end\":54141,\"start\":54039},{\"end\":54993,\"start\":54950},{\"end\":55182,\"start\":55128}]", "bib_author": "[{\"end\":47550,\"start\":47542},{\"end\":47561,\"start\":47550},{\"end\":47572,\"start\":47561},{\"end\":47582,\"start\":47572},{\"end\":47589,\"start\":47582},{\"end\":47593,\"start\":47589},{\"end\":47858,\"start\":47848},{\"end\":48232,\"start\":48221},{\"end\":48245,\"start\":48232},{\"end\":48258,\"start\":48245},{\"end\":48277,\"start\":48258},{\"end\":48294,\"start\":48277},{\"end\":48303,\"start\":48294},{\"end\":48700,\"start\":48687},{\"end\":48719,\"start\":48700},{\"end\":48732,\"start\":48719},{\"end\":49037,\"start\":49023},{\"end\":49051,\"start\":49037},{\"end\":49371,\"start\":49347},{\"end\":49635,\"start\":49622},{\"end\":49823,\"start\":49808},{\"end\":49830,\"start\":49823},{\"end\":49838,\"start\":49830},{\"end\":49847,\"start\":49838},{\"end\":49856,\"start\":49847},{\"end\":49871,\"start\":49856},{\"end\":49886,\"start\":49871},{\"end\":50357,\"start\":50344},{\"end\":50363,\"start\":50357},{\"end\":50379,\"start\":50363},{\"end\":50386,\"start\":50379},{\"end\":50395,\"start\":50386},{\"end\":50406,\"start\":50395},{\"end\":50421,\"start\":50406},{\"end\":50713,\"start\":50702},{\"end\":50724,\"start\":50713},{\"end\":50838,\"start\":50831},{\"end\":50846,\"start\":50838},{\"end\":50853,\"start\":50846},{\"end\":50859,\"start\":50853},{\"end\":50867,\"start\":50859},{\"end\":50873,\"start\":50867},{\"end\":51190,\"start\":51174},{\"end\":51198,\"start\":51190},{\"end\":51206,\"start\":51198},{\"end\":51222,\"start\":51206},{\"end\":51481,\"start\":51465},{\"end\":51489,\"start\":51481},{\"end\":51498,\"start\":51489},{\"end\":51517,\"start\":51498},{\"end\":51528,\"start\":51517},{\"end\":51540,\"start\":51528},{\"end\":51792,\"start\":51776},{\"end\":51806,\"start\":51792},{\"end\":52159,\"start\":52143},{\"end\":52173,\"start\":52159},{\"end\":52451,\"start\":52430},{\"end\":52466,\"start\":52451},{\"end\":52475,\"start\":52466},{\"end\":52839,\"start\":52822},{\"end\":52844,\"start\":52839},{\"end\":52853,\"start\":52844},{\"end\":52866,\"start\":52853},{\"end\":53147,\"start\":53132},{\"end\":53169,\"start\":53147},{\"end\":53186,\"start\":53169},{\"end\":53399,\"start\":53390},{\"end\":53416,\"start\":53399},{\"end\":53520,\"start\":53502},{\"end\":53529,\"start\":53520},{\"end\":53536,\"start\":53529},{\"end\":53540,\"start\":53536},{\"end\":53547,\"start\":53540},{\"end\":53560,\"start\":53547},{\"end\":53575,\"start\":53560},{\"end\":53875,\"start\":53859},{\"end\":53890,\"start\":53875},{\"end\":54152,\"start\":54143},{\"end\":54163,\"start\":54152},{\"end\":54173,\"start\":54163},{\"end\":54186,\"start\":54173},{\"end\":54638,\"start\":54629},{\"end\":54811,\"start\":54802},{\"end\":54822,\"start\":54811},{\"end\":54831,\"start\":54822},{\"end\":54835,\"start\":54831},{\"end\":55008,\"start\":54995},{\"end\":55194,\"start\":55184},{\"end\":55205,\"start\":55194},{\"end\":55216,\"start\":55205},{\"end\":55229,\"start\":55216},{\"end\":55546,\"start\":55523},{\"end\":55758,\"start\":55740},{\"end\":55880,\"start\":55864},{\"end\":55887,\"start\":55880},{\"end\":55895,\"start\":55887},{\"end\":55903,\"start\":55895},{\"end\":55921,\"start\":55903},{\"end\":55928,\"start\":55921},{\"end\":55947,\"start\":55928},{\"end\":56294,\"start\":56278},{\"end\":56301,\"start\":56294},{\"end\":56309,\"start\":56301},{\"end\":56317,\"start\":56309},{\"end\":56335,\"start\":56317},{\"end\":56342,\"start\":56335},{\"end\":56361,\"start\":56342},{\"end\":56650,\"start\":56633},{\"end\":56654,\"start\":56650},{\"end\":56658,\"start\":56654},{\"end\":56667,\"start\":56658},{\"end\":56674,\"start\":56667},{\"end\":56691,\"start\":56674},{\"end\":56706,\"start\":56691},{\"end\":56975,\"start\":56958}]", "bib_venue": "[{\"end\":50027,\"start\":49965},{\"end\":54323,\"start\":54263},{\"end\":47540,\"start\":47473},{\"end\":47846,\"start\":47804},{\"end\":48006,\"start\":47999},{\"end\":48352,\"start\":48303},{\"end\":48761,\"start\":48732},{\"end\":49086,\"start\":49051},{\"end\":49345,\"start\":49254},{\"end\":49620,\"start\":49561},{\"end\":49963,\"start\":49886},{\"end\":50467,\"start\":50421},{\"end\":50922,\"start\":50873},{\"end\":51172,\"start\":51104},{\"end\":51463,\"start\":51425},{\"end\":51847,\"start\":51806},{\"end\":52206,\"start\":52173},{\"end\":52512,\"start\":52475},{\"end\":52894,\"start\":52866},{\"end\":53130,\"start\":53070},{\"end\":53623,\"start\":53591},{\"end\":53899,\"start\":53890},{\"end\":54261,\"start\":54186},{\"end\":54627,\"start\":54585},{\"end\":54800,\"start\":54760},{\"end\":55026,\"start\":55008},{\"end\":55265,\"start\":55229},{\"end\":55521,\"start\":55491},{\"end\":55738,\"start\":55665},{\"end\":56276,\"start\":56166},{\"end\":56745,\"start\":56706},{\"end\":56956,\"start\":56891}]"}}}, "year": 2023, "month": 12, "day": 17}