{"id": 244478080, "updated": "2023-10-05 19:47:22.989", "metadata": {"title": "MetaFormer Is Actually What You Need for Vision", "authors": "[{\"first\":\"Weihao\",\"last\":\"Yu\",\"middle\":[]},{\"first\":\"Mi\",\"last\":\"Luo\",\"middle\":[]},{\"first\":\"Pan\",\"last\":\"Zhou\",\"middle\":[]},{\"first\":\"Chenyang\",\"last\":\"Si\",\"middle\":[]},{\"first\":\"Yichen\",\"last\":\"Zhou\",\"middle\":[]},{\"first\":\"Xinchao\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Jiashi\",\"last\":\"Feng\",\"middle\":[]},{\"first\":\"Shuicheng\",\"last\":\"Yan\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Transformers have shown great potential in computer vision tasks. A common belief is their attention-based token mixer module contributes most to their competence. However, recent works show the attention-based module in Transformers can be replaced by spatial MLPs and the resulted models still perform quite well. Based on this observation, we hypothesize that the general architecture of the Transformers, instead of the specific token mixer module, is more essential to the model's performance. To verify this, we deliberately replace the attention module in Transformers with an embarrassingly simple spatial pooling operator to conduct only basic token mixing. Surprisingly, we observe that the derived model, termed as PoolFormer, achieves competitive performance on multiple computer vision tasks. For example, on ImageNet-1K, PoolFormer achieves 82.1% top-1 accuracy, surpassing well-tuned Vision Transformer/MLP-like baselines DeiT-B/ResMLP-B24 by 0.3%/1.1% accuracy with 35%/52% fewer parameters and 50%/62% fewer MACs. The effectiveness of PoolFormer verifies our hypothesis and urges us to initiate the concept of\"MetaFormer\", a general architecture abstracted from Transformers without specifying the token mixer. Based on the extensive experiments, we argue that MetaFormer is the key player in achieving superior results for recent Transformer and MLP-like models on vision tasks. This work calls for more future research dedicated to improving MetaFormer instead of focusing on the token mixer modules. Additionally, our proposed PoolFormer could serve as a starting baseline for future MetaFormer architecture design. Code is available at https://github.com/sail-sg/poolformer.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2111.11418", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/YuLZSZWFY22", "doi": "10.1109/cvpr52688.2022.01055"}}, "content": {"source": {"pdf_hash": "57150ca7d793d6f784cf82da1c349edf7beb6bc2", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2111.11418v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "fcc5fb4af233ca23926621e1a4704214871198e2", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/57150ca7d793d6f784cf82da1c349edf7beb6bc2.txt", "contents": "\nMetaFormer Is Actually What You Need for Vision\n\n\nWeihao Yu weihaoyu6@gmail.com \nSea AI Lab\n\n\nNational University of Singapore\n\n\nMi Luo luomi@sea.com \nSea AI Lab\n\n\nPan Zhou zhoupan@sea.com \nSea AI Lab\n\n\nChenyang Si \nSea AI Lab\n\n\nYichen Zhou zhouyc@sea.com \nNational University of Singapore\n\n\nXinchao Wang xinchao@nus.edu.sgcode:https:@poolformer \nSea AI Lab\n\n\nNational University of Singapore\n\n\nJiashi Feng fengjs@sea.com \nSea AI Lab\n\n\nShuicheng Yan \nSea AI Lab\n\n\nMetaFormer Is Actually What You Need for Vision\n\n\n\nMetaFormer as a general architecture abstracted from Transformers [56] by not specifying the token mixer. When using attention/spatial MLP as the token mixer, MetaFormer is instantiated as Transformer/MLP-like models. We argue that the competence of Transformer/MLPlike models primarily stems from the general architecture MetaFormer instead of the equipped specific token mixers. To demonstrate this, we exploit an embarrassingly simple non-parametric operator, pooling, to conduct extremely basic token mixing. Surprisingly, the resulted model PoolFormer consistently outperforms the well-tuned vision Transformer [17] baseline (DeiT [53]) and MLP-like [51] baseline (ResMLP [52]) as shown in (b), which well supports that MetaFormer is actually what we need to achieve competitive performance. RSB-ResNet in (b) means the results are from \"ResNet Strikes Back\" [59] where ResNet [24] are trained with improved training procedure for 300 epochs.\n\n\nAbstract\n\nTransformers have shown great potential in computer vision tasks. A common belief is their attention-based token mixer module contributes most to their competence. However, recent works show the attention-based module in Transformers can be replaced by spatial MLPs and the resulted models still perform quite well. Based on this observation, we hypothesize that the general architecture of the Transformers, instead of the specific token mixer module, is more essential to the model's performance. To verify this, we deliberately replace the attention module in Transformers with an embarrassingly simple spatial pooling operator to conduct only basic token mixing. Surprisingly, we observe that the derived model, termed as PoolFormer,\n\n\nIntroduction\n\nTransformers have gained much interest and success in the computer vision field [3,8,44,55]. Since the seminal work of Vision Transformer (ViT) [17] that adapts pure Transformers to image classification tasks, many follow-up models are developed to make further improvements and achieve promising performance in various computer vision tasks [36,53,63].\n\nThe Transformer encoder, as shown in Figure 1(a), consists of two components. One is the attention module for mixing information among tokens and we term it as token mixer. The other component contains the remaining modules, such as channel MLPs and residual connections. By regarding the attention module as a specific token mixer, we further abstract the overall Transformer into a general architecture MetaFormer where the token mixer is not specified, as shown in Figure 1(a).\n\nThe success of Transformers has been long attributed to the attention-based token mixer [56]. Based on this common belief, many variants of the attention modules [13,22,57,68] have been developed to improve the Vision Transformer. However, a very recent work [51] replaces the attention module completely with spatial MLPs as token mixers, and finds the derived MLP-like model can readily attain competitive performance on image classification benchmarks. The follow-up works [26,35,52] further improve MLP-like models by data-efficient training and specific MLP module design, gradually narrowing the performance gap to ViT and challenging the dominance of attention as token mixers.\n\nSome recent approaches [32,39,40,45] explore other types of token mixers within the MetaFormer architecture, and have demonstrated encouraging performance. For example, [32] replaces attention with Fourier Transform and still achieves around 97% of the accuracy of vanilla Transformers. Taking all these results together, it seems as long as a model adopts MetaFormer as the general architecture, promising results could be attained. We thus hypothesize compared with specific token mixers, MetaFormer is more essential for the model to achieve competitive performance.\n\nTo verify this hypothesis, we apply an extremely simple non-parametric operator, pooling, as the token mixer to conduct only basic token mixing. Astonishingly, this derived model, termed PoolFormer, achieves competitive performance, and even consistently outperforms welltuned Transformer and MLP-like models, including DeiT [53] and ResMLP [52], as shown in Figure 1(b). More specifically, PoolFormer-M36 achieves 82.1% top-1 accuracy on ImageNet-1K classification benchmark, surpassing well-tuned vision Transformer/MLP-like baselines DeiT-B/ResMLP-B24 by 0.3%/1.1% accuracy with 35%/52% fewer parameters and 50%/62% fewer MACs. These results demonstrate that MetaFormer, even with a naive token mixer, can still deliver promising performance. We thus argue that MetaFormer is our de facto need for vision models which is more essential to achieve competitive performance rather than specific token mixers. Note that it does not mean the token mixer is insignificant. MetaFormer still has this abstracted component. It means token mixer is not limited to a specific type, e.g. attention.\n\nThe contributions of our paper are two-fold. Firstly, we abstract Transformers into a general architecture MetaFormer, and empirically demonstrate that the success of Transformer/MLP-like models is largely attributed to the MetaFormer architecture. Specifically, by only employing a simple non-parametric operator, pooling, as an extremely weak token mixer for MetaFormer, we build a simple model named PoolFormer and find it can still achieve highly competitive performance. We hope our findings inspire more future research dedicated to improving MetaFormer instead of focusing on the token mixer modules. Secondly, we evaluate the proposed PoolFormer on multiple vision tasks including image classification [14], object detection [34], instance segmentation [34], and semantic segmentation [67], and find it achieves competitive performance compared with the SOTA models using sophistic design of token mixers. The PoolFormer can readily serve as a good starting baseline for future MetaFormer architecture design.\n\n\nRelated work\n\nTransformers are first proposed by [56] for translation tasks and then rapidly become popular in various NLP tasks. In language pre-training tasks, Transformers are trained on large-scale unlabeled text corpus and achieve amazing performance [2,15]. Inspired by the success of Transformers in NLP, many researchers apply attention mechanism and Transformers to vision tasks [3,8,44,55]. Notably, Chen et al. introduce iGPT [6] where the Transformer is trained to auto-regressively predict pixels on images for self-supervised learning. Dosovitskiy et al. propose Vision Transformer (ViT) with hard patch embedding as input [17]. They show that on supervised image classification tasks, a ViT pre-trained on a large propriety dataset (JFT dataset with 300 million images) can achieve excellent performance. DeiT [53] and T2T-ViT [63] further demonstrate that the ViT pre-trained on only ImageNet-1K (\u223c 1.3 million images) from scratch can achieve promising performance. A lot of works have been focusing on improving the token mixing approach of Transformers by shifted windows [36], relative position encoding [61], refining attention map [68], or incorporating convolution [12,21,60], etc. In addition to attention-like token mixers, [51,52] surprisingly find that merely adopting MLPs as token mixers can still achieve competitive performance. This discovery challenges the dominance of attention-based token mixers and triggers a heated discussion in the research community about which token mixer is better [7,26]. However, the target of this work is neither to be engaged in this debate nor to design new complicated token mixers to achieve new state of the art. Instead, we examine a fundamental question: What is truly responsible for the success of the Transformers and their variants? Our answer is the general architecture i.e., MetaFormer. We simply utilize pooling as basic token mixers to probe the power of MetaFormer. Contemporarily, some works contribute to answering the same question. Dong et al. prove that without residual connections or MLPs, the output converges doubly exponentially to a rank one matrix [16]. Raghu et al. [43] compare the feature difference between ViT and CNNs, finding that self-attention allows early gathering of global information while residual connections greatly propagate features from lower layers to higher ones. Park et al. [42] shows that multi-head self-attentions improve accuracy and generalization by flattening the loss landscapes. Unfortunately, they do not abstract Transformers into a general architecture and study them from the aspect of general framework.\n\n\nMethod\n\n\nMetaFormer\n\nWe present the core concept \"MetaFormer\" for this work at first. As shown in Figure 1, abstracted from Transformers [56], MetaFormer is a general architecture where the token mixer is not specified while the other components are kept the same as Transformers. The input I is first processed by input embedding, such as patch embedding for ViTs [17],\nX = InputEmb(I),(1)\nwhere X \u2208 R N \u00d7C denotes the embedding tokens with sequence length N and embedding dimension C. Then, embedding tokens are fed to repeated MetaFormer blocks, each of which includes two residual sub-blocks. Specifically, the first sub-block mainly contains a token mixer to communicate information among tokens and this sub-block can be expressed as\nY = TokenMixer(Norm(X)) + X,(2)\nwhere Norm(\u00b7) denotes the normalization such as Layer Normalization [1] or Batch Normalization [28]; TokenMixer(\u00b7) means a module mainly working for mixing token information. It is implemented by various attention mechanism in recent vision Transformer models [17,63,68] or spatial MLP in MLP-like models [51,52]. Note that the main function of the token mixer is to propagate token information although some token mixers can also mix channels, like attention. The second sub-block primarily consists of a twolayered MLP with non-linear activation,\nZ = \u03c3(Norm(Y )W 1 )W 2 + Y,(3)\nwhere W 1 \u2208 R C\u00d7rC and W 2 \u2208 R rC\u00d7C are learnable parameters with MLP expansion ratio r; \u03c3(\u00b7) is a non-linear activation function, such as GELU [25] or ReLU [41].\n\nInstantiations of MetaFormer. MetaFormer describes a general architecture with which different models can be obtained immediately by specifying the concrete design of the token mixers. As shown in Figure 1(a), if the token mixer is specified as attention or spatial MLP, MetaFormer then becomes a Transformer or MLP-like model respectively.\n\n\nPoolFormer\n\nFrom the introduction of Transformers [56], lots of works attach much importance to the attention and focus on designing various attention-based token mixer components. In contrast, these works pay little attention to the general architecture, i.e., the MetaFormer.\n\nIn this work, we argue that this MetaFormer general architecture contributes mostly to the success of the recent Transformer and MLP-like models. To demonstrate it, we deliberately employ an embarrassingly simple operator, pooling, as the token mixer. This operator has no learnable parameters and it just makes each token averagely aggregate its nearby token features.\n\nSince this work is targeted at vision tasks, we assume the input is in channel-first data format, i.e., T \u2208 R C\u00d7H\u00d7W . The pooling operator can be expressed as\nT \u2032 :,i,j = 1 K \u00d7 K K p,q=1 T :,i+p\u2212 K+1 2 ,i+q\u2212 K+1 2 \u2212 T :,i,j , (4)\nwhere K is the pooling size. Since the MetaFormer block already has a residual connection, subtraction of the input itself is added in Equation (4). The PyTorch-like code of the pooling is shown in Algorithm 1.  As well known, self-attention and spatial MLP have computational complexity quadratic to the number of tokens to mix. Even worse, spatial MLPs bring much more parameters when handling longer sequences. As a result, self-attention and spatial MLPs usually can only process hundreds of tokens. In contrast, the pooling needs a computational complexity linear to the sequence length without any learnable parameters. Thus, we take advantage of pooling by adopting a hierarchical structure similar to traditional CNNs [24,31,49] and recent hierarchical Transformer variants [36,57]. Figure 2  There are two groups of embedding size: 1) small-sized models with embedding dimensions of 64, 128, 320, and 512 responding to the four stages; 2) medium-sized models with embedding dimensions 96, 192, 384, and 768. Assuming there are L PoolFormer blocks in total, stages 1, 2, 3, and 4 will contain L/6, L/6, L/2, and L/6 PoolFormer blocks respectively. The MLP expansion ratio is set as 4. According to the above simple model scaling rule, we obtain 5 different model sizes of PoolFormer and their hyperparameters are shown in Table 1.\n\n\nExperiments\n\n\nImage classification\n\nSetup. ImageNet-1K [14] is one of the most widely used datasets in computer vision. It contains about 1.3M training images and 50K validation images, covering common 1K classes. Our training scheme mainly follows [53] and [54]. Specifically, MixUp [65], CutMix [64], CutOut [66] and RandAugment [11] are used for data augmentation. The models are trained for 300 epochs using AdamW opti-  Table 2. Performance of different types of models on ImageNet-1K classification. All these models are only trained on the ImageNet-1K training set and the accuracy on the validation set is reported. RSB-ResNet means the results are from \"ResNet Strikes Back\" [59] where ResNet [24] is trained with improved training procedure for 300 epochs. * denotes results of ViT trained with extra regularization from [51]. The numbers of MACs of PoolFormer are counted by fvcore [19] library.   [59] where ResNet [24] is trained with improved training procedure for 300 epochs.\n\nused to help train deep models. We modified Layer Normalization [1] [59] where ResNet [24] is trained with improved training procedure for the same 300 epochs, PoolFormer still performs better. With \u223c 22M parameters/3.7G MACs, RSB-ResNet-34 [59] gets 75.5 accuracy while PoolFormer-S24 can obtain 80.3. Since the local spatial modeling ability of the pooling layer is much worse than the neural convolution layer, the competitive performance of PoolFormer can only be attributed to its general architecture MetaFormer.\n\nWith the pooling operator, each token evenly aggregates the features from its nearby tokens. Thus it is an extremely basic token mixing operation. However, the experiment results show that even with this embarrassingly simple token mixer, MetaFormer still obtains highly competitive performance. Figure 3 clearly shows that PoolFormer surpasses other models with fewer MACs and parameters. This finding conveys that the general architecture MetaFormer is actually what we need when designing vision models. By adopting MetaFormer, it is guaranteed that the derived models would have the potential to achieve reasonable performance.\n\n\nObject detection and instance segmentation\n\nSetup.\n\nWe evaluate PoolFormer on the challenging COCO benchmark [34] that includes 118K training images (train2017) and 5K validation images (val2017). The models are trained on training set and the performance on validation set is reported. PoolFormer is employed as the backbone for two standard detectors, i.e., RetinaNet [33] and Mask R-CNN [23]. ImageNet pre-trained weights are utilized to initialize the backbones and Xavier [20] to initialize the added layers. AdamW [29,37] is adopted for training with an initial learning rate of 1\u00d710 \u22124 and batch size of 16. Following [23,33], we employ 1\u00d7 training schedule, i.e., training the detection models for 12 epochs. The training images are resized into shorter side of 800 pixels and longer side of no more than 1,333 pixels. For testing, the shorter side of the images is also resized to 800 pixels. The imple-mentation is based on the mmdetection [4] codebase and the experiments are run on 8 NVIDIA A100 GPUs.\n\nResults. Equipped with RetinaNet for object detection, PoolFormer-based models consistently outperform their comparable ResNet counterparts as shown in Table 3. For instance, PoolFormer-S12 achieves 36.2 AP, largely surpassing that of ResNet-18 (31.8 AP). Similar results are observed for those models based on Mask R-CNN on object detection and instance segmentation. For example, PoolFormer-S12 largely surpasses ResNet-18 (bounding box AP 37.3 vs. 34.0, and mask AP 34.6 vs. 31.2). Overall, for COCO object detection and instance segmentation, PoolForemrs achieve competitive performance, consistently outperforming those counterparts of ResNet.\n\n\nSemantic segmentation\n\nSetup. ADE20K [67], a challenging scene parsing benchmark, is selected to evaluate the models for semantic segmentation. The dataset includes 20K and 2K images in the training and validation set, respectively, covering 150 finegrained semantic categories. PoolFormers are evaluated as backbones equipped with Semantic FPN [30]. ImageNet-1K trained checkpoints are used to initialize the backbones while Xavier [20] is utilized to initialize other newly added layers. Common practices [5,30] train models for 80K iterations with a batch size of 16. To speed up training, we double the batch size to 32 and decrease the iteration number to 40K. The AdamW [29,37] is employed with an initial learning rate of 2 \u00d7 10 \u22124 that will decay in the polynomial decay schedule with a power of 0.9. Images are resized and cropped into 512 \u00d7 512 for training and are resized to shorter side of 512 pixels for testing. Our implementation is based on the mmsegmentation [10] codebase and the experiments are conducted on 8 NVIDIA A100 GPUs.\n\nResults. Table 4 shows the ADE20K semantic segmentation performance of different backbones using FPN [30]. PoolFormer-based models consistently outperform the models with backbones of CNN-based ResNet [24] and ResNeXt [62] as well as Transformer-based PVT. For instance, PoolFormer-12 achieves mIoU of 37.1, 4.3 and 1.5 better than ResNet-18 and PVT-Tiny, respectively.\n\nThese results demonstrate that our PoorFormer which serves as backbone can attain competitive performance on semantic segmentation although it only utilizes pooling for basically communicating information among tokens. This further indicates the great potential of MetaFormer and supports our claim that MetaFormer is actually what we need.\n\n\nAblation studies\n\nThe experiments of ablation studies are conducted on ImageNet-1K [14]. Table 5 reports the ablation study of PoolFormer. We discuss the ablation below according to the following aspects.  Table 3. Performance of object detection using RetinaNet, and object detection and instance segmentation using Mask R-CNN on COCO val2017 [34]. 1\u00d7 training schedule (i.e.12 epochs) is used for training detection models. AP b and AP m represent bounding box AP and mask AP, respectively.\n\n\nBackbone\n\nSemantic FPN Params (M) mIoU (%) ResNet-18 [24] 15.5 32.9 PVT-Tiny [57] 17.0 35.7 PoolFormer-S12 15.7 37.2 ResNet-50 [24] 28.5 36.7 PVT-Small [57] 28. Token mixers. Compared with Transformers, the main change made by PoolFormer is using simple pooling as a token mixer. We first conduct ablation for this operator by directly replacing pooling with identity mapping. Surprisingly, MetaFormer with identity mapping can still achieve 74.3% top-1 accuracy, supporting the claim that MetaFormer is actually what we need to guarantee reasonable performance.\n\nThen the pooling is replaced with global random matrix W R \u2208 R N\u00d7N for each block. The matrix is initialized with random values from a uniform distribution on the interval [0, 1), and then Softmax is utilized to normalize each row. After random initialization, the matrix parameters are frozen and it conducts token mixing by X \u2032 = W R X where X \u2208 R N\u00d7C are the input token features with the token length of N and channel dimension of C. The token mixer of random matrix introduces extra 21M frozen parameters for the S12 model since the token lengths are extremely large at the first stage. Even with such random token mixing method, the model can still achieve reasonable performance of 75.8% accuracy, 1.5% higher than that of identity mapping. It shows that MetaFormer can still work well even with random token mixing, not to say with other well-designed token mixers.\n\nFurther, pooling is replaced with Depthwise Convolution [9,38] that has learnable parameters for spatial modeling. Not surprisingly, the derived model still achieve highly competitive performance with top-1 accuracy of 78.1%, 0.9% higher than PoolFormer-S12 due to its better local spatial modeling ability. Until now, we have specified multiple token mixers in Metaformer, and all resulted models keep promising results, well supporting the claim that MetaFormer is the key to guaranteeing models' competitiveness. Due to the simplicity of pooling, it is mainly utilized as a tool to demonstrate MetaFormer.\n\nWe test the effects of pooling size on PoolFormer. We observe similar performance when pooling sizes are 3, 5, and 7. However, when the pooling size increases to 9, there is an obvious performance drop of 0.5%. Thus, we adopt the default pooing size of 3 for PoolFormer.\n\nNormalization. We modify Layer Normalization [1] into Modified Layer Normalization (MLN) that computes the mean and variance along token and channel dimensions compared with only channel dimension in vanilla Layer Normalization. The shape of learnable affine parameters of MLN keeps the same as that of Layer Normalization, i.e., R C . MLN can be implemented with GroupNorm API in Py-Torch by setting the group number as 1. See the appendix for details. We find PoolFormer prefers MLN with 0.7% or 0.8% higher than Layer Normalization or Batch Normalization. Thus, MLN is set as default for PoolFormer. When removing normalization, the model can not be trained to converge well, and its performance dramatically drops to only 46.1%.\n\nActivation. We change GELU [25] to ReLU [41] or SiLU [18]. When ReLU is adopted for activation, an obvious performance drop of 0.8% is observed. For SiLU, its performance is almost the same as that of GELU. Thus, we still adopt GELU as default activation.\n\nOther components. Besides token mixer and normalization discussed above, residual connection [24] and channel MLP [46,47] \n\n\nConclusion and future work\n\nIn this work, we abstracted the attention in Transformers as a token mixer, and the overall Transformer as a general architecture termed MetaFormer where the token mixer is not specified. Instead of focusing on specific token mixers, we point out that MetaFormer is actually what we need to guarantee achieving reasonable performance. To verify this, we deliberately specify token mixer as extremely simple pooling for MetaFormer. It is found that the derived Pool-Former model can achieve competitive performance on different vision tasks, which well supports that \"MetaFormer is actually what you need for vision\".\n\nIn the future, we will further evaluate PoolFormer under more different learning settings, such as self-supervised learning and transfer learning. Moreover, it is interesting to see whether PoolFormer still works on NLP tasks to further support the claim \"MetaFormer is actually what you need\" in the NLP domain. We hope that this work can inspire more future research devoted to improving the fundamental architecture MetaFormer instead of paying too much attention to the token mixer modules.\n\n\nA. Detailed hyper-parameters on ImageNet-1K\n\nPoolFormer. On ImageNet-1K classification benchmark, we utilize the hyper-parameters shown in Table 6 to train models in our paper. Based on the relation between batch size and learning rate in Table 6, we set the batch size as 4096 and learning rate as 4 \u00d7 10 \u22123 . For stochastic depth, following the original paper [27], we linearly increase the probability of dropping a layer from 0.0 for the bottom block to d r for the top block.\n\nHybrid Models. We use the hyper-parameters for all models except for the hybrid models with token mixers of pooling and attention. For these hybrid models, we find that they achieve much better performances by setting batch size as 1024, learning rate as 10 \u22123 , and normalization as Layer Normalization [1].\n\n\nB. Training for longer epochs\n\nIn our paper, PoolFormer models are trained for the default 300 epochs on ImageNet-1K. For DeiT [53]/ResMLP [52], it is observed that the performance saturates after 400/800 epochs. Thus, we also conduct the experiments of training longer for PoolFormer-S12 and the results are shown in Table 7. We observe that PoolFormer-S12 obtains saturated performance after around 2000 epochs with a top-1 accuracy improvement of 1.8%. However, for fair comparison with other ViT/MLP-like models, we still train PoolFormers for 300 epochs by default.\n\n\nC. Qualitative results\n\nWe use Grad-CAM [48] to visualize the results of different models trained on ImageNet-1K. We find that although ResMLP [52] also activates some irrelevant parts, all models can locate the semantic objects. The activation parts of DeiT [53] and ResMLP [52] in the maps are more scattered, while those of RSB-ResNet [24,59] and PoolFormer are more gathered.\n\n\nD. Comparison between Layer Normalization and Modified Layer Normalization\n\nWe modify Layer Normalization [1] into Modified Layer Normalization (MNN). It computes the mean and variance along spatial and channel dimensions, compared with only channel dimension in vanilla Layer Normalization. The shape of learnable affine parameters of MLN keeps the same as that of Layer Normalization, i.e., R C . MLN can be implemented with GroupNorm API in PyTorch by setting the group number as 1. The comparison details are shown in Algorithm 2.\n\n\nE. Code in PyTorch\n\nWe provide the PyTorch-like code in Algorithm 3 associated with the modules used in the PoolFormer block. Algorithm 4 further shows the PoolFormer block built with these modules. \n\n\nPoolFormer\n\nFigure 1 .\n1MetaFormer and performance of MetaFormer-based models on ImageNet-1K validation set. As shown in (a), we present\n\nFigure 2 .\n2(a) The overall framework of PoolFormer. Similar to[24,36,57], PoolFormer adopts hierarchical architecture with 4 stages. For a model with L PoolFormer blocks, stage[1,2,3,4] have [L/6, L/6, L/2, L/6] blocks, respectively. The feature dimension Di of stage i is shown in thefigure. (b)The architecture of PoolFormer block. Compared with Transformer block, it replaces attention with extremely simple non-parametric operator, pooling, to conduct only basic token mixing.\n\nFigure 3 .\n3ImageNet-1K validation accuracy vs. MACs/Model Size. RSB-ResNet means the results are from \"ResNet Strikes Back\"\n\n\nAlgorithm 1 Pooling for PoolFormer, PyTorch-like Codeimport torch.nn as nn \n\nclass Pooling(nn.Module): \ndef __init__(self, pool_size=3): \nsuper().__init__() \nself.pool = nn.AvgPool2d( \npool_size, stride=1, \npadding=pool_size//2, \ncount_include_pad=False, \n) \ndef forward(self, x): \n\"\"\" \n[B, C, H, W] = x.shape \nSubtraction of the input itself is added \nsince the block already has a \nresidual connection. \n\"\"\" \nreturn self.pool(x) -x \n\n\n\n\nshows the overall framework of Pool-Former. Specifically, PoolFormer has 4 stages with H and W represent the width and height of the input image.4 \u00d7 W \n4 , \n\nH \n\n8 \u00d7 W \n8 , H \n16 \u00d7 W \n16 , and H \n32 \u00d7 W \n32 tokens respectively, where \nH \n\n\nare two other important components in MetaFormer. Without residual connection or channel MLP, Ablation for PoolFormer on ImageNet-1K classification benchmark. PoolFormer-S12 is utilized as the baseline to conduct ablation study. The top-1 accuracy on the validation set is reported. * This token mixer utilizes global random matrix WR \u2208 R N \u00d7N (parameters are frozen after random initialization) to conduct token mixing by X \u2032 = WRX where X \u2208 R N \u00d7C are input tokens with the token length of N and channel dimension of C. \u2020 Modified Layer Normalization (MLN) computes the mean and variance along token and channel dimensions compared with vanilla Layer Normalization only along channel dimension. MLN can be implemented with GroupNorm API in PyTorch by specifying the group number equal to 1. The numbers of MACs are counted by fvcore[19] library. the model cannot converge and only achieves the accuracy of 0.1%/5.7%, proving the indispensability of these parts.Hybrid stages. Among token mixers based on pooling, attention, and spatial MLP, the pooling-based one can handle much longer input sequences while attention and spatial MLP are good at capturing global information. Therefore, it is intuitive to stack MetaFormers with pooling in the bottom stages to handle long sequences and use attention or spatial MLP-based mixer in the top stages, considering the sequences have been largely shortened. Thus, we replace the token mixer pooling with attention or spatial FC 1 in the top one or two stages in PoolFormer. From Table 5, the hybrid models perform quite well. The variant with pooling in the bottom two stages and attention in the top two stages delivers highly competitive performance. It achieves 81.0% accuracy with only 16.5M parameters and 2.5G MACs. As a comparison, ResMLP-B24 needs 7.0\u00d7 parameters (116M) and 9.2\u00d7 MACs (23.0G) to achieve the same accuracy. These results indicate that combining pooling with other token mixers for MetaFormer may be a promising direction to further improve the performance.Ablation \n\nVariant \nParams (M) MACs (G) Top-1 (%) \nBaseline \nNone (PoolFormer-S12) \n11.9 \n1.8 \n77.2 \n\nToken mixers \n\nPooling \u2192 Identity mapping \n11.9 \n1.8 \n74.3 \nPooling \u2192 Global random matrix  *  (extra 21M frozen parameters) \n11.9 \n3.3 \n75.8 \nPooling \u2192 Depthwise Convolution [9, 38] \n11.9 \n1.8 \n78.1 \nPooling size 3 \u2192 5 \n11.9 \n1.8 \n77.2 \nPooling size 3 \u2192 7 \n11.9 \n1.8 \n77.1 \nPooling size 3 \u2192 9 \n11.9 \n1.8 \n76.8 \n\nNormalization \n\nModified Layer Normalization  \u2020 \u2192 Layer Normalization [1] \n11.9 \n1.8 \n76.5 \nModified Layer Normalization  \u2020 \u2192 Batch Normalization [28] \n11.9 \n1.8 \n76.4 \nModified Layer Normalization  \u2020 \u2192 None \n11.9 \n1.8 \n46.1 \n\nActivation \nGELU [25] \u2192 ReLU [41] \n11.9 \n1.8 \n76.4 \nGELU \u2192 SiLU [18] \n11.9 \n1.8 \n77.2 \n\nOther components \nResidual connection [25] \u2192 None \n11.9 \n1.8 \n0.1 \nChannel MLP \u2192 None \n2.5 \n0.2 \n5.7 \n\nHybrid Stages \n\n[Pool, Pool, Pool, Pool] \u2192 [Pool, Pool, Pool, Attention] \n14.0 \n1.9 \n78.3 \n[Pool, Pool, Pool, Pool] \u2192 [Pool, Pool, Attention, Attention] \n16.5 \n2.5 \n81.0 \n[Pool, Pool, Pool, Pool] \u2192 [Pool, Pool, Pool, SpatialFC] \n11.9 \n1.8 \n77.5 \n[Pool, Pool, Pool, Pool] \u2192 [Pool, Pool, SpatialFC, SpatialFC] \n12.2 \n1.9 \n77.9 \n\nTable 5. \n\nTable 6 .Table 7 .\n67Batch size used in the paper 4096 Peak learning rate used in the paper 4 \u00d7 10 \u22124 Hyper-parameters for image classification on ImageNet-1K Performance of PoolFormer trained for different numbers of epochs.S12 \nS24 \nS36 \nM36 M48 \n\n\nFollowing[52], we use only one spatial fully connected layer as a token mixer, so we call it FC.\nAcknowledgementThe authors would like to thank Quanhong Fu at Sea AI Lab for the help to improve the technical writing aspect of this paper. Weihao Yu would like to thank TPU Research Cloud (TRC) program and Google Cloud research credits for the support of partial computational resources. This project is in part supported by NUS Faculty Research Committee Grant (WBS: A-0009440-00-00). Shuicheng Yan and Xinchao Wang are the corresponding authors.x = x + self.drop_path( self.layer_scale_1.unsqueeze(-1).unsqueeze(-1) * self.token_mixer(self.norm1(x))) x = x + self.drop_path( self.layer_scale_2.unsqueeze(-1).unsqueeze(-1) * self.mlp(self.norm2(x))) else:x = x + self.drop_path(self.token_mixer(self.norm1(x))) x = x + self.drop_path(self.mlp(self.norm2(x))) return x\nToken Mixer Outcome Model Image Size Params (M) MACs (G) Top-1 (%). Token Mixer Outcome Model Image Size Params (M) MACs (G) Top-1 (%)\n\nConvolutional Neural Netowrks -RSB-ResNet-18. 24224Convolutional Neural Netowrks - RSB-ResNet-18 [24, 59] 224\n\n. Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E Hin, arXiv:1607.06450812ton. Layer normalization. arXiv preprintJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin- ton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. 3, 5, 7, 8, 12\n\nAlec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems. H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. LinScott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish33Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub- biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan- tan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand- hini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Infor- mation Processing Systems, volume 33, pages 1877-1901.\n\nEnd-toend object detection with transformers. Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko, European Conference on Computer Vision. SpringerNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to- end object detection with transformers. In European Confer- ence on Computer Vision, pages 213-229. Springer, 2020. 2\n\nKai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tianheng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue Wu, Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang, arXiv:1906.07155Chen Change Loy, and Dahua Lin. MMDetection: Open mmlab detection toolbox and benchmark. arXiv preprintKai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tian- heng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue Wu, Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang, Chen Change Loy, and Dahua Lin. MMDetection: Open mmlab detection toolbox and benchmark. arXiv preprint arXiv:1906.07155, 2019. 6\n\nDeeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan L Yuille, IEEE transactions on pattern analysis and machine intelligence. 40Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolu- tion, and fully connected crfs. IEEE transactions on pattern analysis and machine intelligence, 40(4):834-848, 2017. 6\n\nGenerative pretraining from pixels. Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, Ilya Sutskever, PMLR, 2020. 2International Conference on Machine Learning. Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Hee- woo Jun, David Luan, and Ilya Sutskever. Generative pre- training from pixels. In International Conference on Ma- chine Learning, pages 1691-1703. PMLR, 2020. 2\n\nCyclemlp: A mlp-like architecture for dense prediction. Shoufa Chen, Enze Xie, Chongjian Ge, Ding Liang, Ping Luo, arXiv:2107.10224arXiv preprintShoufa Chen, Enze Xie, Chongjian Ge, Ding Liang, and Ping Luo. Cyclemlp: A mlp-like architecture for dense prediction. arXiv preprint arXiv:2107.10224, 2021. 3\n\nA\u02c62-nets: Double attention networks. Yunpeng Chen, Yannis Kalantidis, Jianshu Li, Shuicheng Yan, Jiashi Feng, Advances in Neural Information Processing Systems. 31Yunpeng Chen, Yannis Kalantidis, Jianshu Li, Shuicheng Yan, and Jiashi Feng. A\u02c62-nets: Double attention net- works. Advances in Neural Information Processing Systems, 31:352-361, 2018. 2\n\nXception: Deep learning with depthwise separable convolutions. Fran\u00e7ois Chollet, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition7Fran\u00e7ois Chollet. Xception: Deep learning with depthwise separable convolutions. In Proceedings of the IEEE con- ference on computer vision and pattern recognition, pages 1251-1258, 2017. 7, 8\n\nMMSegmentation: Openmmlab semantic segmentation toolbox and benchmark. MMSegmentation Contributors. MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark. https : / / github . com / open - mmlab/mmsegmentation, 2020. 6\n\nRandaugment: Practical automated data augmentation with a reduced search space. Barret Ekin D Cubuk, Jonathon Zoph, Quoc V Shlens, Le, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. the IEEE/CVF Conference on Computer Vision and Pattern Recognition WorkshopsEkin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmenta- tion with a reduced search space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 702-703, 2020. 4\n\nHugo St\u00e9phane D&apos;ascoli, Matthew Touvron, Ari Leavitt, Giulio Morcos, Levent Biroli, Sagun, arXiv:2103.10697Convit: Improving vision transformers with soft convolutional inductive biases. arXiv preprintSt\u00e9phane d'Ascoli, Hugo Touvron, Matthew Leavitt, Ari Morcos, Giulio Biroli, and Levent Sagun. Convit: Improving vision transformers with soft convolutional inductive biases. arXiv preprint arXiv:2103.10697, 2021. 2\n\nConvit: Improving vision transformers with soft convolutional inductive biases. Hugo St\u00e9phane D&apos;ascoli, Touvron, L Matthew, Ari S Leavitt, Giulio Morcos, Levent Biroli, Sagun, PMLRProceedings of the 38th International Conference on Machine Learning. Marina Meila and Tong Zhangthe 38th International Conference on Machine Learning139St\u00e9phane D'Ascoli, Hugo Touvron, Matthew L Leavitt, Ari S Morcos, Giulio Biroli, and Levent Sagun. Convit: Improv- ing vision transformers with soft convolutional inductive bi- ases. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 2286-2296. PMLR, 18-24 Jul 2021. 2\n\nImagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, 2009 IEEE conference on computer vision and pattern recognition. Ieee6Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248-255. Ieee, 2009. 2, 4, 6\n\nBert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, NAACL-HLT (1). Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional trans- formers for language understanding. In NAACL-HLT (1), 2019. 2\n\nAttention is not all you need: Pure attention loses rank doubly exponentially with depth. Yihe Dong, Jean-Baptiste Cordonnier, Andreas Loukas, arXiv:2103.03404arXiv preprintYihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. Attention is not all you need: Pure attention loses rank doubly exponentially with depth. arXiv preprint arXiv:2103.03404, 2021. 3\n\nSylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, International Conference on Learning Representations. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, et al. An image is worth 16x16 words: Trans- formers for image recognition at scale. In International Con- ference on Learning Representations, 2020. 1, 2, 3, 5\n\nSigmoidweighted linear units for neural network function approximation in reinforcement learning. Stefan Elfwing, Eiji Uchibe, Kenji Doya, Neural Networks. 1078Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid- weighted linear units for neural network function approxima- tion in reinforcement learning. Neural Networks, 107:3-11, 2018. 7, 8\n\n. Fvcore Contributors, 5fvcore Contributors. fvcore. https://github.com/ facebookresearch/fvcore, 2021. 4, 5, 8\n\nUnderstanding the difficulty of training deep feedforward neural networks. Xavier Glorot, Yoshua Bengio, Proceedings of the thirteenth international conference on artificial intelligence and statistics. the thirteenth international conference on artificial intelligence and statisticsJMLR Workshop and Conference ProceedingsXavier Glorot and Yoshua Bengio. Understanding the diffi- culty of training deep feedforward neural networks. In Pro- ceedings of the thirteenth international conference on artifi- cial intelligence and statistics, pages 249-256. JMLR Work- shop and Conference Proceedings, 2010. 6\n\nJianyuan Guo, Kai Han, Han Wu, Chang Xu, Yehui Tang, Chunjing Xu, Yunhe Wang, Cmt, arXiv:2107.06263Convolutional neural networks meet vision transformers. arXiv preprintJianyuan Guo, Kai Han, Han Wu, Chang Xu, Yehui Tang, Chunjing Xu, and Yunhe Wang. Cmt: Convolutional neural networks meet vision transformers. arXiv preprint arXiv:2107.06263, 2021. 2\n\nTransformer in transformer. Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, Yunhe Wang, arXiv:2103.00112arXiv preprintKai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in transformer. arXiv preprint arXiv:2103.00112, 2021. 2\n\nPiotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. Kaiming He, Georgia Gkioxari, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionKaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Gir- shick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 2961-2969, 2017. 6\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition712Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016. 1, 4, 5, 6, 7, 12\n\nGaussian error linear units (gelus). Dan Hendrycks, Kevin Gimpel, arXiv:1606.084157arXiv preprintDan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. 3, 7, 8\n\nVision permutator: A permutable mlp-like architecture for visual recognition. Qibin Hou, Zihang Jiang, Li Yuan, Ming-Ming Cheng, Shuicheng Yan, Jiashi Feng, arXiv:2106.1236823arXiv preprintQibin Hou, Zihang Jiang, Li Yuan, Ming-Ming Cheng, Shuicheng Yan, and Jiashi Feng. Vision permutator: A per- mutable mlp-like architecture for visual recognition. arXiv preprint arXiv:2106.12368, 2021. 2, 3\n\nDeep networks with stochastic depth. Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, Kilian Q Weinberger, European conference on computer vision. Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kil- ian Q Weinberger. Deep networks with stochastic depth. In European conference on computer vision, pages 646-661.\n\n. Springer, 412Springer, 2016. 4, 12\n\nBatch normalization: Accelerating deep network training by reducing internal covariate shift. Sergey Ioffe, Christian Szegedy, PMLRInternational conference on machine learning. 3Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal co- variate shift. In International conference on machine learn- ing, pages 448-456. PMLR, 2015. 3, 8\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.698046arXiv preprintDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 4, 6\n\nPanoptic feature pyramid networks. Alexander Kirillov, Ross Girshick, Kaiming He, Piotr Doll\u00e1r, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition67Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Panoptic feature pyramid networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pat- tern Recognition, pages 6399-6408, 2019. 6, 7\n\nImagenet classification with deep convolutional neural networks. Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, Advances in neural information processing systems. 25Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural net- works. Advances in neural information processing systems, 25:1097-1105, 2012. 4\n\nJames Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon, arXiv:2105.03824Fnet: Mixing tokens with fourier transforms. arXiv preprintJames Lee-Thorp, Joshua Ainslie, Ilya Eckstein, and Santi- ago Ontanon. Fnet: Mixing tokens with fourier transforms. arXiv preprint arXiv:2105.03824, 2021. 2\n\nKaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. Tsung-Yi Lin, Priya Goyal, Ross Girshick, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionTsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. In Pro- ceedings of the IEEE international conference on computer vision, pages 2980-2988, 2017. 6\n\nMicrosoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, C Lawrence Zitnick, European conference on computer vision. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740-755.\n\n. Springer, 7Springer, 2014. 2, 6, 7\n\nHanxiao Liu, Zihang Dai, R David, Quoc V So, Le, arXiv:2105.08050Pay attention to mlps. 6arXiv preprintHanxiao Liu, Zihang Dai, David R So, and Quoc V Le. Pay attention to mlps. arXiv preprint arXiv:2105.08050, 2021. 2, 5, 6\n\nSwin transformer: Hierarchical vision transformer using shifted windows. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)5Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin trans- former: Hierarchical vision transformer using shifted win- dows. In Proceedings of the IEEE/CVF International Con- ference on Computer Vision (ICCV), pages 10012-10022, October 2021. 2, 4, 5\n\nDecoupled weight decay regularization. Ilya Loshchilov, Frank Hutter, International Conference on Learning Representations. 46Ilya Loshchilov and Frank Hutter. Decoupled weight de- cay regularization. In International Conference on Learning Representations, 2018. 4, 6\n\nSimplifying convnets for fast learning. Franck Mamalet, Christophe Garcia, International Conference on Artificial Neural Networks. Springer7Franck Mamalet and Christophe Garcia. Simplifying con- vnets for fast learning. In International Conference on Arti- ficial Neural Networks, pages 58-65. Springer, 2012. 7, 8\n\nSparse and continuous attention mechanisms. Andr\u00e9 Martins, Ant\u00f3nio Farinhas, Marcos Treviso, Vlad Niculae, Pedro Aguiar, Mario Figueiredo, Advances in Neural Information Processing Systems. H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. LinCurran Associates, Inc33Andr\u00e9 Martins, Ant\u00f3nio Farinhas, Marcos Treviso, Vlad Niculae, Pedro Aguiar, and Mario Figueiredo. Sparse and continuous attention mechanisms. In H. Larochelle, M. Ran- zato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 20989-21001. Curran Associates, Inc., 2020. 2\n\nPedro Henrique Martins, arXiv:2109.00301Zita Marinho, and Andr\u00e9 FT Martins. \u221e-former: Infinite memory transformer. arXiv preprintPedro Henrique Martins, Zita Marinho, and Andr\u00e9 FT Mar- tins. \u221e-former: Infinite memory transformer. arXiv preprint arXiv:2109.00301, 2021. 2\n\nRectified linear units improve restricted boltzmann machines. Vinod Nair, Geoffrey E Hinton, Icml. 7Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In Icml, 2010. 3, 7, 8\n\nHow do vision transformers work. Namuk Park, Songkuk Kim, International Conference on Learning Representations. 2022Namuk Park and Songkuk Kim. How do vision transformers work? In International Conference on Learning Represen- tations, 2022. 3\n\nDo vision transformers see like convolutional neural networks?. Maithra Raghu, Thomas Unterthiner, Simon Kornblith, Chiyuan Zhang, Alexey Dosovitskiy, arXiv:2108.08810arXiv preprintMaithra Raghu, Thomas Unterthiner, Simon Kornblith, Chiyuan Zhang, and Alexey Dosovitskiy. Do vision trans- formers see like convolutional neural networks? arXiv preprint arXiv:2108.08810, 2021. 3\n\nStand-alone selfattention in vision models. Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, Jon Shlens, Advances in Neural Information Processing Systems. 322Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jon Shlens. Stand-alone self- attention in vision models. Advances in Neural Information Processing Systems, 32, 2019. 2\n\nGlobal filter networks for image classification. Yongming Rao, Wenliang Zhao, Zheng Zhu, Jiwen Lu, Jie Zhou, arXiv:2107.00645arXiv preprintYongming Rao, Wenliang Zhao, Zheng Zhu, Jiwen Lu, and Jie Zhou. Global filter networks for image classification. arXiv preprint arXiv:2107.00645, 2021. 2\n\nPrinciples of neurodynamics. perceptrons and the theory of brain mechanisms. Frank Rosenblatt, Cornell Aeronautical Lab Inc Buffalo NYTechnical reportFrank Rosenblatt. Principles of neurodynamics. perceptrons and the theory of brain mechanisms. Technical report, Cor- nell Aeronautical Lab Inc Buffalo NY, 1961. 7\n\nLearning internal representations by error propagation. Geoffrey E David E Rumelhart, Ronald J Hinton, Williams, California Univ San Diego La Jolla Inst for Cognitive ScienceTechnical reportDavid E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning internal representations by error propa- gation. Technical report, California Univ San Diego La Jolla Inst for Cognitive Science, 1985. 7\n\nGrad-cam: Visual explanations from deep networks via gradient-based localization. R Ramprasaath, Michael Selvaraju, Abhishek Cogswell, Ramakrishna Das, Devi Vedantam, Dhruv Parikh, Batra, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision1214Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE in- ternational conference on computer vision, pages 618-626, 2017. 12, 14\n\nVery deep convolutional networks for large-scale image recognition. Karen Simonyan, Andrew Zisserman, 3rd International Conference on Learning Representations. San Diego, CA, USAConference Track ProceedingsKaren Simonyan and Andrew Zisserman. Very deep con- volutional networks for large-scale image recognition. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceed- ings, 2015. 4\n\nRethinking the inception architecture for computer vision. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, Zbigniew Wojna, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception archi- tecture for computer vision. In Proceedings of the IEEE con- ference on computer vision and pattern recognition, pages 2818-2826, 2016. 4\n\nMlp-mixer: An all-mlp architecture for vision. Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, arXiv:2105.01601arXiv preprintIlya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lu- cas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al. Mlp-mixer: An all-mlp architecture for vision. arXiv preprint arXiv:2105.01601, 2021. 1, 2, 3, 5\n\nHugo Touvron, Piotr Bojanowski, Mathilde Caron, Matthieu Cord, Alaaeldin El-Nouby, Edouard Grave, Gautier Izacard, Armand Joulin, Gabriel Synnaeve, Jakob Verbeek, arXiv:2105.03404Feedforward networks for image classification with data-efficient training. 1214arXiv preprintHugo Touvron, Piotr Bojanowski, Mathilde Caron, Matthieu Cord, Alaaeldin El-Nouby, Edouard Grave, Gautier Izac- ard, Armand Joulin, Gabriel Synnaeve, Jakob Verbeek, et al. Resmlp: Feedforward networks for image classification with data-efficient training. arXiv preprint arXiv:2105.03404, 2021. 1, 2, 3, 5, 6, 8, 12, 14\n\nTraining data-efficient image transformers & distillation through attention. Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Herv\u00e9 J\u00e9gou, PMLRInternational Conference on Machine Learning. 1214Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through at- tention. In International Conference on Machine Learning, pages 10347-10357. PMLR, 2021. 1, 2, 4, 5, 6, 12, 14\n\nHugo Touvron, Matthieu Cord, arXiv:2103.17239Alexandre Sablayrolles, Gabriel Synnaeve, and Herv\u00e9 J\u00e9gou. Going deeper with image transformers. arXiv preprintHugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Herv\u00e9 J\u00e9gou. Going deeper with im- age transformers. arXiv preprint arXiv:2103.17239, 2021.\n\nScaling local self-attention for parameter efficient visual backbones. Ashish Vaswani, Prajit Ramachandran, Aravind Srinivas, Niki Parmar, Blake Hechtman, Jonathon Shlens, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionAshish Vaswani, Prajit Ramachandran, Aravind Srinivas, Niki Parmar, Blake Hechtman, and Jonathon Shlens. Scaling local self-attention for parameter efficient visual backbones. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12894-12904, 2021. 2\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. 13Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998-6008, 2017. 1, 2, 3\n\nPyramid vision transformer: A versatile backbone for dense prediction without convolutions. Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, Ling Shao, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)67Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 568-578, October 2021. 2, 4, 5, 6, 7\n\nPytorch image models. Ross Wightman, Ross Wightman. Pytorch image models. https : / / github . com / rwightman / pytorch -image - models, 2019. 5\n\nResnet strikes back: An improved training procedure in timm. Ross Wightman, Hugo Touvron, Herv\u00e9 J\u00e9gou, arXiv:2110.00476614arXiv preprintRoss Wightman, Hugo Touvron, and Herv\u00e9 J\u00e9gou. Resnet strikes back: An improved training procedure in timm. arXiv preprint arXiv:2110.00476, 2021. 1, 5, 6, 12, 14\n\nHaiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, Lei Zhang, arXiv:2103.15808Cvt: Introducing convolutions to vision transformers. arXiv preprintHaiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introduc- ing convolutions to vision transformers. arXiv preprint arXiv:2103.15808, 2021. 2\n\nRethinking and improving relative position encoding for vision transformer. Kan Wu, Houwen Peng, Minghao Chen, Jianlong Fu, Hongyang Chao, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionKan Wu, Houwen Peng, Minghao Chen, Jianlong Fu, and Hongyang Chao. Rethinking and improving relative posi- tion encoding for vision transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10033-10041, 2021. 2\n\nAggregated residual transformations for deep neural networks. Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, Kaiming He, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition67Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1492-1500, 2017. 6, 7\n\nTokens-to-token vit: Training vision transformers from scratch on imagenet. Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis E H Tay, Jiashi Feng, Shuicheng Yan, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)23Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis E.H. Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. In Proceedings of the IEEE/CVF In- ternational Conference on Computer Vision (ICCV), pages 558-567, October 2021. 2, 3\n\nCutmix: Regularization strategy to train strong classifiers with localizable features. Sangdoo Yun, Dongyoon Han, Sanghyuk Seong Joon Oh, Junsuk Chun, Youngjoon Choe, Yoo, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionSangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regular- ization strategy to train strong classifiers with localizable fea- tures. In Proceedings of the IEEE/CVF International Con- ference on Computer Vision, pages 6023-6032, 2019. 4\n\nmixup: Beyond empirical risk minimization. Hongyi Zhang, Moustapha Cisse, David Yann N Dauphin, Lopez-Paz, International Conference on Learning Representations. Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimiza- tion. In International Conference on Learning Representa- tions, 2018. 4\n\nRandom erasing data augmentation. Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, Yi Yang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 13001-13008, 2020. 4\n\nScene parsing through ade20k dataset. Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, Antonio Torralba, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition67Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 633-641, 2017. 2, 6, 7\n\nRefiner: Refining self-attention for vision transformers. Daquan Zhou, Yujun Shi, Bingyi Kang, Weihao Yu, Zihang Jiang, Yuan Li, Xiaojie Jin, Qibin Hou, Jiashi Feng, arXiv:2106.0371423arXiv preprintDaquan Zhou, Yujun Shi, Bingyi Kang, Weihao Yu, Zihang Jiang, Yuan Li, Xiaojie Jin, Qibin Hou, and Jiashi Feng. Re- finer: Refining self-attention for vision transformers. arXiv preprint arXiv:2106.03714, 2021. 2, 3\n\n. Deit-Small, ResMLP-S24 [5253DeiT-small [53] ResMLP-S24 [52]\n\n. Poolformer-S24, PoolFormer-S24\n\nGrad-CAM [48] activation maps of the models trained on ImageNet-1K. The visualized images are from validation set. Figure 4.Figure 4. Grad-CAM [48] activation maps of the models trained on ImageNet-1K. The visualized images are from validation set.\n", "annotations": {"author": "[{\"end\":129,\"start\":51},{\"end\":164,\"start\":130},{\"end\":203,\"start\":165},{\"end\":229,\"start\":204},{\"end\":292,\"start\":230},{\"end\":395,\"start\":293},{\"end\":436,\"start\":396},{\"end\":464,\"start\":437}]", "publisher": null, "author_last_name": "[{\"end\":60,\"start\":58},{\"end\":136,\"start\":133},{\"end\":173,\"start\":169},{\"end\":215,\"start\":213},{\"end\":241,\"start\":237},{\"end\":305,\"start\":301},{\"end\":407,\"start\":403},{\"end\":450,\"start\":447}]", "author_first_name": "[{\"end\":57,\"start\":51},{\"end\":132,\"start\":130},{\"end\":168,\"start\":165},{\"end\":212,\"start\":204},{\"end\":236,\"start\":230},{\"end\":300,\"start\":293},{\"end\":402,\"start\":396},{\"end\":446,\"start\":437}]", "author_affiliation": "[{\"end\":93,\"start\":82},{\"end\":128,\"start\":95},{\"end\":163,\"start\":152},{\"end\":202,\"start\":191},{\"end\":228,\"start\":217},{\"end\":291,\"start\":258},{\"end\":359,\"start\":348},{\"end\":394,\"start\":361},{\"end\":435,\"start\":424},{\"end\":463,\"start\":452}]", "title": "[{\"end\":48,\"start\":1},{\"end\":512,\"start\":465}]", "venue": null, "abstract": null, "bib_ref": "[{\"attributes\":{\"ref_id\":\"b59\"},\"end\":586,\"start\":582},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":1136,\"start\":1132},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":1156,\"start\":1152},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":1175,\"start\":1171},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":1197,\"start\":1193},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":1384,\"start\":1380},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":1402,\"start\":1398},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2313,\"start\":2310},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2315,\"start\":2313},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":2318,\"start\":2315},{\"end\":2321,\"start\":2318},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2378,\"start\":2374},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":2576,\"start\":2572},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":2579,\"start\":2576},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":2582,\"start\":2579},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":3159,\"start\":3155},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3233,\"start\":3229},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3236,\"start\":3233},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":3239,\"start\":3236},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":3242,\"start\":3239},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":3330,\"start\":3326},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3547,\"start\":3543},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":3550,\"start\":3547},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":3553,\"start\":3550},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":3780,\"start\":3776},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":3783,\"start\":3780},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":3786,\"start\":3783},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":3789,\"start\":3786},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":3926,\"start\":3922},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":4653,\"start\":4649},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":4669,\"start\":4665},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6129,\"start\":6125},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":6152,\"start\":6148},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":6180,\"start\":6176},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":6212,\"start\":6208},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":6488,\"start\":6484},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6694,\"start\":6691},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6697,\"start\":6694},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6826,\"start\":6823},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6828,\"start\":6826},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":6831,\"start\":6828},{\"end\":6834,\"start\":6831},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6875,\"start\":6872},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7076,\"start\":7072},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":7264,\"start\":7260},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":7281,\"start\":7277},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":7530,\"start\":7526},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":7563,\"start\":7559},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":7592,\"start\":7588},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7627,\"start\":7623},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7630,\"start\":7627},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":7633,\"start\":7630},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":7688,\"start\":7684},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":7691,\"start\":7688},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7963,\"start\":7960},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7966,\"start\":7963},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8580,\"start\":8576},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":8599,\"start\":8595},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":8830,\"start\":8826},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":9213,\"start\":9209},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9441,\"start\":9437},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9915,\"start\":9912},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9943,\"start\":9939},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10108,\"start\":10104},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":10111,\"start\":10108},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":10114,\"start\":10111},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":10153,\"start\":10149},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":10156,\"start\":10153},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10572,\"start\":10568},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":10585,\"start\":10581},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":10985,\"start\":10981},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":12541,\"start\":12537},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":12544,\"start\":12541},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":12547,\"start\":12544},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":12597,\"start\":12593},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":12600,\"start\":12597},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":13211,\"start\":13207},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":13405,\"start\":13401},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":13414,\"start\":13410},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":13440,\"start\":13436},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":13453,\"start\":13449},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":13466,\"start\":13462},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":13487,\"start\":13483},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":13840,\"start\":13836},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":13858,\"start\":13854},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":13987,\"start\":13983},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":14049,\"start\":14045},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":14065,\"start\":14061},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":14083,\"start\":14079},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":14212,\"start\":14209},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":14217,\"start\":14213},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":14235,\"start\":14231},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":14390,\"start\":14386},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":15412,\"start\":15408},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":15673,\"start\":15669},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":15693,\"start\":15689},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":15780,\"start\":15776},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":15823,\"start\":15819},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":15826,\"start\":15823},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":15928,\"start\":15924},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":15931,\"start\":15928},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":16252,\"start\":16249},{\"end\":16568,\"start\":16559},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":17006,\"start\":17002},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":17314,\"start\":17310},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":17402,\"start\":17398},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":17475,\"start\":17472},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":17478,\"start\":17475},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":17645,\"start\":17641},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":17648,\"start\":17645},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":17946,\"start\":17942},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":18119,\"start\":18115},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":18219,\"start\":18215},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":18236,\"start\":18232},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":18815,\"start\":18811},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":19076,\"start\":19072},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":19280,\"start\":19276},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":19304,\"start\":19300},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":19354,\"start\":19350},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":19379,\"start\":19375},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":20721,\"start\":20718},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":20724,\"start\":20721},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":21592,\"start\":21589},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":22309,\"start\":22305},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":22322,\"start\":22318},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":22335,\"start\":22331},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":22632,\"start\":22628},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":22653,\"start\":22649},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":22656,\"start\":22653},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":24169,\"start\":24165},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":24592,\"start\":24589},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":24727,\"start\":24723},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":24739,\"start\":24735},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":25213,\"start\":25209},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":25316,\"start\":25312},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":25432,\"start\":25428},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":25448,\"start\":25444},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":25511,\"start\":25507},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":25514,\"start\":25511},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":25660,\"start\":25657},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":26495,\"start\":26491},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":26498,\"start\":26495},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":26501,\"start\":26498},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":26608,\"start\":26605},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":26610,\"start\":26608},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":26612,\"start\":26610},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":26614,\"start\":26612},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":28555,\"start\":28551},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":31178,\"start\":31174}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":26426,\"start\":26301},{\"attributes\":{\"id\":\"fig_2\"},\"end\":26909,\"start\":26427},{\"attributes\":{\"id\":\"fig_4\"},\"end\":27035,\"start\":26910},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":27474,\"start\":27036},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":27714,\"start\":27475},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":30912,\"start\":27715},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":31164,\"start\":30913}]", "paragraph": "[{\"end\":1463,\"start\":516},{\"end\":2213,\"start\":1476},{\"end\":2583,\"start\":2230},{\"end\":3065,\"start\":2585},{\"end\":3751,\"start\":3067},{\"end\":4322,\"start\":3753},{\"end\":5413,\"start\":4324},{\"end\":6432,\"start\":5415},{\"end\":9069,\"start\":6449},{\"end\":9442,\"start\":9093},{\"end\":9811,\"start\":9463},{\"end\":10392,\"start\":9844},{\"end\":10586,\"start\":10424},{\"end\":10928,\"start\":10588},{\"end\":11208,\"start\":10943},{\"end\":11579,\"start\":11210},{\"end\":11739,\"start\":11581},{\"end\":13149,\"start\":11811},{\"end\":14143,\"start\":13188},{\"end\":14663,\"start\":14145},{\"end\":15296,\"start\":14665},{\"end\":15349,\"start\":15343},{\"end\":16312,\"start\":15351},{\"end\":16962,\"start\":16314},{\"end\":18012,\"start\":16988},{\"end\":18383,\"start\":18014},{\"end\":18725,\"start\":18385},{\"end\":19220,\"start\":18746},{\"end\":19785,\"start\":19233},{\"end\":20660,\"start\":19787},{\"end\":21270,\"start\":20662},{\"end\":21542,\"start\":21272},{\"end\":22276,\"start\":21544},{\"end\":22533,\"start\":22278},{\"end\":22657,\"start\":22535},{\"end\":23304,\"start\":22688},{\"end\":23800,\"start\":23306},{\"end\":24283,\"start\":23848},{\"end\":24593,\"start\":24285},{\"end\":25166,\"start\":24627},{\"end\":25548,\"start\":25193},{\"end\":26085,\"start\":25627},{\"end\":26287,\"start\":26108}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9462,\"start\":9443},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9843,\"start\":9812},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10423,\"start\":10393},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11810,\"start\":11740}]", "table_ref": "[{\"end\":13148,\"start\":13141},{\"end\":13584,\"start\":13577},{\"end\":16473,\"start\":16466},{\"end\":18030,\"start\":18023},{\"end\":18824,\"start\":18817},{\"end\":18941,\"start\":18934},{\"end\":23949,\"start\":23942},{\"end\":24049,\"start\":24042},{\"end\":24921,\"start\":24914}]", "section_header": "[{\"end\":1474,\"start\":1466},{\"attributes\":{\"n\":\"1.\"},\"end\":2228,\"start\":2216},{\"attributes\":{\"n\":\"2.\"},\"end\":6447,\"start\":6435},{\"attributes\":{\"n\":\"3.\"},\"end\":9078,\"start\":9072},{\"attributes\":{\"n\":\"3.1.\"},\"end\":9091,\"start\":9081},{\"attributes\":{\"n\":\"3.2.\"},\"end\":10941,\"start\":10931},{\"attributes\":{\"n\":\"4.\"},\"end\":13163,\"start\":13152},{\"attributes\":{\"n\":\"4.1.\"},\"end\":13186,\"start\":13166},{\"attributes\":{\"n\":\"4.2.\"},\"end\":15341,\"start\":15299},{\"attributes\":{\"n\":\"4.3.\"},\"end\":16986,\"start\":16965},{\"attributes\":{\"n\":\"4.4.\"},\"end\":18744,\"start\":18728},{\"end\":19231,\"start\":19223},{\"attributes\":{\"n\":\"5.\"},\"end\":22686,\"start\":22660},{\"end\":23846,\"start\":23803},{\"end\":24625,\"start\":24596},{\"end\":25191,\"start\":25169},{\"end\":25625,\"start\":25551},{\"end\":26106,\"start\":26088},{\"end\":26300,\"start\":26290},{\"end\":26312,\"start\":26302},{\"end\":26438,\"start\":26428},{\"end\":26921,\"start\":26911},{\"end\":30932,\"start\":30914}]", "table": "[{\"end\":27474,\"start\":27091},{\"end\":27714,\"start\":27622},{\"end\":30912,\"start\":29743},{\"end\":31164,\"start\":31139}]", "figure_caption": "[{\"end\":26426,\"start\":26314},{\"end\":26909,\"start\":26440},{\"end\":27035,\"start\":26923},{\"end\":27091,\"start\":27038},{\"end\":27622,\"start\":27477},{\"end\":29743,\"start\":27717},{\"end\":31139,\"start\":30935}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":2630,\"start\":2622},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3061,\"start\":3053},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4691,\"start\":4683},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9178,\"start\":9170},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10793,\"start\":10785},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12610,\"start\":12602},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":14969,\"start\":14961}]", "bib_author_first_name": "[{\"end\":32287,\"start\":32282},{\"end\":32291,\"start\":32288},{\"end\":32301,\"start\":32296},{\"end\":32306,\"start\":32302},{\"end\":32322,\"start\":32314},{\"end\":32324,\"start\":32323},{\"end\":32612,\"start\":32609},{\"end\":32628,\"start\":32620},{\"end\":32639,\"start\":32635},{\"end\":32654,\"start\":32647},{\"end\":32669,\"start\":32664},{\"end\":32671,\"start\":32670},{\"end\":32688,\"start\":32680},{\"end\":32705,\"start\":32699},{\"end\":32725,\"start\":32719},{\"end\":32739,\"start\":32733},{\"end\":32754,\"start\":32748},{\"end\":32771,\"start\":32763},{\"end\":32786,\"start\":32781},{\"end\":32809,\"start\":32801},{\"end\":32822,\"start\":32819},{\"end\":32838,\"start\":32833},{\"end\":32852,\"start\":32846},{\"end\":32867,\"start\":32861},{\"end\":32884,\"start\":32877},{\"end\":32896,\"start\":32889},{\"end\":32910,\"start\":32905},{\"end\":32922,\"start\":32918},{\"end\":32933,\"start\":32929},{\"end\":32949,\"start\":32942},{\"end\":33878,\"start\":33871},{\"end\":33896,\"start\":33887},{\"end\":33911,\"start\":33904},{\"end\":33929,\"start\":33922},{\"end\":33948,\"start\":33939},{\"end\":33965,\"start\":33959},{\"end\":34265,\"start\":34262},{\"end\":34277,\"start\":34272},{\"end\":34293,\"start\":34284},{\"end\":34306,\"start\":34300},{\"end\":34314,\"start\":34312},{\"end\":34330,\"start\":34322},{\"end\":34342,\"start\":34335},{\"end\":34354,\"start\":34348},{\"end\":34366,\"start\":34361},{\"end\":34378,\"start\":34372},{\"end\":34388,\"start\":34383},{\"end\":34401,\"start\":34396},{\"end\":34417,\"start\":34409},{\"end\":34431,\"start\":34423},{\"end\":34444,\"start\":34439},{\"end\":34455,\"start\":34451},{\"end\":34463,\"start\":34460},{\"end\":34471,\"start\":34468},{\"end\":34480,\"start\":34477},{\"end\":34491,\"start\":34485},{\"end\":34505,\"start\":34497},{\"end\":34520,\"start\":34512},{\"end\":34531,\"start\":34526},{\"end\":35194,\"start\":35183},{\"end\":35207,\"start\":35201},{\"end\":35227,\"start\":35220},{\"end\":35243,\"start\":35238},{\"end\":35256,\"start\":35252},{\"end\":35258,\"start\":35257},{\"end\":35664,\"start\":35660},{\"end\":35675,\"start\":35671},{\"end\":35690,\"start\":35685},{\"end\":35705,\"start\":35698},{\"end\":35716,\"start\":35710},{\"end\":35727,\"start\":35722},{\"end\":35738,\"start\":35734},{\"end\":36088,\"start\":36082},{\"end\":36099,\"start\":36095},{\"end\":36114,\"start\":36105},{\"end\":36123,\"start\":36119},{\"end\":36135,\"start\":36131},{\"end\":36376,\"start\":36369},{\"end\":36389,\"start\":36383},{\"end\":36409,\"start\":36402},{\"end\":36423,\"start\":36414},{\"end\":36435,\"start\":36429},{\"end\":36754,\"start\":36746},{\"end\":37422,\"start\":37416},{\"end\":37445,\"start\":37437},{\"end\":37458,\"start\":37452},{\"end\":37905,\"start\":37901},{\"end\":37937,\"start\":37930},{\"end\":37950,\"start\":37947},{\"end\":37966,\"start\":37960},{\"end\":37981,\"start\":37975},{\"end\":38408,\"start\":38404},{\"end\":38443,\"start\":38442},{\"end\":38456,\"start\":38453},{\"end\":38458,\"start\":38457},{\"end\":38474,\"start\":38468},{\"end\":38489,\"start\":38483},{\"end\":39110,\"start\":39107},{\"end\":39120,\"start\":39117},{\"end\":39134,\"start\":39127},{\"end\":39149,\"start\":39143},{\"end\":39157,\"start\":39154},{\"end\":39164,\"start\":39162},{\"end\":39559,\"start\":39554},{\"end\":39576,\"start\":39568},{\"end\":39590,\"start\":39584},{\"end\":39604,\"start\":39596},{\"end\":39902,\"start\":39898},{\"end\":39922,\"start\":39909},{\"end\":39942,\"start\":39935},{\"end\":40274,\"start\":40268},{\"end\":40293,\"start\":40288},{\"end\":40310,\"start\":40301},{\"end\":40327,\"start\":40323},{\"end\":40348,\"start\":40341},{\"end\":40361,\"start\":40355},{\"end\":40382,\"start\":40375},{\"end\":40401,\"start\":40393},{\"end\":40417,\"start\":40412},{\"end\":40923,\"start\":40917},{\"end\":40937,\"start\":40933},{\"end\":40951,\"start\":40946},{\"end\":41359,\"start\":41353},{\"end\":41374,\"start\":41368},{\"end\":41893,\"start\":41885},{\"end\":41902,\"start\":41899},{\"end\":41911,\"start\":41908},{\"end\":41921,\"start\":41916},{\"end\":41931,\"start\":41926},{\"end\":41946,\"start\":41938},{\"end\":41956,\"start\":41951},{\"end\":42270,\"start\":42267},{\"end\":42278,\"start\":42276},{\"end\":42290,\"start\":42285},{\"end\":42303,\"start\":42295},{\"end\":42317,\"start\":42309},{\"end\":42327,\"start\":42322},{\"end\":42557,\"start\":42550},{\"end\":42569,\"start\":42562},{\"end\":42929,\"start\":42922},{\"end\":42941,\"start\":42934},{\"end\":42957,\"start\":42949},{\"end\":42967,\"start\":42963},{\"end\":43382,\"start\":43379},{\"end\":43399,\"start\":43394},{\"end\":43639,\"start\":43634},{\"end\":43651,\"start\":43645},{\"end\":43661,\"start\":43659},{\"end\":43677,\"start\":43668},{\"end\":43694,\"start\":43685},{\"end\":43706,\"start\":43700},{\"end\":43993,\"start\":43990},{\"end\":44003,\"start\":44001},{\"end\":44015,\"start\":44009},{\"end\":44027,\"start\":44021},{\"end\":44043,\"start\":44035},{\"end\":44402,\"start\":44396},{\"end\":44419,\"start\":44410},{\"end\":44741,\"start\":44740},{\"end\":44757,\"start\":44752},{\"end\":44965,\"start\":44956},{\"end\":44980,\"start\":44976},{\"end\":44998,\"start\":44991},{\"end\":45008,\"start\":45003},{\"end\":45454,\"start\":45450},{\"end\":45471,\"start\":45467},{\"end\":45491,\"start\":45483},{\"end\":45493,\"start\":45492},{\"end\":45757,\"start\":45752},{\"end\":45775,\"start\":45769},{\"end\":45789,\"start\":45785},{\"end\":45808,\"start\":45800},{\"end\":46129,\"start\":46121},{\"end\":46140,\"start\":46135},{\"end\":46152,\"start\":46148},{\"end\":46546,\"start\":46538},{\"end\":46559,\"start\":46552},{\"end\":46572,\"start\":46567},{\"end\":46588,\"start\":46583},{\"end\":46601,\"start\":46595},{\"end\":46614,\"start\":46610},{\"end\":46629,\"start\":46624},{\"end\":46648,\"start\":46638},{\"end\":46969,\"start\":46962},{\"end\":46981,\"start\":46975},{\"end\":46988,\"start\":46987},{\"end\":47002,\"start\":46996},{\"end\":47263,\"start\":47261},{\"end\":47275,\"start\":47269},{\"end\":47284,\"start\":47281},{\"end\":47293,\"start\":47290},{\"end\":47304,\"start\":47298},{\"end\":47315,\"start\":47310},{\"end\":47330,\"start\":47323},{\"end\":47343,\"start\":47336},{\"end\":47832,\"start\":47828},{\"end\":47850,\"start\":47845},{\"end\":48105,\"start\":48099},{\"end\":48125,\"start\":48115},{\"end\":48424,\"start\":48419},{\"end\":48441,\"start\":48434},{\"end\":48458,\"start\":48452},{\"end\":48472,\"start\":48468},{\"end\":48487,\"start\":48482},{\"end\":48501,\"start\":48496},{\"end\":48994,\"start\":48989},{\"end\":49003,\"start\":48995},{\"end\":49328,\"start\":49323},{\"end\":49343,\"start\":49335},{\"end\":49345,\"start\":49344},{\"end\":49519,\"start\":49514},{\"end\":49533,\"start\":49526},{\"end\":49797,\"start\":49790},{\"end\":49811,\"start\":49805},{\"end\":49830,\"start\":49825},{\"end\":49849,\"start\":49842},{\"end\":49863,\"start\":49857},{\"end\":50155,\"start\":50149},{\"end\":50174,\"start\":50170},{\"end\":50189,\"start\":50183},{\"end\":50204,\"start\":50199},{\"end\":50218,\"start\":50212},{\"end\":50232,\"start\":50229},{\"end\":50558,\"start\":50550},{\"end\":50572,\"start\":50564},{\"end\":50584,\"start\":50579},{\"end\":50595,\"start\":50590},{\"end\":50603,\"start\":50600},{\"end\":50877,\"start\":50872},{\"end\":51174,\"start\":51166},{\"end\":51176,\"start\":51175},{\"end\":51204,\"start\":51196},{\"end\":51592,\"start\":51591},{\"end\":51613,\"start\":51606},{\"end\":51633,\"start\":51625},{\"end\":51655,\"start\":51644},{\"end\":51665,\"start\":51661},{\"end\":51681,\"start\":51676},{\"end\":52189,\"start\":52184},{\"end\":52206,\"start\":52200},{\"end\":52684,\"start\":52675},{\"end\":52701,\"start\":52694},{\"end\":52719,\"start\":52713},{\"end\":52730,\"start\":52727},{\"end\":52747,\"start\":52739},{\"end\":53202,\"start\":53198},{\"end\":53219,\"start\":53215},{\"end\":53238,\"start\":53229},{\"end\":53256,\"start\":53251},{\"end\":53271,\"start\":53264},{\"end\":53284,\"start\":53278},{\"end\":53305,\"start\":53298},{\"end\":53319,\"start\":53312},{\"end\":53335,\"start\":53329},{\"end\":53350,\"start\":53345},{\"end\":53667,\"start\":53663},{\"end\":53682,\"start\":53677},{\"end\":53703,\"start\":53695},{\"end\":53719,\"start\":53711},{\"end\":53735,\"start\":53726},{\"end\":53753,\"start\":53746},{\"end\":53768,\"start\":53761},{\"end\":53784,\"start\":53778},{\"end\":53800,\"start\":53793},{\"end\":53816,\"start\":53811},{\"end\":54338,\"start\":54334},{\"end\":54356,\"start\":54348},{\"end\":54371,\"start\":54363},{\"end\":54388,\"start\":54379},{\"end\":54405,\"start\":54396},{\"end\":54425,\"start\":54420},{\"end\":54776,\"start\":54772},{\"end\":54794,\"start\":54786},{\"end\":55173,\"start\":55167},{\"end\":55189,\"start\":55183},{\"end\":55211,\"start\":55204},{\"end\":55226,\"start\":55222},{\"end\":55240,\"start\":55235},{\"end\":55259,\"start\":55251},{\"end\":55740,\"start\":55734},{\"end\":55754,\"start\":55750},{\"end\":55768,\"start\":55764},{\"end\":55782,\"start\":55777},{\"end\":55799,\"start\":55794},{\"end\":55812,\"start\":55807},{\"end\":55814,\"start\":55813},{\"end\":55828,\"start\":55822},{\"end\":55842,\"start\":55837},{\"end\":56246,\"start\":56240},{\"end\":56257,\"start\":56253},{\"end\":56268,\"start\":56263},{\"end\":56282,\"start\":56273},{\"end\":56294,\"start\":56288},{\"end\":56305,\"start\":56301},{\"end\":56317,\"start\":56313},{\"end\":56326,\"start\":56322},{\"end\":56336,\"start\":56332},{\"end\":56840,\"start\":56836},{\"end\":57026,\"start\":57022},{\"end\":57041,\"start\":57037},{\"end\":57056,\"start\":57051},{\"end\":57267,\"start\":57260},{\"end\":57275,\"start\":57272},{\"end\":57286,\"start\":57282},{\"end\":57304,\"start\":57296},{\"end\":57316,\"start\":57310},{\"end\":57324,\"start\":57322},{\"end\":57334,\"start\":57331},{\"end\":57689,\"start\":57686},{\"end\":57700,\"start\":57694},{\"end\":57714,\"start\":57707},{\"end\":57729,\"start\":57721},{\"end\":57742,\"start\":57734},{\"end\":58196,\"start\":58189},{\"end\":58206,\"start\":58202},{\"end\":58222,\"start\":58217},{\"end\":58238,\"start\":58231},{\"end\":58250,\"start\":58243},{\"end\":58719,\"start\":58717},{\"end\":58733,\"start\":58726},{\"end\":58743,\"start\":58740},{\"end\":58756,\"start\":58750},{\"end\":58766,\"start\":58761},{\"end\":58779,\"start\":58772},{\"end\":58794,\"start\":58787},{\"end\":58798,\"start\":58795},{\"end\":58810,\"start\":58804},{\"end\":58826,\"start\":58817},{\"end\":59387,\"start\":59380},{\"end\":59401,\"start\":59393},{\"end\":59415,\"start\":59407},{\"end\":59437,\"start\":59431},{\"end\":59453,\"start\":59444},{\"end\":59927,\"start\":59921},{\"end\":59944,\"start\":59935},{\"end\":59957,\"start\":59952},{\"end\":60258,\"start\":60254},{\"end\":60271,\"start\":60266},{\"end\":60287,\"start\":60279},{\"end\":60300,\"start\":60294},{\"end\":60307,\"start\":60305},{\"end\":60671,\"start\":60666},{\"end\":60682,\"start\":60678},{\"end\":60695,\"start\":60689},{\"end\":60707,\"start\":60702},{\"end\":60721,\"start\":60716},{\"end\":60739,\"start\":60732},{\"end\":61195,\"start\":61189},{\"end\":61207,\"start\":61202},{\"end\":61219,\"start\":61213},{\"end\":61232,\"start\":61226},{\"end\":61243,\"start\":61237},{\"end\":61255,\"start\":61251},{\"end\":61267,\"start\":61260},{\"end\":61278,\"start\":61273},{\"end\":61290,\"start\":61284}]", "bib_author_last_name": "[{\"end\":32294,\"start\":32292},{\"end\":32312,\"start\":32307},{\"end\":32328,\"start\":32325},{\"end\":32618,\"start\":32613},{\"end\":32633,\"start\":32629},{\"end\":32645,\"start\":32640},{\"end\":32662,\"start\":32655},{\"end\":32678,\"start\":32672},{\"end\":32697,\"start\":32689},{\"end\":32717,\"start\":32706},{\"end\":32731,\"start\":32726},{\"end\":32746,\"start\":32740},{\"end\":32761,\"start\":32755},{\"end\":32779,\"start\":32772},{\"end\":32799,\"start\":32787},{\"end\":32817,\"start\":32810},{\"end\":32831,\"start\":32823},{\"end\":32844,\"start\":32839},{\"end\":32859,\"start\":32853},{\"end\":32875,\"start\":32868},{\"end\":32887,\"start\":32885},{\"end\":32903,\"start\":32897},{\"end\":32916,\"start\":32911},{\"end\":32927,\"start\":32923},{\"end\":32940,\"start\":32934},{\"end\":32956,\"start\":32950},{\"end\":33885,\"start\":33879},{\"end\":33902,\"start\":33897},{\"end\":33920,\"start\":33912},{\"end\":33937,\"start\":33930},{\"end\":33957,\"start\":33949},{\"end\":33975,\"start\":33966},{\"end\":34270,\"start\":34266},{\"end\":34282,\"start\":34278},{\"end\":34298,\"start\":34294},{\"end\":34310,\"start\":34307},{\"end\":34320,\"start\":34315},{\"end\":34333,\"start\":34331},{\"end\":34346,\"start\":34343},{\"end\":34359,\"start\":34355},{\"end\":34370,\"start\":34367},{\"end\":34381,\"start\":34379},{\"end\":34394,\"start\":34389},{\"end\":34407,\"start\":34402},{\"end\":34421,\"start\":34418},{\"end\":34437,\"start\":34432},{\"end\":34449,\"start\":34445},{\"end\":34458,\"start\":34456},{\"end\":34466,\"start\":34464},{\"end\":34475,\"start\":34472},{\"end\":34483,\"start\":34481},{\"end\":34495,\"start\":34492},{\"end\":34510,\"start\":34506},{\"end\":34524,\"start\":34521},{\"end\":34538,\"start\":34532},{\"end\":35199,\"start\":35195},{\"end\":35218,\"start\":35208},{\"end\":35236,\"start\":35228},{\"end\":35250,\"start\":35244},{\"end\":35265,\"start\":35259},{\"end\":35669,\"start\":35665},{\"end\":35683,\"start\":35676},{\"end\":35696,\"start\":35691},{\"end\":35708,\"start\":35706},{\"end\":35720,\"start\":35717},{\"end\":35732,\"start\":35728},{\"end\":35748,\"start\":35739},{\"end\":36093,\"start\":36089},{\"end\":36103,\"start\":36100},{\"end\":36117,\"start\":36115},{\"end\":36129,\"start\":36124},{\"end\":36139,\"start\":36136},{\"end\":36381,\"start\":36377},{\"end\":36400,\"start\":36390},{\"end\":36412,\"start\":36410},{\"end\":36427,\"start\":36424},{\"end\":36440,\"start\":36436},{\"end\":36762,\"start\":36755},{\"end\":37435,\"start\":37423},{\"end\":37450,\"start\":37446},{\"end\":37465,\"start\":37459},{\"end\":37469,\"start\":37467},{\"end\":37928,\"start\":37906},{\"end\":37945,\"start\":37938},{\"end\":37958,\"start\":37951},{\"end\":37973,\"start\":37967},{\"end\":37988,\"start\":37982},{\"end\":37995,\"start\":37990},{\"end\":38431,\"start\":38409},{\"end\":38440,\"start\":38433},{\"end\":38451,\"start\":38444},{\"end\":38466,\"start\":38459},{\"end\":38481,\"start\":38475},{\"end\":38496,\"start\":38490},{\"end\":38503,\"start\":38498},{\"end\":39115,\"start\":39111},{\"end\":39125,\"start\":39121},{\"end\":39141,\"start\":39135},{\"end\":39152,\"start\":39150},{\"end\":39160,\"start\":39158},{\"end\":39172,\"start\":39165},{\"end\":39566,\"start\":39560},{\"end\":39582,\"start\":39577},{\"end\":39594,\"start\":39591},{\"end\":39614,\"start\":39605},{\"end\":39907,\"start\":39903},{\"end\":39933,\"start\":39923},{\"end\":39949,\"start\":39943},{\"end\":40286,\"start\":40275},{\"end\":40299,\"start\":40294},{\"end\":40321,\"start\":40311},{\"end\":40339,\"start\":40328},{\"end\":40353,\"start\":40349},{\"end\":40373,\"start\":40362},{\"end\":40391,\"start\":40383},{\"end\":40410,\"start\":40402},{\"end\":40425,\"start\":40418},{\"end\":40931,\"start\":40924},{\"end\":40944,\"start\":40938},{\"end\":40956,\"start\":40952},{\"end\":41186,\"start\":41167},{\"end\":41366,\"start\":41360},{\"end\":41381,\"start\":41375},{\"end\":41897,\"start\":41894},{\"end\":41906,\"start\":41903},{\"end\":41914,\"start\":41912},{\"end\":41924,\"start\":41922},{\"end\":41936,\"start\":41932},{\"end\":41949,\"start\":41947},{\"end\":41961,\"start\":41957},{\"end\":41966,\"start\":41963},{\"end\":42274,\"start\":42271},{\"end\":42283,\"start\":42279},{\"end\":42293,\"start\":42291},{\"end\":42307,\"start\":42304},{\"end\":42320,\"start\":42318},{\"end\":42332,\"start\":42328},{\"end\":42560,\"start\":42558},{\"end\":42578,\"start\":42570},{\"end\":42932,\"start\":42930},{\"end\":42947,\"start\":42942},{\"end\":42961,\"start\":42958},{\"end\":42971,\"start\":42968},{\"end\":43392,\"start\":43383},{\"end\":43406,\"start\":43400},{\"end\":43643,\"start\":43640},{\"end\":43657,\"start\":43652},{\"end\":43666,\"start\":43662},{\"end\":43683,\"start\":43678},{\"end\":43698,\"start\":43695},{\"end\":43711,\"start\":43707},{\"end\":43999,\"start\":43994},{\"end\":44007,\"start\":44004},{\"end\":44019,\"start\":44016},{\"end\":44033,\"start\":44028},{\"end\":44054,\"start\":44044},{\"end\":44274,\"start\":44266},{\"end\":44408,\"start\":44403},{\"end\":44427,\"start\":44420},{\"end\":44750,\"start\":44742},{\"end\":44764,\"start\":44758},{\"end\":44768,\"start\":44766},{\"end\":44974,\"start\":44966},{\"end\":44989,\"start\":44981},{\"end\":45001,\"start\":44999},{\"end\":45015,\"start\":45009},{\"end\":45465,\"start\":45455},{\"end\":45481,\"start\":45472},{\"end\":45500,\"start\":45494},{\"end\":45767,\"start\":45758},{\"end\":45783,\"start\":45776},{\"end\":45798,\"start\":45790},{\"end\":45816,\"start\":45809},{\"end\":46133,\"start\":46130},{\"end\":46146,\"start\":46141},{\"end\":46161,\"start\":46153},{\"end\":46550,\"start\":46547},{\"end\":46565,\"start\":46560},{\"end\":46581,\"start\":46573},{\"end\":46593,\"start\":46589},{\"end\":46608,\"start\":46602},{\"end\":46622,\"start\":46615},{\"end\":46636,\"start\":46630},{\"end\":46656,\"start\":46649},{\"end\":46934,\"start\":46926},{\"end\":46973,\"start\":46970},{\"end\":46985,\"start\":46982},{\"end\":46994,\"start\":46989},{\"end\":47005,\"start\":47003},{\"end\":47009,\"start\":47007},{\"end\":47267,\"start\":47264},{\"end\":47279,\"start\":47276},{\"end\":47288,\"start\":47285},{\"end\":47296,\"start\":47294},{\"end\":47308,\"start\":47305},{\"end\":47321,\"start\":47316},{\"end\":47334,\"start\":47331},{\"end\":47347,\"start\":47344},{\"end\":47843,\"start\":47833},{\"end\":47857,\"start\":47851},{\"end\":48113,\"start\":48106},{\"end\":48132,\"start\":48126},{\"end\":48432,\"start\":48425},{\"end\":48450,\"start\":48442},{\"end\":48466,\"start\":48459},{\"end\":48480,\"start\":48473},{\"end\":48494,\"start\":48488},{\"end\":48512,\"start\":48502},{\"end\":49011,\"start\":49004},{\"end\":49333,\"start\":49329},{\"end\":49352,\"start\":49346},{\"end\":49524,\"start\":49520},{\"end\":49537,\"start\":49534},{\"end\":49803,\"start\":49798},{\"end\":49823,\"start\":49812},{\"end\":49840,\"start\":49831},{\"end\":49855,\"start\":49850},{\"end\":49875,\"start\":49864},{\"end\":50168,\"start\":50156},{\"end\":50181,\"start\":50175},{\"end\":50197,\"start\":50190},{\"end\":50210,\"start\":50205},{\"end\":50227,\"start\":50219},{\"end\":50239,\"start\":50233},{\"end\":50562,\"start\":50559},{\"end\":50577,\"start\":50573},{\"end\":50588,\"start\":50585},{\"end\":50598,\"start\":50596},{\"end\":50608,\"start\":50604},{\"end\":50888,\"start\":50878},{\"end\":51194,\"start\":51177},{\"end\":51211,\"start\":51205},{\"end\":51221,\"start\":51213},{\"end\":51604,\"start\":51593},{\"end\":51623,\"start\":51614},{\"end\":51642,\"start\":51634},{\"end\":51659,\"start\":51656},{\"end\":51674,\"start\":51666},{\"end\":51688,\"start\":51682},{\"end\":51695,\"start\":51690},{\"end\":52198,\"start\":52190},{\"end\":52216,\"start\":52207},{\"end\":52692,\"start\":52685},{\"end\":52711,\"start\":52702},{\"end\":52725,\"start\":52720},{\"end\":52737,\"start\":52731},{\"end\":52753,\"start\":52748},{\"end\":53213,\"start\":53203},{\"end\":53227,\"start\":53220},{\"end\":53249,\"start\":53239},{\"end\":53262,\"start\":53257},{\"end\":53276,\"start\":53272},{\"end\":53296,\"start\":53285},{\"end\":53310,\"start\":53306},{\"end\":53327,\"start\":53320},{\"end\":53343,\"start\":53336},{\"end\":53360,\"start\":53351},{\"end\":53675,\"start\":53668},{\"end\":53693,\"start\":53683},{\"end\":53709,\"start\":53704},{\"end\":53724,\"start\":53720},{\"end\":53744,\"start\":53736},{\"end\":53759,\"start\":53754},{\"end\":53776,\"start\":53769},{\"end\":53791,\"start\":53785},{\"end\":53809,\"start\":53801},{\"end\":53824,\"start\":53817},{\"end\":54346,\"start\":54339},{\"end\":54361,\"start\":54357},{\"end\":54377,\"start\":54372},{\"end\":54394,\"start\":54389},{\"end\":54418,\"start\":54406},{\"end\":54431,\"start\":54426},{\"end\":54784,\"start\":54777},{\"end\":54799,\"start\":54795},{\"end\":55181,\"start\":55174},{\"end\":55202,\"start\":55190},{\"end\":55220,\"start\":55212},{\"end\":55233,\"start\":55227},{\"end\":55249,\"start\":55241},{\"end\":55266,\"start\":55260},{\"end\":55748,\"start\":55741},{\"end\":55762,\"start\":55755},{\"end\":55775,\"start\":55769},{\"end\":55792,\"start\":55783},{\"end\":55805,\"start\":55800},{\"end\":55820,\"start\":55815},{\"end\":55835,\"start\":55829},{\"end\":55853,\"start\":55843},{\"end\":56251,\"start\":56247},{\"end\":56261,\"start\":56258},{\"end\":56271,\"start\":56269},{\"end\":56286,\"start\":56283},{\"end\":56299,\"start\":56295},{\"end\":56311,\"start\":56306},{\"end\":56320,\"start\":56318},{\"end\":56330,\"start\":56327},{\"end\":56341,\"start\":56337},{\"end\":56849,\"start\":56841},{\"end\":57035,\"start\":57027},{\"end\":57049,\"start\":57042},{\"end\":57062,\"start\":57057},{\"end\":57270,\"start\":57268},{\"end\":57280,\"start\":57276},{\"end\":57294,\"start\":57287},{\"end\":57308,\"start\":57305},{\"end\":57320,\"start\":57317},{\"end\":57329,\"start\":57325},{\"end\":57340,\"start\":57335},{\"end\":57692,\"start\":57690},{\"end\":57705,\"start\":57701},{\"end\":57719,\"start\":57715},{\"end\":57732,\"start\":57730},{\"end\":57747,\"start\":57743},{\"end\":58200,\"start\":58197},{\"end\":58215,\"start\":58207},{\"end\":58229,\"start\":58223},{\"end\":58241,\"start\":58239},{\"end\":58253,\"start\":58251},{\"end\":58724,\"start\":58720},{\"end\":58738,\"start\":58734},{\"end\":58748,\"start\":58744},{\"end\":58759,\"start\":58757},{\"end\":58770,\"start\":58767},{\"end\":58785,\"start\":58780},{\"end\":58802,\"start\":58799},{\"end\":58815,\"start\":58811},{\"end\":58830,\"start\":58827},{\"end\":59391,\"start\":59388},{\"end\":59405,\"start\":59402},{\"end\":59429,\"start\":59416},{\"end\":59442,\"start\":59438},{\"end\":59458,\"start\":59454},{\"end\":59463,\"start\":59460},{\"end\":59933,\"start\":59928},{\"end\":59950,\"start\":59945},{\"end\":59972,\"start\":59958},{\"end\":59983,\"start\":59974},{\"end\":60264,\"start\":60259},{\"end\":60277,\"start\":60272},{\"end\":60292,\"start\":60288},{\"end\":60303,\"start\":60301},{\"end\":60312,\"start\":60308},{\"end\":60676,\"start\":60672},{\"end\":60687,\"start\":60683},{\"end\":60700,\"start\":60696},{\"end\":60714,\"start\":60708},{\"end\":60730,\"start\":60722},{\"end\":60748,\"start\":60740},{\"end\":61200,\"start\":61196},{\"end\":61211,\"start\":61208},{\"end\":61224,\"start\":61220},{\"end\":61235,\"start\":61233},{\"end\":61249,\"start\":61244},{\"end\":61258,\"start\":61256},{\"end\":61271,\"start\":61268},{\"end\":61282,\"start\":61279},{\"end\":61295,\"start\":61291},{\"end\":61558,\"start\":61548},{\"end\":61625,\"start\":61611}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":32167,\"start\":32033},{\"attributes\":{\"id\":\"b1\"},\"end\":32278,\"start\":32169},{\"attributes\":{\"doi\":\"arXiv:1607.06450\",\"id\":\"b2\"},\"end\":32520,\"start\":32280},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":218971783},\"end\":33823,\"start\":32522},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":218889832},\"end\":34260,\"start\":33825},{\"attributes\":{\"doi\":\"arXiv:1906.07155\",\"id\":\"b5\"},\"end\":35068,\"start\":34262},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":3429309},\"end\":35622,\"start\":35070},{\"attributes\":{\"doi\":\"PMLR, 2020. 2\",\"id\":\"b7\",\"matched_paper_id\":219781060},\"end\":36024,\"start\":35624},{\"attributes\":{\"doi\":\"arXiv:2107.10224\",\"id\":\"b8\"},\"end\":36330,\"start\":36026},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":53106274},\"end\":36681,\"start\":36332},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":2375110},\"end\":37098,\"start\":36683},{\"attributes\":{\"id\":\"b11\"},\"end\":37334,\"start\":37100},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":208006202},\"end\":37899,\"start\":37336},{\"attributes\":{\"doi\":\"arXiv:2103.10697\",\"id\":\"b13\"},\"end\":38322,\"start\":37901},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b14\",\"matched_paper_id\":232290742},\"end\":39052,\"start\":38324},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":57246310},\"end\":39470,\"start\":39054},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":52967399},\"end\":39806,\"start\":39472},{\"attributes\":{\"doi\":\"arXiv:2103.03404\",\"id\":\"b17\"},\"end\":40168,\"start\":39808},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":225039882},\"end\":40817,\"start\":40170},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":6940861},\"end\":41163,\"start\":40819},{\"attributes\":{\"id\":\"b20\"},\"end\":41276,\"start\":41165},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":5575601},\"end\":41883,\"start\":41278},{\"attributes\":{\"doi\":\"arXiv:2107.06263\",\"id\":\"b22\"},\"end\":42237,\"start\":41885},{\"attributes\":{\"doi\":\"arXiv:2103.00112\",\"id\":\"b23\"},\"end\":42503,\"start\":42239},{\"attributes\":{\"id\":\"b24\"},\"end\":42874,\"start\":42505},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":206594692},\"end\":43340,\"start\":42876},{\"attributes\":{\"doi\":\"arXiv:1606.08415\",\"id\":\"b26\"},\"end\":43554,\"start\":43342},{\"attributes\":{\"doi\":\"arXiv:2106.12368\",\"id\":\"b27\"},\"end\":43951,\"start\":43556},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":6773885},\"end\":44262,\"start\":43953},{\"attributes\":{\"id\":\"b29\"},\"end\":44300,\"start\":44264},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b30\",\"matched_paper_id\":5808102},\"end\":44694,\"start\":44302},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b31\"},\"end\":44919,\"start\":44696},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":57721164},\"end\":45383,\"start\":44921},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":195908774},\"end\":45750,\"start\":45385},{\"attributes\":{\"doi\":\"arXiv:2105.03824\",\"id\":\"b34\"},\"end\":46050,\"start\":45752},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":206771220},\"end\":46493,\"start\":46052},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":14113767},\"end\":46922,\"start\":46495},{\"attributes\":{\"id\":\"b37\"},\"end\":46960,\"start\":46924},{\"attributes\":{\"doi\":\"arXiv:2105.08050\",\"id\":\"b38\"},\"end\":47186,\"start\":46962},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":232352874},\"end\":47787,\"start\":47188},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":53592270},\"end\":48057,\"start\":47789},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":17018486},\"end\":48373,\"start\":48059},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":219636190},\"end\":48987,\"start\":48375},{\"attributes\":{\"doi\":\"arXiv:2109.00301\",\"id\":\"b43\"},\"end\":49259,\"start\":48989},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":15539264},\"end\":49479,\"start\":49261},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":246823327},\"end\":49724,\"start\":49481},{\"attributes\":{\"doi\":\"arXiv:2108.08810\",\"id\":\"b46\"},\"end\":50103,\"start\":49726},{\"attributes\":{\"id\":\"b47\"},\"end\":50499,\"start\":50105},{\"attributes\":{\"doi\":\"arXiv:2107.00645\",\"id\":\"b48\"},\"end\":50793,\"start\":50501},{\"attributes\":{\"id\":\"b49\"},\"end\":51108,\"start\":50795},{\"attributes\":{\"id\":\"b50\"},\"end\":51507,\"start\":51110},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":15019293},\"end\":52114,\"start\":51509},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":14124313},\"end\":52614,\"start\":52116},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":206593880},\"end\":53149,\"start\":52616},{\"attributes\":{\"doi\":\"arXiv:2105.01601\",\"id\":\"b54\"},\"end\":53661,\"start\":53151},{\"attributes\":{\"doi\":\"arXiv:2105.03404\",\"id\":\"b55\"},\"end\":54255,\"start\":53663},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b56\",\"matched_paper_id\":229363322},\"end\":54770,\"start\":54257},{\"attributes\":{\"doi\":\"arXiv:2103.17239\",\"id\":\"b57\"},\"end\":55094,\"start\":54772},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":232320340},\"end\":55705,\"start\":55096},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":13756489},\"end\":56146,\"start\":55707},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":232035922},\"end\":56812,\"start\":56148},{\"attributes\":{\"id\":\"b61\"},\"end\":56959,\"start\":56814},{\"attributes\":{\"doi\":\"arXiv:2110.00476\",\"id\":\"b62\"},\"end\":57258,\"start\":56961},{\"attributes\":{\"doi\":\"arXiv:2103.15808\",\"id\":\"b63\"},\"end\":57608,\"start\":57260},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":236493453},\"end\":58125,\"start\":57610},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":8485068},\"end\":58639,\"start\":58127},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":231719476},\"end\":59291,\"start\":58641},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":152282661},\"end\":59876,\"start\":59293},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":3162051},\"end\":60218,\"start\":59878},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":2035600},\"end\":60626,\"start\":60220},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":5636055},\"end\":61129,\"start\":60628},{\"attributes\":{\"doi\":\"arXiv:2106.03714\",\"id\":\"b71\"},\"end\":61544,\"start\":61131},{\"attributes\":{\"doi\":\"ResMLP-S24 [52\",\"id\":\"b72\"},\"end\":61607,\"start\":61546},{\"attributes\":{\"id\":\"b73\"},\"end\":61641,\"start\":61609},{\"attributes\":{\"id\":\"b74\"},\"end\":61891,\"start\":61643}]", "bib_title": "[{\"end\":32607,\"start\":32522},{\"end\":33869,\"start\":33825},{\"end\":35181,\"start\":35070},{\"end\":35658,\"start\":35624},{\"end\":36367,\"start\":36332},{\"end\":36744,\"start\":36683},{\"end\":37414,\"start\":37336},{\"end\":38402,\"start\":38324},{\"end\":39105,\"start\":39054},{\"end\":39552,\"start\":39472},{\"end\":40266,\"start\":40170},{\"end\":40915,\"start\":40819},{\"end\":41351,\"start\":41278},{\"end\":42548,\"start\":42505},{\"end\":42920,\"start\":42876},{\"end\":43988,\"start\":43953},{\"end\":44394,\"start\":44302},{\"end\":44954,\"start\":44921},{\"end\":45448,\"start\":45385},{\"end\":46119,\"start\":46052},{\"end\":46536,\"start\":46495},{\"end\":47259,\"start\":47188},{\"end\":47826,\"start\":47789},{\"end\":48097,\"start\":48059},{\"end\":48417,\"start\":48375},{\"end\":49321,\"start\":49261},{\"end\":49512,\"start\":49481},{\"end\":50147,\"start\":50105},{\"end\":51589,\"start\":51509},{\"end\":52182,\"start\":52116},{\"end\":52673,\"start\":52616},{\"end\":54332,\"start\":54257},{\"end\":55165,\"start\":55096},{\"end\":55732,\"start\":55707},{\"end\":56238,\"start\":56148},{\"end\":57684,\"start\":57610},{\"end\":58187,\"start\":58127},{\"end\":58715,\"start\":58641},{\"end\":59378,\"start\":59293},{\"end\":59919,\"start\":59878},{\"end\":60252,\"start\":60220},{\"end\":60664,\"start\":60628}]", "bib_author": "[{\"end\":32296,\"start\":32282},{\"end\":32314,\"start\":32296},{\"end\":32330,\"start\":32314},{\"end\":32620,\"start\":32609},{\"end\":32635,\"start\":32620},{\"end\":32647,\"start\":32635},{\"end\":32664,\"start\":32647},{\"end\":32680,\"start\":32664},{\"end\":32699,\"start\":32680},{\"end\":32719,\"start\":32699},{\"end\":32733,\"start\":32719},{\"end\":32748,\"start\":32733},{\"end\":32763,\"start\":32748},{\"end\":32781,\"start\":32763},{\"end\":32801,\"start\":32781},{\"end\":32819,\"start\":32801},{\"end\":32833,\"start\":32819},{\"end\":32846,\"start\":32833},{\"end\":32861,\"start\":32846},{\"end\":32877,\"start\":32861},{\"end\":32889,\"start\":32877},{\"end\":32905,\"start\":32889},{\"end\":32918,\"start\":32905},{\"end\":32929,\"start\":32918},{\"end\":32942,\"start\":32929},{\"end\":32958,\"start\":32942},{\"end\":33887,\"start\":33871},{\"end\":33904,\"start\":33887},{\"end\":33922,\"start\":33904},{\"end\":33939,\"start\":33922},{\"end\":33959,\"start\":33939},{\"end\":33977,\"start\":33959},{\"end\":34272,\"start\":34262},{\"end\":34284,\"start\":34272},{\"end\":34300,\"start\":34284},{\"end\":34312,\"start\":34300},{\"end\":34322,\"start\":34312},{\"end\":34335,\"start\":34322},{\"end\":34348,\"start\":34335},{\"end\":34361,\"start\":34348},{\"end\":34372,\"start\":34361},{\"end\":34383,\"start\":34372},{\"end\":34396,\"start\":34383},{\"end\":34409,\"start\":34396},{\"end\":34423,\"start\":34409},{\"end\":34439,\"start\":34423},{\"end\":34451,\"start\":34439},{\"end\":34460,\"start\":34451},{\"end\":34468,\"start\":34460},{\"end\":34477,\"start\":34468},{\"end\":34485,\"start\":34477},{\"end\":34497,\"start\":34485},{\"end\":34512,\"start\":34497},{\"end\":34526,\"start\":34512},{\"end\":34540,\"start\":34526},{\"end\":35201,\"start\":35183},{\"end\":35220,\"start\":35201},{\"end\":35238,\"start\":35220},{\"end\":35252,\"start\":35238},{\"end\":35267,\"start\":35252},{\"end\":35671,\"start\":35660},{\"end\":35685,\"start\":35671},{\"end\":35698,\"start\":35685},{\"end\":35710,\"start\":35698},{\"end\":35722,\"start\":35710},{\"end\":35734,\"start\":35722},{\"end\":35750,\"start\":35734},{\"end\":36095,\"start\":36082},{\"end\":36105,\"start\":36095},{\"end\":36119,\"start\":36105},{\"end\":36131,\"start\":36119},{\"end\":36141,\"start\":36131},{\"end\":36383,\"start\":36369},{\"end\":36402,\"start\":36383},{\"end\":36414,\"start\":36402},{\"end\":36429,\"start\":36414},{\"end\":36442,\"start\":36429},{\"end\":36764,\"start\":36746},{\"end\":37437,\"start\":37416},{\"end\":37452,\"start\":37437},{\"end\":37467,\"start\":37452},{\"end\":37471,\"start\":37467},{\"end\":37930,\"start\":37901},{\"end\":37947,\"start\":37930},{\"end\":37960,\"start\":37947},{\"end\":37975,\"start\":37960},{\"end\":37990,\"start\":37975},{\"end\":37997,\"start\":37990},{\"end\":38433,\"start\":38404},{\"end\":38442,\"start\":38433},{\"end\":38453,\"start\":38442},{\"end\":38468,\"start\":38453},{\"end\":38483,\"start\":38468},{\"end\":38498,\"start\":38483},{\"end\":38505,\"start\":38498},{\"end\":39117,\"start\":39107},{\"end\":39127,\"start\":39117},{\"end\":39143,\"start\":39127},{\"end\":39154,\"start\":39143},{\"end\":39162,\"start\":39154},{\"end\":39174,\"start\":39162},{\"end\":39568,\"start\":39554},{\"end\":39584,\"start\":39568},{\"end\":39596,\"start\":39584},{\"end\":39616,\"start\":39596},{\"end\":39909,\"start\":39898},{\"end\":39935,\"start\":39909},{\"end\":39951,\"start\":39935},{\"end\":40288,\"start\":40268},{\"end\":40301,\"start\":40288},{\"end\":40323,\"start\":40301},{\"end\":40341,\"start\":40323},{\"end\":40355,\"start\":40341},{\"end\":40375,\"start\":40355},{\"end\":40393,\"start\":40375},{\"end\":40412,\"start\":40393},{\"end\":40427,\"start\":40412},{\"end\":40933,\"start\":40917},{\"end\":40946,\"start\":40933},{\"end\":40958,\"start\":40946},{\"end\":41188,\"start\":41167},{\"end\":41368,\"start\":41353},{\"end\":41383,\"start\":41368},{\"end\":41899,\"start\":41885},{\"end\":41908,\"start\":41899},{\"end\":41916,\"start\":41908},{\"end\":41926,\"start\":41916},{\"end\":41938,\"start\":41926},{\"end\":41951,\"start\":41938},{\"end\":41963,\"start\":41951},{\"end\":41968,\"start\":41963},{\"end\":42276,\"start\":42267},{\"end\":42285,\"start\":42276},{\"end\":42295,\"start\":42285},{\"end\":42309,\"start\":42295},{\"end\":42322,\"start\":42309},{\"end\":42334,\"start\":42322},{\"end\":42562,\"start\":42550},{\"end\":42580,\"start\":42562},{\"end\":42934,\"start\":42922},{\"end\":42949,\"start\":42934},{\"end\":42963,\"start\":42949},{\"end\":42973,\"start\":42963},{\"end\":43394,\"start\":43379},{\"end\":43408,\"start\":43394},{\"end\":43645,\"start\":43634},{\"end\":43659,\"start\":43645},{\"end\":43668,\"start\":43659},{\"end\":43685,\"start\":43668},{\"end\":43700,\"start\":43685},{\"end\":43713,\"start\":43700},{\"end\":44001,\"start\":43990},{\"end\":44009,\"start\":44001},{\"end\":44021,\"start\":44009},{\"end\":44035,\"start\":44021},{\"end\":44056,\"start\":44035},{\"end\":44276,\"start\":44266},{\"end\":44410,\"start\":44396},{\"end\":44429,\"start\":44410},{\"end\":44752,\"start\":44740},{\"end\":44766,\"start\":44752},{\"end\":44770,\"start\":44766},{\"end\":44976,\"start\":44956},{\"end\":44991,\"start\":44976},{\"end\":45003,\"start\":44991},{\"end\":45017,\"start\":45003},{\"end\":45467,\"start\":45450},{\"end\":45483,\"start\":45467},{\"end\":45502,\"start\":45483},{\"end\":45769,\"start\":45752},{\"end\":45785,\"start\":45769},{\"end\":45800,\"start\":45785},{\"end\":45818,\"start\":45800},{\"end\":46135,\"start\":46121},{\"end\":46148,\"start\":46135},{\"end\":46163,\"start\":46148},{\"end\":46552,\"start\":46538},{\"end\":46567,\"start\":46552},{\"end\":46583,\"start\":46567},{\"end\":46595,\"start\":46583},{\"end\":46610,\"start\":46595},{\"end\":46624,\"start\":46610},{\"end\":46638,\"start\":46624},{\"end\":46658,\"start\":46638},{\"end\":46936,\"start\":46926},{\"end\":46975,\"start\":46962},{\"end\":46987,\"start\":46975},{\"end\":46996,\"start\":46987},{\"end\":47007,\"start\":46996},{\"end\":47011,\"start\":47007},{\"end\":47269,\"start\":47261},{\"end\":47281,\"start\":47269},{\"end\":47290,\"start\":47281},{\"end\":47298,\"start\":47290},{\"end\":47310,\"start\":47298},{\"end\":47323,\"start\":47310},{\"end\":47336,\"start\":47323},{\"end\":47349,\"start\":47336},{\"end\":47845,\"start\":47828},{\"end\":47859,\"start\":47845},{\"end\":48115,\"start\":48099},{\"end\":48134,\"start\":48115},{\"end\":48434,\"start\":48419},{\"end\":48452,\"start\":48434},{\"end\":48468,\"start\":48452},{\"end\":48482,\"start\":48468},{\"end\":48496,\"start\":48482},{\"end\":48514,\"start\":48496},{\"end\":49013,\"start\":48989},{\"end\":49335,\"start\":49323},{\"end\":49354,\"start\":49335},{\"end\":49526,\"start\":49514},{\"end\":49539,\"start\":49526},{\"end\":49805,\"start\":49790},{\"end\":49825,\"start\":49805},{\"end\":49842,\"start\":49825},{\"end\":49857,\"start\":49842},{\"end\":49877,\"start\":49857},{\"end\":50170,\"start\":50149},{\"end\":50183,\"start\":50170},{\"end\":50199,\"start\":50183},{\"end\":50212,\"start\":50199},{\"end\":50229,\"start\":50212},{\"end\":50241,\"start\":50229},{\"end\":50564,\"start\":50550},{\"end\":50579,\"start\":50564},{\"end\":50590,\"start\":50579},{\"end\":50600,\"start\":50590},{\"end\":50610,\"start\":50600},{\"end\":50890,\"start\":50872},{\"end\":51196,\"start\":51166},{\"end\":51213,\"start\":51196},{\"end\":51223,\"start\":51213},{\"end\":51606,\"start\":51591},{\"end\":51625,\"start\":51606},{\"end\":51644,\"start\":51625},{\"end\":51661,\"start\":51644},{\"end\":51676,\"start\":51661},{\"end\":51690,\"start\":51676},{\"end\":51697,\"start\":51690},{\"end\":52200,\"start\":52184},{\"end\":52218,\"start\":52200},{\"end\":52694,\"start\":52675},{\"end\":52713,\"start\":52694},{\"end\":52727,\"start\":52713},{\"end\":52739,\"start\":52727},{\"end\":52755,\"start\":52739},{\"end\":53215,\"start\":53198},{\"end\":53229,\"start\":53215},{\"end\":53251,\"start\":53229},{\"end\":53264,\"start\":53251},{\"end\":53278,\"start\":53264},{\"end\":53298,\"start\":53278},{\"end\":53312,\"start\":53298},{\"end\":53329,\"start\":53312},{\"end\":53345,\"start\":53329},{\"end\":53362,\"start\":53345},{\"end\":53677,\"start\":53663},{\"end\":53695,\"start\":53677},{\"end\":53711,\"start\":53695},{\"end\":53726,\"start\":53711},{\"end\":53746,\"start\":53726},{\"end\":53761,\"start\":53746},{\"end\":53778,\"start\":53761},{\"end\":53793,\"start\":53778},{\"end\":53811,\"start\":53793},{\"end\":53826,\"start\":53811},{\"end\":54348,\"start\":54334},{\"end\":54363,\"start\":54348},{\"end\":54379,\"start\":54363},{\"end\":54396,\"start\":54379},{\"end\":54420,\"start\":54396},{\"end\":54433,\"start\":54420},{\"end\":54786,\"start\":54772},{\"end\":54801,\"start\":54786},{\"end\":55183,\"start\":55167},{\"end\":55204,\"start\":55183},{\"end\":55222,\"start\":55204},{\"end\":55235,\"start\":55222},{\"end\":55251,\"start\":55235},{\"end\":55268,\"start\":55251},{\"end\":55750,\"start\":55734},{\"end\":55764,\"start\":55750},{\"end\":55777,\"start\":55764},{\"end\":55794,\"start\":55777},{\"end\":55807,\"start\":55794},{\"end\":55822,\"start\":55807},{\"end\":55837,\"start\":55822},{\"end\":55855,\"start\":55837},{\"end\":56253,\"start\":56240},{\"end\":56263,\"start\":56253},{\"end\":56273,\"start\":56263},{\"end\":56288,\"start\":56273},{\"end\":56301,\"start\":56288},{\"end\":56313,\"start\":56301},{\"end\":56322,\"start\":56313},{\"end\":56332,\"start\":56322},{\"end\":56343,\"start\":56332},{\"end\":56851,\"start\":56836},{\"end\":57037,\"start\":57022},{\"end\":57051,\"start\":57037},{\"end\":57064,\"start\":57051},{\"end\":57272,\"start\":57260},{\"end\":57282,\"start\":57272},{\"end\":57296,\"start\":57282},{\"end\":57310,\"start\":57296},{\"end\":57322,\"start\":57310},{\"end\":57331,\"start\":57322},{\"end\":57342,\"start\":57331},{\"end\":57694,\"start\":57686},{\"end\":57707,\"start\":57694},{\"end\":57721,\"start\":57707},{\"end\":57734,\"start\":57721},{\"end\":57749,\"start\":57734},{\"end\":58202,\"start\":58189},{\"end\":58217,\"start\":58202},{\"end\":58231,\"start\":58217},{\"end\":58243,\"start\":58231},{\"end\":58255,\"start\":58243},{\"end\":58726,\"start\":58717},{\"end\":58740,\"start\":58726},{\"end\":58750,\"start\":58740},{\"end\":58761,\"start\":58750},{\"end\":58772,\"start\":58761},{\"end\":58787,\"start\":58772},{\"end\":58804,\"start\":58787},{\"end\":58817,\"start\":58804},{\"end\":58832,\"start\":58817},{\"end\":59393,\"start\":59380},{\"end\":59407,\"start\":59393},{\"end\":59431,\"start\":59407},{\"end\":59444,\"start\":59431},{\"end\":59460,\"start\":59444},{\"end\":59465,\"start\":59460},{\"end\":59935,\"start\":59921},{\"end\":59952,\"start\":59935},{\"end\":59974,\"start\":59952},{\"end\":59985,\"start\":59974},{\"end\":60266,\"start\":60254},{\"end\":60279,\"start\":60266},{\"end\":60294,\"start\":60279},{\"end\":60305,\"start\":60294},{\"end\":60314,\"start\":60305},{\"end\":60678,\"start\":60666},{\"end\":60689,\"start\":60678},{\"end\":60702,\"start\":60689},{\"end\":60716,\"start\":60702},{\"end\":60732,\"start\":60716},{\"end\":60750,\"start\":60732},{\"end\":61202,\"start\":61189},{\"end\":61213,\"start\":61202},{\"end\":61226,\"start\":61213},{\"end\":61237,\"start\":61226},{\"end\":61251,\"start\":61237},{\"end\":61260,\"start\":61251},{\"end\":61273,\"start\":61260},{\"end\":61284,\"start\":61273},{\"end\":61297,\"start\":61284},{\"end\":61560,\"start\":61548},{\"end\":61627,\"start\":61611}]", "bib_venue": "[{\"end\":33146,\"start\":33072},{\"end\":36905,\"start\":36843},{\"end\":37640,\"start\":37564},{\"end\":38659,\"start\":38606},{\"end\":41562,\"start\":41481},{\"end\":42701,\"start\":42649},{\"end\":43114,\"start\":43052},{\"end\":45166,\"start\":45100},{\"end\":46284,\"start\":46232},{\"end\":47492,\"start\":47429},{\"end\":51818,\"start\":51766},{\"end\":52294,\"start\":52276},{\"end\":52896,\"start\":52834},{\"end\":55417,\"start\":55351},{\"end\":56486,\"start\":56423},{\"end\":57878,\"start\":57822},{\"end\":58396,\"start\":58334},{\"end\":58975,\"start\":58912},{\"end\":59594,\"start\":59538},{\"end\":60423,\"start\":60377},{\"end\":60891,\"start\":60829},{\"end\":32099,\"start\":32033},{\"end\":32213,\"start\":32169},{\"end\":33007,\"start\":32958},{\"end\":34015,\"start\":33977},{\"end\":34643,\"start\":34556},{\"end\":35329,\"start\":35267},{\"end\":35807,\"start\":35763},{\"end\":36080,\"start\":36026},{\"end\":36491,\"start\":36442},{\"end\":36841,\"start\":36764},{\"end\":37169,\"start\":37100},{\"end\":37562,\"start\":37471},{\"end\":38091,\"start\":38013},{\"end\":38577,\"start\":38509},{\"end\":39237,\"start\":39174},{\"end\":39629,\"start\":39616},{\"end\":39896,\"start\":39808},{\"end\":40479,\"start\":40427},{\"end\":40973,\"start\":40958},{\"end\":41479,\"start\":41383},{\"end\":42038,\"start\":41984},{\"end\":42265,\"start\":42239},{\"end\":42647,\"start\":42580},{\"end\":43050,\"start\":42973},{\"end\":43377,\"start\":43342},{\"end\":43632,\"start\":43556},{\"end\":44094,\"start\":44056},{\"end\":44477,\"start\":44433},{\"end\":44738,\"start\":44696},{\"end\":45098,\"start\":45017},{\"end\":45551,\"start\":45502},{\"end\":45877,\"start\":45834},{\"end\":46230,\"start\":46163},{\"end\":46696,\"start\":46658},{\"end\":47048,\"start\":47027},{\"end\":47427,\"start\":47349},{\"end\":47911,\"start\":47859},{\"end\":48188,\"start\":48134},{\"end\":48563,\"start\":48514},{\"end\":49102,\"start\":49029},{\"end\":49358,\"start\":49354},{\"end\":49591,\"start\":49539},{\"end\":49788,\"start\":49726},{\"end\":50290,\"start\":50241},{\"end\":50548,\"start\":50501},{\"end\":50870,\"start\":50795},{\"end\":51164,\"start\":51110},{\"end\":51764,\"start\":51697},{\"end\":52274,\"start\":52218},{\"end\":52832,\"start\":52755},{\"end\":53196,\"start\":53151},{\"end\":53916,\"start\":53842},{\"end\":54481,\"start\":54437},{\"end\":54912,\"start\":54817},{\"end\":55349,\"start\":55268},{\"end\":55904,\"start\":55855},{\"end\":56421,\"start\":56343},{\"end\":56834,\"start\":56814},{\"end\":57020,\"start\":56961},{\"end\":57410,\"start\":57358},{\"end\":57820,\"start\":57749},{\"end\":58332,\"start\":58255},{\"end\":58910,\"start\":58832},{\"end\":59536,\"start\":59465},{\"end\":60037,\"start\":59985},{\"end\":60375,\"start\":60314},{\"end\":60827,\"start\":60750},{\"end\":61187,\"start\":61131},{\"end\":61756,\"start\":61643}]"}}}, "year": 2023, "month": 12, "day": 17}