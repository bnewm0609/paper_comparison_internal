{"id": 236151316, "updated": "2022-04-09 22:44:04.458", "metadata": {"title": "OnlineHD: Robust, Efficient, and Single-Pass Online Learning Using Hyperdimensional System", "authors": "[{\"first\":\"Alejandro\",\"last\":\"Hern\u00e1ndez-Cano\",\"middle\":[]},{\"first\":\"Namiko\",\"last\":\"Matsumoto\",\"middle\":[]},{\"first\":\"Eric\",\"last\":\"Ping\",\"middle\":[]},{\"first\":\"Mohsen\",\"last\":\"Imani\",\"middle\":[]}]", "venue": "2021 Design, Automation & Test in Europe Conference & Exhibition (DATE)", "journal": "2021 Design, Automation & Test in Europe Conference & Exhibition (DATE)", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Hyper-Dimensional computing (HDC) is a braininspired learning approach for efficient and robust learning on today\u2019s embedded devices. HDC supports single-pass learning, where it generates a classification model by one-time looking at each training data point. However, the single-pass model provides weak classification accuracy due to model saturation caused by naively accumulating high-dimensional data. Although the retraining model for hundreds of iterations addresses the model saturation and boosts the accuracy, it comes with significant training costs. In this paper, we propose OnlineHD, an adaptive HDC training framework for accurate, efficient, and robust learning. During single-pass training, OnlineHD identifies common patterns and eliminates model saturation. For each data point, OnlineHD updates the model depending on how similar it is to the existing model, instead of naive data accumulation. We expand the OnlineHD framework to support highly-accurate iterative training. We also exploit the holographic distribution of patterns in high-dimensional space to make OnlineHD ultra-robust against possible noise and hardware failure. Our evaluations on a wide range of classification problems show that OnlineHD adaptive training provides comparable classification accuracy to the retrained model while getting all efficiency benefits that a singlepass training provides. OnlineHD achieves, on average, 3.5\u00d7 and 6.9\u00d7 (3.7\u00d7 and 5.8\u00d7) faster and more efficient training as compared to state-of-the-art machine learning (HDC algorithms), while providing similar classification accuracy and 8.5\u00d7 higher robustness to a hardware error.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/date/Hernandez-CaneM21", "doi": "10.23919/date51398.2021.9474107"}}, "content": {"source": {"pdf_hash": "16afe485e26d4801ba1dcc6bfcaf961169480304", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "b0a946b3559b4a9171e4b5455b1c045985ef426d", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/16afe485e26d4801ba1dcc6bfcaf961169480304.txt", "contents": "\nOnlineHD: Robust, Efficient, and Single-Pass Online Learning Using Hyperdimensional System\n\n\nAlejandro Hern\u00e1ndez-Cano \nNamiko Matsumoto \nUniversity of California San Diego\n\n\nEric Ping \nUniversity of California San Diego\n\n\nMohsen Imani m.imani@uci.edu \nUniversity of California Irvine\n\n\n\nUniversidad Nacional Aut\u00f3noma de M\u00e9xico\n\n\nOnlineHD: Robust, Efficient, and Single-Pass Online Learning Using Hyperdimensional System\n\nHyper-Dimensional computing (HDC) is a braininspired learning approach for efficient and robust learning on today's embedded devices. HDC supports single-pass learning, where it generates a classification model by one-time looking at each training data point. However, the single-pass model provides weak classification accuracy due to model saturation caused by naively accumulating high-dimensional data. Although the retraining model for hundreds of iterations addresses the model saturation and boosts the accuracy, it comes with significant training costs. In this paper, we propose OnlineHD, an adaptive HDC training framework for accurate, efficient, and robust learning. During single-pass training, OnlineHD identifies common patterns and eliminates model saturation. For each data point, OnlineHD updates the model depending on how similar it is to the existing model, instead of naive data accumulation. We expand the OnlineHD framework to support highly-accurate iterative training. We also exploit the holographic distribution of patterns in high-dimensional space to make OnlineHD ultra-robust against possible noise and hardware failure. Our evaluations on a wide range of classification problems show that OnlineHD adaptive training provides comparable classification accuracy to the retrained model while getting all efficiency benefits that a singlepass training provides. OnlineHD achieves, on average, 3.5\u00d7 and 6.9\u00d7 (3.7\u00d7 and 5.8\u00d7) faster and more efficient training as compared to state-of-the-art machine learning (HDC algorithms), while providing similar classification accuracy and 8.5\u00d7 higher robustness to a hardware error.\n\nI. INTRODUCTION\n\nInternet of Things (IoT) applications collect large amounts of data from various devices and apply machine learning (ML) algorithms to transform that data into actionable knowledge. Running ML algorithms requires significant computational power and storage, resulting in systems that stream most or all the data to the cloud for analysis [1]. This inefficiency comes from the following key technical challenges: (i) Edge devices often do not have sufficient resources to support startof-the-art ML workloads in real-time [2], [3], [4]. For example, Deep Neural Networks (DNNs) use complex gradient-based computation, which requires memory and resource over the capability of today's embedded devices to enable online learning [5]. (ii) Existing ML algorithms have high sensitivity to noise and failure and often require floating-point precision for training [3]. However, today's devices depend on unreliable battery sources and have major scalability issues that add a large amount of noise to the hardware [6].\n\nTo address the efficiency and robustness issues, we exploit Hyper-Dimensional Computing (HDC) as an alternative computational model mimicking \"the human brain\" in the functionality level [7], [8], [9]. HDC is based on the fact that the brain works with neural activities in high-dimensional space. It maps data points into high-dimensional space and then perform a nearly-linear training to learn a model. HDC is well suited to address learning tasks for IoT systems as: (i) HDC models are computationally efficient and highly parallel at heart to train and amenable to hardware level optimization [10], [11], [12], [13], [14], [15], (ii) HDC models offer an intuitive and human-interpretable model [16], (iii) it offers a complete computational paradigm that can be applied to cognitive as well as learning problems [17], [18], [19], and (iv) it provides strong robustness to noise -a key strength for IoT systems [20], [21].\n\nExisting HDC algorithms are supporting single-pass training, where learning can perform in by single time looking at each training data points [22], [8]. Although this approach enables fast and real-time learning from a stream of data, it provides very weak classification accuracy. For example, for face recognition [23], single-pass training can result in \u223c70% classification accuracy, which is 25% lower than state-of-theart algorithms. To address this issue, prior work introduced the idea of HDC iterative training, called retraining [8], [24]. Although retraining boosts HDC classification accuracy to a similar level as the state-of-the-art, it removes advantages that the HDC single-pass model provides. For example, to support retraining, devices require to use large off-chip memory to store all training samples.\n\nWe observe that the main limitation of the HDC single-pass training is coming from a naive data accumulating to generate each class hypervector. This causes forgetting and saturation in each class hypervector, where the pattern of common data dominates each class. In this paper, we propose OnlineHD, an adaptive HDC training framework for accurate, efficient, and robust learning. OnlineHD supports single-pass training while ensuring accuracy comparable to the retrained model.\n\n\u2022 During single-pass training, OnlineHD identifies common patterns in each class hypervector and eliminates model saturation. For each data point, OnlineHD updates the model depending on how similar is a data point to the existing model. For data points with high similarity, OnlineHD gives very small weights to them during the model update, while dissimilar data points get higher weight depending on how far they are to the current model. OnlineHD adaptive training provides comparable accuracy to the retrained model while getting all benefits that a single-pass model provides. \u2022 OnlineHD framework also supports highly-accurate and efficient iterative training. OnlineHD starts training from a well-developed single-pass model, and in each iteration, it adaptively updates the model based on the distance of each data to the model. OnlineHD adaptive update ensures high classification accuracy as well as fast converge, which is up to 29.6\u00d7 faster than state-of-the-art retraining approaches [24], [8]. \u2022 We study the impact of different data representation on\n\nOnlineHD robustness to possible noise in the hardware. We show OnlineHD provides maximum robustness when storing information as holographic distribution of patterns in highdimensional space, when failures on a dimension will not result in losing the entire data.\n\nWe evaluate OnlineHD efficiency on a wide range of hD h2 h1  classification problems. Our evaluation shows that OnlineHD provides, on average, 12.1% higher classification accuracy as compared to the state-of-the-art HDC-based algorithm [22], [24] during single-pass training. OnlineHD also provides better accuracy than the HDC-based algorithms using iterative learning while converging with 13.1\u00d7 lower number of iterations.\n\nIn terms of efficiency, OnlineHD provides 3.5\u00d7 and 6.9\u00d7 (3.7\u00d7 and 5.8\u00d7) faster and more efficient training than stateof-the-art DNN (HDC) algorithms. In addition, OnlineHD data representation enables 8.5\u00d7 higher robustness to hardware error as compared to DNN. Our code is available open-source 1 .\n\nII. HYPER-DIMENSIONAL CLASSIFICATION Figure 1 shows an overview of Hyper-Dimensional classification (HDC).\n\nEncoding: The first step in HDC is to map each data points into high-dimensional space. The mapping procedure is often referred to as encoding (shown in Figure 1). HDC uses different encoding methods depending on data types [22], [25], [24]. The encoded data should satisfy the commonsense principle: data points different from each other in the original space should also be different in the HDC space. For example, if a data point is entirely different from another, the corresponding hypervectors should be orthogonal in the HDC space. Assume an input vector (an image, voice, etc.) in original space F = {f 1 , f 2 , \u00b7 \u00b7 \u00b7 , f n } and F \u2208 R n . The encoding module maps this vector into high-dimensional vector, H \u2208 {0, 1} D , where D >> n. The following equation shows an encoding method that maps input vector into highdimensional space [8]: H = Div\u22121 k=0 |f k | \u2208F \u00b7 B k , where B k s are randomly chosen hence orthogonal bipolar base hypervectors of dimension D 10k to retain the spatial or temporal location of features in an input. That is, B k \u2208 {\u22121, +1} D and \u03b4( B k1 , B k2 ) 0, where \u03b4 denotes the cosine similarity\n\nSingle-pass Training: To find the universal property for training dataset, the trainer module linearly combines hypervectors belonging to each class, i.e., adding the hypervectors to create a single hypervector for each class. Once combining all hypervectors, we treat per-class accumulated hypervectors, called class hypervectors, as the learned model. Figure 1b shows HDC functionality during single-pass training. Assuming a problem with k classes, the model represents using: M = { C 1 , C 2 , \u00b7 \u00b7 \u00b7 , C k }. For example, after generating all encoding hypervector of inputs belonging to class/label l, the class hypervector C l can be obtained by bundling (adding) all H l s. Assuming there are J inputs having label l: C l = J j H l j Inference: checks the similarity of each encoded test data with the class hypervector in two steps. The first step encodes 1 https://gitlab.com/biaslab/onlinehd the input (the same encoding used for training) to produce a query hypervector H. Then, as Figure 1 shows, we compute the similarity (\u03b4) of H and all class hypervectors. Query data gets the label of the class with the highest similarity.\n\nRetraining: HDC classification using single-pass training provides poor classification accuracy. Recently, several work [8], [24] showed the necessity of using iterative training, called retraining, in order to improve HDC classification accuracy (shown in Figure 1). Retraining can boost the HDC model's accuracy by discarding the mispredicted queries from corresponding mispredicted classes and adding them to the right class. The retraining continues for multiple iterations until the classification accuracy (over validation data) has small changes during the last few iterations.\n\nHDC retraining involves large number of iterations to converge. Each iteration is computationally expensive as it requires both associative search and model update. In this paper, our goal is to design a novel HDC training algorithm which can provide classification accuracy of iterative and training efficiency of single-pass training at the same time.\n\n\nIII. ONLINEHD ADAPTIVE LEARNING\n\nIn HDC classification, the single-pass model does not well represent the entire dataset. The naive hypervector addition (explained in Section II) results in saturation of class hypervectors by data points with the most common patterns. This saturation hides the information of non-common patterns stored in class hypervectors. For example, consider cat and dog classification problem. HDC training crates two hypervectors; one representing cat and one for dog. Lets assume training data consists of 80% of cat with similar encoded patterns. This common pattern will dominate the cat class and vanishes the pattern of non-common pattern in the class hypervector. Due to model saturation, data points with noncommon patterns are likely to miss-classified by the model. This retraining gives higher weight to data with a non-common pattern to have a higher contribution to the final model. In other words, the retraining is equivalent to giving higher weight to non-common inputs in each class hypervector. In this paper, we explore the opportunity of giving weights to each data point during single-pass training.\n\n\nA. OnlineHD Single-Pass Training\n\nWe propose OnlineHD, an adaptive training framework for efficient and accurate HDC learning. OnlineHD identifies common patterns during training and eliminates the saturation of the class hypervectors during single-pass training. Instead of naively combining all encoded data, our approach adds each encoded data to class hypervectors depending on how much new information the pattern adds to class hypervectors. If a data point already exists in a class hypervector, OnlineHD will add no or a tiny portion of data to the model to eliminate hypervector saturation. If the prediction matches the expected output, no update will be made to avoid overfitting. Mathematically, OnlineHD adaptive learning is equivalent to the retraining phase, as it provides a higher chance and weight to non-common patterns to represent on the final model. This advantage comes without paying the cost of iterative training. Figure 2a shows OnlineHD functionality during adaptive initial training. Let's assume H as a new training data point. OnlineHD computes the cosine similarity of H with all class hypervectors. We compute similarity of a data point with C i as:   \nC l \u2190 C l + \u03b7 (1 \u2212 \u03b4 l ) \u00d7 H C l \u2190 C l \u2212 \u03b7 (1 \u2212 \u03b4 l ) \u00d7 H (1)\nwhere \u03b7 is a learning rate. A large \u03b4 l indicates that the input is a common data point which is already exist in the model. Therefore, our update adds a very small portion of encoded query to model to eliminate model saturation (1 \u2212 \u03b4 l 0). However, small \u03b4 l means that the query has new pattern which does not exist in the model. Thus, the model is updated with a larger factor (1 \u2212 \u03b4 l 1). Figure 3 shows OnlineHD classification accuracy during single-pass training. Our evaluation indicates that OnlineHD learns a much more accurate model as compared to the baseline in the first training iteration. OnlineHD advantage comes from its adaptive learning that bolds the impact of all data points in the model, regardless of their frequency and dominance in the model.\n\n\nB. OnlineHD Iterative Learning\n\nAlthough single-pass training is suitable for fast and ultraefficient learning, embedded devices may have enough resources to perform more accurate learning tasks. OnlineHD supports retraining to enhance the quality of the model. Instead of starting to retrain from a naive initial model, OnlineHD retraining starts from the initial adaptive model (explained in Section III-A). OnlineHD initial model already considered the weight of each input data during single-pass training. Therefore, OnlineHD retraining starts from a well-trained initial model with relatively high classification accuracy. This enables OnlineHD to retrain the model with a much lower number of iterations, resulting in fast convergence. Figure 2b shows OnlineHD functionality during adaptive retraining. OnlineHD follows a similar learning procedure as initial training. \nC l \u2190 C l + \u03b7 (1 \u2212 \u03b4 l ) \u00d7 H C l \u2190 C l \u2212 \u03b7 (1 \u2212 \u03b4 l ) \u00d7 H(2)\nwhere \u03b4 l = \u03b4(H, C l ) and \u03b4 l = \u03b4(H, C l ) are the similarity of data with correct and miss-predicted classes, respectively. This ensures that we update the model based on how far a train data point is miss-classified with the current model. In case of of a very far miss-prediction, \u03b4 l 0, OnlineHD retraining makes a major changes on the mode. While in case of marginal missprediction, \u03b4 l 0, the update makes smaller changes on the model. We also provide separate coefficients for the true and miss-predicted labels, allowing OnlineHD to update each class hypervector independently. Figure 3 shows the classification accuracy of OnlineHD and the baseline HDC during different retraining iterations. Our evaluation indicates that OnlineHD starts learning from higher accuracy using initial adaptive learning. In addition, OnlineHD achieves maximum accuracy with a lower number of iterations as compared to the baseline. Our adaptive retraining also enables HDC to provides higher final accuracy as compared to the baseline HDC algorithm.\n\n\nIV. ONLINEHD ROBUSTNESS AND EFFICIENCY\n\nThe technological and fabrication issues in highly scaled technology nodes add a significant amount of noise to both memory and computing units [26], [27]. In addition, embedded devices are often powered based on unreliable battery sources. All these issues result in adding an extra computational error, which degrades the quality of learning. Unfortunately, the existing ML algorithms have very low robustness to noise in hardware. Deep neural networks (DNNs), as the state-of-theart machine learning algorithms, have very high sensitivity to noise in hardware, especially during training. Earlier work showed that, without enough bit precision, the model training is likely to diverge or provide low accuracy [28] In DNNs, weights represent using fixed-point or floating-point value. An error bit on the exponents or Most Significant Bits (MSBs) results in a major change in the weight value. This makes a traditional floating point or fixed point representation vulnerable and sensitive to an error on the hardware.\n\n\nA. Hypervector Representation\n\nOne of the main advantages of OnlineHD is its high robustness to noise and failure. In OnlineHD, hypervectors are random and holographic with i.i.d. components. Each hypervector stores the information across all its components so that no component is more responsible for storing any piece of information than another. This makes a hypervector robust against errors in its components. OnlineHD efficiency and robustness depend on two parameters: (i) the hypervector dimensionality that determines the hypervector capacity and the level of redundancy, and (ii) the precision of each hypervector element. Increasing dimensionality or precision of elements results in improving the classification accuracy. However, increasing dimensional results in an efficiency issue, while high precision representation reduces the robustness.\n\nIn OnlineHD, the encoding module represents original data as a pattern of vectors in high-dimensional space. The goal of this encoding is to preserve the distance of data points in high-dimensional space. The encoding maps original data, F \u2208 R n to D dimensional vectors. The encoded hypervector can be represented using binary ( H \u2208 {0, 1} D ) or nonbinary values ( H \u2208 R D ). OnlineHD training and inference preserve the hypervector representation. Using binary hypervector, OnlineHD generates binary class hypervectors and uses Hamming distance as the similarity metric. In contrast, using non-binary representation, OnlineHD training generates a nonbinary model and use cosine as a similarity metric. OnlineHD with non-binary hypervector is equivalent to using analog neurons [29], where each dimension gets a high precision value. Figure 4 shows OnlineHD accuracy using binary and integer hypervectors. Our results indicate that OnlineHD with integer elements provides maximum accuracy in much lower dimensionality as compared to using binary hypervector. For example, OnlineHD using 1-bit precision requires D = 6\u22128K dimensions for maximum accuracy, while OnlineHD using 8bit precision provides the same accuracy on much lower dimensionality (D = 2\u22123K). We explore impact of hypervector representation on OnlineHD robustness and efficiency.\n\n\nB. OnlineHD Robustness\n\nAs we explained in Section IV-A, OnlineHD has the option of working with relatively low-dimensional vectors with higher precision elements or high-dimensional binary vectors. This representation affects OnlineHD robustness. In binary representation, an error just flips a dimension from 0 to 1 or 1 to 0. This results in tiny changes in the entire hypervector pattern. In contrast, error in high-precision and low-dimensional representation can have a significant impact on OnlineHD robustness. This lower robustness comes from: (i) low dimensionality of OnlineHD that directly translates to less redundancy, thus fewer dimensions can cover up for a possible loss of an element. (ii) Using high precision elements, an error in most significant bits can make major changes in the absolute values. This can cause a corrupted dimension to get a huge value, resulting in a significant change in the hypervector pattern. Figure 5 shows OnlineHD required dimensionality using different bit precision. The results are average dimensions measured over several applications (listed in Section V). Our evaluation shows that increased precision has a direct impact on OnlineHD dimensionality and robustness. Using low precision vectors, OnlineHD requires to use high dimension to provide acceptable accuracy. This translates to providing higher redundancy and higher robustness in OnlineHD. Figure 6a shows the impact of precision on preserving the distance similarity between the hypervectors in D = 10, 000 dimensions. The results are reported for vectors with different precision and when the hardware platform has different error rates. The error is modeled by randomly flipping arbitrary bits. Our evaluation shows that vectors with low precision have lower sensitivity to noise, meaning that they preserve the similarity. Using high precision vectors, an error has major changes in the patterns and similarity between the vectors. Figure 4b also shows the impact of hardware noise on OnlineHD classification accuracy. The results show the quality loss as compared to OnlineHD with no error rate. Our evaluation indicates that OnlineHD quality loss increases with hypervector precision. In terms of robustness, it is more desirable to use vectors in high-dimensional space and lowprecision values. Ideally, we can use vectors with binary and high dimensional representations that provide maximum redundancy and lowest sensitivity to noise.\n\n\nC. OnlineHD Efficiency: Hardware Platform\n\nAs a light-weight classifier, OnlineHD often runs on embedded devices with limited resources and battery. Embedded CPUs and FPGAs are common devices used in IoT systems [30], [14], [13]. These devices have different resources; thus, they are optimized to run different operations and data representations. As we explained, OnlineHD can work with high-dimensional vectors with low precision elements as well as low-dimensional vectors with high precision elements. In terms of efficiency, selecting between these two configurations depends on the underlying hardware platform. Figure 5 shows OnlineHD dimensionality and parallelism during different bit precision. CPUs are traditionally designed to work with integer values, rather than high-dimensional vectors. In other words, CPUs have limited parallelism, meaning that they can process a few dimensions at a time. This makes CPUs inefficient to process long hypervectors. Therefore, CPUs are more efficient, running OnlineHD in low dimensionality with high precision elements. In contrast, FPGAs consist of several Lookup Tables (LUTs) and Flip-Flops that are significantly efficient for implementing low precision arithmetic operations. For highprecision computation, FPGA computation relies on limited DSP blocks, which are computationally expensive. Therefore, FPGAs are promising to accelerate OnlineHD with highdimensional and low precision elements. Section V-C explores OnlineHD on different hardware platforms. V. EVALUATION\n\n\nA. Experimental Setup\n\nThe proposed OnlineHD framework has been implemented with the two co-designed modules, software implementation and hardware acceleration. In software, we verified the effectiveness of the OnlineHD framework on large-scale learning problems. In hardware, We implement OnlineHD training and testing on two embedded platforms: FPGA and CPU. For FPGA, we design the OnlineHD functionality using Verilog and synthesize it using Xilinx Vivado Design Suite [31]. The synthesis code has been tested on the Kintex-7 FPGA. We ensure our efficiency is higher than the automated FPGA implementation at [12]. For CPU, the OnlineHD code has been written in Python and optimized for performance. The code has been implemented on Raspberry Pi (RPi) 3B+ using ARM Cortex A53 CPU. We evaluate OnlineHD accuracy and efficiency on six popular datasets, listed in Table I.\n\n\nB. OnlineHD Accuracy\n\nState-of-the-art ML Algorithms: We compare OnlineHD classification accuracy with state-of-the-art learning algorithms, including Deep Neural Networks (DNN), Support Vector Machine (SVM), and AdaBoost. The DNN models are trained with Tensorflow, and we exploited the Scikit-learn library to train other ML algorithms. We exploit the common practice of the grid search to identify the best hyper-parameters for each model. Our evaluation shows that OnlineHD provides comparable accuracy to other state-of-the-art algorithms.\n\nState-of-the-art HDC Algorithms: Figure 7 compares OnlineHD classification accuracy with state-of-the-art HDC algorithms [8], [22], [24]. The results are reported when OnlineHD and the baseline are trained using a single-pass and iterative way using D = 10k. Our evaluation shows that the existing HDC algorithms provide very low classification accuracy during single-pass training. OnlineHD address this issue by enabling adaptive training, which avoids HDC model saturation. Over all tested applications, OnlineHD singlepass model provides, on average, 12.1% higher classification accuracy compared to the existing HDC algorithms. Interestingly, OnlineHD single-pass accuracy is even 1.2% more accurate than the costly baseline HDC algorithms with iterative training. Since the initial model is already well-trained, the retraining has a lower impact on OnlineHD accuracy. The results show that OnlineHD iterative learning can improve accuracy by 3.0% as compared to a single-pass model.\n\nPartial Training: Figure 8 shows OnlineHD accuracy during single-pass training when we train on a different portion of data. Our evaluation indicates that OnlineHD achieves maximum accuracy using a much lower portion of training data, while the existing HDC algorithms achieve lower accuracy even using the entire dataset. This makes a fast learner and suited for light-weight embedded devices.  \n\n\nC. OnlineHD Efficiency\n\nWe compare OnlineHD efficiency with DNN and the baseline HDC during training and inference phase on FPGA. All results are reported using binary vectors using dimensionality that both baseline and OnlineHD provide comparable accuracy to DNN. FPGA implementations are optimized to maximize performance by utilizing FPGA resources. During training, OnlineHD provides significantly higher efficiency as compared to DNN and the baseline HDC. This higher efficiency comes from OnlineHD capability in lowering the number of required training iterations. The table in Figure 9a shows the number of training iterations required by OnlineHD and the baseline HDC. OnlineHD adaptive learning significantly reduces the number of required iterations by 13.1\u00d7. In contrast, in baseline HDC, the lack of suitable initial model and naive iterative training increases the number of iterations. As compared to DNNs, OnlineHD not only reduces the number of training iterations but also improves the efficiency of a single training iteration. DNNs are using costly gradient operations for training, while OnlineHD computation happens in highly efficient and parallel. As Figure 9a shows, OnlineHD in single-pass further improves the training efficiency by eliminating iterative learning. In this configuration, OnlineHD can train a model with minimum data communications between memory and the computing units. Our evaluation shows that OnlineHD iterative (single-pass) training provides 5.9\u00d7 and 9.4\u00d7 (17.1\u00d7 and 28.7\u00d7) faster and higher energy efficiency than DNN. Figure 9b also compares the OnlineHD inference efficiency with the state-of-the-art. In HDC-based algorithms, i.e., OnlineHD and baseline HD, the inference efficiency directly depends on the hypervector dimensionality. Since OnlineHD and the baseline are using the same dimensions, they provide the same inference efficiency. This efficiency is higher than DNN as its computation relies on limited and costly DSPs on FPGA. In contrast, OnlineHD uses simple bitwise operations, which can be accelerated using FPGA lookup tables (LUTs). Our evaluation shows that OnlineHD provides 3.5\u00d7 faster and 6.9\u00d7 higher energy efficiency than DNN.\n\n\nD. Precision & Platform\n\nWe compare OnlineHD efficiency on two embedded platforms: Raspberry Pi 3B+ using ARM CPU and Xilinx Kintex-7 FPGA. Table II lists the number of required OnlineHD dimensions in each bit precision that results in maximum classification accuracy. Table II also   Energy-Delay Product (EDP) of FPGA and CPU running OnlineHD using different hypervector precision. All results are normalized to CPU EDP using hypervectors with 32bit precision. Our evaluation shows that the CPU provides the highest efficiency using lower-dimensional vectors. This is because CPUs are taking the same number of resources to perform 1-bit or 8-bit arithmetic operations. This limits the amount of parallelism in the CPU. In contrast, FPGAs are more efficient in processing high-dimensional but low precision vectors. The lookup table and flip-flops resources on FPGA can perform several parallel bitwise operations and enable fast and efficient OnlineHD computation. Our goal is to maximum FPGA throughput, where we can process the maximum number of data points at a time. To eliminate offchip memory to be a bottleneck of computation, FPGA needs to perform maximum computation over each read. We observe that FPGA provides minimum EDP using 2-bit precision. In this precision, OnlineHD maximize FPGA resources while avoiding high precision arithmetic, as the complexity FPGA arithmetic increase exponentially with the bit-width. E. Robustness to Noise Table III compares DNN and OnlineHD computation robustness to the noise in the hardware. For OnlineHD, the results are reported using vectors with different precision. The dimensionality of the vector is selected when OnlineHD provides maximum accuracy. Our evaluation shows that OnlineHD provides significantly higher robustness to noise as compared to DNN. In DNNs, the weights are represented as 8-bit values; thus, an error can result in major changes in the weights and classification accuracy. In OnlineHD, information is stored as a holographic distribution of patterns in high-dimensional space. In this representation, all dimensions are equally contributing to storing information. Therefore, failures on data only result in failure on a portion of each hypervector, not losing the entire information. OnlineHD has maximum robustness using vectors with 1-bit precision, while this robustness reduces with the increase in the precision. During 10% hardware error, OnlineHD robustness is 8.5\u00d7 and 3.8\u00d7 higher than DNN and OnlineHD using 8-bit precision elements, respectively.\n\nVI. CONCLUSION In this paper, we propose OnlineHD, an adaptive HDC training framework for accurate, efficient, and robust learning. During single-pass training, OnlineHD identifies common patterns and eliminates model saturation. We expand OnlineHD to support highly-accurate iterative training. We also exploit the holographic distribution of patterns in high-dimensional space to make OnlineHD ultra-robust against possible noise and hardware failure. Our evaluations show that OnlineHD provides comparable accuracy to the retrained model while providing all efficiency benefits of a single-pass model. ACKNOWLEDGMENT This work was partially supported by Semiconductor Research Corporation (SRC) Task No. 2988.001.\n\nFig. 1 .\n1(a) HDC classification steps, (b) single-pass training.\n\nFig. 2 .\n2(a) OnlineHD single-pass training, (b) adaptive iterative training. \u03b4( H, C l ) = H\u00b7 C l H \u00b7 C l where H \u00b7 C l is a dot product between a query and class hypervector ( \u2022 A ). The \u03b4 value shows the similarity of a data point to its class hypervector. Instead of naively adding data point to the model, OnlineHD updates the model based on the \u03b4 similarity. For example, if an input data has label l, and the most similar class was label l , the model updates as follows ( \u2022 B ).\n\n\nFor each training data point, say H, OnlineHD checks the similarity of data with all class hypervectors in the model ( \u2022 C ) and updates the model for each miss-prediction ( \u2022 D ). Retraining examines if the model correctly returns the label l for an encoded query H. If the model mispredicts it as label l , the model updates as follows ( \u2022 E ).\n\nFig. 3 .\n3OnlineHD and baseline accuracy during retraining iterations: (a) speech recognition, (b) activity recognition.\n\nFig. 5 .\n5OnlineHD dimensionality, parallelism, and robustness using different data representations.\n\nFig. 6 .\n6(a) Error in distance similarity, and (b) sensitivity of OnlineHD accuracy to noise in the hardware.\n\nFig. 7 .\n7Comparing OnlineHD accuracy to state-of-the-art.\n\nFig. 8 .\n8OnlineHD partial single-pass training.\n\nFig. 9 .\n9OnlineHD efficiency vs. state-of-the-art.\n\nTABLE I\nIDATASETS (n: FEATURE SIZE, k: NUMBER OF CLASSES) \n\nn \nk \n\nTrain \nSize \n\nTest \nSize \nDescription \n\nMNIST \n784 10 \n60,000 \n10,000 \nHandwritten Recognition[32] \nUCIHAR \n561 12 \n6,213 \n1,554 \nActivity Recognition(Mobile)[33] \nISOLET \n617 26 \n6,238 \n1,559 \nVoice Recognition [34] \nFACE \n608 \n2 \n522,441 \n2,494 \nFace Recognition[23] \nPAMAP \n75 \n5 \n611,142 \n101,582 \nActivity Recognition(IMU) [35] \nPECAN \n312 \n3 \n22,290 \n5,574 \nUrban Electricity Prediction [36] \n\n\n\nTABLE II IMPACT\nIIOF BIT PRECISION ON CPU & FPGA EFFICIENCY32-bits \n16-bits \n8-bits \n4-bits \n2-bits \n1-bits \n\nDimensions (D) \n1.2K \n2.1K \n3.6K \n5.6K \n7.5K \n8.8K \n\nEDP \nCPU \n1\u00d7 \n1.11\u00d7 \n1.83\u00d7 \n2.24\u00d7 \n3.1\u00d7 \n4.02\u00d7 \nFPGA \n12.90\u00d7 \n11.39\u00d7 \n6.91\u00d7 \n5.64\u00d7 4.08\u00d7 \n4.19\u00d7 \n\n\n\nTABLE III ONLINEHD\nIIIQUALITY LOSS USING NOISY HARDWAREHardware Error \n1% \n2% \n5% \n10% \n15% \n\nDNN \n3.9% \n9.4% \n16.3% \n26.4% \n40.0% \n\nOnlineHD \n\n1-bit \n0.0% \n0.0% \n0.9% \n3.1% \n5.2% \n2-bits \n0.0% \n0.4% \n1.4% \n4.7% \n7.9% \n4-bits \n0.3% \n1.1% \n2.6% \n7.3% \n11.9% \n8-bits \n1.2% \n3.7% \n5.5% \n12.4% \n18.7% \n\n\n\nEdge-centric computing: Vision and challenges. P , Garcia Lopez, SIGCOMM. 455P. Garcia Lopez et al., \"Edge-centric computing: Vision and challenges,\" SIGCOMM, vol. 45, no. 5, pp. 37-42, 2015.\n\nMulti-target adaptive reconfigurable acceleration for low-power iot processing. M Brandalero, TC. M. Brandalero et al., \"Multi-target adaptive reconfigurable acceleration for low-power iot processing,\" TC, 2020.\n\nDual: Acceleration of clustering algorithms using digital-based processing in-memory. M Imani, IEEE/ACM MICRO. IEEEM. Imani et al., \"Dual: Acceleration of clustering algorithms using digital-based processing in-memory,\" in IEEE/ACM MICRO, pp. 356-371, IEEE, 2020.\n\nFinn-l: Library extensions and design trade-off analysis for variable precision lstm networks on fpgas. V Rybalkin, FPL. IEEEV. Rybalkin et al., \"Finn-l: Library extensions and design trade-off analysis for variable precision lstm networks on fpgas,\" in FPL, pp. 89-897, IEEE, 2018.\n\nConstrained evolutionary piecemeal training to design convolutional neural networks. D Sapra, A D Pimentel, IEA/AIE. 2020D. Sapra and A. D. Pimentel, \"Constrained evolutionary piecemeal training to design convolutional neural networks,\" in IEA/AIE, 2020.\n\nOdd-ecc: on-demand dram error correcting codes. A Malek, MEMSYS. A. Malek et al., \"Odd-ecc: on-demand dram error correcting codes,\" in MEMSYS, pp. 96-111, 2017.\n\nHyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors. P Kanerva, Cognitive Computation. 12P. Kanerva, \"Hyperdimensional computing: An introduction to computing in dis- tributed representation with high-dimensional random vectors,\" Cognitive Computa- tion, vol. 1, no. 2, pp. 139-159, 2009.\n\nA framework for collaborative learning in secure high-dimensional space. M Imani, IEEECLOUDM. Imani et al., \"A framework for collaborative learning in secure high-dimensional space,\" in CLOUD, pp. 435-446, IEEE, 2019.\n\nBric: Locality-based encoding for energy-efficient brain-inspired hyperdimensional computing. M Imani, IEEE/ACM DAC. M. Imani et al., \"Bric: Locality-based encoding for energy-efficient brain-inspired hyperdimensional computing,\" in IEEE/ACM DAC, pp. 1-6, 2019.\n\nExploring hyperdimensional associative memory. M Imani, HPCA. IEEEM. Imani et al., \"Exploring hyperdimensional associative memory,\" in HPCA, pp. 445- 456, IEEE, 2017.\n\ntiny-HD: Ultra-Efficient Hyperdimensional Computing Engine for IoT Applications. B Khaleghi, DATE. 2021IEEEB. Khaleghi et al., \"tiny-HD: Ultra-Efficient Hyperdimensional Computing Engine for IoT Applications,\" in DATE, IEEE, 2021.\n\nF5-hd: Fast flexible fpga-based framework for refreshing hyperdimensional computing. S Salamat, FPGA. S. Salamat et al., \"F5-hd: Fast flexible fpga-based framework for refreshing hyperdi- mensional computing,\" in FPGA, pp. 53-62, 2019.\n\nShear er: highly-efficient hyperdimensional computing by software-hardware enabled multifold approximation. B Khaleghi, ISLPED. B. Khaleghi et al., \"Shear er: highly-efficient hyperdimensional computing by software-hardware enabled multifold approximation,\" in ISLPED, pp. 241-246, 2020.\n\nSparsehd: Algorithm-hardware co-optimization for efficient highdimensional computing. M Imani, FCCM. IEEEM. Imani et al., \"Sparsehd: Algorithm-hardware co-optimization for efficient high- dimensional computing,\" in FCCM, pp. 190-198, IEEE, 2019.\n\nThrifty: Training with hyperdimensional computing across flash hierarchy. S Gupta, ICCAD. IEEES. Gupta et al., \"Thrifty: Training with hyperdimensional computing across flash hierarchy,\" in ICCAD, pp. 1-9, IEEE, 2020.\n\nLearning sensorimotor control with neuromorphic sensors: Toward hyperdimensional active perception. A Mitrokhin, Science Robotics. 430A. Mitrokhin et al., \"Learning sensorimotor control with neuromorphic sensors: Toward hyperdimensional active perception,\" Science Robotics, vol. 4, no. 30, 2019.\n\nRevisiting hyperdimensional learning for fpga and low-power architectures. M Imani, HPCA. IEEE2021M. Imani et al., \"Revisiting hyperdimensional learning for fpga and low-power architectures,\" in HPCA, IEEE, 2021.\n\nSynergiclearning: Neural network-based feature extraction for highly-accurate hyperdimensional learning. M Nazemi, arXiv:2007.15222arXiv preprintM. Nazemi et al., \"Synergiclearning: Neural network-based feature extraction for highly-accurate hyperdimensional learning,\" arXiv preprint arXiv:2007.15222, 2020.\n\nAdversarial attacks on brain-inspired hyperdimensional computingbased classifiers. F Yang, arXiv:2006.05594arXiv preprintF. Yang et al., \"Adversarial attacks on brain-inspired hyperdimensional computing- based classifiers,\" arXiv preprint arXiv:2006.05594, 2020.\n\nGeniehd: Efficient dna pattern matching accelerator using hyperdimensional computing. Y Kim, DATE. IEEEY. Kim et al., \"Geniehd: Efficient dna pattern matching accelerator using hyperdimen- sional computing,\" in DATE, IEEE, 2020.\n\nAccelerating hyperdimensional computing on fpgas by exploiting computational reuse. S Salamat, IEEE TC. S. Salamat et al., \"Accelerating hyperdimensional computing on fpgas by exploiting computational reuse,\" IEEE TC, 2020.\n\nA robust and energy-efficient classifier using brain-inspired hyperdimensional computing. A Rahimi, ISLPED. ACMA. Rahimi et al., \"A robust and energy-efficient classifier using brain-inspired hyper- dimensional computing,\" in ISLPED, pp. 64-69, ACM, 2016.\n\nPruning training sets for learning of object categories. A Angelova, CVPR. IEEEA. Angelova et al., \"Pruning training sets for learning of object categories,\" in CVPR, IEEE, 2005.\n\nA binary learning framework for hyperdimensional computing. M Imani, DATE. IEEEM. Imani et al., \"A binary learning framework for hyperdimensional computing,\" in DATE, pp. 126-131, IEEE, 2019.\n\nHyperdimensional biosignal processing: A case study for emg-based hand gesture recognition. A Rahimi, ICRC. IEEEA. Rahimi et al., \"Hyperdimensional biosignal processing: A case study for emg-based hand gesture recognition,\" in ICRC, pp. 1-8, IEEE, 2016.\n\nReliability-aware runtime power management for many-core systems in the dark silicon era. A Rahmani, TVLSI. 252A. Rahmani et al., \"Reliability-aware runtime power management for many-core systems in the dark silicon era,\" TVLSI, vol. 25, no. 2, pp. 427-440, 2016.\n\nA scalable design of multi-bit ferroelectric content addressable memory for data-centric computing. C Li, IEDM. IEEEC. Li et al., \"A scalable design of multi-bit ferroelectric content addressable memory for data-centric computing,\" in IEDM, IEEE, 2020.\n\nDeep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. S Han, arXiv:1510.00149arXiv preprintS. Han et al., \"Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding,\" arXiv preprint arXiv:1510.00149, 2015.\n\nNeuromorphic architectures for spiking deep neural networks. G Indiveri, IEDM. IEEEG. Indiveri et al., \"Neuromorphic architectures for spiking deep neural networks,\" in IEDM, pp. 4-2, IEEE, 2015.\n\nFinn-r: An end-to-end deep-learning framework for fast exploration of quantized neural networks. M Blott, TRETS. 113M. Blott et al., \"Finn-r: An end-to-end deep-learning framework for fast exploration of quantized neural networks,\" TRETS, vol. 11, no. 3, 2018.\n\nVivado design suite. T Feist, White Paper. 5T. Feist, \"Vivado design suite,\" White Paper, vol. 5, 2012.\n\nMulti-column deep neural networks for image classification. D Ciregan, CVPR. IEEED. Ciregan et al., \"Multi-column deep neural networks for image classification,\" in CVPR, pp. 3642-3649, IEEE, 2012.\n\nHuman activity recognition on smartphones using a multiclass hardware-friendly support vector machine. D Anguita, AAL. SpringerD. Anguita et al., \"Human activity recognition on smartphones using a multiclass hardware-friendly support vector machine,\" in AAL, pp. 216-223, Springer, 2012.\n\nUci machine learning repository. \"Uci machine learning repository.\" http://archive.ics.uci.edu/ml/datasets/ISOLET.\n\nIntroducing a new benchmarked dataset for activity monitoring. A Reiss, D Stricker, ISWC. IEEEA. Reiss and D. Stricker, \"Introducing a new benchmarked dataset for activity monitoring,\" in ISWC, pp. 108-109, IEEE, 2012.\n\nPecan street dataport. \"Pecan street dataport.\" https://dataport.cloud/.\n", "annotations": {"author": "[{\"end\":119,\"start\":94},{\"end\":174,\"start\":120},{\"end\":222,\"start\":175},{\"end\":286,\"start\":223},{\"end\":329,\"start\":287}]", "publisher": null, "author_last_name": "[{\"end\":118,\"start\":104},{\"end\":136,\"start\":127},{\"end\":184,\"start\":180},{\"end\":235,\"start\":230}]", "author_first_name": "[{\"end\":103,\"start\":94},{\"end\":126,\"start\":120},{\"end\":179,\"start\":175},{\"end\":229,\"start\":223}]", "author_affiliation": "[{\"end\":173,\"start\":138},{\"end\":221,\"start\":186},{\"end\":285,\"start\":253},{\"end\":328,\"start\":288}]", "title": "[{\"end\":91,\"start\":1},{\"end\":420,\"start\":330}]", "venue": null, "abstract": "[{\"end\":2071,\"start\":422}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2431,\"start\":2428},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2614,\"start\":2611},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2619,\"start\":2616},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2624,\"start\":2621},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2819,\"start\":2816},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2951,\"start\":2948},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3101,\"start\":3098},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3294,\"start\":3291},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3299,\"start\":3296},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3304,\"start\":3301},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3706,\"start\":3702},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3712,\"start\":3708},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3718,\"start\":3714},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3724,\"start\":3720},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3730,\"start\":3726},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3736,\"start\":3732},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3807,\"start\":3803},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3925,\"start\":3921},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3931,\"start\":3927},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3937,\"start\":3933},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4023,\"start\":4019},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4029,\"start\":4025},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4179,\"start\":4175},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4184,\"start\":4181},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4353,\"start\":4349},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4574,\"start\":4571},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4580,\"start\":4576},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6340,\"start\":6336},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6345,\"start\":6342},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6910,\"start\":6906},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6916,\"start\":6912},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7733,\"start\":7729},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7739,\"start\":7735},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7745,\"start\":7741},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8351,\"start\":8348},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9899,\"start\":9896},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9905,\"start\":9901},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":16054,\"start\":16050},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":16060,\"start\":16056},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":16622,\"start\":16618},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":18572,\"start\":18568},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":21813,\"start\":21809},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":21819,\"start\":21815},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":21825,\"start\":21821},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":23605,\"start\":23601},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":23745,\"start\":23741},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":24675,\"start\":24672},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":24681,\"start\":24677},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":24687,\"start\":24683}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":31470,\"start\":31404},{\"attributes\":{\"id\":\"fig_2\"},\"end\":31958,\"start\":31471},{\"attributes\":{\"id\":\"fig_3\"},\"end\":32307,\"start\":31959},{\"attributes\":{\"id\":\"fig_4\"},\"end\":32429,\"start\":32308},{\"attributes\":{\"id\":\"fig_5\"},\"end\":32531,\"start\":32430},{\"attributes\":{\"id\":\"fig_6\"},\"end\":32643,\"start\":32532},{\"attributes\":{\"id\":\"fig_7\"},\"end\":32703,\"start\":32644},{\"attributes\":{\"id\":\"fig_8\"},\"end\":32753,\"start\":32704},{\"attributes\":{\"id\":\"fig_9\"},\"end\":32806,\"start\":32754},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":33275,\"start\":32807},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":33538,\"start\":33276},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":33839,\"start\":33539}]", "paragraph": "[{\"end\":3102,\"start\":2090},{\"end\":4030,\"start\":3104},{\"end\":4855,\"start\":4032},{\"end\":5336,\"start\":4857},{\"end\":6404,\"start\":5338},{\"end\":6668,\"start\":6406},{\"end\":7095,\"start\":6670},{\"end\":7395,\"start\":7097},{\"end\":7503,\"start\":7397},{\"end\":8634,\"start\":7505},{\"end\":9774,\"start\":8636},{\"end\":10360,\"start\":9776},{\"end\":10715,\"start\":10362},{\"end\":11862,\"start\":10751},{\"end\":13049,\"start\":11899},{\"end\":13881,\"start\":13112},{\"end\":14761,\"start\":13916},{\"end\":15863,\"start\":14823},{\"end\":16925,\"start\":15906},{\"end\":17786,\"start\":16959},{\"end\":19134,\"start\":17788},{\"end\":21594,\"start\":19161},{\"end\":23125,\"start\":21640},{\"end\":24002,\"start\":23151},{\"end\":24549,\"start\":24027},{\"end\":25540,\"start\":24551},{\"end\":25938,\"start\":25542},{\"end\":28144,\"start\":25965},{\"end\":30685,\"start\":28172},{\"end\":31403,\"start\":30687}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13111,\"start\":13050},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14822,\"start\":14762}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":24001,\"start\":23994},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":28295,\"start\":28287},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":28429,\"start\":28416},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":29610,\"start\":29601}]", "section_header": "[{\"end\":2088,\"start\":2073},{\"end\":10749,\"start\":10718},{\"end\":11897,\"start\":11865},{\"end\":13914,\"start\":13884},{\"end\":15904,\"start\":15866},{\"end\":16957,\"start\":16928},{\"end\":19159,\"start\":19137},{\"end\":21638,\"start\":21597},{\"end\":23149,\"start\":23128},{\"end\":24025,\"start\":24005},{\"end\":25963,\"start\":25941},{\"end\":28170,\"start\":28147},{\"end\":31413,\"start\":31405},{\"end\":31480,\"start\":31472},{\"end\":32317,\"start\":32309},{\"end\":32439,\"start\":32431},{\"end\":32541,\"start\":32533},{\"end\":32653,\"start\":32645},{\"end\":32713,\"start\":32705},{\"end\":32763,\"start\":32755},{\"end\":32815,\"start\":32808},{\"end\":33292,\"start\":33277},{\"end\":33558,\"start\":33540}]", "table": "[{\"end\":33275,\"start\":32817},{\"end\":33538,\"start\":33336},{\"end\":33839,\"start\":33595}]", "figure_caption": "[{\"end\":31470,\"start\":31415},{\"end\":31958,\"start\":31482},{\"end\":32307,\"start\":31961},{\"end\":32429,\"start\":32319},{\"end\":32531,\"start\":32441},{\"end\":32643,\"start\":32543},{\"end\":32703,\"start\":32655},{\"end\":32753,\"start\":32715},{\"end\":32806,\"start\":32765},{\"end\":33336,\"start\":33295},{\"end\":33595,\"start\":33562}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7442,\"start\":7434},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7666,\"start\":7658},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8999,\"start\":8990},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9636,\"start\":9628},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10041,\"start\":10033},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12813,\"start\":12804},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":13514,\"start\":13506},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":14636,\"start\":14627},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":15418,\"start\":15410},{\"end\":18632,\"start\":18624},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":20085,\"start\":20077},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":20550,\"start\":20541},{\"end\":21096,\"start\":21087},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":22224,\"start\":22216},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":24592,\"start\":24584},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":25568,\"start\":25560},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":26534,\"start\":26525},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":27124,\"start\":27115},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":27519,\"start\":27510}]", "bib_author_first_name": "[{\"end\":33889,\"start\":33888},{\"end\":33898,\"start\":33892},{\"end\":34115,\"start\":34114},{\"end\":34334,\"start\":34333},{\"end\":34617,\"start\":34616},{\"end\":34882,\"start\":34881},{\"end\":34891,\"start\":34890},{\"end\":34893,\"start\":34892},{\"end\":35101,\"start\":35100},{\"end\":35340,\"start\":35339},{\"end\":35650,\"start\":35649},{\"end\":35890,\"start\":35889},{\"end\":36106,\"start\":36105},{\"end\":36308,\"start\":36307},{\"end\":36544,\"start\":36543},{\"end\":36804,\"start\":36803},{\"end\":37071,\"start\":37070},{\"end\":37306,\"start\":37305},{\"end\":37551,\"start\":37550},{\"end\":37824,\"start\":37823},{\"end\":38068,\"start\":38067},{\"end\":38356,\"start\":38355},{\"end\":38623,\"start\":38622},{\"end\":38851,\"start\":38850},{\"end\":39082,\"start\":39081},{\"end\":39306,\"start\":39305},{\"end\":39489,\"start\":39488},{\"end\":39714,\"start\":39713},{\"end\":39967,\"start\":39966},{\"end\":40242,\"start\":40241},{\"end\":40502,\"start\":40501},{\"end\":40763,\"start\":40762},{\"end\":40996,\"start\":40995},{\"end\":41182,\"start\":41181},{\"end\":41326,\"start\":41325},{\"end\":41568,\"start\":41567},{\"end\":41933,\"start\":41932},{\"end\":41942,\"start\":41941}]", "bib_author_last_name": "[{\"end\":33904,\"start\":33899},{\"end\":34126,\"start\":34116},{\"end\":34340,\"start\":34335},{\"end\":34626,\"start\":34618},{\"end\":34888,\"start\":34883},{\"end\":34902,\"start\":34894},{\"end\":35107,\"start\":35102},{\"end\":35348,\"start\":35341},{\"end\":35656,\"start\":35651},{\"end\":35896,\"start\":35891},{\"end\":36112,\"start\":36107},{\"end\":36317,\"start\":36309},{\"end\":36552,\"start\":36545},{\"end\":36813,\"start\":36805},{\"end\":37077,\"start\":37072},{\"end\":37312,\"start\":37307},{\"end\":37561,\"start\":37552},{\"end\":37830,\"start\":37825},{\"end\":38075,\"start\":38069},{\"end\":38361,\"start\":38357},{\"end\":38627,\"start\":38624},{\"end\":38859,\"start\":38852},{\"end\":39089,\"start\":39083},{\"end\":39315,\"start\":39307},{\"end\":39495,\"start\":39490},{\"end\":39721,\"start\":39715},{\"end\":39975,\"start\":39968},{\"end\":40245,\"start\":40243},{\"end\":40506,\"start\":40503},{\"end\":40772,\"start\":40764},{\"end\":41002,\"start\":40997},{\"end\":41188,\"start\":41183},{\"end\":41334,\"start\":41327},{\"end\":41576,\"start\":41569},{\"end\":41939,\"start\":41934},{\"end\":41951,\"start\":41943}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":207232279},\"end\":34032,\"start\":33841},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":216466690},\"end\":34245,\"start\":34034},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":222305243},\"end\":34510,\"start\":34247},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":49669175},\"end\":34794,\"start\":34512},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":216051084},\"end\":35050,\"start\":34796},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":39621273},\"end\":35212,\"start\":35052},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":733980},\"end\":35574,\"start\":35214},{\"attributes\":{\"id\":\"b7\"},\"end\":35793,\"start\":35576},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":163164623},\"end\":36056,\"start\":35795},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":1677864},\"end\":36224,\"start\":36058},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":236150314},\"end\":36456,\"start\":36226},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":67872077},\"end\":36693,\"start\":36458},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":220665748},\"end\":36982,\"start\":36695},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":189824904},\"end\":37229,\"start\":36984},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":227069231},\"end\":37448,\"start\":37231},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":182118830},\"end\":37746,\"start\":37450},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":233376633},\"end\":37960,\"start\":37748},{\"attributes\":{\"doi\":\"arXiv:2007.15222\",\"id\":\"b17\"},\"end\":38270,\"start\":37962},{\"attributes\":{\"doi\":\"arXiv:2006.05594\",\"id\":\"b18\"},\"end\":38534,\"start\":38272},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":219858990},\"end\":38764,\"start\":38536},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":218934679},\"end\":38989,\"start\":38766},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":9812826},\"end\":39246,\"start\":38991},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":14019086},\"end\":39426,\"start\":39248},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":155109576},\"end\":39619,\"start\":39428},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":12008695},\"end\":39874,\"start\":39621},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":6237770},\"end\":40139,\"start\":39876},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":232266194},\"end\":40393,\"start\":40141},{\"attributes\":{\"doi\":\"arXiv:1510.00149\",\"id\":\"b27\"},\"end\":40699,\"start\":40395},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":1065450},\"end\":40896,\"start\":40701},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":219886219},\"end\":41158,\"start\":40898},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":110511037},\"end\":41263,\"start\":41160},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":2161592},\"end\":41462,\"start\":41265},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":13178535},\"end\":41751,\"start\":41464},{\"attributes\":{\"id\":\"b33\"},\"end\":41867,\"start\":41753},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":10337279},\"end\":42087,\"start\":41869},{\"attributes\":{\"id\":\"b35\"},\"end\":42161,\"start\":42089}]", "bib_title": "[{\"end\":33886,\"start\":33841},{\"end\":34112,\"start\":34034},{\"end\":34331,\"start\":34247},{\"end\":34614,\"start\":34512},{\"end\":34879,\"start\":34796},{\"end\":35098,\"start\":35052},{\"end\":35337,\"start\":35214},{\"end\":35887,\"start\":35795},{\"end\":36103,\"start\":36058},{\"end\":36305,\"start\":36226},{\"end\":36541,\"start\":36458},{\"end\":36801,\"start\":36695},{\"end\":37068,\"start\":36984},{\"end\":37303,\"start\":37231},{\"end\":37548,\"start\":37450},{\"end\":37821,\"start\":37748},{\"end\":38620,\"start\":38536},{\"end\":38848,\"start\":38766},{\"end\":39079,\"start\":38991},{\"end\":39303,\"start\":39248},{\"end\":39486,\"start\":39428},{\"end\":39711,\"start\":39621},{\"end\":39964,\"start\":39876},{\"end\":40239,\"start\":40141},{\"end\":40760,\"start\":40701},{\"end\":40993,\"start\":40898},{\"end\":41179,\"start\":41160},{\"end\":41323,\"start\":41265},{\"end\":41565,\"start\":41464},{\"end\":41930,\"start\":41869}]", "bib_author": "[{\"end\":33892,\"start\":33888},{\"end\":33906,\"start\":33892},{\"end\":34128,\"start\":34114},{\"end\":34342,\"start\":34333},{\"end\":34628,\"start\":34616},{\"end\":34890,\"start\":34881},{\"end\":34904,\"start\":34890},{\"end\":35109,\"start\":35100},{\"end\":35350,\"start\":35339},{\"end\":35658,\"start\":35649},{\"end\":35898,\"start\":35889},{\"end\":36114,\"start\":36105},{\"end\":36319,\"start\":36307},{\"end\":36554,\"start\":36543},{\"end\":36815,\"start\":36803},{\"end\":37079,\"start\":37070},{\"end\":37314,\"start\":37305},{\"end\":37563,\"start\":37550},{\"end\":37832,\"start\":37823},{\"end\":38077,\"start\":38067},{\"end\":38363,\"start\":38355},{\"end\":38629,\"start\":38622},{\"end\":38861,\"start\":38850},{\"end\":39091,\"start\":39081},{\"end\":39317,\"start\":39305},{\"end\":39497,\"start\":39488},{\"end\":39723,\"start\":39713},{\"end\":39977,\"start\":39966},{\"end\":40247,\"start\":40241},{\"end\":40508,\"start\":40501},{\"end\":40774,\"start\":40762},{\"end\":41004,\"start\":40995},{\"end\":41190,\"start\":41181},{\"end\":41336,\"start\":41325},{\"end\":41578,\"start\":41567},{\"end\":41941,\"start\":41932},{\"end\":41953,\"start\":41941}]", "bib_venue": "[{\"end\":33913,\"start\":33906},{\"end\":34130,\"start\":34128},{\"end\":34356,\"start\":34342},{\"end\":34631,\"start\":34628},{\"end\":34911,\"start\":34904},{\"end\":35115,\"start\":35109},{\"end\":35371,\"start\":35350},{\"end\":35647,\"start\":35576},{\"end\":35910,\"start\":35898},{\"end\":36118,\"start\":36114},{\"end\":36323,\"start\":36319},{\"end\":36558,\"start\":36554},{\"end\":36821,\"start\":36815},{\"end\":37083,\"start\":37079},{\"end\":37319,\"start\":37314},{\"end\":37579,\"start\":37563},{\"end\":37836,\"start\":37832},{\"end\":38065,\"start\":37962},{\"end\":38353,\"start\":38272},{\"end\":38633,\"start\":38629},{\"end\":38868,\"start\":38861},{\"end\":39097,\"start\":39091},{\"end\":39321,\"start\":39317},{\"end\":39501,\"start\":39497},{\"end\":39727,\"start\":39723},{\"end\":39982,\"start\":39977},{\"end\":40251,\"start\":40247},{\"end\":40499,\"start\":40395},{\"end\":40778,\"start\":40774},{\"end\":41009,\"start\":41004},{\"end\":41201,\"start\":41190},{\"end\":41340,\"start\":41336},{\"end\":41581,\"start\":41578},{\"end\":41784,\"start\":41753},{\"end\":41957,\"start\":41953},{\"end\":42110,\"start\":42089}]"}}}, "year": 2023, "month": 12, "day": 17}