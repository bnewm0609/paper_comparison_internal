{"id": 1410450, "updated": "2023-11-11 01:44:39.999", "metadata": {"title": "Online Object Tracking, Learning and Parsing with And-Or Graphs", "authors": "[{\"first\":\"Tianfu\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Yang\",\"last\":\"Lu\",\"middle\":[]},{\"first\":\"Song-Chun\",\"last\":\"Zhu\",\"middle\":[]}]", "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "publication_date": {"year": 2017, "month": null, "day": null}, "abstract": "This paper presents a method, called <italic>AOGTracker</italic>, for simultaneously tracking, learning and parsing (TLP) of unknown objects in video sequences with a hierarchical and compositional And-Or graph (AOG) representation. The TLP method is formulated in the Bayesian framework with a spatial and a temporal dynamic programming (DP) algorithms inferring object bounding boxes on-the-fly. During online learning, the AOG is discriminatively learned using latent SVM\u00a0<xref ref-type=\"bibr\" rid=\"ref1\">[1]</xref> to account for appearance (e.g., lighting and partial occlusion) and structural (e.g., different poses and viewpoints) variations of a tracked object, as well as distractors (e.g., similar objects) in background. Three key issues in online inference and learning are addressed: (i) maintaining purity of positive and negative examples collected online, (ii) controling model complexity in latent structure learning, and (iii) identifying critical moments to re-learn the structure of AOG based on its intrackability. The intrackability measures uncertainty of an AOG based on its score maps in a frame. In experiments, our AOGTracker is tested on two popular tracking benchmarks with the same parameter setting: the TB-100/50/CVPR2013 benchmarks\u00a0<xref ref-type=\"bibr\" rid=\"ref2\">[2]</xref> , <xref ref-type=\"bibr\" rid=\"ref3\">[3]</xref> , and the VOT benchmarks\u00a0<xref ref-type=\"bibr\" rid=\"ref4\">[4]</xref> \u2014VOT 2013, 2014, 2015 and TIR2015 (thermal imagery tracking). In the former, our AOGTracker outperforms state-of-the-art tracking algorithms including two trackers based on deep convolutional network \u00a0 <xref ref-type=\"bibr\" rid=\"ref5\">[5]</xref> , <xref ref-type=\"bibr\" rid=\"ref6\">[6]</xref> . In the latter, our AOGTracker outperforms all other trackers in VOT2013 and is comparable to the state-of-the-art methods in VOT2014, 2015 and TIR2015.", "fields_of_study": "[\"Medicine\"]", "external_ids": {"arxiv": null, "mag": "2963196719", "acl": null, "pubmed": "28026751", "pubmedcentral": null, "dblp": "journals/pami/WuLZ17", "doi": "10.1109/tpami.2016.2644963"}}, "content": {"source": {"pdf_hash": "9184b9aceb3fc2bdf55656f49d2ac059eaf54ac8", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1509.08067v5.pdf\"]", "oa_url_match": false, "oa_info": {"license": "publisher-specific, author manuscript", "open_access_url": "https://doi.org/10.1109/tpami.2016.2644963", "status": "HYBRID"}}, "grobid": {"id": "3c05a43eaf5a2ea6c7fd2ea5dab4224790b58058", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/9184b9aceb3fc2bdf55656f49d2ac059eaf54ac8.txt", "contents": "\nOnline Object Tracking, Learning and Parsing with And-Or Graphs\n\n\nTianfu Wu \nYang Lu \nSong-Chun Zhu \nOnline Object Tracking, Learning and Parsing with And-Or Graphs\nARXIV VERSION 1Index Terms-Visual TrackingAnd-Or GraphsLatent SVMDynamic ProgrammingIntrackability !\nThis paper presents a method, called AOGTracker, for simultaneously tracking, learning and parsing (TLP) unknown objects in video sequences with a hierarchical and compositional And-Or graph (AOG) representation. The TLP method is formulated in the Bayesian framework with a spatial and a temporal dynamic programming (DP) algorithms inferring object bounding boxes on-the-fly. During online learning, the AOG is discriminatively learned using latent SVM [1] to account for appearance (e.g., lighting and partial occlusion) and structural (e.g., different poses and viewpoints) variations of a tracked object, as well as distractors (e.g., similar objects) in background. Three key issues in online inference and learning are addressed: (i) maintaining purity of positive and negative examples collected online, (ii) controling model complexity in latent structure learning, and (iii) identifying critical moments to re-learn the structure of AOG based on its intrackability. The intrackability measures uncertainty of an AOG based on its score maps in a frame. In experiments, our AOGTracker is tested on two popular tracking benchmarks with the same parameter setting: the TB-100/50/CVPR2013 benchmarks [2],[3], and the VOT benchmarks [4] -VOT 2013, 2015. In the former, our AOGTracker outperforms state-of-the-art tracking algorithms including two trackers based on deep convolutional network [5],[6]. In the latter, our AOGTracker outperforms all other trackers in VOT2013 and is comparable to the state-of-the-art methods in VOT2014, 2015 and TIR2015.\n\nINTRODUCTION\n\n\nMotivation and Objective\n\nO NLINE object tracking is an innate capability in human and animal vision for learning visual concepts [7], and is an important task in computer vision. Given the state of an unknown object (e.g., its bounding box) in the first frame of a video, the task is to infer hidden states of the object in subsequent frames. Online object tracking, especially long-term tracking, is a difficult problem. It needs to handle variations of a tracked object, including appearance and structural variations, scale changes, occlusions (partial or complete), etc. It also needs to tackle complexity of the scene, including camera motion, background clutter, distractors, illumination changes, frame cropping, etc. Fig. 1 illustrates some typical issues in online object tracking. In recent literature, object tracking has received much attention due to practical applications in video surveillance, activity and event prediction, humancomputer interactions and traffic monitoring.\n\nThis paper presents an integrated framework for online tracking, learning and parsing (TLP) unknown objects with a unified representation. We focus on settings in which object state is represented by bounding box, without using pre-trained models. We address five issues associated with online object tracking in the following.\n\nIssue I: Expressive representation accounting for structural and appearance variations of unknown objects in tracking. We are interested in hierarchical and compositional object models. Such\n\nManuscript received MM DD, YYYY; revised MM DD, YYYY. Fig. 1: Illustration of some typical issues in online object tracking using the \"skating1\" video in the benchmark [2]. Starting from the object specified in the first frame, a tracker needs to handle many variations in subsequent frames which include illuminative variation, scale variation, occlusion, deformation, fast motion, inplane and out-of-plane rotation, background clutter, etc. models have shown promising performance in object detection [1], [8], [9], [10], [11] and object recognition [12]. A popular modeling scheme represents object categories by mixtures of deformable part-based models (DPMs) [1]. The number of mixture components is usually predefined and the part configuration of each component is fixed after initialization or directly based on strong supervision. In online tracking, since a tracker can only access the ground-truth object state in the first frame, it is not suitable for it to \"make decisions\" on the number of mixture components and part configurations, and it does not have enough data to learn. It's desirable to have an object representation which has expressive power to represent a large number of part configurations, and can facilitate computationally effective inference and learning. We quantize the space of part configurations recursively in a principled way with a hierarchical and compositional And-Or graph (AOG) representation [8], [11]. We learn and update the Examples of capturing structural and appearance variations of a tracked object by a series of object configurations inferred on-the-fly over key frames #1, #173, #282, etc. (c) Illustration of an object AOG, a parse tree and an object configuration in frame #282. A parse tree is an instantiation of an AOG. A configuration is a layout of latent parts represented by terminal-nodes in a parse tree. An object AOG preserves ambiguities by capturing multiple parse trees.\n\n\nInput\n\nmost discriminative part configurations online by pruning the quantized space based on discriminative power.\n\nIssue II: Computing joint optimal solutions. Online object tracking is usually posed as a maximum a posterior (MAP) problem using first order hidden Markov models (HMMs) [2], [13], [14]. The likelihood or observation density is temporally inhomogeneous due to online updating of object models. Typically, the objective is to infer the most likely hidden state of a tracked object in a frame by maximizing a Bayesian marginal posterior probability given all the data observed so far. The maximization is based on either particle filtering [15] or dense sampling such as the tracking-by-detection methods [16], [17], [18]. In most prior approaches (e.g., the 29 trackers evaluated in the TB-100 benchmark [2]), no feedback inspection is applied to the history of inferred trajectory. We utilize tracking-by-parsing with hierarchical models in inference. We allow feedback inspection by maximizing a Bayesian joint posterior probability of trajectory given all the observation. Thus, a tracker can trace back the trajectory to potentially improve accuracy at each step as more evidence has been observed. By doing so, we simultaneously address another key issue in online learning (Issue III).\n\nIssue III: Maintaining the purity of a training dataset. The dataset consists of a set of positive examples computed based on the current trajectory, and a set of negative examples mined from outside the current trajectory. In the dataset, we can only guarantee that the positives and the negatives in the first frame are true positives and true negatives respectively. A tracker needs to carefully choose frames from which it can learn to avoid model drifting (i.e., self-paced learning). Most prior approaches do not address this issue since they focus on marginally optimal solutions with which object models are updated, except for the P-N learning in TLD [17] and the self-paced learning for tracking [18]. Since we allow feedback inspection in tracking, we can correct previous errors in the training dataset.\n\nIssue IV: Failure-aware online learning of object models. In online learning, we mostly update model parameters incrementally after inference in a frame. Theoretically speaking, after an initial object model is learned in the first frame, model drifting is inevitable in general setting. Thus, in addition to maintaining the purity of a training dataset, it is also important that we can identify critical moments (caused by different structural and appearance variations) automatically. At those moments, a tracker needs to re-learn both the structure and the parameters of object model using the current whole training dataset. We address this issue by computing uncertainty of an object model in a frame based on its response maps.\n\nIssue V: Computational efficiency by dynamic search strategy. Most tracking-by-detection methods run detection in the whole frame since they usually use relatively simple models such as a single object template. With hierarchical models in tracking and sophisticated online inference and updating strategies, the computational complexity is high. To speed up tracking, we need to utilize a dynamic search strategy. This strategy must take into account the trade-off between generating a conservative proposal state space for efficiency and allowing an exhaustive search for accuracy (e.g., to handle the situation where the object is completely occluded for a while or moves out the camera view and then reappears). We address this issue by adopting a simple search cascade with which we run detection in the whole frame only when local search has failed.\n\nOur TLP method obtains state-of-the-art performance on one popular tracking benchmark [2]. We give a brief overview of our method in the next subsection.\n\n\nMethod Overview\n\nAs illustrated in Fig.2 (a), the TLP method consists of four components. We introduce them briefly as follows.\n\n(1) An AOG quantizing the space of part configurations. Given the bounding box of an object in the first frame, we assume object parts are also of rectangular shapes. We first divide it evenly into a small cell-based grid (e.g., 3 \u00d7 3) and a cell defines the smallest part. We then enumerate all possible parts with different aspect ratios and different sizes which can be placed inside the grid. All the enumerated parts are organized into a hierarchical and compositional AOG. Each part is represented by a terminalnode. Two types of nonterminal nodes as compositional rules: an And-node represents the decomposition of a large part into two smaller ones, and an Or-node represents alternative ways of decompositions through different horizontal or vertical binary splits. We call it the full structure AOG 1 . It is capable of 1. By full structure, it means all the possible compositions on top of the grid with binary composition being used for And-nodes exploring a large number of latent part configurations (see some examples in Fig. 2 (b)), meanwhile it makes the problem of online model learning feasible.\n\n(2) Learning object AOGs. An object AOG is a subgraph learned from the full structure AOG (see Fig. 2 (c) 2 ). Learning an object AOG consists of two steps: (i) The initial object AOG are learned by pruning branches of Or-nodes in the full structure AOG based on discriminative power, following breadth-first search (BFS) order. The discriminative power of a node is measured based on its training error rate. We keep multiple branches for each encountered Or-node to preserve ambiguities, whose training error rates are not bigger than the minimum one by a small positive value. (ii) We retrain the initial object AOG using latent SVM (LSVM) as it was done in learning the DPMs [1]. LSVM utilizes positive re-labeling (i.e., inferring the best configuration for each positive example) and hard negative mining. To further control the model complexity, we prune the initial object AOG through majority voting of latent assignments in positive re-labeling.\n\n(3) A spatial dynamic programming (DP) algorithm for computing all the proposals in a frame with the current object AOG. Thanks to the DAG structure of the object AOG, a DP parsing algorithm is utilized to compute the matching scores and the optimal parse trees of all sliding windows inside the search region in a frame. A parse tree is an instantiation of the object AOG which selects the best child for each encountered Or-node according to matching score. A configuration is obtained by collapsing a parse tree onto the image domain, capturing layout of latent parts of a tracked object in a frame.\n\n(4) A temporal DP algorithm for inferring the most likely trajectory. We maintain a DP table memorizing the candidate object states computed by the spatial DP in the past frames. Then, based on the first-order HMM assumption, a temporal DP algorithm is used to find the optimal solution for the past frames jointly with pair-wise motion constraints (i.e., the Viterbi path [14]). The joint solution can help correct potential tracking errors (i.e., false negatives and false positives collected online) by leveraging more spatial and temporal information. This is similar in spirit to methods of keeping N-best maximal decoder for part models [19] and maintaining diverse M-best solutions in MRF [20].\n\n\nRELATED WORK\n\nIn the literature of object tracking, either single object tracking or multiple-object tracking, there are often two settings.\n\nOffline visual tracking [21], [22], [23], [24]. These methods assume the whole video sequence has been recorded, and consist of two steps. i) It first computes object proposals in all frames using some pre-trained detectors (e.g., the DPMs [1]) and then form \"tracklets\" in consecutive frames. ii) It seeks the optimal object trajectory (or trajectories for multiple objects) by solving an optimization problem (e.g., the K-shortest path or min-cost flow formulation) for the data association. Most work assumed firstorder HMMs in the formulation. Recently, Hong and Han [25] proposed an offline single object tracking method by sampling tree-structured graphical models which exploit the underlying intrinsic structure of input video in an orderless tracking [26]. 2. We note that there are some Or-nodes in the object AOGs which have only one child node since they are subgraphs of the full structure AOG and we keep their original structures.\n\nOnline visual tracking for streaming videos. It starts tracking after the state of an object is specified in certain frame. In the literature, particle filtering [15] has been widely adopted, which approximately represents the posterior probability in a nonparametric form by maintaining a set of particles (i.e., weighted candidates). In practice, particle filtering does not perform well in high-dimensional state spaces. More recently, tracking-bydetection methods [16], [17] have become popular which learn and update object models online and encode the posterior probability using dense sampling through sliding-window based detection onthe-fly. Thus, object tracking is treated as instance-based object detection. To leverage the recent advance in object detection, object tracking research has made progress by incorporating discriminatively trained part-based models [1], [8], [27] (or more generally grammar models [9], [10], [11]). Most popular methods also assume first-order HMMs except for the recently proposed online graph-based tracker [28]. There are four streams in the literature of online visual tracking:\n\ni) Appearance modeling of the whole object, such as incremental learning [29], kernel-based [30], particle filtering [15], sparse coding [31] and 3D-DCT representation [32]; More recently, Convolutional neural networks are utilized in improving tracking performance [5], [6], [33], which are usually pre-trained on some large scale image datasets such as the ImageNet [34] or on video sequences in a benchmark with the testing one excluded. ii) Appearance modeling of objects with parts, such as patchbased [35], coupled 2-layer models [36] and adaptive sparse appearance [37]. The major limitation of appearance modeling of a tracked object is the lack of background models, especially in preventing model drift from distracotrs (e.g., players in sport games). To address this issue, it leads to discriminant tracking; iii) Tracking by discrimination using a single classifier, such as support vector tracking [38], multiple instance learning [39], STRUCK [40], circulant structure-based kernel method [41], and discriminant saliency based tracking [42]; iv) Tracking by part-based discriminative models, such as online extensions of DPMs [43], and structure preserving tracking method [27], [44].\n\nOur method belongs to the fourth stream of online visual tracking. Unlike predefined or fixed part configurations with starmodel structure used in previous work, our method learns both structure and appearance of object AOGs online, which is, to our knowledge, the first method to address the problem of online explicit structure learning in tracking. The advantage of introducing AOG representation are three-fold.\n\ni) More representational power: Unlike TLD [17] and many other methods (e.g., [18]) which model an object as a single template or a mixture of templates and thus do not perform well in tracking objects with large structural and appearance variations, an AOG represents an object in a hierarchical and compositional graph expressing a large number of latent part configurations. ii) More robust tracking and online learning strategies: While the whole object has large variations or might be partially occluded from time to time during tracking, some other parts remain stable and are less likely to be occluded. Some of the parts can be learned to robustly track the object, which can also improve accuracy of appearance adaptation of terminal-nodes. This idea is similar in spirit to finding good features to track objects [45], and we find good part configurations online for both tracking and learning. iii) Fine-grained tracking results: In addition to predicting bounding boxes of a tracked object, outputs of our AOG-Tracker (i.e., the parse trees) have more information which are potentially useful for other modules beyond tracking such as activity or event prediction. Our preliminary work has been published in [46] and the method for constructing full structure AOG was published in [8]. This paper extends them by: (i) adding more experimental results with state-of-the-art performance obtained and full source code released; (ii) elaborating details substantially in deriving the formulation of inference and learning algorithms; and (iii) adding more analyses on different aspects of our method. This paper makes three contributions to the online object tracking problem:\n\ni) It presents a tracking-learning-parsing (TLP) framework which can learn and track objects AOGs. ii) It presents a spatial and a temporal DP algorithms for tracking-by-parsing with AOGs and outputs fine-grained tracking results using parse trees. iii) It outperforms the state-of-the-art tracking methods in a recent public benchmark, TB-100 [2], and obtains comparable performance on a series of VOT benchmarks [4].\n\nPaper Organization. The remainder of this paper is organized as follows. Section 3 presents the formulation of our TLP framework under the Bayesian framework. Section 4 gives the details of spatial-temporal DP algorithm. Section 5 presents the online learning algorithm using the latent SVM method. Section 6 shows the experimental results and analyses. Section 7 concludes this paper and discusses issues and future work.\n\n\nPROBLEM FORMULATION\n\n\nFormulation of Online Object Tracking\n\nIn this section, we first derive a generic formulation from generative perspective in the Bayesian framework, and then derive the discriminative counterpart.\n\n\nTracking with HMM\n\nLet \u039b denote the image lattice on which video frames are defined. Denote a sequence of video frames within time range [1, T ] by,\nI 1:T = {I 1 , \u00b7 \u00b7 \u00b7 , I T }.(1)\nDenote by B t the bounding box of a target object in I t . In online object tracking, B 1 is given and B t 's are inferred by a tracker (t \u2208 [2, T ]). With first-order HMM, we have,\n\nThe prior model:\nB 1 \u223c p(B 1 ) ,(2)\nThe motion model:\nB t |B t\u22121 \u223c p(B t |B t\u22121 ) ,(3)\nThe likelihood:\nI t |B t \u223c p(I t |B t ).(4)\nThen, the prediction model is defined by,\np(B t |I 1:t\u22121 ) = \u2126 B t\u22121 p(B t |B t\u22121 )p(B t\u22121 |I 1:t\u22121 )dB t\u22121 ,(5)\nwhere \u2126 Bt\u22121 is the candidate space of B t\u22121 , and the updating model is defined by,\np(B t |I 1:t ) = p(I t |B t )p(B t |I 1:t\u22121 )/p(I t |I 1:t\u22121 ),(6)\nwhich is a marginal posterior probability. The tracking result, the best bounding box B * t , is computed by,\nB * t = arg max Bt\u2208\u2126 B t p(B t |I 1:t ),(7)\nwhich is usually solved using particle filtering [15] in practice.\n\nTo allow feedback inspection of the history of a trajectory, we seek to maximize a joint posterior probability,\np(B 1:t |I 1:t ) = p(B 1:t\u22121 |I 1:t\u22121 ) p(B t |B t\u22121 )p(I t |B t ) p(I t |I 1:t\u22121 ) = p(B 1 |I 1 ) t i=2 p(B i |B i\u22121 )p(I i |B i ) p(I i |I 1:i\u22121 ) .(8)\nBy taking the logarithm of both sides of Eqn.(8), we have,\nB * 1:t = arg max B1:t log p(B 1:t |I 1:t ) = arg max B1:t {log p(B 1 ) + log p(I 1 |B 1 )+ t i=2 [log p(B i |B i\u22121 ) + log p(I i |B i )]}.(9)\nwhere the image data term p(I 1 ) and t i=2 p(I i |I 1:i\u22121 ) are not included in the maximization as they are treated as constant terms.\n\nSince we have ground-truth for B 1 , p(I 1 |B 1 ) can also be treated as known after the object model is learned based on B 1 . Then, Eqn.(9) can be reproduced as,\nB * 2:t = arg max B2:t log p(B 2:t |I 1:t , B 1 ) (10) = arg max B2:t { t i=2 [log p(B i |B i\u22121 ) + log p(I i |B i )]}.\n\nTracking as Energy Minimization over Trajectories\n\nTo derive the discriminative formulation of Eqn.(10), we show that only the log-likelihood ratio matters in computing log p(I i |B i ) in Eqn.(10) with very mild assumptions. Let \u039b Bi be the image domain occupied by a tracked object, and \u039b Bi the remaining domain (i.e., \u039b Bi \u222a \u039b Bi = \u039b and \u039b Bi \u2229 \u039b Bi = \u2205) in a frame I i . With the independence assumption between I \u039b B i and I \u039b B i given B i , we have,\np(I i |B i ) = p(I \u039b B i , I \u039b B i |B i ) = p(I \u039b B i |B i )p(I \u039b B i |B i ) = p(I \u039b B i |B i )q(I \u039b B i ) = q(I \u039b ) p(I \u039b B i |B i ) q(I \u039b B i ) ,(11)\nwhere q(I \u039b ) is the probability model of background scene and we have q(\nI \u039b B i ) = p(I \u039b B i |B i ) w.r.t.\ncontext-free assumption. So, q(I \u039b ) does not need to be specified explicitly and can be omitted in the maximization. This derivation gives an alternative explanation for discriminant tracking v.s. tracking by generative appearance modeling of an object [47].\n\nBased on Eqn.(10), we define an energy function by,\nE(B 2:t |I 1:t , B 0 ) \u221d \u2212 log p(B 2:t |I 1:t , B 1 ).(12)\nAnd, we do not compute log p(I i |B i ) in the probabilistic way, instead we compute matching score defined by, which we can apply discriminative learning methods. Also, denote the motion cost by,\nScore(I i |B i ) = log p(I \u039b B i |B i ) q(I \u039b B i )(13)= log p(I i |B i ) \u2212 log q(I \u039b ). 1 2 3 (a) (b) (c)Cost(B i |B i\u22121 ) = \u2212 log p(B i |B i\u22121 ).(14)\nWe use a thresholded motion model in experiments: the cost is 0 if the transition is accepted based on the median flow [17] (which is a forward-backward extension of the Lucas-Kanade optimal flow [48]) and +\u221e otherwise. A similar method was experimented in [18]. So, we can re-write Eqn. (10) in the minimization form,\nB * 2:t = arg min B2:t E(B 2:t |I 1:t , B 1 ) (15) = arg min B2:t { t i=2 [Cost(B i |B i\u22121 ) \u2212 Score(I i |B i )]}.\nIn our TLP framework, we compute Score(I i |B i ) in Eqn.( 15) with an object AOG. So, we interpret a sliding window by the optimal parse tree inferred from object AOG. We treat parts as latent variables which are modeled to leverage more information for inferring object bounding box. We note that we do not track parts explicitly in this paper.\n\n\nQuantizing the Space of Part Configurations\n\nIn this section, we first present the construction of a full structure AOG which quantizes the space of part configurations. We then introduce notations in defining an AOG.\n\nPart configurations. For an input bounding box, a part configuration is defined by a partition with different number of parts of different shapes (see Fig. 3 (a)). Two natural questions arise: (i) How many part configurations (i.e., the space) can be defined in a bounding box? (ii) How to organize them into a compact representation? Without posing some structural constraints, it is a combinatorial problem.\n\nWe assume rectangular shapes are used for parts. Then, a configuration can be treated as a tiling of input bounding box using either horizontal or vertical cuts. We utilize binary splitting rule only in decomposition (see Fig. 3 (b) and (c)). With these two constraints, we represent all possible part configurations by a hierarchical and compositional AOG constructed in the following.\n\nGiven a bounding box, we first divide it evenly into a cellbased grid (e.g., 9 \u00d7 10 grid in the right of Fig. 4). Then, in the grid, we define a dictionary of part types and enumerate all instances for all part types. A dictionary of part types. A part type is defined by its width and height. Starting from some minimal size (such as 2 \u00d7 2 cells), we enumerate all possible part types with different aspect ratios and sizes which fit the grid (see A, B, C, D in Fig.4 (a)).\n\nPart instances. An instance of a part type is obtained by placing the part type at a position. Thus, a part instance is defined by a \"sliding window\" in the grid. Fig.4 (b) shows an example of placing part type D (2 \u00d7 5 cells) in a 9 \u00d7 10 grid with 48 instances in total.\n\nTo represent part configurations compactly, we exploit the compositional relationships between enumerated part instances.\n\nThe full structure AOG. For any sub-grid indexed by the left-top position, width and height (e.g., (2, 3, 5, 2) in the rightmiddle of Fig.4 (c)), we can either terminate it directly to the corresponding part instance (Fig.4 (c.1)), or decompose it into two smaller sub-grids using either horizontal or vertical binary splits. Depending on the side length, we may have multiple valid splits along both directions (Fig.4 (c.2)). When splitting either side we allow overlaps between the two sub-grids up to some ratio (Fig.4 (c.3)). Then, we represent the sub-grid as an Or-node, which has a set of child nodes including a terminal-node (i.e. the part instance directly terminated from it), and a number of And-nodes (each of which represents a valid decomposition). This procedure is applied recursively for all child sub-grids. Starting from the whole grid and using BFS order, we construct a full structure AOG, all summarized in Algorithm 1 (see Fig. 5 for an example). Table. 1 lists the number of part configurations for three cases from which we can see that full structure AOGs cover a large number of part configurations using a relatively small set of part instances.\n\n\nOr-node\n\nAnd-node Terminal-node \n-node O \u039b for the grid \u039b, V = {O \u039b }, E = \u2205, BFSqueue= {O \u039b }; while BFSqueue is not empty do Pop a node v from the BFSqueue; if v is an Or-node then i) Add a terminal-node t (i.e. the part instance) V = V \u222a {t}, E = E \u222a {< v, t >};\nii) Create And-nodes A i for all valid cuts;\nE = E \u222a {< v, A i >}; if A i / \u2208 V then V = V \u222a {A i };\nPush A i to the back of BFSqueue; end else if v is an And-node then Create two Or-nodes O i for the two sub-grids;\nE = E \u222a {< v, O i >}; if O i / \u2208 V then V = V \u222a {O i };\nPush O i to the back of BFSqueue; end end end Algorithm 1: Constructing the grid AOG using BFS We denote an AOG by,\nG = (V And , V Or , V T , E, \u0398)(16)\nwhere V And , V Or and V T represent a set of And-nodes, Or-nodes and terminal-nodes respectively, E a set of edges and \u0398 a set of parameters (to be defined in Section 4.1). We have,\n\ni) The object/root Or-node (plotted by green circles), which represents alternative object configurations; ii) A set of And-nodes (solid blue circles), each of which represents the rule of decomposing a complex structure (e.g., a walking person or a running basketball player) into simpler ones;  iii) A set of part Or-nodes, which handle local variations and configurations in a recursive way; iv) A set of terminal-nodes (red rectangles), which link an object and its parts to image data (i.e., grounding symbols) to account for appearance variations and occlusions (e.g., head-shoulder of a walking person before and after opening a sun umbrella).\n\nAn object AOG is a subgraph of a full structure AOG with the same root Or-node. For notational simplicity, we also denote by G an object AOG. So, we will write Score(I i |B i ; G) in Eqn. ( 15) with G added.\n\nA parse tree is an instantiation of an object AOG with the best child node (w.r.t. matching scores) selected for each encountered Or-node. All the terminal-nodes in a parse tree represents a part configuration when collapsed to image domain.\n\nWe note that an object AOG contains multiple parse trees to preserve ambiguities in interpreting a tracked object (see examples in Fig. 2 (c) and Fig. 7).\n\n\nTRACKING-BY-PARSING WITH OBJECT AOGS\n\nIn this section, we present details of inference with object AOGs. We first define scoring functions of nodes in an AOG. Then, we present a spatial DP algorithm for computing Score(I i |B i ; G), and a temporal DP algorithm for inferring the trajectory B * 2:t in Eqn. (15).\n\n\nScoring Functions of Nodes in an AOG\n\nLet F be the feature pyramid computed for either the local ROI or the whole image I t , and \u039b the position space of pyramid F. Let p = (l, x, y) \u2208 \u039b specify a position (x, y) in the l-th level of pyramid F.\n\nGiven an AOG G = (V T , V And , V Or , E, \u0398) (e.g., the left in Fig.6), we define four types of edges, i.e., E = E T \u222a E Def \u222a E Dec \u222a E Switch as shown in Fig.6. We elaborate the definitions of parameters \u0398 = (\u0398 app , \u0398 def , \u0398 bias ): i) Each terminal-node t \u2208 V T has appearance parameters \u03b8 app t \u2282 \u0398 app , which is used to ground a terminal-node to image data. i) The parent And-node A of a part terminal-node with deformation edge has deformation parameters \u03b8 def A \u2282 \u0398 def . They are used for penalizing local displacements when placing a terminal-node around its anchor position. We note that the object template is not allowed to perturb locally in inference, so the parent And-node of the object terminal-node does not have deformation parameters. iii) A child And-node of the root Or-node has a bias term \u0398 bias = {b}. We do not define bias terms for child nodes of other Ornodes.\n\nAppearance Features. We use three types of features: histogram of oriented gradient (HOG) [49], local binary pattern features (LBP) [50], and RGB color histograms (for color videos).\n\nDeformation Features. Denote by \u03b4 = [dx, dy] the displacement of placing a terminal-node around its anchor location. The deformation feature is defined by \u03a6 def (\u03b4) = [dx 2 , dx, dy 2 , dy] as done in DPMs [1].\n\nWe use linear functions to evaluate both appearance scores and deformation scores. The score functions of nodes in an AOG are defined as follows:\n\ni) For a terminal-node t, its score at a position p is computed by,\nScore(t, p|F) =< \u03b8 app t , F(t, p) >(17)\nwhere < \u00b7, \u00b7 > represents inner product and F(t, p) extracts features in feature pyramid. ii) For an Or-node O, its score at position p takes the maximum score over its child nodes,\nScore(O, p|F) = max c\u2208ch(O) Score(c, p|F)(18)\nwhere ch(v) denotes the set of child nodes of a node v. iii) For an And-node A, we have three different functions w.r.t.\n\nthe type of its out-edge (i.e., Terminal-, Deformation-, or Decomposition-edge),\nScore(A, p|F) = (19) \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 Score(t, p|F), e A,t \u2208 E T max \u03b4 [Score(t, p \u2295 \u03b4|F)\u2212 < \u03b8 def A , \u03a6 def (\u03b4) >], e A,t \u2208 E Def c\u2208ch(A) Score(c, p|F), e A,c \u2208 E Dec\nwhere the first case is for sharing score maps between the object terminal-node and its parent And-node since we do not allow local deformation for the whole object, the second case for computing transformed score maps of parent Andnode of a part terminal-node which is allowed to find the best placement through distance transformation [1], \u2295 represents the displacement operator in the position space in \u039b, and the third case for computing the score maps of an And-node which has two child nodes through composition.\n\n\nTracking-by-Parsing\n\nWith scoring functions defined above, we present a spatial DP and a temporal DP algorithms in solving Eqn. (15).\n\nSpatial DP: The DP algorithm (see Algorithm 2) consists of two stages: (i) The bottom-up pass computes score map pyramids  Fig. 6: Illustration of the spatial DP algorithm for parsing with AOGs (e.g., AOG 172 in the left). Right-middle: The input image (ROI in the 173-th frame in the \"Skating1\" sequence) and the inferred object configuration. Right-top: The score map pyramid for root Or-node. Middle: For each node in AOG, we show one level of score map pyramid at which the optimal parse tree is retrieved.\n\n(as illustrated in Fig. 6) for all nodes following the depth-firstsearch (DFS) order of nodes. It computes matching scores of all possible parse trees at all possible positions in feature pyramid.\n\n(ii) In the top-down pass, we first find all candidate positions for the root Or-node O based on its score maps and current threshold \u03c4 G of the object AOG, denoted by\n\u2126 cand = {p; Score(O, p|F) \u2265 \u03c4 G and p \u2208 \u039b}.(20)\nThen, following BFS order of nodes, we retrieve the optimal parse tree at each p \u2208 P: starting from the root Or-node, we select the optimal branch (with the largest score) of each encountered Ornode, keep the two child nodes of each encountered And-node, and retrieve the optimal position of each encountered part terminalnode (by taking arg max for the second case in Eqn. (19)).\n\nAfter spatial parsing, we apply non-maximum suppression (NMS) in computing the optimal parse trees with a predefined intersection-over-union (IoU) overlap threshold, denoted by \u03c4 NMS . We keep top N best parse trees to infer the best B * t together with a temporal DP algorithm, similar to the strategies used in [19], [20].\n\nTemporal DP: Assuming that all the N-best candidates for B 2 , \u00b7 \u00b7 \u00b7 , B t are memoized after running spatial DP algorithm in I 2 to I t , Eqn. (15) corresponds to the classic DP formulation of forward and backward inference for decoding HMMs Input: An image I i , a bounding box B i , and an AOG G Output: Score(I i |B i ; G) in Eqn. (8) and the optimal configuration C * i from the parse tree for the object at frame i. Initialization: Build the depth-first search (DFS) ordering queue (Q DF S ) of all nodes in the AOG;\n\nStep 0: Compute scores for all nodes in Q DF S ; while Q DF S is not empty do Pop a node v from the Q DF S ;\n\nif v is an Or-node then Score(v) = max u\u2208ch(v) Score(u); // ch(v) is the set of child nodes of v else if v is an And-node then\nScore(v) = u\u2208ch(v) LocalMax(Score(u))\nelse if v is a Terminal-node then Compute the filter response map for I N (\u039bv) . // N (\u039b v ) represents the image domain of the LocalMax operation of Terminal-node v. end end Score(I i |B i ; G) = Score(RootOrNode).;\n\nStep 1: Compute C * i using the breadth-first search;\nQ BF S = {RootOrNode}, C * i = (B i ), k = 1; while Q BF S is not empty do Pop a node v from the Q BF S ;\nif v is an Or-node then Push the child node u with maximum score into Q BF S (i.e., Score(u)=Score(v)).\n\n\nelse if v is an And-node then\n\nPush all the child nodes\nv's into Q BF S . else if v is a Terminal-node then Add B (k) i = Deformed(\u039b v ) to C * i = (C * i , B (k)\ni ). Increase k = k + 1. end end Algorithm 2: The spatial DP algorithm for parsing with the AOG, Parse(I i |B i ; G) with \u2212Score(I i |B i ; G) being the singleton \"data\" term and Cost(B i |B i\u22121 ) the pairwise cost term.\n\nLet B i [B i ] be energy of the best object states in the first i frames with the constraint that the i-th one is B i . We have,\nB 1 [B 1 ] = \u2212Score(I 1 |B 1 ; G), B i [B i ] = \u2212Score(I i |B i ; G) + min Bi\u22121 (B i\u22121 [B i\u22121 ] + Cost(B i |B i\u22121 )). (21)\nWhen B 1 is the input bounding box. Then, the temporal DP algorithm consists of two steps: i) The forward step for computing all B i [B i ]'s, and caching the optimal solution for B i\u22121 as a function of B i for later back-tracing starting at i = 2,\nT i [B i ] = arg min Bi\u22121 {B i\u22121 [B i\u22121 ] + Cost(B i |B i\u22121 )}.\nii) The backward step for finding the optimal trajectory (B 1 , B * 2 , \u00b7 \u00b7 \u00b7 , B * t ), where we first take,\nB * t = arg min Bt B t [B t ],(22)\nand then in the order of i = t \u2212 1, \u00b7 \u00b7 \u00b7 , 2 trace back,\nB * i = T i+1 [B * i+1 ].(23)\nIn practice, we often do not need to run temporal DP in the whole time range [1, t], especially for long-term tracking, since the target object might have changed significantly or we might have camera motion, instead we only focus on some short time range, [t \u2212 \u2206t, t] (see settings in experiments).\n\nRemarks: In our TLP method, we apply the spatial and the temporal DP algorithms in a stage-wise manner and without tracking parts explicitly. Thus, we do not introduce loops in inference. If we instead attempt to learn a joint spatial-temporal AOG, it will be a much more difficult problem due to loops in joint spatial-temporal inference, and approximate inference is used.\n\nSearch Strategy: During tracking, at time t, B t is initialized by B t\u22121 , and then a rectangular region of interest (ROI) centered at the center of B t is used to compute feature pyramid and run parsing with AOG. The ROI is first computed as a square area with the side length being s ROI times longer than the maximum of width and height of B t and then is clipped with the image domain. If no candidates are found (i.e., \u2126 cand is empty), we will run the parsing in whole image domain. So, our AOGTracker is capable of re-detecting a tracked object. If there are still no candidates (e.g., the target object was completely occluded or went out of camera view), the tracking result of this frame is set to be invalid and we do not need to run the temporal DP.\n\n\nThe Trackability of an Object AOG\n\nTo detect critical moments online, we need to measure the quality of an object AOG, G at time t. We compute its trackability based on the score maps in which the optimal parse tree is placed. For each node v in the parse tree, we have its position in score map pyramid (i.e., the level of pyramid and the location in that level), (l v , x v , y v ). We define the trackability of node v by,\nTrackability(v|I t , G) = S(l v , x v , y v ) \u2212 \u00b5 S(24)\nwhere S(l v , x v , y v ) is the score of node v, \u00b5 S the mean score computed from the whole score map. Intuitively, we expect the score map of a discriminative node v has peak and steep landscape, as investigated in [51]. The trackabilities of part nodes are used to infer partial occlusion and local structure variations, and trackability of the inferred parse tree indicate the \"goodness\" of current object AOG. We note that we treat trackability and intrackability (i.e., the inverse of th trackability) exchangeably. More sophisticated definitions of intrackability in tracking are referred to [52]. We model trackability by a Gaussian model whose mean and standard derivation are computed incrementally in [2, t]. At time t, a tracked object is said to be \"intrackable\" if its trackability is less than mean trackability (t) \u2212 3 \u00b7 std trackability (t). We note that the tracking result could be still valid even if it is \"intrackable\" (e.g., in the first few frames in which the target object is occluded partially, especially by similar distractors).\n\n\nONLINE LEARNING OF OBJECT AOGS\n\nIn this section, we present online learning of object AOGs, which consists of three components: (i) Maintaining a training dataset based on tracking results; (ii) Estimating parameters of a given object AOG; and (iii) Learning structure of the object AOG by pruning full structure AOG, which requires (ii) in the process.\n\n\nMaintaining the Training Dataset Online\n\nDenote by D t = D + t \u222a D \u2212 t the training dataset at time t, consisting of D + t , a positive dataset, and D \u2212 t , a negative dataset. In the first frame, we have D + 1 = {(I 1 , B 1 )} and let B 1 = (x 1 , y 1 , w 1 , h 1 ). We augment it with eight locally shifted positives, i.e., {I 1 , B 1,i ; i = 1, \u00b7 \u00b7 \u00b7 , 8} where x 1,i \u2208 {x 1 \u00b1 d} and y 1,i \u2208 {y \u00b1 d} with width and height not changed. d is set to the cell size in computing HOG features. The initial D \u2212 1 uses the whole remaining image I \u039b B 1 for mining hard negatives in training.\n\nAt time t, if B t is valid according to tracking-by-parsing, we have D + t = D + t\u22121 \u222a {(I t , B t )}, and add to D \u2212 t all other candidates in \u2126 cand (Eqn. 20) which are not suppressed by B t according to NMS (i.e., hard negatives). Otherwise, we have D t = D t\u22121 .\n\n\nEstimating Parameters of a Given Object AOG\n\nWe use latent SVM method (LSVM) [1]. Based on the scoring functions defined in Section 4.1, we can re-write the scoring function of applying a given object AOG, G on a training example (denoted by I B for simplicity),\nScore(I B ; G) = max pt\u2208\u2126 G < \u0398, \u03a6(F, pt) >(25)\nwhere pt represents a parse tree, \u2126 G the space of parse trees, \u0398 the concatenated vector of all parameters, \u03a6(F, pg) the concatenated vector of appearance and deformation features in feature pyramid where C is the trade-off parameter in learning. Eqn.( 26) is a semiconvexity function of the parameters \u0398 due to the empirical loss term on positives. In optimization, we utilize an iterative procedure in a \"coordinate descent\" way. We first convert the object function to a convex function by assigning latent values for all positives using the spatial DP algorithm. Then, we estimate parameters. While we can use stochastic gradient descent as done in DPMs [1], we adopt LBFGS method in practice 3 [53] since it is more robust and efficient with parallel implementation as investigated in [9], [54]. The detection threshold, \u03c4 G is estimated as the minimum score of positives.\n\n\nLearning Object AOGs\n\nWith the training dataset D t and the full structure AOG constructed based on B 1 , an object AOG is learned in three steps:\n\ni) Evaluating the figure of merits of nodes in the full structure AOG. We first train the root classifier (i.e., object appearance parameters and bias term) by linear SVM using D + t and datamining hard negatives in D \u2212 t . Then, the appearance parameters for each part terminal-node t is initialized by cropping out the 3. We reimplemented the matlab code available at http://www.cs.ubc.ca/\u223cschmidtm/Software/minConf.html in c++. corresponding portion in the object template 4 . Following DFS order, we evaluate the figure of merit of each node in the full structure AOG by its training error rate. The error rate is calculated on D t where the score of a node is computed w.r.t. scoring functions defined in Section 4.1. The smaller the error rate is, the more discriminative a node is. ii) Retrieving an initial object AOG and re-estimating parameters. We retrieve the most discriminative subgraph in the full structure AOG as initial object AOG. Following BFS order, we start from the root Or-node, select for each encountered Or-node the best child node (with the smallest training error rate among all children) and the child nodes whose training error rates are not bigger than that of the best child by some predefined small positive value (i.e., preserving ambiguities), keep the two child nodes for each encountered And-node, and stop at each encountered terminal-node. We show two examples in the left of Fig. 7. We train the parameters of initial object AOG using LSVM [1] with two rounds of positive re-labeling and hard negative mining respectively.\n\niii) Controlling model complexity. To do that, a refined object OPE SRE Spatially Scaled Spatially Shifted TRE Fig. 8: Illustration of the three types of evaluation methods in TB-100/50/CVPR2013. In one-pass evaluation (OPE), a tracker is initialized in the first frame and let it track the target until the end of the sequence. In temporal robustness evaluation (TRE), a tracker starts at different starting frames initialized with the corresponding ground-truth bounding boxes and then tracks the object until the end. 20 starting frames (including the first frame) are used in TB-100. In spatial robustness evaluation (SRE), a tracker runs multiple times with spatially scaled (4 types) and shifted (8 types of perturbation) initializations in the first frame.\n\nAOG for tracking is obtained by further selecting the most discriminative part configuration(s) in the initial object AOG learned in the step ii). The selection process is based on latent assignment in relabeling positives in LSVM training. A part configuration in the initial object AOG is pruned if it relabeled less than 10% positives (see the right of Fig. 7). We further train the refined object AOG with one round latent positive re-labeling and hard negative mining. By reducing model complexity, we can speed up the tracking-by-parsing procedure.\n\nVerification of a refined object AOG. We run parsing with a refined object AOG in the first frame. The refined object AOG is accepted if the score of the optimal parse tree is greater than the threshold estimated in training and the IoU overlap between the predicted bounding box and the input bounding box is greater than or equals the IoU NMS threshold, \u03c4 NMS in detection.\n\nIdentifying critical moments in tracking. A critical moment means a tracker has become \"uncertain\" and at the same time accumulated \"enough\" new samples, which is triggered in tracking when two conditions were satisfied. The first is that the number of frames in which a tracked object is \"intrackable\" was larger than some value, N Intrackable . The second is that the number of new valid tracking results are greater than some value, N NewSample . Both are accumulated from the last time an object AOG was re-learned.\n\nThe spatial resolution of placing parts. In learning object AOGs, we first place parts at the same spatial resolution as the object. If the learned object AOG was not accepted in verification, we then place parts at twice the spatial resolution w.r.t. the object and re-learn the object AOG. In our experiments, the two specifications handled all testing sequences successfully.\n\nOverall flow of online learning. In the first frame or when a critical moment is identified in tracking, we learn both structure and parameters of an object AOG, otherwise we update parameters only if the tracking result is valid in a frame based on tracking-byparsing.\n\n\nEXPERIMENTS\n\nIn this section, we present comparison results on the TB-50/100/CVPR2013 benchmarks [2], [3] and the VOT benchmarks [4]. We also analyze different aspects of our method. The source  [57] CSK [58] CT [59] H CXT [60] B DFT [61] FOT [62] FRAG [63] IVT [29] KMS [30] L1APG [64] LOT [65] LSHT [66] H LSK [67] LSS [68] MIL [39] H MTT [69] OAB [70] H ORIA [71] H PCOM [72] SCM [73] SMS [74] SBT [75] H STRUCK [40] H TLD [17] B VR [76] VTD [77] VTS [78] AOG HOG [+Color] code 5 is released with this paper for reproducing all results. We denote the proposed method by AOG in tables and plots.\n\nParameter Setting. We use the same parameters for all experiments since we emphasize online learning in this paper. In learning object AOGs, the side length of the grid used for constructing the full structure AOG is either 3 or 4 depending the slide length of input bounding box (to reduce the time complexity of online learning). The number of intervals in computing feature pyramid is set to 6 with cell size being 4. The factor s in computing search ROI is set to s ROI = 3. The NMS IoU threshold is set to \u03c4 NMS = 0.7. The number of top parse trees kept after spatial DP parsing is set N Best = 10. The time range in temporal DP algorithm is set to \u2206t = 5. In identifying critical moments, we set N Intrackable = 5 and N NewSample = 10. The LSVM trade-off parameter in Eqn. (26) is set to C = 0.001. When re-learning structure and parameters, we could use all the frames with valid tracking results. To reduce the time complexity, the number of frames used in relearning is at most 100 in our experiments. At time t, we first take the first 10 frames with valid tracking results in [1, t] with the underlying intuition that they have high probabilities of being tracked correctly (note that we alway use the first frame since the ground-truth bounding box is given), and then take the remaining frames in reversed time order.\n\nSpeed. In our current c++ implementation, we adopt FFT in computing score pyramids as done in [54] which also utilizes multi-threads with OpenMP. We also provide a distributed version based on MPI 6 in evaluation. The FPS is about 2 to 3. We are experimenting GPU implementations to speed up our TLP.\n\n\nResults on TB-50/100/CVPR2013\n\nThe TB-100 benchmark has 100 target objects (58, 897 frames in total) with 29 publicly available trackers evaluated. It is extended from a previous benchmark with 51 target objects released at 5 3: Performance gain (in %) of our AOGTracker in term of success rate and precision rate in the benchmark [2]. Success plots of TB-100/50/CVPR2013 are shown in Fig. 9. The success plots of the 11 subsets in TB-50 are shown in Fig. 10. Precision plots are provided in the supplementary material due to space limit here.  Fig. 9: Performance comparison in TB-100 (1st row), TB-50 (2nd row) and TB-CVPR2013 (3rd row) in term of success plots of OPE (1st column), SRE (2nd column) and TRE (3rd colum). For clarity, only top 10 trackers are shown in color curves and listed in the legend. Two deep learning based trackers, CNT [5] and SO-DLT [6], are evaluated in TB-CVPR2013 using OPE (with their performance plots manually added in the left-bottom figure). We note that the plots are reproduced with the raw results provided at http://cvlab.hanyang.ac.kr/tracker benchmark/. (Best viewed in color and with magnification) CVPR2013 (denoted by TB-CVPR2013). Further, since some target objects are similar or less challenging, a subset of 50 difficult and representative ones (denoted by TB-50) is selected for an indepth analysis. Two types of performance metric are used, the precision plot (i.e., the percentage of frames in which estimated locations are within a given threshold distance of ground-truth positions) and the success plot (i.e., based on IoU overlap scores which are commonly used in object detection benchmarks, e.g., PASCAL VOC [79]). The higher a success rate or a precision rate is, the better a tracker is. Usually, success plots are preferred to rank trackers [2], [4] (thus we focus on success plots in comparison). Three types of evaluation methods are used as illustrated in Fig.8.\n\nTo account for different factors of a test sequence affecting     [2]. In TB-CVPR2013, two recent trackers trained by deep convolutional network (CNT [5], SO-DLT [6]) were evaluated using OPE.\n\nWe summarize the performance gain of our AOGTracker in    Table.3. Our AOGTracker obtains significant improvement (more than 12%) in the 10 subsets in TB-50. Our AOGTracker handles out-of-view situations much better than other trackers since it is capable of re-detecting target objects in the whole image, and it performs very well in the scale variation subset (see examples in the second and fourth rows in Fig. 11) since it searches over feature pyramid explicitly (with the expense of more computation). Our AOGTracker obtains the least improvement in the lowresolution subset since it uses HOG features and the discrepancy between HOG cell-based coordinate and pixel-based one can cause some loss in overlap measurement, especially in the low resolution subset. We will add automatic selection of feature types (e.g., HOG v.s. pixel-based features such as intensity and gradient) according to the resolution, as well as other factors in future work. Fig.9 shows success plots of OPE, SRE and TRE in TB-100/50/CVPR2013. Our AOGTracker consistently outperforms all other trackers. We note that for OPE in TB-CVPR2013, although the improvement of our AOGTracker over the SO-DLT [6] is not very big, the SO-DLT utilized two deep convolutional networks with different model update strategies in tracking, both of which are pretrained on the ImageNet [34]. Fig. 11 shows some qualita-tive results.\n\n\nAnalyses of AOG models and the TLP Algorithm\n\nTo show advantage of the proposed method, we compare performance of four different variants of our AOGTracker.\n\nAOG-s vs AOG-st. As stated above, in our AOGTracker, we use a very simple setting for temporal DP which takes into account \u2206t = 5 frames, [t\u22125, t] in our experiments. We denote it by AOGst in comparison. We show the advantage of this simple spatial and temporal DP algorithms by comparing it with the baseline tracker, denoted by AOG-s, which uses spatial DP only. Everything else in online inference and learning is the same as AOG-st.\n\nSingle object template vs AOG. We test our tracker without learning object part configurations, that is to learn and update object templates only in tracking. Similarly, we also have two settings, with temporal DP (denoted by ObjectOnly-st) and without temporal DP (denoted by ObjectOnly-s). Fig. 12 shows performance comparison of the four variants. The full method (AOG-st) obtains the best performance consistently and the two trackers with AOGs significantly outperforms the other two variants. For the two trackers with object templates Fig. 13: Performance comparison in VOT2013. Left: Ranking plot for the baseline experiment. The smaller the rank number is, the better a tracker is w.r.t. accuracy and/or robust (i.e., the right-top region indicates better performance) Right: Accuracy-Robustness plot. The larger the rate is, the better a tracker is. only, the one without temporal DP (ObjectOnly-s) slightly outperform the one with temporal DP (ObjectOnly-st), which shows that we might need strong enough object models in integrating spatial and temporal information for better performance.\n\n\nResults on VOT\n\nIn VOT, the evaluation focuses on short-term tracking (i.e., a tracker is not expected to perform re-detection after losing a target object), so the evaluation toolkit will re-initialize a tracker after it loses the target (w.r.t. the condition the overlap between the predicted bounding box and the ground-truth one drops to zero) with the number of failures counted. In VOT protocol, a tracker is tested on each sequence multiple times. The performance is measured in terms of accuracy and robustness. Accuracy is computed as the average of per-frame accuracies which themselves are computed by taking the average over the repetitions. Robustness is computed as the average number of failure times over repetitions.\n\nWe integrate our AOGTracker in the latest VOT toolkit 7 to run experiments with the baseline protocol and to generate plots 8 . 7. Available at https://github.com/votchallenge/vot-toolkit, version 3.2 8. The plots for VOT2013 and 2014 might be different compared to those in the original VOT reports [80], [81] due to the new version of vot-toolkit. The VOT2013 dataset [80] has 16 sequences which was selected from a large pool such that various visual phenomena like occlusion and illumination changes, were still represented well within the selection. 7 sequences are also used in TB-100. There are 27 trackers evaluated. The readers are referred to the VOT technical report [80] for details. Fig.13 shows the ranking plot and AR plot in VOT2013. Our AOGTracker obtains the best accuracy while its robustness is slightly worse than three other trackers (i.e., PLT [80], LGT [82] and LGTpp [83], and PLT was the winner in VOT2013 challenge). Our AOGTracker obtains the best overall rank.\n\nThe VOT2014 dataset [81] has 25 sequences extended from VOT2013. The annotation is based on rotated bounding box instead of up-right rectangle. There are 33 trackers evaluated. Details on the trackers are referred to [81]. Fig.14 shows the ranking plot and AR plot. Our AOGTracker is comparable to other trackers. One main limitation of AOGTracker is that it does not handle rotated bounding boxes well.\n\nThe VOT2015 dataset [84] consists of 60 short sequences (with rotated bounding box annotations) and VOT-TIR2015 comprises 20 sequences (with bounding box annotations). There are 62 and 28 trackers evaluated in VOT2015 and VOT-TIR2015 respectively. Our AOGTracker obtains 51% and 65% (tied for third place) in accuracy in VOT2015 and VOT-TIR2015 respectively. The details are referred to the reports [84] due to space limit here.\n\n\nDISCUSSION AND FUTURE WORK\n\nWe have presented a tracking, learning and parsing (TLP) framework and derived a spatial dynamic programming (DP) and a temporal DP algorithm for online object tracking with AOGs. We also have presented a method of online learning object AOGs including its structure and parameters. In experiments, we test our method on two main public benchmark datasets and experimental results show better or comparable performance.\n\nIn our on-going work, we are studying more flexible computing schemes in tracking with AOGs. The compositional property embedded in an AOG naturally leads to different bottom-up/topdown computing schemes such as the three computing processes studied by Wu and Zhu [85]. We can track an object by matching the object template directly (i.e. \u03b1-process), or computing some discriminative parts first and then combine them into object (\u03b2process), or doing both (\u03b1 + \u03b2-process, as done in this paper). In tracking, as time evolves, the object AOG might grow through online learning, especially for objects with large variations in longterm tracking. Thus, faster inference is entailed for the sake of real time applications. We are trying to learn near optimal decision policies for tracking using the framework proposed by Wu and Zhu [86].\n\nIn our future work, we will extend the TLP framework by incorporating generic category-level AOGs [8] to scale up the TLP framework. The generic AOGs are pre-trained offline (e.g., using the PASCAL VOC [79] or the imagenet [34]), and will help the online learning of specific AOGs for a target object (e.g., help to maintain the purity of the positive and negative datasets collected online). The generic AOGs will also be updated online together with the specific AOGs. By integrating generic and specific AOGs, we aim at the life-long learning of objects in videos without annotations. Furthermore, we are also interested in integrating scene grammar [87] and event grammar [88] to leverage more top-down information.\n\n\nFig. 2: Overview of our AOGTracker. (a) Illustration of the tracking, learning and parsing (TLP) framework. It consists of four components. (b) Examples of capturing structural and appearance variations of a tracked object by a series of object configurations inferred on-the-fly over key frames #1, #173, #282, etc. (c) Illustration of an object AOG, a parse tree and an object configuration in frame #282. A parse tree is an instantiation of an AOG. A configuration is a layout of latent parts represented by terminal-nodes in a parse tree. An object AOG preserves ambiguities by capturing multiple parse trees.\n\nFig. 3 :\n3We assume parts are of rectangular shapes. (a) shows a configuration with 3 parts. Two different, yet equivalent, decomposition rules in representing a configuration are shown in (b) for decomposition with branching factor equal to the number of parts (i.e., a flat structure), and in (c) for a hierarchical decomposition with branching factor being set to 2 at all levels.\n\nFig. 4 :\n4Illustration of (a) the dictionary of part types, and (b) part instances generated by placing a part type in a grid. Given part instances, (c) shows how a sub-grid is decomposed in different ways. We allow overlap between child nodes (see (c.3)).\n\nFig. 5 :\n5Illustration of full structure And-Or Graph (AOG) representing the space of part configurations. It is of directed acyclic graph (DAG) structure. For clarity, we show a toy example constructed for a 3 \u00d7 3 grid. The AOG can generate all possible part configurations (the number is often huge for typical grid sizes, seeTable.1), while allowing efficient exploration with a DP algorithm due to the DAG structure. See text for details. (Best viewed in color and with magnification) Input: Image grid \u039b with W \u00d7 H cells; Minimal size of a part type (w 0 , h 0 ); Maximal overlap ratio r between two sub-grids. Output: The And-Or graph G =< V, E > (seeFig.5) Initialization: Create an Or\n\nF, 1 \u2212\n1w.r.t. parse tree pt, and the bias term. The objective function in estimating parameters is defined by the l 2 -regularized empirical hinge loss function, Score(I B ; G)) I B \u2208D \u2212 t max(0, 1 + Score(I B ; G))] (26)\n\nFig. 7 :\n7Illustration of learning an object AOG in the first frame (top) and re-learning an object AOG in the 281-th frame when a critical moment has triggered. It consists of two steps: (a) learning initial object AOG by pruning branches of Or-nodes in full structure AOG, and (b) learning refined object AOG by pruning part configurations w.r.t. majority voting in positive relabeling in LSVM.\n\nFig. 10 :\n10Performance comparison in the 11 subsets (with different attributes and different number of sequences as shown by the titles in the sub-figures) of TB-50 based on the success plots of OPE. performance, the testing sequences are further categorized w.r.t. 11 attributes for more ind-depth comparisons: (1) Illumination Variation (IV, 38/22/21 sequences in TB-100/50/CVPR2013), (2) Scale Variation (SV, 64/38/28 sequences), (3) Occlusion (OCC, 49/29/29 sequences), (4) Deformation (DEF, 44/23/19 sequences), (5) Motion Blur (MB, 29/19/12 sequences), (6) Fast Motion (FM, 39/25/17 sequences), (7) In-Plane Rotation (IPR, 51/29/31 sequences), (8) Out-of-Plane Rotation (OPR, 63/32/39 sequences), (9) Out-of-View (OV, 14/11/6 sequences), (10) Background Clut-ters (BC, 31/20/21 sequences), and (11) Low Resolution (LR, 9/8/4 sequences).More details on the attributes and their distributions in the benchmark are referred to[2],[3].\n\nFig. 11 :\n11Qualitative results. For clarity, we show tracking results (bounding boxes) in 6 randomly sampled frames for the top 10 trackers according to their OPE performance in TB-100. (Best viewed in color and with magnification.)\n\nFig. 12 :\n12Performance comparison of the four variants of our AOGTracker in TB-100/50/CVPR2013 in term of the success plots of OPE (1st column), SRE (2nd column) and TRE (3rd colum).\n\nFig. 14 :\n14Performance comparison in VOT2014.\n\nTABLE 1 :\n1The number of part configurations generated from our AOG without considering overlapped compositions.\n\nTABLE 2 :\n2Tracking algorithms evaluated in the TB-100 bench-\nmark (reproduced from [2]). \n\n\n\n\n. Available at https://github.com/tfwu/RGM-AOGTracker 6. https://www.mpich.org/Metric \nSuccess Rate / Precision Rate \nEvaluation \nOPE \nSRE \nTRE \nSubset \n100 \n50 \nCVPR2013 \n100 \n50 \nCVPR2013 \n100 \n50 \nCVPR2013 \n\nAOG Gain \n13.93 / 18.06 \n16.84 / 22.23 \n2.74 / 19.37 \n11.47 / 16.79 \n12.52 / 17.82 \n11.89 / 17.55 \n9.25 / 11.06 \n11.37 / 14.61 \n11.59 / 14.38 \nRunner-up \nSTRUCK [40] \nSO-DLT [6] / STRUCK [40] \nSTRUCK [40] \n\nSubsets in TB-50 \nDEF(23) \nFM(25) \nMB(19) \nIPR(29) \nBC(20) \nOPR(32) \nOCC(29) \nIV(22) \nLR(8) \nSV(38) \nOV(11) \n\nAOG Gain (success rate) \n15.89 \n15.56 \n17.29 \n12.29 \n17.81 \n14.04 \n14.7 \n15.73 \n6.65 \n18.38 \n15.99 \nRunner-up \nSTRUCK [40] \nTLD [17] \nSCM [73] \nMIL [39] \n\n\n\nTABLE\n\n\n\nSuccess plots of OPE \u2212 OV(Out\u2212of\u2212View) (11) AOG [50.69] MIL [34.70] VTD [34.15] TLD [34.13] STRUCK [34.03] CT [33.63] VTS [32.64] TM [31.72] LSS [31.44] SCM [31.15] Success plots of OPE \u2212 SV(ScaleVariation) (38) AOG [56.33] SCM [37.95] STRUCK [36.24] TLD [34.34] ASLA [34.31] CXT [30.83] VTD [29.99] LSK [29.98] VTS [28.86] OAB[28.71]    AOG [55.87] \nSCM [38.06] \nTLD [37.05] \nSTRUCK [36.29] \nCSK [36.00] \nVTD [35.51] \nVTS [35.37] \nASLA [34.84] \nL1APG [31.48] \nDFT [31.29] \n\n0 \n0.2 \n0.4 \n0.6 \n0.8 \n1 \n0 \n\n10 \n\n20 \n\n30 \n\n40 \n\n50 \n\n60 \n\n70 \n\n80 \n\nthresholds \n\nSuccess plots of OPE \u2212 DEF(Deformation) (23) \n\nAOG [48.21] \nSTRUCK [32.32] \nSCM [31.78] \nDFT [29.52] \nVTD [29.27] \nASLA [28.96] \nLSHT [28.72] \nCPF [28.05] \nVTS [27.44] \nKMS [27.28] \n\n0 \n0.2 \n0.4 \n0.6 \n0.8 \n1 \n0 \n\n10 \n\n20 \n\n30 \n\n40 \n\n50 \n\n60 \n\n70 \n\n80 \n\n90 \n\nthresholds \n\nSuccess plots of OPE \u2212 FM(FastMotion) (25) \n\nAOG [56.50] \nSTRUCK [40.94] \nTLD [38.20] \nCXT [37.22] \nOAB [31.30] \nKMS [30.26] \nTM [29.83] \nLOT [29.81] \nPD [28.49] \nFRAG [28.44] \n\n0 \n0.2 \n0.4 \n0.6 \n0.8 \n1 \n0 \n\n10 \n\n20 \n\n30 \n\n40 \n\n50 \n\n60 \n\n70 \n\n80 \n\n90 \n\nthresholds \n\nSuccess plots of OPE \u2212 MB(MotionBlur) (19) \n\nAOG [58.26] \nTLD [40.97] \nSTRUCK [40.65] \nCXT [38.35] \nOAB [32.72] \nTM [32.38] \nPD [30.92] \nCSK [29.96] \nKMS [29.18] \nL1APG [28.95] \n\n0 \n0.2 \n0.4 \n0.6 \n0.8 \n1 \n0 \n\n10 \n\n20 \n\n30 \n\n40 \n\n50 \n\n60 \n\n70 \n\n80 \n\nthresholds \n\nSuccess plots of OPE \u2212 IPR(In\u2212PlaneRotation) (29) \n\nAOG [51.71] \nTLD [39.42] \nSTRUCK [38.13] \nCXT [37.32] \nVTD [33.75] \nSCM [33.74] \nVTS [31.95] \nL1APG [31.81] \nASLA [31.46] \nLSK [31.19] \n\n0 \n0.2 \n0.4 \n0.6 \n0.8 \n1 \n0 \n\n10 \n\n20 \n\n30 \n\n40 \n\n50 \n\n60 \n\n70 \n\n80 \n\n90 \n\nthresholds \n\nSuccess plots of OPE \u2212 OPR(Out\u2212of\u2212PlaneRotatio) (32) \n\nAOG [50.37] \nSCM [36.33] \nVTD [35.47] \nSTRUCK [33.53] \nVTS [33.01] \nASLA [32.85] \nTLD [32.18] \nLSK [30.39] \nLOT [30.30] \nLSHT [30.14] \n\n0 \n0.2 \n0.4 \n0.6 \n0.8 \n1 \n0 \n\n10 \n\n20 \n\n30 \n\n40 \n\n50 \n\n60 \n\n70 \n\n80 \n\n90 \n\nthresholds \n\nSuccess plots of OPE \u2212 OCC(Occlusion) (29) \n\nAOG [51.84] \nSCM [37.14] \nVTD [35.60] \nVTS [33.90] \nASLA [33.88] \nSTRUCK [33.65] \nLSK [31.52] \nLOT [31.27] \nL1APG [30.76] \nOAB [29.06] \n\n0 \n0.2 \n0.4 \n0.6 \n0.8 \n1 \n0 \n\n10 \n\n20 \n\n30 \n\n40 \n\n50 \n\n60 \n\n70 \n\n80 \n\n90 \n\nthresholds \n\nSuccess plots of OPE \u2212 IV(IlluminationVariation) (22) \n\nAOG [57.05] \nSCM [41.32] \nASLA [38.33] \nVTD [35.01] \nVTS [33.53] \nSTRUCK [33.01] \nLSHT [32.85] \nTLD [32.61] \nDFT [32.04] \nCSK [31.66] \n\n0 \n0.2 \n0.4 \n0.6 \n0.8 \n1 \n0 \n\n10 \n\n20 \n\n30 \n\n40 \n\n50 \n\n60 \n\n70 \n\n80 \n\n90 \n\nthresholds \n\nSuccess plots of OPE \u2212 LR(LowResolution) (8) \n\nAOG [51.53] \nSCM [44.88] \nASLA [43.70] \nTLD [33.45] \nSTRUCK [31.88] \nLSK [31.87] \nCXT [31.78] \nL1APG [30.81] \nIVT [29.95] \nLSS [29.43] \n\n0 \n0.2 \n0.4 \n0.6 \n0.8 \n1 \n0 \n\n10 \n\n20 \n\n30 \n\n40 \n\n50 \n\n60 \n\n70 \n\n80 \n\n90 \n\nthresholds \n\n0 \n0.2 \n0.4 \n0.6 \n0.8 \n1 \n0 \n\n10 \n\n20 \n\n30 \n\n40 \n\n50 \n\n60 \n\n70 \n\n80 \n\n90 \n\nthresholds \n\n\n\nTable . 2\n.lists the 29 evaluated tracking algorithms which are categorized based on representation and search scheme. See more details about categorizing these trackers in\n\n\nAOG\u2212st [60.10] AOG\u2212s [56.22] ObjectOnly\u2212s [38.68] ObjectOnly\u2212st [36.50] AOG\u2212st [62.94] AOG\u2212s [56.36] ObjectOnly\u2212s [38.06] ObjectOnly\u2212st [36.16] AOG\u2212st [56.19] AOG\u2212s [51.18] ObjectOnly\u2212s [34.64] ObjectOnly\u2212st [34.33] AOG\u2212st [62.96] AOG\u2212s [58.40] ObjectOnly\u2212st [40.93] ObjectOnly\u2212s [40.56]0 \n0.2 \n0.4 \n0.6 \n0.8 \n1 \n0 \n\n10 \n\n20 \n\n30 \n\n40 \n\n50 \n\n60 \n\n70 \n\n80 \n\n90 \n\nthresholds \n\nSRE\u2212100(sequence average) \n\nAOG\u2212st [55.11] \nAOG\u2212s [51.47] \nObjectOnly\u2212s [35.24] \nObjectOnly\u2212st [34.76] \n\n0 \n0.2 \n0.4 \n0.6 \n0.8 \n1 \n0 \n\n20 \n\n40 \n\n60 \n\n80 \n\n100 \n\nthresholds \n\nTRE\u2212100(sequence average) \n\nAOG\u2212st [61.02] \nAOG\u2212s [57.97] \nObjectOnly\u2212s [39.89] \nObjectOnly\u2212st [39.49] \n\n0 \n0.2 \n0.4 \n0.6 \n0.8 \n1 \n0 \n\n10 \n\n20 \n\n30 \n\n40 \n\n50 \n\n60 \n\n70 \n\n80 \n\n90 \n\nthresholds \n\nOPE\u221250(sequence average) \n\nAOG\u2212st [55.24] \nAOG\u2212s [50.50] \nObjectOnly\u2212s [33.87] \nObjectOnly\u2212st [31.19] \n\n0 \n0.2 \n0.4 \n0.6 \n0.8 \n1 \n0 \n\n10 \n\n20 \n\n30 \n\n40 \n\n50 \n\n60 \n\n70 \n\n80 \n\n90 \n\nthresholds \n\nSRE\u221250(sequence average) \n\nAOG\u2212st [49.85] \nAOG\u2212s [45.88] \nObjectOnly\u2212s [31.00] \nObjectOnly\u2212st [30.17] \n\n0 \n0.2 \n0.4 \n0.6 \n0.8 \n1 \n0 \n\n10 \n\n20 \n\n30 \n\n40 \n\n50 \n\n60 \n\n70 \n\n80 \n\n90 \n\nthresholds \n\nTRE\u221250(sequence average) \n\nAOG\u2212st [56.19] \nAOG\u2212s [52.15] \nObjectOnly\u2212s [34.58] \nObjectOnly\u2212st [34.45] \n\n0 \n0.2 \n0.4 \n0.6 \n0.8 \n1 \n0 \n\n20 \n\n40 \n\n60 \n\n80 \n\n100 \n\nthresholds \n\nOPE\u2212CVPR2013(51, sequence average) \n\n0 \n0.2 \n0.4 \n0.6 \n0.8 \n1 \n0 \n\n10 \n\n20 \n\n30 \n\n40 \n\n50 \n\n60 \n\n70 \n\n80 \n\n90 \n\nthresholds \n\nSRE\u2212CVPR2013(51, sequence average) \n\n0 \n0.2 \n0.4 \n0.6 \n0.8 \n1 \n0 \n\n20 \n\n40 \n\n60 \n\n80 \n\n100 \n\nthresholds \n\nTRE\u2212CVPR2013(51, sequence average) \n\n\n. We also tried to train the linear SVM classifiers for all the terminal-nodes individually using cropped examples, which increases the runtime, but does not improve the tracking performance in experiments. So, we use the simplified method above.\nACKNOWLEDGMENTSThis work is supported by the DARPA SIMPLEX Award N66001-15-C-4035, the ONR MURI grant N00014-16-1-2007, and NSF IIS-1423305. We thank Steven Holtzen for proofreading this paper. We also gratefully acknowledge the support of NVIDIA Corporation with the donation of one GPU.\nObject detection with discriminatively trained part-based models. P Felzenszwalb, R Girshick, D Mcallester, D Ramanan, PAMI. 329P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ramanan, \"Ob- ject detection with discriminatively trained part-based models,\" PAMI, vol. 32, no. 9, pp. 1627-1645, 2010.\n\nObject tracking benchmark. Y Wu, J Lim, M.-H Yang, PAMI. 379Y. Wu, J. Lim, and M.-H. Yang, \"Object tracking benchmark,\" PAMI, vol. 37, no. 9, pp. 1834-1848, 2015.\n\nOnline object tracking: A benchmark. CVPR. --, \"Online object tracking: A benchmark,\" in CVPR, 2013.\n\nA novel performance evaluation methodology for single-target trackers. M Kristan, J Matas, A Leonardis, T Vojir, R P Pflugfelder, G Fern\u00e1ndez, G Nebehay, F Porikli, L Cehovin, abs/1503.01313CoRR. M. Kristan, J. Matas, A. Leonardis, T. Vojir, R. P. Pflugfelder, G. Fern\u00e1ndez, G. Nebehay, F. Porikli, and L. Cehovin, \"A novel performance evaluation methodology for single-target trackers,\" CoRR, vol. abs/1503.01313, 2015. [Online]. Available: http://arxiv.org/abs/ 1503.01313\n\nRobust visual tracking via convolutional networks. K Zhang, Q Liu, Y Wu, M.-H Yang, arXiv:1501.04505v2arXiv preprintK. Zhang, Q. Liu, Y. Wu, and M.-H. Yang, \"Robust visual tracking via convolutional networks,\" arXiv preprint arXiv:1501.04505v2, 2015.\n\nTransferring rich feature hierarchies for robust visual tracking. N Wang, S Li, A Gupta, D.-Y Yeung, arXiv:1501.04587v2arXiv preprintN. Wang, S. Li, A. Gupta, and D.-Y. Yeung, \"Transferring rich feature hi- erarchies for robust visual tracking,\" arXiv preprint arXiv:1501.04587v2, 2015.\n\nThe Origin of Concepts. S Carey, Oxford University PressS. Carey, The Origin of Concepts. Oxford University Press, 2011.\n\nDiscriminatively trained and-or tree models for object detection. X Song, T Wu, Y Jia, S.-C Zhu, CVPR. X. Song, T. Wu, Y. Jia, and S.-C. Zhu, \"Discriminatively trained and-or tree models for object detection,\" in CVPR, 2013.\n\nObject detection with grammar models. R Girshick, P Felzenszwalb, D Mcallester, NIPS. R. Girshick, P. Felzenszwalb, and D. McAllester, \"Object detection with grammar models,\" in NIPS, 2011.\n\nObject detection grammars. P Felzenszwalb, D Mcallester, TR-2010-02Computer Science. Tech. Rep.P. Felzenszwalb and D. McAllester, \"Object detection grammars,\" Uni- versity of Chicago, Computer Science TR-2010-02, Tech. Rep., 2010.\n\nA stochastic grammar of images. S C Zhu, D Mumford, Foundations and Trends in Computer Graphics and Vision. 24S. C. Zhu and D. Mumford, \"A stochastic grammar of images,\" Foun- dations and Trends in Computer Graphics and Vision, vol. 2, no. 4, pp. 259-362, 2006.\n\nPOP: patchwork of parts models for object recognition. Y Amit, A Trouv\u00e9, IJCV. 752Y. Amit and A. Trouv\u00e9, \"POP: patchwork of parts models for object recognition,\" IJCV, vol. 75, no. 2, pp. 267-282, 2007. [Online].\n\n. 10.1007/s11263-006-0033-9Available: http://dx.doi.org/10.1007/s11263-006-0033-9\n\nObject tracking: A survey. A Yilmaz, O Javed, M Shah, ACM Comput. Surv. 384A. Yilmaz, O. Javed, and M. Shah, \"Object tracking: A survey,\" ACM Comput. Surv., vol. 38, no. 4, 2006.\n\nReadings in speech recognition. L R Rabiner, Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition. A. Waibel and K.-F. LeeSan Francisco, CA, USAMorgan Kaufmann Publishers IncL. R. Rabiner, \"Readings in speech recognition,\" A. Waibel and K.-F. Lee, Eds. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc., 1990, ch. A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition, pp. 267-296. [Online]. Available: http://dl.acm.org/citation.cfm?id=108235.108253\n\nCondensation -conditional density propagation for visual tracking. M Isard, A Blake, IJCV. 291M. Isard and A. Blake, \"Condensation -conditional density propagation for visual tracking,\" IJCV, vol. 29, no. 1, pp. 5-28, 1998.\n\nPeople-tracking-by-detection and people-detection-by-tracking. M Andriluka, S Roth, B Schiele, CVPR. M. Andriluka, S. Roth, and B. Schiele, \"People-tracking-by-detection and people-detection-by-tracking,\" in CVPR, 2008.\n\nTracking-learning-detection. Z Kalal, K Mikolajczyk, J Matas, PAMI. 347Z. Kalal, K. Mikolajczyk, and J. Matas, \"Tracking-learning-detection,\" PAMI, vol. 34, no. 7, pp. 1409-1422, 2012.\n\nSelf-paced learning for long-term tracking. J S S Iii, D Ramanan, CVPR. J. S. S. III and D. Ramanan, \"Self-paced learning for long-term tracking,\" in CVPR, 2013.\n\nN-best maximal decoder for part models. D Park, D Ramanan, ICCV. D. Park and D. Ramanan, \"N-best maximal decoder for part models,\" in ICCV, 2011.\n\nDiverse m-best solutions in markov random fields. D Batra, P Yadollahpour, A Guzm\u00e1n-Rivera, G Shakhnarovich, ECCV. D. Batra, P. Yadollahpour, A. Guzm\u00e1n-Rivera, and G. Shakhnarovich, \"Diverse m-best solutions in markov random fields,\" in ECCV, 2012.\n\nGlobal data association for multi-object tracking using network flows. L Zhang, Y Li, R Nevatia, CVPR. L. Zhang, Y. Li, and R. Nevatia, \"Global data association for multi-object tracking using network flows,\" in CVPR, 2008.\n\nGlobally-optimal greedy algorithms for tracking a variable number of objects. H Pirsiavash, D Ramanan, C C Fowlkes, CVPR. H. Pirsiavash, D. Ramanan, and C. C. Fowlkes, \"Globally-optimal greedy algorithms for tracking a variable number of objects,\" in CVPR, 2011.\n\nMultiple object tracking using k-shortest paths optimization. J Berclaz, F Fleuret, E T\u00fcretken, P Fua, PAMI. 339J. Berclaz, F. Fleuret, E. T\u00fcretken, and P. Fua, \"Multiple object tracking using k-shortest paths optimization,\" PAMI, vol. 33, no. 9, pp. 1806- 1819, 2011.\n\nAn efficient implementation of a scaling minimum-cost flow algorithm. A V Goldberg, J. Algorithms. 221A. V. Goldberg, \"An efficient implementation of a scaling minimum-cost flow algorithm,\" J. Algorithms, vol. 22, no. 1, pp. 1-29, 1997.\n\nVisual tracking by sampling tree-structured graphical models. S Hong, B Han, ECCV. S. Hong and B. Han, \"Visual tracking by sampling tree-structured graphical models,\" in ECCV, 2014.\n\nOrderless tracking through modelaveraged posterior estimation. S Hong, S Kwak, B Han, ICCV. S. Hong, S. Kwak, and B. Han, \"Orderless tracking through model- averaged posterior estimation,\" in ICCV, 2013.\n\nPart-based visual tracking with online latent structural learning. R Yao, Q Shi, C Shen, Y Zhang, A Van Den, Hengel, CVPR. R. Yao, Q. Shi, C. Shen, Y. Zhang, and A. van den Hengel, \"Part-based visual tracking with online latent structural learning,\" in CVPR, 2013.\n\nOnline graph-based tracking. H Nam, S Hong, B Han, ECCV. H. Nam, S. Hong, and B. Han, \"Online graph-based tracking,\" in ECCV, 2014.\n\nIncremental learning for robust visual tracking. D A Ross, J Lim, R.-S Lin, M.-H Yang, IJCV. 771-3D. A. Ross, J. Lim, R.-S. Lin, and M.-H. Yang, \"Incremental learning for robust visual tracking,\" IJCV, vol. 77, no. 1-3, pp. 125-141, 2008.\n\nKernel-based object tracking. D Comaniciu, V Ramesh, P Meer, PAMI. 255D. Comaniciu, V. Ramesh, and P. Meer, \"Kernel-based object tracking,\" PAMI, vol. 25, no. 5, pp. 564-575, 2003.\n\nRobust visual tracking and vehicle classification via sparse representation. X Mei, H Ling, PAMI. 3311X. Mei and H. Ling, \"Robust visual tracking and vehicle classification via sparse representation,\" PAMI, vol. 33, no. 11, pp. 2259-2272, 2011.\n\nIncremental learning of 3d-dct compact representations for robust visual tracking. X Li, A R Dick, C Shen, A Van Den Hengel, H Wang, PAMI. 354X. Li, A. R. Dick, C. Shen, A. van den Hengel, and H. Wang, \"Incremen- tal learning of 3d-dct compact representations for robust visual tracking,\" PAMI, vol. 35, no. 4, pp. 863-881, 2013.\n\nLearning multi-domain convolutional neural networks for visual tracking. H Nam, B Han, CVPR. H. Nam and B. Han, \"Learning multi-domain convolutional neural networks for visual tracking,\" in CVPR, 2016.\n\nImageNet: A Large-Scale Hierarchical Image Database. J Deng, W Dong, R Socher, L.-J Li, K Li, L Fei-Fei, CVPR. J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, \"ImageNet: A Large-Scale Hierarchical Image Database,\" in CVPR, 2009.\n\nHighly nonrigid object tracking via patch-based dynamic appearance modeling. J Kwon, K M Lee, PAMI. 3510J. Kwon and K. M. Lee, \"Highly nonrigid object tracking via patch-based dynamic appearance modeling,\" PAMI, vol. 35, no. 10, pp. 2427-2441, 2013.\n\nRobust visual tracking using an adaptive coupled-layer visual model. L Cehovin, M Kristan, A Leonardis, PAMI. 354L. Cehovin, M. Kristan, and A. Leonardis, \"Robust visual tracking using an adaptive coupled-layer visual model,\" PAMI, vol. 35, no. 4, pp. 941- 953, 2013.\n\nVisual tracking via adaptive structural local sparse appearance model. X Jia, H Lu, M.-H Yang, CVPR. X. Jia, H. Lu, and M.-H. Yang, \"Visual tracking via adaptive structural local sparse appearance model,\" in CVPR, 2012.\n\nSupport vector tracking. S Avidan, PAMI. 268S. Avidan, \"Support vector tracking,\" PAMI, vol. 26, no. 8, pp. 1064- 1072, 2004.\n\nRobust object tracking with online multiple instance learning. B Babenko, M.-H Yang, S Belongie, PAMI. 338B. Babenko, M.-H. Yang, and S. Belongie, \"Robust object tracking with online multiple instance learning,\" PAMI, vol. 33, no. 8, pp. 1619-1632, 2011.\n\nStruck: Structured output tracking with kernels. S Hare, A Saffari, P H S Torr, ICCV. S. Hare, A. Saffari, and P. H. S. Torr, \"Struck: Structured output tracking with kernels,\" in ICCV, 2011.\n\nExploiting the circulant structure of tracking-by-detection with kernels. J Henriques, R Caseiro, P Martins, J Batista, ECCV. J. Henriques, R. Caseiro, P. Martins, and J. Batista, \"Exploiting the circulant structure of tracking-by-detection with kernels,\" in ECCV, 2012.\n\nBiologically inspired object tracking using center-surround saliency mechanisms. V Mahadevan, N Vasconcelos, PAMI. 353V. Mahadevan and N. Vasconcelos, \"Biologically inspired object tracking using center-surround saliency mechanisms,\" PAMI, vol. 35, no. 3, pp. 541-554, 2013.\n\nPart-based visual tracking with online latent structural learning. R Yao, Q Shi, C Shen, Y Zhang, A Van Den, Hengel, CVPR. R. Yao, Q. Shi, C. Shen, Y. Zhang, and A. van den Hengel, \"Part-based visual tracking with online latent structural learning,\" in CVPR, 2013.\n\nStructure preserving object tracking. L Zhang, L Van Der Maaten, CVPR. L. Zhang and L. van der Maaten, \"Structure preserving object tracking,\" in CVPR, 2013.\n\nGood feature to track. J Shi, C Tomasi, CVPR. J. Shi and C. Tomasi, \"Good feature to track,\" in CVPR, 1994.\n\nOnline object tracking, learning and parsing with and-or graphs. Y Lu, T Wu, S.-C Zhu, CVPR. Y. Lu, T. Wu, and S.-C. Zhu, \"Online object tracking, learning and parsing with and-or graphs,\" in CVPR, 2014.\n\nA survey of appearance models in visual object tracking. X Li, W Hu, C Shen, Z Zhang, A R Dick, A Van Den, Hengel, abs/1303.4803CoRR. X. Li, W. Hu, C. Shen, Z. Zhang, A. R. Dick, and A. van den Hengel, \"A survey of appearance models in visual object tracking,\" CoRR, vol. abs/1303.4803, 2013. [Online]. Available: http://arxiv.org/abs/1303.4803\n\nLucas-kanade 20 years on: A unifying framework. S Baker, I Matthews, IJCV. 563S. Baker and I. Matthews, \"Lucas-kanade 20 years on: A unifying framework,\" IJCV, vol. 56, no. 3, pp. 221-255, 2004.\n\nHistograms of oriented gradients for human detection. N Dalal, B Triggs, CVPR. N. Dalal and B. Triggs, \"Histograms of oriented gradients for human detection,\" in CVPR, 2005.\n\nPerformance evaluation of texture measures with classification based on kullback discrimination of distributions. T Ojala, M Pietikainen, D Harwood, ICPR. T. Ojala, M. Pietikainen, and D. Harwood, \"Performance evaluation of texture measures with classification based on kullback discrimination of distributions,\" in ICPR, 1994.\n\nHighly nonrigid object tracking via patch-based dynamic appearance modeling. J Kwon, K M Lee, TPAMI. 3510J. Kwon and K. M. Lee, \"Highly nonrigid object tracking via patch-based dynamic appearance modeling,\" TPAMI, vol. 35, no. 10, pp. 2427-2441, 2013.\n\nIntrackability: Characterizing video statistics and pursuing video representations. H Gong, S C Zhu, IJCV. 973H. Gong and S. C. Zhu, \"Intrackability: Characterizing video statistics and pursuing video representations,\" IJCV, vol. 97, no. 3, pp. 255-275, 2012.\n\nA limited memory algorithm for bound constrained optimization. R H Byrd, P Lu, J Nocedal, C Zhu, SIAM J. Sci. Comput. 165R. H. Byrd, P. Lu, J. Nocedal, and C. Zhu, \"A limited memory algorithm for bound constrained optimization,\" SIAM J. Sci. Comput., vol. 16, no. 5, pp. 1190-1208, 1995.\n\nExact acceleration of linear object detectors. C Dubout, F Fleuret, ECCV. C. Dubout and F. Fleuret, \"Exact acceleration of linear object detectors,\" in ECCV, 2012.\n\nVisual tracking via adaptive structural local sparse appearance model. X Jia, H Lu, M.-H Yang, CVPR. X. Jia, H. Lu, and M.-H. Yang, \"Visual tracking via adaptive structural local sparse appearance model,\" in CVPR, 2012.\n\nBeyond semi-supervised tracking: Tracking should be as simple as detection, but not simpler than recognition. S Stalder, H Grabner, L Van Gool, ICCV Workshop. S. Stalder, H. Grabner, and L. van Gool, \"Beyond semi-supervised tracking: Tracking should be as simple as detection, but not simpler than recognition,\" in ICCV Workshop, 2009.\n\nColor-based probabilistic tracking. P P\u00e9rez, C Hue, J Vermaak, M Gangnet, ECCV. P. P\u00e9rez, C. Hue, J. Vermaak, and M. Gangnet, \"Color-based probabilistic tracking,\" in ECCV, 2002.\n\nExploiting the circulant structure of tracking-by-detection with kernels. J F Henriques, R Caseiro, P Martins, J Batista, ECCV. J. F. Henriques, R. Caseiro, P. Martins, and J. Batista, \"Exploiting the circulant structure of tracking-by-detection with kernels,\" in ECCV, 2012.\n\nFast compressive tracking. K Zhang, L Zhang, M Yang, PAMI. 3610K. Zhang, L. Zhang, and M. Yang, \"Fast compressive tracking,\" PAMI, vol. 36, no. 10, pp. 2002-2015, 2014.\n\nContext tracker: Exploring supporters and distracters in unconstrained environments. T B Dinh, N Vo, G G Medioni, CVPR. T. B. Dinh, N. Vo, and G. G. Medioni, \"Context tracker: Exploring supporters and distracters in unconstrained environments,\" in CVPR, 2011.\n\nDistribution fields for tracking. L Sevilla-Lara, E Learned-Miller, CVPR. L. Sevilla-Lara and E. Learned-Miller, \"Distribution fields for tracking,\" in CVPR, 2012.\n\nRobustifying the flock of trackers. T Vojir, J Matas, Computer Vision Winter Workshop. T. Vojir and J. Matas, \"Robustifying the flock of trackers,\" in Computer Vision Winter Workshop, 2011.\n\nRobust fragments-based tracking using the integral histogram. A Adam, E Rivlin, I Shimshoni, CVPR. A. Adam, E. Rivlin, and I. Shimshoni, \"Robust fragments-based tracking using the integral histogram,\" in CVPR, 2006.\n\nReal time robust L1 tracker using accelerated proximal gradient approach. C Bao, Y Wu, H Ling, H Ji, CVPR. C. Bao, Y. Wu, H. Ling, and H. Ji, \"Real time robust L1 tracker using accelerated proximal gradient approach,\" in CVPR, 2012.\n\nLocally orderless tracking. S Oron, A Bar-Hillel, D Levi, S Avidan, CVPR. S. Oron, A. Bar-Hillel, D. Levi, and S. Avidan, \"Locally orderless tracking,\" in CVPR, 2012.\n\nVisual tracking via locality sensitive histograms. S He, Q Yang, R W Lau, J Wang, M.-H Yang, CVPR. S. He, Q. Yang, R. W. Lau, J. Wang, and M.-H. Yang, \"Visual tracking via locality sensitive histograms,\" in CVPR, 2013.\n\nRobust tracking using local sparse appearance model and k-selection. B Liu, J Huang, L Yang, C A Kulikowski, CVPR. B. Liu, J. Huang, L. Yang, and C. A. Kulikowski, \"Robust tracking using local sparse appearance model and k-selection,\" in CVPR, 2011.\n\nLeast soft-thresold squares tracking. D Wang, H Lu, M.-H Yang, CVPR. D. Wang, H. Lu, and M.-H. Yang, \"Least soft-thresold squares tracking,\" in CVPR, 2013.\n\nRobust visual tracking via multi-task sparse learning. T Zhang, B Ghanem, S Liu, N Ahuja, CVPR. T.Zhang, B. Ghanem, S. Liu, and N. Ahuja, \"Robust visual tracking via multi-task sparse learning,\" in CVPR, 2012.\n\nReal-time tracking via on-line boosting. H Grabner, M Grabner, H Bischof, BMVC. H. Grabner, M. Grabner, and H. Bischof, \"Real-time tracking via on-line boosting,\" in BMVC, 2006.\n\nOnline robust image alignment via iterative convex optimization. Y Wu, B Shen, H Ling, CVPR. Y. Wu, B. Shen, and H. Ling, \"Online robust image alignment via iterative convex optimization,\" in CVPR, 2012.\n\nVisual tracking via probability continuous outlier model. D Wang, H Lu, CVPR. D. Wang and H. Lu, \"Visual tracking via probability continuous outlier model,\" in CVPR, 2014.\n\nRobust object tracking via sparsitybased collaborative model. W Zhong, H Lu, M Yang, CVPR. W. Zhong, H. Lu, and M. Yang, \"Robust object tracking via sparsity- based collaborative model,\" in CVPR, 2012.\n\nMean-shift blob tracking through scale space. R T Collins, CVPR. R. T. Collins, \"Mean-shift blob tracking through scale space,\" in CVPR, 2003.\n\nSemi-supervised on-line boosting for robust tracking. H Grabner, C Leistner, H Bischof, ECCV. H. Grabner, C. Leistner, and H. Bischof, \"Semi-supervised on-line boosting for robust tracking,\" in ECCV, 2008.\n\nOnline selection of discriminative tracking features. R T Collins, Y Liu, M Leordeanu, PAMI. 2710R. T. Collins, Y. Liu, and M. Leordeanu, \"Online selection of discrimi- native tracking features,\" PAMI, vol. 27, no. 10, pp. 1631-1643, 2005.\n\nVisual tracking decomposition. J Kwon, K M Lee, CVPR. J. Kwon and K. M. Lee, \"Visual tracking decomposition,\" in CVPR, 2010.\n\nTracking by sampling trackers. ICCV. --, \"Tracking by sampling trackers,\" in ICCV, 2011.\n\nThe PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results. M Everingham, L Van Gool, C Williams, J Winn, A Zisserman, M. Everingham, L. Van Gool, C. Williams, J. Winn, and A. Zisserman, \"The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Re- sults.\"\n\nThe visual object tracking vot2013 challenge results. M Kristan, M. Kristan and et al, \"The visual object tracking vot2013 challenge results,\" 2013. [Online]. Available: http://www.votchallenge.net/vot2013/ program.html\n\nThe visual object tracking vot2014 challenge results. --, \"The visual object tracking vot2014 challenge results,\" 2014. [Online]. Available: http://www.votchallenge.net/vot2014/program.html\n\nRobust visual tracking using an adaptive coupled-layer visual model. L Cehovin, M Kristan, A Leonardis, PAMI. 354L. Cehovin, M. Kristan, and A. Leonardis, \"Robust visual tracking using an adaptive coupled-layer visual model,\" PAMI, vol. 35, no. 4, pp. 941- 953, 2013.\n\nAn enhanced adaptive coupledlayer lgtracker++. J Xiao, R Stolkin, A Leonardis, Vis. Obj. Track. Challenge VOT2013. conjunction with ICCV2013J. Xiao, R. Stolkin, and A. Leonardis, \"An enhanced adaptive coupled- layer lgtracker++,\" in Vis. Obj. Track. Challenge VOT2013, In conjunc- tion with ICCV2013, 2013.\n\nThe visual object tracking vot2015 and tir2015 challenge results. M Kristan, M. Kristan and et al, \"The visual object tracking vot2015 and tir2015 challenge results,\" 2015. [Online]. Available: http: //www.votchallenge.net/vot2015/program.html\n\nA numerical study of the bottom-up and top-down inference processes in and-or graphs. T Wu, S C Zhu, IJCV. 932T. Wu and S. C. Zhu, \"A numerical study of the bottom-up and top-down inference processes in and-or graphs,\" IJCV, vol. 93, no. 2, pp. 226-252, 2011.\n\nLearning near-optimal cost-sensitive decision policy for object detection. T Wu, S Zhu, TPAMI. 375T. Wu and S. Zhu, \"Learning near-optimal cost-sensitive decision policy for object detection,\" TPAMI, vol. 37, no. 5, pp. 1013-1027, 2015.\n\nImage parsing with stochastic scene grammar. Y Zhao, S C Zhu, NIPS. Y. Zhao and S. C. Zhu, \"Image parsing with stochastic scene grammar,\" in NIPS, 2011.\n\nLearning and parsing video events with goal and intent prediction. M Pei, Z Si, B Z Yao, S Zhu, CVIU. 11710M. Pei, Z. Si, B. Z. Yao, and S. Zhu, \"Learning and parsing video events with goal and intent prediction,\" CVIU, vol. 117, no. 10, pp. 1369-1383, 2013.\n\nHe is currently research assistant professor in the center for vision, cognition, learning and autonomy (VCLA) at UCLA. His research interests include: (i) Statistical learning of large scale hierarchical and compositional models (e.g., And-Or graphs) from images and videos. (ii) Statistical inference by learning nearoptimal cost-sensitive decision policies. (iii) Statistical theory of performance guaranteed learn. Tianfu Wu received Ph.D. degree in Statistics from University of California, Los Angeles (UCLAing algorithm and inference procedureTianfu Wu received Ph.D. degree in Statis- tics from University of California, Los Angeles (UCLA) in 2011. He is currently research assis- tant professor in the center for vision, cognition, learning and autonomy (VCLA) at UCLA. His research interests include: (i) Statistical learn- ing of large scale hierarchical and compositional models (e.g., And-Or graphs) from images and videos. (ii) Statistical inference by learning near- optimal cost-sensitive decision policies. (iii) Sta- tistical theory of performance guaranteed learn- ing algorithm and inference procedure.\n", "annotations": {"author": "[{\"end\":77,\"start\":67},{\"end\":86,\"start\":78},{\"end\":101,\"start\":87}]", "publisher": null, "author_last_name": "[{\"end\":76,\"start\":74},{\"end\":85,\"start\":83},{\"end\":100,\"start\":97}]", "author_first_name": "[{\"end\":73,\"start\":67},{\"end\":82,\"start\":78},{\"end\":96,\"start\":87}]", "author_affiliation": null, "title": "[{\"end\":64,\"start\":1},{\"end\":165,\"start\":102}]", "venue": null, "abstract": "[{\"end\":1823,\"start\":267}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b6\"},\"end\":1973,\"start\":1970},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3526,\"start\":3523},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3861,\"start\":3858},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3866,\"start\":3863},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3871,\"start\":3868},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3877,\"start\":3873},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3883,\"start\":3879},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3911,\"start\":3907},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4022,\"start\":4019},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4795,\"start\":4792},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4801,\"start\":4797},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5589,\"start\":5586},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5595,\"start\":5591},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5601,\"start\":5597},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5958,\"start\":5954},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6023,\"start\":6019},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6029,\"start\":6025},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6035,\"start\":6031},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6122,\"start\":6119},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7272,\"start\":7268},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7318,\"start\":7314},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9107,\"start\":9104},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11101,\"start\":11098},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12357,\"start\":12353},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12627,\"start\":12623},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":12680,\"start\":12676},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":12854,\"start\":12850},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12860,\"start\":12856},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":12866,\"start\":12862},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12872,\"start\":12868},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":13069,\"start\":13066},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":13401,\"start\":13397},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":13590,\"start\":13586},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":13593,\"start\":13592},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":13939,\"start\":13935},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":14245,\"start\":14241},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":14251,\"start\":14247},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":14651,\"start\":14648},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":14656,\"start\":14653},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":14662,\"start\":14658},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":14700,\"start\":14697},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":14706,\"start\":14702},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":14712,\"start\":14708},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":14829,\"start\":14825},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":14977,\"start\":14973},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":14996,\"start\":14992},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":15021,\"start\":15017},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":15041,\"start\":15037},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":15072,\"start\":15068},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":15169,\"start\":15166},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":15174,\"start\":15171},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":15180,\"start\":15176},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":15272,\"start\":15268},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":15411,\"start\":15407},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":15440,\"start\":15436},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":15476,\"start\":15472},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":15815,\"start\":15811},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":15848,\"start\":15844},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":15861,\"start\":15857},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":15907,\"start\":15903},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":15954,\"start\":15950},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":16044,\"start\":16040},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":16091,\"start\":16087},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":16097,\"start\":16093},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":16564,\"start\":16560},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":16599,\"start\":16595},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":17345,\"start\":17341},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":17742,\"start\":17738},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":17814,\"start\":17811},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":18551,\"start\":18548},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":18621,\"start\":18618},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":20238,\"start\":20234},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":22122,\"start\":22118},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":22708,\"start\":22704},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":22785,\"start\":22781},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":22846,\"start\":22842},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":22877,\"start\":22873},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":28154,\"start\":28149},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":28881,\"start\":28877},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":30118,\"start\":30114},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":30160,\"start\":30156},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":30417,\"start\":30414},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":31608,\"start\":31605},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":31921,\"start\":31917},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":33229,\"start\":33225},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":33550,\"start\":33546},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":33556,\"start\":33552},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":33707,\"start\":33703},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":33897,\"start\":33894},{\"end\":36108,\"start\":36102},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":38169,\"start\":38165},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":38551,\"start\":38547},{\"end\":38666,\"start\":38660},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":40301,\"start\":40298},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":41194,\"start\":41191},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":41236,\"start\":41232},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":41326,\"start\":41323},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":41332,\"start\":41328},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":43045,\"start\":43042},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":46097,\"start\":46094},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":46102,\"start\":46099},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":46129,\"start\":46126},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":46196,\"start\":46192},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":46205,\"start\":46201},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":46213,\"start\":46209},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":46224,\"start\":46220},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":46235,\"start\":46231},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":46244,\"start\":46240},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":46254,\"start\":46250},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":46263,\"start\":46259},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":46272,\"start\":46268},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":46283,\"start\":46279},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":46292,\"start\":46288},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":46302,\"start\":46298},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":46313,\"start\":46309},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":46322,\"start\":46318},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":46331,\"start\":46327},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":46342,\"start\":46338},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":46351,\"start\":46347},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":46363,\"start\":46359},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":46375,\"start\":46371},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":46384,\"start\":46380},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":46393,\"start\":46389},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":46402,\"start\":46398},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":46416,\"start\":46412},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":46427,\"start\":46423},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":46437,\"start\":46433},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":46446,\"start\":46442},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":46455,\"start\":46451},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":47379,\"start\":47375},{\"end\":47689,\"start\":47683},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":48026,\"start\":48022},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":48456,\"start\":48455},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":48565,\"start\":48562},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":49081,\"start\":49078},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":49096,\"start\":49093},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":49902,\"start\":49898},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":50037,\"start\":50034},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":50042,\"start\":50039},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":50229,\"start\":50226},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":50313,\"start\":50310},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":50325,\"start\":50322},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":51538,\"start\":51535},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":51709,\"start\":51705},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":54318,\"start\":54317},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":54493,\"start\":54489},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":54499,\"start\":54495},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":54563,\"start\":54559},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":54871,\"start\":54867},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":55060,\"start\":55056},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":55070,\"start\":55066},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":55085,\"start\":55081},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":55204,\"start\":55200},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":55401,\"start\":55397},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":55609,\"start\":55605},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":55988,\"start\":55984},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":56733,\"start\":56729},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":57299,\"start\":57295},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":57403,\"start\":57400},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":57508,\"start\":57504},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":57529,\"start\":57525},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":57959,\"start\":57955},{\"attributes\":{\"ref_id\":\"b88\"},\"end\":57982,\"start\":57978},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":61531,\"start\":61528},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":61535,\"start\":61532}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":58637,\"start\":58022},{\"attributes\":{\"id\":\"fig_1\"},\"end\":59022,\"start\":58638},{\"attributes\":{\"id\":\"fig_2\"},\"end\":59280,\"start\":59023},{\"attributes\":{\"id\":\"fig_3\"},\"end\":59974,\"start\":59281},{\"attributes\":{\"id\":\"fig_4\"},\"end\":60198,\"start\":59975},{\"attributes\":{\"id\":\"fig_5\"},\"end\":60596,\"start\":60199},{\"attributes\":{\"id\":\"fig_7\"},\"end\":61536,\"start\":60597},{\"attributes\":{\"id\":\"fig_8\"},\"end\":61771,\"start\":61537},{\"attributes\":{\"id\":\"fig_9\"},\"end\":61956,\"start\":61772},{\"attributes\":{\"id\":\"fig_10\"},\"end\":62004,\"start\":61957},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":62118,\"start\":62005},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":62212,\"start\":62119},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":62898,\"start\":62213},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":62906,\"start\":62899},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":65732,\"start\":62907},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":65906,\"start\":65733},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":67476,\"start\":65907}]", "paragraph": "[{\"end\":2832,\"start\":1866},{\"end\":3161,\"start\":2834},{\"end\":3353,\"start\":3163},{\"end\":5296,\"start\":3355},{\"end\":5414,\"start\":5306},{\"end\":6606,\"start\":5416},{\"end\":7423,\"start\":6608},{\"end\":8159,\"start\":7425},{\"end\":9016,\"start\":8161},{\"end\":9171,\"start\":9018},{\"end\":9301,\"start\":9191},{\"end\":10417,\"start\":9303},{\"end\":11374,\"start\":10419},{\"end\":11978,\"start\":11376},{\"end\":12681,\"start\":11980},{\"end\":12824,\"start\":12698},{\"end\":13771,\"start\":12826},{\"end\":14898,\"start\":13773},{\"end\":16098,\"start\":14900},{\"end\":16515,\"start\":16100},{\"end\":18202,\"start\":16517},{\"end\":18622,\"start\":18204},{\"end\":19046,\"start\":18624},{\"end\":19267,\"start\":19110},{\"end\":19418,\"start\":19289},{\"end\":19633,\"start\":19452},{\"end\":19651,\"start\":19635},{\"end\":19688,\"start\":19671},{\"end\":19737,\"start\":19722},{\"end\":19807,\"start\":19766},{\"end\":19963,\"start\":19879},{\"end\":20140,\"start\":20031},{\"end\":20251,\"start\":20185},{\"end\":20364,\"start\":20253},{\"end\":20577,\"start\":20519},{\"end\":20857,\"start\":20721},{\"end\":21022,\"start\":20859},{\"end\":21601,\"start\":21195},{\"end\":21827,\"start\":21754},{\"end\":22123,\"start\":21864},{\"end\":22176,\"start\":22125},{\"end\":22432,\"start\":22236},{\"end\":22903,\"start\":22585},{\"end\":23365,\"start\":23019},{\"end\":23585,\"start\":23413},{\"end\":23996,\"start\":23587},{\"end\":24384,\"start\":23998},{\"end\":24860,\"start\":24386},{\"end\":25133,\"start\":24862},{\"end\":25256,\"start\":25135},{\"end\":26432,\"start\":25258},{\"end\":26467,\"start\":26444},{\"end\":26745,\"start\":26701},{\"end\":26916,\"start\":26802},{\"end\":27088,\"start\":26973},{\"end\":27307,\"start\":27125},{\"end\":27959,\"start\":27309},{\"end\":28168,\"start\":27961},{\"end\":28411,\"start\":28170},{\"end\":28567,\"start\":28413},{\"end\":28882,\"start\":28608},{\"end\":29129,\"start\":28923},{\"end\":30022,\"start\":29131},{\"end\":30206,\"start\":30024},{\"end\":30418,\"start\":30208},{\"end\":30565,\"start\":30420},{\"end\":30634,\"start\":30567},{\"end\":30857,\"start\":30676},{\"end\":31024,\"start\":30904},{\"end\":31106,\"start\":31026},{\"end\":31786,\"start\":31268},{\"end\":31922,\"start\":31810},{\"end\":32434,\"start\":31924},{\"end\":32632,\"start\":32436},{\"end\":32801,\"start\":32634},{\"end\":33231,\"start\":32851},{\"end\":33557,\"start\":33233},{\"end\":34081,\"start\":33559},{\"end\":34191,\"start\":34083},{\"end\":34319,\"start\":34193},{\"end\":34574,\"start\":34358},{\"end\":34629,\"start\":34576},{\"end\":34839,\"start\":34736},{\"end\":34897,\"start\":34873},{\"end\":35225,\"start\":35005},{\"end\":35355,\"start\":35227},{\"end\":35727,\"start\":35479},{\"end\":35901,\"start\":35792},{\"end\":35994,\"start\":35937},{\"end\":36324,\"start\":36025},{\"end\":36700,\"start\":36326},{\"end\":37463,\"start\":36702},{\"end\":37891,\"start\":37501},{\"end\":39005,\"start\":37948},{\"end\":39361,\"start\":39040},{\"end\":39950,\"start\":39405},{\"end\":40218,\"start\":39952},{\"end\":40483,\"start\":40266},{\"end\":41410,\"start\":40532},{\"end\":41559,\"start\":41435},{\"end\":43124,\"start\":41561},{\"end\":43889,\"start\":43126},{\"end\":44445,\"start\":43891},{\"end\":44822,\"start\":44447},{\"end\":45343,\"start\":44824},{\"end\":45723,\"start\":45345},{\"end\":45994,\"start\":45725},{\"end\":46594,\"start\":46010},{\"end\":47926,\"start\":46596},{\"end\":48228,\"start\":47928},{\"end\":50158,\"start\":48262},{\"end\":50352,\"start\":50160},{\"end\":51751,\"start\":50354},{\"end\":51910,\"start\":51800},{\"end\":52348,\"start\":51912},{\"end\":53451,\"start\":52350},{\"end\":54187,\"start\":53470},{\"end\":55178,\"start\":54189},{\"end\":55583,\"start\":55180},{\"end\":56013,\"start\":55585},{\"end\":56463,\"start\":56044},{\"end\":57300,\"start\":56465},{\"end\":58021,\"start\":57302}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":19451,\"start\":19419},{\"attributes\":{\"id\":\"formula_1\"},\"end\":19670,\"start\":19652},{\"attributes\":{\"id\":\"formula_2\"},\"end\":19721,\"start\":19689},{\"attributes\":{\"id\":\"formula_3\"},\"end\":19765,\"start\":19738},{\"attributes\":{\"id\":\"formula_4\"},\"end\":19878,\"start\":19808},{\"attributes\":{\"id\":\"formula_5\"},\"end\":20030,\"start\":19964},{\"attributes\":{\"id\":\"formula_6\"},\"end\":20184,\"start\":20141},{\"attributes\":{\"id\":\"formula_7\"},\"end\":20518,\"start\":20365},{\"attributes\":{\"id\":\"formula_8\"},\"end\":20720,\"start\":20578},{\"attributes\":{\"id\":\"formula_9\"},\"end\":21142,\"start\":21023},{\"attributes\":{\"id\":\"formula_10\"},\"end\":21753,\"start\":21602},{\"attributes\":{\"id\":\"formula_11\"},\"end\":21863,\"start\":21828},{\"attributes\":{\"id\":\"formula_12\"},\"end\":22235,\"start\":22177},{\"attributes\":{\"id\":\"formula_13\"},\"end\":22488,\"start\":22433},{\"attributes\":{\"id\":\"formula_14\"},\"end\":22539,\"start\":22488},{\"attributes\":{\"id\":\"formula_15\"},\"end\":22584,\"start\":22539},{\"attributes\":{\"id\":\"formula_16\"},\"end\":23018,\"start\":22904},{\"attributes\":{\"id\":\"formula_17\"},\"end\":26700,\"start\":26468},{\"attributes\":{\"id\":\"formula_18\"},\"end\":26801,\"start\":26746},{\"attributes\":{\"id\":\"formula_19\"},\"end\":26972,\"start\":26917},{\"attributes\":{\"id\":\"formula_20\"},\"end\":27124,\"start\":27089},{\"attributes\":{\"id\":\"formula_21\"},\"end\":30675,\"start\":30635},{\"attributes\":{\"id\":\"formula_22\"},\"end\":30903,\"start\":30858},{\"attributes\":{\"id\":\"formula_23\"},\"end\":31267,\"start\":31107},{\"attributes\":{\"id\":\"formula_24\"},\"end\":32850,\"start\":32802},{\"attributes\":{\"id\":\"formula_25\"},\"end\":34357,\"start\":34320},{\"attributes\":{\"id\":\"formula_26\"},\"end\":34735,\"start\":34630},{\"attributes\":{\"id\":\"formula_27\"},\"end\":35004,\"start\":34898},{\"attributes\":{\"id\":\"formula_28\"},\"end\":35478,\"start\":35356},{\"attributes\":{\"id\":\"formula_29\"},\"end\":35791,\"start\":35728},{\"attributes\":{\"id\":\"formula_30\"},\"end\":35936,\"start\":35902},{\"attributes\":{\"id\":\"formula_31\"},\"end\":36024,\"start\":35995},{\"attributes\":{\"id\":\"formula_32\"},\"end\":37947,\"start\":37892},{\"attributes\":{\"id\":\"formula_33\"},\"end\":40531,\"start\":40484}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":26235,\"start\":26229},{\"end\":48458,\"start\":48457},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":50418,\"start\":50412}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1837,\"start\":1825},{\"attributes\":{\"n\":\"1.1\"},\"end\":1864,\"start\":1840},{\"end\":5304,\"start\":5299},{\"attributes\":{\"n\":\"1.2\"},\"end\":9189,\"start\":9174},{\"attributes\":{\"n\":\"2\"},\"end\":12696,\"start\":12684},{\"attributes\":{\"n\":\"3\"},\"end\":19068,\"start\":19049},{\"attributes\":{\"n\":\"3.1\"},\"end\":19108,\"start\":19071},{\"attributes\":{\"n\":\"3.1.1\"},\"end\":19287,\"start\":19270},{\"attributes\":{\"n\":\"3.1.2\"},\"end\":21193,\"start\":21144},{\"attributes\":{\"n\":\"3.2\"},\"end\":23411,\"start\":23368},{\"end\":26442,\"start\":26435},{\"attributes\":{\"n\":\"4\"},\"end\":28606,\"start\":28570},{\"attributes\":{\"n\":\"4.1\"},\"end\":28921,\"start\":28885},{\"attributes\":{\"n\":\"4.2\"},\"end\":31808,\"start\":31789},{\"end\":34871,\"start\":34842},{\"attributes\":{\"n\":\"4.3\"},\"end\":37499,\"start\":37466},{\"attributes\":{\"n\":\"5\"},\"end\":39038,\"start\":39008},{\"attributes\":{\"n\":\"5.1\"},\"end\":39403,\"start\":39364},{\"attributes\":{\"n\":\"5.2\"},\"end\":40264,\"start\":40221},{\"attributes\":{\"n\":\"5.3\"},\"end\":41433,\"start\":41413},{\"attributes\":{\"n\":\"6\"},\"end\":46008,\"start\":45997},{\"attributes\":{\"n\":\"6.1\"},\"end\":48260,\"start\":48231},{\"attributes\":{\"n\":\"6.2\"},\"end\":51798,\"start\":51754},{\"attributes\":{\"n\":\"6.3\"},\"end\":53468,\"start\":53454},{\"attributes\":{\"n\":\"7\"},\"end\":56042,\"start\":56016},{\"end\":58647,\"start\":58639},{\"end\":59032,\"start\":59024},{\"end\":59290,\"start\":59282},{\"end\":59982,\"start\":59976},{\"end\":60208,\"start\":60200},{\"end\":60607,\"start\":60598},{\"end\":61547,\"start\":61538},{\"end\":61782,\"start\":61773},{\"end\":61967,\"start\":61958},{\"end\":62015,\"start\":62006},{\"end\":62129,\"start\":62120},{\"end\":62905,\"start\":62900},{\"end\":65743,\"start\":65734}]", "table": "[{\"end\":62212,\"start\":62131},{\"end\":62898,\"start\":62294},{\"end\":65732,\"start\":63247},{\"end\":67476,\"start\":66196}]", "figure_caption": "[{\"end\":58637,\"start\":58024},{\"end\":59022,\"start\":58649},{\"end\":59280,\"start\":59034},{\"end\":59974,\"start\":59292},{\"end\":60198,\"start\":59984},{\"end\":60596,\"start\":60210},{\"end\":61536,\"start\":60610},{\"end\":61771,\"start\":61550},{\"end\":61956,\"start\":61785},{\"end\":62004,\"start\":61970},{\"end\":62118,\"start\":62017},{\"end\":62294,\"start\":62215},{\"end\":63247,\"start\":62909},{\"end\":65906,\"start\":65745},{\"end\":66196,\"start\":65909}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":2572,\"start\":2566},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":3415,\"start\":3409},{\"end\":9218,\"start\":9209},{\"end\":10349,\"start\":10339},{\"end\":10520,\"start\":10514},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":23744,\"start\":23738},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":24226,\"start\":24220},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":24498,\"start\":24491},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":24858,\"start\":24849},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":25034,\"start\":25025},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":25401,\"start\":25392},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":25487,\"start\":25475},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":25682,\"start\":25670},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":25785,\"start\":25773},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26211,\"start\":26205},{\"end\":28554,\"start\":28544},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":28565,\"start\":28559},{\"end\":29200,\"start\":29195},{\"end\":29292,\"start\":29287},{\"end\":32053,\"start\":32047},{\"end\":32461,\"start\":32455},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":42983,\"start\":42977},{\"end\":43243,\"start\":43237},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":44254,\"start\":44247},{\"end\":48622,\"start\":48616},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":48689,\"start\":48682},{\"end\":48782,\"start\":48776},{\"end\":49208,\"start\":49201},{\"end\":50157,\"start\":50152},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":50771,\"start\":50764},{\"end\":51315,\"start\":51310},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":51718,\"start\":51711},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":52649,\"start\":52642},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":52899,\"start\":52892},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":54891,\"start\":54885},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":55409,\"start\":55403}]", "bib_author_first_name": "[{\"end\":68080,\"start\":68079},{\"end\":68096,\"start\":68095},{\"end\":68108,\"start\":68107},{\"end\":68122,\"start\":68121},{\"end\":68344,\"start\":68343},{\"end\":68350,\"start\":68349},{\"end\":68360,\"start\":68356},{\"end\":68654,\"start\":68653},{\"end\":68665,\"start\":68664},{\"end\":68674,\"start\":68673},{\"end\":68687,\"start\":68686},{\"end\":68696,\"start\":68695},{\"end\":68698,\"start\":68697},{\"end\":68713,\"start\":68712},{\"end\":68726,\"start\":68725},{\"end\":68737,\"start\":68736},{\"end\":68748,\"start\":68747},{\"end\":69110,\"start\":69109},{\"end\":69119,\"start\":69118},{\"end\":69126,\"start\":69125},{\"end\":69135,\"start\":69131},{\"end\":69377,\"start\":69376},{\"end\":69385,\"start\":69384},{\"end\":69391,\"start\":69390},{\"end\":69403,\"start\":69399},{\"end\":69623,\"start\":69622},{\"end\":69787,\"start\":69786},{\"end\":69795,\"start\":69794},{\"end\":69801,\"start\":69800},{\"end\":69811,\"start\":69807},{\"end\":69985,\"start\":69984},{\"end\":69997,\"start\":69996},{\"end\":70013,\"start\":70012},{\"end\":70165,\"start\":70164},{\"end\":70181,\"start\":70180},{\"end\":70402,\"start\":70401},{\"end\":70404,\"start\":70403},{\"end\":70411,\"start\":70410},{\"end\":70688,\"start\":70687},{\"end\":70696,\"start\":70695},{\"end\":70957,\"start\":70956},{\"end\":70967,\"start\":70966},{\"end\":70976,\"start\":70975},{\"end\":71142,\"start\":71141},{\"end\":71144,\"start\":71143},{\"end\":71692,\"start\":71691},{\"end\":71701,\"start\":71700},{\"end\":71913,\"start\":71912},{\"end\":71926,\"start\":71925},{\"end\":71934,\"start\":71933},{\"end\":72100,\"start\":72099},{\"end\":72109,\"start\":72108},{\"end\":72124,\"start\":72123},{\"end\":72301,\"start\":72300},{\"end\":72305,\"start\":72302},{\"end\":72312,\"start\":72311},{\"end\":72460,\"start\":72459},{\"end\":72468,\"start\":72467},{\"end\":72617,\"start\":72616},{\"end\":72626,\"start\":72625},{\"end\":72642,\"start\":72641},{\"end\":72659,\"start\":72658},{\"end\":72888,\"start\":72887},{\"end\":72897,\"start\":72896},{\"end\":72903,\"start\":72902},{\"end\":73120,\"start\":73119},{\"end\":73134,\"start\":73133},{\"end\":73145,\"start\":73144},{\"end\":73147,\"start\":73146},{\"end\":73368,\"start\":73367},{\"end\":73379,\"start\":73378},{\"end\":73390,\"start\":73389},{\"end\":73402,\"start\":73401},{\"end\":73646,\"start\":73645},{\"end\":73648,\"start\":73647},{\"end\":73876,\"start\":73875},{\"end\":73884,\"start\":73883},{\"end\":74060,\"start\":74059},{\"end\":74068,\"start\":74067},{\"end\":74076,\"start\":74075},{\"end\":74269,\"start\":74268},{\"end\":74276,\"start\":74275},{\"end\":74283,\"start\":74282},{\"end\":74291,\"start\":74290},{\"end\":74300,\"start\":74299},{\"end\":74497,\"start\":74496},{\"end\":74504,\"start\":74503},{\"end\":74512,\"start\":74511},{\"end\":74650,\"start\":74649},{\"end\":74652,\"start\":74651},{\"end\":74660,\"start\":74659},{\"end\":74670,\"start\":74666},{\"end\":74680,\"start\":74676},{\"end\":74871,\"start\":74870},{\"end\":74884,\"start\":74883},{\"end\":74894,\"start\":74893},{\"end\":75100,\"start\":75099},{\"end\":75107,\"start\":75106},{\"end\":75352,\"start\":75351},{\"end\":75358,\"start\":75357},{\"end\":75360,\"start\":75359},{\"end\":75368,\"start\":75367},{\"end\":75376,\"start\":75375},{\"end\":75394,\"start\":75393},{\"end\":75673,\"start\":75672},{\"end\":75680,\"start\":75679},{\"end\":75856,\"start\":75855},{\"end\":75864,\"start\":75863},{\"end\":75872,\"start\":75871},{\"end\":75885,\"start\":75881},{\"end\":75891,\"start\":75890},{\"end\":75897,\"start\":75896},{\"end\":76124,\"start\":76123},{\"end\":76132,\"start\":76131},{\"end\":76134,\"start\":76133},{\"end\":76367,\"start\":76366},{\"end\":76378,\"start\":76377},{\"end\":76389,\"start\":76388},{\"end\":76638,\"start\":76637},{\"end\":76645,\"start\":76644},{\"end\":76654,\"start\":76650},{\"end\":76813,\"start\":76812},{\"end\":76978,\"start\":76977},{\"end\":76992,\"start\":76988},{\"end\":77000,\"start\":76999},{\"end\":77220,\"start\":77219},{\"end\":77228,\"start\":77227},{\"end\":77239,\"start\":77238},{\"end\":77243,\"start\":77240},{\"end\":77438,\"start\":77437},{\"end\":77451,\"start\":77450},{\"end\":77462,\"start\":77461},{\"end\":77473,\"start\":77472},{\"end\":77717,\"start\":77716},{\"end\":77730,\"start\":77729},{\"end\":77979,\"start\":77978},{\"end\":77986,\"start\":77985},{\"end\":77993,\"start\":77992},{\"end\":78001,\"start\":78000},{\"end\":78010,\"start\":78009},{\"end\":78216,\"start\":78215},{\"end\":78225,\"start\":78224},{\"end\":78360,\"start\":78359},{\"end\":78367,\"start\":78366},{\"end\":78511,\"start\":78510},{\"end\":78517,\"start\":78516},{\"end\":78526,\"start\":78522},{\"end\":78708,\"start\":78707},{\"end\":78714,\"start\":78713},{\"end\":78720,\"start\":78719},{\"end\":78728,\"start\":78727},{\"end\":78737,\"start\":78736},{\"end\":78739,\"start\":78738},{\"end\":78747,\"start\":78746},{\"end\":79045,\"start\":79044},{\"end\":79054,\"start\":79053},{\"end\":79247,\"start\":79246},{\"end\":79256,\"start\":79255},{\"end\":79482,\"start\":79481},{\"end\":79491,\"start\":79490},{\"end\":79506,\"start\":79505},{\"end\":79774,\"start\":79773},{\"end\":79782,\"start\":79781},{\"end\":79784,\"start\":79783},{\"end\":80034,\"start\":80033},{\"end\":80042,\"start\":80041},{\"end\":80044,\"start\":80043},{\"end\":80274,\"start\":80273},{\"end\":80276,\"start\":80275},{\"end\":80284,\"start\":80283},{\"end\":80290,\"start\":80289},{\"end\":80301,\"start\":80300},{\"end\":80547,\"start\":80546},{\"end\":80557,\"start\":80556},{\"end\":80736,\"start\":80735},{\"end\":80743,\"start\":80742},{\"end\":80752,\"start\":80748},{\"end\":80996,\"start\":80995},{\"end\":81007,\"start\":81006},{\"end\":81018,\"start\":81017},{\"end\":81259,\"start\":81258},{\"end\":81268,\"start\":81267},{\"end\":81275,\"start\":81274},{\"end\":81286,\"start\":81285},{\"end\":81477,\"start\":81476},{\"end\":81479,\"start\":81478},{\"end\":81492,\"start\":81491},{\"end\":81503,\"start\":81502},{\"end\":81514,\"start\":81513},{\"end\":81707,\"start\":81706},{\"end\":81716,\"start\":81715},{\"end\":81725,\"start\":81724},{\"end\":81935,\"start\":81934},{\"end\":81937,\"start\":81936},{\"end\":81945,\"start\":81944},{\"end\":81951,\"start\":81950},{\"end\":81953,\"start\":81952},{\"end\":82145,\"start\":82144},{\"end\":82161,\"start\":82160},{\"end\":82312,\"start\":82311},{\"end\":82321,\"start\":82320},{\"end\":82529,\"start\":82528},{\"end\":82537,\"start\":82536},{\"end\":82547,\"start\":82546},{\"end\":82758,\"start\":82757},{\"end\":82765,\"start\":82764},{\"end\":82771,\"start\":82770},{\"end\":82779,\"start\":82778},{\"end\":82946,\"start\":82945},{\"end\":82954,\"start\":82953},{\"end\":82968,\"start\":82967},{\"end\":82976,\"start\":82975},{\"end\":83137,\"start\":83136},{\"end\":83143,\"start\":83142},{\"end\":83151,\"start\":83150},{\"end\":83153,\"start\":83152},{\"end\":83160,\"start\":83159},{\"end\":83171,\"start\":83167},{\"end\":83375,\"start\":83374},{\"end\":83382,\"start\":83381},{\"end\":83391,\"start\":83390},{\"end\":83399,\"start\":83398},{\"end\":83401,\"start\":83400},{\"end\":83595,\"start\":83594},{\"end\":83603,\"start\":83602},{\"end\":83612,\"start\":83608},{\"end\":83769,\"start\":83768},{\"end\":83778,\"start\":83777},{\"end\":83788,\"start\":83787},{\"end\":83795,\"start\":83794},{\"end\":83966,\"start\":83965},{\"end\":83977,\"start\":83976},{\"end\":83988,\"start\":83987},{\"end\":84169,\"start\":84168},{\"end\":84175,\"start\":84174},{\"end\":84183,\"start\":84182},{\"end\":84367,\"start\":84366},{\"end\":84375,\"start\":84374},{\"end\":84544,\"start\":84543},{\"end\":84553,\"start\":84552},{\"end\":84559,\"start\":84558},{\"end\":84731,\"start\":84730},{\"end\":84733,\"start\":84732},{\"end\":84883,\"start\":84882},{\"end\":84894,\"start\":84893},{\"end\":84906,\"start\":84905},{\"end\":85090,\"start\":85089},{\"end\":85092,\"start\":85091},{\"end\":85103,\"start\":85102},{\"end\":85110,\"start\":85109},{\"end\":85308,\"start\":85307},{\"end\":85316,\"start\":85315},{\"end\":85318,\"start\":85317},{\"end\":85560,\"start\":85559},{\"end\":85574,\"start\":85573},{\"end\":85586,\"start\":85585},{\"end\":85598,\"start\":85597},{\"end\":85606,\"start\":85605},{\"end\":85813,\"start\":85812},{\"end\":86240,\"start\":86239},{\"end\":86251,\"start\":86250},{\"end\":86262,\"start\":86261},{\"end\":86487,\"start\":86486},{\"end\":86495,\"start\":86494},{\"end\":86506,\"start\":86505},{\"end\":86814,\"start\":86813},{\"end\":87079,\"start\":87078},{\"end\":87085,\"start\":87084},{\"end\":87087,\"start\":87086},{\"end\":87329,\"start\":87328},{\"end\":87335,\"start\":87334},{\"end\":87537,\"start\":87536},{\"end\":87545,\"start\":87544},{\"end\":87547,\"start\":87546},{\"end\":87713,\"start\":87712},{\"end\":87720,\"start\":87719},{\"end\":87726,\"start\":87725},{\"end\":87728,\"start\":87727},{\"end\":87735,\"start\":87734}]", "bib_author_last_name": "[{\"end\":68093,\"start\":68081},{\"end\":68105,\"start\":68097},{\"end\":68119,\"start\":68109},{\"end\":68130,\"start\":68123},{\"end\":68347,\"start\":68345},{\"end\":68354,\"start\":68351},{\"end\":68365,\"start\":68361},{\"end\":68662,\"start\":68655},{\"end\":68671,\"start\":68666},{\"end\":68684,\"start\":68675},{\"end\":68693,\"start\":68688},{\"end\":68710,\"start\":68699},{\"end\":68723,\"start\":68714},{\"end\":68734,\"start\":68727},{\"end\":68745,\"start\":68738},{\"end\":68756,\"start\":68749},{\"end\":69116,\"start\":69111},{\"end\":69123,\"start\":69120},{\"end\":69129,\"start\":69127},{\"end\":69140,\"start\":69136},{\"end\":69382,\"start\":69378},{\"end\":69388,\"start\":69386},{\"end\":69397,\"start\":69392},{\"end\":69409,\"start\":69404},{\"end\":69629,\"start\":69624},{\"end\":69792,\"start\":69788},{\"end\":69798,\"start\":69796},{\"end\":69805,\"start\":69802},{\"end\":69815,\"start\":69812},{\"end\":69994,\"start\":69986},{\"end\":70010,\"start\":69998},{\"end\":70024,\"start\":70014},{\"end\":70178,\"start\":70166},{\"end\":70192,\"start\":70182},{\"end\":70408,\"start\":70405},{\"end\":70419,\"start\":70412},{\"end\":70693,\"start\":70689},{\"end\":70703,\"start\":70697},{\"end\":70964,\"start\":70958},{\"end\":70973,\"start\":70968},{\"end\":70981,\"start\":70977},{\"end\":71152,\"start\":71145},{\"end\":71698,\"start\":71693},{\"end\":71707,\"start\":71702},{\"end\":71923,\"start\":71914},{\"end\":71931,\"start\":71927},{\"end\":71942,\"start\":71935},{\"end\":72106,\"start\":72101},{\"end\":72121,\"start\":72110},{\"end\":72130,\"start\":72125},{\"end\":72309,\"start\":72306},{\"end\":72320,\"start\":72313},{\"end\":72465,\"start\":72461},{\"end\":72476,\"start\":72469},{\"end\":72623,\"start\":72618},{\"end\":72639,\"start\":72627},{\"end\":72656,\"start\":72643},{\"end\":72673,\"start\":72660},{\"end\":72894,\"start\":72889},{\"end\":72900,\"start\":72898},{\"end\":72911,\"start\":72904},{\"end\":73131,\"start\":73121},{\"end\":73142,\"start\":73135},{\"end\":73155,\"start\":73148},{\"end\":73376,\"start\":73369},{\"end\":73387,\"start\":73380},{\"end\":73399,\"start\":73391},{\"end\":73406,\"start\":73403},{\"end\":73657,\"start\":73649},{\"end\":73881,\"start\":73877},{\"end\":73888,\"start\":73885},{\"end\":74065,\"start\":74061},{\"end\":74073,\"start\":74069},{\"end\":74080,\"start\":74077},{\"end\":74273,\"start\":74270},{\"end\":74280,\"start\":74277},{\"end\":74288,\"start\":74284},{\"end\":74297,\"start\":74292},{\"end\":74308,\"start\":74301},{\"end\":74316,\"start\":74310},{\"end\":74501,\"start\":74498},{\"end\":74509,\"start\":74505},{\"end\":74516,\"start\":74513},{\"end\":74657,\"start\":74653},{\"end\":74664,\"start\":74661},{\"end\":74674,\"start\":74671},{\"end\":74685,\"start\":74681},{\"end\":74881,\"start\":74872},{\"end\":74891,\"start\":74885},{\"end\":74899,\"start\":74895},{\"end\":75104,\"start\":75101},{\"end\":75112,\"start\":75108},{\"end\":75355,\"start\":75353},{\"end\":75365,\"start\":75361},{\"end\":75373,\"start\":75369},{\"end\":75391,\"start\":75377},{\"end\":75399,\"start\":75395},{\"end\":75677,\"start\":75674},{\"end\":75684,\"start\":75681},{\"end\":75861,\"start\":75857},{\"end\":75869,\"start\":75865},{\"end\":75879,\"start\":75873},{\"end\":75888,\"start\":75886},{\"end\":75894,\"start\":75892},{\"end\":75905,\"start\":75898},{\"end\":76129,\"start\":76125},{\"end\":76138,\"start\":76135},{\"end\":76375,\"start\":76368},{\"end\":76386,\"start\":76379},{\"end\":76399,\"start\":76390},{\"end\":76642,\"start\":76639},{\"end\":76648,\"start\":76646},{\"end\":76659,\"start\":76655},{\"end\":76820,\"start\":76814},{\"end\":76986,\"start\":76979},{\"end\":76997,\"start\":76993},{\"end\":77009,\"start\":77001},{\"end\":77225,\"start\":77221},{\"end\":77236,\"start\":77229},{\"end\":77248,\"start\":77244},{\"end\":77448,\"start\":77439},{\"end\":77459,\"start\":77452},{\"end\":77470,\"start\":77463},{\"end\":77481,\"start\":77474},{\"end\":77727,\"start\":77718},{\"end\":77742,\"start\":77731},{\"end\":77983,\"start\":77980},{\"end\":77990,\"start\":77987},{\"end\":77998,\"start\":77994},{\"end\":78007,\"start\":78002},{\"end\":78018,\"start\":78011},{\"end\":78026,\"start\":78020},{\"end\":78222,\"start\":78217},{\"end\":78240,\"start\":78226},{\"end\":78364,\"start\":78361},{\"end\":78374,\"start\":78368},{\"end\":78514,\"start\":78512},{\"end\":78520,\"start\":78518},{\"end\":78530,\"start\":78527},{\"end\":78711,\"start\":78709},{\"end\":78717,\"start\":78715},{\"end\":78725,\"start\":78721},{\"end\":78734,\"start\":78729},{\"end\":78744,\"start\":78740},{\"end\":78755,\"start\":78748},{\"end\":78763,\"start\":78757},{\"end\":79051,\"start\":79046},{\"end\":79063,\"start\":79055},{\"end\":79253,\"start\":79248},{\"end\":79263,\"start\":79257},{\"end\":79488,\"start\":79483},{\"end\":79503,\"start\":79492},{\"end\":79514,\"start\":79507},{\"end\":79779,\"start\":79775},{\"end\":79788,\"start\":79785},{\"end\":80039,\"start\":80035},{\"end\":80048,\"start\":80045},{\"end\":80281,\"start\":80277},{\"end\":80287,\"start\":80285},{\"end\":80298,\"start\":80291},{\"end\":80305,\"start\":80302},{\"end\":80554,\"start\":80548},{\"end\":80565,\"start\":80558},{\"end\":80740,\"start\":80737},{\"end\":80746,\"start\":80744},{\"end\":80757,\"start\":80753},{\"end\":81004,\"start\":80997},{\"end\":81015,\"start\":81008},{\"end\":81027,\"start\":81019},{\"end\":81265,\"start\":81260},{\"end\":81272,\"start\":81269},{\"end\":81283,\"start\":81276},{\"end\":81294,\"start\":81287},{\"end\":81489,\"start\":81480},{\"end\":81500,\"start\":81493},{\"end\":81511,\"start\":81504},{\"end\":81522,\"start\":81515},{\"end\":81713,\"start\":81708},{\"end\":81722,\"start\":81717},{\"end\":81730,\"start\":81726},{\"end\":81942,\"start\":81938},{\"end\":81948,\"start\":81946},{\"end\":81961,\"start\":81954},{\"end\":82158,\"start\":82146},{\"end\":82176,\"start\":82162},{\"end\":82318,\"start\":82313},{\"end\":82327,\"start\":82322},{\"end\":82534,\"start\":82530},{\"end\":82544,\"start\":82538},{\"end\":82557,\"start\":82548},{\"end\":82762,\"start\":82759},{\"end\":82768,\"start\":82766},{\"end\":82776,\"start\":82772},{\"end\":82782,\"start\":82780},{\"end\":82951,\"start\":82947},{\"end\":82965,\"start\":82955},{\"end\":82973,\"start\":82969},{\"end\":82983,\"start\":82977},{\"end\":83140,\"start\":83138},{\"end\":83148,\"start\":83144},{\"end\":83157,\"start\":83154},{\"end\":83165,\"start\":83161},{\"end\":83176,\"start\":83172},{\"end\":83379,\"start\":83376},{\"end\":83388,\"start\":83383},{\"end\":83396,\"start\":83392},{\"end\":83412,\"start\":83402},{\"end\":83600,\"start\":83596},{\"end\":83606,\"start\":83604},{\"end\":83617,\"start\":83613},{\"end\":83775,\"start\":83770},{\"end\":83785,\"start\":83779},{\"end\":83792,\"start\":83789},{\"end\":83801,\"start\":83796},{\"end\":83974,\"start\":83967},{\"end\":83985,\"start\":83978},{\"end\":83996,\"start\":83989},{\"end\":84172,\"start\":84170},{\"end\":84180,\"start\":84176},{\"end\":84188,\"start\":84184},{\"end\":84372,\"start\":84368},{\"end\":84378,\"start\":84376},{\"end\":84550,\"start\":84545},{\"end\":84556,\"start\":84554},{\"end\":84564,\"start\":84560},{\"end\":84741,\"start\":84734},{\"end\":84891,\"start\":84884},{\"end\":84903,\"start\":84895},{\"end\":84914,\"start\":84907},{\"end\":85100,\"start\":85093},{\"end\":85107,\"start\":85104},{\"end\":85120,\"start\":85111},{\"end\":85313,\"start\":85309},{\"end\":85322,\"start\":85319},{\"end\":85571,\"start\":85561},{\"end\":85583,\"start\":85575},{\"end\":85595,\"start\":85587},{\"end\":85603,\"start\":85599},{\"end\":85616,\"start\":85607},{\"end\":85821,\"start\":85814},{\"end\":86248,\"start\":86241},{\"end\":86259,\"start\":86252},{\"end\":86272,\"start\":86263},{\"end\":86492,\"start\":86488},{\"end\":86503,\"start\":86496},{\"end\":86516,\"start\":86507},{\"end\":86822,\"start\":86815},{\"end\":87082,\"start\":87080},{\"end\":87091,\"start\":87088},{\"end\":87332,\"start\":87330},{\"end\":87339,\"start\":87336},{\"end\":87542,\"start\":87538},{\"end\":87551,\"start\":87548},{\"end\":87717,\"start\":87714},{\"end\":87723,\"start\":87721},{\"end\":87732,\"start\":87729},{\"end\":87739,\"start\":87736}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":68314,\"start\":68013},{\"attributes\":{\"id\":\"b1\"},\"end\":68478,\"start\":68316},{\"attributes\":{\"id\":\"b2\"},\"end\":68580,\"start\":68480},{\"attributes\":{\"id\":\"b3\"},\"end\":69056,\"start\":68582},{\"attributes\":{\"id\":\"b4\"},\"end\":69308,\"start\":69058},{\"attributes\":{\"id\":\"b5\"},\"end\":69596,\"start\":69310},{\"attributes\":{\"id\":\"b6\"},\"end\":69718,\"start\":69598},{\"attributes\":{\"id\":\"b7\"},\"end\":69944,\"start\":69720},{\"attributes\":{\"id\":\"b8\"},\"end\":70135,\"start\":69946},{\"attributes\":{\"id\":\"b9\"},\"end\":70367,\"start\":70137},{\"attributes\":{\"id\":\"b10\"},\"end\":70630,\"start\":70369},{\"attributes\":{\"id\":\"b11\"},\"end\":70844,\"start\":70632},{\"attributes\":{\"id\":\"b12\"},\"end\":70927,\"start\":70846},{\"attributes\":{\"id\":\"b13\"},\"end\":71107,\"start\":70929},{\"attributes\":{\"id\":\"b14\"},\"end\":71622,\"start\":71109},{\"attributes\":{\"id\":\"b15\"},\"end\":71847,\"start\":71624},{\"attributes\":{\"id\":\"b16\"},\"end\":72068,\"start\":71849},{\"attributes\":{\"id\":\"b17\"},\"end\":72254,\"start\":72070},{\"attributes\":{\"id\":\"b18\"},\"end\":72417,\"start\":72256},{\"attributes\":{\"id\":\"b19\"},\"end\":72564,\"start\":72419},{\"attributes\":{\"id\":\"b20\"},\"end\":72814,\"start\":72566},{\"attributes\":{\"id\":\"b21\"},\"end\":73039,\"start\":72816},{\"attributes\":{\"id\":\"b22\"},\"end\":73303,\"start\":73041},{\"attributes\":{\"id\":\"b23\"},\"end\":73573,\"start\":73305},{\"attributes\":{\"id\":\"b24\"},\"end\":73811,\"start\":73575},{\"attributes\":{\"id\":\"b25\"},\"end\":73994,\"start\":73813},{\"attributes\":{\"id\":\"b26\"},\"end\":74199,\"start\":73996},{\"attributes\":{\"id\":\"b27\"},\"end\":74465,\"start\":74201},{\"attributes\":{\"id\":\"b28\"},\"end\":74598,\"start\":74467},{\"attributes\":{\"id\":\"b29\"},\"end\":74838,\"start\":74600},{\"attributes\":{\"id\":\"b30\"},\"end\":75020,\"start\":74840},{\"attributes\":{\"id\":\"b31\"},\"end\":75266,\"start\":75022},{\"attributes\":{\"id\":\"b32\"},\"end\":75597,\"start\":75268},{\"attributes\":{\"id\":\"b33\"},\"end\":75800,\"start\":75599},{\"attributes\":{\"id\":\"b34\"},\"end\":76044,\"start\":75802},{\"attributes\":{\"id\":\"b35\"},\"end\":76295,\"start\":76046},{\"attributes\":{\"id\":\"b36\"},\"end\":76564,\"start\":76297},{\"attributes\":{\"id\":\"b37\"},\"end\":76785,\"start\":76566},{\"attributes\":{\"id\":\"b38\"},\"end\":76912,\"start\":76787},{\"attributes\":{\"id\":\"b39\"},\"end\":77168,\"start\":76914},{\"attributes\":{\"id\":\"b40\"},\"end\":77361,\"start\":77170},{\"attributes\":{\"id\":\"b41\"},\"end\":77633,\"start\":77363},{\"attributes\":{\"id\":\"b42\"},\"end\":77909,\"start\":77635},{\"attributes\":{\"id\":\"b43\"},\"end\":78175,\"start\":77911},{\"attributes\":{\"id\":\"b44\"},\"end\":78334,\"start\":78177},{\"attributes\":{\"id\":\"b45\"},\"end\":78443,\"start\":78336},{\"attributes\":{\"id\":\"b46\"},\"end\":78648,\"start\":78445},{\"attributes\":{\"id\":\"b47\"},\"end\":78994,\"start\":78650},{\"attributes\":{\"id\":\"b48\"},\"end\":79190,\"start\":78996},{\"attributes\":{\"id\":\"b49\"},\"end\":79365,\"start\":79192},{\"attributes\":{\"id\":\"b50\"},\"end\":79694,\"start\":79367},{\"attributes\":{\"id\":\"b51\"},\"end\":79947,\"start\":79696},{\"attributes\":{\"id\":\"b52\"},\"end\":80208,\"start\":79949},{\"attributes\":{\"id\":\"b53\"},\"end\":80497,\"start\":80210},{\"attributes\":{\"id\":\"b54\"},\"end\":80662,\"start\":80499},{\"attributes\":{\"id\":\"b55\"},\"end\":80883,\"start\":80664},{\"attributes\":{\"id\":\"b56\"},\"end\":81220,\"start\":80885},{\"attributes\":{\"id\":\"b57\"},\"end\":81400,\"start\":81222},{\"attributes\":{\"id\":\"b58\"},\"end\":81677,\"start\":81402},{\"attributes\":{\"id\":\"b59\"},\"end\":81847,\"start\":81679},{\"attributes\":{\"id\":\"b60\"},\"end\":82108,\"start\":81849},{\"attributes\":{\"id\":\"b61\"},\"end\":82273,\"start\":82110},{\"attributes\":{\"id\":\"b62\"},\"end\":82464,\"start\":82275},{\"attributes\":{\"id\":\"b63\"},\"end\":82681,\"start\":82466},{\"attributes\":{\"id\":\"b64\"},\"end\":82915,\"start\":82683},{\"attributes\":{\"id\":\"b65\"},\"end\":83083,\"start\":82917},{\"attributes\":{\"id\":\"b66\"},\"end\":83303,\"start\":83085},{\"attributes\":{\"id\":\"b67\"},\"end\":83554,\"start\":83305},{\"attributes\":{\"id\":\"b68\"},\"end\":83711,\"start\":83556},{\"attributes\":{\"id\":\"b69\"},\"end\":83922,\"start\":83713},{\"attributes\":{\"id\":\"b70\"},\"end\":84101,\"start\":83924},{\"attributes\":{\"id\":\"b71\"},\"end\":84306,\"start\":84103},{\"attributes\":{\"id\":\"b72\"},\"end\":84479,\"start\":84308},{\"attributes\":{\"id\":\"b73\"},\"end\":84682,\"start\":84481},{\"attributes\":{\"id\":\"b74\"},\"end\":84826,\"start\":84684},{\"attributes\":{\"id\":\"b75\"},\"end\":85033,\"start\":84828},{\"attributes\":{\"id\":\"b76\"},\"end\":85274,\"start\":85035},{\"attributes\":{\"id\":\"b77\"},\"end\":85400,\"start\":85276},{\"attributes\":{\"id\":\"b78\"},\"end\":85490,\"start\":85402},{\"attributes\":{\"id\":\"b79\"},\"end\":85756,\"start\":85492},{\"attributes\":{\"id\":\"b80\"},\"end\":85977,\"start\":85758},{\"attributes\":{\"id\":\"b81\"},\"end\":86168,\"start\":85979},{\"attributes\":{\"id\":\"b82\"},\"end\":86437,\"start\":86170},{\"attributes\":{\"id\":\"b83\"},\"end\":86745,\"start\":86439},{\"attributes\":{\"id\":\"b84\"},\"end\":86990,\"start\":86747},{\"attributes\":{\"id\":\"b85\"},\"end\":87251,\"start\":86992},{\"attributes\":{\"id\":\"b86\"},\"end\":87489,\"start\":87253},{\"attributes\":{\"id\":\"b87\"},\"end\":87643,\"start\":87491},{\"attributes\":{\"id\":\"b88\"},\"end\":87903,\"start\":87645},{\"attributes\":{\"id\":\"b89\"},\"end\":89027,\"start\":87905}]", "bib_title": "[{\"end\":68077,\"start\":68013},{\"end\":68341,\"start\":68316},{\"end\":68515,\"start\":68480},{\"end\":68651,\"start\":68582},{\"end\":69784,\"start\":69720},{\"end\":69982,\"start\":69946},{\"end\":70162,\"start\":70137},{\"end\":70399,\"start\":70369},{\"end\":70685,\"start\":70632},{\"end\":70954,\"start\":70929},{\"end\":71139,\"start\":71109},{\"end\":71689,\"start\":71624},{\"end\":71910,\"start\":71849},{\"end\":72097,\"start\":72070},{\"end\":72298,\"start\":72256},{\"end\":72457,\"start\":72419},{\"end\":72614,\"start\":72566},{\"end\":72885,\"start\":72816},{\"end\":73117,\"start\":73041},{\"end\":73365,\"start\":73305},{\"end\":73643,\"start\":73575},{\"end\":73873,\"start\":73813},{\"end\":74057,\"start\":73996},{\"end\":74266,\"start\":74201},{\"end\":74494,\"start\":74467},{\"end\":74647,\"start\":74600},{\"end\":74868,\"start\":74840},{\"end\":75097,\"start\":75022},{\"end\":75349,\"start\":75268},{\"end\":75670,\"start\":75599},{\"end\":75853,\"start\":75802},{\"end\":76121,\"start\":76046},{\"end\":76364,\"start\":76297},{\"end\":76635,\"start\":76566},{\"end\":76810,\"start\":76787},{\"end\":76975,\"start\":76914},{\"end\":77217,\"start\":77170},{\"end\":77435,\"start\":77363},{\"end\":77714,\"start\":77635},{\"end\":77976,\"start\":77911},{\"end\":78213,\"start\":78177},{\"end\":78357,\"start\":78336},{\"end\":78508,\"start\":78445},{\"end\":78705,\"start\":78650},{\"end\":79042,\"start\":78996},{\"end\":79244,\"start\":79192},{\"end\":79479,\"start\":79367},{\"end\":79771,\"start\":79696},{\"end\":80031,\"start\":79949},{\"end\":80271,\"start\":80210},{\"end\":80544,\"start\":80499},{\"end\":80733,\"start\":80664},{\"end\":80993,\"start\":80885},{\"end\":81256,\"start\":81222},{\"end\":81474,\"start\":81402},{\"end\":81704,\"start\":81679},{\"end\":81932,\"start\":81849},{\"end\":82142,\"start\":82110},{\"end\":82309,\"start\":82275},{\"end\":82526,\"start\":82466},{\"end\":82755,\"start\":82683},{\"end\":82943,\"start\":82917},{\"end\":83134,\"start\":83085},{\"end\":83372,\"start\":83305},{\"end\":83592,\"start\":83556},{\"end\":83766,\"start\":83713},{\"end\":83963,\"start\":83924},{\"end\":84166,\"start\":84103},{\"end\":84364,\"start\":84308},{\"end\":84541,\"start\":84481},{\"end\":84728,\"start\":84684},{\"end\":84880,\"start\":84828},{\"end\":85087,\"start\":85035},{\"end\":85305,\"start\":85276},{\"end\":85431,\"start\":85402},{\"end\":86237,\"start\":86170},{\"end\":86484,\"start\":86439},{\"end\":87076,\"start\":86992},{\"end\":87326,\"start\":87253},{\"end\":87534,\"start\":87491},{\"end\":87710,\"start\":87645}]", "bib_author": "[{\"end\":68095,\"start\":68079},{\"end\":68107,\"start\":68095},{\"end\":68121,\"start\":68107},{\"end\":68132,\"start\":68121},{\"end\":68349,\"start\":68343},{\"end\":68356,\"start\":68349},{\"end\":68367,\"start\":68356},{\"end\":68664,\"start\":68653},{\"end\":68673,\"start\":68664},{\"end\":68686,\"start\":68673},{\"end\":68695,\"start\":68686},{\"end\":68712,\"start\":68695},{\"end\":68725,\"start\":68712},{\"end\":68736,\"start\":68725},{\"end\":68747,\"start\":68736},{\"end\":68758,\"start\":68747},{\"end\":69118,\"start\":69109},{\"end\":69125,\"start\":69118},{\"end\":69131,\"start\":69125},{\"end\":69142,\"start\":69131},{\"end\":69384,\"start\":69376},{\"end\":69390,\"start\":69384},{\"end\":69399,\"start\":69390},{\"end\":69411,\"start\":69399},{\"end\":69631,\"start\":69622},{\"end\":69794,\"start\":69786},{\"end\":69800,\"start\":69794},{\"end\":69807,\"start\":69800},{\"end\":69817,\"start\":69807},{\"end\":69996,\"start\":69984},{\"end\":70012,\"start\":69996},{\"end\":70026,\"start\":70012},{\"end\":70180,\"start\":70164},{\"end\":70194,\"start\":70180},{\"end\":70410,\"start\":70401},{\"end\":70421,\"start\":70410},{\"end\":70695,\"start\":70687},{\"end\":70705,\"start\":70695},{\"end\":70966,\"start\":70956},{\"end\":70975,\"start\":70966},{\"end\":70983,\"start\":70975},{\"end\":71154,\"start\":71141},{\"end\":71700,\"start\":71691},{\"end\":71709,\"start\":71700},{\"end\":71925,\"start\":71912},{\"end\":71933,\"start\":71925},{\"end\":71944,\"start\":71933},{\"end\":72108,\"start\":72099},{\"end\":72123,\"start\":72108},{\"end\":72132,\"start\":72123},{\"end\":72311,\"start\":72300},{\"end\":72322,\"start\":72311},{\"end\":72467,\"start\":72459},{\"end\":72478,\"start\":72467},{\"end\":72625,\"start\":72616},{\"end\":72641,\"start\":72625},{\"end\":72658,\"start\":72641},{\"end\":72675,\"start\":72658},{\"end\":72896,\"start\":72887},{\"end\":72902,\"start\":72896},{\"end\":72913,\"start\":72902},{\"end\":73133,\"start\":73119},{\"end\":73144,\"start\":73133},{\"end\":73157,\"start\":73144},{\"end\":73378,\"start\":73367},{\"end\":73389,\"start\":73378},{\"end\":73401,\"start\":73389},{\"end\":73408,\"start\":73401},{\"end\":73659,\"start\":73645},{\"end\":73883,\"start\":73875},{\"end\":73890,\"start\":73883},{\"end\":74067,\"start\":74059},{\"end\":74075,\"start\":74067},{\"end\":74082,\"start\":74075},{\"end\":74275,\"start\":74268},{\"end\":74282,\"start\":74275},{\"end\":74290,\"start\":74282},{\"end\":74299,\"start\":74290},{\"end\":74310,\"start\":74299},{\"end\":74318,\"start\":74310},{\"end\":74503,\"start\":74496},{\"end\":74511,\"start\":74503},{\"end\":74518,\"start\":74511},{\"end\":74659,\"start\":74649},{\"end\":74666,\"start\":74659},{\"end\":74676,\"start\":74666},{\"end\":74687,\"start\":74676},{\"end\":74883,\"start\":74870},{\"end\":74893,\"start\":74883},{\"end\":74901,\"start\":74893},{\"end\":75106,\"start\":75099},{\"end\":75114,\"start\":75106},{\"end\":75357,\"start\":75351},{\"end\":75367,\"start\":75357},{\"end\":75375,\"start\":75367},{\"end\":75393,\"start\":75375},{\"end\":75401,\"start\":75393},{\"end\":75679,\"start\":75672},{\"end\":75686,\"start\":75679},{\"end\":75863,\"start\":75855},{\"end\":75871,\"start\":75863},{\"end\":75881,\"start\":75871},{\"end\":75890,\"start\":75881},{\"end\":75896,\"start\":75890},{\"end\":75907,\"start\":75896},{\"end\":76131,\"start\":76123},{\"end\":76140,\"start\":76131},{\"end\":76377,\"start\":76366},{\"end\":76388,\"start\":76377},{\"end\":76401,\"start\":76388},{\"end\":76644,\"start\":76637},{\"end\":76650,\"start\":76644},{\"end\":76661,\"start\":76650},{\"end\":76822,\"start\":76812},{\"end\":76988,\"start\":76977},{\"end\":76999,\"start\":76988},{\"end\":77011,\"start\":76999},{\"end\":77227,\"start\":77219},{\"end\":77238,\"start\":77227},{\"end\":77250,\"start\":77238},{\"end\":77450,\"start\":77437},{\"end\":77461,\"start\":77450},{\"end\":77472,\"start\":77461},{\"end\":77483,\"start\":77472},{\"end\":77729,\"start\":77716},{\"end\":77744,\"start\":77729},{\"end\":77985,\"start\":77978},{\"end\":77992,\"start\":77985},{\"end\":78000,\"start\":77992},{\"end\":78009,\"start\":78000},{\"end\":78020,\"start\":78009},{\"end\":78028,\"start\":78020},{\"end\":78224,\"start\":78215},{\"end\":78242,\"start\":78224},{\"end\":78366,\"start\":78359},{\"end\":78376,\"start\":78366},{\"end\":78516,\"start\":78510},{\"end\":78522,\"start\":78516},{\"end\":78532,\"start\":78522},{\"end\":78713,\"start\":78707},{\"end\":78719,\"start\":78713},{\"end\":78727,\"start\":78719},{\"end\":78736,\"start\":78727},{\"end\":78746,\"start\":78736},{\"end\":78757,\"start\":78746},{\"end\":78765,\"start\":78757},{\"end\":79053,\"start\":79044},{\"end\":79065,\"start\":79053},{\"end\":79255,\"start\":79246},{\"end\":79265,\"start\":79255},{\"end\":79490,\"start\":79481},{\"end\":79505,\"start\":79490},{\"end\":79516,\"start\":79505},{\"end\":79781,\"start\":79773},{\"end\":79790,\"start\":79781},{\"end\":80041,\"start\":80033},{\"end\":80050,\"start\":80041},{\"end\":80283,\"start\":80273},{\"end\":80289,\"start\":80283},{\"end\":80300,\"start\":80289},{\"end\":80307,\"start\":80300},{\"end\":80556,\"start\":80546},{\"end\":80567,\"start\":80556},{\"end\":80742,\"start\":80735},{\"end\":80748,\"start\":80742},{\"end\":80759,\"start\":80748},{\"end\":81006,\"start\":80995},{\"end\":81017,\"start\":81006},{\"end\":81029,\"start\":81017},{\"end\":81267,\"start\":81258},{\"end\":81274,\"start\":81267},{\"end\":81285,\"start\":81274},{\"end\":81296,\"start\":81285},{\"end\":81491,\"start\":81476},{\"end\":81502,\"start\":81491},{\"end\":81513,\"start\":81502},{\"end\":81524,\"start\":81513},{\"end\":81715,\"start\":81706},{\"end\":81724,\"start\":81715},{\"end\":81732,\"start\":81724},{\"end\":81944,\"start\":81934},{\"end\":81950,\"start\":81944},{\"end\":81963,\"start\":81950},{\"end\":82160,\"start\":82144},{\"end\":82178,\"start\":82160},{\"end\":82320,\"start\":82311},{\"end\":82329,\"start\":82320},{\"end\":82536,\"start\":82528},{\"end\":82546,\"start\":82536},{\"end\":82559,\"start\":82546},{\"end\":82764,\"start\":82757},{\"end\":82770,\"start\":82764},{\"end\":82778,\"start\":82770},{\"end\":82784,\"start\":82778},{\"end\":82953,\"start\":82945},{\"end\":82967,\"start\":82953},{\"end\":82975,\"start\":82967},{\"end\":82985,\"start\":82975},{\"end\":83142,\"start\":83136},{\"end\":83150,\"start\":83142},{\"end\":83159,\"start\":83150},{\"end\":83167,\"start\":83159},{\"end\":83178,\"start\":83167},{\"end\":83381,\"start\":83374},{\"end\":83390,\"start\":83381},{\"end\":83398,\"start\":83390},{\"end\":83414,\"start\":83398},{\"end\":83602,\"start\":83594},{\"end\":83608,\"start\":83602},{\"end\":83619,\"start\":83608},{\"end\":83777,\"start\":83768},{\"end\":83787,\"start\":83777},{\"end\":83794,\"start\":83787},{\"end\":83803,\"start\":83794},{\"end\":83976,\"start\":83965},{\"end\":83987,\"start\":83976},{\"end\":83998,\"start\":83987},{\"end\":84174,\"start\":84168},{\"end\":84182,\"start\":84174},{\"end\":84190,\"start\":84182},{\"end\":84374,\"start\":84366},{\"end\":84380,\"start\":84374},{\"end\":84552,\"start\":84543},{\"end\":84558,\"start\":84552},{\"end\":84566,\"start\":84558},{\"end\":84743,\"start\":84730},{\"end\":84893,\"start\":84882},{\"end\":84905,\"start\":84893},{\"end\":84916,\"start\":84905},{\"end\":85102,\"start\":85089},{\"end\":85109,\"start\":85102},{\"end\":85122,\"start\":85109},{\"end\":85315,\"start\":85307},{\"end\":85324,\"start\":85315},{\"end\":85573,\"start\":85559},{\"end\":85585,\"start\":85573},{\"end\":85597,\"start\":85585},{\"end\":85605,\"start\":85597},{\"end\":85618,\"start\":85605},{\"end\":85823,\"start\":85812},{\"end\":86250,\"start\":86239},{\"end\":86261,\"start\":86250},{\"end\":86274,\"start\":86261},{\"end\":86494,\"start\":86486},{\"end\":86505,\"start\":86494},{\"end\":86518,\"start\":86505},{\"end\":86824,\"start\":86813},{\"end\":87084,\"start\":87078},{\"end\":87093,\"start\":87084},{\"end\":87334,\"start\":87328},{\"end\":87341,\"start\":87334},{\"end\":87544,\"start\":87536},{\"end\":87553,\"start\":87544},{\"end\":87719,\"start\":87712},{\"end\":87725,\"start\":87719},{\"end\":87734,\"start\":87725},{\"end\":87741,\"start\":87734}]", "bib_venue": "[{\"end\":68136,\"start\":68132},{\"end\":68371,\"start\":68367},{\"end\":68521,\"start\":68517},{\"end\":68776,\"start\":68772},{\"end\":69107,\"start\":69058},{\"end\":69374,\"start\":69310},{\"end\":69620,\"start\":69598},{\"end\":69821,\"start\":69817},{\"end\":70030,\"start\":70026},{\"end\":70220,\"start\":70204},{\"end\":70475,\"start\":70421},{\"end\":70709,\"start\":70705},{\"end\":70999,\"start\":70983},{\"end\":71234,\"start\":71154},{\"end\":71713,\"start\":71709},{\"end\":71948,\"start\":71944},{\"end\":72136,\"start\":72132},{\"end\":72326,\"start\":72322},{\"end\":72482,\"start\":72478},{\"end\":72679,\"start\":72675},{\"end\":72917,\"start\":72913},{\"end\":73161,\"start\":73157},{\"end\":73412,\"start\":73408},{\"end\":73672,\"start\":73659},{\"end\":73894,\"start\":73890},{\"end\":74086,\"start\":74082},{\"end\":74322,\"start\":74318},{\"end\":74522,\"start\":74518},{\"end\":74691,\"start\":74687},{\"end\":74905,\"start\":74901},{\"end\":75118,\"start\":75114},{\"end\":75405,\"start\":75401},{\"end\":75690,\"start\":75686},{\"end\":75911,\"start\":75907},{\"end\":76144,\"start\":76140},{\"end\":76405,\"start\":76401},{\"end\":76665,\"start\":76661},{\"end\":76826,\"start\":76822},{\"end\":77015,\"start\":77011},{\"end\":77254,\"start\":77250},{\"end\":77487,\"start\":77483},{\"end\":77748,\"start\":77744},{\"end\":78032,\"start\":78028},{\"end\":78246,\"start\":78242},{\"end\":78380,\"start\":78376},{\"end\":78536,\"start\":78532},{\"end\":78782,\"start\":78778},{\"end\":79069,\"start\":79065},{\"end\":79269,\"start\":79265},{\"end\":79520,\"start\":79516},{\"end\":79795,\"start\":79790},{\"end\":80054,\"start\":80050},{\"end\":80326,\"start\":80307},{\"end\":80571,\"start\":80567},{\"end\":80763,\"start\":80759},{\"end\":81042,\"start\":81029},{\"end\":81300,\"start\":81296},{\"end\":81528,\"start\":81524},{\"end\":81736,\"start\":81732},{\"end\":81967,\"start\":81963},{\"end\":82182,\"start\":82178},{\"end\":82360,\"start\":82329},{\"end\":82563,\"start\":82559},{\"end\":82788,\"start\":82784},{\"end\":82989,\"start\":82985},{\"end\":83182,\"start\":83178},{\"end\":83418,\"start\":83414},{\"end\":83623,\"start\":83619},{\"end\":83807,\"start\":83803},{\"end\":84002,\"start\":83998},{\"end\":84194,\"start\":84190},{\"end\":84384,\"start\":84380},{\"end\":84570,\"start\":84566},{\"end\":84747,\"start\":84743},{\"end\":84920,\"start\":84916},{\"end\":85126,\"start\":85122},{\"end\":85328,\"start\":85324},{\"end\":85437,\"start\":85433},{\"end\":85557,\"start\":85492},{\"end\":85810,\"start\":85758},{\"end\":86031,\"start\":85979},{\"end\":86278,\"start\":86274},{\"end\":86552,\"start\":86518},{\"end\":86811,\"start\":86747},{\"end\":87097,\"start\":87093},{\"end\":87346,\"start\":87341},{\"end\":87557,\"start\":87553},{\"end\":87745,\"start\":87741},{\"end\":88322,\"start\":87905},{\"end\":71281,\"start\":71259}]"}}}, "year": 2023, "month": 12, "day": 17}