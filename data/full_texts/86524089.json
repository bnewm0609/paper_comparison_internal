{"id": 86524089, "updated": "2023-10-02 04:11:25.519", "metadata": {"title": "CoaCor: Code Annotation for Code Retrieval with Reinforcement Learning", "authors": "[{\"first\":\"Ziyu\",\"last\":\"Yao\",\"middle\":[]},{\"first\":\"Jayavardhan\",\"last\":\"Peddamail\",\"middle\":[\"Reddy\"]},{\"first\":\"Huan\",\"last\":\"Sun\",\"middle\":[]}]", "venue": null, "journal": "The World Wide Web Conference", "publication_date": {"year": 2019, "month": 3, "day": 13}, "abstract": "To accelerate software development, much research has been performed to help people understand and reuse the huge amount of available code resources. Two important tasks have been widely studied: code retrieval, which aims to retrieve code snippets relevant to a given natural language query from a code base, and code annotation, where the goal is to annotate a code snippet with a natural language description. Despite their advancement in recent years, the two tasks are mostly explored separately. In this work, we investigate a novel perspective of Code annotation for Code retrieval (hence called `CoaCor'), where a code annotation model is trained to generate a natural language annotation that can represent the semantic meaning of a given code snippet and can be leveraged by a code retrieval model to better distinguish relevant code snippets from others. To this end, we propose an effective framework based on reinforcement learning, which explicitly encourages the code annotation model to generate annotations that can be used for the retrieval task. Through extensive experiments, we show that code annotations generated by our framework are much more detailed and more useful for code retrieval, and they can further improve the performance of existing code retrieval models significantly.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1904.00720", "mag": "3098323906", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/www/YaoPS19", "doi": "10.1145/3308558.3313632"}}, "content": {"source": {"pdf_hash": "e0f3f5fa4306d2ec7667ed80353d7e53439a7c25", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1904.00720v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1904.00720", "status": "GREEN"}}, "grobid": {"id": "320d4bf485325ff2cf9ab9161a84414c22a575bb", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/e0f3f5fa4306d2ec7667ed80353d7e53439a7c25.txt", "contents": "\nCoaCor: Code Annotation for Code Retrieval with Reinforcement Learning\nMay 13-17, 2019\n\nZiyu Yao \nThe Ohio State University\nThe Ohio State University\nThe Ohio State University\n\n\nJayavardhan Reddy Peddamail \nThe Ohio State University\nThe Ohio State University\nThe Ohio State University\n\n\nHuan Sun \nThe Ohio State University\nThe Ohio State University\nThe Ohio State University\n\n\nCoaCor: Code Annotation for Code Retrieval with Reinforcement Learning\nMay 13-17, 201910.1145/3308558.3313632ACM Reference Format: Ziyu Yao, Jayavardhan Reddy Peddamail, and Huan Sun. 2019. CoaCor: Code Annotation for Code Retrieval with Reinforcement Learning. In Proceedings of the 2019 World Wide Web Conference (WWW '19),CCS CONCEPTS \u2022 Information systems \u2192 Novelty in information retrievalSummarization\u2022 Software and its engineering\u2022 Comput- ing methodologies \u2192 Reinforcement learningMarkov decision processesNeural networksKEYWORDS Code AnnotationCode RetrievalReinforcement Learning\nTo accelerate software development, much research has been performed to help people understand and reuse the huge amount of available code resources. Two important tasks have been widely studied: code retrieval, which aims to retrieve code snippets relevant to a given natural language query from a code base, and code annotation, where the goal is to annotate a code snippet with a natural language description. Despite their advancement in recent years, the two tasks are mostly explored separately. In this work, we investigate a novel perspective of Code annotation for Code retrieval (hence called \"CoaCor\"), where a code annotation model is trained to generate a natural language annotation that can represent the semantic meaning of a given code snippet and can be leveraged by a code retrieval model to better distinguish relevant code snippets from others. To this end, we propose an effective framework based on reinforcement learning, which explicitly encourages the code annotation model to generate annotations that can be used for the retrieval task. Through extensive experiments, we show that code annotations generated by our framework are much more detailed and more useful for code retrieval, and they can further improve the performance of existing code retrieval models significantly. 1\n\ntransportation and web security, depends on reliable software [1]. Unfortunately, developing and maintaining large code bases are very costly. Understanding and reusing billions of lines of code in online open-source repositories can significantly speed up the software development process. Towards that, code retrieval (CR) and code annotation (CA) are two important tasks that have been widely studied in the past few years [1,13,19,21,58], where the former aims to retrieve relevant code snippets based on a natural language (NL) query while the latter is to generate natural language descriptions to describe what a given code snippet does.\n\nMost existing work [2,13,19,22,58,60] study either code annotation or code retrieval individually. Earlier approaches for code retrieval drew inspiration from the information retrieval field [14,17,23,32] and suffered from surface form mismatches between natural language queries and code snippets [6,34]. More recently, advanced deep learning approaches have been successfully applied to both code retrieval and code annotation [1, 2, 9, 13, 19-22, 31, 58, 60]. For example, the code retrieval model proposed by Gu et al. [13] utilized two deep neural networks to learn the vector representation of a natural language query and that of a code snippet respectively, and adopted cosine similarity to measure their matching degree. For code annotation, Iyer et al. [21] and Hu et al. [19] utilized encoder-decoder models with an attention mechanism to generate an NL annotation for a code snippet. They aim to generate annotations similar to the human-provided ones, and therefore trained the models using the standard maximum likelihood estimation (MLE) objective. For the same purpose, Wan et al. [58] trained the code annotation model in a reinforcement learning (RL) framework with reward being the BLEU score [41], which measures n-gram matching precision between the currently generated annotation and the human-provided one.\n\nIn this work, we explore a novel perspective -code annotation for code retrieval (CoaCor), which is to generate an NL annotation for a code snippet so that the generated annotation can be used for code retrieval (i.e., can represent the semantic meaning of a code snippet and distinguish it from others w.r.t. a given NL query in the code retrieval task). As exemplified by [56], such an annotation can be taken as the representation of the corresponding code snippet, based on which the aforementioned lexical mismatch issue in a naive keyword-based search engine can be alleviated. A similar idea of improving retrieval by adding extra annotations to items is also explored in document retrieval [47] and image search [61]. However, most of them rely on humans to provide the annotations. Intuitively, our perspective can be interpreted as one type of machine-machine collaboration: On the one hand, the NL annotation generated by the code annotation model can serve as a second view of a code snippet (in addition to its programming content) and can be utilized to match with an NL query in code retrieval. On the other hand, with   Figure 1: Our CoaCor framework: (1) Training phase. A code annotation model is trained via reinforcement learning to maximize retrieval-based rewards given by a QC-based code retrieval model (pre-trained using <NL query, code snippet> pairs).\n\n(2) Testing phase. Each code snippet is first annotated by the trained CA model. For the code retrieval task, given query Q, a code snippet gets two scores -one matching Q with its code content and the other matching Q with its code annotation N, and is ranked by a simple ensemble strategy + . (3) Example. We show an example of a code snippet and its associated multiple NL queries in our dataset. The code annotation generated by our framework (denoted as RL MRR ) is much more detailed with many keywords semantically aligned with Qs, when compared with CA models trained via MLE or RL with BLEU rewards (RL BLEU ). + We simply use a weighted combination of the two scores, and other ensemble strategies can also apply here. a goal to facilitate code retrieval, the code annotation model can be stimulated to produce rich and detailed annotations. Unlike existing work [19][20][21]58], our goal is not to generate an NL annotation as close as possible to a human-provided one; hence, the MLE objective or the BLEU score as rewards for code annotation will not fit our setting. Instead, we design a novel rewarding mechanism in an RL framework, which guides the code annotation model directly based on how effectively the currently generated code annotation distinguishes the code snippet from a candidate set.\n\nLeveraging collaborations and interactions among machine learning models to improve the task performance has been explored in other scenarios. Goodfellow et al. [12] proposed the Generative Adversatial Nets (GANs), where a generative model produces difficult examples to fool a discriminative model and the latter is further trained to conquer the challenge. He et al. [15] proposed another framework called dual learning, which jointly learns two dual machine translation tasks (e.g., En -> Fr and Fr -> En). However, none of the existing frameworks are directly applicable to accomplish our goal (i.e., to train a code annotation model for generating annotations that can be utilized for code retrieval). Figure 1 shows our reinforcement learning-based CoaCor framework. In the training phase, we first train a CR model based on <natural language query, code snippet> (QC) pairs (referred to as QC-based CR model). Then given a code snippet, the CA model generates a sequence of NL tokens as its annotation and receives a reward from the trained CR model, which measures how effectively the generated annotation can distinguish the code snippet from others. We formulate the annotation generation process as a Markov Decision Process [5] and train the CA model to maximize the received reward via an advanced reinforcement learning [53] framework called Advantage Actor-Critic or A2C [36]. Once the CA model is trained, we use it to generate an NL annotation N for each code snippet C in the code base. Therefore, for each QC pair we originally have, we can derive a QN pair. We utilize the generated annotation as a second view of the code snippet to match with a query and train another CR model based on the derived QN pairs (referred to as QN-based CR model). In the testing phase, given an NL query, we rank code snippets by combining their scores from both the QC-based as well as QN-based CR models, which utilize both the programming content as well as the NL annotation of a code snippet.\n\nOn a widely used benchmark dataset [21] and a recently collected large-scale dataset [63], we show that the automatically generated annotation can significantly improve the retrieval performance. More impressively, without looking at the code content, the QNbased CR model trained on our generated code annotations obtains a retrieval performance comparable to one of the state-of-the-art QC-based CR models. It also surpasses other QN-based CR models trained using code annotations generated by existing CA models.\n\nTo summarize, our major contributions are as follows:\n\n\u2022 First, we explored a novel perspective of generating useful code annotations for code retrieval. Unlike existing work [19,21,58], we do not emphasize the n-gram overlap between the generated annotation and the human-provided one. Instead, we examined the real usefulness of the generated annotations and developed a machine-machine collaboration paradigm, where a code annotation model is trained to generate annotations that can be used for code retrieval. \u2022 Second, in order to accomplish our goal, we developed an effective RL-based framework with a novel rewarding mechanism, in which a code retrieval model is directly used to formulate rewards and guide the annotation generation. \u2022 Last, we conducted extensive experiments by comparing our framework with various baselines including state-of-the-art models and variants of our framework. We showed significant improvements of code retrieval performance on both a widely used benchmark dataset and a recently collected large-scale dataset. The rest of this paper is organized as follows. Section 2 introduces the background on code annotation and code retrieval tasks. Section 3 gives an overview of our proposed framework, with algorithm details followed in Section 4. Experiments are shown in Section 5. Finally, we discuss related work and conclude in Section 6 and 7.\n\n\nBACKGROUND\n\nWe adopt the same definitions for code retrieval and code annotation as previous work [9,21]. Given a natural language query Q and a set of code snippet candidates C, code retrieval is to retrieve code snippets C * \u2208 C that can match with the query. On the other hand, given a code snippet C, code annotation is to generate a natural language (NL) annotation N * which describes the code snippet appropriately. In this work, we use code search and code retrieval interchangeably (and same for code annotation/summary/description).\n\nFormally, for a training corpus with <natural language query, code snippet> pairs, e.g., those collected from Stack Overflow [51] by [21,63], we define the two tasks as:\n\nCode Retrieval (CR): Given an NL Query Q, a model F r will be learnt to retrieve the highest scoring code snippet C * \u2208 C.\nC * = argmax C \u2208C F r (Q, C)(1)\nCode Annotation (CA): For a given code snippet C, the goal is to generate an NL annotation N * that maximizes a scoring function F a :\nN * = argmax N F a (C, N )(2)\nNote that one can use the same scoring model for F r and F a as in [9,21], but for most of the prior work [13,19,20,58], which consider either code retrieval or code annotation, researchers usually develop their own models and objective functions for F r or F a . In our work, we choose two vanilla models as our base models for CR and CA, but explore a novel perspective of how to train F a so that it can generate NL annotations that can be used for code retrieval. This perspective is inspired by various machine-machine collaboration mechanisms [15,28,55,59] where one machine learning task can help improve another.\n\n\nFRAMEWORK OVERVIEW\n\nIn this section, we first introduce our intuition and give an overview of the entire framework, before diving into more details.\n\n\nIntuition behind CoaCor\n\nTo the best of our knowledge, previous code annotation work like [19][20][21]58] focused on getting a large n-gram overlap between generated and human-provided annotations. However, it is still uncertain (and non-trivial to test) how helpful the generated annotations can be. Driven by this observation, we are the first to examine the real usefulness of the generated code annotations and how they can help a relevant task, of which we choose code retrieval as an example. Intuitively, CoaCor can be interpreted as a collaboration mechanism between code annotation and code retrieval. On the one hand, the annotation produced by the CA model provides a second view of a code snippet (in addition to its programming content) to assist code retrieval. On the other hand, when the CA model is trained to be useful for the retrieval task, we expect it to produce richer and more detailed annotations, which we verify in experiments later.\n\n\nOverview\n\nThe main challenge to realize the above intuition lies in how to train the CA model effectively. Our key idea to address the challenge is shown in Figure 1.\n\nWe first train a base CR model on <natural language query, code snippet> (QC) pairs. Intuitively, a QC-based CR model ranks a code snippet C by measuring how well it matches with the given query Q (in comparison with other code snippets in the code base). From another point of view, a well-trained QC-based CR model can work as a measurement on whether the query Q describes the code snippet C precisely or not. Drawing inspiration from this view, we propose using the trained QC-based CR model to determine whether an annotation describes its code snippet precisely or not and thereby, train the CA model to generate rich annotations to maximize the retrieval-based reward from the CR model. Specifically, given a code snippet C, the CA model generates a sequence of NL words as its annotation N . At the end of the sequence, we let the trained QC-based CR model use N to search for relevant code snippets from the code base. If C can be ranked at top places, the annotation N is treated as well-written and gets a high reward; otherwise, a low reward will be returned. We formulate this generation process as the Markov Decision Process [5] and train the CA model with reinforcement learning [53] (specifically, the Advantage Actor-Critic algorithm [36]) to maximize the retrieval-based rewards it can receive from the QC-based CR model. We elaborate the CR and CA model details as well as the RL algorithm for training the CA model in Section 4.1 \u223c 4.3.\n\nOnce the CA model is trained, in the testing phase, it generates an NL annotation N for each code snippet C in the code base. Now for each <NL query, code snippet> pair originally in the datasets, we derive an <NL query, code annotation> (QN) pair and train another CR model based on such QN pairs. This QN-based CR model complements the QC-based CR model, as they respectively use the annotation and programming content of a code snippet to match with the query. We finally combine the matching scores from the two CR models to rank code snippets for a given query.\n\nNote that we aim to outline a general paradigm to explore the perspective of code annotation for code retrieval, where the specific model structures for the two CR models and the CA model can be instantiated in various ways. In this work, we choose one of the simplest and most fundamental deep structures for each of them. Using more complicated model structures will make the training more challenging and we leave it as future work.\n\n\nCoDE ANNOTATION FOR CoDE RETRIEVAL\n\nNow we introduce model and algorithm details in our framework.\n\n\nCode Retrieval Model\n\nBoth QC-based and QN-based code retrieval models adopt the same deep learning structure as the previous CR work [13]. Here for simplicity, we only illustrate the QC-based CR model in detail, and the QN-based model structure is the same except that we use the generated annotation on the code snippet side.\n\nAs shown in Figure 2, given an NL query Q = w 1.. |Q | and a code snippet C = c 1.. |C | , we first embed the tokens of both code and NL query into vectors through a randomly initialized word embedding matrix, which will be learned during model training. We then use a bidirectional Long Short-Term Memory (LSTM)-based Recurrent Neural Network (RNN) [10,11,18,35,48,64] to learn the token representation by summarizing the contextual information from both directions. The LSTM unit is composed of three multiplicative gates. At every time step t, it tracks the state of sequences by controlling how much information is updated into the new hidden state h t and memory cell \u0434 t from the previous state h t \u22121 , the previous memory cell \u0434 t \u22121 and the current input x t . On the code side, x t is the embedding vector for c t , and on the NL query side, it is the embedding vector for w t . At every time step t, the LSTM hidden state is updated as:\ni = \u03c3 (W i h t \u22121 + U i x t + b i ) f = \u03c3 (W f h t \u22121 + U f x t + b f ) o = \u03c3 (W o h t \u22121 + U o x t + b o ) \u0434 = tanh(W \u0434 h t \u22121 + U \u0434 x t + b \u0434 ) \u0434 t = f \u2299 \u0434 t \u22121 + i \u2299 \u0434 h t = o \u2299 tanh(\u0434 t )\nwhere \u03c3 is the element-wise sigmoid function and \u2299 is the elementwise product.\nU i , U f , U \u0434 , U o denote the weight matrices of different gates for input x t and W i , W f , W \u0434 , W o are the weight matrices for hidden state h t , while b i , b f , b \u0434 , b o denote the bias vectors.\nFor simplicity, we denote the above calculation as below (the memory cell vector \u0434 t \u22121 is omitted):\nh t = LSTM(x t , h t \u22121 )(3)\nThe vanilla LSTM's hidden state h t takes information from the past, knowing nothing about the future. Our CR model instead incorporates a bidirectional LSTM [48] (i.e., Bi-LSTM in Figure 2), which contains a forward LSTM reading a sequence X from start to end and a backward LSTM which reads from end to start. The basic idea of using two directions is to capture past and future information at each step. Then the two hidden states at each time step t are concatenated to form the final hidden state h t .\n\u2192 h t = LSTM(x t , \u2192 h t \u22121 ) \u2190 h t = LSTM(x t , \u2190 h t +1 ) h t = [ \u2192 h t , \u2190 h t ]\nFinally, we adopt the commonly used max pooling strategy [24] followed by a tanh layer to get the embedding vector for a sequence\nof length T . v = tanh(maxpooling([h 1 , h 2 , ..., h T ]))(4)\nBy applying the above encoding algorithm, we encode the code snippet C and the NL query Q into v c and v q , respectively. Similar to Gu et al. [13], to measure the relevance between the code snippet and the query, we use cosine similarity denoted as cos(v q , v c ). The higher the similarity, the more related the code is to the query.\n\nTraining. The CR model is trained by minimizing a ranking loss similar to [13]. Specifically, for each query Q in the training corpus, we prepare a triple of <Q, C, C \u2212 > as a training instance, where C is the correct code snippet that answers Q and C \u2212 is a negative code snippet that does not answer Q (which is randomly sampled from the entire code base). The ranking loss is defined as:\nL(\u03b8 ) = <Q,C,C \u2212 > max(0, \u03f5 \u2212 cos(v q , v c ) + cos(v q , v c \u2212 )) (5)\nwhere \u03b8 denotes the model parameters, \u03f5 is a constant margin, and v q , v c and v c \u2212 are the encoded vectors of Q, C and C \u2212 respectively.\n\nEssentially, the ranking loss is a kind of hinge loss [46] that promotes the cosine similarity between Q and C to be greater than that between Q and C \u2212 by at least a margin \u03f5. The training leads the model to project relevant queries and code snippets to be close in the vector space.\n\n\nCode Annotation Model\n\nFormally, given a code snippet C, the CA model computes the probability of generating a sequence of NL tokens n 1.. |N | = (n 1 , n 2 , ..., n |N | ) as its annotation N by:\nP(N |C) = P(n 1 |n 0 , C) |N | t =2 P(n t |n 1..t \u22121 , C)(6)\nwhere n 0 is a special token \"<START>\" indicating the start of the annotation generation, n 1..t \u22121 = (n 1 , ..., n t \u22121 ) is the partially generated annotation till time step t-1, and P(n t |n 1..t \u22121 , C) is the probability of producing n t as the next word given the code snippet C and the generated n 1..t \u22121 . The generation stops once a special token \"<EOS>\" is observed.\n\nIn this work, we choose the popular sequence-to-sequence model [52] as our CA model structure, which is composed of an encoder and a decoder. We employ the aforementioned bidirectional LSTMbased RNN structure as the encoder for code snippet C, and use another LSTM-based RNN as the decoder to compute Eqn. (6):\nh dec t = LSTM(n t \u22121 , h dec t \u22121 ), \u2200t = 1, ..., |N |\nwhere h dec t is the decoder hidden state at step t, and h dec 0 is initialized by concatenating the last hidden states of the code snippet encoder in both directions. In addition, a standard global attention layer [33] is applied in the decoder, in order to attend to important code tokens in C:\nh dec t = tanh(W \u03b1 [v attn , h dec t ]) v attn = |C | t \u2032 =1 \u03b1 t \u2032 h enc t \u2032 \u03b1 t \u2032 = softmax((h enc t \u2032 ) T h dec t )\nwhere \u03b1 t \u2032 is the attention weight on the t \u2032 -th code token when generating the t-th word in the annotation, and W \u03b1 is a learnable weight matrix. Finally, the t-th word is selected based on:\nP(n t |n 0..t \u22121 , C) = softmax(Wh dec t + b)(7)\nwhere W \u2208 R |V n |\u00d7d , b \u2208 R |V n | project the d-dim hidden stateh dec t to the NL vocabulary of size |V n |.\n\n\nTraining Code Annotation via RL\n\n\nCode Annotation as Markov Decision Process.\n\nMost previous work [9,19,21] trained the CA model by maximizing the loglikelihood of generating human annotations, which suffers from two drawbacks: (1) The exposure bias issue [4,44,45]. That is, during training, the model predicts the next word given the ground-truth annotation prefix, while at testing time, it generates the next word based on previous words generated by itself. This mismatch between training and testing may result in error accumulation in testing phase. (2) More importantly, maximizing the likelihood is not aligned with our goal to produce annotations that can be useful for code retrieval. To address the above issues, we propose to formulate code annotation within the reinforcement learning (RL) framework [53]. Specifically, the annotation generation process is viewed as a Markov Decision Process (MDP) [5] consisting of four main components:\n\nState. At step t during decoding, a state s t maintains the source code snippet and the previously generated words n 1..t \u22121 , i.e., s t = {C, n 1..t \u22121 }. In particular, the initial decoding state s 0 = {C} only contains the given code snippet C. In this work, we take the hidden state vectorh dec t as the vector representation of state s t , and the MDP is thus processing on a continuous and infinite state space.\n\nAction. The CA model decides the next word (or action) n t \u2208 V n , where V n is the NL vocabulary. Thus, the action space in our formulation is the NL vocabulary.\n\nReward. As introduced in Section 3, to encourage it to generate words useful for the code retrieval task, the CA model is rewarded by a well-trained QC-based CR model based on whether the code snippet C can be ranked at the top positions if using the generated complete annotation N as a query. Therefore, we define the reward at each step t as:\nr (s t , n t ) = RetrievalReward(C, n 1..t ) if n t = <EOS> 0 otherwise\nwhere we use the popular ranking metric Mean Reciprocal Rank [57] (defined in Section 5.2) as the RetrievalReward(C, N ) value. Note that, in this work, we let the CR model give a valid reward only when the annotation generation stops, and assign a zero reward to intermediate steps. However, other designs such as giving rewards to a partial generation with the reward shaping technique [4] can be reasonable and explored in the future.\n\nPolicy. The policy function P(n t |s t ) takes as input the current state s t and outputs the probability of generating n t as the next word. Given our definition about state s t and Eqn. (7),\nP(n t |s t ) = P(n t |n 1..t \u22121 , C) = softmax(Wh dec t + b)(8)\nHere, the policy function is stochastic in that the next word can be sampled according to the probability distribution, which allows action space exploration and can be optimized using policy gradient methods [53]. The objective of the CA model training is to find a policy function P(N |C) that maximizes the expected accumulative future reward:\nmax \u03d5 L(\u03d5) = max \u03d5 E N \u223cP (\u00b7 |C;\u03d5) [R(C, N )](9)\nwhere \u03d5 is the parameter of P and R(C, N ) = |N | t =1 r (s t , n t ) is the accumulative future reward (called \"return\").\n\nThe gradient of the above objective is derived as below.\n\u2207 \u03d5 L(\u03d5) = E N \u223cP (\u00b7 |C;\u03d5) [R(C, N )\u2207 \u03d5 log P(N |C; \u03d5)] = E n 1. .|N | \u223cP (\u00b7 |C;\u03d5) [ |N | t =1 R t (s t , n t )\u2207 \u03d5 log P(n t |n 1..t \u22121 , C; \u03d5)](10)\nwhere R t (s t , n t ) = t \u2032 \u2265t r (s t \u2032 , n t \u2032 ) is the return for generating word n t given state s t .\n\n\nAdvantage Actor-Critic for Code Annotation.\n\nGiven the gradient in Eqn. (10), the objective in Eqn. (9) can be optimized by policy gradient approaches such as REINFORCE [62] and Q-value Actor-Critic algorithm [4,54]. However, such methods may yield very high variance when the action space (i.e., the NL vocabulary) is large and suffer from biases when estimating the return of rarely taken actions [39], leading to unstable training. To tackle the challenge, we resort to the more advanced Advantage Actor-Critic or A2C algorithm [36], which has been adopted in other sequence generation tasks [39,58]. Specifically, the gradient function in Eqn. (10) is replaced by an advantage function:\n\u2207 \u03d5 L(\u03d5) = E[ |N | t =1 A t \u2207 \u03d5 log P(n t |n 1..t \u22121 , C; \u03d5)](11)A t = R t (s t , n t ) \u2212 V (s t ) V (s t ) = En t \u223cP (\u00b7 |n 1. .t \u22121 ,C) [R t (s t ,n t )]\nwhere V (s t ) is the state value function that estimates the future reward given the current state s t . Intuitively, V (s t ) works as a baseline function [62] to help the model assess its action n t more precisely: When advantage A t is greater than zero, it means that the return for taking action n t is better than the \"average\" return over all possible actions, given state s t ; otherwise, action n t performs worse than the average. Following previous work [36,39,58], we approximate V (s t ) by learning another model V (s t ; \u03c1) parameterized by \u03c1, and the RL framework thus contains two components: the policy function P(N |C; \u03d5) that generates the annotation (called \"Actor\"), and the state value function V (s t ; \u03c1) that approximates the return under state s t (called \"Critic\"). Similar to the actor model (i.e., the CA model in Section 4.2), we train a separate attention-based sequenceto-sequence model as the critic network. The critic value is finally computed by:\nV (s t ; \u03c1) = w T \u03c1 (h dec t ) cr t + b \u03c1(12)\nwhere (h dec t ) cr t \u2208 R d is the critic decoder hidden state at step t, and w \u03c1 \u2208 R d , b \u03c1 \u2208 R are trainable parameters that project the hidden state to a scalar value (i.e., the estimated state value).\n\nThe critic network is trained to minimize the Mean Square Error between its estimation and the true state value:\nmin \u03c1 L(\u03c1) = min \u03c1 E N \u223cP (\u00b7 |C) [ |N | t =1 (V (s t ; \u03c1) \u2212 R(C, N )) 2 ](13)\nThe entire training procedure of CoaCor is shown in Algorithm 1.\n\n\nGenerated Annotations for Code Retrieval\n\nAs previously shown in Figure 1, in the testing phase, we utilize the generated annotations to assist the code retrieval task. Now we detail the procedure in Algorithm 2. Specifically, for each <NL query, code snippet> pair (i.e., QC pair) in the dataset, we first derive an <NL query, code annotation> pair (i.e., QN pair) using the code annotation model. We then build another code retrieval (CR) model based on the QN pairs in our training corpus. In this work, for simplicity, we choose the same structure as the QC-based CR model (Section 4.1) to match QN pairs. However, more advanced methods for modeling the semantic similarity between two NL Receive a code snippet C.\n\n\n6:\n\nSample an annotation N \u223c P(\u00b7|C; \u03d5) according to Eqn. (8). 7: Receive the final reward R(C, N ).\n\n\n8:\n\nUpdate the code annotation model (\u03d5) using Eqn. (11). 9: Update the critic network (\u03c1) using Eqn. (13). 10: end for Algorithm 2 : Generated Annotations for Code Retrieval.\n\nInput: NL query Q, code snippet candidate C. Output: The matching score, score(Q, C). sentences (e.g., previous work on NL paraphase detection [16,26]) are applicable and can be explored as future work.\n\nThe final matching score between query Q and code snippet C combines those from the QN-based and QC-based CR model:\nscore(Q, C) = \u03bb * cos(v q , v n ) + (1 \u2212 \u03bb) * cos(v q , v c )(14)\nwhere v q , v c , v n are the encoded vectors of Q, C, and the code annotation N respectively. \u03bb \u2208 [0, 1] is a weighting factor for the two scores to be tuned on the validation set.\n\n\nEXPERIMENTS\n\nIn this section, we conduct extensive experiments and compare our framework with various models to show its effectiveness.\n\n\nExperimental Setup\n\nDataset. (1) We experimented with the StaQC dataset presented by Yao et al. [63]. The dataset contains 119,519 SQL <question title, code snippet> pairs mined from Stack Overflow [51], making itself the largest-to-date in SQL domain. In our code retrieval task, the question title is considered as the NL query Q, which is paired with the code snippet C to form the QC pair. We randomly selected 75% of the pairs for training, 10% for validation (containing 11,900 pairs), and the left 15% for testing (containing 17,850 pairs). As mentioned in [63], the dataset may contain multiple code snippets for the same NL query. We examine the dataset split to ensure that alternative relevant code snippets for the same query would not be sampled as negative code snippets when training the CR model. For pretraining the CA model, we consider the question title as a code annotation N and form QN pairs accordingly.\n\n(2) Iyer et al. [21] collected two small sets of SQL code snippets (called \"DEV\" and \"EVAL\" respectively) from Stack Overflow for validation and testing. In addition to the originally paired question title, each code snippet is manually annotated by two different NL descriptions. Therefore, in total, each set contains around 100 code snippets with three NL descriptions (resulting in around 300 QC pairs). We use them as additional datasets for model comparison. 2 Following [21], QC pairs occuring in DEV and EVAL set or being used as negative code snippets by them are removed from the StaQC training set.\n\nData Preprocessing. We followed [21] to perform code tokenization, which replaced table/column names with placeholder tokens and numbered them to preserve their dependencies. For text tokenization, we utilized the \"word_tokenize\" tool in the NLTK toolkit [7]. All code tokens and NL tokens with a frequency of less than 2 were replaced with an <UNK> token, resulting in totally 7726 code tokens and 7775 word tokens in the vocabulary. The average lengths of the NL query and the code snippet are 9 and 60 respectively.\n\n\nEvaluation\n\nWe evaluate a model's retrieval performance on four datasets: the validation (denoted as \"StaQC-val\") and test set (denoted as \"StaQCtest\") from StaQC [63], and the DEV and EVAL set from [21]. For each <NL query Q, code snippet C> pair (or QC pair) in a dataset, we take C as a positive code snippet and randomly sample K negative code snippets from all others except C in the dataset, 3 and calculate the rank of C among the K + 1 candidates. We follow [9,21,63] to set K = 49. The retrieval performance of a model is then assessed by the Mean Reciprocal Rank (MRR) metric [57] over the entire set D = {(Q 1 , C 1 ), (Q 2 , C 2 ), ..., (Q | D | , C | D | )}:\nMRR = 1 |D| | D | i=1 1 Rank i\nwhere Rank i is the rank of C i for query Q i . The higher the MRR value, the better the code retrieval performance.\n\n\nMethods to Compare\n\nIn order to test the effectiveness of our CoaCor framework, we compare it with both existing baselines and our proposed variants.\n\nExisting Baselines. We choose the following state-of-the-art code retrieval models, which are based on QC pairs, for comparison.\n\n\u2022 Deep Code Search (DCS) [13]. The original DCS model [13] adopts a similar structure as Figure 2 for CR in Java domain.\n\nTo learn the vector representations for code snippets, in addition to code tokens, it also considers features like function names and API sequences, all of which are combined into a fully connected layer. In our dataset, we do not have these features, and thus slightly modify their original model to be the same as our QC-based CR model (Figure 2). \u2022 CODE-NN [21]. CODE-NN is one of the state-of-the-art models for both code retrieval and code annotation. Its core component is an LSTM-based RNN with an attention mechanism, which models the probability of generating an NL sentence conditioned on a given code snippet. For code retrieval, given an NL query, CODE-NN computes the likelihood of generating the query as an annotation for each code snippet and ranks code snippets based on the likelihood.\n\nQN-based CR Variants. As discussed in Section 4.4, a trained CA model is used to annotate each code snippet C in our datasets with an annotation N . The resulting <NL query Q, code annotation N > pairs can be used to train a QN-based CR model. Depending on how we train the CA model, we have the following variants:\n\n\u2022 QN-MLE. Similar to most previous work [9,19,21], we simply train the CA model in the standard MLE manner, i.e., by maximizing the likelihood of a human-provided annotation. \u2022 QN-RL BLEU . As introduced in Section 1, Wan et al. [58] proposed to train the CA model via reinforcement learning with BLEU scores [41] as rewards. We compare this variant with our rewarding mechanism. \u2022 QN-RL MRR . In our CoaCor framework, we propose to train the CA model using retrieval rewards from a QC-based CR model (see Section 3). Here we use the MRR score as the retrieval reward.\n\nSince CODE-NN [21] can be used for code annotation as well, we also use its generated code annotations to train a QN-based CR model, denoted as \"QN-CodeNN\". 4\n\nEnsemble CR Variants. As introduced in Section 4.4, we tried ensembling the QC-based CR model and the QN-based CR model to improve the retrieval performance. We choose the DCS structure as the QC-based CR model as mentioned in Section 4.1. Since different QN-based CR models can be applied, we present the following 4 variants: (1) \n\n\nImplementation Details\n\nOur implementation is based on Pytorch [42]. For CR models, we set the embedding size of words and code tokens to 200, and chose batch size in {128, 256, 512}, LSTM unit size in {100, 200, 400} and dropout rate [50] in {0.1, 0.35, 0.5}. A small, fixed \u03f5 value of 0.05 is used in all the experiments. Hyper-parameters for each model were chosen based on the DEV set. For CODE-NN baseline, we followed Yao et al. [63] to use the same model hyper-parameters as the original paper, except that the dropout rate is tuned in {0.5, 0.7}. The StaQC-val set was used to decay the learning rate and the best model parameters were decided based on the retrieval performance on the DEV set. For CA models, the embedding size of words and code tokens and the LSTM unit size were selected from {256, 512}. The dropout rate is selected from {0.1, 0.3, 0.5} and the batch size is 64. We updated model parameters using the Adam optimizer [25] with learning rate 0.001 for MLE training and 0.0001 for RL training. The maximum length of the generated annotation is set to 20. For CodeNN, MLEbased and RL BLEU -based CA models, the best model parameters  [13] in one-tailed t-test (p < 0.01).\n\nwere picked based on the model's BLEU score on DEV, while for RL MRR -based CA model, we chose the best model according to its MRR reward on StaQC-val. For RL models, after pretraining the actor network via MLE, we first pretrain the critic network for 10 epochs, then jointly train the two networks for 40 epochs. Finally, for ensemble variants, the ensemble weight \u03bb in all variants is selected from 0.0 \u223c 1.0 based on its performance on DEV.\n\n\nResults\n\nTo understand our CoaCor framework, we first show several concrete examples to understand the differences between annotations generated by our model and by baseline/variant models, and then focus on two research questions (RQs):\n\n\u2022 RQ1 (CR improves CA): Is the proposed retrieval rewarddriven CA model capable of generating rich code annotations that can be used for code retrieval (i.e., can represent the code snippet and distinguish it from others)? \u2022 RQ2 (CA improves CR): Can the generated annotations further improve existing QC-based code retrieval models?\n\n5.5.1 Qualitative Analysis. Table 2 presents two examples of annotations generated by each CA model. Note that we do not target at human language-like annotations; rather, we focus on annotations that can describe/capture the functionality of a code snippet. In comparison with baseline CA models, our proposed RL MRR -based CA model can produce more concrete and precise descriptions for corresponding code snippets. As shown in Example 1, the annotation generated by RL MRR covers more conceptual keywords semantically aligned with the three NL queries (e.g., \"average\", \"difference\", \"group\"), while the baseline CODE-NN and the variants generate short descriptions covering a very limited amount of conceptual keywords (e.g., without mentioning the concept \"subtracting\"). We also notice that our CA model can generate different forms of a stem word (e.g., \"average\", \"avg\" in Example 1), partly because the retrieval-based reward tends to make the generated annotation semantically aligned with the code snippet and these diverse forms of words can help strengthen such semantic alignment and benefit the code retrieval task when there are various ways to express user search intent. Table 1 shows the code retrieval evaluation results, based on which we discuss RQ1 and RQ2 as below:\n\n\nCode Retrieval Performance Evaluation.\n\nRQ1: To examine whether or not the code annotations generated by a CA model can represent the corresponding code snippet in the code retrieval task, we analyze its corresponding QN-based CR model, which retrieves relevant code snippets by matching the NL query Q with the code annotation N generated by this CA model. Across all of the four datasets, our proposed QN-RL MRR model, which is based on a retrieval reward-driven CA model, achieves the best results and outperforms other QN-based CR models by a wide margin of around 0.1 \u223c 0.2 absolute MRR. More impressively, its performance is already on a par with the CODE-NN model, which is one of the state-of-the-art models for the code retrieval task, even though it understands a code snippet solely based on its annotation and without looking at the code content. This demonstrates that the code annotation generated by our proposed framework can reflect the semantic meaning of each code snippet more precisely.\n\nTo further understand whether or not the retrieval-based reward can serve as a better reward metric than BLEU (in terms of stimulating a CA model to generate useful annotations), we present the BLEU score of each CA model in Table 3. 5 When connecting this  table with Table 1, we observe an inverse trend: For the RL BLEU model which is trained for a higher BLEU score, although it can improve the MLE-based CA model by more than 2% absolute BLEU on three sets, it harms the latter's ability on producing useful code annotations (as revealed by the performance of QN-RL BLEU in Table  1, which is worse than QN-MLE by around 0.04 absolute MRR on StaQC-val and StaQC-test). In contrast, our proposed RL MRR model, despite getting the lowest BLEU score, is capable of generating annotations useful for the retrieval task. This is mainly because that  Table 2: Two examples of code snippets and their annotations generated by different CA models. \"Human-provided\" refers to (multiple) human-provided NL annotations or queries. Words semantically aligned between the generated and the human-provided annotations are highlighted.\n\nBLEU score calculates surface form overlaps while the retrievalbased reward measures the semantically aligned correspondences.\n\nThese observations imply an interesting conclusion: Compared with BLEU, a (task-oriented) semantic measuring reward, such as our retrieval-based MRR score, can better stimulate the model to produce detailed and useful generations. This is in line with the recent discussions on whether the automatic BLEU score is an appropriate evaluation metric for generation tasks or not [30,40]. In our work, we study the potential to use the performance of a relevant model to guide the learning of the target model, which can be generalized to many other scenarios, e.g., conversation generation [27], machine translation [4,44], etc.\n\n\nRQ2:\n\nWe first inspect whether the generated code annotations can assist the base code retrieval model (i.e., DCS) or not by comparing several ensemble CR variants. It is shown that, by simply combining the matching scores from QN-RL MRR and DCS with a weighting factor, our proposed model is able to significantly outperform the DCS model by 0.01 \u223c 0.03 and the CODE-NN baseline by 0.03  Table 3: The BLEU score of each code annotation model. \u223c 0.06 consistently across all datasets, showing the advantage of utilizing code annotations for code retrieval. Particularly, the best performance is achieved when the ensemble weight \u03bb = 0.4 (i.e., 0.4 weight on the QN-based CR score and 0.6 on the QC-based CR score), meaning that the model relies heavily on the code annotation to achieve better performance.\n\nIn contrast, QN-CodeNN, QN-MLE and QN-RL BLEU can hardly improve the base DCS model, and their best performances are all achieved when the ensemble weight \u03bb = 0.0 \u223c 0.2, indicating little help from annotations generated by CODE-NN, MLE-based and BLEU-rewarded CA. This is consistent with our conclusions to RQ1.\n\nWe also investigate the benefit of our generated annotations to other code retrieval models (besides DCS) by examining a baseline \"QN-RL MRR + CODE-NN\", which combines QN-RL MRR and CODE-NN (as a QC-based CR model) to score a code snippet candidate. As mentioned in Section 5.3, CODE-NN scores a code snippet by the likelihood of generating the given NL query when taking this code snippet as the input. Since the score is in a different range from the cosine similarity given by QN-RL MRR , we first rescale it by taking its log value and dividing it by the largest absolute log score among all code candidates. The rescaled score is then combined with the cosine similarity score from QN-RL MRR following Eqn. (14). The result is shown in the last row of Table 1. It is very impressive that, with the help of QN-RL MRR , the CODE-NN model can be improved by \u2265 0.05 absolute MRR value across all test sets.\n\nIn summary, through extensive experiments, we show that our proposed framework can generate code annotations that are much more useful for building effective code retrieval models, in comparison with existing CA models or those trained by MLE or BLEUbased RL. Additionally, the generated code annotations can further improve the retrieval performance, when combined with existing CR models like DCS and CODE-NN.\n\n\nDISCUSSION\n\nIn this work, we propose a novel perspective of using a relevant downstream task (i.e., code retrieval) to guide the learning of a target task (i.e., code annotation), illustrating a novel machinemachine collaboration paradigm. It is shown that the annotations generated by the RL MRR CA model (trained with rewards from the DCS model) can boost the performance of the CODE-NN model, which was not involved in any stage of the training process. It is interesting to explore more about machine-machine collaboration mechanisms, where multiple models for either the same task or relevant tasks can be utilized in tandem to provide different views or effective rewards to improve the final performance.\n\nIn terms of training, we also experimented with directly using a QN-based CR model or an ensemble CR model for rewarding the CA model. However, these approaches do not work well, since we do not have a rich set of QN pairs as training data in the beginning. Collecting paraphrases of queries to form QN pairs is non-trivial, which we leave to the future.\n\nFinally, our CoaCor framework is applicable to other programming languages, such as Python and C#, extension to which is interesting to study as future work.\n\n\nRELATED WORK\n\nCode Retrieval. As introduced in Section 1, code retrieval has been studied widely with information retrieval methods [14,17,23,32,56] and recent deep learning models [3,13,21]. Particularly, Keivanloo et al. [23] extracted abstract programming patterns and their associated NL keywords from code snippets in a code base, with which a given NL query can be projected to a set of associated programming patterns facilitating code content-based search. Similarly, Vinayakarao et al. [56] built an entity discovery system to mine NL phrases and their associated syntactic patterns, based on which they annotated each line of code snippets with NL phrases. Such annotations were utilized to improve NL keywordbased search engines. Different from these work, we construct a neural network-based code annotation model to describe the functionality of an entire code snippet. Our code annotation model is explicitly trained to produce meaningful words that can be used for code search. In our framework, the code retrieval model adopts a similar deep structure as the Deep Code Search model proposed by Gu et al. [13], which projects a NL query and a code snippet into a vector space and measures the cosine similarity between them.\n\nCode Annotation. Code annotation/summarization has drawn a lot of attention in recent years. Earlier works tackled the problem using template-based approaches [37,49] and topic n-grams models [38] while the recent techniques [2,[19][20][21][22]31] are mostly built upon deep neural networks. Specifically, Sridhara et al. [49] developed a software word usage model to identify action, theme and other arguments from a given code snippet, and generated code comments with templates. Allamanis et al. [2] employed convolution on the input tokens to detect local time-invariant and long-range topical attention features to summarize a code snippet into a short, descriptive function name-like summary. Most related to our work are [19] and [58], which utilized sequence-to-sequence networks with attention over code tokens to generate natural language annotations. They aimed to generate NL annotations as close as possible to human-provided annotations for human readability, and hence adopted the Maximum Likelihood Estimation (MLE) or BLEU score optimization as the objective. However, our goal is to generate code annotations which can be used for code retrieval, and therefore we design a retrieval-based reward to drive our training.\n\nDeep Reinforcement Learning for Sequence Generation. Reinforcement learning (RL) [53] has shown great success in various tasks where an agent has to perform multiple actions before obtaining a reward or when the metric to optimize is not differentiable. The sequence generation tasks, such as machine translation [4,39,44], image captioning [45], dialogue generation [27] and text summarization [43], have all benefitted from RL to address the exposure bias issue [4,44,45] and to directly optimize the model towards a certain metric (e.g., BLEU). Particularly, Ranzato et al. [44] were among the first to successfully apply the REINFORCE algorithm [62] to train RNN models for several sequence generation tasks, indicating that directly optimizing the metric used at test time can lead to significantly better models than those trained via MLE. Bahdanau et al. [4] additionally learned a critic network to better estimate the return (i.e., future rewards) of taking a certain action under a specific state, and trained the entire generation model via the Actor-Critic algorithm [54]. We follow Nguyen et al. [39] and Wan et al. [58] to further introduce an advantage function and train the code annotation model via the Advantage Actor-Critic algorithm [36], which is helpful for reducing biases from rarely taken actions. However, unlike their work, the reward in our framework is based on the performance on a different yet relevant task (i.e., code retrieval), rather than the BLEU metric.\n\nMachine-Machine Collaboration via Adversarial Training and Dual/Joint Learning. Various kinds of machine-machine collaboration mechanisms have been studied in many scenarios [12,15,28,55,59]. For example, Goodfellow et al. [12] proposed the Generative Adversarial Nets (GANs) framework, where a generative model generates images to fool a discriminative classifier, and the latter is further improved to distinguish the generated from the real ones. He et al. [15] proposed the dual learning framework and jointly optimized the machine translation from English to French and from French to English. Li et al. [29] trained a paraphrase generator by rewards from a paraphrase evaluator model. In the context of code retrieval and annotation, Chen and Zhou [9] and Iyer et al. [21] showed that their models can be used directly or with slight modification for both tasks, but their training objective only considered one of the two tasks. All these frameworks are not directly applicable to achieve our goal, i.e., training a code annotation model to generate rich NL annotations that can be used for code search.\n\n\nCONCLUSION\n\nThis paper explored a novel perspective of generating code annotations for code retrieval. To this end, we proposed a reinforcement learning-based framework (named \"CoaCor\") to maximize a retrieval-based reward. Through comprehensive experiments, we demonstrated that the annotation generated by our framework is more detailed to represent the semantic meaning of a code snippet. Such annotations can also improve the existing code content-based retrieval models significantly. In the future, we will explore other usages of the generated code annotations, as well as generalizing our framework to other tasks such as machine translation.\n\n\nquery to get the last record from a\n\nFigure 2 :\n2The base code retrieval model encodes the input code snippet C = (c 1 , c 2 , ..., c |C | ) and NL query Q = (w 1 , w 2 , ..., w |Q| ) into a vector space and outputs a similarity score.\n\n1 :\n1Receive score 1 (Q, C) = cos(v q , v c ) from a QC-based code retrieval model. 2: Generate a code annotation N \u223c P(\u00b7|C; \u03d5) via greedy search, according to Eqn. (8). 3: Receive score 2 (Q, C) = cos(v q , v n ) from a QN-based code retrieval model. 4: Calculate score(Q, C) according to Eqn.(14).\n\n\nQN-MLE + DCS, (2) QN-RL BLEU + DCS, (3) QN-RL MRR + DCS, and (4) QN-CodeNN + DCS, where QN-MLE, QN-RL BLEU , QN-RL MRR , and QN-CodeNN have been introduced.\n\n\nAlgorithm 1 : Training Procedure for CoaCor.Input: <NL query, code snippet> (QC) pairs in training set, number \nof iterations E. \n1: Train a base code retrieval model based on QC pairs, according \nto Eqn. (5). \n2: Initialize a base code annotation model (\u03d5) and pretrain it via \nMLE according to Eqn. (7), using Q as the desired N for C. \n3: Pretrain a critic network (\u03c1) according to Eqn. (13). \n4: for iteration = 1 to E do \n\n5: \n\n\n\n\nThe main code retrieval results (MRR). * denotes significantly different from DCSModel \n\nDEV \nEVAL StaQC-val StaQC-test \n\nExisting (QC-based) CR Baselines \nDCS [13] \n0.566 \n0.555 \n0.534 \n0.529 \nCODE-NN [21] \n0.530 \n0.514 \n0.526 \n0.522 \n\nQN-based CR Variants \nQN-CodeNN \n0.369 \n0.360 \n0.336 \n0.333 \nQN-MLE \n0.429 \n0.411 \n0.427 \n0.424 \nQN-RL BLEU \n0.426 \n0.402 \n0.386 \n0.381 \nQN-RL MRR (ours) \n0.534 \n0.512 \n0.516 \n0.523 \n\nEnsemble CR Variants \nQN-CodeNN + DCS \n0.566 \n0.555 \n0.534 \n0.529 \nQN-MLE + DCS \n0.571 \n0.561 \n0.543 \n0.537 \nQN-RL BLEU + DCS \n0.570 \n0.559 \n0.541 \n0.534 \nQN-RL MRR + DCS (ours) \n0.582  *  0.572  *  \n0.558  *  \n0.559  *  \n\nQN-RL MRR + CODE-NN (ours) 0.586  *  0.571  *  \n0.575  *  \n0.576  *  \nTable 1: \n\n\nhow do i get the count of distinct rows? MLE mysql query to get count of distinct values in a column RL BLEU how to count in mysql sql query RL MRR group_concat count concatenate distinct comma group mysql concat column in one row rows select multiple columns of same id resultModel \n\nAnnotation \n\nExample 1 from EVAL set \nSQL Code \n\nSELECT col3, Format(Avg([col2]-[col1]), \"hh:mm:ss\") \nAS TimeDiff FROM Table1 \nGROUP BY col3; \n\nHuman-\nprovided \n\n(1) find the average time in hours , mins and seconds \nbetween 2 values and show them in groups of another \ncolumn \n(2) group rows of a table and find average difference \nbetween them as a formatted date \n(3) ms access average after subtracting \n\nCODE-NN \n\nhow do i get the average of a column in sql? \n\nMLE \n\nhow to get average of the average of a column in sql \n\nRL BLEU \n\nhow to average in sql query \n\nRL MRR \n\naverage avg calculating difference day in access select \ndistinct column value sql group by month mysql for-\nmat date function? \n\nExample 2 from StaQC-test set \nSQL Code \n\nSELECT Group_concat(DISTINCT( p.products_id )) \nAS comma_separated, \nCOUNT(DISTINCT p.products_id) AS product_count \nFROM ... \n\nHuman-\nprovided \n\nhow to count how many comma separated values in \na group_concat \n\nCODE-NN \n\n\nPrevious work[9,21] used only one of the three descriptions while we utilize all of them to enrich and enlarge the datasets for a more reliable evaluation.3 For DEV and EVAL, we use the same negative examples as[21].\nThere is another recent code annotation method named DeepCom[19]. We did not include it as baseline, since it achieved a similar performance as our MLE-based CA model (seeTable 3) when evaluated with the standard BLEU script by[21].\nBLEU is evaluated with the script provided by Iyer et al.[21]: https://github.com/ sriniiyer/codenn/blob/master/src/utils/bleu.py.\nACKNOWLEDGMENTSThis research was sponsored in part by the Army Research Office under cooperative agreements W911NF-17-1-0412, NSF Grant IIS1815674, Fujitsu gift grant, and Ohio Supercomputer Center[8]. The views and conclusions contained herein are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Office or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notice herein.\nA survey of machine learning for big code and naturalness. Miltiadis Allamanis, T Earl, Premkumar Barr, Charles Devanbu, Sutton, ACM Computing Surveys (CSUR). 5181Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. 2018. A survey of machine learning for big code and naturalness. ACM Computing Surveys (CSUR) 51, 4 (2018), 81.\n\nA convolutional attention network for extreme summarization of source code. Miltiadis Allamanis, Hao Peng, Charles Sutton, International Conference on Machine Learning. Miltiadis Allamanis, Hao Peng, and Charles Sutton. 2016. A convolutional at- tention network for extreme summarization of source code. In International Conference on Machine Learning. 2091-2100.\n\nBimodal modelling of source code and natural language. Miltos Allamanis, Daniel Tarlow, Andrew Gordon, Yi Wei, International Conference on Machine Learning. Miltos Allamanis, Daniel Tarlow, Andrew Gordon, and Yi Wei. 2015. Bimodal modelling of source code and natural language. In International Conference on Machine Learning. 2123-2132.\n\nAn actor-critic algorithm for sequence prediction. Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron Courville, Yoshua Bengio, arXiv:1607.07086arXiv preprintDzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron Courville, and Yoshua Bengio. 2016. An actor-critic algorithm for sequence prediction. arXiv preprint arXiv:1607.07086 (2016).\n\nA Markovian decision process. Richard Bellman, Journal of Mathematics and Mechanics. Richard Bellman. 1957. A Markovian decision process. Journal of Mathematics and Mechanics (1957), 679-684.\n\nProgram understanding and the concept assignment problem. J Ted, Bharat G Biggerstaff, Dallas E Mitbander, Webster, Commun. ACM. 37Ted J Biggerstaff, Bharat G Mitbander, and Dallas E Webster. 1994. Program understanding and the concept assignment problem. Commun. ACM 37, 5 (1994), 72-82.\n\nNLTK: the natural language toolkit. Steven Bird, Edward Loper, Proceedings of the ACL 2004 on Interactive poster and demonstration sessions. the ACL 2004 on Interactive poster and demonstration sessionsAssociation for Computational Linguistics31Steven Bird and Edward Loper. 2004. NLTK: the natural language toolkit. In Proceedings of the ACL 2004 on Interactive poster and demonstration sessions. Association for Computational Linguistics, 31.\n\n. Ohio Supercomputer, Center, Ohio Supercomputer Center. Ohio Supercomputer Center. 1987. Ohio Supercomputer Center. http://osc.edu/ ark:/19495/f5s1ph73.\n\nA neural framework for retrieval and summarization of source code. Qingying Chen, Minghui Zhou, Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering. the 33rd ACM/IEEE International Conference on Automated Software EngineeringACMQingying Chen and Minghui Zhou. 2018. A neural framework for retrieval and summarization of source code. In Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering. ACM, 826-831.\n\nLearning to forget: Continual prediction with LSTM. J\u00fcrgen Felix A Gers, Fred Schmidhuber, Cummins, Felix A Gers, J\u00fcrgen Schmidhuber, and Fred Cummins. 1999. Learning to forget: Continual prediction with LSTM. (1999).\n\nLearning task-dependent distributed representations by backpropagation through structure. Christoph Goller, Andreas Kuchler, IEEE International Conference on. IEEE1Neural NetworksChristoph Goller and Andreas Kuchler. 1996. Learning task-dependent distributed representations by backpropagation through structure. In Neural Networks, 1996., IEEE International Conference on, Vol. 1. IEEE, 347-352.\n\nGenerative adversarial nets. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, Advances in neural information processing systems. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. In Advances in neural information processing systems. 2672-2680.\n\nDeep code search. Xiaodong Gu, Hongyu Zhang, Sunghun Kim, Proceedings of the 40th International Conference on Software Engineering. the 40th International Conference on Software EngineeringACMXiaodong Gu, Hongyu Zhang, and Sunghun Kim. 2018. Deep code search. In Proceedings of the 40th International Conference on Software Engineering. ACM, 933-944.\n\nAutomatic query reformulations for text retrieval in software engineering. Sonia Haiduc, Gabriele Bavota, Andrian Marcus, Rocco Oliveto, Andrea De Lucia, Tim Menzies, Proceedings of the 2013 International Conference on Software Engineering. the 2013 International Conference on Software EngineeringIEEE PressSonia Haiduc, Gabriele Bavota, Andrian Marcus, Rocco Oliveto, Andrea De Lucia, and Tim Menzies. 2013. Automatic query reformulations for text retrieval in software engineering. In Proceedings of the 2013 International Conference on Software Engineering. IEEE Press, 842-851.\n\nDual learning for machine translation. Di He, Yingce Xia, Tao Qin, Liwei Wang, Nenghai Yu, Tieyan Liu, Wei-Ying Ma, Advances in Neural Information Processing Systems. Di He, Yingce Xia, Tao Qin, Liwei Wang, Nenghai Yu, Tieyan Liu, and Wei- Ying Ma. 2016. Dual learning for machine translation. In Advances in Neural Information Processing Systems. 820-828.\n\nPairwise word interaction modeling with deep neural networks for semantic similarity measurement. Hua He, Jimmy Lin, Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesHua He and Jimmy Lin. 2016. Pairwise word interaction modeling with deep neural networks for semantic similarity measurement. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 937-948.\n\nNLbased query refinement and contextualized code search results: A user study. Emily Hill, Manuel Roldan-Vega, Software Maintenance, Reengineering and Reverse Engineering. CSMR-WCRE2014 Software Evolution Week-IEEE Conference on. IEEEEmily Hill, Manuel Roldan-Vega, Jerry Alan Fails, and Greg Mallet. 2014. NL- based query refinement and contextualized code search results: A user study. In Software Maintenance, Reengineering and Reverse Engineering (CSMR-WCRE), 2014 Software Evolution Week-IEEE Conference on. IEEE, 34-43.\n\nLong short-term memory. Sepp Hochreiter, J\u00fcrgen Schmidhuber, Neural computation. 9Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural computation 9, 8 (1997), 1735-1780.\n\nDeep Code Comment Generation. Xing Hu, Ge Li, Xin Xia, David Lo, Zhi Jin, Proceedings of the 2017 26th IEEE/ACM International Conference on Program Comprehension. the 2017 26th IEEE/ACM International Conference on Program ComprehensionACMXing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2018. Deep Code Comment Generation. In Proceedings of the 2017 26th IEEE/ACM International Conference on Program Comprehension. ACM.\n\nSummarizing Source Code with Transferred API Knowledge. Xing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu, Zhi Jin, 10.24963/ijcai.2018/314Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18. International Joint Conferences on Artificial Intelligence Organization. the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18. International Joint Conferences on Artificial Intelligence OrganizationXing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu, and Zhi Jin. 2018. Summarizing Source Code with Transferred API Knowledge. In Proceedings of the Twenty- Seventh International Joint Conference on Artificial Intelligence, IJCAI-18. Inter- national Joint Conferences on Artificial Intelligence Organization, 2269-2275. https://doi.org/10.24963/ijcai.2018/314\n\nSummarizing source code using a neural attention model. Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, Luke Zettlemoyer, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational Linguistics1Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016. Summarizing source code using a neural attention model. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Vol. 1. 2073-2083.\n\nAutomatically generating commit messages from diffs using neural machine translation. Siyuan Jiang, Ameer Armaly, Collin Mcmillan, Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering. the 32nd IEEE/ACM International Conference on Automated Software EngineeringIEEE PressSiyuan Jiang, Ameer Armaly, and Collin McMillan. 2017. Automatically generat- ing commit messages from diffs using neural machine translation. In Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering. IEEE Press, 135-146.\n\nSpotting working code examples. Iman Keivanloo, Juergen Rilling, Ying Zou, Proceedings of the 36th International Conference on Software Engineering. the 36th International Conference on Software EngineeringACMIman Keivanloo, Juergen Rilling, and Ying Zou. 2014. Spotting working code ex- amples. In Proceedings of the 36th International Conference on Software Engineering. ACM, 664-675.\n\nConvolutional neural networks for sentence classification. Yoon Kim, arXiv:1408.5882arXiv preprintYoon Kim. 2014. Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882 (2014).\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980arXiv preprintDiederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti- mization. arXiv preprint arXiv:1412.6980 (2014).\n\nNeural Network Models for Paraphrase Identification. Wuwei Lan, Wei Xu, arXiv:1806.04330Semantic Textual Similarity, Natural Language Inference, and Question Answering. arXiv preprintWuwei Lan and Wei Xu. 2018. Neural Network Models for Paraphrase Identifi- cation, Semantic Textual Similarity, Natural Language Inference, and Question Answering. arXiv preprint arXiv:1806.04330 (2018).\n\nDeep reinforcement learning for dialogue generation. Jiwei Li, Will Monroe, Alan Ritter, Michel Galley, Jianfeng Gao, Dan Jurafsky, arXiv:1606.01541arXiv preprintJiwei Li, Will Monroe, Alan Ritter, Michel Galley, Jianfeng Gao, and Dan Jurafsky. 2016. Deep reinforcement learning for dialogue generation. arXiv preprint arXiv:1606.01541 (2016).\n\nVisual question generation as dual task of visual question answering. Yikang Li, Nan Duan, Bolei Zhou, Xiao Chu, Wanli Ouyang, Xiaogang Wang, Ming Zhou, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionYikang Li, Nan Duan, Bolei Zhou, Xiao Chu, Wanli Ouyang, Xiaogang Wang, and Ming Zhou. 2018. Visual question generation as dual task of visual question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 6116-6124.\n\nParaphrase Generation with Deep Reinforcement Learning. Zichao Li, Xin Jiang, Lifeng Shang, Hang Li, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational LinguisticsZichao Li, Xin Jiang, Lifeng Shang, and Hang Li. 2018. Paraphrase Generation with Deep Reinforcement Learning. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 3865-3878.\n\nHow not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation. Chia-Wei Liu, Ryan Lowe, V Iulian, Michael Serban, Laurent Noseworthy, Joelle Charlin, Pineau, arXiv:1603.08023arXiv preprintChia-Wei Liu, Ryan Lowe, Iulian V Serban, Michael Noseworthy, Laurent Charlin, and Joelle Pineau. 2016. How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation. arXiv preprint arXiv:1603.08023 (2016).\n\nA neural architecture for generating natural language descriptions from source code changes. Pablo Loyola, Edison Marrese-Taylor, Yutaka Matsuo, arXiv:1704.04856arXiv preprintPablo Loyola, Edison Marrese-Taylor, and Yutaka Matsuo. 2017. A neural archi- tecture for generating natural language descriptions from source code changes. arXiv preprint arXiv:1704.04856 (2017).\n\nQuery expansion via wordnet for effective code search. Meili Lu, Xiaobing Sun, Shaowei Wang, David Lo, Yucong Duan, Software Analysis, Evolution and Reengineering (SANER). IEEE 22nd International Conference on. IEEEMeili Lu, Xiaobing Sun, Shaowei Wang, David Lo, and Yucong Duan. 2015. Query expansion via wordnet for effective code search. In Software Analysis, Evolution and Reengineering (SANER), 2015 IEEE 22nd International Conference on. IEEE, 545-549.\n\nEffective approaches to attention-based neural machine translation. Minh-Thang Luong, Hieu Pham, Christopher D Manning, arXiv:1508.04025arXiv preprintMinh-Thang Luong, Hieu Pham, and Christopher D Manning. 2015. Effec- tive approaches to attention-based neural machine translation. arXiv preprint arXiv:1508.04025 (2015).\n\nExemplar: A source code search engine for finding highly relevant applications. Collin Mcmillan, Mark Grechanik, Denys Poshyvanyk, Chen Fu, Qing Xie, IEEE Transactions on Software Engineering. 38Collin McMillan, Mark Grechanik, Denys Poshyvanyk, Chen Fu, and Qing Xie. 2012. Exemplar: A source code search engine for finding highly relevant applica- tions. IEEE Transactions on Software Engineering 38, 5 (2012), 1069-1087.\n\nRecurrent neural networks: design and applications. Larry Medsker, C Lakhmi, Jain, CRC pressLarry Medsker and Lakhmi C Jain. 1999. Recurrent neural networks: design and applications. CRC press.\n\nAsynchronous methods for deep reinforcement learning. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu, International conference on machine learning. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Tim- othy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. 2016. Asyn- chronous methods for deep reinforcement learning. In International conference on machine learning. 1928-1937.\n\nAutomatic generation of natural language summaries for java classes. Laura Moreno, Jairo Aponte, Giriprasad Sridhara, Andrian Marcus, Lori Pollock, K Vijay-Shanker, IEEE 21st International Conference on. IEEE. Program Comprehension (ICPC)Laura Moreno, Jairo Aponte, Giriprasad Sridhara, Andrian Marcus, Lori Pollock, and K Vijay-Shanker. 2013. Automatic generation of natural language summaries for java classes. In Program Comprehension (ICPC), 2013 IEEE 21st International Conference on. IEEE, 23-32.\n\nNatural language models for predicting programming comments. Dana Movshovitz, -Attias William W Cohen, Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics. the 51st Annual Meeting of the Association for Computational Linguistics2Dana Movshovitz-Attias and William W Cohen. 2013. Natural language models for predicting programming comments. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), Vol. 2. 35-40.\n\nReinforcement Learning for Bandit Neural Machine Translation with Simulated Human Feedback. Khanh Nguyen, Hal Daum\u00e9, Iii , Jordan Boyd-Graber, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingKhanh Nguyen, Hal Daum\u00e9 III, and Jordan Boyd-Graber. 2017. Reinforcement Learning for Bandit Neural Machine Translation with Simulated Human Feedback. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. 1464-1474.\n\nAmanda Cercas Curry, and Verena Rieser. Jekaterina Novikova, Ond\u0159ej Du\u0161ek, arXiv:1707.06875Why we need new evaluation metrics for nlg. arXiv preprintJekaterina Novikova, Ond\u0159ej Du\u0161ek, Amanda Cercas Curry, and Verena Rieser. 2017. Why we need new evaluation metrics for nlg. arXiv preprint arXiv:1707.06875 (2017).\n\nBLEU: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting on association for computational linguistics. the 40th annual meeting on association for computational linguisticsAssociation for Computational LinguisticsKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics. Association for Computational Linguistics, 311-318.\n\nAutomatic differentiation in PyTorch. Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary Devito, Zeming Lin, Alban Desmaison, Luca Antiga, Adam Lerer, Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. 2017. Automatic differentiation in PyTorch. (2017).\n\nA deep reinforced model for abstractive summarization. Romain Paulus, Caiming Xiong, Richard Socher, arXiv:1705.04304arXiv preprintRomain Paulus, Caiming Xiong, and Richard Socher. 2017. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304 (2017).\n\nAurelio Marc, Sumit Ranzato, Michael Chopra, Wojciech Auli, Zaremba, arXiv:1511.06732Sequence level training with recurrent neural networks. arXiv preprintMarc'Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. 2015. Sequence level training with recurrent neural networks. arXiv preprint arXiv:1511.06732 (2015).\n\nSelf-critical sequence training for image captioning. J Steven, Etienne Rennie, Youssef Marcheret, Jarret Mroueh, Vaibhava Ross, Goel, CVPR. 1Steven J Rennie, Etienne Marcheret, Youssef Mroueh, Jarret Ross, and Vaibhava Goel. 2017. Self-critical sequence training for image captioning. In CVPR, Vol. 1. 3.\n\nAre loss functions all the same?. Lorenzo Rosasco, Ernesto De Vito, Andrea Caponnetto, Michele Piana, Alessandro Verri, Neural Computation. 16Lorenzo Rosasco, Ernesto De Vito, Andrea Caponnetto, Michele Piana, and Alessandro Verri. 2004. Are loss functions all the same? Neural Computation 16, 5 (2004), 1063-1076.\n\nQuery association surrogates for web search. Falk Scholer, Hugh E Williams, Andrew Turpin, Journal of the American Society for Information Science and Technology. 55Falk Scholer, Hugh E Williams, and Andrew Turpin. 2004. Query association surrogates for web search. Journal of the American Society for Information Science and Technology 55, 7 (2004), 637-650.\n\nBidirectional recurrent neural networks. Mike Schuster, K Kuldip, Paliwal, IEEE Transactions on Signal Processing. 45Mike Schuster and Kuldip K Paliwal. 1997. Bidirectional recurrent neural net- works. IEEE Transactions on Signal Processing 45, 11 (1997), 2673-2681.\n\nTowards automatically generating summary comments for java methods. Giriprasad Sridhara, Emily Hill, Divya Muppaneni, Lori Pollock, Vijay-Shanker, Proceedings of the IEEE/ACM international conference on Automated software engineering. the IEEE/ACM international conference on Automated software engineeringACMGiriprasad Sridhara, Emily Hill, Divya Muppaneni, Lori Pollock, and K Vijay- Shanker. 2010. Towards automatically generating summary comments for java methods. In Proceedings of the IEEE/ACM international conference on Automated software engineering. ACM, 43-52.\n\nDropout: A simple way to prevent neural networks from overfitting. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov, The Journal of Machine Learning Research. 15Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research 15, 1 (2014), 1929-1958.\n\n. Stack Overflow, Stack Overflow. 2018. Stack Overflow. https://stackoverflow.com/.\n\nSequence to sequence learning with neural networks. Ilya Sutskever, Oriol Vinyals, Quoc V Le, Advances in neural information processing systems. Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In Advances in neural information processing systems. 3104- 3112.\n\nIntroduction to reinforcement learning. S Richard, Andrew G Sutton, Barto, MIT press Cambridge135Richard S Sutton and Andrew G Barto. 1998. Introduction to reinforcement learning. Vol. 135. MIT press Cambridge.\n\nPolicy gradient methods for reinforcement learning with function approximation. S Richard, David A Sutton, Mcallester, P Satinder, Yishay Singh, Mansour, Advances in neural information processing systems. Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. 2000. Policy gradient methods for reinforcement learning with function approximation. In Advances in neural information processing systems. 1057-1063.\n\nQuestion answering and question generation as dual tasks. Duyu Tang, Nan Duan, Tao Qin, Zhao Yan, Ming Zhou, arXiv:1706.02027arXiv preprintDuyu Tang, Nan Duan, Tao Qin, Zhao Yan, and Ming Zhou. 2017. Question answering and question generation as dual tasks. arXiv preprint arXiv:1706.02027 (2017).\n\nANNE: Improving Source Code Search Using Entity Retrieval Approach. Venkatesh Vinayakarao, Anita Sarma, Rahul Purandare, Shuktika Jain, Saumya Jain, 10.1145/3018661.3018691Proceedings of the Tenth ACM International Conference on Web Search and Data Mining (WSDM '17). the Tenth ACM International Conference on Web Search and Data Mining (WSDM '17)New York, NY, USAACMVenkatesh Vinayakarao, Anita Sarma, Rahul Purandare, Shuktika Jain, and Saumya Jain. 2017. ANNE: Improving Source Code Search Using Entity Re- trieval Approach. In Proceedings of the Tenth ACM International Conference on Web Search and Data Mining (WSDM '17). ACM, New York, NY, USA, 211-220. https://doi.org/10.1145/3018661.3018691\n\nThe TREC-8 Question Answering Track Report. M Ellen, Voorhees, Trec. 99Ellen M Voorhees et al. 1999. The TREC-8 Question Answering Track Report.. In Trec, Vol. 99. 77-82.\n\nImproving automatic source code summarization via deep reinforcement learning. Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, Philip S Yu, Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering. the 33rd ACM/IEEE International Conference on Automated Software EngineeringACMYao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, and Philip S Yu. 2018. Improving automatic source code summarization via deep rein- forcement learning. In Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering. ACM, 397-407.\n\nIrgan: A minimax game for unifying generative and discriminative information retrieval models. Jun Wang, Lantao Yu, Weinan Zhang, Yu Gong, Yinghui Xu, Benyou Wang, Peng Zhang, Dell Zhang, Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval. the 40th International ACM SIGIR conference on Research and Development in Information RetrievalACMJun Wang, Lantao Yu, Weinan Zhang, Yu Gong, Yinghui Xu, Benyou Wang, Peng Zhang, and Dell Zhang. 2017. Irgan: A minimax game for unifying generative and discriminative information retrieval models. In Proceedings of the 40th In- ternational ACM SIGIR conference on Research and Development in Information Retrieval. ACM, 515-524.\n\nComment Generation for Source Code: State of the Art. Xiaoran Wang, Yifan Peng, Benwen Zhang, arXiv:1802.02971Challenges and Opportunities. arXiv preprintXiaoran Wang, Yifan Peng, and Benwen Zhang. 2018. Comment Generation for Source Code: State of the Art, Challenges and Opportunities. arXiv preprint arXiv:1802.02971 (2018).\n\nWikipedia contributors. 2019. Google Image Labeler -Wikipedia, The Free Encyclopedia. Online; accessed 4-February-2019Wikipedia contributors. 2019. Google Image Labeler -Wikipedia, The Free Ency- clopedia. https://en.wikipedia.org/w/index.php?title=Google_Image_Labeler& oldid=881738511 [Online; accessed 4-February-2019].\n\nSimple statistical gradient-following algorithms for connectionist reinforcement learning. J Ronald, Williams, Machine learning. 8Ronald J Williams. 1992. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning 8, 3-4 (1992), 229-256.\n\nZiyu Yao, S Daniel, Wei-Peng Weld, Huan Chen, Sun, arXiv:1803.09371StaQC: A Systematically Mined Question-Code Dataset from Stack Overflow. arXiv preprintZiyu Yao, Daniel S Weld, Wei-Peng Chen, and Huan Sun. 2018. StaQC: A Sys- tematically Mined Question-Code Dataset from Stack Overflow. arXiv preprint arXiv:1803.09371 (2018).\n\nWojciech Zaremba, arXiv:1409.2329Ilya Sutskever, and Oriol Vinyals. 2014. Recurrent neural network regularization. arXiv preprintWojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. 2014. Recurrent neural network regularization. arXiv preprint arXiv:1409.2329 (2014).\n", "annotations": {"author": "[{\"end\":178,\"start\":89},{\"end\":287,\"start\":179},{\"end\":377,\"start\":288}]", "publisher": null, "author_last_name": "[{\"end\":97,\"start\":94},{\"end\":206,\"start\":191},{\"end\":296,\"start\":293}]", "author_first_name": "[{\"end\":93,\"start\":89},{\"end\":190,\"start\":179},{\"end\":292,\"start\":288}]", "author_affiliation": "[{\"end\":177,\"start\":99},{\"end\":286,\"start\":208},{\"end\":376,\"start\":298}]", "title": "[{\"end\":71,\"start\":1},{\"end\":448,\"start\":378}]", "venue": null, "abstract": "[{\"end\":2275,\"start\":968}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2342,\"start\":2339},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2706,\"start\":2703},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2709,\"start\":2706},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2712,\"start\":2709},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2715,\"start\":2712},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":2718,\"start\":2715},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2945,\"start\":2942},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2948,\"start\":2945},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2951,\"start\":2948},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2954,\"start\":2951},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":2957,\"start\":2954},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":2960,\"start\":2957},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3118,\"start\":3114},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3121,\"start\":3118},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3124,\"start\":3121},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3127,\"start\":3124},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3224,\"start\":3221},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":3227,\"start\":3224},{\"end\":3384,\"start\":3352},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3450,\"start\":3446},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3690,\"start\":3686},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3709,\"start\":3705},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":4024,\"start\":4020},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":4139,\"start\":4135},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":4632,\"start\":4628},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":4956,\"start\":4952},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":4978,\"start\":4974},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6511,\"start\":6507},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6515,\"start\":6511},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6519,\"start\":6515},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":6522,\"start\":6519},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7114,\"start\":7110},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7322,\"start\":7318},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8188,\"start\":8185},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":8287,\"start\":8283},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8339,\"start\":8335},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8989,\"start\":8985},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":9039,\"start\":9035},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9646,\"start\":9642},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9649,\"start\":9646},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":9652,\"start\":9649},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10955,\"start\":10952},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10958,\"start\":10955},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":11527,\"start\":11523},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":11535,\"start\":11531},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":11538,\"start\":11535},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11959,\"start\":11956},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":11962,\"start\":11959},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11999,\"start\":11995},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12002,\"start\":11999},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12005,\"start\":12002},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":12008,\"start\":12005},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12442,\"start\":12438},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":12445,\"start\":12442},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":12448,\"start\":12445},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":12451,\"start\":12448},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12757,\"start\":12753},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12761,\"start\":12757},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":12765,\"start\":12761},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":12767,\"start\":12765},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14937,\"start\":14934},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":14993,\"start\":14989},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":15050,\"start\":15046},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":16498,\"start\":16494},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":17043,\"start\":17039},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":17046,\"start\":17043},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":17049,\"start\":17046},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":17052,\"start\":17049},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":17055,\"start\":17052},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":17058,\"start\":17055},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":18408,\"start\":18404},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":18899,\"start\":18895},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":19179,\"start\":19175},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":19448,\"start\":19444},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":20031,\"start\":20027},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":20964,\"start\":20960},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":21483,\"start\":21479},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":22136,\"start\":22133},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":22139,\"start\":22136},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":22142,\"start\":22139},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":22294,\"start\":22291},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":22297,\"start\":22294},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":22300,\"start\":22297},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":22853,\"start\":22849},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":22951,\"start\":22948},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":24055,\"start\":24051},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":24381,\"start\":24378},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":24899,\"start\":24895},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":25597,\"start\":25593},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":25624,\"start\":25621},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":25694,\"start\":25690},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":25733,\"start\":25730},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":25736,\"start\":25733},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":25924,\"start\":25920},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":26056,\"start\":26052},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":26120,\"start\":26116},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":26123,\"start\":26120},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":26173,\"start\":26169},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":26528,\"start\":26524},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":26837,\"start\":26833},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":26840,\"start\":26837},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":26843,\"start\":26840},{\"end\":28648,\"start\":28646},{\"end\":28746,\"start\":28744},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":28792,\"start\":28788},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":28796,\"start\":28794},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":29010,\"start\":29006},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":29013,\"start\":29010},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":29671,\"start\":29667},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":29773,\"start\":29769},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":30139,\"start\":30135},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":30520,\"start\":30516},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":30966,\"start\":30965},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":30981,\"start\":30977},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":31147,\"start\":31143},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":31369,\"start\":31366},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":31799,\"start\":31795},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":31835,\"start\":31831},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":32031,\"start\":32030},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":32101,\"start\":32098},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":32104,\"start\":32101},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":32107,\"start\":32104},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":32222,\"start\":32218},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":32764,\"start\":32760},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":32793,\"start\":32789},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":33221,\"start\":33217},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":34022,\"start\":34019},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":34025,\"start\":34022},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":34028,\"start\":34025},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":34212,\"start\":34208},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":34292,\"start\":34288},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":34567,\"start\":34563},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":35111,\"start\":35107},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":35283,\"start\":35279},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":35483,\"start\":35479},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":35993,\"start\":35989},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":36207,\"start\":36203},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":41198,\"start\":41194},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":41201,\"start\":41198},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":41409,\"start\":41405},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":41434,\"start\":41431},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":41437,\"start\":41434},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":43283,\"start\":43279},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":45255,\"start\":45251},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":45258,\"start\":45255},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":45261,\"start\":45258},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":45264,\"start\":45261},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":45267,\"start\":45264},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":45303,\"start\":45300},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":45306,\"start\":45303},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":45309,\"start\":45306},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":45346,\"start\":45342},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":45618,\"start\":45614},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":46243,\"start\":46239},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":46523,\"start\":46519},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":46526,\"start\":46523},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":46556,\"start\":46552},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":46588,\"start\":46585},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":46592,\"start\":46588},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":46596,\"start\":46592},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":46600,\"start\":46596},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":46604,\"start\":46600},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":46607,\"start\":46604},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":46686,\"start\":46682},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":46862,\"start\":46859},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":47092,\"start\":47088},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":47101,\"start\":47097},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":47683,\"start\":47679},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":47914,\"start\":47911},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":47917,\"start\":47914},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":47920,\"start\":47917},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":47943,\"start\":47939},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":47969,\"start\":47965},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":47997,\"start\":47993},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":48065,\"start\":48062},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":48068,\"start\":48065},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":48071,\"start\":48068},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":48179,\"start\":48175},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":48251,\"start\":48247},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":48463,\"start\":48460},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":48681,\"start\":48677},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":48711,\"start\":48707},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":48731,\"start\":48727},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":48856,\"start\":48852},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":49271,\"start\":49267},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":49274,\"start\":49271},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":49277,\"start\":49274},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":49280,\"start\":49277},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":49283,\"start\":49280},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":49320,\"start\":49316},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":49557,\"start\":49553},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":49706,\"start\":49702},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":49850,\"start\":49847},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":49871,\"start\":49867},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":51394,\"start\":51390},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":53991,\"start\":53988},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":53994,\"start\":53991},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":54131,\"start\":54130},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":54190,\"start\":54186},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":54256,\"start\":54252},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":54423,\"start\":54419},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":54486,\"start\":54482}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":50894,\"start\":50857},{\"attributes\":{\"id\":\"fig_1\"},\"end\":51094,\"start\":50895},{\"attributes\":{\"id\":\"fig_2\"},\"end\":51395,\"start\":51095},{\"attributes\":{\"id\":\"fig_3\"},\"end\":51554,\"start\":51396},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":51990,\"start\":51555},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":52716,\"start\":51991},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":53974,\"start\":52717}]", "paragraph": "[{\"end\":2921,\"start\":2277},{\"end\":4252,\"start\":2923},{\"end\":5632,\"start\":4254},{\"end\":6947,\"start\":5634},{\"end\":8948,\"start\":6949},{\"end\":9465,\"start\":8950},{\"end\":9520,\"start\":9467},{\"end\":10851,\"start\":9522},{\"end\":11396,\"start\":10866},{\"end\":11567,\"start\":11398},{\"end\":11691,\"start\":11569},{\"end\":11858,\"start\":11724},{\"end\":12509,\"start\":11889},{\"end\":12660,\"start\":12532},{\"end\":13623,\"start\":12688},{\"end\":13792,\"start\":13636},{\"end\":15251,\"start\":13794},{\"end\":15819,\"start\":15253},{\"end\":16256,\"start\":15821},{\"end\":16357,\"start\":16295},{\"end\":16687,\"start\":16382},{\"end\":17636,\"start\":16689},{\"end\":17907,\"start\":17829},{\"end\":18216,\"start\":18116},{\"end\":18753,\"start\":18246},{\"end\":18967,\"start\":18838},{\"end\":19368,\"start\":19031},{\"end\":19760,\"start\":19370},{\"end\":19971,\"start\":19832},{\"end\":20257,\"start\":19973},{\"end\":20456,\"start\":20283},{\"end\":20895,\"start\":20518},{\"end\":21207,\"start\":20897},{\"end\":21560,\"start\":21264},{\"end\":21872,\"start\":21679},{\"end\":22032,\"start\":21922},{\"end\":22987,\"start\":22114},{\"end\":23406,\"start\":22989},{\"end\":23570,\"start\":23408},{\"end\":23917,\"start\":23572},{\"end\":24427,\"start\":23990},{\"end\":24621,\"start\":24429},{\"end\":25032,\"start\":24686},{\"end\":25204,\"start\":25082},{\"end\":25262,\"start\":25206},{\"end\":25518,\"start\":25412},{\"end\":26211,\"start\":25566},{\"end\":27351,\"start\":26367},{\"end\":27603,\"start\":27398},{\"end\":27717,\"start\":27605},{\"end\":27860,\"start\":27796},{\"end\":28581,\"start\":27905},{\"end\":28683,\"start\":28588},{\"end\":28861,\"start\":28690},{\"end\":29065,\"start\":28863},{\"end\":29182,\"start\":29067},{\"end\":29430,\"start\":29249},{\"end\":29568,\"start\":29446},{\"end\":30498,\"start\":29591},{\"end\":31109,\"start\":30500},{\"end\":31629,\"start\":31111},{\"end\":32303,\"start\":31644},{\"end\":32451,\"start\":32335},{\"end\":32603,\"start\":32474},{\"end\":32733,\"start\":32605},{\"end\":32855,\"start\":32735},{\"end\":33660,\"start\":32857},{\"end\":33977,\"start\":33662},{\"end\":34547,\"start\":33979},{\"end\":34707,\"start\":34549},{\"end\":35041,\"start\":34709},{\"end\":36240,\"start\":35068},{\"end\":36686,\"start\":36242},{\"end\":36926,\"start\":36698},{\"end\":37261,\"start\":36928},{\"end\":38552,\"start\":37263},{\"end\":39562,\"start\":38595},{\"end\":40689,\"start\":39564},{\"end\":40817,\"start\":40691},{\"end\":41443,\"start\":40819},{\"end\":42252,\"start\":41452},{\"end\":42565,\"start\":42254},{\"end\":43474,\"start\":42567},{\"end\":43887,\"start\":43476},{\"end\":44601,\"start\":43902},{\"end\":44957,\"start\":44603},{\"end\":45116,\"start\":44959},{\"end\":46358,\"start\":45133},{\"end\":47596,\"start\":46360},{\"end\":49091,\"start\":47598},{\"end\":50203,\"start\":49093},{\"end\":50856,\"start\":50218}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11723,\"start\":11692},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11888,\"start\":11859},{\"attributes\":{\"id\":\"formula_2\"},\"end\":17828,\"start\":17637},{\"attributes\":{\"id\":\"formula_3\"},\"end\":18115,\"start\":17908},{\"attributes\":{\"id\":\"formula_4\"},\"end\":18245,\"start\":18217},{\"attributes\":{\"id\":\"formula_5\"},\"end\":18837,\"start\":18754},{\"attributes\":{\"id\":\"formula_6\"},\"end\":19030,\"start\":18968},{\"attributes\":{\"id\":\"formula_7\"},\"end\":19831,\"start\":19761},{\"attributes\":{\"id\":\"formula_8\"},\"end\":20517,\"start\":20457},{\"attributes\":{\"id\":\"formula_9\"},\"end\":21263,\"start\":21208},{\"attributes\":{\"id\":\"formula_10\"},\"end\":21678,\"start\":21561},{\"attributes\":{\"id\":\"formula_11\"},\"end\":21921,\"start\":21873},{\"attributes\":{\"id\":\"formula_12\"},\"end\":23989,\"start\":23918},{\"attributes\":{\"id\":\"formula_13\"},\"end\":24685,\"start\":24622},{\"attributes\":{\"id\":\"formula_14\"},\"end\":25081,\"start\":25033},{\"attributes\":{\"id\":\"formula_15\"},\"end\":25411,\"start\":25263},{\"attributes\":{\"id\":\"formula_16\"},\"end\":26277,\"start\":26212},{\"attributes\":{\"id\":\"formula_17\"},\"end\":26366,\"start\":26277},{\"attributes\":{\"id\":\"formula_18\"},\"end\":27397,\"start\":27352},{\"attributes\":{\"id\":\"formula_19\"},\"end\":27795,\"start\":27718},{\"attributes\":{\"id\":\"formula_20\"},\"end\":29248,\"start\":29183},{\"attributes\":{\"id\":\"formula_21\"},\"end\":32334,\"start\":32304}]", "table_ref": "[{\"end\":37298,\"start\":37291},{\"end\":38459,\"start\":38452},{\"end\":39840,\"start\":39789},{\"end\":40151,\"start\":40143},{\"end\":40421,\"start\":40414},{\"end\":41842,\"start\":41835},{\"end\":43331,\"start\":43324}]", "section_header": "[{\"attributes\":{\"n\":\"2\"},\"end\":10864,\"start\":10854},{\"attributes\":{\"n\":\"3\"},\"end\":12530,\"start\":12512},{\"attributes\":{\"n\":\"3.1\"},\"end\":12686,\"start\":12663},{\"attributes\":{\"n\":\"3.2\"},\"end\":13634,\"start\":13626},{\"attributes\":{\"n\":\"4\"},\"end\":16293,\"start\":16259},{\"attributes\":{\"n\":\"4.1\"},\"end\":16380,\"start\":16360},{\"attributes\":{\"n\":\"4.2\"},\"end\":20281,\"start\":20260},{\"attributes\":{\"n\":\"4.3\"},\"end\":22066,\"start\":22035},{\"attributes\":{\"n\":\"4.3.1\"},\"end\":22112,\"start\":22069},{\"attributes\":{\"n\":\"4.3.2\"},\"end\":25564,\"start\":25521},{\"attributes\":{\"n\":\"4.4\"},\"end\":27903,\"start\":27863},{\"end\":28586,\"start\":28584},{\"end\":28688,\"start\":28686},{\"attributes\":{\"n\":\"5\"},\"end\":29444,\"start\":29433},{\"attributes\":{\"n\":\"5.1\"},\"end\":29589,\"start\":29571},{\"attributes\":{\"n\":\"5.2\"},\"end\":31642,\"start\":31632},{\"attributes\":{\"n\":\"5.3\"},\"end\":32472,\"start\":32454},{\"attributes\":{\"n\":\"5.4\"},\"end\":35066,\"start\":35044},{\"attributes\":{\"n\":\"5.5\"},\"end\":36696,\"start\":36689},{\"attributes\":{\"n\":\"5.5.2\"},\"end\":38593,\"start\":38555},{\"end\":41450,\"start\":41446},{\"attributes\":{\"n\":\"6\"},\"end\":43900,\"start\":43890},{\"attributes\":{\"n\":\"7\"},\"end\":45131,\"start\":45119},{\"attributes\":{\"n\":\"8\"},\"end\":50216,\"start\":50206},{\"end\":50906,\"start\":50896},{\"end\":51099,\"start\":51096}]", "table": "[{\"end\":51990,\"start\":51601},{\"end\":52716,\"start\":52074},{\"end\":53974,\"start\":52996}]", "figure_caption": "[{\"end\":50894,\"start\":50859},{\"end\":51094,\"start\":50908},{\"end\":51395,\"start\":51101},{\"end\":51554,\"start\":51398},{\"end\":51601,\"start\":51557},{\"end\":52074,\"start\":51993},{\"end\":52996,\"start\":52719}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":5398,\"start\":5390},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":7664,\"start\":7656},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":13791,\"start\":13783},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":16709,\"start\":16701},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18435,\"start\":18427},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":27936,\"start\":27928},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":32832,\"start\":32824},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":33204,\"start\":33195}]", "bib_author_first_name": "[{\"end\":55184,\"start\":55175},{\"end\":55197,\"start\":55196},{\"end\":55213,\"start\":55204},{\"end\":55227,\"start\":55220},{\"end\":55550,\"start\":55541},{\"end\":55565,\"start\":55562},{\"end\":55579,\"start\":55572},{\"end\":55891,\"start\":55885},{\"end\":55909,\"start\":55903},{\"end\":55924,\"start\":55918},{\"end\":55935,\"start\":55933},{\"end\":56227,\"start\":56220},{\"end\":56246,\"start\":56238},{\"end\":56261,\"start\":56255},{\"end\":56273,\"start\":56266},{\"end\":56285,\"start\":56281},{\"end\":56298,\"start\":56292},{\"end\":56312,\"start\":56307},{\"end\":56330,\"start\":56324},{\"end\":56627,\"start\":56620},{\"end\":56842,\"start\":56841},{\"end\":56854,\"start\":56848},{\"end\":56856,\"start\":56855},{\"end\":56876,\"start\":56870},{\"end\":56878,\"start\":56877},{\"end\":57115,\"start\":57109},{\"end\":57128,\"start\":57122},{\"end\":57749,\"start\":57741},{\"end\":57763,\"start\":57756},{\"end\":58216,\"start\":58210},{\"end\":58235,\"start\":58231},{\"end\":58476,\"start\":58467},{\"end\":58492,\"start\":58485},{\"end\":58807,\"start\":58804},{\"end\":58824,\"start\":58820},{\"end\":58845,\"start\":58840},{\"end\":58857,\"start\":58853},{\"end\":58867,\"start\":58862},{\"end\":58889,\"start\":58882},{\"end\":58902,\"start\":58897},{\"end\":58920,\"start\":58914},{\"end\":59236,\"start\":59228},{\"end\":59247,\"start\":59241},{\"end\":59262,\"start\":59255},{\"end\":59642,\"start\":59637},{\"end\":59659,\"start\":59651},{\"end\":59675,\"start\":59668},{\"end\":59689,\"start\":59684},{\"end\":59705,\"start\":59699},{\"end\":59708,\"start\":59706},{\"end\":59719,\"start\":59716},{\"end\":60187,\"start\":60185},{\"end\":60198,\"start\":60192},{\"end\":60207,\"start\":60204},{\"end\":60218,\"start\":60213},{\"end\":60232,\"start\":60225},{\"end\":60243,\"start\":60237},{\"end\":60257,\"start\":60249},{\"end\":60605,\"start\":60602},{\"end\":60615,\"start\":60610},{\"end\":61259,\"start\":61254},{\"end\":61272,\"start\":61266},{\"end\":61730,\"start\":61726},{\"end\":61749,\"start\":61743},{\"end\":61932,\"start\":61928},{\"end\":61939,\"start\":61937},{\"end\":61947,\"start\":61944},{\"end\":61958,\"start\":61953},{\"end\":61966,\"start\":61963},{\"end\":62378,\"start\":62374},{\"end\":62385,\"start\":62383},{\"end\":62393,\"start\":62390},{\"end\":62404,\"start\":62399},{\"end\":62414,\"start\":62409},{\"end\":62422,\"start\":62419},{\"end\":63206,\"start\":63196},{\"end\":63220,\"start\":63213},{\"end\":63235,\"start\":63230},{\"end\":63248,\"start\":63244},{\"end\":63784,\"start\":63778},{\"end\":63797,\"start\":63792},{\"end\":63812,\"start\":63806},{\"end\":64299,\"start\":64295},{\"end\":64318,\"start\":64311},{\"end\":64332,\"start\":64328},{\"end\":64714,\"start\":64710},{\"end\":64909,\"start\":64908},{\"end\":64925,\"start\":64920},{\"end\":65149,\"start\":65144},{\"end\":65158,\"start\":65155},{\"end\":65537,\"start\":65532},{\"end\":65546,\"start\":65542},{\"end\":65559,\"start\":65555},{\"end\":65574,\"start\":65568},{\"end\":65591,\"start\":65583},{\"end\":65600,\"start\":65597},{\"end\":65900,\"start\":65894},{\"end\":65908,\"start\":65905},{\"end\":65920,\"start\":65915},{\"end\":65931,\"start\":65927},{\"end\":65942,\"start\":65937},{\"end\":65959,\"start\":65951},{\"end\":65970,\"start\":65966},{\"end\":66437,\"start\":66431},{\"end\":66445,\"start\":66442},{\"end\":66459,\"start\":66453},{\"end\":66471,\"start\":66467},{\"end\":67071,\"start\":67063},{\"end\":67081,\"start\":67077},{\"end\":67089,\"start\":67088},{\"end\":67105,\"start\":67098},{\"end\":67121,\"start\":67114},{\"end\":67140,\"start\":67134},{\"end\":67561,\"start\":67556},{\"end\":67576,\"start\":67570},{\"end\":67599,\"start\":67593},{\"end\":67896,\"start\":67891},{\"end\":67909,\"start\":67901},{\"end\":67922,\"start\":67915},{\"end\":67934,\"start\":67929},{\"end\":67945,\"start\":67939},{\"end\":68374,\"start\":68364},{\"end\":68386,\"start\":68382},{\"end\":68406,\"start\":68393},{\"end\":68705,\"start\":68699},{\"end\":68720,\"start\":68716},{\"end\":68737,\"start\":68732},{\"end\":68754,\"start\":68750},{\"end\":68763,\"start\":68759},{\"end\":69101,\"start\":69096},{\"end\":69112,\"start\":69111},{\"end\":69302,\"start\":69293},{\"end\":69314,\"start\":69309},{\"end\":69327,\"start\":69315},{\"end\":69340,\"start\":69335},{\"end\":69352,\"start\":69348},{\"end\":69368,\"start\":69361},{\"end\":69383,\"start\":69380},{\"end\":69397,\"start\":69392},{\"end\":69411,\"start\":69406},{\"end\":69806,\"start\":69801},{\"end\":69820,\"start\":69815},{\"end\":69839,\"start\":69829},{\"end\":69857,\"start\":69850},{\"end\":69870,\"start\":69866},{\"end\":69881,\"start\":69880},{\"end\":70301,\"start\":70297},{\"end\":70321,\"start\":70314},{\"end\":70842,\"start\":70837},{\"end\":70854,\"start\":70851},{\"end\":70865,\"start\":70862},{\"end\":70874,\"start\":70868},{\"end\":71351,\"start\":71341},{\"end\":71368,\"start\":71362},{\"end\":71687,\"start\":71680},{\"end\":71703,\"start\":71698},{\"end\":71716,\"start\":71712},{\"end\":71731,\"start\":71723},{\"end\":72245,\"start\":72241},{\"end\":72257,\"start\":72254},{\"end\":72272,\"start\":72265},{\"end\":72290,\"start\":72283},{\"end\":72305,\"start\":72299},{\"end\":72319,\"start\":72312},{\"end\":72334,\"start\":72328},{\"end\":72345,\"start\":72340},{\"end\":72361,\"start\":72357},{\"end\":72374,\"start\":72370},{\"end\":72641,\"start\":72635},{\"end\":72657,\"start\":72650},{\"end\":72672,\"start\":72665},{\"end\":72870,\"start\":72863},{\"end\":72882,\"start\":72877},{\"end\":72899,\"start\":72892},{\"end\":72916,\"start\":72908},{\"end\":73248,\"start\":73247},{\"end\":73264,\"start\":73257},{\"end\":73280,\"start\":73273},{\"end\":73298,\"start\":73292},{\"end\":73315,\"start\":73307},{\"end\":73541,\"start\":73534},{\"end\":73558,\"start\":73551},{\"end\":73561,\"start\":73559},{\"end\":73574,\"start\":73568},{\"end\":73594,\"start\":73587},{\"end\":73612,\"start\":73602},{\"end\":73865,\"start\":73861},{\"end\":73879,\"start\":73875},{\"end\":73881,\"start\":73880},{\"end\":73898,\"start\":73892},{\"end\":74222,\"start\":74218},{\"end\":74234,\"start\":74233},{\"end\":74523,\"start\":74513},{\"end\":74539,\"start\":74534},{\"end\":74551,\"start\":74546},{\"end\":74567,\"start\":74563},{\"end\":75091,\"start\":75085},{\"end\":75112,\"start\":75104},{\"end\":75125,\"start\":75121},{\"end\":75142,\"start\":75138},{\"end\":75160,\"start\":75154},{\"end\":75462,\"start\":75457},{\"end\":75596,\"start\":75592},{\"end\":75613,\"start\":75608},{\"end\":75629,\"start\":75623},{\"end\":75897,\"start\":75896},{\"end\":75913,\"start\":75907},{\"end\":75915,\"start\":75914},{\"end\":76149,\"start\":76148},{\"end\":76164,\"start\":76159},{\"end\":76166,\"start\":76165},{\"end\":76188,\"start\":76187},{\"end\":76205,\"start\":76199},{\"end\":76563,\"start\":76559},{\"end\":76573,\"start\":76570},{\"end\":76583,\"start\":76580},{\"end\":76593,\"start\":76589},{\"end\":76603,\"start\":76599},{\"end\":76877,\"start\":76868},{\"end\":76896,\"start\":76891},{\"end\":76909,\"start\":76904},{\"end\":76929,\"start\":76921},{\"end\":76942,\"start\":76936},{\"end\":77546,\"start\":77545},{\"end\":77755,\"start\":77752},{\"end\":77765,\"start\":77761},{\"end\":77775,\"start\":77772},{\"end\":77790,\"start\":77782},{\"end\":77802,\"start\":77795},{\"end\":77813,\"start\":77809},{\"end\":77826,\"start\":77818},{\"end\":78382,\"start\":78379},{\"end\":78395,\"start\":78389},{\"end\":78406,\"start\":78400},{\"end\":78416,\"start\":78414},{\"end\":78430,\"start\":78423},{\"end\":78441,\"start\":78435},{\"end\":78452,\"start\":78448},{\"end\":78464,\"start\":78460},{\"end\":79076,\"start\":79069},{\"end\":79088,\"start\":79083},{\"end\":79101,\"start\":79095},{\"end\":79760,\"start\":79759},{\"end\":79960,\"start\":79956},{\"end\":79967,\"start\":79966},{\"end\":79984,\"start\":79976},{\"end\":79995,\"start\":79991},{\"end\":80294,\"start\":80286}]", "bib_author_last_name": "[{\"end\":55194,\"start\":55185},{\"end\":55202,\"start\":55198},{\"end\":55218,\"start\":55214},{\"end\":55235,\"start\":55228},{\"end\":55243,\"start\":55237},{\"end\":55560,\"start\":55551},{\"end\":55570,\"start\":55566},{\"end\":55586,\"start\":55580},{\"end\":55901,\"start\":55892},{\"end\":55916,\"start\":55910},{\"end\":55931,\"start\":55925},{\"end\":55939,\"start\":55936},{\"end\":56236,\"start\":56228},{\"end\":56253,\"start\":56247},{\"end\":56264,\"start\":56262},{\"end\":56279,\"start\":56274},{\"end\":56290,\"start\":56286},{\"end\":56305,\"start\":56299},{\"end\":56322,\"start\":56313},{\"end\":56337,\"start\":56331},{\"end\":56635,\"start\":56628},{\"end\":56846,\"start\":56843},{\"end\":56868,\"start\":56857},{\"end\":56888,\"start\":56879},{\"end\":56897,\"start\":56890},{\"end\":57120,\"start\":57116},{\"end\":57134,\"start\":57129},{\"end\":57539,\"start\":57521},{\"end\":57547,\"start\":57541},{\"end\":57754,\"start\":57750},{\"end\":57768,\"start\":57764},{\"end\":58229,\"start\":58217},{\"end\":58247,\"start\":58236},{\"end\":58256,\"start\":58249},{\"end\":58483,\"start\":58477},{\"end\":58500,\"start\":58493},{\"end\":58818,\"start\":58808},{\"end\":58838,\"start\":58825},{\"end\":58851,\"start\":58846},{\"end\":58860,\"start\":58858},{\"end\":58880,\"start\":58868},{\"end\":58895,\"start\":58890},{\"end\":58912,\"start\":58903},{\"end\":58927,\"start\":58921},{\"end\":59239,\"start\":59237},{\"end\":59253,\"start\":59248},{\"end\":59266,\"start\":59263},{\"end\":59649,\"start\":59643},{\"end\":59666,\"start\":59660},{\"end\":59682,\"start\":59676},{\"end\":59697,\"start\":59690},{\"end\":59714,\"start\":59709},{\"end\":59727,\"start\":59720},{\"end\":60190,\"start\":60188},{\"end\":60202,\"start\":60199},{\"end\":60211,\"start\":60208},{\"end\":60223,\"start\":60219},{\"end\":60235,\"start\":60233},{\"end\":60247,\"start\":60244},{\"end\":60260,\"start\":60258},{\"end\":60608,\"start\":60606},{\"end\":60619,\"start\":60616},{\"end\":61264,\"start\":61260},{\"end\":61284,\"start\":61273},{\"end\":61741,\"start\":61731},{\"end\":61761,\"start\":61750},{\"end\":61935,\"start\":61933},{\"end\":61942,\"start\":61940},{\"end\":61951,\"start\":61948},{\"end\":61961,\"start\":61959},{\"end\":61970,\"start\":61967},{\"end\":62381,\"start\":62379},{\"end\":62388,\"start\":62386},{\"end\":62397,\"start\":62394},{\"end\":62407,\"start\":62405},{\"end\":62417,\"start\":62415},{\"end\":62426,\"start\":62423},{\"end\":63211,\"start\":63207},{\"end\":63228,\"start\":63221},{\"end\":63242,\"start\":63236},{\"end\":63260,\"start\":63249},{\"end\":63790,\"start\":63785},{\"end\":63804,\"start\":63798},{\"end\":63821,\"start\":63813},{\"end\":64309,\"start\":64300},{\"end\":64326,\"start\":64319},{\"end\":64336,\"start\":64333},{\"end\":64718,\"start\":64715},{\"end\":64918,\"start\":64910},{\"end\":64932,\"start\":64926},{\"end\":64936,\"start\":64934},{\"end\":65153,\"start\":65150},{\"end\":65161,\"start\":65159},{\"end\":65540,\"start\":65538},{\"end\":65553,\"start\":65547},{\"end\":65566,\"start\":65560},{\"end\":65581,\"start\":65575},{\"end\":65595,\"start\":65592},{\"end\":65609,\"start\":65601},{\"end\":65903,\"start\":65901},{\"end\":65913,\"start\":65909},{\"end\":65925,\"start\":65921},{\"end\":65935,\"start\":65932},{\"end\":65949,\"start\":65943},{\"end\":65964,\"start\":65960},{\"end\":65975,\"start\":65971},{\"end\":66440,\"start\":66438},{\"end\":66451,\"start\":66446},{\"end\":66465,\"start\":66460},{\"end\":66474,\"start\":66472},{\"end\":67075,\"start\":67072},{\"end\":67086,\"start\":67082},{\"end\":67096,\"start\":67090},{\"end\":67112,\"start\":67106},{\"end\":67132,\"start\":67122},{\"end\":67148,\"start\":67141},{\"end\":67156,\"start\":67150},{\"end\":67568,\"start\":67562},{\"end\":67591,\"start\":67577},{\"end\":67606,\"start\":67600},{\"end\":67899,\"start\":67897},{\"end\":67913,\"start\":67910},{\"end\":67927,\"start\":67923},{\"end\":67937,\"start\":67935},{\"end\":67950,\"start\":67946},{\"end\":68380,\"start\":68375},{\"end\":68391,\"start\":68387},{\"end\":68414,\"start\":68407},{\"end\":68714,\"start\":68706},{\"end\":68730,\"start\":68721},{\"end\":68748,\"start\":68738},{\"end\":68757,\"start\":68755},{\"end\":68767,\"start\":68764},{\"end\":69109,\"start\":69102},{\"end\":69119,\"start\":69113},{\"end\":69125,\"start\":69121},{\"end\":69307,\"start\":69303},{\"end\":69333,\"start\":69328},{\"end\":69346,\"start\":69341},{\"end\":69359,\"start\":69353},{\"end\":69378,\"start\":69369},{\"end\":69390,\"start\":69384},{\"end\":69404,\"start\":69398},{\"end\":69423,\"start\":69412},{\"end\":69813,\"start\":69807},{\"end\":69827,\"start\":69821},{\"end\":69848,\"start\":69840},{\"end\":69864,\"start\":69858},{\"end\":69878,\"start\":69871},{\"end\":69895,\"start\":69882},{\"end\":70312,\"start\":70302},{\"end\":70337,\"start\":70322},{\"end\":70849,\"start\":70843},{\"end\":70860,\"start\":70855},{\"end\":70886,\"start\":70875},{\"end\":71360,\"start\":71352},{\"end\":71374,\"start\":71369},{\"end\":71696,\"start\":71688},{\"end\":71710,\"start\":71704},{\"end\":71721,\"start\":71717},{\"end\":71735,\"start\":71732},{\"end\":72252,\"start\":72246},{\"end\":72263,\"start\":72258},{\"end\":72281,\"start\":72273},{\"end\":72297,\"start\":72291},{\"end\":72310,\"start\":72306},{\"end\":72326,\"start\":72320},{\"end\":72338,\"start\":72335},{\"end\":72355,\"start\":72346},{\"end\":72368,\"start\":72362},{\"end\":72380,\"start\":72375},{\"end\":72648,\"start\":72642},{\"end\":72663,\"start\":72658},{\"end\":72679,\"start\":72673},{\"end\":72875,\"start\":72871},{\"end\":72890,\"start\":72883},{\"end\":72906,\"start\":72900},{\"end\":72921,\"start\":72917},{\"end\":72930,\"start\":72923},{\"end\":73255,\"start\":73249},{\"end\":73271,\"start\":73265},{\"end\":73290,\"start\":73281},{\"end\":73305,\"start\":73299},{\"end\":73320,\"start\":73316},{\"end\":73326,\"start\":73322},{\"end\":73549,\"start\":73542},{\"end\":73566,\"start\":73562},{\"end\":73585,\"start\":73575},{\"end\":73600,\"start\":73595},{\"end\":73618,\"start\":73613},{\"end\":73873,\"start\":73866},{\"end\":73890,\"start\":73882},{\"end\":73905,\"start\":73899},{\"end\":74231,\"start\":74223},{\"end\":74241,\"start\":74235},{\"end\":74250,\"start\":74243},{\"end\":74532,\"start\":74524},{\"end\":74544,\"start\":74540},{\"end\":74561,\"start\":74552},{\"end\":74575,\"start\":74568},{\"end\":74590,\"start\":74577},{\"end\":75102,\"start\":75092},{\"end\":75119,\"start\":75113},{\"end\":75136,\"start\":75126},{\"end\":75152,\"start\":75143},{\"end\":75174,\"start\":75161},{\"end\":75471,\"start\":75463},{\"end\":75606,\"start\":75597},{\"end\":75621,\"start\":75614},{\"end\":75632,\"start\":75630},{\"end\":75905,\"start\":75898},{\"end\":75922,\"start\":75916},{\"end\":75929,\"start\":75924},{\"end\":76157,\"start\":76150},{\"end\":76173,\"start\":76167},{\"end\":76185,\"start\":76175},{\"end\":76197,\"start\":76189},{\"end\":76211,\"start\":76206},{\"end\":76220,\"start\":76213},{\"end\":76568,\"start\":76564},{\"end\":76578,\"start\":76574},{\"end\":76587,\"start\":76584},{\"end\":76597,\"start\":76594},{\"end\":76608,\"start\":76604},{\"end\":76889,\"start\":76878},{\"end\":76902,\"start\":76897},{\"end\":76919,\"start\":76910},{\"end\":76934,\"start\":76930},{\"end\":76947,\"start\":76943},{\"end\":77552,\"start\":77547},{\"end\":77562,\"start\":77554},{\"end\":77759,\"start\":77756},{\"end\":77770,\"start\":77766},{\"end\":77780,\"start\":77776},{\"end\":77793,\"start\":77791},{\"end\":77807,\"start\":77803},{\"end\":77816,\"start\":77814},{\"end\":77829,\"start\":77827},{\"end\":78387,\"start\":78383},{\"end\":78398,\"start\":78396},{\"end\":78412,\"start\":78407},{\"end\":78421,\"start\":78417},{\"end\":78433,\"start\":78431},{\"end\":78446,\"start\":78442},{\"end\":78458,\"start\":78453},{\"end\":78470,\"start\":78465},{\"end\":79081,\"start\":79077},{\"end\":79093,\"start\":79089},{\"end\":79107,\"start\":79102},{\"end\":79767,\"start\":79761},{\"end\":79777,\"start\":79769},{\"end\":79964,\"start\":79961},{\"end\":79974,\"start\":79968},{\"end\":79989,\"start\":79985},{\"end\":80000,\"start\":79996},{\"end\":80005,\"start\":80002},{\"end\":80302,\"start\":80295}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":207591052},\"end\":55463,\"start\":55116},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":2723946},\"end\":55828,\"start\":55465},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":2706277},\"end\":56167,\"start\":55830},{\"attributes\":{\"doi\":\"arXiv:1607.07086\",\"id\":\"b3\"},\"end\":56588,\"start\":56169},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":123329493},\"end\":56781,\"start\":56590},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":14535341},\"end\":57071,\"start\":56783},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":1438450},\"end\":57517,\"start\":57073},{\"attributes\":{\"id\":\"b7\"},\"end\":57672,\"start\":57519},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":52068576},\"end\":58156,\"start\":57674},{\"attributes\":{\"id\":\"b9\"},\"end\":58375,\"start\":58158},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":6536466},\"end\":58773,\"start\":58377},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":1033682},\"end\":59208,\"start\":58775},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":47021242},\"end\":59560,\"start\":59210},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":8969778},\"end\":60144,\"start\":59562},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":5758868},\"end\":60502,\"start\":60146},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":16787742},\"end\":61173,\"start\":60504},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":7894522},\"end\":61700,\"start\":61175},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":1915014},\"end\":61896,\"start\":61702},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":49584534},\"end\":62316,\"start\":61898},{\"attributes\":{\"doi\":\"10.24963/ijcai.2018/314\",\"id\":\"b19\",\"matched_paper_id\":49584957},\"end\":63138,\"start\":62318},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":8820379},\"end\":63690,\"start\":63140},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":9237290},\"end\":64261,\"start\":63692},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":4612234},\"end\":64649,\"start\":64263},{\"attributes\":{\"doi\":\"arXiv:1408.5882\",\"id\":\"b23\"},\"end\":64862,\"start\":64651},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b24\"},\"end\":65089,\"start\":64864},{\"attributes\":{\"doi\":\"arXiv:1806.04330\",\"id\":\"b25\"},\"end\":65477,\"start\":65091},{\"attributes\":{\"doi\":\"arXiv:1606.01541\",\"id\":\"b26\"},\"end\":65822,\"start\":65479},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":3184591},\"end\":66373,\"start\":65824},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":21646317},\"end\":66931,\"start\":66375},{\"attributes\":{\"doi\":\"arXiv:1603.08023\",\"id\":\"b29\"},\"end\":67461,\"start\":66933},{\"attributes\":{\"doi\":\"arXiv:1704.04856\",\"id\":\"b30\"},\"end\":67834,\"start\":67463},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":10991311},\"end\":68294,\"start\":67836},{\"attributes\":{\"doi\":\"arXiv:1508.04025\",\"id\":\"b32\"},\"end\":68617,\"start\":68296},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":5752985},\"end\":69042,\"start\":68619},{\"attributes\":{\"id\":\"b34\"},\"end\":69237,\"start\":69044},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":6875312},\"end\":69730,\"start\":69239},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":1129667},\"end\":70234,\"start\":69732},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":6706547},\"end\":70743,\"start\":70236},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":215824512},\"end\":71299,\"start\":70745},{\"attributes\":{\"doi\":\"arXiv:1707.06875\",\"id\":\"b39\"},\"end\":71614,\"start\":71301},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":11080756},\"end\":72201,\"start\":71616},{\"attributes\":{\"id\":\"b41\"},\"end\":72578,\"start\":72203},{\"attributes\":{\"doi\":\"arXiv:1705.04304\",\"id\":\"b42\"},\"end\":72861,\"start\":72580},{\"attributes\":{\"doi\":\"arXiv:1511.06732\",\"id\":\"b43\"},\"end\":73191,\"start\":72863},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":206594923},\"end\":73498,\"start\":73193},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":11845688},\"end\":73814,\"start\":73500},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":28395428},\"end\":74175,\"start\":73816},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":18375389},\"end\":74443,\"start\":74177},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":9790585},\"end\":75016,\"start\":74445},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":6844431},\"end\":75453,\"start\":75018},{\"attributes\":{\"id\":\"b50\"},\"end\":75538,\"start\":75455},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":7961699},\"end\":75854,\"start\":75540},{\"attributes\":{\"id\":\"b52\"},\"end\":76066,\"start\":75856},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":1211821},\"end\":76499,\"start\":76068},{\"attributes\":{\"doi\":\"arXiv:1706.02027\",\"id\":\"b54\"},\"end\":76798,\"start\":76501},{\"attributes\":{\"doi\":\"10.1145/3018661.3018691\",\"id\":\"b55\",\"matched_paper_id\":5437729},\"end\":77499,\"start\":76800},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":16944215},\"end\":77671,\"start\":77501},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":52069701},\"end\":78282,\"start\":77673},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":3331356},\"end\":79013,\"start\":78284},{\"attributes\":{\"doi\":\"arXiv:1802.02971\",\"id\":\"b59\",\"matched_paper_id\":3649647},\"end\":79342,\"start\":79015},{\"attributes\":{\"id\":\"b60\"},\"end\":79666,\"start\":79344},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":2332513},\"end\":79954,\"start\":79668},{\"attributes\":{\"doi\":\"arXiv:1803.09371\",\"id\":\"b62\"},\"end\":80284,\"start\":79956},{\"attributes\":{\"doi\":\"arXiv:1409.2329\",\"id\":\"b63\"},\"end\":80553,\"start\":80286}]", "bib_title": "[{\"end\":55173,\"start\":55116},{\"end\":55539,\"start\":55465},{\"end\":55883,\"start\":55830},{\"end\":56618,\"start\":56590},{\"end\":56839,\"start\":56783},{\"end\":57107,\"start\":57073},{\"end\":57739,\"start\":57674},{\"end\":58465,\"start\":58377},{\"end\":58802,\"start\":58775},{\"end\":59226,\"start\":59210},{\"end\":59635,\"start\":59562},{\"end\":60183,\"start\":60146},{\"end\":60600,\"start\":60504},{\"end\":61252,\"start\":61175},{\"end\":61724,\"start\":61702},{\"end\":61926,\"start\":61898},{\"end\":62372,\"start\":62318},{\"end\":63194,\"start\":63140},{\"end\":63776,\"start\":63692},{\"end\":64293,\"start\":64263},{\"end\":65142,\"start\":65091},{\"end\":65892,\"start\":65824},{\"end\":66429,\"start\":66375},{\"end\":67889,\"start\":67836},{\"end\":68697,\"start\":68619},{\"end\":69291,\"start\":69239},{\"end\":69799,\"start\":69732},{\"end\":70295,\"start\":70236},{\"end\":70835,\"start\":70745},{\"end\":71339,\"start\":71301},{\"end\":71678,\"start\":71616},{\"end\":73245,\"start\":73193},{\"end\":73532,\"start\":73500},{\"end\":73859,\"start\":73816},{\"end\":74216,\"start\":74177},{\"end\":74511,\"start\":74445},{\"end\":75083,\"start\":75018},{\"end\":75590,\"start\":75540},{\"end\":76146,\"start\":76068},{\"end\":76866,\"start\":76800},{\"end\":77543,\"start\":77501},{\"end\":77750,\"start\":77673},{\"end\":78377,\"start\":78284},{\"end\":79067,\"start\":79015},{\"end\":79757,\"start\":79668}]", "bib_author": "[{\"end\":55196,\"start\":55175},{\"end\":55204,\"start\":55196},{\"end\":55220,\"start\":55204},{\"end\":55237,\"start\":55220},{\"end\":55245,\"start\":55237},{\"end\":55562,\"start\":55541},{\"end\":55572,\"start\":55562},{\"end\":55588,\"start\":55572},{\"end\":55903,\"start\":55885},{\"end\":55918,\"start\":55903},{\"end\":55933,\"start\":55918},{\"end\":55941,\"start\":55933},{\"end\":56238,\"start\":56220},{\"end\":56255,\"start\":56238},{\"end\":56266,\"start\":56255},{\"end\":56281,\"start\":56266},{\"end\":56292,\"start\":56281},{\"end\":56307,\"start\":56292},{\"end\":56324,\"start\":56307},{\"end\":56339,\"start\":56324},{\"end\":56637,\"start\":56620},{\"end\":56848,\"start\":56841},{\"end\":56870,\"start\":56848},{\"end\":56890,\"start\":56870},{\"end\":56899,\"start\":56890},{\"end\":57122,\"start\":57109},{\"end\":57136,\"start\":57122},{\"end\":57541,\"start\":57521},{\"end\":57549,\"start\":57541},{\"end\":57756,\"start\":57741},{\"end\":57770,\"start\":57756},{\"end\":58231,\"start\":58210},{\"end\":58249,\"start\":58231},{\"end\":58258,\"start\":58249},{\"end\":58485,\"start\":58467},{\"end\":58502,\"start\":58485},{\"end\":58820,\"start\":58804},{\"end\":58840,\"start\":58820},{\"end\":58853,\"start\":58840},{\"end\":58862,\"start\":58853},{\"end\":58882,\"start\":58862},{\"end\":58897,\"start\":58882},{\"end\":58914,\"start\":58897},{\"end\":58929,\"start\":58914},{\"end\":59241,\"start\":59228},{\"end\":59255,\"start\":59241},{\"end\":59268,\"start\":59255},{\"end\":59651,\"start\":59637},{\"end\":59668,\"start\":59651},{\"end\":59684,\"start\":59668},{\"end\":59699,\"start\":59684},{\"end\":59716,\"start\":59699},{\"end\":59729,\"start\":59716},{\"end\":60192,\"start\":60185},{\"end\":60204,\"start\":60192},{\"end\":60213,\"start\":60204},{\"end\":60225,\"start\":60213},{\"end\":60237,\"start\":60225},{\"end\":60249,\"start\":60237},{\"end\":60262,\"start\":60249},{\"end\":60610,\"start\":60602},{\"end\":60621,\"start\":60610},{\"end\":61266,\"start\":61254},{\"end\":61286,\"start\":61266},{\"end\":61743,\"start\":61726},{\"end\":61763,\"start\":61743},{\"end\":61937,\"start\":61928},{\"end\":61944,\"start\":61937},{\"end\":61953,\"start\":61944},{\"end\":61963,\"start\":61953},{\"end\":61972,\"start\":61963},{\"end\":62383,\"start\":62374},{\"end\":62390,\"start\":62383},{\"end\":62399,\"start\":62390},{\"end\":62409,\"start\":62399},{\"end\":62419,\"start\":62409},{\"end\":62428,\"start\":62419},{\"end\":63213,\"start\":63196},{\"end\":63230,\"start\":63213},{\"end\":63244,\"start\":63230},{\"end\":63262,\"start\":63244},{\"end\":63792,\"start\":63778},{\"end\":63806,\"start\":63792},{\"end\":63823,\"start\":63806},{\"end\":64311,\"start\":64295},{\"end\":64328,\"start\":64311},{\"end\":64338,\"start\":64328},{\"end\":64720,\"start\":64710},{\"end\":64920,\"start\":64908},{\"end\":64934,\"start\":64920},{\"end\":64938,\"start\":64934},{\"end\":65155,\"start\":65144},{\"end\":65163,\"start\":65155},{\"end\":65542,\"start\":65532},{\"end\":65555,\"start\":65542},{\"end\":65568,\"start\":65555},{\"end\":65583,\"start\":65568},{\"end\":65597,\"start\":65583},{\"end\":65611,\"start\":65597},{\"end\":65905,\"start\":65894},{\"end\":65915,\"start\":65905},{\"end\":65927,\"start\":65915},{\"end\":65937,\"start\":65927},{\"end\":65951,\"start\":65937},{\"end\":65966,\"start\":65951},{\"end\":65977,\"start\":65966},{\"end\":66442,\"start\":66431},{\"end\":66453,\"start\":66442},{\"end\":66467,\"start\":66453},{\"end\":66476,\"start\":66467},{\"end\":67077,\"start\":67063},{\"end\":67088,\"start\":67077},{\"end\":67098,\"start\":67088},{\"end\":67114,\"start\":67098},{\"end\":67134,\"start\":67114},{\"end\":67150,\"start\":67134},{\"end\":67158,\"start\":67150},{\"end\":67570,\"start\":67556},{\"end\":67593,\"start\":67570},{\"end\":67608,\"start\":67593},{\"end\":67901,\"start\":67891},{\"end\":67915,\"start\":67901},{\"end\":67929,\"start\":67915},{\"end\":67939,\"start\":67929},{\"end\":67952,\"start\":67939},{\"end\":68382,\"start\":68364},{\"end\":68393,\"start\":68382},{\"end\":68416,\"start\":68393},{\"end\":68716,\"start\":68699},{\"end\":68732,\"start\":68716},{\"end\":68750,\"start\":68732},{\"end\":68759,\"start\":68750},{\"end\":68769,\"start\":68759},{\"end\":69111,\"start\":69096},{\"end\":69121,\"start\":69111},{\"end\":69127,\"start\":69121},{\"end\":69309,\"start\":69293},{\"end\":69335,\"start\":69309},{\"end\":69348,\"start\":69335},{\"end\":69361,\"start\":69348},{\"end\":69380,\"start\":69361},{\"end\":69392,\"start\":69380},{\"end\":69406,\"start\":69392},{\"end\":69425,\"start\":69406},{\"end\":69815,\"start\":69801},{\"end\":69829,\"start\":69815},{\"end\":69850,\"start\":69829},{\"end\":69866,\"start\":69850},{\"end\":69880,\"start\":69866},{\"end\":69897,\"start\":69880},{\"end\":70314,\"start\":70297},{\"end\":70339,\"start\":70314},{\"end\":70851,\"start\":70837},{\"end\":70862,\"start\":70851},{\"end\":70868,\"start\":70862},{\"end\":70888,\"start\":70868},{\"end\":71362,\"start\":71341},{\"end\":71376,\"start\":71362},{\"end\":71698,\"start\":71680},{\"end\":71712,\"start\":71698},{\"end\":71723,\"start\":71712},{\"end\":71737,\"start\":71723},{\"end\":72254,\"start\":72241},{\"end\":72265,\"start\":72254},{\"end\":72283,\"start\":72265},{\"end\":72299,\"start\":72283},{\"end\":72312,\"start\":72299},{\"end\":72328,\"start\":72312},{\"end\":72340,\"start\":72328},{\"end\":72357,\"start\":72340},{\"end\":72370,\"start\":72357},{\"end\":72382,\"start\":72370},{\"end\":72650,\"start\":72635},{\"end\":72665,\"start\":72650},{\"end\":72681,\"start\":72665},{\"end\":72877,\"start\":72863},{\"end\":72892,\"start\":72877},{\"end\":72908,\"start\":72892},{\"end\":72923,\"start\":72908},{\"end\":72932,\"start\":72923},{\"end\":73257,\"start\":73247},{\"end\":73273,\"start\":73257},{\"end\":73292,\"start\":73273},{\"end\":73307,\"start\":73292},{\"end\":73322,\"start\":73307},{\"end\":73328,\"start\":73322},{\"end\":73551,\"start\":73534},{\"end\":73568,\"start\":73551},{\"end\":73587,\"start\":73568},{\"end\":73602,\"start\":73587},{\"end\":73620,\"start\":73602},{\"end\":73875,\"start\":73861},{\"end\":73892,\"start\":73875},{\"end\":73907,\"start\":73892},{\"end\":74233,\"start\":74218},{\"end\":74243,\"start\":74233},{\"end\":74252,\"start\":74243},{\"end\":74534,\"start\":74513},{\"end\":74546,\"start\":74534},{\"end\":74563,\"start\":74546},{\"end\":74577,\"start\":74563},{\"end\":74592,\"start\":74577},{\"end\":75104,\"start\":75085},{\"end\":75121,\"start\":75104},{\"end\":75138,\"start\":75121},{\"end\":75154,\"start\":75138},{\"end\":75176,\"start\":75154},{\"end\":75473,\"start\":75457},{\"end\":75608,\"start\":75592},{\"end\":75623,\"start\":75608},{\"end\":75634,\"start\":75623},{\"end\":75907,\"start\":75896},{\"end\":75924,\"start\":75907},{\"end\":75931,\"start\":75924},{\"end\":76159,\"start\":76148},{\"end\":76175,\"start\":76159},{\"end\":76187,\"start\":76175},{\"end\":76199,\"start\":76187},{\"end\":76213,\"start\":76199},{\"end\":76222,\"start\":76213},{\"end\":76570,\"start\":76559},{\"end\":76580,\"start\":76570},{\"end\":76589,\"start\":76580},{\"end\":76599,\"start\":76589},{\"end\":76610,\"start\":76599},{\"end\":76891,\"start\":76868},{\"end\":76904,\"start\":76891},{\"end\":76921,\"start\":76904},{\"end\":76936,\"start\":76921},{\"end\":76949,\"start\":76936},{\"end\":77554,\"start\":77545},{\"end\":77564,\"start\":77554},{\"end\":77761,\"start\":77752},{\"end\":77772,\"start\":77761},{\"end\":77782,\"start\":77772},{\"end\":77795,\"start\":77782},{\"end\":77809,\"start\":77795},{\"end\":77818,\"start\":77809},{\"end\":77831,\"start\":77818},{\"end\":78389,\"start\":78379},{\"end\":78400,\"start\":78389},{\"end\":78414,\"start\":78400},{\"end\":78423,\"start\":78414},{\"end\":78435,\"start\":78423},{\"end\":78448,\"start\":78435},{\"end\":78460,\"start\":78448},{\"end\":78472,\"start\":78460},{\"end\":79083,\"start\":79069},{\"end\":79095,\"start\":79083},{\"end\":79109,\"start\":79095},{\"end\":79769,\"start\":79759},{\"end\":79779,\"start\":79769},{\"end\":79966,\"start\":79956},{\"end\":79976,\"start\":79966},{\"end\":79991,\"start\":79976},{\"end\":80002,\"start\":79991},{\"end\":80007,\"start\":80002},{\"end\":80304,\"start\":80286}]", "bib_venue": "[{\"end\":55273,\"start\":55245},{\"end\":55632,\"start\":55588},{\"end\":55985,\"start\":55941},{\"end\":56218,\"start\":56169},{\"end\":56673,\"start\":56637},{\"end\":56910,\"start\":56899},{\"end\":57212,\"start\":57136},{\"end\":57574,\"start\":57549},{\"end\":57861,\"start\":57770},{\"end\":58208,\"start\":58158},{\"end\":58534,\"start\":58502},{\"end\":58978,\"start\":58929},{\"end\":59340,\"start\":59268},{\"end\":59801,\"start\":59729},{\"end\":60311,\"start\":60262},{\"end\":60763,\"start\":60621},{\"end\":61345,\"start\":61286},{\"end\":61781,\"start\":61763},{\"end\":62059,\"start\":61972},{\"end\":62625,\"start\":62451},{\"end\":63349,\"start\":63262},{\"end\":63914,\"start\":63823},{\"end\":64410,\"start\":64338},{\"end\":64708,\"start\":64651},{\"end\":64906,\"start\":64864},{\"end\":65258,\"start\":65179},{\"end\":65530,\"start\":65479},{\"end\":66054,\"start\":65977},{\"end\":66562,\"start\":66476},{\"end\":67061,\"start\":66933},{\"end\":67554,\"start\":67463},{\"end\":68006,\"start\":67952},{\"end\":68362,\"start\":68296},{\"end\":68810,\"start\":68769},{\"end\":69094,\"start\":69044},{\"end\":69469,\"start\":69425},{\"end\":69940,\"start\":69897},{\"end\":70426,\"start\":70339},{\"end\":70974,\"start\":70888},{\"end\":71434,\"start\":71392},{\"end\":71820,\"start\":71737},{\"end\":72239,\"start\":72203},{\"end\":72633,\"start\":72580},{\"end\":73002,\"start\":72948},{\"end\":73332,\"start\":73328},{\"end\":73638,\"start\":73620},{\"end\":73977,\"start\":73907},{\"end\":74290,\"start\":74252},{\"end\":74678,\"start\":74592},{\"end\":75216,\"start\":75176},{\"end\":75683,\"start\":75634},{\"end\":75894,\"start\":75856},{\"end\":76271,\"start\":76222},{\"end\":76557,\"start\":76501},{\"end\":77066,\"start\":76972},{\"end\":77568,\"start\":77564},{\"end\":77922,\"start\":77831},{\"end\":78583,\"start\":78472},{\"end\":79153,\"start\":79125},{\"end\":79428,\"start\":79344},{\"end\":79795,\"start\":79779},{\"end\":80094,\"start\":80023},{\"end\":80399,\"start\":80319},{\"end\":57275,\"start\":57214},{\"end\":57939,\"start\":57863},{\"end\":59399,\"start\":59342},{\"end\":59860,\"start\":59803},{\"end\":60892,\"start\":60765},{\"end\":62133,\"start\":62061},{\"end\":62786,\"start\":62627},{\"end\":63423,\"start\":63351},{\"end\":63992,\"start\":63916},{\"end\":64469,\"start\":64412},{\"end\":66118,\"start\":66056},{\"end\":66635,\"start\":66564},{\"end\":70500,\"start\":70428},{\"end\":71047,\"start\":70976},{\"end\":71890,\"start\":71822},{\"end\":74751,\"start\":74680},{\"end\":77164,\"start\":77068},{\"end\":78000,\"start\":77924},{\"end\":78681,\"start\":78585}]"}}}, "year": 2023, "month": 12, "day": 17}