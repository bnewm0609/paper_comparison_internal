{"id": 253098270, "updated": "2023-10-06 02:25:18.941", "metadata": {"title": "Efficient (Soft) Q-Learning for Text Generation with Limited Good Data", "authors": "[{\"first\":\"Han\",\"last\":\"Guo\",\"middle\":[]},{\"first\":\"Bowen\",\"last\":\"Tan\",\"middle\":[]},{\"first\":\"Zhengzhong\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Eric\",\"last\":\"Xing\",\"middle\":[\"P.\"]},{\"first\":\"Zhiting\",\"last\":\"Hu\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Maximum likelihood estimation (MLE) is the predominant algorithm for training text generation models. This paradigm relies on direct supervision examples, which is not applicable to many emerging applications, such as generating adversarial attacks or generating prompts to control language models. Reinforcement learning (RL) on the other hand offers a more flexible solution by allowing users to plug in arbitrary task metrics as reward. Yet previous RL algorithms for text generation, such as policy gradient (on-policy RL) and Q-learning (off-policy RL), are often notoriously inefficient or unstable to train due to the large sequence space and the sparse reward received only at the end of sequences. In this paper, we introduce a new RL formulation for text generation from the soft Q-learning (SQL) perspective. It enables us to draw from the latest RL advances, such as path consistency learning, to combine the best of on-/off-policy updates, and learn effectively from sparse reward. We apply the approach to a wide range of novel text generation tasks, including learning from noisy/negative examples, adversarial attacks, and prompt generation. Experiments show our approach consistently outperforms both task-specialized algorithms and the previous RL methods.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2106.07704", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/emnlp/GuoT0XH22", "doi": "10.18653/v1/2022.findings-emnlp.518"}}, "content": {"source": {"pdf_hash": "f3a2bc491bb9c96e14a728de0b0adf9228569ffd", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2106.07704v4.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "b0a75adec5dce5ff69081b0b52d73792a3943a28", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/f3a2bc491bb9c96e14a728de0b0adf9228569ffd.txt", "contents": "\nEfficient (Soft) Q-Learning for Text Generation with Limited Good Data\n\n\nHan Guo hanguo@cs.cmu.edu \nCarnegie Mellon University\n\n\nBowen Tan \nCarnegie Mellon University\n\n\nZhengzhong Liu \nCarnegie Mellon University\n\n\nPetuum Inc\n\n\nEric P Xing epxing@cs.cmu.edu \nCarnegie Mellon University\n\n\nPetuum Inc\n\n\nZayed University of Artificial Intelligence\n\n\nZhiting Hu \nUC San Diego\n\n\nEfficient (Soft) Q-Learning for Text Generation with Limited Good Data\n\nMaximum likelihood estimation (MLE) is the predominant algorithm for training text generation models. This paradigm relies on direct supervision examples, which is not applicable to many emerging applications, such as generating adversarial attacks or generating prompts to control language models. Reinforcement learning (RL) on the other hand offers a more flexible solution by allowing users to plug in arbitrary task metrics as reward. Yet previous RL algorithms for text generation, such as policy gradient (on-policy RL) and Q-learning (off-policy RL), are often notoriously inefficient or unstable to train due to the large sequence space and the sparse reward received only at the end of sequences. In this paper, we introduce a new RL formulation for text generation from the soft Q-learning (SQL) perspective. It enables us to draw from the latest RL advances, such as path consistency learning, to combine the best of on-/off-policy updates, and learn effectively from sparse reward. We apply the approach to a wide range of novel text generation tasks, including learning from noisy/negative examples, adversarial attacks, and prompt generation. Experiments show our approach consistently outperforms both task-specialized algorithms and the previous RL methods. 1\n\nIntroduction\n\nRecent natural language generation systems have made remarkable progress in producing wellformed text, especially with massive pretrained language models. Those models are typically trained using maximum likelihood estimation (MLE) with a large amount of data supervisions. Despite its successes, the standard training method suffers from limited applicability to many emerging text generation problems, where little or no supervised data is available. Prominent examples of such low-data problems include generating prompts to control the massive LMs (Yin et al., 2019;Shin et al., 2020;Zhong et al., 2021;Liu et al., 2021), learning text generation from noisy or even negative data, generating adversarial text attacks for robustness study (Wallace et al., 2019;Atanasova et al., 2020), and others (Figure 1, right). Due to the failure of standard MLE, people have had to devise specialized algorithms for those problems respectively.\n\nReinforcement learning (RL) (Sutton and Barto, 2018) offers an alternative principled framework for learning from arbitrary reward functions. However, RL by far has made limited success for training text generation, primarily due to the key challenges of sparse reward (i.e., a single reward signal is received only after the whole text sequence is generated) and large action space (i.e., a vocabulary of millions of words). For instance, a popular family of RL algorithms studied extensively for text generation is the policy-based (Williams, 1992) or actor-critic based (Bahdanau et al., 2016;Rennie et al., 2017) algorithms, with policy gradient (PG) being the most prevalent example (Ranzato et al., 2016;Li et al., 2016;Rennie et al., 2017;Tan et al., 2018;Pasunuru and Bansal, 2018;Paulus et al., 2018). Those algorithms train the model with on-policy updates, i.e., the text samples used for estimating policy gradients are from the target model itself. Due to the exponentially large space of sequences, on-policy updates often suffer from extremely high variance and low data efficiency (e.g., most model samples are not useful for learning). Thus directly training with PG from scratch is usually impossible. In practice, the model has to be initialized by MLE training, followed by PG as finetuning, which often leads to limited improvement (Choshen et al., 2020;Wu et al., 2018).\n\nAnother set of work has resorted to off-policy RL. The key advantage is that samples from other sources, e.g., human-written text, can be used, making them more data efficient than on-policy meth- ods. Previous work has used either importance weighted PG (Pang and He, 2021;Zhou et al., 2017;Kandasamy et al., 2017) or Q-learning based algorithms (Guo, 2015;Jaques et al., 2020;Narasimhan et al., 2015). However, off-policy methods have been considered to be less stable. For example, the Q-learning performance relies heavily on how accurate the learned Q-function assesses the quality of intermediate subsequences -a challenging task due to the sparse reward signals.\n\nIn this paper, we develop a new RL formulation for text generation that tackles the above issues ( Figure 1, left). We reframe the text generation problem from the soft Q-learning perspective originally developed in robotics (Haarnoja et al., 2017;Schulman et al., 2017). The resulting connection allows us to seamlessly take advantage of the latest successful techniques from the RL literature. In particular, we introduce and adapt the principled path consistency learning (Nachum et al., 2017) to text generation, that (1) offers a natural way to train the model with both on-and off-policy updates, hence combining the best of the two strategies, (2) bridges the sparse reward signal to directly supervise the Q function learning, leading to more accurate Q estimation and credit assignment, and (3) makes efficient updates to Q-values by considering all candidate actions together.\n\nThe generality and efficiency of the proposed method allows us to train text generation in a wide range of applications: (1) With noisy and negative training examples, our approach learns to generate accurate entailment text that greatly improves upon the data itself as well as other various training methods; (2) Our approach also manages to train an effective adversarial text generator for robustness test for classifiers; (3) We train a prompt generator with our algorithm to achieve controllable generation of pretrained LMs in terms of topics. 2 On all the three tasks, our approach consistently improves over not only previous RL algorithms for text generation, but also diverse task-specialized methods designed specifically for each of the problems, respectively. In the appendix ( \u00a7A.1.4), we also show that on standard supervised tasks where MLE prevails, our approach is competitive to train text generation models from scratch, which was usually impossible for previous RL algorithms.\n\nThe contributions can be summarized as follows. On the technical side, we propose a new RL formulation for text generation based on soft Q-Learning. This new formulation allows us to seamlessly take advantage of the RL literature's latest successful techniques (notably the path con-sistency algorithm) to overcome the longstanding challenges (e.g., sparse reward and large action space) in text generation. On the empirical side, we conduct studies on a wide variety of text generation tasks with limited data (i.e., generating from noisy/negative data, adversarial text generation, prompt generation). We propose their RL formulations, and show that our general approach consistently improves over not only previous text RL algorithms, but also diverse task-specialized methods.\n\n\nBackground and Challenges\n\nWe aim to learn a generation model p \u03b8 (y) = T t=0 p \u03b8 (y t |y <t ), where y t is a token from a vocabulary V. The distribution at each step t is obtained by applying softmax on the logits f \u03b8 (y|y <t ):\np \u03b8 (yt|y<t) = exp f \u03b8 (yt|y<t) y \u2208V exp f \u03b8 (y |y<t)\n.\n\n(1)\n\nDespite its popularity, MLE-based training only applies when clean supervised data y * is available, and cannot be used to optimize arbitrary task metrics (e.g., BLEU, entailment score) which are typically the goal in many text generation tasks. Previous research has formulated text generation as an RL problem by considering the following finitetime Markov Decision Process (MDP). At each time step t, let the \"state\" be s t = y <t , namely the partial sequence generated so far. The model (\"agent\") takes as input the current state s t and outputs a token (\"action\") a t \u2208 V according to a policy \u03c0(a t |s t ). The agent then receives a reward r t = r(s t , a t ) and deterministically transitions to next state s t+1 (i.e., the concatenation of the tokens in s t and the new token a t ).\n\nFollowing the notation convention in RL, let \u03c4 be the trajectory (i.e., text sample) generated by the policy. The agent's objective is to maximize the accumulative reward,\nJ(\u03c0) = E \u03c4 \u223c\u03c0 T t=0 \u03b3 t r t ,\nwhere \u03b3 is the discount factor. A central concept in RL is the Q-function of policy \u03c0, Q \u03c0 (s t , a t ) = E \u03c0 T t =t \u03b3 t r t | s t , a t , the expected future reward of taking action a t (i.e., generating token a t ) in state s t and continuing with the policy \u03c0. Challenges. Text generation poses significant challenges to RL, particularly because (1) the reward signal is usually sparse, i.e., r t = 0, \u2200t < T and the agent receives a non-zero reward r T only after it generates the full sequence, (2) the action space (i.e., the vocabulary V) is extremely large.\n\nThe challenges have led to difficulties of the two major families of RL approaches applied to text generation problems, as detailed below. Policy-based RL techniques directly parameterize the policy \u03c0 \u03b8 with parameters \u03b8. Thus the policy \u03c0 \u03b8 (a t |s t ) exactly corresponds to the above generation model p \u03b8 (y t |y <t ). Policy gradient (PG) is one of the most widely used algorithms for text generation (Ranzato et al., 2016). It optimizes the cumulative reward with the policy gradient using the estimated Q \u03c0 \u03b8 value based on sample \u03c4 . PG is an on-policy algorithm, meaning that the sample \u03c4 needs to come from the the current policy \u03c0 \u03b8 itself. In practice, however, optimizing this objective alone from scratch is unlikely going to work because most samples \u03c4 \u223c \u03c0 \u03b8 are just gibberish with zero reward, failing to provide meaningful training signals for updating the policy. Previous literature either initializes the policy \u03c0 \u03b8 with MLE training, and/or use a combination of MLE and PG updates, which often leads to marginal gains in practice (Wu et al., 2018;Choshen et al., 2020). Value-based RL techniques, such as Q-learning, implicitly learn the policy \u03c0 by approximating the value Q \u03c0 (s, a) directly. Deep Q-learning (Mnih et al., 2013) parameterizes the Q-function as Q \u03b8 (x, a), and train the parameters by minimizing the following regression objective L(\u03b8) based on the Bellman temporal consistency:\nE \u03c0 1 2 rt+\u03b3 max a t+1 Q\u03b8(st+1, at+1)\u2212Q \u03b8 (st, at) 2(2)\nwhere\u03b8 is the parameters of the target Q-network, which is a slow copy of \u03b8 and considered as constant for gradient computation of \u03b8. Here \u03c0 is an behavior policy which can be an arbitrary distribution over text, such as the data distribution or replay buffer (Mnih et al., 2013). This makes Q-learning an off-policy algorithm because of its ability to use samples coming from other policies. After learning Q \u03b8 , one can induce a policy \u03c0 from it that takes arg max a Q \u03b8 (s, a) at each state s. Jaques et al.\n\n(2017) instead sample tokens from the softmax function applied to Q \u03b8 . However, the training can be unstable and inefficient due to several challenges: (1) The bootstrapping nature of the above regression problem can make the training unstable. That is, the regression target r t + \u03b3 max a t+1 Q\u03b8(s t+1 , a t+1 ) itself is derived from the Q-function to be learned (Kumar et al., 2019). The problem is exacerbated in the presence of sparse reward in text generation, where the real observed signal r t is zero for all intermediate t < T ; (2) The large action space (e.g., 10 4 ) in text generation results in slow updates. In particular, notice that Eq.(2) applies the gradient update to the Q \u03b8 -value of the only one particular token a t (out of, say, the 10 4 candidate tokens in the vocabulary), making the training inefficient; (3) Besides, pure off-policy updates could be highly sensitive to the quality of training data, and miss the opportunity of on-policy exploration that maximizes the reward of interest in a more direct way.\n\n\nThe Soft Q-Learning Framework\n\nWe introduce the soft Q-learning (SQL) formulation of text generation. It is seamlessly compatible with the common architecture of text generation model (Eq.1), permits easy implementation ( \u00a73.1), and enables efficient and stable RL training in practice ( \u00a73.2). Figure 2 and Algorithm 1 summarizes the resulting SQL framework.\n\n\nSQL Formulation for Text Generation\n\nSoft Q-learning (Haarnoja et al., 2017;Schulman et al., 2017;Nachum et al., 2017) is an maximum-entropy (MaxEnt) extension to the standard (hard) Q-learning (Mnih et al., 2015;Sutton and Barto, 2018). Under this framework, the agent is encouraged to optimize the reward while staying as stochastic as possible, with the objective J MaxEnt (\u03c0) = E \u03c4 \u223c\u03c0 T t=0 \u03b3 t r t + \u03b1H (\u03c0 (\u00b7 | s t )) , which augments the vanilla J(\u03c0) with the additional Shannon entropy term H with coefficient \u03b1. 3 This is appealing because it seamlessly connects the Q-values to the familiar output logits of a text generation model, which enables straightforward implementation of the SQL formulation. Q-values as Generation Model Logits. We show the connection of the Q-values with the logits, i.e., outputs right before the softmax layer. Concretely, with the SQL objective, the following relationship between optimal policy \u03c0 * and action-value Q * holds (Haarnoja et al., 2017;Schulman et al., 2017):\n\u03c0 * (a|s) = exp Q * (s, a) a exp Q * (s, a ) .(3)\nThis form is highly reminiscent of the softmax layer of the generation model in Eq.(1). The con-nection suggests that we can naturally parameterize the Q-function in SQL as the generation model logit function, i.e., Q \u03b8 (s, a) \u2261 f \u03b8 (a|s). In other words, the model output f \u03b8 (a|s), originally interpretted as the \"logit\" of token a given the preceding tokens s, is now re-interpretted as the Q-value of action a in state s. When achieving optimality, f \u03b8 * (a|s), namely Q * (s, a), represents the best possible future reward achievable by generating token a in state s. Similarly, the full generation model p \u03b8 (a|s) in Eq.(1) that applies softmax to f \u03b8 now precisely corresponds to the policy \u03c0 \u03b8 induced from Q \u03b8 (s, a). That is,\n\u03c0 \u03b8 (a|s) = exp Q \u03b8 (s, a) a exp Q \u03b8 (s, a ) \u2261 exp f \u03b8 (a|s) a exp f \u03b8 (a |s) = p \u03b8 (a|s).(4)\nWe could further gain even more intuitive interpretation of the above generation policy \u03c0 * from the lens of advantage function (Sutton and Barto, 2018). Specifically, in SQL, the optimal statevalue function is the log-normalizer of the optimal Q-values (Haarnoja et al., 2017;Schulman et al., 2017). This allows a more concise form of Eq. (3):\nV * (s) = log a exp Q * s, a \u03c0 * (a|s) = exp Q * (s, a) \u2212 V * (s) = exp A * (s, a),(5)\nwhere A * is the optimal advantage function. The equation says that, in the proposed text generation SQL formulation, the optimal policy generates token a in state s according to the token's advantage.\n\n\nEfficient Training with Path Consistency\n\nVanilla training based on the Bellman temporal consistency can suffer from the instability and inefficiency issues similar to the conventional Qlearning ( \u00a72), as we discuss more in the appendix ( \u00a7A.3.2). Fortunately, our SQL formulation allows us to import latest advances of RL techniques to overcome the difficulties. Specifically, we adapt the unified path consistency learning (PCL) that has excelled in game control (Nachum et al., 2017). The PCL-based training updates Q-values of all tokens at once through a connection between the value function and the induced policy. More specifically, Nachum et al. (2017) showed that the optimal policy \u03c0 * (Eq.3) and the optimal state value function V * (Eq.5) in SQL must satisfy the following consistency property for all states and actions:  Figure 2: Soft Q-Learning with path consistency learning (PCL) objectives. Left: Single-step objective (Eq.7), where for each (s t , a t ), the computation involves step t and t + 1. Dashed boxes in dark green and gray indicate the regression target, where the intermediate reward r t is often 0 due to sparsity. The gradient is applied to parameters \u03b8 at step t (indicated by orange color). Right: Multi-step objective (Eq.9) which aggregates from step t all the way to T . In this way, the final-step non-zero reward r T is used as the regression target.\nV * (st) \u2212 \u03b3V * (st+1) = rt \u2212 log \u03c0 * (at|st) ,\nAccordingly, the PCL-based training attempts to encourage the satisfaction of the consistency with the following regression objective L SQL, PCL (\u03b8):\nE \u03c0 1 2 \u2212V\u03b8 (st) +\u03b3V\u03b8 (st+1) +rt\u2212 log \u03c0 \u03b8 (at|st) 2 ,(7)\nwhere \u03c0 \u03b8 is the induced policy defined in Eq.(4); V\u03b8 is defined similarly as in Eq.(5) but depends on the target Q\u03b8 network (i.e., a slow copy of the Q \u03b8 to be learned), and recall that \u03c0 is an arbitrary behavior policy (e.g., data distribution). Please see Figure 2 (left) for an illustration. Crucially, notice that the gradient update is applied to \u03b8 through the log \u03c0 \u03b8 term which explicitly involves the Q \u03b8 -values of all tokens a in the vocabulary. This shows an important difference from the above vanilla training in conventional Q-learning ( \u00a72) where Q \u03b8 is updated only through the particular a t token. The PCL training thus offers more efficient updates for the Q \u03b8 function. In the appendix ( \u00a7A.3.1), we also discuss the difference from the MLE objective. Intuitively, MLE trains the model to (blindly) increase the probability of the observed tokens, while PCL encourages the (log) probability of the tokens to match the approximate advantage values.\n\nMulti-step PCL for Sparse Reward. The above PCL objective Eq.(7) alone does not resolve the potential instability issue due to the bootstrapped V\u03b8(s t+1 ) value and the sparse reward (i.e., r(s t , a t ) = 0 for t < T ). Our SQL formulation allows us to additionally incorporate the multi-step variant of the PCL training (Nachum et al., 2017) to resolve the issue. Specifically, by applying a telescoping sum on the consistency equation (Eq.6) starting from t up to T , we arrive at the multi-step temporal consistency:\nV * (st) \u2212\u03b3 T \u2212t V * (sT +1) = T l=t \u03b3 l\u2212t r l \u2212 log \u03c0 * (a l |s l ) ,(8)\nwhere the value of past-terminal state is zero, V * (s T +1 ) = 0; and the rewards are only available at the end, T l=t \u03b3 l\u2212t r l = \u03b3 T \u2212t r T . We can then come to the following multi-step objective function L SQL, PCL-ms (\u03b8),\nE \u03c0 \uf8ee \uf8f0 1 2 \u2212V\u03b8 (st) +\u03b3 T \u2212t rT \u2212 T l=t \u03b3 l\u2212t log \u03c0 \u03b8 (a l |s l ) 2 \uf8f9 \uf8fb .(9)\nWe can see the objective side-steps the need to bootstrap intermediate value functions V\u03b8(s t ) for t > t. Instead, it directly uses the non-zero end reward r T to derive the update for \u03b8. Please see Figure 2 (right) for an illustration. In practice, we combine the single-and multi-step objectives (Eqs.7 and 9) together for training.\n\nJoint On-and Off-policy Training. Finally, we highlight that the behavior policy \u03c0 involved in the objectives Eqs. (7) and (9) can be an arbitrary policy. For example, \u03c0 can be a (possibly noisy) text dataset, or a set of text samples produced by other generation models, resulting in off-policy training. We can also set \u03c0 to be the current generation model \u03c0 \u03b8 to be learned, resulting in on-policy training. In practice, we could first train the model with only off-policy data for warming up, and then continue with joint on-and off-policy training to further maximize the reward.\n\n\nApplications and Experiments\n\nWe show broad applications of the proposed RL text generation framework to a variety of problems Draw a batch of on-policy samples {\u03c4 on } by decoding with policy \u03c0 \u03b8 (a t | s t ) (Eq.4) 5:\nCompute Q \u03b8 (s t , a t ) values (the model logits) and target Q\u03b8(s t , a t ) for (s t , a t ) \u2208 {\u03c4 off } \u222a {\u03c4 on } 6:\nCompute the objectives in Eqs. (7) and (9) 7:\n\nUpdate the model parameters \u03b8 via gradient descent 8: Update the target model parameters\u03b8 by\u03b8 \u2190 \u03c1\u03b8 + (1 \u2212 \u03c1)\u03b8 with update rate \u03c1 9: until convergence Output: The trained Q \u03b8 * and the induced generator \u03c0 \u03b8 * where no clean supervision data is available. These include learning with noisy or even negative data ( \u00a74.1), generating adversarial text attacks ( \u00a74.2), and generating prompts to steer pretrained LMs ( \u00a74.3). We also study the performance on standard supervised generation tasks ( \u00a7A.1.4) and show that our approach is competitive to train text generation models from scratch. We provide detailed configurations in the appendix ( \u00a7A.2).\n\n\nLearning from Noisy (Negative) Text\n\nThe popular MLE algorithm learns by (blindly) imitating training data. However, it is often expensive to curate clean quality data. It is thus highly desirable to be able to learn from data with noises, or even negative examples. With the guidance of task metrics (rewards), the model can even learn to \"outperform\" the training data and achieve desired generation behaviors. To this end, we consider the task of entailment generation (Pasunuru and Bansal, 2017). Given a sentence (premise), the goal is to generate a new sentence (hypothesis) that logically follows the premise.\n\nSetup (more in \u00a7A.2.1). We sub-sampled 50k training examples from the SNLI dataset (Bowman et al., 2015), a commonly used entailment classification dataset. The hypotheses have an average entailment probability of only 50%, and over 2/5 of them less than 20% (negative/contradictive examples) -a significant challenge for the models to learn from the noises. The rewards include (1) the entailment score of the generation measured by a robust entailment classifier (Nie et al., 2020), (2) the log-likelihood of the generation as an indicator of language quality measured by a GPT-2 language model (Radford et al., 2019), and (3) BLEU score w.r.t the input premises as another language quality reward that avoids trivial outputs. We sum together all rewards with weights 1.0.\n\nWe compare our approach with a broad range of baselines, including (1) (6) one of the latest methods GOLD-s (Pang and He, 2021) which is a pure off-policy method based on importance-sampling PG. To ablate the effect of multi-step training ( \u00a73.2), we additionally compare with a simplified variant of our approach that uses only vanilla single-step PCL training (SQL(single)). We include more baselines such as MLE weighted by rewards in \u00a7A.1.1.\n\nWe evaluate generation results in terms of entailment rate, language quality (perplexity), and diversity which is measured by the Shannon entropy over unigrams and bigrams (H 1 , H 2 ) (Gehrmann et al., 2021). Since text generation models intrinsically trade off diversity and quality (Caccia et al., 2019;Hashimoto et al., 2019), we vary the generation diversity by generating samples via top-p sampling (Holtzman et al., 2019) with different p values, and plot the entailment rate and perplexity against diversity, resp. We also evaluate the samples produced by beam-search decoding.  Table A.3 for additional results. Right: entailment attack performance against diversity. Only a few MLE+PG dots are visible because the model is not able to generate more diverse samples even with increasing p value in top-p decoding, i.e., the model collapses.\n\nResults. Figure 3 (left) shows the results, and Table A.5 shows samples. First, notice that MLE performs poorly, while MLE+reward improves upon it. This is not surprising as the training data contain noisy/negative examples. Similarly, since the pure off-policy algorithm GOLD-s relies heavily on the data distribution, we observed that it achieves suboptimal performance. The on-policy MLE+PG with MLE initialization gives better entailment rate. In comparison, our full SQL framework achieves the best entailment-diversity trade-off. The comparison between SQL and SQL(single) highlights the importance of having the multi-step objective which directly uses the end reward rather than bootstrapping intermediate Q-values for supervision.\n\n\nUniversal Adversarial Attacks\n\nWe next study the application in text adversarial attacks, where again no supervised data is available. Adversarial attacks is an increasingly important research topic as they reveal models' vulnerabilities and flaws. This is especially true for universal attacks (Wallace et al., 2019; Atanasova et al., 2020), where we want to generate universal examples that trick the model on all possible inputs. For instance, consider the context of entailment classification. Our goal is to find universal humanreadable hypotheses that are going to be classified as \"entailment\" with as high probability as possible, regardless of the input premises. This is a more challenging setting compared to previous instancespecific attack (Morris et al., 2020;Jin et al., 2020;Ebrahimi et al., 2017) where the attack model conditions on a premise and generates an adversarial hypothesis specific to the premise.\n\nSetup (more in \u00a7A.2.2). We aim to attack one of the most popular MultiNLI (Williams et al., 2018) entailment classifiers on HuggingFaceHub. 4 The attack generation model generates adversarial text without conditioning on any inputs so that the generated attacks are universal to all premises. We compare our SQL with MLE+PG. We use all hypotheses in the MultiNLI dataset as the training data for the MLE training in MLE+PG and the offpolicy updates for our SQL. We do not compare with previous specialized adversarial text attack methods, because they either are not applicable to the challenging universal attack setting (Morris et al., 2020;Jin et al., 2020;Ebrahimi et al., 2017), or were not designed to generate human-readable sentences (Wallace et al., 2019). We use similar settings as in \u00a74.1 to explore the diversity-quality trade-off by plotting the entailment rate and perplexity against diversity, respectively. The entailment classifier to be attacked is used as entailment score reward functions. We also include a tokenlevel repetition penalty reward for readability. Figure 3 (right) shows the results, and   \n\n\nResults.\n\n\nPrompt Generation for Controlling Pretrained Language Models\n\nA reward function does not just have to be a metric like the BLEU score, but also a complicated pipeline that eventually returns a score. To demonstrate this, we consider the emerging task of prompting a large pretrained LM for controllable generation (Hu et al., 2017;Radford et al., 2019;Brown et al., 2020). The goal is to learn to generate text prompts that steer the LM to generate sentences of certain desired attributes (e.g., topics). \n\n\nReward Function\n\nTopic Classi er Perplexity \"science\" topic Figure 5: The scheme of prompt generation for controlling the outputs of pretraind LMs.\n\ngeneration setting, as illustrated in Figure 5. In contrast, the RL framework is generally applicable to any differentiable or discrete pipelines.\n\nSetup (more in \u00a7A.2.3). Following (Dathathri et al., 2019), we aim to control the generation to have one of 7 topics (e.g., \"science\"); the generated prompt is prepended to one of 20 input sentences for the pretrained LM to generate continuation sentences. Figure 5 shows the architecture of prompt-based controllable generation. We compare our SQL method with MLE+PG as before.\n\nSince the prompt length could impact the generated sentences, we conducted experiments with maximum prompt length 5, 10, and 15. As ablation study, we also evaluate the SQL algorithm with only off-policy updates (i.e., without on-policy exploration), denoted as SQL(off), and compare it with vanilla MLE training. Finally, we also compare with two specialized controllable generation techniques based on pretrained LMs, namely PPLM (Dathathri et al., 2019) and GeDi (Krause et al., 2020), following similar procedures using their open-sourced code. We use a distilled GPT-2 model 5 as the pretrained LM to be controlled. For rewards, we use the topic accuracy of the continuation sentences measured by a zero-shot classifier, plus the the log-likelihood of continuation sentences as the language quality reward measured by a distilled  Results. Figure 4 shows the topic accuracy of the controlled LM outputs averaged across the 7 topics, and Table 1 shows the respective language quality results. More detailed topic accuracy results and samples are provided in the appendix ( \u00a7A.1.3) (where GeDi obtained low accuracy on 2 of the 7 topics, possibly because the topic tokens are tokenized into two subwords for which the model released by the authors was not specifically trained). 5 https://huggingface.co/distilgpt2 6 Note that the language quality emphasis is on the generated sentences. Prompts themselves do not necessarily have to be human-readable (Wallace et al., 2019;Sheng et al., 2020).\n\nWe can see that the prompts generated by our SQL cause the LM to generate sentences with high topic accuracy while maintaining low perplexity in most settings. Increasing the prompt length positively impacts the topic accuracy, which makes sense because longer prompts give more flexible for steering the LM. The comparison between MLE and SQL(off) shows that the off-policy component of SQL is better than standard MLE training, as it incorporates reward signals instead of just blindly following the (noisy) data.\n\nNext, comparing with the previous steered decoding such as PPLM and GeDi, we can see the prompt-based control trained with RL achieves better trade-off between topic accuracy and language quality. Moreover, once a prompt is produced, we can use the pretrained LM to generate text of desired topics efficiently, with the same time cost as standard non-controlled decoding. In comparison, the dedicated steered decoding is often orders-ofmagnitude slower, as shown in Table 2.\n\n\nRelated Work\n\nStandard RL algorithms can sometimes be oversensitive to the randomness in the environment. Recent works have considered maximum-entropy RL extensions, such as the soft Q-learning (SQL) (Haarnoja et al., 2017;Nachum et al., 2017;Schulman et al., 2017), that maximize the entropy of policy besides the rewards, and demonstrated substantial improvement in robotic and game control (Ziebart et al., 2008;O'Donoghue et al., 2017;Nachum et al., 2018;Eysenbach and Levine, 2021). Our work is the first to adapt SQL and its advanced variants (in particular the path consistency learning (Nachum et al., 2017)) to the challenging text generation problem and show significant results on diverse applications.\n\nApplying RL for text generation has been discussed in alleviating the exposure bias problem and optimizing task metrics . As a result, the opportunity of directly improving the reward (as in on-policy updates) for other rich tasks is missed. Our proposed framework combines onand off-policy training, and further offers solutions for efficient training from scratch in the presence of large action space and sparse sequence-level reward in text generation.\n\n\nConclusion\n\nWe develop a new RL formulation for text generation based on soft Q-learning and path consistency learning. We conduct experiments on learning with noisy and negative data, black box adversarial attack, prompting a pretrained language model for controllable generation, and standard supervised tasks. This formulation opens up new opportunities to integrate more advances made in the fertile RL literature to improve text generation problems.\n\n\nLimitations\n\nA well-documented limitation of RL methods is the importance of the reward function. The proposed methods are no different in this aspect. This is especially relevant as our reward function could involve a learned model itself, which we proactively leveraged in Sec. 4.2. We refer interested readers to Deng et al. (2022) for more algorithmic considerations. We also noticed that adapting the pretraining-finetuning paradigm to the proposed methods requires careful designs. A hypothesis points to the discrepancy between MLE objectives (commonly used in pretraining context) and SQL objectives. As discussed in Sec. 3.1, the SQL formulation re-interprets the \"logit\" as the Q-value, for many good reasons. However, our preliminary experiments suggest that, as a downside, this makes finetuning an MLE-trained model with SQL objectives more challenging. Future work to scale the proposed methods to tasks such as machine translation and language modeling, and with significantly larger and (MLE-)pretrained models would be exciting.\n\n\nEthics Statement\n\nThis work develops a new RL formulation for text generation. While we demonstrate the framework in four applications, it could be adapted to other (emerging) applications. One major component in these applications is the design of the reward function, which influences the behavior of the trained agent. While we believe the MaxEnt RL framework is more robust against reward misspecification (Eysenbach and Levine, 2021), the potential failures of sub-optimal reward functions are widely known and discussed. 7 To this end, deploying this model to the wild requires careful and extensive examination, using tools such as Ribeiro et al. (2020). Further, we highlight the application for (blackbox) adversarial attacks in the paper, with the intention of using adversarial attacks to understand the model's inner workings. That being said, this could potentially be misused to conduct malicious attacks against systems. Hence, users of this framework might want to conduct adversarial attacks against their own models to avoid being attacked by other people with bad intentions.  \n\n\nA.1.4 Supervised Text Generation Tasks\n\nFinally, we conduct experiment on standard generation tasks where clean supervised data is available.\n\nThe study is to examine the capabilities of the proposed RL method to train a text generation model from scratch, which has been considered as exceedingly challenging for previous RL algorithms.\n\nSetup. We study on two tasks, E2E (Novikova et al., 2017) and CommonGEN (Lin et al., 2020), and use the respective datasets pre-processed by (Gehrmann et al., 2021) which allow sequence-tosequence modeling with standard transformers. We run four sets of methods: the standard MLE training (MLE); PG training from scratch (PG); joint MLE and PG training, with MLE initialization (MLE+PG); and our SQL training from scratch with both off-policy and on-policy updates (SQL). We use the standard BLEU as reward. We additionally investigate the training stability and sensitivity w.r.t hyperparameters, in particular the scale of reward. To this end, for MLE+PG and SQL, we vary the reward scale in {1, 10, 50, 100, 500, 1000} and evaluate the respective performance under different scales.\n\nResults. Table A.1 shows the performance on E2E of different models whose hyperparameters are picked using the validation set. We can see the proposed SQL that trains models from scratch achieves competitive results with the common MLE and MLE+PG. In contrast, the PG algorithm alone without MLE fails the training. Figure A.2 (left) shows the respective training curves (on the validation set), demonstrating that SQL converges in an efficient and stable way as MLE.\n\nWe further demonstrate the sensitive of MLE+PG and SQL w.r.t the reward scale as a key hyperparameter. Figure A.2 (middle and right) shows the training curves of the two methods with varying reward scales. We can see SQL is significantly more robust as reward scale changes, while MLE+PG tends to collapse with improper reward scale configurations.\n\n\nA.2 Setup Details\n\nOur evaluation follows the GEM Benchmark (Gehrmann et al., 2021)  negative (contradictive) examples. The resulting training set poses a significant challenge for the models to learn from the noises.\n\nThe RL algorithms (including PG and ours) permit us to plug in arbitrary reward functions to drive learning. Based on the goal of the task, we use the following intuitive rewards to ensure entailment accuracy and language quality: (1) a robust entailment classifier (Nie et al., 2020) that measures the entailment score of a generation in terms of the input premise, (2) a GPT-2 language model (Radford et al., 2019) that measures the log-likelihood of the generation as an indicator of language quality, and (3) BLEU score w.r.t the input premises as another language quality reward that avoids trivial outputs. We sum together all rewards with weights 1.0.\n\n\nA.2.2 Setup Details: \u00a74.2\n\nWe study the task of attacking an entailment classifier. In particular, we aim to attack one of the most popular entailment classifiers on Hugging-FaceHub. 13 The attack generation model gener- ates adversarial text without conditioning on any inputs so that the generated attacks are universal to all premises. The generation model is trained with mostly the same setting as in \u00a74.1, where the entailment classifier to be attacked is used as entailment score reward functions. Besides, we additionally include a token-level repetition penalty reward, which empirically benefits readability. Finally, we use the MultiNLI dataset (Williams et al., 2018) which includes more diverse examples than the SNLI used above. We compare our SQL with MLE+PG. We use all hypotheses in the MultiNLI dataset as the training data for the MLE training in MLE+PG and the offpolicy updates for our SQL. We do not compare with previous specialized adversarial text attack methods, because they either are not applicable to the universal attack setting (Morris et al., 2020;Jin et al., 2020;Ebrahimi et al., 2017), or were not designed to generate human-readable sentences (Wallace et al., 2019). Besides, it is worth noting that the general RL algorithms have an additional advantage of doing black-box attacks. That is, the algorithms only require the ability to query the entailment classifier for entailment probability, without need of knowing the internal structure of the classifier (e.g., for computing gradients) as in previous attack algorithms (Ebrahimi et al., 2017;Wallace et al., 2019).\n\nFor top-p sampling results, we sample a hypothesis for each premise and measure the average attack rate across the dataset. This is because sampling multiple hypotheses, each for all premises, and measure performance are expensive. Since the hypotheses are sampled input-independently, this should be a good approximation.\n\n\nA.2.3 Setup Details: \u00a74.3\n\nFollowing (Dathathri et al., 2019), we aim to control the generation to have one of 7 topics (e.g., \"science\"); the generated prompt is prepended to one //huggingface.co/models?search=nli. of 20 input sentences ( Figure 5) for the pretrained LM to generate continuation sentences. There is no direct supervision data available for training the prompt generator. We randomly create some noisy text as the training data for MLE baselines below and for off-policy updates for our algorithm. Specifically, the noisy text is created by sampling keywords and topics from the list used in (Dathathri et al., 2020) and a paraphrase generation model. Figure 5 shows the architecture of prompt-based controllable generation. We compare our SQL method with MLE+PG as before. At training time, for each generated prompt sample, the pretrained LM generates 2 continuation sentences for evaluating average reward. We use a zero-shot classifier to evaluate the topic accuracy of the continuation sentences. That is, we do not assume access to classifiers pretrained on topic-specific sentences, because generating such topic-specific sentences is the goal of the task in the first place. We additionally use an LM to evaluate the loglikelihood of continuation sentences for measuring language quality. Since the prompt length could impact the generated sentences, we conducted experiments with maximum prompt length 5, 10, and 15. As ablation study, we also evaluate the SQL algorithm with only off-policy updates (i.e., without on-policy exploration), denoted as SQL(off), and compare it with vanilla MLE training. At test time, given a topic, the trained prompt generator produces one prompt using beam search decoding. For each generated prompt, the pretrained LM generates 100 sentences using top-k decoding (with k = 50) for evaluation. Finally, we also compare with two specialized controllable generation techniques based on pretrained LMs, namely PPLM (Dathathri et al., 2019) and GeDi (Krause et al., 2020), following similar procedures using their open-sourced code. We use a distilled GPT-2 model 14 as the pretrained LM to be controlled. We use the paraphrase generation model based on Zhang et al. (2019). 15 Table A.4: Prompt generation results. Note that some of the numbers from GeDi are low because the topics are tokenized into two subword tokens, which the model was not trained with.\n\nFigure 1 :\n1Left: An overview of the proposed SQL algorithm. Text generation is challenging due to sparse reward (i.e., the rewards of all intermediate steps are 0) and large action space (i.e., large vocabulary). Our SQL formulation enables several key algorithmic features as highlighted with yellow color, including (1) the combined on-and off-policy updates for the best of both, (2) bridging the final non-zero reward to directly supervise the Q-value estimation at intermediate steps for learning stability, and (3) simultaneously updating the Q-values of all candidate actions for efficiency. Right: We explore diverse applications of the text-generation RL algorithm.\n\n\nthe standard MLE training (MLE); (2) MLE+reward, where we use the reward function to filter examples; (3) joint MLE and PG training with MLE initialization (MLE+PG), where we initialize the model with MLE training, then train it with combined MLE and PG losses; previous text-generation RL algorithms including (4) MIXER (Ranzato et al., 2016), (5) Self-critic (Rennie et al., 2017), and\n\nFigure 3 :\n3Left: entailment generation performance plotted against diversity (average of H 1 and H 2 ). Circles represent results of top-p sample outputs, and triangles represent results of beam-search outputs. Please see\n\n\n(Guo, 2015; Li et al., 2016;  Wu et al., 2016; Rennie et al., 2017; Paulus et al.,  2018; Chen and Bansal, 2018; Liu et al., 2020;  Pang et al., 2021). For example, Ranzato et al.(2016) used the REINFORCE algorithm (Williams, 1992), and Bahdanau et al. (2016) used the actorcritic algorithm; Guo et al. (2018) and Shi et al. (2018) tried to relieve the sparsity problem via hierarchical and inverse RL methods, resp. They are all on-policy RL algorithms with the need of pretraining their models using MLE. RAML (Norouzi et al., 2016) implicitly relies on the quality of off-policy data; this does not necessarily apply in our experiments with limited good data. Tan et al. (2018) and Hu and Xing (2022) offer a unified view of RAML, RL, and other training methods.Another line of work focused mostly on using only off-policy data, often for offline training of chatbots(Kandasamy et al., 2017; Zhou et al., 2017;  Jaques et al., 2020; Pang and He, 2021)\n\n\nAlgorithm 1 Efficient Soft Q-Learning for Text GenerationDraw a batch of off-policy samples {\u03c4 off } \u223c DInput: Q \u03b8 (i.e., generation model logit function f \u03b8 in Eq.1) \nReward function r(s, t) \nTraining examples D (for off-policy updates; optional) \n1: Initialize \u03b8 and target model parameters\u03b8 \n2: repeat \n\n3: \n\n4: \n\n\n\n\nTable A.2 shows samples. We can see that SQL outperforms MLE+PG consistently across different diversity values. The outputs from MLE+PG are not diverse even with high p's, indicating the model collapses and can only generate a small set of unique adversarial examples. The model by SQL discovers the pattern \"saint-pierre-et-saint-paul\" (an entity name), and exploits this to generate samples with high universal entailment rate.Figure 4: Average topic accuracy. Please seeTable A.4 for more details.PPLM GeDi \nMLE (5) SQL (off, 5) \n\n13.07 \n123.88 \n25.70 \n25.77 \n\nMLE+PG (5/10/15) SQL (5/10/15, ours) \n\n25.52/28.16/28.71 \n25.94/26.95/29.10 \n\n\n\nTable 1 :\n1Average perplexity across topics. The lower, the more fluent the generated continuation sentences.Model PPLM GeDi SQL \n\nSeconds 5.58 1.05 0.07 \n\n\n\nTable 2 :\n2Average sentence generation time cost.\n\n\nIt side-steps the needs for expensive LM fine-tuning, and adapts LMs to new scenarios with prompt as the (computefriendly) interface. Most existing approaches (Wallace et al., 2019; Li and Liang, 2021; Lester et al., 2021) rely on gradient backpropagation and are applicable only when the whole training pipeline is differentiable. This does not hold for the textThe problem of controlling the genera-\ntion of pretrained LMs was previously approached \nthrough specialized algorithms such as modifying \nthe LM hidden states during decoding (Dathathri \net al., 2020; Krause et al., 2020; Qin et al., 2020). \nHere we show that prompts offer an easier, faster, \nmore effective way for controlled generation. \nLearning to generate/tune prompts is gaining in-\ncreasing attention recently. Pretrained \nLM \n\nGenerated \nSentence N \n\nreward: \naverage score \n\n\"the chemical microscope is In summary\" \nPrompt \nGenerator \n\nprompt (model's output) \ninput sentence \n\nGenerated \nSentence 1 \n... \n\n\n\n\nLeshem Choshen, Lior Fox, Zohar Aizenbud, and OmriAbend. 2020. On the weaknesses of reinforcement learning for neural machine translation. In International Conference on Learning Representations.Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691.Tom Brown, Benjamin Mann, Nick Ryder, Melanie \nSubbiah, Jared D Kaplan, Prafulla Dhariwal, \nArvind Neelakantan, Pranav Shyam, Girish Sastry, \nAmanda Askell, Sandhini Agarwal, Ariel Herbert-\nVoss, Gretchen Krueger, Tom Henighan, Rewon \nChild, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, \nClemens Winter, Chris Hesse, Mark Chen, Eric \nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess, \nJack Clark, Christopher Berner, Sam McCandlish, \nAlec Radford, Ilya Sutskever, and Dario Amodei. \n2020. Language models are few-shot learners. In \nAdvances in Neural Information Processing Systems, \nvolume 33, pages 1877-1901. Curran Associates, \nInc. \n\nMassimo Caccia, Lucas Caccia, William Fedus, Hugo \nLarochelle, Joelle Pineau, and Laurent Charlin. \n2019. Language GANs falling short. In Interna-\ntional Conference on Learning Representations. \n\nYen-Chun Chen and Mohit Bansal. 2018. Fast abstrac-\ntive summarization with reinforce-selected sentence \nrewriting. In Proceedings of the 56th Annual Meet-\ning of the Association for Computational Linguistics \n(Volume 1: Long Papers), pages 675-686. \n\nSumanth Dathathri, Andrea Madotto, Janice Lan, Jane \nHung, Eric Frank, Piero Molino, Jason Yosinski, and \nRosanne Liu. 2019. Plug and play language mod-\nels: A simple approach to controlled text generation. \nIn International Conference on Learning Represen-\ntations. \n\nSumanth Dathathri, Andrea Madotto, Janice Lan, Jane \nHung, Eric Frank, Piero Molino, Jason Yosinski, and \nRosanne Liu. 2020. Plug and play language mod-\nels: A simple approach to controlled text generation. \nIn International Conference on Learning Represen-\ntations. \n\nMingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan \nWang, Han Guo, Tianmin Shu, Meng Song, Eric P \nXing, and Zhiting Hu. 2022. RLPrompt: Optimizing \ndiscrete text prompts with reinforcement learning. \nIn Proceedings of the 2020 Conference on Empirical \nMethods in Natural Language Processing (EMNLP). \n\nJavid Ebrahimi, Anyi Rao, Daniel Lowd, and De-\njing Dou. 2017. Hotflip: White-box adversarial \nexamples for text classification. arXiv preprint \narXiv:1712.06751. \n\nBenjamin Eysenbach and Sergey Levine. 2021. Max-\nimum entropy rl (provably) solves some robust rl \nproblems. arXiv preprint arXiv:2103.06257. \n\nRoy Fox, Ari Pakman, and Naftali Tishby. 2016. Tam-\ning the noise in reinforcement learning via soft up-\ndates. In Proceedings of the Thirty-Second Confer-\nence on Uncertainty in Artificial Intelligence, pages \n202-211. \n\nSebastian Gehrmann, Tosin Adewumi, Karmanya Ag-\ngarwal, Pawan Sasanka Ammanamanchi, Aremu \nAnuoluwapo, Antoine Bosselut, Khyathi Raghavi \nChandu, Miruna Clinciu, Dipanjan Das, Kaustubh D \nDhole, et al. 2021. The gem benchmark: Natu-\nral language generation, its evaluation and metrics. \narXiv preprint arXiv:2102.01672. \n\nHongyu Guo. 2015. \nGenerating text with \ndeep reinforcement learning. \narXiv preprint \narXiv:1510.09202. \n\nJiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong \nYu, and Jun Wang. 2018. Long text generation via \nadversarial training with leaked information. In Pro-\nceedings of the AAAI Conference on Artificial Intel-\nligence. \n\nTuomas Haarnoja, Haoran Tang, Pieter Abbeel, and \nSergey Levine. 2017. Reinforcement learning with \ndeep energy-based policies. In International Con-\nference on Machine Learning, pages 1352-1361. \nPMLR. \n\nTatsunori Hashimoto, Hugh Zhang, and Percy Liang. \n2019. Unifying human and statistical evaluation for \nnatural language generation. In Proceedings of the \n2019 Conference of the North American Chapter of \nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long and \nShort Papers), pages 1689-1701. \n\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and \nYejin Choi. 2019. The curious case of neural text de-\ngeneration. In International Conference on Learn-\ning Representations. \n\nZhiting Hu, Haoran Shi, Bowen Tan, Wentao Wang, \nZichao Yang, Tiancheng Zhao, Junxian He, Lianhui \nQin, Di Wang, Xuezhe Ma, et al. 2019. Texar: A \nmodularized, versatile, and extensible toolkit for text \ngeneration. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics: \nSystem Demonstrations, pages 159-164. \n\nZhiting Hu and Eric P Xing. 2022. Towards a \"standard \nmodel\" of machine learning. Harvard Data Science \nReview. \n\nZhiting Hu, Zichao Yang, Xiaodan Liang, R. Salakhut-\ndinov, and E. Xing. 2017. Toward controlled genera-\ntion of text. In International Conference on Machine \nLearning (ICML). \n\nNatasha Jaques, Shixiang Gu, Dzmitry Bahdanau, \nJos\u00e9 Miguel Hern\u00e1ndez-Lobato, Richard E Turner, \nand Douglas Eck. 2017. Sequence tutor: Conserva-\ntive fine-tuning of sequence generation models with \nkl-control. In International Conference on Machine \nLearning, pages 1645-1654. PMLR. \n\nNatasha Jaques, Judy Hanwen Shen, Asma Ghande-\nharioun, Craig Ferguson, Agata Lapedriza, Noah \nJones, Shixiang Gu, and Rosalind Picard. 2020. \n\nHuman-centric dialog training via offline reinforce-\nment learning. In Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language \nProcessing (EMNLP), pages 3985-4003. \n\nDi Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter \nSzolovits. 2020. Is bert really robust? a strong base-\nline for natural language attack on text classification \nand entailment. In Proceedings of the AAAI confer-\nence on artificial intelligence. \n\nKirthevasan Kandasamy, Yoram Bachrach, Ryota \nTomioka, Daniel Tarlow, and David Carter. 2017. \nBatch policy gradient methods for improving neural \nconversation models. In ICLR. \n\nBen Krause, Akhilesh Deepak Gotmare, Bryan Mc-\nCann, Nitish Shirish Keskar, Shafiq Joty, Richard \nSocher, and Nazneen Fatema Rajani. 2020. Gedi: \nGenerative discriminator guided sequence genera-\ntion. arXiv preprint arXiv:2009.06367. \n\nAviral Kumar, Justin Fu, George Tucker, and Sergey \nLevine. 2019. Stabilizing off-policy q-learning via \nbootstrapping error reduction. In NeurIPS. \n\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer \nLevy, Veselin Stoyanov, and Luke Zettlemoyer. \n2020. Bart: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation, \nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational \nLinguistics, pages 7871-7880. \n\nJiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky, \nMichel Galley, and Jianfeng Gao. 2016. Deep rein-\nforcement learning for dialogue generation. In Pro-\nceedings of the 2016 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1192-\n1202. \n\nXiang Lisa Li and Percy Liang. 2021. \nPrefix-\ntuning: Optimizing continuous prompts for genera-\ntion. arXiv preprint arXiv:2101.00190. \n\nBill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei \nZhou, Chandra Bhagavatula, Yejin Choi, and Xiang \nRen. 2020. CommonGen: A constrained text gen-\neration challenge for generative commonsense rea-\nsoning. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 1823-1840, \nOnline. Association for Computational Linguistics. \n\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, \nHiroaki Hayashi, and Graham Neubig. 2021. Pre-\ntrain, prompt, and predict: A systematic survey of \nprompting methods in natural language processing. \narXiv preprint arXiv:2107.13586. \n\nRuibo Liu, Guangxuan Xu, Chenyan Jia, Weicheng \nMa, Lili Wang, and Soroush Vosoughi. 2020. Data \nboost: Text data augmentation through reinforce-\nment learning guided conditional generation. In \nProceedings of the 2020 Conference on Empirical \nMethods in Natural Language Processing (EMNLP), \npages 9031-9041. \n\nVolodymyr Mnih, Koray Kavukcuoglu, David Sil-\nver, Alex Graves, Ioannis Antonoglou, Daan Wier-\nstra, and Martin Riedmiller. 2013. Playing atari \nwith deep reinforcement learning. arXiv preprint \narXiv:1312.5602. \n\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, \nAndrei A Rusu, Joel Veness, Marc G Bellemare, \nAlex Graves, Martin Riedmiller, Andreas K Fidje-\nland, Georg Ostrovski, et al. 2015. Human-level \ncontrol through deep reinforcement learning. nature, \n518(7540):529-533. \n\nJohn Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby, \nDi Jin, and Yanjun Qi. 2020. Textattack: A frame-\nwork for adversarial attacks, data augmentation, and \nadversarial training in nlp. In Proceedings of the \n2020 Conference on Empirical Methods in Natu-\nral Language Processing: System Demonstrations, \npages 119-126. \n\nOfir Nachum, Mohammad Norouzi, Kelvin Xu, and \nDale Schuurmans. 2017. Bridging the gap between \nvalue and policy based reinforcement learning. In \nNIPS. \n\nOfir Nachum, Mohammad Norouzi, Kelvin Xu, and \nDale Schuurmans. 2018. Trust-PCL: An off-policy \ntrust region method for continuous control. In Inter-\nnational Conference on Learning Representations. \n\nKarthik Narasimhan, Tejas Kulkarni, and Regina Barzi-\nlay. 2015. Language understanding for text-based \ngames using deep reinforcement learning. In Pro-\nceedings of the 2015 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1-11. \n\nYixin Nie, Adina Williams, Emily Dinan, Mohit \nBansal, Jason Weston, and Douwe Kiela. 2020. Ad-\nversarial nli: A new benchmark for natural language \nunderstanding. In Proceedings of the 58th Annual \nMeeting of the Association for Computational Lin-\nguistics, pages 4885-4901. \n\nMohammad Norouzi, Samy Bengio, Navdeep Jaitly, \nMike Schuster, Yonghui Wu, Dale Schuurmans, et al. \n2016. Reward augmented maximum likelihood for \nneural structured prediction. NeurIPS, 29:1723-\n1731. \n\nJekaterina Novikova, Ond\u0159ej Du\u0161ek, and Verena Rieser. \n2017. The e2e dataset: New challenges for end-to-\nend generation. In Proceedings of the 18th Annual \nSIGdial Meeting on Discourse and Dialogue, pages \n201-206. \n\nBrendan O'Donoghue, R. Munos, K. Kavukcuoglu, \nand V. Mnih. 2017. Combining policy gradient and \nq-learning. In ICLR. \n\nRichard Yuanzhe Pang and He He. 2021. Text gener-\nation by learning from demonstrations. In Interna-\ntional Conference on Learning Representations. \n\nRichard Yuanzhe Pang, He He, and Kyunghyun Cho. \n2021. Amortized noisy channel neural machine \ntranslation. arXiv preprint arXiv:2112.08670. \n\nRamakanth Pasunuru and Mohit Bansal. 2017. Multi-\ntask video captioning with video and entailment gen-\neration. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics \n(Volume 1: Long Papers), pages 1273-1283. \n\nRamakanth Pasunuru and Mohit Bansal. 2018. Multi-\nreward reinforced summarization with saliency and \nentailment. In Proceedings of the 2018 Conference \nof the North American Chapter of the Association \nfor Computational Linguistics: Human Language \nTechnologies, Volume 2 (Short Papers), pages 646-\n653. \n\nRomain Paulus, Caiming Xiong, and Richard Socher. \n2018. A deep reinforced model for abstractive sum-\nmarization. In International Conference on Learn-\ning Representations. \n\nLianhui Qin, Vered Shwartz, Peter West, Chandra Bha-\ngavatula, Jena D Hwang, Ronan Le Bras, Antoine \nBosselut, and Yejin Choi. 2020. Backpropagation-\nbased decoding for unsupervised counterfactual and \nabductive reasoning. In Proceedings of the 2020 \nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 794-805. \n\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, \nDario Amodei, and Ilya Sutskever. 2019. Language \nmodels are unsupervised multitask learners. OpenAI \nblog, 1(8):9. \n\nMarc'Aurelio Ranzato, Sumit Chopra, Michael Auli, \nand Wojciech Zaremba. 2016. Sequence level train-\ning with recurrent neural networks. In ICLR. \n\nSteven J Rennie, Etienne Marcheret, Youssef Mroueh, \nJerret Ross, and Vaibhava Goel. 2017. Self-critical \nsequence training for image captioning. In Proceed-\nings of the IEEE Conference on Computer Vision \nand Pattern Recognition, pages 7008-7024. \n\nMarco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, \nand Sameer Singh. 2020. Beyond accuracy: Behav-\nioral testing of nlp models with checklist. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4902-\n4912. \n\nJohn Schulman, Xi Chen, and Pieter Abbeel. 2017. \nEquivalence between policy gradients and soft Q-\nlearning. arXiv preprint arXiv:1704.06440. \n\nEmily Sheng, Kai-Wei Chang, Prem Natarajan, and \nNanyun Peng. 2020. Towards controllable biases in \nlanguage generation. In Findings of the Association \nfor Computational Linguistics: EMNLP 2020, pages \n3239-3254. \n\nZhan Shi, Xinchi Chen, Xipeng Qiu, and Xuanjing \nHuang. 2018. Toward diverse text generation with \ninverse reinforcement learning. In Proceedings of \nthe 27th International Joint Conference on Artificial \nIntelligence, pages 4361-4367. \n\nTaylor Shin, Yasaman Razeghi, Robert L Logan IV, \nEric Wallace, and Sameer Singh. 2020. Autoprompt: \nEliciting knowledge from language models with \nautomatically generated prompts. arXiv preprint \narXiv:2010.15980. \n\nRichard S Sutton and Andrew G Barto. 2018. Rein-\nforcement learning: An introduction. MIT press. \n\nBowen Tan, Zhiting Hu, Zichao Yang, Ruslan Salakhut-\ndinov, and Eric Xing. 2018. Connecting the dots \nbetween mle and rl for sequence prediction. arXiv \npreprint arXiv:1811.09740. \n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob \nUszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz \nKaiser, and Illia Polosukhin. 2017. Attention is all \nyou need. Advances in Neural Information Process-\ning Systems, 30:5998-6008. \n\nEric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, \nand Sameer Singh. 2019. Universal adversarial trig-\ngers for attacking and analyzing nlp. In Proceed-\nings of the 2019 Conference on Empirical Methods \nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 2153-2162. \n\nAdina Williams, Nikita Nangia, and Samuel Bowman. \n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American \nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1 \n(Long Papers), pages 1112-1122. \n\nRonald J Williams. 1992. Simple statistical gradient-\nfollowing algorithms for connectionist reinforce-\nment learning. Machine learning, 8(3-4):229-256. \n\nLijun Wu, Fei Tian, Tao Qin, Jianhuang Lai, and Tie-\nYan Liu. 2018. A study of reinforcement learning \nfor neural machine translation. In Proceedings of \nthe 2018 Conference on Empirical Methods in Natu-\nral Language Processing, pages 3612-3621. \n\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V \nLe, Mohammad Norouzi, Wolfgang Macherey, \nMaxim Krikun, Yuan Cao, Qin Gao, Klaus \nMacherey, et al. 2016. Google's neural machine \ntranslation system: Bridging the gap between hu-\nman and machine translation. \narXiv preprint \narXiv:1609.08144. \n\nWenpeng Yin, Jamaal Hay, and Dan Roth. 2019. \nBenchmarking zero-shot text classification: \nDatasets, evaluation and entailment approach. \nIn Proceedings of the 2019 Conference on Empiri-\ncal Methods in Natural Language Processing and \nthe 9th International Joint Conference on Natural \n\nLanguage Processing (EMNLP-IJCNLP), pages \n3905-3914. \n\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Pe-\nter J. Liu. 2019. Pegasus: Pre-training with ex-\ntracted gap-sentences for abstractive summarization. \n\nRuiqi Zhong, Kristy Lee, Zheng Zhang, and Dan Klein. \n2021. Meta-tuning language models to answer \nprompts better. arXiv preprint arXiv:2104.04670. \n\nLi Zhou, Kevin Small, Oleg Rokhlenko, and Charles \nElkan. 2017. End-to-end offline goal-oriented di-\nalog policy learning via policy gradient. arXiv \npreprint arXiv:1712.02838. \n\nBrian D Ziebart. 2010. Modeling purposeful adaptive \nbehavior with the principle of maximum causal en-\ntropy. \n\nBrian D Ziebart, Andrew L Maas, J Andrew Bagnell, \nand Anind K Dey. 2008. Maximum entropy inverse \nreinforcement learning. In Aaai, volume 8, pages \n1433-1438. Chicago, IL, USA. \n\nModel MLE \n\nPG \nMLE+PG SQL (ours) \n\nval \n45.67 0.00 49.08 \n47.04 \ntest \n41.75 0.00 42.26 \n41.70 \n\n\n\nTable A .\nA1: BLEU results on the E2E val/test sets.A Appendix \n\nA.1 Applications and Experiments \n\nA.1.1 Learning from Noisy (Negative) Text \n\nPlease see Table A.3 for beam search results, Fig-\nure A.1 for additional results for MLE+reward, \nand Table A.5 for examples. \n\nA.1.2 Universal Adversarial Attacks \n\nPlease see Table A.2 for examples. \n\nA.1.3 Prompt Generation for Controlling \nPretrained Language Models \n\nPlease see Table A.4 for detailed results breakdown, \nand Table A.6-A.9 for examples. Examples are in \nthe format: topic: [prompt] input sentence gener-\nated text. \n\n\n\n\nwhen applicable,8   and otherwise same with the reward function used in training. We use a transformer model(Vaswani  et al., 2017)  based onTexar-Pytorch (Hu et al.,  2019)  by default, with 64 hidden dimension, 3 blocks, and 4 heads. For experiments that involve policy gradient training, we initialize the model with maximum likelihood training by default unless specified otherwise. We train soft Q-learning model from scratch with both off-policy (using data) and on-policy (using samples) by default except in \u00a74.1 and \u00a74.3, in which we find it beneficial to warm-up the model with just off-policy training. We apply similar tuning budgets to both soft Q-learning model, and policy-gradient (mostly the reward scale and top-k), based on performance on the validation dataset and sample qualities. Most of the experiments are conducted using Nvidia 1080 or 2080 series GPUs with around 12GB memory. Most of the datasets are based in English.Figure A.1: Entailment generation performance plotted against diversity (average of H 1 and H 2 ).Figure A.2: Training curves on validation sets. Left: Training curves on E2E with best hyperparameter configurations. Middle: Training curves on E2E with varying reward scale. Right: Training curves on CommonGen with varying reward scale.BART (Lewis et al., 2020) to compute the topic score in \u00a74.3. 11 To compute perplexities, we use a GPT-2 model (124M parameters) (Radford et al., 2019) fine-tuned on the corresponding datasets for computing perplexity in \u00a74.1 and 4.2, and a distilled GPT-2 model in \u00a74.3 without fine-tuning.12   We simply set reward weights to 1.0, except in \u00a74.2, where we changed the entailment weight to 0.5, log-likelihood and repetition penalty weight to 5.0.We study using the SNLI dataset(Bowman et al., 2015), a dataset commonly used in training an entailment classifier. The original dataset contains (premise, hypothesis) sentence pairs, where the hypothesis may or may not entail the premise. We sub-sampled 50, 000 training examples from the corpus such that the hypotheses have an average entailment probability of only 50% in terms of the premises, and over 2/5 examples have entailment probabilities less than 20%, which can be seen as 11 https://huggingface.co/facebook/ bart-large-mnli. 407M parameters. 12 https://huggingface.co/distilgpt2. 82M parameters.E2E \n\nPG from scratch \n\nE2E \nCommonGen \n\nSQL (ours) \n\nSQL (ours) \nSQL (ours) \n\nA.2.1 Setup Details:  \u00a74.1 \n\n\n\n\nSQL (ours) the person saint-pierre-et-saint-paul is saint-pierre-et-saint-paul .Table A.2: Entailment attack samples and respective entailment rates across all test premises. For example, the adversarial sample by SQL is considered to entail 97.40% test premises by the entailment classifier.Model \n\nGeneration \nRate \n\nMLE+PG \nit 's . \n90.48 \n97.40 \n\n\n\n\nDuring decoding, we include no_repeat_ngram_size= 2, which improves readability. 16 Model Entl. Prob \u2191 Entl. Rate \u2191 PPL \u2193 Table A.3: Beam search results on entailment generation, in the format val/test. \u2191/\u2193 indicates higher/lower is better. \u2020 SQL (single) achieves zero in H 1 /H 2 as it generates a single token.H1 \u2191 \nH2 \u2191 \n\nMLE \n75.62/75.86 \n79.75/80.23 \n5.49/5.45 \n5.46/5.42 8.47/8.40 \nGOLD-s (Pang and He, 2021) 74.55/76.03 \n78.69/79.89 \n5.55/5.50 \n5.50/5.49 8.48/8.45 \nMLE+PG \n90.16/89.73 \n95.18/94.13 \n6.38/6.31 \n5.23/5.20 8.02/7.99 \nSQL \n91.94/91.55 \n96.26/96.21 \n8.41/8.42 \n5.59/5.58 8.20/8.21 \nSQL (single)  \u2020 \n89.90/89.92 \n94.94/94.82 \n214.42/214.42 0.00/0.00 0.00/0.00 \n\nLength Model \nlegal \npolitics \ncomputers space \nreligion \nscience \nmilitary Average \n\nTopic Scores \n\n/ \nPPLM \n16.52 \n25.09 \n13.35 \n26.23 \n5.39 \n38.87 \n19.33 \n20.68 \n/ \nGeDi \n40.51 \n83.40 \n9.32 \n70.90 \n18.69 \n12.46 \n86.40 \n45.96 \n5 \nMLE \n17.28 \n13.44 \n7.26 \n42.27 \n45.24 \n39.31 \n63.75 \n32.65 \n5 \nSQL (off) 23.79 \n61.11 \n24.07 \n7.91 \n61.77 \n64.67 \n67.83 \n44.45 \n5 \nMLE+PG 29.45 \n74.16 \n72.49 \n57.39 \n65.62 \n74.31 \n76.86 \n64.33 \n5 \nSQL \n11.79 \n70.57 \n66.37 \n58.80 \n65.60 \n69.24 \n83.15 \n60.79 \n10 \nMLE+PG 17.72 \n75.29 \n71.01 \n73.92 \n58.29 \n80.85 \n80.84 \n65.42 \n10 \nSQL \n29.62 \n86.58 \n75.72 \n58.38 \n71.29 \n81.05 \n91.40 \n70.58 \n15 \nMLE+PG 40.18 \n81.47 \n47.14 \n82.64 \n76.21 \n84.82 \n89.31 \n71.68 \n15 \nSQL \n48.08 \n77.94 \n70.04 \n87.43 \n75.46 \n85.94 \n77.36 \n74.61 \n\nPerplexity \n\n/ \nPPLM \n13.52 \n12.81 \n12.79 \n13.56 \n12.98 \n12.43 \n13.38 \n13.07 \n/ \nGeDi \n204.44 \n80.01 \n132.82 \n116.94 \n132.19 \n90.00 \n110.77 \n123.88 \n5 \nMLE \n24.52 \n25.05 \n23.79 \n26.26 \n26.07 \n25.63 \n28.56 \n25.70 \n5 \nSQL (off) 25.48 \n22.70 \n25.10 \n26.64 \n25.84 \n27.45 \n27.19 \n25.77 \n5 \nMLE+PG 24.42 \n22.60 \n27.74 \n23.17 \n25.38 \n24.84 \n30.50 \n25.52 \n5 \nSQL \n25.31 \n24.15 \n26.40 \n24.31 \n27.02 \n25.73 \n28.67 \n25.94 \n10 \nMLE+PG 28.25 \n23.49 \n27.82 \n26.88 \n31.62 \n25.31 \n33.74 \n28.16 \n10 \nSQL \n25.23 \n25.37 \n26.20 \n26.97 \n25.02 \n27.11 \n32.76 \n26.95 \n15 \nMLE+PG 28.38 \n28.24 \n28.16 \n27.21 \n26.43 \n29.99 \n32.54 \n28.71 \n15 \nSQL \n35.16 \n27.72 \n29.70 \n31.89 \n24.04 \n28.46 \n26.74 \n29.10 \n\n\nCode at https://github.com/HanGuo97/ soft-Q-learning-for-text-generation.\n More recently, Deng et al. (2022)  extend this line of work to optimize discrete text prompts with reinforcement learning.\nWLOG, we can assume \u03b1=1, as it can be folded into the reward function by scaling the latter with 1/\u03b1.\nhttps://github.com/pytorch/fairseq/ tree/master/examples/roberta\nhttps://openai.com/blog/ faulty-reward-functions/\nhttps://github.com/pytorch/fairseq/ tree/master/examples/roberta, which is ranked #1 as of May 20, 2021 based on https:\nhttps://huggingface.co/distilgpt2 15 https://huggingface.co/tuner007/ pegasus_paraphrase 16 https://huggingface.co/blog/ how-to-generate\nAcknowledgementWe thank all reviewers for their invaluable comments and feedback. This research was supported by NSF IIS1563887, NSF CCF1629559, NSF IIS1617583, NGA HM04762010002, NSF IIS1955532, NSF CNS2008248, NSF IIS2123952, and NSF BCS2040381. The views in this article are those of the authors and not the funding agency.Input: two men on bicycles competing in a race . Generated: two men are riding bikes .A.3 The Soft Q-Learning FrameworkA.3.1 Comparison with MLE ObjectiveIt is interesting to take a closer look at the above objective and compare with the common MLE training. Specifically, we notice the relations between the optimal Q * , V * , and A * functions: A * (s t , a t ) = Q * (s t , a t ) \u2212 V * (s t ) = r t + \u03b3V * (s t+1 ) \u2212 V * (s t ), where the first equation is the definition of A * (see Eq.5) and the second equation is due to Eqs.(10) and (5). We thus can see the regression target in the above objective as an approximation to the advantage function:\u00c3\u03b8 (s t , a t ) := \u2212V\u03b8 (s t ) + \u03b3V\u03b8 (s t+1 ) + r t . Therefore, by optimizing the regression objective, log \u03c0 \u03b8 (a t |s t ), which is the log probability of generating token a t given preceding tokens s t , is encouraged to match the approximate advantage valu\u1ebd A\u03b8 (s t , a t ), no more and no less. This is different from the objective of MLE where the model is trained to (blindly) increase the probability of the observed token a t given s t and decrease the probability of the rest.A.3.2 Vanilla Training with Temporal ConsistencyMuch like the Bellman temporal consistency in standard Q-learning, in SQL, the optimal actionvalue function follows the softmax form of the temporal consistency(Ziebart et al., 2008;Ziebart, 2010;Fox et al., 2016;Nachum et al., 2017):exp Q * (st+1, at+1) .We thus can derive a regression objective similar to the standard Q-learning (Eq.2):LSQL, vanilla(\u03b8)= E \u03c0 0.5 \u00b7 rt + \u03b3 logRecall that \u03c0 is an arbitrary behavior policy (e.g., data distribution), and Q\u03b8 is the target Q-network which is a slow copy of the Q \u03b8 to be learned and is held fixed during the gradient updates. However, the above objective is inefficient due to exact the same reasons as in standard Q-learning discussed earlier, namely the unstable per-step bootstrappingstyle training with sparse reward signals, plus the slow updates w.r.t only one token a t out of the large vocabulary (action space).Input: families waiting in line at an amusement park for their turn to ride . Generated: families at a amusement park .Input: man in a black suit , white shirt and black bowtie playing an instrument with the rest of his symphony surrounding him . Generated: a man is playing music .Input: a white dog with long hair jumps to catch a red and green toy . Generated: a dog is jumping Input: a man in a black shirt is playing golf outside . Generated: a man is playing golf Input: a man wearing sunglasses is sitting on the steps outside , reading a magazine . Generated: a man is sitting outside .Input: a young child is jumping into the arms of a woman wearing a black swimming suit while in a pool . Generated: a child is jumping into a pool .Input: a carefully balanced male stands on one foot near a clean ocean beach area . Generated: a man is on the beach .Input: two young boys in swimming trunks are in the water looking at each other . Generated: two boys are in the water .Input: a woman riding a bicycle past a car and a group of people on a sidewalk . Generated: a woman rides a bicycle .Input: a female sings and plays into a microphone and a male in green striped shorts plays a hand drum Generated: a woman is playing music .Input: a little girl wearing a cardboard diner hat is finishing off some onion rings at a restaurant . Generated: a little girl is at a restaurant .Input: young woman celebrates getting a strike during a bowling game . Generated: a young woman is present Input: black greyhound dog racing down a track . Generated: a dog runs down a track .Input: several men on stage having a discussion . Generated: men are talking on stage . legal: legal space religion and space In summary, a good understanding of these concepts is that by giving an explicit understanding to a person, they provide an avenue to be studied and studied. But the concept of one person being a space is also very confusing, and can be very difficult to obtain.\\n So, politics: the primary referendum is In summary, the outcome will be a referendum on the EU membership for the first time of its kind for EU citizens, and the full extent of the benefits of a single market and a flexible single EU state.\" computers: macintoshintoshintoshintosh In summary, it appears that the company and IBM products are currently in need of upgrading the computer. This can be seen in a detailed review of the Macintosh version of Windows Vista and XP. However, when looking at the changes made by the HP Macintosh hardware and software versions of space: legal space science and space In summary:\\n\\n The purpose of this paper is to investigate and test the theory of space space and other objects. This project will support NASA.s efforts to demonstrate these theories, and to propose other relevant new theories.\\n This paper provides the following arguments for the religion: space legal religion religion religion In summary, to the author the current discussion is the position of the Church and the community. While we acknowledge that we should not be commenting upon claims such as our recent cases or the other ones that contradict our view, we conclude it is appropriate to include these cases. Further science: the chemical microscope is In summary, the most sophisticated of these experiments is a technique that gives no obvious, no apparent way of revealing that the material was obtained. In this study, we examine how the compounds in the samples in question make up the composition of the chemical and its properties. The chemical composition military: arms defense battalion battalion cavalry In summary: 6th Panzer Field Division, Second Division.\\n\\n The main task of the battalion in the main counterinsurgency campaign was to counter the enemy in any counter-incursion. The main objective of this campaign is to eliminate enemy groups and the remnants of legal: legal space religion and space This essay discusses the idea of space and time as a space, in both theoretical and conceptual terms, as not an individual time period or anything else. The emphasis is on time itself, rather than having a fixed central space. Space was the object of the first chapter, and politics: the primary referendum is This essay discusses the nature of the EU referendum. The purpose of this essay is to shed light on the importance of a public referendum, on a question of whether the decision of an EU member states to remain in the European Union is constitutional and thus in accord with constitutional guarantees of sovereignty computers: macintoshintoshintoshintosh This essay discusses hardware devices and software systems for Mac OS X, MacOS X and Linux. To view the latest version of Macintosh OS: Mac 8.7.x\\n\\n For more information or for information about Macintosh systems, visit Mac MacSystems.\\n More space: legal space science and space This essay discusses science for teens, adults and teenagers.\\n\\n When the idea of studying space was first implemented as a method to test, the question was: What if a student has been \"comfortable\" with space without its body? What would their body like to be religion: space legal religion religion religion This essay discusses an alternative religion that focuses on the role of a particular religion and views some form of religious ethics as the form when the law is applied to that particular religious community . This discussion is concerned with the status of faith for individuals or groups which may be members and members science: the chemical microscope is This essay discusses the mechanisms of reaction with a focus on the molecular structure of nucleite and of enzymes within the cytoskeleton, thus making it easier to understand the process of metabolism and other elements of cellular life. In this essay, we use techniques such as the photochemical transfer military: arms defense battalion battalion cavalry This essay discusses three main themes:\\n\\n 1) Lack of uniformed soldiers is an unacceptable and unconscionable strategy for the Army.\\n 2) Poor and inadequate training does not compensate the soldiers, and may deprive them of the necessary and competitive training from their instructors legal: there liable injunction In summary, the court decided that defendants had a right to petition the high court to intervene, and therefore granted the injunction. However, a Court of Appeal decision in the US District Court ruled with no effect on the petitioned by the plaintiffs, citing two reasons.\\n\\n The US politics: the primary referendum was In summary, the majority of Russians would support the idea of banning all human rights, and that would be a major part of the government's effort to build a new liberal economy, which President Vladimir Putin says would boost Russian tourism.\\n\\n The results were published in the computers: the macintoshintosh In summary, the kernel has the best quality of life, and kernel security is at the heart of any OS development. The first time Linux released was when i386 released the Macintosh version, which had a rather low standard. Other issues like the introduction of the OS6 operating system space: comet asteroid spaceship In summary, this asteroid asteroid ship is about to make its maiden flight to explore deep-space.\\n\\n \"The asteroid craft was a very early attempt at the discovery of a new home for the world's first black hole,\" NASA said in a news release. \"Today we religion: faith faith faith salvation In summary, Christian beliefs are not a new way to use the time spent thinking about God's world as a source for faith. Faith is an effort to think of the world without fear that it might become a dangerous place for the human family. Because it represents the very essence that science: climate research chemistry In summary of the study, this review aims to determine how in a single study where the same number of data was analysed, a new methodology is needed to better understand who produced a different graph than the one suggested. The paper will be published in issue #5, Issue #18. military: the cavalry battalion a In summary, the army are a unit of the same type and in all, so there is no need to declare one. The unit does not constitute a cavalry unit or for use on troops.\\n\\n The army is not under the command of a brigade from the front. For legal: there liable injunction This essay discusses the potential legal consequences of a stay in the United States for an indefinite period of time if the government continues to delay the process of de-instituting it. To apply such a request, all applicable laws shall apply either the same terms as the existing statutes. In politics: the primary referendum was This essay discusses the electoral strategy against a candidate for governor of the Commonwealth.\\n\\n The survey of British voters in this survey provides an overview of what the candidates for the United Kingdom will be seeking in the next Parliament. In the general election a few seats will lead up to a computers: the macintoshintosh This essay discusses the various problems of the Macintosh, the first two-year running environment. An early version of this paper was originally published in 1982. The MacSX was not designed and managed by Kia.\\n\\n Macintosh\\n The mac has been a family invention space: comet asteroid spaceship This essay discusses a topic: the impact of two of the Earth's two-thirds comet-sized moon Charon on Earth, and why asteroids are so close to the sun; why people are looking for ways to find a way to keep Earth-shaped asteroids out of orbit. religion: faith faith faith salvation This essay discusses the impact religion has on the American experience and in American culture. Since the beginning of my career I have found that faith and belief have often been linked to economic growth, social development and education. I believe that all people need to know that there is no reason for science: climate research chemistry This essay discusses the role of molecular information and its interaction with the general organism and human health.\\n\\n \"The idea of biological information is not really a new concept. We used genetic information as a medium to define, identify, and store information about biology and biology,\" explains Dr. military: the cavalry battalion a This essay discusses the potential for the development of a small infantry brigade as an infantry regiment. It is also a contribution to the larger cavalry corps as it would require a larger brigade for battle. For more information see the original article on this page.  legal: In summary we have published in the journal Nature Neuroscience: A systematic review of human brain tissue has found no evidence for any association between the presence of the presence of a particular form of human neuropathy in the brain, a condition that is not normally associated with cognitive impairment. We found that politics: In summary we have a list of 10 of the best and most common types of drugs for people with HIV. This is a very short list of recommendations from a national and international community.\\n\\n\\n\\n This article has been updated to make the official state of the EU state of computers: In summary, we believe that the current system has no way of doing anything about it.\\n\\n\\n\\n The following steps are taken to get the system working.\\n\\n 1. Install a new operating system with a Linux Mint operating system\\n 2. Start a new Linux Mint operating space: In summary we have some important news from the moment of the year and some important information about these two major planets. This new discovery is the first to confirm this important planet has an active life in its home planet, a planet with a mass of about 5.8 billion tons. It religion: In summary, we believe that the current administration has no way of doing anything about the Benghazi attacks. This is a very interesting story, and I think it has been a very nice surprise. This is a very nice and well thought out piece that is a must for the science: In summary we use this approach to evaluate if the number of data points (in the dataset) that are relevant for each data set is the same (in this case, the data are not in one data set). In this approach we can test the data points in a different way. military: In summary we have some important news from the moment of the year and some important information from the moment of the year.\\n\\n\\n\\n\\n We've also added an additional update for our new feature, which includes:\\n \u2022 Improved access and access in all of the main legal: This essay discusses how you can build a community of dedicated people. If you're a member of a community of people who want to contribute to the environment, you'll also be helping them build communities in order to support the local economy, and the future of the city. The latest report politics: This essay discusses how we can build on previous research findings about the role religion plays in human development in human development. This is a very interesting and highly entertaining story. What is an \"independent\" political party in the United States, the U.S. political party, and the United computers: This essay discusses how you can build a new browser to view and share your favorite web sites.\\n\\n\\n A browser that is open source can also be built from a web browser, which can be a browser that does not allow browser extensions (e.g. Firefox, Chrome, Opera space: This essay discusses how you can build a life with a healthy diet and how you can use it when you're ready to move forward. It's a very simple approach to building a life with a healthy diet and what it means to be healthy and healthy for the religion: This essay discusses how you can build a new game without having to play the original game, and how you can make a new title that is completely different to the original. It has been around since 2007, when the first game, The Elder Scrolls IV: Oblivion, was released in the PlayStation science: This essay discusses how we can build on previous research findings about the role of obesity in human metabolism and how we can improve our health.\\n\\n\\n\\n In this essay, we explore why eating a whole whole diet does not help prevent obesity (1). We find that a whole food diet military: This essay discusses how you can build a community with the help of friends and family.\\n\\n\\n\\n\\n \"The people around me are the ones who need help. They are the ones who need help. They are the ones who are not alone.\"\\n -Michael\\n \"It'sTable A.9: Prompt samples from PPLM.\nGenerating label cohesive and wellformed adversarial claims. Pepa Atanasova, Dustin Wright, Isabelle Augenstein, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Pepa Atanasova, Dustin Wright, and Isabelle Augen- stein. 2020. Generating label cohesive and well- formed adversarial claims. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3168-3177.\n\nAn actor-critic algorithm for sequence prediction. Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron Courville, Yoshua Bengio, arXiv:1607.07086arXiv preprintDzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron Courville, and Yoshua Bengio. 2016. An actor-critic algorithm for sequence prediction. arXiv preprint arXiv:1607.07086.\n\nA large annotated corpus for learning natural language inference. Samuel Bowman, Gabor Angeli, Christopher Potts, Christopher D Manning, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingSamuel Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632-642.\n", "annotations": {"author": "[{\"end\":129,\"start\":74},{\"end\":169,\"start\":130},{\"end\":227,\"start\":170},{\"end\":346,\"start\":228},{\"end\":373,\"start\":347}]", "publisher": null, "author_last_name": "[{\"end\":81,\"start\":78},{\"end\":139,\"start\":136},{\"end\":184,\"start\":181},{\"end\":239,\"start\":235},{\"end\":357,\"start\":355}]", "author_first_name": "[{\"end\":77,\"start\":74},{\"end\":135,\"start\":130},{\"end\":180,\"start\":170},{\"end\":232,\"start\":228},{\"end\":234,\"start\":233},{\"end\":354,\"start\":347}]", "author_affiliation": "[{\"end\":128,\"start\":101},{\"end\":168,\"start\":141},{\"end\":213,\"start\":186},{\"end\":226,\"start\":215},{\"end\":286,\"start\":259},{\"end\":299,\"start\":288},{\"end\":345,\"start\":301},{\"end\":372,\"start\":359}]", "title": "[{\"end\":71,\"start\":1},{\"end\":444,\"start\":374}]", "venue": null, "abstract": "[{\"end\":1722,\"start\":446}]", "bib_ref": "[{\"end\":2308,\"start\":2290},{\"end\":2326,\"start\":2308},{\"end\":2345,\"start\":2326},{\"end\":2362,\"start\":2345},{\"end\":2502,\"start\":2480},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2525,\"start\":2502},{\"end\":3226,\"start\":3210},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3272,\"start\":3249},{\"end\":3292,\"start\":3272},{\"end\":3386,\"start\":3364},{\"end\":3402,\"start\":3386},{\"end\":3422,\"start\":3402},{\"end\":3439,\"start\":3422},{\"end\":3465,\"start\":3439},{\"end\":3485,\"start\":3465},{\"end\":4051,\"start\":4029},{\"end\":4067,\"start\":4051},{\"end\":4344,\"start\":4325},{\"end\":4362,\"start\":4344},{\"end\":4385,\"start\":4362},{\"end\":4428,\"start\":4417},{\"end\":4448,\"start\":4428},{\"end\":4472,\"start\":4448},{\"end\":4989,\"start\":4966},{\"end\":5011,\"start\":4989},{\"end\":5237,\"start\":5216},{\"end\":9694,\"start\":9672},{\"end\":10335,\"start\":10318},{\"end\":10356,\"start\":10335},{\"end\":10518,\"start\":10499},{\"end\":11020,\"start\":11001},{\"end\":11639,\"start\":11619},{\"end\":12734,\"start\":12711},{\"end\":12756,\"start\":12734},{\"end\":12776,\"start\":12756},{\"end\":12871,\"start\":12852},{\"end\":12894,\"start\":12871},{\"end\":13648,\"start\":13625},{\"end\":13670,\"start\":13648},{\"end\":14704,\"start\":14692},{\"end\":14829,\"start\":14806},{\"end\":14851,\"start\":14829},{\"end\":15674,\"start\":15653},{\"end\":15849,\"start\":15829},{\"end\":18149,\"start\":18128},{\"end\":18922,\"start\":18906},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":21387,\"start\":21366},{\"end\":22812,\"start\":22791},{\"end\":22835,\"start\":22812},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":24440,\"start\":24417},{\"end\":24873,\"start\":24852},{\"end\":24890,\"start\":24873},{\"end\":24912,\"start\":24890},{\"end\":25669,\"start\":25648},{\"end\":25686,\"start\":25669},{\"end\":25708,\"start\":25686},{\"end\":26496,\"start\":26479},{\"end\":26517,\"start\":26496},{\"end\":26536,\"start\":26517},{\"end\":28827,\"start\":28805},{\"end\":28846,\"start\":28827},{\"end\":30066,\"start\":30043},{\"end\":30086,\"start\":30066},{\"end\":30108,\"start\":30086},{\"end\":30258,\"start\":30236},{\"end\":30282,\"start\":30258},{\"end\":30302,\"start\":30282},{\"end\":30329,\"start\":30302},{\"end\":33182,\"start\":33161},{\"end\":34017,\"start\":33990},{\"end\":34050,\"start\":34022},{\"end\":34124,\"start\":34101},{\"end\":35650,\"start\":35627},{\"end\":36632,\"start\":36630},{\"end\":37528,\"start\":37507},{\"end\":37545,\"start\":37528},{\"end\":37567,\"start\":37545},{\"end\":37649,\"start\":37627},{\"end\":38032,\"start\":38009},{\"end\":38053,\"start\":38032},{\"end\":39014,\"start\":38990},{\"end\":40610,\"start\":40591},{\"end\":40614,\"start\":40612},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":63847,\"start\":63826}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":41473,\"start\":40797},{\"attributes\":{\"id\":\"fig_1\"},\"end\":41863,\"start\":41474},{\"attributes\":{\"id\":\"fig_2\"},\"end\":42087,\"start\":41864},{\"attributes\":{\"id\":\"fig_3\"},\"end\":43044,\"start\":42088},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":43364,\"start\":43045},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":44009,\"start\":43365},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":44167,\"start\":44010},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":44218,\"start\":44168},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":45202,\"start\":44219},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":61476,\"start\":45203},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":62062,\"start\":61477},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":64513,\"start\":62063},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":64867,\"start\":64514},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":66984,\"start\":64868}]", "paragraph": "[{\"end\":2674,\"start\":1738},{\"end\":4068,\"start\":2676},{\"end\":4739,\"start\":4070},{\"end\":5627,\"start\":4741},{\"end\":6627,\"start\":5629},{\"end\":7409,\"start\":6629},{\"end\":7642,\"start\":7439},{\"end\":7698,\"start\":7697},{\"end\":7703,\"start\":7700},{\"end\":8496,\"start\":7705},{\"end\":8669,\"start\":8498},{\"end\":9265,\"start\":8700},{\"end\":10684,\"start\":9267},{\"end\":11251,\"start\":10741},{\"end\":12293,\"start\":11253},{\"end\":12655,\"start\":12327},{\"end\":13671,\"start\":12695},{\"end\":14457,\"start\":13722},{\"end\":14896,\"start\":14552},{\"end\":15185,\"start\":14984},{\"end\":16580,\"start\":15230},{\"end\":16778,\"start\":16629},{\"end\":17804,\"start\":16836},{\"end\":18326,\"start\":17806},{\"end\":18628,\"start\":18401},{\"end\":19041,\"start\":18706},{\"end\":19627,\"start\":19043},{\"end\":19849,\"start\":19660},{\"end\":20013,\"start\":19968},{\"end\":20662,\"start\":20015},{\"end\":21281,\"start\":20702},{\"end\":22057,\"start\":21283},{\"end\":22504,\"start\":22059},{\"end\":23355,\"start\":22506},{\"end\":24096,\"start\":23357},{\"end\":25024,\"start\":24130},{\"end\":26151,\"start\":25026},{\"end\":26670,\"start\":26227},{\"end\":26820,\"start\":26690},{\"end\":26968,\"start\":26822},{\"end\":27348,\"start\":26970},{\"end\":28847,\"start\":27350},{\"end\":29364,\"start\":28849},{\"end\":29840,\"start\":29366},{\"end\":30556,\"start\":29857},{\"end\":31014,\"start\":30558},{\"end\":31471,\"start\":31029},{\"end\":32519,\"start\":31487},{\"end\":33618,\"start\":32540},{\"end\":33762,\"start\":33661},{\"end\":33958,\"start\":33764},{\"end\":34745,\"start\":33960},{\"end\":35214,\"start\":34747},{\"end\":35564,\"start\":35216},{\"end\":35784,\"start\":35586},{\"end\":36444,\"start\":35786},{\"end\":38054,\"start\":36474},{\"end\":38378,\"start\":38056},{\"end\":40796,\"start\":38408}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7696,\"start\":7643},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8699,\"start\":8670},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10740,\"start\":10685},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13721,\"start\":13672},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14551,\"start\":14458},{\"attributes\":{\"id\":\"formula_5\"},\"end\":14983,\"start\":14897},{\"attributes\":{\"id\":\"formula_6\"},\"end\":16628,\"start\":16581},{\"attributes\":{\"id\":\"formula_7\"},\"end\":16835,\"start\":16779},{\"attributes\":{\"id\":\"formula_8\"},\"end\":18400,\"start\":18327},{\"attributes\":{\"id\":\"formula_9\"},\"end\":18705,\"start\":18629},{\"attributes\":{\"id\":\"formula_10\"},\"end\":19967,\"start\":19850}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":23100,\"start\":23093},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":28299,\"start\":28292},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":29839,\"start\":29832},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":34763,\"start\":34756},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":40622,\"start\":40615}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1736,\"start\":1724},{\"attributes\":{\"n\":\"2\"},\"end\":7437,\"start\":7412},{\"attributes\":{\"n\":\"3\"},\"end\":12325,\"start\":12296},{\"attributes\":{\"n\":\"3.1\"},\"end\":12693,\"start\":12658},{\"attributes\":{\"n\":\"3.2\"},\"end\":15228,\"start\":15188},{\"attributes\":{\"n\":\"4\"},\"end\":19658,\"start\":19630},{\"attributes\":{\"n\":\"4.1\"},\"end\":20700,\"start\":20665},{\"attributes\":{\"n\":\"4.2\"},\"end\":24128,\"start\":24099},{\"end\":26162,\"start\":26154},{\"attributes\":{\"n\":\"4.3\"},\"end\":26225,\"start\":26165},{\"end\":26688,\"start\":26673},{\"attributes\":{\"n\":\"5\"},\"end\":29855,\"start\":29843},{\"attributes\":{\"n\":\"6\"},\"end\":31027,\"start\":31017},{\"end\":31485,\"start\":31474},{\"end\":32538,\"start\":32522},{\"end\":33659,\"start\":33621},{\"end\":35584,\"start\":35567},{\"end\":36472,\"start\":36447},{\"end\":38406,\"start\":38381},{\"end\":40808,\"start\":40798},{\"end\":41875,\"start\":41865},{\"end\":44020,\"start\":44011},{\"end\":44178,\"start\":44169},{\"end\":61487,\"start\":61478}]", "table": "[{\"end\":43364,\"start\":43151},{\"end\":44009,\"start\":43867},{\"end\":44167,\"start\":44120},{\"end\":45202,\"start\":44584},{\"end\":61476,\"start\":45543},{\"end\":62062,\"start\":61530},{\"end\":64513,\"start\":64405},{\"end\":64867,\"start\":64808},{\"end\":66984,\"start\":65183}]", "figure_caption": "[{\"end\":41473,\"start\":40810},{\"end\":41863,\"start\":41476},{\"end\":42087,\"start\":41877},{\"end\":43044,\"start\":42090},{\"end\":43151,\"start\":43047},{\"end\":43867,\"start\":43367},{\"end\":44120,\"start\":44022},{\"end\":44218,\"start\":44180},{\"end\":44584,\"start\":44221},{\"end\":45543,\"start\":45205},{\"end\":61530,\"start\":61489},{\"end\":64405,\"start\":62065},{\"end\":64808,\"start\":64516},{\"end\":65183,\"start\":64870}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":2547,\"start\":2538},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4848,\"start\":4840},{\"end\":12599,\"start\":12591},{\"end\":13805,\"start\":13802},{\"end\":16032,\"start\":16024},{\"end\":17103,\"start\":17095},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":23374,\"start\":23366},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":26117,\"start\":26109},{\"end\":26741,\"start\":26733},{\"end\":26868,\"start\":26860},{\"end\":27235,\"start\":27227},{\"end\":28203,\"start\":28195},{\"end\":35071,\"start\":35063},{\"end\":35348,\"start\":35319},{\"end\":38629,\"start\":38621},{\"end\":39058,\"start\":39050}]", "bib_author_first_name": "[{\"end\":84634,\"start\":84630},{\"end\":84652,\"start\":84646},{\"end\":84669,\"start\":84661},{\"end\":85159,\"start\":85152},{\"end\":85178,\"start\":85170},{\"end\":85193,\"start\":85187},{\"end\":85205,\"start\":85198},{\"end\":85217,\"start\":85213},{\"end\":85230,\"start\":85224},{\"end\":85244,\"start\":85239},{\"end\":85262,\"start\":85256},{\"end\":85587,\"start\":85581},{\"end\":85601,\"start\":85596},{\"end\":85621,\"start\":85610},{\"end\":85642,\"start\":85629}]", "bib_author_last_name": "[{\"end\":84644,\"start\":84635},{\"end\":84659,\"start\":84653},{\"end\":84680,\"start\":84670},{\"end\":85168,\"start\":85160},{\"end\":85185,\"start\":85179},{\"end\":85196,\"start\":85194},{\"end\":85211,\"start\":85206},{\"end\":85222,\"start\":85218},{\"end\":85237,\"start\":85231},{\"end\":85254,\"start\":85245},{\"end\":85269,\"start\":85263},{\"end\":85594,\"start\":85588},{\"end\":85608,\"start\":85602},{\"end\":85627,\"start\":85622},{\"end\":85650,\"start\":85643}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":221761373},\"end\":85099,\"start\":84569},{\"attributes\":{\"doi\":\"arXiv:1607.07086\",\"id\":\"b1\"},\"end\":85513,\"start\":85101},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":14604520},\"end\":86063,\"start\":85515}]", "bib_title": "[{\"end\":84628,\"start\":84569},{\"end\":85579,\"start\":85515}]", "bib_author": "[{\"end\":84646,\"start\":84630},{\"end\":84661,\"start\":84646},{\"end\":84682,\"start\":84661},{\"end\":85170,\"start\":85152},{\"end\":85187,\"start\":85170},{\"end\":85198,\"start\":85187},{\"end\":85213,\"start\":85198},{\"end\":85224,\"start\":85213},{\"end\":85239,\"start\":85224},{\"end\":85256,\"start\":85239},{\"end\":85271,\"start\":85256},{\"end\":85596,\"start\":85581},{\"end\":85610,\"start\":85596},{\"end\":85629,\"start\":85610},{\"end\":85652,\"start\":85629}]", "bib_venue": "[{\"end\":84857,\"start\":84778},{\"end\":85811,\"start\":85740},{\"end\":84776,\"start\":84682},{\"end\":85150,\"start\":85101},{\"end\":85738,\"start\":85652}]"}}}, "year": 2023, "month": 12, "day": 17}