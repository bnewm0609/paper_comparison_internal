{"id": 155109576, "updated": "2022-02-11 19:07:15.251", "metadata": {"title": "A Binary Learning Framework for Hyperdimensional Computing", "authors": "[{\"first\":\"Mohsen\",\"last\":\"Imani\",\"middle\":[]},{\"first\":\"John\",\"last\":\"Messerly\",\"middle\":[]},{\"first\":\"Fan\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Wang\",\"last\":\"Pi\",\"middle\":[]},{\"first\":\"Tajana\",\"last\":\"Rosing\",\"middle\":[]}]", "venue": "2019 Design, Automation & Test in Europe Conference & Exhibition (DATE)", "journal": "2019 Design, Automation & Test in Europe Conference & Exhibition (DATE)", "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "Brain-inspired Hyperdimensional (HD) computing is a computing paradigm emulating a neuron\u2019s activity in high-dimensional space. In practice, HD first encodes all data points to high-dimensional vectors, called hypervectors, and then performs the classification task in an efficient way using a well-defined set of operations. In order to provide acceptable classification accuracy, the current HD computing algorithms need to map data points to hypervectors with non-binary elements. However, working with non-binary vectors significantly increases the HD computation cost and the amount of memory requirement for both training and inference. In this paper, we propose BinHD, a novel learning framework which enables HD computing to be trained and tested using binary hypervectors. BinHD encodes data points to binary hypervectors and provides a framework which enables HD to perform the training task with significantly low resources and memory footprint. In inference, BinHD binarizes the model and simplifies the costly Cosine similarity used in existing HD computing algorithms to a hardware-friendly Hamming distance metric. In addition, for the first time, BinHD introduces the concept of learning rate in HD computing which gives an extra knob to the HD in order to control the training efficiency and accuracy. We accordingly design a digital hardware to accelerate BinHD computation. Our evaluations on four practical classification applications show that BinHD in training (inference) can achieve 12.4\u00d7 and 6.3\u00d7 (13.8\u00d7 and 9.9\u00d7) energy efficiency and speedup as compared to the state-of-the-art HD computing algorithm while providing the similar classification accuracy.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2945276917", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/date/ImaniMWPR19", "doi": "10.23919/date.2019.8714821"}}, "content": {"source": {"pdf_hash": "6cc93f63003834dc642b2988c56233152540bdac", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "ec335ebdb877038bae2e82cc1d2ff73f53ecb281", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/6cc93f63003834dc642b2988c56233152540bdac.txt", "contents": "\nA Binary Learning Framework for Hyperdimensional Computing\n\n\nMohsen Imani moimani@ucsd.edu \nComputer Science and Engineering Department\nUC San Diego\nLa Jolla92093CAUSA\n\nJohn Messerly jmesserl@ucsd.edu \nComputer Science and Engineering Department\nUC San Diego\nLa Jolla92093CAUSA\n\nFan Wu \nDepartment of Computer Science and Engineering\nUC Riverside\n92521RiversideCA\n\nWang Pi piwang@pku.edu.cn \nSchool of Electronics Engineering and Computer Science\nPeking University\n100871BeijingP.R.China\n\nTajana Rosing tajana@ucsd.edu \nComputer Science and Engineering Department\nUC San Diego\nLa Jolla92093CAUSA\n\nA Binary Learning Framework for Hyperdimensional Computing\nIndex Terms-Brain-inspired computingEnergy-efficiency classificationHyperdimensional computing\nBrain-inspired Hyperdimensional (HD) computing is a computing paradigm emulating a neuron's activity in highdimensional space. In practice, HD first encodes all data points to high-dimensional vectors, called hypervectors, and then performs the classification task in an efficient way using a well-defined set of operations. In order to provide acceptable classification accuracy, the current HD computing algorithms need to map data points to hypervectors with non-binary elements. However, working with non-binary vectors significantly increases the HD computation cost and the amount of memory requirement for both training and inference. In this paper, we propose BinHD, a novel learning framework which enables HD computing to be trained and tested using binary hypervectors. BinHD encodes data points to binary hypervectors and provides a framework which enables HD to perform the training task with significantly low resources and memory footprint. In inference, BinHD binarizes the model and simplifies the costly Cosine similarity used in existing HD computing algorithms to a hardware-friendly Hamming distance metric. In addition, for the first time, BinHD introduces the concept of learning rate in HD computing which gives an extra knob to the HD in order to control the training efficiency and accuracy. We accordingly design a digital hardware to accelerate BinHD computation. Our evaluations on four practical classification applications show that BinHD in training (inference) can achieve 12.4\u00d7 and 6.3\u00d7 (13.8\u00d7 and 9.9\u00d7) energy efficiency and speedup as compared to the state-of-the-art HD computing algorithm while providing the similar classification accuracy.\n\nI. INTRODUCTION\n\nDeep Learning has risen to prominence in complex and big data applications. For example, Deep Neural Networks (DNNs) have provided high classification accuracy for complex image classification tasks [1], [2]. However, high computational complexity and memory requirement of DNNs hinder usability to a broad variety of real-life (embedded) applications where the device resources and power budget is limited [3]- [5]. For example, in health care we often require learning algorithms to have a real time control on the patient daily behaviour, speech, and bio-medical sensors [6]- [8]. Sending all data point to the powerful computing environment, e.g., cloud, cannot guarantee scalability and real-time response. It is also often undesirable due to privacy and security concerns [9]- [11]. Thus, we need alternative computing methods that can run a large amount of data at least partly on the less-powerful embedded devices.\n\nBrain-inspired Hyperdimensional (HD) computing has been proposed as a computing method that processes the cognitive tasks in a more light-weight way [12]. HD performs computa-tion on ultra-wide words -that is, with very high-dimensional vectors, or hypervectors. HD works based on the existence of a huge number of nearly orthogonal hypervectors which can be combined using well-defined vector space operations. The mathematics governing the high dimensional space enable HD to be easily applied to different learning problems [13]- [19]. The first step in HD computing is to encode/map data points from the original domain to high-dimensional space. In training, HD combines the encoded hypervectors in order to generate a hypervector representing each class. The classification task at inference performs by checking the similarity of an encoded test hypervector with all trained classes.\n\nIn this work, we observe that in order to provide acceptable classification accuracy, the current HD computing algorithms need to encode data points to hypervectors with non-binary elements [18], [20]- [23]. This means that even to perform a single addition between the hypervectors, HD needs to compute thousands (e.g., 10,000) operations. For example, in HD computing, the training performs by the accumulation of several hypervectors. This makes the training operations very expensive. In addition, to perform iterative training, BinHD needs to store all encoded hypervectors which significantly increases the memory footprint. After training, the existing HD computing algorithms generate a model with non-binarized class hypervectors. This model forces the HD inference to use costly Cosine metric for similarity check, which involves a large amount of non-binarized additions/multiplications. To reduce the inference cost, prior work tried to binarize the class hypervectors after the training [20], [21], [24]. Although this approach simplifies the inference similarity metric to Hamming distance, we observed that it significantly reduces the HD classification accuracy on practical applications, e.g., 11.8% on face recognition application [25].\n\nIn this paper, we propose BinHD, a novel framework which enables HD computing to be trained and tested using binary hypervectors. BinHD introduces the concept of learning rate in HD computing by assigning a counter with limited bit-width to each HD model element. BinHD performs training by the accumulation of the binary hypervectors which can process with significantly lower memory footprint and the computation cost. In inference, BinHD removes the necessity of using a non-binary model and costly Cosine similarity by creating a binarized model. This simplifies the HD similarity metric to hardware-friendly Hamming distance. We accordingly design a digital hardware to accelerate BinHD computation in both training and inference. Our evaluation on four practical classi-fication applications shows that BinHD in training (inference) can achieve on average 12.4\u00d7 and 6.3\u00d7 (13.8\u00d7 and 9.9\u00d7) energy efficiency and speedup as compared to baseline HD computing algorithm while providing the similar classification accuracy. In addition, we observe that BinHD can provide up to 10.8% higher classification accuracy as compared to the baseline HD computing algorithms [18], [20] using the similar binary hypervectors.\n\nII. PROPOSED BINHD TRAINING Here, we proposed BinHD, a framework for binarization of the HD computation during training and inference. Figure 1a shows an overview of BinHD performing the classification task on high-dimensional space. In BinHD, the first step is to map/encode all data points from original to a hypervector, where each element represents using a binary value. Next, the encoded hypervectors are combined in a training module in order to create a single binary hypervector representing each class. In the inference, a test data encodes to high-dimensional space using the same encoding module used for training. Finally, the classification task performs by finding a pre-stored class hypervector which has the highest similarity with the test hypervector. Since BinHD works with a binary model, it enables the inference to use hardware-friendly Hamming distance as a similarity metric. In the following, we explain the details of the HD functionality.\n\n\nA. BinHD Encoding\n\nBinHD functionality is independent to the encoding module. Here, we consider a general encoding approach which maps a feature vector [12], [18]. Figure 1b shows an overview of the encoding module. This encoding finds the minimum and maximum feature values and quantizes that range into m levels. Then, it assigns a random binary hypervector with D dimension to each quantized level {L 1 ,..., L m }. The level hypervectors are generated such the the neighbor levels have higher similarity, as their absolute values have closer distance [18]. In addition, the encoding module assigns a random binary hypervector to each existing feature index, {ID 1 ,..., ID n }, where ID \u2208 {0, 1} D . The encoding can happen by linearly combining the feature values over different indices, where a hypervector corresponding to a feature index preserves the position of each feature value in a combined set:\nF = { f 1 , f 2 ,..., f n }, with n features ( f i \u2208 N) to high-dimensional vector H = {h 1 , h 2 ,..., h D } with D dimensions (h i \u2208 {0, 1} D )H = ID 1 \u2295 L 1 + ID 2 \u2295 L 2 + ... + ID n \u2295 L n .\nwhere H is the (non-binary) encoded hypervector, \u2295 denotes the XOR operation, and L i is the (binary) hypervector corresponding to the i-th feature of vector F. The binarization of the encoded hypervector can happen by comparing each dimension of H with n/2 value. All dimensions with a smaller value than n/2 are assigned to 0, while other elements are assigned to 1.\n\n\nB. BinHD Training\n\nBinary Vector Accumulation: Assume A and B are two binary vectors (A, B \u2208 {0, 1} D ), we define a similar accumulator operation as Sparse Distributed Memory [26], which satisfies the following constraints:  Figure 2 shows an example of the accumulation operation. The accumulation of A and B binary vectors performs in two steps. First, a counter update, where A C updates depending on B vector elements. The B elements with \"1\" value increment the A C counter, while \"0\" elements decrement A C . For the example shown in Figure 2b, we use N = 5, thus counter values saturates between -15 and +16 range. The second step is updating the binary vector A, depending on changes on A C . As Figure 2c shows, the value of the accumulator vector, A, flips on all dimensions that the counter values changed from positive to zero/negative or vice versa. We explore the impact of counter size on Section II-D.\n{A = A[+]B | A, A , B \u2208 {0, 1} D , \u03b4 (A, B) < \u03b4 (A , B)}\nInitial Model Generation: We accumulate all binary encoded hypervectors in training dataset to create k binary prototype vectors {C 1 , \u00b7,C k }, where k is number of classes and C i \u2208 {0, 1} D . These prototype vectors represent the average (centroid) of that particular class, with respect to Hamming space. We can view each class hypervector as a linear combination of the encoded hypervectors in that class. For example, i th class hypervector can be computed as:\nC i = \u2211 \u2200 j\u2208class i H j .\nAs we explained, the accumulation of the binary hypervectors happens by assigning a counter vector to each existing class (e.g., C count i for class i th ). The counter keeps track of the number of 0s and 1s in each class dimension and assigns C i dimension to 1 if the number of 1s exceeds 0s. After initialization, the model is ready for the classification. The inference is performed by taking the Hamming distance between the query data, and all k class hypervectors. We label the query as a class with the highest Hamming distance.\n\nHD model initialization comes with two advantages: first, by making a strong assumption about how each class is an average of the training data, our classifier is no longer a black box model that requires iterative convergence on the training data, but a prototype model that can be generated in a single pass. This initial model often provides acceptable accuracy, and gradient descent becomes an optional optimization, rather than a necessary obstacle. Second, for gradient descent, these initial model vectors give vital information about which dimensions are the more significant than others, which is information that would have otherwise been lost through binarization. We further discuss it on Section II-C.\n\n\nC. BinHD Model Adjustment\n\nWe can significantly reduce the error rate of the initial HD model by employing gradient descent. We propose an online stochastic approach to descent. Figure 3 shows the overview of BinHD functionality during model adjustment. BinHD first encodes all training data point into high-dimensional binary vectors (Figure 3a). For each incoming sample of training data, our approach attempts to classify it by measuring its Hamming distance with the trained model ( Figure 3b). If the model with the highest Hamming distance matches the correct label, BinHD ignores updating the model. However, if a train data point, H, incorrectly matches with HD model, we add the query to a correct class (C i ), while subtracting it from an incorrect one (C j ). BinHD model adjustment has two main advantages as compared to the existing HD computing algorithms [18], [21]. First, unlike existing approaches which need to store nonbinary encoded hypervectors, BinHD stores binary encoded hypervectors with significantly lower memory size. Second, BinHD exploits a binary model with Hamming distance similarity during retraining, while the computation of prior work depends on the non-binary model with costly Cosine similarity [21]. These facts make BinHD an efficient lightweight classification approach for embedded devices with limited resources.\nC i = C i [+]H & C j = C j [\u2212]H\n\nD. Controlling the Learning Rate\n\nThe learning rate is a crucial parameter in stochastic gradient descent that controls the step size. In most applications, the learning rate is a floating point value between 0 and 1 that is multiplied by training samples before they are added or subtracted from the vector of weights. The intuition is that this regulates how much individual training samples are allowed to move the classifier's hyperplane. Learning rates that are too small allow for the descent to get trapped in local optima while learning rates that are too large cause the descent to diverge.\n\nIn the Hamming Distance classifier's, such as BinHD, the learning rate is more difficult to define. The binary weights of the model vectors only flip when the counters cross the zero thresholds. This means that there is no guarantee that adding a single binary vector to the counters will take any immediate effect on the model. We choose to define our learning rate as the average number of bits flipped per accumulation operation. We modify this rate by saturating the counters with a reasonable ceiling and increasing the number of counter increments per accumulation. In fact, the size of a counter determines an application learning rate. Figure 4 shows the impact of counter size on the BinHD classification accuracy during the model adjustment iterations. The results are reported for activity recognition dataset [27] using three different counter sizes. BinHD using 32-bit counters results on average 2-5 bits flipped per accumulation. It means that the training samples are only allowed to modify the vector about 0.05% per accumulation. This is unsurprisingly low since using counters that can accommodate values as high as 6,213 or as low as -6,213 (the number of training samples in the set), some dimensions will take thousands of accumulations to flip. Convergence on the training data will take hundreds of epochs with such a low learning rate. BinHD can increase the learning rate by using a smaller counter size. For example, using counter size equal to 6-bits (N = 6) increases the learning rate and results in higher classification accuracy. In fact, 6-bits counter size is equivalent of using a learning rate of 5%, which results in a faster training. From the other side, using very small counter size, i.e., 4-bit, is equivalent to use very large learning rate which increases accuracy fluctuation and increases the chance of divergence.\n\nIII. HARDWARE IMPLEMENTATION The main computation of HD can perform using three main blocks: encoding, associative search and counter modules. Figure 5 shows the details implementation of these three blocks.\n\nA. Encoding Figure 5 shows the implementation of encoding module. Our approach stores all position (ID) and level (L) hypervectors in position and item memory blocks respectively ( \u2022 A ).\n\nAfter access to the feature values in the original domain, BinHD compares each feature value with the quantized feature values. Each feature is assigned to a quantized level which it has a minimum distance with. BinHD reads a level hypervector from the memory and XORs it with the position hypervector corresponding to that feature ( \u2022 B ). This process can perform in parallel for all features. The result of XOR operations are accumulated using D counter blocks and compared with a threshold value. In BinHD, a threshold value is the half of the number of features (T HR = n/2). This results in generating an encoded hypervector with D binary elements ( \u2022 C ).\n\n\nB. Training\n\nIn the existing HD computing algorithms, the training happens by accumulating the non-binarized hypervectors, resulting in large memory requirement and expensive computational cost [18]. BinHD implements training by accumulating all encoded hypervectors corresponding to a class. Since the encoded hypervectors are binary, this addition happens using the binary accumulation approach introduced in section II-B. BinHD uses a counter array for each corresponding class which keeps track of the number of 0 and 1 bits in each dimension ( Figure 5G). After updating the counter values for the entire training data, BinHD creates an initial training model by assigning any dimensions with positive value to 1, while counters with zero or negative values are assigned to 0.\n\n\nC. Associative Search\n\nIn inference and model adjustment, the associative search is the main cost of HD computing where we compare the similarity of an encoded hypervector with a binarized HD model. Unlike prior HD computing algorithms that use costly Cosine as the similarity metric, BinHD performs similarity check using Hamming distance. BinHD pre-stores the trained class hypervectors in a memory block ( \u2022 D ). The similarity check of a query and class hypervectors performs using an array of XOR gates ( \u2022 E ). Each XOR row computes the Hamming distance similarity of a query and class hypervector. A counter block has been located at the right side of the array is responsible to count the number of mismatches in each class. Finally, a tree-based comparator block identifies a class with the minimum Hamming distance ( \u2022 F ).\n\n\nD. Model Adjustment\n\nThe model adjustment can be implemented using the same XOR array used for similarity check and a counter array used for initial training ( \u2022 G ). As we explained in section II-C, BinHD uses a single N-bits counter to keep track of changes in each dimension of a class hypervector. For example, for an application with k classes and D dimensions, we require k \u00d7 D counters, where each counter corresponds to a single dimension of a class hypervector. Model adjustment block uses the same XOR array to check the similarity of a training data point with the HD model. Depending on the correctness of match, BinHD implements the model adjustment using the following steps: \u2022 If an encoded hypervector matches with a correct class, BinHD continues the search for the next data point without updating the counter array.   hypervector using the same memory block which stored the trained class hypervectors. Every element of an XOR vector with 1 (0) bit increments (decrements) the counter value of the corresponding class. BinHD also updates the counter values on a class that the data point belongs to by using a Tag control signal. BinHD XORs the encoded hypervector with the correct class hypervector. Depending on the result of XOR, we increment (decrement) the counter values on the dimensions that XOR results have 0 (1) bit. During the above update steps, if a sign of a counter changes, our approach fillips the corresponding dimensions of class hypervector. This update happens in two steps: detecting any changes in the counters corresponding to a class by \n\n\nIV. EVALUATION A. Experimental Setup\n\nWe verified the functionality of BinHD using both software and hardware implementations. In software, we implement HD training and inference on Intel Core i7 7600 CPU using an optimized C++ implementation. For a hardware implementation, we use a standard digital ASIC flow to design dedicated hardware. We describe HD functionality using RTL System-Verilog. For the synthesis, we use Synopsys Design Compiler with the TSMC 45 nm technology library, the general purpose process with high V T H cells. We extract its switching activity during post-synthesis simulations in ModelSim by applying the test sentences. We compare the BinHD efficiency and accuracy with the state-of-the-art HD computing algorithm proposed in [18], [21]. Table I summarizes the evaluated datasets. The tested benchmarks range from relatively small datasets collected in a small IoT network, e.g., UCIHAR, to a large dataset which includes hundreds of thousands of images of facial and non-facial data.\n\n\nB. BinHD vs Existing Algorithms\n\nTable II compares the classification accuracy of BinHD with the baseline HD computing algorithm [18] in three different configurations. First, the baseline HD using non-binary encoding and non-binary model (Float/Float) which provides the highest classification accuracy. Second, HD computing with non-binary encoding and binary model (Float/Binary). Third, HD computing with binary encoding and binary model (Binary/Binary) which has the maximum efficiency. Our evaluation shows that HD computing in Float/Float configuration provides on average 6.7% and 9.3% higher classification accuracy as compared to HD in Float/Binary and Binary/Binary configurations respectively. In fact, naively binarization of the encoded hypervector or HD model results in a significant drop in the classification accuracy. In contrast, our proposed BinHD framework binarizes both encoded and class hypervectors with minimal impact on the classification accuracy. Our evaluation shows that BinHD can provide the similar accuracy to the baseline HD in Float/Float configuration (less than 0.6%).\n\nTable II also compares proposed BinHD and the baseline HD computing algorithms in terms of training memory footprint and model size. The baseline HD computing algorithm encodes data points to non-binary hypervectors, thus they require large memory footprint during training. BinHD enables HD computing to work with binarized encoded hypervectors. Our evaluation shows that BinHD on average requires 24.6\u00d7 lower memory footprint as compared to the baseline HD using non-binary encoded hypervectors. In terms of model size, BinHD provides the same memory size as HD with the binarized mode, which is 32\u00d7 smaller than the baseline HD with the non-binary model. In summary, BinHD can provide the memory/computing efficiency of the fully binary HD (Binary/Binary) as well as the classification accuracy of the baseline HD with non-binary encoding and model (Float/Float). Table III lists the classification accuracy of BinHD when the counter width increases from 6-bits to 32-bits. Choosing the width of the counters, like choosing a learning rate, depends on the dataset size. Choosing a counter that is small does not provide enough memory for retraining to converge on large datasets. In another word, accumulating enough changes cause forgetfulness during the descent. Our results in Table III shows that using counters smaller than 10-bits results in a divergence of the FACE recognition application. Similarity, the accuracy of the other applications diverge when the counter is smaller than 4-bits. Choosing a counter width that is too large will not give the significant dimensions a high enough probability to be flipped. Therefore, the gradient descent will choose to fit the data based on insignificant features, leading to overfitting. Our evaluation shows that the best counter size is predictable depending on the dataset size. Depending on the number of train data, we should select a counter size which provides a learning rate of 5%. For example, for a large dataset such as FACE, BinHD requires to use 10-bits counter size, while for smaller datasets such as ISOLET and UCIHAR using 6-bit counters provides 5% learning rate.\n\n\nC. BinHD & Counter Size\n\n\nD. Training Efficiency\n\nHere, we compare the efficiency of BinHD and the baseline HD computing algorithm with Float/Float configuration which provides the similar accuracy as BinHD. The baseline HD encodes data point to the non-binary domain and then adds the encoded hypervectors in order to create each class hypervector. BinHD simplifies the training operation by performing accumulation of the binary encoded hypervectors. Figure 6 compares the energy consumption and execution time of BinHD and baseline HD running on digital hardware. Our evaluation shows that BinHD can provide 6.3\u00d7 faster and 12.4\u00d7 higher energy efficiency as compared to the baseline HD computing algorithm [18], while providing the similar classification accuracy. Figure 7a compares BinHD and the baseline HD computing algorithm during inference. Since BinHD uses a binary model, it can exploit a hardware-friendly Hamming distance as the similarity metric, while the baseline HD using non-binary model uses costly Cosine for the similarity check. Unlike Hamming distance, calculating Cosine similarity is so costly as it involves a large number of non-binary multiplications. Our evaluation shows that BinHD can achieve 13.8\u00d7 higher energy efficiency and 9.9\u00d7 speedup as compared to the baseline HD using non-binary model. Figure 7b shows the efficiency of BinHD and the baseline HD computing during a single iteration of model adjustment. The retraining consists of an associative search and model update. Similar to the inference, BinHD provides significantly higher efficiency than the baseline HD computing. This is because the retraining in HD performs by applying similarity check to the binary model, while the baseline HD needs to use costly Cosine similarity on the non-binarized model. The model update in BinHD can perform using bitwise XOR operation, while the baseline HD requires to perform nonbinary addition on two class hypervector. Our evaluation shows that BinHD can achieve on average 13.6\u00d7 speedup and 7.8\u00d7 higher energy efficiency as compared to the baseline HD computing algorithms.\n\n\nE. Testing and Model Adjustment Efficiency\n\nV. CONCLUSION In this paper, we proposed a novel framework for binarization of the Hyperdimensional computing algorithm during training and inference. BinHD encodes data points into binary hypervectors and performs training using the binary accumulation. In the inference, BinHD creates a binary model which enables the computation happens using light-weight Hamming distance similarity check. Our evaluation shows that BinHD in training (inference) can achieve on average 12.4\u00d7 and 6.3\u00d7 (13.8\u00d7 and 9.9\u00d7) energy efficiency and speedup as compared to baseline HD computing algorithm while providing the similar classification accuracy.\n\n\nIS O L E T U C IH A R F A C E C A R D IO\n\nExecution Time (\u03bcs) \n\n\nSince the query and class hypervectors are both binary, the addition ([+]) and subtraction ([\u2212]) happen by updating the corresponding counter array shown in Figure 3c. These counters update using the accumulation definition introduced in Section II-B. For example, C i [+]H, increments/decrements the class counter values on all dimensions that H has 1/0 values. After updating the counters, BinHD flips the class elements on all dimensions that their counter values are changed from positive to zero/negative values or vice versa (Figure 3d). The model adjustment continues until for a pre-defined number of iterations (maximum 30 iterations), unless if the accuracy converges earlier. The convergence condition is having less than \u03b5 = 0.1% change in accuracy in three consecutive iterations.\n\nFig. 2 .Fig. 3 .Fig. 4 .\n234Example of the binary accumulation using counter thresholding. BinHD model adjustment in binarized domain. Impact of the counter size on the BinHD classification accuracy.\n\nFig. 5 .\n5The hardware implementation of BinHD including encoding, associative search, and model adjustment modules.\n\n\u2022\nIf an encoded hypervector match with an incorrect class hypervector, BinHD XORs the query/encoded hypervector with that class ( \u2022 H ). Model adjustment accesses to the class\n\n\nANDing their signs signals ( \u2022 I ). Second, XORing the sign signal with the corresponding class and write the results back to the same memory location ( \u2022 J ).\n\nFig. 6 .\n6Execution time and energy consumption of BinHD and the baseline HD during training. Execution time and energy consumption of BinHD and the baseline HD running (a) a single query in the inference (b) a single iteration of the model adjustment.\n\n\nHamming distance. It means that for all D dimensional binary vectors A and B, there exists a D dimensional binary vector A that is the result of accumulating A and B with [+]. This satisfies the distance constraint that A is closer in Hamming space to B than it was to A. The accumulation operation involves representing the accumulator as both a binary vector and as an integer vector of counters. The binary vector keeps the current vector values, while the counter decides to update the binary vector elements during accumulation. Let us assume the binary addition of A and B vectors, where A C is composed of N-bit counters that saturate at [\u22122 N\u22121 + 1, 2 N\u22121 ]. While calculating A[+]B, the values of A C counter and the binary vector update as follows:hD Query \n\nk \n\nh1 \n\nD \n\nD \n\nk \nD \nk \n1 \n\nf 1 \n\nf 2 \n\nf n \n\nhD Query h1 \n\nNon-binary \n\nBinary \n\nQuery \n\nFig. 1. (a) Overview of HD computing performing the classification task. (b) \nFunctionality of HD encoding module. \n\nWhere [+] is a binary addition and \u03b4 ( * ) is a function \nthat calculates the \n\nTABLE I\nIDATASETS (n: FEATURE SIZE, k: NUMBER OF CLASSES).n \nK \n\nTrain \nSize \n\nTest \nSize \nDescription \nISOLET \n617 \n26 \n6,238 \n1,559 \nSpeech recognition [28] \nUCIHAR \n561 \n12 \n6,213 \n1,554 \nActivity recognition(Mobile) [27] \nFACE \n608 \n2 \n522,441 \n2,494 \nFace recognition [25] \nCARDIO \n21 \n3 \n1,913 \n213 \nCardiotocograms classification [29] \n\n\nTABLE II COMPARING\nIITHE CLASSIFICATION ACCURACY, TRAINING MEMORY FOOTPRINT, AND MODEL SIZE OF BINHD AND THE BASELINE HD COMPUTING IN DIFFERENT CONFIGURATIONS.Classification Accuracy \nTraining Memory Footprint (MB) \nModel Size (KB) \nEncoding/Model \nISOLET UCIHAR \nFACE \nCARDIO \nISOLET \nUCIHAR FACE CARDIO \nISOLET UCIHAR FACE \nCARDIO \n\nFloat/Float HD [18] \n93.5% \n95.8% \n95.3% \n99.0% \n251 \n249 \n898 \n77 \n1,015.6 \n468.7 \n78.1 \n117.2 \nFloat/Binary HD [20] \n88.1% \n91.3% \n91.9% \n93.8% \n251 \n249 \n898 \n77 \n31.7 \n14.6 \n2.4 \n3.7 \nBinary/Binary HD [20] \n85.6% \n87.3% \n83.5% \n90.2% \n10 \n13 \n34 \n3 \n31.7 \n14.6 \n2.4 \n3.7 \nProposed BinHD \n91.5% \n95.7% \n94.3% \n99.5% \n10 \n13 \n34 \n3 \n31.7 \n14.6 \n2.4 \n3.7 \n\n\n\nTABLE III IMPACT\nIIIOF THE COUNTER SIZE ON BINHD CLASSIFICATION ACCURACY.Counter Size \n5-bits \n6-bits \n8-bits \n10-bits \n16-bits \n32-bits \n\nISOLET \n91.7 \n91.7 \n91.5 \n91.2 \n90.5 \n89.2 \nUCIHAR \n95.8 \n95.7 \n94.3 \n93.1 \n93.7 \n92.5 \nFACE \nNA \nNA \nNA \n94.2 \n93.1 \n92.4 \nCARDIO \n99.5 \n99.5 \n99.1 \n97.6 \n95.8 \n97.6 \n\n\nAuthorized licensed use limited to: Univ of Calif San Diego. Downloaded on May 11,2020 at 08:39:51 UTC from IEEE Xplore. Restrictions apply.\nDesign, Automation And Test in Europe (DATE 2019)\nACKNOWLEDGEMENTSThis work was partially supported by CRISP, one of six centers in JUMP, an SRC program sponsored by DARPA, and also NSF grants #1730158 and #1527034.\nImagenet large scale visual recognition challenge. O Russakovsky, International Journal of Computer Vision. 1153O. Russakovsky et al., \"Imagenet large scale visual recognition challenge,\" International Journal of Computer Vision, vol. 115, no. 3, pp. 211-252, 2015.\n\nBayesian control of large MDPs with unknown dynamics in data-poor environments. M Imani, NIPS. M. Imani et al., \"Bayesian control of large MDPs with unknown dynamics in data-poor environments,\" in NIPS, 2018.\n\nPredicting parameters in deep learning. M , Advances in neural information processing systems. M. Denil et al., \"Predicting parameters in deep learning,\" in Advances in neural information processing systems, pp. 2148-2156, 2013.\n\nFast and efficient fpga-based feature detection employing the surf algorithm. D Bouris, FCCM. 10D. Bouris et al., \"Fast and efficient fpga-based feature detection employing the surf algo- rithm.,\" in FCCM, vol. 10, pp. 3-10, 2010.\n\nRapidnn: In-memory deep neural network acceleration framework. M Imani, arXiv:1806.05794arXiv preprintM. Imani et al., \"Rapidnn: In-memory deep neural network acceleration framework,\" arXiv preprint arXiv:1806.05794, 2018.\n\nOnline obstructive sleep apnea detection on medical wearable sensors. G Surrel, IEEE Transactions on Biomedical Circuits and Systems. 99G. Surrel et al., \"Online obstructive sleep apnea detection on medical wearable sensors,\" IEEE Transactions on Biomedical Circuits and Systems, no. 99, pp. 1-12, 2018.\n\nDisease prediction by machine learning over big data from healthcare communities. M Chen, IEEE Access. 5M. Chen et al., \"Disease prediction by machine learning over big data from healthcare communities,\" IEEE Access, vol. 5, pp. 8869-8879, 2017.\n\nReal-time classification technique for early detection and prevention of myocardial infarction on wearable devices. D Sopic, BioCAS. IEEED. Sopic et al., \"Real-time classification technique for early detection and prevention of myocardial infarction on wearable devices,\" in BioCAS, pp. 1-4, IEEE, 2017.\n\nBig data classification: Problems and challenges in network intrusion prediction with machine learning. S Suthaharan, ACM SIGMETRICS Performance Evaluation Review. 414S. Suthaharan, \"Big data classification: Problems and challenges in network intrusion predic- tion with machine learning,\" ACM SIGMETRICS Performance Evaluation Review, vol. 41, no. 4, pp. 70-73, 2014.\n\nA survey of data mining and machine learning methods for cyber security intrusion detection. A L Buczak, E Guven, IEEE Communications Surveys & Tutorials. 182A. L. Buczak and E. Guven, \"A survey of data mining and machine learning methods for cyber security intrusion detection,\" IEEE Communications Surveys & Tutorials, vol. 18, no. 2, pp. 1153-1176, 2016.\n\nSecroute: End-to-end secure communications for wireless ad-hoc networks. G Hatzivasilis, ISCC. IEEEG. Hatzivasilis et al., \"Secroute: End-to-end secure communications for wireless ad-hoc networks,\" in ISCC, pp. 558-563, IEEE, 2017.\n\nHyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors. P Kanerva, Cognitive Computation. 12P. Kanerva, \"Hyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors,\" Cognitive Computation, vol. 1, no. 2, pp. 139-159, 2009.\n\nFach: Fpga-based acceleration of hyperdimensional computing by reducing computational complexity. M Imani, ASP-DAC. IEEEM. Imani et al., \"Fach: Fpga-based acceleration of hyperdimensional computing by reducing computational complexity,\" in ASP-DAC, IEEE, 2019.\n\nSequence prediction with sparse distributed hyperdimensional coding applied to the analysis of mobile phone use patterns. O Rasanen, J Saarinen, IEEE Transactions on Neural Networks and Learning Systems. 99O. Rasanen and J. Saarinen, \"Sequence prediction with sparse distributed hyperdimensional coding applied to the analysis of mobile phone use patterns,\" IEEE Transactions on Neural Networks and Learning Systems, vol. PP, no. 99, pp. 1-12, 2015.\n\nLow-power sparse hyperdimensional encoder for language recognition. M Imani, IEEE Design & Test. 346M. Imani et al., \"Low-power sparse hyperdimensional encoder for language recognition,\" IEEE Design & Test, vol. 34, no. 6, pp. 94-101, 2017.\n\nExploring hyperdimensional associative memory. M Imani, HPCA. IEEEM. Imani et al., \"Exploring hyperdimensional associative memory,\" in HPCA, pp. 445-456, IEEE, 2017.\n\nF5-hd: Fast flexible fpga-based framework for refreshing hyperdimensional computing. S Salamat, FPGA. ACMS. Salamat et al., \"F5-hd: Fast flexible fpga-based framework for refreshing hyperdimensional computing,\" in FPGA, ACM, 2019.\n\nHierarchical hyperdimensional computing for energy efficient classification. M Imani, DAC. ACM108M. Imani et al., \"Hierarchical hyperdimensional computing for energy efficient classifica- tion,\" in DAC, p. 108, ACM, 2018.\n\nHdna: Energy-efficient dna sequencing using hyperdimensional computing. M Imani, BHI. IEEEM. Imani et al., \"Hdna: Energy-efficient dna sequencing using hyperdimensional computing,\" in BHI, pp. 271-274, IEEE, 2018.\n\nA robust and energy-efficient classifier using brain-inspired hyperdimensional computing. A Rahimi, ISLPED. ACMA. Rahimi et al., \"A robust and energy-efficient classifier using brain-inspired hyperdimen- sional computing,\" in ISLPED, pp. 64-69, ACM, 2016.\n\nVoicehd: Hyperdimensional computing for efficient speech recognition. M Imani, ICRC. IEEEM. Imani et al., \"Voicehd: Hyperdimensional computing for efficient speech recognition,\" in ICRC, pp. 1-6, IEEE, 2017.\n\nAn emg gesture recognition system with flexible high-density sensors and brain-inspired high-dimensional classifier. A Moin, ISCAS. IEEEA. Moin et al., \"An emg gesture recognition system with flexible high-density sensors and brain-inspired high-dimensional classifier,\" in ISCAS, pp. 1-5, IEEE, 2018.\n\nFelix: fast and energy-efficient logic in memory. S Gupta, ICCAD. ACM55S. Gupta et al., \"Felix: fast and energy-efficient logic in memory,\" in ICCAD, p. 55, ACM, 2018.\n\nEfficient human activity recognition using hyperdimensional computing. Y Kim, IoTACM38Y. Kim et al., \"Efficient human activity recognition using hyperdimensional computing,\" in IoT, p. 38, ACM, 2018.\n\nOrchard: Visual object recognition accelerator based on approximate inmemory processing. Y Kim, ICCAD. IEEEY. Kim et al., \"Orchard: Visual object recognition accelerator based on approximate in- memory processing,\" in ICCAD, pp. 25-32, IEEE, 2017.\n\nThe kanerva machine: A generative distributed memory. Y Wu, arXiv:1804.01756arXiv preprintY. Wu et al., \"The kanerva machine: A generative distributed memory,\" arXiv preprint arXiv:1804.01756, 2018.\n\nHuman activity recognition on smartphones using a multiclass hardwarefriendly support vector machine. D Anguita, IWAAL. SpringerD. Anguita et al., \"Human activity recognition on smartphones using a multiclass hardware- friendly support vector machine,\" in IWAAL, pp. 216-223, Springer, 2012.\n\nUci machine learning repository. \"Uci machine learning repository.\" http://archive.ics.uci.edu/ml/datasets/ISOLET.\n\nSisporto 2.0: a program for automated analysis of cardiotocograms. D Ayres-De Campos, Journal of Maternal-Fetal Medicine. 95D. Ayres-de Campos et al., \"Sisporto 2.0: a program for automated analysis of car- diotocograms,\" Journal of Maternal-Fetal Medicine, vol. 9, no. 5, pp. 311-318, 2000.\n", "annotations": {"author": "[{\"end\":169,\"start\":62},{\"end\":279,\"start\":170},{\"end\":365,\"start\":280},{\"end\":489,\"start\":366},{\"end\":597,\"start\":490}]", "publisher": null, "author_last_name": "[{\"end\":74,\"start\":69},{\"end\":183,\"start\":175},{\"end\":286,\"start\":284},{\"end\":373,\"start\":371},{\"end\":503,\"start\":497}]", "author_first_name": "[{\"end\":68,\"start\":62},{\"end\":174,\"start\":170},{\"end\":283,\"start\":280},{\"end\":370,\"start\":366},{\"end\":496,\"start\":490}]", "author_affiliation": "[{\"end\":168,\"start\":93},{\"end\":278,\"start\":203},{\"end\":364,\"start\":288},{\"end\":488,\"start\":393},{\"end\":596,\"start\":521}]", "title": "[{\"end\":59,\"start\":1},{\"end\":656,\"start\":598}]", "venue": null, "abstract": "[{\"end\":2431,\"start\":752}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2652,\"start\":2649},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2657,\"start\":2654},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2860,\"start\":2857},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2865,\"start\":2862},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3027,\"start\":3024},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3032,\"start\":3029},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3231,\"start\":3228},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3237,\"start\":3233},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3528,\"start\":3524},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3906,\"start\":3902},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3912,\"start\":3908},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4461,\"start\":4457},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4467,\"start\":4463},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4473,\"start\":4469},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5271,\"start\":5267},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5277,\"start\":5273},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5283,\"start\":5279},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5480,\"start\":5478},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5520,\"start\":5516},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6693,\"start\":6689},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6699,\"start\":6695},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7865,\"start\":7861},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7871,\"start\":7867},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8268,\"start\":8264},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9364,\"start\":9360},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12783,\"start\":12779},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":12789,\"start\":12785},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":13148,\"start\":13144},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":14726,\"start\":14722},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":17024,\"start\":17020},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":20791,\"start\":20787},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":20797,\"start\":20793},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":21181,\"start\":21177},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":25010,\"start\":25006}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":27949,\"start\":27154},{\"attributes\":{\"id\":\"fig_1\"},\"end\":28150,\"start\":27950},{\"attributes\":{\"id\":\"fig_2\"},\"end\":28268,\"start\":28151},{\"attributes\":{\"id\":\"fig_3\"},\"end\":28445,\"start\":28269},{\"attributes\":{\"id\":\"fig_4\"},\"end\":28607,\"start\":28446},{\"attributes\":{\"id\":\"fig_5\"},\"end\":28861,\"start\":28608},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":29920,\"start\":28862},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":30265,\"start\":29921},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":30960,\"start\":30266},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":31270,\"start\":30961}]", "paragraph": "[{\"end\":3373,\"start\":2450},{\"end\":4265,\"start\":3375},{\"end\":5521,\"start\":4267},{\"end\":6738,\"start\":5523},{\"end\":7706,\"start\":6740},{\"end\":8618,\"start\":7728},{\"end\":9181,\"start\":8813},{\"end\":10102,\"start\":9203},{\"end\":10626,\"start\":10160},{\"end\":11189,\"start\":10653},{\"end\":11905,\"start\":11191},{\"end\":13266,\"start\":11935},{\"end\":13899,\"start\":13334},{\"end\":15761,\"start\":13901},{\"end\":15970,\"start\":15763},{\"end\":16159,\"start\":15972},{\"end\":16823,\"start\":16161},{\"end\":17607,\"start\":16839},{\"end\":18443,\"start\":17633},{\"end\":20028,\"start\":18467},{\"end\":21045,\"start\":20069},{\"end\":22155,\"start\":21081},{\"end\":24294,\"start\":22157},{\"end\":26407,\"start\":24347},{\"end\":27088,\"start\":26454},{\"end\":27153,\"start\":27133}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8764,\"start\":8619},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8812,\"start\":8764},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10159,\"start\":10103},{\"attributes\":{\"id\":\"formula_3\"},\"end\":10652,\"start\":10627},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13298,\"start\":13267}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":20806,\"start\":20799},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":23033,\"start\":23024},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":23449,\"start\":23440}]", "section_header": "[{\"end\":2448,\"start\":2433},{\"end\":7726,\"start\":7709},{\"end\":9201,\"start\":9184},{\"end\":11933,\"start\":11908},{\"end\":13332,\"start\":13300},{\"end\":16837,\"start\":16826},{\"end\":17631,\"start\":17610},{\"end\":18465,\"start\":18446},{\"end\":20067,\"start\":20031},{\"end\":21079,\"start\":21048},{\"end\":24320,\"start\":24297},{\"end\":24345,\"start\":24323},{\"end\":26452,\"start\":26410},{\"end\":27131,\"start\":27091},{\"end\":27975,\"start\":27951},{\"end\":28160,\"start\":28152},{\"end\":28271,\"start\":28270},{\"end\":28617,\"start\":28609},{\"end\":29929,\"start\":29922},{\"end\":30285,\"start\":30267},{\"end\":30978,\"start\":30962}]", "table": "[{\"end\":29920,\"start\":29622},{\"end\":30265,\"start\":29980},{\"end\":30960,\"start\":30426},{\"end\":31270,\"start\":31035}]", "figure_caption": "[{\"end\":27949,\"start\":27156},{\"end\":28150,\"start\":27979},{\"end\":28268,\"start\":28162},{\"end\":28445,\"start\":28272},{\"end\":28607,\"start\":28448},{\"end\":28861,\"start\":28619},{\"end\":29622,\"start\":28864},{\"end\":29980,\"start\":29931},{\"end\":30426,\"start\":30288},{\"end\":31035,\"start\":30982}]", "figure_ref": "[{\"end\":6884,\"start\":6875},{\"end\":7882,\"start\":7873},{\"end\":9418,\"start\":9410},{\"end\":9734,\"start\":9725},{\"end\":9898,\"start\":9889},{\"end\":12094,\"start\":12086},{\"end\":12254,\"start\":12243},{\"end\":12404,\"start\":12395},{\"end\":14553,\"start\":14545},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15914,\"start\":15906},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15992,\"start\":15984},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":17384,\"start\":17375},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":24758,\"start\":24750},{\"end\":25074,\"start\":25065},{\"end\":25634,\"start\":25625}]", "bib_author_first_name": "[{\"end\":31680,\"start\":31679},{\"end\":31976,\"start\":31975},{\"end\":32146,\"start\":32145},{\"end\":32414,\"start\":32413},{\"end\":32631,\"start\":32630},{\"end\":32862,\"start\":32861},{\"end\":33179,\"start\":33178},{\"end\":33460,\"start\":33459},{\"end\":33753,\"start\":33752},{\"end\":34112,\"start\":34111},{\"end\":34114,\"start\":34113},{\"end\":34124,\"start\":34123},{\"end\":34451,\"start\":34450},{\"end\":34736,\"start\":34735},{\"end\":35067,\"start\":35066},{\"end\":35353,\"start\":35352},{\"end\":35364,\"start\":35363},{\"end\":35750,\"start\":35749},{\"end\":35971,\"start\":35970},{\"end\":36176,\"start\":36175},{\"end\":36400,\"start\":36399},{\"end\":36618,\"start\":36617},{\"end\":36851,\"start\":36850},{\"end\":37088,\"start\":37087},{\"end\":37344,\"start\":37343},{\"end\":37580,\"start\":37579},{\"end\":37770,\"start\":37769},{\"end\":37989,\"start\":37988},{\"end\":38203,\"start\":38202},{\"end\":38451,\"start\":38450},{\"end\":38825,\"start\":38824}]", "bib_author_last_name": "[{\"end\":31692,\"start\":31681},{\"end\":31982,\"start\":31977},{\"end\":32421,\"start\":32415},{\"end\":32637,\"start\":32632},{\"end\":32869,\"start\":32863},{\"end\":33184,\"start\":33180},{\"end\":33466,\"start\":33461},{\"end\":33764,\"start\":33754},{\"end\":34121,\"start\":34115},{\"end\":34130,\"start\":34125},{\"end\":34464,\"start\":34452},{\"end\":34744,\"start\":34737},{\"end\":35073,\"start\":35068},{\"end\":35361,\"start\":35354},{\"end\":35373,\"start\":35365},{\"end\":35756,\"start\":35751},{\"end\":35977,\"start\":35972},{\"end\":36184,\"start\":36177},{\"end\":36406,\"start\":36401},{\"end\":36624,\"start\":36619},{\"end\":36858,\"start\":36852},{\"end\":37094,\"start\":37089},{\"end\":37349,\"start\":37345},{\"end\":37586,\"start\":37581},{\"end\":37774,\"start\":37771},{\"end\":37993,\"start\":37990},{\"end\":38206,\"start\":38204},{\"end\":38459,\"start\":38452},{\"end\":38841,\"start\":38826}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":2930547},\"end\":31893,\"start\":31628},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":53965616},\"end\":32103,\"start\":31895},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":1639981},\"end\":32333,\"start\":32105},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":2547330},\"end\":32565,\"start\":32335},{\"attributes\":{\"doi\":\"arXiv:1806.05794\",\"id\":\"b4\"},\"end\":32789,\"start\":32567},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":51611525},\"end\":33094,\"start\":32791},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":10804988},\"end\":33341,\"start\":33096},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":4626057},\"end\":33646,\"start\":33343},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":916066},\"end\":34016,\"start\":33648},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":44606951},\"end\":34375,\"start\":34018},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":6250422},\"end\":34608,\"start\":34377},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":733980},\"end\":34966,\"start\":34610},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":58027670},\"end\":35228,\"start\":34968},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":15258913},\"end\":35679,\"start\":35230},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":8038292},\"end\":35921,\"start\":35681},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":1677864},\"end\":36088,\"start\":35923},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":67872077},\"end\":36320,\"start\":36090},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":49301394},\"end\":36543,\"start\":36322},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":4708051},\"end\":36758,\"start\":36545},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":9812826},\"end\":37015,\"start\":36760},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":21351739},\"end\":37224,\"start\":37017},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":3553418},\"end\":37527,\"start\":37226},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":53235957},\"end\":37696,\"start\":37529},{\"attributes\":{\"id\":\"b23\"},\"end\":37897,\"start\":37698},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":4701912},\"end\":38146,\"start\":37899},{\"attributes\":{\"doi\":\"arXiv:1804.01756\",\"id\":\"b25\"},\"end\":38346,\"start\":38148},{\"attributes\":{\"id\":\"b26\"},\"end\":38639,\"start\":38348},{\"attributes\":{\"id\":\"b27\"},\"end\":38755,\"start\":38641},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":22677559},\"end\":39048,\"start\":38757}]", "bib_title": "[{\"end\":31677,\"start\":31628},{\"end\":31973,\"start\":31895},{\"end\":32143,\"start\":32105},{\"end\":32411,\"start\":32335},{\"end\":32859,\"start\":32791},{\"end\":33176,\"start\":33096},{\"end\":33457,\"start\":33343},{\"end\":33750,\"start\":33648},{\"end\":34109,\"start\":34018},{\"end\":34448,\"start\":34377},{\"end\":34733,\"start\":34610},{\"end\":35064,\"start\":34968},{\"end\":35350,\"start\":35230},{\"end\":35747,\"start\":35681},{\"end\":35968,\"start\":35923},{\"end\":36173,\"start\":36090},{\"end\":36397,\"start\":36322},{\"end\":36615,\"start\":36545},{\"end\":36848,\"start\":36760},{\"end\":37085,\"start\":37017},{\"end\":37341,\"start\":37226},{\"end\":37577,\"start\":37529},{\"end\":37986,\"start\":37899},{\"end\":38448,\"start\":38348},{\"end\":38822,\"start\":38757}]", "bib_author": "[{\"end\":31694,\"start\":31679},{\"end\":31984,\"start\":31975},{\"end\":32149,\"start\":32145},{\"end\":32423,\"start\":32413},{\"end\":32639,\"start\":32630},{\"end\":32871,\"start\":32861},{\"end\":33186,\"start\":33178},{\"end\":33468,\"start\":33459},{\"end\":33766,\"start\":33752},{\"end\":34123,\"start\":34111},{\"end\":34132,\"start\":34123},{\"end\":34466,\"start\":34450},{\"end\":34746,\"start\":34735},{\"end\":35075,\"start\":35066},{\"end\":35363,\"start\":35352},{\"end\":35375,\"start\":35363},{\"end\":35758,\"start\":35749},{\"end\":35979,\"start\":35970},{\"end\":36186,\"start\":36175},{\"end\":36408,\"start\":36399},{\"end\":36626,\"start\":36617},{\"end\":36860,\"start\":36850},{\"end\":37096,\"start\":37087},{\"end\":37351,\"start\":37343},{\"end\":37588,\"start\":37579},{\"end\":37776,\"start\":37769},{\"end\":37995,\"start\":37988},{\"end\":38208,\"start\":38202},{\"end\":38461,\"start\":38450},{\"end\":38843,\"start\":38824}]", "bib_venue": "[{\"end\":31734,\"start\":31694},{\"end\":31988,\"start\":31984},{\"end\":32198,\"start\":32149},{\"end\":32427,\"start\":32423},{\"end\":32628,\"start\":32567},{\"end\":32923,\"start\":32871},{\"end\":33197,\"start\":33186},{\"end\":33474,\"start\":33468},{\"end\":33810,\"start\":33766},{\"end\":34171,\"start\":34132},{\"end\":34470,\"start\":34466},{\"end\":34767,\"start\":34746},{\"end\":35082,\"start\":35075},{\"end\":35432,\"start\":35375},{\"end\":35776,\"start\":35758},{\"end\":35983,\"start\":35979},{\"end\":36190,\"start\":36186},{\"end\":36411,\"start\":36408},{\"end\":36629,\"start\":36626},{\"end\":36866,\"start\":36860},{\"end\":37100,\"start\":37096},{\"end\":37356,\"start\":37351},{\"end\":37593,\"start\":37588},{\"end\":37767,\"start\":37698},{\"end\":38000,\"start\":37995},{\"end\":38200,\"start\":38148},{\"end\":38466,\"start\":38461},{\"end\":38672,\"start\":38641},{\"end\":38877,\"start\":38843}]"}}}, "year": 2023, "month": 12, "day": 17}