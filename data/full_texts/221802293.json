{"id": 221802293, "updated": "2023-10-06 11:09:40.44", "metadata": {"title": "Progressive Semantic-Aware Style Transformation for Blind Face Restoration", "authors": "[{\"first\":\"Chaofeng\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Xiaoming\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Lingbo\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Xianhui\",\"last\":\"Lin\",\"middle\":[]},{\"first\":\"Lei\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Kwan-Yee\",\"last\":\"Wong\",\"middle\":[\"K.\"]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 9, "day": 18}, "abstract": "Face restoration is important in face image processing, and has been widely studied in recent years. However, previous works often fail to generate plausible high quality (HQ) results for real-world low quality (LQ) face images. In this paper, we propose a new progressive semantic-aware style transformation framework, named PSFR-GAN, for face restoration. Specifically, instead of using an encoder-decoder framework as previous methods, we formulate the restoration of LQ face images as a multi-scale progressive restoration procedure through semantic-aware style transformation. Given a pair of LQ face image and its corresponding parsing map, we first generate a multi-scale pyramid of the inputs, and then progressively modulate different scale features from coarse-to-fine in a semantic-aware style transfer way. Compared with previous networks, the proposed PSFR-GAN makes full use of the semantic (parsing maps) and pixel (LQ images) space information from different scales of input pairs. In addition, we further introduce a semantic aware style loss which calculates the feature style loss for each semantic region individually to improve the details of face textures. Finally, we pretrain a face parsing network which can generate decent parsing maps from real-world LQ face images. Experiment results show that our model trained with synthetic data can not only produce more realistic high-resolution results for synthetic LQ inputs and but also generalize better to natural LQ face images compared with state-of-the-art methods. Codes are available at https://github.com/chaofengc/PSFRGAN.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2009.08709", "mag": "3087745341", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/ChenLYL0W21", "doi": "10.1109/cvpr46437.2021.01172"}}, "content": {"source": {"pdf_hash": "1b8e9c1cc34846e4396f4accd4114bab686829f4", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2009.08709v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2009.08709", "status": "GREEN"}}, "grobid": {"id": "4b2bf0c03dfbd42f4f5ee3fe915851a1f67e36af", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/1b8e9c1cc34846e4396f4accd4114bab686829f4.txt", "contents": "\nProgressive Semantic-Aware Style Transformation for Blind Face Restoration\n\n\nChaofeng Chen \nDepartment of Computer Science\nThe University of Hong\nKong\n\nDAMO Academy\nAlibaba Group\n\n\nXiaoming Li \nFaculty of Computing\nHarbin Institute of Technology\nChina\n\nDAMO Academy\nAlibaba Group\n\n\nDepartment of Computing\nThe Hong Kong Polytechnic University\n\n\nLingbo Yang \nDAMO Academy\nAlibaba Group\n\n\nInstitute of Digital Media\nPeking University\n\n\nXianhui Lin \nLei Zhang \nDAMO Academy\nAlibaba Group\n\n\nDepartment of Computing\nThe Hong Kong Polytechnic University\n\n\nKwan-Yee K Wong \nDepartment of Computer Science\nThe University of Hong\nKong\n\nDAMO Academy\nAlibaba Group\n\n\nProgressive Semantic-Aware Style Transformation for Blind Face Restoration\n\nLQSuper-FAN WaveletSRNet PULSE HiFaceGAN DFDNet Ours Figure 1: Result comparison with state-of-the-art methods. (Please zoom in to see the details)AbstractFace restoration is important in face image processing, and has been widely studied in recent years. However, previous works often fail to generate plausible high quality (HQ) results for real-world low quality (LQ) face images.In this paper, we propose a new progressive semantic-aware style transformation framework, named PSFR-GAN, for face restoration. Specifically, instead of using an encoder-decoder framework as previous methods, we formulate the restoration of LQ face images as a multi-scale progressive restoration procedure through semantic-aware style transformation. Given a pair of LQ face image and its corresponding parsing map, we first generate a multi-scale pyramid of the inputs, and then progressively modulate different scale features from coarse-to-fine in a semantic-aware style transfer way. Compared with previous networks, the proposed PSFR-GAN makes full use of the semantic (parsing maps) and pixel (LQ images) space information from different scales of input pairs. In addition, we further introduce a semantic aware style loss which calculates the feature style loss for each semantic region individually to improve the details of face textures. Finally, we pretrain a face parsing network which can generate decent parsing maps from real-world LQ face images. Experiment results show that our model trained with synthetic data can not only produce more realistic high-resolution results for synthetic LQ inputs and but also generalize better to natural LQ face images compared with state-of-the-art methods. Codes are available at https://github.com/chaofengc/PSFRGAN.\n\nIntroduction\n\nBlind face restoration refers to recovering the HQ images from the LQ inputs which suffer from unknown degradation such as low resolution, noise, blur and lossy compression. It has drawn more and more interest due to its wide applications. However, most current restoration methods still focus on a specific type of restoration, especially super resolution, and few of them can generalize well to real LQ images.\n\nUnlike general image restoration, face restoration can exploit strong prior knowledge of the face to recover details of the face components even when the images are severely degraded. Therefore, many recent works about face super resolution incorporate face prior knowledge to improve the performance, such as parsing maps (Chen et al. 2018), face landmarks Deokyun et al. 2019;Yu et al. 2018a) and identity prior (Zhang et al. 2018a). Most of these works are based on an encoderdecoder like structure, which follow the practice of general image restoration and aim to learn a direct black-box mapping from LQ to HQ images. Although they managed to get better results with extra face prior knowledge as inputs or supervision, few of them reports satisfactory results for real LQ images. Other methods (Li et al. 2018(Li et al. , 2020b try to utilize high quality references to facilitate the restoration of LQ images. However, their practical applications are limited when there are no high quality references.\n\nIn this work, we propose a new progressive framework, named PSFR-GAN, which formulates face restoration as multi-scale semantic-aware style transformation procedure. Inspired by recent success of style based generative adversarial networks, StyleGAN (Karras, Laine, and Aila 2019), we use a semantic-aware style transfer approach to modulate the features of different scales progressively. To be specific, the proposed PSFR-GAN starts with a learned constant latent code and then generates features of different scales through several upsample layers. We modulate the \"styles\" of different scale features by generating the corresponding style transformation parameters from different scale inputs.\n\nThe LQ input provides color information and the parsing map provides shape and semantic information. In this way, more details are added to the final features in a coarse-tofine manner. In addition, we proposed a semantic aware style loss which calculates the gram matrix loss for each semantic region separately. Gram matrix loss is usually applied in neural style transfer (Gatys, Ecker, and Bethge 2016), recent works (Gondal, Sch\u00f6lkopf, and Hirsch 2018) found it also effective in recovering textures. In this work, we show that the semantic aware style loss can help to improve the restoration of textures and alleviate the occurrence of artifacts in different face regions.\n\nFinally, to make our framework more practical, we pretrain a face parsing network (FPN) for LQ face images. Intuitively, predicting face parsing maps is easier than face restoration because we do not need to care the texture details. Experiments demonstrate that the FPN is pretty robust on parsing real-world LQ face images. During test time, we first generate parsing maps for LQ inputs with FPN, and then produce the HQ outputs with the proposed PSFR-GAN.\n\nOur contributions are summarized as follows:\n\n1. We propose a novel multi-scale progressive framework for practical blind face restoration, i.e. PSFR-GAN. Our model can recover high quality face details progressively through semantic aware style transformation with multiscale LQ images and parsing maps as inputs. Compared with previous works, the proposed PSFR-GAN can make better use of multi-scale inputs in both pixel domain and semantic domain.\n\n2. We introduce the semantic aware style loss which helps to improve the texture restoration of different semantic regions and reduce the occurrence of artifacts.\n\n3. Extensive experiments demonstrate that our model trained with synthetic dataset generalizes better to natural LQ images than current state-of-the-arts.\n\n4. By introducing a pretrained LQ face parsing network, our model can generate HQ images given only LQ inputs, making it highly practical and applicable.\n\n\nRelated Works\n\nIn this section, we briefly review the existing methods for face super-resolution, blind face restoration and HQ face generation based on generative adversarial networks.\n\nFace Super-Resolution. Face super resolution, aka. face hallucination, is often studied as a basic task for face restoration. Zhu et al. (Zhu et al. 2016) proposed a cascaded twobranch network to optimize face hallucination and dense correspondence field estimation in a unified framework. Yu et al. (Yu and Porikli 2016) exploited generative adversarial networks (GAN) to directly super-resolve LR inputs.\n\nThey further improved their model to handle unaligned faces (Yu and Porikli 2017a), noisy faces (Yu and Porikli 2017b) and faces with different attributes  However, most of these works followed a general encoderdecoder framework and did not fully utilize the face prior, making them unsuitable to handle real-world LQ images.\n\nBlind Face Restoration To deal with blind face restoration, recent works either tried to improve the framework or adopted reference based approach. Adrian et al. (Bulat, Yang, and Tzimiropoulos 2018) proposed a two-stage GAN framework to learn real degradation model. Yang et al. (Yang et al. 2020) introduced the HiFaceGAN, a multistage framework that progressively replenish face details. As for reference based methods, Song et al. (Song et al. 2017) constructed a high-resolution dictionary to help enhance face components. Li et al. (Li et al. 2018) proposed GRFNet which learns to warp a guidance image for blind face restoration. They further improve their work by replacing single reference with multiple reference images (Li et al. 2020b) and feature dictionaries (Li et al. 2020a). The aforementioned model based methods also fail to make full use of face prior, and the reference based approaches require HQ references which limits their practical application.  \n\n\nProposed Method\n\nIn this section, we first describe our formulation and framework in detail, then analyze how the proposed PFSR-GAN works. After that, we introduce the semantic-aware style loss and the other objectives used to train our networks.\n\n\nProgressive Semantic-Aware Style Transformation\n\nAs illustrated in Fig. 2, our PFSR-GAN starts with a learned constant of size C \u00d7 16 \u00d7 16, denoted as F 0 , where C is the channel size. Then, F 0 goes through several upsample residual blocks and generates the final features with the same size as HQ images. Let's define the output features of i-th residual block as F i , then the features are progressively upsampled in the following way\nF i = \uf8f1 \uf8f2 \uf8f3 \u03a6 ST \u03a6 RES (F i\u22121 ) , i = 1 \u03a6 ST \u03a6 U P (F i\u22121 ) , 1 < i \u2264 6\n(1)\n\nwhere \u03a6 RES (\u00b7) denotes the residual convolution block, \u03a6 U P (\u00b7) denotes the upsample residual convolution block and \u03a6 ST (\u00b7) denotes the style transformation block. The last feature F 6 goes through a single ToRGB convolution layer and predicts the final output\u00ce H . The \u03a6 ST (\u00b7) blocks are the key parts of our framework. Each of them learns the style transformation parameters y i = (y s,i , y b,i ) for F i from corresponding scale of input pairs, i.e., LQ images and parsing maps denoted as (I i L , I i P ). (I i L , I i P ) are resized to the same size as F i through bicubic interpolation. Then, \u03a6 ST (\u00b7) can be formulated as follows\ny i = \u03a8(I i L , I i P ),(2)F i = y i,s \u03a6 U P (F i\u22121 ) \u2212 \u00b5 \u03a6 U P (F i\u22121 ) \u03c3 \u03a6 U P (F i\u22121 ) + y i,b ,(3)\nwhere \u03a8(\u00b7) is a lightweight network composed of several convolution layers. \u00b5(\u00b7) and \u03c3(\u00b7) are the mean and standard variation of features. Compared with StyleGAN which adpots spatially invariant styles, we follow GauGAN (Park et al. 2019) and compute the spatially adaptive style parameters y i with the same size as F i . This helps to make full use of the spatial-wise color and texture information from I L as well as shape and semantic guidance from I P . We use the pretrained face parsing network (FPN) to generate I P from LQ inputs I L .\n\n\nAnalysis of PFSR-GAN\n\nThere are two key designs in the architecture of PSFR-GAN:\n\n(1) the features are modulated progressively from coarse-tofine;\n\n(2) the multi-scale input pairs I L and I P work together to provide sufficient color, shape and semantic information. In Fig. 3(b), we show how details are progressively added to the final outputs by first zeroing all inputs and then add them back from coarse to fine. We can observe that the restoration process is consistent with our hypothesis, and happens in the following order: high level semantic information comes first, then the mid-level shape and edges, and finally lowlevel color and details. Similarly, we analysis the effect of I i P and I i L separately in Fig. 3(c) and Fig. 3(d). The first column of Fig. 3(c) and Fig. 3(d) show the result of using I i L and I i P as inputs separately. We can see that in the first image of Fig. 3(c), the nose and mouth borders are not clear and there are many artifacts in the cheek region. This indicates that network without parsing map as inputs only makes the bicubic results (first row and last column in Fig. 3(b)) sharper and has difficulties in understanding the semantic meaning of each region. When we add I i P progressively, the artifacts are gradually removed and the edges are clearer. As for Fig.  3(d), we can observe that the semantic regions are clear in all stages, for example the nose and teeth part. Then, color and texture details are added with the entry of I i L . In summary, PSFR-GAN restores the LQ images in a progressive way by modulating the features with multi-scale inputs, where I i L provides the low level color and texture informaiton and I i P contributes the semantic and shape information.\n\n\nSemantic-Aware Style Loss\n\nRecent super-resolution work (Gondal, Sch\u00f6lkopf, and Hirsch 2018) has demonstrated that gram matrix loss which is usually used in style transfer helps a lot in recovering textures. To better synthesize texture details, we introduce the semantic-aware style loss L ss which calculates the gram matrix loss for each semantic region separately. We use VGG19 features of layer relu1 1, relu2 1, relu3 1, relu4 1 and relu5 1 to calculate L ss . Denote \u03c6 i as the  i-th layer feature in VGG19 and the parsing mask with label j as M j (the background is denoted as M 0 ), the semantic aware style loss is formulated as\nL ss = 5 i=1 18 j=0 G \u03c6 i (\u00ce H ), M j \u2212 G \u03c6 i (I H ), M j 2 ,(4)\nG(\u00b7) computes the gram matrix of feature \u03c6 i (\u00b7) with semantic label mask M j as below\nG(\u03c6 i , M j ) = \u03c6 i M j ) T \u03c6 i M j ) M j + ,(5)\nwhere is the element-wise product, and = 1e \u2212 8 is used to avoid zero division.\n\n\nModel Objectives\n\nFollowing previous image restoration works (Chen et al. 2018;Wang et al. 2018b,a;Li et al. 2020b,a), we apply reconstruction loss and adversarial loss apart from L ss .\n\nReconstruction Loss. It is the combination of pixel and feature space mean square error (MSE) which aims to constrain the network output\u00ce H close to ground truth I H . We formulate the reconstruction loss as\nL rec = \u00ce H \u2212 I H 2 + s 4 k=1 D k s (\u00ce s H ) \u2212 D k s (I s H ) 2 (6)\nThe second term in L rec is the multi-scale feature matching loss (Wang et al. 2018a) which matches the discriminator features of\u00ce H and I H . s \u2208 {1, 1 2 , 1 4 } is the downscale factors, and D k s (\u00b7) denotes the k-th layer features in D s .\n\nAdversarial Loss. It has been proved to be effective and critical in generating realistic textures in image restoration tasks. In this work, we use multi-scale discriminators and hinge loss as the objective function, defined as\nL GAN G = s \u2212E(\u00ce s H ),(7)L GAN D = s E max(0, 1 \u2212 D s (I s H )) + E max(0, 1 + D s (\u00ce s H )) ,(8)\nFor training stability, we incorporate spectral normalization (Miyato et al. 2018) in both generator and discriminators. In summary, the final loss function for our generator network is defined as\nL G = \u03bb ss L ss + \u03bb rec L rec + \u03bb adv L GAN G ,(9)\nwhere \u03bb are the weights for different terms. PFSR-GAN is trained by minimizing L G and L GAN D alternatively.\n\n\nImplementation Details Degradation Model\n\nAccording to previous works (Li et al. 2018;Xu et al. 2017) and common practice in SISR framework, we generate the LR image I r L with the following degradation model:\nI r L = ((I H \u2297 k ) \u2193 r +n \u03b4 ) JP EGq ,(10)\nwhere \u2297 represents the convolution operation between the HQ image I H and a blur kernel k with parameter . \u2193 r is the downsampling operation with a scale factor r. n \u03b4 denotes the additive white Gaussian noise (AWGN) with a noise level \u03b4. (\u00b7) JP EGq indicates the JPEG compression operation with quality factor q. The hyper parameters , r, \u03b4, q are randomly selected for each HR image I H , and I r L is generated online. More details are described in the supplementary materials. After we get I r L , I L = (I r L ) \u2191 r is used for the parsing map prediction and face restoration.\n\n\nDatasets\n\nTraining Data. We adopt the FFHQ (Karras, Laine, and Aila 2019) as the training dataset. This dataset consists of 70, 000 high-quality images at a size of 1024\u00d71024 crawled from the internet. All images are automatically cropped and aligned. We resize the images to 512 \u00d7 512 with bilinear downsampling as the ground-truth HR images, and synthesize the LQ inputs online with Eq. 10 where the parameters are randomly selected.  Table  1. Please zoom in to see the details.\n\nTesting Data. We construct two testing datasets, a synthetic one and a real one. For the synthetic test dataset, we randomly choose 2, 800 HQ images from CelebAHQ (Karras et al. 2017) which has no identity intersection with FFHQ, and then generate the corresponding LQ images in the same way as training dataset. We denote this synthetic test dataset as CelebAHQ-Test. For the real LQ test dataset, we collect 1, 020 faces smaller than 48 \u00d7 48 from CelebA (Liu et al. 2015) and 106 images provided by GFRNet (Li et al. 2018). The GFRNet-Test contains LR images from VGGFace2 (Cao et al. 2018) and IMDB-WIKI (Shrivastava, Gupta, and Girshick 2016). We also collect some old photos from the internet for testing. All images are cropped and aligned in the same manner as FFHQ, and then resized to 512 \u00d7 512 using bicubic upsampling. We merge all these images together and create a new dataset containing 1, 157 real LQ faces, denoted as PSFR-RealTest.\n\n\nTraining Details\n\nWe use Adam optimizer (Kingma and Ba 2014) to train our networks. We choose \u03b2 1 = 0.5, \u03b2 2 = 0.999, and set the learning rate of the generator and discriminator to 0.0001 and 0.0004 respectively. The trade-off parameters of different losses are set as \u03bb rec = 10, \u03bb ss = 100 and \u03bb adv = 1. The training batch size is set to 4. All models were implemented by PyTorch 1 and trained on a Tesla V100 GPU.\n\n\nExperiments\n\nIn this section, we conduct experiments to compare our framework with other methods on both synthetic and real 1 https://pytorch.org/ LQ test datasets, and carry out extensive ablation studies to evaluate the effectiveness of the multi-scale parsing map guidance and semantic aware style loss.\n\n\nEvaluation Metrics\n\nFor CelebAHQ-Test with ground truth, we take the widely used PSNR, SSIM and MSSIM metrics. However, these pixel space metrics prefer smooth results and are not consistent with human perception, therefore we also adopt LPIPS score (Zhang et al. 2018b) to evaluate the perceptual realism of generated faces. For natural LQ images without ground truth, we use FID score (Heusel et al. 2017) to measure the statistic distance between the restoration results and a reference HQ face dataset. Compared with other no reference metrics such as NIQE (Mittal, Soundararajan, and Bovik 2012) and IS (Inception Score) (Salimans et al. 2016) which focus on natural images, FID utilizes a reference HQ face dataset and gives better measurement. We use the ground truth images from CelebAHQ-Test as the reference dataset to evaluate results of PSFR-RealTest. We also provide FID scores for results of CelebAHQ-Test for reference.\n\n\nComparison on Synthetic Datasets\n\nWe first evaluate the performance of different methods on the synthetic CelebAHQ-Test dataset.    Table 1. More results are provided in supplementary materials.\n\nWaveletSRNet  for face super-resolution; and recent methods HiFaceGAN (Yang et al. 2020) and DFDNet (Li et al. 2020a) for blind face restoration. Table  1 shows the performance of both statistical metrics (PSNR, SSIM, MSSIM) and perceptual metrics (LPIPS, FID). We can observe that methods designed for specific tasks generally show low perceptual scores than blind face restoration methods. Both DFDNet and PSFR-GAN utilize extra face prior, and their performance is much better than HiFace-GAN. Compared with DFDNet, the proposed PSFR-GAN still outperforms by a large margin in terms of most evaluation metrics, especially in FID score (50% improvement), which indicates the superiority of our proposed PSFR-GAN.\n\nThe visualization examples in Fig. 4 help us understand the quantitative results better. We show three kinds of LQ inputs in each row of Fig. 4: LQ with light degradation, LQ with severe degradation and LQ with large pose. It can be observed that results of both ESRGAN and Super-FAN are over smoothed and fail to recover clear face components and textures compared with blind face restoration methods, see 2-nd and 3-rd rows. This indicates that methods designed for specific task are not able to handle LQ inputs with complicated blind degradation. The results of HiFace-GAN are clearer and sharper but contain too much artifacts in the mouth, eyes and backgrounds. This is most likely be-cause of the unstable training of GAN without face prior guidance. Different from PSFR-GAN which utilizes parsing map, DFDNet needs to first detect the LQ face components and then matches them to a HQ dictionary. It may fail to find the correct reference when the LQ are too blurry or with large pose. For example, the results of DFDNet in the 3-rd row seems to have two eyeballs which do not exist in the other methods. Meanwhile, PSFR-GAN incorporates the semantic information through parsing map which is more generic than explicit HQ reference. Therefore, the results of PSFR-GAN are more realistic and robust with all kinds of LQ inputs. Moreover, our results are generated using the predicted parsing maps of the pretrained FPN, see last column in Fig. 4. This means our PSFR-GAN does not need extra information such as HQ dictionaries during the test time, making PSFR-GAN available in most situations.\n\n\nComparison on Real World Low Quality Images\n\nThe final target of all methods is to restore real world LQ face images. To evaluate the generalization ability of different methods, we also compare the performance of PSFR-GAN on PSFR-RealTest dataset with methods in Table 1. The same as previous results on CelebAHQ-Test, methods designed for blind face restoration still outperform the others. FID score of PFSR-GAN surpasses other methods by a large margin, and is 20% higher than the second best result of DFDNet. Fig. 5 gives some examples from PFSR-RealTest dataset. We can observe that there are many artifacts in the results of HiFaceGAN. DFDNet seems to have difficulties finding the correct component references for real LQ inputs, for example, teeth in the first row and right eye in the second row. In contrast, the results of our method is much more natural with few conspicuous artifacts. This is because PSFR-GAN generates the results in a progressive way, and the multi-scale parsing maps provide multiscale style guidance which makes PSFR-GAN able to synthesis realistic textures for each semantic regions, e.g., teeth textures and eyes. The parsing map results in the last column illustrate that the pretrained FPN works well for real LQ inputs. We show more results of PSFR-GAN on PSFR-RealTest datasets in supplementary materials, which demonstrate that PSFR-GAN is practical for real world scenarios.\n\n\nComparison with PULSE\n\nPULSE (Menon et al. 2020) is a recent popular method for face restoration. Different from other methods, PULSE is an optimization based method which needs carefully finetune for each LQ input. Therefore, it is unfair to compare the quantitative result of PULSE with others on PSFR-RealTest. Instead, we follow HiFaceGAN and use the historic group photograph of famous physicists taken at the 5th Solvay Conference 1927 for visual comparison. We carefully finetune PULSE on these photos and get the best results as much as we can. Even so, we still observe several typical failure cases of PULSE shown in Fig. 6: (1) age mismatch and shape deformation in the first column; (2) large pose in the second column; (3) background interference in the third column. The final column shows the best result of the test photos, but there are many slight shape changes in the eyes, nose and mouth which make it look like another person. To conclude, although PULSE can generate better details, for example hair textures, the results of our PSFR-GAN are still much better than PULSE in face restoration. Besides, PSFR- GAN is 40 times faster than PULSE (0.1s vs. 4s) to process the same image on GPU. Complete results of Solvay conference are in the supplementary materials. To explore the effectiveness of parsing map guidance and semantic-aware style loss L ss , we evaluate four variants of our framework in Table 2: A, baseline model with only I L as inputs; B, baseline model with (I L , I P ) as inputs but without L ss ; C, baseline model with I L as inputs and L ss ; D, the proposed PSFR-GAN. We can observe that the parsing map plays an important role in face image restoration and achieves the most improvements, and the L ss can also benefit the restoration results. With the combination of them, our PSFR-GAN can achieve the best performance. Figure  7 shows some example results on PSFR-RealTest. We can observe from the bottom row that models without parsing map (i.e., model A and C) fail to generate clear shapes when the face border in LQ image is not clear. On the other hand, models without L ss (i.e., model A and B) produce apparent artifacts especially in the eyes. In contrast, model D with parsing map and L ss has none of the above flaws. It can be inferred from the above observation that 1) parsing map helps to regularize face structure, and 2) L ss helps to synthesize realistic textures for each semantic region.\n\n\nAblation Study\n\n\nConclusion\n\nThis paper proposes a multi-scale progressive face restoration network, named PSFR-GAN, which restores LQ face inputs in a coarse to fine manner through semantic-aware style transformation. We also proposed the semantic-aware style loss based on original gram matrix loss. Experiments on both synthetic and real LQ test datasets demonstrate the superority and robustness of our PSFR-GAN. By pretraining the face parsing network (FPN) for LQ inputs, our framework can generate high-resolution and realistic HQ outputs without requiring extra inputs. In summary, PSFR-GAN provides a robust and easy-to-use solution for face restoration in real-world scenarios.\n\nFigure 2 :\n2Visulization of the proposed progressive semantic-aware style transformation network for face restoration.\n\n\nof adding multi-scale inputs (I i L , I i P ) progressively.(c) Keep all scales of LQ inputs I i L , and add different scales of parsing maps I i P progressively.(d) Keep all scales of parsing map inputs I i P , and add different scales of LQ inputs I i L progressively.\n\nFigure 3 :\n3Analysis of PSFR-GAN. Zoom in to see details.\n\nFigure 4 :\n4Visual comparisons on CelebAHQ-Test dataset. The proposed PSFR-GAN shows the best results for LQ inputs with light degradation, severe degradation and large pose. The predicted parsing maps of LQ inputs through pretrained FPN are overlapped at the corner of GT images. For better visualization experience, we only show results with top-5 FID scores in\n\n\nFollowing HiFace-GAN (Yang et al. 2020), we compare the proposed PSFR-GAN with architectures designed for different resotraiton tasks: ARCNN (Dong et al. 2015) for JPEG artifacts removal; DeblurGANv2 (Kupyn et al. 2019) for image deblurring; ESRGAN (Wang et al. 2018b) for natural image superresolution; Super-FAN (Bulat and Tzimiropoulos 2018) and\n\nFigure 5 :\n5Visual comparisons on PSFR-RealTest dataset. Results of the proposed PSFR-GAN are clearer and more realistic. We show results with top-5 FID scores in\n\nFigure 6 :\n6Visual comparison between PULSE and PSFR-GAN. Complete results are provided in the supplementary materials. Please zoom in to see the details.\n\nFigure 7 :\n7Visual comparison between different variations of our model. Please zoom in to see the details.\n\nTable 1 :\n1Quantitative comparison on different restoration tasks with state-of-the-art methods. The test datasets are generated \nwith FFHQ-Test using random parameters of each specific degradation type. \n\nTask \nMethods \nCelebAHQ-Test \nPSFR-RealTest \nPSNR\u2191 SSIM\u2191 MSSIM\u2191 LPIPS\u2193 \nFID\u2193 \nFID\u2193 \nJPEG artifacts removal \nARCNN \n22.78 \n0.6538 \n0.7462 \n0.5862 133.38 \n124.46 \nDeblur \nDeblurGANv2 \n22.66 \n0.6587 \n0.7493 \n0.5546 113.85 \n97.42 \n\nSuper-Resolution \n\nESRGAN \n21.95 \n0.6096 \n0.7293 \n0.5515 \n97.02 \n57.51 \nSuper-FAN \n22.71 \n0.6527 \n0.7459 \n0.4908 \n94.95 \n65.45 \nWaveletSRNet \n23.50 \n0.6595 \n0.7542 \n0.5409 111.60 \n108.21 \n\nBlind-Restoration \n\nHiFaceGAN \n21.50 \n0.5495 \n0.6900 \n0.4569 \n57.81 \n56.48 \nDFDNet \n22.28 \n0.6589 \n0.7650 \n0.3791 \n37.34 \n37.63 \nPSFR-GAN (ours) \n23.64 \n0.6557 \n0.7740 \n0.3042 \n23.20 \n30.39 \n\nLQ Input \nESRGAN \nSuper-FAN \nHiFaceGAN \nDFDNet \nPSFR-GAN \n\n\n\nTable 2 :\n2Ablation study of the proposed method. ID Model Variations FID\u2193 A Baseline with I L 47.48 B + I P 32.37 C + L ss 34.22 D + I P + L ss 30.39\n\nSuper-FAN: Integrated facial landmark localization and super-resolution of real-world low resolution faces in arbitrary poses with GANs. A Bulat, G Tzimiropoulos, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Bulat, A.; and Tzimiropoulos, G. 2018. Super-FAN: Integrated facial landmark localization and super-resolution of real-world low resolution faces in arbitrary poses with GANs. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\n\nTo learn image super-resolution, use a GAN to learn how to do image degradation first. A Bulat, J Yang, G Tzimiropoulos, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Bulat, A.; Yang, J.; and Tzimiropoulos, G. 2018. To learn image super-resolution, use a GAN to learn how to do image degradation first. In Proceedings of the European Conference on Computer Vision (ECCV).\n\nVGGFace2: A dataset for recognising faces across pose and age. Q Cao, L Shen, W Xie, O M Parkhi, A Zisserman, International Conference on Automatic Face and Gesture Recognition. Cao, Q.; Shen, L.; Xie, W.; Parkhi, O. M.; and Zisserman, A. 2018. VGGFace2: A dataset for recognising faces across pose and age. In International Conference on Automatic Face and Gesture Recogni- tion.\n\nFSRNet: End-to-End Learning Face Super-Resolution with Facial Priors. Y Chen, Y Tai, X Liu, C Shen, Yang , J , Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Chen, Y.; Tai, Y.; Liu, X.; Shen, C.; and Yang, J. 2018. FSRNet: End-to-End Learning Face Super-Resolution with Facial Priors. In Proceedings of the IEEE Conference on Computer Vision and Pat- tern Recognition (CVPR).\n\nProgressive Face Super-Resolution via Attention to Facial Landmark. K Deokyun, K Minseon, K Gihyun, K Dae-Shik, British Machine Vision Conference (BMVC). Deokyun, K.; Minseon, K.; Gihyun, K.; and Dae-Shik, K. 2019. Progressive Face Super-Resolution via Attention to Facial Land- mark. In British Machine Vision Conference (BMVC).\n\nCompression artifacts reduction by a deep convolutional network. C Dong, Y Deng, C Loy, X Tang, Proceedings of the IEEE International Conference on Computer Vision (ICCV). the IEEE International Conference on Computer Vision (ICCV)Dong, C.; Deng, Y.; Change Loy, C.; and Tang, X. 2015. Com- pression artifacts reduction by a deep convolutional network. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 576-584.\n\nImage style transfer using convolutional neural networks. L A Gatys, A S Ecker, M Bethge, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Gatys, L. A.; Ecker, A. S.; and Bethge, M. 2016. Image style transfer using convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2414-2423.\n\nThe unreasonable effectiveness of texture transfer for single image superresolution. M W Gondal, B Sch\u00f6lkopf, M Hirsch, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)SpringerGondal, M. W.; Sch\u00f6lkopf, B.; and Hirsch, M. 2018. The unrea- sonable effectiveness of texture transfer for single image super- resolution. In Proceedings of the European Conference on Com- puter Vision (ECCV), 80-97. Springer.\n\nGans trained by a two time-scale update rule converge to a local nash equilibrium. M Heusel, H Ramsauer, T Unterthiner, B Nessler, S Hochreiter, Proceedings of Advances in Neural Information Processing Systems (NIPS). Advances in Neural Information Processing Systems (NIPS)Heusel, M.; Ramsauer, H.; Unterthiner, T.; Nessler, B.; and Hochreiter, S. 2017. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Proceedings of Advances in Neural Information Processing Systems (NIPS), 6626-6637.\n\nWavelet-srnet: A wavelet-based cnn for multi-scale face super resolution. H Huang, R He, Z Sun, T Tan, Proceedings of the IEEE International Conference on Computer Vision (ICCV). the IEEE International Conference on Computer Vision (ICCV)Huang, H.; He, R.; Sun, Z.; and Tan, T. 2017. Wavelet-srnet: A wavelet-based cnn for multi-scale face super resolution. In Pro- ceedings of the IEEE International Conference on Computer Vision (ICCV), 1689-1697.\n\nArbitrary Style Transfer in Realtime with Adaptive Instance Normalization. X Huang, S Belongie, Proceedings of the IEEE International Conference on Computer Vision (ICCV). the IEEE International Conference on Computer Vision (ICCV)Huang, X.; and Belongie, S. 2017. Arbitrary Style Transfer in Real- time with Adaptive Instance Normalization. In Proceedings of the IEEE International Conference on Computer Vision (ICCV).\n\nProgressive growing of gans for improved quality, stability, and variation. T Karras, T Aila, S Laine, J Lehtinen, arXiv:1710.10196arXiv preprintKarras, T.; Aila, T.; Laine, S.; and Lehtinen, J. 2017. Progressive growing of gans for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196 .\n\nA style-based generator architecture for generative adversarial networks. T Karras, S Laine, Aila , T , Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Karras, T.; Laine, S.; and Aila, T. 2019. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 4401-4410.\n\nAnalyzing and improving the image quality of stylegan. T Karras, S Laine, M Aittala, J Hellsten, J Lehtinen, Aila , T , arXiv:1912.04958arXiv preprintKarras, T.; Laine, S.; Aittala, M.; Hellsten, J.; Lehtinen, J.; and Aila, T. 2019. Analyzing and improving the image quality of style- gan. arXiv preprint arXiv:1912.04958 .\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, arXiv:1412.6980arXiv preprintKingma, D. P.; and Ba, J. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 .\n\nDeblurgan-v2: Deblurring (orders-of-magnitude) faster and better. O Kupyn, T Martyniuk, J Wu, Z Wang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Kupyn, O.; Martyniuk, T.; Wu, J.; and Wang, Z. 2019. Deblurgan- v2: Deblurring (orders-of-magnitude) faster and better. In Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 8878-8887.\n\nBlind Face Restoration via Deep Multi-scale Component Dictionaries. X Li, C Chen, S Zhou, X Lin, W Zuo, L Zhang, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Li, X.; Chen, C.; Zhou, S.; Lin, X.; Zuo, W.; and Zhang, L. 2020a. Blind Face Restoration via Deep Multi-scale Component Dictio- naries. In Proceedings of the European Conference on Computer Vision (ECCV).\n\nEnhanced Blind Face Restoration with Multi-Exemplar Images and Adaptive Spatial Feature Fusion. X Li, W Li, D Ren, H Zhang, M Wang, W Zuo, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Li, X.; Li, W.; Ren, D.; Zhang, H.; Wang, M.; and Zuo, W. 2020b. Enhanced Blind Face Restoration with Multi-Exemplar Images and Adaptive Spatial Feature Fusion. In Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recognition (CVPR).\n\nLearning Warped Guidance for Blind Face Restoration. X Li, M Liu, Y Ye, W Zuo, L Lin, Yang , R , Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Li, X.; Liu, M.; Ye, Y.; Zuo, W.; Lin, L.; and Yang, R. 2018. Learn- ing Warped Guidance for Blind Face Restoration. In Proceedings of the European Conference on Computer Vision (ECCV).\n\nDeep Learning Face Attributes in the Wild. Z Liu, P Luo, X Wang, X Tang, Proceedings of the IEEE International Conference on Computer Vision (ICCV). the IEEE International Conference on Computer Vision (ICCV)Liu, Z.; Luo, P.; Wang, X.; and Tang, X. 2015. Deep Learning Face Attributes in the Wild. In Proceedings of the IEEE International Conference on Computer Vision (ICCV).\n\nPULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models. S Menon, A Damian, S Hu, N Ravi, C Rudin, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Menon, S.; Damian, A.; Hu, S.; Ravi, N.; and Rudin, C. 2020. PULSE: Self-Supervised Photo Upsampling via Latent Space Ex- ploration of Generative Models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2437-2445.\n\nMaking a completely blind image quality analyzer. A Mittal, R Soundararajan, A C Bovik, IEEE Signal Processing Letters. 203Mittal, A.; Soundararajan, R.; and Bovik, A. C. 2012. Making a completely blind image quality analyzer. IEEE Signal Processing Letters 20(3): 209-212.\n\nT Miyato, T Kataoka, M Koyama, Y Yoshida, arXiv:1802.05957Spectral normalization for generative adversarial networks. arXiv preprintMiyato, T.; Kataoka, T.; Koyama, M.; and Yoshida, Y. 2018. Spectral normalization for generative adversarial networks. arXiv preprint arXiv:1802.05957 .\n\nSemantic Image Synthesis with Spatially-Adaptive Normalization. T Park, M.-Y Liu, T.-C Wang, J.-Y Zhu, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Park, T.; Liu, M.-Y.; Wang, T.-C.; and Zhu, J.-Y. 2019. Semantic Image Synthesis with Spatially-Adaptive Normalization. In Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\n\nImproved techniques for training gans. T Salimans, I Goodfellow, W Zaremba, V Cheung, A Radford, X Chen, Proceedings of Advances in Neural Information Processing Systems (NIPS). Advances in Neural Information Processing Systems (NIPS)Salimans, T.; Goodfellow, I.; Zaremba, W.; Cheung, V.; Radford, A.; and Chen, X. 2016. Improved techniques for training gans. In Proceedings of Advances in Neural Information Processing Sys- tems (NIPS), 2234-2242.\n\nTraining Region-based Object Detectors with Online Hard Example Mining. A Shrivastava, A Gupta, R Girshick, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Shrivastava, A.; Gupta, A.; and Girshick, R. 2016. Training Region-based Object Detectors with Online Hard Example Min- ing. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\n\nLearning to Hallucinate Face Images via Component Generation and Enhancement. Y Song, J Zhang, S He, L Bao, Yang , Q , Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI). the International Joint Conference on Artificial Intelligence (IJCAI)Song, Y.; Zhang, J.; He, S.; Bao, L.; and Yang, Q. 2017. Learn- ing to Hallucinate Face Images via Component Generation and En- hancement. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI), 4537-4543.\n\nHigh-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs. T.-C Wang, M.-Y Liu, J.-Y Zhu, A Tao, J Kautz, B Catanzaro, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Wang, T.-C.; Liu, M.-Y.; Zhu, J.-Y.; Tao, A.; Kautz, J.; and Catan- zaro, B. 2018a. High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\n\nESRGAN: Enhanced super-resolution generative adversarial networks. X Wang, K Yu, S Wu, J Gu, Y Liu, C Dong, Y Qiao, C C Loy, Proceedings of the European Conference on Computer Vision Workshops. the European Conference on Computer Vision WorkshopsECCVWWang, X.; Yu, K.; Wu, S.; Gu, J.; Liu, Y.; Dong, C.; Qiao, Y.; and Loy, C. C. 2018b. ESRGAN: Enhanced super-resolution generative adversarial networks. In Proceedings of the European Conference on Computer Vision Workshops (ECCVW).\n\nLearning to Super-Resolve Blurry Face and Text Images. X Xu, D Sun, J Pan, Y Zhang, H Pfister, M.-H Yang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Xu, X.; Sun, D.; Pan, J.; Zhang, Y.; Pfister, H.; and Yang, M.-H. 2017. Learning to Super-Resolve Blurry Face and Text Images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 251-260.\n\nHiFaceGAN: Face Renovation via Collaborative Suppression and Replenishment. L Yang, C Liu, P Wang, S Wang, P Ren, S Ma, W Gao, Yang, L.; Liu, C.; Wang, P.; Wang, S.; Ren, P.; Ma, S.; and Gao, W. 2020. HiFaceGAN: Face Renovation via Collaborative Suppres- sion and Replenishment. URL https://arxiv.org/abs/2005.05005.\n\nFace Super-resolution Guided by Facial Component Heatmaps. X Yu, B Fernando, B Ghanem, F Porikli, Hartley , R , Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Yu, X.; Fernando, B.; Ghanem, B.; Porikli, F.; and HARTLEY, R. 2018a. Face Super-resolution Guided by Facial Component Heatmaps. In Proceedings of the European Conference on Com- puter Vision (ECCV), 217-233.\n\nSuper-Resolving Very Low-Resolution Face Images with Supplementary Attributes. X Yu, B Fernando, R Hartley, F Porikli, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Yu, X.; Fernando, B.; Hartley, R.; and Porikli, F. 2018b. Super- Resolving Very Low-Resolution Face Images with Supplementary Attributes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\n\nUltra-resolving face images by discriminative generative networks. X Yu, F Porikli, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)SpringerYu, X.; and Porikli, F. 2016. Ultra-resolving face images by dis- criminative generative networks. In Proceedings of the European Conference on Computer Vision (ECCV), 318-333. Springer.\n\nFace Hallucination with Tiny Unaligned Images by Transformative Discriminative Neural Networks. X Yu, F Porikli, Proceedings of the AAAI Conference on Artificial Intelligence (AAAI). the AAAI Conference on Artificial Intelligence (AAAI)23Yu, X.; and Porikli, F. 2017a. Face Hallucination with Tiny Unaligned Images by Transformative Discriminative Neural Net- works. In Proceedings of the AAAI Conference on Artificial Intel- ligence (AAAI), volume 2, 3.\n\nHallucinating very low-resolution unaligned and noisy face images by transformative discriminative autoencoders. X Yu, F Porikli, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Yu, X.; and Porikli, F. 2017b. Hallucinating very low-resolution unaligned and noisy face images by transformative discriminative autoencoders. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR), 3760-3768.\n\nSuper-Identity Convolutional Neural Network for Face Hallucination. K Zhang, Z Zhang, C.-W Cheng, W Hsu, Y Qiao, W Liu, T Zhang, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Zhang, K.; ZHANG, Z.; Cheng, C.-W.; Hsu, W.; Qiao, Y.; Liu, W.; and Zhang, T. 2018a. Super-Identity Convolutional Neural Net- work for Face Hallucination. In Proceedings of the European Con- ference on Computer Vision (ECCV), 183-198.\n\nThe Unreasonable Effectiveness of Deep Features as a Perceptual Metric. R Zhang, P Isola, A A Efros, E Shechtman, O Wang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Zhang, R.; Isola, P.; Efros, A. A.; Shechtman, E.; and Wang, O. 2018b. The Unreasonable Effectiveness of Deep Features as a Per- ceptual Metric. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR).\n\nSEAN: Image Synthesis With Semantic Region-Adaptive Normalization. P Zhu, R Abdal, Y Qin, P Wonka, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Zhu, P.; Abdal, R.; Qin, Y.; and Wonka, P. 2020. SEAN: Image Synthesis With Semantic Region-Adaptive Normalization. In Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\n\nDeep Cascaded Bi-Network for Face Hallucination. S Zhu, S Liu, C C Loy, X Tang, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Zhu, S.; Liu, S.; Loy, C. C.; and Tang, X. 2016. Deep Cascaded Bi- Network for Face Hallucination. In Proceedings of the European Conference on Computer Vision (ECCV).\n", "annotations": {"author": "[{\"end\":181,\"start\":78},{\"end\":345,\"start\":182},{\"end\":434,\"start\":346},{\"end\":447,\"start\":435},{\"end\":550,\"start\":448},{\"end\":656,\"start\":551}]", "publisher": null, "author_last_name": "[{\"end\":91,\"start\":87},{\"end\":193,\"start\":191},{\"end\":357,\"start\":353},{\"end\":446,\"start\":443},{\"end\":457,\"start\":452},{\"end\":566,\"start\":562}]", "author_first_name": "[{\"end\":86,\"start\":78},{\"end\":190,\"start\":182},{\"end\":352,\"start\":346},{\"end\":442,\"start\":435},{\"end\":451,\"start\":448},{\"end\":559,\"start\":551},{\"end\":561,\"start\":560}]", "author_affiliation": "[{\"end\":151,\"start\":93},{\"end\":180,\"start\":153},{\"end\":252,\"start\":195},{\"end\":281,\"start\":254},{\"end\":344,\"start\":283},{\"end\":386,\"start\":359},{\"end\":433,\"start\":388},{\"end\":486,\"start\":459},{\"end\":549,\"start\":488},{\"end\":626,\"start\":568},{\"end\":655,\"start\":628}]", "title": "[{\"end\":75,\"start\":1},{\"end\":731,\"start\":657}]", "venue": null, "abstract": "[{\"end\":2489,\"start\":733}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3260,\"start\":3242},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3297,\"start\":3277},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3312,\"start\":3297},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":3352,\"start\":3333},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3735,\"start\":3720},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3753,\"start\":3735},{\"end\":4200,\"start\":4181},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5036,\"start\":5005},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5087,\"start\":5051},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":7039,\"start\":7012},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7207,\"start\":7186},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":7376,\"start\":7354},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":7412,\"start\":7390},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7919,\"start\":7889},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8073,\"start\":8056},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8175,\"start\":8159},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8367,\"start\":8351},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8410,\"start\":8394},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10346,\"start\":10329},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":12483,\"start\":12447},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":13392,\"start\":13374},{\"end\":13412,\"start\":13392},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":13427,\"start\":13412},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":13861,\"start\":13843},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":14431,\"start\":14411},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":14795,\"start\":14779},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":14810,\"start\":14795},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":16212,\"start\":16193},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":16503,\"start\":16486},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":16554,\"start\":16538},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":16621,\"start\":16605},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":16676,\"start\":16637},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":17041,\"start\":17021},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":17980,\"start\":17961},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":18118,\"start\":18098},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":18311,\"start\":18272},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":18358,\"start\":18337},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":18931,\"start\":18914},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":18960,\"start\":18944},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":22631,\"start\":22613}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":25847,\"start\":25728},{\"attributes\":{\"id\":\"fig_1\"},\"end\":26120,\"start\":25848},{\"attributes\":{\"id\":\"fig_2\"},\"end\":26179,\"start\":26121},{\"attributes\":{\"id\":\"fig_3\"},\"end\":26544,\"start\":26180},{\"attributes\":{\"id\":\"fig_4\"},\"end\":26895,\"start\":26545},{\"attributes\":{\"id\":\"fig_5\"},\"end\":27059,\"start\":26896},{\"attributes\":{\"id\":\"fig_6\"},\"end\":27215,\"start\":27060},{\"attributes\":{\"id\":\"fig_7\"},\"end\":27324,\"start\":27216},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":28200,\"start\":27325},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":28352,\"start\":28201}]", "paragraph": "[{\"end\":2917,\"start\":2505},{\"end\":3929,\"start\":2919},{\"end\":4628,\"start\":3931},{\"end\":5309,\"start\":4630},{\"end\":5769,\"start\":5311},{\"end\":5815,\"start\":5771},{\"end\":6221,\"start\":5817},{\"end\":6385,\"start\":6223},{\"end\":6541,\"start\":6387},{\"end\":6696,\"start\":6543},{\"end\":6884,\"start\":6714},{\"end\":7292,\"start\":6886},{\"end\":7619,\"start\":7294},{\"end\":8594,\"start\":7621},{\"end\":8843,\"start\":8614},{\"end\":9285,\"start\":8895},{\"end\":9361,\"start\":9358},{\"end\":10005,\"start\":9363},{\"end\":10654,\"start\":10109},{\"end\":10737,\"start\":10679},{\"end\":10803,\"start\":10739},{\"end\":12388,\"start\":10805},{\"end\":13029,\"start\":12418},{\"end\":13181,\"start\":13095},{\"end\":13310,\"start\":13231},{\"end\":13499,\"start\":13331},{\"end\":13708,\"start\":13501},{\"end\":14020,\"start\":13777},{\"end\":14249,\"start\":14022},{\"end\":14545,\"start\":14349},{\"end\":14706,\"start\":14597},{\"end\":14918,\"start\":14751},{\"end\":15544,\"start\":14963},{\"end\":16028,\"start\":15557},{\"end\":16978,\"start\":16030},{\"end\":17399,\"start\":16999},{\"end\":17708,\"start\":17415},{\"end\":18645,\"start\":17731},{\"end\":18842,\"start\":18682},{\"end\":19558,\"start\":18844},{\"end\":21160,\"start\":19560},{\"end\":22581,\"start\":21208},{\"end\":25037,\"start\":22607},{\"end\":25727,\"start\":25069}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9357,\"start\":9286},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10033,\"start\":10006},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10108,\"start\":10033},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13094,\"start\":13030},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13230,\"start\":13182},{\"attributes\":{\"id\":\"formula_5\"},\"end\":13776,\"start\":13709},{\"attributes\":{\"id\":\"formula_6\"},\"end\":14276,\"start\":14250},{\"attributes\":{\"id\":\"formula_7\"},\"end\":14348,\"start\":14276},{\"attributes\":{\"id\":\"formula_8\"},\"end\":14596,\"start\":14546},{\"attributes\":{\"id\":\"formula_9\"},\"end\":14962,\"start\":14919}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":15992,\"start\":15984},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":18787,\"start\":18780},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":18998,\"start\":18990},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":21434,\"start\":21427},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":24012,\"start\":24005}]", "section_header": "[{\"end\":2503,\"start\":2491},{\"end\":6712,\"start\":6699},{\"end\":8612,\"start\":8597},{\"end\":8893,\"start\":8846},{\"end\":10677,\"start\":10657},{\"end\":12416,\"start\":12391},{\"end\":13329,\"start\":13313},{\"end\":14749,\"start\":14709},{\"end\":15555,\"start\":15547},{\"end\":16997,\"start\":16981},{\"end\":17413,\"start\":17402},{\"end\":17729,\"start\":17711},{\"end\":18680,\"start\":18648},{\"end\":21206,\"start\":21163},{\"end\":22605,\"start\":22584},{\"end\":25054,\"start\":25040},{\"end\":25067,\"start\":25057},{\"end\":25739,\"start\":25729},{\"end\":26132,\"start\":26122},{\"end\":26191,\"start\":26181},{\"end\":26907,\"start\":26897},{\"end\":27071,\"start\":27061},{\"end\":27227,\"start\":27217},{\"end\":27335,\"start\":27326},{\"end\":28211,\"start\":28202}]", "table": "[{\"end\":28200,\"start\":27337}]", "figure_caption": "[{\"end\":25847,\"start\":25741},{\"end\":26120,\"start\":25850},{\"end\":26179,\"start\":26134},{\"end\":26544,\"start\":26193},{\"end\":26895,\"start\":26547},{\"end\":27059,\"start\":26909},{\"end\":27215,\"start\":27073},{\"end\":27324,\"start\":27229},{\"end\":28352,\"start\":28213}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8919,\"start\":8913},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":10933,\"start\":10927},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":11384,\"start\":11378},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":11398,\"start\":11392},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":11429,\"start\":11423},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":11446,\"start\":11437},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":11557,\"start\":11548},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":11775,\"start\":11769},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":11973,\"start\":11966},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":19596,\"start\":19590},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":19703,\"start\":19697},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":21011,\"start\":21005},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":21684,\"start\":21678},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":23218,\"start\":23211},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":24459,\"start\":24450}]", "bib_author_first_name": "[{\"end\":28492,\"start\":28491},{\"end\":28501,\"start\":28500},{\"end\":29026,\"start\":29025},{\"end\":29035,\"start\":29034},{\"end\":29043,\"start\":29042},{\"end\":29444,\"start\":29443},{\"end\":29451,\"start\":29450},{\"end\":29459,\"start\":29458},{\"end\":29466,\"start\":29465},{\"end\":29468,\"start\":29467},{\"end\":29478,\"start\":29477},{\"end\":29833,\"start\":29832},{\"end\":29841,\"start\":29840},{\"end\":29848,\"start\":29847},{\"end\":29855,\"start\":29854},{\"end\":29866,\"start\":29862},{\"end\":29870,\"start\":29869},{\"end\":30316,\"start\":30315},{\"end\":30327,\"start\":30326},{\"end\":30338,\"start\":30337},{\"end\":30348,\"start\":30347},{\"end\":30644,\"start\":30643},{\"end\":30652,\"start\":30651},{\"end\":30660,\"start\":30659},{\"end\":30667,\"start\":30666},{\"end\":31079,\"start\":31078},{\"end\":31081,\"start\":31080},{\"end\":31090,\"start\":31089},{\"end\":31092,\"start\":31091},{\"end\":31101,\"start\":31100},{\"end\":31559,\"start\":31558},{\"end\":31561,\"start\":31560},{\"end\":31571,\"start\":31570},{\"end\":31584,\"start\":31583},{\"end\":32029,\"start\":32028},{\"end\":32039,\"start\":32038},{\"end\":32051,\"start\":32050},{\"end\":32066,\"start\":32065},{\"end\":32077,\"start\":32076},{\"end\":32546,\"start\":32545},{\"end\":32555,\"start\":32554},{\"end\":32561,\"start\":32560},{\"end\":32568,\"start\":32567},{\"end\":32998,\"start\":32997},{\"end\":33007,\"start\":33006},{\"end\":33421,\"start\":33420},{\"end\":33431,\"start\":33430},{\"end\":33439,\"start\":33438},{\"end\":33448,\"start\":33447},{\"end\":33731,\"start\":33730},{\"end\":33741,\"start\":33740},{\"end\":33753,\"start\":33749},{\"end\":33757,\"start\":33756},{\"end\":34188,\"start\":34187},{\"end\":34198,\"start\":34197},{\"end\":34207,\"start\":34206},{\"end\":34218,\"start\":34217},{\"end\":34230,\"start\":34229},{\"end\":34245,\"start\":34241},{\"end\":34249,\"start\":34248},{\"end\":34502,\"start\":34501},{\"end\":34504,\"start\":34503},{\"end\":34514,\"start\":34513},{\"end\":34725,\"start\":34724},{\"end\":34734,\"start\":34733},{\"end\":34747,\"start\":34746},{\"end\":34753,\"start\":34752},{\"end\":35207,\"start\":35206},{\"end\":35213,\"start\":35212},{\"end\":35221,\"start\":35220},{\"end\":35229,\"start\":35228},{\"end\":35236,\"start\":35235},{\"end\":35243,\"start\":35242},{\"end\":35670,\"start\":35669},{\"end\":35676,\"start\":35675},{\"end\":35682,\"start\":35681},{\"end\":35689,\"start\":35688},{\"end\":35698,\"start\":35697},{\"end\":35706,\"start\":35705},{\"end\":36174,\"start\":36173},{\"end\":36180,\"start\":36179},{\"end\":36187,\"start\":36186},{\"end\":36193,\"start\":36192},{\"end\":36200,\"start\":36199},{\"end\":36210,\"start\":36206},{\"end\":36214,\"start\":36213},{\"end\":36563,\"start\":36562},{\"end\":36570,\"start\":36569},{\"end\":36577,\"start\":36576},{\"end\":36585,\"start\":36584},{\"end\":36989,\"start\":36988},{\"end\":36998,\"start\":36997},{\"end\":37008,\"start\":37007},{\"end\":37014,\"start\":37013},{\"end\":37022,\"start\":37021},{\"end\":37491,\"start\":37490},{\"end\":37501,\"start\":37500},{\"end\":37518,\"start\":37517},{\"end\":37520,\"start\":37519},{\"end\":37716,\"start\":37715},{\"end\":37726,\"start\":37725},{\"end\":37737,\"start\":37736},{\"end\":37747,\"start\":37746},{\"end\":38066,\"start\":38065},{\"end\":38077,\"start\":38073},{\"end\":38087,\"start\":38083},{\"end\":38098,\"start\":38094},{\"end\":38511,\"start\":38510},{\"end\":38523,\"start\":38522},{\"end\":38537,\"start\":38536},{\"end\":38548,\"start\":38547},{\"end\":38558,\"start\":38557},{\"end\":38569,\"start\":38568},{\"end\":38994,\"start\":38993},{\"end\":39009,\"start\":39008},{\"end\":39018,\"start\":39017},{\"end\":39478,\"start\":39477},{\"end\":39486,\"start\":39485},{\"end\":39495,\"start\":39494},{\"end\":39501,\"start\":39500},{\"end\":39511,\"start\":39507},{\"end\":39515,\"start\":39514},{\"end\":39998,\"start\":39994},{\"end\":40009,\"start\":40005},{\"end\":40019,\"start\":40015},{\"end\":40026,\"start\":40025},{\"end\":40033,\"start\":40032},{\"end\":40042,\"start\":40041},{\"end\":40532,\"start\":40531},{\"end\":40540,\"start\":40539},{\"end\":40546,\"start\":40545},{\"end\":40552,\"start\":40551},{\"end\":40558,\"start\":40557},{\"end\":40565,\"start\":40564},{\"end\":40573,\"start\":40572},{\"end\":40581,\"start\":40580},{\"end\":40583,\"start\":40582},{\"end\":41004,\"start\":41003},{\"end\":41010,\"start\":41009},{\"end\":41017,\"start\":41016},{\"end\":41024,\"start\":41023},{\"end\":41033,\"start\":41032},{\"end\":41047,\"start\":41043},{\"end\":41512,\"start\":41511},{\"end\":41520,\"start\":41519},{\"end\":41527,\"start\":41526},{\"end\":41535,\"start\":41534},{\"end\":41543,\"start\":41542},{\"end\":41550,\"start\":41549},{\"end\":41556,\"start\":41555},{\"end\":41813,\"start\":41812},{\"end\":41819,\"start\":41818},{\"end\":41831,\"start\":41830},{\"end\":41841,\"start\":41840},{\"end\":41858,\"start\":41851},{\"end\":41862,\"start\":41861},{\"end\":42270,\"start\":42269},{\"end\":42276,\"start\":42275},{\"end\":42288,\"start\":42287},{\"end\":42299,\"start\":42298},{\"end\":42760,\"start\":42759},{\"end\":42766,\"start\":42765},{\"end\":43184,\"start\":43183},{\"end\":43190,\"start\":43189},{\"end\":43657,\"start\":43656},{\"end\":43663,\"start\":43662},{\"end\":44144,\"start\":44143},{\"end\":44153,\"start\":44152},{\"end\":44165,\"start\":44161},{\"end\":44174,\"start\":44173},{\"end\":44181,\"start\":44180},{\"end\":44189,\"start\":44188},{\"end\":44196,\"start\":44195},{\"end\":44628,\"start\":44627},{\"end\":44637,\"start\":44636},{\"end\":44646,\"start\":44645},{\"end\":44648,\"start\":44647},{\"end\":44657,\"start\":44656},{\"end\":44670,\"start\":44669},{\"end\":45137,\"start\":45136},{\"end\":45144,\"start\":45143},{\"end\":45153,\"start\":45152},{\"end\":45160,\"start\":45159},{\"end\":45581,\"start\":45580},{\"end\":45588,\"start\":45587},{\"end\":45595,\"start\":45594},{\"end\":45597,\"start\":45596},{\"end\":45604,\"start\":45603}]", "bib_author_last_name": "[{\"end\":28498,\"start\":28493},{\"end\":28515,\"start\":28502},{\"end\":29032,\"start\":29027},{\"end\":29040,\"start\":29036},{\"end\":29057,\"start\":29044},{\"end\":29448,\"start\":29445},{\"end\":29456,\"start\":29452},{\"end\":29463,\"start\":29460},{\"end\":29475,\"start\":29469},{\"end\":29488,\"start\":29479},{\"end\":29838,\"start\":29834},{\"end\":29845,\"start\":29842},{\"end\":29852,\"start\":29849},{\"end\":29860,\"start\":29856},{\"end\":30324,\"start\":30317},{\"end\":30335,\"start\":30328},{\"end\":30345,\"start\":30339},{\"end\":30357,\"start\":30349},{\"end\":30649,\"start\":30645},{\"end\":30657,\"start\":30653},{\"end\":30664,\"start\":30661},{\"end\":30672,\"start\":30668},{\"end\":31087,\"start\":31082},{\"end\":31098,\"start\":31093},{\"end\":31108,\"start\":31102},{\"end\":31568,\"start\":31562},{\"end\":31581,\"start\":31572},{\"end\":31591,\"start\":31585},{\"end\":32036,\"start\":32030},{\"end\":32048,\"start\":32040},{\"end\":32063,\"start\":32052},{\"end\":32074,\"start\":32067},{\"end\":32088,\"start\":32078},{\"end\":32552,\"start\":32547},{\"end\":32558,\"start\":32556},{\"end\":32565,\"start\":32562},{\"end\":32572,\"start\":32569},{\"end\":33004,\"start\":32999},{\"end\":33016,\"start\":33008},{\"end\":33428,\"start\":33422},{\"end\":33436,\"start\":33432},{\"end\":33445,\"start\":33440},{\"end\":33457,\"start\":33449},{\"end\":33738,\"start\":33732},{\"end\":33747,\"start\":33742},{\"end\":34195,\"start\":34189},{\"end\":34204,\"start\":34199},{\"end\":34215,\"start\":34208},{\"end\":34227,\"start\":34219},{\"end\":34239,\"start\":34231},{\"end\":34511,\"start\":34505},{\"end\":34517,\"start\":34515},{\"end\":34731,\"start\":34726},{\"end\":34744,\"start\":34735},{\"end\":34750,\"start\":34748},{\"end\":34758,\"start\":34754},{\"end\":35210,\"start\":35208},{\"end\":35218,\"start\":35214},{\"end\":35226,\"start\":35222},{\"end\":35233,\"start\":35230},{\"end\":35240,\"start\":35237},{\"end\":35249,\"start\":35244},{\"end\":35673,\"start\":35671},{\"end\":35679,\"start\":35677},{\"end\":35686,\"start\":35683},{\"end\":35695,\"start\":35690},{\"end\":35703,\"start\":35699},{\"end\":35710,\"start\":35707},{\"end\":36177,\"start\":36175},{\"end\":36184,\"start\":36181},{\"end\":36190,\"start\":36188},{\"end\":36197,\"start\":36194},{\"end\":36204,\"start\":36201},{\"end\":36567,\"start\":36564},{\"end\":36574,\"start\":36571},{\"end\":36582,\"start\":36578},{\"end\":36590,\"start\":36586},{\"end\":36995,\"start\":36990},{\"end\":37005,\"start\":36999},{\"end\":37011,\"start\":37009},{\"end\":37019,\"start\":37015},{\"end\":37028,\"start\":37023},{\"end\":37498,\"start\":37492},{\"end\":37515,\"start\":37502},{\"end\":37526,\"start\":37521},{\"end\":37723,\"start\":37717},{\"end\":37734,\"start\":37727},{\"end\":37744,\"start\":37738},{\"end\":37755,\"start\":37748},{\"end\":38071,\"start\":38067},{\"end\":38081,\"start\":38078},{\"end\":38092,\"start\":38088},{\"end\":38102,\"start\":38099},{\"end\":38520,\"start\":38512},{\"end\":38534,\"start\":38524},{\"end\":38545,\"start\":38538},{\"end\":38555,\"start\":38549},{\"end\":38566,\"start\":38559},{\"end\":38574,\"start\":38570},{\"end\":39006,\"start\":38995},{\"end\":39015,\"start\":39010},{\"end\":39027,\"start\":39019},{\"end\":39483,\"start\":39479},{\"end\":39492,\"start\":39487},{\"end\":39498,\"start\":39496},{\"end\":39505,\"start\":39502},{\"end\":40003,\"start\":39999},{\"end\":40013,\"start\":40010},{\"end\":40023,\"start\":40020},{\"end\":40030,\"start\":40027},{\"end\":40039,\"start\":40034},{\"end\":40052,\"start\":40043},{\"end\":40537,\"start\":40533},{\"end\":40543,\"start\":40541},{\"end\":40549,\"start\":40547},{\"end\":40555,\"start\":40553},{\"end\":40562,\"start\":40559},{\"end\":40570,\"start\":40566},{\"end\":40578,\"start\":40574},{\"end\":40587,\"start\":40584},{\"end\":41007,\"start\":41005},{\"end\":41014,\"start\":41011},{\"end\":41021,\"start\":41018},{\"end\":41030,\"start\":41025},{\"end\":41041,\"start\":41034},{\"end\":41052,\"start\":41048},{\"end\":41517,\"start\":41513},{\"end\":41524,\"start\":41521},{\"end\":41532,\"start\":41528},{\"end\":41540,\"start\":41536},{\"end\":41547,\"start\":41544},{\"end\":41553,\"start\":41551},{\"end\":41560,\"start\":41557},{\"end\":41816,\"start\":41814},{\"end\":41828,\"start\":41820},{\"end\":41838,\"start\":41832},{\"end\":41849,\"start\":41842},{\"end\":42273,\"start\":42271},{\"end\":42285,\"start\":42277},{\"end\":42296,\"start\":42289},{\"end\":42307,\"start\":42300},{\"end\":42763,\"start\":42761},{\"end\":42774,\"start\":42767},{\"end\":43187,\"start\":43185},{\"end\":43198,\"start\":43191},{\"end\":43660,\"start\":43658},{\"end\":43671,\"start\":43664},{\"end\":44150,\"start\":44145},{\"end\":44159,\"start\":44154},{\"end\":44171,\"start\":44166},{\"end\":44178,\"start\":44175},{\"end\":44186,\"start\":44182},{\"end\":44193,\"start\":44190},{\"end\":44202,\"start\":44197},{\"end\":44634,\"start\":44629},{\"end\":44643,\"start\":44638},{\"end\":44654,\"start\":44649},{\"end\":44667,\"start\":44658},{\"end\":44675,\"start\":44671},{\"end\":45141,\"start\":45138},{\"end\":45150,\"start\":45145},{\"end\":45157,\"start\":45154},{\"end\":45166,\"start\":45161},{\"end\":45585,\"start\":45582},{\"end\":45592,\"start\":45589},{\"end\":45601,\"start\":45598},{\"end\":45609,\"start\":45605}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":4319595},\"end\":28936,\"start\":28354},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":51882734},\"end\":29378,\"start\":28938},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":216009},\"end\":29760,\"start\":29380},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":4564405},\"end\":30245,\"start\":29762},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":201317137},\"end\":30576,\"start\":30247},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":9569924},\"end\":31018,\"start\":30578},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":206593710},\"end\":31471,\"start\":31020},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":51894650},\"end\":31943,\"start\":31473},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":326772},\"end\":32469,\"start\":31945},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":22066308},\"end\":32920,\"start\":32471},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":6576859},\"end\":33342,\"start\":32922},{\"attributes\":{\"doi\":\"arXiv:1710.10196\",\"id\":\"b11\"},\"end\":33654,\"start\":33344},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":54482423},\"end\":34130,\"start\":33656},{\"attributes\":{\"doi\":\"arXiv:1912.04958\",\"id\":\"b13\"},\"end\":34455,\"start\":34132},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b14\"},\"end\":34656,\"start\":34457},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":199543931},\"end\":35136,\"start\":34658},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":220935924},\"end\":35571,\"start\":35138},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":219618216},\"end\":36118,\"start\":35573},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":4889032},\"end\":36517,\"start\":36120},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":459456},\"end\":36895,\"start\":36519},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":212634162},\"end\":37438,\"start\":36897},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":16892725},\"end\":37713,\"start\":37440},{\"attributes\":{\"doi\":\"arXiv:1802.05957\",\"id\":\"b22\"},\"end\":37999,\"start\":37715},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":81981856},\"end\":38469,\"start\":38001},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":1687220},\"end\":38919,\"start\":38471},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":2843566},\"end\":39397,\"start\":38921},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":10973561},\"end\":39911,\"start\":39399},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":41805341},\"end\":40462,\"start\":39913},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":52154773},\"end\":40946,\"start\":40464},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":19668149},\"end\":41433,\"start\":40948},{\"attributes\":{\"id\":\"b30\"},\"end\":41751,\"start\":41435},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":52084187},\"end\":42188,\"start\":41753},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":3721829},\"end\":42690,\"start\":42190},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":35258007},\"end\":43085,\"start\":42692},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":39018476},\"end\":43541,\"start\":43087},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":2064528},\"end\":44073,\"start\":43543},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":52955830},\"end\":44553,\"start\":44075},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":4766599},\"end\":45067,\"start\":44555},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":208512930},\"end\":45529,\"start\":45069},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":11859568},\"end\":45893,\"start\":45531}]", "bib_title": "[{\"end\":28489,\"start\":28354},{\"end\":29023,\"start\":28938},{\"end\":29441,\"start\":29380},{\"end\":29830,\"start\":29762},{\"end\":30313,\"start\":30247},{\"end\":30641,\"start\":30578},{\"end\":31076,\"start\":31020},{\"end\":31556,\"start\":31473},{\"end\":32026,\"start\":31945},{\"end\":32543,\"start\":32471},{\"end\":32995,\"start\":32922},{\"end\":33728,\"start\":33656},{\"end\":34722,\"start\":34658},{\"end\":35204,\"start\":35138},{\"end\":35667,\"start\":35573},{\"end\":36171,\"start\":36120},{\"end\":36560,\"start\":36519},{\"end\":36986,\"start\":36897},{\"end\":37488,\"start\":37440},{\"end\":38063,\"start\":38001},{\"end\":38508,\"start\":38471},{\"end\":38991,\"start\":38921},{\"end\":39475,\"start\":39399},{\"end\":39992,\"start\":39913},{\"end\":40529,\"start\":40464},{\"end\":41001,\"start\":40948},{\"end\":41810,\"start\":41753},{\"end\":42267,\"start\":42190},{\"end\":42757,\"start\":42692},{\"end\":43181,\"start\":43087},{\"end\":43654,\"start\":43543},{\"end\":44141,\"start\":44075},{\"end\":44625,\"start\":44555},{\"end\":45134,\"start\":45069},{\"end\":45578,\"start\":45531}]", "bib_author": "[{\"end\":28500,\"start\":28491},{\"end\":28517,\"start\":28500},{\"end\":29034,\"start\":29025},{\"end\":29042,\"start\":29034},{\"end\":29059,\"start\":29042},{\"end\":29450,\"start\":29443},{\"end\":29458,\"start\":29450},{\"end\":29465,\"start\":29458},{\"end\":29477,\"start\":29465},{\"end\":29490,\"start\":29477},{\"end\":29840,\"start\":29832},{\"end\":29847,\"start\":29840},{\"end\":29854,\"start\":29847},{\"end\":29862,\"start\":29854},{\"end\":29869,\"start\":29862},{\"end\":29873,\"start\":29869},{\"end\":30326,\"start\":30315},{\"end\":30337,\"start\":30326},{\"end\":30347,\"start\":30337},{\"end\":30359,\"start\":30347},{\"end\":30651,\"start\":30643},{\"end\":30659,\"start\":30651},{\"end\":30666,\"start\":30659},{\"end\":30674,\"start\":30666},{\"end\":31089,\"start\":31078},{\"end\":31100,\"start\":31089},{\"end\":31110,\"start\":31100},{\"end\":31570,\"start\":31558},{\"end\":31583,\"start\":31570},{\"end\":31593,\"start\":31583},{\"end\":32038,\"start\":32028},{\"end\":32050,\"start\":32038},{\"end\":32065,\"start\":32050},{\"end\":32076,\"start\":32065},{\"end\":32090,\"start\":32076},{\"end\":32554,\"start\":32545},{\"end\":32560,\"start\":32554},{\"end\":32567,\"start\":32560},{\"end\":32574,\"start\":32567},{\"end\":33006,\"start\":32997},{\"end\":33018,\"start\":33006},{\"end\":33430,\"start\":33420},{\"end\":33438,\"start\":33430},{\"end\":33447,\"start\":33438},{\"end\":33459,\"start\":33447},{\"end\":33740,\"start\":33730},{\"end\":33749,\"start\":33740},{\"end\":33756,\"start\":33749},{\"end\":33760,\"start\":33756},{\"end\":34197,\"start\":34187},{\"end\":34206,\"start\":34197},{\"end\":34217,\"start\":34206},{\"end\":34229,\"start\":34217},{\"end\":34241,\"start\":34229},{\"end\":34248,\"start\":34241},{\"end\":34252,\"start\":34248},{\"end\":34513,\"start\":34501},{\"end\":34519,\"start\":34513},{\"end\":34733,\"start\":34724},{\"end\":34746,\"start\":34733},{\"end\":34752,\"start\":34746},{\"end\":34760,\"start\":34752},{\"end\":35212,\"start\":35206},{\"end\":35220,\"start\":35212},{\"end\":35228,\"start\":35220},{\"end\":35235,\"start\":35228},{\"end\":35242,\"start\":35235},{\"end\":35251,\"start\":35242},{\"end\":35675,\"start\":35669},{\"end\":35681,\"start\":35675},{\"end\":35688,\"start\":35681},{\"end\":35697,\"start\":35688},{\"end\":35705,\"start\":35697},{\"end\":35712,\"start\":35705},{\"end\":36179,\"start\":36173},{\"end\":36186,\"start\":36179},{\"end\":36192,\"start\":36186},{\"end\":36199,\"start\":36192},{\"end\":36206,\"start\":36199},{\"end\":36213,\"start\":36206},{\"end\":36217,\"start\":36213},{\"end\":36569,\"start\":36562},{\"end\":36576,\"start\":36569},{\"end\":36584,\"start\":36576},{\"end\":36592,\"start\":36584},{\"end\":36997,\"start\":36988},{\"end\":37007,\"start\":36997},{\"end\":37013,\"start\":37007},{\"end\":37021,\"start\":37013},{\"end\":37030,\"start\":37021},{\"end\":37500,\"start\":37490},{\"end\":37517,\"start\":37500},{\"end\":37528,\"start\":37517},{\"end\":37725,\"start\":37715},{\"end\":37736,\"start\":37725},{\"end\":37746,\"start\":37736},{\"end\":37757,\"start\":37746},{\"end\":38073,\"start\":38065},{\"end\":38083,\"start\":38073},{\"end\":38094,\"start\":38083},{\"end\":38104,\"start\":38094},{\"end\":38522,\"start\":38510},{\"end\":38536,\"start\":38522},{\"end\":38547,\"start\":38536},{\"end\":38557,\"start\":38547},{\"end\":38568,\"start\":38557},{\"end\":38576,\"start\":38568},{\"end\":39008,\"start\":38993},{\"end\":39017,\"start\":39008},{\"end\":39029,\"start\":39017},{\"end\":39485,\"start\":39477},{\"end\":39494,\"start\":39485},{\"end\":39500,\"start\":39494},{\"end\":39507,\"start\":39500},{\"end\":39514,\"start\":39507},{\"end\":39518,\"start\":39514},{\"end\":40005,\"start\":39994},{\"end\":40015,\"start\":40005},{\"end\":40025,\"start\":40015},{\"end\":40032,\"start\":40025},{\"end\":40041,\"start\":40032},{\"end\":40054,\"start\":40041},{\"end\":40539,\"start\":40531},{\"end\":40545,\"start\":40539},{\"end\":40551,\"start\":40545},{\"end\":40557,\"start\":40551},{\"end\":40564,\"start\":40557},{\"end\":40572,\"start\":40564},{\"end\":40580,\"start\":40572},{\"end\":40589,\"start\":40580},{\"end\":41009,\"start\":41003},{\"end\":41016,\"start\":41009},{\"end\":41023,\"start\":41016},{\"end\":41032,\"start\":41023},{\"end\":41043,\"start\":41032},{\"end\":41054,\"start\":41043},{\"end\":41519,\"start\":41511},{\"end\":41526,\"start\":41519},{\"end\":41534,\"start\":41526},{\"end\":41542,\"start\":41534},{\"end\":41549,\"start\":41542},{\"end\":41555,\"start\":41549},{\"end\":41562,\"start\":41555},{\"end\":41818,\"start\":41812},{\"end\":41830,\"start\":41818},{\"end\":41840,\"start\":41830},{\"end\":41851,\"start\":41840},{\"end\":41861,\"start\":41851},{\"end\":41865,\"start\":41861},{\"end\":42275,\"start\":42269},{\"end\":42287,\"start\":42275},{\"end\":42298,\"start\":42287},{\"end\":42309,\"start\":42298},{\"end\":42765,\"start\":42759},{\"end\":42776,\"start\":42765},{\"end\":43189,\"start\":43183},{\"end\":43200,\"start\":43189},{\"end\":43662,\"start\":43656},{\"end\":43673,\"start\":43662},{\"end\":44152,\"start\":44143},{\"end\":44161,\"start\":44152},{\"end\":44173,\"start\":44161},{\"end\":44180,\"start\":44173},{\"end\":44188,\"start\":44180},{\"end\":44195,\"start\":44188},{\"end\":44204,\"start\":44195},{\"end\":44636,\"start\":44627},{\"end\":44645,\"start\":44636},{\"end\":44656,\"start\":44645},{\"end\":44669,\"start\":44656},{\"end\":44677,\"start\":44669},{\"end\":45143,\"start\":45136},{\"end\":45152,\"start\":45143},{\"end\":45159,\"start\":45152},{\"end\":45168,\"start\":45159},{\"end\":45587,\"start\":45580},{\"end\":45594,\"start\":45587},{\"end\":45603,\"start\":45594},{\"end\":45611,\"start\":45603}]", "bib_venue": "[{\"end\":28672,\"start\":28603},{\"end\":29174,\"start\":29125},{\"end\":30028,\"start\":29959},{\"end\":30809,\"start\":30750},{\"end\":31265,\"start\":31196},{\"end\":31708,\"start\":31659},{\"end\":32219,\"start\":32163},{\"end\":32709,\"start\":32650},{\"end\":33153,\"start\":33094},{\"end\":33915,\"start\":33846},{\"end\":34915,\"start\":34846},{\"end\":35366,\"start\":35317},{\"end\":35867,\"start\":35798},{\"end\":36332,\"start\":36283},{\"end\":36727,\"start\":36668},{\"end\":37185,\"start\":37116},{\"end\":38259,\"start\":38190},{\"end\":38705,\"start\":38649},{\"end\":39184,\"start\":39115},{\"end\":39673,\"start\":39604},{\"end\":40209,\"start\":40140},{\"end\":40710,\"start\":40658},{\"end\":41209,\"start\":41140},{\"end\":41980,\"start\":41931},{\"end\":42464,\"start\":42395},{\"end\":42891,\"start\":42842},{\"end\":43323,\"start\":43270},{\"end\":43828,\"start\":43759},{\"end\":44319,\"start\":44270},{\"end\":44832,\"start\":44763},{\"end\":45323,\"start\":45254},{\"end\":45726,\"start\":45677},{\"end\":28601,\"start\":28517},{\"end\":29123,\"start\":29059},{\"end\":29556,\"start\":29490},{\"end\":29957,\"start\":29873},{\"end\":30399,\"start\":30359},{\"end\":30748,\"start\":30674},{\"end\":31194,\"start\":31110},{\"end\":31657,\"start\":31593},{\"end\":32161,\"start\":32090},{\"end\":32648,\"start\":32574},{\"end\":33092,\"start\":33018},{\"end\":33418,\"start\":33344},{\"end\":33844,\"start\":33760},{\"end\":34185,\"start\":34132},{\"end\":34499,\"start\":34457},{\"end\":34844,\"start\":34760},{\"end\":35315,\"start\":35251},{\"end\":35796,\"start\":35712},{\"end\":36281,\"start\":36217},{\"end\":36666,\"start\":36592},{\"end\":37114,\"start\":37030},{\"end\":37558,\"start\":37528},{\"end\":37831,\"start\":37773},{\"end\":38188,\"start\":38104},{\"end\":38647,\"start\":38576},{\"end\":39113,\"start\":39029},{\"end\":39602,\"start\":39518},{\"end\":40138,\"start\":40054},{\"end\":40656,\"start\":40589},{\"end\":41138,\"start\":41054},{\"end\":41509,\"start\":41435},{\"end\":41929,\"start\":41865},{\"end\":42393,\"start\":42309},{\"end\":42840,\"start\":42776},{\"end\":43268,\"start\":43200},{\"end\":43757,\"start\":43673},{\"end\":44268,\"start\":44204},{\"end\":44761,\"start\":44677},{\"end\":45252,\"start\":45168},{\"end\":45675,\"start\":45611}]"}}}, "year": 2023, "month": 12, "day": 17}