{"id": 247922558, "updated": "2023-11-08 10:32:16.024", "metadata": {"title": "GALA: Toward Geometry-and-Lighting-Aware Object Search for Compositing", "authors": "[{\"first\":\"Sijie\",\"last\":\"Zhu\",\"middle\":[]},{\"first\":\"Zhe\",\"last\":\"Lin\",\"middle\":[]},{\"first\":\"Scott\",\"last\":\"Cohen\",\"middle\":[]},{\"first\":\"Jason\",\"last\":\"Kuen\",\"middle\":[]},{\"first\":\"Zhifei\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Chen\",\"last\":\"Chen\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Compositing-aware object search aims to find the most compatible objects for compositing given a background image and a query bounding box. Previous works focus on learning compatibility between the foreground object and background, but fail to learn other important factors from large-scale data, i.e. geometry and lighting. To move a step further, this paper proposes GALA (Geometry-and-Lighting-Aware), a generic foreground object search method with discriminative modeling on geometry and lighting compatibility for open-world image compositing. Remarkably, it achieves state-of-the-art results on the CAIS dataset and generalizes well on large-scale open-world datasets, i.e. Pixabay and Open Images. In addition, our method can effectively handle non-box scenarios, where users only provide background images without any input bounding box. A web demo (see supplementary materials) is built to showcase applications of the proposed method for compositing-aware search and automatic location/scale prediction for the foreground object.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2204.00125", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/eccv/ZhuLCKZ022", "doi": "10.48550/arxiv.2204.00125"}}, "content": {"source": {"pdf_hash": "4dcf1ba1921e9a24bdacbd4f2f9fc0c67f920f09", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2204.00125v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "43df68848d045f44419b48eb596406a6f06a1a3b", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/4dcf1ba1921e9a24bdacbd4f2f9fc0c67f920f09.txt", "contents": "\nGALA: Toward Geometry-and-Lighting-Aware Object Search for Compositing\n\n\nSijie Zhu sizhu@knights.ucf.edu \nCenter for Research in Computer Vision\nUniversity of Central Florida\n\n\nZhe Lin \nAdobe Research\n\n\nScott Cohen \nAdobe Research\n\n\nJason Kuen \nAdobe Research\n\n\nZhifei Zhang zzhang@adobe.com \nAdobe Research\n\n\nChen Chen chen.chen@crcv.ucf.edu \nCenter for Research in Computer Vision\nUniversity of Central Florida\n\n\nGALA: Toward Geometry-and-Lighting-Aware Object Search for Compositing\nForeground Object Retrieval, Image Compositing\nCompositing-aware object search aims to find the most compatible objects for compositing given a background image and a query bounding box. Previous works focus on learning compatibility between the foreground object and background, but fail to learn other important factors from large-scale data, i.e. geometry and lighting. To move a step further, this paper proposes GALA (Geometry-and-Lighting-Aware), a generic foreground object search method with discriminative modeling on geometry and lighting compatibility for open-world image compositing. Remarkably, it achieves state-of-the-art results on the CAIS dataset and generalizes well on large-scale open-world datasets, i.e. Pixabay and Open Images. In addition, our method can effectively handle non-box scenarios, where users only provide background images without any input bounding box. A web demo (see supplementary materials) is built to showcase applications of the proposed method for compositing-aware search and automatic location/scale prediction for the foreground object.\n\nIntroduction\n\nCompositing-aware object search/retrieval [27] aims to find suitable source images for compositing [18,25]. Specifically, given a background image and a bounding box indicating the compositing location, the objective is to retrieve compatible foreground objects from a large reference database, so that the composite image appears realistic. Harmonization [7,30,23,2,16] is then applied to adjust the color and edge pixels, but it is extremely challenging to automatically adjust the semantics, lighting, or geometry of foreground objects. Although recent relighting method [19] can generate realistic lighting change for human portraits, it does not tackle general object categories and the required 3D reconstruction is not always available. Therefore, the quality of the final composite image highly depends on the performance of foreground retrieval system, and a good system should be aware of semantics, lighting, and geometry of foreground objects. \u2020 This work was done during the first author's internship at Adobe Research.\n\n\narXiv:2204.00125v1 [cs.CV] 31 Mar 2022\n\nEarly work [27] on foreground retrieval follows a constrained setting, which requires the user to specify the object category for retrieval. Specifying the category sets a limit on the search space, thus preventing the system from recommending diverse sets of objects. Later, UFO [28] proposes an unconstrained search method, i.e. objects from all categories are considered as candidates for retrieval. The unconstrained setting is closer to real-world scenarios, which require a large and diverse foreground object gallery to satisfy different users. However, UFO [28] focuses on finding semantically compatible foreground object and does not explicitly model lighting and geometry, which are critical factors for making object compositing realistic, as shown in Figs. 1 and 2. More recent work [14,24] either explores additional annotation [14] for constrained setting, or only focuses on indoor furniture [24] with fine-grained sub-categories. Therefore, how to encourage awareness on geometry and lighting is still unclear for general unconstrained foreground retrieval. One way to consider these factors is to explicitly estimate the 3D geometry and lighting of the scene, but it is extremely\n\n\nQuery\n\nTop Retrieved Results\n\n\nUFO Ours\n\nOriginal Image Fig. 1: Comparison between state-of-the-art method (UFO [28] in the first row) and the proposed geometry-and-lighting-aware search (second row). The retrieved objects in the first row do not respect the geometry of the background scene, while results of the proposed method have better geometry compatibility.\n\n\nQuery vs Background Random UFO Ours\n\nCopy-Paste Harmonization Fig. 2: Comparison between state-of-the-art method (UFO [28]) and the proposed method. Objects retrieved by \"Random\" and \"UFO\" do not have good lighting compatibility even after harmonization. Our method better respects the lighting condition (light coming from front-right direction), and thus the final composite image is more realistic.\n\nchallenging to obtain realistic training data with 3D labels to do so. Instead, we aim to build a discriminative model which is sensitive to lighting and geometry mismatches using real-world image datasets, leading to a more generalizable and scalable solution.\n\nIn this paper, we propose a novel Geometry-And-Lighting-Aware (GALA) foreground object search system, which aims to retrieve objects that are compatible in terms of semantics, geometry, and lighting. Specifically, we design a model consisting of a foreground object encoder and background encoder, such that only the matching pairs of foreground object and background are closer to each other in the embedding space. To encode geometry and lighting sensitivity, we generate negative samples by augmenting the same objects with very different geometry and lighting conditions through homography transformation with leftright flip and non-linear illumination modification on foreground objects during training. Contrastive learning is then applied to push the transformed foreground object far away from the original one in the embedding space. However, the semantic compatibility of retrieved objects could degrade significantly if we train the foreground network jointly with the background network. Thus, we introduce an alternating training strategy to maintain semantic compatibility, while learning to respect geometry and lighting. GALA can also be extended to handle non-box scenarios, where the users only provide the background image without any bounding box or text input. Our model automatically retrieves foreground objects and predicts the best location and scale for compositing. Furthermore, we perform experiments and validate GALA for open-world datasets with an arbitrarily large number of categories, targeting real-world unconstrained applications. Previous works are mostly implemented on medium scale datasets (< 100, 000 objects) with limited categories, e.g. MSCOCO [17] only has 80 object categories. In contrast, we conduct experiments on large-scale real-world datasets, i.e. Pixabay [1] and Open Images [10], which contain significantly more categories and images with diverse contents. We show comparisons of various settings between the proposed method and previous works in Table 1 \n\n\nRelated Work\n\nSearch for Compositing. The idea of searching for foregrounds to insert into a new background is first proposed by Lalonde et al. [11], by explicitly estimating the 3D geometry of foreground objects, and the lighting map of background scenes. Zhao et al. [27] propose a learning-based compositing-aware search method, where the users provide a specific class as text input. The positive samples are augmented based on shape and semantics. UFO [28] assumes that the user does not specify any category, which is the most relevant setting to our work, but it only focuses on the semantic compatibility and ignores lighting and geometry. It trains a discriminator to distinguish real/fake object for a certain background and selects candidate positive samples for each background image. Li et al. [14] manually annotates multiple attributes as additional information to determine \"Interpretable Foreground Object\" with given categories. Wu et al. [24] propose a teacher-student framework for fine-grained indoor categories, which adopts multiple pre-trained features for foreground objects as teacher and trains the background network with KL-divergence. None of them learn discrimitive features on geometry and lighting for general unconstrained foreground search. Object placement. A thread of works [12,26,15] focus on object placement along with adversarial training, which predicts locations and scales to insert an object. Lee et al. [12] train a spatial transformer network to predict the location for placement and a shape mask to guide generation. Zhang et al. [26] train a location-scale prediction model using inpainted pure background images. Li et al. [15] first find candidate locations in indoor scenes, then predict the best human pose for a specific location. These methods either focus on street scenes with limited object categories, e.g. person, car, or indoor scenes with only human pose joints. Our location-scale prediction deals with a more challenging setting with general objects from diverse scenes by directly adopting our retrieval model.\n\n\nMethod\n\nIn this section, we introduce GALA, Geometry-and-Lighting-Aware foreground object search for compositing. We first formulate the problem and describe training data generation in Sec. 3.1. Then we present the details of contrastive learning with self-transformations and the alternating training strategy in Sec. 3.2 and 3.3. Finally, we show how GALA handles non-box scenarios in Sec. 3.4.\n\n\nProblem Statement\n\nGiven a background image I b with a bounding box (l, r, w, h), our objective is to retrieve a set of most compatible foreground object images {I f }, so that realistic composite images can be generated with simple harmonization techniques. Our framework aims to learn an embedding space so that compatible images are close to each other in this space and the ranking can be obtained by simply computing the cosine similarity or euclidean distance. Since foreground and background images have very different distributions and appearances, we use two different encoder networks N b , N f to generate the embedding features for I b , I f , respectively. During training, the only available annotation is semantic segmentation mask for each object. We use the mask to crop out the object as I f . Then I b is generated by applying a rectangle mask on the image covering the whole object. Since there is no manual annotation on positive and negative samples, we consider I b and I f generated from the same image as a positive sample to each other, while other image pairs are considered as negative samples. With a hardmargin triplet loss [3], the optimization can be formulated as arg\nmin N b ,N f L t , where L t = [S(N b (I b ), N f (I \u2212 f )) \u2212 S(N b (I b ), N f (I + f )) + m] + .(1)\nHere S means cosine similarity, and [.] + denotes the hinge function. m is the margin for triplet loss and I + f , I \u2212 f indicate the positive and negative foreground objects for a given I b .\n\n\nContrastive Learning with Self-transformation\n\nThe standard loss in Eq. 1 only considers the contrastive information between the original foreground object and other objects, which mainly focuses on semantic information. We argue that a Geometry-and-Lighting-aware system should be able to tell the difference if the foreground object has exactly the same semantics with different lighting and geometry conditions. We thus perform transformation on the original positive foreground object so that it can be considered as negative. As shown in Fig ferent lighting or geometry conditions, which do not match with the background image anymore. We consider the transformed object image I t f as negative and formulate another triplet loss ( Fig. 4):\nL c = [S(N b (I b ), N f (I t f )) \u2212 S(N b (I b ), N f (I f )) + m] + .(2)\nThe final loss is given by L = L t + L c from Eqs. 1 and 2. In Fig. 3, we show the pipeline of self transformations. For geometry transformation, we first apply random homography transformations, then left-right flip is applied with a 50% probability. For lighting transformation, we first find a random background image and apply Gaussian blur with a large radius, e.g. 100. The blurred map is then resized with interpolation to the size of foreground object, and masked with the segmentation mask. Finally, we enhance the variance of the lighting map with an exponential function so that the largest value of the map is 5. In this way, we get a non-linear illumination map which highlights a random region in the object image, and it is multiplied by the original object image to generate the final transformed image. Although the lighting may not be as realistic as 3D relighting [19], it is enough to be considered as a negative sample with very different illuminations.  \n\n\nAlternating Training\n\nWe notice that previous works [28,24] generally use pre-trained weights for foreground network N f and freeze its parameters during training (denoted as \"Fix Foreground\"), which means N f cannot learn from data. Since data augmentation like left-right flip is adopted for pre-training on ImageNet [4], the feature would be invariant to the left-right flip, which could change the geometry and direction of lighting. Therefore, the pre-trained weights are not discriminative to lighting and geometry changes, which will be further demonstrated in Sec. 4.4.  To learn geometry-and-lighting-aware representation, N f must be learnable. However, performance drops significantly if N f and N b are directly trained with Eq. 1 (denoted as \"Direct Training\" in Table 2), partly due to the imperfect segmentation mask, as some annotations (MSCOCO) are based on polygon. When cropping with an imperfect mask (Fig. 5), some edge pixels of the object are actually background pixels, which are very likely to be the same as other background edge pixels. Direct training may optimize the model to match the edge pixels without compatibility on semantics, as the edge pixel is a very strong cue for positive pairs. This issue could be tackled by additional mask augmentations (denoted as \"Aug\") to prevent the model from using such cue. We randomly erode the foreground mask and extend the background mask so that the cue becomes random. Examples are included in the supplementary material. \"Aug\" significantly improves the performance over direct training.\n\n\nEdge Pixels\n\nHowever, the performance is still much lower than using fixed foreground network. Another issue is that the foreground object images have very different appearances and distributions from regular images. And the \"Direct Training\" model N f does not respect semantics very well as compared with using ImageNet pre-trained weights. As shown in Fig. 6, \"tuned foreground\" retrieves irrelevant categories for a sidewalk query, e.g. bottle, car. Our solution is to alternatively train N b and N f as shown in Fig. 7. Since there is only one network trained in one stage, the embedding features of the trained network will not have much drift. In this way, our method maintains semantic information in foreground feature, while allowing N f to learn from data for other factors, i.e. lighting and geometry. When the user only provides a background image without any bounding box, we adopt the following random seed and greedy search algorithm to retrieve matching foreground objects. We first sample multiple random locations. Each location is assigned with multiple bounding boxes with different aspect ratios and scales. We retrieve foreground objects based on each of the bounding boxes and re-rank all the results based on cosine similarity. We select the best one as the default object, then assign an initial bounding box with the same aspect ratio as the object. The area of the initial box is empirically assigned as 1/25 of the query image. Then we apply k\u00d7k grid of locations ( Fig. 8) with initial bounding box to cover the query image in a sliding window manner. By computing the similarity score between the object and background with all the possible boxes, the box with the highest score is considered as the best location. A location heatmap is generated by interpolating the k \u00d7 k score matrix to the size of query image. One can always increase k or apply further refinement with smaller stride to improve the location. The scale is then selected by applying a range of scale ratios on the initial box at the best location, as shown in Fig. 8.\n\n\nExtension to Non-box Scenarios\n\n\nExperiment\n\nWe first conduct experiment on CAIS [27], which is specifically annotated for compositing-aware search.  [17], PASCAL VOC 2012 [5], ADE20K [29]. The training set has 86, 800 background images, and the original foreground object in each image is considered as ground-truth. The evaluation set has 80 manually selected background images and query bounding box to insert objects. About 16 \u223c 140 compatible foreground objects are annotated for each background image. Pixabay: \"pixabay.com\" is a stock image website containing tons of high-quality photos which are perfect for composing. The images are highly diverse, free to use, and substantial in number. We first collect about 928, 018 images, then apply object detection [22] and segmentation [13] to generate foreground objects and background images based on masks. In total, we get 5, 771, 912 foreground objects and 928, 018 background images. But some objects are not well segmented or have extremely small size which is not likely to be used for composting. Background images with overly large box is also not suitable as there is nothing left to tell what should be here. Therefore we only keep images with high confidence score (e.g. > 0.6) and proper bounding box size (e.g. box area in 5 \u223c 50% of the area of whole image). Finally, we get 833, 964 foreground and background pairs with 914 non-zero categories. They are then randomly split into training/evaluation set with portion of 90%/10%. Open Images: Open Images originally contains about 9 million images with 9, 605 trainable classes, thus is perfect for open-world evaluation. Up to 2.8 million objects from 350 categories are annotated with segmentation masks. We originally select 944, 024 images with 2, 686, 666 objects, then apply the same filtering procedure as Pixabay based on box size only, as the mask is annotated. In total, we keep 1, 374, 344 background and foreground object pairs in our experiments. They are then randomly split into training/evaluation set with portion of 90%/10%. Open Images is only used in ablation study.\n\n\nImplementation Details\n\nWe implement our method based on PyTorch [20] with multiprocessing to support large-scale training and evaluation. The models are trained on 8 Tesla V100 GPUs. Batch size is 40 per card for training. Following [28], we use VGG-19 [21] pre-trained on ImageNet [4] as backbone, as it achieves the best performance among different networks. All foreground objects are padded with white pixels as square images. The original object in background image is covered with rectangle mask with average value. Both foreground and background images are resized to 224\u00d7224 and normalized with average value before feed into networks. We use Adam [9] optimizer with learning rate of 0.00008 based on linear scaling rule [6] of multi-card training. The margins of Eq. 1, 2 are set as 0.3, 0.1.\n\n\nEvaluation Metrics\n\nFor retrieval, we use the most widely used metric mAP (mean Average Precision) and R@k (Recall@k). We also report mAP-100 which is the mAP for top 100 retrievals as a fair comparison with constrained methods, because constrained retrieval methods do not rank object in all categories. When there is only one ground-truth (the original object) for each background query, we report R@k as the percentage of background queries whose ground-truth foreground reference appears in top k retrievals. Although other objects may also be compatible, the original one should always have a good rank among all the objects.\n\n\nComparison with Previous Methods\n\nIn this section, we compare our method with previous works on CAIS [27] and Pixabay dataset. [14,24] are infeasible for comparison because they both have very different settings, and their codes and datasets are not released. More qualitative results are included in supplementary materials. \n\n\nCAIS:\n\nWe compare our method with previous state-of-the-art methods and their variants in Table 3. \"Shape\" [27] ranks all the foreground images by comparing their aspect ratio with the aspect ratio of query bounding box. \"RealismCNN\" [30] trains a discriminator to distinguish real/fake composite images by copypaste original and irrelevant objects into a background image. The score of the discriminator measures the realism of each composite, which is used to rank all the objects. \"CFO-C Search\" [27] and \"CFO-D Search\" [27] are two constrained search methods evaluated in unconstrained scenarios. \"CFO-C Search\" first trains a classifier to specify the category, then apply constrained retrieval only from this class. The results will be completely wrong if the classification fails. \"CFO-D Search\" first apply constrained search to retrieve 100 samples from each category. Then it adopts the discriminator of \"RealismCNN\" to rerank all these retrievals by compositing with each background. Note that this is computationally expensive and not scalable if there are hundreds of classes. \"UFO Search\" [28] applies the discriminator to generate extra positive samples.\n\nIn Table 3, we show the mAP-100 per class and the overall performance. The proposed method significantly (+5.33%) outperforms previous state-of-theart methods on overall performance. It also achieves better performance than UFO [28] on most of the categories. \"CFO-C Search\" and \"CFO-D Search\" may performs well on two categories, but they are both not scalable, because it is infeasible to train one model for each category or scan all objects with discriminator for each background on large-scale datasets with hundreds of categories.\n\nThe results indicate that geometry and lighting awareness can help the retrieval in general unconstrained retrieval.   Table 4 shows the Re-call@10 on selected categories in Pixabay. We select 4 majority and 4 medium classes, with about 5000 and 50 samples per class in evaluation set (Note that training set is 9 times larger). Then we compute the average Recall@10 on the last 50 long-tail classes. The proposed method significantly outperforms UFO [28] on all the classes. We also show the overall R@k on Pixabay in Table 5. The proposed method significantly outperforms UFO, which means the proposed method generalizes well on large-scale open-world setting.\n\n\nAblation Study\n\nGeometry and Lighting Sensitivity: Since our goal is to gain awareness on geometry and lighting, a good model should be discriminative/sensitive to the change of geometry and lighting on foreground images. We conduct experiment on Pixabay [1] and Open Images [10] to verify this point. We randomly select 2, 000 foregrounds along with their corresponding background images. Then for each foreground, we use the geometry and lighting transformations in Sec. 3.2 to generate transformed image. In total, we generate 100 transformed images for each foreground image, 50/50 for geometry/lighting transformation. Then we rank the original one along with the 50 geometry or lighting transformed images to compute the Recall@k in Table 6. We also measure the discriminative ability as the sensitivity to these transformations, i.e. the square euclidean distance between normalized embedding features of the original and transformed foregrounds. With L2 normalization, the square euclidean distance is d = 2 \u2212 2s, where s is the cosine similarity. Therefore higher sensitivity value means larger distance between the features of original and transformed objects, thus indicates stronger ability on distinguishing geometry or lighting transformations.  In Table 6, the overall proposed method has much higher sensitivity to both geometry and lighting transformations for both datasets. UFO [28] and \"Baseline\" both use fixed foreground features, thus have a low sensitivity. \"No Contrastive\" removes the proposed contrastive transformation but keeps the alternating training, which has a lower sensitivity and retrieval performance than \"Overall\", but better than \"Baseline\". The results demonstrate the effectiveness of both alternating training and contrastive learning on improving discriminative ability. We also show qualitative results of our ablations in Fig. 9 with diverse scenes, viewpoints and lighting. Our overall method better respects the geometry and lighting than baseline methods. Components Ablations: We further conduct detailed ablation study on CAIS to show the effectiveness of each component. \"Direct Training\" mean directly training two networks with loss in Eq. 1. \"Aug\" denotes direct training along with mask augmentations in Sec. 3.3 to prevent the model from overfitting on edge pixels. \"Fix+Aug\" is also used as \"Baseline\" in other tables. It uses a fixed foreground network with mask augmentations. Our \"Overall\" method adopt both alternating training and contrastive learning with mask augmentations. \"No Alternating\" removes alternating training from \"Overall\", and \"No Contrastive\" removes the contrastive learning loss in Eq. 2. As shown in Table 7, \"Overall\" performs the best among all the ablations, indicating the effectiveness of each component.  Training Strategy Ablations: We conduct ablation study on different alternating training strategies and select the best strategy for our overall method. All these ablations adopt mask augmentations without using contrastive learning. \"Zero Round\" means no alternating training, which is equivalent to the \"Baseline\". \"One Round\" (argmin N b L, then argmin N f L) trains foreground network after the background network is trained. \"Two Round\" trains the background network for another round based on the \"One Round\" model. \"Reverse Order\" (argmin N f L, then argmin N b L) first trains the foreground network, then trains the background network. Table 8 shows that \"One Round\" strategy performs the best among all the strategies, indicating that training more rounds is not beneficial. We thus use one round for our overall method.\n\nTransformation Ablations: To show the effectiveness of each transformation, we train our method with different transformations in contrastive learning. \"No Contrastive\" removes contrastive learning loss in Eq. 2. \"Geometry\" only adopts geometry transformation in Fig. 3, and \"Geometry+Lighting\" means each foreground is applied with both transformations. \"Geometry+Color\" uses linear color jittering instead of our non-linear lighting transformation on the top of \"Geometry\". In Table 9, the \"Geometry+Lighting\" outperforms other ablations, indicating that both transformations are necessary in our framework, and linear color jittering does not well simulate lighting changes.\n\n\nLocation and Scale Prediction\n\nSince CAIS has annotated bounding boxes on intact background images, we evaluate our location and scale prediction method based on these boxes for non-box scenarios. We compare the proposed method with random strategy and \"Baseline\" model. For each background image, we randomly select 5 of its annotated compatible foregrounds for evaluation and compute the average value to report. Qualitative results are included in Fig. 10.\n\n\nQuery\n\nComposite Images Fig. 10: Qualitative results of composite images for non-box scenarios, generated based on top retrieved objects with automatically predicted location/scale and harmonization [8].\n\nFor location prediction, given a background along with its annotated foreground object, we first apply 10 grid search with sliding window method in Sec. 3.4. To evaluate location and scale separately, the scale here is fixed as the scale of annotated box. Then bilinear interpolation is adopted to generate the heapmap, as shown in Fig. 11. The heapmap value is then normalized with the maximum and minimal value in the grid matrix, so that all values are in [0, 1]. Higher value means the model assigns high compatibility on the corresponding location. Note that the annotated box location may not be the only good location, other boxes with zero IOU (Intersection Over Union) can also have compatible locations, e.g. all the non-occluded locations on the wall in Fig. 11. However, the ground-truth location should always have a good compatibility score as compared with scores of other locations. Therefore, we compute the similarity score on the annotated location and normalize it with the same normalization factors as the heatmap.\n\nA good location prediction model should give a high normalized similarity (NS) on the annotated locations. For \"Random\" strategy, the normalized similarity is randomly selected in [0, 1] since the heatmap is random.\n\nFor scale prediction, given a background along with its annotated foreground object, all methods use the annotated ground-truth locations as the center of the box. With a fixed initial size with 1/25 area of the whole background image, we assign 9 different scales by scale = 1.2 k\u22124 , k = 0, 1, ..., 8. The box size is then multiplied by the scale to get 9 candidate boxes. \"Random\" strategy selects one scale randomly, while the other two methods use the similarities between the background with boxes and the foreground to rank all the scales. We then compute the IOU between the predicted box and the annotated box. Table 10: Evaluation on location and scale prediction. NS (normalized similarity) denotes the cosine similarity of the ground truth location normalized by the maximum and minimum similarity of the other sliding window locations.\n\n\nMethod\n\nLocation Scale N S > 0.99 N S > 0.95 N S > 0.9 IOU > 0.9 IOU > 0.75 IOU > 0.  As shown in Table 10, we report the percentage of correct predictions with different thresholds on two evaluation metrics, i.e. NS and IOU. Our method performs the best on both location and scale prediction, indicating the superiority of the proposed method on non-box scenarios.\n\n\nLimitations and Potential Social Impact\n\nCurrent framework only uses 2D transform for contrastive learning, which is efficient and scalable, but not as accurate as 3D transform. This limitation could be addressed in the future by adopting 3D transform as better pre-processing w/o changing the proposed learning framework. Another limitation is that the search space is currently bounded by the gallery, and hence there may not be any perfectly compatible object images in the database even with the large-scale open-world database setting we adopt in our work. One solution is to augment the search space by allowing transformation of objects in the database.\n\nRetrieval-based compositing could be indirectly used to generate fake images, but we can make sure all the gallery object images do not have ethical or privacy issues. Also, fake images usually have incompatible geometry or lighting conditions which can be detected using our model.\n\n\nConclusion\n\nWe propose a novel unconstrained foreground search method for compositing (GALA), as the first to learn geometry-and-lighting awareness from large-scale data for real-world scenarios. GALA achieves state-of-the-art results on CAIS and Pixabay dataset. It also tackles non-box scenarios by automatic locationscale prediction, which is not explored by previous works.\n\nFig. 3 :\n3Pipeline of generating lighting and geometry transformations used in contrastive learning.\n\nFig. 4 :\n4Contrastive learning with selftransformations.\n\nFig. 5 :\n5Imperfect segmentation mask and edge pixel example.\n\nFig. 6 :\n6An example of poor compatibility on semantics when training foreground network.\n\nFig. 7 :\n7The proposed alternating training strategy.\n\nFig. 8 :\n8Example of location/scale prediction.\n\nFig. 9 :\n9Qualitative comparison on Pixabay and Open Images. Each of the four examples contains three rows of retrieval results, corresponding to: \"Baseline\" (1st row), \"No Contrastive\" (2nd row), \"Overall\" model (3rd row).\n\nFig. 11 :\n11Example heatmaps for location prediction evaluation.\n\n\n. Our contributions are summarized as follows:-A novel compositing-aware search method that discriminatively models geometry and lighting with contrastive training.-An alternating training strategy to address the challenge of losing semantic compatibility when learning the foreground network in an unconstrained setting. -Extensive experiments to demonstrate significant improvements over previous works on the CAIS dataset, as well as a new large-scale open-world dataset, i.e. Pixabay. -An extension of the method for non-box conditioned scenarios which are never considered in previous works.\n\nTable 1 :\n1Comparison between our method and previous works on settings.Zhao [27] UFO [28] Li [14] Wu [24] Ours \nOne model for all \n\n\n\n\n\nUnspecified Class \n\n\n\n\n\nNon-box Scenarios \n\n\n\n\n\nLarge-scale Dataset \n\n\n\n\n\nOpen-world Setting \n\n\n\n\n\n\n\n\nTable 2 :\n2Performance of different training strategies on CAIS.mAP \nFix Foreground \n29.10 \nDirect Training \n17.93 \nAug \n23.45 \nAug + Alternating 31.20 \n\n\n\nTable 3 :\n3Comparison with previous works on CAIS in terms of mAP-100. \" \u2020\" denotes constrained methods with multiple models, which are not scalable in practice.Method \nBoat Bottle Car Chair Dog Painting Person Plant Overall \nShape [27] \n7.47 1.16 10.40 12.25 12.22 3.89 \n6.37 8.82 7.82 \nRealismCNN [30] \n12.33 7.19 7.55 1.81 7.58 \n6.45 \n1.47 12.74 7.14 \n \u2020CFO-C Search [27] 57.48 14.24 18.85 21.61 38.01 27.72 47.33 20.20 30.68 \n \u2020CFO-D Search [27] 55.48 8.93 24.10 18.16 57.82 21.59 27.66 23.13 29.61 \nUFO Search [28] \n59.73 21.12 36.63 19.27 36.51 25.84 27.11 31.19 32.17 \nOurs \n70.58 19.41 40.22 24.17 37.81 28.20 44.72 34.91 37.50 \n\n\n\nTable 4 :\n4Retrieval performance of selected classes in terms of Recall@10 (%) on Pixabay.Category \n\nMajority \nMedium \nMinority \n#samples\u2248 5000 \n#samples\u2248 50 \n#samples< 5 \n\nPerson Flower Birds Vehicle \nCell Mandarin Christmas Boiled \nLast 50 \nPhone Orange \nTree \nEgg \nClasses \nUFO [28] 3.84 \n8.00 6.74 8.41 5.71 \n36.36 \n2.94 \n6.06 \n8.00 \nOurs \n19.36 28.55 21.11 26.83 20.00 63.64 \n20.59 30.30 \n24.00 \n\n\n\nTable 5 :\n5Overall retrieval accuracy in terms of Recall@k (%) on Pixabay.Method R@1 R@5 R@10 R@1% \nUFO [28] 2.04 6.66 10.24 61.76 \nOurs \n7.75 20.13 28.20 85.61 \n\nPixabay: Since none of the previous works conduct experiments on large-scale \nopen-world setting, we implement the state-of-the-art unconstrained search method \n(UFO [28]) on Pixabay and provide detailed comparison. \n\nTable 6 :\n6Evaluation of sensitivity (discriminative ability) to lighting and geometry transformation. The \"sensitivity\" denotes the squared L2 distance between the normalized embedding of original and transformed foreground images.Ablation \nLighting \nGeometry \nSensitivity(\u2191) R@5 R@10 R@15 Sensitivity(\u2191) R@5 R@10 R@15 \nPixabay \nUFO [28] \n0.27 \n53.70 67.90 75.50 \n0.39 \n58.25 69.90 77.95 \nBaseline \n0.27 \n53.40 67.30 75.35 \n0.39 \n58.10 69.70 77.65 \nNo Contrastive \n0.51 \n55.60 70.70 79.70 \n0.72 \n61.30 74.30 82.75 \nOverall \n0.57 \n60.55 74.70 82.85 \n1.12 \n98.55 99.45 99.70 \nOpen Images \nBaseline \n0.24 \n51.70 65.45 74.60 \n0.40 \n60.10 71.80 78.45 \nNo Contrastive \n0.53 \n54.80 71.55 80.10 \n0.98 \n71.70 82.60 89.10 \nOverall \n0.56 \n59.35 73.90 81.80 \n1.58 \n99.50 99.75 99.90 \n\n\n\nTable 7 :\n7Ablation study on components.Ablations \nmAP mAP-100 \nDirect Training 17.93 24.02 \nAug \n23.45 31.78 \nFix+Aug \n28.65 30.33 \nNo Alternating 29.99 32.13 \nNo Contrastive 31.20 36.30 \nOverall \n32.67 37.49 \n\n\n\nTable 8 :\n8Ablation study on different alternating training strategies.Ablations \nmAP mAP-100 \nZero Round \n28.65 30.33 \nOne Round \n31.20 36.30 \nTwo Rounds 30.35 36.22 \nReverse Order 28.82 36.14 \n\n\n\nTable 9 :\n9Ablation study on different transformations.Ablations \nmAP mAP-100 \nNo Contrastive \n31.20 36.30 \nGeometry \n32.11 36.68 \nGeometry+Color \n30.80 35.06 \nGeometry+Lighting 32.67 37.49 \n\n\n\nCompositional gan: Learning image-conditional binary composition. S Azadi, D Pathak, S Ebrahimi, T Darrell, International Journal of Computer Vision. 128101Azadi, S., Pathak, D., Ebrahimi, S., Darrell, T.: Compositional gan: Learning image-conditional binary composition. International Journal of Computer Vision 128(10), 2570-2585 (2020) 1\n\nLarge scale online learning of image similarity through ranking. G Chechik, V Sharma, U Shalit, S Bengio, Journal of Machine Learning Research. 1135Chechik, G., Sharma, V., Shalit, U., Bengio, S.: Large scale online learning of image similarity through ranking. Journal of Machine Learning Research 11(3) (2010) 5\n\nImagenet: A largescale hierarchical image database. J Deng, W Dong, R Socher, L J Li, K Li, L Fei-Fei, 2009 IEEE conference on computer vision and pattern recognition. Ieee69Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large- scale hierarchical image database. In: 2009 IEEE conference on computer vision and pattern recognition. pp. 248-255. Ieee (2009) 6, 9\n\nThe pascal visual object classes (voc) challenge. M Everingham, L Van Gool, C K Williams, J Winn, A Zisserman, International journal of computer vision. 8828Everingham, M., Van Gool, L., Williams, C.K., Winn, J., Zisserman, A.: The pascal visual object classes (voc) challenge. International journal of computer vision 88(2), 303-338 (2010) 8\n\nP Goyal, P Doll\u00e1r, R Girshick, P Noordhuis, L Wesolowski, A Kyrola, A Tulloch, Y Jia, K He, arXiv:1706.02677Accurate, large minibatch sgd: Training imagenet in 1 hour. 9arXiv preprintGoyal, P., Doll\u00e1r, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., Tul- loch, A., Jia, Y., He, K.: Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677 (2017) 9\n\nSsh: A self-supervised framework for image harmonization. Y Jiang, H Zhang, J Zhang, Y Wang, Z Lin, K Sunkavalli, S Chen, S Amirghodsi, S Kong, Z Wang, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision1Jiang, Y., Zhang, H., Zhang, J., Wang, Y., Lin, Z., Sunkavalli, K., Chen, S., Amirghodsi, S., Kong, S., Wang, Z.: Ssh: A self-supervised framework for im- age harmonization. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 4832-4841 (2021) 1\n\nSsh: A self-supervised framework for image harmonization. Y Jiang, H Zhang, J Zhang, Y Wang, Z Lin, K Sunkavalli, S Chen, S Amirghodsi, S Kong, Z Wang, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)13Jiang, Y., Zhang, H., Zhang, J., Wang, Y., Lin, Z., Sunkavalli, K., Chen, S., Amirghodsi, S., Kong, S., Wang, Z.: Ssh: A self-supervised framework for im- age harmonization. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 4832-4841 (October 2021) 13\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, arXiv:1412.69809arXiv preprintKingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014) 9\n\nThe open images dataset v4. A Kuznetsova, H Rom, N Alldrin, J Uijlings, I Krasin, J Pont-Tuset, S Kamali, S Popov, M Malloci, A Kolesnikov, International Journal of Computer Vision. 128711Kuznetsova, A., Rom, H., Alldrin, N., Uijlings, J., Krasin, I., Pont-Tuset, J., Ka- mali, S., Popov, S., Malloci, M., Kolesnikov, A., et al.: The open images dataset v4. International Journal of Computer Vision 128(7), 1956-1981 (2020) 3, 8, 11\n\nPhoto clip art. J F Lalonde, D Hoiem, A A Efros, C Rother, J Winn, A Criminisi, ACM transactions on graphics (TOG). 2634Lalonde, J.F., Hoiem, D., Efros, A.A., Rother, C., Winn, J., Criminisi, A.: Photo clip art. ACM transactions on graphics (TOG) 26(3), 3-es (2007) 4\n\nContext-aware synthesis and placement of object instances. D Lee, S Liu, J Gu, M Y Liu, M H Yang, J Kautz, arXiv:1812.023504arXiv preprintLee, D., Liu, S., Gu, J., Liu, M.Y., Yang, M.H., Kautz, J.: Context-aware synthesis and placement of object instances. arXiv preprint arXiv:1812.02350 (2018) 4\n\nCentermask: Real-time anchor-free instance segmentation. Y Lee, J Park, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition8Lee, Y., Park, J.: Centermask: Real-time anchor-free instance segmentation. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recog- nition. pp. 13906-13915 (2020) 8\n\nInterpretable foreground object search as knowledge distillation. B Li, P Y Zhuang, J Gu, M Li, P Tan, European Conference on Computer Vision. Springer29Li, B., Zhuang, P.Y., Gu, J., Li, M., Tan, P.: Interpretable foreground object search as knowledge distillation. In: European Conference on Computer Vision. pp. 189- 204. Springer (2020) 2, 4, 9\n\nPutting humans in a scene: Learning affordance in 3d indoor environments. X Li, S Liu, K Kim, X Wang, M H Yang, J Kautz, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition4Li, X., Liu, S., Kim, K., Wang, X., Yang, M.H., Kautz, J.: Putting humans in a scene: Learning affordance in 3d indoor environments. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 12368- 12376 (2019) 4\n\nSt-gan: Spatial transformer generative adversarial networks for image compositing. C H Lin, E Yumer, O Wang, E Shechtman, S Lucey, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition1Lin, C.H., Yumer, E., Wang, O., Shechtman, E., Lucey, S.: St-gan: Spatial trans- former generative adversarial networks for image compositing. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 9455-9464 (2018) 1\n\nMicrosoft coco: Common objects in context. T Y Lin, M Maire, S Belongie, J Hays, P Perona, D Ramanan, P Doll\u00e1r, C L Zitnick, European conference on computer vision. Springer3Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll\u00e1r, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: European conference on computer vision. pp. 740-755. Springer (2014) 3, 8\n\nMaking images real again: A comprehensive survey on deep image composition. L Niu, W Cong, L Liu, Y Hong, B Zhang, J Liang, L Zhang, arXiv:2106.144901arXiv preprintNiu, L., Cong, W., Liu, L., Hong, Y., Zhang, B., Liang, J., Zhang, L.: Making im- ages real again: A comprehensive survey on deep image composition. arXiv preprint arXiv:2106.14490 (2021) 1\n\nTotal relighting: learning to relight portraits for background replacement. R Pandey, S O Escolano, C Legendre, C Haene, S Bouaziz, C Rhemann, P Debevec, S Fanello, ACM Transactions on Graphics (TOG). 4046Pandey, R., Escolano, S.O., Legendre, C., Haene, C., Bouaziz, S., Rhemann, C., Debevec, P., Fanello, S.: Total relighting: learning to relight portraits for back- ground replacement. ACM Transactions on Graphics (TOG) 40(4), 1-21 (2021) 6\n\nPytorch: An imperative style, highperformance deep learning library. A Paszke, S Gross, F Massa, A Lerer, J Bradbury, G Chanan, T Killeen, Z Lin, N Gimelshein, L Antiga, Advances in neural information processing systems. 329Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al.: Pytorch: An imperative style, high- performance deep learning library. Advances in neural information processing sys- tems 32, 8026-8037 (2019) 9\n\nK Simonyan, A Zisserman, arXiv:1409.1556Very deep convolutional networks for large-scale image recognition. 9arXiv preprintSimonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014) 9\n\nFcos: Fully convolutional one-stage object detection. Z Tian, C Shen, H Chen, T He, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision8Tian, Z., Shen, C., Chen, H., He, T.: Fcos: Fully convolutional one-stage object detection. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 9627-9636 (2019) 8\n\nY H Tsai, X Shen, Z Lin, K Sunkavalli, X Lu, M H Yang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition1Deep image harmonizationTsai, Y.H., Shen, X., Lin, Z., Sunkavalli, K., Lu, X., Yang, M.H.: Deep image harmonization. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 3789-3797 (2017) 1\n\nFine-grained foreground retrieval via teacher-student learning. Z Wu, D Lischinski, E Shechtman, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer Vision29Wu, Z., Lischinski, D., Shechtman, E.: Fine-grained foreground retrieval via teacher-student learning. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. pp. 3646-3654 (2021) 2, 4, 6, 9\n\nH Zhang, J Zhang, F Perazzi, Z Lin, V M Patel, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer Vision1Deep image compositingZhang, H., Zhang, J., Perazzi, F., Lin, Z., Patel, V.M.: Deep image compositing. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. pp. 365-374 (2021) 1\n\nLearning object placement by inpainting for compositional data augmentation. L Zhang, T Wen, J Min, J Wang, D Han, J Shi, Computer Vision-ECCV 2020: 16th European Conference. Glasgow, UKSpringer4Proceedings, Part XIII 16Zhang, L., Wen, T., Min, J., Wang, J., Han, D., Shi, J.: Learning object placement by inpainting for compositional data augmentation. In: Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XIII 16. pp. 566-581. Springer (2020) 4\n\nCompositing-aware image search. H Zhao, X Shen, Z Lin, K Sunkavalli, B Price, J Jia, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)10Zhao, H., Shen, X., Lin, Z., Sunkavalli, K., Price, B., Jia, J.: Compositing-aware image search. In: Proceedings of the European Conference on Computer Vision (ECCV). pp. 502-516 (2018) 1, 2, 4, 8, 9, 10\n\nUnconstrained foreground object search. Y Zhao, B Price, S Cohen, D Gurari, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision1012Zhao, Y., Price, B., Cohen, S., Gurari, D.: Unconstrained foreground object search. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 2030-2039 (2019) 2, 4, 6, 9, 10, 11, 12\n\nScene parsing through ade20k dataset. B Zhou, H Zhao, X Puig, S Fidler, A Barriuso, A Torralba, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition8Zhou, B., Zhao, H., Puig, X., Fidler, S., Barriuso, A., Torralba, A.: Scene parsing through ade20k dataset. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 633-641 (2017) 8\n\nLearning a discriminative model for the perception of realism in composite images. J Y Zhu, P Krahenbuhl, E Shechtman, A A Efros, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer Vision19Zhu, J.Y., Krahenbuhl, P., Shechtman, E., Efros, A.A.: Learning a discriminative model for the perception of realism in composite images. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 3943-3951 (2015) 1, 9\n", "annotations": {"author": "[{\"end\":177,\"start\":74},{\"end\":203,\"start\":178},{\"end\":233,\"start\":204},{\"end\":262,\"start\":234},{\"end\":310,\"start\":263},{\"end\":415,\"start\":311}]", "publisher": null, "author_last_name": "[{\"end\":83,\"start\":80},{\"end\":185,\"start\":182},{\"end\":215,\"start\":210},{\"end\":244,\"start\":240},{\"end\":275,\"start\":270},{\"end\":320,\"start\":316}]", "author_first_name": "[{\"end\":79,\"start\":74},{\"end\":181,\"start\":178},{\"end\":209,\"start\":204},{\"end\":239,\"start\":234},{\"end\":269,\"start\":263},{\"end\":315,\"start\":311}]", "author_affiliation": "[{\"end\":176,\"start\":107},{\"end\":202,\"start\":187},{\"end\":232,\"start\":217},{\"end\":261,\"start\":246},{\"end\":309,\"start\":294},{\"end\":414,\"start\":345}]", "title": "[{\"end\":71,\"start\":1},{\"end\":486,\"start\":416}]", "venue": null, "abstract": "[{\"end\":1574,\"start\":534}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b25\"},\"end\":1636,\"start\":1632},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":1693,\"start\":1689},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":1696,\"start\":1693},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":1949,\"start\":1946},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":1952,\"start\":1949},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":1955,\"start\":1952},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1957,\"start\":1955},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":1960,\"start\":1957},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2168,\"start\":2164},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2680,\"start\":2676},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2949,\"start\":2945},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3234,\"start\":3230},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3465,\"start\":3461},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3468,\"start\":3465},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3511,\"start\":3507},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3577,\"start\":3573},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3981,\"start\":3977},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":4355,\"start\":4351},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6592,\"start\":6588},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6733,\"start\":6729},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7062,\"start\":7058},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7187,\"start\":7183},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7375,\"start\":7371},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7725,\"start\":7721},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7875,\"start\":7871},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8230,\"start\":8226},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8233,\"start\":8230},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8236,\"start\":8233},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8368,\"start\":8364},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8498,\"start\":8494},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8593,\"start\":8589},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10551,\"start\":10548},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12600,\"start\":12596},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":12748,\"start\":12744},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12751,\"start\":12748},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":13014,\"start\":13011},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":16416,\"start\":16412},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":16485,\"start\":16481},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":16506,\"start\":16503},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":16519,\"start\":16515},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":17102,\"start\":17098},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":17124,\"start\":17120},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":18507,\"start\":18503},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":18676,\"start\":18672},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":18696,\"start\":18692},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":18724,\"start\":18721},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":19098,\"start\":19095},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":19171,\"start\":19168},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":19981,\"start\":19977},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":20007,\"start\":20003},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":20010,\"start\":20007},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":20316,\"start\":20312},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":20443,\"start\":20439},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":20708,\"start\":20704},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":20732,\"start\":20728},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":21312,\"start\":21308},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":21608,\"start\":21604},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":22369,\"start\":22365},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":22858,\"start\":22854},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":23980,\"start\":23976},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":27550,\"start\":27547}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":31453,\"start\":31352},{\"attributes\":{\"id\":\"fig_1\"},\"end\":31511,\"start\":31454},{\"attributes\":{\"id\":\"fig_2\"},\"end\":31574,\"start\":31512},{\"attributes\":{\"id\":\"fig_3\"},\"end\":31665,\"start\":31575},{\"attributes\":{\"id\":\"fig_4\"},\"end\":31720,\"start\":31666},{\"attributes\":{\"id\":\"fig_5\"},\"end\":31769,\"start\":31721},{\"attributes\":{\"id\":\"fig_6\"},\"end\":31994,\"start\":31770},{\"attributes\":{\"id\":\"fig_7\"},\"end\":32060,\"start\":31995},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":32659,\"start\":32061},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":32898,\"start\":32660},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":33054,\"start\":32899},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":33694,\"start\":33055},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":34098,\"start\":33695},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":34479,\"start\":34099},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":35255,\"start\":34480},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":35469,\"start\":35256},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":35667,\"start\":35470},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":35861,\"start\":35668}]", "paragraph": "[{\"end\":2622,\"start\":1590},{\"end\":3862,\"start\":2665},{\"end\":3893,\"start\":3872},{\"end\":4230,\"start\":3906},{\"end\":4634,\"start\":4270},{\"end\":4897,\"start\":4636},{\"end\":6911,\"start\":4899},{\"end\":8991,\"start\":6928},{\"end\":9391,\"start\":9002},{\"end\":10594,\"start\":9413},{\"end\":10889,\"start\":10697},{\"end\":11637,\"start\":10939},{\"end\":12689,\"start\":11713},{\"end\":14257,\"start\":12714},{\"end\":16328,\"start\":14273},{\"end\":18435,\"start\":16376},{\"end\":19240,\"start\":18462},{\"end\":19873,\"start\":19263},{\"end\":20202,\"start\":19910},{\"end\":21374,\"start\":20212},{\"end\":21912,\"start\":21376},{\"end\":22576,\"start\":21914},{\"end\":26204,\"start\":22595},{\"end\":26883,\"start\":26206},{\"end\":27345,\"start\":26917},{\"end\":27551,\"start\":27355},{\"end\":28589,\"start\":27553},{\"end\":28806,\"start\":28591},{\"end\":29656,\"start\":28808},{\"end\":30024,\"start\":29667},{\"end\":30687,\"start\":30068},{\"end\":30971,\"start\":30689},{\"end\":31351,\"start\":30986}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10696,\"start\":10595},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11712,\"start\":11638}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":6910,\"start\":6903},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":13475,\"start\":13468},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":20302,\"start\":20295},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":21386,\"start\":21379},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":22040,\"start\":22033},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":22440,\"start\":22433},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":23325,\"start\":23318},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":23849,\"start\":23842},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":25270,\"start\":25263},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":26026,\"start\":26019},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":26692,\"start\":26685},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":29436,\"start\":29428},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":29765,\"start\":29757}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1588,\"start\":1576},{\"end\":2663,\"start\":2625},{\"end\":3870,\"start\":3865},{\"end\":3904,\"start\":3896},{\"end\":4268,\"start\":4233},{\"attributes\":{\"n\":\"2\"},\"end\":6926,\"start\":6914},{\"attributes\":{\"n\":\"3\"},\"end\":9000,\"start\":8994},{\"attributes\":{\"n\":\"3.1\"},\"end\":9411,\"start\":9394},{\"attributes\":{\"n\":\"3.2\"},\"end\":10937,\"start\":10892},{\"attributes\":{\"n\":\"3.3\"},\"end\":12712,\"start\":12692},{\"end\":14271,\"start\":14260},{\"attributes\":{\"n\":\"3.4\"},\"end\":16361,\"start\":16331},{\"attributes\":{\"n\":\"4\"},\"end\":16374,\"start\":16364},{\"attributes\":{\"n\":\"4.1\"},\"end\":18460,\"start\":18438},{\"attributes\":{\"n\":\"4.2\"},\"end\":19261,\"start\":19243},{\"attributes\":{\"n\":\"4.3\"},\"end\":19908,\"start\":19876},{\"end\":20210,\"start\":20205},{\"attributes\":{\"n\":\"4.4\"},\"end\":22593,\"start\":22579},{\"attributes\":{\"n\":\"4.5\"},\"end\":26915,\"start\":26886},{\"end\":27353,\"start\":27348},{\"end\":29665,\"start\":29659},{\"attributes\":{\"n\":\"5\"},\"end\":30066,\"start\":30027},{\"attributes\":{\"n\":\"6\"},\"end\":30984,\"start\":30974},{\"end\":31361,\"start\":31353},{\"end\":31463,\"start\":31455},{\"end\":31521,\"start\":31513},{\"end\":31584,\"start\":31576},{\"end\":31675,\"start\":31667},{\"end\":31730,\"start\":31722},{\"end\":31779,\"start\":31771},{\"end\":32005,\"start\":31996},{\"end\":32670,\"start\":32661},{\"end\":32909,\"start\":32900},{\"end\":33065,\"start\":33056},{\"end\":33705,\"start\":33696},{\"end\":34109,\"start\":34100},{\"end\":34490,\"start\":34481},{\"end\":35266,\"start\":35257},{\"end\":35480,\"start\":35471},{\"end\":35678,\"start\":35669}]", "table": "[{\"end\":32898,\"start\":32733},{\"end\":33054,\"start\":32964},{\"end\":33694,\"start\":33217},{\"end\":34098,\"start\":33786},{\"end\":34479,\"start\":34174},{\"end\":35255,\"start\":34713},{\"end\":35469,\"start\":35297},{\"end\":35667,\"start\":35542},{\"end\":35861,\"start\":35724}]", "figure_caption": "[{\"end\":31453,\"start\":31363},{\"end\":31511,\"start\":31465},{\"end\":31574,\"start\":31523},{\"end\":31665,\"start\":31586},{\"end\":31720,\"start\":31677},{\"end\":31769,\"start\":31732},{\"end\":31994,\"start\":31781},{\"end\":32060,\"start\":32008},{\"end\":32659,\"start\":32063},{\"end\":32733,\"start\":32672},{\"end\":32964,\"start\":32911},{\"end\":33217,\"start\":33067},{\"end\":33786,\"start\":33707},{\"end\":34174,\"start\":34111},{\"end\":34713,\"start\":34492},{\"end\":35297,\"start\":35268},{\"end\":35542,\"start\":35482},{\"end\":35724,\"start\":35680}]", "figure_ref": "[{\"end\":3927,\"start\":3921},{\"end\":4301,\"start\":4295},{\"end\":11438,\"start\":11435},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11636,\"start\":11629},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11782,\"start\":11776},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":13621,\"start\":13613},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":14621,\"start\":14615},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":14783,\"start\":14777},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":15762,\"start\":15755},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":16327,\"start\":16321},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":24454,\"start\":24448},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":26475,\"start\":26469},{\"end\":27344,\"start\":27337},{\"end\":27379,\"start\":27372},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":27892,\"start\":27885},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":28325,\"start\":28318}]", "bib_author_first_name": "[{\"end\":35930,\"start\":35929},{\"end\":35939,\"start\":35938},{\"end\":35949,\"start\":35948},{\"end\":35961,\"start\":35960},{\"end\":36271,\"start\":36270},{\"end\":36282,\"start\":36281},{\"end\":36292,\"start\":36291},{\"end\":36302,\"start\":36301},{\"end\":36573,\"start\":36572},{\"end\":36581,\"start\":36580},{\"end\":36589,\"start\":36588},{\"end\":36599,\"start\":36598},{\"end\":36601,\"start\":36600},{\"end\":36607,\"start\":36606},{\"end\":36613,\"start\":36612},{\"end\":36962,\"start\":36961},{\"end\":36976,\"start\":36975},{\"end\":36988,\"start\":36987},{\"end\":36990,\"start\":36989},{\"end\":37002,\"start\":37001},{\"end\":37010,\"start\":37009},{\"end\":37256,\"start\":37255},{\"end\":37265,\"start\":37264},{\"end\":37275,\"start\":37274},{\"end\":37287,\"start\":37286},{\"end\":37300,\"start\":37299},{\"end\":37314,\"start\":37313},{\"end\":37324,\"start\":37323},{\"end\":37335,\"start\":37334},{\"end\":37342,\"start\":37341},{\"end\":37711,\"start\":37710},{\"end\":37720,\"start\":37719},{\"end\":37729,\"start\":37728},{\"end\":37738,\"start\":37737},{\"end\":37746,\"start\":37745},{\"end\":37753,\"start\":37752},{\"end\":37767,\"start\":37766},{\"end\":37775,\"start\":37774},{\"end\":37789,\"start\":37788},{\"end\":37797,\"start\":37796},{\"end\":38268,\"start\":38267},{\"end\":38277,\"start\":38276},{\"end\":38286,\"start\":38285},{\"end\":38295,\"start\":38294},{\"end\":38303,\"start\":38302},{\"end\":38310,\"start\":38309},{\"end\":38324,\"start\":38323},{\"end\":38332,\"start\":38331},{\"end\":38346,\"start\":38345},{\"end\":38354,\"start\":38353},{\"end\":38842,\"start\":38841},{\"end\":38844,\"start\":38843},{\"end\":38854,\"start\":38853},{\"end\":39025,\"start\":39024},{\"end\":39039,\"start\":39038},{\"end\":39046,\"start\":39045},{\"end\":39057,\"start\":39056},{\"end\":39069,\"start\":39068},{\"end\":39079,\"start\":39078},{\"end\":39093,\"start\":39092},{\"end\":39103,\"start\":39102},{\"end\":39112,\"start\":39111},{\"end\":39123,\"start\":39122},{\"end\":39447,\"start\":39446},{\"end\":39449,\"start\":39448},{\"end\":39460,\"start\":39459},{\"end\":39469,\"start\":39468},{\"end\":39471,\"start\":39470},{\"end\":39480,\"start\":39479},{\"end\":39490,\"start\":39489},{\"end\":39498,\"start\":39497},{\"end\":39759,\"start\":39758},{\"end\":39766,\"start\":39765},{\"end\":39773,\"start\":39772},{\"end\":39779,\"start\":39778},{\"end\":39781,\"start\":39780},{\"end\":39788,\"start\":39787},{\"end\":39790,\"start\":39789},{\"end\":39798,\"start\":39797},{\"end\":40056,\"start\":40055},{\"end\":40063,\"start\":40062},{\"end\":40478,\"start\":40477},{\"end\":40484,\"start\":40483},{\"end\":40486,\"start\":40485},{\"end\":40496,\"start\":40495},{\"end\":40502,\"start\":40501},{\"end\":40508,\"start\":40507},{\"end\":40835,\"start\":40834},{\"end\":40841,\"start\":40840},{\"end\":40848,\"start\":40847},{\"end\":40855,\"start\":40854},{\"end\":40863,\"start\":40862},{\"end\":40865,\"start\":40864},{\"end\":40873,\"start\":40872},{\"end\":41362,\"start\":41361},{\"end\":41364,\"start\":41363},{\"end\":41371,\"start\":41370},{\"end\":41380,\"start\":41379},{\"end\":41388,\"start\":41387},{\"end\":41401,\"start\":41400},{\"end\":41845,\"start\":41844},{\"end\":41847,\"start\":41846},{\"end\":41854,\"start\":41853},{\"end\":41863,\"start\":41862},{\"end\":41875,\"start\":41874},{\"end\":41883,\"start\":41882},{\"end\":41893,\"start\":41892},{\"end\":41904,\"start\":41903},{\"end\":41914,\"start\":41913},{\"end\":41916,\"start\":41915},{\"end\":42272,\"start\":42271},{\"end\":42279,\"start\":42278},{\"end\":42287,\"start\":42286},{\"end\":42294,\"start\":42293},{\"end\":42302,\"start\":42301},{\"end\":42311,\"start\":42310},{\"end\":42320,\"start\":42319},{\"end\":42627,\"start\":42626},{\"end\":42637,\"start\":42636},{\"end\":42639,\"start\":42638},{\"end\":42651,\"start\":42650},{\"end\":42663,\"start\":42662},{\"end\":42672,\"start\":42671},{\"end\":42683,\"start\":42682},{\"end\":42694,\"start\":42693},{\"end\":42705,\"start\":42704},{\"end\":43065,\"start\":43064},{\"end\":43075,\"start\":43074},{\"end\":43084,\"start\":43083},{\"end\":43093,\"start\":43092},{\"end\":43102,\"start\":43101},{\"end\":43114,\"start\":43113},{\"end\":43124,\"start\":43123},{\"end\":43135,\"start\":43134},{\"end\":43142,\"start\":43141},{\"end\":43156,\"start\":43155},{\"end\":43496,\"start\":43495},{\"end\":43508,\"start\":43507},{\"end\":43811,\"start\":43810},{\"end\":43819,\"start\":43818},{\"end\":43827,\"start\":43826},{\"end\":43835,\"start\":43834},{\"end\":44164,\"start\":44163},{\"end\":44166,\"start\":44165},{\"end\":44174,\"start\":44173},{\"end\":44182,\"start\":44181},{\"end\":44189,\"start\":44188},{\"end\":44203,\"start\":44202},{\"end\":44209,\"start\":44208},{\"end\":44211,\"start\":44210},{\"end\":44649,\"start\":44648},{\"end\":44655,\"start\":44654},{\"end\":44669,\"start\":44668},{\"end\":45053,\"start\":45052},{\"end\":45062,\"start\":45061},{\"end\":45071,\"start\":45070},{\"end\":45082,\"start\":45081},{\"end\":45089,\"start\":45088},{\"end\":45091,\"start\":45090},{\"end\":45536,\"start\":45535},{\"end\":45545,\"start\":45544},{\"end\":45552,\"start\":45551},{\"end\":45559,\"start\":45558},{\"end\":45567,\"start\":45566},{\"end\":45574,\"start\":45573},{\"end\":45994,\"start\":45993},{\"end\":46002,\"start\":46001},{\"end\":46010,\"start\":46009},{\"end\":46017,\"start\":46016},{\"end\":46031,\"start\":46030},{\"end\":46040,\"start\":46039},{\"end\":46409,\"start\":46408},{\"end\":46417,\"start\":46416},{\"end\":46426,\"start\":46425},{\"end\":46435,\"start\":46434},{\"end\":46822,\"start\":46821},{\"end\":46830,\"start\":46829},{\"end\":46838,\"start\":46837},{\"end\":46846,\"start\":46845},{\"end\":46856,\"start\":46855},{\"end\":46868,\"start\":46867},{\"end\":47318,\"start\":47317},{\"end\":47320,\"start\":47319},{\"end\":47327,\"start\":47326},{\"end\":47341,\"start\":47340},{\"end\":47354,\"start\":47353},{\"end\":47356,\"start\":47355}]", "bib_author_last_name": "[{\"end\":35936,\"start\":35931},{\"end\":35946,\"start\":35940},{\"end\":35958,\"start\":35950},{\"end\":35969,\"start\":35962},{\"end\":36279,\"start\":36272},{\"end\":36289,\"start\":36283},{\"end\":36299,\"start\":36293},{\"end\":36309,\"start\":36303},{\"end\":36578,\"start\":36574},{\"end\":36586,\"start\":36582},{\"end\":36596,\"start\":36590},{\"end\":36604,\"start\":36602},{\"end\":36610,\"start\":36608},{\"end\":36621,\"start\":36614},{\"end\":36973,\"start\":36963},{\"end\":36985,\"start\":36977},{\"end\":36999,\"start\":36991},{\"end\":37007,\"start\":37003},{\"end\":37020,\"start\":37011},{\"end\":37262,\"start\":37257},{\"end\":37272,\"start\":37266},{\"end\":37284,\"start\":37276},{\"end\":37297,\"start\":37288},{\"end\":37311,\"start\":37301},{\"end\":37321,\"start\":37315},{\"end\":37332,\"start\":37325},{\"end\":37339,\"start\":37336},{\"end\":37345,\"start\":37343},{\"end\":37717,\"start\":37712},{\"end\":37726,\"start\":37721},{\"end\":37735,\"start\":37730},{\"end\":37743,\"start\":37739},{\"end\":37750,\"start\":37747},{\"end\":37764,\"start\":37754},{\"end\":37772,\"start\":37768},{\"end\":37786,\"start\":37776},{\"end\":37794,\"start\":37790},{\"end\":37802,\"start\":37798},{\"end\":38274,\"start\":38269},{\"end\":38283,\"start\":38278},{\"end\":38292,\"start\":38287},{\"end\":38300,\"start\":38296},{\"end\":38307,\"start\":38304},{\"end\":38321,\"start\":38311},{\"end\":38329,\"start\":38325},{\"end\":38343,\"start\":38333},{\"end\":38351,\"start\":38347},{\"end\":38359,\"start\":38355},{\"end\":38851,\"start\":38845},{\"end\":38857,\"start\":38855},{\"end\":39036,\"start\":39026},{\"end\":39043,\"start\":39040},{\"end\":39054,\"start\":39047},{\"end\":39066,\"start\":39058},{\"end\":39076,\"start\":39070},{\"end\":39090,\"start\":39080},{\"end\":39100,\"start\":39094},{\"end\":39109,\"start\":39104},{\"end\":39120,\"start\":39113},{\"end\":39134,\"start\":39124},{\"end\":39457,\"start\":39450},{\"end\":39466,\"start\":39461},{\"end\":39477,\"start\":39472},{\"end\":39487,\"start\":39481},{\"end\":39495,\"start\":39491},{\"end\":39508,\"start\":39499},{\"end\":39763,\"start\":39760},{\"end\":39770,\"start\":39767},{\"end\":39776,\"start\":39774},{\"end\":39785,\"start\":39782},{\"end\":39795,\"start\":39791},{\"end\":39804,\"start\":39799},{\"end\":40060,\"start\":40057},{\"end\":40068,\"start\":40064},{\"end\":40481,\"start\":40479},{\"end\":40493,\"start\":40487},{\"end\":40499,\"start\":40497},{\"end\":40505,\"start\":40503},{\"end\":40512,\"start\":40509},{\"end\":40838,\"start\":40836},{\"end\":40845,\"start\":40842},{\"end\":40852,\"start\":40849},{\"end\":40860,\"start\":40856},{\"end\":40870,\"start\":40866},{\"end\":40879,\"start\":40874},{\"end\":41368,\"start\":41365},{\"end\":41377,\"start\":41372},{\"end\":41385,\"start\":41381},{\"end\":41398,\"start\":41389},{\"end\":41407,\"start\":41402},{\"end\":41851,\"start\":41848},{\"end\":41860,\"start\":41855},{\"end\":41872,\"start\":41864},{\"end\":41880,\"start\":41876},{\"end\":41890,\"start\":41884},{\"end\":41901,\"start\":41894},{\"end\":41911,\"start\":41905},{\"end\":41924,\"start\":41917},{\"end\":42276,\"start\":42273},{\"end\":42284,\"start\":42280},{\"end\":42291,\"start\":42288},{\"end\":42299,\"start\":42295},{\"end\":42308,\"start\":42303},{\"end\":42317,\"start\":42312},{\"end\":42326,\"start\":42321},{\"end\":42634,\"start\":42628},{\"end\":42648,\"start\":42640},{\"end\":42660,\"start\":42652},{\"end\":42669,\"start\":42664},{\"end\":42680,\"start\":42673},{\"end\":42691,\"start\":42684},{\"end\":42702,\"start\":42695},{\"end\":42713,\"start\":42706},{\"end\":43072,\"start\":43066},{\"end\":43081,\"start\":43076},{\"end\":43090,\"start\":43085},{\"end\":43099,\"start\":43094},{\"end\":43111,\"start\":43103},{\"end\":43121,\"start\":43115},{\"end\":43132,\"start\":43125},{\"end\":43139,\"start\":43136},{\"end\":43153,\"start\":43143},{\"end\":43163,\"start\":43157},{\"end\":43505,\"start\":43497},{\"end\":43518,\"start\":43509},{\"end\":43816,\"start\":43812},{\"end\":43824,\"start\":43820},{\"end\":43832,\"start\":43828},{\"end\":43838,\"start\":43836},{\"end\":44171,\"start\":44167},{\"end\":44179,\"start\":44175},{\"end\":44186,\"start\":44183},{\"end\":44200,\"start\":44190},{\"end\":44206,\"start\":44204},{\"end\":44216,\"start\":44212},{\"end\":44652,\"start\":44650},{\"end\":44666,\"start\":44656},{\"end\":44679,\"start\":44670},{\"end\":45059,\"start\":45054},{\"end\":45068,\"start\":45063},{\"end\":45079,\"start\":45072},{\"end\":45086,\"start\":45083},{\"end\":45097,\"start\":45092},{\"end\":45542,\"start\":45537},{\"end\":45549,\"start\":45546},{\"end\":45556,\"start\":45553},{\"end\":45564,\"start\":45560},{\"end\":45571,\"start\":45568},{\"end\":45578,\"start\":45575},{\"end\":45999,\"start\":45995},{\"end\":46007,\"start\":46003},{\"end\":46014,\"start\":46011},{\"end\":46028,\"start\":46018},{\"end\":46037,\"start\":46032},{\"end\":46044,\"start\":46041},{\"end\":46414,\"start\":46410},{\"end\":46423,\"start\":46418},{\"end\":46432,\"start\":46427},{\"end\":46442,\"start\":46436},{\"end\":46827,\"start\":46823},{\"end\":46835,\"start\":46831},{\"end\":46843,\"start\":46839},{\"end\":46853,\"start\":46847},{\"end\":46865,\"start\":46857},{\"end\":46877,\"start\":46869},{\"end\":47324,\"start\":47321},{\"end\":47338,\"start\":47328},{\"end\":47351,\"start\":47342},{\"end\":47362,\"start\":47357}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":145982865},\"end\":36203,\"start\":35863},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":2087262},\"end\":36518,\"start\":36205},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":206597351},\"end\":36909,\"start\":36520},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":4246903},\"end\":37253,\"start\":36911},{\"attributes\":{\"doi\":\"arXiv:1706.02677\",\"id\":\"b4\"},\"end\":37650,\"start\":37255},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":237091128},\"end\":38207,\"start\":37652},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":237091128},\"end\":38795,\"start\":38209},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b7\"},\"end\":38994,\"start\":38797},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":53296866},\"end\":39428,\"start\":38996},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":12312803},\"end\":39697,\"start\":39430},{\"attributes\":{\"doi\":\"arXiv:1812.02350\",\"id\":\"b10\"},\"end\":39996,\"start\":39699},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":208075876},\"end\":40409,\"start\":39998},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":220646788},\"end\":40758,\"start\":40411},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":76660586},\"end\":41276,\"start\":40760},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":3692201},\"end\":41799,\"start\":41278},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":14113767},\"end\":42193,\"start\":41801},{\"attributes\":{\"doi\":\"arXiv:2106.14490\",\"id\":\"b16\"},\"end\":42548,\"start\":42195},{\"attributes\":{\"id\":\"b17\"},\"end\":42993,\"start\":42550},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":202786778},\"end\":43493,\"start\":42995},{\"attributes\":{\"doi\":\"arXiv:1409.1556\",\"id\":\"b19\"},\"end\":43754,\"start\":43495},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":91184137},\"end\":44161,\"start\":43756},{\"attributes\":{\"id\":\"b21\"},\"end\":44582,\"start\":44163},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":230120522},\"end\":45050,\"start\":44584},{\"attributes\":{\"id\":\"b23\"},\"end\":45456,\"start\":45052},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":227232033},\"end\":45959,\"start\":45458},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":52958518},\"end\":46366,\"start\":45961},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":199543270},\"end\":46781,\"start\":46368},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":5636055},\"end\":47232,\"start\":46783},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":2842850},\"end\":47723,\"start\":47234}]", "bib_title": "[{\"end\":35927,\"start\":35863},{\"end\":36268,\"start\":36205},{\"end\":36570,\"start\":36520},{\"end\":36959,\"start\":36911},{\"end\":37708,\"start\":37652},{\"end\":38265,\"start\":38209},{\"end\":39022,\"start\":38996},{\"end\":39444,\"start\":39430},{\"end\":40053,\"start\":39998},{\"end\":40475,\"start\":40411},{\"end\":40832,\"start\":40760},{\"end\":41359,\"start\":41278},{\"end\":41842,\"start\":41801},{\"end\":42624,\"start\":42550},{\"end\":43062,\"start\":42995},{\"end\":43808,\"start\":43756},{\"end\":44646,\"start\":44584},{\"end\":45533,\"start\":45458},{\"end\":45991,\"start\":45961},{\"end\":46406,\"start\":46368},{\"end\":46819,\"start\":46783},{\"end\":47315,\"start\":47234}]", "bib_author": "[{\"end\":35938,\"start\":35929},{\"end\":35948,\"start\":35938},{\"end\":35960,\"start\":35948},{\"end\":35971,\"start\":35960},{\"end\":36281,\"start\":36270},{\"end\":36291,\"start\":36281},{\"end\":36301,\"start\":36291},{\"end\":36311,\"start\":36301},{\"end\":36580,\"start\":36572},{\"end\":36588,\"start\":36580},{\"end\":36598,\"start\":36588},{\"end\":36606,\"start\":36598},{\"end\":36612,\"start\":36606},{\"end\":36623,\"start\":36612},{\"end\":36975,\"start\":36961},{\"end\":36987,\"start\":36975},{\"end\":37001,\"start\":36987},{\"end\":37009,\"start\":37001},{\"end\":37022,\"start\":37009},{\"end\":37264,\"start\":37255},{\"end\":37274,\"start\":37264},{\"end\":37286,\"start\":37274},{\"end\":37299,\"start\":37286},{\"end\":37313,\"start\":37299},{\"end\":37323,\"start\":37313},{\"end\":37334,\"start\":37323},{\"end\":37341,\"start\":37334},{\"end\":37347,\"start\":37341},{\"end\":37719,\"start\":37710},{\"end\":37728,\"start\":37719},{\"end\":37737,\"start\":37728},{\"end\":37745,\"start\":37737},{\"end\":37752,\"start\":37745},{\"end\":37766,\"start\":37752},{\"end\":37774,\"start\":37766},{\"end\":37788,\"start\":37774},{\"end\":37796,\"start\":37788},{\"end\":37804,\"start\":37796},{\"end\":38276,\"start\":38267},{\"end\":38285,\"start\":38276},{\"end\":38294,\"start\":38285},{\"end\":38302,\"start\":38294},{\"end\":38309,\"start\":38302},{\"end\":38323,\"start\":38309},{\"end\":38331,\"start\":38323},{\"end\":38345,\"start\":38331},{\"end\":38353,\"start\":38345},{\"end\":38361,\"start\":38353},{\"end\":38853,\"start\":38841},{\"end\":38859,\"start\":38853},{\"end\":39038,\"start\":39024},{\"end\":39045,\"start\":39038},{\"end\":39056,\"start\":39045},{\"end\":39068,\"start\":39056},{\"end\":39078,\"start\":39068},{\"end\":39092,\"start\":39078},{\"end\":39102,\"start\":39092},{\"end\":39111,\"start\":39102},{\"end\":39122,\"start\":39111},{\"end\":39136,\"start\":39122},{\"end\":39459,\"start\":39446},{\"end\":39468,\"start\":39459},{\"end\":39479,\"start\":39468},{\"end\":39489,\"start\":39479},{\"end\":39497,\"start\":39489},{\"end\":39510,\"start\":39497},{\"end\":39765,\"start\":39758},{\"end\":39772,\"start\":39765},{\"end\":39778,\"start\":39772},{\"end\":39787,\"start\":39778},{\"end\":39797,\"start\":39787},{\"end\":39806,\"start\":39797},{\"end\":40062,\"start\":40055},{\"end\":40070,\"start\":40062},{\"end\":40483,\"start\":40477},{\"end\":40495,\"start\":40483},{\"end\":40501,\"start\":40495},{\"end\":40507,\"start\":40501},{\"end\":40514,\"start\":40507},{\"end\":40840,\"start\":40834},{\"end\":40847,\"start\":40840},{\"end\":40854,\"start\":40847},{\"end\":40862,\"start\":40854},{\"end\":40872,\"start\":40862},{\"end\":40881,\"start\":40872},{\"end\":41370,\"start\":41361},{\"end\":41379,\"start\":41370},{\"end\":41387,\"start\":41379},{\"end\":41400,\"start\":41387},{\"end\":41409,\"start\":41400},{\"end\":41853,\"start\":41844},{\"end\":41862,\"start\":41853},{\"end\":41874,\"start\":41862},{\"end\":41882,\"start\":41874},{\"end\":41892,\"start\":41882},{\"end\":41903,\"start\":41892},{\"end\":41913,\"start\":41903},{\"end\":41926,\"start\":41913},{\"end\":42278,\"start\":42271},{\"end\":42286,\"start\":42278},{\"end\":42293,\"start\":42286},{\"end\":42301,\"start\":42293},{\"end\":42310,\"start\":42301},{\"end\":42319,\"start\":42310},{\"end\":42328,\"start\":42319},{\"end\":42636,\"start\":42626},{\"end\":42650,\"start\":42636},{\"end\":42662,\"start\":42650},{\"end\":42671,\"start\":42662},{\"end\":42682,\"start\":42671},{\"end\":42693,\"start\":42682},{\"end\":42704,\"start\":42693},{\"end\":42715,\"start\":42704},{\"end\":43074,\"start\":43064},{\"end\":43083,\"start\":43074},{\"end\":43092,\"start\":43083},{\"end\":43101,\"start\":43092},{\"end\":43113,\"start\":43101},{\"end\":43123,\"start\":43113},{\"end\":43134,\"start\":43123},{\"end\":43141,\"start\":43134},{\"end\":43155,\"start\":43141},{\"end\":43165,\"start\":43155},{\"end\":43507,\"start\":43495},{\"end\":43520,\"start\":43507},{\"end\":43818,\"start\":43810},{\"end\":43826,\"start\":43818},{\"end\":43834,\"start\":43826},{\"end\":43840,\"start\":43834},{\"end\":44173,\"start\":44163},{\"end\":44181,\"start\":44173},{\"end\":44188,\"start\":44181},{\"end\":44202,\"start\":44188},{\"end\":44208,\"start\":44202},{\"end\":44218,\"start\":44208},{\"end\":44654,\"start\":44648},{\"end\":44668,\"start\":44654},{\"end\":44681,\"start\":44668},{\"end\":45061,\"start\":45052},{\"end\":45070,\"start\":45061},{\"end\":45081,\"start\":45070},{\"end\":45088,\"start\":45081},{\"end\":45099,\"start\":45088},{\"end\":45544,\"start\":45535},{\"end\":45551,\"start\":45544},{\"end\":45558,\"start\":45551},{\"end\":45566,\"start\":45558},{\"end\":45573,\"start\":45566},{\"end\":45580,\"start\":45573},{\"end\":46001,\"start\":45993},{\"end\":46009,\"start\":46001},{\"end\":46016,\"start\":46009},{\"end\":46030,\"start\":46016},{\"end\":46039,\"start\":46030},{\"end\":46046,\"start\":46039},{\"end\":46416,\"start\":46408},{\"end\":46425,\"start\":46416},{\"end\":46434,\"start\":46425},{\"end\":46444,\"start\":46434},{\"end\":46829,\"start\":46821},{\"end\":46837,\"start\":46829},{\"end\":46845,\"start\":46837},{\"end\":46855,\"start\":46845},{\"end\":46867,\"start\":46855},{\"end\":46879,\"start\":46867},{\"end\":47326,\"start\":47317},{\"end\":47340,\"start\":47326},{\"end\":47353,\"start\":47340},{\"end\":47364,\"start\":47353}]", "bib_venue": "[{\"end\":36011,\"start\":35971},{\"end\":36347,\"start\":36311},{\"end\":36686,\"start\":36623},{\"end\":37062,\"start\":37022},{\"end\":37421,\"start\":37363},{\"end\":37875,\"start\":37804},{\"end\":38439,\"start\":38361},{\"end\":38839,\"start\":38797},{\"end\":39176,\"start\":39136},{\"end\":39544,\"start\":39510},{\"end\":39756,\"start\":39699},{\"end\":40151,\"start\":40070},{\"end\":40552,\"start\":40514},{\"end\":40962,\"start\":40881},{\"end\":41486,\"start\":41409},{\"end\":41964,\"start\":41926},{\"end\":42269,\"start\":42195},{\"end\":42749,\"start\":42715},{\"end\":43214,\"start\":43165},{\"end\":43601,\"start\":43535},{\"end\":43911,\"start\":43840},{\"end\":44295,\"start\":44218},{\"end\":44761,\"start\":44681},{\"end\":45179,\"start\":45099},{\"end\":45631,\"start\":45580},{\"end\":46110,\"start\":46046},{\"end\":46515,\"start\":46444},{\"end\":46956,\"start\":46879},{\"end\":47431,\"start\":47364},{\"end\":37933,\"start\":37877},{\"end\":38504,\"start\":38441},{\"end\":40219,\"start\":40153},{\"end\":41030,\"start\":40964},{\"end\":41550,\"start\":41488},{\"end\":43969,\"start\":43913},{\"end\":44359,\"start\":44297},{\"end\":44828,\"start\":44763},{\"end\":45246,\"start\":45181},{\"end\":45644,\"start\":45633},{\"end\":46161,\"start\":46112},{\"end\":46573,\"start\":46517},{\"end\":47020,\"start\":46958},{\"end\":47485,\"start\":47433}]"}}}, "year": 2023, "month": 12, "day": 17}