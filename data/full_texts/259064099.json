{"id": 259064099, "updated": "2023-11-02 07:39:06.726", "metadata": {"title": "Fine-Grained Human Feedback Gives Better Rewards for Language Model Training", "authors": "[{\"first\":\"Zeqiu\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Yushi\",\"last\":\"Hu\",\"middle\":[]},{\"first\":\"Weijia\",\"last\":\"Shi\",\"middle\":[]},{\"first\":\"Nouha\",\"last\":\"Dziri\",\"middle\":[]},{\"first\":\"Alane\",\"last\":\"Suhr\",\"middle\":[]},{\"first\":\"Prithviraj\",\"last\":\"Ammanabrolu\",\"middle\":[]},{\"first\":\"Noah\",\"last\":\"Smith\",\"middle\":[\"A.\"]},{\"first\":\"Mari\",\"last\":\"Ostendorf\",\"middle\":[]},{\"first\":\"Hannaneh\",\"last\":\"Hajishirzi\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Language models (LMs) often exhibit undesirable text generation behaviors, including generating false, toxic, or irrelevant outputs. Reinforcement learning from human feedback (RLHF) - where human preference judgments on LM outputs are transformed into a learning signal - has recently shown promise in addressing these issues. However, such holistic feedback conveys limited information on long text outputs; it does not indicate which aspects of the outputs influenced user preference; e.g., which parts contain what type(s) of errors. In this paper, we use fine-grained human feedback (e.g., which sentence is false, which sub-sentence is irrelevant) as an explicit training signal. We introduce Fine-Grained RLHF, a framework that enables training and learning from reward functions that are fine-grained in two respects: (1) density, providing a reward after every segment (e.g., a sentence) is generated; and (2) incorporating multiple reward models associated with different feedback types (e.g., factual incorrectness, irrelevance, and information incompleteness). We conduct experiments on detoxification and long-form question answering to illustrate how learning with such reward functions leads to improved performance, supported by both automatic and human evaluation. Additionally, we show that LM behaviors can be customized using different combinations of fine-grained reward models. We release all data, collected human feedback, and codes at https://FineGrainedRLHF.github.io.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2306.01693", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2306-01693", "doi": "10.48550/arxiv.2306.01693"}}, "content": {"source": {"pdf_hash": "e2e52461194bc81351da7caa978ac42e9e9549cc", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2306.01693v2.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2306.01693", "status": "CLOSED"}}, "grobid": {"id": "9b7ab89c378c67d35e5452f0b39625b256f524e8", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/e2e52461194bc81351da7caa978ac42e9e9549cc.txt", "contents": "\nFine-Grained Human Feedback Gives Better Rewards for Language Model Training\n\n\nZeqiu Wu \nUniversity of Washington\n\n\nYushi Hu \nUniversity of Washington\n\n\nWeijia Shi \nNouha Dziri \nUniversity of Washington\n\n\nAllen Institute for Artificial Intelligence\n\n\nAlane Suhr \nUniversity of California\nBerkeley\n\nPrithviraj Ammanabrolu \nNoah A Smith \nMari Ostendorf \nUniversity of Washington\n\n\nHannaneh Hajishirzi \n\nUniversity of California\nSan Diego 5 MosaicML\n\nFine-Grained Human Feedback Gives Better Rewards for Language Model Training\n7F63C77BC5492BE2DD03225C28CB2178\nLanguage models (LMs) often exhibit undesirable text generation behaviors, including generating false, toxic, or irrelevant outputs.Reinforcement learning from human feedback (RLHF)-where human preference judgments on LM outputs are transformed into a learning signal-has recently shown promise in addressing these issues.However, such holistic feedback conveys limited information on long text outputs; it does not indicate which aspects of the outputs influenced user preference; e.g., which parts contain what type(s) of errors.In this paper, we use fine-grained human feedback (e.g., which sentence is false, which sub-sentence is irrelevant) as an explicit training signal.We introduce FINE-GRAINED RLHF, a framework that enables training and learning from reward functions that are fine-grained in two respects: (1) density, providing a reward after every segment (e.g., a sentence) is generated; and (2) incorporating multiple reward models associated with different feedback types (e.g., factual incorrectness, irrelevance, and information incompleteness).We conduct experiments on detoxification and long-form question answering to illustrate how learning with such reward functions leads to improved performance, supported by both automatic and human evaluation.Additionally, we show that LM behaviors can be customized using different combinations of fine-grained reward models.We release all data, collected human feedback, and codes at https://FineGrainedRLHF.github.io.\n\nIntroduction\n\nState-of-the-art AI is built on pre-trained language models that are then trained through interaction with humans [29,28,9], with a combination of supervised learning and reinforcement learning.Incorporating human feedback into the process of language model (LM) training has been shown as effective to reduce false, toxic and other undesired model generation outputs [29,3,2,33,10].Many of these studies adopt reinforcement learning from human feedback (RLHF) [46], a framework that converts human feedback into an effective LM training signal to reach these goals.Specifically, humans are presented with two or more outputs and asked to select one or rank them, and this signal is then used to train a reward model, which computes a single scalar reward for each LM-generated the layer of gases, generally known as air\u2026 > = >\n\nFigure 1: Comparison of (a) RL with human preference and (b) our FINE-GRAINED RLHF on long-form QA.Different from (a), which collects human preferences on the overall quality of LM outputs, we ask annotators to mark which part of an output contains what type(s) of errors.We train a fine-grained reward model for each type of error and optimize LM against these reward models.\n\nIn this example, we provide a relevance reward and a factuality reward after each sentence is generated.There is also a holistic information completeness reward after the whole text is generated.\n\nIn this paper, we propose that humans give fine-grained feedback to LM output, associating categories of undesired behavior (e.g., false or irrelevant generations) and a text span at a density (e.g., sentence or sub-sentence-level).To enable LMs to learn from such fine-grained feedback, we introduce the FINE-GRAINED RLHF framework.As shown in Figure 1, we first use collected human feedback to train fine-grained reward models such that each of them focuses on one category and provides rewards at the density associated with that category.We then integrate these reward models into Proximal Policy Optimization (PPO) [37], a commonly used RL algorithm for training LMs with preference-based human feedback ( \u00a72).\n\nWe conduct experiments on two language generation tasks-detoxification [12] ( \u00a73) and long-form question answering (QA) [39] ( \u00a74).For detoxification, toxicity is the only error category and we explore learning with a dense reward.We adopt PERSPECTIVE [1], a widely used language toxicity detection model trained on millions of human annotations, as our reward model.We use it to calculate a fine-grained reward after the generation of every sentence.Our experimental results show the efficacy and data efficiency of training models with dense reward compared to a holistic sequence-level reward, supported by automatic evaluation results.\n\nWith experiments on long-form QA, we aim to examine training models with fine-grained rewards at the two granularity dimensions (density and error category), for which we construct a long-form QA dataset, QA-FEEDBACK, along with our collected human feedback.We carefully develop a pipeline to collect fine-grained human feedback on three error categories at different density levels: i) irrelevance, repetition, or incoherence (sub-sentence), ii) incorrect or unverifiable facts (sentence), and iii) incomplete information (whole sequence; see Figure 1).Our experimental results show improved results in each error category by learning with such fine-grained feedback, supported by both automatic and human evaluation results.In a scenario with multiple reward models representing different error types, we also show FINE-GRAINED RLHF allows us to combine reward models with different weights and thus control the model training process towards a customized combination of desired behaviors.\n\n\nFINE-GRAINED RLHF\n\nWe introduce FINE-GRAINED RLHF, a framework that enables us to train fine-grained reward functions for generation outputs across different feedback types.We first define the RL environment and learning algorithm.Then we define the fine-grained reward models and describe how to incorporate the fine-grained reward model(s) into an RL algorithm, in contrast to previous RLHF studies that only consider a single reward.\n\nEnvironment: language generation as a MDP.We focus on language generation tasks.For each task, we are given a set of task input prompts D = {x n } N n=1 .We follow [33] to define language generation as a Markov Decision Process (MDP) \u27e8S, A, R, P, \u03b3, T max \u27e9 with a finite vocabulary V.Each MDP episode starts with a sampled prompt x = (x 1 , x 2 , . . ., x l ) with x i \u2208 V, and ends when the current time step exceeds T max or an end of sequence token is generated.S is the state space and s 0 = (x 1 , x 2 , . . ., x l ) \u2208 S is the initial state.An action in the environment a t \u2208 A is a generated token (by the policy LM model P \u03b8 ) at time t from V (a 0 is the begin sequence token).The transition function P : S \u00d7 A \u2192 \u2206S appends a t at the end of the state s t = (x 1 , x 2 , . . ., x l , a 0 , a 1 , . . ., a t\u22121 ).This process continues until the end time step T \u2264 T max is reached, which gives a generated sequence y = (a 1 , . . ., a T ).A reward function R : S \u00d7 A \u2192 R, which comes from the reward model(s) in FINE-GRAINED RLHF, provides dense rewards before and when T is reached.P \u03b8 can be initialized with a pre-trained language model, and sometimes also with supervised fine-tuning on task-specific demonstrations.The reward function is defined later.\n\nLearning algorithm: proximal policy optimization (PPO).PPO [37] is an actor-critic RL algorithm that is widely used in previous RLHF work to optimize the policy model against a reward model of human feedback.It uses a value model V \u03c8 (s t ) to estimate the value of state s t , and optimizes the policy model with a PPO clipped surrogate training objective.The advantage A t at timestep t is estimated by a generalized advantage estimation function [36]:\nA t = T t \u2032 =t (\u03b3\u03bb) t \u2032 \u2212t (r t \u2032 + \u03b3V \u03c8 (s t \u2032 +1 ) \u2212 V \u03c8 (s t \u2032 ))\n, with \u03b3 as a hyperparameter and \u03bb as the discounting factor for rewards.r t is the reward assigned to a t , which in our case is acquired using one or multiple learned reward models.The value model V \u03c8 (s t ) is optimized with an expected squared-error loss with the value target as\nV targ (s t ) = T \u22121 t \u2032 =t \u03b3 t \u2032 \u2212t r t \u2032 + \u03b3 T \u2212t V \u03c8old (s T )\n, where V \u03c8old is the lagging value model.Finally, PPO is trained to optimize both policy (P \u03b8 ) and value (V \u03c8 ) models with their respective objectives.No reward model is being optimized during PPO training.See Appendix B for more details.\n\nFine-grained reward models.Previous RLHF work adopts a holistic reward model R \u03d5 that maps input prompt x and generated output y to a single scalar reward representing its overall quality (Figure 1(a)).This single scalar reward is only assigned to the final token in the generated sequence, a T .Formally, r t = R \u03d5 (x, y) if t = T and 0 otherwise.\n\nIn contrast, we consider a reward function that is derived from one or multiple fine-grained reward models that (1) provide rewards densely (i.e., for subsequences of the generated output), and (2) compute rewards on distinct categories of undesired behaviors (e.g., false or repetitive generation), where each category is associated with an individual reward model.\n\nFor a fine-grained reward model R \u03d5 k that gives feedback on error category C k , we first segment y into L k segments (y k 1 , y k 2 , . . ., y k L k ) corresponding to the density (e.g., sentence-level) of R \u03d5 k , where each segment y k j ends at timestep T k j .R \u03d5 k outputs a reward R \u03d5 k (x, y, j) for each segment y k j given x and y as the input, which is assigned to the final token in y k j .Additionally, to ensure the fluency of generated outputs, we follow [41] to add an approximate KL divergence penalty to each token a t with a weight \u03b2, that is not backpropagated through during training.Formally, assuming that we have K fine-grained reward models that represent different error categories, we will have a combined reward function for each token a t as:\nr t = K k=1 L k j=1 1(t = T k j ) w k R \u03d5 k (x, y, j) \u2212 \u03b2 log P \u03b8 (a t | s t ) P \u03b8init (a t | s t )(1)\nwhere w k \u2208 R is a weight assigned to reward model R \u03d5 k .Then we follow the same PPO training algorithm to optimize the policy model.We discuss how we define and train fine-grained reward models for the detoxification and long-form QA task in our experiments in \u00a7 3 and \u00a7 4 respectively.\n\n\nTask 1: Detoxification\n\nThe task of detoxification aims to reduce the toxicity in the model generation y when given a prompt x.Toxicity is the only undesired behavior in this task, and we aim to explore learning with a dense reward in comparison to a single holistic reward.We conduct our experiments on REALTOXICITYPROMPTS, a dataset of 100K sentence-level prompts derived from the web that are known to easily elicit problematic generations in GPT-2 [31].Using a dense sentence-level finegrained reward, we demonstrate that our fine-grained reward exhibits greater sample efficiency compared to a holistic reward, achieving lower toxicity with fewer training steps while maintaining better fluency ( \u00a73.1).\n\nHolistic reward for (non-)Toxicity.We use the PERSPECTIVE API [1] as our reward model, which is widely used for language toxicity detection and is trained with millions of examples gathered from several online platforms and annotated by human annotators for toxicity.That means we use an off-policy reward model that is not trained on outputs from P \u03b8init .The API outputs a score between 0 (non-toxic) and 1 (toxic).Given the entire model output y, the holistic reward for RL is 1\u2212PERSPECTIVE(y).\n\nSentence-level (fine-grained) reward for (non-)Toxicity.To calculate the fine-grained reward, we query the API after the model generates each sentence instead of generating the full sequence.For each generated sentence y j , we assign PERSPECTIVE([y 1 , . . ., y j\u22121 ]) -PERSPECTIVE([y 1 , . . ., y j ]) as the sentence reward (i.e., how much toxicity is changed from generating y j ).Since there is only one error category, we omit the category superscript, using y j to denote the j th segment (e.g., sentence) in y.\n\n\nExperiments\n\nImplementation details.We follow previous work [17,21] and use GPT-2 large model as the initial policy model P \u03b8init .During both the exploration stage in RL training and inference, we use nucleus sampling decoding with p = 0.9 and temperature = 1.0.The generation length limit is set to 48.The value model used during RL training is initialized with GPT-2-base due to GPU memory constraint.We report RL training parameters in Appendix B. All scores are averaged over 3 independent runs.\n\nCompared systems and evaluation.We report the performance of FINE-GRAINED RLHF, RLHF with holistic reward (Hol.RLHF), and the state-of-the-art controlled generation approaches GeDi [17] and DEXPERTS [21].We follow previous work [17,21] to report the toxicity score calculated on each full generation sequence from the PERPLEXITY API, as well as other commonly used metrics for REALTOXICITYPROMPTS, including n-gram diversity and GPT-2 XL perplexity (PPL) as a proxy for fluency.The lower the perplexity, the more fluent the generated text.The toxicity score is reported as the maximum score among 4 sampled model outputs, averaged over all test input prompts.Other metrics are reported as the average score of the same 4 samples.Main results.Table 1 shows the experimental results on the REALTOXICITYPROMPTS test set.FINE-GRAINED RLHF with sentence-level fine-grained reward attains the lowest toxicity and perplexity among all methods, while maintaining a similar level of diversity.\n\nSample efficiency analysis.Figure 2 shows the max toxicity and average perplexity on the development set during training.FINE-GRAINED RLHF has the toxicity drop much faster while keeping a low-level perplexity.This shows that learning from denser fine-grained reward is more sample efficient than holistic reward.One explanation is that fine-grained reward locates where the toxic content is, which is a stronger training signal compared with a scalar reward for the whole text.The cost is that we have to query the reward model more times per example.\n\n4 Task 2: Long-Form Question Answering (QA)\n\nLong-form QA requires an LM to generate a textual response to a question with a comprehensive answer and explanation.To examine learning with fine-grained rewards at the two granularity dimensions (error category and density), we collect QA-FEEDBACK ( \u00a74.1), a long-form QA dataset annotated with human feedback on LM-generated responses.We define three error categories at different density levels and train a reward model for each ( \u00a74.2).We describe the experimental setup in \u00a74.3.Both human and automatic evaluation show that FINE-GRAINED RLHF outperforms preference-based RLHF and supervised fine-tuning models on all error categories ( \u00a74.4).We then show that adjusting the weights of fine-grained reward models during RL training leads to distinct behaviors in LM generation, allowing us to customize the LM for users with different needs ( \u00a74.5).Finally, we conduct an in-depth analysis of the fine-grained reward models, revealing that they compete against each other, and provide an analysis of their impact on the resulting policy model.\n\n4.1 QA-FEEDBACK: Long Form QA with Human Feedback QA-FEEDBACK is based on ASQA [39], a dataset that focuses on answering ambiguous factoid questions [26] in an open-domain setting.We use their provided oracle knowledge contexts to reformulate the task into a reading comprehension setting: given the input x that contains a question q and a set of knowledge passages P = {p 1 , . . ., p |P | }, generate a long-form response y.On average, there are roughly 65 words in each gold response.Since ASQA does not release the test set, we create our own train/development/test data split from the original train and development sets.We name our newly constructed data, along with collected human feedback (discussed next), QA-FEEDBACK.Overall, we have 3,853 training, 500 development, and 948 test examples (details in Appendix C).\n\nInitial policy and fine-grained human feedback.Before collecting human feedback, we follow [33] to initialize the policy model with supervised fine-tuning on a small set of examples.Specifically, we use 1K training examples to supervise fine-tuning of T5-large (the original baseline for ASQA) [32] to get P \u03b8init .We name this initial policy model SFT.We then sample outputs from SFT for the remaining training and development examples and collect fine-grained human feedback in three error categories-C 1 : irrelevance, repetition, or incoherence ; C 2 : incorrect or unverifiable facts based on knowledge passages; and C 3 : incomplete information .The collected feedback instances are then used as the training and development examples for training reward models.For each task prompt x, we only collect fine-grained feedback for one model output.Our data collection has IRB approval and is deemed exempt.\n\nWe instruct workers to identify any error in each model output y = (a 1 , . . ., a T ), marking the span of text associated with each identified error type.Formally, we define the set of user-annotated feedback for a task prompt x and model output y as F = {f i } where each f i = \u27e8c i , b i , e i \u27e9 represents the user-identified span (a bi , . . ., a ei ) of the error category C ci , where c i \u2208 {1, 2, 3}.Importantly, we impose three restrictions in the annotation: (1) error spans of category C 1 or C 2 should not overlap with each other; (2) only spans that do not have error C 1 need to be assessed as containing error C 2 or not; (3) C 3 can only apply to whole output sequences.Additionally, we ask workers to mark passage sentences that contain missing information if a C 3 error is annotated.We also ask workers to rewrite y into a corrected version y \u2032 that addresses all annotated feedback F. Details about the feedback collection interface, instructions, and quality control are in Appendix C.\n\nTo analyze human-human agreement, a subset of 300 examples receive annotations from two distinct workers.We observe that while exact agreement in error span boundaries is low, workers achieve reasonably high agreement on whether a sub-sentence contains C 1 and whether a sentence contains C 2 .2Therefore, we decide to have the density for error type C 1 , C 2 , and C 3 as sub-sentence, sentence and full sequence.We provide more data analysis including human agreement in Appendix C.\n\nPreference-based human feedback.For comparison purposes, we follow [29] to separately collect pairwise human preferences from the same group of workers.We sample 4 model outputs for each prompt x, which gives 6 pairs of model outputs.We ask the workers to indicate pairwise preferences (ties are allowed) based on all errors they can find in each model output.They are not asked to explicitly annotate these errors.\n\nAnnotation details.On average, both annotation tasks of fine-grained and preference feedback for one question take a worker about 6 minutes to finish.In contrast, [39] report that they spend about 15 minutes to label a human-written response for each question, which is much more time-consuming than our feedback annotation.On average, we pay $1.65 per example for both tasks, leading to $16.50 hourly pay for our workers.We include details of the pay structure in Appendix C. We observe that human annotators can reach a higher agreement in each aspect of fine-grained feedback compared to pairwise comparisons because the feedback definitions are more concrete.\n\n\nFine-Grained Reward Models\n\nWe train three separate reward models R \u03d51 , R \u03d52 , and R \u03d53 for C 1 , C 2 , and C 3 error categories respectively with a density of sub-sentence, sentence, and full sequence, respectively.Since reward models provide scalar reward scores and do not perform generation, we use the encoder-only Longformer-base [4] as our backbone model to handle long input sequences (more details of each reward model are in Appendix D).\n\nC 1 : Irrelevance, repetition, or incoherence.R \u03d51 targets to predict whether each sub-sentence in y contains a C 1 type error.We denote y = (y 1 1 , . . ., y 1 L1 ), where y 1 j is the jth segment at R \u03d51 's density (i.e., sub-sentence), with L 1 segments in total.We add a 2-class token-level classification layer (a single feed-forward layer) on the top of the Longformer encoder.The model input has the format of \"question: q answer: [sep] y 1 1 [sep] y 1 2 . . .\", and we take the classification output at each [sep] token to indicate whether the following y 1 j contains a C 1 error.We do not add passages in the model input because, intuitively, the detection of C 1 errors does not depend on them.To train R \u03d51 , we apply a token-level classification loss to each [sep] token before y 1 j , where its gold label g j is \"has error\" if there is a f i \u2208 F that has (a bi , . . ., a ei ) overlapped with y 1 j and c i = 1, and \"no error\" otherwise.When R \u03d51 provides a reward during RL training as in Eq. 1, we read a reward R \u03d51 (x, y, j) for every y 1 j given x and y.We define R \u03d51 (x, y, j) = +1 if R \u03d51 predicts \"no error\" for y 1 j and \u22121 otherwise.C 2 : Incorrect or unverifiable facts.R \u03d52 is developed for detecting a C 2 error at the sentence level in a similar way.The model input has the format of \"question: q context: p 1 p 2 . . .answer:\n[sep] y 2 1 [sep] y 2 2 . . . \"\n, where p's denotes the grounding passages and y 2 j represents the jth sentence.We train R \u03d52 similarly to R \u03d51 , with one exception: as we instruct the workers not to annotate a C 2 error for a span that is already labeled as containing a C 1 error, we do not calculate loss on sentences that are labeled as containing C 1 but not C 2 during R \u03d52 training.C 3 : Incomplete information.R \u03d53 is trained to measure the information completeness of y, at the full sequence level.Motivated by [19], R \u03d53 predicts a single scalar reward and is trained with a pairwise comparison loss [29]:\nL r (\u03d5) = \u2212E (x,\u0233p,\u0233 l )\u223cDp log \u03c3(R \u03d53 (x, \u0233p ) \u2212 R \u03d53 (x, \u0233l ))(2)\nwhere R \u03d53 (x, y) is the scalar output of the reward model for input x and output y; \u0233p and \u0233l are sampled from the same input x, and \u0233p has less missed information compared with \u0233l ; D p contains the pairwise comparisons bootstraped from human feedback on C 3 errors (see details in Appendix D).\n\nPreference-based reward model.The preference-based reward model is trained in a similar way to R \u03d53 , with \u0233p representing the human preferred response against \u0233l in the loss function Eq. 2. It outputs a scalar score for the given x and y that represents the overall response quality.\n\n\nExperimental Setup\n\nCompared systems.We compare our proposed method, FINE-GRAINED RLHF with the initial T5 policy model trained with 1K examples (SFT) and RLHF with holistic preference-based rewards (Preference RLHF).The reward models used in RLHF experiments are trained on 2.8K examples with annotated feedback (but no gold human response).For analysis, we also use the human gold responses of all training examples to finetune a fully supervised T5 model (SFT-Full).Notice that SFT-Full requires much higher annotation cost because it takes longer (15 minutes per example [39]) for annotators to draft long-form responses.\n\nImplementation details.Our policy model is based on T5-large [32] 2) incorrect or unverifiable facts (fact.) .They are also asked to compare the information completeness (comp.)for each output pair.To report evaluation scores for rel.and fact.error spans, we first map them to their corresponding error type density (sub-sentence and sentence).Then we report the error rate for each error type, measured as the percentage of sub-sentences that contains this type of error.Since spans with rel.error are not checked for fact.error (discussed in \u00a74.1), we exclude sub-sentences with only rel.error when report the error rate of fact.error.For automatic evaluation, we report RougeLSum [20] as used for the original ASQA data, as well as the score from each fine-grained reward model (R \u03d51 , R \u03d52 , and R \u03d53 ).Specifically, we report the percentage of all sub-sentences (or sentences) in the test set predicted as \"no error\" by R \u03d51 (or R \u03d52 ).For R \u03d53 , we report the averaged output score for all test examples.\n\n\nMain Results\n\nFigure 3 shows the human evaluation results for rel.and fact.error types.Table 2 shows the human pairwise comparison results for information completeness (comp.).\n\nError Rate   RLHF is particularly effective in reducing factual errors.Figure 3 shows that both FINE-GRAINED RLHF and Preference RLHF are effective in reducing factual errors in model generation.Meanwhile, we see little or no improvement in reducing irrelevance, repetition, or incoherence errors.We provide more in-depth analysis for this observation in \u00a74.5.\n\nTable 3 shows automatic scores on the QA-FEEDBACK test set, which show similar trends as human evaluation in terms of system comparisons, while all four systems achieve similar Rouge scores.\n\n\nLM Customization with FINE-GRAINED RLHF\n\nSince we use multiple reward models in FINE-GRAINED RLHF, adjusting their weights (see Eq. 1) during RL may lead to different LM behaviors.For example, adding more weight to a reward model associated with one specific desired behavior type (e.g., information completeness) may lead the generation more towards that behavior type compared to others (e.g., information relevance).This flexibility can potentially fit users with diverse needs.Therefore, in this section, we explore FINE-GRAINED RLHF's ability to customize the LM behavior.\n\nLM customization.As in Table 4, we explore three configurations of reward model weights (w 1 , w 2 , and w 3 for R \u03d51 , R \u03d52 , and R \u03d53 ) and name them 'short', 'medium', and 'long' according to the LM's average generation length.For simplicity, we fix w 2 = 0. Trade-off between error types.We observe that a higher w 1 leads to a bigger rel.reward, smaller fact.and comp.rewards, and shorter generated outputs.One interpretation is that R \u03d51 penalizes text spans that are irrelevant to the questions.As such, it encourages answering the question directly and penalizes referencing passages and generating auxiliary information.This reduces the model generation length and information completeness, and induces more factual errors.Reward models are competing against each other.In the prior section, we find that there is a trade-off between error types.To further look into this phenomenon, we explore the dynamics of each reward model during training.Figure 4 shows each reward model's rewards on the development set during training.All rewards are z-normalized for visualization.We see that the fact.reward is consistently increasing.The rel. reward increases rapidly in the first 250 steps and then starts decreasing, while the comp.reward exhibits an opposite trend, decreasing at first and then starting to increase.As discussed earlier, one interpretation is that relevance (precision) and information completeness (recall) can be adversarial objectives, so the rewards are competing.The three rewards reach an equilibrium point in later steps.\n\n\nAnalysis\n\nAblation: Does the LM learn from all reward models?What if we remove one reward model?Table 5 explores the policy LM behavior when one of the three reward models is removed during training.Qualitative examples are in Appendix A. First, we observe that the corresponding reward decreases dramatically when the model is removed.When the rel.reward model ( R \u03d51 ) is removed, the outputs become extremely long and the comp.reward is extremely high.We observe the outputs and find the model is copying a lot of content from the passages.When the fact.reward model ( R \u03d52 ) is removed, the rel.reward becomes the highest.We observe that the LM tends to answer the question directly and not reference the passages, which causes a lot of hallucinations.When the comp.reward model ( R \u03d53 ) is removed, the outputs are concise and factual but not providing all relevant information to the question.Thus, it has lower information completeness and Rouge score compared with the LM trained with all reward models.\n\nReward model performance.We report and analyze the performance of each reward model in predicting its corresponding error category.The rel. reward model R \u03d51 has a binary classification accuracy of 69.6, and an F1 score (for the \"has error\" class) of 68.5 on model-generated subsentences from the development set.We sample 20 sub-sentences where R \u03d51 predicts the opposite of the human label, and observe that all of them either 1) contain relevant auxiliary information and are marked as \"no error\" by humans, or 2) are marked as irrelevant by humans but provide closely related background information to the question.In other words, R \u03d51 is mostly struggling with predicting the relevance of auxiliary information, and it rarely fails to predict a direct answer as \"no error\".\n\nThe fact.reward model R \u03d52 has an accuracy of 77.8 and an F1 score of 67.5.We sample 20 sentences where R \u03d52 makes a prediction mistake and we observe that the mistakes often happen when the generated sentence is highly abstractive instead of directly copying information from the passage.We also observe that more than 80% of human labeled factual errors occur when the model generates a direct answer (not auxiliary information) that contains hallucinated information or a random entity from a passage.We notice that R \u03d52 correctly captures more than 80% of such errors.\n\nThe comp.reward model R \u03d53 has an accuracy of 70.9 in pairwise comparison.In contrast, the preference-based reward model only reaches an accuracy of 57.2.This helps confirm our intuition that assessing long-form generation outputs holistically can be more ambiguous and subjective than evaluating the outputs with a focus on a specific undesired behavior type.\n\nComparison with ChatGPT responses.We experiment with answering the questions with ChatGPT.\n\nTo familiarize ChatGPT with the style of our LFQA task, we prompt it with the task instruction and a single random QA example (due to length limitation).ChatGPT achieves a RougeLSum score of 40.92 on the test set, which is much lower than our models.We do not use our trained reward models to evaluate ChatGPT outputs because reward models trained on T5-large may not generalize well to ChatGPT.We instead manually inspect the ChatGPT responses, and observe that they are mostly concise and factual, yet lack the auxiliary information necessary to clarify ambiguous questions.Qualitative examples are in Appendix A. This shows the difficulty for ChatGPT in learning user-desired behaviors through simple prompting.\n\n\nRelated Work\n\nReinforcement learning from human feedback (RLHF).RLHF [46,42,29] aims to optimize the policy language model to generate content that is desired by human.This framework has been explored to improve the model performance on a variety of natural language processing tasks such as text summarization [40], instruction following [29], question answering [24,27] and reducing harmfulness [3,2,22,10].Most of these studies collect human preferences over pairs of model outputs on one or a set of desired attributes, in order to train a reward model to assign a holistic score for a generation output during RL training.[13] trains separate reward models that assign scores for different desired attributes, but still uses a single reward that combines scores from all reward models.In contrast, we explore RLHF with fine-grained reward models trained on human feedback where each reward model provides dense reward after every small text segment for a specific type of desired behavior.[30] explores using intermediate rewards to improves LM performance on reasoning tasks.\n\nLearning from human feedback in NLP.There also exists work that explores non-RL methods to learn from human feedback.[44] trains a reward model that predicts a single score for each model output and selects samples with the highest reward scores for supervised fine-tuning.[38,14,42] train a conversational model to predict both the response and a binary user satisfaction score in order to improve the response generation.Besides such numerical human feedback, natural language (NL) human feedback has also been explored.[23,6] collect and store NL human feedback in a feedback memory for the model to retrieve and then perform the end task conditioning on the retrieved feedback.[5,35,34] use a refinement model to refine model outputs conditioning on NL human feedback and then use a reward model to select the best refined outputs for supervised fine-tuning.Methods for using a reward model to guide LM generation towards desired behaviors at inference time [21,7] can complement our work that aims to improve the LM during training.[16] also explores incorporating human feedback into LM pre-training.\n\n\nDiscussion\n\nAnnotation Costs.It is important to note that the fine-grained human feedback used for training our fine-grained reward models does not incur a greater cost than holistic human preference.As outlined in \u00a7 4.2, our observations reveal that annotators require a substantial amount of time to compare two lengthy text outputs.For the long-form QA task, both fine-grained feedback and preference-based feedback takes approximately 6 minutes per sample for an annotator.\n\n\nBroader Impacts\n\nWe propose the FINE-GRAINED RLHF framework that can incorporate multiple reward models to provide dense rewards for RL training, which leads to LM outputs that are optimized towards such rewards.Our framework can be applied to any text generation task, thereby enhancing LM performance by offering more nuanced guidance than holistic feedback.The key advantages of the FINE-GRAINED RLHF framework are two-fold:\n\nFlexibility.Our framework significantly expands the versatility of reward models for RLHF.For example, future work involving fact-checking, sentiment classification, toxicity detection, among others, can all be incorporated within this framework.LMs can be trained against all these reward models via FINE-GRAINED RLHF.\n\nControllablility.Having multiple reward models that stand for different feedback types allows the end user to exert greater control over RL training (e.g., through different combinations of reward model weights; see details in \u00a7 4.5).This leads to customized model behaviors, a benefit particularly valuable for applications like educational tools where model personalization is crucial.\n\n\nLimitations and Future Work\n\nOne major limitation of our framework comes from the additional compute cost of getting finegrained rewards, compared to RLHF with a holistic reward.For instance, in the detoxification task, we need to make multiple PERSPECTIVE API calls for each model output depending on how many sentences are generated, while RLHF with a holistic reward only requires one.In the long-form QA task, we need to calculate a dense reward from multiple reward models, which takes more compute time and GPU memory than a single reward model.\n\nAnother limitation is that different tasks may have different definitions of fine-grained feedback in terms of the feedback types and the density level of each type.Therefore, defining feedback that is well-suited for a task and training reward models accordingly requires non-trivial manual effort.\n\nFinally, in this work, we carefully control the quality of annotated feedback, which is then used to train reward models for RL.In practice, when a deployed model is released to the public, end users don't always give clean feedback.Therefore, how to obtain effective learning signals from noisy human feedback in the wild still needs further investigation.\n\nSome other interesting questions to explore in the future include: 1) Can we obtain fine-grained feedback from LMs like GPT-4 instead of humans to improve model performance and reduce annotation costs?2) How can other non-RL approaches of using human feedback such as controlled generation during inference time complement FINE-GRAINED RLHF? 3) How would fine-grained reward and value model sizes affect policy model performance during RL training?\n\n\nConclusion\n\nIn this work, we introduce FINE-GRAINED RLHF, a framework that enables LMs to learn from multiple fine-grained reward models trained from human feedback, where each reward model detects a specific error category and provides dense rewards.We conduct experimental analysis on two text generation tasks to illustrate the performance gain of FINE-GRAINED RLHF than RLHF over holistic rewards, supported by both automatic and human evaluation.Furthermore, we show that an LM can be customized for specific needs using different combinations of fine-grained reward models.\n\n\nAppendices\n\n\nQuestion:\n\nWhere does the flat iron cut of meat come from?\n\nPassages: Article Title: Flat iron steak Flat iron steak (US), butlers' steak (UK), or oyster blade steak (Australia and New Zealand) is a cut of steak cut with the grain from the shoulder of the animal.This produces a flavorful cut that is a bit tough because it contains a gristly fascia membrane unless removed.Some restaurants offer it on their menu, often at lower price than the more popular rib-eye and strip steaks of the same grade.This is used, in some places, as a means of selling a less expensive cut from the same animal, for example Kobe beef.This cut of steak is from the shoulder of a beef animal.It is located adjacent to the heart of the shoulder clod, under the seven or paddle bone (shoulder blade or scapula).The steak encompasses the infraspinatus muscles of beef, and one may see this displayed in some butcher shops and meat markets as a \"top blade\" roast.Anatomically, the muscle forms the dorsal part of the rotator cuff of the steer.This cut is anatomically distinct from the shoulder tender, which lies directly below it and is the teres major.Flat iron steaks usually have a significant amount of marbling.\n\n\nLM outputs SFT:\n\nFlat iron steak is a cut of steak cut with the grain from the shoulder of the animal.This produces a flavorful cut that is a bit tough because it contains a gristly fascia membrane unless removed.Some restaurants offer it on their menu, often at lower price than the more popular rib-eye and strip steaks of the same grade.This is used, in some places, as a means of selling a less expensive cut from the same animal, for example Kobe beef.\n\n\nPref. RLHF:\n\nFlat iron steak is a cut of steak cut with the grain from the shoulder of the animal.This produces a flavorful cut that is a bit tough because it contains a gristly fascia membrane unless removed.Some restaurants offer it on their menu, often at lower price than the more popular rib-eye and strip steaks of the same grade.This is used, in some places, as a means of selling a less expensive cut from the same animal, for example Kobe beef.\n\n\nFINE-GRAINED RLHF\n\nFlat iron steak is a cut of steak cut with the grain from the shoulder of the animal.This is located adjacent to the heart of the shoulder clod, under the seven or paddle bone (shoulder blade or scapula).This steak encompasses the infraspinatus muscles of beef, and one may see this displayed in some butcher shops and meat markets as a \"top blade roast.\"\n\nTable 8: Qualitative examples on another question.Here we highlight the information in blue that is covered by the LM trained with FINE-GRAINED RLHF but missed ( C 3 : incomplete information error) by SFT and Pref.RLHF.\n\n\nA.3 Examples on Reward Model Ablation\n\nAs discussed in \u00a7 4.6, reward models are competing against each other, and we experiment with removing one of the three reward models during RL training.Table 9 shows an example of how LMs behave in such scenarios.See \u00a7 4.6 for our observations.\n\n\nQuestion:\n\nHow much does it cost to raise the Costa Concordia?\n\nPassages: Article Title: Costa Pacifica Costa Pacifica is a \"Concordia\"-class cruise ship for Costa Crociere.She was handed over to Costa Crociere on 29 May 2009.Her sister ships, \"Costa Concordia\" and \"Costa Serena\", were launched in 2006 and in 2007, with \"Costa Favolosa\" and \"Costa Fascinosa\" launched in 2011 and 2012 respectively.Onboard facilities include the Samsara Spa and PlayStation World, an area boasting PS3 consoles, and the first recording studio at sea. \"Costa Pacifica\" is the third ship of the \"Concordia\"-class, preceded by sister ships \"Costa Concordia\" and \"Costa Serena\" and was followed by \"Costa Favolosa\" and \"Costa Fascinosa\" on 2011 and all part of the expansion program of Costa which entailed an investment of 2.4 billion Euro, and is currently the largest fleet expansion program in the world.\"Costa Pacifica\" was ordered on 14 December 2005, by Carnival Corporation, the parent company of Costa Crociere.The order was four ships, which includes; \"Carnival Dream\", for Carnival Cruise Lines; \"Nieuw Amsterdam\" for Holland America Line; and \"Ruby Princess\" for Princess Cruises, at a total cost of US $3 billion.\n\nArticle Title: Costa Concordia disaster On 13 January 2012, the Italian cruise ship \"Costa Concordia\" ran aground and overturned after striking an underwater rock off Isola del Giglio, Tuscany, resulting in 32 deaths.The eight year old Costa Cruises vessel was on the first leg of a cruise around the Mediterranean Sea when she deviated from her planned route at the Isola del Giglio, sailed closer to the island, and struck a rock formation on the sea floor.A six-hour rescue effort resulted in most of the passengers being brought ashore.\"Costa Concordia\" was officially declared a \"constructive total loss\" by the insurance company, and her salvage was \"one of the biggest maritime salvage operations\".On 16 September 2013, the parbuckle salvage of the ship began, and by the early hours of 17 September 2013, the ship was set upright on its underwater cradle.In July 2014, the ship was refloated by large sponsons (metal tanks) welded to its sides and was towed 320 kilometres (200 miles) to its home port of Genoa for scrapping which was finished in July 2017.The total cost of the disaster, including victims' compensation, refloating, towing and scrapping costs, is estimated at approximately $2 billion, more than three times the $612 million construction cost of the ship.\n\nLM outputs: removing one fine-grained reward model during training\n\n\nAll reward models:\n\nCosta Concordia is a cruise ship for Costa Crociere.On 13 January 2012, the Italian cruise ship Costa Concordia ran aground and overturned after striking an underwater rock off Isola del Giglio, Tuscany, resulting in 32 deaths.The total cost of the disaster, including victims' compensation, refloating, towing and scrapping costs, is estimated at approximately $2 billion, more than three times the $612 million construction cost of the ship.\n\nWithout relevance RM R\u03d5 1 :\n\nOn 13 January 2012, the Italian cruise ship Costa Concordia ran aground and overturned after striking an underwater rock off Isola del Giglio, Tuscany, resulting in 32 deaths.On 16 September 2013, the parbuckle salvage of the ship began, and by the early hours of 17 September 2013, the ship was set upright on its underwater cradle.In July 2014, the ship was refloated by large sponsons (metal tanks welded to its sides and was towed 320 kilometres (200 miles) to its home port of Genoa for scrapping which was finished in July 2017.The total cost of the disaster, including victims' compensation, refloating, refloating, towing and scrapping costs, is estimated at approximately $2 billion, more than three times the $612 million construction cost of the ship.The total cost of the disaster, including victims' compensation, refloating, towing and scrapping costs, is estimated at approximately $2 billion.\n\nComment: Too much irrelevant information & repetition error.\n\nWithout factuality RM R\u03d5 2 :\n\nThe cost to raise the Costa Concordia was estimated at approximately $2 billion , more than three times the $612 million construction cost of the ship.Comment: Factuality error.Direct answer without any supporting evidence from the passages.\n\nWithout info.completeness RM R\u03d5 3 :\n\nCosta Concordia is a \"Concordia\"-class cruise ship for Costa Crociere.She was handed over to Costa Crociere on 29 May 2009.The total cost of the disaster, including victims' compensation, refloating, towing and scrapping costs, is estimated at approximately $2 billion, more than three times the $612 million construction cost of the ship.\n\nComment: Miss the auxiliary information about the disaster .\n\nTable 9: Qualitative examples on the reward model ablation experiment.We show the LM outputs when we use all the reward models and when one of the reward models is removed during training.We also mark the errors in each output.\n\n\nA.4 Comparison with ChatGPT responses\n\nWe compare the responses generated by ChatGPT (one-shot) 3 and our system in Table 10.As discussed in \u00a7 4.6, We find that ChatGPT responses are relevant and factual, yet lack the auxiliary information to answer the ambiguous questions.This shows that it is challenging for ChatGPT to learn user-desired behaviors through prompting and in-context learning.\n\n\nB Algorithm and Training Details of FINE-GRAINED RLHF B.1 PPO with Fine-Grained Rewards\n\nThe algorithm below shows in detail how PPO updates the policy LM P \u03b8 and the value model V \u03c8 with K fine-grained reward models R \u03d5 k .\n\nAlgorithm for PPO iteration = 1, . . ., \u00b5 do 8:\n\nUpdate the policy model by maximizing the PPO clipped surrogate objective:\n\u03b8 \u2190 arg max \u03b8 1 |D b | |D b | n=1 1 |y n | |y n | t=1 min P \u03b8 (a t | s t ) P \u03b8old (a t | s t ) A t , clip(v t , 1 \u2212 \u03b5, 1 + \u03b5)A t 9:\nUpdate the value model by minimizing a square-error objective:\n\u03c8 \u2190 arg min \u03c8 1 |D b | |D b | n=1 1 |y n | |y n | t=1 V \u03c8 (s t ) \u2212 V targ (s t ) 2 Output P \u03b8 B.\n\nImplementation Details\n\nModel architectures.For the detoxification experiments, the policy model is initialized with GPT2large [31], and the value model is initialized with GPT2-base.For the long-form QA experiments, the policy model is initialized with a supervised fine-tuned T5-large [32], and the value model is initialized with T5-base.This design follows InstructGPT [29], which uses a larger (175B) policy model, and smaller value and reward (6B) models.\n\nTraining details on detoxification.For both the holistic reward baseline and the sentence-level (fine-grained) reward, we do a hyper-parameter search with the same set of hyper-parameters.For training, we run 200K episodes.The batch size (number of episodes per card during training) is 64.We use Adam optimizer with a linear learning rate scheduler and 10 warmup steps.We perform a hyper-parameter grid-search for peak learning rate \u2208 {5e \u2212 6, 1e \u2212 5, 2e \u2212 5}, KL coefficient \u03b2 \u2208 {0.1, 0.2, 0.3}, discounting factor \u03bb \u2208 {0.95, 0.97, 0.99}, and the frequency of exploration (number of sampled outputs) \u2208 {2, 4, 8}.We find that the higher the KL coefficient, the lower the perplexity, and the higher toxicity.This is consistent with findings from previous RLHF studies ( [29], [33]).For a fair comparison, we eventually choose a set of parameters that achieve a similar level of perplexity for both reward models.The optimal set of hyper-parameters for holistic reward is \u03b2 = 0.3, \u03bb = 0.99.For sentence-level reward \u03b2 = 0.1, \u03bb = 0.95.The learning rate is 1e \u2212 5, and the exploration frequency is 4 for both experiments.We choose the checkpoint with the lowest validation set toxicity for evaluation.Regarding computation time, we use 2\u00d7 80G NVIDIA A100 GPU for training, and the run time is about 22 hours.\n\nTraining details on long-form QA.We conduct a similar hyper-parameter grid search as our detoxification experiments.For long-Form QA, the input length limit is 1024, and the output length limit is 200.Notice that this is much longer than detoxification, so we use a smaller batch size and fewer training episodes.We experiment with multiple combinations of reward model weights.Fixing w 2 = 0.5 (factuality reward weight), we perform a grid search on w 1 , w 3 \u2208 [0.0, 0.5].\n\nWe eventually choose w 1 = 0.3, w 2 = 0.5, w 3 = 0.3, which reaches a balance between three reward models and allows all three rewards to increase during training.For training, the batch size (number of episodes per card during training) is 32.We use Adam optimizer with a linear learning rate scheduler and 100 warmup steps.We perform a hyper-parameter grid-search for peak learning rate \u2208 {5e \u2212 6, 1e \u2212 5, 2e \u2212 5}, KL coefficient \u03b2 \u2208 {0.1, 0.2, 0.3}, discounting factor \u03bb \u2208 {0.95, 0.97, 0.99}, and the frequency of exploration \u2208 {2, 4, 8}.The optimal set of hyperparameters for Pref.RLHF is \u03b2 = 0.2, \u03bb = 0.99.For FINE-GRAINED RLHF, \u03b2 = 0.3, \u03bb = 0.95.The learning rate is 1e \u2212 5, and the exploration frequency is 4 for both experiments.we run 80K episodes, which is approximately 5 epochs.We choose the checkpoint with the highest validation reward for evaluation.Regarding computation time, we use 2\u00d7 80G NVIDIA A100 GPU for training, and the run time is about 15 hours.\n\nA note on the error bars.All results we report in the paper are from 3 independent runs.The scores reported are all averaged across all runs.The error bars are represented as the shades behind each training curve in our figures.It shows the standard error across three runs.\n\nC Long-Form QA Data and Human Feedback Annotation C.1 Data Construction ASQA [39] is a long-form QA dataset that focuses on answering ambiguous factoid questions in an open-domain setting that requires passage retrieval from a given Wikipedia passage corpus.We reformulate it into a reading comprehension setting: given the input x that contains a question q and a set of knowledge passages P = {p 1 , ..., p |P | }, generate a long-form response y.To construct P for each input x, we use the oracle knowledge contexts provided by ASQA for each x, that are text snippets from the passage corpus.We use BM25 4  Figure 5 shows the fine-grained human feedback annotation interface with an example from QA-FEEDBACK.In addition to the task input-question q and oracle passages P , we also provide a human-written response from ASQA to the worker as reference.However, it is important to note that, in practice, the annotation of our fine-grained feedback should not require the human-written response.The only purpose for us to provide the gold response is to have our workers follow the same question interpretation and expected response of the workers who annotate for ASQA, such that our experimental comparison with supervised models (SFT and SFT-Full; details in \u00a7 4.3) is fair.However, we still instruct our workers to strictly use the given passages for checking factual errors.For each span error, we ask the worker to select one out of 5 categories shown in Figure 6 (left). 6owever, we collapse these 5 categories into two categories (C 1 and C 2 mentioned in \u00a7 4.1) based on whether the error detection depends on the passages or not.When workers mark passage sentences as containing missing information, we instruct them to categorize each sentence as missing \"answer\", \"major auxiliary information\" or \"minor auxiliary information,\" as shown in Figure 6 (right).Our instruction to the worker is provided in Figure 8.\n\nQuality control.Before feedback collection, we design a qualification task to select qualified workers for this feedback annotation task.The qualification task consists of 5 questions with their corresponding passages and model outputs for the workers to annotate.We manually review about 70 submissions of the qualification task and select 15 workers whose annotation is marked by us as of high quality.Throughout the actual feedback annotation process, we constantly monitor the annotated data and send feedback to workers.\n\nPreference-based feedback.For comparison purposes, we follow [29] to collect pairwise human preferences from the same group of workers we select from the qualification task.We sample four model outputs for each prompt x, which gives 6 pairs of model outputs.Similarly, we provide the worker with the human-written response and ask the workers to indicate pairwise preferences (ties are allowed) based on all errors they can find each model output.Figure 7 shows the preference-based human feedback annotation interface with an example from QA-FEEDBACK.\n\nPay structure.We pay a base rate of $1.5 per example for annotating fine-grained or preference feedback.If the example consists of \u2265 3 passages to read, we assign an additional $0.3 bonus to the example.On average, we pay roughly $1.65 per example for both tasks, which gives an $16.5 hourly pay for our workers.\n\n\nC.3 Analysis of Collected Fine-Grained Feedback\n\nOverall, among all error spans we collect, 76% of them are C 1 errors and the remaining 24% are C 2 errors.However, it is important to note that we instruct workers to label C 2 errors only at places that don't have a C 1 error.75% examples are labeled as being incomplete; i.e., containing missing information that can be found in the given passages (C 3 ).Among all marked passage sentences that contain missing information, 31%, 42% and 27% are missing answer, major auxiliary information and minor auxiliary information respectively.\n\nTo analyze human-human agreement, a subset of 300 examples receive annotations from two distinct workers.We observe that while the exact agreement in error span boundaries is low, workers achieve reasonably high agreement on whether a sub-sentence contains C 1 (reach an agreement for 83% of all sub-sentences) and whether a sentence contains C 2 (92%). 7The agreement on whether a model output contains complete information or not (C 3 ) is 85%.Therefore, we decide to have the density for error type C 1 , C 2 , and C 3 as sub-sentence, sentence and full sequence.\n\n\nD Long-Form QA Reward Model Training Details\n\nWe train reward models with the 2,835 training examples with feedback collected and select the best model for each error category based on the their performance on the development set.The batch size and training epochs are 24 and 50 for R \u03d51 and R \u03d52 .Each training is run on a single 80G NVIDIA A100 GPU, taking 1 and 2 hours for training R \u03d51 and R \u03d52 respectively. 8The batch size and training epochs are 12 (per GPU) and 30 for R \u03d53 and the preference-based reward model.Each training is run on 2\u00d7 80G NVIDIA A100 GPU and takes 2 hours.We use Adam optimizer with a linear learning rate scheduler for all reward model training.For each reward model, we search the learning rate over {5e \u22126 , 1e \u22125 , 5e \u22125 }, weight decay over {0.001, 0.01}, and warm-up step ratio over {0.1, 0.2} based on the dev set performance.Specifically, we use the model for R \u03d51 and R \u03d52 that achieve the best binary classification accuracy.For R \u03d53 and the preference-based reward model, we select the model that achieves the best pairwise comparison accuracy.We also provide more training details for each reward model below.R \u03d51 for C 1 : Irrelevance, repetition, or incoherence.To train the reward model R \u03d51 that detects error of irrelevance, repetition, or incoherence, we apply a token-level classification loss to each [sep] token before y 1 j , where its gold label g j is \"has error\" if there is a f i \u2208 F that has a bi,...,ei overlapped with y 1 j and c i = 1, and \"no error\" otherwise.We observe that most of the spans marked as error type C 1 that are shorter than 5 words usually carry very little information or are annotated as a result of workers being very careful or strict.Therefore, we filter out such short spans before constructing training examples for R \u03d51 .Overall, we get 7379 and 8059 sub-sentences with the \"has error\" and \"no error\" label respectively.R \u03d52 for C 2 : Incorrect or unverifiable facts.We train R \u03d52 in a similar way as how we train R \u03d51 .Instead of predicting the error for each sub-sentence, R \u03d52 is trained to predict at the sentence level (i.e., y 2 j is the j th sentence in y).Since workers do not annotate C 2 error for spans that are already labeled as having C 1 error, in order to avoid false negatives in training R \u03d52 , we do not provide gold label nor calculate loss for sentences that only contain C 1 error from training.In other words, all sentences that contain a C 2 error has the gold label \"has error\" and sentences that contain no error has the gold label \"no error\".Overall, we get 1600 and 3411 sentences with the \"has error\" and \"no error\" label respectively.R \u03d53 for C 3 : Incomplete information.Instead of casting this as a classification task, R \u03d53 predicts a single scalar reward given x and y and is trained with a pairwise comparison loss [29].This is motivated by early work [19] that shows the better reliability of pairwise comparison than error classification when assessing a full generation sequence.To construct training data for R \u03d53 , we bootstrap pairwise comparisons from the corrected model output y \u2032 as follows.We first map each sub-sentence in y \u2032 to a passage sentence in P that has a sub-string with the highest token-level F1 score with the sub-sentence, 9 and denote all mapped sentences as S. We then sample four responses from SFT, for each we do the same sentence mapping to get a set of passages sentences S \u2032 .We calculate score(y) = |S \u2032 \u2229 S|/|S| as the information completeness score for each model response y.\n\nWe follow [29] to pair up sampled responses for q and denote each sampled response pair as (\u0233 p , \u0233l ), where score(\u0233 p ) > score(\u0233 l ).We drop the pairs where score(\u0233 p ) = score(\u0233 l ).Then we follow [29] to train R \u03d53 with the loss function in Eq. 2. We have a total number of 6821 pair examples in training.\n\nPreference-based reward model.The preference-based reward model is trained in a similar way as R \u03d53 , with \u0233p representing the human preferred response against \u0233l in the loss function Eq. 2. We drop the pairs where a tie is indicated.We have a total number of 14981 pair examples in training.\n\nFigure 2 :\n2\nFigure 2: Curves of toxicity and perplexity on the dev set vs. training steps.\n\n\nEvaluation.\n\nWe conduct both human and automatic evaluation.Human evaluation is run on 200 randomly sampled test set examples of QA-FEEDBACK to compare Fine-Grained RLHF with all baselines.Each model output is sampled from inference results of 3 training runs.We use the same protocol of feedback collection to have the same set of workers annotate spans in each model output that contain (1) irrelevance, repetition, or incoherence error (rel.) and (\n\n\nFigure 3 :\n3\nFigure 3: Human evaluation on rel.(left) and fact.(right) error, measured by % of sub-sentences that contain the error type (\u2193).\n\n\n\n\n5 and w 3 = 0.3, and use 0.4, 0.3, and 0.2 for w 1 , which leads to 'short', 'medium', and 'long' generation outputs respectively.We manually inspect 30 random examples and observe that (1) 'short' generates more relevant content, but is less factual and complete; (2) 'long', in contrast, gives the most factual and complete generation.This reflects that the LM is referencing a large amount of content from passages; (3) The 'medium' configuration balances the three rewards and has the highest Rouge score.24/30 examples follow the above rule.Qualitative analysis and examples of LM customization are in Appendix A.\n\n\nFigure 4 :\n4\nFigure 4: Dynamics of each type of reward during training (reward vs. training steps).All rewards are z-normalized.\n\n\n\n\nto map each knowledge context (text snippet) to the closest passage from the passage corpus and use the resulting passages as P .Our train and dev examples come from the original ASQA train set and our test examples are the original ASQA dev examples.C.2 Human Feedback Annotation Fine-grained feedback.As discussed in \u00a7 4.1, we first use 1K randomly sampled training examples to train a T5-large based supervised model SFT as the initial policy model P \u03b8init .Then we collect feedback on sampled outputs from SFT for the remaining 2,853 training examples and the 500 development examples, using the Amazon Machanical Turk platform. 5\n\n\nFigure 5 :\n5\nFigure 5: Fine-grained feedback annotation interface.\n\n\nFigure 6 :\n6\nFigure 6: Error types in the fine-grained feedback annotation interface.\n\n\nFigure 7 :\n7\nFigure7: Preference feedback annotation interface.The task input is omitted (same as in Figure5).\n\n\nFigure 8 :\n8\nFigure 8: Fine-grained feedback annotation instructions.\n\n\nTable 1 :\n1\nResults on the REALTOXICI-TYPROMPTS test set.\nToxicityFluencyDiversityavg max (\u2193) PPL (\u2193) dist-2 (\u2191) dist-3 (\u2191)GPT-20.1929.580.9470.931Controlled GenerationGeDi0.15424.780.9380.938DEXPERTS0.13622.830.9320.922Hol. RLHF0.13011.750.9430.926F.G. RLHF0.0819.770.9490.932\n\n\n\n[33]is supervised finetuned on 1K training examples, as explained in \u00a74.During RL exploration, we use top-k (k = 20) sampling decoding with temperature = 0.7, which is set based on previous RLHF work[33].The value model used during RL training is initialized with T5-base due to GPU memory constraint.The reward model weights we used in FINE-GRAINED RLHF are w 1 = 0.3, w 2 = 0.5, w 3 = 0.3, unless otherwise specified.Although we use three reward models during RL training, we only observe very small relative additional cost (roughly 1% training time) compared to preference RLHF.During inference, we use greedy decoding to generate responses.We report more details including RL training parameters in Appendix B. All scores reported are averaged over 3 independent runs.\n\n\nTable 2 :\n2\nHuman pairwise comparison on information completeness (comp.),wherewin/loserefers to FINE-GRAINED RLHF.FINE-GRAINED RLHF outperforms SFT and Preference RLHF on all error types.Figure3and Table2show that our FINE-GRAINED RLHF leads to generation that is much more factually correct and contains more complete information, compared to all other systems.It generates fewer irrelevance, repetition, andincoherence errors, compared with SFT and Preference RLHF.In the meantime, Preference RLHF, despite greatly reducing factual errors compared to the initial policy model SFT, generates even more irrelevance, repetition, and incoherence errors than SFT.FINE-GRAINED RLHF outperforms Preference RLHF potentially due to more specific and localized training signals.In addition, we ask annotators to compare the overall generation quality of FINE-GRAINED RLHF and preference RLHF.Although Preference RLHF is trained directly with such preference feedback, FINE-GRAINED RLHF was rated better than Preference RLHF in 30.5% of all examples and worse in 24.5% of examples.The annotators indicate a tie in the remaining 45% of cases.Surprisingly, FINE-GRAINED RLHF outperforms SFT-Full with more factual and complete generation, despite a much lower annotation cost.\n\n\nTable 3 :\n3\nAutomatic evaluation on the QA-FEEDBACK test set.\nrel.fact.comp.rel.fact.comp.avg.R\u03d5 1 (\u2191) R\u03d5 2 (\u2191) R\u03d5 3 (\u2191) Rouge(\u2191)Config R\u03d5 1 (\u2191) R\u03d5 2 (\u2191) R\u03d5 3 (\u2191) Rouge(\u2191)lenSFT-Full0.5080.7560.04449.63Short0.6370.760-0.23148.9974.92SFT + Pref. RLHF 0.482 0.5130.749 0.781-0.053 0.10148.96 49.84Medium 0.513 Long 0.4250.816 0.8600.139 0.24149.93 48.7298.66 109.63+ F.G. RLHF 0.5130.8160.13949.93\n\nTable 4 :\n4\nAutomatic evaluation results (test set) of FINE-GRAINED RLHF trained with different reward model weight configurations.\n\n\nTable 5 :\n5\nAblation of reward models on the development set.R \u03d51 , R \u03d52 , and R \u03d53 correspond to the reward model for relevance, factuality, and information completeness.\n\n\nTable 7 and\n7\nExamples on LM Customization . . . . . . . . . . . . . . . . . . . . . . . . ...16A.2 Examples on LM Errors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ...17A.3 Examples on Reward Model Ablation . . . . . . . . . . . . . . . . . . . . . ...19 A.4 Comparison with ChatGPT responses . . . . . . . . . . . . . . . . . . . . . ...19 B Algorithm and Training Details of FINE-GRAINED RLHF 21 B.1 PPO with Fine-Grained Rewards . . . . . . . . . . . . . . . . . . . . . . . . ...21B.2 Implementation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ...21Table8show examples of LM outputs from all the compared systems (SFT, Pref.RLHF, and FINE-GRAINED RLHF).We mark the fine-grained errors on the model outputs.Overall, our FINE-GRAINED RLHF outperforms SFT and Pref.RLHF in all three error types.Not only did Borden's persistence allow him to represent Canada in Paris as a nation, it also ensured that each of the dominions could sign the Treaty of Versailles in its own right, and receive a separate membership in the League of Nations.During the conference Borden tried to act as an intermediary between the United States and other members of the British Empire delegation, particularly Australia and New Zealand over the issue of Mandates.Borden also discussed with Lloyd George, the possibility of Canada taking over the administration of Belize and the West Indies, but no agreement was reached.At Borden's insistence, the treaty was ratified by the Canadian Parliament.Borden was the last Prime Minister to be knighted after the House of Commons indicated its desire for the discontinuation of the granting of any future titles to Canadians in 1919 with the adoption of the Nickle Resolution.In 1919 Borden approved the use of troops to put down the Winnipeg general strike, which was feared to be the result of Bolshevik agitation from the Soviet Union.Sir Robert Borden retired from office in 1920.He was the Chancellor of Queen's University from 1924 to 1930 and also was Chancellor of McGill University from 1918 to 1920 while still Prime Minister.Borden also served as Vice-President of The Champlain Society between 1923 and 1925.He was the Society's first Honorary President between 1925 and 1938.Article Title: Robert Borden Sir Robert Laird Borden, (June 26, 1854 -June 10, 1937) was a Canadian lawyer and politician who served as the eighth Prime Minister of Canada, in office from 1911 to 1920.He is best known for his leadership of Canada during World War I. Borden was born in Grand-Pr\u00e9, Nova Scotia.He worked as a schoolteacher for a period and then served his articles of clerkship at a Halifax law firm.He was called to the bar in 1878, and soon became one of Nova Scotia's most prominent barristers.Borden was elected to the House of Commons of Canada in 1896, representing the Conservative Party.He replaced Charles Tupper as party leader in 1901, and became prime minister after the party's victory at the 1911 federal election.As prime minister, Borden led Canada through World War I and its immediate aftermath.His government passed the \"War Measures Act\", created the Canadian Expeditionary Force, and eventually introduced compulsory military service, which sparked the 1917 conscription crisis.On the home front, it dealt with the consequences of the Halifax Explosion, introduced women's suffrage for federal elections, and used the North-West Mounted Police to break up the 1919 Winnipeg general strike.Sir Robert Borden was the eighth Prime Minister of Canada from 1911 to 1920.Arthur Meighen was the ninth Prime Minister of Canada from 1920 to 1926.Sir Robert Laird Borden was a Canadian lawyer and politician who served as the eighth Prime Minister of Canada, in office from 1911 to 1920.Arthur Meighen was a Canadian lawyer and politician who served as the ninth Prime Minister of Canada, in office from July 1920 to December 1921 and again from June to September 1926.He led the Conservative Party from 1920 to 1926 and from 1941 to 1942.Canadian lawyer and politician who served as the eighth Prime Minister of Canada, in office from 1911 to 1920.Arthur Meighen was a Canadian lawyer and politician who served as the ninth Prime Minister of Canada, in office from July 1920 to December 1921 and again from June to September 1926.\nA Qualitative Examples for Long-Form QA16A.1\nC Long-Form QA Data and Human Feedback Annotation 22 C.1 Data Construction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .22 C.2 Human Feedback Annotation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .22 C.3 Analysis of Collected Fine-Grained Feedback . . . . . . . . . . . . . . . . . . . .23 Article Title: Arthur Meighen Arthur Meighen Arthur Meighen (; 16 June 1874 -5 August 1960) was a Canadian lawyer and politician who served as the ninth Prime Minister of Canada, in office from July 1920 to December 1921 and again from June to September 1926.He led the Conservative Party from 1920 to 1926 and from 1941 to 1942.Meighen was born in rural Perth County, Ontario.He studied mathematics at the University of Toronto, and then went on to Osgoode Hall Law School.After qualifying to practice law, he moved to Portage la Prairie, Manitoba.Meighen entered the House of Commons of Canada in 1908, aged 34, and in 1913 was appointed to the Cabinet of Prime Minister Sir Robert Borden.Under Borden, Meighen served as Solicitor General (1913-1917), Secretary of State for Canada (1917), Minister of Mines (1917; 1919-1920), Minister of the Interior (1917-1920), and Superintendent-General of Indian Affairs (1917-1920).In 1920, Meighen succeeded Borden as Conservative leader and Prime Minister -the first born after Confederation, and the only one from a Manitoba riding.He suffered a heavy defeat in the 1921 election to Mackenzie King and the Liberal Party, but re-entered Parliament through a 1922 by-election and remained as Opposition Leader.\n\n\nTable 7 :\n7\nExamples of errors in LM outputs of SFT, Pref.RLHF, and our FINE-GRAINED RLHF.We mark the C 1 (irrelevance, repetition, or incoherence) and C 2 (incorrect or unverifiable facts) types of errors in the model outputs.\n\n\n1\n\nFine-Grained Reinforcement Learning from Human Feedback (FINE-GRAINED RLHF) Input initial policy model P \u03b8init ; initial value model V \u03c8init ; K reward models R \u03d5 k trained from human feedback; task prompts D; hyperparameters \u03b3, \u03bb, \u03f5, \u03b2 \u25b7 \u00a7 2 1: policy model P \u03b8 \u2190 P \u03b8init , value model V \u03c8 \u2190 V \u03c8init 2: for step = 1, . . ., M do Sample output sequence y n \u223c P \u03b8 (\u2022 | x n ) for each prompt x n \u2208 D b\n3:Sample a batch D b from D4:5: 6:Compute rewards {r n t } |y n | t=1 for each sampled output y n by running R \u03d5 k Compute advantages {A t } |y n | t=1 and value targets {V targ (s t )} |y n | t=1 for each y n with V \u03c8\u25b7 Eq. 17:\nWe use spaCy[15] to segment generated model outputs into sentences. We then split sentences into sub-sentences using a comma or semicolon.\nSince the input for each example is very long, we cannot fit more than one in-context example into the model.\nhttps://github.com/castorini/pyserini\nhttps://www.mturk.com/\nWe see very few \"incoherence\" errors (1%), so the majority of labeled errors are from the other four categories during annotation.\nWe use spaCy[15] to segment generated model outputs into sentences. We then split sentences into sub-sentences using a comma or semicolon.\nNote that training R \u03d5 1 takes shorter time as its input does not contain passages.\nWe manually review 50 mapped passage sentences and find over 90% of them are correctly mapped, which indicates frequent extractive behaviors from P \u03b8 init .\nAcknowledgmentsWe thank Jiacheng Liu for sharing the standard PPO training code, and Yizhong Wang for providing insights during early discussions of the project.We also thank UW TIAL members for participating in our pilot feedback annotation.We extend our thanks to UW NLP members who provided insights or feedback to our project.Lastly, we especially thank all our AMT workers for helping us annotate the high quality feedback data.This research was developed with funding from the Defense Advanced Research Projects Agency (DARPA) under Contract No. FA8650-23-C-7316.This work was also funded in part by the DARPA MCS program through NIWC Pacific (N66001-19-2-4031), NSF IIS-2044660, and ONR N00014-18-1-2826.The views, opinions and/or findings expressed are those of the author and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government.A Qualitative Examples for Long-Form QAA.1 Examples on LM CustomizationAs discussed in \u00a7 4.5, we can modify the weight of each fine-grained reward model during RL training to get LM with different behaviors.Here, we explore three configurations of reward model weights and name them 'short', 'medium', and 'long' based on the LM's average generation length.The 'short' configuration generates concise and short responses, while the 'long' configuration generates detailed and long responses.Table6demonstrates the different behaviors of our customized LMs.Given the same question, each LM generates different amount of auxiliary information in the response.Question:When did the French join revolution on colonists' side?Passages: Article Title: France in the American Revolutionary War French involvement in the American Revolutionary War began in 1775, when France, a rival of the British Empire, secretly shipped supplies to the Continental Army.A Treaty of Alliance in 1778 soon followed, which led to shipments of money and material to the United States.Subsequently, the Spanish Empire and the Dutch Republic also began to send assistance, leaving the British Empire with no allies.France's help is considered a vital and decisive contribution to the United States' victory against the British.As a cost of participation in the war, France accumulated over 1 billion livres in debt.After its defeat in the Seven Years' War in 1763, France lost its vast holdings in North America.Meanwhile, the American colonists and the British government began to fight over whether Parliament in London or the colonial assemblies had primary responsibility for taxation.As part of that conflict, the colonists organized the Boston Tea Party in response to a tax on tea.The British government responded by passing the Intolerable Acts, which included the closing of Boston Harbor and the revocation of Massachusetts's colonial charter.This conflict exacerbated tensions further.The ideological conflict escalated into open warfare in 1775, at which point the American patriots revolted against British rule.France, who had been rebuilding their Navy and other forces, saw this as an opportunity to seriously weaken her perennial enemy.Article Title: France in the American Revolutionary War France bitterly resented its loss in the Seven Years' War and sought revenge.It also wanted to strategically weaken Britain.Following the Declaration of Independence, the American Revolution was well received by both the general population and the aristocracy in France.The Revolution was perceived as the incarnation of the Enlightenment Spirit against the \"English tyranny.\"Benjamin Franklin traveled to France in December 1776 in order to rally the nation's support, and he was welcomed with great enthusiasm.At first, French support was covert: French agents sent the Patriots military aid (predominantly gunpowder) through a company called \"Rodrigue Hortalez et Compagnie\", beginning in the spring of 1776.Estimates place the percentage of French supplied arms to the Americans in the Saratoga campaign at up to 90%.By 1777, over five million \"livres\" of aid had been sent to the American rebels.Motivated by the prospect of glory in battle or animated by the sincere ideals of liberty and republicanism, volunteers like Pierre Charles L'Enfant joined the American army.The most famous was Lafayette, a charming young aristocrat who defied the king's order and enlisted in 1777 at age 20.He became an aide to George Washington and a combat general.More importantly, he solidified a favorable American view of France.Article Title: Treaty of Alliance (1778) On March 17, 1778, four days after a French ambassador informed the British government that France had officially recognized the United States as an independent nation with the signing of the Treaty of Amity and Commerce and the Treaty of Alliance, Great Britain declared war on France, thereby engaging that nation in the American Revolutionary War.French entry into the war would lead to further escalation of the war when Spain entered the fight against Britain as France's ally, after the signing of the Treaty of Aranjuez on April 12, 1779, and again in December 1780 when Britain declared war on the Dutch Republic after seizing a Dutch merchant ship they claimed was carrying contraband to France during the Affair of Fielding and Bylandt.After the signing of the treaty French supplies of arms, ammunition, and uniforms proved vital for the Continental Army, while their actions in the Anglo French War of 1778-83 in the West Indies and elsewhere forced Britain to redeploy troops and naval units away from the North American colonies to secure their holdings.LM outputs with different reward model configurations:Short:French involvement in the American Revolutionary War began in 1775, when France, a rival of the British Empire, secretly shipped supplies to the Continental Army.A Treaty of Alliance in 1778 soon followed, which led to shipments of money and mat\u00e9riel to the United States.Subsequently, the Spanish Empire and the Dutch Republic also began to send assistance, leaving the British Empire with no allies.Medium:French involvement in the American Revolutionary War began in 1775, when France, a rival of the British Empire, secretly shipped supplies to the Continental Army.A Treaty of Alliance in 1778 soon followed, which led to shipments of money and mat\u00e9riel to the United States.Subsequently, the Spanish Empire and the Dutch Republic also began to send assistance, leaving the British Empire with no allies.On March 17, 1778, four days after a French ambassador informed the British government that France had officially recognized the United States as an independent nation, Great Britain declared war on France, thereby engaging that nation in the American Revolutionary War.Long:French involvement in the American Revolutionary War began in 1775, when France, a rival of the British Empire, secretly shipped supplies to the Continental Army.A Treaty of Alliance in 1778 soon followed, which led to shipments of money and mat\u00e9riel to the United States.Subsequently, the Spanish Empire and the Dutch Republic also began to send assistance, leaving the British Empire with no allies.On March 17, 1778, four days after a French ambassador informed the British government that France had officially recognized the United States as an independent nation with the signing of the Treaty of Amity and Commerce and the Treaty of Alliance, Great Britain declared war on France, thereby engaging that nation in the American Revolutionary War.Comparison between our output and ChatGPT outputChatGPT:The Rams moved to St. Louis in 1995 and were known as the St. Louis Rams from 1995 to 2015.FINE-GRAINED RLHFThe Los Angeles Rams are a professional American football team that play in National Football (NFL).The Rams franchise was founded in 1936 as the Cleveland Rams in the short-lived second American Football League before joining the NFL the next year.In 1946, the franchise moved to Los Angeles.The Rams franchise remained in the metro area until 1994, when they moved to St. Louis, and were known as the St. Louis Rams from 1995 to 2015.The Rams franchise returned to Los Angeles in 2016.Question:Who sang the song I'm Coming Out?Passages: Article Title: I'm Coming OutThere's also been a cover by Marcia Hines.It was featured as a cover for the video game \"Karaoke Revolution\" as well.In more recent times, the song was sampled in Keyshia Cole's remake of \"I'm Coming Out\" (featuring Iggy Azalea) for the original soundtrack of the 2014 movie The Other Woman, starring Cameron Diaz, Kate Upton and Leslie Mann.The song is sampled in \"Break Your Heart Right Back\" from Ariana Grande's sophomore album \"My Everything\" (2014).This song was also covered for the soundtrack to the 2016 movie \"Trolls\".In 1997, German comedian Hella von Sinnen sampled \"I'm Coming Out\" for her single \"Mein Coming Out\", using the term \"coming out\" in the context of coming out of the closet, or revealing one's homosexuality.\n. Perspective API. \n\nTraining a helpful and harmless assistant with reinforcement learning from human feedback. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova Dassarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, arXiv:2204.058622022arXiv preprint\n\nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron Mckinnon, arXiv:2212.08073Constitutional ai: Harmlessness from ai feedback. 2022arXiv preprint\n\nLongformer: The long-document transformer. Iz Beltagy, Matthew E Peters, Arman Cohan, arXiv:2004.051502020arXiv preprint\n\nImproving code generation by training with natural language feedback. Angelica Chen, J\u00e9r\u00e9my Scheurer, Tomasz Korbak, Jon Ander Campos, Jun Shern Chan, Kyunghyun Samuel R Bowman, Ethan Cho, Perez, arXiv:2303.167492023arXiv preprint\n\nTowards teachable reasoning systems: Using a dynamic memory of user feedback for continual system improvement. Oyvind Bhavana Dalvi Mishra, Peter Tafjord, Clark, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDecember 2022\n\nPlug and play language models: A simple approach to controlled text generation. Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, Rosanne Liu, International Conference on Learning Representations. 2020\n\nIs GPT-3 text indistinguishable from human text? scarecrow: A framework for scrutinizing machine text. Yao Dou, Maxwell Forbes, Rik Koncel-Kedziorski, Noah A Smith, Yejin Choi, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational LinguisticsMay 20221\n\nChain-of-thought hub: A continuous effort to measure large language models' reasoning performance. Yao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, Tushar Khot, arXiv:2305.173062023arXiv preprint\n\nThe capacity for moral self-correction in large language models. Deep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas Liao, Kamil\u0117 Luko\u0161i\u016bt\u0117, Anna Chen, Anna Goldie, Azalia Mirhoseini, Catherine Olsson, Danny Hernandez, arXiv:2302.074592023arXiv preprint\n\nEnabling large language models to generate text with citations. Tianyu Gao, Howard Yen, Jiatong Yu, Danqi Chen, arXiv:2305.146272023arXiv preprint\n\nReal-ToxicityPrompts: Evaluating neural toxic degeneration in language models. Suchin Samuel Gehman, Maarten Gururangan, Yejin Sap, Noah A Choi, Smith, Findings of the Association for Computational Linguistics: EMNLP 2020. Association for Computational LinguisticsNovember 2020\n\nImproving alignment of dialogue agents via targeted human judgements. Amelia Glaese, Nat Mcaleese, Maja Tr\u0119bacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, arXiv:2209.143752022arXiv preprint\n\nLearning from dialogue after deployment: Feed yourself, chatbot!. Braden Hancock, Antoine Bordes, Pierre-Emmanuel Mazare, Jason Weston, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsJuly 2019\n\nspaCy: Industrialstrength Natural Language Processing in Python. Matthew Honnibal, Ines Montani, Sofie Van Landeghem, Adriane Boyd, 2020\n\nPretraining language models with human preferences. Tomasz Korbak, Kejian Shi, Angelica Chen, Rasika Bhalerao, Christopher L Buckley, Jason Phang, Ethan Samuel R Bowman, Perez, arXiv:2302.085822023arXiv preprint\n\nGeDi: Generative discriminator guided sequence generation. Ben Krause, Akhilesh Deepak Gotmare, Bryan Mccann, Nitish Shirish Keskar, Shafiq Joty, Richard Socher, Nazneen Fatema, Rajani , Findings of the Association for Computational Linguistics: EMNLP 2021. Punta Cana, Dominican RepublicAssociation for Computational LinguisticsNovember 2021\n\nLongEval: Guidelines for human evaluation of faithfulness in long-form summarization. Kalpesh Krishna, Erin Bransom, Bailey Kuehl, Mohit Iyyer, Pradeep Dasigi, Arman Cohan, Kyle Lo, Proceedings of the 17th Conference of the European Chapter. the 17th Conference of the European ChapterDubrovnik, CroatiaAssociation for Computational LinguisticsMay 2023\n\nAcute-eval: Improved dialogue evaluation with optimized questions and multi-turn comparisons. Margaret Li, Jason Weston, Stephen Roller, arXiv:1909.030872019arXiv preprint\n\nROUGE: A package for automatic evaluation of summaries. Chin-Yew Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational LinguisticsJuly 2004\n\nDExperts: Decoding-time controlled text generation with experts and anti-experts. Alisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula, Noah A Smith, Yejin Choi, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingAssociation for Computational LinguisticsAugust 2021\n\nQuark: Controllable text generation with reinforced unlearning. Ximing Lu, Sean Welleck, Liwei Jiang, Jack Hessel, Lianhui Qin, Peter West, Prithviraj Ammanabrolu, Yejin Choi, Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS). 2022\n\nMemory-assisted prompt editing to improve GPT-3 after deployment. Aman Madaan, Niket Tandon, Peter Clark, Yiming Yang, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDecember 2022\n\nTeaching language models to support answers with verified quotes. Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick, Mia Glaese, Susannah Young, Lucy Campbell-Gillingham, Geoffrey Irving, arXiv:2203.111472022arXiv preprint\n\nFactscore: Fine-grained atomic evaluation of factual precision in long form text generation. Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-Tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, Hannaneh Hajishirzi, arXiv:2305.142512023arXiv preprint\n\nAmbigQA: Answering ambiguous open-domain questions. Sewon Min, Julian Michael, Hannaneh Hajishirzi, Luke Zettlemoyer, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Association for Computational LinguisticsNovember 2020\n\nWebgpt: Browser-assisted question-answering with human feedback. Reiichiro Nakano, Jacob Hilton, S Arun Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, ArXiv, abs/2112.09332Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman2021\n\n. OpenAI. Gpt-4 technical report. 2023\n\nTraining language models to follow instructions with human feedback. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, ; , Ryan Lowe, Jan Leike,. 2022\n\nRefiner: Reasoning feedback on intermediate representations. Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, Boi Faltings, 2023\n\nLanguage models are unsupervised multitask learners. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, 2019\n\nExploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, The Journal of Machine Learning Research. 2112020\n\nIs reinforcement learning (not) for natural language processing: Benchmarks, baselines, and building blocks for natural language policy optimization. Rajkumar Ramamurthy, * , Prithviraj Ammanabrolu, * , Kiant\u00e9 Brantley, Jack Hessel, Rafet Sifa, Christian Bauckhage, Hannaneh Hajishirzi, Yejin Choi, International Conference on Learning Representations (ICLR). 2023\n\nTraining language models with natural language feedback. J\u00e9r\u00e9my Scheurer, Jon Ander Campos, Jun Shern Chan, Angelica Chen, Kyunghyun Cho, Ethan Perez, arXiv:2204.141462022arXiv preprint\n\nTraining language models with language feedback at scale. J\u00e9r\u00e9my Scheurer, Jon Ander Campos, Tomasz Korbak, Jun Shern Chan, Angelica Chen, Kyunghyun Cho, Ethan Perez, 2023\n\nHighdimensional continuous control using generalized advantage estimation. John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, Pieter Abbeel, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2016\n\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint\n\nWhen life gives you lemons, make cherryade: Converting feedback from bad responses into good labels. Weiyan Shi, Emily Dinan, Kurt Shuster, Jason Weston, Jing Xu, arXiv:2210.158932022arXiv preprint\n\nASQA: Factoid questions meet long-form answers. Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, Ming-Wei Chang, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDecember 2022\n\nLearning to summarize from human feedback. Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M Ziegler, Ryan J Lowe, Chelsea Voss, Alec Radford, Dario Amodei, Paul Christiano, ArXiv, abs/2009.013252020\n\nJeff Wu, Long Ouyang, M Daniel, Nisan Ziegler, Ryan Stiennon, Jan Lowe, Paul Leike, Christiano, arXiv:2109.10862Recursively summarizing books with human feedback. 2021arXiv preprint\n\nLearning new skills after deployment: Improving open-domain internet-driven dialogue with human feedback. Jing Xu, Megan Ung, Mojtaba Komeili, Kushal Arora, Y-Lan Boureau, Jason Weston, arXiv:2208.032702022arXiv preprint\n\nInstructscore: Towards explainable text generation evaluation with automatic feedback. Wenda Xu, Danqing Wang, Liangming Pan, Zhenqiao Song, Markus Freitag, William Yang, Wang , Lei Li, arXiv:2305.142822023arXiv preprint\n\nRrhf: Rank responses to align language models with human feedback without tears. Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, Fei Huang, 2023\n\nAutomatic evaluation of attribution by large language models. Xiang Yue, Boshi Wang, Kai Zhang, Ziru Chen, Yu Su, Huan Sun, arXiv:2305.063112023arXiv preprint\n\nFine-tuning language models from human preferences. M Daniel, Nisan Ziegler, Jeff Stiennon, Tom B Wu, Alec Brown, Dario Radford, Paul Amodei, Geoffrey Christiano, Irving, ArXiv, abs/1909.085932019\n", "annotations": {"author": "[{\"end\":116,\"start\":80},{\"end\":153,\"start\":117},{\"end\":165,\"start\":154},{\"end\":251,\"start\":166},{\"end\":298,\"start\":252},{\"end\":322,\"start\":299},{\"end\":336,\"start\":323},{\"end\":379,\"start\":337},{\"end\":400,\"start\":380},{\"end\":448,\"start\":401}]", "publisher": null, "author_last_name": "[{\"end\":88,\"start\":86},{\"end\":125,\"start\":123},{\"end\":164,\"start\":161},{\"end\":177,\"start\":172},{\"end\":262,\"start\":258},{\"end\":321,\"start\":310},{\"end\":335,\"start\":330},{\"end\":351,\"start\":342},{\"end\":399,\"start\":389}]", "author_first_name": "[{\"end\":85,\"start\":80},{\"end\":122,\"start\":117},{\"end\":160,\"start\":154},{\"end\":171,\"start\":166},{\"end\":257,\"start\":252},{\"end\":309,\"start\":299},{\"end\":327,\"start\":323},{\"end\":329,\"start\":328},{\"end\":341,\"start\":337},{\"end\":388,\"start\":380}]", "author_affiliation": "[{\"end\":115,\"start\":90},{\"end\":152,\"start\":127},{\"end\":204,\"start\":179},{\"end\":250,\"start\":206},{\"end\":297,\"start\":264},{\"end\":378,\"start\":353},{\"end\":447,\"start\":402}]", "title": "[{\"end\":77,\"start\":1},{\"end\":525,\"start\":449}]", "venue": null, "abstract": "[{\"end\":2042,\"start\":559}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2176,\"start\":2172},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2179,\"start\":2176},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2181,\"start\":2179},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2430,\"start\":2426},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2432,\"start\":2430},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2434,\"start\":2432},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":2437,\"start\":2434},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2440,\"start\":2437},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":2523,\"start\":2519},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":4086,\"start\":4082},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4254,\"start\":4250},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":4303,\"start\":4299},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4434,\"start\":4431},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6420,\"start\":6416},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":7582,\"start\":7578},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":7972,\"start\":7968},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":9828,\"start\":9824},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":10976,\"start\":10972},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11295,\"start\":11292},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12314,\"start\":12310},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":12317,\"start\":12314},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12937,\"start\":12933},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":12955,\"start\":12951},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12984,\"start\":12980},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":12987,\"start\":12984},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":15470,\"start\":15466},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":15540,\"start\":15536},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":16309,\"start\":16305},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":16512,\"start\":16508},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":18692,\"start\":18688},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":19205,\"start\":19201},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":20044,\"start\":20041},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":22036,\"start\":22032},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":22126,\"start\":22122},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":23360,\"start\":23356},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":23473,\"start\":23469},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":24095,\"start\":24091},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":30899,\"start\":30895},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":30902,\"start\":30899},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":30905,\"start\":30902},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":31141,\"start\":31137},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":31169,\"start\":31165},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":31194,\"start\":31190},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":31197,\"start\":31194},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":31226,\"start\":31223},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":31228,\"start\":31226},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":31231,\"start\":31228},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":31234,\"start\":31231},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":31457,\"start\":31453},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":31824,\"start\":31820},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":32030,\"start\":32026},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":32186,\"start\":32182},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":32189,\"start\":32186},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":32192,\"start\":32189},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":32435,\"start\":32431},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":32437,\"start\":32435},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":32593,\"start\":32590},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":32596,\"start\":32593},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":32598,\"start\":32596},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":32875,\"start\":32871},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":32877,\"start\":32875},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":32950,\"start\":32946},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":46038,\"start\":46034},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":46198,\"start\":46194},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":46284,\"start\":46280},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":47144,\"start\":47140},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":47150,\"start\":47146},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":49484,\"start\":49480},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":51921,\"start\":51917},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":56722,\"start\":56718},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":56759,\"start\":56755},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":57431,\"start\":57427},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":57622,\"start\":57618},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":70293,\"start\":70289},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":70734,\"start\":70730}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":58115,\"start\":58022},{\"attributes\":{\"id\":\"fig_1\"},\"end\":58569,\"start\":58116},{\"attributes\":{\"id\":\"fig_2\"},\"end\":58713,\"start\":58570},{\"attributes\":{\"id\":\"fig_3\"},\"end\":59336,\"start\":58714},{\"attributes\":{\"id\":\"fig_4\"},\"end\":59467,\"start\":59337},{\"attributes\":{\"id\":\"fig_5\"},\"end\":60106,\"start\":59468},{\"attributes\":{\"id\":\"fig_6\"},\"end\":60175,\"start\":60107},{\"attributes\":{\"id\":\"fig_7\"},\"end\":60263,\"start\":60176},{\"attributes\":{\"id\":\"fig_8\"},\"end\":60376,\"start\":60264},{\"attributes\":{\"id\":\"fig_9\"},\"end\":60448,\"start\":60377},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":60727,\"start\":60449},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":61505,\"start\":60728},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":62774,\"start\":61506},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":63171,\"start\":62775},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":63305,\"start\":63172},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":63479,\"start\":63306},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":69414,\"start\":63480},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":69644,\"start\":69415},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":70276,\"start\":69645}]", "paragraph": "[{\"end\":2885,\"start\":2058},{\"end\":3263,\"start\":2887},{\"end\":3460,\"start\":3265},{\"end\":4177,\"start\":3462},{\"end\":4818,\"start\":4179},{\"end\":5811,\"start\":4820},{\"end\":6250,\"start\":5833},{\"end\":7517,\"start\":6252},{\"end\":7973,\"start\":7519},{\"end\":8326,\"start\":8043},{\"end\":8634,\"start\":8393},{\"end\":8984,\"start\":8636},{\"end\":9352,\"start\":8986},{\"end\":10125,\"start\":9354},{\"end\":10517,\"start\":10229},{\"end\":11228,\"start\":10544},{\"end\":11727,\"start\":11230},{\"end\":12247,\"start\":11729},{\"end\":12750,\"start\":12263},{\"end\":13736,\"start\":12752},{\"end\":14290,\"start\":13738},{\"end\":14335,\"start\":14292},{\"end\":15385,\"start\":14337},{\"end\":16212,\"start\":15387},{\"end\":17122,\"start\":16214},{\"end\":18132,\"start\":17124},{\"end\":18619,\"start\":18134},{\"end\":19036,\"start\":18621},{\"end\":19701,\"start\":19038},{\"end\":20152,\"start\":19732},{\"end\":21510,\"start\":20154},{\"end\":22127,\"start\":21543},{\"end\":22492,\"start\":22196},{\"end\":22778,\"start\":22494},{\"end\":23406,\"start\":22801},{\"end\":24418,\"start\":23408},{\"end\":24597,\"start\":24435},{\"end\":24959,\"start\":24599},{\"end\":25151,\"start\":24961},{\"end\":25731,\"start\":25195},{\"end\":27285,\"start\":25733},{\"end\":28299,\"start\":27298},{\"end\":29079,\"start\":28301},{\"end\":29653,\"start\":29081},{\"end\":30015,\"start\":29655},{\"end\":30107,\"start\":30017},{\"end\":30823,\"start\":30109},{\"end\":31907,\"start\":30840},{\"end\":33015,\"start\":31909},{\"end\":33495,\"start\":33030},{\"end\":33925,\"start\":33515},{\"end\":34246,\"start\":33927},{\"end\":34635,\"start\":34248},{\"end\":35189,\"start\":34667},{\"end\":35490,\"start\":35191},{\"end\":35849,\"start\":35492},{\"end\":36299,\"start\":35851},{\"end\":36881,\"start\":36314},{\"end\":36955,\"start\":36908},{\"end\":38093,\"start\":36957},{\"end\":38553,\"start\":38113},{\"end\":39009,\"start\":38569},{\"end\":39386,\"start\":39031},{\"end\":39607,\"start\":39388},{\"end\":39894,\"start\":39649},{\"end\":39959,\"start\":39908},{\"end\":41104,\"start\":39961},{\"end\":42387,\"start\":41106},{\"end\":42455,\"start\":42389},{\"end\":42921,\"start\":42478},{\"end\":42950,\"start\":42923},{\"end\":43860,\"start\":42952},{\"end\":43922,\"start\":43862},{\"end\":43952,\"start\":43924},{\"end\":44195,\"start\":43954},{\"end\":44232,\"start\":44197},{\"end\":44573,\"start\":44234},{\"end\":44635,\"start\":44575},{\"end\":44864,\"start\":44637},{\"end\":45261,\"start\":44906},{\"end\":45488,\"start\":45353},{\"end\":45537,\"start\":45490},{\"end\":45613,\"start\":45539},{\"end\":45808,\"start\":45746},{\"end\":46368,\"start\":45931},{\"end\":47675,\"start\":46370},{\"end\":48151,\"start\":47677},{\"end\":49125,\"start\":48153},{\"end\":49401,\"start\":49127},{\"end\":51327,\"start\":49403},{\"end\":51854,\"start\":51329},{\"end\":52408,\"start\":51856},{\"end\":52722,\"start\":52410},{\"end\":53311,\"start\":52774},{\"end\":53879,\"start\":53313},{\"end\":57415,\"start\":53928},{\"end\":57727,\"start\":57417},{\"end\":58021,\"start\":57729},{\"end\":58114,\"start\":58036},{\"end\":58568,\"start\":58130},{\"end\":58712,\"start\":58584},{\"end\":59335,\"start\":58717},{\"end\":59466,\"start\":59351},{\"end\":60105,\"start\":59471},{\"end\":60174,\"start\":60121},{\"end\":60262,\"start\":60190},{\"end\":60375,\"start\":60278},{\"end\":60447,\"start\":60391},{\"end\":60507,\"start\":60462},{\"end\":61504,\"start\":60731},{\"end\":62773,\"start\":61519},{\"end\":62837,\"start\":62788},{\"end\":63304,\"start\":63185},{\"end\":63478,\"start\":63319},{\"end\":67787,\"start\":63495},{\"end\":69413,\"start\":67833},{\"end\":69643,\"start\":69428},{\"end\":70048,\"start\":69649}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8042,\"start\":7974},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8392,\"start\":8327},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10228,\"start\":10126},{\"attributes\":{\"id\":\"formula_3\"},\"end\":21542,\"start\":21511},{\"attributes\":{\"id\":\"formula_4\"},\"end\":22195,\"start\":22128},{\"attributes\":{\"id\":\"formula_5\"},\"end\":45745,\"start\":45614},{\"attributes\":{\"id\":\"formula_6\"},\"end\":45905,\"start\":45809}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":13501,\"start\":13500},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":24515,\"start\":24514},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":24968,\"start\":24967},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":25763,\"start\":25762},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":27391,\"start\":27390},{\"end\":39395,\"start\":39394},{\"end\":39809,\"start\":39808},{\"end\":44644,\"start\":44643},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":44991,\"start\":44989}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2056,\"start\":2044},{\"attributes\":{\"n\":\"2\"},\"end\":5831,\"start\":5814},{\"attributes\":{\"n\":\"3\"},\"end\":10542,\"start\":10520},{\"attributes\":{\"n\":\"3.1\"},\"end\":12261,\"start\":12250},{\"attributes\":{\"n\":\"4.2\"},\"end\":19730,\"start\":19704},{\"attributes\":{\"n\":\"4.3\"},\"end\":22799,\"start\":22781},{\"attributes\":{\"n\":\"4.4\"},\"end\":24433,\"start\":24421},{\"attributes\":{\"n\":\"4.5\"},\"end\":25193,\"start\":25154},{\"attributes\":{\"n\":\"4.6\"},\"end\":27296,\"start\":27288},{\"attributes\":{\"n\":\"5\"},\"end\":30838,\"start\":30826},{\"attributes\":{\"n\":\"6\"},\"end\":33028,\"start\":33018},{\"attributes\":{\"n\":\"6.1\"},\"end\":33513,\"start\":33498},{\"attributes\":{\"n\":\"6.2\"},\"end\":34665,\"start\":34638},{\"attributes\":{\"n\":\"7\"},\"end\":36312,\"start\":36302},{\"end\":36894,\"start\":36884},{\"end\":36906,\"start\":36897},{\"end\":38111,\"start\":38096},{\"end\":38567,\"start\":38556},{\"end\":39029,\"start\":39012},{\"end\":39647,\"start\":39610},{\"end\":39906,\"start\":39897},{\"end\":42476,\"start\":42458},{\"end\":44904,\"start\":44867},{\"end\":45351,\"start\":45264},{\"attributes\":{\"n\":\"2\"},\"end\":45929,\"start\":45907},{\"end\":52772,\"start\":52725},{\"end\":53926,\"start\":53882},{\"end\":58033,\"start\":58023},{\"end\":58128,\"start\":58117},{\"end\":58581,\"start\":58571},{\"end\":59348,\"start\":59338},{\"end\":60118,\"start\":60108},{\"end\":60187,\"start\":60177},{\"end\":60275,\"start\":60265},{\"end\":60388,\"start\":60378},{\"end\":60459,\"start\":60450},{\"end\":61516,\"start\":61507},{\"end\":62785,\"start\":62776},{\"end\":63182,\"start\":63173},{\"end\":63316,\"start\":63307},{\"end\":63492,\"start\":63481},{\"end\":69425,\"start\":69416},{\"end\":69647,\"start\":69646}]", "table": "[{\"end\":60727,\"start\":60508},{\"end\":63171,\"start\":62838},{\"end\":67832,\"start\":67788},{\"end\":70276,\"start\":70049}]", "figure_caption": "[{\"end\":58115,\"start\":58035},{\"end\":58569,\"start\":58129},{\"end\":58713,\"start\":58583},{\"end\":59336,\"start\":58716},{\"end\":59467,\"start\":59350},{\"end\":60106,\"start\":59470},{\"end\":60175,\"start\":60120},{\"end\":60263,\"start\":60189},{\"end\":60376,\"start\":60277},{\"end\":60448,\"start\":60390},{\"end\":60508,\"start\":60461},{\"end\":61505,\"start\":60730},{\"end\":62838,\"start\":62787},{\"end\":63305,\"start\":63184},{\"end\":63479,\"start\":63318},{\"end\":67788,\"start\":63494},{\"end\":69644,\"start\":69427},{\"end\":70049,\"start\":69648}]", "figure_ref": "[{\"end\":2895,\"start\":2894},{\"end\":3815,\"start\":3814},{\"end\":5372,\"start\":5371},{\"end\":8836,\"start\":8832},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13773,\"start\":13772},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":24443,\"start\":24442},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":24678,\"start\":24677},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":26695,\"start\":26694},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":50021,\"start\":50020},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":50873,\"start\":50872},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":51264,\"start\":51263},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":51326,\"start\":51325},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":52311,\"start\":52310}]", "bib_author_first_name": "[{\"end\":80200,\"start\":80194},{\"end\":80210,\"start\":80206},{\"end\":80223,\"start\":80218},{\"end\":80239,\"start\":80233},{\"end\":80252,\"start\":80248},{\"end\":80263,\"start\":80259},{\"end\":80278,\"start\":80274},{\"end\":80295,\"start\":80286},{\"end\":80306,\"start\":80302},{\"end\":80319,\"start\":80316},{\"end\":80372,\"start\":80366},{\"end\":80384,\"start\":80378},{\"end\":80403,\"start\":80395},{\"end\":80417,\"start\":80411},{\"end\":80433,\"start\":80426},{\"end\":80447,\"start\":80443},{\"end\":80459,\"start\":80455},{\"end\":80470,\"start\":80466},{\"end\":80485,\"start\":80479},{\"end\":80505,\"start\":80498},{\"end\":80647,\"start\":80645},{\"end\":80664,\"start\":80657},{\"end\":80666,\"start\":80665},{\"end\":80680,\"start\":80675},{\"end\":80802,\"start\":80794},{\"end\":80815,\"start\":80809},{\"end\":80832,\"start\":80826},{\"end\":80844,\"start\":80841},{\"end\":80850,\"start\":80845},{\"end\":80862,\"start\":80859},{\"end\":80884,\"start\":80875},{\"end\":80907,\"start\":80902},{\"end\":81073,\"start\":81067},{\"end\":81101,\"start\":81096},{\"end\":81451,\"start\":81444},{\"end\":81469,\"start\":81463},{\"end\":81485,\"start\":81479},{\"end\":81495,\"start\":81491},{\"end\":81506,\"start\":81502},{\"end\":81519,\"start\":81514},{\"end\":81533,\"start\":81528},{\"end\":81551,\"start\":81544},{\"end\":81723,\"start\":81720},{\"end\":81736,\"start\":81729},{\"end\":81748,\"start\":81745},{\"end\":81772,\"start\":81768},{\"end\":81774,\"start\":81773},{\"end\":81787,\"start\":81782},{\"end\":82137,\"start\":82134},{\"end\":82146,\"start\":82142},{\"end\":82157,\"start\":82151},{\"end\":82169,\"start\":82164},{\"end\":82178,\"start\":82175},{\"end\":82191,\"start\":82185},{\"end\":82303,\"start\":82299},{\"end\":82319,\"start\":82313},{\"end\":82336,\"start\":82328},{\"end\":82353,\"start\":82347},{\"end\":82366,\"start\":82360},{\"end\":82382,\"start\":82378},{\"end\":82393,\"start\":82389},{\"end\":82408,\"start\":82402},{\"end\":82430,\"start\":82421},{\"end\":82444,\"start\":82439},{\"end\":82562,\"start\":82556},{\"end\":82574,\"start\":82568},{\"end\":82587,\"start\":82580},{\"end\":82597,\"start\":82592},{\"end\":82725,\"start\":82719},{\"end\":82748,\"start\":82741},{\"end\":82766,\"start\":82761},{\"end\":82776,\"start\":82772},{\"end\":82778,\"start\":82777},{\"end\":82995,\"start\":82989},{\"end\":83007,\"start\":83004},{\"end\":83022,\"start\":83018},{\"end\":83036,\"start\":83032},{\"end\":83052,\"start\":83048},{\"end\":83065,\"start\":83061},{\"end\":83082,\"start\":83074},{\"end\":83094,\"start\":83089},{\"end\":83112,\"start\":83106},{\"end\":83129,\"start\":83123},{\"end\":83247,\"start\":83241},{\"end\":83264,\"start\":83257},{\"end\":83288,\"start\":83273},{\"end\":83302,\"start\":83297},{\"end\":83611,\"start\":83604},{\"end\":83626,\"start\":83622},{\"end\":83641,\"start\":83636},{\"end\":83664,\"start\":83657},{\"end\":83735,\"start\":83729},{\"end\":83750,\"start\":83744},{\"end\":83764,\"start\":83756},{\"end\":83777,\"start\":83771},{\"end\":83799,\"start\":83788},{\"end\":83801,\"start\":83800},{\"end\":83816,\"start\":83811},{\"end\":83829,\"start\":83824},{\"end\":83952,\"start\":83949},{\"end\":83969,\"start\":83961},{\"end\":83991,\"start\":83986},{\"end\":84006,\"start\":84000},{\"end\":84029,\"start\":84023},{\"end\":84043,\"start\":84036},{\"end\":84059,\"start\":84052},{\"end\":84074,\"start\":84068},{\"end\":84327,\"start\":84320},{\"end\":84341,\"start\":84337},{\"end\":84357,\"start\":84351},{\"end\":84370,\"start\":84365},{\"end\":84385,\"start\":84378},{\"end\":84399,\"start\":84394},{\"end\":84411,\"start\":84407},{\"end\":84690,\"start\":84682},{\"end\":84700,\"start\":84695},{\"end\":84716,\"start\":84709},{\"end\":84825,\"start\":84817},{\"end\":85019,\"start\":85014},{\"end\":85032,\"start\":85025},{\"end\":85044,\"start\":85038},{\"end\":85055,\"start\":85049},{\"end\":85076,\"start\":85069},{\"end\":85094,\"start\":85090},{\"end\":85096,\"start\":85095},{\"end\":85109,\"start\":85104},{\"end\":85564,\"start\":85558},{\"end\":85573,\"start\":85569},{\"end\":85588,\"start\":85583},{\"end\":85600,\"start\":85596},{\"end\":85616,\"start\":85609},{\"end\":85627,\"start\":85622},{\"end\":85644,\"start\":85634},{\"end\":85663,\"start\":85658},{\"end\":85822,\"start\":85818},{\"end\":85836,\"start\":85831},{\"end\":85850,\"start\":85845},{\"end\":85864,\"start\":85858},{\"end\":86188,\"start\":86183},{\"end\":86201,\"start\":86197},{\"end\":86219,\"start\":86211},{\"end\":86233,\"start\":86229},{\"end\":86252,\"start\":86245},{\"end\":86265,\"start\":86259},{\"end\":86279,\"start\":86276},{\"end\":86296,\"start\":86288},{\"end\":86308,\"start\":86304},{\"end\":86338,\"start\":86330},{\"end\":86481,\"start\":86476},{\"end\":86494,\"start\":86487},{\"end\":86509,\"start\":86504},{\"end\":86519,\"start\":86515},{\"end\":86534,\"start\":86527},{\"end\":86544,\"start\":86540},{\"end\":86559,\"start\":86554},{\"end\":86571,\"start\":86567},{\"end\":86593,\"start\":86585},{\"end\":86699,\"start\":86694},{\"end\":86711,\"start\":86705},{\"end\":86729,\"start\":86721},{\"end\":86746,\"start\":86742},{\"end\":87065,\"start\":87056},{\"end\":87079,\"start\":87074},{\"end\":87089,\"start\":87088},{\"end\":87094,\"start\":87090},{\"end\":87107,\"start\":87103},{\"end\":87116,\"start\":87112},{\"end\":87134,\"start\":87125},{\"end\":87151,\"start\":87140},{\"end\":87167,\"start\":87159},{\"end\":87180,\"start\":87174},{\"end\":87198,\"start\":87191},{\"end\":87211,\"start\":87209},{\"end\":87223,\"start\":87219},{\"end\":87467,\"start\":87463},{\"end\":87480,\"start\":87476},{\"end\":87487,\"start\":87485},{\"end\":87500,\"start\":87495},{\"end\":87517,\"start\":87510},{\"end\":87519,\"start\":87518},{\"end\":87538,\"start\":87532},{\"end\":87553,\"start\":87548},{\"end\":87569,\"start\":87561},{\"end\":87587,\"start\":87579},{\"end\":87599,\"start\":87595},{\"end\":87609,\"start\":87605},{\"end\":87625,\"start\":87620},{\"end\":87640,\"start\":87634},{\"end\":87653,\"start\":87649},{\"end\":87668,\"start\":87662},{\"end\":87683,\"start\":87677},{\"end\":87697,\"start\":87692},{\"end\":87712,\"start\":87708},{\"end\":87726,\"start\":87725},{\"end\":87733,\"start\":87729},{\"end\":87825,\"start\":87819},{\"end\":87836,\"start\":87832},{\"end\":87856,\"start\":87850},{\"end\":87873,\"start\":87866},{\"end\":87889,\"start\":87882},{\"end\":87906,\"start\":87900},{\"end\":87916,\"start\":87913},{\"end\":87990,\"start\":87986},{\"end\":88004,\"start\":88000},{\"end\":88014,\"start\":88009},{\"end\":88027,\"start\":88022},{\"end\":88039,\"start\":88034},{\"end\":88052,\"start\":88048},{\"end\":88158,\"start\":88153},{\"end\":88171,\"start\":88167},{\"end\":88185,\"start\":88181},{\"end\":88204,\"start\":88195},{\"end\":88216,\"start\":88210},{\"end\":88232,\"start\":88225},{\"end\":88246,\"start\":88241},{\"end\":88256,\"start\":88253},{\"end\":88266,\"start\":88261},{\"end\":88268,\"start\":88267},{\"end\":88483,\"start\":88475},{\"end\":88497,\"start\":88496},{\"end\":88510,\"start\":88500},{\"end\":88525,\"start\":88524},{\"end\":88534,\"start\":88528},{\"end\":88549,\"start\":88545},{\"end\":88563,\"start\":88558},{\"end\":88579,\"start\":88570},{\"end\":88599,\"start\":88591},{\"end\":88617,\"start\":88612},{\"end\":88754,\"start\":88748},{\"end\":88768,\"start\":88765},{\"end\":88774,\"start\":88769},{\"end\":88786,\"start\":88783},{\"end\":88807,\"start\":88799},{\"end\":88823,\"start\":88814},{\"end\":88834,\"start\":88829},{\"end\":88942,\"start\":88936},{\"end\":88956,\"start\":88953},{\"end\":88962,\"start\":88957},{\"end\":88977,\"start\":88971},{\"end\":88989,\"start\":88986},{\"end\":89010,\"start\":89002},{\"end\":89026,\"start\":89017},{\"end\":89037,\"start\":89032},{\"end\":89130,\"start\":89126},{\"end\":89148,\"start\":89141},{\"end\":89163,\"start\":89157},{\"end\":89179,\"start\":89172},{\"end\":89194,\"start\":89188},{\"end\":89356,\"start\":89352},{\"end\":89372,\"start\":89367},{\"end\":89389,\"start\":89381},{\"end\":89404,\"start\":89400},{\"end\":89418,\"start\":89414},{\"end\":89611,\"start\":89605},{\"end\":89622,\"start\":89617},{\"end\":89634,\"start\":89630},{\"end\":89649,\"start\":89644},{\"end\":89662,\"start\":89658},{\"end\":89755,\"start\":89751},{\"end\":89768,\"start\":89766},{\"end\":89781,\"start\":89775},{\"end\":89799,\"start\":89791},{\"end\":90101,\"start\":90096},{\"end\":90116,\"start\":90112},{\"end\":90129,\"start\":90125},{\"end\":90140,\"start\":90134},{\"end\":90142,\"start\":90141},{\"end\":90156,\"start\":90152},{\"end\":90158,\"start\":90157},{\"end\":90172,\"start\":90165},{\"end\":90183,\"start\":90179},{\"end\":90198,\"start\":90193},{\"end\":90211,\"start\":90207},{\"end\":90255,\"start\":90251},{\"end\":90264,\"start\":90260},{\"end\":90274,\"start\":90273},{\"end\":90288,\"start\":90283},{\"end\":90302,\"start\":90298},{\"end\":90316,\"start\":90313},{\"end\":90327,\"start\":90323},{\"end\":90544,\"start\":90540},{\"end\":90554,\"start\":90549},{\"end\":90567,\"start\":90560},{\"end\":90583,\"start\":90577},{\"end\":90596,\"start\":90591},{\"end\":90611,\"start\":90606},{\"end\":90748,\"start\":90743},{\"end\":90760,\"start\":90753},{\"end\":90776,\"start\":90767},{\"end\":90790,\"start\":90782},{\"end\":90803,\"start\":90797},{\"end\":90820,\"start\":90813},{\"end\":90831,\"start\":90827},{\"end\":90837,\"start\":90834},{\"end\":90964,\"start\":90959},{\"end\":90977,\"start\":90971},{\"end\":90991,\"start\":90984},{\"end\":91000,\"start\":90997},{\"end\":91015,\"start\":91007},{\"end\":91026,\"start\":91023},{\"end\":91107,\"start\":91102},{\"end\":91118,\"start\":91113},{\"end\":91128,\"start\":91125},{\"end\":91140,\"start\":91136},{\"end\":91149,\"start\":91147},{\"end\":91158,\"start\":91154},{\"end\":91253,\"start\":91252},{\"end\":91267,\"start\":91262},{\"end\":91281,\"start\":91277},{\"end\":91295,\"start\":91292},{\"end\":91297,\"start\":91296},{\"end\":91306,\"start\":91302},{\"end\":91319,\"start\":91314},{\"end\":91333,\"start\":91329},{\"end\":91350,\"start\":91342}]", "bib_author_last_name": "[{\"end\":80204,\"start\":80201},{\"end\":80216,\"start\":80211},{\"end\":80231,\"start\":80224},{\"end\":80246,\"start\":80240},{\"end\":80257,\"start\":80253},{\"end\":80272,\"start\":80264},{\"end\":80284,\"start\":80279},{\"end\":80300,\"start\":80296},{\"end\":80314,\"start\":80307},{\"end\":80328,\"start\":80320},{\"end\":80376,\"start\":80373},{\"end\":80393,\"start\":80385},{\"end\":80409,\"start\":80404},{\"end\":80424,\"start\":80418},{\"end\":80441,\"start\":80434},{\"end\":80453,\"start\":80448},{\"end\":80464,\"start\":80460},{\"end\":80477,\"start\":80471},{\"end\":80496,\"start\":80486},{\"end\":80514,\"start\":80506},{\"end\":80655,\"start\":80648},{\"end\":80673,\"start\":80667},{\"end\":80686,\"start\":80681},{\"end\":80807,\"start\":80803},{\"end\":80824,\"start\":80816},{\"end\":80839,\"start\":80833},{\"end\":80857,\"start\":80851},{\"end\":80873,\"start\":80863},{\"end\":80900,\"start\":80885},{\"end\":80911,\"start\":80908},{\"end\":80918,\"start\":80913},{\"end\":81094,\"start\":81074},{\"end\":81109,\"start\":81102},{\"end\":81116,\"start\":81111},{\"end\":81461,\"start\":81452},{\"end\":81477,\"start\":81470},{\"end\":81489,\"start\":81486},{\"end\":81500,\"start\":81496},{\"end\":81512,\"start\":81507},{\"end\":81526,\"start\":81520},{\"end\":81542,\"start\":81534},{\"end\":81555,\"start\":81552},{\"end\":81727,\"start\":81724},{\"end\":81743,\"start\":81737},{\"end\":81766,\"start\":81749},{\"end\":81780,\"start\":81775},{\"end\":81792,\"start\":81788},{\"end\":82140,\"start\":82138},{\"end\":82149,\"start\":82147},{\"end\":82162,\"start\":82158},{\"end\":82173,\"start\":82170},{\"end\":82183,\"start\":82179},{\"end\":82196,\"start\":82192},{\"end\":82311,\"start\":82304},{\"end\":82326,\"start\":82320},{\"end\":82345,\"start\":82337},{\"end\":82358,\"start\":82354},{\"end\":82376,\"start\":82367},{\"end\":82387,\"start\":82383},{\"end\":82400,\"start\":82394},{\"end\":82419,\"start\":82409},{\"end\":82437,\"start\":82431},{\"end\":82454,\"start\":82445},{\"end\":82566,\"start\":82563},{\"end\":82578,\"start\":82575},{\"end\":82590,\"start\":82588},{\"end\":82602,\"start\":82598},{\"end\":82739,\"start\":82726},{\"end\":82759,\"start\":82749},{\"end\":82770,\"start\":82767},{\"end\":82783,\"start\":82779},{\"end\":82790,\"start\":82785},{\"end\":83002,\"start\":82996},{\"end\":83016,\"start\":83008},{\"end\":83030,\"start\":83023},{\"end\":83046,\"start\":83037},{\"end\":83059,\"start\":83053},{\"end\":83072,\"start\":83066},{\"end\":83087,\"start\":83083},{\"end\":83104,\"start\":83095},{\"end\":83121,\"start\":83113},{\"end\":83137,\"start\":83130},{\"end\":83255,\"start\":83248},{\"end\":83271,\"start\":83265},{\"end\":83295,\"start\":83289},{\"end\":83309,\"start\":83303},{\"end\":83620,\"start\":83612},{\"end\":83634,\"start\":83627},{\"end\":83655,\"start\":83642},{\"end\":83669,\"start\":83665},{\"end\":83742,\"start\":83736},{\"end\":83754,\"start\":83751},{\"end\":83769,\"start\":83765},{\"end\":83786,\"start\":83778},{\"end\":83809,\"start\":83802},{\"end\":83822,\"start\":83817},{\"end\":83845,\"start\":83830},{\"end\":83852,\"start\":83847},{\"end\":83959,\"start\":83953},{\"end\":83984,\"start\":83970},{\"end\":83998,\"start\":83992},{\"end\":84021,\"start\":84007},{\"end\":84034,\"start\":84030},{\"end\":84050,\"start\":84044},{\"end\":84066,\"start\":84060},{\"end\":84335,\"start\":84328},{\"end\":84349,\"start\":84342},{\"end\":84363,\"start\":84358},{\"end\":84376,\"start\":84371},{\"end\":84392,\"start\":84386},{\"end\":84405,\"start\":84400},{\"end\":84414,\"start\":84412},{\"end\":84693,\"start\":84691},{\"end\":84707,\"start\":84701},{\"end\":84723,\"start\":84717},{\"end\":84829,\"start\":84826},{\"end\":85023,\"start\":85020},{\"end\":85036,\"start\":85033},{\"end\":85047,\"start\":85045},{\"end\":85067,\"start\":85056},{\"end\":85088,\"start\":85077},{\"end\":85102,\"start\":85097},{\"end\":85114,\"start\":85110},{\"end\":85567,\"start\":85565},{\"end\":85581,\"start\":85574},{\"end\":85594,\"start\":85589},{\"end\":85607,\"start\":85601},{\"end\":85620,\"start\":85617},{\"end\":85632,\"start\":85628},{\"end\":85656,\"start\":85645},{\"end\":85668,\"start\":85664},{\"end\":85829,\"start\":85823},{\"end\":85843,\"start\":85837},{\"end\":85856,\"start\":85851},{\"end\":85869,\"start\":85865},{\"end\":86195,\"start\":86189},{\"end\":86209,\"start\":86202},{\"end\":86227,\"start\":86220},{\"end\":86243,\"start\":86234},{\"end\":86257,\"start\":86253},{\"end\":86274,\"start\":86266},{\"end\":86286,\"start\":86280},{\"end\":86302,\"start\":86297},{\"end\":86328,\"start\":86309},{\"end\":86345,\"start\":86339},{\"end\":86485,\"start\":86482},{\"end\":86502,\"start\":86495},{\"end\":86513,\"start\":86510},{\"end\":86525,\"start\":86520},{\"end\":86538,\"start\":86535},{\"end\":86552,\"start\":86545},{\"end\":86565,\"start\":86560},{\"end\":86583,\"start\":86572},{\"end\":86604,\"start\":86594},{\"end\":86703,\"start\":86700},{\"end\":86719,\"start\":86712},{\"end\":86740,\"start\":86730},{\"end\":86758,\"start\":86747},{\"end\":87072,\"start\":87066},{\"end\":87086,\"start\":87080},{\"end\":87101,\"start\":87095},{\"end\":87110,\"start\":87108},{\"end\":87123,\"start\":87117},{\"end\":87138,\"start\":87135},{\"end\":87157,\"start\":87152},{\"end\":87172,\"start\":87168},{\"end\":87189,\"start\":87181},{\"end\":87207,\"start\":87199},{\"end\":87217,\"start\":87212},{\"end\":87229,\"start\":87224},{\"end\":87474,\"start\":87468},{\"end\":87483,\"start\":87481},{\"end\":87493,\"start\":87488},{\"end\":87508,\"start\":87501},{\"end\":87530,\"start\":87520},{\"end\":87546,\"start\":87539},{\"end\":87559,\"start\":87554},{\"end\":87577,\"start\":87570},{\"end\":87593,\"start\":87588},{\"end\":87603,\"start\":87600},{\"end\":87618,\"start\":87610},{\"end\":87632,\"start\":87626},{\"end\":87647,\"start\":87641},{\"end\":87660,\"start\":87654},{\"end\":87675,\"start\":87669},{\"end\":87690,\"start\":87684},{\"end\":87706,\"start\":87698},{\"end\":87723,\"start\":87713},{\"end\":87738,\"start\":87734},{\"end\":87830,\"start\":87826},{\"end\":87848,\"start\":87837},{\"end\":87864,\"start\":87857},{\"end\":87880,\"start\":87874},{\"end\":87898,\"start\":87890},{\"end\":87911,\"start\":87907},{\"end\":87925,\"start\":87917},{\"end\":87998,\"start\":87991},{\"end\":88007,\"start\":88005},{\"end\":88020,\"start\":88015},{\"end\":88032,\"start\":88028},{\"end\":88046,\"start\":88040},{\"end\":88062,\"start\":88053},{\"end\":88165,\"start\":88159},{\"end\":88179,\"start\":88172},{\"end\":88193,\"start\":88186},{\"end\":88208,\"start\":88205},{\"end\":88223,\"start\":88217},{\"end\":88239,\"start\":88233},{\"end\":88251,\"start\":88247},{\"end\":88259,\"start\":88257},{\"end\":88272,\"start\":88269},{\"end\":88494,\"start\":88484},{\"end\":88522,\"start\":88511},{\"end\":88543,\"start\":88535},{\"end\":88556,\"start\":88550},{\"end\":88568,\"start\":88564},{\"end\":88589,\"start\":88580},{\"end\":88610,\"start\":88600},{\"end\":88622,\"start\":88618},{\"end\":88763,\"start\":88755},{\"end\":88781,\"start\":88775},{\"end\":88797,\"start\":88787},{\"end\":88812,\"start\":88808},{\"end\":88827,\"start\":88824},{\"end\":88840,\"start\":88835},{\"end\":88951,\"start\":88943},{\"end\":88969,\"start\":88963},{\"end\":88984,\"start\":88978},{\"end\":89000,\"start\":88990},{\"end\":89015,\"start\":89011},{\"end\":89030,\"start\":89027},{\"end\":89043,\"start\":89038},{\"end\":89139,\"start\":89131},{\"end\":89155,\"start\":89149},{\"end\":89170,\"start\":89164},{\"end\":89186,\"start\":89180},{\"end\":89201,\"start\":89195},{\"end\":89365,\"start\":89357},{\"end\":89379,\"start\":89373},{\"end\":89398,\"start\":89390},{\"end\":89412,\"start\":89405},{\"end\":89425,\"start\":89419},{\"end\":89615,\"start\":89612},{\"end\":89628,\"start\":89623},{\"end\":89642,\"start\":89635},{\"end\":89656,\"start\":89650},{\"end\":89665,\"start\":89663},{\"end\":89764,\"start\":89756},{\"end\":89773,\"start\":89769},{\"end\":89789,\"start\":89782},{\"end\":89805,\"start\":89800},{\"end\":90110,\"start\":90102},{\"end\":90123,\"start\":90117},{\"end\":90132,\"start\":90130},{\"end\":90150,\"start\":90143},{\"end\":90163,\"start\":90159},{\"end\":90177,\"start\":90173},{\"end\":90191,\"start\":90184},{\"end\":90205,\"start\":90199},{\"end\":90222,\"start\":90212},{\"end\":90258,\"start\":90256},{\"end\":90271,\"start\":90265},{\"end\":90281,\"start\":90275},{\"end\":90296,\"start\":90289},{\"end\":90311,\"start\":90303},{\"end\":90321,\"start\":90317},{\"end\":90333,\"start\":90328},{\"end\":90345,\"start\":90335},{\"end\":90547,\"start\":90545},{\"end\":90558,\"start\":90555},{\"end\":90575,\"start\":90568},{\"end\":90589,\"start\":90584},{\"end\":90604,\"start\":90597},{\"end\":90618,\"start\":90612},{\"end\":90751,\"start\":90749},{\"end\":90765,\"start\":90761},{\"end\":90780,\"start\":90777},{\"end\":90795,\"start\":90791},{\"end\":90811,\"start\":90804},{\"end\":90825,\"start\":90821},{\"end\":90840,\"start\":90838},{\"end\":90969,\"start\":90965},{\"end\":90982,\"start\":90978},{\"end\":90995,\"start\":90992},{\"end\":91005,\"start\":91001},{\"end\":91021,\"start\":91016},{\"end\":91032,\"start\":91027},{\"end\":91111,\"start\":91108},{\"end\":91123,\"start\":91119},{\"end\":91134,\"start\":91129},{\"end\":91145,\"start\":91141},{\"end\":91152,\"start\":91150},{\"end\":91162,\"start\":91159},{\"end\":91260,\"start\":91254},{\"end\":91275,\"start\":91268},{\"end\":91290,\"start\":91282},{\"end\":91300,\"start\":91298},{\"end\":91312,\"start\":91307},{\"end\":91327,\"start\":91320},{\"end\":91340,\"start\":91334},{\"end\":91361,\"start\":91351},{\"end\":91369,\"start\":91363}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":80101,\"start\":80082},{\"attributes\":{\"doi\":\"arXiv:2204.05862\",\"id\":\"b1\"},\"end\":80364,\"start\":80103},{\"attributes\":{\"doi\":\"arXiv:2212.08073\",\"id\":\"b2\"},\"end\":80600,\"start\":80366},{\"attributes\":{\"doi\":\"arXiv:2004.05150\",\"id\":\"b3\"},\"end\":80722,\"start\":80602},{\"attributes\":{\"doi\":\"arXiv:2303.16749\",\"id\":\"b4\"},\"end\":80954,\"start\":80724},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":248405661},\"end\":81362,\"start\":80956},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":208617790},\"end\":81615,\"start\":81364},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":247315430},\"end\":82033,\"start\":81617},{\"attributes\":{\"doi\":\"arXiv:2305.17306\",\"id\":\"b8\"},\"end\":82232,\"start\":82035},{\"attributes\":{\"doi\":\"arXiv:2302.07459\",\"id\":\"b9\"},\"end\":82490,\"start\":82234},{\"attributes\":{\"doi\":\"arXiv:2305.14627\",\"id\":\"b10\"},\"end\":82638,\"start\":82492},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":221878771},\"end\":82917,\"start\":82640},{\"attributes\":{\"doi\":\"arXiv:2209.14375\",\"id\":\"b12\"},\"end\":83173,\"start\":82919},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":58007087},\"end\":83537,\"start\":83175},{\"attributes\":{\"id\":\"b14\"},\"end\":83675,\"start\":83539},{\"attributes\":{\"doi\":\"arXiv:2302.08582\",\"id\":\"b15\"},\"end\":83888,\"start\":83677},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":221655075},\"end\":84232,\"start\":83890},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":256416131},\"end\":84586,\"start\":84234},{\"attributes\":{\"doi\":\"arXiv:1909.03087\",\"id\":\"b18\"},\"end\":84759,\"start\":84588},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":964287},\"end\":84930,\"start\":84761},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":235313967},\"end\":85492,\"start\":84932},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":249152301},\"end\":85750,\"start\":85494},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":246016194},\"end\":86115,\"start\":85752},{\"attributes\":{\"doi\":\"arXiv:2203.11147\",\"id\":\"b23\"},\"end\":86381,\"start\":86117},{\"attributes\":{\"doi\":\"arXiv:2305.14251\",\"id\":\"b24\"},\"end\":86640,\"start\":86383},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":216056269},\"end\":86989,\"start\":86642},{\"attributes\":{\"doi\":\"ArXiv, abs/2112.09332\",\"id\":\"b26\"},\"end\":87352,\"start\":86991},{\"attributes\":{\"id\":\"b27\"},\"end\":87392,\"start\":87354},{\"attributes\":{\"id\":\"b28\"},\"end\":87756,\"start\":87394},{\"attributes\":{\"id\":\"b29\"},\"end\":87931,\"start\":87758},{\"attributes\":{\"id\":\"b30\"},\"end\":88068,\"start\":87933},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":204838007},\"end\":88323,\"start\":88070},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":252693405},\"end\":88689,\"start\":88325},{\"attributes\":{\"doi\":\"arXiv:2204.14146\",\"id\":\"b33\"},\"end\":88876,\"start\":88691},{\"attributes\":{\"id\":\"b34\"},\"end\":89049,\"start\":88878},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":3075448},\"end\":89350,\"start\":89051},{\"attributes\":{\"doi\":\"arXiv:1707.06347\",\"id\":\"b36\"},\"end\":89502,\"start\":89352},{\"attributes\":{\"doi\":\"arXiv:2210.15893\",\"id\":\"b37\"},\"end\":89701,\"start\":89504},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":248157463},\"end\":90051,\"start\":89703},{\"attributes\":{\"doi\":\"ArXiv, abs/2009.01325\",\"id\":\"b39\"},\"end\":90249,\"start\":90053},{\"attributes\":{\"doi\":\"arXiv:2109.10862\",\"id\":\"b40\"},\"end\":90432,\"start\":90251},{\"attributes\":{\"doi\":\"arXiv:2208.03270\",\"id\":\"b41\"},\"end\":90654,\"start\":90434},{\"attributes\":{\"doi\":\"arXiv:2305.14282\",\"id\":\"b42\"},\"end\":90876,\"start\":90656},{\"attributes\":{\"id\":\"b43\"},\"end\":91038,\"start\":90878},{\"attributes\":{\"doi\":\"arXiv:2305.06311\",\"id\":\"b44\"},\"end\":91198,\"start\":91040},{\"attributes\":{\"doi\":\"ArXiv, abs/1909.08593\",\"id\":\"b45\"},\"end\":91396,\"start\":91200}]", "bib_title": "[{\"end\":81065,\"start\":80956},{\"end\":81442,\"start\":81364},{\"end\":81718,\"start\":81617},{\"end\":82717,\"start\":82640},{\"end\":83239,\"start\":83175},{\"end\":83947,\"start\":83890},{\"end\":84318,\"start\":84234},{\"end\":84815,\"start\":84761},{\"end\":85012,\"start\":84932},{\"end\":85556,\"start\":85494},{\"end\":85816,\"start\":85752},{\"end\":86692,\"start\":86642},{\"end\":88151,\"start\":88070},{\"end\":88473,\"start\":88325},{\"end\":89124,\"start\":89051},{\"end\":89749,\"start\":89703}]", "bib_author": "[{\"end\":80206,\"start\":80194},{\"end\":80218,\"start\":80206},{\"end\":80233,\"start\":80218},{\"end\":80248,\"start\":80233},{\"end\":80259,\"start\":80248},{\"end\":80274,\"start\":80259},{\"end\":80286,\"start\":80274},{\"end\":80302,\"start\":80286},{\"end\":80316,\"start\":80302},{\"end\":80330,\"start\":80316},{\"end\":80378,\"start\":80366},{\"end\":80395,\"start\":80378},{\"end\":80411,\"start\":80395},{\"end\":80426,\"start\":80411},{\"end\":80443,\"start\":80426},{\"end\":80455,\"start\":80443},{\"end\":80466,\"start\":80455},{\"end\":80479,\"start\":80466},{\"end\":80498,\"start\":80479},{\"end\":80516,\"start\":80498},{\"end\":80657,\"start\":80645},{\"end\":80675,\"start\":80657},{\"end\":80688,\"start\":80675},{\"end\":80809,\"start\":80794},{\"end\":80826,\"start\":80809},{\"end\":80841,\"start\":80826},{\"end\":80859,\"start\":80841},{\"end\":80875,\"start\":80859},{\"end\":80902,\"start\":80875},{\"end\":80913,\"start\":80902},{\"end\":80920,\"start\":80913},{\"end\":81096,\"start\":81067},{\"end\":81111,\"start\":81096},{\"end\":81118,\"start\":81111},{\"end\":81463,\"start\":81444},{\"end\":81479,\"start\":81463},{\"end\":81491,\"start\":81479},{\"end\":81502,\"start\":81491},{\"end\":81514,\"start\":81502},{\"end\":81528,\"start\":81514},{\"end\":81544,\"start\":81528},{\"end\":81557,\"start\":81544},{\"end\":81729,\"start\":81720},{\"end\":81745,\"start\":81729},{\"end\":81768,\"start\":81745},{\"end\":81782,\"start\":81768},{\"end\":81794,\"start\":81782},{\"end\":82142,\"start\":82134},{\"end\":82151,\"start\":82142},{\"end\":82164,\"start\":82151},{\"end\":82175,\"start\":82164},{\"end\":82185,\"start\":82175},{\"end\":82198,\"start\":82185},{\"end\":82313,\"start\":82299},{\"end\":82328,\"start\":82313},{\"end\":82347,\"start\":82328},{\"end\":82360,\"start\":82347},{\"end\":82378,\"start\":82360},{\"end\":82389,\"start\":82378},{\"end\":82402,\"start\":82389},{\"end\":82421,\"start\":82402},{\"end\":82439,\"start\":82421},{\"end\":82456,\"start\":82439},{\"end\":82568,\"start\":82556},{\"end\":82580,\"start\":82568},{\"end\":82592,\"start\":82580},{\"end\":82604,\"start\":82592},{\"end\":82741,\"start\":82719},{\"end\":82761,\"start\":82741},{\"end\":82772,\"start\":82761},{\"end\":82785,\"start\":82772},{\"end\":82792,\"start\":82785},{\"end\":83004,\"start\":82989},{\"end\":83018,\"start\":83004},{\"end\":83032,\"start\":83018},{\"end\":83048,\"start\":83032},{\"end\":83061,\"start\":83048},{\"end\":83074,\"start\":83061},{\"end\":83089,\"start\":83074},{\"end\":83106,\"start\":83089},{\"end\":83123,\"start\":83106},{\"end\":83139,\"start\":83123},{\"end\":83257,\"start\":83241},{\"end\":83273,\"start\":83257},{\"end\":83297,\"start\":83273},{\"end\":83311,\"start\":83297},{\"end\":83622,\"start\":83604},{\"end\":83636,\"start\":83622},{\"end\":83657,\"start\":83636},{\"end\":83671,\"start\":83657},{\"end\":83744,\"start\":83729},{\"end\":83756,\"start\":83744},{\"end\":83771,\"start\":83756},{\"end\":83788,\"start\":83771},{\"end\":83811,\"start\":83788},{\"end\":83824,\"start\":83811},{\"end\":83847,\"start\":83824},{\"end\":83854,\"start\":83847},{\"end\":83961,\"start\":83949},{\"end\":83986,\"start\":83961},{\"end\":84000,\"start\":83986},{\"end\":84023,\"start\":84000},{\"end\":84036,\"start\":84023},{\"end\":84052,\"start\":84036},{\"end\":84068,\"start\":84052},{\"end\":84077,\"start\":84068},{\"end\":84337,\"start\":84320},{\"end\":84351,\"start\":84337},{\"end\":84365,\"start\":84351},{\"end\":84378,\"start\":84365},{\"end\":84394,\"start\":84378},{\"end\":84407,\"start\":84394},{\"end\":84416,\"start\":84407},{\"end\":84695,\"start\":84682},{\"end\":84709,\"start\":84695},{\"end\":84725,\"start\":84709},{\"end\":84831,\"start\":84817},{\"end\":85025,\"start\":85014},{\"end\":85038,\"start\":85025},{\"end\":85049,\"start\":85038},{\"end\":85069,\"start\":85049},{\"end\":85090,\"start\":85069},{\"end\":85104,\"start\":85090},{\"end\":85116,\"start\":85104},{\"end\":85569,\"start\":85558},{\"end\":85583,\"start\":85569},{\"end\":85596,\"start\":85583},{\"end\":85609,\"start\":85596},{\"end\":85622,\"start\":85609},{\"end\":85634,\"start\":85622},{\"end\":85658,\"start\":85634},{\"end\":85670,\"start\":85658},{\"end\":85831,\"start\":85818},{\"end\":85845,\"start\":85831},{\"end\":85858,\"start\":85845},{\"end\":85871,\"start\":85858},{\"end\":86197,\"start\":86183},{\"end\":86211,\"start\":86197},{\"end\":86229,\"start\":86211},{\"end\":86245,\"start\":86229},{\"end\":86259,\"start\":86245},{\"end\":86276,\"start\":86259},{\"end\":86288,\"start\":86276},{\"end\":86304,\"start\":86288},{\"end\":86330,\"start\":86304},{\"end\":86347,\"start\":86330},{\"end\":86487,\"start\":86476},{\"end\":86504,\"start\":86487},{\"end\":86515,\"start\":86504},{\"end\":86527,\"start\":86515},{\"end\":86540,\"start\":86527},{\"end\":86554,\"start\":86540},{\"end\":86567,\"start\":86554},{\"end\":86585,\"start\":86567},{\"end\":86606,\"start\":86585},{\"end\":86705,\"start\":86694},{\"end\":86721,\"start\":86705},{\"end\":86742,\"start\":86721},{\"end\":86760,\"start\":86742},{\"end\":87074,\"start\":87056},{\"end\":87088,\"start\":87074},{\"end\":87103,\"start\":87088},{\"end\":87112,\"start\":87103},{\"end\":87125,\"start\":87112},{\"end\":87140,\"start\":87125},{\"end\":87159,\"start\":87140},{\"end\":87174,\"start\":87159},{\"end\":87191,\"start\":87174},{\"end\":87209,\"start\":87191},{\"end\":87219,\"start\":87209},{\"end\":87231,\"start\":87219},{\"end\":87476,\"start\":87463},{\"end\":87485,\"start\":87476},{\"end\":87495,\"start\":87485},{\"end\":87510,\"start\":87495},{\"end\":87532,\"start\":87510},{\"end\":87548,\"start\":87532},{\"end\":87561,\"start\":87548},{\"end\":87579,\"start\":87561},{\"end\":87595,\"start\":87579},{\"end\":87605,\"start\":87595},{\"end\":87620,\"start\":87605},{\"end\":87634,\"start\":87620},{\"end\":87649,\"start\":87634},{\"end\":87662,\"start\":87649},{\"end\":87677,\"start\":87662},{\"end\":87692,\"start\":87677},{\"end\":87708,\"start\":87692},{\"end\":87725,\"start\":87708},{\"end\":87729,\"start\":87725},{\"end\":87740,\"start\":87729},{\"end\":87832,\"start\":87819},{\"end\":87850,\"start\":87832},{\"end\":87866,\"start\":87850},{\"end\":87882,\"start\":87866},{\"end\":87900,\"start\":87882},{\"end\":87913,\"start\":87900},{\"end\":87927,\"start\":87913},{\"end\":88000,\"start\":87986},{\"end\":88009,\"start\":88000},{\"end\":88022,\"start\":88009},{\"end\":88034,\"start\":88022},{\"end\":88048,\"start\":88034},{\"end\":88064,\"start\":88048},{\"end\":88167,\"start\":88153},{\"end\":88181,\"start\":88167},{\"end\":88195,\"start\":88181},{\"end\":88210,\"start\":88195},{\"end\":88225,\"start\":88210},{\"end\":88241,\"start\":88225},{\"end\":88253,\"start\":88241},{\"end\":88261,\"start\":88253},{\"end\":88274,\"start\":88261},{\"end\":88496,\"start\":88475},{\"end\":88500,\"start\":88496},{\"end\":88524,\"start\":88500},{\"end\":88528,\"start\":88524},{\"end\":88545,\"start\":88528},{\"end\":88558,\"start\":88545},{\"end\":88570,\"start\":88558},{\"end\":88591,\"start\":88570},{\"end\":88612,\"start\":88591},{\"end\":88624,\"start\":88612},{\"end\":88765,\"start\":88748},{\"end\":88783,\"start\":88765},{\"end\":88799,\"start\":88783},{\"end\":88814,\"start\":88799},{\"end\":88829,\"start\":88814},{\"end\":88842,\"start\":88829},{\"end\":88953,\"start\":88936},{\"end\":88971,\"start\":88953},{\"end\":88986,\"start\":88971},{\"end\":89002,\"start\":88986},{\"end\":89017,\"start\":89002},{\"end\":89032,\"start\":89017},{\"end\":89045,\"start\":89032},{\"end\":89141,\"start\":89126},{\"end\":89157,\"start\":89141},{\"end\":89172,\"start\":89157},{\"end\":89188,\"start\":89172},{\"end\":89203,\"start\":89188},{\"end\":89367,\"start\":89352},{\"end\":89381,\"start\":89367},{\"end\":89400,\"start\":89381},{\"end\":89414,\"start\":89400},{\"end\":89427,\"start\":89414},{\"end\":89617,\"start\":89605},{\"end\":89630,\"start\":89617},{\"end\":89644,\"start\":89630},{\"end\":89658,\"start\":89644},{\"end\":89667,\"start\":89658},{\"end\":89766,\"start\":89751},{\"end\":89775,\"start\":89766},{\"end\":89791,\"start\":89775},{\"end\":89807,\"start\":89791},{\"end\":90112,\"start\":90096},{\"end\":90125,\"start\":90112},{\"end\":90134,\"start\":90125},{\"end\":90152,\"start\":90134},{\"end\":90165,\"start\":90152},{\"end\":90179,\"start\":90165},{\"end\":90193,\"start\":90179},{\"end\":90207,\"start\":90193},{\"end\":90224,\"start\":90207},{\"end\":90260,\"start\":90251},{\"end\":90273,\"start\":90260},{\"end\":90283,\"start\":90273},{\"end\":90298,\"start\":90283},{\"end\":90313,\"start\":90298},{\"end\":90323,\"start\":90313},{\"end\":90335,\"start\":90323},{\"end\":90347,\"start\":90335},{\"end\":90549,\"start\":90540},{\"end\":90560,\"start\":90549},{\"end\":90577,\"start\":90560},{\"end\":90591,\"start\":90577},{\"end\":90606,\"start\":90591},{\"end\":90620,\"start\":90606},{\"end\":90753,\"start\":90743},{\"end\":90767,\"start\":90753},{\"end\":90782,\"start\":90767},{\"end\":90797,\"start\":90782},{\"end\":90813,\"start\":90797},{\"end\":90827,\"start\":90813},{\"end\":90834,\"start\":90827},{\"end\":90842,\"start\":90834},{\"end\":90971,\"start\":90959},{\"end\":90984,\"start\":90971},{\"end\":90997,\"start\":90984},{\"end\":91007,\"start\":90997},{\"end\":91023,\"start\":91007},{\"end\":91034,\"start\":91023},{\"end\":91113,\"start\":91102},{\"end\":91125,\"start\":91113},{\"end\":91136,\"start\":91125},{\"end\":91147,\"start\":91136},{\"end\":91154,\"start\":91147},{\"end\":91164,\"start\":91154},{\"end\":91262,\"start\":91252},{\"end\":91277,\"start\":91262},{\"end\":91292,\"start\":91277},{\"end\":91302,\"start\":91292},{\"end\":91314,\"start\":91302},{\"end\":91329,\"start\":91314},{\"end\":91342,\"start\":91329},{\"end\":91363,\"start\":91342},{\"end\":91371,\"start\":91363}]", "bib_venue": "[{\"end\":80099,\"start\":80084},{\"end\":80192,\"start\":80103},{\"end\":80580,\"start\":80532},{\"end\":80643,\"start\":80602},{\"end\":80792,\"start\":80724},{\"end\":81204,\"start\":81118},{\"end\":81609,\"start\":81557},{\"end\":81881,\"start\":81794},{\"end\":81894,\"start\":81883},{\"end\":82132,\"start\":82035},{\"end\":82297,\"start\":82234},{\"end\":82554,\"start\":82492},{\"end\":82861,\"start\":82792},{\"end\":82987,\"start\":82919},{\"end\":83398,\"start\":83311},{\"end\":83602,\"start\":83539},{\"end\":83727,\"start\":83677},{\"end\":84146,\"start\":84077},{\"end\":84474,\"start\":84416},{\"end\":84680,\"start\":84588},{\"end\":84862,\"start\":84831},{\"end\":85278,\"start\":85116},{\"end\":85291,\"start\":85280},{\"end\":85744,\"start\":85670},{\"end\":85957,\"start\":85871},{\"end\":86181,\"start\":86117},{\"end\":86474,\"start\":86383},{\"end\":86854,\"start\":86760},{\"end\":87054,\"start\":86991},{\"end\":87386,\"start\":87356},{\"end\":87461,\"start\":87394},{\"end\":87817,\"start\":87758},{\"end\":87984,\"start\":87933},{\"end\":88314,\"start\":88274},{\"end\":88683,\"start\":88624},{\"end\":88746,\"start\":88691},{\"end\":88934,\"start\":88878},{\"end\":89281,\"start\":89203},{\"end\":89482,\"start\":89443},{\"end\":89603,\"start\":89504},{\"end\":89893,\"start\":89807},{\"end\":90094,\"start\":90053},{\"end\":90412,\"start\":90363},{\"end\":90538,\"start\":90434},{\"end\":90741,\"start\":90656},{\"end\":90957,\"start\":90878},{\"end\":91100,\"start\":91040},{\"end\":91250,\"start\":91200},{\"end\":81308,\"start\":81206},{\"end\":81983,\"start\":81896},{\"end\":83487,\"start\":83400},{\"end\":84178,\"start\":84148},{\"end\":84537,\"start\":84476},{\"end\":84880,\"start\":84864},{\"end\":85440,\"start\":85293},{\"end\":86061,\"start\":85959},{\"end\":86935,\"start\":86856},{\"end\":89346,\"start\":89283},{\"end\":89997,\"start\":89895}]"}}}, "year": 2023, "month": 12, "day": 17}