{"id": 237363279, "updated": "2023-10-05 23:49:12.014", "metadata": {"title": "Self-balanced Learning For Domain Generalization", "authors": "[{\"first\":\"Jin\",\"last\":\"Kim\",\"middle\":[]},{\"first\":\"Jiyoung\",\"last\":\"Lee\",\"middle\":[]},{\"first\":\"Jungin\",\"last\":\"Park\",\"middle\":[]},{\"first\":\"Dongbo\",\"last\":\"Min\",\"middle\":[]},{\"first\":\"Kwanghoon\",\"last\":\"Sohn\",\"middle\":[]}]", "venue": "ICIP, 2021, pp. 779-783", "journal": null, "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Domain generalization aims to learn a prediction model on multi-domain source data such that the model can generalize to a target domain with unknown statistics. Most existing approaches have been developed under the assumption that the source data is well-balanced in terms of both domain and class. However, real-world training data collected with different composition biases often exhibits severe distribution gaps for domain and class, leading to substantial performance degradation. In this paper, we propose a self-balanced domain generalization framework that adaptively learns the weights of losses to alleviate the bias caused by different distributions of the multi-domain source data. The self-balanced scheme is based on an auxiliary reweighting network that iteratively updates the weight of loss conditioned on the domain and class information by leveraging balanced meta data. Experimental results demonstrate the effectiveness of our method overwhelming state-of-the-art works for domain generalization.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2108.13597", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icip/KimLPMS21", "doi": "10.1109/icip42928.2021.9506516"}}, "content": {"source": {"pdf_hash": "e5b8c6f14946a4adff44e27596bb45aa3f762c28", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2108.13597v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2108.13597", "status": "GREEN"}}, "grobid": {"id": "85fae2d2d483a41f3d6ffeef79581ae66b147cd5", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/e5b8c6f14946a4adff44e27596bb45aa3f762c28.txt", "contents": "\nSELF-BALANCED LEARNING FOR DOMAIN GENERALIZATION\n\n\nJin Kim \nJiyoung Lee \nJungin Park \nDongbo Min \nDepartment of Computer Science and Engineering\nEwha Womans University\nSeoulKorea\n\nKwanghoon Sohn khsohn@yonsei.ac.kr \n\nSchool of Electrical and Electronic Engineering\nYonsei University\nSeoulKorea\n\nSELF-BALANCED LEARNING FOR DOMAIN GENERALIZATION\nIndex Terms-Domain generalizationdomain imbal- anceclass imbalancemeta-learning\nDomain generalization aims to learn a prediction model on multi-domain source data such that the model can generalize to a target domain with unknown statistics. Most existing approaches have been developed under the assumption that the source data is well-balanced in terms of both domain and class. However, real-world training data collected with different composition biases often exhibits severe distribution gaps for domain and class, leading to substantial performance degradation. In this paper, we propose a self-balanced domain generalization framework that adaptively learns the weights of losses to alleviate the bias caused by different distributions of the multi-domain source data. The self-balanced scheme is based on an auxiliary reweighting network that iteratively updates the weight of loss conditioned on the domain and class information by leveraging balanced meta data. Experimental results demonstrate the effectiveness of our method overwhelming state-of-the-art works for domain generalization.\n\nINTRODUCTION\n\nDespite the impressive advent of deep learning technologies, the domain shift is still a huge obstacle for deploying them in numerous computer vision tasks. As most approaches are trained under the assumption that training and test data are sampled from the same distribution, they often fail to infer accurate predictions for unseen test data sampled from outof-distribution [1,2,3,4,5].\n\nTo overcome this issue, the domain generalization (DG) approaches have attempted to design a generalizable model to minimize the domain gap between train and test domains and perform evenly well under multiple data distributions. As part of this effort, the data augmentation methods have been proposed to synthesize the source domains to a wider span of the data space. Volpi et al. [6] adversarially generated fictitious samples by defining target distributions within a certain Wasserstein distance. Recently, Yue et al. [7] employed style transfer [8] to synthesize new training samples, and Carlucci et al. [9] proposed to solve a jigsaw puzzle by shuffling image patches to improve the generalization performance of image classification. Although these approaches have demonstrated promising results with data augmentation, it is fairly hard to generate new training samples that can fully cover the real-world distribution, often leading to overfitting issues.\n\nLearning domain invariant feature representations has become an attractive alternative to make the model robust to the unseen target data [10,11,12,13,14]. As a pioneering work, maximum mean discrepancy (MMD) constraint [15] was applied to adversarial autoencoders to measure the alignment of the distribution among multiple source domains [10]. Some approaches have improved generalization ability through arXiv:2108.13597v1 [cs.CV] 31 Aug 2021\n(1) ( ) = ( ) \u2212 =1 \u2112 \u2131 ; ( ) ,(2)( +1) = ( ) \u2212 ( ) =1 \u2112 ; ( ) ,(3)( +1) = ( ) \u2212 =1 \u2112 \u2131 ; ( ) , 0 1 0 ( ) ( ) Imbalanced Source Data \u2112 \u2131 ( )\n\nUpdate\n\nStep 1 ( ) Balanced Meta Data \u2112 ( +1)\n\n\nUpdate\n\nStep 2\n0 1 0 ( ) ( +1) Imbalanced Source Data \u2112 \u2131 ( +1)\n\nUpdate\n\nStep 3 \u2131 \u2131 \u2131 \u2131 L C S 1k 0 Domain 1 Domain 2 Domain 3 Fig. 2. An overall methodological flow of SBDG. In step 1, the parameter \u0398 (t) of the task network F is updated to\u0398 (t) with a mini-batch from the imbalanced set. The parameter \u03a8 (t) of the reweighting network G is updated to \u03a8 (t+1) with a mini-batch from the balanced set in step 2. As a final step, \u0398 (t) is optimized to \u0398 (t+1) with the imbalanced set and the adaptive weight\u0175 i derived from G(|\u03a8 (t+1) ).\n\na contrastive loss to embed training samples nearby latent space [14] or an episodic training procedure to simulate domain shift in training phase [11]. The model-agnostic meta-learning approaches [16,17,18] have been introduced to learn a way that improves the domain generality of a base learner by finding a route to optimization. While they have achieved remarkable advances in domain generalization, biased training data distributions in terms of domains and classes behave as the main obstacle impeding the aforementioned approaches from achieving higher accuracy. For example, we depict the classification performance on the different target domain using the representative meta-learning based DG method [18] and our proposed method in Fig. 1. The variances for the number of data with respect to domains and classes are also reported. The accuracy has been degraded significantly from the imbalanced source domains, when evaluating the model on the L dataset in Fig. 1\n(b).\nIn this paper, we propose a novel self-balanced domain generalization framework, termed SBDG, that acts as a more effective alternative for the domain/class imbalance problem by explicitly weighting a training loss. To this end, we extend the sample reweighting scheme [19], which was originally proposed for solving the class imbalance issue, to prioritize the minority domain with relatively higher training losses. Specifically, we formulate an auxiliary reweighting network that can be integrated with the domain generalization methods through the adaptive balancing of the training loss to fully leverage the information of a given domain. Furthermore, model-agnostic meta-learning [16] is employed to train the reweighting network guided by an unbiased metadataset that is uniformly distributed in terms of domains and classes as shown in Fig. 2. Experimental results show that our framework helps to prevent the performance degeneration by imbalanced training samples and achieves state-of-the-art performance compared to prior works.\n\n\nPROPOSED METHOD\n\n\nProblem Statement\n\nThe objective of domain generalization is to learn a system to perform well in an unseen domain T using a set of observable source domains S = {D k } K k=1 , where K is the number of source domains. The k th source domain for c th class contains N k c pairs of input images and class labels\n{x k i , y k i } N k c i=1\n, where N k c largely varies. The drawback of most existing methods is to give equal weight to sample in training without consideration of the imbalance.\n\nOur method improves generalization performance by estimating adaptive learning weights between dominant and minor samples in each training iteration. As shown in Fig. 2, the proposed networks are composed of two parts; (1) the task network to perform a target task and (2) the auxiliary reweighting network to balance the task loss of each sample. The task network F predicts the probability of the image class from an input image x k i with a parameter \u0398. The auxiliary reweighting network G takes a loss and the conditional domain vector d i as inputs and outputs an adaptive weight w i for i th sample with a network parameter \u03a8. Although the image classification is employed as the target task in this work, our framework can also be extended in numerous computer vision tasks such as object detection [20] and semantic segmentation [21]. We remain this as future work.\n\n\nTraining\n\nTo jointly train the reweighting and task networks, we first divide the source domains into balanced meta dataset S B and imbalanced set S I :\nS = S B \u222a S I .(1)\nThe images in the balanced meta dataset S B are uniformly distributed in terms of domains and classes. The overall framework is illustrated in Fig. 2. We take two state-of-thearts (MLDG [18] or RSC [1]) as the task network F for the image classification, but any kind of DG models can also be adopted. The auxiliary reweighting network G is composed of 2 fully connected layers with the ReLU activation function, and the sigmoid output layer is adopted to ensure w i \u2208 [0, 1]. Next, we describe the three training steps in detail.\n\nStep 1. In the first step, we update the current parameter of the task network with the imbalanced set. The image class is predicted by F with softmax function as follows:\ny I i = sof tmax(F(x I i |\u0398 (t) )),(2)\nwhere\u0177 I i \u2208 R C is the probability of image classes, x I i is a training image sampled from imbalanced set S I , and C is the number of class. The task loss L F is cross-entropy loss between\u0177 I i and y I i . While the task loss L F can be defined in a different form depending on F, we just use the cross-entropy loss function for simplification. Different from [19] that uses only loss of the task network, the reweighting network G takes the task loss L F and the one-hot domain conditional vector d i to ensure that G recognizes the characteristics of the domain. Thus, the weight w i for loss is estimated as follows:\nw i = G(\u03a0(d i , L F )|\u03a8 (t) ),(3)\nwhere \u03a0(\u00b7) represents a concatenation operation. The current parameter \u0398 (t) of the task network is optimized by minimizing the following weighted loss:\n\u0398 (t) = argmin \u0398 (t) n i=1 w i L F (\u0177 I i , y I i ),(4)\nwhere n is the number of samples in a mini-batch.\n\nStep 2. The parameter \u03a8 (t) of the reweighting network is optimized to be guided by the parameter\u0398 (t) on the balanced meta dataset S B by optimizing the following equation:\n\u03a8 (t+1) = argmin \u03a8 (t) m j=1 L G (\u0177 B j , y B j ),(5)\nwhere L G is a cross entropy loss, and x B j is a training image sampled from the balanced meta dataset S B ,\u0177 B j is the estimated class vector with the parameter\u0398 (t) , and m is the size of mini-batch.\n\nStep 3. At the last step, the updated parameter \u03a8 (t+1) is used to produce the adaptive weight\u0175 i and the parameter \u0398 (t) is optimized as follows:\n\u0398 (t+1) = argmin \u0398 (t) n i=1\u0175 i L F (\u0177 I i , y I i ),(6)\nwhere\u0175 i = G(\u03a0(d i , L F )|\u03a8 (t+1) ) and\u0177 I i is derived from the task network with the parameter \u0398 (t) . The flow of SBDG algorithm is summarized in Alg. 1 and the convergence of this second gradient update procedure can be mathematically proven in a similar way to [19]. {x I i , y I i , d i } \u2190 SampleM iniBatch(S I , n).\n\n\n5:\n\n{x B j , y B j } \u2190 SampleM iniBatch(S B .m).\n\n6:\u0177 I i = sof tmax(F(x I i |\u0398 (t) )).\n\n7:\nw i = G(\u03a0(d i , L F )|\u03a8 (t) ). 8:\u0398 (t) \u2190 \u0398 (t) \u2212 \u03b1 1 n n i=1 w i \u0398 L F (\u0177 I i , y I i ). 9:\u0177 B j = sof tmax(F(x B j |\u0398 (t) )).\n10:\n\u03a8 (t+1) \u2190 \u03a8 (t) \u2212 \u03b2 1 m m j=1 \u03a8 L G (\u0177 B j , y B j ). 11:\u0175 i = G(\u03a0(d i , L F )|\u03a8 (t+1) ). 12: \u0398 (t+1) \u2190 \u0398 (t) \u2212 \u03b1 1 n n i=1\u0175 i \u0398 L F (\u0177 I i , y I i ).\n\n13:\n\nend for 14: end procedure 3. EXPERIMENTS\n\n\nExperimental Settings\n\nDataset. We evaluate our method on VLCS dataset [22], which is the commonly used domain generalization benchmark for image classification. It contains images from four different datasets (domains): VOC2007 (V), LabelMe (L), Caltech (C), and SUN09 (S). Each domain includes five classes: bird, car, chair, dog, and person. Following the standard protocol of [23], the VLCS dataset is randomly divided for each domain into 70% training and 30% test set. Implementation details. We picked 12 samples from each domain-class pair set to construct the balanced meta dataset by leveraging random-over-sampling from 30% of the training set. The mini-batch size of the reweighting network was 9 for each domain, totaling 27 for three source domains. The learning rate of reweighting network \u03b2 is set as 5\u00d710 \u22125 . Following the previous works [9,1], we perform random cropping, horizontal flipping, and RGB to greyscale converting.\n\nTo evaluate the influence of our framework, two baseline methods were adopted as the task network F; (1) MLDG [18] and (2) RSC [1]. MLDG [18] is the well-known metalearning based DG algorithm that meta-learns how to generalize across domains. RSC [1] is the current state-of-the-art DG algorithm that discards a few percent of high gradient features at each epoch and trains the model with the remaining information. To train with MLDG [18], outer and inner update's gradients are added equally. The reweighted loss was used both in the inner and outer update of MLDG [18]. The batch size is 128 for each source domain and the learning rate \u03b1 is set as 5 \u00d7 10 \u22124 . The remaining hyper-parameters  for RSC were used the same as the original method [1]. We used AlexNet pre-trained on ImageNet as a backbone network for each baseline method. We selected the best model via leave-one-domain-out validation model [24].\n\n\nResults\n\nIn Table 1, we compared our model with several state-ofthe-art models on VLCS dataset [22] in terms of classification accuracy on different target domains. The variation of source domains (\u03c3 2 domain ) is computed for each target domain, e.g. using 'L', 'S, and 'V' source domains for 'C' target domain. While most algorithms work reasonably well for 'C' domain, they perform poorly when 'L', 'S', and 'V' are target domains. The reason is that when the target domain is the 'C', \u03c3 2 domain is lower than in other cases as shown in Table 1. Our method shows the significant improvement of the accuracy when \u03c3 2 domain is high (i.e. target domain is 'L' and 'S'). We achieve 75.90% accuracy on average when the task network is RSC [1]. Especially, the performance gain is 3.99% in the 'L' domain and 0.36% in the 'S' domain over RSC [1]. A similar performance gain is also observed when MLDG [18] is employed as the task network. Table 2 shows the effectiveness of the domain condition vector used in the auxiliary reweighting network. The domain in- formation assigned to the reweighting network creates more appropriate weights. The conditional domain vector makes the reweighting network to be aware that there are other types of imbalances depending on the source domains. In Fig. 3, we investigated the adaptive weight (blue line) and the accuracy (red line) for each class corresponding to the source domains at the same iteration (top row) and the number of training data for each class and domain (bottom row). Note that, the accuracy is measured before training step 1 and the adaptive weight is measured at training step 3. The results show that the domain-class imbalances degrade the performance in some classes and domains (green box in Fig. 3). The weight learned from the proposed reweighting network is inversely proportional to the accuracy, indicating that it emphasizes the loss to learn in a balanced manner. This demonstrates that the proposed method copes well with the domain-class imbalance problem.\n\n\nAnalysis\n\n\nCONCLUSION\n\nIn this work, we introduce a novel self-balanced domain generalization method to deal with the imbalanced distribution in terms of domains and classes. we predict adaptive weights of losses through the auxiliary reweighting network in the training phase to effectively balance the impact of training samples. Our method outperforms prior approaches especially when the source domains are largely imbalanced. Furthermore, we hope that this method can offer a new research direction by addressing the bias problem in the domain adaptation.\n\nFig. 1 .\n1Left: the classification result of each class on the VLCS dataset. Right: the number of samples in source domains. The variances of samples with respect to classes and domains, \u03c3 2 class and \u03c3 2 domain , are reported, where a large variance value implies severe imbalance of training data.\n\nAlgorithm 1\n1Self-Balanced Domain Generalization Hyperparameters: Mini-batch size n,m; max iteration T ; step size \u03b1, \u03b2 Input: Source training domains S; imbalanced training set S I ; balanced training set S B Output: Task parameter \u0398 (T ) 1: procedure TRAINING(S I ,S\n\nFig. 3 .\n3The accuracy (red line) and the adaptive weight (blue line) are depicted in the top row, and the number of images is shown in the bottom row for each class in three source domains on VLCS[22] dataset. The target domain is set to (a) V, (b) L, (c) C, and (d) S, respectively. We measure the accuracy before training step 1 and the weight at training step 3 (see Sec. 2.2).\n\nTable 1 .\n1Performance comparison on VLCS dataset with state-of-the-art methods. \u2020 denotes the combination of SBDG with MLDG[18] as the task network, and \u2021 denotes that the task network is RSC[1]. (\u00b7) denotes the normalized variance of source domains corresponding to the target domain.Method \nCaltech101 LabelMe SUN09 VOC2007 Avg. \n(0.016) \n(0.169) \n(0.160) \n(0.150) \nDeep-All [9] \n96.25 \n59.72 \n64.51 \n70.58 \n72.76 \nCIDDG [25] \n88.83 \n63.06 \n62.10 \n64.38 \n69.59 \nUndo-Bias [26] \n93.63 \n63.49 \n61.32 \n69.99 \n72.11 \nMMD-AAE [10] \n94.40 \n62.60 \n64.40 \n67.70 \n72.28 \nEpi-FCR [11] \n94.10 \n64.30 \n65.90 \n67.10 \n72.90 \nJiGen [9] \n96.93 \n60.90 \n64.30 \n70.62 \n73.19 \nMASF [24] \n94.78 \n64.90 \n67.64 \n69.14 \n74.11 \nMLDG [18] \n94.40 \n61.30 \n65.90 \n67.70 \n72.30 \nRSC [1] \n97.61 \n61.86 \n68.32 \n73.93 \n75.43 \nOurs  \u2020 with MLDG \n95.05 \n63.97 \n66.08 \n67.45 \n73.14 \nOurs  \u2021 with RSC \n96.47 \n65.85 \n68.68 \n72.59 \n75.90 \n\n\n\nTable 2 .\n2Ablation study for the effect of the conditional domain vector on VLCS dataset. We use RSC[1] as the task network.Domain vector Caltech101 LabelMe SUN09 VOC2007 Avg. \n\n95.97 \n62.31 \n67.58 \n71.34 \n74.30 \n\n96.47 \n65.85 \n68.68 \n72.59 \n75.90 \n\n\n\nSelf-challenging improves cross-domain generalization. Zeyi Huang, Haohan Wang, Eric P Xing, Dong Huang, ECCV. Zeyi Huang, Haohan Wang, Eric P. Xing, and Dong Huang, \"Self-challenging improves cross-domain gen- eralization,\" in ECCV, 2020.\n\nUnbiased look at dataset bias. Antonio Torralba, Alexei A Efros, CVPR. Antonio Torralba and Alexei A Efros, \"Unbiased look at dataset bias,\" in CVPR, 2011.\n\nUnsupervised domain adaptation by backpropagation. Yaroslav Ganin, Victor Lempitsky, ICML. Yaroslav Ganin and Victor Lempitsky, \"Unsupervised domain adaptation by backpropagation,\" in ICML, 2015.\n\nAnalysis of representations for domain adaptation. Shai Ben-David, John Blitzer, Koby Crammer, Fernando Pereira, NeurIPSShai Ben-David, John Blitzer, Koby Crammer, and Fer- nando Pereira, \"Analysis of representations for domain adaptation,\" in NeurIPS, 2007.\n\nReturn of frustratingly easy domain adaptation. Baochen Sun, Jiashi Feng, Kate Saenko, AAAI. Baochen Sun, Jiashi Feng, and Kate Saenko, \"Return of frustratingly easy domain adaptation,\" in AAAI, 2016.\n\nGeneralizing to unseen domains via adversarial data augmentation. Riccardo Volpi, Hongseok Namkoong, Ozan Sener, C John, Vittorio Duchi, Silvio Murino, Savarese, NeurIPSRiccardo Volpi, Hongseok Namkoong, Ozan Sener, John C Duchi, Vittorio Murino, and Silvio Savarese, \"Generalizing to unseen domains via adversarial data augmentation,\" in NeurIPS, 2018.\n\nDomain randomization and pyramid consistency: Simulation-to-real generalization without accessing target domain data. Xiangyu Yue, Yang Zhang, Sicheng Zhao, Alberto Sangiovanni-Vincentelli, Kurt Keutzer, Boqing Gong, ICCV. Xiangyu Yue, Yang Zhang, Sicheng Zhao, Alberto Sangiovanni-Vincentelli, Kurt Keutzer, and Boqing Gong, \"Domain randomization and pyramid consis- tency: Simulation-to-real generalization without access- ing target domain data,\" in ICCV, 2019.\n\nUnpaired image-to-image translation using cycle-consistent adversarial networks. Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A Efros, ICCV. Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros, \"Unpaired image-to-image translation using cycle-consistent adversarial networks,\" in ICCV, 2017.\n\nDomain generalization by solving jigsaw puzzles. M Fabio, Antonio D&apos; Carlucci, Silvia Innocente, Barbara Bucci, Tatiana Caputo, Tommasi, CVPR. Fabio M Carlucci, Antonio D'Innocente, Silvia Bucci, Barbara Caputo, and Tatiana Tommasi, \"Domain gener- alization by solving jigsaw puzzles,\" in CVPR, 2019.\n\nDomain generalization with adversarial feature learning. Haoliang Li, Shiqi Sinno Jialin Pan, Alex C Wang, Kot, CVPR. Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C Kot, \"Domain generalization with adversarial feature learning,\" in CVPR, 2018.\n\nEpisodic training for domain generalization. Da Li, Jianshu Zhang, Yongxin Yang, Cong Liu, Yi-Zhe Song, Timothy M Hospedales, ICCV. Da Li, Jianshu Zhang, Yongxin Yang, Cong Liu, Yi-Zhe Song, and Timothy M Hospedales, \"Episodic training for domain generalization,\" in ICCV, 2019.\n\nLearning to optimize domain specific normalization for domain generalization. Seonguk Seo, Yumin Suh, Dongwan Kim, Jongwoo Han, Bohyung Han, ECCVSeonguk Seo, Yumin Suh, Dongwan Kim, Jongwoo Han, and Bohyung Han, \"Learning to optimize do- main specific normalization for domain generalization,\" ECCV, 2020.\n\nDomain generalization using a mixture of multiple latent domains. Toshihiko Matsuura, Tatsuya Harada, AAAI. Toshihiko Matsuura and Tatsuya Harada, \"Domain gen- eralization using a mixture of multiple latent domains.,\" in AAAI, 2020.\n\nUnified deep supervised domain adaptation and generalization. Saeid Motiian, Marco Piccirilli, A Donald, Gianfranco Adjeroh, Doretto, ICCV. Saeid Motiian, Marco Piccirilli, Donald A Adjeroh, and Gianfranco Doretto, \"Unified deep supervised domain adaptation and generalization,\" in ICCV, 2017.\n\nGenerative moment matching networks. Yujia Li, Kevin Swersky, Rich Zemel, ICML. Yujia Li, Kevin Swersky, and Rich Zemel, \"Generative moment matching networks,\" in ICML, 2015.\n\nModel-agnostic meta-learning for fast adaptation of deep networks. Chelsea Finn, Pieter Abbeel, Sergey Levine, ICML. Chelsea Finn, Pieter Abbeel, and Sergey Levine, \"Model-agnostic meta-learning for fast adaptation of deep networks,\" in ICML, 2017.\n\nMetareg: Towards domain generalization using meta-regularization. Yogesh Balaji, Swami Sankaranarayanan, Rama Chellappa, NeurIPSYogesh Balaji, Swami Sankaranarayanan, and Rama Chellappa, \"Metareg: Towards domain generalization using meta-regularization,\" in NeurIPS, 2018.\n\nLearning to generalize: Meta-learning for domain generalization. Da Li, Yongxin Yang, Yi-Zhe Song, Timothy Hospedales, AAAIDa Li, Yongxin Yang, Yi-Zhe Song, and Timothy Hospedales, \"Learning to generalize: Meta-learning for domain generalization,\" in AAAI, 2018.\n\nMeta-weight-net: Learning an explicit mapping for sample weighting. Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, Deyu Meng, NeurIPSJun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng, \"Meta-weight-net: Learning an explicit mapping for sample weighting,\" in NeurIPS, 2019.\n\nLearning attributes equals multi-source domain generalization. Chuang Gan, Tianbao Yang, Boqing Gong, CVPR. Chuang Gan, Tianbao Yang, and Boqing Gong, \"Learn- ing attributes equals multi-source domain generaliza- tion,\" in CVPR, 2016.\n\nMultisource domain adaptation for semantic segmentation. Sicheng Zhao, Bo Li, Xiangyu Yue, Yang Gu, Pengfei Xu, Runbo Hu, Hua Chai, Kurt Keutzer, NeurIPSSicheng Zhao, Bo Li, Xiangyu Yue, Yang Gu, Pengfei Xu, Runbo Hu, Hua Chai, and Kurt Keutzer, \"Multi- source domain adaptation for semantic segmentation,\" in NeurIPS, 2019.\n\nUnbiased metric learning: On the utilization of multiple datasets and web images for softening bias. Chen Fang, Ye Xu, Daniel N Rockmore, ICCV. Chen Fang, Ye Xu, and Daniel N Rockmore, \"Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias,\" in ICCV, 2013.\n\nDomain generalization for object recognition with multi-task autoencoders. Muhammad Ghifary, Mengjie Bastiaan Kleijn, David Zhang, Balduzzi, ICCV. Muhammad Ghifary, W Bastiaan Kleijn, Mengjie Zhang, and David Balduzzi, \"Domain generalization for object recognition with multi-task autoencoders,\" in ICCV, 2015.\n\nDomain generalization via model-agnostic learning of semantic features. Qi Dou, Daniel Coelho De Castro, Konstantinos Kamnitsas, Ben Glocker, NeurIPSQi Dou, Daniel Coelho de Castro, Konstantinos Kam- nitsas, and Ben Glocker, \"Domain generalization via model-agnostic learning of semantic features,\" in NeurIPS, 2019.\n\nDeep domain generalization via conditional invariant adversarial networks. Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, Dacheng Tao, ECCV. Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao, \"Deep domain generalization via conditional invariant adver- sarial networks,\" in ECCV, 2018.\n\nDeeper, broader and artier domain generalization. Da Li, Yongxin Yang, Yi-Zhe Song, Timothy M Hospedales, ICCV. Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales, \"Deeper, broader and artier domain gen- eralization,\" in ICCV, 2017.\n", "annotations": {"author": "[{\"end\":60,\"start\":52},{\"end\":73,\"start\":61},{\"end\":86,\"start\":74},{\"end\":180,\"start\":87},{\"end\":216,\"start\":181},{\"end\":295,\"start\":217}]", "publisher": null, "author_last_name": "[{\"end\":59,\"start\":56},{\"end\":72,\"start\":69},{\"end\":85,\"start\":81},{\"end\":97,\"start\":94},{\"end\":195,\"start\":191}]", "author_first_name": "[{\"end\":55,\"start\":52},{\"end\":68,\"start\":61},{\"end\":80,\"start\":74},{\"end\":93,\"start\":87},{\"end\":190,\"start\":181}]", "author_affiliation": "[{\"end\":179,\"start\":99},{\"end\":294,\"start\":218}]", "title": "[{\"end\":49,\"start\":1},{\"end\":344,\"start\":296}]", "venue": null, "abstract": "[{\"end\":1445,\"start\":425}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1840,\"start\":1837},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1842,\"start\":1840},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1844,\"start\":1842},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1846,\"start\":1844},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1848,\"start\":1846},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2238,\"start\":2235},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2378,\"start\":2375},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2406,\"start\":2403},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2466,\"start\":2463},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2962,\"start\":2958},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2965,\"start\":2962},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2968,\"start\":2965},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2971,\"start\":2968},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2974,\"start\":2971},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3044,\"start\":3040},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3164,\"start\":3160},{\"end\":3425,\"start\":3422},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4061,\"start\":4057},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4143,\"start\":4139},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4193,\"start\":4189},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4196,\"start\":4193},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4199,\"start\":4196},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4707,\"start\":4703},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5247,\"start\":5243},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5665,\"start\":5661},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7338,\"start\":7334},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7369,\"start\":7365},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7766,\"start\":7762},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7777,\"start\":7774},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8686,\"start\":8682},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10144,\"start\":10140},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10698,\"start\":10694},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":11007,\"start\":11003},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11482,\"start\":11479},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11484,\"start\":11482},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11683,\"start\":11679},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11699,\"start\":11696},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11710,\"start\":11706},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11819,\"start\":11816},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12009,\"start\":12005},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12141,\"start\":12137},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":12319,\"start\":12316},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":12482,\"start\":12478},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":12585,\"start\":12581},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":13228,\"start\":13225},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":13330,\"start\":13327},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":13390,\"start\":13386},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":15854,\"start\":15850},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":16164,\"start\":16160},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":16231,\"start\":16228},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":17046,\"start\":17043}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":15381,\"start\":15081},{\"attributes\":{\"id\":\"fig_1\"},\"end\":15651,\"start\":15382},{\"attributes\":{\"id\":\"fig_2\"},\"end\":16034,\"start\":15652},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":16940,\"start\":16035},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":17193,\"start\":16941}]", "paragraph": "[{\"end\":1849,\"start\":1461},{\"end\":2818,\"start\":1851},{\"end\":3265,\"start\":2820},{\"end\":3452,\"start\":3415},{\"end\":3469,\"start\":3463},{\"end\":3990,\"start\":3528},{\"end\":4968,\"start\":3992},{\"end\":6015,\"start\":4974},{\"end\":6345,\"start\":6055},{\"end\":6526,\"start\":6373},{\"end\":7401,\"start\":6528},{\"end\":7556,\"start\":7414},{\"end\":8106,\"start\":7576},{\"end\":8279,\"start\":8108},{\"end\":8941,\"start\":8319},{\"end\":9128,\"start\":8976},{\"end\":9234,\"start\":9185},{\"end\":9409,\"start\":9236},{\"end\":9667,\"start\":9464},{\"end\":9815,\"start\":9669},{\"end\":10197,\"start\":9873},{\"end\":10248,\"start\":10204},{\"end\":10287,\"start\":10250},{\"end\":10291,\"start\":10289},{\"end\":10422,\"start\":10419},{\"end\":10620,\"start\":10580},{\"end\":11567,\"start\":10646},{\"end\":12483,\"start\":11569},{\"end\":14517,\"start\":12495},{\"end\":15080,\"start\":14543}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":3299,\"start\":3266},{\"attributes\":{\"id\":\"formula_1\"},\"end\":3332,\"start\":3299},{\"attributes\":{\"id\":\"formula_2\"},\"end\":3405,\"start\":3332},{\"attributes\":{\"id\":\"formula_3\"},\"end\":3518,\"start\":3470},{\"attributes\":{\"id\":\"formula_4\"},\"end\":4973,\"start\":4969},{\"attributes\":{\"id\":\"formula_5\"},\"end\":6372,\"start\":6346},{\"attributes\":{\"id\":\"formula_6\"},\"end\":7575,\"start\":7557},{\"attributes\":{\"id\":\"formula_7\"},\"end\":8318,\"start\":8280},{\"attributes\":{\"id\":\"formula_8\"},\"end\":8975,\"start\":8942},{\"attributes\":{\"id\":\"formula_9\"},\"end\":9184,\"start\":9129},{\"attributes\":{\"id\":\"formula_10\"},\"end\":9463,\"start\":9410},{\"attributes\":{\"id\":\"formula_11\"},\"end\":9872,\"start\":9816},{\"attributes\":{\"id\":\"formula_12\"},\"end\":10418,\"start\":10292},{\"attributes\":{\"id\":\"formula_13\"},\"end\":10573,\"start\":10423}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":12505,\"start\":12498},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":13034,\"start\":13027},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":13431,\"start\":13424}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1459,\"start\":1447},{\"end\":3413,\"start\":3407},{\"end\":3461,\"start\":3455},{\"end\":3526,\"start\":3520},{\"attributes\":{\"n\":\"2.\"},\"end\":6033,\"start\":6018},{\"attributes\":{\"n\":\"2.1.\"},\"end\":6053,\"start\":6036},{\"attributes\":{\"n\":\"2.2.\"},\"end\":7412,\"start\":7404},{\"end\":10202,\"start\":10200},{\"end\":10578,\"start\":10575},{\"attributes\":{\"n\":\"3.1.\"},\"end\":10644,\"start\":10623},{\"attributes\":{\"n\":\"3.2.\"},\"end\":12493,\"start\":12486},{\"attributes\":{\"n\":\"3.3.\"},\"end\":14528,\"start\":14520},{\"attributes\":{\"n\":\"4.\"},\"end\":14541,\"start\":14531},{\"end\":15090,\"start\":15082},{\"end\":15394,\"start\":15383},{\"end\":15661,\"start\":15653},{\"end\":16045,\"start\":16036},{\"end\":16951,\"start\":16942}]", "table": "[{\"end\":16940,\"start\":16322},{\"end\":17193,\"start\":17067}]", "figure_caption": "[{\"end\":15381,\"start\":15092},{\"end\":15651,\"start\":15396},{\"end\":16034,\"start\":15663},{\"end\":16322,\"start\":16047},{\"end\":17067,\"start\":16953}]", "figure_ref": "[{\"end\":3587,\"start\":3581},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4741,\"start\":4735},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4968,\"start\":4962},{\"end\":5825,\"start\":5819},{\"end\":6696,\"start\":6690},{\"end\":7725,\"start\":7719},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":13780,\"start\":13774},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":14250,\"start\":14244}]", "bib_author_first_name": "[{\"end\":17254,\"start\":17250},{\"end\":17268,\"start\":17262},{\"end\":17279,\"start\":17275},{\"end\":17281,\"start\":17280},{\"end\":17292,\"start\":17288},{\"end\":17474,\"start\":17467},{\"end\":17491,\"start\":17485},{\"end\":17493,\"start\":17492},{\"end\":17652,\"start\":17644},{\"end\":17666,\"start\":17660},{\"end\":17845,\"start\":17841},{\"end\":17861,\"start\":17857},{\"end\":17875,\"start\":17871},{\"end\":17893,\"start\":17885},{\"end\":18105,\"start\":18098},{\"end\":18117,\"start\":18111},{\"end\":18128,\"start\":18124},{\"end\":18326,\"start\":18318},{\"end\":18342,\"start\":18334},{\"end\":18357,\"start\":18353},{\"end\":18366,\"start\":18365},{\"end\":18381,\"start\":18373},{\"end\":18395,\"start\":18389},{\"end\":18732,\"start\":18725},{\"end\":18742,\"start\":18738},{\"end\":18757,\"start\":18750},{\"end\":18771,\"start\":18764},{\"end\":18801,\"start\":18797},{\"end\":18817,\"start\":18811},{\"end\":19161,\"start\":19154},{\"end\":19174,\"start\":19167},{\"end\":19188,\"start\":19181},{\"end\":19202,\"start\":19196},{\"end\":19204,\"start\":19203},{\"end\":19429,\"start\":19428},{\"end\":19452,\"start\":19437},{\"end\":19469,\"start\":19463},{\"end\":19488,\"start\":19481},{\"end\":19503,\"start\":19496},{\"end\":19751,\"start\":19743},{\"end\":19761,\"start\":19756},{\"end\":19784,\"start\":19780},{\"end\":19786,\"start\":19785},{\"end\":19985,\"start\":19983},{\"end\":19997,\"start\":19990},{\"end\":20012,\"start\":20005},{\"end\":20023,\"start\":20019},{\"end\":20035,\"start\":20029},{\"end\":20049,\"start\":20042},{\"end\":20051,\"start\":20050},{\"end\":20303,\"start\":20296},{\"end\":20314,\"start\":20309},{\"end\":20327,\"start\":20320},{\"end\":20340,\"start\":20333},{\"end\":20353,\"start\":20346},{\"end\":20600,\"start\":20591},{\"end\":20618,\"start\":20611},{\"end\":20826,\"start\":20821},{\"end\":20841,\"start\":20836},{\"end\":20855,\"start\":20854},{\"end\":20874,\"start\":20864},{\"end\":21096,\"start\":21091},{\"end\":21106,\"start\":21101},{\"end\":21120,\"start\":21116},{\"end\":21304,\"start\":21297},{\"end\":21317,\"start\":21311},{\"end\":21332,\"start\":21326},{\"end\":21552,\"start\":21546},{\"end\":21566,\"start\":21561},{\"end\":21589,\"start\":21585},{\"end\":21821,\"start\":21819},{\"end\":21833,\"start\":21826},{\"end\":21846,\"start\":21840},{\"end\":21860,\"start\":21853},{\"end\":22089,\"start\":22086},{\"end\":22097,\"start\":22095},{\"end\":22109,\"start\":22103},{\"end\":22118,\"start\":22114},{\"end\":22132,\"start\":22125},{\"end\":22146,\"start\":22139},{\"end\":22155,\"start\":22151},{\"end\":22407,\"start\":22401},{\"end\":22420,\"start\":22413},{\"end\":22433,\"start\":22427},{\"end\":22638,\"start\":22631},{\"end\":22647,\"start\":22645},{\"end\":22659,\"start\":22652},{\"end\":22669,\"start\":22665},{\"end\":22681,\"start\":22674},{\"end\":22691,\"start\":22686},{\"end\":22699,\"start\":22696},{\"end\":22710,\"start\":22706},{\"end\":23005,\"start\":23001},{\"end\":23014,\"start\":23012},{\"end\":23025,\"start\":23019},{\"end\":23027,\"start\":23026},{\"end\":23287,\"start\":23279},{\"end\":23304,\"start\":23297},{\"end\":23327,\"start\":23322},{\"end\":23590,\"start\":23588},{\"end\":23602,\"start\":23596},{\"end\":23633,\"start\":23621},{\"end\":23648,\"start\":23645},{\"end\":23911,\"start\":23909},{\"end\":23922,\"start\":23916},{\"end\":23937,\"start\":23929},{\"end\":23950,\"start\":23944},{\"end\":23965,\"start\":23956},{\"end\":23974,\"start\":23971},{\"end\":23989,\"start\":23982},{\"end\":24238,\"start\":24236},{\"end\":24250,\"start\":24243},{\"end\":24263,\"start\":24257},{\"end\":24277,\"start\":24270},{\"end\":24279,\"start\":24278}]", "bib_author_last_name": "[{\"end\":17260,\"start\":17255},{\"end\":17273,\"start\":17269},{\"end\":17286,\"start\":17282},{\"end\":17298,\"start\":17293},{\"end\":17483,\"start\":17475},{\"end\":17499,\"start\":17494},{\"end\":17658,\"start\":17653},{\"end\":17676,\"start\":17667},{\"end\":17855,\"start\":17846},{\"end\":17869,\"start\":17862},{\"end\":17883,\"start\":17876},{\"end\":17901,\"start\":17894},{\"end\":18109,\"start\":18106},{\"end\":18122,\"start\":18118},{\"end\":18135,\"start\":18129},{\"end\":18332,\"start\":18327},{\"end\":18351,\"start\":18343},{\"end\":18363,\"start\":18358},{\"end\":18371,\"start\":18367},{\"end\":18387,\"start\":18382},{\"end\":18402,\"start\":18396},{\"end\":18412,\"start\":18404},{\"end\":18736,\"start\":18733},{\"end\":18748,\"start\":18743},{\"end\":18762,\"start\":18758},{\"end\":18795,\"start\":18772},{\"end\":18809,\"start\":18802},{\"end\":18822,\"start\":18818},{\"end\":19165,\"start\":19162},{\"end\":19179,\"start\":19175},{\"end\":19194,\"start\":19189},{\"end\":19210,\"start\":19205},{\"end\":19435,\"start\":19430},{\"end\":19461,\"start\":19453},{\"end\":19479,\"start\":19470},{\"end\":19494,\"start\":19489},{\"end\":19510,\"start\":19504},{\"end\":19519,\"start\":19512},{\"end\":19754,\"start\":19752},{\"end\":19778,\"start\":19762},{\"end\":19791,\"start\":19787},{\"end\":19796,\"start\":19793},{\"end\":19988,\"start\":19986},{\"end\":20003,\"start\":19998},{\"end\":20017,\"start\":20013},{\"end\":20027,\"start\":20024},{\"end\":20040,\"start\":20036},{\"end\":20062,\"start\":20052},{\"end\":20307,\"start\":20304},{\"end\":20318,\"start\":20315},{\"end\":20331,\"start\":20328},{\"end\":20344,\"start\":20341},{\"end\":20357,\"start\":20354},{\"end\":20609,\"start\":20601},{\"end\":20625,\"start\":20619},{\"end\":20834,\"start\":20827},{\"end\":20852,\"start\":20842},{\"end\":20862,\"start\":20856},{\"end\":20882,\"start\":20875},{\"end\":20891,\"start\":20884},{\"end\":21099,\"start\":21097},{\"end\":21114,\"start\":21107},{\"end\":21126,\"start\":21121},{\"end\":21309,\"start\":21305},{\"end\":21324,\"start\":21318},{\"end\":21339,\"start\":21333},{\"end\":21559,\"start\":21553},{\"end\":21583,\"start\":21567},{\"end\":21599,\"start\":21590},{\"end\":21824,\"start\":21822},{\"end\":21838,\"start\":21834},{\"end\":21851,\"start\":21847},{\"end\":21871,\"start\":21861},{\"end\":22093,\"start\":22090},{\"end\":22101,\"start\":22098},{\"end\":22112,\"start\":22110},{\"end\":22123,\"start\":22119},{\"end\":22137,\"start\":22133},{\"end\":22149,\"start\":22147},{\"end\":22160,\"start\":22156},{\"end\":22411,\"start\":22408},{\"end\":22425,\"start\":22421},{\"end\":22438,\"start\":22434},{\"end\":22643,\"start\":22639},{\"end\":22650,\"start\":22648},{\"end\":22663,\"start\":22660},{\"end\":22672,\"start\":22670},{\"end\":22684,\"start\":22682},{\"end\":22694,\"start\":22692},{\"end\":22704,\"start\":22700},{\"end\":22718,\"start\":22711},{\"end\":23010,\"start\":23006},{\"end\":23017,\"start\":23015},{\"end\":23036,\"start\":23028},{\"end\":23295,\"start\":23288},{\"end\":23320,\"start\":23305},{\"end\":23333,\"start\":23328},{\"end\":23343,\"start\":23335},{\"end\":23594,\"start\":23591},{\"end\":23619,\"start\":23603},{\"end\":23643,\"start\":23634},{\"end\":23656,\"start\":23649},{\"end\":23914,\"start\":23912},{\"end\":23927,\"start\":23923},{\"end\":23942,\"start\":23938},{\"end\":23954,\"start\":23951},{\"end\":23969,\"start\":23966},{\"end\":23980,\"start\":23975},{\"end\":23993,\"start\":23990},{\"end\":24241,\"start\":24239},{\"end\":24255,\"start\":24251},{\"end\":24268,\"start\":24264},{\"end\":24290,\"start\":24280}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":220363892},\"end\":17434,\"start\":17195},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":2777306},\"end\":17591,\"start\":17436},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":6755881},\"end\":17788,\"start\":17593},{\"attributes\":{\"id\":\"b3\"},\"end\":18048,\"start\":17790},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":16439870},\"end\":18250,\"start\":18050},{\"attributes\":{\"id\":\"b5\"},\"end\":18605,\"start\":18252},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":202540251},\"end\":19071,\"start\":18607},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":195944196},\"end\":19377,\"start\":19073},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":81978372},\"end\":19684,\"start\":19379},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":52833113},\"end\":19936,\"start\":19686},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":59553457},\"end\":20216,\"start\":19938},{\"attributes\":{\"id\":\"b11\"},\"end\":20523,\"start\":20218},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":208138410},\"end\":20757,\"start\":20525},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":3172259},\"end\":21052,\"start\":20759},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":536962},\"end\":21228,\"start\":21054},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":6719686},\"end\":21478,\"start\":21230},{\"attributes\":{\"id\":\"b16\"},\"end\":21752,\"start\":21480},{\"attributes\":{\"id\":\"b17\"},\"end\":22016,\"start\":21754},{\"attributes\":{\"id\":\"b18\"},\"end\":22336,\"start\":22018},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":2472547},\"end\":22572,\"start\":22338},{\"attributes\":{\"id\":\"b20\"},\"end\":22898,\"start\":22574},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":722896},\"end\":23202,\"start\":22900},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":12825123},\"end\":23514,\"start\":23204},{\"attributes\":{\"id\":\"b23\"},\"end\":23832,\"start\":23516},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":52956008},\"end\":24184,\"start\":23834},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":6037691},\"end\":24426,\"start\":24186}]", "bib_title": "[{\"end\":17248,\"start\":17195},{\"end\":17465,\"start\":17436},{\"end\":17642,\"start\":17593},{\"end\":18096,\"start\":18050},{\"end\":18723,\"start\":18607},{\"end\":19152,\"start\":19073},{\"end\":19426,\"start\":19379},{\"end\":19741,\"start\":19686},{\"end\":19981,\"start\":19938},{\"end\":20589,\"start\":20525},{\"end\":20819,\"start\":20759},{\"end\":21089,\"start\":21054},{\"end\":21295,\"start\":21230},{\"end\":22399,\"start\":22338},{\"end\":22999,\"start\":22900},{\"end\":23277,\"start\":23204},{\"end\":23907,\"start\":23834},{\"end\":24234,\"start\":24186}]", "bib_author": "[{\"end\":17262,\"start\":17250},{\"end\":17275,\"start\":17262},{\"end\":17288,\"start\":17275},{\"end\":17300,\"start\":17288},{\"end\":17485,\"start\":17467},{\"end\":17501,\"start\":17485},{\"end\":17660,\"start\":17644},{\"end\":17678,\"start\":17660},{\"end\":17857,\"start\":17841},{\"end\":17871,\"start\":17857},{\"end\":17885,\"start\":17871},{\"end\":17903,\"start\":17885},{\"end\":18111,\"start\":18098},{\"end\":18124,\"start\":18111},{\"end\":18137,\"start\":18124},{\"end\":18334,\"start\":18318},{\"end\":18353,\"start\":18334},{\"end\":18365,\"start\":18353},{\"end\":18373,\"start\":18365},{\"end\":18389,\"start\":18373},{\"end\":18404,\"start\":18389},{\"end\":18414,\"start\":18404},{\"end\":18738,\"start\":18725},{\"end\":18750,\"start\":18738},{\"end\":18764,\"start\":18750},{\"end\":18797,\"start\":18764},{\"end\":18811,\"start\":18797},{\"end\":18824,\"start\":18811},{\"end\":19167,\"start\":19154},{\"end\":19181,\"start\":19167},{\"end\":19196,\"start\":19181},{\"end\":19212,\"start\":19196},{\"end\":19437,\"start\":19428},{\"end\":19463,\"start\":19437},{\"end\":19481,\"start\":19463},{\"end\":19496,\"start\":19481},{\"end\":19512,\"start\":19496},{\"end\":19521,\"start\":19512},{\"end\":19756,\"start\":19743},{\"end\":19780,\"start\":19756},{\"end\":19793,\"start\":19780},{\"end\":19798,\"start\":19793},{\"end\":19990,\"start\":19983},{\"end\":20005,\"start\":19990},{\"end\":20019,\"start\":20005},{\"end\":20029,\"start\":20019},{\"end\":20042,\"start\":20029},{\"end\":20064,\"start\":20042},{\"end\":20309,\"start\":20296},{\"end\":20320,\"start\":20309},{\"end\":20333,\"start\":20320},{\"end\":20346,\"start\":20333},{\"end\":20359,\"start\":20346},{\"end\":20611,\"start\":20591},{\"end\":20627,\"start\":20611},{\"end\":20836,\"start\":20821},{\"end\":20854,\"start\":20836},{\"end\":20864,\"start\":20854},{\"end\":20884,\"start\":20864},{\"end\":20893,\"start\":20884},{\"end\":21101,\"start\":21091},{\"end\":21116,\"start\":21101},{\"end\":21128,\"start\":21116},{\"end\":21311,\"start\":21297},{\"end\":21326,\"start\":21311},{\"end\":21341,\"start\":21326},{\"end\":21561,\"start\":21546},{\"end\":21585,\"start\":21561},{\"end\":21601,\"start\":21585},{\"end\":21826,\"start\":21819},{\"end\":21840,\"start\":21826},{\"end\":21853,\"start\":21840},{\"end\":21873,\"start\":21853},{\"end\":22095,\"start\":22086},{\"end\":22103,\"start\":22095},{\"end\":22114,\"start\":22103},{\"end\":22125,\"start\":22114},{\"end\":22139,\"start\":22125},{\"end\":22151,\"start\":22139},{\"end\":22162,\"start\":22151},{\"end\":22413,\"start\":22401},{\"end\":22427,\"start\":22413},{\"end\":22440,\"start\":22427},{\"end\":22645,\"start\":22631},{\"end\":22652,\"start\":22645},{\"end\":22665,\"start\":22652},{\"end\":22674,\"start\":22665},{\"end\":22686,\"start\":22674},{\"end\":22696,\"start\":22686},{\"end\":22706,\"start\":22696},{\"end\":22720,\"start\":22706},{\"end\":23012,\"start\":23001},{\"end\":23019,\"start\":23012},{\"end\":23038,\"start\":23019},{\"end\":23297,\"start\":23279},{\"end\":23322,\"start\":23297},{\"end\":23335,\"start\":23322},{\"end\":23345,\"start\":23335},{\"end\":23596,\"start\":23588},{\"end\":23621,\"start\":23596},{\"end\":23645,\"start\":23621},{\"end\":23658,\"start\":23645},{\"end\":23916,\"start\":23909},{\"end\":23929,\"start\":23916},{\"end\":23944,\"start\":23929},{\"end\":23956,\"start\":23944},{\"end\":23971,\"start\":23956},{\"end\":23982,\"start\":23971},{\"end\":23995,\"start\":23982},{\"end\":24243,\"start\":24236},{\"end\":24257,\"start\":24243},{\"end\":24270,\"start\":24257},{\"end\":24292,\"start\":24270}]", "bib_venue": "[{\"end\":17304,\"start\":17300},{\"end\":17505,\"start\":17501},{\"end\":17682,\"start\":17678},{\"end\":17839,\"start\":17790},{\"end\":18141,\"start\":18137},{\"end\":18316,\"start\":18252},{\"end\":18828,\"start\":18824},{\"end\":19216,\"start\":19212},{\"end\":19525,\"start\":19521},{\"end\":19802,\"start\":19798},{\"end\":20068,\"start\":20064},{\"end\":20294,\"start\":20218},{\"end\":20631,\"start\":20627},{\"end\":20897,\"start\":20893},{\"end\":21132,\"start\":21128},{\"end\":21345,\"start\":21341},{\"end\":21544,\"start\":21480},{\"end\":21817,\"start\":21754},{\"end\":22084,\"start\":22018},{\"end\":22444,\"start\":22440},{\"end\":22629,\"start\":22574},{\"end\":23042,\"start\":23038},{\"end\":23349,\"start\":23345},{\"end\":23586,\"start\":23516},{\"end\":23999,\"start\":23995},{\"end\":24296,\"start\":24292}]"}}}, "year": 2023, "month": 12, "day": 17}