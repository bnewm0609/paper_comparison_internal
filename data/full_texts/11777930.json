{"id": 11777930, "updated": "2023-12-11 05:43:27.577", "metadata": {"title": "Ieee Transactions on Pattern Analysis and Machine Intelligence Classification with Noisy Labels by Importance Reweighting", "authors": "[{\"first\":\"Tongliang\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Dacheng\",\"last\":\"Tao\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2014, "month": 11, "day": 27}, "abstract": "\u2014In this paper, we study a classification problem in which sample labels are randomly corrupted. In this scenario, there is an unobservable sample with noise-free labels. However, before being observed, the true labels are independently flipped with a probability \u03c1 \u2208 [0, 0.5), and the random label noise can be class-conditional. Here, we address two fundamental problems raised by this scenario. The first is how to best use the abundant surrogate loss functions designed for the traditional classification problem when there is label noise. We prove that any surrogate loss function can be used for classification with noisy labels by using importance reweighting, with consistency assurance that the label noise does not ultimately hinder the search for the optimal classifier of the noise-free sample. The other is the open problem of how to obtain the noise rate \u03c1. We show that the rate is upper bounded by the conditional probability P (\u02c6 Y |X) of the noisy sample. Consequently, the rate can be estimated, because the upper bound can be easily reached in classification problems. Experimental results on synthetic and real datasets confirm the efficiency of our methods.", "fields_of_study": "[\"Mathematics\",\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": null, "doi": "10.1109/tpami.2015.2456899"}}, "content": {"source": {"pdf_hash": "0bda6a2b5ed513048fdfc7d09ff4db67767e1d9d", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1411.7718v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1411.7718", "status": "GREEN"}}, "grobid": {"id": "ebf2bb96a370fd4d1932efcf947d6b49f71fde40", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/0bda6a2b5ed513048fdfc7d09ff4db67767e1d9d.txt", "contents": "\nClassification with Noisy Labels by Importance Reweighting\n27 September 2014\n\nTongliang Liu tliang.liu@gmail.com \nCentre for Quantum Computation & Intelligent Systems Faculty of Engineering & Information Technology\nUniversity of Technology Sydney\n81-115 BroadwayUltimoNSWAustralia\n\nDacheng Tao dacheng.tao@uts.edu.au \nCentre for Quantum Computation & Intelligent Systems Faculty of Engineering & Information Technology\nUniversity of Technology Sydney\n81-115 BroadwayUltimoNSWAustralia\n\nClassification with Noisy Labels by Importance Reweighting\n27 September 2014\nIn this paper, we study a classification problem in which sample labels are randomly corrupted. In this scenario, there is an unobservable sample with noise-free labels. However, before being observed, the true labels are independently flipped with a probability \u03c1 \u2208 [0, 0.5), and the random label noise can be classconditional. Here, we address two fundamental problems raised by this scenario. The first is how to best use the abundant surrogate loss functions designed for the traditional classification problem when there is label noise. We prove that any surrogate loss function can be used for classification with noisy labels by using importance reweighting, with consistency assurance that the label noise does not ultimately hinder the search for the optimal classifier of the noise-free sample. The other is the open problem of how to obtain the noise rate \u03c1. We show that the rate is upper bounded by the conditional probability P (y|x) of the noisy sample. Consequently, the rate can be estimated, because the upper bound can be easily reached in classification problems. Experimental results on synthetic and real datasets confirm the efficiency of our methods.\n\nIntroduction\n\nClassification crucially relies on the accuracy of the dataset labels. In some situations, observation labels are easily corrupted and, therefore, inaccurate. Designing learning algorithms that account for noisy labeled data is therefore of great practical importance and has attracted a significant amount of interest in the machine learning community.\n\nThe random classification noise (RCN), in which each label is flipped independently with a probability \u03c1 \u2208 [0, 0.5), has been proposed; it was proven to be PAC-learnable by Angluin and Laird (1988) soon after the noise-free PAC learning model was introduced by Valiant (1984). Many related works then followed: Kearns (1998) proposed the statistical query model to learn with RCN. The restriction he enforced is that learning be based not on the particular properties of individual random examples, but instead on the global statistical properties of large samples. Such an approach to learning seems intuitively more robust. Lawrence and Sch\u00f6lkopf (2001) proposed a Bayesian model for this noise and applied it to sky moving in images. Biggio et al (2011) enabled support vector machine learning with RCN via a kernel matrix correction. And Yang et al (2012) developed multiple kernel learning for classification with noisy labels using stochastic programming. The interested reader is referred to further examples in the survey (Fr\u00e9nay and Verleysen 2014). However, most of these algorithms are designed for specific surrogate loss functions, and the use and benefit of the large number of surrogate loss functions designed for the traditional (noise-free) classification problem is important to investigate to solve classification problems in the presence of label noise. Aslam and Decatur (1996) proved that the RCN exploited using a 0-1 loss function is PAC-learnable if the function class is of finite VC-dimension. Manwani and Sastry (2013) analyzed the tolerance properties of RCN for risk minimization under several frequently used surrogate loss functions and showed that many of them do not tolerate RCN. Natarajan et al (2013) reported two methods for learning asymmetric RCN models, in which the random label noise is class-conditional. Their methods exploit many different classificationcalibrated surrogate loss functions (Bartlett et al 2006): the first model uses unbiased estimators of surrogate loss functions for empirical risk minimization, but the unbiased estimator may be non-convex, even if the original surrogate loss function is convex; their second method uses label-dependent costs. This latter approach is notable because it can be applied to all the convex and classification-calibrated surrogate loss functions, but this modification is based on the asymmetric classification-calibrated results (Scott 2012) and cannot be used to improve the performance of symmetric RCN problems.\n\nTherefore, to use and benefit from the abundant surrogate loss functions designed for the traditional classification problems, here we propose an importance reweighting method in which any surrogate loss function designed for a traditional classification problem can be used for classification with noisy labels. In our method, the weights are non-negative, so the convexity of objective functions does not change. In addition, our method inherits most batch learning optimization procedures designed for traditional classification problems with different regularizations; see, for examples, (Belkin et al 2006;Zou and Hastie 2005;Pong et al 2010;Nesterov 2005;Gong et al 2013).\n\nAlthough many works have focused on the RCN model, how to best estimate the noise rate \u03c1 remains an open problem (Yang et al 2012) and severely limits the practical application of the existing algorithms. Most previous works make the assumption that the noise rate is known or learn it using cross-validation, which is time-consuming and lacks a guarantee of generalization accuracy. To our best knowledge, the only work that tries to learn the unknown noise rate was proposed by Scott et al (2013). They provided estimators for the noise rate, however, no efficient stochastic programming algorithm has been designed for optimizing the estimators. In this paper, we set the noise rate to be asymmetric and unknown and denote the flip probabilities of positive and negative labels by \u03c1 +1 and \u03c1 \u22121 , respectively. We show that the noise rate \u03c1 +1 (or \u03c1 \u22121 ) is upper bounded by the conditional probability P (\u22121|x) (or P (+1|x)) of the noisy data. Moreover, the upper bound can be reached if there exists x \u2208 X such that the probability P (+1|x) (or P (\u22121|x)) of the \"clean\" sample is zero, which is very likely to hold for classification problems. The noise rates \u03c1 \u00b11 are therefore estimated by finding the minimal P (\u22131|x) of the noisy training sample. Kearns and Li (1993) introduced the malicious noise (MN) model, in which an adversary can access the sample and randomly replace a fraction of them with adversarial ones. It has been proven that any nontrivial target function class cannot be PAC learned with accuracy and malicious noise rate \u03b7 \u2265 (1 + ); see, for examples, (Kearns and Li 1993;Cesa-Bianchi et al 1999;Bshouty et al 2002). Long and Servedio (2011) proved that an algorithm for learning \u03b3-margin half-spaces that minimizes a convex surrogate loss function for misclassification risk cannot tolerate malicious noise at a rate greater than O( \u03b3). They therefore proposed an algorithm, that does not optimize a convex loss function and that can tolerate a higher rate of malicious noise than order O( \u03b3). Further details about the MN model can be found in (Klivans et al 2009).\n\n\nRelated works\n\nPerceptron algorithms that tolerate RCN have also been widely studied; see, for examples, (Bylander 1994;Cohen 1997;Blum et al 1998;Stempfel and Ralaivola 2007). Cesa-Bianchi et al (2011) considered a more complicated model in which the features and labels are both added with zero-mean and variancebounded noise. They used unbiased estimates of the gradient of the surrogate loss function to learn from the noisy sample in an online learning setting. See Khardon and Wachman (2007) for a survey of noise-tolerant variants of perceptron algorithms.\n\nAs well as these model-motivated algorithms, many algorithms that exploit robust surrogate loss functions have been designed for learning with any kind of feature and label noise. Robust surrogate loss functions, such as the Cauchy loss (Moore 1977) function and correntropy (Welsch loss function) (Liu et al 2007;He et al 2011) have been empirically proven to be robust to noise. Some other algorithms, such as confidence weighted learning (Crammer and Lee 2010), have also been proposed for noise-tolerant learning.\n\nThe rest of this paper is organized as follows. The problem is set up in Section 2. Section 3 presents some useful results applied to the traditional classification problem. In Section 4, we discuss how to perform classification in the presence of RCN and benefit from the abundant surrogate loss functions and algorithms designed for the traditional classification problem. In Section 5, we discuss how to reduce the uncertainty introduced by RCN by estimating the conditional probability P (y|x) of the noisy sample; theoretical guarantees for the consistency of the learned classifiers are also provided. In Section 6, an approach for estimating the value of the noise rate is proposed. We present the proofs of our assertions in Section 7 and discuss the convergence rate in Section 8. In Section 9, we present experimental results on synthetic and benchmark datasets, before concluding in Section 10.\n\n\nProblem setup\n\nLet D be the distribution of a pair of random variables (x, y) \u2208 X \u00d7 {\u00b11}, where X \u2286 R m . Our goal is to predict a label for any given observation x \u2208 X using a sample drawn i.i.d. from the distribution D. However, in many real-world classification problems, sample labels are randomly corrupted. We therefore consider the asymmetric RCN model (see (Natarajan et al 2013)). Let (x 1 , y 1 ), . . . , (x n , y n ) be an i.i.d. sample drawn from the distribution D and (x 1 ,\u0177 1 ), . . . , (x n ,\u0177 n ) the corresponding corrupted ones. The asymmetric RCN model is given by:\nP (\u0177 = +1|y = \u22121) = \u03c1 \u22121 , P (\u0177 = \u22121|y = +1) = \u03c1 +1 , and \u03c1 +1 + \u03c1 \u22121 \u2264 1.\nWe denote by D \u03c1 the distribution of the corrupted variables (x,\u0177). In our setting, the \"clean\" sample (x 1 , y 1 ), . . . , (x n , y n ) and the noise rates \u03c1 +1 and \u03c1 \u22121 are not available for learning algorithms. The noise rates and classifier are learned only by using the knowledge from the corrupted sample (x 1 ,\u0177 1 ), . . . , (x n ,\u0177 n ).\n\nSince no confusion may result, we will write P D\u03c1 (x, y), P D\u03c1 (y|x) and P D\u03c1 (y) instead of P D\u03c1 (x,\u0177), P D\u03c1 (\u0177|x) and P D\u03c1 (\u0177), respectively, for random variables (x,\u0177). However, for examples (x i ,\u0177 i ), i = 1, . . . , n, we will use the notations P D\u03c1 (x i ,\u0177 i ), P D\u03c1 (\u0177 i |x i ) and P D\u03c1 (\u0177 i ), i = 1, . . . , n, to denote the corresponding probabilities.\n\n\nThe traditional classification problem\n\nClassification is a fundamental machine learning problem. One intuitive way to learn the classifier is to find a decision function f \u2208 F , such that the expected risk R 1,D (f ) = E (x,y)\u223cD [1 sign(f (x)) =y ] is minimized, where F is the function class for searching. However, two problems remain when minimizing the expected risk: first, that the 0-1 loss function is neither convex nor smooth, and second that the distribution D is unknown. The solutions to these two problems are summarized below.\n\nFor the problem that 0-1 loss function is neither convex nor smooth, abundant convex surrogate loss functions (most are smooth) with the classification-calibrated property (Bartlett et al 2006;Scott 2012) have been proposed. These surrogate loss functions, such as square loss, logistic loss, and hinge loss, have been proven useful in many real-world applications. Apart from the classification-calibrated surrogate loss functions, many other non-convex surrogate loss functions empirically proven to be robust to noise, such as Cauchy loss and Welsch loss, are also frequently used. We show that all these surrogate loss functions can be used directly for classification in the presence of RCN.\n\nFor the problem that distribution D is unknown, empirical risk is proposed to approximate the expected risk. The empirical risk is defined asR\n,D (f ) = 1 n n i=1 (f (x i ), y i ),\nwhere the corresponding expected risk is\nR ,D (f ) = R[D, f, ] = E (x,y)\u223cDR ,D (f )\nand denotes any surrogate loss function. The classifier is then learned by empirical risk minimization (ERM) (Vapnik 2000):\nf n = arg min f \u2208FR ,D (f ).\nThe consistency of R ,D (f n ) to min f \u2208F R ,D (f ) is therefore essential for designing surrogate loss functions and learning algorithms. Let\nf * = arg min f \u2208F R ,D (f ).\nIt (Anthony and Bartlett 2009) is easily proven that\nR ,D (f n ) \u2212 R ,D (f * ) \u2264 2 sup f \u2208F |R ,D (f ) \u2212R ,D (f )|.\nThe right hand side term is known as the generalization error, and the consistency is guaranteed by convergence of the generalization error. We note that learning algorithms which are based on ERM, such as those using Tikhonov or manifold regularization, will not have a slower convergence rate of consistency than that of ERM. In this paper, we therefore provide consistency guarantees for learning algorithms dealing with RCN by deriving the generalization error of the corresponding ERM algorithms.\n\n\nLearning with importance reweighting\n\nImportance reweighting is widely used for domain adaptation (Gretton et al 2009;Bruzzone and Marconcini 2010), but here we introduce it to classification in the presence of label noise. One observation from the field of importance reweighting is as follows:\nR ,D (f ) = R[D, f, (x, y, f )] = E (x,y)\u223cD [ (x, y, f )] = E (x,y)\u223cD\u03c1 P D (x, y) P D\u03c1 (x, y) (x, y, f ) = R D \u03c1 , f, P D (x, y) P D\u03c1 (x, y) (x, y, f ) = R [D \u03c1 , f, \u03b2(x, y) (x, y, f )] = R \u03b2 ,D\u03c1 (f ),\nwhere \u03b2(x, y) = P D (x,y) P D\u03c1 (x,y) . For the problem of classification in the presence of label noise, note that P D (x) = P D\u03c1 (x). We therefore have\n\u03b2(x, y) = P D (x, y) P D\u03c1 (x, y) = P D (y|x)P D (x) P D\u03c1 (y|x)P D\u03c1 (x) = P D (y|x) P D\u03c1 (y|x) .\nThus, even though the labels are corrupted, classification can still be implemented if only the weight \u03b2(x, y) = P D (y|x)/P D\u03c1 (y|x) could be accessed to the loss (x, y, f ).\n\n\nLemma 1\n\nThe asymmetric RCN problem can be addressed by reweighting the surrogate loss functions of the traditional classification problem via importance reweighting. The weight given to a noisy example (x, y) \u223c D \u03c1 is\n\u03b2(x, y) = P D (y|x) P D\u03c1 (y|x) = P D\u03c1 (y|x) \u2212 \u03c1 \u2212y (1 \u2212 \u03c1 +1 \u2212 \u03c1 \u22121 )P D\u03c1 (y|x) .\nThe weight \u03b2(x, y) is non-negative if P D\u03c1 (y|x) = 0. If P D\u03c1 (y|x) = 0, we intuitively let \u03b2(x, y) = 0.\n\nA classifier can be learned in the presence of asymmetric RCN by minimizing the following reweighted empirical risk:f\nn = arg min f \u2208FR \u03b2 ,D\u03c1 = arg min f \u2208F 1 n n i=1\u03b2 (x i ,\u0177 i ) (x i ,\u0177 i , f ), where\u03b2 (x i ,\u0177 i ) = P D (x i , y i ) P D\u03c1 (x i ,\u0177 i ) = P D (y i |x i ) P D\u03c1 (\u0177 i |x i ) = P D\u03c1 (\u0177 i |x i ) \u2212 \u03c1 \u2212y (1 \u2212 \u03c1 +1 \u2212 \u03c1 \u22121 )P D\u03c1 (\u0177 i |x i )\n.\n\nBy the following proposition, we know that, given P D\u03c1 (y|x), the above weighted empirical risk will converge to the unweighted expected risk of the \"clean\" data for any f \u2208 F . So, R ,D can be approximated byR \u03b2 ,D\u03c1 .\n\nProposition 1 Given the conditional probability P D\u03c1 (y|x) and the noise rates \u03c1 +1 and \u03c1 \u22121 . Let \u03b2(x, y) (x, y, f ) be upper bounded by b. Then, for any \u03b4 > 0, with probability at least 1 \u2212 \u03b4, we have\nmax f \u2208F |R ,D \u2212R \u03b2 ,D\u03c1 | = max f \u2208F |E (x,y)\u223cD\u03c1R\u03b2 ,D\u03c1 \u2212R \u03b2 ,D\u03c1 | \u2264 2 1 \u2212 U 1 \u2212 \u03c1 \u22121 \u2212 \u03c1 +1 R( \u2022 F ) + b log(2/\u03b4) 2n , where U = min (x,y) \u03c1\u2212y P D\u03c1 (y|x) , and the Rademacher complexity R( \u2022 F ) (Bartlett and Mendelson 2003) is defined by R( \u2022 F ) = E (x,y)\u223cD\u03c1,\u03c3 sup f \u2208F 1 n n i=1 \u03c3 i (x i ,\u0177 i , f )\nand \u03c3 1 , . . . , \u03c3 n are i.i.d. Rademacher variables.\n\nThe Rademacher complexity has a convergence rate of order O( 1/n). If the function class has proper conditions on its variance, the Rademacher complexity will quickly converge and is of order O(1/n); see, for example, (Bartlett et al 2005). Proposition 1 is derived using the Rademacher complexity method. Many other hypothesis complexities and methods can also be used to derive the generalization bound.\n\nSince\nR ,D (f n ) \u2212 R ,D (f * ) = R \u03b2 ,D\u03c1 (f n ) \u2212 R \u03b2 ,D\u03c1 (f * ) \u2264 2 sup f \u2208F |R \u03b2 ,D\u03c1 (f ) \u2212R \u03b2 ,D\u03c1 (f )|,\nthe consistency rate will therefore be inherited for learning with label noise, provided that the conditional probability P D\u03c1 (y|x) is accurately estimated. Based on Proposition 1, we can now state our first main result for classification in the presence of label noise using our framework.\n\nTheorem 1 Any surrogate loss functions designed for the traditional classification problem can be used for classification in the presence of asymmetric RCN using importance reweighting. The consistency rate for classification with asymmetric RCN will be the same as that of the traditional classification problem, provided that the conditional probability P D\u03c1 (y|x) can be accurately estimated.\n\nThe trade-off for using and benefitting from the abundant surrogate loss functions designed for traditional classification problems is the need to estimate the distribution P D\u03c1 (y|x). Next, we address how to estimate this distribution.\n\n\nEstimating P D \u03c1 (y|x)\n\nWe have shown that the uncertainty introduced by classification label noise can be reduced by the knowledge of weight \u03b2(x, y) = P D (x, y)/P D\u03c1 (x, y).\n\nIn the asymmetric RCN problem,\n\u03b2(x, y) = P D\u03c1 (y|x) \u2212 \u03c1 \u2212y (1 \u2212 \u03c1 +1 \u2212 \u03c1 \u22121 )P D\u03c1 (y|x) ,\nand therefore the weight can be learned by using the noisy sample and the noise rates. In this section, we present two methods to estimate the conditional probability P D\u03c1 (y|x) with consistency guarantees; how to estimate the noise rates is discussed in the next section.\n\nUsing Bayesian rules, we have\nP D\u03c1 (y|x) = P D\u03c1 (x|y)P D\u03c1 (y) P D\u03c1 (x) . (5.1)\nThe probabilities P D\u03c1 (x|y), P D\u03c1 (y) and P D\u03c1 (x) can be easily and efficiently estimated using the noisy sample when the dimensionality of X is low. If we useP\nD\u03c1 (y) = 1 n n i=1 1\u0177 i=y (5.2) andP D\u03c1 (x) = 1 n n i=1 K(x, x i ) (5.3)\nto estimate P D\u03c1 (y) and P D\u03c1 (x), respectively (where K(x, x i ) = k(x)k(x i ) is a universal kernel, see (Steinwart 2002)), the consistency of classification with label noise is guaranteed by the following theorem.\n\nTheorem 2 LetP D\u03c1 (y|x) be an estimator for P D\u03c1 (y|x) using equations (5.1), (5.2) and (5.3), and\n\u03b2(x, y) =P D\u03c1 (y|x) \u2212 \u03c1 \u2212y (1 \u2212 \u03c1 \u22121 \u2212 \u03c1 +1 )P D\u03c1 (y|x) . Letf n,\u03b2 = min f \u2208F 1 n n i=1\u03b2 (x i ,\u0177 i ) (k(x i ),\u0177 i , f ) and f * = min f \u2208F R[D, f, (k(x), y, f )].\nFor any > 0, we have\nlim n\u2192\u221e P (R[D,f n,\u03b2 , (k(x), y,f n,\u03b2 )] \u2212 R[D, f * , (k(x), y, f * )] > ) = 0.\nWhen P D\u03c1 (x|y) and P D\u03c1 (x) are estimated separately, although consistency is guaranteed by mapping features into a universal kernel induced reproducing kernel Hilbert space (RKHS), the convergence rate may be slow. It is preferable to directly estimate the density ratio and avoid estimating the densities separately, since density estimation is known to be a hard problem for high-dimensional variables. Density ratio estimation (Vapnik et al 2013) provides a way to significantly reduce the curse of dimensionality and can be estimated accurately for high-dimensional variables. Therefore, we first introduce density ratio estimation to estimate the conditional probability distribution P D\u03c1 (y|x) for classification in the presence of RCN.\n\nThree methods are frequently used for density ratio estimation, including the moment matching approach, the probabilistic classification approach and the ratio matching approach; see (Sugiyama et al 2010). It can be easily proven that the ratio matching approach exploiting Bregman divergence (Nock and Nielsen 2009) is consistent with the optimal approximation in the hypothesis class 1 . The following theorem provides an assurance that our importance reweighting method that exploits density ratio estimation is consistent.\n\nTheorem 3 When using density ratio estimation to estimate the conditional probability distribution P D\u03c1 (y|x) (and \u03b2(x, y)), if the hypothesis class for estimating the density ratio is chosen properly so that the approximation error is zero, for any > 0, we have\nlim n\u2192\u221e P (R[D,f n,\u03b2 , (x, y,f n,\u03b2 )] \u2212 R[D, f * , (x, y, f * )] > ) = 0,\nwheref n,\u03b2 and f * are the same as those defined in Theorem 2.\n\nWe provide an example in Section 8 to show that the rate of consistency in Theorem 3 can be explicitly fast.\n\n\nEstimating the noise rates\n\nMost existing algorithms designed for RCN problems need to use knowledge of the noise rates. Scott et al (2013) and Blanchard et al (2010) developed lower bounds for asymmetric RCN rates, which are consistent with the true noise rates and can be used as estimators for the noise rates. However, there are no efficient algorithms that can be used to calculate the estimators. To the best of our knowledge, no efficient method has been proposed to estimate the noise rates and how to estimate them remains an open problem (Yang et al 2012). We first provide upper bounds for the noise rates and show that with a mild assumption on the \"clean\" data, they can be used to efficiently estimate the noise rates.\n\nTheorem 4 We have that \u03c1 y \u2264 P D\u03c1 (\u2212y|x).\n\nMoreover, if there exists x \u22121 , x +1 \u2208 X , such that P D (y = +1|x \u22121 ) = P D (y = \u22121|x +1 ) = 0, we have \u03c1 \u22121 = P D\u03c1 (y = +1|x \u22121 ) and \u03c1 +1 = P D\u03c1 (y = \u22121|x +1 ).\n\n\nProof.\n\nP D\u03c1 (+1|x) = P (\u0177 = +1, y = +1|x) + P (\u0177 = +1, y = \u22121|x) = P (\u0177 = +1|y = +1)P D (y = +1|x) +P (\u0177 = +1|y = \u22121)P D (y = \u22121|x)\n= (1 \u2212 \u03c1 +1 )P D (y = +1|x) +\u03c1 \u22121 (1 \u2212 P D (y = +1|x)) = (1 \u2212 \u03c1 \u22121 \u2212 \u03c1 +1 )P D (y = +1|x) + \u03c1 \u22121 \u2265 \u03c1 \u22121 ,\nIf there exists x \u22121 \u2208 X such that P D (y = +1|x \u22121 ) = 0, 1 Parametric modeling is used for estimating density ratio. We provide the proof of consistency in Section 7.4. then P D\u03c1 (y = +1|x \u22121 ) = \u03c1 \u22121 .\n\nSimilarly, P D\u03c1 (\u22121|x) \u2265 \u03c1 +1 , and if there exists x +1 \u2208 X such that\nP D (y = \u22121|x +1 ) = 0, we have P D\u03c1 (y = \u22121|x +1 ) = \u03c1 +1 .\nFor classification problems, even if a sample is inseparable, if an observation x \u2208 X is far form the target classifier it is likely that the conditional probability P D (y = +1|x) (or P D (y = \u22121|x)) is equal to zero or very small. With the assumption that there exist x \u22121 , x +1 \u2208 X such that P D (y = +1|x \u22121 ) and P D (y = \u22121|x +1 ) is very small, we can estimate \u03c1 y by\u03c1 \u2212y = min x\u2208X P D\u03c1 (y|x).\n\nIn our experiments, we estimate \u03c1 y by\u03c1 \u2212y = min x\u2208x1,...,xn P D\u03c1 (y|x).\n\n\nProof\n\nIn this section, we provide detailed proofs of the assertions made in previous sections.\n\n\nProof of Lemma 1\n\nFor label noise problem, we have shown that \u03b2(x, y) = P D (y|x) P D\u03c1 (y|x) .\n\nWhen the label noise is of asymmetric RCN, we have P D\u03c1 (+1|x) = P (\u0177 = +1, y = +1|x) + P (\u0177 = +1, y = \u22121|x)\n\n= P (\u0177 = +1|y = +1)P D (y = +1|x) +P (\u0177 = +1|y = \u22121)P D (y = \u22121|x)\n= (1 \u2212 \u03c1 +1 )P D (y = +1|x) + \u03c1 \u22121 (1 \u2212 P D (y = +1|x)) = (1 \u2212 \u03c1 \u22121 \u2212 \u03c1 +1 )P D (y = +1|x) + \u03c1 \u22121 .\nSimilarly, it gives\nP D\u03c1 (\u22121|x) = (1 \u2212 \u03c1 \u22121 \u2212 \u03c1 +1 )P D (y = \u22121|x) + \u03c1 +1 .\nWe therefore have\nP D (y|x) = P D\u03c1 (y|x) \u2212 \u03c1 \u2212y (1 \u2212 \u03c1 \u22121 \u2212 \u03c1 +1 ) .\nThus,\n\u03b2(x, y) = P D (y|x) P D\u03c1 (y|x) = P D\u03c1 (y|x) \u2212 \u03c1 \u2212y (1 \u2212 \u03c1 +1 \u2212 \u03c1 \u22121 )P D\u03c1 (y|x)\n.\n\nWe intuitively let \u03b2(x, y) = 0, if P D\u03c1 (y|x) = 0. Since P D\u03c1 (\u22121|x) \u2265 \u03c1 +1 and P D\u03c1 (+1|x) \u2265 \u03c1 \u22121 , we can conclude that \u03b2(x, y) \u2265 0.\n\n\nProofs of Proposition 1 and Theorem 1\n\nUsing the Rademacher complexity method provided by Bartlett and Mendelson (2003), we can easily prove that for any [0, b]-valued function class and \u03b4 > 0, with probability at least 1 \u2212 \u03b4, the following holds\nmax f \u2208F |E (x,y)\u223cD\u03c1R\u03b2 ,D\u03c1 \u2212R \u03b2 ,D\u03c1 | \u2264 2R(\u03b2 \u2022 \u2022 F ) + b log(2/\u03b4) 2n .\nSince \u03b2 is upper bounded by\n1 \u2212 U 1 \u2212 \u03c1 \u22121 \u2212 \u03c1 +1 , where U = min (x,y) \u03c1 \u2212y P D\u03c1 (y|x) ,\nusing the Lipschitz composition property of Rademacher complexity, which is also known as the Talagrand's Lemma, we have\nR(\u03b2 \u2022 \u2022 F ) \u2264 1 \u2212 U 1 \u2212 \u03c1 \u22121 \u2212 \u03c1 +1 R( \u2022 F ).\nPropostion 1 can be proven together with the fact that E (x,y)\u223cD\u03c1R\u03b2 ,D\u03c1 = R \u03b2 ,D\u03c1 = R ,D . Theorem 1 follows form Proposition 1.\n\n\nProof of Theorem 2\n\nWe begin with the following lemma.\n\nLemma 2 Let K(x, y) = k(x)k(y) be a universal kernel, where k : X \u2192 H is a feature map into a feature space. LetP\nD\u03c1 (y) = 1 n n i=1 1\u0177 i=y andP D\u03c1 (x) = 1 n n i=1 K(x, x i ).\nThen,P D\u03c1 (y) andP D\u03c1 (x) will converge to their target distributions P D\u03c1 (y) and P D\u03c1 (x) in the induced RKHS H, respectively.\n\nThe proof relies on the following theorem proven by Gretton et al (2009).\n\nTheorem 5 Let P be the space of all probability distributions on an RKHS H induced by a universal kernel K(x, y) = k(x)k(y). Define \u00b5 : P \u2192 H as the expectation operator that \u00b5(P ) = E x\u223cP (x) k(x). The operator \u00b5 is a bijection between P and {\u00b5(P )|P \u2208 P}.\n\nProof of Lemma 2. Since\nEP D\u03c1 (y) = 1 n n i=1\nE1\u0177 i=y = P D\u03c1 (y), using the weak law of large numbers, for any > 0, we have lim n\u2192\u221e P |P D\u03c1 (y) \u2212 P D\u03c1 (y)| \u2265 = 0.\n\nSo,P D\u03c1 (y) will converge to its target distribution P D\u03c1 (y).\n\nWe then prove thatP D\u03c1 (x) converges to P D\u03c1 (x) in the RKHS by using Theorem 5 and showing that\nP D\u03c1 (x)k(x)dx = E x\u223cP D\u03c1 (x) k(x), when n \u2192 \u221e.\nWe have thatP\nD\u03c1 (x) = 1 n n i=1 K(x, x i ) = 1 n n i=1 k(x)k(x i ) = k(x) n n i=1 k(x i ).\nBy properly modifying the kernel map k by a constant so that k 2 (x)dx = 1, we have\nP D\u03c1 (x)k(x)dx = 1 n n i=1 k(x i ) k 2 (x)dx = 1 n n i=1 k(x i ).\nMoreover, for any > 0, using Hoeffding's inequality, the following holds\nlim n\u2192\u221e P 1 n n i=1 k(x i ) \u2212 E x\u223cP D\u03c1 (x) k(x) \u2265 = 0.\nBy combing the above two equations, we can conclude that\nlim n\u2192\u221e P P D\u03c1 (x)k(x)dx \u2212 E x\u223cP D\u03c1 (x) k(x) \u2265 = 0.\nAccording to Theorem 5, we have that the estimatorP D\u03c1 (x) will converge to P D\u03c1 (x) in H. Proof of Theorem 2. In the universal kernel induced RKHS, we have proven that P D\u03c1 (y) = P D\u03c1 (y), when n \u2192 \u221e andP D\u03c1 (x) = P D\u03c1 (x), when n \u2192 \u221e.\n\nThus, we have\u03b2 (x, y) = \u03b2(x, y), when n \u2192 \u221e.\n\n(7.1)\n\nIn Proposition 1, we have proven that\nsup f \u2208F |R[D \u03c1 , f, \u03b2(x, y) (x, y, f )] \u2212R[D \u03c1 , f, \u03b2(x, y) (x, y, f )]| = 0, when n \u2192 \u221e. (7.2)\nBy substitution from equation (7.1) into equation (7.2), we have\nsup f \u2208F |R[D \u03c1 , f,\u03b2(x, y) (x, y, f )] \u2212R[D \u03c1 , f,\u03b2(x, y) (x, y, f )]| = 0, when n \u2192 \u221e. (7.3) Letf n,\u03b2 = min f \u2208F 1 n n i=1\u03b2 (x i ,\u0177 i ) (k(x i ),\u0177 i , f ) and f * = min f \u2208F R[D, f, (k(x), y, f )].\nWe have inequalities (7.4) where the first inequality holds because of the definition off n,\u03b2 . For sufficiently large n, using equations (7.1), (7.3) and (7.4), we have\nR[D \u03c1 ,f n,\u03b2 ,\u03b2(x, y) (k(x), y,f n,\u03b2 )] \u2212 R[D \u03c1 , f * ,\u03b2(x, y) (k(x), y, f * )] = R[D \u03c1 ,f n,\u03b2 ,\u03b2(x, y) (k(x), y,f n,\u03b2 )] \u2212R[D \u03c1 ,f n,\u03b2 ,\u03b2(x, y) (k(x), y,f n,\u03b2 )] +R[D \u03c1 ,f n,\u03b2 ,\u03b2(x, y) (k(x), y,f n,\u03b2 )] \u2212R[D \u03c1 , f * ,\u03b2(x, y) (k(x), y, f * )] +R[D \u03c1 , f * ,\u03b2(x, y) (k(x), y, f * )] \u2212 R[D \u03c1 , f * ,\u03b2(x, y) (k(x), y, f * )] \u2264 R[D \u03c1 ,f n,\u03b2 ,\u03b2(x, y) (k(x), y,f n,\u03b2 )] \u2212R[D \u03c1 ,f n,\u03b2 ,\u03b2(x, y) (k(x), y,f n,\u03b2 )] +R[D \u03c1 , f * ,\u03b2(x, y) (k(x), y, f * )] \u2212 R[D \u03c1 , f * ,\u03b2(x, y) (k(x), y, f * )] \u2264 2 sup f \u2208F |R[D \u03c1 , f,\u03b2(x, y) (k(x), y, f )] \u2212 R[D \u03c1 , f,\u03b2(x, y) (k(x), y, f )]|,R[D,f n,\u03b2 , (k(x), y,f n,\u03b2 )] \u2212 R[D, f * , (k(x), y, f * )] = R[D \u03c1 ,f n,\u03b2 , \u03b2(x, y) (k(x), y,f n,\u03b2 )] \u2212 R[D \u03c1 , f * , \u03b2(x, y) (k(x), y, f * )] = R[D \u03c1 ,f n,\u03b2 ,\u03b2(x, y) (k(x), y,f n,\u03b2 )] \u2212 R[D \u03c1 , f * ,\u03b2(x, y) (k(x), y, f * )] \u2264 2 sup f \u2208F |R[D \u03c1 , f,\u03b2(x, y) (k(x), y, f )] \u2212 R[D \u03c1 , f,\u03b2(x, y) (k(x), y, f )]| = 0.\nThis concludes the proof of Theorem 2.\n\n\nConsistency of density ratio estimation\n\nWe first introduce how to use the ratio matching method under the Bregman divergence to estimate r * (x) = P D\u03c1 (x|y) P D\u03c1 (x) .\n\nThe discrepancy from the true density ratio r * to a density ratio model r measured by the Bregman divergence (BD) is as follows\nBD f (r * r) = P D\u03c1 (x) {f (r * (x)) \u2212 f (r(x)) \u2212 \u2207f (r(x))(r * (x) \u2212 r(x))} dx,\nwhere f is a convex function and \u2207f (x) denotes the subgradient of f (x). Let x nu 1 , . . . , x nu n1 be the i.i.d. sample of the numerator distribution and x de 1 , . . . , x de n2 the i.i.d. sample of the denominator distribution. An empirical approximation of BD f (r * r) is given b\u0177\nBD f (r * r) = 1 n 2 n2 i=1 \u2207f (r(x de i ))r(x de i ) \u2212 1 n 2 n2 i=1 f (r(x de i )) + 1 n 1 n1 i=1 \u2207f (r(x nu i )).\nLetr (x) = arg min rB D f (r * r) and r = arg min r BD f (r * r).\n\nIf the hypothesis class includes r * , we have\nBD f (r * r) = BD f (r * r) \u2212 BD f (r * r ) = BD f (r * r) \u2212BD f (r * r) +BD f (r * r) \u2212BD f (r * r ) +BD f (r * r ) \u2212 BD f (r * r ) \u2264 BD f (r * r) \u2212BD f (r * r) +BD f (r * r ) \u2212 BD f (r * r ) \u2264 2 sup r |BD f (r * r) \u2212 BD f (r * r)|,\nwhere the first inequality holds because of the definition ofr.\n\nFor asymmetric RCN problems, P D\u03c1 (\u2212y|x) \u2265 \u03c1 y . Then, r \u2265 min(\u03c1 +1 , \u03c1 \u22121 ) for all observations. We can assume that r(x) has the range [a, b]. Using the Rademacher method again, for any \u03b4 > 0, with probability at least 1 \u2212 \u03b4, we have\nBD f (r * r) \u2264 2 sup r |BD f (r * r) \u2212 BD f (r * r)| \u2264 4R BDR + C log(2/\u03b4) 2n , (7.5)\nwhere R BDR is the Rademacher complexity induced by estimating the density ratio exploiting Bregman divergence and C is a constant. The convergence rate of R BDR can be proven to be as fast as the order O( 1/ min(n + , n \u2212 )), where n + and n \u2212 denote the number of positive labels and negative labels of the noisy sample, respectively. So, the ratio matching approach exploiting Bregman divergence is consistent to the optimal approximation in the hypothesis class.\n\n\nProof of Theorem 3\n\nProof of Theorem 3. If the hypothesis class for estimating the density ratio is set properly so that the approximation error is zero, the target density ratio r(x) = P D\u03c1 (x|y)/P D\u03c1 (x) will be included in the hypothesis class. The consistency of the ratio matching approach exploiting Bregman divergence guarantees that the target density ratio r(x) can be learned when n is sufficiently large. Using the proof method of Theorem 2, for any > 0, we can prove that 8 Consistency rate analysis of our method\n\nWe show an example that the consistency in Theorem 3 can be explicitly fast.\n\nExample 1 If the Bregman divergence degenerates to square distance, for any \u03b4 > 0, with probability at least 1 \u2212 2\u03b4, the following holds:\nR[D,f n,\u03b2 , (x, y,f n,\u03b2 )] \u2212 R[D, f * , (x, y, f * )] = O \uf8eb \uf8ed R( \u2022 F ) + log(1/\u03b4) n + R SDR + log(1/\u03b4) n \uf8f6 \uf8f8 ,\nwheref n,\u03b2 and f * are the same as those defined in Theorem 2 and R SDR is the Rademacher complexity, in (7.5), induced by estimating the density ratio using square distance.\n\nSo, the convergence rate in Theorem 3 could be explicit and of order O(1/ min(n + , n \u2212 ) 4 ).\n\nProof of Example 1. If we let f (r) = (t \u2212 1) 2 /2, the Bregman divergence degenerates to the square distance as follows BD f (r * r) = SD(r * r) = 1 2 (r * \u2212 r) 2 . Using (7.5), for any \u03b4 > 0, with probability at least 1 \u2212 \u03b4, we have\nBD(r * r) = 1 2 (r * \u2212r) 2 \u2264 2 sup r |\u015cD(r * r) \u2212 SD(r * r)| \u2264 4R SDR + C log(2/\u03b4) 2n .\nThus, with probability at least 1 \u2212 \u03b4, the following holds\nr * \u2264r + O \uf8eb \uf8ed R SDR + log(1/\u03b4) n \uf8f6 \uf8f8 .\nThe convergence rate of R SDR can be proven to be of order O( 1/ min(n + , n \u2212 )). Hence, with probability at least 1 \u2212 \u03b4, we have that\nR \u03b2 ,D\u03c1 (f n,\u03b2 ) = R[D \u03c1 ,f n,\u03b2 , \u03b2(x, y) (x, y,f n,\u03b2 )] = E (x,y)\u223cD\u03c1 \u03b2(x, y) (x, y,f n,\u03b2 ) \u2264 E (x,y)\u223cD\u03c1 \uf8eb \uf8ed\u03b2 (x, y) + O \uf8eb \uf8ed R SDR + log(1/\u03b4) n \uf8f6 \uf8f8 \uf8f6 \uf8f8 (x, y,f n,\u03b2 ) = R[D \u03c1 ,f n,\u03b2 ,\u03b2(x, y) (x, y,f n,\u03b2 )] + O \uf8eb \uf8ed R SDR + log(1/\u03b4) n \uf8f6 \uf8f8 = R\u03b2 ,D\u03c1 (f n,\u03b2 ) + O \uf8eb \uf8ed R SDR + log(1/\u03b4) n \uf8f6 \uf8f8 . (8.1)\nUsing the proof method of Lemma 1, with probability at least 1 \u2212 \u03b4, we have\nR\u03b2 ,D\u03c1 (f n,\u03b2 ) \u2212 R\u03b2 ,D\u03c1 (f * ) \u2264 2 max f \u2208F E (x,y)\u223cD\u03c1R\u03b2 ,D\u03c1 \u2212R\u03b2 ,D\u03c1 \u2264 4 1 \u2212 U 1 \u2212 \u03c1 \u22121 \u2212 \u03c1 +1 R( \u2022 F ) + 2b log(2/\u03b4) 2n , or R\u03b2 ,D\u03c1 (f n,\u03b2 ) \u2212 R\u03b2 ,D\u03c1 (f * ) = O R( \u2022 F ) + log(1/\u03b4) n . (8.2)\nCombining Equations (8.1) and (8.2), with probability at least 1 \u2212 2\u03b4, we have\nR \u03b2 ,D\u03c1 (f n,\u03b2 ) \u2212 R\u03b2 ,D\u03c1 (f * ) = O \uf8eb \uf8ed R( \u2022 F ) + log(1/\u03b4) n + R SDR + log(1/\u03b4) n \uf8f6 \uf8f8 ,\nwhich completes the proof.   \n\n\nExperiments\n\nWe next conducted experiments on synthetic and real data to illustrate the performance of the proposed approaches. Each dataset was randomly split 10 times, 75% for training and 25% for testing, and the labels of the training sample flipped according to given noise rates \u03c1 +1 and \u03c1 \u22121 . The mean accuracies of the 10 datasets are presented. The unbiased estimator (UB ) and label-dependent costs (LD ) models, developed by Natarajan et al (2013), and empirically shown to be competitive with, and perform better more often than, three 2 of state-of-the-art methods for dealing with asymmetric RCN, were chosen as baselines for comparison. IW denotes our method using importance reweighting. eIW denotes our method using importance reweighting and estimated noise rates. Three-fold cross-validation was used to tune the noise rates \u03c1 +1 and \u03c1 \u22121 for UB , LD and IW on the training sets.\n\n\nSynthetic data\n\nWe first tested the performance of noise rate estimation on the synthetic dataset 3 . We used kernel density and density ratio estimation methods to estimate the noise rates on 2-dimensional and 20-dimensional synthetic data, respectively. The kernel width for kernel density estimation method was chosen by Silverman's rule, and the density ratio was estimated using the KLIEP method (Sugiyama et al 2010). Table 1 shows that the estimated rates are close to the true values with small variances.\n\nWe next tested the performance of IW , UB , and LD on synthetic data. For fair comparison, we used the true noise rates for each model so that there was no tuning parameter for all the methods. Even though our method still needed the conditional probability P D\u03c1 (y|x) to be estimated, Figure 1 shows that  our method is more effective when tested on these synthetic data. The empirical results also show that hinge loss performs worse on asymmetric RCN datasets.\n\n\nComparison on UCI benchmarks\n\nThe results of noise rate estimation on two UCI datasets using KLIEP are shown in Table 2. The estimated noise rates have errors less than 0.1. These errors occur for two reasons: the first is that no example has very small P D (y|x) in the Euclidian space, and the second is that density ratio estimation is inaccurate due to the difficulty in choosing kernel width. However, when using the estimated noise rates, the performance of our method (eIW ) is not markedly worse than others (see Table 3 and Table 4). The noise rate estimation is also valuable, since some methods can benefit when the noise rates are approximately known (Natarajan et al 2013).\n\nTo further demonstrate the efficiency of our importance reweighting method, we also tested multiple kernel learning from noisy labels by stochastic programming (StPMKL) (Yang et al 2012) as a baseline. All single kernel learning methods used Gaussian kernel with width 1, while StPMKL used Gaussian kernels with 10 different widths {2 \u22123 , 2 \u22122 , . . . , 2 6 }. The performances of the methods for different noise rates are shown in Table 3, with entries in each row within 1% of the best shown in bold. IW is competitive in all datasets and, furthermore, our method that exploits hinge loss (IW hinge ) performs best more often than the others. Comparisons of linear classification algorithms on UCI benchmarks, which are similar to the comparisons shown in Table 3, are presented in Table 4. IW is competitive in all datasets and, also, our method that exploits hinge loss (IW hinge ) performs best more often than the others. These are consistent with the comparison results of kernelized algorithms. In this paper, we presented an importance reweighting framework for classification in the presence of label noise. Theoretical analyses were provided to assure that the learned classifier will converge to the optimal one for the noise-free sample. Empirical studies on synthetic and real-world datasets verified the effectiveness and robustness of our proposed learning framework. We also provided a method to estimate the noise rates. All our proposed methods crucially depend on the accuracy of density ratio estimation. In future work, we need to consider how to accurately learn the conditional probability distribution P D\u03c1 for the noisy sample. One possible way to achieve this is to use density ratio estimation methods that exploit the knowledge that the feature set of the numerator is a subset of that of the denominator. The problem of estimating noise rates in spaces other than the Euclidian space also needs to be considered, so that the assumption that there are some P D (\u00b11|x) \u2248 0 can be easily satisfied. Some other harder noise models, such as label noise depending on sample features, should also be considered.\n\nP\n(R[D,f n,\u03b2 , (x, y,f n,\u03b2 )] \u2212 R[D, f * , (x, y, f * )] > ) = 0.\n\nFigure 1 :\n1Accuracy comparison of classification algorithms on synthetic data (m=2, n=1000). The 6 different noise rate pairs (\u03c1 +1 , \u03c1 \u22121 ) are: . The IW , UB , LD and methods are compared. The left hand side figure shows the results of the methods using logistic loss, and the right hand side figure shows the results of the methods using hinge loss. UB hinge loss was not implemented due to non-convexity.\n\nTable 1 :\n1Estimating the noise rates on synthetic data\n\nTable 2 :\n2Estimating the noise rates on UCI benchmarks\n\nTable 3 :\n3Accuracy comparison of kernelized classification algorithms on UCI benchmarksBechmark dataset \nNoise rate \n(m, n+, n\u2212) \n(\u03c1+1, \u03c1\u22121) IW log UB log eIW log \n\nlog \n\nIW hinge LD hinge \n\nhinge \n\nStPMKL \n(0.2, 0.2) \n68.46 \n68.62 \n64.62 \n68.46 \n68.92 \n68.92 \n68.07 \n68.31 \nBreast cancer \n(0.3, 0.1) \n70.46 \n70.62 \n70.77 \n69.46 \n68.76 \n68.46 \n67.30 \n69.38 \n(9, 77, 186) \n(0.4, 0.4) \n58.77 \n54.77 \n53.85 \n58.85 \n59.38 \n59.23 \n59.92 \n63.23 \n(0.2, 0.2) \n75.68 \n76.93 \n74.64 \n74.68 \n75.57 \n75.52 \n74.52 \n65.36 \nDiabetis \n(0.3, 0.1) \n74.58 \n74.01 \n72.14 \n73.80 \n75.05 \n72.08 \n71.19 \n65.26 \n(8, 268, 500) \n(0.4, 0.4) \n62.03 \n64.27 \n60.31 \n63.23 \n64.58 \n65.20 \n63.90 \n64.90 \n(0.2, 0.2) \n73.36 \n73.60 \n70.68 \n71.32 \n73.21 \n72.44 \n71.40 \n69.88 \nGerman \n(0.3, 0.1) \n73.28 \n72.84 \n72.04 \n72.84 \n73.16 \n70.08 \n69.72 \n69.72 \n(20, 300, 700) \n(0.4, 0.4) \n68.04 \n66.76 \n59.12 \n61.24 \n69.32 \n67.56 \n64.88 \n69.82 \n(0.2, 0.2) \n80.15 \n79.55 \n80.00 \n79.19 \n82.83 \n80.44 \n80.64 \n75.52 \nHeart \n(0.3, 0.1) \n76.57 \n75.67 \n75.07 \n75.72 \n77.91 \n78.05 \n75.56 \n51.04 \n(13, 120, 150) \n(0.4, 0.4) \n66.72 \n66.72 \n68.96 \n66.31 \n68.65 \n60.00 \n65.97 \n69.70 \n(0.2, 0.2) \n80.08 \n83.51 \n78.20 \n79.00 \n88.82 \n86.98 \n86.48 \n74.24 \nImage \n(0.3, 0.1) \n75.12 \n81.27 \n70.63 \n68.21 \n85.75 \n85.25 \n84.62 \n62.07 \n(18, 1188, 898) \n(0.4, 0.4) \n66.01 \n77.49 \n62.32 \n65.66 \n76.19 \n77.54 \n75.27 \n74.07 \n(0.2, 0.2) \n89.06 \n89.25 \n85.66 \n88.87 \n87.73 \n89.43 \n87.62 \n85.28 \nThyroid \n(0.3, 0.1) \n88.30 \n88.11 \n86.60 \n90.94 \n90.00 \n90.37 \n90.37 \n74.53 \n(5, 65, 150) \n(0.4, 0.4) \n76.41 \n68.68 \n64.91 \n70.57 \n65.66 \n78.49 \n68.11 \n70.75 \n\n\n\nTable 4 :\n4Accuracy comparison of linear classification algorithms on UCI benchmarks Bechmark dataset Noise rate (m, n+, n\u2212) (\u03c1+1, \u03c1\u22121) IW log UB log eIW log log IW hinge LD hinge hinge\nThe three methods are random projection classifier(Stempfel and Ralaivola 2007), NHERD(Crammer and Lee 2010), and the perceptron algorithm with margin(Khardon and Wachman 2007). Comparisons in our paper and those in(Natarajan et al 2013) are implemented on the same standard UCI classification datasets provided by Gunnar R\u00e4tsch: http://theoval.cmp. uea.ac.uk/matlab.3  We generated data by x=rand(m,n)-0.5, w=rand(m,1) and y=sign(x'*w) using MATLAB.\n\nReal-world dataset (m, n) Heart(13, 270) Diabetis. 768Real-world dataset (m, n) Heart(13, 270) Diabetis(8, 768)\n\nLearning from noisy examples. D References Angluin, P Laird, Machine Learning. 24References Angluin D, Laird P (1988) Learning from noisy examples. Machine Learning 2(4):343-370\n\nNeural network learning: Theoretical foundations. M Anthony, P L Bartlett, cambridge university pressAnthony M, Bartlett PL (2009) Neural network learning: Theoretical foundations. cambridge university press\n\nOn the sample complexity of noise-tolerant learning. J A Aslam, S E Decatur, Information Processing Letters. 574Aslam JA, Decatur SE (1996) On the sample complexity of noise-tolerant learning. Information Processing Letters 57(4):189-195\n\nRademacher and gaussian complexities: Risk bounds and structural results. P L Bartlett, S Mendelson, The Journal of Machine Learning Research. 3Bartlett PL, Mendelson S (2003) Rademacher and gaussian complexities: Risk bounds and structural results. The Journal of Machine Learning Research 3:463-482\n\nLocal rademacher complexities. P L Bartlett, O Bousquet, S Mendelson, The Annals of Statistics. 334Bartlett PL, Bousquet O, Mendelson S (2005) Local rademacher complexities. The Annals of Statistics 33(4):1497-1537\n\nConvexity, classification, and risk bounds. P L Bartlett, M I Jordan, J D Mcauliffe, Journal of the American Statistical Association. 101473Bartlett PL, Jordan MI, McAuliffe JD (2006) Convexity, classification, and risk bounds. Journal of the American Statistical Association 101(473):138-156\n\nManifold regularization: A geometric framework for learning from labeled and unlabeled examples. M Belkin, P Niyogi, V Sindhwani, The Journal of Machine Learning Research. 7Belkin M, Niyogi P, Sindhwani V (2006) Manifold regularization: A geometric framework for learning from labeled and unlabeled examples. The Journal of Machine Learning Research 7:2399-2434\n\nSupport vector machines under adversarial label noise. B Biggio, B Nelson, P Laskov, ACMLBiggio B, Nelson B, Laskov P (2011) Support vector machines under adversarial label noise. In: ACML, pp 97-112\n\nSemi-supervised novelty detection. G Blanchard, G Lee, C Scott, The Journal of Machine Learning Research. 9999Blanchard G, Lee G, Scott C (2010) Semi-supervised novelty detection. The Journal of Machine Learning Research 9999:2973-3009\n\nA polynomial-time algorithm for learning noisy linear threshold functions. A Blum, A Frieze, R Kannan, S Vempala, Algorithmica. 221-2Blum A, Frieze A, Kannan R, Vempala S (1998) A polynomial-time algorithm for learning noisy linear threshold functions. Algorithmica 22(1-2):35-52\n\nDomain adaptation problems: A dasvm classification technique and a circular validation strategy. Pattern Analysis and Machine Intelligence. L Bruzzone, M Marconcini, IEEE Transactions on. 325Bruzzone L, Marconcini M (2010) Domain adaptation problems: A dasvm classification technique and a circular validation strategy. Pattern Analysis and Machine Intelligence, IEEE Transactions on 32(5):770- 787\n\nPac learning with nasty noise. N H Bshouty, N Eiron, E Kushilevitz, Theoretical Computer Science. 2882Bshouty NH, Eiron N, Kushilevitz E (2002) Pac learning with nasty noise. Theoretical Computer Science 288(2):255-275\n\nLearning linear threshold functions in the presence of classification noise. T Bylander, COLT, ACM. Bylander T (1994) Learning linear threshold functions in the presence of classification noise. In: COLT, ACM, pp 340-347\n\nSample-efficient strategies for learning in the presence of noise. N Cesa-Bianchi, E Dichterman, P Fischer, E Shamir, H U Simon, Journal of the ACM. 465Cesa-Bianchi N, Dichterman E, Fischer P, Shamir E, Simon HU (1999) Sample-efficient strategies for learning in the presence of noise. Journal of the ACM 46(5):684-719\n\nOnline learning of noisy data. Information Theory. N Cesa-Bianchi, S Shalev-Shwartz, O Shamir, IEEE Transactions on. 5712Cesa-Bianchi N, Shalev-Shwartz S, Shamir O (2011) Online learning of noisy data. Information Theory, IEEE Transactions on 57(12):7907-7931\n\nLearning noisy perceptrons by a perceptron in polynomial time. E Cohen, FOCS, IEEE. Cohen E (1997) Learning noisy perceptrons by a perceptron in polynomial time. In: FOCS, IEEE, pp 514-523\n\nLearning via gaussian herding. K Crammer, D D Lee, NIPS. Crammer K, Lee DD (2010) Learning via gaussian herding. In: NIPS, pp 451-459\n\nClassification in the presence of label noise: A survey. Neural Networks and Learning Systems. B Fr\u00e9nay, M Verleysen, IEEE Transactions on. 255Fr\u00e9nay B, Verleysen M (2014) Classification in the presence of label noise: A survey. Neural Networks and Learning Systems, IEEE Transactions on 25(5):845-869\n\nA general iterative shrinkage and thresholding algorithm for non-convex regularized optimization problems. P Gong, C Zhang, Z Lu, J Huang, J Ye, ICML. Gong P, Zhang C, Lu Z, Huang J, Ye J (2013) A general iterative shrinkage and thresholding algorithm for non-convex regularized optimization problems. In: ICML, pp 37-45\n\nCovariate shift by kernel mean matching. A Gretton, A Smola, J Huang, M Schmittfull, K Borgwardt, B Sch\u00f6lkopf, Dataset shift in machine learning. MIT pressGretton A, Smola A, Huang J, Schmittfull M, Borgwardt K, Sch\u00f6lkopf B (2009) Covariate shift by kernel mean matching. In: Dataset shift in machine learning, MIT press, pp 131-160\n\nMaximum correntropy criterion for robust face recognition. Pattern Analysis and Machine Intelligence. R He, W Zheng, B Hu, IEEE Transactions on. 338He R, Zheng W, Hu B (2011) Maximum correntropy criterion for robust face recognition. Pattern Analysis and Machine Intelligence, IEEE Transactions on 33(8):1561-1576\n\nEfficient noise-tolerant learning from statistical queries. M Kearns, Journal of the ACM. 456Kearns M (1998) Efficient noise-tolerant learning from statistical queries. Journal of the ACM 45(6):983-1006\n\nLearning in the presence of malicious errors. M Kearns, M Li, SIAM Journal on Computing. 224Kearns M, Li M (1993) Learning in the presence of malicious errors. SIAM Journal on Computing 22(4):807- 837\n\nNoise tolerant variants of the perceptron algorithm. R Khardon, G Wachman, The journal of machine learning research. 8Khardon R, Wachman G (2007) Noise tolerant variants of the perceptron algorithm. The journal of machine learning research 8:227-248\n\nLearning halfspaces with malicious noise. A R Klivans, P M Long, R A Servedio, The Journal of Machine Learning Research. 10Klivans AR, Long PM, Servedio RA (2009) Learning halfspaces with malicious noise. The Journal of Machine Learning Research 10:2715-2740\n\nEstimating a kernel fisher discriminant in the presence of label noise. N D Lawrence, B Sch\u00f6lkopf, ICML. Lawrence ND, Sch\u00f6lkopf B (2001) Estimating a kernel fisher discriminant in the presence of label noise. In: ICML, pp 306-313\n\nCorrentropy: properties and applications in non-gaussian signal processing. W Liu, P P Pokharel, J C Pr\u00edncipe, IEEE Transactions on. 5511Signal ProcessingLiu W, Pokharel PP, Pr\u00edncipe JC (2007) Correntropy: properties and applications in non-gaussian signal processing. Signal Processing, IEEE Transactions on 55(11):5286-5298\n\nLearning large-margin halfspaces with more malicious noise. P M Long, R A Servedio, NIPS. Long PM, Servedio RA (2011) Learning large-margin halfspaces with more malicious noise. In: NIPS, pp 91-99\n\nNoise tolerance under risk minimization. Cybernetics. N Manwani, P Sastry, IEEE Transactions on. 433Manwani N, Sastry P (2013) Noise tolerance under risk minimization. Cybernetics, IEEE Transactions on 43(3):1146-1151\n\nRobust Regression Using Maximum-Likelihood Weighting and Assuming Cauchy-Distributed Random Error. Defense Technical Information Center. H Moore, Moore H (1977) Robust Regression Using Maximum-Likelihood Weighting and Assuming Cauchy-Distributed Random Error. Defense Technical Information Center\n\nLearning with noisy labels. N Natarajan, I Dhillon, P Ravikumar, A Tewari, NIPS. Natarajan N, Dhillon I, Ravikumar P, Tewari A (2013) Learning with noisy labels. In: NIPS, pp 1196-1204\n\nSmooth minimization of non-smooth functions. Y Nesterov, Mathematical programming. 1031Nesterov Y (2005) Smooth minimization of non-smooth functions. Mathematical programming 103(1):127- 152\n\nBregman divergences and surrogates for learning. Pattern Analysis and Machine Intelligence. R Nock, F Nielsen, IEEE Transactions on. 3111Nock R, Nielsen F (2009) Bregman divergences and surrogates for learning. Pattern Analysis and Machine Intelligence, IEEE Transactions on 31(11):2048-2059\n\nTrace norm regularization: reformulations, algorithms, and multi-task learning. T K Pong, P Tseng, Ji S Ye, J , SIAM Journal on Optimization. 206Pong TK, Tseng P, Ji S, Ye J (2010) Trace norm regularization: reformulations, algorithms, and multi-task learning. SIAM Journal on Optimization 20(6):3465-3489\n\nCalibrated asymmetric surrogate losses. C Scott, Electronic Journal of Statistics. 6Scott C (2012) Calibrated asymmetric surrogate losses. Electronic Journal of Statistics 6:958-992\n\nClassification with asymmetric label noise: Consistency and maximal denoising. C Scott, G Blanchard, G Handy, COLT. Scott C, Blanchard G, Handy G (2013) Classification with asymmetric label noise: Consistency and maximal denoising. In: COLT, pp 489-511\n\nSupport vector machines are universally consistent. I Steinwart, Journal of Complexity. 183Steinwart I (2002) Support vector machines are universally consistent. Journal of Complexity 18(3):768-791\n\nLearning kernel perceptrons on noisy data using random projections. G Stempfel, L Ralaivola, SpringerStempfel G, Ralaivola L (2007) Learning kernel perceptrons on noisy data using random projections. In: ALT, Springer, pp 328-342\n\nDensity ratio estimation: A comprehensive review. M Sugiyama, T Suzuki, T Kanamori, RIMS Kokyuroku ppSugiyama M, Suzuki T, Kanamori T (2010) Density ratio estimation: A comprehensive review. RIMS Kokyuroku pp 10-31\n\nA theory of the learnable. L G Valiant, Communications of the ACM. 2711Valiant LG (1984) A theory of the learnable. Communications of the ACM 27(11):1134-1142\n\nThe nature of statistical learning theory. V Vapnik, springerVapnik V (2000) The nature of statistical learning theory. springer\n\nConstructive setting of the density ratio estimation problem and its rigorous solution. V Vapnik, I Braga, R Izmailov, arXiv:13060407arXiv preprintVapnik V, Braga I, Izmailov R (2013) Constructive setting of the density ratio estimation problem and its rigorous solution. arXiv preprint arXiv:13060407\n\nMultiple kernel learning from noisy labels by stochastic programming. T Yang, M Mahdavi, Jin R Zhang, L Zhou, Y , Yang T, Mahdavi M, Jin R, Zhang L, Zhou Y (2012) Multiple kernel learning from noisy labels by stochastic programming. In: ICML\n\nRegularization and variable selection via the elastic net. H Zou, T Hastie, Journal of the Royal Statistical Society: Series B (Statistical Methodology). 672Zou H, Hastie T (2005) Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 67(2):301-320\n", "annotations": {"author": "[{\"end\":282,\"start\":79},{\"end\":486,\"start\":283}]", "publisher": null, "author_last_name": "[{\"end\":92,\"start\":89},{\"end\":294,\"start\":291}]", "author_first_name": "[{\"end\":88,\"start\":79},{\"end\":290,\"start\":283}]", "author_affiliation": "[{\"end\":281,\"start\":115},{\"end\":485,\"start\":319}]", "title": "[{\"end\":59,\"start\":1},{\"end\":545,\"start\":487}]", "venue": null, "abstract": "[{\"end\":1738,\"start\":564}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2306,\"start\":2282},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":2384,\"start\":2370},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2433,\"start\":2420},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2764,\"start\":2735},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2865,\"start\":2846},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":2968,\"start\":2951},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3166,\"start\":3139},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3508,\"start\":3484},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3656,\"start\":3631},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3847,\"start\":3825},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4067,\"start\":4046},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":4548,\"start\":4536},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5234,\"start\":5215},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":5254,\"start\":5234},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":5270,\"start\":5254},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":5284,\"start\":5270},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5300,\"start\":5284},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":5433,\"start\":5416},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":5801,\"start\":5783},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6579,\"start\":6559},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6903,\"start\":6883},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6927,\"start\":6903},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6946,\"start\":6927},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":6972,\"start\":6948},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7397,\"start\":7377},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7521,\"start\":7506},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7532,\"start\":7521},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7548,\"start\":7532},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":7576,\"start\":7548},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7603,\"start\":7578},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7898,\"start\":7872},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8215,\"start\":8203},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8280,\"start\":8264},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8294,\"start\":8280},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8429,\"start\":8407},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9780,\"start\":9758},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":11505,\"start\":11484},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":11516,\"start\":11505},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":12397,\"start\":12384},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":12631,\"start\":12605},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":13340,\"start\":13320},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":13369,\"start\":13340},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":15925,\"start\":15904},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":18110,\"start\":18094},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":19019,\"start\":19000},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":19518,\"start\":19497},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":19630,\"start\":19607},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":20493,\"start\":20475},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":20520,\"start\":20498},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":20919,\"start\":20902},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":23315,\"start\":23286},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":24336,\"start\":24316},{\"end\":26167,\"start\":26162},{\"end\":28575,\"start\":28569},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":32129,\"start\":32107},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":32994,\"start\":32973},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":34238,\"start\":34216},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":38818,\"start\":38789},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":38847,\"start\":38825},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":38915,\"start\":38889},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":38976,\"start\":38954}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":36443,\"start\":36377},{\"attributes\":{\"id\":\"fig_1\"},\"end\":36854,\"start\":36444},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":36911,\"start\":36855},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":36968,\"start\":36912},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":38551,\"start\":36969},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":38738,\"start\":38552}]", "paragraph": "[{\"end\":2107,\"start\":1754},{\"end\":4621,\"start\":2109},{\"end\":5301,\"start\":4623},{\"end\":7398,\"start\":5303},{\"end\":7964,\"start\":7416},{\"end\":8483,\"start\":7966},{\"end\":9390,\"start\":8485},{\"end\":9980,\"start\":9408},{\"end\":10401,\"start\":10056},{\"end\":10766,\"start\":10403},{\"end\":11310,\"start\":10809},{\"end\":12008,\"start\":11312},{\"end\":12152,\"start\":12010},{\"end\":12231,\"start\":12191},{\"end\":12398,\"start\":12275},{\"end\":12571,\"start\":12428},{\"end\":12654,\"start\":12602},{\"end\":13219,\"start\":12718},{\"end\":13517,\"start\":13260},{\"end\":13872,\"start\":13720},{\"end\":14144,\"start\":13969},{\"end\":14365,\"start\":14156},{\"end\":14552,\"start\":14448},{\"end\":14671,\"start\":14554},{\"end\":14903,\"start\":14902},{\"end\":15123,\"start\":14905},{\"end\":15327,\"start\":15125},{\"end\":15684,\"start\":15630},{\"end\":16091,\"start\":15686},{\"end\":16098,\"start\":16093},{\"end\":16493,\"start\":16202},{\"end\":16890,\"start\":16495},{\"end\":17128,\"start\":16892},{\"end\":17306,\"start\":17155},{\"end\":17338,\"start\":17308},{\"end\":17670,\"start\":17398},{\"end\":17701,\"start\":17672},{\"end\":17913,\"start\":17751},{\"end\":18203,\"start\":17987},{\"end\":18303,\"start\":18205},{\"end\":18487,\"start\":18467},{\"end\":19312,\"start\":18568},{\"end\":19840,\"start\":19314},{\"end\":20104,\"start\":19842},{\"end\":20241,\"start\":20179},{\"end\":20351,\"start\":20243},{\"end\":21086,\"start\":20382},{\"end\":21129,\"start\":21088},{\"end\":21296,\"start\":21131},{\"end\":21431,\"start\":21307},{\"end\":21742,\"start\":21538},{\"end\":21814,\"start\":21744},{\"end\":22277,\"start\":21876},{\"end\":22351,\"start\":22279},{\"end\":22449,\"start\":22361},{\"end\":22546,\"start\":22470},{\"end\":22656,\"start\":22548},{\"end\":22724,\"start\":22658},{\"end\":22844,\"start\":22825},{\"end\":22918,\"start\":22901},{\"end\":22975,\"start\":22970},{\"end\":23057,\"start\":23056},{\"end\":23193,\"start\":23059},{\"end\":23442,\"start\":23235},{\"end\":23541,\"start\":23514},{\"end\":23724,\"start\":23604},{\"end\":23899,\"start\":23771},{\"end\":23956,\"start\":23922},{\"end\":24071,\"start\":23958},{\"end\":24262,\"start\":24134},{\"end\":24337,\"start\":24264},{\"end\":24596,\"start\":24339},{\"end\":24621,\"start\":24598},{\"end\":24760,\"start\":24644},{\"end\":24824,\"start\":24762},{\"end\":24922,\"start\":24826},{\"end\":24984,\"start\":24971},{\"end\":25146,\"start\":25063},{\"end\":25285,\"start\":25213},{\"end\":25397,\"start\":25341},{\"end\":25686,\"start\":25450},{\"end\":25732,\"start\":25688},{\"end\":25739,\"start\":25734},{\"end\":25778,\"start\":25741},{\"end\":25940,\"start\":25876},{\"end\":26310,\"start\":26141},{\"end\":27230,\"start\":27192},{\"end\":27402,\"start\":27274},{\"end\":27532,\"start\":27404},{\"end\":27902,\"start\":27614},{\"end\":28084,\"start\":28019},{\"end\":28132,\"start\":28086},{\"end\":28430,\"start\":28367},{\"end\":28667,\"start\":28432},{\"end\":29220,\"start\":28754},{\"end\":29748,\"start\":29243},{\"end\":29826,\"start\":29750},{\"end\":29965,\"start\":29828},{\"end\":30251,\"start\":30077},{\"end\":30347,\"start\":30253},{\"end\":30583,\"start\":30349},{\"end\":30730,\"start\":30672},{\"end\":30906,\"start\":30771},{\"end\":31275,\"start\":31200},{\"end\":31547,\"start\":31469},{\"end\":31667,\"start\":31638},{\"end\":32569,\"start\":31683},{\"end\":33085,\"start\":32588},{\"end\":33550,\"start\":33087},{\"end\":34239,\"start\":33583},{\"end\":36376,\"start\":34241}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10055,\"start\":9981},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12190,\"start\":12153},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12274,\"start\":12232},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12427,\"start\":12399},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12601,\"start\":12572},{\"attributes\":{\"id\":\"formula_5\"},\"end\":12717,\"start\":12655},{\"attributes\":{\"id\":\"formula_6\"},\"end\":13719,\"start\":13518},{\"attributes\":{\"id\":\"formula_7\"},\"end\":13968,\"start\":13873},{\"attributes\":{\"id\":\"formula_8\"},\"end\":14447,\"start\":14366},{\"attributes\":{\"id\":\"formula_9\"},\"end\":14901,\"start\":14672},{\"attributes\":{\"id\":\"formula_10\"},\"end\":15629,\"start\":15328},{\"attributes\":{\"id\":\"formula_11\"},\"end\":16201,\"start\":16099},{\"attributes\":{\"id\":\"formula_12\"},\"end\":17397,\"start\":17339},{\"attributes\":{\"id\":\"formula_13\"},\"end\":17750,\"start\":17702},{\"attributes\":{\"id\":\"formula_14\"},\"end\":17986,\"start\":17914},{\"attributes\":{\"id\":\"formula_15\"},\"end\":18466,\"start\":18304},{\"attributes\":{\"id\":\"formula_16\"},\"end\":18567,\"start\":18488},{\"attributes\":{\"id\":\"formula_17\"},\"end\":20178,\"start\":20105},{\"attributes\":{\"id\":\"formula_18\"},\"end\":21537,\"start\":21432},{\"attributes\":{\"id\":\"formula_19\"},\"end\":21875,\"start\":21815},{\"attributes\":{\"id\":\"formula_20\"},\"end\":22824,\"start\":22725},{\"attributes\":{\"id\":\"formula_21\"},\"end\":22900,\"start\":22845},{\"attributes\":{\"id\":\"formula_22\"},\"end\":22969,\"start\":22919},{\"attributes\":{\"id\":\"formula_23\"},\"end\":23055,\"start\":22976},{\"attributes\":{\"id\":\"formula_24\"},\"end\":23513,\"start\":23443},{\"attributes\":{\"id\":\"formula_25\"},\"end\":23603,\"start\":23542},{\"attributes\":{\"id\":\"formula_26\"},\"end\":23770,\"start\":23725},{\"attributes\":{\"id\":\"formula_27\"},\"end\":24133,\"start\":24072},{\"attributes\":{\"id\":\"formula_28\"},\"end\":24643,\"start\":24622},{\"attributes\":{\"id\":\"formula_29\"},\"end\":24970,\"start\":24923},{\"attributes\":{\"id\":\"formula_30\"},\"end\":25062,\"start\":24985},{\"attributes\":{\"id\":\"formula_31\"},\"end\":25212,\"start\":25147},{\"attributes\":{\"id\":\"formula_32\"},\"end\":25340,\"start\":25286},{\"attributes\":{\"id\":\"formula_33\"},\"end\":25449,\"start\":25398},{\"attributes\":{\"id\":\"formula_34\"},\"end\":25875,\"start\":25779},{\"attributes\":{\"id\":\"formula_35\"},\"end\":26140,\"start\":25941},{\"attributes\":{\"id\":\"formula_36\"},\"end\":26878,\"start\":26311},{\"attributes\":{\"id\":\"formula_37\"},\"end\":27191,\"start\":26878},{\"attributes\":{\"id\":\"formula_38\"},\"end\":27613,\"start\":27533},{\"attributes\":{\"id\":\"formula_39\"},\"end\":28018,\"start\":27903},{\"attributes\":{\"id\":\"formula_40\"},\"end\":28366,\"start\":28133},{\"attributes\":{\"id\":\"formula_41\"},\"end\":28753,\"start\":28668},{\"attributes\":{\"id\":\"formula_42\"},\"end\":30076,\"start\":29966},{\"attributes\":{\"id\":\"formula_43\"},\"end\":30671,\"start\":30584},{\"attributes\":{\"id\":\"formula_44\"},\"end\":30770,\"start\":30731},{\"attributes\":{\"id\":\"formula_45\"},\"end\":31199,\"start\":30907},{\"attributes\":{\"id\":\"formula_46\"},\"end\":31468,\"start\":31276},{\"attributes\":{\"id\":\"formula_47\"},\"end\":31637,\"start\":31548}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":33003,\"start\":32996},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":33672,\"start\":33665},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":34081,\"start\":34074},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":34093,\"start\":34086},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":34681,\"start\":34674},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":35007,\"start\":35000},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":35033,\"start\":35026}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1752,\"start\":1740},{\"attributes\":{\"n\":\"1.1\"},\"end\":7414,\"start\":7401},{\"attributes\":{\"n\":\"2\"},\"end\":9406,\"start\":9393},{\"attributes\":{\"n\":\"3\"},\"end\":10807,\"start\":10769},{\"attributes\":{\"n\":\"4\"},\"end\":13258,\"start\":13222},{\"end\":14154,\"start\":14147},{\"attributes\":{\"n\":\"5\"},\"end\":17153,\"start\":17131},{\"attributes\":{\"n\":\"6\"},\"end\":20380,\"start\":20354},{\"end\":21305,\"start\":21299},{\"attributes\":{\"n\":\"7\"},\"end\":22359,\"start\":22354},{\"attributes\":{\"n\":\"7.1\"},\"end\":22468,\"start\":22452},{\"attributes\":{\"n\":\"7.2\"},\"end\":23233,\"start\":23196},{\"attributes\":{\"n\":\"7.3\"},\"end\":23920,\"start\":23902},{\"attributes\":{\"n\":\"7.4\"},\"end\":27272,\"start\":27233},{\"attributes\":{\"n\":\"7.5\"},\"end\":29241,\"start\":29223},{\"attributes\":{\"n\":\"9\"},\"end\":31681,\"start\":31670},{\"attributes\":{\"n\":\"9.1\"},\"end\":32586,\"start\":32572},{\"attributes\":{\"n\":\"9.2\"},\"end\":33581,\"start\":33553},{\"end\":36379,\"start\":36378},{\"end\":36455,\"start\":36445},{\"end\":36865,\"start\":36856},{\"end\":36922,\"start\":36913},{\"end\":36979,\"start\":36970},{\"end\":38562,\"start\":38553}]", "table": "[{\"end\":38551,\"start\":37058}]", "figure_caption": "[{\"end\":36443,\"start\":36380},{\"end\":36854,\"start\":36457},{\"end\":36911,\"start\":36867},{\"end\":36968,\"start\":36924},{\"end\":37058,\"start\":36981},{\"end\":38738,\"start\":38564}]", "figure_ref": "[{\"end\":19964,\"start\":19952},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":33381,\"start\":33373}]", "bib_author_first_name": "[{\"end\":39335,\"start\":39334},{\"end\":39357,\"start\":39356},{\"end\":39534,\"start\":39533},{\"end\":39545,\"start\":39544},{\"end\":39547,\"start\":39546},{\"end\":39746,\"start\":39745},{\"end\":39748,\"start\":39747},{\"end\":39757,\"start\":39756},{\"end\":39759,\"start\":39758},{\"end\":40006,\"start\":40005},{\"end\":40008,\"start\":40007},{\"end\":40020,\"start\":40019},{\"end\":40265,\"start\":40264},{\"end\":40267,\"start\":40266},{\"end\":40279,\"start\":40278},{\"end\":40291,\"start\":40290},{\"end\":40494,\"start\":40493},{\"end\":40496,\"start\":40495},{\"end\":40508,\"start\":40507},{\"end\":40510,\"start\":40509},{\"end\":40520,\"start\":40519},{\"end\":40522,\"start\":40521},{\"end\":40841,\"start\":40840},{\"end\":40851,\"start\":40850},{\"end\":40861,\"start\":40860},{\"end\":41162,\"start\":41161},{\"end\":41172,\"start\":41171},{\"end\":41182,\"start\":41181},{\"end\":41343,\"start\":41342},{\"end\":41356,\"start\":41355},{\"end\":41363,\"start\":41362},{\"end\":41620,\"start\":41619},{\"end\":41628,\"start\":41627},{\"end\":41638,\"start\":41637},{\"end\":41648,\"start\":41647},{\"end\":41966,\"start\":41965},{\"end\":41978,\"start\":41977},{\"end\":42257,\"start\":42256},{\"end\":42259,\"start\":42258},{\"end\":42270,\"start\":42269},{\"end\":42279,\"start\":42278},{\"end\":42523,\"start\":42522},{\"end\":42735,\"start\":42734},{\"end\":42751,\"start\":42750},{\"end\":42765,\"start\":42764},{\"end\":42776,\"start\":42775},{\"end\":42786,\"start\":42785},{\"end\":42788,\"start\":42787},{\"end\":43039,\"start\":43038},{\"end\":43055,\"start\":43054},{\"end\":43073,\"start\":43072},{\"end\":43312,\"start\":43311},{\"end\":43470,\"start\":43469},{\"end\":43481,\"start\":43480},{\"end\":43483,\"start\":43482},{\"end\":43669,\"start\":43668},{\"end\":43679,\"start\":43678},{\"end\":43984,\"start\":43983},{\"end\":43992,\"start\":43991},{\"end\":44001,\"start\":44000},{\"end\":44007,\"start\":44006},{\"end\":44016,\"start\":44015},{\"end\":44240,\"start\":44239},{\"end\":44251,\"start\":44250},{\"end\":44260,\"start\":44259},{\"end\":44269,\"start\":44268},{\"end\":44284,\"start\":44283},{\"end\":44297,\"start\":44296},{\"end\":44635,\"start\":44634},{\"end\":44641,\"start\":44640},{\"end\":44650,\"start\":44649},{\"end\":44908,\"start\":44907},{\"end\":45098,\"start\":45097},{\"end\":45108,\"start\":45107},{\"end\":45307,\"start\":45306},{\"end\":45318,\"start\":45317},{\"end\":45547,\"start\":45546},{\"end\":45549,\"start\":45548},{\"end\":45560,\"start\":45559},{\"end\":45562,\"start\":45561},{\"end\":45570,\"start\":45569},{\"end\":45572,\"start\":45571},{\"end\":45837,\"start\":45836},{\"end\":45839,\"start\":45838},{\"end\":45851,\"start\":45850},{\"end\":46072,\"start\":46071},{\"end\":46079,\"start\":46078},{\"end\":46081,\"start\":46080},{\"end\":46093,\"start\":46092},{\"end\":46095,\"start\":46094},{\"end\":46383,\"start\":46382},{\"end\":46385,\"start\":46384},{\"end\":46393,\"start\":46392},{\"end\":46395,\"start\":46394},{\"end\":46575,\"start\":46574},{\"end\":46586,\"start\":46585},{\"end\":46877,\"start\":46876},{\"end\":47066,\"start\":47065},{\"end\":47079,\"start\":47078},{\"end\":47090,\"start\":47089},{\"end\":47103,\"start\":47102},{\"end\":47269,\"start\":47268},{\"end\":47508,\"start\":47507},{\"end\":47516,\"start\":47515},{\"end\":47789,\"start\":47788},{\"end\":47791,\"start\":47790},{\"end\":47799,\"start\":47798},{\"end\":47809,\"start\":47807},{\"end\":47811,\"start\":47810},{\"end\":47817,\"start\":47816},{\"end\":48056,\"start\":48055},{\"end\":48278,\"start\":48277},{\"end\":48287,\"start\":48286},{\"end\":48300,\"start\":48299},{\"end\":48505,\"start\":48504},{\"end\":48720,\"start\":48719},{\"end\":48732,\"start\":48731},{\"end\":48933,\"start\":48932},{\"end\":48945,\"start\":48944},{\"end\":48955,\"start\":48954},{\"end\":49126,\"start\":49125},{\"end\":49128,\"start\":49127},{\"end\":49302,\"start\":49301},{\"end\":49477,\"start\":49476},{\"end\":49487,\"start\":49486},{\"end\":49496,\"start\":49495},{\"end\":49762,\"start\":49761},{\"end\":49770,\"start\":49769},{\"end\":49783,\"start\":49780},{\"end\":49785,\"start\":49784},{\"end\":49794,\"start\":49793},{\"end\":49802,\"start\":49801},{\"end\":49994,\"start\":49993},{\"end\":50001,\"start\":50000}]", "bib_author_last_name": "[{\"end\":39354,\"start\":39336},{\"end\":39363,\"start\":39358},{\"end\":39542,\"start\":39535},{\"end\":39556,\"start\":39548},{\"end\":39754,\"start\":39749},{\"end\":39767,\"start\":39760},{\"end\":40017,\"start\":40009},{\"end\":40030,\"start\":40021},{\"end\":40276,\"start\":40268},{\"end\":40288,\"start\":40280},{\"end\":40301,\"start\":40292},{\"end\":40505,\"start\":40497},{\"end\":40517,\"start\":40511},{\"end\":40532,\"start\":40523},{\"end\":40848,\"start\":40842},{\"end\":40858,\"start\":40852},{\"end\":40871,\"start\":40862},{\"end\":41169,\"start\":41163},{\"end\":41179,\"start\":41173},{\"end\":41189,\"start\":41183},{\"end\":41353,\"start\":41344},{\"end\":41360,\"start\":41357},{\"end\":41369,\"start\":41364},{\"end\":41625,\"start\":41621},{\"end\":41635,\"start\":41629},{\"end\":41645,\"start\":41639},{\"end\":41656,\"start\":41649},{\"end\":41975,\"start\":41967},{\"end\":41989,\"start\":41979},{\"end\":42267,\"start\":42260},{\"end\":42276,\"start\":42271},{\"end\":42291,\"start\":42280},{\"end\":42532,\"start\":42524},{\"end\":42748,\"start\":42736},{\"end\":42762,\"start\":42752},{\"end\":42773,\"start\":42766},{\"end\":42783,\"start\":42777},{\"end\":42794,\"start\":42789},{\"end\":43052,\"start\":43040},{\"end\":43070,\"start\":43056},{\"end\":43080,\"start\":43074},{\"end\":43318,\"start\":43313},{\"end\":43478,\"start\":43471},{\"end\":43487,\"start\":43484},{\"end\":43676,\"start\":43670},{\"end\":43689,\"start\":43680},{\"end\":43989,\"start\":43985},{\"end\":43998,\"start\":43993},{\"end\":44004,\"start\":44002},{\"end\":44013,\"start\":44008},{\"end\":44019,\"start\":44017},{\"end\":44248,\"start\":44241},{\"end\":44257,\"start\":44252},{\"end\":44266,\"start\":44261},{\"end\":44281,\"start\":44270},{\"end\":44294,\"start\":44285},{\"end\":44307,\"start\":44298},{\"end\":44638,\"start\":44636},{\"end\":44647,\"start\":44642},{\"end\":44653,\"start\":44651},{\"end\":44915,\"start\":44909},{\"end\":45105,\"start\":45099},{\"end\":45111,\"start\":45109},{\"end\":45315,\"start\":45308},{\"end\":45326,\"start\":45319},{\"end\":45557,\"start\":45550},{\"end\":45567,\"start\":45563},{\"end\":45581,\"start\":45573},{\"end\":45848,\"start\":45840},{\"end\":45861,\"start\":45852},{\"end\":46076,\"start\":46073},{\"end\":46090,\"start\":46082},{\"end\":46104,\"start\":46096},{\"end\":46390,\"start\":46386},{\"end\":46404,\"start\":46396},{\"end\":46583,\"start\":46576},{\"end\":46593,\"start\":46587},{\"end\":46883,\"start\":46878},{\"end\":47076,\"start\":47067},{\"end\":47087,\"start\":47080},{\"end\":47100,\"start\":47091},{\"end\":47110,\"start\":47104},{\"end\":47278,\"start\":47270},{\"end\":47513,\"start\":47509},{\"end\":47524,\"start\":47517},{\"end\":47796,\"start\":47792},{\"end\":47805,\"start\":47800},{\"end\":47814,\"start\":47812},{\"end\":48062,\"start\":48057},{\"end\":48284,\"start\":48279},{\"end\":48297,\"start\":48288},{\"end\":48306,\"start\":48301},{\"end\":48515,\"start\":48506},{\"end\":48729,\"start\":48721},{\"end\":48742,\"start\":48733},{\"end\":48942,\"start\":48934},{\"end\":48952,\"start\":48946},{\"end\":48964,\"start\":48956},{\"end\":49136,\"start\":49129},{\"end\":49309,\"start\":49303},{\"end\":49484,\"start\":49478},{\"end\":49493,\"start\":49488},{\"end\":49505,\"start\":49497},{\"end\":49767,\"start\":49763},{\"end\":49778,\"start\":49771},{\"end\":49791,\"start\":49786},{\"end\":49799,\"start\":49795},{\"end\":49998,\"start\":49995},{\"end\":50008,\"start\":50002}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":39302,\"start\":39191},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":5508562},\"end\":39481,\"start\":39304},{\"attributes\":{\"id\":\"b2\"},\"end\":39690,\"start\":39483},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":15465469},\"end\":39929,\"start\":39692},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":463216},\"end\":40231,\"start\":39931},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":2259087},\"end\":40447,\"start\":40233},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":2833811},\"end\":40741,\"start\":40449},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":16902615},\"end\":41104,\"start\":40743},{\"attributes\":{\"id\":\"b8\"},\"end\":41305,\"start\":41106},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":16296545},\"end\":41542,\"start\":41307},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":15479137},\"end\":41823,\"start\":41544},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":14739721},\"end\":42223,\"start\":41825},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":8842156},\"end\":42443,\"start\":42225},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":16357542},\"end\":42665,\"start\":42445},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":9383472},\"end\":42985,\"start\":42667},{\"attributes\":{\"id\":\"b15\"},\"end\":43246,\"start\":42987},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":27244147},\"end\":43436,\"start\":43248},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":9056472},\"end\":43571,\"start\":43438},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":6054025},\"end\":43874,\"start\":43573},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":16894434},\"end\":44196,\"start\":43876},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":108301245},\"end\":44530,\"start\":44198},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":15873697},\"end\":44845,\"start\":44532},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":6392609},\"end\":45049,\"start\":44847},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":1507349},\"end\":45251,\"start\":45051},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":10647676},\"end\":45502,\"start\":45253},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":7642513},\"end\":45762,\"start\":45504},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":6597362},\"end\":45993,\"start\":45764},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":7496788},\"end\":46320,\"start\":45995},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":10297723},\"end\":46518,\"start\":46322},{\"attributes\":{\"id\":\"b29\"},\"end\":46737,\"start\":46520},{\"attributes\":{\"id\":\"b30\"},\"end\":47035,\"start\":46739},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":423350},\"end\":47221,\"start\":47037},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":2391217},\"end\":47413,\"start\":47223},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":243363},\"end\":47706,\"start\":47415},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":1433599},\"end\":48013,\"start\":47708},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":120595203},\"end\":48196,\"start\":48015},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":5370766},\"end\":48450,\"start\":48198},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":6337970},\"end\":48649,\"start\":48452},{\"attributes\":{\"id\":\"b38\"},\"end\":48880,\"start\":48651},{\"attributes\":{\"id\":\"b39\"},\"end\":49096,\"start\":48882},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":59712},\"end\":49256,\"start\":49098},{\"attributes\":{\"id\":\"b41\"},\"end\":49386,\"start\":49258},{\"attributes\":{\"doi\":\"arXiv:13060407\",\"id\":\"b42\"},\"end\":49689,\"start\":49388},{\"attributes\":{\"id\":\"b43\"},\"end\":49932,\"start\":49691},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":122419596},\"end\":50263,\"start\":49934}]", "bib_title": "[{\"end\":39332,\"start\":39304},{\"end\":39743,\"start\":39692},{\"end\":40003,\"start\":39931},{\"end\":40262,\"start\":40233},{\"end\":40491,\"start\":40449},{\"end\":40838,\"start\":40743},{\"end\":41340,\"start\":41307},{\"end\":41617,\"start\":41544},{\"end\":41963,\"start\":41825},{\"end\":42254,\"start\":42225},{\"end\":42520,\"start\":42445},{\"end\":42732,\"start\":42667},{\"end\":43036,\"start\":42987},{\"end\":43309,\"start\":43248},{\"end\":43467,\"start\":43438},{\"end\":43666,\"start\":43573},{\"end\":43981,\"start\":43876},{\"end\":44237,\"start\":44198},{\"end\":44632,\"start\":44532},{\"end\":44905,\"start\":44847},{\"end\":45095,\"start\":45051},{\"end\":45304,\"start\":45253},{\"end\":45544,\"start\":45504},{\"end\":45834,\"start\":45764},{\"end\":46069,\"start\":45995},{\"end\":46380,\"start\":46322},{\"end\":46572,\"start\":46520},{\"end\":47063,\"start\":47037},{\"end\":47266,\"start\":47223},{\"end\":47505,\"start\":47415},{\"end\":47786,\"start\":47708},{\"end\":48053,\"start\":48015},{\"end\":48275,\"start\":48198},{\"end\":48502,\"start\":48452},{\"end\":49123,\"start\":49098},{\"end\":49991,\"start\":49934}]", "bib_author": "[{\"end\":39356,\"start\":39334},{\"end\":39365,\"start\":39356},{\"end\":39544,\"start\":39533},{\"end\":39558,\"start\":39544},{\"end\":39756,\"start\":39745},{\"end\":39769,\"start\":39756},{\"end\":40019,\"start\":40005},{\"end\":40032,\"start\":40019},{\"end\":40278,\"start\":40264},{\"end\":40290,\"start\":40278},{\"end\":40303,\"start\":40290},{\"end\":40507,\"start\":40493},{\"end\":40519,\"start\":40507},{\"end\":40534,\"start\":40519},{\"end\":40850,\"start\":40840},{\"end\":40860,\"start\":40850},{\"end\":40873,\"start\":40860},{\"end\":41171,\"start\":41161},{\"end\":41181,\"start\":41171},{\"end\":41191,\"start\":41181},{\"end\":41355,\"start\":41342},{\"end\":41362,\"start\":41355},{\"end\":41371,\"start\":41362},{\"end\":41627,\"start\":41619},{\"end\":41637,\"start\":41627},{\"end\":41647,\"start\":41637},{\"end\":41658,\"start\":41647},{\"end\":41977,\"start\":41965},{\"end\":41991,\"start\":41977},{\"end\":42269,\"start\":42256},{\"end\":42278,\"start\":42269},{\"end\":42293,\"start\":42278},{\"end\":42534,\"start\":42522},{\"end\":42750,\"start\":42734},{\"end\":42764,\"start\":42750},{\"end\":42775,\"start\":42764},{\"end\":42785,\"start\":42775},{\"end\":42796,\"start\":42785},{\"end\":43054,\"start\":43038},{\"end\":43072,\"start\":43054},{\"end\":43082,\"start\":43072},{\"end\":43320,\"start\":43311},{\"end\":43480,\"start\":43469},{\"end\":43489,\"start\":43480},{\"end\":43678,\"start\":43668},{\"end\":43691,\"start\":43678},{\"end\":43991,\"start\":43983},{\"end\":44000,\"start\":43991},{\"end\":44006,\"start\":44000},{\"end\":44015,\"start\":44006},{\"end\":44021,\"start\":44015},{\"end\":44250,\"start\":44239},{\"end\":44259,\"start\":44250},{\"end\":44268,\"start\":44259},{\"end\":44283,\"start\":44268},{\"end\":44296,\"start\":44283},{\"end\":44309,\"start\":44296},{\"end\":44640,\"start\":44634},{\"end\":44649,\"start\":44640},{\"end\":44655,\"start\":44649},{\"end\":44917,\"start\":44907},{\"end\":45107,\"start\":45097},{\"end\":45113,\"start\":45107},{\"end\":45317,\"start\":45306},{\"end\":45328,\"start\":45317},{\"end\":45559,\"start\":45546},{\"end\":45569,\"start\":45559},{\"end\":45583,\"start\":45569},{\"end\":45850,\"start\":45836},{\"end\":45863,\"start\":45850},{\"end\":46078,\"start\":46071},{\"end\":46092,\"start\":46078},{\"end\":46106,\"start\":46092},{\"end\":46392,\"start\":46382},{\"end\":46406,\"start\":46392},{\"end\":46585,\"start\":46574},{\"end\":46595,\"start\":46585},{\"end\":46885,\"start\":46876},{\"end\":47078,\"start\":47065},{\"end\":47089,\"start\":47078},{\"end\":47102,\"start\":47089},{\"end\":47112,\"start\":47102},{\"end\":47280,\"start\":47268},{\"end\":47515,\"start\":47507},{\"end\":47526,\"start\":47515},{\"end\":47798,\"start\":47788},{\"end\":47807,\"start\":47798},{\"end\":47816,\"start\":47807},{\"end\":47820,\"start\":47816},{\"end\":48064,\"start\":48055},{\"end\":48286,\"start\":48277},{\"end\":48299,\"start\":48286},{\"end\":48308,\"start\":48299},{\"end\":48517,\"start\":48504},{\"end\":48731,\"start\":48719},{\"end\":48744,\"start\":48731},{\"end\":48944,\"start\":48932},{\"end\":48954,\"start\":48944},{\"end\":48966,\"start\":48954},{\"end\":49138,\"start\":49125},{\"end\":49311,\"start\":49301},{\"end\":49486,\"start\":49476},{\"end\":49495,\"start\":49486},{\"end\":49507,\"start\":49495},{\"end\":49769,\"start\":49761},{\"end\":49780,\"start\":49769},{\"end\":49793,\"start\":49780},{\"end\":49801,\"start\":49793},{\"end\":49805,\"start\":49801},{\"end\":50000,\"start\":49993},{\"end\":50010,\"start\":50000}]", "bib_venue": "[{\"end\":39240,\"start\":39191},{\"end\":39381,\"start\":39365},{\"end\":39531,\"start\":39483},{\"end\":39799,\"start\":39769},{\"end\":40072,\"start\":40032},{\"end\":40327,\"start\":40303},{\"end\":40581,\"start\":40534},{\"end\":40913,\"start\":40873},{\"end\":41159,\"start\":41106},{\"end\":41411,\"start\":41371},{\"end\":41670,\"start\":41658},{\"end\":42011,\"start\":41991},{\"end\":42321,\"start\":42293},{\"end\":42543,\"start\":42534},{\"end\":42814,\"start\":42796},{\"end\":43102,\"start\":43082},{\"end\":43330,\"start\":43320},{\"end\":43493,\"start\":43489},{\"end\":43711,\"start\":43691},{\"end\":44025,\"start\":44021},{\"end\":44342,\"start\":44309},{\"end\":44675,\"start\":44655},{\"end\":44935,\"start\":44917},{\"end\":45138,\"start\":45113},{\"end\":45368,\"start\":45328},{\"end\":45623,\"start\":45583},{\"end\":45867,\"start\":45863},{\"end\":46126,\"start\":46106},{\"end\":46410,\"start\":46406},{\"end\":46615,\"start\":46595},{\"end\":46874,\"start\":46739},{\"end\":47116,\"start\":47112},{\"end\":47304,\"start\":47280},{\"end\":47546,\"start\":47526},{\"end\":47848,\"start\":47820},{\"end\":48096,\"start\":48064},{\"end\":48312,\"start\":48308},{\"end\":48538,\"start\":48517},{\"end\":48717,\"start\":48651},{\"end\":48930,\"start\":48882},{\"end\":49163,\"start\":49138},{\"end\":49299,\"start\":49258},{\"end\":49474,\"start\":49388},{\"end\":49759,\"start\":49691},{\"end\":50086,\"start\":50010}]"}}}, "year": 2023, "month": 12, "day": 17}