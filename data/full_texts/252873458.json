{"id": 252873458, "updated": "2023-10-05 09:55:13.069", "metadata": {"title": "OpenOOD: Benchmarking Generalized Out-of-Distribution Detection", "authors": "[{\"first\":\"Jingkang\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Pengyun\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Dejian\",\"last\":\"Zou\",\"middle\":[]},{\"first\":\"Zitang\",\"last\":\"Zhou\",\"middle\":[]},{\"first\":\"Kunyuan\",\"last\":\"Ding\",\"middle\":[]},{\"first\":\"Wenxuan\",\"last\":\"Peng\",\"middle\":[]},{\"first\":\"Haoqi\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Guangyao\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Bo\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Yiyou\",\"last\":\"Sun\",\"middle\":[]},{\"first\":\"Xuefeng\",\"last\":\"Du\",\"middle\":[]},{\"first\":\"Kaiyang\",\"last\":\"Zhou\",\"middle\":[]},{\"first\":\"Wayne\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Dan\",\"last\":\"Hendrycks\",\"middle\":[]},{\"first\":\"Yixuan\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Ziwei\",\"last\":\"Liu\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Out-of-distribution (OOD) detection is vital to safety-critical machine learning applications and has thus been extensively studied, with a plethora of methods developed in the literature. However, the field currently lacks a unified, strictly formulated, and comprehensive benchmark, which often results in unfair comparisons and inconclusive results. From the problem setting perspective, OOD detection is closely related to neighboring fields including anomaly detection (AD), open set recognition (OSR), and model uncertainty, since methods developed for one domain are often applicable to each other. To help the community to improve the evaluation and advance, we build a unified, well-structured codebase called OpenOOD, which implements over 30 methods developed in relevant fields and provides a comprehensive benchmark under the recently proposed generalized OOD detection framework. With a comprehensive comparison of these methods, we are gratified that the field has progressed significantly over the past few years, where both preprocessing methods and the orthogonal post-hoc methods show strong potential.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2210.07242", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/YangWZZDPWCLSDZ22", "doi": "10.48550/arxiv.2210.07242"}}, "content": {"source": {"pdf_hash": "4e9e30f4702f64af5aacbb5791172c5b37510dc3", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2210.07242v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "e2ef917fe5fdadbcc3b435fe87bd9485de7687a2", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/4e9e30f4702f64af5aacbb5791172c5b37510dc3.txt", "contents": "\nOpenOOD: Benchmarking Generalized Out-of-Distribution Detection\n\n\nJingkang Yang \nS-Lab\nNanyang Technological University\nSingapore\n\nPengyun Wang \nBeijing University of Posts and Telecommunications\nBeijingChina\n\nQueen Mary University of London\nLondonUK\n\nDejian Zou \nBeijing University of Posts and Telecommunications\nBeijingChina\n\nQueen Mary University of London\nLondonUK\n\nZitang Zhou \nBeijing University of Posts and Telecommunications\nBeijingChina\n\nQueen Mary University of London\nLondonUK\n\nKunyuan Ding \nBeijing University of Posts and Telecommunications\nBeijingChina\n\nQueen Mary University of London\nLondonUK\n\nWenxuan Peng \nS-Lab\nNanyang Technological University\nSingapore\n\nHaoqi Wang \nSenseTime Research\nShenzhenChina\n\nGuangyao Chen \nPeking University\nBeijingChina\n\nBo Li \nS-Lab\nNanyang Technological University\nSingapore\n\nYiyou Sun \nUniversity of Wisconsin-Madison\nMadisonWIUSA\n\nXuefeng Du \nUniversity of Wisconsin-Madison\nMadisonWIUSA\n\nKaiyang Zhou \nS-Lab\nNanyang Technological University\nSingapore\n\nWayne Zhang \nSenseTime Research\nShenzhenChina\n\nDan Hendrycks \nUniversity of California\nBerkeleyCAUSA\n\nYixuan Li \nUniversity of Wisconsin-Madison\nMadisonWIUSA\n\nZiwei Liu \nS-Lab\nNanyang Technological University\nSingapore\n\nOpenOOD: Benchmarking Generalized Out-of-Distribution Detection\n/Jingkang50/OpenOOD\nOut-of-distribution (OOD) detection is vital to safety-critical machine learning applications and has thus been extensively studied, with a plethora of methods developed in the literature. However, the field currently lacks a unified, strictly formulated, and comprehensive benchmark, which often results in unfair comparisons and inconclusive results. From the problem setting perspective, OOD detection is closely related to neighboring fields including anomaly detection (AD), open set recognition (OSR), and model uncertainty, since methods developed for one domain are often applicable to each other. To help the community to improve the evaluation and advance, we build a unified, well-structured codebase called OpenOOD, which implements over 30 methods developed in relevant fields and provides a comprehensive benchmark under the recently proposed generalized OOD detection framework. With a comprehensive comparison of these methods, we are gratified that the field has progressed significantly over the past few years, where both preprocessing methods and the orthogonal post-hoc methods show strong potential. We invite readers to use our OpenOOD codebase to develop and contribute. The full experimental results are available in this table.\n\nIntroduction\n\nMost existing machine learning (ML) models are trained on the closed-world assumption, where all the test data is assumed to be drawn from in-distribution (ID), i.e., the same distribution as the training data [1,2]. However, the closed-world assumption is difficult to maintain in the real world [3]. In practice, a deployed model will be inevitably exposed to unseen examples that deviated from the training distribution, which are known as out-of-distribution (OOD) samples [4,5], which can affect ML model safety [6,7]. While the neighborhood OOD generalization community focuses on ensuring the robustness of models to maintain high performance on OOD samples with domain shift [8], OOD detection, on the other hand, emphasizes the model reliability by requiring the identification of samples with semantic shift [5]. In other words, the goal of OOD detection is to detect samples to which the model cannot or does not want to generalize [5].\n\nA plethora of methodologies for OOD detection has been developed in the past five years, ranging from classification-based to density-based to distance-based methods [5]. Classification-based methods take the major part of OOD detectors, which gets confidence directly from the classifier with some design that is beneficial to OOD detection [9,10]. The design can focus on loss function [11], classifier architecture [12], and some post-hoc processing techniques [4,13]. Specially, post-hoc methods are more suitable for real-world practice due to their plug-in simplicity without interference on the pretrained backbones that require expensive training process [14]. Density-based methods model the in-distribution with probabilistic models, which also achieve good performance and are easier for theoretical analysis [15,16,17]. Distance-based methods usually compute distance in the high-dimension space such as feature space and gradient space to distinguish ID and OOD [18,19]. Some minor categories include reconstruction-based methods, which rely on the discrepancy of reconstruction-error between ID and OOD samples [20,21].\n\nAlthough more popular in the community in the past few years, there is no uniform and comprehensive benchmark to make sure the developed methods are truly effective. This brings up problems, such as some methods only reporting results on certain datasets where they are good at [22,23]. In fact, it is normal to see that the OOD detection performance for each method varies a lot on different OOD test dataset. Also, some benchmarks are saturated with scores exceeding 99% [24], so further improvements on these benchmarks (e.g., from 99.2% to 99.4%) are no longer considered valuable. Besides, even with the same benchmark, some technical details such as image preprocessing procedures [25,26], are not unified, increasing the difficulty of a fair comparison.\n\nMoreover, some closely related topics, such as anomaly detection (AD), open set recognition (OSR), and model uncertainty, have been developed in isolation for a long while. In fact, methods developed for OSR [27,28], model uncertainty [29,30], and even data augmentation methods [31] can seamlessly solve the OOD detection problem. Similarly, AD methods can apply to OOD detection task by ignoring all the labels within the in-distribution [32,33]. Recently, a generalized OOD detection framework [5] is proposed, which unifies similar tasks such as AD, OSR, and OOD detection. Considering the inherent connections among all these neighborhood tasks, a comprehensive comparison beyond OOD detection methods is expected, so that every task can inspire each other and a joint force can be formed to promote the development of the broader model reliability community.\n\nTo address the problems, we develop a well-structured codebase called OpenOOD, which provides 9 benchmarks (1 from AD, 4 from OSR, 4 from OOD detection) under the generalized OOD detection framework [5], for comprehensive evaluation. Especially, in the OOD detection benchmarks, we systematically design different types of OOD (i.e., Near-OOD & Far-OOD) for detailed analysis. All the benchmarks are carefully examined to prevent ID samples being wrongly introduced into OOD sets. Besides, we integrated 35 methods (4 from AD, 3 from OSR, 22 from OOD detection, 6 from model uncertainty plus data augmentation) using a unified, well-structured code framework in the OpenOOD, so that the majority of representative methods in all related fields can be fairly compared. In the later part, we report the comparison among all the reproduced methods across several benchmarks, followed by in-depth discussion on the results. We end up the paper with discussions on the future direction. In general, we summarize our main contributions as follows:\n\nComprehensive OOD Detection Benchmarks We provide 9 benchmarks to comprehensively evaluate OOD detection methods. The benchmarks are systematically designed with different OOD types with careful data cleaning procedure.\n\nComprehensive Comparison Across Different Tasks We reproduce 35 methods that are originated from OOD detection-related tasks, including AD, OSR, and model uncertainty, and compare them under the comprehensive OOD detection benchmarks.\n\n\nA Unified Codebase for OOD Detection\n\nWe provide an open-source codebase called OpenOOD, with well-designed code structure that is able to accommodate different kinds of OOD detection methods. Codebase is available at https://github.com/Jingkang50/OpenOOD.\n\nInsights Through a comprehensive comparison of these methods, we share several findings: 1) simple preprocessing methods can achieve the best score among the benchmark; 2) Extra data seems not necessary or requires further exploration; 3) Post-hoc methods make significant progress and are generally no worse than methods that require training.  Figure 1: Diagram of benchmarks supported by OpenOOD. OpenOOD supports 9 benchmarks that originated from anomaly detection (AD), open set recognition (OSR), and OOD detection. Three example benchmarks are highlighted to represent AD, OSR, and OOD detection, respectively. The different benchmarking styles of AD, OSR and OOD detection clearly clearly indicate their focus. While AD requires models to be aware of the pixel-level difference like scratch, OSR and OOD detection focuses on detecting the semantic shift, where OOD (open-set) samples come from other dataset (other label-split of the dataset). All those benchmarks can be easy downloaded via this script.\n\n\nSupported Tasks, Benchmarks, and Metrics\n\nIn this section, we first introduce 9 supported benchmarks in OpenOOD codebase, compassing most of the popular benchmarks across anomaly detection (AD), open set recognition (OSR), and OOD detection. Then, we introduce the metrics that are used to report the experimental results. Figure 1 shows and compares the benchmarks from different subfields.\n\n\nAnomaly Detection Definition\n\nAnomaly detection refers to the problem of finding patterns in data that do not conform to expected behavior [34]. Current anomaly detection settings often restrict the in-distribution (normality) to be with a single class [35]. According to different distribution shifts that causes the anomalies, AD tasks can be further divided into sensory AD that to detect low-level sensory anomalies, and semantic AD that to detect high-level semantic anomalies [36,37]. However, most of the anomaly detection methods are required to address both sensory and semantic AD. The AD task can also be divided into unsupervised AD, and (semi-)supervised AD in regard to the data supervision [37].\n\nAD Benchmark: MVTec-AD To evaluate methods developed for anomaly detection, we use the widely used MVTec-AD benchmark [38], which addresses the realistic industrial inspection task. MVTec-AD consists of 15 categories with 3629 images for training and validation and 1725 images for testing. While the training set only contains defect-free images, the test set contains both normal images and anomalous images with various types of defects, expecting anomaly detectors to distinguish abnormal samples from normal ones. Notice that although MVTec-AD contains 15 categories, anomaly detectors only focus on one category at one time and are trained in an unsupervised manner. Therefore, although AD methods can address OOD detection tasks by ignoring ID classes, OSR and OOD detection methods are not applicable to MVTec-AD. OpenOOD supports MVTec-AD mainly to guarantee the correctness of AD methods, and encourage more methods under the generalized OOD detection framework to be applied in all of AD, OSR, and OOD settings.\n\n\nOpen Set Recognition Definition\n\nMachine learning models trained in the closed-world setting can incorrectly classify test samples from unknown classes as one of the known categories with high confidence. Open set recognition (OSR) task is proposed to address this problem, with their own terminology of \"known known classes\" to represent the categories that exist at training, and \"unknown unknown classes\" for test categories that do not fall into any training category [39]. Formally, OSR requires a multi-class classifier to simultaneously: 1) accurately classify test samples from \"known known classes\", and 2) detect test samples from \"unknown unknown classes\" [5].\n\n\nOSR Benchmarks\n\nWe include 4 common-used OSR benchmarks. The common practice for building OSR benchmarks is to divide the categories of existing datasets into two parts, called closed and open set [40,41]. Machine learning models are trained only on the closed set. When testing, the models are evaluated on the entire test set and need to separate open set samples from closed set samples. MNIST-6/4 is based on MNIST [42] and splits the dataset into 6 classes for training and 4 classes for testing. The experiments need to run and average on 5 different splits. Similarly, CIFAR-6/4 and CIFAR-50/50 are constructed on CIFAR-10 [43] and CIFAR-100 [44], respectively. TinyImageNet-20/180 splits close and open set from TinyImageNet [23].\n\n\nOut-of-Distribution Detection Definition\n\nOut-of-distribution detection, or OOD detection, aims to detect test samples drawn from a distribution different from the training distribution, with the definition of the distribution to be well-defined according to the application in the target. For most machine learning tasks, especially image classification tasks, the distribution should refer to \"label distribution\", meaning that OOD samples should not have overlapping labels w.r.t. training data. Note that the training set usually contains multiple classes, and OOD detection should NOT harm the ID classification capability [5].\n\n\nOOD Benchmarks\n\nThe common practice for building OOD detection benchmarks is to consider an entire dataset as in-distribution (ID), and then collect several datasets that are disconnected from any ID categories as OOD datasets. OpenOOD supports 4 OOD benchmarks, which are named after ID datasets, including MNIST [45], CIFAR-10 [43], CIFAR-100 [44], and ImageNet [2]. We further design near-OOD and far-OOD datasets to facilitate detailed analysis of the OOD detectors. Near-OOD datasets only have semantic shift compared with ID datasets, while far-OOD further contains obvious covariate (domain) shift.\n\n\nMNIST\n\nMNIST [42] is a 10-class handwriting digit dataset that contains 60k images for training and 10k for test. For OOD dataset, near-OOD includes NOTMNIST [46] and FashionMNIST [47], which share a similar background style with MNIST. Far-OOD consists of textural dataset Texture [48], two object datasets including CIFAR-10 [43] and TinyImageNet [2], and a scene dataset Places-365 [49]. All the far-OOD datasets have obviously different styles compared to MNIST. If applicable, we only utilize test set from OOD datasets. CIFAR-10 and Tiny-ImageNet test sets have 10k images each. Places365 contains 36.5k test images. We use the entire 5,640 Texture images.\n\n\nCIFAR-10\n\nCIFAR-10 [43] is a 10-class dataset for general object classification, which contains 50k training images and 10k test images. As for OOD dataset, we construct near-OOD with CIFAR-100 [44] and TinyImageNet [2]. Notice that 1,207 images are removed from TinyImageNet since they actually belong to CIFAR-10 classes [50]. Far-OOD is built by MNIST [42], FashionMNIST [47], Texture [48], and Places365 [49] with 1,305 images are removed due to semantic overlaps.\n\n\nCIFAR-100\n\nAnother OOD detection benchmark uses CIFAR-100 [44] as in-distribution, which contains 50k training images and 10k test images with 100 classes. For OOD dataset, near-OOD includes CIFAR-10 [43] and TinyImageNet [23]. Similar to CIFAR-10 benchmark, 2,502 images are removed from TinyImageNet due to the overlapping semantics with CIFAR-100 classes [50]. Far-OOD consists of MNIST [42], FashionMNIST [47], Texture [48], and Places365 [49] with 1,305 images removed.\n\n\nImageNet-1K\n\nImageNet is a large-scale image classification dataset with 1000 classes. To build OOD dataset, we use a 10k subset of Species [51] with 713k images, iNaturalist [52] with 10k images, ImageNet-O [53] with 2k images, and OpenImage-O [54] with 17k images. All of these datasets are carefully curated to get rid of samples that should belong to ID classes. Meanwhile, Texture [48], MNIST [42] and SVHN [55] are considered as far-OOD.\n\n\nMetrics\n\nOpenOOD mainly use the following three metrics to evaluate methods on all the supported benchmarks: 1) FPR@95 measures the false positive rate (FPR) when the true positive rate (TPR) is equal to 95%. Lower scores indicate better performance. 2) AUROC measures the area under the Receiver Operating Characteristic (ROC) curve, which displays the relationship between TPR and FPR. The area under the ROC curve can be interpreted as the probability that a positive ID example will have a higher detection score than a negative OOD example. 3) AUPR measures the area under the Precision-Recall (PR) curve. The PR curve is created by plotting precision versus recall. Similar to AUROC, we consider ID samples as positive, so that the score corresponds to the AUPR-In metric in some works. In the main paper, we use AUROC as the main metric. We provide the full results in the form of \"FPR@95 / AUROC / AUPR\".\n\n\nDiscussion\n\nIn this section, we discuss all 9 benchmarks that are involved in OpenOOD. The benchmark for anomaly detection can only support AD methods because their training sets do not have categorical labels. The inclusion of the AD benchmark is to guarantee the reproducibility of AD methods, and encourage more methods under the generalized OOD detection framework to be also applied to AD.\n\nOSR and OOD detection are interchangeable in some literature due to their identical motivation to identify samples with semantic shift compared to training distribution. The only difference between them is the evaluation protocol. While OSR benchmarks are inherently difficult as datasets are split according to classes, e.g., CIFAR-4/6 splits CIFAR-10 into two parts, the pretrained models turn out to be trained on the non-standard CIFAR-4 dataset. OOD detection benchmarks are designed to maintain the integrity of the training dataset, i.e., using the entire CIFAR-10 for training, and introduce other datasets as OOD datasets. Some blame the early OOD benchmarks and claim that they often select easy OOD datasets and a good performance can be achieved by detecting superficial domain shift between datasets. However, recent works focus more on near-OOD, and detecting semantic shift becomes the mainstream. The gap between OSR and OOD detection is just getting smaller.\n\n\nSupported Methodologies\n\nIn this section, we briefly introduce all 35 methods that are supported in OpenOOD. We prioritize work with open-source code for inclusion. Figure 2 lists all these methods by chronology. Different method types are marked by different colors, and different fields are marked by different patterns.\n\n\nMethodologies for Anomaly Detection\n\nOpenOOD supports 4 AD methods. Deep-SVDD [56] is a classic distance-based AD method, which enforces that the distance between a training (ID) sample and its centroid is below a certain value in the penultimate feature space. CutPaste [26] simply cuts out a patch from an image and pastes it back with transformation to generate anomaly samples, which further helps better density estimation for in-distribution data. PatchCore [57] samples ID features into a memory bank and performs the nearest neighbor searching to discover anomalies. DRAEM [58] is a reconstruction-based method that feeds the given image and its reconstructed version into a discriminative network to produce anomaly scores. Note that AD does not have multiple classes in the training set. To be applicable to OSR and OOD detection tasks, AD methods can simply treat all ID samples as a whole.\n\n\nMethodologies for Open Set Recognition\n\nOpenOOD supports 3 OSR methods. OpenMax [59] is the first deep learning method to address the open set problem. During inference, it replaces the classic SoftMax layer with an OpenMax layer, which fits ID samples with Weibull distribution and estimates the ID probability of test samples accordingly. ARPL [41] minimizes the overlap of known distributions and unknown distributions by learning discriminative reciprocal points to represent \"otherness\" with respect to a class. OpenGAN [60] uses the idea of GAN to generate negative features that are similar to external anomalous samples to enhance the open-set discriminator.\n\n\nMethodologies for Out-of-Distribution Detection\n\nThe phenomenon of neural networks' overconfidence in out-of-distribution data attracts growing research attention over the past six years. To facilitate the comparison and reproduction, OpenOOD integrates 22 OOD detection methods in our codebase in several thriving directions:\n\n\nPost-hoc Methods\n\nOne line of work attempts to perform post-hoc OOD detection: MSP [4] is the first and the most basic baseline that directly uses the maximum SoftMax score to indicate ID-ness. Later works explore other simple and more efficient indicators to distinguish ID and OOD, such as ODIN [13] [51] that calculates the minimum KL-divergence between the softmax and the mean class-conditional distributions, VIM [54] that integrates both the norm of feature residual against the principal space formed by training features and the original logits to compute the OOD-ness, and KNN [65] that explores the efficacy of non-parametric nearest-neighbor distance for OOD detection.\n\nAll the methods above perform inference with a pretrained model in a post hoc manner, which provides several advantages including: a) Easy-to-use: Most OOD scoring methods are designed in a plug-and-play manner, which is simple to integrate in the existing pipeline; b) Model-agnostic: The testing procedure applies to a variety of model architectures, including CNNs and the recent transformer-based model ViT [66]. Moreover, the post hoc methods are agnostic to the training procedure as well, and are compatible with models trained under different losses.\n\nTraining-time Regularization Another promising line of work addresses OOD detection by training-time regularization. For example, ConfBranch [9] builds an extra branch from the penultimate layer to estimate confidence scores. G-ODIN [67] decomposes the posterior to explicitly model the probability of ID-ness. CSI [68] explores the effectiveness of contrast learning objectives for OOD detectors (with fully unsupervised setting in this paper). MOS [52] uses the prior of super category to perform hierarchical OOD detection. VOS [69] produces better energy scores with the support of synthetic virtual outliers. LogitNorm [70] provides an alternative to the cross-entropy loss, which decouples the influence of logits' norm from the training procedure. Unlike post hoc methods, this line of work attempts to achieve better uncertainty estimates by training stronger models with better representations. As compensation, these methods require more computational resources.\n\n\nTraining with Outlier Exposure\n\nSome practical works collect a bunch of external OOD samples during training to help OOD detectors to better learn ID/OOD discrepancy. Starting from OE [22] which encourages a flat or high-entropic prediction on given OOD samples, MCD [24] uses a two-branch network to enlarge branches' entropic discrepancy on OOD training data. UDG [50] further considers the realistic scenario where given OOD samples are not purely from OOD, so to use a clustering-based method to filter out ID samples and enhance the feature representation in an unsupervised learning manner. Although using external data is a common practice especially in industry, how to efficiently select the extra data and how to prevent the model to overfit the given OOD is still the open problem.\n\n\nMethodologies for Model Uncertainty (including Data Augmentation Methods)\n\nIn addition to the above methods that are designed for OOD detection or OSR problems, other methods use Bayesian modeling to address model reliability problems with less-principled approximations, such as MC-Dropout [71] and DeepEnsemble [72]. Besides, TempScaling [73] is shown as the first and one of the simplest methods for uncertainty calibration. Some work observes that regularizing the model with data augmentation procedure will be helpful for a better estimation on uncertainty. Representative methods include Mixup [74], CutMix [75], and PixMix [76]. Methods that we include in this category are all require training, except temperature scaling.\n\n\nExperiments\n\nWe run all the 35 methods that supported by OpenOOD, and compare them on the unified generalized OOD detection benchmarks, as shown in Table 1. This section mainly explains our systematic implementation and discussion on the results.\n\n\nImplementation Details\n\nTo ensure the fair comparison across methods originated from different fields with different implementations, we use unified settings with common hyperparameters and architecture choices. For example, we only support LeNet [77] for benchmarks with MNIST as ID, and use ResNet-18 [78] whenever CIFAR and TinyImageNet are ID. For large-scale experiments when ImageNet is the in-distribution dataset, we use ResNet50 [78] in our benchmark comparison. If the implemented method requires training, we use the well-accepted setting with SGD optimizer, the learning rate of 0.1, the momentum of 0.9, and weight decay of 0.0005 for 100 epochs, to prevent over-tuning. If the method requires hyperparameter tuning, we only try the 5 most common values and pick the hyperparameter based on the performance of AUROC on the validation set, which is introduced in Section 2. For example, to test TempScaling [73] on ImageNet benchmark, we search the optimal temperature among 0.1, 1, 10, 100, and 1000 based on the AUROC considering ImageNet validation set (we randomly pick 10% of images from the test set) as ID, and OpenImage-O validation set as OOD. The logic behind OOD validation set selection is based on real-world practice, All these designs are for the fairness and the practicality of the comparison on the benchmark. The main benchmark development and testing are performed using 2 Nvidia RTX 3060 cards. Details of each method are listed in our GitHub wiki.\n\n\nMain Results\n\nData Augmentation is the Most Effective Method Type We split Table 1 vertically into several sections based on the type of method. Generally, the most effective methods lie in the category of model uncertainty works using data augmentation techniques. This group mainly contains simple and effective methods, such as the preprocessing methods such as PixMix [76] and CutMix [75]. Especially, PixMix achieves 93.1% on Near-OOD in CIFAR-10, which is the best among all the methods in this benchmark. They also ace in the most of other benchmarks. Similarly, other simple and effective methods to enhance model uncertainty estimation such as Ensemble [79] and Mixup [74] also demonstrate excellent performance.\n\n\nExtra Data Seems Not Necessary\n\nFor the block of OOD detection, we split it into two parts based on the necessity of extra data. By comparing UDG [50] (the best from extra-data part) with KNN [65] (the best from extra data-free part), the advantage of UDG only lies in CIFAR-10 near-OOD, which does not meet the expectation as a large quantity of real outlier data is required. The Table 1: Main Results on Generalized OOD Detection Benchmark. The generalized OOD detection benchmark composes 9 benchmarks from AD, OSR, and OOD detection. We denote MNIST-6/4 as M-6, CIFAR-6/4 as C-6, CIFAR-50/50 as C-50, TinyImageNet-20/180 as TIN-20 to save space. We only report the metric of AUROC. \u263c denotes methods that require training. denotes post-hoc methods. + denotes methods with extra data. means running time beyond 48 hours. \"Avg.\" averages all the provided AUROCs within the block. \"Acc.\" and \"Time\" reports the ID classification performance and the running time on the CIFAR-100 benchmark for universal comparability. Notice that this table only reports average AUROCs results for each benchmark. We also provide an Excel table to show the full experiment results, where \"FPR@95 / AUROC / AUPR\" is reported for each dataset in each benchmark. The \"Avg.\" value of the OOD detection benchmark can be compared with each other only if the background color is the same. MNIST CIFAR-10 CIFAR-100 ImageNet Avg.\n\n-Anomaly Detection extra data we use in this benchmark is the entire TinyImageNet training set, which is not purely OOD. In this case, among outlier-based methods, only UDG has a reasonable performance considering other methods are not tuned to accept this kind of extra data we provide. However, the choice of training outliers can greatly affect the performance of OOD detectors, so further discussion on this topic is worth exploring.\n\n\nPost-Hoc Methods Outperform Training in General\n\nFor OOD and OSR methods without extra data, we further split them into two parts, one that needs a training process and the other does not. Surprisingly, those methods that require training do not necessarily obtain higher performance. Generally, methods that require training do not outperform inference-only methods. Nevertheless, the trained models can be generally used in a combined way with the post-hoc methods, which could potentially further increase their performance.\n\nCIFAR Benchmark is NOT Easier than ImageNet Benchmark We find that the OOD detection performance score for the ImageNet dataset is generally higher than that of CIFAR-10 and CIFAR-100, which is another surprising discovery considering ImageNet is composed of more complex data than others and seems difficult. Admittedly, from the perspective of real-world application, solutions that perform well on higher-resolution datasets like ImageNet is more practical.\n\n\nPost-Hoc Methods are Making Progress\n\nIn general, recent post-hoc methods have better performance compared to previous methods, suggesting the direction of inference-only methods is promising and enjoy progress. It could be found that while previous methods focus on toy datasets, recent methods improve performance on more realistic datasets. For example, the classic MDS performs well on MNIST but poorly on CIFAR-10 and CIFAR-100, and recent KNN maintains good performance on MNIST, CIFAR-10, CIFAR-100, and also shows outstanding performance on ImageNet. Notably, methods that are compatible between toy datasets (MNIST) and real-world datasets (ImageNet) have received increasing attention.\n\n\nSome AD Methods are Good at Far-OOD\n\nAlthough AD methods were originally designed to detect pixel-level appearance differences on MVTec-AD dataset, it proves to be potent when it comes to far-OOD detection such as DRAEM and CutPaste. Both methods achieved high performance on far-OOD detection, especially when using CIFAR-100 as the in-distribution dataset.\n\n\nOSR Benchmarks Aligns with OOD Detection Benchmarks\n\nAt the beginning of the development of the OOD detection field, OOD was defined as those data that differs significantly from the in-distribution data. However, as the topic develops, the expectation of OOD detection today is to be able to discriminate class out-of-distribution samples, which is a more practical and challenging task. OSR benchmarks manually divide the categories into closed set and open set, so that the ID and OOD are differ in label distribution but with the same domain. The setting actually aligns with the near-OOD detection task, with negligible domain difference but explicit semantic shift. As the result, the experiment shows that better OSR methods usually have a better near-OOD result (e.g., KNN). In sum, OSR benchmarks align with OOD detection benchmarks to a great extent.\n\nComparison on ID Accuracy As both OSR and OOD detection methods should not downgrade the classification capability, we also report the ID classification results on the CIFAR-100 benchmark for easy comparison. The result indicates that most of the OSR/OOD methods do not affect ID classification performance.\n\n\nModel Efficiency\n\nWe also report the entire training plus the testing time of the OOD detectors. For those methods that only require pretrained models, the training time for pretrained models is dismissed. Apart from the post-hoc methods that have minimal computational cost as expected, training with modified loss function (ConfBranch, G-ODIN, ARPL) takes a short training time but with decent performance. Data augmentation methods can help achieve great results but the computational cost is a bit higher than the aforementioned ones.\n\n\nOutlook and Conclusion\n\nIn this paper, we compare over 30 methods across the fields of AD, OSR, OOD detection, and model uncertainty, under a unified generalized OOD detection benchmark. Several insights are highlighted: 1) simple preprocessing methods can achieve the best score among the benchmark; 2) Extra data seems not necessary or requires further exploration; 3) Post-hoc methods make significant progress and generally outperform methods that require training. We also provide good protocols for our developers to easily integrate their methods into OpenOOD for fair and comprehensive comparisons to make substantial progress.\n\n\nWeakness\n\nThe weakness of the benchmark results is that every method only runs one time, without multiple runs with random seeds, due to the limited computational resource. For a similar reason, we do not include training methods on large-scale ImageNet.\n\n\nML Safety\n\nOut-of-distribution detection can be used to detect unexpected anomalies [7], emergent phenomena [80], unknown unknowns [6], and Black Swans [81]. Moreover, OOD detection can be used to detect malicious use or network intruders, and OOD detectors could detect when an adversary is trying to cause a system to fail. By reducing exposure to hazards, OOD detection can reduce risks and improve safety.\n\n\nFuture Work\n\nIn the future, apart from maintaining the codebase, it is promising to extend our benchmark towards more robust OOD detectors [82] and object-level OOD detection and generalization [69,83,84,85,86,87], which provides finer-grained visual guidance in safety-critical applications, such as autonomous driving and medical image analysis, etc.\n\n\nSocial Impact\n\nWe provide a unified and comprehensive evaluation of generalized OOD detection benchmark, which guides the community to conveniently pick out the most suitable methods. Also, the release of the open-source codebase OpenOOD greatly reduces the potential redundant work, which has a favorable societal impact.\n\n\n) OSR benchmark (CIFAR-6/4) (a) AD Benchmark (MVTec-AD)4\u00d7 OOD benchmark \u00d7 OSR benchmark 1\u00d7 AD benchmarkSpecies    \n\nFigure 2 :\n2Timeline and Taxonomy of Methodologies supported by OpenOOD. OpenOOD supports 35 methods (by the time of publication) that originated from anomaly detection (AD), open set recognition (OSR), OOD detection, and model uncertainty & data augmentation. Methodologies can be categorized into classification-based, density-based, distance-based, and reconstruction-based ones.\n\n\nthat uses temperature scaling with gradient-based input perturbations, MDS [18] that measures minimum mahalanobis distance from class centroids, EBO [14] that uses energy-based functions, GRAM [61] that computes gram matrix within hidden layers, DICE [62] with weight sparsification in the last layer, GradNorm [63] that focuses on gradient statistics, ReAct [64] that uses rectified activation, MLS [51] that uses maximum logits scores rather than softmax scores, KL-Matching\nAcknowledgments and Disclosure of Funding\nDelving deep into rectifiers: Surpassing human-level performance on imagenet classification. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, ICCV. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In ICCV, 2015. 1\n\nImagenet classification with deep convolutional neural networks. Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, NIPS. 14Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012. 1, 4\n\nThe open world assumption. Nick Drummond, Rob Shearer, eSI Workshop. Nick Drummond and Rob Shearer. The open world assumption. In eSI Workshop, 2006. 1\n\nA baseline for detecting misclassified and out-of-distribution examples in neural networks. Dan Hendrycks, Kevin Gimpel, ICLR. Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks. In ICLR, 2017. 1, 2, 6, 8\n\nGeneralized out-of-distribution detection: A survey. Jingkang Yang, Kaiyang Zhou, Yixuan Li, Ziwei Liu, arXiv:2110.11334arXiv preprintJingkang Yang, Kaiyang Zhou, Yixuan Li, and Ziwei Liu. Generalized out-of-distribution detection: A survey. arXiv preprint arXiv:2110.11334, 2021. 1, 2, 4\n\nUnsolved problems in ml safety. Dan Hendrycks, Nicholas Carlini, John Schulman, Jacob Steinhardt, arXiv:2109.13916110arXiv preprintDan Hendrycks, Nicholas Carlini, John Schulman, and Jacob Steinhardt. Unsolved problems in ml safety. arXiv preprint arXiv:2109.13916, 2021. 1, 10\n\nX-risk analysis for ai research. Dan Hendrycks, Mantas Mazeika, abs/2206.05862ArXiv. 110Dan Hendrycks and Mantas Mazeika. X-risk analysis for ai research. ArXiv, abs/2206.05862, 2022. 1, 10\n\nKaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, Chen Change Loy, arXiv:2103.02503Domain generalization: A survey. arXiv preprintKaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, and Chen Change Loy. Domain generalization: A survey. arXiv preprint arXiv:2103.02503, 2021. 1\n\nLearning confidence for out-of-distribution detection in neural networks. Terrance Devries, W Graham, Taylor, arXiv:1802.04865arXiv preprintTerrance DeVries and Graham W Taylor. Learning confidence for out-of-distribution detection in neural networks. arXiv preprint arXiv:1802.04865, 2018. 2, 6, 8\n\nEnergy-based open-world uncertainty modeling for confidence calibration. Yezhen Wang, Bo Li, Tong Che, Kaiyang Zhou, Ziwei Liu, Dongsheng Li, ICCV. Yezhen Wang, Bo Li, Tong Che, Kaiyang Zhou, Ziwei Liu, and Dongsheng Li. Energy-based open-world uncertainty modeling for confidence calibration. In ICCV, 2021. 2\n\nOn the importance of gradients for detecting distributional shifts in the wild. Rui Huang, Andrew Geng, Yixuan Li, NeurIPS. Rui Huang, Andrew Geng, and Yixuan Li. On the importance of gradients for detecting distributional shifts in the wild. In NeurIPS, 2021. 2\n\nPredictive uncertainty estimation via prior networks. Andrey Malinin, Mark Gales, NeurIPS. Andrey Malinin and Mark Gales. Predictive uncertainty estimation via prior networks. In NeurIPS, 2018. 2\n\nEnhancing the reliability of out-of-distribution image detection in neural networks. Shiyu Liang, Yixuan Li, Rayadurgam Srikant, In ICLR. Shiyu Liang, Yixuan Li, and Rayadurgam Srikant. Enhancing the reliability of out-of-distribution image detection in neural networks. In ICLR, 2018. 2, 6, 8\n\nEnergy-based out-of-distribution detection. Weitang Liu, Xiaoyun Wang, D John, Yixuan Owens, Li, NeurIPS, 2020. 6Weitang Liu, Xiaoyun Wang, John D Owens, and Yixuan Li. Energy-based out-of-distribution detection. In NeurIPS, 2020. 2, 6, 8\n\nLatent space autoregression for novelty detection. Davide Abati, Angelo Porrello, Simone Calderara, Rita Cucchiara, CVPR. Davide Abati, Angelo Porrello, Simone Calderara, and Rita Cucchiara. Latent space autoregression for novelty detection. In CVPR, 2019. 2\n\nGenerative probabilistic novelty detection with adversarial autoencoders. Stanislav Pidhorskyi, Ranya Almohsen, A Donald, Gianfranco Adjeroh, Doretto, NeurIPS. Stanislav Pidhorskyi, Ranya Almohsen, Donald A Adjeroh, and Gianfranco Doretto. Generative proba- bilistic novelty detection with adversarial autoencoders. In NeurIPS, 2018. 2\n\nDeep autoencoding gaussian mixture model for unsupervised anomaly detection. Bo Zong, Qi Song, Martin Renqiang Min, Wei Cheng, Cristian Lumezanu, Daeki Cho, Haifeng Chen, In ICLR. 2Bo Zong, Qi Song, Martin Renqiang Min, Wei Cheng, Cristian Lumezanu, Daeki Cho, and Haifeng Chen. Deep autoencoding gaussian mixture model for unsupervised anomaly detection. In ICLR, 2018. 2\n\nA simple unified framework for detecting out-of-distribution samples and adversarial attacks. Kimin Lee, Kibok Lee, Honglak Lee, Jinwoo Shin, NeurIPS. Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. In NeurIPS, 2018. 2, 6, 8\n\nA simple fix to mahalanobis distance for improving near-ood detection. Jie Ren, Stanislav Fort, Jeremiah Liu, Abhijit Guha Roy, Shreyas Padhy, Balaji Lakshminarayanan, arXiv:2106.09022arXiv preprintJie Ren, Stanislav Fort, Jeremiah Liu, Abhijit Guha Roy, Shreyas Padhy, and Balaji Lakshminarayanan. A simple fix to mahalanobis distance for improving near-ood detection. arXiv preprint arXiv:2106.09022, 2021. 2\n\nImproving reconstruction autoencoder out-of-distribution detection with mahalanobis distance. Taylor Denouden, Rick Salay, Krzysztof Czarnecki, Vahdat Abdelzad, Buu Phan, Sachin Vernekar, arXiv:1812.02765arXiv preprintTaylor Denouden, Rick Salay, Krzysztof Czarnecki, Vahdat Abdelzad, Buu Phan, and Sachin Vernekar. Improving reconstruction autoencoder out-of-distribution detection with mahalanobis distance. arXiv preprint arXiv:1812.02765, 2018. 2\n\nRethinking reconstruction autoencoder-based out-of-distribution detection. Yibo Zhou, CVPR. 2022Yibo Zhou. Rethinking reconstruction autoencoder-based out-of-distribution detection. In CVPR, 2022. 2\n\nDeep anomaly detection with outlier exposure. Dan Hendrycks, Mantas Mazeika, Thomas Dietterich, ICLR. Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich. Deep anomaly detection with outlier exposure. In ICLR, 2019. 2, 7, 8\n\n80 million tiny images: A large data set for nonparametric object and scene recognition. Antonio Torralba, Rob Fergus, William T Freeman, TPAMI. 24Antonio Torralba, Rob Fergus, and William T Freeman. 80 million tiny images: A large data set for nonparametric object and scene recognition. TPAMI, 2008. 2, 4\n\nUnsupervised out-of-distribution detection by maximum classifier discrepancy. Qing Yu, Kiyoharu Aizawa, ICCV. Qing Yu and Kiyoharu Aizawa. Unsupervised out-of-distribution detection by maximum classifier discrep- ancy. In ICCV, 2019. 2, 7, 8\n\nWhy relu networks yield high-confidence predictions far away from the training data and how to mitigate the problem. Matthias Hein, Maksym Andriushchenko, Julian Bitterwolf, CVPR. Matthias Hein, Maksym Andriushchenko, and Julian Bitterwolf. Why relu networks yield high-confidence predictions far away from the training data and how to mitigate the problem. In CVPR, 2019. 2\n\nCutpaste: Self-supervised learning for anomaly detection and localization. Chun-Liang Li, Kihyuk Sohn, Jinsung Yoon, Tomas Pfister, CVPR, 2021. 6Chun-Liang Li, Kihyuk Sohn, Jinsung Yoon, and Tomas Pfister. Cutpaste: Self-supervised learning for anomaly detection and localization. In CVPR, 2021. 2, 6, 8\n\nGenerative openmax for multi-class open set classification. Zongyuan Ge, Sergey Demyanov, Zetao Chen, Rahil Garnavi, BMVC. ZongYuan Ge, Sergey Demyanov, Zetao Chen, and Rahil Garnavi. Generative openmax for multi-class open set classification. In BMVC, 2017. 2\n\nOpen set learning with counterfactual images. Lawrence Neal, Matthew Olson, Xiaoli Fern, Weng-Keen Wong, Fuxin Li, ECCV. Lawrence Neal, Matthew Olson, Xiaoli Fern, Weng-Keen Wong, and Fuxin Li. Open set learning with counterfactual images. In ECCV, 2018. 2\n\nProbability models for open set recognition. J Walter, Scheirer, P Lalit, Terrance E Jain, Boult, TPAMI. 2Walter J Scheirer, Lalit P Jain, and Terrance E Boult. Probability models for open set recognition. TPAMI, 2014. 2\n\nExtreme value theory. Handbook of applicable mathematics. L Richard, Smith, Richard L Smith. Extreme value theory. Handbook of applicable mathematics, 1990. 2\n\nA survey on image data augmentation for deep learning. Connor Shorten, M Taghi, Khoshgoftaar, Journal of big data. 2Connor Shorten and Taghi M Khoshgoftaar. A survey on image data augmentation for deep learning. Journal of big data, 2019. 2\n\nTony Fei, Kai Ming Liu, Zhi-Hua Ting, Zhou, ICDM. Isolation forestFei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. Isolation forest. In ICDM, 2008. 2\n\nOne-class classification: Concept learning in the absence of counterexamples. David Martinus Johannes Tax, David Martinus Johannes Tax. One-class classification: Concept learning in the absence of counter- examples. 2002. 2\n\nAnomaly detection: A survey. Varun Chandola, Arindam Banerjee, Vipin Kumar, ACM computing surveys. 3Varun Chandola, Arindam Banerjee, and Vipin Kumar. Anomaly detection: A survey. ACM computing surveys (CSUR), 2009. 3\n\nSongqiao Han, Xiyang Hu, Hailiang Huang, Mingqi Jiang, Yue Zhao, arXiv:2206.09426Adbench: Anomaly detection benchmark. 2022arXiv preprintSongqiao Han, Xiyang Hu, Hailiang Huang, Mingqi Jiang, and Yue Zhao. Adbench: Anomaly detection benchmark. arXiv preprint arXiv:2206.09426, 2022. 3\n\nDetecting semantic anomalies. Faruk Ahmed, Aaron Courville, AAAI. 2020Faruk Ahmed and Aaron Courville. Detecting semantic anomalies. In AAAI, 2020. 3\n\nA unifying review of deep and shallow anomaly detection. Lukas Ruff, Robert A Jacob R Kauffmann, Gr\u00e9goire Vandermeulen, Wojciech Montavon, Marius Samek, Kloft, G Thomas, Klaus-Robert Dietterich, M\u00fcller, Proceedings of the IEEE. 20213Lukas Ruff, Jacob R Kauffmann, Robert A Vandermeulen, Gr\u00e9goire Montavon, Wojciech Samek, Marius Kloft, Thomas G Dietterich, and Klaus-Robert M\u00fcller. A unifying review of deep and shallow anomaly detection. Proceedings of the IEEE, 2021. 3\n\nMvtec ad-a comprehensive real-world dataset for unsupervised anomaly detection. Paul Bergmann, Michael Fauser, David Sattlegger, Carsten Steger, CVPR. Paul Bergmann, Michael Fauser, David Sattlegger, and Carsten Steger. Mvtec ad-a comprehensive real-world dataset for unsupervised anomaly detection. In CVPR, 2019. 3\n\nToward open set recognition. J Walter, Anderson Scheirer, De Rezende, Archana Rocha, Terrance E Sapkota, Boult, TPAMI. 4Walter J Scheirer, Anderson de Rezende Rocha, Archana Sapkota, and Terrance E Boult. Toward open set recognition. TPAMI, 2013. 4\n\nOpen-set recognition: A good closed-set classifier is all you need. Sagar Vaze, Kai Han, Andrea Vedaldi, Andrew Zisserman, ICLR, 2022. 4Sagar Vaze, Kai Han, Andrea Vedaldi, and Andrew Zisserman. Open-set recognition: A good closed-set classifier is all you need. In ICLR, 2022. 4\n\nAdversarial reciprocal points learning for open set recognition. Guangyao Chen, Peixi Peng, Xiangqian Wang, Yonghong Tian, arXiv:2103.009534arXiv preprintGuangyao Chen, Peixi Peng, Xiangqian Wang, and Yonghong Tian. Adversarial reciprocal points learning for open set recognition. arXiv preprint arXiv:2103.00953, 2021. 4, 6, 8\n\nThe mnist database of handwritten digit images for machine learning research. Li Deng, IEEE Signal Processing Magazine. 45Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal Processing Magazine, 2012. 4, 5\n\nLearning multiple layers of features from tiny images. Alex Krizhevsky, Geoffrey Hinton, Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. 4\n\nCifar-10 and cifar-100 datasets. Alex Krizhevsky, Vinod Nair, Geoffrey Hinton, 6Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 and cifar-100 datasets. URl: https://www. cs. toronto. edu/kriz/cifar. html, 6(1):1, 2009. 4\n\nMnist-c: A robustness benchmark for computer vision. Norman Mu, Justin Gilmer, Norman Mu and Justin Gilmer. Mnist-c: A robustness benchmark for computer vision. ICML-W, 2019. 4\n\nYaroslav Bulatov, Notmnist dataset. Google (Books/OCR). 2Tech. RepYaroslav Bulatov. Notmnist dataset. Google (Books/OCR), Tech. Rep.[Online]. Available: http://yaroslavvb. blogspot. it/2011/09/notmnist-dataset. html, 2, 2011. 4\n\nFashion-mnist: a novel image dataset for benchmarking machine learning algorithms. Han Xiao, Kashif Rasul, Roland Vollgraf, arXiv:1708.07747arXiv preprintHan Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017. 4\n\n. Gustaf Kylberg, Kylberg texture dataset v. 1.0. 2011. 4, 5Gustaf Kylberg. Kylberg texture dataset v. 1.0. 2011. 4, 5\n\nPlaces: A 10 million image database for scene recognition. Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, Antonio Torralba, IEEE Transactions on Pattern Analysis and Machine Intelligence. 20174Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017. 4\n\nSemantically coherent out-of-distribution detection. Jingkang Yang, Haoqi Wang, Litong Feng, Xiaopeng Yan, Huabin Zheng, Wayne Zhang, Ziwei Liu, ICCV, 2021. 4. 7Jingkang Yang, Haoqi Wang, Litong Feng, Xiaopeng Yan, Huabin Zheng, Wayne Zhang, and Ziwei Liu. Semantically coherent out-of-distribution detection. In ICCV, 2021. 4, 7, 8\n\nScaling out-of-distribution detection for real-world settings. Dan Hendrycks, Steven Basart, Mantas Mazeika, Mohammadreza Mostajabi, Jacob Steinhardt, Dawn Song, ICML, 2022. 4. 6Dan Hendrycks, Steven Basart, Mantas Mazeika, Mohammadreza Mostajabi, Jacob Steinhardt, and Dawn Song. Scaling out-of-distribution detection for real-world settings. In ICML, 2022. 4, 6, 8\n\nMos: Towards scaling out-of-distribution detection for large semantic space. Rui Huang, Yixuan Li, CVPR, 2021. 4. 6Rui Huang and Yixuan Li. Mos: Towards scaling out-of-distribution detection for large semantic space. In CVPR, 2021. 4, 6, 8\n\n. Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, Dawn Song, Natural adversarial examples. CVPR. 4Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. CVPR, 2021. 4\n\nVim: Out-of-distribution with virtual-logit matching. Haoqi Wang, Zhizhong Li, Litong Feng, Wayne Zhang, CVPR, 2022. 4. 6Haoqi Wang, Zhizhong Li, Litong Feng, and Wayne Zhang. Vim: Out-of-distribution with virtual-logit matching. In CVPR, 2022. 4, 6, 8\n\nReading digits in natural images with unsupervised feature learning. Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, Andrew Y Ng, Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. 2011. 5\n\nDeep one-class classification. Lukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Ahmed Shoaib, Alexander Siddiqui, Emmanuel Binder, Marius M\u00fcller, Kloft, ICML. 6Lukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Shoaib Ahmed Siddiqui, Alexander Binder, Emmanuel M\u00fcller, and Marius Kloft. Deep one-class classification. In ICML, 2018. 6, 8\n\nKarsten Roth, Latha Pemula, Joaquin Zepeda, Bernhard Sch\u00f6lkopf, Thomas Brox, Peter Gehler, arXiv:2106.08265Towards total recall in industrial anomaly detection. 6arXiv preprintKarsten Roth, Latha Pemula, Joaquin Zepeda, Bernhard Sch\u00f6lkopf, Thomas Brox, and Peter Gehler. Towards total recall in industrial anomaly detection. arXiv preprint arXiv:2106.08265, 2021. 6, 8\n\nDraem-a discriminatively trained reconstruction embedding for surface anomaly detection. Vitjan Zavrtanik, Matej Kristan, Danijel Sko\u010daj, ICCV. 6Vitjan Zavrtanik, Matej Kristan, and Danijel Sko\u010daj. Draem-a discriminatively trained reconstruction embedding for surface anomaly detection. In ICCV, 2021. 6, 8\n\nTowards open set deep networks. Abhijit Bendale, Terrance E Boult, CVPR. 6Abhijit Bendale and Terrance E Boult. Towards open set deep networks. In CVPR, 2016. 6, 8\n\nOpengan: Open-set recognition via open data generation. Shu Kong, Deva Ramanan, ICCV. 6Shu Kong and Deva Ramanan. Opengan: Open-set recognition via open data generation. In ICCV, 2021. 6, 8\n\nDetecting out-of-distribution examples with gram matrices. Shama Chandramouli, Sageev Sastry, Oore, ICML. 6Chandramouli Shama Sastry and Sageev Oore. Detecting out-of-distribution examples with gram matrices. In ICML, 2020. 6, 8\n\nDice: Leveraging sparsification for out-of-distribution detection. Yiyou Sun, Sharon Li, ECCV, 2022. 6Yiyou Sun and Sharon Li. Dice: Leveraging sparsification for out-of-distribution detection. In ECCV, 2022. 6, 8\n\nOn the importance of gradients for detecting distributional shifts in the wild. Rui Huang, Andrew Geng, Yixuan Li, NeurIPS. 6Rui Huang, Andrew Geng, and Yixuan Li. On the importance of gradients for detecting distributional shifts in the wild. In NeurIPS, 2021. 6, 8\n\nReact: Out-of-distribution detection with rectified activations. Yiyou Sun, Chuan Guo, Yixuan Li, NeurIPS. 6Yiyou Sun, Chuan Guo, and Yixuan Li. React: Out-of-distribution detection with rectified activations. In NeurIPS, 2021. 6, 8\n\nOut-of-distribution detection with deep nearest neighbors. Yiyou Sun, Yifei Ming, Xiaojin Zhu, Yixuan Li, 7Yiyou Sun, Yifei Ming, Xiaojin Zhu, and Yixuan Li. Out-of-distribution detection with deep nearest neighbors. ICML, 2022. 6, 7, 8\n\nAn image is worth 16x16 words: Transformers for image recognition at scale. ICLR. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR, 2021. 6\n\nGeneralized odin: Detecting out-of-distribution image without learning from out-of-distribution data. Yen-Chang Hsu, Yilin Shen, Hongxia Jin, Zsolt Kira, CVPR. 6Yen-Chang Hsu, Yilin Shen, Hongxia Jin, and Zsolt Kira. Generalized odin: Detecting out-of-distribution image without learning from out-of-distribution data. In CVPR, 2020. 6, 8\n\nCsi: Novelty detection via contrastive learning on distributionally shifted instances. Jihoon Tack, Sangwoo Mo, Jongheon Jeong, Jinwoo Shin, NeurIPS. 6Jihoon Tack, Sangwoo Mo, Jongheon Jeong, and Jinwoo Shin. Csi: Novelty detection via contrastive learning on distributionally shifted instances. In NeurIPS, 2020. 6, 8\n\nVos: Learning what you don't know by virtual outlier synthesis. Xuefeng Du, Zhaoning Wang, Mu Cai, Yixuan Li, ICLR, 2022. 810Xuefeng Du, Zhaoning Wang, Mu Cai, and Yixuan Li. Vos: Learning what you don't know by virtual outlier synthesis. In ICLR, 2022. 6, 8, 10\n\nMitigating neural network overconfidence with logit normalization. Hongxin Wei, Renchunzi Xie, Hao Cheng, Lei Feng, Bo An, Yixuan Li, ICML. 6Hongxin Wei, Renchunzi Xie, Hao Cheng, Lei Feng, Bo An, and Yixuan Li. Mitigating neural network overconfidence with logit normalization. In ICML, 2022. 6, 8\n\nDropout as a bayesian approximation: Representing model uncertainty in deep learning. Yarin Gal, Zoubin Ghahramani, ICML. 7Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In ICML, 2016. 7, 8\n\nSimple and scalable predictive uncertainty estimation using deep ensembles. Balaji Lakshminarayanan, Alexander Pritzel, Charles Blundell, 7Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. NeurIPS, 2017. 7, 8\n\nOn calibration of modern neural networks. Chuan Guo, Geoff Pleiss, Yu Sun, Kilian Q Weinberger, ICML. 7Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In ICML, 2017. 7, 8\n\nOn mixup training: Improved calibration and predictive uncertainty for deep neural networks. Gopinath Sunil Thulasidasan, Jeff Chennupati, Tanmoy Bilmes, Sarah Bhattacharya, Michalak, NeurIPS. 7Sunil Thulasidasan, Gopinath Chennupati, Jeff Bilmes, Tanmoy Bhattacharya, and Sarah Michalak. On mixup training: Improved calibration and predictive uncertainty for deep neural networks. In NeurIPS, 2019. 7, 8\n\nCutmix: Regularization strategy to train strong classifiers with localizable features. Sangdoo Yun, Dongyoon Han, Sanghyuk Seong Joon Oh, Junsuk Chun, Youngjoon Choe, Yoo, CVPR. 7Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In CVPR, 2019. 7, 8\n\nDan Hendrycks, Andy Zou, Mantas Mazeika, Leonard Tang, Dawn Song, Jacob Steinhardt, Pixmix, arXiv:2112.05135Dreamlike pictures comprehensively improve safety measures. 7arXiv preprintDan Hendrycks, Andy Zou, Mantas Mazeika, Leonard Tang, Dawn Song, and Jacob Steinhardt. Pixmix: Dreamlike pictures comprehensively improve safety measures. arXiv preprint arXiv:2112.05135, 2021. 7, 8\n\nGradient-based learning applied to document recognition. Yann Lecun, L\u00e9on Bottou, Yoshua Bengio, Patrick Haffner, Proceedings of the IEEE. 8611Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998. 7\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, CVPR. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 7\n\nEnsemble methods in machine learning. G Thomas, Dietterich, International workshop on multiple classifier systems. Thomas G Dietterich. Ensemble methods in machine learning. In International workshop on multiple classifier systems, 2000. 7\n\n. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, William Fedus, Emergent abilities of large language models. ArXiv, abs/2206.07682, 2022Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. ArXiv, abs/2206.07682, 2022. 10\n\nThe black swan: The impact of the highly improbable. Nassim Taleb, Nassim Taleb. The black swan: The impact of the highly improbable. 2007. 10\n\nFull-spectrum out-of-distribution detection. Jingkang Yang, Kaiyang Zhou, Ziwei Liu, arXiv:2204.05306arXiv preprintJingkang Yang, Kaiyang Zhou, and Ziwei Liu. Full-spectrum out-of-distribution detection. arXiv preprint arXiv:2204.05306, 2022. 10\n\nUnknown-aware object detection: Learning what you don't know from videos in the wild. Xuefeng Du, Xin Wang, Gabriel Gozum, Yixuan Li, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionXuefeng Du, Xin Wang, Gabriel Gozum, and Yixuan Li. Unknown-aware object detection: Learning what you don't know from videos in the wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13678-13688, 2022. 10\n\nSiren: Shaping representations for detecting out-of-distribution objects. Xuefeng Du, Gabriel Gozum, Yifei Ming, Yixuan Li, Advances in Neural Information Processing Systems. Xuefeng Du, Gabriel Gozum, Yifei Ming, and Yixuan Li. Siren: Shaping representations for detecting out-of-distribution objects. In Advances in Neural Information Processing Systems, 2022. 10\n\nOpen-vocabulary object detection via vision and language knowledge distillation. Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, Yin Cui, arXiv:2104.13921arXiv preprintXiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui. Open-vocabulary object detection via vision and language knowledge distillation. arXiv preprint arXiv:2104.13921, 2021. 10\n\nSimple openvocabulary object detection with vision transformers. Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, arXiv:2205.06230arXiv preprintMatthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Doso- vitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, et al. Simple open- vocabulary object detection with vision transformers. arXiv preprint arXiv:2205.06230, 2022. 10\n\nOpen-vocabulary object detection using captions. Alireza Zareian, Kevin Dela Rosa, Derek Hao Hu, Shih-Fu Chang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionAlireza Zareian, Kevin Dela Rosa, Derek Hao Hu, and Shih-Fu Chang. Open-vocabulary object detection using captions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14393-14402, 2021. 10\n", "annotations": {"author": "[{\"end\":131,\"start\":67},{\"end\":252,\"start\":132},{\"end\":371,\"start\":253},{\"end\":491,\"start\":372},{\"end\":612,\"start\":492},{\"end\":676,\"start\":613},{\"end\":722,\"start\":677},{\"end\":769,\"start\":723},{\"end\":826,\"start\":770},{\"end\":883,\"start\":827},{\"end\":941,\"start\":884},{\"end\":1005,\"start\":942},{\"end\":1052,\"start\":1006},{\"end\":1107,\"start\":1053},{\"end\":1164,\"start\":1108},{\"end\":1225,\"start\":1165}]", "publisher": null, "author_last_name": "[{\"end\":80,\"start\":76},{\"end\":144,\"start\":140},{\"end\":263,\"start\":260},{\"end\":383,\"start\":379},{\"end\":504,\"start\":500},{\"end\":625,\"start\":621},{\"end\":687,\"start\":683},{\"end\":736,\"start\":732},{\"end\":775,\"start\":773},{\"end\":836,\"start\":833},{\"end\":894,\"start\":892},{\"end\":954,\"start\":950},{\"end\":1017,\"start\":1012},{\"end\":1066,\"start\":1057},{\"end\":1117,\"start\":1115},{\"end\":1174,\"start\":1171}]", "author_first_name": "[{\"end\":75,\"start\":67},{\"end\":139,\"start\":132},{\"end\":259,\"start\":253},{\"end\":378,\"start\":372},{\"end\":499,\"start\":492},{\"end\":620,\"start\":613},{\"end\":682,\"start\":677},{\"end\":731,\"start\":723},{\"end\":772,\"start\":770},{\"end\":832,\"start\":827},{\"end\":891,\"start\":884},{\"end\":949,\"start\":942},{\"end\":1011,\"start\":1006},{\"end\":1056,\"start\":1053},{\"end\":1114,\"start\":1108},{\"end\":1170,\"start\":1165}]", "author_affiliation": "[{\"end\":130,\"start\":82},{\"end\":209,\"start\":146},{\"end\":251,\"start\":211},{\"end\":328,\"start\":265},{\"end\":370,\"start\":330},{\"end\":448,\"start\":385},{\"end\":490,\"start\":450},{\"end\":569,\"start\":506},{\"end\":611,\"start\":571},{\"end\":675,\"start\":627},{\"end\":721,\"start\":689},{\"end\":768,\"start\":738},{\"end\":825,\"start\":777},{\"end\":882,\"start\":838},{\"end\":940,\"start\":896},{\"end\":1004,\"start\":956},{\"end\":1051,\"start\":1019},{\"end\":1106,\"start\":1068},{\"end\":1163,\"start\":1119},{\"end\":1224,\"start\":1176}]", "title": "[{\"end\":64,\"start\":1},{\"end\":1289,\"start\":1226}]", "venue": null, "abstract": "[{\"end\":2563,\"start\":1310}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2792,\"start\":2789},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2794,\"start\":2792},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2879,\"start\":2876},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3059,\"start\":3056},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3061,\"start\":3059},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3099,\"start\":3096},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3101,\"start\":3099},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3265,\"start\":3262},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3400,\"start\":3397},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3525,\"start\":3522},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3697,\"start\":3694},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3873,\"start\":3870},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3876,\"start\":3873},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3920,\"start\":3916},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3950,\"start\":3946},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3995,\"start\":3992},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3998,\"start\":3995},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4195,\"start\":4191},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4352,\"start\":4348},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4355,\"start\":4352},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4358,\"start\":4355},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4507,\"start\":4503},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4510,\"start\":4507},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4657,\"start\":4653},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4660,\"start\":4657},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4945,\"start\":4941},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4948,\"start\":4945},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5140,\"start\":5136},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5354,\"start\":5350},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":5357,\"start\":5354},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5637,\"start\":5633},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":5640,\"start\":5637},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":5664,\"start\":5660},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":5667,\"start\":5664},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5708,\"start\":5704},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":5869,\"start\":5865},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":5872,\"start\":5869},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5925,\"start\":5922},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6493,\"start\":6490},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":9602,\"start\":9598},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9716,\"start\":9712},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9945,\"start\":9941},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9948,\"start\":9945},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":10168,\"start\":10164},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10293,\"start\":10289},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":11672,\"start\":11668},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11866,\"start\":11863},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":12071,\"start\":12067},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":12074,\"start\":12071},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":12293,\"start\":12289},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":12504,\"start\":12500},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":12523,\"start\":12519},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12607,\"start\":12603},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":13242,\"start\":13239},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":13564,\"start\":13560},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":13579,\"start\":13575},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":13595,\"start\":13591},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":13613,\"start\":13610},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":13871,\"start\":13867},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":14016,\"start\":14012},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":14038,\"start\":14034},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":14140,\"start\":14136},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":14185,\"start\":14181},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":14206,\"start\":14203},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":14243,\"start\":14239},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":14717,\"start\":14713},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":14738,\"start\":14735},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":14846,\"start\":14842},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":14878,\"start\":14874},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":14897,\"start\":14893},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":14911,\"start\":14907},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":14931,\"start\":14927},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":15052,\"start\":15048},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":15194,\"start\":15190},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":15216,\"start\":15212},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":15352,\"start\":15348},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":15384,\"start\":15380},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":15403,\"start\":15399},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":15417,\"start\":15413},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":15437,\"start\":15433},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":15611,\"start\":15607},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":15646,\"start\":15642},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":15679,\"start\":15675},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":15716,\"start\":15712},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":15857,\"start\":15853},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":15869,\"start\":15865},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":15883,\"start\":15879},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":18609,\"start\":18605},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":18802,\"start\":18798},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":18995,\"start\":18991},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":19112,\"start\":19108},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":19515,\"start\":19511},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":19781,\"start\":19777},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":19960,\"start\":19956},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":20515,\"start\":20512},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":20730,\"start\":20726},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":20735,\"start\":20731},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":20852,\"start\":20848},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":21020,\"start\":21016},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":21527,\"start\":21523},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":21816,\"start\":21813},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":21909,\"start\":21905},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":21991,\"start\":21987},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":22126,\"start\":22122},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":22207,\"start\":22203},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":22300,\"start\":22296},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":22835,\"start\":22831},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":22918,\"start\":22914},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":23017,\"start\":23013},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":23737,\"start\":23733},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":23759,\"start\":23755},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":23786,\"start\":23782},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":24047,\"start\":24043},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":24060,\"start\":24056},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":24077,\"start\":24073},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":24676,\"start\":24672},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":24732,\"start\":24728},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":24867,\"start\":24863},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":25348,\"start\":25344},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":26285,\"start\":26281},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":26301,\"start\":26297},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":26575,\"start\":26571},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":26590,\"start\":26586},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":26783,\"start\":26779},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":26829,\"start\":26825},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":33226,\"start\":33223},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":33251,\"start\":33247},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":33273,\"start\":33270},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":33295,\"start\":33291},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":33694,\"start\":33690},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":33749,\"start\":33745},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":33752,\"start\":33749},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":33755,\"start\":33752},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":33758,\"start\":33755},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":33761,\"start\":33758},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":33764,\"start\":33761}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":34345,\"start\":34229},{\"attributes\":{\"id\":\"fig_1\"},\"end\":34729,\"start\":34346},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":35208,\"start\":34730}]", "paragraph": "[{\"end\":3526,\"start\":2579},{\"end\":4661,\"start\":3528},{\"end\":5423,\"start\":4663},{\"end\":6289,\"start\":5425},{\"end\":7332,\"start\":6291},{\"end\":7553,\"start\":7334},{\"end\":7789,\"start\":7555},{\"end\":8048,\"start\":7830},{\"end\":9062,\"start\":8050},{\"end\":9456,\"start\":9107},{\"end\":10169,\"start\":9489},{\"end\":11193,\"start\":10171},{\"end\":11867,\"start\":11229},{\"end\":12608,\"start\":11886},{\"end\":13243,\"start\":12653},{\"end\":13851,\"start\":13262},{\"end\":14516,\"start\":13861},{\"end\":14987,\"start\":14529},{\"end\":15464,\"start\":15001},{\"end\":15910,\"start\":15480},{\"end\":16825,\"start\":15922},{\"end\":17222,\"start\":16840},{\"end\":18199,\"start\":17224},{\"end\":18524,\"start\":18227},{\"end\":19428,\"start\":18564},{\"end\":20097,\"start\":19471},{\"end\":20426,\"start\":20149},{\"end\":21110,\"start\":20447},{\"end\":21670,\"start\":21112},{\"end\":22644,\"start\":21672},{\"end\":23439,\"start\":22679},{\"end\":24173,\"start\":23517},{\"end\":24422,\"start\":24189},{\"end\":25906,\"start\":24449},{\"end\":26630,\"start\":25923},{\"end\":28038,\"start\":26665},{\"end\":28477,\"start\":28040},{\"end\":29007,\"start\":28529},{\"end\":29469,\"start\":29009},{\"end\":30167,\"start\":29510},{\"end\":30528,\"start\":30207},{\"end\":31391,\"start\":30584},{\"end\":31700,\"start\":31393},{\"end\":32241,\"start\":31721},{\"end\":32879,\"start\":32268},{\"end\":33136,\"start\":32892},{\"end\":33548,\"start\":33150},{\"end\":33903,\"start\":33564},{\"end\":34228,\"start\":33921}]", "formula": null, "table_ref": "[{\"end\":24331,\"start\":24324},{\"end\":25991,\"start\":25984},{\"end\":27022,\"start\":27015}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2577,\"start\":2565},{\"end\":7828,\"start\":7792},{\"attributes\":{\"n\":\"2\"},\"end\":9105,\"start\":9065},{\"attributes\":{\"n\":\"2.1\"},\"end\":9487,\"start\":9459},{\"attributes\":{\"n\":\"2.2\"},\"end\":11227,\"start\":11196},{\"end\":11884,\"start\":11870},{\"attributes\":{\"n\":\"2.3\"},\"end\":12651,\"start\":12611},{\"end\":13260,\"start\":13246},{\"end\":13859,\"start\":13854},{\"end\":14527,\"start\":14519},{\"end\":14999,\"start\":14990},{\"end\":15478,\"start\":15467},{\"attributes\":{\"n\":\"2.4\"},\"end\":15920,\"start\":15913},{\"attributes\":{\"n\":\"2.5\"},\"end\":16838,\"start\":16828},{\"attributes\":{\"n\":\"3\"},\"end\":18225,\"start\":18202},{\"attributes\":{\"n\":\"3.1\"},\"end\":18562,\"start\":18527},{\"attributes\":{\"n\":\"3.2\"},\"end\":19469,\"start\":19431},{\"attributes\":{\"n\":\"3.3\"},\"end\":20147,\"start\":20100},{\"end\":20445,\"start\":20429},{\"end\":22677,\"start\":22647},{\"attributes\":{\"n\":\"3.4\"},\"end\":23515,\"start\":23442},{\"attributes\":{\"n\":\"4\"},\"end\":24187,\"start\":24176},{\"attributes\":{\"n\":\"4.1\"},\"end\":24447,\"start\":24425},{\"attributes\":{\"n\":\"4.2\"},\"end\":25921,\"start\":25909},{\"end\":26663,\"start\":26633},{\"end\":28527,\"start\":28480},{\"end\":29508,\"start\":29472},{\"end\":30205,\"start\":30170},{\"end\":30582,\"start\":30531},{\"end\":31719,\"start\":31703},{\"attributes\":{\"n\":\"5\"},\"end\":32266,\"start\":32244},{\"end\":32890,\"start\":32882},{\"end\":33148,\"start\":33139},{\"end\":33562,\"start\":33551},{\"end\":33919,\"start\":33906},{\"end\":34357,\"start\":34347}]", "table": null, "figure_caption": "[{\"end\":34345,\"start\":34231},{\"end\":34729,\"start\":34359},{\"end\":35208,\"start\":34732}]", "figure_ref": "[{\"end\":8404,\"start\":8396},{\"end\":9396,\"start\":9388},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18375,\"start\":18367}]", "bib_author_first_name": "[{\"end\":35351,\"start\":35344},{\"end\":35363,\"start\":35356},{\"end\":35379,\"start\":35371},{\"end\":35389,\"start\":35385},{\"end\":35636,\"start\":35632},{\"end\":35653,\"start\":35649},{\"end\":35673,\"start\":35665},{\"end\":35675,\"start\":35674},{\"end\":35865,\"start\":35861},{\"end\":35879,\"start\":35876},{\"end\":36082,\"start\":36079},{\"end\":36099,\"start\":36094},{\"end\":36326,\"start\":36318},{\"end\":36340,\"start\":36333},{\"end\":36353,\"start\":36347},{\"end\":36363,\"start\":36358},{\"end\":36590,\"start\":36587},{\"end\":36610,\"start\":36602},{\"end\":36624,\"start\":36620},{\"end\":36640,\"start\":36635},{\"end\":36870,\"start\":36867},{\"end\":36888,\"start\":36882},{\"end\":37032,\"start\":37025},{\"end\":37044,\"start\":37039},{\"end\":37052,\"start\":37050},{\"end\":37062,\"start\":37059},{\"end\":37081,\"start\":37070},{\"end\":37373,\"start\":37365},{\"end\":37384,\"start\":37383},{\"end\":37670,\"start\":37664},{\"end\":37679,\"start\":37677},{\"end\":37688,\"start\":37684},{\"end\":37701,\"start\":37694},{\"end\":37713,\"start\":37708},{\"end\":37728,\"start\":37719},{\"end\":37986,\"start\":37983},{\"end\":38000,\"start\":37994},{\"end\":38013,\"start\":38007},{\"end\":38227,\"start\":38221},{\"end\":38241,\"start\":38237},{\"end\":38454,\"start\":38449},{\"end\":38468,\"start\":38462},{\"end\":38483,\"start\":38473},{\"end\":38710,\"start\":38703},{\"end\":38723,\"start\":38716},{\"end\":38731,\"start\":38730},{\"end\":38744,\"start\":38738},{\"end\":38956,\"start\":38950},{\"end\":38970,\"start\":38964},{\"end\":38987,\"start\":38981},{\"end\":39003,\"start\":38999},{\"end\":39242,\"start\":39233},{\"end\":39260,\"start\":39255},{\"end\":39272,\"start\":39271},{\"end\":39291,\"start\":39281},{\"end\":39575,\"start\":39573},{\"end\":39584,\"start\":39582},{\"end\":39597,\"start\":39591},{\"end\":39606,\"start\":39598},{\"end\":39615,\"start\":39612},{\"end\":39631,\"start\":39623},{\"end\":39647,\"start\":39642},{\"end\":39660,\"start\":39653},{\"end\":39969,\"start\":39964},{\"end\":39980,\"start\":39975},{\"end\":39993,\"start\":39986},{\"end\":40005,\"start\":39999},{\"end\":40268,\"start\":40265},{\"end\":40283,\"start\":40274},{\"end\":40298,\"start\":40290},{\"end\":40316,\"start\":40304},{\"end\":40329,\"start\":40322},{\"end\":40343,\"start\":40337},{\"end\":40706,\"start\":40700},{\"end\":40721,\"start\":40717},{\"end\":40738,\"start\":40729},{\"end\":40756,\"start\":40750},{\"end\":40770,\"start\":40767},{\"end\":40783,\"start\":40777},{\"end\":41137,\"start\":41133},{\"end\":41307,\"start\":41304},{\"end\":41325,\"start\":41319},{\"end\":41341,\"start\":41335},{\"end\":41580,\"start\":41573},{\"end\":41594,\"start\":41591},{\"end\":41612,\"start\":41603},{\"end\":41874,\"start\":41870},{\"end\":41887,\"start\":41879},{\"end\":42160,\"start\":42152},{\"end\":42173,\"start\":42167},{\"end\":42196,\"start\":42190},{\"end\":42496,\"start\":42486},{\"end\":42507,\"start\":42501},{\"end\":42521,\"start\":42514},{\"end\":42533,\"start\":42528},{\"end\":42784,\"start\":42776},{\"end\":42795,\"start\":42789},{\"end\":42811,\"start\":42806},{\"end\":42823,\"start\":42818},{\"end\":43032,\"start\":43024},{\"end\":43046,\"start\":43039},{\"end\":43060,\"start\":43054},{\"end\":43076,\"start\":43067},{\"end\":43088,\"start\":43083},{\"end\":43282,\"start\":43281},{\"end\":43302,\"start\":43301},{\"end\":43318,\"start\":43310},{\"end\":43320,\"start\":43319},{\"end\":43517,\"start\":43516},{\"end\":43679,\"start\":43673},{\"end\":43690,\"start\":43689},{\"end\":43864,\"start\":43860},{\"end\":43873,\"start\":43870},{\"end\":43878,\"start\":43874},{\"end\":43891,\"start\":43884},{\"end\":44110,\"start\":44087},{\"end\":44268,\"start\":44263},{\"end\":44286,\"start\":44279},{\"end\":44302,\"start\":44297},{\"end\":44461,\"start\":44453},{\"end\":44473,\"start\":44467},{\"end\":44486,\"start\":44478},{\"end\":44500,\"start\":44494},{\"end\":44511,\"start\":44508},{\"end\":44774,\"start\":44769},{\"end\":44787,\"start\":44782},{\"end\":44952,\"start\":44947},{\"end\":44965,\"start\":44959},{\"end\":44967,\"start\":44966},{\"end\":44995,\"start\":44987},{\"end\":45018,\"start\":45010},{\"end\":45035,\"start\":45029},{\"end\":45051,\"start\":45050},{\"end\":45072,\"start\":45060},{\"end\":45447,\"start\":45443},{\"end\":45465,\"start\":45458},{\"end\":45479,\"start\":45474},{\"end\":45499,\"start\":45492},{\"end\":45711,\"start\":45710},{\"end\":45728,\"start\":45720},{\"end\":45758,\"start\":45751},{\"end\":45774,\"start\":45766},{\"end\":45776,\"start\":45775},{\"end\":46004,\"start\":45999},{\"end\":46014,\"start\":46011},{\"end\":46026,\"start\":46020},{\"end\":46042,\"start\":46036},{\"end\":46285,\"start\":46277},{\"end\":46297,\"start\":46292},{\"end\":46313,\"start\":46304},{\"end\":46328,\"start\":46320},{\"end\":46621,\"start\":46619},{\"end\":46854,\"start\":46850},{\"end\":46875,\"start\":46867},{\"end\":47026,\"start\":47022},{\"end\":47044,\"start\":47039},{\"end\":47059,\"start\":47051},{\"end\":47281,\"start\":47275},{\"end\":47292,\"start\":47286},{\"end\":47408,\"start\":47400},{\"end\":47715,\"start\":47712},{\"end\":47728,\"start\":47722},{\"end\":47742,\"start\":47736},{\"end\":47961,\"start\":47955},{\"end\":48137,\"start\":48132},{\"end\":48149,\"start\":48144},{\"end\":48167,\"start\":48161},{\"end\":48180,\"start\":48176},{\"end\":48195,\"start\":48188},{\"end\":48546,\"start\":48538},{\"end\":48558,\"start\":48553},{\"end\":48571,\"start\":48565},{\"end\":48586,\"start\":48578},{\"end\":48598,\"start\":48592},{\"end\":48611,\"start\":48606},{\"end\":48624,\"start\":48619},{\"end\":48885,\"start\":48882},{\"end\":48903,\"start\":48897},{\"end\":48918,\"start\":48912},{\"end\":48940,\"start\":48928},{\"end\":48957,\"start\":48952},{\"end\":48974,\"start\":48970},{\"end\":49267,\"start\":49264},{\"end\":49281,\"start\":49275},{\"end\":49433,\"start\":49430},{\"end\":49450,\"start\":49445},{\"end\":49463,\"start\":49457},{\"end\":49477,\"start\":49472},{\"end\":49494,\"start\":49490},{\"end\":49717,\"start\":49712},{\"end\":49732,\"start\":49724},{\"end\":49743,\"start\":49737},{\"end\":49755,\"start\":49750},{\"end\":49986,\"start\":49981},{\"end\":49998,\"start\":49995},{\"end\":50009,\"start\":50005},{\"end\":50028,\"start\":50018},{\"end\":50041,\"start\":50039},{\"end\":50054,\"start\":50046},{\"end\":50255,\"start\":50250},{\"end\":50268,\"start\":50262},{\"end\":50287,\"start\":50283},{\"end\":50303,\"start\":50298},{\"end\":50317,\"start\":50312},{\"end\":50335,\"start\":50326},{\"end\":50354,\"start\":50346},{\"end\":50369,\"start\":50363},{\"end\":50589,\"start\":50582},{\"end\":50601,\"start\":50596},{\"end\":50617,\"start\":50610},{\"end\":50634,\"start\":50626},{\"end\":50652,\"start\":50646},{\"end\":50664,\"start\":50659},{\"end\":51047,\"start\":51041},{\"end\":51064,\"start\":51059},{\"end\":51081,\"start\":51074},{\"end\":51299,\"start\":51292},{\"end\":51317,\"start\":51309},{\"end\":51319,\"start\":51318},{\"end\":51484,\"start\":51481},{\"end\":51495,\"start\":51491},{\"end\":51680,\"start\":51675},{\"end\":51701,\"start\":51695},{\"end\":51918,\"start\":51913},{\"end\":51930,\"start\":51924},{\"end\":52144,\"start\":52141},{\"end\":52158,\"start\":52152},{\"end\":52171,\"start\":52165},{\"end\":52399,\"start\":52394},{\"end\":52410,\"start\":52405},{\"end\":52422,\"start\":52416},{\"end\":52627,\"start\":52622},{\"end\":52638,\"start\":52633},{\"end\":52652,\"start\":52645},{\"end\":52664,\"start\":52658},{\"end\":52889,\"start\":52883},{\"end\":52908,\"start\":52903},{\"end\":52925,\"start\":52916},{\"end\":52942,\"start\":52938},{\"end\":52963,\"start\":52956},{\"end\":52976,\"start\":52970},{\"end\":52997,\"start\":52990},{\"end\":53016,\"start\":53008},{\"end\":53032,\"start\":53027},{\"end\":53049,\"start\":53042},{\"end\":53440,\"start\":53431},{\"end\":53451,\"start\":53446},{\"end\":53465,\"start\":53458},{\"end\":53476,\"start\":53471},{\"end\":53762,\"start\":53756},{\"end\":53776,\"start\":53769},{\"end\":53789,\"start\":53781},{\"end\":53803,\"start\":53797},{\"end\":54060,\"start\":54053},{\"end\":54073,\"start\":54065},{\"end\":54082,\"start\":54080},{\"end\":54094,\"start\":54088},{\"end\":54327,\"start\":54320},{\"end\":54342,\"start\":54333},{\"end\":54351,\"start\":54348},{\"end\":54362,\"start\":54359},{\"end\":54371,\"start\":54369},{\"end\":54382,\"start\":54376},{\"end\":54644,\"start\":54639},{\"end\":54656,\"start\":54650},{\"end\":54898,\"start\":54892},{\"end\":54926,\"start\":54917},{\"end\":54943,\"start\":54936},{\"end\":55165,\"start\":55160},{\"end\":55176,\"start\":55171},{\"end\":55187,\"start\":55185},{\"end\":55201,\"start\":55193},{\"end\":55443,\"start\":55435},{\"end\":55468,\"start\":55464},{\"end\":55487,\"start\":55481},{\"end\":55501,\"start\":55496},{\"end\":55842,\"start\":55835},{\"end\":55856,\"start\":55848},{\"end\":55870,\"start\":55862},{\"end\":55892,\"start\":55886},{\"end\":55908,\"start\":55899},{\"end\":56127,\"start\":56124},{\"end\":56143,\"start\":56139},{\"end\":56155,\"start\":56149},{\"end\":56172,\"start\":56165},{\"end\":56183,\"start\":56179},{\"end\":56195,\"start\":56190},{\"end\":56569,\"start\":56565},{\"end\":56581,\"start\":56577},{\"end\":56596,\"start\":56590},{\"end\":56612,\"start\":56605},{\"end\":56874,\"start\":56867},{\"end\":56886,\"start\":56879},{\"end\":56902,\"start\":56894},{\"end\":56912,\"start\":56908},{\"end\":57082,\"start\":57081},{\"end\":57291,\"start\":57286},{\"end\":57299,\"start\":57297},{\"end\":57310,\"start\":57305},{\"end\":57327,\"start\":57322},{\"end\":57342,\"start\":57336},{\"end\":57358,\"start\":57349},{\"end\":57373,\"start\":57369},{\"end\":57391,\"start\":57384},{\"end\":57404,\"start\":57399},{\"end\":57417,\"start\":57411},{\"end\":57429,\"start\":57427},{\"end\":57444,\"start\":57435},{\"end\":57461,\"start\":57456},{\"end\":57476,\"start\":57471},{\"end\":57488,\"start\":57484},{\"end\":57502,\"start\":57495},{\"end\":57947,\"start\":57941},{\"end\":58085,\"start\":58077},{\"end\":58099,\"start\":58092},{\"end\":58111,\"start\":58106},{\"end\":58372,\"start\":58365},{\"end\":58380,\"start\":58377},{\"end\":58394,\"start\":58387},{\"end\":58408,\"start\":58402},{\"end\":58896,\"start\":58889},{\"end\":58908,\"start\":58901},{\"end\":58921,\"start\":58916},{\"end\":58934,\"start\":58928},{\"end\":59268,\"start\":59263},{\"end\":59281,\"start\":59273},{\"end\":59295,\"start\":59287},{\"end\":59304,\"start\":59301},{\"end\":59588,\"start\":59580},{\"end\":59605,\"start\":59599},{\"end\":59623,\"start\":59617},{\"end\":59636,\"start\":59631},{\"end\":59650,\"start\":59646},{\"end\":59670,\"start\":59664},{\"end\":59692,\"start\":59684},{\"end\":59710,\"start\":59704},{\"end\":59725,\"start\":59718},{\"end\":59743,\"start\":59736},{\"end\":60125,\"start\":60118},{\"end\":60140,\"start\":60135},{\"end\":60145,\"start\":60141},{\"end\":60157,\"start\":60152},{\"end\":60161,\"start\":60158},{\"end\":60173,\"start\":60166}]", "bib_author_last_name": "[{\"end\":35354,\"start\":35352},{\"end\":35369,\"start\":35364},{\"end\":35383,\"start\":35380},{\"end\":35393,\"start\":35390},{\"end\":35647,\"start\":35637},{\"end\":35663,\"start\":35654},{\"end\":35682,\"start\":35676},{\"end\":35874,\"start\":35866},{\"end\":35887,\"start\":35880},{\"end\":36092,\"start\":36083},{\"end\":36106,\"start\":36100},{\"end\":36331,\"start\":36327},{\"end\":36345,\"start\":36341},{\"end\":36356,\"start\":36354},{\"end\":36367,\"start\":36364},{\"end\":36600,\"start\":36591},{\"end\":36618,\"start\":36611},{\"end\":36633,\"start\":36625},{\"end\":36651,\"start\":36641},{\"end\":36880,\"start\":36871},{\"end\":36896,\"start\":36889},{\"end\":37037,\"start\":37033},{\"end\":37048,\"start\":37045},{\"end\":37057,\"start\":37053},{\"end\":37068,\"start\":37063},{\"end\":37085,\"start\":37082},{\"end\":37381,\"start\":37374},{\"end\":37391,\"start\":37385},{\"end\":37399,\"start\":37393},{\"end\":37675,\"start\":37671},{\"end\":37682,\"start\":37680},{\"end\":37692,\"start\":37689},{\"end\":37706,\"start\":37702},{\"end\":37717,\"start\":37714},{\"end\":37731,\"start\":37729},{\"end\":37992,\"start\":37987},{\"end\":38005,\"start\":38001},{\"end\":38016,\"start\":38014},{\"end\":38235,\"start\":38228},{\"end\":38247,\"start\":38242},{\"end\":38460,\"start\":38455},{\"end\":38471,\"start\":38469},{\"end\":38491,\"start\":38484},{\"end\":38714,\"start\":38711},{\"end\":38728,\"start\":38724},{\"end\":38736,\"start\":38732},{\"end\":38750,\"start\":38745},{\"end\":38754,\"start\":38752},{\"end\":38962,\"start\":38957},{\"end\":38979,\"start\":38971},{\"end\":38997,\"start\":38988},{\"end\":39013,\"start\":39004},{\"end\":39253,\"start\":39243},{\"end\":39269,\"start\":39261},{\"end\":39279,\"start\":39273},{\"end\":39299,\"start\":39292},{\"end\":39308,\"start\":39301},{\"end\":39580,\"start\":39576},{\"end\":39589,\"start\":39585},{\"end\":39610,\"start\":39607},{\"end\":39621,\"start\":39616},{\"end\":39640,\"start\":39632},{\"end\":39651,\"start\":39648},{\"end\":39665,\"start\":39661},{\"end\":39973,\"start\":39970},{\"end\":39984,\"start\":39981},{\"end\":39997,\"start\":39994},{\"end\":40010,\"start\":40006},{\"end\":40272,\"start\":40269},{\"end\":40288,\"start\":40284},{\"end\":40302,\"start\":40299},{\"end\":40320,\"start\":40317},{\"end\":40335,\"start\":40330},{\"end\":40360,\"start\":40344},{\"end\":40715,\"start\":40707},{\"end\":40727,\"start\":40722},{\"end\":40748,\"start\":40739},{\"end\":40765,\"start\":40757},{\"end\":40775,\"start\":40771},{\"end\":40792,\"start\":40784},{\"end\":41142,\"start\":41138},{\"end\":41317,\"start\":41308},{\"end\":41333,\"start\":41326},{\"end\":41352,\"start\":41342},{\"end\":41589,\"start\":41581},{\"end\":41601,\"start\":41595},{\"end\":41620,\"start\":41613},{\"end\":41877,\"start\":41875},{\"end\":41894,\"start\":41888},{\"end\":42165,\"start\":42161},{\"end\":42188,\"start\":42174},{\"end\":42207,\"start\":42197},{\"end\":42499,\"start\":42497},{\"end\":42512,\"start\":42508},{\"end\":42526,\"start\":42522},{\"end\":42541,\"start\":42534},{\"end\":42787,\"start\":42785},{\"end\":42804,\"start\":42796},{\"end\":42816,\"start\":42812},{\"end\":42831,\"start\":42824},{\"end\":43037,\"start\":43033},{\"end\":43052,\"start\":43047},{\"end\":43065,\"start\":43061},{\"end\":43081,\"start\":43077},{\"end\":43091,\"start\":43089},{\"end\":43289,\"start\":43283},{\"end\":43299,\"start\":43291},{\"end\":43308,\"start\":43303},{\"end\":43325,\"start\":43321},{\"end\":43332,\"start\":43327},{\"end\":43525,\"start\":43518},{\"end\":43532,\"start\":43527},{\"end\":43687,\"start\":43680},{\"end\":43696,\"start\":43691},{\"end\":43710,\"start\":43698},{\"end\":43868,\"start\":43865},{\"end\":43882,\"start\":43879},{\"end\":43896,\"start\":43892},{\"end\":43902,\"start\":43898},{\"end\":44114,\"start\":44111},{\"end\":44277,\"start\":44269},{\"end\":44295,\"start\":44287},{\"end\":44308,\"start\":44303},{\"end\":44465,\"start\":44462},{\"end\":44476,\"start\":44474},{\"end\":44492,\"start\":44487},{\"end\":44506,\"start\":44501},{\"end\":44516,\"start\":44512},{\"end\":44780,\"start\":44775},{\"end\":44797,\"start\":44788},{\"end\":44957,\"start\":44953},{\"end\":44985,\"start\":44968},{\"end\":45008,\"start\":44996},{\"end\":45027,\"start\":45019},{\"end\":45041,\"start\":45036},{\"end\":45048,\"start\":45043},{\"end\":45058,\"start\":45052},{\"end\":45083,\"start\":45073},{\"end\":45091,\"start\":45085},{\"end\":45456,\"start\":45448},{\"end\":45472,\"start\":45466},{\"end\":45490,\"start\":45480},{\"end\":45506,\"start\":45500},{\"end\":45718,\"start\":45712},{\"end\":45737,\"start\":45729},{\"end\":45749,\"start\":45739},{\"end\":45764,\"start\":45759},{\"end\":45784,\"start\":45777},{\"end\":45791,\"start\":45786},{\"end\":46009,\"start\":46005},{\"end\":46018,\"start\":46015},{\"end\":46034,\"start\":46027},{\"end\":46052,\"start\":46043},{\"end\":46290,\"start\":46286},{\"end\":46302,\"start\":46298},{\"end\":46318,\"start\":46314},{\"end\":46333,\"start\":46329},{\"end\":46626,\"start\":46622},{\"end\":46865,\"start\":46855},{\"end\":46882,\"start\":46876},{\"end\":47037,\"start\":47027},{\"end\":47049,\"start\":47045},{\"end\":47066,\"start\":47060},{\"end\":47284,\"start\":47282},{\"end\":47299,\"start\":47293},{\"end\":47416,\"start\":47409},{\"end\":47720,\"start\":47716},{\"end\":47734,\"start\":47729},{\"end\":47751,\"start\":47743},{\"end\":47969,\"start\":47962},{\"end\":48142,\"start\":48138},{\"end\":48159,\"start\":48150},{\"end\":48174,\"start\":48168},{\"end\":48186,\"start\":48181},{\"end\":48204,\"start\":48196},{\"end\":48551,\"start\":48547},{\"end\":48563,\"start\":48559},{\"end\":48576,\"start\":48572},{\"end\":48590,\"start\":48587},{\"end\":48604,\"start\":48599},{\"end\":48617,\"start\":48612},{\"end\":48628,\"start\":48625},{\"end\":48895,\"start\":48886},{\"end\":48910,\"start\":48904},{\"end\":48926,\"start\":48919},{\"end\":48950,\"start\":48941},{\"end\":48968,\"start\":48958},{\"end\":48979,\"start\":48975},{\"end\":49273,\"start\":49268},{\"end\":49284,\"start\":49282},{\"end\":49443,\"start\":49434},{\"end\":49455,\"start\":49451},{\"end\":49470,\"start\":49464},{\"end\":49488,\"start\":49478},{\"end\":49499,\"start\":49495},{\"end\":49722,\"start\":49718},{\"end\":49735,\"start\":49733},{\"end\":49748,\"start\":49744},{\"end\":49761,\"start\":49756},{\"end\":49993,\"start\":49987},{\"end\":50003,\"start\":49999},{\"end\":50016,\"start\":50010},{\"end\":50037,\"start\":50029},{\"end\":50044,\"start\":50042},{\"end\":50057,\"start\":50055},{\"end\":50260,\"start\":50256},{\"end\":50281,\"start\":50269},{\"end\":50296,\"start\":50288},{\"end\":50310,\"start\":50304},{\"end\":50324,\"start\":50318},{\"end\":50344,\"start\":50336},{\"end\":50361,\"start\":50355},{\"end\":50376,\"start\":50370},{\"end\":50383,\"start\":50378},{\"end\":50594,\"start\":50590},{\"end\":50608,\"start\":50602},{\"end\":50624,\"start\":50618},{\"end\":50644,\"start\":50635},{\"end\":50657,\"start\":50653},{\"end\":50671,\"start\":50665},{\"end\":51057,\"start\":51048},{\"end\":51072,\"start\":51065},{\"end\":51088,\"start\":51082},{\"end\":51307,\"start\":51300},{\"end\":51325,\"start\":51320},{\"end\":51489,\"start\":51485},{\"end\":51503,\"start\":51496},{\"end\":51693,\"start\":51681},{\"end\":51708,\"start\":51702},{\"end\":51714,\"start\":51710},{\"end\":51922,\"start\":51919},{\"end\":51933,\"start\":51931},{\"end\":52150,\"start\":52145},{\"end\":52163,\"start\":52159},{\"end\":52174,\"start\":52172},{\"end\":52403,\"start\":52400},{\"end\":52414,\"start\":52411},{\"end\":52425,\"start\":52423},{\"end\":52631,\"start\":52628},{\"end\":52643,\"start\":52639},{\"end\":52656,\"start\":52653},{\"end\":52667,\"start\":52665},{\"end\":52901,\"start\":52890},{\"end\":52914,\"start\":52909},{\"end\":52936,\"start\":52926},{\"end\":52954,\"start\":52943},{\"end\":52968,\"start\":52964},{\"end\":52988,\"start\":52977},{\"end\":53006,\"start\":52998},{\"end\":53025,\"start\":53017},{\"end\":53040,\"start\":53033},{\"end\":53055,\"start\":53050},{\"end\":53444,\"start\":53441},{\"end\":53456,\"start\":53452},{\"end\":53469,\"start\":53466},{\"end\":53481,\"start\":53477},{\"end\":53767,\"start\":53763},{\"end\":53779,\"start\":53777},{\"end\":53795,\"start\":53790},{\"end\":53808,\"start\":53804},{\"end\":54063,\"start\":54061},{\"end\":54078,\"start\":54074},{\"end\":54086,\"start\":54083},{\"end\":54097,\"start\":54095},{\"end\":54331,\"start\":54328},{\"end\":54346,\"start\":54343},{\"end\":54357,\"start\":54352},{\"end\":54367,\"start\":54363},{\"end\":54374,\"start\":54372},{\"end\":54385,\"start\":54383},{\"end\":54648,\"start\":54645},{\"end\":54667,\"start\":54657},{\"end\":54915,\"start\":54899},{\"end\":54934,\"start\":54927},{\"end\":54952,\"start\":54944},{\"end\":55169,\"start\":55166},{\"end\":55183,\"start\":55177},{\"end\":55191,\"start\":55188},{\"end\":55212,\"start\":55202},{\"end\":55462,\"start\":55444},{\"end\":55479,\"start\":55469},{\"end\":55494,\"start\":55488},{\"end\":55514,\"start\":55502},{\"end\":55524,\"start\":55516},{\"end\":55846,\"start\":55843},{\"end\":55860,\"start\":55857},{\"end\":55884,\"start\":55871},{\"end\":55897,\"start\":55893},{\"end\":55913,\"start\":55909},{\"end\":55918,\"start\":55915},{\"end\":56137,\"start\":56128},{\"end\":56147,\"start\":56144},{\"end\":56163,\"start\":56156},{\"end\":56177,\"start\":56173},{\"end\":56188,\"start\":56184},{\"end\":56206,\"start\":56196},{\"end\":56214,\"start\":56208},{\"end\":56575,\"start\":56570},{\"end\":56588,\"start\":56582},{\"end\":56603,\"start\":56597},{\"end\":56620,\"start\":56613},{\"end\":56877,\"start\":56875},{\"end\":56892,\"start\":56887},{\"end\":56906,\"start\":56903},{\"end\":56916,\"start\":56913},{\"end\":57089,\"start\":57083},{\"end\":57101,\"start\":57091},{\"end\":57295,\"start\":57292},{\"end\":57303,\"start\":57300},{\"end\":57320,\"start\":57311},{\"end\":57334,\"start\":57328},{\"end\":57347,\"start\":57343},{\"end\":57367,\"start\":57359},{\"end\":57382,\"start\":57374},{\"end\":57397,\"start\":57392},{\"end\":57409,\"start\":57405},{\"end\":57425,\"start\":57418},{\"end\":57433,\"start\":57430},{\"end\":57454,\"start\":57445},{\"end\":57469,\"start\":57462},{\"end\":57482,\"start\":57477},{\"end\":57493,\"start\":57489},{\"end\":57508,\"start\":57503},{\"end\":57953,\"start\":57948},{\"end\":58090,\"start\":58086},{\"end\":58104,\"start\":58100},{\"end\":58115,\"start\":58112},{\"end\":58375,\"start\":58373},{\"end\":58385,\"start\":58381},{\"end\":58400,\"start\":58395},{\"end\":58411,\"start\":58409},{\"end\":58899,\"start\":58897},{\"end\":58914,\"start\":58909},{\"end\":58926,\"start\":58922},{\"end\":58937,\"start\":58935},{\"end\":59271,\"start\":59269},{\"end\":59285,\"start\":59282},{\"end\":59299,\"start\":59296},{\"end\":59308,\"start\":59305},{\"end\":59597,\"start\":59589},{\"end\":59615,\"start\":59606},{\"end\":59629,\"start\":59624},{\"end\":59644,\"start\":59637},{\"end\":59662,\"start\":59651},{\"end\":59682,\"start\":59671},{\"end\":59702,\"start\":59693},{\"end\":59716,\"start\":59711},{\"end\":59734,\"start\":59726},{\"end\":59748,\"start\":59744},{\"end\":60133,\"start\":60126},{\"end\":60150,\"start\":60146},{\"end\":60164,\"start\":60162},{\"end\":60179,\"start\":60174}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":13740328},\"end\":35565,\"start\":35251},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":195908774},\"end\":35832,\"start\":35567},{\"attributes\":{\"id\":\"b2\"},\"end\":35985,\"start\":35834},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":13046179},\"end\":36263,\"start\":35987},{\"attributes\":{\"doi\":\"arXiv:2110.11334\",\"id\":\"b4\"},\"end\":36553,\"start\":36265},{\"attributes\":{\"doi\":\"arXiv:2109.13916\",\"id\":\"b5\"},\"end\":36832,\"start\":36555},{\"attributes\":{\"doi\":\"abs/2206.05862\",\"id\":\"b6\",\"matched_paper_id\":249626439},\"end\":37023,\"start\":36834},{\"attributes\":{\"doi\":\"arXiv:2103.02503\",\"id\":\"b7\"},\"end\":37289,\"start\":37025},{\"attributes\":{\"doi\":\"arXiv:1802.04865\",\"id\":\"b8\"},\"end\":37589,\"start\":37291},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":236447335},\"end\":37901,\"start\":37591},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":238253184},\"end\":38165,\"start\":37903},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":3580844},\"end\":38362,\"start\":38167},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":3526391},\"end\":38657,\"start\":38364},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":222208700},\"end\":38897,\"start\":38659},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":70349897},\"end\":39157,\"start\":38899},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":49657108},\"end\":39494,\"start\":39159},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":51805340},\"end\":39868,\"start\":39496},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":49667948},\"end\":40192,\"start\":39870},{\"attributes\":{\"doi\":\"arXiv:2106.09022\",\"id\":\"b18\"},\"end\":40604,\"start\":40194},{\"attributes\":{\"doi\":\"arXiv:1812.02765\",\"id\":\"b19\"},\"end\":41056,\"start\":40606},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":247244877},\"end\":41256,\"start\":41058},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":54558282},\"end\":41482,\"start\":41258},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":7487588},\"end\":41790,\"start\":41484},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":199577454},\"end\":42033,\"start\":41792},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":55700923},\"end\":42409,\"start\":42035},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":233204792},\"end\":42714,\"start\":42411},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":40464958},\"end\":42976,\"start\":42716},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":51913282},\"end\":43234,\"start\":42978},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":9584833},\"end\":43456,\"start\":43236},{\"attributes\":{\"id\":\"b29\"},\"end\":43616,\"start\":43458},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":195811894},\"end\":43858,\"start\":43618},{\"attributes\":{\"id\":\"b31\"},\"end\":44007,\"start\":43860},{\"attributes\":{\"id\":\"b32\"},\"end\":44232,\"start\":44009},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":207172599},\"end\":44451,\"start\":44234},{\"attributes\":{\"doi\":\"arXiv:2206.09426\",\"id\":\"b34\"},\"end\":44737,\"start\":44453},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":199551903},\"end\":44888,\"start\":44739},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":221879428},\"end\":45361,\"start\":44890},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":189857704},\"end\":45679,\"start\":45363},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":12035411},\"end\":45929,\"start\":45681},{\"attributes\":{\"doi\":\"ICLR, 2022. 4\",\"id\":\"b39\"},\"end\":46210,\"start\":45931},{\"attributes\":{\"doi\":\"arXiv:2103.00953\",\"id\":\"b40\"},\"end\":46539,\"start\":46212},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":5280072},\"end\":46793,\"start\":46541},{\"attributes\":{\"id\":\"b42\"},\"end\":46987,\"start\":46795},{\"attributes\":{\"id\":\"b43\"},\"end\":47220,\"start\":46989},{\"attributes\":{\"id\":\"b44\"},\"end\":47398,\"start\":47222},{\"attributes\":{\"id\":\"b45\"},\"end\":47627,\"start\":47400},{\"attributes\":{\"doi\":\"arXiv:1708.07747\",\"id\":\"b46\"},\"end\":47951,\"start\":47629},{\"attributes\":{\"id\":\"b47\"},\"end\":48071,\"start\":47953},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":2608922},\"end\":48483,\"start\":48073},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":237303921},\"end\":48817,\"start\":48485},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":227407829},\"end\":49185,\"start\":48819},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":233739635},\"end\":49426,\"start\":49187},{\"attributes\":{\"id\":\"b52\"},\"end\":49656,\"start\":49428},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":247595202},\"end\":49910,\"start\":49658},{\"attributes\":{\"id\":\"b54\"},\"end\":50217,\"start\":49912},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":49312162},\"end\":50580,\"start\":50219},{\"attributes\":{\"doi\":\"arXiv:2106.08265\",\"id\":\"b56\"},\"end\":50950,\"start\":50582},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":237142564},\"end\":51258,\"start\":50952},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":14240373},\"end\":51423,\"start\":51260},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":233168799},\"end\":51614,\"start\":51425},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":221081652},\"end\":51844,\"start\":51616},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":250626952},\"end\":52059,\"start\":51846},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":238253184},\"end\":52327,\"start\":52061},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":244709089},\"end\":52561,\"start\":52329},{\"attributes\":{\"id\":\"b64\"},\"end\":52799,\"start\":52563},{\"attributes\":{\"id\":\"b65\"},\"end\":53327,\"start\":52801},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":211506988},\"end\":53667,\"start\":53329},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":220546269},\"end\":53987,\"start\":53669},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":246473127},\"end\":54251,\"start\":53989},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":248887336},\"end\":54551,\"start\":54253},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":160705},\"end\":54814,\"start\":54553},{\"attributes\":{\"id\":\"b71\"},\"end\":55116,\"start\":54816},{\"attributes\":{\"id\":\"b72\",\"matched_paper_id\":28671436},\"end\":55340,\"start\":55118},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":166228660},\"end\":55746,\"start\":55342},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":152282661},\"end\":56122,\"start\":55748},{\"attributes\":{\"doi\":\"arXiv:2112.05135\",\"id\":\"b75\"},\"end\":56506,\"start\":56124},{\"attributes\":{\"id\":\"b76\",\"matched_paper_id\":14542261},\"end\":56819,\"start\":56508},{\"attributes\":{\"id\":\"b77\",\"matched_paper_id\":206594692},\"end\":57041,\"start\":56821},{\"attributes\":{\"id\":\"b78\",\"matched_paper_id\":56776745},\"end\":57282,\"start\":57043},{\"attributes\":{\"id\":\"b79\"},\"end\":57886,\"start\":57284},{\"attributes\":{\"id\":\"b80\"},\"end\":58030,\"start\":57888},{\"attributes\":{\"doi\":\"arXiv:2204.05306\",\"id\":\"b81\"},\"end\":58277,\"start\":58032},{\"attributes\":{\"id\":\"b82\",\"matched_paper_id\":247315245},\"end\":58813,\"start\":58279},{\"attributes\":{\"id\":\"b83\"},\"end\":59180,\"start\":58815},{\"attributes\":{\"doi\":\"arXiv:2104.13921\",\"id\":\"b84\"},\"end\":59513,\"start\":59182},{\"attributes\":{\"doi\":\"arXiv:2205.06230\",\"id\":\"b85\"},\"end\":60067,\"start\":59515},{\"attributes\":{\"id\":\"b86\",\"matched_paper_id\":227126502},\"end\":60559,\"start\":60069}]", "bib_title": "[{\"end\":35342,\"start\":35251},{\"end\":35630,\"start\":35567},{\"end\":35859,\"start\":35834},{\"end\":36077,\"start\":35987},{\"end\":36865,\"start\":36834},{\"end\":37662,\"start\":37591},{\"end\":37981,\"start\":37903},{\"end\":38219,\"start\":38167},{\"end\":38447,\"start\":38364},{\"end\":38701,\"start\":38659},{\"end\":38948,\"start\":38899},{\"end\":39231,\"start\":39159},{\"end\":39571,\"start\":39496},{\"end\":39962,\"start\":39870},{\"end\":41131,\"start\":41058},{\"end\":41302,\"start\":41258},{\"end\":41571,\"start\":41484},{\"end\":41868,\"start\":41792},{\"end\":42150,\"start\":42035},{\"end\":42484,\"start\":42411},{\"end\":42774,\"start\":42716},{\"end\":43022,\"start\":42978},{\"end\":43279,\"start\":43236},{\"end\":43671,\"start\":43618},{\"end\":44261,\"start\":44234},{\"end\":44767,\"start\":44739},{\"end\":44945,\"start\":44890},{\"end\":45441,\"start\":45363},{\"end\":45708,\"start\":45681},{\"end\":46617,\"start\":46541},{\"end\":48130,\"start\":48073},{\"end\":48536,\"start\":48485},{\"end\":48880,\"start\":48819},{\"end\":49262,\"start\":49187},{\"end\":49710,\"start\":49658},{\"end\":50248,\"start\":50219},{\"end\":51039,\"start\":50952},{\"end\":51290,\"start\":51260},{\"end\":51479,\"start\":51425},{\"end\":51673,\"start\":51616},{\"end\":51911,\"start\":51846},{\"end\":52139,\"start\":52061},{\"end\":52392,\"start\":52329},{\"end\":53429,\"start\":53329},{\"end\":53754,\"start\":53669},{\"end\":54051,\"start\":53989},{\"end\":54318,\"start\":54253},{\"end\":54637,\"start\":54553},{\"end\":55158,\"start\":55118},{\"end\":55433,\"start\":55342},{\"end\":55833,\"start\":55748},{\"end\":56563,\"start\":56508},{\"end\":56865,\"start\":56821},{\"end\":57079,\"start\":57043},{\"end\":58363,\"start\":58279},{\"end\":58887,\"start\":58815},{\"end\":60116,\"start\":60069}]", "bib_author": "[{\"end\":35356,\"start\":35344},{\"end\":35371,\"start\":35356},{\"end\":35385,\"start\":35371},{\"end\":35395,\"start\":35385},{\"end\":35649,\"start\":35632},{\"end\":35665,\"start\":35649},{\"end\":35684,\"start\":35665},{\"end\":35876,\"start\":35861},{\"end\":35889,\"start\":35876},{\"end\":36094,\"start\":36079},{\"end\":36108,\"start\":36094},{\"end\":36333,\"start\":36318},{\"end\":36347,\"start\":36333},{\"end\":36358,\"start\":36347},{\"end\":36369,\"start\":36358},{\"end\":36602,\"start\":36587},{\"end\":36620,\"start\":36602},{\"end\":36635,\"start\":36620},{\"end\":36653,\"start\":36635},{\"end\":36882,\"start\":36867},{\"end\":36898,\"start\":36882},{\"end\":37039,\"start\":37025},{\"end\":37050,\"start\":37039},{\"end\":37059,\"start\":37050},{\"end\":37070,\"start\":37059},{\"end\":37087,\"start\":37070},{\"end\":37383,\"start\":37365},{\"end\":37393,\"start\":37383},{\"end\":37401,\"start\":37393},{\"end\":37677,\"start\":37664},{\"end\":37684,\"start\":37677},{\"end\":37694,\"start\":37684},{\"end\":37708,\"start\":37694},{\"end\":37719,\"start\":37708},{\"end\":37733,\"start\":37719},{\"end\":37994,\"start\":37983},{\"end\":38007,\"start\":37994},{\"end\":38018,\"start\":38007},{\"end\":38237,\"start\":38221},{\"end\":38249,\"start\":38237},{\"end\":38462,\"start\":38449},{\"end\":38473,\"start\":38462},{\"end\":38493,\"start\":38473},{\"end\":38716,\"start\":38703},{\"end\":38730,\"start\":38716},{\"end\":38738,\"start\":38730},{\"end\":38752,\"start\":38738},{\"end\":38756,\"start\":38752},{\"end\":38964,\"start\":38950},{\"end\":38981,\"start\":38964},{\"end\":38999,\"start\":38981},{\"end\":39015,\"start\":38999},{\"end\":39255,\"start\":39233},{\"end\":39271,\"start\":39255},{\"end\":39281,\"start\":39271},{\"end\":39301,\"start\":39281},{\"end\":39310,\"start\":39301},{\"end\":39582,\"start\":39573},{\"end\":39591,\"start\":39582},{\"end\":39612,\"start\":39591},{\"end\":39623,\"start\":39612},{\"end\":39642,\"start\":39623},{\"end\":39653,\"start\":39642},{\"end\":39667,\"start\":39653},{\"end\":39975,\"start\":39964},{\"end\":39986,\"start\":39975},{\"end\":39999,\"start\":39986},{\"end\":40012,\"start\":39999},{\"end\":40274,\"start\":40265},{\"end\":40290,\"start\":40274},{\"end\":40304,\"start\":40290},{\"end\":40322,\"start\":40304},{\"end\":40337,\"start\":40322},{\"end\":40362,\"start\":40337},{\"end\":40717,\"start\":40700},{\"end\":40729,\"start\":40717},{\"end\":40750,\"start\":40729},{\"end\":40767,\"start\":40750},{\"end\":40777,\"start\":40767},{\"end\":40794,\"start\":40777},{\"end\":41144,\"start\":41133},{\"end\":41319,\"start\":41304},{\"end\":41335,\"start\":41319},{\"end\":41354,\"start\":41335},{\"end\":41591,\"start\":41573},{\"end\":41603,\"start\":41591},{\"end\":41622,\"start\":41603},{\"end\":41879,\"start\":41870},{\"end\":41896,\"start\":41879},{\"end\":42167,\"start\":42152},{\"end\":42190,\"start\":42167},{\"end\":42209,\"start\":42190},{\"end\":42501,\"start\":42486},{\"end\":42514,\"start\":42501},{\"end\":42528,\"start\":42514},{\"end\":42543,\"start\":42528},{\"end\":42789,\"start\":42776},{\"end\":42806,\"start\":42789},{\"end\":42818,\"start\":42806},{\"end\":42833,\"start\":42818},{\"end\":43039,\"start\":43024},{\"end\":43054,\"start\":43039},{\"end\":43067,\"start\":43054},{\"end\":43083,\"start\":43067},{\"end\":43093,\"start\":43083},{\"end\":43291,\"start\":43281},{\"end\":43301,\"start\":43291},{\"end\":43310,\"start\":43301},{\"end\":43327,\"start\":43310},{\"end\":43334,\"start\":43327},{\"end\":43527,\"start\":43516},{\"end\":43534,\"start\":43527},{\"end\":43689,\"start\":43673},{\"end\":43698,\"start\":43689},{\"end\":43712,\"start\":43698},{\"end\":43870,\"start\":43860},{\"end\":43884,\"start\":43870},{\"end\":43898,\"start\":43884},{\"end\":43904,\"start\":43898},{\"end\":44116,\"start\":44087},{\"end\":44279,\"start\":44263},{\"end\":44297,\"start\":44279},{\"end\":44310,\"start\":44297},{\"end\":44467,\"start\":44453},{\"end\":44478,\"start\":44467},{\"end\":44494,\"start\":44478},{\"end\":44508,\"start\":44494},{\"end\":44518,\"start\":44508},{\"end\":44782,\"start\":44769},{\"end\":44799,\"start\":44782},{\"end\":44959,\"start\":44947},{\"end\":44987,\"start\":44959},{\"end\":45010,\"start\":44987},{\"end\":45029,\"start\":45010},{\"end\":45043,\"start\":45029},{\"end\":45050,\"start\":45043},{\"end\":45060,\"start\":45050},{\"end\":45085,\"start\":45060},{\"end\":45093,\"start\":45085},{\"end\":45458,\"start\":45443},{\"end\":45474,\"start\":45458},{\"end\":45492,\"start\":45474},{\"end\":45508,\"start\":45492},{\"end\":45720,\"start\":45710},{\"end\":45739,\"start\":45720},{\"end\":45751,\"start\":45739},{\"end\":45766,\"start\":45751},{\"end\":45786,\"start\":45766},{\"end\":45793,\"start\":45786},{\"end\":46011,\"start\":45999},{\"end\":46020,\"start\":46011},{\"end\":46036,\"start\":46020},{\"end\":46054,\"start\":46036},{\"end\":46292,\"start\":46277},{\"end\":46304,\"start\":46292},{\"end\":46320,\"start\":46304},{\"end\":46335,\"start\":46320},{\"end\":46628,\"start\":46619},{\"end\":46867,\"start\":46850},{\"end\":46884,\"start\":46867},{\"end\":47039,\"start\":47022},{\"end\":47051,\"start\":47039},{\"end\":47068,\"start\":47051},{\"end\":47286,\"start\":47275},{\"end\":47301,\"start\":47286},{\"end\":47418,\"start\":47400},{\"end\":47722,\"start\":47712},{\"end\":47736,\"start\":47722},{\"end\":47753,\"start\":47736},{\"end\":47971,\"start\":47955},{\"end\":48144,\"start\":48132},{\"end\":48161,\"start\":48144},{\"end\":48176,\"start\":48161},{\"end\":48188,\"start\":48176},{\"end\":48206,\"start\":48188},{\"end\":48553,\"start\":48538},{\"end\":48565,\"start\":48553},{\"end\":48578,\"start\":48565},{\"end\":48592,\"start\":48578},{\"end\":48606,\"start\":48592},{\"end\":48619,\"start\":48606},{\"end\":48630,\"start\":48619},{\"end\":48897,\"start\":48882},{\"end\":48912,\"start\":48897},{\"end\":48928,\"start\":48912},{\"end\":48952,\"start\":48928},{\"end\":48970,\"start\":48952},{\"end\":48981,\"start\":48970},{\"end\":49275,\"start\":49264},{\"end\":49286,\"start\":49275},{\"end\":49445,\"start\":49430},{\"end\":49457,\"start\":49445},{\"end\":49472,\"start\":49457},{\"end\":49490,\"start\":49472},{\"end\":49501,\"start\":49490},{\"end\":49724,\"start\":49712},{\"end\":49737,\"start\":49724},{\"end\":49750,\"start\":49737},{\"end\":49763,\"start\":49750},{\"end\":49995,\"start\":49981},{\"end\":50005,\"start\":49995},{\"end\":50018,\"start\":50005},{\"end\":50039,\"start\":50018},{\"end\":50046,\"start\":50039},{\"end\":50059,\"start\":50046},{\"end\":50262,\"start\":50250},{\"end\":50283,\"start\":50262},{\"end\":50298,\"start\":50283},{\"end\":50312,\"start\":50298},{\"end\":50326,\"start\":50312},{\"end\":50346,\"start\":50326},{\"end\":50363,\"start\":50346},{\"end\":50378,\"start\":50363},{\"end\":50385,\"start\":50378},{\"end\":50596,\"start\":50582},{\"end\":50610,\"start\":50596},{\"end\":50626,\"start\":50610},{\"end\":50646,\"start\":50626},{\"end\":50659,\"start\":50646},{\"end\":50673,\"start\":50659},{\"end\":51059,\"start\":51041},{\"end\":51074,\"start\":51059},{\"end\":51090,\"start\":51074},{\"end\":51309,\"start\":51292},{\"end\":51327,\"start\":51309},{\"end\":51491,\"start\":51481},{\"end\":51505,\"start\":51491},{\"end\":51695,\"start\":51675},{\"end\":51710,\"start\":51695},{\"end\":51716,\"start\":51710},{\"end\":51924,\"start\":51913},{\"end\":51935,\"start\":51924},{\"end\":52152,\"start\":52141},{\"end\":52165,\"start\":52152},{\"end\":52176,\"start\":52165},{\"end\":52405,\"start\":52394},{\"end\":52416,\"start\":52405},{\"end\":52427,\"start\":52416},{\"end\":52633,\"start\":52622},{\"end\":52645,\"start\":52633},{\"end\":52658,\"start\":52645},{\"end\":52669,\"start\":52658},{\"end\":52903,\"start\":52883},{\"end\":52916,\"start\":52903},{\"end\":52938,\"start\":52916},{\"end\":52956,\"start\":52938},{\"end\":52970,\"start\":52956},{\"end\":52990,\"start\":52970},{\"end\":53008,\"start\":52990},{\"end\":53027,\"start\":53008},{\"end\":53042,\"start\":53027},{\"end\":53057,\"start\":53042},{\"end\":53446,\"start\":53431},{\"end\":53458,\"start\":53446},{\"end\":53471,\"start\":53458},{\"end\":53483,\"start\":53471},{\"end\":53769,\"start\":53756},{\"end\":53781,\"start\":53769},{\"end\":53797,\"start\":53781},{\"end\":53810,\"start\":53797},{\"end\":54065,\"start\":54053},{\"end\":54080,\"start\":54065},{\"end\":54088,\"start\":54080},{\"end\":54099,\"start\":54088},{\"end\":54333,\"start\":54320},{\"end\":54348,\"start\":54333},{\"end\":54359,\"start\":54348},{\"end\":54369,\"start\":54359},{\"end\":54376,\"start\":54369},{\"end\":54387,\"start\":54376},{\"end\":54650,\"start\":54639},{\"end\":54669,\"start\":54650},{\"end\":54917,\"start\":54892},{\"end\":54936,\"start\":54917},{\"end\":54954,\"start\":54936},{\"end\":55171,\"start\":55160},{\"end\":55185,\"start\":55171},{\"end\":55193,\"start\":55185},{\"end\":55214,\"start\":55193},{\"end\":55464,\"start\":55435},{\"end\":55481,\"start\":55464},{\"end\":55496,\"start\":55481},{\"end\":55516,\"start\":55496},{\"end\":55526,\"start\":55516},{\"end\":55848,\"start\":55835},{\"end\":55862,\"start\":55848},{\"end\":55886,\"start\":55862},{\"end\":55899,\"start\":55886},{\"end\":55915,\"start\":55899},{\"end\":55920,\"start\":55915},{\"end\":56139,\"start\":56124},{\"end\":56149,\"start\":56139},{\"end\":56165,\"start\":56149},{\"end\":56179,\"start\":56165},{\"end\":56190,\"start\":56179},{\"end\":56208,\"start\":56190},{\"end\":56216,\"start\":56208},{\"end\":56577,\"start\":56565},{\"end\":56590,\"start\":56577},{\"end\":56605,\"start\":56590},{\"end\":56622,\"start\":56605},{\"end\":56879,\"start\":56867},{\"end\":56894,\"start\":56879},{\"end\":56908,\"start\":56894},{\"end\":56918,\"start\":56908},{\"end\":57091,\"start\":57081},{\"end\":57103,\"start\":57091},{\"end\":57297,\"start\":57286},{\"end\":57305,\"start\":57297},{\"end\":57322,\"start\":57305},{\"end\":57336,\"start\":57322},{\"end\":57349,\"start\":57336},{\"end\":57369,\"start\":57349},{\"end\":57384,\"start\":57369},{\"end\":57399,\"start\":57384},{\"end\":57411,\"start\":57399},{\"end\":57427,\"start\":57411},{\"end\":57435,\"start\":57427},{\"end\":57456,\"start\":57435},{\"end\":57471,\"start\":57456},{\"end\":57484,\"start\":57471},{\"end\":57495,\"start\":57484},{\"end\":57510,\"start\":57495},{\"end\":57955,\"start\":57941},{\"end\":58092,\"start\":58077},{\"end\":58106,\"start\":58092},{\"end\":58117,\"start\":58106},{\"end\":58377,\"start\":58365},{\"end\":58387,\"start\":58377},{\"end\":58402,\"start\":58387},{\"end\":58413,\"start\":58402},{\"end\":58901,\"start\":58889},{\"end\":58916,\"start\":58901},{\"end\":58928,\"start\":58916},{\"end\":58939,\"start\":58928},{\"end\":59273,\"start\":59263},{\"end\":59287,\"start\":59273},{\"end\":59301,\"start\":59287},{\"end\":59310,\"start\":59301},{\"end\":59599,\"start\":59580},{\"end\":59617,\"start\":59599},{\"end\":59631,\"start\":59617},{\"end\":59646,\"start\":59631},{\"end\":59664,\"start\":59646},{\"end\":59684,\"start\":59664},{\"end\":59704,\"start\":59684},{\"end\":59718,\"start\":59704},{\"end\":59736,\"start\":59718},{\"end\":59750,\"start\":59736},{\"end\":60135,\"start\":60118},{\"end\":60152,\"start\":60135},{\"end\":60166,\"start\":60152},{\"end\":60181,\"start\":60166}]", "bib_venue": "[{\"end\":58562,\"start\":58496},{\"end\":60330,\"start\":60264},{\"end\":35399,\"start\":35395},{\"end\":35688,\"start\":35684},{\"end\":35901,\"start\":35889},{\"end\":36112,\"start\":36108},{\"end\":36316,\"start\":36265},{\"end\":36585,\"start\":36555},{\"end\":36917,\"start\":36912},{\"end\":37134,\"start\":37103},{\"end\":37363,\"start\":37291},{\"end\":37737,\"start\":37733},{\"end\":38025,\"start\":38018},{\"end\":38256,\"start\":38249},{\"end\":38500,\"start\":38493},{\"end\":38769,\"start\":38756},{\"end\":39019,\"start\":39015},{\"end\":39317,\"start\":39310},{\"end\":39674,\"start\":39667},{\"end\":40019,\"start\":40012},{\"end\":40263,\"start\":40194},{\"end\":40698,\"start\":40606},{\"end\":41148,\"start\":41144},{\"end\":41358,\"start\":41354},{\"end\":41627,\"start\":41622},{\"end\":41900,\"start\":41896},{\"end\":42213,\"start\":42209},{\"end\":42553,\"start\":42543},{\"end\":42837,\"start\":42833},{\"end\":43097,\"start\":43093},{\"end\":43339,\"start\":43334},{\"end\":43514,\"start\":43458},{\"end\":43731,\"start\":43712},{\"end\":43908,\"start\":43904},{\"end\":44085,\"start\":44009},{\"end\":44331,\"start\":44310},{\"end\":44570,\"start\":44534},{\"end\":44803,\"start\":44799},{\"end\":45116,\"start\":45093},{\"end\":45512,\"start\":45508},{\"end\":45798,\"start\":45793},{\"end\":45997,\"start\":45931},{\"end\":46275,\"start\":46212},{\"end\":46659,\"start\":46628},{\"end\":46848,\"start\":46795},{\"end\":47020,\"start\":46989},{\"end\":47273,\"start\":47222},{\"end\":47454,\"start\":47418},{\"end\":47710,\"start\":47629},{\"end\":48268,\"start\":48206},{\"end\":48643,\"start\":48630},{\"end\":48994,\"start\":48981},{\"end\":49299,\"start\":49286},{\"end\":49535,\"start\":49501},{\"end\":49776,\"start\":49763},{\"end\":49979,\"start\":49912},{\"end\":50389,\"start\":50385},{\"end\":50741,\"start\":50689},{\"end\":51094,\"start\":51090},{\"end\":51331,\"start\":51327},{\"end\":51509,\"start\":51505},{\"end\":51720,\"start\":51716},{\"end\":51945,\"start\":51935},{\"end\":52183,\"start\":52176},{\"end\":52434,\"start\":52427},{\"end\":52620,\"start\":52563},{\"end\":52881,\"start\":52801},{\"end\":53487,\"start\":53483},{\"end\":53817,\"start\":53810},{\"end\":54109,\"start\":54099},{\"end\":54391,\"start\":54387},{\"end\":54673,\"start\":54669},{\"end\":54890,\"start\":54816},{\"end\":55218,\"start\":55214},{\"end\":55533,\"start\":55526},{\"end\":55924,\"start\":55920},{\"end\":56290,\"start\":56232},{\"end\":56645,\"start\":56622},{\"end\":56922,\"start\":56918},{\"end\":57156,\"start\":57103},{\"end\":57939,\"start\":57888},{\"end\":58075,\"start\":58032},{\"end\":58494,\"start\":58413},{\"end\":58988,\"start\":58939},{\"end\":59261,\"start\":59182},{\"end\":59578,\"start\":59515},{\"end\":60262,\"start\":60181}]"}}}, "year": 2023, "month": 12, "day": 17}