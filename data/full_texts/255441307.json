{"id": 255441307, "updated": "2023-01-19 23:29:24.178", "metadata": {"title": "A Light Bug Triage Framework for Applying Large Pre-trained Language Model", "authors": "[{\"first\":\"Jaehyung\",\"last\":\"Lee\",\"middle\":[]},{\"first\":\"Kisun\",\"last\":\"Han\",\"middle\":[]},{\"first\":\"Hwanjo\",\"last\":\"Yu\",\"middle\":[]}]", "venue": null, "journal": "Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Assigning appropriate developers to the bugs is one of the main challenges in bug triage. Demands for automatic bug triage are increasing in the industry, as manual bug triage is labor-intensive and time-consuming in large projects. The key to the bug triage task is extracting semantic information from a bug report. In recent years, large Pre-trained Language Models (PLMs) including BERT [4] have achieved dramatic progress in the natural language processing (NLP) domain. However, applying large PLMs to the bug triage task for extracting semantic information has several challenges. In this paper, we address the challenges and propose a novel framework for bug triage named LBT-P, standing for Light Bug Triage framework with a Pre-trained language model. It compresses a large PLM into small and fast models using knowledge distillation techniques and also prevents catastrophic forgetting of PLM by introducing knowledge preservation fine-tuning. We also develop a new loss function exploiting representations of earlier layers as well as deeper layers in order to handle the overthinking problem. We demonstrate our proposed framework on the real-world private dataset and three public real-world datasets [11]: Google Chromium, Mozilla Core, and Mozilla Firefox. The result of the experiments shows the superiority of LBT-P.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/kbse/0002HY22", "doi": "10.1145/3551349.3556898"}}, "content": {"source": {"pdf_hash": "fa28e75fc93267021ecd7ee7f8a36b60261243b0", "pdf_src": "ACM", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "7824a62a691d1168b874912f6e3ace8b5b57071b", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/fa28e75fc93267021ecd7ee7f8a36b60261243b0.txt", "contents": "\nA Light Bug Triage Framework for Applying Large Pre-trained Language Model\n\n\nJaehyung Lee \nKisun Han ksun.han@samsung.com \nHwanjo Yu hwanjoyu@postech.ac.kr \n\nPohang University of Science and Technology (POSTECH\n) Republic of Korea\n\n\nSamsung Research\nRepublic of Korea\n\n\nPohang University of Science and Technology\n(POSTECH) Republic of Korea\n\nA Light Bug Triage Framework for Applying Large Pre-trained Language Model\n10.1145/3551349.3556898ACM Reference Format: Jaehyung Lee, Kisun Han, and Hwanjo Yu. 2022. A Light Bug Triage Frame-work for Applying Large Pre-trained Language Model. In 37th IEEE/ACM International Conference on Automated Software Engineering (ASE '22), Oc-tober 10-14, 2022, Rochester, MI, USA. ACM, New York, NY, USA, 11 pages.Bug triagePre-trained language modelBERTKnowledge distilla- tionCatastrophic forgettingOverthinking\nAssigning appropriate developers to the bugs is one of the main challenges in bug triage. Demands for automatic bug triage are increasing in the industry, as manual bug triage is labor-intensive and time-consuming in large projects. The key to the bug triage task is extracting semantic information from a bug report. In recent years, large Pre-trained Language Models (PLMs) including BERT [4]  have achieved dramatic progress in the natural language processing (NLP) domain. However, applying large PLMs to the bug triage task for extracting semantic information has several challenges. In this paper, we address the challenges and propose a novel framework for bug triage named LBT-P, standing for Light Bug Triage framework with a Pre-trained language model. It compresses a large PLM into small and fast models using knowledge distillation techniques and also prevents catastrophic forgetting of PLM by introducing knowledge preservation fine-tuning. We also develop a new loss function exploiting representations of earlier layers as well as deeper layers in order to handle the overthinking problem. We demonstrate our proposed framework on the realworld private dataset and three public real-world datasets [11]: Google Chromium, Mozilla Core, and Mozilla Firefox. The result of the experiments shows the superiority of LBT-P.\n\nINTRODUCTION\n\nDuring the development and maintenance of large projects, numerous bugs are reported. These bugs often cause serious problems in the projects, so it is important to fix them quickly. Bug triage helps to manage all reported issues properly. Triage, which is a term from medicine, is a process of assigning priority to patients' treatments based on their conditions and circumstances. Triage helps to treat as many patients as possible with limited resources. Bug triage is a process of prioritizing the bugs and assigning appropriate developers to the bugs. When the project is small, assigning developers to bugs is not a big deal. However, as the project grows, it becomes more difficult to find suitable developers to fix bugs. This is because each developer's skills are various, and each bug requires different skills. Sometimes a bug even requires multiple skills to fix. Thus, assigning developers to bugs is the primary task in bug triage. In this paper, we aim to deal with developer assignments.\n\nAssigning developers is usually done manually in the industry, and manual assignment is labor-intensive and time-consuming. Nowadays, it is easy to find a project with over 1,000 developers, and new bugs are reported every single day in the project. When a bug report is registered, a human expert first identifies the characteristic of the bug and assigns the developer with the most suitable skills from thousands of developers. If this process is not done quickly, other upcoming bugs cannot be handled. Manual bug triage often significantly delays the development and maintenance of largescale systems.\n\nTo overcome the delay of manual bug triage, automatic bug triage techniques have been studied and these aim to capture useful information from bug reports. Early studies [3,6,22] tried to extract traditional textual features such as TF-IDF from a bug report. Word2vec [13] is adopted to get text representations [11], and ELMo [15] is applied to the bug triage task [25].\n\nRecently, Transformer-based large Pre-trained Language Models (PLMs), e.g., BERT [4] and its variants, have achieved state-of-theart performance on Natural Language Understanding (NLU) tasks. However, applying large PLMs to NLP applications including bug triage is challenging: First, large PLMs require high-end computing resources to be deployed in practice, as their size is typically larger than the size of memory in commodity hardware. Second, during fine-tuning of a PLM on bug triage data, the PLM forgets general language knowledge learned in pre-training. This phenomenon is called catastrophic forgetting [12], i.e., a phenomenon that a neural network forgets previously learned information when it learns new information. Finally, when the representation of an input at the network's earlier layer is sufficient to make a correct prediction, progressing into deeper layers could \"overthink\" the input and deteriorate the prediction performance, which is called the overthinking problem [7]. Likewise, humans, thinking too complex about a simple problem, can also lead to incorrect conclusions. In this paper, we aim to solve these challenges. This paper proposes a novel framework for bug triage named LBT-P, standing for Light Bug Triage framework with a Pre-trained language model. LBT-P handles the aforementioned problems and captures semantic information from a bug report. First, it compresses a large PLM into small and fast models using knowledge distillation techniques [5]. To handle catastrophic forgetting of PLM, we develop a fine-tuning method preserving language knowledge. To handle the overthinking problem, we propose new loss functions exploiting representations of earlier layers as well as deeper layers. We evaluate our methods on a real-world private dataset and three public datasets: Google Chromium, Mozilla Core, and Mozilla Firefox. The public datasets were used in the previous study [11]. These datasets have 118,607, 128,104, and 24,158 bug reports respectively. Experimental results show that our proposing methods significantly outperform competitors including [11].\n\nThe main contributions of our work are summarized as follows:\n\n\u2022 We deal with challenges for applying PLM. First, we compress the large PLM into a smaller and faster model using knowledge distillation. Also, we prevent catastrophic forgetting using knowledge preservation fine-tuning. \u2022 We address the overthinking problem by exploiting representations of earlier layers. We propose a new loss function to use earlier representations effectively. \u2022 The empirical result shows the superiority of the proposed framework on real-world datasets.\n\n\nPRELIMINARY 2.1 Bug Triage Method\n\nThere have been various attempts to automate bug triage tasks. Since these studies use text in a bug report as input, bug triage research has been greatly influenced by the development of the NLP technique. Early studies exploited traditional text features, which are about statistics in a document, from the bug report. For example, TF-IDF, the most representative traditional text feature, is multiply of term frequency and inverse document frequency. Xuan et al. [22] extracted traditional text features, reduced train data size by a combination of instance selection with feature selection, and applied Na\u00efve Bayes classifier to assign developer. Ded\u00edk et al. [3] extracted TF-IDF from the bug reports and adopted SVM as a classifier. Jonsson et al. [6] used TF-IDF and introduced an ensemble learner called Stacked Generalization to boost performance. Since knowing the context within a sentence is a great help in understanding the sentence, various techniques to capture context have been introduced in NLP. Recent bug triage studies have utilized these NLP techniques. Mani et al. [11] adopted Word2vec [13], which is a technique for vectorizing words, to get text representations and used bidirectional RNN with attention as a classifier. Although Word2vec learns word representations so that words in a sentence can predict nearby words well, it cannot capture the context change according to sentence, because Word2vec does not generate different word vectors from sentence to sentence. Zaidi et al. [25] applied ELMo [15], an early pre-trained language model, to the bug triage task. In contrast to Word2vec, ELMo takes the entire sentence as input, and reflects the context of the given sentence in the word representation. Thus, it can capture context change according to sentence, and also it is possible to distinguish between homonyms.\n\n\nLarge Pre-trained Language Model\n\nIn recent years, studies of large Pre-trained Language Models (PLMs) have achieved great progress in the NLP domain. These studies adopted the Transformer-based [19] model as a language model. The self-attention used in the Transformer can capture semantic information, which is information about the meaning of a sentence, at a higher level than previous NLP methods. Also, the studies of large PLMs applied self-supervised learning to train the model. In self-supervised learning used in these studies, pseudo-labels are generated from unlabeled data, and the model is trained in a supervised manner using pseudo-labels. For example, BERT [4], the most representative PLM, uses Masked Language Model (MLM) and Next Sentence Prediction (NSP) to train the language model. In the case of MLM, random words (or tokens) in the input sentence are masked and the original words are used as labels. During the training, the language model predicts what the masked words are. Using self-supervised learning, PLM can learn general language knowledge from a huge unlabeled corpus.\n\nAfter BERT is introduced, various BERT-based models are proposed to improve BERT. RoBERTa [10] boosts the performance of BERT by applying dynamic masking to MLM, removing NSP, and increasing the size of training data. ALBERT [9] reduces the size of BERT by factorization of the embedding parameters and sharing all parameters across layers. ELECTRA [2] improves the performance of BERT by introducing a new pre-training task called Replaced Token Detection (RTD).\n\nBefore the advent of large PLM, to deal with NLP applications, researchers had to collect large text corpus and train a language model from the scratch with their limited corpus. This process takes a large time and a huge computational cost. Since the large PLM can can capture high-quality semantic information from text using pre-trained general language knowledge, researchers can skip the complex process to get a language model. Thus, a large PLM allows researchers to focus on their target task.\n\n\nTHE PROPOSED FRAMEWORK\n\nIn this section, we propose a novel Light Bug Triage framework with a large Pre-trained language model called LBT-P. Figure 1 shows the basic architecture of the proposed bug triage framework. The overall model is mainly composed of two parts, which are a text embedding module and a classifier. The text embedding module processes raw text. After the process, the classifier generates a score for each developer using the representations processed by the text embedding module.\n\nIn the sections that follow, we present the details of our proposed framework. We define our task in Section 3.1. In Section 3.2, we present detail of the text embedding module. In Section 3.3, we present detail of the classifier. Then, we present challenges and their solutions for our text embedding module in Section 3.4 and Section 3.5. In Section 3.6, we present a solution for dealing with the  Figure 1: The basic architecture of the proposed bug triage framework. The framework consists of a text embedding module and classifier. L is the max length of the bug report and N is the total number of developers. Four kernels with k \u2208 {3, 4, 5, 6} are used for the convolution layers.\n\noverthinking problem, a problem separate from the text embedding module.\n\n\nProblem Formulation\n\nIn this work, we focus on the bug triage task, which assigns an appropriate developer to the given bug. Each bug in the dataset D consists of a pair of the bug report and the developer who fixed the bug. The max length of the bug report is L and the total number of developers is N . This bug triage task can be considered a classification problem. In this case, the bug report can be regarded as an input and the developer can be regarded as a label. Given a bug report, the bug triage model computes the score for each developer.\n\nIn practice, not only one person but also several developers can fix a bug. However, only one developer who actually fixed the bug is recorded in the dataset. If performance is evaluated by accuracy, the case where another developer who is likely to fix the bug gets the highest score is ignored. For this reason, it is inappropriate to use accuracy as an evaluation metric in this task, so we evaluate performance by top-k accuracy (acc@k). Acc@k is formulated as follows:\nacc@k = (x,y)\u2208D 1 r ec x @k (y) |D| ,(1)\nwhere x is a bug report, y is label of the bug report, rec x @k is the k recommended developers for the given bug report x, and 1 A (b) is an indicator function that returns 1 if b belongs to A and 0 otherwise. Acc@k counts the number of times a developer who fixed the bug belongs to the k developers with higher scores.\n\n\nText Embedding Module\n\nThe text embedding module converts bug reports into text embedding vectors. Bug reports registered in the bug tracking system usually contain title and description, and the title and description contain rich semantic information. The objective of the text embedding module can be regarded as capturing semantic information from the title and description. Intuitively, the titles and descriptions are closely related, and this relationship should be considered in order to properly extract semantic information. When the title and description are embedded separately, an additional process is required to capture the interaction between them. Thus, we simply concatenate the title and the description into one text and used it as an input for the text embedding module. Then, the text embedding module treats the concatenated title and description as consecutive sentences, so it can consider the correlation between the title and description without extra process. Before putting the concatenated text as input, we remove special characters, extra spaces, line breaks, and URLs from the text. To extract semantic information from a bug report, we adopt a large PLM as a text embedding module. To find the best performing PLM on our task, we compared the performance of BERT, RoBERTa, ALBERT, and ELECTRA. Detailed results of this comparison are reported in Section 4.4. RoBERTa outperformed other competitors in this comparison, so we adopt RoBERTa-large, increased size of RoBERTa, as the text embedding module. We obtained a pre-trained RoBERTa-large model from the HuggingFace Transformers library [21]. RoBERTa-large, with the same architecture as BERT-large, has 24 layers, 1024-hidden size, 16 heads, and 355M parameters.\n\nRoBERTa adopts byte-level BPE, first used in GPT-2 [16], as a tokenizer. Since byte-level BPE is a universal encoding scheme, it is free from the out-of-vocabulary problem. In particular, bug reports in some domains contain technical terms that are not commonly used and these terms may have important semantic information. Thus the out-of-vocabulary problem may cause performance degradation, so it is better to avoid it for practical use. From this point of view, choosing RoBERTa also has the advantage to avoid the out-of-vocabulary problem.\n\nThere are many advantages to using large PLM, but applying large PLM has several challenges. Because of these challenges, applying PLM in a naive way has some limits. We present these challenges and their solutions in Section 3.4 and Section 3.5.\n\n\nClassifier\n\nIn general, a RNN-based model is used to process sequential data, but RNN has a disadvantage in that the longer the sequence, the longer the operation time. And also, recently, it is not difficult to find the case of using a CNN-based model to process sequential data. For example, WaveNet [14] handles audio data using CNN. Since we also observed that the CNN-based model performed better than the RNN-based model in our task, in this work, the CNN-based model is used as a classifier.\n\nWe extract features from the text embeddings using four parallel convolution layers of kernel size k \u2208 {3, 4, 5, 6}. Each convolution layer generates H c feature maps, and max-pooling layer subsamples the feature map into a H c dimension feature vector. In this work, H c is set to 256. In the BERT-based model, the representation of [CLS], which is a special token added in front of every input text, is considered to contain the embedding for the entire text. For residual connection, we concatenate the representation of [CLS] in the text embeddings and the feature vectors. The concatenated vector is used as an input to fully connected layers, and the fully connected layers output a score for each developer as a result.\n\n\nCompressing Text Embedding Module\n\nThe large PLM requires considerable computational cost because of its huge size of the PLM. Since this large PLM is very heavy and slow, practical applications of the large PLM are difficult. To overcome this problem, the size of the PLM should be reduced.\n\nUsing small PLM can be one solution to this problem. As mentioned before, ALBERT is a reduced size model of BERT, so applying ALBERT reduces the computational cost and consequently reduces training and inference time. However, the comparison in Section 4.4 shows that ALBERT performs significantly worse than RoBERTa in our task. Thus, ALBERT, despite its lightness, is not suitable for our task.\n\n3.4.1 Knowledge Distillation. Addressing this problem requires not only reducing the size of the PLM, but also maintaining the performance of the PLM. One solution to this problem can be found in knowledge distillation (KD), a concept first proposed by Hinton et al. [5]. KD is a sort of network compression technique that transfers knowledge from a large model into a small model. In KD, the trained large model is called the teacher and the small model to which knowledge is transmitted is called the student. KD trains the student model to mimic the teacher model through distillation loss. In our task, for a given input, the student model is trained to generate a representation similar to that generated by the teacher model. Thus, the distillation loss L K D in our task is formulated as follows:\nL K D = x \u2208D K D MSE F t (x), F s (x) ,(2)\nwhere MSE is mean squared error loss, F t is the teacher model, F s is the student model, and D K D is the dataset for training the student. Note that our distillation loss L K D does not use a label. Bug reports in the bug tracking system are divided into open bugs and closed bugs. Open bugs are bugs that have not yet been assigned to a developer, and closed bugs are bugs that have already been fixed by an assigned developer. In other words, open bugs are unlabeled bugs and closed bugs are labeled bugs. While open bugs are not used in many bug triage studies, DBRNN-A [11] exploits open bugs for learning Word2vec. Inspired by DBRNN-A, we use open bugs as datasets for training the student model. This allows the student model to learn from richer data.\n\nWe use the RoBERTa architecture with reduced parameters as the student model. Specifically, we set the number of layers of the student model to 3, and all other parameters are set the same as Roberta-large. In this case, since Roberta-large has 24 layers, the size of the student model is 8 times smaller than that of Roberta-large.\n\n\nPatient Knowledge Distillation.\n\nIn general, the student model performs worse than the teacher model. This is because some information is lost when transferring the knowledge of the teacher to the student. Therefore, it is important for the student to reduce this information loss in order to preserve the performance of the teacher. Patient knowledge distillation (PKD) [18] offers a solution to this problem. In PKD, the student model learns not only to imitate the final outputs of the teacher but also to follow the thinking process of the teacher. To achieve this, a new loss has been introduced. Let l t be the number of layers in the teacher model and l s be the number of layers in the student model, where l t > l s . Then the new loss L P K D [18] is formulated as follows:\nL P K D = x \u2208D K D l s \u22121 i=0 F t l t \u2212s \u00b7i (x) \u2225F t l t \u2212s \u00b7i (x)\u2225 2 \u2212 F s l s \u2212i (x) \u2225F s l s \u2212i (x)\u2225 2 2 2 ,(3)\nwhere s = \u230al t /l s \u230b and F i (x) is the output of the i-th layer of the model F . The above loss is specifically a loss according to the PKD-Skip strategy. When the student model is trained by L P K D , the intermediate layers of the student model are trained to imitate the representation of the corresponding intermediate layer of the teacher model, respectively. This makes the embedding process of the student model more similar to that of the teacher model. The experimental results detailed in Section 4.6.1 show that training the student model using L P K D has a lower performance penalty than using L K D . Therefore, we adopt L P K D to train the student model.\n\n\nPreventing Catastrophic Forgetting\n\nApplying the PLM implies exploiting transfer learning. Catastrophic forgetting is one of the main challenges of transfer learning. Catastrophic forgetting, which was first introduced by McCloskey and Cohen in 1989 [12], is a phenomenon in which when a neural network learns new information, it forgets previously learned information. During fine-tuning for bug triage tasks, the PLM is trained to generate representations from which the train data can be classified well. This implies that the PLM implicitly learns task-specific knowledge contained in the train data during fine-tuning. However, as the fine-tuning progresses, the PLM forgets how to generate representations for general vocabulary which is not included in the train data. In other words, catastrophic forgetting causes the PLM to lose previously learned general language knowledge after fine-tuning.\n\n\nNaive Solutions.\n\nCatastrophic forgetting leads the model to overfit. Consequently, the model cannot achieve its maximum potential performance without resolving catastrophic forgetting. One naive solution to avoid catastrophic forgetting is early stopping. If training is stopped before the model's performance deteriorates due to overfitting, the model can achieve somewhat high performance. However, it is hard to decide when to stop the training. Any performance degradation in the previous several epochs during training does not mean that the maximum performance has been reached or that overfitting has occurred. Because of this, for example, early stopping when performance degrades does not guarantee maximum performance. Thus, using this early stopping scheme to avoid catastrophic forgetting relies heavily on heuristics. Also, overfitting can occur before the model's potential maximum performance is reached. Thus, while early stopping mitigates the effect of catastrophic forgetting, it does not completely solve the problem. Freezing the PLM is another naive solution. In this solution, only the classifier is trained during the fine-tuning. Also, because the PLM is not updated, it does not suffer from catastrophic forgetting and preserves its general language knowledge. Even though the PLM is not fine-tuned, it can generate generally good representations for text. Hence, a model trained in this way has proper performance without overfitting during training. However, despite avoiding catastrophic forgetting by freezing the PLM, the PLM cannot learn task knowledge from train data. Thus, the frozen PLM cannot generate task-specific representations, and consequently, underfitting occurs. Therefore, freezing the PLM guarantees the model's general language knowledge, but the maximum potential performance of the model cannot be achieved.\n\n\nKnowledge Preservation Fine-tuning.\n\nWith naive solutions, the model suffers from overfitting or underfitting. When the PLM loses its general language knowledge, the bug triage model overfits. In contrast, when the PLM does not learn task knowledge, the model underfits. Thus, the main goal of fine-tuning to address this problem is to have the PLM learn task knowledge while preserving general language knowledge. General language knowledge and task knowledge seem like a trade-off relationship, but actually, they are not. Intuitively, earlier layers of the PLM are strongly associated with the input text, and later layers of the PLM are strongly associated with the output representation. Thus, we can consider the earlier layers are more important to understand the text and the later layers are more important to generate the representation. In other words, general language knowledge is mainly stored in the earlier layers and task knowledge is mainly stored in the later layers. So, we use this intuition to achieve our goal.  To overcome catastrophic forgetting, we propose knowledge preservation fine-tuning. It is a fine-tuning that freezes the earlier layers of the PLM and only updates the later layers. The earlier layers are frozen to preserve general language knowledge and later layers are updated to learn task knowledge. So, the PLM trained with knowledge preservation fine-tuning has both general language knowledge and task knowledge. We show the effectiveness of knowledge preservation fine-tuning in Section 4.6.2 with experimental results.\n\n\nHandling Overthinking\n\nOverthinking of networks is another challenge, which is not related to applying the PLM. When representation of an input at the network's earlier layer is sufficient to make a correct classification, the network \"overthinks\" on the input. [7] Intuitively, when a person thinks too complexly, it can lead to an incorrect conclusion for a simple problem. Similarly, in the case of the neural network, when the earlier layer generates sufficiently informative representations, some noise can be added to the representations through the later layers. Simply reducing the number of layers of the model cannot be a solution for overthinking because hard problems still require complex thinking. Thus, although the model shows good performance for complex problems, overthinking may lead to performing badly on simple problems.\n\n3.6.1 Combined Thinking. Since hard problems require complex thinking and uncomplicated thinking helps to solve simple problems, one of the intuitive solutions is to combine deep and shallow thinking. So, we exploit the output of the early layers to overcome overthinking. To do this, we select later k layers of the text embedding module including the last layer, and attached the classifier to each selected layer. Then the final output of the model is the weighted sum of each classifier's output. The modified model F is formulated as follows:\nF (x) = k\u22121 i=0 |w i | \u00b7 C i (T l \u2212i (x)) ,(4)\nwhere x is input text, T i (x) is the output of the i-th layer of the text embedding module, C i is the classifier attached to the i-th layer, and w is weight. Figure 2 shows our combined thinking architecture.\n\nIn this work, we set k to 3. That is all layers of the text embedding module are used. To train the model, first, we used the cross-entropy loss as a loss function without any special changes. The loss function L C E is formulated as follows:\nL C E = (x,y)\u2208D CE (F (x), y) ,(5)\nwhere CE is cross-entropy loss. We observed that when using this loss function, the model is trained to depend too much on the output of a particular layer. Because of this tendency, the classifiers of the remaining layers are not properly trained. So each result of the remaining layers' classifier tends to be far from recommending an appropriate developer.\n\nTo combine deep and shallow thinking, each classifier needs to be properly trained. We introduce combined thinking loss to tackle this problem. Combined thinking loss L CT is formulated as follows:\nL CT = L C E + (x,y)\u2208D k \u22121 i=0 CE (F i (x), y) ,(6)\nwhere\nF i (x) = C i (T l \u2212i (x)\n) is the output of the i-th classifier. In combined thinking loss L CT , a new term is added, and it forces each classifier to get the classification ability properly. So, using combined thinking loss, the model does not suffer from training failure problems for each classifier, and deep and shallow thinking are properly combined. We demonstrate the effectiveness of combined thinking loss in Section 4.6.3.\n\n\nEXPERIMENTS 4.1 Research Questions\n\nIn this paper, we aim to answer the following research questions:\n\n\u2022 RQ1: What is the most effective PLM for the bug triage task?\n\n\u2022 RQ2: How effective is LBT-P for the bug triage task? LBT-P aims for performance and lightness, so this RQ can be divided into two RQs as follows:\n\n-RQ2.1: How accurate is LBT-P at assigning developers to the given bug? -RQ2.2: How faster and lighter is LBT-P than a model which naively applies the PLM? \u2022 RQ3: How effective are the components of LBT-P in addressing the challenges? To answer this RQ, we provide an ablation study that presents an experimental basis for our proposed methods. This RQ is specified as follows: -RQ3.1: How effective is patient knowledge distillation at reducing information loss? -RQ3.2: How effective is knowledge preservation finetuning at preventing the catastrophic forgetting? -RQ3.3: How effective are combined thinking architecture and loss at handling the overthinking problem?\n\n\nDataset\n\nFor a fair comparison, we use 3 public real-world datasets used in the baseline method [11]: Google Chromium, Mozilla Core, and Mozilla Firefox. In preprocessing, we filtered out bug reports of 15 words or less whose text was too short to contain contextual information. Furthermore, we also validate our framework with real industry private data. The private dataset has 84,267 closed bugs. The number of developers in the dataset is 2,910. In the case of the private data, there are no open bugs so the student model is pre-trained by train data of closed bugs. Table 1 shows a summary of data statistics.\n\n\nExperimental Setup\n\nTo evaluate our proposed framework, LBT-P, we use top-k accuracy (acc@k). Specifically, we report results where k \u2208 {1, 2, 3, 4, 5, 10, 20}. The Adam optimizer [8] is used to train LBT-P, and the learning rate is set to 0.00001. We train LBT-P for 100 epochs. In the case of the student model used as a text embedding module, as mentioned in Section 3.4, the number of layers of the student model is set to 3, and all other parameters are set the same as Roberta-large. The student model is trained for 2 epochs before fine-tuning.\n\nIn the experiment, to present the validity of proposed techniques and the superiority of LBT-P, we compare the performance of the following methods:\n\n(1) Baseline methods\n\n\u2022 TF-IDF: It is the baseline for traditional text feature. Given bug report, it computes the similarity to the bugs in the train dataset based on the TF-IDF, selects the reports with the top k similarity, and finally recommends the developers of the selected reports.    \u2022 DBRNN-A [11]: It embeds text in a bug report using Word2vec, and assigns the bug report to developers using bidirectional RNN with attention mechanism. \u2022 RoBERTa-large [10] + CNN : RoBERTa-large is naively applied to it as a text embedding module, and it adopts CNN-based model as a classifier. Because BERT outperforms ELMo [15], we skip to report the results for ELMo-CNN [25].\n\n(2) Basic PLM methods to which a PLM is naively applied \u2022 BERT [4] + CNN: It adopts BERT as a text embedding module. BERT is most representative Transformer-based PLM. \u2022 RoBERTa [10] + CNN: It adopts RoBERTa as a text embedding module. RoBERTa improves BERT by changing the pre-training approach and increasing training data.  \n\n\nComparison Between Basic PLM Methods (RQ1)\n\nTo select the PLM to use for the bug triage task, we compared the performance of basic PLM methods. In the comparison, BERT, RoBERTa, ALBERT, and ELECTRA are used. These PLMs were applied naively for the bug triage task. Since updating all layers in the PLM during fine-tuning suffered out of GPU memory or not a good performance, the result of freezing all layers in the PLM is reported in Table 2. Basically, BERT has higher performance than the baseline methods TF-IDF and DBRNN-A. ALBERT and ELECTRA perform worse than BERT, while RoBERTa outperforms BERT. Based on these results, we set RoBERTa-large as the base model for the text embedding module.\n\n\nComparison With The Baseline Methods (RQ2)\n\n4.5.1 Performance (RQ2.1). In this section, we compare baseline methods and LBT-P. Table 2 shows the performance of each method for Google Chrome dataset. Table 3, 4, and 5 show the performance for Mozilla Core dataset, Mozilla Firefox dataset, and the private dataset respectively.\n\nIn terms of performance, TF-IDF, the traditional method, seems to be superior to DBRNN-A, the existing deep learning method, but as the train dataset increases, the inference time of TF-IDF increases. If the training data is small, this property of TF-IDF is not a big problem. However, in the real world, the number of bug reports that can be used as training data increases over time. Even if a representative bug report is selected to limit the size of the training data for TF-IDF, additional instance selection is required for this. Thus, in practice, TF-IDF is not suitable for real-time service, which requires fast inference time. Furthermore, the performance of TF-IDF is inferior to RoBERTa-large + CNN to which the PLM is naively applied. This shows that the deep learning method applying the latest NLP technique is more effective for the bug triage task than the traditional text feature method.\n\nFor all datasets, LBT-P outperforms other methods. One remarkable observation is that LBT-P*, which only solves problems related to PLM, also outperforms RoBERTa-large + CNN, which outperforms other baseline methods. This shows that the maximum potential performance of the bug triage model cannot be derived by just applying the PLM naively.  Table 6 shows the number of parameters and training time of RoBERTa-large + CNN and LBT-P. Since LBT-P uses the small student model which has fewer layers than RoBERTa-large, the number of parameters in LBT-P is reduced to 33% of the number of parameters in RoBERTa-large + CNN. Also, in the case of training time, LBT-P is about 2.73 times faster than RoBERTa-large, despite the additional training time of the student model required. Therefore, the proposed model not only outperforms RoBERTa-large, but is also faster and smaller.\n\n\nAblation Study (RQ3)\n\n\nThe Effectiveness of Patient Knowledge Distillation (RQ3.1).\n\nIn this section, we present the effectiveness of PKD. To do this, we compare the student model trained by naive KD loss L K D and the student model trained by PKD loss L P K D . As aforementioned, we use the open bugs to pre-train the student model. Thus, train size (KD) in Table 1 Table 2. Since, information loss occurs when a teacher's knowledge is transferred to a student, both students have lower performance than the teacher, RoBERTa-large. The performance degradation of the student trained by L P K D is lower than that of the student trained by L K D , and this implies that using PKD, which mimics the thinking process of the teacher, reduces information loss.  In this section, we present the effectiveness of knowledge preservation fine-tuning. We compared the test rank 1 accuracy per epoch by varying the number of frozen layers in the text embedding module. Figure 3 shows the result of this comparison. In this experiment, we use Student(PKD) + CNN as the base model. In the figure, for example, stu21cnn indicates that the first 2 layers of the text embedding module are frozen and the remaining 1 layer is updated during fine-tuning. The full layer freezing (stu30cnn) is exactly the same as Student(PKD) + CNN. When all layers are updated without any freezing, performance degradation due to overfitting is observed at the fine-tuning midpoint. When all layers are frozen, overfitting is not observed but the model does not achieve the best performance. Note that the highest accuracy of the no layer freezing (stu03cnn) is lower than the highest accuracy of the full layer freezing (stu30cnn). This shows that overfitting occurs before reaching the maximum potential performance of the model in full layer update. So, this implies that early stopping cannot completely solve catastrophic forgetting.\n\nIn the case of stu12cnn, it updates the earlier layer, so, similar to full layer update, overfitting occurs. Since stu12cnn does not modify the first layer which contains the most essential knowledge about language, it outperforms stu30cnn. The highest performance is achieved when only the last layer is updated and the remaining layers in the text embedding module are frozen (stu21cnn). Also, in this case, the model did not suffer performance degradation during fine-tuning.\n\n\nThe Effectiveness of Combined Thinking Loss (RQ3.3).\n\nTo handle the overthinking problem, we modified our framework to do combined thinking. We also introduce combined thinking loss to fine-tune the modified framework. In this section, we present the effectiveness of the combined thinking loss. Figure 4 shows the accuracy per epoch according to the loss. In the case of naive crossentropy loss L C E , only the final layer is trained properly, while the rest of the layers are not properly trained. That is the model relies heavily on layer 3. On the other hand, in the case of combined thinking loss L CT , all layers are properly trained. Moreover, all layers with L CT achieve higher performance than the last layer with L C E . Figure 5 shows performance comparison between L C E and L CT . No CT in the figure stands for the model which is not modified for combined thinking and this is identical to LBT-P*. In this comparison, the performance of the models with L C E is not significantly different from that of LBT-P*. This is because, as shown in Figure 4, the models with L C E heavily depend on the last layer, so there is no big difference from LBT-P*, which uses only the last layer. The model with L CT , in which all layers are properly trained, shows higher performance than LBT-P*. This shows that both shallow and deep thinking can be used with the proposed loss L CT .\n\n\nRELATED WORK\n\nRich semantic information in the bug report can be used in various ways. Therefore, bug reports are used not only in the developer assignment task but also in other various tasks. In this section, we introduce several studies that exploit the bug report. Chen et al. [1] study generating the title from the description of the bug report. They regard title generation as a one-sentence summarization task. To do this, they build a Seq2Seq model to summarize the description. Yanqi el al. [23] aim to deal with bug component assignment task. In particular, they focus on solving the bug tossing phenomenon. To solve the problem, they construct a bug tossing knowledge graph and exploit derived features from the graph. They adopt a learning-to-rank model to assign the bug component. This bug component assignment task is similar to our task. The main difference between these tasks is the number of classes. Their study covers 186 components, but in the case of the developer assignment task, the number of developers exceeds a thousand. Therefore, although their study is effective in a small class size task, it is hard   to extend their study to our task because the size of the knowledge graph increases exponentially. Yang et al. [17] introduce a tool that automatically analyzes bug reports. The tool uses the sentence in the bug report as input, and it identifies whether the sentence corresponds to three types: the observed behavior, the expected behavior, and the steps to reproduce the bug. To do this, they extract n-grams and part-of-speech tags from the sentence and use SVM as a classifier. Ye et al. [24] and Wang et al. [20] studied information retrieval-based bug localization technique using bug reports.\n\n\nCONCLUSION & DISCUSSION\n\nAssigning an appropriate developer to the bug, which is called bug triage, is important to maintaining a large project. In this work, we propose a novel light bug triage framework with a pre-trained language model called LBT-P that solves the challenges of applying large PLM and the overthinking problem. Applying large PLM to bug triage tasks has problems with the size of the large PLM and problem of catastrophic forgetting. We compress the size of the PLM using patient knowledge distillation [18] and prevent catastrophic forgetting by introducing knowledge preservation finetuning. Furthermore, we propose combined thinking architecture and combined thinking loss to handle the overthinking problem. The experimental result on three real-world datasets [11] and the private industry dataset shows the effectiveness of our proposed framework.\n\nBecause the framework we propose aims to solve the task of classification, it can be extended easily to other tasks on bugs in addition to developer assignment task such as department assignment and bug component assignment. In addition, as the PLM can process various text data, proposed knowledge preservation finetuning can also be applied with the PLM to various tasks that need text embeddings.\n\nFigure 2 :\n2The architecture of modified bug triage framework for combined thinking.\n\n( 3 )\n3Methods applying KD \u2022 Student(KD) + CNN: It adopts a student model of RoBERTalarge as a text embedding module. The student model is trained by naive KD loss L K D . \u2022 Student(PKD) + CNN : It adopts a student model, which is trained by patient KD [18] loss L P K D , as a text embedding module. (4) Proposed methods\u2022 LBT-P : It is our proposed method. It adopts student(PKD) as a text embedding module and CNN-based model as a classifier. During training, it applies knowledge preservation fine-tuning to prevent catastrophic forgetting problem. It avoids the overthinking problem by using combined thinking. \u2022 LBT-P* : It is LBT-P without combined thinking. It only aims to solve challenges related to applying PLM.\n\n\nsize and training time (RQ2.2).\n\nFigure 3 :\n3Test accuracy per epoch according to the number of frozen layers in the text embedding module. The results are obtained on Google Chrome dataset. 4.6.2 The Effectiveness of Knowledge Preservation Fine-tuning (RQ3.2).\n\n\n-entropy loss L C E . (b) Combined thinking loss L CT .\n\nFigure 4 :\n4Test accuracy per epoch of each layer of combined thinking architecture when trained by cross-entropy loss L C E and combined thinking loss L CT . The results are obtained on Google Chrome dataset.\n\nFigure 5 :\n5Comparison of combined thinking model performance according to loss. The results are obtained on Google Chrome dataset.\n\n\nMozilla Core and Mozilla Firefox datasets have 186,173 and 138,093 open bugs, respectively, and they have 128,104 and 24,158 closed bugs, respectively. The number of developers in Mozilla Core and Mozilla Firefox datasets are 2,548 and 1,314, respectively.As mentioned earlier, the open bugs, which have no label, are used to pre-train the student model used as a text embedding module. Closed bugs are used to fine-tune the bug triage model. 90% of closed bugs are used for training and 10% of closed bugs are used for the test.Google Chromium dataset has 163,695 open bugs and 118,607 closed \nbugs. The number of developers in Google Chromium dataset is \n2,529. \n\nTable 1 :\n1Data statistics (after preprocessing).Dataset \nTrain size (KD) Train size Test size Total developers \n\nGoogle Chromium 163,695 \n106,778 \n11,829 \n2,529 \nMozilla Core \n186,173 \n115,393 \n12,711 \n2,548 \nMozilla Firefox \n138,093 \n21,792 \n2,366 \n1,314 \n\nPrivate \n75,883 \n75,883 \n8,384 \n2,910 \n\n\n\nTable 2 :\n2Top-k accuracy obtained on Google Chrome dataset.Model \nAcc@1 Acc@2 Acc@3 Acc@4 Acc@5 Acc@10 Acc@20 \n\nBasic PLM \n\nBERT + CNN \n27.179 37.247 43.495 47.663 51.162 61.087 \n70.251 \nRoBERTa + CNN \n27.754 37.763 43.799 48.229 51.644 62.093 \n71.173 \nALBERT + CNN \n4.6242 6.9828 8.5045 9.7810 10.787 14.870 \n20.678 \nELECTRA + CNN \n16.392 22.901 26.993 30.239 33.097 42.083 \n51.644 \n\nBaseline \n\nTF-IDF \n20.044 27.796 32.750 36.402 39.150 47.975 \n56.176 \nDBRNN-A \n11.269 16.526 20.091 22.947 25.412 33.652 \n42.519 \nRoBERTa-large + CNN 28.887 39.445 45.752 50.402 53.994 63.716 \n72.652 \n\nKD \nStudent(KD) + CNN \n24.998 34.128 40.587 44.797 48.001 57.909 \n67.605 \nStudent(PKD) + CNN \n27.779 37.788 43.901 48.449 51.923 61.907 \n70.843 \n\nProposed \nLBT-P* \n30.070 40.350 46.741 51.315 54.417 64.350 \n72.686 \nLBT-P \n32.395 43.376 49.894 54.400 57.773 67.132 \n75.230 \n\n\n\nTable 3 :\n3Top-k accuracy obtained on Mozilla Core dataset.Model \nAcc@1 Acc@2 Acc@3 Acc@4 Acc@5 Acc@10 Acc@20 \n\nBaseline \n\nTF-IDF \n23.452 32.114 37.259 41.161 44.135 54.394 \n64.354 \nDBRNN-A \n14.401 21.318 26.334 29.965 33.313 42.884 \n53.520 \nRoBERTa-large + CNN 35.804 46.700 53.363 57.525 61.018 70.931 \n79.608 \n\nProposed \nLBT-P* \n37.235 48.556 54.795 59.091 62.340 71.780 \n79.931 \nLBT-P \n40.492 52.010 58.571 62.930 66.368 75.439 \n82.669 \n\n\n\nTable 4 :\n4Top-k accuracy obtained on Mozilla Firefox dataset.Model \nAcc@1 Acc@2 Acc@3 Acc@4 Acc@5 Acc@10 Acc@20 \n\nBaseline \n\nTF-IDF \n22.739 33.178 38.673 43.576 47.506 58.369 \n67.878 \nDBRNN-A \n8.4038 14.178 18.357 22.535 25.728 36.432 \n49.765 \nRoBERTa-large + CNN 29.670 40.828 48.267 53.043 56.932 66.822 \n76.078 \n\nProposed \nLBT-P* \n33.981 45.097 50.676 55.156 58.326 68.047 \n76.458 \nLBT-P \n34.108 46.830 53.762 58.538 61.581 70.287 \n78.191 \n\n\n\nTable 5 :\n5Top-k accuracy obtained on the private dataset.Model \nAcc@1 Acc@2 Acc@3 Acc@4 Acc@5 Acc@10 Acc@20 \n\nBaseline \n\nTF-IDF \n25.584 33.922 38.943 42.510 45.277 54.413 \n62.870 \nDBRNN-A \n9.5815 15.493 19.715 22.916 26.070 35.544 \n46.301 \nRoBERTa-large + CNN 32.896 43.321 49.952 54.342 57.574 67.402 \n76.610 \n\nProposed \nLBT-P* \n37.989 48.712 55.069 58.862 61.892 70.873 \n78.817 \nLBT-P \n38.645 49.332 55.582 59.673 62.739 71.386 \n79.330 \n\n\u2022 ALBERT [9] + CNN : It adopts ALBERT as a text em-\nbedding module. ALBERT reduces the size of BERT by \nfactorization of the embedding parameters and parameter \nsharing. \n\u2022 ELECTRA [2] + CNN : It adopts ELECTRA as a text em-\nbedding module. ELECTRA improves BERT by introduc-\ning RTD. \n\n\n\ncorresponds to the size of the open bugs. The table shows that using the open bugs can train students with richer data. The comparison result is reported in\n\nTable 6 :\n6The number of parameters and training time of RoBERTa-large-based methods on Google Chrome dataset.Model \nTotal parameters \nTraining time \nSpeedup \nStudent \nFine-tuning \nTotal \n\nRoBERTa-large + CNN \n364,769,761 \n-\n277,273.7 s 277,273.7 s \n1.00 \nLBT-P* \n100,249,057 \n11,420.69 s 70,144.60 s 81,565.29 s \n3.40\u00d7 \nLBT-P \n119,069,094 \n11,393.61 s 90,313.64 s 101,707.4 s \n2.73\u00d7 \n\n\nACKNOWLEDGMENTS\nStay professional and efficient: automatically generate titles for your bug reports. Songqiang Chen, Xiaoyuan Xie, Bangguo Yin, Yuanxiang Ji, Lin Chen, Baowen Xu, 2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEESongqiang Chen, Xiaoyuan Xie, Bangguo Yin, Yuanxiang Ji, Lin Chen, and Baowen Xu. 2020. Stay professional and efficient: automatically generate ti- tles for your bug reports. In 2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 385-397.\n\nKevin Clark, Minh-Thang Luong, V Quoc, Christopher D Le, Manning, arXiv:2003.10555Electra: Pre-training text encoders as discriminators rather than generators. arXiv preprintKevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. 2020. Electra: Pre-training text encoders as discriminators rather than generators. arXiv preprint arXiv:2003.10555 (2020).\n\nAutomated Bug Triaging in an Industrial Context. V\u00e1clav Ded\u00edk, Bruno Rossi, 10.1109/SEAA.2016.2042th Euromicro Conference on Software Engineering and Advanced Applications (SEAA). V\u00e1clav Ded\u00edk and Bruno Rossi. 2016. Automated Bug Triaging in an Industrial Context. In 2016 42th Euromicro Conference on Software Engineering and Advanced Applications (SEAA). 363-367. https://doi.org/10.1109/SEAA.2016.20\n\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics1Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). Association for Computational Linguistics, Minneapolis, Minnesota, 4171-4186. https://doi.org/10.18653/v1/N19-1423\n\nGeoffrey Hinton, Oriol Vinyals, Jeff Dean, arXiv:1503.02531Distilling the knowledge in a neural network. arXiv preprintGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531 (2015).\n\nAutomated bug assignment: Ensemble-based machine learning in large scale industrial contexts. Leif Jonsson, Markus Borg, David Broman, Kristian Sandahl, Empirical Software Engineering. 21Sigrid Eldh, and Per RunesonLeif Jonsson, Markus Borg, David Broman, Kristian Sandahl, Sigrid Eldh, and Per Runeson. 2016. Automated bug assignment: Ensemble-based machine learning in large scale industrial contexts. Empirical Software Engineering 21, 4 (2016), 1533-1578.\n\nShallow-deep networks: Understanding and mitigating network overthinking. Yigitcan Kaya, Sanghyun Hong, Tudor Dumitras, PMLRInternational Conference on Machine Learning. Yigitcan Kaya, Sanghyun Hong, and Tudor Dumitras. 2019. Shallow-deep net- works: Understanding and mitigating network overthinking. In International Conference on Machine Learning. PMLR, 3301-3310.\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980arXiv preprintDiederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti- mization. arXiv preprint arXiv:1412.6980 (2014).\n\nAlbert: A lite bert for self-supervised learning of language representations. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut, arXiv:1909.11942arXiv preprintZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942 (2019).\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. arXiv preprintYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 (2019).\n\nDeeptriage: Exploring the effectiveness of deep learning for bug triaging. Senthil Mani, Anush Sankaran, Rahul Aralikatte, Proceedings of the ACM India Joint International Conference on Data Science and Management of Data. the ACM India Joint International Conference on Data Science and Management of DataSenthil Mani, Anush Sankaran, and Rahul Aralikatte. 2019. Deeptriage: Exploring the effectiveness of deep learning for bug triaging. In Proceedings of the ACM India Joint International Conference on Data Science and Management of Data. 171-179.\n\nCatastrophic interference in connectionist networks: The sequential learning problem. Michael Mccloskey, J Neal, Cohen, Psychology of learning and motivation. Elsevier24Michael McCloskey and Neal J Cohen. 1989. Catastrophic interference in con- nectionist networks: The sequential learning problem. In Psychology of learning and motivation. Vol. 24. Elsevier, 109-165.\n\nEfficient estimation of word representations in vector space. Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean, arXiv:1301.3781arXiv preprintTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781 (2013).\n\nAaron Van Den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, Koray Kavukcuoglu, arXiv:1609.03499Wavenet: A generative model for raw audio. arXiv preprintAaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. 2016. Wavenet: A generative model for raw audio. arXiv preprint arXiv:1609.03499 (2016).\n\nDeep Contextualized Word Representations. Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer, 10.18653/v1/N18-1202Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, LouisianaAssociation for Computational Linguistics1Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep Contextualized Word Rep- resentations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers). Association for Computational Linguistics, New Orleans, Louisiana, 2227-2237. https://doi.org/10.18653/v1/N18-1202\n\nLanguage models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 19Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog 1, 8 (2019), 9.\n\nBee: a tool for structuring and analyzing bug reports. Yang Song, Oscar Chaparro, Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software EngineeringYang Song and Oscar Chaparro. 2020. Bee: a tool for structuring and analyzing bug reports. In Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 1551-1555.\n\nSiqi Sun, Yu Cheng, Zhe Gan, Jingjing Liu, arXiv:1908.09355Patient knowledge distillation for bert model compression. arXiv preprintSiqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019. Patient knowledge distilla- tion for bert model compression. arXiv preprint arXiv:1908.09355 (2019).\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems. 5998-6008.\n\nEvaluating the usefulness of ir-based fault localization techniques. Qianqian Wang, Chris Parnin, Alessandro Orso, Proceedings of the 2015 international symposium on software testing and analysis. the 2015 international symposium on software testing and analysisQianqian Wang, Chris Parnin, and Alessandro Orso. 2015. Evaluating the useful- ness of ir-based fault localization techniques. In Proceedings of the 2015 interna- tional symposium on software testing and analysis. 1-11.\n\nHuggingface's transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, arXiv:1910.03771arXiv preprintThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, et al. 2019. Huggingface's transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771 (2019).\n\nTowards Effective Bug Triage with Software Data Reduction Techniques. Jifeng Xuan, He Jiang, Yan Hu, Zhilei Ren, Weiqin Zou, Zhongxuan Luo, Xindong Wu, 10.1109/TKDE.2014.2324590IEEE Transactions on Knowledge and Data Engineering. 27Jifeng Xuan, He Jiang, Yan Hu, Zhilei Ren, Weiqin Zou, Zhongxuan Luo, and Xindong Wu. 2015. Towards Effective Bug Triage with Software Data Reduction Techniques. IEEE Transactions on Knowledge and Data Engineering 27, 1 (2015), 264-280. https://doi.org/10.1109/TKDE.2014.2324590\n\nReducing Bug Triaging Confusion by Learning from Mistakes with a Bug Tossing Knowledge Graph. 36th IEEE/ACM International Conference on Automated Software Engineering (ASE). Xin Peng Xin Xia Chong Wang Xiwei Xu Liming Zhu Yanqi SuIEEEXin Peng Xin Xia Chong Wang Xiwei Xu Liming Zhu Yanqi Su, Zhenchang Xing. 2021. Reducing Bug Triaging Confusion by Learning from Mistakes with a Bug Tossing Knowledge Graph. In 36th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE.\n\nLearning to rank relevant files for bug reports using domain knowledge. Xin Ye, Razvan Bunescu, Chang Liu, Proceedings of the 22nd ACM SIGSOFT international symposium on foundations of software engineering. the 22nd ACM SIGSOFT international symposium on foundations of software engineeringXin Ye, Razvan Bunescu, and Chang Liu. 2014. Learning to rank relevant files for bug reports using domain knowledge. In Proceedings of the 22nd ACM SIGSOFT international symposium on foundations of software engineering. 689-699.\n\nApplying Convolutional Neural Networks With Different Word Representation Techniques to Recommend Bug Fixers. Alam Syed Farhan, Faraz Zaidi, Minsoo Malik Awan, Honguk Lee, Chan-Gun Woo, Lee, 10.1109/ACCESS.2020.3040065IEEE Access. 8Syed Farhan Alam Zaidi, Faraz Malik Awan, Minsoo Lee, Honguk Woo, and Chan-Gun Lee. 2020. Applying Convolutional Neural Networks With Different Word Representation Techniques to Recommend Bug Fixers. IEEE Access 8 (2020), 213729-213747. https://doi.org/10.1109/ACCESS.2020.3040065\n", "annotations": {"author": "[{\"end\":91,\"start\":78},{\"end\":123,\"start\":92},{\"end\":157,\"start\":124},{\"end\":232,\"start\":158},{\"end\":269,\"start\":233},{\"end\":343,\"start\":270}]", "publisher": null, "author_last_name": "[{\"end\":90,\"start\":87},{\"end\":101,\"start\":98},{\"end\":133,\"start\":131}]", "author_first_name": "[{\"end\":86,\"start\":78},{\"end\":97,\"start\":92},{\"end\":130,\"start\":124}]", "author_affiliation": "[{\"end\":231,\"start\":159},{\"end\":268,\"start\":234},{\"end\":342,\"start\":271}]", "title": "[{\"end\":75,\"start\":1},{\"end\":418,\"start\":344}]", "venue": null, "abstract": "[{\"end\":2183,\"start\":849}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3986,\"start\":3983},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3988,\"start\":3986},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3991,\"start\":3988},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4085,\"start\":4081},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4129,\"start\":4125},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4144,\"start\":4140},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4183,\"start\":4179},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4270,\"start\":4267},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4806,\"start\":4802},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5187,\"start\":5184},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5680,\"start\":5677},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6115,\"start\":6111},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6296,\"start\":6292},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7348,\"start\":7344},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7545,\"start\":7542},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7635,\"start\":7632},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7971,\"start\":7967},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7993,\"start\":7989},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8393,\"start\":8389},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8411,\"start\":8407},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8932,\"start\":8928},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9411,\"start\":9408},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9934,\"start\":9930},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10068,\"start\":10065},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10192,\"start\":10189},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":15099,\"start\":15095},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":15278,\"start\":15274},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":16325,\"start\":16321},{\"end\":17048,\"start\":17043},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":18209,\"start\":18206},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":19365,\"start\":19361},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":20258,\"start\":20254},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":20640,\"start\":20636},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":21711,\"start\":21707},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":26056,\"start\":26053},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":29865,\"start\":29861},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":30567,\"start\":30564},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":31394,\"start\":31390},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":31711,\"start\":31707},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":31760,\"start\":31756},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":38976,\"start\":38973},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":39197,\"start\":39193},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":39944,\"start\":39940},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":40325,\"start\":40321},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":40346,\"start\":40342},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":40958,\"start\":40954},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":41220,\"start\":41216}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":41791,\"start\":41706},{\"attributes\":{\"id\":\"fig_2\"},\"end\":42515,\"start\":41792},{\"attributes\":{\"id\":\"fig_3\"},\"end\":42549,\"start\":42516},{\"attributes\":{\"id\":\"fig_4\"},\"end\":42779,\"start\":42550},{\"attributes\":{\"id\":\"fig_5\"},\"end\":42837,\"start\":42780},{\"attributes\":{\"id\":\"fig_6\"},\"end\":43048,\"start\":42838},{\"attributes\":{\"id\":\"fig_7\"},\"end\":43181,\"start\":43049},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":43848,\"start\":43182},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":44149,\"start\":43849},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":45013,\"start\":44150},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":45457,\"start\":45014},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":45904,\"start\":45458},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":46633,\"start\":45905},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":46792,\"start\":46634},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":47180,\"start\":46793}]", "paragraph": "[{\"end\":3203,\"start\":2199},{\"end\":3811,\"start\":3205},{\"end\":4184,\"start\":3813},{\"end\":6297,\"start\":4186},{\"end\":6360,\"start\":6299},{\"end\":6840,\"start\":6362},{\"end\":8730,\"start\":6878},{\"end\":9838,\"start\":8767},{\"end\":10303,\"start\":9840},{\"end\":10806,\"start\":10305},{\"end\":11311,\"start\":10833},{\"end\":12001,\"start\":11313},{\"end\":12075,\"start\":12003},{\"end\":12630,\"start\":12099},{\"end\":13105,\"start\":12632},{\"end\":13468,\"start\":13147},{\"end\":15221,\"start\":13494},{\"end\":15768,\"start\":15223},{\"end\":16016,\"start\":15770},{\"end\":16517,\"start\":16031},{\"end\":17245,\"start\":16519},{\"end\":17539,\"start\":17283},{\"end\":17937,\"start\":17541},{\"end\":18742,\"start\":17939},{\"end\":19546,\"start\":18786},{\"end\":19880,\"start\":19548},{\"end\":20666,\"start\":19916},{\"end\":21454,\"start\":20782},{\"end\":22360,\"start\":21493},{\"end\":24222,\"start\":22381},{\"end\":25788,\"start\":24262},{\"end\":26634,\"start\":25814},{\"end\":27183,\"start\":26636},{\"end\":27441,\"start\":27231},{\"end\":27685,\"start\":27443},{\"end\":28080,\"start\":27721},{\"end\":28279,\"start\":28082},{\"end\":28338,\"start\":28333},{\"end\":28774,\"start\":28365},{\"end\":28878,\"start\":28813},{\"end\":28942,\"start\":28880},{\"end\":29091,\"start\":28944},{\"end\":29762,\"start\":29093},{\"end\":30381,\"start\":29774},{\"end\":30935,\"start\":30404},{\"end\":31085,\"start\":30937},{\"end\":31107,\"start\":31087},{\"end\":31761,\"start\":31109},{\"end\":32090,\"start\":31763},{\"end\":32791,\"start\":32137},{\"end\":33120,\"start\":32838},{\"end\":34030,\"start\":33122},{\"end\":34909,\"start\":34032},{\"end\":36818,\"start\":34997},{\"end\":37298,\"start\":36820},{\"end\":38689,\"start\":37355},{\"end\":40428,\"start\":38706},{\"end\":41304,\"start\":40456},{\"end\":41705,\"start\":41306}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13146,\"start\":13106},{\"attributes\":{\"id\":\"formula_1\"},\"end\":18785,\"start\":18743},{\"attributes\":{\"id\":\"formula_2\"},\"end\":20781,\"start\":20667},{\"attributes\":{\"id\":\"formula_3\"},\"end\":27230,\"start\":27184},{\"attributes\":{\"id\":\"formula_4\"},\"end\":27720,\"start\":27686},{\"attributes\":{\"id\":\"formula_5\"},\"end\":28332,\"start\":28280},{\"attributes\":{\"id\":\"formula_6\"},\"end\":28364,\"start\":28339}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":30345,\"start\":30338},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":32535,\"start\":32528},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":32928,\"start\":32921},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":33000,\"start\":32993},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":34383,\"start\":34376},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":35279,\"start\":35272},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":35287,\"start\":35280}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2197,\"start\":2185},{\"attributes\":{\"n\":\"2\"},\"end\":6876,\"start\":6843},{\"attributes\":{\"n\":\"2.2\"},\"end\":8765,\"start\":8733},{\"attributes\":{\"n\":\"3\"},\"end\":10831,\"start\":10809},{\"attributes\":{\"n\":\"3.1\"},\"end\":12097,\"start\":12078},{\"attributes\":{\"n\":\"3.2\"},\"end\":13492,\"start\":13471},{\"attributes\":{\"n\":\"3.3\"},\"end\":16029,\"start\":16019},{\"attributes\":{\"n\":\"3.4\"},\"end\":17281,\"start\":17248},{\"attributes\":{\"n\":\"3.4.2\"},\"end\":19914,\"start\":19883},{\"attributes\":{\"n\":\"3.5\"},\"end\":21491,\"start\":21457},{\"attributes\":{\"n\":\"3.5.1\"},\"end\":22379,\"start\":22363},{\"attributes\":{\"n\":\"3.5.2\"},\"end\":24260,\"start\":24225},{\"attributes\":{\"n\":\"3.6\"},\"end\":25812,\"start\":25791},{\"attributes\":{\"n\":\"4\"},\"end\":28811,\"start\":28777},{\"attributes\":{\"n\":\"4.2\"},\"end\":29772,\"start\":29765},{\"attributes\":{\"n\":\"4.3\"},\"end\":30402,\"start\":30384},{\"attributes\":{\"n\":\"4.4\"},\"end\":32135,\"start\":32093},{\"attributes\":{\"n\":\"4.5\"},\"end\":32836,\"start\":32794},{\"attributes\":{\"n\":\"4.6\"},\"end\":34932,\"start\":34912},{\"attributes\":{\"n\":\"4.6.1\"},\"end\":34995,\"start\":34935},{\"attributes\":{\"n\":\"4.6.3\"},\"end\":37353,\"start\":37301},{\"attributes\":{\"n\":\"5\"},\"end\":38704,\"start\":38692},{\"attributes\":{\"n\":\"6\"},\"end\":40454,\"start\":40431},{\"end\":41717,\"start\":41707},{\"end\":41798,\"start\":41793},{\"end\":42561,\"start\":42551},{\"end\":42849,\"start\":42839},{\"end\":43060,\"start\":43050},{\"end\":43859,\"start\":43850},{\"end\":44160,\"start\":44151},{\"end\":45024,\"start\":45015},{\"end\":45468,\"start\":45459},{\"end\":45915,\"start\":45906},{\"end\":46803,\"start\":46794}]", "table": "[{\"end\":43848,\"start\":43713},{\"end\":44149,\"start\":43899},{\"end\":45013,\"start\":44211},{\"end\":45457,\"start\":45074},{\"end\":45904,\"start\":45521},{\"end\":46633,\"start\":45964},{\"end\":47180,\"start\":46904}]", "figure_caption": "[{\"end\":41791,\"start\":41719},{\"end\":42515,\"start\":41800},{\"end\":42549,\"start\":42518},{\"end\":42779,\"start\":42563},{\"end\":42837,\"start\":42782},{\"end\":43048,\"start\":42851},{\"end\":43181,\"start\":43062},{\"end\":43713,\"start\":43184},{\"end\":43899,\"start\":43861},{\"end\":44211,\"start\":44162},{\"end\":45074,\"start\":45026},{\"end\":45521,\"start\":45470},{\"end\":45964,\"start\":45917},{\"end\":46792,\"start\":46636},{\"end\":46904,\"start\":46805}]", "figure_ref": "[{\"end\":10958,\"start\":10950},{\"end\":11722,\"start\":11714},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":27399,\"start\":27391},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":35880,\"start\":35872},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":37605,\"start\":37597},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":38043,\"start\":38035},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":38366,\"start\":38358}]", "bib_author_first_name": "[{\"end\":47291,\"start\":47282},{\"end\":47306,\"start\":47298},{\"end\":47319,\"start\":47312},{\"end\":47334,\"start\":47325},{\"end\":47342,\"start\":47339},{\"end\":47355,\"start\":47349},{\"end\":47733,\"start\":47728},{\"end\":47751,\"start\":47741},{\"end\":47760,\"start\":47759},{\"end\":47780,\"start\":47767},{\"end\":48151,\"start\":48145},{\"end\":48164,\"start\":48159},{\"end\":48587,\"start\":48582},{\"end\":48604,\"start\":48596},{\"end\":48618,\"start\":48612},{\"end\":48632,\"start\":48624},{\"end\":49458,\"start\":49450},{\"end\":49472,\"start\":49467},{\"end\":49486,\"start\":49482},{\"end\":49807,\"start\":49803},{\"end\":49823,\"start\":49817},{\"end\":49835,\"start\":49830},{\"end\":49852,\"start\":49844},{\"end\":50252,\"start\":50244},{\"end\":50267,\"start\":50259},{\"end\":50279,\"start\":50274},{\"end\":50584,\"start\":50583},{\"end\":50600,\"start\":50595},{\"end\":50853,\"start\":50844},{\"end\":50865,\"start\":50859},{\"end\":50881,\"start\":50872},{\"end\":50896,\"start\":50891},{\"end\":50911,\"start\":50905},{\"end\":50924,\"start\":50920},{\"end\":51189,\"start\":51183},{\"end\":51199,\"start\":51195},{\"end\":51210,\"start\":51205},{\"end\":51225,\"start\":51218},{\"end\":51236,\"start\":51230},{\"end\":51249,\"start\":51244},{\"end\":51260,\"start\":51256},{\"end\":51271,\"start\":51267},{\"end\":51283,\"start\":51279},{\"end\":51304,\"start\":51297},{\"end\":51724,\"start\":51717},{\"end\":51736,\"start\":51731},{\"end\":51752,\"start\":51747},{\"end\":52287,\"start\":52280},{\"end\":52300,\"start\":52299},{\"end\":52631,\"start\":52626},{\"end\":52644,\"start\":52641},{\"end\":52655,\"start\":52651},{\"end\":52672,\"start\":52665},{\"end\":52878,\"start\":52873},{\"end\":52899,\"start\":52893},{\"end\":52915,\"start\":52910},{\"end\":52926,\"start\":52921},{\"end\":52942,\"start\":52937},{\"end\":52956,\"start\":52952},{\"end\":52968,\"start\":52965},{\"end\":52989,\"start\":52983},{\"end\":53003,\"start\":52998},{\"end\":53377,\"start\":53370},{\"end\":53379,\"start\":53378},{\"end\":53392,\"start\":53388},{\"end\":53407,\"start\":53402},{\"end\":53419,\"start\":53415},{\"end\":53440,\"start\":53429},{\"end\":53454,\"start\":53448},{\"end\":53464,\"start\":53460},{\"end\":54340,\"start\":54336},{\"end\":54357,\"start\":54350},{\"end\":54367,\"start\":54362},{\"end\":54380,\"start\":54375},{\"end\":54392,\"start\":54387},{\"end\":54405,\"start\":54401},{\"end\":54667,\"start\":54663},{\"end\":54679,\"start\":54674},{\"end\":55215,\"start\":55211},{\"end\":55223,\"start\":55221},{\"end\":55234,\"start\":55231},{\"end\":55248,\"start\":55240},{\"end\":55531,\"start\":55525},{\"end\":55545,\"start\":55541},{\"end\":55559,\"start\":55555},{\"end\":55573,\"start\":55568},{\"end\":55590,\"start\":55585},{\"end\":55603,\"start\":55598},{\"end\":55605,\"start\":55604},{\"end\":55619,\"start\":55613},{\"end\":55633,\"start\":55628},{\"end\":55998,\"start\":55990},{\"end\":56010,\"start\":56005},{\"end\":56029,\"start\":56019},{\"end\":56484,\"start\":56478},{\"end\":56499,\"start\":56491},{\"end\":56513,\"start\":56507},{\"end\":56526,\"start\":56520},{\"end\":56544,\"start\":56537},{\"end\":56562,\"start\":56555},{\"end\":56575,\"start\":56568},{\"end\":56587,\"start\":56584},{\"end\":56599,\"start\":56595},{\"end\":56612,\"start\":56606},{\"end\":57004,\"start\":56998},{\"end\":57013,\"start\":57011},{\"end\":57024,\"start\":57021},{\"end\":57035,\"start\":57029},{\"end\":57047,\"start\":57041},{\"end\":57062,\"start\":57053},{\"end\":57075,\"start\":57068},{\"end\":58013,\"start\":58010},{\"end\":58024,\"start\":58018},{\"end\":58039,\"start\":58034},{\"end\":58572,\"start\":58568},{\"end\":58591,\"start\":58586},{\"end\":58605,\"start\":58599},{\"end\":58624,\"start\":58618},{\"end\":58638,\"start\":58630}]", "bib_author_last_name": "[{\"end\":47296,\"start\":47292},{\"end\":47310,\"start\":47307},{\"end\":47323,\"start\":47320},{\"end\":47337,\"start\":47335},{\"end\":47347,\"start\":47343},{\"end\":47358,\"start\":47356},{\"end\":47739,\"start\":47734},{\"end\":47757,\"start\":47752},{\"end\":47765,\"start\":47761},{\"end\":47783,\"start\":47781},{\"end\":47792,\"start\":47785},{\"end\":48157,\"start\":48152},{\"end\":48170,\"start\":48165},{\"end\":48594,\"start\":48588},{\"end\":48610,\"start\":48605},{\"end\":48622,\"start\":48619},{\"end\":48642,\"start\":48633},{\"end\":49465,\"start\":49459},{\"end\":49480,\"start\":49473},{\"end\":49491,\"start\":49487},{\"end\":49815,\"start\":49808},{\"end\":49828,\"start\":49824},{\"end\":49842,\"start\":49836},{\"end\":49860,\"start\":49853},{\"end\":50257,\"start\":50253},{\"end\":50272,\"start\":50268},{\"end\":50288,\"start\":50280},{\"end\":50593,\"start\":50585},{\"end\":50607,\"start\":50601},{\"end\":50611,\"start\":50609},{\"end\":50857,\"start\":50854},{\"end\":50870,\"start\":50866},{\"end\":50889,\"start\":50882},{\"end\":50903,\"start\":50897},{\"end\":50918,\"start\":50912},{\"end\":50932,\"start\":50925},{\"end\":51193,\"start\":51190},{\"end\":51203,\"start\":51200},{\"end\":51216,\"start\":51211},{\"end\":51228,\"start\":51226},{\"end\":51242,\"start\":51237},{\"end\":51254,\"start\":51250},{\"end\":51265,\"start\":51261},{\"end\":51277,\"start\":51272},{\"end\":51295,\"start\":51284},{\"end\":51313,\"start\":51305},{\"end\":51729,\"start\":51725},{\"end\":51745,\"start\":51737},{\"end\":51763,\"start\":51753},{\"end\":52297,\"start\":52288},{\"end\":52305,\"start\":52301},{\"end\":52312,\"start\":52307},{\"end\":52639,\"start\":52632},{\"end\":52649,\"start\":52645},{\"end\":52663,\"start\":52656},{\"end\":52677,\"start\":52673},{\"end\":52891,\"start\":52879},{\"end\":52908,\"start\":52900},{\"end\":52919,\"start\":52916},{\"end\":52935,\"start\":52927},{\"end\":52950,\"start\":52943},{\"end\":52963,\"start\":52957},{\"end\":52981,\"start\":52969},{\"end\":52996,\"start\":52990},{\"end\":53015,\"start\":53004},{\"end\":53386,\"start\":53380},{\"end\":53400,\"start\":53393},{\"end\":53413,\"start\":53408},{\"end\":53427,\"start\":53420},{\"end\":53446,\"start\":53441},{\"end\":53458,\"start\":53455},{\"end\":53476,\"start\":53465},{\"end\":54348,\"start\":54341},{\"end\":54360,\"start\":54358},{\"end\":54373,\"start\":54368},{\"end\":54385,\"start\":54381},{\"end\":54399,\"start\":54393},{\"end\":54415,\"start\":54406},{\"end\":54672,\"start\":54668},{\"end\":54688,\"start\":54680},{\"end\":55219,\"start\":55216},{\"end\":55229,\"start\":55224},{\"end\":55238,\"start\":55235},{\"end\":55252,\"start\":55249},{\"end\":55539,\"start\":55532},{\"end\":55553,\"start\":55546},{\"end\":55566,\"start\":55560},{\"end\":55583,\"start\":55574},{\"end\":55596,\"start\":55591},{\"end\":55611,\"start\":55606},{\"end\":55626,\"start\":55620},{\"end\":55644,\"start\":55634},{\"end\":56003,\"start\":55999},{\"end\":56017,\"start\":56011},{\"end\":56034,\"start\":56030},{\"end\":56489,\"start\":56485},{\"end\":56505,\"start\":56500},{\"end\":56518,\"start\":56514},{\"end\":56535,\"start\":56527},{\"end\":56553,\"start\":56545},{\"end\":56566,\"start\":56563},{\"end\":56582,\"start\":56576},{\"end\":56593,\"start\":56588},{\"end\":56604,\"start\":56600},{\"end\":56622,\"start\":56613},{\"end\":57009,\"start\":57005},{\"end\":57019,\"start\":57014},{\"end\":57027,\"start\":57025},{\"end\":57039,\"start\":57036},{\"end\":57051,\"start\":57048},{\"end\":57066,\"start\":57063},{\"end\":57078,\"start\":57076},{\"end\":58016,\"start\":58014},{\"end\":58032,\"start\":58025},{\"end\":58043,\"start\":58040},{\"end\":58584,\"start\":58573},{\"end\":58597,\"start\":58592},{\"end\":58616,\"start\":58606},{\"end\":58628,\"start\":58625},{\"end\":58642,\"start\":58639},{\"end\":58647,\"start\":58644}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":229703923},\"end\":47726,\"start\":47197},{\"attributes\":{\"doi\":\"arXiv:2003.10555\",\"id\":\"b1\"},\"end\":48094,\"start\":47728},{\"attributes\":{\"doi\":\"10.1109/SEAA.2016.20\",\"id\":\"b2\",\"matched_paper_id\":16413906},\"end\":48498,\"start\":48096},{\"attributes\":{\"doi\":\"10.18653/v1/N19-1423\",\"id\":\"b3\",\"matched_paper_id\":52967399},\"end\":49448,\"start\":48500},{\"attributes\":{\"doi\":\"arXiv:1503.02531\",\"id\":\"b4\"},\"end\":49707,\"start\":49450},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":4602109},\"end\":50168,\"start\":49709},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b6\",\"matched_paper_id\":147703951},\"end\":50537,\"start\":50170},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b7\"},\"end\":50764,\"start\":50539},{\"attributes\":{\"doi\":\"arXiv:1909.11942\",\"id\":\"b8\"},\"end\":51181,\"start\":50766},{\"attributes\":{\"doi\":\"arXiv:1907.11692\",\"id\":\"b9\"},\"end\":51640,\"start\":51183},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":19735420},\"end\":52192,\"start\":51642},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":61019113},\"end\":52562,\"start\":52194},{\"attributes\":{\"doi\":\"arXiv:1301.3781\",\"id\":\"b12\"},\"end\":52871,\"start\":52564},{\"attributes\":{\"doi\":\"arXiv:1609.03499\",\"id\":\"b13\"},\"end\":53326,\"start\":52873},{\"attributes\":{\"doi\":\"10.18653/v1/N18-1202\",\"id\":\"b14\",\"matched_paper_id\":3626819},\"end\":54281,\"start\":53328},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":160025533},\"end\":54606,\"start\":54283},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":226274248},\"end\":55209,\"start\":54608},{\"attributes\":{\"doi\":\"arXiv:1908.09355\",\"id\":\"b17\"},\"end\":55496,\"start\":55211},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":13756489},\"end\":55919,\"start\":55498},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":14469144},\"end\":56402,\"start\":55921},{\"attributes\":{\"doi\":\"arXiv:1910.03771\",\"id\":\"b20\"},\"end\":56926,\"start\":56404},{\"attributes\":{\"doi\":\"10.1109/TKDE.2014.2324590\",\"id\":\"b21\",\"matched_paper_id\":12332144},\"end\":57438,\"start\":56928},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":246081537},\"end\":57936,\"start\":57440},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":2361519},\"end\":58456,\"start\":57938},{\"attributes\":{\"doi\":\"10.1109/ACCESS.2020.3040065\",\"id\":\"b24\",\"matched_paper_id\":228093976},\"end\":58970,\"start\":58458}]", "bib_title": "[{\"end\":47280,\"start\":47197},{\"end\":48143,\"start\":48096},{\"end\":48580,\"start\":48500},{\"end\":49801,\"start\":49709},{\"end\":50242,\"start\":50170},{\"end\":51715,\"start\":51642},{\"end\":52278,\"start\":52194},{\"end\":53368,\"start\":53328},{\"end\":54334,\"start\":54283},{\"end\":54661,\"start\":54608},{\"end\":55523,\"start\":55498},{\"end\":55988,\"start\":55921},{\"end\":56996,\"start\":56928},{\"end\":57532,\"start\":57440},{\"end\":58008,\"start\":57938},{\"end\":58566,\"start\":58458}]", "bib_author": "[{\"end\":47298,\"start\":47282},{\"end\":47312,\"start\":47298},{\"end\":47325,\"start\":47312},{\"end\":47339,\"start\":47325},{\"end\":47349,\"start\":47339},{\"end\":47360,\"start\":47349},{\"end\":47741,\"start\":47728},{\"end\":47759,\"start\":47741},{\"end\":47767,\"start\":47759},{\"end\":47785,\"start\":47767},{\"end\":47794,\"start\":47785},{\"end\":48159,\"start\":48145},{\"end\":48172,\"start\":48159},{\"end\":48596,\"start\":48582},{\"end\":48612,\"start\":48596},{\"end\":48624,\"start\":48612},{\"end\":48644,\"start\":48624},{\"end\":49467,\"start\":49450},{\"end\":49482,\"start\":49467},{\"end\":49493,\"start\":49482},{\"end\":49817,\"start\":49803},{\"end\":49830,\"start\":49817},{\"end\":49844,\"start\":49830},{\"end\":49862,\"start\":49844},{\"end\":50259,\"start\":50244},{\"end\":50274,\"start\":50259},{\"end\":50290,\"start\":50274},{\"end\":50595,\"start\":50583},{\"end\":50609,\"start\":50595},{\"end\":50613,\"start\":50609},{\"end\":50859,\"start\":50844},{\"end\":50872,\"start\":50859},{\"end\":50891,\"start\":50872},{\"end\":50905,\"start\":50891},{\"end\":50920,\"start\":50905},{\"end\":50934,\"start\":50920},{\"end\":51195,\"start\":51183},{\"end\":51205,\"start\":51195},{\"end\":51218,\"start\":51205},{\"end\":51230,\"start\":51218},{\"end\":51244,\"start\":51230},{\"end\":51256,\"start\":51244},{\"end\":51267,\"start\":51256},{\"end\":51279,\"start\":51267},{\"end\":51297,\"start\":51279},{\"end\":51315,\"start\":51297},{\"end\":51731,\"start\":51717},{\"end\":51747,\"start\":51731},{\"end\":51765,\"start\":51747},{\"end\":52299,\"start\":52280},{\"end\":52307,\"start\":52299},{\"end\":52314,\"start\":52307},{\"end\":52641,\"start\":52626},{\"end\":52651,\"start\":52641},{\"end\":52665,\"start\":52651},{\"end\":52679,\"start\":52665},{\"end\":52893,\"start\":52873},{\"end\":52910,\"start\":52893},{\"end\":52921,\"start\":52910},{\"end\":52937,\"start\":52921},{\"end\":52952,\"start\":52937},{\"end\":52965,\"start\":52952},{\"end\":52983,\"start\":52965},{\"end\":52998,\"start\":52983},{\"end\":53017,\"start\":52998},{\"end\":53388,\"start\":53370},{\"end\":53402,\"start\":53388},{\"end\":53415,\"start\":53402},{\"end\":53429,\"start\":53415},{\"end\":53448,\"start\":53429},{\"end\":53460,\"start\":53448},{\"end\":53478,\"start\":53460},{\"end\":54350,\"start\":54336},{\"end\":54362,\"start\":54350},{\"end\":54375,\"start\":54362},{\"end\":54387,\"start\":54375},{\"end\":54401,\"start\":54387},{\"end\":54417,\"start\":54401},{\"end\":54674,\"start\":54663},{\"end\":54690,\"start\":54674},{\"end\":55221,\"start\":55211},{\"end\":55231,\"start\":55221},{\"end\":55240,\"start\":55231},{\"end\":55254,\"start\":55240},{\"end\":55541,\"start\":55525},{\"end\":55555,\"start\":55541},{\"end\":55568,\"start\":55555},{\"end\":55585,\"start\":55568},{\"end\":55598,\"start\":55585},{\"end\":55613,\"start\":55598},{\"end\":55628,\"start\":55613},{\"end\":55646,\"start\":55628},{\"end\":56005,\"start\":55990},{\"end\":56019,\"start\":56005},{\"end\":56036,\"start\":56019},{\"end\":56491,\"start\":56478},{\"end\":56507,\"start\":56491},{\"end\":56520,\"start\":56507},{\"end\":56537,\"start\":56520},{\"end\":56555,\"start\":56537},{\"end\":56568,\"start\":56555},{\"end\":56584,\"start\":56568},{\"end\":56595,\"start\":56584},{\"end\":56606,\"start\":56595},{\"end\":56624,\"start\":56606},{\"end\":57011,\"start\":56998},{\"end\":57021,\"start\":57011},{\"end\":57029,\"start\":57021},{\"end\":57041,\"start\":57029},{\"end\":57053,\"start\":57041},{\"end\":57068,\"start\":57053},{\"end\":57080,\"start\":57068},{\"end\":58018,\"start\":58010},{\"end\":58034,\"start\":58018},{\"end\":58045,\"start\":58034},{\"end\":58586,\"start\":58568},{\"end\":58599,\"start\":58586},{\"end\":58618,\"start\":58599},{\"end\":58630,\"start\":58618},{\"end\":58644,\"start\":58630},{\"end\":58649,\"start\":58644}]", "bib_venue": "[{\"end\":47443,\"start\":47360},{\"end\":47886,\"start\":47810},{\"end\":48274,\"start\":48192},{\"end\":48806,\"start\":48664},{\"end\":49553,\"start\":49509},{\"end\":49892,\"start\":49862},{\"end\":50338,\"start\":50294},{\"end\":50581,\"start\":50539},{\"end\":50842,\"start\":50766},{\"end\":51386,\"start\":51331},{\"end\":51863,\"start\":51765},{\"end\":52351,\"start\":52314},{\"end\":52624,\"start\":52564},{\"end\":53074,\"start\":53033},{\"end\":53640,\"start\":53498},{\"end\":54428,\"start\":54417},{\"end\":54832,\"start\":54690},{\"end\":55327,\"start\":55270},{\"end\":55695,\"start\":55646},{\"end\":56116,\"start\":56036},{\"end\":56476,\"start\":56404},{\"end\":57156,\"start\":57105},{\"end\":57612,\"start\":57534},{\"end\":58143,\"start\":58045},{\"end\":58687,\"start\":58676},{\"end\":48957,\"start\":48808},{\"end\":51948,\"start\":51865},{\"end\":53791,\"start\":53642},{\"end\":54961,\"start\":54834},{\"end\":56183,\"start\":56118},{\"end\":58228,\"start\":58145}]"}}}, "year": 2023, "month": 12, "day": 17}