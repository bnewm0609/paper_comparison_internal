{"id": 52912195, "updated": "2023-09-30 15:09:44.352", "metadata": {"title": "CNN-SVO: Improving the Mapping in Semi-Direct Visual Odometry Using Single-Image Depth Prediction", "authors": "[{\"first\":\"Shing\",\"last\":\"Loo\",\"middle\":[\"Yan\"]},{\"first\":\"Ali\",\"last\":\"Amiri\",\"middle\":[\"Jahani\"]},{\"first\":\"Syamsiah\",\"last\":\"Mashohor\",\"middle\":[]},{\"first\":\"Sai\",\"last\":\"Tang\",\"middle\":[\"Hong\"]},{\"first\":\"Hong\",\"last\":\"Zhang\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2018, "month": 10, "day": 1}, "abstract": "Reliable feature correspondence between frames is a critical step in visual odometry (VO) and visual simultaneous localization and mapping (V-SLAM) algorithms. In comparison with existing VO and V-SLAM algorithms, semi-direct visual odometry (SVO) has two main advantages that lead to state-of-the-art frame rate camera motion estimation: direct pixel correspondence and efficient implementation of probabilistic mapping method. This paper improves the SVO mapping by initializing the mean and the variance of the depth at a feature location according to the depth prediction from a single-image depth prediction network. By significantly reducing the depth uncertainty of the initialized map point (i.e., small variance centred about the depth prediction), the benefits are twofold: reliable feature correspondence between views and fast convergence to the true depth in order to create new map points. We evaluate our method with two outdoor datasets: KITTI dataset and Oxford Robotcar dataset. The experimental results indicate that the improved SVO mapping results in increased robustness and camera tracking accuracy.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1810.01011", "mag": "2968288611", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icra/LooAMT019", "doi": "10.1109/icra.2019.8794425"}}, "content": {"source": {"pdf_hash": "bab7bf463de7d4cdc9c9c0c9612de571a9790cef", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1810.01011v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://psasir.upm.edu.my/id/eprint/36196/1/CNN-SVO%20improving%20the%20mapping%20in%20semi-direct%20visual%20odometry%20using%20single-image%20depth%20prediction.pdf", "status": "GREEN"}}, "grobid": {"id": "fe9b7f69eb871d1ce27d6bcf08f71cd5b7640eac", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/bab7bf463de7d4cdc9c9c0c9612de571a9790cef.txt", "contents": "\nCNN-SVO: Improving the Mapping in Semi-Direct Visual Odometry Using Single-Image Depth Prediction\n1 Oct 2018\n\nShing Yan Loo \nAli Jahani Amiri \nSyamsiah Mashohor \nHong Sai \nTang \nHong Zhang \nCNN-SVO: Improving the Mapping in Semi-Direct Visual Odometry Using Single-Image Depth Prediction\n1 Oct 2018\nReliable feature correspondence between frames is a critical step in visual odometry (VO) and visual simultaneous localization and mapping (V-SLAM) algorithms. In comparison with existing VO and V-SLAM algorithms, semi-direct visual odometry (SVO) has two main advantages that lead to stateof-the-art frame rate camera motion estimation: direct pixel correspondence and efficient implementation of probabilistic mapping method. This paper improves the SVO mapping by initializing the mean and the variance of the depth at a feature location according to the depth prediction from a singleimage depth prediction network. By significantly reducing the depth uncertainty of the initialized map point (i.e., small variance centred about the depth prediction), the benefits are twofold: reliable feature correspondence between views and fast convergence to the true depth in order to create new map points. We evaluate our method with two outdoor datasets: KITTI dataset and Oxford Robotcar dataset. The experimental results indicate that the improved SVO mapping results in increased robustness and camera tracking accuracy.\n\nI. INTRODUCTION\n\nVisual odometry (VO) and visual simultaneous localization and mapping (V-SLAM) have been actively researched and explored in the robotics field, including autonomous driving. As cameras become affordable and ubiquitous, being able to estimate camera poses reliably from an image sequence leads to important robotics applications, such as autonomous vehicle navigation.\n\nMatching features between the current frame and previous frames have been one of the most important steps in solving visual odometry (VO) and visual simultaneous localization and mapping (V-SLAM). There are two main feature matching methods: indirect method and direct method. Indirect methods [1]- [3] require feature extraction, feature description, and feature matching. These methods rely on matching the intermediate features (e.g., descriptors), and they perform poorly in images with weak gradients and textureless surfaces where descriptors are failed to be matched. Direct methods [4], [5], by contrast, do not need feature description, and they operate directly on pixel intensities; therefore, any arbitrary pixels (e.g., corners, edges, or the whole image) can be sampled and matched, resulting in reliable feature matching even in images with poor texture. However, matching pixels directly requires depths of the pixels to be recovered, and such matching is defined in the direct formulation that jointly optimizes structure and motion.   Each initialized map point has a mean depth (black dot) and an interval in which the corresponding feature should lie, as shown by the magenta line. Note that larger depth uncertainty can allow the erroneous match to happen (as illustrated in (a) where the depth filter could converge to the \"similar feature\" rather than the \"corresponding feature\"). Our improved map point initialization method (see (b)) has lower depth uncertainty for identifying the corresponding feature Interestingly, semi-direct visual odometry (SVO) [6] is a hybrid method that combines the strength of direct and indirect methods for solving structure and motion, offering an efficient probabilistic mapping method to provide reliable map points for direct camera motion estimation. Unfortunately, one main limitation in SVO is that the map point is initialized with large depth uncertainty. Fig. 1(a) shows that the initialization of a map point with large depth uncertainty by SVO can lead to erroneous feature correspondence due to the large search range along the epipolar line.\n\nIn this paper, we propose to initialize new map points with depth prior from a single-image depth prediction neural network [7] (i.e., small variance centred about the pre-Near Far Single-image depth prediction The single-image depth prediction model demonstrates the illumination invariance property in estimating depth maps, and the colour-coded reprojected map points on a sample sequence of five consecutive frames show the reprojected map points to those frames for camera motion estimation (best viewed in colour). Note that CNN-SVO only predicts depth maps for the keyframes. (Right) Camera trajectory (depicted with line) and map points in magenta generated by CNN-SVO 1 dicted depth), such that the uncertainty for identifying the corresponding features is vastly reduced (see Fig. 1(b)). Because of the robust feature correspondence and small depth uncertainty, the map point is likely to converge to its true depth quickly. Overall, the improved SVO mapping, we refer to as CNN-SVO, is able to handle challenging lighting condition, thanks to the illumination invariance property in estimating depth maps (see Fig. 2).\n\n\nII. METHODS\n\nIn this section, we briefly cover the working principle of SVO in Section II-A for the sake of completeness. Next, we detail our improved initialization of the map points in Section II-B.\n\n\nA. Review of the SVO algorithm\n\nFirst, we explain the terminology used in the rest of the paper. A feature is one of the 2D points in the image extracted from FAST corner detector [8]. To perform feature correspondence, a small image patch that is centred at the feature location is used to find its corresponding patch in the nearby view; therefore, we refer to feature correspondence as matching small image patches. A map point is a 3D point projected from a feature location whose depth is known.\n\nSVO [6] contains two threads running in parallel: tracking thread and mapping thread. In the tracking thread, the camera pose of a new frame is obtained by minimizing the photometric residuals between the reference image patches (from which the map points are back-projected) and the 1 https://github.com/yan99033/CNN-SVO image patches that are centred at the reprojected locations in the new frame. The optimization steps for obtaining camera pose through the inverse compositional formulation [9] can be found in [6]. Concurrently, the mapping thread creates new map points using two processes: initialization of new map points with large depth uncertainty and update of depth uncertainty of the map points with depth-filters; consequently, a new map point is inserted in the map if the depth uncertainty of the map point is small.\n\nGiven the camera poses of two frames one of which is the keyframe, the depth of a feature can be obtained using the following two steps: finding the feature correspondence along the epipolar line in the non-keyframe, and then recover the depth via triangulation. Since the occurrence of outlier matching is inevitable, a depth-filter is modeled as a two-dimensional distribution [10], [11]: the first dimension describes the probability distribution of the depth, and the second dimension models the inlier probability. Therefore, given a set of depth measurements, a depth-filter approximates the mean depth and the variance (the first dimension) of the feature and separates the outliers from the inliers (the second dimension). The depth uncertainty (i.e., approximated variance) of the feature is updated when there is a new depth measurement, and the depth-filter is considered to have converged if the updated depth uncertainty is small. Then, the converged depth-filters that contain the true depths are used to create new map points by back-projecting the points at those feature locations according to their true depth. In this paper, we are focusing on improving the mapping in SVO [6]; therefore, we assume that the poses of the images can be successfully recovered in the tracking thread.\n\n\nB. Improved initialization of map points in SVO mapping\n\nThe effective implementation of depth-filter in SVO mapping and the use of direct matching of pixels has enabled SVO to achieve high frame rate camera motion estimation. However, SVO mapping initializes new map points in a reference keyframe with large uncertainty and their mean depths are set to the average scene depth in the reference frame. While such an initialization strategy is reasonable for the scene with one dominant plane-e.g., the floor planewhere the dominant depth information exists, the large depth uncertainty has limited the capability of the mapping to determine the true depths of the map points for the scene in which the depths of the map points vary considerably. Particularly, large depth uncertainty introduces two problems: possible erroneous feature correspondence along the epipolar line in the nearby frames and a high number of depth measurements to converge to the true depth.\n\nWith single-image depth prediction as the prior knowledge of the scene geometry, our CNN-SVO able to obtain a much better estimate of the mean and a smaller initial variance of the depth-filter than SVO to allow it to converge to the true depth of the map point. Fig. 3 illustrates the CNN-SVO pipeline, in which we add the CNN depth estimation module (marked in green) to provide strong depth priors in the map points initialization process when a keyframe is selectedthe initialization of depth-filters.\n\nGiven a set of triangulated depth measurements, the goal of using depth-filter is to separate the good measurements from the bad measurements: good measurements are normally distributed around the true depth, and bad measurements are uniformly distributed within an interval\n[\u03c1 min i , \u03c1 max i ].\nSpecifically given a set of triangulated inverse depth measurements \u03c1 1 i , \u03c1 2 i , \u00b7 \u00b7 \u00b7 , \u03c1 N i that correspond to the same feature, the measurement \u03c1 n i is modeled in SVO using a Gaussian + Uniform mixture model:\np(\u03c1 n i |\u03c1 i , \u03b3 i ) = \u03b3 i N (\u03c1 n i |\u03c1 i , \u03c4 2 i ) + (1 \u2212 \u03b3 i )U(\u03c1 n i |\u03c1 min i , \u03c1 max i ) (1) where \u03c1 i is the true inverse depth, \u03c4 2\ni the variance of the inverse depth, and \u03b3 i the inlier ratio. Assuming the inverse depth measurements \u03c1 1 i , \u03c1 2 i , \u00b7 \u00b7 \u00b7 , \u03c1 N i are independent, [10] shows that the approximation of the true inverse depth posterior can be computed incrementally by the product of a Gaussian distribution for the depth and a Beta distribution for the inlier ratio:\nq(\u03c1 i , \u03b3 i |a n , b n , \u00b5 n , \u03c3 2 n ) = Beta(\u03b3 i |a n , b n )N (\u03c1 i |\u00b5 n , \u03c3 2 n )(2)\nwhere a n and b n are the parameters in the Beta distribution, and \u00b5 n and \u03c3 2 n the mean and variance of the Gaussian depth estimate. The incremental Bayesian update step for a n , b n , \u00b5 n , and \u03c3 2 n is described in detail in [10], [11]. Once \u03c3 2 n is lower than a threshold, the depth-filter is converged to the true depth.\n\nHence, each depth-filter is initialized with the following parameters: the mean of the inverse depth \u00b5 n , the variance of  Fig. 3: The CNN-SVO pipeline. Our work augments the SVO pipeline [6] with the CNN depth estimation module (marked in green) to improve the mapping in SVO the inverse depth \u03c3 2 n , and the inlier ratio a n and b n . Table I compares the initialization of the parameters between SVO and CNN-SVO. The key difference is that CNN-SVO initializes the mean and the variance of the feature using learned scene depth instead of using the average and minimum scene depths in the reference keyframe. We empirically found that setting the depth variance to 1 (6dCNN) 2 provides adequate room for noisy depth prediction to converge; we will be losing the absolute scale if the depth variance is large (e.g., replacing 6 with a higher number) by allowing more uncertainty in the measurement. Based on the initialized \u00b5 n and \u03c3 2 n , a depth interval [\u03c1 min i , \u03c1 max i ] can be defined by\n\u03c1 min i = \u00b5 n + \u03c3 2 n (3) \u03c1 max i = 0.00000001, if \u00b5 n \u2212 \u03c3 2 n < 0 (4a) \u00b5 n \u2212 \u03c3 2 n , otherwise(4b)\nso that the corresponding feature can be found in the limited search range along the epipolar line in the nearby view (see Fig. 1). By obtaining strong depth prior from the singleimage depth prediction network, the benefits are twofold: smaller uncertainty in identifying feature correspondence and faster map point convergence, as illustrated in Fig. 4.\n\n\nIII. EVALUATION\n\nWe compare our method against the state-of-the-art direct and indirect methods, namely direct sparse odometry (DSO) [5], semi-direct visual odometry (SVO) [6], and ORB-SLAM without loop closure [12]. We use the absolute trajectory error (ATE) as the performance metric that has been used in the aforementioned papers. In addition, we indicate with 'X' for methods that are unable to complete the sequence due to lost tracking in the middle of the sequence (see Section III-A).  To provide depth prediction in the initialization of map points in CNN-SVO, we adopt the Resnet50 variant of the encoder-decoder architecture from [7] that has already been trained on Cityscape dataset. Next, we fine-tune the network on stereo images in KITTI raw data excluding KITTI Odometry Sequence 00-10 using original settings in [7] for 50 epochs. To produce consistent structural information, even on overexposed or underexposed images, the brightness of the images has been randomly adjusted throughout the training, creating the effect of illumination variation. This consideration is useful for a neural network to handle high dynamic range (HDR) environments (see Fig. 2).\n1 d CNN \u03c3 2 n 1 (6d min ) 2 1 (6d CNN ) 2 SVO CNN-SVO (a) (b) (c) (a) (b) (c)\nTo design the system with real-time capability, we resize the images to 512 \u00d7 256 for depth map inference, and then we resize the depth map back to original shape for VO processing. While two separate threads have been designed to handle mapping and tracking, GPU is used to provide the depth maps for the keyframes. The hardware is an Intel i7 processor 2 with NVidia GeForce GTX Titan X graphics card.\n\nTo scale the depth prediction for other datasets, the scaled depth d current can be obtained by the inferred depth d trained multiplied by the ratio of current focal length f current to trained focal length f trained , that is:\nd current = f current f trained d trained (5)\nWe use eleven KITTI Odometry sequences and nine Oxford Robotcar sequences for performance benchmarking. As for the images, we use the left camera from KITTI binocular stereo setup and the centre camera of the Bumblebee XB3 trinocular stereo setup from Oxford Robotcar. Both of the image streams are captured using global shutter cameras. Note that the ground truth poses from Oxford Robotcar dataset are not reliable for evaluation [13], because of the poor and inconsistent GPS signals; we still use the ground truth for both quantitative and qualitative evaluation purposes. The frame rates are 10 frames per second (FPS) and 16 FPS for KITTI and Oxford Robotcar, respectively. To maintain the same aspect ratio that is used by the network input, the images in the Oxford Robotcar dataset have been cropped to 1248x376 throughout the evaluation process. We skip the first 200 frames for all the Oxford Robotcar sequences because of the extremely overexposed images at the beginning of the sequences. Since the network has not been trained on Oxford Robotcar dataset, we analyze the scale of the odometry relative to absolute scale for both datasets (see Section III-C).\n\nWe set the maximum and the minimum number of tracked features in a frame to 200 and 100, respectively. Regarding the depth-filter, we modify SVO to use 5 previous keyframes to increase the number of measurements in the depth-filters. We also enable bundle adjustment during the evaluation process.\n\n\nA. Accuracy evaluation\n\nThe ATEs of KITTI dataset and Oxford Robotcar dataset are collected with a median of 5 runs, and they are shown in Table II and Table III, respectively. Our system is able to track all the sequences except for KITTI Sequence 01, because of failure to match features accurately in the scene with repetitive structure. We also demonstrate that our competitors fail to track most of the Oxford Robotcar sequences, which contain severely overexposed images. While ORB-SLAM is able to track features in consistent lighting conditions, the vanished textural information in overexposed and underexposed images has resulted in failure to match feature in these HDR environments. The main reason of tracking failure in DSO is its inability of affine brightness modeling to handle severe brightness change in the sequences, and the problem has also been reported in stereo DSO [14]. Scale drift is also noticeable in the trajectories on KITTI dataset produced by DSO and ORB-SLAM (without loop closure). SVO is designed to perform well in a planar scene; therefore, it fails to identify corresponding features effectively in the outdoor  We attribute the robust tracking of CNN-SVO to its ability to match features in consecutive frames with additional depth information, even when the images are overexposed or underexposed (see Fig. 2). The qualitative comparison of the camera trajectories can be found in Fig. 5 for KITTI dataset and Robotcar dataset, respectively. In Fig. 5 (b), an S-like curve is produced by CNN-SVO near the end of trajectory in Sequence 2014-05-06-12-54-54, which is caused by a moving car in front of the camera. Since the network has not been trained on Oxford Robotcar sequences, the experimental results suggest generalization ability to the structurally similar scene.\n\n\nB. Runtime evaluation\n\nLocal BA (about 29 ms) and single-image depth prediction (about 37 ms) have been the most demanding processes in the pipeline, but both processes are only required when new keyframes are created. Despite the computational demand, we experimentally found that CNN-SVO runs faster at 16 FPS with Oxford Robotcar dataset than 10 FPS with KITTI dataset. This is due to the close distance between frames in high frame rate sequence, and hence lesser keyframes are selected relative to the total number of frames from  \n\n\nIV. CONCLUSION\n\nIn this paper, we have improved SVO mapping, called CNN-SVO, by initializing the map points with low uncertainty and the mean depth obtained from a single-image depth prediction neural network. The proposed method has two main advantages: (1) features can be matched effectively by limiting the search range along the epipolar line in nearby views, assuming the camera poses are known, and (2) the map points are initialized with lower depth uncertainty, therefore they are able to converge to their true depths faster. With the combination of single-image depth prediction and implementation of depth-filters, CNN-SVO can perform mapping and estimate camera motion reliably. Thanks to the illumination invariance property in the single-image depth prediction network, depth maps produced from overexposed or underexposed images can still be used to facilitate feature correspondence between views, overcoming a key limitation of the original SVO.\n\nNevertheless, there are still shortcomings we are planning to address in the future. First, the threshold of the map point uncertainty is increased to allow map points with larger uncertainty to be inserted for camera motion tracking. This is due to the limited observations of the corresponding features that can be found in the nearby frame as a consequence of limited frame rate. Hence, the increase in uncertainty threshold implicitly assumes accurate depth prediction from the single-image depth prediction network. Second, although the network is able to produce depth maps from overexposed images, it still could not produce useful depth map with blank image-i.e., completely overexposed image. Because blank images rarely occur in an extended period of time, we estimate the pose of the blank images with constant velocity model until new features can be extracted. Then local BA is applied to jointly correct the map points and camera poses. This problem can be mitigated using exposure compensation algorithm [15]. Lastly, we facilitate feature matching by limiting the search space of the corresponding feature along the epipolar line in nearby frames. This feature matching strategy does increase the tolerance of illumination change, but it does not solve the inherent problem of photometric constancy assumption in direct methods. Thus, incorporation of additional photometric calibration [16] can further improve the feature matching performance.\n\n1\nThe authors are with Department of Computing Science, University of Alberta, Canada {lsyan,jahaniam,hzhang}@ualberta.ca 2 The authors are with the Faculty of Engineering, Universiti Putra Malaysia, Malaysia {syamsiah,saihong}@upm.edu.my\n\nFig. 1 :\n1Proposed map point initialization strategy.\n\nFig. 2 :\n2CNN-SVO: Camera motion estimation in the HDR environment. (Left)\n\nFig. 4 :\n4The improved mapping strategy is able to provide faster convergence of the map points. The length of the magenta line represents the depth uncertainty. (a) initialization of the depth filters where SVO uses a large interval to model the uncertainty of each initial map point whereas CNN-SVO uses a short interval; (b) depth estimates of the map points by the depth-filters after three updates; (c) depth estimates of the map points by the depth-filters after five updates\n\nFig. 5 :\n5Qualitative comparison of camera trajectories produced by ORB-SLAM (without loop closure), DSO, and CNN-SVO. (a) KITTI Sequence 00 and 08; (b) Oxford Robotcar Sequence 2014-05-06-12-54-54 and 2014-06-25-16-22-15. SVO is not included in this figure because it is not able to complete the trajectory due to tracking and mapping failures\n\nTABLE I :\nIA comparison between SVO and CNN-SVO in the intialization of parameters. The parameters are defined by some prior knowledge of the scene, where d avg is the average scene depth in the reference keyframe, d CNN the depth prediction from the single-image depth prediction network, and d min the minimum scene depth in the reference keyframeSVO \nCNN-SVO \n\n\u00b5n \n\n1 \ndavg \n\n\n\nTABLE II :\nIIAbsolute keyframe trajectory RMSE (in metre) \non KITTI dataset \n\nSequence \nSVO \nCNN-SVO \nDSO \nORB-SLAM \n(w/o loop closure) \n00 \nX \n17.5269 \n113.1838 \n77.9502 \n01 \nX \nX \nX \nX \n02 \nX \n50.5119 \n116.8108 \n41.0064 \n03 \nX \n3.4588 \n1.3943 \n1.0182 \n04 \n58.3970 \n2.4414 \n0.422 \n0.9302 \n05 \nX \n8.1513 \n47.4605 \n40.3542 \n06 \nX \n11.5091 \n55.6173 \n52.2282 \n07 \nX \n6.5141 \n16.7192 \n16.546 \n08 \nX \n10.9755 \n111.0832 \n51.6215 \n09 \nX \n10.6873 \n52.2251 \n58.1742 \n10 \nX \n4.8354 \n11.090 \n18.4765 \n\nscene, where depths of the features can vary considerably. \n\n\nTABLE III :\nIIIAbsolute keyframe trajectory RMSE (in metre) on Oxford Robotcar dataset\n\nTABLE IV :\nIVScale relative to absolute scale in VO output from CNN-SVO (a) KITTI DatasetSequence \nScale \nSequence 00 \n0.9296 \nSequence 01 \nX \nSequence 02 \n0.921 \nSequence 03 \n1.0811 \nSequence 04 \n1.1876 \nSequence 05 \n0.9837 \nSequence 06 \n0.9602 \nSequence 07 \n1.0246 \nSequence 08 \n1.0014 \nSequence 09 \n1.043 \nSequence 10 \n1.0512 \n\n(b) \nOxford \nRobotcar \nDataset \n\nSequence \nScale \n2014-05-06-12-54-54 \n0.8953 \n2014-05-06-13-09-52 \n0.9321 \n2014-05-06-13-14-58 \n0.9172 \n2014-05-06-13-17-51 \n0.9399 \n2014-05-14-13-46-12 \n0.9103 \n2014-05-14-13-50-20 \n0.9737 \n2014-05-14-13-53-47 \n0.9427 \n2014-05-14-13-59-05 \n0.9473 \n2014-06-25-16-22-15 \n0.9236 \n\n\nIntel i7-4790K, 4 cores, 4.0GHz, 32GB RAM\nthe sequence. For this reason, real-time computation can be achieved.C. Scale evaluationSince the network is trained on rectified stereo images with known baseline, we examine the scale of the odometry based on predicted depth from the network. Table IV (a) shows that the scale of the odometry is close to absolute scale in KITTI dataset because the training images are mostly from KITTI dataset. For Oxford Robotcar dataset, we scale the depth predictions using Eq. 5, and the scale of the VO is between 0.9 and 0.97 (seeTable IV(b)). We offer two possible explanations for the inconsistent odometry scale. First, as mentioned in the Oxford Robotcar dataset documentation, the provided ground truth poses are not accurate, and the reasons are as follows: inconsistent GPS signals and scale drift in the large-scale map (see Section III in[13]). Second, the single-image depth prediction network has not been trained on the images in the Oxford Robotcar dataset, so the recovery of absolute scale cannot be guaranteed.\nReal-Time 6-DOF Monocular Visual SLAM in a Large-Scale Environment. H Lim, J Lim, H J Kim, Proc. IEEE International Conference on Robotics and Automation (ICRA'14). IEEE International Conference on Robotics and Automation (ICRA'14)Hong Kong, ChinaIEEEH. Lim, J. Lim, and H. J. Kim, \"Real-Time 6-DOF Monocular Visual SLAM in a Large-Scale Environment,\" in Proc. IEEE International Conference on Robotics and Automation (ICRA'14). Hong Kong, China: IEEE, May 2014, pp. 1532-1539.\n\nORB-SLAM: A Versatile and Accurate Monocular SLAM System. R Mur-Artal, J M M Montiel, J D Tardos, IEEE Trans. Robot. 315R. Mur-Artal, J. M. M. Montiel, and J. D. Tardos, \"ORB-SLAM: A Versatile and Accurate Monocular SLAM System,\" IEEE Trans. Robot., vol. 31, no. 5, pp. 1147-1163, 2015.\n\nParallel, Real-Time Monocular Visual Odometry. S Song, M Chandraker, C C Guest, Proc. IEEE International Conference on Robotics and Automation (ICRA'13). IEEE International Conference on Robotics and Automation (ICRA'13)Karlsruhe, GermanyIEEES. Song, M. Chandraker, and C. C. Guest, \"Parallel, Real-Time Monocular Visual Odometry,\" in Proc. IEEE International Conference on Robotics and Automation (ICRA'13). Karlsruhe, Germany: IEEE, May 2013, pp. 4698-4705.\n\nLSD-SLAM: Large-scale Direct Monocular SLAM. J Engel, T Schps, D Cremers, Proc. European Conference on Computer Vision (ECCV'14). European Conference on Computer Vision (ECCV'14)Zurich, SwitzerlandSpringerJ. Engel, T. Schps, and D. Cremers, \"LSD-SLAM: Large-scale Direct Monocular SLAM,\" in Proc. European Conference on Computer Vision (ECCV'14). Zurich, Switzerland: Springer, Sept. 2014, pp. 834-849.\n\nDirect Sparse Odometry. J Engel, V Koltun, D Cremers, IEEE Trans. Pattern Anal. Mach. Intell. 403J. Engel, V. Koltun, and D. Cremers, \"Direct Sparse Odometry,\" IEEE Trans. Pattern Anal. Mach. Intell., vol. 40, no. 3, pp. 611-625, 2018.\n\nSVO: Fast Semi-Direct Monocular Visual Odometry. C Forster, M Pizzoli, D Scaramuzza, Proc. IEEE International Conference on Robotics and Automation (ICRA'14). IEEE International Conference on Robotics and Automation (ICRA'14)Hong Kong, ChinaIEEEC. Forster, M. Pizzoli, and D. Scaramuzza, \"SVO: Fast Semi-Direct Monocular Visual Odometry,\" in Proc. IEEE International Conference on Robotics and Automation (ICRA'14). Hong Kong, China: IEEE, May 2014, pp. 15-22.\n\nUnsupervised Monocular Depth Sstimation with Left-Right Consistency. C Godard, O M Aodha, G J Brostow, Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR'17). IEEE Conference on Computer Vision and Pattern Recognition (CVPR'17)C. Godard, O. M. Aodha, and G. J. Brostow, \"Unsupervised Monoc- ular Depth Sstimation with Left-Right Consistency,\" in Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR'17).\n\n. Hawaii Honolulu, IEEEHonolulu, Hawaii: IEEE, July 2017.\n\nFaster and better: A machine learning approach to corner detection. E Rosten, R Porter, T Drummond, IEEE Trans. Pattern Anal. Mach. Intell. 321E. Rosten, R. Porter, and T. Drummond, \"Faster and better: A machine learning approach to corner detection,\" IEEE Trans. Pattern Anal. Mach. Intell., vol. 32, no. 1, pp. 105-119, 2010.\n\nLucas-kanade 20 years on: A unifying framework. S Baker, I Matthews, Int. J. Comput. Vis. 563S. Baker and I. Matthews, \"Lucas-kanade 20 years on: A unifying framework,\" Int. J. Comput. Vis., vol. 56, no. 3, pp. 221-255, 2004.\n\nSVO: Semi-Direct Visual Odometry for Monocular and Multicamera Systems. C Forster, Z Zhang, M Gassner, M Werlberger, D Scaramuzza, IEEE Trans. Robot. 332C. Forster, Z. Zhang, M. Gassner, M. Werlberger, and D. Scaramuzza, \"SVO: Semi-Direct Visual Odometry for Monocular and Multicamera Systems,\" IEEE Trans. Robot., vol. 33, no. 2, pp. 249-265, 2017.\n\nVideo-based, real-time multi-view stereo. G Vogiatzis, C Hernndez, Image Vis. Comput. 297G. Vogiatzis and C. Hernndez, \"Video-based, real-time multi-view stereo,\" Image Vis. Comput., vol. 29, no. 7, pp. 434 -441, 2011.\n\nORB-SLAM2: an Open-Source SLAM System for Monocular, Stereo and RGB-D Cameras. R Mur-Artal, J D Tardos, IEEE Trans. Robot. 335R. Mur-Artal and J. D. Tardos, \"ORB-SLAM2: an Open-Source SLAM System for Monocular, Stereo and RGB-D Cameras,\" IEEE Trans. Robot., vol. 33, no. 5, pp. 1255-1262, 2016.\n\n1 year, 1000 km: The Oxford RobotCar Dataset. W Maddern, G Pascoe, C Linegar, P Newman, Int. J. Robotics Res. 361W. Maddern, G. Pascoe, C. Linegar, and P. Newman, \"1 year, 1000 km: The Oxford RobotCar Dataset,\" Int. J. Robotics Res., vol. 36, no. 1, pp. 3-15, 2017.\n\nStereo DSO: Large-Scale Direct Sparse Visual Odometry with Stereo Cameras. R Wang, M Schwrer, D Cremers, Proc. IEEE Internation Conference on Computer Vision (ICCV'17). IEEE Internation Conference on Computer Vision (ICCV'17)Venice, ItalyR. Wang, M. Schwrer, and D. Cremers, \"Stereo DSO: Large-Scale Direct Sparse Visual Odometry with Stereo Cameras,\" in Proc. IEEE Internation Conference on Computer Vision (ICCV'17), Venice, Italy, Oct. 2017.\n\nActive exposure control for robust visual odometry in hdr environments. Z Zhang, C Forster, D Scaramuzza, Proc. IEEE International Conference on Robotics and Automation (ICRA'17). IEEE International Conference on Robotics and Automation (ICRA'17)Z. Zhang, C. Forster, and D. Scaramuzza, \"Active exposure control for robust visual odometry in hdr environments,\" in Proc. IEEE International Conference on Robotics and Automation (ICRA'17).\n\n. Marina Bay Sands, IEEESingaporeMarina Bay Sands, Singapore: IEEE, May 2017, pp. 3894-3901.\n\nOnline Photometric Calibration of Auto Exposure Video for Realtime Visual Odometry and SLAM. P Bergmann, R Wang, D Cremers, IEEE Robot. Autom. Lett. 32P. Bergmann, R. Wang, and D. Cremers, \"Online Photometric Cali- bration of Auto Exposure Video for Realtime Visual Odometry and SLAM,\" IEEE Robot. Autom. Lett., vol. 3, no. 2, pp. 627-634, 2018.\n", "annotations": {"author": "[{\"end\":125,\"start\":111},{\"end\":143,\"start\":126},{\"end\":162,\"start\":144},{\"end\":172,\"start\":163},{\"end\":178,\"start\":173},{\"end\":190,\"start\":179}]", "publisher": null, "author_last_name": "[{\"end\":124,\"start\":121},{\"end\":142,\"start\":137},{\"end\":161,\"start\":153},{\"end\":171,\"start\":168},{\"end\":177,\"start\":173},{\"end\":189,\"start\":184}]", "author_first_name": "[{\"end\":116,\"start\":111},{\"end\":120,\"start\":117},{\"end\":129,\"start\":126},{\"end\":136,\"start\":130},{\"end\":152,\"start\":144},{\"end\":167,\"start\":163},{\"end\":183,\"start\":179}]", "author_affiliation": null, "title": "[{\"end\":98,\"start\":1},{\"end\":288,\"start\":191}]", "venue": null, "abstract": "[{\"end\":1420,\"start\":300}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2106,\"start\":2103},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2111,\"start\":2108},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2402,\"start\":2399},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2407,\"start\":2404},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3391,\"start\":3388},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4050,\"start\":4047},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5441,\"start\":5438},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5767,\"start\":5764},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6258,\"start\":6255},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6278,\"start\":6275},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6978,\"start\":6974},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6984,\"start\":6980},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7790,\"start\":7787},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10179,\"start\":10175},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10698,\"start\":10694},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10704,\"start\":10700},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10986,\"start\":10983},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":12386,\"start\":12383},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":12425,\"start\":12422},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":12465,\"start\":12461},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":12895,\"start\":12892},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":13084,\"start\":13081},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":14623,\"start\":14619},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":16555,\"start\":16551},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":20003,\"start\":19999},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":20387,\"start\":20383}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":20681,\"start\":20442},{\"attributes\":{\"id\":\"fig_2\"},\"end\":20736,\"start\":20682},{\"attributes\":{\"id\":\"fig_3\"},\"end\":20812,\"start\":20737},{\"attributes\":{\"id\":\"fig_4\"},\"end\":21295,\"start\":20813},{\"attributes\":{\"id\":\"fig_5\"},\"end\":21641,\"start\":21296},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":22022,\"start\":21642},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":22575,\"start\":22023},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":22663,\"start\":22576},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":23308,\"start\":22664}]", "paragraph": "[{\"end\":1807,\"start\":1439},{\"end\":3921,\"start\":1809},{\"end\":5052,\"start\":3923},{\"end\":5255,\"start\":5068},{\"end\":5758,\"start\":5290},{\"end\":6593,\"start\":5760},{\"end\":7895,\"start\":6595},{\"end\":8865,\"start\":7955},{\"end\":9372,\"start\":8867},{\"end\":9648,\"start\":9374},{\"end\":9887,\"start\":9671},{\"end\":10376,\"start\":10025},{\"end\":10792,\"start\":10464},{\"end\":11792,\"start\":10794},{\"end\":12247,\"start\":11893},{\"end\":13429,\"start\":12267},{\"end\":13911,\"start\":13508},{\"end\":14140,\"start\":13913},{\"end\":15358,\"start\":14187},{\"end\":15657,\"start\":15360},{\"end\":17473,\"start\":15684},{\"end\":18012,\"start\":17499},{\"end\":18978,\"start\":18031},{\"end\":20441,\"start\":18980}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9670,\"start\":9649},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10024,\"start\":9888},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10463,\"start\":10377},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11892,\"start\":11793},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13507,\"start\":13430},{\"attributes\":{\"id\":\"formula_5\"},\"end\":14186,\"start\":14141}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":11140,\"start\":11133},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":15821,\"start\":15799}]", "section_header": "[{\"end\":1437,\"start\":1422},{\"end\":5066,\"start\":5055},{\"end\":5288,\"start\":5258},{\"end\":7953,\"start\":7898},{\"end\":12265,\"start\":12250},{\"end\":15682,\"start\":15660},{\"end\":17497,\"start\":17476},{\"end\":18029,\"start\":18015},{\"end\":20444,\"start\":20443},{\"end\":20691,\"start\":20683},{\"end\":20746,\"start\":20738},{\"end\":20822,\"start\":20814},{\"end\":21305,\"start\":21297},{\"end\":21652,\"start\":21643},{\"end\":22034,\"start\":22024},{\"end\":22588,\"start\":22577},{\"end\":22675,\"start\":22665}]", "table": "[{\"end\":22022,\"start\":21992},{\"end\":22575,\"start\":22037},{\"end\":23308,\"start\":22754}]", "figure_caption": "[{\"end\":20681,\"start\":20445},{\"end\":20736,\"start\":20693},{\"end\":20812,\"start\":20748},{\"end\":21295,\"start\":20824},{\"end\":21641,\"start\":21307},{\"end\":21992,\"start\":21654},{\"end\":22663,\"start\":22592},{\"end\":22754,\"start\":22678}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":3740,\"start\":3731},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":4718,\"start\":4709},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":5050,\"start\":5044},{\"end\":9136,\"start\":9130},{\"end\":10924,\"start\":10918},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12022,\"start\":12016},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":12246,\"start\":12240},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":13427,\"start\":13421},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":17010,\"start\":17004},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":17089,\"start\":17083},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":17157,\"start\":17147}]", "bib_author_first_name": "[{\"end\":24440,\"start\":24439},{\"end\":24447,\"start\":24446},{\"end\":24454,\"start\":24453},{\"end\":24456,\"start\":24455},{\"end\":24909,\"start\":24908},{\"end\":24922,\"start\":24921},{\"end\":24926,\"start\":24923},{\"end\":24937,\"start\":24936},{\"end\":24939,\"start\":24938},{\"end\":25186,\"start\":25185},{\"end\":25194,\"start\":25193},{\"end\":25208,\"start\":25207},{\"end\":25210,\"start\":25209},{\"end\":25645,\"start\":25644},{\"end\":25654,\"start\":25653},{\"end\":25663,\"start\":25662},{\"end\":26028,\"start\":26027},{\"end\":26037,\"start\":26036},{\"end\":26047,\"start\":26046},{\"end\":26290,\"start\":26289},{\"end\":26301,\"start\":26300},{\"end\":26312,\"start\":26311},{\"end\":26772,\"start\":26771},{\"end\":26782,\"start\":26781},{\"end\":26784,\"start\":26783},{\"end\":26793,\"start\":26792},{\"end\":26795,\"start\":26794},{\"end\":27153,\"start\":27147},{\"end\":27273,\"start\":27272},{\"end\":27283,\"start\":27282},{\"end\":27293,\"start\":27292},{\"end\":27582,\"start\":27581},{\"end\":27591,\"start\":27590},{\"end\":27833,\"start\":27832},{\"end\":27844,\"start\":27843},{\"end\":27853,\"start\":27852},{\"end\":27864,\"start\":27863},{\"end\":27878,\"start\":27877},{\"end\":28154,\"start\":28153},{\"end\":28167,\"start\":28166},{\"end\":28411,\"start\":28410},{\"end\":28424,\"start\":28423},{\"end\":28426,\"start\":28425},{\"end\":28674,\"start\":28673},{\"end\":28685,\"start\":28684},{\"end\":28695,\"start\":28694},{\"end\":28706,\"start\":28705},{\"end\":28970,\"start\":28969},{\"end\":28978,\"start\":28977},{\"end\":28989,\"start\":28988},{\"end\":29413,\"start\":29412},{\"end\":29422,\"start\":29421},{\"end\":29433,\"start\":29432},{\"end\":29787,\"start\":29781},{\"end\":29791,\"start\":29788},{\"end\":29967,\"start\":29966},{\"end\":29979,\"start\":29978},{\"end\":29987,\"start\":29986}]", "bib_author_last_name": "[{\"end\":24444,\"start\":24441},{\"end\":24451,\"start\":24448},{\"end\":24460,\"start\":24457},{\"end\":24919,\"start\":24910},{\"end\":24934,\"start\":24927},{\"end\":24946,\"start\":24940},{\"end\":25191,\"start\":25187},{\"end\":25205,\"start\":25195},{\"end\":25216,\"start\":25211},{\"end\":25651,\"start\":25646},{\"end\":25660,\"start\":25655},{\"end\":25671,\"start\":25664},{\"end\":26034,\"start\":26029},{\"end\":26044,\"start\":26038},{\"end\":26055,\"start\":26048},{\"end\":26298,\"start\":26291},{\"end\":26309,\"start\":26302},{\"end\":26323,\"start\":26313},{\"end\":26779,\"start\":26773},{\"end\":26790,\"start\":26785},{\"end\":26803,\"start\":26796},{\"end\":27162,\"start\":27154},{\"end\":27280,\"start\":27274},{\"end\":27290,\"start\":27284},{\"end\":27302,\"start\":27294},{\"end\":27588,\"start\":27583},{\"end\":27600,\"start\":27592},{\"end\":27841,\"start\":27834},{\"end\":27850,\"start\":27845},{\"end\":27861,\"start\":27854},{\"end\":27875,\"start\":27865},{\"end\":27889,\"start\":27879},{\"end\":28164,\"start\":28155},{\"end\":28176,\"start\":28168},{\"end\":28421,\"start\":28412},{\"end\":28433,\"start\":28427},{\"end\":28682,\"start\":28675},{\"end\":28692,\"start\":28686},{\"end\":28703,\"start\":28696},{\"end\":28713,\"start\":28707},{\"end\":28975,\"start\":28971},{\"end\":28986,\"start\":28979},{\"end\":28997,\"start\":28990},{\"end\":29419,\"start\":29414},{\"end\":29430,\"start\":29423},{\"end\":29444,\"start\":29434},{\"end\":29797,\"start\":29792},{\"end\":29976,\"start\":29968},{\"end\":29984,\"start\":29980},{\"end\":29995,\"start\":29988}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":16868165},\"end\":24848,\"start\":24371},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":206775100},\"end\":25136,\"start\":24850},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":13240528},\"end\":25597,\"start\":25138},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":14547347},\"end\":26001,\"start\":25599},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":3299195},\"end\":26238,\"start\":26003},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":206850490},\"end\":26700,\"start\":26240},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":206596513},\"end\":27143,\"start\":26702},{\"attributes\":{\"id\":\"b7\"},\"end\":27202,\"start\":27145},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":206764370},\"end\":27531,\"start\":27204},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":186689463},\"end\":27758,\"start\":27533},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":218417520},\"end\":28109,\"start\":27760},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":9352024},\"end\":28329,\"start\":28111},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":206775640},\"end\":28625,\"start\":28331},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":22556995},\"end\":28892,\"start\":28627},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":8515741},\"end\":29338,\"start\":28894},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":8524627},\"end\":29777,\"start\":29340},{\"attributes\":{\"id\":\"b16\"},\"end\":29871,\"start\":29779},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":3676655},\"end\":30218,\"start\":29873}]", "bib_title": "[{\"end\":24437,\"start\":24371},{\"end\":24906,\"start\":24850},{\"end\":25183,\"start\":25138},{\"end\":25642,\"start\":25599},{\"end\":26025,\"start\":26003},{\"end\":26287,\"start\":26240},{\"end\":26769,\"start\":26702},{\"end\":27270,\"start\":27204},{\"end\":27579,\"start\":27533},{\"end\":27830,\"start\":27760},{\"end\":28151,\"start\":28111},{\"end\":28408,\"start\":28331},{\"end\":28671,\"start\":28627},{\"end\":28967,\"start\":28894},{\"end\":29410,\"start\":29340},{\"end\":29964,\"start\":29873}]", "bib_author": "[{\"end\":24446,\"start\":24439},{\"end\":24453,\"start\":24446},{\"end\":24462,\"start\":24453},{\"end\":24921,\"start\":24908},{\"end\":24936,\"start\":24921},{\"end\":24948,\"start\":24936},{\"end\":25193,\"start\":25185},{\"end\":25207,\"start\":25193},{\"end\":25218,\"start\":25207},{\"end\":25653,\"start\":25644},{\"end\":25662,\"start\":25653},{\"end\":25673,\"start\":25662},{\"end\":26036,\"start\":26027},{\"end\":26046,\"start\":26036},{\"end\":26057,\"start\":26046},{\"end\":26300,\"start\":26289},{\"end\":26311,\"start\":26300},{\"end\":26325,\"start\":26311},{\"end\":26781,\"start\":26771},{\"end\":26792,\"start\":26781},{\"end\":26805,\"start\":26792},{\"end\":27164,\"start\":27147},{\"end\":27282,\"start\":27272},{\"end\":27292,\"start\":27282},{\"end\":27304,\"start\":27292},{\"end\":27590,\"start\":27581},{\"end\":27602,\"start\":27590},{\"end\":27843,\"start\":27832},{\"end\":27852,\"start\":27843},{\"end\":27863,\"start\":27852},{\"end\":27877,\"start\":27863},{\"end\":27891,\"start\":27877},{\"end\":28166,\"start\":28153},{\"end\":28178,\"start\":28166},{\"end\":28423,\"start\":28410},{\"end\":28435,\"start\":28423},{\"end\":28684,\"start\":28673},{\"end\":28694,\"start\":28684},{\"end\":28705,\"start\":28694},{\"end\":28715,\"start\":28705},{\"end\":28977,\"start\":28969},{\"end\":28988,\"start\":28977},{\"end\":28999,\"start\":28988},{\"end\":29421,\"start\":29412},{\"end\":29432,\"start\":29421},{\"end\":29446,\"start\":29432},{\"end\":29799,\"start\":29781},{\"end\":29978,\"start\":29966},{\"end\":29986,\"start\":29978},{\"end\":29997,\"start\":29986}]", "bib_venue": "[{\"end\":24618,\"start\":24536},{\"end\":25376,\"start\":25292},{\"end\":25796,\"start\":25729},{\"end\":26481,\"start\":26399},{\"end\":26949,\"start\":26881},{\"end\":29132,\"start\":29063},{\"end\":29586,\"start\":29520},{\"end\":24534,\"start\":24462},{\"end\":24965,\"start\":24948},{\"end\":25290,\"start\":25218},{\"end\":25727,\"start\":25673},{\"end\":26095,\"start\":26057},{\"end\":26397,\"start\":26325},{\"end\":26879,\"start\":26805},{\"end\":27342,\"start\":27304},{\"end\":27621,\"start\":27602},{\"end\":27908,\"start\":27891},{\"end\":28195,\"start\":28178},{\"end\":28452,\"start\":28435},{\"end\":28735,\"start\":28715},{\"end\":29061,\"start\":28999},{\"end\":29518,\"start\":29446},{\"end\":30020,\"start\":29997}]"}}}, "year": 2023, "month": 12, "day": 17}