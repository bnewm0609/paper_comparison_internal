{"id": 211677466, "updated": "2023-10-06 18:21:34.805", "metadata": {"title": "D3VO: Deep Depth, Deep Pose and Deep Uncertainty for Monocular Visual Odometry", "authors": "[{\"first\":\"Nan\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Lukas\",\"last\":\"Stumberg\",\"middle\":[\"von\"]},{\"first\":\"Rui\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Daniel\",\"last\":\"Cremers\",\"middle\":[]}]", "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2020, "month": 3, "day": 2}, "abstract": "We propose D3VO as a novel framework for monocular visual odometry that exploits deep networks on three levels -- deep depth, pose and uncertainty estimation. We first propose a novel self-supervised monocular depth estimation network trained on stereo videos without any external supervision. In particular, it aligns the training image pairs into similar lighting condition with predictive brightness transformation parameters. Besides, we model the photometric uncertainties of pixels on the input images, which improves the depth estimation accuracy and provides a learned weighting function for the photometric residuals in direct (feature-less) visual odometry. Evaluation results show that the proposed network outperforms state-of-the-art self-supervised depth estimation networks. D3VO tightly incorporates the predicted depth, pose and uncertainty into a direct visual odometry method to boost both the front-end tracking as well as the back-end non-linear optimization. We evaluate D3VO in terms of monocular visual odometry on both the KITTI odometry benchmark and the EuRoC MAV dataset.The results show that D3VO outperforms state-of-the-art traditional monocular VO methods by a large margin. It also achieves comparable results to state-of-the-art stereo/LiDAR odometry on KITTI and to the state-of-the-art visual-inertial odometry on EuRoC MAV, while using only a single camera.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2003.01060", "mag": "3035056458", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/YangSWC20", "doi": "10.1109/cvpr42600.2020.00136"}}, "content": {"source": {"pdf_hash": "61bbec51fa32571925c17efe8a2c7da48473f419", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2003.01060v2.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2003.01060", "status": "GREEN"}}, "grobid": {"id": "bab42301863e85b1c838f2e3016d2f849f1bcd2f", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/61bbec51fa32571925c17efe8a2c7da48473f419.txt", "contents": "\nD3VO: Deep Depth, Deep Pose and Deep Uncertainty for Monocular Visual Odometry\n\n\nNan Yang \nTechnical University of Munich\n\n\nArtisense\n\nLukas Von Stumberg \nTechnical University of Munich\n\n\nArtisense\n\nRui Wang \nTechnical University of Munich\n\n\nArtisense\n\nDaniel Cremers \nTechnical University of Munich\n\n\nArtisense\n\nD3VO: Deep Depth, Deep Pose and Deep Uncertainty for Monocular Visual Odometry\n\nWe propose D3VO as a novel framework for monocular visual odometry that exploits deep networks on three levels -deep depth, pose and uncertainty estimation. We first propose a novel self-supervised monocular depth estimation network trained on stereo videos without any external supervision. In particular, it aligns the training image pairs into similar lighting condition with predictive brightness transformation parameters. Besides, we model the photometric uncertainties of pixels on the input images, which improves the depth estimation accuracy and provides a learned weighting function for the photometric residuals in direct (feature-less) visual odometry. Evaluation results show that the proposed network outperforms state-ofthe-art self-supervised depth estimation networks. D3VO tightly incorporates the predicted depth, pose and uncertainty into a direct visual odometry method to boost both the front-end tracking as well as the back-end non-linear optimization. We evaluate D3VO in terms of monocular visual odometry on both the KITTI odometry benchmark and the EuRoC MAV dataset. The results show that D3VO outperforms state-of-the-art traditional monocular VO methods by a large margin. It also achieves comparable results to state-of-the-art stereo/LiDAR odometry on KITTI and to the state-of-the-art visual-inertial odometry on Eu-RoC MAV, while using only a single camera.\n\nIntroduction\n\nDeep learning has swept most areas of computer vision -not only high-level tasks like object classification, detection and segmentation [30,39,58], but also low-level ones such as optical flow estimation [12,65] and interest point detection and description [11,13,79]. Yet, in the field of Simultaneously Localization And Mapping (SLAM) or Visual Odometry (VO) which estimates the relative camera poses from image sequences, traditional geometric-based approaches [16,17,53] still dominate the field. While monocular methods [16,52] have the advantage of low hardware cost and less calibration effort, they cannot achieve\n\n\nFront-end Tracking\n\nBack-end Non-linear opt.\n\n\nFront-end Tracking\n\nBack-end Non-linear opt.\n\n\nFront-end Tracking\n\nBack-end Non-linear opt.\n\n\nFront-end Tracking\n\nBack-end Non-linear opt.\n\n\nFront-end Tracking\n\nBack-end Non-linear opt. ) and Deep uncertainty (\u03a3) estimation. D3VO integrates the three estimations tightly into both the front-end tracking and the back-end non-linear optimization of a sparse direct odometry framework [16].\n\n\nEuRoC MAV V2_03_difficult\n\ncompetitive performance compared to stereo [53,74] or visual-inertial odometry (VIO) [44,54,56,72], due to the scale drift [62,77] and low robustness. Recently, there have been many efforts to address this by leveraging deep neural networks [48,68,80,83]. It has been shown that with deep monocular depth estimation networks [26,27,43,78], the performance of monocular VO is boosted, since deep networks are able to estimate depth maps with consistent metric scale by learning a-priori knowledge from a large amount of data [42].\n\nIn this way, however, deep neural networks are only used to a limited degree. Recent advances of self-and unsupervised monocular depth estimation networks [26,86] show that the poses of the adjacent monocular frames can be predicted together with the depth. Since the pose estimation from deep neural networks shows high robustness, one question arises: Can the deep-predicted poses be employed to boost traditional VO? On the other hand, since SLAM/VO is essentially a state estimation problem where uncertainty plays an important role [19,63,69] and meanwhile many learning based methods have started estimating uncertainties, the next question is, how can we incorporate such uncertainty-predictions into optimization-based VO?\n\nIn this paper, we propose D3VO as a framework for monocular direct (feature-less) visual VO that exploits selfsupervised monocular depth estimation network on three levels: deep depth, pose and uncertainty estimation, as shown in Fig. 1. To this end, we first propose a purely self-supervised network trained with stereo videos. The proposed self-supervised network predicts the depth from a single image with DepthNet and the pose between two adjacent frames with PoseNet. The two networks are bridged by minimizing the photometric error originated from both static stereo warping with the rectified baseline and temporal warping using the predicted pose. In this way, the temporal information is incorporated into the training of depth, which leads to more accurate estimation. To deal with the inconsistent illumination between the training image pairs, our network predicts the brightness transformation parameters which align the brightness of source and target images during training on the fly. The evaluation on the EuRoC MAV dataset shows that the proposed brightness transformation significantly improves the depth estimation accuracy. To integrate the deep depth into VO system, we firstly initialize every new 3D point with the predicted depth with a metric scale. Then we adopt the virtual stereo term proposed in Deep Virtual Stereo Odometry (DVSO) [78] to incorporate the predicted pose into the non-linear optimization. Unlike DVSO which uses a semi-supervised monocular depth estimation network relying on auxiliary depth extracted from state-of-the-art stereo VO system [74], our network uses only stereo videos without any external depth supervision.\n\nAlthough the illumination change is explicitly modeled, it is not the only factor which may violate the brightness constancy assumption [40]. Other factors, e.g., non-Lambertian surfaces, high-frequency areas and moving objects, also corrupt it. Inspired by the recent research on aleatoric uncertainty by deep neural networks [35,40], the proposed network estimates the photometric uncertainty as predictive variance conditioned on the input image. As a result, the errors originated from pixels which are likely to violate the brightness constancy assumption are downweighted. The learned weights of the photometric residuals also drive us to the idea of incorporating it into direct VO -since both the self-supervised training scheme and the direct VO share a similar photometric objective, we propose to use the learned weights to replace the weighting function of the photometric residual in traditional direct VO which is empirically set [61] or only accounts for the intrinsic uncertainty of the specific algorithm itself [16,37].\n\nRobustness is one of the most important factors in designing VO algorithm. However, traditional monocular visual VO suffers from a lack of robustness when confronted with low textured areas or fast movement [72]. The typical solution is to introduce an inertial measurement unit (IMU). But this increases the calibration effort and, more importantly, at constant velocity, IMUs cannot deliver the metric scale in constant velocity [50]. We propose to increase the robustness of monocular VO by incorporating the estimated pose from the deep network into both the front-end tracking and the back-end non-linear optimization. For the frontend tracking, we replace the pose from the constant velocity motion model with the estimated pose from the network. Besides, the estimated pose is also used as a squared regularizer in addition to direct image alignment [66]. For the back-end non-linear optimization, we propose a pose energy term which is jointly minimized with the photometric energy term of direct VO.\n\nWe evaluate the proposed monocular depth estimation network and D3VO on both KITTI [25] and EuRoC MAV [5]. We achieve state-of-the-art performances on both monocular depth estimation and camera tracking. In particular, by incorporating deep depth, deep uncertainty and deep pose, D3VO achieves comparable results to state-ofthe-art stereo/LiDAR methods on KITTI Odometry, and also comparable results to the state-of-the-art VIO methods on EuRoC MAV, while being a monocular method.\n\n\nRelated Work\n\nDeep learning for monocular depth estimation. Supervised learning [15,43,45] shows great performance on monocular depth estimation. Eigen et al. [14,15] propose to use multi-scale CNNs which directly regresses the pixel-wise depth map from a single input image. Laina et al. [43] propose a robust loss function to improve the estimation accuracy. Fu et al. [24] recast the monocular depth estimation network as an ordinal regression problem and achieve superior performance. More recent works start to tackle the problem in a self-and unsupervised way by learning the depth map using the photometric error [27,28,49,73,81,82,86] and adopting differentiable interpolation [32]. Our self-supervised depth estimation network builds upon MonoDepth2 [26] and extends it by predicting the brightness transformation parameters and the photometric uncertainty.\n\nDeep learning for uncertainty estimation. The uncertainty estimation of deep learning has recently been investigated in [35,36] where two types of uncertainties are proposed. Klodt et al. [40] propose to leverage the concept of aleatoric uncertainty to estimate the photometric and the depth uncertainties in order to improve the depth estimation accuracy. However, when formulating the photometric uncertainty, they do not consider brightness changes across different images which in fact can be modeled explicitly. Our method predicts the photometric uncertainty conditioned on the brightness-aligned image, which can deliver better photometric uncertainty estimation. Besides, we also seek to make better use of our learned uncertainties and propose to incorporate them into traditional VO systems [16].\n\nDeep learning for VO / SLAM. End-to-end learned deep neural networks have been explored to directly predict the relative poses between images with supervised [70,75,85] or unsupervised learning [46,73,82,86]. Besides pose estimation, CodeSLAM [2] delivers dense reconstruction by jointly optimizing the learned prior of the dense geometry together with camera poses. However, in terms of pose estimation accuracy all these end-to-end methods are inferior to classical stereo or visual inertial based VO methods. Building on the success of deep monocular depth estimation, several works integrate the predicted depth/disparity map into monocular VO systems [68,78] to improve performance and eliminate the scale drift. CNN-SLAM [68] fuses the depth predicted by a supervised deep neural network into LSD-SLAM [17] and the depth maps are refined with Bayesian filtering, achieving superior performance in indoor environments [29,64]. Other works [10,67] explore the application of deep neural networks on feature based methods ,and [34] uses Generative Adversarial Networks (GANs) as an image enhancement method to improve the robustness of VO in low light. The most related work to ours is Deep Virtual Stereo Odometry (DVSO). DVSO proposes a virtual stereo term that incooperates the depth estimation from a semi-supervised network into a direct VO pipeline. In particular, DVSO outperforms other monocular VO systems by a large margin, and even achieves comparable performance to state-of-the-art stereo visual odometry systems [53,74]. While DVSO merely leverages the depth, the proposed D3VO exploits the power of deep networks on multiple levels thereby incorporating more information into the direct VO pipeline.\n\n\nMethod\n\nWe first introduce a novel self-supervised neural network that predicts depth, pose and uncertainty. The network also estimates affine brightness transformation parameters to align the illumination of the training images in a self-supervised manner. The photometric uncertainty is predicted based on a distribution over the possible brightness values [35,40] for each pixel. Thereafter we introduce D3VO as a direct visual odometry framework that incorporates the predicted properties into both the tracking frontend and the photometric bundle adjustment backend.\n\n\nSelf-supervised Network\n\nThe core concept of the proposed monocular depth estimation network is the self-supervised training scheme which simultaneously learns depth with DepthNet and motion with PoseNet using video sequences [26,86]. The selfsupervised training is realized by minimizing the minimum of the photometric re-projection errors between the temporal and static stereo images:\nL self = 1 |V | p\u2208V min t r(I t , I t \u2192t ).(1)\nwhere V is the set of all pixels on I t and t is the index of all source frames. In our setting I t is the left image and I t contains its two adjacent temporal frames and its opposite (right) frame, i.e., I t \u2208 {I t\u22121 , I t+1 , I t s }. The perpixel minimum loss is proposed in Monodepth2 [26] in order to handle the occlusion among different source frames.\n\nTo simplify notation, we use I instead of I(p) in the remainder of this section. I t \u2192t is the sythesized I t by warping the temporal stereo images with the predicted depth D t , the camera pose T t t , the camera intrinsics K, and the differentialble bilinear sampler [32]. Note that for I t s \u2192t , the transformation T t s t is known and constant. DepthNet also predicts the depth map D t s of the right image I t s by feeding only the left image I t as proposed in [27]. The training of D t s requires to synthesize I t\u2192t s and compare with I t s . For simplicity, we will in the following only detail the loss \u2032 , \u2032 \u2032 Figure 3: Examples of affine brightness transformation on EuRoC MAV [5]. Originally the source image (I t ) and the target image (It) show different brightness. With the predicted parameters a, b, the transformed target images I a ,b have similar brightness as the source images, which facilitates the self-supervised training based on the brightness constancy assumption.\n\nregarding the left image.\n\nThe common practice [27] is to formulate the photometric error as\nr(I a , I b ) = \u03b1 2 (1 \u2212 SSIM(I a , I b )) + (1 \u2212 \u03b1)||I a \u2212 I b || 1 (2)\nbased on the brightness constancy assumption. However, it can be violated due to illumination changes and autoexposure of the camera to which both L1 and SSIM [76] are not invariant. Therefore, we propose to explicitly model the camera exposure change with predictive brightness transformation parameters.\n\nBrightness transformation parameters. The change of the image intensity due to the adjustment of camera exposure can be modeled as an affine transformation with two parameters a, b\nI a,b = aI + b.(3)\nDespite its simplicity, this formulation has been shown to be effective in direct VO/SLAM, e.g., [16,18,33,74], which builds upon the brightness constancy assumption as well. Inspired by these works, we propose predicting the transformation parameters a, b which align the brightness condition of I t with I t . We reformulate Eq. (1) as\nL self = 1 |V | p\u2208V min t r(I a t ,b t t , I t \u2192t ) (4) with I a t ,b t t = a t\u2192t I t + b t\u2192t ,(5)\nwhere a t\u2192t and b t\u2192t are the transformation parameters aligning the illumination of I t to I t . Note that both parameters can be trained in a self-supervised way without any supervisional signal. Fig. 3 shows the affine transformation examples from EuRoC MAV [5]. Photometric uncertainty. Only modeling affine brightness change is not enough to capture all failure cases of the brightness constancy assumption. Other cases like non-Lambertian surfaces and moving objects, are caused by the intrinsic properties of the corresponding objects which are not trivial to model analytically [40]. Since these aspects can be seen as observation noise, we leverage the concept of heteroscedastic aleatoric uncertainty of deep neural networks proposed by Kendall et al. [35]. The key idea is to predict a posterior probability distribution for each pixel parameterized with its mean as well as its variance p(y|\u1ef9, \u03c3) over ground-truth labels y. For instance, by assuming the noise is Laplacian, the negative log-likelihood to be minimized is\n\u2212 log p(y|\u1ef9, \u03c3) = |y \u2212\u1ef9| \u03c3 + log \u03c3 + const.(6)\nNote that no ground-truth label for \u03c3 is needed for training. The predictive uncertainty allows the network to adapt the weighting of the residual dependent on the data input, which improves the robustness of the model to noisy data or erroneous labels [35]. In our case where the \"ground-truth\" y are the pixel intensities on the target images, the network will predict higher \u03c3 for the pixel areas on I t where the brightness constancy assumption may be violated. Similar to [40], we implement this by converting Eq. (4) to\nL self = 1 |V | p\u2208V min t r(I a t ,b t t , I t \u2192t ) \u03a3 t + log \u03a3 t ,(7)\nwhere \u03a3 t is the uncertainty map of I t . Fig. 4 shows the qualitative results of the predicted uncertainty maps on KITTI [25] and EuRoC [5] datasets, respectively. In the next section, we will show that the learned \u03a3 t is useful for weighting the photometric residuals for D3VO. The total loss function is the summation of the selfsupervised losses and the regularization losses on multiscale images:\nL total = 1 s s (L s self + \u03bbL s reg ),(8)\nwhere s = 4 is the number of scales and L reg = L smooth + \u03b2L ab (9) with\nL ab = t (a t \u2212 1) 2 + b 2 t(10)\nis the regularizer of the brightness parameters and L smooth is the edge-aware smoothness on D t [27].\n\nTo summarize, the proposed DepthNet predicts D t , D t s and \u03a3 t with one single input I t . PoseNet predicts T t t , a t\u2192t and b t\u2192t with channel-wise concatenated (I t , I t ) as the input. Both DepthNet and PoseNet are convolutional networks following the widely used UNet-like architecture [59]. Please refer to our supplementary materials for network architecture and implementation details.\n\n\nD3VO\n\nIn the previous section, we introduced the selfsupervised depth estimation network which predicts the depth map D, the uncertainty map \u03a3 and the relative pose T t t . In this section, we will describe how D3VO integrates these predictions into a windowed sparse photometric bundle adjustment formulation as proposed in [16]. Note that in the following we use \u00b7 denoting the predictions from the network as D, \u03a3 and T t t to avoid ambiguity. Photometric energy. D3VO aims to minimize a total photometric error E photo defined as\nE photo = i\u2208F p\u2208Pi j\u2208obs(p) E pj ,(11)\nwhere F is the set of all keyframes, P i is the set of points hosted in keyframe i, obs(p) is the set of keyframes in which point p is observable and E pj is the weighted photometric energy term when p is projected onto keyframe j:\nE pj := p\u2208Np w p (I j [p ] \u2212 b j ) \u2212 e aj e ai (I i [p] \u2212 b i ) \u03b3 ,(12)\nwhere N is the set of 8 neighboring pixels of p defined in [16], a,b are the affine brightness parameters jointly estimated by non-linear optimization as in [16] and || \u00b7 || \u03b3 is the Huber norm. In [16], the residual is down-weighted when the pixels are with high image gradient to compensate small independent geometric noise [16]. In realistic scenarios, there are more sources of noise, e.g., reflection [40], that need to be modeled in order to deliver accurate and robust motion estimation. We propose to use the learned uncertainty \u03a3 to formulate the weighting function\nw p = \u03b1 2 \u03b1 2 + \u03a3(p) 2 2 ,(13)\nwhich may not only depend on local image gradient, but also on higher level noise pattern. As shown in Fig. 4, the proposed network is able to predict high uncertainty on the areas of reflectance, e.g., the windows of the vehicles, the moving object like the cyclist and the object boundaries where depth discontinuity occurs. The projected point position of p is given by p = \u03a0(T j i \u03a0 \u22121 (p, d p )), where d p is the depth of the point p in the coordinate system of keyframe i and \u03a0(\u00b7) is the projection function with the known camera intrinsics. Instead of randomly initializing d p as in traditional monocular direct methods [16,17], we initialize the point with\nd p = D i [p]\nwhich provides the metric scale. Inspired by [78], we introduce a virtual stereo term E \u2020 p to Eq. (11)\nE photo = i\u2208F p\u2208Pi \uf8eb \uf8ed \u03bbE \u2020 p + j\u2208obs(p) E pj \uf8f6 \uf8f8(14)\nwith\nE \u2020 p = w p I \u2020 i [p \u2020 ] \u2212 I i [p] \u03b3 ,(15)I \u2020 i [p \u2020 ] = I i [\u03a0(T s \u22121 \u03a0 \u22121 (p \u2020 , D i s [p \u2020 ]))](16)\nwith T s the transformation matrix from the left to the right image used for training DepthNet and p \u2020 = \u03a0(T s \u03a0 \u22121 (p, d p )).\n\nThe virtual stereo term optimizes the estimated depth d p from VO to be consistent with the depth predicted by the proposed deep network [78]. Pose energy.\n\nUnlike traditional direct VO approaches [19,23] which initialize the front-end tracking for each new frame with a constant velocity motion model, we leverage the predicted poses between consecutive frames to build a non-linear factor graph [41,47]. Specifically, we create a new factor graph whenever the newest keyframe, which is also the reference frame for the front-end tracking, is updated. Every new frame is tracked with respect to the reference keyframe with direct image alignment [66]. Additionally, the predicted relative pose from the deep network is used as a factor between the current frame and the last frame. After the optimization is finished, we marginalize the last frame and the factor graph will be used for the front-end tracking of the following frame. Please refer to our supp. materials for the visualization of the factor graph.\n\nThe pose estimated from the tracking front-end is then used to initialize the photometric bundle adjustment backend. We further introduce a prior for the relative keyframe pose T i i\u22121 using the predicted pose\nT i i\u22121 . Note that T i i\u22121\nis calculated by concatenating all the predicted frame-toframe poses between keyframe i \u2212 1 and i. Let\nE pose = i\u2208F \u2212{0} Log( T i i\u22121 T i\u22121 i ) \u03a3 \u22121 \u03be i i\u22121 Log( T i i\u22121 T i\u22121 i ),(18)\nwhere Log: SE(3) \u2192 R 6 maps from the transformation matrix T \u2208 R 4\u00d74 in the Lie group SE(3) to its corresponding twist coordinate \u03be \u2208 R 6 in the Lie algebra se(3). The diagonal inverse covariance matrix \u03a3 \u22121\n\u03be i i\u22121\nis obtained by propagating the covariance matrix between each consecutive frame pairs that is modeled as a constant diagonal matrix.\n\nThe total energy function is defined as\nE total = E photo + wE pose .(19)\nIncluding the pose prior term E pose in Eq. 19 can be considered as an analogy to integrating the pre-integrated IMU pose prior into the system with a Gaussian noise model. E total is minimized using the Gauss-Newton method. To summarize, we boost the direct VO method by introducing the predicted poses as initializations to both the tracking front-end and the optimization backend, as well as adding them as a regularizer to the energy function of the photometric bundle adjustment.\n\n\nExperiments\n\nWe evaluate the proposed self-supervised monocular depth estimation network as well as D3VO on both the KITTI [25] [15]. M: self-supervised monocular supervision; S: self-supervised stereo supervision; D: ground-truth depth supervison; D*: sparse auxiliary depth supervision. The upper part shows the comparison with the SOTA self-supervised network Monodepth2 [26] under the same setting and the ablation study of the brightness transformation parameters (ab) and the photometric uncertainty (uncer). The lower part shows the comparison with the SOTA semi-supervised methods using stereo as well as depth supervision. Our method outperforms Monodepth2 on all metrics and can also deliver comparable performance to the SOTA semi-supervised method DVSO [78] that additionally uses the depth from Stereo DSO [74] as sparse supervision signal.\n\n\nMonocular Depth Estimation\n\nKITTI. We train and evalutate the proposed selfsupervised depth estimation network on the split of Eigen at el. [15]. The network is trained on stereo sequences with the pre-processing proposed by Zhou et al. [86], which gives us 39,810 training quadruplets, each of which contains 3 (left) temporal images and 1 (right) stereo image, and 4,424 for validation. The upper part of Table 1 shows the comparison with Monodepth2 [26] which is the state-of-the-art method trained with stereo and monocular setting, and also the ablation study of the proposed brightness transformation prediction (ab) and the photometric uncertainty estimation (uncer). The results demonstrate that the proposed depth estimation network outperforms Monodepth2 on all metrics. The ablation studies unveil that the significant improvement over Monodepth2 comes largely with uncer, possibly because in KITTI there are many objects with non-Lambertian surfaces like windows and also objects that move independently such as cars and leaves which violate the brightness constancy assumption. The lower part of the table shows the comparison to the state-of-the-art semi-supervised methods and the results show that our method can achieve competitive performance without using any depth supervision.\n\nIn Figure 4 we show some qualitative results obtained from the Eigen test set [15]. From left to right, the original image, the depth maps and the uncertainty maps are shown respectively. For more qualitative results and the generalization capability on the Cityscapses dataset [8], please refer to our supp. materials.\n\nEuRoC MAV. The EuRoC MAV Dataset [5] is a dataset containing 11 sequences categorized as easy, medium and difficult according to the illumination and camera motion. This dataset is very challenging due to the strong motion and significant illumination changes both between stereo and temporal images. We therefore consider it as a nice test bench for validating the effectiveness of our predictive brightness transformation parameters for depth predic-KITTI EuRoC MAV Figure 4: Qualitative results from KITTI and EuRoC MAV. The original image, the predicted depth maps and the uncertainty maps are shown from the left to the right, respectively. In particular, the network is able to predict high uncertainty on object boundaries, moving objects, highly reflecting and high frequency areas.  tion. Inspired by Gordon et al. [28] who recently generated ground truth depth maps for the sequence V2 01 by projecting the provided Vicon 3D scans and filtering out occluded points, we also use this sequence for depth evaluations 1 . Our first experiment is set up to be consistent as in [28], for which we train models with the monocular setting on all MH sequences and test on V2 01 and show the results in Table 3.\n\n\nRMSE RMSE (log) ARD SRD\n\nIn the second experiment, we use 5 sequences MH 01, MH 02, MH 04, V1 01 and V1 02 as the training set to check the performance of our method in a relatively loosened setting. We remove the static frames for training and this results in 12,691 images of which 11,422 images are used for training and 1269 images are used for validation. We train our model with different ablations, as well as Mon-odepth2 [26] as the baseline. The results in Table 2 show that all our variations outperform the baseline and, in contrast to the case in KITTI, the proposed ab improves the results on this dataset significantly. Please refer to the supp. materials for more experiments on ab. In fact, it is worth noting that the results in Table 3 (trained on one scene MH and tested on another scene V) are worse than the ones in Table 2 (trained on both MH and V), which implies that it is still a challenge to improve the generalization capability of monocular depth estimation among very different scenarios.\n\n\nMonocular Visual Odometry\n\nWe evaluate the VO performance of D3VO on both KITTI Odometry and EuRoC MAV with the network trained on the splits described in the previous section.\n\nKITTI Odometry. The KITTI Odometry Benchmark contains 11 (0-10) sequences with provided ground-truth poses. As summarized in [78], sequences 00, 03, 04, 05, 07 are in the training set of the Eigen split that the proposed network uses, so we consider the rest of the sequences as the testing set for evaluating the pose estimation of D3VO. We use the relative translational (t rel ) error proposed in [25] as the main metric for evaluation. Table 4 shows the comparison with other state-of-the-art mono (M) as well as stereo (S) VO methods on the rest of the sequences. We refer to [78] for the results of the compared methods. Traditional monocular methods show high errors in the large-scale outdoor scene like the sequences in KITTI due to the scale drift. D3VO achieves the best performance on average, despite being a monocular methods as well. The table also contains the ablation study on the integration of deep depth (Dd), pose (Dp) and uncertainty (Du). It can be noticed that, consistent with the results in Table 1, the predicted uncertainty helps a lot on KITTI. We also submit the results on the testing sequences (11)(12)(13)(14)(15)(16)(17)(18)(19)(20) to the KITTI Odometry evaluation server (link). At the time of submission, D3VO outperforms DVSO and achieves the best monocular VO performance and comparable to other state-of-the-art LiDAR and stereo methods.\n\nWe further compare D3VO with state-of-the-art end-toend deep learning methods and other recent hybrid methods and show the results in Table 5. Note that here we only show the results on Seq.09 and 10, since most of the end-to-end methods only provide the results on these two sequences.  Table 4: Results on our test split of KITTI Odometry. The results of the SOTA monocular (M) methods are shown as baselines. The comparison with the SOTA stereo (S) methods shows that D3VO achieves better average performance than other methods, while being a monocular VO. We also show the ablation study for the integration of deep depth(Dd), pose(Dp) as well as uncertainty(Du).\n\n\nSeq. 09\n\nSeq. 10\n\nEnd-to-end  Table 5: Comparison to other hybrid methods as well as end-toend methods on Seq.09 and 10 of KITTI Odometry.\n\nWe refer to [28,78,83] for the results for the compared methods. D3VO achieves better performance than all the end-to-end methods by a notable margin. In general, hybrid methods which combine deep learning with traditional methods deliver better results than end-to-end methods.\n\nEuRoC MAV. As introduced in Sec. 4.1, EuRoC MAV is very challenging for purely vision-based VO due to the strong motion and significant illumination changes. VIO methods [44,56,71,72] dominate this benchmark by integrating IMU measurements to get a pose or motion prior and meanwhile estimating the absolute scale. We compare D3VO with other state-of-the-art monocular VIO (M+I) as well as stereo VIO (S+I) methods on sequences MH 03 medium, MH 05 difficult, V1 03 difficult, V2 02 medium and V2 03 difficult. All the other sequences are used for training. We refer to [9] for the results of the M+I methods. The results of DSO and ORB-SLAM are shown as baselines. We also show the results from the proposed PoseNet (End-end VO). For the evaluation metric, we use the root mean square (RMS) of the absolute trajectory error (ATE) after aligning the estimates with ground truth. The results in Table 6 show that with the proposed framework integrating depth, pose and uncertainty from the pro-  posed deep neural network, D3VO shows high accuracy as well as robustness and is able to deliver comparable results to other state-of-the-art VIO methods with only a single camera. We also show the ablation study for the integration of predicted depth (Dd), pose (Dp) and uncertainty (Du) and the integration of pose prediction improves the performance significantly on V1 03 difficult and V2 03 difficult where violent camera motion occurs. Figure 5 shows the qualitative comparison of trajectories obtained from DSO [16], ORB-SLAM [52], visual inertial DSO [72], the end-to-end predicted poses from our network and D3VO on the MH 03 and V1 03 sequences. All the 5 methods can deliver fairly good results on MH 05 difficult. On V1 03 difficult where the motions are stronger and there are many brightness inconsistencies between temporal and stereo images, D3VO can still deliver comparable results to VI-DSO, while using only a single camera.\n\n\nConclusion\n\nWe presented D3VO as a monocular VO method that enhances the performance of geometric VO methods by exploiting the predictive power of deep networks on three levels integrating predictions of monocular depth, photometric uncertainty and relative camera pose. To this end, we first introduced a novel self-supervised monocular depth estimation network which explicitly addresses the illumination change in the training set with predictive brightness transformation parameters. The network achieves state-ofthe-art results on KITTI and EuRoC MAV. The predicted depth, uncertainty and pose are then incorporated into both the front-end tracking and back-end non-linear optimization of a direct VO pipeline. We systematically evaluated the VO performance of D3VO on the two datasets. D3VO sets a new state-of-the-art on KITTI Odometry and also achieves state-of-the-art performance on the challenging EuRoC MAV, rivaling with leading mono-inertial and stereo-inertial methods while using only a single camera. Table 8: Network architecture of PoseNet. Except for the global average pooling layer (avg pool), all layers are convolutional layers with k the kernel size, s the stride, chns the channels and scale the downscaling factor relative to the input image.\n\n\nC. Factor Graph of Front-end Tracking\n\nIn Figure 6, we show the visualization of the factor graphs created for the front-end tracking in D3VO. The nonkeyframes are tracked with respect to the reference frame, which is the latest keyframe in the optimization window with direct image alignment. With the predicted relative poses from PoseNet, we also add a prior factor between the consecutive frames. When the new non-keyframe comes, the oldest non-keyframe in the factor graph is marginalized. The figure shows the status of the factor graph for the first (I t ), second (I t+1 ) and third non-keyframe (I t+2 ) comes.  k  e  y  f  r  a  m  e  p  o  s  e  n  o  n  -k  e  y  f  r  a  m  e  p  o  s  e  d  e  e  p  p  o  s  e  f  a  c  t o r Figure 6: Visualization of the factor graph created for the frontend tracking in D3VO. From left to right are the factor graph when the first (It), second (It+1) and third (It+2) frame comes after the newest keyframe, which is the reference frame for the front-end tracking, is added to the optimization window. The predicted relative poses from the proposed PoseNet is used as the prior between the consecutive frames.\n\navg photometric error w/o ab 0.10 w/ ab 0.03 w/ ab (LS) 0.07 \n\n\nD. Additional Experiments on Brightness Parameters\n\nIn our main paper, we have shown that the predictive brightness parameters effectively improve the depth estimation accuracy, especially on EuRoC MAV where the illumination change is quite strong. To further validate the correctness of the predicted brightness parameters, we measure the photometric errors when projecting the pixels from the source images to the next consecutive images using the ground-truth depth and poses in V2 03 difficult. An example of the ground-truth depth is shown in Figure 7 for which we use the code from the authors of [28]. We first calculate the photometric errors using the original image pairs and then calculate the absolute photometric errors by transforming the left images with the predicted parameters from PoseNet. We also implemented a simple baseline method to estimate the affine brightness parameters by solving linear least squares (LS). We formulated the normal equation with the dense optical flow method [20] implemented in OpenCV [4]. As shown in Table 9, the average photometric error is decreased by a large margin when the affine brightness transformation is performed and the predicted parameters from PoseNet are better than the ones estimated from LS. We show more examples of the affine brightness transformation in Figure 9.   (3) alignment, respectively. Note that ATE is very sensitive to the error occurs at one specific time [84].  \n\n\nE. Absolute Translational Error on KITTI\n\nThe evaluation metrics proposed with the KITTI benchmark [25] measures the relative pose accuracy. It is important to measure the global consistency of the pose estimations. Therefore, we also show the absolute translational error (ATE) as RMSE in Table 10 where the upper part shows the evaluation results without the SE(3) alignment and the lower part shows the results with the SE(3) alignment. For some sequences, e.g., KITTI 01, the ATE without SE(3) alignment is very large, while the ATE with SE(3) alignment dramatically decreases. The trajectories on KITTI 01 F. Cityscapes Figure 10 shows the results on the Cityscapes dataset [8] with our model trained on KITTI. The results show the generalization capability of our network on both depth and uncertainty prediction. In particular, the network can generalize to predict high uncertainties on reflectance, object boundaries, high-frequency areas, and moving objects. \n\nFigure 1 :\n1We propose D3VO -a novel monocular visual odometry (VO) framework which exploits deep neural networks on three levels: Deep depth (D), Deep pose (T t\u22121 t\n\nFigure 2 :\n2Examples of point clouds and trajectories delivered by D3VO on KITTI Odometry Seq. 00, EuRoC MH 05 difficult and V1 03 difficult. The insets on EuRoC show the scenarios with low illumination and motion blur which are among the main reasons causing failures of traditional purely vision-based VO systems.\n\nFigure 7 :\n7An example of the ground-truth depth map of V2 03 difficult in EuRoC MAV.\n\nFigure 8 :\n8Trajectories on KITTI 01 to compare between w/o and w/ SE(3) alignment for the ATE evaluation. The upper part of the figure shows the trajectories on the x-z plane and the lower part shows the trajectories on the x-y plane. We can see that less accurate pose estimations for the initial frames may result in a large overall ATE, if no SE(3) alignment is performed.\n\nFigure 9 :\n9Examples of affine brightness transformation in V2 03 difficult from EuRoC MAV.are shown inFigure.8 where we can see that the less accurate pose estimations for the initial frames may result in a large overall ATE.\n\nFigure 10 :\n10Results on Cityscapes with the model trained on KITTI.\n\n\nand the EuRoC MAV[5] datasets.RMSE RMSE (log) ARD \n\nSRD \n\u03b4 < 1.25 \u03b4 < 1.25 2 \u03b4 < 1.25 3 \nApproach \nTrain \nlower is better \nhigher is better \nMonoDepth2 [27] \nMS \n4.750 \n0.196 \n0.106 0.818 \n0.874 \n0.957 \n0.979 \nOurs, uncer \nMS \n4.532 \n0.190 \n0.101 0.772 \n0.884 \n0.956 \n0.978 \nOurs, ab \nMS \n4.650 \n0.193 \n0.105 0.791 \n0.878 \n0.957 \n0.979 \nOurs, full \nMS \n4.485 \n0.185 \n0.099 0.763 \n0.885 \n0.958 \n0.979 \n\nKuznietsov et al. [42] \nDS \n4.621 \n0.189 \n0.113 0.741 \n0.862 \n0.960 \n0.986 \nDVSO [78] \nD*S \n4.442 \n0.187 \n0.097 0.734 \n0.888 \n0.958 \n0.980 \nOurs \nMS \n4.485 \n0.185 \n0.099 0.763 \n0.885 \n0.958 \n0.979 \n\nTable 1: Depth evaluation results on the KITTI Eigen split \n\nTable 2 :\n2Evaluation results of V2 01 in EuRoC MAV[5]. The performance of monocular depth estimation is boosted largely by the proposed predictive brightness transformation parameters. Evaluation results of V2 01 in EuRoC MAV[5] with the model trained with all MH sequences.RMSE RMSE (log) ARD \nSRD \u03b4 < 1.25 \n[28] \n0.971 \n0.396 \n0.332 0.389 \n0.420 \nOurs \n0.943 \n0.391 \n0.330 0.375 \n0.438 \nTable 3: \n\nTable 6 :\n6Evaluation results on EuRoC MAV [5]. We show the re-\nsults of DSO and ORB-SLAM as baselines and compare D3VO \nwith other SOTA monocular VIO (M+I) and stereo VIO (S+I) \nmethods. Note that for stereo methods, V2 03 difficult is excluded \ndue to many missing images from one of the cameras [71]. Despite \nbeing a monocular method, D3VO shows comparable results to \nSOTA monocular/stereo VIO. The best results among the monoc-\nular methods are shown as black bold and the best among the \nstereo methods are shown as blue bold. The ablation study shows \nthat Dd+Dp delivers large improvement on V1 03 difficult and \nV2 03 difficult where the camera motions are very strong. \n\n\n\nTable 9 :\n9Average photometric errors on V2 03 difficult. We project the visible 3D points with ground-truth depth of the left images onto the corresponding right images fo the stereo pairs, and then calculate the absolute photometric errors. Note that the intensity values are normalized to [0, 1]. The results show that by transforming the left images with the predicted ab, the average photometric error is largely decreased.\n\n\n53] 21.4 15.0 3.52 11.1 6.34 5.25 10.4 S. DSO [74] 26.5 16.4 3.11 11.0 9.39 3.11 11.Table 10: Absolute translational error (ATE) as RMSE on KITTI. The upper part and the lower part show the results w/o and w/ SE01 \n02 \n06 \n08 \n09 \n10 mean \nORB2 [6 \nD3VO \n26.9 10.4 2.92 12.7 5.30 2.44 10.1 \n\nORB2 [53] 9.95 9.55 2.45 3.75 3.07 0.99 4.96 \nS. DSO [74] 5.08 7.82 1.93 3.02 4.31 0.84 3.83 \nD3VO \n1.73 5.43 1.69 3.53 2.68 0.87 2.65 \n\nWe thank the authors of[28] to provide the processing code.\nAcknowledgements We would like to thank Niclas Zeller, Lukas K\u00f6stler, Oleg Muratov and other colleagues from Artisense for their continuous feedbacks. Besides, we would like to thank Jakob Engel and Tao Wu for the fruitful discussions during the early stages of the project. Last but not least, we also would like to thank the reviewers and Klaus H. Strobl for their constructive comments.Supplementary A. Network Training DetailsBoth DepthNet and PoseNet are implemented with Py-Torch[55]and trained on a single Titan X Pascal GPU. We resize the images to 512 \u00d7 256 for both KITTI[25]and EuRoC MAV[5]. We use ResNet-18[31]as the encoder of DepthNet and it is initialized with ImageNet[60]pre-trained weights. Note that since EuRoC MAV provides grayscale images only, we duplicate the images to form 3channel inputs. The decoder of DepthNet and the entire PoseNet are initialized randomly. We use a batch size of 8 and the Adam optimizer[38]with the number of epochs 20 and 40 for KITTI and EuRoC MAV, respectively. The learning rate is set to 10 \u22124 initially and decreased to 10 \u22125 for the last 5 epochs.The predicted brightness transformation parameters are the same for the 3 channels of the input images. We mask out the over-exposure pixels when applying affine brightness transformation, since we found they negatively affect the estimation of the brightness parameters. Engel et al. also find similar issues in[18].For the total loss functionwe use s = 4 output scales with and \u03bb s = 10 \u22123 \u00d7 1 2 s\u22121 . For the regularization L reg = L smooth + \u03b2L ab(21)with(22)andwe set \u03b2 = 10 \u22122 .B. Network ArchitecturesDepthNet. We adopt ResNet-18[31]as the encoder of DepthNet with the implementation from the torchvision package in PyTorch[55]. The decoder architecture is built upon the implementation in[26]with skip connections from the encoder, while the difference is that our final outputs contain 3 channels including D t , D s t and \u03a3 t .Table 7shows the detailed architecture of DepthNet decoder.PoseNet. The architecture of PoseNet is similar to[86]without the explainability mask decoder. PoseNet takes 2 channel-wise concatenated images as the input and outputs the relative pose and the relative brightness parameters a and b. The predicted pose is parameterized with translation vector and Euler angles.\nUnsupervised scale-consistent depth and ego-motion learning from monocular video. Jia-Wang Bian, Zhichao Li, Naiyan Wang, Huangying Zhan, Chunhua Shen, Ming-Ming Cheng, Ian Reid, Thirty-third Conference on Neural Information Processing Systems (NeurIPS). Jia-Wang Bian, Zhichao Li, Naiyan Wang, Huangying Zhan, Chunhua Shen, Ming-Ming Cheng, and Ian Reid. Unsuper- vised scale-consistent depth and ego-motion learning from monocular video. In Thirty-third Conference on Neural In- formation Processing Systems (NeurIPS), 2019. 7\n\nMichael Bloesch, Jan Czarnowski, Ronald Clark, Stefan Leutenegger, Andrew J Davison, arXiv:1804.00874CodeSLAM-learning a compact, optimisable representation for dense visual SLAM. arXiv preprintMichael Bloesch, Jan Czarnowski, Ronald Clark, Stefan Leutenegger, and Andrew J Davison. CodeSLAM-learning a compact, optimisable representation for dense visual SLAM. arXiv preprint arXiv:1804.00874, 2018. 3\n\nRobust visual inertial odometry using a direct EKF-based approach. Michael Bloesch, Sammy Omari, Marco Hutter, Roland Siegwart, 2015 IEEE/RSJ international conference on intelligent robots and systems (IROS). IEEEMichael Bloesch, Sammy Omari, Marco Hutter, and Roland Siegwart. Robust visual inertial odometry using a direct EKF-based approach. In 2015 IEEE/RSJ international con- ference on intelligent robots and systems (IROS), pages 298- 304. IEEE, 2015. 8\n\nThe OpenCV Library. Dr. Dobb's Journal of Software Tools. G Bradski, 14G. Bradski. The OpenCV Library. Dr. Dobb's Journal of Software Tools, 2000. 14\n\nThe EuRoC micro aerial vehicle datasets. Michael Burri, Janosch Nikolic, Pascal Gohl, Thomas Schneider, Joern Rehder, Sammy Omari, Markus W Achtelik, Roland Siegwart, The International Journal of Robotics Research. 813Michael Burri, Janosch Nikolic, Pascal Gohl, Thomas Schneider, Joern Rehder, Sammy Omari, Markus W Achte- lik, and Roland Siegwart. The EuRoC micro aerial vehicle datasets. The International Journal of Robotics Research, 2016. 2, 4, 5, 6, 8, 13\n\nDepth prediction without the sensors: Leveraging structure for unsupervised learning from monocular videos. Vincent Casser, Soeren Pirk, Reza Mahjourian, Anelia Angelova, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence33Vincent Casser, Soeren Pirk, Reza Mahjourian, and Anelia Angelova. Depth prediction without the sensors: Leveraging structure for unsupervised learning from monocular videos. In Proceedings of the AAAI Conference on Artificial Intelli- gence, volume 33, pages 8001-8008, 2019. 7\n\nFast and accurate deep network learning by exponential linear units (elus). Djork-Arn\u00e9 Clevert, Thomas Unterthiner, Sepp Hochreiter, arXiv:1511.07289arXiv preprintDjork-Arn\u00e9 Clevert, Thomas Unterthiner, and Sepp Hochre- iter. Fast and accurate deep network learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015. 13\n\nThe Cityscapes dataset for semantic urban scene understanding. Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, Bernt Schiele, Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)615Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The Cityscapes dataset for semantic urban scene understanding. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 6, 15\n\nA benchmark comparison of monocular visual-inertial odometry algorithms for flying robots. Jeffrey Delmerico, Davide Scaramuzza, 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEEJeffrey Delmerico and Davide Scaramuzza. A benchmark comparison of monocular visual-inertial odometry algo- rithms for flying robots. In 2018 IEEE International Confer- ence on Robotics and Automation (ICRA), pages 2502-2509. IEEE, 2018. 7\n\nDaniel Detone, Tomasz Malisiewicz, Andrew Rabinovich, arXiv:1812.03245Self-improving visual odometry. arXiv preprintDaniel DeTone, Tomasz Malisiewicz, and Andrew Rabi- novich. Self-improving visual odometry. arXiv preprint arXiv:1812.03245, 2018. 3\n\nSuperPoint: Self-supervised interest point detection and description. Daniel Detone, Tomasz Malisiewicz, Andrew Rabinovich, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. the IEEE Conference on Computer Vision and Pattern Recognition WorkshopsDaniel DeTone, Tomasz Malisiewicz, and Andrew Rabi- novich. SuperPoint: Self-supervised interest point detection and description. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 224-236, 2018. 1\n\nFlowNet: Learning optical flow with convolutional networks. Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas, Vladimir Golkov, Patrick Van Der, Daniel Smagt, Thomas Cremers, Brox, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionAlexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas, Vladimir Golkov, Patrick van der Smagt, Daniel Cremers, and Thomas Brox. FlowNet: Learn- ing optical flow with convolutional networks. In Proceedings of the IEEE International Conference on Computer Vision, pages 2758-2766, 2015. 1\n\nD2-Net: A trainable CNN for joint detection and description of local features. Mihai Dusmanu, Ignacio Rocco, Tomas Pajdla, Marc Pollefeys, Josef Sivic, Akihiko Torii, Torsten Sattler, Mihai Dusmanu, Ignacio Rocco, Tomas Pajdla, Marc Polle- feys, Josef Sivic, Akihiko Torii, and Torsten Sattler. D2-Net: A trainable CNN for joint detection and description of local features. 2019. 1\n\nPredicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture. David Eigen, Rob Fergus, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionDavid Eigen and Rob Fergus. Predicting depth, surface nor- mals and semantic labels with a common multi-scale con- volutional architecture. In Proceedings of the IEEE Inter- national Conference on Computer Vision, pages 2650-2658, 2015. 2\n\nDepth map prediction from a single image using a multi-scale deep network. David Eigen, Christian Puhrsch, Rob Fergus, Advances in neural information processing systems. 26David Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from a single image using a multi-scale deep net- work. In Advances in neural information processing systems, pages 2366-2374, 2014. 2, 6\n\nDirect sparse odometry. Jakob Engel, Vladlen Koltun, Daniel Cremers, 7Jakob Engel, Vladlen Koltun, and Daniel Cremers. Direct sparse odometry. IEEE transactions on pattern analysis and machine intelligence, 2017. 1, 2, 3, 4, 5, 7, 8\n\nLSD-SLAM: Large-scale direct monocular SLAM. Jakob Engel, Thomas Sch\u00f6ps, Daniel Cremers, European Conference on Computer Vision. Springer15Jakob Engel, Thomas Sch\u00f6ps, and Daniel Cremers. LSD- SLAM: Large-scale direct monocular SLAM. In European Conference on Computer Vision, pages 834-849. Springer, 2014. 1, 3, 5\n\nLarge-scale direct SLAM with stereo cameras. Jakob Engel, J\u00f6rg St\u00fcckler, Daniel Cremers, Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on. IEEE413Jakob Engel, J\u00f6rg St\u00fcckler, and Daniel Cremers. Large-scale direct SLAM with stereo cameras. In Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on, pages 1935-1942. IEEE, 2015. 4, 7, 13\n\nSemi-dense visual odometry for a monocular camera. J Engel, J Sturm, D Cremers, IEEE International Conference on Computer Vision (ICCV). 5J. Engel, J. Sturm, and D. Cremers. Semi-dense visual odom- etry for a monocular camera. In IEEE International Confer- ence on Computer Vision (ICCV), 2013. 2, 5\n\nTwo-frame motion estimation based on polynomial expansion. Gunnar Farneb\u00e4ck, Scandinavian conference on Image analysis. Springer14Gunnar Farneb\u00e4ck. Two-frame motion estimation based on polynomial expansion. In Scandinavian conference on Im- age analysis, pages 363-370. Springer, 2003. 14\n\nSGANVO: Unsupervised deep visual odometry and depth estimation with stacked generative adversarial networks. Tuo Feng, Dongbing Gu, IEEE Robotics and Automation Letters. 44Tuo Feng and Dongbing Gu. SGANVO: Unsupervised deep visual odometry and depth estimation with stacked genera- tive adversarial networks. IEEE Robotics and Automation Letters, 4(4):4431-4437, 2019. 7\n\nOn-manifold preintegration for real-time visual-inertial odometry. Christian Forster, Luca Carlone, Frank Dellaert, Davide Scaramuzza, IEEE Transactions on Robotics. 331Christian Forster, Luca Carlone, Frank Dellaert, and Da- vide Scaramuzza. On-manifold preintegration for real-time visual-inertial odometry. IEEE Transactions on Robotics, 33(1):1-21, 2016. 8\n\nSVO: Fast semi-direct monocular visual odometry. Christian Forster, Matia Pizzoli, Davide Scaramuzza, 2014 IEEE international conference on robotics and automation (ICRA). IEEEChristian Forster, Matia Pizzoli, and Davide Scaramuzza. SVO: Fast semi-direct monocular visual odometry. In 2014 IEEE international conference on robotics and automation (ICRA), pages 15-22. IEEE, 2014. 5\n\nDeep ordinal regression network for monocular depth estimation. Huan Fu, Mingming Gong, Chaohui Wang, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Salt Lake City; United StatesKayhan Batmanghelich, and Dacheng TaoHuan Fu, Mingming Gong, Chaohui Wang, Kayhan Bat- manghelich, and Dacheng Tao. Deep ordinal regression net- work for monocular depth estimation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Salt Lake City, United States, 2018. 2\n\nAre we ready for autonomous driving? the KITTI vision benchmark suite. Andreas Geiger, Philip Lenz, Raquel Urtasun, Conference on Computer Vision and Pattern Recognition (CVPR). 14Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the KITTI vision benchmark suite. In Conference on Computer Vision and Pattern Recog- nition (CVPR), 2012. 2, 4, 5, 7, 13, 14\n\nDigging into self-supervised monocular depth estimation. Cl\u00e9ment Godard, Oisin Mac Aodha, Michael Firman, Gabriel J Brostow, The IEEE International Conference on Computer Vision (ICCV). 713Cl\u00e9ment Godard, Oisin Mac Aodha, Michael Firman, and Gabriel J. Brostow. Digging into self-supervised monocular depth estimation. In The IEEE International Conference on Computer Vision (ICCV), October 2019. 1, 2, 3, 6, 7, 13\n\nUnsupervised monocular depth estimation with leftright consistency. Cl\u00e9ment Godard, Oisin Mac Aodha, Gabriel J Brostow, arXiv:1609.03677arXiv preprintCl\u00e9ment Godard, Oisin Mac Aodha, and Gabriel J Bros- tow. Unsupervised monocular depth estimation with left- right consistency. arXiv preprint arXiv:1609.03677, 2016. 1, 2, 3, 4, 6\n\nDepth from videos in the wild: Unsupervised monocular depth learning from unknown cameras. Ariel Gordon, Hanhan Li, Rico Jonschkowski, Anelia Angelova, The IEEE International Conference on Computer Vision (ICCV). 714Ariel Gordon, Hanhan Li, Rico Jonschkowski, and Anelia Angelova. Depth from videos in the wild: Unsupervised monocular depth learning from unknown cameras. In The IEEE International Conference on Computer Vision (ICCV), October 2019. 2, 6, 7, 14\n\nA benchmark for RGB-D visual odometry, 3D reconstruction and SLAM. Ankur Handa, Thomas Whelan, John Mcdonald, Andrew J Davison, Robotics and automation (ICRA), 2014 IEEE international conference on. IEEEAnkur Handa, Thomas Whelan, John McDonald, and An- drew J Davison. A benchmark for RGB-D visual odome- try, 3D reconstruction and SLAM. In Robotics and automa- tion (ICRA), 2014 IEEE international conference on, pages 1524-1531. IEEE, 2014. 3\n\nKaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, Ross Girshick Mask R-Cnn, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionKaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Gir- shick. Mask R-CNN. In Proceedings of the IEEE inter- national conference on computer vision, pages 2961-2969, 2017. 1\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016. 13\n\nSpatial transformer networks. Max Jaderberg, Karen Simonyan, Andrew Zisserman, Advances in neural information processing systems. 23Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. In Advances in neural infor- mation processing systems, pages 2017-2025, 2015. 2, 3\n\nReal-time feature tracking and outlier rejection with changes in illumination. Hailin Jin, Paolo Favaro, Stefano Soatto, Proceedings Eighth IEEE International Conference on Computer Vision. ICCV. Eighth IEEE International Conference on Computer Vision. ICCVIEEE1Hailin Jin, Paolo Favaro, and Stefano Soatto. Real-time fea- ture tracking and outlier rejection with changes in illumina- tion. In Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001, volume 1, pages 684-689. IEEE, 2001. 4\n\nMulti-Frame GAN: Image Enhancement for Stereo Visual Odometry in Low Light. E Jung, N Yang, D Cremers, Conference on Robot Learning (CoRL). E. Jung, N. Yang, and D. Cremers. Multi-Frame GAN: Im- age Enhancement for Stereo Visual Odometry in Low Light. In Conference on Robot Learning (CoRL), 2019. 3\n\nWhat uncertainties do we need in bayesian deep learning for computer vision?. Alex Kendall, Yarin Gal, Advances in neural information processing systems. 24Alex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for computer vision? In Advances in neural information processing systems, pages 5574-5584, 2017. 2, 3, 4\n\nMulti-task learning using uncertainty to weigh losses for scene geometry and semantics. Alex Kendall, Yarin Gal, Roberto Cipolla, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task learning using uncertainty to weigh losses for scene geome- try and semantics. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 2\n\nDense visual SLAM for RGB-D cameras. Christian Kerl, J\u00fcrgen Sturm, Daniel Cremers, 2013 IEEE/RSJ International Conference on Intelligent Robots and Systems. Christian Kerl, J\u00fcrgen Sturm, and Daniel Cremers. Dense visual SLAM for RGB-D cameras. In 2013 IEEE/RSJ In- ternational Conference on Intelligent Robots and Systems, pages 2100-2106. IEEE, 2013. 2\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980arXiv preprintDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 13\n\nPanoptic segmentation. Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, Piotr Doll\u00e1r, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionAlexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, and Piotr Doll\u00e1r. Panoptic segmentation. In Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9404-9413, 2019. 1\n\nSupervising the new with the old: learning SFM from SFM. Maria Klodt, Andrea Vedaldi, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Maria Klodt and Andrea Vedaldi. Supervising the new with the old: learning SFM from SFM. In Proceedings of the Eu- ropean Conference on Computer Vision (ECCV), pages 698- 713, 2018. 2, 3, 4, 5\n\nFactor graphs and the sum-product algorithm. Brendan J Frank R Kschischang, H-A Frey, Loeliger, IEEE Transactions on information theory. 472Frank R Kschischang, Brendan J Frey, and H-A Loeliger. Factor graphs and the sum-product algorithm. IEEE Trans- actions on information theory, 47(2):498-519, 2001. 5\n\nSemisupervised deep learning for monocular depth map prediction. Yevhen Kuznietsov, J\u00f6rg St\u00fcckler, Bastian Leibe, arXiv:1702.0270616arXiv preprintYevhen Kuznietsov, J\u00f6rg St\u00fcckler, and Bastian Leibe. Semi- supervised deep learning for monocular depth map predic- tion. arXiv preprint arXiv:1702.02706, 2017. 1, 6\n\nDeeper depth prediction with fully convolutional residual networks. Iro Laina, Christian Rupprecht, Vasileios Belagiannis, 3D Vision (3DV), 2016 Fourth International Conference on. IEEE1Federico Tombari, and Nassir NavabIro Laina, Christian Rupprecht, Vasileios Belagiannis, Fed- erico Tombari, and Nassir Navab. Deeper depth prediction with fully convolutional residual networks. In 3D Vision (3DV), 2016 Fourth International Conference on, pages 239- 248. IEEE, 2016. 1, 2\n\nKeyframe-based visual-inertial odometry using nonlinear optimization. Stefan Leutenegger, Simon Lynen, Michael Bosse, Roland Siegwart, Paul Furgale, The International Journal of Robotics Research. 343Stefan Leutenegger, Simon Lynen, Michael Bosse, Roland Siegwart, and Paul Furgale. Keyframe-based visual-inertial odometry using nonlinear optimization. The International Journal of Robotics Research, 34(3):314-334, 2015. 1, 7, 8\n\nAnton van den Hengel, and Mingyi He. Depth and surface normal estimation from monocular images using regression on deep features and hierarchical CRFs. Bo Li, Chunhua Shen, Yuchao Dai, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionBo Li, Chunhua Shen, Yuchao Dai, Anton van den Hengel, and Mingyi He. Depth and surface normal estimation from monocular images using regression on deep features and hi- erarchical CRFs. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1119- 1127, 2015. 2\n\nUnDeepVO: Monocular visual odometry through unsupervised deep learning. Ruihao Li, Sen Wang, Zhiqiang Long, Dongbing Gu, arXiv:1709.0684137arXiv preprintRuihao Li, Sen Wang, Zhiqiang Long, and Dongbing Gu. UnDeepVO: Monocular visual odometry through unsuper- vised deep learning. arXiv preprint arXiv:1709.06841, 2017. 3, 7\n\nAn introduction to factor graphs. H-A Loeliger, IEEE Signal Processing Magazine. 211H-A Loeliger. An introduction to factor graphs. IEEE Signal Processing Magazine, 21(1):28-41, 2004. 5\n\nCNN-SVO: Improving the mapping in semi-direct visual odometry using singleimage depth prediction. Yan Shing, Ali Jahani Loo, Syamsiah Amiri, Mashohor, Hong Sai, Hong Tang, Zhang, 2019 International Conference on Robotics and Automation (ICRA). IEEE17Shing Yan Loo, Ali Jahani Amiri, Syamsiah Mashohor, Sai Hong Tang, and Hong Zhang. CNN-SVO: Improving the mapping in semi-direct visual odometry using single- image depth prediction. In 2019 International Conference on Robotics and Automation (ICRA), pages 5218-5223. IEEE, 2019. 1, 7\n\nUnsupervised learning of depth and ego-motion from monocular video using 3d geometric constraints. R Mahjourian, M Wicke, A Angelova, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. R. Mahjourian, M. Wicke, and A. Angelova. Unsupervised learning of depth and ego-motion from monocular video us- ing 3d geometric constraints. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5667- 5675, June 2018. 2\n\nClosed-form solution of visual-inertial structure from motion. Agostino Martinelli, International journal of computer vision. 1062Agostino Martinelli. Closed-form solution of visual-inertial structure from motion. International journal of computer vision, 106(2):138-152, 2014. 2\n\nA multistate constraint Kalman filter for vision-aided inertial navigation. I Anastasios, Mourikis, Stergios I Roumeliotis, Proceedings 2007 IEEE International Conference on Robotics and Automation. 2007 IEEE International Conference on Robotics and AutomationIEEEAnastasios I Mourikis and Stergios I Roumeliotis. A multi- state constraint Kalman filter for vision-aided inertial navi- gation. In Proceedings 2007 IEEE International Conference on Robotics and Automation, pages 3565-3572. IEEE, 2007. 8\n\nORB-SLAM: a versatile and accurate monocular SLAM system. Raul Mur-Artal, Jose Maria Martinez Montiel, Juan D Tardos, IEEE Transactions on Robotics. 315Raul Mur-Artal, Jose Maria Martinez Montiel, and Juan D Tardos. ORB-SLAM: a versatile and accurate monocular SLAM system. IEEE Transactions on Robotics, 31(5):1147- 1163, 2015. 1, 7, 8\n\nORB-SLAM2: An open-source slam system for monocular, stereo, and rgb-d cameras. Raul Mur, -Artal , Juan D Tard\u00f3s, IEEE Transactions on Robotics. 33514Raul Mur-Artal and Juan D Tard\u00f3s. ORB-SLAM2: An open-source slam system for monocular, stereo, and rgb-d cameras. IEEE Transactions on Robotics, 33(5):1255-1262, 2017. 1, 3, 7, 14\n\nVisual-inertial monocular SLAM with map reuse. Ra\u00fal Mur, -Artal , Juan D Tard\u00f3s, IEEE Robotics and Automation Letters. 22Ra\u00fal Mur-Artal and Juan D Tard\u00f3s. Visual-inertial monocu- lar SLAM with map reuse. IEEE Robotics and Automation Letters, 2(2):796-803, 2017. 1, 8\n\nAutomatic differentiation in PyTorch. Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary Devito, Zeming Lin, Alban Desmaison, Luca Antiga, Adam Lerer, NIPS Autodiff Workshop. Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic dif- ferentiation in PyTorch. In NIPS Autodiff Workshop, 2017. 13\n\nVINS-Mono: A robust and versatile monocular visual-inertial state estimator. Peiliang Tong Qin, Shaojie Li, Shen, IEEE Transactions on Robotics. 3447Tong Qin, Peiliang Li, and Shaojie Shen. VINS-Mono: A robust and versatile monocular visual-inertial state estimator. IEEE Transactions on Robotics, 34(4):1004-1020, 2018. 1, 7\n\nA general optimization-based framework for local odometry estimation with multiple sensors. Jie Tong Qin, Shaozu Pan, Shaojie Cao, Shen, arXiv:1901.03638arXiv preprintTong Qin, Jie Pan, Shaozu Cao, and Shaojie Shen. A general optimization-based framework for local odometry estima- tion with multiple sensors. arXiv preprint arXiv:1901.03638, 2019. 8\n\nFaster R-CNN: Towards real-time object detection with region proposal networks. Kaiming Shaoqing Ren, Ross He, Jian Girshick, Sun, Advances in neural information processing systems. Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards real-time object detection with re- gion proposal networks. In Advances in neural information processing systems, pages 91-99, 2015. 1\n\nU-Net: Convolutional networks for biomedical image segmentation. Olaf Ronneberger, Philipp Fischer, Thomas Brox, International Conference on Medical image computing and computer-assisted intervention. SpringerOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U- Net: Convolutional networks for biomedical image segmen- tation. In International Conference on Medical image com- puting and computer-assisted intervention, pages 234-241. Springer, 2015. 4\n\nImagenet large scale visual recognition challenge. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, International journal of computer vision. 1153Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San- jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3):211-252, 2015. 13\n\nBAD SLAM: Bundle adjusted direct RGB-D SLAM. Thomas Schops, Torsten Sattler, Marc Pollefeys, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionThomas Schops, Torsten Sattler, and Marc Pollefeys. BAD SLAM: Bundle adjusted direct RGB-D SLAM. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 134-144, 2019. 2\n\nHauke Strasdat, Andrew J Montiel, Davison, Scale drift-aware large scale monocular SLAM. Robotics: Science and Systems VI. 21Hauke Strasdat, JMM Montiel, and Andrew J Davison. Scale drift-aware large scale monocular SLAM. Robotics: Science and Systems VI, 2, 2010. 1\n\nReal-time monocular SLAM: Why filter?. H Strasdat, J M M Montiel, A J Davison, 2010 IEEE International Conference on Robotics and Automation. H. Strasdat, J. M. M. Montiel, and A. J. Davison. Real-time monocular SLAM: Why filter? In 2010 IEEE International Conference on Robotics and Automation, pages 2657-2664, May 2010. 2\n\nA benchmark for the evaluation of RGB-D SLAM systems. J\u00fcrgen Sturm, Nikolas Engelhard, Felix Endres, Wolfram Burgard, Daniel Cremers, Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on. IEEEJ\u00fcrgen Sturm, Nikolas Engelhard, Felix Endres, Wolfram Burgard, and Daniel Cremers. A benchmark for the evalua- tion of RGB-D SLAM systems. In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pages 573-580. IEEE, 2012. 3\n\nPWC-Net: CNNs for optical flow using pyramid, warping, and cost volume. Deqing Sun, Xiaodong Yang, Ming-Yu Liu, Jan Kautz, 2018 IEEE Conference on Computer Vision and Pattern Recognition. Salt Lake City, UT, USADeqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz. PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 8934-8943, 2018. 1\n\nImage alignment and stitching: A tutorial. Foundations and Trends R in Computer Graphics and Vision. Richard Szeliski, 25Richard Szeliski. Image alignment and stitching: A tuto- rial. Foundations and Trends R in Computer Graphics and Vision, 2(1):1-104, 2006. 2, 5\n\nGCNv2: Efficient correspondence prediction for real-time slam. Jiexiong Tang, Ludvig Ericson, John Folkesson, Patric Jensfelt, IEEE Robotics and Automation Letters. 44Jiexiong Tang, Ludvig Ericson, John Folkesson, and Patric Jensfelt. GCNv2: Efficient correspondence prediction for real-time slam. IEEE Robotics and Automation Letters, 4(4):3505-3512, 2019. 3\n\nIro Laina, and Nassir Navab. CNN-SLAM: Real-time dense monocular SLAM with learned depth prediction. Keisuke Tateno, Federico Tombari, arXiv:1704.0348913arXiv preprintKeisuke Tateno, Federico Tombari, Iro Laina, and Nas- sir Navab. CNN-SLAM: Real-time dense monocular SLAM with learned depth prediction. arXiv preprint arXiv:1704.03489, 2017. 1, 3\n\nProbabilistic robotics. Sebastian Thrun, Wolfram Burgard, Dieter Fox, MIT pressSebastian Thrun, Wolfram Burgard, and Dieter Fox. Proba- bilistic robotics. MIT press, 2005. 2\n\nDeMon: Depth and motion network for learning monocular stereo. Benjamin Ummenhofer, Huizhong Zhou, Jonas Uhrig, Nikolaus Mayer, Eddy Ilg, Alexey Dosovitskiy, Thomas Brox, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionBenjamin Ummenhofer, Huizhong Zhou, Jonas Uhrig, Niko- laus Mayer, Eddy Ilg, Alexey Dosovitskiy, and Thomas Brox. DeMon: Depth and motion network for learning monocular stereo. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5038- 5047, 2017. 3\n\nVisual-inertial mapping with non-linear factor recovery. Vladyslav Usenko, Nikolaus Demmel, David Schubert, J\u00f6rg St\u00fcckler, Daniel Cremers, arXiv:1904.065047arXiv preprintVladyslav Usenko, Nikolaus Demmel, David Schubert, J\u00f6rg St\u00fcckler, and Daniel Cremers. Visual-inertial mapping with non-linear factor recovery. arXiv preprint arXiv:1904.06504, 2019. 7, 8\n\nDirect sparse visual-inertial odometry using dynamic marginalization. Vladyslav Lukas Von Stumberg, Daniel Usenko, Cremers, 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEELukas Von Stumberg, Vladyslav Usenko, and Daniel Cre- mers. Direct sparse visual-inertial odometry using dynamic marginalization. In 2018 IEEE International Conference on Robotics and Automation (ICRA), pages 2510-2517. IEEE, 2018. 1, 2, 7, 8\n\nLearning depth from monocular videos using direct methods. C Wang, J M Buenaposada, R Zhu, S Lucey, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. 23C. Wang, J. M. Buenaposada, R. Zhu, and S. Lucey. Learn- ing depth from monocular videos using direct methods. In 2018 IEEE/CVF Conference on Computer Vision and Pat- tern Recognition, pages 2022-2030, June 2018. 2, 3\n\nStereo DSO: Largescale direct sparse visual odometry with stereo cameras. R Wang, M Schw\u00f6rer, D Cremers, International Conference on Computer Vision (ICCV). Venice, Italy714R. Wang, M. Schw\u00f6rer, and D. Cremers. Stereo DSO: Large- scale direct sparse visual odometry with stereo cameras. In International Conference on Computer Vision (ICCV), Venice, Italy, October 2017. 1, 2, 3, 4, 6, 7, 14\n\nDeepVO: Towards end-to-end visual odometry with deep recurrent convolutional neural networks. Sen Wang, Ronald Clark, Hongkai Wen, Niki Trigoni, 2017 IEEE International Conference on. IEEEIn Robotics and Automation (ICRASen Wang, Ronald Clark, Hongkai Wen, and Niki Trigoni. DeepVO: Towards end-to-end visual odometry with deep re- current convolutional neural networks. In Robotics and Au- tomation (ICRA), 2017 IEEE International Conference on, pages 2043-2050. IEEE, 2017. 3\n\nImage quality assessment: from error visibility to structural similarity. Zhou Wang, Alan C Bovik, R Hamid, Eero P Sheikh, Simoncelli, IEEE transactions on image processing. 134Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si- moncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600-612, 2004. 4\n\nChallenges in monocular visual odometry: Photometric calibration, motion bias and rolling shutter effect. N Yang, R Wang, X Gao, D Cremers, IEEE Robotics and Automation Letters (RA-L). 3N. Yang, R. Wang, X. Gao, and D. Cremers. Challenges in monocular visual odometry: Photometric calibration, motion bias and rolling shutter effect. IEEE Robotics and Automa- tion Letters (RA-L), 3:2878-2885, Oct 2018. 1\n\nDeep virtual stereo odometry: Leveraging deep depth prediction for monocular direct sparse odometry. Nan Yang, Rui Wang, Jorg Stuckler, Daniel Cremers, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)67Nan Yang, Rui Wang, Jorg Stuckler, and Daniel Cremers. Deep virtual stereo odometry: Leveraging deep depth predic- tion for monocular direct sparse odometry. In Proceedings of the European Conference on Computer Vision (ECCV), pages 817-833, 2018. 1, 2, 3, 5, 6, 7\n\nLIFT: Learned invariant feature transform. Eduard Kwang Moo Yi, Vincent Trulls, Pascal Lepetit, Fua, European Conference on Computer Vision. SpringerKwang Moo Yi, Eduard Trulls, Vincent Lepetit, and Pascal Fua. LIFT: Learned invariant feature transform. In European Conference on Computer Vision, pages 467-483. Springer, 2016. 1\n\nScale recovery for monocular visual odometry using depth estimated with deep convolutional neural fields. Xiaochuan Yin, Xiangwei Wang, Xiaoguo Du, Qijun Chen, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition17Xiaochuan Yin, Xiangwei Wang, Xiaoguo Du, and Qijun Chen. Scale recovery for monocular visual odometry us- ing depth estimated with deep convolutional neural fields. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5870-5878, 2017. 1, 7\n\nGeoNet: Unsupervised learning of dense depth, optical flow and camera pose. Zhichao Yin, Jianping Shi, CVPR. Zhichao Yin and Jianping Shi. GeoNet: Unsupervised learn- ing of dense depth, optical flow and camera pose. In CVPR, 2018. 2\n\nUnsupervised learning of monocular depth estimation and visual odometry with deep feature reconstruction. H Zhan, R Garg, C S Weerasekera, K Li, H Agarwal, I M Reid, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. 7H. Zhan, R. Garg, C. S. Weerasekera, K. Li, H. Agarwal, and I. M. Reid. Unsupervised learning of monocular depth es- timation and visual odometry with deep feature reconstruc- tion. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 340-349, June 2018. 2, 3, 7\n\nHuangying Zhan, Chamara Saroj Weerasekera, Jiawang Bian, Ian Reid, arXiv:1909.09803Visual odometry revisited: What should be learnt. 17arXiv preprintHuangying Zhan, Chamara Saroj Weerasekera, Jiawang Bian, and Ian Reid. Visual odometry revisited: What should be learnt? arXiv preprint arXiv:1909.09803, 2019. 1, 7\n\nA tutorial on quantitative trajectory evaluation for visual (-inertial) odometry. Zichao Zhang, Davide Scaramuzza, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE14Zichao Zhang and Davide Scaramuzza. A tutorial on quan- titative trajectory evaluation for visual (-inertial) odometry. In 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 7244-7251. IEEE, 2018. 14\n\nDeepTAM: Deep tracking and mapping. Huizhong Zhou, Benjamin Ummenhofer, Thomas Brox, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Huizhong Zhou, Benjamin Ummenhofer, and Thomas Brox. DeepTAM: Deep tracking and mapping. In Proceedings of the European Conference on Computer Vision (ECCV), pages 822-838, 2018. 3\n\nUnsupervised learning of depth and ego-motion from video. Tinghui Zhou, Matthew Brown, Noah Snavely, David G Lowe, CVPR. 213Tinghui Zhou, Matthew Brown, Noah Snavely, and David G Lowe. Unsupervised learning of depth and ego-motion from video. In CVPR, volume 2, page 7, 2017. 1, 2, 3, 6, 7, 13\n", "annotations": {"author": "[{\"end\":135,\"start\":82},{\"end\":199,\"start\":136},{\"end\":253,\"start\":200},{\"end\":313,\"start\":254}]", "publisher": null, "author_last_name": "[{\"end\":90,\"start\":86},{\"end\":154,\"start\":142},{\"end\":208,\"start\":204},{\"end\":268,\"start\":261}]", "author_first_name": "[{\"end\":85,\"start\":82},{\"end\":141,\"start\":136},{\"end\":203,\"start\":200},{\"end\":260,\"start\":254}]", "author_affiliation": "[{\"end\":123,\"start\":92},{\"end\":134,\"start\":125},{\"end\":187,\"start\":156},{\"end\":198,\"start\":189},{\"end\":241,\"start\":210},{\"end\":252,\"start\":243},{\"end\":301,\"start\":270},{\"end\":312,\"start\":303}]", "title": "[{\"end\":79,\"start\":1},{\"end\":392,\"start\":314}]", "venue": null, "abstract": "[{\"end\":1787,\"start\":394}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b29\"},\"end\":1943,\"start\":1939},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":1946,\"start\":1943},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":1948,\"start\":1946},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2011,\"start\":2007},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":2014,\"start\":2011},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2064,\"start\":2060},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2067,\"start\":2064},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":2070,\"start\":2067},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2271,\"start\":2267},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2274,\"start\":2271},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":2277,\"start\":2274},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2332,\"start\":2328},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":2335,\"start\":2332},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2861,\"start\":2857},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":2939,\"start\":2935},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":2942,\"start\":2939},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":2981,\"start\":2977},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":2984,\"start\":2981},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":2987,\"start\":2984},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":2990,\"start\":2987},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":3019,\"start\":3015},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":3022,\"start\":3019},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":3137,\"start\":3133},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":3140,\"start\":3137},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":3143,\"start\":3140},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":3146,\"start\":3143},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3221,\"start\":3217},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3224,\"start\":3221},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":3227,\"start\":3224},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":3230,\"start\":3227},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":3420,\"start\":3416},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3582,\"start\":3578},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":3585,\"start\":3582},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3964,\"start\":3960},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":3967,\"start\":3964},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":3970,\"start\":3967},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":5522,\"start\":5518},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":5747,\"start\":5743},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":5966,\"start\":5962},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":6157,\"start\":6153},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":6160,\"start\":6157},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":6774,\"start\":6770},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6859,\"start\":6855},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":6862,\"start\":6859},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":7076,\"start\":7072},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":7300,\"start\":7296},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":7726,\"start\":7722},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7962,\"start\":7958},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7980,\"start\":7977},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8443,\"start\":8439},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":8446,\"start\":8443},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":8449,\"start\":8446},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8522,\"start\":8518},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8525,\"start\":8522},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":8652,\"start\":8648},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8734,\"start\":8730},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8983,\"start\":8979},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8986,\"start\":8983},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":8989,\"start\":8986},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":8992,\"start\":8989},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":8995,\"start\":8992},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":8998,\"start\":8995},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":9001,\"start\":8998},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9048,\"start\":9044},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9122,\"start\":9118},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9351,\"start\":9347},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9354,\"start\":9351},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":9419,\"start\":9415},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10032,\"start\":10028},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":10197,\"start\":10193},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":10200,\"start\":10197},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":10203,\"start\":10200},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":10233,\"start\":10229},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":10236,\"start\":10233},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":10239,\"start\":10236},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":10242,\"start\":10239},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10281,\"start\":10278},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":10695,\"start\":10691},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":10698,\"start\":10695},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":10766,\"start\":10762},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10847,\"start\":10843},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10962,\"start\":10958},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":10965,\"start\":10962},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":10983,\"start\":10979},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":10986,\"start\":10983},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":11069,\"start\":11065},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":11568,\"start\":11564},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":11571,\"start\":11568},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":12118,\"start\":12114},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":12121,\"start\":12118},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":12559,\"start\":12555},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":12562,\"start\":12559},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":13058,\"start\":13054},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":13397,\"start\":13393},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":13596,\"start\":13592},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":13817,\"start\":13814},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":14171,\"start\":14167},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":14449,\"start\":14445},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":14894,\"start\":14890},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":14897,\"start\":14894},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":14900,\"start\":14897},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":14903,\"start\":14900},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":15494,\"start\":15491},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":15820,\"start\":15816},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":15996,\"start\":15992},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":16568,\"start\":16564},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":16792,\"start\":16788},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":17034,\"start\":17030},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":17048,\"start\":17045},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":17421,\"start\":17418},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":17561,\"start\":17557},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":17862,\"start\":17858},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":18292,\"start\":18288},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":18903,\"start\":18899},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":19001,\"start\":18997},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":19042,\"start\":19038},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":19171,\"start\":19167},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":19251,\"start\":19247},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":20080,\"start\":20076},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":20083,\"start\":20080},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":20177,\"start\":20173},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":20664,\"start\":20660},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":20724,\"start\":20720},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":20727,\"start\":20724},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":20924,\"start\":20920},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":20927,\"start\":20924},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":21174,\"start\":21170},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":22998,\"start\":22994},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":23003,\"start\":22999},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":23249,\"start\":23245},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":23640,\"start\":23636},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":23694,\"start\":23690},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":23871,\"start\":23867},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":23968,\"start\":23964},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":24183,\"start\":24179},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":25108,\"start\":25104},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":25307,\"start\":25304},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":25383,\"start\":25380},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":26175,\"start\":26171},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":26433,\"start\":26429},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":26994,\"start\":26990},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":27889,\"start\":27885},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":28164,\"start\":28160},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":28345,\"start\":28341},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":28891,\"start\":28887},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":28895,\"start\":28891},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":28899,\"start\":28895},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":28903,\"start\":28899},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":28907,\"start\":28903},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":28911,\"start\":28907},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":28915,\"start\":28911},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":28919,\"start\":28915},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":28923,\"start\":28919},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":28927,\"start\":28923},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":29966,\"start\":29962},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":29969,\"start\":29966},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":29972,\"start\":29969},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":30404,\"start\":30400},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":30407,\"start\":30404},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":30410,\"start\":30407},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":30413,\"start\":30410},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":30802,\"start\":30799},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":31746,\"start\":31742},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":31761,\"start\":31757},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":31787,\"start\":31783},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":35277,\"start\":35273},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":35680,\"start\":35676},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":35706,\"start\":35703},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":36114,\"start\":36110},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":36223,\"start\":36219},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":36802,\"start\":36799},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":38359,\"start\":38356},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":39055,\"start\":39052},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":39230,\"start\":39227},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":40973,\"start\":40969}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":37256,\"start\":37090},{\"attributes\":{\"id\":\"fig_1\"},\"end\":37573,\"start\":37257},{\"attributes\":{\"id\":\"fig_2\"},\"end\":37660,\"start\":37574},{\"attributes\":{\"id\":\"fig_4\"},\"end\":38038,\"start\":37661},{\"attributes\":{\"id\":\"fig_5\"},\"end\":38266,\"start\":38039},{\"attributes\":{\"id\":\"fig_6\"},\"end\":38336,\"start\":38267},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":38999,\"start\":38337},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":39400,\"start\":39000},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":40084,\"start\":39401},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":40514,\"start\":40085},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":40945,\"start\":40515}]", "paragraph": "[{\"end\":2424,\"start\":1803},{\"end\":2471,\"start\":2447},{\"end\":2518,\"start\":2494},{\"end\":2565,\"start\":2541},{\"end\":2612,\"start\":2588},{\"end\":2862,\"start\":2635},{\"end\":3421,\"start\":2892},{\"end\":4153,\"start\":3423},{\"end\":5824,\"start\":4155},{\"end\":6863,\"start\":5826},{\"end\":7873,\"start\":6865},{\"end\":8356,\"start\":7875},{\"end\":9225,\"start\":8373},{\"end\":10033,\"start\":9227},{\"end\":11752,\"start\":10035},{\"end\":12326,\"start\":11763},{\"end\":12716,\"start\":12354},{\"end\":13122,\"start\":12764},{\"end\":14118,\"start\":13124},{\"end\":14145,\"start\":14120},{\"end\":14212,\"start\":14147},{\"end\":14591,\"start\":14286},{\"end\":14773,\"start\":14593},{\"end\":15130,\"start\":14793},{\"end\":16263,\"start\":15230},{\"end\":16836,\"start\":16311},{\"end\":17309,\"start\":16908},{\"end\":17426,\"start\":17353},{\"end\":17562,\"start\":17460},{\"end\":17960,\"start\":17564},{\"end\":18496,\"start\":17969},{\"end\":18767,\"start\":18536},{\"end\":19415,\"start\":18840},{\"end\":20113,\"start\":19447},{\"end\":20231,\"start\":20128},{\"end\":20290,\"start\":20286},{\"end\":20521,\"start\":20394},{\"end\":20678,\"start\":20523},{\"end\":21535,\"start\":20680},{\"end\":21746,\"start\":21537},{\"end\":21877,\"start\":21775},{\"end\":22167,\"start\":21960},{\"end\":22308,\"start\":22176},{\"end\":22349,\"start\":22310},{\"end\":22868,\"start\":22384},{\"end\":23724,\"start\":22884},{\"end\":25024,\"start\":23755},{\"end\":25345,\"start\":25026},{\"end\":26558,\"start\":25347},{\"end\":27579,\"start\":26586},{\"end\":27758,\"start\":27609},{\"end\":29138,\"start\":27760},{\"end\":29807,\"start\":29140},{\"end\":29826,\"start\":29819},{\"end\":29948,\"start\":29828},{\"end\":30228,\"start\":29950},{\"end\":32168,\"start\":30230},{\"end\":33440,\"start\":32183},{\"end\":34604,\"start\":33482},{\"end\":34667,\"start\":34606},{\"end\":36117,\"start\":34722},{\"end\":37089,\"start\":36162}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12763,\"start\":12717},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14285,\"start\":14213},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14792,\"start\":14774},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15229,\"start\":15131},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16310,\"start\":16264},{\"attributes\":{\"id\":\"formula_5\"},\"end\":16907,\"start\":16837},{\"attributes\":{\"id\":\"formula_6\"},\"end\":17352,\"start\":17310},{\"attributes\":{\"id\":\"formula_7\"},\"end\":17459,\"start\":17427},{\"attributes\":{\"id\":\"formula_8\"},\"end\":18535,\"start\":18497},{\"attributes\":{\"id\":\"formula_9\"},\"end\":18839,\"start\":18768},{\"attributes\":{\"id\":\"formula_10\"},\"end\":19446,\"start\":19416},{\"attributes\":{\"id\":\"formula_11\"},\"end\":20127,\"start\":20114},{\"attributes\":{\"id\":\"formula_12\"},\"end\":20285,\"start\":20232},{\"attributes\":{\"id\":\"formula_13\"},\"end\":20333,\"start\":20291},{\"attributes\":{\"id\":\"formula_14\"},\"end\":20393,\"start\":20333},{\"attributes\":{\"id\":\"formula_16\"},\"end\":21774,\"start\":21747},{\"attributes\":{\"id\":\"formula_17\"},\"end\":21959,\"start\":21878},{\"attributes\":{\"id\":\"formula_18\"},\"end\":22175,\"start\":22168},{\"attributes\":{\"id\":\"formula_19\"},\"end\":22383,\"start\":22350}]", "table_ref": "[{\"end\":24141,\"start\":24134},{\"end\":26557,\"start\":26550},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":27034,\"start\":27027},{\"end\":27314,\"start\":27307},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":27405,\"start\":27398},{\"end\":28207,\"start\":28200},{\"end\":28785,\"start\":28778},{\"end\":29281,\"start\":29274},{\"end\":29435,\"start\":29428},{\"end\":29847,\"start\":29840},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":31130,\"start\":31123},{\"end\":33196,\"start\":33189},{\"end\":34180,\"start\":34064},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":35727,\"start\":35720},{\"end\":36418,\"start\":36410}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1801,\"start\":1789},{\"end\":2445,\"start\":2427},{\"end\":2492,\"start\":2474},{\"end\":2539,\"start\":2521},{\"end\":2586,\"start\":2568},{\"end\":2633,\"start\":2615},{\"end\":2890,\"start\":2865},{\"attributes\":{\"n\":\"2.\"},\"end\":8371,\"start\":8359},{\"attributes\":{\"n\":\"3.\"},\"end\":11761,\"start\":11755},{\"attributes\":{\"n\":\"3.1.\"},\"end\":12352,\"start\":12329},{\"attributes\":{\"n\":\"3.2.\"},\"end\":17967,\"start\":17963},{\"attributes\":{\"n\":\"4.\"},\"end\":22882,\"start\":22871},{\"attributes\":{\"n\":\"4.1.\"},\"end\":23753,\"start\":23727},{\"end\":26584,\"start\":26561},{\"attributes\":{\"n\":\"4.2.\"},\"end\":27607,\"start\":27582},{\"end\":29817,\"start\":29810},{\"attributes\":{\"n\":\"5.\"},\"end\":32181,\"start\":32171},{\"end\":33480,\"start\":33443},{\"end\":34720,\"start\":34670},{\"end\":36160,\"start\":36120},{\"end\":37101,\"start\":37091},{\"end\":37268,\"start\":37258},{\"end\":37585,\"start\":37575},{\"end\":37672,\"start\":37662},{\"end\":38050,\"start\":38040},{\"end\":38279,\"start\":38268},{\"end\":39010,\"start\":39001},{\"end\":39411,\"start\":39402},{\"end\":40095,\"start\":40086}]", "table": "[{\"end\":38999,\"start\":38369},{\"end\":39400,\"start\":39276},{\"end\":40084,\"start\":39413},{\"end\":40945,\"start\":40728}]", "figure_caption": "[{\"end\":37256,\"start\":37103},{\"end\":37573,\"start\":37270},{\"end\":37660,\"start\":37587},{\"end\":38038,\"start\":37674},{\"end\":38266,\"start\":38052},{\"end\":38336,\"start\":38282},{\"end\":38369,\"start\":38339},{\"end\":39276,\"start\":39012},{\"end\":40514,\"start\":40097},{\"end\":40728,\"start\":40517}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4391,\"start\":4385},{\"end\":13754,\"start\":13746},{\"end\":15434,\"start\":15428},{\"end\":16956,\"start\":16950},{\"end\":19556,\"start\":19550},{\"end\":25037,\"start\":25029},{\"end\":25823,\"start\":25815},{\"end\":31674,\"start\":31666},{\"end\":33493,\"start\":33485},{\"end\":34193,\"start\":34185},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":35226,\"start\":35218},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":36004,\"start\":35996},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":36754,\"start\":36745}]", "bib_author_first_name": "[{\"end\":43410,\"start\":43402},{\"end\":43424,\"start\":43417},{\"end\":43435,\"start\":43429},{\"end\":43451,\"start\":43442},{\"end\":43465,\"start\":43458},{\"end\":43481,\"start\":43472},{\"end\":43492,\"start\":43489},{\"end\":43857,\"start\":43850},{\"end\":43870,\"start\":43867},{\"end\":43889,\"start\":43883},{\"end\":43903,\"start\":43897},{\"end\":43925,\"start\":43917},{\"end\":44328,\"start\":44321},{\"end\":44343,\"start\":44338},{\"end\":44356,\"start\":44351},{\"end\":44371,\"start\":44365},{\"end\":44775,\"start\":44774},{\"end\":44915,\"start\":44908},{\"end\":44930,\"start\":44923},{\"end\":44946,\"start\":44940},{\"end\":44959,\"start\":44953},{\"end\":44976,\"start\":44971},{\"end\":44990,\"start\":44985},{\"end\":45004,\"start\":44998},{\"end\":45006,\"start\":45005},{\"end\":45023,\"start\":45017},{\"end\":45446,\"start\":45439},{\"end\":45461,\"start\":45455},{\"end\":45472,\"start\":45468},{\"end\":45491,\"start\":45485},{\"end\":45979,\"start\":45969},{\"end\":45995,\"start\":45989},{\"end\":46013,\"start\":46009},{\"end\":46307,\"start\":46301},{\"end\":46323,\"start\":46316},{\"end\":46340,\"start\":46331},{\"end\":46352,\"start\":46348},{\"end\":46368,\"start\":46362},{\"end\":46387,\"start\":46380},{\"end\":46401,\"start\":46398},{\"end\":46416,\"start\":46410},{\"end\":46428,\"start\":46423},{\"end\":46991,\"start\":46984},{\"end\":47009,\"start\":47003},{\"end\":47343,\"start\":47337},{\"end\":47358,\"start\":47352},{\"end\":47378,\"start\":47372},{\"end\":47663,\"start\":47657},{\"end\":47678,\"start\":47672},{\"end\":47698,\"start\":47692},{\"end\":48184,\"start\":48178},{\"end\":48205,\"start\":48198},{\"end\":48219,\"start\":48215},{\"end\":48231,\"start\":48225},{\"end\":48246,\"start\":48241},{\"end\":48265,\"start\":48257},{\"end\":48281,\"start\":48274},{\"end\":48297,\"start\":48291},{\"end\":48311,\"start\":48305},{\"end\":48844,\"start\":48839},{\"end\":48861,\"start\":48854},{\"end\":48874,\"start\":48869},{\"end\":48887,\"start\":48883},{\"end\":48904,\"start\":48899},{\"end\":48919,\"start\":48912},{\"end\":48934,\"start\":48927},{\"end\":49256,\"start\":49251},{\"end\":49267,\"start\":49264},{\"end\":49717,\"start\":49712},{\"end\":49734,\"start\":49725},{\"end\":49747,\"start\":49744},{\"end\":50046,\"start\":50041},{\"end\":50061,\"start\":50054},{\"end\":50076,\"start\":50070},{\"end\":50301,\"start\":50296},{\"end\":50315,\"start\":50309},{\"end\":50330,\"start\":50324},{\"end\":50617,\"start\":50612},{\"end\":50629,\"start\":50625},{\"end\":50646,\"start\":50640},{\"end\":51014,\"start\":51013},{\"end\":51023,\"start\":51022},{\"end\":51032,\"start\":51031},{\"end\":51328,\"start\":51322},{\"end\":51665,\"start\":51662},{\"end\":51680,\"start\":51672},{\"end\":52001,\"start\":51992},{\"end\":52015,\"start\":52011},{\"end\":52030,\"start\":52025},{\"end\":52047,\"start\":52041},{\"end\":52345,\"start\":52336},{\"end\":52360,\"start\":52355},{\"end\":52376,\"start\":52370},{\"end\":52738,\"start\":52734},{\"end\":52751,\"start\":52743},{\"end\":52765,\"start\":52758},{\"end\":53238,\"start\":53231},{\"end\":53253,\"start\":53247},{\"end\":53266,\"start\":53260},{\"end\":53617,\"start\":53610},{\"end\":53631,\"start\":53626},{\"end\":53635,\"start\":53632},{\"end\":53650,\"start\":53643},{\"end\":53666,\"start\":53659},{\"end\":53668,\"start\":53667},{\"end\":54044,\"start\":54037},{\"end\":54058,\"start\":54053},{\"end\":54077,\"start\":54070},{\"end\":54079,\"start\":54078},{\"end\":54397,\"start\":54392},{\"end\":54412,\"start\":54406},{\"end\":54421,\"start\":54417},{\"end\":54442,\"start\":54436},{\"end\":54836,\"start\":54831},{\"end\":54850,\"start\":54844},{\"end\":54863,\"start\":54859},{\"end\":54882,\"start\":54874},{\"end\":55218,\"start\":55211},{\"end\":55230,\"start\":55223},{\"end\":55246,\"start\":55241},{\"end\":55259,\"start\":55255},{\"end\":55268,\"start\":55260},{\"end\":55632,\"start\":55625},{\"end\":55644,\"start\":55637},{\"end\":55660,\"start\":55652},{\"end\":55670,\"start\":55666},{\"end\":56060,\"start\":56057},{\"end\":56077,\"start\":56072},{\"end\":56094,\"start\":56088},{\"end\":56415,\"start\":56409},{\"end\":56426,\"start\":56421},{\"end\":56442,\"start\":56435},{\"end\":56921,\"start\":56920},{\"end\":56929,\"start\":56928},{\"end\":56937,\"start\":56936},{\"end\":57227,\"start\":57223},{\"end\":57242,\"start\":57237},{\"end\":57584,\"start\":57580},{\"end\":57599,\"start\":57594},{\"end\":57612,\"start\":57605},{\"end\":58057,\"start\":58048},{\"end\":58070,\"start\":58064},{\"end\":58084,\"start\":58078},{\"end\":58411,\"start\":58410},{\"end\":58427,\"start\":58422},{\"end\":58619,\"start\":58610},{\"end\":58637,\"start\":58630},{\"end\":58646,\"start\":58642},{\"end\":58664,\"start\":58657},{\"end\":58678,\"start\":58673},{\"end\":59104,\"start\":59099},{\"end\":59118,\"start\":59112},{\"end\":59489,\"start\":59482},{\"end\":59491,\"start\":59490},{\"end\":59516,\"start\":59513},{\"end\":59815,\"start\":59809},{\"end\":59832,\"start\":59828},{\"end\":59850,\"start\":59843},{\"end\":60128,\"start\":60125},{\"end\":60145,\"start\":60136},{\"end\":60166,\"start\":60157},{\"end\":60609,\"start\":60603},{\"end\":60628,\"start\":60623},{\"end\":60643,\"start\":60636},{\"end\":60657,\"start\":60651},{\"end\":60672,\"start\":60668},{\"end\":61118,\"start\":61116},{\"end\":61130,\"start\":61123},{\"end\":61143,\"start\":61137},{\"end\":61664,\"start\":61658},{\"end\":61672,\"start\":61669},{\"end\":61687,\"start\":61679},{\"end\":61702,\"start\":61694},{\"end\":61948,\"start\":61945},{\"end\":62199,\"start\":62196},{\"end\":62210,\"start\":62207},{\"end\":62217,\"start\":62211},{\"end\":62231,\"start\":62223},{\"end\":62253,\"start\":62249},{\"end\":62263,\"start\":62259},{\"end\":62734,\"start\":62733},{\"end\":62748,\"start\":62747},{\"end\":62757,\"start\":62756},{\"end\":63155,\"start\":63147},{\"end\":63442,\"start\":63441},{\"end\":63931,\"start\":63927},{\"end\":63962,\"start\":63943},{\"end\":63976,\"start\":63972},{\"end\":63978,\"start\":63977},{\"end\":64291,\"start\":64287},{\"end\":64303,\"start\":64297},{\"end\":64310,\"start\":64306},{\"end\":64312,\"start\":64311},{\"end\":64589,\"start\":64585},{\"end\":64601,\"start\":64595},{\"end\":64608,\"start\":64604},{\"end\":64610,\"start\":64609},{\"end\":64848,\"start\":64844},{\"end\":64860,\"start\":64857},{\"end\":64875,\"start\":64868},{\"end\":64893,\"start\":64886},{\"end\":64908,\"start\":64902},{\"end\":64922,\"start\":64915},{\"end\":64937,\"start\":64931},{\"end\":64948,\"start\":64943},{\"end\":64964,\"start\":64960},{\"end\":64977,\"start\":64973},{\"end\":65316,\"start\":65308},{\"end\":65334,\"start\":65327},{\"end\":65653,\"start\":65650},{\"end\":65670,\"start\":65664},{\"end\":65683,\"start\":65676},{\"end\":65997,\"start\":65990},{\"end\":66016,\"start\":66012},{\"end\":66025,\"start\":66021},{\"end\":66374,\"start\":66370},{\"end\":66395,\"start\":66388},{\"end\":66411,\"start\":66405},{\"end\":66816,\"start\":66812},{\"end\":66833,\"start\":66830},{\"end\":66843,\"start\":66840},{\"end\":66856,\"start\":66848},{\"end\":66872,\"start\":66865},{\"end\":66887,\"start\":66883},{\"end\":66899,\"start\":66892},{\"end\":66913,\"start\":66907},{\"end\":66930,\"start\":66924},{\"end\":66946,\"start\":66939},{\"end\":67329,\"start\":67323},{\"end\":67345,\"start\":67338},{\"end\":67359,\"start\":67355},{\"end\":67722,\"start\":67717},{\"end\":67741,\"start\":67733},{\"end\":68025,\"start\":68024},{\"end\":68037,\"start\":68036},{\"end\":68041,\"start\":68038},{\"end\":68052,\"start\":68051},{\"end\":68054,\"start\":68053},{\"end\":68371,\"start\":68365},{\"end\":68386,\"start\":68379},{\"end\":68403,\"start\":68398},{\"end\":68419,\"start\":68412},{\"end\":68435,\"start\":68429},{\"end\":68864,\"start\":68858},{\"end\":68878,\"start\":68870},{\"end\":68892,\"start\":68885},{\"end\":68901,\"start\":68898},{\"end\":69380,\"start\":69373},{\"end\":69609,\"start\":69601},{\"end\":69622,\"start\":69616},{\"end\":69636,\"start\":69632},{\"end\":69654,\"start\":69648},{\"end\":70007,\"start\":70000},{\"end\":70024,\"start\":70016},{\"end\":70281,\"start\":70272},{\"end\":70296,\"start\":70289},{\"end\":70312,\"start\":70306},{\"end\":70494,\"start\":70486},{\"end\":70515,\"start\":70507},{\"end\":70527,\"start\":70522},{\"end\":70543,\"start\":70535},{\"end\":70555,\"start\":70551},{\"end\":70567,\"start\":70561},{\"end\":70587,\"start\":70581},{\"end\":71087,\"start\":71078},{\"end\":71104,\"start\":71096},{\"end\":71118,\"start\":71113},{\"end\":71133,\"start\":71129},{\"end\":71150,\"start\":71144},{\"end\":71458,\"start\":71449},{\"end\":71485,\"start\":71479},{\"end\":71881,\"start\":71880},{\"end\":71889,\"start\":71888},{\"end\":71891,\"start\":71890},{\"end\":71906,\"start\":71905},{\"end\":71913,\"start\":71912},{\"end\":72286,\"start\":72285},{\"end\":72294,\"start\":72293},{\"end\":72306,\"start\":72305},{\"end\":72701,\"start\":72698},{\"end\":72714,\"start\":72708},{\"end\":72729,\"start\":72722},{\"end\":72739,\"start\":72735},{\"end\":73161,\"start\":73157},{\"end\":73172,\"start\":73168},{\"end\":73174,\"start\":73173},{\"end\":73183,\"start\":73182},{\"end\":73197,\"start\":73191},{\"end\":73570,\"start\":73569},{\"end\":73578,\"start\":73577},{\"end\":73586,\"start\":73585},{\"end\":73593,\"start\":73592},{\"end\":73974,\"start\":73971},{\"end\":73984,\"start\":73981},{\"end\":73995,\"start\":73991},{\"end\":74012,\"start\":74006},{\"end\":74454,\"start\":74448},{\"end\":74476,\"start\":74469},{\"end\":74491,\"start\":74485},{\"end\":74851,\"start\":74842},{\"end\":74865,\"start\":74857},{\"end\":74879,\"start\":74872},{\"end\":74889,\"start\":74884},{\"end\":75399,\"start\":75392},{\"end\":75413,\"start\":75405},{\"end\":75658,\"start\":75657},{\"end\":75666,\"start\":75665},{\"end\":75674,\"start\":75673},{\"end\":75676,\"start\":75675},{\"end\":75691,\"start\":75690},{\"end\":75697,\"start\":75696},{\"end\":75708,\"start\":75707},{\"end\":75710,\"start\":75709},{\"end\":76085,\"start\":76076},{\"end\":76099,\"start\":76092},{\"end\":76126,\"start\":76119},{\"end\":76136,\"start\":76133},{\"end\":76479,\"start\":76473},{\"end\":76493,\"start\":76487},{\"end\":76869,\"start\":76861},{\"end\":76884,\"start\":76876},{\"end\":76903,\"start\":76897},{\"end\":77272,\"start\":77265},{\"end\":77286,\"start\":77279},{\"end\":77298,\"start\":77294},{\"end\":77315,\"start\":77308}]", "bib_author_last_name": "[{\"end\":43415,\"start\":43411},{\"end\":43427,\"start\":43425},{\"end\":43440,\"start\":43436},{\"end\":43456,\"start\":43452},{\"end\":43470,\"start\":43466},{\"end\":43487,\"start\":43482},{\"end\":43497,\"start\":43493},{\"end\":43865,\"start\":43858},{\"end\":43881,\"start\":43871},{\"end\":43895,\"start\":43890},{\"end\":43915,\"start\":43904},{\"end\":43933,\"start\":43926},{\"end\":44336,\"start\":44329},{\"end\":44349,\"start\":44344},{\"end\":44363,\"start\":44357},{\"end\":44380,\"start\":44372},{\"end\":44783,\"start\":44776},{\"end\":44921,\"start\":44916},{\"end\":44938,\"start\":44931},{\"end\":44951,\"start\":44947},{\"end\":44969,\"start\":44960},{\"end\":44983,\"start\":44977},{\"end\":44996,\"start\":44991},{\"end\":45015,\"start\":45007},{\"end\":45032,\"start\":45024},{\"end\":45453,\"start\":45447},{\"end\":45466,\"start\":45462},{\"end\":45483,\"start\":45473},{\"end\":45500,\"start\":45492},{\"end\":45987,\"start\":45980},{\"end\":46007,\"start\":45996},{\"end\":46024,\"start\":46014},{\"end\":46314,\"start\":46308},{\"end\":46329,\"start\":46324},{\"end\":46346,\"start\":46341},{\"end\":46360,\"start\":46353},{\"end\":46378,\"start\":46369},{\"end\":46396,\"start\":46388},{\"end\":46408,\"start\":46402},{\"end\":46421,\"start\":46417},{\"end\":46436,\"start\":46429},{\"end\":47001,\"start\":46992},{\"end\":47020,\"start\":47010},{\"end\":47350,\"start\":47344},{\"end\":47370,\"start\":47359},{\"end\":47389,\"start\":47379},{\"end\":47670,\"start\":47664},{\"end\":47690,\"start\":47679},{\"end\":47709,\"start\":47699},{\"end\":48196,\"start\":48185},{\"end\":48213,\"start\":48206},{\"end\":48223,\"start\":48220},{\"end\":48239,\"start\":48232},{\"end\":48255,\"start\":48247},{\"end\":48272,\"start\":48266},{\"end\":48289,\"start\":48282},{\"end\":48303,\"start\":48298},{\"end\":48319,\"start\":48312},{\"end\":48325,\"start\":48321},{\"end\":48852,\"start\":48845},{\"end\":48867,\"start\":48862},{\"end\":48881,\"start\":48875},{\"end\":48897,\"start\":48888},{\"end\":48910,\"start\":48905},{\"end\":48925,\"start\":48920},{\"end\":48942,\"start\":48935},{\"end\":49262,\"start\":49257},{\"end\":49274,\"start\":49268},{\"end\":49723,\"start\":49718},{\"end\":49742,\"start\":49735},{\"end\":49754,\"start\":49748},{\"end\":50052,\"start\":50047},{\"end\":50068,\"start\":50062},{\"end\":50084,\"start\":50077},{\"end\":50307,\"start\":50302},{\"end\":50322,\"start\":50316},{\"end\":50338,\"start\":50331},{\"end\":50623,\"start\":50618},{\"end\":50638,\"start\":50630},{\"end\":50654,\"start\":50647},{\"end\":51020,\"start\":51015},{\"end\":51029,\"start\":51024},{\"end\":51040,\"start\":51033},{\"end\":51338,\"start\":51329},{\"end\":51670,\"start\":51666},{\"end\":51683,\"start\":51681},{\"end\":52009,\"start\":52002},{\"end\":52023,\"start\":52016},{\"end\":52039,\"start\":52031},{\"end\":52058,\"start\":52048},{\"end\":52353,\"start\":52346},{\"end\":52368,\"start\":52361},{\"end\":52387,\"start\":52377},{\"end\":52741,\"start\":52739},{\"end\":52756,\"start\":52752},{\"end\":52770,\"start\":52766},{\"end\":53245,\"start\":53239},{\"end\":53258,\"start\":53254},{\"end\":53274,\"start\":53267},{\"end\":53624,\"start\":53618},{\"end\":53641,\"start\":53636},{\"end\":53657,\"start\":53651},{\"end\":53676,\"start\":53669},{\"end\":54051,\"start\":54045},{\"end\":54068,\"start\":54059},{\"end\":54087,\"start\":54080},{\"end\":54404,\"start\":54398},{\"end\":54415,\"start\":54413},{\"end\":54434,\"start\":54422},{\"end\":54451,\"start\":54443},{\"end\":54842,\"start\":54837},{\"end\":54857,\"start\":54851},{\"end\":54872,\"start\":54864},{\"end\":54890,\"start\":54883},{\"end\":55221,\"start\":55219},{\"end\":55239,\"start\":55231},{\"end\":55253,\"start\":55247},{\"end\":55279,\"start\":55269},{\"end\":55635,\"start\":55633},{\"end\":55650,\"start\":55645},{\"end\":55664,\"start\":55661},{\"end\":55674,\"start\":55671},{\"end\":56070,\"start\":56061},{\"end\":56086,\"start\":56078},{\"end\":56104,\"start\":56095},{\"end\":56419,\"start\":56416},{\"end\":56433,\"start\":56427},{\"end\":56449,\"start\":56443},{\"end\":56926,\"start\":56922},{\"end\":56934,\"start\":56930},{\"end\":56945,\"start\":56938},{\"end\":57235,\"start\":57228},{\"end\":57246,\"start\":57243},{\"end\":57592,\"start\":57585},{\"end\":57603,\"start\":57600},{\"end\":57620,\"start\":57613},{\"end\":58062,\"start\":58058},{\"end\":58076,\"start\":58071},{\"end\":58092,\"start\":58085},{\"end\":58420,\"start\":58412},{\"end\":58434,\"start\":58428},{\"end\":58438,\"start\":58436},{\"end\":58628,\"start\":58620},{\"end\":58640,\"start\":58638},{\"end\":58655,\"start\":58647},{\"end\":58671,\"start\":58665},{\"end\":58685,\"start\":58679},{\"end\":59110,\"start\":59105},{\"end\":59126,\"start\":59119},{\"end\":59511,\"start\":59492},{\"end\":59521,\"start\":59517},{\"end\":59531,\"start\":59523},{\"end\":59826,\"start\":59816},{\"end\":59841,\"start\":59833},{\"end\":59856,\"start\":59851},{\"end\":60134,\"start\":60129},{\"end\":60155,\"start\":60146},{\"end\":60178,\"start\":60167},{\"end\":60621,\"start\":60610},{\"end\":60634,\"start\":60629},{\"end\":60649,\"start\":60644},{\"end\":60666,\"start\":60658},{\"end\":60680,\"start\":60673},{\"end\":61121,\"start\":61119},{\"end\":61135,\"start\":61131},{\"end\":61147,\"start\":61144},{\"end\":61667,\"start\":61665},{\"end\":61677,\"start\":61673},{\"end\":61692,\"start\":61688},{\"end\":61705,\"start\":61703},{\"end\":61957,\"start\":61949},{\"end\":62205,\"start\":62200},{\"end\":62221,\"start\":62218},{\"end\":62237,\"start\":62232},{\"end\":62247,\"start\":62239},{\"end\":62257,\"start\":62254},{\"end\":62268,\"start\":62264},{\"end\":62275,\"start\":62270},{\"end\":62745,\"start\":62735},{\"end\":62754,\"start\":62749},{\"end\":62766,\"start\":62758},{\"end\":63166,\"start\":63156},{\"end\":63453,\"start\":63443},{\"end\":63463,\"start\":63455},{\"end\":63487,\"start\":63465},{\"end\":63941,\"start\":63932},{\"end\":63970,\"start\":63963},{\"end\":63985,\"start\":63979},{\"end\":64295,\"start\":64292},{\"end\":64319,\"start\":64313},{\"end\":64593,\"start\":64590},{\"end\":64617,\"start\":64611},{\"end\":64855,\"start\":64849},{\"end\":64866,\"start\":64861},{\"end\":64884,\"start\":64876},{\"end\":64900,\"start\":64894},{\"end\":64913,\"start\":64909},{\"end\":64929,\"start\":64923},{\"end\":64941,\"start\":64938},{\"end\":64958,\"start\":64949},{\"end\":64971,\"start\":64965},{\"end\":64983,\"start\":64978},{\"end\":65325,\"start\":65317},{\"end\":65337,\"start\":65335},{\"end\":65343,\"start\":65339},{\"end\":65662,\"start\":65654},{\"end\":65674,\"start\":65671},{\"end\":65687,\"start\":65684},{\"end\":65693,\"start\":65689},{\"end\":66010,\"start\":65998},{\"end\":66019,\"start\":66017},{\"end\":66034,\"start\":66026},{\"end\":66039,\"start\":66036},{\"end\":66386,\"start\":66375},{\"end\":66403,\"start\":66396},{\"end\":66416,\"start\":66412},{\"end\":66828,\"start\":66817},{\"end\":66838,\"start\":66834},{\"end\":66846,\"start\":66844},{\"end\":66863,\"start\":66857},{\"end\":66881,\"start\":66873},{\"end\":66890,\"start\":66888},{\"end\":66905,\"start\":66900},{\"end\":66922,\"start\":66914},{\"end\":66937,\"start\":66931},{\"end\":66956,\"start\":66947},{\"end\":67336,\"start\":67330},{\"end\":67353,\"start\":67346},{\"end\":67369,\"start\":67360},{\"end\":67731,\"start\":67723},{\"end\":67749,\"start\":67742},{\"end\":67758,\"start\":67751},{\"end\":68034,\"start\":68026},{\"end\":68049,\"start\":68042},{\"end\":68062,\"start\":68055},{\"end\":68377,\"start\":68372},{\"end\":68396,\"start\":68387},{\"end\":68410,\"start\":68404},{\"end\":68427,\"start\":68420},{\"end\":68443,\"start\":68436},{\"end\":68868,\"start\":68865},{\"end\":68883,\"start\":68879},{\"end\":68896,\"start\":68893},{\"end\":68907,\"start\":68902},{\"end\":69389,\"start\":69381},{\"end\":69614,\"start\":69610},{\"end\":69630,\"start\":69623},{\"end\":69646,\"start\":69637},{\"end\":69663,\"start\":69655},{\"end\":70014,\"start\":70008},{\"end\":70032,\"start\":70025},{\"end\":70287,\"start\":70282},{\"end\":70304,\"start\":70297},{\"end\":70316,\"start\":70313},{\"end\":70505,\"start\":70495},{\"end\":70520,\"start\":70516},{\"end\":70533,\"start\":70528},{\"end\":70549,\"start\":70544},{\"end\":70559,\"start\":70556},{\"end\":70579,\"start\":70568},{\"end\":70592,\"start\":70588},{\"end\":71094,\"start\":71088},{\"end\":71111,\"start\":71105},{\"end\":71127,\"start\":71119},{\"end\":71142,\"start\":71134},{\"end\":71158,\"start\":71151},{\"end\":71477,\"start\":71459},{\"end\":71492,\"start\":71486},{\"end\":71501,\"start\":71494},{\"end\":71886,\"start\":71882},{\"end\":71903,\"start\":71892},{\"end\":71910,\"start\":71907},{\"end\":71919,\"start\":71914},{\"end\":72291,\"start\":72287},{\"end\":72303,\"start\":72295},{\"end\":72314,\"start\":72307},{\"end\":72706,\"start\":72702},{\"end\":72720,\"start\":72715},{\"end\":72733,\"start\":72730},{\"end\":72747,\"start\":72740},{\"end\":73166,\"start\":73162},{\"end\":73180,\"start\":73175},{\"end\":73189,\"start\":73184},{\"end\":73204,\"start\":73198},{\"end\":73216,\"start\":73206},{\"end\":73575,\"start\":73571},{\"end\":73583,\"start\":73579},{\"end\":73590,\"start\":73587},{\"end\":73601,\"start\":73594},{\"end\":73979,\"start\":73975},{\"end\":73989,\"start\":73985},{\"end\":74004,\"start\":73996},{\"end\":74020,\"start\":74013},{\"end\":74467,\"start\":74455},{\"end\":74483,\"start\":74477},{\"end\":74499,\"start\":74492},{\"end\":74504,\"start\":74501},{\"end\":74855,\"start\":74852},{\"end\":74870,\"start\":74866},{\"end\":74882,\"start\":74880},{\"end\":74894,\"start\":74890},{\"end\":75403,\"start\":75400},{\"end\":75417,\"start\":75414},{\"end\":75663,\"start\":75659},{\"end\":75671,\"start\":75667},{\"end\":75688,\"start\":75677},{\"end\":75694,\"start\":75692},{\"end\":75705,\"start\":75698},{\"end\":75715,\"start\":75711},{\"end\":76090,\"start\":76086},{\"end\":76117,\"start\":76100},{\"end\":76131,\"start\":76127},{\"end\":76141,\"start\":76137},{\"end\":76485,\"start\":76480},{\"end\":76504,\"start\":76494},{\"end\":76874,\"start\":76870},{\"end\":76895,\"start\":76885},{\"end\":76908,\"start\":76904},{\"end\":77277,\"start\":77273},{\"end\":77292,\"start\":77287},{\"end\":77306,\"start\":77299},{\"end\":77320,\"start\":77316}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":201660344},\"end\":43848,\"start\":43320},{\"attributes\":{\"doi\":\"arXiv:1804.00874\",\"id\":\"b1\"},\"end\":44252,\"start\":43850},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":62421353},\"end\":44714,\"start\":44254},{\"attributes\":{\"id\":\"b3\"},\"end\":44865,\"start\":44716},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":9999787},\"end\":45329,\"start\":44867},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":53437459},\"end\":45891,\"start\":45331},{\"attributes\":{\"doi\":\"arXiv:1511.07289\",\"id\":\"b6\"},\"end\":46236,\"start\":45893},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":502946},\"end\":46891,\"start\":46238},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":1616473},\"end\":47335,\"start\":46893},{\"attributes\":{\"doi\":\"arXiv:1812.03245\",\"id\":\"b9\"},\"end\":47585,\"start\":47337},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":4918026},\"end\":48116,\"start\":47587},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":12552176},\"end\":48758,\"start\":48118},{\"attributes\":{\"id\":\"b12\"},\"end\":49141,\"start\":48760},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":102496818},\"end\":49635,\"start\":49143},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":2255738},\"end\":50015,\"start\":49637},{\"attributes\":{\"id\":\"b15\"},\"end\":50249,\"start\":50017},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":14547347},\"end\":50565,\"start\":50251},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":206943983},\"end\":50960,\"start\":50567},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":7110290},\"end\":51261,\"start\":50962},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":15601477},\"end\":51551,\"start\":51263},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":195316982},\"end\":51923,\"start\":51553},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":14042524},\"end\":52285,\"start\":51925},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":206850490},\"end\":52668,\"start\":52287},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":46968214},\"end\":53158,\"start\":52670},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":6724907},\"end\":53551,\"start\":53160},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":46936649},\"end\":53967,\"start\":53553},{\"attributes\":{\"doi\":\"arXiv:1609.03677\",\"id\":\"b26\"},\"end\":54299,\"start\":53969},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":131774103},\"end\":54762,\"start\":54301},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":206850587},\"end\":55209,\"start\":54764},{\"attributes\":{\"id\":\"b29\"},\"end\":55577,\"start\":55211},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":206594692},\"end\":56025,\"start\":55579},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":6099034},\"end\":56328,\"start\":56027},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":6346856},\"end\":56842,\"start\":56330},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":204575713},\"end\":57143,\"start\":56844},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":71134},\"end\":57490,\"start\":57145},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":4800342},\"end\":58009,\"start\":57492},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":10038098},\"end\":58364,\"start\":58011},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b37\"},\"end\":58585,\"start\":58366},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":4853375},\"end\":59040,\"start\":58587},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":52956678},\"end\":59435,\"start\":59042},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":14394619},\"end\":59742,\"start\":59437},{\"attributes\":{\"doi\":\"arXiv:1702.02706\",\"id\":\"b41\"},\"end\":60055,\"start\":59744},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":11091110},\"end\":60531,\"start\":60057},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":206500609},\"end\":60962,\"start\":60533},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":206592782},\"end\":61584,\"start\":60964},{\"attributes\":{\"doi\":\"arXiv:1709.06841\",\"id\":\"b45\"},\"end\":61909,\"start\":61586},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":7722934},\"end\":62096,\"start\":61911},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":52912195},\"end\":62632,\"start\":62098},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":3645349},\"end\":63082,\"start\":62634},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":215796513},\"end\":63363,\"start\":63084},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":12751695},\"end\":63867,\"start\":63365},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":206775100},\"end\":64205,\"start\":63869},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":206775640},\"end\":64536,\"start\":64207},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":206439358},\"end\":64804,\"start\":64538},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":40027675},\"end\":65229,\"start\":64806},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":7334757},\"end\":65556,\"start\":65231},{\"attributes\":{\"doi\":\"arXiv:1901.03638\",\"id\":\"b56\"},\"end\":65908,\"start\":65558},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":10328909},\"end\":66303,\"start\":65910},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":3719281},\"end\":66759,\"start\":66305},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":2930547},\"end\":67276,\"start\":66761},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":196201321},\"end\":67715,\"start\":67278},{\"attributes\":{\"id\":\"b61\"},\"end\":67983,\"start\":67717},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":6396561},\"end\":68309,\"start\":67985},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":206942855},\"end\":68784,\"start\":68311},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":30824366},\"end\":69270,\"start\":68786},{\"attributes\":{\"id\":\"b65\"},\"end\":69536,\"start\":69272},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":67856705},\"end\":69897,\"start\":69538},{\"attributes\":{\"doi\":\"arXiv:1704.03489\",\"id\":\"b67\"},\"end\":70246,\"start\":69899},{\"attributes\":{\"id\":\"b68\"},\"end\":70421,\"start\":70248},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":6159584},\"end\":71019,\"start\":70423},{\"attributes\":{\"doi\":\"arXiv:1904.06504\",\"id\":\"b70\"},\"end\":71377,\"start\":71021},{\"attributes\":{\"id\":\"b71\",\"matched_paper_id\":4899187},\"end\":71819,\"start\":71379},{\"attributes\":{\"id\":\"b72\",\"matched_paper_id\":21352010},\"end\":72209,\"start\":71821},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":8515741},\"end\":72602,\"start\":72211},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":9114952},\"end\":73081,\"start\":72604},{\"attributes\":{\"id\":\"b75\",\"matched_paper_id\":207761262},\"end\":73461,\"start\":73083},{\"attributes\":{\"id\":\"b76\",\"matched_paper_id\":6761996},\"end\":73868,\"start\":73463},{\"attributes\":{\"id\":\"b77\",\"matched_paper_id\":49658377},\"end\":74403,\"start\":73870},{\"attributes\":{\"id\":\"b78\",\"matched_paper_id\":602850},\"end\":74734,\"start\":74405},{\"attributes\":{\"id\":\"b79\",\"matched_paper_id\":8176169},\"end\":75314,\"start\":74736},{\"attributes\":{\"id\":\"b80\",\"matched_paper_id\":3714620},\"end\":75549,\"start\":75316},{\"attributes\":{\"id\":\"b81\",\"matched_paper_id\":4578162},\"end\":76074,\"start\":75551},{\"attributes\":{\"doi\":\"arXiv:1909.09803\",\"id\":\"b82\"},\"end\":76389,\"start\":76076},{\"attributes\":{\"id\":\"b83\",\"matched_paper_id\":53453235},\"end\":76823,\"start\":76391},{\"attributes\":{\"id\":\"b84\",\"matched_paper_id\":51929808},\"end\":77205,\"start\":76825},{\"attributes\":{\"id\":\"b85\",\"matched_paper_id\":11977588},\"end\":77500,\"start\":77207}]", "bib_title": "[{\"end\":43400,\"start\":43320},{\"end\":44319,\"start\":44254},{\"end\":44906,\"start\":44867},{\"end\":45437,\"start\":45331},{\"end\":46299,\"start\":46238},{\"end\":46982,\"start\":46893},{\"end\":47655,\"start\":47587},{\"end\":48176,\"start\":48118},{\"end\":49249,\"start\":49143},{\"end\":49710,\"start\":49637},{\"end\":50294,\"start\":50251},{\"end\":50610,\"start\":50567},{\"end\":51011,\"start\":50962},{\"end\":51320,\"start\":51263},{\"end\":51660,\"start\":51553},{\"end\":51990,\"start\":51925},{\"end\":52334,\"start\":52287},{\"end\":52732,\"start\":52670},{\"end\":53229,\"start\":53160},{\"end\":53608,\"start\":53553},{\"end\":54390,\"start\":54301},{\"end\":54829,\"start\":54764},{\"end\":55623,\"start\":55579},{\"end\":56055,\"start\":56027},{\"end\":56407,\"start\":56330},{\"end\":56918,\"start\":56844},{\"end\":57221,\"start\":57145},{\"end\":57578,\"start\":57492},{\"end\":58046,\"start\":58011},{\"end\":58608,\"start\":58587},{\"end\":59097,\"start\":59042},{\"end\":59480,\"start\":59437},{\"end\":60123,\"start\":60057},{\"end\":60601,\"start\":60533},{\"end\":61114,\"start\":60964},{\"end\":61943,\"start\":61911},{\"end\":62194,\"start\":62098},{\"end\":62731,\"start\":62634},{\"end\":63145,\"start\":63084},{\"end\":63439,\"start\":63365},{\"end\":63925,\"start\":63869},{\"end\":64285,\"start\":64207},{\"end\":64583,\"start\":64538},{\"end\":64842,\"start\":64806},{\"end\":65306,\"start\":65231},{\"end\":65988,\"start\":65910},{\"end\":66368,\"start\":66305},{\"end\":66810,\"start\":66761},{\"end\":67321,\"start\":67278},{\"end\":68022,\"start\":67985},{\"end\":68363,\"start\":68311},{\"end\":68856,\"start\":68786},{\"end\":69599,\"start\":69538},{\"end\":70484,\"start\":70423},{\"end\":71447,\"start\":71379},{\"end\":71878,\"start\":71821},{\"end\":72283,\"start\":72211},{\"end\":72696,\"start\":72604},{\"end\":73155,\"start\":73083},{\"end\":73567,\"start\":73463},{\"end\":73969,\"start\":73870},{\"end\":74446,\"start\":74405},{\"end\":74840,\"start\":74736},{\"end\":75390,\"start\":75316},{\"end\":75655,\"start\":75551},{\"end\":76471,\"start\":76391},{\"end\":76859,\"start\":76825},{\"end\":77263,\"start\":77207}]", "bib_author": "[{\"end\":43417,\"start\":43402},{\"end\":43429,\"start\":43417},{\"end\":43442,\"start\":43429},{\"end\":43458,\"start\":43442},{\"end\":43472,\"start\":43458},{\"end\":43489,\"start\":43472},{\"end\":43499,\"start\":43489},{\"end\":43867,\"start\":43850},{\"end\":43883,\"start\":43867},{\"end\":43897,\"start\":43883},{\"end\":43917,\"start\":43897},{\"end\":43935,\"start\":43917},{\"end\":44338,\"start\":44321},{\"end\":44351,\"start\":44338},{\"end\":44365,\"start\":44351},{\"end\":44382,\"start\":44365},{\"end\":44785,\"start\":44774},{\"end\":44923,\"start\":44908},{\"end\":44940,\"start\":44923},{\"end\":44953,\"start\":44940},{\"end\":44971,\"start\":44953},{\"end\":44985,\"start\":44971},{\"end\":44998,\"start\":44985},{\"end\":45017,\"start\":44998},{\"end\":45034,\"start\":45017},{\"end\":45455,\"start\":45439},{\"end\":45468,\"start\":45455},{\"end\":45485,\"start\":45468},{\"end\":45502,\"start\":45485},{\"end\":45989,\"start\":45969},{\"end\":46009,\"start\":45989},{\"end\":46026,\"start\":46009},{\"end\":46316,\"start\":46301},{\"end\":46331,\"start\":46316},{\"end\":46348,\"start\":46331},{\"end\":46362,\"start\":46348},{\"end\":46380,\"start\":46362},{\"end\":46398,\"start\":46380},{\"end\":46410,\"start\":46398},{\"end\":46423,\"start\":46410},{\"end\":46438,\"start\":46423},{\"end\":47003,\"start\":46984},{\"end\":47022,\"start\":47003},{\"end\":47352,\"start\":47337},{\"end\":47372,\"start\":47352},{\"end\":47391,\"start\":47372},{\"end\":47672,\"start\":47657},{\"end\":47692,\"start\":47672},{\"end\":47711,\"start\":47692},{\"end\":48198,\"start\":48178},{\"end\":48215,\"start\":48198},{\"end\":48225,\"start\":48215},{\"end\":48241,\"start\":48225},{\"end\":48257,\"start\":48241},{\"end\":48274,\"start\":48257},{\"end\":48291,\"start\":48274},{\"end\":48305,\"start\":48291},{\"end\":48321,\"start\":48305},{\"end\":48327,\"start\":48321},{\"end\":48854,\"start\":48839},{\"end\":48869,\"start\":48854},{\"end\":48883,\"start\":48869},{\"end\":48899,\"start\":48883},{\"end\":48912,\"start\":48899},{\"end\":48927,\"start\":48912},{\"end\":48944,\"start\":48927},{\"end\":49264,\"start\":49251},{\"end\":49276,\"start\":49264},{\"end\":49725,\"start\":49712},{\"end\":49744,\"start\":49725},{\"end\":49756,\"start\":49744},{\"end\":50054,\"start\":50041},{\"end\":50070,\"start\":50054},{\"end\":50086,\"start\":50070},{\"end\":50309,\"start\":50296},{\"end\":50324,\"start\":50309},{\"end\":50340,\"start\":50324},{\"end\":50625,\"start\":50612},{\"end\":50640,\"start\":50625},{\"end\":50656,\"start\":50640},{\"end\":51022,\"start\":51013},{\"end\":51031,\"start\":51022},{\"end\":51042,\"start\":51031},{\"end\":51340,\"start\":51322},{\"end\":51672,\"start\":51662},{\"end\":51685,\"start\":51672},{\"end\":52011,\"start\":51992},{\"end\":52025,\"start\":52011},{\"end\":52041,\"start\":52025},{\"end\":52060,\"start\":52041},{\"end\":52355,\"start\":52336},{\"end\":52370,\"start\":52355},{\"end\":52389,\"start\":52370},{\"end\":52743,\"start\":52734},{\"end\":52758,\"start\":52743},{\"end\":52772,\"start\":52758},{\"end\":53247,\"start\":53231},{\"end\":53260,\"start\":53247},{\"end\":53276,\"start\":53260},{\"end\":53626,\"start\":53610},{\"end\":53643,\"start\":53626},{\"end\":53659,\"start\":53643},{\"end\":53678,\"start\":53659},{\"end\":54053,\"start\":54037},{\"end\":54070,\"start\":54053},{\"end\":54089,\"start\":54070},{\"end\":54406,\"start\":54392},{\"end\":54417,\"start\":54406},{\"end\":54436,\"start\":54417},{\"end\":54453,\"start\":54436},{\"end\":54844,\"start\":54831},{\"end\":54859,\"start\":54844},{\"end\":54874,\"start\":54859},{\"end\":54892,\"start\":54874},{\"end\":55223,\"start\":55211},{\"end\":55241,\"start\":55223},{\"end\":55255,\"start\":55241},{\"end\":55281,\"start\":55255},{\"end\":55637,\"start\":55625},{\"end\":55652,\"start\":55637},{\"end\":55666,\"start\":55652},{\"end\":55676,\"start\":55666},{\"end\":56072,\"start\":56057},{\"end\":56088,\"start\":56072},{\"end\":56106,\"start\":56088},{\"end\":56421,\"start\":56409},{\"end\":56435,\"start\":56421},{\"end\":56451,\"start\":56435},{\"end\":56928,\"start\":56920},{\"end\":56936,\"start\":56928},{\"end\":56947,\"start\":56936},{\"end\":57237,\"start\":57223},{\"end\":57248,\"start\":57237},{\"end\":57594,\"start\":57580},{\"end\":57605,\"start\":57594},{\"end\":57622,\"start\":57605},{\"end\":58064,\"start\":58048},{\"end\":58078,\"start\":58064},{\"end\":58094,\"start\":58078},{\"end\":58422,\"start\":58410},{\"end\":58436,\"start\":58422},{\"end\":58440,\"start\":58436},{\"end\":58630,\"start\":58610},{\"end\":58642,\"start\":58630},{\"end\":58657,\"start\":58642},{\"end\":58673,\"start\":58657},{\"end\":58687,\"start\":58673},{\"end\":59112,\"start\":59099},{\"end\":59128,\"start\":59112},{\"end\":59513,\"start\":59482},{\"end\":59523,\"start\":59513},{\"end\":59533,\"start\":59523},{\"end\":59828,\"start\":59809},{\"end\":59843,\"start\":59828},{\"end\":59858,\"start\":59843},{\"end\":60136,\"start\":60125},{\"end\":60157,\"start\":60136},{\"end\":60180,\"start\":60157},{\"end\":60623,\"start\":60603},{\"end\":60636,\"start\":60623},{\"end\":60651,\"start\":60636},{\"end\":60668,\"start\":60651},{\"end\":60682,\"start\":60668},{\"end\":61123,\"start\":61116},{\"end\":61137,\"start\":61123},{\"end\":61149,\"start\":61137},{\"end\":61669,\"start\":61658},{\"end\":61679,\"start\":61669},{\"end\":61694,\"start\":61679},{\"end\":61707,\"start\":61694},{\"end\":61959,\"start\":61945},{\"end\":62207,\"start\":62196},{\"end\":62223,\"start\":62207},{\"end\":62239,\"start\":62223},{\"end\":62249,\"start\":62239},{\"end\":62259,\"start\":62249},{\"end\":62270,\"start\":62259},{\"end\":62277,\"start\":62270},{\"end\":62747,\"start\":62733},{\"end\":62756,\"start\":62747},{\"end\":62768,\"start\":62756},{\"end\":63168,\"start\":63147},{\"end\":63455,\"start\":63441},{\"end\":63465,\"start\":63455},{\"end\":63489,\"start\":63465},{\"end\":63943,\"start\":63927},{\"end\":63972,\"start\":63943},{\"end\":63987,\"start\":63972},{\"end\":64297,\"start\":64287},{\"end\":64306,\"start\":64297},{\"end\":64321,\"start\":64306},{\"end\":64595,\"start\":64585},{\"end\":64604,\"start\":64595},{\"end\":64619,\"start\":64604},{\"end\":64857,\"start\":64844},{\"end\":64868,\"start\":64857},{\"end\":64886,\"start\":64868},{\"end\":64902,\"start\":64886},{\"end\":64915,\"start\":64902},{\"end\":64931,\"start\":64915},{\"end\":64943,\"start\":64931},{\"end\":64960,\"start\":64943},{\"end\":64973,\"start\":64960},{\"end\":64985,\"start\":64973},{\"end\":65327,\"start\":65308},{\"end\":65339,\"start\":65327},{\"end\":65345,\"start\":65339},{\"end\":65664,\"start\":65650},{\"end\":65676,\"start\":65664},{\"end\":65689,\"start\":65676},{\"end\":65695,\"start\":65689},{\"end\":66012,\"start\":65990},{\"end\":66021,\"start\":66012},{\"end\":66036,\"start\":66021},{\"end\":66041,\"start\":66036},{\"end\":66388,\"start\":66370},{\"end\":66405,\"start\":66388},{\"end\":66418,\"start\":66405},{\"end\":66830,\"start\":66812},{\"end\":66840,\"start\":66830},{\"end\":66848,\"start\":66840},{\"end\":66865,\"start\":66848},{\"end\":66883,\"start\":66865},{\"end\":66892,\"start\":66883},{\"end\":66907,\"start\":66892},{\"end\":66924,\"start\":66907},{\"end\":66939,\"start\":66924},{\"end\":66958,\"start\":66939},{\"end\":67338,\"start\":67323},{\"end\":67355,\"start\":67338},{\"end\":67371,\"start\":67355},{\"end\":67733,\"start\":67717},{\"end\":67751,\"start\":67733},{\"end\":67760,\"start\":67751},{\"end\":68036,\"start\":68024},{\"end\":68051,\"start\":68036},{\"end\":68064,\"start\":68051},{\"end\":68379,\"start\":68365},{\"end\":68398,\"start\":68379},{\"end\":68412,\"start\":68398},{\"end\":68429,\"start\":68412},{\"end\":68445,\"start\":68429},{\"end\":68870,\"start\":68858},{\"end\":68885,\"start\":68870},{\"end\":68898,\"start\":68885},{\"end\":68909,\"start\":68898},{\"end\":69391,\"start\":69373},{\"end\":69616,\"start\":69601},{\"end\":69632,\"start\":69616},{\"end\":69648,\"start\":69632},{\"end\":69665,\"start\":69648},{\"end\":70016,\"start\":70000},{\"end\":70034,\"start\":70016},{\"end\":70289,\"start\":70272},{\"end\":70306,\"start\":70289},{\"end\":70318,\"start\":70306},{\"end\":70507,\"start\":70486},{\"end\":70522,\"start\":70507},{\"end\":70535,\"start\":70522},{\"end\":70551,\"start\":70535},{\"end\":70561,\"start\":70551},{\"end\":70581,\"start\":70561},{\"end\":70594,\"start\":70581},{\"end\":71096,\"start\":71078},{\"end\":71113,\"start\":71096},{\"end\":71129,\"start\":71113},{\"end\":71144,\"start\":71129},{\"end\":71160,\"start\":71144},{\"end\":71479,\"start\":71449},{\"end\":71494,\"start\":71479},{\"end\":71503,\"start\":71494},{\"end\":71888,\"start\":71880},{\"end\":71905,\"start\":71888},{\"end\":71912,\"start\":71905},{\"end\":71921,\"start\":71912},{\"end\":72293,\"start\":72285},{\"end\":72305,\"start\":72293},{\"end\":72316,\"start\":72305},{\"end\":72708,\"start\":72698},{\"end\":72722,\"start\":72708},{\"end\":72735,\"start\":72722},{\"end\":72749,\"start\":72735},{\"end\":73168,\"start\":73157},{\"end\":73182,\"start\":73168},{\"end\":73191,\"start\":73182},{\"end\":73206,\"start\":73191},{\"end\":73218,\"start\":73206},{\"end\":73577,\"start\":73569},{\"end\":73585,\"start\":73577},{\"end\":73592,\"start\":73585},{\"end\":73603,\"start\":73592},{\"end\":73981,\"start\":73971},{\"end\":73991,\"start\":73981},{\"end\":74006,\"start\":73991},{\"end\":74022,\"start\":74006},{\"end\":74469,\"start\":74448},{\"end\":74485,\"start\":74469},{\"end\":74501,\"start\":74485},{\"end\":74506,\"start\":74501},{\"end\":74857,\"start\":74842},{\"end\":74872,\"start\":74857},{\"end\":74884,\"start\":74872},{\"end\":74896,\"start\":74884},{\"end\":75405,\"start\":75392},{\"end\":75419,\"start\":75405},{\"end\":75665,\"start\":75657},{\"end\":75673,\"start\":75665},{\"end\":75690,\"start\":75673},{\"end\":75696,\"start\":75690},{\"end\":75707,\"start\":75696},{\"end\":75717,\"start\":75707},{\"end\":76092,\"start\":76076},{\"end\":76119,\"start\":76092},{\"end\":76133,\"start\":76119},{\"end\":76143,\"start\":76133},{\"end\":76487,\"start\":76473},{\"end\":76506,\"start\":76487},{\"end\":76876,\"start\":76861},{\"end\":76897,\"start\":76876},{\"end\":76910,\"start\":76897},{\"end\":77279,\"start\":77265},{\"end\":77294,\"start\":77279},{\"end\":77308,\"start\":77294},{\"end\":77322,\"start\":77308}]", "bib_venue": "[{\"end\":43573,\"start\":43499},{\"end\":44028,\"start\":43951},{\"end\":44461,\"start\":44382},{\"end\":44772,\"start\":44716},{\"end\":45080,\"start\":45034},{\"end\":45563,\"start\":45502},{\"end\":45967,\"start\":45893},{\"end\":46516,\"start\":46438},{\"end\":47090,\"start\":47022},{\"end\":47437,\"start\":47407},{\"end\":47798,\"start\":47711},{\"end\":48394,\"start\":48327},{\"end\":48837,\"start\":48760},{\"end\":49343,\"start\":49276},{\"end\":49805,\"start\":49756},{\"end\":50039,\"start\":50017},{\"end\":50378,\"start\":50340},{\"end\":50736,\"start\":50656},{\"end\":51097,\"start\":51042},{\"end\":51381,\"start\":51340},{\"end\":51721,\"start\":51685},{\"end\":52089,\"start\":52060},{\"end\":52457,\"start\":52389},{\"end\":52837,\"start\":52772},{\"end\":53336,\"start\":53276},{\"end\":53737,\"start\":53678},{\"end\":54035,\"start\":53969},{\"end\":54512,\"start\":54453},{\"end\":54961,\"start\":54892},{\"end\":55348,\"start\":55281},{\"end\":55753,\"start\":55676},{\"end\":56155,\"start\":56106},{\"end\":56524,\"start\":56451},{\"end\":56982,\"start\":56947},{\"end\":57297,\"start\":57248},{\"end\":57706,\"start\":57622},{\"end\":58166,\"start\":58094},{\"end\":58408,\"start\":58366},{\"end\":58764,\"start\":58687},{\"end\":59192,\"start\":59128},{\"end\":59572,\"start\":59533},{\"end\":59807,\"start\":59744},{\"end\":60236,\"start\":60180},{\"end\":60728,\"start\":60682},{\"end\":61226,\"start\":61149},{\"end\":61656,\"start\":61586},{\"end\":61990,\"start\":61959},{\"end\":62340,\"start\":62277},{\"end\":62835,\"start\":62768},{\"end\":63208,\"start\":63168},{\"end\":63562,\"start\":63489},{\"end\":64016,\"start\":63987},{\"end\":64350,\"start\":64321},{\"end\":64655,\"start\":64619},{\"end\":65007,\"start\":64985},{\"end\":65374,\"start\":65345},{\"end\":65648,\"start\":65558},{\"end\":66090,\"start\":66041},{\"end\":66504,\"start\":66418},{\"end\":66998,\"start\":66958},{\"end\":67448,\"start\":67371},{\"end\":67838,\"start\":67760},{\"end\":68125,\"start\":68064},{\"end\":68525,\"start\":68445},{\"end\":68972,\"start\":68909},{\"end\":69371,\"start\":69272},{\"end\":69701,\"start\":69665},{\"end\":69998,\"start\":69899},{\"end\":70270,\"start\":70248},{\"end\":70671,\"start\":70594},{\"end\":71076,\"start\":71021},{\"end\":71571,\"start\":71503},{\"end\":71988,\"start\":71921},{\"end\":72366,\"start\":72316},{\"end\":72786,\"start\":72749},{\"end\":73255,\"start\":73218},{\"end\":73646,\"start\":73603},{\"end\":74086,\"start\":74022},{\"end\":74544,\"start\":74506},{\"end\":74973,\"start\":74896},{\"end\":75423,\"start\":75419},{\"end\":75784,\"start\":75717},{\"end\":76207,\"start\":76159},{\"end\":76580,\"start\":76506},{\"end\":76974,\"start\":76910},{\"end\":77326,\"start\":77322},{\"end\":45611,\"start\":45565},{\"end\":46590,\"start\":46518},{\"end\":47872,\"start\":47800},{\"end\":48448,\"start\":48396},{\"end\":49397,\"start\":49345},{\"end\":52868,\"start\":52839},{\"end\":55402,\"start\":55350},{\"end\":55817,\"start\":55755},{\"end\":56587,\"start\":56526},{\"end\":57777,\"start\":57708},{\"end\":58828,\"start\":58766},{\"end\":59243,\"start\":59194},{\"end\":61290,\"start\":61228},{\"end\":63625,\"start\":63564},{\"end\":67512,\"start\":67450},{\"end\":68997,\"start\":68974},{\"end\":70735,\"start\":70673},{\"end\":72381,\"start\":72368},{\"end\":74137,\"start\":74088},{\"end\":75037,\"start\":74975},{\"end\":77025,\"start\":76976}]"}}}, "year": 2023, "month": 12, "day": 17}