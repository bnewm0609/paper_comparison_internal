{"id": 249305308, "updated": "2022-07-06 15:19:52.679", "metadata": {"title": "AST-trans: code summarization with efficient tree-structured attention", "authors": "[{\"first\":\"Ze\",\"last\":\"Tang\",\"middle\":[]},{\"first\":\"Xiaoyu\",\"last\":\"Shen\",\"middle\":[]},{\"first\":\"Chuanyi\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Jidong\",\"last\":\"Ge\",\"middle\":[]},{\"first\":\"Liguo\",\"last\":\"Huang\",\"middle\":[]},{\"first\":\"Zhelin\",\"last\":\"Zhu\",\"middle\":[]},{\"first\":\"Bin\",\"last\":\"Luo\",\"middle\":[]}]", "venue": "2022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)", "journal": "Proceedings of the 44th International Conference on Software Engineering", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Code summarization aims to generate brief natural language descriptions for source codes. The state-of-the-art approaches follow a transformer-based encoder-decoder architecture. As the source code is highly structured and follows strict grammars, its Abstract Syntax Tree (AST) is widely used for encoding structural information. However, ASTs are much longer than the corresponding source code. Existing approaches ignore the size constraint and simply feed the whole linearized AST into the encoders. We argue that such a simple process makes it difficult to extract the truly useful dependency relations from the overlong input sequence. It also incurs significant computational overhead since each node needs to apply self-attention to all other nodes in the AST. To encode the AST more effectively and efficiently, we propose AST-Trans in this paper which exploits two types of node relationships in the AST: ancestor-descendant and sibling relationships. It applies the tree-structured attention to dynamically allocate weights for relevant nodes and exclude irrelevant nodes based on these two relationships. We further propose an efficient implementation to support fast parallel computation for tree-structure attention. On the two code summarization datasets, experimental results show that AST-Trans significantly outperforms the state-of-the-arts while being times more efficient than standard transformers1.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icse/TangSLGHZ022", "doi": "10.1145/3510003.3510224"}}, "content": {"source": {"pdf_hash": "cafc40504b3e20193a1c324a0da007c9b3b99f01", "pdf_src": "ACM", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "808b415ebe5c5b78c9b269b9721d9cf8a477ff1e", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/cafc40504b3e20193a1c324a0da007c9b3b99f01.txt", "contents": "\nAST-Trans: Code Summarization with Efficient Tree-Structured Attention CCS CONCEPTS \u2022 Software and its engineering \u2192 Documentation; \u2022 Comput- ing methodologies \u2192 Natural language generation. KEYWORDS tree-based neural network, source code summarization ACM Reference Format\nMay 21-29, 2022\n\nZe Tang \nXiaoyu Shen \nChuanyi Li \nJidong Ge \nLiguo Huang lghuang@lyle.smu.edu \nZhelin Zhu \nBin Luo luobin@nju.edu.cn \nZe Tang \nXiaoyu Shen \nChuanyi Li \nJidong Ge \nLiguo Huang \nZhelin \n\nState Key Laboratory for Novel Software Technology\nNanjing University\nNanjingChina\n\n\nDepartment of Computer Science\nState Key Laboratory for Novel Software Technology Nanjing University Nanjing\nAlexa AI Amazon\nBerlinGermany, China\n\n\nState Key Laboratory for Novel Software Technology Nanjing University Nanjing\nSouthern Methodist University Dallas\nTexasUSA, China\n\nAST-Trans: Code Summarization with Efficient Tree-Structured Attention CCS CONCEPTS \u2022 Software and its engineering \u2192 Documentation; \u2022 Comput- ing methodologies \u2192 Natural language generation. KEYWORDS tree-based neural network, source code summarization ACM Reference Format\nPittsburgh, PA, USAMay 21-29, 202210.1145/3510003.3510224* Work done before joining. 1 All the codes and data are available at https://github.com/zetang94/ICSE2022_AST_ Trans.git ACM ISBN 978-1-4503-9221-1/22/05. . . $15.00 Zhu, Bin Luo. 2022. AST-Trans: Code Summarization with Efficient Tree-Structured Attention. In 44th International Conference on Software Engineer-ing (ICSE '22), May 21-29, 2022, Pittsburgh, PA, USA. ACM, New York, NY, USA, 13 pages. https://\nCode summarization aims to generate brief natural language descriptions for source codes. The state-of-the-art approaches follow a transformer-based encoder-decoder architecture. As the source code is highly structured and follows strict grammars, its Abstract Syntax Tree (AST) is widely used for encoding structural information. However, ASTs are much longer than the corresponding source code. Existing approaches ignore the size constraint and simply feed the whole linearized AST into the encoders. We argue that such a simple process makes it difficult to extract the truly useful dependency relations from the overlong input sequence. It also incurs significant computational overhead since each node needs to apply self-attention to all other nodes in the AST. To encode the AST more effectively and efficiently, we propose AST-Trans in this paper which exploits two types of node relationships in the AST: ancestor-descendant and sibling relationships. It applies the tree-structured attention to dynamically allocate weights for relevant nodes and exclude irrelevant nodes based on these two relationships. We further propose an efficient implementation to support fast parallel computation for tree-structure attention. On the two code summarization datasets, experimental results show that AST-Trans significantly outperforms the state-of-the-arts while being times more efficient than standard transformers 1 .\n\nneural networks start to raise more and more attention [20,[37][38][39]56]. Current state-of-the-arts all follow the Transformer-based encoder-decoder architecture [5,8,45,48,49] and can be trained end-to-end with code-summary pairs. Since the source code is highly structured and follows strict programming language grammars, a common practice is to also leverage the Abstract Syntax Tree (AST) to help the encoder digest the structured information. The AST is usually linearized by different algorithms like pre-order traversal [21], structure-based traversal (SBT) [18] and path decomposition [4], then fed into the encoder. Several works also proposed architectures specific for tree encoding like tree-LSTM [11,51].\n\nHowever, the linearized ASTs, as containing additional structured information, are much longer than their corresponding source code sequence. Some linearization algorithms can further increase the length. For example, linearizing with SBT usually makes the size times longer. This makes the model extremely difficult to accurately detect useful dependency relations from the overlong input sequence 2 . Moreover, it brings significant computational overhead, especially for state-of-the-art Transformer-based models where the number of self-attention operations grows quadratically with the sequence length. Encoding ASTs with tree-based models like tree-LSTM will incur extra complexity because it needs to traverse the whole tree to obtain the state of each node.\n\nIn this work, we assume that the state of a node in the AST is affected most by its (1) ancestor-descendent nodes, which represent the hierarchical relationship across different blocks, and (2) sibling nodes, which represent the temporal relationship within one block. We show an example of code summarization in Figure 1. As can be seen, we need the ancestor-descendent relationship to understand the high-level procedure, and the sibling relationship to understand the low-level details within a block. Capturing these two relationships are enough for producing the summary and modelling the full attention among all nodes is unnecessary.\n\nBased on this intuition, we propose AST-Trans, a simple variant of the Transformer model to efficiently handle the tree-structured AST. AST-Trans exploits ancestor-descendant and sibling relationship matrices to represent the tree-structure, and uses these matrices to dynamically exclude irrelevant nodes in different selfattention layers. The absolute position embedding from the original Transformer is replaced with relative position embeddings defined by the two relationship matrices to better model the dependency. We further describe several implementations of the proposed AST-Trans and have a comprehensive analysis of their computational complexity. In short, the contributions of this paper are as below:\n\n\u2022 We propose AST-Trans that can efficiently encode long AST sequences with linear complexity, in contrast with the quadratic complexity of the standard Transformer. \u2022 We perform a comprehensive analysis, with both theoretical and empirical evidences, on the computational complexity of different implementations. \u2022 We validate our proposed model on two datasets of Java and Python. Experimental results show that AST-Trans outperforms the state-of-the-arts by a substantial margin. \u2022 We compare representative methods for AST encoding and discuss their pros and cons.\n\n\nPaper Organization\n\nThe remainder of this paper is organized as follows. Section 2 presents background knowledge on the Transformer and AST. Section 3 elaborates on the details of AST-Trans, section 4 presents its different implementation and the complexity is analyzed in section 5. Section 6 explains the experimental setup and analyzes the results. Section 7 discusses threats to validity. Section 8 surveys the related work. Finally, section 9 concludes the paper and points out future research directions.\n\n\nBACKGROUND\n\nTransformer. The Transformer architecture was initially proposed for neural machine translation [49]. It consists of multi-head stacked encoder and decoder layers. In each encoder stack, the inputs first flow through a self-attention sublayer, and then are fed into a position-wise feed-forward network followed by a layer normalization. The decoder has a set of the cross-attention layers to help the decoder focus on relevant parts of the input sequence. The Transformer architecture removes the recurrence mechanism in favor of the self-attention. As each word in a sentence simultaneously flows through the encoder and decoder stack, the model itself does not have any sense of the word order. Therefore, a position embedding is added to each word embedding to inform the order information. Abstract Syntax Tree (AST). An Abstract Syntax Tree (AST) uniquely represents a source code snippet in a given language and grammar [4]. The leaves of the tree are terminals, usually referring to variables, types and method names. The non-leaf nodes are non-terminals and represent a restricted set of structures in the programming language, e.g., loops, expressions, and variable declarations. For example, in Figure 1, variables (such as NameLoad(x)) are represented as terminals of AST. Syntactic structures (such as Compare) are represented as non-terminals. Since the variable and method names can be rather freely defined, directly processing the source code can be challenging. Its corresponding AST, due to its strict structure, often serves as substitute when encoding the source code.\n\n\nAST-TRANS\n\nThis section details our proposed AST-Trans. For an AST, it will be firstly linearized into a sequence. Then the ancestor-descendent and sibling relationships among its nodes will be denoted through  (0) ) constant(0) ( Lt ) Lt ( NameLoad(x) ) NameLoad(x) ) Compare ( body ( constant(0) ) constant(0) ) body ( orelse ( NameLoad(x) ) NameLoad(x) ) orelse ) IfExp ) Return two specific matrices. Based on the matrices, we replace the standard self-attention with tree-structured attention to better model these two relationships. Irrelevant nodes are dynamically ruled out to reduce computational cost. We will first introduce different linearization methods (section 3.1), then explain the construction of two relationship matrices (section 3.2), and finally present the tree-structure attention to utilize the matrices(section 3.3).\n\n\nAST Linearization\n\nIn order to encode the tree-shaped AST, it first needs to be converted into a sequence with a linearization method. There are the three most representative linearization methods used in current works:\n\n(1) Pre-order Traversal (POT): It visits the tree nodes with preorder traversal. Sequences obtained by pre-order traversal are lossy since the original ASTs cannot be unambiguously reconstructed back from them. (2) Structure-based Traversal (SBT): It adds additional brackets [18] to indicate the parental-descendent relationship such that each sequence can be unambiguously mapped back to the AST, but it also doubles the size of the linearized sequence. (3) Path Decomposition (PD): It represents the AST by concatenating the path between two random leaf nodes. The total number of paths can be too large for computing and therefore random sampling is needed [4]. Table 1 shows the AST in Figure 1 linearized with the above three different methods. For POT and SBT, the linearized trees can be directly fed into the encoder. For PD, the average total number of paths can be over 200, concatenating them all to train is infeasible [4]. In practice, mean pooling is run over the states of each path such that each path has one unique representation. The decoder only attends to these unique representations of paths instead of specific nodes within paths. This can affect the model when copying user-defined names (in leaf nodes) is needed.\n\nWe adopt the simplest POT linearization for our model. We show that it has already achieved SOTA results and more complex linearization methods like SBT do not help. PD does not apply to our model since it treats one path as a whole. We will show in section 6.3 that this leads to poor performance in code summarization.\n\n\nRelationship Matrices\n\nWe define two kinds of relationships between nodes in the tree that we care about: ancestor-descendant ( ) and sibling ( ) relationships. The former represents the hierarchical information across blocks, and the latter represents the temporal information within one block. Specifically, two nodes have the ancestor-descendant relationship if there exists a directed path from root node that can traverse through them. Two nodes have the sibling relationship if they share the same parent node.\n\nWe use two position matrices \u00d7 and \u00d7 to represent the ancestor-descendent and sibling relationships respectively. is the total number of nodes in AST. We denote the th node in the linearized AST as .\n\nis the distance of the shortest path between and in the AST. is horizontal sibling distance between and in the AST if they satisfy the sibling relationship. If one relationship is not satisfied, its value in the matrix will be infinity. Note that we consider the relative relationship between two nodes, which means = \u2212 and = \u2212 if a relationship exists between and .\n\nFormally, we use SPD( , ) and SID( , ) to denote the Shorted Path Distance and horizontal SIbling Distance between and in the AST. The values in the relationship matrices are defined as:\n= SPD( , ) if |SPD( , )| \u2264 \u221e otherwise = SID( , ) if |SID( , )| \u2264 \u221e otherwise(1)\nis a pre-defined threshold and nodes with relative distance beyond will be ignored. We hypothesize that precise relative distance is not useful beyond a certain range. It can both constrain the computation complexity within a constant range and save memory space for storing the relative position embeddings. Figure 2 shows an example of generating matrix and , in comparison with the position matrix generated from a linear relationship, which is used in standard Transformers. In the next section, we will introduce how to use these two matrices to dynamically incorporate such relationship information through a tree-structured attention.\n\n\nTree-Structured Attention\n\nTree-structured attention is built on the standard self-attention with relative position embeddings and disentangled attention. It replaces the relative position embeddings derived from the linear relationship into the two matrices derived from the tree structure. Self-Attention. Standard self-attention transforms the input sequence x = ( 1 , . . . , ) ( \u2208 R which stands for the embedding of ) into a sequence of output vectors o = ( 1 , . . . , ) ( \u2208 R ).\n\nThe single-head self-attention [49] can be formulated as:\n= ( ) ( ) \u221a = =1 ( ) ( )(2)\nwhere , : R \u2192 R are query and key functions respectively, : R \u2192 R is a value function, is a scoring function (e.g. softmax or hardmax).\n\nRelative position embedding. Eq 2 is a content-only attention without any position information. The initial Transformer model uses absolute position embeddings to inform about the position. Shaw et al. [36] proposed replacing them with relative position embeddings, which has shown more effective in code summarization tasks [1]. The relative position ( , ) reflects the pairwise distance between and . Denote as the max relative distance, ( , ) \u2208 [0, 2 ] can be defined as:\n( , ) = \u23a7 \u23aa \u23aa \u23a8 \u23aa \u23aa \u23a9 0 for \u2212 \u2264 \u2212 2 for \u2212 \u2265 \u2212 + others.(3)\nIn this way, we can map each relative distance into an embedding representation. The relative position embeddings can be added on top of Eq 2 to inform the pairwise distance. Disentangled Attention. Disentangled Attention [16] uses relative position embedding as bias in self-attention process. Each word is represented using two vectors that encode its content and relative position in an disentangled way. The attention computation is then divided into three parts: content-to-content, contentto-position and position-to-content, defined as:\n, = ( ) ( ) content-to-content + ( ) ( , )\ncontent-to-position\n+ ( , ) ( ) position-to-content(4)\nwhere , \u2208 R (2 +1)\u00d7 represent the query and key projection matrices of relative positions. ( , ) is the ( , )-th row of and ( , ) is the ( , )-th row of respectively. The last two items, i.e., content-to-position and position-to-content, are used to measure the relative positions between a word pair.\n\nBesides, for content-to-position computation, as all possible relative positions are always in [0, 2 ], the scores of query content ( ) to all key positions can be first computed as ( ) , and then gathered into\u02dcwith ( , ) as index. In this way, The relative position embedding can be reused for all query contents and thus reduce the space complexity to (2 ) .\n\nAttention with Tree-Structured Relationships. Our method essentially replaces ( , ), the relative distance defined under the linear relationship, with ( , ) where stands for either the ancestor-descendent relationship or the sibling relationship in the tree structure. ( , ) is defined as:\n( , ) = + + 1 if \u2208 [\u2212 , ] 0 if = \u221e(5)\nrefers to either or defined in Eq 1. As there are two kinds of relationships, we consider only one relationship in each head so that it will not add any additional parameter on top of the standard Transformer. \u210e heads will use ( , ) and the rest \u210e heads will use ( , ). Information from the two relationships will be merged together through multi-head attention. We then replace ( , ) in Eq 4 with ( , ) in Formula 5, and apply a scaling factor of 1 \u221a 3 o\u00f1 , (because it has 3 items). The final output vector is computed as in Eq (6), where represents the value project matrix of relative distances and is the -th row of .\n= \u2208{ | ( , ) >0} (\u02dc, \u221a 3 )( ( ) + )(6)\nNote that we only compute the attention weights for node pairs where ( , ) > 0), which is similar to the idea of sliding window [7] and can reduce the time and space complexity of the selfattention process. We will discuss its implementation and analyze its complexity in sections 4 and 5 respectively.\n\n\nEFFICIENT IMPLEMENTATION\n\nA limitation of the full attention mechanism in standard Transformers is the computational and memory cost that grows quadratically with the sequence length. AST-Trans we proposed can alleviate this problem since the attention scores only need to be computed for node pairs where ( , ) > 0. Nevertheless, a memory and computational efficient implementation of AST-Trans that supports parallel processing is non-trivial. The essence of AST-Trans is similar to previous works that apply sliding windows to constrain the attention within a fixed range [7,54]. With sliding windows, the node pairs in the sequence data can be planned into a linear distribution (by ignoring node pairs with ( , ) = 0 or 2 \u2212 1) and computed in parallel with matrix partitioning. However, this technique does not apply to us since the position distribution of relevant nodes changes with every tree structure, which makes matrix blocking infeasible. In this section, we present the following 5 alternative implementations of AST-Trans and discuss the pros and cons:\n\nMask. Mask out the attention scores where ( , ) = 0 after computing the full attention among all nodes. It has the same quadratic time and space complexity as in the standard Transformer.\n\nLoop. Loop over node pairs where ( , ) > 0 and compute the attention scores. It is memory and computational efficient but does not support parallel processing.\n\nSparse. We can store as a sparse tensor ( ) and deep learning frameworks, such as Pytorch, can automatically skip operations with zero elements when multiplying a sparse tensor with a normal tensor. The mask operation can be optimized (for example, contentto-position attention scores in Eq 4 can be computed by gathering ( ) with ( )). However, it can only apply to content-toposition and position-to-content. For content-to-content, we still have to use the Mask or Loop strategy since the production of two sparse tensors is not directly supported.\n\nGather with COO (GC). On the basis of Sparse, the contentto-content computation can be optimized by additional gather operations. The core idea of GC is to put query-key pairs that need to be computed into one-to-one correspondence, and store them as dense matrices. Coordinate format (COO) is a common way to store sparse tensors, where only non-zero elements are stored as tuples of To reduce the number of gather operations in GC, we can add a matrix decomposition operation on top of it. First, we decompose by such that each sub-matrix contains only node-pairs with the same relative distance . An example is shown in Figure 3, where the original contains 3 distinct values and we decompose it into 3 sub-matrices accordingly. We transfer each sub-matrix into its COO format and use to indicates the sub-matrix with = . For each sub-matrix , we gather content embeddings of nodes by:\n= ( )[ ; :], = ( )[ ; :]\nwhere indicates the query content ordered by , and represents the key content ordered by . The attention scores can then be computed as:\n= ( + ) ( + ) \u2212 ( )\nwhere corresponds to the attention scores of node pairs in . Note that is a vector of the same shape as . By padding all to the same length, the attention scores can be computed in parallel and the final attention scores equal to the sum of all :\n= 2 +1 =1\nThere are 3 benefits of this approach compared with GC:\n\n\u2022 and can be reused, as each and have the same relative distance . The position embeddings of can be directly added into the content without gather operations.\n\n\u2022 Only a quarter of number of gather operation is needed (discussed in 5.3). \u2022 Only one dot production is required, as the second can be reused and only ( + ) ( + ) needs to be calculated. See Appendix A for the complete algorithm.\n\n\nCOMPLEXITY ANALYSIS\n\nIn this section, we will discuss the best, worst and average complexity of 5 implementations mentioned above. We use FLOPs (floating point operations) to measure the computational complexity. The considered operations includes: matrix multiplication, matrix dot production, add and gather operation which are the main operations involved for the attention computation. FLOPs of these operations are listed below:\n( + ) = ( \u2212 1); ( [ ; :]) = | | * ( ) = 2 + ( \u2212 1) ( \u00d7 ) = * ( )(7)\nwhere and are two matrices with shape [ , ], [ ; :] indicates gather with as the index, | | is the number of elements in . We will focus our analysis on attention heads using the ancestordescendent relationship ( ), but similar ideas can be used to analyze the sibling relationship ( ) straightforwardly. As the complexity is related to the number of non-zero elements in (denoted with | > 0|). We first analyze the range of | > 0|, then present the complexity of each implementation.\n\n\nRange of | > 0|\n\nTheorem 5.1. For any directed tree , let E(i) represent the number of paths in with length , represent the length of the longest path in , we have:\n(1) > (2) > \u00b7 \u00b7 \u00b7 > ( )\nProof. Assuming there are nodes in the tree, and the root node is at level 1. Define as the number of nodes at level . For each node at level , if \u2212 > 0, there exists one path of length ending with this node, otherwise no such path exists. Hence, ( ) = \u2212 =1 and > 0. Therefore we must have ( ) > ( + 1).\n\nTheorem 5.2. Every tree with nodes has exactly \u2212 1 edges.\n\nProof. Imagine starting with isolated nodes and adding edges one at a time. By adding one edge, we will either (1) connect two components together, or (2) close a circuit. Since a tree is fully connected and has no circuit, we must add exactly \u2212 1 edges.\n\n\nLeast upper & Greatest lower bound.\n\nLet (0) = denote the number of nodes in a tree. We have | > 0| = (0) + 2( (1) +\n\n(2) + . . . ( )) since we consider both positive and negative distance in . Based on the above two theorems, we can have: \n( ) \u2264 ( \u2212 1) \u2212 1 \u2264 . . . (0) \u2212 = \u2212| > 0| \u2264 + 2( \u2212 1 + \u2212 2 + . . . \u2212 ) = ( \u2212 )(2 + 1)\nIt is the least upper bound for the ancestor-descendent relationship and is achieved only when each node has strictly one child node. The greatest lower bound can be achieved when the tree's depth is 2. In this situation, ( ) = 0 for \u2265 2 and | > 0| = 3 \u2212 2.\n\nAverage. We can use the Pr\u00fcfer sequence [35] to simulate random trees so we can estimate the average of | > 0| with different tree structures. The tree size is set in the range of [50,500] and the out-degree of each node is randomly selected from 1 to \u2212 1 (controlled by the max value in Pr\u00fcfer sequence). We did 1,000 simulation experiments and Figure 4 shows the result.\n\nThe average | > 0| when is sampled from a uniform distribution in [1, 50] is 1.16 . We can see that the coefficient in Figure 4 gradually decreases. For larger , the average | > 0| will be much smaller than the upper bound of (2 + 1)( \u2212 ). for content-toposition and position-to-content, and 2 add operations are used for final score computation. The complexity is ( 2 + (2 + 1) ) * ( 2 + \u2212 1) + 2 2 + \u2212 1.\n\n\nMask & Loop & Sparse & GC\n\nLoop As loop only computes non-zero elements in , the complexity includes 1 dot production of | > 0|( 2 + \u2212 1) and 2 add operations | > 0| * 2( \u2212 1), and equals to | > 0|( 2 + 3 \u2212 3). Sparse's complexity is same as Mask apart from the gather operation with index shape | > 0| (the time complexity for gathering sparse tensor as index equals to the number of non-zero elements in it), which equals to ( 2 + (2 +1) ) * ( 2 + \u22121) +2| > 0|+ \u22121. GC The complexity in GC is all related to | > 0|. It contains 4 gather operations, 3 dot production and 2 add operations, which leads to the complexity of | > 0|( 2 + 3 + 4) + 2(2 + 1) .\n\n\nGDC\n\nThere are two implementation details in GDC to optimize the time and space complexity. Firstly, in a tree, if \u2265 + 1, the decomposed sub-matrix has at most one non-zero value in each row. (for example, each non-root node has exactly one parent node in Figure 3.) We can fix to [0, 1, . . . , \u2212 1] and only store the corresponding . When < + 1, as the relationship is symmetric, can be represented with 2 +2\u2212 . Based on this, when \u2265 + 1, the query content does not need to be gathered  (as is the same index of query), and when < +1, the key content does not need to be gathered. Hence, we only need (2 +1) gather operations from content. Secondly, padding positions do not need to be computed in dot production as the padding positions of both and are the same. After adding the position bias, all and can be packed before dot production, then unpacked to their original length afterwards. By this way, we only need to compute related node pairs with one dot production.\n\nIn consequence, the complexity of GDC includes (2 + 1) gather operations, 1 dot production with shape [| > 0|, ] and 3 add operations with shape [| > 0|], which equals to | > 0|( 2 + \u2212 1) + (6 + 3) + (2 + 1) . For better comparison, we also show the theoretical complexity in Figure 5 under the hyper-parameters in our experiments. As can be seen, loop has the lowest complexity but cannot be parallelized. mask and sparse grow quadratically with the AST size. GDC slightly outperforms GC and has a complexity close to loop.\n\n\nEXPERIMENTS\n\nIn this section, we first explain the experimental setup, evaluation metrics and baseline approaches, then report the main results and perform ablation studies. The runtime speed and memory cost of different implementations are provided for comparison. Finally, we present a qualitative analysis and discuss the future directions.\n\n\nExperimental Setup\n\nDatasets. Experiments are conducted on the two public code summarization benchmarks, one in Java [19] and the other in Python [51].\n\nTo ensure the quality of comments, we filter the comments with less than 4 words, constructors, setters, getters, and tester methods, same as in Shido et al. [41]. When the comment has two or more sentences, only the first sentence is kept as the description of the   Table 2 shows the statistics of the datasets. We also count the distribution of relative distances in Fig 6. As can be seen, most ancestor-descendent and sibling relationships are within the range of 5 and 10 respectively. Pre-processing. First, we pre-process the summaries by removing the punctuations. Next, we split multi-words, such as \"gettabletypes\", in summaries with wordninja 3 since their corresponding tokens in the source code are split too [53]. We also split the leaf nodes in ASTs into sub-tokens if they are in form of the CamelCase or snake_case. The split nodes are treated as new children of the original parent node. Finally, we reverse the children of the root node to prevent the important information, such as function names or parameters, from being cut when the size of input AST exceeds the maximum size allowed.\n\n\nHyper-parameters.\n\nIf not specified, the maximum size of AST is set to 200 for all experiments, and the vocabulary sizes of both ASTs and comments are set to 30, 000. We use 4 layers of stacked encoder-decoder and set the hidden size = 256, = 32. For each attention layer, we set \u210e = 1 and \u210e = 7. The max relative distance for ancestor-descendant/sibling relationship is set to 10/5 respectively. Feed-forward inner-layer dimension is 2048 and the activation function is gelu [17]. While training, the batch size is 128 and the maximum epochs is 500. Models are trained using the 3 https://github.com/keredson/wordninja AdamW optimizer [28] with = 1 \u22123, 1 = 0.9, 2 = 0.999, = 1 \u2212 6, label smoothing with = 0.1 [46] and dropout probability [44] of 0.2. The patience in the early stopping mechanism [32] is set to 20 and we select the model based on the BLEU in the validation set 4 . Evaluation Metrics. We evaluate the performance with corpus BLEU [33], METEOR [6], and ROUGE-L [27].\n\nThe experiments used the GPUs provided by Aliyun, which use EFLOPS [9] architecture and ACCL [10]. EFlops architecture improves the scalability and efficiency of commodilty clusters (CoW), and ACCL bring the performant efficiency of EFlops architecture to general cluster systems and Cloud scenarios.\n\n\nBaselines\n\nWe compare the proposed AST-Transformer with 16 baseline methods. They can be divided into 5 groups based on the input type: 1: Code. Models with the code as input. It treats code as plain text and does not leverage ASTs. Code-NN [20] used RNN while BaseTrans [1] used the Transformer. On the basis of Code-NN, Dual Model [53] used dual learning to train code summarization and generation together. API+CODE [19] used multi encoders to encode code along with the API call sequence. To make up for the lack of structural information, Code-Transformer [57] additionally adds four structure distances, including two kinds of distance mentioned in Sec 3.2, to the code tokens and does attention computation separately for each kind of distance. Differently, it does not distinguish embeddings of different relations and uses sine and cosine functions to represent distance embeddings.\n\n2: AST(Tree). Models with the AST as input and encode it with tree-specific encoders. There are two main types of such encoders. One uses Tree-LSTM, such as Tree2Seq [11] and RL+Hybrid2Seq [51]. RL+Hybrid2Seq adds the code information and deep reinforcement for training. The other treats the AST as graph and encodes it with graph neural network (GNN) models. We consider three kinds of GNN models including GCN [22], GAT [50] and Graph-Transformer [40]. The edges fed to GNN includes the ancestordescendant and sibling edges, distinguished by the edge attributes.\n\n3: AST(PD). Models with the AST linearized with path decomposition as input. Path representation needs to be encoded from the nodes, then the whole AST representation is encoded from the path representations. Code2Seq [4] is the first approach using PD, and it used two LSTM models to encode hierarchical networks. For fairness of comparison, we also design a new baseline Code2Seq(Transformer) by replacing these two LSTM models with the Transformer. 4: AST(SBT). Models with the AST linearized with Structurebased Traversal as input. DeepCom [18] is the first work that uses AST (SBT) as input, which encodes it with LSTM. We design a new baseline Transformer (SBT) that encodes AST (SBT) with the Transformer. AST-Trans(SBT) is our proposed model that inputs SBT with relationship matrices.\n\n\n5: AST(POT).\n\nModels with the AST linearized with pre-ordertraversal as input. Transformer (POT) is the standard Transformer architecture with AST (POT) as input and AST-Trans is our proposed model with tree-structured attention.\n\nAll Transformer-based models are based on the relative position embeddings with disentangled attention mentioned in Section 3.3 with the same number of parameters. The same hype-parameters are used through the way for a fully fair comparison.\n\n\nMain Results\n\nThe main result of AST-Trans and the baselines are presented in Table 3  Code vs AST (Tree) vs AST (linearized). Apart from AST-Trans, on both two datasets, using GNNs to encode AST (Tree) achieved the best results. The reason is that the AST has both structural and semantic information, and the other two input types both lose part of the structural information. All three variants of GNNs achieve similar results and outperform the Tree-LSTM in encoding the AST (Tree). Compared with taking the linearized AST as input, models only using the code perform better on the Java dataset but worse on the Python dataset. This could be related to the code length. As code and corresponding ASTs in Python are relatively shorter, encoding ASTs is more effective than in the Java dataset. Therefore, models using linearized ASTs, with the help of additional structural information, are able to outperform models using only the code.\n\n\nAST(PD) vs AST(SBT) vs AST(POT).\n\nAmong three linearization methods, when using the Transformer encoder/decoders, AST (SBT) performs the best on the Java dataset and AST (POT) performs the best on the Python dataset. AST(SBT) and AST(POT) both have their own advantages. AST(SBT) maintains more structural information than AST(POT) while AST(POT) has the shortest length 5 The results of BaseTrans [1] in the Python dataset are lower than reported in the paper (-6.75 BLEU, -3.44 METEOR and -7.78 ROUGE), then we set max relative distance to 16 (kept the same as original paper) and get 27.27 (-5.25) BLEU, 15.90(-3.87) METEOR, 38.58 (-8.15) ROUGE-L. This reduction may be because that we additionally segment multi-words in comments.  among these three linearization methods. Using the AST (PD) as input leads to poor performance on both datasets. There are two main reasons. On the one hand, AST(PD) method was first proposed for method name completion. Method names are much shorter than the code summaries, and do not include many details. PD linearization extracts features from paths, which aggregates high-level characters but ignores the detailed information in the node. However, code summarization requires more detailed information in the code such as the type of the return value, which is stored in the leaf nodes. On the other hand, Code2Seq(Transformer) uses a hierarchical network and the amount of trained parameters is much larger. It is thereby harder to converge than Transformer(SBT) and Transformer(POT). Impact of relationship matrix . We compared the performance of three kinds of inputs with or without the relation matrix : Code-Transformer vs BaseTrans, AST-Trans (SBT) vs Transformer (SBT) and AST-Trans (POT) vs Transformer(POT). Results show that adding improves the performance for all these inputs and AST-Trans (POT) performs the best. This is because Code-Transformer ignores non-leaf node information, and AST-Trans (SBT) stores duplicate information, resulting in too long sequence length. AST-Trans (POT) maintains a short sequence length without losing necessary structural or semantic information.\n\nAST-Trans vs GNN. AST-Trans outperforms GNNs, the bestperformed baseline model in both datasets. With the help of relationship matrix, AST-Trans includes additional relative distance information. Nodes can perceive information from its -distance neighbors at each layer. For GNN, however, each node needs hops to propagate information from these neighbors. In addition, AST-Trans uses multi-head mechanism to compute different relationships in different heads, while all relationships, distinguished by edge attribute, are calculated together in GNNs. AST-Trans also uses extra feed-forward layers and residual connections in the encoder, which could help improve the model generalization.  \n\n\nAblation studies\n\nWe conducted ablation studies on four hyper-parameters: use of each relationship, number of heads used for ancestor-descendant (\u210e ) and sibling relationships (\u210e ), max relative distance and the number of layers. In every study, apart from the hype-parameter that needs to be analyzed, we keep the rest settings unchanged. Use of two relationships. We verified the impact of using ancestor-descendant or sibling relationship separately in Table 4. Results show that the performance is achieved when using them all. However, using one of the relationships alone can already achieve close results and outperform all previous baselines.\n\nNumber of attention heads. We change the number of heads used for the ancestor-descendant relationship \u210e from 0 to 8 and fix the total number of heads to 8. As can be seem from Table 5, the best performance is obtained with \u210e = 1 and \u210e = 7, but there is no significant difference among all combinations of \u210e and \u210e . Even when one relationship is missing (\u210e = 0 or \u210e = 0), the effects are still marginal. However, when both relationships are removed \u210e = \u210e = 0, the performance drops a lot. We conjecture that this phenomenon is related to the characteristics of AST. Knowing about one relationship can help the model \"guess\" the other relationship properly. For example, the node \"Compare\" can be the child node of \"WhileExp\", \"IFExp\" or \"SwitchExp\", etc, but when it is the sibling of node \"Case\", it can only be the child of node \"SwitchExp\". The information about its parent can be \"guessed\" in attention computation with its sibling \"Case\". Similarly, node \"NameStore\" can only appear on the left side of a statement, and nodes with the same parent as it must be its right siblings. Messages of these siblings can be passed to \"NameStore\" through their common parent. However, there are many cases that the \"guess\" will not be successful.  Max relative distance We analyze the impact of the max relative distance in Table 6 . According to Table 6, the out-degree and depth of most nodes in AST is in [0, 5] and [0, 10]. Therefore, the max relative distance of ancestor-descendant ( ) and sibling relation ( ) are selected from [1,5,10] and [1,3,5] respectively. Results show that as the relative distance grows, the performance improves too, suggesting a wider view of nodes in AST relationships is helpful. However, the improvement is marginal and even with = 1, the model performance can already outperform all other baselines. This might be ascribed to the multi-layer stacked encoders. Even for = 1, longer-distance nodes can still be attended to indirectly on upper layers. In practice, can be set as a hyperparameter to balance the performance-efficiency trade-off.\n\nNumber of Layers Finally, we perform ablation study by varying the number of layers, and the results are presented in Table 7.\n\nIn our experiments, we observe that a deeper model (more layers) performs better, but the improvement saturates after 4 layers. Even with the batched parallelism in GPUs, the implementation of mask and sparse are still slower than GDC and GC while requiring significantly more memory cost. GDC is faster and with less memory usage than GC. The main reason is that GDC uses one quarter of gather operations compared with GC. Loop shows a linear growth in memory usage with AST size, but its time cost is much higher as it does not support parallel operations. When the AST size grows further, we can expect the difference across implementations will become larger and larger. \n\n\nComplexity analysis\n\n\nVisualization and Qualitative Analysis\n\nVisualization. We further visualize the relative position representations of ancestor-descendant ( ) and sibling ( ) relationships in Fig 8. As can be seen, the variance of relative position embeddings in is much larger than in . It implies that our model is not sensitive to the relative distance between ancestor and descendant nodes, as the embeddings are almost the same regardless of the positions. In contrast, the variance for sibling nodes is relatively large, and the model can distinguish the sibling nodes with different relative distances. In addition, the relative embeddings in are demarcated between the upper and lower part, suggesting a clear distinction between ancestor and descendant nodes. It shows that our model pays more attention to direction rather than distance in . It is likely that the exact distance between sibling nodes are more important than that between ancestor-descendant nodes in ASTs. Qualitative analysis. We provide a couple of examples for qualitative analysis in Table 8. It can be observed that AST-Trans generates the closest summary to the reference, and lack of or hurts the quality of summarization. In the first case, the key information is the connection between the sibling nodes method call (\"addAll\") and parameter (\"actions\"). Both AST-Trans and AST-Trans w/o generates the summary as a batch add operation, while AST-Trans w/o misunderstands it as \"adds an action\". On the contrary, the meaning of the third case is to get job by the tag first then delete it. The order of execution is controlled by the ancestor-descent relationship (the method call \"get\" is the child node of \"delete\"), and AST-Trans w/o just ignores the \"get\" operation. The summaries of AST-Trans w/o and w/o are both correct in the second case. The statements of the second case are relatively simple and ignoring the order of statements will not affect the function comprehension.\n\n\nTHREATS TO VALIDITY\n\nThere are three main threats to the validity of our evaluation. Firstly, many public datasets are proposed to explore code summarization. AST-Trans w/o S: delete a job and return tag AST-Trans w/o A: delete a job objects AST-Trans: delete a job based on its tag Human Written: deletes a job entry based on its tag\n\nWe select two widely used ones to evaluate the proposed AST-Transformer, but they may not be representative of other programming languages. Secondly, to ensure a fair comparison as much as possible, we build baselines on the top of the same Transformer architecture. The architecture and hyperparameter choice might be sub-optimal for certain approaches 6 . Finally, there will be a certain gap between the automatic evaluation and the manual evaluation of the summarization results. We select three different automatic evaluation methods to avoid bias as much as possible.\n\n\nRELATED WORKS\n\nCode Summarization. Most approaches on code summarization frame the problem as a sequence generation task and use an encoderdecoder architecture. The only difference between it and traditional machine translation is that programming languages are unambiguous and follow rigid grammar rules. Most approaches either treat the source code as natural language (i.e., a sequence of tokens without specified structures), or utilize its structural information with the help from ASTs or other parsed forms. To encode the code sequence, there exist many encoder architectures like CNN [3], RNN [20,55] and the Transformer [1]. To leverage the tree-structured AST, tree-based models such as Recursive NN [26], Tree-LSTM [41,51] and Tree-Transformer [15,52], are used to encode AST directly. As tree is a special kind of graph, graph-based approaches [2,12,23] can also be used to encode ASTs. Some works also combine the code token sequence with the AST and observe improvement [23][24][25]. Our approach only needs the linearized AST and can be built upon the Transformer architecture. More importantly, it restricts the attention range and makes it possible to encode very long AST sequences. Tree-based Neural Networks. The existing tree-based neural networks can be grouped into two categories depending on their inputs:\n\n(1) The models that directly take the tree as input [15,31,34,47]. These models are strongly coupled with the tree structure, and the calculation process needs to be performed simultaneously with the tree traversal. Since trees generally have different shapes by nature, parallization of training these models is non-trivial. (2) The models that take the sequence(s) extracted from the tree as input, such as the sampled paths in the tree [4,21], the traversal sequence with tree positional embedding [42] or the structure based traversal (SBT) sequence [18]. Taking sampled paths as input is with a certain degree of randomness and instability, and the method of tree positional embedding ignores the concept of paths in the tree (all nodes, even if not related, will participate in the calculation together). Our method improves from these two methods, which can be guaranteed that each node exchanges message on and only on all paths containing it.\n\n\nCONCLUSION\n\nIn this paper, we present AST-Trans which can encode ASTs effectively for code summarization. In AST-Trans, each node only pays attention to nodes which share the ancestor-descendent or sibling relationships with it. It brings two benefits: (1) the model is given an inductive bias and will not get lost in the overlong AST sequence, and (2) it can reduce the computational complexity from quadratic to linear. The latter makes it possible to encode long code sequence, e.g., a whole file, which is prohibitively expensive for standard Transformers. We conduct comprehensive experiments, showing that AST-Trans achieve SOTA results on two popular benchmarks while significantly reducing the computational cost. We believe the basic idea of AST-Trans can also be applied in other structured data like data dependence and control flow graphs. The code is made publicly available to benefit the relevant research. In future work, we plan to improve AST-Trans by incorporating more features of the code snippet, such as API sequence and node type, into the self-attention mechanism.\n\n\nACKNOWLEDGMENTS\n\nThis work is supported by National Natural Science Foundation of China (61802167,61802095) ,Natural Science Foundation of Jiangsu Province (No.BK20201250),Cooperation Fund of Huawei-NJU Creative Laboratory for the Next Programming, and NSF award 2034508. We thank Alibaba Cloud for its high-efficient AI computing service from EFlops Cluster. We also thank the reviewers for their helpful comments. Chuanyi Li and Jidong Ge are the corresponding authors. or better re-implementation, we also show the algorithm of GDC. line 1-10 describes the attention score computation process.\n\n\nA ALGORITHM OF GDC\n\n,\u02dcand\u02dcare reshaped to [2 + 1, , ]. Note that the attention scores\u02dchave a different shape with traditional attention scores, so we redesigned the softmax function in line 11-16. The attention scores belonging to the same query vector, distinguished by [ * + ], are added together as\u02dc. Then the softmax function can be formed as\u02dcdivide by\u02dc. Finally in line 17-21, relative distance bias is added to the value context, and then is multiplied with the attention scores\u02dc.\n\n\nB THE INFLUENCE OF MODEL SELECTION STRATEGY\n\nThe results reported in the paper come from the model with best BLEU score in the validation dataset. We then separately select two other models with the best METEOR, and ROUGE-L score in the valid dataset, and then evaluate their performances on test dataset. Results in Table 9 show that the model selection strategy indeed influences the performance. This may explain why that the improvement of AST-Trans is inconsistent in different metrics.\n\nFigure 1 :\n1Example of code-AST-summary triples. We mainly need to understand the ancestor-descendent and sibling relationships in the AST to generate a summary.\n\nFigure 2 :\n2Example of generating position matrices for ancestordescendent (A) and sibling relationship (S). Position matrix generated from the linear relationship is used in standard Transformers.\n\nFigure 3 :\n3Decompose the relative distance matrix of the tree \"abcd\" with max relative distance = 1. element indices and the corresponding values. Let / denotes the list of row/column indexes, and denotes the list of values in the COO format of . We then use them as indexes to gather the query and key of content as: way, each column in the query content corresponds to the same column in the key content . Then we can use matrix dot production to compute attention scores: = + + where indicates dot production. is a vector and corresponds to the non-zero values in\u02dc(Eq. 4), and\u02dc[ [ ]; [ ]] = [ ]. The content-to-position or position-to-content can be computed the same as in Sparse, and the total number of gather operations in attention computation is 4 times of non-zero elements in : 2 for gathering the content and 2 for gathering the position. Gather with decomposed COO (GDC).\n\nFigure 4 :\n4| > 0| in case of random trees, the abscissa is the max relative distance and the ordinate is the non-zero elements in with the unit of ( ). The coefficient decreases with growing .\n\nMask contains 1\n1matrix multiplication with [ , ] \u00d7 [ , ] in content-to-content, 2 matrix multiplication with [ , ]\u00d7[ , 2 +1] and 2 gather operations with index shape [ , ]\n\nFigure 5 :\n5Theoretical complexity with = 5, = 32. loop has the lowest complexity but cannot be parallelized in practice.\n\nFigure 6 :\n6Distribution of relative distance in training setsmethod.\n\n5 .\n5AST-Trans outperforms all the baselines on all the three metrics. Specifically, it outperforms the best baseline by 3.61, 2.17 in BLEU, 1.65, 1.08 in METEOR and 0.87, 3.04 in ROUGE-L on the Java and Python datasets respectively.\n\n\nFor example, statements > and > have the same child nodes and can only be distinguished by sibling relationship, while statements = + ; = \u2212 and = \u2212 ; = + only differ in ancestor-descendant relationship. It could be that the testset does not have enough hard examples that need this fine-grained distinction or the current metrics are not enough to reflect the difference.\n\nFigure 7 :\n7Runtime and memory cost of five implementations with batch size=16. The cost of the mask implementation is equal to the standard Transformer, which grows quadratically with the AST size.\n\nIn Fig 7 ,\n7We analyzed the rum time and memory usage of different implementations mentioned in section 4. Different from the theoretical complexity which analyze the attention computation in isolate, operations in GPU can be computed in parallel, and there are other factors, e.g. decoder parameters, dependent libraries, vocabulary embeddings that all need memory usage. Therefore, the need for computing attention scores is only one part of it and leads to the gap between Fig 7 and 5, where the difference across implementations in Fig 7 is much larger. Nevertheless, the trend stays the same. Time and memory usage of GDC and GC both scale linearly with the AST size, while the cost of Mask and Sparse grows quadratically.\n\nFigure 8 :\n8Heatmaps of relative position representations. x-axis is the relative position representation and the y-axis is the relative positions. The variance for the sibling relation ( ) is much larger than that for the ancestor-descendent relation ( ).\n\nAlgorithm 1\n1Self-Attention with Relationship matrix Input: Hidden state , COO format of relationship martix , content functions , , , relative distance projection matrix , , for = 0, . . . , 2 + 1 do 3: for = 0, . . . , \u2212 1 do 4:\u02dc[ ; ; :] for = 0, . . . , 2 + 1 do 12: for = 0, . . . , for = 0, . . . , 2 + 1 do 18: for = 0, . . . , \u2212 1 do 19:\u02dc[ [ * + ]; :] = (\u02dc[ ; ; :] + [ ; :]) \u00b7\u02dc[ , ] 20: end for 21: end for Output:F\n\nTable 1 :\n1Linearized AST of the tree in Fig 1 with POT,SBT and PD.Methods \nLinearized AST sequence \n\nPOT \nReturn IfExp Compare NameLoad(x) Lt constant(0) body constant(0) orelse \nNameLoad(x) \n\nSBT \n\n( Return ( IfExp ( Compare ( constant\n\nTable 2 :\n2Statistics of Java and Python DatasetsPerspectives \nJava \nPython \n# of Train instances \n69,708 \n55,538 \n# of Validation instances \n8,714 \n18,505 \n# of Test instances \n8,714 \n18,502 \nAvg. # of tokens in code \n120 \n48 \nAvg. # of nodes in AST \n158 \n100 \nAvg. # of tokens in SBT \n632 \n402 \nAvg. # of tokens in summary \n18 \n9 \n\n\n\nTable 3 :\n3Comparison of AST-Trans with the baseline methods, categorized based on the input type. * means implemented by ourselves.Methods \nInput \nJava \nPython \nBLEU (%) \nMETEOR (%) \nROUGE-L (%) \nBLEU (%) \nMETEOR (%) \nROUGE-L (%) \nCODE-NN[20] \n\nCode \n\n27.6 \n12.61 \n41.10 \n17.36 \n09.29 \n37.81 \nAPI+CODE[19] \n41.31 \n23.73 \n52.25 \n15.36 \n08.57 \n33.65 \nDual Model[53] \n42.39 \n25.77 \n53.61 \n21.80 \n11.14 \n39.45 \nBaseTrans*[1] \n44.58 \n29.12 \n53.63 \n25.77 \n16.33 \n38.95 \nCode-Transformer*[57] \n45.74 \n29.65 \n54.96 \n30.93 \n18.42 \n43.67 \nTree2Seq[11] \n\nAST(Tree) \n\n37.88 \n22.55 \n51.50 \n20.07 \n08.96 \n35.64 \nRL+Hybrid2Seq[51] \n38.22 \n22.75 \n51.91 \n19.28 \n09.75 \n39.34 \nGCN*[22] \n43.94 \n28.92 \n55.45 \n32.31 \n19.54 \n39.67 \nGAT*[50] \n44.63 \n29.19 \n55.84 \n32.16 \n19.30 \n39.12 \nGraph-Transformer*[40] \n44.68 \n29.29 \n54.98 \n32.55 \n19.58 \n39.66 \nCode2Seq*[4] \nAST(PD) \n24.42 \n15.35 \n33.95 \n17.54 \n08.49 \n20.93 \nCode2Seq(Transformer)* \n35.08 \n21.69 \n42.77 \n29.79 \n16.73 \n40.59 \nDeepCom[18] \nAST(SBT) \n\n39.75 \n23.06 \n52.67 \n20.78 \n09.98 \n37.35 \nTransformer(SBT)* \n43.37 \n28.36 \n52.37 \n31.33 \n19.02 \n44.09 \nAST-Trans(SBT)* \n44.15 \n29.58 \n54.73 \n32.86 \n19.89 \n45.92 \nTransformer(POT)* \nAST(POT) \n39.62 \n26.30 \n50.63 \n31.86 \n19.63 \n44.73 \nAST-Trans \n48.29 \n30.94 \n55.85 \n34.72 \n20.71 \n47.77 \n\n\n\nTable 4 :\n4Ablation study on AST-Trans with/without and .Model \nDataset \nBLEU (%) \nMETEOR (%) \nROUGE (%) \nAST-Trans w/o A \nJava \n\n47.74 \n30.21 \n54.56 \nAST-Trans w/o S \n48.07 \n30.62 \n55.29 \nAST-Trans \n48.29 \n30.94 \n55.85 \nAST-Trans w/o A \nPython \n\n34.35 \n20.15 \n46.62 \nAST-Trans w/o S \n34.32 \n20.28 \n46.87 \nAST-Trans \n34.72 \n20.71 \n47.77 \n\n\n\nTable 5 :\n5Ablation study on \u210e and \u210e on Java Dataset.\u210e \n\u210e \n\nBLEU (%) \nMETEOR (%) \nROUGE-L (%) \n0 \n8 \n47.74 \n30.21 \n54.56 \n1 \n7 \n48.29 \n30.94 \n55.85 \n2 \n6 \n48.28 \n30.94 \n55.64 \n3 \n5 \n48.25 \n30.92 \n55.66 \n4 \n4 \n48.23 \n30.96 \n55.68 \n5 \n3 \n48.11 \n30.93 \n55.46 \n6 \n2 \n48.1 \n30.74 \n55.22 \n7 \n1 \n48.24 \n30.91 \n55.57 \n8 \n0 \n48.07 \n30.62 \n55.29 \n\n\n\nTable 6 :\n6Ablation study onand \non Java Dataset. \n\nBLEU (%) \nMETEOR (%) \nROUGE-L (%) \n0 \n0 \n36.34 \n23.83 \n45.58 \n1 \n1 \n46.95 \n30.33 \n54.24 \n5 \n1 \n47.45 \n30.11 \n54.28 \n5 \n3 \n47.82 \n30.29 \n54.62 \n5 \n5 \n48.14 \n30.77 \n55.45 \n10 \n5 \n48.29 \n30.94 \n55.85 \n\n\n\nTable 7 :\n7Ablation study on the number of layers on Java Dataset.BLEU (%) \nMETEOR (%) \nROUGE-L (%) \n1 \n46.11 \n29.36 \n53.07 \n2 \n47.68 \n30.53 \n54.97 \n3 \n47.41 \n30.04 \n54.07 \n4 \n48.29 \n30.94 \n55.85 \n5 \n47.8 \n30.39 \n54.61 \n6 \n48.31 \n30.58 \n55.09 \n\n\n\nTable 8 :\n8Qualitative examples. AST-Trans w/o S: adds a sub -action to the menu AST-Trans w/o A: adds the given actions to the list of actions AST-Trans: adds a collection of actions to the quick action view Human Written: adds a collection of actions to the quick action view AST-Trans w/o S: creates a new object initialized to the string object AST-Trans w/o A: returns a new instance of the object class AST-Trans: returns a new instance of the object Human Written: creates a new instance of a class def job_delete_by_tag(tag):Job.objects.get(tag=tag).delete() return (job_get_by_tag(tag) is None)public QuickActionView addActions(Collection <Action> actions){ \ncheckShown(); \nmActions.addAll(actions); \nreturn this; \n} \n\npublic java.lang.Object newInstance() { \nObject o = newInstanceImpl(); \nif(o == null){ \nthrow new InstantiationException(); \n} \nreturn o; \n} \n\n\n\nTable 9 :\n9Comparison of AST-Trans with different model selection strategy on Java Dataset.Model \nBLEU \nMETEOR \nROUGE-L \nAST-Trans(best_eval_BLEU) \n48.29 \n30.94 \n55.85 \nAST-Trans(best_eval_METEOR) \n47.02 \n31.90 \n55.72 \nAST-Trans(best_eval_ROUGE-L) \n46.92 \n29.99 \n57.01 \n\n\nIndeed, encoding the overlong AST with SBT even underperforms directly encoding the source code when using Transformer with relative position embeddings[1].\nWe also report the results with best METEOR and ROUGE-L in the validation set in Appendix B\nNevertheless, AST-Trans performs best among all reported results on both datasets.\n\nA Transformer-based Approach for Source Code Summarization. Saikat Wasi Uddin Ahmad, Baishakhi Chakraborty, Kai-Wei Ray, Chang, 10.18653/v1/2020.acl-main.449Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreaultthe 58th Annual Meeting of the Association for Computational LinguisticsOnlineAssociation for Computational Linguistics2020Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2020. A Transformer-based Approach for Source Code Summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault (Eds.). Association for Computational Linguistics, 4998-5007. https://doi.org/10.18653/v1/2020.acl-main.449\n\nLearning to Represent Programs with Graphs. Miltiadis Allamanis, Marc Brockschmidt, Mahmoud Khademi, 6th International Conference on Learning Representations. Vancouver, BC, CanadaConference Track Proceedings. OpenReview.netMiltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. 2018. Learning to Represent Programs with Graphs. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 -May 3, 2018, Con- ference Track Proceedings. OpenReview.net. https://openreview.net/forum?id= BJOFETxR-\n\nA Convolutional Attention Network for Extreme Summarization of Source Code. Miltiadis Allamanis, Hao Peng, Charles Sutton, Proceedings of the 33nd International Conference on Machine Learning. Maria-Florina Balcan and Kilian Q. Weinbergerthe 33nd International Conference on Machine LearningNew York City, NY, USA48JMLR.orgMiltiadis Allamanis, Hao Peng, and Charles Sutton. 2016. A Convolutional Attention Network for Extreme Summarization of Source Code. In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016 (JMLR Workshop and Conference Proceedings, Vol. 48), Maria-Florina Balcan and Kilian Q. Weinberger (Eds.). JMLR.org, 2091-2100. http://proceedings.mlr.press/v48/allamanis16.html\n\ncode2seq: Generating Sequences from Structured Representations of Code. Uri Alon, Shaked Brody, Omer Levy, Eran Yahav, 7th International Conference on Learning Representations, ICLR 2019. New Orleans, LA, USAUri Alon, Shaked Brody, Omer Levy, and Eran Yahav. 2019. code2seq: Generating Sequences from Structured Representations of Code. In 7th International Con- ference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. https://openreview.net/forum?id=H1gKYo09tX\n\nNeural Machine Translation by Jointly Learning to Align and Translate. Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio, 3rd International Conference on Learning Representations. Bengio and Yann LeCunSan Diego, CA, USAConference Track ProceedingsDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural Machine Translation by Jointly Learning to Align and Translate. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, Yoshua Bengio and Yann LeCun (Eds.). http://arxiv.org/abs/1409.0473\n\nMETEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments. Satanjeev Banerjee, Alon Lavie, Proceedings of the Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization@ACL 2005. Jade Goldstein, Alon Lavie, Chin-Yew Lin, and Clare R. Vossthe Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization@ACL 2005Ann Arbor, Michigan, USAAssociation for Computational LinguisticsSatanjeev Banerjee and Alon Lavie. 2005. METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments. In Proceedings of the Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Transla- tion and/or Summarization@ACL 2005, Ann Arbor, Michigan, USA, June 29, 2005, Jade Goldstein, Alon Lavie, Chin-Yew Lin, and Clare R. Voss (Eds.). Association for Computational Linguistics, 65-72. https://www.aclweb.org/anthology/W05- 0909/\n\nLongformer: The Long-Document Transformer. Iz Beltagy, Matthew E Peters, Arman Cohan, arXiv:2004.05150Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The Long- Document Transformer. CoRR abs/2004.05150 (2020). arXiv:2004.05150 https: //arxiv.org/abs/2004.05150\n\nOn Training Instance Selection for Few-Shot Neural Text Generation. Ernie Chang, Xiaoyu Shen, Hui-Syuan Yeh, Vera Demberg, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingShort Papers2Ernie Chang, Xiaoyu Shen, Hui-Syuan Yeh, and Vera Demberg. 2021. On Training Instance Selection for Few-Shot Neural Text Generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers). 8-13.\n\nEflops: Algorithm and system co-design for a high performance distributed training platform. Jianbo Dong, Zheng Cao, Tao Zhang, Jianxi Ye, Shaochuang Wang, Fei Feng, Li Zhao, Xiaoyong Liu, Liuyihan Song, Liwei Peng, 2020 IEEE International Symposium on High Performance Computer Architecture (HPCA). IEEEJianbo Dong, Zheng Cao, Tao Zhang, Jianxi Ye, Shaochuang Wang, Fei Feng, Li Zhao, Xiaoyong Liu, Liuyihan Song, Liwei Peng, et al. 2020. Eflops: Algorithm and system co-design for a high performance distributed training platform. In 2020 IEEE International Symposium on High Performance Computer Architecture (HPCA). IEEE, 610-622.\n\nACCL: Architecting Highly Scalable Distributed Training Systems with Highly-Efficient Collective Communication Library. Jianbo Dong, Shaochuang Wang, Fei Feng, Zheng Cao, Lingbo Heng Pan, Pengcheng Tang, Hao Li, Qianyuan Li, Yiqun Ran, Guo, IEEEMicroJianbo Dong, Shaochuang Wang, Fei Feng, Zheng Cao, Heng Pan, Lingbo Tang, Pengcheng Li, Hao Li, Qianyuan Ran, Yiqun Guo, et al. 2021. ACCL: Architecting Highly Scalable Distributed Training Systems with Highly-Efficient Collective Communication Library. IEEE Micro (2021).\n\nTree-to-Sequence Attentional Neural Machine Translation. Akiko Eriguchi, Kazuma Hashimoto, Yoshimasa Tsuruoka, Proceedings of the 54th. the 54thAkiko Eriguchi, Kazuma Hashimoto, and Yoshimasa Tsuruoka. 2016. Tree-to- Sequence Attentional Neural Machine Translation. In Proceedings of the 54th\n\nLong Papers. The Association for Computer Linguistics. 10.18653/v1/p16-1078Annual Meeting of the Association for Computational Linguistics, ACL 2016. Berlin, Germany1Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers. The Association for Computer Linguistics. https://doi.org/10.18653/v1/p16-1078\n\nStructured Neural Summarization. Patrick Fernandes, Miltiadis Allamanis, Marc Brockschmidt, ICLR 20197th International Conference on Learning Representations. New Orleans, LA, USAPatrick Fernandes, Miltiadis Allamanis, and Marc Brockschmidt. 2019. Struc- tured Neural Summarization. In 7th International Conference on Learning Rep- resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. https://openreview.net/forum?id=H1ersoRqtm\n\nSupporting program comprehension with source code summarization. Sonia Haiduc, Jairo Aponte, Andrian Marcus, Proceedings of the 32nd. the 32ndSonia Haiduc, Jairo Aponte, and Andrian Marcus. 2010. Supporting program comprehension with source code summarization. In Proceedings of the 32nd\n\n10.1145/1810295.1810335ACM/IEEE International Conference on Software Engineering. Jeff Kramer, Judith Bishop, Premkumar T. Devanbu, and Sebasti\u00e1n UchitelCape Town, South AfricaACM2ICSE 2010ACM/IEEE International Conference on Software Engineering -Volume 2, ICSE 2010, Cape Town, South Africa, 1-8 May 2010, Jeff Kramer, Judith Bishop, Premkumar T. Devanbu, and Sebasti\u00e1n Uchitel (Eds.). ACM, 223-226. https://doi.org/10.1145/ 1810295.1810335\n\nOn the Use of Automated Text Summarization Techniques for Summarizing Source Code. Sonia Haiduc, Jairo Aponte, Laura Moreno, Andrian Marcus, M A Beverly, Usa , Giuliano Antoniol, 10.1109/WCRE.2010.1317th Working Conference on Reverse Engineering, WCRE 2010. Martin Pinzger, and Elliot J. ChikofskyIEEE Computer SocietySonia Haiduc, Jairo Aponte, Laura Moreno, and Andrian Marcus. 2010. On the Use of Automated Text Summarization Techniques for Summarizing Source Code. In 17th Working Conference on Reverse Engineering, WCRE 2010, 13-16 October 2010, Beverly, MA, USA, Giuliano Antoniol, Martin Pinzger, and Elliot J. Chikofsky (Eds.). IEEE Computer Society, 35-44. https://doi.org/10.1109/WCRE.2010.13\n\nTree-Transformer: A Transformer-Based Method for Correction of Tree-Structured Data. Jacob Harer, Christopher P Reale, Peter Chin, arXiv:1908.00449Jacob Harer, Christopher P. Reale, and Peter Chin. 2019. Tree-Transformer: A Transformer-Based Method for Correction of Tree-Structured Data. CoRR abs/1908.00449 (2019). arXiv:1908.00449 http://arxiv.org/abs/1908.00449\n\nDeberta: decoding-Enhanced Bert with Disentangled Attention. Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, 9th International Conference on Learning Representations, ICLR 2021, Virtual Event. AustriaPengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2021. Deberta: decoding-Enhanced Bert with Disentangled Attention. In 9th International Con- ference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. https://openreview.net/forum?id=XPZIaotutsD\n\nBridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units. Dan Hendrycks, Kevin Gimpel, arXiv:1606.08415Dan Hendrycks and Kevin Gimpel. 2016. Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units. CoRR abs/1606.08415 (2016). arXiv:1606.08415 http://arxiv.org/abs/1606.08415\n\nDeep code comment generation. Xing Hu, Ge Li, Xin Xia, David Lo, Zhi Jin, 10.1145/3196321.3196334Proceedings of the 26th Conference on Program Comprehension, ICPC 2018. Khomh, Chanchal K. Roy, and Janet Siegmundthe 26th Conference on Program Comprehension, ICPC 2018Gothenburg, SwedenACMXing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2018. Deep code comment generation. In Proceedings of the 26th Conference on Program Comprehension, ICPC 2018, Gothenburg, Sweden, May 27-28, 2018, Foutse Khomh, Chanchal K. Roy, and Janet Siegmund (Eds.). ACM, 200-210. https://doi.org/10.1145/3196321.3196334\n\nSummarizing Source Code with Transferred API Knowledge. Xing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu, Zhi Jin, 10.24963/ijcai.2018/314Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018. the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018Stockholm, Sweden, J\u00e9r\u00f4me Langijcai.orgXing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu, and Zhi Jin. 2018. Summarizing Source Code with Transferred API Knowledge. In Proceedings of the Twenty- Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden, J\u00e9r\u00f4me Lang (Ed.). ijcai.org, 2269-2275. https: //doi.org/10.24963/ijcai.2018/314\n\nSummarizing Source Code using a Neural Attention Model. Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, Luke Zettlemoyer, 10.18653/v1/p16-1195Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016. the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016Berlin, Germany1Long Papers. The Association for Computer LinguisticsSrinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016. Summarizing Source Code using a Neural Attention Model. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers. The Association for Computer Linguistics. https://doi.org/10.18653/v1/p16-1195\n\nCode Prediction by Feeding Trees to Transformers. Seohyun Kim, Jinman Zhao, Yuchi Tian, Satish Chandra, 10.1109/ICSE43902.2021.0002643rd IEEE/ACM International Conference on Software Engineering, ICSE 2021. Madrid, SpainIEEESeohyun Kim, Jinman Zhao, Yuchi Tian, and Satish Chandra. 2021. Code Predic- tion by Feeding Trees to Transformers. In 43rd IEEE/ACM International Conference on Software Engineering, ICSE 2021, Madrid, Spain, 22-30 May 2021. IEEE, 150-162. https://doi.org/10.1109/ICSE43902.2021.00026\n\nSemi-Supervised Classification with Graph Convolutional Networks. N Thomas, Max Kipf, Welling, 5th International Conference on Learning Representations. Toulon, FranceConference Track Proceedings. OpenReview.netThomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with Graph Convolutional Networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net. https://openreview.net/forum?id=SJU4ayYgl\n\nImproved Code Summarization via a Graph Neural Network. Alexander Leclair, Sakib Haque, Lingfei Wu, Collin Mcmillan, 10.1145/3387904.3389268ICPC '20: 28th International Conference on Program Comprehension. Seoul, Republic of KoreaACMAlexander LeClair, Sakib Haque, Lingfei Wu, and Collin McMillan. 2020. Improved Code Summarization via a Graph Neural Network. In ICPC '20: 28th International Conference on Program Comprehension, Seoul, Republic of Korea, July 13-15, 2020. ACM, 184-195. https://doi.org/10.1145/3387904.3389268\n\nA neural model for generating natural language summaries of program subroutines. Alexander Leclair, Siyuan Jiang, Collin Mcmillan, 10.1109/ICSE.2019.00087Proceedings of the 41st International Conference on Software Engineering. Joanne M. Atlee, Tevfik Bultan, and Jon Whittlethe 41st International Conference on Software EngineeringMontreal, QC, CanadaAlexander LeClair, Siyuan Jiang, and Collin McMillan. 2019. A neural model for generating natural language summaries of program subroutines. In Proceedings of the 41st International Conference on Software Engineering, ICSE 2019, Montreal, QC, Canada, May 25-31, 2019, Joanne M. Atlee, Tevfik Bultan, and Jon Whittle (Eds.). IEEE / ACM, 795-806. https://doi.org/10.1109/ICSE.2019.00087\n\nDeepCommenter: a deep code comment generation tool with hybrid lexical and syntactical information. Boao Li, Meng Yan, Xin Xia, Xing Hu, Ge Li, David Lo, 10.1145/3368089.3417926ESEC/FSE '20: 28th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. Prem Devanbu, Myra B. Cohen, and Thomas ZimmermannACMVirtual Event, USABoao Li, Meng Yan, Xin Xia, Xing Hu, Ge Li, and David Lo. 2020. DeepCom- menter: a deep code comment generation tool with hybrid lexical and syntactical information. In ESEC/FSE '20: 28th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, Virtual Event, USA, November 8-13, 2020, Prem Devanbu, Myra B. Cohen, and Thomas Zimmermann (Eds.). ACM, 1571-1575. https://doi.org/10.1145/3368089.3417926\n\nAutomatic Generation of Text Descriptive Comments for Code Blocks. Yuding Liang, Kenny Qili Zhu, Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18). Sheila A. McIlraith and Kilian Q. Weinbergerthe Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)New Orleans, Louisiana, USAAAAI PressYuding Liang and Kenny Qili Zhu. 2018. Automatic Generation of Text Descriptive Comments for Code Blocks. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, Sheila A. McIlraith and Kilian Q. Weinberger (Eds.). AAAI Press, 5229-5236. https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16492\n\nROUGE: A Package for Automatic Evaluation of Summaries. Chin-Yew Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational LinguisticsChin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In Text Summarization Branches Out. Association for Computational Linguistics, Barcelona, Spain, 74-81. https://www.aclweb.org/anthology/W04-1013\n\nDecoupled Weight Decay Regularization. Ilya Loshchilov, Frank Hutter, 7th International Conference on Learning Representations. New Orleans, LA, USAIlya Loshchilov and Frank Hutter. 2019. Decoupled Weight Decay Regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Or- leans, LA, USA, May 6-9, 2019. OpenReview.net. https://openreview.net/forum? id=Bkg6RiCqY7\n\nAutomatic Source Code Summarization of Context for Java Methods. W Paul, Collin Mcburney, Mcmillan, 10.1109/TSE.2015.2465386IEEE Trans. Software Eng. 42Paul W. McBurney and Collin McMillan. 2016. Automatic Source Code Sum- marization of Context for Java Methods. IEEE Trans. Software Eng. 42, 2 (2016), 103-119. https://doi.org/10.1109/TSE.2015.2465386\n\nAutomatic generation of natural language summaries for Java classes. Laura Moreno, Jairo Aponte, Giriprasad Sridhara, Andrian Marcus, Lori L Pollock, K Vijay-Shanker, 10.1109/ICPC.2013.6613830IEEE 21st International Conference on Program Comprehension, ICPC 2013. San Francisco, CA, USAIEEE Computer SocietyLaura Moreno, Jairo Aponte, Giriprasad Sridhara, Andrian Marcus, Lori L. Pollock, and K. Vijay-Shanker. 2013. Automatic generation of natural language summaries for Java classes. In IEEE 21st International Conference on Program Comprehension, ICPC 2013, San Francisco, CA, USA, 20-21 May, 2013. IEEE Computer Society, 23-32. https://doi.org/10.1109/ICPC.2013.6613830\n\nConvolutional Neural Networks over Tree Structures for Programming Language Processing. Lili Mou, Ge Li, Lu Zhang, Tao Wang, Zhi Jin, Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence. Schuurmans and Michael P. Wellmanthe Thirtieth AAAI Conference on Artificial IntelligencePhoenix, Arizona, USA, DaleAAAI PressLili Mou, Ge Li, Lu Zhang, Tao Wang, and Zhi Jin. 2016. Convolutional Neural Networks over Tree Structures for Programming Language Processing. In Pro- ceedings of the Thirtieth AAAI Conference on Artificial Intelligence, February 12-17, 2016, Phoenix, Arizona, USA, Dale Schuurmans and Michael P. Wellman (Eds.). AAAI Press, 1287-1293. http://www.aaai.org/ocs/index.php/AAAI/AAAI16/ paper/view/11775\n\nNeural Networks: Tricks of the Trade. 10.1007/3-540-49430-8Lecture Notes in Computer Science. Genevieve B. Orr and Klaus-Robert M\u00fcller1524SpringerGenevieve B. Orr and Klaus-Robert M\u00fcller (Eds.). 1998. Neural Networks: Tricks of the Trade. Lecture Notes in Computer Science, Vol. 1524. Springer. https: //doi.org/10.1007/3-540-49430-8\n\nBleu: a Method for Automatic Evaluation of Machine Translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational LinguisticsPhiladelphia, PA, USA. ACLKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002, Philadelphia, PA, USA. ACL, 311-318. https://www.aclweb.org/anthology/ P02-1040/\n\nRecursive Distributed Representations. B Jordan, Pollack, 10.1016/0004-3702(90)90005-KArtif. Intell. 46Jordan B. Pollack. 1990. Recursive Distributed Representations. Artif. Intell. 46, 1-2 (1990), 77-105. https://doi.org/10.1016/0004-3702(90)90005-K\n\nNeuer beweis eines satzes \u00fcber permutationen. Heinz Pr\u00fcfer, Arch. Math. Phys. 27Heinz Pr\u00fcfer. 1918. Neuer beweis eines satzes \u00fcber permutationen. Arch. Math. Phys 27, 1918 (1918), 742-744.\n\nSelf-Attention with Relative Position Representations. Peter Shaw, Jakob Uszkoreit, Ashish Vaswani, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT. Marilyn A. Walker, Heng Ji, and Amanda Stentthe 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLTNew Orleans, Louisiana, USA2Short PapersPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-Attention with Relative Position Representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 2 (Short Papers), Marilyn A. Walker, Heng Ji, and Amanda Stent (Eds.).\n\n. 10.18653/v1/n18-2074Association for Computational LinguisticsAssociation for Computational Linguistics, 464-468. https://doi.org/10.18653/ v1/n18-2074\n\nEstimation of Gap Between Current Language Models and Human Performance. Xiaoyu Shen, Youssef Oualil, Clayton Greenberg, Mittul Singh, Dietrich Klakow, Proc. Interspeech. InterspeechXiaoyu Shen, Youssef Oualil, Clayton Greenberg, Mittul Singh, and Dietrich Klakow. 2017. Estimation of Gap Between Current Language Models and Human Performance. Proc. Interspeech 2017 (2017), 553-557.\n\nSelect and Attend: Towards Controllable Content Selection in Text Generation. Xiaoyu Shen, Jun Suzuki, Kentaro Inui, Hui Su, Dietrich Klakow, Satoshi Sekine, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingXiaoyu Shen, Jun Suzuki, Kentaro Inui, Hui Su, Dietrich Klakow, and Satoshi Sekine. 2019. Select and Attend: Towards Controllable Content Selection in Text Generation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 579-590.\n\nImproving latent alignment in text summarization by generalizing the pointer generator. Xiaoyu Shen, Yang Zhao, Hui Su, Dietrich Klakow, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingXiaoyu Shen, Yang Zhao, Hui Su, and Dietrich Klakow. 2019. Improving la- tent alignment in text summarization by generalizing the pointer generator. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Pro- cessing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 3753-3764.\n\nMasked Label Prediction: Unified Message Passing Model for Semi-Supervised Classification. Yunsheng Shi, Zhengjie Huang, Shikun Feng, Hui Zhong, Wenjing Wang, Yu Sun, 10.24963/ijcai.2021/214Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event. Zhi-Hua Zhouthe Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual EventMontreal, Canadaijcai.orgYunsheng Shi, Zhengjie Huang, Shikun Feng, Hui Zhong, Wenjing Wang, and Yu Sun. 2021. Masked Label Prediction: Unified Message Passing Model for Semi-Supervised Classification. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021, Zhi-Hua Zhou (Ed.). ijcai.org, 1548-1554. https://doi.org/10. 24963/ijcai.2021/214\n\nAutomatic Source Code Summarization with Extended Tree-LSTM. Yusuke Shido, Yasuaki Kobayashi, Akihiro Yamamoto, Atsushi Miyamoto, Tadayuki Matsumura, 10.1109/IJCNN.2019.8851751International Joint Conference on Neural Networks, IJCNN 2019. Budapest, HungaryYusuke Shido, Yasuaki Kobayashi, Akihiro Yamamoto, Atsushi Miyamoto, and Tadayuki Matsumura. 2019. Automatic Source Code Summarization with Ex- tended Tree-LSTM. In International Joint Conference on Neural Networks, IJCNN 2019 Budapest, Hungary, July 14-19, 2019. IEEE, 1-8. https://doi.org/10.1109/ IJCNN.2019.8851751\n\nNovel positional encodings to enable tree-based transformers. Leonardo Vighnesh, Chris Shiv, Quirk, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alch\u00e9-Buc, Emily B. Fox, and Roman GarnettNeurIPS; Vancouver, BC, CanadaVighnesh Leonardo Shiv and Chris Quirk. 2019. Novel positional encodings to enable tree-based transformers. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alch\u00e9-Buc, Emily B. Fox, and Roman Garnett (Eds.). 12058-12068.\n\nTowards automatically generating summary comments for Java methods. Giriprasad Sridhara, Emily Hill, Divya Muppaneni, Lori L Pollock, K Vijay-Shanker, 10.1145/1858996.1859006ASE 2010, 25th IEEE/ACM International Conference on Automated Software Engineering. Charles Pecheur, Jamie Andrews, and Elisabetta Di NittoAntwerp, BelgiumACMGiriprasad Sridhara, Emily Hill, Divya Muppaneni, Lori L. Pollock, and K. Vijay- Shanker. 2010. Towards automatically generating summary comments for Java methods. In ASE 2010, 25th IEEE/ACM International Conference on Automated Software Engineering, Antwerp, Belgium, September 20-24, 2010, Charles Pecheur, Jamie Andrews, and Elisabetta Di Nitto (Eds.). ACM, 43-52. https://doi.org/10. 1145/1858996.1859006\n\nDropout: a simple way to prevent neural networks from overfitting. Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov, J. Mach. Learn. Res. 15Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from overfitting. J. Mach. Learn. Res. 15, 1 (2014), 1929-1958. http://dl.acm.org/citation. cfm?id=2670313\n\nMoviechats: Chat like humans in a closed domain. Hui Su, Xiaoyu Shen, Zhou Xiao, Zheng Zhang, Ernie Chang, Cheng Zhang, Cheng Niu, Jie Zhou, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP. the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLPHui Su, Xiaoyu Shen, Zhou Xiao, Zheng Zhang, Ernie Chang, Cheng Zhang, Cheng Niu, and Jie Zhou. 2020. Moviechats: Chat like humans in a closed domain. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 6605-6619.\n\nRethinking the Inception Architecture for Computer Vision. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna, 10.1109/CVPR.2016.3082016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016. Las Vegas, NV, USAIEEE Computer SocietyChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbig- niew Wojna. 2016. Rethinking the Inception Architecture for Computer Vi- sion. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016. IEEE Computer Society, 2818-2826. https://doi.org/10.1109/CVPR.2016.308\n\nImproved Semantic Representations From Tree-Structured Long Short-Term Memory Networks. Kai Sheng Tai, Richard Socher, Christopher D Manning, 10.3115/v1/p15-1150Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing. the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language ProcessingBeijing, China1Long Papers. The Association for Computer LinguisticsKai Sheng Tai, Richard Socher, and Christopher D. Manning. 2015. Improved Semantic Representations From Tree-Structured Long Short-Term Memory Net- works. In Proceedings of the 53rd Annual Meeting of the Association for Computa- tional Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL 2015, July 26-31, 2015, Beijing, China, Volume 1: Long Papers. The Association for Computer Linguistics, 1556-1566. https://doi.org/10.3115/v1/p15-1150\n\nZe Tang, Chuanyi Li, Jidong Ge, Xiaoyu Shen, arXiv:2112.01184Zheling Zhu, and Bin Luo. 2021. AST-Transformer: Encoding Abstract Syntax Trees Efficiently for Code Summarization. arXiv preprintZe Tang, Chuanyi Li, Jidong Ge, Xiaoyu Shen, Zheling Zhu, and Bin Luo. 2021. AST-Transformer: Encoding Abstract Syntax Trees Efficiently for Code Summa- rization. arXiv preprint arXiv:2112.01184 (2021).\n\nAttention is All you Need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman GarnettLong Beach, CA, USA, Isabelle GuyonAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In Advances in Neural Information Processing Systems 30: Annual Con- ference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (Eds.). 5998-6008. http://papers.nips.cc/paper/7181-attention-is-all-you-need\n\nGraph Attention Networks. Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, Yoshua Bengio, 6th International Conference on Learning Representations. Vancouver, BC, CanadaConference Track Proceedings. OpenReview.netPetar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, and Yoshua Bengio. 2018. Graph Attention Networks. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 -May 3, 2018, Conference Track Proceedings. OpenReview.net. https://openreview. net/forum?id=rJXMpikCZ\n\nImproving automatic source code summarization via deep reinforcement learning. Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, Philip S Yu, 10.1145/3238147.3238206Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering, ASE 2018. Marianne Huchard, Christian K\u00e4stner, and Gordon Fraserthe 33rd ACM/IEEE International Conference on Automated Software Engineering, ASE 2018Montpellier, FranceACMYao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, and Philip S. Yu. 2018. Improving automatic source code summarization via deep reinforcement learning. In Proceedings of the 33rd ACM/IEEE International Confer- ence on Automated Software Engineering, ASE 2018, Montpellier, France, September 3-7, 2018, Marianne Huchard, Christian K\u00e4stner, and Gordon Fraser (Eds.). ACM, 397-407. https://doi.org/10.1145/3238147.3238206\n\nTranS\u02c63: A Transformer-based Framework for Unifying Code Summarization and Code Search. Wenhua Wang, Yuqun Zhang, Zhengran Zeng, Guandong Xu, arXiv:2003.03238Wenhua Wang, Yuqun Zhang, Zhengran Zeng, and Guandong Xu. 2020. TranS\u02c63: A Transformer-based Framework for Unifying Code Summarization and Code Search. CoRR abs/2003.03238 (2020). arXiv:2003.03238 https://arxiv.org/abs/2003. 03238\n\nCode Generation as a Dual Task of Code Summarization. Bolin Wei, Ge Li, Xin Xia, Zhiyi Fu, Zhi Jin, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alch\u00e9-Buc, Emily B. Fox, and Roman GarnettNeurIPS; Vancouver, BC, CanadaBolin Wei, Ge Li, Xin Xia, Zhiyi Fu, and Zhi Jin. 2019. Code Generation as a Dual Task of Code Summarization. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alch\u00e9-Buc, Emily B. Fox, and Roman Garnett (Eds.). 6559-6569.\n\nBig Bird: Transformers for Longer Sequences. Manzil Zaheer, Guru Guruganesh, Joshua Kumar Avinava Dubey, Chris Ainslie, Santiago Alberti, Philip Onta\u00f1\u00f3n, Anirudh Pham, Qifan Ravula, Li Wang, Amr Yang, Ahmed, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020. virtual, Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin2020Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Onta\u00f1\u00f3n, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. 2020. Big Bird: Transformers for Longer Sequences. In Ad- vances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.). https://proceedings.neurips.cc/paper/2020/hash/ c8512d142a2d849725f31a9a7a361ab9-Abstract.html\n\nUnsupervised rewriter for multi-sentence compression. Yang Zhao, Xiaoyu Shen, Wei Bi, Akiko Aizawa, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsYang Zhao, Xiaoyu Shen, Wei Bi, and Akiko Aizawa. 2019. Unsupervised rewriter for multi-sentence compression. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 2235-2240.\n\nAutomatic Code Summarization: A Systematic Literature Review. Yuxiang Zhu, Minxue Pan, arXiv:1909.04352Yuxiang Zhu and Minxue Pan. 2019. Automatic Code Summarization: A Sys- tematic Literature Review. CoRR abs/1909.04352 (2019). arXiv:1909.04352 http://arxiv.org/abs/1909.04352\n\nLanguage-Agnostic Representation Learning of Source Code from Structure and Context. Daniel Z\u00fcgner, Tobias Kirschstein, Michele Catasta, Jure Leskovec, Stephan G\u00fcnnemann, 9th International Conference on Learning Representations. AustriaICLR 2021, Virtual EventDaniel Z\u00fcgner, Tobias Kirschstein, Michele Catasta, Jure Leskovec, and Stephan G\u00fcnnemann. 2021. Language-Agnostic Representation Learning of Source Code from Structure and Context. In 9th International Conference on Learning Rep- resentations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. https://openreview.net/forum?id=Xh5eMZVONGF\n", "annotations": {"author": "[{\"end\":300,\"start\":292},{\"end\":313,\"start\":301},{\"end\":325,\"start\":314},{\"end\":336,\"start\":326},{\"end\":370,\"start\":337},{\"end\":382,\"start\":371},{\"end\":409,\"start\":383},{\"end\":418,\"start\":410},{\"end\":431,\"start\":419},{\"end\":443,\"start\":432},{\"end\":454,\"start\":444},{\"end\":467,\"start\":455},{\"end\":475,\"start\":468},{\"end\":560,\"start\":476},{\"end\":708,\"start\":561},{\"end\":841,\"start\":709}]", "publisher": null, "author_last_name": "[{\"end\":299,\"start\":295},{\"end\":312,\"start\":308},{\"end\":324,\"start\":322},{\"end\":335,\"start\":333},{\"end\":348,\"start\":343},{\"end\":381,\"start\":378},{\"end\":390,\"start\":387},{\"end\":417,\"start\":413},{\"end\":430,\"start\":426},{\"end\":442,\"start\":440},{\"end\":453,\"start\":451},{\"end\":466,\"start\":461}]", "author_first_name": "[{\"end\":294,\"start\":292},{\"end\":307,\"start\":301},{\"end\":321,\"start\":314},{\"end\":332,\"start\":326},{\"end\":342,\"start\":337},{\"end\":377,\"start\":371},{\"end\":386,\"start\":383},{\"end\":412,\"start\":410},{\"end\":425,\"start\":419},{\"end\":439,\"start\":432},{\"end\":450,\"start\":444},{\"end\":460,\"start\":455},{\"end\":474,\"start\":468}]", "author_affiliation": "[{\"end\":559,\"start\":477},{\"end\":707,\"start\":562},{\"end\":840,\"start\":710}]", "title": "[{\"end\":274,\"start\":1},{\"end\":1115,\"start\":842}]", "venue": null, "abstract": "[{\"end\":3006,\"start\":1583}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3067,\"start\":3063},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":3071,\"start\":3067},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3075,\"start\":3071},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":3079,\"start\":3075},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":3082,\"start\":3079},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3175,\"start\":3172},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3177,\"start\":3175},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":3180,\"start\":3177},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":3183,\"start\":3180},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":3186,\"start\":3183},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3542,\"start\":3538},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3580,\"start\":3576},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3607,\"start\":3604},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3724,\"start\":3720},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":3727,\"start\":3724},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":7052,\"start\":7048},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7882,\"start\":7879},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9891,\"start\":9887},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10275,\"start\":10272},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10546,\"start\":10543},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":13698,\"start\":13694},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":14092,\"start\":14088},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":14214,\"start\":14211},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":14646,\"start\":14642},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":15723,\"start\":15719},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":16848,\"start\":16845},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":17600,\"start\":17597},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":17603,\"start\":17600},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":23203,\"start\":23199},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":23343,\"start\":23339},{\"end\":23347,\"start\":23343},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":26569,\"start\":26565},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":26598,\"start\":26594},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":26763,\"start\":26759},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":27327,\"start\":27323},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":28191,\"start\":28187},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":28351,\"start\":28347},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":28425,\"start\":28421},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":28454,\"start\":28450},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":28512,\"start\":28508},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":28663,\"start\":28659},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":28675,\"start\":28672},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":28693,\"start\":28689},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":28766,\"start\":28763},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":28793,\"start\":28789},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":29244,\"start\":29240},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":29273,\"start\":29270},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":29336,\"start\":29332},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":29422,\"start\":29418},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":29564,\"start\":29560},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":30062,\"start\":30058},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":30085,\"start\":30081},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":30309,\"start\":30305},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":30319,\"start\":30315},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":30346,\"start\":30342},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":30680,\"start\":30677},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":31007,\"start\":31003},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":33046,\"start\":33045},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":33075,\"start\":33072},{\"end\":33273,\"start\":33267},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":33304,\"start\":33302},{\"end\":33314,\"start\":33308},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":37691,\"start\":37688},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":37693,\"start\":37691},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":37696,\"start\":37693},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":37704,\"start\":37701},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":37706,\"start\":37704},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":37708,\"start\":37706},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":41705,\"start\":41704},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":42521,\"start\":42518},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":42531,\"start\":42527},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":42534,\"start\":42531},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":42558,\"start\":42555},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":42640,\"start\":42636},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":42656,\"start\":42652},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":42659,\"start\":42656},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":42685,\"start\":42681},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":42688,\"start\":42685},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":42785,\"start\":42782},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":42788,\"start\":42785},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":42791,\"start\":42788},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":42914,\"start\":42910},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":42918,\"start\":42914},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":42922,\"start\":42918},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":43314,\"start\":43310},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":43317,\"start\":43314},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":43320,\"start\":43317},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":43323,\"start\":43320},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":43700,\"start\":43697},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":43703,\"start\":43700},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":43763,\"start\":43759},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":43816,\"start\":43812},{\"end\":45957,\"start\":45946},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":55248,\"start\":55245}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":47047,\"start\":46885},{\"attributes\":{\"id\":\"fig_1\"},\"end\":47246,\"start\":47048},{\"attributes\":{\"id\":\"fig_2\"},\"end\":48133,\"start\":47247},{\"attributes\":{\"id\":\"fig_3\"},\"end\":48328,\"start\":48134},{\"attributes\":{\"id\":\"fig_4\"},\"end\":48502,\"start\":48329},{\"attributes\":{\"id\":\"fig_5\"},\"end\":48625,\"start\":48503},{\"attributes\":{\"id\":\"fig_6\"},\"end\":48696,\"start\":48626},{\"attributes\":{\"id\":\"fig_7\"},\"end\":48931,\"start\":48697},{\"attributes\":{\"id\":\"fig_8\"},\"end\":49305,\"start\":48932},{\"attributes\":{\"id\":\"fig_9\"},\"end\":49505,\"start\":49306},{\"attributes\":{\"id\":\"fig_10\"},\"end\":50234,\"start\":49506},{\"attributes\":{\"id\":\"fig_11\"},\"end\":50492,\"start\":50235},{\"attributes\":{\"id\":\"fig_12\"},\"end\":50916,\"start\":50493},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":51155,\"start\":50917},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":51491,\"start\":51156},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":52765,\"start\":51492},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":53106,\"start\":52766},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":53446,\"start\":53107},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":53699,\"start\":53447},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":53946,\"start\":53700},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":54819,\"start\":53947},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":55092,\"start\":54820}]", "paragraph": "[{\"end\":3728,\"start\":3008},{\"end\":4495,\"start\":3730},{\"end\":5137,\"start\":4497},{\"end\":5855,\"start\":5139},{\"end\":6424,\"start\":5857},{\"end\":6937,\"start\":6447},{\"end\":8541,\"start\":6952},{\"end\":9387,\"start\":8555},{\"end\":9609,\"start\":9409},{\"end\":10851,\"start\":9611},{\"end\":11173,\"start\":10853},{\"end\":11692,\"start\":11199},{\"end\":11893,\"start\":11694},{\"end\":12261,\"start\":11895},{\"end\":12449,\"start\":12263},{\"end\":13172,\"start\":12531},{\"end\":13661,\"start\":13202},{\"end\":13720,\"start\":13663},{\"end\":13884,\"start\":13749},{\"end\":14360,\"start\":13886},{\"end\":14963,\"start\":14420},{\"end\":15026,\"start\":15007},{\"end\":15363,\"start\":15062},{\"end\":15725,\"start\":15365},{\"end\":16016,\"start\":15727},{\"end\":16677,\"start\":16055},{\"end\":17019,\"start\":16717},{\"end\":18090,\"start\":17048},{\"end\":18279,\"start\":18092},{\"end\":18440,\"start\":18281},{\"end\":18993,\"start\":18442},{\"end\":19883,\"start\":18995},{\"end\":20045,\"start\":19909},{\"end\":20312,\"start\":20066},{\"end\":20378,\"start\":20323},{\"end\":20539,\"start\":20380},{\"end\":20772,\"start\":20541},{\"end\":21208,\"start\":20796},{\"end\":21761,\"start\":21277},{\"end\":21928,\"start\":21781},{\"end\":22256,\"start\":21953},{\"end\":22315,\"start\":22258},{\"end\":22571,\"start\":22317},{\"end\":22690,\"start\":22611},{\"end\":22814,\"start\":22692},{\"end\":23157,\"start\":22900},{\"end\":23531,\"start\":23159},{\"end\":23939,\"start\":23533},{\"end\":24596,\"start\":23969},{\"end\":25573,\"start\":24604},{\"end\":26099,\"start\":25575},{\"end\":26445,\"start\":26115},{\"end\":26599,\"start\":26468},{\"end\":27708,\"start\":26601},{\"end\":28694,\"start\":27730},{\"end\":28996,\"start\":28696},{\"end\":29890,\"start\":29010},{\"end\":30457,\"start\":29892},{\"end\":31252,\"start\":30459},{\"end\":31484,\"start\":31269},{\"end\":31728,\"start\":31486},{\"end\":32671,\"start\":31745},{\"end\":34810,\"start\":32708},{\"end\":35503,\"start\":34812},{\"end\":36156,\"start\":35524},{\"end\":38232,\"start\":36158},{\"end\":38360,\"start\":38234},{\"end\":39037,\"start\":38362},{\"end\":41011,\"start\":39102},{\"end\":41348,\"start\":41035},{\"end\":41923,\"start\":41350},{\"end\":43256,\"start\":41941},{\"end\":44209,\"start\":43258},{\"end\":45302,\"start\":44224},{\"end\":45901,\"start\":45322},{\"end\":46390,\"start\":45924},{\"end\":46884,\"start\":46438}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12530,\"start\":12450},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13748,\"start\":13721},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14419,\"start\":14361},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15006,\"start\":14964},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15061,\"start\":15027},{\"attributes\":{\"id\":\"formula_5\"},\"end\":16054,\"start\":16017},{\"attributes\":{\"id\":\"formula_6\"},\"end\":16716,\"start\":16678},{\"attributes\":{\"id\":\"formula_7\"},\"end\":19908,\"start\":19884},{\"attributes\":{\"id\":\"formula_8\"},\"end\":20065,\"start\":20046},{\"attributes\":{\"id\":\"formula_9\"},\"end\":20322,\"start\":20313},{\"attributes\":{\"id\":\"formula_10\"},\"end\":21276,\"start\":21209},{\"attributes\":{\"id\":\"formula_11\"},\"end\":21952,\"start\":21929},{\"attributes\":{\"id\":\"formula_12\"},\"end\":22849,\"start\":22815},{\"attributes\":{\"id\":\"formula_13\"},\"end\":22899,\"start\":22849}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":10284,\"start\":10277},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":26876,\"start\":26869},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":31816,\"start\":31809},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":35969,\"start\":35962},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":36342,\"start\":36335},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":37484,\"start\":37477},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":37507,\"start\":37500},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":38359,\"start\":38352},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":40116,\"start\":40109},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":46717,\"start\":46710}]", "section_header": "[{\"end\":6445,\"start\":6427},{\"attributes\":{\"n\":\"2\"},\"end\":6950,\"start\":6940},{\"attributes\":{\"n\":\"3\"},\"end\":8553,\"start\":8544},{\"attributes\":{\"n\":\"3.1\"},\"end\":9407,\"start\":9390},{\"attributes\":{\"n\":\"3.2\"},\"end\":11197,\"start\":11176},{\"attributes\":{\"n\":\"3.3\"},\"end\":13200,\"start\":13175},{\"attributes\":{\"n\":\"4\"},\"end\":17046,\"start\":17022},{\"attributes\":{\"n\":\"5\"},\"end\":20794,\"start\":20775},{\"attributes\":{\"n\":\"5.1\"},\"end\":21779,\"start\":21764},{\"end\":22609,\"start\":22574},{\"attributes\":{\"n\":\"5.2\"},\"end\":23967,\"start\":23942},{\"attributes\":{\"n\":\"5.3\"},\"end\":24602,\"start\":24599},{\"attributes\":{\"n\":\"6\"},\"end\":26113,\"start\":26102},{\"attributes\":{\"n\":\"6.1\"},\"end\":26466,\"start\":26448},{\"end\":27728,\"start\":27711},{\"attributes\":{\"n\":\"6.2\"},\"end\":29008,\"start\":28999},{\"end\":31267,\"start\":31255},{\"attributes\":{\"n\":\"6.3\"},\"end\":31743,\"start\":31731},{\"end\":32706,\"start\":32674},{\"attributes\":{\"n\":\"6.4\"},\"end\":35522,\"start\":35506},{\"attributes\":{\"n\":\"6.5\"},\"end\":39059,\"start\":39040},{\"attributes\":{\"n\":\"6.6\"},\"end\":39100,\"start\":39062},{\"attributes\":{\"n\":\"7\"},\"end\":41033,\"start\":41014},{\"attributes\":{\"n\":\"8\"},\"end\":41939,\"start\":41926},{\"attributes\":{\"n\":\"9\"},\"end\":44222,\"start\":44212},{\"attributes\":{\"n\":\"10\"},\"end\":45320,\"start\":45305},{\"end\":45922,\"start\":45904},{\"end\":46436,\"start\":46393},{\"end\":46896,\"start\":46886},{\"end\":47059,\"start\":47049},{\"end\":47258,\"start\":47248},{\"end\":48145,\"start\":48135},{\"end\":48345,\"start\":48330},{\"end\":48514,\"start\":48504},{\"end\":48637,\"start\":48627},{\"end\":48701,\"start\":48698},{\"end\":49317,\"start\":49307},{\"end\":49517,\"start\":49507},{\"end\":50246,\"start\":50236},{\"end\":50505,\"start\":50494},{\"end\":50927,\"start\":50918},{\"end\":51166,\"start\":51157},{\"end\":51502,\"start\":51493},{\"end\":52776,\"start\":52767},{\"end\":53117,\"start\":53108},{\"end\":53457,\"start\":53448},{\"end\":53710,\"start\":53701},{\"end\":53957,\"start\":53948},{\"end\":54830,\"start\":54821}]", "table": "[{\"end\":51155,\"start\":50985},{\"end\":51491,\"start\":51206},{\"end\":52765,\"start\":51625},{\"end\":53106,\"start\":52824},{\"end\":53446,\"start\":53161},{\"end\":53699,\"start\":53476},{\"end\":53946,\"start\":53767},{\"end\":54819,\"start\":54551},{\"end\":55092,\"start\":54912}]", "figure_caption": "[{\"end\":47047,\"start\":46898},{\"end\":47246,\"start\":47061},{\"end\":48133,\"start\":47260},{\"end\":48328,\"start\":48147},{\"end\":48502,\"start\":48347},{\"end\":48625,\"start\":48516},{\"end\":48696,\"start\":48639},{\"end\":48931,\"start\":48703},{\"end\":49305,\"start\":48934},{\"end\":49505,\"start\":49319},{\"end\":50234,\"start\":49519},{\"end\":50492,\"start\":50248},{\"end\":50916,\"start\":50507},{\"end\":50985,\"start\":50929},{\"end\":51206,\"start\":51168},{\"end\":51625,\"start\":51504},{\"end\":52824,\"start\":52778},{\"end\":53161,\"start\":53119},{\"end\":53476,\"start\":53459},{\"end\":53767,\"start\":53712},{\"end\":54551,\"start\":53959},{\"end\":54912,\"start\":54832}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4818,\"start\":4810},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8166,\"start\":8158},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10310,\"start\":10302},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12848,\"start\":12840},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":19626,\"start\":19618},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":23513,\"start\":23505},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":23660,\"start\":23652},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":24863,\"start\":24855},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":25859,\"start\":25851},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":26977,\"start\":26971},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":39242,\"start\":39236}]", "bib_author_first_name": "[{\"end\":55492,\"start\":55486},{\"end\":55520,\"start\":55511},{\"end\":55541,\"start\":55534},{\"end\":56353,\"start\":56344},{\"end\":56369,\"start\":56365},{\"end\":56391,\"start\":56384},{\"end\":56930,\"start\":56921},{\"end\":56945,\"start\":56942},{\"end\":56959,\"start\":56952},{\"end\":57680,\"start\":57677},{\"end\":57693,\"start\":57687},{\"end\":57705,\"start\":57701},{\"end\":57716,\"start\":57712},{\"end\":58191,\"start\":58184},{\"end\":58211,\"start\":58202},{\"end\":58223,\"start\":58217},{\"end\":58795,\"start\":58786},{\"end\":58810,\"start\":58806},{\"end\":59704,\"start\":59702},{\"end\":59721,\"start\":59714},{\"end\":59723,\"start\":59722},{\"end\":59737,\"start\":59732},{\"end\":60012,\"start\":60007},{\"end\":60026,\"start\":60020},{\"end\":60042,\"start\":60033},{\"end\":60052,\"start\":60048},{\"end\":60817,\"start\":60811},{\"end\":60829,\"start\":60824},{\"end\":60838,\"start\":60835},{\"end\":60852,\"start\":60846},{\"end\":60867,\"start\":60857},{\"end\":60877,\"start\":60874},{\"end\":60886,\"start\":60884},{\"end\":60901,\"start\":60893},{\"end\":60915,\"start\":60907},{\"end\":60927,\"start\":60922},{\"end\":61480,\"start\":61474},{\"end\":61497,\"start\":61487},{\"end\":61507,\"start\":61504},{\"end\":61519,\"start\":61514},{\"end\":61531,\"start\":61525},{\"end\":61551,\"start\":61542},{\"end\":61561,\"start\":61558},{\"end\":61574,\"start\":61566},{\"end\":61584,\"start\":61579},{\"end\":61940,\"start\":61935},{\"end\":61957,\"start\":61951},{\"end\":61978,\"start\":61969},{\"end\":62592,\"start\":62585},{\"end\":62613,\"start\":62604},{\"end\":62629,\"start\":62625},{\"end\":63076,\"start\":63071},{\"end\":63090,\"start\":63085},{\"end\":63106,\"start\":63099},{\"end\":63827,\"start\":63822},{\"end\":63841,\"start\":63836},{\"end\":63855,\"start\":63850},{\"end\":63871,\"start\":63864},{\"end\":63881,\"start\":63880},{\"end\":63883,\"start\":63882},{\"end\":63896,\"start\":63893},{\"end\":63907,\"start\":63899},{\"end\":64533,\"start\":64528},{\"end\":64552,\"start\":64541},{\"end\":64554,\"start\":64553},{\"end\":64567,\"start\":64562},{\"end\":64880,\"start\":64871},{\"end\":64893,\"start\":64885},{\"end\":64907,\"start\":64899},{\"end\":64919,\"start\":64913},{\"end\":65406,\"start\":65403},{\"end\":65423,\"start\":65418},{\"end\":65684,\"start\":65680},{\"end\":65691,\"start\":65689},{\"end\":65699,\"start\":65696},{\"end\":65710,\"start\":65705},{\"end\":65718,\"start\":65715},{\"end\":66306,\"start\":66302},{\"end\":66313,\"start\":66311},{\"end\":66321,\"start\":66318},{\"end\":66332,\"start\":66327},{\"end\":66342,\"start\":66337},{\"end\":66350,\"start\":66347},{\"end\":67026,\"start\":67016},{\"end\":67040,\"start\":67033},{\"end\":67055,\"start\":67050},{\"end\":67068,\"start\":67064},{\"end\":67782,\"start\":67775},{\"end\":67794,\"start\":67788},{\"end\":67806,\"start\":67801},{\"end\":67819,\"start\":67813},{\"end\":68302,\"start\":68301},{\"end\":68314,\"start\":68311},{\"end\":68811,\"start\":68802},{\"end\":68826,\"start\":68821},{\"end\":68841,\"start\":68834},{\"end\":68852,\"start\":68846},{\"end\":69364,\"start\":69355},{\"end\":69380,\"start\":69374},{\"end\":69394,\"start\":69388},{\"end\":70116,\"start\":70112},{\"end\":70125,\"start\":70121},{\"end\":70134,\"start\":70131},{\"end\":70144,\"start\":70140},{\"end\":70151,\"start\":70149},{\"end\":70161,\"start\":70156},{\"end\":70915,\"start\":70909},{\"end\":70933,\"start\":70923},{\"end\":72110,\"start\":72102},{\"end\":72472,\"start\":72468},{\"end\":72490,\"start\":72485},{\"end\":72894,\"start\":72893},{\"end\":72907,\"start\":72901},{\"end\":73256,\"start\":73251},{\"end\":73270,\"start\":73265},{\"end\":73289,\"start\":73279},{\"end\":73307,\"start\":73300},{\"end\":73320,\"start\":73316},{\"end\":73322,\"start\":73321},{\"end\":73333,\"start\":73332},{\"end\":73949,\"start\":73945},{\"end\":73957,\"start\":73955},{\"end\":73964,\"start\":73962},{\"end\":73975,\"start\":73972},{\"end\":73985,\"start\":73982},{\"end\":74998,\"start\":74991},{\"end\":75014,\"start\":75009},{\"end\":75027,\"start\":75023},{\"end\":75042,\"start\":75034},{\"end\":75597,\"start\":75596},{\"end\":75860,\"start\":75855},{\"end\":76059,\"start\":76054},{\"end\":76071,\"start\":76066},{\"end\":76089,\"start\":76083},{\"end\":77101,\"start\":77095},{\"end\":77115,\"start\":77108},{\"end\":77131,\"start\":77124},{\"end\":77149,\"start\":77143},{\"end\":77165,\"start\":77157},{\"end\":77491,\"start\":77485},{\"end\":77501,\"start\":77498},{\"end\":77517,\"start\":77510},{\"end\":77527,\"start\":77524},{\"end\":77540,\"start\":77532},{\"end\":77556,\"start\":77549},{\"end\":78324,\"start\":78318},{\"end\":78335,\"start\":78331},{\"end\":78345,\"start\":78342},{\"end\":78358,\"start\":78350},{\"end\":79116,\"start\":79108},{\"end\":79130,\"start\":79122},{\"end\":79144,\"start\":79138},{\"end\":79154,\"start\":79151},{\"end\":79169,\"start\":79162},{\"end\":79178,\"start\":79176},{\"end\":79941,\"start\":79935},{\"end\":79956,\"start\":79949},{\"end\":79975,\"start\":79968},{\"end\":79993,\"start\":79986},{\"end\":80012,\"start\":80004},{\"end\":80520,\"start\":80512},{\"end\":80536,\"start\":80531},{\"end\":81297,\"start\":81287},{\"end\":81313,\"start\":81308},{\"end\":81325,\"start\":81320},{\"end\":81341,\"start\":81337},{\"end\":81343,\"start\":81342},{\"end\":81354,\"start\":81353},{\"end\":82034,\"start\":82028},{\"end\":82055,\"start\":82047},{\"end\":82057,\"start\":82056},{\"end\":82070,\"start\":82066},{\"end\":82087,\"start\":82083},{\"end\":82105,\"start\":82099},{\"end\":82457,\"start\":82454},{\"end\":82468,\"start\":82462},{\"end\":82479,\"start\":82475},{\"end\":82491,\"start\":82486},{\"end\":82504,\"start\":82499},{\"end\":82517,\"start\":82512},{\"end\":82530,\"start\":82525},{\"end\":82539,\"start\":82536},{\"end\":83049,\"start\":83040},{\"end\":83066,\"start\":83059},{\"end\":83084,\"start\":83078},{\"end\":83100,\"start\":83092},{\"end\":83117,\"start\":83109},{\"end\":83706,\"start\":83697},{\"end\":83719,\"start\":83712},{\"end\":83739,\"start\":83728},{\"end\":83741,\"start\":83740},{\"end\":84794,\"start\":84792},{\"end\":84808,\"start\":84801},{\"end\":84819,\"start\":84813},{\"end\":84830,\"start\":84824},{\"end\":85220,\"start\":85214},{\"end\":85234,\"start\":85230},{\"end\":85248,\"start\":85244},{\"end\":85262,\"start\":85257},{\"end\":85279,\"start\":85274},{\"end\":85292,\"start\":85287},{\"end\":85294,\"start\":85293},{\"end\":85308,\"start\":85302},{\"end\":85322,\"start\":85317},{\"end\":86129,\"start\":86124},{\"end\":86149,\"start\":86142},{\"end\":86167,\"start\":86160},{\"end\":86185,\"start\":86178},{\"end\":86200,\"start\":86194},{\"end\":86212,\"start\":86206},{\"end\":86767,\"start\":86764},{\"end\":86777,\"start\":86773},{\"end\":86787,\"start\":86784},{\"end\":86802,\"start\":86794},{\"end\":86814,\"start\":86807},{\"end\":86825,\"start\":86821},{\"end\":86836,\"start\":86830},{\"end\":86838,\"start\":86837},{\"end\":87662,\"start\":87656},{\"end\":87674,\"start\":87669},{\"end\":87690,\"start\":87682},{\"end\":87705,\"start\":87697},{\"end\":88017,\"start\":88012},{\"end\":88025,\"start\":88023},{\"end\":88033,\"start\":88030},{\"end\":88044,\"start\":88039},{\"end\":88052,\"start\":88049},{\"end\":88778,\"start\":88772},{\"end\":88791,\"start\":88787},{\"end\":88810,\"start\":88804},{\"end\":88837,\"start\":88832},{\"end\":88855,\"start\":88847},{\"end\":88871,\"start\":88865},{\"end\":88888,\"start\":88881},{\"end\":88900,\"start\":88895},{\"end\":88911,\"start\":88909},{\"end\":88921,\"start\":88918},{\"end\":89802,\"start\":89798},{\"end\":89815,\"start\":89809},{\"end\":89825,\"start\":89822},{\"end\":89835,\"start\":89830},{\"end\":90288,\"start\":90281},{\"end\":90300,\"start\":90294},{\"end\":90589,\"start\":90583},{\"end\":90604,\"start\":90598},{\"end\":90625,\"start\":90618},{\"end\":90639,\"start\":90635},{\"end\":90657,\"start\":90650}]", "bib_author_last_name": "[{\"end\":55509,\"start\":55493},{\"end\":55532,\"start\":55521},{\"end\":55545,\"start\":55542},{\"end\":55552,\"start\":55547},{\"end\":56363,\"start\":56354},{\"end\":56382,\"start\":56370},{\"end\":56399,\"start\":56392},{\"end\":56940,\"start\":56931},{\"end\":56950,\"start\":56946},{\"end\":56966,\"start\":56960},{\"end\":57685,\"start\":57681},{\"end\":57699,\"start\":57694},{\"end\":57710,\"start\":57706},{\"end\":57722,\"start\":57717},{\"end\":58200,\"start\":58192},{\"end\":58215,\"start\":58212},{\"end\":58230,\"start\":58224},{\"end\":58804,\"start\":58796},{\"end\":58816,\"start\":58811},{\"end\":59712,\"start\":59705},{\"end\":59730,\"start\":59724},{\"end\":59743,\"start\":59738},{\"end\":60018,\"start\":60013},{\"end\":60031,\"start\":60027},{\"end\":60046,\"start\":60043},{\"end\":60060,\"start\":60053},{\"end\":60822,\"start\":60818},{\"end\":60833,\"start\":60830},{\"end\":60844,\"start\":60839},{\"end\":60855,\"start\":60853},{\"end\":60872,\"start\":60868},{\"end\":60882,\"start\":60878},{\"end\":60891,\"start\":60887},{\"end\":60905,\"start\":60902},{\"end\":60920,\"start\":60916},{\"end\":60932,\"start\":60928},{\"end\":61485,\"start\":61481},{\"end\":61502,\"start\":61498},{\"end\":61512,\"start\":61508},{\"end\":61523,\"start\":61520},{\"end\":61540,\"start\":61532},{\"end\":61556,\"start\":61552},{\"end\":61564,\"start\":61562},{\"end\":61577,\"start\":61575},{\"end\":61588,\"start\":61585},{\"end\":61593,\"start\":61590},{\"end\":61949,\"start\":61941},{\"end\":61967,\"start\":61958},{\"end\":61987,\"start\":61979},{\"end\":62602,\"start\":62593},{\"end\":62623,\"start\":62614},{\"end\":62642,\"start\":62630},{\"end\":63083,\"start\":63077},{\"end\":63097,\"start\":63091},{\"end\":63113,\"start\":63107},{\"end\":63834,\"start\":63828},{\"end\":63848,\"start\":63842},{\"end\":63862,\"start\":63856},{\"end\":63878,\"start\":63872},{\"end\":63891,\"start\":63884},{\"end\":63916,\"start\":63908},{\"end\":64539,\"start\":64534},{\"end\":64560,\"start\":64555},{\"end\":64572,\"start\":64568},{\"end\":64883,\"start\":64881},{\"end\":64897,\"start\":64894},{\"end\":64911,\"start\":64908},{\"end\":64924,\"start\":64920},{\"end\":65416,\"start\":65407},{\"end\":65430,\"start\":65424},{\"end\":65687,\"start\":65685},{\"end\":65694,\"start\":65692},{\"end\":65703,\"start\":65700},{\"end\":65713,\"start\":65711},{\"end\":65722,\"start\":65719},{\"end\":66309,\"start\":66307},{\"end\":66316,\"start\":66314},{\"end\":66325,\"start\":66322},{\"end\":66335,\"start\":66333},{\"end\":66345,\"start\":66343},{\"end\":66354,\"start\":66351},{\"end\":67031,\"start\":67027},{\"end\":67048,\"start\":67041},{\"end\":67062,\"start\":67056},{\"end\":67080,\"start\":67069},{\"end\":67786,\"start\":67783},{\"end\":67799,\"start\":67795},{\"end\":67811,\"start\":67807},{\"end\":67827,\"start\":67820},{\"end\":68309,\"start\":68303},{\"end\":68319,\"start\":68315},{\"end\":68328,\"start\":68321},{\"end\":68819,\"start\":68812},{\"end\":68832,\"start\":68827},{\"end\":68844,\"start\":68842},{\"end\":68861,\"start\":68853},{\"end\":69372,\"start\":69365},{\"end\":69386,\"start\":69381},{\"end\":69403,\"start\":69395},{\"end\":70119,\"start\":70117},{\"end\":70129,\"start\":70126},{\"end\":70138,\"start\":70135},{\"end\":70147,\"start\":70145},{\"end\":70154,\"start\":70152},{\"end\":70164,\"start\":70162},{\"end\":70921,\"start\":70916},{\"end\":70937,\"start\":70934},{\"end\":72114,\"start\":72111},{\"end\":72483,\"start\":72473},{\"end\":72497,\"start\":72491},{\"end\":72899,\"start\":72895},{\"end\":72916,\"start\":72908},{\"end\":72926,\"start\":72918},{\"end\":73263,\"start\":73257},{\"end\":73277,\"start\":73271},{\"end\":73298,\"start\":73290},{\"end\":73314,\"start\":73308},{\"end\":73330,\"start\":73323},{\"end\":73347,\"start\":73334},{\"end\":73953,\"start\":73950},{\"end\":73960,\"start\":73958},{\"end\":73970,\"start\":73965},{\"end\":73980,\"start\":73976},{\"end\":73989,\"start\":73986},{\"end\":75007,\"start\":74999},{\"end\":75021,\"start\":75015},{\"end\":75032,\"start\":75028},{\"end\":75046,\"start\":75043},{\"end\":75604,\"start\":75598},{\"end\":75613,\"start\":75606},{\"end\":75867,\"start\":75861},{\"end\":76064,\"start\":76060},{\"end\":76081,\"start\":76072},{\"end\":76097,\"start\":76090},{\"end\":77106,\"start\":77102},{\"end\":77122,\"start\":77116},{\"end\":77141,\"start\":77132},{\"end\":77155,\"start\":77150},{\"end\":77172,\"start\":77166},{\"end\":77496,\"start\":77492},{\"end\":77508,\"start\":77502},{\"end\":77522,\"start\":77518},{\"end\":77530,\"start\":77528},{\"end\":77547,\"start\":77541},{\"end\":77563,\"start\":77557},{\"end\":78329,\"start\":78325},{\"end\":78340,\"start\":78336},{\"end\":78348,\"start\":78346},{\"end\":78365,\"start\":78359},{\"end\":79120,\"start\":79117},{\"end\":79136,\"start\":79131},{\"end\":79149,\"start\":79145},{\"end\":79160,\"start\":79155},{\"end\":79174,\"start\":79170},{\"end\":79182,\"start\":79179},{\"end\":79947,\"start\":79942},{\"end\":79966,\"start\":79957},{\"end\":79984,\"start\":79976},{\"end\":80002,\"start\":79994},{\"end\":80022,\"start\":80013},{\"end\":80529,\"start\":80521},{\"end\":80541,\"start\":80537},{\"end\":80548,\"start\":80543},{\"end\":81306,\"start\":81298},{\"end\":81318,\"start\":81314},{\"end\":81335,\"start\":81326},{\"end\":81351,\"start\":81344},{\"end\":81368,\"start\":81355},{\"end\":82045,\"start\":82035},{\"end\":82064,\"start\":82058},{\"end\":82081,\"start\":82071},{\"end\":82097,\"start\":82088},{\"end\":82119,\"start\":82106},{\"end\":82460,\"start\":82458},{\"end\":82473,\"start\":82469},{\"end\":82484,\"start\":82480},{\"end\":82497,\"start\":82492},{\"end\":82510,\"start\":82505},{\"end\":82523,\"start\":82518},{\"end\":82534,\"start\":82531},{\"end\":82544,\"start\":82540},{\"end\":83057,\"start\":83050},{\"end\":83076,\"start\":83067},{\"end\":83090,\"start\":83085},{\"end\":83107,\"start\":83101},{\"end\":83123,\"start\":83118},{\"end\":83710,\"start\":83707},{\"end\":83726,\"start\":83720},{\"end\":83749,\"start\":83742},{\"end\":84799,\"start\":84795},{\"end\":84811,\"start\":84809},{\"end\":84822,\"start\":84820},{\"end\":84835,\"start\":84831},{\"end\":85228,\"start\":85221},{\"end\":85242,\"start\":85235},{\"end\":85255,\"start\":85249},{\"end\":85272,\"start\":85263},{\"end\":85285,\"start\":85280},{\"end\":85300,\"start\":85295},{\"end\":85315,\"start\":85309},{\"end\":85333,\"start\":85323},{\"end\":86140,\"start\":86130},{\"end\":86158,\"start\":86150},{\"end\":86176,\"start\":86168},{\"end\":86192,\"start\":86186},{\"end\":86204,\"start\":86201},{\"end\":86219,\"start\":86213},{\"end\":86771,\"start\":86768},{\"end\":86782,\"start\":86778},{\"end\":86792,\"start\":86788},{\"end\":86805,\"start\":86803},{\"end\":86819,\"start\":86815},{\"end\":86828,\"start\":86826},{\"end\":86841,\"start\":86839},{\"end\":87667,\"start\":87663},{\"end\":87680,\"start\":87675},{\"end\":87695,\"start\":87691},{\"end\":87708,\"start\":87706},{\"end\":88021,\"start\":88018},{\"end\":88028,\"start\":88026},{\"end\":88037,\"start\":88034},{\"end\":88047,\"start\":88045},{\"end\":88056,\"start\":88053},{\"end\":88785,\"start\":88779},{\"end\":88802,\"start\":88792},{\"end\":88830,\"start\":88811},{\"end\":88845,\"start\":88838},{\"end\":88863,\"start\":88856},{\"end\":88879,\"start\":88872},{\"end\":88893,\"start\":88889},{\"end\":88907,\"start\":88901},{\"end\":88916,\"start\":88912},{\"end\":88926,\"start\":88922},{\"end\":88933,\"start\":88928},{\"end\":89807,\"start\":89803},{\"end\":89820,\"start\":89816},{\"end\":89828,\"start\":89826},{\"end\":89842,\"start\":89836},{\"end\":90292,\"start\":90289},{\"end\":90304,\"start\":90301},{\"end\":90596,\"start\":90590},{\"end\":90616,\"start\":90605},{\"end\":90633,\"start\":90626},{\"end\":90648,\"start\":90640},{\"end\":90667,\"start\":90658}]", "bib_entry": "[{\"attributes\":{\"doi\":\"10.18653/v1/2020.acl-main.449\",\"id\":\"b0\",\"matched_paper_id\":218486987},\"end\":56298,\"start\":55426},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":3495200},\"end\":56843,\"start\":56300},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":2723946},\"end\":57603,\"start\":56845},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":51926976},\"end\":58111,\"start\":57605},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":11212020},\"end\":58690,\"start\":58113},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":7164502},\"end\":59657,\"start\":58692},{\"attributes\":{\"doi\":\"arXiv:2004.05150\",\"id\":\"b6\"},\"end\":59937,\"start\":59659},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":235755110},\"end\":60716,\"start\":59939},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":215817496},\"end\":61352,\"start\":60718},{\"attributes\":{\"id\":\"b9\"},\"end\":61876,\"start\":61354},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":12851711},\"end\":62170,\"start\":61878},{\"attributes\":{\"doi\":\"10.18653/v1/p16-1078\",\"id\":\"b11\"},\"end\":62550,\"start\":62172},{\"attributes\":{\"doi\":\"ICLR 2019\",\"id\":\"b12\",\"matched_paper_id\":53216170},\"end\":63004,\"start\":62552},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":10747140},\"end\":63293,\"start\":63006},{\"attributes\":{\"doi\":\"10.1145/1810295.1810335\",\"id\":\"b14\"},\"end\":63737,\"start\":63295},{\"attributes\":{\"doi\":\"10.1109/WCRE.2010.13\",\"id\":\"b15\",\"matched_paper_id\":7843537},\"end\":64441,\"start\":63739},{\"attributes\":{\"doi\":\"arXiv:1908.00449\",\"id\":\"b16\"},\"end\":64808,\"start\":64443},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":219531210},\"end\":65315,\"start\":64810},{\"attributes\":{\"doi\":\"arXiv:1606.08415\",\"id\":\"b18\"},\"end\":65648,\"start\":65317},{\"attributes\":{\"doi\":\"10.1145/3196321.3196334\",\"id\":\"b19\",\"matched_paper_id\":49584534},\"end\":66244,\"start\":65650},{\"attributes\":{\"doi\":\"10.24963/ijcai.2018/314\",\"id\":\"b20\",\"matched_paper_id\":49584957},\"end\":66958,\"start\":66246},{\"attributes\":{\"doi\":\"10.18653/v1/p16-1195\",\"id\":\"b21\",\"matched_paper_id\":8820379},\"end\":67723,\"start\":66960},{\"attributes\":{\"doi\":\"10.1109/ICSE43902.2021.00026\",\"id\":\"b22\",\"matched_paper_id\":214727958},\"end\":68233,\"start\":67725},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":3144218},\"end\":68744,\"start\":68235},{\"attributes\":{\"doi\":\"10.1145/3387904.3389268\",\"id\":\"b24\",\"matched_paper_id\":214802082},\"end\":69272,\"start\":68746},{\"attributes\":{\"doi\":\"10.1109/ICSE.2019.00087\",\"id\":\"b25\",\"matched_paper_id\":59606259},\"end\":70010,\"start\":69274},{\"attributes\":{\"doi\":\"10.1145/3368089.3417926\",\"id\":\"b26\",\"matched_paper_id\":226274234},\"end\":70840,\"start\":70012},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":19194935},\"end\":72044,\"start\":70842},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":964287},\"end\":72427,\"start\":72046},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":53592270},\"end\":72826,\"start\":72429},{\"attributes\":{\"doi\":\"10.1109/TSE.2015.2465386\",\"id\":\"b30\",\"matched_paper_id\":17406565},\"end\":73180,\"start\":72828},{\"attributes\":{\"doi\":\"10.1109/ICPC.2013.6613830\",\"id\":\"b31\",\"matched_paper_id\":1129667},\"end\":73855,\"start\":73182},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":1914494},\"end\":74590,\"start\":73857},{\"attributes\":{\"doi\":\"10.1007/3-540-49430-8\",\"id\":\"b33\",\"matched_paper_id\":26661612},\"end\":74925,\"start\":74592},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":11080756},\"end\":75555,\"start\":74927},{\"attributes\":{\"doi\":\"10.1016/0004-3702(90)90005-K\",\"id\":\"b35\",\"matched_paper_id\":770011},\"end\":75807,\"start\":75557},{\"attributes\":{\"id\":\"b36\"},\"end\":75997,\"start\":75809},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":3725815},\"end\":76866,\"start\":75999},{\"attributes\":{\"doi\":\"10.18653/v1/n18-2074\",\"id\":\"b38\"},\"end\":77020,\"start\":76868},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":32761288},\"end\":77405,\"start\":77022},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":202542540},\"end\":78228,\"start\":77407},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":202770038},\"end\":79015,\"start\":78230},{\"attributes\":{\"doi\":\"10.24963/ijcai.2021/214\",\"id\":\"b42\",\"matched_paper_id\":221534325},\"end\":79872,\"start\":79017},{\"attributes\":{\"doi\":\"10.1109/IJCNN.2019.8851751\",\"id\":\"b43\",\"matched_paper_id\":195069474},\"end\":80448,\"start\":79874},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":202784970},\"end\":81217,\"start\":80450},{\"attributes\":{\"doi\":\"10.1145/1858996.1859006\",\"id\":\"b45\",\"matched_paper_id\":9790585},\"end\":81959,\"start\":81219},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":6844431},\"end\":82403,\"start\":81961},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":226262299},\"end\":82979,\"start\":82405},{\"attributes\":{\"doi\":\"10.1109/CVPR.2016.308\",\"id\":\"b48\",\"matched_paper_id\":206593880},\"end\":83607,\"start\":82981},{\"attributes\":{\"doi\":\"10.3115/v1/p15-1150\",\"id\":\"b49\",\"matched_paper_id\":3033526},\"end\":84790,\"start\":83609},{\"attributes\":{\"doi\":\"arXiv:2112.01184\",\"id\":\"b50\"},\"end\":85185,\"start\":84792},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":13756489},\"end\":86096,\"start\":85187},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":3292002},\"end\":86683,\"start\":86098},{\"attributes\":{\"doi\":\"10.1145/3238147.3238206\",\"id\":\"b53\",\"matched_paper_id\":52069701},\"end\":87566,\"start\":86685},{\"attributes\":{\"doi\":\"arXiv:2003.03238\",\"id\":\"b54\"},\"end\":87956,\"start\":87568},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":202769028},\"end\":88725,\"start\":87958},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":220831004},\"end\":89742,\"start\":88727},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":196193115},\"end\":90217,\"start\":89744},{\"attributes\":{\"doi\":\"arXiv:1909.04352\",\"id\":\"b58\"},\"end\":90496,\"start\":90219},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":232307359},\"end\":91111,\"start\":90498}]", "bib_title": "[{\"end\":55484,\"start\":55426},{\"end\":56342,\"start\":56300},{\"end\":56919,\"start\":56845},{\"end\":57675,\"start\":57605},{\"end\":58182,\"start\":58113},{\"end\":58784,\"start\":58692},{\"end\":60005,\"start\":59939},{\"end\":60809,\"start\":60718},{\"end\":61933,\"start\":61878},{\"end\":62225,\"start\":62172},{\"end\":62583,\"start\":62552},{\"end\":63069,\"start\":63006},{\"end\":63820,\"start\":63739},{\"end\":64869,\"start\":64810},{\"end\":65678,\"start\":65650},{\"end\":66300,\"start\":66246},{\"end\":67014,\"start\":66960},{\"end\":67773,\"start\":67725},{\"end\":68299,\"start\":68235},{\"end\":68800,\"start\":68746},{\"end\":69353,\"start\":69274},{\"end\":70110,\"start\":70012},{\"end\":70907,\"start\":70842},{\"end\":72100,\"start\":72046},{\"end\":72466,\"start\":72429},{\"end\":72891,\"start\":72828},{\"end\":73249,\"start\":73182},{\"end\":73943,\"start\":73857},{\"end\":74628,\"start\":74592},{\"end\":74989,\"start\":74927},{\"end\":75594,\"start\":75557},{\"end\":75853,\"start\":75809},{\"end\":76052,\"start\":75999},{\"end\":77093,\"start\":77022},{\"end\":77483,\"start\":77407},{\"end\":78316,\"start\":78230},{\"end\":79106,\"start\":79017},{\"end\":79933,\"start\":79874},{\"end\":80510,\"start\":80450},{\"end\":81285,\"start\":81219},{\"end\":82026,\"start\":81961},{\"end\":82452,\"start\":82405},{\"end\":83038,\"start\":82981},{\"end\":83695,\"start\":83609},{\"end\":85212,\"start\":85187},{\"end\":86122,\"start\":86098},{\"end\":86762,\"start\":86685},{\"end\":88010,\"start\":87958},{\"end\":88770,\"start\":88727},{\"end\":89796,\"start\":89744},{\"end\":90581,\"start\":90498}]", "bib_author": "[{\"end\":55511,\"start\":55486},{\"end\":55534,\"start\":55511},{\"end\":55547,\"start\":55534},{\"end\":55554,\"start\":55547},{\"end\":56365,\"start\":56344},{\"end\":56384,\"start\":56365},{\"end\":56401,\"start\":56384},{\"end\":56942,\"start\":56921},{\"end\":56952,\"start\":56942},{\"end\":56968,\"start\":56952},{\"end\":57687,\"start\":57677},{\"end\":57701,\"start\":57687},{\"end\":57712,\"start\":57701},{\"end\":57724,\"start\":57712},{\"end\":58202,\"start\":58184},{\"end\":58217,\"start\":58202},{\"end\":58232,\"start\":58217},{\"end\":58806,\"start\":58786},{\"end\":58818,\"start\":58806},{\"end\":59714,\"start\":59702},{\"end\":59732,\"start\":59714},{\"end\":59745,\"start\":59732},{\"end\":60020,\"start\":60007},{\"end\":60033,\"start\":60020},{\"end\":60048,\"start\":60033},{\"end\":60062,\"start\":60048},{\"end\":60824,\"start\":60811},{\"end\":60835,\"start\":60824},{\"end\":60846,\"start\":60835},{\"end\":60857,\"start\":60846},{\"end\":60874,\"start\":60857},{\"end\":60884,\"start\":60874},{\"end\":60893,\"start\":60884},{\"end\":60907,\"start\":60893},{\"end\":60922,\"start\":60907},{\"end\":60934,\"start\":60922},{\"end\":61487,\"start\":61474},{\"end\":61504,\"start\":61487},{\"end\":61514,\"start\":61504},{\"end\":61525,\"start\":61514},{\"end\":61542,\"start\":61525},{\"end\":61558,\"start\":61542},{\"end\":61566,\"start\":61558},{\"end\":61579,\"start\":61566},{\"end\":61590,\"start\":61579},{\"end\":61595,\"start\":61590},{\"end\":61951,\"start\":61935},{\"end\":61969,\"start\":61951},{\"end\":61989,\"start\":61969},{\"end\":62604,\"start\":62585},{\"end\":62625,\"start\":62604},{\"end\":62644,\"start\":62625},{\"end\":63085,\"start\":63071},{\"end\":63099,\"start\":63085},{\"end\":63115,\"start\":63099},{\"end\":63836,\"start\":63822},{\"end\":63850,\"start\":63836},{\"end\":63864,\"start\":63850},{\"end\":63880,\"start\":63864},{\"end\":63893,\"start\":63880},{\"end\":63899,\"start\":63893},{\"end\":63918,\"start\":63899},{\"end\":64541,\"start\":64528},{\"end\":64562,\"start\":64541},{\"end\":64574,\"start\":64562},{\"end\":64885,\"start\":64871},{\"end\":64899,\"start\":64885},{\"end\":64913,\"start\":64899},{\"end\":64926,\"start\":64913},{\"end\":65418,\"start\":65403},{\"end\":65432,\"start\":65418},{\"end\":65689,\"start\":65680},{\"end\":65696,\"start\":65689},{\"end\":65705,\"start\":65696},{\"end\":65715,\"start\":65705},{\"end\":65724,\"start\":65715},{\"end\":66311,\"start\":66302},{\"end\":66318,\"start\":66311},{\"end\":66327,\"start\":66318},{\"end\":66337,\"start\":66327},{\"end\":66347,\"start\":66337},{\"end\":66356,\"start\":66347},{\"end\":67033,\"start\":67016},{\"end\":67050,\"start\":67033},{\"end\":67064,\"start\":67050},{\"end\":67082,\"start\":67064},{\"end\":67788,\"start\":67775},{\"end\":67801,\"start\":67788},{\"end\":67813,\"start\":67801},{\"end\":67829,\"start\":67813},{\"end\":68311,\"start\":68301},{\"end\":68321,\"start\":68311},{\"end\":68330,\"start\":68321},{\"end\":68821,\"start\":68802},{\"end\":68834,\"start\":68821},{\"end\":68846,\"start\":68834},{\"end\":68863,\"start\":68846},{\"end\":69374,\"start\":69355},{\"end\":69388,\"start\":69374},{\"end\":69405,\"start\":69388},{\"end\":70121,\"start\":70112},{\"end\":70131,\"start\":70121},{\"end\":70140,\"start\":70131},{\"end\":70149,\"start\":70140},{\"end\":70156,\"start\":70149},{\"end\":70166,\"start\":70156},{\"end\":70923,\"start\":70909},{\"end\":70939,\"start\":70923},{\"end\":72116,\"start\":72102},{\"end\":72485,\"start\":72468},{\"end\":72499,\"start\":72485},{\"end\":72901,\"start\":72893},{\"end\":72918,\"start\":72901},{\"end\":72928,\"start\":72918},{\"end\":73265,\"start\":73251},{\"end\":73279,\"start\":73265},{\"end\":73300,\"start\":73279},{\"end\":73316,\"start\":73300},{\"end\":73332,\"start\":73316},{\"end\":73349,\"start\":73332},{\"end\":73955,\"start\":73945},{\"end\":73962,\"start\":73955},{\"end\":73972,\"start\":73962},{\"end\":73982,\"start\":73972},{\"end\":73991,\"start\":73982},{\"end\":75009,\"start\":74991},{\"end\":75023,\"start\":75009},{\"end\":75034,\"start\":75023},{\"end\":75048,\"start\":75034},{\"end\":75606,\"start\":75596},{\"end\":75615,\"start\":75606},{\"end\":75869,\"start\":75855},{\"end\":76066,\"start\":76054},{\"end\":76083,\"start\":76066},{\"end\":76099,\"start\":76083},{\"end\":77108,\"start\":77095},{\"end\":77124,\"start\":77108},{\"end\":77143,\"start\":77124},{\"end\":77157,\"start\":77143},{\"end\":77174,\"start\":77157},{\"end\":77498,\"start\":77485},{\"end\":77510,\"start\":77498},{\"end\":77524,\"start\":77510},{\"end\":77532,\"start\":77524},{\"end\":77549,\"start\":77532},{\"end\":77565,\"start\":77549},{\"end\":78331,\"start\":78318},{\"end\":78342,\"start\":78331},{\"end\":78350,\"start\":78342},{\"end\":78367,\"start\":78350},{\"end\":79122,\"start\":79108},{\"end\":79138,\"start\":79122},{\"end\":79151,\"start\":79138},{\"end\":79162,\"start\":79151},{\"end\":79176,\"start\":79162},{\"end\":79184,\"start\":79176},{\"end\":79949,\"start\":79935},{\"end\":79968,\"start\":79949},{\"end\":79986,\"start\":79968},{\"end\":80004,\"start\":79986},{\"end\":80024,\"start\":80004},{\"end\":80531,\"start\":80512},{\"end\":80543,\"start\":80531},{\"end\":80550,\"start\":80543},{\"end\":81308,\"start\":81287},{\"end\":81320,\"start\":81308},{\"end\":81337,\"start\":81320},{\"end\":81353,\"start\":81337},{\"end\":81370,\"start\":81353},{\"end\":82047,\"start\":82028},{\"end\":82066,\"start\":82047},{\"end\":82083,\"start\":82066},{\"end\":82099,\"start\":82083},{\"end\":82121,\"start\":82099},{\"end\":82462,\"start\":82454},{\"end\":82475,\"start\":82462},{\"end\":82486,\"start\":82475},{\"end\":82499,\"start\":82486},{\"end\":82512,\"start\":82499},{\"end\":82525,\"start\":82512},{\"end\":82536,\"start\":82525},{\"end\":82546,\"start\":82536},{\"end\":83059,\"start\":83040},{\"end\":83078,\"start\":83059},{\"end\":83092,\"start\":83078},{\"end\":83109,\"start\":83092},{\"end\":83125,\"start\":83109},{\"end\":83712,\"start\":83697},{\"end\":83728,\"start\":83712},{\"end\":83751,\"start\":83728},{\"end\":84801,\"start\":84792},{\"end\":84813,\"start\":84801},{\"end\":84824,\"start\":84813},{\"end\":84837,\"start\":84824},{\"end\":85230,\"start\":85214},{\"end\":85244,\"start\":85230},{\"end\":85257,\"start\":85244},{\"end\":85274,\"start\":85257},{\"end\":85287,\"start\":85274},{\"end\":85302,\"start\":85287},{\"end\":85317,\"start\":85302},{\"end\":85335,\"start\":85317},{\"end\":86142,\"start\":86124},{\"end\":86160,\"start\":86142},{\"end\":86178,\"start\":86160},{\"end\":86194,\"start\":86178},{\"end\":86206,\"start\":86194},{\"end\":86221,\"start\":86206},{\"end\":86773,\"start\":86764},{\"end\":86784,\"start\":86773},{\"end\":86794,\"start\":86784},{\"end\":86807,\"start\":86794},{\"end\":86821,\"start\":86807},{\"end\":86830,\"start\":86821},{\"end\":86843,\"start\":86830},{\"end\":87669,\"start\":87656},{\"end\":87682,\"start\":87669},{\"end\":87697,\"start\":87682},{\"end\":87710,\"start\":87697},{\"end\":88023,\"start\":88012},{\"end\":88030,\"start\":88023},{\"end\":88039,\"start\":88030},{\"end\":88049,\"start\":88039},{\"end\":88058,\"start\":88049},{\"end\":88787,\"start\":88772},{\"end\":88804,\"start\":88787},{\"end\":88832,\"start\":88804},{\"end\":88847,\"start\":88832},{\"end\":88865,\"start\":88847},{\"end\":88881,\"start\":88865},{\"end\":88895,\"start\":88881},{\"end\":88909,\"start\":88895},{\"end\":88918,\"start\":88909},{\"end\":88928,\"start\":88918},{\"end\":88935,\"start\":88928},{\"end\":89809,\"start\":89798},{\"end\":89822,\"start\":89809},{\"end\":89830,\"start\":89822},{\"end\":89844,\"start\":89830},{\"end\":90294,\"start\":90281},{\"end\":90306,\"start\":90294},{\"end\":90598,\"start\":90583},{\"end\":90618,\"start\":90598},{\"end\":90635,\"start\":90618},{\"end\":90650,\"start\":90635},{\"end\":90669,\"start\":90650}]", "bib_venue": "[{\"end\":55815,\"start\":55737},{\"end\":56480,\"start\":56459},{\"end\":57158,\"start\":57083},{\"end\":57813,\"start\":57793},{\"end\":58329,\"start\":58311},{\"end\":59144,\"start\":59007},{\"end\":60373,\"start\":60226},{\"end\":62022,\"start\":62014},{\"end\":62337,\"start\":62322},{\"end\":62731,\"start\":62711},{\"end\":63148,\"start\":63140},{\"end\":63471,\"start\":63448},{\"end\":65017,\"start\":65010},{\"end\":65934,\"start\":65861},{\"end\":66602,\"start\":66484},{\"end\":67298,\"start\":67201},{\"end\":67945,\"start\":67932},{\"end\":68402,\"start\":68388},{\"end\":68976,\"start\":68952},{\"end\":69626,\"start\":69549},{\"end\":71489,\"start\":71231},{\"end\":72165,\"start\":72149},{\"end\":72577,\"start\":72557},{\"end\":73468,\"start\":73446},{\"end\":74180,\"start\":74097},{\"end\":75235,\"start\":75137},{\"end\":76463,\"start\":76298},{\"end\":77204,\"start\":77193},{\"end\":77872,\"start\":77727},{\"end\":78674,\"start\":78529},{\"end\":79448,\"start\":79334},{\"end\":80130,\"start\":80113},{\"end\":80801,\"start\":80771},{\"end\":81548,\"start\":81532},{\"end\":82719,\"start\":82641},{\"end\":83240,\"start\":83222},{\"end\":84203,\"start\":83988},{\"end\":85576,\"start\":85541},{\"end\":86300,\"start\":86279},{\"end\":87128,\"start\":87023},{\"end\":88309,\"start\":88279},{\"end\":90005,\"start\":89933},{\"end\":90734,\"start\":90727},{\"end\":55670,\"start\":55583},{\"end\":56457,\"start\":56401},{\"end\":57036,\"start\":56968},{\"end\":57791,\"start\":57724},{\"end\":58288,\"start\":58232},{\"end\":58946,\"start\":58818},{\"end\":59700,\"start\":59659},{\"end\":60224,\"start\":60062},{\"end\":61016,\"start\":60934},{\"end\":61472,\"start\":61354},{\"end\":62012,\"start\":61989},{\"end\":62320,\"start\":62247},{\"end\":62709,\"start\":62653},{\"end\":63138,\"start\":63115},{\"end\":63375,\"start\":63318},{\"end\":63995,\"start\":63938},{\"end\":64526,\"start\":64443},{\"end\":65008,\"start\":64926},{\"end\":65401,\"start\":65317},{\"end\":65817,\"start\":65747},{\"end\":66482,\"start\":66379},{\"end\":67199,\"start\":67102},{\"end\":67930,\"start\":67857},{\"end\":68386,\"start\":68330},{\"end\":68950,\"start\":68886},{\"end\":69500,\"start\":69428},{\"end\":70315,\"start\":70189},{\"end\":71185,\"start\":70939},{\"end\":72147,\"start\":72116},{\"end\":72555,\"start\":72499},{\"end\":72976,\"start\":72952},{\"end\":73444,\"start\":73374},{\"end\":74062,\"start\":73991},{\"end\":74684,\"start\":74651},{\"end\":75135,\"start\":75048},{\"end\":75656,\"start\":75643},{\"end\":75885,\"start\":75869},{\"end\":76252,\"start\":76099},{\"end\":77191,\"start\":77174},{\"end\":77725,\"start\":77565},{\"end\":78527,\"start\":78367},{\"end\":79320,\"start\":79207},{\"end\":80111,\"start\":80050},{\"end\":80662,\"start\":80550},{\"end\":81475,\"start\":81393},{\"end\":82140,\"start\":82121},{\"end\":82639,\"start\":82546},{\"end\":83220,\"start\":83146},{\"end\":83986,\"start\":83770},{\"end\":84967,\"start\":84853},{\"end\":85447,\"start\":85335},{\"end\":86277,\"start\":86221},{\"end\":86967,\"start\":86866},{\"end\":87654,\"start\":87568},{\"end\":88170,\"start\":88058},{\"end\":89052,\"start\":88935},{\"end\":89931,\"start\":89844},{\"end\":90279,\"start\":90219},{\"end\":90725,\"start\":90669}]"}}}, "year": 2023, "month": 12, "day": 17}