{"id": 53112003, "updated": "2023-10-01 13:27:51.847", "metadata": {"title": "On the Effectiveness of Interval Bound Propagation for Training Verifiably Robust Models", "authors": "[{\"first\":\"Sven\",\"last\":\"Gowal\",\"middle\":[]},{\"first\":\"Krishnamurthy\",\"last\":\"Dvijotham\",\"middle\":[]},{\"first\":\"Robert\",\"last\":\"Stanforth\",\"middle\":[]},{\"first\":\"Rudy\",\"last\":\"Bunel\",\"middle\":[]},{\"first\":\"Chongli\",\"last\":\"Qin\",\"middle\":[]},{\"first\":\"Jonathan\",\"last\":\"Uesato\",\"middle\":[]},{\"first\":\"Relja\",\"last\":\"Arandjelovic\",\"middle\":[]},{\"first\":\"Timothy\",\"last\":\"Mann\",\"middle\":[]},{\"first\":\"Pushmeet\",\"last\":\"Kohli\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2018, "month": 10, "day": 30}, "abstract": "Recent works have shown that it is possible to train models that are verifiably robust to norm-bounded adversarial perturbations. While these recent methods show promise, they remain hard to scale and difficult to tune. This paper investigates how interval bound propagation (IBP) using simple interval arithmetic can be exploited to train verifiably robust neural networks that are surprisingly effective. While IBP itself has been studied in prior work, our contribution is in showing that, with an appropriate loss and careful tuning of hyper-parameters, verified training with IBP leads to a fast and stable learning algorithm. We compare our approach with recent techniques, and train classifiers that improve on the state-of-the-art in single-model adversarial robustness: we reduce the verified error rate from 3.67% to 2.23% on MNIST (with $\\ell_\\infty$ perturbations of $\\epsilon = 0.1$), from 19.32% to 8.05% on MNIST (at $\\epsilon = 0.3$), and from 78.22% to 72.91% on CIFAR-10 (at $\\epsilon = 8/255$).", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "1810.12715", "mag": "2898963688", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-1810-12715", "doi": null}}, "content": {"source": {"pdf_hash": "246c1f44332cb28b28f5d847236bdfbd4ba26bae", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1810.12715v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "5a41eeb5c31ae90ab9481ac163a81f1c44f0a601", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/246c1f44332cb28b28f5d847236bdfbd4ba26bae.txt", "contents": "\nOn the Effectiveness of Interval Bound Propagation for Training Verifiably Robust Models\n\n\nSven Gowal sgowal@google.com \nUniversity of Oxford\n\n\nKrishnamurthy ( Dj \nUniversity of Oxford\n\n\n) Dvijotham \nUniversity of Oxford\n\n\nRobert Stanforth stanforth@google.com \nUniversity of Oxford\n\n\nRudy Bunel \nUniversity of Oxford\n\n\nChongli Qin Deepmind chongliqin@google.com \nUniversity of Oxford\n\n\nJonathan Uesato Deepmind \nUniversity of Oxford\n\n\nRelja Arandjelovi\u0107 Deepmind \nUniversity of Oxford\n\n\nTimothy Mann Deepmind timothymann@google.com \nUniversity of Oxford\n\n\nPushmeet Kohli Deepmind pushmeet@google.com \nUniversity of Oxford\n\n\nOn the Effectiveness of Interval Bound Propagation for Training Verifiably Robust Models\n\nRecent works have shown that it is possible to train models that are verifiably robust to norm-bounded adversarial perturbations. While these recent methods show promise, they remain hard to scale and difficult to tune. This paper investigates how interval bound propagation (IBP) using simple interval arithmetic can be exploited to train verifiably robust neural networks that are surprisingly effective. While IBP itself has been studied in prior work, our contribution is in showing that, with an appropriate loss and careful tuning of hyper-parameters, verified training with IBP leads to a fast and stable learning algorithm. We compare our approach with recent techniques, and train classifiers that improve on the state-of-the-art in single-model adversarial robustness: we reduce the verified error rate from 3.67% to 2.23% on MNIST (with \u221e perturbations of = 0.1), from 19.32% to 8.05% on MNIST (at = 0.3), and from 78.22% to 72.91% on CIFAR-10 (at = 8/255).\n\nIntroduction\n\nDespite the successes of deep learning, it is well-known that neural networks are not robust. In particular, it has been shown that the addition of small but carefully chosen deviations to the input, called adversarial perturbations, can cause the neural network to make incorrect predictions with high confidence [2,3,10,13,16]. Robust optimization techniques, like the one developed by Madry et al. [14], overcome this problem by trying to find the worst case adversarial examples at each training step and adding it to the training data. While the resulting models show strong empirical evidence that they are robust against many attacks, we cannot yet guarantee that a stronger adversary (for example, one that does brute-force enumeration to compute adversarial perturbations) cannot find inputs that cause the model to predict incorrectly. In Appendix E, we provide an example that motivates why Projected Gradient Descent (PGD) -the technique at the core of Madry et al.'s method -is not always capable of finding the worst case attack.\n\nThis has driven the need for formal verification: a provable guarantee that neural networks are consistent with a specification for all possible inputs to the network. Substantial progress has been made: from complete methods that use Satisfiability Modulo Theory (SMT) [4,8,11] or Mixed-Integer Programming (MIP) [1,5,17] to incomplete methods that rely on solving a convex relaxation of the verification problem [6,7,9,15,18,19,20]. Incomplete methods scale to larger models than complete methods and, as such, can be used inside the training loop to build models that are not only robust, but also intrinsically easier to verify [6,15,19,21].\n\nIn this paper, we study interval bound propagation (IBP), a simple algorithm for training verifiably robust classifiers. IBP allows to define a loss by computing an upper bound on the maximum difference between any pair of logits when the input can be perturbed within an \u221e norm-bounded ball. We show that this approach can achieve strong results, outperforming state-of-the art. The core approach behind IBP has been studied in previous papers -it is equivalent to the constant verifier used by Dvijotham et al. [6] and to the interval domain from Mirman et al. [15]. Beyond providing new baseline results, the contributions of this paper are as follows:\n\n\u2022 We propose several enhancements that improve the performance of IBP for verified training.\n\nIn particular, we differentiate ourselves from [15] by using a different loss function, and by eliding the last linear layer of the neural network, thereby improving our estimate of the worst case logits. We explain our training methodology by detailing how key hyper-parameters are scheduled throughout training. \u2022 We compare our trained models to those from other approaches in terms of robustness to Projected Gradient Descent (PGD) attacks [3] and show that they are competitive against Madry et al. [14] and Wong et al. [20] across a wide range of \u221e perturbation radii (hereafter denoted by ). We also compare IBP to Wong \n\n\nMethodology\n\nNeural network. We focus on feed-forward neural networks trained for classification tasks. The input to the network is denoted x 0 and its output is a vector of raw un-normalized predictions (hereafter logits) corresponding to its beliefs about which class x 0 belongs to. During training, the network is fed pairs of input x 0 and correct output label y true , and trained to minimize a misclassification loss, such as cross-entropy.\n\nFor clarity of presentation, we assume that the neural network is defined by a sequence of transformations h k for each of its K layers. That is, for an input z 0 (which we define formally in the next paragraph), we have\nz k = h k (z k\u22121 ) k = 1, . . . , K(1)\nVerification problem. We are interested in verifying that neural networks satisfy a specification by generating a proof that this specification holds. We consider specifications that require that for all inputs in some set X (x 0 ) around x 0 , the network output satisfies a linear relationship\nc T z K + d \u2264 0 \u2200z 0 \u2208 X (x 0 )(2)\nwhere c and d are a vector and a scalar that may depend on the nominal input x 0 and label y true . As shown by Dvijotham et al. [7], many useful verification problems fit this definition. In this paper, however, we focus on the robustness to adversarial perturbations within some \u221e norm-bounded ball around the nominal input x 0 . Adversarial robustness is achieved if no adversary can succeed in changing the classification outcome away from the true label y true , i.e., argmax z K = y true for all elements z 0 \u2208 X (x 0 ). Formally, we want to verify that for each class y:\n(e y \u2212 e ytrue ) T z K \u2264 0 \u2200z 0 \u2208 X (x 0 ) = {x | x \u2212 x 0 \u221e < }(3)\nwhere e i is the standard i th basis vector and is the perturbation radius.\n\nUltimately, verifying a specification like (2) can be done by searching for a counter-example:\nmaximize z0\u2208X (x0) c T z K + d subject to z k = h k (z k\u22121 ) k = 1, . . . , K(4)\nIf the optimal value of the above optimization problem is smaller than 0, the specification (2) is satisfied.\n\nInterval bound propagation. IBP's goal is to find an upper bound to the optimization problem (4). In this context, the simplest approach is to bound the activation z k of each layer by an axis-aligned bounding box (i.e., z k ( ) \u2264 z k \u2264 z k ( )) using interval arithmetic. For \u221e adversarial perturbations of size , we have for each element z k,i of z k :\nz k,i ( ) = minimize z k\u22121 ( )\u2264z k\u22121 \u2264z k\u22121 ( ) e T i h k (z k\u22121 ) and z k,i ( ) = maximize z k\u22121 ( )\u2264z k\u22121 \u2264z k\u22121 ( ) e T i h k (z k\u22121 )(5)\nwhere z 0 ( ) = x 0 \u2212 1 and z 0 ( ) = x 0 + 1. As shown in Appendix A, the above optimization problems can be solved quickly for linear and ReLU layers.\n\nFinally, the upper and lower bounds of the output logits z K can be used to construct an upper bound on the solution of (4): maximize\nz K ( )\u2264z K \u2264z K ( ) c T z K + d(6)\nAs shown in Appendix A, a tighter upper bound can be computed by eliding the final linear layer with the specification. Overall, the adversarial specification (3) is upper-bounded by z K,y ( ) \u2212 z K,ytrue ( ). It corresponds to an upper bound on the worst-case logit difference between the true class y true and any other class y.\n\nLoss. In the context of classification under adversarial perturbation, the above specification corresponds to having the logit corresponding to the true class equal to its lower bound and the other logits equal to their upper bound:\u1e91\nK,y ( ) = z K,y ( ) if y = y true z K,ytrue ( ) otherwise(7)\nWe can then formulate our training loss as\nL = \u03ba (z K , y true ) Lfit +(1 \u2212 \u03ba) (\u1e91 K ( ), y true ) Lspec(8)\nwhere is the supervised cross-entropy loss and \u03ba is a hyperparameter that governs the relative weight of satisfying the specification (L spec ) versus fitting the data (L fit ). If = 0 then z K =\u1e91 K ( ), and thus (8) becomes equivalent to a standard classification loss. We note that L spec can also be used as a standard regularizer within other techniques (like adversarial training).\n\nTraining procedure. To stabilize the training process and get a good trade-off between nominal and adversarial accuracy, we create a learning curriculum by scheduling the values of \u03ba and when computing the loss (8).\n\nStarting with \u03ba = 1 and slowly reducing it throughout training helps get more balanced models that reach higher nominal accuracy (under no perturbation). In practice, we found that using a final value of \u03ba = 1/2 works well on MNIST and CIFAR-10. More importantly, starting with = 0 and slowly raising it up to a target perturbation radius train is necessary. We note that train does not need to be equal to the perturbation radius used during testing, as using higher values creates robust models that generalize better. For more details on the training procedure, see Appendix B.\n\n\nResults\n\nWe demonstrate that IBP can train verifiable networks and compare its performance to the state-ofthe-art on MNIST and CIFAR-10. For both datasets, we compare IBP to three alternative approaches: the nominal method, which corresponds to standard non-robust training with cross-entropy loss; Table 1: Comparison with the state-of-the-art. Comparison of the nominal test error, empirical PGD error rate, and verified bound on the error rate, along with the best known results from the literature. The test error corresponds to the test set error rate when there is no adversarial perturbation. The empirical PGD error rate is calculated using 200 iterations of PGD and 10 random restarts. When computationally feasible, we calculate the verified adversarial error rate using an exact verifier that relies on solving a MIP system [1] using the commercial Gurobi Solver with a timeout of 10 minutes (dashes \"-\" indicate that we were unable to verify these networks beyond the trivial 100% error rate bound within the imparted time limit). * Results reported from the literature (these results may use different architectures and different verification procedures). On these rows a dash \"-\" indicates that the corresponding result in not available.  Across the full spectrum, IBP acheives higher empirical accuracy (computed using PGD attacks) and higher provable accuracy (computed by an exact verifier). Table 1 provides punctual results (for more results see Table 3 in Appendix C \n\n\nDataset\n\n\nConclusion\n\nWe have presented an approach for training verifiable models and provided baseline results for MNIST and CIFAR-10. Our experiments have shown that the proposed approach outperforms competing techniques in terms of verified bounds on adversarial error rates in image classification problems, while also training faster. In the future, we hope that these results can serve as a useful baseline. We believe that this is an important step towards the vision of specification-driven ML. \n\n\nA Algorithmic details\n\nBound propagation for affine layers. As explained in Section 2, given bounds on the inputs z k\u22121 \u2264 z k\u22121 \u2264 z k\u22121 of layer k, it is useful to be able to efficiently compute bounds on the outputs z k \u2264 h k (z k\u22121 ) \u2264 z k . In other words, we are interested in solving the following optimization of all i:\nz k,i / z k,i = min / max z k\u22121 \u2264z k\u22121 \u2264z k\u22121 e T i h k (z k\u22121 )(9)\nFor the affine layers (e.g., fully connected layers, convolutions) of the form h k (z k\u22121 ) = W z k\u22121 + b solving this optimization problem can be done efficiently with only two matrix multiplies:\n\u00b5 k\u22121 = z k\u22121 + z k\u22121 2 r k\u22121 = z k\u22121 \u2212 z k\u22121 2 \u00b5 k = W \u00b5 k\u22121 + b r k = |W |r k\u22121 z k = \u00b5 k \u2212 r k z k = \u00b5 k + r k(10)\nBound propagation for element-wise monotonic activations. Propagating bounds through any element-wise monotonic activation function (e.g., ReLU, tanh, sigmoid) is trivial. Assuming an increasing function h k , we have:\nz k = h k (z k\u22121 ) z k = h k (z k\u22121 )(11)\nNotice how for element-wise non-linearities the (z k , z k ) formulation is better, while for affine transformations (\u00b5 k , r k ) is more efficient. Switching between parametrizations depending on h k incurs a slight computational overhead, but since affine layers are typically more computationally intensive , the formulation (10) is worth it.\n\nElision of last layer. Bound propagation is not necessary for the last linear layer of the network. Indeed, we can find an upper bound to the solution of (4) that is tighter than proposed by (6).\n\nAssuming h K (z K\u22121 ) = W z K\u22121 + b, we have:\nmaximize z K \u2264z K \u2264z K c T z K + d \u2265 maximize z K\u22121 \u2264z K\u22121 \u2264z K\u22121 c T h K (z K\u22121 ) + d = maximize z K\u22121 \u2264z K\u22121 \u2264z K\u22121 c T W z K\u22121 + c T b + d = maximize z K\u22121 \u2264z K\u22121 \u2264z K\u22121 c T z K\u22121 + d(12)\nwith c = W T c and d = c T b + d.\n\n\nB Network sizes and training parameters\n\nArchitectures. The architectures of the three models used in this paper are presented in Table 2. The first two models (i.e., small and medium) are equivalent to the small and large models in Wong et al. [20] 5 . To the best of our knowledge, the large model is significantly larger (in terms of number of hidden units) than any other verified model presented in the literature. Table 2: Architecture of the three models used in this paper. All layers are followed by RELU activations. The last fully connected layer is omitted. \"CONV k w\u00d7h+s\" corresponds to a 2D convolutional layer with k filters of size w\u00d7h using a stride of s in both dimensions. \"FC n\" corresponds to a fully connected layer with n outputs. Training procedure. We train for 100 and 350 epochs with batch sizes of 100 and 50 on MNIST and CIFAR-10 respectively (using the train and valid set). Hence, the total number of training steps is 60K for MNIST and 350K for CIFAR-10. The networks were trained using the Adam [12] algorithm with an initial learning rate of 10 \u22123 . For MNIST, we decay the learning rate by 10\u00d7 at steps 15K and 25K. For CIFAR-10, we decay the learning rate by 10\u00d7 at steps 200K, 250K and 300K.\n\nTo stabilize the training process and get a good trade-off between nominal and adversarial accuracy, we create a curriculum using \u03ba and (see (8)):\n\n\u2022 \u03ba controls the relative weight of satisfying the specification versus fitting the data. Hence, we found that starting with \u03ba = 1 and slowly reducing it throughout training helps get more balanced models with higher nominal accuracy. In practice, we found that using a final value of \u03ba = 1/2 works well on MNIST and CIFAR-10, although we have not particularly tuned this value. \u2022 More importantly, we found that starting with = 0 and slowly raising it up to a target perturbation radius train was necessary. We note that train does not need to be equal to the perturbation radius used during testing, using higher values creates robust models that generalize better.\n\nOverall, after a warm-up period of 2K and 10K steps for MNIST and CIFAR-10 respectively, we linearly anneal both hyperparameters for 10K and 150K steps.\n\nThe networks trained using Wong et al. [20] were trained using the schedule and learning rate proposed by the authors. For Madry et al. [14], we used a learning rate schedule identical to IBP and, for the inner optimization, adversarial examples are generated by 7 steps of PGD with Adam [12] and a learning rate of 10 \u22121 . Note that our reported results for these two methods closely match or beat published results, giving us confidence that we performed a fair comparison.  . This plot shows the median performance (along with the 25 th and 75 th percentiles across 10 independent training processes) and confirms that IBP is stable and produces consistent results. Additionally, for IBP, we clearly see the effect of ramping the value of up during training (which happens between steps 2K and 12K). Table 3 reports the verified robustness for a larger set of perturbation radii than Table 1 and also provides additional results from the literature. We always report results with respect to the complete test set of 10K images for both MNIST and CIFAR-10. The test error corresponds to the test set error rate when there is no adversarial perturbation. For self-trained models, the PGD error rate is calculated using 200 iterations of PGD and 10 random restarts. We calculate the verified adversarial error rate using an exact verifier that relies on solving a MIP system [1] using the commercial Gurobi Solver with a timeout of 10 minutes 6 . Upon timeout, we fallback to solving a relaxation of the verification problem with an LP [8] (using Gurobi again). When both approaches fail to provide a solution within the imparted time, we count the example as attackable. Thus, the verified error rate reported in this table may be over-estimating the exact verified error rate. All methods use the same model architectures (except results from the literature). For clarity, we do not report the results for all train values (i.e., train \u2208 {0.1, 0.2, 0.3, 0.4} for MNIST and train \u2208 {2/255, 8/255} for CIFAR-10). Figure 1a shows the effect of train in an easily digestible form.\n\n\nC Additional results\n\nOverall, IBP is not only competitive with the state-of-the-art under PGD attack, but also demonstrates better verified robustness across the board -except for CIFAR-10 with = 2/255 where it performs similarly to Xiao et al. [21] and is worse than Wong et al. [20]. This effect remains to be investigated in more details: from our experience, the method from Wong et al. provides tighter bounds when the perturbation radius is small, thus giving better feedback when training on CIFAR-10 at = 2/255. We also note that, although IBP is a subset of Dvijotham et al. [6], it performs on par (or even better), which highlights the difficulty of training robust model with more complicated methods.\n\nFor completeness, Figure 3 shows the empirical accuracy against PGD attacks of varying intensity for all models. We observe that IBP tends to be slightly worse than Madry et al. for similar network sizes -except for the large model where Madry et al. is likely overfitting (as it performs worse than the medium-sized model).   [21], the reported verified bound on the error rate is not computed with an exact solver and may be over-estimated. ** For this model, [21] only provides estimates computed from 1000 samples (rather than the full 10K images). *** [15] only provides estimates computed from 500 samples (rather than the full 10K images). Additionally, they use a slightly smaller = 0.007 = 1. D Convolutional filters Figure 4 shows the first layer convolutional filters resulting from training a small robust model on MNIST against a perturbation radius of = 0.1. Overall, the filters tend to be extremely sparse -at least when compared to the filters obtained by training a nominal non-robust model (this observation is consistent with [20]). We can qualitatively observe that Wong et al. produces the sparsest set of filters.\n\nSimilarly, as shown in Figure 5, robust models trained on CIFAR-10 exhibit high levels of sparsity in their convolutional filters. Madry et al. seems to produce more meaningful filters, but they remain sparse compared to the non-robust model.    \n\n\nE When Projected Gradient Descent is not enough\n\nFor a given example in MNIST, this section compares the worst-case attack found by PGD with the one found using a complete solver. The underlying model is a medium sized network trained using IBP with = 0.1. The nominal image, visible in Figure 6a, has the label \"eight\", and corresponds to the 1365 th image of the test set.\n\nThe worst-case perturbation of size = 0.1 found using 200 PGD iterations and 10 random restarts is shown in Figure 6b. In this particular case, the robust network is still able to successfully classify the attack as an \"eight\". Without any verifiable proof, we could wrongly assume that our network is robust to \u221e perturbation on that image. However, when running a complete solver (using a MIP formulation), we are able to find a counter-example that successfully induces the model to misclassify the \"eight\" as a \"two\" (as shown in Figure 6c).  Figure 7 shows the untargeted adversarial loss (optimized by PGD) around the nominal image. In these loss landscapes, we vary the input along a linear space defined by the worse perturbations found by PGD and the MIP solver. The u and v axes represent the magnitude of the perturbation added in each of these directions respectively and the z axis represents the loss. Typical cases where PGD is not optimal are often a combination of two factors that are qualitively visible in this figure:\n\n\u2022 We can observe that the MIP attack only exists in a corner of the projected \u221e -bounded ball around the nominal image. Indeed, since PGD is a gradient-based method, it relies on taking gradient steps of a given magnitude (that depends on the learning rate) at each iteration. That is, unless we allow the learning rate to decay to a sufficiently small value, the reprojection on the norm-bounded ball at each iteration will force the PGD solution to bounce between the edges of that ball without hitting its corner. \u2022 The second, more subtle, effect concerns the gradient direction. Figure 7b, which shows a top-view of the loss landscape, indicates that a large portion of \u221e ball around the nominal image pushes the PGD solution towards the right (rather than the bottom). In other words, gradients cannot always be trusted to point towards the true worst-case attack. : Loss landscapes around the nominal image of an \"eight\". It is generated by varying the input to the model, starting from the original input image toward either the worst attack found using PGD (u direction) or the one found using a complete solver (v direction). In (a), the z axis represents the loss and the orange and blue colors on the surface represent the classification predicted by the model. We observe that while the PGD attack (blue dot) is correctly classified as an \"eight\", the MIP attack (red dot) is misclassified as a \"two\". Panel (b) shows a top-view of the same landscape with the decision boundary in black. For both panels, the diamond-shape represents the projected \u221e ball of size = 0.1 around the nominal image.\n\nFigure 1 compares\n1IBP to Wong et al. on MNIST for all perturbation radii between 0 and 0.45 across all models.\n\nFigure 1 :\n1Accuracy against different adversarial perturbations: (a) shows the verified/provable worst-case accuracy, and (b) shows the empirical adversarial accuracy computed by running PGD. Faded lines show individual models, while bold lines show the best accuracy across all models. In (a), for Wong et al., the dots correspond to exact bounds computed using a MIP solver, while the black bold line corresponds to a lower bound computed using [20] without random projections.\n\nFigure 2 :\n2Median evolution of the nominal (no attacks) and empirical PGD accuracy (under perturbations of = 0.3) as training progresses for 10 independently trained large models on MNIST. The shaded areas indicate the 25 th and 75 th percentiles.\n\nFigure 2\n2shows how the empirical PGD accuracy (on the test set) increases as training progresses for IBP and Madry et al.\n\n6Figure 3 :\n3As an example, for Wong et al., there are 3 timeouts at = 0.1, 18 timeouts at = 0.2 and 58 timeouts at = 0.3 for the 10K examples of the test set. 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 Perturbation radius ( Empirical adversarial accuracy computed by running PGD. Faded lines show individual models, while bold lines show the best accuracy across all models.\n\nFigure 4 :\n4First layer convolutional filters resulting from training a small robust model on MNIST against a perturbation radius of = 0.1 for all methods.\n\nFigure 5 :\n5First layer convolutional filters resulting from training a small robust model on CIFAR-10 against a perturbation radius of = 2/255 for all methods.\n\nFigure 6 :\n6Attacks of size = 0.1 found on the 1365 th image of the MNIST test set. For (b) and (c), the left pane shows the adversarial image, while the right pane shows the perturbation rescaled for clarity.\n\nFigure 7\n7Figure 7: Loss landscapes around the nominal image of an \"eight\". It is generated by varying the input to the model, starting from the original input image toward either the worst attack found using PGD (u direction) or the one found using a complete solver (v direction). In (a), the z axis represents the loss and the orange and blue colors on the surface represent the classification predicted by the model. We observe that while the PGD attack (blue dot) is correctly classified as an \"eight\", the MIP attack (red dot) is misclassified as a \"two\". Panel (b) shows a top-view of the same landscape with the decision boundary in black. For both panels, the diamond-shape represents the projected \u221e ball of size = 0.1 around the nominal image.\n\n\net al. in terms of verified robustness by using a complete Mixed-Integer Programming (MIP) solver. \u2022 We demonstrate that IBP is not only computationally cheaper, but achieves the state-of-theart in single-model 2 verified robustness on MNIST (with verified upper bounds on the error rate of 2.23% and 8.05% at = 0.1 and = 0.3 respectively) and CIFAR-10 (with a verified error rate of 72.91% at = 8/255).\n\n\nadversarial training, following Madry et al.[14], which generates adversarial examples on the fly during training; and Wong et al.[20], which trains models that are provably robust. As detailed in Appendix B, we train three different model sizes (i.e., small, medium and large) for each of the four methods. On MNIST, for each model and each method, we trained models that are robust to a wide range of perturbation radii by setting train to 0.1, 0.2, 0.3 or 0.4. On CIFAR-10, we train the same models using IBP and Madry et al. with train \u2208 {2/255, 8/255} 3 .Epsilon \nMethod \nTest error \nPGD Verified \n\nMNIST \n= 0.1 \n\nNominal \n0.65% \n27.72% \n-\nMadry et al. \n0.59% \n1.34% \n-\nWong et al. \n1.08% \n2.89% \n3.01% \nIBP \n1.06% \n2.11% \n2.23% \nWong et al. [20]* \n1.08% \n-\n3.67% \n\nMNIST \n= 0.3 \n\nNominal \n0.65% \n99.63% \n-\nMadry et al. \n0.70% \n3.73% \n-\nWong et al. \n13.52% \n26.16% \n26.92% \nIBP \n1.66% \n6.12% \n8.05% \nXiao et al. [21]* \n2.67% \n7.95% \n19.32% \n\nCIFAR-10 \n= 8/255 \n\nNominal \n16.66% 100.00% 100.00% \nMadry et al. \n20.33% \n72.02% \n-\nIBP \n47.91% \n64.53% \n72.91% \nWong et al. [20]* \n71.33% \n-\n78.22% \n\n\n\n\n). Compared to Wong et al., IBP achieves lower error rates under normal and adversarial conditions, as well as better verifiable bounds, settings the state-of-the-art in verified robustness to adversarial attacks. Additionally, IBP remains competitive against Madry et al. by achieving a lower PGD error rate on CIFAR-10 (albeit at the cost of an increased nominal error rate) 4 . Finally, we note that when training the small network on MNIST with a Titan Xp GPU (where standard training takes 1.5 seconds per epoch), IBP only takes 3.5 seconds per epoch compared to 8.5 seconds for Madry et al. and 2 minutes for Wong et al. (using random projection of 50 dimensions). Indeed, IBP creates only two additional passes through the network (as detailed in Appendix A), compared to Madry et al. for which we used seven PGD steps.\n\nTable 3 :\n3Comparison with the state-of-the-art.Comparison of the nominal test error (under no perturbation), \n\nThe use of ensembles or cascades (as done by Wong et al.[20]) is orthogonal to the work presented here.\nDuring testing, we can verity robustness against different values (not necessarily the ones trained on).4 This result only holds for our constrained set of network sizes. The best known empirical adversarial error rate for CIFAR-10 at = 8/255 using Madry et al. is 52.96%.\nWe do not train our large model with Wong et al. as we could not scale the method to our largest model.\n\nR Bunel, I Turkaslan, P H Torr, P Kohli, M P Kumar, arXiv:1711.00455Piecewise linear neural network verification: A comparative study. arXiv preprintR. Bunel, I. Turkaslan, P. H. Torr, P. Kohli, and M. P. Kumar. Piecewise linear neural network verification: A comparative study. arXiv preprint arXiv:1711.00455, 2017.\n\nAdversarial examples are not easily detected: Bypassing ten detection methods. N Carlini, D Wagner, Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security. the 10th ACM Workshop on Artificial Intelligence and SecurityACMN. Carlini and D. Wagner. Adversarial examples are not easily detected: Bypassing ten detection methods. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, pages 3-14. ACM, 2017.\n\nTowards evaluating the robustness of neural networks. N Carlini, D Wagner, Security and Privacy (SP), 2017 IEEE Symposium on. IEEEN. Carlini and D. Wagner. Towards evaluating the robustness of neural networks. In Security and Privacy (SP), 2017 IEEE Symposium on, pages 39-57. IEEE, 2017.\n\nN Carlini, G Katz, C Barrett, D L Dill, arXiv:1709.10207Ground-truth adversarial examples. arXiv preprintN. Carlini, G. Katz, C. Barrett, and D. L. Dill. Ground-truth adversarial examples. arXiv preprint arXiv:1709.10207, 2017.\n\nMaximum resilience of artificial neural networks. C.-H Cheng, G N\u00fchrenberg, H Ruess, International Symposium on Automated Technology for Verification and Analysis. SpringerC.-H. Cheng, G. N\u00fchrenberg, and H. Ruess. Maximum resilience of artificial neural networks. In International Symposium on Automated Technology for Verification and Analysis, pages 251-268. Springer, 2017.\n\nTraining verified learners with learned verifiers. K Dvijotham, S Gowal, R Stanforth, R Arandjelovic, B O&apos;donoghue, J Uesato, P Kohli, arXiv:1805.10265arXiv preprintK. Dvijotham, S. Gowal, R. Stanforth, R. Arandjelovic, B. O'Donoghue, J. Uesato, and P. Kohli. Training verified learners with learned verifiers. arXiv preprint arXiv:1805.10265, 2018.\n\nA dual approach to scalable verification of deep networks. K Dvijotham, R Stanforth, S Gowal, T Mann, P Kohli, arXiv:1803.06567arXiv preprintK. Dvijotham, R. Stanforth, S. Gowal, T. Mann, and P. Kohli. A dual approach to scalable verification of deep networks. arXiv preprint arXiv:1803.06567, 2018.\n\nFormal verification of piece-wise linear feed-forward neural networks. R Ehlers, International Symposium on Automated Technology for Verification and Analysis. SpringerR. Ehlers. Formal verification of piece-wise linear feed-forward neural networks. In International Symposium on Automated Technology for Verification and Analysis, pages 269-286. Springer, 2017.\n\nAi 2: Safety and robustness certification of neural networks with abstract interpretation. T Gehr, M Mirman, D Drachsler-Cohen, P Tsankov, S Chaudhuri, M Vechev, Security and Privacy. SP)T. Gehr, M. Mirman, D. Drachsler-Cohen, P. Tsankov, S. Chaudhuri, and M. Vechev. Ai 2: Safety and robustness certification of neural networks with abstract interpretation. In Security and Privacy (SP), 2018 IEEE Symposium on, 2018.\n\nExplaining and harnessing adversarial examples. I J Goodfellow, J Shlens, C Szegedy, arXiv:1412.6572arXiv preprintI. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.\n\nReluplex: An efficient smt solver for verifying deep neural networks. G Katz, C Barrett, D L Dill, K Julian, M J Kochenderfer, International Conference on Computer Aided Verification. SpringerG. Katz, C. Barrett, D. L. Dill, K. Julian, and M. J. Kochenderfer. Reluplex: An efficient smt solver for verifying deep neural networks. In International Conference on Computer Aided Verification, pages 97-117. Springer, 2017.\n\nAdam: A method for stochastic optimization. CoRR, abs/1412. D P Kingma, J Ba, 6980D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014. URL http://arxiv.org/abs/1412.6980.\n\nAdversarial examples in the physical world. A Kurakin, I Goodfellow, S Bengio, arXiv:1607.02533arXiv preprintA. Kurakin, I. Goodfellow, and S. Bengio. Adversarial examples in the physical world. arXiv preprint arXiv:1607.02533, 2016.\n\nTowards deep learning models resistant to adversarial attacks. A Madry, A Makelov, L Schmidt, D Tsipras, A Vladu, arXiv:1706.06083arXiv preprintA. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.\n\nDifferentiable abstract interpretation for provably robust neural networks. M Mirman, T Gehr, M Vechev, Proceedings of the 35th International Conference on Machine Learning. J. Dy and A. Krausethe 35th International Conference on Machine LearningStockholmsm\u00e4ssan, Stockholm Sweden80M. Mirman, T. Gehr, and M. Vechev. Differentiable abstract interpretation for provably robust neural networks. In J. Dy and A. Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 3578-3586, Stockholmsm\u00e4ssan, Stockholm Sweden, 10-15 Jul 2018. PMLR. URL http://proceedings.mlr.press/v80/mirman18b. html.\n\nC Szegedy, W Zaremba, I Sutskever, J Bruna, D Erhan, I Goodfellow, R Fergus, arXiv:1312.6199Intriguing properties of neural networks. arXiv preprintC. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.\n\nV Tjeng, R Tedrake, arXiv:1711.07356Verifying neural networks with mixed integer programming. arXiv preprintV. Tjeng and R. Tedrake. Verifying neural networks with mixed integer programming. arXiv preprint arXiv:1711.07356, 2017.\n\nTowards fast computation of certified robustness for ReLU networks. T.-W Weng, H Zhang, H Chen, Z Song, C.-J Hsieh, L Daniel, D Boning, I Dhillon, Proceedings of the 35th International Conference on Machine Learning. J. Dy and A. Krausethe 35th International Conference on Machine LearningStockholmsm\u00e4ssan, Stockholm Sweden80T.-W. Weng, H. Zhang, H. Chen, Z. Song, C.-J. Hsieh, L. Daniel, D. Boning, and I. Dhillon. Towards fast computation of certified robustness for ReLU networks. In J. Dy and A. Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 5273-5282, Stockholmsm\u00e4ssan, Stockholm Sweden, 10-15 Jul 2018. PMLR. URL http://proceedings.mlr.press/v80/weng18a.html.\n\nProvable defenses against adversarial examples via the convex outer adversarial polytope. E Wong, Z Kolter, International Conference on Machine Learning. E. Wong and Z. Kolter. Provable defenses against adversarial examples via the convex outer adversarial polytope. In International Conference on Machine Learning, pages 5283-5292, 2018.\n\n. E Wong, F Schmidt, J H Metzen, J Z Kolter, arXiv:1805.12514Scaling provable adversarial defenses. arXiv preprintE. Wong, F. Schmidt, J. H. Metzen, and J. Z. Kolter. Scaling provable adversarial defenses. arXiv preprint arXiv:1805.12514, 2018.\n\nK Y Xiao, V Tjeng, N M Shafiullah, A Madry, arXiv:1809.03008Training for faster adversarial robustness verification via inducing relu stability. arXiv preprintK. Y. Xiao, V. Tjeng, N. M. Shafiullah, and A. Madry. Training for faster adversarial robustness verification via inducing relu stability. arXiv preprint arXiv:1809.03008, 2018.\n", "annotations": {"author": "[{\"end\":144,\"start\":92},{\"end\":187,\"start\":145},{\"end\":223,\"start\":188},{\"end\":285,\"start\":224},{\"end\":320,\"start\":286},{\"end\":387,\"start\":321},{\"end\":436,\"start\":388},{\"end\":488,\"start\":437},{\"end\":557,\"start\":489},{\"end\":625,\"start\":558}]", "publisher": null, "author_last_name": "[{\"end\":102,\"start\":97},{\"end\":163,\"start\":161},{\"end\":199,\"start\":190},{\"end\":240,\"start\":231},{\"end\":296,\"start\":291},{\"end\":341,\"start\":333},{\"end\":412,\"start\":404},{\"end\":464,\"start\":456},{\"end\":510,\"start\":502},{\"end\":581,\"start\":573}]", "author_first_name": "[{\"end\":96,\"start\":92},{\"end\":158,\"start\":145},{\"end\":160,\"start\":159},{\"end\":189,\"start\":188},{\"end\":230,\"start\":224},{\"end\":290,\"start\":286},{\"end\":328,\"start\":321},{\"end\":332,\"start\":329},{\"end\":396,\"start\":388},{\"end\":403,\"start\":397},{\"end\":442,\"start\":437},{\"end\":455,\"start\":443},{\"end\":496,\"start\":489},{\"end\":501,\"start\":497},{\"end\":566,\"start\":558},{\"end\":572,\"start\":567}]", "author_affiliation": "[{\"end\":143,\"start\":122},{\"end\":186,\"start\":165},{\"end\":222,\"start\":201},{\"end\":284,\"start\":263},{\"end\":319,\"start\":298},{\"end\":386,\"start\":365},{\"end\":435,\"start\":414},{\"end\":487,\"start\":466},{\"end\":556,\"start\":535},{\"end\":624,\"start\":603}]", "title": "[{\"end\":89,\"start\":1},{\"end\":714,\"start\":626}]", "venue": null, "abstract": "[{\"end\":1684,\"start\":716}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2017,\"start\":2014},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2019,\"start\":2017},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2022,\"start\":2019},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2025,\"start\":2022},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2028,\"start\":2025},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2105,\"start\":2101},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3018,\"start\":3015},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3020,\"start\":3018},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3023,\"start\":3020},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3062,\"start\":3059},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3064,\"start\":3062},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3067,\"start\":3064},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3162,\"start\":3159},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3164,\"start\":3162},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3166,\"start\":3164},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3169,\"start\":3166},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3172,\"start\":3169},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3175,\"start\":3172},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3178,\"start\":3175},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3380,\"start\":3377},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3383,\"start\":3380},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3386,\"start\":3383},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3389,\"start\":3386},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3908,\"start\":3905},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3959,\"start\":3955},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4194,\"start\":4190},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4590,\"start\":4587},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4651,\"start\":4647},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4672,\"start\":4668},{\"end\":4769,\"start\":4765},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5945,\"start\":5942},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6918,\"start\":6915},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10402,\"start\":10399},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":13072,\"start\":13069},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":13597,\"start\":13593},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":13599,\"start\":13598},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":14380,\"start\":14376},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":15592,\"start\":15588},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":15689,\"start\":15685},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":15841,\"start\":15837},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":16927,\"start\":16924},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":17088,\"start\":17085},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":17880,\"start\":17876},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":17915,\"start\":17911},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":18218,\"start\":18215},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":18677,\"start\":18673},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":18812,\"start\":18808},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":18907,\"start\":18903},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":19396,\"start\":19392},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":25852,\"start\":25848},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":25938,\"start\":25934},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":27906,\"start\":27902},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":28055,\"start\":28054}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":22869,\"start\":22757},{\"attributes\":{\"id\":\"fig_1\"},\"end\":23351,\"start\":22870},{\"attributes\":{\"id\":\"fig_2\"},\"end\":23601,\"start\":23352},{\"attributes\":{\"id\":\"fig_3\"},\"end\":23725,\"start\":23602},{\"attributes\":{\"id\":\"fig_4\"},\"end\":24109,\"start\":23726},{\"attributes\":{\"id\":\"fig_6\"},\"end\":24266,\"start\":24110},{\"attributes\":{\"id\":\"fig_8\"},\"end\":24428,\"start\":24267},{\"attributes\":{\"id\":\"fig_9\"},\"end\":24639,\"start\":24429},{\"attributes\":{\"id\":\"fig_10\"},\"end\":25395,\"start\":24640},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":25801,\"start\":25396},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":26903,\"start\":25802},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":27732,\"start\":26904},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":27845,\"start\":27733}]", "paragraph": "[{\"end\":2743,\"start\":1700},{\"end\":3390,\"start\":2745},{\"end\":4047,\"start\":3392},{\"end\":4141,\"start\":4049},{\"end\":4770,\"start\":4143},{\"end\":5220,\"start\":4786},{\"end\":5442,\"start\":5222},{\"end\":5777,\"start\":5482},{\"end\":6390,\"start\":5813},{\"end\":6533,\"start\":6458},{\"end\":6629,\"start\":6535},{\"end\":6820,\"start\":6711},{\"end\":7176,\"start\":6822},{\"end\":7470,\"start\":7318},{\"end\":7605,\"start\":7472},{\"end\":7972,\"start\":7642},{\"end\":8207,\"start\":7974},{\"end\":8311,\"start\":8269},{\"end\":8762,\"start\":8376},{\"end\":8979,\"start\":8764},{\"end\":9561,\"start\":8981},{\"end\":11051,\"start\":9573},{\"end\":11558,\"start\":11076},{\"end\":11886,\"start\":11584},{\"end\":12151,\"start\":11955},{\"end\":12488,\"start\":12270},{\"end\":12876,\"start\":12531},{\"end\":13073,\"start\":12878},{\"end\":13120,\"start\":13075},{\"end\":13345,\"start\":13312},{\"end\":14576,\"start\":13389},{\"end\":14724,\"start\":14578},{\"end\":15393,\"start\":14726},{\"end\":15547,\"start\":15395},{\"end\":17627,\"start\":15549},{\"end\":18344,\"start\":17652},{\"end\":19482,\"start\":18346},{\"end\":19730,\"start\":19484},{\"end\":20107,\"start\":19782},{\"end\":21147,\"start\":20109},{\"end\":22756,\"start\":21149}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":5481,\"start\":5443},{\"attributes\":{\"id\":\"formula_1\"},\"end\":5812,\"start\":5778},{\"attributes\":{\"id\":\"formula_2\"},\"end\":6457,\"start\":6391},{\"attributes\":{\"id\":\"formula_3\"},\"end\":6710,\"start\":6630},{\"attributes\":{\"id\":\"formula_4\"},\"end\":7317,\"start\":7177},{\"attributes\":{\"id\":\"formula_5\"},\"end\":7641,\"start\":7606},{\"attributes\":{\"id\":\"formula_6\"},\"end\":8268,\"start\":8208},{\"attributes\":{\"id\":\"formula_7\"},\"end\":8375,\"start\":8312},{\"attributes\":{\"id\":\"formula_8\"},\"end\":11954,\"start\":11887},{\"attributes\":{\"id\":\"formula_9\"},\"end\":12269,\"start\":12152},{\"attributes\":{\"id\":\"formula_10\"},\"end\":12530,\"start\":12489},{\"attributes\":{\"id\":\"formula_11\"},\"end\":13311,\"start\":13121}]", "table_ref": "[{\"end\":9870,\"start\":9863},{\"end\":10980,\"start\":10973},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":11036,\"start\":11029},{\"end\":13485,\"start\":13478},{\"end\":13775,\"start\":13768},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":16359,\"start\":16352},{\"end\":16443,\"start\":16436}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1698,\"start\":1686},{\"attributes\":{\"n\":\"2\"},\"end\":4784,\"start\":4773},{\"attributes\":{\"n\":\"3\"},\"end\":9571,\"start\":9564},{\"end\":11061,\"start\":11054},{\"attributes\":{\"n\":\"4\"},\"end\":11074,\"start\":11064},{\"end\":11582,\"start\":11561},{\"end\":13387,\"start\":13348},{\"end\":17650,\"start\":17630},{\"end\":19780,\"start\":19733},{\"end\":22775,\"start\":22758},{\"end\":22881,\"start\":22871},{\"end\":23363,\"start\":23353},{\"end\":23611,\"start\":23603},{\"end\":23738,\"start\":23727},{\"end\":24121,\"start\":24111},{\"end\":24278,\"start\":24268},{\"end\":24440,\"start\":24430},{\"end\":24649,\"start\":24641},{\"end\":27743,\"start\":27734}]", "table": "[{\"end\":26903,\"start\":26364},{\"end\":27845,\"start\":27782}]", "figure_caption": "[{\"end\":22869,\"start\":22777},{\"end\":23351,\"start\":22883},{\"end\":23601,\"start\":23365},{\"end\":23725,\"start\":23613},{\"end\":24109,\"start\":23740},{\"end\":24266,\"start\":24123},{\"end\":24428,\"start\":24280},{\"end\":24639,\"start\":24442},{\"end\":25395,\"start\":24651},{\"end\":25801,\"start\":25398},{\"end\":26364,\"start\":25804},{\"end\":27732,\"start\":26906},{\"end\":27782,\"start\":27745}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":17571,\"start\":17562},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":18372,\"start\":18364},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":19080,\"start\":19072},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":19515,\"start\":19507},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":20029,\"start\":20020},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":20226,\"start\":20217},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":20652,\"start\":20643},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":20664,\"start\":20656},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":21742,\"start\":21733}]", "bib_author_first_name": "[{\"end\":28329,\"start\":28328},{\"end\":28338,\"start\":28337},{\"end\":28351,\"start\":28350},{\"end\":28353,\"start\":28352},{\"end\":28361,\"start\":28360},{\"end\":28370,\"start\":28369},{\"end\":28372,\"start\":28371},{\"end\":28727,\"start\":28726},{\"end\":28738,\"start\":28737},{\"end\":29154,\"start\":29153},{\"end\":29165,\"start\":29164},{\"end\":29390,\"start\":29389},{\"end\":29401,\"start\":29400},{\"end\":29409,\"start\":29408},{\"end\":29420,\"start\":29419},{\"end\":29422,\"start\":29421},{\"end\":29672,\"start\":29668},{\"end\":29681,\"start\":29680},{\"end\":29695,\"start\":29694},{\"end\":30048,\"start\":30047},{\"end\":30061,\"start\":30060},{\"end\":30070,\"start\":30069},{\"end\":30083,\"start\":30082},{\"end\":30099,\"start\":30098},{\"end\":30118,\"start\":30117},{\"end\":30128,\"start\":30127},{\"end\":30412,\"start\":30411},{\"end\":30425,\"start\":30424},{\"end\":30438,\"start\":30437},{\"end\":30447,\"start\":30446},{\"end\":30455,\"start\":30454},{\"end\":30725,\"start\":30724},{\"end\":31109,\"start\":31108},{\"end\":31117,\"start\":31116},{\"end\":31127,\"start\":31126},{\"end\":31146,\"start\":31145},{\"end\":31157,\"start\":31156},{\"end\":31170,\"start\":31169},{\"end\":31486,\"start\":31485},{\"end\":31488,\"start\":31487},{\"end\":31502,\"start\":31501},{\"end\":31512,\"start\":31511},{\"end\":31754,\"start\":31753},{\"end\":31762,\"start\":31761},{\"end\":31773,\"start\":31772},{\"end\":31775,\"start\":31774},{\"end\":31783,\"start\":31782},{\"end\":31793,\"start\":31792},{\"end\":31795,\"start\":31794},{\"end\":32165,\"start\":32164},{\"end\":32167,\"start\":32166},{\"end\":32177,\"start\":32176},{\"end\":32363,\"start\":32362},{\"end\":32374,\"start\":32373},{\"end\":32388,\"start\":32387},{\"end\":32617,\"start\":32616},{\"end\":32626,\"start\":32625},{\"end\":32637,\"start\":32636},{\"end\":32648,\"start\":32647},{\"end\":32659,\"start\":32658},{\"end\":32937,\"start\":32936},{\"end\":32947,\"start\":32946},{\"end\":32955,\"start\":32954},{\"end\":33542,\"start\":33541},{\"end\":33553,\"start\":33552},{\"end\":33564,\"start\":33563},{\"end\":33577,\"start\":33576},{\"end\":33586,\"start\":33585},{\"end\":33595,\"start\":33594},{\"end\":33609,\"start\":33608},{\"end\":33859,\"start\":33858},{\"end\":33868,\"start\":33867},{\"end\":34161,\"start\":34157},{\"end\":34169,\"start\":34168},{\"end\":34178,\"start\":34177},{\"end\":34186,\"start\":34185},{\"end\":34197,\"start\":34193},{\"end\":34206,\"start\":34205},{\"end\":34216,\"start\":34215},{\"end\":34226,\"start\":34225},{\"end\":34949,\"start\":34948},{\"end\":34957,\"start\":34956},{\"end\":35201,\"start\":35200},{\"end\":35209,\"start\":35208},{\"end\":35220,\"start\":35219},{\"end\":35222,\"start\":35221},{\"end\":35232,\"start\":35231},{\"end\":35234,\"start\":35233},{\"end\":35445,\"start\":35444},{\"end\":35447,\"start\":35446},{\"end\":35455,\"start\":35454},{\"end\":35464,\"start\":35463},{\"end\":35466,\"start\":35465},{\"end\":35480,\"start\":35479}]", "bib_author_last_name": "[{\"end\":28335,\"start\":28330},{\"end\":28348,\"start\":28339},{\"end\":28358,\"start\":28354},{\"end\":28367,\"start\":28362},{\"end\":28378,\"start\":28373},{\"end\":28735,\"start\":28728},{\"end\":28745,\"start\":28739},{\"end\":29162,\"start\":29155},{\"end\":29172,\"start\":29166},{\"end\":29398,\"start\":29391},{\"end\":29406,\"start\":29402},{\"end\":29417,\"start\":29410},{\"end\":29427,\"start\":29423},{\"end\":29678,\"start\":29673},{\"end\":29692,\"start\":29682},{\"end\":29701,\"start\":29696},{\"end\":30058,\"start\":30049},{\"end\":30067,\"start\":30062},{\"end\":30080,\"start\":30071},{\"end\":30096,\"start\":30084},{\"end\":30115,\"start\":30100},{\"end\":30125,\"start\":30119},{\"end\":30134,\"start\":30129},{\"end\":30422,\"start\":30413},{\"end\":30435,\"start\":30426},{\"end\":30444,\"start\":30439},{\"end\":30452,\"start\":30448},{\"end\":30461,\"start\":30456},{\"end\":30732,\"start\":30726},{\"end\":31114,\"start\":31110},{\"end\":31124,\"start\":31118},{\"end\":31143,\"start\":31128},{\"end\":31154,\"start\":31147},{\"end\":31167,\"start\":31158},{\"end\":31177,\"start\":31171},{\"end\":31499,\"start\":31489},{\"end\":31509,\"start\":31503},{\"end\":31520,\"start\":31513},{\"end\":31759,\"start\":31755},{\"end\":31770,\"start\":31763},{\"end\":31780,\"start\":31776},{\"end\":31790,\"start\":31784},{\"end\":31808,\"start\":31796},{\"end\":32174,\"start\":32168},{\"end\":32180,\"start\":32178},{\"end\":32371,\"start\":32364},{\"end\":32385,\"start\":32375},{\"end\":32395,\"start\":32389},{\"end\":32623,\"start\":32618},{\"end\":32634,\"start\":32627},{\"end\":32645,\"start\":32638},{\"end\":32656,\"start\":32649},{\"end\":32665,\"start\":32660},{\"end\":32944,\"start\":32938},{\"end\":32952,\"start\":32948},{\"end\":32962,\"start\":32956},{\"end\":33550,\"start\":33543},{\"end\":33561,\"start\":33554},{\"end\":33574,\"start\":33565},{\"end\":33583,\"start\":33578},{\"end\":33592,\"start\":33587},{\"end\":33606,\"start\":33596},{\"end\":33616,\"start\":33610},{\"end\":33865,\"start\":33860},{\"end\":33876,\"start\":33869},{\"end\":34166,\"start\":34162},{\"end\":34175,\"start\":34170},{\"end\":34183,\"start\":34179},{\"end\":34191,\"start\":34187},{\"end\":34203,\"start\":34198},{\"end\":34213,\"start\":34207},{\"end\":34223,\"start\":34217},{\"end\":34234,\"start\":34227},{\"end\":34954,\"start\":34950},{\"end\":34964,\"start\":34958},{\"end\":35206,\"start\":35202},{\"end\":35217,\"start\":35210},{\"end\":35229,\"start\":35223},{\"end\":35241,\"start\":35235},{\"end\":35452,\"start\":35448},{\"end\":35461,\"start\":35456},{\"end\":35477,\"start\":35467},{\"end\":35486,\"start\":35481}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1711.00455\",\"id\":\"b0\"},\"end\":28645,\"start\":28328},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":207599948},\"end\":29097,\"start\":28647},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":2893830},\"end\":29387,\"start\":29099},{\"attributes\":{\"doi\":\"arXiv:1709.10207\",\"id\":\"b3\"},\"end\":29616,\"start\":29389},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":26530174},\"end\":29994,\"start\":29618},{\"attributes\":{\"doi\":\"arXiv:1805.10265\",\"id\":\"b5\"},\"end\":30350,\"start\":29996},{\"attributes\":{\"doi\":\"arXiv:1803.06567\",\"id\":\"b6\"},\"end\":30651,\"start\":30352},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":1931807},\"end\":31015,\"start\":30653},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":206579396},\"end\":31435,\"start\":31017},{\"attributes\":{\"doi\":\"arXiv:1412.6572\",\"id\":\"b9\"},\"end\":31681,\"start\":31437},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":516928},\"end\":32102,\"start\":31683},{\"attributes\":{\"id\":\"b11\"},\"end\":32316,\"start\":32104},{\"attributes\":{\"doi\":\"arXiv:1607.02533\",\"id\":\"b12\"},\"end\":32551,\"start\":32318},{\"attributes\":{\"doi\":\"arXiv:1706.06083\",\"id\":\"b13\"},\"end\":32858,\"start\":32553},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":51872670},\"end\":33539,\"start\":32860},{\"attributes\":{\"doi\":\"arXiv:1312.6199\",\"id\":\"b15\"},\"end\":33856,\"start\":33541},{\"attributes\":{\"doi\":\"arXiv:1711.07356\",\"id\":\"b16\"},\"end\":34087,\"start\":33858},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":13750928},\"end\":34856,\"start\":34089},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":3659467},\"end\":35196,\"start\":34858},{\"attributes\":{\"doi\":\"arXiv:1805.12514\",\"id\":\"b19\"},\"end\":35442,\"start\":35198},{\"attributes\":{\"doi\":\"arXiv:1809.03008\",\"id\":\"b20\"},\"end\":35780,\"start\":35444}]", "bib_title": "[{\"end\":28724,\"start\":28647},{\"end\":29151,\"start\":29099},{\"end\":29666,\"start\":29618},{\"end\":30722,\"start\":30653},{\"end\":31106,\"start\":31017},{\"end\":31751,\"start\":31683},{\"end\":32934,\"start\":32860},{\"end\":34155,\"start\":34089},{\"end\":34946,\"start\":34858}]", "bib_author": "[{\"end\":28337,\"start\":28328},{\"end\":28350,\"start\":28337},{\"end\":28360,\"start\":28350},{\"end\":28369,\"start\":28360},{\"end\":28380,\"start\":28369},{\"end\":28737,\"start\":28726},{\"end\":28747,\"start\":28737},{\"end\":29164,\"start\":29153},{\"end\":29174,\"start\":29164},{\"end\":29400,\"start\":29389},{\"end\":29408,\"start\":29400},{\"end\":29419,\"start\":29408},{\"end\":29429,\"start\":29419},{\"end\":29680,\"start\":29668},{\"end\":29694,\"start\":29680},{\"end\":29703,\"start\":29694},{\"end\":30060,\"start\":30047},{\"end\":30069,\"start\":30060},{\"end\":30082,\"start\":30069},{\"end\":30098,\"start\":30082},{\"end\":30117,\"start\":30098},{\"end\":30127,\"start\":30117},{\"end\":30136,\"start\":30127},{\"end\":30424,\"start\":30411},{\"end\":30437,\"start\":30424},{\"end\":30446,\"start\":30437},{\"end\":30454,\"start\":30446},{\"end\":30463,\"start\":30454},{\"end\":30734,\"start\":30724},{\"end\":31116,\"start\":31108},{\"end\":31126,\"start\":31116},{\"end\":31145,\"start\":31126},{\"end\":31156,\"start\":31145},{\"end\":31169,\"start\":31156},{\"end\":31179,\"start\":31169},{\"end\":31501,\"start\":31485},{\"end\":31511,\"start\":31501},{\"end\":31522,\"start\":31511},{\"end\":31761,\"start\":31753},{\"end\":31772,\"start\":31761},{\"end\":31782,\"start\":31772},{\"end\":31792,\"start\":31782},{\"end\":31810,\"start\":31792},{\"end\":32176,\"start\":32164},{\"end\":32182,\"start\":32176},{\"end\":32373,\"start\":32362},{\"end\":32387,\"start\":32373},{\"end\":32397,\"start\":32387},{\"end\":32625,\"start\":32616},{\"end\":32636,\"start\":32625},{\"end\":32647,\"start\":32636},{\"end\":32658,\"start\":32647},{\"end\":32667,\"start\":32658},{\"end\":32946,\"start\":32936},{\"end\":32954,\"start\":32946},{\"end\":32964,\"start\":32954},{\"end\":33552,\"start\":33541},{\"end\":33563,\"start\":33552},{\"end\":33576,\"start\":33563},{\"end\":33585,\"start\":33576},{\"end\":33594,\"start\":33585},{\"end\":33608,\"start\":33594},{\"end\":33618,\"start\":33608},{\"end\":33867,\"start\":33858},{\"end\":33878,\"start\":33867},{\"end\":34168,\"start\":34157},{\"end\":34177,\"start\":34168},{\"end\":34185,\"start\":34177},{\"end\":34193,\"start\":34185},{\"end\":34205,\"start\":34193},{\"end\":34215,\"start\":34205},{\"end\":34225,\"start\":34215},{\"end\":34236,\"start\":34225},{\"end\":34956,\"start\":34948},{\"end\":34966,\"start\":34956},{\"end\":35208,\"start\":35200},{\"end\":35219,\"start\":35208},{\"end\":35231,\"start\":35219},{\"end\":35243,\"start\":35231},{\"end\":35454,\"start\":35444},{\"end\":35463,\"start\":35454},{\"end\":35479,\"start\":35463},{\"end\":35488,\"start\":35479}]", "bib_venue": "[{\"end\":28886,\"start\":28825},{\"end\":33140,\"start\":33053},{\"end\":34412,\"start\":34325},{\"end\":28461,\"start\":28396},{\"end\":28823,\"start\":28747},{\"end\":29223,\"start\":29174},{\"end\":29478,\"start\":29445},{\"end\":29780,\"start\":29703},{\"end\":30045,\"start\":29996},{\"end\":30409,\"start\":30352},{\"end\":30811,\"start\":30734},{\"end\":31199,\"start\":31179},{\"end\":31483,\"start\":31437},{\"end\":31865,\"start\":31810},{\"end\":32162,\"start\":32104},{\"end\":32360,\"start\":32318},{\"end\":32614,\"start\":32553},{\"end\":33032,\"start\":32964},{\"end\":33673,\"start\":33633},{\"end\":33950,\"start\":33894},{\"end\":34304,\"start\":34236},{\"end\":35010,\"start\":34966},{\"end\":35587,\"start\":35504}]"}}}, "year": 2023, "month": 12, "day": 17}