{"id": 53285437, "updated": "2023-11-11 02:44:14.179", "metadata": {"title": "Quantum-inspired low-rank stochastic regression with logarithmic dependence on the dimension", "authors": "[{\"first\":\"Andr\u00e1s\",\"last\":\"Gily\u00e9n\",\"middle\":[]},{\"first\":\"Seth\",\"last\":\"Lloyd\",\"middle\":[]},{\"first\":\"Ewin\",\"last\":\"Tang\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2018, "month": null, "day": null}, "abstract": "We construct an e\ufb03cient classical analogue of the quantum matrix inversion algorithm [HHL09] for low-rank matrices. Inspired by recent work of Tang [Tan18a], assuming length-square sampling access to input data, we implement the pseudoinverse of a low-rank matrix and sample from the solution to the problem Ax = b using fast sampling techniques. We implement the pseudo-inverse by \ufb01nding an approximate singular value decomposition of A via subsampling, then inverting the singular values. In principle, the approach can also be used to apply any desired \u201csmooth\u201d function to the singular values. Since many quantum algorithms can be expressed as a singular value transformation problem [GSLW18], our result suggests that more low-rank quantum algorithms can be e\ufb00ectively \u201cdequantised\u201d into classical length-square sampling algorithms.", "fields_of_study": "[\"Computer Science\",\"Physics\"]", "external_ids": {"arxiv": null, "mag": "2899791076", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-1811-04909", "doi": null}}, "content": {"source": {"pdf_hash": "1dfaf1b8bf6e4a91a4a66aa5030fb4f6d5fb79fd", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1811.04909v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "b5426f0cf3302a7269586c008688aa8f68870076", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/1dfaf1b8bf6e4a91a4a66aa5030fb4f6d5fb79fd.txt", "contents": "\nQuantum-inspired low-rank stochastic regression with logarithmic dependence on the dimension\nNovember 13, 2018\n\nAndr\u00e1s Gily\u00e9n \nSeth Lloyd \nEwin Tang \nQuantum-inspired low-rank stochastic regression with logarithmic dependence on the dimension\nNovember 13, 2018\nWe construct an efficient classical analogue of the quantum matrix inversion algorithm [HHL09] for low-rank matrices. Inspired by recent work of Tang [Tan18a], assuming lengthsquare sampling access to input data, we implement the pseudoinverse of a low-rank matrix and sample from the solution to the problem Ax = b using fast sampling techniques. We implement the pseudo-inverse by finding an approximate singular value decomposition of A via subsampling, then inverting the singular values. In principle, the approach can also be used to apply any desired \"smooth\" function to the singular values. Since many quantum algorithms can be expressed as a singular value transformation problem [GSLW18], our result suggests that more low-rank quantum algorithms can be effectively \"dequantised\" into classical length-square sampling algorithms.\n\nIntroduction\n\nQuantum computing provides potential exponential speed-ups over classical computers for a variety of linear algebraic tasks, including an operational version of matrix inversion [HHL09]. Recently, inspired by the quantum algorithm for recommendation systems [KP17], Tang showed how to generalize the well-known FKV algorithm [FKV04] to sample from the singular vectors of low-rank matrices [Tan18a] and to implement principal component component analysis [Tan18b]. Intriguingly, Tang's work suggests that many of the quantum algorithms for low-rank matrix manipulation [RSML18] can be extended to provide fast classical algorithms under suitable sampling assumptions, achieving logarithmic dependence on the dimension. In this work, we show that such exponential speed-ups are indeed possible in the case of low-rank matrix inversion. Our treatment is self-contained and improves some aspects of previous approaches [Tan18a], leading to smaller exponents in our runtime bounds.\n\nSuppose we want to solve Ax = b, where we are given A \u2208 R m\u00d7n and b \u2208 R m , and wish to recover x \u2208 R n . The equation might not have a solution, but we can always find an x minimizing Ax \u2212 b . Namely,\nx = A + b works, where A + is the pseudoinverse of A. If A = k =1 \u03c3 u ( ) v ( ) T\nis a singular value decomposition of A, such that \u03c3 > 0, then the pseudoinverse is simply A + = k =1 v ( ) u ( ) T /\u03c3 , and x = k =1 v ( ) u ( ) , b /\u03c3 . The problem of finding the pseudoinverse of a low-rank matrix occurs widely, such as in problems of data fitting, stochastic regression, and quadratic optimization with linear constraints. Applications of the classical stochastic regression algorithm presented here include a wide variety of data analysis problems. Consider, for example, the problem of finding the optimal investment portfolio amongst n stocks. Let r i be the m vectors of historical returns on the stocks (e.g., the returns on the i'th day or the i'th tick of the stock market), and let r = (1/m) i r i , so that A is the matrix with rows r T i . The covariance matrix is C = i r i r T i /m = A T A. Typically, the covariance matrix C is (approximately) low-rank, with each singular value corresponding to an underlying trend of correlated motion of investment returns. Classical portfolio management operates by finding the vector of investments w that maximizes the expected return w T r subject to a constraint on the variance w T Cw. As shown in [RL18], finding and sampling from the optimal portfolio and mapping out the optimal risk-return curve is a low-rank matrix inversion problem which can be solved on a quantum computer in time O(polylog(mn)), exponentially faster than conventional classical portfolio optimization methods. The results presented here show that our quantum-inspired classical algorithm can similarly allow one to map out the risk-return curve and sample from the optimal portfolio using O(polylog(mn)) classical time.\n\n\nThe algorithm\n\nWe use the following notation. For v \u2208 C d we denote the Euclidean norm by b . For a matrix A \u2208 C m\u00d7n we denote by A the operator norm, and by A F the Frobenius norm. We use notation A i. for the i-th row, A .j for the j-th column, and A \u2020 for the adjoint of A. We use \"bra-ket\" notation: for v \u2208 C d we denote the corresponding column vector by |v \u2208 C d\u00d71 , and we denote by v| \u2208 C 1\u00d7d its adjoint. Accordingly we denote the inner product v, w by v|w . We call the probability distribution |v\ni | 2 / v 2 over i \u2208 [d] the length-square distribution of v.\nIn this paper for simplicity we treat the case when the matrix A has rank k m, n, and does not have too small singular values. 1 For normalization purposes we assume that A \u2264 1 and A + \u2264 \u03ba. Our program is the following: we first show how to describe an approximate singular value decomposition k =1\u03c3 |\u0169 ( ) \u1e7d ( ) | using a succinct representation of the vectors. Then we show how to estimate the values \u0169 ( ) |b /\u03c3 via sampling, and how to sample from the corresponding linear combination of the vectors\u1e7d ( ) . The overall algorithm allows us to sample or query elements of an approximate solutionx \u2248 A + b to the equation Ax = b in time poly-logarithmic in the size of the matrix.\n\nThe idea of the approximate singular value decomposition of A using length-square sampling comes from [FKV04]. Consider the following: first use length-square sampling to sample some rows R. If we sample enough rows, then A T A \u2212 R T R is small as shown by Theorem 3. Then the singular values and right singular vectors of R are very close to the singular values and right singular vectors of A. Using the approximate right singular vectors we can recover approximate left singular vectors as well, e.g., by applying the matrix A to the right singular vector, as shown by Lemma 4. This is promising, but since R still can have very high-dimensional rows, computing the singular value decomposition of R can still be prohibitive. However, we can apply the trick once more! We can sample columns of R resulting in the matrix C. Again the singular values and left singular vectors of C are very close to the singular values and left singular vectors of R provided that RR T \u2212 CC T is small.\n\nTheorem 1 (Correctness). If A has rank at most 2 k, A \u2264 1, A + \u2264 \u03ba, and the projection of b to the column space of A has norm \u2126( b ), then Algorithm 1 solves Ax = b up to \u03b5-multiplicative accuracy, such that x \u2212 A + b \u2264 \u03b5 A + b with probability at least 1 \u2212 \u03b7.\n\nIn order to execute Algorithm 1 we use length-square sampling techniques, which have found great applications in randomized linear algebra [KV17], as well as recent quantum-inspired classical algorithms for recommendation systems [Tan18a]. For simplicity we assign unit cost to arithmetic operations such as addition or multiplication of real or complex numbers, assuming that all numbers are represented with a small number of bits. If A \u2208 C m\u00d7n is stored in an efficient tree-like data-structure as in [FKV04,KP17,Tan18a], then we can implement the sampling and query operations of A required by Algorithm 1 in complexity O(log(mn)) assuming that datastructure queries have unit cost. Under these assumption we get the following bound:\n\nTheorem 2 (Complexity). If we have O(1)-time 3 query access to b and O(1)-time length-square access to A, then under the conditions of Theorem 1 we can execute Algorithm 1 in complexity O \u03ba 16 k 6 A 6 F /\u03b5 6 , outputting an implicit description ofx suitable for query and sample access.\n\nNote that our complexity bound has smaller exponents than e.g. [Tan18a]. This partly comes from the fact that we only consider low-rank matrices, but we also get improvements by adapting and reanalysing the FKV algorithm [FKV04]. We only work out the constant factors for the number of rows and columns to be sampled, because these parameters dominate the complexity.\n\nFor comparison with the quantum analogue, note that under the assumption that the data structure for A is stored in quantum memory, an \u03b5-approximate quantum state |x / x can be prepared in complexity O(\u03ba A F polylog(1/\u03b5)), as shown in [CGJ18,GSLW18]. This directly enables length-square sampling, and its entries can be estimated with poly(\u03ba/\u03b5) overheads.\n\nAlgorithm 1 Low-rank stochastic regression via length-square sampling\nInput: A vector b \u2208 C m and a matrix A \u2208 C m\u00d7n s.t. A \u2264 1, rank(A) = k and A + \u2264 \u03ba. Goal 1: Query elements of a vectorx such that x \u2212 x \u2264 \u03b5 x for x = A + b. Goal 2: Sample from a distribution 2\u03b5-close in total-variation distance to |x j | 2 x 2 . 1: Init: Set r = 2 10 ln 8n \u03b7 \u03ba 4 k 2 A 2 F \u03b5 2 and c = 2 6 \u00b7 3 4 ln 8r \u03b7 \u03ba 8 k 2 A 2 F \u03b5 2 .\n2: Sample rows: Sample r row indices i 1 , i 2 , . . . , i r according to the row norm squares\nA i. 2 A 2 F .\nDefine R to be the matrix whose s-th row is\nA F \u221a r A is.\nA is. . 3: Sample columns: Sample s \u2208 [r] uniformly, then sample a column index j distributed as\n|R sj | 2 Rs. 2 = |A isj | 2 A is.\n2 . Sample a total number of c column indices j 1 , j 2 , . . . , j c this way. Define the matrix C whose t-th column is \nR F \u221a c R .j t R .j t = A F \u221a c R .j t R .j t= r s=1 R \u2020 is. w ( ) s \u03c3 . 6: Matrix elements: For each \u2208 [k] compute\u03bb such that \u03bb \u2212 \u1e7d ( ) |A \u2020 |b = O \u03b5\u03c3 2 b \u221a k . 7: Output: Row indices i 1 , i 2 , . . . , i r and w :=\u03bb \u03c3 3 w ( ) \u2208 C r such that w = O \u03ba 2 \u221a k b .\nQueries tox: Definex := R \u2020 w, where R is implicitly defined by the row indices. A query tox j can be computed by querying R s,j for all s \u2208 [r] and taking their linear combination. Sampling from |x j | 2 / x 2 : Rejection sample (Lemma 12), using T samples from the distribution |R s,j | 2 Rs,. 2 for some s \u2208 [r], and querying rT entries of R,\ns.t. E[T ] \u2264 w 2 A 2 F / x 2 .\nIn the above algorithm, we first convert left singular vectors of C (w ( ) ) to approximate right singular vectors of R (\u1e7d ( ) ), which also approximate right singular vectors of A. Then we \"convert\" these to left singular vectors of A in the form ( \u1e7d ( ) |A \u2020 /\u03c3 ). To clarify the formula for x, notice the following sequence of approximations:\nx \u2248 k =1 1 \u03c3 4 |R \u2020 w ( ) R \u2020 w ( ) |A \u2020 b \u2248 k =1 1 \u03c3 2 |\u1e7d ( ) \u1e7d ( ) |A \u2020 b \u2248 (R \u2020 R) + A \u2020 b \u2248 (A \u2020 A) + A \u2020 b = A + b\nThe conversion step from right to left singular vectors of A magnifies previous inaccuracies. For this reason, it is beneficial to sample a higher number of columns than rows, unlike in earlier works [FKV04,Tan18a].\n\n\nCorrectness of Algorithm 1\n\nThe goal is to output a description of an approximate solutionx. If x \u2212 x \u2264 \u03b5 x , then the length-square distributions of x andx are 2\u03b5-close as shown by Lemma 10. Thus for our purposes it suffices to find approximate right singular vectors\u1e7d ( ) and approximate singular values\u03c3 such that\nk =1 |\u1e7d ( ) \u1e7d ( ) | \u03c3 2 A \u2020 A \u2212 \u03a0 rows(A) \u2264 \u03b5 2 .\n(1)\nLet us define |x = k =1 |\u1e7d ( ) \u1e7d ( ) | \u03c3 2 A \u2020 |b = k =1 \u1e7d ( ) |A \u2020 |b \u03c3 2 |\u1e7d ( ) = k =1 \u03bb \u03c3 2 |\u1e7d ( ) ,(2)\nthen due to Equation (1) we have that\nx \u2212 x = k =1 |\u1e7d ( ) \u1e7d ( ) | \u03c3 2 A \u2020 |b \u2212 |x = k =1 |\u1e7d ( ) \u1e7d ( ) | \u03c3 2 A \u2020 A \u2212 \u03a0 rows(A) |x \u2264 \u03b5 2 x .\nRemember that we assumed that the projection of b to the column space of A has norm \u2126( b ).\n\nSince A \u2264 1 we also have that x = \u2126( b ). Therefore it suffices to findx such that\nx \u2212 x = O(\u03b5 b ) in order to ensure x \u2212 x \u2264 \u03b5 2\nx . If we compute approximate values\u03bb such that \u03bb \u2212\u03bb = O  with probability\n\n\nFinding approximate singular values and right singular vectors\np i = A i. 2 A 2 F\n, and upon picking index i set the random output Y = A i. \u221a p i . Notice that in Algorithm 1 both R and C can be characterized as length-square (row) sampled matrices (the latter holding because every row of R has the same norm).\n\nTheorem 3. Let A \u2208 C m\u00d7n be a matrix and let R \u2208 C s\u00d7n be the sample matrix obtained by length-squared sampling and scaling to have E[R \u2020 R] = A \u2020 A. (R consists of rows Y 1 , Y 2 , . . . , Y s , which are i.i.d. copies of Y / \u221a s, as defined above.) Then, for all \u03b5 \u2208 [0, A / A F ], 5 we have\nP R \u2020 R \u2212 A \u2020 A \u2265 \u03b5 A A F \u2264 2ne \u2212 \u03b5 2 s 4 .\nHence, for s \u2265 4 ln(2n/\u03b7)\n\u03b5 2\n, with probability at least (1 \u2212 \u03b7) we have\nR \u2020 R \u2212 A \u2020 A \u2264 \u03b5 A A F . 4\nIn [KV17] the theorem is stated for real matrices, but the proof works for complex matrices as well. 5 If \u03b5 \u2265 A / A F , then the zero matrix is a good enough approximation to AA \u2020 .\n\nIn the following lemma M denotes the operator norm, but the proof would also work for the Frobenius norm. Note that the following lemmas are independent of the dimensions of the matrices, which is the reason why we do not specify the dimensions. We use \u03b4 ij to denote the Kronecker delta function, which is defined to be 1 if i = j and 0 otherwise.\n\nLemma 4 (Converting approximate left and right singular vectors). Suppose that w ( ) is a system of orthonormal vectors spanning the column space of C such that\nk =1 |w ( ) w ( ) | = \u03a0 cols(C)\nand\nw (i) |CC \u2020 |w (j) = \u03b4 ij\u03c3 2 i .\nSuppose that rank(R) = rank(C) = k and\nRR \u2020 \u2212 CC \u2020 \u2264 \u03b3. Let\u1e7d ( ) := R \u2020 w ( ) \u03c3 , then | \u1e7d (i) |\u1e7d (j) \u2212 \u03b4 ij | \u2264 \u03b3 \u03c3 i\u03c3j , and \u1e7d (i) |R \u2020 R|\u1e7d (j) \u2212 \u03b4 ij\u03c3 2 i \u2264 \u03b3 2 R 2 + \u03b3 \u03c3 i\u03c3j .\nProof. Let V be the matrix whose -th column is the vector\u1e7d ( ) and let us define the Gram matrix G = V \u2020 V . We have that\nG ij = | \u1e7d (i) ,\u1e7d (j) \u2212 \u03b4 ij | = w (i) |RR \u2020 |w (j) \u03c3 i\u03c3j \u2212 \u03b4 ij \u2264 w (i) |CC \u2020 |w (j) \u03c3 i\u03c3j \u2212 \u03b4 ij + \u03b3 \u03c3 i\u03c3j = \u03b3 \u03c3 i\u03c3j .\n\nNow observe that\nRR \u2020 RR \u2020 \u2212 CC \u2020 CC \u2020 \u2264 RR \u2020 (RR \u2020 \u2212 CC \u2020 ) + (RR \u2020 \u2212 CC \u2020 )CC \u2020 \u2264 \u03b3 2 R 2 + \u03b3 . Let i, j \u2208 [k], then w (i) |CC \u2020 CC \u2020 |w (j) = w (i) |CC \u2020 k =1 |w ( ) w ( ) | CC \u2020 |w (j) = \u03b4 ij \u03c3 4 i .\nFinally we get that\n\u1e7d (i) |R \u2020 R|\u1e7d (j) \u2212 \u03b4 ij \u03c3 2 i = w (i) |RR \u2020 RR \u2020 |w (j) \u03c3 i\u03c3j \u2212 \u03b4 ij \u03c3 2 i \u2264 w (i) |CC \u2020 CC \u2020 |w (j) \u03c3 i\u03c3j \u2212 \u03b4 ij \u03c3 2 i + \u03b3 2 R 2 + \u03b3 \u03c3 i\u03c3j = \u03b3 2 R 2 + \u03b3 \u03c3 i\u03c3j .\nLemma 5. Let B be a matrix of rank at most k, and suppose that V has k columns that span the row and column spaces of B. Then\nB \u2264 (V \u2020 V ) \u22121 V \u2020 BV .\nProof. Let G := V \u2020 V be the Gram matrix of V and let\u1e7c := V G \u2212 1 2 . It is easy to see that\u1e7c is an isometry and its columns still span the the row and column spaces of B. Since\u1e7c is an isometry we get that\nB = \u1e7c \u2020 B\u1e7c = G \u2212 1 2 V \u2020 BV G \u2212 1 2 \u2264 G \u22121 V \u2020 BV = (V \u2020 V ) \u22121 V \u2020 BV .\nLemma 6 (Approximate left and right singular vectors). Suppose that\u1e7d (i) is a system of approximately orthonormal vectors spanning the row space of A such that\n| \u1e7d (i) |\u1e7d (j) \u2212 \u03b4 ij | \u2264 \u03b1 \u2264 1 4k ,(3)\nand\n\u1e7d (i) |R \u2020 R|\u1e7d (j) \u2212 \u03b4 ij\u03c3 2 i \u2264 \u03b2, where\u03c3 2 i \u2265 4 5\u03ba 2 . Suppose that rank(A) = rank(R) = k and A \u2020 A \u2212 R \u2020 R \u2264 \u03b8, then \u03a0 rows(A) \u2212 k =1 |\u1e7d ( ) \u1e7d ( ) | \u03c3 2 A \u2020 A \u2264 8k 3 (\u03b2\u03ba 2 + \u03b8\u03ba 2 + \u03b1). (4) Proof. Let B := k =1 |\u1e7d ( ) \u1e7d ( ) | \u03c3 2 A \u2020 A \u2212 \u03a0 rows(A)\n, we will apply Lemma 5. For this observe\n\u1e7d (i) |B|\u1e7d (j) = k =1 \u1e7d (i) |\u1e7d ( ) \u1e7d ( ) |A \u2020 A|\u1e7d (j) \u03c3 2 \u2212 \u1e7d (i) |\u1e7d (j) \u2264 k =1 \u1e7d (i) |\u1e7d ( ) \u1e7d ( ) |R \u2020 R|\u1e7d (j) \u03c3 2 \u2212 \u03b4 ij + k =1 | \u1e7d (i) |\u1e7d ( ) |\u03b8 \u1e7d ( ) \u1e7d (j) \u03c3 2 + \u03b1 \u2264 k =1 \u1e7d (i) |\u1e7d ( ) \u1e7d ( ) |R \u2020 R|\u1e7d (j) \u03c3 2 \u2212 \u03b4 ij + 2\u03b8\u03ba 2 + \u03b1 \u2264 k =j \u1e7d (i) |\u1e7d ( ) \u1e7d ( ) |R \u2020 R|\u1e7d (j) \u03c3 2 + \u1e7d (i) |\u1e7d (j) \u1e7d (j) |R \u2020 R|\u1e7d (j) \u03c3 2 j \u2212 \u03b4 ij + 2\u03b8\u03ba 2 + \u03b1 \u2264 k =j \u1e7d (i) |\u1e7d ( ) \u1e7d ( ) |R \u2020 R|\u1e7d (j) \u03c3 2 + \u03b1(1 + \u03b2/\u03c3 2 j ) + \u03b4 ij \u03b2/\u03c3 2 j + 2\u03b8\u03ba 2 + \u03b1 \u2264 (1 + k\u03b1)\u03b2 5 4 \u03ba 2 + 2\u03b8\u03ba 2 + 2\u03b1\n\u2264 2(\u03b2\u03ba 2 + \u03b8\u03ba 2 + \u03b1).\n\nLet e \u2208 C k denote the -th standard basis vector and let us define V := k =1 |\u1e7d ( ) e |. It follows that V \u2020 BV \u2264 2k(\u03b2\u03ba 2 + \u03b8\u03ba 2 + \u03b1). By (3) we have that V \u2020 V \u2212 I \u2264 k\u03b1 \u2264 1/4, and thus (V \u2020 V ) \u22121 \u2264 4/3. By Lemma 5 we get that B \u2264 8k(\u03b2\u03ba 2 + \u03b8\u03ba 2 + \u03b1)/3.\n\nIf \u03b3 \u2264 1 10\u03ba 2 and \u03b8 \u2264 1 10\u03ba 2 , we get\u03c3 2 min \u2265 4 5\u03ba 2 . Then by Lemma 4 we get that \u03b1 \u2264 5 4 \u03ba 2 \u03b3 and \u03b2 \u2264 3\u03ba 2 \u03b3. Substituting this into Equation (4) we get the upper bound \u03b3 8k\u03ba 4 + 10k\u03ba 2 /3 + \u03b8 8k 3\n\u03ba 2 \u2264 \u03b312k\u03ba 4 + \u03b8 8k 3 \u03ba 2 .(5)\nChoosing \u03b8 = 1 16 \u03b5 k\u03ba 2 and \u03b3 = 1 36 \u03b5 k\u03ba 4 , the above bound (5) becomes \u03b5/2. Therefore to succeed with probability at least 1 \u2212 \u03b7/2 it suffices to sample r = 2 10 ln(8n/\u03b7)\u03ba 4 k 2 A 2 F /\u03b5 2 row indices, and then subsequently c = 2 6 \u00b7 3 4 ln(8r/\u03b7)\u03ba 8 k 2 A 2 F /\u03b5 2 column indices as shown by Theorem 3.\n\n\nThe required precision for matrix element estimation\n\nRecall from Equation (2) that\nx = k =1 \u03bb \u03c3 2 \u1e7d ( ) ,\nandx is as above except we replace \u03bb with\u03bb . As we argued in the beginning of the section, for the correctness of Algorithm 1 it suffices to ensure x \u2212 x = O(\u03b5), assuming that b = 1.\nNow we show that if we have \u03bb \u2212\u03bb = O \u03b5\u03c3 2 b \u221a k\n, then the magnitude of perturbation can be bounded by O(\u03b5), and we also get that w = O \u03ba 2 \u221a k . Let e \u2208 C k denote the -th standard basis vector; we rewrite x \u2212 x as\nk =1 \u03bb \u2212\u03bb \u03c3 2 |\u1e7d ( ) = k =1 \u03bb \u2212\u03bb \u03c3 2 |\u1e7d ( ) e |e 2 = k =1 |\u1e7d ( ) e | k =1 \u03bb \u2212\u03bb \u03c3 2 |e 2 . Let us define V := k =1 |\u1e7d ( ) e |, and |z := k =1 \u03bb \u2212\u03bb \u03c3 2 |e , then we have that k =1 \u03bb \u2212\u03bb \u03c3 2 |\u1e7d ( ) = z|V \u2020 V |z \u2264 V \u2020 V z = O(\u03b5),\nwhere we used that V \u2020 V \u2264 1 + k\u03b1 \u2264 4 3 as we have shown in the proof of Lemma 6. Now we show that\nw = O \u03ba 2 \u221a k . Remember that\u1e7d ( ) = R \u2020 w ( ) \u03c3 , thusx = R \u2020 . k =1\u03bb \u03c3 3 w ( ) . Let w := k =1\u03bb \u03c3 3 w ( ) , then we get w = k =1 |\u03bb | 2 \u03c3 2 \u2264 k =1 |\u03bb | 2 \u03c3 6 + k =1 |\u03bb \u2212 \u03bb | 2 \u03c3 6 \u2264 O \u03ba 2 k =1 |\u03bb | 2 \u03c3 2 + O \u03ba 2 \u03b5 .\nFinally observe that\nk =1 |\u03bb | 2 \u03c3 2 = k =1 b|A|\u1e7d ( ) \u1e7d ( ) |A \u2020 |b \u03c3 2 \u2264 Tr k =1 A|\u1e7d ( ) \u1e7d ( ) |A \u2020 \u03c3 2 = Tr k =1 \u1e7d ( ) |A \u2020 A|\u1e7d ( ) \u03c3 2 \u2264 k =1 \u1e7d ( ) |R \u2020 R|\u1e7d ( ) \u03c3 2 + O k\u03ba 2 A \u2020 A \u2212 R \u2020 R \u2264 O k + k\u03b2\u03ba 2 + k\u03b8\u03ba 2 \u2264 O(k + \u03b5),\nwhere the last two inequalities follow from Lemma 6 and its follow-up discussion.\n\n\nComplexity of Algorithm 1\n\nThe complexity is dominated by two parts of the algorithm: finding the left singular vectors of an r by c matrix, and estimating some matrix elements of A. If we use naive matrix multiplication, then computing the singular value decomposition of CC \u2020 costs\nO r 2 c = O \u03ba 16 k 6 A 6 F \u03b5 6 .\nIn this section, we prove that this dominates the runtime of the algorithm. First, we use length-square sampling techniques similarly to Tang [Tan18a] to approximate the matrix elements \u03bb := \u1e7d ( ) |A \u2020 |b , which has complexity O \u03ba 8 k 4 A 4 F \u03b5 4 as we show in Section 4.2. Second, we show how to efficiently length-square sample fromx := k =1\u03bb \u03c3 2 \u1e7d ( ) using rejection sampling.\n\n\nLength-square sampling techniques\n\nDefinition 7 (Length-square distribution). For a non-zero vector v \u2208 C n we define the probability distribution q (v) on [n] such that q\n(v) i = |v i | 2 v 2 .\nNote that if v describes a normalized pure quantum state, the above distribution is exactly the distribution we get through measurement in the computational basis by the Born rule.\n\nDefinition 8 (Length-square access to a vector). We say that we have length-square access to the vector v \u2208 C n if we can request a sample from the distribution q (v) that takes cost S(v). We also assume that we can query the elements of v with cost Q(v), and that we can query the value of v with cost N (v). 6 We denote by L(v) := S(v) + Q(v) + N (v) the overall access cost.\n\nIf the matrix A is stored in an tree-like dynamic data structure [FKV04,KP17,Tan18a], then the complexity of length-square accessing A is O(log(mn)). During the complexity analysis we will assume such efficient access.\n\nDefinition 9 (Length-square access to a matrix). We say that we have (row) length-square access to the matrix A \u2208 C m\u00d7n if we have length-square access to the rows A i. of A for all i \u2208 [m] and length-square access to the vector of row norms a \u2208 R m , where a i := A i. . We denote by L(A) the complexity of the length-square access to A.\n\nNote that length-square access to A implies the ability to determine A F in N (A) time. We will use that the closeness of two vectors in Euclidean distance implies closeness of their corresponding distributions.\n\nLemma 10 (Bounding Total Variation distance by Euclidean distance [Tan18a, Lemma 6.1]).\nFor v, w \u2208 C n , q (v) , q (w) T V \u2264 2 v\u2212w max( v , w ) .\n\nEstimating the matrix element \u1e7d ( ) |A \u2020 |b\n\nWe use the inner product estimation method of Tang [Tan18a] for matrix element estimation.\n\nLemma 11 (Trace inner product estimation). Suppose that we have length-square access to A \u2208 C m\u00d7n and query access to the matrix B \u2208 C m\u00d7n in complexity Q(B). Then we can estimate Tr A \u2020 B to precision \u03be A F B F with probability at least 1\u2212\u03b7 in time\nO log(1/\u03b7) \u03be 2 (L(A) + Q(B)) .\nProof. This exactly follows from [Tan18a, Proposition 6.2], since Tr A \u2020 B is the inner product of order-two tensors. Let X be the random variable given by length-square sampling i from a, the vector of row norms of A, sampling j from A i. , and setting the random output to X =\nA 2 F A ij B ij . Then E[X] = m i=1 n j=1 |A ij | 2 A 2 F A 2 F A ij B ij = n j=1 m i=1 (A \u2020 ) ji B ij = Tr A \u2020 B E[|X| 2 ] = m i=1 n j=1 |A ij | 2 A 2 F A 4 F |A ij | 2 |B ij | 2 = m i=1 n j=1 A 2 F |B ij | 2 = A 2 F B 2 F .\nSo X is an unbiased estimator of Tr A \u2020 B . To compute the expectation, we use standard techniques: it suffices to estimate the the real and imaginary parts separately to additive precision \u03be A F B F / \u221a 2 with success probability at least 1 \u2212 \u03b7/2. For each, we compute the mean of 9 \u03be 2 copies of X, and take the median of 6 log(2/\u03b7) such empirical mean estimators, giving the desired result.\n\nWe can estimate \u03bb = \u1e7d ( ) |A \u2020 |b = Tr \u1e7d ( ) |A \u2020 |b = Tr A \u2020 |b \u1e7d ( ) | using this lemma. Observe that |b \u1e7d ( ) | F = \u1e7d ( ) b \u2264 (1 + \u03b5) b , and we can query the (i, j) matrix element of |b \u1e7d ( ) | by querying b i and\u1e7d ( ) j , which has O(1) and O(r) cost respectively. We desire to estimate \u03bb to additive precision O\n\u03b5\u03c3 2 b \u221a k\nwith success probability \u03b7 2k . By applying Lemma 11, we can compute such an estimate\u03bb with complexity\nO log(2k/\u03b7) k A 2 F \u03b5 2 \u03c3 4 r = O \u03ba 4 k A 2 F \u03b5 2 r = O \u03ba 8 k 3 A 4 F \u03b5 4 .\n\nSampling from the approximate solution\n\nOur goal is to sample from the length-square distribution ofx = R \u2020 w. In order to tackle this problem we invoke a result from [Tan18a] about length-square sampling a vector that is a linear-combination of length-square accessible vectors. For completeness we present its proof too, following the approach of Tang [Tan18a].\n\nLemma 12 (Length-square sample a linear combination of vectors [Tan18a, Proposition 6.4]).\n\nSuppose that we have length-square access to R \u2208 C r\u00d7n having normalized rows, and we are given w \u2208 C r (as a list of numbers in memory). Then we can implement queries to the vector y := R \u2020 w \u2208 C n with complexity Q(y) = O(rQ(R)) and we can length-square sample from q (y)\n\nwith complexity S(y) such that E[S(y)] = O r w 2 y 2 (S(R) + rQ(R)) .\n\nProof. The algorithm is simple, it proceeds by rejection sampling: one should first sample a row index i \u2208 [r] uniformly, then draw a column index j distributed as |R ij | 2 , then compute |y j | 2 = | w|R .j | 2 , R .j 2 and either output j with probability | w|R .j | 2 w 2 R .j 2 or sample (i, j) again. In each round the probability that we pick the column index j is r j=1 |R ij | 2 /r = R .j 2 /r, and the probability that we output j is |y j | 2 /(r w 2 ). The success probability in each round is y 2 /(r w 2 ), therefore the expected number of rounds is r w 2 / y 2 .\n\nSince all rows of R have norm A F / \u221a r, and x = \u2126(1) by Lemma 12 we can length-square sample fromx in expected complexity\nO r w 2 A 2 F /r x 2 r = O w 2 A 2 F x 2 r = O \u03ba 4 k A 2 F r = O \u03ba 8 k 3 A 4 F \u03b5 2 .\n\nDiscussion\n\nWe presented a proof-of-principle algorithm for approximately inverting low-rank matrices in runtime that is logarithmic in the dimensions. For simplicity we analysed the case when the matrix has low rank, however it should be possible to devise similar results when the matrix does not have low rank, but one only intends to invert the matrix on a \"well-conditioned\" subspace. We expect that the complexity can be further improved by using optimized algorithms for finding an approximate singular vale decomposition of the subsampled matrix C. Also, one might use another variant of the algorithm where the left singular vectors of A are approximated using some variant of the FKV algorithm [FKV04], instead of the right singular vectors. Another approach could be to use a different low-rank approximation method, which reconstructs an approximation of A by using a linear combination of rows and columns such as described in [KV17]. Although in this paper we focus on implementing the pseudo-inverse of a matrix by inverting the singular values, one could in principle apply any desired function to the singular values. It has recently been shown that many quantum algorithms can be expressed as a singular value transformation problem [GSLW18]. This supports Tang's suggestion [Tan18b] that many quantum algorithms can be effectively turned to randomized classical algorithms via length-square sampling techniques incurring only polynomial overheads. Our work gives evidence that this conversion can be done in general for low-rank problems, suggesting that exponential quantum speed-ups are tightly related to problems where high-rank matrices play a crucial role, like in Hamiltonian simulation or the Fourier transform. However, more work remains to be done on understanding the class of problems for which exponential quantum speed-up can be achieved.\n\n. 4 :\n4SVD: Query all elements of A corresponding to elements of C. Compute the left singular vectors and singular values of C and denote the left singular vectors by w (1) , . . . , w (k) and the corresponding singular values by\u03c3 1 , . . . ,\u03c3 k . 5: Approximate right singular vectors of A: Implicitly define\u1e7d ( ) :\n\n\nmagnitude of perturbation x \u2212 x can be bounded by O(\u03b5 b ), and w = O \u03ba 2 k b as we show in Section 3.2.\n\nFirst\nwe invoke some improved bounds on length-square sampling from [KV17, Theorem 4.4]. 4 Length-square row sampling of a matrix A \u2208 C m\u00d7n is as follows: pick a row index i \u2208 [m]\nThis assumption can be relaxed by inverting A only on the \"well-conditioned\" subspace, and dealing with small singular values similarly to the earlier works[FKV04,Tan18a], but we leave such details for future work.2 While running the algorithm we can actually detect if A has lower rank and adapt the algorithm accordingly.\nIn this paper by O(T ) we hide poly-logarithmic factors in T , the dimensions m, n and the failure probability \u03b7.\nWe assume for simplicity that S(v), Q(v), and N (v) \u2265 1.\nAcknowledgments A.G. thanks M\u00e1ri\u00f3 Szegedy for introduction to the problem and sharing insights, and Ronald de Wolf for helpful comments on the manuscript. S.L was supported by ARO and OSD under a Blue Sky Program.\nThe power of blockencoded matrix powers: improved regression techniques via faster Hamiltonian simulation. Shantanav Chakraborty, Andr\u00e1s Gily\u00e9n, Stacey Jeffery, arXiv:1804.01973Shantanav Chakraborty, Andr\u00e1s Gily\u00e9n, and Stacey Jeffery. The power of block- encoded matrix powers: improved regression techniques via faster Hamiltonian sim- ulation. arXiv: 1804.01973, 2018.\n\nFast Monte-Carlo algorithms for finding low-rank approximations. Alan Frieze, Ravi Kannan, Santosh Vempala, 10.1145/1039488.1039494Journal of the ACM. 516Alan Frieze, Ravi Kannan, and Santosh Vempala. Fast Monte-Carlo algorithms for finding low-rank approximations. Journal of the ACM, 51(6):1025-1041, 2004.\n\nQuantum singular value transformation and beyond: exponential improvements for quantum matrix arithmetics. Andr\u00e1s Gily\u00e9n, Yuan Su, Guang Hao Low, Nathan Wiebe, arXiv:1806.01838Andr\u00e1s Gily\u00e9n, Yuan Su, Guang Hao Low, and Nathan Wiebe. Quantum singular value transformation and beyond: exponential improvements for quantum matrix arithmetics. arXiv: 1806.01838, 2018.\n\nQuantum algorithm for linear systems of equations. Aram W Harrow, Avinatan Hassidim, Seth Lloyd, 10.1103/PhysRevLett.103.150502arXiv:0811.3171Physical Review Letters. 10315150502Aram W. Harrow, Avinatan Hassidim, and Seth Lloyd. Quantum algorithm for linear systems of equations. Physical Review Letters, 103(15):150502, 2009. arXiv: 0811.3171\n\nQuantum recommendation systems. Iordanis Kerenidis, Anupam Prakash, 10.4230/LIPIcs.ITCS.2017.49arXiv:1603.08675Proceedings of the 8th Innovations in Theoretical Computer Science Conference (ITCS). the 8th Innovations in Theoretical Computer Science Conference (ITCS)49Iordanis Kerenidis and Anupam Prakash. Quantum recommendation systems. In Proceedings of the 8th Innovations in Theoretical Computer Science Conference (ITCS), pages 49:1-49:21, 2017. arXiv: 1603.08675\n\nRandomized algorithms in numerical linear algebra. Ravindran Kannan, Santosh Vempala, 10.1017/S0962492917000058Acta Numerica. 26Ravindran Kannan and Santosh Vempala. Randomized algorithms in numerical lin- ear algebra. Acta Numerica, 26:95-135, 2017.\n\nQuantum computational finance: quantum algorithm for portfolio optimization. Patrick Rebentrost, Seth Lloyd, arXiv:1811.03975Patrick Rebentrost and Seth Lloyd. Quantum computational finance: quantum al- gorithm for portfolio optimization. arXiv: 1811.03975, 2018.\n\nQuantum singular-value decomposition of nonsparse low-rank matrices. Patrick Rebentrost, Adrian Steffens, Iman Marvian, Seth Lloyd, 10.1103/PhysRevA.97.012327arXiv:1607.05404Physical Review A. 9712327Patrick Rebentrost, Adrian Steffens, Iman Marvian, and Seth Lloyd. Quantum singular-value decomposition of nonsparse low-rank matrices. Physical Review A, 97:012327, 2018. arXiv: 1607.05404\n\nA quantum-inspired classical algorithm for recommendation systems. Ewin Tang, arXiv:1807.04271Electronic Colloquium on Computational Complexity. 128Ewin Tang. A quantum-inspired classical algorithm for recommendation sys- tems. Electronic Colloquium on Computational Complexity, page 128, 2018. arXiv: 1807.04271\n\nQuantum-inspired classical algorithms for principal component analysis and supervised clustering. Ewin Tang, arXiv:1811.00414Ewin Tang. Quantum-inspired classical algorithms for principal component analysis and supervised clustering. arXiv: 1811.00414, 2018.\n", "annotations": {"author": "[{\"end\":127,\"start\":113},{\"end\":139,\"start\":128},{\"end\":150,\"start\":140}]", "publisher": null, "author_last_name": "[{\"end\":126,\"start\":120},{\"end\":138,\"start\":133},{\"end\":149,\"start\":145}]", "author_first_name": "[{\"end\":119,\"start\":113},{\"end\":132,\"start\":128},{\"end\":144,\"start\":140}]", "author_affiliation": null, "title": "[{\"end\":93,\"start\":1},{\"end\":243,\"start\":151}]", "venue": null, "abstract": "[{\"end\":1102,\"start\":262}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1303,\"start\":1296},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1382,\"start\":1376},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1450,\"start\":1443},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":1516,\"start\":1508},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":1581,\"start\":1573},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":1695,\"start\":1687},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2042,\"start\":2034},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3560,\"start\":3554},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5417,\"start\":5410},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6704,\"start\":6698},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6797,\"start\":6789},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7070,\"start\":7063},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7075,\"start\":7070},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7082,\"start\":7075},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7657,\"start\":7649},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7814,\"start\":7807},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8197,\"start\":8190},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8204,\"start\":8197},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10458,\"start\":10451},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10465,\"start\":10458},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":12148,\"start\":12142},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":17742,\"start\":17734},{\"end\":18664,\"start\":18663},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":18804,\"start\":18797},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":18809,\"start\":18804},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":18816,\"start\":18809},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":19756,\"start\":19748},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":21654,\"start\":21646},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":21841,\"start\":21833},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":23780,\"start\":23773},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":24015,\"start\":24009},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":24328,\"start\":24320},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":24370,\"start\":24362},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":25709,\"start\":25702},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":25716,\"start\":25709}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":25258,\"start\":24941},{\"attributes\":{\"id\":\"fig_1\"},\"end\":25364,\"start\":25259},{\"attributes\":{\"id\":\"fig_2\"},\"end\":25545,\"start\":25365}]", "paragraph": "[{\"end\":2095,\"start\":1118},{\"end\":2298,\"start\":2097},{\"end\":4051,\"start\":2381},{\"end\":4562,\"start\":4069},{\"end\":5306,\"start\":4625},{\"end\":6295,\"start\":5308},{\"end\":6557,\"start\":6297},{\"end\":7296,\"start\":6559},{\"end\":7584,\"start\":7298},{\"end\":7953,\"start\":7586},{\"end\":8310,\"start\":7955},{\"end\":8381,\"start\":8312},{\"end\":8817,\"start\":8723},{\"end\":8876,\"start\":8833},{\"end\":8987,\"start\":8891},{\"end\":9144,\"start\":9023},{\"end\":9753,\"start\":9408},{\"end\":10130,\"start\":9785},{\"end\":10466,\"start\":10251},{\"end\":10785,\"start\":10497},{\"end\":10839,\"start\":10836},{\"end\":10984,\"start\":10947},{\"end\":11177,\"start\":11086},{\"end\":11261,\"start\":11179},{\"end\":11383,\"start\":11309},{\"end\":11697,\"start\":11468},{\"end\":11992,\"start\":11699},{\"end\":12062,\"start\":12037},{\"end\":12110,\"start\":12067},{\"end\":12320,\"start\":12139},{\"end\":12670,\"start\":12322},{\"end\":12832,\"start\":12672},{\"end\":12868,\"start\":12865},{\"end\":12940,\"start\":12902},{\"end\":13203,\"start\":13082},{\"end\":13549,\"start\":13530},{\"end\":13839,\"start\":13714},{\"end\":14070,\"start\":13865},{\"end\":14303,\"start\":14144},{\"end\":14347,\"start\":14344},{\"end\":14640,\"start\":14599},{\"end\":15115,\"start\":15094},{\"end\":15371,\"start\":15117},{\"end\":15576,\"start\":15373},{\"end\":15915,\"start\":15609},{\"end\":16001,\"start\":15972},{\"end\":16207,\"start\":16025},{\"end\":16423,\"start\":16256},{\"end\":16747,\"start\":16649},{\"end\":16986,\"start\":16966},{\"end\":17272,\"start\":17191},{\"end\":17558,\"start\":17302},{\"end\":17973,\"start\":17592},{\"end\":18147,\"start\":18011},{\"end\":18351,\"start\":18171},{\"end\":18730,\"start\":18353},{\"end\":18950,\"start\":18732},{\"end\":19290,\"start\":18952},{\"end\":19503,\"start\":19292},{\"end\":19592,\"start\":19505},{\"end\":19787,\"start\":19697},{\"end\":20038,\"start\":19789},{\"end\":20348,\"start\":20070},{\"end\":20968,\"start\":20575},{\"end\":21287,\"start\":20970},{\"end\":21401,\"start\":21299},{\"end\":21842,\"start\":21519},{\"end\":21934,\"start\":21844},{\"end\":22209,\"start\":21936},{\"end\":22280,\"start\":22211},{\"end\":22858,\"start\":22282},{\"end\":22982,\"start\":22860},{\"end\":24940,\"start\":23081}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":2380,\"start\":2299},{\"attributes\":{\"id\":\"formula_1\"},\"end\":4624,\"start\":4563},{\"attributes\":{\"id\":\"formula_2\"},\"end\":8722,\"start\":8382},{\"attributes\":{\"id\":\"formula_3\"},\"end\":8832,\"start\":8818},{\"attributes\":{\"id\":\"formula_4\"},\"end\":8890,\"start\":8877},{\"attributes\":{\"id\":\"formula_5\"},\"end\":9022,\"start\":8988},{\"attributes\":{\"id\":\"formula_6\"},\"end\":9190,\"start\":9145},{\"attributes\":{\"id\":\"formula_7\"},\"end\":9407,\"start\":9190},{\"attributes\":{\"id\":\"formula_8\"},\"end\":9784,\"start\":9754},{\"attributes\":{\"id\":\"formula_9\"},\"end\":10250,\"start\":10131},{\"attributes\":{\"id\":\"formula_10\"},\"end\":10835,\"start\":10786},{\"attributes\":{\"id\":\"formula_11\"},\"end\":10946,\"start\":10840},{\"attributes\":{\"id\":\"formula_12\"},\"end\":11085,\"start\":10985},{\"attributes\":{\"id\":\"formula_13\"},\"end\":11308,\"start\":11262},{\"attributes\":{\"id\":\"formula_14\"},\"end\":11467,\"start\":11449},{\"attributes\":{\"id\":\"formula_15\"},\"end\":12036,\"start\":11993},{\"attributes\":{\"id\":\"formula_16\"},\"end\":12066,\"start\":12063},{\"attributes\":{\"id\":\"formula_17\"},\"end\":12138,\"start\":12111},{\"attributes\":{\"id\":\"formula_18\"},\"end\":12864,\"start\":12833},{\"attributes\":{\"id\":\"formula_19\"},\"end\":12901,\"start\":12869},{\"attributes\":{\"id\":\"formula_20\"},\"end\":13081,\"start\":12941},{\"attributes\":{\"id\":\"formula_21\"},\"end\":13324,\"start\":13204},{\"attributes\":{\"id\":\"formula_22\"},\"end\":13529,\"start\":13343},{\"attributes\":{\"id\":\"formula_23\"},\"end\":13713,\"start\":13550},{\"attributes\":{\"id\":\"formula_24\"},\"end\":13864,\"start\":13840},{\"attributes\":{\"id\":\"formula_25\"},\"end\":14143,\"start\":14071},{\"attributes\":{\"id\":\"formula_26\"},\"end\":14343,\"start\":14304},{\"attributes\":{\"id\":\"formula_27\"},\"end\":14598,\"start\":14348},{\"attributes\":{\"id\":\"formula_28\"},\"end\":15093,\"start\":14641},{\"attributes\":{\"id\":\"formula_29\"},\"end\":15608,\"start\":15577},{\"attributes\":{\"id\":\"formula_30\"},\"end\":16024,\"start\":16002},{\"attributes\":{\"id\":\"formula_31\"},\"end\":16255,\"start\":16208},{\"attributes\":{\"id\":\"formula_32\"},\"end\":16648,\"start\":16424},{\"attributes\":{\"id\":\"formula_33\"},\"end\":16965,\"start\":16748},{\"attributes\":{\"id\":\"formula_34\"},\"end\":17190,\"start\":16987},{\"attributes\":{\"id\":\"formula_35\"},\"end\":17591,\"start\":17559},{\"attributes\":{\"id\":\"formula_36\"},\"end\":18170,\"start\":18148},{\"attributes\":{\"id\":\"formula_37\"},\"end\":19650,\"start\":19593},{\"attributes\":{\"id\":\"formula_38\"},\"end\":20069,\"start\":20039},{\"attributes\":{\"id\":\"formula_39\"},\"end\":20574,\"start\":20349},{\"attributes\":{\"id\":\"formula_40\"},\"end\":21298,\"start\":21288},{\"attributes\":{\"id\":\"formula_41\"},\"end\":21477,\"start\":21402},{\"attributes\":{\"id\":\"formula_42\"},\"end\":23067,\"start\":22983}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1116,\"start\":1104},{\"attributes\":{\"n\":\"2\"},\"end\":4067,\"start\":4054},{\"attributes\":{\"n\":\"3\"},\"end\":10495,\"start\":10469},{\"attributes\":{\"n\":\"3.1\"},\"end\":11448,\"start\":11386},{\"end\":13342,\"start\":13326},{\"attributes\":{\"n\":\"3.2\"},\"end\":15970,\"start\":15918},{\"attributes\":{\"n\":\"4\"},\"end\":17300,\"start\":17275},{\"attributes\":{\"n\":\"4.1\"},\"end\":18009,\"start\":17976},{\"attributes\":{\"n\":\"4.2\"},\"end\":19695,\"start\":19652},{\"attributes\":{\"n\":\"4.3\"},\"end\":21517,\"start\":21479},{\"attributes\":{\"n\":\"5\"},\"end\":23079,\"start\":23069},{\"end\":24947,\"start\":24942},{\"end\":25371,\"start\":25366}]", "table": null, "figure_caption": "[{\"end\":25258,\"start\":24949},{\"end\":25364,\"start\":25261},{\"end\":25545,\"start\":25372}]", "figure_ref": null, "bib_author_first_name": "[{\"end\":26371,\"start\":26362},{\"end\":26391,\"start\":26385},{\"end\":26406,\"start\":26400},{\"end\":26696,\"start\":26692},{\"end\":26709,\"start\":26705},{\"end\":26725,\"start\":26718},{\"end\":27050,\"start\":27044},{\"end\":27063,\"start\":27059},{\"end\":27073,\"start\":27068},{\"end\":27089,\"start\":27083},{\"end\":27358,\"start\":27354},{\"end\":27360,\"start\":27359},{\"end\":27377,\"start\":27369},{\"end\":27392,\"start\":27388},{\"end\":27688,\"start\":27680},{\"end\":27706,\"start\":27700},{\"end\":28179,\"start\":28170},{\"end\":28195,\"start\":28188},{\"end\":28455,\"start\":28448},{\"end\":28472,\"start\":28468},{\"end\":28712,\"start\":28705},{\"end\":28731,\"start\":28725},{\"end\":28746,\"start\":28742},{\"end\":28760,\"start\":28756},{\"end\":29098,\"start\":29094},{\"end\":29443,\"start\":29439}]", "bib_author_last_name": "[{\"end\":26383,\"start\":26372},{\"end\":26398,\"start\":26392},{\"end\":26414,\"start\":26407},{\"end\":26703,\"start\":26697},{\"end\":26716,\"start\":26710},{\"end\":26733,\"start\":26726},{\"end\":27057,\"start\":27051},{\"end\":27066,\"start\":27064},{\"end\":27081,\"start\":27074},{\"end\":27095,\"start\":27090},{\"end\":27367,\"start\":27361},{\"end\":27386,\"start\":27378},{\"end\":27398,\"start\":27393},{\"end\":27698,\"start\":27689},{\"end\":27714,\"start\":27707},{\"end\":28186,\"start\":28180},{\"end\":28203,\"start\":28196},{\"end\":28466,\"start\":28456},{\"end\":28478,\"start\":28473},{\"end\":28723,\"start\":28713},{\"end\":28740,\"start\":28732},{\"end\":28754,\"start\":28747},{\"end\":28766,\"start\":28761},{\"end\":29103,\"start\":29099},{\"end\":29448,\"start\":29444}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1804.01973\",\"id\":\"b0\"},\"end\":26625,\"start\":26255},{\"attributes\":{\"doi\":\"10.1145/1039488.1039494\",\"id\":\"b1\",\"matched_paper_id\":2483891},\"end\":26935,\"start\":26627},{\"attributes\":{\"doi\":\"arXiv:1806.01838\",\"id\":\"b2\"},\"end\":27301,\"start\":26937},{\"attributes\":{\"doi\":\"10.1103/PhysRevLett.103.150502\",\"id\":\"b3\",\"matched_paper_id\":5187993},\"end\":27646,\"start\":27303},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":579463},\"end\":28117,\"start\":27648},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":26830284},\"end\":28369,\"start\":28119},{\"attributes\":{\"id\":\"b6\"},\"end\":28634,\"start\":28371},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":54931888},\"end\":29025,\"start\":28636},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":44036160},\"end\":29339,\"start\":29027},{\"attributes\":{\"id\":\"b9\"},\"end\":29599,\"start\":29341}]", "bib_title": "[{\"end\":26690,\"start\":26627},{\"end\":27352,\"start\":27303},{\"end\":27678,\"start\":27648},{\"end\":28168,\"start\":28119},{\"end\":28703,\"start\":28636},{\"end\":29092,\"start\":29027}]", "bib_author": "[{\"end\":26385,\"start\":26362},{\"end\":26400,\"start\":26385},{\"end\":26416,\"start\":26400},{\"end\":26705,\"start\":26692},{\"end\":26718,\"start\":26705},{\"end\":26735,\"start\":26718},{\"end\":27059,\"start\":27044},{\"end\":27068,\"start\":27059},{\"end\":27083,\"start\":27068},{\"end\":27097,\"start\":27083},{\"end\":27369,\"start\":27354},{\"end\":27388,\"start\":27369},{\"end\":27400,\"start\":27388},{\"end\":27700,\"start\":27680},{\"end\":27716,\"start\":27700},{\"end\":28188,\"start\":28170},{\"end\":28205,\"start\":28188},{\"end\":28468,\"start\":28448},{\"end\":28480,\"start\":28468},{\"end\":28725,\"start\":28705},{\"end\":28742,\"start\":28725},{\"end\":28756,\"start\":28742},{\"end\":28768,\"start\":28756},{\"end\":29105,\"start\":29094},{\"end\":29450,\"start\":29439}]", "bib_venue": "[{\"end\":27914,\"start\":27845},{\"end\":26360,\"start\":26255},{\"end\":26776,\"start\":26758},{\"end\":27042,\"start\":26937},{\"end\":27468,\"start\":27445},{\"end\":27843,\"start\":27759},{\"end\":28243,\"start\":28230},{\"end\":28446,\"start\":28371},{\"end\":28827,\"start\":28810},{\"end\":29170,\"start\":29121},{\"end\":29437,\"start\":29341}]"}}}, "year": 2023, "month": 12, "day": 17}