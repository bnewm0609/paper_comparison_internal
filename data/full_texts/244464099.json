{"id": 244464099, "updated": "2022-02-24 04:32:30.104", "metadata": {"title": "Towards a Language Model for Temporal Commonsense Reasoning", "authors": "[{\"first\":\"Mayuko\",\"last\":\"Kimura\",\"middle\":[]},{\"first\":\"Lis\",\"last\":\"Kanashiro Pereira\",\"middle\":[]},{\"first\":\"Ichiro\",\"last\":\"Kobayashi\",\"middle\":[]}]", "venue": "RANLP", "journal": "Proceedings of the Student Research Workshop Associated with RANLP 2021", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Temporal commonsense reasoning is a challenging task as it requires temporal knowledge usually not explicit in text. In this work, we propose an ensemble model for temporal commonsense reasoning. Our model relies on pre-trained contextual representations from transformer-based language models (i.e., BERT), and on a variety of training methods for enhancing model generalization: 1) multi-step fine-tuning using carefully selected auxiliary tasks and datasets, and 2) a specifically designed temporal masked language model task aimed to capture temporal commonsense knowledge. Our model greatly outperforms the standard fine-tuning approach and strong baselines on the MC-TACO dataset.", "fields_of_study": null, "external_ids": {"arxiv": null, "mag": null, "acl": "2021.ranlp-srw.12", "pubmed": null, "pubmedcentral": null, "dblp": null, "doi": "10.26615/issn.2603-2821.2021_012"}}, "content": {"source": {"pdf_hash": "8b383bd2eb054787d84636dab7b6ecf4a318c6c7", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclanthology.org/2021.ranlp-srw.12.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "dc1fb5a2a59c4021dc9f77c3e63c9491f8bc0604", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/8b383bd2eb054787d84636dab7b6ecf4a318c6c7.txt", "contents": "\nTowards a Language Model for Temporal Commonsense Reasoning\nSep 1-3, 2021\n\nMayuko Kimura \nOchanomizu University\nJapan 13 {g1720512\n\nKanashiro Lis 2kanashiro.pereira@ocha.ac.jp \nOchanomizu University\nJapan 13 {g1720512\n\nPereira \nOchanomizu University\nJapan 13 {g1720512\n\nIchiro Kobayashi \nOchanomizu University\nJapan 13 {g1720512\n\nTowards a Language Model for Temporal Commonsense Reasoning\n\nProceedings of the Student Research Workshop associated with RANLP-2021\nthe Student Research Workshop associated with RANLP-2021Sep 1-3, 202110.26615/issn.2603-2821.2021_01278\nTemporal commonsense reasoning is a challenging task as it requires temporal knowledge usually not explicitly stated in text. In this work, we propose an ensemble model for temporal commonsense reasoning. Our model relies on pre-trained contextual representations from transformer-based language models (i.e., BERT), and on a variety of training methods for enhancing model generalization: 1) multistep fine-tuning using carefully selected auxiliary tasks and datasets, and 2) a specifically designed temporal task-adaptive pre-trainig task aimed to capture temporal commonsense knowledge. Our model greatly outperforms the standard fine-tuning approach and strong baselines on the MC-TACO dataset.\n\nIntroduction\n\nAlthough recent pre-trained language models such as BERT (Devlin et al., 2019) have achieved great success in a wide range of natural language processing (NLP) tasks, these models may still perform poorly on temporal reasoning scenarios. Ribeiro et al. (2020) has shown that such models often fail to make even simple temporal distinctions, for example, to distinguish the words before and after, resulting in degraded performance. An especially challenging task is temporal commonsense reasoning. For instance, given two events \"going on a vacation\" and \"going for a walk\", while most humans would know that a vacation is typically longer and occurs less often than a walk, computers have difficulty understanding and reasoning about temporal commonsense (Zhou et al., 2019).\n\nIn this paper, we focus on developing a model for temporal commonsense reasoning. Following best practices from recent work on enhancing model generalization, we propose a model that enriches pre-trained contextual representations with temporal knowledge and general commonsense knowledge by leveraging carefully selected auxiliary datasets in a multi-step fine-tuning setting. Moreover, we specifically designed a temporal taskadaptive pre-training task aimed to capture temporal commonsense knowledge by masking temporal indicators in text.\n\nWe evaluate our model on the challenging Multiple Choice Temporal Common-sense (MC-TACO) dataset (Zhou et al., 2019). In our experiments, our model substantially outperforms the standard finetuning approach, as well as other strong baselines.\n\n\nTemporal Commonsense Reasoning Task\n\nThis task entirely focuses on a specific reasoning capability: temporal commonsense. Zhou et al. (2019) used crowdsourcing to create the Multiple Choice Temporal Common-sense (MC-TACO) dataset, which collects the temporal knowledge of five temporal properties: (1) duration (how long an event takes), (2) temporal ordering (typical order of events), (3) typical time (when an event occurs), (4) frequency (how often an event occurs), and (5) stationarity (whether a state is maintained for a very long time or indefinitely). It contains 13k tuples, each consisting of a sentence, a question, and a candidate answer, that should be judged as plausible or not. The sentences are taken from different sources such as news, Wikipedia and textbooks. An example from the dataset is below. The correct answer is in bold.\n\nParagraph: Growing up on a farm near St. Paul, L. Mark Bailey didn't dream of becoming a judge. Question: How many years did it take for Mark to become a judge? a) 63 years b) 7 weeks c) 7 years d) 7 seconds e) 7 hours Reasoning Type: Duration We use the MC-TACO dataset for evaluating the performance of our model.\n\n\nModel\n\nOur work uses BERT (Devlin et al., 2019) as the text encoder. It has obtained high performance on several natural language understanding (NLU) benchmarks and it is relatively simple to adapt its architecture to downstream tasks. We focus on exploring different training techniques using BERT (Devlin et al., 2019), given its superior performance on a wide range of NLP tasks. The text encoder and the training methods used in our model are detailed below.\n\n\nText Encoder\n\nBERT (Devlin et al., 2019): We use the BERT BASE model and the BERT LARGE model released by the authors. The BERT BASE model consists of 12 transformer layers, 12 self-attention heads per layer, and a hidden size of 768. The BERT LARGE model consists of 24 transformer layers, 16 self-attention heads per layer, and a hidden size of 1024.\n\n\nTraining Methods\n\nMulti-Step Fine-Tuning: Multi-step fine-tuning works by performing a second stage of pre-training with data-rich related supervised tasks. It has been shown to improve model robustness and performance, especially for data-constrained scenarios (Phang et al., 2018;Camburu et al., 2019). We first fine-tune BERT on carefully selected auxiliary tasks and datasets. This model's parameters are further refined by fine-tuning on the MC-TACO dataset. The auxiliary tasks and datasets we use are detailed below:\n\nEvent Duration Prediction Task: This task involves predicting the duration of an event in a span of text. We use TimeML (Saur\u00ed et al., 2006;Pan et al., 2006), a dataset with event duration annotated as lower and upper bounds. The task is to decide whether a given event has a duration longer or shorter than a day. An example of a sentence with an event (in bold) that has a duration shorter than a day is shown below:\n\nIn Singapore, stocks hit a five year low.\n\nEvent Ordering Prediction Task: This task involves predicting the temporal relationship between a pair of input events in a span of text. In our work, we use the MATRES dataset (Ning et al., 2018).\n\nIt originally contains 13,577 pairs of events annotated with a temporal relation (BEFORE, AFTER, EQUAL, VAGUE). The temporal annotations are performed on 256 English documents (and 20 more for evaluation) from the TimeBank (Pustejovsky et al., 2003), AQUAINT (Graff, 2002) and Platinum (UzZaman et al., 2013) datasets. An example of a sentence with two events (in bold) that hold the BEFORE relation:\n\nAt one point , when it (e1:became) clear controllers could not contact the plane, someone (e2:said) a prayer.\n\nCommonsense Reasoning Task: We propose to enrich the temporal commonsense reasoning task training by leveraging data from general commonsense knowledge task. Since the commonsense reasoning task commonly also involves reasoning about temporal events, e.g. what event(s) might happen before or after the current event, we hypothesize that temporal reasoning might benefit from it. In our experiments, we use the CosmosQA (Huang et al., 2019) and the SWAG (Zellers et al., 2018) datasets. An example from the CosmosQA dataset is below. The task is to choose the correct answer among four options. The correct answer is in bold.\n\nParagraph: Did some errands today. My prime objectives were to get textbooks, find computer lab, find career services, get some groceries, turn in payment plan application, and find out when KEES money kicks in. I think it acts as a refund at the end of the semester at Murray, but I would be quite happy if it would work now.\n\nQuestion: What happens after I get the refund?\n\nOption 1: I can pay my bills.\n\nOption 2: I can relax. Option 3: I can sleep. Option 4: None of the above choices.\n\nAn example from the SWAG dataset is below. The task is to choose the correct ending among four options. The correct answer is in bold.\n\nQuestion: On stage, a woman takes a seat at the piano. She\n\nOption 1: sits on a bench as her sister plays with the doll.\n\nOption 2: smiles with someone as the music plays.\n\nOption 3: is in the crowd, watching the dancers.\n\nOption 4: nervously sets her fingers on the keys. We also experimented with task-adaptive pretraining on the MC-TACO dataset followed by fine-tuning on MC-TACO. The task-adaptive pretraining method is explained below.\n\nTask-Adaptive Pre-Training (TAPT): Although BERT achieves good performance on only finetuning it on the target task, there might be a distributional mismatch between the pre-trained model and the target dataset. To alleviate this issue, performing continual pre-training using the target dataset can be useful to adapt the pre-trained model to the target task (Gururangan et al., 2020). In this setting, we perform continual pre-training on BERT using the MC-TACO dataset. More specifically, we conduct the masked language modeling and the next sentence prediction tasks on BERT using the MC-TACO dataset. The masked language modeling task randomly replaces a subset of tokens by a special token (e.g., [MASK]), and asks the model to predict them. The next sentence prediction task is a binary classification task that for a given sentence pair determines whether one follows the other in the original text (Liu et al., 2020). In addition, we also experimented with masking only the tokens that have a high TF-IDF score.\n\nTemporal Task-Adaptive Pre-Training (Temporal TAPT): In this setting, instead of randomly mask words in the masked language modeling task, we mask time-related words. Those words include numbers, adverbs, adjectives, prepositions (before/after, every, often, etc.), and units of time (hours, years, etc.).\n\n\nEnsemble Model\n\nEnsemble of deep learning models has proven effective in improving test accuracy (Allen-Zhu and Li, 2020). We built different ensemble models by taking a majority vote of the outputs of a few independently trained models. Each single model was trained on standard fine-tuning, multi-step fine-tuning, task-adaptive pre-Training, or temporal task-adaptive pre-Training using BERT.\n\n\nExperiments\n\n\nDatasets\n\nIn this paper, we use MC-TACO as the training and evaluation dataset. In addition, we use the TimeML, MATRES, CosmosQA, and SWAG datasets as auxiliary datasets in the multi-step fine-tuning setting, as detailed in Section 3.2. The summary of the datasets is shown in Table 1 \n\n\nImplementation Details\n\nFor the multi-step fine-tuning experiments, the maximum sequence length, batch size, number of epochs, and the learning rate settings are shown in Table 2  The maximum sequence length, batch size, number of epochs, and the learning rate settings for the TAPT and Temporal TAPT experiments are set to 128, 32, 3, and 3e-5, respectively.\n\nWe use the exact match (EM) and F1-score as the evaluation metrics. EM measures how many questions a system correctly labeled all candidate answers, while F1-score measures the average overlap between one's predictions and the ground truth.\n\n\nResults\n\n\nMulti-\n\nStep Fine-Tuning: The results of the multi-step fine-tuning experiments are shown in Table 3. We used BERT BASE as the text encoder.  The MC-TACO model denotes the model that uses standard single-stage fine-tuning using MC-TACO, and the TimeML\u2192MC-TACO, TimeML + MATRES, the CosmosQA\u2192MC-TACO, and the SWAG\u2192MC-TACO models denote the models that use multi-step fine-tuning using other datasets as the first stage fine-tuning and MC-TACO as the second-stage fine-tuning. The TimeML + MA-TRES \u2192MC-TACO model denotes the model that combined the TimeML and MATRES datasets for the first stage of fine-tuning. We can observe that multi-step fine-tuning improved the overall accuracy, although there were some differences depending on the dataset used. The best results were obtained when we fine-tune on SWAG followed by MC-TACO (SWAG\u2192MC-TACO model). This indicates that enriching training with general commonsense knowledge is beneficial.\n\nWe also conducted experiments using BERT LARGE , where we can observe similar tendency in the results compared to BERT BASE . The results are shown in Table 4.  Task-Adaptive Pre-Training (TAPT): Table 5 shows the Task-Adaptive Pre-Training (TAPT) results where we randomly mask a subset of tokens. In order to check if we could further reduce the mismatch between the pre-trained model and the target task dataset, we also experimented with masking rates higher than BERT's default masking rate of 15%. However, the best accuracy was obtained with the 15% masking probability.  We also experimented with masking the tokens that have a high TF-IDF score. For each sentence, the candidate words for masking are the top-half words with the highest TF-IDF score. We also remove the stopwords when computing the TF-IDF. We experimented with two stopwords lists: nltk stopwords list 1 and sklearn stopwords list 2 . The results are shown in Table 6. Overall, using the nltk' stopwords achieved the best results.  Since the accuracy differs between using nltk's stopwords and sklearn's stopwords, we looked at the contents of each stopword and found that the stopwords from sklearn contained words related to time (i.e. numbers, prepositions such as before and after, adverbs, etc.) that ended up being removed, and not being masking candidate words. Therefore, we conducted a similar experiment in which we manually removed the time-related words from the sklearn stopwords. The results of the experiment are shown in Table 7.\n\nAs we can observe, the accuracy is higher compared to when using the original sklearn's stop-  words, indicating that it is not optimal to exclude words related to time from the calculation of TF-IDF. The accuracy of the TF-IDF experiment is about the same as that of randomly selecting words to be masked. Next, we set a threshold value and try to mask words where TF-IDF exceeds this value. We set different threshold values based on the percentage of the total number of words that will be masked, and we treat them as a hyperparameter. For the stopwords, we use the nltk's stopwords and the sklearn's stopwords excluding the words related to time. The results are shown in Table 8.  We found that if we masked all the words where TF-IDF exceeded the threshold, the accuracy decreases. Therefore, we randomly select tokens to mask from the words that exceed the threshold value. The results for this setting are shown in Table 9. In some cases, the accuracy was improved over the default case where the tokens are randomly masked. On the other hand, it is difficult to find regularities in the threshold setting and the percentage of masking, thus masking focusing on TF-IDF may not be effective.\n\nTemporal Task-Adaptive Pre-Training (Temporal TAPT): Here, time-related words (numbers,  before/after, every, hour, etc.) have a higher masking probability than the other words. The results are shown in Table 10. Different form the TAPT experimentes, here, masking rates higher than BERT's default masking rate of 15% improves the performance, indicating that masking temporal indicators with a higher masking rate further helps the model to acquire temporal knowledge. Moreover, we found that if we masked all the time-related words (100% masking probability ), the accuracy would decrease, but if we left a few words unmasked, the accuracy improves.\n\nEnsemble Model: We built different ensemble models by taking a majority vote of the outputs of a few independently trained models. Each single model was trained on standard fine-tuning, multistep fine-tuning, Task-Adaptive Pre-Training, or Temporal Task-Adaptive Pre-Training using BERT.\n\nThe results are shown in Table 11. The experimental results show that ensembling improves accuracy. In particular, pattern 4, which uses three models: multi-step fine-tuning with Cos-mosQA, multi-step fine-tuning with SWAG, and    We also compared our best ensemble model with TACOLM, proposed by Zhou et al. (2020). TACOLM is a BERT model pre-trained on explicit and implicit mentions of temporal commonsense, extracted from a large corpus using pattern rules. The results are shown in Table 13. As we can observe, the accuracy of all the five temporal properties was improved by our model.  \n\n\nDiscussion\n\nIn our experiments, we could observe that multistep fine-tuning outperforms standard single-stage fine-tuning. Also, fine-tuning BERT in the first stage using Temporal TAPT followed by finetuning on MC-TACO obtained the best performance among all single models. This indicates that a careful choice of the words to be masked has an impact on the performance. On the other hand, when all the words related to time were masked, the accuracy deteriorated. We hypothesize this is because if all the words were masked, the information about time would disappear from the context, and inferences about temporal common sense would be difficult.\n\nIn the TAPT experiments with TF-IDF, we found it difficult to find a regularity regarding the threshold and the ratio of masking, and it is hard to claim that masking based on TF-IDF is effective. In this study, we focus on the temporal commonsense task, and since the data contains more words related to time than other tasks, the value of IDF becomes smaller, and it may be said that TF-IDF might not be optimal.\n\n\nConclusion\n\nIn this paper, we proposed a model for temporal commonsense reasoning. We specifically designed a temporal masked language model task aimed to capture temporal commonsense knowledge by masking temporal indicators in text and used it in a multi-step fine-tuning setting. Moreover, we found out that an ensemble of this model with other models achieves the best results, outperforming other state-of-the-art models. In order to improve our model, we plan to conduct attention and saliency analysis.\n\n\n.train \nval \ntest \nMC-TACO \n-\n3,783 9,442 \nTimeML 1,248 \n-\n1,003 \nMATRES 12,716 \n-\n838 \nCosmosQA 25,588 3,000 7,000 \nSWAG 73,546 20,006 20,005 \n\nTable 1: Summary of the datasets used in our experi-\nments. \n\n\n\nTable 2 :\n2Hyperparameter settings for the multi-step fine-tuning experiments. The parameters with best performance are shown in bold.\n\nTable 3 :\n3Test results on multi-step fine-tuning using \nBERT BASE . The cross-validation results are shown in \nparenthesis. \n\n\n\nTable 4 :\n4Test results on multi-step fine-tuning using BERT LARGE . The cross-validation results are shown in parenthesis.\n\nTable 5 :\n5Task-Adaptive Pre-Training (TAPT) results when masking words randomly. The cross-validation results are shown in parenthesis.\n\nTable 6 :\n6Task-Adaptive Pre-Training (TAPT) results when masking words with a high TF-IDF score. The cross-validation results are shown in parenthesis.\n\nTable 7 :\n7Task-Adaptive Pre-Training (TAPT) results \nwhen masking words with a high TF-IDF score. The \ncross-validation results are shown in parenthesis. Here, \nwe use the sklearn's stopwords without the time-related \nwords. \n\n\n\nTable 8 :\n8Task-Adaptive Pre-Training (TAPT) results \nwhen masking words with a high TF-IDF score. The \ncross-validation results are shown in parenthesis. Here, \nwe mask all words that exceed a TF-IDF threshold \nvalue. \n\n\n\nTable 9 :\n9Task-Adaptive Pre-Training (TAPT) results when masking words with a high TF-IDF score. The cross-validation results are shown in parenthesis. Here, we randomly select tokens to mask from the words that exceed the threshold value.\n\nTable 10 :\n10Temporal Task-Adaptive Pre-Training (Temporal TAPT) results. The cross-validation results are shown in parenthesis. Here, time-related words (numbers, before/after, every, hour, etc.) are masked with higher probability than the other words.model \npattern1 pattern2 pattern3 pattern4 \n\nMC-TACO \nTimeML \n\u2192MC-TACO \nCosmosQA \n\u2192MC-TACO \nSWAG \n\u2192MC-TACO \nTAPT(random) \nTemporal TAPT \nEM [%] \n45.0 \n45.6 \n44.4 \n46.5 \nF1 [%] \n72.9 \n73.2 \n72.0 \n73.9 \n\n\n\nTable 11 :\n11Ensemble Model results.Temporal Task-Adaptive Pre-Training obtained the best performance, with an EM score of 46.5% and an F1-score of 73.9%.This model also outperformed the model fromZhou et al. (2019): a BERT model with standard fine-tuning, and a time unit normalized BERT model, where the authors further add unit normalization to temporal expressions in candidate answers and fine-tune on the MC-TACO dataset. Table 12 shows the results.model \nEM [%] F1 [%] \nBERT \n39.6 \n66.1 \nBERT + unit \n42.7 \n69.9 \nnormalization \nOurs \n46.5 \n73.9 \nHuman \n75.8 \n87.1 \n\n\n\nTable 12 :\n12Comparison of our best ensemble model with the model from Zhou et al. (2019).\n\nTable 13 :\n13Comparison of our best ensemble model with TACOLM(Zhou et al., 2020).\nhttps://www.nltk.org/nltk data/ 2 https://scikit-learn.org/stable/modules/feature extraction.html#stopwords\nAcknowledgementsThis study was financially supported by KAKENHI (18H05521). We express our gratitude for the support.\nTowards understanding ensemble, knowledge distillation and self-distillation in deep learning. Zeyuan Allen, -Zhu , Yuanzhi Li, arXiv:2012.09816arXiv preprintZeyuan Allen-Zhu and Yuanzhi Li. 2020. Towards understanding ensemble, knowledge distillation and self-distillation in deep learning. arXiv preprint arXiv:2012.09816.\n\nA surprisingly robust trick for the winograd schema challenge. Oana-Maria Camburu, Vid Kocijan, Thomas Lukasiewicz, Yordan Yordanov, Oana-Maria Camburu, Vid Kocijan, Thomas Lukasiewicz, and Yordan Yordanov. 2019. A surprisingly robust trick for the winograd schema challenge.\n\nBERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaLong and Short Papers1Association for Computational LinguisticsJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Associ- ation for Computational Linguistics.\n\nThe AQUAINT corpus of English news text. David Graff, content copyrightDavid Graff. 2002. The AQUAINT corpus of English news text:[content copyright] Portions\u00a9 1998-2000\n\n. Xinhua News Service. Linguistic Data Consortium. Associated Press, IncNew York TimesNew York Times, Inc.,\u00a9 1998-2000 Associated Press, Inc.,\u00a9 1996-2000 Xinhua News Service. Linguistic Data Consortium.\n\nDon't stop pretraining: Adapt language models to domains and tasks. Ana Suchin Gururangan, Swabha Marasovi\u0107, Kyle Swayamdipta, Iz Lo, Doug Beltagy, Noah A Downey, Smith, 10.18653/v1/2020.acl-main.740Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational LinguisticsSuchin Gururangan, Ana Marasovi\u0107, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. 2020. Don't stop pretraining: Adapt language models to domains and tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8342-8360, Online. Association for Computational Linguistics.\n\nCosmos QA: Machine reading comprehension with contextual commonsense reasoning. Lifu Huang, Le Ronan, Chandra Bras, Yejin Bhagavatula, Choi, 10.18653/v1/D19-1243Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsLifu Huang, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2019. Cosmos QA: Machine reading comprehension with contextual commonsense rea- soning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu- ral Language Processing (EMNLP-IJCNLP), pages 2391-2401, Hong Kong, China. Association for Computational Linguistics.\n\nXiaodong Liu, Hao Cheng, Pengcheng He, Weizhu Chen, Yu Wang, Hoifung Poon, Jianfeng Gao, arXiv:2004.08994Adversarial training for large neural language models. arXiv preprintXiaodong Liu, Hao Cheng, Pengcheng He, Weizhu Chen, Yu Wang, Hoifung Poon, and Jianfeng Gao. 2020. Adversarial training for large neural language models. arXiv preprint arXiv:2004.08994.\n\nA multiaxis annotation scheme for event temporal relations. Qiang Ning, Hao Wu, Dan Roth, 10.18653/v1/P18-1122Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational Linguistics1Qiang Ning, Hao Wu, and Dan Roth. 2018. A multi- axis annotation scheme for event temporal relations\". In Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1: Long Papers), pages 1318-1328, Melbourne, Aus- tralia. Association for Computational Linguistics.\n\nExtending timeml with typical durations of events. Feng Pan, Rutu Mulkar-Mehta, Jerry R Hobbs, Proceedings of the Workshop on Annotating and Reasoning about Time and Events. the Workshop on Annotating and Reasoning about Time and EventsFeng Pan, Rutu Mulkar-Mehta, and Jerry R Hobbs. 2006. Extending timeml with typical durations of events. In Proceedings of the Workshop on Anno- tating and Reasoning about Time and Events, pages 38-45.\n\nJason Phang, Thibault F\u00e9vry, Samuel R Bowman, arXiv:1811.01088Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks. arXiv preprintJason Phang, Thibault F\u00e9vry, and Samuel R Bowman. 2018. Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks. arXiv preprint arXiv:1811.01088.\n\nThe timebank corpus. James Pustejovsky, Patrick Hanks, Roser Sauri, Andrew See, Robert Gaizauskas, Andrea Setzer, Dragomir Radev, Beth Sundheim, David Day, Lisa Ferro, Corpus linguistics. Lancaster, UK40James Pustejovsky, Patrick Hanks, Roser Sauri, An- drew See, Robert Gaizauskas, Andrea Setzer, Dragomir Radev, Beth Sundheim, David Day, Lisa Ferro, et al. 2003. The timebank corpus. In Corpus linguistics, volume 2003, page 40. Lancaster, UK.\n\nTongshuang Marco Tulio Ribeiro, Carlos Wu, Sameer Guestrin, Singh, arXiv:2005.04118Beyond accuracy: Behavioral testing of nlp models with checklist. arXiv preprintMarco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. 2020. Beyond accuracy: Behav- ioral testing of nlp models with checklist. arXiv preprint arXiv:2005.04118.\n\n. Roser Saur\u00ed, Jessica Littman, Bob Knippen, Robert Gaizauskas, Andrea Setzer, James Pustejovsky, Timeml annotation guidelines. Version. 1131Roser Saur\u00ed, Jessica Littman, Bob Knippen, Robert Gaizauskas, Andrea Setzer, and James Pustejovsky. 2006. Timeml annotation guidelines. Version, 1(1):31.\n\nSemeval-2013 task 1: Tempeval-3: Evaluating time expressions, events, and temporal relations. Naushad Uzzaman, Hector Llorens, Leon Derczynski, James Allen, Marc Verhagen, James Pustejovsky, Proceedings of the Seventh International Workshop on Semantic Evaluation. the Seventh International Workshop on Semantic Evaluation2Second Joint Conference on Lexical and Computational Semantics (* SEM)Naushad UzZaman, Hector Llorens, Leon Derczyn- ski, James Allen, Marc Verhagen, and James Puste- jovsky. 2013. Semeval-2013 task 1: Tempeval-3: Evaluating time expressions, events, and temporal relations. In Second Joint Conference on Lexical and Computational Semantics (* SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 1-9.\n\nSWAG: A large-scale adversarial dataset for grounded commonsense inference. Rowan Zellers, Yonatan Bisk, Roy Schwartz, Yejin Choi, 10.18653/v1/D18-1009Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsRowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018. SWAG: A large-scale adversar- ial dataset for grounded commonsense inference. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 93- 104, Brussels, Belgium. Association for Computa- tional Linguistics.\n\ngoing on a vacation\" takes longer than \"going for a walk\": A study of temporal commonsense understanding. Ben Zhou, Daniel Khashabi, Qiang Ning, Dan Roth, 10.18653/v1/D19-1332Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsBen Zhou, Daniel Khashabi, Qiang Ning, and Dan Roth. 2019. \"going on a vacation\" takes longer than \"going for a walk\": A study of temporal com- monsense understanding. In Proceedings of the 2019 Conference on Empirical Methods in Natu- ral Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3363-3369, Hong Kong, China. Association for Computational Linguistics.\n\nTemporal common sense acquisition with minimal supervision. Ben Zhou, Qiang Ning, Daniel Khashabi, Dan Roth, 10.18653/v1/2020.acl-main.678Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational LinguisticsBen Zhou, Qiang Ning, Daniel Khashabi, and Dan Roth. 2020. Temporal common sense acquisition with minimal supervision. In Proceedings of the 58th Annual Meeting of the Association for Compu- tational Linguistics, pages 7579-7589, Online. As- sociation for Computational Linguistics.\n", "annotations": {"author": "[{\"end\":132,\"start\":76},{\"end\":219,\"start\":133},{\"end\":270,\"start\":220},{\"end\":330,\"start\":271}]", "publisher": null, "author_last_name": "[{\"end\":89,\"start\":83},{\"end\":146,\"start\":143},{\"end\":227,\"start\":220},{\"end\":287,\"start\":278}]", "author_first_name": "[{\"end\":82,\"start\":76},{\"end\":142,\"start\":133},{\"end\":277,\"start\":271}]", "author_affiliation": "[{\"end\":131,\"start\":91},{\"end\":218,\"start\":178},{\"end\":269,\"start\":229},{\"end\":329,\"start\":289}]", "title": "[{\"end\":60,\"start\":1},{\"end\":390,\"start\":331}]", "venue": "[{\"end\":463,\"start\":392}]", "abstract": "[{\"end\":1266,\"start\":568}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1360,\"start\":1339},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":1541,\"start\":1520},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2057,\"start\":2038},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2720,\"start\":2701},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2989,\"start\":2971},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4066,\"start\":4045},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4339,\"start\":4318},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4524,\"start\":4503},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5121,\"start\":5101},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5142,\"start\":5121},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5504,\"start\":5484},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5521,\"start\":5504},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6023,\"start\":6004},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6275,\"start\":6249},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6298,\"start\":6285},{\"end\":6334,\"start\":6303},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6979,\"start\":6959},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7015,\"start\":6993},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8620,\"start\":8595},{\"end\":8944,\"start\":8938},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9160,\"start\":9142},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9686,\"start\":9662},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":15801,\"start\":15783},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":19919,\"start\":19901},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":20452,\"start\":20433}]", "figure": "[{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":17868,\"start\":17659},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":18004,\"start\":17869},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":18133,\"start\":18005},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":18258,\"start\":18134},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":18396,\"start\":18259},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":18550,\"start\":18397},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":18780,\"start\":18551},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":19003,\"start\":18781},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":19245,\"start\":19004},{\"attributes\":{\"id\":\"tab_18\",\"type\":\"table\"},\"end\":19702,\"start\":19246},{\"attributes\":{\"id\":\"tab_19\",\"type\":\"table\"},\"end\":20277,\"start\":19703},{\"attributes\":{\"id\":\"tab_20\",\"type\":\"table\"},\"end\":20369,\"start\":20278},{\"attributes\":{\"id\":\"tab_22\",\"type\":\"table\"},\"end\":20453,\"start\":20370}]", "paragraph": "[{\"end\":2058,\"start\":1282},{\"end\":2602,\"start\":2060},{\"end\":2846,\"start\":2604},{\"end\":3699,\"start\":2886},{\"end\":4016,\"start\":3701},{\"end\":4481,\"start\":4026},{\"end\":4836,\"start\":4498},{\"end\":5362,\"start\":4857},{\"end\":5782,\"start\":5364},{\"end\":5825,\"start\":5784},{\"end\":6024,\"start\":5827},{\"end\":6426,\"start\":6026},{\"end\":6537,\"start\":6428},{\"end\":7164,\"start\":6539},{\"end\":7492,\"start\":7166},{\"end\":7540,\"start\":7494},{\"end\":7571,\"start\":7542},{\"end\":7655,\"start\":7573},{\"end\":7791,\"start\":7657},{\"end\":7851,\"start\":7793},{\"end\":7913,\"start\":7853},{\"end\":7964,\"start\":7915},{\"end\":8014,\"start\":7966},{\"end\":8233,\"start\":8016},{\"end\":9255,\"start\":8235},{\"end\":9562,\"start\":9257},{\"end\":9960,\"start\":9581},{\"end\":10262,\"start\":9987},{\"end\":10624,\"start\":10289},{\"end\":10866,\"start\":10626},{\"end\":11818,\"start\":10887},{\"end\":13341,\"start\":11820},{\"end\":14542,\"start\":13343},{\"end\":15195,\"start\":14544},{\"end\":15484,\"start\":15197},{\"end\":16079,\"start\":15486},{\"end\":16731,\"start\":16094},{\"end\":17147,\"start\":16733},{\"end\":17658,\"start\":17162}]", "formula": null, "table_ref": "[{\"end\":10261,\"start\":10254},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":10443,\"start\":10436},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":10979,\"start\":10972},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":11978,\"start\":11971},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":12023,\"start\":12016},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":12763,\"start\":12756},{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":13340,\"start\":13333},{\"attributes\":{\"ref_id\":\"tab_14\"},\"end\":14027,\"start\":14020},{\"attributes\":{\"ref_id\":\"tab_18\"},\"end\":14755,\"start\":14747},{\"attributes\":{\"ref_id\":\"tab_19\"},\"end\":15519,\"start\":15511},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":15981,\"start\":15973}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1280,\"start\":1268},{\"attributes\":{\"n\":\"2\"},\"end\":2884,\"start\":2849},{\"attributes\":{\"n\":\"3\"},\"end\":4024,\"start\":4019},{\"attributes\":{\"n\":\"3.1\"},\"end\":4496,\"start\":4484},{\"attributes\":{\"n\":\"3.2\"},\"end\":4855,\"start\":4839},{\"attributes\":{\"n\":\"3.3\"},\"end\":9579,\"start\":9565},{\"attributes\":{\"n\":\"4\"},\"end\":9974,\"start\":9963},{\"attributes\":{\"n\":\"4.1\"},\"end\":9985,\"start\":9977},{\"attributes\":{\"n\":\"4.2\"},\"end\":10287,\"start\":10265},{\"attributes\":{\"n\":\"4.3\"},\"end\":10876,\"start\":10869},{\"end\":10885,\"start\":10879},{\"attributes\":{\"n\":\"4.4\"},\"end\":16092,\"start\":16082},{\"attributes\":{\"n\":\"5\"},\"end\":17160,\"start\":17150},{\"end\":17879,\"start\":17870},{\"end\":18015,\"start\":18006},{\"end\":18144,\"start\":18135},{\"end\":18269,\"start\":18260},{\"end\":18407,\"start\":18398},{\"end\":18561,\"start\":18552},{\"end\":18791,\"start\":18782},{\"end\":19014,\"start\":19005},{\"end\":19257,\"start\":19247},{\"end\":19714,\"start\":19704},{\"end\":20289,\"start\":20279},{\"end\":20381,\"start\":20371}]", "table": "[{\"end\":17868,\"start\":17662},{\"end\":18133,\"start\":18017},{\"end\":18780,\"start\":18563},{\"end\":19003,\"start\":18793},{\"end\":19702,\"start\":19500},{\"end\":20277,\"start\":20159}]", "figure_caption": "[{\"end\":17662,\"start\":17661},{\"end\":18004,\"start\":17881},{\"end\":18258,\"start\":18146},{\"end\":18396,\"start\":18271},{\"end\":18550,\"start\":18409},{\"end\":19245,\"start\":19016},{\"end\":19500,\"start\":19260},{\"end\":20159,\"start\":19717},{\"end\":20369,\"start\":20292},{\"end\":20453,\"start\":20384}]", "figure_ref": null, "bib_author_first_name": "[{\"end\":20781,\"start\":20775},{\"end\":20793,\"start\":20789},{\"end\":20803,\"start\":20796},{\"end\":21079,\"start\":21069},{\"end\":21092,\"start\":21089},{\"end\":21108,\"start\":21102},{\"end\":21128,\"start\":21122},{\"end\":21370,\"start\":21365},{\"end\":21387,\"start\":21379},{\"end\":21401,\"start\":21395},{\"end\":21415,\"start\":21407},{\"end\":22273,\"start\":22268},{\"end\":22673,\"start\":22670},{\"end\":22699,\"start\":22693},{\"end\":22715,\"start\":22711},{\"end\":22731,\"start\":22729},{\"end\":22740,\"start\":22736},{\"end\":22754,\"start\":22750},{\"end\":22756,\"start\":22755},{\"end\":23437,\"start\":23433},{\"end\":23447,\"start\":23445},{\"end\":23462,\"start\":23455},{\"end\":23474,\"start\":23469},{\"end\":24329,\"start\":24321},{\"end\":24338,\"start\":24335},{\"end\":24355,\"start\":24346},{\"end\":24366,\"start\":24360},{\"end\":24375,\"start\":24373},{\"end\":24389,\"start\":24382},{\"end\":24404,\"start\":24396},{\"end\":24748,\"start\":24743},{\"end\":24758,\"start\":24755},{\"end\":24766,\"start\":24763},{\"end\":25377,\"start\":25373},{\"end\":25387,\"start\":25383},{\"end\":25407,\"start\":25402},{\"end\":25409,\"start\":25408},{\"end\":25766,\"start\":25761},{\"end\":25782,\"start\":25774},{\"end\":25798,\"start\":25790},{\"end\":26129,\"start\":26124},{\"end\":26150,\"start\":26143},{\"end\":26163,\"start\":26158},{\"end\":26177,\"start\":26171},{\"end\":26189,\"start\":26183},{\"end\":26208,\"start\":26202},{\"end\":26225,\"start\":26217},{\"end\":26237,\"start\":26233},{\"end\":26253,\"start\":26248},{\"end\":26263,\"start\":26259},{\"end\":26560,\"start\":26550},{\"end\":26588,\"start\":26582},{\"end\":26599,\"start\":26593},{\"end\":26899,\"start\":26894},{\"end\":26914,\"start\":26907},{\"end\":26927,\"start\":26924},{\"end\":26943,\"start\":26937},{\"end\":26962,\"start\":26956},{\"end\":26976,\"start\":26971},{\"end\":27289,\"start\":27282},{\"end\":27305,\"start\":27299},{\"end\":27319,\"start\":27315},{\"end\":27337,\"start\":27332},{\"end\":27349,\"start\":27345},{\"end\":27365,\"start\":27360},{\"end\":28053,\"start\":28048},{\"end\":28070,\"start\":28063},{\"end\":28080,\"start\":28077},{\"end\":28096,\"start\":28091},{\"end\":28763,\"start\":28760},{\"end\":28776,\"start\":28770},{\"end\":28792,\"start\":28787},{\"end\":28802,\"start\":28799},{\"end\":29715,\"start\":29712},{\"end\":29727,\"start\":29722},{\"end\":29740,\"start\":29734},{\"end\":29754,\"start\":29751}]", "bib_author_last_name": "[{\"end\":20787,\"start\":20782},{\"end\":20806,\"start\":20804},{\"end\":21087,\"start\":21080},{\"end\":21100,\"start\":21093},{\"end\":21120,\"start\":21109},{\"end\":21137,\"start\":21129},{\"end\":21377,\"start\":21371},{\"end\":21393,\"start\":21388},{\"end\":21405,\"start\":21402},{\"end\":21425,\"start\":21416},{\"end\":22279,\"start\":22274},{\"end\":22691,\"start\":22674},{\"end\":22709,\"start\":22700},{\"end\":22727,\"start\":22716},{\"end\":22734,\"start\":22732},{\"end\":22748,\"start\":22741},{\"end\":22763,\"start\":22757},{\"end\":22770,\"start\":22765},{\"end\":23443,\"start\":23438},{\"end\":23453,\"start\":23448},{\"end\":23467,\"start\":23463},{\"end\":23486,\"start\":23475},{\"end\":23492,\"start\":23488},{\"end\":24333,\"start\":24330},{\"end\":24344,\"start\":24339},{\"end\":24358,\"start\":24356},{\"end\":24371,\"start\":24367},{\"end\":24380,\"start\":24376},{\"end\":24394,\"start\":24390},{\"end\":24408,\"start\":24405},{\"end\":24753,\"start\":24749},{\"end\":24761,\"start\":24759},{\"end\":24771,\"start\":24767},{\"end\":25381,\"start\":25378},{\"end\":25400,\"start\":25388},{\"end\":25415,\"start\":25410},{\"end\":25772,\"start\":25767},{\"end\":25788,\"start\":25783},{\"end\":25805,\"start\":25799},{\"end\":26141,\"start\":26130},{\"end\":26156,\"start\":26151},{\"end\":26169,\"start\":26164},{\"end\":26181,\"start\":26178},{\"end\":26200,\"start\":26190},{\"end\":26215,\"start\":26209},{\"end\":26231,\"start\":26226},{\"end\":26246,\"start\":26238},{\"end\":26257,\"start\":26254},{\"end\":26269,\"start\":26264},{\"end\":26580,\"start\":26561},{\"end\":26591,\"start\":26589},{\"end\":26608,\"start\":26600},{\"end\":26615,\"start\":26610},{\"end\":26905,\"start\":26900},{\"end\":26922,\"start\":26915},{\"end\":26935,\"start\":26928},{\"end\":26954,\"start\":26944},{\"end\":26969,\"start\":26963},{\"end\":26988,\"start\":26977},{\"end\":27297,\"start\":27290},{\"end\":27313,\"start\":27306},{\"end\":27330,\"start\":27320},{\"end\":27343,\"start\":27338},{\"end\":27358,\"start\":27350},{\"end\":27377,\"start\":27366},{\"end\":28061,\"start\":28054},{\"end\":28075,\"start\":28071},{\"end\":28089,\"start\":28081},{\"end\":28101,\"start\":28097},{\"end\":28768,\"start\":28764},{\"end\":28785,\"start\":28777},{\"end\":28797,\"start\":28793},{\"end\":28807,\"start\":28803},{\"end\":29720,\"start\":29716},{\"end\":29732,\"start\":29728},{\"end\":29749,\"start\":29741},{\"end\":29759,\"start\":29755}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:2012.09816\",\"id\":\"b0\"},\"end\":21004,\"start\":20680},{\"attributes\":{\"id\":\"b1\"},\"end\":21281,\"start\":21006},{\"attributes\":{\"doi\":\"10.18653/v1/N19-1423\",\"id\":\"b2\",\"matched_paper_id\":52967399},\"end\":22225,\"start\":21283},{\"attributes\":{\"id\":\"b3\"},\"end\":22396,\"start\":22227},{\"attributes\":{\"id\":\"b4\"},\"end\":22600,\"start\":22398},{\"attributes\":{\"doi\":\"10.18653/v1/2020.acl-main.740\",\"id\":\"b5\",\"matched_paper_id\":216080466},\"end\":23351,\"start\":22602},{\"attributes\":{\"doi\":\"10.18653/v1/D19-1243\",\"id\":\"b6\",\"matched_paper_id\":202540590},\"end\":24319,\"start\":23353},{\"attributes\":{\"doi\":\"arXiv:2004.08994\",\"id\":\"b7\"},\"end\":24681,\"start\":24321},{\"attributes\":{\"doi\":\"10.18653/v1/P18-1122\",\"id\":\"b8\",\"matched_paper_id\":5066019},\"end\":25320,\"start\":24683},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":2896894},\"end\":25759,\"start\":25322},{\"attributes\":{\"doi\":\"arXiv:1811.01088\",\"id\":\"b10\"},\"end\":26101,\"start\":25761},{\"attributes\":{\"id\":\"b11\"},\"end\":26548,\"start\":26103},{\"attributes\":{\"doi\":\"arXiv:2005.04118\",\"id\":\"b12\"},\"end\":26890,\"start\":26550},{\"attributes\":{\"id\":\"b13\"},\"end\":27186,\"start\":26892},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":640783},\"end\":27970,\"start\":27188},{\"attributes\":{\"doi\":\"10.18653/v1/D18-1009\",\"id\":\"b15\",\"matched_paper_id\":52019251},\"end\":28652,\"start\":27972},{\"attributes\":{\"doi\":\"10.18653/v1/D19-1332\",\"id\":\"b16\",\"matched_paper_id\":202541184},\"end\":29650,\"start\":28654},{\"attributes\":{\"doi\":\"10.18653/v1/2020.acl-main.678\",\"id\":\"b17\",\"matched_paper_id\":218581125},\"end\":30282,\"start\":29652}]", "bib_title": "[{\"end\":21363,\"start\":21283},{\"end\":22668,\"start\":22602},{\"end\":23431,\"start\":23353},{\"end\":24741,\"start\":24683},{\"end\":25371,\"start\":25322},{\"end\":26122,\"start\":26103},{\"end\":27280,\"start\":27188},{\"end\":28046,\"start\":27972},{\"end\":28758,\"start\":28654},{\"end\":29710,\"start\":29652}]", "bib_author": "[{\"end\":20789,\"start\":20775},{\"end\":20796,\"start\":20789},{\"end\":20808,\"start\":20796},{\"end\":21089,\"start\":21069},{\"end\":21102,\"start\":21089},{\"end\":21122,\"start\":21102},{\"end\":21139,\"start\":21122},{\"end\":21379,\"start\":21365},{\"end\":21395,\"start\":21379},{\"end\":21407,\"start\":21395},{\"end\":21427,\"start\":21407},{\"end\":22281,\"start\":22268},{\"end\":22693,\"start\":22670},{\"end\":22711,\"start\":22693},{\"end\":22729,\"start\":22711},{\"end\":22736,\"start\":22729},{\"end\":22750,\"start\":22736},{\"end\":22765,\"start\":22750},{\"end\":22772,\"start\":22765},{\"end\":23445,\"start\":23433},{\"end\":23455,\"start\":23445},{\"end\":23469,\"start\":23455},{\"end\":23488,\"start\":23469},{\"end\":23494,\"start\":23488},{\"end\":24335,\"start\":24321},{\"end\":24346,\"start\":24335},{\"end\":24360,\"start\":24346},{\"end\":24373,\"start\":24360},{\"end\":24382,\"start\":24373},{\"end\":24396,\"start\":24382},{\"end\":24410,\"start\":24396},{\"end\":24755,\"start\":24743},{\"end\":24763,\"start\":24755},{\"end\":24773,\"start\":24763},{\"end\":25383,\"start\":25373},{\"end\":25402,\"start\":25383},{\"end\":25417,\"start\":25402},{\"end\":25774,\"start\":25761},{\"end\":25790,\"start\":25774},{\"end\":25807,\"start\":25790},{\"end\":26143,\"start\":26124},{\"end\":26158,\"start\":26143},{\"end\":26171,\"start\":26158},{\"end\":26183,\"start\":26171},{\"end\":26202,\"start\":26183},{\"end\":26217,\"start\":26202},{\"end\":26233,\"start\":26217},{\"end\":26248,\"start\":26233},{\"end\":26259,\"start\":26248},{\"end\":26271,\"start\":26259},{\"end\":26582,\"start\":26550},{\"end\":26593,\"start\":26582},{\"end\":26610,\"start\":26593},{\"end\":26617,\"start\":26610},{\"end\":26907,\"start\":26894},{\"end\":26924,\"start\":26907},{\"end\":26937,\"start\":26924},{\"end\":26956,\"start\":26937},{\"end\":26971,\"start\":26956},{\"end\":26990,\"start\":26971},{\"end\":27299,\"start\":27282},{\"end\":27315,\"start\":27299},{\"end\":27332,\"start\":27315},{\"end\":27345,\"start\":27332},{\"end\":27360,\"start\":27345},{\"end\":27379,\"start\":27360},{\"end\":28063,\"start\":28048},{\"end\":28077,\"start\":28063},{\"end\":28091,\"start\":28077},{\"end\":28103,\"start\":28091},{\"end\":28770,\"start\":28760},{\"end\":28787,\"start\":28770},{\"end\":28799,\"start\":28787},{\"end\":28809,\"start\":28799},{\"end\":29722,\"start\":29712},{\"end\":29734,\"start\":29722},{\"end\":29751,\"start\":29734},{\"end\":29761,\"start\":29751}]", "bib_venue": "[{\"end\":20773,\"start\":20680},{\"end\":21067,\"start\":21006},{\"end\":21589,\"start\":21447},{\"end\":22266,\"start\":22227},{\"end\":22447,\"start\":22400},{\"end\":22888,\"start\":22801},{\"end\":23689,\"start\":23514},{\"end\":24479,\"start\":24426},{\"end\":24880,\"start\":24793},{\"end\":25494,\"start\":25417},{\"end\":25909,\"start\":25823},{\"end\":26289,\"start\":26271},{\"end\":26697,\"start\":26633},{\"end\":27027,\"start\":26990},{\"end\":27451,\"start\":27379},{\"end\":28209,\"start\":28123},{\"end\":29004,\"start\":28829},{\"end\":29877,\"start\":29790},{\"end\":21740,\"start\":21591},{\"end\":22962,\"start\":22890},{\"end\":23867,\"start\":23691},{\"end\":24974,\"start\":24882},{\"end\":25558,\"start\":25496},{\"end\":26304,\"start\":26291},{\"end\":27510,\"start\":27453},{\"end\":28299,\"start\":28211},{\"end\":29182,\"start\":29006},{\"end\":29951,\"start\":29879}]"}}}, "year": 2023, "month": 12, "day": 17}