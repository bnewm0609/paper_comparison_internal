{"id": 227227883, "updated": "2023-10-06 08:57:45.278", "metadata": {"title": "Time-series Change Point Detection with Self-Supervised Contrastive Predictive Coding", "authors": "[{\"first\":\"Shohreh\",\"last\":\"Deldari\",\"middle\":[]},{\"first\":\"Daniel\",\"last\":\"Smith\",\"middle\":[\"V.\"]},{\"first\":\"Hao\",\"last\":\"Xue\",\"middle\":[]},{\"first\":\"Flora\",\"last\":\"Salim\",\"middle\":[\"D.\"]}]", "venue": "Proceedings of the Web Conference 2021", "journal": "Proceedings of the Web Conference 2021", "publication_date": {"year": 2020, "month": 11, "day": 28}, "abstract": "Change Point Detection techniques aim to capture changes in trends and sequences in time-series data to describe the underlying behaviour of the system. Detecting changes and anomalies in the web services, the trend of applications usage can provide valuable insight towards the system, however, many existing approaches are done in a supervised manner, requiring well-labelled data. As the amount of data produced and captured by sensors are growing rapidly, it is getting harder and even impossible to annotate the data. Therefore, coming up with a self-supervised solution is a necessity these days. In this work, we propose TSCP2 a novel self-supervised technique for temporal change point detection, based on representation learning with Temporal Convolutional Network (TCN). To the best of our knowledge, our proposed method is the first method which employs Contrastive Learning for prediction with the aim change point detection. Through extensive evaluations, we demonstrate that our method outperforms multiple state-of-the-art change point detection and anomaly detection baselines, including those adopting either unsupervised or semi-supervised approach. TSCP2 is shown to improve both non-Deep learning- and Deep learning-based methods by 0.28 and 0.12 in terms of average F1-score across three datasets.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2011.14097", "mag": "3109518304", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/www/DeldariSXS21", "doi": "10.1145/3442381.3449903"}}, "content": {"source": {"pdf_hash": "854c93fdb747f1ed4905d0211e0419e91db27b45", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2011.14097v2.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2011.14097", "status": "GREEN"}}, "grobid": {"id": "9f93addeba3b0b03b3a63db01713653d21d607c6", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/854c93fdb747f1ed4905d0211e0419e91db27b45.txt", "contents": "\nTime-series Change Point Detection with Self-Supervised Contrastive Predictive Coding\nACMCopyright ACM2018\n\nShohreh Deldari shohreh.deldari@rmit.edu.au \nDaniel V Smith daniel.v.smith@data61.csiro.au \nHao Xue hao.xue@rmit.edu.au \nFlora D Salim flora.salim@rmit.edu.au \nShohreh Deldari \nDaniel V Smith \nHao Xue \nFlora D Salim \n\nSchool of Science\nData61, CSIRO\nSchool of Science\nRMIT University Melbourne\nHobartAustralia, Australia\n\n\nSchool of Science\nRMIT University Melbourne\nAustralia\n\n\nRMIT University Melbourne\nAustralia\n\nTime-series Change Point Detection with Self-Supervised Contrastive Predictive Coding\n\nThe Web Conference 2021\nNew York, NY, USAACM12201810.1145/1122445.1122456ACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00CCS CONCEPTS \u2022 Computing methodologies \u2192 Machine learningUnsuper- vised learningAnomaly detectionLearning latent representa- tions\u2022 Information systems \u2192 Data stream mining KEYWORDS Unsupervised learning, Time series change point detection, Anom- aly detection, Contrastive learning\nChange Point Detection techniques aim to capture changes in trends and sequences in time-series data to describe the underlying behaviour of the system. Detecting changes and anomalies in the web services, the trend of applications usage can provide valuable insights into the system. However, many existing approaches are done in a supervised manner, requiring well-labelled data. As the amount of data produced and captured by sensors are growing rapidly, it is getting harder and even impossible to annotate the data. Therefore, coming up with a self-supervised solution is a necessity these days. In this work, we propose \u2212 2 a novel self-supervised technique for temporal change point detection, based on representation learning with a Temporal Convolutional Network (TCN). To the best of our knowledge, our proposed method is the first method which employs Contrastive Learning for prediction with the aim of change point detection. Through extensive evaluations, we demonstrate that our method outperforms multiple state-of-the-art change point detection and anomaly detection baselines, including those adopting the either unsupervised or semi-supervised approach. \u2212 2 is shown to improve both non-Deep learning-and Deep learningbased methods by 0.28 and 0.12 in terms of average F1-score across three datasets.\n\nINTRODUCTION\n\nThe ubiquity of digital technologies in combination with unlimited processing power and storage capacity, means that our ability to access and analyse data is unprecedented. The scale and velocity in which data is being stored and shared, however, means that we do not have the resources to utilise traditional data curation processes. Consequently, this is why self-supervised and unsupervised learning are hot topics in the Artificial Intelligence community. The challenge is to maximise the value of the unprecedented scale of raw and non-annotated data.\n\nChange Point Detection (CPD) techniques are one such analytical approach that can be used to extract meaning from time series. Identifying change points in time series, whether the data has been generated from video cameras, microphones, environmental sensors or mobile applications can be critical to understanding the underlying behaviour of the system being modelled. For instance, change points can represent alterations in the system state that require human attention. For instance, they may be indicative of a system fault or an upcoming emergency. Furthermore, they can be used in the closely related problems of temporal segmentation, event detection and temporal anomaly detection.\n\nCPD techniques have been applied to multivariate time series data in a broad range of research areas including network traffic analysis [24], IoT applications and smart homes [3], human activity recognition (HAR) [6,12,27,39,44], human physiological and emotional analysis [12], factory automation [50], trajectory prediction [35], user authentication [21], life-logging [8], elderly rehabilitation [25], and daily work routine studies [11].\n\nIn addition to time series, CPD can applied to other data modes with a temporal dimension such as video. CPD techniques have been used in video captioning [13,15] and video summarising [1,45] applications.\n\nChange points can be associated with salient changes in one of a number of different properties of a time series including its continuity, distribution or temporal shape patterns. Unsupervised CPD methods are generally developed to represent one property, for instance, FLOSS [17] was developed to detect changes in the temporal shape patterns, whilst RuLSIF [28], aHSIC [51] were developed to identify changes in the underlying statistical distributions.\n\nCurrent methods do not generalise well across time series for tasks such as temporal change point detection [12]. This is because the temporal boundaries (i.e. change points) that provide semantic meaning to the time series are often associated with changes of different properties. For example, the abnormalities in the rhythm of the human heart are best characterised by changes in the temporal shape pattern of electrocardiogram (ECG) time series. Whereas changes in human posture measured with an RFID sensor system are best characterised by abrupt statistical changes in the time series. As suggested by [12] most of the time these changes take place slowly in real life and continuous data capturing scenarios. Therefore gradual drift in both shape and the distribution of data is a challenging situation for existing change point detection methods.\n\nIn this work, we propose a novel approach for self-supervised CPD using contrastive loss-based representation learning. We pose the question of whether self-supervised learning can be used to provide an effective, general representation for change point detection. The intuition here is to exploit the local correlation present within a time series by learning an auto-regressive representation that maximises the similarity between contiguous time intervals, whilst minimizing the similarity between pairs of time intervals located at a greater temporal distance from one another (i.e. pairs of intervals likely to be less correlated). It is hypothesized that whenever the learnt representation differs significantly between contiguous time intervals, we can consider this to be a change point.\n\nWe aim to show that this self-supervised representation of time series is capable of detecting a broader range of change points than previous methods specifically designed to exploit a narrower range of time series properties (i.e. either its temporal continuity, distribution or shape patterns.) A high-level overview of the whole idea and process is shown in Figure 1. This is the first paper that has proposed using contrastive based representation learning for CPD problem. Furthermore, whilst there are contrastive learning methods for image [9,41], speech [33] and text [9], this is the first approach utilising contrastive learning on general time series, which in turn, introduces some unique challenges. The main contributions of our paper are as follows:\n\n\u2022 We leverage the power of contrastive learning as an unsupervised objective function for a generic time series change point detection task. To the best of our knowledge, we are the first to employ contrastive learning for time-series change point detection problem. \u2022 We propose a representation learning framework based on contrastive learning to tackle the problem of self-supervised time-series change point detection by capturing the compact vectors representing the history and future sequences. \u2022 We compare the proposed method against other CPD methods, including the deep learning-and non deep learningbased methods and investigate the benefits of each through extensive experiments. \u2022 We investigate the impact of each hyperparameter setting used in the self-supervised setting and contrastive learning methods such as batch size, code size, and window size.\n\nAll the code, data and experiments are available in the project webpage at removed for anonymity\n\n\nRELATED WORK AND BACKGROUND\n\nIn this section we review the existing approaches for time series change point detection problem. In addition to CPD methods, since we employ contrastive learning for time series based CPD, we outline recent work in this space. Then we explore current representation learning methods, not only with respect to time-series, but recent work with other data modalities as well.\n\n\nTime-series change point detection\n\nAlthough self-supervised time series CPD has recently attracted the interest of Deep Learning community, current methods are mostly based on non-deep learning approaches.\n\nExisting approaches can be categorized based upon the features of the time series they consider for change point detection. Statistical methods often compute change points on the basis of identifying where there are statistical differences between adjacent short intervals of a time series. The statistical differences between intervals are usually measured with either parametric or non-parametric approaches. Parametric methods use a Probability Density Function (PDF) such as [5] or auto-regressive model [52] to represent the time intervals, however, such convenient representations limit the types of statistical changes that can be detected.\n\nNon-parametric methods offer a greater degree of flexibility to represent the density functions of time intervals by utilising kernel functions. It was found that directly estimating the ratio of the time interval PDFs was a simpler problem to address than estimating the individual PDFs of the time intervals. The methods in RuLSIF [28], KLIEP [53] and SEP [3] used a non-parametric Gaussian kernel to model the density ratio distribution between subsequent time intervals. [51] detected change points by utilising the non-parametric additive Hilbert-Schmidt Independence Criterion (aHSIC) to compute the dependency between time adjacent intervals and a pseudo label of statistical change between the intervals. Kernel approaches assume there is statistical homogeneity within each interval, which can be problematic for change point detection. Furthermore, kernel functions often require parameters to be carefully tuned.\n\nThere is another line of statistical approaches which search for the interval boundaries (change points) of a time series in order optimize a statistical cost function across its constituent intervals. IGTS [34] and OnlineIGTS [55] proposed top-down and dynamic programming approaches to find the boundaries that maximise the information gain of each time interval. GGS [19] was an online multivariate time series change point detection that used a greedy search to identify the interval boundaries that maximize the regularized likelihood estimate of the segmented Gaussian model.\n\nThe other category of CPD methods attempt to identify changes in the temporal shape of time series. The main intuition is that change points relate to the samples where there is minimal repetition in the shape patterns of time series [17]. Authors of [50] proposed a motif discovery approach in order to extract rare patterns that can distinguish separate segments [22].\n\nRecently, ESPRESSO [12] proposed a hybrid model representing both the temporal shape pattern and statistical distribution of time series for temporal change point detection. It was shown that a hybrid model was able to detect change points of multi-variate time series across a diverse range of datasets encompassing continuous data with gradual changes in both its shape and statistical features.\n\nAuthors of [2] and [42] have done extensive reviews on time series change point detection approaches and explored different categories of state-of-the-art non-Deep Learning change point detection techniques.\n\nIn addition, a small number of deep learning based CPD methods have been proposed for time series data. Kernel Learning Change Point Detection, KLCPD [7], is a recent state-of-the-art end-to-end CPD method which solves the problem of parameter tuning in kernel-based methods, by automatically learning and updating the kernel parameters and combining multiple kernels to capture different types of change points. KLCPD utilised a two-sample test for measuring the difference between contiguous sub-sequences. It was shown that KLCPD can significantly outperform other existing deep learning and non deep learning change point detection methods.\n\nApart from time series, change point detection is useful in video processing application for summarizing video, extracting the segments of interest [45], and automatic caption generation or synchronisation [1,13,15]. Existing video segmentation approaches are mostly supervised or weakly supervised and benefit from the knowledge of the order of actions. In contrast, [1] proposed an auto-regressive model to predict the next video frames based on the most recently seen frames. The prediction error was used as a measure of detecting segment boundaries.\n\n\nRepresentation Learning\n\nIn recent years, there has been an increased attention on representation learning in order to capture informative and compact representations of video [1,31], image [9,20], text [33], and timeseries [16,29,36,37] data.\n\n\nContrastive-based Representation\n\nLearning. Contrastive predictive coding, CPC [33], is proposed to use auto-regressive models to learn representations within a latent emdedding space as opposed to the raw data. The main intuition of this idea is that learning representations of a signal across its more abstract, global features is more effective than learning across its high dimension, low level features. The authors demonstrate they can learn effective representations across different data modalities such as images, text and speech with respect to downstream modeling tasks.\n\nFirstly, a deep network encode was used to map the signal into a lower dimension latent space before an auto-regressive model was then applied to predict future frames. A contrastive loss function maximizing the mutual information between the density ratio of the current and future frame was employed. CPCv2 [20] showed replacing the auto-regressive RNN of CPC with a convolutional neural network (CNN) improved the quality of the learnt representations for weakly supervised image processing classification tasks. Authors in [4] and [48] conducted a systematic evaluation of generic convolutional and recurrent architectures for time series across diverse range of data and discovered that CNN techniques can outperform their RNN rivals.\n\nAdditionally, CNN models are more scalable due to the ability of efficient parallelization on GPUs. Besides this, exponentially dilated convolutions have also been introduced to better capture, compared to full convolutions, long-range dependencies at constant depth by exponentially increasing the receptive field of the network [4,32,54]. Additionally, recurrent networks can be outperformed by convolutional networks on the issue of exploding and vanishing gradient [4].\n\nAlthough many existing works have been done on image and text representation learning tasks, few articles have explored multivariate or high-dimensional time-series representation learning. Authors in [16] proposed general-purposed method to learn representation from variable length time-series using deep dilated convolutional network (WaveNet [32]) and employed unsupervised triplet loss function based on negative sampling.\n\n\nContrastive\n\nLoss. Contrastive loss functions are used to learn network representations that minimize the distance between pairs of positive instances (i.e. neighbouring instances or instances from the same semantic class) and maximise the distance between pairs of negative instances (i.e. instances that are far from one another or instances of different semantic classes).\n\nContrastive loss [10] and Triplet loss [46] are the most commonly used loss functions in contrastive learning problems. In general, the Triplet loss function outperforms the Contrastive loss because the relationship between positive and negative pairs are both considered in the loss calculation, whereas the positive and negative pairs are considered separately in the Contrastive loss function. The Triplet loss suffers from only considering one positive and one negative pair of instances at each time. Both functions can suffer from slow convergence and require expensive data sampling methods to provide informative instance pairs, or triplets of instances, that accelerate training [41].\n\nTo solve the forementioned problems, loss functions have been calculated based on multiple negative pairs which are called Multiple Negative Learning. N-Paired loss [41] and infoNCE base on Noise Contrastive Estimation [18,30] are examples of recent multiple negative learning loss functions. These approaches, however, require expensive data sampling methods to select nontrivial negative instances for training. This requirement introduces the new concept of Hard Negative Instance Mining, which has been shown to play a critical role in ensuring these cost functions are effective [14,47]. A number of different sampling strategies have been proposed to addresses this issue, including hard negative sampling [40], semihard mining [38], distance weighted sampling [47], hard negative class mining [41], and rank-based negative mining [43].\n\n\nMETHOD 3.1 Problem Definition\n\nGiven a sequence of dimensional time-series { 1 , 2 , ..., , ...}, \u2208 R which are representing the behaviour of the system, we attempt to detect transitions times, , that are indicative of a change in the system state. Our technique does not rely on any assumptions about the distribution or statistical characteristics of the data making it applicable to a broad range of real-world applications.\n\nWe define change points (or segment boundaries) as the time points in future can not be anticipated from the data before this point. Hence, each time segment between two consequent change points can be used to anticipate future, and the prediction error can be used as a measure for transition to the next segment.\n\n\n\u2212 2 Overview\n\nWe propose a self-supervised approach to learn compact latent representations of time series with strong statistical dependencies over short time periods. Change points are identified when contiguous time intervals are mapped into this latent space and a salient reduction in the statistical dependencies are identified. There are several change point and temporal anomaly detection methods in video [1] and time-series [23] that use auto-regressive models for prediction by exploiting the smoothness of the signal across time. In these approaches, change points are detected at samples associated with a large increase in the prediction error. However, since the prediction error is highly dependent upon the distribution of the data, we propose to use representation learning to extract a compact latent representation which is invariant to the distribution of the data. Figure 1 illustrates the main idea of our \u2212 2 approach. We hypothesize that this approach is much more effective to detect change points because the latent space extracted from contiguous time intervals are likely to be dependent upon the same shared information.\n\nHere we adopt an idea similar to [33] to learn compact representations of time-series (through the use of an encoder) that maximise the mutual information over short time scales. We do not use an LSTM to encode the time series, however, given it has been shown that temporal convolutional networks (TCN) can often produce superior prediction performance with sequential data [20] and are generally easier to train. Consequently, we employ the auto-regressive deep convolution network, WaveNet [32], to learn our encoded representation.\n\nWe use the contrastive learning approach to train the encoder for each time series by extracting a single pair of contiguous time intervals and a set of time interval pairs that are temporally separate in each batch. predict the future representation vector amongst the set of distracting representation vectors that we have taken from other parts of the time-series in the training batch. Our proposed method first compresses high-dimensional data into a much more compact latent embedding space in which conditional predictions are easier to model. Secondly, we use 3-layer fully connected network on top of this latent space to make predictions about future steps. As we discussed earlier we apply the \u2212 [41] as the contrastive deep learning metric to learn the best match among other negative samples. Contrastive learning approaches have attracted attention for learning word embeddings in natural language models [33] and image classification [9,20]. To the best of our knowledge, it is the first time contrastive learning has been used for change point detection.\n\nWe exploit differences between the prediction error and moving average of the prediction error across multiple past time steps to identify changes in the time-series properties. Using a moving average of the time-steps allows us to discard the effect of noise in the time-series. Hence, larger prediction errors are indicative of a higher likelihood of a change point occurrence Figure 2 shows the overall architecture of the proposed method. In the following section we will describe the main modules in the following order: 1) Representation learning block, 2) Negative sampling, and 3) Boundary detection modules.\n\n\nRepresentation Learning\n\nAt the core of this approach is an encoder that maps pairs of temporally contiguous intervals of a time series into a compact latent representation that maximises the mutual information between them.  Figure 3: Model architecture. We use two stacks of TCN with kernel size 4 and dilation sizes of 1, 4, and 16 followed by three Dense layers as the prediction head.\n\nWe use a temporal Convolutional network (TCN)-based encoder to extract representation vectors from the history and future data. Although our framework places no constraints upon the network architecture chosen as the encoder, the TCN was relatively parameter light and found to be easier to train than recurrent neural networks. Figure 3 illustrates the encoder architecture for each history or future branch which consists of two blocks of TCN with kernel size 4 and three layers of dilation equal to [1,4,16]. The model applies 64 filters and followed by a simple three-layer prediction/projection head with ReLU activation function and batch normalization. The modified illustration of TCN layer 1 is shown in the figure.\n\nA pair of temporally contiguous frames (called the history and future time intervals) are fed into the encoder to extract an embedding vector. We then add a simple projection head (shown as (\u00b7) and \u2032 (\u00b7) in Figure 2, respectively) to produce a low dimension embedding representation for both the history and future time intervals. To this end, an MLP neural network with three hidden layers is used as the projection head to map the encoded values into a lower dimensional latent space. The aim of the contrastive learning approach here is to train an embedded representation that maximises the shared information between pairs of contiguous time intervals (positive pairs), whilst minimizing the shared information between pairs of time intervals that are well separated (negative pairs) in the time series. In the next subsection we define the cost function that will be used for this purpose.\n\n\nCost function.\n\nWe applied the loss function as it is based upon the Noise Contrastive Estimation [30] proposed for natural language processing and recently used for image representation learning techniques [9,20,33].\n\nThe cost function is defined to maximize the mutual information between the positive pair of time intervals in training. In each training instance, two different sets of time intervals belonging to the time series are included. A single positive pair of time adjacent intervals ( ((\u210e , ))), history (\u210e ) and future time intervals ( ), and a set of \u2212 1 negative pairs ((\u210e , ) \u2260 ) where the intervals \u210e and are well separated within the time series.\n\nUsing the loss function, we first calculate the probability of the positive pair in each batch using the scaledfunction:\n= ( (\u210e , )/ ) =1 ( (\u210e , )/ )(1)\nwhere is a scaling parameter and is the cosine similarity between each pair of interval embeddings. The final loss is calculated with the binary cross-entropy function over the probabilities of all positive pairs belonging to the training batch. Since the probabilities associated with the positive pairs are computed using the similarity scores of the negative pairs in (), the cross entropy loss function can be simplified to:\n\u2112 = \u2212 \u2211\ufe01 , ( ) + (1 \u2212 ) (1 \u2212 ) (2) = 1 if = 0 if \u2260 (3) \u2112 = \u2211\ufe01 \u2212 ( )(4)\n3.3.2 Negative Sampling. Following the proposed hard negative class mining in [41], we propose a simpler method to randomly select positive pairs, which are then used to construct the negative pairs within each batch. Figure 4 depicts the process of batch construction in our model. We choose random pairs of contiguous frames (\u210e , ) that are used as the positive pairs in each training batch. Each pair must adhere to the constraint of being a minimum temporal distance from each other. This minimum temporal distance constraint is used to enable each batch to adopt the future intervals of the other \u2212 1 positive pairs as negative pairs, given they are guaranteed to be sufficiently separated from the history interval of the batch's own positive pair. The intuition is that time series are commonly non-stationary, and hence, subsequences that are temporally separate from one another are likely to exhibit far weaker statistical dependencies than adjacent subsequences. We need to set the threshold of minimum temporal distance based upon the time series application being considered.\n\n\nFigure 4: Batch construction\n\nConsequently, we can select positive and negative pairs with a minimal computational cost relative to the other negative mining approaches mentioned in Section 2. Figure 5 and 6 show examples of the time interval associated with positive and negative pairs, respectively, with their corresponding extracted representation vectors.\n\n\nChange Point Detection Module\n\nWe hypothesize that when a change point intersects the embeddings of the history and future intervals, they will be distributed differently. To detect change points we utilise the cosine similarity between the latent embeddings of the history and future intervals ( (\u210e , )) across the entire time series. A valley picking algorithm is used to detect local minima that fall below a predefined threshold ; the intervals associated with these local minima become our change point estimates. Figure 7 shows an example of cosine similarity between the latent embeddings of the history and future intervals across an entire time series. The green areas show the interval pairs (\u210e , ) which contain a change point for a subset of Benchamark-4 of Yahoo dataset [26]. Comparing mean value of similarity between recent frames can detect the actual change points (green area) very effectively.\n\n\nEXPERIMENTS\n\nIn this section we evaluate the effectiveness of our proposed method. We first briefly describe the baselines and then introduce the datasets used in our experiments. Finally, the results of the comparisons with state-of-the-art methods and also sensitivity analysis are presented.\n\n\nDatasets\n\nA wide range of datasets were chosen to validate our proposed method and then benchmark it against other state-of-the-art CPD methods. We show the effectiveness of our method across a diverse range of different applications including web service traffic analysis, human activity recognition using wearable sensors and usage analysis of mobile applications.\n\n\u2022 Yahoo!Benchmark 2 [26]. Yahoo! benchmark dataset is one of the most cited benchmarks for anomaly detection. It contains time series with the varying trend, seasonality, and noise including random anomaly change points. We used all 100 time series of the forth benchmark as it is the only benchmark includes anomaly change points. \u2022 HASC 3 . HASC challenge 2011 dataset provides human activity data collected by multiple sensors such as accelerometer, gyrometer, etc. We used subset of HASC (The same subset used by recent state-of-the-art method, KLCPD) including only 3-axis accelerometer recordings. The aim of detecting change point detection with this dataset was to find transitions between physical activities such as \"stay\", \"walk\", \"jog\", \"skip\", \"stair up\", \"stair down\". \u2022 USC-HAD 4 [56]. USC-HAD dataset includes twelve human activities that were recorded separately across multiple subjects. Each human subject was fitted with a 3-axis accelerometer and 3-axis gyrometer that were attached to the front of the right hip and sampled at 100Hz. Activities were repeated five times for each subject and consisted of: \"walking forward\", \"walking left\", \"walking right\", \"walking upstairs\", \"walking downstairs\", \"running forward\", \"jumping up\", \"sitting\", \"standing\", \"sleeping\", \"elevator up\", and \"elevator down\". We randomly chose 30 activities from the first six participants and stitched the selected recordings together in a random manner.\n\nIn this research only the accelerometer data was used. Table 1 provides the characteristics of each dataset.    \n\n\nBaseline Methods\n\nThe performance of the proposed \u2212 2 method was compared against five state-of-the-art unsupervised change point detection techniques including ESPRESSO [12], FLOSS [17], aHSIC [51], RuL-SIF [28], and KLCPD [7]. To avoid inconsistencies and implementation errors, and provide a fair comparison, baseline methods were evaluated using publicly available source code.\n\nAs kernel based methods, RulSIF and aHSIC both need several parameters to be tuned such as regularization constant and kernel bandwidth. For RuLSIF the regularization constant was fixed at 0.01, as suggested by the authors. For aHSIC, the regularization constant and the kernel bandwidth parameter were set to 0.01 and 1, respectively, as suggested in the paper. Performance was compared across a range of frame sizes that were unique to each dataset based on its sample rate and minimum segment duration. As a Deep Learning-based method, KLCPD requires several hyperparameters to be tuned. To provide a fair comparison, different sets of hyper-parameters were investigated with respect to the window size, batch size and learning rate. Only the hyper-parameters that provided the highest true positives rates were presented in the paper. We used the same evaluation approach provided in paper 5 to calculate F1-score. The rest of the parameters remained unchanged from the original paper. Although the training process of KLCPD was unsupervised, the method still used labels as ground truth for fine-tuning model hyper-parameters during the validation phase.\n\n\nEvaluation Metrics\n\nThe performance of the models were evaluated with respect to the F1-score. Each change point estimate was defined as a true positive when it was located within a specified time window of the ground truth change point. When multiple change point estimates fell within a specified time window of the ground truth change point, only the closest estimate was considered a true positive and the remaining estimates were considered to be False Positives. A ground truth without any close estimated change points was considered to be a False Negative.\n\n\nFine-Tuning and Sensitivity Analysis\n\nIn this section, we have done extensive experiments to analyse the sensitivity of our proposed method to:\n\n\u2022 Window Size is the length of the time series intervals used (i.e. history and future interval). The window size should be large enough to represent important properties of the time series, however, should be less than the smallest interval between two consecutive change points. \u2022 Batch Size specifies the number of training instances ( ) used to update the model during the training phase. It also specifies the number of negative pairs ( \u2212 1) used in a training instance \u2022 Code Size specifies the length of the embedding vector that is extracted from the encoder network. Figure 8 compares the performance of \u2212 2 base on F1-score across Yahoo!Benchmark for different parameter settings.\n\n\nWindow size.\n\nSince the Yahoo data is sampled hourly and the minimum length between two consecutive change points is about 160, we chose window size ranging between 24 (1 day) to 100 ( 4 days). We found a monotonic increasing relationship between window size and detection performance. The largest window size led to the highest F1 scores given increased amounts of input data meant more important time series properties could be represented.\n\n\nBatch\n\nSize. We varied the batch size according to the set {4, 8, 16, 31, 64, 128} to see how this impacts performance. It was shown that batch sizes of more than 32 do not provide a noticeable improvement in F1-score and even tend to decrease performance. We believe this may be because larger batch sizes have a higher likelihood of possessing pairs of instances from same underlying class with similar temporal patterns. Each positive pair in the batch is considered to be a negative pair for the other instances in the batch based on the temporal distance constraints. This may not be necessarily true, given temporal patterns (associated with the same semantic class) can often repeat themselves at different points within a time series. Using contrastive learning to push these instances to have a different representation can degrade the model.\n\n\nCode Size.\n\nIn contrast to many of the representation learning approaches, we investigated how the embedding dimensionality affected the model's performance. We vary the code size from 4 to 20 (which is %4 to %20 of the window size). As we can see in Figure 8 the optimal code size depends on the window size. A very small code size decrease the performance, given it has insufficient capacity to represent crucial information in the original data. Similarly, as code size grows more than about %30 of input data there will be less gain in representation learning (shown in Figure 8(b) and 8(c)).\n\n\nBaseline Comparison\n\nIn this section, we investigate the performance of the proposed method and six other baselines across three datasets with respect to the F1-score.\n\nIn change point or anomaly detection methods, unlike classification tasks, the delay of capturing change point or anomaly is very important [2]. This delay time shows how much time after the occurrence of a change point, it is detectable by the method. Hence, it is critical to detect changes/anomalies as soon as possible. We have reported the F1-score regarding three different levels of detection delay. To conduct a fair comparison between methods, we have done extensive experiments by varying the input parameters of each model and reported only the best F1-score with corresponding parameters in Table 2.\n\n\nYahoo! Benchmark Evaluation.\n\nTo compare the ability of each method in capturing the change points and anomalies, we set three levels of maximum delay (temporal distance) including 24, 50, and 75 time-steps. If the temporal distance between the actual and estimated change point is less than the specified margin, it will be considered as true positive. Table 2 shows the highest F1-score and corresponding window size ( length of the input frame) for each method.\n\nYahoo! Benchmark dataset is one of the well-cited [49] and one of the most complex ones for temporal anomaly detection as the anomalies are mostly based on changes in seasonality, trend and noise in the data. Based on the results reported in Table 2, our proposed method, \u2212 2 strongly outperforms all other baselines. Although all the baselines are among the state-of-the-art methods, this dataset still is challenging for them. Four randomly selected sequences of this dataset are illustrated in Figure 9. and are able to detect changes in temporal shape patterns, however, they cannot distinguish between subtle statistical changes (i.e. in trends), which happens in most of the sequences in this dataset.\n\nis based on changes in the ratio of the distribution of adjacent frames. It is clear from the examples that some time entropy or distribution of adjacent segments (classes) remain the same, but we can observe changes in shapes.\n\nThe comparison on this dataset shows the superiority of the proposed method in these challenging situations which frequently happen in real world.\n\n\nUSC-HAD Dataset Evaluation.\n\nHence the sampling rate for this dataset is 100Hz, we vary the maximum tolerable delay between 1, 2, and 4 seconds (100, 200, and 400 time-steps) for an estimated change point to be considered as true positive. We explore different values as Kernel bandwidth for RuLSIF and Window size for FLOSS, ESPRESSO, KLCPD, and \u2212 2 (varied between 100, 200, and 400). Different kernel sizes (20, 40, and 50) are also investigated for aHSIC. We also use different learning rate for KLCPD and \u2212 2 and come up with 3 \u00d7 10 \u22124 and 1 \u00d7 10 \u22124 , respectively.\n\nAs it is shown in Table 2, our proposed method outperforms other baselines across all delay thresholds with a relatively large margin. By increasing the maximum delay threshold, higher F1scores for all the baselines are observed. In contrast, \u2212 2 is the only method that delivers a high F1-score even for the lowest input size (100-time steps) which means it can detect changes accurately in less than only one seconds after the occurrence.\n\nSimilar to Yahoo! dataset, we also compare the effect of batch size across different window sizes which are shown in Figure 10. Based on this Figure, for larger window sizes (for example when we need to consider a larger history of data), \u2212 2 will benefit from a larger batch size. On the other hand, when there are frequent change points with a shorter duration, the smaller batch size is better to join with smaller window sizes.\n\n\nHASC Dataset Evaluation.\n\nWe find that HASC dataset is the most challenging dataset out of the three for \u2212 2 and other baselines. Although \u2212 2 achieves the second place in this dataset for each separate window size, it still has a subtle higher F1-score in average. It provides 0.07, 0.02, 0.04, 0.02, 0.02 higher F1-score in average against FLOSS, aHSIC, RulSIF, ESPRESSO, and KLCPD, respectively. This is because that the characteristics of the dataset makes it difficult for change point detection methods to capture changes early. HASC dataset has the smallest number of sequence (around 39K time-steps) but the largest number of change points (65 change points). By applying sliding window across the sequence, there are a large number of pairs which encompass change points. This leads to a high ratio of fake positive pairs (samples with change in either history or future frame) to pure positive pairs in the training set. Since the model is self-supervised, it does not use the ground truth labels to clean the training set. Therefore, the constructed batches   contains many fake positive samples in the training phase which cause the problem. As aforementioned, to make the model computationally lighter and simpler, \u2212 2 does not employ any hard negative mining technique inside the batch construction module. The large number of fake positive pairs confuses the model. To avoid this problem, we suggest to enhance the model only by injecting a light negative mining. We also can generate more positive samples through augmentation.\n\n\nDiscussion.\n\nWe showed that \u2212 2 was able to outperform FLOSS, aHSIC, RulSIF and ESPRESSO as non-deep learning methods by 0.33, 0.33, 0.27, and 0.17 in terms of F1-score averaged over all of the datasets. In addition, \u2212 2 showed a 12% improvement in F1-score, in comparison to KLCDP which is the most recent and competitive deep-learning-based change point detection method.\n\nSince each baseline utilise different features, they may find some of datasets more challenging than others. For example, ESPRESSO performs very well on USC-HAD dataset which includes abrupt changes in both temporal patterns and statistical features. But neither ESPRESSO nor other non deep learning approaches are not able to distinguish changes in Yahoo! Benchmark. In contrast, we showed that our proposed method can achieve the first or the second best results across explored datasets. \u2212 2 has a significant improvement across Yahoo! and USC-HAD dataset, its performance is almost close the best method (KLCPD) for HASC dataset.\n\nWe also suggest the usage of augmentation techniques or negative mining batch construction to avoid the problem of high frequency change points that present themselves in some datasets as discussed in section 4.5.3.\n\nLast but not least, although the training time is not explored in our baselines and other mentioned related works, and also, we didn't claim that our TS-CP2 will be trained on the edge, we found this question very interesting and we wish we had included a discussion on it in the paper. We would like to explore the execution time in two steps. First, in the training step, we have used a very compact structure for representation learning with a shared model for history and future frame which result in speedy convergence compared to KLCPD (ICLR 2020).\n\nSecond, once the representation model (Encoder) is trained, change point detection is a very straight forward comparison between learnt representations and can be done online on even low resource devices. In contrast, KLCPD and RuLSIF as DL and non-DL Kernel-based method are not fast. The other baselines also cannot be applied online as they need to consider a batch of input data to capture repeated pattern or optimizing the entropy-based loss function (FLOSS, ESPRESSO, aHSIC).\n\n\nCONCLUSION\n\nIn this paper we proposed a novel self-supervised change point detection CPD method, \u2212 2 for time series and sensor data. \u2212 2 works on the predictability of the future according to the past data (history). Our proposed method is the first change point/anomaly detection method which employs contrastive learning with the aim of extracting the most compact, crucial and informative representation vector for every frame and estimate the change point based on the agreement between the learnt representation of subsequent frames.\n\nWe evaluated the ability of \u2212 2 in detecting change points against six other well-known state-of-the-art methods across three datasets. We have shown that our proposed method significantly outperform other baselines in two dataset and reaches a comparable score for the other dataset.\n\nAlthough the pre-trained \u2212 2 can detect changes in online applications, we aim to expand this method in our future work to continuously learn changes, anomalies and drifts in data.\n\nFigure 1 :\n1Overview of presented change point detection approach based on predictive representation learning.\n\nFigure 2 :\n2Illustration of the overall architecture of our \u2212 2 . Blue dash arrows indicate the back propagation in the training phase.\n\nFigure 5 :\n5Three examples of Positive pairs (left, length = 100) and their corresponding representation (right, length = 16). Positive pairs are subsequent frames.\n\nFigure 6 :\n6Three examples of Negative pairs (left, length = 100) and their corresponding representation (right, length = 16). A change point or anomaly has occurred somewhere in either history or future frame of the negative pairs. Change Points in cosine similarity\n\nFigure 7 :\n7Example of extracting change points using moving average of prediction error. The black line shows the cosine similarity between subsequent frames (history and future frames) in Benchmark-4 of Yahoo dataset[26]. The green areas highlight frames with change points/anomalies and the yellow line shows the moving average of calculated cosine similarity for previous frames.\n\nFigure 8 :\n8Code size and Batch Size sensitivity analysis across Yahoo!Benchmark dataset for different range of window size (length of input frame) equal to: (a) 24, (b) 50, (c) 75, and (d) 100 time steps.\n\nFigure 9 :\n9Four random time-series from Yahoo! Benchmark. The Yellow vertical lines show the change points. The spatial anomalies are not highlighted here as they are not the focus of neither our method nor the compared baselines.\n\nFigure 10 :\n10Comparing the effect of batch size across different window sizes for USC-HAD Dataset.\n\n\nKernel Size = 4, Dilation =[1,4,16] Layout of each block of TCN Prediction/projection headDense \nDense \nDense \n\nTCN1 \n\nTCN1 \n\nTCN1 \n\nTCN1 \n\n... \n\nTCN2 \n\nTCN2 \n\nTCN2 \n\nTCN2 \n\n... \n\nEncoder \n\n64 \n\n\n\n\nDataset characteristics. T is the total number of data samples, #sequences and #channels show the number of time-series and dimensions, respectively, and #CP is the total number of change points.Dataset \nT \n#sequences #channels #CP \nYahoo! Benchmark 164K \n100 \n1 \n208 \nHASC \n39K \n1 \n3 \n65 \nUSC-HAD \n97K \n6 \n3 \n30 \nTable 1: \n\nTable 2 :\n2Comparing the proposed method against other baselines based on F1-score across Yahoo!Benchmark, HASC, and USC dataset. Bold and underline texts represent the first and second highest score, respectively. Maximum Delay shows the maximum distance between an estimated boundary and its nearest actual change point that can be considered as True Positive. Best window shows the size input frame for each method that leads to the highest F1-score. We also presented the average F1-score across all different window sizes.\nWe acknowledge the main illustration and TCN implementation: https://github.com/philipperemy/keras-tcn\nhttps://webscope.sandbox.yahoo.com/catalog.php?datatype=s&did=70 3 http://hasc.jp/hc2011 4 http://sipi.usc.edu/had\nKLCPD source code: https://github.com/OctoberChang/klcpd_code\nACKNOWLEDGMENTSRemoved.\nA Perceptual Prediction Framework for Self Supervised Event Segmentation. N Sathyanarayanan, Sudeep Aakur, Sarkar, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionSathyanarayanan N Aakur and Sudeep Sarkar. 2019. A Perceptual Prediction Framework for Self Supervised Event Segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 1197-1206.\n\nA Survey of Methods for Time Series Change Point Detection. Knowledge and information systems. Samaneh Aminikhanghahi, Diane J Cook, 51Samaneh Aminikhanghahi and Diane J. Cook. 2017. A Survey of Methods for Time Series Change Point Detection. Knowledge and information systems 51, 2 (01 May 2017), 339-367.\n\nEnhancing Activity Recognition Using CPD-based Activity Segmentation. Samaneh Aminikhanghahi, Diane J Cook, Pervasive and Mobile Computing. 53Samaneh Aminikhanghahi and Diane J Cook. 2019. Enhancing Activity Recogni- tion Using CPD-based Activity Segmentation. Pervasive and Mobile Computing 53 (2019).\n\nAn Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling. Shaojie Bai, J Kolter, Vladlen Koltun, arXiv:1803.01271arXiv preprintShaojie Bai, J. Kolter, and Vladlen Koltun. 2018. An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling. arXiv preprint arXiv:1803.01271 (03 2018).\n\nDetection of abrupt changes: theory and application. Michelle Basseville, V Igor, Nikiforov, Prentice HallMichelle Basseville and Igor V Nikiforov. 1993. Detection of abrupt changes: theory and application. Prentice Hall.\n\nJoint Segmentation of Multivariate Time Series with Hidden Process Regression for Human Activity Recognition. Faicel Chamroukhi, Samer Mohammed, Dorra Trabelsi, Latifa Oukhellou, Yacine Amirat, Neurocomputing. 120Faicel Chamroukhi, Samer Mohammed, Dorra Trabelsi, Latifa Oukhellou, and Yacine Amirat. 2013. Joint Segmentation of Multivariate Time Series with Hidden Process Regression for Human Activity Recognition. Neurocomputing 120 (2013), 633-644.\n\nWei-Cheng Chang, Chun-Liang Li, arXiv:1901.06077Yiming Yang, and Barnab\u00e1s P\u00f3czos. 2019. Kernel change-point detection with auxiliary deep generative models. arXiv preprintWei-Cheng Chang, Chun-Liang Li, Yiming Yang, and Barnab\u00e1s P\u00f3czos. 2019. Kernel change-point detection with auxiliary deep generative models. arXiv preprint arXiv:1901.06077 (2019).\n\nThe Opportunity challenge: A benchmark Database for On-Body Sensor-based Activity Recognition. Ricardo Chavarriaga, Hesam Sagha, Alberto Calatroni, Gerhard Sundara Tejaswi Digumarti, Jos\u00e9 Tr\u00f6ster, Daniel Del R Mill\u00e1n, Roggen, Pattern Recognition Letters. 34Ricardo Chavarriaga, Hesam Sagha, Alberto Calatroni, Sundara Tejaswi Digu- marti, Gerhard Tr\u00f6ster, Jos\u00e9 del R Mill\u00e1n, and Daniel Roggen. 2013. The Op- portunity challenge: A benchmark Database for On-Body Sensor-based Activity Recognition. Pattern Recognition Letters 34, 15 (2013), 2033-2042.\n\nA Simple Framework for Contrastive Learning of Visual Representations. Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, ICML. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A Simple Framework for Contrastive Learning of Visual Representations. ICML (2020).\n\nLearning a similarity metric discriminatively, with application to face verification. Sumit Chopra, Raia Hadsell, Yann Lecun, IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05). IEEE1Sumit Chopra, Raia Hadsell, and Yann LeCun. 2005. Learning a similarity metric discriminatively, with application to face verification. In 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05), Vol. 1. IEEE, 539-546.\n\nInferring Work Routines and Behavior Deviations with Life-logging Sensor Data. Shohreh Deldari, Jonathan Liono, D Flora, Daniel V Salim, Smith, Proceedings of ACM International Conference on Web Search and Data Mining (WSDM) workshop on Task Intelligence (TI@WSDM. ACM International Conference on Web Search and Data Mining (WSDM) workshop on Task Intelligence (TI@WSDMACMShohreh Deldari, Jonathan Liono, Flora D Salim, and Daniel V Smith. 2019. Inferring Work Routines and Behavior Deviations with Life-logging Sensor Data. In Proceedings of ACM International Conference on Web Search and Data Mining (WSDM) workshop on Task Intelligence (TI@WSDM) (2019). ACM.\n\nESPRESSO: Entropy and ShaPe AwaRe TimE-Series SegmentatiOn for Processing Heterogeneous Sensor Data. Shohreh Deldari, Daniel V Smith, Amin Sadri, Flora Salim, 10.1145/3411832Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 477Shohreh Deldari, Daniel V. Smith, Amin Sadri, and Flora Salim. 2020. ESPRESSO: Entropy and ShaPe AwaRe TimE-Series SegmentatiOn for Processing Heteroge- neous Sensor Data. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 4, 3, Article 77 (Sept. 2020), 24 pages. https://doi.org/10.1145/3411832\n\nWeakly-supervised action segmentation with iterative soft boundary assignment. Li Ding, Chenliang Xu, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionLi Ding and Chenliang Xu. 2018. Weakly-supervised action segmentation with iterative soft boundary assignment. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 6508-6516.\n\nDeep embedding learning with discriminative sampling policy. Yueqi Duan, Lei Chen, Jiwen Lu, Jie Zhou, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionYueqi Duan, Lei Chen, Jiwen Lu, and Jie Zhou. 2019. Deep embedding learning with discriminative sampling policy. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 4964-4973.\n\nMs-tcn: Multi-stage temporal convolutional network for action segmentation. Abu Yazan, Jurgen Farha, Gall, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionYazan Abu Farha and Jurgen Gall. 2019. Ms-tcn: Multi-stage temporal convolu- tional network for action segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 3575-3584.\n\nUnsupervised scalable representation learning for multivariate time series. Jean-Yves Franceschi, Aymeric Dieuleveut, Martin Jaggi, Advances in Neural Information Processing Systems. Jean-Yves Franceschi, Aymeric Dieuleveut, and Martin Jaggi. 2019. Unsupervised scalable representation learning for multivariate time series. In Advances in Neural Information Processing Systems. 4650-4661.\n\nDomain Agnostic Online Semantic Segmentation for Multi-dimensional Time Series. Shaghayegh Gharghabi, Chin-Chia Michael Yeh, Yifei Ding, Wei Ding, Paul Hibbing, Samuel Lamunion, Andrew Kaplan, E Scott, Eamonn Crouter, Keogh, Data Mining and Knowledge Discovery. 33Shaghayegh Gharghabi, Chin-Chia Michael Yeh, Yifei Ding, Wei Ding, Paul Hibbing, Samuel LaMunion, Andrew Kaplan, Scott E Crouter, and Eamonn Keogh. 2019. Domain Agnostic Online Semantic Segmentation for Multi-dimensional Time Series. Data Mining and Knowledge Discovery 33, 1 (2019), 96-130.\n\nNoise-contrastive estimation: A new estimation principle for unnormalized statistical models. Michael Gutmann, Aapo Hyv\u00e4rinen, Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics. the Thirteenth International Conference on Artificial Intelligence and StatisticsMichael Gutmann and Aapo Hyv\u00e4rinen. 2010. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics. 297-304.\n\nGreedy Gaussian Segmentation of Multivariate Time Series. David Hallac, Peter Nystrup, Stephen Boyd, Advances in Data Analysis and Classification. 13David Hallac, Peter Nystrup, and Stephen Boyd. 2019. Greedy Gaussian Segmen- tation of Multivariate Time Series. Advances in Data Analysis and Classification 13, 3 (2019), 727-751.\n\n. J Olivier, Aravind H\u00e9naff, Jeffrey De Srinivas, Ali Fauw, Carl Razavi, Doersch, Eslami, and Aaron van den Oord. 2020. Data-efficient image recognition with contrastive predictive coding. ICML (2020Olivier J H\u00e9naff, Aravind Srinivas, Jeffrey De Fauw, Ali Razavi, Carl Doersch, SM Eslami, and Aaron van den Oord. 2020. Data-efficient image recognition with contrastive predictive coding. ICML (2020).\n\nAu-Id: Automatic User Identification and Authentication Through the Motions Captured from Sequential Human Activities Using RFID. Anna Huang, Dong Wang, Run Zhao, Qian Zhang, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies. the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies3Anna Huang, Dong Wang, Run Zhao, and Qian Zhang. 2019. Au-Id: Automatic User Identification and Authentication Through the Motions Captured from Sequential Human Activities Using RFID. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT) 3, 2 (2019), 1-26.\n\nDetecting Changes in Rare Patterns from Data Streams. David Tse Jung Huang, Yun Sing Koh, Gillian Dobbie, Russel Pears, Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD). SpringerDavid Tse Jung Huang, Yun Sing Koh, Gillian Dobbie, and Russel Pears. 2014. Detecting Changes in Rare Patterns from Data Streams. In Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD). Springer, 437-448.\n\nNetwork anomaly detection based on wavenet. Tero Kokkonen, Samir Puuska, Janne Alatalo, Eppu Heilimo, Antti M\u00e4kel\u00e4, Internet of Things, Smart Spaces, and Next Generation Networks and Systems. SpringerTero Kokkonen, Samir Puuska, Janne Alatalo, Eppu Heilimo, and Antti M\u00e4kel\u00e4. 2019. Network anomaly detection based on wavenet. In Internet of Things, Smart Spaces, and Next Generation Networks and Systems. Springer, 424-433.\n\nA Bayesian change point model for detecting SIP-based DDoS attacks. Bar\u0131\u015f Kurt, \u00c7a\u011fatay Y\u0131ld\u0131z, Yusuf Taha, B\u00fclent Ceritli, Ali Taylan Sankur, Cemgil, Digital Signal Processing. 77Bar\u0131\u015f Kurt, \u00c7a\u011fatay Y\u0131ld\u0131z, Taha Yusuf Ceritli, B\u00fclent Sankur, and Ali Taylan Cemgil. 2018. A Bayesian change point model for detecting SIP-based DDoS attacks. Digital Signal Processing 77 (2018), 48-62.\n\nAutomated Rehabilitation System: Movement Measurement and Feedback for Patients and Physiotherapists in the Rehabilitation Clinic. W K Agnes, Danniel Lam, Yeti Varona-Marin, Mitchell Li, Dana Fergenbaum, Kuli\u0107, Human-Computer Interaction. 31Agnes WK Lam, Danniel Varona-Marin, Yeti Li, Mitchell Fergenbaum, and Dana Kuli\u0107. 2016. Automated Rehabilitation System: Movement Measurement and Feedback for Patients and Physiotherapists in the Rehabilitation Clinic. Human- Computer Interaction 31, 3-4 (2016), 294-334.\n\nS5 -A Labeled Anomaly Detection Dataset. Nikolay Laptev, Saeed Amizadeh, Youssef Billawala, n.d.. version 1.0 (16M). ([n. d.Nikolay Laptev, Saeed Amizadeh, and Youssef Billawala. [n.d.]. \"S5 -A Labeled Anomaly Detection Dataset, version 1.0 (16M). ([n. d.]). https://webscope. sandbox.yahoo.com/catalog.php?datatype=s&did=70\n\nOptimal Time Window for Temporal Segmentation of Sensor Streams in multi-activity recognition. Jonathan Liono, Flora D Kai Qin, Salim, Proceedings of the 13th International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services. the 13th International Conference on Mobile and Ubiquitous Systems: Computing, Networking and ServicesJonathan Liono, A Kai Qin, and Flora D Salim. 2016. Optimal Time Window for Temporal Segmentation of Sensor Streams in multi-activity recognition. In Proceedings of the 13th International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services. 10-19.\n\nChangepoint Detection in Time-series Data by Relative Density-Ratio Estimation. Song Liu, Makoto Yamada, Nigel Collier, Masashi Sugiyama, Neural Networks. 43Song Liu, Makoto Yamada, Nigel Collier, and Masashi Sugiyama. 2013. Change- point Detection in Time-series Data by Relative Density-Ratio Estimation. Neural Networks 43 (2013), 72-83.\n\nSegmentation of Multivariate Industrial Time Series Data Based on Dynamic Latent Variable Predictability. Shaowen Lu, Shuyu Huang, IEEE Access. 8Shaowen Lu and Shuyu Huang. 2020. Segmentation of Multivariate Industrial Time Series Data Based on Dynamic Latent Variable Predictability. IEEE Access 8 (2020), 112092-112103.\n\nLearning word embeddings efficiently with noise-contrastive estimation. Andriy Mnih, Koray Kavukcuoglu, Advances in neural information processing systems. Andriy Mnih and Koray Kavukcuoglu. 2013. Learning word embeddings ef- ficiently with noise-contrastive estimation. In Advances in neural information processing systems. 2265-2273.\n\nTemporal Event Segmentation using Attention-based Perceptual Prediction Model for Continual Learning. Ramy Mounir, Roman Gula, J\u00f6rn Theuerkauf, Sudeep Sarkar, arXiv:2005.02463arXiv preprintRamy Mounir, Roman Gula, J\u00f6rn Theuerkauf, and Sudeep Sarkar. 2020. Temporal Event Segmentation using Attention-based Perceptual Prediction Model for Continual Learning. arXiv preprint arXiv:2005.02463 (2020).\n\nAaron Van Den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, Koray Kavukcuoglu, arXiv:1609.03499Wavenet: A generative model for raw audio. arXiv preprintAaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. 2016. Wavenet: A generative model for raw audio. arXiv preprint arXiv:1609.03499 (2016).\n\nAaron Van Den Oord, Yazhe Li, Oriol Vinyals, arXiv:1807.03748Representation learning with contrastive predictive coding. arXiv preprintAaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018).\n\nInformation Gain-based Metric for Recognizing Transitions in Human Activities. Amin Sadri, Yongli Ren, Flora D Salim, Pervasive and Mobile Computing. 38Amin Sadri, Yongli Ren, and Flora D Salim. 2017. Information Gain-based Metric for Recognizing Transitions in Human Activities. Pervasive and Mobile Computing 38 (2017), 92-109.\n\nWhat Will You Do for the Rest of the Day? an approach to continuous trajectory prediction. Amin Sadri, D Flora, Yongli Salim, Wei Ren, Shao, C John, Cecilia Krumm, Mascolo, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT). the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT)2Amin Sadri, Flora D Salim, Yongli Ren, Wei Shao, John C Krumm, and Cecilia Mascolo. 2018. What Will You Do for the Rest of the Day? an approach to continuous trajectory prediction. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT) 2, 4 (2018), 1-26.\n\nMulti-task self-supervised learning for human activity detection. Aaqib Saeed, Tanir Ozcelebi, Johan Lukkien, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies. the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies3Aaqib Saeed, Tanir Ozcelebi, and Johan Lukkien. 2019. Multi-task self-supervised learning for human activity detection. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 3, 2 (2019), 1-30.\n\nFederated Self-Supervised Learning of Multi-Sensor Representations for Embedded Intelligence. A Saeed, F D Salim, T Ozcelebi, J Lukkien, IEEE Internet of Things Journal. A. Saeed, F. D. Salim, T. Ozcelebi, and J. Lukkien. 2020. Federated Self-Supervised Learning of Multi-Sensor Representations for Embedded Intelligence. IEEE Internet of Things Journal (2020), 1-1.\n\nFacenet: A unified embedding for face recognition and clustering. Florian Schroff, Dmitry Kalenichenko, James Philbin, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionFlorian Schroff, Dmitry Kalenichenko, and James Philbin. 2015. Facenet: A unified embedding for face recognition and clustering. In Proceedings of the IEEE conference on computer vision and pattern recognition. 815-823.\n\nComplex Human Activity Recognition Using Smartphone and Wrist-Worn Motion Sensors. Muhammad Shoaib, Stephan Bosch, Ozlem Durmaz Incel, Hans Scholten, Havinga, Sensors. 16426Muhammad Shoaib, Stephan Bosch, Ozlem Durmaz Incel, Hans Scholten, and Paul JM Havinga. 2016. Complex Human Activity Recognition Using Smartphone and Wrist-Worn Motion Sensors. Sensors 16, 4 (2016), 426.\n\nDiscriminative learning of deep convolutional feature point descriptors. Edgar Simo-Serra, Eduard Trulls, Luis Ferraz, Iasonas Kokkinos, Pascal Fua, Francesc Moreno-Noguer, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionEdgar Simo-Serra, Eduard Trulls, Luis Ferraz, Iasonas Kokkinos, Pascal Fua, and Francesc Moreno-Noguer. 2015. Discriminative learning of deep convolutional feature point descriptors. In Proceedings of the IEEE International Conference on Computer Vision. 118-126.\n\nImproved deep metric learning with multi-class n-pair loss objective. Kihyuk Sohn, Advances in neural information processing systems. Kihyuk Sohn. 2016. Improved deep metric learning with multi-class n-pair loss objective. In Advances in neural information processing systems. 1857-1865.\n\nSelective review of offline change point detection methods. Charles Truong, Laurent Oudre, Nicolas Vayatis, Signal Processing. 167107299Charles Truong, Laurent Oudre, and Nicolas Vayatis. 2020. Selective review of offline change point detection methods. Signal Processing 167 (2020), 107299.\n\nRanked list loss for deep metric learning. Xinshao Wang, Yang Hua, Elyor Kodirov, Guosheng Hu, Romain Garnier, Neil M Robertson, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionXinshao Wang, Yang Hua, Elyor Kodirov, Guosheng Hu, Romain Garnier, and Neil M Robertson. 2019. Ranked list loss for deep metric learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 5207-5216.\n\nModeling RFID Signal Reflection for Contact-free Activity Recognition. Yanwen Wang, Yuanqing Zheng, 10.1145/3287071Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies. the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies193IMWUT) 2, 4, ArticleYanwen Wang and Yuanqing Zheng. 2018. Modeling RFID Signal Reflection for Contact-free Activity Recognition. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT) 2, 4, Article 193 (2018), 22 pages. https://doi.org/10.1145/3287071\n\nSequence-to-segment networks for segment detection. Zijun Wei, Boyu Wang, Minh Hoai Nguyen, Jianming Zhang, Zhe Lin, Xiaohui Shen, Radom\u00edr Mech, Dimitris Samaras, Advances in Neural Information Processing Systems. Zijun Wei, Boyu Wang, Minh Hoai Nguyen, Jianming Zhang, Zhe Lin, Xiaohui Shen, Radom\u00edr Mech, and Dimitris Samaras. 2018. Sequence-to-segment networks for segment detection. In Advances in Neural Information Processing Systems. 3507-3516.\n\nDistance metric learning for large margin nearest neighbor classification. Q Kilian, Lawrence K Weinberger, Saul, Journal of Machine Learning Research. 10Kilian Q Weinberger and Lawrence K Saul. 2009. Distance metric learning for large margin nearest neighbor classification. Journal of Machine Learning Research 10, 2 (2009).\n\nSampling matters in deep embedding learning. Chao-Yuan, R Wu, Alexander J Manmatha, Philipp Smola, Krahenbuhl, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionChao-Yuan Wu, R Manmatha, Alexander J Smola, and Philipp Krahenbuhl. 2017. Sampling matters in deep embedding learning. In Proceedings of the IEEE Interna- tional Conference on Computer Vision. 2840-2848.\n\nFelix Wu, Angela Fan, Alexei Baevski, Michael Yann N Dauphin, Auli, arXiv:1901.10430Pay less attention with lightweight and dynamic convolutions. arXiv preprintFelix Wu, Angela Fan, Alexei Baevski, Yann N Dauphin, and Michael Auli. 2019. Pay less attention with lightweight and dynamic convolutions. arXiv preprint arXiv:1901.10430 (2019).\n\nRenjie Wu, J Eamonn, Keogh, arXiv:2009.13807Current Time Series Anomaly Detection Benchmarks are Flawed and are Creating the Illusion of Progress. arXiv preprintRenjie Wu and Eamonn J Keogh. 2020. Current Time Series Anomaly Detection Benchmarks are Flawed and are Creating the Illusion of Progress. arXiv preprint arXiv:2009.13807 (2020).\n\nRobust Unsupervised Factory Activity Recognition with Body-worn Accelerometer Using Temporal Structure of Multiple Sensor Data Motifs. Qingxin Xia, Joseph Korpela, Yasuo Namioka, Takuya Maekawa, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies. the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies4Qingxin Xia, Joseph Korpela, Yasuo Namioka, and Takuya Maekawa. 2020. Robust Unsupervised Factory Activity Recognition with Body-worn Accelerometer Using Temporal Structure of Multiple Sensor Data Motifs. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 4, 3 (2020), 1-30.\n\nChange-point Detection with Feature Selection in High-dimensional Time-series Data. Makoto Yamada, Akisato Kimura, Futoshi Naya, Hiroshi Sawada, Proc. of 23th International Joint Conference on Artificial Intelligence (IJ-CAI). of 23th International Joint Conference on Artificial Intelligence (IJ-CAI)Makoto Yamada, Akisato Kimura, Futoshi Naya, and Hiroshi Sawada. 2013. Change-point Detection with Feature Selection in High-dimensional Time-series Data. In Proc. of 23th International Joint Conference on Artificial Intelligence (IJ- CAI).\n\nA unifying framework for detecting outliers and change points from non-stationary time series data. Kenji Yamanishi, Jun-Ichi Takeuchi, Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining. the eighth ACM SIGKDD international conference on Knowledge discovery and data miningKenji Yamanishi and Jun-ichi Takeuchi. 2002. A unifying framework for detecting outliers and change points from non-stationary time series data. In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining. 676-681.\n\nSequential Change-Point Detection Based on Direct Density-Ratio Estimation. Kawahara Yoshinobu, Masashi Sugiyama, Statistical Analysis and Data Mining. 5Kawahara Yoshinobu and Masashi Sugiyama. 2012. Sequential Change-Point Detection Based on Direct Density-Ratio Estimation. Statistical Analysis and Data Mining 5, 2 (2012), 114-127.\n\nFisher Yu, Vladlen Koltun, arXiv:1511.07122Multi-scale context aggregation by dilated convolutions. arXiv preprintFisher Yu and Vladlen Koltun. 2015. Multi-scale context aggregation by dilated convolutions. arXiv preprint arXiv:1511.07122 (2015).\n\nUnsupervised Online Change Point Detection in High-Dimensional Time Series. Masoomeh Zameni, Amin Sadri, Zahra Ghafoori, Masud Moshtaghi, Flora D Salim, Christopher Leckie, Kotagiri Ramamohanarao, Knowledge and Information Systems. KAISMasoomeh Zameni, Amin Sadri, Zahra Ghafoori, Masud Moshtaghi, Flora D. Salim, Christopher Leckie, and Kotagiri Ramamohanarao. 2019. Unsupervised Online Change Point Detection in High-Dimensional Time Series. Knowledge and Information Systems (KAIS) (2019), 719-750.\n\nUSC-HAD: a Daily Activity Dataset for Ubiquitous Activity Recognition Using Wearable Sensors. Mi Zhang, Alexander A Sawchuk, Proceedings of the 2012 ACM Conference on Ubiquitous Computing. the 2012 ACM Conference on Ubiquitous ComputingPittsburgh, PennsylvaniaMi Zhang and Alexander A Sawchuk. 2012. USC-HAD: a Daily Activity Dataset for Ubiquitous Activity Recognition Using Wearable Sensors. In Proceedings of the 2012 ACM Conference on Ubiquitous Computing (Pittsburgh, Pennsylvania) (UbiComp '12). 1036-1043.\n", "annotations": {"author": "[{\"end\":153,\"start\":109},{\"end\":200,\"start\":154},{\"end\":229,\"start\":201},{\"end\":268,\"start\":230},{\"end\":285,\"start\":269},{\"end\":301,\"start\":286},{\"end\":310,\"start\":302},{\"end\":325,\"start\":311},{\"end\":430,\"start\":326},{\"end\":486,\"start\":431},{\"end\":524,\"start\":487}]", "publisher": "[{\"end\":90,\"start\":87},{\"end\":656,\"start\":653}]", "author_last_name": "[{\"end\":124,\"start\":117},{\"end\":168,\"start\":163},{\"end\":208,\"start\":205},{\"end\":243,\"start\":238},{\"end\":284,\"start\":277},{\"end\":300,\"start\":295},{\"end\":309,\"start\":306},{\"end\":324,\"start\":319}]", "author_first_name": "[{\"end\":116,\"start\":109},{\"end\":160,\"start\":154},{\"end\":162,\"start\":161},{\"end\":204,\"start\":201},{\"end\":235,\"start\":230},{\"end\":237,\"start\":236},{\"end\":276,\"start\":269},{\"end\":292,\"start\":286},{\"end\":294,\"start\":293},{\"end\":305,\"start\":302},{\"end\":316,\"start\":311},{\"end\":318,\"start\":317}]", "author_affiliation": "[{\"end\":429,\"start\":327},{\"end\":485,\"start\":432},{\"end\":523,\"start\":488}]", "title": "[{\"end\":86,\"start\":1},{\"end\":610,\"start\":525}]", "venue": "[{\"end\":635,\"start\":612}]", "abstract": "[{\"end\":2331,\"start\":1012}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3739,\"start\":3735},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3777,\"start\":3774},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3815,\"start\":3812},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3818,\"start\":3815},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3821,\"start\":3818},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":3824,\"start\":3821},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":3827,\"start\":3824},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3876,\"start\":3872},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":3901,\"start\":3897},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":3929,\"start\":3925},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3955,\"start\":3951},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3973,\"start\":3970},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4002,\"start\":3998},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4039,\"start\":4035},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4201,\"start\":4197},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4204,\"start\":4201},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4230,\"start\":4227},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":4233,\"start\":4230},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4529,\"start\":4525},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4612,\"start\":4608},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":4624,\"start\":4620},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4818,\"start\":4814},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5319,\"start\":5315},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6910,\"start\":6907},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":6913,\"start\":6910},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6926,\"start\":6922},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6939,\"start\":6936},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9191,\"start\":9188},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":9221,\"start\":9217},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9695,\"start\":9691},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":9707,\"start\":9703},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9719,\"start\":9716},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":9837,\"start\":9833},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":10494,\"start\":10490},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":10514,\"start\":10510},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10657,\"start\":10653},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":11104,\"start\":11100},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":11121,\"start\":11117},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":11235,\"start\":11231},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11261,\"start\":11257},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":11651,\"start\":11648},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":11660,\"start\":11656},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":11999,\"start\":11996},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":12644,\"start\":12640},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":12701,\"start\":12698},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":12704,\"start\":12701},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12707,\"start\":12704},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":12863,\"start\":12860},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":13228,\"start\":13225},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":13231,\"start\":13228},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":13242,\"start\":13239},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":13245,\"start\":13242},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":13256,\"start\":13252},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":13277,\"start\":13273},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":13280,\"start\":13277},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":13283,\"start\":13280},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":13286,\"start\":13283},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":13378,\"start\":13374},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":14192,\"start\":14188},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":14409,\"start\":14406},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":14418,\"start\":14414},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":14953,\"start\":14950},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":14956,\"start\":14953},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":14959,\"start\":14956},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":15092,\"start\":15089},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":15300,\"start\":15296},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":15445,\"start\":15441},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":15923,\"start\":15919},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":15945,\"start\":15941},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":16594,\"start\":16590},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":16766,\"start\":16762},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":16820,\"start\":16816},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":16823,\"start\":16820},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":17185,\"start\":17181},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":17188,\"start\":17185},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":17313,\"start\":17309},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":17335,\"start\":17331},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":17368,\"start\":17364},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":17401,\"start\":17397},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":17438,\"start\":17434},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":18605,\"start\":18602},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":18626,\"start\":18622},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":19377,\"start\":19373},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":19719,\"start\":19715},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":19837,\"start\":19833},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":20800,\"start\":20796},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":20829,\"start\":20826},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":20832,\"start\":20829},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":22464,\"start\":22461},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":22466,\"start\":22464},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":22469,\"start\":22466},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":23685,\"start\":23681},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":23793,\"start\":23790},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":23796,\"start\":23793},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":23799,\"start\":23796},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":24986,\"start\":24982},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":27146,\"start\":27142},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":27963,\"start\":27959},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":28738,\"start\":28734},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":29684,\"start\":29680},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":29696,\"start\":29692},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":29708,\"start\":29704},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":29722,\"start\":29718},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":29737,\"start\":29734},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":34670,\"start\":34667},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":35661,\"start\":35657},{\"end\":37120,\"start\":37104},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":43871,\"start\":43867},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":44606,\"start\":44603},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":44608,\"start\":44606},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":44611,\"start\":44608}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":43075,\"start\":42964},{\"attributes\":{\"id\":\"fig_1\"},\"end\":43212,\"start\":43076},{\"attributes\":{\"id\":\"fig_2\"},\"end\":43378,\"start\":43213},{\"attributes\":{\"id\":\"fig_3\"},\"end\":43647,\"start\":43379},{\"attributes\":{\"id\":\"fig_4\"},\"end\":44032,\"start\":43648},{\"attributes\":{\"id\":\"fig_5\"},\"end\":44239,\"start\":44033},{\"attributes\":{\"id\":\"fig_6\"},\"end\":44472,\"start\":44240},{\"attributes\":{\"id\":\"fig_7\"},\"end\":44573,\"start\":44473},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":44771,\"start\":44574},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":45097,\"start\":44772},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":45626,\"start\":45098}]", "paragraph": "[{\"end\":2904,\"start\":2347},{\"end\":3597,\"start\":2906},{\"end\":4040,\"start\":3599},{\"end\":4247,\"start\":4042},{\"end\":4704,\"start\":4249},{\"end\":5561,\"start\":4706},{\"end\":6358,\"start\":5563},{\"end\":7124,\"start\":6360},{\"end\":7994,\"start\":7126},{\"end\":8092,\"start\":7996},{\"end\":8498,\"start\":8124},{\"end\":8707,\"start\":8537},{\"end\":9356,\"start\":8709},{\"end\":10281,\"start\":9358},{\"end\":10864,\"start\":10283},{\"end\":11236,\"start\":10866},{\"end\":11635,\"start\":11238},{\"end\":11844,\"start\":11637},{\"end\":12490,\"start\":11846},{\"end\":13046,\"start\":12492},{\"end\":13292,\"start\":13074},{\"end\":13877,\"start\":13329},{\"end\":14618,\"start\":13879},{\"end\":15093,\"start\":14620},{\"end\":15522,\"start\":15095},{\"end\":15900,\"start\":15538},{\"end\":16595,\"start\":15902},{\"end\":17439,\"start\":16597},{\"end\":17869,\"start\":17473},{\"end\":18185,\"start\":17871},{\"end\":19338,\"start\":18202},{\"end\":19875,\"start\":19340},{\"end\":20947,\"start\":19877},{\"end\":21565,\"start\":20949},{\"end\":21957,\"start\":21593},{\"end\":22683,\"start\":21959},{\"end\":23580,\"start\":22685},{\"end\":23800,\"start\":23599},{\"end\":24249,\"start\":23802},{\"end\":24371,\"start\":24251},{\"end\":24832,\"start\":24404},{\"end\":25992,\"start\":24904},{\"end\":26355,\"start\":26025},{\"end\":27271,\"start\":26389},{\"end\":27568,\"start\":27287},{\"end\":27937,\"start\":27581},{\"end\":29393,\"start\":27939},{\"end\":29507,\"start\":29395},{\"end\":29891,\"start\":29528},{\"end\":31052,\"start\":29893},{\"end\":31619,\"start\":31075},{\"end\":31765,\"start\":31660},{\"end\":32457,\"start\":31767},{\"end\":32902,\"start\":32474},{\"end\":33756,\"start\":32912},{\"end\":34355,\"start\":33771},{\"end\":34525,\"start\":34379},{\"end\":35138,\"start\":34527},{\"end\":35605,\"start\":35171},{\"end\":36314,\"start\":35607},{\"end\":36543,\"start\":36316},{\"end\":36691,\"start\":36545},{\"end\":37264,\"start\":36723},{\"end\":37706,\"start\":37266},{\"end\":38139,\"start\":37708},{\"end\":39685,\"start\":38168},{\"end\":40061,\"start\":39701},{\"end\":40696,\"start\":40063},{\"end\":40913,\"start\":40698},{\"end\":41469,\"start\":40915},{\"end\":41953,\"start\":41471},{\"end\":42495,\"start\":41968},{\"end\":42781,\"start\":42497},{\"end\":42963,\"start\":42783}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":24403,\"start\":24372},{\"attributes\":{\"id\":\"formula_1\"},\"end\":24903,\"start\":24833}]", "table_ref": "[{\"end\":29457,\"start\":29450},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":35137,\"start\":35130},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":35502,\"start\":35495},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":35856,\"start\":35849},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":37291,\"start\":37284}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2345,\"start\":2333},{\"attributes\":{\"n\":\"2\"},\"end\":8122,\"start\":8095},{\"attributes\":{\"n\":\"2.1\"},\"end\":8535,\"start\":8501},{\"attributes\":{\"n\":\"2.2\"},\"end\":13072,\"start\":13049},{\"attributes\":{\"n\":\"2.2.1\"},\"end\":13327,\"start\":13295},{\"attributes\":{\"n\":\"2.2.2\"},\"end\":15536,\"start\":15525},{\"attributes\":{\"n\":\"3\"},\"end\":17471,\"start\":17442},{\"attributes\":{\"n\":\"3.2\"},\"end\":18200,\"start\":18188},{\"attributes\":{\"n\":\"3.3\"},\"end\":21591,\"start\":21568},{\"attributes\":{\"n\":\"3.3.1\"},\"end\":23597,\"start\":23583},{\"end\":26023,\"start\":25995},{\"attributes\":{\"n\":\"3.4\"},\"end\":26387,\"start\":26358},{\"attributes\":{\"n\":\"4\"},\"end\":27285,\"start\":27274},{\"attributes\":{\"n\":\"4.1\"},\"end\":27579,\"start\":27571},{\"attributes\":{\"n\":\"4.2\"},\"end\":29526,\"start\":29510},{\"attributes\":{\"n\":\"4.3\"},\"end\":31073,\"start\":31055},{\"attributes\":{\"n\":\"4.4\"},\"end\":31658,\"start\":31622},{\"attributes\":{\"n\":\"4.4.1\"},\"end\":32472,\"start\":32460},{\"attributes\":{\"n\":\"4.4.2\"},\"end\":32910,\"start\":32905},{\"attributes\":{\"n\":\"4.4.3\"},\"end\":33769,\"start\":33759},{\"attributes\":{\"n\":\"4.5\"},\"end\":34377,\"start\":34358},{\"attributes\":{\"n\":\"4.5.1\"},\"end\":35169,\"start\":35141},{\"attributes\":{\"n\":\"4.5.2\"},\"end\":36721,\"start\":36694},{\"attributes\":{\"n\":\"4.5.3\"},\"end\":38166,\"start\":38142},{\"attributes\":{\"n\":\"4.5.4\"},\"end\":39699,\"start\":39688},{\"attributes\":{\"n\":\"5\"},\"end\":41966,\"start\":41956},{\"end\":42975,\"start\":42965},{\"end\":43087,\"start\":43077},{\"end\":43224,\"start\":43214},{\"end\":43390,\"start\":43380},{\"end\":43659,\"start\":43649},{\"end\":44044,\"start\":44034},{\"end\":44251,\"start\":44241},{\"end\":44485,\"start\":44474},{\"end\":45108,\"start\":45099}]", "table": "[{\"end\":44771,\"start\":44666},{\"end\":45097,\"start\":44969}]", "figure_caption": "[{\"end\":43075,\"start\":42977},{\"end\":43212,\"start\":43089},{\"end\":43378,\"start\":43226},{\"end\":43647,\"start\":43392},{\"end\":44032,\"start\":43661},{\"end\":44239,\"start\":44046},{\"end\":44472,\"start\":44253},{\"end\":44573,\"start\":44488},{\"end\":44666,\"start\":44576},{\"end\":44969,\"start\":44774},{\"end\":45626,\"start\":45110}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6729,\"start\":6721},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":19083,\"start\":19075},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":21336,\"start\":21328},{\"end\":21802,\"start\":21794},{\"end\":22296,\"start\":22288},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22900,\"start\":22892},{\"end\":25130,\"start\":25122},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":26202,\"start\":26188},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":26885,\"start\":26877},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":32351,\"start\":32343},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":34018,\"start\":34010},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":34341,\"start\":34333},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":36112,\"start\":36104},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":37834,\"start\":37825}]", "bib_author_first_name": "[{\"end\":46006,\"start\":46005},{\"end\":46030,\"start\":46024},{\"end\":46506,\"start\":46499},{\"end\":46528,\"start\":46523},{\"end\":46530,\"start\":46529},{\"end\":46789,\"start\":46782},{\"end\":46811,\"start\":46806},{\"end\":46813,\"start\":46812},{\"end\":47118,\"start\":47111},{\"end\":47125,\"start\":47124},{\"end\":47141,\"start\":47134},{\"end\":47430,\"start\":47422},{\"end\":47444,\"start\":47443},{\"end\":47708,\"start\":47702},{\"end\":47726,\"start\":47721},{\"end\":47742,\"start\":47737},{\"end\":47759,\"start\":47753},{\"end\":47777,\"start\":47771},{\"end\":48055,\"start\":48046},{\"end\":48073,\"start\":48063},{\"end\":48501,\"start\":48494},{\"end\":48520,\"start\":48515},{\"end\":48535,\"start\":48528},{\"end\":48554,\"start\":48547},{\"end\":48586,\"start\":48582},{\"end\":48602,\"start\":48596},{\"end\":49026,\"start\":49022},{\"end\":49038,\"start\":49033},{\"end\":49058,\"start\":49050},{\"end\":49076,\"start\":49068},{\"end\":49340,\"start\":49335},{\"end\":49353,\"start\":49349},{\"end\":49367,\"start\":49363},{\"end\":49808,\"start\":49801},{\"end\":49826,\"start\":49818},{\"end\":49835,\"start\":49834},{\"end\":49851,\"start\":49843},{\"end\":50493,\"start\":50486},{\"end\":50509,\"start\":50503},{\"end\":50511,\"start\":50510},{\"end\":50523,\"start\":50519},{\"end\":50536,\"start\":50531},{\"end\":50997,\"start\":50995},{\"end\":51013,\"start\":51004},{\"end\":51430,\"start\":51425},{\"end\":51440,\"start\":51437},{\"end\":51452,\"start\":51447},{\"end\":51460,\"start\":51457},{\"end\":51894,\"start\":51891},{\"end\":51908,\"start\":51902},{\"end\":52359,\"start\":52350},{\"end\":52379,\"start\":52372},{\"end\":52398,\"start\":52392},{\"end\":52755,\"start\":52745},{\"end\":52784,\"start\":52767},{\"end\":52795,\"start\":52790},{\"end\":52805,\"start\":52802},{\"end\":52816,\"start\":52812},{\"end\":52832,\"start\":52826},{\"end\":52849,\"start\":52843},{\"end\":52859,\"start\":52858},{\"end\":52873,\"start\":52867},{\"end\":53323,\"start\":53316},{\"end\":53337,\"start\":53333},{\"end\":53838,\"start\":53833},{\"end\":53852,\"start\":53847},{\"end\":53869,\"start\":53862},{\"end\":54109,\"start\":54108},{\"end\":54126,\"start\":54119},{\"end\":54142,\"start\":54135},{\"end\":54145,\"start\":54143},{\"end\":54159,\"start\":54156},{\"end\":54170,\"start\":54166},{\"end\":54642,\"start\":54638},{\"end\":54654,\"start\":54650},{\"end\":54664,\"start\":54661},{\"end\":54675,\"start\":54671},{\"end\":55202,\"start\":55188},{\"end\":55213,\"start\":55210},{\"end\":55218,\"start\":55214},{\"end\":55231,\"start\":55224},{\"end\":55246,\"start\":55240},{\"end\":55607,\"start\":55603},{\"end\":55623,\"start\":55618},{\"end\":55637,\"start\":55632},{\"end\":55651,\"start\":55647},{\"end\":55666,\"start\":55661},{\"end\":56057,\"start\":56052},{\"end\":56071,\"start\":56064},{\"end\":56085,\"start\":56080},{\"end\":56098,\"start\":56092},{\"end\":56118,\"start\":56108},{\"end\":56501,\"start\":56500},{\"end\":56503,\"start\":56502},{\"end\":56518,\"start\":56511},{\"end\":56528,\"start\":56524},{\"end\":56551,\"start\":56543},{\"end\":56560,\"start\":56556},{\"end\":56931,\"start\":56924},{\"end\":56945,\"start\":56940},{\"end\":56963,\"start\":56956},{\"end\":57312,\"start\":57304},{\"end\":57327,\"start\":57320},{\"end\":57926,\"start\":57922},{\"end\":57938,\"start\":57932},{\"end\":57952,\"start\":57947},{\"end\":57969,\"start\":57962},{\"end\":58297,\"start\":58290},{\"end\":58307,\"start\":58302},{\"end\":58585,\"start\":58579},{\"end\":58597,\"start\":58592},{\"end\":58949,\"start\":58945},{\"end\":58963,\"start\":58958},{\"end\":58974,\"start\":58970},{\"end\":58993,\"start\":58987},{\"end\":59247,\"start\":59242},{\"end\":59268,\"start\":59262},{\"end\":59284,\"start\":59279},{\"end\":59295,\"start\":59290},{\"end\":59311,\"start\":59306},{\"end\":59325,\"start\":59321},{\"end\":59337,\"start\":59334},{\"end\":59358,\"start\":59352},{\"end\":59372,\"start\":59367},{\"end\":59702,\"start\":59697},{\"end\":59722,\"start\":59717},{\"end\":59732,\"start\":59727},{\"end\":60071,\"start\":60067},{\"end\":60085,\"start\":60079},{\"end\":60098,\"start\":60091},{\"end\":60414,\"start\":60410},{\"end\":60423,\"start\":60422},{\"end\":60437,\"start\":60431},{\"end\":60448,\"start\":60445},{\"end\":60461,\"start\":60460},{\"end\":60475,\"start\":60468},{\"end\":61026,\"start\":61021},{\"end\":61039,\"start\":61034},{\"end\":61055,\"start\":61050},{\"end\":61538,\"start\":61537},{\"end\":61547,\"start\":61546},{\"end\":61549,\"start\":61548},{\"end\":61558,\"start\":61557},{\"end\":61570,\"start\":61569},{\"end\":61884,\"start\":61877},{\"end\":61900,\"start\":61894},{\"end\":61920,\"start\":61915},{\"end\":62383,\"start\":62375},{\"end\":62399,\"start\":62392},{\"end\":62412,\"start\":62407},{\"end\":62419,\"start\":62413},{\"end\":62431,\"start\":62427},{\"end\":62748,\"start\":62743},{\"end\":62767,\"start\":62761},{\"end\":62780,\"start\":62776},{\"end\":62796,\"start\":62789},{\"end\":62813,\"start\":62807},{\"end\":62827,\"start\":62819},{\"end\":63305,\"start\":63299},{\"end\":63585,\"start\":63578},{\"end\":63601,\"start\":63594},{\"end\":63616,\"start\":63609},{\"end\":63861,\"start\":63854},{\"end\":63872,\"start\":63868},{\"end\":63883,\"start\":63878},{\"end\":63901,\"start\":63893},{\"end\":63912,\"start\":63906},{\"end\":63928,\"start\":63922},{\"end\":64391,\"start\":64385},{\"end\":64406,\"start\":64398},{\"end\":64932,\"start\":64927},{\"end\":64942,\"start\":64938},{\"end\":64953,\"start\":64949},{\"end\":64958,\"start\":64954},{\"end\":64975,\"start\":64967},{\"end\":64986,\"start\":64983},{\"end\":64999,\"start\":64992},{\"end\":65013,\"start\":65006},{\"end\":65028,\"start\":65020},{\"end\":65404,\"start\":65403},{\"end\":65423,\"start\":65413},{\"end\":65713,\"start\":65712},{\"end\":65727,\"start\":65718},{\"end\":65729,\"start\":65728},{\"end\":65747,\"start\":65740},{\"end\":66099,\"start\":66094},{\"end\":66110,\"start\":66104},{\"end\":66122,\"start\":66116},{\"end\":66139,\"start\":66132},{\"end\":66441,\"start\":66435},{\"end\":66447,\"start\":66446},{\"end\":66918,\"start\":66911},{\"end\":66930,\"start\":66924},{\"end\":66945,\"start\":66940},{\"end\":66961,\"start\":66955},{\"end\":67524,\"start\":67518},{\"end\":67540,\"start\":67533},{\"end\":67556,\"start\":67549},{\"end\":67570,\"start\":67563},{\"end\":68082,\"start\":68077},{\"end\":68102,\"start\":68094},{\"end\":68644,\"start\":68636},{\"end\":68663,\"start\":68656},{\"end\":68902,\"start\":68896},{\"end\":68914,\"start\":68907},{\"end\":69228,\"start\":69220},{\"end\":69241,\"start\":69237},{\"end\":69254,\"start\":69249},{\"end\":69270,\"start\":69265},{\"end\":69287,\"start\":69282},{\"end\":69289,\"start\":69288},{\"end\":69308,\"start\":69297},{\"end\":69325,\"start\":69317},{\"end\":69743,\"start\":69741},{\"end\":69762,\"start\":69751}]", "bib_author_last_name": "[{\"end\":46022,\"start\":46007},{\"end\":46036,\"start\":46031},{\"end\":46044,\"start\":46038},{\"end\":46521,\"start\":46507},{\"end\":46535,\"start\":46531},{\"end\":46804,\"start\":46790},{\"end\":46818,\"start\":46814},{\"end\":47122,\"start\":47119},{\"end\":47132,\"start\":47126},{\"end\":47148,\"start\":47142},{\"end\":47441,\"start\":47431},{\"end\":47449,\"start\":47445},{\"end\":47460,\"start\":47451},{\"end\":47719,\"start\":47709},{\"end\":47735,\"start\":47727},{\"end\":47751,\"start\":47743},{\"end\":47769,\"start\":47760},{\"end\":47784,\"start\":47778},{\"end\":48061,\"start\":48056},{\"end\":48076,\"start\":48074},{\"end\":48513,\"start\":48502},{\"end\":48526,\"start\":48521},{\"end\":48545,\"start\":48536},{\"end\":48580,\"start\":48555},{\"end\":48594,\"start\":48587},{\"end\":48615,\"start\":48603},{\"end\":48623,\"start\":48617},{\"end\":49031,\"start\":49027},{\"end\":49048,\"start\":49039},{\"end\":49066,\"start\":49059},{\"end\":49083,\"start\":49077},{\"end\":49347,\"start\":49341},{\"end\":49361,\"start\":49354},{\"end\":49373,\"start\":49368},{\"end\":49816,\"start\":49809},{\"end\":49832,\"start\":49827},{\"end\":49841,\"start\":49836},{\"end\":49857,\"start\":49852},{\"end\":49864,\"start\":49859},{\"end\":50501,\"start\":50494},{\"end\":50517,\"start\":50512},{\"end\":50529,\"start\":50524},{\"end\":50542,\"start\":50537},{\"end\":51002,\"start\":50998},{\"end\":51016,\"start\":51014},{\"end\":51435,\"start\":51431},{\"end\":51445,\"start\":51441},{\"end\":51455,\"start\":51453},{\"end\":51465,\"start\":51461},{\"end\":51900,\"start\":51895},{\"end\":51914,\"start\":51909},{\"end\":51920,\"start\":51916},{\"end\":52370,\"start\":52360},{\"end\":52390,\"start\":52380},{\"end\":52404,\"start\":52399},{\"end\":52765,\"start\":52756},{\"end\":52788,\"start\":52785},{\"end\":52800,\"start\":52796},{\"end\":52810,\"start\":52806},{\"end\":52824,\"start\":52817},{\"end\":52841,\"start\":52833},{\"end\":52856,\"start\":52850},{\"end\":52865,\"start\":52860},{\"end\":52881,\"start\":52874},{\"end\":52888,\"start\":52883},{\"end\":53331,\"start\":53324},{\"end\":53347,\"start\":53338},{\"end\":53845,\"start\":53839},{\"end\":53860,\"start\":53853},{\"end\":53874,\"start\":53870},{\"end\":54117,\"start\":54110},{\"end\":54133,\"start\":54127},{\"end\":54154,\"start\":54146},{\"end\":54164,\"start\":54160},{\"end\":54177,\"start\":54171},{\"end\":54186,\"start\":54179},{\"end\":54194,\"start\":54188},{\"end\":54648,\"start\":54643},{\"end\":54659,\"start\":54655},{\"end\":54669,\"start\":54665},{\"end\":54681,\"start\":54676},{\"end\":55208,\"start\":55203},{\"end\":55222,\"start\":55219},{\"end\":55238,\"start\":55232},{\"end\":55252,\"start\":55247},{\"end\":55616,\"start\":55608},{\"end\":55630,\"start\":55624},{\"end\":55645,\"start\":55638},{\"end\":55659,\"start\":55652},{\"end\":55673,\"start\":55667},{\"end\":56062,\"start\":56058},{\"end\":56078,\"start\":56072},{\"end\":56090,\"start\":56086},{\"end\":56106,\"start\":56099},{\"end\":56125,\"start\":56119},{\"end\":56133,\"start\":56127},{\"end\":56509,\"start\":56504},{\"end\":56522,\"start\":56519},{\"end\":56541,\"start\":56529},{\"end\":56554,\"start\":56552},{\"end\":56571,\"start\":56561},{\"end\":56578,\"start\":56573},{\"end\":56938,\"start\":56932},{\"end\":56954,\"start\":56946},{\"end\":56973,\"start\":56964},{\"end\":57318,\"start\":57313},{\"end\":57335,\"start\":57328},{\"end\":57342,\"start\":57337},{\"end\":57930,\"start\":57927},{\"end\":57945,\"start\":57939},{\"end\":57960,\"start\":57953},{\"end\":57978,\"start\":57970},{\"end\":58300,\"start\":58298},{\"end\":58313,\"start\":58308},{\"end\":58590,\"start\":58586},{\"end\":58609,\"start\":58598},{\"end\":58956,\"start\":58950},{\"end\":58968,\"start\":58964},{\"end\":58985,\"start\":58975},{\"end\":59000,\"start\":58994},{\"end\":59260,\"start\":59248},{\"end\":59277,\"start\":59269},{\"end\":59288,\"start\":59285},{\"end\":59304,\"start\":59296},{\"end\":59319,\"start\":59312},{\"end\":59332,\"start\":59326},{\"end\":59350,\"start\":59338},{\"end\":59365,\"start\":59359},{\"end\":59384,\"start\":59373},{\"end\":59715,\"start\":59703},{\"end\":59725,\"start\":59723},{\"end\":59740,\"start\":59733},{\"end\":60077,\"start\":60072},{\"end\":60089,\"start\":60086},{\"end\":60104,\"start\":60099},{\"end\":60420,\"start\":60415},{\"end\":60429,\"start\":60424},{\"end\":60443,\"start\":60438},{\"end\":60452,\"start\":60449},{\"end\":60458,\"start\":60454},{\"end\":60466,\"start\":60462},{\"end\":60481,\"start\":60476},{\"end\":60490,\"start\":60483},{\"end\":61032,\"start\":61027},{\"end\":61048,\"start\":61040},{\"end\":61063,\"start\":61056},{\"end\":61544,\"start\":61539},{\"end\":61555,\"start\":61550},{\"end\":61567,\"start\":61559},{\"end\":61578,\"start\":61571},{\"end\":61892,\"start\":61885},{\"end\":61913,\"start\":61901},{\"end\":61928,\"start\":61921},{\"end\":62390,\"start\":62384},{\"end\":62405,\"start\":62400},{\"end\":62425,\"start\":62420},{\"end\":62440,\"start\":62432},{\"end\":62449,\"start\":62442},{\"end\":62759,\"start\":62749},{\"end\":62774,\"start\":62768},{\"end\":62787,\"start\":62781},{\"end\":62805,\"start\":62797},{\"end\":62817,\"start\":62814},{\"end\":62841,\"start\":62828},{\"end\":63310,\"start\":63306},{\"end\":63592,\"start\":63586},{\"end\":63607,\"start\":63602},{\"end\":63624,\"start\":63617},{\"end\":63866,\"start\":63862},{\"end\":63876,\"start\":63873},{\"end\":63891,\"start\":63884},{\"end\":63904,\"start\":63902},{\"end\":63920,\"start\":63913},{\"end\":63938,\"start\":63929},{\"end\":64396,\"start\":64392},{\"end\":64412,\"start\":64407},{\"end\":64936,\"start\":64933},{\"end\":64947,\"start\":64943},{\"end\":64965,\"start\":64959},{\"end\":64981,\"start\":64976},{\"end\":64990,\"start\":64987},{\"end\":65004,\"start\":65000},{\"end\":65018,\"start\":65014},{\"end\":65036,\"start\":65029},{\"end\":65411,\"start\":65405},{\"end\":65434,\"start\":65424},{\"end\":65440,\"start\":65436},{\"end\":65710,\"start\":65701},{\"end\":65716,\"start\":65714},{\"end\":65738,\"start\":65730},{\"end\":65753,\"start\":65748},{\"end\":65765,\"start\":65755},{\"end\":66102,\"start\":66100},{\"end\":66114,\"start\":66111},{\"end\":66130,\"start\":66123},{\"end\":66154,\"start\":66140},{\"end\":66160,\"start\":66156},{\"end\":66444,\"start\":66442},{\"end\":66454,\"start\":66448},{\"end\":66461,\"start\":66456},{\"end\":66922,\"start\":66919},{\"end\":66938,\"start\":66931},{\"end\":66953,\"start\":66946},{\"end\":66969,\"start\":66962},{\"end\":67531,\"start\":67525},{\"end\":67547,\"start\":67541},{\"end\":67561,\"start\":67557},{\"end\":67577,\"start\":67571},{\"end\":68092,\"start\":68083},{\"end\":68111,\"start\":68103},{\"end\":68654,\"start\":68645},{\"end\":68672,\"start\":68664},{\"end\":68905,\"start\":68903},{\"end\":68921,\"start\":68915},{\"end\":69235,\"start\":69229},{\"end\":69247,\"start\":69242},{\"end\":69263,\"start\":69255},{\"end\":69280,\"start\":69271},{\"end\":69295,\"start\":69290},{\"end\":69315,\"start\":69309},{\"end\":69339,\"start\":69326},{\"end\":69749,\"start\":69744},{\"end\":69770,\"start\":69763}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":53281392},\"end\":46402,\"start\":45931},{\"attributes\":{\"id\":\"b1\"},\"end\":46710,\"start\":46404},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":59616366},\"end\":47014,\"start\":46712},{\"attributes\":{\"doi\":\"arXiv:1803.01271\",\"id\":\"b3\"},\"end\":47367,\"start\":47016},{\"attributes\":{\"id\":\"b4\"},\"end\":47590,\"start\":47369},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":16236833},\"end\":48044,\"start\":47592},{\"attributes\":{\"doi\":\"arXiv:1901.06077\",\"id\":\"b6\"},\"end\":48397,\"start\":48046},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":16372911},\"end\":48949,\"start\":48399},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":211096730},\"end\":49247,\"start\":48951},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":5555257},\"end\":49720,\"start\":49249},{\"attributes\":{\"id\":\"b10\"},\"end\":50383,\"start\":49722},{\"attributes\":{\"doi\":\"10.1145/3411832\",\"id\":\"b11\",\"matched_paper_id\":221083378},\"end\":50914,\"start\":50385},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":4401064},\"end\":51362,\"start\":50916},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":196591462},\"end\":51813,\"start\":51364},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":67877065},\"end\":52272,\"start\":51815},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":59413908},\"end\":52663,\"start\":52274},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":52824600},\"end\":53220,\"start\":52665},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":15816723},\"end\":53773,\"start\":53222},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":7433701},\"end\":54104,\"start\":53775},{\"attributes\":{\"id\":\"b19\"},\"end\":54506,\"start\":54106},{\"attributes\":{\"id\":\"b20\"},\"end\":55132,\"start\":54508},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":44671515},\"end\":55557,\"start\":55134},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":202640334},\"end\":55982,\"start\":55559},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":44146161},\"end\":56367,\"start\":55984},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":9993944},\"end\":56881,\"start\":56369},{\"attributes\":{\"id\":\"b25\"},\"end\":57207,\"start\":56883},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":6985598},\"end\":57840,\"start\":57209},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":204171915},\"end\":58182,\"start\":57842},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":220071750},\"end\":58505,\"start\":58184},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":14992849},\"end\":58841,\"start\":58507},{\"attributes\":{\"doi\":\"arXiv:2005.02463\",\"id\":\"b30\"},\"end\":59240,\"start\":58843},{\"attributes\":{\"doi\":\"arXiv:1609.03499\",\"id\":\"b31\"},\"end\":59695,\"start\":59242},{\"attributes\":{\"doi\":\"arXiv:1807.03748\",\"id\":\"b32\"},\"end\":59986,\"start\":59697},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":1241495},\"end\":60317,\"start\":59988},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":216768875},\"end\":60953,\"start\":60319},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":195357258},\"end\":61441,\"start\":60955},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":220793490},\"end\":61809,\"start\":61443},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":206592766},\"end\":62290,\"start\":61811},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":2112296},\"end\":62668,\"start\":62292},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":12646079},\"end\":63227,\"start\":62670},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":911406},\"end\":63516,\"start\":63229},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":232155578},\"end\":63809,\"start\":63518},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":72940786},\"end\":64312,\"start\":63811},{\"attributes\":{\"doi\":\"10.1145/3287071\",\"id\":\"b43\",\"matched_paper_id\":57253669},\"end\":64873,\"start\":64314},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":54059625},\"end\":65326,\"start\":64875},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":47325215},\"end\":65654,\"start\":65328},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":24718057},\"end\":66092,\"start\":65656},{\"attributes\":{\"doi\":\"arXiv:1901.10430\",\"id\":\"b47\"},\"end\":66433,\"start\":66094},{\"attributes\":{\"doi\":\"arXiv:2009.13807\",\"id\":\"b48\"},\"end\":66774,\"start\":66435},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":221499291},\"end\":67432,\"start\":66776},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":504577},\"end\":67975,\"start\":67434},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":207674406},\"end\":68558,\"start\":67977},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":14818935},\"end\":68894,\"start\":68560},{\"attributes\":{\"doi\":\"arXiv:1511.07122\",\"id\":\"b53\"},\"end\":69142,\"start\":68896},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":181624250},\"end\":69645,\"start\":69144},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":6870922},\"end\":70159,\"start\":69647}]", "bib_title": "[{\"end\":46003,\"start\":45931},{\"end\":46780,\"start\":46712},{\"end\":47700,\"start\":47592},{\"end\":48492,\"start\":48399},{\"end\":49020,\"start\":48951},{\"end\":49333,\"start\":49249},{\"end\":49799,\"start\":49722},{\"end\":50484,\"start\":50385},{\"end\":50993,\"start\":50916},{\"end\":51423,\"start\":51364},{\"end\":51889,\"start\":51815},{\"end\":52348,\"start\":52274},{\"end\":52743,\"start\":52665},{\"end\":53314,\"start\":53222},{\"end\":53831,\"start\":53775},{\"end\":54636,\"start\":54508},{\"end\":55186,\"start\":55134},{\"end\":55601,\"start\":55559},{\"end\":56050,\"start\":55984},{\"end\":56498,\"start\":56369},{\"end\":57302,\"start\":57209},{\"end\":57920,\"start\":57842},{\"end\":58288,\"start\":58184},{\"end\":58577,\"start\":58507},{\"end\":60065,\"start\":59988},{\"end\":60408,\"start\":60319},{\"end\":61019,\"start\":60955},{\"end\":61535,\"start\":61443},{\"end\":61875,\"start\":61811},{\"end\":62373,\"start\":62292},{\"end\":62741,\"start\":62670},{\"end\":63297,\"start\":63229},{\"end\":63576,\"start\":63518},{\"end\":63852,\"start\":63811},{\"end\":64383,\"start\":64314},{\"end\":64925,\"start\":64875},{\"end\":65401,\"start\":65328},{\"end\":65699,\"start\":65656},{\"end\":66909,\"start\":66776},{\"end\":67516,\"start\":67434},{\"end\":68075,\"start\":67977},{\"end\":68634,\"start\":68560},{\"end\":69218,\"start\":69144},{\"end\":69739,\"start\":69647}]", "bib_author": "[{\"end\":46024,\"start\":46005},{\"end\":46038,\"start\":46024},{\"end\":46046,\"start\":46038},{\"end\":46523,\"start\":46499},{\"end\":46537,\"start\":46523},{\"end\":46806,\"start\":46782},{\"end\":46820,\"start\":46806},{\"end\":47124,\"start\":47111},{\"end\":47134,\"start\":47124},{\"end\":47150,\"start\":47134},{\"end\":47443,\"start\":47422},{\"end\":47451,\"start\":47443},{\"end\":47462,\"start\":47451},{\"end\":47721,\"start\":47702},{\"end\":47737,\"start\":47721},{\"end\":47753,\"start\":47737},{\"end\":47771,\"start\":47753},{\"end\":47786,\"start\":47771},{\"end\":48063,\"start\":48046},{\"end\":48078,\"start\":48063},{\"end\":48515,\"start\":48494},{\"end\":48528,\"start\":48515},{\"end\":48547,\"start\":48528},{\"end\":48582,\"start\":48547},{\"end\":48596,\"start\":48582},{\"end\":48617,\"start\":48596},{\"end\":48625,\"start\":48617},{\"end\":49033,\"start\":49022},{\"end\":49050,\"start\":49033},{\"end\":49068,\"start\":49050},{\"end\":49085,\"start\":49068},{\"end\":49349,\"start\":49335},{\"end\":49363,\"start\":49349},{\"end\":49375,\"start\":49363},{\"end\":49818,\"start\":49801},{\"end\":49834,\"start\":49818},{\"end\":49843,\"start\":49834},{\"end\":49859,\"start\":49843},{\"end\":49866,\"start\":49859},{\"end\":50503,\"start\":50486},{\"end\":50519,\"start\":50503},{\"end\":50531,\"start\":50519},{\"end\":50544,\"start\":50531},{\"end\":51004,\"start\":50995},{\"end\":51018,\"start\":51004},{\"end\":51437,\"start\":51425},{\"end\":51447,\"start\":51437},{\"end\":51457,\"start\":51447},{\"end\":51467,\"start\":51457},{\"end\":51902,\"start\":51891},{\"end\":51916,\"start\":51902},{\"end\":51922,\"start\":51916},{\"end\":52372,\"start\":52350},{\"end\":52392,\"start\":52372},{\"end\":52406,\"start\":52392},{\"end\":52767,\"start\":52745},{\"end\":52790,\"start\":52767},{\"end\":52802,\"start\":52790},{\"end\":52812,\"start\":52802},{\"end\":52826,\"start\":52812},{\"end\":52843,\"start\":52826},{\"end\":52858,\"start\":52843},{\"end\":52867,\"start\":52858},{\"end\":52883,\"start\":52867},{\"end\":52890,\"start\":52883},{\"end\":53333,\"start\":53316},{\"end\":53349,\"start\":53333},{\"end\":53847,\"start\":53833},{\"end\":53862,\"start\":53847},{\"end\":53876,\"start\":53862},{\"end\":54119,\"start\":54108},{\"end\":54135,\"start\":54119},{\"end\":54156,\"start\":54135},{\"end\":54166,\"start\":54156},{\"end\":54179,\"start\":54166},{\"end\":54188,\"start\":54179},{\"end\":54196,\"start\":54188},{\"end\":54650,\"start\":54638},{\"end\":54661,\"start\":54650},{\"end\":54671,\"start\":54661},{\"end\":54683,\"start\":54671},{\"end\":55210,\"start\":55188},{\"end\":55224,\"start\":55210},{\"end\":55240,\"start\":55224},{\"end\":55254,\"start\":55240},{\"end\":55618,\"start\":55603},{\"end\":55632,\"start\":55618},{\"end\":55647,\"start\":55632},{\"end\":55661,\"start\":55647},{\"end\":55675,\"start\":55661},{\"end\":56064,\"start\":56052},{\"end\":56080,\"start\":56064},{\"end\":56092,\"start\":56080},{\"end\":56108,\"start\":56092},{\"end\":56127,\"start\":56108},{\"end\":56135,\"start\":56127},{\"end\":56511,\"start\":56500},{\"end\":56524,\"start\":56511},{\"end\":56543,\"start\":56524},{\"end\":56556,\"start\":56543},{\"end\":56573,\"start\":56556},{\"end\":56580,\"start\":56573},{\"end\":56940,\"start\":56924},{\"end\":56956,\"start\":56940},{\"end\":56975,\"start\":56956},{\"end\":57320,\"start\":57304},{\"end\":57337,\"start\":57320},{\"end\":57344,\"start\":57337},{\"end\":57932,\"start\":57922},{\"end\":57947,\"start\":57932},{\"end\":57962,\"start\":57947},{\"end\":57980,\"start\":57962},{\"end\":58302,\"start\":58290},{\"end\":58315,\"start\":58302},{\"end\":58592,\"start\":58579},{\"end\":58611,\"start\":58592},{\"end\":58958,\"start\":58945},{\"end\":58970,\"start\":58958},{\"end\":58987,\"start\":58970},{\"end\":59002,\"start\":58987},{\"end\":59262,\"start\":59242},{\"end\":59279,\"start\":59262},{\"end\":59290,\"start\":59279},{\"end\":59306,\"start\":59290},{\"end\":59321,\"start\":59306},{\"end\":59334,\"start\":59321},{\"end\":59352,\"start\":59334},{\"end\":59367,\"start\":59352},{\"end\":59386,\"start\":59367},{\"end\":59717,\"start\":59697},{\"end\":59727,\"start\":59717},{\"end\":59742,\"start\":59727},{\"end\":60079,\"start\":60067},{\"end\":60091,\"start\":60079},{\"end\":60106,\"start\":60091},{\"end\":60422,\"start\":60410},{\"end\":60431,\"start\":60422},{\"end\":60445,\"start\":60431},{\"end\":60454,\"start\":60445},{\"end\":60460,\"start\":60454},{\"end\":60468,\"start\":60460},{\"end\":60483,\"start\":60468},{\"end\":60492,\"start\":60483},{\"end\":61034,\"start\":61021},{\"end\":61050,\"start\":61034},{\"end\":61065,\"start\":61050},{\"end\":61546,\"start\":61537},{\"end\":61557,\"start\":61546},{\"end\":61569,\"start\":61557},{\"end\":61580,\"start\":61569},{\"end\":61894,\"start\":61877},{\"end\":61915,\"start\":61894},{\"end\":61930,\"start\":61915},{\"end\":62392,\"start\":62375},{\"end\":62407,\"start\":62392},{\"end\":62427,\"start\":62407},{\"end\":62442,\"start\":62427},{\"end\":62451,\"start\":62442},{\"end\":62761,\"start\":62743},{\"end\":62776,\"start\":62761},{\"end\":62789,\"start\":62776},{\"end\":62807,\"start\":62789},{\"end\":62819,\"start\":62807},{\"end\":62843,\"start\":62819},{\"end\":63312,\"start\":63299},{\"end\":63594,\"start\":63578},{\"end\":63609,\"start\":63594},{\"end\":63626,\"start\":63609},{\"end\":63868,\"start\":63854},{\"end\":63878,\"start\":63868},{\"end\":63893,\"start\":63878},{\"end\":63906,\"start\":63893},{\"end\":63922,\"start\":63906},{\"end\":63940,\"start\":63922},{\"end\":64398,\"start\":64385},{\"end\":64414,\"start\":64398},{\"end\":64938,\"start\":64927},{\"end\":64949,\"start\":64938},{\"end\":64967,\"start\":64949},{\"end\":64983,\"start\":64967},{\"end\":64992,\"start\":64983},{\"end\":65006,\"start\":64992},{\"end\":65020,\"start\":65006},{\"end\":65038,\"start\":65020},{\"end\":65413,\"start\":65403},{\"end\":65436,\"start\":65413},{\"end\":65442,\"start\":65436},{\"end\":65712,\"start\":65701},{\"end\":65718,\"start\":65712},{\"end\":65740,\"start\":65718},{\"end\":65755,\"start\":65740},{\"end\":65767,\"start\":65755},{\"end\":66104,\"start\":66094},{\"end\":66116,\"start\":66104},{\"end\":66132,\"start\":66116},{\"end\":66156,\"start\":66132},{\"end\":66162,\"start\":66156},{\"end\":66446,\"start\":66435},{\"end\":66456,\"start\":66446},{\"end\":66463,\"start\":66456},{\"end\":66924,\"start\":66911},{\"end\":66940,\"start\":66924},{\"end\":66955,\"start\":66940},{\"end\":66971,\"start\":66955},{\"end\":67533,\"start\":67518},{\"end\":67549,\"start\":67533},{\"end\":67563,\"start\":67549},{\"end\":67579,\"start\":67563},{\"end\":68094,\"start\":68077},{\"end\":68113,\"start\":68094},{\"end\":68656,\"start\":68636},{\"end\":68674,\"start\":68656},{\"end\":68907,\"start\":68896},{\"end\":68923,\"start\":68907},{\"end\":69237,\"start\":69220},{\"end\":69249,\"start\":69237},{\"end\":69265,\"start\":69249},{\"end\":69282,\"start\":69265},{\"end\":69297,\"start\":69282},{\"end\":69317,\"start\":69297},{\"end\":69341,\"start\":69317},{\"end\":69751,\"start\":69741},{\"end\":69772,\"start\":69751}]", "bib_venue": "[{\"end\":46123,\"start\":46046},{\"end\":46497,\"start\":46404},{\"end\":46850,\"start\":46820},{\"end\":47109,\"start\":47016},{\"end\":47420,\"start\":47369},{\"end\":47800,\"start\":47786},{\"end\":48201,\"start\":48094},{\"end\":48652,\"start\":48625},{\"end\":49089,\"start\":49085},{\"end\":49460,\"start\":49375},{\"end\":49985,\"start\":49866},{\"end\":50611,\"start\":50559},{\"end\":51095,\"start\":51018},{\"end\":51544,\"start\":51467},{\"end\":51999,\"start\":51922},{\"end\":52455,\"start\":52406},{\"end\":52925,\"start\":52890},{\"end\":53445,\"start\":53349},{\"end\":53920,\"start\":53876},{\"end\":54766,\"start\":54683},{\"end\":55324,\"start\":55254},{\"end\":55749,\"start\":55675},{\"end\":56160,\"start\":56135},{\"end\":56606,\"start\":56580},{\"end\":56922,\"start\":56883},{\"end\":57461,\"start\":57344},{\"end\":57995,\"start\":57980},{\"end\":58326,\"start\":58315},{\"end\":58660,\"start\":58611},{\"end\":58943,\"start\":58843},{\"end\":59443,\"start\":59402},{\"end\":59816,\"start\":59758},{\"end\":60136,\"start\":60106},{\"end\":60583,\"start\":60492},{\"end\":61148,\"start\":61065},{\"end\":61611,\"start\":61580},{\"end\":62007,\"start\":61930},{\"end\":62458,\"start\":62451},{\"end\":62910,\"start\":62843},{\"end\":63361,\"start\":63312},{\"end\":63643,\"start\":63626},{\"end\":64017,\"start\":63940},{\"end\":64512,\"start\":64429},{\"end\":65087,\"start\":65038},{\"end\":65478,\"start\":65442},{\"end\":65834,\"start\":65767},{\"end\":66238,\"start\":66178},{\"end\":66580,\"start\":66479},{\"end\":67054,\"start\":66971},{\"end\":67659,\"start\":67579},{\"end\":68213,\"start\":68113},{\"end\":68710,\"start\":68674},{\"end\":68994,\"start\":68939},{\"end\":69374,\"start\":69341},{\"end\":69834,\"start\":69772},{\"end\":46187,\"start\":46125},{\"end\":50091,\"start\":49987},{\"end\":51159,\"start\":51097},{\"end\":51608,\"start\":51546},{\"end\":52063,\"start\":52001},{\"end\":53528,\"start\":53447},{\"end\":54836,\"start\":54768},{\"end\":57565,\"start\":57463},{\"end\":60661,\"start\":60585},{\"end\":61218,\"start\":61150},{\"end\":62071,\"start\":62009},{\"end\":62964,\"start\":62912},{\"end\":64081,\"start\":64019},{\"end\":64582,\"start\":64514},{\"end\":65888,\"start\":65836},{\"end\":67124,\"start\":67056},{\"end\":67735,\"start\":67661},{\"end\":68300,\"start\":68215},{\"end\":69907,\"start\":69836}]"}}}, "year": 2023, "month": 12, "day": 17}