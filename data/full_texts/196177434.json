{"id": 196177434, "updated": "2022-02-06 04:36:56.119", "metadata": {"title": "Sets2Sets: Learning from Sequential Sets with Neural Networks", "authors": "[{\"middle\":[],\"last\":\"Hu\",\"first\":\"Haoji\"},{\"middle\":[],\"last\":\"He\",\"first\":\"Xiangnan\"}]", "venue": null, "journal": "Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining", "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "Given past sequential sets of elements, predicting the subsequent sets of elements is an important problem in different domains. With the past orders of customers given, predicting the items that are likely to be bought in their following orders can provide information about the future purchase intentions. With the past clinical records of patients at each visit to the hospitals given, predicting the future clinical records in the subsequent visits can provide information about the future disease progression. These useful information can help to make better decisions in different domains. However, existing methods have not studied this problem well. In this paper, we formulate this problem as a sequential sets to sequential sets learning problem. We propose an end-to-end learning approach based on an encoder-decoder framework to solve the problem. In the encoder, our approach maps the set of elements at each past time step into a vector. In the decoder, our method decodes the set of elements at each subsequent time step from the vectors with a set-based attention mechanism. The repeated elements pattern is also considered in our method to further improve the performance. In addition, our objective function addresses the imbalance and correlation existing among the predicted elements. The experimental results on three real-world data sets showthat our method outperforms the best performance of the compared methods with respect to recall and person-wise hit ratio by 2.7-20.6% and 2.1-26.3%, respectively. Our analysis also shows that our decoder has good generalization to output sequential sets that are even longer than the output of training instances.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2951227353", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/kdd/HuH19", "doi": "10.1145/3292500.3330979"}}, "content": {"source": {"pdf_hash": "58786e54bfc6ba03bb14db01bd9bf3ecbdbae866", "pdf_src": "ACM", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://dl.acm.org/doi/pdf/10.1145/3292500.3330979", "status": "BRONZE"}}, "grobid": {"id": "b51c1ac8972b7915a11353767a796058b6507f7a", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/58786e54bfc6ba03bb14db01bd9bf3ecbdbae866.txt", "contents": "\nSets2Sets: Learning from Sequential Sets with Neural Networks\n2019. August 4-8, 2019. August 4-8, 2019\n\nHaoji Hu \nXiangnan He \nSets2Sets: Learning from Sequential Sets with Neural Networks\n\nThe 25th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '19)\nAnchorage, AK, USA; Anchorage, AK, USA192019. August 4-8, 2019. August 4-8, 201910.1145/3292500.3330979ACM Reference Format: ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3292500.3330979 * Xiangnan He is the corresponding author. ACM ISBN 978-1-4503-6201-6/19/08. . . $15.00Sequential setsdeep learningtemporal data forecasting\nGiven past sequential sets of elements, predicting the subsequent sets of elements is an important problem in different domains. With the past orders of customers given, predicting the items that are likely to be bought in their following orders can provide information about the future purchase intentions. With the past clinical records of patients at each visit to the hospitals given, predicting the future clinical records in the subsequent visits can provide information about the future disease progression. These useful information can help to make better decisions in different domains.However, existing methods have not studied this problem well. In this paper, we formulate this problem as a sequential sets to sequential sets learning problem. We propose an end-to-end learning approach based on an encoder-decoder framework to solve the problem. In the encoder, our approach maps the set of elements at each past time step into a vector. In the decoder, our method decodes the set of elements at each subsequent time step from the vectors with a set-based attention mechanism. The repeated elements pattern is also considered in our method to further improve the performance. In addition, our objective function addresses the imbalance and correlation existing among the predicted elements. The experimental results on three real-world data sets show that our method outperforms the best performance of the compared methods with respect to recall and person-wise hit ratio by 2.7 \u2212 20.6% and 2.1 \u2212 26.3%, respectively. Our analysis also shows that our decoder has good generalization to output sequential sets that are even longer than the output of training instances.CCS CONCEPTS\u2022 Information systems \u2192 Information systems applications.\n\nINTRODUCTION\n\nGiven the history records, forecasting the future events is widely studied in different domains. It provides useful information for decision-making and planing. Forecasting the NYSE (The New York Stock Exchange) composite index can help the traders to evaluate the risk in stock market [19]. Predicting the grade of a course that the students will obtain in the next term can support them to make decisions on choosing the suitable courses [21]. Predicting the items that the customers will buy next time can help to make decisions on choosing items recommended to the customers [31]. In these applications, the methods predict an aggregated element/value or a set/sequence of elements/values based on the past records.\n\nHowever, in some applications, we need to deal with more complex data -sequential sets in which each sequence consists of sets instead of elements. In retail area, given the past orders of the customers, predicting the items that will appear in their subsequent orders can provide information about the change of the purchase intentions in the future. In medical area, given the services and diagnoses in patients' past visits to the hospitals, predicting the services and diagnoses in their subsequent visits can provide information about the possible future disease progression. If we denote a visit/order as a set, the task becomes a new type of forecasting problem -sequential sets forecasting in which the input and output are both sequential sets. This problem is a generalization to time series forecasting [6] and sequence prediction [4] that only has a single value/element at each time step.\n\nAnother line of research related to our problem can be categorized as next set prediction. Next basket recommendation [31] [29] [22], next clinical events prediction [3][9] and next repeat set prediction [5] belong to this category. All of these methods predict the next set with the past sequential sets given. Even though we can tweak this type of methods to recurrently or directly predict the subsequent sets, our empirical analysis shows that it causes the sequential structure information loss in the output and the error accumulation in the input.\n\nIn this paper, we aim to use the past sequential sets to predict the subsequent sequential sets. We formulate this sequential sets forecasting to sequential sets to sequential sets learning problem whose input and output both are a sequence of sets (sequential sets). It can be seen as a generalization to sequence to sequence learning (seq2seq) [25] whose input and output both are a sequence of elements. We introduce a recurrent neural networks based method Sets2Sets to solve this problem. Our contributions are as follows:\n\n\u2022 We formulate a new type of problem -sequential sets to sequential sets learning problem. An encoder-decoder framework called Sets2Sets is proposed to solve this problem. Sets2Sets can learn the complex relations among the elements within and across different sets.\n\n\u2022 Our encoder-decoder framework uses an element-embeddingbased set embedding to represent each set and uses an attention mechanism to leverage the information in different input sets for different output sets accordingly. \u2022 A repeated elements based element-element component is proposed to capture the element-element interactions across different sets. It utilizes the observation that the elements frequently appearing in the past can have high probability to appear in the future. \u2022 Our objective function addresses the label imbalance and exploits the correlation among labels. \u2022 We conduct experiments on three real-world data sets , one about diabetes medical claims data and two about grocery data, to demonstrate the effectiveness of our method. Our analysis also shows the superiority of our decoding method, compared to other options.\n\nThe rest of the paper is organized as follows: Section 2 surveys the related work. Section 3 describes the limitations of related methods. Section 4 defines the problem. Section 5 introduces our proposed method. Section 6 evaluates our method. Finally, section 7 provides some concluding remarks.\n\n\nRELATED WORK\n\nTime Series Forecasting and Sequence Prediction Time series forecasting [6] and sequence prediction [4] have been widely studied. Our problem is also a sequential data forecasting problem. However, the study towards forecasting sequence of sets is lacked.\n\nNext Set Prediction Next set prediction has been studied in different areas. In e-commerce, next basket recommendation [31][29] [22] is an important component for many e-commerce website. FPMC [22] is a classical next basket recommendation method proposed by Rendle et al. It learns both sequential behaviors and users' personal tastes. Yu et. al. [31] propose a state-of-the-art next basket recommendation based on RNNs to capture the temporal dynamic of the orders that is ignored by Wang et al. [29]. In medical area, predicting the clinical events [9] in the next visit can allow clinicians to anticipate the patient status at the time of visit. The method proposed by Choi et al. [9] is similar to the next basket recommendation method [31] except they use linear dimension reduction instead of embedding for the input vector. Tian et.al. [3] also propose a similar RNN based next visit prediction method with extra attention mechanism based on the elapsed time between consecutive visits. In our setting, we only focus on the temporal order. Benson et al. [5] study sequential repetition behaviors existing in different kinds of sequences of sets data and propose a stochastic model to capture this behavior. However, there are two limitations of their method. First, their method has the assumption that the next set only consists of elements that have appeared in the past sets. In our applications, the new elements dominate the subsequent sets. Second, their training steps need to generate and store all the ordered set partitions 1 of a set, which is only applicable when the size of the set is very small 2 .\n\nEncoder-decoder for Sequence to Sequence Learning Learning a mapping from a sequence to another sequence is called sequence to sequence learning, which is widely studied in machine translation. Existing methods achieve a great success with different encoder-decoder structures [2][25] [27]. Our problem is different from the sequence to sequence learning problem in two ways. First, we focus on modelling sequential sets instead of sequence of elements, which indicates our problem has more complex patterns. Second, the sequence to sequence learning is to learn a mapping between two different sequences while the sequential sets in the inputs and outputs of our problem belong to the same sequence.\n\nRecurrent Neural Networks and Attention Mechanism Recurrent neural networks (RNNs) have been successfully applied in many sequence related tasks including speech recognition [24], image captioning [28], and demand forecasting [13]. Different attention mechanisms [2] [20] are introduced to improve the performance of RNNs by guiding the networks to focus on particular parts of the input. Recently, attention mechanism is also introduced into recommend system and improves the performance at different recommendation systems [17] [7]. We use the attention mechanism to adaptively utilize the information in different past sets.\n\nMultilabel Classification Multilabel classification is to classify instances with a set of labels simultaneously [33]. The correlation among labels can usually improve the performance. Ghamrawi et al. [14] exploit the pair-wised label co-occurrences to improve the performance. Zhang et al. [32] propose a loss function to simultaneously maximize the margin between positive and negative the labels. Since the prediction at each step in our problem is a multilabel classification, we can leverage the characteristics of multilabel classification to improve the performance.\n\n\nLIMITATIONS OF RELATED METHODS\n\nLimitation of time series forecasting As more structure information contained within and across the sets, forecasting sequential sets not only requires us to consider the temporal dynamic but also consider the relations among different elements within and across sets. Current time series forecasting and sequence prediction methods only have one value/element at each step and miss to consider the complex relations in sequential sets data.\n\nLimitation of next set prediction Our problem can be seen as an extension to the next set prediction problem. We can borrow the ideas from time series forecasting [15] to recurrently predict next set or directly predict all the subsequent sets together with next set prediction methods. However, it has limitations. Recurrently predicting next set introduces errors into the input as there are always errors at each next set prediction. And it also loses part of temporal correlation in the subsequent sets. Directly predicting the subsequent sets together seems to avoid this problem, but there is a max number of sets that can be predicted at one shot as the output dimension is fixed. If we need to predict sets longer than this, we still need to use the similar recurrent strategy and suffer from the same problems. We will discuss this in section 6.3.\n\n\nPROBLEM DEFINITION\n\nDenote D as the set that consists of all the possible elements. Each set (e.g. visit/basket) of a person is represented as a |D|-dimensional vector v in which each dimension c m is set to 1 if the corresponding element (e.g. code/item) appears in the set, and 0 otherwise. Then we can formulate our sequential sets to sequential sets learning problem as follows:\n\nGiven a sequence of sets {v 1 , v 2 , ..., v k } of a person, where v i is the vector containing the elements appearing in the i-th set, predict the subsequent l sets {v k +1 ,v k +2 , ...,v k +l }. l is a given parameter.\n\nExisting next set prediction methods treat the predicted set size as a given parameter. We argue that manually setting this parameter can let the users look for results based on their needs. So we still insist on this setting.\n\n\nPROPOSED METHOD\n\nOur encoder-decoder framework is shown in Figure 1. In the encoder, each past sequential set v i is first embedded into low dimensional representation x i by set embedding which will be introduced latter. Then, x i is forwarded into corresponding RNN unit that\nh i = f (x i , h i\u22121 ),\nto generate the corresponding hidden state vector h i . The f is some nonlinear function chosen by different variants of RNNs. All the hidden state vectors will be used in the set-based attention. The last hidden state h k is passed as the initial hidden vector of the decoder.\n\nIn the decoder, we have the RNN unit that\ns i = \u0434(x i , s i \u22121 , z i ),(1)\nwherex i is the low dimensional representation of embeddingv i\u22121 and s i is the hidden state. The z i is a context vector obtained from the set-based attention mechanism that will be introduced latter. \u0434 is some nonlinear function chosen by different variants of RNNs. The output o(v i ) of the unit is calculated by where W s is a matrix to project the hidden state back to the space of elements. The softmax function is used to normalize all the entries into the range of [0, 1]. The repeat pattern is recorded in vector \u03b3 to further improve the prediction.\no(v i ) = softmax(W s s i ),(2)\n\nSet Embedding\n\nFor the original set vectors, we can observe two properties. First, the vector v has high dimensionality and extreme level of sparsity. Directly using these vectors as the input of RNN will result in a lot of parameters in RNN and each instance only updates a few parameters, which increases the complexity of the RNN model and requires a large number of training data. Second, there are taskspecific correlations among different elements within each set. For example, the ICD codes in our data classify all the codes into many categories based on their clinical semantics so that different codes can belong to the same categories. Associate rule analysis shows that some items co-occur in the same baskets [26]. Based on this, we propose to use word embedding that learns a dense vector with small dimensionality for each element in the set. Word embedding is known to generate similar representations for elements with similar task-specific similarity to improve the performance [12]. The word embeddings of all the elements in the same set are aggregated as the embedding for the set by the average pooling. Figure 2 shows an example of the set embedding. The element matrix which transfers the original one-hot vector to the corresponding embedding will be jointly learned within the encoder-decoder framework.\n\n\nSet-based Attention\n\nThe output sequential sets have temporal correlation relation with input sequential sets. Specifically, we argue that different input sets may have different effects on different output sets. For example, as diabetes causes many complications [23], patients need to visit hospitals regularly. Once the patients are diagnosed with some complications, the treatments and tests will be changed due to this diagnosis. In the grocery shopping case, customers usually consume different items with different time, which means the next time to repeatedly purchase the same type of items are different. So when we try to predict the future elements, the past related elements are expected to have higher effect than other unrelated elements. We introduce an attention mechanism to focus on the sets containing these past related elements to leverage the effects from the past accordingly. The set-based attention mechanism is used in Equation 1, in which the context vector z i is a weighted sum of all the hidden states in the encoder and is calculated by\nz i = j \u03b1 i j h j .\nThe weight \u03b1 i j is calculated by\n\u03b1 i j = exp(e i j ) k exp(e ik )\nwhere e i j = a(s i\u22121 , h j ). The a(x, y) is implemented as a multi-layer perception that takes the concatenation of x and y as input.\n\n\nModeling Repeated Elements\n\nThe set-based attention mechanism only models the set-element interactions (from past set to future element). The finer grain elementelement (from past element to future element) is not explicitly modeled. In this section, we introduce a mechanism to model repeatedelement-specified element-element relation.\n\nThere are usually some common regular tests and procedures repeatedly applied to diabetic patients (i.e. checking blood pressure). People usually have repeat purchase in shopping [1]. Our analysis on all data sets shows that 15%-60% of the elements in the future sets have appeared in the past sets. We also calculate the probability of each item appearing in the past to appear in the subsequent sets and observe that the elements frequently appearing in the past have high probability to appear in the future. So we propose to integrate a component to model this repeated pattern. We revise the equation 2 as follows:\no(v i ) = softmax(W s s i \u2022 (1 \u2212 \u03b2 \u2022 \u03b1 ) + \u03b3 \u2022 \u03b1 ),(3)\nwhere \u2022 is the element-wise product. Each entry of the vector \u03b3 is the probability of the corresponding element appearing in the past sets of a given person. Vector \u03b2 is a k-hot vector that records all the non-zero entries of the \u03b3 . It is used to control that the elements which have appeared in the past can get the contributions from our encoder-decoder and repeated pattern adaptively while the elements not appearing in the past are only predicted by our encoderdecoder. Vector \u03b1 is the coefficient that balances the contributions coming from our proposed encoder-decoder and the probability from the repeated element interaction. The \u03b1 is calculated by:\n\u03b1 = sigmoid(W \u03b3 \u03b3 + b),\nwhere matrix W \u03b3 and vector b are learned from the data.\n\n\nTraining and Inference\n\nAccording to the problem definition, the prediction at each subsequent set can be seen as a multilabel classification. So we need to consider the characteristics of multilabel classification. First, multilabel classification can suffer from label imbalance problem [11], which means some labels appear in much more instances than other labels. Figure 3 shows the frequency distribution of the top 30 most frequent codes in our diabetic data set. It shows that the frequency across different labels are imbalance. Similar phenomenons can be observed in other data sets. Thus, we need to resolve label imbalance. Second, the labels in multilabel classification has some patterns that can be exploited to improve the performance. Zhang et. al [32] show that the correlation among labels can be captured by forcing the predicted probabilities for the labels belonging to an instance to be higher than the ones not belonging to the instance.\n\nHere, we use an objective function shown in the Equation 4 to consider both characteristics,\narg min \u03b8 Y l i =1 WMSE(v i , o(v i )) + \u03bb PSM(v i , o(v i )),(4)\nwhere the accumulated sum outside goes through all the output sequential sets of different persons in the training data. The v i is the ground truth and o(v i ) is the output from decoder for the i-th set. WMSE is weighted mean square loss and PSM is partitioned set margin constraint. The \u03bb is a hyperparameter. \nThe WMSE(v i , o(v i )) is calculated by: m w (m)(c m \u2212 o(\u0109 m )) 2 ,(5)v i , o(v i )) is calculated by: 1 |P v i | |P v i | (k,l )\u2208Pv i \u00d7Pv i marg(o(\u0109 i k ), o(\u0109 i l )),(6)\nwhere P v i is the positive set that contains all the elements appearing in v i , P v i is the negative set that contains all the elements not appearing in v i and the pair-wise margin marg\n(o(\u0109 i k ), o(\u0109 i l )) is cal- culated by exp(\u2212(o(\u0109 i k ) \u2212 o(\u0109 i l ))\n). This constraint is to maximize the pair-wise margin between the predicted positive and negative sets.\n\nInference: We use a greedy algorithm to predict the elements with the top k highest probabilities at each subsequent set. We set the output vector's k entries that correspond to the top k highest values in o(v i ) to 1 and the other entries to 0 as the prediction for each set. k and l are parameters given by users.\n\n\nEXPERIMENTS\n\nIn this section, we conduct extensive experiments to answer the following research questions:\n\nRQ1 How is the effectiveness of our designed framework? Can it provide better performance compared to the designed baselines and the tweaked state-of-the-art next set prediction methods?\n\nRQ2 What kind of benefit the decoder RNN can bring? What are the problems in recurrently predicting the next set and directly predicting the subsequent sets?\n\nRQ3 How does the repeated-element-specified element-element interaction contributes to the performance? 6.1 Experimental Settings 6.1.1 Data Sets. We experimente with three real-world data sets.\n\nOPTUM. This data set from OPTUM consists of 4-year longitudinal medical claims data of diabetic patients. Medical claims data contains facility claims and physician claims that record all the services, the physicians' checking and diagnoses given to the patients. It also records all the procedures that the patients receive in the hospitals. All the events are encoded by standard codes including ICD code, Diagnosis Related Group code, and CPT/HCPCS Procedure code. We treat all the codes appearing in one visit to the medical facilities as a set.\n\nDunnhumby. This data set is opened sourced by dunnhumby, a global customer data science company. It contains the transactions about which items are bought by each customer in each order with the time stamp. We treat all the items bought in the same order as a set.\n\nTa-Feng. It is a public data set. It also contains the transactions about which items are bought by each customer in each basket with the time stamp. For simplicity, we only consider the top 5000 most frequent items that cover 83% items appearing in all the baskets. We treat all the items bought in the same order as a set.\n\nThe statistic information of all the data sets after pre-processing is shown in the Table 1. 6.1.2 Evaluation Measurement. We use recall, NDCG and personwise hit ratio (PHR) to evaluate our methods. The prediction for each set can be seen as a multilabel classification. Recall is a wildlyused measurement for multi-label classification [33]. We use the average recall of all the predicted sets as the measurement. NDCG is a ranking based measure which takes into account the order of elements in a list [16]. We calculate the NDCG for each set based on the top k sorted elements list. The average NDCG of all the predicted sets is used as the measurement. Original hit ratio is used to measure the ratio of the users whose recommended items appear in the ground truth in the recommend system [10]. Since we have multiple sets in the predictions for each person, we define personwise hit ratio (PHR) as the ratio of the persons whose predicted sets all contain the elements appearing in the corresponding ground truth sets. Unlike the recall focuses on the accuracy of the methods at set level, PHR focuses on the accuracy of the methods at person level.\n\n\nEvaluation Method.\n\nIn OPTUM data set, we have the records of patients across 4 years after they are diagnosed with diabetes. We partition the visits of each patient into to two parts. The visits in the first two years is used as history records to forecast the visits in the second two years. In Dunnhumby and Ta-Feng data sets, we partition the baskets of each user into two equal parts based on the time stamps. The first part with earlier time stamps is used as the history records to forecast the baskets in the second part. All the data sets are partitioned across persons. We reserve the data of 10% persons as the validation set for hyperparameters searching in all the methods. The left data is applied 5-fold cross-validation across persons to evaluate the methods.\n\n\nCompared Methods.\n\nWe compare our method with following methods.\n\nTop-k frequent (TopKFreq): It uses the most frequent k elements that appear in all the sets of the training data as the prediction for all the subsequent sets.\n\nPersonalized Top-k frequent (PersonTopKFreq): It uses the most frequent k elements that appear in the past sets of a given person as the prediction for all the subsequent sets.\n\nPersonKNN: It is a k-nearest neighbors (k-NN) based baseline proposed by us. The basic idea is to predict each subsequent set by selecting the top n most frequent elements appearing in the corresponding subsequent sets of the k nearest persons. To calculate the similarity between persons, we propose a way to represent the past sets of each person as a feature vector. First, the past sets are represented as m-hot vectors. Then all the m-hot vectors are partitioned into 3 groups (long time ago group, short time ago group, and recent group) of equal size according to the temporal order. If the number of the sets cannot be equally partitioned, we have the priority, the recent group > short time ago group > long time ago group, to enlarge the size of the group. The vectors within each group are summed up to one vector as the group vector. After applying a time decay weight to each group vector, we sum up all the weighted group vectors as the feature vector for similarity calculation.\n\nFPMC: A classical hybrid model for next basket recommendation based on markov chain and factorization method [22]. Both sequential behaviors and users' personal tastes are taken into account for prediction. Since our problem can be seen as an extension to next basket recommendation as mentioned in the related work, we tweak this next basket recommendation method as our compared method. As the FPMC is a first-order markov model, we recurrently applying this method to predict the next set to get all the subsequent sets.  DREAM: A state-of-the-art deep model based on RNN for next basket recommendation [31]. It considers personal dynamic interests at different time and the global interactions of all baskets of the user over time. To predict the subsequent sets, we consider both recurrently predicting next set and predicting a concatenation of all the subsequent sets that are widely used in time series forecasting [15]. We only report the results of the latter way as it achieves better performance.\n\nWe tune the hyper parameters in all the compared methods with grid search.\n\n6.1.5 Configuration of Our Method. In our method, the RNNs are implemented by gated recurrent unit [8]. Adam [18] is used as the optimizer. The number of units in RNN and the dimension of embedding are both set to 32.\n\n\nPerformance Comparison (RQ1)\n\nThe comparisons with different methods are shown in Table 2, Table 3, and Table 4. Several observations can be made. First, the simple TOPKFreq can achieve comparable recall and PHR compared to other model-based methods. It indicates that people have some common elements. But TOPKFreq achieves the worst NDCG as people still have distinct elements. TopKFreq is better than PersonTopK-Freq in the OPTUM data set but is worse than PersonTopKFreq in other two shopping transactions data sets. The reason is that many regular tests and diagnoses are common across different patients as diabetes results in some shared pattern for different patients. But customers have distinct preferences when they purchase items.\n\nSecond, the personalized top-k frequent method outperforms other baselines in Dunnhumby and Ta-Fang data sets. It even achieves better NDCG than our proposed method in Dunnhumby data set. Our analysis shows that, in both customers' shopping orders data sets, different customers have some repeated purchases on different items. Especially, different customers in the Dunnhumby data set have their must-buy items in many of their orders. Personalized top-k frequent method can exactly capture this property and put the frequently bought items at high ranks. Thus, this baseline achieves high NDCG. It also indicates existing methods fail to effectively utilize this important property in the data. However, the OPTUM data set does not have this property as the disease progression results in the change of the codes over the time. So the personalized top-k frequent method is worse than other modelbased baselines in OPTUM data set. Third, our proposed PeronKNN is generally better than other baselines in OPTUM data with respect to recall and NDCG. We believe the reason is that the patients with similar historical records will have some common disease progression in the future which results in some shared codes appearing in the subsequent sets. But PeronKNN is still worse than PersonTopKFreq in two shopping transactions data sets as the items appearing in the future shopping transactions are distinct across different people.\n\nFourth, FPMC and DREAM can not outperform personalized top-k frequent method in Dunnhumby and Ta-Fang data sets. We believe that too many interactions between different elements in FPMC and DREAM make the effect of the important interactions between repeated items vague. FPMC achieves higher PHR than PersonKNN, which indicates the transition from the elements in the last set affecting the elements in the next set is personalized and FPMC can capture it accordingly while the elements from the nearest neighbors are sometimes different from the given person's elements. FPMC outperforms DREAM in OPTUM data set while DREAM outperforms FPMC in Dunnhumby and Ta-Fang data sets. We believe the reason is that the next set is largely affected by the previous set in OPTUM data. FPMC is first-order markov model so that it achieves better performance in this data.\n\nFifth, our Sets2Sets method consistently outperform the best performance of all the baselines with respect to recall and PHR    \n\n\nEffect of the Decoder (RQ2)\n\nIn this section, we investigate if it is necessary to use our decoder to predict sequential sets. We compare our method with three variants of our method. The first two variants are based on the widely-used ideas in time series forecasting [15]. The first variant is to predict subsequent sequential sets by recurrently feeding both past sets and previous predicted set into the encoder to predict the next set.\n\nDenote it as Sets2Set. The second variant is to predict subsequent sequential sets by concatenating these sets into one large vector, which means we only have one RNN unit in the decoder to predict all the sets together. Denote it as Sets2Merge. The third variant is to limit the supervision with only the first subsequent set. No matter how many subsequent sets we predict in the test step, we ignore all the supervision information after the first subsequent set in the training. Denote it as Sets2Sets(1-set). Our method is denoted as Sets2Sets(s-sets), which means we use subsequent s sets as the supervision in the training step.\n\nThe experimental results are shown in the Figure 4 and 5 . We can observe that Sets2Sets(s-sets) outperforms Sets2Set. We believe there are two reasons. First, Sets2Sets can be seen as a multi-task learning that jointly predicts different sets in the outputs, which leverages the correlation among different sets in the outputs. Specifically, each set in the outputs is also associated with other sets after it during the training. But in the training of Sets2Set, each set is predicted without the information of the sets after them. Second, Sets2Set requires us to forward the predicted set as part of the inputs, which means the errors in the predictions enter into the inputs. Note that the average set size is less than 10 while the predicted set size are larger than this. Many fake elements is introduced into the inputs. It is supported by the observation that the NDCG decreases with the set size k increased. Also, we can observe that the performance decrease of Sets2Set is larger than other methods. It implies that the error is accumulated when we increase the number of subsequent sets s. We believe that tweaking the existing next basket recommendation methods in the similar recurrent way also suffers from the same problems. Another observation is that Sets2Merge is competitive with Sets2Sets. By predicting the subsequent sets together, we also avoid to introduce errors into the input. The correlation existing in the subsequent sets can also be captured by our constraint in the loss function. It indicates our method can properly utilize the supervised information from the subsequent sets.\n\nThe superiority of our way to predict subsequent sets exists in predicting subsequent sets beyond the supervision. As the number of the subsequent sets is given by users, we may predict subsequent sets that are longer than any training instances. As Sets2Merge has a max number of sets to predict at one shot, we can use the similar idea in Sets2Set to recurrently predict the part beyond the max number. Actually, when s > 1, Sets2Set can be seen as using an enhanced version of Sets2Merge(1-set) to predict subsequent s sets with recurrent strategy. Sets2Set has more data than Sets2Merge(1set) as Sets2Set contains the data with the supervision after the first subsequent set in the training. However, Sets2Sets(1-set) still achieves better performance than Sets2Set. Sets2Sets(1-set) even achieves comparable performance compared to Sets2Merge and Sets2Sets(s-set) with respect to the recall. Even though the NDCG of Sets2Sets(1-set) decreases with the increase of s, we believe it is expected as Sets2Sets(1-set) has less supervision information than other methods. The results support that our decoding way not only avoids introducing error into the input but also generalizes fairly to subsequent sets beyond the supervised part.\n\n\nEffect of the Repeated Elements (RQ3)\n\nIn this section, we investigate how effective the repeated element component is. We compare our method with a variant without this component. The results are shown in the Figure 6. The component improves the performance in both data sets. In Ta-Feng data, our method gets improvement by significant margin. We believe that this component can properly capture the important property that many customers repeatedly buy some items and improve the performance largely. However, similar repeat pattern is not strong in OPTUM data as the codes change with the disease progression.\n\n\nCONCLUSION AND FUTURE WORK\n\nIn this paper, we propose a sequential sets to sequential sets problem to model subsequent sets forecasting. Existing methods that only predict the sets in the next steps have limitations to utilize the information of elements existing within and across different sets. To address these issues, we develop an encoder-decoder framework. In future, we will consider to learn from sequential graph, which may bring benefit by introducing pre-existing relations between elements within each set [30].\n\nWith the emergence of many applications, the study towards mining the pattern in the new type of data -sequences of setsstarts to get attention recently [5]. Our work makes a step towards studying how to predict the new type of output -sequential sets. To our best knowledge, this paper is the first attempt to predict this new structured output. We envision that the proposed method will serve as a starting point for the prediction of the sequential sets. Code accompanying with this paper is available at Github.\n\nFigure 1 :\n1The Encoder-Decoder framework. The input of the first RNN unit in the decoder RNN is the last input in the encoder RNN. The inputs of other RNN units in the decoder RNN are the outputs of their last units.\n\nFigure 2 :\n2Input a visit/basket vector to set embedding and forward the generated representation into a RNN unit.\n\nFigure 3 :\n3Frequency distribution of the top 30 most frequent codes.\n\n\nwhere c m is the m-th entry of v i and o(\u0109 m ) is the m-th entry of o(v i ). The w(m) is a weighted function for element m that is calculated by f req(m) is the frequency of the element m in the training set. w(m) is used to balance the contributions of different elements to the loss. Other loss functions like cross entropy, binary cross entropy and so on are also considered but cannot show better performance. The PSM(\n\nFigure 4 :\n4Comparison with different variants by predicting next 2 sets. The k is the number of elements predicted at each set.\n\nFigure 5 :\n5Comparison with different variants by predicting each set with 40 elements. The s is the number of sets predicted.\n\nFigure 6 :\n6The effect of the repeated element component. We compare the methods by predicting next 2 sets. k is the number of elements predicted at each set.\n\nTable 1 :\n1Statistic information after pre-processing.Data \n#elements #sets \n#persons ave. set \nsize \n\nave. \n#sets \n/person \n\nOPTUM \n4,226 \n100,682 \n7,280 \n2.67 \n13.87 \nDunnhumby 4,997 \n289,918 \n36,241 \n7.33 \n7.99 \nTa-Feng \n5,000 \n66,714 \n7,384 \n5.35 \n9.03 \n\n\n\nTable 2 :\n2Comparison with different methods on OPTUM data set. The k is the number of elements predicted for each set.methods \nk = 20 \nk = 40 \nk = 60 \nk = 80 \npredict next 2 visits Recall NDCG \nPHR Recall NDCG \nPHR Recall NDCG \nPHR Recall NDCG \nPHR \n\nTopKFreq \n0.3307 \n0.1786 \n0.5578 \n0.3870 \n0.1369 \n0.6688 \n0.4141 \n0.1431 \n0.7009 \n0.4313 \n0.1432 \n0.7277 \nPersonTopKFreq \n0.2813 \n0.2346 \n0.3809 \n0.2836 \n0.2354 \n0.3857 \n0.2846 \n0.2356 \n0.3864 \n0.2855 \n0.2358 \n0.3877 \nPersonKNN \n0.3778 0.3097 \n0.6094 \n0.4147 0.3198 \n0.6561 0.4472 0.3273 \n0.7035 0.4765 0.3336 \n0.7426 \nFPMC \n0.3646 \n0.2709 0.6266 0.4232 \n0.2872 0.7114 \n0.4428 \n0.2944 0.7740 \n0.4601 \n0.2984 0.8180 \nDREAM \n0.3472 \n0.2105 \n0.5552 \n0.4191 \n0.2303 \n0.6774 \n0.4329 \n0.2401 \n0.7323 \n0.4575 \n0.2479 \n0.7680 \nSets2Sets \n0.4050 \n0.3224 \n0.6616 \n0.4425 \n0.3318 \n0.7378 \n0.4682 \n0.3385 \n0.7906 \n0.4908 \n0.3439 \n0.8496 \nImprovement \n7.2% \n4.0% \n5.9% \n4.6% \n3.7% \n3.7% \n4.7% \n3.4% \n2.1% \n3.0% \n3.1% \n3.9% \n\npredict next 3 visits Recall NDCG \nPHR Recall NDCG \nPHR Recall NDCG \nPHR Recall NDCG \nPHR \n\nTopKFreq \n0.2904 \n0.1447 \n0.4402 \n0.3338 \n0.1320 \n0.5489 \n0.3644 \n0.1343 \n0.5651 \n0.4083 \n0.1357 \n0.6122 \nPersonTopKFreq \n0.2820 \n0.2400 \n0.2953 \n0.2858 \n0.2412 \n0.3046 \n0.2869 \n0.2414 \n0.3046 \n0.2880 \n0.2417 \n0.3046 \nPersonKNN \n0.3501 0.2816 \n0.4241 0.3917 0.2929 \n0.4778 \n0.4341 0.3028 \n0.5583 0.4686 0.3100 \n0.6080 \nFPMC \n0.3292 \n0.2263 0.4268 \n0.3778 \n0.2453 0.6067 0.4360 \n0.2546 0.6714 \n0.4627 \n0.2604 0.7382 \nDREAM \n0.2891 \n0.1593 \n0.3181 \n0.3564 \n0.1835 \n0.4724 0.423 7 \n0.1975 \n0.5879 \n0.4605 \n0.2073 \n0.6617 \nSets2Sets \n0.3842 \n0.2934 \n0.5288 \n0.4299 \n0.3007 \n0.6335 \n0.4599 \n0.3062 \n0.7060 \n0.4822 \n0.3121 \n0.7704 \nImprovement \n9.7% \n4.1% \n23.9% \n9.7% \n2.7% \n4.4% \n5.5% \n1.1% \n5.1% \n2.7% \n0.6% \n3.2% \n\n\n\nTable 3 :\n3Comparison with different methods on Dunnhumby data set. The k is the number of elements predicted for each set.methods \nk = 20 \nk = 40 \nk = 60 \nk = 80 \npredict next 2 baskets Recall NDCG \nPHR Recall NDCG \nPHR Recall NDCG \nPHR Recall NDCG \nPHR \n\nTopKFreq \n0.1158 \n0.0653 \n0.2602 \n0.1520 \n0.0676 \n0.3174 \n0.1791 \n0.0727 \n0.3623 \n0.2010 \n0.0734 \n0.3914 \nPersonTopKFreq \n0.2343 0.2182 0.4074 0.2940 0.2205 0.4673 0.3057 0.2312 0.4757 0.3100 0.2387 0.4791 \nPersonKNN \n0.1151 \n0.1171 \n0.2442 \n0.1291 \n0.1203 \n0.2687 \n0.1413 \n0.1240 \n0.2918 \n0.1533 \n0.1272 \n0.3131 \nFPMC \n0.1106 \n0.0968 \n0.2479 \n0.1454 \n0.1082 \n0.3028 \n0.1753 \n0.1174 \n0.3491 \n0.2023 \n0.1248 \n0.3901 \nDREAM \n0.1129 \n0.0955 \n0.2491 \n0.1478 \n0.1050 \n0.3279 \n0.1828 \n0.1274 \n0.3727 \n0.2173 \n0.1430 \n0.4130 \nSets2Sets \n0.2435 \n0.1701 \n0.4151 \n0.3167 \n0.1975 \n0.5062 \n0.3488 \n0.2091 \n0.5371 \n0.3645 \n0.2144 \n0.5538 \nImprovement \n3.9% \n-22.0% \n1.9% \n7.7% \n-10.4% \n8.3% \n14.1% \n-9.5% \n12.9% \n17.6% \n-10.1% \n15.6% \n\npredict next 3 baskets Recall NDCG \nPHR Recall NDCG \nPHR Recall NDCG \nPHR Recall NDCG \nPHR \n\nTopKFreq \n0.1167 \n0.0655 \n0.1741 \n0.1517 \n0.0674 \n0.2218 \n0.1793 \n0.0727 \n0.2591 \n0.2007 \n0.0733 \n0.2861 \nPersonTopKFreq \n0.2323 0.2156 0.3201 0.2922 0.2300 0.3643 0.3042 0.2347 0.3725 0.3087 0.2364 0.3761 \nPersonKNN \n0.1153 \n0.1181 \n0.1593 \n0.1284 \n0.1210 \n0.1792 \n0.1409 \n0.1246 \n0.1957 \n0.1528 \n0.1279 \n0.2119 \nFPMC \n0.1095 \n0.0964 \n0.1628 \n0.1440 \n0.1075 \n0.2083 \n0.1814 \n0.1186 \n0.2584 \n0.2034 \n0.1241 \n0.2864 \nDREAM \n0.1102 \n0.0951 \n0.1598 \n0.1565 \n0.1113 \n0.2392 \n0.1855 \n0.1231 \n0.2757 \n0.2072 \n0.1325 \n0.3025 \nSets2Sets \n0.2560 \n0.1939 \n0.3315 \n0.3025 \n0.2112 \n0.3747 \n0.3195 \n0.2183 \n0.3912 \n0.3309 \n0.2227 \n0.4025 \nImprovement \n10.2% \n-10.0% \n3.6% \n3.5% \n-8.1% \n2.9% \n5.0% \n-6.9% \n5.0% \n7.2% \n-5.8% \n7.0% \n\n\n\nTable 4 :\n4Comparison with different methods on Ta-Feng data set. The k is the number of elements predicted for each set.methods \nk = 20 \nk = 40 \nk = 60 \nk = 80 \npredict next 2 baskets Recall NDCG \nPHR Recall NDCG \nPHR Recall NDCG \nPHR Recall NDCG \nPHR \n\nTopKFreq \n0.0905 \n0.0541 \n0.1111 \n0.1287 \n0.0505 \n0.1779 \n0.1606 \n0.0563 \n0.2433 0.1876 \n0.0642 \n0.2911 \nPersonTopKFreq \n0.1187 0.0859 0.1870 0.1464 0.0945 0.2419 0.1662 0.0998 0.2798 \n0.1816 0.1038 0.3122 \nPersonKNN \n0.0923 \n0.0675 \n0.0752 \n0.1095 \n0.0716 \n0.0970 \n0.1236 \n0.0763 \n0.1315 \n0.1384 \n0.0799 \n0.1582 \nFPMC \n0.0866 \n0.0488 \n0.0893 \n0.1236 \n0.0606 \n0.1701 \n0.1505 \n0.0682 \n0.2271 \n0.1692 \n0.0732 \n0.2693 \nDREAM \n0.0947 \n0.0538 \n0.0904 \n0.1245 \n0.0620 \n0.1743 \n0.1526 \n0.0706 \n0.2416 \n0.1758 \n0.0744 \n0.2780 \nSets2Sets \n0.1271 \n0.0890 \n0.2067 \n0.1739 \n0.1027 \n0.2876 \n0.2029 \n0.1107 \n0.3488 \n0.2263 \n0.1160 \n0.3909 \nImprovement \n7.1% \n3.6% \n10.5% \n18.8% \n8.7% \n18.9% \n22.1% \n10.9% \n24.7% \n20.6% \n11.7% \n25.2% \n\npredict next 3 baskets Recall NDCG \nPHR Recall NDCG \nPHR Recall NDCG \nPHR Recall NDCG \nPHR \n\nTopKFreq \n0.0904 \n0.0428 \n0.0347 \n0.1259 \n0.0501 \n0.0765 \n0.1544 \n0.0507 \n0.1232 0.1846 \n0.0602 \n0.1600 \nPersonTopKFreq \n0.1163 0.0806 0.1073 0.1522 0.0989 0.1510 0.1669 0.1032 0.1660 \n0.1796 0.1066 0.1938 \nPersonKNN \n0.0812 \n0.0624 \n0.0268 \n0.1000 \n0.0668 \n0.0417 \n0.1146 \n0.0719 \n0.0516 \n0.1279 \n0.0754 \n0.0715 \nFPMC \n0.0875 \n0.0536 \n0.0278 \n0.1211 \n0.0640 \n0.0725 \n0.1438 \n0.0709 \n0.1123 \n0.1627 \n0.0758 \n0.1332 \nDREAM \n0.0926 \n0.0553 \n0.0284 \n0.1232 \n0.0648 \n0.0748 \n0.1470 \n0.0720 \n0.1263 \n0.1686 \n0.0773 \n0.1530 \nSets2Sets \n0.1216 \n0.0856 \n0.1123 \n0.1731 \n0.1005 \n0.1779 \n0.1946 \n0.1061 \n0.2097 \n0.2116 \n0.1097 \n0.2395 \nImprovement \n4.5% \n6.2% \n4.6% \n13.7% \n1.6% \n17.8% \n16.6% \n2.8% \n26.3% \n17.7% \n2.9% \n23.6% \n\n\n\n\n2.7 \u2212 20.6% and 2.1 \u2212 26.3%, respectively. Our method also achieves better NDCG in OPTUM and Ta-Feng data sets. It indicates our method can address the issues mentioned in the baselines. Our encoder with embedding can model the personal tastes and the sequential behaviors. Our set attention mechanism can focus on the most related sets, which can adaptively handle the different properties of different data. Our repeated-element-specified element-element interaction component enhances the signal of the repeated elements.(a) Recall at OPTUM \n\n(b) Recall at Ta-Feng \n\n(c) NDCG at OPTUM \n(d) NDCG at Ta-Feng \n\n\nhttps://en.wikipedia.org/wiki/Ordered_Bell_number.2 The number of ordered set partitions reaches 2,677,687,796,244,384,203,115 when the set size is 20. The set size in our applications is larger than 20.\n\nRepeat purchase intentions in online shopping: The role of satisfaction, attitude, and online retailers' performance. Alhassan G Abdul-Muhmin, Journal of International Consumer Marketing. 23Alhassan G Abdul-Muhmin. 2010. Repeat purchase intentions in online shopping: The role of satisfaction, attitude, and online retailers' performance. Journal of International Consumer Marketing 23, 1 (2010), 5-20.\n\nNeural machine translation by jointly learning to align and translate. Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio, arXiv:1409.0473arXiv preprintDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural ma- chine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473 (2014).\n\nInterpretable Representation Learning for Healthcare via Capturing Disease Progression through Time. Tian Bai, Shanshan Zhang, Slobodan Brian L Egleston, Vucetic, Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data MiningACMTian Bai, Shanshan Zhang, Brian L Egleston, and Slobodan Vucetic. 2018. In- terpretable Representation Learning for Healthcare via Capturing Disease Pro- gression through Time. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, 43-51.\n\nScheduled sampling for sequence prediction with recurrent neural networks. Samy Bengio, Oriol Vinyals, Navdeep Jaitly, Noam Shazeer, Advances in Neural Information Processing Systems. Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. 2015. Scheduled sampling for sequence prediction with recurrent neural networks. In Advances in Neural Information Processing Systems. 1171-1179.\n\nSequences of Sets. Austin R Benson, Ravi Kumar, Andrew Tomkins, Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data MiningAustin R. Benson, Ravi Kumar, and Andrew Tomkins. 2018. Sequences of Sets. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 1148-1157.\n\nTime series analysis: forecasting and control. E P George, Box, M Gwilym, Jenkins, C Gregory, Greta M Reinsel, Ljung, John Wiley & SonsGeorge EP Box, Gwilym M Jenkins, Gregory C Reinsel, and Greta M Ljung. 2015. Time series analysis: forecasting and control. John Wiley & Sons.\n\nAttentive collaborative filtering: Multimedia recommendation with item-and component-level attention. Jingyuan Chen, Hanwang Zhang, Xiangnan He, Liqiang Nie, Wei Liu, Tat-Seng Chua, Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval. the 40th International ACM SIGIR conference on Research and Development in Information RetrievalACMJingyuan Chen, Hanwang Zhang, Xiangnan He, Liqiang Nie, Wei Liu, and Tat- Seng Chua. 2017. Attentive collaborative filtering: Multimedia recommendation with item-and component-level attention. In Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval. ACM, 335-344.\n\nKyunghyun Cho, Bart Van Merri\u00ebnboer, Dzmitry Bahdanau, Yoshua Bengio, arXiv:1409.1259On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprintKyunghyun Cho, Bart Van Merri\u00ebnboer, Dzmitry Bahdanau, and Yoshua Ben- gio. 2014. On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259 (2014).\n\nDoctor ai: Predicting clinical events via recurrent neural networks. Edward Choi, Mohammad Taha Bahadori, Andy Schuetz, F Walter, Jimeng Stewart, Sun, Machine Learning for Healthcare Conference. Edward Choi, Mohammad Taha Bahadori, Andy Schuetz, Walter F Stewart, and Jimeng Sun. 2016. Doctor ai: Predicting clinical events via recurrent neural networks. In Machine Learning for Healthcare Conference. 301-318.\n\nLocal latent space models for top-n recommendation. Evangelia Christakopoulou, George Karypis, Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data MiningACMEvangelia Christakopoulou and George Karypis. 2018. Local latent space models for top-n recommendation. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, 1235-1243.\n\nAddressing Imbalance in Multi-Label Classification Using Structured Hellinger Forests. Alan Zachary, Dimitris N Daniels, Metaxas, AAAI. Zachary Alan Daniels and Dimitris N Metaxas. 2017. Addressing Imbalance in Multi-Label Classification Using Structured Hellinger Forests.. In AAAI. 1826- 1832.\n\nProblems with evaluation of word embeddings using word similarity tasks. Manaal Faruqui, Yulia Tsvetkov, Pushpendre Rastogi, Chris Dyer, arXiv:1605.02276arXiv preprintManaal Faruqui, Yulia Tsvetkov, Pushpendre Rastogi, and Chris Dyer. 2016. Prob- lems with evaluation of word embeddings using word similarity tasks. arXiv preprint arXiv:1605.02276 (2016).\n\nValentin Flunkert, David Salinas, arXiv:1704.04110DeepAR: Probabilistic forecasting with autoregressive recurrent networks. arXiv preprintValentin Flunkert, David Salinas, and Jan Gasthaus. 2017. DeepAR: Proba- bilistic forecasting with autoregressive recurrent networks. arXiv preprint arXiv:1704.04110 (2017).\n\nCollective multi-label classification. Nadia Ghamrawi, Andrew Mccallum, Proceedings of the 14th ACM international conference on Information and knowledge management. the 14th ACM international conference on Information and knowledge managementACMNadia Ghamrawi and Andrew McCallum. 2005. Collective multi-label classifica- tion. In Proceedings of the 14th ACM international conference on Information and knowledge management. ACM, 195-200.\n\nComparison of direct and iterative artificial neural network forecast approaches in multi-periodic time series forecasting. Co\u015fkun Hamza\u00e7ebi, Diyar Akay, Fevzi Kutay, Expert Systems with Applications. 36Co\u015fkun Hamza\u00e7ebi, Diyar Akay, and Fevzi Kutay. 2009. Comparison of direct and iterative artificial neural network forecast approaches in multi-periodic time series forecasting. Expert Systems with Applications 36, 2 (2009), 3839-3844.\n\nTrirank: Reviewaware explainable recommendation by modeling aspects. Xiangnan He, Tao Chen, Min-Yen Kan, Xiao Chen, Proceedings of the 24th ACM International on Conference on Information and Knowledge Management. the 24th ACM International on Conference on Information and Knowledge ManagementACMXiangnan He, Tao Chen, Min-Yen Kan, and Xiao Chen. 2015. Trirank: Review- aware explainable recommendation by modeling aspects. In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management. ACM, 1661-1670.\n\nNAIS: Neural Attentive Item Similarity Model for Recommendation. Xiangnan He, Zhenkui He, Jingkuan Song, Zhenguang Liu, Yu-Gang Jiang, Tat-Seng Chua, IEEE Transactions on Knowledge and Data Engineering. Xiangnan He, Zhenkui He, Jingkuan Song, Zhenguang Liu, Yu-Gang Jiang, and Tat-Seng Chua. 2018. NAIS: Neural Attentive Item Similarity Model for Recom- mendation. IEEE Transactions on Knowledge and Data Engineering (2018).\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980arXiv preprintDiederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti- mization. arXiv preprint arXiv:1412.6980 (2014).\n\nForecasting the NYSE composite index with technical analysis, pattern recognizer, neural network, and genetic algorithm: a case study in romantic decision support. William Leigh, Russell Purvis, James M Ragusa, Decision support systems. 32William Leigh, Russell Purvis, and James M Ragusa. 2002. Forecasting the NYSE composite index with technical analysis, pattern recognizer, neural network, and genetic algorithm: a case study in romantic decision support. Decision support systems 32, 4 (2002), 361-377.\n\nRecurrent models of visual attention. Volodymyr Mnih, Nicolas Heess, Alex Graves, Advances in neural information processing systems. Volodymyr Mnih, Nicolas Heess, Alex Graves, et al. 2014. Recurrent models of visual attention. In Advances in neural information processing systems. 2204-2212.\n\nCumulative knowledge-based regression models for next-term grade prediction. Sara Morsy, George Karypis, Proceedings of the 2017 SIAM International Conference on Data Mining. SIAM. the 2017 SIAM International Conference on Data Mining. SIAMSara Morsy and George Karypis. 2017. Cumulative knowledge-based regres- sion models for next-term grade prediction. In Proceedings of the 2017 SIAM International Conference on Data Mining. SIAM, 552-560.\n\nFactorizing personalized markov chains for next-basket recommendation. Steffen Rendle, Christoph Freudenthaler, Lars Schmidt-Thieme, Proceedings of the 19th international conference on World wide web. the 19th international conference on World wide webACMSteffen Rendle, Christoph Freudenthaler, and Lars Schmidt-Thieme. 2010. Factor- izing personalized markov chains for next-basket recommendation. In Proceedings of the 19th international conference on World wide web. ACM, 811-820.\n\nCognitive dysfunction in adults with type 1 (insulin-dependent) diabetes mellitus of long duration: effects of recurrent hypoglycaemia and other chronic complications. Cm Ryan, Williams, T J Dn Finegold, Orchard, Diabetologia. 36CM Ryan, TM Williams, DN Finegold, and TJ Orchard. 1993. Cognitive dysfunc- tion in adults with type 1 (insulin-dependent) diabetes mellitus of long duration: effects of recurrent hypoglycaemia and other chronic complications. Diabetologia 36, 4 (1993), 329-334.\n\nNeural speech recognizer: Acoustic-to-word LSTM model for large vocabulary speech recognition. Hagen Soltau, Hank Liao, Hasim Sak, arXiv:1610.09975arXiv preprintHagen Soltau, Hank Liao, and Hasim Sak. 2016. Neural speech recognizer: Acoustic-to-word LSTM model for large vocabulary speech recognition. arXiv preprint arXiv:1610.09975 (2016).\n\nSequence to sequence learning with neural networks. Ilya Sutskever, Oriol Vinyals, Quoc V Le, Advances in neural information processing systems. Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In Advances in neural information processing systems. 3104- 3112.\n\nIntroduction to Data Mining. Pang-Ning Tan, Michael Steinbach, Vipin Kumar, Pang-Ning Tan, Michael Steinbach, and Vipin Kumar. 2005. Introduction to Data Mining.\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems. 5998-6008.\n\nShow and tell: A neural image caption generator. Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan, Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on. IEEE. Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. 2015. Show and tell: A neural image caption generator. In Computer Vision and Pattern Recog- nition (CVPR), 2015 IEEE Conference on. IEEE, 3156-3164.\n\nLearning hierarchical representation model for nextbasket recommendation. Pengfei Wang, Jiafeng Guo, Yanyan Lan, Jun Xu, Shengxian Wan, Xueqi Cheng, Proceedings of the 38th International ACM SIGIR conference on Research and Development in Information Retrieval. the 38th International ACM SIGIR conference on Research and Development in Information RetrievalACMPengfei Wang, Jiafeng Guo, Yanyan Lan, Jun Xu, Shengxian Wan, and Xueqi Cheng. 2015. Learning hierarchical representation model for nextbasket rec- ommendation. In Proceedings of the 38th International ACM SIGIR conference on Research and Development in Information Retrieval. ACM, 403-412.\n\nNeural Graph Collaborative Filtering. Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, Tat-Seng Chua, Proceedings of the 42th International ACM SIGIR conference on Research and Development in Information Retrieval. the 42th International ACM SIGIR conference on Research and Development in Information RetrievalACMXiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019. Neural Graph Collaborative Filtering. In Proceedings of the 42th International ACM SIGIR conference on Research and Development in Information Retrieval. ACM.\n\nA dynamic recurrent model for next basket recommendation. Feng Yu, Qiang Liu, Shu Wu, Liang Wang, Tieniu Tan, Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval. the 39th International ACM SIGIR conference on Research and Development in Information RetrievalACMFeng Yu, Qiang Liu, Shu Wu, Liang Wang, and Tieniu Tan. 2016. A dynamic recurrent model for next basket recommendation. In Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval. ACM, 729-732.\n\nMultilabel neural networks with applications to functional genomics and text categorization. Min-Ling Zhang, Zhi-Hua Zhou, IEEE transactions on Knowledge and Data Engineering. 1810Min-Ling Zhang and Zhi-Hua Zhou. 2006. Multilabel neural networks with applications to functional genomics and text categorization. IEEE transactions on Knowledge and Data Engineering 18, 10 (2006), 1338-1351.\n\nA review on multi-label learning algorithms. Min-Ling Zhang, Zhi-Hua Zhou, IEEE transactions on knowledge and data engineering. 26Min-Ling Zhang and Zhi-Hua Zhou. 2014. A review on multi-label learning algorithms. IEEE transactions on knowledge and data engineering 26, 8 (2014), 1819-1837.\n", "annotations": {"author": "[{\"start\":\"105\",\"end\":\"114\"},{\"start\":\"115\",\"end\":\"127\"}]", "publisher": null, "author_last_name": "[{\"start\":\"111\",\"end\":\"113\"},{\"start\":\"124\",\"end\":\"126\"}]", "author_first_name": "[{\"start\":\"105\",\"end\":\"110\"},{\"start\":\"115\",\"end\":\"123\"}]", "author_affiliation": null, "title": "[{\"start\":\"1\",\"end\":\"62\"},{\"start\":\"128\",\"end\":\"189\"}]", "venue": "[{\"start\":\"191\",\"end\":\"270\"}]", "abstract": "[{\"start\":\"610\",\"end\":\"2361\"}]", "bib_ref": "[{\"start\":\"2663\",\"end\":\"2667\",\"attributes\":{\"ref_id\":\"b18\"}},{\"start\":\"2817\",\"end\":\"2821\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"2956\",\"end\":\"2960\",\"attributes\":{\"ref_id\":\"b30\"}},{\"start\":\"3912\",\"end\":\"3915\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"3940\",\"end\":\"3943\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"4119\",\"end\":\"4123\",\"attributes\":{\"ref_id\":\"b30\"}},{\"start\":\"4124\",\"end\":\"4128\",\"attributes\":{\"ref_id\":\"b28\"}},{\"start\":\"4129\",\"end\":\"4133\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"4167\",\"end\":\"4170\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"4205\",\"end\":\"4208\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"4903\",\"end\":\"4907\",\"attributes\":{\"ref_id\":\"b24\"}},{\"start\":\"6586\",\"end\":\"6589\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"6614\",\"end\":\"6617\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"6890\",\"end\":\"6894\",\"attributes\":{\"ref_id\":\"b30\"}},{\"start\":\"6899\",\"end\":\"6903\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"6964\",\"end\":\"6968\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"7119\",\"end\":\"7123\",\"attributes\":{\"ref_id\":\"b30\"}},{\"start\":\"7269\",\"end\":\"7273\",\"attributes\":{\"ref_id\":\"b28\"}},{\"start\":\"7323\",\"end\":\"7326\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"7456\",\"end\":\"7459\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"7512\",\"end\":\"7516\",\"attributes\":{\"ref_id\":\"b30\"}},{\"start\":\"7615\",\"end\":\"7618\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"7833\",\"end\":\"7836\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"8671\",\"end\":\"8674\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"8679\",\"end\":\"8683\",\"attributes\":{\"ref_id\":\"b26\"}},{\"start\":\"9270\",\"end\":\"9274\",\"attributes\":{\"ref_id\":\"b23\"}},{\"start\":\"9293\",\"end\":\"9297\",\"attributes\":{\"ref_id\":\"b27\"}},{\"start\":\"9322\",\"end\":\"9326\",\"attributes\":{\"ref_id\":\"b12\"}},{\"start\":\"9359\",\"end\":\"9362\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"9363\",\"end\":\"9367\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"9621\",\"end\":\"9625\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"9626\",\"end\":\"9629\",\"attributes\":{\"ref_id\":\"b6\"}},{\"start\":\"9838\",\"end\":\"9842\",\"attributes\":{\"ref_id\":\"b32\"}},{\"start\":\"9926\",\"end\":\"9930\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"10016\",\"end\":\"10020\",\"attributes\":{\"ref_id\":\"b31\"}},{\"start\":\"10939\",\"end\":\"10943\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"14443\",\"end\":\"14447\",\"attributes\":{\"ref_id\":\"b25\"}},{\"start\":\"14717\",\"end\":\"14721\",\"attributes\":{\"ref_id\":\"b11\"}},{\"start\":\"15317\",\"end\":\"15321\",\"attributes\":{\"ref_id\":\"b22\"}},{\"start\":\"15999\",\"end\":\"16009\"},{\"start\":\"16864\",\"end\":\"16867\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"18392\",\"end\":\"18396\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"18867\",\"end\":\"18871\",\"attributes\":{\"ref_id\":\"b31\"}},{\"start\":\"22528\",\"end\":\"22532\",\"attributes\":{\"ref_id\":\"b32\"}},{\"start\":\"22695\",\"end\":\"22699\",\"attributes\":{\"ref_id\":\"b15\"}},{\"start\":\"22984\",\"end\":\"22988\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"25635\",\"end\":\"25639\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"26132\",\"end\":\"26136\",\"attributes\":{\"ref_id\":\"b30\"}},{\"start\":\"26449\",\"end\":\"26453\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"26711\",\"end\":\"26714\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"26721\",\"end\":\"26725\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"30274\",\"end\":\"30278\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"35071\",\"end\":\"35075\",\"attributes\":{\"ref_id\":\"b29\"}},{\"start\":\"35231\",\"end\":\"35234\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"43121\",\"end\":\"43122\",\"attributes\":{\"ref_id\":\"b1\"}}]", "figure": "[{\"start\":\"35594\",\"end\":\"35812\",\"attributes\":{\"id\":\"fig_0\"}},{\"start\":\"35813\",\"end\":\"35928\",\"attributes\":{\"id\":\"fig_1\"}},{\"start\":\"35929\",\"end\":\"35999\",\"attributes\":{\"id\":\"fig_2\"}},{\"start\":\"36000\",\"end\":\"36424\",\"attributes\":{\"id\":\"fig_3\"}},{\"start\":\"36425\",\"end\":\"36554\",\"attributes\":{\"id\":\"fig_4\"}},{\"start\":\"36555\",\"end\":\"36682\",\"attributes\":{\"id\":\"fig_6\"}},{\"start\":\"36683\",\"end\":\"36842\",\"attributes\":{\"id\":\"fig_7\"}},{\"start\":\"36843\",\"end\":\"37103\",\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"}},{\"start\":\"37104\",\"end\":\"38874\",\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"}},{\"start\":\"38875\",\"end\":\"40667\",\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"}},{\"start\":\"40668\",\"end\":\"42456\",\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"}},{\"start\":\"42457\",\"end\":\"43070\",\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"}}]", "paragraph": "[{\"start\":\"2377\",\"end\":\"3096\"},{\"start\":\"3098\",\"end\":\"3999\"},{\"start\":\"4001\",\"end\":\"4555\"},{\"start\":\"4557\",\"end\":\"5084\"},{\"start\":\"5086\",\"end\":\"5352\"},{\"start\":\"5354\",\"end\":\"6199\"},{\"start\":\"6201\",\"end\":\"6497\"},{\"start\":\"6514\",\"end\":\"6769\"},{\"start\":\"6771\",\"end\":\"8392\"},{\"start\":\"8394\",\"end\":\"9094\"},{\"start\":\"9096\",\"end\":\"9723\"},{\"start\":\"9725\",\"end\":\"10298\"},{\"start\":\"10333\",\"end\":\"10774\"},{\"start\":\"10776\",\"end\":\"11632\"},{\"start\":\"11655\",\"end\":\"12017\"},{\"start\":\"12019\",\"end\":\"12241\"},{\"start\":\"12243\",\"end\":\"12469\"},{\"start\":\"12489\",\"end\":\"12749\"},{\"start\":\"12774\",\"end\":\"13051\"},{\"start\":\"13053\",\"end\":\"13094\"},{\"start\":\"13128\",\"end\":\"13687\"},{\"start\":\"13736\",\"end\":\"15050\"},{\"start\":\"15074\",\"end\":\"16121\"},{\"start\":\"16142\",\"end\":\"16175\"},{\"start\":\"16209\",\"end\":\"16344\"},{\"start\":\"16375\",\"end\":\"16683\"},{\"start\":\"16685\",\"end\":\"17304\"},{\"start\":\"17360\",\"end\":\"18019\"},{\"start\":\"18044\",\"end\":\"18100\"},{\"start\":\"18127\",\"end\":\"19063\"},{\"start\":\"19065\",\"end\":\"19157\"},{\"start\":\"19224\",\"end\":\"19537\"},{\"start\":\"19711\",\"end\":\"19900\"},{\"start\":\"19972\",\"end\":\"20076\"},{\"start\":\"20078\",\"end\":\"20394\"},{\"start\":\"20410\",\"end\":\"20503\"},{\"start\":\"20505\",\"end\":\"20691\"},{\"start\":\"20693\",\"end\":\"20850\"},{\"start\":\"20852\",\"end\":\"21046\"},{\"start\":\"21048\",\"end\":\"21597\"},{\"start\":\"21599\",\"end\":\"21863\"},{\"start\":\"21865\",\"end\":\"22189\"},{\"start\":\"22191\",\"end\":\"23345\"},{\"start\":\"23368\",\"end\":\"24123\"},{\"start\":\"24145\",\"end\":\"24190\"},{\"start\":\"24192\",\"end\":\"24351\"},{\"start\":\"24353\",\"end\":\"24529\"},{\"start\":\"24531\",\"end\":\"25524\"},{\"start\":\"25526\",\"end\":\"26534\"},{\"start\":\"26536\",\"end\":\"26610\"},{\"start\":\"26612\",\"end\":\"26829\"},{\"start\":\"26862\",\"end\":\"27574\"},{\"start\":\"27576\",\"end\":\"29008\"},{\"start\":\"29010\",\"end\":\"29872\"},{\"start\":\"29874\",\"end\":\"30002\"},{\"start\":\"30034\",\"end\":\"30445\"},{\"start\":\"30447\",\"end\":\"31081\"},{\"start\":\"31083\",\"end\":\"32695\"},{\"start\":\"32697\",\"end\":\"33933\"},{\"start\":\"33975\",\"end\":\"34549\"},{\"start\":\"34580\",\"end\":\"35076\"},{\"start\":\"35078\",\"end\":\"35593\"}]", "formula": "[{\"start\":\"12750\",\"end\":\"12773\",\"attributes\":{\"id\":\"formula_0\"}},{\"start\":\"13095\",\"end\":\"13127\",\"attributes\":{\"id\":\"formula_1\"}},{\"start\":\"13688\",\"end\":\"13719\",\"attributes\":{\"id\":\"formula_2\"}},{\"start\":\"16122\",\"end\":\"16141\",\"attributes\":{\"id\":\"formula_3\"}},{\"start\":\"16176\",\"end\":\"16208\",\"attributes\":{\"id\":\"formula_4\"}},{\"start\":\"17305\",\"end\":\"17359\",\"attributes\":{\"id\":\"formula_5\"}},{\"start\":\"18020\",\"end\":\"18043\",\"attributes\":{\"id\":\"formula_6\"}},{\"start\":\"19158\",\"end\":\"19223\",\"attributes\":{\"id\":\"formula_7\"}},{\"start\":\"19538\",\"end\":\"19609\",\"attributes\":{\"id\":\"formula_8\"}},{\"start\":\"19609\",\"end\":\"19710\",\"attributes\":{\"id\":\"formula_9\"}},{\"start\":\"19901\",\"end\":\"19971\",\"attributes\":{\"id\":\"formula_10\"}}]", "table_ref": "[{\"start\":\"22275\",\"end\":\"22282\",\"attributes\":{\"ref_id\":\"tab_0\"}},{\"start\":\"26914\",\"end\":\"26943\",\"attributes\":{\"ref_id\":\"tab_1\"}}]", "section_header": "[{\"start\":\"2363\",\"end\":\"2375\",\"attributes\":{\"n\":\"1\"}},{\"start\":\"6500\",\"end\":\"6512\",\"attributes\":{\"n\":\"2\"}},{\"start\":\"10301\",\"end\":\"10331\",\"attributes\":{\"n\":\"3\"}},{\"start\":\"11635\",\"end\":\"11653\",\"attributes\":{\"n\":\"4\"}},{\"start\":\"12472\",\"end\":\"12487\",\"attributes\":{\"n\":\"5\"}},{\"start\":\"13721\",\"end\":\"13734\",\"attributes\":{\"n\":\"5.1\"}},{\"start\":\"15053\",\"end\":\"15072\",\"attributes\":{\"n\":\"5.2\"}},{\"start\":\"16347\",\"end\":\"16373\",\"attributes\":{\"n\":\"5.3\"}},{\"start\":\"18103\",\"end\":\"18125\",\"attributes\":{\"n\":\"5.4\"}},{\"start\":\"20397\",\"end\":\"20408\",\"attributes\":{\"n\":\"6\"}},{\"start\":\"23348\",\"end\":\"23366\",\"attributes\":{\"n\":\"6.1.3\"}},{\"start\":\"24126\",\"end\":\"24143\",\"attributes\":{\"n\":\"6.1.4\"}},{\"start\":\"26832\",\"end\":\"26860\",\"attributes\":{\"n\":\"6.2\"}},{\"start\":\"30005\",\"end\":\"30032\",\"attributes\":{\"n\":\"6.3\"}},{\"start\":\"33936\",\"end\":\"33973\",\"attributes\":{\"n\":\"6.4\"}},{\"start\":\"34552\",\"end\":\"34578\",\"attributes\":{\"n\":\"7\"}},{\"start\":\"35595\",\"end\":\"35605\"},{\"start\":\"35814\",\"end\":\"35824\"},{\"start\":\"35930\",\"end\":\"35940\"},{\"start\":\"36426\",\"end\":\"36436\"},{\"start\":\"36556\",\"end\":\"36566\"},{\"start\":\"36684\",\"end\":\"36694\"},{\"start\":\"36844\",\"end\":\"36853\"},{\"start\":\"37105\",\"end\":\"37114\"},{\"start\":\"38876\",\"end\":\"38885\"},{\"start\":\"40669\",\"end\":\"40678\"}]", "table": "[{\"start\":\"36898\",\"end\":\"37103\"},{\"start\":\"37224\",\"end\":\"38874\"},{\"start\":\"38999\",\"end\":\"40667\"},{\"start\":\"40790\",\"end\":\"42456\"},{\"start\":\"42983\",\"end\":\"43070\"}]", "figure_caption": "[{\"start\":\"35607\",\"end\":\"35812\"},{\"start\":\"35826\",\"end\":\"35928\"},{\"start\":\"35942\",\"end\":\"35999\"},{\"start\":\"36002\",\"end\":\"36424\"},{\"start\":\"36438\",\"end\":\"36554\"},{\"start\":\"36568\",\"end\":\"36682\"},{\"start\":\"36696\",\"end\":\"36842\"},{\"start\":\"36855\",\"end\":\"36898\"},{\"start\":\"37116\",\"end\":\"37224\"},{\"start\":\"38887\",\"end\":\"38999\"},{\"start\":\"40680\",\"end\":\"40790\"},{\"start\":\"42459\",\"end\":\"42983\"}]", "figure_ref": "[{\"start\":\"12531\",\"end\":\"12539\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"14847\",\"end\":\"14855\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"18471\",\"end\":\"18479\",\"attributes\":{\"ref_id\":\"fig_2\"}},{\"start\":\"31125\",\"end\":\"31133\",\"attributes\":{\"ref_id\":\"fig_4\"}},{\"start\":\"34146\",\"end\":\"34154\",\"attributes\":{\"ref_id\":\"fig_7\"}}]", "bib_author_first_name": "[{\"start\":\"43751\",\"end\":\"43758\"},{\"start\":\"43769\",\"end\":\"43778\"},{\"start\":\"43784\",\"end\":\"43790\"},{\"start\":\"44100\",\"end\":\"44104\"},{\"start\":\"44110\",\"end\":\"44118\"},{\"start\":\"44126\",\"end\":\"44134\"},{\"start\":\"44710\",\"end\":\"44714\"},{\"start\":\"44723\",\"end\":\"44728\"},{\"start\":\"44738\",\"end\":\"44745\"},{\"start\":\"44754\",\"end\":\"44758\"},{\"start\":\"45047\",\"end\":\"45053\"},{\"start\":\"45054\",\"end\":\"45055\"},{\"start\":\"45064\",\"end\":\"45068\"},{\"start\":\"45076\",\"end\":\"45082\"},{\"start\":\"45506\",\"end\":\"45507\"},{\"start\":\"45508\",\"end\":\"45509\"},{\"start\":\"45523\",\"end\":\"45524\"},{\"start\":\"45542\",\"end\":\"45543\"},{\"start\":\"45553\",\"end\":\"45558\"},{\"start\":\"45559\",\"end\":\"45560\"},{\"start\":\"45840\",\"end\":\"45848\"},{\"start\":\"45855\",\"end\":\"45862\"},{\"start\":\"45870\",\"end\":\"45878\"},{\"start\":\"45883\",\"end\":\"45890\"},{\"start\":\"45896\",\"end\":\"45899\"},{\"start\":\"45905\",\"end\":\"45913\"},{\"start\":\"46456\",\"end\":\"46465\"},{\"start\":\"46471\",\"end\":\"46475\"},{\"start\":\"46493\",\"end\":\"46500\"},{\"start\":\"46511\",\"end\":\"46517\"},{\"start\":\"46900\",\"end\":\"46906\"},{\"start\":\"46913\",\"end\":\"46921\"},{\"start\":\"46922\",\"end\":\"46926\"},{\"start\":\"46937\",\"end\":\"46941\"},{\"start\":\"46951\",\"end\":\"46952\"},{\"start\":\"46961\",\"end\":\"46967\"},{\"start\":\"47295\",\"end\":\"47304\"},{\"start\":\"47322\",\"end\":\"47328\"},{\"start\":\"47829\",\"end\":\"47833\"},{\"start\":\"47843\",\"end\":\"47851\"},{\"start\":\"47852\",\"end\":\"47853\"},{\"start\":\"48112\",\"end\":\"48118\"},{\"start\":\"48128\",\"end\":\"48133\"},{\"start\":\"48144\",\"end\":\"48154\"},{\"start\":\"48164\",\"end\":\"48169\"},{\"start\":\"48396\",\"end\":\"48404\"},{\"start\":\"48415\",\"end\":\"48420\"},{\"start\":\"48748\",\"end\":\"48753\"},{\"start\":\"48764\",\"end\":\"48770\"},{\"start\":\"49274\",\"end\":\"49280\"},{\"start\":\"49292\",\"end\":\"49297\"},{\"start\":\"49304\",\"end\":\"49309\"},{\"start\":\"49658\",\"end\":\"49666\"},{\"start\":\"49671\",\"end\":\"49674\"},{\"start\":\"49681\",\"end\":\"49688\"},{\"start\":\"49694\",\"end\":\"49698\"},{\"start\":\"50195\",\"end\":\"50203\"},{\"start\":\"50208\",\"end\":\"50215\"},{\"start\":\"50220\",\"end\":\"50228\"},{\"start\":\"50235\",\"end\":\"50244\"},{\"start\":\"50250\",\"end\":\"50257\"},{\"start\":\"50265\",\"end\":\"50273\"},{\"start\":\"50600\",\"end\":\"50601\"},{\"start\":\"50612\",\"end\":\"50617\"},{\"start\":\"50947\",\"end\":\"50954\"},{\"start\":\"50962\",\"end\":\"50969\"},{\"start\":\"50978\",\"end\":\"50983\"},{\"start\":\"50984\",\"end\":\"50985\"},{\"start\":\"51330\",\"end\":\"51339\"},{\"start\":\"51346\",\"end\":\"51353\"},{\"start\":\"51361\",\"end\":\"51365\"},{\"start\":\"51663\",\"end\":\"51667\"},{\"start\":\"51675\",\"end\":\"51681\"},{\"start\":\"52102\",\"end\":\"52109\"},{\"start\":\"52118\",\"end\":\"52127\"},{\"start\":\"52143\",\"end\":\"52147\"},{\"start\":\"52704\",\"end\":\"52705\"},{\"start\":\"52706\",\"end\":\"52707\"},{\"start\":\"53105\",\"end\":\"53110\"},{\"start\":\"53119\",\"end\":\"53123\"},{\"start\":\"53130\",\"end\":\"53135\"},{\"start\":\"53405\",\"end\":\"53409\"},{\"start\":\"53421\",\"end\":\"53426\"},{\"start\":\"53436\",\"end\":\"53442\"},{\"start\":\"53698\",\"end\":\"53707\"},{\"start\":\"53713\",\"end\":\"53720\"},{\"start\":\"53732\",\"end\":\"53737\"},{\"start\":\"53859\",\"end\":\"53865\"},{\"start\":\"53875\",\"end\":\"53879\"},{\"start\":\"53889\",\"end\":\"53893\"},{\"start\":\"53902\",\"end\":\"53907\"},{\"start\":\"53919\",\"end\":\"53924\"},{\"start\":\"53932\",\"end\":\"53937\"},{\"start\":\"53938\",\"end\":\"53939\"},{\"start\":\"53947\",\"end\":\"53953\"},{\"start\":\"53962\",\"end\":\"53967\"},{\"start\":\"54304\",\"end\":\"54309\"},{\"start\":\"54319\",\"end\":\"54328\"},{\"start\":\"54337\",\"end\":\"54341\"},{\"start\":\"54350\",\"end\":\"54357\"},{\"start\":\"54734\",\"end\":\"54741\"},{\"start\":\"54748\",\"end\":\"54755\"},{\"start\":\"54761\",\"end\":\"54767\"},{\"start\":\"54773\",\"end\":\"54776\"},{\"start\":\"54781\",\"end\":\"54790\"},{\"start\":\"54796\",\"end\":\"54801\"},{\"start\":\"55351\",\"end\":\"55356\"},{\"start\":\"55363\",\"end\":\"55371\"},{\"start\":\"55376\",\"end\":\"55380\"},{\"start\":\"55387\",\"end\":\"55391\"},{\"start\":\"55398\",\"end\":\"55406\"},{\"start\":\"55915\",\"end\":\"55919\"},{\"start\":\"55924\",\"end\":\"55929\"},{\"start\":\"55935\",\"end\":\"55938\"},{\"start\":\"55943\",\"end\":\"55948\"},{\"start\":\"55955\",\"end\":\"55961\"},{\"start\":\"56523\",\"end\":\"56531\"},{\"start\":\"56539\",\"end\":\"56546\"},{\"start\":\"56866\",\"end\":\"56874\"},{\"start\":\"56882\",\"end\":\"56889\"}]", "bib_author_last_name": "[{\"start\":\"43394\",\"end\":\"43417\"},{\"start\":\"43759\",\"end\":\"43767\"},{\"start\":\"43779\",\"end\":\"43782\"},{\"start\":\"43791\",\"end\":\"43797\"},{\"start\":\"44105\",\"end\":\"44108\"},{\"start\":\"44119\",\"end\":\"44124\"},{\"start\":\"44135\",\"end\":\"44151\"},{\"start\":\"44153\",\"end\":\"44160\"},{\"start\":\"44715\",\"end\":\"44721\"},{\"start\":\"44729\",\"end\":\"44736\"},{\"start\":\"44746\",\"end\":\"44752\"},{\"start\":\"44759\",\"end\":\"44766\"},{\"start\":\"45056\",\"end\":\"45062\"},{\"start\":\"45069\",\"end\":\"45074\"},{\"start\":\"45083\",\"end\":\"45090\"},{\"start\":\"45510\",\"end\":\"45516\"},{\"start\":\"45518\",\"end\":\"45521\"},{\"start\":\"45525\",\"end\":\"45531\"},{\"start\":\"45533\",\"end\":\"45540\"},{\"start\":\"45544\",\"end\":\"45551\"},{\"start\":\"45561\",\"end\":\"45568\"},{\"start\":\"45570\",\"end\":\"45575\"},{\"start\":\"45849\",\"end\":\"45853\"},{\"start\":\"45863\",\"end\":\"45868\"},{\"start\":\"45879\",\"end\":\"45881\"},{\"start\":\"45891\",\"end\":\"45894\"},{\"start\":\"45900\",\"end\":\"45903\"},{\"start\":\"45914\",\"end\":\"45918\"},{\"start\":\"46466\",\"end\":\"46469\"},{\"start\":\"46476\",\"end\":\"46491\"},{\"start\":\"46501\",\"end\":\"46509\"},{\"start\":\"46518\",\"end\":\"46524\"},{\"start\":\"46907\",\"end\":\"46911\"},{\"start\":\"46927\",\"end\":\"46935\"},{\"start\":\"46942\",\"end\":\"46949\"},{\"start\":\"46953\",\"end\":\"46959\"},{\"start\":\"46968\",\"end\":\"46975\"},{\"start\":\"46977\",\"end\":\"46980\"},{\"start\":\"47305\",\"end\":\"47320\"},{\"start\":\"47329\",\"end\":\"47336\"},{\"start\":\"47834\",\"end\":\"47841\"},{\"start\":\"47854\",\"end\":\"47861\"},{\"start\":\"47863\",\"end\":\"47870\"},{\"start\":\"48119\",\"end\":\"48126\"},{\"start\":\"48134\",\"end\":\"48142\"},{\"start\":\"48155\",\"end\":\"48162\"},{\"start\":\"48170\",\"end\":\"48174\"},{\"start\":\"48405\",\"end\":\"48413\"},{\"start\":\"48421\",\"end\":\"48428\"},{\"start\":\"48754\",\"end\":\"48762\"},{\"start\":\"48771\",\"end\":\"48779\"},{\"start\":\"49281\",\"end\":\"49290\"},{\"start\":\"49298\",\"end\":\"49302\"},{\"start\":\"49310\",\"end\":\"49315\"},{\"start\":\"49667\",\"end\":\"49669\"},{\"start\":\"49675\",\"end\":\"49679\"},{\"start\":\"49689\",\"end\":\"49692\"},{\"start\":\"49699\",\"end\":\"49703\"},{\"start\":\"50204\",\"end\":\"50206\"},{\"start\":\"50216\",\"end\":\"50218\"},{\"start\":\"50229\",\"end\":\"50233\"},{\"start\":\"50245\",\"end\":\"50248\"},{\"start\":\"50258\",\"end\":\"50263\"},{\"start\":\"50274\",\"end\":\"50278\"},{\"start\":\"50602\",\"end\":\"50610\"},{\"start\":\"50618\",\"end\":\"50624\"},{\"start\":\"50626\",\"end\":\"50628\"},{\"start\":\"50955\",\"end\":\"50960\"},{\"start\":\"50970\",\"end\":\"50976\"},{\"start\":\"50986\",\"end\":\"50992\"},{\"start\":\"51340\",\"end\":\"51344\"},{\"start\":\"51354\",\"end\":\"51359\"},{\"start\":\"51366\",\"end\":\"51372\"},{\"start\":\"51668\",\"end\":\"51673\"},{\"start\":\"51682\",\"end\":\"51689\"},{\"start\":\"52110\",\"end\":\"52116\"},{\"start\":\"52128\",\"end\":\"52141\"},{\"start\":\"52148\",\"end\":\"52162\"},{\"start\":\"52685\",\"end\":\"52692\"},{\"start\":\"52694\",\"end\":\"52702\"},{\"start\":\"52708\",\"end\":\"52719\"},{\"start\":\"52721\",\"end\":\"52728\"},{\"start\":\"53111\",\"end\":\"53117\"},{\"start\":\"53124\",\"end\":\"53128\"},{\"start\":\"53136\",\"end\":\"53139\"},{\"start\":\"53410\",\"end\":\"53419\"},{\"start\":\"53427\",\"end\":\"53434\"},{\"start\":\"53443\",\"end\":\"53445\"},{\"start\":\"53708\",\"end\":\"53711\"},{\"start\":\"53721\",\"end\":\"53730\"},{\"start\":\"53738\",\"end\":\"53743\"},{\"start\":\"53866\",\"end\":\"53873\"},{\"start\":\"53880\",\"end\":\"53887\"},{\"start\":\"53894\",\"end\":\"53900\"},{\"start\":\"53908\",\"end\":\"53917\"},{\"start\":\"53925\",\"end\":\"53930\"},{\"start\":\"53940\",\"end\":\"53945\"},{\"start\":\"53954\",\"end\":\"53960\"},{\"start\":\"53968\",\"end\":\"53978\"},{\"start\":\"54310\",\"end\":\"54317\"},{\"start\":\"54329\",\"end\":\"54335\"},{\"start\":\"54342\",\"end\":\"54348\"},{\"start\":\"54358\",\"end\":\"54363\"},{\"start\":\"54742\",\"end\":\"54746\"},{\"start\":\"54756\",\"end\":\"54759\"},{\"start\":\"54768\",\"end\":\"54771\"},{\"start\":\"54777\",\"end\":\"54779\"},{\"start\":\"54791\",\"end\":\"54794\"},{\"start\":\"54802\",\"end\":\"54807\"},{\"start\":\"55357\",\"end\":\"55361\"},{\"start\":\"55372\",\"end\":\"55374\"},{\"start\":\"55381\",\"end\":\"55385\"},{\"start\":\"55392\",\"end\":\"55396\"},{\"start\":\"55407\",\"end\":\"55411\"},{\"start\":\"55920\",\"end\":\"55922\"},{\"start\":\"55930\",\"end\":\"55933\"},{\"start\":\"55939\",\"end\":\"55941\"},{\"start\":\"55949\",\"end\":\"55953\"},{\"start\":\"55962\",\"end\":\"55965\"},{\"start\":\"56532\",\"end\":\"56537\"},{\"start\":\"56547\",\"end\":\"56551\"},{\"start\":\"56875\",\"end\":\"56880\"},{\"start\":\"56890\",\"end\":\"56894\"}]", "bib_entry": "[{\"start\":\"43276\",\"end\":\"43678\",\"attributes\":{\"matched_paper_id\":\"59378336\",\"id\":\"b0\"}},{\"start\":\"43680\",\"end\":\"43997\",\"attributes\":{\"id\":\"b1\",\"doi\":\"arXiv:1409.0473\"}},{\"start\":\"43999\",\"end\":\"44633\",\"attributes\":{\"matched_paper_id\":\"50766964\",\"id\":\"b2\"}},{\"start\":\"44635\",\"end\":\"45026\",\"attributes\":{\"matched_paper_id\":\"1820089\",\"id\":\"b3\"}},{\"start\":\"45028\",\"end\":\"45457\",\"attributes\":{\"matched_paper_id\":\"46939182\",\"id\":\"b4\"}},{\"start\":\"45459\",\"end\":\"45736\",\"attributes\":{\"id\":\"b5\"}},{\"start\":\"45738\",\"end\":\"46454\",\"attributes\":{\"matched_paper_id\":\"20970043\",\"id\":\"b6\"}},{\"start\":\"46456\",\"end\":\"46829\",\"attributes\":{\"id\":\"b7\",\"doi\":\"arXiv:1409.1259\"}},{\"start\":\"46831\",\"end\":\"47241\",\"attributes\":{\"matched_paper_id\":\"5842463\",\"id\":\"b8\"}},{\"start\":\"47243\",\"end\":\"47740\",\"attributes\":{\"matched_paper_id\":\"50773451\",\"id\":\"b9\"}},{\"start\":\"47742\",\"end\":\"48037\",\"attributes\":{\"matched_paper_id\":\"29151031\",\"id\":\"b10\"}},{\"start\":\"48039\",\"end\":\"48394\",\"attributes\":{\"id\":\"b11\",\"doi\":\"arXiv:1605.02276\"}},{\"start\":\"48396\",\"end\":\"48707\",\"attributes\":{\"id\":\"b12\",\"doi\":\"arXiv:1704.04110\"}},{\"start\":\"48709\",\"end\":\"49148\",\"attributes\":{\"matched_paper_id\":\"5241746\",\"id\":\"b13\"}},{\"start\":\"49150\",\"end\":\"49587\",\"attributes\":{\"matched_paper_id\":\"1368427\",\"id\":\"b14\"}},{\"start\":\"49589\",\"end\":\"50128\",\"attributes\":{\"matched_paper_id\":\"2023519\",\"id\":\"b15\"}},{\"start\":\"50130\",\"end\":\"50554\",\"attributes\":{\"matched_paper_id\":\"206744664\",\"id\":\"b16\"}},{\"start\":\"50556\",\"end\":\"50781\",\"attributes\":{\"id\":\"b17\",\"doi\":\"arXiv:1412.6980\"}},{\"start\":\"50783\",\"end\":\"51290\",\"attributes\":{\"matched_paper_id\":\"5027797\",\"id\":\"b18\"}},{\"start\":\"51292\",\"end\":\"51584\",\"attributes\":{\"matched_paper_id\":\"17195923\",\"id\":\"b19\"}},{\"start\":\"51586\",\"end\":\"52029\",\"attributes\":{\"matched_paper_id\":\"742515\",\"id\":\"b20\"}},{\"start\":\"52031\",\"end\":\"52515\",\"attributes\":{\"matched_paper_id\":\"207178809\",\"id\":\"b21\"}},{\"start\":\"52517\",\"end\":\"53008\",\"attributes\":{\"matched_paper_id\":\"11258378\",\"id\":\"b22\"}},{\"start\":\"53010\",\"end\":\"53351\",\"attributes\":{\"id\":\"b23\",\"doi\":\"arXiv:1610.09975\"}},{\"start\":\"53353\",\"end\":\"53667\",\"attributes\":{\"matched_paper_id\":\"7961699\",\"id\":\"b24\"}},{\"start\":\"53669\",\"end\":\"53830\",\"attributes\":{\"id\":\"b25\"}},{\"start\":\"53832\",\"end\":\"54253\",\"attributes\":{\"matched_paper_id\":\"13756489\",\"id\":\"b26\"}},{\"start\":\"54255\",\"end\":\"54658\",\"attributes\":{\"matched_paper_id\":\"1169492\",\"id\":\"b27\"}},{\"start\":\"54660\",\"end\":\"55311\",\"attributes\":{\"matched_paper_id\":\"4002880\",\"id\":\"b28\"}},{\"start\":\"55313\",\"end\":\"55855\",\"attributes\":{\"matched_paper_id\":\"150380651\",\"id\":\"b29\"}},{\"start\":\"55857\",\"end\":\"56428\",\"attributes\":{\"matched_paper_id\":\"2023817\",\"id\":\"b30\"}},{\"start\":\"56430\",\"end\":\"56819\",\"attributes\":{\"matched_paper_id\":\"97681\",\"id\":\"b31\"}},{\"start\":\"56821\",\"end\":\"57111\",\"attributes\":{\"matched_paper_id\":\"1008003\",\"id\":\"b32\"}}]", "bib_title": "[{\"start\":\"43276\",\"end\":\"43392\"},{\"start\":\"43999\",\"end\":\"44098\"},{\"start\":\"44635\",\"end\":\"44708\"},{\"start\":\"45028\",\"end\":\"45045\"},{\"start\":\"45738\",\"end\":\"45838\"},{\"start\":\"46831\",\"end\":\"46898\"},{\"start\":\"47243\",\"end\":\"47293\"},{\"start\":\"47742\",\"end\":\"47827\"},{\"start\":\"48709\",\"end\":\"48746\"},{\"start\":\"49150\",\"end\":\"49272\"},{\"start\":\"49589\",\"end\":\"49656\"},{\"start\":\"50130\",\"end\":\"50193\"},{\"start\":\"50783\",\"end\":\"50945\"},{\"start\":\"51292\",\"end\":\"51328\"},{\"start\":\"51586\",\"end\":\"51661\"},{\"start\":\"52031\",\"end\":\"52100\"},{\"start\":\"52517\",\"end\":\"52683\"},{\"start\":\"53353\",\"end\":\"53403\"},{\"start\":\"53832\",\"end\":\"53857\"},{\"start\":\"54255\",\"end\":\"54302\"},{\"start\":\"54660\",\"end\":\"54732\"},{\"start\":\"55313\",\"end\":\"55349\"},{\"start\":\"55857\",\"end\":\"55913\"},{\"start\":\"56430\",\"end\":\"56521\"},{\"start\":\"56821\",\"end\":\"56864\"}]", "bib_author": "[{\"start\":\"43394\",\"end\":\"43419\"},{\"start\":\"43751\",\"end\":\"43769\"},{\"start\":\"43769\",\"end\":\"43784\"},{\"start\":\"43784\",\"end\":\"43799\"},{\"start\":\"44100\",\"end\":\"44110\"},{\"start\":\"44110\",\"end\":\"44126\"},{\"start\":\"44126\",\"end\":\"44153\"},{\"start\":\"44153\",\"end\":\"44162\"},{\"start\":\"44710\",\"end\":\"44723\"},{\"start\":\"44723\",\"end\":\"44738\"},{\"start\":\"44738\",\"end\":\"44754\"},{\"start\":\"44754\",\"end\":\"44768\"},{\"start\":\"45047\",\"end\":\"45064\"},{\"start\":\"45064\",\"end\":\"45076\"},{\"start\":\"45076\",\"end\":\"45092\"},{\"start\":\"45506\",\"end\":\"45518\"},{\"start\":\"45518\",\"end\":\"45523\"},{\"start\":\"45523\",\"end\":\"45533\"},{\"start\":\"45533\",\"end\":\"45542\"},{\"start\":\"45542\",\"end\":\"45553\"},{\"start\":\"45553\",\"end\":\"45570\"},{\"start\":\"45570\",\"end\":\"45577\"},{\"start\":\"45840\",\"end\":\"45855\"},{\"start\":\"45855\",\"end\":\"45870\"},{\"start\":\"45870\",\"end\":\"45883\"},{\"start\":\"45883\",\"end\":\"45896\"},{\"start\":\"45896\",\"end\":\"45905\"},{\"start\":\"45905\",\"end\":\"45920\"},{\"start\":\"46456\",\"end\":\"46471\"},{\"start\":\"46471\",\"end\":\"46493\"},{\"start\":\"46493\",\"end\":\"46511\"},{\"start\":\"46511\",\"end\":\"46526\"},{\"start\":\"46900\",\"end\":\"46913\"},{\"start\":\"46913\",\"end\":\"46937\"},{\"start\":\"46937\",\"end\":\"46951\"},{\"start\":\"46951\",\"end\":\"46961\"},{\"start\":\"46961\",\"end\":\"46977\"},{\"start\":\"46977\",\"end\":\"46982\"},{\"start\":\"47295\",\"end\":\"47322\"},{\"start\":\"47322\",\"end\":\"47338\"},{\"start\":\"47829\",\"end\":\"47843\"},{\"start\":\"47843\",\"end\":\"47863\"},{\"start\":\"47863\",\"end\":\"47872\"},{\"start\":\"48112\",\"end\":\"48128\"},{\"start\":\"48128\",\"end\":\"48144\"},{\"start\":\"48144\",\"end\":\"48164\"},{\"start\":\"48164\",\"end\":\"48176\"},{\"start\":\"48396\",\"end\":\"48415\"},{\"start\":\"48415\",\"end\":\"48430\"},{\"start\":\"48748\",\"end\":\"48764\"},{\"start\":\"48764\",\"end\":\"48781\"},{\"start\":\"49274\",\"end\":\"49292\"},{\"start\":\"49292\",\"end\":\"49304\"},{\"start\":\"49304\",\"end\":\"49317\"},{\"start\":\"49658\",\"end\":\"49671\"},{\"start\":\"49671\",\"end\":\"49681\"},{\"start\":\"49681\",\"end\":\"49694\"},{\"start\":\"49694\",\"end\":\"49705\"},{\"start\":\"50195\",\"end\":\"50208\"},{\"start\":\"50208\",\"end\":\"50220\"},{\"start\":\"50220\",\"end\":\"50235\"},{\"start\":\"50235\",\"end\":\"50250\"},{\"start\":\"50250\",\"end\":\"50265\"},{\"start\":\"50265\",\"end\":\"50280\"},{\"start\":\"50600\",\"end\":\"50612\"},{\"start\":\"50612\",\"end\":\"50626\"},{\"start\":\"50626\",\"end\":\"50630\"},{\"start\":\"50947\",\"end\":\"50962\"},{\"start\":\"50962\",\"end\":\"50978\"},{\"start\":\"50978\",\"end\":\"50994\"},{\"start\":\"51330\",\"end\":\"51346\"},{\"start\":\"51346\",\"end\":\"51361\"},{\"start\":\"51361\",\"end\":\"51374\"},{\"start\":\"51663\",\"end\":\"51675\"},{\"start\":\"51675\",\"end\":\"51691\"},{\"start\":\"52102\",\"end\":\"52118\"},{\"start\":\"52118\",\"end\":\"52143\"},{\"start\":\"52143\",\"end\":\"52164\"},{\"start\":\"52685\",\"end\":\"52694\"},{\"start\":\"52694\",\"end\":\"52704\"},{\"start\":\"52704\",\"end\":\"52721\"},{\"start\":\"52721\",\"end\":\"52730\"},{\"start\":\"53105\",\"end\":\"53119\"},{\"start\":\"53119\",\"end\":\"53130\"},{\"start\":\"53130\",\"end\":\"53141\"},{\"start\":\"53405\",\"end\":\"53421\"},{\"start\":\"53421\",\"end\":\"53436\"},{\"start\":\"53436\",\"end\":\"53447\"},{\"start\":\"53698\",\"end\":\"53713\"},{\"start\":\"53713\",\"end\":\"53732\"},{\"start\":\"53732\",\"end\":\"53745\"},{\"start\":\"53859\",\"end\":\"53875\"},{\"start\":\"53875\",\"end\":\"53889\"},{\"start\":\"53889\",\"end\":\"53902\"},{\"start\":\"53902\",\"end\":\"53919\"},{\"start\":\"53919\",\"end\":\"53932\"},{\"start\":\"53932\",\"end\":\"53947\"},{\"start\":\"53947\",\"end\":\"53962\"},{\"start\":\"53962\",\"end\":\"53980\"},{\"start\":\"54304\",\"end\":\"54319\"},{\"start\":\"54319\",\"end\":\"54337\"},{\"start\":\"54337\",\"end\":\"54350\"},{\"start\":\"54350\",\"end\":\"54365\"},{\"start\":\"54734\",\"end\":\"54748\"},{\"start\":\"54748\",\"end\":\"54761\"},{\"start\":\"54761\",\"end\":\"54773\"},{\"start\":\"54773\",\"end\":\"54781\"},{\"start\":\"54781\",\"end\":\"54796\"},{\"start\":\"54796\",\"end\":\"54809\"},{\"start\":\"55351\",\"end\":\"55363\"},{\"start\":\"55363\",\"end\":\"55376\"},{\"start\":\"55376\",\"end\":\"55387\"},{\"start\":\"55387\",\"end\":\"55398\"},{\"start\":\"55398\",\"end\":\"55413\"},{\"start\":\"55915\",\"end\":\"55924\"},{\"start\":\"55924\",\"end\":\"55935\"},{\"start\":\"55935\",\"end\":\"55943\"},{\"start\":\"55943\",\"end\":\"55955\"},{\"start\":\"55955\",\"end\":\"55967\"},{\"start\":\"56523\",\"end\":\"56539\"},{\"start\":\"56539\",\"end\":\"56553\"},{\"start\":\"56866\",\"end\":\"56882\"},{\"start\":\"56882\",\"end\":\"56896\"}]", "bib_venue": "[{\"start\":\"43419\",\"end\":\"43462\"},{\"start\":\"43680\",\"end\":\"43749\"},{\"start\":\"44162\",\"end\":\"44258\"},{\"start\":\"44768\",\"end\":\"44817\"},{\"start\":\"45092\",\"end\":\"45188\"},{\"start\":\"45459\",\"end\":\"45504\"},{\"start\":\"45920\",\"end\":\"46031\"},{\"start\":\"46541\",\"end\":\"46616\"},{\"start\":\"46982\",\"end\":\"47024\"},{\"start\":\"47338\",\"end\":\"47434\"},{\"start\":\"47872\",\"end\":\"47876\"},{\"start\":\"48039\",\"end\":\"48110\"},{\"start\":\"48446\",\"end\":\"48518\"},{\"start\":\"48781\",\"end\":\"48873\"},{\"start\":\"49317\",\"end\":\"49349\"},{\"start\":\"49705\",\"end\":\"49800\"},{\"start\":\"50280\",\"end\":\"50331\"},{\"start\":\"50556\",\"end\":\"50598\"},{\"start\":\"50994\",\"end\":\"51018\"},{\"start\":\"51374\",\"end\":\"51423\"},{\"start\":\"51691\",\"end\":\"51765\"},{\"start\":\"52164\",\"end\":\"52230\"},{\"start\":\"52730\",\"end\":\"52742\"},{\"start\":\"53010\",\"end\":\"53103\"},{\"start\":\"53447\",\"end\":\"53496\"},{\"start\":\"53669\",\"end\":\"53696\"},{\"start\":\"53980\",\"end\":\"54029\"},{\"start\":\"54365\",\"end\":\"54442\"},{\"start\":\"54809\",\"end\":\"54920\"},{\"start\":\"55413\",\"end\":\"55524\"},{\"start\":\"55967\",\"end\":\"56078\"},{\"start\":\"56553\",\"end\":\"56604\"},{\"start\":\"56896\",\"end\":\"56947\"},{\"start\":\"44260\",\"end\":\"44341\"},{\"start\":\"45190\",\"end\":\"45271\"},{\"start\":\"46033\",\"end\":\"46129\"},{\"start\":\"47436\",\"end\":\"47517\"},{\"start\":\"48875\",\"end\":\"48952\"},{\"start\":\"49802\",\"end\":\"49882\"},{\"start\":\"51767\",\"end\":\"51826\"},{\"start\":\"52232\",\"end\":\"52283\"},{\"start\":\"54922\",\"end\":\"55018\"},{\"start\":\"55526\",\"end\":\"55622\"},{\"start\":\"56080\",\"end\":\"56176\"}]"}}}, "year": 2023, "month": 12, "day": 17}