{"id": 253098274, "updated": "2023-11-25 15:19:33.972", "metadata": {"title": "Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks", "authors": "[{\"first\":\"Yizhong\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Swaroop\",\"last\":\"Mishra\",\"middle\":[]},{\"first\":\"Pegah\",\"last\":\"Alipoormolabashi\",\"middle\":[]},{\"first\":\"Yeganeh\",\"last\":\"Kordi\",\"middle\":[]},{\"first\":\"Amirreza\",\"last\":\"Mirzaei\",\"middle\":[]},{\"first\":\"Atharva\",\"last\":\"Naik\",\"middle\":[]},{\"first\":\"Arjun\",\"last\":\"Ashok\",\"middle\":[]},{\"first\":\"Arut Selvan\",\"last\":\"Dhanasekaran\",\"middle\":[]},{\"first\":\"Anjana\",\"last\":\"Arunkumar\",\"middle\":[]},{\"first\":\"David\",\"last\":\"Stap\",\"middle\":[]},{\"first\":\"Eshaan\",\"last\":\"Pathak\",\"middle\":[]},{\"first\":\"Giannis\",\"last\":\"Karamanolakis\",\"middle\":[]},{\"first\":\"Haizhi\",\"last\":\"Lai\",\"middle\":[]},{\"first\":\"Ishan\",\"last\":\"Purohit\",\"middle\":[]},{\"first\":\"Ishani\",\"last\":\"Mondal\",\"middle\":[]},{\"first\":\"Jacob\",\"last\":\"Anderson\",\"middle\":[]},{\"first\":\"Kirby\",\"last\":\"Kuznia\",\"middle\":[]},{\"first\":\"Krima\",\"last\":\"Doshi\",\"middle\":[]},{\"first\":\"Kuntal Kumar\",\"last\":\"Pal\",\"middle\":[]},{\"first\":\"Maitreya\",\"last\":\"Patel\",\"middle\":[]},{\"first\":\"Mehrad\",\"last\":\"Moradshahi\",\"middle\":[]},{\"first\":\"Mihir\",\"last\":\"Parmar\",\"middle\":[]},{\"first\":\"Mirali\",\"last\":\"Purohit\",\"middle\":[]},{\"first\":\"Neeraj\",\"last\":\"Varshney\",\"middle\":[]},{\"first\":\"Phani Rohitha\",\"last\":\"Kaza\",\"middle\":[]},{\"first\":\"Pulkit\",\"last\":\"Verma\",\"middle\":[]},{\"first\":\"Ravsehaj Singh\",\"last\":\"Puri\",\"middle\":[]},{\"first\":\"Rushang\",\"last\":\"Karia\",\"middle\":[]},{\"first\":\"Savan\",\"last\":\"Doshi\",\"middle\":[]},{\"first\":\"Shailaja Keyur\",\"last\":\"Sampat\",\"middle\":[]},{\"first\":\"Siddhartha\",\"last\":\"Mishra\",\"middle\":[]},{\"first\":\"Sujan\",\"last\":\"Reddy A\",\"middle\":[]},{\"first\":\"Sumanta\",\"last\":\"Patro\",\"middle\":[]},{\"first\":\"Tanay\",\"last\":\"Dixit\",\"middle\":[]},{\"first\":\"Xudong\",\"last\":\"Shen\",\"middle\":[]}]", "venue": "EMNLP", "journal": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "How well can NLP models generalize to a variety of unseen tasks when provided with task instructions? To address this question, we first introduce Super-NaturalInstructions, a benchmark of 1,616 diverse NLP tasks and their expert-written instructions. Our collection covers 76 distinct task types, including but not limited to classification, extraction, infilling, sequence tagging, text rewriting, and text composition. This large and diverse collection of tasks enables rigorous benchmarking of cross-task generalization under instructions\u2014training models to follow instructions on a subset of tasks and evaluating them on the remaining unseen ones.Furthermore, we build Tk-Instruct, a transformer model trained to follow a variety of in-context instructions (plain language task definitions or k-shot examples). Our experiments show that Tk-Instruct outperforms existing instruction-following models such as InstructGPT by over 9% on our benchmark despite being an order of magnitude smaller. We further analyze generalization as a function of various scaling parameters, such as the number of observed tasks, the number of instances per task, and model sizes. We hope our dataset and model facilitate future progress towards more general-purpose NLP models.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": "2022.emnlp-main.340", "pubmed": null, "pubmedcentral": null, "dblp": "conf/emnlp/WangMAKMNADASPK22", "doi": "10.18653/v1/2022.emnlp-main.340"}}, "content": {"source": {"pdf_hash": "cd57e7566e15f7c07e86a476bdd454746249c6f0", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclanthology.org/2022.emnlp-main.340.pdf\"]", "oa_url_match": true, "oa_info": {"license": "CCBY", "open_access_url": "https://aclanthology.org/2022.emnlp-main.340.pdf", "status": "HYBRID"}}, "grobid": {"id": "761c5d1ef9ee36224280ded3ed58383c09d8662b", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/cd57e7566e15f7c07e86a476bdd454746249c6f0.txt", "contents": "\nSUPER-NATURALINSTRUCTIONS: Generalization via Declarative Instructions on 1600+ NLP Tasks\nDecember 7-11, 2022\n\nYizhong Wang \nUniv. of Washington\n\n\nSwaroop Mishra \nArizona State Univ\n\n\n\u2663 Pegah \nAlipoormolabashi \nYeganeh Kordi \nSharif Univ. of Tech\n\n\nTehran Polytechnic\n\n\nAmirreza Mirzaei \nSharif Univ. of Tech\n\n\nAnjana Arunkumar \nArizona State Univ\n\n\nArjun Ashok \nPSG College of Tech\n7 IITKharagpur\n\nArut Selvan Dhanasekaran \nArizona State Univ\n\n\nAtharva Naik \nDavid Stap \nUniv. of Amsterdam\n\n\nEshaan Pathak \nBerkeley\n\nGiannis Karamanolakis \nColumbia Univ\n\n\nHaizhi Gary Lai \nIshan Purohit \nIshani Mondal \nJacob Anderson \nArizona State Univ\n\n\nKirby Kuznia \nArizona State Univ\n\n\nKrima Doshi \nArizona State Univ\n\n\nMaitreya Patel \nArizona State Univ\n\n\nKuntal Kumar Pal \nArizona State Univ\n\n\nMehrad Moradshahi \nMihir Parmar \nArizona State Univ\n\n\nMirali Purohit \nNeeraj Varshney \nArizona State Univ\n\n\nPhani Rohitha Kaza \nArizona State Univ\n\n\nPulkit Verma \nArizona State Univ\n\n\nRavsehaj Singh Puri \nArizona State Univ\n\n\nRushang Karia \nArizona State Univ\n\n\nShailaja Keyur Sampat \nArizona State Univ\n\n\nSavan Doshi \nArizona State Univ\n\n\nSiddhartha Mishra \nSujan Reddy \nSumanta Patro \nTanay Dixit \nXudong Shen \nChitta Baral \nArizona State Univ\n\n\nYejin Choi \nAllen Institute for AI\n\n\nUniv. of Washington\n\n\nNoah A Smith \nAllen Institute for AI\n\n\nUniv. of Washington\n\n\nHannaneh Hajishirzi \nAllen Institute for AI\n\n\nUniv. of Washington\n\n\nDaniel Khashabi \nSUPER-NATURALINSTRUCTIONS: Generalization via Declarative Instructions on 1600+ NLP Tasks\n\n11 Factored AI 12 Govt. Polytechnic Rajkot 13 Microsoft Research 14 Stanford Univ. 15 Zycus Infotech 16 Univ. of Massachusetts Amherst 17 National Inst. of Tech. Karnataka 18 TCS Research 19 IIT Madras 20\nDecember 7-11, 2022\nHow well can NLP models generalize to a variety of unseen tasks when provided with task instructions? To address this question, we first introduce SUPER-NATURALINSTRUCTIONS, 1 a benchmark of 1,616 diverse NLP tasks and their expert-written instructions. Our collection covers 76 distinct task types, including but not limited to classification, extraction, infilling, sequence tagging, text rewriting, and text composition. This large and diverse collection of tasks enables rigorous benchmarking of cross-task generalization under instructionstraining models to follow instructions on a subset of tasks and evaluating them on the remaining unseen ones. Furthermore, we build Tk-INSTRUCT, a transformer model trained to follow a variety of in-context instructions (plain language task definitions or k-shot examples). Our experiments show that Tk-INSTRUCT outperforms existing instruction-following models such as Instruct-GPT by over 9% on our benchmark despite being an order of magnitude smaller. We further analyze generalization as a function of various scaling parameters, such as the number of observed tasks, the number of instances per task, and model sizes. We hope our dataset and model facilitate future progress towards more general-purpose NLP models. 2\n\nIntroduction\n\nThe NLP community has witnessed great progress in building models for generalization to unseen tasks via in-context instructions (Mishra et al., 1 SUPER-NATURALINSTRUCTIONS represents a supersized expansion of NATURALINSTRUCTIONS (Mishra et al., 2022b) which had 61 tasks. 2 The dataset, models, and a leaderboard can be found at https:// instructions.apps.allenai.org.\n\n\u2662 Co-first authors \u2663 Co-second authors \u2022 Input: \"Context: \u2026 'That's fantastic, I'm glad we came to something we both agree with.' Utterance: 'Me too. I hope you have a wonderful camping trip.'\" \u2022 Output: \"Yes\" \u2022 Explanation: \"The participant engages in small talk when wishing their opponent to have a wonderful trip.\"\n\n\u2022 Input: \"Context: \u2026 'Sounds good, I need food the most, what is your most needed item?!' Utterance: 'My item is food too'.\" \u2022 Output: \"Yes\" \u2022 Explanation: \"The utterance only takes the negotiation forward and there is no side talk. Hence, the correct answer is 'No'.\" Definition \"... Given an utterance and recent dialogue context containing past 3 utterances (wherever available), output 'Yes' if the utterance contains the small-talk strategy, otherwise output 'No'. Small-talk is a cooperative negotiation strategy. It is used for discussing topics apart from the negotiation, to build a rapport with the opponent.\"\n\n\nTask Instruction\n\n\u2022 Input: \"Context: \u2026 'I am excited to spend time with everyone from camp!' Utterance: 'That's awesome! I really love being out here with my son. Do you think you could spare some food?' \" \u2022 Expected Output: \"Yes\"\n\n\nPositive Examples\n\nNegative Examples\n\n\nEvaluation Instances\n\nTk-Instruct Figure 1: An example task from SUP-NATINST adopted from Chawla et al. (2021). A successful model is expected to use the provided instructions (including task definition and demonstration examples) to output responses to a pool of evaluation instances. 2022b;Sanh et al., 2022;Wei et al., 2022) using large pretrained language models (Raffel et al., 2020;Brown et al., 2020). As remarkable as models like InstructGPT (Ouyang et al., 2022) are, the contribution of various design choices to their success is opaque. In particular, the role of supervised data has remained understudied due to limited data released by the corporate entities behind major models. In addition, it is nearly impossible for the research community to extend and re-train these gigantic models. Addressing these two chal-  Table 1: A comparison of SUP-NATINST to a few notable datasets in the field. We obtain the number of tasks, instructions, and task types of other datasets from their original paper. \"-\" indicates the fields are not applicable or unknown. Standards for categorizing task types vary across different datasets (see Fig. 2). *PROMPTSOURCE does not provide task type annotation for all their tasks, for which we report only the 13 task types annotated for training T0 (Sanh et al., 2022) instead. lenges necessitates the availability of large-scale public benchmarks of a broad range of NLP tasks and their instructions to facilitate developing and evaluating models that can generalize to unseen tasks.\n\nIn this paper, we construct a meta-dataset (i.e., dataset of datasets; Triantafillou et al., 2019) that consists of a wide variety of NLP tasks with their instructions, and train a model that can perform a new task given the instruction, outperforming InstructGPT (which uses 16\u00d7 more parameters).\n\nOur dataset, SUPER-NATURALINSTRUCTIONS (SUP-NATINST for short), is a large benchmark of 1,616 NLP tasks and their natural language instructions. It brings in a diverse variety of tasks-76 broad task types spanning 55 different languages. Each task is paired up with an instruction that consists of the task definition for mapping an input text to a task output and several examples for demon-strating the desired or undesired output (see Fig.1 as an example task). These tasks and their instructions are contributed by 88 NLP practitioners, in response to our public call. These contributions are consolidated after several rounds of peer-review and crowdsourced feedback to ensure quality. Having this diverse and large-scale data enables us to carefully split the tasks into training and test sets and systematically study how state-of-the-art methods perform on them. Table 1 and Figure 2 highlight properties of SUP-NATINST compared to relevant benchmarks, emphasizing the diversity of tasks and instruction types in our benchmark.\n\nOur model, Tk-INSTRUCT, is a generative model for transforming task inputs given declarative in-context instructions (task definition or kshot examples). It is built by multi-task training of the T5 model (Raffel et al., 2020) over all the task instructions in our training set, and is eval-uated on unseen tasks in the test set. Interestingly, an 11B-parameter Tk-INSTRUCT can outperform the 175B-parameter InstructGPT model by 9.9 ROUGE-L points when evaluated on 119 unseen English tasks, and the multilingual variant mTk-INSTRUCT outperforms InstructGPT by 13.3 points on 35 non-English tasks ( \u00a76.1). According to human evaluation, Tk-INSTRUCT generates responses at least as well as the ground truth for 77% of the testing instances ( \u00a76.2), confirming its strong generalization to unseen tasks.\n\nThe compelling empirical performance of Tk-INSTRUCT confirms the importance of super-sized meta datasets such as our SUP-NATINST to facilitate research towards generalizable NLP models. We conduct extensive analysis to understand the important factors for this generalization ( \u00a77). Our analysis shows that scaling up the diversity of training tasks and the model size are both important for strong generalization to unseen tasks. Finally, we estimate performance upper bounds, suggesting further room for improvement.\n\n\nRelated Work\n\nLanguage instructions are a versatile way of defining goals, which is why they have been studied in the context of a variety of applications, such as instructions in grounded environments (Shridhar et al., 2020;Stepputtis et al., 2020;Min et al., 2022b;Weir et al., 2022) and database commands (Kim et al., 2020). Here, we focus on applications of instructions for general NLP tasks.\n\nRecent literature has been motivated by building models that are generalizable across a variety of NLP tasks, when prompted with either a few examples (Ye and Ren, 2021;Bragg et al., 2021) or language definitions (Efrat and Levy, 2020;Weller et al., 2020;Zhong et al., 2021;Mishra et al., 2022b,a;Parmar et al., 2022). Our work is related to the existing benchmarks in this line of work, as delineated in Table 1 along various dimensions. Our benchmark extends NATINST (Mishra et al., 2022b) with 26\u00d7 more tasks and greater variety of task types (Fig. 2). While CROSSFIT (Ye et al., 2021) focuses on benchmarking with a few in-context examples, our benchmark also offers task instructions.\n\nConcurrent to our work, PROMPTSOURCE (Bach et al., 2022) is another benchmark of tasks and their language instructions (prompts). An important distinction between this benchmark and ours is the phrasing of the task definitions: while PROMPTSOURCE task definitions are relatively concise, our task definitions are collected with the intention of providing a complete definition of each task and therefore are longer (24 tokens vs. 56 tokens on average; Table 1). More recently, BIG-BENCH (Srivastava et al., 2022) introduces a collection of 204 tasks and also provides short task descriptions and input prefixes that can be used for prompting LMs. With little overlap to our collection of tasks, they focus more on finding challenging tasks that can be used to test different behaviors of current LMs. Nevertheless, we believe that all these efforts in collecting different tasks as well as the task instructions are complementary, and the community will benefit from considering different benchmarks. Finally, the well-adopted InstructGPT model (Ouyang et al., 2022) is partially enabled by a large dataset of prompts that are collected via various synthetic data augmentation which, unfortunately, is not publicly available.\n\nBeyond cross-task generalization, our benchmark can also be used to study multi-task learning more broadly, which is a longstanding goal for AI (Caruana, 1997). Traditionally, this literature focuses on setups that involve evaluation on tasks that are observed during training (Collobert and Weston, 2008;Hashimoto et al., 2017). More recent studies show promise that large-scale multi-task learning can enable strong generalization to similar tasks via unified encoding (Khashabi et al., 2020;Xie et al., 2022) or better finetuning results on downstream tasks (McCann et al., 2018;Aribandi et al., 2022). Our proposed benchmark provides diverse tasks for studying multi-tasking at a massive scale.\n\n\nSUPER-NATURALINSTRUCTIONS\n\nSUPER-NATURALINSTRUCTIONS is a metadataset (Triantafillou et al., 2019) consisting of a variety of NLP tasks (see Fig. 2a) and instructions that describe them in plain language. Instruction schema. All task instructions follow the same uniform schema (see Fig. 1) which is composed of the following parts: \u2022 DEFINITION defines a given task in natural language. This is a complete definition of how an input text (e.g., a sentence or a document) is expected to be mapped to an output text.\n\n\u2022 POSITIVE EXAMPLES are samples of inputs and their correct outputs, along with a short explanation for each.\n\n\u2022 NEGATIVE EXAMPLES are samples of inputs and their incorrect/invalid outputs, along with a short explanation for each. The above schema is based on that of Mishra et al. (2022b), though it is simplified. See Appendix C for the comparison. Task instances. Given the instructions for each task, a model is expected to solve instances of that task. We use a unified format to organize the instances of all our tasks. More precisely, each instance consists of a textual input and a list of acceptable textual outputs. We limit the number of instances in each task to 6.5K to avoid an imbalance of instances between tasks. Benchmark collection. The benchmark was collected through a large community effort on GitHub. 3 Tasks were collected and contributed by NLP practitioners who were either responding to our public invitation 4 or students who were encouraged to contribute as part of their class project. 5 Contributors were encouraged to be creative and source the tasks from several resources: (a) existing public NLP datasets, (b) available intermediate annotations in crowdsourcing experiments (e.g., paraphrasing questions or rating their quality during crowdsourcing a QA dataset), or (c) synthetic tasks that can be communicated to an average human in a few sentences (e.g., basic algebraic operations like number comparison, finding the longest palindrome substring, etc.). When using existing datasets or crowdsourcing annotations, contributors were encouraged to adopt the instructions used to create this dataset whenever available. This was done to ensure that the instructions were sufficient to define the tasks to average human readers. Tasks along with instructions and other meta information were contributed as JSON files via GitHub pull requests, which were reviewed by automated checks and peers. We had 88 contributors from diverse locations and backgrounds contribute to our repository. Quality control. Controlling the quality of this community-contributed data was done in several phases: (1) Upon creating a GitHub pull request of the proposed task, it immediately went through an automatic test. This process verified that the introduced file contained the expected fields and adhered to our desired properties (e.g., no duplicate instances, the output labels are not heavily imbalanced, etc.) and (2) The proposed task was then peer-reviewed by 1-2 other expert contributors to ensure the clarity and sufficiency of instruction content. The review process was done iteratively until the reviewers were content with the quality of the proposed instruction. Specifically, reviewers were asked to verify that the instruction is clear and sufficient for an average language speaker to solve the underlying task (evaluation instances) while being grammatical, fluent, and concise. On average, the review of each GitHub pull request took about 4-6 iterations over the span of multiple days before being merged. (3) Lastly, the added tasks were presented to crowdworkers in order to collect feedback on the quality of the provided instructions, such as typos, clarity, or other issues (details in \u00a7A). Subsequently, one of the authors used this feedback to improve the task definitions of the instances. This feedback was done only for English tasks, as finding high-quality crowdworkers in other languages is nontrivial (Pavlick et al., 2014).\n\n\nDiversity of tasks.\n\nCollecting tasks for SUP-NATINST was carefully supervised to cover a wide variety of natural language understanding tasks, domains, and languages. To better understand this diversity, we comprehensively categorize tasks along three different dimensions:\n\n\u2022 TASK TYPE defines the nature of the mapping from instance inputs to outputs (e.g., question answering, classification, etc.).\n\n\u2022 LANGUAGE indicates the language(s) of the instances.\n\n\u2022 DOMAIN indicates the domain(s) to which the text of the tasks belong to (e.g., politics, medicine, dialogue, etc.).\n\nThese different measures of categorization can be used to study different senses of generalization. In our empirical studies ( \u00a75), we study generalization along the axis of task types. We refer the reader to Fig. 10 in the appendix for the distribution of tasks among different task types, languages, and domains.\n\nStatistics.  Defining Generalization to Unseen Tasks. Each task t is defined via its natural language instruction I t , and each task has a set of input/output instances (X t , Y t ). A model M is expected to produce the output y, given the input x and the task instruction\nI t : M (I t , x) = y, for (x, y) \u2208 (X t , Y t ).\nIn particular, we would like to evaluate model M on tasks that are not observed (i.e., their instances were not used for training M ). The only source of signal for learning the task at inference time is in-context instructions I t that contain a definition and demonstration examples of the task.\n\nTk-INSTRUCT. We introduce Tk-INSTRUCT, a model that is meta-trained on SUP-NATINST for solving tasks given their in-context instructions. Previous work has shown the effectiveness of such meta-training in improving model's ability to do incontext learning with either prompts (Zhong et al., 2021;Sanh et al., 2022) or demonstration examples (Min et al., 2022a). Because of the large variety of tasks in SUP-NATINST, we are able to do this multi-task meta-training at a larger scale than before. We conduct our experiments and analysis based on the T5 model (Raffel et al., 2020). Since each instruction I t consists of multiple elements as described in our instruction schema ( \u00a73), we map these elements to textual format and append them before the input instance. Fig. 8 in the appendix shows how we encode the full instructions. We study different combinations of these instruction elements in \u00a77.2. By default, we will use our most effective instruction elements (i.e., task definition and two positive examples) unless otherwise specified. In the same manner, we train the multilingual variant mTk-INSTRUCT based on the mT5 model (Xue et al., 2021).\n\n\nBenchmarking Cross-Task Generalization with SUP-NATINST\n\nHere we provide our recommended recipe for benchmarking generalization via SUP-NATINST.\n\n\nEvaluation Setup\n\nAn Evaluation Split of Unseen Tasks. We split the large collection of tasks in SUP-NATINST into two subsets: one for evaluation and the other for supervision. For evaluation tasks, we fix a manuallyselected collection of 12 categories that represent 154 tasks. The large variety of tasks in SUP-NATINST enables us to choose a diverse set of tasks for evaluation -such as those at word, sentence, and document levels, covering both classification and generation formats. Appendix G lists our evaluation tasks with examples for representative tasks.\n\nFor an efficient evaluation, we sample a maximum of 100 instances for each task, which results in 15,310 testing instances in total. The remaining tasks are used for training models. 6 Divided Tracks for English and X-lignual Tasks. SUP-NATINST consists of tasks across multiple languages, which enables evaluating the model's generalization to unseen tasks not only in English but also in other languages. Therefore, we divide our evaluation tasks into two tracks: one for English cross-task generalization (119 tasks) and the other for cross-lingual cross-task generalization (35 tasks). To the best of our knowledge, this is the first study in cross-lingual cross-task generalization (i.e., generalization to unseen tasks in different languages). Fig. 11 and Fig. 12 in the appendix contain the evaluation tasks for each track. Evaluation Metrics. Due to the diversity of our tasks and the open-ended generation nature of our formulation, 7 we adopt ROUGE-L (Lin, 2004) for reporting aggregated performance results. This is a soft string overlap metric that can be applied to a wide range of text generation tasks. We show that the ranking from this metric correlates well with accuracy for classification tasks in Appendix E. We also conduct a human evaluation in \u00a76.2. 6 To avoid data leakage, we exclude tasks from the training set if they are sourced from the same dataset as any test task. This results in 757 training tasks for the English track and 1271 training tasks for the cross-lingual track. 7 Unlike Sanh et al. (2022) and Wei et al. (2022), who evaluate their models on classification tasks via option ranking (i.e., scoring the correct answer(s) higher than other candidate answers), we evaluate our models in an open-ended generation setting with no task-specific assumptions. We believe this is a more realistic measure of generalization to unseen tasks.\n\n\nBaselines and Existing Models\n\nHere we discuss a variety of baselines and competitive models for our target application. See Appendix D for implementation details.\n\nHeuristic baselines. We first evaluate the following heuristics to evaluate the possible shortcuts in the data. Copying Demo Output copies the output of a random demonstration example. Since we balance the labels for our test tasks, the performance of this baseline will roughly equal a random guess or a majority baseline for classification tasks. Copying Instance Input copies the given instance input. This strategy performs well on tasks where the target output largely overlaps with the input (e.g., question rewriting, grammar error correction).\n\nOff-the-shelf pretrained language models. We evaluate existing LMs that are not fine-tuned with instruction-specific data. Specifically, we evaluate the 11B-parameter T5 (Raffel et al., 2020) as a direct counterpart of Tk-INSTRUCT. Due to the infilling pretraining objective of the original T5 model, it cannot continue text well. Therefore, we evaluate its \"LM-adapted\" version, which is further trained with a language modeling objective (Lester et al., 2021). Additionally, we evaluate GPT-3 (Brown et al., 2020), a 175B-parameter autoregressive LM that has shown remarkable ability in following demonstrations provided in its prompt.\n\nInstruction-tuned models. In addition to our Tk-INSTRUCT ( \u00a74), we evaluate existing models that are fine-tuned to follow language instructions. In particular, we evaluate InstructGPT (Ouyang et al., 2022) which uses reinforcement learning to incorporate human preferences into a GPT-3 pretrained model, and T0 (Sanh et al., 2022) which finetunes T5 on a collection of task prompts in PROMPT-SOURCE (Bach et al., 2022).\n\nUpper bound estimates. We estimate an upper bound on models' generalization to unseen tasks by fine-tuning an oracle model on the tasks' labeled instances. Since this model observes the hidden instances of the evaluation tasks, it is, by definition, an estimated upper bound to our generalizationbased models. Specifically, we fine-tune a T5-11B model on the 119 English evaluation tasks, and a mT5-13B model on the 35 non-English tasks, with 1K random training instances per task, without overlap with the evaluation instances.   6 Experimental Results Table 3 summarizes our overall benchmarking results. We use the same input encoding that contains the most effective instructional elements (task definition and two positive examples without the negative examples and explanations) for all the methods. To better understand models' generalization to different tasks, we also break down the performance according to the task categories in Fig. 4. We refer the reader to Appendix H for more detailed analysis on each individual task.\n\n\nOverall Results\n\nInstruction-tuning enables stronger generalization to unseen tasks. Generally instruction-tuned models perform better compared to their untuned LM counterparts (Tk-INSTRUCT vs. T5-LM, In-structGPT vs. GPT-3) and heuristic baselines. This indicates models do learn to follow instructions by finetuning on instruction data, and this can generalize to new instructions for unseen tasks. T0 is an exception, which is only slightly better than T5-LM. We suspect this is because the style of prompting in T0's training data is very different from our style of instructions.\n\n\nOur Tk-INSTRUCT outperforms InstructGPT.\n\nOur Tk-INSTRUCT and mTk-INSTRUCT models, which are trained with a variety of tasks, generalize best to unseen tasks for both English and non-English tasks in all evaluation task categories.\n\nInstructGPT also shows a great extent of generalization to our evaluation tasks. However, we want to note it is not clear if InstructGPT's training data overlaps with our evaluation tasks since their data is unavailable.\n\nThere is a sizable gap for improvement. Despite the impressive performance of current models, there is a sizable gap between the generalization of instruction-based models and the supervised training approach, leaving more room for improvement.\n\n\nHuman Evaluation\n\nFor language generation tasks, automatic metrics are only an approximation of human judgments; we conduct a human evaluation to confirm the findings so far. Specifically, we ask crowdworkers to indicate if they prefer the predicted answer by the model or the ground truth outputs for each instance with ties being allowed (see Appendix B for details). The resulting human evaluation metric indicates how often model predictions were rated as at least as good as our ground truth labels. The theoretical upper bound of this metric is 100% when the model is rated at least as good as the ground truth for all the instances. The results of human evaluation (shown in Fig. 3) align quite well with our automatic metrics and confirm the human-perceived quality of our models.\n\n\nFurther Analysis\n\nWe conduct further analysis to understand the important factors for models to generalize across tasks. Due to the computational cost, this analysis is done on the English track and using the T5-3B checkpoint, except for the experiments on model sizes.\n\n\nScaling Trends of Generalization\n\nWe study Tk-INSTRUCT's generalization performance with respect to three scaling factors: the number of training tasks, the number of instances per task, and the model sizes. Fig. 5 presents the performance change by scaling each of them.\n\nMore observed tasks improve the generalization.\n\nWe fine-tune Tk-INSTRUCT with different numbers of tasks that are randomly sampled from the whole training set (Fig. 5a). The model generalization performance grows log-linearly 8 as we increase the set of tasks used for training. Previous work (Mishra et al., 2022b;Sanh et al., 2022;Wei et al., 2022) has made similar observations on a much smaller scale, while we show that this trend holds even with 757 diverse training tasks.\n\nA large number of training instances do not help generalization. We then vary the number of instances per task that are used for finetuning ( Fig. 5b). While the conventional wisdom in supervised learning is that more training instances usually helps (Banko and Brill, 2001;Sun et al., 2017;Hestness et al., 2017), in our setup, the model's performance saturates when only 64 instances per task are used for training. A large number of training instances would instead lead to longer training time and risk overfitting to the training tasks.    Tuning larger models with instructions consistently lead to gains. We study the effect of model scaling by initializing Tk-INSTRUCT from different sizes of pretrained T5 checkpoints, including the small, base, large, xl and xxl sizes (Fig. 5c). We found that increasing the model sizes consistently bring significant improvement (log-linearly with parameter size). This finding contradicts the claim in Xu et al. (2022) \n\n\nInstructing with Different Elements\n\nWe evaluate the performance of Tk-INSTRUCT under different instructional elements.\n\nBenefit of different instructional elements. As shown in Fig. 1, SUP-NATINST provides multiple elements for instructing a task. We train multiple models with different combinations of these elements. The diagonal cells of Table 4 show the performance of our models when trained and evaluated on a particular instruction encoding. Based on the diagonal numbers, including the task definition consistently helps the model to generalize better. \n\n\nConclusion\n\nWe construct a large-scale benchmark consisting of a diverse set of NLP tasks and their instructions. This benchmark can serve as a rich playground for training or evaluation of models that can generalize to unseen tasks by following instructions. Furthermore, we train Tk-INSTRUCT using this data, and demonstrate its capability to perform unseen tasks to a surprising extent. We provide extensive analysis to understand the important factors for such generalization. We hope our data and model will facilitate future work towards more general-purpose models.\n\n\nLimitations\n\nWhile the presented data offers a notable variety (e.g., diverse task types), its underlying distributions suffer from skews which should be addressed in future work (see Appendix F). On language diversity, the proposed benchmark is biased toward English. On output diversity, the collected tasks are generally still skewed to short responses, which might reflect the distribution of the available tasks in the field. This under-representation of the longtail of tasks poses a challenge for building generalpurpose models in the future. We hope future work addresses such distributional imbalances. Moreover, we see natural extensions of the instructionfollowing setup here in the context of other modalities such as vision or speech. Automatic evaluation of models' performance is another challenge, considering the diverse set of tasks in our benchmark, and many of them being open-ended generation tasks. We use ROUGE-L as an aggregated metric in this paper and find it as a good proxy for the overall performance of the mod-els, aligning well with human evaluation. However, there are specific tasks for which ROUGE-L might not serve as an effective proxy of quality (such as rewriting tasks or error correction tasks where copying the input can result in a high ROUGE-L score). We hope these issues will be addressed with the development of more powerful evaluation metrics for text generation.\n\nIn terms of computing power, we have experimented with models that were accessible to us and have made the resulting models publicly available. We also acknowledge that there are larger models that we were not able to train due to the limitations of our computational budget. \n\n\nSupplemental Material A Crowdsourcing Human Feedback\n\nWe use Amazon Mechanical Turk (AMT) to crowdsource feedback on the quality of the collected instructions. We limit our crowdworkers to predominantly English-speaking countries (USA, UK, Canada, and Australia), and to those who have finished over 1K HITs with an approval rating of over 99%. Fig. 6 shows the crowdsourcing template used for collecting crowdworker feedback on our instructions. We show the instructions (the task definition, along with positive and negative examples) followed by forms for their feedback. We allow the crowdworkers to give us a qualitative measure of their perceived quality as well as text boxes for more concrete items (such as typos or phrasings that may benefit from more clear articulation). For each task, we solicit the feedback of 3 crowdworkers and then use this feedback to improve the task definitions or the examples for each task.\n\n\nB Crowdsourcing Human Judgements of Generation Quality\n\nWe perform a crowdsourcing experiment on Amazon Mechanical Turk (AMT) to assess the quality of the generated responses of models. Specifically, we ask crowdworkers to indicate if they prefer the predicted answer by the model or the ground truth outputs for each instances. The annotation interface is shown in Fig. 7. It is essentially the same template used for the quality assessment of the dataset ( \u00a7A), except that here the crowdworkers are shown a pair of responses for each instancesthe reference text (from our benchmark) and the one generated by the model-turning the task into a comparative evaluation. For each instance, we obtain annotations from an annotator as to whether they prefer either response over the other or they would rate them equally (\"tie\"). The model receives a credit of 1.0 if the worker favors the model's prediction at least as well as the ground truth label (otherwise, the model would receive a credit of 0.0). The overall accuracy score for the model is computed by averaging instance-level scores. To reduce the costs, the human evaluation of our models is done on 60 randomly selected tasks (about half of our evaluation tasks), and on 10 random instances of each task.\n\nSince it is non-trivial to find non-English speaking crowdworkers (Pavlick et al., 2014), this evaluation was restricted to English language tasks. Therefore, since our task is focused on English tasks, we required workers to be based in a country with a population predominantly of native English speakers (USA, Canada, UK, and Australia) and have completed at least 5000 HITs with \u226599% assignment approval rate.\n\nThe resulting human-evaluation metric indicates how often were model predictions equal or preferred to our ground truth labels. In this evaluation, the theoretical upper bound is 100% where the model is rated at least as well as the ground truth. The results of human evaluation are shown in the bottom row of Fig. 3.\n\n\nC Instruction Schema\n\nOur instruction schema is based on that of NATINST (Mishra et al., 2022b), but we simplify it to make data collection easier. Our Our experiments that finetune the T5-11B model are conducted based on the Google's T5 library 9 and we use their T5.1.1.xxl checkpoint 10 by default, which is pre-trained only on C4. 11 These experiments are run on Google V3-256 TPUs using a batch size of 1,048,576 tokens (1,024 examples), a constant learning rate of 1e-5 and a total of 1000 steps. Each training run takes 4 hours to complete.\n\nOur analyses that use T5 models smaller than 11B parameters are conducted based on Huggingface's transformers library and model checkpoints 12 (Wolf et al., 2020) on GPU machines.  When fine-tuning models, we train them for two epochs with a batch size of 16 and a constant learning rate of 1e-5. The maximum input length is set to 1024, and the maximum output length is set to 128. These experiments are conducted with 8 A100 GPUs with 48GB GPU memory per each. We use DeepSpeed 13 for model parallelization, with bfloat16 precision enabled to save the GPU mem-13 https:// github.com/ microsoft/ DeepSpeed ory. Each training run takes 6 hours to complete.\n\nGPT-3 and InstructGPT experiments. We use the OpenAI API 14 for conducting the GPT-3 experiments. We use their \"davinci\" engine for the GPT-3 language model experiments and their \"textdavinci-001\" engine for the InstructGPT experiments. When making the requests, we set the temperature as 0, top_p as 1 and the maximum gen-eration length as 128. Due to the high cost, we randomly sample 20 instances from each of our 119 test tasks to estimate the performance of GPT-3 and InstructGPT. All API requests were made on May 30, 2022.\n\nEncoding instruction with input For every problem setup, we map a given instruction I t and an input instance x into a textual format, obtaining enc(I t , x). Each instruction I t consists of multiple elements as described in our instruction schema ( \u00a73). We map each element of the instruction to a textual format and prepend it to the input instance. Fig. 8 shows how we encode the full instruction. We study different combinations of these instruction elements in \u00a77.2. The encoded instance is then fed to an encoder-decoder model to predict y: M : enc(I t , x) \u2192 y. \n\n\nE Evaluation Metrics\n\nWe adopt ROUGE-L as our automatic evaluation metric in this work. However, it remains a question for how much ROUGE-L can reflect model's performance on different tasks. Although we cannot test ROUGE-L's correlation with each task-specific metric of the tasks included in our data, we do investigate whether ROUGE-L can be used for classification tasks. Fig. 9 plots the ROUGE-L scores and accuracy of several models on different types of tasks. These task types are usually regarded as classification tasks and have very short ground truth output. We can see that for all these task types, the trend of ROUGE-L correlates well with the trend of accuracy. For some task types, we do see some gap between these two metrics. The reason is because there are some generation tasks categorized into these types. These results indicate that ROUGE-L is a good proxy for accuracy for classification tasks.\n\n\nF Distribution of Tasks\n\nAs is described in \u00a73, SUP-NATINST provides the annotation for categorizing tasks along three different dimensions: task type, language, and domain. Fig. 10 shows the distribution of tasks among these three dimensions. This meta-information can be used to study model's generalization ability in different senses. Despite the diversity of the data, we acknowledge the skew toward certain tasks and languages, which we leave to be addressed by future work. Tasks   Table 5 lists the 12 task categories used for our evaluation and all the tasks included in each category (introduced in \u00a75.1). To provide a better sense of what those tasks look like, we also select one representative task from each category and list them in Tables 6-17. Due to the large number of tasks in our dataset, we cannot list all 1,616 tasks in this paper. We refer the reader to our dataset.\n\n\nG Evaluation\n\n\nH Performance Improvement per Evaluation Task\n\nTo provide more detailed analysis of Tk-INSTRUCT on each individual task, Fig. 11 Task Type   Textual Entailment  Task ID  task1344_rte_textual_entailment  Definition In this task, you're given two sentences. Indicate if the first sentence clearly entails the second sentence (i.e., one can conclude the 2nd sentence by reading the 1st one). Indicate your answer with \"1\" if the first sentence entails the second sentence, otherwise answer with \"0\". Positive Example Input: Sentence 1: No Weapons of Mass Destruction Found in Iraq Yet. Sentence 2:Weapons of Mass Destruction Found in Iraq. Output: 0 Explanation: In our first statement we clearly say that Iraq does not have any weapon of mass destruction but the second sentence says that weapon of mass destruction is found in Iraq which is a contradiction. Hence output will be 0 for non entailment. Negative Example Input: Sentence 1: Valero Energy Corp., on Monday, said it found \"extensive\" additional damage at its 250,000barrel-per-day Port Arthur refinery. Sentence 2: Valero Energy Corp. produces 250,000 barrels per day. Output: 0 Explanation: The first statement mentions that there was damage found in the 250,000 barrel-per-day Port Aurthur refinery. Which means that they produce 250,000 barrels a day. Hence the output should have been 1 for entailment.  In this task your given two statements. You must judge whether the second sentence is the cause or effect of the first one. Label the instances as \"cause\" or \"effect\" based on your judgment. The sentences are separated by a newline character. Positive Example Input: The women met for coffee. They wanted to catch up with each other. Output: cause Explanation: The women met for coffee because they wanted to catch up with each other. Negative Example Input: My body cast a shadow over the grass. The sun was rising.\n\nOutput: effect Explanation: The rising of the sun isn't an effect of casting a shadow over the grass. Instance\n\nInput: The woman tolerated her friend's difficult behavior. The woman knew her friend was going through a hard time.\n\nValid Output: [\"cause\"] In this task, you are given a question containing a blank (_) and two options. You should pick the best option to answer the question. Please answer with \"A\" or \"B\". Positive Example Input: Katrina gave Christine a stuffed animal for their birthday, but _ already had this one. (A) Katrina (B) Christine Output: B Explanation: Since the blank is someone who received the gift and already had a stuffed animal, the answer must be \"Christine\". Negative Example\n\nInput: Kevin had to use less sunscreen when at the beach tanning than Justin because _ had less sensitive skin. (A) Kevin (B) Justin Output: (A) Explanation: Here, an additonal parentheses has been added to the answer. Note that, a valid answer must be \"A\" or \"B\". Instance\n\nInput: Benjamin hated being in the sand and just watched Nelson make castle since _ hated to be messy.  In this task, you are given four sentences: a bot task sentence, a bot role sentence, a user task sentence and a user role sentence. Your job is to classify given sentences into one of the 47 different domains. The domains are: \"UPDATE_CALENDAR\", \"PRESENT_IDEAS\", \"MOVIE_LISTINGS\", \"AUTO_SORT\", \"GAME_RULES\", \"CONTACT_MANAGER\", \"BANK_BOT\", \"MUSIC_SUGGESTER\", \"CHECK_STATUS\", \"PET_ADVICE\", \"HOW_TO_BASIC\", \"NAME_SUGGESTER\", \"QUOTE_OF_THE_DAY_BOT\", \"GUI-NESS_CHECK\", \"INSURANCE\", \"RESTAURANT_PICKER\", \"MAKE_RESTAURANT_RESERVATIONS\", \"WEDDING_PLANNER\", \"SKI_BOT\", \"HOME_BOT\", \"PLAY_TIMES\", \"BUS_SCHEDULE_BOT\", \"WHAT_IS_IT\", \"PHONE_PLAN_BOT\", \"DECIDER_BOT\", \"PHONE_SETTINGS\", \"TIME_ZONE\", \"LIBRARY_REQUEST\", \"UPDATE_CONTACT\", \"CATALOGUE_BOT\", \"PROMPT_GENERATOR\", \"SCAM_LOOKUP\", \"SPORTS_INFO\", \"POLICY_BOT\", \"CITY_INFO\", \"APARTMENT_FINDER\", \"EVENT_RESERVE\", \"SHOPPING\", \"EDIT_PLAYLIST\", \"LOOK_UP_INFO\", \"ORDER_PIZZA\", \"WEATHER_CHECK\", \"APPOINTMENT_REMINDER\", \"GEOGRAPHY\", \"STORE_DETAILS\", \"AGREE-MENT_BOT\", \"ALARM_SET\". Positive Example Input: Bot's task: Inform the user that the topping they are asking for is unavailable. Bot's role: You are a bot designed to help customers order pizza. User's task: Ask if a certain pizza topping is available. User's role: You are interacting with a pizza restaurant bot. Output: ORDER_PIZZA Explanation: According to the descriptions of the four sentences, we know that the type of task is ORDER_PIZZA. Negative Example Input: Bot's task: Help the user with their pizza order. Bot's role: You are a bot designed to help customers order pizza. User's task: Ask the bot for three different pizzas. User's role: You are interacting with a pizza restaurant bot. Output: UPDATE_CALENDAR Explanation: According to the descriptions of the tasks and roles, we know that the type of task is ORDER_PIZZA, but the output is UPDATE_CALENDAR, so it is incorrect. Instance\n\nInput: Bot's task: Tell the user when the movie is playing on Friday night. Bot's role: You are a a bot designed to provide movie listings. User's task: Ask the bot for the movie times for a movie on Friday night. User's role: You are interacting with a bot designed to provide movie listings. Valid Output: [\"MOVIE_LISTINGS\"] Table 9: An example task in the Dialogue Act Recognition category of our dataset, adopted from MetaLWOz (Shalyminov et al., 2020).\n\nTask Type Answerability Classification Task ID  task1640_adverserial_qa_answerability_classification  Definition Given a paragraph from a wikipedia article about some topic, and a question related to the topic, determine whether the question is answerable from the paragraph. If the question is answerable, answer \"True\", otherwise, answer \"False\". Positive Example Input: Another approach to brain function is to examine the consequences of damage to specific brain areas. Even though it is protected by the skull and meninges, surrounded by cerebrospinal fluid, and isolated from the bloodstream by the blood 2013 brain barrier, the delicate nature of the brain makes it vulnerable to numerous diseases and several types of damage. In humans, the effects of strokes and other types of brain damage have been a key source of information about brain function. Because there is no ability to experimentally control the nature of the damage, however, this information is often difficult to interpret. In animal studies, most commonly involving rats, it is possible to use electrodes or locally injected chemicals to produce precise patterns of damage and then examine the consequences for behavior. Question: What is surrounded by cerebrospinal fluid? Output: True Explanation: The paragraph comes from the wikipedia page on the brain. The answer to the question is the brain which can be found in the paragraph. Negative Ex- Input: Another approach to brain function is to examine the consequences of damage to specific brain areas. Even though it is protected by the skull and meninges, surrounded by cerebrospinal fluid, and isolated from the bloodstream by the blood 2013 brain barrier, the delicate nature of the brain makes it vulnerable to numerous diseases and several types of damage. In humans, the effects of strokes and other types of brain damage have been a key source of information about brain function. Because there is no ability to experimentally control the nature of the damage, however, this information is often difficult to interpret. In animal studies, most commonly involving rats, it is possible to use electrodes or locally injected chemicals to produce precise patterns of damage and then examine the consequences for behavior. Question: What are the benefts of the blood brain barrier? Valid Output: [\"True\"] Table 10: An example task in the Answerability Classification category of our dataset, adopted from Adversari-alQA (Bartolo et al., 2020).\n\n\n5103\n\nTask Type\n\nWord Analogy Task ID task1156_bard_word_analogy Definition\n\nTwo analogies that relate actions to the tools used to perform the action is given in the form \"A : B. C : ?\". \"A : B\" relates action A to tool B. Your task is to replace the question mark (?) with the appropriate tool for the given action C, following the \"A : B\" relation. Positive Ex-  You will be given three sentences. Read them, then identify a noun phrase (person, place, or thing) or event that is shared between all three sentences. As the output, write the span of the text corresponding to that phrase in each sentence. Keep the order of the sentences, that is, your answer should look like: 1: *a phrase from sentence 1* 2: *a phrase from sentence 2* 3: *a phrase from sentence 3* Positive Example Input: 1: Four employees of the store have been arrested , but its manager -herself a woman -was still at large Saturday , said Goa police superintendent Kartik Kashyap . 2: If convicted , they could spend up to three years in jail , Kashyap said . 3: The four store workers arrested could spend 3 years each in prison if convicted . Output: 1: Four employees of the store 2: they 3: The four store workers Explanation: All three mentioned parts refer to the same entity, the four employees. \"of the store\" in first sentence must be included, since it is part of the same noun phrase describing the employees. Negative Ex-  \n\n\nTask Type\n\nKeyword Tagging  Task ID  task620_ohsumed_keyword_tagging  Definition Given an abstract, generate a keyword (a noun phrase) that best describes the focus or contribution of the paper. Such keywords can be directly from the given abstract or outside it. Positive Example Input: Abstract: Our results suggest that ethylene oxide retention after sterilization is increased in cuprammonium cellulose plate dialyzers containing potting compound. In contrast, cuprammonium cellulose plate dialyzers without potting compound were characterized by a rapid disappearance of retained ethylene oxide after sterilization. Whether these findings explain the low incidence of SARD with cuprammonium cellulose plate dialyzers that do not contain potting material is a matter for continued study and experimentation. Output: Sterilization Explanation: This term is directly present in the abstract and it is one of the main topic in it. So can be chosen as the medical subject heading. Negative Example Input: Abstract: Our results suggest that ethylene oxide retention after sterilization is increased in cuprammonium cellulose plate dialyzers containing potting compound. In contrast, cuprammonium cellulose plate dialyzers without potting compound were characterized by a rapid disappearance of retained ethylene oxide after sterilization. Whether these findings explain the low incidence of SARD with cuprammonium cellulose plate dialyzers that do not contain potting material is a matter for continued study and experimentation. Output: Plasma Volume Explanation: This term is not directly present in the abstract and it is no way related to the abstract. So can not be chosen as the medical subject heading. \"Cellulose\" can be become a mesh term Instance Input: Abstract: There is controversy regarding the appropriate utilization of health care resources in the management of tricyclic antidepressant overdosage. Antidepressant overdose patients presenting to the emergency department (ED) are routinely admitted to intensive care units, but only a small proportion develop cardiac arrhythmias or other complications requiring such an environment. The authors reviewed the findings in 165 patients presenting to an ED with antidepressant overdose. They found that major manifestations of toxicity on ED evaluation (altered mental status, seizures, arrhythmias, an d conduction defects) were commonly associated with a complicated hospital course. Patients with the isolated findings of sinus tachycardia or QTc prolongation had no complications.\n\nNo patient experienced a serious toxic event without major evidence of toxicity on ED evaluation and continued evidence of toxicity during the hospital course. These data support the concept that proper ED evaluation can identify a large body of patients with trivial ingestions who may not require hospital observation. Valid Output: [\"Antidepressive Agents, Tricyclic\", \"Arrhythmia\", \"California\", \"Electrocardiography\", \"Emergencies\", \"Emergency Service, Hospital\", \"Female\", \"Human\", \"Length of Stay\", \"Male\", \"Prognosis\", \"Retrospective Studies\", \"Tachycardia, Sinus\"]  Input: In a letter posted on the White House web site Tuesday John Brennan assistant to President Barack Obama for homeland security and counterterrorism said Schmidt will have regular access to the president and play a vital role in the country's security. Schmidt's selection comes more than 10 months after Obama declared cyber security a priority and ordered a broad administration review. A senior White House official said Obama was personally involved in the selection process and chose Schmidt because of his unique background and skills. Schmidt will have regular and direct access to the president for cyber security issues the official said. The official spoke on the condition of anonymity to discuss the selection process. At the same time cyber experts and potential job candidates have complained that the position lacks the budgetary and policy-making authority needed to be successful. Schmidt will report to the National Security Council and closely support the National Economic Council on cyber issues. Schmidt's selection suggests that economic and business interests in the White House held more sway in the selection process. Schmidt president and CEO of the Information Security Forum a nonprofit international consortium that conducts research in information security has served as chief security officer for Microsoft and as cyber security chief for online auction giant eBay. He was reportedly preferred by Lawrence Summers director of the economic council. A good format for the titel can be the simple subject + object + verb.   In this task, you will be shown an incorrect English sentence. You need to generate a corrected form of the input sentence. Positive Example\n\nInput: The car's wheel are loose.\n\nOutput: The car's wheel is loose. Explanation: The instance of are is replaced by the word is. This makes the sentence grammatically correct. Negative Example\n\nInput: This way is the way to go.\n\nOutput: This way may be the way to go. Explanation: The example does not correct the misuse of the word way. Instead, it should shorten the sentence to: this is the way to go. Instance Input: I think it 's harder for successful preson to risk somethnig , thay coluld lost much more then others . Valid Output: [\"I think it 's harder for a successful person to risk something becuase they could lose much more than others .\"]  \n\nFigure 2 :\n2Compared to other datasets, SUP-NATINST covers a more diverse range of task types. InstructGPT reports a very coarse categorization of their task types. Bubble size represents the number of tasks of each type in log scale.\n\nFigure 3 :\n3Human evaluation vs. ROUGE-L for several methods ( \u00a76.2). The trends of these two metrics are highly correlated with a Pearson coefficient of 0.998.\n\nFigure 4 :\n4Performance per evaluation task type. Tk-INSTRUCT consistently performs better than other generalizationbased methods on all task types, while there is still a sizable gap compared to supervised training.\n\nFigure 5 :\n5Scaling trends of models performance ( \u00a77.1) as a function of (a) the number of training tasks; (b) the number of instances per training task; (c) model sizes. x-axes are in log scale. The linear growth of model performance with exponential increase in observed tasks and model size is a promising trend. Evidently, the performance gain from more instances is limited.\n\n4 :\n4Performance (ROUGE-L) of models trained and evaluated with various encodings. Diagonal numbers (underlined) represent performances of models trained and evaluated with the same instruction encoding. Each encoding is a combination of the elements in the instructions(Fig. 1). Task ID is a short string composed of dataset name and task category; Def represents the task definition; Pos(k) represents k positive examples; Neg (k) represents k negative examples; Expl represents explanation. These results (a) show the gains from various instructional elements, and (b) indicate surprising reliability of the models to various input encoding. A model trained with definition and positive examples (i.e., the last row) remains robust for different encodings.\n\n\nDEFINITION field serves as the union of Mishra et al. (2022b)'s DEFINITION, THINGS TO AVOID, and EMPHASIS & CAUTION. Additionally, we drop their TITLE and PROMPT as their content is most often covered by DEFINITION.D Model Implementation DetailsT5 experiments. We use T5 for training our Tk-INSTRUCT, estimating the performance of the supervised approach and conducting analysis.\n\nFigure 6 :\n6The crowdsourcing template we use to receive feedback on our collected tasks.\n\nFigure 7 :\n7Crowdsourcing interface used for human assessment of our baselines ( \u00a76.2).\n\nFigure 8 :\n8Encoding task instruction with input.\n\nFigure 9 :\n9presents the per-task improvement of our Tk-INSTRUCT (3B) model over the best of two heuristic baselines on the English evaluation tasks, andFig. 12presents the per-task improvement of the mTk-INSTRUCT model on the cross-lingual evaluation tasks. For most of the evaluation tasks, we see a notable extent of generalization by Tk-Rouge-L v.s. Accuracy for task types that are usually regarded as classification tasks. The trends of these two metrics are highly correlated with a Pearson coefficient of 0.970.(a) Task Types\n\nFigure 10 :\n10Distribution of SUP-NATINST tasks in terms of their (a) task types (b) languages (c) domains. y-axes are in log scale.\n\n\nOutput: [\"A\"]\n\nFigure 11 :Figure 12 :\n1112Tk-INSTRUCT's per-task performance improvement over the best of two heuristic baselines on the 119 evaluation tasks of the English track. Tk-INSTRUCT's per-task performance improvement over the best of two heuristic baselines on the 35 evaluation tasks of the cross-lingual track.\n\nTable 2\n2shows various statistics for the \nbenchmark. In total, the dataset includes 1616 tasks \nand 5M instances. On average, each instruction is \npaired with 2.8 positive and 2.4 negative examples. \nThe average definition length is 56.6 in words. \n\n\nTable 2 :\n2Statistics of SUP-NATINST.4 Tk-INSTRUCT: Learning to Follow \nInstructions at Scale \n\n\n\nTable 3 :\n3The overall performance of different methods \non unseen tasks in the test set of SUP-NATINST ( \u00a76.1). \nWe report ROUGE-L here as our aggregated metric. \nModels that leverage instructions show stronger gen-\neralization to unseen tasks. In particular, our model \nthat is fine-tuned on a diverse set of tasks outperforms \nInstructGPT and T0 by a large margin. \n\n15.2 \n\n40.0 \n\n64.6 \n64.1 \n\n76.9 \n\n14.2 \n\n32.3 \n\n52.1 \n54.3 \n\n62.0 \n\n0 \n\n10 \n\n20 \n\n30 \n\n40 \n\n50 \n\n60 \n\n70 \n\n80 \n\nCopying \nInstance Input \n\nT0 (11B) \nInstructGPT \nTk-Instruct \n(3B) \n\nTk-Instruct \n(11B) \n\nHuman \nROUGE-L \n\n\n\nTable\n\n\n\nthat \"model size has little impact on performance with an extremely large number of tasks.\" Combining Fig. 5(a) and Fig. 5(c), one can create a correspondence between model size and task size. For example, a T5-large model trained with 757 tasks can achieve comparable performance (48.0 ROUGE-L) to the T5-3B model trained with 128 tasks (48.4 ROUGE-L), indicating that increasing the diversity of training tasks is an alternative to scaling model sizes.\n\n\nMoreover, combining the task definition with positive demonstration examples yields further improvement. However, adding more demonstration examples is negligible. Negative examples help a little bit; explanations decrease performance, which is consistent with the observations of Mishra et al. (2022b) and Lampinen et al. (2022) whenthe model is not large enough. Future work can explore whether more powerful models can benefit from these elements. Generalization to different input encodings. We further investigate whether a model trained on a particular encoding can generalize to other encodings. This can be read from the non-diagonal cells ofTable 4. The negative result here is that definitiononly models cannot generalize to example-only test encodings; and similarly, example-only models cannot generalize to definition-only test encodings. However, models trained on encodings that contain both definition and examples are surprisingly robust across different encoding variations.\n\n\nMohaddeseh Bastan, Mahnaz Koupaee, Youngseo Son, Richard Sicoli, and Niranjan Balasubramanian. 2020. Author's sentiment prediction. In International Conference on Computational Linguistics (COLING). Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. 2008. The sixth pascal recognizing textual entailment challenge. In Text Analysis Conference (TAC). Jonathan Bragg, Arman Cohan, Kyle Lo, and Iz Beltagy. 2021. Flex: Unifying evaluation for few-shot nlp. In Advances in Neural Information Processing Systems (NeurIPS). Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, and et al. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems (NeurIPS). Kushal Chawla, Jaysa Ramirez, Rene Clever, Gale Lucas, Jonathan May, and Jonathan Gratch. 2021. CaSiNo: A Corpus of Campsite Negotiation Dialogues for Automatic Negotiation Systems. In Conference of the North American Chapter of the Association for Computational Linguistics (NAACL). Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In International Conference on Machine Learning (ICML). Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The pascal recognising textual entailment challenge. In Machine Learning Challenges Workshop. Avia Efrat and Omer Levy. 2020. The Turking Test: Can Language Models Understand Instructions? arXiv preprint arXiv:2010.11982. Nancy Fulda, Nathan Tibbetts, Zachary Brown, and David Wingate. 2017. Harvesting common-sense navigational knowledge for robotics from uncurated text corpora. In Conference on Robot Learning (IJ-CAI). Aditya Gupta, Jiacheng Xu, Shyam Upadhyay, Diyi Yang, and Manaal Faruqui. 2021. Disfl-qa: A benchmark dataset for understanding disfluencies in question answering. In Annual Meeting of the Association for Computational Linguistics (ACL) -Findings. Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsuruoka, and Richard Socher. 2017. A joint many-task model: Growing a neural network for multiple nlp tasks. In Conference on Empirical Methods in Natural Language Processing (EMNLP). William Hersh, Chris Buckley, TJ Leone, and David Hickam. 1994. Ohsumed: An interactive retrieval evaluation and new large test collection for research. In Conference of the Association for Computing Machinery Special Interest Group in Information Retrieval (SIGIR). Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md Patwary, Mostofa Ali, Yang Yang, and Yanqi Zhou. 2017. Deep Learning Scaling is Predictable, Empirically. arXiv preprint arXiv:1712.00409. Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. 2020. UnifiedQA: Crossing Format Boundaries With a Single QA System. In Conference on Empirical Methods in Natural Language Processing (EMNLP) -Findings. Logan Lebanoff, John Muchovej, Franck Dernoncourt, Doo Soon Kim, Lidan Wang, Walter Chang, and Fei Liu. 2020. Understanding points of correspondence between sentences for abstractive summarization. In Annual Meeting of the Association for Computational Linguistics (ACL) -Student Research Workshop. Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In ACL Workshop on Text Summarization Branches Out. Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer. In Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).Rich Caruana. 1997. Multitask learning. Machine \nlearning, 28(1):41-75. \n\nHyeonji Kim, Byeong-Hoon So, Wook-Shin Han, and \nHongrae Lee. 2020. Natural language to sql: Where \nare we today? Proceedings of the VLDB Endowment, \n13(10):1737-1750. \n\nAndrew K Lampinen, Ishita Dasgupta, Stephanie CY \nChan, Kory Matthewson, Michael Henry Tessler, An-\ntonia Creswell, James L McClelland, Jane X Wang, \nand Felix Hill. 2022. Can Language Models Learn \nfrom Explanations in Context? \narXiv preprint \narXiv:2204.02329. \n\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021. \nThe power of scale for parameter-efficient prompt \ntuning. In Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP). \n\nBryan McCann, Nitish Shirish Keskar, Caiming Xiong, \nand Richard Socher. 2018. The natural language \ndecathlon: Multitask learning as question answering. \narXiv preprint arXiv:1806.08730. \n\nSewon Min, Mike Lewis, Luke Zettlemoyer, and Han-\nnaneh Hajishirzi. 2022a. MetaICL: Learning to learn \nin context. In NAACL-HLT. \n\nSo Yeon Min, Devendra Singh Chaplot, Pradeep Raviku-\nmar, Yonatan Bisk, and Ruslan Salakhutdinov. 2022b. \nFILM: Following Instructions in Language with Mod-\nular Methods. In International Conference on Learn-\ning Representations (ICLR). \n\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin \nChoi, and Hannaneh Hajishirzi. 2022a. Reframing \ninstructional prompts to gptk's language. In Annual \nMeeting of the Association for Computational Lin-\nguistics (ACL) -Findings. \nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and \nHannaneh Hajishirzi. 2022b. Cross-Task Generaliza-\ntion via Natural Language Crowdsourcing Instruc-\ntions. In Annual Meeting of the Association for Com-\nputational Linguistics (ACL). \n\nCourtney Napoles, Keisuke Sakaguchi, and Joel \nTetreault. 2017. Jfleg: A fluency corpus and bench-\nmark for grammatical error correction. In Confer-\nence of the European Chapter of the Association for \nComputational Linguistics (EACL). \n\nJekaterina Novikova, Ond\u0159ej Du\u0161ek, and Verena Rieser. \n2017. The e2e dataset: New challenges for end-\nto-end generation. In Annual SIGdial Meeting on \nDiscourse and Dialogue. \n\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L Wainwright, Pamela Mishkin, Chong Zhang, \nSandhini Agarwal, Katarina Slama, Alex Ray, et al. \n2022. Training Language Models to Follow In-\nstructions with Human Feedback. arXiv preprint \narXiv:2203.02155. \n\nMihir Parmar, Swaroop Mishra, Mirali Purohit, Man \nLuo, Murad Mohammad, and Chitta Baral. 2022. In-\nBoXBART: Get instructions into biomedical multi-\ntask learning. In Findings of the Association for Com-\nputational Linguistics: NAACL 2022, pages 112-128, \nSeattle, United States. Association for Computational \nLinguistics. \n\nEllie Pavlick, Matt Post, Ann Irvine, Dmitry Kachaev, \nand Chris Callison-Burch. 2014. The Language De-\nmographics of Amazon Mechanical Turk. Transac-\ntions of the Association for Computational Linguis-\ntics (TACL). \nThomas Wolf, Lysandre Debut, Victor Sanh, Julien \nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, \nJoe Davison, Sam Shleifer, Patrick von Platen, Clara \nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le \nScao, Sylvain Gugger, Mariama Drame, Quentin \nLhoest, and Alexander M. Rush. 2020. Transform-\ners: State-of-the-Art Natural Language Processing. \nIn Conference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP) -System Demonstra-\ntions. \n\nTianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong, \nTorsten Scholak, Michihiro Yasunaga, Chien-Sheng \nWu, Ming Zhong, Pengcheng Yin, Sida I Wang, et al. \n2022. UnifiedSKG: Unifying and Multi-Tasking \nStructured Knowledge Grounding with Text-to-Text \nLanguage Models. arXiv preprint arXiv:2201.05966. \n\nHanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Yang-\ngang Wang, Haiyu Li, and Zhilin Yang. 2022. Zero-\nPrompt: Scaling Prompt-Based Pretraining to 1,000 \nTasks Improves Zero-Shot Generalization. arXiv \npreprint arXiv:2201.06910. \n\nQinyuan Ye, Bill Yuchen Lin, and Xiang Ren. 2021. \nCrossFit: A Few-shot Learning Challenge for Cross-\ntask Generalization in NLP. In Conference on Em-\npirical Methods in Natural Language Processing \n(EMNLP). \n\nQinyuan Ye and Xiang Ren. 2021. Learning to Generate \nTask-Specific Adapters from Task Description. In \nAnnual Meeting of the Association for Computational \nLinguistics (ACL). \n\nRuiqi Zhong, Kristy Lee, Zheng Zhang, and Dan Klein. \n2021. Adapting Language Models for Zero-shot \nLearning by Meta-tuning on Dataset and Prompt Col-\nlections. In Conference on Empirical Methods in \nNatural Language Processing (EMNLP) -Findings. \n\n\nTable 5 :\n512 Evaluation categories ( \u00a75.1), their evaluation metrics (Exact Matching or ROUGE-L,  \u00a75.1), and all the \ntasks in each category. \n\n5101 \n\n\n\nTable 6 :\n6An example task in the Textual Entailment category of our dataset, adopted from RTE (Dagan et al., 2005; \nBentivogli et al., 2008). \n\nTask Type \nCause Effect Classification \nTask ID \ntask828_copa_cause_effect_classification \nDefinition \n\n\nTable 7 :\n7An example task in the Cause Effect Classification category of our dataset, adopted from COPA (Roemmele \net al., 2011). \n\nTask Type \nCoreference Resolution \nTask ID \ntask1391_winogrande_coreference_resolution \nDefinition \n\n\nTable 8 :\n8An example task in the Cause Effect Classification category of our dataset, adopted from WinoGrande (Sak-\naguchi et al., 2020). \n\n5102 \n\n\n\nTable 11 :\n11An example task in the Word Analogy category of our dataset, adopted fromBARD (Fulda et al., 2017).Task Type \nOverlap Extraction \nTask ID \ntask281_points_of_correspondence_overlap_extraction \nDefinition \n\n\nTable 12 :\n12An example task in the Overlap Extraction category of our dataset, adopted from PointsOfCorrespon-\ndence (Lebanoff et al., 2020). \n\n\n\nTable 13 :\n13An example task in the Keyword Tagging category of our dataset, adopted from OHSUMED(Hersh et al.,  1994). disfluent question to a proper question. A disfluent question is a question that has some interruptions in it while framing. A proper question is the correct form of the question without any disfluency. Positive Example Input: Why was uh where was the Rhine regulated with an upper canal? Output: Where was the Rhine regulated with an upper canal? Explanation: The above disfluent question is correctly converted to a proper question. The 'uh' and the 'why' before the correction should be removed to make it fluent. Negative Example Input: When did Maududi exert the least impact no where did he exert? Output: When did Maududi exert the least impact? Explanation: The above disfluent question is not correctly converted to a proper question since the original meaning in the disfluent question is where did Maudidi exert the least impact but the output asks when did Maudidi exert the least impact. InstanceInput: What kind of, no hold up, what describes the proportionality of acceleration to force and mass? Valid Output: [\"What describes the proportionality of acceleration to force and mass?\"]Task Type \nQuestion Rewriting \nTask ID \ntask1622_disfl_qa_question_rewriting \nDefinition \nConvert a \n\nTable 14 :\n14An example task in the Question Rewriting category of our dataset, adopted from Disfl-QA (Gupta et al., 2021). document, generate a short title of the document. The title should convey the main idea/event/topic about which the document is being written. Note that URLs in the text have been replaced with [Link]. Positive Example5105 \n\n\n\nOutput :\nOutputWhite House picks new cyber coordinator Explanation: The title is relevant to the main topic of document, that is, the selection of Schmidt as the cybersecutiy chief. Negative Example Input: Lauren Cohan's Walking Dead fate may be written in the Whiskey Cavalier. While the show hasn't been picked up yet Cohan hasn't yet signed on for season 9 of Walking Dead and rumors have circulated for months about her possible exit from the zombie show. Just two days ago show runner Scott Gimple told TV Line that he is incredibly hopeful about working out a new deal with Cohan who has played Maggie since the second season. \"These things do happen in TV\" he said. But we are talking. Output: Scott Gimple signed Lauren Cohan for season 9 Explanation: Though the topic is relevant and have correct names from the document, this is incorrect fact, as it is mentioned that Cohan hasn't yet signed. InstanceInput: Days after at least 58 people were killed in a Las Vegas mass shooting , Hillary Clinton called for better gun control . Clinton also had some words for President Trump , particularly of his handling of Hurricane Maria and the devastation in Puerto Rico . Clinton , on her book tour for \"What Happened ,\" called her memoir \"a story of resilience .\" Fallon also had female staff writers write thank you notes to Clinton . \"Thank you , Miley , tonight 's show writers and all of the women and young girls out there who are smart , strong and deserving of every opportunity ,\" Clinton said . As for election night , Clinton said she was disappointed both that she lost and that President Trump won . Valid Output: [\"Hillary Clinton calls for gun control after Las Vegas shooting\"]\n\nTable 15 :\n15An example task in the Title Generation category of our dataset, adopted fromPerSenT (Bastan et al.,  2020). In this task, we ask you convert a data table of restaurant descriptions into fluent natural-sounding English sentences.The input is a string of key-value pairs; the output should be a natural and grammatical English sentence containing all the information from the input. Positive Ex-ample Input: name[Aromi], eatType[restaurant], food[English], area[city centre] Output: Aromi is an English restaurant in the city centre. Explanation: The output sentence faithfully converts the data in the input into a natural-sounding sentence. Negative Example Input: name[Blue Spice], eatType[coffee shop], priceRange[more than00a330], customer rating[5 out of 5], area[riverside], familyFriendly[yes], near[Avalon] Output: Blue Spice is a Colombian coffee shop located by the riverside, near Avalon in Boston. Its prices are ove\u0213 00a330. Its customer ratings are 5 out of 5. Explanation: While the output contains most of the information from the input, it hallucinates by adding ungrounded information such as \"Colombian\" and \"Boston\". Instance Input: name[The Mill], eatType[restaurant], area[riverside], near[The Rice Boat] Valid Output: [\"A restaurant called The Mill, can be found near the riverside next to The Rice Boat.\"]Task Type \nData to Text \nTask ID \ntask957_e2e_data_to_text \nDefinition \n\n\nTable 16 :\n16An example task in the Data to Text category of our dataset, adopted fromE2E (Novikova et al., 2017).. \n\n5106 \n\n\n\nTable 17 :\n17An example task in the Grammar Error Correction category of our dataset, adopted from JFLEG(Napoles  et al., 2017).\nhttps:// github.com/ allenai/ natural-instructions 4 https:// blog.allenai.org/ 9d3f24d5a9db 5 CSE 576 \"Topics in NLP\" course, Arizona State Univ.\nA linear function of an exponential increase of parameters, i.e., growth at a constant multiplicative rate.\nhttps:// github.com/ google-research/ text-to-text-transfer-transformer 10 https:// console.cloud.google.com/ storage/ browser/ t5-data/ pretrained_models/ t5.1.1.xxl11  We also tried to finetune Tk-INSTRUCT from the T5-LM checkpoint but the final performance is worse. Therefore, we decided to use the T5.1.1.xxl checkpoint. 12 https:// huggingface.co/ models?sort=downloads& search=google%2Ft5\nhttps:// beta.openai.com/ docs/ introduction/ overview\n1 1 1 1 1 1 1 1 1 1 1 1 1\nAcknowledgmentsWe thank the anonymous reviewers, our colleagues from AI2 and UWNLP, especially Matthew Peters for his encouraging conversations that motivated this project. We also thank the student contributors of Arizona State University's CSE 576 \"Topics in NLP\" course and all other contributors to our data repository. All experiments were run on AI2's Beaker GPU clusters and Google's research TPUs. This work was supported in part by ONR MURI N00014-18-1-2670, ONR N00014-18-1-2826, and DARPA MCS N66001-19-2-4031 grants.1  1  1   2  2  2  2  2  2  2  3  3  3  3  3  3  4  4  4  4  4  5  6  6  6  6  7  7  10  12  12  12  13  14  15  16  17  19  24  30  32  32  35  37  39  39  42  44  51  53  59  65  69  76  78  80  95  101  122  137  143  207  211  278\nExT5: Towards Extreme Multi-Task Scaling for Transfer Learning. Vamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao, Huaixiu Steven Zheng, Sanket Vaibhav Mehta, Honglei Zhuang, Q Vinh, Dara Tran, Jianmo Bahri, Ni, International Conference on Learning Representations. ICLRVamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao, Huaixiu Steven Zheng, Sanket Vaibhav Mehta, Hon- glei Zhuang, Vinh Q Tran, Dara Bahri, Jianmo Ni, et al. 2022. ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning. In International Con- ference on Learning Representations (ICLR).\n\nPromptSource: An Integrated Development Environment and Repository for Natural Language Prompts. H Stephen, Victor Bach, Zheng-Xin Sanh, Albert Yong, Colin Webson, Raffel, V Nihal, Abheesht Nayak, Taewoon Sharma, Kim, Thibault Bari, Fevry, Annual Meeting of the Association for Computational Linguistics (ACL) -System Demonstrations. Stephen H Bach, Victor Sanh, Zheng-Xin Yong, Al- bert Webson, Colin Raffel, Nihal V Nayak, Abheesht Sharma, Taewoon Kim, M Saiful Bari, Thibault Fevry, et al. 2022. PromptSource: An Integrated Development Environment and Repository for Nat- ural Language Prompts. In Annual Meeting of the Association for Computational Linguistics (ACL) - System Demonstrations.\n\nScaling to Very Very Large Corpora for Natural Language Disambiguation. Michele Banko, Eric Brill, Annual Meeting of the Association for Computational Linguistics (ACL). Michele Banko and Eric Brill. 2001. Scaling to Very Very Large Corpora for Natural Language Disam- biguation. In Annual Meeting of the Association for Computational Linguistics (ACL).\n\nMax Bartolo, Alastair Roberts, Johannes Welbl, Sebastian Riedel, and Pontus Stenetorp. 2020. Beat the ai: Investigating adversarial human annotation for reading comprehension. Transactions of the Association for Computational Linguistics (TACL). 8Max Bartolo, Alastair Roberts, Johannes Welbl, Sebas- tian Riedel, and Pontus Stenetorp. 2020. Beat the ai: Investigating adversarial human annotation for read- ing comprehension. Transactions of the Association for Computational Linguistics (TACL), 8:662-678.\n", "annotations": {"author": "[{\"end\":147,\"start\":112},{\"end\":184,\"start\":148},{\"end\":193,\"start\":185},{\"end\":211,\"start\":194},{\"end\":270,\"start\":212},{\"end\":311,\"start\":271},{\"end\":350,\"start\":312},{\"end\":399,\"start\":351},{\"end\":446,\"start\":400},{\"end\":460,\"start\":447},{\"end\":493,\"start\":461},{\"end\":518,\"start\":494},{\"end\":557,\"start\":519},{\"end\":574,\"start\":558},{\"end\":589,\"start\":575},{\"end\":604,\"start\":590},{\"end\":641,\"start\":605},{\"end\":676,\"start\":642},{\"end\":710,\"start\":677},{\"end\":747,\"start\":711},{\"end\":786,\"start\":748},{\"end\":805,\"start\":787},{\"end\":840,\"start\":806},{\"end\":856,\"start\":841},{\"end\":894,\"start\":857},{\"end\":935,\"start\":895},{\"end\":970,\"start\":936},{\"end\":1012,\"start\":971},{\"end\":1048,\"start\":1013},{\"end\":1092,\"start\":1049},{\"end\":1126,\"start\":1093},{\"end\":1145,\"start\":1127},{\"end\":1158,\"start\":1146},{\"end\":1173,\"start\":1159},{\"end\":1186,\"start\":1174},{\"end\":1199,\"start\":1187},{\"end\":1234,\"start\":1200},{\"end\":1293,\"start\":1235},{\"end\":1354,\"start\":1294},{\"end\":1422,\"start\":1355},{\"end\":1439,\"start\":1423}]", "publisher": null, "author_last_name": "[{\"end\":124,\"start\":120},{\"end\":162,\"start\":156},{\"end\":192,\"start\":187},{\"end\":225,\"start\":220},{\"end\":287,\"start\":280},{\"end\":328,\"start\":319},{\"end\":362,\"start\":357},{\"end\":424,\"start\":412},{\"end\":459,\"start\":455},{\"end\":471,\"start\":467},{\"end\":507,\"start\":501},{\"end\":540,\"start\":527},{\"end\":573,\"start\":570},{\"end\":588,\"start\":581},{\"end\":603,\"start\":597},{\"end\":619,\"start\":611},{\"end\":654,\"start\":648},{\"end\":688,\"start\":683},{\"end\":725,\"start\":720},{\"end\":764,\"start\":755},{\"end\":804,\"start\":794},{\"end\":818,\"start\":812},{\"end\":855,\"start\":848},{\"end\":872,\"start\":864},{\"end\":913,\"start\":909},{\"end\":948,\"start\":943},{\"end\":990,\"start\":980},{\"end\":1026,\"start\":1021},{\"end\":1070,\"start\":1064},{\"end\":1104,\"start\":1099},{\"end\":1144,\"start\":1138},{\"end\":1157,\"start\":1152},{\"end\":1172,\"start\":1167},{\"end\":1185,\"start\":1180},{\"end\":1198,\"start\":1194},{\"end\":1212,\"start\":1207},{\"end\":1245,\"start\":1241},{\"end\":1306,\"start\":1301},{\"end\":1374,\"start\":1364},{\"end\":1438,\"start\":1430}]", "author_first_name": "[{\"end\":119,\"start\":112},{\"end\":155,\"start\":148},{\"end\":186,\"start\":185},{\"end\":210,\"start\":194},{\"end\":219,\"start\":212},{\"end\":279,\"start\":271},{\"end\":318,\"start\":312},{\"end\":356,\"start\":351},{\"end\":404,\"start\":400},{\"end\":411,\"start\":405},{\"end\":454,\"start\":447},{\"end\":466,\"start\":461},{\"end\":500,\"start\":494},{\"end\":526,\"start\":519},{\"end\":564,\"start\":558},{\"end\":569,\"start\":565},{\"end\":580,\"start\":575},{\"end\":596,\"start\":590},{\"end\":610,\"start\":605},{\"end\":647,\"start\":642},{\"end\":682,\"start\":677},{\"end\":719,\"start\":711},{\"end\":754,\"start\":748},{\"end\":793,\"start\":787},{\"end\":811,\"start\":806},{\"end\":847,\"start\":841},{\"end\":863,\"start\":857},{\"end\":900,\"start\":895},{\"end\":908,\"start\":901},{\"end\":942,\"start\":936},{\"end\":979,\"start\":971},{\"end\":1020,\"start\":1013},{\"end\":1057,\"start\":1049},{\"end\":1063,\"start\":1058},{\"end\":1098,\"start\":1093},{\"end\":1137,\"start\":1127},{\"end\":1151,\"start\":1146},{\"end\":1166,\"start\":1159},{\"end\":1179,\"start\":1174},{\"end\":1193,\"start\":1187},{\"end\":1206,\"start\":1200},{\"end\":1240,\"start\":1235},{\"end\":1298,\"start\":1294},{\"end\":1300,\"start\":1299},{\"end\":1363,\"start\":1355},{\"end\":1429,\"start\":1423}]", "author_affiliation": "[{\"end\":146,\"start\":126},{\"end\":183,\"start\":164},{\"end\":248,\"start\":227},{\"end\":269,\"start\":250},{\"end\":310,\"start\":289},{\"end\":349,\"start\":330},{\"end\":398,\"start\":364},{\"end\":445,\"start\":426},{\"end\":492,\"start\":473},{\"end\":517,\"start\":509},{\"end\":556,\"start\":542},{\"end\":640,\"start\":621},{\"end\":675,\"start\":656},{\"end\":709,\"start\":690},{\"end\":746,\"start\":727},{\"end\":785,\"start\":766},{\"end\":839,\"start\":820},{\"end\":893,\"start\":874},{\"end\":934,\"start\":915},{\"end\":969,\"start\":950},{\"end\":1011,\"start\":992},{\"end\":1047,\"start\":1028},{\"end\":1091,\"start\":1072},{\"end\":1125,\"start\":1106},{\"end\":1233,\"start\":1214},{\"end\":1270,\"start\":1247},{\"end\":1292,\"start\":1272},{\"end\":1331,\"start\":1308},{\"end\":1353,\"start\":1333},{\"end\":1399,\"start\":1376},{\"end\":1421,\"start\":1401}]", "title": "[{\"end\":90,\"start\":1},{\"end\":1529,\"start\":1440}]", "venue": "[{\"end\":1735,\"start\":1531}]", "abstract": "[{\"end\":3023,\"start\":1756}]", "bib_ref": "[{\"end\":3291,\"start\":3269},{\"end\":3313,\"start\":3312},{\"end\":4734,\"start\":4714},{\"end\":4916,\"start\":4910},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4934,\"start\":4916},{\"end\":4951,\"start\":4934},{\"end\":5012,\"start\":4991},{\"end\":5031,\"start\":5012},{\"end\":5095,\"start\":5062},{\"end\":5937,\"start\":5915},{\"end\":7717,\"start\":7696},{\"end\":9040,\"start\":9017},{\"end\":9064,\"start\":9040},{\"end\":9082,\"start\":9064},{\"end\":9100,\"start\":9082},{\"end\":9141,\"start\":9123},{\"end\":9383,\"start\":9365},{\"end\":9402,\"start\":9383},{\"end\":9449,\"start\":9427},{\"end\":9469,\"start\":9449},{\"end\":9488,\"start\":9469},{\"end\":9511,\"start\":9488},{\"end\":9531,\"start\":9511},{\"end\":9705,\"start\":9675},{\"end\":9802,\"start\":9776},{\"end\":9961,\"start\":9929},{\"end\":10417,\"start\":10382},{\"end\":10971,\"start\":10950},{\"end\":11291,\"start\":11276},{\"end\":11437,\"start\":11409},{\"end\":11460,\"start\":11437},{\"end\":11626,\"start\":11603},{\"end\":11643,\"start\":11626},{\"end\":11714,\"start\":11693},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11736,\"start\":11714},{\"end\":12639,\"start\":12618},{\"end\":15824,\"start\":15802},{\"end\":17643,\"start\":17623},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":17661,\"start\":17643},{\"end\":17707,\"start\":17688},{\"end\":17925,\"start\":17904},{\"end\":18500,\"start\":18482},{\"end\":20493,\"start\":20492},{\"end\":20753,\"start\":20747},{\"end\":20775,\"start\":20758},{\"end\":22275,\"start\":22254},{\"end\":22658,\"start\":22625},{\"end\":22871,\"start\":22838},{\"end\":26852,\"start\":26830},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":26870,\"start\":26852},{\"end\":26887,\"start\":26870},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":27292,\"start\":27269},{\"end\":27309,\"start\":27292},{\"end\":27331,\"start\":27309},{\"end\":33105,\"start\":33083},{\"end\":45701,\"start\":45679}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":52835,\"start\":52600},{\"attributes\":{\"id\":\"fig_1\"},\"end\":52997,\"start\":52836},{\"attributes\":{\"id\":\"fig_2\"},\"end\":53215,\"start\":52998},{\"attributes\":{\"id\":\"fig_3\"},\"end\":53597,\"start\":53216},{\"attributes\":{\"id\":\"fig_4\"},\"end\":54358,\"start\":53598},{\"attributes\":{\"id\":\"fig_5\"},\"end\":54740,\"start\":54359},{\"attributes\":{\"id\":\"fig_6\"},\"end\":54831,\"start\":54741},{\"attributes\":{\"id\":\"fig_7\"},\"end\":54920,\"start\":54832},{\"attributes\":{\"id\":\"fig_8\"},\"end\":54971,\"start\":54921},{\"attributes\":{\"id\":\"fig_9\"},\"end\":55506,\"start\":54972},{\"attributes\":{\"id\":\"fig_10\"},\"end\":55640,\"start\":55507},{\"attributes\":{\"id\":\"fig_11\"},\"end\":55656,\"start\":55641},{\"attributes\":{\"id\":\"fig_12\"},\"end\":55965,\"start\":55657},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":56217,\"start\":55966},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":56315,\"start\":56218},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":56906,\"start\":56316},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":56914,\"start\":56907},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":57371,\"start\":56915},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":58366,\"start\":57372},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":66747,\"start\":58367},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":66901,\"start\":66748},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":67151,\"start\":66902},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":67386,\"start\":67152},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":67536,\"start\":67387},{\"attributes\":{\"id\":\"tab_19\",\"type\":\"table\"},\"end\":67755,\"start\":67537},{\"attributes\":{\"id\":\"tab_21\",\"type\":\"table\"},\"end\":67902,\"start\":67756},{\"attributes\":{\"id\":\"tab_22\",\"type\":\"table\"},\"end\":69223,\"start\":67903},{\"attributes\":{\"id\":\"tab_23\",\"type\":\"table\"},\"end\":69574,\"start\":69224},{\"attributes\":{\"id\":\"tab_24\",\"type\":\"table\"},\"end\":71272,\"start\":69575},{\"attributes\":{\"id\":\"tab_25\",\"type\":\"table\"},\"end\":72688,\"start\":71273},{\"attributes\":{\"id\":\"tab_26\",\"type\":\"table\"},\"end\":72815,\"start\":72689},{\"attributes\":{\"id\":\"tab_27\",\"type\":\"table\"},\"end\":72945,\"start\":72816}]", "paragraph": "[{\"end\":3408,\"start\":3039},{\"end\":3728,\"start\":3410},{\"end\":4349,\"start\":3730},{\"end\":4582,\"start\":4370},{\"end\":4621,\"start\":4604},{\"end\":6153,\"start\":4646},{\"end\":6452,\"start\":6155},{\"end\":7489,\"start\":6454},{\"end\":8292,\"start\":7491},{\"end\":8812,\"start\":8294},{\"end\":9212,\"start\":8829},{\"end\":9903,\"start\":9214},{\"end\":11130,\"start\":9905},{\"end\":11830,\"start\":11132},{\"end\":12348,\"start\":11860},{\"end\":12459,\"start\":12350},{\"end\":15825,\"start\":12461},{\"end\":16102,\"start\":15849},{\"end\":16231,\"start\":16104},{\"end\":16287,\"start\":16233},{\"end\":16406,\"start\":16289},{\"end\":16722,\"start\":16408},{\"end\":16997,\"start\":16724},{\"end\":17345,\"start\":17048},{\"end\":18501,\"start\":17347},{\"end\":18648,\"start\":18561},{\"end\":19216,\"start\":18669},{\"end\":21093,\"start\":19218},{\"end\":21259,\"start\":21127},{\"end\":21812,\"start\":21261},{\"end\":22451,\"start\":21814},{\"end\":22872,\"start\":22453},{\"end\":23908,\"start\":22874},{\"end\":24495,\"start\":23928},{\"end\":24729,\"start\":24540},{\"end\":24951,\"start\":24731},{\"end\":25197,\"start\":24953},{\"end\":25988,\"start\":25218},{\"end\":26260,\"start\":26009},{\"end\":26534,\"start\":26297},{\"end\":26583,\"start\":26536},{\"end\":27016,\"start\":26585},{\"end\":27983,\"start\":27018},{\"end\":28105,\"start\":28023},{\"end\":28549,\"start\":28107},{\"end\":29124,\"start\":28564},{\"end\":30539,\"start\":29140},{\"end\":30817,\"start\":30541},{\"end\":31749,\"start\":30874},{\"end\":33015,\"start\":31808},{\"end\":33430,\"start\":33017},{\"end\":33749,\"start\":33432},{\"end\":34299,\"start\":33774},{\"end\":34957,\"start\":34301},{\"end\":35488,\"start\":34959},{\"end\":36060,\"start\":35490},{\"end\":36982,\"start\":36085},{\"end\":37876,\"start\":37010},{\"end\":39778,\"start\":37941},{\"end\":39890,\"start\":39780},{\"end\":40008,\"start\":39892},{\"end\":40492,\"start\":40010},{\"end\":40767,\"start\":40494},{\"end\":42766,\"start\":40769},{\"end\":43225,\"start\":42768},{\"end\":45702,\"start\":43227},{\"end\":45720,\"start\":45711},{\"end\":45780,\"start\":45722},{\"end\":47116,\"start\":45782},{\"end\":49666,\"start\":47130},{\"end\":51941,\"start\":49668},{\"end\":51976,\"start\":51943},{\"end\":52136,\"start\":51978},{\"end\":52171,\"start\":52138},{\"end\":52599,\"start\":52173}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":17047,\"start\":16998}]", "table_ref": "[{\"end\":5462,\"start\":5455},{\"end\":7332,\"start\":7325},{\"end\":9626,\"start\":9619},{\"end\":10364,\"start\":10357},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":23435,\"start\":23428},{\"end\":28336,\"start\":28329},{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":37481,\"start\":37466},{\"attributes\":{\"ref_id\":\"tab_14\"},\"end\":37744,\"start\":37733},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":38107,\"start\":38023},{\"end\":43102,\"start\":43095},{\"attributes\":{\"ref_id\":\"tab_14\"},\"end\":43339,\"start\":43266},{\"end\":45572,\"start\":45564},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":47199,\"start\":47138}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":3037,\"start\":3025},{\"end\":4368,\"start\":4352},{\"end\":4602,\"start\":4585},{\"end\":4644,\"start\":4624},{\"attributes\":{\"n\":\"2\"},\"end\":8827,\"start\":8815},{\"attributes\":{\"n\":\"3\"},\"end\":11858,\"start\":11833},{\"end\":15847,\"start\":15828},{\"attributes\":{\"n\":\"5\"},\"end\":18559,\"start\":18504},{\"attributes\":{\"n\":\"5.1\"},\"end\":18667,\"start\":18651},{\"attributes\":{\"n\":\"5.2\"},\"end\":21125,\"start\":21096},{\"attributes\":{\"n\":\"6.1\"},\"end\":23926,\"start\":23911},{\"end\":24538,\"start\":24498},{\"attributes\":{\"n\":\"6.2\"},\"end\":25216,\"start\":25200},{\"attributes\":{\"n\":\"7\"},\"end\":26007,\"start\":25991},{\"attributes\":{\"n\":\"7.1\"},\"end\":26295,\"start\":26263},{\"attributes\":{\"n\":\"7.2\"},\"end\":28021,\"start\":27986},{\"attributes\":{\"n\":\"8\"},\"end\":28562,\"start\":28552},{\"attributes\":{\"n\":\"9\"},\"end\":29138,\"start\":29127},{\"end\":30872,\"start\":30820},{\"end\":31806,\"start\":31752},{\"end\":33772,\"start\":33752},{\"end\":36083,\"start\":36063},{\"end\":37008,\"start\":36985},{\"end\":37891,\"start\":37879},{\"end\":37939,\"start\":37894},{\"end\":45709,\"start\":45705},{\"attributes\":{\"n\":\"5104\"},\"end\":47128,\"start\":47119},{\"end\":52611,\"start\":52601},{\"end\":52847,\"start\":52837},{\"end\":53009,\"start\":52999},{\"end\":53227,\"start\":53217},{\"end\":53602,\"start\":53599},{\"end\":54752,\"start\":54742},{\"end\":54843,\"start\":54833},{\"end\":54932,\"start\":54922},{\"end\":54983,\"start\":54973},{\"end\":55519,\"start\":55508},{\"end\":55680,\"start\":55658},{\"end\":55974,\"start\":55967},{\"end\":56228,\"start\":56219},{\"end\":56326,\"start\":56317},{\"end\":56913,\"start\":56908},{\"end\":66758,\"start\":66749},{\"end\":66912,\"start\":66903},{\"end\":67162,\"start\":67153},{\"end\":67397,\"start\":67388},{\"end\":67548,\"start\":67538},{\"end\":67767,\"start\":67757},{\"end\":67914,\"start\":67904},{\"end\":69235,\"start\":69225},{\"end\":69584,\"start\":69576},{\"end\":71284,\"start\":71274},{\"end\":72700,\"start\":72690},{\"end\":72827,\"start\":72817}]", "table": "[{\"end\":56217,\"start\":55976},{\"end\":56315,\"start\":56256},{\"end\":56906,\"start\":56328},{\"end\":66747,\"start\":62119},{\"end\":66901,\"start\":66760},{\"end\":67151,\"start\":66914},{\"end\":67386,\"start\":67164},{\"end\":67536,\"start\":67399},{\"end\":67755,\"start\":67650},{\"end\":67902,\"start\":67770},{\"end\":69223,\"start\":69123},{\"end\":69574,\"start\":69567},{\"end\":72688,\"start\":72616},{\"end\":72815,\"start\":72804}]", "figure_caption": "[{\"end\":52835,\"start\":52613},{\"end\":52997,\"start\":52849},{\"end\":53215,\"start\":53011},{\"end\":53597,\"start\":53229},{\"end\":54358,\"start\":53604},{\"end\":54740,\"start\":54361},{\"end\":54831,\"start\":54754},{\"end\":54920,\"start\":54845},{\"end\":54971,\"start\":54934},{\"end\":55506,\"start\":54985},{\"end\":55640,\"start\":55522},{\"end\":55656,\"start\":55643},{\"end\":55965,\"start\":55685},{\"end\":56256,\"start\":56230},{\"end\":57371,\"start\":56917},{\"end\":58366,\"start\":57374},{\"end\":62119,\"start\":58369},{\"end\":67650,\"start\":67551},{\"end\":69123,\"start\":67917},{\"end\":69567,\"start\":69238},{\"end\":71272,\"start\":69591},{\"end\":72616,\"start\":71287},{\"end\":72804,\"start\":72703},{\"end\":72945,\"start\":72830}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5773,\"start\":5767},{\"end\":6897,\"start\":6892},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7345,\"start\":7337},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9768,\"start\":9760},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11981,\"start\":11974},{\"end\":12122,\"start\":12116},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":16624,\"start\":16617},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":18119,\"start\":18113},{\"end\":19975,\"start\":19968},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":19987,\"start\":19980},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":23821,\"start\":23815},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":25888,\"start\":25882},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26477,\"start\":26471},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26704,\"start\":26696},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":27168,\"start\":27160},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":27805,\"start\":27797},{\"end\":28170,\"start\":28164},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":31171,\"start\":31165},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":32124,\"start\":32118},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":33748,\"start\":33742},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":35849,\"start\":35843},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":36445,\"start\":36439},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":37166,\"start\":37159},{\"end\":38022,\"start\":38015}]", "bib_author_first_name": "[{\"end\":74510,\"start\":74505},{\"end\":74523,\"start\":74521},{\"end\":74532,\"start\":74529},{\"end\":74550,\"start\":74543},{\"end\":74563,\"start\":74556},{\"end\":74584,\"start\":74578},{\"end\":74607,\"start\":74600},{\"end\":74617,\"start\":74616},{\"end\":74628,\"start\":74624},{\"end\":74641,\"start\":74635},{\"end\":75102,\"start\":75101},{\"end\":75118,\"start\":75112},{\"end\":75134,\"start\":75125},{\"end\":75147,\"start\":75141},{\"end\":75159,\"start\":75154},{\"end\":75177,\"start\":75176},{\"end\":75193,\"start\":75185},{\"end\":75208,\"start\":75201},{\"end\":75230,\"start\":75222},{\"end\":75780,\"start\":75773},{\"end\":75792,\"start\":75788},{\"end\":76059,\"start\":76056},{\"end\":76077,\"start\":76069},{\"end\":76095,\"start\":76087}]", "bib_author_last_name": "[{\"end\":74519,\"start\":74511},{\"end\":74527,\"start\":74524},{\"end\":74541,\"start\":74533},{\"end\":74554,\"start\":74551},{\"end\":74576,\"start\":74564},{\"end\":74598,\"start\":74585},{\"end\":74614,\"start\":74608},{\"end\":74622,\"start\":74618},{\"end\":74633,\"start\":74629},{\"end\":74647,\"start\":74642},{\"end\":74651,\"start\":74649},{\"end\":75110,\"start\":75103},{\"end\":75123,\"start\":75119},{\"end\":75139,\"start\":75135},{\"end\":75152,\"start\":75148},{\"end\":75166,\"start\":75160},{\"end\":75174,\"start\":75168},{\"end\":75183,\"start\":75178},{\"end\":75199,\"start\":75194},{\"end\":75215,\"start\":75209},{\"end\":75220,\"start\":75217},{\"end\":75235,\"start\":75231},{\"end\":75242,\"start\":75237},{\"end\":75786,\"start\":75781},{\"end\":75798,\"start\":75793},{\"end\":76067,\"start\":76060},{\"end\":76085,\"start\":76078},{\"end\":76101,\"start\":76096}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":244478674},\"end\":75002,\"start\":74441},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":246485605},\"end\":75699,\"start\":75004},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":6645623},\"end\":76054,\"start\":75701},{\"attributes\":{\"id\":\"b3\"},\"end\":76563,\"start\":76056}]", "bib_title": "[{\"end\":74503,\"start\":74441},{\"end\":75099,\"start\":75004},{\"end\":75771,\"start\":75701}]", "bib_author": "[{\"end\":74521,\"start\":74505},{\"end\":74529,\"start\":74521},{\"end\":74543,\"start\":74529},{\"end\":74556,\"start\":74543},{\"end\":74578,\"start\":74556},{\"end\":74600,\"start\":74578},{\"end\":74616,\"start\":74600},{\"end\":74624,\"start\":74616},{\"end\":74635,\"start\":74624},{\"end\":74649,\"start\":74635},{\"end\":74653,\"start\":74649},{\"end\":75112,\"start\":75101},{\"end\":75125,\"start\":75112},{\"end\":75141,\"start\":75125},{\"end\":75154,\"start\":75141},{\"end\":75168,\"start\":75154},{\"end\":75176,\"start\":75168},{\"end\":75185,\"start\":75176},{\"end\":75201,\"start\":75185},{\"end\":75217,\"start\":75201},{\"end\":75222,\"start\":75217},{\"end\":75237,\"start\":75222},{\"end\":75244,\"start\":75237},{\"end\":75788,\"start\":75773},{\"end\":75800,\"start\":75788},{\"end\":76069,\"start\":76056},{\"end\":76087,\"start\":76069},{\"end\":76103,\"start\":76087}]", "bib_venue": "[{\"end\":74705,\"start\":74653},{\"end\":75336,\"start\":75244},{\"end\":75869,\"start\":75800},{\"end\":76300,\"start\":76103}]"}}}, "year": 2023, "month": 12, "day": 17}