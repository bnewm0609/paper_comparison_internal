{"id": 231786709, "updated": "2023-10-06 05:59:02.396", "metadata": {"title": "Learning Graph Embeddings for Compositional Zero-shot Learning", "authors": "[{\"first\":\"Muhammad\",\"last\":\"Naeem\",\"middle\":[\"Ferjad\"]},{\"first\":\"Yongqin\",\"last\":\"Xian\",\"middle\":[]},{\"first\":\"Federico\",\"last\":\"Tombari\",\"middle\":[]},{\"first\":\"Zeynep\",\"last\":\"Akata\",\"middle\":[]}]", "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2021, "month": 2, "day": 3}, "abstract": "In compositional zero-shot learning, the goal is to recognize unseen compositions (e.g. old dog) of observed visual primitives states (e.g. old, cute) and objects (e.g. car, dog) in the training set. This is challenging because the same state can for example alter the visual appearance of a dog drastically differently from a car. As a solution, we propose a novel graph formulation called Compositional Graph Embedding (CGE) that learns image features, compositional classifiers, and latent representations of visual primitives in an end-to-end manner. The key to our approach is exploiting the dependency between states, objects, and their compositions within a graph structure to enforce the relevant knowledge transfer from seen to unseen compositions. By learning a joint compatibility that encodes semantics between concepts, our model allows for generalization to unseen compositions without relying on an external knowledge base like WordNet. We show that in the challenging generalized compositional zero-shot setting our CGE significantly outperforms the state of the art on MIT-States and UT-Zappos. We also propose a new benchmark for this task based on the recent GQA dataset.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2102.01987", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/NaeemXTA21", "doi": "10.1109/cvpr46437.2021.00101"}}, "content": {"source": {"pdf_hash": "480d14138c64f43c8b58a9ddd4e4ae08d2794396", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2102.01987v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2102.01987", "status": "GREEN"}}, "grobid": {"id": "d808f9e54103451646ca24951c7b400a3fd6d750", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/480d14138c64f43c8b58a9ddd4e4ae08d2794396.txt", "contents": "\nLearning Graph Embeddings for Compositional Zero-shot Learning\n\n\nMuhammad Ferjad Naeem \nUniversity of T\u00fcbingen\n\n\nTechnische Universit\u00e4t\nM\u00fcnchen 4 Google\n\nYongqin Xian yxian@mpi-inf.mpg.de \nMax Planck Institute for Informatics\n\n\nFederico Tombari tombari@in.tum.de \nTechnische Universit\u00e4t\nM\u00fcnchen 4 Google\n\nZeynep Akata zeynep.akata@uni-tuebingen.de \nUniversity of T\u00fcbingen\n\n\nLearning Graph Embeddings for Compositional Zero-shot Learning\n\nIn compositional zero-shot learning, the goal is to recognize unseen compositions (e.g. old dog) of observed visual primitives states (e.g. old, cute) and objects (e.g. car, dog) in the training set. This is challenging because the same state can for example alter the visual appearance of a dog drastically differently from a car. As a solution, we propose a novel graph formulation called Compositional Graph Embedding (CGE) that learns image features, compositional classifiers and latent representations of visual primitives in an end-to-end manner. The key to our approach is exploiting the dependency between states, objects and their compositions within a graph structure to enforce the relevant knowledge transfer from seen to unseen compositions. By learning a joint compatibility that encodes semantics between concepts, our model allows for generalization to unseen compositions without relying on an external knowledge base like WordNet. We show that in the challenging generalized compositional zero-shot setting our CGE significantly outperforms the state of the art on MIT-States and UT-Zappos. We also propose a new benchmark for this task based on the recent GQA dataset.\n\nIntroduction\n\nA \"black swan\" was ironically used as a metaphor in the 16th century for an unlikely event because the western world had only seen white swans. Yet when the European settlers observed a black swan for the first time in Australia in 1697, they immediately knew what it was. This is because humans posses the ability to compose their knowledge of known entities to generalize to novel concepts. Since visual concepts follow a long tailed distribution [44,49], it is not possible to gather supervision for all concepts. Therefore, recognizing shared and discriminative properties of objects and reasoning about their various states has evolved as an essential part of human intel-  Figure 1: We aim to build a classifier for a novel state of a known object (e.g. old dog) given the knowledge of the shared primitives state and object in the training set.\n\nligence. Once familiar with the semantic meaning of these concepts, we can recognize unseen compositions of them without any supervision. While there is a certain degree of compositionality in modern vision systems, e.g. feature sharing, most models are not compositional in the classifier space and treat every class as an independent entity requiring training for any new concept.\n\nIn this work, we study the state-object compositionality problem also known as Compositional Zero-Shot Learning (CZSL) [34]. The goal is to learn the compositionality of observed objects and their states as visual primitives to generalize to novel compositions of them as shown in figure 1. Some notable existing works in this field include learning a transformation network on top of individual classifiers [34], treating states as linear transformations of object vectors [35], learning modular networks conditioned on compositional classes [40] and learning object embeddings that are symmetric under different states [28]. However, these works treat each state-object composition independently, ignoring the rich dependency structure of different states, objects and their compositions. For example, learning the composition old dog is not only dependent on the state old and object dog, but also can be supported by other compositions like cute dog, old car, etc. We argue that such dependency structure provides a strong regularization which allows the network to better generalize to novel compositions. We therefore propose to exploit this dependency relationship by constructing a compositional graph to learn embeddings that are globally consistent.\n\nOur contributions are as follows: (1) We introduce a novel graph formulation named Compositional Graph Embedding (CGE) to model the dependency relationship of visual primitives and compositional classes. This graph can be created independently of an external knowledge base like WordNet [32]. (2) Observing that visual primitives are dependent on each other and their compositional classes (figure 1), we propose a multimodal compatibility learning framework that learns to embed related states, objects and their compositions close to each other and far away from the unrelated ones. (3) We propose a new benchmark called C-GQA for the task of CZSL. This dataset is curated from the recent GQA [15] dataset with diverse compositional classes and clean annotations compared to datasets used in the community. (4) Our model significantly improves the state of the art on all the metrics on MIT-States, UT-Zappos and C-GQA datasets.\n\n\nRelated work\n\nCompositionality can loosely be defined as the ability to decompose an observation into its primitives. These primitives can then be used for complex reasoning. One of the earliest attempts in computer vision in this direction can be traced to Hoffman [14] and Biederman [4] who theorized that visual systems can mimic compositionality by decomposing objects to their parts. Compositionality at a fundamental level is already included in modern vision systems. Convolutional Neural Networks (CNN) have been shown to exploit compositionality by learning a hierarchy of features [58,25]. Transfer learning [6,8,10,38] and few-shot learning [12,41,30] exploit the compositionality of pretrained features to generalize to data constraint environments. Visual scene understanding [18,9,17,29] aims to understand the compositionality of concepts in a scene. Nevertheless, these approaches still requires collecting data for new classes.\n\nZero-Shot Learning aims at recognizing novel classes that are not observed during training [24]. This is accomplished by using side information that describes novel classes e.g. attributes [24], text descriptions [42] or word embeddings [45]. Some notable approaches include learning a compatibility function between image and class embeddings [1,59] and learning to generate image features for novel classes [53,60]. Graph convolutional networks (GCN) [21,47,19] have shown to be promising for zero-shot learning. Wang et al. [47] propose to directly regress the classifier weights of novel classes with a GCN operated on an external knowledge graph (WordNet [32]). Kampffmeyer et al. [19] improve this formulation by intro-ducing a dense graph to learn a shallow GCN as a remedy for the laplacian smoothing problem [27]. Graph Convolutional Networks are a special type of neural networks that exploit the dependency structure of data (nodes) defined in a graph. Current methods [21] are limited by the network depth due to over smoothing at deeper layers of the network. The extreme case of this can cause all nodes to converge to the same value [27]. Several works have tried to remedy this by dense skip connections [54,26], randomly dropping edges [43] and applying a linear combination of neighbor features [50,23,22]. A recent work in this direction from Chen et al. [33] combines residual connections with identity mapping. Compositional zero-shot learning stands at the intersection of compositionality and zero-shot learning and focuses on state and object relations. We aim to learn the compositionality of objects and their states from the training set and are tasked with generalizing to unseen combination of these primitives. Approaches in this direction can be divided into two groups. The first group is directly inspired by [14,4]. Some notable methods including learning a transformation upon individual classifiers of states and objects [34], modeling each state as a linear transformation of objects [35], learning a hierarchical decomposition and composition of visual primitives [55] and modeling objects to be symmetric under attribute transformations [28]. An alternate line of works argues that compositionality requires learning a joint compatibility function with respect to the image, the state and the object [2,40,48]. This is achieved by learning a modular networks conditioned on each composition [40,48] that can be \"rewired\" for a new compositions. Finally a recent work from Atzmon et al. [2] argue that achieving generalization in CZSL requires learning the causality of visual transformation through a causal graph where the latent representation of primitives are independent of each other.\n\nOur proposed method lies at the intersection of several discussed approaches. We learn a joint compatibility function similar to [2,40,48] and utilize a GCN similar to [47,19]. However, our approach exploits the dependency structure between states, objects and compositons which has been overlooked by previous CZSL approaches [2,40,48]. Instead of using a predefined knowledge graph like WordNet [32] to regress pretrained classifiers of the seen classes [47,19], we propose a novel way to build a compositional graph and learn classifiers for all classes in an end-to-end manner. In contrast to Atzmon et al. [2] we explicitly promote the dependency between all primitives and their compositions in our graph. This allows us to learn embeddings that are consistent with the whole graph. Finally, unlike all existing methods [34,35,2,40,48,55], we do not rely on a fixed image feature extractor and train our pipeline end-to-end.\n\n\nNovel\n\n\nInput Graph\n\nOutput  Figure 2: Compositional Graph Embed (CGE) learns a globally consistent joint embedding space between image features and classes of seen and unseen compositions from a graph. In our novel graph formulation, nodes are connected if a dependency exists in form of a compositional label e.g. old, car and old car. We backpropagate the classification loss through the seen compositional nodes to the GCN G and the feature extractor F. Hence, the representation of e.g. the dog is compatible with its different states and the representation of old dog aggregates the knowledge from old, dog, cute dog, old car etc.\n\n\nApproach\n\nWe consider the image classification task where each image is associated with a label that is composed of a state (e.g. cute) and an object (e.g. dog). The goal of compositional zero-shot learning (CZSL) [34] is to recognize the compositional labels that are not observed during training. This is particularly challenging as the states significantly change the visual appearance of an object hindering the performance of the classifiers.\n\nWe propose a novel formulation to the problem, namely Compositional Graph Embedding (CGE), which constructs a compositional graph and adopts a graph convolutional network to learn the dependency structure between labels. An overview of our approach is shown in Figure 2. It builds on the compatibility learning framework that learns a classagnostic scoring function between an image and a compositional label. The input image is encoded with an image feature extractor F, while the classifier weights for the compositional label are learned by a composition function G. The key insight of our approach is that leveraging the dependency relationship between states, objects and their compositions is beneficial for recognizing unseen compositions.\n\n\nCompatibility Learning Framework for CZSL\n\nTask formulation. We formalize the CZSL task as follows. Let T = {(x, y)|x \u2208 X , y \u2208 Y s } where T stands for the training set, x denotes an image in the RGB image space X and y is its label belonging to one of the seen labels Y s . Each label is a tuple y = (s, o) of a state s \u2208 S and an object o \u2208 O with S and O being the set of states and objects respectively. The task of CZSL is to predict a set of novel labels Y n that consists of novel compositions of states S and objects O i.e., Y s \u2229 Y n = \u2205. Following [40,52], we study this problem in the generalized compositional zeroshot setting where the test set includes images from both seen and novel compositional labels Y = Y s \u222a Y n . Compatibility function. Learning state and object classifiers separately is prone to overfit to labels observed during training because states and objects are not independent e.g. the appearance of the state sliced varies significantly with the object (e.g. apple or bread). Therefore, we chose to model them jointly by learning a compatibility function f : X \u00d7 S \u00d7 O \u2212\u2192 R that captures the compatibility score between an image, a state and an object. Given a specific input image x, we predict its label y = (s, o) by searching the state and object composition that yields the highest compatibility score:\nf (x, s, o) = F(x; W ) \u00b7 G(s, o; \u0398)(1)\nwhere F(x; W ) \u2208 R d is the image feature extracted from a pretrained feature extractor, G(s, o; \u0398) \u2208 R d is a function that outputs the label embedding of the state-object pair (s, o), (W ,\u0398) are respectively the learnable parameters of F and G, and (\u00b7) is the dot product. The compatibility function assigns high scores to the correct triplets i.e., image x and its label (s, o), and low scores to the incorrect ones. The label embedding can be also interpreted as the classifier weights for the label (s, o) and we use the two terms interchangeably.\n\nOur compatibility learning framework is closely related to [34,40]. LabelEmbed [34] parameterizes the compositional embedding function with a multi-layer perceptron and computes the compositions from the word embeddings (e.g. word2vec [31]) of states and objects, while TMN [40] adopts a modular network as the image feature extractor and a gating network as the compositional embedding function. We argue that there exists a complex de-pendency structure between states, objects and their compositions and learning this dependency structure is crucial. To this end, we propose to integrate the compositional embedding function G as a graph convolutional neural network (GCN) which adds an inductive bias to the inherent structure between states, objects, and their combination defined by our compositional graph introduced next.\n\n\nCompositional Graph Embedding (CGE)\n\nWe propose the Compositional Graph Embedding (CGE) framework integrating the Graph Convolutional Networks (GCN) [21] to the compositional embedding function G(s, o) that learns the label embedding for each compositional label y = (s, o) \u2208 Y in an end to end manner. The GCN network exploits the dependency structure in a predefined compositional graph from states, objects and their compositions (including both seen and unseen labels). In the following, we first define the compositional graph, then introduce the node features and finally explain how to learn a GCN for the CZSL task. Compositional graph. Our graph consists of K = |S| + |O| + |Y| nodes that represent states S, objects O and compositional labels Y. Two nodes are connected if they are related. The key insight of our graph is that each compositional label y = (s, o) \u2208 Y defines a dependency relationship between the state s, object o and their composition y.\n\nTo this end, we build the edges of the graph by connecting (s, o), (s, y) and (o, y) for every y = (s, o) \u2208 Y. In addition, each node is also connected to itself. Note that the edges in our graph are unweighted and undirected, leading to a symmetric adjacency matrix L \u2208 R K\u00d7K where element L ij = 1 if there is a connection between nodes i and j otherwise L ij = 0. Despite its simplicity, we find that our compositional graph provides the accurate dependency structure to recognize unseen compositional labels. Node features. GCN [21,33] operates on node features in a neighborhood defined by the graph. Therefore, after obtaining the compositional graph, we need to represent each node with a proper feature embedding. We chose to use the word embeddings [31,5] pretrained on a large text corpus e.g. Wikipedia, because they capture rich semantic similarities among words i.e., dog is closer to cat than to car in the word embedding space. Specifically, every state or object node in the compositional graph is represented by the word embedding associated to its corresponding state or object name. We compute the node features of the the compositional label (e.g. cute dog) by averaging the word embeddings of the corresponding state (e.g. cute) and object (e.g. dog) names. As indicated in [31], by adding word embeddings we achieve compositionality in the semantic space. We represent the input node features with a matrix E \u2208 R K\u00d7P where N is the total number of nodes and each row denotes the P -dim feature of a graph node.\n\nGraph convolutional network for CZSL. GCN [21] is an efficient multi-layer network to learn new feature representation of nodes for a downstream task that are consistent with the graph structure. Here, we apply the GCN to tackle the CZSL task by directly predicting the compositional label embeddings. The input of our GCN consists of the compositional graph, represented by the adjencency matrix L \u2208 R K\u00d7K and the node feature matrix E \u2208 R K\u00d7P . Specifically, each GCN layer computes the hidden representation of each node by convolving over neighbor nodes using a simple propagation rule [21] also known as a spectral convolution,\nH (l+1) = \u03c3(D \u22121 LH (l) \u0398 (l) )(2)\nwhere \u03c3 represents the non-linearity activation function ReLU, H (l) \u2208 R K\u00d7U denotes the hidden representations in the l th layer with H (0) = E and \u0398 \u2208 R U \u00d7V is the trainable weight matrix with V learnable filters operating over U features of H (l) . D \u2208 R K\u00d7K is a diagonal node degree matrix which normalizes rows in L to preserve the scale of the feature vectors. By stacking multiple such layers, the GCN propagates the information through the graph to obtain better node embeddings for both the seen and unseen compositional labels. For example, our GCN allows an unseen compositional label e.g. old dog to aggregate information from its neighbor nodes e.g. old, dog, cute dog, and old car that are observed (see Figure 2). Objective. As the objective of the GCN is to predict the classifier weights of the compositional labels, the node embedding of the output layer in the GCN has the same dimentionality as the image feature F(x). This indicates that our compositional embedding function becomes\nG(s, o) = H (N ) y\nwhere H (N ) is the output node embedding matrix and H (N ) y denotes the row corresponding to the compositional label y = (s, o). We then optimize the following crossentropy loss to jointly learn the image feature extractor and GCN in an end-to-end manner,\nmin W,\u0398 1 |T | |T | i=1 \u2212log( exp f (x i , s i , o i ) j\u2208Ys exp f (x i , s j , o j ) )(3)\nwhere f is the compatibility function defined in Equation 1, It is worth noting that our model works in the challenging generalized CZSL setting [40], where both seen and unseen compositional classes (i.e. Y = Y s \u222a Y n ) are predicted.\ny = (s i , o i ) is the ground truth label of image x i , label y = (s j , o j ) denotes\nDiscussion. To the best of our knowledge, our Compositional Graph Embedding (CGE) is the first end-to-end learning method that jointly optimizes the feature extractor F and the compositional embedding function G for the task of compositional zero-shot learning. Compared to prior CZSL works [40,28,34,35] our CGE does not overfit while optimizing the CNN backbone of F (see supplementary) as it is regularized by the compositional graph that defines the dependency relationship between classes making the end-to-end training beneficial. Compared to previous GCN work [47,19] that utilizes GCNs to regress the fixed classifier weights to learn classifiers of novel classes, we directly use image information to learn classifiers for both seen and novel classes. Moreover, while [47,19] rely on a known knowledge graph like WordNet [32] describing the relation of novel classes to seen classes, our CGE cannot rely on existing knowledge graphs like WordNet [32] because they do not cover compositional labels. Therefore, we propose to construct the graph by exploiting the dependency relationship defined in the compositional classes. We find that propagating information from seen to unseen labels through this graph is crucial for boosting the CZSL performance.\n\n\nExperiments\n\nAfter introducing our experimental setup, we compare our results with the state of the art, ablate over our design choices and present some qualitative results. Datasets. We perform our experiments on three datasets (see detailed statistics in Table 1). MIT-States [16] consists of natural objects in different states collected using an older search engine with limited human annotation leading to significant label noise [2]. UT-Zappos [56,57] consists of images of a shoes catalogue which is arguably not entirely compositional as states like Faux leather vs Leather are material differences not always observable as visual transformations. We use the generalized CZSL splits from [40].\n\nTo address the limitations of these two datasets,we propose a split built on top of Stanford GQA dataset [15] originally proposed for VQA and name it Compositional GQA (C-GQA) dataset (see supplementary for the details). C-GQA contains over 9.5k compositional labels making it the most extensive dataset for CZSL. With cleaner labels and a larger label space, our hope is that this dataset will inspire further research on the topic. Figure 4 shows some samples from the three datasets. Metrics. As the models in zero-shot learning problems are trained only on seen Y s labels (compositions), there is an inherent bias against the unseen Y n labels. As pointed out by [7,40], the model thus needs to be calibrated by adding a scalar bias to the activations of the novel compositions to find the best operating point and evaluate the generalized CZSL performance [40] for a more realistic setting.\n\nWe adopt the evaluation protocol of [40] and report the Area Under the Curve (AUC) (in %) between the accuracy on seen and unseen compositions at different operating points with respect to the bias. The best unseen accuracy is calculated when the bias term is large leading to predicting only the unseen labels, also known as zero-shot performance. In addition, the best seen (base class) performance is calculated when the bias term is negative leading to predicting only the seen labels. As a balance between the two, we also report the best harmonic mean. To emphasize that this is different from the traditional zero-shot learning evaluation, we add the term \"best\" in our results. Finally, we report the state and object accuracy on the novel labels to show the improvement in classifying the visual primitives. We emphasize that the dataset splits we propose for C-GQA and use from [40] for MIT-States and UT-Zappos do no not violate the zero-shot assumption as results are ablated on the validation set. Some works in CZSL use older splits that lack a validation set and thus use indirect full label supervision [52] by ablating over the test set. We therefore advice future works to rely on the new splits. Training details. To be consistent with the state of the art, we use a ResNet18 [13] backbone pretrained on ImageNet as the image feature extractor F. For a fair comparison with the models that use a fixed feature extractor, we introduce a simplification of our method named CGE f f . We learn a 3 layer fully-connected (FC) network with ReLU [36], LayerNorm [3] and Dropout [46] while keeping the feature extractor fixed for this baseline. We use a shallow 2-layer GCN with a hidden dimension of 4096 as G (detailed ablation on this is presented in section 4.2). On MIT-States, we initialize our word embeddings with a concatenation of pretrained fasttext [5] and word2vec models [31] similar to [51]. On UT-Zappos and C-GQA, we initialize the word embed-\n\n\nMIT-States\n\nUT-Zap50K C-GQA  dings with word2vec(ablation reported in supplementary).\n\nWe use Adam Optimizer [20] with a learning rate of 5e \u22126 for F and a learning rate of 5e \u22125 for G. We implement our method in PyTorch [37] and train on a Nvidia V100 GPU. For state-of-the-art comparisons, we use the authors' implementations where available. The code for our method and the new dataset C-GQA will be released upon acceptance.\n\n\nComparing with the State of the Art\n\nWe compare our results with the state of the art in Table 2 and show that our Compositional Graph Embed(CGE) outperforms all previous methods by a large margin and establishes a new state of the art for Compositional Zero-shot Learning. Our detailed observations are as follows. Generalized CZSL performance. Our framework demonstrates robustness against the label noise on MIT-States noted previously in [2]. For the generalized CZSL task, our CGE achieves a test AUC of 6.5% which is an improvement of over 2\u00d7 compared to the last best 3.0% from SymNet. Similarly, as our method does not only improve results on seen labels but also unseen labels, it significantly boosts the state of the art harmonic mean, i.e. 16.1% to 21.4%. When it comes to state and object prediction accuracy, we observe an improvement from 26.3% to 30.1% for states and 28.3% to 34.7% for objects. Although our results significantly improve the state of the art on all metrics, the state and object accuracies are quite low, partially due to the label noise for this dataset.\n\nSimilar observations are confirmed on UT-Zappos, where we achieve a significant improvement on the state of the art with an AUC of 33.5% compared to 29.3% from TMN. An interesting observation is that SymNet, i.e. the state of the art on MIT States, with an AUC of 23.9% does not achieve the best performance in the generalized CZSL setting on UT Zappos. We conjecture that this is because the state labels in this dataset are not entirely representing vi-sual transformations, something this method was designed to exploit. In this dataset, our fully compositional model improves the best harmonic mean wrt the state of the art significantly (45.0% with TMN vs 60.5% ours). Note that, this is due to a significant accuracy boost achieved on unseen compositions (60.0% vs 71.5%).\n\nFinally on the proposed splits of the GQA dataset [15], i.e. C-GQA dataset, we achieve a test AUC of 2.5% outperforming the closest baseline by a 2.5\u00d7. Note that, since C-GQA has a compositional space of over 9.5k concepts, it is significantly harder than MIT-States and UT-Zappos while being truly compositional and containing cleaner labels. The state and the object accuracies of our method are 22.8% and 32.2%, i.e. significantly higher than the state of the art. However these results also indicate the general difficulty of the task. Similarly, our best seen and best unseen accuracies (28.2% and 10.3%) indicate a large room for improvement on this dataset, which may encourage further research with our C-GQA dataset on the CZSL task.\n\nWe also make an interesting observation on all three datasets. While SymNet uses an object classifier that is trained independently from the compositional pipeline, our method consistently outperforms it on object accuracy. We conjecture that this is because a compositional network sensitive to the information about the states is also a better object classifier, since it disentangles what it means to be an object from the state it is observed in, preventing biases to properties like textures [11]. This insight can be an avenue for future improvement in object classification. Impact of feature representations. To quantify the improvement of our graph formulation on the same feature representations as the state of the art, we also present results of our CGE with a fixed feature extractor (Resnet18), i.e. denoted by CGE f f , in Table 2. We see that this version of our model also consistently outperforms the state of the art by a large margin on MIT-States and C-GQA while match-  Table 3: Ablation over the graph connections validates the structure of our proposed graph on the validation set of MIT-States dataset. We start from directly using the word embeddings as classifier weights to learning a globally consistent embedding from a GCN as the classifier weights (s: states, o: objects, y: compositional labels).\n\n\nConnections in\n\ning the performance on UT-Zappos. Especially on MIT-States, the improvement over the state of the art is remarkable, i.e. 5.1% test AUC vs 3.0% test AUC with SymNet.\n\nIn summary, this shows that our method benefits from both the knowledge propagation in the compositional graph and from learning better image representations. For a fair comparison, we also allowed the previous baselines to train end-to-end with F. However, this results in a significant performance drop indicating they are unable to jointly learn the feature extractor against the RGB space. To address this limitation, some works [53,48] have proposed to use a generative network to learn the distribution of image features in zero-shot problems. We, on the other hand, don't need to rely on an expensive generative network and jointly learn the image representations and the compositional classifiers in an end-to-end manner.\n\n\nAblation study\n\nIn this section we ablate our CGE model with respect to the graph connections, the graph depth and graph convolution variants. Graph connections. We perform an ablation study with respect to the various connections in our compositional graph on the validation set of MIT-States and report results in Table 3. In the Direct Word Embedding variant, i.e. row (a) our label embedding function G is an average operation of state and object word embeddings. We see that, directly using word embedding of compositional labels as the classifier weights leads to an AUC of 5.9. In row (b) we represent a graph with connections between states (s) to compositional labels (y) and objects to compositional labels (y) but remove the self connection for the compositional label. In this case, the final representation of compositional labels from the GCN only combines the hidden representations of states and objects leading to an AUC of 7.6.\n\nRow (c) represents the graph that has self connections from each compositional label in addition to the connec-  Figure 3: Graph convolution and depth: We compare the spectral convolution GCN [21] with the recent GCNII [33] that aims to address the over smoothing issue at increasing depth. We perform the comparison at various depths of the GCN network on the validation set of MIT-States.\n\ntions between states and compositional labels as well as objects and compositional labels as in row (b). We see that this variant achieves an AUC of 8.1 indicating that the hidden representation of compositional classes is beneficial. Row (d) is our final model where we additionally incorporate the connections between states and objects in a pair to model the dependency between them. We observe that learning a representation that is consistent with states, objects and the compositional labels increases the AUC from 8.1 to 8.6 validating our choice of connections in the graph. We again emphasize that in the absence of an existing knowledge graph for compositional relations, our simple but well designed graph structure is able to capture the dependencies between various concepts.\n\nWhile our final CGE does not employ an external knowledge graph, we can utilize an existing graph like Word-Net [32] to get the hierarchy of the object classes similar to some baselines in zero-shot learning [47,19]. Row (e) represents a model that exploits object hierarchy in addition to our compositional graph discussed earlier. This leads to additional 418 nodes to model the parent child relation of the objects. We see that this results in a slight performance drop with an AUC of 7.9 because this graph may not be compatible with the compositional relations.\n\nGraph architecture. We ablate over the architecture of the graph at various depths from 2-8 layers to quantify the degree of knowledge propagation needed to achieve best performance. From Figure 3 we observe that a shallow architecture at 2 layers achieves the best AUC of 8.6 outperforming the deeper configuration. This is an established problem for the spectral graph convolution and is caused by laplacian smoothing across deeper layers [27]. To study if we are limited by a shallow representation, we use a more recent formulation of graph convolution named GCNII [33]. This method introduces a few key improvements like skip connections that remedy the laplacian smoothing problem. We see that while GCNII suffers less from the smoothing prob-\n\n\nSmall Elephant\n\n\nSmall Elephant Tiny Elephant Young Elephant\n\n\nCrushed Granite\n\n\nCrushed Granite Crushed Rock Crushed Stone\n\n\nWet Cat\n\n\nTiny Animal Wet Cat Young Cat\n\n\nThawed Meat\n\n\nDamp Basement Wet Basement Wet Foam\n\n\nRusty Gate\n\n\nRusty Gate Weathered Steel Rusty Fence\n\n\nSatin Sandals\n\n\nSatin Sandals Haircalf Heels Satin Heels\n\n\nSynthetic Midcalf\n\n\nSynthetic Midcalf Leather Kneehigh Synthetic Kneehigh\n\n\nCanvas Loafers\n\n\nCanvas Loafers Canvas Flats Cotton Flats\n\n\nSuede Midcalf\n\nSynthetic Ankle Suede Midcalf\n\n\nSheepskin Ankle\n\n\nLeather Slippers\n\n\nCanvas Flats Leather Flats Canvas Loafers\n\n\nBaseball Field\n\nBaseball Field Brown Field Large Field\n\n\nWhite Bus\n\nGray Bus White Bus Silver Bus\n\n\nStripped Person\n\nStanding Person Orange Jacket Blue Shirt\n\n\nRed Coat\n\nRed Coat Red Jacket Maroon Jacket lem and maintains performance at deeper architectures, It only achieves an AUC of 7.2 for the best performing model. Since our graph is exploiting close relations between the states, objects and the compositions introduced by the dense connections for visual primitives, we are not held back by the shallow architecture. We advice future works to explore richer graphs for compositional tasks that can facilitate deeper models.\n\n\nCovered Hill\n\n\nQualitative results\n\nWe show some qualitative results for the novel compositions with their top-3 predictions in Figure 4 (left). The first three columns present some instances where the top prediction matches the label. For MIT-States and C-GQA, we notice that the remaining two answers of the model contain information visually present in the image but not in the label highlighting a limitation of current CZSL benchmarks. Different groups of states like color, age, material etc. can represent different information for the same object and are thus not mutually exclusive.\n\nFor example in column 4, row 1 the image of the cat consist of a size, surface texture and age all present in the label space of the dataset and the output of the model. However the label for this image only contains its surface texture. This puts an upper limit on compositional class accuracy dependent on the number of groups associated with an object in the label space. Specifically, column 4 of Figure 4 (left) counts as a wrong prediction but all of the top 3 predictions represent correct visual information for MIT-States and C-GQA. Unless the model learns the annotator bias, it can not achieve a perfect accuracy. Finally in column 5, we show some instances of sub-optimal and wrong labels. Specifically, the image in row 1 is entirely missing the thawed meat represented in the label, the image in row 2 can not sufficiently communicate the material information while the label in row 3 does not contain the dominant information in the image.\n\nIn Figure 4 (right) we first show image retrieval results from seen and unseen compositions. We can see that for all three datasets our method returns the correct top images for the query. We then perform cross-dataset retrieval between MIT-States and C-GQA for an unseen composition. We show a representative image from the original dataset and the top-3 retrievals from the cross dataset. While the datasets have a distribution shift between them, we see that retrievals are still meaningful. On MIT-States 2/3 retrieved images match a Mossy pond while the 3rd image is a grass field confused with the query. Similar trend is observed for the model trained on C-GQA for retrieval of a puffy pizza. The model confuses the top retrieval with a casserole followed by two images of pizzas. Nevertheless, the cross dataset retrieval shows promise towards further generalization for future works.\n\n\nConclusion\n\nWe propose a novel graph formulation for Compositional Zero-shot learning in the challenging generalized zero-shot setting. Since our graph does not depend on external knowledge bases, it can readily be applied to a wide variety of compositional problems. By propagating knowledge in the graph against training images of seen compositions, we learn classifiers for all compositions end-to-end. Our graph also acts like a regularizer and allows us to learn image representations consistent with the compositional nature of the task. We benchmark our method against various baselines on three datasets to establish a new state of the art in CZSL in all settings. We also highlight the limitations of existing methods and knowledge bases. We encourage future works to explore datasets with structured compositional relations and richer graphs that will allow for deeper graph models.\n\n\nMIT-States\n\nUT-Zap50K C- GQA  Val AUC  Test AUC  Val AUC  Test AUC  Val AUC  Test AUC  Top k\u2192  1  2  3  1  2  3  1  2  3  1  2  3  1  2  3  1 2 3 AttOp  Table 4: AUC in percentage on MIT-States, UT-Zap50K and GQA. We consistently outperform the baselines by a significant margin.\n\nWe define a novel composition as a state-object pair not present in the training set. We now want to generate a validation and test set consisting of seen and novel compositions. We partition the testset of GQA randomly with respect to scene graphs into the validation and test sets of C-GQA with a probability of 0.45 and 0.55 respectively. These numbers are chosen as the test set of C-GQA losses bounding boxes overlapping with the novel compositions in validation set. From the bounding boxes, we add the novel compositions in these graphs to the unseen set Y n\u2212val and Y n\u2212test respectively. However, the number of novel compositions is very small compared to the compositions in the training set represented by Y s . Therefore, we further divide the remaining compositions in validation randomly into Y s and Y n\u2212val . We then remove the unseen compositions of the validation set from the test set and divide the remaining compositions randomly into Y s and Y n\u2212test . Finally we remove the novel compositions Y n\u2212val and Y n\u2212test from Y s and generate the images from the bounding boxes of the scene graph for the 3 sets.\n\nThis results in a training set consisting of 7882 pairs across 26k images; a validation set consists of 893 seen and 834 unseen pair across 4k images; and a testset consists of 845 seen and 705 unseen pairs across 5k images. In total C-GQA has a compositional space of over 9.5k compositional concepts making it the most extensive dataset for CZSL. With cleaner labels and a bigger label space, we hope this dataset is able to accelerate the research in the field.\n\n\nB. Additional Experiments\nB.1. AUC at different k.\nWe reported the top1 AUC for the three datasets in the  \n\n\nB.2. End-to-end training with baselines\n\nWe studied the impact of feature representations on the performance of the model in section 4 of the main paper. We showed that our model CGE benefits greatly from end to end training. Older baselines TMN and Symnet operate on a frozen ImageNet trained Resnet18 CNN backbone for feature extraction in their respective manuscripts. In this experiment, we train TMN and Symnet end to end (represented by EE) from the ImageNet-pretrained Resnet18 (the same with our CGE) and quantify if they are held back by the ImageNet representations on the validation set of MIT-States.\n\nWe see from table 5 that finetuning the CNN backbone results in worse performance than in the original implementations as they are overfitting to the training set with an AUC of 2.9 for TMN [40] and 3.9 for SymNet [28], while end-toend training is beneficial for our CGE which achieves an AUC of 8.6 because of our graph regularization.\n\n\nB.3. Ablation over the GCN\n\nWe reported ablation over various components of the Graph Convolutional Network (GCN) used in our model in the section 4.2 of the main manuscript. We ablate over the remaining components of the GCN. For these experiments, we use the fixed feature extractor version of our model CGE f f to quantify the improvements directly from  Hidden dim 2056 6.00 6.10 6.00 5.92 5.84 4096 6.11 6.00 6.22 6.11 5.76 8184 6.54 6.14 6.00 5.61 5.32 (b) Ablation over GCNII Table 7: Ablation over the depth and hidden dimension of the GCN on CGE ff the graph wrt to the word embeddings used for initialization and the learnable GCN configuration. Choice of embedding. We test three popular word embedding models and the concatenation of their features for every word to study their impact on the performance of our model. We report the results in Table 6. We see that MIT-States benefits most from the concatenation of fasttext and word2vec models as these models are closely related to achieve a AUC of 6.8. While UT-Zappos and C-GQA achieve the best results with Word2Vec at 38.7 and 3.5 AUC respectively.\n\nGraph architecture. We ablate over the learnable architecture of GCN at different depth and hidden dimension on the validation set of MIT-States and report results in table 7a. We observe that increasing the hidden dimension is generally beneficial when we go from 1024 to 4096 as the performance increases from an AUC of 6.53 to 6.80. However, increasing the hidden dimension from 4096 to 8192 decreases the AUC to 6.59 at 2 layers of GCN. Increasing the depth of the GCN network generally results in a decrease in performance across all hidden dimensions. In particular, at 4096 the AUC decreases from 6.80 AUC to 4.95. In order to validate if this is a consequence of laplacian smoothing we use a recent version of graph convolution called GCNII [33]. We see from table 7b that the performance decrease across columns is less pronounced at different hidden dimensions for this model. However, the best AUC achieved at 6.54 is less than we achieved with the original GCN indicating that this version of graph convolution is less beneficial for our problem.\n\nFigure 4 :\n4Qualitative results. Left: We show the top-3 predictions of our model for some examples. We observe from the first four columns that all the predictions of the model are meaningful, but the model is only incentivized when it matches the label. The task of CZSL is a multi label one and future datasets need to account for this. The last column shows some examples of suboptimal labels and wrong predictions. Right: We show good candidates for retrival on all three dataset and then we perform cross-dataset retrieval for a unseen composition across C-GQA and MIT-States.\n\nTable 1 :\n1any seen compositional class, W and \u0398 are the learnable parameters of the feature extractor and the GCN respectively. Intuitively, the cross-entropy loss enables the compatibility function to assign the high scores for correct input triplets. Inference. At test time, given an input image x, we derive a prediction by searching the compositional label that yields the highest compatibility score, Dataset statistics for CZSL: We use three datasets to benchmark our method against the baselines. C-GQA (ours): our proposed dataset splits from Stanford GQA dataset[15]. (s: # states, o: # objects, sp: # seen compositions, up: # unseen compositions, i: # images)arg max \n\ny=(s,o)\u2208Y \n\nf (x, s, o). \n(4) \n\n\nTable 2 :\n2Comparison with the state of the art: We compare our Compositional Graph Embed (CGE) with the state of theart on Validation and Test AUC (in %); best unseen, seen and harmomic mean (HM) accuracies (in %) as well as state (s) and \nobject (o) prediction accuracies (in %) on widely used MIT-States and UT-Zappos datasets as well as our proposed C-GQA \ndataset. \n\n\n\n\nSymnet[28] 4.3 9.8 14.8 3.0 7.6 12.3 25.9 50.9 64.5 23.9 48.2 64.4 2.9 5.1 7.4 1.0 2.3 3.3 CGEff (Ours) 6.8 14.6 20.2 5.1 11.8 17.3 38.7 60.2 73.2 26.4 55.6 71.0 3.5 7.4 10.4 1.4 3.2 4.5 CGE (Ours) 8.6 17.6 24.9 6.5 14.7 21.3 43.2 64.5 77.7 33.5 64.2 77.5 5.3 10.7 14.6 2.5 4.6 6.4[35] \n2.5 6.2 10.1 1.6 4.7 7.6 21.5 44.2 61.6 25.9 51.3 67.6 0.8 2.2 3.4 0.4 1.1 1.7 \nLE+[34] \n3.0 7.6 12.2 2.0 5.6 9.4 26.4 49.0 66.1 25.7 52.1 67.8 0.9 2.2 3.2 0.4 1.0 1.5 \nTMN[40] \n3.5 8.1 12.4 2.9 7.1 11.5 36.8 57.1 69.2 29.3 55.3 69.8 1.7 4.2 6.4 0.8 1.8 2.9 \n\n\nTable 2\n2of our main manuscript. We report top 1,2,3 AUC for the 3 datasets in table 4 to allow direct comparison with older works that adopt this evaluation. We see that the trend from the main manuscript is consistent at different k. In particular on MIT-States, our model CGE achieves top3 AUC of 21.3 compared to 12.3 of the closest baseline Symnet continuing our 2\u00d7 improvement. On UT-Zappos,Method \nAUC Best HM \n\nTMN EE [40] \n2.9 \n13.0 \nSymnet EE [28] \n3.9 \n15.3 \nCGE (Ours) \n8.6 \n23.4 \n\n\n\nTable 5 :\n5End-to-End training results CGE maintains its lead by achieving a top3 AUC of 77.5 compared to 69.8 of TMN. Finally on C-GQA, CGE again achieves a 2\u00d7 improvement by achieving a top3 AUC of 6.4 compared to 3.3 of Symnet.\n\nTable 6 :\n6Ablation over embedding: We use three popular word embedding models. (ft: Fasttext[5], w2v: Word2Vec[31] and gl: Glove[39])Num layers \n2 \n4 \n6 \n8 \n10 \n\n1024 6.53 6.13 5.58 5.07 4.33 \n\nHidden dim \n\n2056 6.59 6.20 6.14 5.68 5.10 \n4096 6.80 6.30 6.20 5.83 4.95 \n8184 6.59 6.27 6.27 5.63 4.71 \n\n(a) Ablation over GCN \n\nNum layers \n2 \n4 \n6 \n8 \n10 \n\n1024 5.56 5.21 5.44 5.43 5.12 \n\n\nAcknowledgments This work has been partially funded by the ERC under Horizon 2020 program 853489 -DEXIM and by the DFG under Germany's Excellence Strategy -EXC number 2064/1 -Project number 390727645.Supplementary MaterialA. Creating C-GQA We introduced a new benchmark for Compositional Zero-shot Learning (CZSL) in the main manuscript. This benchmarks is based on the original GQA[15]dataset which is annotated with scene graphs where each bounding box is labelled with state-object or any other relations in the scene. For the creation of the benchmark, we only consider the bounding boxes with a single state-object relation to be consistent with existing works. Bounding boxes smaller than 112\u00d7112 are excluded because they are in half the input size of most feature extractors. From these bounding boxes, we collect the vocabulary of state and objects to remove overlapping concepts between state and object classes. We also merge plurals and synonyms using first wordnet[32]and then manual checking. This yields the final vocabulary of 457 states and 893 objects.\nLabel-embedding for attribute-based classification. Zeynep Akata, Florent Perronnin, Zaid Harchaoui, Cordelia Schmid, CVPR. Zeynep Akata, Florent Perronnin, Zaid Harchaoui, and Cordelia Schmid. Label-embedding for attribute-based clas- sification. In CVPR, 2013. 2\n\nA causal view of compositional zero-shot recognition. Yuval Atzmon, Felix Kreuk, Uri Shalit, Gal Chechik, NeurIPS. 6Yuval Atzmon, Felix Kreuk, Uri Shalit, and Gal Chechik. A causal view of compositional zero-shot recognition. In NeurIPS, 2020. 2, 5, 6\n\n. Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E Hin, arXiv:1607.06450ton. Layer normalization. arXiv preprintJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin- ton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. 5\n\nRecognition-by-components: a theory of human image understanding. Irving Biederman, Psychological review. 942115Irving Biederman. Recognition-by-components: a the- ory of human image understanding. Psychological review, 94(2):115, 1987. 2\n\nEnriching word vectors with subword information. Piotr Bojanowski, Edouard Grave, Armand Joulin, Tomas Mikolov, Transactions of the Association for Computational Linguistics. 512Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching word vectors with subword infor- mation. Transactions of the Association for Computational Linguistics, 5:135-146, 2017. 4, 5, 12\n\nMultitask learning. Rich Caruana, Machine learning. 281Rich Caruana. Multitask learning. Machine learning, 28(1):41-75, 1997. 2\n\nAn empirical study and analysis of generalized zeroshot learning for object recognition in the wild. Wei-Lun Chao, Soravit Changpinyo, Boqing Gong, Fei Sha, ECCV. Wei-Lun Chao, Soravit Changpinyo, Boqing Gong, and Fei Sha. An empirical study and analysis of generalized zero- shot learning for object recognition in the wild. In ECCV, 2016. 5\n\nAdding unlabeled samples to categories by learned attributes. Jonghyun Choi, Mohammad Rastegari, Ali Farhadi, Larry S Davis, CVPR. Jonghyun Choi, Mohammad Rastegari, Ali Farhadi, and Larry S Davis. Adding unlabeled samples to categories by learned attributes. In CVPR, 2013. 2\n\nDetecting visual relationships with deep relational networks. Bo Dai, Yuqi Zhang, Dahua Lin, CVPR. Bo Dai, Yuqi Zhang, and Dahua Lin. Detecting visual re- lationships with deep relational networks. In CVPR, 2017. 2\n\nLarge-scale object classification using label relation graphs. Jia Deng, Nan Ding, Yangqing Jia, Andrea Frome, Kevin Murphy, Samy Bengio, Yuan Li, Hartmut Neven, Hartwig Adam, ECCV. Jia Deng, Nan Ding, Yangqing Jia, Andrea Frome, Kevin Murphy, Samy Bengio, Yuan Li, Hartmut Neven, and Hartwig Adam. Large-scale object classification using label relation graphs. In ECCV, 2014. 2\n\nImagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness. Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Wieland Felix A Wichmann, Brendel, ICLR. Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann, and Wieland Brendel. Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness. In ICLR, 2019. 6\n\nLow-shot visual recognition by shrinking and hallucinating features. Bharath Hariharan, Ross Girshick, CVPR. Bharath Hariharan and Ross Girshick. Low-shot visual recognition by shrinking and hallucinating features. In CVPR, 2017. 2\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, CVPR. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 5\n\nParts of recognition. D Donald, Whitman A Hoffman, Richards, Cognition. 181-3Donald D Hoffman and Whitman A Richards. Parts of recog- nition. Cognition, 18(1-3):65-96, 1984. 2\n\nGqa: A new dataset for real-world visual reasoning and compositional question answering. A Drew, Christopher D Hudson, Manning, CVPR. 610Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In CVPR, 2019. 2, 5, 6, 10\n\nDiscovering states and transformations in image collections. Phillip Isola, J Joseph, Edward H Lim, Adelson, CVPR. 512Phillip Isola, Joseph J Lim, and Edward H Adelson. Dis- covering states and transformations in image collections. In CVPR, 2015. 5, 12\n\nTensorize, factorize and regularize: Robust visual relationship learning. Jae Seong, Hwang, N Sathya, Zirui Ravi, Tao, J Hyunwoo, Kim, D Maxwell, Vikas Collins, Singh, CVPR. Seong Jae Hwang, Sathya N Ravi, Zirui Tao, Hyunwoo J Kim, Maxwell D Collins, and Vikas Singh. Tensorize, fac- torize and regularize: Robust visual relationship learning. In CVPR, 2018. 2\n\nImage retrieval using scene graphs. Justin Johnson, Ranjay Krishna, Michael Stark, Li-Jia Li, David Shamma, Michael Bernstein, Li Fei-Fei, CVPR. Justin Johnson, Ranjay Krishna, Michael Stark, Li-Jia Li, David Shamma, Michael Bernstein, and Li Fei-Fei. Image retrieval using scene graphs. In CVPR, 2015. 2\n\nRethinking knowledge graph propagation for zero-shot learning. Michael Kampffmeyer, Yinbo Chen, Xiaodan Liang, Hao Wang, Yujia Zhang, Eric P Xing, CVPR. 7Michael Kampffmeyer, Yinbo Chen, Xiaodan Liang, Hao Wang, Yujia Zhang, and Eric P Xing. Rethinking knowledge graph propagation for zero-shot learning. In CVPR, 2019. 2, 5, 7\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980arXiv preprintDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 6\n\nSemi-supervised classification with graph convolutional networks. N Thomas, Max Kipf, Welling, ICLR. 7Thomas N Kipf and Max Welling. Semi-supervised classifi- cation with graph convolutional networks. In ICLR, 2017. 2, 4, 7\n\nPredict then propagate: Graph neural networks meet personalized pagerank. Johannes Klicpera, Aleksandar Bojchevski, Stephan G\u00fcnnemann, arXiv:1810.05997arXiv preprintJohannes Klicpera, Aleksandar Bojchevski, and Stephan G\u00fcnnemann. Predict then propagate: Graph neural networks meet personalized pagerank. arXiv preprint arXiv:1810.05997, 2018. 2\n\nDiffusion improves graph learning. Johannes Klicpera, Stefan Wei\u00dfenberger, Stephan G\u00fcnnemann, NeurIPS. Johannes Klicpera, Stefan Wei\u00dfenberger, and Stephan G\u00fcnnemann. Diffusion improves graph learning. In NeurIPS, 2019. 2\n\nAttribute-based classification for zero-shot visual object categorization. Christoph Lampert, Hannes Nickisch, and Stefan Harmeling. TPAMIChristoph Lampert, Hannes Nickisch, and Stefan Harmel- ing. Attribute-based classification for zero-shot visual object categorization. In TPAMI, 2013. 2\n\nBackpropagation applied to handwritten zip code recognition. Yann Lecun, Bernhard Boser, S John, Donnie Denker, Richard E Henderson, Wayne Howard, Lawrence D Hubbard, Jackel, Neural computation. 14Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hubbard, and Lawrence D Jackel. Backpropagation applied to handwrit- ten zip code recognition. Neural computation, 1(4):541-551, 1989. 2\n\nDeepgcns: Can gcns go as deep as cnns. Guohao Li, Matthias Muller, Ali Thabet, Bernard Ghanem, CVPR. Guohao Li, Matthias Muller, Ali Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep as cnns? In CVPR, 2019. 2\n\nDeeper Insights into Graph Convolutional Networks for Semi-Supervised Learning. Q Li, Z Han, X.-M Wu, AAAI. 27Q. Li, Z. Han, and X.-M. Wu. Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning. In AAAI, 2018. 2, 7\n\nSymmetry and group in attribute-object compositions. Yong-Lu Li, Yue Xu, Xiaohan Mao, Cewu Lu, CVPR. 611Yong-Lu Li, Yue Xu, Xiaohan Mao, and Cewu Lu. Sym- metry and group in attribute-object compositions. In CVPR, 2020. 1, 2, 5, 6, 11\n\nVisual relationship detection with language priors. Cewu Lu, Ranjay Krishna, Michael Bernstein, Li Fei-Fei, ECCV. Cewu Lu, Ranjay Krishna, Michael Bernstein, and Li Fei- Fei. Visual relationship detection with language priors. In ECCV, 2016. 2\n\nMetric learning for large scale image classification: Generalizing to new classes at near-zero cost. Thomas Mensink, Jakob Verbeek, Florent Perronnin, Gabriela Csurka, ECCV. Thomas Mensink, Jakob Verbeek, Florent Perronnin, and Gabriela Csurka. Metric learning for large scale image clas- sification: Generalizing to new classes at near-zero cost. In ECCV, 2012. 2\n\nDistributed representations of words and phrases and their compositionality. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, Jeff Dean, NIPS. 512Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In NIPS, 2013. 3, 4, 5, 12\n\nIntroduction to wordnet: An on-line lexical database. A George, Richard Miller, Christiane Beckwith, Derek Fellbaum, Katherine J Gross, Miller, International journal of lexicography. 3410George A Miller, Richard Beckwith, Christiane Fellbaum, Derek Gross, and Katherine J Miller. Introduction to word- net: An on-line lexical database. International journal of lexicography, 3(4):235-244, 1990. 2, 5, 7, 10\n\nSimple and deep graph convolutional networks. Zhewei Wei, Ming Chen, Bolin Ding Zengfeng Huang, Yaliang Li, ICML. 712Zhewei Wei Ming Chen, Bolin Ding Zengfeng Huang, and Yaliang Li. Simple and deep graph convolutional networks. In ICML, 2020. 2, 4, 7, 12\n\nFrom red wine to red tomato: Composition with context. Ishan Misra, Abhinav Gupta, Martial Hebert, CVPR. 611Ishan Misra, Abhinav Gupta, and Martial Hebert. From red wine to red tomato: Composition with context. In CVPR, 2017. 1, 2, 3, 5, 6, 11\n\nAttributes as operators: factorizing unseen attribute-object compositions. Tushar Nagarajan, Kristen Grauman, ECCV. 611Tushar Nagarajan and Kristen Grauman. Attributes as op- erators: factorizing unseen attribute-object compositions. In ECCV, 2018. 1, 2, 5, 6, 11\n\nRectified linear units improve restricted boltzmann machines. Vinod Nair, Geoffrey E Hinton, ICML. Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In ICML, 2010. 5\n\nPytorch: An imperative style, high-performance deep learning library. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, NeurIPS. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, 2019. 6\n\nLearning to learn, from transfer learning to domain adaptation: A unifying perspective. Novi Patricia, Barbara Caputo, CVPR. Novi Patricia and Barbara Caputo. Learning to learn, from transfer learning to domain adaptation: A unifying perspec- tive. In CVPR, 2014. 2\n\nGlove: Global vectors for word representation. Jeffrey Pennington, Richard Socher, Christopher D Manning, Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). the 2014 conference on empirical methods in natural language processing (EMNLP)12Jeffrey Pennington, Richard Socher, and Christopher D Man- ning. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532-1543, 2014. 12\n\nTask-driven modular networks for zero-shot compositional learning. Senthil Purushwalkam, Maximilian Nickel, Abhinav Gupta, Marc&apos;aurelio Ranzato, ICCV. 611Senthil Purushwalkam, Maximilian Nickel, Abhinav Gupta, and Marc'Aurelio Ranzato. Task-driven modular networks for zero-shot compositional learning. In ICCV, 2019. 1, 2, 3, 5, 6, 11\n\nOptimization as a model for few-shot learning. Sachin Ravi, Hugo Larochelle, ICLR. Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In ICLR, 2017. 2\n\nLearning deep representations of fine-grained visual descriptions. Scott Reed, Zeynep Akata, Honglak Lee, Bernt Schiele, CVPR. Scott Reed, Zeynep Akata, Honglak Lee, and Bernt Schiele. Learning deep representations of fine-grained visual descrip- tions. In CVPR, 2016. 2\n\nDropedge: Towards deep graph convolutional networks on node classification. Yu Rong, Wenbing Huang, Tingyang Xu, Junzhou Huang, ICLR. Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph convolutional net- works on node classification. In ICLR, 2019. 2\n\nLearning to share visual appearance for multiclass object detection. Ruslan Salakhutdinov, Antonio Torralba, Josh Tenenbaum, CVPR. Ruslan Salakhutdinov, Antonio Torralba, and Josh Tenen- baum. Learning to share visual appearance for multiclass object detection. In CVPR, 2011. 1\n\nZero-shot learning through cross-modal transfer. Richard Socher, Milind Ganjoo, D Christopher, Andrew Manning, Ng, NIPS. Richard Socher, Milind Ganjoo, Christopher D Manning, and Andrew Ng. Zero-shot learning through cross-modal transfer. In NIPS, 2013. 2\n\nDropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov, 15Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1):1929-1958, 2014. 5\n\nZero-shot recognition via semantic embeddings and knowledge graphs. Xiaolong Wang, Yufei Ye, Abhinav Gupta, CVPR. 7Xiaolong Wang, Yufei Ye, and Abhinav Gupta. Zero-shot recognition via semantic embeddings and knowledge graphs. In CVPR, 2018. 2, 5, 7\n\nTask-aware feature generation for zero-shot compositional learning. Xin Wang, Fisher Yu, Trevor Darrell, Joseph E Gonzalez, arXiv:1906.0485427arXiv preprintXin Wang, Fisher Yu, Trevor Darrell, and Joseph E Gonza- lez. Task-aware feature generation for zero-shot composi- tional learning. arXiv preprint arXiv:1906.04854, 2019. 2, 7\n\nLearning to model the tail. Yu-Xiong Wang, Deva Ramanan, Martial Hebert, NIPS. Yu-Xiong Wang, Deva Ramanan, and Martial Hebert. Learn- ing to model the tail. In NIPS, 2017. 1\n\nFelix Wu, Tianyi Zhang, Amauri Holanda De SouzaJr, Christopher Fifty, Tao Yu, Kilian Q Weinberger, arXiv:1902.07153Simplifying graph convolutional networks. arXiv preprintFelix Wu, Tianyi Zhang, Amauri Holanda de Souza Jr, Christopher Fifty, Tao Yu, and Kilian Q Weinberger. Sim- plifying graph convolutional networks. arXiv preprint arXiv:1902.07153, 2019. 2\n\nSemantic projection network for zero-and few-label semantic segmentation. Yongqin Xian, Subhabrata Choudhury, Yang He, Bernt Schiele, Zeynep Akata, CVPR. Yongqin Xian, Subhabrata Choudhury, Yang He, Bernt Schiele, and Zeynep Akata. Semantic projection network for zero-and few-label semantic segmentation. In CVPR, 2019. 5\n\nZero-shot learning-a comprehensive evaluation of the good, the bad and the ugly. Yongqin Xian, H Christoph, Bernt Lampert, Zeynep Schiele, Akata, IEEE TPAMI. 4195Yongqin Xian, Christoph H Lampert, Bernt Schiele, and Zeynep Akata. Zero-shot learning-a comprehensive eval- uation of the good, the bad and the ugly. IEEE TPAMI, 41(9):2251-2265, 2018. 3, 5\n\nFeature generating networks for zero-shot learning. Yongqin Xian, Tobias Lorenz, Bernt Schiele, Zeynep Akata, CVPR. 27Yongqin Xian, Tobias Lorenz, Bernt Schiele, and Zeynep Akata. Feature generating networks for zero-shot learning. In CVPR, 2018. 2, 7\n\nKeyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-Ichi Kawarabayashi, Stefanie Jegelka, arXiv:1806.03536Representation learning on graphs with jumping knowledge networks. arXiv preprintKeyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representa- tion learning on graphs with jumping knowledge networks. arXiv preprint arXiv:1806.03536, 2018. 2\n\nLearning unseen concepts via hierarchical decomposition and composition. Muli Yang, Cheng Deng, Junchi Yan, Xianglong Liu, Dacheng Tao, CVPR. 2020Muli Yang, Cheng Deng, Junchi Yan, Xianglong Liu, and Dacheng Tao. Learning unseen concepts via hierarchical de- composition and composition. In CVPR, 2020. 2\n\nFine-grained visual comparisons with local learning. Aron Yu, Kristen Grauman, CVPR. 512Aron Yu and Kristen Grauman. Fine-grained visual compar- isons with local learning. In CVPR, 2014. 5, 12\n\nSemantic jitter: Dense supervision for visual comparisons via synthetic images. Aron Yu, Kristen Grauman, ICCV. Aron Yu and Kristen Grauman. Semantic jitter: Dense super- vision for visual comparisons via synthetic images. In ICCV, 2017. 5\n\nVisualizing and understanding convolutional networks. D Matthew, Rob Zeiler, Fergus, ECCV. Matthew D Zeiler and Rob Fergus. Visualizing and under- standing convolutional networks. In ECCV, 2014. 2\n\nLearning a deep embedding model for zero-shot learning. Li Zhang, Tao Xiang, Shaogang Gong, CVPR. Li Zhang, Tao Xiang, and Shaogang Gong. Learning a deep embedding model for zero-shot learning. In CVPR, 2017. 2\n\nA generative adversarial approach for zero-shot learning from noisy texts. Yizhe Zhu, Mohamed Elhoseiny, Bingchen Liu, Xi Peng, Ahmed Elgammal, CVPR. Yizhe Zhu, Mohamed Elhoseiny, Bingchen Liu, Xi Peng, and Ahmed Elgammal. A generative adversarial approach for zero-shot learning from noisy texts. In CVPR, 2018. 2\n", "annotations": {"author": "[{\"end\":154,\"start\":66},{\"end\":228,\"start\":155},{\"end\":305,\"start\":229},{\"end\":374,\"start\":306}]", "publisher": null, "author_last_name": "[{\"end\":87,\"start\":82},{\"end\":167,\"start\":163},{\"end\":245,\"start\":238},{\"end\":318,\"start\":313}]", "author_first_name": "[{\"end\":74,\"start\":66},{\"end\":81,\"start\":75},{\"end\":162,\"start\":155},{\"end\":237,\"start\":229},{\"end\":312,\"start\":306}]", "author_affiliation": "[{\"end\":112,\"start\":89},{\"end\":153,\"start\":114},{\"end\":227,\"start\":190},{\"end\":304,\"start\":265},{\"end\":373,\"start\":350}]", "title": "[{\"end\":63,\"start\":1},{\"end\":437,\"start\":375}]", "venue": null, "abstract": "[{\"end\":1627,\"start\":439}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b43\"},\"end\":2096,\"start\":2092},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":2099,\"start\":2096},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":3003,\"start\":2999},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":3292,\"start\":3288},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":3358,\"start\":3354},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":3427,\"start\":3423},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3505,\"start\":3501},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4178,\"start\":4175},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":4432,\"start\":4428},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4437,\"start\":4434},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4840,\"start\":4836},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5344,\"start\":5340},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5362,\"start\":5359},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":5669,\"start\":5665},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5672,\"start\":5669},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5695,\"start\":5692},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5697,\"start\":5695},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5700,\"start\":5697},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":5703,\"start\":5700},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5730,\"start\":5726},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":5733,\"start\":5730},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":5736,\"start\":5733},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5867,\"start\":5863},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5869,\"start\":5867},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5872,\"start\":5869},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":5875,\"start\":5872},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6115,\"start\":6111},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6213,\"start\":6209},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":6237,\"start\":6233},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":6261,\"start\":6257},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6367,\"start\":6364},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":6370,\"start\":6367},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":6433,\"start\":6429},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":6436,\"start\":6433},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6477,\"start\":6473},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":6480,\"start\":6477},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6483,\"start\":6480},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":6551,\"start\":6547},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6684,\"start\":6680},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6710,\"start\":6706},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6841,\"start\":6837},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7004,\"start\":7000},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7172,\"start\":7168},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":7244,\"start\":7240},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7247,\"start\":7244},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":7277,\"start\":7273},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":7337,\"start\":7333},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7340,\"start\":7337},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7343,\"start\":7340},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7398,\"start\":7394},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7866,\"start\":7862},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7868,\"start\":7866},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7981,\"start\":7977},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8045,\"start\":8041},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":8126,\"start\":8122},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8200,\"start\":8196},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8362,\"start\":8359},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":8365,\"start\":8362},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":8368,\"start\":8365},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":8454,\"start\":8450},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":8457,\"start\":8454},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8548,\"start\":8545},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8883,\"start\":8880},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":8886,\"start\":8883},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":8889,\"start\":8886},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":8923,\"start\":8919},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8926,\"start\":8923},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9081,\"start\":9078},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":9084,\"start\":9081},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":9087,\"start\":9084},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9152,\"start\":9148},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":9211,\"start\":9207},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9214,\"start\":9211},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9365,\"start\":9362},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":9581,\"start\":9577},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9584,\"start\":9581},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9586,\"start\":9584},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":9589,\"start\":9586},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":9592,\"start\":9589},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":9595,\"start\":9592},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":10541,\"start\":10537},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":12084,\"start\":12080},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":12087,\"start\":12084},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":13521,\"start\":13517},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":13524,\"start\":13521},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":13541,\"start\":13537},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":13697,\"start\":13693},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":13736,\"start\":13732},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":14443,\"start\":14439},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":15794,\"start\":15790},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":15797,\"start\":15794},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":16020,\"start\":16016},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":16022,\"start\":16020},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":16557,\"start\":16553},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":16838,\"start\":16834},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":17386,\"start\":17382},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":18982,\"start\":18978},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":19454,\"start\":19450},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":19457,\"start\":19454},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":19460,\"start\":19457},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":19463,\"start\":19460},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":19730,\"start\":19726},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":19733,\"start\":19730},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":19940,\"start\":19936},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":19943,\"start\":19940},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":19993,\"start\":19989},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":20118,\"start\":20114},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":20705,\"start\":20701},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":20861,\"start\":20858},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":20877,\"start\":20873},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":20880,\"start\":20877},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":21123,\"start\":21119},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":21235,\"start\":21231},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":21797,\"start\":21794},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":21800,\"start\":21797},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":21992,\"start\":21988},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":22064,\"start\":22060},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":22916,\"start\":22912},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":23147,\"start\":23143},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":23323,\"start\":23319},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":23586,\"start\":23582},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":23601,\"start\":23598},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":23618,\"start\":23614},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":23899,\"start\":23896},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":23924,\"start\":23920},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":23940,\"start\":23936},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":24111,\"start\":24107},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":24223,\"start\":24219},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":24874,\"start\":24871},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":26354,\"start\":26350},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":27545,\"start\":27541},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":28996,\"start\":28992},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":28999,\"start\":28996},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":30434,\"start\":30430},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":30461,\"start\":30457},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":31536,\"start\":31532},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":31632,\"start\":31628},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":31635,\"start\":31632},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":32433,\"start\":32429},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":32561,\"start\":32557},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":40106,\"start\":40102},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":40130,\"start\":40126},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":42122,\"start\":42118},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":43590,\"start\":43586},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":45474,\"start\":45471},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":45493,\"start\":45489},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":45511,\"start\":45507}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":43011,\"start\":42428},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":43725,\"start\":43012},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":44099,\"start\":43726},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":44648,\"start\":44100},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":45144,\"start\":44649},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":45376,\"start\":45145},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":45765,\"start\":45377}]", "paragraph": "[{\"end\":2494,\"start\":1643},{\"end\":2878,\"start\":2496},{\"end\":4139,\"start\":2880},{\"end\":5071,\"start\":4141},{\"end\":6018,\"start\":5088},{\"end\":8749,\"start\":6020},{\"end\":9681,\"start\":8751},{\"end\":10320,\"start\":9705},{\"end\":10770,\"start\":10333},{\"end\":11518,\"start\":10772},{\"end\":12864,\"start\":11564},{\"end\":13456,\"start\":12904},{\"end\":14287,\"start\":13458},{\"end\":15256,\"start\":14327},{\"end\":16790,\"start\":15258},{\"end\":17424,\"start\":16792},{\"end\":18465,\"start\":17460},{\"end\":18742,\"start\":18485},{\"end\":19069,\"start\":18833},{\"end\":20420,\"start\":19159},{\"end\":21124,\"start\":20436},{\"end\":22022,\"start\":21126},{\"end\":23995,\"start\":22024},{\"end\":24083,\"start\":24010},{\"end\":24426,\"start\":24085},{\"end\":25518,\"start\":24466},{\"end\":26298,\"start\":25520},{\"end\":27042,\"start\":26300},{\"end\":28373,\"start\":27044},{\"end\":28557,\"start\":28392},{\"end\":29288,\"start\":28559},{\"end\":30236,\"start\":29307},{\"end\":30628,\"start\":30238},{\"end\":31418,\"start\":30630},{\"end\":31986,\"start\":31420},{\"end\":32737,\"start\":31988},{\"end\":33253,\"start\":33224},{\"end\":33391,\"start\":33353},{\"end\":33434,\"start\":33405},{\"end\":33494,\"start\":33454},{\"end\":33968,\"start\":33507},{\"end\":34562,\"start\":34007},{\"end\":35518,\"start\":34564},{\"end\":36412,\"start\":35520},{\"end\":37307,\"start\":36427},{\"end\":37589,\"start\":37322},{\"end\":38719,\"start\":37591},{\"end\":39185,\"start\":38721},{\"end\":39295,\"start\":39239},{\"end\":39910,\"start\":39339},{\"end\":40248,\"start\":39912},{\"end\":41367,\"start\":40279},{\"end\":42427,\"start\":41369}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12903,\"start\":12865},{\"attributes\":{\"id\":\"formula_1\"},\"end\":17459,\"start\":17425},{\"attributes\":{\"id\":\"formula_2\"},\"end\":18484,\"start\":18466},{\"attributes\":{\"id\":\"formula_3\"},\"end\":18832,\"start\":18743},{\"attributes\":{\"id\":\"formula_4\"},\"end\":19158,\"start\":19070},{\"attributes\":{\"id\":\"formula_5\"},\"end\":39238,\"start\":39214}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":20687,\"start\":20680},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":27889,\"start\":27882},{\"end\":28043,\"start\":28036},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":37455,\"start\":37335},{\"end\":37470,\"start\":37463},{\"end\":40741,\"start\":40734},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":41114,\"start\":41107}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1641,\"start\":1629},{\"attributes\":{\"n\":\"2.\"},\"end\":5086,\"start\":5074},{\"end\":9689,\"start\":9684},{\"end\":9703,\"start\":9692},{\"attributes\":{\"n\":\"3.\"},\"end\":10331,\"start\":10323},{\"attributes\":{\"n\":\"3.1.\"},\"end\":11562,\"start\":11521},{\"attributes\":{\"n\":\"3.2.\"},\"end\":14325,\"start\":14290},{\"attributes\":{\"n\":\"4.\"},\"end\":20434,\"start\":20423},{\"end\":24008,\"start\":23998},{\"attributes\":{\"n\":\"4.1.\"},\"end\":24464,\"start\":24429},{\"end\":28390,\"start\":28376},{\"attributes\":{\"n\":\"4.2.\"},\"end\":29305,\"start\":29291},{\"end\":32754,\"start\":32740},{\"end\":32800,\"start\":32757},{\"end\":32818,\"start\":32803},{\"end\":32863,\"start\":32821},{\"end\":32873,\"start\":32866},{\"end\":32905,\"start\":32876},{\"end\":32919,\"start\":32908},{\"end\":32957,\"start\":32922},{\"end\":32970,\"start\":32960},{\"end\":33011,\"start\":32973},{\"end\":33027,\"start\":33014},{\"end\":33070,\"start\":33030},{\"end\":33090,\"start\":33073},{\"end\":33146,\"start\":33093},{\"end\":33163,\"start\":33149},{\"end\":33206,\"start\":33166},{\"end\":33222,\"start\":33209},{\"end\":33271,\"start\":33256},{\"end\":33290,\"start\":33274},{\"end\":33334,\"start\":33293},{\"end\":33351,\"start\":33337},{\"end\":33403,\"start\":33394},{\"end\":33452,\"start\":33437},{\"end\":33505,\"start\":33497},{\"end\":33983,\"start\":33971},{\"attributes\":{\"n\":\"4.3.\"},\"end\":34005,\"start\":33986},{\"attributes\":{\"n\":\"5.\"},\"end\":36425,\"start\":36415},{\"end\":37320,\"start\":37310},{\"end\":39213,\"start\":39188},{\"end\":39337,\"start\":39298},{\"end\":40277,\"start\":40251},{\"end\":42439,\"start\":42429},{\"end\":43022,\"start\":43013},{\"end\":43736,\"start\":43727},{\"end\":44657,\"start\":44650},{\"end\":45155,\"start\":45146},{\"end\":45387,\"start\":45378}]", "table": "[{\"end\":43725,\"start\":43684},{\"end\":44099,\"start\":43844},{\"end\":44648,\"start\":44383},{\"end\":45144,\"start\":45047},{\"end\":45765,\"start\":45512}]", "figure_caption": "[{\"end\":43011,\"start\":42441},{\"end\":43684,\"start\":43024},{\"end\":43844,\"start\":43738},{\"end\":44383,\"start\":44102},{\"end\":45047,\"start\":44659},{\"end\":45376,\"start\":45157},{\"end\":45512,\"start\":45389}]", "figure_ref": "[{\"end\":2330,\"start\":2322},{\"end\":9721,\"start\":9713},{\"end\":11041,\"start\":11033},{\"end\":18188,\"start\":18180},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":21568,\"start\":21560},{\"end\":30359,\"start\":30351},{\"end\":32184,\"start\":32176},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":34107,\"start\":34099},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":34973,\"start\":34965},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":35531,\"start\":35523}]", "bib_author_first_name": "[{\"end\":46895,\"start\":46889},{\"end\":46910,\"start\":46903},{\"end\":46926,\"start\":46922},{\"end\":46946,\"start\":46938},{\"end\":47162,\"start\":47157},{\"end\":47176,\"start\":47171},{\"end\":47187,\"start\":47184},{\"end\":47199,\"start\":47196},{\"end\":47363,\"start\":47358},{\"end\":47367,\"start\":47364},{\"end\":47377,\"start\":47372},{\"end\":47382,\"start\":47378},{\"end\":47398,\"start\":47390},{\"end\":47400,\"start\":47399},{\"end\":47654,\"start\":47648},{\"end\":47876,\"start\":47871},{\"end\":47896,\"start\":47889},{\"end\":47910,\"start\":47904},{\"end\":47924,\"start\":47919},{\"end\":48232,\"start\":48228},{\"end\":48445,\"start\":48438},{\"end\":48459,\"start\":48452},{\"end\":48478,\"start\":48472},{\"end\":48488,\"start\":48485},{\"end\":48751,\"start\":48743},{\"end\":48766,\"start\":48758},{\"end\":48781,\"start\":48778},{\"end\":48798,\"start\":48791},{\"end\":49023,\"start\":49021},{\"end\":49033,\"start\":49029},{\"end\":49046,\"start\":49041},{\"end\":49241,\"start\":49238},{\"end\":49251,\"start\":49248},{\"end\":49266,\"start\":49258},{\"end\":49278,\"start\":49272},{\"end\":49291,\"start\":49286},{\"end\":49304,\"start\":49300},{\"end\":49317,\"start\":49313},{\"end\":49329,\"start\":49322},{\"end\":49344,\"start\":49337},{\"end\":49667,\"start\":49661},{\"end\":49685,\"start\":49677},{\"end\":49702,\"start\":49695},{\"end\":49722,\"start\":49714},{\"end\":49738,\"start\":49731},{\"end\":50081,\"start\":50074},{\"end\":50097,\"start\":50093},{\"end\":50291,\"start\":50284},{\"end\":50303,\"start\":50296},{\"end\":50319,\"start\":50311},{\"end\":50329,\"start\":50325},{\"end\":50483,\"start\":50482},{\"end\":50501,\"start\":50492},{\"end\":50727,\"start\":50726},{\"end\":50747,\"start\":50734},{\"end\":51000,\"start\":50993},{\"end\":51009,\"start\":51008},{\"end\":51024,\"start\":51018},{\"end\":51026,\"start\":51025},{\"end\":51263,\"start\":51260},{\"end\":51279,\"start\":51278},{\"end\":51293,\"start\":51288},{\"end\":51306,\"start\":51305},{\"end\":51322,\"start\":51321},{\"end\":51337,\"start\":51332},{\"end\":51590,\"start\":51584},{\"end\":51606,\"start\":51600},{\"end\":51623,\"start\":51616},{\"end\":51637,\"start\":51631},{\"end\":51647,\"start\":51642},{\"end\":51663,\"start\":51656},{\"end\":51677,\"start\":51675},{\"end\":51924,\"start\":51917},{\"end\":51943,\"start\":51938},{\"end\":51957,\"start\":51950},{\"end\":51968,\"start\":51965},{\"end\":51980,\"start\":51975},{\"end\":51992,\"start\":51988},{\"end\":51994,\"start\":51993},{\"end\":52228,\"start\":52227},{\"end\":52244,\"start\":52239},{\"end\":52470,\"start\":52469},{\"end\":52482,\"start\":52479},{\"end\":52710,\"start\":52702},{\"end\":52731,\"start\":52721},{\"end\":52751,\"start\":52744},{\"end\":53017,\"start\":53009},{\"end\":53034,\"start\":53028},{\"end\":53056,\"start\":53049},{\"end\":53280,\"start\":53271},{\"end\":53553,\"start\":53549},{\"end\":53569,\"start\":53561},{\"end\":53578,\"start\":53577},{\"end\":53591,\"start\":53585},{\"end\":53607,\"start\":53600},{\"end\":53609,\"start\":53608},{\"end\":53626,\"start\":53621},{\"end\":53643,\"start\":53635},{\"end\":53645,\"start\":53644},{\"end\":53953,\"start\":53947},{\"end\":53966,\"start\":53958},{\"end\":53978,\"start\":53975},{\"end\":53994,\"start\":53987},{\"end\":54207,\"start\":54206},{\"end\":54213,\"start\":54212},{\"end\":54223,\"start\":54219},{\"end\":54426,\"start\":54419},{\"end\":54434,\"start\":54431},{\"end\":54446,\"start\":54439},{\"end\":54456,\"start\":54452},{\"end\":54658,\"start\":54654},{\"end\":54669,\"start\":54663},{\"end\":54686,\"start\":54679},{\"end\":54700,\"start\":54698},{\"end\":54954,\"start\":54948},{\"end\":54969,\"start\":54964},{\"end\":54986,\"start\":54979},{\"end\":55006,\"start\":54998},{\"end\":55295,\"start\":55290},{\"end\":55309,\"start\":55305},{\"end\":55324,\"start\":55321},{\"end\":55335,\"start\":55331},{\"end\":55337,\"start\":55336},{\"end\":55351,\"start\":55347},{\"end\":55599,\"start\":55598},{\"end\":55615,\"start\":55608},{\"end\":55634,\"start\":55624},{\"end\":55650,\"start\":55645},{\"end\":55670,\"start\":55661},{\"end\":55672,\"start\":55671},{\"end\":56004,\"start\":55998},{\"end\":56014,\"start\":56010},{\"end\":56040,\"start\":56021},{\"end\":56055,\"start\":56048},{\"end\":56268,\"start\":56263},{\"end\":56283,\"start\":56276},{\"end\":56298,\"start\":56291},{\"end\":56534,\"start\":56528},{\"end\":56553,\"start\":56546},{\"end\":56785,\"start\":56780},{\"end\":56800,\"start\":56792},{\"end\":56802,\"start\":56801},{\"end\":57005,\"start\":57001},{\"end\":57017,\"start\":57014},{\"end\":57034,\"start\":57025},{\"end\":57046,\"start\":57042},{\"end\":57059,\"start\":57054},{\"end\":57077,\"start\":57070},{\"end\":57092,\"start\":57086},{\"end\":57108,\"start\":57102},{\"end\":57121,\"start\":57114},{\"end\":57138,\"start\":57134},{\"end\":57492,\"start\":57488},{\"end\":57510,\"start\":57503},{\"end\":57721,\"start\":57714},{\"end\":57741,\"start\":57734},{\"end\":57763,\"start\":57750},{\"end\":58262,\"start\":58255},{\"end\":58287,\"start\":58277},{\"end\":58303,\"start\":58296},{\"end\":58328,\"start\":58311},{\"end\":58583,\"start\":58577},{\"end\":58594,\"start\":58590},{\"end\":58783,\"start\":58778},{\"end\":58796,\"start\":58790},{\"end\":58811,\"start\":58804},{\"end\":58822,\"start\":58817},{\"end\":59061,\"start\":59059},{\"end\":59075,\"start\":59068},{\"end\":59091,\"start\":59083},{\"end\":59103,\"start\":59096},{\"end\":59344,\"start\":59338},{\"end\":59367,\"start\":59360},{\"end\":59382,\"start\":59378},{\"end\":59605,\"start\":59598},{\"end\":59620,\"start\":59614},{\"end\":59630,\"start\":59629},{\"end\":59650,\"start\":59644},{\"end\":59921,\"start\":59915},{\"end\":59942,\"start\":59934},{\"end\":59955,\"start\":59951},{\"end\":59972,\"start\":59968},{\"end\":59990,\"start\":59984},{\"end\":60314,\"start\":60306},{\"end\":60326,\"start\":60321},{\"end\":60338,\"start\":60331},{\"end\":60560,\"start\":60557},{\"end\":60573,\"start\":60567},{\"end\":60584,\"start\":60578},{\"end\":60600,\"start\":60594},{\"end\":60602,\"start\":60601},{\"end\":60858,\"start\":60850},{\"end\":60869,\"start\":60865},{\"end\":60886,\"start\":60879},{\"end\":61003,\"start\":60998},{\"end\":61014,\"start\":61008},{\"end\":61028,\"start\":61022},{\"end\":61060,\"start\":61049},{\"end\":61071,\"start\":61068},{\"end\":61084,\"start\":61076},{\"end\":61440,\"start\":61433},{\"end\":61457,\"start\":61447},{\"end\":61473,\"start\":61469},{\"end\":61483,\"start\":61478},{\"end\":61499,\"start\":61493},{\"end\":61771,\"start\":61764},{\"end\":61779,\"start\":61778},{\"end\":61796,\"start\":61791},{\"end\":61812,\"start\":61806},{\"end\":62096,\"start\":62089},{\"end\":62109,\"start\":62103},{\"end\":62123,\"start\":62118},{\"end\":62139,\"start\":62133},{\"end\":62296,\"start\":62290},{\"end\":62309,\"start\":62301},{\"end\":62322,\"start\":62314},{\"end\":62337,\"start\":62329},{\"end\":62354,\"start\":62346},{\"end\":62378,\"start\":62370},{\"end\":62775,\"start\":62771},{\"end\":62787,\"start\":62782},{\"end\":62800,\"start\":62794},{\"end\":62815,\"start\":62806},{\"end\":62828,\"start\":62821},{\"end\":63061,\"start\":63057},{\"end\":63073,\"start\":63066},{\"end\":63282,\"start\":63278},{\"end\":63294,\"start\":63287},{\"end\":63494,\"start\":63493},{\"end\":63507,\"start\":63504},{\"end\":63695,\"start\":63693},{\"end\":63706,\"start\":63703},{\"end\":63722,\"start\":63714},{\"end\":63929,\"start\":63924},{\"end\":63942,\"start\":63935},{\"end\":63962,\"start\":63954},{\"end\":63970,\"start\":63968},{\"end\":63982,\"start\":63977}]", "bib_author_last_name": "[{\"end\":46901,\"start\":46896},{\"end\":46920,\"start\":46911},{\"end\":46936,\"start\":46927},{\"end\":46953,\"start\":46947},{\"end\":47169,\"start\":47163},{\"end\":47182,\"start\":47177},{\"end\":47194,\"start\":47188},{\"end\":47207,\"start\":47200},{\"end\":47370,\"start\":47368},{\"end\":47388,\"start\":47383},{\"end\":47404,\"start\":47401},{\"end\":47664,\"start\":47655},{\"end\":47887,\"start\":47877},{\"end\":47902,\"start\":47897},{\"end\":47917,\"start\":47911},{\"end\":47932,\"start\":47925},{\"end\":48240,\"start\":48233},{\"end\":48450,\"start\":48446},{\"end\":48470,\"start\":48460},{\"end\":48483,\"start\":48479},{\"end\":48492,\"start\":48489},{\"end\":48756,\"start\":48752},{\"end\":48776,\"start\":48767},{\"end\":48789,\"start\":48782},{\"end\":48804,\"start\":48799},{\"end\":49027,\"start\":49024},{\"end\":49039,\"start\":49034},{\"end\":49050,\"start\":49047},{\"end\":49246,\"start\":49242},{\"end\":49256,\"start\":49252},{\"end\":49270,\"start\":49267},{\"end\":49284,\"start\":49279},{\"end\":49298,\"start\":49292},{\"end\":49311,\"start\":49305},{\"end\":49320,\"start\":49318},{\"end\":49335,\"start\":49330},{\"end\":49349,\"start\":49345},{\"end\":49675,\"start\":49668},{\"end\":49693,\"start\":49686},{\"end\":49712,\"start\":49703},{\"end\":49729,\"start\":49723},{\"end\":49755,\"start\":49739},{\"end\":49764,\"start\":49757},{\"end\":50091,\"start\":50082},{\"end\":50106,\"start\":50098},{\"end\":50294,\"start\":50292},{\"end\":50309,\"start\":50304},{\"end\":50323,\"start\":50320},{\"end\":50333,\"start\":50330},{\"end\":50490,\"start\":50484},{\"end\":50509,\"start\":50502},{\"end\":50519,\"start\":50511},{\"end\":50732,\"start\":50728},{\"end\":50754,\"start\":50748},{\"end\":50763,\"start\":50756},{\"end\":51006,\"start\":51001},{\"end\":51016,\"start\":51010},{\"end\":51030,\"start\":51027},{\"end\":51039,\"start\":51032},{\"end\":51269,\"start\":51264},{\"end\":51276,\"start\":51271},{\"end\":51286,\"start\":51280},{\"end\":51298,\"start\":51294},{\"end\":51303,\"start\":51300},{\"end\":51314,\"start\":51307},{\"end\":51319,\"start\":51316},{\"end\":51330,\"start\":51323},{\"end\":51345,\"start\":51338},{\"end\":51352,\"start\":51347},{\"end\":51598,\"start\":51591},{\"end\":51614,\"start\":51607},{\"end\":51629,\"start\":51624},{\"end\":51640,\"start\":51638},{\"end\":51654,\"start\":51648},{\"end\":51673,\"start\":51664},{\"end\":51685,\"start\":51678},{\"end\":51936,\"start\":51925},{\"end\":51948,\"start\":51944},{\"end\":51963,\"start\":51958},{\"end\":51973,\"start\":51969},{\"end\":51986,\"start\":51981},{\"end\":51999,\"start\":51995},{\"end\":52237,\"start\":52229},{\"end\":52251,\"start\":52245},{\"end\":52255,\"start\":52253},{\"end\":52477,\"start\":52471},{\"end\":52487,\"start\":52483},{\"end\":52496,\"start\":52489},{\"end\":52719,\"start\":52711},{\"end\":52742,\"start\":52732},{\"end\":52761,\"start\":52752},{\"end\":53026,\"start\":53018},{\"end\":53047,\"start\":53035},{\"end\":53066,\"start\":53057},{\"end\":53288,\"start\":53281},{\"end\":53559,\"start\":53554},{\"end\":53575,\"start\":53570},{\"end\":53583,\"start\":53579},{\"end\":53598,\"start\":53592},{\"end\":53619,\"start\":53610},{\"end\":53633,\"start\":53627},{\"end\":53653,\"start\":53646},{\"end\":53661,\"start\":53655},{\"end\":53956,\"start\":53954},{\"end\":53973,\"start\":53967},{\"end\":53985,\"start\":53979},{\"end\":54001,\"start\":53995},{\"end\":54210,\"start\":54208},{\"end\":54217,\"start\":54214},{\"end\":54226,\"start\":54224},{\"end\":54429,\"start\":54427},{\"end\":54437,\"start\":54435},{\"end\":54450,\"start\":54447},{\"end\":54459,\"start\":54457},{\"end\":54661,\"start\":54659},{\"end\":54677,\"start\":54670},{\"end\":54696,\"start\":54687},{\"end\":54708,\"start\":54701},{\"end\":54962,\"start\":54955},{\"end\":54977,\"start\":54970},{\"end\":54996,\"start\":54987},{\"end\":55013,\"start\":55007},{\"end\":55303,\"start\":55296},{\"end\":55319,\"start\":55310},{\"end\":55329,\"start\":55325},{\"end\":55345,\"start\":55338},{\"end\":55356,\"start\":55352},{\"end\":55606,\"start\":55600},{\"end\":55622,\"start\":55616},{\"end\":55643,\"start\":55635},{\"end\":55659,\"start\":55651},{\"end\":55678,\"start\":55673},{\"end\":55686,\"start\":55680},{\"end\":56008,\"start\":56005},{\"end\":56019,\"start\":56015},{\"end\":56046,\"start\":56041},{\"end\":56058,\"start\":56056},{\"end\":56274,\"start\":56269},{\"end\":56289,\"start\":56284},{\"end\":56305,\"start\":56299},{\"end\":56544,\"start\":56535},{\"end\":56561,\"start\":56554},{\"end\":56790,\"start\":56786},{\"end\":56809,\"start\":56803},{\"end\":57012,\"start\":57006},{\"end\":57023,\"start\":57018},{\"end\":57040,\"start\":57035},{\"end\":57052,\"start\":57047},{\"end\":57068,\"start\":57060},{\"end\":57084,\"start\":57078},{\"end\":57100,\"start\":57093},{\"end\":57112,\"start\":57109},{\"end\":57132,\"start\":57122},{\"end\":57145,\"start\":57139},{\"end\":57501,\"start\":57493},{\"end\":57517,\"start\":57511},{\"end\":57732,\"start\":57722},{\"end\":57748,\"start\":57742},{\"end\":57771,\"start\":57764},{\"end\":58275,\"start\":58263},{\"end\":58294,\"start\":58288},{\"end\":58309,\"start\":58304},{\"end\":58336,\"start\":58329},{\"end\":58588,\"start\":58584},{\"end\":58605,\"start\":58595},{\"end\":58788,\"start\":58784},{\"end\":58802,\"start\":58797},{\"end\":58815,\"start\":58812},{\"end\":58830,\"start\":58823},{\"end\":59066,\"start\":59062},{\"end\":59081,\"start\":59076},{\"end\":59094,\"start\":59092},{\"end\":59109,\"start\":59104},{\"end\":59358,\"start\":59345},{\"end\":59376,\"start\":59368},{\"end\":59392,\"start\":59383},{\"end\":59612,\"start\":59606},{\"end\":59627,\"start\":59621},{\"end\":59642,\"start\":59631},{\"end\":59658,\"start\":59651},{\"end\":59662,\"start\":59660},{\"end\":59932,\"start\":59922},{\"end\":59949,\"start\":59943},{\"end\":59966,\"start\":59956},{\"end\":59982,\"start\":59973},{\"end\":60004,\"start\":59991},{\"end\":60319,\"start\":60315},{\"end\":60329,\"start\":60327},{\"end\":60344,\"start\":60339},{\"end\":60565,\"start\":60561},{\"end\":60576,\"start\":60574},{\"end\":60592,\"start\":60585},{\"end\":60611,\"start\":60603},{\"end\":60863,\"start\":60859},{\"end\":60877,\"start\":60870},{\"end\":60893,\"start\":60887},{\"end\":61006,\"start\":61004},{\"end\":61020,\"start\":61015},{\"end\":61045,\"start\":61029},{\"end\":61066,\"start\":61061},{\"end\":61074,\"start\":61072},{\"end\":61095,\"start\":61085},{\"end\":61445,\"start\":61441},{\"end\":61467,\"start\":61458},{\"end\":61476,\"start\":61474},{\"end\":61491,\"start\":61484},{\"end\":61505,\"start\":61500},{\"end\":61776,\"start\":61772},{\"end\":61789,\"start\":61780},{\"end\":61804,\"start\":61797},{\"end\":61820,\"start\":61813},{\"end\":61827,\"start\":61822},{\"end\":62101,\"start\":62097},{\"end\":62116,\"start\":62110},{\"end\":62131,\"start\":62124},{\"end\":62145,\"start\":62140},{\"end\":62299,\"start\":62297},{\"end\":62312,\"start\":62310},{\"end\":62327,\"start\":62323},{\"end\":62344,\"start\":62338},{\"end\":62368,\"start\":62355},{\"end\":62386,\"start\":62379},{\"end\":62780,\"start\":62776},{\"end\":62792,\"start\":62788},{\"end\":62804,\"start\":62801},{\"end\":62819,\"start\":62816},{\"end\":62832,\"start\":62829},{\"end\":63064,\"start\":63062},{\"end\":63081,\"start\":63074},{\"end\":63285,\"start\":63283},{\"end\":63302,\"start\":63295},{\"end\":63502,\"start\":63495},{\"end\":63514,\"start\":63508},{\"end\":63522,\"start\":63516},{\"end\":63701,\"start\":63696},{\"end\":63712,\"start\":63707},{\"end\":63727,\"start\":63723},{\"end\":63933,\"start\":63930},{\"end\":63952,\"start\":63943},{\"end\":63966,\"start\":63963},{\"end\":63975,\"start\":63971},{\"end\":63991,\"start\":63983}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":8288863},\"end\":47101,\"start\":46837},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":220055717},\"end\":47354,\"start\":47103},{\"attributes\":{\"doi\":\"arXiv:1607.06450\",\"id\":\"b2\"},\"end\":47580,\"start\":47356},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":8054340},\"end\":47820,\"start\":47582},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":207556454},\"end\":48206,\"start\":47822},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":45998148},\"end\":48335,\"start\":48208},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":519822},\"end\":48679,\"start\":48337},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":1670087},\"end\":48957,\"start\":48681},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":2634827},\"end\":49173,\"start\":48959},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":10559817},\"end\":49553,\"start\":49175},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":54101493},\"end\":50003,\"start\":49555},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":9099040},\"end\":50236,\"start\":50005},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":206594692},\"end\":50458,\"start\":50238},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":6397710},\"end\":50635,\"start\":50460},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":152282269},\"end\":50930,\"start\":50637},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":15870772},\"end\":51184,\"start\":50932},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":52970776},\"end\":51546,\"start\":51186},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":16414666},\"end\":51852,\"start\":51548},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":44075854},\"end\":52181,\"start\":51854},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b19\"},\"end\":52401,\"start\":52183},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":3144218},\"end\":52626,\"start\":52403},{\"attributes\":{\"doi\":\"arXiv:1810.05997\",\"id\":\"b21\"},\"end\":52972,\"start\":52628},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":202783932},\"end\":53194,\"start\":52974},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":7016601},\"end\":53486,\"start\":53196},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":41312633},\"end\":53906,\"start\":53488},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":201070021},\"end\":54124,\"start\":53908},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":11118105},\"end\":54364,\"start\":54126},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":214743077},\"end\":54600,\"start\":54366},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":8701238},\"end\":54845,\"start\":54602},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":9296691},\"end\":55211,\"start\":54847},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":16447573},\"end\":55542,\"start\":55213},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":2146137},\"end\":55950,\"start\":55544},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":220363476},\"end\":56206,\"start\":55952},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":19886856},\"end\":56451,\"start\":56208},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":52000169},\"end\":56716,\"start\":56453},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":15539264},\"end\":56929,\"start\":56718},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":202786778},\"end\":57398,\"start\":56931},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":15569196},\"end\":57665,\"start\":57400},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":1957433},\"end\":58186,\"start\":57667},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":155089387},\"end\":58528,\"start\":58188},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":67413369},\"end\":58709,\"start\":58530},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":7102424},\"end\":58981,\"start\":58711},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":212859361},\"end\":59267,\"start\":58983},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":9920696},\"end\":59547,\"start\":59269},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":2808203},\"end\":59804,\"start\":59549},{\"attributes\":{\"id\":\"b45\"},\"end\":60236,\"start\":59806},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":4703853},\"end\":60487,\"start\":60238},{\"attributes\":{\"doi\":\"arXiv:1906.04854\",\"id\":\"b47\"},\"end\":60820,\"start\":60489},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":26537848},\"end\":60996,\"start\":60822},{\"attributes\":{\"doi\":\"arXiv:1902.07153\",\"id\":\"b49\"},\"end\":61357,\"start\":60998},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":108379906},\"end\":61681,\"start\":61359},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":4852047},\"end\":62035,\"start\":61683},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":179895},\"end\":62288,\"start\":62037},{\"attributes\":{\"doi\":\"arXiv:1806.03536\",\"id\":\"b53\"},\"end\":62696,\"start\":62290},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":219630137},\"end\":63002,\"start\":62698},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":7698906},\"end\":63196,\"start\":63004},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":4668606},\"end\":63437,\"start\":63198},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":3960646},\"end\":63635,\"start\":63439},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":206595438},\"end\":63847,\"start\":63637},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":51987847},\"end\":64163,\"start\":63849}]", "bib_title": "[{\"end\":46887,\"start\":46837},{\"end\":47155,\"start\":47103},{\"end\":47646,\"start\":47582},{\"end\":47869,\"start\":47822},{\"end\":48226,\"start\":48208},{\"end\":48436,\"start\":48337},{\"end\":48741,\"start\":48681},{\"end\":49019,\"start\":48959},{\"end\":49236,\"start\":49175},{\"end\":49659,\"start\":49555},{\"end\":50072,\"start\":50005},{\"end\":50282,\"start\":50238},{\"end\":50480,\"start\":50460},{\"end\":50724,\"start\":50637},{\"end\":50991,\"start\":50932},{\"end\":51258,\"start\":51186},{\"end\":51582,\"start\":51548},{\"end\":51915,\"start\":51854},{\"end\":52467,\"start\":52403},{\"end\":53007,\"start\":52974},{\"end\":53269,\"start\":53196},{\"end\":53547,\"start\":53488},{\"end\":53945,\"start\":53908},{\"end\":54204,\"start\":54126},{\"end\":54417,\"start\":54366},{\"end\":54652,\"start\":54602},{\"end\":54946,\"start\":54847},{\"end\":55288,\"start\":55213},{\"end\":55596,\"start\":55544},{\"end\":55996,\"start\":55952},{\"end\":56261,\"start\":56208},{\"end\":56526,\"start\":56453},{\"end\":56778,\"start\":56718},{\"end\":56999,\"start\":56931},{\"end\":57486,\"start\":57400},{\"end\":57712,\"start\":57667},{\"end\":58253,\"start\":58188},{\"end\":58575,\"start\":58530},{\"end\":58776,\"start\":58711},{\"end\":59057,\"start\":58983},{\"end\":59336,\"start\":59269},{\"end\":59596,\"start\":59549},{\"end\":60304,\"start\":60238},{\"end\":60848,\"start\":60822},{\"end\":61431,\"start\":61359},{\"end\":61762,\"start\":61683},{\"end\":62087,\"start\":62037},{\"end\":62769,\"start\":62698},{\"end\":63055,\"start\":63004},{\"end\":63276,\"start\":63198},{\"end\":63491,\"start\":63439},{\"end\":63691,\"start\":63637},{\"end\":63922,\"start\":63849}]", "bib_author": "[{\"end\":46903,\"start\":46889},{\"end\":46922,\"start\":46903},{\"end\":46938,\"start\":46922},{\"end\":46955,\"start\":46938},{\"end\":47171,\"start\":47157},{\"end\":47184,\"start\":47171},{\"end\":47196,\"start\":47184},{\"end\":47209,\"start\":47196},{\"end\":47372,\"start\":47358},{\"end\":47390,\"start\":47372},{\"end\":47406,\"start\":47390},{\"end\":47666,\"start\":47648},{\"end\":47889,\"start\":47871},{\"end\":47904,\"start\":47889},{\"end\":47919,\"start\":47904},{\"end\":47934,\"start\":47919},{\"end\":48242,\"start\":48228},{\"end\":48452,\"start\":48438},{\"end\":48472,\"start\":48452},{\"end\":48485,\"start\":48472},{\"end\":48494,\"start\":48485},{\"end\":48758,\"start\":48743},{\"end\":48778,\"start\":48758},{\"end\":48791,\"start\":48778},{\"end\":48806,\"start\":48791},{\"end\":49029,\"start\":49021},{\"end\":49041,\"start\":49029},{\"end\":49052,\"start\":49041},{\"end\":49248,\"start\":49238},{\"end\":49258,\"start\":49248},{\"end\":49272,\"start\":49258},{\"end\":49286,\"start\":49272},{\"end\":49300,\"start\":49286},{\"end\":49313,\"start\":49300},{\"end\":49322,\"start\":49313},{\"end\":49337,\"start\":49322},{\"end\":49351,\"start\":49337},{\"end\":49677,\"start\":49661},{\"end\":49695,\"start\":49677},{\"end\":49714,\"start\":49695},{\"end\":49731,\"start\":49714},{\"end\":49757,\"start\":49731},{\"end\":49766,\"start\":49757},{\"end\":50093,\"start\":50074},{\"end\":50108,\"start\":50093},{\"end\":50296,\"start\":50284},{\"end\":50311,\"start\":50296},{\"end\":50325,\"start\":50311},{\"end\":50335,\"start\":50325},{\"end\":50492,\"start\":50482},{\"end\":50511,\"start\":50492},{\"end\":50521,\"start\":50511},{\"end\":50734,\"start\":50726},{\"end\":50756,\"start\":50734},{\"end\":50765,\"start\":50756},{\"end\":51008,\"start\":50993},{\"end\":51018,\"start\":51008},{\"end\":51032,\"start\":51018},{\"end\":51041,\"start\":51032},{\"end\":51271,\"start\":51260},{\"end\":51278,\"start\":51271},{\"end\":51288,\"start\":51278},{\"end\":51300,\"start\":51288},{\"end\":51305,\"start\":51300},{\"end\":51316,\"start\":51305},{\"end\":51321,\"start\":51316},{\"end\":51332,\"start\":51321},{\"end\":51347,\"start\":51332},{\"end\":51354,\"start\":51347},{\"end\":51600,\"start\":51584},{\"end\":51616,\"start\":51600},{\"end\":51631,\"start\":51616},{\"end\":51642,\"start\":51631},{\"end\":51656,\"start\":51642},{\"end\":51675,\"start\":51656},{\"end\":51687,\"start\":51675},{\"end\":51938,\"start\":51917},{\"end\":51950,\"start\":51938},{\"end\":51965,\"start\":51950},{\"end\":51975,\"start\":51965},{\"end\":51988,\"start\":51975},{\"end\":52001,\"start\":51988},{\"end\":52239,\"start\":52227},{\"end\":52253,\"start\":52239},{\"end\":52257,\"start\":52253},{\"end\":52479,\"start\":52469},{\"end\":52489,\"start\":52479},{\"end\":52498,\"start\":52489},{\"end\":52721,\"start\":52702},{\"end\":52744,\"start\":52721},{\"end\":52763,\"start\":52744},{\"end\":53028,\"start\":53009},{\"end\":53049,\"start\":53028},{\"end\":53068,\"start\":53049},{\"end\":53290,\"start\":53271},{\"end\":53561,\"start\":53549},{\"end\":53577,\"start\":53561},{\"end\":53585,\"start\":53577},{\"end\":53600,\"start\":53585},{\"end\":53621,\"start\":53600},{\"end\":53635,\"start\":53621},{\"end\":53655,\"start\":53635},{\"end\":53663,\"start\":53655},{\"end\":53958,\"start\":53947},{\"end\":53975,\"start\":53958},{\"end\":53987,\"start\":53975},{\"end\":54003,\"start\":53987},{\"end\":54212,\"start\":54206},{\"end\":54219,\"start\":54212},{\"end\":54228,\"start\":54219},{\"end\":54431,\"start\":54419},{\"end\":54439,\"start\":54431},{\"end\":54452,\"start\":54439},{\"end\":54461,\"start\":54452},{\"end\":54663,\"start\":54654},{\"end\":54679,\"start\":54663},{\"end\":54698,\"start\":54679},{\"end\":54710,\"start\":54698},{\"end\":54964,\"start\":54948},{\"end\":54979,\"start\":54964},{\"end\":54998,\"start\":54979},{\"end\":55015,\"start\":54998},{\"end\":55305,\"start\":55290},{\"end\":55321,\"start\":55305},{\"end\":55331,\"start\":55321},{\"end\":55347,\"start\":55331},{\"end\":55358,\"start\":55347},{\"end\":55608,\"start\":55598},{\"end\":55624,\"start\":55608},{\"end\":55645,\"start\":55624},{\"end\":55661,\"start\":55645},{\"end\":55680,\"start\":55661},{\"end\":55688,\"start\":55680},{\"end\":56010,\"start\":55998},{\"end\":56021,\"start\":56010},{\"end\":56048,\"start\":56021},{\"end\":56060,\"start\":56048},{\"end\":56276,\"start\":56263},{\"end\":56291,\"start\":56276},{\"end\":56307,\"start\":56291},{\"end\":56546,\"start\":56528},{\"end\":56563,\"start\":56546},{\"end\":56792,\"start\":56780},{\"end\":56811,\"start\":56792},{\"end\":57014,\"start\":57001},{\"end\":57025,\"start\":57014},{\"end\":57042,\"start\":57025},{\"end\":57054,\"start\":57042},{\"end\":57070,\"start\":57054},{\"end\":57086,\"start\":57070},{\"end\":57102,\"start\":57086},{\"end\":57114,\"start\":57102},{\"end\":57134,\"start\":57114},{\"end\":57147,\"start\":57134},{\"end\":57503,\"start\":57488},{\"end\":57519,\"start\":57503},{\"end\":57734,\"start\":57714},{\"end\":57750,\"start\":57734},{\"end\":57773,\"start\":57750},{\"end\":58277,\"start\":58255},{\"end\":58296,\"start\":58277},{\"end\":58311,\"start\":58296},{\"end\":58338,\"start\":58311},{\"end\":58590,\"start\":58577},{\"end\":58607,\"start\":58590},{\"end\":58790,\"start\":58778},{\"end\":58804,\"start\":58790},{\"end\":58817,\"start\":58804},{\"end\":58832,\"start\":58817},{\"end\":59068,\"start\":59059},{\"end\":59083,\"start\":59068},{\"end\":59096,\"start\":59083},{\"end\":59111,\"start\":59096},{\"end\":59360,\"start\":59338},{\"end\":59378,\"start\":59360},{\"end\":59394,\"start\":59378},{\"end\":59614,\"start\":59598},{\"end\":59629,\"start\":59614},{\"end\":59644,\"start\":59629},{\"end\":59660,\"start\":59644},{\"end\":59664,\"start\":59660},{\"end\":59934,\"start\":59915},{\"end\":59951,\"start\":59934},{\"end\":59968,\"start\":59951},{\"end\":59984,\"start\":59968},{\"end\":60006,\"start\":59984},{\"end\":60321,\"start\":60306},{\"end\":60331,\"start\":60321},{\"end\":60346,\"start\":60331},{\"end\":60567,\"start\":60557},{\"end\":60578,\"start\":60567},{\"end\":60594,\"start\":60578},{\"end\":60613,\"start\":60594},{\"end\":60865,\"start\":60850},{\"end\":60879,\"start\":60865},{\"end\":60895,\"start\":60879},{\"end\":61008,\"start\":60998},{\"end\":61022,\"start\":61008},{\"end\":61049,\"start\":61022},{\"end\":61068,\"start\":61049},{\"end\":61076,\"start\":61068},{\"end\":61097,\"start\":61076},{\"end\":61447,\"start\":61433},{\"end\":61469,\"start\":61447},{\"end\":61478,\"start\":61469},{\"end\":61493,\"start\":61478},{\"end\":61507,\"start\":61493},{\"end\":61778,\"start\":61764},{\"end\":61791,\"start\":61778},{\"end\":61806,\"start\":61791},{\"end\":61822,\"start\":61806},{\"end\":61829,\"start\":61822},{\"end\":62103,\"start\":62089},{\"end\":62118,\"start\":62103},{\"end\":62133,\"start\":62118},{\"end\":62147,\"start\":62133},{\"end\":62301,\"start\":62290},{\"end\":62314,\"start\":62301},{\"end\":62329,\"start\":62314},{\"end\":62346,\"start\":62329},{\"end\":62370,\"start\":62346},{\"end\":62388,\"start\":62370},{\"end\":62782,\"start\":62771},{\"end\":62794,\"start\":62782},{\"end\":62806,\"start\":62794},{\"end\":62821,\"start\":62806},{\"end\":62834,\"start\":62821},{\"end\":63066,\"start\":63057},{\"end\":63083,\"start\":63066},{\"end\":63287,\"start\":63278},{\"end\":63304,\"start\":63287},{\"end\":63504,\"start\":63493},{\"end\":63516,\"start\":63504},{\"end\":63524,\"start\":63516},{\"end\":63703,\"start\":63693},{\"end\":63714,\"start\":63703},{\"end\":63729,\"start\":63714},{\"end\":63935,\"start\":63924},{\"end\":63954,\"start\":63935},{\"end\":63968,\"start\":63954},{\"end\":63977,\"start\":63968},{\"end\":63993,\"start\":63977}]", "bib_venue": "[{\"end\":57948,\"start\":57869},{\"end\":46959,\"start\":46955},{\"end\":47216,\"start\":47209},{\"end\":47686,\"start\":47666},{\"end\":47995,\"start\":47934},{\"end\":48258,\"start\":48242},{\"end\":48498,\"start\":48494},{\"end\":48810,\"start\":48806},{\"end\":49056,\"start\":49052},{\"end\":49355,\"start\":49351},{\"end\":49770,\"start\":49766},{\"end\":50112,\"start\":50108},{\"end\":50339,\"start\":50335},{\"end\":50530,\"start\":50521},{\"end\":50769,\"start\":50765},{\"end\":51045,\"start\":51041},{\"end\":51358,\"start\":51354},{\"end\":51691,\"start\":51687},{\"end\":52005,\"start\":52001},{\"end\":52225,\"start\":52183},{\"end\":52502,\"start\":52498},{\"end\":52700,\"start\":52628},{\"end\":53075,\"start\":53068},{\"end\":53327,\"start\":53290},{\"end\":53681,\"start\":53663},{\"end\":54007,\"start\":54003},{\"end\":54232,\"start\":54228},{\"end\":54465,\"start\":54461},{\"end\":54714,\"start\":54710},{\"end\":55019,\"start\":55015},{\"end\":55362,\"start\":55358},{\"end\":55725,\"start\":55688},{\"end\":56064,\"start\":56060},{\"end\":56311,\"start\":56307},{\"end\":56567,\"start\":56563},{\"end\":56815,\"start\":56811},{\"end\":57154,\"start\":57147},{\"end\":57523,\"start\":57519},{\"end\":57867,\"start\":57773},{\"end\":58342,\"start\":58338},{\"end\":58611,\"start\":58607},{\"end\":58836,\"start\":58832},{\"end\":59115,\"start\":59111},{\"end\":59398,\"start\":59394},{\"end\":59668,\"start\":59664},{\"end\":59913,\"start\":59806},{\"end\":60350,\"start\":60346},{\"end\":60555,\"start\":60489},{\"end\":60899,\"start\":60895},{\"end\":61153,\"start\":61113},{\"end\":61511,\"start\":61507},{\"end\":61839,\"start\":61829},{\"end\":62151,\"start\":62147},{\"end\":62469,\"start\":62404},{\"end\":62838,\"start\":62834},{\"end\":63087,\"start\":63083},{\"end\":63308,\"start\":63304},{\"end\":63528,\"start\":63524},{\"end\":63733,\"start\":63729},{\"end\":63997,\"start\":63993}]"}}}, "year": 2023, "month": 12, "day": 17}