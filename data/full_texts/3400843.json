{"id": 3400843, "updated": "2023-08-30 20:21:49.578", "metadata": {"title": "Visual SLAM and Structure from Motion in Dynamic Environments: A Survey", "authors": "[{\"first\":\"Muhamad\",\"last\":\"Saputra\",\"middle\":[\"Risqi U.\"]},{\"first\":\"Andrew\",\"last\":\"Markham\",\"middle\":[]},{\"first\":\"Niki\",\"last\":\"Trigoni\",\"middle\":[]}]", "venue": null, "journal": "ACM Computing Surveys (CSUR)", "publication_date": {"year": 2018, "month": null, "day": null}, "abstract": "In the last few decades, Structure from Motion (SfM) and visual Simultaneous Localization and Mapping (visual SLAM) techniques have gained significant interest from both the computer vision and robotic communities. Many variants of these techniques have started to make an impact in a wide range of applications, including robot navigation and augmented reality. However, despite some remarkable results in these areas, most SfM and visual SLAM techniques operate based on the assumption that the observed environment is static. However, when faced with moving objects, overall system accuracy can be jeopardized. In this article, we present for the first time a survey of visual SLAM and SfM techniques that are targeted toward operation in dynamic environments. We identify three main problems: how to perform reconstruction (robust visual SLAM), how to segment and track dynamic objects, and how to achieve joint motion segmentation and reconstruction. Based on this categorization, we provide a comprehensive taxonomy of existing approaches. Finally, the advantages and disadvantages of each solution class are critically discussed from the perspective of practicality and robustness.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2788608285", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/csur/SaputraMT18", "doi": "10.1145/3177853"}}, "content": {"source": {"pdf_hash": "cabecdfab3dcae6ecb70b1507acbf2a484c40552", "pdf_src": "ACM", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "01b002c61e088e21571122d7fbd1f5e1e5a4ce0d", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/cabecdfab3dcae6ecb70b1507acbf2a484c40552.txt", "contents": "\n37 Visual SLAM and Structure from Motion in Dynamic Environments: A Survey\n\n\nMuhamad Risqi \nDepartment of Computer Science\nUniversity of Oxford\n\n\nU Saputra \nDepartment of Computer Science\nUniversity of Oxford\n\n\nAndrew Markham \nDepartment of Computer Science\nUniversity of Oxford\n\n\nNiki Trigoni \nDepartment of Computer Science\nUniversity of Oxford\n\n\n37 Visual SLAM and Structure from Motion in Dynamic Environments: A Survey\n10.1145/3177853CCS Concepts: \u2022 General and reference \u2192 Surveys and overviews\u2022 Computing methodologies \u2192 Image segmentationTrackingReconstructionAdditional Key Words and Phrases: Structure from motion, visual SLAM, deep learning, 3D reconstruction, visual odometry, motion segmentation, dynamic object segmentation, 3D tracking, dynamic environments\nIn the last few decades, Structure from Motion (SfM) and visual Simultaneous Localization and Mapping (visual SLAM) techniques have gained significant interest from both the computer vision and robotic communities. Many variants of these techniques have started to make an impact in a wide range of applications, including robot navigation and augmented reality. However, despite some remarkable results in these areas, most SfM and visual SLAM techniques operate based on the assumption that the observed environment is static. However, when faced with moving objects, overall system accuracy can be jeopardized. In this article, we present for the first time a survey of visual SLAM and SfM techniques that are targeted toward operation in dynamic environments. We identify three main problems: how to perform reconstruction (robust visual SLAM), how to segment and track dynamic objects, and how to achieve joint motion segmentation and reconstruction. Based on this categorization, we provide a comprehensive taxonomy of existing approaches. Finally, the advantages and disadvantages of each solution class are critically discussed from the perspective of practicality and robustness. 37:2 M. R. U. Saputra et al.estimating the probability distribution or to optimize over selected images, the estimation problem can be solved by filter-based approaches (e.g., Kalman filter) or bundle adjustment (BA)[150].MonoSLAM[29]can be considered as the first filter-based approach to bring the general SLAM problem from the robotic community into pure vision. It enables the propagation of first-order uncertainty of camera positions and feature measurement through a Bayesian framework under real-time computational constraints for robot navigation. In the computer vision community, Longuet-Higgins's paper[53]was probably the first work that led to the emergence of a flurry of SfM techniques. He discovered that the computation of relative camera pose can be done using 8-point correspondences from two views under epipolar geometry. Subsequently, other different perspectives for solving the problem including factorization[151,154]and rotation averaging[50,107]appeared. Some widely adopted systems are publicly available such as Bundler[147,148]or VisualSfM [174], although they work best in batch mode.The different goals and characteristics in the early work of SfM (offline) and visual SLAM (online) made the paths traveled by the computer vision and robotics communities different and largely disconnected. However, the work of[109,110,117]and PTAM [75] brought the two communities together by introducing incremental SfM that can operate in real time. Furthermore, the results from[150]indicate that incremental SfM based on bundle adjustment is more accurate than visual SLAM based on filtering given the same amount of computation time. Many visual SLAM solutions from the robotic community such as[94]or[113]were then developed based on incremental SfM. On the other hand, due to the growing need for more detailed maps and the availability of affordable depth cameras like Microsoft Kinect, solutions capable of producing a dense or semidense map, e.g., KinectFusion [115]  or LSD-SLAM[36], are gaining more popularity.Despite the remarkable results in SfM and visual SLAM, most approaches work based on the assumption that the observed environments are static. Since the real world contains dynamic objects, current approaches are prone to failure due to false correspondences or occlusion of previously tracked features[152]. Pose estimation might drift or even be lost as there are not sufficiently many features to be matched. There is a clear need to devise localization techniques that are robust under these circumstances. Robust pose estimation or localization in a dynamic environment is paramount for a number of applications such as robot navigation[10,108,149], driverless cars[102,145], or emergency response tasks[23,127].Another perspective to look at the SLAM problem in dynamic environments is not only to provide robust localization but also to extend its capability into detecting, tracking, and reconstructing the shape of the dynamic objects. To this end, [169] and [170] employed a laser scanner to track moving objects using a Bayesian approach and created in a system called SLAMMOT (Simultaneous Localization, Mapping, and Moving Object Tracking). The computer vision community also studied the Multibody Structure from Motion (MBSfM) topic, a generalization of SfM for multiple rigid body motions[12,25]. With the proliferation of mobile and wearable devices, this natural extension of visual SLAM in dynamic environments will benefit many applications, including obstacle avoidance [63], human-robot interaction [51], people following [183], path planning [19], cooperative robotics [46], collaborative mapping [28], driverless cars [102], augmented reality (e.g., mobile phone [76], wearable device [18]), or navigation assistance for the visually impaired [4, 134].This article reviews visual localization and 3D reconstruction techniques in dynamic environments, which covers three main problems: how to perform robust visual SLAM, how to segment and track dynamic objects in 3D, and how to achieve joint motion segmentation and reconstruction. We provide a taxonomy of the existing approaches and connect the fields of visual SLAM and dynamic object segmentation. Finally, we critically discuss the advantages and disadvantages of existing approaches from a practical perspective.Comparison to Other SurveysThere are a number of survey papers related to SfM and visual SLAM. Huang et al. (1994)  [64] discussed early development of SfM algorithms that focused on the reconstruction algorithm and its performance depending on the feature correspondence types.Oliensis (2000)[118] provided a critical review of multiple view reconstruction approaches (i.e., optimization, fusing by Kalman filter, projective methods, and invariant-based methods). They suggested that experiments and algorithm design should be based on theoretical analyses of the algorithm behavior.Bonin-Font et al. (2008)[10] discussed visual navigation for mobile robotics and divided the techniques into map-based navigation and mapless-based navigation.Fuentes-Pacheco et al. (2012)[40] reviewed visual SLAM approaches highlighting that visual SLAM techniques are prone to failure if the dynamic elements of the environment are not taken into account. However, the paper did not delve into the problems of dynamic scenes or describe existing techniques in this area.Recent review papers discussed various flavors of visual SLAM.Yousif et al. (2015)[181] surveyed general visual SLAM approaches covering Visual Odometry (VO) and Visual SLAM, including filter, nonfilter, and RGB-D-based solutions. The fundamental techniques used in both VO and Visual SLAM are presented to assist the community to choose the best techniques for a particular task. Similarly,Younes et al. (2016)[180] also discussed recent techniques in visual SLAM but focused on non-filter-based techniques only. They compared and made a critical assessment of specific strategies used by each technique. On the other hand,Garcia-Fidalgo et al. (2015)[43] focused on topological mapping that models the environment as a graph. They categorized the main solutions from 2000 to 2015 based on the type of image descriptors and discussed the advantages and disadvantages of each solution.From the existing surveys, it can be seen that no work has addressed the specific problem of dynamic environments. To the best of our knowledge, this article is the first survey article discussing in detail visual localization and 3D reconstruction techniques in dynamic environments.Article OrganizationThis article is organized as follows: Section 2 defines the problem and the general application of visual SLAM in dynamic environments. A taxonomy of existing approaches and the high level pipeline connecting them is also provided. Sections 3, 4, and 5 discuss existing techniques on robust visual SLAM, dynamic object segmentation and 3D tracking, and joint motion segmentation and reconstruction, respectively. Advantages and disadvantages of each approach are critically reviewed in Section 6. Finally, Section 7 concludes the article and discusses directions for future work.TAXONOMY OF EXISTING APPROACHESThe problem of simultaneous localization and reconstruction in dynamic environments can be viewed from two different perspectives: either as a robustness problem or as an extension of standard visual SLAM in dynamic environments. As a robustness problem, pose estimation in visual SLAM should remain accurate despite the presence of multiple moving objects in front of the camera, which might result in false correspondences or occlusion of the previously tracked features. Robustness is achieved by segmenting the static and dynamic features in the image and regarding the dynamic parts as outliers. Pose estimation is then computed based on the static parts only. From the perspective of extending visual SLAM into dynamic environments, the system should be capable of segmenting the tracked features into different clusters, each associated with a different object or body. Then, each object structure (shape) can be reconstructed and its trajectory tracked. If and when the static point cloud is available, the system can even insert the dynamic object into the static map.\n\nINTRODUCTION\n\nThe problems of estimating camera pose and reconstructing the three-dimensional model of the environment has drawn significant attention from many researchers over the past few decades. Techniques for solving this problem come from both computer vision and robotic research communities by means of Structure from Motion (SfM) and visual Simultaneous Localization and Mapping (visual SLAM). Standard SfM and visual SLAM aim to simultaneously estimate the camera pose and 3D structure of the scene through a set of feature correspondences detected from multiple images. By choosing whether to integrate feature measurements from all images by Although we only classify the application into three categories, more applications are possible with different configurations; e.g., for the robot-following-people scenario, reconstructing static features and tracking the object in image space or in 2D (instead of 3D) might be enough. Finally, the output from application A and B can be combined to obtain a similar output from application C.\n\n\nROBUST VISUAL SLAM\n\nRobust visual SLAM in dynamic environments can be achieved if pose estimation is computed based solely on static features. Figure 2 depicts the flow diagram of robust visual SLAM together  with the available approaches and the corresponding references. It can be seen that the input of the application is either the image sequence directly or the extracted feature correspondences depending on whether a deep-learning-based approach is employed or not. The application contains two major modules: (1) motion segmentation and (2) localization and 3D reconstruction. Motion segmentation classifies features into static and dynamic features, but only static features are used for localization and 3D reconstruction of the world. On the other hand, dynamic features and 3D point cloud data (output OA) can be directed to application B through action module B.1. and B.2 for further processing. This section discusses approaches in motion segmentation and standard localization and 3D reconstruction techniques for robust visual SLAM.\n\n\nMotion Segmentation\n\nMotion segmentation (also known as moving object detection/segmentation [30,74,84]) detects moving parts in the image by classifying the features into two different groups, static and dynamic features. Specifically, given a set of feature points W = {x i \u2208 IR 2 } n i=1 in image space, motion segmentation clusters the feature points into W 1 = {x 1 , . . . , x m } and W 2 = {x m+1 , . . . , x n } for the static and dynamic set, respectively, whereW 1 \u2229 W 2 = \u2205. Standard visual SLAM achieves this by computing geometric models (e.g., fundamental matrix, homography) using a robust statistical approach, such as by Random Sample Consensus (RANSAC) [37], and excludes feature points that do not conform with the model. Specific distance metrics such as the Sampson distance [59] are used to determine the exclusion. This approach will work well if the static features are in the majority. When the dynamic objects in front of the camera are dominant or the captured scene is occluded by a large moving object, these types of approaches may fail. Other approaches leverage external sensors such as an inertial measurement unit (IMU) to solve this problem [67,92] by estimating the camera ego-motion. Pose estimation from the IMU can be used to initialize the camera pose and segment static and dynamic features robustly. In this section, we discuss alternative approaches to segment static and dynamic features beyond the standard visual SLAM or visual-inertial SLAM techniques (see Table 1 for a summary of existing approaches).\n\n\nBackground-Foreground Initialization.\n\nBackground-foreground initialization techniques assume that the system has prior knowledge about the environment and leverages that information to segment static and dynamic features. This prior knowledge can be attached to either background (static features) or foreground objects (dynamic features). If the information is about the foreground object, it means that the system has knowledge about the type or the shape of the object that moves in front of the camera.\n\nMost approaches in foreground initialization make use of the tracking-by-detection scheme [14,89]. Wangsiripitak et al. [173] assume a 3D object where the dynamic features lie is known. They used a 3D polyhedral object modeled by a set of control points along the edges and tracked it using Harris's RaPid tracker [56]. If the previously tracked features lie on the tracking object, it will be removed as soon as the object is detected as moving. Any static features that are occluded by the object are removed as well. Similarly, Wang et al. [172] assumed that a set of SURF feature descriptors [8] belonging to the moving object are known and stored in the database. By comparing the descriptors obtained from the feature detection step, the moving object is identified and its displacement and orientation are estimated. Chhaya et al. [21] modeled vehicles in front of the camera using a deformable wireframe object class model. The model is trained on 3D CAD data using Principal Component Analysis (PCA). This model is used to recognize and to segment the car from pose estimation computation. On the other hand, Lee et al. [89,90] used a pretrained human detector to track pedestrians via the tracking-by-detection scheme. They employed the Constrained Multiple-Kernel (CMK) approach to handle occlusions during tracking by taking into account depth information.\n\nInstead of initializing the foreground object, background initialization sets a background model similarly found in background subtraction techniques [7,126]. Zhang et al. [184] initialized a set of feature points that belong to the background and set it as the background model. They assumed that there is no foreground object when the visual localization is first initialized. Then, when a new frame is processed, 3D motion segmentation is applied using GPCA [165]. Segmented motion with the highest correspondence to the prior background model is used to update the background. Pose estimation is computed using standard epipolar geometry based on the new background model.\n\n\nGeometric Constraints.\n\nTechniques that rely on geometric constraints leverage epipolar geometry properties [59] to segment static and dynamic features. They are based on the fact that dynamic features will violate standard constraints defined in multiple-view geometry for static Visual SLAM and Structure from Motion in Dynamic Environments 37:7 scenes (see Figure 3(a)). The constraints can be derived from the equation of epipolar lines, triangulation, fundamental matrix estimation, or reprojection error as seen in Figure 3\n(b).\nKundu et al. [84] construct the fundamental matrix from robot odometry to define two geometric constraints. The first constraint is derived from the epipolar geometry, which states that a matched point in the subsequent view should lie on the corresponding epipolar line. If the tracked feature resides too far from the epipolar line, then it is most likely a dynamic feature. The second constraint is Flow Vector Bound (FVB), which is aimed to segment degenerate motion that occurs when a 3D point moves along the epipolar line. By setting upper and lower bounds on the flow of the tracked features, a tracked feature that lies outside the bound will be detected as moving. Finally, the decision of classifying features as static or dynamic is determined by a recursive Bayes filter. Instead of using the epipolar line, Migliore et al. [104] segment static and dynamic features by the principle of triangulation. They continuously check the intersection between three projected viewing rays in three different views under a probabilistic filtering framework. If a feature is dynamic, the intersection of the rays is not the same or may not even occur during motion. However, since the sensor measurement is noisy, they employed Uncertain Projective Geometry [61] to check the relationships between viewing rays while taking into account the uncertainty of measurement. The classification of static and dynamic features is then determined via a statistical hypothesis test.\n\nLin et al. [95] detect moving objects based on an observation that misclassifying a moving object into a static object and incorporating it into the pose estimation would significantly degrade the SLAM performance. They compute the difference of pose estimation under two distinct conditions, one without adding the detected new feature and the other one with including the new feature under assumption that it is stationary. By computing the distance between the two results, setting a threshold value, and integrating it through a binary Bayes filter, they are able to segment stationary and moving features with high accuracy.\n\nAnother geometric approach is to leverage the reprojection error. Zou and Tan [28] project features from the previous frame into the current frame and measure the distance from the tracked features. The classification of static and dynamic features is determined by the magnitude of their reprojection distances. Tan et al. [152] also use a similar projection principle to detect dynamic features. However, they also take into account occlusion handling to provide robust visual SLAM. After a feature is projected into the current frame, appearance differences are used to check whether a part of the image has changed. If the appearance changes significantly, it is very likely that the region may be occluded by a dynamic object or by a static object due to viewpoint changes. 3D points occluded by those conditions will be kept and used to robustly estimate the camera pose.\n\n\nOptical Flow.\n\nOptical flow defines the apparent motion of brightness patterns computed from two consecutive images [62]. Generally, it corresponds to the motion field in an image, and thus it can be used to segment a moving object. Klappstein [74] defined a likelihood of a moving object based on a motion metric computed from the optical flow. The motion metric measures to what extent the optical flow is violated if there is a moving object on the scene. The graph-cut algorithm is utilized to segment the moving objects based on the motion metric.\n\nAlcantarilla et al. [4] segment moving objects based on the modulus of the 3D motion vector in scene flow (3D version of optical flow) through residual motion likelihoods. The Mahalanobis distance is used to take into account measurement uncertainty in computing scene flow based on dense optical flow and stereo reconstruction. If the residual is low, the feature point most likely belongs to the static object. By thresholding on the residual motion likelihoods, the feature points that reside on the moving object can be deleted from the SLAM process, making visual odometry estimation more robust. Derome et al. [30,31] compute optical flow by calculating the residual between the predicted image with the observed image from a stereo camera. By processing backward in time, the predicted image is computed by transforming the current stereo frame into the previous frame using estimated camera ego-motion. Moving objects are then observed by detecting blobs in the residual field.\n\n\nEgo-Motion Constraints.\n\nStandard SfM and visual SLAM compute the motion of the camera by means of the 8-point [53] or the 5-point algorithms [116]. This general ego-motion estimation is calculated without making any assumption on how the camera moves. Another way to estimate the camera pose is by assuming that the camera moves according to particular parameterization given external information (e.g., wheel odometry information). By enforcing this ego-motion constraint, classifying static features can be done by fitting feature points that match with the camera motion constraints.\n\nScaramuzza [136] proposed to use nonholonomic constraints of wheeled vehicles to compute camera motion. He modeled the ego-motion based on the assumption that the camera motion is planar and circular. By using this constraint, the camera ego motion can be parameterized by one Degree of Freedom (DOF) and can be computed by the 1-point algorithm [137]. Similarly, Sabzevari et al. [133] also employed the wheeled vehicle constraint to estimate camera motion by leveraging Ackermann steering geometry. Feature points satisfying the estimated camera motion are considered as static features, while other points are regarded as dynamic features.\n\n\nDeep Learning for Motion Segmentation.\n\nAfter winning the ImageNet object recognition competition by reducing classification errors by half compared with state-of-the-art techniques [79], Deep Neural Networks (DNNs) have gained much popularity in the computer vision community. DNNs are a representation learning technique that aim to learn high-level abstractions of the data by using multiple hierarchical layers of neural networks [52,88]. The main characteristic of DNNs are that they can process raw input data directly without the necessity of handengineered feature extraction. This technique has started to make significant changes in many research areas, including ones that were previously considered as not possible to cast them as a learning problem due to the involvement of geometric transformations [55]. While a number of implementations of DNN for visual localization and 3D reconstruction have started to emerge (discussed in Section 3.2.2), DNNs for motion segmentation are still scarce.  [30,31] S From feature-based motion segmentation, we know that the moving objects can be segmented by leveraging optical flow. Dosovitskiy et al. [33] show that estimating optical flow can be done through supervised learning. They proposed two different architectures of Convolutional Neural Network (CNN) for predicting optical flow. The first architecture (FlowNetS) is designed by stacking two consecutive images as an input of CNN and the other one (FlowNetC) is by introducing a correlation layer to compare two feature maps resulting from two identical CNN streams. Ilg et al. [66] improved this approach into \"FlowNet 2.0\" by stacking FlowNetS and FlowNetC into a deeper network and adding a new parallel network to handle small displacements. Experimental results show that FlowNet 2.0 can achieve competitive results with the state-of-the-art methods. An extension to scene flow estimation using stereo images is also shown by Mayer et al. [101]. This optical flow can be fed into a deeper network to discover the motion features as shown in [48]. These motion features are shown to be useful for action recognition [47,146], although it is not clear whether the same network can be used to segment moving objects and provide motion boundaries since it is not explicitly designed for solving the motion segmentation problem.\nf M S M P R T - - - Ego-Motion Constraints (Section 3.1.4) Scaramuzza [136] S f M M M P R T - - - - - Sabzevari et al. [133] S f M M M P R T - - - - - Deep\nLin and Wang [96] construct a network to explicitly segment moving objects in an image space. They employ Reconstruction Independent Component Analysis (RICA) autoencoders [86,87] to learn spatiotemporal features. However, geometric features are still used to help segment the motion since the spatiotemporal features cannot learn the 3D geometry of the motion. Both geometric and spatiotemporal features are fed into Recursive Neural Networks (RNNs) for final motion segmentation. Using a different approach, Fragkiadaki et al. [38] segments moving objects by regressing the objectness score given RGB image and optical flow. Two parallel CNNs similar to AlexNet [79] are constructed to process RGB images and optical flow before feeding it to the regression network and generating the motion proposal. Recently, Valipour et al. [160] propose Recurrent Fully Convolutional Network (R-FCN) to incorporate temporal data in segmenting foreground motion from online image sequences. Fully Convolutional Network (FCN) [98] is used to learn spatial features and to produce the pixel dense prediction, but Gated Recurrent Unit (GRU) is employed to model temporal features before deconvolution is applied.\n\n\nLocalization and 3D Reconstruction\n\nLocalization and 3D reconstruction refer to the estimation of relative camera pose (translation and rotation) and the 3D structure of the observed environment from multiple images. Standard visual SLAM achieves this by leveraging feature correspondences. Let {x 1j , x 2j } p j=1 \u2208 IP 2 be a set of feature correspondences in the first and the second image, where p is the total number of points. Visual SLAM estimates the camera pose containing a translation vector t \u2208 IR 3 and rotation matrix R \u2208 SO (3) and the 3D structure of all features {X j } p j=1 \u2208 IP 3 by implementing epipolar geometry [59] on the feature correspondences. In robust visual SLAM, instead of computing the camera pose and 3D structure from all feature correspondences, only static features resulting from techniques described in Section 3.1 are employed. All dynamic features are regarded as outliers and excluded from the computation. On the other hand, deep learning techniques can process the image sequences directly without computing feature correspondences. This section discusses both feature-based and deep-learning-based approaches for solving the localization and 3D reconstruction problem.\n\n\nFeature-Based Approaches.\n\nIn feature-based visual SLAM, salient features are extracted to solve the image correspondence problem. The computer vision community has developed a large number of feature extraction techniques. While early work in SfM [157] including the prominent \"Visual Odometry\" [117] made use of the Harris corner detector [57], most recent work [142,174] employs robust feature detection techniques such as Scale Invariant Feature Transform (SIFT) [99] or its lightweight variants like Speeded Up Robust Features (SURF) [8]. However, since SIFT and SURF are considered computationally expensive, a faster approach such as Features from Accelerated Segment Test (FAST) [130] is utilized for real-time applications [76,94].\n\nTo find correspondences, extracted features are matched using feature-matching techniques. The techniques can be divided by how far the distance between the optical centers of two cameras (termed baseline/parallax) are separated. For short baselines, optical flow-based techniques (e.g., Kanade-Lucas-Tomashi (KLT) tracker [100]) can be used for matching. On the contrary, for long baselines, highly discriminative feature descriptors (e.g., SIFT [99], SURF [8], BRIEF [17], BRISK [91], etc.) are necessary to find correspondences by calculating dissimilarity between those descriptors. Unfortunately, using these feature-matching techniques does not guarantee perfect correspondences, especially when the data contains outliers. Implementation of robust estimators (e.g., RANSAC [37], PROSAC [22], MLESAC [158], etc.) is useful to reject outliers and handle false correspondences.\n\nIf the image correspondences are known, the relative pose between two or three images can be recovered up to a scale factor. By enforcing the epipolar constraint, the pose from two views can be computed by the 8-point [53] or the 5-point algorithm [116], while the trifocal tensor [156] can be utilized if three views are available. In case some 3D points of the scene have been reconstructed, camera poses can be obtained with respect to the 3D model by solving the perspective-n-point problems (e.g., P3P algorithm [42]).\n\nWhen the camera pose is recovered, one can easily reconstruct 3D points of the scene by intersecting two projection ray lines through triangulation. As the rays do not always intersect due to erroneous correspondences, the midpoint method [9] or least-square-based method [60] is proposed to estimate the intersection. Then, to avoid the drifting problem, bundle adjustment (BA) [175] is employed to refine both the camera pose and 3D points by minimizing reprojection errors. A variant of the Gauss-Newton method, namely, Levenberg-Marquardt (LM) optimization, is the prevalent method to jointly optimize the structure of the scene and the motion of the camera.\n\nIn practice, there are some variations on how to implement feature-based visual SLAM. Instead of optimizing the camera pose and 3D structure of the environment over all images, Mouragnon et al. [110,111] propose to optimize the last few images by employing local bundle adjustment (LBA). Klein and Murray [75] introduce \"PTAM,\" which shows that tracking and mapping can run in real time if the pipeline is executed on different threads. Furthermore, PTAM also introduced the idea of choosing key frames, and thus LBA can also be implemented over the selected key frames. On the other hand, Lim et al. [94] used binary descriptors and a metric topological mapping such that large-scale mapping can operate in real time without any parallel computation. Recent state-of-the-art techniques like ORB-SLAM [113] integrate hardware and algorithmic advancement in the past decade by including parallel computing, ORB features [131], statistical model selection [155], loop closures based on bag-of-words place recognition [26,41], local bundle adjustment [111], and graph optimization [81]. For a more detailed review of ORB-SLAM or other standard feature-based techniques, interested readers can follow [40] or [180].\n\n\nDeep Learning for Pose Estimation and 3D Reconstruction.\n\nRecent developments on deep learning show that pose estimation can be regarded as a learning problem. While many end-to-end architectures for ego-motion computation have emerged [103,171], there is no end-to-end learning for 3D reconstruction yet. Most recent works only stop the learning process at depth prediction [168,187], although the resulting depth data can be used to reconstruct the 3D environment using point-based fusion as seen in [85].\n\nThere are two common methods for training pose estimation found in the existing literature, namely, supervised learning and unsupervised learning.\n\n1) Supervised Learning. Supervised learning trains CNNs by minimizing errors in predicting the ego-motion compared to the ground-truth pose. As CNN is best known for classification tasks, in early works, pose estimation is considered as a classification problem over the discretized space of translation and rotation of the camera. Konda and Memisevic [78] were probably the first to propose the estimation of visual odometry using this principle. They utilized a stereo camera to predict the velocity and the direction of the camera. The network trains the representation of motion and depth from stereo pairs by using synchrony autoencoders [77]. These motion and depth representations are fed into a CNN to estimate the velocities and orientations through softmax-based classification. Instead of estimating general motion similar to fundamental matrix, DeTone et al. [32] proposed \"HomographyNet\" to train a CNN for computing homography between two frames using 4-point parameterization of homography. They proposed two different networks: one is a classification network based on cross-entropy loss function and the other one is a regression network based on Euclidean loss function. They showed that the regression network is more accurate than the classification network due to its continuous nature of the prediction.\n\nAfter realizing that CNNs can be used accurately for the regression problem, all recent techniques for pose estimation employ regression-based CNN. Mohanty et al. [105] utilized a pretrained AlexNet network [79] for the input of the regression network. Two consecutive images are fed into two parallel AlexNet networks and then the outputs are concatenated for regressing the camera odometry through the fully connected layer. Based on the experiments, they observed that the extracted features from AlexNet are not generic for the problem of visual odometry, and thus the odometry only works well in a known environment.\n\nSince pretrained convolutional layers for object detection and classification are not suitable for odometry estimation, researchers turned to optical flow-based networks to generalize the learned parameters in different environments. Muller and Savakis [112] designed \"Flowdometry,\" a network consisting of two sequential CNNs: the first one for predicting optical flow and the latter for estimating camera motion. FlowNetS [33] architecture is used for both networks, although the second network replaces the refinement part by a fully connected layer in order to incorporate interframe odometry computation. Melekhov et al. [103] developed an end-to-end CNN for computing ego-motion between two views. They stacked two parallel CNNs with weight sharing followed by a spatial pyramid pooling (SPP) layer to tackle arbitrary input images while maintaining spatial information in the feature maps. The regression layer consists of two fully connected layers for predicting camera translation and rotation.\n\nWhile the previous works only learn geometric feature representation of the scene through CNNs, Wang et al. [171] propose \"DeepVO\" as an end-to-end learning framework capable of learning sequential motion dynamics from image sequences through a Recurrent Convolutional Neural Network (RCNN), a combination of CNN and Recurrent Neural Network (RNN). RNNs are prominent for learning sequential data such as speech or language since they maintain a history of all elements of the sequence in the network [88]. It turns out that by utilizing both CNN and RNN, the output odometry is much better and has competitive performance over the state-of-the-art methods (compared to VISO2 Monocular and Stereo system [45]). Nonetheless, they stated that the moving objects in front of the camera might reduce the accuracy of pose estimation, but it is unclear how to deal with it under a deep learning framework.\n\n2) Unsupervised Learning. In the unsupervised case, the CNN is trained without the availability of ground-truth data. Instead, the network learns to predict the camera pose by minimizing the photometric error similar to LSD-SLAM [36]. Given I r ef as a reference image where I : \u03a9 \u2192 IR provides the color intensity, the photometric error minimizes the following objective function:\nE (\u03be ) = i \u2208\u03a9 r ef (I r ef (x i ) \u2212 I new (\u03c9 (x i , D r ef (x i ), \u03be ))) 2 ,(1)\nwhere\n\u03c9 (x i , D r ef (x i ), \u03be )\nis a warp function that projects the image point x i \u2208 \u03a9 r ef in the reference image I r ef to the respective point in the new image I new based on the inverse depth value of the reference image D r ef (x i ) and the camera transformation \u03be \u2208 se (3). Zhou et al. [187] developed this unsupervised learning mechanism using the principle of novel view synthesis (the problem of synthesizing a target image with different poses given a source image). They constructed two parallel CNN networks for predicting depth and estimating the camera pose. The predicted depth from the source image is used for synthesizing the target image given the camera transformation matrix and the source image. By minimizing the photometric error as in Equation (1), depth and camera pose can be jointly trained. Instead of generating the target image from depth prediction, Vijayanarasimhan et al. [168] constructed a 3D scene flow based on depth prediction, camera motion, and dynamic object segmentation resulting from the convolutional/deconvolutional network. The scene flow is transformed by the camera motion and then back-projected to the current frame for evaluating the photometric error.\n\n\nDYNAMIC OBJECT SEGMENTATION AND 3D TRACKING\n\nDynamic object segmentation and 3D tracking clusters feature correspondences into different groups based on their motion and tracks their trajectories in 3D. Figure 4 shows the flow diagram of the existing approaches and the corresponding literature references in dynamic object segmentation and 3D tracking. It can be seen that the input of feature-based techniques for dynamic object segmentation consists of either full features or dynamic features only (obtained from action module A.1). On the other hand, the deep-learning-based approach can process the image sequences directly. The segmented dynamic objects are then fed into the 3D tracking module to obtain the object trajectories. Camera ego-motion and a 3D point cloud obtained from action module A.2 can be optionally utilized to help the tracking process. The availability of the 3D point cloud can make the output object trajectories consistent with the static world. This section discusses techniques for segmenting and tracking the dynamic objects in the scene.\n\n\nDynamic Object Segmentation\n\nDynamic object segmentation (also known as multibody motion segmentation [73,132,153] or eorumotion segmentation [133]) clusters all feature correspondences into n number of different object motions. It is considered a difficult problem due to the chicken-and-egg characteristic of the problem. In order to estimate the motion of the object, the features should be clustered first; on the other hand, the motion models for all moving objects are required to cluster the features. The problem is compounded by the presence of noise, outliers, or missing feature correspondences due to occlusion, motion blur, or losing tracked features. Another challenge is to deal with degenerate motion (e.g., when an object moves on the same plane and the same direction and velocity with the camera motion) or dependent motion (e.g., two people moving together, articulated motion). This section discusses existing approaches to handling this problem (see Table 2 for the summary).\n\n\nStatistical Model Selection.\n\nIn a static scene, the transformation of the feature points between consecutive images can be described by one motion model. In contrast, the feature points in dynamic scenes might have arisen from more than one motion model, each associated with a different body. Motion models can be based on one of the following categories: fundamental matrix (F ), affine fundamental matrix (F A ), essential matrix (E), homography/projectivity (H ), or affinity (A). The model selection problem tries to fit all possible motion models with the data and select the one that best fits the data. If the data can be described by several models like in a dynamic scene, many hypotheses are required to segment the data based on the motion models.\n\n3D motion segmentation approaches based on statistical techniques sample a subset of the data and fit a motion model into the sampled data under RANSAC [37] or the Monte-Carlo sampling iteration [139]. The motion model is used to build an inlier set and excludes the remaining data as the outliers of the model. Then, sampling is conducted again for the remaining data (outliers of the previous model) to find and fit another model that best describes the remaining data. This process is repeated until all data can be described by n motion models or the remaining outliers are not sufficient to generate more motion models. This motion segmentation process can be repeated again from the beginning to generate many candidate hypotheses (see Figure 5(b)).\n\nThe method to determine which model is best to describe the data is based on an information criterion. Several information criteria exist in the literature. Akaike's information criterion (AIC) [2] selects the model that maximizes the likelihood function yet minimizes the number of estimated parameters to generate the model. The penalization in the number of parameters is based on the observation that the maximum likelihood estimation always selects the most general model as the best fit model [155]. An intuitive example is that the errors of any points with respect to a point are higher or equal to the errors with respect to a line; thus, a line is always selected as the best model to describe the data points. AIC tackles this drawback by balancing the tradeoff between the goodness of the fit with the complexity of the model. It has the following form:\nAIC = (\u22122)lo\u0434(L) + 2K,(2)\nwhere L is the log likelihood function and K is the number of parameters of the model. The likelihood function is generally estimated to maximize the likelihood of observing correspondences based on a particular distance metric such as reprojection error or Sampson distance approximation [59]. Then, AIC selects the model that has the minimum AIC score under the minimum AIC estimate (MAICE) procedure.\n\nDespite its popularity, AIC does not have asymptotically consistent estimates and is prone to overfitting because it does not take into account the number of observations. Schwarz [143] proposes a revision using the Bayesian theorem termed Bayes Information Criterion (BIC). BIC extends the posterior probability of observing the data by modeling the prior based on its complexity. On the other hand, Rissanen [129] developed Minimum Description Length (MDL) by using a minimum-bit representation to minimize the coding length of the data. Based on the limitation of previous works, Kanatani [71,72] proposed Geometric Information Criterion (G-AIC, or in some literature called GIC) by taking into account the number of observations and the dimension of the model; it has the following form:\nGIC = (\u22122)lo\u0434(L) + 2(DN + K ),(3)\nwhere N is the number of data and D is the dimension of the model (e.g., two for a homography, three for a fundamental matrix). Another extension based on BIC is Geometrically Robust Information Criterion (GRIC) devised by Torr [155]. By incorporating robustness to outliers and the capability to deal with different dimensions, GRIC has the following form:\nGRIC = (\u22122)lo\u0434(L) + DNlo\u0434(R) + Klo\u0434(RN ),(4)\nwhere R is the dimension of data.\n\nThere are different ways to implement statistical model selection for 3D motion segmentation. Torr [155] samples nearby feature correspondences and computes different motion models (F , F A , H , A) under the RANSAC iteration. GRIC is used to select the best motion model that fits with a particular inlier cluster. However, Expectation-Maximization (EM) is applied when the number of inliers for the selected model is lower than a threshold. In order to avoid the expensive computation of brute-force sampling, Schindler and Suter [138,139] propose local Monte-Carlo sampling by drawing samples from a defined subregion on the image. They present a method to estimate the noise scale from the data, thus allowing the residual distribution for each motion and its standard deviation to be recovered. Moreover, they derived a new likelihood function that allows the motion models (F , H ) to overlap, while the best model is selected by GRIC as shown in Equation (4).\n\nWhile the previous approaches operate on two image sequences, Schindler et al. [141] extended the technique in [138] to several perspective images under a general motion model (essential matrix E). In order to link several essential matrix candidates from more than two image sequences, temporal coherence is enforced by connecting only essential matrices with similar inlier sets. Finally, an MDL-like approach is utilized to select the best model that describes the motion. This method has been generalized for any camera model (not only perspective camera) and motion model (not only essential matrix E) by Schindler et al. [140]. Practical considerations have also been taken into account by Ozden et al. [122]. They handled how to merge a previously moving object with the background or how to split a cluster into two different motions.\n\nThakoor et al. [153] formulated the model selection problem as a combinatorial optimization. The branch-and-bound technique is employed to optimize the segmentation of motion using AIC as the cost function, by splitting the optimization problem into smaller subproblems. Local sampling of correspondences is also used to generate the motions, while the null hypothesis is introduced to handle outliers. Recently, Sabzevari and Scaramuzza [132] utilized a statistical model selection technique under factorization of the projective trajectory matrix framework. Epipolar geometry is used to generate the motion models, while reprojection error is employed to reject invalid hypotheses. The hypotheses are evaluated by iteratively refining the structure estimation and motion segmentation. This has been extended in [133] by enforcing ego-motion constraints such that the camera motion and the moving object motions can be computed by using the one-point algorithm [136,137] and the two-point algorithm [119], respectively.\n\n\nSubspace Clustering.\n\nSubspace clustering is developed based on the observation that many high-dimensional data can be represented by a union of low-dimensional subspaces. A subspace of data points can be represented by basis vectors and low-dimensional representation of the data. The problem of 3D motion segmentation under the subspace clustering framework is basically finding each individual subspace associated with each body motion and fitting the data into the subspaces (see Figure 5(a)). However, since the subspaces and the data segmentation are not known in practice, estimating the subspace parameters and clustering the data into different subspaces should be done simultaneously. This problem was originally pointed out by Costeira-Kanade [25] and Gear [44] based on the observation that independent rigid body motion lies in a linear subspace. By enforcing the rank constraint (see Section 5.1 for more details), each linear subspace can be recovered.\n\nKanatani [72] coined the term of subspace separation as a general method for clustering lowdimensional subspace (not only limited to motion segmentation). The subspace separation is done by borrowing the principle of statistical model selection, but a subspace is fitted instead of a motion model. AIC is used to select the best subspace configuration by balancing the increase of the residual when data points are fitted to a subspace and the decrease of the degree of freedom when merging two subspaces into one group. Least median of squares is employed to fit the data points that contain outliers. Differently, Vidal et al. [164,165] proposed Generalized Principal Component Analysis (GPCA) as an extension of PCA. While PCA only works for data lying in a linear subspace, GPCA generalizes the problem into data points arising from multiple linear subspaces. In GPCA, the problem of finding subspaces is done by fitting of the homogeneous polynomial of degree n into the data through polynomial embedding (or Veronese map) and finding the normals of each subspace by computing the derivatives of the polynomial at a particular point. Then, the segmentation is obtained by computing the similarity matrix from the angle between the normal vectors and clustering it using spectral clustering. For practical consideration in motion segmentation, GPCA is extended in [165] by projecting the data into a lower-dimensional space before clustering is executed. Then, the number of motions n can be computed by finding the rank of the polynomial embedding.\n\nWhile the previous works assumed that the motions are rigid, Yan and Pollefeys [177] proposed a general framework called Local Subspace Affinity (LSA) for independent, articulated, rigid, nonrigid, degenerate, and nondegenerate motions. LSA estimates a subspace by sampling a point and its nearest neighbors and fitting a local subspace to the sampled data. The nearest neighbors can be found by computing the angles or the distance between the vectors. Then, an affinity matrix is computed as the principal angles between two local subspaces and the clustering is done by applying spectral clustering to the affinity matrix. Projection into a lower-dimensional subspace is also carried out before the subspace is estimated. Similar to LSA, Goh and Vidal [49] also fit a local subspace to a point and its nearest neighbors. The method, known as Locally Linear Manifold Clustering (LLMC), is developed based on the Locally Linear Embedding (LLE) [135] algorithm. They cluster separated manifolds associated with each motion by transforming the data into low-dimensional representation using LLE and computing the null space of the matrix resulting from LLE. They showed that the segmentation of the data is indicated by the vectors in the null space.\n\nAnother point of view is given by Elhamifar and Vidal [34,35] that leverages a sparse representation to cluster motion. They propose Sparse Subspace Clustering (SSC) based on the observation that a point in a union of linear or affine subspaces can be represented as a linear or affine combination of all data points in the subspaces. However, the sparsest representation is only obtained when the point is written as a linear or affine combination of the data lying in the same subspace. Under noiseless data, the sparsest coefficient can be estimated by solving the L 1 minimization problem. Given the sparsest coefficient, an affinity matrix can be built and the clustering can be done by spectral clustering. An extension of SSC is developed by Rao et al. [128]. They fused sparse representation and data compression to deal with practical issues such as when the data is missing, is incomplete, or contains outliers. Recently, Yang et al. [179] also improved SSC by proposing various matrix completion techniques for data with missing entries. Instead of using sparse representation, Liu et al. [97] and Chen et al. [20] employ Low-Rank Representation (LRR), which can also be used to define the affinity matrix for subspace segmentation using spectral clustering.\n\nIt is worth noting that most subspace clustering techniques operate in batch mode. Vidal [161] devised an iterative clustering technique for data lying in multiple moving hyperplanes. He modeled the union of moving hyperplanes by a set of time-varying polynomials. The segmentation is done recursively by estimating the normal vector of the hyperplanes within the normalized gradient descent framework. Another implementation of online subspace clustering was proposed by Zhang et al. [185]. They modified the K-flats algorithm such that it can take the input data incrementally. L 1 is used as the objective function instead of L 2 in order to boost its performance under noise and data containing outliers.\n\nIn past decades, subspace clustering has become a widely studied topic, and many approaches have been developed by diverse research communities. There are several survey papers related to subspace clustering, from general techniques to those focusing on the application of motion segmentation and face clustering. For a more detailed review of subspace clustering, interested readers can follow [162].\n\n\nGeometry.\n\nGeometry approaches extend the standard formulation of geometry of multiple views from static scenes to dynamic scenes containing independent moving objects. While there is one fundamental matrix that describes general motion of the camera with respect to the static scene, in a dynamic environment, there will be n fundamental matrices that describe the motion of n bodies, including one for static features. Vidal et al. [166] study a generalization of this problem by proposing multibody epipolar constraints. Given x 1 and x 2 as feature correspondence between the first and the second image, respectively, x T 2 Fx 1 = 0 represents the epipolar constraint for the static scene, where F \u2208 IR 3\u00d73 is the fundamental matrix (see Figure 3(a)). If the scene contains n independent moving objects, there are a set of fundamental matrices {F i } n i=1 associated with each moving object such that the following multibody epipolar constraint is satisfied [167]:\n\u03b5 (x 1 , x 2 ) n i=1 (x T 2 F i x 1 ) = 0.(5)\nThis multibody epipolar constraint transforms the standard epipolar constraint equation from a bilinear to a homogeneous polynomial of degree n (in x 1 and x 2 ). This homogeneous polynomial equation can be converted into the bilinear problem again by mapping the polynomial equation into a vector containing M n monomials using the veronese map v n : IR 3 \u2192 IR M n , where M n ( n + 2 n ). Thus, the multibody epipolar constraint in Equation (5) can be transformed into\nv n (x 2 ) TF v n (x 1 ) = 0,(6)\nwhereF is the multibody fundamental matrix, a symmetric tensor product representation of all fundamental matrices [166,167]. If n is known, by reordering the entries of v n (x 1 ) and v n (x 2 ) using the Kronecker product and stacking the row ofF into f \u2208 IR M 2 n , Equation (6) can be transformed into a linear equation in f and can be estimated by least squares. Individual fundamental matrices F i can then be recovered by finding the epipolar line associated with each motion through  polynomial factorization of multibody epipolar linel F v n (x 1 ) \u2208 IR M n . Subsequently, the motion segmentation of dynamic features can be done by assigning each feature correspondence with the correct fundamental matrix [167].\n\n\nCamera a Motion b Practical Consideration d Author(s) T ST M SQ N T AC c RT SO NP ND OT MD DP DG\n\nVidal and Hartley [163] extended the multibody SfM formulation from two views into three views by introducing the multibody trilinear constraint and multibody trifocal tensor. It is the generalization of the trilinear constraint and trifocal tensor [59,156] from a static scene to a dy-namic scene containing multiple objects. The multibody trifocal tensor can be solved linearly by embedding the feature correspondences as in Equation (6) and estimating using least squares. Each trifocal tensor corresponding to each object is recovered from the multibody trilinear constraint by computing its second-order derivative.\n\n\nDeep Learning for Dynamic Object Segmentation.\n\nCurrent works of DNNs for solving the dynamic object segmentation problem rely on a predefined number of rigid body motions. The network and its associated cost function to produce dense object masks might be derived from 3D point cloud data or optical flow. Byravan and Fox [15] introduce \"SE3-Net\" as a DNN that is capable of segmenting predefined n dynamic objects represented in SE(3) transforms from a 3D point cloud. A convolutional/deconvolutional encoder-decoder network is constructed to predict object masks and a rigid body transformation for each object. The encoder consists of two parallel convolutional and fully connected networks that produce latent variables from the point cloud and encode the control vector, respectively. The decoder processes the concatenated output from the encoder to produce pointwise object masks and SE(3) transformation through two parallel deconvolutional and fully connected networks. A transform layer is used to fuse the 3D point cloud data, the object masks, and their SE(3)s to generate a predicted point cloud for data training.\n\nVijayanarasimhan et al. [168] have shown the utilization of optical flow for segmenting dynamic objects using DNN. They designed a network termed \"SfM-Net,\" a geometry-aware network capable of predicting depths, camera motion, and dynamic object segmentation. The networks consist of two stream convolutional/deconvolutional subnetworks, acting as structure and motion networks. The structure network learns to predict depth, while the motion network estimates camera and object motion. While the object motion is computed by two fully connected layers on top of the embedding layer produced by CNN, the dynamic object segmentation is predicted by feeding the embedding layer to the deconvolutional network. The outputs from both structure and motion networks are then converted into optical flow by transforming the point cloud from depth prediction according to camera and object motion, followed by reprojecting the transformed point cloud into the image space. By using this technique, the network can be trained by self-supervision through minimizing photometric error as in Equation (1), although full supervised learning is also possible.\n\n\n3D Tracking of Dynamic Objects\n\nThe problem of tracking dynamic objects in 3D, knowing the position of the moving object in 3D coordinates, including depth information, is substantial. The challenge is that the standard approach in visual SLAM for estimating the 3D structure of the scene, which is triangulation [60], does not work for dynamic objects since the rays back-projected from the corresponding feature points do not meet. Given x 1 and x 2 as the feature correspondences from the first and the second image, respectively, the corresponding 3D point X should be able to be computed by intersecting the back-projected rays of x 1 and x 2 via their associated camera projection matrix P 1 and P 2 . Since the object has independent motion (from camera motion), the projection rays from the first to the second frame are also moving, and thus do not intersect (see Figure 3). Alternative techniques are required to solve this problem. This section discusses existing approaches for recovering 3D trajectories of the objects moving in front of camera (see Table 3 for the summary). [60] cannot be used to reconstruct the 3D structure of the moving objects since the back-projected rays do not intersect. Avidan and Shashua [5,6] coined the term trajectory triangulation as a technique to reconstruct 3D points of the moving object when the object trajectory is known or satisfies a parametric form. They assumed that the 3D point is moving along an unknown 3D line. Then, the reconstruction problem is turned into the problem of finding a 3D line that intersects projected rays from t views. In order to have a unique solution, at least t = 5 is required since the set of intersecting lines from three views will form a quadric surface that makes the ray from the fourth view intersect at two points. Thus, five views result in a unique solution.\n\n\nTrajectory Triangulation. Standard triangulation\n\nSpecifically 3 be the 3D points on the line L represented in homogeneous coordinates. If x i and l i are the projection of the 3D point and the line L on frame i, respectively, it is clear that\n, let A = [1, X A , Y A , Z A ] \u2208 IP 3 and B = [1, X B , Y B , Z B ] \u2208 IPx T i l i = 0,(7)\nsince x i lies on l i (see Figure 6(a)). Line L can be represented in a Plucker coordinate as follows:\nL = A \u2227 B = [X A \u2212 X B , Y A \u2212 Y B , Z A \u2212 Z B , X A Y B \u2212 Y A X B , X A Z B \u2212 Z A X B , Y A Z B \u2212 Z A Y B ]. (8)\nBy using Plucker representation, projection matrix P i can be transformed into a 3 \u00d7 6 matrixP i such that li P iL , whereP\ni = \u23a1 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a3 P 2 \u2227 P 3 P 3 \u2227 P 1 P 1 \u2227 P 2 \u23a4 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a6 ,(9)\nand P k represents the kth row of projection matrix P i . Subsequently, Equation (7) becomes the following equation, which is linear inL:\nx T iP iL = 0.(10)\nBy stacking Equation (10) from five frames,L can be estimated by least squares. Finally, each moving 3D point on the lineL can be found by the intersecting ray from each frame with the lin\u1ebd L [5]. Instead of assuming that the object is moving along a line, Shashua et al. [144] assumed that the object is moving over a conic section. Nine views are required to get a unique solution, although seven views are adequate if the type of conic is known, such as a circle in 3D Euclidean space. They solved the nonlinear optimization problem by fitting a random conic to the moving points in 2D space or by minimizing the error of estimated conic radius in 3D such that the a priori constraint can be enforced. Based on previous works, Kaminski and Teicher [69,70] generalized trajectory triangulation by representing a curve as a family of hypersurfaces in the projective space. This polynomial representation transforms the nonlinear trajectory problem into a linear problem in the unknown parameters. On the other hand, to handle missing data, Park et al. [124] represented a 3D trajectory as a linear combination of trajectory basis vectors such that the recovery of 3D points can be estimated robustly using least squares. They also proposed reconstructability criteria by analyzing the relationship among ego-motion, point motion, and trajectory basis vectors. Since reconstructability is inversely proportional to 3D reconstruction error, this criterion allows precise inspection of the possibility of accurate reconstruction [125].\n\n\nParticle\n\nFilter. Due to the observability issue (the distance between the observer and the target cannot be observed), the problem of tracking moving objects in 3D using monocular cameras can be seen as the Bearing-only-Tracking (BOT) problem. A monocular camera can be viewed as a BOT sensor since it can only provide bearing information of the tracked feature points (e.g., the angle between observed features in the previous and the current frame with respect to camera center) on the moving object. A filter-based approach is preferable for the BOT problem since it can model the uncertainty of the position and velocity of the observer and the target and has been studied widely as a target motion analysis problem [1,16].\n\nKundu et al. [83] employed particle filters to estimate the position and velocity of the moving objects. Instantaneous constant velocity motion model and Lie algebra are used to model the unknown motion and parameterize the rigid transformation of the objects, respectively. In initialization, the moving object is segmented by geometric constraints and Flow Vector Bound (FVB) as in [84] and [82] and the particles are spread uniformly along the ray of projection. An estimated ground plane from the 3D point cloud of the static scene and the maximum allowed depth value are leveraged to constrain the space of the particles (see Figure 6(b)). For importance sampling, the weight of the particle is updated by projecting each particle into the current frame and computing the projection error compared to the actual feature position. As particles with lower error or higher weight have a higher probability to be resampled, they concentrate on the depth value that gives the smallest reprojection error.\n\n\nJOINT MOTION SEGMENTATION AND RECONSTRUCTION\n\nInstead of performing multibody motion segmentation and reconstructing the 3D structure of dynamic objects as a separate and sequential task, factorization can do both simultaneously. Given the feature correspondences, dynamic object segmentation and reconstruction produce the motion of the segmented features as well as their 3D structures. Figure 7 describes the flow of this joint motion segmentation and reconstruction task. Although factorization can produce both segmented objects and their 3D structures, generally, the output from applications A (OA) and B (OB) can be combined to have a similar result as this technique.\n\n\nFactorization\n\nFactorization is probably one of the most prominent techniques in SfM. It has an elegant mathematical formulation and can solve the problem of segmentation and reconstruction simultaneously. It was first formulated by Tomasi and Kanade [154] based on the rank theorem in 1992. The theorem states that in short sequences of static scenes, a measurement matrix, a matrix containing all tracked feature points through all frames, is at most of rank four (or rank three if using the orthographic projection model under Euclidean coordinates) [24].\n\nSpecifically, let W \u2208 IR 2f \u00d7p be a measurement matrix where f is the number of frames and p is the number feature points. W can be factorized into motion matrix M \u2208 IR 2f \u00d74 and shape matrix S \u2208 IR 4\u00d7p such that W = MS \n\u23a1 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a3 u 11 \u00b7 \u00b7 \u00b7 u 1p . . . . . . . . . u f 1 \u00b7 \u00b7 \u00b7 u f p v 11 \u00b7 \u00b7 \u00b7 v 1p . . . . . . . . . v f 1 \u00b7 \u00b7 \u00b7 v f p \u23a4 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a6 = \u23a1 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a3 i T 1 t x 1 . . . . . . i T f t x f j T 1 t y 1 . . . . . . j T f t y f \u23a4 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a6 S 1 , . . . , S p ,(11)\nwhere (u f p , v f p ) are the position of feature points in the image space; i T f and j T f are the first and the second row of rotation matrix R \u2208 SO (3), respectively; and (t x f , t y f ) are the coordinates of the translation vector in x and y directions. Exploiting the rank constraint, W can be decomposed using Singular Value Decomposition (SVD) such that\nW = U \u03a3 V T ,(12)\nwhere \u03a3 \u2208 IR 4\u00d74 is a diagonal matrix containing the four biggest eigenvalues and U \u2208 IR 2f \u00d74 and V \u2208 IR p\u00d74 are eigenvectors corresponding to the four biggest eigenvalues. Subsequently, both motion and shape matrices are estimated asM \u2261 U \u03a3 1/2 and\u015c \u2261 \u03a3 1/2 V T . Since the decomposition in Equation (12) is not unique, the exact value of M and S should be computed by finding matrix A such that W = MS = (MA)(A \u22121\u015c ). Matrix A can be found by enforcing rotation and translation constraints and solving the resulting linear equation through least squares [25,154]. This basic formulation of a static scene can be used to reconstruct a moving object in front of the camera as long as the scene is static. Nonetheless, it can also be extended to multibody formulation for a moving camera depending on the camera model (orthography, affine, or perspective) or the type of motion (rigid or nonrigid). If the scene contains n motions, then the columns of measurement matrix can be sorted such that\nW = W \u0393 = [W 1 , . . . ,W n ],(13)\nwhere \u0393 \u2208 IR p\u00d7p is an unknown permutation matrix. Without noise, each W i , where i = {1, 2, . . . , n}, lies in a subspace of at most rank four [25]. Then, as each W i can be factorized into a  motion and shape matrix,W has a canonical form as follows:\nW =MS = [ M 1 , . . . , M n ] \u23a1 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a3 S 1 . . . S n \u23a4 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a6 .(14)\nThe problem of motion segmentation and reconstruction is then transformed into the problem of finding the correct permutation matrix \u0393 in Equation (13) such that matrixS has block diagonal. In general, the techniques addressing this problem can be divided based on the motion types, namely, rigid and nonrigid motion.  [25] introduced the shape interaction matrix, a mathematical construct of object shapes that is invariant to object motions and coordinate systems selection. This shape interaction matrix was found to be preserving the original subspace structure. LetW = U \u03a3V T be rank-r SVD decomposition of measurement matrix such that U \u2208 IR 2f \u00d7r , \u03a3 \u2208 IR r \u00d7r , andV \u2208 IR p\u00d7r . The shape interaction matrix Q is defined as follows:\nQ = VV T \u2208 IR p\u00d7p .(15)\nEquation (15) has an interesting property that the entry is zero if feature trajectory a and b belong to different objects. This property has been proved mathematically by Kanatani [72] as well.\n\nBased on this observation, motion segmentation and reconstruction can be done by sorting and thresholding the entries of Q. Costeira and Kanade [25] cluster the structure by maximizing the sum-of-squares entries of a block diagonal subject to the constraint that each block represents a physical object. Ichimura [65] used a discriminant criterion [120] to separate the sorted rows of Q into different motions that maximize separation among subspaces. On the other hand, instead of clustering the subspace through SVD, Gear [44] showed that echelon canonical form provides direct information on the grouping of points to the subspaces.\n\nFor factorization using projective cameras, the problem is trickier since factorization cannot be done without first recovering an unknown scale factor \u03bb \u2208 IR called projective depth. Let {X j \u2208 IP 3 } p j=1 and {x i j \u2208 IP 2 } j=1, ...,p i=1, ...,f be a set of p 3D points and p feature points in f frames, both represented in homogeneous coordinates. If {P i \u2208 IR 3\u00d74 } f i=1 are a set of projection matrices that map all 3D points to feature points for every frame i, then the image projection equation is calculated as \u03bb i j x i j = P i X j . The complete image projection matrices can be written as follows:\nW = \u23a1 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a3 \u03bb 11 x 11 \u00b7 \u00b7 \u00b7 \u03bb 1p x 1p . . . . . . . . . \u03bb f 1 x f 1 \u00b7 \u00b7 \u00b7 \u03bb f p x f p \u23a4 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a6 = \u23a1 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a3 P 1 . . . P f \u23a4 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a6 X 1 , . . . , X p .(16)\nSturm and Trigss [151] recover the projective depths in Equation (16) based on the computation of fundamental matrices and epipoles. By choosing an arbitrary initial depth value (such as \u03bb 1p = 1), the overall projective depth can be recovered up to an arbitrary initial value using the following equation:\n\u03bb mp = (e mn \u2227 x mp ).(F mn x np ) e mn \u2227 x mp \u03bb np ,(17)\nwhere m, n \u2208 {1, 2, . . . , f }, and F mn and e mn are the fundamental matrix and the epipole computed from frame m and n, respectively. Once the projective depth is obtained, shape and motion can be recovered using SVD. Hartley and Schaffalitzky [58] generalized the factorization based on a perspective camera for missing and uncertain data. They developed an iterative method based on power factorization to approximate data with missing entries with a low-rank matrix. Li et al. [93] iterate between motion segmentation using subspace separation and projective depth estimation in order to get a convergence result. The projective depth is estimated by minimizing the reprojection errors, followed by iterative refinement. On the contrary, Murakami et al. [114] tried to avoid the computation of projective depth by formulating depth-estimation-free conditions. The computation of Equation (17) is unnecessary if two conditions are met. First, the origins of the camera coordinates are on a plane. Second, the axes of the coordinate systems point to the perpendicular direction of the plane.  3\u00d7p and l i \u2208 IR. By normalizing the feature points as in [154] and eliminating the translation vector, the measurement matrix becomes\nB = k i=1 l i .B i , where B, B i \u2208 IRW = N B = \u23a1 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a3 l 11 R 1 \u00b7 \u00b7 \u00b7 l 1k R 1 . . . . . . . . . l f 1 R f \u00b7 \u00b7 \u00b7 l f k R f \u23a4 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a6 . \u23a1 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a3 B 1 . . . B k \u23a4 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a6 ,(18)\nwhere R is the first two rows of rotation matrix R (due to the orthogonal projection of the orthographic camera model, the last row of R can be estimated by computing the cross-product of the first and the second row of R). The factorization ofW can be done using SVD by choosing the first 3k singular vectors and singular values. The estimated rotation matrix R f and the shape basis weights l i can be recovered from N by reordering the entries of N and factorizing it using SVD. Finally, orthonomality constraints are enforced such that there is a matrix G that maps R f and B k into a unique solution [13].\n\nAs an alternative to orthonomality constraints, Xiao et al. [176] introduce basis constraints such that the nonrigid factorization problem can be solved in a closed-form solution. Instead of enforcing the metric constraints directly, Paladini et al. [123] project the motion matrix onto the manifold of matrix constraints and thus the factorization can be done iteratively through least squares. In contrast, Akhter et al. [3] propose a dual solution by presenting a trajectory-space-based technique such that the computation of basis vectors is not needed. The Discrete Cosine Transform (DCT) is used to compactly describe the body motions. Following these results, Dai et al. [27] tried to remove any additional constraints for nonrigid reconstruction (e.g., information about the nonrigid scene, nonrigid shape basis, the coefficients, the deformations, the shapes, etc.) by proposing a prior-free method using low-rank constraints only. Recently, Kumar et al. [80] proposed to fuse together MBSfM and NRSfM into a multibody nonrigid deformations system. They modeled the feature trajectories as a union of multiple linear or affine subspaces. It enables one to jointly optimize nonrigid reconstruction and nonrigid motion segmentation using the alternating direction method of multipliers (ADMM). Despite the recent advances in visual SLAM and Structure from Motion in dynamic environments, each proposed approach comes with advantages and disadvantages. Tables 1, 2, and 3 list the summary of approaches for motion segmentation, dynamic object segmentation and 3D tracking, and joint motion segmentation and reconstruction, respectively. We define some practical considerations for each class and point out whether the proposed method has considered these practical aspects. The table is populated to the best of our knowledge given the information provided in the article.\n\n\nMotion Segmentation\n\nThe main advantage of background/foreground initialization is the ability to keep track of the moving objects when they are temporarily stationary. This capability enables the system to easily retrack the moving object without the need to perform a new segmentation when the temporarily stopped object starts to move again. Moreover, thanks to the tracking-by-detection scheme, this approach will have no problem in dealing with degenerate motion (e.g., the object moves along the epipolar plane and its direction and velocity are similar to the camera). However, this approach has two main drawbacks. First, information related to the background or the object needs to be defined beforehand. Second, this tracking-by-detection scheme may hinder the real-time capability, especially when the environment contains many moving objects since it needs to exhaustively match all objects with the detector except when a cascade architecture is carefully implemented [68].\n\nCompared to background and foreground initialization, geometric constraints do not possess the capability to handle temporary stopping since the determination of segmentation is based on the motion only, which is indicated by the high geometric error. Another shortcoming is that this approach cannot differentiate between the residual error caused by the moving object or caused by the false correspondence (outliers) since both conditions result in high geometric errors. Furthermore, some techniques cannot handle motion in degenerate conditions unless other measures are imposed. Kundu et al. [84] set a fixed threshold on the flow vector such that the degenerate motion lies outside the bound. Although this approach works well for particular motions, it is not applicable in general conditions since arbitrary object motion may violate the threshold. However, a geometric constraints-based approach needs no prior knowledge about the background or the moving objects. Moreover, since all computations of the residual errors are part of the standard visual SLAM or SfM technique, there is no additional computational burden in performing the segmentation, and thus real-time implementation is common.\n\nOptical flow-based techniques have similar properties with geometric-based approaches. They need no prior knowledge about the environment and can work in real time. However, they work based on the brightness constancy assumption, which is sensitive to changes in lighting conditions [62]. Without proper implementation of the image pyramid, it is also sensitive to a large pixel movement [11]. Moreover, it has difficulty in handling degenerate motion since when the object is moving at the same plane, direction, and velocity with the camera, the flow vector will be small and the moving object looks like a part of static background. New segmentation is also needed when the object starts to move after a stop.\n\nEgo-motion constraints can easily segment static features from dynamic ones by fitting features that conform with the defined ego-motion. This approach can run in real time. It also can handle degenerate motion since it relies on external information and thus only static features will conform with the correct ego-motion. However, it needs prior knowledge about the motion of the camera. Since it fits directly features that satisfy the ego-motion, when the object is temporary stopping, it will be viewed as part of the static scene.\n\n\nDynamic Object Segmentation\n\nThere are a few advantages of the statistical model selection technique for dynamic object segmentation. First, it can handle degenerate motion as long as the system allows the computation of lower-dimensional or lower-degree-of-freedom motion. Second, it needs no prior knowledge about the environment (the number of moving objects is automatically captured when the whole data is described by n different motion models). Third, since statistic-based approaches fit a model based on the cardinality of the inlier set, noise and outliers are automatically tackled. Nevertheless, by explicitly estimating the scale of noise, model fitting can have better performance [139]. Finally, statistical model selection can be implemented as a sequential algorithm that processes one new image at a time, although real-time implementation remains difficult.\n\nThe main problem of statistical model selection is that fitting a motion model from randomly sampled data is computationally expensive. Under RANSAC, the number of iterations required to guarantee a correct solution is\nN = lo\u0434(1 \u2212 p) lo\u0434(1 \u2212 (1 \u2212 \u03b5) s ) ,\nwhere s is the number of data points, \u03b5 is the percentage of outliers, and p is the probability of success (confidence level) [39]. If we assume that a motion lies in 20% of the whole data and we want a 99% probability of success, then it needs 359,777 iterations for computing the fundamental matrix to fit the motion correctly (fundamental matrix needs minimal 7 points) [139]. One of the most effective ways to reduce the number of iterations is by using a motion model with lower minimal point requirements such as in [132] and [133]. However, this approach needs an assumption of how the camera moves over time. Finally, dependent motion remains a challenging problem for statistical model selection since a group of features can be part of two different motion models. Incorporating overlapping motions in the joint likelihood function [139] can tackle the problem, although it remains difficult in the presence of outliers. Compared to statistical model selection, most subspace clustering methods are relatively cheaper in computation time because they are mostly based on an algebraic method (particularly SVD, which needs O ( f p 2 ) operations, where f is the number of frames and p is the number of points). Furthermore, some recent techniques allow intersection between subspaces, thus allowing it to deal with dependent motions [35,128,165]. However, current developments of subspace clustering have several limitations. First, they cannot run sequentially (except [161,185]) or in real time since they need the whole sequence to be available before processing (batch mode). Second, some methods need the information about the number of motions in the scene or the dimension where the subspace lies, although a recent technique provides a means to find it [34,177,179]. Third, most approaches make use of the affine camera model, which will fail if the scene contains a major perspective effect. Using the affine camera model, the motion is assumed to lie in a linear or affine manifold. Under perspective projection, the problem becomes more difficult since a motion might lie in a nonlinear manifold. Fourth, although recent techniques specify how to handle noise [35,177], outliers [35,185], and missing data [128,179], they only work to some extent and a practical implementation to long sequences remains difficult.\n\nUnlike subspace clustering techniques that can only segment data in a linear or affine subspace, the geometry-based approach works under the perspective camera model; thus, it can handle data that lies in a nonlinear manifold. However, since the current approach extends standard multiple-view geometry to static scenes, it only supports the fundamental matrix as the motion model, which means that there is no way to handle degenerate motion. Another problem is that the number of image pairs needed for computing a multibody fundamental matrix grows exponentially with respect to the number of motions (e.g., for n = 1, 2, 3, 4, the required number of images is 8, 35, 99, and 225). For large motions, the number of images can reach O (n 4 ) [166], an effect of transforming multibody epipolar constraints into a linear representation. Finally, the effect of noise, outliers, and missing data have not been well studied.\n\n\n3D Tracking of Dynamic Objects\n\nOne advantage of trajectory triangulation is that it can work incrementally, although it might need several frames for each iteration (5, 9, etc., depending on the trajectory assumption [6]). Prior knowledge about the camera motion is not needed, although some approaches [5,6] assume that the camera pose is available. The main limitation of trajectory triangulation is that the object trajectory should be known or at least should follow a specific parametric form. This assumption limits the application of trajectory triangulation for arbitrary object motion, although some researchers attempt to extend it into general motion [69,70,124]. Moreover, it remains difficult to handle outliers and missing data because it needs several image sequences in order to have a unique solution. Finally, most techniques can only reconstruct rigid body motion (except [124,125]), which limits the application into a specific problem.\n\nParticle filters are probably the only technique for doing 3D reconstruction and tracking of dynamic objects that can work in real time so far, although they are strictly limited to a small number of moving objects (computationally expensive for many objects) [83]. Knowledge about the object trajectory is not needed, although the assumption of the object velocity is required since it is used for the prediction. Some constraints also need to be enforced to limit the spread of the particles in the space. Moreover, it is probably difficult to extend it into nonrigid or articulated reconstruction since nonrigid and articulated motion may not conform to the constant velocity motion model.\n\n\nJoint Motion Segmentation and Reconstruction\n\nThe main advantage of the factorization-based approach is that the problem of motion segmentation and reconstruction can be solved simultaneously. Knowledge about the camera motion is not needed and it can be extended elegantly into nonrigid reconstruction. However, factorization has some limitations. First, most approaches work based on orthography or the affine camera model, which prevents its implementation in conditions with a large perspective effect, a condition often found in exploratory tasks. Reconstruction with a large perspective effect is still possible, although it remains a challenging problem since projective depth should be recovered first [114,151]. Second, it cannot run in real time (or even incrementally, except [106]) because all feature point trajectories should be available beforehand (batch mode). Additionally, most approaches derive their technique based on SVD, which needs O ( f p 2 ) complexity. Third, some techniques may need prior knowledge, such as the number of moving objects in the scene, rank of the measurement matrix, or the dimension of the object [25,58,80]. Fourth, it is sensitive to noise and outliers since the segmentation and the reconstruction are generally based on thresholding on the entries of the interaction matrix. Finally, missing data is also a problem since the entries of the measurement matrix should be complete before doing SVD.\n\n\nDeep Learning\n\nThe key advantages of the deep-learning-based approach is that it can eliminate the handengineered feature extraction step [88], which results in the reduction of problems in feature correspondences such as noisy correspondences, outliers, and missing data due to losing track or occlusions. Deep learning also does not need to specify the camera model, which currently limits approaches like subspace clustering or factorization to be applied for the general perspective camera model. Moreover, the ability to learn the nonlinear representation of the data gives an opportunity to generalize well in different environments, a problem that remains difficult to handle using standard feature-based approaches that typically manually fine-tune the algorithm parameters for different environments.\n\nHowever, since techniques in dynamic object segmentation and reconstruction involve some geometry computations, it remains a challenge to construct a DNN architecture that can understand this geometry and gain competitive accuracy compared to standard feature-based techniques. Current approaches such as [96] still need the help of conventional feature extraction techniques since the extracted spatiotemporal feature is not precise and does not understand the geometry of the moving objects. The approaches in [168] and [15] show that training the network to segment motions can be done, although a certain number of motions in the image are required. It also needs camera intrinsic parameters in order to predict depth. Moreover, the technique is developed based on the optical flow principle, yet optical flow has difficulty detecting degenerate object motion.\n\n\nCONCLUSIONS\n\nSignificant progress has been made in the past few decades to solve the problem of visual simultaneous localization and reconstruction in dynamic environments. This article surveys and highlights existing approaches and connects the field of SfM and visual SLAM with dynamic object segmentation and tracking. We have classified approaches according to the type of problem they solve and their corresponding applications. Various approaches, both feature based and deep learning based, are presented and critically discussed from a practical perspective.\n\nFurther research is needed to enable practical implementations of simultaneous localization and reconstruction in dynamic environments. In general, handling missing, noisy, and outlier data remains a future challenge for most of the discussed techniques. Although statistical-based techniques can tackle this problem due to their recursive sampling approach, they have to trade off accuracy for computation cost. Most techniques also have difficulty in dealing with degenerate and dependent motion. While some subspace clustering techniques allow the intersection among motions, it is still limited to the case of noiseless data. Moreover, real-time implementation remains a difficult problem for dynamic object segmentation and 3D tracking due to the offline nature of the algorithms and their high computational cost. In order for dynamic object segmentation and 3D tracking techniques to be fused with standard visual SLAM, this real-time problem should be solved first. Finally, the deep-learning-based approach opens a new perspective by casting the localization and 3D reconstruction problem as a learning problem and eliminating the hand-crafted feature engineering step and the need to specify the camera model. Deep learning approaches, however, are still in their infancy, and the area presents a plethora of interesting challenges for future work.\n\nFigure 1\n1depicts how each approach connects to others and forms a full pipeline of visual SLAM in dynamic environments. The pipeline consists of three main applications: (A) robust visual SLAM (input: feature correspondences/image sequences, output: 3D point cloud of static world), (B) Dynamic Object Segmentation and 3D Tracking (input: feature correspondences/image sequences, output: 3D trajectory of each object), and (C) Joint Motion Segmentation and Reconstruction (input: feature correspondences, output: 3D point cloud of static features and dynamic features).\n\nFig. 1 .\n1High-level diagram describing the pipeline of visual localization and 3D reconstruction in dynamic environments. Rounded rectangles indicate an action module (approach category), solid arrows denote data transfer, and dashed arrows reflect an optional input. Some actions have input from both feature correspondences and image sequences since the corresponding techniques consist of feature-based and deep-learningbased approaches.\n\nFig. 2 .\n2The flow diagram of the first application, robust visual SLAM. Solid rounded rectangles indicate an action and dashed rounded rectangles show existing approaches for a specific action module. A square box shows the output of a particular module. Solid arrows denote data transfer and dashed arrows reflect an optional input. The table on the right side shows the list of relevant literature references for each approach.\n\nFig. 3 .\n3(a) In static scenes, the transformation of image point from x 1 to x 2 is defined by epipolar constraintx T 2 Fx 1 = 0. (b)The violation of geometric constraints in a dynamic environment: (1) the tracked feature lies too far from the epipolar line, (2) back-projected rays from the tracked features do not meet, (3) faulty fundamental matrix estimation when dynamic feature is included in pose estimation, (4) high distance between reprojected feature and the observed feature.\n\na\nCamera Type (T): Monocular (M), Stereo (S). Camera State (S): Static (S), Moving (M). Camera Model (M): Orthography (O), Affine (A), Perspective (P). b CT: Computation Time (RT: Real time, NT: Near real time, NB: Need to be batched, FO: Fully offline), OT: Handle outliers due to false feature correspondences (I: irrelevant for the technique), NP: No prior knowledge (e.g., background/ foreground information, camera motion), TS: Supports temporary stopping (ability to keep track of the dynamic objects when they are temporarily stationary), TU: Takes into account uncertainty, OH: Occlusion handling, DM: Supports degenerate motion for the moving objects.\n\nFig. 4 .\n4The flow diagram of the second application, dynamic object segmentation, and 3D tracking. Solid rounded rectangles indicate an action and dashed rounded rectangles show existing approaches for a specific action module. A square box shows the output of a particular module. Solid arrows denote data transfer, and dashed arrows reflect an optional input. The table on the right side shows the list of relevant literature references for each approach.\n\nFig. 5 .\n5Illustration of (a) subspace clustering and (b) statistical model selection technique for dynamic object segmentation.\n\na\nCamera Type (T): Monocular (M), Stereo (S), Depth (D). Camera State (ST): Static (S), Moving (M). Camera Model (M): Orthography (O), Affine (A), Perspective (P). Camera Sequences (SQ): Short (S, f < 11), Medium (M, 10 < f < 501), Long (L, f > 500), where f is the number of images. b Number of Motions (N): Single (S), Multiple (M). Motion Type (T): Rigid (R), Nonrigid (N), Articulated (A). c AC: Accuracy defined by the percentage of segmentation errors. s: Evaluated on synthetic data, r: Evaluated on real data, h: Evaluated on Hopkins 155 dataset [159]. Only the results from three motion sequences or the overall mean are displayed. d CT: Computation Time (RT: Real time, NT: Near real time, NB: Need to be batched, FO: Fully offline), SO: Support sequential operation, NP: No prior knowledge (e.g., number and dimension of the moving objects), ND: Handle noise in data, OT: Handle outliers due to false feature correspondences (I: irrelevant for the technique), MD: Handle missing data (e.g., due to occlusion, lost tracks, motion blur), DP: Support dependent motion, DG: Handle degenerate motion.\n\nFig. 6 .\n6(a) Illustration of a moving point along the line L and the projection of the point and the line to the image plane i by the projection matrix P i . (b) Illustration of particle filter technique for tracking dynamic object. The particles are spread along the ray of projection and are constrained by the estimated/predefined ground plane and maximum/minimum allowed depth value.\n\nFig. 7 .\n7The flow diagram of the third application, joint motion segmentation and reconstruction. Solid rounded rectangles indicate an action and dashed rounded rectangles show existing approaches for a specific action module. A square box shows the output of a particular module. Solid arrows denote data transfer, and dashed arrows reflect an optional input. The table at the bottom shows the list of relevant literature references for each approach.\n\na\nCamera Type (T): Monocular (M), Stereo (S), RGB-D (R). Camera State (ST): Static (S), Moving (M). Camera Model (M): Orthography (O), Affine (A), Perspective (P). Camera Sequences (SQ): Short (S, f < 11), Medium (M, 10 < f < 501), Long (L, f > 500), where f is the number of images. b Number of Moving Objects (N): Single (S), Multiple (M). Motion Type (T): Rigid (R), Nonrigid (N), Articulated (A). c CT: Computation Time (RT: Real time, NT: Near real time, NB: Need to be batched, FO: Fully offline), SO: Supports sequential operation, CK: No knowledge about the camera motion (e.g., trajectory, velocity), OK:No knowledge about the moving objects (e.g., number, dimension, rank, trajectory), ND: Handles noise in data (e.g., Gaussian noise), OT: Handles outliers (e.g., due to false correspondence), MD: Handles missing data (e.g., due to occlusion, lost tracks, motion blur).\n\n5.1. 1\n1Multibody Structure from Motion (MBSfM). Multibody Structure from Motion (MBSfM) generalizes standard SfM for a rigid camera motion into n bodies of rigid motions. To solve the MBSfM problem, under the affine camera model, Costeira and Kanade\n\n\n5.1.2 Nonrigid Structure from Motion(NRSfM). In 2000, Bregler et al.[13] proposed Nonrigid Structure from Motion (NRSfM) technique based on Tomasi-Kanade factorization under a scaled orthography camera model for the first time. They represented a nonrigid object as a k key frame basis set {B i } k i=1 , where each B i denotes a 3 \u00d7 p matrix describing p feature points. The linear combination of this basis set forms the shape of a specific configuration such that\n\nTable 1 .\n1Summary of Existing Approaches for Motion SegmentationCamera a \nPractical Consideration b \nAuthor(s) \nSLAM \nT \nS \nM \nCT \nOT \nNP \nTS \nTU \nOH \nDM \nBackground/Foreground Initialization (Section 3.1.1) \nWang et al. [172] \nFilter \nM \nM \nP \nNT \n-\n-\n-\n-\nZhang et al. [184] \nS f M \nM \nM \nP \nN B \n-\n-\n-\nLee et al. [89] \nS f M \nM \nM \nP \nN T \n-\n-\n-\nChhaya et al. [21] \nS f M \nM \nM \nP \nN T \n-\n-\n-\nLee et al. [90] \nS f M \nM \nM \nP \nN T \n-\n-\n-\nWangsiripitak et al. [173] \nFilter \nM \nM \nP \nRT \n-\n-\nGeometric Constraints (Section 3.1.2) \nLin et al. [95] \nFilter \nS \nM \nP \nRT \n-\n-\n-\n-\nMigliore et al. [104] \nFilter \nM \nM \nP \nRT \n-\n-\n-\nZou et al. [28] \nS f M \nM \nM \nP \nR T \n-\n-\n-\nTan et al. [152] \nS f M \nM \nM \nP \nR T \n-\n-\n-\nKundu et al. [84] \nS f M \nM \nM \nP \nR T \n-\n-\nOptical Flow (Section 3.1.3) \nAlcantarilla et al. [4] \nS f M \nS \nM \nP \nR T \n-\n-\n-\n-\nKlappstein et al. [74] \nS f M \nM , S \nM \nP \nN T \n-\n-\n-\n-\nDerome et al. \n\nTable 2 .\n2Summary of Existing Approaches on Dynamic Object Segmentation\n\nTable 3 .\n3Summary of Existing Approaches on 3D Tracking of Dynamic Objects and Joint Motion Segmentation and Reconstruction\nACM Computing Surveys, Vol. 51, No. 2, Article 37. Publication date: February 2018.\nReceived August 2017; revised December 2017; accepted December 2017 ACM Computing Surveys, Vol. 51, No. 2, Article 37. Publication date: February 2018.\nACKNOWLEDGMENTSThe authors would like to thank anonymous reviewers for their helpful suggestions towards improving our paper. Muhamad Risqi U. Saputra was supported by the Indonesia Endowment Fund for Education (LPDP).\nUtilization of modified polar coordinates for bearings-only tracking. J Vincent, Sherry E Aidala, Hammel, IEEE Trans. Automat. Contr. 28Vincent J. Aidala and Sherry E. Hammel. 1983. Utilization of modified polar coordinates for bearings-only tracking. IEEE Trans. Automat. Contr. 28, 3 (1983), 283-294.\n\nInformation theory and an extension of the maximum likelihood principle. Hirotogu Akaike, Int. Symp. Inf. Theory. Hirotogu Akaike. 1973. Information theory and an extension of the maximum likelihood principle. In Int. Symp. Inf. Theory. 267-281.\n\nNonrigid structure from motion in trajectory space. Ijaz Akhter, Sohaib Khan, Yaser Sheikh, Takeo Kanade, In Adv. Neural Inf. Process. Syst. 1Ijaz Akhter, Sohaib Khan, Yaser Sheikh, and Takeo Kanade. 2008. Nonrigid structure from motion in trajectory space. In Adv. Neural Inf. Process. Syst., Vol. 1. 1-8.\n\nOn combining visual slam and dense scene flow to increase the robustness of localization and mapping in dynamic environments. Pablo F Alcantarilla, Jos\u00e9 J Yebes, Javier Almaz\u00e1n, Luis M Bergasa, IEEE Int. Conf. Robot. Autom. Pablo F. Alcantarilla, Jos\u00e9 J. Yebes, Javier Almaz\u00e1n, and Luis M. Bergasa. 2012. On combining visual slam and dense scene flow to increase the robustness of localization and mapping in dynamic environments. In IEEE Int. Conf. Robot. Autom. 1290-1297.\n\nTrajectory triangulation of lines: Reconstruction of a 3D point moving along a line from a monocular image sequence. Shai Avidan, Amnon Shashua, IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. 266Shai Avidan and Amnon Shashua. 1999. Trajectory triangulation of lines: Reconstruction of a 3D point moving along a line from a monocular image sequence. In IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit., Vol. 2. 66.\n\nTrajectory triangulation: 3D reconstruction of moving points from a monocular image sequence. Shai Avidan, Amnon Shashua, IEEE Trans. Pattern Anal. Mach. Intell. 22Shai Avidan and Amnon Shashua. 2000. Trajectory triangulation: 3D reconstruction of moving points from a monoc- ular image sequence. IEEE Trans. Pattern Anal. Mach. Intell. 22, 4 (2000), 348-357.\n\nA deep convolutional neural network for background subtraction. Mohammadreza Babaee, Gerhard Duc Tung Dinh, Rigoll, arXiv:1702.01731Mohammadreza Babaee, Duc Tung Dinh, and Gerhard Rigoll. 2017. A deep convolutional neural network for back- ground subtraction. In arXiv:1702.01731.\n\nSpeeded-up robust features (SURF). Herbert Bay, Andreas Ess, Tinne Tuytelaars, Luc Van Gool, Comput. Vis. Image Underst. 110Herbert Bay, Andreas Ess, Tinne Tuytelaars, and Luc Van Gool. 2008. Speeded-up robust features (SURF). Comput. Vis. Image Underst. 110, 3 (2008), 346-359.\n\nNavigation using affine structure from motion. Paul A Beardsley, Andrew Zisserman, David W Murray, Eur. Conf. Comput. Vis. Paul A. Beardsley, Andrew Zisserman, and David W. Murray. 1994. Navigation using affine structure from motion. In Eur. Conf. Comput. Vis. 85-96.\n\nVisual navigation for mobile robots: A survey. Francisco Bonin-Font, Alberto Ortiz, Gabriel Oliver, Francisco Bonin-Font, Alberto , Ortiz Gabriel, J. Intell. Robot. Syst. 53Francisco Bonin-Font, Alberto Ortiz, Gabriel Oliver, Francisco Bonin-font Alberto, and Ortiz Gabriel. 2008. Visual navigation for mobile robots: A survey. J. Intell. Robot. Syst. 53 (2008), 263-296.\n\nPyramidal implementation of the affine Lucas Kanade feature tracker -Description of the algorithm. Jean-Yves Bouguet, Intel Corp. Microprocess. Res. Labs. Jean-Yves Bouguet. 2000. Pyramidal implementation of the affine Lucas Kanade feature tracker -Description of the algorithm. Intel Corp. Microprocess. Res. Labs.\n\nFactorization-based segmentation of motions. Terrance E Boult, Lisa Gottesfeld Brown, IEEE Work. Vis. Motion. Terrance E. Boult and Lisa Gottesfeld Brown. 1991. Factorization-based segmentation of motions. In IEEE Work. Vis. Motion.\n\nRecovering non-rigid 3D shape from image streams. Christoph Bregler, Aaron Herzmann, Henning Biermann, IEEE Conf. Comput. Vis. Pattern Recognit. Christoph Bregler, Aaron Herzmann, and Henning Biermann. 2000. Recovering non-rigid 3D shape from image streams. In IEEE Conf. Comput. Vis. Pattern Recognit.\n\nOnline multiperson tracking-by-detection from a single, uncalibrated camera. Michael D Breitenstein, Fabian Reichlin, Bastian Leibe, Esther Koller-Meier, Luc Van Gool, IEEE Trans. Pattern Anal. Mach. Intell. 33Michael D. Breitenstein, Fabian Reichlin, Bastian Leibe, Esther Koller-Meier, and Luc Van Gool. 2011. Online multi- person tracking-by-detection from a single, uncalibrated camera. IEEE Trans. Pattern Anal. Mach. Intell. 33, 9 (2011), 1820-1833.\n\nSE3-Nets: Learning rigid body motion using deep neural networks. Arunkumar Byravan, Dieter Fox, In IEEE Int. Conf. Robot. Autom. Arunkumar Byravan and Dieter Fox. 2017. SE3-Nets: Learning rigid body motion using deep neural networks. In IEEE Int. Conf. Robot. Autom.\n\nBearings-only tracking for maneuvering sources. L E Jean-Pierre, Olivier Cadre, Tremois, IEEE Trans. Aerosp. Electron. Syst. 34Jean-pierre L. E. Cadre and Olivier Tremois. 1998. Bearings-only tracking for maneuvering sources. IEEE Trans. Aerosp. Electron. Syst. 34, 1 (1998), 179-193.\n\nBRIEF: Binary robust independent elementary features. Michael Calonder, Vincent Lepetit, Christoph Strecha, Pascal Fua, Eur. Conf. Comput. Vis. Michael Calonder, Vincent Lepetit, Christoph Strecha, and Pascal Fua. 2010. BRIEF: Binary robust independent elementary features. In Eur. Conf. Comput. Vis. 778-792.\n\nWide-area augmented reality using camera tracking and mapping in multiple regions. Robert O Castle, Georg Klein, David W Murray, Comput. Vis. Image Underst. 115Robert O. Castle, Georg Klein, and David W. Murray. 2011. Wide-area augmented reality using camera tracking and mapping in multiple regions. Comput. Vis. Image Underst. 115, 6 (2011), 854-867.\n\nOpportunistic sampling-based planning for active visual SLAM. M Stephen, Ayoung Chaves, Ryan M Kim, Eustice, IEEE/RSJ Int. Conf. Intell. Robot. Syst. Stephen M. Chaves, Ayoung Kim, and Ryan M. Eustice. 2014. Opportunistic sampling-based planning for active visual SLAM. In IEEE/RSJ Int. Conf. Intell. Robot. Syst.\n\nRobust subspace segmentation by low-rank representation. Jinhui Chen, Jian Yang, IEEE Trans. Cybern. 44Jinhui Chen and Jian Yang. 2014. Robust subspace segmentation by low-rank representation. IEEE Trans. Cybern. 44, 8 (2014), 1432-1445.\n\nMonocular reconstruction of vehicles: Combining SLAM with shape priors. Falak Chhaya, Dinesh Reddy, Sarthak Upadhyay, Visesh Chari, M Zia, K. Madhava Krishna, IEEE Int. Conf. Robot. Autom. Falak Chhaya, Dinesh Reddy, Sarthak Upadhyay, Visesh Chari, M. Zeeshan Zia, and K. Madhava Krishna. 2016. Monocular reconstruction of vehicles: Combining SLAM with shape priors. In IEEE Int. Conf. Robot. Autom. 5758-5765.\n\nMatching with PROSAC-Progressive Sample Consensus. Ondrej Chum, Jiri Matas, IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. Ondrej Chum and Jiri Matas. 2005. Matching with PROSAC-Progressive Sample Consensus. In IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. 220-226.\n\nHeadSLAM -Simultaneous localization and mapping with head-mounted inertial and laser range sensors. Burcu Cinaz, Holger Kenn, IEEE Int. Symp. Wearable Comput. Burcu Cinaz and Holger Kenn. 2008. HeadSLAM -Simultaneous localization and mapping with head-mounted in- ertial and laser range sensors. In IEEE Int. Symp. Wearable Comput.\n\nA multi-body factorization method for motion analysis. Joao Costeira, Takeo Kanade, Int. Conf. Comput. Vis. Joao Costeira and Takeo Kanade. 1995. A multi-body factorization method for motion analysis. In Int. Conf. Comput. Vis. 1071-1076.\n\nA multibody factorization method for independently moving objects. Paulo Jo\u00e3o, Takeo Costeira, Kanade, Int. J. Comput. Vis. 29Jo\u00e3o Paulo Costeira and Takeo Kanade. 1998. A multibody factorization method for independently moving objects. Int. J. Comput. Vis. 29, 3 (1998), 159-179.\n\nFAB-MAP: Probabilistic localization and mapping in the space of appearance. Mark Cummins, Paul Newman, Int. J. Rob. Res. 27Mark Cummins and Paul Newman. 2008. FAB-MAP: Probabilistic localization and mapping in the space of appear- ance. Int. J. Rob. Res. 27, 6 (2008), 647-665.\n\nA simple prior-free method for non-rigid structure-from-motion factorization. Yuchao Dai, Hongdong Li, Mingyi He, Int. J. Comput. Vis. 107Yuchao Dai, Hongdong Li, and Mingyi He. 2014. A simple prior-free method for non-rigid structure-from-motion factorization. Int. J. Comput. Vis. 107, 2 (2014), 101-122.\n\nCoSLAM: Collaborative visual SLAM in dynamic environments. Danping Zhou, Ping Tan, IEEE Trans. Pattern Anal. Mach. Intell. 35Danping Zhou and Ping Tan. 2012. CoSLAM: Collaborative visual SLAM in dynamic environments. IEEE Trans. Pattern Anal. Mach. Intell. 35, 2 (2012), 354-366.\n\nReal-time simultaneous localisation and mapping with a single camera. Andrew J Davison, IEEE Int. Conf. Comput. Vis. Andrew J. Davison. 2003. Real-time simultaneous localisation and mapping with a single camera. In IEEE Int. Conf. Comput. Vis.\n\nMoving object detection in realtime using stereo from a mobile platform. Maxime Derome, Aurelien Plyer, Martial Sanfourche, Guy Le Besnerais, Unmanned Syst. 3Maxime Derome, Aurelien Plyer, Martial Sanfourche, and Guy Le Besnerais. 2015. Moving object detection in real- time using stereo from a mobile platform. Unmanned Syst. 3, 4 (2015), 253-266.\n\nReal-time mobile object detection using stereo. Maxime Derome, Aurelien Plyer, Martial Sanfourche, Guy Le Besnerais, 13th Int. Conf. Control Autom. Robot. Vis. (ICARCV'14. Maxime Derome, Aurelien Plyer, Martial Sanfourche, and Guy Le Besnerais. 2014. Real-time mobile object detection using stereo. In 13th Int. Conf. Control Autom. Robot. Vis. (ICARCV'14). 1021-1026.\n\nDeep image homography estimation. Daniel Detone, Tomasz Malisiewicz, Andrew Rabinovich, arXiv:1606.03798Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. 2016. Deep image homography estimation. In arXiv:1606.03798.\n\nFlowNet: Learning optical flow with convolutional networks. Alexey Dosovitskiy, Philipp Fischery, Eddy Ilg, Philip Hausser, Caner Hazirbas, Vladimir Golkov, Patrick Van Der, Daniel Smagt, Thomas Cremers, Brox, IEEE Int. Conf. Comput. Vis. 11Alexey Dosovitskiy, Philipp Fischery, Eddy Ilg, Philip Hausser, Caner Hazirbas, Vladimir Golkov, Patrick Van Der Smagt, Daniel Cremers, and Thomas Brox. 2016. FlowNet: Learning optical flow with convolutional networks. In IEEE Int. Conf. Comput. Vis., Vol. 11-18-Dece. 2758-2766.\n\nSparse subspace clustering. Ehsan Elhamifar, Rene Vidal, IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. Ehsan Elhamifar and Rene Vidal. 2009. Sparse subspace clustering. In IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. Work. 2790-2797.\n\nSparse subspace clustering: Algorithm, theory, and applications. Ehsan Elhamifar, Rene Vidal, IEEE Trans. Pattern Anal. Mach. Intell. 35Ehsan Elhamifar and Rene Vidal. 2013. Sparse subspace clustering: Algorithm, theory, and applications. IEEE Trans. Pattern Anal. Mach. Intell. 35, 11 (2013), 2765-2781.\n\nLSD-SLAM: Direct monocular SLAM. Jakob Engel, Thomas Sch, Daniel Cremers, Eur. Conf. Comput. Vis. Jakob Engel, Thomas Sch, and Daniel Cremers. 2014. LSD-SLAM: Direct monocular SLAM. In Eur. Conf. Comput. Vis. 834-849.\n\nRandom sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography. A Martin, Robert C Fischler, Bolles, Commun. ACM. 24Martin A. Fischler and Robert C. Bolles. 1981. Random sample consensus: A paradigm for model fitting with appli- cations to image analysis and automated cartography. Commun. ACM 24 (1981), 381-395.\n\nLearning to segment moving objects in videos. Katerina Fragkiadaki, Pablo Arbelaez, Panna Felsen, Jitendra Malik, IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. Katerina Fragkiadaki, Pablo Arbelaez, Panna Felsen, and Jitendra Malik. 2015. Learning to segment moving objects in videos. In IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. 4083-4090.\n\nVisual odometry: Part II -matching, robustness, optimization, and applications. Friedrich Fraundorfer, Davide Scaramuzza, IEEE Robot. Autom. Mag. 19Friedrich Fraundorfer and Davide Scaramuzza. 2012. Visual odometry: Part II -matching, robustness, optimization, and applications. IEEE Robot. Autom. Mag. 19, 2 (2012), 78-90.\n\nVisual simultaneous localization and mapping: A survey. Jorge Fuentes-Pacheco, Jose Ruiz-Ascencio, Juan Manuel Rendon-Mancha, Artif. Intell. Rev. 43Jorge Fuentes-Pacheco, Jose Ruiz-Ascencio, and Juan Manuel Rendon-Mancha. 2012. Visual simultaneous localiza- tion and mapping: A survey. Artif. Intell. Rev. 43, 1 (2012), 55-81.\n\nBags of binary words for fast place recognition in image sequences. Dorian Galvez, - Lopez, Juan D Tardos, IEEE Trans. Robot. 28Dorian Galvez-Lopez and Juan D. Tardos. 2012. Bags of binary words for fast place recognition in image sequences. IEEE Trans. Robot. 28, 5 (2012), 1188-1197.\n\nComplete solution classification for the perspective-three-point problem. Xiao Shan Gao, Xiao Rong Hou, Jianliang Tang, Hang Fei Cheng, IEEE Trans. Pattern Anal. Mach. Intell. 25Xiao Shan Gao, Xiao Rong Hou, Jianliang Tang, and Hang Fei Cheng. 2003. Complete solution classification for the perspective-three-point problem. IEEE Trans. Pattern Anal. Mach. Intell. 25, 8 (2003), 930-943.\n\nVision-based topological mapping and localization methods: A survey. Emilio Garcia-Fidalgo, Alberto Ortiz, Rob. Auton. Syst. 64Emilio Garcia-Fidalgo and Alberto Ortiz. 2015. Vision-based topological mapping and localization methods: A sur- vey. Rob. Auton. Syst. 64 (2015), 1-20.\n\nMultibody grouping from motion images. C W Gear, Int. J. Comput. Vis. 29C. W. Gear. 1998. Multibody grouping from motion images. Int. J. Comput. Vis. 29, 2 (1998), 133-150.\n\nStereoScan: Dense 3D reconstruction in real-time. Andreas Geiger, Julius Ziegler, Christoph Stiller, In IEEE Intell. Veh. Symp. Andreas Geiger, Julius Ziegler, and Christoph Stiller. 2011. StereoScan: Dense 3D reconstruction in real-time. In IEEE Intell. Veh. Symp. 1-9.\n\nMulti-robot visual SLAM using a Rao-Blackwellized particle filter. Arturo Gil, Oscar Reinoso, Monica Ballesta, Miguel Julia, Rob. Auton. Syst. 58Arturo Gil, Oscar Reinoso, Monica Ballesta, and Miguel Julia. 2010. Multi-robot visual SLAM using a Rao- Blackwellized particle filter. Rob. Auton. Syst. 58, 1 (2010), 68-80.\n\nFinding action tubes. Georgia Gkioxari, Jitendra Malik, IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. Georgia Gkioxari and Jitendra Malik. 2015. Finding action tubes. In IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.\n\nDeep motion features for visual tracking. Susanna Gladh, Martin Danelljan, Michael Fahad Shahbaz Khan, Felsberg, Int. Conf. Pattern Recognit. Susanna Gladh, Martin Danelljan, Fahad Shahbaz Khan, and Michael Felsberg. 2016. Deep motion features for visual tracking. In Int. Conf. Pattern Recognit.\n\nSegmenting motions of different types by unsupervised manifold clustering. Alvina Goh, Rene Vidal, IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. Alvina Goh and Rene Vidal. 2007. Segmenting motions of different types by unsupervised manifold clustering. In IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.\n\nCombining two-view constraints for motion estimation. Govindu Venu Madhav, IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. Venu Madhav Govindu. 2001. Combining two-view constraints for motion estimation. In IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.\n\nShopbot: Progress in developing an interactive mobile shopping assistant for everyday use. H M Gross, H J Boehme, C Schroeter, S Mueller, A Koenig, Ch Martin, M Merten, A Bley, IEEE Int. Conf. Syst. Man Cybern. H. M. Gross, H. J. Boehme, C. Schroeter, S. Mueller, A. Koenig, Ch. Martin, M. Merten, and A. Bley. 2008. Shopbot: Progress in developing an interactive mobile shopping assistant for everyday use. In IEEE Int. Conf. Syst. Man Cybern. 3471-3478.\n\nDeep learning for visual understanding: A review. Yanming Guo, Yu Liu, Ard Oerlemans, Songyang Lao, Song Wu, Michael S Lew, Neurocomputing. 187Yanming Guo, Yu Liu, Ard Oerlemans, Songyang Lao, Song Wu, and Michael S. Lew. 2015. Deep learning for visual understanding: A review. Neurocomputing 187 (2015), 27-48.\n\nA computer algorithm for reconstructing a scene from two projections. C Hugh, Longuet-Higgins, Nature. 293Hugh C. Longuet-Higgins. 1981. A computer algorithm for reconstructing a scene from two projections. Nature 293 (1981), 133-135.\n\nReconstruction of a scene with multiple linearly moving objects. Mei Han, Takeo Kanade, Int. J. Comput. Vis. 59Mei Han and Takeo Kanade. 2004. Reconstruction of a scene with multiple linearly moving objects. Int. J. Comput. Vis. 59, 3 (2004), 285-300.\n\ngvnn: Neural network library for geometric computer vision. Ankur Handa, Michael Bloesch, Viorica Patraucean, Simon Stent, John Mccormac, Andrew Davison, arXiv:1607.07405Ankur Handa, Michael Bloesch, Viorica Patraucean, Simon Stent, John McCormac, and Andrew Davison. 2016. gvnn: Neural network library for geometric computer vision. In arXiv:1607.07405.\n\nRAPID -A video rate object tracker. Chris Harris, Carl Stennett, Br. Mach. Vis. Conf. Chris Harris and Carl Stennett. 1990. RAPID -A video rate object tracker. In Br. Mach. Vis. Conf.\n\nA combined corner and edge detector. Chris Harris, Mike Stephens, Alvey Vis. Conf. Chris Harris and Mike Stephens. 1988. A combined corner and edge detector. In Alvey Vis. Conf. 147-151.\n\nPowerFactorization: 3D reconstruction with missing or uncertain data. Richard Hartley, Frederik Schaffalitzky, In Aust. Adv. Work. Comput. Vis. 74Richard Hartley and Frederik Schaffalitzky. 2003. PowerFactorization: 3D reconstruction with missing or uncertain data. In Aust. Adv. Work. Comput. Vis., Vol. 74. 1-9.\n\nMultiple View Geometry in Computer Vision. Richard Hartley, Andrew Zisserman, Cambridge University Press2nd ed.Richard Hartley and Andrew Zisserman. 2004. Multiple View Geometry in Computer Vision (2nd ed.). Cambridge University Press.\n\n. Richard I Hartley, Peter Sturm, Triangulation. Comput. Vis. Image Underst. 68Richard I. Hartley and Peter Sturm. 1997. Triangulation. Comput. Vis. Image Underst. 68, 2 (1997), 146-157.\n\nMatching, reconstructing and grouping 3D lines from multiple views using uncertain projective geometry. Stephan Heuel, Wolfgang F\u00f6rstner, IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. Stephan Heuel and Wolfgang F\u00f6rstner. 2001. Matching, reconstructing and grouping 3D lines from multiple views using uncertain projective geometry. In IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.\n\nDetermining optical flow. K P Berthold, Brian G Horn, Schunck, Artif. Intell. 17Berthold K. P. Horn and Brian G. Schunck. 1981. Determining optical flow. Artif. Intell. 17, 1-3 (1981), 185-203.\n\nCombined optic-flow and stereo-based navigation of urban canyons for a UAV. Stefan Hrabar, Gaurav S Sukhatme, Peter Corke, Kane Usher, Jonathan Roberts, IEEE/RSJ Int. Conf. Intell. Robot. Syst. Stefan Hrabar, Gaurav S. Sukhatme, Peter Corke, Kane Usher, and Jonathan Roberts. 2005. Combined optic-flow and stereo-based navigation of urban canyons for a UAV. In IEEE/RSJ Int. Conf. Intell. Robot. Syst. 302-309.\n\nMotion and structure from feature correspondences: A review. Thomas S Huang, Arun N Netravali, Proc. IEEE. IEEE82Thomas S. Huang and Arun N. Netravali. 1994. Motion and structure from feature correspondences: A review. Proc. IEEE 82, 2 (1994), 252-268.\n\nMotion segmentation based on factorization method and discriminant critea. Naoyuki Ichimura, IEEE Int. Conf. Comput. Vis. Naoyuki Ichimura. 1999. Motion segmentation based on factorization method and discriminant critea. In IEEE Int. Conf. Comput. Vis.\n\nFlowNet 2.0: Evolution of optical flow estimation with deep networks. Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy, Thomas Brox, IEEE Conf. Comput. Vis. Pattern Recognit. Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy, and Thomas Brox. 2017. FlowNet 2.0: Evolution of optical flow estimation with deep networks. In IEEE Conf. Comput. Vis. Pattern Recognit.\n\nVisual-inertial navigation, mapping and localization: A scalable real-time causal approach. S Eagle, Stefano Jones, Soatto, Int. J. Rob. Res. 30Eagle S. Jones and Stefano Soatto. 2011. Visual-inertial navigation, mapping and localization: A scalable real-time causal approach. Int. J. Rob. Res. 30, 4 (2011), 1-38.\n\nTracking-learning-detection. Zdenek Kalal, Krystian Mikolajczyk, Jiri Matas, IEEE Trans. Pattern Anal. Mach. Intell. 34Zdenek Kalal, Krystian Mikolajczyk, and Jiri Matas. 2012. Tracking-learning-detection. IEEE Trans. Pattern Anal. Mach. Intell. 34, 7 (2012), 1409-1422.\n\nGeneral trajectory triangulation. Jeremy Yirmeyahu Kaminski, Mina Teicher, Eur. Conf. Comput. Vis. Jeremy Yirmeyahu Kaminski and Mina Teicher. 2002. General trajectory triangulation. In Eur. Conf. Comput. Vis. 823-836.\n\nA general framework for trajectory optimization. Jeremy Yirmeyahu Kaminski, Mina Teicher, J. Math. Imaging Vis. 21Jeremy Yirmeyahu Kaminski and Mina Teicher. 2004. A general framework for trajectory optimization. J. Math. Imaging Vis. 21 (2004), 27-41.\n\nStatistical Optimization for Geometric Computation: Theory and Practice. Kenichi Kanatani, ElsevierKenichi Kanatani. 1996. Statistical Optimization for Geometric Computation: Theory and Practice. Elsevier.\n\nMotion segmentation by subspace separation and model selection. Kenichi Kanatani, IEEE Int. Conf. Comput. Vis. Kenichi Kanatani. 2001. Motion segmentation by subspace separation and model selection. In IEEE Int. Conf. Comput. Vis. 586-591.\n\nEstimating the number of independent motions for multibody motion segmentation. Kenichi Kanatani, Chikara Matsunaga, Asian Conf. Comput. Vis. Kenichi Kanatani and Chikara Matsunaga. 2002. Estimating the number of independent motions for multibody motion segmentation. In Asian Conf. Comput. Vis.\n\nMoving object segmentation using optical flow and depth information. Jens Klappstein, Tobi Vaudrey, Clemens Rabe, Andreas Wedel, Reinhard Klette, Pacific-Rim Symp. Image Video Technol. Jens Klappstein, Tobi Vaudrey, Clemens Rabe, Andreas Wedel, and Reinhard Klette. 2009. Moving object segmenta- tion using optical flow and depth information. In Pacific-Rim Symp. Image Video Technol. 611-623.\n\nParallel tracking and mapping for small AR workspaces. Georg Klein, David Murray, IEEE ACM Int. Symp. Mix. Augment. Real. Georg Klein and David Murray. 2007. Parallel tracking and mapping for small AR workspaces. In IEEE ACM Int. Symp. Mix. Augment. Real.\n\nParallel tracking and mapping on a camera phone. Georg Klein, David Murray, IEEE Int. Symp. Mix. Augment. Real. Georg Klein and David Murray. 2009. Parallel tracking and mapping on a camera phone. In 8th IEEE Int. Symp. Mix. Augment. Real. 83-86.\n\nUnsupervised learning of depth and motion. Kishore Konda, Roland Memisevic, arXiv:1312.3429Kishore Konda and Roland Memisevic. 2013. Unsupervised learning of depth and motion. In arXiv:1312.3429.\n\nLearning visual odometry with a convolutional network. Kishore Konda, Roland Memisevic, Int. Conf. Comput. Vis. Theory Appl. Kishore Konda and Roland Memisevic. 2015. Learning visual odometry with a convolutional network. In Int. Conf. Comput. Vis. Theory Appl. 486-490.\n\nImageNet classification with deep convolutional neural networks. Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, In Adv. Neural Inf. Process. Syst. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. 2012. ImageNet classification with deep convolutional neural networks. In Adv. Neural Inf. Process. Syst. 1-9.\n\nMulti-body non-rigid structure-from-motion. Suryansh Kumar, Yuchao Dai, Hongdong Li, Int. Conf. 3D Vis. Suryansh Kumar, Yuchao Dai, and Hongdong Li. 2016. Multi-body non-rigid structure-from-motion. In Int. Conf. 3D Vis. 148-156.\n\nG2o: A general framework for graph optimization. Rainer Kummerle, Giorgio Grisetti, Hauke Strasdat, Kurt Konolige, Wolfram Burgard, IEEE Int. Conf. Robot. Autom. Rainer Kummerle, Giorgio Grisetti, Hauke Strasdat, Kurt Konolige, and Wolfram Burgard. 2011. G2o: A general framework for graph optimization. In IEEE Int. Conf. Robot. Autom. 3607-3613.\n\nRealtime motion segmentation based multibody visual SLAM. Abhijit Kundu, K Krishna, C V Jawahar, 7th Indian Conf. Comput. Vision, Graph. Image Process. Abhijit Kundu, K. Madhava Krishna, and C. V. Jawahar. 2010. Realtime motion segmentation based multibody visual SLAM. In 7th Indian Conf. Comput. Vision, Graph. Image Process. 251-258.\n\nRealtime multibody visual SLAM and tracking with a smoothly moving monocular camera. Abhijit Kundu, K Krishna, C V Jawahar, IEEE Int. Conf. Comput. Vis. Abhijit Kundu, K. Madhava Krishna, and C. V. Jawahar. 2011. Realtime multibody visual SLAM and tracking with a smoothly moving monocular camera. In IEEE Int. Conf. Comput. Vis.\n\nMoving object detection by multi-view geometric techniques from a single camera mounted robot. Abhijit Kundu, K Krishna, Jayanthi Sivaswamy, IEEE/RSJ Int. Conf. Intell. Robot. Syst. Abhijit Kundu, K. Madhava Krishna, and Jayanthi Sivaswamy. 2009. Moving object detection by multi-view geo- metric techniques from a single camera mounted robot. In IEEE/RSJ Int. Conf. Intell. Robot. Syst. 4306-4312.\n\nDeeper depth prediction with fully convolutional residual networks. Iro Laina, Christian Rupprecht, Vasileios Belagiannis, Federico Tombari, Nassir Navab, Int. Conf. 3D Vis. Iro Laina, Christian Rupprecht, Vasileios Belagiannis, Federico Tombari, and Nassir Navab. 2016. Deeper depth prediction with fully convolutional residual networks. In Int. Conf. 3D Vis. 239-248.\n\nICA with reconstruction cost for efficient overcomplete feature learning. V Quoc, Alexandre Le, Jiquan Karpenko, Andrew Y Ngiam, Ng, Adv. Neural Inf. Process. Syst. Quoc V. Le, Alexandre Karpenko, Jiquan Ngiam, and Andrew Y. Ng. 2011. ICA with reconstruction cost for efficient overcomplete feature learning. In Adv. Neural Inf. Process. Syst. 1-9.\n\nBuilding high-level features using large scale unsupervised learning. V Quoc, Marc&apos;aurelio Le, Rajat Ranzato, Matthieu Monga, Kai Devin, Greg S Chen, Jeff Corrado, Andrew Y Dean, Ng, Int. Conf. Mach. Learn. 38115Quoc V. Le, Marc'Aurelio Ranzato, Rajat Monga, Matthieu Devin, Kai Chen, Greg S. Corrado, Jeff Dean, and Andrew Y. Ng. 2011. Building high-level features using large scale unsupervised learning. In Int. Conf. Mach. Learn. 38115.\n\nDeep learning. Yann Lecun, Yoshua Bengio, Geoffrey Hinton, Nature. 521Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2016. Deep learning. Nature 521 (2016), 436-444.\n\nDriving recorder based on-road pedestrian tracking using visual SLAM and constrained multiple-kernel. Jenq Neng Kuan Hui Lee, Greg Hwang, James Okapal, Pitton, 17th IEEE Int. Conf. Intell. Transp. Syst. Kuan Hui Lee, Jenq Neng Hwang, Greg Okapal, and James Pitton. 2014. Driving recorder based on-road pedes- trian tracking using visual SLAM and constrained multiple-kernel. In 17th IEEE Int. Conf. Intell. Transp. Syst. 2629-2635.\n\nGround-moving-platform-based human tracking using visual SLAM and constrained multiple kernels. Jenq-Neng Kuan-Hui Lee, Greg Hwang, James Okopal, Pitton, IEEE Trans. Intell. Transp. Syst. 17Kuan-hui Lee, Jenq-neng Hwang, Greg Okopal, and James Pitton. 2016. Ground-moving-platform-based human tracking using visual SLAM and constrained multiple kernels. IEEE Trans. Intell. Transp. Syst. 17, 12 (2016), 3602-3612.\n\nBRISK: Binary robust invariant scalable keypoints. Stefan Leutenegger, Margarita Chli, Roland Y Siegwart, IEEE Int. Conf. Comput. Vis. Stefan Leutenegger, Margarita Chli, and Roland Y. Siegwart. 2011. BRISK: Binary robust invariant scalable keypoints. In IEEE Int. Conf. Comput. Vis. 2548-2555.\n\nKeyframe-based visual-inertial SLAM using nonlinear optimization. Stefan Leutenegger, Paul Furgale, Vincent Rabaud, Margarita Chli, Kurt Konolige, Roland Siegwart, Int. J. Rob. Res. 34Stefan Leutenegger, Paul Furgale, Vincent Rabaud, Margarita Chli, Kurt Konolige, and Roland Siegwart. 2013. Keyframe-based visual-inertial SLAM using nonlinear optimization. Int. J. Rob. Res. 34, 3 (2013), 314-334.\n\nProjective factorization of multiple rigid-body motions. Ting Li, Vinutha Kallem, Dheeraj Singaraju, Rene Vidal, IEEE Conf. Comput. Vis. Pattern Recognit. Ting Li, Vinutha Kallem, Dheeraj Singaraju, and Rene Vidal. 2007. Projective factorization of multiple rigid-body motions. In IEEE Conf. Comput. Vis. Pattern Recognit.\n\nReal-time 6-DOF monocular visual SLAM in a large-scale environment. Hyon Lim, Jongwoo Lim, H. Jin Kim, IEEE Int. Conf. Robot. Autom. Hyon Lim, Jongwoo Lim, and H. Jin Kim. 2014. Real-time 6-DOF monocular visual SLAM in a large-scale environ- ment. In IEEE Int. Conf. Robot. Autom.\n\nStereo-based simultaneous localization, mapping and moving object tracking. Chieh-Chih Kuen-Han Lin, Wang, IEEE/RSJ Int. Conf. Intell. Robot. Syst. Kuen-Han Lin and Chieh-Chih Wang. 2010. Stereo-based simultaneous localization, mapping and moving object tracking. In IEEE/RSJ Int. Conf. Intell. Robot. Syst.\n\nDeep learning of spatio-temporal features with geometric-based moving point detection for motion segmentation. Chieh-Chih Tsung Han Lin, Wang, IEEE Int. Conf. Robot. Autom. Tsung Han Lin and Chieh-Chih Wang. 2014. Deep learning of spatio-temporal features with geometric-based moving point detection for motion segmentation. In IEEE Int. Conf. Robot. Autom. 3058-3065.\n\nRobust recovery of subspace structures by low-rank representation. Guangcan Liu, Zhouchen Lin, Shuicheng Yan, Ju Sun, Yong Yu, Yi Ma, IEEE Trans. Pattern Anal. Mach. Intell. 35Guangcan Liu, Zhouchen Lin, Shuicheng Yan, Ju Sun, Yong Yu, and Yi Ma. 2013. Robust recovery of subspace struc- tures by low-rank representation. IEEE Trans. Pattern Anal. Mach. Intell. 35, 1 (2013), 171-184.\n\nFully convolutional networks for semantic segmentation. Jonathan Long, Evan Shelhamer, Trevor Darrell, IEEE Conf. Comput. Vis. Pattern Recognit. Jonathan Long, Evan Shelhamer, and Trevor Darrell. 2015. Fully convolutional networks for semantic segmentation. In IEEE Conf. Comput. Vis. Pattern Recognit. 3431-3440.\n\nDistinctive image features from scale-invariant keypoints. David G Lowe, Int. J. Comput. Vis. 60David G. Lowe. 2004. Distinctive image features from scale-invariant keypoints. Int. J. Comput. Vis. 60, 2 (2004), 91-110.\n\nAn Iterative Image Registration Technique with an Application to Stereo Vision. D Bruce, Takeo Lucas, Kanade, DARPA Image Underst. Bruce D. Lucas and Takeo Kanade. 1981. An Iterative Image Registration Technique with an Application to Stereo Vision. In DARPA Image Underst. Work. 121-130.\n\nA large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. Nikolaus Mayer, Eddy Ilg, Philip H\u00e4usser, Philipp Fischer, Daniel Cremers, Alexey Dosovitskiy, Thomas Brox, IEEE Conf. Comput. Vis. Pattern Recognit. Nikolaus Mayer, Eddy Ilg, Philip H\u00e4usser, Philipp Fischer, Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox. 2016. A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. In IEEE Conf. Comput. Vis. Pattern Recognit.\n\nRSLAM: A system for large-scale mapping in constant-time using stereo. Christopher Mei, Gabe Sibley, Mark Cummins, Paul Newman, Ian Reid, Int. J. Comput. Vis. 94Christopher Mei, Gabe Sibley, Mark Cummins, Paul Newman, and Ian Reid. 2011. RSLAM: A system for large-scale mapping in constant-time using stereo. Int. J. Comput. Vis. 94, 2 (2011), 198-214.\n\nRelative camera pose estimation using convolutional neural networks. Iaroslav Melekhov, Juha Ylioinas, Juho Kannala, Esa Rahtu, arXiv:1702.01381Iaroslav Melekhov, Juha Ylioinas, Juho Kannala, and Esa Rahtu. 2017. Relative camera pose estimation using con- volutional neural networks. In arXiv:1702.01381.\n\nUse a single camera for simultaneous localization and mapping with mobile object tracking in dynamic environments. Davide Migliore, Roberto Rigamonti, Daniele Marzorati, Matteo Matteucci, Domenico G Sorrenti, In ICRA Work. Safe Navig. Open Dyn. Environ. Appl. to Auton. Veh. Davide Migliore, Roberto Rigamonti, Daniele Marzorati, Matteo Matteucci, and Domenico G. Sorrenti. 2009. Use a single camera for simultaneous localization and mapping with mobile object tracking in dynamic environments. In ICRA Work. Safe Navig. Open Dyn. Environ. Appl. to Auton. Veh.\n\nDeepVO: A deep learning approach for monocular visual odometry. Vikram Mohanty, Shubh Agrawal, Shaswat Datta, Arna Ghosh, Dutt Vishnu, Debashish Sharma, Chakravarty, arXiv:1611.06069Vikram Mohanty, Shubh Agrawal, Shaswat Datta, Arna Ghosh, Vishnu Dutt Sharma, and Debashish Chakravarty. 2016. DeepVO: A deep learning approach for monocular visual odometry. In arXiv:1611.06069.\n\nA sequential factorization method for recovering shape and motion from image streams. Toshihiko Morita, Takeo Kanade, Proc. Natl. Acad. Sci. 90Toshihiko Morita and Takeo Kanade. 1993. A sequential factorization method for recovering shape and motion from image streams. Proc. Natl. Acad. Sci. 90, 21 (1993), 9795-9802.\n\nGlobal fusion of relative motions for robust, accurate and scalable structure from motion. Pierre Moulon, Pascal Monasse, Renaud Marlet, IEEE Int. Conf. Comput. Vis. Pierre Moulon, Pascal Monasse, and Renaud Marlet. 2013. Global fusion of relative motions for robust, accurate and scalable structure from motion. In IEEE Int. Conf. Comput. Vis. 3248-3255.\n\nMonocular vision based SLAM for mobile robots. Etienne Mouragnon, Maxime Lhuillier, Michel Dhome, Fabien Dekeyser, Patrick Sayd, 18th Int. Conf. Pattern Recognit. Etienne Mouragnon, Maxime Lhuillier, Michel Dhome, Fabien Dekeyser, and Patrick Sayd. 2006. Monocular vision based SLAM for mobile robots. In 18th Int. Conf. Pattern Recognit.\n\nReal time localization and 3D reconstruction. Etienne Mouragnon, Maxime Lhuillier, Michel Dhome, Fabien Dekeyser, Patrick Sayd, In IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. Etienne Mouragnon, Maxime Lhuillier, Michel Dhome, Fabien Dekeyser, and Patrick Sayd. 2006. Real time localiza- tion and 3D reconstruction. In IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. 1-8.\n\nGeneric and realtime structure from motion. Etienne Mouragnon, Maxime Lhuillier, Michel Dhome, Fabien Dekeyser, Patrick Sayd, Br. Mach. Vis. Conf. 64.1-64.10. Etienne Mouragnon, Maxime Lhuillier, Michel Dhome, Fabien Dekeyser, and Patrick Sayd. 2007. Generic and real- time structure from motion. In Br. Mach. Vis. Conf. 64.1-64.10.\n\nGeneric and realtime structure from motion using local bundle adjustment. Etienne Mouragnon, Maxime Lhuillier, Michel Dhome, Fabien Dekeyser, Patrick Sayd, Image Vis. Comput. 27Etienne Mouragnon, Maxime Lhuillier, Michel Dhome, Fabien Dekeyser, and Patrick Sayd. 2009. Generic and real- time structure from motion using local bundle adjustment. Image Vis. Comput. 27, 8 (2009), 1178-1193.\n\nFlowdometry: An optical flow and deep learning based approach to visual odometry. Peter Muller, Andreas Savakis, IEEE Winter Conf. Appl. Comput. Vis. Peter Muller and Andreas Savakis. 2017. Flowdometry: An optical flow and deep learning based approach to visual odometry. In IEEE Winter Conf. Appl. Comput. Vis.\n\nORB-SLAM: A versatile and accurate monocular SLAM system. Raul Mur-Artal, J M M Montiel, Juan D Tardos, IEEE Trans. Robot. 31Raul Mur-Artal, J. M. M. Montiel, and Juan D. Tardos. 2015. ORB-SLAM: A versatile and accurate monocular SLAM system. IEEE Trans. Robot. 31, 5 (2015), 1147-1163.\n\nDepth-estimation-free projective factorization and its application to 3D reconstruction. Yohei Murakami, Takeshi Endo, Yoshimichi Ito, Noboru Babaguchi, Asian Conf. Comput. Vis. Yohei Murakami, Takeshi Endo, Yoshimichi Ito, and Noboru Babaguchi. 2012. Depth-estimation-free projective fac- torization and its application to 3D reconstruction. In Asian Conf. Comput. Vis. 150-162.\n\nKinectFusion: Real-time dense surface mapping and tracking. Richard A Newcombe, David Molyneaux, David Kim, Andrew J Davison, Jamie Shotton, Steve Hodges, Andrew Fitzgibbon, Shahram Izadi, Otmar Hilliges, David Molyneaux, David Kim, Andrew J Davison, Pushmeet Kohli, IEEE Int. Symp. Mix. Augment. Real. Richard A. Newcombe, David Molyneaux, David Kim, Andrew J. Davison, Jamie Shotton, Steve Hodges, Andrew Fitzgibbon, Shahram Izadi, Otmar Hilliges, David Molyneaux, David Kim, Andrew J. Davison, Pushmeet Kohli, Jamie Shotton, Steve Hodges, and Andrew Fitzgibbon. 2011. KinectFusion: Real-time dense surface mapping and tracking. In IEEE Int. Symp. Mix. Augment. Real. 127-136.\n\nAn efficient solution to the five-point relative pose problem. David Nister, IEEE Trans. Pattern Anal. Mach. Intell. 26David Nister. 2004. An efficient solution to the five-point relative pose problem. IEEE Trans. Pattern Anal. Mach. Intell. 26, 6 (2004), 756-770.\n\nVisual odometry. David Nist\u00e9r, Oleg Naroditsky, James Bergen, IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. David Nist\u00e9r, Oleg Naroditsky, and James Bergen. 2004. Visual odometry. In IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. 652-659.\n\nA critique of structure-from-motion algorithms. John Oliensis, Comput. Vis. Image Underst. 80John Oliensis. 2000. A critique of structure-from-motion algorithms. Comput. Vis. Image Underst. 80, 2 (2000), 172-214.\n\nIndoor robot motion based on monocular images. D Ort\u00edn, J Montiel, Robotica. 19D. Ort\u00edn and J. Montiel. 2001. Indoor robot motion based on monocular images. Robotica 19, 3 (2001), 331-342.\n\nA threshold selection method from gray-level histograms. Nobuyuki Otsu, IEEE Trans. Syst. Man. Cybern. SMC. 9Nobuyuki Otsu. 1979. A threshold selection method from gray-level histograms. IEEE Trans. Syst. Man. Cybern. SMC-9, 1 (1979), 62-66.\n\nReconstructing 3D trajectories of independently moving objects using generic constraints. Kurt Kemal Egemen Ozden, Luc Cornelis, Luc Van Eycken, Van Gool, Comput. Vis. Image Underst. 96Kemal Egemen Ozden, Kurt Cornelis, Luc Van Eycken, and Luc Van Gool. 2004. Reconstructing 3D trajec- tories of independently moving objects using generic constraints. Comput. Vis. Image Underst. 96, 3 (2004), 453-471.\n\nMultibody structure-from-motion in practice. E Kemal, Konrad Ozden, Luc Schindler, Van Gool, IEEE Trans. Pattern Anal. Mach. Intell. 32Kemal E. Ozden, Konrad Schindler, and Luc Van Gool. 2010. Multibody structure-from-motion in practice. IEEE Trans. Pattern Anal. Mach. Intell. 32, 6 (2010), 1134-1141.\n\nFactorization for non-rigid and articulated structure using metric projections. Marco Paladini, Alessio Del Bue, Marko Sto\u0161i\u0107, Marija Dodig, Jo\u00e3o Xavier, Lourdes Agapito, IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. Marco Paladini, Alessio Del Bue, Marko Sto\u0161i\u0107, Marija Dodig, Jo\u00e3o Xavier, and Lourdes Agapito. 2009. Factorization for non-rigid and articulated structure using metric projections. In IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. 2898-2905.\n\n3D reconstruction of a moving point from a series of 2D projections. Hyun Soo Park, Takaaki Shiratori, Iain Matthews, Yaser Sheikh, Eur. Conf. Comput. Vis. Hyun Soo Park, Takaaki Shiratori, Iain Matthews, and Yaser Sheikh. 2010. 3D reconstruction of a moving point from a series of 2D projections. In Eur. Conf. Comput. Vis. 158-171.\n\n3D trajectory reconstruction under perspective projection. Hyun Soo Park, Takaaki Shiratori, Iain Matthews, Yaser Sheikh, Int. J. Comput. Vis. 115Hyun Soo Park, Takaaki Shiratori, Iain Matthews, and Yaser Sheikh. 2015. 3D trajectory reconstruction under per- spective projection. Int. J. Comput. Vis. 115, 2 (2015), 115-135.\n\nBackground subtraction techniques: A review. Massimo Piccardi, EEE Int. Conf. Syst. Man Cybern. 4Massimo Piccardi. 2004. Background subtraction techniques: A review. In EEE Int. Conf. Syst. Man Cybern., Vol. 4. 3099-3104.\n\nAccurate and reliable soldier and first responder indoor positioning: Multisensor systems and cooperative localization. Jouni Rantakokko, Joakim Rydell, Peter Str\u00f6mb\u00e4ck, Peter H\u00e4ndel, Jonas Callmer, David T\u00f6rnqvist, Magnus Jobs, and Mathias Grud\u00e9n. 18Fredrik GustafssonJouni Rantakokko, Joakim Rydell, Peter Str\u00f6mb\u00e4ck, Peter H\u00e4ndel, Jonas Callmer, David T\u00f6rnqvist, Fredrik Gustafs- son, Magnus Jobs, and Mathias Grud\u00e9n. 2011. Accurate and reliable soldier and first responder indoor positioning: Multisensor systems and cooperative localization. IEEE Wirel. Commun. 18, 2 (2011), 10-18.\n\nMotion segmentation in the presence of outlying, incomplete, or corrupted trajectories. Shankar Rao, Roberto Tron, Rene Vidal, Yi Ma, IEEE Trans. Pattern Anal. Mach. Intell. 32Shankar Rao, Roberto Tron, Rene Vidal, and Yi Ma. 2010. Motion segmentation in the presence of outlying, incom- plete, or corrupted trajectories. IEEE Trans. Pattern Anal. Mach. Intell. 32, 10 (2010), 1832-1845.\n\nUniversal coding, information, prediction, and eestimation. Jorma Rissanen, IEEE Trans. Inf. Theory. 30Jorma Rissanen. 1984. Universal coding, information, prediction, and eestimation. IEEE Trans. Inf. Theory 30, 4 (1984), 629-636.\n\nMachine learning for high-speed corner detection. Edward Rosten, Tom Drummond, Eur. Conf. Comput. Vis. 1Edward Rosten and Tom Drummond. 2006. Machine learning for high-speed corner detection. In Eur. Conf. Comput. Vis., Vol. 1. 430-443.\n\nORB: An efficient alternative to SIFT or SURF. Ethan Rublee, Vincent Rabaud, Kurt Konolige, Gary Bradski, IEEE Int. Conf. Comput. Vis. Ethan Rublee, Vincent Rabaud, Kurt Konolige, and Gary Bradski. 2011. ORB: An efficient alternative to SIFT or SURF. In IEEE Int. Conf. Comput. Vis. 2564-2571.\n\nMonocular simultaneous multi-body motion segmentation and reconstruction from perspective views. Reza Sabzevari, Davide Scaramuzza, IEEE Int. Conf. Robot. Autom. Reza Sabzevari and Davide Scaramuzza. 2014. Monocular simultaneous multi-body motion segmentation and re- construction from perspective views. In IEEE Int. Conf. Robot. Autom. 23-30.\n\nMulti-body motion estimation from monocular vehicle-mounted cameras. Reza Sabzevari, Davide Scaramuzza, IEEE Trans. Robot. 32Reza Sabzevari and Davide Scaramuzza. 2016. Multi-body motion estimation from monocular vehicle-mounted cam- eras. IEEE Trans. Robot. 32, 3 (2016), 638-651.\n\nObstacle avoidance for visually impaired using auto-adaptive thresholding on Kinect's depth image. Utama Muhamad Risqi, Widyawan Saputra, Paulus Insap Santosa, 11th IEEE Int. Conf. Ubiquitous Intell. Comput. Muhamad Risqi Utama Saputra, Widyawan, and Paulus Insap Santosa. 2014. Obstacle avoidance for visually impaired using auto-adaptive thresholding on Kinect's depth image. In 11th IEEE Int. Conf. Ubiquitous Intell. Comput. 337-342.\n\nThink globally, fit locally: Unsupervised learning of low dimensional manifolds. K Lawrence, Sam T Saul, Roweis, J. Mach. Learn. Res. 4Lawrence K. Saul and Sam T. Roweis. 2003. Think globally, fit locally: Unsupervised learning of low dimensional manifolds. J. Mach. Learn. Res. 4, 1999 (2003), 119-155.\n\n1-point-RANSAC structure from motion for vehicle-mounted cameras by exploiting nonholonomic constraints. Davide Scaramuzza, Int. J. Comput. Vis. 95Davide Scaramuzza. 2011. 1-point-RANSAC structure from motion for vehicle-mounted cameras by exploiting non- holonomic constraints. Int. J. Comput. Vis. 95, 1 (2011), 74-85.\n\nReal-time monocular visual odometry for on-road vehicles with 1-point RANSAC. Davide Scaramuzza, Friedrich Fraundorfer, Roland Siegwart, IEEE Int. Conf. Robot. Autom. Davide Scaramuzza, Friedrich Fraundorfer, and Roland Siegwart. 2009. Real-time monocular visual odometry for on-road vehicles with 1-point RANSAC. In IEEE Int. Conf. Robot. Autom. 4293-4299.\n\nTwo-view multibody structure-and-motion with outliers. Konrad Schindler, David Suter, IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. Konrad Schindler and David Suter. 2005. Two-view multibody structure-and-motion with outliers. In IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.\n\nTwo-view multibody structure-and-motion with outliers through model selection. Konrad Schindler, David Suter, IEEE Trans. Pattern Anal. Mach. Intell. 286Konrad Schindler and David Suter. 2006. Two-view multibody structure-and-motion with outliers through model selection. IEEE Trans. Pattern Anal. Mach. Intell. 28, 6 (2006), 983-995.\n\nA model-selection framework for multibody structure-andmotion of image sequences. Konrad Schindler, David Suter, Hanzi Wang, Int. J. Comput. Vis. 79Konrad Schindler, David Suter, and Hanzi Wang. 2008. A model-selection framework for multibody structure-and- motion of image sequences. Int. J. Comput. Vis. 79, 2 (2008), 159-177.\n\nPerspective n-view multibody structure-and-motion through model selection. Konrad Schindler, James U , Hanzi Wang, Eur. Conf. Comput. Vis. 1Konrad Schindler, James U., and Hanzi Wang. 2006. Perspective n-view multibody structure-and-motion through model selection. In Eur. Conf. Comput. Vis., Vol. 1. 606-619.\n\nStructure-from-motion revisited. Johannes Lutz Sch\u00f6nberger, Jan-Michael Frahm, IEEE Conf. Comput. Vis. Pattern Recognit. Johannes Lutz Sch\u00f6nberger and Jan-Michael Frahm. 2016. Structure-from-motion revisited. In IEEE Conf. Comput. Vis. Pattern Recognit. 4104-4113.\n\nEstimating the dimension of a model. Gideon Schwarz, Ann. Stat. 6Gideon Schwarz. 1978. Estimating the dimension of a model. Ann. Stat. 6, 2 (1978), 461-464.\n\nTrajectory triangulation over conic sections. Amnon Shashua, Shai Avidan, Michael Werman, IEEE Int. Conf. Comput. Vis. Amnon Shashua, Shai Avidan, and Michael Werman. 1999. Trajectory triangulation over conic sections. In IEEE Int. Conf. Comput. Vis.\n\nVast-scale outdoor navigation using adaptive relative bundle adjustment. Gabe Sibley, Christopher Mei, Ian Reid, Paul Newman, Int. J. Rob. Res. 29Gabe Sibley, Christopher Mei, Ian Reid, and Paul Newman. 2010. Vast-scale outdoor navigation using adaptive relative bundle adjustment. Int. J. Rob. Res. 29, 8 (2010), 958-980.\n\nTwo-stream convolutional networks for action recognition in videos. Karen Simonyan, Andrew Zisserman, Adv. Neural Inf. Process. Syst. Karen Simonyan and Andrew Zisserman. 2014. Two-stream convolutional networks for action recognition in videos. In Adv. Neural Inf. Process. Syst. 1-9.\n\nPhotoTourism: Exploring photo collections in 3D. Noah Snavely, Steven Seitz, Richard Szeliski, SIG-GRAPH Conf. Proc. Noah Snavely, Steven Seitz, and Richard Szeliski. 2006. PhotoTourism: Exploring photo collections in 3D. In SIG- GRAPH Conf. Proc. 835-846.\n\nModeling the world from internet photo collections. Noah Snavely, Steven M Seitz, Richard Szeliski, Int. J. Comput. Vis. 80Noah Snavely, Steven M. Seitz, and Richard Szeliski. 2008. Modeling the world from internet photo collections. Int. J. Comput. Vis. 80, 2 (2008), 189-210.\n\nTowards Visual Localization, Mapping and Moving Objects Tracking by a Mobile Robot: A Geometric and Probabilistic Approach. Joan Sol\u00e0, Ph.D. Dissertation. Institut National Politechnique de ToulouseJoan Sol\u00e0. 2007. Towards Visual Localization, Mapping and Moving Objects Tracking by a Mobile Robot: A Geometric and Probabilistic Approach. Ph.D. Dissertation. Institut National Politechnique de Toulouse.\n\nVisual SLAM: Why filter? Image Vis. Hauke Strasdat, J M M Montiel, Andrew J Davison, Comput. 30Hauke Strasdat, J. M. M. Montiel, and Andrew J. Davison. 2012. Visual SLAM: Why filter? Image Vis. Comput. 30, 2 (2012), 65-77.\n\nA factorization based algorithm for multi-image projective structure and motion. Peter Sturm, Bill Triggs, Eur. Conf. Comput. Vis. 1065Peter Sturm and Bill Triggs. 1996. A factorization based algorithm for multi-image projective structure and motion. In Eur. Conf. Comput. Vis., Vol. 1065. 710-720.\n\nRobust monocular SLAM in dynamic environments. Wei Tan, Haomin Liu, Zilong Dong, Guofeng Zhang, Hujun Bao, In IEEE Int. Symp. Mix. Augment. Real. Wei Tan, Haomin Liu, Zilong Dong, Guofeng Zhang, and Hujun Bao. 2013. Robust monocular SLAM in dynamic environments. In IEEE Int. Symp. Mix. Augment. Real.\n\nMultibody structure-and-motion segmentation by branchand-bound model selection. Ninad Thakoor, Jean Gao, Venkat Devarajan, IEEE Trans. Image Process. 19Ninad Thakoor, Jean Gao, and Venkat Devarajan. 2010. Multibody structure-and-motion segmentation by branch- and-bound model selection. IEEE Trans. Image Process. 19, 6 (2010), 1393-1402.\n\nShape and motion from image streams under orthography: A factorization method. Carlo Tomasi, Takeo Kanade, In Int. J. Comput. Vis. 9Carlo Tomasi and Takeo Kanade. 1992. Shape and motion from image streams under orthography: A factorization method. In Int. J. Comput. Vis., Vol. 9. 137-154.\n\nGeometric motion segmentation and model selection. H S Philip, Torr, Philos. Trans. R. Soc. A Math. Phys. Eng. Sci. 356Philip H. S. Torr. 1998. Geometric motion segmentation and model selection. Philos. Trans. R. Soc. A Math. Phys. Eng. Sci. 356, 1740 (1998), 1321-1340.\n\nRobust parameterization and computation of the trifocal tensor. H S Philip, Andrew Torr, Zisserman, Image Vis. Comput. 15Philip H. S. Torr and Andrew Zisserman. 1997. Robust parameterization and computation of the trifocal tensor. Image Vis. Comput. 15, 8 (1997), 591-605.\n\nFeature based methods for structure and motion estimation. H S Philip, Andrew Torr, Zisserman, Int. Work. Vis. Algorithms. Philip H. S. Torr and Andrew Zisserman. 1999. Feature based methods for structure and motion estimation. In Int. Work. Vis. Algorithms.\n\nMLESAC: A new robust estimator with application to estimating image geometry. H S Philip, Andrew Torr, Zisserman, Comput. Vis. Image Underst. 78Philip H. S. Torr and Andrew Zisserman. 2000. MLESAC: A new robust estimator with application to estimating image geometry. Comput. Vis. Image Underst. 78, 1 (2000), 138-156.\n\nA benchmark for the comparison of 3-D motion segmentation algorithms. Roberto Tron, Rene Vidal, IEEE Conf. Comput. Vis. Pattern Recognit. Roberto Tron and Rene Vidal. 2007. A benchmark for the comparison of 3-D motion segmentation algorithms. In IEEE Conf. Comput. Vis. Pattern Recognit. 1-8.\n\nRecurrent fully convolutional networks for video segmentation. Sepehr Valipour, Mennatullah Siam, Martin Jagersand, Nilanjan Ray, IEEE Winter Conf. Appl. Comput. Vis. Sepehr Valipour, Mennatullah Siam, Martin Jagersand, and Nilanjan Ray. 2017. Recurrent fully convolutional net- works for video segmentation. In IEEE Winter Conf. Appl. Comput. Vis. 1-12.\n\nOnline clustering of moving hyperplanes. Ren\u00e9 Vidal, Adv. Neural Inf. Process. Syst. Ren\u00e9 Vidal. 2006. Online clustering of moving hyperplanes. In Adv. Neural Inf. Process. Syst. 1433-1440.\n\nSubspace clustering. Rene Vidal, IEEE Signal Process. Mag. 28Rene Vidal. 2011. Subspace clustering. IEEE Signal Process. Mag. 28, 2 (2011), 52-68.\n\nThree-view multibody structure from motion. Ren\u00e9 Vidal, Richard Hartley, IEEE Trans. Pattern Anal. Mach. Intell. 30Ren\u00e9 Vidal and Richard Hartley. 2008. Three-view multibody structure from motion. IEEE Trans. Pattern Anal. Mach. Intell. 30, 2 (2008), 214-227.\n\nGeneralized principal component analysis (GPCA). Ren\u00e9 Vidal, Yi Ma, Shankar Sastry, IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. 40Ren\u00e9 Vidal, Yi Ma, and Shankar Sastry. 2005. Generalized principal component analysis (GPCA). In IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. 40, 12 (2005), 1945-1959.\n\nGeneralized principal component analysis (GPCA). Ren\u00e9 Vidal, Yi Ma, Shankar Sastry, IEEE Trans. Pattern Anal. Mach. Intell. 27Ren\u00e9 Vidal, Yi Ma, and Shankar Sastry. 2005. Generalized principal component analysis (GPCA). IEEE Trans. Pattern Anal. Mach. Intell. 27, 12 (2005), 1945-1959.\n\nTwo-view multibody structure from motion. Ren\u00e9 Vidal, Yi Ma, Stefano Soatto, Shankar Sastry, Int. J. Comput. Vis. 681Ren\u00e9 Vidal, Yi Ma, Stefano Soatto, and Shankar Sastry. 2006. Two-view multibody structure from motion. Int. J. Comput. Vis. 68, 1 (2006), 7-25.\n\nSegmentation of dynamic scenes from the multibody fundamental matrix. Ren\u00e9 Vidal, Stefano Soatto, Yi Ma, Shankar Sastry, ECCV Work. Vis. Model. Dyn. Scenes. Ren\u00e9 Vidal, Stefano Soatto, Yi Ma, and Shankar Sastry. 2002. Segmentation of dynamic scenes from the multibody fundamental matrix. In ECCV Work. Vis. Model. Dyn. Scenes.\n\nSudheendra Vijayanarasimhan, Susanna Ricco, Cordelia Schmid, Rahul Sukthankar, Katerina Fragkiadaki, arXiv:1704.07804SfM-Net: Learning of structure and motion from video. Sudheendra Vijayanarasimhan, Susanna Ricco, Cordelia Schmid, Rahul Sukthankar, and Katerina Fragkiadaki. 2017. SfM-Net: Learning of structure and motion from video. In arXiv:1704.07804.\n\nSimultaneous localization and mapping with detection and tracking of moving objects. Chieh-Chih Wang, Chuck Thorpe, IEEE Int. Conf. Robot. Autom. 3Chieh-Chih Wang and Chuck Thorpe. 2002. Simultaneous localization and mapping with detection and tracking of moving objects. In IEEE Int. Conf. Robot. Autom., Vol. 3. 2918-2924.\n\nSimultaneous localization, mapping and moving object tracking. Chieh-Chih Wang, Charles Thorpe, M Sebastian Thrun, H Hebert, Durrant-Whyte, Int. J. Rob. Res. 26Chieh-Chih Wang, Charles Thorpe, Sebastian Thrun, M. Hebert, and H. Durrant-Whyte. 2007. Simultaneous local- ization, mapping and moving object tracking. Int. J. Rob. Res. 26, 9 (2007), 889-916.\n\nDeepVO: Towards end-to-end visual odometry with deep recurrent convolutional neural networks. Sen Wang, Ronald Clark, Hongkai Wen, Niki Trigoni, In IEEE Int. Conf. Robot. Autom. Sen Wang, Ronald Clark, Hongkai Wen, and Niki Trigoni. 2017. DeepVO: Towards end-to-end visual odometry with deep recurrent convolutional neural networks. In IEEE Int. Conf. Robot. Autom.\n\nVisual SLAM and moving-object detection for a small-size humanoid robot. Ming Chun Yin Tien Wang, Rung Chi Lin, Ju, Int. J. Adv. Robot. Syst. 72Yin Tien Wang, Ming Chun Lin, and Rung Chi Ju. 2010. Visual SLAM and moving-object detection for a small-size humanoid robot. Int. J. Adv. Robot. Syst. 7, 2 (2010), 133-138.\n\nAvoiding moving outliers in visual SLAM by tracking moving objects. Somkiat Wangsiripitak, David W Murray, IEEE Int. Conf. Robot. Autom. Somkiat Wangsiripitak and David W. Murray. 2009. Avoiding moving outliers in visual SLAM by tracking moving objects. In IEEE Int. Conf. Robot. Autom.\n\nTowards linear-time incremental structure from motion. Changchang Wu, Int. Conf. 3D Vis. Changchang Wu. 2013. Towards linear-time incremental structure from motion. In Int. Conf. 3D Vis. 127-134.\n\nMulticore bundle adjustment. Changchang Wu, Sameer Agarwal, Brian Curless, Steven M Seitz, IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. Changchang Wu, Sameer Agarwal, Brian Curless, and Steven M. Seitz. 2011. Multicore bundle adjustment. In IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. 3057-3064.\n\nA closed-form solution to non-rigid shape and motion recovery. Jing Xiao, Jin-Xiang Chai, Takeo Kanade, Eur. Conf. Comput. Vis. Jing Xiao, Jin-xiang Chai, and Takeo Kanade. 2004. A closed-form solution to non-rigid shape and motion recovery. In Eur. Conf. Comput. Vis. 573-587.\n\nA general framework for motion segmentation: Independent, articulated, rigid, non-rigid, degenerate and non-degenerate. Jingyu Yan, Marc Pollefeys, Eur. Conf. Comput. Vis. Jingyu Yan and Marc Pollefeys. 2006. A general framework for motion segmentation: Independent, articulated, rigid, non-rigid, degenerate and non-degenerate. In Eur. Conf. Comput. Vis.\n\nA factorization-based approach for articulated nonrigid shape, motion, and kinematic chain recovery from video. Jingyu Yan, Marc Pollefeys, IEEE Trans. Pattern Anal. Mach. Intell. 30Jingyu Yan and Marc Pollefeys. 2008. A factorization-based approach for articulated nonrigid shape, motion, and kinematic chain recovery from video. IEEE Trans. Pattern Anal. Mach. Intell. 30, 5 (2008), 865-877.\n\nSparse subspace clustering with missing entries. Congyuan Yang, Daniel Robinson, Rene Vidal, Int. Conf. Mach. Learn. Congyuan Yang, Daniel Robinson, and Rene Vidal. 2015. Sparse subspace clustering with missing entries. In Int. Conf. Mach. Learn. 2463-2472.\n\nA survey on non-filter-based monocular visual SLAM systems. Georges Younes, Daniel Asmar, Elie Shammas, arXiv:1607.00470Georges Younes, Daniel Asmar, and Elie Shammas. 2016. A survey on non-filter-based monocular visual SLAM systems. In arXiv:1607.00470.\n\nAn overview to visual odometry and visual SLAM: Applications to mobile robotics. Khalid Yousif, Alireza Bab-Hadiashar, Reza Hoseinnezhad, Intell. Ind. Syst. 1Khalid Yousif, Alireza Bab-Hadiashar, and Reza Hoseinnezhad. 2015. An overview to visual odometry and visual SLAM: Applications to mobile robotics. Intell. Ind. Syst. 1, 4 (2015), 289-311.\n\nJoint estimation of segmentation and structure from motion. Luca Zappella, Alessio Del Bue, Xavier Llad\u00f3, Joaquim Salvi, Comput. Vis. Image Underst. 117Luca Zappella, Alessio Del Bue, Xavier Llad\u00f3, and Joaquim Salvi. 2013. Joint estimation of segmentation and structure from motion. Comput. Vis. Image Underst. 117, 2 (2013), 113-129.\n\nHuman-and situation-aware people following. Hendrik Zender, Patric Jensfelt, Geert Jan, M Kruijff, In IEEE Int. Work. Robot Hum. Interact. Commun. Hendrik Zender, Patric Jensfelt, and Geert Jan M. Kruijff. 2007. Human-and situation-aware people following. In IEEE Int. Work. Robot Hum. Interact. Commun. 1131-1136.\n\nVisual odometry in dynamical scenes. Dong Zhang, Ping Li, Sensors Transducers J. 147Dong Zhang and Ping Li. 2012. Visual odometry in dynamical scenes. Sensors Transducers J. 147, 12 (2012), 78-86.\n\nMedian K-flats for hybrid linear modeling with many outliers. Teng Zhang, Arthur Szlam, Gilad Lerman, Int. Conf. Comput. Vis. Work. Teng Zhang, Arthur Szlam, and Gilad Lerman. 2009. Median K-flats for hybrid linear modeling with many outliers. In Int. Conf. Comput. Vis. Work. 234-241.\n\nJoint object class sequencing and trajectory triangulation (JOST). Enliang Zheng, Ke Wang, Enrique Dunn, Jan Michael Frahm, Eur. Conf. Comput. Vis. Enliang Zheng, Ke Wang, Enrique Dunn, and Jan Michael Frahm. 2014. Joint object class sequencing and trajectory triangulation (JOST). In Eur. Conf. Comput. Vis. 599-614.\n\nUnsupervised learning of depth and egomotion from video. Tinghui Zhou, Matthew Brown, Noah Snavely, David G Lowe, IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. Tinghui Zhou, Matthew Brown, Noah Snavely, and David G. Lowe. 2017. Unsupervised learning of depth and ego- motion from video. In IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.\n", "annotations": {"author": "[{\"end\":146,\"start\":78},{\"end\":211,\"start\":147},{\"end\":281,\"start\":212},{\"end\":349,\"start\":282}]", "publisher": null, "author_last_name": "[{\"end\":91,\"start\":86},{\"end\":156,\"start\":149},{\"end\":226,\"start\":219},{\"end\":294,\"start\":287}]", "author_first_name": "[{\"end\":85,\"start\":78},{\"end\":148,\"start\":147},{\"end\":218,\"start\":212},{\"end\":286,\"start\":282}]", "author_affiliation": "[{\"end\":145,\"start\":93},{\"end\":210,\"start\":158},{\"end\":280,\"start\":228},{\"end\":348,\"start\":296}]", "title": "[{\"end\":75,\"start\":1},{\"end\":424,\"start\":350}]", "venue": null, "abstract": "[{\"end\":10228,\"start\":774}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b29\"},\"end\":12430,\"start\":12426},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":12433,\"start\":12430},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":12436,\"start\":12433},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":13008,\"start\":13004},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":13133,\"start\":13129},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":13513,\"start\":13509},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":13516,\"start\":13513},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":14489,\"start\":14485},{\"attributes\":{\"ref_id\":\"b88\"},\"end\":14492,\"start\":14489},{\"attributes\":{\"ref_id\":\"b172\"},\"end\":14520,\"start\":14515},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":14713,\"start\":14709},{\"attributes\":{\"ref_id\":\"b171\"},\"end\":14943,\"start\":14938},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":14994,\"start\":14991},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":15237,\"start\":15233},{\"attributes\":{\"ref_id\":\"b88\"},\"end\":15528,\"start\":15524},{\"attributes\":{\"ref_id\":\"b89\"},\"end\":15531,\"start\":15528},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":15918,\"start\":15915},{\"attributes\":{\"ref_id\":\"b125\"},\"end\":15922,\"start\":15918},{\"attributes\":{\"ref_id\":\"b183\"},\"end\":15942,\"start\":15937},{\"attributes\":{\"ref_id\":\"b164\"},\"end\":16231,\"start\":16226},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":16556,\"start\":16552},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":16996,\"start\":16992},{\"attributes\":{\"ref_id\":\"b103\"},\"end\":17821,\"start\":17816},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":18242,\"start\":18238},{\"attributes\":{\"ref_id\":\"b94\"},\"end\":18469,\"start\":18465},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":19167,\"start\":19163},{\"attributes\":{\"ref_id\":\"b151\"},\"end\":19414,\"start\":19409},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":20085,\"start\":20081},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":20213,\"start\":20209},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":20542,\"start\":20539},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":21139,\"start\":21135},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":21142,\"start\":21139},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":21622,\"start\":21618},{\"attributes\":{\"ref_id\":\"b115\"},\"end\":21654,\"start\":21649},{\"attributes\":{\"ref_id\":\"b135\"},\"end\":22112,\"start\":22107},{\"attributes\":{\"ref_id\":\"b136\"},\"end\":22447,\"start\":22442},{\"attributes\":{\"ref_id\":\"b132\"},\"end\":22482,\"start\":22477},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":22927,\"start\":22923},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":23179,\"start\":23175},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":23182,\"start\":23179},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":23559,\"start\":23555},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":23753,\"start\":23749},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":23756,\"start\":23753},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":23899,\"start\":23895},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":24336,\"start\":24332},{\"attributes\":{\"ref_id\":\"b100\"},\"end\":24703,\"start\":24698},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":24804,\"start\":24800},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":24878,\"start\":24874},{\"attributes\":{\"ref_id\":\"b145\"},\"end\":24882,\"start\":24878},{\"attributes\":{\"ref_id\":\"b95\"},\"end\":25256,\"start\":25252},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":25415,\"start\":25411},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":25418,\"start\":25415},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":25772,\"start\":25768},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":25907,\"start\":25903},{\"attributes\":{\"ref_id\":\"b159\"},\"end\":26074,\"start\":26069},{\"attributes\":{\"ref_id\":\"b97\"},\"end\":26257,\"start\":26253},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":27078,\"start\":27074},{\"attributes\":{\"ref_id\":\"b156\"},\"end\":27909,\"start\":27904},{\"attributes\":{\"ref_id\":\"b116\"},\"end\":27957,\"start\":27952},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":28001,\"start\":27997},{\"attributes\":{\"ref_id\":\"b141\"},\"end\":28025,\"start\":28020},{\"attributes\":{\"ref_id\":\"b173\"},\"end\":28029,\"start\":28025},{\"attributes\":{\"ref_id\":\"b98\"},\"end\":28127,\"start\":28123},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":28198,\"start\":28195},{\"attributes\":{\"ref_id\":\"b129\"},\"end\":28348,\"start\":28343},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":28392,\"start\":28388},{\"attributes\":{\"ref_id\":\"b93\"},\"end\":28395,\"start\":28392},{\"attributes\":{\"ref_id\":\"b99\"},\"end\":28726,\"start\":28721},{\"attributes\":{\"ref_id\":\"b98\"},\"end\":28849,\"start\":28845},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":28859,\"start\":28856},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":28871,\"start\":28867},{\"attributes\":{\"ref_id\":\"b90\"},\"end\":28883,\"start\":28879},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":29182,\"start\":29178},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":29195,\"start\":29191},{\"attributes\":{\"ref_id\":\"b157\"},\"end\":29209,\"start\":29204},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":29503,\"start\":29499},{\"attributes\":{\"ref_id\":\"b115\"},\"end\":29534,\"start\":29529},{\"attributes\":{\"ref_id\":\"b155\"},\"end\":29567,\"start\":29562},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":29802,\"start\":29798},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":30048,\"start\":30045},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":30082,\"start\":30078},{\"attributes\":{\"ref_id\":\"b174\"},\"end\":30190,\"start\":30185},{\"attributes\":{\"ref_id\":\"b109\"},\"end\":30669,\"start\":30664},{\"attributes\":{\"ref_id\":\"b110\"},\"end\":30673,\"start\":30669},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":30779,\"start\":30775},{\"attributes\":{\"ref_id\":\"b93\"},\"end\":31075,\"start\":31071},{\"attributes\":{\"ref_id\":\"b112\"},\"end\":31276,\"start\":31271},{\"attributes\":{\"ref_id\":\"b130\"},\"end\":31394,\"start\":31389},{\"attributes\":{\"ref_id\":\"b154\"},\"end\":31429,\"start\":31424},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":31489,\"start\":31485},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":31492,\"start\":31489},{\"attributes\":{\"ref_id\":\"b110\"},\"end\":31523,\"start\":31518},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":31552,\"start\":31548},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":31671,\"start\":31667},{\"attributes\":{\"ref_id\":\"b179\"},\"end\":31680,\"start\":31675},{\"attributes\":{\"ref_id\":\"b102\"},\"end\":31925,\"start\":31920},{\"attributes\":{\"ref_id\":\"b170\"},\"end\":31929,\"start\":31925},{\"attributes\":{\"ref_id\":\"b167\"},\"end\":32064,\"start\":32059},{\"attributes\":{\"ref_id\":\"b186\"},\"end\":32068,\"start\":32064},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":32190,\"start\":32186},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":32697,\"start\":32693},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":32988,\"start\":32984},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":33216,\"start\":33212},{\"attributes\":{\"ref_id\":\"b104\"},\"end\":33836,\"start\":33831},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":33879,\"start\":33875},{\"attributes\":{\"ref_id\":\"b111\"},\"end\":34549,\"start\":34544},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":34719,\"start\":34715},{\"attributes\":{\"ref_id\":\"b102\"},\"end\":34922,\"start\":34917},{\"attributes\":{\"ref_id\":\"b170\"},\"end\":35410,\"start\":35405},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":35802,\"start\":35798},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":36005,\"start\":36001},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":36431,\"start\":36427},{\"attributes\":{\"ref_id\":\"b186\"},\"end\":36962,\"start\":36957},{\"attributes\":{\"ref_id\":\"b167\"},\"end\":37576,\"start\":37571},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":39055,\"start\":39051},{\"attributes\":{\"ref_id\":\"b131\"},\"end\":39059,\"start\":39055},{\"attributes\":{\"ref_id\":\"b152\"},\"end\":39063,\"start\":39059},{\"attributes\":{\"ref_id\":\"b132\"},\"end\":39096,\"start\":39091},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":40867,\"start\":40863},{\"attributes\":{\"ref_id\":\"b138\"},\"end\":40911,\"start\":40906},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":41665,\"start\":41662},{\"attributes\":{\"ref_id\":\"b154\"},\"end\":41972,\"start\":41967},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":42653,\"start\":42649},{\"attributes\":{\"ref_id\":\"b142\"},\"end\":42950,\"start\":42945},{\"attributes\":{\"ref_id\":\"b128\"},\"end\":43180,\"start\":43175},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":43361,\"start\":43357},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":43364,\"start\":43361},{\"attributes\":{\"ref_id\":\"b154\"},\"end\":43824,\"start\":43819},{\"attributes\":{\"ref_id\":\"b154\"},\"end\":44133,\"start\":44128},{\"attributes\":{\"ref_id\":\"b137\"},\"end\":44566,\"start\":44561},{\"attributes\":{\"ref_id\":\"b138\"},\"end\":44570,\"start\":44566},{\"attributes\":{\"ref_id\":\"b140\"},\"end\":45081,\"start\":45076},{\"attributes\":{\"ref_id\":\"b137\"},\"end\":45113,\"start\":45108},{\"attributes\":{\"ref_id\":\"b139\"},\"end\":45629,\"start\":45624},{\"attributes\":{\"ref_id\":\"b121\"},\"end\":45711,\"start\":45706},{\"attributes\":{\"ref_id\":\"b152\"},\"end\":45861,\"start\":45856},{\"attributes\":{\"ref_id\":\"b131\"},\"end\":46284,\"start\":46279},{\"attributes\":{\"ref_id\":\"b132\"},\"end\":46659,\"start\":46654},{\"attributes\":{\"ref_id\":\"b135\"},\"end\":46808,\"start\":46803},{\"attributes\":{\"ref_id\":\"b136\"},\"end\":46812,\"start\":46808},{\"attributes\":{\"ref_id\":\"b118\"},\"end\":46846,\"start\":46841},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":47622,\"start\":47618},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":47636,\"start\":47632},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":47846,\"start\":47842},{\"attributes\":{\"ref_id\":\"b163\"},\"end\":48467,\"start\":48462},{\"attributes\":{\"ref_id\":\"b164\"},\"end\":48471,\"start\":48467},{\"attributes\":{\"ref_id\":\"b164\"},\"end\":49206,\"start\":49201},{\"attributes\":{\"ref_id\":\"b176\"},\"end\":49472,\"start\":49467},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":50147,\"start\":50143},{\"attributes\":{\"ref_id\":\"b134\"},\"end\":50338,\"start\":50333},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":50697,\"start\":50693},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":50700,\"start\":50697},{\"attributes\":{\"ref_id\":\"b127\"},\"end\":51404,\"start\":51399},{\"attributes\":{\"ref_id\":\"b178\"},\"end\":51588,\"start\":51583},{\"attributes\":{\"ref_id\":\"b96\"},\"end\":51743,\"start\":51739},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":51764,\"start\":51760},{\"attributes\":{\"ref_id\":\"b160\"},\"end\":52004,\"start\":51999},{\"attributes\":{\"ref_id\":\"b184\"},\"end\":52400,\"start\":52395},{\"attributes\":{\"ref_id\":\"b161\"},\"end\":53020,\"start\":53015},{\"attributes\":{\"ref_id\":\"b165\"},\"end\":53463,\"start\":53458},{\"attributes\":{\"ref_id\":\"b166\"},\"end\":53992,\"start\":53987},{\"attributes\":{\"ref_id\":\"b165\"},\"end\":54663,\"start\":54658},{\"attributes\":{\"ref_id\":\"b166\"},\"end\":54667,\"start\":54663},{\"attributes\":{\"ref_id\":\"b166\"},\"end\":55264,\"start\":55259},{\"attributes\":{\"ref_id\":\"b162\"},\"end\":55389,\"start\":55384},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":55619,\"start\":55615},{\"attributes\":{\"ref_id\":\"b155\"},\"end\":55623,\"start\":55619},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":56316,\"start\":56312},{\"attributes\":{\"ref_id\":\"b167\"},\"end\":57148,\"start\":57143},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":58584,\"start\":58580},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":59360,\"start\":59356},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":59500,\"start\":59497},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":59502,\"start\":59500},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":60187,\"start\":60186},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":61218,\"start\":61215},{\"attributes\":{\"ref_id\":\"b143\"},\"end\":61300,\"start\":61295},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":61778,\"start\":61774},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":61781,\"start\":61778},{\"attributes\":{\"ref_id\":\"b123\"},\"end\":62081,\"start\":62076},{\"attributes\":{\"ref_id\":\"b124\"},\"end\":62555,\"start\":62550},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":63283,\"start\":63280},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":63286,\"start\":63283},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":63306,\"start\":63302},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":63677,\"start\":63673},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":63686,\"start\":63682},{\"attributes\":{\"ref_id\":\"b153\"},\"end\":65231,\"start\":65226},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":65532,\"start\":65528},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":67042,\"start\":67038},{\"attributes\":{\"ref_id\":\"b153\"},\"end\":67046,\"start\":67042},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":67661,\"start\":67657},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":67999,\"start\":67995},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":68171,\"start\":68167},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":68797,\"start\":68793},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":68956,\"start\":68952},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":69125,\"start\":69121},{\"attributes\":{\"ref_id\":\"b119\"},\"end\":69161,\"start\":69156},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":69336,\"start\":69332},{\"attributes\":{\"ref_id\":\"b150\"},\"end\":70262,\"start\":70257},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":70856,\"start\":70852},{\"attributes\":{\"ref_id\":\"b92\"},\"end\":71092,\"start\":71088},{\"attributes\":{\"ref_id\":\"b113\"},\"end\":71370,\"start\":71365},{\"end\":71705,\"start\":71702},{\"attributes\":{\"ref_id\":\"b153\"},\"end\":71765,\"start\":71760},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":72652,\"start\":72648},{\"attributes\":{\"ref_id\":\"b175\"},\"end\":72720,\"start\":72715},{\"attributes\":{\"ref_id\":\"b122\"},\"end\":72910,\"start\":72905},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":73081,\"start\":73078},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":73337,\"start\":73333},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":73623,\"start\":73619},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":75521,\"start\":75517},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":76125,\"start\":76121},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":77018,\"start\":77014},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":77123,\"start\":77119},{\"attributes\":{\"ref_id\":\"b138\"},\"end\":78683,\"start\":78678},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":79247,\"start\":79243},{\"attributes\":{\"ref_id\":\"b138\"},\"end\":79495,\"start\":79490},{\"attributes\":{\"ref_id\":\"b131\"},\"end\":79644,\"start\":79639},{\"attributes\":{\"ref_id\":\"b132\"},\"end\":79654,\"start\":79649},{\"attributes\":{\"ref_id\":\"b138\"},\"end\":79964,\"start\":79959},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":80463,\"start\":80459},{\"attributes\":{\"ref_id\":\"b127\"},\"end\":80467,\"start\":80463},{\"attributes\":{\"ref_id\":\"b164\"},\"end\":80471,\"start\":80467},{\"attributes\":{\"ref_id\":\"b160\"},\"end\":80601,\"start\":80596},{\"attributes\":{\"ref_id\":\"b184\"},\"end\":80605,\"start\":80601},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":80891,\"start\":80887},{\"attributes\":{\"ref_id\":\"b176\"},\"end\":80895,\"start\":80891},{\"attributes\":{\"ref_id\":\"b178\"},\"end\":80899,\"start\":80895},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":81301,\"start\":81297},{\"attributes\":{\"ref_id\":\"b176\"},\"end\":81305,\"start\":81301},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":81320,\"start\":81316},{\"attributes\":{\"ref_id\":\"b184\"},\"end\":81324,\"start\":81320},{\"attributes\":{\"ref_id\":\"b127\"},\"end\":81348,\"start\":81343},{\"attributes\":{\"ref_id\":\"b178\"},\"end\":81352,\"start\":81348},{\"end\":82136,\"start\":82117},{\"attributes\":{\"ref_id\":\"b165\"},\"end\":82202,\"start\":82197},{\"end\":82554,\"start\":82544},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":82599,\"start\":82596},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":82685,\"start\":82682},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":82687,\"start\":82685},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":83045,\"start\":83041},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":83048,\"start\":83045},{\"attributes\":{\"ref_id\":\"b123\"},\"end\":83052,\"start\":83048},{\"attributes\":{\"ref_id\":\"b123\"},\"end\":83275,\"start\":83270},{\"attributes\":{\"ref_id\":\"b124\"},\"end\":83279,\"start\":83275},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":83601,\"start\":83597},{\"attributes\":{\"ref_id\":\"b113\"},\"end\":84747,\"start\":84742},{\"attributes\":{\"ref_id\":\"b150\"},\"end\":84751,\"start\":84747},{\"attributes\":{\"ref_id\":\"b105\"},\"end\":84824,\"start\":84819},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":85180,\"start\":85176},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":85183,\"start\":85180},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":85186,\"start\":85183},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":85623,\"start\":85619},{\"attributes\":{\"ref_id\":\"b95\"},\"end\":86601,\"start\":86597},{\"attributes\":{\"ref_id\":\"b167\"},\"end\":86809,\"start\":86804},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":86818,\"start\":86814},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":95436,\"start\":95432}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":89657,\"start\":89086},{\"attributes\":{\"id\":\"fig_1\"},\"end\":90100,\"start\":89658},{\"attributes\":{\"id\":\"fig_2\"},\"end\":90532,\"start\":90101},{\"attributes\":{\"id\":\"fig_3\"},\"end\":91022,\"start\":90533},{\"attributes\":{\"id\":\"fig_4\"},\"end\":91684,\"start\":91023},{\"attributes\":{\"id\":\"fig_5\"},\"end\":92144,\"start\":91685},{\"attributes\":{\"id\":\"fig_6\"},\"end\":92274,\"start\":92145},{\"attributes\":{\"id\":\"fig_7\"},\"end\":93382,\"start\":92275},{\"attributes\":{\"id\":\"fig_8\"},\"end\":93772,\"start\":93383},{\"attributes\":{\"id\":\"fig_9\"},\"end\":94227,\"start\":93773},{\"attributes\":{\"id\":\"fig_10\"},\"end\":95109,\"start\":94228},{\"attributes\":{\"id\":\"fig_11\"},\"end\":95361,\"start\":95110},{\"attributes\":{\"id\":\"fig_12\"},\"end\":95830,\"start\":95362},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":96748,\"start\":95831},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":96822,\"start\":96749},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":96948,\"start\":96823}]", "paragraph": "[{\"end\":11278,\"start\":10244},{\"end\":12330,\"start\":11301},{\"end\":13883,\"start\":12354},{\"end\":14393,\"start\":13925},{\"end\":15763,\"start\":14395},{\"end\":16441,\"start\":15765},{\"end\":16973,\"start\":16468},{\"end\":18452,\"start\":16979},{\"end\":19083,\"start\":18454},{\"end\":19962,\"start\":19085},{\"end\":20517,\"start\":19980},{\"end\":21504,\"start\":20519},{\"end\":22094,\"start\":21532},{\"end\":22738,\"start\":22096},{\"end\":25082,\"start\":22781},{\"end\":26437,\"start\":25239},{\"end\":27653,\"start\":26476},{\"end\":28396,\"start\":27683},{\"end\":29279,\"start\":28398},{\"end\":29804,\"start\":29281},{\"end\":30468,\"start\":29806},{\"end\":31681,\"start\":30470},{\"end\":32191,\"start\":31742},{\"end\":32339,\"start\":32193},{\"end\":33666,\"start\":32341},{\"end\":34289,\"start\":33668},{\"end\":35295,\"start\":34291},{\"end\":36196,\"start\":35297},{\"end\":36579,\"start\":36198},{\"end\":36665,\"start\":36660},{\"end\":37870,\"start\":36694},{\"end\":38946,\"start\":37918},{\"end\":39946,\"start\":38978},{\"end\":40709,\"start\":39979},{\"end\":41466,\"start\":40711},{\"end\":42333,\"start\":41468},{\"end\":42763,\"start\":42360},{\"end\":43556,\"start\":42765},{\"end\":43948,\"start\":43591},{\"end\":44027,\"start\":43994},{\"end\":44995,\"start\":44029},{\"end\":45839,\"start\":44997},{\"end\":46861,\"start\":45841},{\"end\":47831,\"start\":46886},{\"end\":49386,\"start\":47833},{\"end\":50637,\"start\":49388},{\"end\":51908,\"start\":50639},{\"end\":52618,\"start\":51910},{\"end\":53021,\"start\":52620},{\"end\":53993,\"start\":53035},{\"end\":54510,\"start\":54040},{\"end\":55265,\"start\":54544},{\"end\":55986,\"start\":55366},{\"end\":57117,\"start\":56037},{\"end\":58264,\"start\":57119},{\"end\":60120,\"start\":58299},{\"end\":60366,\"start\":60173},{\"end\":60560,\"start\":60458},{\"end\":60798,\"start\":60675},{\"end\":61003,\"start\":60866},{\"end\":62556,\"start\":61023},{\"end\":63287,\"start\":62569},{\"end\":64293,\"start\":63289},{\"end\":64972,\"start\":64342},{\"end\":65533,\"start\":64990},{\"end\":65755,\"start\":65535},{\"end\":66462,\"start\":66098},{\"end\":67475,\"start\":66481},{\"end\":67765,\"start\":67511},{\"end\":68587,\"start\":67848},{\"end\":68806,\"start\":68612},{\"end\":69443,\"start\":68808},{\"end\":70057,\"start\":69445},{\"end\":70546,\"start\":70240},{\"end\":71836,\"start\":70605},{\"end\":72653,\"start\":72043},{\"end\":74533,\"start\":72655},{\"end\":75522,\"start\":74557},{\"end\":76729,\"start\":75524},{\"end\":77443,\"start\":76731},{\"end\":77980,\"start\":77445},{\"end\":78859,\"start\":78012},{\"end\":79079,\"start\":78861},{\"end\":81451,\"start\":79117},{\"end\":82375,\"start\":81453},{\"end\":83335,\"start\":82410},{\"end\":84029,\"start\":83337},{\"end\":85478,\"start\":84078},{\"end\":86290,\"start\":85496},{\"end\":87156,\"start\":86292},{\"end\":87725,\"start\":87172},{\"end\":89085,\"start\":87727}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":16978,\"start\":16974},{\"attributes\":{\"id\":\"formula_1\"},\"end\":25238,\"start\":25083},{\"attributes\":{\"id\":\"formula_2\"},\"end\":36659,\"start\":36580},{\"attributes\":{\"id\":\"formula_3\"},\"end\":36693,\"start\":36666},{\"attributes\":{\"id\":\"formula_4\"},\"end\":42359,\"start\":42334},{\"attributes\":{\"id\":\"formula_5\"},\"end\":43590,\"start\":43557},{\"attributes\":{\"id\":\"formula_6\"},\"end\":43993,\"start\":43949},{\"attributes\":{\"id\":\"formula_7\"},\"end\":54039,\"start\":53994},{\"attributes\":{\"id\":\"formula_8\"},\"end\":54543,\"start\":54511},{\"attributes\":{\"id\":\"formula_9\"},\"end\":60440,\"start\":60367},{\"attributes\":{\"id\":\"formula_10\"},\"end\":60457,\"start\":60440},{\"attributes\":{\"id\":\"formula_11\"},\"end\":60674,\"start\":60561},{\"attributes\":{\"id\":\"formula_12\"},\"end\":60865,\"start\":60799},{\"attributes\":{\"id\":\"formula_13\"},\"end\":61022,\"start\":61004},{\"attributes\":{\"id\":\"formula_14\"},\"end\":66097,\"start\":65756},{\"attributes\":{\"id\":\"formula_15\"},\"end\":66480,\"start\":66463},{\"attributes\":{\"id\":\"formula_16\"},\"end\":67510,\"start\":67476},{\"attributes\":{\"id\":\"formula_17\"},\"end\":67847,\"start\":67766},{\"attributes\":{\"id\":\"formula_18\"},\"end\":68611,\"start\":68588},{\"attributes\":{\"id\":\"formula_19\"},\"end\":70239,\"start\":70058},{\"attributes\":{\"id\":\"formula_20\"},\"end\":70604,\"start\":70547},{\"attributes\":{\"id\":\"formula_21\"},\"end\":71875,\"start\":71837},{\"attributes\":{\"id\":\"formula_22\"},\"end\":72042,\"start\":71875},{\"attributes\":{\"id\":\"formula_23\"},\"end\":79116,\"start\":79080}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":13844,\"start\":13837},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":39928,\"start\":39921},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":59337,\"start\":59330}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":10242,\"start\":10230},{\"attributes\":{\"n\":\"3\"},\"end\":11299,\"start\":11281},{\"attributes\":{\"n\":\"3.1\"},\"end\":12352,\"start\":12333},{\"attributes\":{\"n\":\"3.1.1\"},\"end\":13923,\"start\":13886},{\"attributes\":{\"n\":\"3.1.2\"},\"end\":16466,\"start\":16444},{\"attributes\":{\"n\":\"3.1.3\"},\"end\":19978,\"start\":19965},{\"attributes\":{\"n\":\"3.1.4\"},\"end\":21530,\"start\":21507},{\"attributes\":{\"n\":\"3.1.5\"},\"end\":22779,\"start\":22741},{\"attributes\":{\"n\":\"3.2\"},\"end\":26474,\"start\":26440},{\"attributes\":{\"n\":\"3.2.1\"},\"end\":27681,\"start\":27656},{\"attributes\":{\"n\":\"3.2.2\"},\"end\":31740,\"start\":31684},{\"attributes\":{\"n\":\"4\"},\"end\":37916,\"start\":37873},{\"attributes\":{\"n\":\"4.1\"},\"end\":38976,\"start\":38949},{\"attributes\":{\"n\":\"4.1.1\"},\"end\":39977,\"start\":39949},{\"attributes\":{\"n\":\"4.1.2\"},\"end\":46884,\"start\":46864},{\"attributes\":{\"n\":\"4.1.3\"},\"end\":53033,\"start\":53024},{\"end\":55364,\"start\":55268},{\"attributes\":{\"n\":\"4.1.4\"},\"end\":56035,\"start\":55989},{\"attributes\":{\"n\":\"4.2\"},\"end\":58297,\"start\":58267},{\"attributes\":{\"n\":\"4.2.1\"},\"end\":60171,\"start\":60123},{\"attributes\":{\"n\":\"4.2.2\"},\"end\":62567,\"start\":62559},{\"attributes\":{\"n\":\"5\"},\"end\":64340,\"start\":64296},{\"attributes\":{\"n\":\"5.1\"},\"end\":64988,\"start\":64975},{\"attributes\":{\"n\":\"6.1\"},\"end\":74555,\"start\":74536},{\"attributes\":{\"n\":\"6.2\"},\"end\":78010,\"start\":77983},{\"attributes\":{\"n\":\"6.3\"},\"end\":82408,\"start\":82378},{\"attributes\":{\"n\":\"6.4\"},\"end\":84076,\"start\":84032},{\"attributes\":{\"n\":\"6.5\"},\"end\":85494,\"start\":85481},{\"attributes\":{\"n\":\"7\"},\"end\":87170,\"start\":87159},{\"end\":89095,\"start\":89087},{\"end\":89667,\"start\":89659},{\"end\":90110,\"start\":90102},{\"end\":90542,\"start\":90534},{\"end\":91025,\"start\":91024},{\"end\":91694,\"start\":91686},{\"end\":92154,\"start\":92146},{\"end\":92277,\"start\":92276},{\"end\":93392,\"start\":93384},{\"end\":93782,\"start\":93774},{\"end\":94230,\"start\":94229},{\"end\":95117,\"start\":95111},{\"end\":95841,\"start\":95832},{\"end\":96759,\"start\":96750},{\"end\":96833,\"start\":96824}]", "table": "[{\"end\":96748,\"start\":95897}]", "figure_caption": "[{\"end\":89657,\"start\":89097},{\"end\":90100,\"start\":89669},{\"end\":90532,\"start\":90112},{\"end\":91022,\"start\":90544},{\"end\":91684,\"start\":91026},{\"end\":92144,\"start\":91696},{\"end\":92274,\"start\":92156},{\"end\":93382,\"start\":92278},{\"end\":93772,\"start\":93394},{\"end\":94227,\"start\":93784},{\"end\":95109,\"start\":94231},{\"end\":95361,\"start\":95119},{\"end\":95830,\"start\":95364},{\"end\":95897,\"start\":95843},{\"end\":96822,\"start\":96761},{\"end\":96948,\"start\":96835}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":11432,\"start\":11424},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":16812,\"start\":16804},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":16973,\"start\":16965},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":38084,\"start\":38076},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":41461,\"start\":41453},{\"end\":44227,\"start\":44210},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":47356,\"start\":47348},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":53774,\"start\":53766},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":59148,\"start\":59140},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":60493,\"start\":60485},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":63928,\"start\":63920},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":64693,\"start\":64685}]", "bib_author_first_name": "[{\"end\":97475,\"start\":97474},{\"end\":97491,\"start\":97485},{\"end\":97493,\"start\":97492},{\"end\":98011,\"start\":98007},{\"end\":98026,\"start\":98020},{\"end\":98038,\"start\":98033},{\"end\":98052,\"start\":98047},{\"end\":98394,\"start\":98389},{\"end\":98396,\"start\":98395},{\"end\":98415,\"start\":98411},{\"end\":98417,\"start\":98416},{\"end\":98431,\"start\":98425},{\"end\":98445,\"start\":98441},{\"end\":98447,\"start\":98446},{\"end\":98860,\"start\":98856},{\"end\":98874,\"start\":98869},{\"end\":99266,\"start\":99262},{\"end\":99280,\"start\":99275},{\"end\":99605,\"start\":99593},{\"end\":99621,\"start\":99614},{\"end\":99853,\"start\":99846},{\"end\":99866,\"start\":99859},{\"end\":99877,\"start\":99872},{\"end\":99893,\"start\":99890},{\"end\":100142,\"start\":100138},{\"end\":100144,\"start\":100143},{\"end\":100162,\"start\":100156},{\"end\":100179,\"start\":100174},{\"end\":100181,\"start\":100180},{\"end\":100416,\"start\":100407},{\"end\":100436,\"start\":100429},{\"end\":100451,\"start\":100444},{\"end\":100469,\"start\":100460},{\"end\":100489,\"start\":100482},{\"end\":100497,\"start\":100492},{\"end\":100841,\"start\":100832},{\"end\":101103,\"start\":101095},{\"end\":101105,\"start\":101104},{\"end\":101117,\"start\":101113},{\"end\":101128,\"start\":101118},{\"end\":101343,\"start\":101334},{\"end\":101358,\"start\":101353},{\"end\":101376,\"start\":101369},{\"end\":101672,\"start\":101665},{\"end\":101674,\"start\":101673},{\"end\":101695,\"start\":101689},{\"end\":101713,\"start\":101706},{\"end\":101727,\"start\":101721},{\"end\":101745,\"start\":101742},{\"end\":102119,\"start\":102110},{\"end\":102135,\"start\":102129},{\"end\":102362,\"start\":102361},{\"end\":102364,\"start\":102363},{\"end\":102385,\"start\":102378},{\"end\":102660,\"start\":102653},{\"end\":102678,\"start\":102671},{\"end\":102697,\"start\":102688},{\"end\":102713,\"start\":102707},{\"end\":102999,\"start\":102993},{\"end\":103001,\"start\":103000},{\"end\":103015,\"start\":103010},{\"end\":103028,\"start\":103023},{\"end\":103030,\"start\":103029},{\"end\":103327,\"start\":103326},{\"end\":103343,\"start\":103337},{\"end\":103356,\"start\":103352},{\"end\":103358,\"start\":103357},{\"end\":103642,\"start\":103636},{\"end\":103653,\"start\":103649},{\"end\":103895,\"start\":103890},{\"end\":103910,\"start\":103904},{\"end\":103925,\"start\":103918},{\"end\":103942,\"start\":103936},{\"end\":103951,\"start\":103950},{\"end\":103967,\"start\":103957},{\"end\":104287,\"start\":104281},{\"end\":104298,\"start\":104294},{\"end\":104619,\"start\":104614},{\"end\":104633,\"start\":104627},{\"end\":104906,\"start\":104902},{\"end\":104922,\"start\":104917},{\"end\":105159,\"start\":105154},{\"end\":105171,\"start\":105166},{\"end\":105449,\"start\":105445},{\"end\":105463,\"start\":105459},{\"end\":105732,\"start\":105726},{\"end\":105746,\"start\":105738},{\"end\":105757,\"start\":105751},{\"end\":106022,\"start\":106015},{\"end\":106033,\"start\":106029},{\"end\":106313,\"start\":106307},{\"end\":106315,\"start\":106314},{\"end\":106561,\"start\":106555},{\"end\":106578,\"start\":106570},{\"end\":106593,\"start\":106586},{\"end\":106609,\"start\":106606},{\"end\":106612,\"start\":106610},{\"end\":106886,\"start\":106880},{\"end\":106903,\"start\":106895},{\"end\":106918,\"start\":106911},{\"end\":106934,\"start\":106931},{\"end\":106937,\"start\":106935},{\"end\":107242,\"start\":107236},{\"end\":107257,\"start\":107251},{\"end\":107277,\"start\":107271},{\"end\":107492,\"start\":107486},{\"end\":107513,\"start\":107506},{\"end\":107528,\"start\":107524},{\"end\":107540,\"start\":107534},{\"end\":107555,\"start\":107550},{\"end\":107574,\"start\":107566},{\"end\":107590,\"start\":107583},{\"end\":107606,\"start\":107600},{\"end\":107620,\"start\":107614},{\"end\":107981,\"start\":107976},{\"end\":107997,\"start\":107993},{\"end\":108272,\"start\":108267},{\"end\":108288,\"start\":108284},{\"end\":108546,\"start\":108541},{\"end\":108560,\"start\":108554},{\"end\":108572,\"start\":108566},{\"end\":108845,\"start\":108844},{\"end\":108860,\"start\":108854},{\"end\":108862,\"start\":108861},{\"end\":109149,\"start\":109141},{\"end\":109168,\"start\":109163},{\"end\":109184,\"start\":109179},{\"end\":109201,\"start\":109193},{\"end\":109547,\"start\":109538},{\"end\":109567,\"start\":109561},{\"end\":109844,\"start\":109839},{\"end\":109866,\"start\":109862},{\"end\":109886,\"start\":109882},{\"end\":110185,\"start\":110179},{\"end\":110195,\"start\":110194},{\"end\":110207,\"start\":110203},{\"end\":110209,\"start\":110208},{\"end\":110476,\"start\":110472},{\"end\":110491,\"start\":110487},{\"end\":110496,\"start\":110492},{\"end\":110511,\"start\":110502},{\"end\":110522,\"start\":110518},{\"end\":110526,\"start\":110523},{\"end\":110861,\"start\":110855},{\"end\":110885,\"start\":110878},{\"end\":111107,\"start\":111106},{\"end\":111109,\"start\":111108},{\"end\":111298,\"start\":111291},{\"end\":111313,\"start\":111307},{\"end\":111332,\"start\":111323},{\"end\":111586,\"start\":111580},{\"end\":111597,\"start\":111592},{\"end\":111613,\"start\":111607},{\"end\":111630,\"start\":111624},{\"end\":111863,\"start\":111856},{\"end\":111882,\"start\":111874},{\"end\":112118,\"start\":112111},{\"end\":112132,\"start\":112126},{\"end\":112151,\"start\":112144},{\"end\":112448,\"start\":112442},{\"end\":112458,\"start\":112454},{\"end\":112749,\"start\":112742},{\"end\":113050,\"start\":113049},{\"end\":113052,\"start\":113051},{\"end\":113061,\"start\":113060},{\"end\":113063,\"start\":113062},{\"end\":113073,\"start\":113072},{\"end\":113086,\"start\":113085},{\"end\":113097,\"start\":113096},{\"end\":113108,\"start\":113106},{\"end\":113118,\"start\":113117},{\"end\":113128,\"start\":113127},{\"end\":113472,\"start\":113465},{\"end\":113480,\"start\":113478},{\"end\":113489,\"start\":113486},{\"end\":113509,\"start\":113501},{\"end\":113519,\"start\":113515},{\"end\":113531,\"start\":113524},{\"end\":113533,\"start\":113532},{\"end\":113799,\"start\":113798},{\"end\":114032,\"start\":114029},{\"end\":114043,\"start\":114038},{\"end\":114282,\"start\":114277},{\"end\":114297,\"start\":114290},{\"end\":114314,\"start\":114307},{\"end\":114332,\"start\":114327},{\"end\":114344,\"start\":114340},{\"end\":114361,\"start\":114355},{\"end\":114614,\"start\":114609},{\"end\":114627,\"start\":114623},{\"end\":114800,\"start\":114795},{\"end\":114813,\"start\":114809},{\"end\":115023,\"start\":115016},{\"end\":115041,\"start\":115033},{\"end\":115311,\"start\":115304},{\"end\":115327,\"start\":115321},{\"end\":115507,\"start\":115500},{\"end\":115509,\"start\":115508},{\"end\":115524,\"start\":115519},{\"end\":115797,\"start\":115790},{\"end\":115813,\"start\":115805},{\"end\":116112,\"start\":116111},{\"end\":116114,\"start\":116113},{\"end\":116130,\"start\":116125},{\"end\":116132,\"start\":116131},{\"end\":116362,\"start\":116356},{\"end\":116377,\"start\":116371},{\"end\":116379,\"start\":116378},{\"end\":116395,\"start\":116390},{\"end\":116407,\"start\":116403},{\"end\":116423,\"start\":116415},{\"end\":116759,\"start\":116753},{\"end\":116761,\"start\":116760},{\"end\":116773,\"start\":116769},{\"end\":116775,\"start\":116774},{\"end\":117028,\"start\":117021},{\"end\":117274,\"start\":117270},{\"end\":117288,\"start\":117280},{\"end\":117302,\"start\":117296},{\"end\":117318,\"start\":117311},{\"end\":117333,\"start\":117327},{\"end\":117353,\"start\":117347},{\"end\":117711,\"start\":117710},{\"end\":117726,\"start\":117719},{\"end\":117969,\"start\":117963},{\"end\":117985,\"start\":117977},{\"end\":118003,\"start\":117999},{\"end\":118246,\"start\":118240},{\"end\":118256,\"start\":118247},{\"end\":118271,\"start\":118267},{\"end\":118481,\"start\":118475},{\"end\":118491,\"start\":118482},{\"end\":118506,\"start\":118502},{\"end\":118760,\"start\":118753},{\"end\":118958,\"start\":118951},{\"end\":119215,\"start\":119208},{\"end\":119233,\"start\":119226},{\"end\":119498,\"start\":119494},{\"end\":119515,\"start\":119511},{\"end\":119532,\"start\":119525},{\"end\":119546,\"start\":119539},{\"end\":119562,\"start\":119554},{\"end\":119880,\"start\":119875},{\"end\":119893,\"start\":119888},{\"end\":120131,\"start\":120126},{\"end\":120144,\"start\":120139},{\"end\":120375,\"start\":120368},{\"end\":120389,\"start\":120383},{\"end\":120584,\"start\":120577},{\"end\":120598,\"start\":120592},{\"end\":120863,\"start\":120859},{\"end\":120880,\"start\":120876},{\"end\":120900,\"start\":120892},{\"end\":120902,\"start\":120901},{\"end\":121167,\"start\":121159},{\"end\":121181,\"start\":121175},{\"end\":121195,\"start\":121187},{\"end\":121401,\"start\":121395},{\"end\":121419,\"start\":121412},{\"end\":121435,\"start\":121430},{\"end\":121450,\"start\":121446},{\"end\":121468,\"start\":121461},{\"end\":121760,\"start\":121753},{\"end\":121769,\"start\":121768},{\"end\":121780,\"start\":121779},{\"end\":121782,\"start\":121781},{\"end\":122125,\"start\":122118},{\"end\":122134,\"start\":122133},{\"end\":122145,\"start\":122144},{\"end\":122147,\"start\":122146},{\"end\":122466,\"start\":122459},{\"end\":122475,\"start\":122474},{\"end\":122493,\"start\":122485},{\"end\":122835,\"start\":122832},{\"end\":122852,\"start\":122843},{\"end\":122873,\"start\":122864},{\"end\":122895,\"start\":122887},{\"end\":122911,\"start\":122905},{\"end\":123210,\"start\":123209},{\"end\":123226,\"start\":123217},{\"end\":123237,\"start\":123231},{\"end\":123254,\"start\":123248},{\"end\":123256,\"start\":123255},{\"end\":123556,\"start\":123555},{\"end\":123580,\"start\":123563},{\"end\":123590,\"start\":123585},{\"end\":123608,\"start\":123600},{\"end\":123619,\"start\":123616},{\"end\":123631,\"start\":123627},{\"end\":123633,\"start\":123632},{\"end\":123644,\"start\":123640},{\"end\":123660,\"start\":123654},{\"end\":123662,\"start\":123661},{\"end\":123951,\"start\":123947},{\"end\":123965,\"start\":123959},{\"end\":123982,\"start\":123974},{\"end\":124211,\"start\":124202},{\"end\":124230,\"start\":124226},{\"end\":124243,\"start\":124238},{\"end\":124638,\"start\":124629},{\"end\":124657,\"start\":124653},{\"end\":124670,\"start\":124665},{\"end\":125005,\"start\":124999},{\"end\":125028,\"start\":125019},{\"end\":125041,\"start\":125035},{\"end\":125043,\"start\":125042},{\"end\":125316,\"start\":125310},{\"end\":125334,\"start\":125330},{\"end\":125351,\"start\":125344},{\"end\":125369,\"start\":125360},{\"end\":125380,\"start\":125376},{\"end\":125397,\"start\":125391},{\"end\":125705,\"start\":125701},{\"end\":125717,\"start\":125710},{\"end\":125733,\"start\":125726},{\"end\":125749,\"start\":125745},{\"end\":126040,\"start\":126036},{\"end\":126053,\"start\":126046},{\"end\":126065,\"start\":126059},{\"end\":126336,\"start\":126326},{\"end\":126680,\"start\":126670},{\"end\":127004,\"start\":126996},{\"end\":127018,\"start\":127010},{\"end\":127033,\"start\":127024},{\"end\":127041,\"start\":127039},{\"end\":127051,\"start\":127047},{\"end\":127058,\"start\":127056},{\"end\":127379,\"start\":127371},{\"end\":127390,\"start\":127386},{\"end\":127408,\"start\":127402},{\"end\":127694,\"start\":127689},{\"end\":127696,\"start\":127695},{\"end\":127931,\"start\":127930},{\"end\":127944,\"start\":127939},{\"end\":128252,\"start\":128244},{\"end\":128264,\"start\":128260},{\"end\":128276,\"start\":128270},{\"end\":128293,\"start\":128286},{\"end\":128309,\"start\":128303},{\"end\":128325,\"start\":128319},{\"end\":128345,\"start\":128339},{\"end\":128744,\"start\":128733},{\"end\":128754,\"start\":128750},{\"end\":128767,\"start\":128763},{\"end\":128781,\"start\":128777},{\"end\":128793,\"start\":128790},{\"end\":129093,\"start\":129085},{\"end\":129108,\"start\":129104},{\"end\":129123,\"start\":129119},{\"end\":129136,\"start\":129133},{\"end\":129443,\"start\":129437},{\"end\":129461,\"start\":129454},{\"end\":129480,\"start\":129473},{\"end\":129498,\"start\":129492},{\"end\":129518,\"start\":129510},{\"end\":129520,\"start\":129519},{\"end\":129954,\"start\":129948},{\"end\":129969,\"start\":129964},{\"end\":129986,\"start\":129979},{\"end\":129998,\"start\":129994},{\"end\":130010,\"start\":130006},{\"end\":130028,\"start\":130019},{\"end\":130358,\"start\":130349},{\"end\":130372,\"start\":130367},{\"end\":130680,\"start\":130674},{\"end\":130695,\"start\":130689},{\"end\":130711,\"start\":130705},{\"end\":130994,\"start\":130987},{\"end\":131012,\"start\":131006},{\"end\":131030,\"start\":131024},{\"end\":131044,\"start\":131038},{\"end\":131062,\"start\":131055},{\"end\":131333,\"start\":131326},{\"end\":131351,\"start\":131345},{\"end\":131369,\"start\":131363},{\"end\":131383,\"start\":131377},{\"end\":131401,\"start\":131394},{\"end\":131721,\"start\":131714},{\"end\":131739,\"start\":131733},{\"end\":131757,\"start\":131751},{\"end\":131771,\"start\":131765},{\"end\":131789,\"start\":131782},{\"end\":132085,\"start\":132078},{\"end\":132103,\"start\":132097},{\"end\":132121,\"start\":132115},{\"end\":132135,\"start\":132129},{\"end\":132153,\"start\":132146},{\"end\":132481,\"start\":132476},{\"end\":132497,\"start\":132490},{\"end\":132769,\"start\":132765},{\"end\":132782,\"start\":132781},{\"end\":132786,\"start\":132783},{\"end\":132800,\"start\":132796},{\"end\":132802,\"start\":132801},{\"end\":133089,\"start\":133084},{\"end\":133107,\"start\":133100},{\"end\":133124,\"start\":133114},{\"end\":133136,\"start\":133130},{\"end\":133443,\"start\":133436},{\"end\":133445,\"start\":133444},{\"end\":133461,\"start\":133456},{\"end\":133478,\"start\":133473},{\"end\":133490,\"start\":133484},{\"end\":133492,\"start\":133491},{\"end\":133507,\"start\":133502},{\"end\":133522,\"start\":133517},{\"end\":133537,\"start\":133531},{\"end\":133557,\"start\":133550},{\"end\":133570,\"start\":133565},{\"end\":133586,\"start\":133581},{\"end\":133603,\"start\":133598},{\"end\":133615,\"start\":133609},{\"end\":133617,\"start\":133616},{\"end\":133635,\"start\":133627},{\"end\":134124,\"start\":134119},{\"end\":134344,\"start\":134339},{\"end\":134357,\"start\":134353},{\"end\":134375,\"start\":134370},{\"end\":134631,\"start\":134627},{\"end\":134841,\"start\":134840},{\"end\":134850,\"start\":134849},{\"end\":135048,\"start\":135040},{\"end\":135320,\"start\":135316},{\"end\":135344,\"start\":135341},{\"end\":135358,\"start\":135355},{\"end\":135676,\"start\":135675},{\"end\":135690,\"start\":135684},{\"end\":135701,\"start\":135698},{\"end\":136019,\"start\":136014},{\"end\":136037,\"start\":136030},{\"end\":136041,\"start\":136038},{\"end\":136052,\"start\":136047},{\"end\":136067,\"start\":136061},{\"end\":136079,\"start\":136075},{\"end\":136095,\"start\":136088},{\"end\":136484,\"start\":136480},{\"end\":136488,\"start\":136485},{\"end\":136502,\"start\":136495},{\"end\":136518,\"start\":136514},{\"end\":136534,\"start\":136529},{\"end\":136809,\"start\":136805},{\"end\":136813,\"start\":136810},{\"end\":136827,\"start\":136820},{\"end\":136843,\"start\":136839},{\"end\":136859,\"start\":136854},{\"end\":137124,\"start\":137117},{\"end\":137420,\"start\":137415},{\"end\":137439,\"start\":137433},{\"end\":137453,\"start\":137448},{\"end\":137470,\"start\":137465},{\"end\":137484,\"start\":137479},{\"end\":137499,\"start\":137494},{\"end\":137978,\"start\":137971},{\"end\":137991,\"start\":137984},{\"end\":138002,\"start\":137998},{\"end\":138012,\"start\":138010},{\"end\":138337,\"start\":138332},{\"end\":138561,\"start\":138555},{\"end\":138573,\"start\":138570},{\"end\":138795,\"start\":138790},{\"end\":138811,\"start\":138804},{\"end\":138824,\"start\":138820},{\"end\":138839,\"start\":138835},{\"end\":139139,\"start\":139135},{\"end\":139157,\"start\":139151},{\"end\":139457,\"start\":139453},{\"end\":139475,\"start\":139469},{\"end\":139771,\"start\":139766},{\"end\":139795,\"start\":139787},{\"end\":139817,\"start\":139805},{\"end\":140188,\"start\":140187},{\"end\":140202,\"start\":140199},{\"end\":140204,\"start\":140203},{\"end\":140522,\"start\":140516},{\"end\":140817,\"start\":140811},{\"end\":140839,\"start\":140830},{\"end\":140859,\"start\":140853},{\"end\":141153,\"start\":141147},{\"end\":141170,\"start\":141165},{\"end\":141472,\"start\":141466},{\"end\":141489,\"start\":141484},{\"end\":141811,\"start\":141805},{\"end\":141828,\"start\":141823},{\"end\":141841,\"start\":141836},{\"end\":142134,\"start\":142128},{\"end\":142151,\"start\":142146},{\"end\":142153,\"start\":142152},{\"end\":142161,\"start\":142156},{\"end\":142405,\"start\":142397},{\"end\":142435,\"start\":142424},{\"end\":142673,\"start\":142667},{\"end\":142839,\"start\":142834},{\"end\":142853,\"start\":142849},{\"end\":142869,\"start\":142862},{\"end\":143117,\"start\":143113},{\"end\":143137,\"start\":143126},{\"end\":143146,\"start\":143143},{\"end\":143157,\"start\":143153},{\"end\":143437,\"start\":143432},{\"end\":143454,\"start\":143448},{\"end\":143703,\"start\":143699},{\"end\":143719,\"start\":143713},{\"end\":143734,\"start\":143727},{\"end\":143964,\"start\":143960},{\"end\":143980,\"start\":143974},{\"end\":143982,\"start\":143981},{\"end\":143997,\"start\":143990},{\"end\":144315,\"start\":144311},{\"end\":144633,\"start\":144628},{\"end\":144645,\"start\":144644},{\"end\":144649,\"start\":144646},{\"end\":144665,\"start\":144659},{\"end\":144667,\"start\":144666},{\"end\":144902,\"start\":144897},{\"end\":144914,\"start\":144910},{\"end\":145166,\"start\":145163},{\"end\":145178,\"start\":145172},{\"end\":145190,\"start\":145184},{\"end\":145204,\"start\":145197},{\"end\":145217,\"start\":145212},{\"end\":145504,\"start\":145499},{\"end\":145518,\"start\":145514},{\"end\":145530,\"start\":145524},{\"end\":145843,\"start\":145838},{\"end\":145857,\"start\":145852},{\"end\":146102,\"start\":146101},{\"end\":146104,\"start\":146103},{\"end\":146387,\"start\":146386},{\"end\":146389,\"start\":146388},{\"end\":146404,\"start\":146398},{\"end\":146656,\"start\":146655},{\"end\":146658,\"start\":146657},{\"end\":146673,\"start\":146667},{\"end\":146935,\"start\":146934},{\"end\":146937,\"start\":146936},{\"end\":146952,\"start\":146946},{\"end\":147253,\"start\":147246},{\"end\":147264,\"start\":147260},{\"end\":147539,\"start\":147533},{\"end\":147561,\"start\":147550},{\"end\":147574,\"start\":147568},{\"end\":147594,\"start\":147586},{\"end\":147871,\"start\":147867},{\"end\":148042,\"start\":148038},{\"end\":148213,\"start\":148209},{\"end\":148228,\"start\":148221},{\"end\":148479,\"start\":148475},{\"end\":148489,\"start\":148487},{\"end\":148501,\"start\":148494},{\"end\":148799,\"start\":148795},{\"end\":148809,\"start\":148807},{\"end\":148821,\"start\":148814},{\"end\":149079,\"start\":149075},{\"end\":149089,\"start\":149087},{\"end\":149101,\"start\":149094},{\"end\":149117,\"start\":149110},{\"end\":149369,\"start\":149365},{\"end\":149384,\"start\":149377},{\"end\":149395,\"start\":149393},{\"end\":149407,\"start\":149400},{\"end\":149633,\"start\":149623},{\"end\":149659,\"start\":149652},{\"end\":149675,\"start\":149667},{\"end\":149689,\"start\":149684},{\"end\":149710,\"start\":149702},{\"end\":150076,\"start\":150066},{\"end\":150088,\"start\":150083},{\"end\":150380,\"start\":150370},{\"end\":150394,\"start\":150387},{\"end\":150404,\"start\":150403},{\"end\":150423,\"start\":150422},{\"end\":150760,\"start\":150757},{\"end\":150773,\"start\":150767},{\"end\":150788,\"start\":150781},{\"end\":150798,\"start\":150794},{\"end\":151107,\"start\":151103},{\"end\":151112,\"start\":151108},{\"end\":151136,\"start\":151128},{\"end\":151424,\"start\":151417},{\"end\":151445,\"start\":151440},{\"end\":151447,\"start\":151446},{\"end\":151702,\"start\":151692},{\"end\":151873,\"start\":151863},{\"end\":151884,\"start\":151878},{\"end\":151899,\"start\":151894},{\"end\":151915,\"start\":151909},{\"end\":151917,\"start\":151916},{\"end\":152219,\"start\":152215},{\"end\":152235,\"start\":152226},{\"end\":152247,\"start\":152242},{\"end\":152557,\"start\":152551},{\"end\":152567,\"start\":152563},{\"end\":152906,\"start\":152900},{\"end\":152916,\"start\":152912},{\"end\":153240,\"start\":153232},{\"end\":153253,\"start\":153247},{\"end\":153268,\"start\":153264},{\"end\":153509,\"start\":153502},{\"end\":153524,\"start\":153518},{\"end\":153536,\"start\":153532},{\"end\":153785,\"start\":153779},{\"end\":153801,\"start\":153794},{\"end\":153821,\"start\":153817},{\"end\":154110,\"start\":154106},{\"end\":154128,\"start\":154121},{\"end\":154132,\"start\":154129},{\"end\":154144,\"start\":154138},{\"end\":154159,\"start\":154152},{\"end\":154433,\"start\":154426},{\"end\":154448,\"start\":154442},{\"end\":154464,\"start\":154459},{\"end\":154471,\"start\":154470},{\"end\":154739,\"start\":154735},{\"end\":154751,\"start\":154747},{\"end\":154962,\"start\":154958},{\"end\":154976,\"start\":154970},{\"end\":154989,\"start\":154984},{\"end\":155257,\"start\":155250},{\"end\":155267,\"start\":155265},{\"end\":155281,\"start\":155274},{\"end\":155291,\"start\":155288},{\"end\":155299,\"start\":155292},{\"end\":155566,\"start\":155559},{\"end\":155580,\"start\":155573},{\"end\":155592,\"start\":155588},{\"end\":155607,\"start\":155602},{\"end\":155609,\"start\":155608}]", "bib_author_last_name": "[{\"end\":97483,\"start\":97476},{\"end\":97500,\"start\":97494},{\"end\":97508,\"start\":97502},{\"end\":97796,\"start\":97781},{\"end\":98018,\"start\":98012},{\"end\":98031,\"start\":98027},{\"end\":98045,\"start\":98039},{\"end\":98059,\"start\":98053},{\"end\":98409,\"start\":98397},{\"end\":98423,\"start\":98418},{\"end\":98439,\"start\":98432},{\"end\":98455,\"start\":98448},{\"end\":98867,\"start\":98861},{\"end\":98882,\"start\":98875},{\"end\":99273,\"start\":99267},{\"end\":99288,\"start\":99281},{\"end\":99612,\"start\":99606},{\"end\":99635,\"start\":99622},{\"end\":99643,\"start\":99637},{\"end\":99857,\"start\":99854},{\"end\":99870,\"start\":99867},{\"end\":99888,\"start\":99878},{\"end\":99902,\"start\":99894},{\"end\":100154,\"start\":100145},{\"end\":100172,\"start\":100163},{\"end\":100188,\"start\":100182},{\"end\":100427,\"start\":100417},{\"end\":100442,\"start\":100437},{\"end\":100458,\"start\":100452},{\"end\":100480,\"start\":100470},{\"end\":100505,\"start\":100498},{\"end\":100849,\"start\":100842},{\"end\":101111,\"start\":101106},{\"end\":101134,\"start\":101129},{\"end\":101351,\"start\":101344},{\"end\":101367,\"start\":101359},{\"end\":101385,\"start\":101377},{\"end\":101687,\"start\":101675},{\"end\":101704,\"start\":101696},{\"end\":101719,\"start\":101714},{\"end\":101740,\"start\":101728},{\"end\":101754,\"start\":101746},{\"end\":102127,\"start\":102120},{\"end\":102139,\"start\":102136},{\"end\":102376,\"start\":102365},{\"end\":102391,\"start\":102386},{\"end\":102400,\"start\":102393},{\"end\":102669,\"start\":102661},{\"end\":102686,\"start\":102679},{\"end\":102705,\"start\":102698},{\"end\":102717,\"start\":102714},{\"end\":103008,\"start\":103002},{\"end\":103021,\"start\":103016},{\"end\":103037,\"start\":103031},{\"end\":103335,\"start\":103328},{\"end\":103350,\"start\":103344},{\"end\":103362,\"start\":103359},{\"end\":103371,\"start\":103364},{\"end\":103647,\"start\":103643},{\"end\":103658,\"start\":103654},{\"end\":103902,\"start\":103896},{\"end\":103916,\"start\":103911},{\"end\":103934,\"start\":103926},{\"end\":103948,\"start\":103943},{\"end\":103955,\"start\":103952},{\"end\":103975,\"start\":103968},{\"end\":104292,\"start\":104288},{\"end\":104304,\"start\":104299},{\"end\":104625,\"start\":104620},{\"end\":104638,\"start\":104634},{\"end\":104915,\"start\":104907},{\"end\":104929,\"start\":104923},{\"end\":105164,\"start\":105160},{\"end\":105180,\"start\":105172},{\"end\":105188,\"start\":105182},{\"end\":105457,\"start\":105450},{\"end\":105470,\"start\":105464},{\"end\":105736,\"start\":105733},{\"end\":105749,\"start\":105747},{\"end\":105760,\"start\":105758},{\"end\":106027,\"start\":106023},{\"end\":106037,\"start\":106034},{\"end\":106323,\"start\":106316},{\"end\":106568,\"start\":106562},{\"end\":106584,\"start\":106579},{\"end\":106604,\"start\":106594},{\"end\":106622,\"start\":106613},{\"end\":106893,\"start\":106887},{\"end\":106909,\"start\":106904},{\"end\":106929,\"start\":106919},{\"end\":106947,\"start\":106938},{\"end\":107249,\"start\":107243},{\"end\":107269,\"start\":107258},{\"end\":107288,\"start\":107278},{\"end\":107504,\"start\":107493},{\"end\":107522,\"start\":107514},{\"end\":107532,\"start\":107529},{\"end\":107548,\"start\":107541},{\"end\":107564,\"start\":107556},{\"end\":107581,\"start\":107575},{\"end\":107598,\"start\":107591},{\"end\":107612,\"start\":107607},{\"end\":107628,\"start\":107621},{\"end\":107634,\"start\":107630},{\"end\":107991,\"start\":107982},{\"end\":108003,\"start\":107998},{\"end\":108282,\"start\":108273},{\"end\":108294,\"start\":108289},{\"end\":108552,\"start\":108547},{\"end\":108564,\"start\":108561},{\"end\":108580,\"start\":108573},{\"end\":108852,\"start\":108846},{\"end\":108871,\"start\":108863},{\"end\":108879,\"start\":108873},{\"end\":109161,\"start\":109150},{\"end\":109177,\"start\":109169},{\"end\":109191,\"start\":109185},{\"end\":109207,\"start\":109202},{\"end\":109559,\"start\":109548},{\"end\":109578,\"start\":109568},{\"end\":109860,\"start\":109845},{\"end\":109880,\"start\":109867},{\"end\":109907,\"start\":109887},{\"end\":110192,\"start\":110186},{\"end\":110201,\"start\":110196},{\"end\":110216,\"start\":110210},{\"end\":110485,\"start\":110477},{\"end\":110500,\"start\":110497},{\"end\":110516,\"start\":110512},{\"end\":110532,\"start\":110527},{\"end\":110876,\"start\":110862},{\"end\":110891,\"start\":110886},{\"end\":111114,\"start\":111110},{\"end\":111305,\"start\":111299},{\"end\":111321,\"start\":111314},{\"end\":111340,\"start\":111333},{\"end\":111590,\"start\":111587},{\"end\":111605,\"start\":111598},{\"end\":111622,\"start\":111614},{\"end\":111636,\"start\":111631},{\"end\":111872,\"start\":111864},{\"end\":111888,\"start\":111883},{\"end\":112124,\"start\":112119},{\"end\":112142,\"start\":112133},{\"end\":112170,\"start\":112152},{\"end\":112180,\"start\":112172},{\"end\":112452,\"start\":112449},{\"end\":112464,\"start\":112459},{\"end\":112761,\"start\":112750},{\"end\":113058,\"start\":113053},{\"end\":113070,\"start\":113064},{\"end\":113083,\"start\":113074},{\"end\":113094,\"start\":113087},{\"end\":113104,\"start\":113098},{\"end\":113115,\"start\":113109},{\"end\":113125,\"start\":113119},{\"end\":113133,\"start\":113129},{\"end\":113476,\"start\":113473},{\"end\":113484,\"start\":113481},{\"end\":113499,\"start\":113490},{\"end\":113513,\"start\":113510},{\"end\":113522,\"start\":113520},{\"end\":113537,\"start\":113534},{\"end\":113804,\"start\":113800},{\"end\":113821,\"start\":113806},{\"end\":114036,\"start\":114033},{\"end\":114050,\"start\":114044},{\"end\":114288,\"start\":114283},{\"end\":114305,\"start\":114298},{\"end\":114325,\"start\":114315},{\"end\":114338,\"start\":114333},{\"end\":114353,\"start\":114345},{\"end\":114369,\"start\":114362},{\"end\":114621,\"start\":114615},{\"end\":114636,\"start\":114628},{\"end\":114807,\"start\":114801},{\"end\":114822,\"start\":114814},{\"end\":115031,\"start\":115024},{\"end\":115055,\"start\":115042},{\"end\":115319,\"start\":115312},{\"end\":115337,\"start\":115328},{\"end\":115517,\"start\":115510},{\"end\":115530,\"start\":115525},{\"end\":115803,\"start\":115798},{\"end\":115822,\"start\":115814},{\"end\":116123,\"start\":116115},{\"end\":116137,\"start\":116133},{\"end\":116146,\"start\":116139},{\"end\":116369,\"start\":116363},{\"end\":116388,\"start\":116380},{\"end\":116401,\"start\":116396},{\"end\":116413,\"start\":116408},{\"end\":116431,\"start\":116424},{\"end\":116767,\"start\":116762},{\"end\":116785,\"start\":116776},{\"end\":117037,\"start\":117029},{\"end\":117278,\"start\":117275},{\"end\":117294,\"start\":117289},{\"end\":117309,\"start\":117303},{\"end\":117325,\"start\":117319},{\"end\":117345,\"start\":117334},{\"end\":117358,\"start\":117354},{\"end\":117717,\"start\":117712},{\"end\":117732,\"start\":117727},{\"end\":117740,\"start\":117734},{\"end\":117975,\"start\":117970},{\"end\":117997,\"start\":117986},{\"end\":118009,\"start\":118004},{\"end\":118265,\"start\":118257},{\"end\":118279,\"start\":118272},{\"end\":118500,\"start\":118492},{\"end\":118514,\"start\":118507},{\"end\":118769,\"start\":118761},{\"end\":118967,\"start\":118959},{\"end\":119224,\"start\":119216},{\"end\":119243,\"start\":119234},{\"end\":119509,\"start\":119499},{\"end\":119523,\"start\":119516},{\"end\":119537,\"start\":119533},{\"end\":119552,\"start\":119547},{\"end\":119569,\"start\":119563},{\"end\":119886,\"start\":119881},{\"end\":119900,\"start\":119894},{\"end\":120137,\"start\":120132},{\"end\":120151,\"start\":120145},{\"end\":120381,\"start\":120376},{\"end\":120399,\"start\":120390},{\"end\":120590,\"start\":120585},{\"end\":120608,\"start\":120599},{\"end\":120874,\"start\":120864},{\"end\":120890,\"start\":120881},{\"end\":120909,\"start\":120903},{\"end\":121173,\"start\":121168},{\"end\":121185,\"start\":121182},{\"end\":121198,\"start\":121196},{\"end\":121410,\"start\":121402},{\"end\":121428,\"start\":121420},{\"end\":121444,\"start\":121436},{\"end\":121459,\"start\":121451},{\"end\":121476,\"start\":121469},{\"end\":121766,\"start\":121761},{\"end\":121777,\"start\":121770},{\"end\":121790,\"start\":121783},{\"end\":122131,\"start\":122126},{\"end\":122142,\"start\":122135},{\"end\":122155,\"start\":122148},{\"end\":122472,\"start\":122467},{\"end\":122483,\"start\":122476},{\"end\":122503,\"start\":122494},{\"end\":122841,\"start\":122836},{\"end\":122862,\"start\":122853},{\"end\":122885,\"start\":122874},{\"end\":122903,\"start\":122896},{\"end\":122917,\"start\":122912},{\"end\":123215,\"start\":123211},{\"end\":123229,\"start\":123227},{\"end\":123246,\"start\":123238},{\"end\":123262,\"start\":123257},{\"end\":123266,\"start\":123264},{\"end\":123561,\"start\":123557},{\"end\":123583,\"start\":123581},{\"end\":123598,\"start\":123591},{\"end\":123614,\"start\":123609},{\"end\":123625,\"start\":123620},{\"end\":123638,\"start\":123634},{\"end\":123652,\"start\":123645},{\"end\":123667,\"start\":123663},{\"end\":123671,\"start\":123669},{\"end\":123957,\"start\":123952},{\"end\":123972,\"start\":123966},{\"end\":123989,\"start\":123983},{\"end\":124224,\"start\":124212},{\"end\":124236,\"start\":124231},{\"end\":124250,\"start\":124244},{\"end\":124258,\"start\":124252},{\"end\":124651,\"start\":124639},{\"end\":124663,\"start\":124658},{\"end\":124677,\"start\":124671},{\"end\":124685,\"start\":124679},{\"end\":125017,\"start\":125006},{\"end\":125033,\"start\":125029},{\"end\":125052,\"start\":125044},{\"end\":125328,\"start\":125317},{\"end\":125342,\"start\":125335},{\"end\":125358,\"start\":125352},{\"end\":125374,\"start\":125370},{\"end\":125389,\"start\":125381},{\"end\":125406,\"start\":125398},{\"end\":125708,\"start\":125706},{\"end\":125724,\"start\":125718},{\"end\":125743,\"start\":125734},{\"end\":125755,\"start\":125750},{\"end\":126044,\"start\":126041},{\"end\":126057,\"start\":126054},{\"end\":126069,\"start\":126066},{\"end\":126349,\"start\":126337},{\"end\":126355,\"start\":126351},{\"end\":126694,\"start\":126681},{\"end\":126700,\"start\":126696},{\"end\":127008,\"start\":127005},{\"end\":127022,\"start\":127019},{\"end\":127037,\"start\":127034},{\"end\":127045,\"start\":127042},{\"end\":127054,\"start\":127052},{\"end\":127061,\"start\":127059},{\"end\":127384,\"start\":127380},{\"end\":127400,\"start\":127391},{\"end\":127416,\"start\":127409},{\"end\":127701,\"start\":127697},{\"end\":127937,\"start\":127932},{\"end\":127950,\"start\":127945},{\"end\":127958,\"start\":127952},{\"end\":128258,\"start\":128253},{\"end\":128268,\"start\":128265},{\"end\":128284,\"start\":128277},{\"end\":128301,\"start\":128294},{\"end\":128317,\"start\":128310},{\"end\":128337,\"start\":128326},{\"end\":128350,\"start\":128346},{\"end\":128748,\"start\":128745},{\"end\":128761,\"start\":128755},{\"end\":128775,\"start\":128768},{\"end\":128788,\"start\":128782},{\"end\":128798,\"start\":128794},{\"end\":129102,\"start\":129094},{\"end\":129117,\"start\":129109},{\"end\":129131,\"start\":129124},{\"end\":129142,\"start\":129137},{\"end\":129452,\"start\":129444},{\"end\":129471,\"start\":129462},{\"end\":129490,\"start\":129481},{\"end\":129508,\"start\":129499},{\"end\":129529,\"start\":129521},{\"end\":129962,\"start\":129955},{\"end\":129977,\"start\":129970},{\"end\":129992,\"start\":129987},{\"end\":130004,\"start\":129999},{\"end\":130017,\"start\":130011},{\"end\":130035,\"start\":130029},{\"end\":130048,\"start\":130037},{\"end\":130365,\"start\":130359},{\"end\":130379,\"start\":130373},{\"end\":130687,\"start\":130681},{\"end\":130703,\"start\":130696},{\"end\":130718,\"start\":130712},{\"end\":131004,\"start\":130995},{\"end\":131022,\"start\":131013},{\"end\":131036,\"start\":131031},{\"end\":131053,\"start\":131045},{\"end\":131067,\"start\":131063},{\"end\":131343,\"start\":131334},{\"end\":131361,\"start\":131352},{\"end\":131375,\"start\":131370},{\"end\":131392,\"start\":131384},{\"end\":131406,\"start\":131402},{\"end\":131731,\"start\":131722},{\"end\":131749,\"start\":131740},{\"end\":131763,\"start\":131758},{\"end\":131780,\"start\":131772},{\"end\":131794,\"start\":131790},{\"end\":132095,\"start\":132086},{\"end\":132113,\"start\":132104},{\"end\":132127,\"start\":132122},{\"end\":132144,\"start\":132136},{\"end\":132158,\"start\":132154},{\"end\":132488,\"start\":132482},{\"end\":132505,\"start\":132498},{\"end\":132779,\"start\":132770},{\"end\":132794,\"start\":132787},{\"end\":132809,\"start\":132803},{\"end\":133098,\"start\":133090},{\"end\":133112,\"start\":133108},{\"end\":133128,\"start\":133125},{\"end\":133146,\"start\":133137},{\"end\":133454,\"start\":133446},{\"end\":133471,\"start\":133462},{\"end\":133482,\"start\":133479},{\"end\":133500,\"start\":133493},{\"end\":133515,\"start\":133508},{\"end\":133529,\"start\":133523},{\"end\":133548,\"start\":133538},{\"end\":133563,\"start\":133558},{\"end\":133579,\"start\":133571},{\"end\":133596,\"start\":133587},{\"end\":133607,\"start\":133604},{\"end\":133625,\"start\":133618},{\"end\":133641,\"start\":133636},{\"end\":134131,\"start\":134125},{\"end\":134351,\"start\":134345},{\"end\":134368,\"start\":134358},{\"end\":134382,\"start\":134376},{\"end\":134640,\"start\":134632},{\"end\":134847,\"start\":134842},{\"end\":134858,\"start\":134851},{\"end\":135053,\"start\":135049},{\"end\":135339,\"start\":135321},{\"end\":135353,\"start\":135345},{\"end\":135369,\"start\":135359},{\"end\":135379,\"start\":135371},{\"end\":135682,\"start\":135677},{\"end\":135696,\"start\":135691},{\"end\":135711,\"start\":135702},{\"end\":135721,\"start\":135713},{\"end\":136028,\"start\":136020},{\"end\":136045,\"start\":136042},{\"end\":136059,\"start\":136053},{\"end\":136073,\"start\":136068},{\"end\":136086,\"start\":136080},{\"end\":136103,\"start\":136096},{\"end\":136493,\"start\":136489},{\"end\":136512,\"start\":136503},{\"end\":136527,\"start\":136519},{\"end\":136541,\"start\":136535},{\"end\":136818,\"start\":136814},{\"end\":136837,\"start\":136828},{\"end\":136852,\"start\":136844},{\"end\":136866,\"start\":136860},{\"end\":137133,\"start\":137125},{\"end\":137431,\"start\":137421},{\"end\":137446,\"start\":137440},{\"end\":137463,\"start\":137454},{\"end\":137477,\"start\":137471},{\"end\":137492,\"start\":137485},{\"end\":137509,\"start\":137500},{\"end\":137982,\"start\":137979},{\"end\":137996,\"start\":137992},{\"end\":138008,\"start\":138003},{\"end\":138015,\"start\":138013},{\"end\":138346,\"start\":138338},{\"end\":138568,\"start\":138562},{\"end\":138582,\"start\":138574},{\"end\":138802,\"start\":138796},{\"end\":138818,\"start\":138812},{\"end\":138833,\"start\":138825},{\"end\":138847,\"start\":138840},{\"end\":139149,\"start\":139140},{\"end\":139168,\"start\":139158},{\"end\":139467,\"start\":139458},{\"end\":139486,\"start\":139476},{\"end\":139785,\"start\":139772},{\"end\":139803,\"start\":139796},{\"end\":139825,\"start\":139818},{\"end\":140197,\"start\":140189},{\"end\":140209,\"start\":140205},{\"end\":140217,\"start\":140211},{\"end\":140533,\"start\":140523},{\"end\":140828,\"start\":140818},{\"end\":140851,\"start\":140840},{\"end\":140868,\"start\":140860},{\"end\":141163,\"start\":141154},{\"end\":141176,\"start\":141171},{\"end\":141482,\"start\":141473},{\"end\":141495,\"start\":141490},{\"end\":141821,\"start\":141812},{\"end\":141834,\"start\":141829},{\"end\":141846,\"start\":141842},{\"end\":142144,\"start\":142135},{\"end\":142166,\"start\":142162},{\"end\":142422,\"start\":142406},{\"end\":142441,\"start\":142436},{\"end\":142681,\"start\":142674},{\"end\":142847,\"start\":142840},{\"end\":142860,\"start\":142854},{\"end\":142876,\"start\":142870},{\"end\":143124,\"start\":143118},{\"end\":143141,\"start\":143138},{\"end\":143151,\"start\":143147},{\"end\":143164,\"start\":143158},{\"end\":143446,\"start\":143438},{\"end\":143464,\"start\":143455},{\"end\":143711,\"start\":143704},{\"end\":143725,\"start\":143720},{\"end\":143743,\"start\":143735},{\"end\":143972,\"start\":143965},{\"end\":143988,\"start\":143983},{\"end\":144006,\"start\":143998},{\"end\":144320,\"start\":144316},{\"end\":144642,\"start\":144634},{\"end\":144657,\"start\":144650},{\"end\":144675,\"start\":144668},{\"end\":144908,\"start\":144903},{\"end\":144921,\"start\":144915},{\"end\":145170,\"start\":145167},{\"end\":145182,\"start\":145179},{\"end\":145195,\"start\":145191},{\"end\":145210,\"start\":145205},{\"end\":145221,\"start\":145218},{\"end\":145512,\"start\":145505},{\"end\":145522,\"start\":145519},{\"end\":145540,\"start\":145531},{\"end\":145850,\"start\":145844},{\"end\":145864,\"start\":145858},{\"end\":146111,\"start\":146105},{\"end\":146117,\"start\":146113},{\"end\":146396,\"start\":146390},{\"end\":146409,\"start\":146405},{\"end\":146420,\"start\":146411},{\"end\":146665,\"start\":146659},{\"end\":146678,\"start\":146674},{\"end\":146689,\"start\":146680},{\"end\":146944,\"start\":146938},{\"end\":146957,\"start\":146953},{\"end\":146968,\"start\":146959},{\"end\":147258,\"start\":147254},{\"end\":147270,\"start\":147265},{\"end\":147548,\"start\":147540},{\"end\":147566,\"start\":147562},{\"end\":147584,\"start\":147575},{\"end\":147598,\"start\":147595},{\"end\":147877,\"start\":147872},{\"end\":148048,\"start\":148043},{\"end\":148219,\"start\":148214},{\"end\":148236,\"start\":148229},{\"end\":148485,\"start\":148480},{\"end\":148492,\"start\":148490},{\"end\":148508,\"start\":148502},{\"end\":148805,\"start\":148800},{\"end\":148812,\"start\":148810},{\"end\":148828,\"start\":148822},{\"end\":149085,\"start\":149080},{\"end\":149092,\"start\":149090},{\"end\":149108,\"start\":149102},{\"end\":149124,\"start\":149118},{\"end\":149375,\"start\":149370},{\"end\":149391,\"start\":149385},{\"end\":149398,\"start\":149396},{\"end\":149414,\"start\":149408},{\"end\":149650,\"start\":149634},{\"end\":149665,\"start\":149660},{\"end\":149682,\"start\":149676},{\"end\":149700,\"start\":149690},{\"end\":149722,\"start\":149711},{\"end\":150081,\"start\":150077},{\"end\":150095,\"start\":150089},{\"end\":150385,\"start\":150381},{\"end\":150401,\"start\":150395},{\"end\":150420,\"start\":150405},{\"end\":150430,\"start\":150424},{\"end\":150445,\"start\":150432},{\"end\":150765,\"start\":150761},{\"end\":150779,\"start\":150774},{\"end\":150792,\"start\":150789},{\"end\":150806,\"start\":150799},{\"end\":151126,\"start\":151113},{\"end\":151140,\"start\":151137},{\"end\":151144,\"start\":151142},{\"end\":151438,\"start\":151425},{\"end\":151454,\"start\":151448},{\"end\":151705,\"start\":151703},{\"end\":151876,\"start\":151874},{\"end\":151892,\"start\":151885},{\"end\":151907,\"start\":151900},{\"end\":151923,\"start\":151918},{\"end\":152224,\"start\":152220},{\"end\":152240,\"start\":152236},{\"end\":152254,\"start\":152248},{\"end\":152561,\"start\":152558},{\"end\":152577,\"start\":152568},{\"end\":152910,\"start\":152907},{\"end\":152926,\"start\":152917},{\"end\":153245,\"start\":153241},{\"end\":153262,\"start\":153254},{\"end\":153274,\"start\":153269},{\"end\":153516,\"start\":153510},{\"end\":153530,\"start\":153525},{\"end\":153544,\"start\":153537},{\"end\":153792,\"start\":153786},{\"end\":153815,\"start\":153802},{\"end\":153834,\"start\":153822},{\"end\":154119,\"start\":154111},{\"end\":154136,\"start\":154133},{\"end\":154150,\"start\":154145},{\"end\":154165,\"start\":154160},{\"end\":154440,\"start\":154434},{\"end\":154457,\"start\":154449},{\"end\":154468,\"start\":154465},{\"end\":154479,\"start\":154472},{\"end\":154745,\"start\":154740},{\"end\":154754,\"start\":154752},{\"end\":154968,\"start\":154963},{\"end\":154982,\"start\":154977},{\"end\":154996,\"start\":154990},{\"end\":155263,\"start\":155258},{\"end\":155272,\"start\":155268},{\"end\":155286,\"start\":155282},{\"end\":155305,\"start\":155300},{\"end\":155571,\"start\":155567},{\"end\":155586,\"start\":155581},{\"end\":155600,\"start\":155593},{\"end\":155614,\"start\":155610}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":121542898},\"end\":97706,\"start\":97404},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":64903870},\"end\":97953,\"start\":97708},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":170213},\"end\":98261,\"start\":97955},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":16021058},\"end\":98737,\"start\":98263},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":12420849},\"end\":99166,\"start\":98739},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":2491676},\"end\":99527,\"start\":99168},{\"attributes\":{\"doi\":\"arXiv:1702.01731\",\"id\":\"b6\"},\"end\":99809,\"start\":99529},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":14777911},\"end\":100089,\"start\":99811},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":15565956},\"end\":100358,\"start\":100091},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":2054748},\"end\":100731,\"start\":100360},{\"attributes\":{\"id\":\"b10\"},\"end\":101048,\"start\":100733},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":122389226},\"end\":101282,\"start\":101050},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":114221},\"end\":101586,\"start\":101284},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":1179359},\"end\":102043,\"start\":101588},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":142734},\"end\":102311,\"start\":102045},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":8174852},\"end\":102597,\"start\":102313},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":1815530},\"end\":102908,\"start\":102599},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":16667152},\"end\":103262,\"start\":102910},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":14411975},\"end\":103577,\"start\":103264},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":8329460},\"end\":103816,\"start\":103579},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":206852383},\"end\":104228,\"start\":103818},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":15380537},\"end\":104512,\"start\":104230},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":2621129},\"end\":104845,\"start\":104514},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":5182349},\"end\":105085,\"start\":104847},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":440343},\"end\":105367,\"start\":105087},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":17969052},\"end\":105646,\"start\":105369},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":7451915},\"end\":105954,\"start\":105648},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":9517281},\"end\":106235,\"start\":105956},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":14399225},\"end\":106480,\"start\":106237},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":61871873},\"end\":106830,\"start\":106482},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":18629523},\"end\":107200,\"start\":106832},{\"attributes\":{\"doi\":\"arXiv:1606.03798\",\"id\":\"b31\"},\"end\":107424,\"start\":107202},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":12552176},\"end\":107946,\"start\":107426},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":847078},\"end\":108200,\"start\":107948},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":10102189},\"end\":108506,\"start\":108202},{\"attributes\":{\"id\":\"b35\"},\"end\":108725,\"start\":108508},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":972888},\"end\":109093,\"start\":108727},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":1501163},\"end\":109456,\"start\":109095},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":1473690},\"end\":109781,\"start\":109458},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":17942780},\"end\":110109,\"start\":109783},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":710586},\"end\":110396,\"start\":110111},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":15869446},\"end\":110784,\"start\":110398},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":28459899},\"end\":111065,\"start\":110786},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":29663790},\"end\":111239,\"start\":111067},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":16284071},\"end\":111511,\"start\":111241},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":17254092},\"end\":111832,\"start\":111513},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":1035098},\"end\":112067,\"start\":111834},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":12238230},\"end\":112365,\"start\":112069},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":3172398},\"end\":112686,\"start\":112367},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":8252027},\"end\":112956,\"start\":112688},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":6856816},\"end\":113413,\"start\":112958},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":207111722},\"end\":113726,\"start\":113415},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":4327732},\"end\":113962,\"start\":113728},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":2315873},\"end\":114215,\"start\":113964},{\"attributes\":{\"doi\":\"arXiv:1607.07405\",\"id\":\"b54\"},\"end\":114571,\"start\":114217},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":11566402},\"end\":114756,\"start\":114573},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":1694378},\"end\":114944,\"start\":114758},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":10523248},\"end\":115259,\"start\":114946},{\"attributes\":{\"id\":\"b58\"},\"end\":115496,\"start\":115261},{\"attributes\":{\"id\":\"b59\"},\"end\":115684,\"start\":115498},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":10378716},\"end\":116083,\"start\":115686},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":1371968},\"end\":116278,\"start\":116085},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":14250253},\"end\":116690,\"start\":116280},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":17566357},\"end\":116944,\"start\":116692},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":13452146},\"end\":117198,\"start\":116946},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":3759573},\"end\":117616,\"start\":117200},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":17834704},\"end\":117932,\"start\":117618},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":208933582},\"end\":118204,\"start\":117934},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":33148754},\"end\":118424,\"start\":118206},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":12106529},\"end\":118678,\"start\":118426},{\"attributes\":{\"id\":\"b70\"},\"end\":118885,\"start\":118680},{\"attributes\":{\"id\":\"b71\",\"matched_paper_id\":6953970},\"end\":119126,\"start\":118887},{\"attributes\":{\"id\":\"b72\",\"matched_paper_id\":14016873},\"end\":119423,\"start\":119128},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":1193000},\"end\":119818,\"start\":119425},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":206986664},\"end\":120075,\"start\":119820},{\"attributes\":{\"id\":\"b75\",\"matched_paper_id\":13136001},\"end\":120323,\"start\":120077},{\"attributes\":{\"doi\":\"arXiv:1312.3429\",\"id\":\"b76\"},\"end\":120520,\"start\":120325},{\"attributes\":{\"id\":\"b77\",\"matched_paper_id\":9527002},\"end\":120792,\"start\":120522},{\"attributes\":{\"id\":\"b78\",\"matched_paper_id\":195908774},\"end\":121113,\"start\":120794},{\"attributes\":{\"id\":\"b79\",\"matched_paper_id\":1305588},\"end\":121344,\"start\":121115},{\"attributes\":{\"id\":\"b80\",\"matched_paper_id\":206849534},\"end\":121693,\"start\":121346},{\"attributes\":{\"id\":\"b81\",\"matched_paper_id\":14080215},\"end\":122031,\"start\":121695},{\"attributes\":{\"id\":\"b82\"},\"end\":122362,\"start\":122033},{\"attributes\":{\"id\":\"b83\",\"matched_paper_id\":109240},\"end\":122762,\"start\":122364},{\"attributes\":{\"id\":\"b84\",\"matched_paper_id\":11091110},\"end\":123133,\"start\":122764},{\"attributes\":{\"id\":\"b85\",\"matched_paper_id\":2714905},\"end\":123483,\"start\":123135},{\"attributes\":{\"id\":\"b86\",\"matched_paper_id\":206741597},\"end\":123930,\"start\":123485},{\"attributes\":{\"id\":\"b87\",\"matched_paper_id\":1779661},\"end\":124098,\"start\":123932},{\"attributes\":{\"id\":\"b88\",\"matched_paper_id\":18199008},\"end\":124531,\"start\":124100},{\"attributes\":{\"id\":\"b89\",\"matched_paper_id\":8426914},\"end\":124946,\"start\":124533},{\"attributes\":{\"id\":\"b90\",\"matched_paper_id\":1211102},\"end\":125242,\"start\":124948},{\"attributes\":{\"id\":\"b91\",\"matched_paper_id\":13984777},\"end\":125642,\"start\":125244},{\"attributes\":{\"id\":\"b92\",\"matched_paper_id\":2459532},\"end\":125966,\"start\":125644},{\"attributes\":{\"id\":\"b93\",\"matched_paper_id\":16868165},\"end\":126248,\"start\":125968},{\"attributes\":{\"id\":\"b94\",\"matched_paper_id\":3093606},\"end\":126557,\"start\":126250},{\"attributes\":{\"id\":\"b95\",\"matched_paper_id\":16732348},\"end\":126927,\"start\":126559},{\"attributes\":{\"id\":\"b96\",\"matched_paper_id\":2105943},\"end\":127313,\"start\":126929},{\"attributes\":{\"id\":\"b97\",\"matched_paper_id\":1629541},\"end\":127628,\"start\":127315},{\"attributes\":{\"id\":\"b98\",\"matched_paper_id\":174065},\"end\":127848,\"start\":127630},{\"attributes\":{\"id\":\"b99\",\"matched_paper_id\":2121536},\"end\":128138,\"start\":127850},{\"attributes\":{\"id\":\"b100\",\"matched_paper_id\":206594275},\"end\":128660,\"start\":128140},{\"attributes\":{\"id\":\"b101\",\"matched_paper_id\":163588},\"end\":129014,\"start\":128662},{\"attributes\":{\"doi\":\"arXiv:1702.01381\",\"id\":\"b102\"},\"end\":129320,\"start\":129016},{\"attributes\":{\"id\":\"b103\",\"matched_paper_id\":16205020},\"end\":129882,\"start\":129322},{\"attributes\":{\"doi\":\"arXiv:1611.06069\",\"id\":\"b104\"},\"end\":130261,\"start\":129884},{\"attributes\":{\"id\":\"b105\",\"matched_paper_id\":8012658},\"end\":130581,\"start\":130263},{\"attributes\":{\"id\":\"b106\",\"matched_paper_id\":215753544},\"end\":130938,\"start\":130583},{\"attributes\":{\"id\":\"b107\",\"matched_paper_id\":17407555},\"end\":131278,\"start\":130940},{\"attributes\":{\"id\":\"b108\",\"matched_paper_id\":17737459},\"end\":131668,\"start\":131280},{\"attributes\":{\"id\":\"b109\"},\"end\":132002,\"start\":131670},{\"attributes\":{\"id\":\"b110\",\"matched_paper_id\":5963070},\"end\":132392,\"start\":132004},{\"attributes\":{\"id\":\"b111\",\"matched_paper_id\":206858678},\"end\":132705,\"start\":132394},{\"attributes\":{\"id\":\"b112\",\"matched_paper_id\":206775100},\"end\":132993,\"start\":132707},{\"attributes\":{\"id\":\"b113\"},\"end\":133374,\"start\":132995},{\"attributes\":{\"id\":\"b114\",\"matched_paper_id\":11830123},\"end\":134054,\"start\":133376},{\"attributes\":{\"id\":\"b115\",\"matched_paper_id\":886598},\"end\":134320,\"start\":134056},{\"attributes\":{\"id\":\"b116\",\"matched_paper_id\":9224113},\"end\":134577,\"start\":134322},{\"attributes\":{\"id\":\"b117\",\"matched_paper_id\":12115976},\"end\":134791,\"start\":134579},{\"attributes\":{\"id\":\"b118\",\"matched_paper_id\":5884124},\"end\":134981,\"start\":134793},{\"attributes\":{\"id\":\"b119\",\"matched_paper_id\":15326934},\"end\":135224,\"start\":134983},{\"attributes\":{\"id\":\"b120\",\"matched_paper_id\":1504032},\"end\":135628,\"start\":135226},{\"attributes\":{\"id\":\"b121\",\"matched_paper_id\":3034008},\"end\":135932,\"start\":135630},{\"attributes\":{\"id\":\"b122\",\"matched_paper_id\":14091250},\"end\":136409,\"start\":135934},{\"attributes\":{\"id\":\"b123\",\"matched_paper_id\":153561},\"end\":136744,\"start\":136411},{\"attributes\":{\"id\":\"b124\",\"matched_paper_id\":15063027},\"end\":137070,\"start\":136746},{\"attributes\":{\"id\":\"b125\",\"matched_paper_id\":12127129},\"end\":137293,\"start\":137072},{\"attributes\":{\"id\":\"b126\",\"matched_paper_id\":28272188},\"end\":137881,\"start\":137295},{\"attributes\":{\"id\":\"b127\",\"matched_paper_id\":344242},\"end\":138270,\"start\":137883},{\"attributes\":{\"id\":\"b128\",\"matched_paper_id\":206735464},\"end\":138503,\"start\":138272},{\"attributes\":{\"id\":\"b129\",\"matched_paper_id\":1388140},\"end\":138741,\"start\":138505},{\"attributes\":{\"id\":\"b130\",\"matched_paper_id\":206769866},\"end\":139036,\"start\":138743},{\"attributes\":{\"id\":\"b131\",\"matched_paper_id\":15704506},\"end\":139382,\"start\":139038},{\"attributes\":{\"id\":\"b132\",\"matched_paper_id\":5392672},\"end\":139665,\"start\":139384},{\"attributes\":{\"id\":\"b133\",\"matched_paper_id\":6994572},\"end\":140104,\"start\":139667},{\"attributes\":{\"id\":\"b134\",\"matched_paper_id\":2071866},\"end\":140409,\"start\":140106},{\"attributes\":{\"id\":\"b135\",\"matched_paper_id\":2438165},\"end\":140731,\"start\":140411},{\"attributes\":{\"id\":\"b136\",\"matched_paper_id\":546494},\"end\":141090,\"start\":140733},{\"attributes\":{\"id\":\"b137\",\"matched_paper_id\":22424},\"end\":141385,\"start\":141092},{\"attributes\":{\"id\":\"b138\",\"matched_paper_id\":22424},\"end\":141721,\"start\":141387},{\"attributes\":{\"id\":\"b139\",\"matched_paper_id\":6190642},\"end\":142051,\"start\":141723},{\"attributes\":{\"id\":\"b140\",\"matched_paper_id\":6395091},\"end\":142362,\"start\":142053},{\"attributes\":{\"id\":\"b141\",\"matched_paper_id\":1728538},\"end\":142628,\"start\":142364},{\"attributes\":{\"id\":\"b142\",\"matched_paper_id\":123722079},\"end\":142786,\"start\":142630},{\"attributes\":{\"id\":\"b143\"},\"end\":143038,\"start\":142788},{\"attributes\":{\"id\":\"b144\",\"matched_paper_id\":17821336},\"end\":143362,\"start\":143040},{\"attributes\":{\"id\":\"b145\",\"matched_paper_id\":11797475},\"end\":143648,\"start\":143364},{\"attributes\":{\"id\":\"b146\",\"matched_paper_id\":13385757},\"end\":143906,\"start\":143650},{\"attributes\":{\"id\":\"b147\",\"matched_paper_id\":711203},\"end\":144185,\"start\":143908},{\"attributes\":{\"id\":\"b148\",\"matched_paper_id\":126150573},\"end\":144590,\"start\":144187},{\"attributes\":{\"id\":\"b149\"},\"end\":144814,\"start\":144592},{\"attributes\":{\"id\":\"b150\",\"matched_paper_id\":10083976},\"end\":145114,\"start\":144816},{\"attributes\":{\"id\":\"b151\",\"matched_paper_id\":2554592},\"end\":145417,\"start\":145116},{\"attributes\":{\"id\":\"b152\",\"matched_paper_id\":15964295},\"end\":145757,\"start\":145419},{\"attributes\":{\"id\":\"b153\",\"matched_paper_id\":2931825},\"end\":146048,\"start\":145759},{\"attributes\":{\"id\":\"b154\",\"matched_paper_id\":91496181},\"end\":146320,\"start\":146050},{\"attributes\":{\"id\":\"b155\",\"matched_paper_id\":1017288},\"end\":146594,\"start\":146322},{\"attributes\":{\"id\":\"b156\",\"matched_paper_id\":42127929},\"end\":146854,\"start\":146596},{\"attributes\":{\"id\":\"b157\",\"matched_paper_id\":2714075},\"end\":147174,\"start\":146856},{\"attributes\":{\"id\":\"b158\",\"matched_paper_id\":9381082},\"end\":147468,\"start\":147176},{\"attributes\":{\"id\":\"b159\",\"matched_paper_id\":2558259},\"end\":147824,\"start\":147470},{\"attributes\":{\"id\":\"b160\",\"matched_paper_id\":10169974},\"end\":148015,\"start\":147826},{\"attributes\":{\"id\":\"b161\",\"matched_paper_id\":7241355},\"end\":148163,\"start\":148017},{\"attributes\":{\"id\":\"b162\",\"matched_paper_id\":9289661},\"end\":148424,\"start\":148165},{\"attributes\":{\"id\":\"b163\",\"matched_paper_id\":696914},\"end\":148744,\"start\":148426},{\"attributes\":{\"id\":\"b164\",\"matched_paper_id\":696914},\"end\":149031,\"start\":148746},{\"attributes\":{\"id\":\"b165\",\"matched_paper_id\":3209399},\"end\":149293,\"start\":149033},{\"attributes\":{\"id\":\"b166\",\"matched_paper_id\":8568622},\"end\":149621,\"start\":149295},{\"attributes\":{\"doi\":\"arXiv:1704.07804\",\"id\":\"b167\"},\"end\":149979,\"start\":149623},{\"attributes\":{\"id\":\"b168\",\"matched_paper_id\":10130085},\"end\":150305,\"start\":149981},{\"attributes\":{\"id\":\"b169\",\"matched_paper_id\":14526806},\"end\":150661,\"start\":150307},{\"attributes\":{\"id\":\"b170\",\"matched_paper_id\":9114952},\"end\":151028,\"start\":150663},{\"attributes\":{\"id\":\"b171\",\"matched_paper_id\":62204441},\"end\":151347,\"start\":151030},{\"attributes\":{\"id\":\"b172\",\"matched_paper_id\":6474817},\"end\":151635,\"start\":151349},{\"attributes\":{\"id\":\"b173\",\"matched_paper_id\":5296119},\"end\":151832,\"start\":151637},{\"attributes\":{\"id\":\"b174\",\"matched_paper_id\":11832473},\"end\":152150,\"start\":151834},{\"attributes\":{\"id\":\"b175\",\"matched_paper_id\":5247846},\"end\":152429,\"start\":152152},{\"attributes\":{\"id\":\"b176\",\"matched_paper_id\":7886588},\"end\":152786,\"start\":152431},{\"attributes\":{\"id\":\"b177\",\"matched_paper_id\":12371478},\"end\":153181,\"start\":152788},{\"attributes\":{\"id\":\"b178\",\"matched_paper_id\":7740116},\"end\":153440,\"start\":153183},{\"attributes\":{\"doi\":\"arXiv:1607.00470\",\"id\":\"b179\"},\"end\":153696,\"start\":153442},{\"attributes\":{\"id\":\"b180\",\"matched_paper_id\":131102208},\"end\":154044,\"start\":153698},{\"attributes\":{\"id\":\"b181\",\"matched_paper_id\":7336637},\"end\":154380,\"start\":154046},{\"attributes\":{\"id\":\"b182\",\"matched_paper_id\":15171751},\"end\":154696,\"start\":154382},{\"attributes\":{\"id\":\"b183\"},\"end\":154894,\"start\":154698},{\"attributes\":{\"id\":\"b184\",\"matched_paper_id\":8995726},\"end\":155181,\"start\":154896},{\"attributes\":{\"id\":\"b185\",\"matched_paper_id\":7719541},\"end\":155500,\"start\":155183},{\"attributes\":{\"id\":\"b186\",\"matched_paper_id\":11977588},\"end\":155855,\"start\":155502}]", "bib_title": "[{\"end\":97472,\"start\":97404},{\"end\":97779,\"start\":97708},{\"end\":98005,\"start\":97955},{\"end\":98387,\"start\":98263},{\"end\":98854,\"start\":98739},{\"end\":99260,\"start\":99168},{\"end\":99844,\"start\":99811},{\"end\":100136,\"start\":100091},{\"end\":100405,\"start\":100360},{\"end\":100830,\"start\":100733},{\"end\":101093,\"start\":101050},{\"end\":101332,\"start\":101284},{\"end\":101663,\"start\":101588},{\"end\":102108,\"start\":102045},{\"end\":102359,\"start\":102313},{\"end\":102651,\"start\":102599},{\"end\":102991,\"start\":102910},{\"end\":103324,\"start\":103264},{\"end\":103634,\"start\":103579},{\"end\":103888,\"start\":103818},{\"end\":104279,\"start\":104230},{\"end\":104612,\"start\":104514},{\"end\":104900,\"start\":104847},{\"end\":105152,\"start\":105087},{\"end\":105443,\"start\":105369},{\"end\":105724,\"start\":105648},{\"end\":106013,\"start\":105956},{\"end\":106305,\"start\":106237},{\"end\":106553,\"start\":106482},{\"end\":106878,\"start\":106832},{\"end\":107484,\"start\":107426},{\"end\":107974,\"start\":107948},{\"end\":108265,\"start\":108202},{\"end\":108539,\"start\":108508},{\"end\":108842,\"start\":108727},{\"end\":109139,\"start\":109095},{\"end\":109536,\"start\":109458},{\"end\":109837,\"start\":109783},{\"end\":110177,\"start\":110111},{\"end\":110470,\"start\":110398},{\"end\":110853,\"start\":110786},{\"end\":111104,\"start\":111067},{\"end\":111289,\"start\":111241},{\"end\":111578,\"start\":111513},{\"end\":111854,\"start\":111834},{\"end\":112109,\"start\":112069},{\"end\":112440,\"start\":112367},{\"end\":112740,\"start\":112688},{\"end\":113047,\"start\":112958},{\"end\":113463,\"start\":113415},{\"end\":113796,\"start\":113728},{\"end\":114027,\"start\":113964},{\"end\":114607,\"start\":114573},{\"end\":114793,\"start\":114758},{\"end\":115014,\"start\":114946},{\"end\":115788,\"start\":115686},{\"end\":116109,\"start\":116085},{\"end\":116354,\"start\":116280},{\"end\":116751,\"start\":116692},{\"end\":117019,\"start\":116946},{\"end\":117268,\"start\":117200},{\"end\":117708,\"start\":117618},{\"end\":117961,\"start\":117934},{\"end\":118238,\"start\":118206},{\"end\":118473,\"start\":118426},{\"end\":118949,\"start\":118887},{\"end\":119206,\"start\":119128},{\"end\":119492,\"start\":119425},{\"end\":119873,\"start\":119820},{\"end\":120124,\"start\":120077},{\"end\":120575,\"start\":120522},{\"end\":120857,\"start\":120794},{\"end\":121157,\"start\":121115},{\"end\":121393,\"start\":121346},{\"end\":121751,\"start\":121695},{\"end\":122116,\"start\":122033},{\"end\":122457,\"start\":122364},{\"end\":122830,\"start\":122764},{\"end\":123207,\"start\":123135},{\"end\":123553,\"start\":123485},{\"end\":123945,\"start\":123932},{\"end\":124200,\"start\":124100},{\"end\":124627,\"start\":124533},{\"end\":124997,\"start\":124948},{\"end\":125308,\"start\":125244},{\"end\":125699,\"start\":125644},{\"end\":126034,\"start\":125968},{\"end\":126324,\"start\":126250},{\"end\":126668,\"start\":126559},{\"end\":126994,\"start\":126929},{\"end\":127369,\"start\":127315},{\"end\":127687,\"start\":127630},{\"end\":127928,\"start\":127850},{\"end\":128242,\"start\":128140},{\"end\":128731,\"start\":128662},{\"end\":129435,\"start\":129322},{\"end\":130347,\"start\":130263},{\"end\":130672,\"start\":130583},{\"end\":130985,\"start\":130940},{\"end\":131324,\"start\":131280},{\"end\":131712,\"start\":131670},{\"end\":132076,\"start\":132004},{\"end\":132474,\"start\":132394},{\"end\":132763,\"start\":132707},{\"end\":133082,\"start\":132995},{\"end\":133434,\"start\":133376},{\"end\":134117,\"start\":134056},{\"end\":134337,\"start\":134322},{\"end\":134625,\"start\":134579},{\"end\":134838,\"start\":134793},{\"end\":135038,\"start\":134983},{\"end\":135314,\"start\":135226},{\"end\":135673,\"start\":135630},{\"end\":136012,\"start\":135934},{\"end\":136478,\"start\":136411},{\"end\":136803,\"start\":136746},{\"end\":137115,\"start\":137072},{\"end\":137413,\"start\":137295},{\"end\":137969,\"start\":137883},{\"end\":138330,\"start\":138272},{\"end\":138553,\"start\":138505},{\"end\":138788,\"start\":138743},{\"end\":139133,\"start\":139038},{\"end\":139451,\"start\":139384},{\"end\":139764,\"start\":139667},{\"end\":140185,\"start\":140106},{\"end\":140514,\"start\":140411},{\"end\":140809,\"start\":140733},{\"end\":141145,\"start\":141092},{\"end\":141464,\"start\":141387},{\"end\":141803,\"start\":141723},{\"end\":142126,\"start\":142053},{\"end\":142395,\"start\":142364},{\"end\":142665,\"start\":142630},{\"end\":142832,\"start\":142788},{\"end\":143111,\"start\":143040},{\"end\":143430,\"start\":143364},{\"end\":143697,\"start\":143650},{\"end\":143958,\"start\":143908},{\"end\":144309,\"start\":144187},{\"end\":144626,\"start\":144592},{\"end\":144895,\"start\":144816},{\"end\":145161,\"start\":145116},{\"end\":145497,\"start\":145419},{\"end\":145836,\"start\":145759},{\"end\":146099,\"start\":146050},{\"end\":146384,\"start\":146322},{\"end\":146653,\"start\":146596},{\"end\":146932,\"start\":146856},{\"end\":147244,\"start\":147176},{\"end\":147531,\"start\":147470},{\"end\":147865,\"start\":147826},{\"end\":148036,\"start\":148017},{\"end\":148207,\"start\":148165},{\"end\":148473,\"start\":148426},{\"end\":148793,\"start\":148746},{\"end\":149073,\"start\":149033},{\"end\":149363,\"start\":149295},{\"end\":150064,\"start\":149981},{\"end\":150368,\"start\":150307},{\"end\":150755,\"start\":150663},{\"end\":151101,\"start\":151030},{\"end\":151415,\"start\":151349},{\"end\":151690,\"start\":151637},{\"end\":151861,\"start\":151834},{\"end\":152213,\"start\":152152},{\"end\":152549,\"start\":152431},{\"end\":152898,\"start\":152788},{\"end\":153230,\"start\":153183},{\"end\":153777,\"start\":153698},{\"end\":154104,\"start\":154046},{\"end\":154424,\"start\":154382},{\"end\":154733,\"start\":154698},{\"end\":154956,\"start\":154896},{\"end\":155248,\"start\":155183},{\"end\":155557,\"start\":155502}]", "bib_author": "[{\"end\":97485,\"start\":97474},{\"end\":97502,\"start\":97485},{\"end\":97510,\"start\":97502},{\"end\":97798,\"start\":97781},{\"end\":98020,\"start\":98007},{\"end\":98033,\"start\":98020},{\"end\":98047,\"start\":98033},{\"end\":98061,\"start\":98047},{\"end\":98411,\"start\":98389},{\"end\":98425,\"start\":98411},{\"end\":98441,\"start\":98425},{\"end\":98457,\"start\":98441},{\"end\":98869,\"start\":98856},{\"end\":98884,\"start\":98869},{\"end\":99275,\"start\":99262},{\"end\":99290,\"start\":99275},{\"end\":99614,\"start\":99593},{\"end\":99637,\"start\":99614},{\"end\":99645,\"start\":99637},{\"end\":99859,\"start\":99846},{\"end\":99872,\"start\":99859},{\"end\":99890,\"start\":99872},{\"end\":99904,\"start\":99890},{\"end\":100156,\"start\":100138},{\"end\":100174,\"start\":100156},{\"end\":100190,\"start\":100174},{\"end\":100429,\"start\":100407},{\"end\":100444,\"start\":100429},{\"end\":100460,\"start\":100444},{\"end\":100482,\"start\":100460},{\"end\":100492,\"start\":100482},{\"end\":100507,\"start\":100492},{\"end\":100851,\"start\":100832},{\"end\":101113,\"start\":101095},{\"end\":101136,\"start\":101113},{\"end\":101353,\"start\":101334},{\"end\":101369,\"start\":101353},{\"end\":101387,\"start\":101369},{\"end\":101689,\"start\":101665},{\"end\":101706,\"start\":101689},{\"end\":101721,\"start\":101706},{\"end\":101742,\"start\":101721},{\"end\":101756,\"start\":101742},{\"end\":102129,\"start\":102110},{\"end\":102141,\"start\":102129},{\"end\":102378,\"start\":102361},{\"end\":102393,\"start\":102378},{\"end\":102402,\"start\":102393},{\"end\":102671,\"start\":102653},{\"end\":102688,\"start\":102671},{\"end\":102707,\"start\":102688},{\"end\":102719,\"start\":102707},{\"end\":103010,\"start\":102993},{\"end\":103023,\"start\":103010},{\"end\":103039,\"start\":103023},{\"end\":103337,\"start\":103326},{\"end\":103352,\"start\":103337},{\"end\":103364,\"start\":103352},{\"end\":103373,\"start\":103364},{\"end\":103649,\"start\":103636},{\"end\":103660,\"start\":103649},{\"end\":103904,\"start\":103890},{\"end\":103918,\"start\":103904},{\"end\":103936,\"start\":103918},{\"end\":103950,\"start\":103936},{\"end\":103957,\"start\":103950},{\"end\":103977,\"start\":103957},{\"end\":104294,\"start\":104281},{\"end\":104306,\"start\":104294},{\"end\":104627,\"start\":104614},{\"end\":104640,\"start\":104627},{\"end\":104917,\"start\":104902},{\"end\":104931,\"start\":104917},{\"end\":105166,\"start\":105154},{\"end\":105182,\"start\":105166},{\"end\":105190,\"start\":105182},{\"end\":105459,\"start\":105445},{\"end\":105472,\"start\":105459},{\"end\":105738,\"start\":105726},{\"end\":105751,\"start\":105738},{\"end\":105762,\"start\":105751},{\"end\":106029,\"start\":106015},{\"end\":106039,\"start\":106029},{\"end\":106325,\"start\":106307},{\"end\":106570,\"start\":106555},{\"end\":106586,\"start\":106570},{\"end\":106606,\"start\":106586},{\"end\":106624,\"start\":106606},{\"end\":106895,\"start\":106880},{\"end\":106911,\"start\":106895},{\"end\":106931,\"start\":106911},{\"end\":106949,\"start\":106931},{\"end\":107251,\"start\":107236},{\"end\":107271,\"start\":107251},{\"end\":107290,\"start\":107271},{\"end\":107506,\"start\":107486},{\"end\":107524,\"start\":107506},{\"end\":107534,\"start\":107524},{\"end\":107550,\"start\":107534},{\"end\":107566,\"start\":107550},{\"end\":107583,\"start\":107566},{\"end\":107600,\"start\":107583},{\"end\":107614,\"start\":107600},{\"end\":107630,\"start\":107614},{\"end\":107636,\"start\":107630},{\"end\":107993,\"start\":107976},{\"end\":108005,\"start\":107993},{\"end\":108284,\"start\":108267},{\"end\":108296,\"start\":108284},{\"end\":108554,\"start\":108541},{\"end\":108566,\"start\":108554},{\"end\":108582,\"start\":108566},{\"end\":108854,\"start\":108844},{\"end\":108873,\"start\":108854},{\"end\":108881,\"start\":108873},{\"end\":109163,\"start\":109141},{\"end\":109179,\"start\":109163},{\"end\":109193,\"start\":109179},{\"end\":109209,\"start\":109193},{\"end\":109561,\"start\":109538},{\"end\":109580,\"start\":109561},{\"end\":109862,\"start\":109839},{\"end\":109882,\"start\":109862},{\"end\":109909,\"start\":109882},{\"end\":110194,\"start\":110179},{\"end\":110203,\"start\":110194},{\"end\":110218,\"start\":110203},{\"end\":110487,\"start\":110472},{\"end\":110502,\"start\":110487},{\"end\":110518,\"start\":110502},{\"end\":110534,\"start\":110518},{\"end\":110878,\"start\":110855},{\"end\":110893,\"start\":110878},{\"end\":111116,\"start\":111106},{\"end\":111307,\"start\":111291},{\"end\":111323,\"start\":111307},{\"end\":111342,\"start\":111323},{\"end\":111592,\"start\":111580},{\"end\":111607,\"start\":111592},{\"end\":111624,\"start\":111607},{\"end\":111638,\"start\":111624},{\"end\":111874,\"start\":111856},{\"end\":111890,\"start\":111874},{\"end\":112126,\"start\":112111},{\"end\":112144,\"start\":112126},{\"end\":112172,\"start\":112144},{\"end\":112182,\"start\":112172},{\"end\":112454,\"start\":112442},{\"end\":112466,\"start\":112454},{\"end\":112763,\"start\":112742},{\"end\":113060,\"start\":113049},{\"end\":113072,\"start\":113060},{\"end\":113085,\"start\":113072},{\"end\":113096,\"start\":113085},{\"end\":113106,\"start\":113096},{\"end\":113117,\"start\":113106},{\"end\":113127,\"start\":113117},{\"end\":113135,\"start\":113127},{\"end\":113478,\"start\":113465},{\"end\":113486,\"start\":113478},{\"end\":113501,\"start\":113486},{\"end\":113515,\"start\":113501},{\"end\":113524,\"start\":113515},{\"end\":113539,\"start\":113524},{\"end\":113806,\"start\":113798},{\"end\":113823,\"start\":113806},{\"end\":114038,\"start\":114029},{\"end\":114052,\"start\":114038},{\"end\":114290,\"start\":114277},{\"end\":114307,\"start\":114290},{\"end\":114327,\"start\":114307},{\"end\":114340,\"start\":114327},{\"end\":114355,\"start\":114340},{\"end\":114371,\"start\":114355},{\"end\":114623,\"start\":114609},{\"end\":114638,\"start\":114623},{\"end\":114809,\"start\":114795},{\"end\":114824,\"start\":114809},{\"end\":115033,\"start\":115016},{\"end\":115057,\"start\":115033},{\"end\":115321,\"start\":115304},{\"end\":115339,\"start\":115321},{\"end\":115519,\"start\":115500},{\"end\":115532,\"start\":115519},{\"end\":115805,\"start\":115790},{\"end\":115824,\"start\":115805},{\"end\":116125,\"start\":116111},{\"end\":116139,\"start\":116125},{\"end\":116148,\"start\":116139},{\"end\":116371,\"start\":116356},{\"end\":116390,\"start\":116371},{\"end\":116403,\"start\":116390},{\"end\":116415,\"start\":116403},{\"end\":116433,\"start\":116415},{\"end\":116769,\"start\":116753},{\"end\":116787,\"start\":116769},{\"end\":117039,\"start\":117021},{\"end\":117280,\"start\":117270},{\"end\":117296,\"start\":117280},{\"end\":117311,\"start\":117296},{\"end\":117327,\"start\":117311},{\"end\":117347,\"start\":117327},{\"end\":117360,\"start\":117347},{\"end\":117719,\"start\":117710},{\"end\":117734,\"start\":117719},{\"end\":117742,\"start\":117734},{\"end\":117977,\"start\":117963},{\"end\":117999,\"start\":117977},{\"end\":118011,\"start\":117999},{\"end\":118267,\"start\":118240},{\"end\":118281,\"start\":118267},{\"end\":118502,\"start\":118475},{\"end\":118516,\"start\":118502},{\"end\":118771,\"start\":118753},{\"end\":118969,\"start\":118951},{\"end\":119226,\"start\":119208},{\"end\":119245,\"start\":119226},{\"end\":119511,\"start\":119494},{\"end\":119525,\"start\":119511},{\"end\":119539,\"start\":119525},{\"end\":119554,\"start\":119539},{\"end\":119571,\"start\":119554},{\"end\":119888,\"start\":119875},{\"end\":119902,\"start\":119888},{\"end\":120139,\"start\":120126},{\"end\":120153,\"start\":120139},{\"end\":120383,\"start\":120368},{\"end\":120401,\"start\":120383},{\"end\":120592,\"start\":120577},{\"end\":120610,\"start\":120592},{\"end\":120876,\"start\":120859},{\"end\":120892,\"start\":120876},{\"end\":120911,\"start\":120892},{\"end\":121175,\"start\":121159},{\"end\":121187,\"start\":121175},{\"end\":121200,\"start\":121187},{\"end\":121412,\"start\":121395},{\"end\":121430,\"start\":121412},{\"end\":121446,\"start\":121430},{\"end\":121461,\"start\":121446},{\"end\":121478,\"start\":121461},{\"end\":121768,\"start\":121753},{\"end\":121779,\"start\":121768},{\"end\":121792,\"start\":121779},{\"end\":122133,\"start\":122118},{\"end\":122144,\"start\":122133},{\"end\":122157,\"start\":122144},{\"end\":122474,\"start\":122459},{\"end\":122485,\"start\":122474},{\"end\":122505,\"start\":122485},{\"end\":122843,\"start\":122832},{\"end\":122864,\"start\":122843},{\"end\":122887,\"start\":122864},{\"end\":122905,\"start\":122887},{\"end\":122919,\"start\":122905},{\"end\":123217,\"start\":123209},{\"end\":123231,\"start\":123217},{\"end\":123248,\"start\":123231},{\"end\":123264,\"start\":123248},{\"end\":123268,\"start\":123264},{\"end\":123563,\"start\":123555},{\"end\":123585,\"start\":123563},{\"end\":123600,\"start\":123585},{\"end\":123616,\"start\":123600},{\"end\":123627,\"start\":123616},{\"end\":123640,\"start\":123627},{\"end\":123654,\"start\":123640},{\"end\":123669,\"start\":123654},{\"end\":123673,\"start\":123669},{\"end\":123959,\"start\":123947},{\"end\":123974,\"start\":123959},{\"end\":123991,\"start\":123974},{\"end\":124226,\"start\":124202},{\"end\":124238,\"start\":124226},{\"end\":124252,\"start\":124238},{\"end\":124260,\"start\":124252},{\"end\":124653,\"start\":124629},{\"end\":124665,\"start\":124653},{\"end\":124679,\"start\":124665},{\"end\":124687,\"start\":124679},{\"end\":125019,\"start\":124999},{\"end\":125035,\"start\":125019},{\"end\":125054,\"start\":125035},{\"end\":125330,\"start\":125310},{\"end\":125344,\"start\":125330},{\"end\":125360,\"start\":125344},{\"end\":125376,\"start\":125360},{\"end\":125391,\"start\":125376},{\"end\":125408,\"start\":125391},{\"end\":125710,\"start\":125701},{\"end\":125726,\"start\":125710},{\"end\":125745,\"start\":125726},{\"end\":125757,\"start\":125745},{\"end\":126046,\"start\":126036},{\"end\":126059,\"start\":126046},{\"end\":126071,\"start\":126059},{\"end\":126351,\"start\":126326},{\"end\":126357,\"start\":126351},{\"end\":126696,\"start\":126670},{\"end\":126702,\"start\":126696},{\"end\":127010,\"start\":126996},{\"end\":127024,\"start\":127010},{\"end\":127039,\"start\":127024},{\"end\":127047,\"start\":127039},{\"end\":127056,\"start\":127047},{\"end\":127063,\"start\":127056},{\"end\":127386,\"start\":127371},{\"end\":127402,\"start\":127386},{\"end\":127418,\"start\":127402},{\"end\":127703,\"start\":127689},{\"end\":127939,\"start\":127930},{\"end\":127952,\"start\":127939},{\"end\":127960,\"start\":127952},{\"end\":128260,\"start\":128244},{\"end\":128270,\"start\":128260},{\"end\":128286,\"start\":128270},{\"end\":128303,\"start\":128286},{\"end\":128319,\"start\":128303},{\"end\":128339,\"start\":128319},{\"end\":128352,\"start\":128339},{\"end\":128750,\"start\":128733},{\"end\":128763,\"start\":128750},{\"end\":128777,\"start\":128763},{\"end\":128790,\"start\":128777},{\"end\":128800,\"start\":128790},{\"end\":129104,\"start\":129085},{\"end\":129119,\"start\":129104},{\"end\":129133,\"start\":129119},{\"end\":129144,\"start\":129133},{\"end\":129454,\"start\":129437},{\"end\":129473,\"start\":129454},{\"end\":129492,\"start\":129473},{\"end\":129510,\"start\":129492},{\"end\":129531,\"start\":129510},{\"end\":129964,\"start\":129948},{\"end\":129979,\"start\":129964},{\"end\":129994,\"start\":129979},{\"end\":130006,\"start\":129994},{\"end\":130019,\"start\":130006},{\"end\":130037,\"start\":130019},{\"end\":130050,\"start\":130037},{\"end\":130367,\"start\":130349},{\"end\":130381,\"start\":130367},{\"end\":130689,\"start\":130674},{\"end\":130705,\"start\":130689},{\"end\":130720,\"start\":130705},{\"end\":131006,\"start\":130987},{\"end\":131024,\"start\":131006},{\"end\":131038,\"start\":131024},{\"end\":131055,\"start\":131038},{\"end\":131069,\"start\":131055},{\"end\":131345,\"start\":131326},{\"end\":131363,\"start\":131345},{\"end\":131377,\"start\":131363},{\"end\":131394,\"start\":131377},{\"end\":131408,\"start\":131394},{\"end\":131733,\"start\":131714},{\"end\":131751,\"start\":131733},{\"end\":131765,\"start\":131751},{\"end\":131782,\"start\":131765},{\"end\":131796,\"start\":131782},{\"end\":132097,\"start\":132078},{\"end\":132115,\"start\":132097},{\"end\":132129,\"start\":132115},{\"end\":132146,\"start\":132129},{\"end\":132160,\"start\":132146},{\"end\":132490,\"start\":132476},{\"end\":132507,\"start\":132490},{\"end\":132781,\"start\":132765},{\"end\":132796,\"start\":132781},{\"end\":132811,\"start\":132796},{\"end\":133100,\"start\":133084},{\"end\":133114,\"start\":133100},{\"end\":133130,\"start\":133114},{\"end\":133148,\"start\":133130},{\"end\":133456,\"start\":133436},{\"end\":133473,\"start\":133456},{\"end\":133484,\"start\":133473},{\"end\":133502,\"start\":133484},{\"end\":133517,\"start\":133502},{\"end\":133531,\"start\":133517},{\"end\":133550,\"start\":133531},{\"end\":133565,\"start\":133550},{\"end\":133581,\"start\":133565},{\"end\":133598,\"start\":133581},{\"end\":133609,\"start\":133598},{\"end\":133627,\"start\":133609},{\"end\":133643,\"start\":133627},{\"end\":134133,\"start\":134119},{\"end\":134353,\"start\":134339},{\"end\":134370,\"start\":134353},{\"end\":134384,\"start\":134370},{\"end\":134642,\"start\":134627},{\"end\":134849,\"start\":134840},{\"end\":134860,\"start\":134849},{\"end\":135055,\"start\":135040},{\"end\":135341,\"start\":135316},{\"end\":135355,\"start\":135341},{\"end\":135371,\"start\":135355},{\"end\":135381,\"start\":135371},{\"end\":135684,\"start\":135675},{\"end\":135698,\"start\":135684},{\"end\":135713,\"start\":135698},{\"end\":135723,\"start\":135713},{\"end\":136030,\"start\":136014},{\"end\":136047,\"start\":136030},{\"end\":136061,\"start\":136047},{\"end\":136075,\"start\":136061},{\"end\":136088,\"start\":136075},{\"end\":136105,\"start\":136088},{\"end\":136495,\"start\":136480},{\"end\":136514,\"start\":136495},{\"end\":136529,\"start\":136514},{\"end\":136543,\"start\":136529},{\"end\":136820,\"start\":136805},{\"end\":136839,\"start\":136820},{\"end\":136854,\"start\":136839},{\"end\":136868,\"start\":136854},{\"end\":137135,\"start\":137117},{\"end\":137433,\"start\":137415},{\"end\":137448,\"start\":137433},{\"end\":137465,\"start\":137448},{\"end\":137479,\"start\":137465},{\"end\":137494,\"start\":137479},{\"end\":137511,\"start\":137494},{\"end\":137984,\"start\":137971},{\"end\":137998,\"start\":137984},{\"end\":138010,\"start\":137998},{\"end\":138017,\"start\":138010},{\"end\":138348,\"start\":138332},{\"end\":138570,\"start\":138555},{\"end\":138584,\"start\":138570},{\"end\":138804,\"start\":138790},{\"end\":138820,\"start\":138804},{\"end\":138835,\"start\":138820},{\"end\":138849,\"start\":138835},{\"end\":139151,\"start\":139135},{\"end\":139170,\"start\":139151},{\"end\":139469,\"start\":139453},{\"end\":139488,\"start\":139469},{\"end\":139787,\"start\":139766},{\"end\":139805,\"start\":139787},{\"end\":139827,\"start\":139805},{\"end\":140199,\"start\":140187},{\"end\":140211,\"start\":140199},{\"end\":140219,\"start\":140211},{\"end\":140535,\"start\":140516},{\"end\":140830,\"start\":140811},{\"end\":140853,\"start\":140830},{\"end\":140870,\"start\":140853},{\"end\":141165,\"start\":141147},{\"end\":141178,\"start\":141165},{\"end\":141484,\"start\":141466},{\"end\":141497,\"start\":141484},{\"end\":141823,\"start\":141805},{\"end\":141836,\"start\":141823},{\"end\":141848,\"start\":141836},{\"end\":142146,\"start\":142128},{\"end\":142156,\"start\":142146},{\"end\":142168,\"start\":142156},{\"end\":142424,\"start\":142397},{\"end\":142443,\"start\":142424},{\"end\":142683,\"start\":142667},{\"end\":142849,\"start\":142834},{\"end\":142862,\"start\":142849},{\"end\":142878,\"start\":142862},{\"end\":143126,\"start\":143113},{\"end\":143143,\"start\":143126},{\"end\":143153,\"start\":143143},{\"end\":143166,\"start\":143153},{\"end\":143448,\"start\":143432},{\"end\":143466,\"start\":143448},{\"end\":143713,\"start\":143699},{\"end\":143727,\"start\":143713},{\"end\":143745,\"start\":143727},{\"end\":143974,\"start\":143960},{\"end\":143990,\"start\":143974},{\"end\":144008,\"start\":143990},{\"end\":144322,\"start\":144311},{\"end\":144644,\"start\":144628},{\"end\":144659,\"start\":144644},{\"end\":144677,\"start\":144659},{\"end\":144910,\"start\":144897},{\"end\":144923,\"start\":144910},{\"end\":145172,\"start\":145163},{\"end\":145184,\"start\":145172},{\"end\":145197,\"start\":145184},{\"end\":145212,\"start\":145197},{\"end\":145223,\"start\":145212},{\"end\":145514,\"start\":145499},{\"end\":145524,\"start\":145514},{\"end\":145542,\"start\":145524},{\"end\":145852,\"start\":145838},{\"end\":145866,\"start\":145852},{\"end\":146113,\"start\":146101},{\"end\":146119,\"start\":146113},{\"end\":146398,\"start\":146386},{\"end\":146411,\"start\":146398},{\"end\":146422,\"start\":146411},{\"end\":146667,\"start\":146655},{\"end\":146680,\"start\":146667},{\"end\":146691,\"start\":146680},{\"end\":146946,\"start\":146934},{\"end\":146959,\"start\":146946},{\"end\":146970,\"start\":146959},{\"end\":147260,\"start\":147246},{\"end\":147272,\"start\":147260},{\"end\":147550,\"start\":147533},{\"end\":147568,\"start\":147550},{\"end\":147586,\"start\":147568},{\"end\":147600,\"start\":147586},{\"end\":147879,\"start\":147867},{\"end\":148050,\"start\":148038},{\"end\":148221,\"start\":148209},{\"end\":148238,\"start\":148221},{\"end\":148487,\"start\":148475},{\"end\":148494,\"start\":148487},{\"end\":148510,\"start\":148494},{\"end\":148807,\"start\":148795},{\"end\":148814,\"start\":148807},{\"end\":148830,\"start\":148814},{\"end\":149087,\"start\":149075},{\"end\":149094,\"start\":149087},{\"end\":149110,\"start\":149094},{\"end\":149126,\"start\":149110},{\"end\":149377,\"start\":149365},{\"end\":149393,\"start\":149377},{\"end\":149400,\"start\":149393},{\"end\":149416,\"start\":149400},{\"end\":149652,\"start\":149623},{\"end\":149667,\"start\":149652},{\"end\":149684,\"start\":149667},{\"end\":149702,\"start\":149684},{\"end\":149724,\"start\":149702},{\"end\":150083,\"start\":150066},{\"end\":150097,\"start\":150083},{\"end\":150387,\"start\":150370},{\"end\":150403,\"start\":150387},{\"end\":150422,\"start\":150403},{\"end\":150432,\"start\":150422},{\"end\":150447,\"start\":150432},{\"end\":150767,\"start\":150757},{\"end\":150781,\"start\":150767},{\"end\":150794,\"start\":150781},{\"end\":150808,\"start\":150794},{\"end\":151128,\"start\":151103},{\"end\":151142,\"start\":151128},{\"end\":151146,\"start\":151142},{\"end\":151440,\"start\":151417},{\"end\":151456,\"start\":151440},{\"end\":151707,\"start\":151692},{\"end\":151878,\"start\":151863},{\"end\":151894,\"start\":151878},{\"end\":151909,\"start\":151894},{\"end\":151925,\"start\":151909},{\"end\":152226,\"start\":152215},{\"end\":152242,\"start\":152226},{\"end\":152256,\"start\":152242},{\"end\":152563,\"start\":152551},{\"end\":152579,\"start\":152563},{\"end\":152912,\"start\":152900},{\"end\":152928,\"start\":152912},{\"end\":153247,\"start\":153232},{\"end\":153264,\"start\":153247},{\"end\":153276,\"start\":153264},{\"end\":153518,\"start\":153502},{\"end\":153532,\"start\":153518},{\"end\":153546,\"start\":153532},{\"end\":153794,\"start\":153779},{\"end\":153817,\"start\":153794},{\"end\":153836,\"start\":153817},{\"end\":154121,\"start\":154106},{\"end\":154138,\"start\":154121},{\"end\":154152,\"start\":154138},{\"end\":154167,\"start\":154152},{\"end\":154442,\"start\":154426},{\"end\":154459,\"start\":154442},{\"end\":154470,\"start\":154459},{\"end\":154481,\"start\":154470},{\"end\":154747,\"start\":154735},{\"end\":154756,\"start\":154747},{\"end\":154970,\"start\":154958},{\"end\":154984,\"start\":154970},{\"end\":154998,\"start\":154984},{\"end\":155265,\"start\":155250},{\"end\":155274,\"start\":155265},{\"end\":155288,\"start\":155274},{\"end\":155307,\"start\":155288},{\"end\":155573,\"start\":155559},{\"end\":155588,\"start\":155573},{\"end\":155602,\"start\":155588},{\"end\":155616,\"start\":155602}]", "bib_venue": "[{\"end\":116803,\"start\":116799},{\"end\":97536,\"start\":97510},{\"end\":97820,\"start\":97798},{\"end\":98094,\"start\":98061},{\"end\":98485,\"start\":98457},{\"end\":98937,\"start\":98884},{\"end\":99328,\"start\":99290},{\"end\":99591,\"start\":99529},{\"end\":99930,\"start\":99904},{\"end\":100212,\"start\":100190},{\"end\":100529,\"start\":100507},{\"end\":100886,\"start\":100851},{\"end\":101158,\"start\":101136},{\"end\":101427,\"start\":101387},{\"end\":101794,\"start\":101756},{\"end\":102172,\"start\":102141},{\"end\":102436,\"start\":102402},{\"end\":102741,\"start\":102719},{\"end\":103065,\"start\":103039},{\"end\":103412,\"start\":103373},{\"end\":103678,\"start\":103660},{\"end\":104005,\"start\":103977},{\"end\":104359,\"start\":104306},{\"end\":104671,\"start\":104640},{\"end\":104953,\"start\":104931},{\"end\":105209,\"start\":105190},{\"end\":105488,\"start\":105472},{\"end\":105781,\"start\":105762},{\"end\":106077,\"start\":106039},{\"end\":106352,\"start\":106325},{\"end\":106637,\"start\":106624},{\"end\":107002,\"start\":106949},{\"end\":107234,\"start\":107202},{\"end\":107663,\"start\":107636},{\"end\":108058,\"start\":108005},{\"end\":108334,\"start\":108296},{\"end\":108604,\"start\":108582},{\"end\":108892,\"start\":108881},{\"end\":109262,\"start\":109209},{\"end\":109602,\"start\":109580},{\"end\":109927,\"start\":109909},{\"end\":110235,\"start\":110218},{\"end\":110572,\"start\":110534},{\"end\":110909,\"start\":110893},{\"end\":111135,\"start\":111116},{\"end\":111367,\"start\":111342},{\"end\":111654,\"start\":111638},{\"end\":111943,\"start\":111890},{\"end\":112209,\"start\":112182},{\"end\":112519,\"start\":112466},{\"end\":112816,\"start\":112763},{\"end\":113167,\"start\":113135},{\"end\":113553,\"start\":113539},{\"end\":113829,\"start\":113823},{\"end\":114071,\"start\":114052},{\"end\":114275,\"start\":114217},{\"end\":114657,\"start\":114638},{\"end\":114839,\"start\":114824},{\"end\":115088,\"start\":115057},{\"end\":115302,\"start\":115261},{\"end\":115573,\"start\":115532},{\"end\":115877,\"start\":115824},{\"end\":116161,\"start\":116148},{\"end\":116472,\"start\":116433},{\"end\":116797,\"start\":116787},{\"end\":117066,\"start\":117039},{\"end\":117400,\"start\":117360},{\"end\":117758,\"start\":117742},{\"end\":118049,\"start\":118011},{\"end\":118303,\"start\":118281},{\"end\":118536,\"start\":118516},{\"end\":118751,\"start\":118680},{\"end\":118996,\"start\":118969},{\"end\":119268,\"start\":119245},{\"end\":119608,\"start\":119571},{\"end\":119940,\"start\":119902},{\"end\":120187,\"start\":120153},{\"end\":120366,\"start\":120325},{\"end\":120645,\"start\":120610},{\"end\":120944,\"start\":120911},{\"end\":121217,\"start\":121200},{\"end\":121506,\"start\":121478},{\"end\":121845,\"start\":121792},{\"end\":122184,\"start\":122157},{\"end\":122544,\"start\":122505},{\"end\":122936,\"start\":122919},{\"end\":123298,\"start\":123268},{\"end\":123695,\"start\":123673},{\"end\":123997,\"start\":123991},{\"end\":124301,\"start\":124260},{\"end\":124719,\"start\":124687},{\"end\":125081,\"start\":125054},{\"end\":125424,\"start\":125408},{\"end\":125797,\"start\":125757},{\"end\":126099,\"start\":126071},{\"end\":126396,\"start\":126357},{\"end\":126730,\"start\":126702},{\"end\":127101,\"start\":127063},{\"end\":127458,\"start\":127418},{\"end\":127722,\"start\":127703},{\"end\":127979,\"start\":127960},{\"end\":128392,\"start\":128352},{\"end\":128819,\"start\":128800},{\"end\":129083,\"start\":129016},{\"end\":129595,\"start\":129531},{\"end\":129946,\"start\":129884},{\"end\":130402,\"start\":130381},{\"end\":130747,\"start\":130720},{\"end\":131101,\"start\":131069},{\"end\":131464,\"start\":131408},{\"end\":131827,\"start\":131796},{\"end\":132177,\"start\":132160},{\"end\":132542,\"start\":132507},{\"end\":132828,\"start\":132811},{\"end\":133171,\"start\":133148},{\"end\":133677,\"start\":133643},{\"end\":134171,\"start\":134133},{\"end\":134437,\"start\":134384},{\"end\":134668,\"start\":134642},{\"end\":134868,\"start\":134860},{\"end\":135089,\"start\":135055},{\"end\":135407,\"start\":135381},{\"end\":135761,\"start\":135723},{\"end\":136158,\"start\":136105},{\"end\":136565,\"start\":136543},{\"end\":136887,\"start\":136868},{\"end\":137166,\"start\":137135},{\"end\":137542,\"start\":137511},{\"end\":138055,\"start\":138017},{\"end\":138371,\"start\":138348},{\"end\":138606,\"start\":138584},{\"end\":138876,\"start\":138849},{\"end\":139198,\"start\":139170},{\"end\":139505,\"start\":139488},{\"end\":139873,\"start\":139827},{\"end\":140238,\"start\":140219},{\"end\":140554,\"start\":140535},{\"end\":140898,\"start\":140870},{\"end\":141231,\"start\":141178},{\"end\":141535,\"start\":141497},{\"end\":141867,\"start\":141848},{\"end\":142190,\"start\":142168},{\"end\":142483,\"start\":142443},{\"end\":142692,\"start\":142683},{\"end\":142905,\"start\":142878},{\"end\":143182,\"start\":143166},{\"end\":143496,\"start\":143466},{\"end\":143765,\"start\":143745},{\"end\":144027,\"start\":144008},{\"end\":144340,\"start\":144322},{\"end\":144683,\"start\":144677},{\"end\":144945,\"start\":144923},{\"end\":145260,\"start\":145223},{\"end\":145567,\"start\":145542},{\"end\":145888,\"start\":145866},{\"end\":146164,\"start\":146119},{\"end\":146439,\"start\":146422},{\"end\":146717,\"start\":146691},{\"end\":146996,\"start\":146970},{\"end\":147312,\"start\":147272},{\"end\":147635,\"start\":147600},{\"end\":147909,\"start\":147879},{\"end\":148074,\"start\":148050},{\"end\":148276,\"start\":148238},{\"end\":148563,\"start\":148510},{\"end\":148868,\"start\":148830},{\"end\":149145,\"start\":149126},{\"end\":149450,\"start\":149416},{\"end\":149792,\"start\":149740},{\"end\":150125,\"start\":150097},{\"end\":150463,\"start\":150447},{\"end\":150839,\"start\":150808},{\"end\":151170,\"start\":151146},{\"end\":151484,\"start\":151456},{\"end\":151724,\"start\":151707},{\"end\":151978,\"start\":151925},{\"end\":152278,\"start\":152256},{\"end\":152601,\"start\":152579},{\"end\":152966,\"start\":152928},{\"end\":153298,\"start\":153276},{\"end\":153500,\"start\":153442},{\"end\":153853,\"start\":153836},{\"end\":154193,\"start\":154167},{\"end\":154527,\"start\":154481},{\"end\":154777,\"start\":154756},{\"end\":155026,\"start\":154998},{\"end\":155329,\"start\":155307},{\"end\":155669,\"start\":155616}]"}}}, "year": 2023, "month": 12, "day": 17}