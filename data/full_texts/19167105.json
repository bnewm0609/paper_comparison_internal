{"id": 19167105, "updated": "2023-11-06 14:28:29.725", "metadata": {"title": "Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition", "authors": "[{\"first\":\"Sijie\",\"last\":\"Yan\",\"middle\":[]},{\"first\":\"Yuanjun\",\"last\":\"Xiong\",\"middle\":[]},{\"first\":\"Dahua\",\"last\":\"Lin\",\"middle\":[]}]", "venue": "Proceedings of the AAAI Conference on Artificial Intelligence", "journal": "Proceedings of the AAAI Conference on Artificial Intelligence", "publication_date": {"year": 2018, "month": 1, "day": 23}, "abstract": "Dynamics of human body skeletons convey significant information for human action recognition. Conventional approaches for modeling skeletons usually rely on hand-crafted parts or traversal rules, thus resulting in limited expressive power and difficulties of generalization. In this work, we propose a novel model of dynamic skeletons called Spatial-Temporal Graph Convolutional Networks (ST-GCN), which moves beyond the limitations of previous methods by automatically learning both the spatial and temporal patterns from data. This formulation not only leads to greater expressive power but also stronger generalization capability. On two large datasets, Kinetics and NTU-RGBD, it achieves substantial improvements over mainstream methods.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1801.07455", "mag": "2963076818", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/aaai/YanXL18", "doi": "10.1609/aaai.v32i1.12328"}}, "content": {"source": {"pdf_hash": "dd7fd497397c571a627914ccb40c3b57031b0f20", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1801.07455v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "7430140aead6cf2034ccc6085c2cec72afb25ce7", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/dd7fd497397c571a627914ccb40c3b57031b0f20.txt", "contents": "\nSpatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition\n\n\nSijie Yan \nDepartment of Information Engineering\nThe Chinese University of Hong Kong\n\n\nYuanjun Xiong bitxiong@gmail.com \nDepartment of Information Engineering\nThe Chinese University of Hong Kong\n\n\nDahua Lin dhlin@ie.cuhk.edu.hk \nDepartment of Information Engineering\nThe Chinese University of Hong Kong\n\n\nSpatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition\n\nDynamics of human body skeletons convey significant information for human action recognition. Conventional approaches for modeling skeletons usually rely on hand-crafted parts or traversal rules, thus resulting in limited expressive power and difficulties of generalization. In this work, we propose a novel model of dynamic skeletons called Spatial-Temporal Graph Convolutional Networks (ST-GCN), which moves beyond the limitations of previous methods by automatically learning both the spatial and temporal patterns from data. This formulation not only leads to greater expressive power but also stronger generalization capability. On two large datasets, Kinetics and NTU-RGBD, it achieves substantial improvements over mainstream methods.\n\nIntroduction\n\nHuman action recognition has become an active research area in recent years, as it plays a significant role in video understanding. In general, human action can be recognized from multiple modalities(Simonyan and Zisserman 2014; Tran et al. 2015;Wang, Qiao, and Tang 2015;Zhao et al. 2017), such as appearance, depth, optical flows, and body skeletons (Du, Wang, and Wang 2015;). Among these modalities, dynamic human skeletons usually convey significant information that is complementary to others. However, the modeling of dynamic skeletons has received relatively less attention than that of appearance and optical flows. In this work, we systematically study this modality, with an aim to develop a principled and effective method to model dynamic skeletons and leverage them for action recognition.\n\nThe dynamic skeleton modality can be naturally represented by a time series of human joint locations, in the form of 2D or 3D coordinates. Human actions can then be recognized by analyzing the motion patterns thereof. Earlier methods of using skeletons for action recognition simply employ the joint coordinates at individual time steps to form feature vectors, and apply temporal analysis thereon (Wang et al. 2012;Fernando et al. 2015). The capability of these methods is limited as they do not explicitly exploit the spatial relationships among the joints, which are crucial for understanding human actions. Recently, new methods that Copyright c 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Figure 1: The spatial temporal graph of a skeleton sequence used in this work where the proposed ST-GCN operate on. Blue dots denote the body joints. The intra-body edges between body joints are defined based on the natural connections in human bodies. The inter-frame edges connect the same joints between consecutive frames. Joint coordinates are used as inputs to the ST-GCN. attempt to leverage the natural connections between joints have been developed Du, Wang, and Wang 2015). These methods show encouraging improvement, which suggests the significance of the connectivity. Yet, most existing methods rely on hand-crafted parts or rules to analyze the spatial patterns. As a result, the models devised for a specific application are difficult to be generalized to others.\n\nTo move beyond such limitations, we need a new method that can automatically capture the patterns embedded in the spatial configuration of the joints as well as their temporal dynamics. This is the strength of deep neural networks. However, as mentioned, the skeletons are in the form of graphs instead of a 2D or 3D grids, which makes it difficult to use proven models like convolutional networks. Recently, Graph Neural networks (GCNs), which generalize convolutional neural networks (CNNs) to graphs of arbitrary structures, have received increasing attention and successfully been adopted in a number of applications, such as image classification (Bruna et al. 2014), document classification (Defferrard, Bresson, and Vandergheynst 2016), and semi-supervised learning (Kipf and Welling 2017). However, much of the prior work along this line assumes a fixed graph as input. The application of GCNs to model dynamic graphs over large-scale datasets, e.g. human skeleton sequences, is yet to be explored.\n\nIn this paper, we propose to design a generic representation of skeleton sequences for action recognition by extending graph neural networks to a spatial-temporal graph model, called Spatial-Temporal Graph Convolutional Networks (ST-GCN). As illustrated in Figure 1 this model is formulated on top of a sequence of skeleton graphs, where each node corresponds to a joint of the human body. There are two types of edges, namely the spatial edges that conform to the natural connectivity of joints and the temporal edges that connect the same joints across consecutive time steps. Multiple layers of spatial temporal graph convolution are constructed thereon, which allow information to be integrated along both the spatial and the temporal dimension.\n\nThe hierarchical nature of ST-GCN eliminates the need of hand-crafted part assignment or traversal rules. This not only leads to greater expressive power and thus higher performance (as shown in our experiments), but also makes it easy to generalize to different contexts. Upon the generic GCN formulation, we also study new strategies to design graph convolution kernels, with inspirations from image models.\n\nThe major contributions of this work lie in three aspects: 1) We propose ST-GCN, a generic graph-based formulation for modeling dynamic skeletons, which is the first that applies graph-based neural networks for this task. 2) We propose several principles in designing convolution kernels in ST-GCN to meet the specific demands in skeleton modeling. 3) On two large scale datasets for skeleton-based action recognition, the proposed model achieves superior performance as compared to previous methods using hand-crafted parts or traversal rules, with considerably less effort in manual design. The code and models of ST-GCN are made publicly available 1 .\n\n\nRelated work\n\nNeural Networks on Graphs. Generalizing neural networks to data with graph structures is an emerging topic in deep learning research. The discussed neural network architectures include both recurrent neural networks (Tai, Socher, and Manning 2015;Van Oord, Kalchbrenner, and Kavukcuoglu 2016) and convolutional neural networks (CNNs) (Bruna et al. 2014;Henaff, Bruna, and LeCun 2015;Duvenaud et al. 2015;Defferrard, Bresson, and Vandergheynst 2016). This work is more related to the generalization of CNNs, or graph convolutional networks (GCNs). The principle of constructing GCNs on graph generally follows two streams: 1) the spectral perspective, where the locality of the graph convolution is considered in the form of spectral analysis (Henaff, Bruna, and Le-Cun 2015;Duvenaud et al. 2015;Kipf and Welling 2017); 2) the spatial perspective, where the convolution filters are applied directly on the graph nodes and their neighbors (Bruna et al. 2014;Niepert, Ahmed, and Kutzkov 2016). This work follows the spirit of the second stream. We 1 https://github.com/yysijie/st-gcn construct the CNN filters on the spatial domain, by limiting the application of each filter to the 1-neighbor of each node.\n\nSkeleton Based Action Recognition. Skeleton and joint trajectories of human bodies are robust to illumination change and scene variation, and they are easy to obtain owing to the highly accurate depth sensors or pose estimation algorithms (Shotton et al. 2011;Cao et al. 2017a). There is thus a broad array of skeleton based action recognition approaches. The approaches can be categorized into handcrafted feature based methods and deep learning methods. The first type of approaches design several handcrafted features to capture the dynamics of joint motion. These could be covariance matrices of joint trajectories (Hussein et al. 2013), relative positions of joints (Wang et al. 2012), or rotations and translations between body parts (Vemulapalli, Arrate, and Chellappa 2014). The recent success of deep learning has lead to the surge of deep learning based skeleton modeling methods. These works have been using recurrent neural networks Zhu et al. 2016;Zhang, Liu, and Xiao 2017) and temporal CNNs Ke et al. 2017;Kim and Reiter 2017) to learn action recognition models in an end-to-end manner. Among these approaches, many have emphasized the importance of modeling the joints within parts of human bodies. But these parts are usually explicitly assigned using domain knowledge. Our ST-GCN is the first to apply graph CNNs to the task of skeleton based action recognition. It differentiates from previous approaches in that it can learn the part information implicitly by harnessing locality of graph convolution together with the temporal dynamics. By eliminating the need for manual part assignment, the model is easier to design and potent to learn better action representations.\n\n\nSpatial Temporal Graph ConvNet\n\nWhen performing activities, human joints move in small local groups, known as \"body parts\". Existing approaches for skeleton based action recognition have verified the effectiveness of introducing body parts in the modeling Zhang, Liu, and Xiao 2017). We argue that the improvement is largely due to that parts restrict the modeling of joints trajectories within \"local regions\" compared with the whole skeleton, thus forming a hierarchical representation of the skeleton sequences. In tasks such as image object recognition, the hierarchical representation and locality are usually achieved by the intrinsic properties of convolutional neural networks (Krizhevsky, Sutskever, and Hinton 2012), rather than manually assigning object parts. It motivates us to introduce the appealing property of CNNs to skeleton based action recognition. The result of this attempt is the ST-GCN model.\n\n\nPipeline Overview\n\nSkeleton based data can be obtained from motion-capture devices or pose estimation algorithms from videos. Usually the data is a sequence of frames, each frame will have a set of joint coordinates. Given the sequences of body joints in ST-GCNs Pose Estimation ...\n\n\nInput Video\n\nAction Classification Class Score Figure 2: We perform pose estimation on videos and construct spatial temporal graph on skeleton sequences. Multiple layers of spatial-temporal graph convolution (ST-GCN) will be applied and gradually generate higher-level feature maps on the graph. It will then be classified by the standard Softmax classifier to the corresponding action category. the form of 2D or 3D coordinates, we construct a spatial temporal graph with the joints as graph nodes and natural connectivities in both human body structures and time as graph edges. The input to the ST-GCN is therefore the joint coordinate vectors on the graph nodes. This can be considered as an analog to image based CNNs where the input is formed by pixel intensity vectors residing on the 2D image grid. Multiple layers of spatial-temporal graph convolution operations will be applied on the input data and generating higher-level feature maps on the graph. It will then be classified by the standard SoftMax classifier to the corresponding action category. The whole model is trained in an end-toend manner with backpropagation. We will now go over the components in the ST-GCN model.\n\n\nRunning\n\n\nSkeleton Graph Construction\n\nA skeleton sequence is usually represented by 2D or 3D coordinates of each human joint in each frame. Previous work using convolution for skeleton action recognition (Kim and Reiter 2017) concatenates coordinate vectors of all joints to form a single feature vector per frame. In our work, we utilize the spatial temporal graph to form hierarchical representation of the skeleton sequences. Particularly, we construct an undirected spatial temporal graph G = (V, E) on a skeleton sequence with N joints and T frames featuring both intra-body and inter-frame connection.\n\nIn this graph, the node set V = {v ti |t = 1, . . . , T, i = 1, . . . , N } includes the all the joints in a skeleton sequence. As ST-GCN's input, the feature vector on a node F (v ti ) consists of coordinate vectors, as well as estimation confidence, of the i-th joint on frame t. We construct the spatial temporal graph on the skeleton sequences in two steps. First, the joints within one frame are connected with edges according to the connectivity of human body structure, which is illustrated in Fig. 1. Then each joint will be connected to the same joint in the consecutive frame. The connections in this setup are thus naturally defined without the manual part assignment. This also enables the network architecture to work on datasets with different number of joints or joint connectivities. For example, on the Kinetics dataset, we use the 2D pose estimation results from the OpenPose (Cao et al. 2017b) toolbox which outputs 18 joints, while on the NTU-RGB+D dataset  we use 3D joint tracking results as input, which produces 25 joints. The ST-GCN can operate in both situations and provide consistent superior performance. An example of the constructed spatial temporal graph is illustrated in Fig. 1.\n\nFormally, the edge set E is composed of two subsets, the first subset depicts the intra-skeleton connection at each frame, denoted as E S = {v ti v tj |(i, j) \u2208 H}, where H is the set of naturally connected human body joints. The second subset contains the inter-frame edges, which connect the same joints in consecutive frames as\nE F = {v ti v (t+1)i }.\nTherefore all edges in E F for one particular joint i will represent its trajectory over time.\n\n\nSpatial Graph Convolutional Neural Network\n\nBefore we dive into the full-fledged ST-GCN, we first look at the graph CNN model within one single frame. In this case, on a single frame at time \u03c4 , there will be N joint nodes V t , along with the skeleton edges E S (\u03c4 ) = {v ti v tj |t = \u03c4, (i, j) \u2208 H}. Recall the definition of convolution operation on the 2D natural images or feature maps, which can be both treated as 2D grids. The output feature map of a convolution operation is again a 2D grid. With stride 1 and appropriate padding, the output feature maps can have the same size as the input feature maps. We will assume this condition in the following discussion. Given a convolution operator with the kernel size of K \u00d7 K, and an input feature map f in with the number of channels c. The output value for a single channel at the spatial location x can be written as\nf out (x) = K h=1 K w=1 f in (p(x, h, w)) \u00b7 w(h, w),(1)\nwhere the sampling function p : Z 2 \u00d7 Z 2 \u2192 Z 2 enumerates the neighbors of location x. In the case of image convolution, it can also be represented as p(x, h, w) = x + p (h, w). The weight function w : Z 2 \u2192 R c provides a weight vector in c-dimension real space for computing the inner product with the sampled input feature vectors of dimension c. Note that the weight function is irrelevant to the input location x. Thus the filter weights are shared everywhere on the input image. Standard convolution on the image domain is therefore achieved by encoding a rectangular grid in p(x). More detailed explanation and other applications of this formulation can be found in (Dai et al. 2017).\n\nThe convolution operation on graphs is then defined by extending the formulation above to the cases where the input features map resides on a spatial graph V t . That is, the feature map f t in : V t \u2192 R c has a vector on each node of the graph. The next step of the extension is to redefine the sampling function p and the weight function w.\n\nSampling function. On images, the sampling function p(h, w) is defined on the neighboring pixels with respect to the center location x. On graphs, we can similarly define the sampling function on the neighbor set B\n(v ti ) = {v tj |d(v tj , v ti ) \u2264 D} of a node v ti . Here d(v tj , v ti ) de- notes the minimum length of any path from v tj to v ti . Thus the sampling function p : B(v ti ) \u2192 V can be written as p(v ti , v tj ) = v tj .(2)\nIn this work we use D = 1 for all cases, that is, the 1neighbor set of joint nodes. The higher number of D is left for future works.\n\nWeight function. Compared with the sampling function, the weight function is trickier to define. In 2D convolution, a rigid grid naturally exists around the center location. So pixels within the neighbor can have a fixed spatial order. The weight function can then be implemented by indexing a tensor of (c, K, K) dimensions according to the spatial order. For general graphs like the one we just constructed, there is no such implicit arrangement. The solution to this problem is first investigated in (Niepert, Ahmed, and Kutzkov 2016), where the order is defined by a graph labeling process in the neighbor graph around the root node. We follow this idea to construct our weight function. Instead of giving every neighbor node a unique labeling, we simplify the process by partitioning the neighbor set B(v ti ) of a joint node v ti into a fixed number of K subsets, where each subset has a numeric label. Thus we can have a mapping l ti : B(v ti ) \u2192 {0, . . . , K \u2212 1} which maps a node in the neighborhood to its subset label. The weight function w(v ti , v tj ) : B(v ti ) \u2192 R c can be implemented by indexing a tensor of (c, K) dimension or\nw(v ti , v tj ) = w (l ti (v tj )).(3)\nWe will discuss several partitioning strategies in Sec. 3.4.\n\nSpatial Graph Convolution. With the refined sampling function and weight function, we now rewrite Eq. 1 in terms of graph convolution as\nf out (v ti ) = vtj \u2208B(vti) 1 Z ti (v tj ) f in (p(v ti , v tj )) \u00b7 w(v ti , v tj ),(4)\nwhere the normalizing term Z ti (v tj ) =| {v tk |l ti (v tk ) = l ti (v tj )} | equals the cardinality of the corresponding subset. This term is added to balance the contributions of different subsets to the output. Substituting Eq. 2 and Eq. 3 into Eq. 4, we arrive at\nf out (v ti ) = vtj \u2208B(vti) 1 Z ti (v tj ) f in (v tj ) \u00b7 w(l ti (v tj )). (5)\nIt is worth noting this formulation can resemble the standard 2D convolution if we treat a image as a regular 2D grid. For example, to resemble a 3 \u00d7 3 convolution operation, we have a neighbor of 9 pixels in the 3 \u00d7 3 grid centered on a pixel. The neighbor set should then be partitioned into 9 subsets, each having one pixel.\n\nSpatial Temporal Modeling. Having formulated spatial graph CNN, we now advance to the task of modeling the spatial temporal dynamics within skeleton sequence. Recall that in the construction of the graph, the temporal aspect of the graph is constructed by connecting the same joints across consecutive frames. This enable us to define a very simple strategy to extend the spatial graph CNN to the spatial temporal domain. That is, we extend the concept of neighborhood to also include temporally connected joints as\nB(v ti ) = {v qj |d(v tj , v ti ) \u2264 K, |q \u2212 t| \u2264 \u0393/2 }. (6)\nThe parameter \u0393 controls the temporal range to be included in the neighbor graph and can thus be called the temporal kernel size. To complete the convolution operation on the spatial temporal graph, we also need the sampling function, which is the same as the spatial only case, and the weight function, or in particular, the labeling map l ST . Because the temporal axis is well-ordered, we directly modify the label map l ST for a spatial temporal neighborhood rooted at v ti to be\nl ST (v qj ) = l ti (v tj ) + (q \u2212 t + \u0393/2 ) \u00d7 K,(7)\nwhere l ti (v tj ) is the label map for the single frame case at v ti . In this way, we have a well-defined convolution operation on the constructed spatial temporal graphs.\n\n\nPartition Strategies.\n\nGiven the high-level formulation of spatial temporal graph convolution, it is important to design a partitioning strategy to implement the label map l. In this work we explore several partition strategies. For simplicity, we only discuss the cases in a single frame because they can be naturally extended to the spatial-temporal domain using Eq. 7.\n\nUni-labeling. The simplest and most straight forward partition strategy is to have subset, which is the whole neighbor set itself. In this strategy, feature vectors on every neighboring node will have a inner product with the same weight vector. Actually, this strategy resembles the propagation rule introduced in (Kipf and Welling 2017). It has an obvious drawback that in the single frame case, using this strategy is equivalent to computing the inner product between the weight vector and the average feature vector of all neighboring nodes. This is suboptimal for skeleton sequence classification as the local differential properties could be lost in this operation. Formally, we have K = 1 and l ti (v tj ) = 0, \u2200i, j \u2208 V . Distance partitioning. Another natural partitioning strategy is to partition the neighbor set according to the nodes' distance d(\u00b7, v ti ) to the root node v ti . In this work, because we set D = 1, the neighbor set will then be separated into two subsets, where d = 0 refers to the root node itself and remaining neighbor nodes are in the d = 1 subset. Thus we will have two different weight vectors and they are capable of modeling local differential properties such as the relative translation between joints. Formally, we have K = 2 and\nl ti (v tj ) = d(v tj , v ti ) .\nSpatial configuration partitioning. Since the body skeleton is spatially localized, we can still utilize this specific spatial configuration in the partitioning process. We design a strategy to divide the neighbor set into three subsets: 1) the root node itself; 2)centripetal group: the neighboring nodes that are closer to the gravity center of the skeleton than the root node; 3) otherwise the centrifugal group. Here the average coordinate of all joints in the skeleton at a frame is treated as its gravity center. This strategy is inspired by the fact that motions of body parts can be broadly categorized as concentric and eccentric motions. Formally, we have\nl ti (v t j) = \uf8f1 \uf8f2 \uf8f3 0 if r j = r i 1 if r j < r i 2 if r j > r i(8)\nwhere r i is the average distance from gravity center to joint i over all frames in the training set. Visualization of the three partitioning strategies is shown in Fig. 3. We will empirically examine the proposed partioning strategies on skeleton based action recognition experiments. It is expected that a more advanced partitioning strategy will lead to better modeling capacity and recognition performance.\n\n\nLearnable edge importance weighting.\n\nAlthough joints move in groups when people are performing actions, one joint could appear in multiple body parts. These appearances, however, should have different importance in modeling the dynamics of these parts. In this sense, we add a learnable mask M on every layer of spatial temporal graph convolution. The mask will scale the contribution of a node's feature to its neighboring nodes based on the learned importance weight of each spatial graph edge in E S . Empirically we find adding this mask can further improve the recognition performance of ST-GCN. It is also possible to have a data dependent attention map for this sake. We leave this to future works.\n\n\nImplementing ST-GCN\n\nThe implementation of graph-based convolution is not as straightforward as 2D or 3D convolution. Here we provide details on implementing ST-GCN for skeleton based action recognition.\n\nWe adopt a similar implementation of graph convolution as in (Kipf and Welling 2017). The intra-body connections of joints within a single frame are represented by an adjacency matrix A and an identity matrix I representing selfconnections. In the single frame case, ST-GCN with the first partitioning strategy can be implemented with the following formula (Kipf and Welling 2017)\nf out = \u039b \u2212 1 2 (A + I)\u039b \u2212 1 2 f in W,(9)\nwhere \u039b ii = j (A ij + I ij ). Here the weight vectors of multiple output channels are stacked to form the weight matrix W. In practice, under the spatial temporal cases, we can represent the input feature map as a tensor of (C, V, T ) dimensions. The graph convolution is implemented by performing a 1 \u00d7 \u0393 standard 2D convolution and multiplies the resulting tensor with the normalized adjacency matrix \u039b \u2212 1 2 (A + I)\u039b \u2212 1 2 on the second dimension. For partitioning strategies with multiple subsets, i.e., distance partitioning and spatial configuration partitioning, we again utilize this implementation. But note now the adjacency matrix is dismantled into several matrixes A j where A + I = j A j . For example in the distance partitioning strategy, A 0 = I and A 1 = A. The Eq. 9 is transformed into\nf out = j \u039b \u2212 1 2 j A j \u039b \u2212 1 2 j f in W j ,(10)\nwhere similarly \u039b ii j = k (A ik j )+\u03b1. Here we set \u03b1 = 0.001 to avoid empty rows in A j .\n\nIt is straightforward to implement the learnable edge importance weighting. For each adjacency matrix, we accompany it with a learnable weight matrix M. And we substitute the matrix A + I in Eq. 9 and A j in A j in Eq. 10 with (A + I) \u2297 M and A j \u2297 M, respectively. Here \u2297 denotes element-wise product between two matrixes. The mask M is initialized as an all-one matrix.\n\nNetwork architecture and training. Since the ST-GCN share weights on different nodes, it is important to keep the scale of input data consistent on different joints. In our experiments, we first feed input skeletons to a batch normalization layer to normalize data. The ST-GCN model is composed of 9 layers of spatial temporal graph convolution operators (ST-GCN units). The first three layers have 64 channels for output. The follow three layers have 128 channels for output. And the last three layers have 256 channels for output. These layers have 9 temporal kernel size. The Resnet mechanism is applied on each ST-GCN unit. And we randomly dropout the features at 0.5 probability after each ST-GCN unit to avoid overfitting. The strides of the 4-th and the 7-th temporal convolution layers are set to 2 as pooling layer. After that, a global pooling was performed on the resulting tensor to get a 256 dimension feature vector for each sequence. Finally, we feed them to a SoftMax classifier. The models are learned using stochastic gradient descent with a learning rate of 0.01. We decay the learning rate by 0.1 after every 10 epochs. To avoid overfitting, we perform two kinds of augmentation to replace dropout layers when training on the Kinetics dataset (Kay et al. 2017). First, to simulate the camera movement, we perform random affine transformations on the skeleton sequences of all frames. Particularly, from the first frame to the last frame, we select a few fixed angle, translation and scaling factors as candidates and then randomly sampled two combinations of three factors to generate an affine transformation. This transformation is interpolated for intermediate frames to generate a effect as if we smoothly move the view point during playback. We name this augmentation as random moving. Second, we randomly sample fragments from the original skeleton sequences in training and use all frames in the test. Global pooling at the top of the network enables the network to handle the input sequences with indefinite length.\n\n\nExperiments\n\nIn this section we evaluate the performance of ST-GCN in skeleton based action recognition experiments. We experiment on two large-scale action recognition datasets with vastly different properties: Kinetics human action dataset (Kinetics) (Kay et al. 2017) is by far the largest unconstrained action recognition dataset, and NTU-RGB+D ) the largest in-house cap-tured action recognition dataset. In particular, we first perform detailed ablation study on the Kinetics dataset to examine the contributions of the proposed model components to the recognition performance. Then we compare the recognition results of ST-GCN with other state-of-the-art methods and other input modalities. To verify whether the experience we gained on in the unconstrained setting is universal, we experiment with the constraint setting on NTU-RGB+D and compare ST-GCN with other state-of-the-art approaches. All experiments were conducted on PyTorch deep learning framework with 8 TITANX GPUs.\n\n\nDataset & Evaluation Metrics\n\nKinetics. Deepmind Kinetics human action dataset (Kay et al. 2017) contains around 300, 000 video clips retrieved from YouTube. The videos cover as many as 400 human action classes, ranging from daily activities, sports scenes, to complex actions with interactions. Each clip in Kinetics lasts around 10 seconds.\n\nThis Kinetics dataset provides only raw video clips without skeleton data. In this work we are focusing on skeleton based action recognition, so we use the estimated joint locations in the pixel coordinate system as our input and discard the raw RGB frames. To obtain the joint locations, we first resize all videos to the resolution of 340 \u00d7 256 and convert the frame rate to 30 FPS. Then we use the public available OpenPose (Cao et al. 2017b) toolbox to estimate the location of 18 joints on every frame of the clips. The toolbox gives 2D coordinates (X, Y ) in the pixel coordinate system and confidence scores C for the 18 human joints. We thus represent each joint with a tuple of (X, Y, C) and a skeleton frame is recorded as an array of 18 tuples. For the multi-person cases, we select 2 people with the highest average joint confidence in each clip. In this way, one clip with T frames is transformed into a skeleton sequence of these tuples. In practice, we represent the clips with tensors of (3, T, 18, 2) dimensions. For simplicity, we pad every clip by replaying the sequence from the start to have T = 300. We will release the estimated joint locations on Kinetics for reproducing the results.\n\nWe evaluate the recognition performance by top-1 and top-5 classification accuracy as recommended by the dataset authors (Kay et al. 2017). The dataset provides a training set of 240, 000 clips and a validation set of 20, 000. We train the compared models on the training set and report the accuracies on the validation set.\n\nNTU-RGB+D: NTU-RGB+D ) is currently the largest dataset with 3D joints annotations for human action recognition task. This dataset contains 56, 000 action clips in 60 action classes. These clips are all performed by 40 volunteers captured in a constrained lab environment, with three camera views recorded simultaneously. The provided annotations give 3D joint locations (X, Y, Z) in the camera coordinate system, detected by the Kinect depth sensors. There are 25 joints for each subject in the skeleton sequences. Each clip is guaranteed to have at most 2 subjects.\n\nThe authors of this dataset recommend two benchmarks: 1) cross-subject (X-Sub) benchmark with 40, 320 and 16, 560 clips for training and evaluation. In this setting the training clips come from one subset of actors and the models are evaluated on clips from the remaining actors; 2) crossview(X-View) benchmark 37, 920 and 18, 960 clips. Training clips in this setting come from the camera views 2 and 3, and the evaluation clips are all from the camera view 1.\n\nWe follow this convention and report the top-1 recognition accuracy on both benchmarks.\n\n\nAblation Study\n\nWe examine the effectiveness of the proposed components in ST-GCN in this section by action recognition experiments on the Kinetics dataset (Kay et al. 2017).\n\nSpatial temporal graph convolution. First, we evaluate the necessity of using spatial temporal graph convolution operation. We use a baseline network architecture (Kim and Reiter 2017) where all spatial temporal convolutions are replaced by only temporal convolution. That is, we concatenate all input joint locations to form the input features at each frame t. The temporal convolution will then operate on this input and convolves over time. We call this model \"baseline TCN\". This kind of recognition models is known to work well on constraint dataset such as NTU-RGB+D (Kim and Reiter 2017). Seen from Table 1, models with spatial temporal graph convolution, with reasonable partitioning strategies, consistently outperform the baseline model on Kinetics. Actually, this temporal convolution is equivalent to spatial temporal graph convolution with unshared weights on a fully connected joint graph. So the major difference between the baseline model and ST-GCN models are the sparse natural connections and shared weights in convolution operation. Additionally, we evaluate an intermediate model between the baseline model and ST-GCN, referred as \"local convolution\". In this model we use the sparse joint graph as ST-GCN, but use convolution filters with unshared weights. We believe the better performance of ST-GCN based models could justify the power of the spatial temporal graph convolution in skeleton based action recognition.\n\nPartition strategies In this work we present three partitioning strategies: 1) uni-labeling; 2) distance partitioning; and 3) spatial configuration partitioning. We evaluate the performance of ST-GCN with these partitioning strategies.\n\nThe results are summarized in Table 1. We observe that partitioning with multiple subsets is generally much better than uni-labeling. This is in accordance with the obvious problem of uni-labeling that it is equivalent to simply averaging features before the convolution operation. Given this observation, we experiment with an intermediate between the distance partitioning and uni-labeling, referred to as \"distance partitioning*\". In this setting we bind the weights of the two subsets in distance partitioning to be different only by a scaling factor \u22121, or w 0 = \u2212w 1 . This setting still achieves better performance than uni-labeling, which again demonstrate the importance of the partitioning with multiple subsets. Among multi-subset partitioning strategies, the spatial configuration partitioning achieves better performance. This corroborates our motivation in designing this strategy, which takes into consideration the concentric and eccentric motion patterns. Based on these observations, we use the spatial configuration partitioning strategy in the following experiments.\n\nLearnable edge importance weighting. Another component in ST-GCN is the learnable edge importance weighting. We experiment with adding this component on the ST-GCN model with spatial configuration partitioning. This is referred to as \"ST-GCN+Imp.\" in Table 1 \n\n\nComparison with State of the Arts\n\nTo verify the performance of ST-GCN in both unconstrained and constraint environment, we perform experiments on Kinetics dataset (Kay et al. 2017) and NTU-RGB+D dataset , respectively.\n\nKinetics. On Kinetics, we compare with three characteristic approaches for skeleton based action recognition. The first is the feature encoding approach on hand-crafted features (Fernando et al. 2015), referred to as \"Feature Encoding\" in Table 2. We also implemented two deep learning based approaches on Kinetics, i.e. Deep LSTM  and Temporal ConvNet (Kim and Reiter 2017). We compare the approaches' recognition performance in terms of top-1 and top-5 accuracies. In Table 2 NTU-RGB+D. The NTU-RGB+D dataset is captured in a constraint environment, which allows for methods that require well stabilized skeleton sequences to work well. We   Table 3: Skeleton based action recognition performance on NTU-RGB+D datasets. We report the accuracies on both the cross-subject (X-Sub) and cross-view (X-View) benchmarks.\n\nalso compare our ST-GCN model with the previous state-ofthe-art methods on this dataset. Due to the constraint nature of this dataset, we do not use any data augmentation when training ST-GCN models. We follow the standard practice in literature to report cross-subject (X-Sub) and crossview (X-View) recognition performance in terms of top-1 classification accuracies. The compared methods include Lie Group (Veeriah, Zhuang, and Qi 2015), Hierarchical RNN (Du, Wang, and Wang 2015), Discussion. The two datasets in experiments have very different natures. On Kinetics the input is 2D skeletons detected with deep neural networks (Cao et al. 2017a), while on NTU-RGB+D the input is from Kinect depth sensor. On NTU-RGB+D the cameras are fixed, while on Kinetics the videos are usually shot by hand-held devices, leading to large camera motion. The fact that the proposed ST-GCN can work well on both datasets demonstrates the effectiveness of the proposed spatial temporal graph convolution operation and the resultant ST-GCN model. We also notice that on Kinetics the accuracies of skeleton based methods are inferior to video frame based models (Kay et al. 2017). We argue that this is due to a lot of ac-    Table 4. We can see that on this subset the performance gap is much smaller. We also explore using ST-GCN to capture motion information in two-stream style action recognition. As shown as in Fig. 5, our skeleton based model ST-GCN can also provide complementary information to RGB and optical flow models. We train the standard TSN ) models from scratches on Kinetics with RGB and optical flow models. Adding ST-GCN to the RGB model leads to 0.9% increase, even better than optical flows (0.8%). Combining RGB, optical flow, and ST-GCN further raises the performance to 71.7%. These results clearly show that the skeletons can provide complementary information when leveraged effectively (e.g. using ST-GCN).\n\n\nConclusion\n\nIn this paper, we present a novel model for skeleton based action recognition, the spatial temporal graph convolutional networks (ST-GCN). The model constructs a set of spatial temporal graph convolutions on the skeleton sequences. On two challenging large-scale datasets, the proposed ST-GCN outperforms the previous state-of-the-art skeleton based model. In addition, ST-GCN can capture motion information in dynamic skeleton sequences which is complementary to RGB modality. The combination of skeleton based model and frame based model further improves the performance in action recognition. The flexibility of ST-GCN model also opens up many possible directions for future works. For example, how to incorporate contextual information, such as scenes, objects, and interactions into ST-GCN becomes a natural question.\n\nFigure 3 :\n3The proposed partitioning strategies for constructing convolution operations. From left to right: (a) An example frame of input skeleton. Body joints are drawn with blue dots. The receptive fields of a filter with D = 1 are drawn with red dashed circles. (b) Uni-labeling partitioning strategy, where all nodes in a neighborhood has the same label (green). (c) Distance partitioning. The two subsets are the root node itself with distance 0 (green) and other neighboring points with distance 1. (blue). (d) Spatial configuration partitioning. The nodes are labeled according to their distances to the skeleton gravity center (black cross) compared with that of the root node (green). Centripetal nodes have shorter distances (blue), while centrifugal nodes have longer distances (yellow) than the root node.\n\n\nDeep LSTM (Shahroudy et al. 2016), Part-Aware LSTM (PA-LSTM) (Shahroudy et al. 2016), Spatial Temporal LSTM with Trust Gates (ST-LSTM+TS) (Liu et al. 2016), Temporal Convolutional Neural Networks (Temporal Conv.) (Kim and Reiter 2017), and Clips CNN + Multi-task learning (C-CNN+MTLN) (Ke et al. 2017). Our ST-GCN model, with rather simple architecture and no data augmentation as used in (Kim and Reiter 2017; Ke et al. 2017), is able to outperform previous stateof-the-art approaches on this dataset.\n\n\n. Given the high performing vanilla ST-GCN, this component is still able to raise the recognition performance by more than 1 percent. Recall that this component is inspired by the fact that joints in different parts have different importances. It is verified that the ST-GCN model can now learn to express the joint importance and improve the recognition performance. Based on this observation, we always use this component with ST-GCN in comparison with other state-of-the-art models.\n\n\n, ST-GCN is able to outperform previous representative approaches. For references, we list the performance of using RGB frames and optical flow for recognition as reported in(Kay et al. 2017).\n\n\nKay et al. 2017) 57.0% 77.3% Optical Flow (Kay et al. 2017) 49.5% 71.9% Feature Enc. (Fernando et al. 2015) 14.9% 25.8% Deep LSTM (Shahroudy et al. 2016) 16.4% 35.3% Temporal Conv. (Kim and Reiter 2017) 20.3% 40.0% ST-GCN 30.7% 52.8%Top-1 \n\nTop-5 \nRGB(\n\nTable 2 :\n2Action recognition performance for skeleton based models on the Kinetics dataset. On top of the table we list the performance of frame based methods.X-Sub X-View \nLie Group (Veeriah, Zhuang, and Qi 2015) 50.1% \n52.8% \nH-RNN (Du, Wang, and Wang 2015) \n59.1% \n64.0% \nDeep LSTM (Shahroudy et al. 2016) \n60.7% \n67.3% \nPA-LSTM (Shahroudy et al. 2016) \n62.9% \n70.3% \nST-LSTM+TS (Liu et al. 2016) \n69.2% \n77.7% \nTemporal Conv (Kim and Reiter 2017). \n74.3% \n83.1% \nC-CNN + MTLN (Ke et al. 2017) \n79.6% \n84.8% \nST-GCN \n81.5% 88.3% \n\n\n\nTable 4 :\n4Mean class accuracies on the \"Kinetics Motion\" subset of the Kinetics dataset. This subset contains 30 action classes in Kinetics which are strongly related to body motions.RGB TSN Flow TSN ST-GCN Acc(%) \nSingle \n70.3 \nModel \n51.0 \n30.7 \nEnsemble \n71.1 \nModel \n71.2 \n71.7 \n\n\n\nTable 5 :\n5Class accuracies on the Kinects dataset without ImageNet pretraining. Although our skeleton based model ST-GCN can not achieve the accuracy of the state of the art model performed on RGB and optical flow modalities, it can provide stronger complementary information than optical flow based model. tion classes in Kinetics requires recognizing the objects and scenes that the actors are interacting with. To verify this, we select a subset of 30 classes strongly related with body motions, named as \"Kinetics-Motion\" and list the mean class accuracies of skeleton and frame based models (Kay et al. 2017) on this subset in\n\nSpectral networks and locally connected networks on graphs. [ References, Bruna, ICLR. References [Bruna et al. 2014] Bruna, J.; Zaremba, W.; Szlam, A.; and Lecun, Y. 2014. Spectral networks and locally connected networks on graphs. In ICLR.\n\nRealtime multi-person 2d pose estimation using part affinity fields. CVPR. CVPRet al. 2017a] Cao, Z.; Simon, T.; Wei, S.-E.; and Sheikh, Y. 2017a. Realtime multi-person 2d pose estima- tion using part affinity fields. In CVPR. [Cao et al. 2017b] Cao, Z.; Simon, T.; Wei, S.-E.; and Sheikh, Y. 2017b. Realtime multi-person 2d pose estima- tion using part affinity fields. In CVPR.\n\nDeformable convolutional networks. arXiv:1703.06211Convolutional neural networks on graphs with fast localized spectral filtering. NIPSet al. 2017] Dai, J.; Qi, H.; Xiong, Y.; Li, Y.; Zhang, G.; Hu, H.; and Wei, Y. 2017. Deformable convolutional net- works. In arXiv:1703.06211. [Defferrard, Bresson, and Vandergheynst 2016] Defferrard, M.; Bresson, X.; and Vandergheynst, P. 2016. Convolu- tional neural networks on graphs with fast localized spectral filtering. In NIPS.\n\nHierarchical recurrent neural network for skeleton based action recognition. Wang , Wang ; Du, Y Wang, W Wang, L Duvenaud, D K Maclaurin, D Iparraguirre, J Bombarell, R Hirzel, T Aspuru-Guzik, A Adams, R P Fernando, B Gavves, E Oramas, J M Ghodrati, A Tuytelaars, T , Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionCVPR, Wang, and Wang 2015] Du, Y.; Wang, W.; and Wang, L. 2015. Hierarchical recurrent neural network for skeleton based action recognition. In CVPR, 1110-1118. [Duvenaud et al. 2015] Duvenaud, D. K.; Maclaurin, D.; Iparraguirre, J.; Bombarell, R.; Hirzel, T.; Aspuru-Guzik, A.; and Adams, R. P. 2015. Convolutional networks on graphs for learning molecular fingerprints. In NIPS. [Fernando et al. 2015] Fernando, B.; Gavves, E.; Oramas, J. M.; Ghodrati, A.; and Tuytelaars, T. 2015. Modeling video evolution for action recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion, 5378-5387.\n\nHuman action recognition using a temporal hierarchy of covariance descriptors on 3d joint locations. Bruna Henaff, M Henaff, J Bruna, Y Lecun, Hussein, arXiv:1506.05163arXiv:1705.06950Deep convolutional networks on graphstructured data. BNMW CVPRWHenaff, Bruna, and LeCun 2015] Henaff, M.; Bruna, J.; and LeCun, Y. 2015. Deep convolutional networks on graph- structured data. In arXiv:1506.05163. [Hussein et al. 2013] Hussein, M. E.; Torki, M.; Gowayyed, M. A.; and El-Saban, M. 2013. Human action recognition using a temporal hierarchy of covariance descriptors on 3d joint locations. In IJCAI. [Kay et al. 2017] Kay, W.; Carreira, J.; Simonyan, K.; Zhang, B.; Hillier, C.; Vijayanarasimhan, S.; Viola, F.; Green, T.; Back, T.; Natsev, P.; et al. 2017. The kinetics human action video dataset. In arXiv:1705.06950. [Ke et al. 2017] Ke, Q.; Bennamoun, M.; An, S.; Sohel, F.; and Boussaid, F. 2017. A new representation of skeleton sequences for 3d action recognition. In CVPR. [Kim and Reiter 2017] Kim, T. S., and Reiter, A. 2017. In- terpretable 3d human action analysis with temporal convo- lutional networks. In BNMW CVPRW.\n\nSemi-supervised classification with graph convolutional networks. T N Kipf, M Welling, A Krizhevsky, I Sutskever, G E Hinton, NIPS. HintonImagenet classification with deep convolutional neural networksand Welling 2017] Kipf, T. N., and Welling, M. 2017. Semi-supervised classification with graph convolutional net- works. In ICLR 2017. [Krizhevsky, Sutskever, and Hinton 2012] Krizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2012. Imagenet clas- sification with deep convolutional neural networks. In NIPS.\n\nGated graph sequence neural networks. In ICLRet al. 2016] Li, Y.; Zemel, R.; Brockschmidt, M.; and Tar- low, D. 2016. Gated graph sequence neural networks. In ICLR.\n\nSkeleton-based action recognition with convolutional neural networks. C Li, Q Zhong, D Xie, S Pu, arXiv:1704.07595et al. 2017] Li, C.; Zhong, Q.; Xie, D.; and Pu, S. 2017. Skeleton-based action recognition with convolutional neural networks. In arXiv:1704.07595.\n\nSpatio-temporal lstm with trust gates for 3d human action recognition. International Conference on Machine Learning. SpringerECCVet al. 2016] Liu, J.; Shahroudy, A.; Xu, D.; and Wang, G. 2016. Spatio-temporal lstm with trust gates for 3d human action recognition. In ECCV, 816-833. Springer. [Niepert, Ahmed, and Kutzkov 2016] Niepert, M.; Ahmed, M.; and Kutzkov, K. 2016. Learning convolutional neural networks for graphs. In International Conference on Ma- chine Learning.\n\nReal-time human pose recognition in parts from single depth images. Shahroudy, Advances in neural information processing systems. CVPR[Shahroudy et al. 2016] Shahroudy, A.; Liu, J.; Ng, T.-T.; and Wang, G. 2016. Ntu rgb+ d: A large scale dataset for 3d human activity analysis. In CVPR, 1010-1019. [Shotton et al. 2011] Shotton, J.; Sharp, T.; Kipman, A.; Fitzgibbon, A.; Finocchio, M.; Blake, A.; Cook, M.; and Moore, R. 2011. Real-time human pose recognition in parts from single depth images. In CVPR. [Simonyan and Zisserman 2014] Simonyan, K., and Zisser- man, A. 2014. Two-stream convolutional networks for ac- tion recognition in videos. In Advances in neural informa- tion processing systems, 568-576.\n\nImproved semantic representations from tree-structured long short-term memory networks. Socher Tai, Manning, K S Tai, R Socher, C D Manning, ACL. Tai, Socher, and Manning 2015] Tai, K. S.; Socher, R.; and Manning, C. D. 2015. Improved semantic representations from tree-structured long short-term memory networks. In ACL.\n\nLearning spatiotemporal features with 3d convolutional networks. Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionVan Oord, Kalchbrenner, and Kavukcuogluet al. 2015] Tran, D.; Bourdev, L.; Fergus, R.; Torre- sani, L.; and Paluri, M. 2015. Learning spatiotemporal fea- tures with 3d convolutional networks. In Proceedings of the IEEE international conference on computer vision, 4489- 4497. [Van Oord, Kalchbrenner, and Kavukcuoglu 2016]\n\nDifferential recurrent neural networks for action recognition. A Van Oord, N Kalchbrenner, K Kavukcuoglu, V Veeriah, N Zhuang, G.-J Qi, ICML. [Veeriah, Zhuang, and Qi. CVPRVan Oord, A.; Kalchbrenner, N.; and Kavukcuoglu, K. 2016. Pixel recurrent neural networks. In ICML. [Veeriah, Zhuang, and Qi 2015] Veeriah, V.; Zhuang, N.; and Qi, G.-J. 2015. Differential recurrent neural networks for action recognition. In CVPR, 4041-4049.\n\nHuman action recognition by representing 3d skeletons as points in a lie group. Arrate Vemulapalli, R Vemulapalli, F Arrate, R Chellappa, CVPR. Vemulapalli, Arrate, and Chellappa 2014] Vemulapalli, R.; Arrate, F.; and Chellappa, R. 2014. Human action recog- nition by representing 3d skeletons as points in a lie group. In CVPR, 588-595.\n\nTemporal segment networks: Towards good practices for deep action recognition. [ Wang, CVPR. IEEE. ECCV[Wang et al. 2012] Wang, J.; Liu, Z.; Wu, Y.; and Yuan, J. 2012. Mining actionlet ensemble for action recognition with depth cameras. In CVPR. IEEE. [Wang et al. 2016] Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.; and Val Gool, L. 2016. Temporal segment networks: Towards good practices for deep action recogni- tion. In ECCV.\n\nOn geometric features for skeleton-based action recognition using multilayer lstm networks. Qiao Tang, L Wang, Y Qiao, X Tang, S Zhang, X Liu, J Xiao, Y Zhao, Y Xiong, L Wang, Z Wu, X Tang, D Lin, Proceedings of the IEEE conference on computer vision and pattern recognition. WACV. IEEEthe IEEE conference on computer vision and pattern recognitionICCV, Qiao, and Tang 2015] Wang, L.; Qiao, Y.; and Tang, X. 2015. Action recognition with trajectory-pooled deep- convolutional descriptors. In Proceedings of the IEEE con- ference on computer vision and pattern recognition, 4305- 4314. [Zhang, Liu, and Xiao 2017] Zhang, S.; Liu, X.; and Xiao, J. 2017. On geometric features for skeleton-based ac- tion recognition using multilayer lstm networks. In WACV. IEEE. [Zhao et al. 2017] Zhao, Y.; Xiong, Y.; Wang, L.; Wu, Z.; Tang, X.; and Lin, D. 2017. Temporal action detection with structured segment networks. In ICCV.\n\nCo-occurrence feature learning for skeleton based action recognition using regularized deep lstm networks. AAAI. et al. 2016] Zhu, W.; Lan, C.; Xing, J.; Zeng, W.; Li, Y.; Shen, L.; Xie, X.; et al. 2016. Co-occurrence feature learn- ing for skeleton based action recognition using regularized deep lstm networks. In AAAI.\n", "annotations": {"author": "[{\"end\":173,\"start\":87},{\"end\":283,\"start\":174},{\"end\":391,\"start\":284}]", "publisher": null, "author_last_name": "[{\"end\":96,\"start\":93},{\"end\":187,\"start\":182},{\"end\":293,\"start\":290}]", "author_first_name": "[{\"end\":92,\"start\":87},{\"end\":181,\"start\":174},{\"end\":289,\"start\":284}]", "author_affiliation": "[{\"end\":172,\"start\":98},{\"end\":282,\"start\":208},{\"end\":390,\"start\":316}]", "title": "[{\"end\":84,\"start\":1},{\"end\":475,\"start\":392}]", "venue": null, "abstract": "[{\"end\":1218,\"start\":477}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b11\"},\"end\":1480,\"start\":1463},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":1506,\"start\":1480},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":1523,\"start\":1506},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1611,\"start\":1586},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2455,\"start\":2437},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2476,\"start\":2455},{\"end\":3169,\"start\":3162},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3273,\"start\":3249},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4241,\"start\":4222},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4312,\"start\":4267},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4366,\"start\":4343},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6658,\"start\":6627},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6703,\"start\":6658},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6764,\"start\":6745},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6794,\"start\":6764},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6815,\"start\":6794},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6859,\"start\":6815},{\"end\":7185,\"start\":7153},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7206,\"start\":7185},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7228,\"start\":7206},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7367,\"start\":7348},{\"end\":7400,\"start\":7367},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7877,\"start\":7856},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7894,\"start\":7877},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8257,\"start\":8236},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8306,\"start\":8288},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8578,\"start\":8562},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8604,\"start\":8578},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8638,\"start\":8623},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8658,\"start\":8638},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9592,\"start\":9566},{\"end\":10508,\"start\":10485},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":13227,\"start\":13210},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":15604,\"start\":15587},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":23792,\"start\":23769},{\"end\":24074,\"start\":24065},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":26732,\"start\":26715},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":27767,\"start\":27751},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":29275,\"start\":29258},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":30179,\"start\":30162},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":31662,\"start\":31645},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":35114,\"start\":35092},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":36215,\"start\":36190},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":36380,\"start\":36363},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":36897,\"start\":36880},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":40497,\"start\":40480}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":39310,\"start\":38490},{\"attributes\":{\"id\":\"fig_1\"},\"end\":39815,\"start\":39311},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":40303,\"start\":39816},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":40498,\"start\":40304},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":40753,\"start\":40499},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":41290,\"start\":40754},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":41577,\"start\":41291},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":42211,\"start\":41578}]", "paragraph": "[{\"end\":2037,\"start\":1234},{\"end\":3569,\"start\":2039},{\"end\":4576,\"start\":3571},{\"end\":5327,\"start\":4578},{\"end\":5738,\"start\":5329},{\"end\":6394,\"start\":5740},{\"end\":7615,\"start\":6411},{\"end\":9307,\"start\":7617},{\"end\":10227,\"start\":9342},{\"end\":10512,\"start\":10249},{\"end\":11703,\"start\":10528},{\"end\":12314,\"start\":11745},{\"end\":13528,\"start\":12316},{\"end\":13860,\"start\":13530},{\"end\":13979,\"start\":13885},{\"end\":14856,\"start\":14026},{\"end\":15605,\"start\":14913},{\"end\":15949,\"start\":15607},{\"end\":16165,\"start\":15951},{\"end\":16525,\"start\":16393},{\"end\":17674,\"start\":16527},{\"end\":17774,\"start\":17714},{\"end\":17912,\"start\":17776},{\"end\":18271,\"start\":18001},{\"end\":18678,\"start\":18351},{\"end\":19195,\"start\":18680},{\"end\":19739,\"start\":19256},{\"end\":19966,\"start\":19793},{\"end\":20340,\"start\":19992},{\"end\":21612,\"start\":20342},{\"end\":22311,\"start\":21646},{\"end\":22791,\"start\":22381},{\"end\":23500,\"start\":22832},{\"end\":23706,\"start\":23524},{\"end\":24088,\"start\":23708},{\"end\":24937,\"start\":24131},{\"end\":25077,\"start\":24987},{\"end\":25450,\"start\":25079},{\"end\":27495,\"start\":25452},{\"end\":28484,\"start\":27511},{\"end\":28829,\"start\":28517},{\"end\":30039,\"start\":28831},{\"end\":30365,\"start\":30041},{\"end\":30934,\"start\":30367},{\"end\":31397,\"start\":30936},{\"end\":31486,\"start\":31399},{\"end\":31663,\"start\":31505},{\"end\":33104,\"start\":31665},{\"end\":33341,\"start\":33106},{\"end\":34429,\"start\":33343},{\"end\":34690,\"start\":34431},{\"end\":34912,\"start\":34728},{\"end\":35730,\"start\":34914},{\"end\":37652,\"start\":35732},{\"end\":38489,\"start\":37667}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13884,\"start\":13861},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14912,\"start\":14857},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16392,\"start\":16166},{\"attributes\":{\"id\":\"formula_3\"},\"end\":17713,\"start\":17675},{\"attributes\":{\"id\":\"formula_4\"},\"end\":18000,\"start\":17913},{\"attributes\":{\"id\":\"formula_5\"},\"end\":18350,\"start\":18272},{\"attributes\":{\"id\":\"formula_6\"},\"end\":19255,\"start\":19196},{\"attributes\":{\"id\":\"formula_7\"},\"end\":19792,\"start\":19740},{\"attributes\":{\"id\":\"formula_8\"},\"end\":21645,\"start\":21613},{\"attributes\":{\"id\":\"formula_9\"},\"end\":22380,\"start\":22312},{\"attributes\":{\"id\":\"formula_10\"},\"end\":24130,\"start\":24089},{\"attributes\":{\"id\":\"formula_11\"},\"end\":24986,\"start\":24938}]", "table_ref": "[{\"end\":32278,\"start\":32271},{\"end\":33380,\"start\":33373},{\"end\":34689,\"start\":34682},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":35160,\"start\":35153},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":35391,\"start\":35384},{\"end\":35565,\"start\":35558},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":36951,\"start\":36944}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1232,\"start\":1220},{\"attributes\":{\"n\":\"2\"},\"end\":6409,\"start\":6397},{\"attributes\":{\"n\":\"3\"},\"end\":9340,\"start\":9310},{\"attributes\":{\"n\":\"3.1\"},\"end\":10247,\"start\":10230},{\"end\":10526,\"start\":10515},{\"end\":11713,\"start\":11706},{\"attributes\":{\"n\":\"3.2\"},\"end\":11743,\"start\":11716},{\"attributes\":{\"n\":\"3.3\"},\"end\":14024,\"start\":13982},{\"attributes\":{\"n\":\"3.4\"},\"end\":19990,\"start\":19969},{\"attributes\":{\"n\":\"3.5\"},\"end\":22830,\"start\":22794},{\"attributes\":{\"n\":\"3.6\"},\"end\":23522,\"start\":23503},{\"attributes\":{\"n\":\"4\"},\"end\":27509,\"start\":27498},{\"attributes\":{\"n\":\"4.1\"},\"end\":28515,\"start\":28487},{\"attributes\":{\"n\":\"4.2\"},\"end\":31503,\"start\":31489},{\"attributes\":{\"n\":\"4.3\"},\"end\":34726,\"start\":34693},{\"attributes\":{\"n\":\"5\"},\"end\":37665,\"start\":37655},{\"end\":38501,\"start\":38491},{\"end\":40764,\"start\":40755},{\"end\":41301,\"start\":41292},{\"end\":41588,\"start\":41579}]", "table": "[{\"end\":40753,\"start\":40734},{\"end\":41290,\"start\":40915},{\"end\":41577,\"start\":41476}]", "figure_caption": "[{\"end\":39310,\"start\":38503},{\"end\":39815,\"start\":39313},{\"end\":40303,\"start\":39818},{\"end\":40498,\"start\":40306},{\"end\":40734,\"start\":40501},{\"end\":40915,\"start\":40766},{\"end\":41476,\"start\":41303},{\"end\":42211,\"start\":41590}]", "figure_ref": "[{\"end\":2799,\"start\":2791},{\"end\":4843,\"start\":4835},{\"end\":10570,\"start\":10562},{\"end\":12823,\"start\":12817},{\"end\":13527,\"start\":13521},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":22552,\"start\":22546},{\"end\":37141,\"start\":37135}]", "bib_author_first_name": "[{\"end\":42274,\"start\":42273},{\"end\":43392,\"start\":43388},{\"end\":43399,\"start\":43395},{\"end\":43401,\"start\":43400},{\"end\":43407,\"start\":43406},{\"end\":43415,\"start\":43414},{\"end\":43423,\"start\":43422},{\"end\":43435,\"start\":43434},{\"end\":43437,\"start\":43436},{\"end\":43450,\"start\":43449},{\"end\":43466,\"start\":43465},{\"end\":43479,\"start\":43478},{\"end\":43489,\"start\":43488},{\"end\":43505,\"start\":43504},{\"end\":43514,\"start\":43513},{\"end\":43516,\"start\":43515},{\"end\":43528,\"start\":43527},{\"end\":43538,\"start\":43537},{\"end\":43548,\"start\":43547},{\"end\":43550,\"start\":43549},{\"end\":43562,\"start\":43561},{\"end\":43576,\"start\":43575},{\"end\":44455,\"start\":44450},{\"end\":44465,\"start\":44464},{\"end\":44475,\"start\":44474},{\"end\":44484,\"start\":44483},{\"end\":45546,\"start\":45545},{\"end\":45548,\"start\":45547},{\"end\":45556,\"start\":45555},{\"end\":45567,\"start\":45566},{\"end\":45581,\"start\":45580},{\"end\":45594,\"start\":45593},{\"end\":45596,\"start\":45595},{\"end\":46225,\"start\":46224},{\"end\":46231,\"start\":46230},{\"end\":46240,\"start\":46239},{\"end\":46247,\"start\":46246},{\"end\":47699,\"start\":47693},{\"end\":47715,\"start\":47714},{\"end\":47717,\"start\":47716},{\"end\":47724,\"start\":47723},{\"end\":47734,\"start\":47733},{\"end\":47736,\"start\":47735},{\"end\":48502,\"start\":48501},{\"end\":48514,\"start\":48513},{\"end\":48530,\"start\":48529},{\"end\":48545,\"start\":48544},{\"end\":48556,\"start\":48555},{\"end\":48569,\"start\":48565},{\"end\":48956,\"start\":48950},{\"end\":48971,\"start\":48970},{\"end\":48986,\"start\":48985},{\"end\":48996,\"start\":48995},{\"end\":49289,\"start\":49288},{\"end\":49750,\"start\":49746},{\"end\":49758,\"start\":49757},{\"end\":49766,\"start\":49765},{\"end\":49774,\"start\":49773},{\"end\":49782,\"start\":49781},{\"end\":49791,\"start\":49790},{\"end\":49798,\"start\":49797},{\"end\":49806,\"start\":49805},{\"end\":49814,\"start\":49813},{\"end\":49823,\"start\":49822},{\"end\":49831,\"start\":49830},{\"end\":49837,\"start\":49836},{\"end\":49845,\"start\":49844}]", "bib_author_last_name": "[{\"end\":42285,\"start\":42275},{\"end\":42292,\"start\":42287},{\"end\":43404,\"start\":43402},{\"end\":43412,\"start\":43408},{\"end\":43420,\"start\":43416},{\"end\":43432,\"start\":43424},{\"end\":43447,\"start\":43438},{\"end\":43463,\"start\":43451},{\"end\":43476,\"start\":43467},{\"end\":43486,\"start\":43480},{\"end\":43502,\"start\":43490},{\"end\":43511,\"start\":43506},{\"end\":43525,\"start\":43517},{\"end\":43535,\"start\":43529},{\"end\":43545,\"start\":43539},{\"end\":43559,\"start\":43551},{\"end\":43573,\"start\":43563},{\"end\":44462,\"start\":44456},{\"end\":44472,\"start\":44466},{\"end\":44481,\"start\":44476},{\"end\":44490,\"start\":44485},{\"end\":44499,\"start\":44492},{\"end\":45553,\"start\":45549},{\"end\":45564,\"start\":45557},{\"end\":45578,\"start\":45568},{\"end\":45591,\"start\":45582},{\"end\":45603,\"start\":45597},{\"end\":46228,\"start\":46226},{\"end\":46237,\"start\":46232},{\"end\":46244,\"start\":46241},{\"end\":46250,\"start\":46248},{\"end\":46971,\"start\":46962},{\"end\":47703,\"start\":47700},{\"end\":47712,\"start\":47705},{\"end\":47721,\"start\":47718},{\"end\":47731,\"start\":47725},{\"end\":47744,\"start\":47737},{\"end\":48511,\"start\":48503},{\"end\":48527,\"start\":48515},{\"end\":48542,\"start\":48531},{\"end\":48553,\"start\":48546},{\"end\":48563,\"start\":48557},{\"end\":48572,\"start\":48570},{\"end\":48968,\"start\":48957},{\"end\":48983,\"start\":48972},{\"end\":48993,\"start\":48987},{\"end\":49006,\"start\":48997},{\"end\":49294,\"start\":49290},{\"end\":49755,\"start\":49751},{\"end\":49763,\"start\":49759},{\"end\":49771,\"start\":49767},{\"end\":49779,\"start\":49775},{\"end\":49788,\"start\":49783},{\"end\":49795,\"start\":49792},{\"end\":49803,\"start\":49799},{\"end\":49811,\"start\":49807},{\"end\":49820,\"start\":49815},{\"end\":49828,\"start\":49824},{\"end\":49834,\"start\":49832},{\"end\":49842,\"start\":49838},{\"end\":49849,\"start\":49846}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":17682909},\"end\":42454,\"start\":42213},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":16224674},\"end\":42835,\"start\":42456},{\"attributes\":{\"doi\":\"arXiv:1703.06211\",\"id\":\"b2\",\"matched_paper_id\":4028864},\"end\":43309,\"start\":42837},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":8040013},\"end\":44347,\"start\":43311},{\"attributes\":{\"doi\":\"arXiv:1506.05163\",\"id\":\"b4\",\"matched_paper_id\":1030329},\"end\":45477,\"start\":44349},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":3144218},\"end\":45986,\"start\":45479},{\"attributes\":{\"id\":\"b6\"},\"end\":46152,\"start\":45988},{\"attributes\":{\"id\":\"b7\"},\"end\":46416,\"start\":46154},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":2654595},\"end\":46892,\"start\":46418},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":7731948},\"end\":47603,\"start\":46894},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":3033526},\"end\":47926,\"start\":47605},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":1122604},\"end\":48436,\"start\":47928},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":11060863},\"end\":48868,\"start\":48438},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":1732632},\"end\":49207,\"start\":48870},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":5711057},\"end\":49652,\"start\":49209},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":3341382},\"end\":50569,\"start\":49654},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":8172563},\"end\":50892,\"start\":50571}]", "bib_title": "[{\"end\":42271,\"start\":42213},{\"end\":42523,\"start\":42456},{\"end\":42870,\"start\":42837},{\"end\":43386,\"start\":43311},{\"end\":44448,\"start\":44349},{\"end\":45543,\"start\":45479},{\"end\":46487,\"start\":46418},{\"end\":46960,\"start\":46894},{\"end\":47691,\"start\":47605},{\"end\":47991,\"start\":47928},{\"end\":48499,\"start\":48438},{\"end\":48948,\"start\":48870},{\"end\":49286,\"start\":49209},{\"end\":49744,\"start\":49654},{\"end\":50676,\"start\":50571}]", "bib_author": "[{\"end\":42287,\"start\":42273},{\"end\":42294,\"start\":42287},{\"end\":43395,\"start\":43388},{\"end\":43406,\"start\":43395},{\"end\":43414,\"start\":43406},{\"end\":43422,\"start\":43414},{\"end\":43434,\"start\":43422},{\"end\":43449,\"start\":43434},{\"end\":43465,\"start\":43449},{\"end\":43478,\"start\":43465},{\"end\":43488,\"start\":43478},{\"end\":43504,\"start\":43488},{\"end\":43513,\"start\":43504},{\"end\":43527,\"start\":43513},{\"end\":43537,\"start\":43527},{\"end\":43547,\"start\":43537},{\"end\":43561,\"start\":43547},{\"end\":43575,\"start\":43561},{\"end\":43579,\"start\":43575},{\"end\":44464,\"start\":44450},{\"end\":44474,\"start\":44464},{\"end\":44483,\"start\":44474},{\"end\":44492,\"start\":44483},{\"end\":44501,\"start\":44492},{\"end\":45555,\"start\":45545},{\"end\":45566,\"start\":45555},{\"end\":45580,\"start\":45566},{\"end\":45593,\"start\":45580},{\"end\":45605,\"start\":45593},{\"end\":46230,\"start\":46224},{\"end\":46239,\"start\":46230},{\"end\":46246,\"start\":46239},{\"end\":46252,\"start\":46246},{\"end\":46973,\"start\":46962},{\"end\":47705,\"start\":47693},{\"end\":47714,\"start\":47705},{\"end\":47723,\"start\":47714},{\"end\":47733,\"start\":47723},{\"end\":47746,\"start\":47733},{\"end\":48513,\"start\":48501},{\"end\":48529,\"start\":48513},{\"end\":48544,\"start\":48529},{\"end\":48555,\"start\":48544},{\"end\":48565,\"start\":48555},{\"end\":48574,\"start\":48565},{\"end\":48970,\"start\":48950},{\"end\":48985,\"start\":48970},{\"end\":48995,\"start\":48985},{\"end\":49008,\"start\":48995},{\"end\":49296,\"start\":49288},{\"end\":49757,\"start\":49746},{\"end\":49765,\"start\":49757},{\"end\":49773,\"start\":49765},{\"end\":49781,\"start\":49773},{\"end\":49790,\"start\":49781},{\"end\":49797,\"start\":49790},{\"end\":49805,\"start\":49797},{\"end\":49813,\"start\":49805},{\"end\":49822,\"start\":49813},{\"end\":49830,\"start\":49822},{\"end\":49836,\"start\":49830},{\"end\":49844,\"start\":49836},{\"end\":49851,\"start\":49844}]", "bib_venue": "[{\"end\":42298,\"start\":42294},{\"end\":42529,\"start\":42525},{\"end\":42966,\"start\":42888},{\"end\":43656,\"start\":43579},{\"end\":44584,\"start\":44533},{\"end\":45609,\"start\":45605},{\"end\":46024,\"start\":45988},{\"end\":46222,\"start\":46154},{\"end\":46533,\"start\":46489},{\"end\":47022,\"start\":46973},{\"end\":47749,\"start\":47746},{\"end\":48060,\"start\":47993},{\"end\":48604,\"start\":48574},{\"end\":49012,\"start\":49008},{\"end\":49306,\"start\":49296},{\"end\":49928,\"start\":49851},{\"end\":50682,\"start\":50678},{\"end\":43720,\"start\":43658},{\"end\":45617,\"start\":45611},{\"end\":48114,\"start\":48062},{\"end\":50002,\"start\":49940}]"}}}, "year": 2023, "month": 12, "day": 17}