{"id": 260640523, "updated": "2023-10-23 21:07:06.532", "metadata": {"title": "PerfHD: Efficient ViT Architecture Performance Ranking using Hyperdimensional Computing", "authors": "[{\"first\":\"Dongning\",\"last\":\"Ma\",\"middle\":[]},{\"first\":\"Pengfei\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Xun\",\"last\":\"Jiao\",\"middle\":[]}]", "venue": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)", "journal": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)", "publication_date": {"year": 2023, "month": 6, "day": 1}, "abstract": "Neural Architecture Search (NAS) aims at identifying the optimal network architecture for a specific need in an automated manner, which serves as an alternative to the manual process of model development, selection, evaluation and performance estimation. However, evaluating performance of candidate architectures in the search space during NAS, which often requires training and ranking a mass amount of architectures, is often prohibitively computation-demanding. To reduce this cost, recent works propose to estimate and rank the architecture performance with-out actual training or inference. In this paper, we present PerfHD, an efficient-while-accurate architecture performance ranking approach using hyperdimensional computing for the emerging vision transformer (ViT), which has demonstrated state-of-the-art (SOTA) performance in vision tasks. Given a set of ViT models, PerfHD can accurately and quickly rank their performance solely based on their hyper-parameters without training. We develop two encoding schemes for PerfHD, Gram-based and Record-based, to encode the features from candidate ViT architecture parameters. Using the VIMER-UFO benchmark dataset of eight tasks from a diverse range of domains, we compare PerfHD with four SOTA methods. Experimental results show that PerfHD can rank nearly 100K ViT models in about just 1 minute, which is up to 10X faster than SOTA methods, while achieving comparable or even superior ranking accuracy. We open-source PerfHD in PyTorch implementation at https://github.com/VU-DETAIL/PerfHD.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/MaZJ23", "doi": "10.1109/cvprw59228.2023.00217"}}, "content": {"source": {"pdf_hash": "3a57b5d32c8214e655fb14abea03f1c15caf2781", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "5bffd8449feea5464324bbbeb9eb8cf86bfa361a", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/3a57b5d32c8214e655fb14abea03f1c15caf2781.txt", "contents": "\nPerfHD: Efficient ViT Architecture Performance Ranking using Hyperdimensional Computing\n\n\nDongning Ma \nPengfei Zhao zhaopengfei2014@xiaochuankeji.cn \nXun Jiao xun.jiao@villanova.edu \n\nVillanova University Villanova\nBeijing Xiaochuan Technology Co., Ltd. Haidian\n19085, 100191BeijingPAChina\n\n\nVillanova University Villanova\n19085PA\n\nPerfHD: Efficient ViT Architecture Performance Ranking using Hyperdimensional Computing\n10.1109/CVPRW59228.2023.00217\nNeural Architecture Search (NAS) aims at identifying the optimal network architecture for a specific need in an automated manner, which serves as an alternative to the manual process of model development, selection, evaluation and performance estimation. However, evaluating performance of candidate architectures in the search space during NAS, which often requires training and ranking a mass amount of architectures, is often prohibitively computationdemanding. To reduce this cost, recent works propose to estimate and rank the architecture performance without actual training or inference. In this paper, we present PerfHD, an efficient-while-accurate architecture performance ranking approach using hyperdimensional computing for the emerging vision transformer (ViT), which has demonstrated state-of-the-art (SOTA) performance in vision tasks. Given a set of ViT models, PerfHD can accurately and quickly rank their performance solely based on their hyper-parameters without training. We develop two encoding schemes for PerfHD, Gram-based and Recordbased, to encode the features from candidate ViT architecture parameters. Using the VIMER-UFO benchmark dataset of eight tasks from a diverse range of domains, we compare PerfHD with four SOTA methods. Experimental results show that PerfHD can rank nearly 100K ViT models in about just 1 minute, which is up to 10X faster than SOTA methods, while achieving comparable or even superior ranking accuracy. We open-source PerfHD in Py-Torch implementation at https://github.com/VU-DETAIL/PerfHD.\n\nIntroduction\n\nAs deep learning models broaden their applications and enhance their capability to various domains, there is a growing demand of developing and optimizing architectures for higher model performance. However, with the architec-tures evolving increasingly deep, the architecture engineering usually requires enormous effort due to the expansion of design space, which is hardly possible using manual effort. To address this issue, neural architecture search (NAS) has been proposed and adopted to identify the optimal neural network architecture in an automated manner [12,15]. Recently, vision transformer (ViT) emerges as a promising algorithm and receives wide attention as it achieves SOTA performance in vision tasks [5]. NAS is particularly useful and necessary for ViT because of the heterogeneity of components and blocks, which serves as the potential knobs in the search process.\n\nGenerally speaking, NAS can be summarized as shown in Fig. 1: First, system designers set custom constraints which defines the search space. Then, within this search space, candidate architectures are sampled using search strategies iteratively. Lastly, the performance of candidate architecture will get evaluated/estimated to obtain a performance ranking, which determines the optimal architecture for this NAS task. NAS process itself can be a resource-demanding process. For example, NAS has spent days or even weeks using clusters of hundreds of GPUs to identify architectures that can achieve comparable results on challenging computer vision tasks [16]. One major computation-expensive process in NAS is the process of evaluating the performance of neural architectures sampled from the predefined search space. A canonical training and validation flow can be applied to evaluate the performance of an architecture. However, this is prohibitively slow, thus many recent works develop various efficient performance evaluation techniques such as sharing weights from a supernet to avoid training models afresh [3,20,21]. However, this still cannot avoid the process of evaluating the performance of sampled architectures by running inference on each one of them. To address this issue, multiple works have been proposed to use machine learning for predicting the performance of a given architecture without training or inference [11,22].\n\nIn this work, we depart from the conventional learning algorithms and embrace the hyperdimensional computing (HDC), to develop an efficient-yet-accurate performance ranking approach for ViT architectures. HDC is an emerging non von Neumann computation paradigm, leveraging the learning and representation capabilities of extremely high dimensional vectors inspired from the abstract brain activity functionalities [18]. We identify two highlighted characteristics of HDC which is suitable to address the performance ranking problem during NAS. First, HDC is known for its efficiency over existing machine learning algorithms such as neural networks, including smaller model size, faster model convergence and less computational intensity [6,10,13,17]. This can reduce the overhead of NAS performance ranking. Second, HDC is better at learning with limited data such as one-shot learning tasks and are less likely to over-fit [1,14]. This can enable HDC models to learn how to rank the performance of a large set of unseen architectures with only a handful of training samples, which is usually the realistic case [2,7]. The main contributions of this paper are as follows:\n\n\u2022 In this paper, we propose PerfHD, a supervised ViT performance ranking algorithm using HDC. PerfHD can efficiently rank the performance of architectures given the configurations of ViTs. To the best of our knowledge, this is the first work to leverage HDC for NAS.\n\n\u2022 We propose two different HDC encoding schemes (Gram and Record) based on the architecture of ViT and evaluate and compare their performance. We also propose retraining methods based on weight-update to enhance model performance after initial training.\n\n\u2022 We evaluate PerfHD performance on the architectures curated from VIMER-UFO benchmark of eight computer vision applications from different domains and compare with four SOTA baselines. Experimental results show that PerfHD can rank around 100K architectures in about 1 minute, which is up to 10X faster than SOTA methods.\n\n\nRelated Works\n\nDifferent machine learning algorithms are proposed to estimate and rank performance of sampled architectures during NAS, particularly given there is a limited training dataset, e.g., just a few architectures with their performance. For example, widely used gradient boosting algorithms for ranking such as LightGBM and Catboost have been applied to rank the ViT architecture performance where the hyper-parameters of ViT models are used as categorical features [7]. TF-TAS leverages two theoretical perspectives of synaptic diversity and synaptic saliency to evaluate and rank ViT architectures which speeds up the architecture search by up to 48X [22]. On the other hand, GP-NAS leverage the capability of Gaussian process and mutual information to accurately model the performance correlations using a small amount of samples [11]. Ensemble learning is also used to enhance the GP-NAS performance, however as ensemble learning usually incorporates a large set of models and require extensive hyper-parameter fine-tuning [2]. This drastically increases the overhead of the algorithm such as run-time by orders of magnitude, which can potentially offset the benefit of efficiency. Therefore, it is of great significance to explore methods that are able to rank the performance of ViT models with desirable accuracy and efficiency.\n\n\nHyperdimensional Computing\n\n\nNotions and Operations\n\n\nHypervector\n\nHypervectors (HV) are the fundamental elements of an HDC model. HVs are numerical vectors with several specific characteristics. 1). high-dimensional: the dimension (the amount of elements) of HVs is extremely high, usually reaching 10,000 and above; 2). holographic: each HV is recognized as the minimal unit in an HDC model, individual number inside HV does not own unique representation; 3). (pseudo-)random: for the initial generated HVs, the numbers are i.i.d., thus two randomly generated HVs are approximately orthogonal to each other due to the extremely high dimensionality. We use \u20d7 V = (v 1 , v 2 , ..., v D ) as the notion of a D-dimensional HV in which v i is the ith number inside the HV. For differentiation purposes, HVs are marked with arrows in this paper, while other vectors such as features are bold instead.\n\n\nHV Operations\n\nIn reality, the information can originate from diverse modalities, and different sources of information can correlate. To aggregate information of the same modality or to combine information from different modalities to also provide hierarchy of information, we use HDC operations. The three mostly used HDC operations are addition, multiplication and permutation. The addition and multiplication operations take two HVs as inputs and perform linear and element-wise vector operations correspondingly. The permutation operation takes only one HV as input and perform cyclic shift of a specific amount n. Note that the dimensions of input HVs and output HVs are the same for all the three operations.\n\nBipolarization (or binarization) is another important operation for HV beyond the three basic operations. Bipolarization takes the sign bit of the HV elements that any number larger than 0 is bipolarized into 1, while any number smaller than 0 is bipolarized into \u22121. Binarization is similar, but numbers are binarized into 1 and 0 instead. Bipolarization and binarization introduces additional non-linearity into HDC models, and also to limit the range of elements inside each HV to prevent overflow issues during aggregation. In PerfHD, we slightly modify the bipolarization behavior that any number larger than 1 or smaller than \u22121 will be capped into 1 and \u22121 while the numbers within (\u22121, 1) are kept as-is.\n\n\nSimilarity\n\nWith HVs representing information and HDC operations aggregating and combining information, there is then a need of metric that can quantitatively measure the similarity between information that different HVs accommodate. Cosine similarity is one of the most frequently used metrics while other similarities such as Euclidean distance and Hamming distance can also be used. A higher cosine similarity indicates that the two HVs compared share more similar information, thus are more alike.\n\n\nHDC Memories\n\nHDC leverages memories to host information. HDC memories are specialized clusters of HVs with different objectives during a learning task. There are two major categories of HDC memories: the item memory and the associative memory. Item memories are related with the input realistic features: each item memory hosts item HVs at the same amount of possible feature values K of the corresponding features as shown by\nI = { \u20d7 I 1 , \u20d7 I 2 , ... \u20d7 I K }.\nIf the feature is a continuous variable, quantization can be applied beforehand to avoid item memories of infinite size. Associative memory, on the other hand, is related to the output of the model. For a classification task, the associative memory hosts class HVs, each of which represents a class of the task, as shown by\nA = { \u20d7 A 1 , \u20d7 A 2 , ... \u20d7 A T }.\nFor this paper, since we are ranking the performance of various ViT architectures, the associative memory of PerfHD hosts an HV aiming to represent high performance architectures. Details of how the associative memory can help with ranking architecture performance are present in Sec. 4.\n\n\nPerfHD Methodology\n\n\nProblem Formulation and Motivation\n\nIn NAS, architectures are usually sampled from a supernet. Instead of training every sampled architecture to obtain the performance, designers usually have a small set of architectures trained with their performance ranked, and want to rank a larger set without actually training them. Additionally the algorithm or method to rank the larger set of architecture should be efficient and fast. We found that two highlighted characteristics of HDC match the requirements and can potentially enable a new direction of addressing this problem: 1. HDC is known for its efficiency over existing machine learning algorithms such as neural networks, including smaller model size, faster model convergence and less computational intensity [10,13,17]. 2. HDC is better at learning with limited data such as one-shot or few-shot learning tasks and are less likely to over-fit [1,14].\n\n\nOverview of PerfHD\n\nAn overview of PerfHD is present in Fig. 2. PerfHD first iterates through the training set, takes parameters of all the architectures and encodes them into the corresponding HVs referred to as the architecture HVs. PerfHD also converts the task rankings into weights of each architecture in the training set. PerfHD uses the weights to perform weighted sum to establish the associative memory which accommodates the task HVs. In brief, architectures with higher rankings are assigned with larger weights thus having more information impact on the task HVs after the weighted sum. When predicting rankings of architectures from the test set, PerfHD performs the same architecture encoding to obtain the architecture HV. Then, PerfHD computes and checks the similarity between the HV and each task HV in the associative memory. Once the similarity metrics of all the architectures from the test set are obtained, PerfHD can obtain the ranking of the similarity metrics as the predicted rankings. To enhance PerfHD performance, we also propose \"retraining by weight-difference\" to update the associative memory accordingly.   Smaller number refers to higher performance, e.g., a ranking of 0 means this architecture is the best-performing architecture of this task.\n\n\nFeature Description\n\n\nViT Architecture Encoding\n\nIn this subsection, we introduce the architecture encoding in detail. Specifically, we propose two schemes of encoding: Gram-based encoding and Record-based encoding, as indicated by Fig. 3. Note that the architecture HVs \u20d7 V are initialized with 0.\n\n\nGram-based Encoding\n\nGram-based encoding recognizes each block inside a ViT as a \"gram\" and groups parameters by tuples based on which block they belong to. It features two item memories: the #head memory and the mlp ratio memory. Since each parameter has three possible values as introduced in Sec. 4.3, each item memory hosts three item HVs, each representing a possible value in the high-dimensional space.\n\nGram-based encoding first obtains the corresponding item HVs from the item memory based on the parameter tuples of #head and mlp ratio. The two indexed HVs are then aggregated by HV multiplication, forming the gram HV. Then, the gram HVs are permuted where the shift amount is based on the depth index of the encoder, i.e., encoders closer to the output of the ViT model are shifted for more dimensions, or vice versa. The permuted gram HVs are summed up into the architecture HV \u20d7 V . Encoded HVs are bipolarized (or binarized).\n\nNote that the Gram-based encoding can be paralleled, since the encoding within each gram is independent from each other, and the shift amount of permutation is solely dependent on the depth index. A major disadvantage of Grambased encoding, however, is that the permutation operation is not very straightforward for implementation of acceleration, since the cyclic rotation of high dimensional vectors are not linear vector operations like addition and multiplication. Therefore, Record-based encoding is an alternative to the Gram-based encoding which eliminates the use of permutation, but only keeps addition and multiplication of HVs.\n\n\nRecord-based Encoding\n\nRecord-based encoding uses one additional item memory, the depth memory as an alternative to the gram-based permutation. The depth memory hosts item HVs at the number of depth index, therefore, there are 12 item HVs in the depth memory given the max possible depth is 12 for this dataset. Within each encoder, the HV aggregation is the same as Gram-based encoding. However, instead of permutation by the depth index, the HV of each encoder is multiplied by the item HV of the corresponding depth, indexed from the depth memory. Then the HVs are summed up to obtain the architecture HV, which is also bipolarized (or binarized). In realistic implementation, based on the commutative properties of multiplication, the item HVs in the item memories can encode first and stored as a unified item memory to avoid repetitive index and HV multiplications.\n\n\nPerfHD Training\n\nTraining is to establish the associative memory that accommodates the task HVs which are initialized with zeros. In PerfHD, training is the weighted sum of the HVs  obtained from the encoding phase as described in Eq. (1), where \u20d7 A t is the HV of task t in the associative memory, w (t) n is the task t weight of the n-th architecture in the training set, and \u20d7 V n is the encoded HV of the n-th architecture as well. Architectures with higher ranks of a specific task are assigned with higher weights, while architectures with lower ranks are assigned with lower weights. A custom learning rate \u03b3 can be specified, however, during training we use the constant 1. The objective of training is to incorporate more information about high performing architectures into the task HV, while still trying to consider information from others HVs as much as possible to avoid over-fitting, since there are only 500 architectures in the training set.\n\u20d7 A t = \u20d7 A t + \u03b3 \u00d7 w (t) n \u00d7 \u20d7 V n (n = 1, 2, \u00b7 \u00b7 \u00b7 , N ) (1)\nIntuitively, we convert the ranking into weights using a straightforward inverse number method as Eq. (2). w n = {w n1 , w n2 , \u00b7 \u00b7 \u00b7 , w nT } is the weight vector of the n-th architecture, in which w it refers to the weight of t-th task. r n = {r n1 , r n2 , \u00b7 \u00b7 \u00b7 , r nT } is the ranking vector from the training set, with rankings of each task correspondingly, as shown in Fig. 2. \u00b5 is a constant scaling factor and we use 1.0 during the experiments. Therefore, worse-thanaverage performing architectures are assigned with negative weights and better-than-average ones are assigned with positive weights.\nw n = \u00b5(1 \u2212 2 r n + 1 )(2)\n\nPerfHD Prediction\n\nInstead of directly predicting the actual ranking, PerfHD uses the similarity metrics to determine the ranking of the test set. For an architecture with unknown ranking, PerfHD first encodes its parameters into the architecture HV using the same item memories and encoding scheme for training. Then, PerfHD checks the similarity between the architecture HV and the task HVs in the associative memory. For each task, the similarity of all the architectures are recorded and then used to rank the task performance. Specifically, as the task HV in the associative memory is the aggregation of architectures with high performance, therefore, a higher similarity will then be predicted with higher ranking by PerfHD.\n\n\nPerfHD Retraining\n\nIn HDC, training usually takes one epoch (a full iteration of the entire training set). Additional epochs of retraining (still using the training set) can be optionally performed to enhance the performance of the model. HDC models for classification tasks leverages prediction labels to update the associative memory when a mis-classification is identified [17]. However, PerfHD targets at a ranking task, therefore we propose a novel retraining methodology referred to as retraining by weight update to increase the consistency of predicted rankings and the ground-truth.\n\nPerfHD retraining is based on updating the associative memory by weight-difference. The main concept is to make sure the similarity ranking of each architecture is consis-tent with the ground-truth ranking to minimize the Kendall tau [9] score difference between the prediction and the ground-truth. First after the training epoch, PerfHD calculates the similarity metrics of all the N architectures in the training set which is referred to as \u03b4 = {\u03b4 1 , \u03b4 2 , \u00b7 \u00b7 \u00b7 , \u03b4 N }. Based on the similarity metrics we are able to obtain the predicted rankingsr = {r 1 ,r 2 , \u00b7 \u00b7 \u00b7 ,r N }, which can be subsequently converted into the speculated weights\u0175 = {\u0175 1 ,\u0175 2 , \u00b7 \u00b7 \u00b7 ,\u0175 N } according to Eq. (2).\n\nWe use the difference between the speculated weights and the ground-truth weights \u2206w = {w 1 \u2212\u0175 1 , w 2 \u2212 w 2 , \u00b7 \u00b7 \u00b7 , w N \u2212\u0175 N } to perform training again to update the associative memory based on Eq. (1). A negative weight difference means the architecture is \"under-ranked\" and its HV should be added into the task HV during retraining, on the other hand, a positive difference means the architecture is \"over-ranked\" and its HV should be subtracted instead. Retraining can iterate for multiple epochs and can be stopped when the (average) difference is smaller than a threshold to prevent over-fitting. We use the starting learning rate of 1 for consistency with training, however, for additional epochs in retraining we apply a decay of 0.8.\n\n\nExperimental Results\n\n\nExperimental Setup\n\nThe dataset of architecture performance is curated from the multi-task VIMER-UFO benchmark, with eight different computer vision tasks: CPLFW, Market1501, DukeMTMC, MSMT-17, Veri-776, VehicleId, VeriWild, and SOP [19], provided by the Second Lightweight NAS Challenge 1 . There are 500 architectures in the training set with performance ranked on each of the task, and 99,500 for ranking prediction as the test set. We implement PerfHD using PyTorch and evaluate PerfHD with one NVIDIA P100 GPU. We also compare PerfHD with four baseline methods:\n\n\u2022 GP-NAS [11], is Gaussian Process Neural Architecture Search where the correlation between performances and architectures and the correlation between different architectures are explicitly modeled. An efficient sampling method is also proposed which enables GP-NAS learning on a small set of samples.\n\n\u2022 LightGBM [7,8], which is a widely adopted gradient boosting decision tree.\n\n\u2022 CatBoost [4], is another gradient boosting baseline but with categorical features support.\n\n\u2022 GP-NAS Ensemble [2], which is an ensemble version of GP-NAS [11], it applies enhancements such as ad-ditional feature engineering, label transformation and weighted ensemble kernels.\n\u03c4 = n c \u2212 n d n(n \u2212 1)/2 (3)\nKendall tau score is used to describe the consistency between the model prediction and the ground-truth, a higher scores means the ranking predictions are more accurate. Kendall tau \u03c4 between two vectors of ranking can be calculated by Eq. (3), where n c and n d are the number of concordant and discordant pairs, respectively and n is the total number of pairs [9].\n\n\nComparison on Accuracy\n\nWe present the comparison between PerfHD and the baselines on the VIMER-UFO benchmark in Tab. 1. Based on this table, we observe several important facts. First, we want to mention that the average performance across all models can drastically vary amongst different datasets. For CPLFW, all the models are having significantly lower score than the other 7 tasks, making this face related dataset the most challenging task in this benchmark. MSMT, on the contrary, is the easiest task amongst all that all the methods evaluated can achieve over 0.75 in score.\n\nMoreover, GP-NAS and LightGBM are the two subperforming baselines as their average scores are less than 0.7. On the other hand, CatBoost and GP-NAS Ensemble can achieve higher accuracy with scores achieving over 0.785. As to HDC-based methods, PerfHD-Record outperforms PerfHD-Gram on all the tasks and in general shows a superior score by around 0.03. We discuss that such advantage comes from the additional depth memory in PerfHD-Record, which provide an additional hyperdimensional space to represent different encoder blocks compared with PerfHD-Gram which only uses permutation in fixed amount of vector shift.\n\nComparing PerfHD with other baselines, we can notice that across all the methods evaluated, PerfHD-Record is able to achieve the second highest score on average, and is only 0.0079 lower than the GP-NAS Ensemble and around 0.0050 higher than the third place Catboost. Specifically, PerfHD-Record achieves 0.6634 score on VehicleId, which is the highest score of this task across all the compared models. For VeriWild, PerfHD-Record also achieves near-top performance of around 0.92. PerfHD-Gram on the other hand, has relatively inferior performance compared with Catboost and GP-NAS Ensemble by 0.0243 and 0.0370 respectively, however still performs better than GP-NAS and LightGBM for over 0.08.\n\n\nComparison on Efficiency\n\nWe also compare the execution time of PerfHD and baselines to show the efficiency of PerfHD. The execu- However, to achieve such enhanced performance, it requires extensive parameter tuning which also takes much longer time than just training and prediction. This parameter optimization is required for each task, which also limits the flexibility of this algorithm. GP-NAS Ensemble, the more complicated model, achieves the highest performance overall however during our evaluation, it spends way more than 10 minutes for training and prediction since each of the ensemble models require intensive training effort. We mark the execution time with >600 since it is already much timeconsuming than most of the baselines. The time reported for both of the PerfHD implementations include the entire process of all the tasks, i.e., no additional pre-training or fine-tuning is required. PerfHD-Gram requires considerably long processing time compared to most of the baselines, as the permutation operations of HVs occupy most of the time spent. As PerfHD-Gram also shows sub-par ranking scores, so we conclude that PerfHD-Record is preferred for this benchmark. Specifically, PerfHD-Record can finish training and prediction together with around 1 minute, which is the fastest method across all the compared baselines and is more than 10X faster than GP-NAS Ensemble which is the baseline with highest score.\n\n\nImpact of HV Dimension\n\nWe also present a case study on the Record-based encoding to analyze the impact of using different HV dimensions on PerfHD performance. In related HDC literature, dimensions of 10,000 -20,000 is usually used as the dimension range and higher dimensions may not guarantee a higher performance. Therefore, we choose 10,000 as the starting point of HV dimension analysis on PerfHD. However, we observed that the dimension of HVs can be increased to over 100,000 without performance saturation. Experiments show that a dimension around 100,000 achieves the best performance across all the dimensions evaluated, as shown in Fig. 4.  \n\n\nDiscussion\n\nThe efficiency of PerfHD comes from the simplicity of HDC training and retraining, which follows a concise linear operations with matrices (record-based encoding). This can hugely benefit from the high parallelism of GPU processing that significantly accelerates the PerfHD. PerfHD also does not need back-propagation like neural networks, which requires maintaining a complicated computation graph. Instead, PerfHD retraining is via the update to the associative memory which is also a weighted sum with much less computational overhead. Another advantage is that when evaluating different tasks, PerfHD can share the same item memory and encoding process. This grants a tremendous advantage over other baselines that different tasks are required to train individual models or apply relatively more complicated transfer processing.\n\n\nConclusion\n\nEfficiency of neural architecture search (NAS) algorithms has been one major bottleneck for this automated model engineering process. Specifically, the performance estimation of searched architectures often requires significant effort. How to efficiently and accurately rank a large set of ViT model performance given a small amount of training set is a realistic and important problem. In this paper we leverage the hyperdimensional computing (HDC) computing and propose PerfHD for ViT performance ranking. On the VIMER-UFO benchmark with eight different tasks, PerfHD is able to achieve top-level results yet also has tremendous acceleration compared with four baselines. The future work will focus on exploring more encoding schemes and hardware-specific designs for further acceleration and/or more energy-efficiency.\n\nFigure 1 .\n1A typical NAS flow.\n\n\nThe features used in PerfHD are the architectural parameters of the ViT model. The available training data consists of three architectural parameters including the depth of encoders (depth), number of attention heads (#head) and the dilation ratio of the MLP (mlp ratio) of each layer. Each parameter has three possible values: depth: {10, 11, 12}; #head: {10, 11, 12}; mlp ratio: {3.0, 3.5, 4.0}. Therefore, for each encoder, there are 3 \u00d7 3 possible parameter\n\nFigure 2 .\n2Training PerfHD to rank ViT performance based on architecture parameters. combinations, so the number of possible architectures in the search space are (3 \u00d7 3) 10 + (3 \u00d7 3) 11 + (3 \u00d7 3) 12 . For convenience in data processing, the parameters are (label-)encoded into numbers of {1, 2, 3}. The labels are the performance rankings of the architecture on different tasks.\n\nFigure 3 .\n3PerfHD encoding schemes to encode the ViT parameters into an HV. Two schemes are introduced: Gram-based and Recordbased encoding.\n\nFigure 4 .\n4PerfHD-Record task-average scores under different HV dimensions.\n\nTable 1 .\n1Comparison of Models on Kendall Tau Scores of the VIMER-UFO Benchmark Table 2. Comparison of Execution Time of the VIMER-UFO Benchmark Model GP-NAS LightGBM CatBoost GP-NAS Ensemble PerfHD-Gram PerfHD-Record tion time for training the model and making predictions are listed in Tab. 2. Less sophisticated models such as GP-NAS and LightGBM are faster to learn and predict, however their performance is relatively sub-par. CatBoost, based on the time reported, uses relatively short execution time.Model \nAverage CPLFW Market MTMC MSMT \nVeri \nVehicleId VeriWild \nSOP \n\nGP-NAS \n0.6196 \n0.2350 \n0.7391 \n0.7052 \n0.8063 \n0.6319 \n0.4012 \n0.6731 \n0.7654 \nLightGBM \n0.6810 \n0.2755 \n0.7742 \n0.7723 \n0.7599 \n0.7469 \n0.5900 \n0.7860 \n0.7433 \nCatboost \n0.7851 \n0.3220 \n0.8687 \n0.8883 \n0.9430 \n0.8930 \n0.6576 \n0.9099 \n0.7980 \nGP-NAS Ensemble \n0.7978 \n0.3188 \n0.8864 \n0.9045 \n0.9678 \n0.9106 \n0.6624 \n0.9199 \n0.8119 \n\nPerfHD-Gram \n0.7608 \n0.2927 \n0.8489 \n0.8580 \n0.9004 \n0.8571 \n0.6598 \n0.8819 \n0.7874 \nPerfHD-Record \n0.7899 \n0.3074 \n0.8754 \n0.8945 \n0.9555 \n0.8974 \n0.6634 \n0.9195 \n0.8060 \n\nExecution Time (second) \n92 \n69 \n80 \n>600 \n425 \n62 \n\n\nhttps : / / aistudio . baidu . com / aistudio / datasetdetail/134077\nAuthorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply.\n\nOne-shot learning for ieeg seizure detection using end-to-end binary operations: Local binary patterns with hyperdimensional computing. Alessio Burrello, Kaspar Schindler, Luca Benini, Abbas Rahimi, BioCAS. IEEE. 23Alessio Burrello, Kaspar Schindler, Luca Benini, and Abbas Rahimi. One-shot learning for ieeg seizure detection using end-to-end binary operations: Local binary patterns with hy- perdimensional computing. In BioCAS. IEEE, 2018. 2, 3\n\nGp-nas-ensemble: a model for the nas performance prediction. Kunlong Chen, Liu Yang, Yitian Chen, Kunjin Chen, Yidan Xu, Lujun Li, Third workshop on Neural Architecture Search, 2022. 26Kunlong Chen, Liu Yang, Yitian Chen, Kunjin Chen, Yidan Xu, and Lujun Li. Gp-nas-ensemble: a model for the nas per- formance prediction. Third workshop on Neural Architecture Search, 2022. 2, 6\n\nWuyang Chen, Xinyu Gong, Zhangyang Wang, arXiv:2102.11535Neural architecture search on imagenet in four gpu hours: A theoretically inspired perspective. Wuyang Chen, Xinyu Gong, and Zhangyang Wang. Neural architecture search on imagenet in four gpu hours: A theo- retically inspired perspective. arXiv:2102.11535, 2021. 1\n\nCatboost: gradient boosting with categorical features support. Anna Veronika Dorogush, Vasily Ershov, Andrey Gulin, arXiv:1810.11363Anna Veronika Dorogush, Vasily Ershov, and Andrey Gulin. Catboost: gradient boosting with categorical features sup- port. arXiv:1810.11363, 2018. 6\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, arXiv:2010.11929Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, et al. An image is worth 16x16 words: Trans- formers for image recognition at scale. arXiv:2010.11929, 2020. 1\n\nHyper-dimensional computing challenges and opportunities for ai applications. Eman Hassan, Yasmin Halawani, Baker Mohammad, Hani Saleh, IEEE Access. 2Eman Hassan, Yasmin Halawani, Baker Mohammad, and Hani Saleh. Hyper-dimensional computing challenges and opportunities for ai applications. IEEE Access, 2021. 2\n\nSkt-nas: Soft kendall's tau based neural architecture search. Di He, Third workshop on Neural Architecture Search, 2022. 26Di He. Skt-nas: Soft kendall's tau based neural architec- ture search. Third workshop on Neural Architecture Search, 2022. 2, 6\n\nLightgbm: A highly efficient gradient boosting decision tree. NeuIPS, 30. Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, Tie-Yan Liu, Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. Lightgbm: A highly efficient gradient boosting decision tree. NeuIPS, 30, 2017. 6\n\nRank correlation methods. Maurice George Kendall, Maurice George Kendall. Rank correlation methods. 1948. 6\n\nGeniehd: Efficient dna pattern matching accelerator using hyperdimensional computing. Yeseong Kim, Mohsen Imani, Niema Moshiri, Tajana Rosing, DATE. 23Yeseong Kim, Mohsen Imani, Niema Moshiri, and Tajana Rosing. Geniehd: Efficient dna pattern matching accelerator using hyperdimensional computing. In DATE, pages 115- 120. IEEE, 2020. 2, 3\n\nGp-nas: Gaussian process based neural architecture search. Zhihang Li, Teng Xi, Jiankang Deng, Gang Zhang, Shengzhao Wen, Ran He, CVPR. 6Zhihang Li, Teng Xi, Jiankang Deng, Gang Zhang, Shengzhao Wen, and Ran He. Gp-nas: Gaussian process based neural architecture search. In CVPR, pages 11933- 11942, 2020. 1, 2, 6\n\nA survey on evolutionary neural architecture search. Yuqiao Liu, Yanan Sun, Bing Xue, Mengjie Zhang, Kay Chen Gary G Yen, Tan, TNNLS. 1Yuqiao Liu, Yanan Sun, Bing Xue, Mengjie Zhang, Gary G Yen, and Kay Chen Tan. A survey on evolutionary neural architecture search. TNNLS, 2021. 1\n\nDongning Ma, Rahul Thapa, Xun Jiao, Molehd, arXiv:2106.02894Drug discovery using brain-inspired hyperdimensional computing. 23arXiv preprintDongning Ma, Rahul Thapa, and Xun Jiao. Molehd: Drug discovery using brain-inspired hyperdimensional computing. arXiv preprint arXiv:2106.02894, 2021. 2, 3\n\nEfficient biosignal processing using hyperdimensional computing: Network templates for combined learning and classification of exg signals. Abbas Rahimi, Pentti Kanerva, Luca Benini, Jan M Rabaey, Proceedings of the IEEE. 10713Abbas Rahimi, Pentti Kanerva, Luca Benini, and Jan M Rabaey. Efficient biosignal processing using hyperdimen- sional computing: Network templates for combined learning and classification of exg signals. Proceedings of the IEEE, 107(1):123-143, 2018. 2, 3\n\nA comprehensive survey of neural architecture search: Challenges and solutions. Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui Li, Xiaojiang Chen, Xin Wang, CSUR54Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui Li, Xiaojiang Chen, and Xin Wang. A comprehen- sive survey of neural architecture search: Challenges and so- lutions. CSUR, 54(4):1-34, 2021. 1\n\nMnasnet: Platform-aware neural architecture search for mobile. Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, Quoc V Le, CVPR. Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and Quoc V Le. Mnas- net: Platform-aware neural architecture search for mobile. In CVPR, pages 2820-2828, 2019. 1\n\nSpamhd: Memory-efficient text spam detection using brain-inspired hyperdimensional computing. Rahul Thapa, Bikal Lamichhane, Dongning Ma, Xun Jiao, ISVLSI. 35Rahul Thapa, Bikal Lamichhane, Dongning Ma, and Xun Jiao. Spamhd: Memory-efficient text spam detection us- ing brain-inspired hyperdimensional computing. In ISVLSI, pages 84-89. IEEE, 2021. 2, 3, 5\n\nTheoretical foundations of hyperdimensional computing. Anthony Thomas, Sanjoy Dasgupta, Tajana Rosing, JAIR. 722Anthony Thomas, Sanjoy Dasgupta, and Tajana Rosing. Theoretical foundations of hyperdimensional computing. JAIR, 72:215-249, 2021. 2\n\nUfo:unified feature optimization. Teng Xi, Yifan Sun, Deli Yu, Bi Li, Nan Peng, Gang Zhang, ECCV. Teng Xi, Yifan Sun, Deli Yu, Bi Li, Nan Peng, and Gang Zhang. Ufo:unified feature optimization. In ECCV, 2022. 6\n\nWeight-sharing neural architecture search: A battle to shrink the optimization gap. Lingxi Xie, Xin Chen, Kaifeng Bi, Longhui Wei, Yuhui Xu, Lanfei Wang, Zhengsu Chen, An Xiao, Jianlong Chang, Xiaopeng Zhang, CSUR. 549Lingxi Xie, Xin Chen, Kaifeng Bi, Longhui Wei, Yuhui Xu, Lanfei Wang, Zhengsu Chen, An Xiao, Jianlong Chang, Xi- aopeng Zhang, et al. Weight-sharing neural architecture search: A battle to shrink the optimization gap. CSUR, 54(9):1-37, 2021. 1\n\nTowards improving the consistency, efficiency, and flexibility of differentiable neural architecture search. Yibo Yang, Shan You, Hongyang Li, Fei Wang, Chen Qian, Zhouchen Lin, CVPR. Yibo Yang, Shan You, Hongyang Li, Fei Wang, Chen Qian, and Zhouchen Lin. Towards improving the consistency, ef- ficiency, and flexibility of differentiable neural architecture search. In CVPR, pages 6667-6676, 2021. 1\n\nTrainingfree transformer architecture search. Qinqin Zhou, Kekai Sheng, Xiawu Zheng, Ke Li, Xing Sun, Yonghong Tian, Jie Chen, Rongrong Ji, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition1Qinqin Zhou, Kekai Sheng, Xiawu Zheng, Ke Li, Xing Sun, Yonghong Tian, Jie Chen, and Rongrong Ji. Training- free transformer architecture search. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10894-10903, 2022. 1, 2\n", "annotations": {"author": "[{\"end\":103,\"start\":91},{\"end\":150,\"start\":104},{\"end\":183,\"start\":151},{\"end\":291,\"start\":184},{\"end\":332,\"start\":292}]", "publisher": null, "author_last_name": "[{\"end\":102,\"start\":100},{\"end\":116,\"start\":112},{\"end\":159,\"start\":155}]", "author_first_name": "[{\"end\":99,\"start\":91},{\"end\":111,\"start\":104},{\"end\":154,\"start\":151}]", "author_affiliation": "[{\"end\":290,\"start\":185},{\"end\":331,\"start\":293}]", "title": "[{\"end\":88,\"start\":1},{\"end\":420,\"start\":333}]", "venue": null, "abstract": "[{\"end\":1999,\"start\":451}]", "bib_ref": "[{\"end\":2476,\"start\":2471},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2586,\"start\":2582},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2589,\"start\":2586},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2738,\"start\":2735},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3563,\"start\":3559},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4022,\"start\":4019},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4025,\"start\":4022},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4028,\"start\":4025},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4342,\"start\":4338},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4345,\"start\":4342},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4766,\"start\":4762},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5089,\"start\":5086},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5092,\"start\":5089},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5095,\"start\":5092},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5098,\"start\":5095},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5276,\"start\":5273},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5279,\"start\":5276},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5464,\"start\":5461},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5466,\"start\":5464},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6849,\"start\":6846},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7037,\"start\":7033},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7217,\"start\":7213},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7410,\"start\":7407},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12454,\"start\":12450},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":12457,\"start\":12454},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12460,\"start\":12457},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":12588,\"start\":12585},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12591,\"start\":12588},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":19409,\"start\":19405},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":19859,\"start\":19856},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":21328,\"start\":21324},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":21672,\"start\":21668},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":21976,\"start\":21973},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":21978,\"start\":21976},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":22054,\"start\":22051},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":22155,\"start\":22152},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":22200,\"start\":22196},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":22713,\"start\":22710}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":28420,\"start\":28388},{\"attributes\":{\"id\":\"fig_1\"},\"end\":28884,\"start\":28421},{\"attributes\":{\"id\":\"fig_2\"},\"end\":29266,\"start\":28885},{\"attributes\":{\"id\":\"fig_4\"},\"end\":29409,\"start\":29267},{\"attributes\":{\"id\":\"fig_6\"},\"end\":29487,\"start\":29410},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":30628,\"start\":29488}]", "paragraph": "[{\"end\":2902,\"start\":2015},{\"end\":4346,\"start\":2904},{\"end\":5520,\"start\":4348},{\"end\":5788,\"start\":5522},{\"end\":6043,\"start\":5790},{\"end\":6367,\"start\":6045},{\"end\":7715,\"start\":6385},{\"end\":8614,\"start\":7785},{\"end\":9331,\"start\":8632},{\"end\":10045,\"start\":9333},{\"end\":10549,\"start\":10060},{\"end\":10979,\"start\":10566},{\"end\":11338,\"start\":11015},{\"end\":11661,\"start\":11374},{\"end\":12592,\"start\":11721},{\"end\":13877,\"start\":12615},{\"end\":14178,\"start\":13929},{\"end\":14590,\"start\":14202},{\"end\":15121,\"start\":14592},{\"end\":15761,\"start\":15123},{\"end\":16635,\"start\":15787},{\"end\":17596,\"start\":16655},{\"end\":18267,\"start\":17660},{\"end\":19026,\"start\":18315},{\"end\":19620,\"start\":19048},{\"end\":20317,\"start\":19622},{\"end\":21065,\"start\":20319},{\"end\":21657,\"start\":21111},{\"end\":21960,\"start\":21659},{\"end\":22038,\"start\":21962},{\"end\":22132,\"start\":22040},{\"end\":22318,\"start\":22134},{\"end\":22714,\"start\":22348},{\"end\":23299,\"start\":22741},{\"end\":23917,\"start\":23301},{\"end\":24616,\"start\":23919},{\"end\":26049,\"start\":24645},{\"end\":26704,\"start\":26076},{\"end\":27551,\"start\":26719},{\"end\":28387,\"start\":27566}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11014,\"start\":10980},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11373,\"start\":11339},{\"attributes\":{\"id\":\"formula_2\"},\"end\":17659,\"start\":17597},{\"attributes\":{\"id\":\"formula_3\"},\"end\":18294,\"start\":18268},{\"attributes\":{\"id\":\"formula_4\"},\"end\":22347,\"start\":22319}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2013,\"start\":2001},{\"attributes\":{\"n\":\"2.\"},\"end\":6383,\"start\":6370},{\"attributes\":{\"n\":\"3.\"},\"end\":7744,\"start\":7718},{\"attributes\":{\"n\":\"3.1.\"},\"end\":7769,\"start\":7747},{\"attributes\":{\"n\":\"3.1.1\"},\"end\":7783,\"start\":7772},{\"attributes\":{\"n\":\"3.1.2\"},\"end\":8630,\"start\":8617},{\"attributes\":{\"n\":\"3.1.3\"},\"end\":10058,\"start\":10048},{\"attributes\":{\"n\":\"3.2.\"},\"end\":10564,\"start\":10552},{\"attributes\":{\"n\":\"4.\"},\"end\":11682,\"start\":11664},{\"attributes\":{\"n\":\"4.1.\"},\"end\":11719,\"start\":11685},{\"attributes\":{\"n\":\"4.2.\"},\"end\":12613,\"start\":12595},{\"attributes\":{\"n\":\"4.3.\"},\"end\":13899,\"start\":13880},{\"attributes\":{\"n\":\"4.4.\"},\"end\":13927,\"start\":13902},{\"attributes\":{\"n\":\"4.4.1\"},\"end\":14200,\"start\":14181},{\"attributes\":{\"n\":\"4.4.2\"},\"end\":15785,\"start\":15764},{\"attributes\":{\"n\":\"4.5.\"},\"end\":16653,\"start\":16638},{\"attributes\":{\"n\":\"4.6.\"},\"end\":18313,\"start\":18296},{\"attributes\":{\"n\":\"4.7.\"},\"end\":19046,\"start\":19029},{\"attributes\":{\"n\":\"5.\"},\"end\":21088,\"start\":21068},{\"attributes\":{\"n\":\"5.1.\"},\"end\":21109,\"start\":21091},{\"attributes\":{\"n\":\"5.2.\"},\"end\":22739,\"start\":22717},{\"attributes\":{\"n\":\"5.3.\"},\"end\":24643,\"start\":24619},{\"attributes\":{\"n\":\"5.4.\"},\"end\":26074,\"start\":26052},{\"attributes\":{\"n\":\"5.5.\"},\"end\":26717,\"start\":26707},{\"attributes\":{\"n\":\"6.\"},\"end\":27564,\"start\":27554},{\"end\":28399,\"start\":28389},{\"end\":28896,\"start\":28886},{\"end\":29278,\"start\":29268},{\"end\":29421,\"start\":29411},{\"end\":29498,\"start\":29489}]", "table": "[{\"end\":30628,\"start\":29997}]", "figure_caption": "[{\"end\":28420,\"start\":28401},{\"end\":28884,\"start\":28423},{\"end\":29266,\"start\":28898},{\"end\":29409,\"start\":29280},{\"end\":29487,\"start\":29423},{\"end\":29997,\"start\":29500}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":2964,\"start\":2958},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12657,\"start\":12651},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":14118,\"start\":14112},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18042,\"start\":18036},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":22591,\"start\":22584},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":26701,\"start\":26695}]", "bib_author_first_name": "[{\"end\":30954,\"start\":30947},{\"end\":30971,\"start\":30965},{\"end\":30987,\"start\":30983},{\"end\":31001,\"start\":30996},{\"end\":31328,\"start\":31321},{\"end\":31338,\"start\":31335},{\"end\":31351,\"start\":31345},{\"end\":31364,\"start\":31358},{\"end\":31376,\"start\":31371},{\"end\":31386,\"start\":31381},{\"end\":31646,\"start\":31640},{\"end\":31658,\"start\":31653},{\"end\":31674,\"start\":31665},{\"end\":32030,\"start\":32026},{\"end\":32056,\"start\":32050},{\"end\":32071,\"start\":32065},{\"end\":32250,\"start\":32244},{\"end\":32269,\"start\":32264},{\"end\":32286,\"start\":32277},{\"end\":32303,\"start\":32299},{\"end\":32324,\"start\":32317},{\"end\":32337,\"start\":32331},{\"end\":32358,\"start\":32351},{\"end\":32377,\"start\":32369},{\"end\":32393,\"start\":32388},{\"end\":32887,\"start\":32883},{\"end\":32902,\"start\":32896},{\"end\":32918,\"start\":32913},{\"end\":32933,\"start\":32929},{\"end\":33181,\"start\":33179},{\"end\":33449,\"start\":33443},{\"end\":33456,\"start\":33454},{\"end\":33469,\"start\":33463},{\"end\":33485,\"start\":33478},{\"end\":33495,\"start\":33492},{\"end\":33509,\"start\":33502},{\"end\":33519,\"start\":33514},{\"end\":33531,\"start\":33524},{\"end\":33751,\"start\":33744},{\"end\":33758,\"start\":33752},{\"end\":33920,\"start\":33913},{\"end\":33932,\"start\":33926},{\"end\":33945,\"start\":33940},{\"end\":33961,\"start\":33955},{\"end\":34234,\"start\":34227},{\"end\":34243,\"start\":34239},{\"end\":34256,\"start\":34248},{\"end\":34267,\"start\":34263},{\"end\":34284,\"start\":34275},{\"end\":34293,\"start\":34290},{\"end\":34542,\"start\":34536},{\"end\":34553,\"start\":34548},{\"end\":34563,\"start\":34559},{\"end\":34576,\"start\":34569},{\"end\":34587,\"start\":34584},{\"end\":34592,\"start\":34588},{\"end\":34773,\"start\":34765},{\"end\":34783,\"start\":34778},{\"end\":34794,\"start\":34791},{\"end\":35207,\"start\":35202},{\"end\":35222,\"start\":35216},{\"end\":35236,\"start\":35232},{\"end\":35248,\"start\":35245},{\"end\":35250,\"start\":35249},{\"end\":35633,\"start\":35625},{\"end\":35642,\"start\":35639},{\"end\":35656,\"start\":35649},{\"end\":35670,\"start\":35664},{\"end\":35684,\"start\":35678},{\"end\":35698,\"start\":35689},{\"end\":35708,\"start\":35705},{\"end\":35997,\"start\":35989},{\"end\":36005,\"start\":36003},{\"end\":36019,\"start\":36012},{\"end\":36031,\"start\":36026},{\"end\":36047,\"start\":36043},{\"end\":36063,\"start\":36057},{\"end\":36078,\"start\":36072},{\"end\":36386,\"start\":36381},{\"end\":36399,\"start\":36394},{\"end\":36420,\"start\":36412},{\"end\":36428,\"start\":36425},{\"end\":36706,\"start\":36699},{\"end\":36721,\"start\":36715},{\"end\":36738,\"start\":36732},{\"end\":36928,\"start\":36924},{\"end\":36938,\"start\":36933},{\"end\":36948,\"start\":36944},{\"end\":36955,\"start\":36953},{\"end\":36963,\"start\":36960},{\"end\":36974,\"start\":36970},{\"end\":37192,\"start\":37186},{\"end\":37201,\"start\":37198},{\"end\":37215,\"start\":37208},{\"end\":37227,\"start\":37220},{\"end\":37238,\"start\":37233},{\"end\":37249,\"start\":37243},{\"end\":37263,\"start\":37256},{\"end\":37272,\"start\":37270},{\"end\":37287,\"start\":37279},{\"end\":37303,\"start\":37295},{\"end\":37678,\"start\":37674},{\"end\":37689,\"start\":37685},{\"end\":37703,\"start\":37695},{\"end\":37711,\"start\":37708},{\"end\":37722,\"start\":37718},{\"end\":37737,\"start\":37729},{\"end\":38020,\"start\":38014},{\"end\":38032,\"start\":38027},{\"end\":38045,\"start\":38040},{\"end\":38055,\"start\":38053},{\"end\":38064,\"start\":38060},{\"end\":38078,\"start\":38070},{\"end\":38088,\"start\":38085},{\"end\":38103,\"start\":38095}]", "bib_author_last_name": "[{\"end\":30963,\"start\":30955},{\"end\":30981,\"start\":30972},{\"end\":30994,\"start\":30988},{\"end\":31008,\"start\":31002},{\"end\":31333,\"start\":31329},{\"end\":31343,\"start\":31339},{\"end\":31356,\"start\":31352},{\"end\":31369,\"start\":31365},{\"end\":31379,\"start\":31377},{\"end\":31389,\"start\":31387},{\"end\":31651,\"start\":31647},{\"end\":31663,\"start\":31659},{\"end\":31679,\"start\":31675},{\"end\":32048,\"start\":32031},{\"end\":32063,\"start\":32057},{\"end\":32077,\"start\":32072},{\"end\":32262,\"start\":32251},{\"end\":32275,\"start\":32270},{\"end\":32297,\"start\":32287},{\"end\":32315,\"start\":32304},{\"end\":32329,\"start\":32325},{\"end\":32349,\"start\":32338},{\"end\":32367,\"start\":32359},{\"end\":32386,\"start\":32378},{\"end\":32401,\"start\":32394},{\"end\":32894,\"start\":32888},{\"end\":32911,\"start\":32903},{\"end\":32927,\"start\":32919},{\"end\":32939,\"start\":32934},{\"end\":33184,\"start\":33182},{\"end\":33452,\"start\":33450},{\"end\":33461,\"start\":33457},{\"end\":33476,\"start\":33470},{\"end\":33490,\"start\":33486},{\"end\":33500,\"start\":33496},{\"end\":33512,\"start\":33510},{\"end\":33522,\"start\":33520},{\"end\":33535,\"start\":33532},{\"end\":33766,\"start\":33759},{\"end\":33924,\"start\":33921},{\"end\":33938,\"start\":33933},{\"end\":33953,\"start\":33946},{\"end\":33968,\"start\":33962},{\"end\":34237,\"start\":34235},{\"end\":34246,\"start\":34244},{\"end\":34261,\"start\":34257},{\"end\":34273,\"start\":34268},{\"end\":34288,\"start\":34285},{\"end\":34296,\"start\":34294},{\"end\":34546,\"start\":34543},{\"end\":34557,\"start\":34554},{\"end\":34567,\"start\":34564},{\"end\":34582,\"start\":34577},{\"end\":34603,\"start\":34593},{\"end\":34608,\"start\":34605},{\"end\":34776,\"start\":34774},{\"end\":34789,\"start\":34784},{\"end\":34799,\"start\":34795},{\"end\":34807,\"start\":34801},{\"end\":35214,\"start\":35208},{\"end\":35230,\"start\":35223},{\"end\":35243,\"start\":35237},{\"end\":35257,\"start\":35251},{\"end\":35637,\"start\":35634},{\"end\":35647,\"start\":35643},{\"end\":35662,\"start\":35657},{\"end\":35676,\"start\":35671},{\"end\":35687,\"start\":35685},{\"end\":35703,\"start\":35699},{\"end\":35713,\"start\":35709},{\"end\":36001,\"start\":35998},{\"end\":36010,\"start\":36006},{\"end\":36024,\"start\":36020},{\"end\":36041,\"start\":36032},{\"end\":36055,\"start\":36048},{\"end\":36070,\"start\":36064},{\"end\":36081,\"start\":36079},{\"end\":36392,\"start\":36387},{\"end\":36410,\"start\":36400},{\"end\":36423,\"start\":36421},{\"end\":36433,\"start\":36429},{\"end\":36713,\"start\":36707},{\"end\":36730,\"start\":36722},{\"end\":36745,\"start\":36739},{\"end\":36931,\"start\":36929},{\"end\":36942,\"start\":36939},{\"end\":36951,\"start\":36949},{\"end\":36958,\"start\":36956},{\"end\":36968,\"start\":36964},{\"end\":36980,\"start\":36975},{\"end\":37196,\"start\":37193},{\"end\":37206,\"start\":37202},{\"end\":37218,\"start\":37216},{\"end\":37231,\"start\":37228},{\"end\":37241,\"start\":37239},{\"end\":37254,\"start\":37250},{\"end\":37268,\"start\":37264},{\"end\":37277,\"start\":37273},{\"end\":37293,\"start\":37288},{\"end\":37309,\"start\":37304},{\"end\":37683,\"start\":37679},{\"end\":37693,\"start\":37690},{\"end\":37706,\"start\":37704},{\"end\":37716,\"start\":37712},{\"end\":37727,\"start\":37723},{\"end\":37741,\"start\":37738},{\"end\":38025,\"start\":38021},{\"end\":38038,\"start\":38033},{\"end\":38051,\"start\":38046},{\"end\":38058,\"start\":38056},{\"end\":38068,\"start\":38065},{\"end\":38083,\"start\":38079},{\"end\":38093,\"start\":38089},{\"end\":38106,\"start\":38104}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":52168429},\"end\":31258,\"start\":30811},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":256105379},\"end\":31638,\"start\":31260},{\"attributes\":{\"doi\":\"arXiv:2102.11535\",\"id\":\"b2\"},\"end\":31961,\"start\":31640},{\"attributes\":{\"doi\":\"arXiv:1810.11363\",\"id\":\"b3\"},\"end\":32242,\"start\":31963},{\"attributes\":{\"doi\":\"arXiv:2010.11929\",\"id\":\"b4\"},\"end\":32803,\"start\":32244},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":233975748},\"end\":33115,\"start\":32805},{\"attributes\":{\"id\":\"b6\"},\"end\":33367,\"start\":33117},{\"attributes\":{\"id\":\"b7\"},\"end\":33716,\"start\":33369},{\"attributes\":{\"id\":\"b8\"},\"end\":33825,\"start\":33718},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":219858990},\"end\":34166,\"start\":33827},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":220127537},\"end\":34481,\"start\":34168},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":221293236},\"end\":34763,\"start\":34483},{\"attributes\":{\"doi\":\"arXiv:2106.02894\",\"id\":\"b12\"},\"end\":35060,\"start\":34765},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":57365377},\"end\":35543,\"start\":35062},{\"attributes\":{\"id\":\"b14\"},\"end\":35924,\"start\":35545},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":51891697},\"end\":36285,\"start\":35926},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":235806322},\"end\":36642,\"start\":36287},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":222378549},\"end\":36888,\"start\":36644},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":250918889},\"end\":37100,\"start\":36890},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":220961460},\"end\":37563,\"start\":37102},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":231718659},\"end\":37966,\"start\":37565},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":247619115},\"end\":38519,\"start\":37968}]", "bib_title": "[{\"end\":30945,\"start\":30811},{\"end\":31319,\"start\":31260},{\"end\":32881,\"start\":32805},{\"end\":33177,\"start\":33117},{\"end\":33911,\"start\":33827},{\"end\":34225,\"start\":34168},{\"end\":34534,\"start\":34483},{\"end\":35200,\"start\":35062},{\"end\":35987,\"start\":35926},{\"end\":36379,\"start\":36287},{\"end\":36697,\"start\":36644},{\"end\":36922,\"start\":36890},{\"end\":37184,\"start\":37102},{\"end\":37672,\"start\":37565},{\"end\":38012,\"start\":37968}]", "bib_author": "[{\"end\":30965,\"start\":30947},{\"end\":30983,\"start\":30965},{\"end\":30996,\"start\":30983},{\"end\":31010,\"start\":30996},{\"end\":31335,\"start\":31321},{\"end\":31345,\"start\":31335},{\"end\":31358,\"start\":31345},{\"end\":31371,\"start\":31358},{\"end\":31381,\"start\":31371},{\"end\":31391,\"start\":31381},{\"end\":31653,\"start\":31640},{\"end\":31665,\"start\":31653},{\"end\":31681,\"start\":31665},{\"end\":32050,\"start\":32026},{\"end\":32065,\"start\":32050},{\"end\":32079,\"start\":32065},{\"end\":32264,\"start\":32244},{\"end\":32277,\"start\":32264},{\"end\":32299,\"start\":32277},{\"end\":32317,\"start\":32299},{\"end\":32331,\"start\":32317},{\"end\":32351,\"start\":32331},{\"end\":32369,\"start\":32351},{\"end\":32388,\"start\":32369},{\"end\":32403,\"start\":32388},{\"end\":32896,\"start\":32883},{\"end\":32913,\"start\":32896},{\"end\":32929,\"start\":32913},{\"end\":32941,\"start\":32929},{\"end\":33186,\"start\":33179},{\"end\":33454,\"start\":33443},{\"end\":33463,\"start\":33454},{\"end\":33478,\"start\":33463},{\"end\":33492,\"start\":33478},{\"end\":33502,\"start\":33492},{\"end\":33514,\"start\":33502},{\"end\":33524,\"start\":33514},{\"end\":33537,\"start\":33524},{\"end\":33768,\"start\":33744},{\"end\":33926,\"start\":33913},{\"end\":33940,\"start\":33926},{\"end\":33955,\"start\":33940},{\"end\":33970,\"start\":33955},{\"end\":34239,\"start\":34227},{\"end\":34248,\"start\":34239},{\"end\":34263,\"start\":34248},{\"end\":34275,\"start\":34263},{\"end\":34290,\"start\":34275},{\"end\":34298,\"start\":34290},{\"end\":34548,\"start\":34536},{\"end\":34559,\"start\":34548},{\"end\":34569,\"start\":34559},{\"end\":34584,\"start\":34569},{\"end\":34605,\"start\":34584},{\"end\":34610,\"start\":34605},{\"end\":34778,\"start\":34765},{\"end\":34791,\"start\":34778},{\"end\":34801,\"start\":34791},{\"end\":34809,\"start\":34801},{\"end\":35216,\"start\":35202},{\"end\":35232,\"start\":35216},{\"end\":35245,\"start\":35232},{\"end\":35259,\"start\":35245},{\"end\":35639,\"start\":35625},{\"end\":35649,\"start\":35639},{\"end\":35664,\"start\":35649},{\"end\":35678,\"start\":35664},{\"end\":35689,\"start\":35678},{\"end\":35705,\"start\":35689},{\"end\":35715,\"start\":35705},{\"end\":36003,\"start\":35989},{\"end\":36012,\"start\":36003},{\"end\":36026,\"start\":36012},{\"end\":36043,\"start\":36026},{\"end\":36057,\"start\":36043},{\"end\":36072,\"start\":36057},{\"end\":36083,\"start\":36072},{\"end\":36394,\"start\":36381},{\"end\":36412,\"start\":36394},{\"end\":36425,\"start\":36412},{\"end\":36435,\"start\":36425},{\"end\":36715,\"start\":36699},{\"end\":36732,\"start\":36715},{\"end\":36747,\"start\":36732},{\"end\":36933,\"start\":36924},{\"end\":36944,\"start\":36933},{\"end\":36953,\"start\":36944},{\"end\":36960,\"start\":36953},{\"end\":36970,\"start\":36960},{\"end\":36982,\"start\":36970},{\"end\":37198,\"start\":37186},{\"end\":37208,\"start\":37198},{\"end\":37220,\"start\":37208},{\"end\":37233,\"start\":37220},{\"end\":37243,\"start\":37233},{\"end\":37256,\"start\":37243},{\"end\":37270,\"start\":37256},{\"end\":37279,\"start\":37270},{\"end\":37295,\"start\":37279},{\"end\":37311,\"start\":37295},{\"end\":37685,\"start\":37674},{\"end\":37695,\"start\":37685},{\"end\":37708,\"start\":37695},{\"end\":37718,\"start\":37708},{\"end\":37729,\"start\":37718},{\"end\":37743,\"start\":37729},{\"end\":38027,\"start\":38014},{\"end\":38040,\"start\":38027},{\"end\":38053,\"start\":38040},{\"end\":38060,\"start\":38053},{\"end\":38070,\"start\":38060},{\"end\":38085,\"start\":38070},{\"end\":38095,\"start\":38085},{\"end\":38108,\"start\":38095}]", "bib_venue": "[{\"end\":38257,\"start\":38191},{\"end\":31022,\"start\":31010},{\"end\":31441,\"start\":31391},{\"end\":31791,\"start\":31697},{\"end\":32024,\"start\":31963},{\"end\":32515,\"start\":32419},{\"end\":32952,\"start\":32941},{\"end\":33236,\"start\":33186},{\"end\":33441,\"start\":33369},{\"end\":33742,\"start\":33718},{\"end\":33974,\"start\":33970},{\"end\":34302,\"start\":34298},{\"end\":34615,\"start\":34610},{\"end\":34887,\"start\":34825},{\"end\":35282,\"start\":35259},{\"end\":35623,\"start\":35545},{\"end\":36087,\"start\":36083},{\"end\":36441,\"start\":36435},{\"end\":36751,\"start\":36747},{\"end\":36986,\"start\":36982},{\"end\":37315,\"start\":37311},{\"end\":37747,\"start\":37743},{\"end\":38189,\"start\":38108}]"}}}, "year": 2023, "month": 12, "day": 17}