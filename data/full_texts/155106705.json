{"id": 155106705, "updated": "2022-02-11 20:10:21.873", "metadata": {"title": "Robust and Unsupervised KPI Anomaly Detection Based on Conditional Variational Autoencoder", "authors": "[{\"first\":\"Zeyan\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Wenxiao\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Dan\",\"last\":\"Pei\",\"middle\":[]}]", "venue": "2018 IEEE 37th International Performance Computing and Communications Conference (IPCCC)", "journal": "2018 IEEE 37th International Performance Computing and Communications Conference (IPCCC)", "publication_date": {"year": 2018, "month": null, "day": null}, "abstract": "To ensure undisrupted web-based services, operators need to closely monitor various KPIs (Key Performance Indicator, such as CPU usages, network throughput, page views, number of online users, and etc), detect anomalies in them, and trigger timely troubleshooting or mitigation. There can be hundreds of thousands to even millions of KPIs to be monitored, thus operators need automatic anomaly detection approaches. However, neither traditional statistical approaches nor supervised ensemble approaches satisfy this requirement in practice when facing large number of KPIs. A state-of-art unsupervised approach Donut offering promising results, but it is not a sequential model thus cannot deal with the time information related anomalies. Thus, in this paper we propose Bagel, a robust and unsupervised anomaly detection algorithm for KPI that can handle time information related anomalies, using CVAE to incorporate time information and dropout layer to avoid overfitting. Our experiments using real data from Internet companies show that, compared to Donut, Bagel improves the anomaly detection best F1-score by 0.08 to 0.43.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2944981198", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/ipccc/LiCP18", "doi": "10.1109/pccc.2018.8710885"}}, "content": {"source": {"pdf_hash": "c04643a3edd2c6b9a6a2c214462dae1fc1cbe9e2", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "007b37a9840f335ca47ab8b5f705c464aec8f262", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/c04643a3edd2c6b9a6a2c214462dae1fc1cbe9e2.txt", "contents": "\nRobust and Unsupervised KPI Anomaly Detection Based on Conditional Variational Autoencoder\n\n\nZeyan Li \nBeijing National Research Center for Information Science and Technology(BNRist)\n\n\nWenxiao Chen \nBeijing National Research Center for Information Science and Technology(BNRist)\n\n\nDan Pei peidan@tsinghua.edu.cn \nBeijing National Research Center for Information Science and Technology(BNRist)\n\n\n\nTsinghua University\n\n\nRobust and Unsupervised KPI Anomaly Detection Based on Conditional Variational Autoencoder\n\nTo ensure undisrupted web-based services, operators need to closely monitor various KPIs (Key Performance Indicator, such as CPU usages, network throughput, page views, number of online users, and etc), detect anomalies in them, and trigger timely troubleshooting or mitigation. There can be hundreds of thousands to even millions of KPIs to be monitored, thus operators need automatic anomaly detection approaches. However, neither traditional statistical approaches nor supervised ensemble approaches satisfy this requirement in practice when facing large number of KPIs. A state-of-art unsupervised approach Donut offering promising results, but it is not a sequential model thus cannot deal with the time information related anomalies. Thus, in this paper we propose Bagel, a robust and unsupervised anomaly detection algorithm for KPI that can handle time information related anomalies, using CVAE to incorporate time information and dropout layer to avoid overfitting. Our experiments using real data from Internet companies show that, compared to Donut, Bagel improves the anomaly detection best F1-score by 0.08 to 0.43.\n\nI. INTRODUCTION\n\nTo ensure undisrupted web-based services, operators need to closely monitor various KPIs (Key Performance Indicator, such as CPU usages, network throughput, page views, number of online users, and etc), detect anomalies in them, and trigger timely troubleshooting or mitigation. Fig. 1 shows a few KPIs that we studied in this paper. There can be hundreds of thousands to even millions of KPIs to be monitored [1], [2], thus operators need automatic anomaly detection approaches.\n\nDespite many proposed anomaly detection approaches in the past [3]- [12], most do not work well in the practice according to [11]. A comparison summary is shown in Table I, which will be elaborated later in \u00a7VI. For Traditional statistical algorithms [3]- [7], operators have to manually select an anomaly detection algorithm and tune its parameters for each KPI. Supervised learning based methods [8], [9] require manually labeling anomalies for each KPI. Thus, neither traditional statistical approaches nor supervised learning based approaches are automatic, and they do not work well in practice when facing large number of KPIs.\n\nMore recently, unsupervised approaches using deep generative models show some very promising results. Based on variational autoencoder (VAE), a state-of-art unsupervised anomaly detection algorithm, Donut [11], significantly outperforms Dan Pei \u2021 is the corresponding author. G has a lot of missing points, and many long missing fragments. There are several normal fragments surrounded by long missing fragments. For example, the fragments in the blues boxes are following the same pattern, but the last one is surrounded by two long missing fragments, making it difficult to reconstruct its normal pattern. H is quite smooth but has many short periodic spikes, but these spikes are not exactly the same every day (such as the valleys highlighted by the brown boxes).\n\nthe state-of-art supervised ensemble approach Opprentince (which outperforms all traditional statistical approaches) [9] on seasonal KPIs (such as A, B, C in Fig. 1). Seasonal KPIs are very common in practice and are business-related (such as number of online users, number of queries), thus are very important in anomaly detection [11]. However, Donut is not robust enough against time information related anomalies. This is because VAE is not a sequential model and Donut uses sliding windows to feed KPIs to VAE but ignores the relationship between windows. i.e., Donut ignores the time information of a KPI window, and the SGD based optimization algorithm Donut uses shuffles the training data. For example, the relatively long fragment of missing data in G in Fig. 1 causes false positives for the data points right after the missing data fragment (highlighted in rightmost blue dashed box in the figure), while we can imagine if we somehow incorporate timing information into the model (the pattern in those blue boxes are very similar across different days), these false negatives can be avoided. Similarly, the periodic (thus normal) spikes at the daily valleys in KPI H in Fig. 1 will be mistakenly classified as anomalous by Donut, while incorporating timing information can help as well.\n\nSince Donut is the state-of-art anomaly detection algorithm  [5] 2: supervised ensemble method, e.g., Opprentice [9] 3: traditional unsupervised method, e.g., one-class SVM [14] 4: sequential deep generative model, e.g., VRNN [15] 5: non-sequential deep generative model, e.g. VAE [11], [16] and more importantly have solid theoretical foundations, in this paper, we aim to push Donut one significant step forward towards practical deployment, by improving Donut's robustness against time information related anomalies, like G, H in Fig. 1. Note that in this paper we focus on anomaly detection in seasonal KPIs as well, just as in Donut [11] .\n\nWe propose Bagel, a robust and unsupervised anomaly detection algorithm for KPI. To incorporate time information, Bagel is based on conditional variational autoencoder (CVAE) as opposed to VAE in Donut, and uses time information as the input condition. However, there is one important challenge in incorporating time information into CVAE model. Because fitting the relationship between timing and KPI value is much easier (similar to traditional statistical model with seasonality, e.g., historical average) than fitting the relationship between the input sliding windows and reconstructed normal patterns, CVAE can be easily overfitted on time information for seasonal KPIs. To avoid overfitting, we add an extra layer of dropout, which can be considered making an ensemble model of many smaller neural networks [13].\n\nThe contributions of this paper can be summarized as follows:\n\n\u2022 For the first time in the literature, we identify the importance of time information for non-sequential deep generative models, such as Donut, in KPI anomaly detection problem. \u2022 To the best of our knowledge, Bagel is the first to apply conditional variational autoencoder (CVAE) to KPI anomaly detection and use dropout technique to successfully avoid overfitting. \u2022 Our experiments using real data from Internet companies show that, compared to Donut, Bagel improves the anomaly detection best F1-score by 0.08 to 0.43 for KPIs G and H, greatly improving Donut's robustness against time information related anomalies.\n\nThe rest of the paper is organized as follows. \u00a7IV-A reviews the background of KPI anomaly detection and reviews the background of VAE and CVAE. \u00a7III presents Bagel's neural network architecture, and its design of training and anomaly detection. \u00a7IV evaluates Bagel's performance. \u00a7V analyzes how Bagel works. \u00a7VI reviews related work. Finally, \u00a7VII concludes the paper.\n\n\nII. BACKGROUND AND PROBLEM\n\n\nA. KPI and KPI Anomaly Detection\n\nIn this paper, we focus on business-related KPIs, as in [11]. These KPIs have seasonal patterns because of the influence from user behavior and schedule. However, the KPI patterns at each repetitive cycle is not exactly the same, since user behavior will not be exactly the same everyday. As [11] does, we name these differences \"local variations\". A KPI anomaly detection algorithm will not work well unless it can handle the local variations well. Besides the seasonal pattern and local variations, there are also noises on KPIs. We assume that the noises follow independent, zero-mean Gaussian distribution.\n\nIn summary, the normal patterns of the KPIs that we study consist of two components: (1) seasonal patterns with local variations, (2) independent, zero-mean Gaussian noises. The anomalies are those data points which do not follow the normal patterns.\n\nThe KPI values are usually collected with a fixed monitoring interval like 10 seconds or 1 minute. However, because of occasional technical errors, sometimes the KPI values are not collected. These data points are are called missing points. Missing points are also some kind of anomalies, but it is easy to distinguish them from normal points. Therefore, in this paper, we use anomaly points to call those points that do not follow normal patterns but are not missing points, and use abnormal points to call both missing points and anomaly points.\n\nKPI anomaly detection problem can be formulated as follows: for any time t, given historical KPI observations v t\u2212W +1:t with length W , determine whether anomaly happens at time t (denoted by \u03b3 t = 1).\n\n\nB. Variational Autoencoder and Conditional Variational Autoencoder\n\nAs previously mentioned in \u00a7I, one particularly promising direction for KPI anomaly detection is deep generative model, such as Variational Auto-Encoder (VAE) [17], [18]. VAE uses neural networks to model data's distribution and generate new samples following it. Donut [11] is a state-of-art VAE based KPI anomaly detection algorithm. In this section, we briefly introduce the background of VAE and conditional variational auto-encoder (CVAE) used in our proposed approach Bagel.\n\nDeep Bayesian networks combines deep learning and probabilistic graphical models (PGM). It models the relationship among random variables with neural networks, which extends the ability of PGM. Variational inference [19] is very useful for solving the posteriors of the distributions derived by neural networks, so it is usually adopted for the training and prediction of deep Bayesian networks.\n\nVariational autoencoder (VAE) and conditional variational autoencoder (CVAE) [20], [21] are typical deep Bayesian networks. VAE models the relationship between two random variables x and z. CVAE models the relationship between x and z, conditioned on y, i.e., it models p(x, z|y). VAE and CVAE are very similar. We choose CVAE in Bagel rather Z X Y \u2713 Fig. 2. The architecture of CVAE. Considering the prior of z as part of the generative process, the whole generative model (solid lines) can be formulated as p \u03b8 (z, x|y) = p \u03b8 (x|z, y)p \u03b8 (z|y). The approximated posterior (dashed lines) is q \u03c6 (z|x, y).\n\nthan VAE because the condition variable is important for KPI anomaly detection (see \u00a7V-B).\n\nThe generative process of CVAE is as follows: 1) Choose a z prior distribution, and sample z from it, i.e., z \u223c p \u03b8 (z|y). As [21] suggests, we can make latent z independent of y, i.e., z \u223c p \u03b8 (z). 2) Sample x from p \u03b8 (x|z, y), which is derived from a neural network with parameter \u03b8, i.e., x \u223c p \u03b8 (x|z, y) Although the true posterior p \u03b8 (z|x, y) plays an important role in training and prediction, it is intractable [18]. In variational inference, it is approximated by a variational distribution q \u03c6 (z|x, y), which is fitted by another neural network with parameter \u03c6. SGVB [17], [18] is a variational inference algorithm which is often used along with VAE. SGVB jointly trains the approximated posterior and generative model by maximizing the evidence lower bound (ELBO, Eqn. (1)). We adopt SGVB because it works for a broad range of applications [19] and is sufficient for our task already.\nlog p(x|y) \u2265 log p(x|y) \u2212 KL q \u03c6 (z|x, y) p \u03b8 (z|x, y) = L(x, y) = E q \u03c6 (z|x,y) [log p \u03b8 (x|z, y) + log p \u03b8 (z) \u2212 log q \u03c6 (z|x, y)](1)\nThe overall architecture of CVAE is summarized in Fig. 2 \n\n\nIII. ARCHITECTURE\n\nIn this section, we will introduce the details of our proposed algorithm Bagel, including the network architecture, training and detection. We will also highlight the major difference between Bagel and Donut.\n\nA. Network Architecture 1) Preprocessing: As mentioned in \u00a7II-A, there are some missing points in KPIs. So first of all we impute these missing points with zero, then the imputed KPIs become time series with fixed monitoring interval. Different KPIs' value ranges are various, so we standardize KPIs with z-score: firstly we calculate the mean \u00b5 and standard deviation \u03c3 of the whole KPI, and then calculate the new value of each point As mentioned in \u00a7II-A, the KPIs that we studied are time series. However, CVAE is not a sequential model. We use sliding windows of a KPI as the input data of CVAE. Formally speaking, for a KPI\nv i by v i \u2190 vi\u2212\u00b5 \u03c3 . z K f \u2713 (z) f \u2713 (z) W SoftPlus+\u0394 W x W \u00b5 x x x W f (x) f (x) K SoftPlus+\u0394 K z K \u00b5 z z y Yv = (v 1 , v 2 , ..., v n ), the i-th window of the KPI is x (i) = (v i , v i+1 , ..., v i+W \u22121 ), where W denotes the window's length.\n2) Architecutre: The overall neural network architecture is shown in Fig. 3. As [21] suggests, we make the latent variable z independent of condition variable y. The z prior is chosen to be p(z|y) = p(z) = N (0, I). Both z and x posterior are chosen to be diagonal Gaussian distributions, i.e., q \u03c6 (z|x, y) = N (\u00b5 z , diag(\u03c3 2 z ))), p \u03b8 (x|z, y) = N (\u00b5 x , diag(\u03c3 2 x ))), where \u00b5 z , \u00b5 x , \u03c3 z , \u03c3 x denote the means and standard deviations of q \u03c6 (z|x, y) and p \u03b8 (x|z, y). It makes sense since we already assume that there are independent Gaussian noises on the KPIs. The hidden neural networks f \u03c6 (x) and f \u03b8 (z) are both several fully connected layers with ReLU [19] activation. They are used to extract hidden features from z or x for deriving Gaussian statistics. The Gaussian means are derived with a linear layer:\n\u00b5 z = W \u00b5 z f \u03c6 (x)+b \u00b5 z , \u00b5 x = W \u00b5 x f \u03b8 (z)+b \u00b5 x .\nThe standard deviations are derived by a softplus layer plus a positive real constant \u2206: \u03c3 z = ln(1 + exp(f \u03c6 (x))) + \u2206, \u03c3 x = ln(1 + exp(f \u03b8 (z))) + \u2206. Softplus gives very similar outputs with the common activation function ReLU [19], but the outputs are all strictly positive, which is required by standard deviation. The positive real constant \u2206 is used to avoid numeric problems, such as underflow.\n\n3) Encoding Time Information: The condition variable y represents the input window x's timestamp (to be precise, the latest point x W 's timestamp)\n\nTo emphasize the seasonality, the timestamp are decomposed into several parts: minute, hour, day of week, since user behavior schedule usually can be factorized to these basic units. We do not use seconds because the interval of KPIs is typically 1 minute or 5 minutes in our context. We do not use month or year since empirically there is no seasonality at these levels. Since neural networks is more sensitive to directions  Fig. 4. The way we construct condition variable for window x. The first line is the time of a window (specifically speaking, the timestamp of the latest point of the window). Then we use some components of the timestamp as the input condition for CVAE: the minute 25, the hour 16, the day of week: 2 (for Tuesday). We do not use second or month and year, because typically there is no seasonality of these levels in our data. Then we convert these three numbers into three one-hot encoded vectors and concatenate them. One-hot encoding means converting a positive integer to a binary vector full of zeros but only a single one, and the position of the single one represents the original integer's value. For example, a \"1\" value the 26 th element in the minute vector means minute = 25 in the timestamp.\n\nthan values [19], we choose to convert the decomposed values to one-hot vectors. Fig. 4 illustrates how Bagel encodes time information.\n\nEncoding time information can help Bagel deal with time information related anomalies. For example, in G, too many missing points makes it hard to reconstruct normal patterns from the sliding windows, but, with the help of time information, Bagel is less affected. In H, the spikes are not exactly the same every day, so the reconstruction may be biased. Since H are otherwise quite smooth except for those periodic spikes, i.e., the x standard deviation is fairly small, those little biases cause unreasonable high anomaly scores from Donut. With the help of time information, Bagel is not confused by these spikes.\n\nBecause fitting the relationship between timing and KPI value are easier (similar to traditional statistical model with seasonality, e.g., historical average) than fitting the relationship between x and reconstructed normal patterns, CVAE can be easily overfitted on time information for KPI anomaly detection. As will be demonstrated in \u00a7V-B, the performance of Bagel without dropout layer is really poor. This is, without dropout layer, the model pays too much attention to the relationship between time and the normal patterns, i.e., it learns too much about the seasonality but too little about the local variations. However, as [11] points out, an anomaly detection algorithm cannot work well unless local variations are handled appropriately.\n\nTherefore, in Bagel, there is an input dropout layer for condition variable y, as shown in Fig. 3. Dropout [13] reduces the risk of overfitting by randomly disabling some connections in a neural network in training. A network with dropout layers can be considered as an ensemble of many smaller networks [13]. Since we notice that CVAE can be easily overfitted on y, we add an extra dropout layer after the condition input layer. The input dropout in Bagel is implemented by randomly setting p dropout radio of dimensions to zero. In the perspective of ensemble as [13], now Bagel becomes an ensemble model of many smaller models, which only take a small part of y as the input condition variable. Fitting the relationship between time information and normal patterns are considered easy, but fitting that between only a small part of time information and normal patterns will not be so easy, thus we avoid overfitting.\n\n4) Summary of improvement over Donut: Compared to the network architecture in Donut, Bagel's major improvement are three-fold:\n\n\u2022 Adoption of CVAE as opposed to VAE so that we can encode time information as the conditional variable. \u2022 Use one-hot encoding to encode time information in vectors. \u2022 Use dropout layer to avoid overfitting introduced by time information.\n\n\nB. Training\n\nThe SGVB algorithm [17], [18] is used for training CVAE. One key technique for SGVB is re-parameterization, which means in training z is produced by z = z \u00b7 \u03c3 z + \u00b5 z , z \u223c N (0, 1), rather than z \u223c N (\u00b5 z , diag(\u03c3 2 z )). It makes it possible to pass gradients to the approximated posterior q \u03c6 (z|x, y) and thus train it.\n\nThe CVAE model is supposed to capture the normal patterns of a KPI, thus it is necessary to avoid learning the abnormal patterns. [11] proposes a loss function called M-ELBO to ignore abnormal pattern for VAE and shows that M-ELBO is satisfying enough. A similar loss function can be applied to CVAE. For an input window x, assume that its corresponding label window is a binary vector \u03b1 (x i is abnormal if and only if \u03b1 i = 1), and \u03b2 denotes the proportion of normal points. Since the x posterior is assumed to be diagonal Gaussian distribution, its log-likelihood can be rewritten as log p \u03b8 (x|z, y) = W i=1 log p \u03b8 (x i |z, y). By multiplying \u03b1 i and log p \u03b8 (x i |z, y), the M-ELBO for CAVE can be formulated as follows:\nL(x, y) = E q \u03c6 (z|x,y) [ W i=1\n\u03b1i \u00b7 log p(xi|z, y) + \u03b2 \u00b7 log p(z|y) \u2212 log q \u03c6 (z|x, y))]\n\n(2) M-ELBO makes the CVAE model ignore the loss from abnormal points in a training window x. Though there may be only occasional labels, ignoring missing points can be very helpful [11]. Therefore it makes our model able to learn reconstructing normal patterns from potential abnormal windows. To take advantage of such an ability, abnormal data injection, which is some kind of data augmentation should be applied. However, since CAVE itself is a generative model with high capacity, using another simpler generative model to generate anomaly points seems unreasonable. Therefore only missing points are injected in our practice. Before each epoch, some points are randomly chosen and set to zero (or other imputed value for missing points).\n\nBagel's training design is similar to that of Donut, and the difference is that we need to introduce conditional variable y in probability functions in Bagel. We do not claim the Bagel's training design as our contribution.\n\n\nOrigin\n\nReconstructed MCMC Imputation \n\n\nC. Detection\n\nWe use the reconstruction term in ELBO (Eqn. (1)) as anomaly detector, i.e., E q \u03c6 (z|x,y) [log p \u03b8 (x|z, y)]. It is called \"reconstruction probability\" in [11], [16], while it is actually not a well-defined probability. As mentioned above, the CVAE model are supposed to learn reconstructing normal patterns from potential abnormal input window x. If a sample of z from q \u03c6 (z|y, x), z (i) , is corresponding to x's normal patterns (i.e., p \u03b8 (x|z (i) , y) gives x's normal patterns' distribution), then the log-likelihood log p \u03b8 (x|z (i) , y) represents how much x follows the normal patterns. Considering z (i) 's likelihood q \u03c6 (z (i) |x, y) as a weight, the negative weighted average \u2212 z (i) q \u03c6 (z (i) |x, y) log p \u03b8 (x|z (i) , y) becomes a reasonable anomaly score. Its expectation form is\n\u2212 E q \u03c6 (z|x,y) [log p \u03b8 (x|z, y)].\nThe abnormal points in the testing windows may have a bad influence on finding a good posterior. In order to reduce the bias introduced by missing points (we don't know which points are anomaly points), we take advantage of the trained generative CVAE model to impute the missing points in the testing windows with MCMC [17]. At each step, we feed a testing window x to the trained model, replace the missing points of x with the corresponding missing points in the reconstructed window, and keep the other points original. Such a step is repeated for L times. An illustration of MCMC imputation is given in Fig. 5 Bagel's design of detection is similar to that of Donut, and the difference is that we need to introduce conditional variable y in probability functions in Bagel. We do not claim the Bagel's detection design as our contribution.\n\n\nD. Improvement over Donut\n\nThe main differences from Donut are highlighted in Fig. 3 with double lines. Firstly, we adopt CVAE rather than VAE, so there is an input condition in both variational and generative networks. We also use sliding windows as Donut does, but with the help of the additional time condition, Bagel will not ignore the time information of the KPI windows. Secondly, to reduce the risk of overfitting, we add an additional dropout layer after the condition input layer.\n\nThese design differences make Bagel more robust than Donut when dealing with time information related anomalies. Later in \u00a7IV-A will show that Bagel outperforms Donut on KPIs such as KPIs H and G which have time information related anomalies, and \u00a7V will explain the results in detail.\n\nSince the condition variable actually do not affect the overall architecture a lot, Bagel re-uses some designs in Donut: we use the same preprocessing methods, and use M-ELBO and reconstruction probability with condition as Bagel's training objective and detector. We also adopt the missing data injection and MCMC imputation techniques from Donut as they are shown to be effective in [11]. We do not claim these are the contributions of Bagel.\n\n\nIV. EXPERIMENTS\n\nIn this section, we use real data from Internet companies to evaluate Bagel's performance and compare with some stateof-art algorithms.\n\n\nA. Evaluation Metrics\n\nA modified anomaly F1-score, which is proposed by [11], will be used as our evaluation metric. F1-score is usually adopted to evaluate classification problems, which takes both precision and recall into consideration. However, in KPI anomaly detection problem, the points to be classified are not independent and operators only cares about the times when anomalies start [11]. Therefore, if an anomaly detection algorithm raises an alert fast enough (i.e., before a maximum allowed delay) after an anomaly begins, the whole anomaly fragment will be considered detected successfully when calculating modified F1-score.A successful alert with huge delay is not useful at all. Thus, if for an anomaly segment in ground truth, an anomaly detection algorithm does not raise any alert before the maximum allowed delay, this whole anomaly segment in the ground truth is considered as false negative, even though the anomaly detection algorithm might raise some alerts after the maximum allowed delay.\n\nAn illustration of the metric is given in Fig. 6. For convenience, we will call this modified F1-score as just F1-score. Similar to [11], to show the best potential performance of the models, we use the best F1-score, which is computed with the threshold which acquires best performance on test set. In other words, we calculate the modified F1-scores on test set with all potential thresholds and use the best one as our evaluation metric.\n\n\nB. Datasets\n\nWe obtain several well-maintained KPIs from several large Internet companies. All the anomaly labels are manually confirmed by operators. The statistics of these KPIs are shown in Table II. A, B, C are similar to those in [11], so they can demonstrate Bagel's performance on those KPIs that Donut claims to handle well. G has many missing points and several long missing fragments (like that shown in Fig. 1, and there are several similar long missing fragments), such that many normal fragments are just small pieces surrounded by missing points. H is quite smooth, but has many periodic spikes every day. G and H are representive KPIs to demonstrate the effect of time information. Since we collect these datasets from  The maximum allowed delay is set to be 1 here (the vertical lines). In the first anomaly segment, there is an alert before the maximum allowed delay, so the whole fragment is considered to be detected successfully. In the second anomaly segment, the first alert occurs after the maximum allowed delay, so the whole fragment is not considered to be detected at all. different sources, they have different monitoring intervals and different number of data points. Fig. 1 plots these KPIs.\n\n\nC. Overall Performance on A, B, C\n\nWe compare Bagel's performance with that of Donut (which is the state-of-art unsupervised algorithm) and Opprentice (which is the state-of-art supervised algorithm and outperforms most traditional statistical algorithms [9]). All experiments are repeated for 10 times.\n\nIn the following experiments, we set latent space dimensions K = 8, missing data injection radio \u03bb = 0.01, dropout rate p dropout = 0.1. We use two fully-connected layers with 100 units in both variational and generative networks. Donut uses the same number of layers with the same number of units. Opprentice uses random forest (the number of components is 200 and the max depth is 6) to ensemble 129 traditional detectors. We set the maximum allowed delay in modified F1score to be 7 as in [11]. Bagel and Donut are both trained without any labels, and Opprentice need complete anomaly labels.\n\nAverage best F1-scores over different W are shown in Fig. 7. On datasets A, B, C, Bagel's performance is similar to that of Donut's, which means Bagel is also able to handle those KPIs that Donut is able to handle.\n\n\nD. Overall Performance on G, H\n\nIn Fig. 8, we compare the performance of Bagel, Donut and Opprentice on KPI G, H. For G, Bagel's average best F1-score is 0.08 higher than that of Donut's, and Opprentice has similar performance with Bagel. A comparison of the anomaly scores of G given by Donut and Bagel are given in Fig. 9. The small normal pieces surrounded by missing fragments (such as that shows in Fig. 9) is hard to reconstruct for Donut, because too many points are missing and Donut does not have enough information to reconstruct the normal pattern. Therefore Donut may give too high anomaly scores for a normal window, which will cause poor performance. With the help of the additional time information, Bagel is less affected by the large amount of missing points, since this KPI have very similar patterns at the same time every day and Bagel is supposed to find this fragment's normal pattern in these patterns. Opprentice also outperforms Donut on G, because Opprentice have many detectors that will not be affected by a large amount of missing points (e.g., the difference from the KPI value from the last season).\n\nBagel's average best F1-score on H is 0.43 more than that of Donut's. Fig. 10 shows that anomaly scores of H given by Donut and Bagel. Since H is very smooth at most points, the x's standard deviation will be quite small (nearly zero).  However, since the periodic spikes do not occur at exactly the same time every day, and do not have the same height every day, such spikes bring some extra bias for reconstruction. Small bias may also cause big impact on standard deviation since the standard deviation is too small on a mostly smooth KPI. Thanks to the time information, Bagel does not suffer so much from this. Although such spikes also bring extra bias for Bagel, the time information can reduce its impact by guiding the reconstruction. Donut finds normal patterns in all patterns that are similar to the input window, but Bagel will only find similar normal patterns in all patterns that have similar y. Note for a seasonal KPI, KPI windows with similar y are supposed to have similar patterns.\n\nDonut's performance in H is much worse than Opprentice because Opprentice has many detectors (thus features) and many of them are not affected by the small standard deviation. Bagel outperforms Opprentice a bit since CVAE has more capacity than simple statistical detectors.\n\n\nV. ANALYSIS\n\nIn this section, we explain in detail how Bagel's two important components work: conditional variable with time information, and dropout layer.\n\n\nA. Conditional KDE explanation\n\nThe effectiveness of M-ELBO, missing data injection and MCMC imputation for VAE have already been shown in [11]. As the additional input condition in CVAE does not change the network structure a lot, these three techniques can be easily applied to CVAE and they are supposed to work in the same way as in VAE in [11]. Generally speaking, M-ELBO and the dimension reduction in autoencoder structure makes Bagel be able to reconstruct normal patterns from a potential abnormal window, and missing data injection amplifies M-ELBO's effect, and MCMC imputation help the model find a good posterior in detection. As a result, since Bagel also uses reconstruction probability as the detector, Bagel still works in the KDE way, as interpreted in [11].\n\nThe difference between Bagel and Donut is that Bagel finds z posterior conditioned on time information. In the KDE perspective, Bagel uses kernels and weights conditioned on time information. We call the way Bagel works as conditional KDE.\n\nGiven a KPI window, its corresponding normal patterns is multimodal. For example, suppose that a script should be executed every day at 3:00pm, so there will be a peak in the KPI at 3:00pm every day. If someday this script fails to be executed, the corresponding KPI fragment will looks very similar to that of different time of day (e.g., 11:00am), where no scripts are executed. Without time information, we cannot tell if this fragment is normal (because it is similar to the normal pattern at 11:00am), or abnormal. However, with time information, we can confidently report that this fragment is abnormal since we know there is a peak at 3:00pm every day.\n\nTime information also helps when x is confusing. For example, in Fig. 9, there is a normal fragment surrounded by missing points. As this normal fragment is much shorter than the training windows used to train the model, Donut cannot determine its normal pattern and, therefore, gives wrong anomaly scores. But Bagel have additional time information, so it is able to reconstruct this fragment's normal pattern even if too many points are missing. Although Bagel also cannot reconstruct its normal pattern by the small piece of normal points, the time information guides Bagel to find normal pattern in those patterns that occur at similar time in a day, which is much easier and robust than find normal patterns in all possible patterns.\n\n\nB. Dropout for avoiding overfitting on time information\n\nModeling the relationship between latent variables and encoded timestamps is easier than that between latent variables and sliding windows, because the KPIs are mostly seasonal and the local variation is not so influential compared to the periodicity. Therefore CVAE model may be overfitted on time information easily. In order to avoid the risk of overfitting on timing, we use an extra dropout [13] layer after the condition variable's input layer (see Fig. 3).\n\nThe time gradient effect originally pointed out by [11] for Donut, in CVAE becomes \"z samples drawn from approximated z posterior q \u03c6 (z|x, y) with more different y should be . far away from each other\". This time gradient is important to find a good z posterior according to the analysis in [11]. If the z samples of two x windows following different patterns are not far away from each other but are fairly close, it will be much more likely to find wrong normal patterns.\n\nSome latent spaces are shown in Fig. 11, which draws the z samples: z \u223c q \u03c6 (z|x, y), x, y \u223c p data (x, y), and a point's color denotes its corresponding time in a day (counting in minutes, i.e., in [0, 1440)). We compare the latent spaces of our proposed model (with dropout layer), that without dropout layer, and a \"time only\" model, in which the shape information is ignored, i.e., q \u03c6 (z|x, y) = q \u03c6 (z|y) and there is no dropout layer.\n\nIn the latent spaces of \"time only\" models, z samples with different colors, i.e., different times and therefore different shapes, are completely mixed together. As for Bagel without dropout, the points with similar colors also get together somewhat, but dissimilar points are not far away enough like those in Bagel's latent space.\n\nIn the latent space of Bagel, the z samples form a circle, and along the circle the colors changes gradually, which means that points with different y are as far away as possible. Therefore, we conclude that dropout layer is helpful to learn a good z space topology, and therefore helpful to reconstruct normal patterns.\n\nSince the latent spaces of Bagel have significant time gradient, similar to that in Donut [11], Bagel has similar ability with Donut to reconstruct normal patterns from x, but Bagel has timing information successfully incorporated without overfitting.\n\n\nVI. RELATED WORK\n\nThis section briefly reviews three main directions in KPI anomaly detection: traditional statistical methods, supervised ensemble methods, and unsupervised learning based methods.\n\nMany traditional statistical methods are used for anomaly detection in the past few decades, such as [1], [3]- [9], [14], [15], [22]- [33]. These methods make some simple assumptions for the KPIs, therefore operators must take efforts to choose an appropriate algorithm and tune the hyper-paramters for each KPI and each service. Simple equations used in these models are often not able to capture the properties of the KPIs in practice. For example, [5] detects the anomalies in search response time KPI with WoW (week-over-week) method, but there are not only weekly periodicity but also daily periodicity, holidays and other factors. In such cases the combination of many different detectors seems necessary. However, simple ensemble of these statistical detectors, like majority vote [24] or normalization [30], does not help a lot according to [9].\n\nTo automatically combine detectors, some supervised ensemble methods, such as EDGAS [8] and Opprentice [9], were proposed. The philosophy under them are clear and simple: since simple combination of detectors is not enough, we can use actual data to model the best combination of detectors. They compute features with traditional statistical detectors firstly, and then train a supervised machine learning classifier model like random forest, with user feedbacks, typically anomaly labels. These supervised methods show excellent performance. For example, experiments in [9] show that Opprentice outperforms all traditional statistical methods. However, they heavily rely on careful labels. However, getting enough careful labels requires too much manual efforts and time, which makes such supervised methods expensive and impractical. Although there might be occasional anomaly labels, the coverage of these labels is usually far from the requirements of traditional supervised methods. Besides, these ensemble algorithms are time-consuming because they have to execute a lot of traditional statistical algorithms, some of which might be time-consuming.\n\nIn general, anomaly detection (not necessarily KPI anomaly detection) based on unsupervised machine learning, such as one-class SVM [14], [23], GMM [26], VAE [11], [16], [34] and VRNN [15], model the normal patterns with machine learning methodology and raise alerts for points that do not follow normal patterns (rather than doing classification between normal and abnormal like Opprentice [9]). Along this direction, Donut [11] is so far the only one that works on KPI anomaly detection, and it outperforms the state-of-art supervised ensemble approach Opprentince. Our experiments have shown that Bagel outperforms it in robustness.\n\n\nVII. CONCLUSION\n\nAnomaly detection for the vast number of KPIs in the web-based services require automatic approaches. Built upon a state-of-art unsupervised VAE-based approach Donut, this paper for the first time identifies Donut's inadequacy in dealing with the time information related anomalies, and proposes Bagel, a robust and unsupervised KPI anomaly detection algorithm that can handle time information related anomalies. Compared to Donut, Bagel greatly improves the robustness of deep generative models in KPI anomaly detection, by using CVAE to incorporate time information and dropout layer to avoid overfitting. Our experiments using real data from Internet companies show that, compared to Donut, Bagel improves the anomaly detection best F1-score by 0.08 to 0.43. Deep generative in general and VAE in specific are not sequential models. By successfully incorporating time information using CVAE to KPI anomaly problem, we believe we make an important step forward in making deep generative models work on more sequential scenarios.\n\nFig. 1 .\n1KPI studied in this paper. We plot 36 hours of A, B, C, H and 72 hours of G. The red lines denotes anomaly parts, and the orange lines denotes missing parts. The rest black lines are normal parts. A, B, C are very similar to those KPIs on which Donut works well.\n\nFig. 3 .\n3The overall neural network architecture. The double-lines highlight the major difference with Donut in network architecture.\n\nFig. 5 .\n5Illustration of a MCMC imputation step. Firstly, use the trained CVAE model to reconstruct the original input vector x, then replace the missing points with those corresponding points in the reconstructed vector.\n\nFig. 6 .\n6Illustration of the evaluation metric. The first line is the ground truth and the second line is the anomaly scores. With the threshold 0.5, the original point-wise alerts are given in the third line.\n\nFig. 7 .Fig. 8 .\n78Best F1-score with different window sizes on A, B, C. Bagel and Donut have very similar performance. The average F1-score of Bagel ranges from 0.7 to 0.8 on A, B, C in most settings. Best F1-score on G, H. On G, Bagel outperforms Donut by 0.08. On H, Bagel outperforms Donut by 0.43. Opprentice always get similar performance with Bagel on these two KPIs, but it is supervised algorithm while the others are unsupervised.\n\nFig. 9 .\n9Anomaly scores of G given by Donut and Bagel. The blue lines are KPI values and the red line marks the ground truth anomalies. The green lines are the anomaly scores for each point. Donut gives too high anomaly scores for the normal fragment surrounded by missing points.\n\nFig. 10 .\n10Anomaly scores of H given by Donut and Bagel. The blue lines are KPI values and the red line marks the ground truth anomalies. The green lines are the anomaly scores for each point. Donut gives too high anomaly scores at many normal valleys, which are mostly smooth but have many periodic spikes.\n\nFig. 11 .\n11Dropout's effect on latent spaces. This figure plots the 3-D latent spaces of G with different models. \"Bagel without dropout\" denotes our model without the dropout layer. \"Time only\" denotes a model use only time information in the encoder, i.e., q \u03c6 (z|x, y) = q \u03c6 (z|y)\n\nTABLE I COMPARISON\nIAMONG ANOMALY DETECTION METHODOLOGIESSuffers from \n1 \n2 \n3 \n4 \n5 \nBagel \nSelecting algorithm Yes \nNo \nSome No \nNo \nNo \nTuning parameters \nYes \nNo \nSome Some Some No \nRelying on labels \nNo \nYes \nNo \nNo \nNo \nNo \nPoor Capacity \nYes \nNo \nSome No \nNo \nNo \nHard to train \nNo \nNo \nSome Some Yes \nNo \nTime consuming \nSome Yes \nSome No \nNo \nNo \n1: traditional statistical method, e.g., time series decomposition \n\nTABLE II\nIISTATISTICS OF KPI DATASETS \n\nKPI \ntotal \nmissing \nanomaly \nanomaly \nmonitering \npoints \npoints \npoints \nfragments \ninterval \n\nA \n296460 \n1222/0.412% \n1213/0.409% \n51 \n1min \nB \n317522 \n1117/0.352% \n1979/0.623% \n49 \n1min \nC \n285120 \n304/0.107% \n4394/1.541% \n126 \n1min \nG \n24950 \n3032/12.152% \n365/1.463% \n31 \n5min \nH \n17568 \n0/0.000% \n103/0.586% \n6 \n5min \n\n\n\nFunnel: Assessing software changes in web-based services. S Zhang, Y Liu, D Pei, Y Chen, X Qu, S Tao, Z Zang, X Jing, M Feng, IEEE Transactions on Services Computing. 111S. Zhang, Y. Liu, D. Pei, Y. Chen, X. Qu, S. Tao, Z. Zang, X. Jing, and M. Feng, \"Funnel: Assessing software changes in web-based services,\" IEEE Transactions on Services Computing, vol. 11, no. 1, pp. 34-48, 2018.\n\nHotspot: Anomaly localization for additive kpis with multi-dimensional attributes. Y Sun, Y Zhao, Y Su, D Liu, X Nie, Y Meng, S Cheng, D Pei, S Zhang, X Qu, IEEE Access. 6Y. Sun, Y. Zhao, Y. Su, D. Liu, X. Nie, Y. Meng, S. Cheng, D. Pei, S. Zhang, X. Qu et al., \"Hotspot: Anomaly localization for additive kpis with multi-dimensional attributes,\" IEEE Access, vol. 6, pp. 10 909- 10 923, 2018.\n\nAdaptive kalman filtering for anomaly detection in software appliances. F Knorn, D J Leith, INFOCOM Workshops. IEEE. IEEEF. Knorn and D. J. Leith, \"Adaptive kalman filtering for anomaly detection in software appliances,\" in INFOCOM Workshops 2008, IEEE. IEEE, 2008, pp. 1-6.\n\nAnomaly detection in time series of graphs using arma processes. B Pincombe, Asor Bulletin. 244B. Pincombe, \"Anomaly detection in time series of graphs using arma processes,\" Asor Bulletin, vol. 24, no. 4, p. 2, 2005.\n\nA providerside view of web search response time. Y Chen, R Mahajan, B Sridharan, Z.-L Zhang, ACM SIGCOMM Computer Communication Review. 434ACMY. Chen, R. Mahajan, B. Sridharan, and Z.-L. Zhang, \"A provider- side view of web search response time,\" in ACM SIGCOMM Computer Communication Review, vol. 43, no. 4. ACM, 2013, pp. 243-254.\n\nArgus: End-to-end service anomaly detection and localization from an isp's point of view. H Yan, A Flavel, Z Ge, A Gerber, D Massey, C Papadopoulos, H Shah, J Yates, INFOCOM. H. Yan, A. Flavel, Z. Ge, A. Gerber, D. Massey, C. Papadopoulos, H. Shah, and J. Yates, \"Argus: End-to-end service anomaly detection and localization from an isp's point of view,\" in INFOCOM, 2012 Proceedings IEEE. IEEE, 2012, pp. 2756-2760.\n\nNetwork anomaly detection based on wavelet analysis. W Lu, A A Ghorbani, EURASIP Journal on Advances in Signal Processing. 4W. Lu and A. A. Ghorbani, \"Network anomaly detection based on wavelet analysis,\" EURASIP Journal on Advances in Signal Processing, vol. 2009, p. 4, 2009.\n\nGeneric and scalable framework for automated time-series anomaly detection. N Laptev, S Amizadeh, I Flint, Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data MiningACMN. Laptev, S. Amizadeh, and I. Flint, \"Generic and scalable framework for automated time-series anomaly detection,\" in Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2015, pp. 1939-1947.\n\nOpprentice: Towards practical and automatic anomaly detection through machine learning. D Liu, Y Zhao, H Xu, Y Sun, D Pei, J Luo, X Jing, M Feng, Proceedings of the 2015 Internet Measurement Conference. the 2015 Internet Measurement ConferenceACMD. Liu, Y. Zhao, H. Xu, Y. Sun, D. Pei, J. Luo, X. Jing, and M. Feng, \"Opprentice: Towards practical and automatic anomaly de- tection through machine learning,\" in Proceedings of the 2015 Internet Measurement Conference. ACM, 2015, pp. 211-224.\n\nAn anomaly detection approach based on isolation forest algorithm for streaming data using sliding window. Z Ding, M Fei, IFAC Proceedings Volumes. 46Z. Ding and M. Fei, \"An anomaly detection approach based on isola- tion forest algorithm for streaming data using sliding window,\" IFAC Proceedings Volumes, vol. 46, no. 20, pp. 12-17, 2013.\n\nUnsupervised anomaly detection via variational autoencoder for seasonal kpis in web applications. H Xu, W Chen, N Zhao, Z Li, J Bu, Z Li, Y Liu, Y Zhao, D Pei, Y Feng, Proceedings of the 2018 World Wide Web Conference on World Wide Web. International World Wide Web Conferences Steering Committee. the 2018 World Wide Web Conference on World Wide Web. International World Wide Web Conferences Steering CommitteeH. Xu, W. Chen, N. Zhao, Z. Li, J. Bu, Z. Li, Y. Liu, Y. Zhao, D. Pei, Y. Feng et al., \"Unsupervised anomaly detection via variational auto- encoder for seasonal kpis in web applications,\" in Proceedings of the 2018 World Wide Web Conference on World Wide Web. International World Wide Web Conferences Steering Committee, 2018, pp. 187-196.\n\nRapid deployment of anomaly detection models for large number of emerging kpi streams. J Bu, Y Liu, S Zhang, W Meng, Q Liu, X Zhu, D Pei, 2018 IEEE 36th International Performance Computing and Communications Conference (IPCCC). IEEEJ. Bu, Y. Liu, S. Zhang, W. Meng, Q. Liu, X. Zhu, and D. Pei, \"Rapid deployment of anomaly detection models for large number of emerging kpi streams,\" in 2018 IEEE 36th International Performance Computing and Communications Conference (IPCCC). IEEE, 2018, pp. 1-8.\n\nDropout: a simple way to prevent neural networks from overfitting. N Srivastava, G Hinton, A Krizhevsky, I Sutskever, R Salakhutdinov, The Journal of Machine Learning Research. 151N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhut- dinov, \"Dropout: a simple way to prevent neural networks from over- fitting,\" The Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929-1958, 2014.\n\nEnhancing one-class support vector machines for unsupervised anomaly detection. M Amer, M Goldstein, S Abdennadher, Proceedings of the ACM SIGKDD Workshop on Outlier Detection and Description. the ACM SIGKDD Workshop on Outlier Detection and DescriptionACMM. Amer, M. Goldstein, and S. Abdennadher, \"Enhancing one-class support vector machines for unsupervised anomaly detection,\" in Pro- ceedings of the ACM SIGKDD Workshop on Outlier Detection and Description. ACM, 2013, pp. 8-15.\n\nVariational inference for on-line anomaly detection in high-dimensional time series. M S\u00f6lch, J Bayer, M Ludersdorfer, P Van Der, Smagt, stat. 105023M. S\u00f6lch, J. Bayer, M. Ludersdorfer, and P. van der Smagt, \"Variational inference for on-line anomaly detection in high-dimensional time series,\" stat, vol. 1050, p. 23, 2016.\n\nVariational autoencoder based anomaly detection using reconstruction probability. J An, S Cho, Special Lecture on IE. 2J. An and S. Cho, \"Variational autoencoder based anomaly detection using reconstruction probability,\" Special Lecture on IE, vol. 2, pp. 1- 18, 2015.\n\nStochastic backpropagation and approximate inference in deep generative models. D J Rezende, S Mohamed, D Wierstra, Proceedings of the 31st International Conference on International Conference on Machine Learning. the 31st International Conference on International Conference on Machine Learning321278D. J. Rezende, S. Mohamed, and D. Wierstra, \"Stochastic backpropaga- tion and approximate inference in deep generative models,\" in Proceed- ings of the 31st International Conference on International Conference on Machine Learning-Volume 32. JMLR. org, 2014, pp. II-1278.\n\nAuto-encoding variational bayes. D P Kingma, M Welling, stat. 10501D. P. Kingma and M. Welling, \"Auto-encoding variational bayes,\" stat, vol. 1050, p. 1, 2014.\n\nDeep Learning. I Goodfellow, Y Bengio, A Courville, MIT PressI. Goodfellow, Y. Bengio, and A. Courville, Deep Learning. MIT Press, 2016, http://www.deeplearningbook.org.\n\nSemisupervised learning with deep generative models. D P Kingma, S Mohamed, D J Rezende, M Welling, Advances in Neural Information Processing Systems. D. P. Kingma, S. Mohamed, D. J. Rezende, and M. Welling, \"Semi- supervised learning with deep generative models,\" in Advances in Neural Information Processing Systems, 2014, pp. 3581-3589.\n\nLearning structured output representation using deep conditional generative models. K Sohn, H Lee, X Yan, Advances in Neural Information Processing Systems. K. Sohn, H. Lee, and X. Yan, \"Learning structured output representation using deep conditional generative models,\" in Advances in Neural Information Processing Systems, 2015, pp. 3483-3491.\n\nAnomaly detection: A survey. V Chandola, A Banerjee, V Kumar, ACM computing surveys (CSUR). 41315V. Chandola, A. Banerjee, and V. Kumar, \"Anomaly detection: A survey,\" ACM computing surveys (CSUR), vol. 41, no. 3, p. 15, 2009.\n\nHighdimensional and large-scale anomaly detection using a linear one-class svm with deep learning. S M Erfani, S Rajasegarar, S Karunasekera, C Leckie, Pattern Recognition. 58S. M. Erfani, S. Rajasegarar, S. Karunasekera, and C. Leckie, \"High- dimensional and large-scale anomaly detection using a linear one-class svm with deep learning,\" Pattern Recognition, vol. 58, pp. 121-134, 2016.\n\nMawilab: combining diverse anomaly detectors for automated anomaly labeling and performance benchmarking. R Fontugne, P Borgnat, P Abry, K Fukuda, Proceedings of the 6th International COnference. the 6th International COnferenceACM8R. Fontugne, P. Borgnat, P. Abry, and K. Fukuda, \"Mawilab: com- bining diverse anomaly detectors for automated anomaly labeling and performance benchmarking,\" in Proceedings of the 6th International COnference. ACM, 2010, p. 8.\n\nSketch-based change detection: methods, evaluation, and applications. B Krishnamurthy, S Sen, Y Zhang, Y Chen, Proceedings of the 3rd ACM SIGCOMM conference on Internet measurement. the 3rd ACM SIGCOMM conference on Internet measurementACMB. Krishnamurthy, S. Sen, Y. Zhang, and Y. Chen, \"Sketch-based change detection: methods, evaluation, and applications,\" in Proceedings of the 3rd ACM SIGCOMM conference on Internet measurement. ACM, 2003, pp. 234-247.\n\nAnomaly detection in sea traffic-a comparison of the gaussian mixture model and the kernel density estimator. R Laxhammar, G Falkman, E Sviestins, Information Fusion, 2009. FUSION'09. 12th International Conference on. IEEER. Laxhammar, G. Falkman, and E. Sviestins, \"Anomaly detection in sea traffic-a comparison of the gaussian mixture model and the kernel density estimator,\" in Information Fusion, 2009. FUSION'09. 12th International Conference on. IEEE, 2009, pp. 756-763.\n\nThreshold compression for 3g scalable monitoring. S.-B Lee, D Pei, M Hajiaghayi, I Pefkianakis, S Lu, H Yan, Z Ge, J Yates, M Kosseifi, INFOCOM. S.-B. Lee, D. Pei, M. Hajiaghayi, I. Pefkianakis, S. Lu, H. Yan, Z. Ge, J. Yates, and M. Kosseifi, \"Threshold compression for 3g scalable monitoring,\" in INFOCOM, 2012 Proceedings IEEE. IEEE, 2012, pp. 1350-1358.\n\nRapid detection of maintenance induced changes in service performance. A Mahimkar, Z Ge, J Wang, J Yates, Y Zhang, J Emmons, B Huntley, M Stockert, Proceedings of the Seventh COnference on emerging Networking EXperiments and Technologies. the Seventh COnference on emerging Networking EXperiments and TechnologiesACM13A. Mahimkar, Z. Ge, J. Wang, J. Yates, Y. Zhang, J. Emmons, B. Hunt- ley, and M. Stockert, \"Rapid detection of maintenance induced changes in service performance,\" in Proceedings of the Seventh COnference on emerging Networking EXperiments and Technologies. ACM, 2011, p. 13.\n\nOne-class classification for anomaly detection with kernel density estimation and genetic programming. M Nicolau, J Mcdermott, European Conference on Genetic Programming. SpringerM. Nicolau, J. McDermott et al., \"One-class classification for anomaly detection with kernel density estimation and genetic programming,\" in European Conference on Genetic Programming. Springer, 2016, pp. 3-18.\n\nAccurate anomaly detection through parallelism. S Shanbhag, T Wolf, IEEE network. 231S. Shanbhag and T. Wolf, \"Accurate anomaly detection through paral- lelism,\" IEEE network, vol. 23, no. 1, pp. 22-28, 2009.\n\nArima based network anomaly detection. A H Yaacob, I K Tan, S F Chien, H K Tan, Communication Software and Networks, 2010. ICCSN'10. Second International Conference on. IEEEA. H. Yaacob, I. K. Tan, S. F. Chien, and H. K. Tan, \"Arima based network anomaly detection,\" in Communication Software and Networks, 2010. ICCSN'10. Second International Conference on. IEEE, 2010, pp. 205-209.\n\nRapid and robust impact assessment of software changes in large internet-based services. S Zhang, Y Liu, D Pei, Y Chen, X Qu, S Tao, Z Zang, Proceedings of the 11th ACM Conference on Emerging Networking Experiments and Technologies. the 11th ACM Conference on Emerging Networking Experiments and TechnologiesACM2S. Zhang, Y. Liu, D. Pei, Y. Chen, X. Qu, S. Tao, and Z. Zang, \"Rapid and robust impact assessment of software changes in large internet-based services,\" in Proceedings of the 11th ACM Conference on Emerging Networking Experiments and Technologies. ACM, 2015, p. 2.\n\nRobust and rapid adaption for concept drift in software system anomaly detection. M Ma, S Zhang, D Pei, X Huang, H Dai, The 29th IEEE International Symposium on Software Reliability Engineering. IEEEM. Ma, S. Zhang, D. Pei, X. Huang, and H. Dai, \"Robust and rapid adaption for concept drift in software system anomaly detection,\" in The 29th IEEE International Symposium on Software Reliability Engineering (ISSRE 2018). IEEE, 2018, pp. 1-12.\n\nDeep autoencoding gaussian mixture model for unsupervised anomaly detection. B Zong, Q Song, M R Min, W Cheng, C Lumezanu, D Cho, H Chen, B. Zong, Q. Song, M. R. Min, W. Cheng, C. Lumezanu, D. Cho, and H. Chen, \"Deep autoencoding gaussian mixture model for unsupervised anomaly detection,\" 2018.\n", "annotations": {"author": "[{\"end\":185,\"start\":94},{\"end\":281,\"start\":186},{\"end\":395,\"start\":282},{\"end\":418,\"start\":396}]", "publisher": null, "author_last_name": "[{\"end\":102,\"start\":100},{\"end\":198,\"start\":194},{\"end\":289,\"start\":286}]", "author_first_name": "[{\"end\":99,\"start\":94},{\"end\":193,\"start\":186},{\"end\":285,\"start\":282}]", "author_affiliation": "[{\"end\":184,\"start\":104},{\"end\":280,\"start\":200},{\"end\":394,\"start\":314},{\"end\":417,\"start\":397}]", "title": "[{\"end\":91,\"start\":1},{\"end\":509,\"start\":419}]", "venue": null, "abstract": "[{\"end\":1639,\"start\":511}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2071,\"start\":2068},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2076,\"start\":2073},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2205,\"start\":2202},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2211,\"start\":2207},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2268,\"start\":2264},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2393,\"start\":2390},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2398,\"start\":2395},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2540,\"start\":2537},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2545,\"start\":2542},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2983,\"start\":2979},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3663,\"start\":3660},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3879,\"start\":3875},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4907,\"start\":4904},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4959,\"start\":4956},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5020,\"start\":5016},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5073,\"start\":5069},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5128,\"start\":5124},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5134,\"start\":5130},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5485,\"start\":5481},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6307,\"start\":6303},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7492,\"start\":7488},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7728,\"start\":7724},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9281,\"start\":9277},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9287,\"start\":9283},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9392,\"start\":9388},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9820,\"start\":9816},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10078,\"start\":10074},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10084,\"start\":10080},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10826,\"start\":10822},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11121,\"start\":11117},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":11281,\"start\":11277},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11287,\"start\":11283},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":11555,\"start\":11551},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":12982,\"start\":12978},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":13572,\"start\":13568},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":14014,\"start\":14010},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":15581,\"start\":15577},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":16957,\"start\":16953},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":17181,\"start\":17177},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":17378,\"start\":17374},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":17639,\"start\":17635},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":18397,\"start\":18393},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":18403,\"start\":18399},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":18833,\"start\":18829},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":19702,\"start\":19698},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":20702,\"start\":20698},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":20708,\"start\":20704},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":21700,\"start\":21696},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":23390,\"start\":23386},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":23680,\"start\":23676},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":24001,\"start\":23997},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":24757,\"start\":24753},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":25303,\"start\":25299},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":26546,\"start\":26543},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":27089,\"start\":27085},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":30122,\"start\":30118},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":30327,\"start\":30323},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":30754,\"start\":30750},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":32857,\"start\":32853},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":32977,\"start\":32973},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":33218,\"start\":33214},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":34591,\"start\":34587},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":35054,\"start\":35051},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":35059,\"start\":35056},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":35064,\"start\":35061},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":35070,\"start\":35066},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":35076,\"start\":35072},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":35082,\"start\":35078},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":35088,\"start\":35084},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":35404,\"start\":35401},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":35742,\"start\":35738},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":35764,\"start\":35760},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":35802,\"start\":35799},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":35892,\"start\":35889},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":35911,\"start\":35908},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":36379,\"start\":36376},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":37097,\"start\":37093},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":37103,\"start\":37099},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":37113,\"start\":37109},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":37123,\"start\":37119},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":37129,\"start\":37125},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":37135,\"start\":37131},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":37149,\"start\":37145},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":37355,\"start\":37352},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":37390,\"start\":37386}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":38920,\"start\":38647},{\"attributes\":{\"id\":\"fig_1\"},\"end\":39056,\"start\":38921},{\"attributes\":{\"id\":\"fig_2\"},\"end\":39280,\"start\":39057},{\"attributes\":{\"id\":\"fig_4\"},\"end\":39492,\"start\":39281},{\"attributes\":{\"id\":\"fig_5\"},\"end\":39934,\"start\":39493},{\"attributes\":{\"id\":\"fig_6\"},\"end\":40217,\"start\":39935},{\"attributes\":{\"id\":\"fig_7\"},\"end\":40527,\"start\":40218},{\"attributes\":{\"id\":\"fig_8\"},\"end\":40813,\"start\":40528},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":41238,\"start\":40814},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":41606,\"start\":41239}]", "paragraph": "[{\"end\":2137,\"start\":1658},{\"end\":2772,\"start\":2139},{\"end\":3541,\"start\":2774},{\"end\":4841,\"start\":3543},{\"end\":5487,\"start\":4843},{\"end\":6308,\"start\":5489},{\"end\":6371,\"start\":6310},{\"end\":6994,\"start\":6373},{\"end\":7366,\"start\":6996},{\"end\":8042,\"start\":7432},{\"end\":8294,\"start\":8044},{\"end\":8843,\"start\":8296},{\"end\":9047,\"start\":8845},{\"end\":9598,\"start\":9118},{\"end\":9995,\"start\":9600},{\"end\":10602,\"start\":9997},{\"end\":10694,\"start\":10604},{\"end\":11595,\"start\":10696},{\"end\":11789,\"start\":11732},{\"end\":12019,\"start\":11811},{\"end\":12650,\"start\":12021},{\"end\":13723,\"start\":12898},{\"end\":14182,\"start\":13780},{\"end\":14331,\"start\":14184},{\"end\":15563,\"start\":14333},{\"end\":15700,\"start\":15565},{\"end\":16318,\"start\":15702},{\"end\":17068,\"start\":16320},{\"end\":17989,\"start\":17070},{\"end\":18117,\"start\":17991},{\"end\":18358,\"start\":18119},{\"end\":18697,\"start\":18374},{\"end\":19425,\"start\":18699},{\"end\":19515,\"start\":19458},{\"end\":20259,\"start\":19517},{\"end\":20484,\"start\":20261},{\"end\":20525,\"start\":20495},{\"end\":21339,\"start\":20542},{\"end\":22219,\"start\":21376},{\"end\":22712,\"start\":22249},{\"end\":22999,\"start\":22714},{\"end\":23445,\"start\":23001},{\"end\":23600,\"start\":23465},{\"end\":24619,\"start\":23626},{\"end\":25061,\"start\":24621},{\"end\":26285,\"start\":25077},{\"end\":26591,\"start\":26323},{\"end\":27188,\"start\":26593},{\"end\":27404,\"start\":27190},{\"end\":28537,\"start\":27439},{\"end\":29541,\"start\":28539},{\"end\":29817,\"start\":29543},{\"end\":29976,\"start\":29833},{\"end\":30755,\"start\":30011},{\"end\":30996,\"start\":30757},{\"end\":31657,\"start\":30998},{\"end\":32397,\"start\":31659},{\"end\":32920,\"start\":32457},{\"end\":33396,\"start\":32922},{\"end\":33839,\"start\":33398},{\"end\":34173,\"start\":33841},{\"end\":34495,\"start\":34175},{\"end\":34748,\"start\":34497},{\"end\":34948,\"start\":34769},{\"end\":35803,\"start\":34950},{\"end\":36959,\"start\":35805},{\"end\":37596,\"start\":36961},{\"end\":38646,\"start\":37616}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11731,\"start\":11596},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12762,\"start\":12651},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12897,\"start\":12762},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13779,\"start\":13724},{\"attributes\":{\"id\":\"formula_4\"},\"end\":19457,\"start\":19426},{\"attributes\":{\"id\":\"formula_5\"},\"end\":21375,\"start\":21340}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":2310,\"start\":2303},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":25265,\"start\":25257}]", "section_header": "[{\"end\":1656,\"start\":1641},{\"end\":7395,\"start\":7369},{\"end\":7430,\"start\":7398},{\"end\":9116,\"start\":9050},{\"end\":11809,\"start\":11792},{\"end\":18372,\"start\":18361},{\"end\":20493,\"start\":20487},{\"end\":20540,\"start\":20528},{\"end\":22247,\"start\":22222},{\"end\":23463,\"start\":23448},{\"end\":23624,\"start\":23603},{\"end\":25075,\"start\":25064},{\"end\":26321,\"start\":26288},{\"end\":27437,\"start\":27407},{\"end\":29831,\"start\":29820},{\"end\":30009,\"start\":29979},{\"end\":32455,\"start\":32400},{\"end\":34767,\"start\":34751},{\"end\":37614,\"start\":37599},{\"end\":38656,\"start\":38648},{\"end\":38930,\"start\":38922},{\"end\":39066,\"start\":39058},{\"end\":39290,\"start\":39282},{\"end\":39510,\"start\":39494},{\"end\":39944,\"start\":39936},{\"end\":40228,\"start\":40219},{\"end\":40538,\"start\":40529},{\"end\":40833,\"start\":40815},{\"end\":41248,\"start\":41240}]", "table": "[{\"end\":41238,\"start\":40872},{\"end\":41606,\"start\":41251}]", "figure_caption": "[{\"end\":38920,\"start\":38658},{\"end\":39056,\"start\":38932},{\"end\":39280,\"start\":39068},{\"end\":39492,\"start\":39292},{\"end\":39934,\"start\":39513},{\"end\":40217,\"start\":39946},{\"end\":40527,\"start\":40231},{\"end\":40813,\"start\":40541},{\"end\":40872,\"start\":40835}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":1943,\"start\":1937},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3707,\"start\":3701},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4314,\"start\":4308},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4731,\"start\":4725},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5382,\"start\":5376},{\"end\":10354,\"start\":10348},{\"end\":11788,\"start\":11782},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12973,\"start\":12967},{\"end\":14766,\"start\":14760},{\"end\":15652,\"start\":15646},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":17167,\"start\":17161},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21990,\"start\":21984},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22306,\"start\":22300},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":24669,\"start\":24663},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":25484,\"start\":25478},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":26267,\"start\":26261},{\"end\":27249,\"start\":27243},{\"end\":27448,\"start\":27442},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":27730,\"start\":27724},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":27817,\"start\":27811},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":28616,\"start\":28609},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":31730,\"start\":31724},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":32918,\"start\":32912},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":33437,\"start\":33430}]", "bib_author_first_name": "[{\"end\":41667,\"start\":41666},{\"end\":41676,\"start\":41675},{\"end\":41683,\"start\":41682},{\"end\":41690,\"start\":41689},{\"end\":41698,\"start\":41697},{\"end\":41704,\"start\":41703},{\"end\":41711,\"start\":41710},{\"end\":41719,\"start\":41718},{\"end\":41727,\"start\":41726},{\"end\":42078,\"start\":42077},{\"end\":42085,\"start\":42084},{\"end\":42093,\"start\":42092},{\"end\":42099,\"start\":42098},{\"end\":42106,\"start\":42105},{\"end\":42113,\"start\":42112},{\"end\":42121,\"start\":42120},{\"end\":42130,\"start\":42129},{\"end\":42137,\"start\":42136},{\"end\":42146,\"start\":42145},{\"end\":42462,\"start\":42461},{\"end\":42471,\"start\":42470},{\"end\":42473,\"start\":42472},{\"end\":42731,\"start\":42730},{\"end\":42934,\"start\":42933},{\"end\":42942,\"start\":42941},{\"end\":42953,\"start\":42952},{\"end\":42969,\"start\":42965},{\"end\":43309,\"start\":43308},{\"end\":43316,\"start\":43315},{\"end\":43326,\"start\":43325},{\"end\":43332,\"start\":43331},{\"end\":43342,\"start\":43341},{\"end\":43352,\"start\":43351},{\"end\":43368,\"start\":43367},{\"end\":43376,\"start\":43375},{\"end\":43690,\"start\":43689},{\"end\":43696,\"start\":43695},{\"end\":43698,\"start\":43697},{\"end\":43992,\"start\":43991},{\"end\":44002,\"start\":44001},{\"end\":44014,\"start\":44013},{\"end\":44543,\"start\":44542},{\"end\":44550,\"start\":44549},{\"end\":44558,\"start\":44557},{\"end\":44564,\"start\":44563},{\"end\":44571,\"start\":44570},{\"end\":44578,\"start\":44577},{\"end\":44585,\"start\":44584},{\"end\":44593,\"start\":44592},{\"end\":45055,\"start\":45054},{\"end\":45063,\"start\":45062},{\"end\":45388,\"start\":45387},{\"end\":45394,\"start\":45393},{\"end\":45402,\"start\":45401},{\"end\":45410,\"start\":45409},{\"end\":45416,\"start\":45415},{\"end\":45422,\"start\":45421},{\"end\":45428,\"start\":45427},{\"end\":45435,\"start\":45434},{\"end\":45443,\"start\":45442},{\"end\":45450,\"start\":45449},{\"end\":46130,\"start\":46129},{\"end\":46136,\"start\":46135},{\"end\":46143,\"start\":46142},{\"end\":46152,\"start\":46151},{\"end\":46160,\"start\":46159},{\"end\":46167,\"start\":46166},{\"end\":46174,\"start\":46173},{\"end\":46608,\"start\":46607},{\"end\":46622,\"start\":46621},{\"end\":46632,\"start\":46631},{\"end\":46646,\"start\":46645},{\"end\":46659,\"start\":46658},{\"end\":47031,\"start\":47030},{\"end\":47039,\"start\":47038},{\"end\":47052,\"start\":47051},{\"end\":47521,\"start\":47520},{\"end\":47530,\"start\":47529},{\"end\":47539,\"start\":47538},{\"end\":47555,\"start\":47554},{\"end\":47844,\"start\":47843},{\"end\":47850,\"start\":47849},{\"end\":48112,\"start\":48111},{\"end\":48114,\"start\":48113},{\"end\":48125,\"start\":48124},{\"end\":48136,\"start\":48135},{\"end\":48638,\"start\":48637},{\"end\":48640,\"start\":48639},{\"end\":48650,\"start\":48649},{\"end\":48781,\"start\":48780},{\"end\":48795,\"start\":48794},{\"end\":48805,\"start\":48804},{\"end\":48990,\"start\":48989},{\"end\":48992,\"start\":48991},{\"end\":49002,\"start\":49001},{\"end\":49013,\"start\":49012},{\"end\":49015,\"start\":49014},{\"end\":49026,\"start\":49025},{\"end\":49362,\"start\":49361},{\"end\":49370,\"start\":49369},{\"end\":49377,\"start\":49376},{\"end\":49655,\"start\":49654},{\"end\":49667,\"start\":49666},{\"end\":49679,\"start\":49678},{\"end\":49953,\"start\":49952},{\"end\":49955,\"start\":49954},{\"end\":49965,\"start\":49964},{\"end\":49980,\"start\":49979},{\"end\":49996,\"start\":49995},{\"end\":50350,\"start\":50349},{\"end\":50362,\"start\":50361},{\"end\":50373,\"start\":50372},{\"end\":50381,\"start\":50380},{\"end\":50775,\"start\":50774},{\"end\":50792,\"start\":50791},{\"end\":50799,\"start\":50798},{\"end\":50808,\"start\":50807},{\"end\":51274,\"start\":51273},{\"end\":51287,\"start\":51286},{\"end\":51298,\"start\":51297},{\"end\":51695,\"start\":51691},{\"end\":51702,\"start\":51701},{\"end\":51709,\"start\":51708},{\"end\":51723,\"start\":51722},{\"end\":51738,\"start\":51737},{\"end\":51744,\"start\":51743},{\"end\":51751,\"start\":51750},{\"end\":51757,\"start\":51756},{\"end\":51766,\"start\":51765},{\"end\":52072,\"start\":52071},{\"end\":52084,\"start\":52083},{\"end\":52090,\"start\":52089},{\"end\":52098,\"start\":52097},{\"end\":52107,\"start\":52106},{\"end\":52116,\"start\":52115},{\"end\":52126,\"start\":52125},{\"end\":52137,\"start\":52136},{\"end\":52699,\"start\":52698},{\"end\":52710,\"start\":52709},{\"end\":53035,\"start\":53034},{\"end\":53047,\"start\":53046},{\"end\":53236,\"start\":53235},{\"end\":53238,\"start\":53237},{\"end\":53248,\"start\":53247},{\"end\":53250,\"start\":53249},{\"end\":53257,\"start\":53256},{\"end\":53259,\"start\":53258},{\"end\":53268,\"start\":53267},{\"end\":53270,\"start\":53269},{\"end\":53671,\"start\":53670},{\"end\":53680,\"start\":53679},{\"end\":53687,\"start\":53686},{\"end\":53694,\"start\":53693},{\"end\":53702,\"start\":53701},{\"end\":53708,\"start\":53707},{\"end\":53715,\"start\":53714},{\"end\":54243,\"start\":54242},{\"end\":54249,\"start\":54248},{\"end\":54258,\"start\":54257},{\"end\":54265,\"start\":54264},{\"end\":54274,\"start\":54273},{\"end\":54682,\"start\":54681},{\"end\":54690,\"start\":54689},{\"end\":54698,\"start\":54697},{\"end\":54700,\"start\":54699},{\"end\":54707,\"start\":54706},{\"end\":54716,\"start\":54715},{\"end\":54728,\"start\":54727},{\"end\":54735,\"start\":54734}]", "bib_author_last_name": "[{\"end\":41673,\"start\":41668},{\"end\":41680,\"start\":41677},{\"end\":41687,\"start\":41684},{\"end\":41695,\"start\":41691},{\"end\":41701,\"start\":41699},{\"end\":41708,\"start\":41705},{\"end\":41716,\"start\":41712},{\"end\":41724,\"start\":41720},{\"end\":41732,\"start\":41728},{\"end\":42082,\"start\":42079},{\"end\":42090,\"start\":42086},{\"end\":42096,\"start\":42094},{\"end\":42103,\"start\":42100},{\"end\":42110,\"start\":42107},{\"end\":42118,\"start\":42114},{\"end\":42127,\"start\":42122},{\"end\":42134,\"start\":42131},{\"end\":42143,\"start\":42138},{\"end\":42149,\"start\":42147},{\"end\":42468,\"start\":42463},{\"end\":42479,\"start\":42474},{\"end\":42740,\"start\":42732},{\"end\":42939,\"start\":42935},{\"end\":42950,\"start\":42943},{\"end\":42963,\"start\":42954},{\"end\":42975,\"start\":42970},{\"end\":43313,\"start\":43310},{\"end\":43323,\"start\":43317},{\"end\":43329,\"start\":43327},{\"end\":43339,\"start\":43333},{\"end\":43349,\"start\":43343},{\"end\":43365,\"start\":43353},{\"end\":43373,\"start\":43369},{\"end\":43382,\"start\":43377},{\"end\":43693,\"start\":43691},{\"end\":43707,\"start\":43699},{\"end\":43999,\"start\":43993},{\"end\":44011,\"start\":44003},{\"end\":44020,\"start\":44015},{\"end\":44547,\"start\":44544},{\"end\":44555,\"start\":44551},{\"end\":44561,\"start\":44559},{\"end\":44568,\"start\":44565},{\"end\":44575,\"start\":44572},{\"end\":44582,\"start\":44579},{\"end\":44590,\"start\":44586},{\"end\":44598,\"start\":44594},{\"end\":45060,\"start\":45056},{\"end\":45067,\"start\":45064},{\"end\":45391,\"start\":45389},{\"end\":45399,\"start\":45395},{\"end\":45407,\"start\":45403},{\"end\":45413,\"start\":45411},{\"end\":45419,\"start\":45417},{\"end\":45425,\"start\":45423},{\"end\":45432,\"start\":45429},{\"end\":45440,\"start\":45436},{\"end\":45447,\"start\":45444},{\"end\":45455,\"start\":45451},{\"end\":46133,\"start\":46131},{\"end\":46140,\"start\":46137},{\"end\":46149,\"start\":46144},{\"end\":46157,\"start\":46153},{\"end\":46164,\"start\":46161},{\"end\":46171,\"start\":46168},{\"end\":46178,\"start\":46175},{\"end\":46619,\"start\":46609},{\"end\":46629,\"start\":46623},{\"end\":46643,\"start\":46633},{\"end\":46656,\"start\":46647},{\"end\":46673,\"start\":46660},{\"end\":47036,\"start\":47032},{\"end\":47049,\"start\":47040},{\"end\":47064,\"start\":47053},{\"end\":47527,\"start\":47522},{\"end\":47536,\"start\":47531},{\"end\":47552,\"start\":47540},{\"end\":47563,\"start\":47556},{\"end\":47570,\"start\":47565},{\"end\":47847,\"start\":47845},{\"end\":47854,\"start\":47851},{\"end\":48122,\"start\":48115},{\"end\":48133,\"start\":48126},{\"end\":48145,\"start\":48137},{\"end\":48647,\"start\":48641},{\"end\":48658,\"start\":48651},{\"end\":48792,\"start\":48782},{\"end\":48802,\"start\":48796},{\"end\":48815,\"start\":48806},{\"end\":48999,\"start\":48993},{\"end\":49010,\"start\":49003},{\"end\":49023,\"start\":49016},{\"end\":49034,\"start\":49027},{\"end\":49367,\"start\":49363},{\"end\":49374,\"start\":49371},{\"end\":49381,\"start\":49378},{\"end\":49664,\"start\":49656},{\"end\":49676,\"start\":49668},{\"end\":49685,\"start\":49680},{\"end\":49962,\"start\":49956},{\"end\":49977,\"start\":49966},{\"end\":49993,\"start\":49981},{\"end\":50003,\"start\":49997},{\"end\":50359,\"start\":50351},{\"end\":50370,\"start\":50363},{\"end\":50378,\"start\":50374},{\"end\":50388,\"start\":50382},{\"end\":50789,\"start\":50776},{\"end\":50796,\"start\":50793},{\"end\":50805,\"start\":50800},{\"end\":50813,\"start\":50809},{\"end\":51284,\"start\":51275},{\"end\":51295,\"start\":51288},{\"end\":51308,\"start\":51299},{\"end\":51699,\"start\":51696},{\"end\":51706,\"start\":51703},{\"end\":51720,\"start\":51710},{\"end\":51735,\"start\":51724},{\"end\":51741,\"start\":51739},{\"end\":51748,\"start\":51745},{\"end\":51754,\"start\":51752},{\"end\":51763,\"start\":51758},{\"end\":51775,\"start\":51767},{\"end\":52081,\"start\":52073},{\"end\":52087,\"start\":52085},{\"end\":52095,\"start\":52091},{\"end\":52104,\"start\":52099},{\"end\":52113,\"start\":52108},{\"end\":52123,\"start\":52117},{\"end\":52134,\"start\":52127},{\"end\":52146,\"start\":52138},{\"end\":52707,\"start\":52700},{\"end\":52720,\"start\":52711},{\"end\":53044,\"start\":53036},{\"end\":53052,\"start\":53048},{\"end\":53245,\"start\":53239},{\"end\":53254,\"start\":53251},{\"end\":53265,\"start\":53260},{\"end\":53274,\"start\":53271},{\"end\":53677,\"start\":53672},{\"end\":53684,\"start\":53681},{\"end\":53691,\"start\":53688},{\"end\":53699,\"start\":53695},{\"end\":53705,\"start\":53703},{\"end\":53712,\"start\":53709},{\"end\":53720,\"start\":53716},{\"end\":54246,\"start\":54244},{\"end\":54255,\"start\":54250},{\"end\":54262,\"start\":54259},{\"end\":54271,\"start\":54266},{\"end\":54278,\"start\":54275},{\"end\":54687,\"start\":54683},{\"end\":54695,\"start\":54691},{\"end\":54704,\"start\":54701},{\"end\":54713,\"start\":54708},{\"end\":54725,\"start\":54717},{\"end\":54732,\"start\":54729},{\"end\":54740,\"start\":54736}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":3423028},\"end\":41992,\"start\":41608},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":3922857},\"end\":42387,\"start\":41994},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":17080213},\"end\":42663,\"start\":42389},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":15281413},\"end\":42882,\"start\":42665},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":224298155},\"end\":43216,\"start\":42884},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":18303205},\"end\":43634,\"start\":43218},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":14850912},\"end\":43913,\"start\":43636},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":207227428},\"end\":44452,\"start\":43915},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":2707750},\"end\":44945,\"start\":44454},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":46173281},\"end\":45287,\"start\":44947},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":3636669},\"end\":46040,\"start\":45289},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":155107845},\"end\":46538,\"start\":46042},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":6844431},\"end\":46948,\"start\":46540},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":10611065},\"end\":47433,\"start\":46950},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":7794175},\"end\":47759,\"start\":47435},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":36663713},\"end\":48029,\"start\":47761},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":16895865},\"end\":48602,\"start\":48031},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":216078090},\"end\":48763,\"start\":48604},{\"attributes\":{\"id\":\"b18\"},\"end\":48934,\"start\":48765},{\"attributes\":{\"id\":\"b19\"},\"end\":49275,\"start\":48936},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":13936837},\"end\":49623,\"start\":49277},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":207172599},\"end\":49851,\"start\":49625},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":19002372},\"end\":50241,\"start\":49853},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":7798047},\"end\":50702,\"start\":50243},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":10441720},\"end\":51161,\"start\":50704},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":13519036},\"end\":51639,\"start\":51163},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":555807},\"end\":51998,\"start\":51641},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":1958648},\"end\":52593,\"start\":52000},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":12041280},\"end\":52984,\"start\":52595},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":17411204},\"end\":53194,\"start\":52986},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":18178967},\"end\":53579,\"start\":53196},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":17582729},\"end\":54158,\"start\":53581},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":53720973},\"end\":54602,\"start\":54160},{\"attributes\":{\"id\":\"b33\"},\"end\":54899,\"start\":54604}]", "bib_title": "[{\"end\":41664,\"start\":41608},{\"end\":42075,\"start\":41994},{\"end\":42459,\"start\":42389},{\"end\":42728,\"start\":42665},{\"end\":42931,\"start\":42884},{\"end\":43306,\"start\":43218},{\"end\":43687,\"start\":43636},{\"end\":43989,\"start\":43915},{\"end\":44540,\"start\":44454},{\"end\":45052,\"start\":44947},{\"end\":45385,\"start\":45289},{\"end\":46127,\"start\":46042},{\"end\":46605,\"start\":46540},{\"end\":47028,\"start\":46950},{\"end\":47518,\"start\":47435},{\"end\":47841,\"start\":47761},{\"end\":48109,\"start\":48031},{\"end\":48635,\"start\":48604},{\"end\":48987,\"start\":48936},{\"end\":49359,\"start\":49277},{\"end\":49652,\"start\":49625},{\"end\":49950,\"start\":49853},{\"end\":50347,\"start\":50243},{\"end\":50772,\"start\":50704},{\"end\":51271,\"start\":51163},{\"end\":51689,\"start\":51641},{\"end\":52069,\"start\":52000},{\"end\":52696,\"start\":52595},{\"end\":53032,\"start\":52986},{\"end\":53233,\"start\":53196},{\"end\":53668,\"start\":53581},{\"end\":54240,\"start\":54160}]", "bib_author": "[{\"end\":41675,\"start\":41666},{\"end\":41682,\"start\":41675},{\"end\":41689,\"start\":41682},{\"end\":41697,\"start\":41689},{\"end\":41703,\"start\":41697},{\"end\":41710,\"start\":41703},{\"end\":41718,\"start\":41710},{\"end\":41726,\"start\":41718},{\"end\":41734,\"start\":41726},{\"end\":42084,\"start\":42077},{\"end\":42092,\"start\":42084},{\"end\":42098,\"start\":42092},{\"end\":42105,\"start\":42098},{\"end\":42112,\"start\":42105},{\"end\":42120,\"start\":42112},{\"end\":42129,\"start\":42120},{\"end\":42136,\"start\":42129},{\"end\":42145,\"start\":42136},{\"end\":42151,\"start\":42145},{\"end\":42470,\"start\":42461},{\"end\":42481,\"start\":42470},{\"end\":42742,\"start\":42730},{\"end\":42941,\"start\":42933},{\"end\":42952,\"start\":42941},{\"end\":42965,\"start\":42952},{\"end\":42977,\"start\":42965},{\"end\":43315,\"start\":43308},{\"end\":43325,\"start\":43315},{\"end\":43331,\"start\":43325},{\"end\":43341,\"start\":43331},{\"end\":43351,\"start\":43341},{\"end\":43367,\"start\":43351},{\"end\":43375,\"start\":43367},{\"end\":43384,\"start\":43375},{\"end\":43695,\"start\":43689},{\"end\":43709,\"start\":43695},{\"end\":44001,\"start\":43991},{\"end\":44013,\"start\":44001},{\"end\":44022,\"start\":44013},{\"end\":44549,\"start\":44542},{\"end\":44557,\"start\":44549},{\"end\":44563,\"start\":44557},{\"end\":44570,\"start\":44563},{\"end\":44577,\"start\":44570},{\"end\":44584,\"start\":44577},{\"end\":44592,\"start\":44584},{\"end\":44600,\"start\":44592},{\"end\":45062,\"start\":45054},{\"end\":45069,\"start\":45062},{\"end\":45393,\"start\":45387},{\"end\":45401,\"start\":45393},{\"end\":45409,\"start\":45401},{\"end\":45415,\"start\":45409},{\"end\":45421,\"start\":45415},{\"end\":45427,\"start\":45421},{\"end\":45434,\"start\":45427},{\"end\":45442,\"start\":45434},{\"end\":45449,\"start\":45442},{\"end\":45457,\"start\":45449},{\"end\":46135,\"start\":46129},{\"end\":46142,\"start\":46135},{\"end\":46151,\"start\":46142},{\"end\":46159,\"start\":46151},{\"end\":46166,\"start\":46159},{\"end\":46173,\"start\":46166},{\"end\":46180,\"start\":46173},{\"end\":46621,\"start\":46607},{\"end\":46631,\"start\":46621},{\"end\":46645,\"start\":46631},{\"end\":46658,\"start\":46645},{\"end\":46675,\"start\":46658},{\"end\":47038,\"start\":47030},{\"end\":47051,\"start\":47038},{\"end\":47066,\"start\":47051},{\"end\":47529,\"start\":47520},{\"end\":47538,\"start\":47529},{\"end\":47554,\"start\":47538},{\"end\":47565,\"start\":47554},{\"end\":47572,\"start\":47565},{\"end\":47849,\"start\":47843},{\"end\":47856,\"start\":47849},{\"end\":48124,\"start\":48111},{\"end\":48135,\"start\":48124},{\"end\":48147,\"start\":48135},{\"end\":48649,\"start\":48637},{\"end\":48660,\"start\":48649},{\"end\":48794,\"start\":48780},{\"end\":48804,\"start\":48794},{\"end\":48817,\"start\":48804},{\"end\":49001,\"start\":48989},{\"end\":49012,\"start\":49001},{\"end\":49025,\"start\":49012},{\"end\":49036,\"start\":49025},{\"end\":49369,\"start\":49361},{\"end\":49376,\"start\":49369},{\"end\":49383,\"start\":49376},{\"end\":49666,\"start\":49654},{\"end\":49678,\"start\":49666},{\"end\":49687,\"start\":49678},{\"end\":49964,\"start\":49952},{\"end\":49979,\"start\":49964},{\"end\":49995,\"start\":49979},{\"end\":50005,\"start\":49995},{\"end\":50361,\"start\":50349},{\"end\":50372,\"start\":50361},{\"end\":50380,\"start\":50372},{\"end\":50390,\"start\":50380},{\"end\":50791,\"start\":50774},{\"end\":50798,\"start\":50791},{\"end\":50807,\"start\":50798},{\"end\":50815,\"start\":50807},{\"end\":51286,\"start\":51273},{\"end\":51297,\"start\":51286},{\"end\":51310,\"start\":51297},{\"end\":51701,\"start\":51691},{\"end\":51708,\"start\":51701},{\"end\":51722,\"start\":51708},{\"end\":51737,\"start\":51722},{\"end\":51743,\"start\":51737},{\"end\":51750,\"start\":51743},{\"end\":51756,\"start\":51750},{\"end\":51765,\"start\":51756},{\"end\":51777,\"start\":51765},{\"end\":52083,\"start\":52071},{\"end\":52089,\"start\":52083},{\"end\":52097,\"start\":52089},{\"end\":52106,\"start\":52097},{\"end\":52115,\"start\":52106},{\"end\":52125,\"start\":52115},{\"end\":52136,\"start\":52125},{\"end\":52148,\"start\":52136},{\"end\":52709,\"start\":52698},{\"end\":52722,\"start\":52709},{\"end\":53046,\"start\":53034},{\"end\":53054,\"start\":53046},{\"end\":53247,\"start\":53235},{\"end\":53256,\"start\":53247},{\"end\":53267,\"start\":53256},{\"end\":53276,\"start\":53267},{\"end\":53679,\"start\":53670},{\"end\":53686,\"start\":53679},{\"end\":53693,\"start\":53686},{\"end\":53701,\"start\":53693},{\"end\":53707,\"start\":53701},{\"end\":53714,\"start\":53707},{\"end\":53722,\"start\":53714},{\"end\":54248,\"start\":54242},{\"end\":54257,\"start\":54248},{\"end\":54264,\"start\":54257},{\"end\":54273,\"start\":54264},{\"end\":54280,\"start\":54273},{\"end\":54689,\"start\":54681},{\"end\":54697,\"start\":54689},{\"end\":54706,\"start\":54697},{\"end\":54715,\"start\":54706},{\"end\":54727,\"start\":54715},{\"end\":54734,\"start\":54727},{\"end\":54742,\"start\":54734}]", "bib_venue": "[{\"end\":44205,\"start\":44122},{\"end\":44697,\"start\":44657},{\"end\":45700,\"start\":45587},{\"end\":47203,\"start\":47143},{\"end\":48326,\"start\":48245},{\"end\":50471,\"start\":50439},{\"end\":50940,\"start\":50886},{\"end\":52313,\"start\":52239},{\"end\":53889,\"start\":53814},{\"end\":41773,\"start\":41734},{\"end\":42162,\"start\":42151},{\"end\":42498,\"start\":42481},{\"end\":42755,\"start\":42742},{\"end\":43018,\"start\":42977},{\"end\":43391,\"start\":43384},{\"end\":43757,\"start\":43709},{\"end\":44120,\"start\":44022},{\"end\":44655,\"start\":44600},{\"end\":45093,\"start\":45069},{\"end\":45585,\"start\":45457},{\"end\":46268,\"start\":46180},{\"end\":46715,\"start\":46675},{\"end\":47141,\"start\":47066},{\"end\":47576,\"start\":47572},{\"end\":47877,\"start\":47856},{\"end\":48243,\"start\":48147},{\"end\":48664,\"start\":48660},{\"end\":48778,\"start\":48765},{\"end\":49085,\"start\":49036},{\"end\":49432,\"start\":49383},{\"end\":49715,\"start\":49687},{\"end\":50024,\"start\":50005},{\"end\":50437,\"start\":50390},{\"end\":50884,\"start\":50815},{\"end\":51379,\"start\":51310},{\"end\":51784,\"start\":51777},{\"end\":52237,\"start\":52148},{\"end\":52764,\"start\":52722},{\"end\":53066,\"start\":53054},{\"end\":53363,\"start\":53276},{\"end\":53812,\"start\":53722},{\"end\":54353,\"start\":54280},{\"end\":54679,\"start\":54604}]"}}}, "year": 2023, "month": 12, "day": 17}