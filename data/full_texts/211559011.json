{"id": 211559011, "updated": "2023-10-06 18:26:16.918", "metadata": {"title": "Reward Constrained Interactive Recommendation with Natural Language Feedback", "authors": "[{\"first\":\"Ruiyi\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Tong\",\"last\":\"Yu\",\"middle\":[]},{\"first\":\"Yilin\",\"last\":\"Shen\",\"middle\":[]},{\"first\":\"Hongxia\",\"last\":\"Jin\",\"middle\":[]},{\"first\":\"Changyou\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Lawrence\",\"last\":\"Carin\",\"middle\":[]}]", "venue": null, "journal": "arXiv: Computation and Language", "publication_date": {"year": 2020, "month": 5, "day": 4}, "abstract": "Text-based interactive recommendation provides richer user feedback and has demonstrated advantages over traditional interactive recommender systems. However, recommendations can easily violate preferences of users from their past natural-language feedback, since the recommender needs to explore new items for further improvement. To alleviate this issue, we propose a novel constraint-augmented reinforcement learning (RL) framework to efficiently incorporate user preferences over time. Specifically, we leverage a discriminator to detect recommendations violating user historical preference, which is incorporated into the standard RL objective of maximizing expected cumulative future rewards. Our proposed framework is general and is further extended to the task of constrained text generation. Empirical results show that the proposed method yields consistent improvement relative to standard RL methods.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2005.01618", "mag": "3021164031", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2005-01618", "doi": null}}, "content": {"source": {"pdf_hash": "16bd24d398b56a5fd876445138dc637237e9a77b", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2005.01618v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "40809ff1cd2681cd6b4970e04872df119567fd04", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/16bd24d398b56a5fd876445138dc637237e9a77b.txt", "contents": "\nReward Constrained Interactive Recommendation with Natural Language Feedback\n4 May 2020\n\nRuiyi Zhang ryzhang@cs.duke.edu \nDuke University\n\n\nYu Tong \nSamsung Research America\n\n\nYilin Shen \nSamsung Research America\n\n\nHongxia Jin \nSamsung Research America\n\n\nChangyou Chen \nUniversity at Buffalo\n\n\nLawrence Carin \nDuke University\n\n\nReward Constrained Interactive Recommendation with Natural Language Feedback\n4 May 2020CBE99AA56F1899569B85C4B440DD4BE8arXiv:2005.01618v1[cs.CL]\nText-based interactive recommendation provides richer user feedback and has demonstrated advantages over traditional interactive recommender systems.However, recommendations can easily violate preferences of users from their past natural-language feedback, since the recommender needs to explore new items for further improvement.To alleviate this issue, we propose a novel constraintaugmented reinforcement learning (RL) framework to efficiently incorporate user preferences over time.Specifically, we leverage a discriminator to detect recommendations violating user historical preference, which is incorporated into the standard RL objective of maximizing expected cumulative future rewards.Our proposed framework is general and is further extended to the task of constrained text generation.Empirical results show that the proposed method yields consistent improvement relative to standard RL methods.\n\nIntroduction\n\nTraditional recommender systems depend heavily on user history.However, these approaches, when implemented in an offline manner, cannot provide satisfactory performance due to sparse history data and unseen dynamic new items (e.g., new products, recent movies, etc.).Recent work on recommender systems has sought to interact with users, to adapt to user preferences over time.Most existing interactive recommender systems are designed based on simple user feedback, such as clicking data or updated ratings [6,30,33].However, this type of feedback contains little information to reflect complex user attitude towards various aspects of an item.For example, a user may like the graphic of a dress but not its color.A click or numeric rating is typically not sufficient to express such a preference, and thus it may lead to poor recommendations.By contrast, allowing a recommender system to use natural-language feedback provides richer information for future recommendation, especially for visual item recommendation [20,21].With natural-language feedback, a user can describe features of desired items that are lacking in the current recommended items.The system can then incorporate feedback and subsequently recommend more suitable items.This type of recommendation is referred to as text-based interactive recommendation.\n\nFlexible feedback with natural language may still induce undesired issues.For example, a system may ignore the previous interactions and keep recommending similar items, for which a user has expressed the preference before.To tackle these issues, we propose a reward constrained recommendation (RCR) framework, where one sequentially incorporates constraints from previous feedback into the recommendation.Specifically, we formulate the text-based interactive recommendation as a constraint-augmented reinforcement learning (RL) problem.Compared to standard constraintaugmented RL, there are no explicit constraints in text-based interactive recommendation.To this end, we use a discriminator to detect violations of user preferences in an adversarial manner.To further 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.validate our proposed RCR framework, we extend it to constrained text generation to discourage undesired text generation.\n\nThe main contributions of this paper are summarized as follows.(i) A novel reward constrained recommendation framework is developed for text-based interactive recommendation, where constraints work as a dynamically updated critic to penalize the recommender.(ii) A novel way of defining constraints is proposed, in an adversarial manner, with better generalization.(iii) Extensive empirical evaluations are performed on text-based interactive recommendation and constrained text generation tasks, demonstrating consistent performance improvement over existing approaches.\n\n\nBackground\n\n\nReinforcement Learning\n\nReinforcement learning aims to learn an optimal policy for an agent interacting with an unknown (and often highly complex) environment.A policy is modeled as a conditional distribution \u03c0(a|s), specifying the probability of choosing action a \u2208 A when in state s \u2208 S. Formally, an RL problem is characterized by a Markov decision process (MDP) [39], M = S, A, P, R .In this work, we consider recommendation for finite-horizon environments with the average reward criterion.If the agent chooses action a \u2208 A at state s \u2208 S, then the agent will receive an immediate reward r(s, a), and the state will transit to s \u2208 S with probability P (s |s, a).The expected total reward of a policy \u03c0 is defined as [43]:\nJ R (\u03c0) = \u221e t=1 E P,\u03c0 [r(s t , a t )] .(1)\nIn (1) the sum is over infinite time steps, but in practice we will be interested in finite horizons.\n\nThe goal of an agent is to learn an optimal policy that maximizes J R (\u03c0).A constrained Markov decision process (CMDP) [3] extends the MDP framework by introducing the constraint C(s, a) (mapping a state-action pair to costs, similar to the usual reward)2 and a threshold \u03b1 \u2208 [0, 1].Denoting the expectation over the constraint C(s, a) as\nJ C (\u03c0) = \u221e t=1 E P,\u03c0 [C(s t , a t )]\n, the constrained policy optimization thus becomes [1]:\nmax \u03c0\u2208\u03a0 J R (\u03c0), s.t. J C (\u03c0) \u2264 \u03b1 .(2)\n\nText-based Interactive Recommendation as Reinforcement Learning\n\nWe employ an RL-based formulation for sequential recommendation of items to users, utilizing user feedback in natural language.Denote s t \u2208 S as the state of the recommendation environment at time t and a t \u2208 A as the recommender-defined items from the candidate items set A. In the context of a recommendation system, as discussed further below, the state s t corresponds to the state of sequential recommender, implemented via a LSTM [24] state tracker.At time t, the system recommends item a t based on the current state s t at time t.After viewing item a t , a user may comment on the recommendation in natural language (a sequence of natural-language text) x t , as feedback.The recommender then receives a reward r t and perceives the new state s t+1 .Accordingly, we can model the recommendation-feedback loop as an MDP M = S, A, P, R , where P : S \u00d7 A \u00d7 S \u2192 R is the environment dynamic of recommendation and R : S \u00d7 A \u2192 R is the reward function used to evaluate recommended items.The recommender seeks to learn a policy parameterized by \u03b8, i.e., \u03c0 \u03b8 (a|s), that corresponds to the distribution of items conditioned on the current state of the recommender.The recommender is represented as an optimal policy that maximizes the expected reward as J R (\u03c0) = t E P,\u03c0 [r(s t , a t )].At each time step, the recommender sequentially selects potential desired items at each time step via a t = arg max a\u2208A \u03c0 \u03b8 (a|s t ).\n\n\nProposed Method\n\nIn text-based interactive recommendation, users provide natural-language-based feedback.We consider the recommendation of visual items [20,21].As shown in Figure 2, the system recommends an item to the user, with its visual appearance.The user then views the recommended item and gives feedback in natural language, describing the desired aspects that the current recommended item Recommended Image a t < l a t e x i t s h a 1 _ b a s e 6 4 = \" 4 k X / A j H N V j q Y 1 T H I 4 q a k y m b L 4 U c = \"\n> A A A B + n i c b V C 7 T s M w F H X K q 5 R X C i O L R Y X E V C W l E o y V W B i L R B 9 S G 0 W O 4 7 Z W n T i y b 0 B V 6 K e w M I A Q K 1 / C x t / g t B m g 5 U i W j 8 6 5 V z 4 + Q S K 4 B s f 5 t k o b m 1 v b O + X d y t 7 + w e G R X T 3 u a p k q y j p U C q n 6 A d F M 8 J h 1 g I N g / U Q x E g W C 9 Y L p T e 7 3 H p j S X M b 3 M E u Y F 5 F x z E e c E j C S b 1 e z Y S B F q G e R u T C Z + + D b N a f u L I D X i V u Q G i r Q 9 u 2 v Y S h p G r E Y q C B a D 1 w n A S 8 j C j g V b F 4 Z p p o l h E 7 J m A 0 M j U n E t J c t o s / x u V F C P J L K n B j w Q v 2 9 k Z F I 5 + H M Z E R g o l e 9 X P z P G 6 Q w u v Y y H i c p s J g u H x q l A o P E e Q 8 4 5 I p R E D N D C F X c Z M V 0 Q h S h Y N q q m B L c 1 S + v k 2 6 j 7 l 7 W G 3 f N W q t Z 1 F F G p + g M X S A X X a E W u k V t 1 E E U P a J n 9 I r e r C f r x X q 3 P p a j J a v Y O U F / Y H 3 + A J d J l C o = < / l a tn h + O U X W U C A + r i W + f R D n n E = \" > A A A B / n i c b V D N S 8 M w H E 3 n 1 5 x f V f H k J T g E Q R j t H O h x 4 M X j B P c B W y l p m m 5 h a V K S V B i l 4 L / i x Y M i X v 0 7 v P n f m G 4 9 6 O a D k M d 7 v x 9 5 e U H C q N K O 8 2 1 V 1 t Y 3 N r e q 2 7 W d 3 b 3 9 A / v w q K d E K j H p Y s G E H A R I E U Y 5 6 W q q G R k k k q A 4 Y K Q f T G 8 L v / 9 I p K K C P + h Z Q r w Y j T m N K E b a S L 5 9 k o 0 C w U I 1 i 8 0 F U e 5 n + t L N f b v u N J w 5 4 C p x S 1 I H J T q + / T U K B U 5 j w j V m S K m h 6 y T a y 5 D U F D O S 1 0 a p I g n C U z Q m Q 0 M 5 i o n y s n n 8 H J 4 b J Y S R k O Z w D e f q 7 4 0 M x a o I a C Z j p C d q 2 S v E / 7 x h q q M b L 6 M 8 S T X h e P F Q l D K o B S y 6 g C G V B G s 2 M w R h S U 1 W i C d I I q x N Y z V T g r v 8 5 V X S a z b c q 0 b z v l V vG J K + A D I X i D O X U E M q U M F k J G 1 N F G Z q i y q Y E Z / H L y 6 R d r z n n t f r d R b V x W d R R g m M 4 g T N w 4 A o a c A t N a A E D B c / w C m / W k / V i v V\ns f 8 9 E V q 9 g 5 g j + w P n 8 A U o m T A g = = < / l a t e x i t >\n\nFigure 1: Overview of the reward constrained recommender model.When receiving the recommended images, the user gives natural-language feedback, and this feedback will be used for the next item recommendation, as well as preventing future violations.lacks.The system then incorporates the user feedback and recommends (ideally) more-suitable items, until the desired item is found.While users provide natural-language feedback on recommendations, standard RL methods may overlook the information from the feedback and recommend items that violate the user's previous feedback.To better understand this issue, consider the example in Figure 2.\n\nIn round 3, the system forgets, and recommends an item that violates previous user preference on the 'ankle boots'.To alleviate this issue, we consider using feedback from users as constraints, and formulate text-based interactive recommendation as a constrained policy optimization problem.The difference between the investigated problem and conventional constrained policy optimization [3,5] is that constraints are added sequentially, affecting the search space of a policy in a different manner.Our model is illustrated in Figure 1.\n\n\nRecommendation as Constrained Policy Optimization\n\nWe consider an RL environment with a large number of discrete actions, deterministic transitions, and deterministic terminal returns.Suppose we have the user preference as constraints J C (\u03c0 \u03b8 ) when making recommendations.The objective of learning a recommender is defined as:\nJ R (\u03c0 \u03b8 ) = \u221e t=1 E P,\u03c0 \u03b8 [r(s t , a t )] , s.t. J C (\u03c0 \u03b8 ) \u2264 \u03b1 .(3)\nIf one naively augments previous user preferences as a hard constraint, i.e., exactly attributes matching, it usually leads to a sub-optimal solution.To alleviate this issue, we propose to use a learned constraint function based on the visual and textual information.\n\nConstraint Functions In text-based interactive recommendation, we explicitly use the user preference as constraints.Specifically, we exploit user feedback and put it as sequentially added constraints.\n\nTo generalize well on the constraints, we learn a discriminator C \u03c6 parameterized by \u03c6 as the constraint function.We define two distributions on feedback-recommendation pairs, i.e., non-violation distribution p r , and violation distribution p f (details provided in Appendix A.2).The objective of the discriminator is to minimize the following objective:\nL(\u03c6) = \u2212E (s,a)\u223cp f [log(C \u03c6 (s, a))] \u2212 E (s,a)\u223cpr [log(1 \u2212 C \u03c6 (s, a))] .(4)\nWith the discriminator as the constraint i.e.,\nJ C \u03c6 (\u03c0 \u03b8 ) \u221e t=1 E P,\u03c0 \u03b8 [C \u03c6 (s t , a t )],\nthe constraint is imposed.However, directly solving the constrained-optimization problem in (3) is difficult, and we employ the Lagrange relaxation technique [4] to transform the original objective to an equivalent problem as:\nmin \u03bb\u22650 max \u03b8 L(\u03bb, \u03b8, \u03c6) = min \u03bb\u22650 max \u03b8 J R (\u03c0 \u03b8 ) \u2212 \u03bb \u2022 (J C \u03c6 (\u03c0 \u03b8 ) \u2212 \u03b1) ,(5)\nwhere \u03bb \u2265 0 is a Lagrange multiplier.Note that as \u03bb increases, the solution to (5) converges to that of (3).The goal is to find a saddle point (\u03b8 * (\u03bb * ), \u03bb * ) of ( 5), that can be achieved approximately by alternating gradient descent/ascent.Specifically, the gradient of ( 5) can be estimated using policy gradient [43] as:\n\u2207 \u03b8 L(\u03b8, \u03bb, \u03c6) = E P,\u03c0 [(r(s t , a t ) \u2212 \u03bbC \u03c6 (s t , a t )) \u2207 \u03b8 log \u03c0 \u03b8 (s t , a t )] ,(6)\u2207 \u03bb L(\u03b8, \u03bb, \u03c6) = \u2212(E P,\u03c0 [C \u03c6 (s t , a t )] \u2212 \u03b1) ,(7)\nwhere C \u03c6 (s t , a t ) is the general constraint, specified in the following.\n\nPenalized Reward Functions Note that the update in ( 6) is similar to the actor-critic method [43].\n\nWhile the original use of a critic in reinforcement learning was for variance reduction [43], here we use it to penalize the policy for constraint violations.In order to ensure the constraints, \u03bb is also optimized using policy gradient via (7).The optimization proceeds intuitively as: i) when a violation happens (i.e., C \u03c6 (s, a) > \u03b1), \u03bb will increase to penalize the policy.ii) If there is no violation (i.e., C \u03c6 (s, a) < \u03b1), \u03bb will decrease to give the policy more reward.\n\n\nModel Training\n\nWe alternatively update the constraint function, i.e., the discriminator and the recommender \u03c0 \u03b8 , similar to the Generative Adversarial Network (GAN) [16].Specifically, the parameters are updated via the following rules:\n\u03b8 k+1 = \u0393 \u03b8 [\u03b8 k + \u03b7 1 (k)\u2207 \u03b8 L(\u03bb k , \u03b8 k , \u03c6 k )] ,(8)\u03c6 k+1 = \u03c6 k + \u03b7 2 (k)\u2207 \u03c6 L(\u03bb k , \u03b8 k , \u03c6 k ) ,(9)\u03bb k+1 = \u0393 \u03bb [\u03bb k \u2212 \u03b7 3 (k)\u2207 \u03bb L(\u03bb k , \u03b8 k , \u03c6 k )] ,(10)Algorithm 1 Reward Constrained Recommendation Input: constraint C(\u2022), threshold \u03b1, learning rates \u03b7 1 (k) > \u03b7 2 (k) > \u03b7 3 (k)\nInitialize recommender and discriminator parameters with pretrained ones, Lagrange multipliers \u03bb 0 = 0 repeat for t = 0, 1, ..., T \u2212 1 do Sample action a t \u223c \u03c0, observe next state s t+1 , reward r t and penalties c t Rt = r t \u2212 \u03bb k c t Recommender update with (8) end for Discriminator update with (9) Lagrange multiplier update with (10) until Model converges return recommender (policy) parameters \u03b8 where \u0393 \u03b8 is a projection operator, which keeps the stability as the parameters are updated within a trust region; \u0393 \u03bb projects \u03bb into the range [0, \u03bb max ].\n\nWe denote a three-timescale Reward Constrained Recommendation process, i.e., the three parts are updated with different frequency and step sizes: the recommender aims to maximize the expected reward with less violations following (8).As described in the Algorithm 1, the discriminator is updated following (9) to detect new violations, and \u03bb is updated following (10).\n\n\nModel Details\n\nWe discuss details on model design when applying the proposed framework in a textbased recommender system.Feature Extractor Our feature extractor consists of the encoders of text and visual inputs.Similar to [21], we consider the case where the visual attributes are available.We encode the raw images of the items by ResNet50 [22] and an attribute network, i.e., the visual feature c vis t of the item a t is the concatenation of ResNet(a t ) and AttrNet(a t ).The input of the attribute network is an item's encoding by ResNet50 and the attribute network outputs this items' attribute values.We further encode the user comments in texts by an embedding layer, a LSTM and a linear mapping.Given a user comment x t , the final output of textual context is denoted as c txt t .The encoded image and comment are further concatenated as the input to an MLP, and then the recommender component.\n\nRecommender With the visual feature c vis t and textual feature c txt t , the recommender perceives the state in an auto-regressive manner.At time t, the state is\ns t = f (g([c vis t , c txt t ]), s t\u22121 )\n, where g is an MLP for textual and visual matching, and f is the LSTM unit [24].Since our goal in each user session is to find items with a set of desired attribute values, we use the policy \u03c0 \u03b8 with multi-discrete action spaces [13,23].For each attribute, the desired attribute value by the user is sampled from a categorical distribution.Given the state s t , the probability of choosing a particular attribute value is output by a three-layer fully connected neural network with a softmax activation function.The recommender samples the values of different attributes from \u03c0 \u03b8 .If K items are recommended at each time, we select the items that are top K closest to the sampled attribute values under Euclidean distance in the visual attribute space.Discriminator The discriminator is designed to discriminate whether a recommended item at time t violates previous user comments in the current session.That is, given the visual feature of current image c vis t , and textual features {c txt j } t\u22121 j=1 , the discriminator outputs whether the image violates the user comment.In practice, this discriminator is a three-layer fully connected neural network and trained on-the-fly to incrementally learn the multimodal matching between the user comments and item visual features.Following Algorithm 1, we update the discriminator after each user session, where a user interacts with the system for several time steps, or quits.To further enhance the results, when making recommendations, we reject some items based on this discriminator.If an item a t sampled by the recommender has high probability of violating the previous comments {x i } t\u22121 i=1 , we ignore this item and sample another item to recommend.\n\n\nExtension to Constrained Text Generation\n\nIn this section, we describe how to extend our framework for constrained text generation.\nW F R I T F V S E D B W Y m E s E n 1 I b V Q 5 j t N a d Z z I d p D a q F / C w g B C r H w K G 3 + D 0 2 a A l i N Z P j r n X v n 4 + A l n S j v O t 1 X a 2 N z a 3 i n v V v b 2 D w 6 r 9 t F x R 8 W p J L R N Y h 7 L n o 8 V 5 U z Q t m a a 0 1 4 i K Y 5 8 T r v + 5 C 7 3 u 0 9 U K h a L R z 1 N q B f h k W A h I 1 g b a W h X s 4 E f 8 0 B N I 3 O h 2 X x o 1 5 y 6 s w B a J 2 5 B a l C g N b S / B k F M 0 o g K T T h W q u 8 6 i f Y y L D U j n M 4 r g 1 T R B J M J H t G + o Q J H V H n Z I v g c n R s l Q G E s z R E a L d T f G x m O V B 7 N T E Z Y j 9 W q l 4 v / e f 1 U h 7 d e x k S S a i r I 8 q E w 5 U j H K G 8 B B U x S o v n U E E w k M 1 k R G W O J i T Z d V U w J 7 u q X 1 0 m n U X c v 6 4 2 H q 1 r z u q i j D K d w B h f g w g 0 0 4 R 5 a 0 A Y C K T z D K 7\nx Z M + v F e r c + l q M l q 9 g 5 g T + w P n 8 A J 0 6 T X g = = < / l a t e x i t >  We consider text generation with specific constraints.Specifically, we consider the scenario of controlling for negative sentiments.For example, a generator may generate some offensive or negative words, which will affect the user experience in some situations, such as with an online chatbot for helping consumers.To alleviate this issue, we applied the proposed RCR methods for text generation.\nF i t u X V 3 D r R K v I L U o E B r W P 0 a h J K k M R W G c K x 1 3 3 M T 4 2 d Y G U Y 4 n Z U H q a Y J J h M 8 o n 1 L B Y 6 p 9 r N 5 8 B k 6 s 0 q I I q n s E Q b N 1 d 8 b G Y 5 1 H s 1 O x t i M 9 b K X i / 9 5 / d R E N 3 7 G R J I a K s j i o S j l y E i U t 4 B C p i g x f G o J J o r Z r I i M s c L E 2 K 7 K t g R v + c u r p N O o e x f 1 x v 1 l r X l V 1 F G C E z i F c / D g G p p w B y 1 o A 4 E U n u E V 3 p w n 5 8 V 5 d z 4 W o 2 t O s X M M f + B 8 / g D z l Z M 8 < / l a t e x i t > Discriminator (General) X < l a t e x i t s h a 1 _ b a s e 6 4 = \" Y 5 k 7 G B E f u n z N 0 P K l 6 R C k O j p K R X E = \" > A A A B / n i c b V D L S s N A F L 3 x W e s r K q 7 c D B b B V U m q q M u C G 5 c V 7 A O a U C a T S T t 0 k g k z E 6 G E g r / i x o U i b v 0 O d / 6 N k z Y L b T 0 w z O G c e 5 k z J 0 g 5 U 9 p x v q 2 V 1 b X 1 j c 3 K V n V 7 Z 3 d v 3 z 4 4 7 C i R S U L b R H A h e w F W l L O E t j X T n P Z S S X E c c N o N x r e F 3 3 2 k U j G R P O h J S v 0 Y D x M W M Y K 1 k Q b 2 s T f C O s + 9 Q P B Q T W J z o d 5 0 O r B r T t 2 Z A S 0 T t y Q 1 K N E a 2 F 9 e K E g W 0 0 Q T j p X q u 0 6 q / R x L z Q i n 0 6 q X K Z p i M s Z D 2 j c 0 w T F V f j 6 L P 0 V n R g l R J K Q 5 i U Y z 9 f d G j m N V Z D O T M d Y j t e g V 4 n 9 e P 9 P R j Z + z J M 0 0 T c j 8 o S j j S A t U d I F C J i n R f G I I J p K Z r I i M s M R E m 8 a q p g R 3 8 c v L p N O o u x f 1 x v 1 l r X l V 1 l G B E z i F c 3 D h G p p w B y 1 o A 4 E c n u E V 3 q w n 6 8 V 6 t z 7 m o y t W u X M E f 2 B 9 / g D h n Z Y J < / l a t e x i t >\nWe assume each sentence is generated from a latent vector z \u223c p(z), where p(z) is the distribution of a latent code.Text generation is then formulated as the learning of a distribution: p(X) = zx p(X|z x )q(z x |X)dz x , where p corresponds to a decoder and q to an encoder model, within the encoder-decoder framework; z is the latent code containing content information.The generator learns a policy \u03c0 \u03b8 to generate a sequence Y = (y 1 , . . ., y T ) of length T .Here each y t is a token from vocabulary A. The objective is to maximize the expected reward with less constraint violations, defined as:\nL(\u03b8, \u03bb, \u03c6) = min \u03bb\u22650 max \u03b8 E Y \u223c\u03c0 \u03b8 [r(Y ) \u2212 \u03bb(C \u03c6 (Y ) \u2212 \u03b1)] ,(11)\nwhere r is the reward function, that can be a metric reward (e.g., BLEU) or a learned reward function with general discriminator [53]; C \u03c6 (\u2022) is the constraint discriminator for the generation.In practice, we pretrain our generator \u03c0 \u03b8 with a variational autoencoder (VAE) [26], and we only use the decoder as our generator.More details about the pretrained model are provided in Appendix A.1.There is a constraint for the generation, and the framework is illustrated in Figure 3.The general discriminator can be a language model [49], and the constraint is a learned function parameterized by a neural network.During inference, the model generates text based on draws from an isotropic Gaussian distribution, i.e., z \u223c N (0, I).Here we only consider the static constraint with non-zero final deterministic reward.\n\n\nRelated Work\n\nConstrained Policy Optimization Constrained Markov Decision Processes [3] are employed in a wide range of applications, including analysis of electric grids [27] and in robotics [9,18].Lagrange multipliers are widely used to solve the CMDP problem [5,44], as adopted in our proposed framework.\n\nOther solutions of CMDP include use of a trust region [1], and integrating prior knowledge [12].Additionally, some previous work manually selects the penalty coefficient [14,32,38].In contrast with standard methods, our constraint functions are: (i) sequentially added via natural-language feedback; (ii) parameterized by a dynamically updated neural network with better generalization.\n\nText-Based Recommender System Communications between a user and recommendation system have been leveraged to understand user preference and provide recommendations.Entropy-based methods and bandits have been studied in question selection [11,35].Deep learning and reinforcement learning models have been proposed to understand user conversations and make recommendations [2,10,17,31,42,54,57].Similar to [11,31,42,54], the items are associated with a set of attributes in our recommendation setting.In the existing works, the content of the conversation serves as the constraint when a system makes recommendations.However, in most existing works, constraints from the conversations are not explicitly modeled.By contrast, this paper proposes a novel constrained reinforcement learning framework to emphasize the constraints when making recommendations.\n\nInteractive Image Retrieval Leveraging user feedback on images to improve image retrieval has been studied extensively [46].Depending on the feedback format, previous works can be categorized into relevance feedback [40,48] and relative-attributes feedback [28,37,50].In these works, the attributes to describe the images are pre-defined and fixed.To achieve more flexible and precise representation of the image attributes, Guo, et al. [20] proposes an end-to-end approach, without pre-defining a set of attributes.Their goal is to improve the ranking of the target item, while we focus on recommending items that do not violate the users' previous comments in the iterative recommendation.Thus, we develop a different evaluation simulator as detailed in Section 5.1.In [54], it is assumed that an accurate discriminator pretrained on huge-amount offline data is available at the beginning, which is usually impractical.Instead, our novel RCR framework learns the discriminator from scratch and dynamically updates the model \u03c6 and its weight \u03bb by ( 9) and ( 10) online.\n\nConstrained Text Generation Adversarial text generation [7,15,34,36,53,55] use reinforcement learning (RL) algorithms for text generation.They use the REINFORCE algorithm to provide an unbiased gradient estimator for the generator, and apply the roll-out policy to obtain the reward from the discriminator.LeakGAN [19] adopts a hierarchical RL framework to improve text generation.GSGAN [29] and TextGAN [25,56] use the Gumbel-softmax and soft-argmax representation, respectively, to deal with discrete data.Wang, et al. [47] put topic-aware priors on the latent codes to generate text on specific topics.All these works consider generating sentences with better quality and diversity, without explicit constraints.\n\n\nExperiments\n\nWe apply the proposed methods in two applications: text-based interactive recommendation and constrained text generation, to demonstrate the effectiveness of our proposed RCR framework.\n\n\nText-Based Interactive Recommendation\n\nDataset and Setup Our approaches are evaluated on the UT-Zappos50K dataset [51,52].UT-Zappos50K is a shoe dataset consisting of 50,025 shoe images.This dataset provides rich attribute data and we focus on shoes category, shoes subcategory, heel height, closure, gender and toe style in our evaluation.Among all the images, 40,020 images are randomly sampled as training data and the rest are used as test data.To validate the generalization ability of our approach, we compare the performance on seen items and unseen items.The seen items are the items in the training data where the item visual attributes are carefully labeled.The unseen items are the items in the test data.We assume the unseen items are newly collected and have no labeled visual attributes.We train the attribute network on the training data, under the cross-entropy loss.The ResNet50 is pretrained on ImageNet and is fixed subsequently.When we report the results on seen and unseen items, their attribute values are predicted by the attribute network.We pretrain the textual encoder, where the labels are the described attribute values, under the cross-entropy loss.The training data consists of the comments collected by annotators as detailed later in this section.In reinforcement learning, we use Adam [26] as the optimizer.We set \u03b1 = 0.5 and \u03bb max = 1.\n\nWe define the reward as the visual similarity between the recommended and desired items.Similar to [21], in our task both images and their visual attributes are available to measure the similarity.It is desired that the recommended item becomes more similar to the desired item with more user interactions.Thus, at time t, given the recommended item a t and the desired item a * , we  want to minimize their visual difference.In detail, we maximize the following visual reward\nr t = \u2212||ResNet(a t ) \u2212 ResNet(a * )|| 2 \u2212 \u03bb att ||AttrNet(a t ) \u2212 AttrNet(a * )|| 0 , where || \u2022 || 2 is the L 2 norm, || \u2022 || 0 is the L 0 norm\n, and we set \u03bb att = 0.5 to ensure the scales of the two distances are similar.If the system is not able to find the desired item before 50 interactions, we will terminate this user session and the system will receive an extra reward \u22123 (i.e., a penalty).\n\nOnline Evaluation We cannot directly detect the violations with existing text-based interactive recommendation dataset [20], since there are no attribute labels for the images.A recent relevant fashion dataset provides the attribute labels3 derived from the text metadata [21].Unfortunately, we observe that the user's comments are usually unrelated to the attribute labels.Therefore, we need to collect the user's comments relevant to attributes with groundtruth, for our evaluation purpose.\n\nFurther, evaluating the proposed system requires the ability to get access to all user reactions to any possible items at each time step.For the evaluation on the UT-Zappos50K dataset, we use a similar simulator to Guo, et al. [20].This simulator acts as a surrogate for real human users by generating their comments in natural language.The generated comments describe the prominent visual attribute differences between any pair of desired and candidate items.\n\nTo achieve this, we collect user comments relevant to the attributes with groundtruth and train a user simulator.A training dataset is collected for 10,000 pairs of images with visual attributes.These pairs are prepared such that in each pair there is a recommended item and a desired item.Given a pair of images, one user comment is collected.The data are collected in a scenario in which the customer talks with the shopping assistant to get the desired items.The annotators act as the customers to express the desired attribute values of items.For the evaluation purpose, we adopt a simplified setting and instruct the annotators to describe the comments related to a fixed set of visual attributes.Thus, the comments in our evaluation are relatively simpler compared to the real-world sentences.Considering this, we further augment the collected user comment data as follows.From the real-world sentences collected from annotators, we derive several sentence templates.Then, we generate 20,000 labeled sentences by filling these templates with the groundtruth attribute label.On the augmented user comment data, we train the user simulator.\n\nOur user simulator is implemented via a sequence-to-sequence model.The inputs of the user simulator are the differences on one attribute value between the candidate and desired items.Given the inputs, the user simulator generates a sentence describing the visual attribute difference between the candidate item and the desired item.We use two LSTMs as the encoder and decoder.The dimensionality of the latent code is set as 256.We use Adam as the optimizer, where the initial learning is set as 0.001 with batch size of 64.Note that for evaluating how the current recommended item's visual attributes satisfy the user's previous feedback, our user simulator on UT-Zappos50K only generates simple comments on the visual attribute difference between the candidate image and the desired image: we can calculate how many attributes violate the users' previous feedback based on the visual attribute groundtruth available in UT-Zappos50K.\n\nWe define four evaluation metrics: i) task success rate (SR@K), which is the success rate after after K interactions; ii) number of user interactions before success (NI); and iii) number of violated attributes (NV).In each user session, we assume the user aims to find items with a set of desired attribute values sampled from the dataset.We report results averaged over 100 sessions with standard error.We develop an RL baseline approach by ignoring the constraints (i.e., discriminator) in RCR.\n\nA major difference between our RL baseline approach and Guo, et al. [20] is that we consider the attributes in the model learning, while the attributes are ignored in [20].We compare RCR with the RL without constraints, as well as RL methods with naive constraints, i.e., naively using hard constraints.That is, we track all the visual attributes previously described by the user in this session, and make further recommendations based on the matching between them and the items in dataset.Analysis All models are trained for 100,000 iterations (user sessions), and the results with standard errors under different metrics are shown in Table 1.The proposed RCR framework shows consistent improvements on most metrics, compared with the baselines.The gap between RL with naive constraints and RCR demonstrate the learned constraint (discriminator) has better generalization.Figure 4 shows the metrics with standard errors of RL and the proposed RCR in the first 40,000 iterations.RCR shows much faster convergence than RL.The last subfigure shows the values of \u03bb with different number of samples.It is interesting to see that \u03bb increases at the initial stage because of too many violations.Then, with less violations, \u03bb decreases to a relatively small value as \u03bb = 0.04 and then remains stable, which is the automatically learned weight of the discriminator.Some examples in Figure 5 show how the constraint improves the recommendation.\n\n\nConstrained Text Generation\n\nExperimental Setup We use the Yelp review dataset [41] to validate the proposed methods.We split the data as 444,000, 63,500, and 127,000 sentences in the training, validation and test sets, respectively.The generator is trained on the Yelp dataset to generate reviews without sentiment labels.\n\nWe define the reward of the generated sentence as the probability of being real and the constraint is to generate positive reviews, i.e., the generator will receive a penalty if it generates negative reviews.The constraint is a neural network with a classification accuracy of 97.4% on the validation set, trained on sentences with the sentiment labels.We follow the strategy in [19,53] and adopt the BLEU score, referenced by test set with only positive reviews (test-BLEU) and themselves (self-BLEU) to evaluate the quality of generated samples.We also report the violation rate (VR), the percentage of generated negative reviews violating the constraint.Analysis As illustrated in Table 2, RCR achieves better test-BLEU scores than standard RL, demonstrating high-quality generated sentences.Further, RCR shows a little higher but reasonable self-BLEU scores, since we only generate sentences with positive sentiments, leading to lower diversity.Our proposed method shows much lower violation rate, demonstrating the effectiveness of RCR.Some randomly generated examples are shown in Table 3.\n\n\nConclusions\n\nMotivated by potential constraints in real-world tasks with RL training, and inspired by constrained policy optimization, we propose the RCR framework, where a neural network is parameterized and dynamically updated to represent constraints for RL training.By applying this new framework to constrained interactive recommendation and text generation, we demonstrate that our proposed model outperforms several baselines.The proposed method is a general framework, and can be extended to other applications, such as vision-and-dialog navigation [45].Future work also includes incorporating user historical information into the recommendation and on-device [8] interactive recommendation.\n\nA More details of the RCR Model\n\n\nA.1 Constrained Text Generation Model\n\nA content vector z x is given by an encoder Enc(\u2022), with inputs X \u223c D, i.e., z x = Enc(X).Based on z x , an LSTM-based [24] decoder G(\u2022) generates a new sentence \u1ef8 that is expected to be the same as the inputs.The auto-encoder can be trained by minimizing the following reconstruction loss:\nL ae (\u03b8) =E X\u223cD [\u2212 log p \u03b8 (X|z x )].(12)\nConstraint Discriminator: Style Classifier The constraint discriminator is trained to detect the violation of the constraints and we consider a classifier C \u03c6 as the discriminator.The classifier is trained to distinguish sentences with violation and non-violation set p r and p f respectively:\nL(\u03c6) = \u2212E (X)\u223cp f [log(C \u03c6 (X))] \u2212 E (X)\u223cpr [log(1 \u2212 C \u03c6 (X))] .(13)\nGeneral Discriminator: Language Model Language model can work as a general discriminator [49], which learns a conditional distribution over the current word given previous words in a sequence.Let p l ( \u0176 ) be the probability of a sentence \u0176 evaluated with the language model D l .We have L lm (\u03b8) =E X\u223cD, \u0176 \u223cp \u03b8 (zx) [\u2212 log p l ( \u0176 )] .\n\nA.2 Constrained Interactive Recommendation Constraint Discriminator We have several ways to build up the violation and non-violation sets for the training of the constraint discriminators in interactive recommendation.We can collect the datasets from two distributions as described in Assumption 1.\n\nAssumption 1 If a user is not satisfied with current recommendations based on her natural language feedback, then the current recommendation violates the user preference.Further, all desired items do not violate the corresponding user historical feedback.\n\nBesides, we can exploit huge-amount offline data, which is available in certain cases in real-world.\n\nBased on the attributes information, we can build up these two datasets.The performance of different ways to collect data and train the discriminator is similar in our case.\n\nUser Simulator We derive the templates from the real-world sentences collected from annotators.Some examples of the templates are\n\n\nB Ablation Study\n\nWe perform an ablation study to understand how \u03bb max affects the performance.To remove the affects by the discriminator in this ablation study, we experiment different values of \u03bb max without using the discriminator to reject the items sampled from the recommender.The results are shown in Figure 6.When \u03bb max \u2264 0.05, increasing \u03bb max leads to improved performance under NI, NV, SR.When \u03bb max > 0.05, only minor improvements can be observed by increasing \u03bb max .This matches the observation in Figure 4 that the value of \u03bb fluctuates around 0.04 after \u03bb being updated on about 1,800 samples.Besides, we observe that when \u03bb max = 0.01, the training is much more computationally expensive since the agent can not succeed and terminate the episode earlier in most cases.This experiment validates that our algorithm can adaptively find a suitable \u03bb to balance the weight between the reward and constraint, which leads to more efficient model learning.\n\n\n\nt 8 o 6 q u A U n I E L 4 I J r 0 A Z 3 o A O 6 A I M M P I N X 8 G Y 9 W S / W u / W x G K 1 Y 5 c 4 x + A P r 8 w d M 2 Z W m < / l a t e x i t > Constraint Penalty s t < l a t e x i t s h a 1 _ b a s e 6 4 = \" E q h c d 0 W C W a 7 e p y Y J f J S R O l 6 C c i 0 = \" > A A A B 9 X i c b V D L S s N A F L 3 x W e u r 6 t L N Y B F c l a S K u i y 4 c V n B P q C N Z T K d t E M n D 2 Z u l B L y H 2 5 c K O L W f 3 H n 3 z h p s 9 D W A w O H c + 7 l n j l e L I V G 2 / 6 2 V l b X 1 j c 2 S 1 v l 7 Z 3 d v f 3 K w W F b R 4 l i v M U i G a m u R z W X I u Q t F C h 5 N 1 a c B p 7 k H W 9 y k / u d R 6 6 0 i M J 7 n M b c D e g o F L 5 g F I 3 0 0 A 8 o j j 0 / 1 d k g x W x Q q d o 1 e w a y T J y C V K F A c 1 D 5 6 g 8 j l g Q 8 R C a p 1 j 3 H j t F N q U L B J M / K / U T z m L I J H f G e o S E N u H b T W e q M n B p l S P x I m R c i m a m / N 1 I a a D 0 N P D O Z p 9 S L X i 7 + 5 / U S 9 K / d V I R x g j x k 8 0 N + I g l\n\n\nFigure 2 :\n2\nFigure 2: An example of text-based interactive recommendation.\n\n\n\n\nt e x i t s h a 1 _ b a s e 6 4 = \" T R o v G 9 K a / 9 / H u 2 E k E 4 j I 8 7 r M P V M = \" > A A A B + H i c b V C 7 T s M w F L 0 p r 1 I e D T C y\n\n\nX\n\n< l a t e x i t s h a 1 _ b a s e 6 4 = \" 8 b o 0K U K z P B 2 1 U / r 5 / 1 M H P A d u Q k 0 = \" > A A A B + H i c b V D L S g M x F L 3 j s 9 Z H q y 7 d B I v g q s x U U Z c F N y 4 r 2 A e 0 Q 8 l k M m 1 o J h m S j F C H f o k b F 4 q 4 9 V P c + T d m 2 l l o 6 4 G Q w z n 3 k p M T J J x p 4 7 r f z t r 6 x u b W d m m n v L u 3 f 1 C p H h 5 1 t E w V o W 0 i u V S 9 A G v K m a B t w w y n v U RR H A e c d o P J b e 5 3 H 6 n S T I o H M 0 2 o H + O R Y B E j 2 F h p W K 1 k g 0 D y U E 9 j e 6 H e b\n\n\nFigure 3 :\n3\nFigure 3: Overview of the constrained textgeneration model: L ae is the reconstruction term from the VAE in pretraining.The constraint will give a penalty when generated text violates the constraint discriminator.\n\n\nFigure 4 :\n4\nFigure 4: Number of Interactions (NI), Number of Violations (NV), Success Rate@30 (SR@30) with respect to training iterations and the values of \u03bb in RCR with respect to number of samples.The RL method converges much slower than the RCR.\n\n\nFigure 5 :\n5\nFigure 5: Three use cases, from logged experimental results.(a) and (b) are successful use cases by RCR.(c) is not successful by RL, which demonstrate the common challenge of failing to meet the constraint in recommendation.\n\n\nTable 1 :\n1\nComparisons between different approaches.Except the row of RCR (seen) reporting results on training data, all the results are on the test data with unseen items.\nRL (Unseen) RL + Naive (Unseen) RCR (Unseen) RCR (Seen)SR@10 \u2191 SR@20 \u2191 SR@30 \u2191 19% 44% 63% 26.75 \u00b1 1.67 70.02 \u00b1 6.20 NI \u2193 NV \u2193 52% 83% 94% 12.72 \u00b1 0.93 16.47 \u00b1 2.75 74% 86% 94% 10.91 \u00b1 1.06 11.32 \u00b1 1.98 78% 91% 92% 10.34 \u00b1 1.18 12.25 \u00b1 2.99\n\nTable 2 :\n2\nComparison between RCR and standard RL for constrained text generation on Yelp.\nTest-BLEU-2345Self-BLEU-234VRRL0.807 0.622 0.469 0.3760.658 0.315 0.098 40.36%RCR (ours)0.840 0.651 0.492 0.3920.683 0.348 0.151 10.49%RL without ConstraintsRCRthe ceiling is low , the place smells awful , gambling sucked .every dish was so absolutely delicious and seasoned perfectly .i have been here a few times and each time has been great !he is the most compassionate vet i have ever met .bad food , bad service , takes too much time .compared to other us cities , this place ranks very generous in my book .food was good , but overall it was a very bad dining experience .then you already know what this tastes like .my entree was a sea bass which was well prepared and tasty .thank you my friends for letting us know this finest dining place in lv .the food is delicious and very consistently so .the waitress was horrible and came by maybe once every hour .\ngreat service and the food was excellent .the lines can get out of hand sometimes but it goes pretty quick .\n\n\nTable 3 :\n3\nRandomly selected examples of text generation by two methods.\n\n\n\n\nThe visual attributes used in our evaluation include shoes category, shoes subcategory, heel height, closure, gender, and toe style.There are 4, 21, 7, 18, 8, 19 classes for these attributes, respectively.\n\u2022 Please show me more \u2022 I am looking for . \u2022 I prefer \u2022 I want the shoes with \u2022 . . ...closure.\nFor simplicity, we here only introduce one constraint function; in practice, there may be many constraint functions.\nAvailable at https://github.com/hongwang600/image_tag_dataset/tree/master/tags.\nC Generated ExamplesWe show some examples of the generated feedback by the user simulator in Figure7and Table4.To evaluating how the recommended item's visual attributes satisfy the user's previous feedback, our simulator only generates simple comments on the visual attribute difference between the candidate image and the desired image: we can calculate how many attributes violate the users' previous feedback based on the visual attribute groundtruth available in UT-Zappos50K.Show me more shoes with laced up closure.I prefer shoes with round toe.I am looking for ankle.I want the shoes with ankle strap.I prefer 3in -3 3/4in.Show me more shoes with close toe.\nConstrained policy optimization. J Achiam, D Held, A Tamar, P Abbeel, ICML. 2017\n\nAsking clarifying questions in open-domain information-seeking conversations. M Aliannejadi, H Zamani, F Crestani, W B Croft, SIGIR. 2019\n\nConstrained Markov decision processes. E Altman, 1999CRC Press\n\nNonlinear programming. D P Bertsekas, Journal of the Operational Research Society. 1997\n\nAn actor-critic algorithm for constrained markov decision processes. V S Borkar, Systems & control letters. 2005\n\nAn empirical evaluation of thompson sampling. O Chapelle, L Li, Advances in neural information processing systems. 2011\n\nMaximum-likelihood augmented discrete generative adversarial networks. T Che, Y Li, R Zhang, R D Hjelm, W Li, Y Song, Y Bengio, CoRR2017\n\nLearning compressed sentence representations for on-device text processing. P Cheng, D Shen, D Sundararaman, X Zhang, Q Yang, M Tang, A Celikyilmaz, L Carin, ACL. 2019\n\nRisk-sensitive and robust decision-making: a cvar optimization approach. Y Chow, A Tamar, S Mannor, M Pavone, NIPS. 2015\n\nQ&r: A two-stage approach toward interactive recommendation. K Christakopoulou, A Beutel, R Li, S Jain, E H Chi, KDD. ACM2018\n\nTowards conversational recommender systems. K Christakopoulou, F Radlinski, K Hofmann, KDD. ACM2016\n\nG Dalal, K Dvijotham, M Vecerik, T Hester, C Paduraru, Y Tassa, arXiv:1801.08757Safe exploration in continuous action spaces. 2018arXiv preprint\n\n. P Dhariwal, C Hesse, O Klimov, A Nichol, M Plappert, A Radford, J Schulman, S Sidor, Y Wu, P Zhokhov, 2017Openai baselines\n\nPolicy gradients with variance related risk criteria. D Di Castro, A Tamar, S Mannor, arXiv:1206.64042012arXiv preprint\n\nMaskgan: Better text generation via filling in the _. ICLR. W Fedus, I Goodfellow, A M Dai, 2018\n\nGenerative adversarial nets. I Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, Y Bengio, NIPS. 2014\n\nConverse-et-impera: Exploiting deep learning and hierarchical reinforcement learning for conversational recommender systems. C Greco, A Suglia, P Basile, G Semeraro, Conference of the Italian Association for Artificial Intelligence. Springer2017\n\nDeep reinforcement learning for robotic manipulation with asynchronous off-policy updates. S Gu, E Holly, T Lillicrap, S Levine, ICRA. 2017\n\nLong text generation via adversarial training with leaked information. J Guo, S Lu, H Cai, W Zhang, Y Yu, J Wang, AAAI. 2017\n\nDialog-based interactive image retrieval. X Guo, H Wu, Y Cheng, S Rennie, G Tesauro, R Feris, NIPS. 2018\n\nThe fashion iq dataset: Retrieving images by combining side information and relative natural language feedback. X Guo, H Wu, Y Gao, S Rennie, R Feris, arXiv:1905.127942019\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, CVPR. 2016\n\n. A Hill, A Raffin, M Ernestus, A Gleave, A Kanervisto, R Traore, P Dhariwal, C Hesse, O Klimov, A Nichol, M Plappert, A Radford, J Schulman, S Sidor, Y Wu, Stable baselines. 2018\n\nLong short-term memory. S Hochreiter, J Schmidhuber, Neural computation. 981997\n\nToward controlled generation of text. Z Hu, Z Yang, X Liang, R Salakhutdinov, E P Xing, ICML. 2017\n\nAdam: A method for stochastic optimization. D Kingma, J Ba, ICLR. 2014\n\nControl and optimization meet the smart power grid: Scheduling of power demands for optimal energy management. I Koutsopoulos, L Tassiulas, ICECN2011\n\nWhittlesearch: Image search with relative attribute feedback. A Kovashka, D Parikh, K Grauman, CVPR. 2012\n\nGans for sequences of discrete elements with the gumbel-softmax distribution. M J Kusner, J Hern\u00e1ndez-Lobato, Miguel, arXiv:1611.040512016arXiv preprint\n\nCascading bandits: Learning to rank in the cascade model. B Kveton, C Szepesvari, Z Wen, A Ashkan, ICML. 2015\n\nEstimation-actionreflection: Towards deep interaction between conversational and recommender systems. W Lei, X He, Y Miao, Q Wu, R Hong, M.-Y Kan, T.-S Chua, WSDM. 2018\n\nGuided policy search. S Levine, V Koltun, ICML. 2013\n\nA contextual-bandit approach to personalized news article recommendation. L Li, W Chu, J Langford, R E Schapire, WWW. ACM2010\n\nAdversarial ranking for language generation. K Lin, D Li, X He, Z Zhang, M.-T Sun, NIPS. 2017\n\nFeature selection methods for conversational recommender systems. N Mirzadeh, F Ricci, M Bansal, IEEE International Conference on e-Technology, e-Commerce and e-Service. 2005\n\nRelgan: Relational generative adversarial networks for text generation. W Nie, N Narodytska, A Patel, In ICLR. 2018\n\nRelative attributes. D Parikh, K Grauman, ICCV. 2011\n\nDeepmimic: Example-guided deep reinforcement learning of physics-based character skills. X B Peng, P Abbeel, S Levine, M Van De Panne, ACM Transactions on Graphics (TOG). 2018\n\nMarkov Decision Processes.: Discrete Stochastic Dynamic Programming. M L Puterman, 2014John Wiley & Sons\n\nRelevance feedback: a power tool for interactive content-based image retrieval. Y Rui, T S Huang, M Ortega, S Mehrotra, IEEE Transactions on circuits and systems for video technology. 1998\n\nStyle transfer from non-parallel text by crossalignment. T Shen, T Lei, R Barzilay, T Jaakkola, NIPS. 2017\n\nConversational recommender system. Y Sun, Y Zhang, SIGIR. \n\nReinforcement learning: An introduction. R S Sutton, A G Barto, 2018MIT press\n\nReward constrained policy optimization. C Tessler, D J Mankowitz, S Mannor, ICLR. 2019\n\nVision-and-dialog navigation. J Thomason, M Murray, M Cakmak, L Zettlemoyer, 2019\n\nInteractive search in image retrieval: a survey. B Thomee, M S Lew, International Journal of Multimedia Information Retrieval. 2012\n\nTopic-guided variational autoencoders for text generation. W Wang, Z Gan, H Xu, R Zhang, G Wang, D Shen, C Chen, L Carin, NAACL. 2019\n\nWillhunter: interactive image retrieval with multilevel relevance. H Wu, H Lu, S Ma, ICPR. 2004\n\nUnsupervised text style transfer using language models as discriminators. Z Yang, Z Hu, C Dyer, E P Xing, T Berg-Kirkpatrick, NeurIPS2018\n\nFine-grained comparisons with attributes. A Yu, K Grauman, Visual Attributes. 2017\n\nA. Fine-grained visual comparisons with local learning. G K Yu, CVPR. 2014\n\nSemantic jitter: Dense supervision for visual comparisons via synthetic images. G K Yu, A , ICCV. 2014\n\nSeqgan: Sequence generative adversarial nets with policy gradient. L Yu, W Zhang, J Wang, Y Yu, AAAI. 2017\n\nVision-language recommendation via attribute augmented multimodal reinforcement learning. T Yu, Y Shen, R Zhang, X Zeng, H Jin, ACM Multimedia. 2019\n\nImproving rl-based sequence generation by modeling the distant future. R Zhang, C Chen, Z Gan, W Wang, L Chen, D Shen, G Wang, L Carin, RSDM, ICML. 2019\n\nAdversarial feature matching for text generation. Y Zhang, Z Gan, K Fan, Z Chen, R Henao, D Shen, L Carin, ICML. 2017\n\nWhat to do next: Modeling user behaviors by time-lstm. Y Zhu, H Li, Y Liao, B Wang, Z Guan, H Liu, D Cai, IJCAI. 2017\n", "annotations": {"author": "[{\"end\":140,\"start\":90},{\"end\":176,\"start\":141},{\"end\":215,\"start\":177},{\"end\":255,\"start\":216},{\"end\":294,\"start\":256},{\"end\":328,\"start\":295}]", "publisher": null, "author_last_name": "[{\"end\":101,\"start\":96},{\"end\":148,\"start\":144},{\"end\":187,\"start\":183},{\"end\":227,\"start\":224},{\"end\":269,\"start\":265},{\"end\":309,\"start\":304}]", "author_first_name": "[{\"end\":95,\"start\":90},{\"end\":143,\"start\":141},{\"end\":182,\"start\":177},{\"end\":223,\"start\":216},{\"end\":264,\"start\":256},{\"end\":303,\"start\":295}]", "author_affiliation": "[{\"end\":139,\"start\":123},{\"end\":175,\"start\":150},{\"end\":214,\"start\":189},{\"end\":254,\"start\":229},{\"end\":293,\"start\":271},{\"end\":327,\"start\":311}]", "title": "[{\"end\":77,\"start\":1},{\"end\":405,\"start\":329}]", "venue": null, "abstract": "[{\"end\":1379,\"start\":474}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b5\"},\"end\":1905,\"start\":1902},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":1908,\"start\":1905},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":1911,\"start\":1908},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2415,\"start\":2411},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2418,\"start\":2415},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":4662,\"start\":4658},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":5017,\"start\":5013},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5287,\"start\":5284},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5596,\"start\":5593},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6143,\"start\":6139},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7283,\"start\":7279},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7286,\"start\":7283},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10714,\"start\":10711},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10716,\"start\":10714},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12421,\"start\":12418},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":12892,\"start\":12888},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":13218,\"start\":13214},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":13313,\"start\":13309},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":13464,\"start\":13461},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":13872,\"start\":13868},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":14563,\"start\":14559},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":15019,\"start\":15016},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":15095,\"start\":15092},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":15384,\"start\":15380},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":15503,\"start\":15499},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":16349,\"start\":16345},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":16503,\"start\":16499},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":16506,\"start\":16503},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":21789,\"start\":21785},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":21934,\"start\":21930},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":22191,\"start\":22187},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":22561,\"start\":22558},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":22649,\"start\":22645},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":22669,\"start\":22666},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":22672,\"start\":22669},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":22739,\"start\":22736},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":22742,\"start\":22739},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":22840,\"start\":22837},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":22878,\"start\":22874},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":22957,\"start\":22953},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":22960,\"start\":22957},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":22963,\"start\":22960},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":23413,\"start\":23409},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":23416,\"start\":23413},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":23545,\"start\":23542},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":23548,\"start\":23545},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":23551,\"start\":23548},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":23554,\"start\":23551},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":23557,\"start\":23554},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":23560,\"start\":23557},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":23563,\"start\":23560},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":23579,\"start\":23575},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":23582,\"start\":23579},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":23585,\"start\":23582},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":23588,\"start\":23585},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":24149,\"start\":24145},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":24246,\"start\":24242},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":24249,\"start\":24246},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":24287,\"start\":24283},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":24290,\"start\":24287},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":24293,\"start\":24290},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":24467,\"start\":24463},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":24801,\"start\":24797},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":25157,\"start\":25154},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":25160,\"start\":25157},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":25163,\"start\":25160},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":25166,\"start\":25163},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":25169,\"start\":25166},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":25172,\"start\":25169},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":25416,\"start\":25412},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":25489,\"start\":25485},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":25506,\"start\":25502},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":25509,\"start\":25506},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":25623,\"start\":25619},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":26135,\"start\":26131},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":26138,\"start\":26135},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":27339,\"start\":27335},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":27491,\"start\":27487},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":28391,\"start\":28387},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":28544,\"start\":28540},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":28993,\"start\":28989},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":31875,\"start\":31871},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":31974,\"start\":31970},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":33324,\"start\":33320},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":33949,\"start\":33945},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":33952,\"start\":33949},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":35225,\"start\":35221},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":35335,\"start\":35332},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":35561,\"start\":35557},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":36227,\"start\":36223}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":39358,\"start\":38404},{\"attributes\":{\"id\":\"fig_1\"},\"end\":39436,\"start\":39359},{\"attributes\":{\"id\":\"fig_2\"},\"end\":39592,\"start\":39437},{\"attributes\":{\"id\":\"fig_3\"},\"end\":40115,\"start\":39593},{\"attributes\":{\"id\":\"fig_4\"},\"end\":40344,\"start\":40116},{\"attributes\":{\"id\":\"fig_5\"},\"end\":40596,\"start\":40345},{\"attributes\":{\"id\":\"fig_6\"},\"end\":40836,\"start\":40597},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":41252,\"start\":40837},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":42322,\"start\":41253},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":42398,\"start\":42323},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":42703,\"start\":42399}]", "paragraph": "[{\"end\":2719,\"start\":1395},{\"end\":3703,\"start\":2721},{\"end\":4276,\"start\":3705},{\"end\":5018,\"start\":4316},{\"end\":5163,\"start\":5062},{\"end\":5503,\"start\":5165},{\"end\":5597,\"start\":5542},{\"end\":7124,\"start\":5703},{\"end\":7646,\"start\":7144},{\"end\":9678,\"start\":9607},{\"end\":10321,\"start\":9680},{\"end\":10859,\"start\":10323},{\"end\":11190,\"start\":10913},{\"end\":11528,\"start\":11261},{\"end\":11730,\"start\":11530},{\"end\":12087,\"start\":11732},{\"end\":12212,\"start\":12166},{\"end\":12486,\"start\":12260},{\"end\":12896,\"start\":12569},{\"end\":13118,\"start\":13041},{\"end\":13219,\"start\":13120},{\"end\":13698,\"start\":13221},{\"end\":13938,\"start\":13717},{\"end\":14784,\"start\":14225},{\"end\":15154,\"start\":14786},{\"end\":16062,\"start\":15172},{\"end\":16226,\"start\":16064},{\"end\":17978,\"start\":16269},{\"end\":18112,\"start\":18023},{\"end\":19394,\"start\":18909},{\"end\":21587,\"start\":20985},{\"end\":22471,\"start\":21656},{\"end\":22781,\"start\":22488},{\"end\":23169,\"start\":22783},{\"end\":24024,\"start\":23171},{\"end\":25096,\"start\":24026},{\"end\":25813,\"start\":25098},{\"end\":26014,\"start\":25829},{\"end\":27386,\"start\":26056},{\"end\":27864,\"start\":27388},{\"end\":28266,\"start\":28011},{\"end\":28760,\"start\":28268},{\"end\":29222,\"start\":28762},{\"end\":30368,\"start\":29224},{\"end\":31303,\"start\":30370},{\"end\":31801,\"start\":31305},{\"end\":33238,\"start\":31803},{\"end\":33564,\"start\":33270},{\"end\":34661,\"start\":33566},{\"end\":35363,\"start\":34677},{\"end\":35396,\"start\":35365},{\"end\":35728,\"start\":35438},{\"end\":36064,\"start\":35771},{\"end\":36470,\"start\":36134},{\"end\":36770,\"start\":36472},{\"end\":37027,\"start\":36772},{\"end\":37129,\"start\":37029},{\"end\":37304,\"start\":37131},{\"end\":37435,\"start\":37306},{\"end\":38403,\"start\":37456},{\"end\":39357,\"start\":38407},{\"end\":39435,\"start\":39373},{\"end\":39591,\"start\":39440},{\"end\":40114,\"start\":39597},{\"end\":40343,\"start\":40130},{\"end\":40595,\"start\":40359},{\"end\":40835,\"start\":40611},{\"end\":41011,\"start\":40850},{\"end\":41345,\"start\":41266},{\"end\":42321,\"start\":42213},{\"end\":42397,\"start\":42336},{\"end\":42607,\"start\":42402}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":5061,\"start\":5019},{\"attributes\":{\"id\":\"formula_1\"},\"end\":5541,\"start\":5504},{\"attributes\":{\"id\":\"formula_2\"},\"end\":5636,\"start\":5598},{\"attributes\":{\"id\":\"formula_3\"},\"end\":8578,\"start\":7647},{\"attributes\":{\"id\":\"formula_4\"},\"end\":9425,\"start\":8578},{\"attributes\":{\"id\":\"formula_5\"},\"end\":9606,\"start\":9425},{\"attributes\":{\"id\":\"formula_6\"},\"end\":11260,\"start\":11191},{\"attributes\":{\"id\":\"formula_7\"},\"end\":12165,\"start\":12088},{\"attributes\":{\"id\":\"formula_8\"},\"end\":12259,\"start\":12213},{\"attributes\":{\"id\":\"formula_9\"},\"end\":12568,\"start\":12487},{\"attributes\":{\"id\":\"formula_10\"},\"end\":12987,\"start\":12897},{\"attributes\":{\"id\":\"formula_11\"},\"end\":13040,\"start\":12987},{\"attributes\":{\"id\":\"formula_12\"},\"end\":13994,\"start\":13939},{\"attributes\":{\"id\":\"formula_13\"},\"end\":14043,\"start\":13994},{\"attributes\":{\"id\":\"formula_14\"},\"end\":14099,\"start\":14043},{\"attributes\":{\"id\":\"formula_15\"},\"end\":14224,\"start\":14099},{\"attributes\":{\"id\":\"formula_16\"},\"end\":16268,\"start\":16227},{\"attributes\":{\"id\":\"formula_17\"},\"end\":18908,\"start\":18113},{\"attributes\":{\"id\":\"formula_18\"},\"end\":20984,\"start\":19395},{\"attributes\":{\"id\":\"formula_19\"},\"end\":21655,\"start\":21588},{\"attributes\":{\"id\":\"formula_20\"},\"end\":28010,\"start\":27865},{\"attributes\":{\"id\":\"formula_21\"},\"end\":35770,\"start\":35729},{\"attributes\":{\"id\":\"formula_22\"},\"end\":36133,\"start\":36065}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":32446,\"start\":32445},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":34257,\"start\":34256},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":34660,\"start\":34659}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1393,\"start\":1381},{\"attributes\":{\"n\":\"2\"},\"end\":4289,\"start\":4279},{\"attributes\":{\"n\":\"2.1\"},\"end\":4314,\"start\":4292},{\"attributes\":{\"n\":\"2.2\"},\"end\":5701,\"start\":5638},{\"attributes\":{\"n\":\"3\"},\"end\":7142,\"start\":7127},{\"attributes\":{\"n\":\"3.1\"},\"end\":10911,\"start\":10862},{\"end\":13715,\"start\":13701},{\"attributes\":{\"n\":\"3.2\"},\"end\":15170,\"start\":15157},{\"attributes\":{\"n\":\"3.3\"},\"end\":18021,\"start\":17981},{\"attributes\":{\"n\":\"4\"},\"end\":22486,\"start\":22474},{\"attributes\":{\"n\":\"5\"},\"end\":25827,\"start\":25816},{\"attributes\":{\"n\":\"5.1\"},\"end\":26054,\"start\":26017},{\"attributes\":{\"n\":\"5.2\"},\"end\":33268,\"start\":33241},{\"attributes\":{\"n\":\"6\"},\"end\":34675,\"start\":34664},{\"end\":35436,\"start\":35399},{\"end\":37454,\"start\":37438},{\"end\":39370,\"start\":39360},{\"end\":39595,\"start\":39594},{\"end\":40127,\"start\":40117},{\"end\":40356,\"start\":40346},{\"end\":40608,\"start\":40598},{\"end\":40847,\"start\":40838},{\"end\":41263,\"start\":41254},{\"end\":42333,\"start\":42324}]", "table": "[{\"end\":41252,\"start\":41012},{\"end\":42212,\"start\":41346},{\"end\":42703,\"start\":42608}]", "figure_caption": "[{\"end\":39358,\"start\":38406},{\"end\":39436,\"start\":39372},{\"end\":39592,\"start\":39439},{\"end\":40115,\"start\":39596},{\"end\":40344,\"start\":40129},{\"end\":40596,\"start\":40358},{\"end\":40836,\"start\":40610},{\"end\":41012,\"start\":40849},{\"end\":41346,\"start\":41265},{\"end\":42398,\"start\":42335},{\"end\":42608,\"start\":42401}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":7307,\"start\":7306},{\"end\":9688,\"start\":9687},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":10320,\"start\":10319},{\"end\":10858,\"start\":10857},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":22136,\"start\":22135},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":32684,\"start\":32683},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":33185,\"start\":33184},{\"end\":37754,\"start\":37753},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":37958,\"start\":37957}]", "bib_author_first_name": "[{\"end\":43601,\"start\":43600},{\"end\":43611,\"start\":43610},{\"end\":43619,\"start\":43618},{\"end\":43628,\"start\":43627},{\"end\":43728,\"start\":43727},{\"end\":43743,\"start\":43742},{\"end\":43753,\"start\":43752},{\"end\":43765,\"start\":43764},{\"end\":43767,\"start\":43766},{\"end\":43828,\"start\":43827},{\"end\":43876,\"start\":43875},{\"end\":43878,\"start\":43877},{\"end\":44011,\"start\":44010},{\"end\":44013,\"start\":44012},{\"end\":44102,\"start\":44101},{\"end\":44114,\"start\":44113},{\"end\":44248,\"start\":44247},{\"end\":44255,\"start\":44254},{\"end\":44261,\"start\":44260},{\"end\":44270,\"start\":44269},{\"end\":44272,\"start\":44271},{\"end\":44281,\"start\":44280},{\"end\":44287,\"start\":44286},{\"end\":44295,\"start\":44294},{\"end\":44391,\"start\":44390},{\"end\":44400,\"start\":44399},{\"end\":44408,\"start\":44407},{\"end\":44424,\"start\":44423},{\"end\":44433,\"start\":44432},{\"end\":44441,\"start\":44440},{\"end\":44449,\"start\":44448},{\"end\":44464,\"start\":44463},{\"end\":44557,\"start\":44556},{\"end\":44565,\"start\":44564},{\"end\":44574,\"start\":44573},{\"end\":44584,\"start\":44583},{\"end\":44667,\"start\":44666},{\"end\":44686,\"start\":44685},{\"end\":44696,\"start\":44695},{\"end\":44702,\"start\":44701},{\"end\":44710,\"start\":44709},{\"end\":44712,\"start\":44711},{\"end\":44777,\"start\":44776},{\"end\":44796,\"start\":44795},{\"end\":44809,\"start\":44808},{\"end\":44834,\"start\":44833},{\"end\":44843,\"start\":44842},{\"end\":44856,\"start\":44855},{\"end\":44867,\"start\":44866},{\"end\":44877,\"start\":44876},{\"end\":44889,\"start\":44888},{\"end\":44982,\"start\":44981},{\"end\":44994,\"start\":44993},{\"end\":45003,\"start\":45002},{\"end\":45013,\"start\":45012},{\"end\":45023,\"start\":45022},{\"end\":45035,\"start\":45034},{\"end\":45046,\"start\":45045},{\"end\":45058,\"start\":45057},{\"end\":45067,\"start\":45066},{\"end\":45073,\"start\":45072},{\"end\":45160,\"start\":45159},{\"end\":45163,\"start\":45161},{\"end\":45173,\"start\":45172},{\"end\":45182,\"start\":45181},{\"end\":45287,\"start\":45286},{\"end\":45296,\"start\":45295},{\"end\":45310,\"start\":45309},{\"end\":45312,\"start\":45311},{\"end\":45354,\"start\":45353},{\"end\":45368,\"start\":45367},{\"end\":45385,\"start\":45384},{\"end\":45394,\"start\":45393},{\"end\":45400,\"start\":45399},{\"end\":45416,\"start\":45415},{\"end\":45425,\"start\":45424},{\"end\":45438,\"start\":45437},{\"end\":45585,\"start\":45584},{\"end\":45594,\"start\":45593},{\"end\":45604,\"start\":45603},{\"end\":45614,\"start\":45613},{\"end\":45798,\"start\":45797},{\"end\":45804,\"start\":45803},{\"end\":45813,\"start\":45812},{\"end\":45826,\"start\":45825},{\"end\":45919,\"start\":45918},{\"end\":45926,\"start\":45925},{\"end\":45932,\"start\":45931},{\"end\":45939,\"start\":45938},{\"end\":45948,\"start\":45947},{\"end\":45954,\"start\":45953},{\"end\":46016,\"start\":46015},{\"end\":46023,\"start\":46022},{\"end\":46029,\"start\":46028},{\"end\":46038,\"start\":46037},{\"end\":46048,\"start\":46047},{\"end\":46059,\"start\":46058},{\"end\":46192,\"start\":46191},{\"end\":46199,\"start\":46198},{\"end\":46205,\"start\":46204},{\"end\":46212,\"start\":46211},{\"end\":46222,\"start\":46221},{\"end\":46299,\"start\":46298},{\"end\":46305,\"start\":46304},{\"end\":46314,\"start\":46313},{\"end\":46321,\"start\":46320},{\"end\":46342,\"start\":46341},{\"end\":46350,\"start\":46349},{\"end\":46360,\"start\":46359},{\"end\":46372,\"start\":46371},{\"end\":46382,\"start\":46381},{\"end\":46396,\"start\":46395},{\"end\":46406,\"start\":46405},{\"end\":46418,\"start\":46417},{\"end\":46427,\"start\":46426},{\"end\":46437,\"start\":46436},{\"end\":46447,\"start\":46446},{\"end\":46459,\"start\":46458},{\"end\":46470,\"start\":46469},{\"end\":46482,\"start\":46481},{\"end\":46491,\"start\":46490},{\"end\":46545,\"start\":46544},{\"end\":46559,\"start\":46558},{\"end\":46640,\"start\":46639},{\"end\":46646,\"start\":46645},{\"end\":46654,\"start\":46653},{\"end\":46663,\"start\":46662},{\"end\":46680,\"start\":46679},{\"end\":46682,\"start\":46681},{\"end\":46746,\"start\":46745},{\"end\":46756,\"start\":46755},{\"end\":46885,\"start\":46884},{\"end\":46901,\"start\":46900},{\"end\":46987,\"start\":46986},{\"end\":46999,\"start\":46998},{\"end\":47009,\"start\":47008},{\"end\":47110,\"start\":47109},{\"end\":47112,\"start\":47111},{\"end\":47122,\"start\":47121},{\"end\":47244,\"start\":47243},{\"end\":47254,\"start\":47253},{\"end\":47268,\"start\":47267},{\"end\":47275,\"start\":47274},{\"end\":47399,\"start\":47398},{\"end\":47406,\"start\":47405},{\"end\":47412,\"start\":47411},{\"end\":47420,\"start\":47419},{\"end\":47426,\"start\":47425},{\"end\":47437,\"start\":47433},{\"end\":47447,\"start\":47443},{\"end\":47489,\"start\":47488},{\"end\":47499,\"start\":47498},{\"end\":47595,\"start\":47594},{\"end\":47601,\"start\":47600},{\"end\":47608,\"start\":47607},{\"end\":47620,\"start\":47619},{\"end\":47622,\"start\":47621},{\"end\":47693,\"start\":47692},{\"end\":47700,\"start\":47699},{\"end\":47706,\"start\":47705},{\"end\":47712,\"start\":47711},{\"end\":47724,\"start\":47720},{\"end\":47809,\"start\":47808},{\"end\":47821,\"start\":47820},{\"end\":47830,\"start\":47829},{\"end\":47991,\"start\":47990},{\"end\":47998,\"start\":47997},{\"end\":48012,\"start\":48011},{\"end\":48057,\"start\":48056},{\"end\":48067,\"start\":48066},{\"end\":48179,\"start\":48178},{\"end\":48181,\"start\":48180},{\"end\":48189,\"start\":48188},{\"end\":48199,\"start\":48198},{\"end\":48209,\"start\":48208},{\"end\":48336,\"start\":48335},{\"end\":48338,\"start\":48337},{\"end\":48453,\"start\":48452},{\"end\":48460,\"start\":48459},{\"end\":48462,\"start\":48461},{\"end\":48471,\"start\":48470},{\"end\":48481,\"start\":48480},{\"end\":48620,\"start\":48619},{\"end\":48628,\"start\":48627},{\"end\":48635,\"start\":48634},{\"end\":48647,\"start\":48646},{\"end\":48706,\"start\":48705},{\"end\":48713,\"start\":48712},{\"end\":48772,\"start\":48771},{\"end\":48774,\"start\":48773},{\"end\":48784,\"start\":48783},{\"end\":48786,\"start\":48785},{\"end\":48850,\"start\":48849},{\"end\":48861,\"start\":48860},{\"end\":48863,\"start\":48862},{\"end\":48876,\"start\":48875},{\"end\":48928,\"start\":48927},{\"end\":48940,\"start\":48939},{\"end\":48950,\"start\":48949},{\"end\":48960,\"start\":48959},{\"end\":49030,\"start\":49029},{\"end\":49040,\"start\":49039},{\"end\":49042,\"start\":49041},{\"end\":49173,\"start\":49172},{\"end\":49181,\"start\":49180},{\"end\":49188,\"start\":49187},{\"end\":49194,\"start\":49193},{\"end\":49203,\"start\":49202},{\"end\":49211,\"start\":49210},{\"end\":49219,\"start\":49218},{\"end\":49227,\"start\":49226},{\"end\":49316,\"start\":49315},{\"end\":49322,\"start\":49321},{\"end\":49328,\"start\":49327},{\"end\":49420,\"start\":49419},{\"end\":49428,\"start\":49427},{\"end\":49434,\"start\":49433},{\"end\":49442,\"start\":49441},{\"end\":49444,\"start\":49443},{\"end\":49452,\"start\":49451},{\"end\":49527,\"start\":49526},{\"end\":49533,\"start\":49532},{\"end\":49625,\"start\":49624},{\"end\":49627,\"start\":49626},{\"end\":49725,\"start\":49724},{\"end\":49727,\"start\":49726},{\"end\":49733,\"start\":49732},{\"end\":49816,\"start\":49815},{\"end\":49822,\"start\":49821},{\"end\":49831,\"start\":49830},{\"end\":49839,\"start\":49838},{\"end\":49947,\"start\":49946},{\"end\":49953,\"start\":49952},{\"end\":49961,\"start\":49960},{\"end\":49970,\"start\":49969},{\"end\":49978,\"start\":49977},{\"end\":50078,\"start\":50077},{\"end\":50087,\"start\":50086},{\"end\":50095,\"start\":50094},{\"end\":50102,\"start\":50101},{\"end\":50110,\"start\":50109},{\"end\":50118,\"start\":50117},{\"end\":50126,\"start\":50125},{\"end\":50134,\"start\":50133},{\"end\":50211,\"start\":50210},{\"end\":50220,\"start\":50219},{\"end\":50227,\"start\":50226},{\"end\":50234,\"start\":50233},{\"end\":50242,\"start\":50241},{\"end\":50251,\"start\":50250},{\"end\":50259,\"start\":50258},{\"end\":50335,\"start\":50334},{\"end\":50342,\"start\":50341},{\"end\":50348,\"start\":50347},{\"end\":50356,\"start\":50355},{\"end\":50364,\"start\":50363},{\"end\":50372,\"start\":50371},{\"end\":50379,\"start\":50378}]", "bib_author_last_name": "[{\"end\":43608,\"start\":43602},{\"end\":43616,\"start\":43612},{\"end\":43625,\"start\":43620},{\"end\":43635,\"start\":43629},{\"end\":43740,\"start\":43729},{\"end\":43750,\"start\":43744},{\"end\":43762,\"start\":43754},{\"end\":43773,\"start\":43768},{\"end\":43835,\"start\":43829},{\"end\":43888,\"start\":43879},{\"end\":44020,\"start\":44014},{\"end\":44111,\"start\":44103},{\"end\":44117,\"start\":44115},{\"end\":44252,\"start\":44249},{\"end\":44258,\"start\":44256},{\"end\":44267,\"start\":44262},{\"end\":44278,\"start\":44273},{\"end\":44284,\"start\":44282},{\"end\":44292,\"start\":44288},{\"end\":44302,\"start\":44296},{\"end\":44397,\"start\":44392},{\"end\":44405,\"start\":44401},{\"end\":44421,\"start\":44409},{\"end\":44430,\"start\":44425},{\"end\":44438,\"start\":44434},{\"end\":44446,\"start\":44442},{\"end\":44461,\"start\":44450},{\"end\":44470,\"start\":44465},{\"end\":44562,\"start\":44558},{\"end\":44571,\"start\":44566},{\"end\":44581,\"start\":44575},{\"end\":44591,\"start\":44585},{\"end\":44683,\"start\":44668},{\"end\":44693,\"start\":44687},{\"end\":44699,\"start\":44697},{\"end\":44707,\"start\":44703},{\"end\":44716,\"start\":44713},{\"end\":44793,\"start\":44778},{\"end\":44806,\"start\":44797},{\"end\":44817,\"start\":44810},{\"end\":44840,\"start\":44835},{\"end\":44853,\"start\":44844},{\"end\":44864,\"start\":44857},{\"end\":44874,\"start\":44868},{\"end\":44886,\"start\":44878},{\"end\":44895,\"start\":44890},{\"end\":44991,\"start\":44983},{\"end\":45000,\"start\":44995},{\"end\":45010,\"start\":45004},{\"end\":45020,\"start\":45014},{\"end\":45032,\"start\":45024},{\"end\":45043,\"start\":45036},{\"end\":45055,\"start\":45047},{\"end\":45064,\"start\":45059},{\"end\":45070,\"start\":45068},{\"end\":45081,\"start\":45074},{\"end\":45170,\"start\":45164},{\"end\":45179,\"start\":45174},{\"end\":45189,\"start\":45183},{\"end\":45293,\"start\":45288},{\"end\":45307,\"start\":45297},{\"end\":45316,\"start\":45313},{\"end\":45365,\"start\":45355},{\"end\":45382,\"start\":45369},{\"end\":45391,\"start\":45386},{\"end\":45397,\"start\":45395},{\"end\":45413,\"start\":45401},{\"end\":45422,\"start\":45417},{\"end\":45435,\"start\":45426},{\"end\":45445,\"start\":45439},{\"end\":45591,\"start\":45586},{\"end\":45601,\"start\":45595},{\"end\":45611,\"start\":45605},{\"end\":45623,\"start\":45615},{\"end\":45801,\"start\":45799},{\"end\":45810,\"start\":45805},{\"end\":45823,\"start\":45814},{\"end\":45833,\"start\":45827},{\"end\":45923,\"start\":45920},{\"end\":45929,\"start\":45927},{\"end\":45936,\"start\":45933},{\"end\":45945,\"start\":45940},{\"end\":45951,\"start\":45949},{\"end\":45959,\"start\":45955},{\"end\":46020,\"start\":46017},{\"end\":46026,\"start\":46024},{\"end\":46035,\"start\":46030},{\"end\":46045,\"start\":46039},{\"end\":46056,\"start\":46049},{\"end\":46065,\"start\":46060},{\"end\":46196,\"start\":46193},{\"end\":46202,\"start\":46200},{\"end\":46209,\"start\":46206},{\"end\":46219,\"start\":46213},{\"end\":46228,\"start\":46223},{\"end\":46302,\"start\":46300},{\"end\":46311,\"start\":46306},{\"end\":46318,\"start\":46315},{\"end\":46325,\"start\":46322},{\"end\":46347,\"start\":46343},{\"end\":46357,\"start\":46351},{\"end\":46369,\"start\":46361},{\"end\":46379,\"start\":46373},{\"end\":46393,\"start\":46383},{\"end\":46403,\"start\":46397},{\"end\":46415,\"start\":46407},{\"end\":46424,\"start\":46419},{\"end\":46434,\"start\":46428},{\"end\":46444,\"start\":46438},{\"end\":46456,\"start\":46448},{\"end\":46467,\"start\":46460},{\"end\":46479,\"start\":46471},{\"end\":46488,\"start\":46483},{\"end\":46494,\"start\":46492},{\"end\":46556,\"start\":46546},{\"end\":46571,\"start\":46560},{\"end\":46643,\"start\":46641},{\"end\":46651,\"start\":46647},{\"end\":46660,\"start\":46655},{\"end\":46677,\"start\":46664},{\"end\":46687,\"start\":46683},{\"end\":46753,\"start\":46747},{\"end\":46759,\"start\":46757},{\"end\":46898,\"start\":46886},{\"end\":46911,\"start\":46902},{\"end\":46996,\"start\":46988},{\"end\":47006,\"start\":47000},{\"end\":47017,\"start\":47010},{\"end\":47119,\"start\":47113},{\"end\":47139,\"start\":47123},{\"end\":47147,\"start\":47141},{\"end\":47251,\"start\":47245},{\"end\":47265,\"start\":47255},{\"end\":47272,\"start\":47269},{\"end\":47282,\"start\":47276},{\"end\":47403,\"start\":47400},{\"end\":47409,\"start\":47407},{\"end\":47417,\"start\":47413},{\"end\":47423,\"start\":47421},{\"end\":47431,\"start\":47427},{\"end\":47441,\"start\":47438},{\"end\":47452,\"start\":47448},{\"end\":47496,\"start\":47490},{\"end\":47506,\"start\":47500},{\"end\":47598,\"start\":47596},{\"end\":47605,\"start\":47602},{\"end\":47617,\"start\":47609},{\"end\":47631,\"start\":47623},{\"end\":47697,\"start\":47694},{\"end\":47703,\"start\":47701},{\"end\":47709,\"start\":47707},{\"end\":47718,\"start\":47713},{\"end\":47728,\"start\":47725},{\"end\":47818,\"start\":47810},{\"end\":47827,\"start\":47822},{\"end\":47837,\"start\":47831},{\"end\":47995,\"start\":47992},{\"end\":48009,\"start\":47999},{\"end\":48018,\"start\":48013},{\"end\":48064,\"start\":48058},{\"end\":48075,\"start\":48068},{\"end\":48186,\"start\":48182},{\"end\":48196,\"start\":48190},{\"end\":48206,\"start\":48200},{\"end\":48222,\"start\":48210},{\"end\":48347,\"start\":48339},{\"end\":48457,\"start\":48454},{\"end\":48468,\"start\":48463},{\"end\":48478,\"start\":48472},{\"end\":48490,\"start\":48482},{\"end\":48625,\"start\":48621},{\"end\":48632,\"start\":48629},{\"end\":48644,\"start\":48636},{\"end\":48656,\"start\":48648},{\"end\":48710,\"start\":48707},{\"end\":48719,\"start\":48714},{\"end\":48781,\"start\":48775},{\"end\":48792,\"start\":48787},{\"end\":48858,\"start\":48851},{\"end\":48873,\"start\":48864},{\"end\":48883,\"start\":48877},{\"end\":48937,\"start\":48929},{\"end\":48947,\"start\":48941},{\"end\":48957,\"start\":48951},{\"end\":48972,\"start\":48961},{\"end\":49037,\"start\":49031},{\"end\":49046,\"start\":49043},{\"end\":49178,\"start\":49174},{\"end\":49185,\"start\":49182},{\"end\":49191,\"start\":49189},{\"end\":49200,\"start\":49195},{\"end\":49208,\"start\":49204},{\"end\":49216,\"start\":49212},{\"end\":49224,\"start\":49220},{\"end\":49233,\"start\":49228},{\"end\":49319,\"start\":49317},{\"end\":49325,\"start\":49323},{\"end\":49331,\"start\":49329},{\"end\":49425,\"start\":49421},{\"end\":49431,\"start\":49429},{\"end\":49439,\"start\":49435},{\"end\":49449,\"start\":49445},{\"end\":49469,\"start\":49453},{\"end\":49530,\"start\":49528},{\"end\":49541,\"start\":49534},{\"end\":49630,\"start\":49628},{\"end\":49730,\"start\":49728},{\"end\":49819,\"start\":49817},{\"end\":49828,\"start\":49823},{\"end\":49836,\"start\":49832},{\"end\":49842,\"start\":49840},{\"end\":49950,\"start\":49948},{\"end\":49958,\"start\":49954},{\"end\":49967,\"start\":49962},{\"end\":49975,\"start\":49971},{\"end\":49982,\"start\":49979},{\"end\":50084,\"start\":50079},{\"end\":50092,\"start\":50088},{\"end\":50099,\"start\":50096},{\"end\":50107,\"start\":50103},{\"end\":50115,\"start\":50111},{\"end\":50123,\"start\":50119},{\"end\":50131,\"start\":50127},{\"end\":50140,\"start\":50135},{\"end\":50217,\"start\":50212},{\"end\":50224,\"start\":50221},{\"end\":50231,\"start\":50228},{\"end\":50239,\"start\":50235},{\"end\":50248,\"start\":50243},{\"end\":50256,\"start\":50252},{\"end\":50265,\"start\":50260},{\"end\":50339,\"start\":50336},{\"end\":50345,\"start\":50343},{\"end\":50353,\"start\":50349},{\"end\":50361,\"start\":50357},{\"end\":50369,\"start\":50365},{\"end\":50376,\"start\":50373},{\"end\":50383,\"start\":50380}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":10647707},\"end\":43647,\"start\":43567},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":196623463},\"end\":43786,\"start\":43649},{\"attributes\":{\"id\":\"b2\"},\"end\":43850,\"start\":43788},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":44060508},\"end\":43939,\"start\":43852},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":20619091},\"end\":44053,\"start\":43941},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":6002655},\"end\":44174,\"start\":44055},{\"attributes\":{\"id\":\"b6\"},\"end\":44312,\"start\":44176},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":192624981},\"end\":44481,\"start\":44314},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":5559664},\"end\":44603,\"start\":44483},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":50769215},\"end\":44730,\"start\":44605},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":11744847},\"end\":44831,\"start\":44732},{\"attributes\":{\"doi\":\"arXiv:1801.08757\",\"id\":\"b11\"},\"end\":44977,\"start\":44833},{\"attributes\":{\"id\":\"b12\"},\"end\":45103,\"start\":44979},{\"attributes\":{\"doi\":\"arXiv:1206.6404\",\"id\":\"b13\"},\"end\":45224,\"start\":45105},{\"attributes\":{\"id\":\"b14\"},\"end\":45322,\"start\":45226},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":10319744},\"end\":45457,\"start\":45324},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":38717023},\"end\":45704,\"start\":45459},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":18389147},\"end\":45845,\"start\":45706},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":3389583},\"end\":45971,\"start\":45847},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":13911587},\"end\":46077,\"start\":45973},{\"attributes\":{\"doi\":\"arXiv:1905.12794\",\"id\":\"b20\"},\"end\":46250,\"start\":46079},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":206594692},\"end\":46337,\"start\":46252},{\"attributes\":{\"id\":\"b22\"},\"end\":46518,\"start\":46339},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":1915014},\"end\":46599,\"start\":46520},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":20981275},\"end\":46699,\"start\":46601},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":6628106},\"end\":46771,\"start\":46701},{\"attributes\":{\"id\":\"b26\"},\"end\":46922,\"start\":46773},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":15277627},\"end\":47029,\"start\":46924},{\"attributes\":{\"doi\":\"arXiv:1611.04051\",\"id\":\"b28\"},\"end\":47183,\"start\":47031},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":1537525},\"end\":47294,\"start\":47185},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":209366827},\"end\":47464,\"start\":47296},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":13971447},\"end\":47518,\"start\":47466},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":207178795},\"end\":47645,\"start\":47520},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":4857922},\"end\":47740,\"start\":47647},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":14848221},\"end\":47916,\"start\":47742},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":68160504},\"end\":48033,\"start\":47918},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":2633340},\"end\":48087,\"start\":48035},{\"attributes\":{\"id\":\"b37\"},\"end\":48264,\"start\":48089},{\"attributes\":{\"id\":\"b38\"},\"end\":48370,\"start\":48266},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":3888393},\"end\":48560,\"start\":48372},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":7296803},\"end\":48668,\"start\":48562},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":47012216},\"end\":48728,\"start\":48670},{\"attributes\":{\"id\":\"b42\"},\"end\":48807,\"start\":48730},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":44095973},\"end\":48895,\"start\":48809},{\"attributes\":{\"id\":\"b44\"},\"end\":48978,\"start\":48897},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":18069725},\"end\":49111,\"start\":48980},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":81982679},\"end\":49246,\"start\":49113},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":22046614},\"end\":49343,\"start\":49248},{\"attributes\":{\"id\":\"b48\"},\"end\":49482,\"start\":49345},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":2403024},\"end\":49566,\"start\":49484},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":7698906},\"end\":49642,\"start\":49568},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":4668606},\"end\":49746,\"start\":49644},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":3439214},\"end\":49854,\"start\":49748},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":204837148},\"end\":50004,\"start\":49856},{\"attributes\":{\"id\":\"b54\"},\"end\":50158,\"start\":50006},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":26668832},\"end\":50277,\"start\":50160},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":7454994},\"end\":50396,\"start\":50279}]", "bib_title": "[{\"end\":43598,\"start\":43567},{\"end\":43725,\"start\":43649},{\"end\":43873,\"start\":43852},{\"end\":44008,\"start\":43941},{\"end\":44099,\"start\":44055},{\"end\":44388,\"start\":44314},{\"end\":44554,\"start\":44483},{\"end\":44664,\"start\":44605},{\"end\":44774,\"start\":44732},{\"end\":45351,\"start\":45324},{\"end\":45582,\"start\":45459},{\"end\":45795,\"start\":45706},{\"end\":45916,\"start\":45847},{\"end\":46013,\"start\":45973},{\"end\":46296,\"start\":46252},{\"end\":46542,\"start\":46520},{\"end\":46637,\"start\":46601},{\"end\":46743,\"start\":46701},{\"end\":46984,\"start\":46924},{\"end\":47241,\"start\":47185},{\"end\":47396,\"start\":47296},{\"end\":47486,\"start\":47466},{\"end\":47592,\"start\":47520},{\"end\":47690,\"start\":47647},{\"end\":47806,\"start\":47742},{\"end\":47988,\"start\":47918},{\"end\":48054,\"start\":48035},{\"end\":48176,\"start\":48089},{\"end\":48450,\"start\":48372},{\"end\":48617,\"start\":48562},{\"end\":48703,\"start\":48670},{\"end\":48847,\"start\":48809},{\"end\":49027,\"start\":48980},{\"end\":49170,\"start\":49113},{\"end\":49313,\"start\":49248},{\"end\":49524,\"start\":49484},{\"end\":49622,\"start\":49568},{\"end\":49722,\"start\":49644},{\"end\":49813,\"start\":49748},{\"end\":49944,\"start\":49856},{\"end\":50075,\"start\":50006},{\"end\":50208,\"start\":50160},{\"end\":50332,\"start\":50279}]", "bib_author": "[{\"end\":43610,\"start\":43600},{\"end\":43618,\"start\":43610},{\"end\":43627,\"start\":43618},{\"end\":43637,\"start\":43627},{\"end\":43742,\"start\":43727},{\"end\":43752,\"start\":43742},{\"end\":43764,\"start\":43752},{\"end\":43775,\"start\":43764},{\"end\":43837,\"start\":43827},{\"end\":43890,\"start\":43875},{\"end\":44022,\"start\":44010},{\"end\":44113,\"start\":44101},{\"end\":44119,\"start\":44113},{\"end\":44254,\"start\":44247},{\"end\":44260,\"start\":44254},{\"end\":44269,\"start\":44260},{\"end\":44280,\"start\":44269},{\"end\":44286,\"start\":44280},{\"end\":44294,\"start\":44286},{\"end\":44304,\"start\":44294},{\"end\":44399,\"start\":44390},{\"end\":44407,\"start\":44399},{\"end\":44423,\"start\":44407},{\"end\":44432,\"start\":44423},{\"end\":44440,\"start\":44432},{\"end\":44448,\"start\":44440},{\"end\":44463,\"start\":44448},{\"end\":44472,\"start\":44463},{\"end\":44564,\"start\":44556},{\"end\":44573,\"start\":44564},{\"end\":44583,\"start\":44573},{\"end\":44593,\"start\":44583},{\"end\":44685,\"start\":44666},{\"end\":44695,\"start\":44685},{\"end\":44701,\"start\":44695},{\"end\":44709,\"start\":44701},{\"end\":44718,\"start\":44709},{\"end\":44795,\"start\":44776},{\"end\":44808,\"start\":44795},{\"end\":44819,\"start\":44808},{\"end\":44842,\"start\":44833},{\"end\":44855,\"start\":44842},{\"end\":44866,\"start\":44855},{\"end\":44876,\"start\":44866},{\"end\":44888,\"start\":44876},{\"end\":44897,\"start\":44888},{\"end\":44993,\"start\":44981},{\"end\":45002,\"start\":44993},{\"end\":45012,\"start\":45002},{\"end\":45022,\"start\":45012},{\"end\":45034,\"start\":45022},{\"end\":45045,\"start\":45034},{\"end\":45057,\"start\":45045},{\"end\":45066,\"start\":45057},{\"end\":45072,\"start\":45066},{\"end\":45083,\"start\":45072},{\"end\":45172,\"start\":45159},{\"end\":45181,\"start\":45172},{\"end\":45191,\"start\":45181},{\"end\":45295,\"start\":45286},{\"end\":45309,\"start\":45295},{\"end\":45318,\"start\":45309},{\"end\":45367,\"start\":45353},{\"end\":45384,\"start\":45367},{\"end\":45393,\"start\":45384},{\"end\":45399,\"start\":45393},{\"end\":45415,\"start\":45399},{\"end\":45424,\"start\":45415},{\"end\":45437,\"start\":45424},{\"end\":45447,\"start\":45437},{\"end\":45593,\"start\":45584},{\"end\":45603,\"start\":45593},{\"end\":45613,\"start\":45603},{\"end\":45625,\"start\":45613},{\"end\":45803,\"start\":45797},{\"end\":45812,\"start\":45803},{\"end\":45825,\"start\":45812},{\"end\":45835,\"start\":45825},{\"end\":45925,\"start\":45918},{\"end\":45931,\"start\":45925},{\"end\":45938,\"start\":45931},{\"end\":45947,\"start\":45938},{\"end\":45953,\"start\":45947},{\"end\":45961,\"start\":45953},{\"end\":46022,\"start\":46015},{\"end\":46028,\"start\":46022},{\"end\":46037,\"start\":46028},{\"end\":46047,\"start\":46037},{\"end\":46058,\"start\":46047},{\"end\":46067,\"start\":46058},{\"end\":46198,\"start\":46191},{\"end\":46204,\"start\":46198},{\"end\":46211,\"start\":46204},{\"end\":46221,\"start\":46211},{\"end\":46230,\"start\":46221},{\"end\":46304,\"start\":46298},{\"end\":46313,\"start\":46304},{\"end\":46320,\"start\":46313},{\"end\":46327,\"start\":46320},{\"end\":46349,\"start\":46341},{\"end\":46359,\"start\":46349},{\"end\":46371,\"start\":46359},{\"end\":46381,\"start\":46371},{\"end\":46395,\"start\":46381},{\"end\":46405,\"start\":46395},{\"end\":46417,\"start\":46405},{\"end\":46426,\"start\":46417},{\"end\":46436,\"start\":46426},{\"end\":46446,\"start\":46436},{\"end\":46458,\"start\":46446},{\"end\":46469,\"start\":46458},{\"end\":46481,\"start\":46469},{\"end\":46490,\"start\":46481},{\"end\":46496,\"start\":46490},{\"end\":46558,\"start\":46544},{\"end\":46573,\"start\":46558},{\"end\":46645,\"start\":46639},{\"end\":46653,\"start\":46645},{\"end\":46662,\"start\":46653},{\"end\":46679,\"start\":46662},{\"end\":46689,\"start\":46679},{\"end\":46755,\"start\":46745},{\"end\":46761,\"start\":46755},{\"end\":46900,\"start\":46884},{\"end\":46913,\"start\":46900},{\"end\":46998,\"start\":46986},{\"end\":47008,\"start\":46998},{\"end\":47019,\"start\":47008},{\"end\":47121,\"start\":47109},{\"end\":47141,\"start\":47121},{\"end\":47149,\"start\":47141},{\"end\":47253,\"start\":47243},{\"end\":47267,\"start\":47253},{\"end\":47274,\"start\":47267},{\"end\":47284,\"start\":47274},{\"end\":47405,\"start\":47398},{\"end\":47411,\"start\":47405},{\"end\":47419,\"start\":47411},{\"end\":47425,\"start\":47419},{\"end\":47433,\"start\":47425},{\"end\":47443,\"start\":47433},{\"end\":47454,\"start\":47443},{\"end\":47498,\"start\":47488},{\"end\":47508,\"start\":47498},{\"end\":47600,\"start\":47594},{\"end\":47607,\"start\":47600},{\"end\":47619,\"start\":47607},{\"end\":47633,\"start\":47619},{\"end\":47699,\"start\":47692},{\"end\":47705,\"start\":47699},{\"end\":47711,\"start\":47705},{\"end\":47720,\"start\":47711},{\"end\":47730,\"start\":47720},{\"end\":47820,\"start\":47808},{\"end\":47829,\"start\":47820},{\"end\":47839,\"start\":47829},{\"end\":47997,\"start\":47990},{\"end\":48011,\"start\":47997},{\"end\":48020,\"start\":48011},{\"end\":48066,\"start\":48056},{\"end\":48077,\"start\":48066},{\"end\":48188,\"start\":48178},{\"end\":48198,\"start\":48188},{\"end\":48208,\"start\":48198},{\"end\":48224,\"start\":48208},{\"end\":48349,\"start\":48335},{\"end\":48459,\"start\":48452},{\"end\":48470,\"start\":48459},{\"end\":48480,\"start\":48470},{\"end\":48492,\"start\":48480},{\"end\":48627,\"start\":48619},{\"end\":48634,\"start\":48627},{\"end\":48646,\"start\":48634},{\"end\":48658,\"start\":48646},{\"end\":48712,\"start\":48705},{\"end\":48721,\"start\":48712},{\"end\":48783,\"start\":48771},{\"end\":48794,\"start\":48783},{\"end\":48860,\"start\":48849},{\"end\":48875,\"start\":48860},{\"end\":48885,\"start\":48875},{\"end\":48939,\"start\":48927},{\"end\":48949,\"start\":48939},{\"end\":48959,\"start\":48949},{\"end\":48974,\"start\":48959},{\"end\":49039,\"start\":49029},{\"end\":49048,\"start\":49039},{\"end\":49180,\"start\":49172},{\"end\":49187,\"start\":49180},{\"end\":49193,\"start\":49187},{\"end\":49202,\"start\":49193},{\"end\":49210,\"start\":49202},{\"end\":49218,\"start\":49210},{\"end\":49226,\"start\":49218},{\"end\":49235,\"start\":49226},{\"end\":49321,\"start\":49315},{\"end\":49327,\"start\":49321},{\"end\":49333,\"start\":49327},{\"end\":49427,\"start\":49419},{\"end\":49433,\"start\":49427},{\"end\":49441,\"start\":49433},{\"end\":49451,\"start\":49441},{\"end\":49471,\"start\":49451},{\"end\":49532,\"start\":49526},{\"end\":49543,\"start\":49532},{\"end\":49632,\"start\":49624},{\"end\":49732,\"start\":49724},{\"end\":49736,\"start\":49732},{\"end\":49821,\"start\":49815},{\"end\":49830,\"start\":49821},{\"end\":49838,\"start\":49830},{\"end\":49844,\"start\":49838},{\"end\":49952,\"start\":49946},{\"end\":49960,\"start\":49952},{\"end\":49969,\"start\":49960},{\"end\":49977,\"start\":49969},{\"end\":49984,\"start\":49977},{\"end\":50086,\"start\":50077},{\"end\":50094,\"start\":50086},{\"end\":50101,\"start\":50094},{\"end\":50109,\"start\":50101},{\"end\":50117,\"start\":50109},{\"end\":50125,\"start\":50117},{\"end\":50133,\"start\":50125},{\"end\":50142,\"start\":50133},{\"end\":50219,\"start\":50210},{\"end\":50226,\"start\":50219},{\"end\":50233,\"start\":50226},{\"end\":50241,\"start\":50233},{\"end\":50250,\"start\":50241},{\"end\":50258,\"start\":50250},{\"end\":50267,\"start\":50258},{\"end\":50341,\"start\":50334},{\"end\":50347,\"start\":50341},{\"end\":50355,\"start\":50347},{\"end\":50363,\"start\":50355},{\"end\":50371,\"start\":50363},{\"end\":50378,\"start\":50371},{\"end\":50385,\"start\":50378}]", "bib_venue": "[{\"end\":43641,\"start\":43637},{\"end\":43780,\"start\":43775},{\"end\":43825,\"start\":43788},{\"end\":43933,\"start\":43890},{\"end\":44047,\"start\":44022},{\"end\":44168,\"start\":44119},{\"end\":44245,\"start\":44176},{\"end\":44475,\"start\":44472},{\"end\":44597,\"start\":44593},{\"end\":44721,\"start\":44718},{\"end\":44822,\"start\":44819},{\"end\":44957,\"start\":44913},{\"end\":45157,\"start\":45105},{\"end\":45284,\"start\":45226},{\"end\":45451,\"start\":45447},{\"end\":45690,\"start\":45625},{\"end\":45839,\"start\":45835},{\"end\":45965,\"start\":45961},{\"end\":46071,\"start\":46067},{\"end\":46189,\"start\":46079},{\"end\":46331,\"start\":46327},{\"end\":46512,\"start\":46496},{\"end\":46591,\"start\":46573},{\"end\":46693,\"start\":46689},{\"end\":46765,\"start\":46761},{\"end\":46882,\"start\":46773},{\"end\":47023,\"start\":47019},{\"end\":47107,\"start\":47031},{\"end\":47288,\"start\":47284},{\"end\":47458,\"start\":47454},{\"end\":47512,\"start\":47508},{\"end\":47636,\"start\":47633},{\"end\":47734,\"start\":47730},{\"end\":47910,\"start\":47839},{\"end\":48027,\"start\":48020},{\"end\":48081,\"start\":48077},{\"end\":48258,\"start\":48224},{\"end\":48333,\"start\":48266},{\"end\":48554,\"start\":48492},{\"end\":48662,\"start\":48658},{\"end\":48726,\"start\":48721},{\"end\":48769,\"start\":48730},{\"end\":48889,\"start\":48885},{\"end\":48925,\"start\":48897},{\"end\":49105,\"start\":49048},{\"end\":49240,\"start\":49235},{\"end\":49337,\"start\":49333},{\"end\":49417,\"start\":49345},{\"end\":49560,\"start\":49543},{\"end\":49636,\"start\":49632},{\"end\":49740,\"start\":49736},{\"end\":49848,\"start\":49844},{\"end\":49998,\"start\":49984},{\"end\":50152,\"start\":50142},{\"end\":50271,\"start\":50267},{\"end\":50390,\"start\":50385}]"}}}, "year": 2023, "month": 12, "day": 17}