{"id": 235790758, "updated": "2023-10-06 06:08:37.882", "metadata": {"title": "Extracting the Auditory Attention in a Dual-Speaker Scenario From EEG Using a Joint CNN-LSTM Model", "authors": "[{\"first\":\"Ivine\",\"last\":\"Kuruvila\",\"middle\":[]},{\"first\":\"Jan\",\"last\":\"Muncke\",\"middle\":[]},{\"first\":\"Eghart\",\"last\":\"Fischer\",\"middle\":[]},{\"first\":\"Ulrich\",\"last\":\"Hoppe\",\"middle\":[]}]", "venue": "Frontiers in Physiology", "journal": "Frontiers in Physiology", "publication_date": {"year": 2021, "month": 8, "day": 2}, "abstract": "Human brain performs remarkably well in segregating a particular speaker from interfering ones in a multispeaker scenario. We can quantitatively evaluate the segregation capability by modeling a relationship between the speech signals present in an auditory scene, and the listener's cortical signals measured using electroencephalography (EEG). This has opened up avenues to integrate neuro-feedback into hearing aids where the device can infer user's attention and enhance the attended speaker. Commonly used algorithms to infer the auditory attention are based on linear systems theory where cues such as speech envelopes are mapped on to the EEG signals. Here, we present a joint convolutional neural network (CNN)\u2014long short-term memory (LSTM) model to infer the auditory attention. Our joint CNN-LSTM model takes the EEG signals and the spectrogram of the multiple speakers as inputs and classifies the attention to one of the speakers. We evaluated the reliability of our network using three different datasets comprising of 61 subjects, where each subject undertook a dual-speaker experiment. The three datasets analyzed corresponded to speech stimuli presented in three different languages namely German, Danish, and Dutch. Using the proposed joint CNN-LSTM model, we obtained a median decoding accuracy of 77.2% at a trial duration of 3 s. Furthermore, we evaluated the amount of sparsity that the model can tolerate by means of magnitude pruning and found a tolerance of up to 50% sparsity without substantial loss of decoding accuracy.", "fields_of_study": "[\"Computer Science\",\"Engineering\"]", "external_ids": {"arxiv": "2102.03957", "mag": "3191474079", "acl": null, "pubmed": "34408661", "pubmedcentral": "8365753", "dblp": null, "doi": "10.3389/fphys.2021.700655"}}, "content": {"source": {"pdf_hash": "6d29cac350527f1bd9139191764fd34afe7d4174", "pdf_src": "PubMedCentral", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": "CCBY", "open_access_url": "https://www.frontiersin.org/articles/10.3389/fphys.2021.700655/pdf", "status": "GOLD"}}, "grobid": {"id": "2194b763f8e32d6e82707938958236787af4608d", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/6d29cac350527f1bd9139191764fd34afe7d4174.txt", "contents": "\nExtracting the Auditory Attention in a Dual-Speaker Scenario From EEG Using a Joint CNN-LSTM Model\nAugust 2021\n\nNicole Y K Li-Jessen \nBehtash Babadi \nMichael Georg Metzen \nUlrich Hoppe ulrich.hoppe@uk-erlangen.de \nIvine Kuruvila \nDepartment of Audiology\nFriedrich-Alexander-Universit\u00e4t Erlangen-N\u00fcrnberg (FAU)\nENT-Clinic\nErlangenGermany\n\nJan Muncke \nDepartment of Audiology\nFriedrich-Alexander-Universit\u00e4t Erlangen-N\u00fcrnberg (FAU)\nENT-Clinic\nErlangenGermany\n\nEghart Fischer \nWS Audiology\nErlangenGermany\n\nUlrich Hoppe \nDepartment of Audiology\nFriedrich-Alexander-Universit\u00e4t Erlangen-N\u00fcrnberg (FAU)\nENT-Clinic\nErlangenGermany\n\n\nMcGill University\nCanada\n\n\nUniversity of Maryland\nUnited States\n\n\nMcGill University\nCanada\n\nExtracting the Auditory Attention in a Dual-Speaker Scenario From EEG Using a Joint CNN-LSTM Model\n\nFrontiers in Physiology | www.frontiersin.org\n1700655August 202110.3389/fphys.2021.700655Received: 26 April 2021 Accepted: 05 July 2021ORIGINAL RESEARCH Edited by: Reviewed by: *Correspondence: Specialty section: This article was submitted to Computational Physiology and Medicine, a section of the journal Frontiers in Physiology Citation: Kuruvila I, Muncke J, Fischer E and Hoppe U (2021) Extracting the Auditory Attention in a Dual-Speaker Scenario From EEG Using a Joint CNN-LSTM Model.EEGcocktail party effectauditory attentionlong short term memory networkshearing aidsspeech enhancementspeech separationconvolutional neural network\nHuman brain performs remarkably well in segregating a particular speaker from interfering ones in a multispeaker scenario. We can quantitatively evaluate the segregation capability by modeling a relationship between the speech signals present in an auditory scene, and the listener's cortical signals measured using electroencephalography (EEG). This has opened up avenues to integrate neuro-feedback into hearing aids where the device can infer user's attention and enhance the attended speaker. Commonly used algorithms to infer the auditory attention are based on linear systems theory where cues such as speech envelopes are mapped on to the EEG signals. Here, we present a joint convolutional neural network (CNN)-long short-term memory (LSTM) model to infer the auditory attention. Our joint CNN-LSTM model takes the EEG signals and the spectrogram of the multiple speakers as inputs and classifies the attention to one of the speakers. We evaluated the reliability of our network using three different datasets comprising of 61 subjects, where each subject undertook a dual-speaker experiment. The three datasets analyzed corresponded to speech stimuli presented in three different languages namely German, Danish, and Dutch. Using the proposed joint CNN-LSTM model, we obtained a median decoding accuracy of 77.2% at a trial duration of 3 s. Furthermore, we evaluated the amount of sparsity that the model can tolerate by means of magnitude pruning and found a tolerance of up to 50% sparsity without substantial loss of decoding accuracy.\n\nINTRODUCTION\n\nHolding a conversation in presence of multiple noise sources and interfering speakers is a task that people with normal hearing carry out exceptionally well. The inherent ability to focus the auditory attention on a particular speech signal in a complex mixture is known as the cocktail party effect (Cherry, 1953). However, an automatic machine based solution to the cocktail party problem is yet to be discovered despite the intense research for more than half a century. Such a solution is highly desirable for a plethora of applications such as human-machine interface (e.g., Amazon Alexa), automatic captioning of audio/video recordings (e.g., YouTube, Netflix), advanced hearing aids etc.\n\nIn the domain of hearing aids, people with hearing loss suffer from deteriorated speech intelligibility when listening to a particular speaker in a multispeaker scenario. Hearing aids currently available in the market often do not provide sufficient amenity in such scenarios due to their inability to distinguish between the attended speaker and the ignored ones. Hence, additional information about the locus of attention is highly desirable. In visual domain, selective attention is explained in terms of visual object formation where an observer focuses on a certain object in a complex visual scene (Feldman, 2003). This was extended to auditory domain where it was suggested that phenomena such as cocktail party effect could be better understood using auditory object formation (Shinn-Cunningham, 2008). In other words, brain forms objects based on the multiple speakers present in an auditory scene and selects those objects belonging to a particular speaker during attentive listening (top-down or late selection). However, flexible locus of attention theory was concurrently proposed where the late selection is hypothesized to occur at low cognitive load and early selection is hypothesized to occur at high cognitive load (Vogel et al., 2005). This has inspired investigation into whether cortical signals could provide additional information that helps to discriminate between the attended speaker and interfering speakers. In a dual-speaker experiment, it was observed that the cortical signals measured using implanted electrodes track the salient features of the attended speaker stronger than the ignored speaker (Mesgarani and Chang, 2012). Similar results were obtained using magnetoencephalography and electroencephalography (EEG) (Ding and Simon, 2012;O'Sullivan et al., 2014). In recent years, EEG analyses have become the commonly used methodology in attention research which is lately known as auditory attention decoding (AAD).\n\nBoth low level acoustic features (speech envelope or speech spectrogram) and high level features (phonemes or phonetics) have been used to investigate the speech tracking in cortical signals (Aiken and Picton, 2008;Lalor and Foxe, 2010;Di Liberto et al., 2015;Broderick et al., 2019). State-of-the-art AAD algorithms are based on linear systems theory where acoustic features are linearly mapped on to the EEG signals. This mapping can be either in the forward direction (Lalor and Foxe, 2010;Fiedler et al., 2017;Kuruvila et al., 2020) or in the backward direction (O'Sullivan et al., 2014;Mirkovic et al., 2015;Biesmans et al., 2017). These algorithms have been successful in providing insights into the underlying neuroscientific processes through which brain suppresses the ignored speaker in a dual-speaker scenario. Using speech envelope as the input acoustic feature, linear algorithms could generate system response functions that characterize the auditory pathway in the forward direction. These system response functions are referred to as temporal response function (TRF) (Lalor and Foxe, 2010). Analysis of the shape of TRFs has revealed that the human brain encodes the attended speaker different to that of the ignored speaker. Specifically, TRFs corresponding to the attended speaker have salient peaks around 100 and 200 ms which are weaker in TRFs corresponding to the ignored speaker (Fiedler et al., 2019;Kuruvila et al., 2021). Similar attention modulation effects were observed when the acoustic input was modified to using speech spectrogram or higher level features such as phonetics (Di Liberto et al., 2015). Likewise using backward models, the input stimulus can be reconstructed from EEG signals (stimulus reconstruction method) and a listener's attention could be inferred by comparing the reconstructed stimulus to the input stimuli (O'Sullivan et al., 2014). These findings give the possibility of integrating AAD algorithms into hearing aids which in combination with robust speech separation algorithms could greatly enhance the amenity provided to the users.\n\nIt has been well-established that the human auditory system is inherently non-linear (Zwicker and Fastl, 2013) and AAD analysis based on linear systems theory addresses the issue of non-linearity to a certain extend in the preprocessing stage. For example, during speech envelope extraction. Another limitation of linear methods is the longer time delay required to classify attention (Fuglsang et al., 2017;Geirnaert et al., 2019), although there were attempts to overcome this limitation (Miran et al., 2018;Kuruvila et al., 2021). In the last few years, deep neural networks have become popular especially in the field of computer vision and natural language processing. Since neural networks have the ability to model non-linearity, they have been used to estimate the dynamic state of brain from EEG signals (Craik et al., 2019). Similarly in AAD paradigm, convolutional neural network (CNN) based models were proposed where the stimulus reconstruction algorithm was implemented using the CNN model to infer attention (Ciccarelli et al., 2019;de Taillez et al., 2020). A direct classification of attention which bypasses the regression task of stimulus reconstruction, instead classifies whether the attention is to speaker 1 or speaker 2 directly was proposed in Ciccarelli et al. (2019) and Vandecappelle et al. (2021). In a non-competing speaker experiment, classifying attention as successful vs unsuccessful or match vs mismatch was further addressed in Monesi et al. (2020) and Tian and Ma (2020).\n\nAll aforementioned neural network models either did not use speech features or made use of only speech envelope as the input feature. As neural networks are data driven models, additional data/information about the speech stimuli may improve the performance of the network. In speech separation algorithms based on neural networks, spectrogram is used as the input feature to separate multiple speakers from a speech mixture (Wang and Chen, 2018). Inspired by the joint audio-visual speech separation model (Ephrat et al., 2018), we present a novel neural network framework that make use the speech spectrogram of multiple speakers and the EEG signals as inputs to classify the auditory attention.\n\nThe rest of the paper is organized as follows. In section 2, details of the datasets that were used to train and validate the neural network are provided. In section 3, the neural network architecture is explained in detail. The results are presented in sections 4, 5 provides a discussion on the results.\n\n\nMATERIALS AND METHODS\n\n\nExamined EEG Datasets\n\nWe evaluated the performance of our neural network model using three different EEG datasets. The first dataset was collected at our lab and it will be referred to as FAU_Dataset (Kuruvila et al., 2021). The second and third datasets are publicly available and they will be referred to as DTU_Dataset (Fuglsang et al., 2018) and KUL_Dataset (Das et al., 2019), respectively.\n\n\nFAU_Dataset\n\nThis dataset comprised of EEG collected from 27 subjects who were all native German speakers. A cocktail party effect was simulated by presenting two speech stimuli simultaneously using loudspeakers and the subject was asked to attend selectively to one of the two stimuli. Speech stimuli were taken from the slowly spoken news section of the German news website www.dw.de and were read by two male speakers. The experiment consisted of six different presentations with each presentation being approximately five minutes long making it a total of 30 min. EEG was collected using 21 AgCl electrodes placed over the scalp according to the 10-20 EEG format. The reference electrode was placed at the right mastoid, the ground electrode was placed at the left earlobe and the EEG signals were sampled at 2,500 Hz. More details of the experiment could be found in Kuruvila et al. (2021).\n\n\nDTU_Dataset\n\nThis is a publicly available dataset that was part of the work presented in Fuglsang et al. (2017). The dataset consisted of 18 subjects who selectively attended to one of the two simultaneous speakers. Speech stimuli were excerpts taken from Danish audiobooks that were narrated by a male and a female speaker. The experiment consisted of 60 segments with each segment being 50 s long making it a total of 50 min. EEG were recorded using 64 electrodes and were sampled at 512 Hz. The reference electrode was chosen either as the left mastoid or as the right mastoid after visual inspection. Further details can be found in Fuglsang et al. (2017Fuglsang et al. ( , 2018.\n\n\nKUL_Dataset\n\nThe final dataset that was analyzed is another publicly available dataset where 16 subjects undertook selective attention experiment. Speech stimuli consisted of four Dutch stories narrated by male speakers. Each story was 12 min long which was further divided into two 6 min presentations. EEG was recorded using 64 electrodes and were sampled at 8,196 Hz. The reference electrode was chosen either as TP7 or as TP8 electrode after visually inspecting the quality of the EEG signal measured at these locations. The experiment consisted of three different conditions namely HRTF, dichotic and repeated stimuli. In this work, we analyzed only the dichotic condition which was 24 min long. Additional details about the experiment and the dataset can be found in Das et al. (2016Das et al. ( , 2019.\n\nDetails of the datasets are summarized again in Table 1. A total of 34.9 h of EEG data were examined in this work. However, the speech stimuli used were identical across subjects per dataset and they totalled 104 min of dual-speaker data. In all the three datasets that were analyzed, the two speakers read out different stimuli. Moreover, the stimuli were presented only once to the subject in order to avoid any learning effect. For each subject, the training and the test data were split as 75-25% and we ensured that no part of the EEG or the speech used in the test data was part of the training data. The test data were further divided equally into two halves and one half was used as a validation set during the training procedure.\n\n\nData Analysis\n\nAs EEG signals analyzed were collected at different sampling frequencies, they were all low pass filtered at a cut off frequency of 32 Hz and downsampled to 64 Hz sampling rate. Additionally, signals measured at only 10 electrode locations were considered for analysis and they were F7, F3, F4, F8, T7, C3, Cz, C4, T8, Pz. We analyzed four different trial durations in this study namely 2, 3, 4, and 5 s. For 2 s trials, an overlap of 1 s was applied. Thus, there were 118,922 trials in total for analysis. In order to maintain the total number of trials constant, 2 s of overlap was used in case of 3 s trial, 3 s of overlap was used in case of 4 s trial and 4 s overlap was used in case of 5 s trial. EEG signals in each trial were further high pass filtered with a cut off frequency of 1 Hz and the filtered signals were normalized to have zero mean and unit variance at each electrode location.\n\nSpeech stimuli were initially low pass filtered with a cut off frequency of 8 kHz and were downsampled to a sampling rate of 16 kHz. Subsequently, they were segmented into trials with a duration of 2, 3, 4, and 5 s at an overlap of 1, 2, 3, and 4 s, respectively. The speech spectrogram for each trial was obtained by taking the absolute value of the short-time Fourier transform (STFT) coefficients. The STFT was computed using a Hann window of 32 ms duration with a 12 ms overlap. Most of the analysis in our work was performed using 3 s trial and other trial durations were used only for comparison purposes. A summary of the dimensions of EEG signals and speech spectrogram after preprocessing for different trial durations is provided in Table 2.\n\n\nNETWORK ARCHITECTURE\n\nA top level view of the proposed neural network architecture is shown in Figure 1. It consists of three subnetworks namely EEG_CNN, Audio_CNN, and AE_Concat. \n\n\nEEG_CNN\n\nThe EEG subnetwork comprised of four different convolutional layers as shown in Table 3. The kernel size of the first layer was chosen as 24 and it corresponded to a latency of 375 ms in the time domain. A longer kernel was chosen because previous studies have shown that the TRFs corresponding to attended and unattended speakers differ around 100 and 200 ms (Fiedler et al., 2019;Kuruvila et al., 2021). Therefore, a latency of 375 ms could help us to extract features that modulate the attention to different speakers in a dual-speaker environment. All other layers were initialized with kernels of shorter duration as shown in Table 3. All convolutions were performed using a stride of 1 \u00d7 1 and after the convolutions, max pooling was used to reduce the dimensionality. To prevent overfitting on the training data and improve generalization, dropout (Srivastava et al., 2014), and batch normalization (BN) (Ioffe and Szegedy, 2015) were applied. Subsequently, the output was passed through a nonlinear activation function which was chosen as rectified linear unit (ReLU). The dimension of the input to EEG_CNN varied according to the length of the trial ( Table 2) but the dimension of the output was fixed at 48 \u00d7 32. The max pooling parameter was slightly modified for different trial durations to obtain the fixed output dimension. The first dimension (48) corresponded to the temporal axis and the second dimension (32) corresponded to the number of convolution kernels. The dimension of the output that mapped the EEG signals measured at different electrodes was reduced to one by the successive application of max pooling along the electrode axis.\n\n\nAudio_CNN\n\nThe audio subnetwork that processed the speech spectrogram consisted of five convolution layers whose parameters are shown in Table 4. All standard procedures such as max pooling, batch normalization, dropout, and ReLU activation were applied to the convolution output. Similar to the EEG_CNN, dimension of the input to the Audio_CNN varied according to the trial duration ( Table 2) but the dimension of the output feature map was always  \n\n\nAE_Concat\n\nThe feature maps obtained from EEG_CNN and Audio_CNN were concatenated along the temporal axis and the dimension of the feature map after concatenation was 48 \u00d7 64. In this way, we ensured that half of the feature map was contributed from the EEG data and half of the feature map was contributed from the speech data. This also provides the flexibility to extend to more than two speakers such as the experiment performed in Sch\u00e4fer et al. (2018). The concatenated feature map was passed through a bidirectional long short-term memory (BLSTM) layer (Hochreiter and Schmidhuber, 1997;Schuster and Paliwal, 1997) which was followed by four fully connected (FC) layers. For the first three FC layers, ReLU activation was used and for the last FC layer, softmax activation was applied which helps us to classify the attention to speaker 1 or speaker 2. The total number of EEG samples and audio samples (trials) available was 118,922 and 75% of the total available samples (89,192) were used to train the network and the rest of the available samples (29,730) were equally split as validation and test data. The network was trained for 80 epochs using a mini batch size of 32 samples and with a learning rate of 5 * 10 \u22124 . The drop out probability was set to 0.25 for the EEG_CNN and the AE_Concat subnetworks but it was increased to 0.4 for the Audio_CNN subnetwork. A larger drop out probability was used for the Audio_CNN because speech stimuli were identical across subjects for a particular dataset. Hence, when trained on data from multiple subjects, the speech data remain identical and the network may remember the speech spectrogram of the training data. The network was optimized using Adam optimizer (Kingma and Ba, 2014) and the loss function used was binary cross entropy. As neural network training can result in random variations from epoch to epoch, the test accuracy was calculated as the median accuracy of the last five epochs (Goyal et al., 2017). The network was trained using an Nvidia Geforce RTX-2060 (6 GB) graphics card and took \u223c36 h to complete the training. The neural network model was developed in PyTorch and the python code is available at: https://github.com/ivine-GIT/joint_CNN_ LSTM_AAD.\n\n\nSparse Neural Network: Magnitude Pruning\n\nDespite neural network learning being a sophisticated algorithm, it is still not widely used in embedded devices due to the high memory and computational power requirements. Sparse neural networks have been recently proposed to overcome these challenges and enable running these models on embedded devices (Han et al., 2015). In sparse networks, majority of the model parameters are zeros and zero-valued multiplications can be ignored thereby reducing the computational requirement. Similarly, only non-zero weights need to be stored on the device and for all the zero-valued weights, only their position needs to be known reducing the memory footprint. Empirical evidences have shown that neural networks tolerate high level of sparsity (Han et al., 2015;Narang et al., 2017;Zhu and Gupta, 2017).\n\nSparse neural networks are found out by using a procedure known as network pruning. It consists of three steps. First, a large over-parameterized network is trained in order to obtain a high test accuracy as over-parameterization has stronger representation power (Luo et al., 2017). Second, from the trained over-parameterized network, only important weights based on certain criterion are retained and all other weights are assumed to be redundant and reinitialized to zero. Finally, the pruned network is fine-tuned by training it further using only the retained weights so as to improve the performance. Searching for the redundant weights can be based on simple criteria such as magnitude pruning (Han et al., 2015) or based on complex algorithms such as variational dropout (Molchanov et al., 2017) or L0 regularization (Louizos et al., 2017). However, it was shown that introducing sparsity using magnitude pruning could achieve comparable or better performance than complex techniques such as variational dropout or L0 regularization (Gale et al., 2019). Hence, we will present results based on only magnitude pruning in this work.\n\n\nRESULTS\n\n\nAttention Decoding Accuracy\n\nTo evaluate the performance of our neural network, we trained the model under different scenarios using a trial duration of 3 s. In the first scenario (Ind set train), attention decoding accuracies were calculated per individual dataset. In other words, to obtain the test accuracy of subjects belonging to FAU_Dataset, the model was trained using training samples only from FAU_Dataset leaving out DTU_dataset and KUL_Dataset. Similarly, to obtain the test accuracy for DTU_Dataset, the model was trained using training samples only from DTU_Dataset. The same procedure was repeated for KUL_Dataset. The median decoding accuracy was 72.6% for FAU_Dataset, 48.1% for DTU_Dataset, and 69.1% for KUL_Dataset (Figure 2). In the second scenario (Full set train), accuracies were calculated by combining training samples from all the three datasets together. The median decoding accuracies obtained in this scenario were 84.5, 52.9, and 77.9% for FAU_Dataset, DTU_Dataset, and KUL_Dataset, respectively. The results from the second scenario showed a clear improvement over the first scenario (p_ FAU < 0.001; p_ DTU < 0.05; p_ KUL < 0.01) suggesting that the model generalizes better in the Full set train. Furthermore, to evaluate the cross-set training performance, we trained the model using one dataset and tested it on the other two datasets. For example, the training would be performed using FAU_Dataset and testing would be performed on both DTU and KUL datasets. The same procedure was repeated by training using the DTU dataset and the KUL dataset. The decoding accuracies obtained were all at chance level across the three cross-set training scenarios (Figure 3). Consequently, all results presented further in this paper are based on Full set train. The statistical analyses are based on paired Wilcoxon signed-rank test with sample sizes given in Table 1.\n\nFrontiers in Physiology | www.frontiersin.org FIGURE 2 | Boxplot depicting the decoding accuracies obtained using two different training scenarios. In the first scenario (Ind set train), individual dataset accuracies were obtained by using training samples only from that particular dataset. For example, to calculate the test accuracy of FAU_Dataset, training samples were taken only from FAU_Dataset. In the second scenario (Full set train), individual dataset accuracies were obtained using training samples from all the three datasets combined. As a result, there are more training samples in the second scenario compared to the first ( * p < 0.05; * * p < 0.01; * * * p < 0.001 based on paired Wilcoxon signed-rank test). \n\n\nDecoding Accuracy vs. Trial Duration\n\nTo analyse the effect of trial duration on the attention decoding accuracy, the model was trained using trials of length 2, 3, 4, and 5 s. For every trial, only 1 s of new data were added and the remaining data were populated by overlapping to the previous trial using a sliding window. Specifically, for 2 s trial, 1 s of FIGURE 4 | Comparison of the decoding accuracies calculated for different trial durations per dataset. Statistical analysis based on paired Wilcoxon signed-rank test and pooled over all subjects together from the three datasets ( * p < 0.05; * * * p < 0.001).\n\noverlap was used and for 3 s trial, 2 s of overlap was used, and so on. In this way, total number of training samples remained constant for different trial durations considered in our analysis. The mean decoding accuracy across all subjects and all datasets in case of 2 s trial duration was 70.9 \u00b1 13.2%. The mean accuracy improved to 73.9 \u00b1 14.8% when the trial duration was increased to 3 s (p < 0.001, r = 0.60). Using a trial duration of 4 s, the mean accuracy obtained was 75.2 \u00b1 14.3% which is a slight improvement over 3 s trials (p < 0.05, r = 0.31). For 5 s trials, our neural network model resulted in a mean accuracy of 75.5 \u00b1 15.7% that was statistically identical to the accuracy obtained using 4 s trials (p > 0.05, r = 0.10). Figure 4 depicts the accuracy calculated for individual datasets.\n\n\nAblation Analysis\n\nIn order to gain further insights into the architecture and understand the contribution of different parts of our neural network, we performed ablation analysis using a trial duration of 3 s. To this end, we modified the neural network architecture by removing specific block such as the BLSTM layer or the FC layers one at a time and retrained the modified network. Similarly, to understand the importance of the audio input feature, decoding accuracies were calculated by zeroing out the EEG input and to understand the importance of the EEG input feature, decoding accuracies were calculated by zeroing out the audio input. As shown in Figure 5, the median decoding accuracy by zeroing out the EEG input was 48.6% whereas zeroing out the audio input resulted in an accuracy of 53.6% resulting in no significant difference (p > 0.05). When the network was retrained by removing the BLSTM layer only, the median decoding accuracy obtained was 68.3% and on removing the FC layers only, median decoding accuracy was 74.7%. Hence, the BLSTM layer contributes more toward the network learning than the FC layer (p < 0.001). To compare, the FIGURE 5 | Boxplots showing the decoding accuracies obtained by ablating the different blocks such as FC layer or BLSTM layer. To obtain the test accuracies after ablating, the ablated network was trained from scratch in case of FC_rem and BLSTM_rem. However, in case of Audio_rem and EEG_rem, accuracies were calculated by zeroing out the corresponding input features before passing them to a fully trained network. The obtained accuracy did not demonstrate a statistically significant difference between Audio_rem and EEG_rem (p > 0.05). For all other cases, there was a significant difference ( * * * p < 0.001 based on paired Wilcoxon signed-rank test). median decoding accuracy calculated using the full the network was 77.2%.\n\n\nSparse Neural Network Using Magnitude Pruning\n\nTo investigate the degree of sparsity that our neural network can tolerate, we pruned the model at 40, 50, 60, 70, and 80% sparsity using the 3 s trial duration. In order to fine-tune the pruned neural network, there are two options: (1) sequential or (2) oneshot. In sequential fine-tuning, weights of the trained original model are reinitialized to zero in smaller steps per epoch until the required sparsity is attained. In one-shot fine-tuning, weights of the trained original model are reinitialized to zero at one shot in the first epoch and the sparse model is further trained to improve performance. We observed that the sequential fine-tuning is less efficient than one-shot fine-tuning in terms of training time budget. Therefore, all results presented here are based on oneshot fine-tuning. We achieved a median decoding accuracy of 76.9% at a sparsity of 40% which is statistically identical to the original model at 77.2% (p > 0.05). When the sparsity was increased to 50%, the median decoding accuracy decreased to 75.7% which was lower than the original model (p < 0.001). Increasing the sparsity level further resulted in deterioration of decoding accuracy reaching 63.2% at a sparsity of 80% (Figure 6). Total number of learnable parameters in our model was 416,741 and to find the sparse network, we pruned only the weights leaving the bias and BN parameters unchanged.\n\n\nDISCUSSION\n\nPeople with hearing loss suffer from deteriorated speech intelligibility in noisy acoustic environments such as multispeaker scenarios. Increasing the audibility by means of hearing aids has not shown to provide sufficient improvement to the speech intelligibility. This is because the hearing aids are unable to estimate apriori to which speaker the user intends to listen. Hence, hearing aids amplify both the wanted signal (attended speaker) and interfering signals (ignored speakers). Recently, it has been shown that the cortical signals measured FIGURE 6 | Plots comparing the trade off between decoding accuracies and sparsity level (*p < 0.05; ***p < 0.001 based on paired Wilcoxon signed-rank test).\n\nusing EEG could infer the auditory attention by discriminating between the attended speaker and the ignored speaker in a dual-speaker scenario (O'Sullivan et al., 2014). Linear system analysis has been the commonly used methodology to analyse the EEG signals measured from a listener performing selective attention. However, in recent years, non-linear analyses based on neural networks have become prominent, thanks to the availability of customized hardware accelerators and associated software libraries.\n\nIn this work, we developed a joint CNN-LSTM model to infer the auditory attention of a listener in a dual-speaker environment. CNNs take the EEG signal and spectrogram of the multiple speakers as inputs and extract features through successive convolutions. These convolutions generate an intermediate embeddings of the inputs which are then given to a BLSTM layer. As LSTMs fall under the category of recurrent neural networks, they can model the temporal relationship between the EEG embedding and the multiple spectrogram embeddings. Finally, the output of the BLSTM is processed through FC layers to infer the auditory attention. The effectiveness of the proposed neural network was evaluated with the help of three different EEG datasets collected from subjects who undertook dual-speaker experiment.\n\nThere are many choices for the acoustic cues of speech signal that could be given as input to the neural network. They are speech onsets (Howard and Poeppel, 2010), speech envelopes (Aiken and Picton, 2008), speech spectrograms (Pasley et al., 2012), or phonemes (Di Liberto et al., 2015). Due to the hierarchical processing of speech, all of the aforementioned cues could be tracked from the cortical signals measured using EEG (Hickok and Poeppel, 2007;Ding and Simon, 2014). Speech envelope is the most commonly used acoustic cues in the linear system analysis of EEG signal. However, we decided to use spectrogram due to its rich representational power of the corresponding speech signal and the ability of neural networks to index these multidimensional inputs efficiently.\n\n\nAttention Decoding Accuracy\n\nWe analyzed the performance of our neural network in two different training scenarios. In the first scenario, individual dataset accuracy was found out by training the network using samples taken only from that particular dataset. In the second scenario, individual dataset accuracy was found out by training using samples combined from all three datasets together. The accuracies obtained in the second scenario were higher than the first scenario by 10.8% on average, which is in agreement with the premise of neural network learning that larger the amount of training data, the better the generalization. The decoding accuracies obtained for subjects belonging to the DTU_Dataset were markedly lower than the other two datasets similar to the observation made in Geirnaert et al. (2020). While the exact reason for the lower performance is unclear, a major difference of the DTU_Dataset compared to the other two datasets was that the former consisted of attention to male and female speakers whereas the latter consisted of attention to only male speakers. Therefore, training with additional EEG data that consist of attention to female speakers can provide more insights into the lower performance. Additionally, we investigated the crossset performance by training the model using one dataset and testing using the other two datasets. The accuracies obtained were all at chance level as seen in Figure 3. This is not against our expectation because if the underlying training set is not representative, neural networks will not generalize. Specifically, features in the training set and the test set are different since they were recorded in different audio settings, languages, and EEG devices. This further affirms the importance of having a large and diverse training set for the neural networks to function efficiently.\n\n\nDecoding Accuracy vs. Trial Duration\n\nOne of the major challenges that AAD algorithms based on linear system theory faces is the deteriorated decoding performance when the trial duration is reduced. To this end, we calculated the accuracies using our neural network for different trial durations of 2, 3, 4, and 5 s. We observed a clear performance improvement when trial duration was increased from 2 to 3 s whereas for all other trial durations, accuracies did not improve substantially (Figure 4). However, increasing the trial duration will result in larger latency needed to infer the auditory attention that can adversely affect applications which require real-time operation. Hence, 3 s trial duration may be an optimal operation point as it is known from a previous study that human brain tracks the sentence phrases and phrases are normally not longer than 3 s (Vander Ghinst et al., 2019). Similarly, our analysis made use of 10 electrodes distributed all over the scalp but future work should investigate the effect of reducing the number of electrodes. This will help in integrating algorithms based on neural networks into devices such as hearing aids. We anticipate that the current network will require modifications with additional hyperparameter tuning in order to accommodate for the reduction in number of electrodes, as the fewer is the number of electrodes, the lower is the amount of data available for training.\n\n\nAblation Analysis\n\nPerforming ablation analysis gives the possibility to evaluate the contribution of different inputs and modules in a neural network. To our model, when only the speech features were given as input, the median decoding accuracy was 48.6% whereas only EEG features as input resulted in an accuracy of 53.6% (Figure 5). However, statistical analysis revealed that there is no significant difference between the two. This is contrary to our anticipation because we expected the model to learn more from the EEG features than from the audio features, as the EEG signal is unique to the subject while the audio stimulus was repeated among subjects per dataset. Nevertheless, in future care must be taken to design the experiment in such a way as to incorporate diverse speech stimuli. Further analysis ablating the BLSTM layer and the FC layers revealed that the BLSTM layer was more important than the FC layers. This is probably due to the ability of the LSTM layer to model the temporal delay between speech cues and the EEG. However, we anticipate that when the training datasets become larger and more dissimilar, FC layers will become more important due to the improved representation and optimization power of dense networks (Luo et al., 2017).\n\n\nSparse Neural Networks\n\nAlthough neural networks achieve state-of-the-art performances for a wide range of applications, they have large memory footprint and require extremely high computation power. Over the years, neural networks were able to extend their scope of applications was by scaling up the network size. In 1998, the CNN model (LeNet) that was successful in recognizing handwritten digits consisted of under a million parameters (LeCun et al., 1998), whereas AlexNet that won the ImageNet challenge in 2012 consisted of 60 million parameters (Krizhevsky et al., 2017). Neural networks were further scaled up to the order of 10 billion parameters and efficient methods to train these extremely large networks were presented in Coates et al. (2013). While these large models are very powerful, running them on embedded devices poses huge challenges due to the large memory and computation requirements. Sparse neural networks are a novel architecture search where redundant weights are reinitialized to zero thereby reducing the computation load.\n\nInvestigation into the amount of sparsity that our neural network can tolerate revealed a tolerance of upto 50% sparsity without substantial loss of accuracy (Figure 6). However, standard benchmarking on sparsity has found that deep networks such as ResNet-50 can tolerate upto 90% sparsity (Gale et al., 2019). One of the potential reasons for the lower level of sparsity in our model is due to its shallow nature. That is, our model is comprised of less than half a million learnable parameters while deep networks such as ResNet-50 is comprised of over 25 million learnable parameters. It is also interesting to note that the accuracy obtained by removing the FC layer in our ablation analysis was 74.6% compared to the full network accuracy of 77.2%. And the ablated network consisted of 105,605 parameters which is approximately only a quarter of the total number of parameters (416,741) of the original network. This shows that by careful design choices, we can reduce the network size considerably compared to an automatic sparse network search using magnitude pruning.\n\nSparsification of neural network has also been investigated as a neural network architecture search rather than merely as an optimization procedure. In the lottery ticket hypothesis presented in Frankle and Carbin (2018), authors posit that, inside the structure of an over-parameterized network, there exist subnetworks (winning tickets) that when trained in isolation reaches accuracies comparable to the original network. The prerequisite to achieve comparable accuracy is to initialize the sparse network using the original random weight initialization that was used to obtain the sparse architecture. However, it was shown that with careful choice of the learning rate, the stringent requirement on original weight initialization can be relaxed and the sparse network can be trained from scratch for any random initialization (Liu et al., 2018).\n\nOne of the assumptions that we have made throughout this paper is the availability of clean speech signal to obtain the spectrogram. In practice, only noisy mixtures are available and speech sources must be separated before the spectrogram can be calculated. This is an active research field and algorithms are already available based on classical signal processing such as beamforming or based on deep neural networks (Wang and Chen, 2018). Another challenge in neural network learning and in particular, its application in EEG research is the scarcity of labeled data to train the network. This limits the ability of network to generalize well to unseen EEG data. To mitigate the aforementioned limitation, data augmentation techniques are widely used in neural network training. Data augmentation is a procedure to generate synthetic dataset that spans unexplored input signal space but corresponding to the true labels (Wen et al., 2020). In auditory attention paradigm, linear system analyses have shown that the TRF properties differ between attended and ignored speakers (Fiedler et al., 2019;Kuruvila et al., 2021). As a result, synthetic EEG can be generated by performing a linear convolution between TRFs and the corresponding speech signal cues (Miran et al., 2018). The signal-to-noise ratio of the synthesized EEG can be varied by adding appropriate noise to the convolved signal. The most commonly used speech cue is the signal envelope obtained using Hilbert transform. However, more sophisticated envelope extraction methods such as the computational models simulating the auditory system could improve the quality of synthesized EEG signals (Kates, 2013;Verhulst et al., 2018). It must be noted that the data augmentation techniques must only be used to train the network. The validation and the testing procedure must still be performed using real datasets.\n\n\nCONCLUSION\n\nIntegrating EEG to track the cortical signals is one of the latest proposals to enhance the quality of service provided by hearing aids to the users. EEG is envisaged to provide neuro-feedback about the user's intention thereby enabling the hearing aid to infer and enhance the attended speech signals. In the present study, we propose a joint CNN-LSTM network to classify the attended speaker in order to infer the auditory attention of a listener. The proposed neural network uses speech spectrograms and EEG signals as inputs to infer the auditory attention. Results obtained by training the network using three different EEG datasets collected from multiple subjects who undertook a dual-speaker experiment showed that our network generalizes well to different scenarios. Investigation into the importance of different constituents of our network architecture revealed that adding an LSTM layer improved the performance of the model considerably. Evaluating sparsity on the proposed joint CNN-LSTM network demonstrates that the network can tolerate upto 50% sparsity without considerable deterioration in performance. These results could pave way to integrate algorithms based on neural networks into hearing aids that have constrained memory and computational power.\n\n\nDATA AVAILABILITY STATEMENT\n\nThe original contributions presented in the study are included in the article/supplementary material, further inquiries can be directed to the corresponding author/s.\n\n\nETHICS STATEMENT\n\nThe studies involving human participants were reviewed and approved by Ethics committee, University of Erlangen-Nuremberg (Protocol Number: 314_18B) on 18th September 2018. The patients/participants provided their written informed consent to participate in this study.\n\n\nAUTHOR CONTRIBUTIONS\n\nUH and EF conceived the study and contributed to data analysis, supervision, and manuscript writing. JM contributed to the results interpretation and data analysis. IK is the main author and performed most of the data analysis. All authors contributed to the article and approved the submitted version.\n\n\nFUNDING\n\nThis work was supported by a grant from Johannes und Frieda Marohn-Stiftung, Erlangen.\n\nFIGURE 3 |\n3Boxplot showing the decoding accuracies obtained for cross-set training scenario. The accuracies obtained were all at chance level.\n\nTABLE 1 |\n1Details of the EEG datasets analyzed.Name \nNumber of \nsubjects \n\nDuration \nper subject \n(minutes) \n\nTotal \nduration \n(hours) \n\nExperiment \ntype \n\nLanguage \n\nFAU_Dataset \n27 \n30 \n13.5 \nMale + \nMale \n\nGerman \n\nDTU_Dataset \n18 \n50 \n15 \nMale + \nFemale \n\nDanish \n\nKUL_Dataset \n16 \n24 \n6.4 \nMale + \nMale \n\nDutch \n\nTABLE 2 | Trial duration vs. dimension of the input. \n\nTrial duration (sec) \nEEG data \n(time \u00d7 num_electrodes) \n\nSpeech data \n(time \u00d7 freq) \n\n2 \n128 \u00d7 10 \n101 \u00d7 257 \n\n3 \n192 \u00d7 10 \n151 \u00d7 257 \n\n4 \n256 \u00d7 10 \n201 \u00d7 257 \n\n5 \n320 \u00d7 10 \n251 \u00d7 257 \n\n\n\nTABLE 3 |\n3CNN parameters of the EEG subnetwork.Number of kernels Kernel size Dilation Padding Maxpool \n\nLayer 1 \n32 \n24 \u00d7 1 \n1.1 \n12.0 \n2.1 \n\nLayer 2 \n32 \n7 \u00d7 1 \n2.1 \n6.0 \n1.2 \n\nLayer 3 \n32 \n7 \u00d7 5 \n1.1 \n3.2 \n2.5 \n\nLayer 4 \n32 \n7 \u00d7 1 \n1.1 \n3.0 \n1.1 \n\n\n\nTABLE 4 |\n4CNN parameters of the Audio subnetwork.FIGURE 1 | The architecture of the proposed joint CNN-LSTM model. Input to the audio stream is the spectrogram of speech signals and input to the EEG stream is the downsampled version of EEG signals. Number of Audio_CNNs depends on the number of speakers present in the auditory scene (here two). From the outputs of Audio_CNN and EEG_CNN, speech and EEG embeddings are created which are concatenated together and passed to a BLSTM layer followed by FC layers.Frontiers in Physiology | www.frontiersin.org fixed at 48 \u00d7 16. As the datasets considered in this study were taken from dual-speaker experiments, the Audio_CNN was run twice resulting in two sets of output.Number of kernels Kernel size Dilation Padding Maxpool \n\nLayer 1 \n32 \n1 \u00d7 7 \n1.1 \n0.3 \n1.1 \n\nLayer 2 \n32 \n7 \u00d7 1 \n1.1 \n0.0 \n1.4 \n\nLayer 3 \n32 \n3 \u00d7 5 \n8.8 \n0.16 \n1.2 \n\nLayer 4 \n32 \n3 \u00d7 3 \n16.16 \n0.16 \n1.1 \n\nLayer 5 \n1 \n1 \u00d7 1 \n1.1 \n0.0 \n2.2 \n\n\nFrontiers in Physiology | www.frontiersin.org\nAugust 2021 | Volume 12 | Article 700655\nACKNOWLEDGMENTSWe convey our gratitude to all participants who took part in the study and would like to thank the student Laura Rupprecht who helped us with data acquisition.Conflict of Interest:The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.Publisher's Note: All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.Copyright \u00a9 2021 Kuruvila, Muncke, Fischer and Hoppe. This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.\nHuman cortical responses to the speech envelope. S J Aiken, T W Picton, 10.1097/AUD.0b013e31816453dcEar Hear. 29Aiken, S. J., and Picton, T. W. (2008). Human cortical responses to the speech envelope. Ear Hear. 29, 139-157. doi: 10.1097/AUD.0b013e31816453dc\n\nAuditory-inspired speech envelope extraction methods for improved eeg-based auditory attention detection in a cocktail party scenario. W Biesmans, N Das, T Francart, A Bertrand, 10.1109/TNSRE.2016.2571900IEEE Trans. Neural Syst. Rehabil. Eng. 25Biesmans, W., Das, N., Francart, T., and Bertrand, A. (2017). Auditory-inspired speech envelope extraction methods for improved eeg-based auditory attention detection in a cocktail party scenario. IEEE Trans. Neural Syst. Rehabil. Eng. 25, 402-412. doi: 10.1109/TNSRE.2016.2571900\n\nSemantic context enhances the early auditory encoding of natural speech. M P Broderick, A J Anderson, E C Lalor, 10.1523/JNEUROSCI.0584-19.2019J. Neurosci. 39Broderick, M. P., Anderson, A. J., and Lalor, E. C. (2019). Semantic context enhances the early auditory encoding of natural speech. J. Neurosci. 39, 7564-7575. doi: 10.1523/JNEUROSCI.0584-19.2019\n\nSome experiments on the recognition of speech, with one and with two ears. E C Cherry, 10.1121/1.1907229J. Acoust. Soc. Am. 25Cherry, E. C. (1953). Some experiments on the recognition of speech, with one and with two ears. J. Acoust. Soc. Am. 25, 975-979. doi: 10.1121/1.1907229\n\nComparison of two-talker attention decoding from EEG with nonlinear neural networks and linear methods. G Ciccarelli, M Nolan, J Perricone, P T Calamia, S Haro, J O&apos;sullivan, 10.1038/s41598-019-47795-0Sci. Rep. 9Ciccarelli, G., Nolan, M., Perricone, J., Calamia, P. T., Haro, S., O'Sullivan, J., et al. (2019). Comparison of two-talker attention decoding from EEG with nonlinear neural networks and linear methods. Sci. Rep. 9, 1-10. doi: 10.1038/s41598-019-47795-0\n\nDeep learning with COTS HPC systems. A Coates, B Huval, T Wang, D Wu, B Catanzaro, Andrew , N , International Conference on Machine Learning. Atlanta, GACoates, A., Huval, B., Wang, T., Wu, D., Catanzaro, B., and Andrew, N. (2013). \"Deep learning with COTS HPC systems, \" in International Conference on Machine Learning (Atlanta, GA), 1337-1345.\n\nDeep learning for electroencephalogram (EEG) classification tasks: a review. A Craik, Y He, J L Contreras-Vidal, 10.1088/1741-2552/ab0ab5J. Neural Eng. 1631001Craik, A., He, Y., and Contreras-Vidal, J. L. (2019). Deep learning for electroencephalogram (EEG) classification tasks: a review. J. Neural Eng. 16:031001. doi: 10.1088/1741-2552/ab0ab5\n\nThe effect of headrelated filtering and ear-specific decoding bias on auditory attention detection. N Das, W Biesmans, A Bertrand, T Francart, 10.1088/1741-2560/13/5/056014J. Neural Eng. 1356014Das, N., Biesmans, W., Bertrand, A., and Francart, T. (2016). The effect of head- related filtering and ear-specific decoding bias on auditory attention detection. J. Neural Eng. 13:056014. doi: 10.1088/1741-2560/13/5/056014\n\nAuditory attention detection dataset KULeuven. N Das, T Francart, A Bertrand, Version 1.0.0) [Data setDas, N., Francart, T., and Bertrand, A. (2019). Auditory attention detection dataset KULeuven (Version 1.0.0) [Data set].\n\n. 10.5281/zenodo.3377911Zenodo. doi: 10.5281/zenodo.3377911\n\nMachine learning for decoding listeners' attention from electroencephalography evoked by continuous speech. T De Taillez, B Kollmeier, B T Meyer, 10.1111/ejn.13790Eur. J. Neurosci. 51de Taillez, T., Kollmeier, B., and Meyer, B. T. (2020). Machine learning for decoding listeners' attention from electroencephalography evoked by continuous speech. Eur. J. Neurosci. 51, 1234-1241. doi: 10.1111/ejn.13790\n\nLow-frequency cortical entrainment to speech reflects phoneme-level processing. G M Di Liberto, J A O&apos;sullivan, E C Lalor, 10.1016/j.cub.2015.08.030Curr. Biol. 25Di Liberto, G. M., O'Sullivan, J. A., and Lalor, E. C. (2015). Low-frequency cortical entrainment to speech reflects phoneme-level processing. Curr. Biol. 25, 2457-2465. doi: 10.1016/j.cub.2015.08.030\n\nNeural coding of continuous speech in auditory cortex during monaural and dichotic listening. N Ding, J Z Simon, 10.1152/jn.00297.2011J. Neurophysiol. 107Ding, N., and Simon, J. Z. (2012). Neural coding of continuous speech in auditory cortex during monaural and dichotic listening. J. Neurophysiol. 107, 78-89. doi: 10.1152/jn.00297.2011\n\nCortical entrainment to continuous speech: functional roles and interpretations. N Ding, J Z Simon, 10.3389/fnhum.2014.00311Front. Hum. Neurosci. 8311Ding, N., and Simon, J. Z. (2014). Cortical entrainment to continuous speech: functional roles and interpretations. Front. Hum. Neurosci. 8:311. doi: 10.3389/fnhum.2014.00311\n\nLooking to listen at the cocktail party: a speaker-independent audio-visual model for speech separation. A Ephrat, I Mosseri, O Lang, T Dekel, K Wilson, A Hassidim, 10.1145/3197517.3201357arXiv:1804.03619arXiv preprintEphrat, A., Mosseri, I., Lang, O., Dekel, T., Wilson, K., Hassidim, A., et al. (2018). Looking to listen at the cocktail party: a speaker-independent audio-visual model for speech separation. arXiv preprint arXiv:1804.03619. doi: 10.1145/3197517.3201357\n\nWhat is a visual object?. J Feldman, 10.1016/S1364-6613(03)00111-6Trends Cogn. Sci. 7Feldman, J. (2003). What is a visual object? Trends Cogn. Sci. 7, 252-256. doi: 10.1016/S1364-6613(03)00111-6\n\nSingle-channel in-ear-EEG detects the focus of auditory attention to concurrent tone streams and mixed speech. L Fiedler, M Woestmann, C Graversen, A Brandmeyer, T Lunner, J Obleser, 10.1088/1741-2552/aa66ddJ. Neural Eng. 1436020Fiedler, L., Woestmann, M., Graversen, C., Brandmeyer, A., Lunner, T., and Obleser, J. (2017). Single-channel in-ear-EEG detects the focus of auditory attention to concurrent tone streams and mixed speech. J. Neural Eng. 14:036020. doi: 10.1088/1741-2552/aa66dd\n\nLate cortical tracking of ignored speech facilitates neural selectivity in acoustically challenging conditions. L Fiedler, M W\u00f6stmann, S K Herbst, J Obleser, 10.1016/j.neuroimage.2018.10.057NeuroImage. 186Fiedler, L., W\u00f6stmann, M., Herbst, S. K., and Obleser, J. (2019). Late cortical tracking of ignored speech facilitates neural selectivity in acoustically challenging conditions. NeuroImage 186, 33-42. doi: 10.1016/j.neuroimage.2018.10.057\n\nThe lottery ticket hypothesis: finding sparse. J Frankle, M Carbin, arXiv:1803.03635TRainable neural networks. arXiv preprintFrankle, J., and Carbin, M. (2018). The lottery ticket hypothesis: finding sparse, TRainable neural networks. arXiv preprint arXiv:1803.03635.\n\nNoise-robust cortical tracking of attended speech in real-world acoustic scenes. S A Fuglsang, T Dau, J Hjortkjaer, 10.1016/j.neuroimage.2017.04.026NeuroImage. 156Fuglsang, S. A., Dau, T., and Hjortkjaer, J. (2017). Noise-robust cortical tracking of attended speech in real-world acoustic scenes. NeuroImage 156, 435-444. doi: 10.1016/j.neuroimage.2017.04.026\n\nEEG and audio dataset for auditory attention decoding (Version 1) [Data set. S A Fuglsang, D D E Wong, J Hjortkjaer, 10.5281/zenodo.1199011Fuglsang, S. A., Wong, D. D. E., and Hjortkjaer, J. (2018). EEG and audio dataset for auditory attention decoding (Version 1) [Data set]. Zenodo. doi: 10.5281/zenodo.1199011\n\nT Gale, E Elsen, S Hooker, arXiv:1902.09574The state of sparsity in deep neural networks. arXiv preprintGale, T., Elsen, E., and Hooker, S. (2019). The state of sparsity in deep neural networks. arXiv preprint arXiv:1902.09574.\n\nAn interpretable performance metric for auditory attention decoding algorithms in a context of neurosteered gain control. S Geirnaert, T Francart, A Bertrand, 10.1101/745695IEEE Trans. Neural Syst. Rehabil. Eng. 28Geirnaert, S., Francart, T., and Bertrand, A. (2019). An interpretable performance metric for auditory attention decoding algorithms in a context of neuro- steered gain control. IEEE Trans. Neural Syst. Rehabil. Eng. 28, 307-317. doi: 10.1101/745695\n\nS Geirnaert, S Vandecappelle, E Alickovic, A De Cheveign\u00e9, E Lalor, B T Meyer, 10.1109/MSP.2021.3075932arXiv:2008.04569Neuro-steered hearing devices: decoding auditory attention from the brain. arXiv preprintGeirnaert, S., Vandecappelle, S., Alickovic, E., de Cheveign\u00e9, A., Lalor, E., Meyer, B. T., et al. (2020). Neuro-steered hearing devices: decoding auditory attention from the brain. arXiv preprint arXiv:2008.04569. doi: 10.1109/MSP.2021.3075932\n\nP Goyal, P Doll\u00e1r, R Girshick, P Noordhuis, L Wesolowski, A Kyrola, arXiv:1706.02677Accurate, large minibatch SGD: training imagenet in 1 hour. arXiv preprintGoyal, P., Doll\u00e1r, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., et al. (2017). Accurate, large minibatch SGD: training imagenet in 1 hour. arXiv preprint arXiv:1706.02677.\n\nLearning both weights and connections for efficient neural network. S Han, J Pool, J Tran, W Dally, Adv. Neural Inform. Process. Syst. 28Han, S., Pool, J., Tran, J., and Dally, W. (2015). Learning both weights and connections for efficient neural network. Adv. Neural Inform. Process. Syst. 28, 1135-1143.\n\nThe cortical organization of speech processing. G Hickok, D Poeppel, 10.1038/nrn2113Nat. Rev. Neurosci. 8Hickok, G., and Poeppel, D. (2007). The cortical organization of speech processing. Nat. Rev. Neurosci. 8, 393-402. doi: 10.1038/nrn2113\n\nLong short-term memory. S Hochreiter, J Schmidhuber, 10.1162/neco.1997.9.8.1735Neural Comput. 9Hochreiter, S., and Schmidhuber, J. (1997). Long short-term memory. Neural Comput. 9, 1735-1780. doi: 10.1162/neco.1997.9.8.1735\n\nDiscrimination of speech stimuli based on neuronal response phase patterns depends on acoustics but not comprehension. M F Howard, D Poeppel, 10.1152/jn.00251.2010J. Neurophysiol. 104Howard, M. F., and Poeppel, D. (2010). Discrimination of speech stimuli based on neuronal response phase patterns depends on acoustics but not comprehension. J. Neurophysiol. 104, 2500-2511. doi: 10.1152/jn.002 51.2010\n\nBatch normalization: accelerating deep network training by reducing internal covariate shift. S Ioffe, C Szegedy, PMLRInternational Conference on Machine Learning. LilleIoffe, S., and Szegedy, C. (2015). \"Batch normalization: accelerating deep network training by reducing internal covariate shift, \" in International Conference on Machine Learning (Lille: PMLR), 448-456.\n\nAn auditory model for intelligibility and quality predictions. J Kates, 10.1121/1.4799223Proceedings of Meetings on Acoustics ICA2013. Meetings on Acoustics ICA2013Montr\u00e9al, QCAcoustical Society of America1950184Kates, J. (2013). \"An auditory model for intelligibility and quality predictions, \" in Proceedings of Meetings on Acoustics ICA2013, Vol. 19 (Montr\u00e9al, QC: Acoustical Society of America), 050184. doi: 10.1121/1.4799223\n\nAdam: a method for stochastic optimization. D P Kingma, J Ba, arXiv:1412.6980arXiv preprintKingma, D. P., and Ba, J. (2014). Adam: a method for stochastic optimization. arXiv preprint arXiv:1412.6980.\n\nImageNet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, 10.1145/3065386Commun. ACM. 60Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2017). ImageNet classification with deep convolutional neural networks. Commun. ACM 60, 84-90. doi: 10.1145/3065386\n\nInference of the selective auditory attention using sequential LMMSE estimation. I Kuruvila, K C Demir, E Fischer, U Hoppe, 10.1109/TBME.2021.3075337IEEE Trans. Biomed. Eng. Kuruvila, I., Demir, K. C., Fischer, E., and Hoppe, U. (2021). Inference of the selective auditory attention using sequential LMMSE estimation. IEEE Trans. Biomed. Eng. doi: 10.1109/TBME.2021.3075337\n\nAn LMMSE-based estimation of temporal response function in auditory attention decoding. I Kuruvila, E Fischer, U Hoppe, 10.1109/EMBC44109.2020.91758662020 42nd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC). Montr\u00e9al, QCKuruvila, I., Fischer, E., and Hoppe, U. (2020). \"An LMMSE-based estimation of temporal response function in auditory attention decoding, \" in 2020 42nd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC) (Montr\u00e9al, QC), 2837-2840. doi: 10.1109/EMBC44109.2020.9175866\n\nNeural responses to uninterrupted natural speech can be extracted with precise temporal resolution. E C Lalor, J J Foxe, 10.1111/j.1460-9568.2009.07055.xEur. J. Neurosci. 31Lalor, E. C., and Foxe, J. J. (2010). Neural responses to uninterrupted natural speech can be extracted with precise temporal resolution. Eur. J. Neurosci. 31, 189-193. doi: 10.1111/j.1460-9568.2009.07055.x\n\nGradient-based learning applied to document recognition. Y Lecun, L Bottou, Y Bengio, P Haffner, 10.1109/5.726791Proc. IEEE. 86LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998). Gradient-based learning applied to document recognition. Proc. IEEE 86, 2278-2324. doi: 10.1109/5.726791\n\nRethinking the value of network pruning. Z Liu, M Sun, T Zhou, G Huang, Darrell , T , arXiv:1810.05270arXiv preprintLiu, Z., Sun, M., Zhou, T., Huang, G., and Darrell, T. (2018). Rethinking the value of network pruning. arXiv preprint arXiv:1810.05270.\n\nC Louizos, M Welling, D P Kingma, arXiv:1712.01312Learning sparse neural networks through L_0 regularization. arXiv preprintLouizos, C., Welling, M., and Kingma, D. P. (2017). Learning sparse neural networks through L_0 regularization. arXiv preprint arXiv:1712.01312.\n\nThinet: a filter level pruning method for deep neural network compression. J.-H Luo, J Wu, Lin , W , 10.1109/ICCV.2017.541Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionVeniceLuo, J.-H., Wu, J., and Lin, W. (2017). \"Thinet: a filter level pruning method for deep neural network compression, \" in Proceedings of the IEEE International Conference on Computer Vision (Venice), 5058-5066. doi: 10.1109/ICCV.2017.541\n\nSelective cortical representation of attended speaker in multi-talker speech perception. N Mesgarani, Chang , E F , 10.1038/nature11020Nature. 485Mesgarani, N., and Chang, E. F. (2012). Selective cortical representation of attended speaker in multi-talker speech perception. Nature 485, 233-236. doi: 10.1038/nature11020\n\nReal-time tracking of selective auditory attention from M/EEG: a Bayesian filtering approach. S Miran, S Akram, A Sheikhattar, J Z Simon, T Zhang, B Babadi, 10.3389/fnins.2018.00262Front. Neurosci. 12262Miran, S., Akram, S., Sheikhattar, A., Simon, J. Z., Zhang, T., and Babadi, B. (2018). Real-time tracking of selective auditory attention from M/EEG: a Bayesian filtering approach. Front. Neurosci. 12:262. doi: 10.3389/fnins.2018.00262\n\nDecoding the attended speech stream with multi-channel EEG: implications for online, daily-life applications. B Mirkovic, S Debener, M Jaeger, De Vos, M , 10.1088/1741-2560/12/4/046007J. Neural Eng. 1246007Mirkovic, B., Debener, S., Jaeger, M., and De Vos, M. (2015). Decoding the attended speech stream with multi-channel EEG: implications for online, daily-life applications. J. Neural Eng. 12:046007. doi: 10.1088/1741-2560/12/4/046007\n\nD Molchanov, A Ashukha, D Vetrov, arXiv:1701.05369Variational dropout sparsifies deep neural networks. arXiv preprintMolchanov, D., Ashukha, A., and Vetrov, D. (2017). Variational dropout sparsifies deep neural networks. arXiv preprint arXiv:1701.05369.\n\nAn LSTM based architecture to relate speech stimulus to EEG. M J Monesi, B Accou, J Montoya-Martinez, T Francart, H Van Hamme, 10.1109/ICASSP40776.2020.9054000ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing. BarcelonaMonesi, M. J., Accou, B., Montoya-Martinez, J., Francart, T., and Van Hamme, H. (2020). \"An LSTM based architecture to relate speech stimulus to EEG, \" in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (Barcelona), 941-945. doi: 10.1109/ICASSP40776.2020.9054000\n\nS Narang, E Elsen, G Diamos, S Sengupta, arXiv:1704.05119Exploring sparsity in recurrent neural networks. arXiv preprintNarang, S., Elsen, E., Diamos, G., and Sengupta, S. (2017). Exploring sparsity in recurrent neural networks. arXiv preprint arXiv:1704.05119.\n\nAttentional selection in a cocktail party environment can be decoded from single-trial EEG. J A O&apos;sullivan, A J Power, N Mesgarani, S Rajaram, J J Foxe, B G Shinn-Cunningham, 10.1093/cercor/bht355Cereb. Cortex. 25O'Sullivan, J. A., Power, A. J., Mesgarani, N., Rajaram, S., Foxe, J. J., Shinn- Cunningham, B. G., et al. (2014). Attentional selection in a cocktail party environment can be decoded from single-trial EEG. Cereb. Cortex 25, 1697-1706. doi: 10.1093/cercor/bht355\n\nReconstructing speech from human auditory cortex. B N Pasley, S V David, N Mesgarani, A Flinker, S A Shamma, N E Crone, 10.1371/journal.pbio.1001251PLoS Biol. 101001251Pasley, B. N., David, S. V., Mesgarani, N., Flinker, A., Shamma, S. A., Crone, N. E., et al. (2012). Reconstructing speech from human auditory cortex. PLoS Biol. 10:e1001251. doi: 10.1371/journal.pbio.1001251\n\nTesting the limits of the stimulus reconstruction approach: auditory attention decoding in a four-speaker free field environment. P J Sch\u00e4fer, F I Corona-Strauss, R Hannemann, S A Hillyard, D J Strauss, 10.1177/2331216518816600Trends Hear. 222331216518816600Sch\u00e4fer, P. J., Corona-Strauss, F. I., Hannemann, R., Hillyard, S. A., and Strauss, D. J. (2018). Testing the limits of the stimulus reconstruction approach: auditory attention decoding in a four-speaker free field environment. Trends Hear. 22:2331216518816600. doi: 10.1177/2331216518816600\n\nBidirectional recurrent neural networks. M Schuster, K K Paliwal, 10.1109/78.650093IEEE Trans. Signal Process. 45Schuster, M., and Paliwal, K. K. (1997). Bidirectional recurrent neural networks. IEEE Trans. Signal Process. 45, 2673-2681. doi: 10.1109/78.650093\n\nObject-based auditory and visual attention. B G Shinn-Cunningham, 10.1016/j.tics.2008.02.003Trends Cogn. Sci. 12Shinn-Cunningham, B. G. (2008). Object-based auditory and visual attention. Trends Cogn. Sci. 12, 182-186. doi: 10.1016/j.tics.2008.02.003\n\nDropout: a simple way to prevent neural networks from overfitting. N Srivastava, G Hinton, A Krizhevsky, I Sutskever, R Salakhutdinov, J. Mach. Learn. Res. 15Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2014). Dropout: a simple way to prevent neural networks from overfitting. J. Mach. Learn. Res. 15, 1929-1958.\n\nAuditory attention tracking states in a cocktail party environment can be decoded by deep convolutional neural networks. Y Tian, L Ma, 10.1088/1741-2552/ab92b2J. Neural Eng. Tian, Y., and Ma, L. (2020). Auditory attention tracking states in a cocktail party environment can be decoded by deep convolutional neural networks. J. Neural Eng. doi: 10.1088/1741-2552/ab92b2\n\nEEG-based detection of the locus of auditory attention with convolutional neural networks. bioRxiv. S Vandecappelle, L Deckers, N Das, A H Ansari, A Bertrand, T Francart, 10.7554/eLife.56481.sa2PreprintVandecappelle, S., Deckers, L., Das, N., Ansari, A. H., Bertrand, A., and Francart, T. (2021). EEG-based detection of the locus of auditory attention with convolutional neural networks. bioRxiv [Preprint]. doi: 10.7554/eLife.564 81.sa2\n\nCortical tracking of speech-in-noise develops from childhood to adulthood. M Vander Ghinst, M Bourguignon, M Niesen, V Wens, S Hassid, G Choufani, 10.1523/JNEUROSCI.1732-18.2019J. Neurosci. 39Vander Ghinst, M., Bourguignon, M., Niesen, M., Wens, V., Hassid, S., Choufani, G., et al. (2019). Cortical tracking of speech-in-noise develops from childhood to adulthood. J. Neurosci. 39, 2938-2950. doi: 10.1523/JNEUROSCI.1732-18.2019\n\nComputational modeling of the human auditory periphery: auditory-nerve responses, evoked potentials and hearing loss. S Verhulst, A Alto\u00e9, V Vasilkov, 10.1016/j.heares.2017.12.018Hear. Res. 360Verhulst, S., Alto\u00e9, A., and Vasilkov, V. (2018). Computational modeling of the human auditory periphery: auditory-nerve responses, evoked potentials and hearing loss. Hear. Res. 360, 55-75. doi: 10.1016/j.heares.2017. 12.018\n\nPushing around the locus of selection: evidence for the flexible-selection hypothesis. E K Vogel, G F Woodman, S J Luck, 10.1162/089892905775008599J. Cogn. Neurosci. 17Vogel, E. K., Woodman, G. F., and Luck, S. J. (2005). Pushing around the locus of selection: evidence for the flexible-selection hypothesis. J. Cogn. Neurosci. 17, 1907-1922. doi: 10.1162/089892905775008599\n\nSupervised speech separation based on deep learning: an overview. D Wang, Chen , J , 10.1109/TASLP.2018.2842159IEEE/ACM Trans. Audio Speech Lang. Process. 26Wang, D., and Chen, J. (2018). Supervised speech separation based on deep learning: an overview. IEEE/ACM Trans. Audio Speech Lang. Process. 26, 1702-1726. doi: 10.1109/TASLP.2018.2842159\n\nTime series data augmentation for deep learning: a survey. Q Wen, L Sun, X Song, J Gao, X Wang, H Xu, arXiv:2002.12478arXiv preprintWen, Q., Sun, L., Song, X., Gao, J., Wang, X., and Xu, H. (2020). Time series data augmentation for deep learning: a survey. arXiv preprint arXiv:2002. 12478.\n\nTo prune, or not to prune: exploring the efficacy of pruning for model compression. M Zhu, S Gupta, arXiv:171 0.01878arXiv preprintZhu, M., and Gupta, S. (2017). To prune, or not to prune: exploring the efficacy of pruning for model compression. arXiv preprint arXiv:171 0.01878.\n\nE Zwicker, H Fastl, Psychoacoustics: Facts and Models. BerlinSpringer Science & Business Media22Zwicker, E., and Fastl, H. (2013). Psychoacoustics: Facts and Models, Vol. 22. Berlin: Springer Science & Business Media.\n", "annotations": {"author": "[{\"end\":134,\"start\":113},{\"end\":150,\"start\":135},{\"end\":172,\"start\":151},{\"end\":214,\"start\":173},{\"end\":338,\"start\":215},{\"end\":458,\"start\":339},{\"end\":504,\"start\":459},{\"end\":626,\"start\":505},{\"end\":653,\"start\":627},{\"end\":692,\"start\":654},{\"end\":719,\"start\":693}]", "publisher": null, "author_last_name": "[{\"end\":133,\"start\":124},{\"end\":149,\"start\":143},{\"end\":171,\"start\":165},{\"end\":185,\"start\":180},{\"end\":229,\"start\":221},{\"end\":349,\"start\":343},{\"end\":473,\"start\":466},{\"end\":517,\"start\":512}]", "author_first_name": "[{\"end\":119,\"start\":113},{\"end\":123,\"start\":120},{\"end\":142,\"start\":135},{\"end\":158,\"start\":151},{\"end\":164,\"start\":159},{\"end\":179,\"start\":173},{\"end\":220,\"start\":215},{\"end\":342,\"start\":339},{\"end\":465,\"start\":459},{\"end\":511,\"start\":505}]", "author_affiliation": "[{\"end\":337,\"start\":231},{\"end\":457,\"start\":351},{\"end\":503,\"start\":475},{\"end\":625,\"start\":519},{\"end\":652,\"start\":628},{\"end\":691,\"start\":655},{\"end\":718,\"start\":694}]", "title": "[{\"end\":99,\"start\":1},{\"end\":818,\"start\":720}]", "venue": "[{\"end\":865,\"start\":820}]", "abstract": "[{\"end\":3007,\"start\":1460}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3337,\"start\":3323},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4338,\"start\":4323},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":4528,\"start\":4504},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":4973,\"start\":4953},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":5376,\"start\":5349},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5492,\"start\":5470},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":5516,\"start\":5492},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5888,\"start\":5864},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":5909,\"start\":5888},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5933,\"start\":5909},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5956,\"start\":5933},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":6166,\"start\":6144},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6187,\"start\":6166},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":6209,\"start\":6187},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":6264,\"start\":6239},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":6286,\"start\":6264},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6308,\"start\":6286},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":6778,\"start\":6756},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7097,\"start\":7075},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7119,\"start\":7097},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7305,\"start\":7280},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":7560,\"start\":7535},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":7876,\"start\":7851},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8174,\"start\":8151},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8197,\"start\":8174},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8276,\"start\":8256},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8298,\"start\":8276},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8599,\"start\":8579},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8814,\"start\":8789},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8838,\"start\":8814},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9059,\"start\":9035},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":9091,\"start\":9064},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":9250,\"start\":9230},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":9273,\"start\":9255},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":9722,\"start\":9701},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9804,\"start\":9783},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":10531,\"start\":10508},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10653,\"start\":10630},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10688,\"start\":10670},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":11600,\"start\":11578},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":11715,\"start\":11693},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12262,\"start\":12241},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":12286,\"start\":12262},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":13079,\"start\":13063},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":13098,\"start\":13079},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":16085,\"start\":16063},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":16107,\"start\":16085},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":16583,\"start\":16558},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":16639,\"start\":16614},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":18275,\"start\":18254},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":18412,\"start\":18378},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":18439,\"start\":18412},{\"end\":18802,\"start\":18798},{\"end\":18806,\"start\":18802},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":19558,\"start\":19537},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":19792,\"start\":19772},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":20418,\"start\":20400},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":20851,\"start\":20833},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":20871,\"start\":20851},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":20891,\"start\":20871},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":21176,\"start\":21158},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":21614,\"start\":21596},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":21698,\"start\":21674},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":21742,\"start\":21720},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":21955,\"start\":21936},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":30319,\"start\":30294},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":31629,\"start\":31603},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":31672,\"start\":31648},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":31715,\"start\":31694},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":31754,\"start\":31729},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":31921,\"start\":31895},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":31942,\"start\":31921},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":33065,\"start\":33042},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":35007,\"start\":34979},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":36809,\"start\":36791},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":37274,\"start\":37254},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":37392,\"start\":37367},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":37571,\"start\":37551},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":38181,\"start\":38162},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":39169,\"start\":39144},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":39798,\"start\":39780},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":40241,\"start\":40220},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":40742,\"start\":40724},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":40901,\"start\":40879},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":40923,\"start\":40901},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":41078,\"start\":41058},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":41473,\"start\":41460},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":41495,\"start\":41473}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":44020,\"start\":43876},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":44583,\"start\":44021},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":44836,\"start\":44584},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":45795,\"start\":44837}]", "paragraph": "[{\"end\":3717,\"start\":3023},{\"end\":5671,\"start\":3719},{\"end\":7764,\"start\":5673},{\"end\":9274,\"start\":7766},{\"end\":9973,\"start\":9276},{\"end\":10280,\"start\":9975},{\"end\":10703,\"start\":10330},{\"end\":11601,\"start\":10719},{\"end\":12287,\"start\":11617},{\"end\":13099,\"start\":12303},{\"end\":13839,\"start\":13101},{\"end\":14755,\"start\":13857},{\"end\":15508,\"start\":14757},{\"end\":15691,\"start\":15533},{\"end\":17361,\"start\":15703},{\"end\":17815,\"start\":17375},{\"end\":20049,\"start\":17829},{\"end\":20892,\"start\":20094},{\"end\":22033,\"start\":20894},{\"end\":23938,\"start\":22075},{\"end\":24667,\"start\":23940},{\"end\":25290,\"start\":24708},{\"end\":26099,\"start\":25292},{\"end\":27989,\"start\":26121},{\"end\":29426,\"start\":28039},{\"end\":30149,\"start\":29441},{\"end\":30658,\"start\":30151},{\"end\":31464,\"start\":30660},{\"end\":32244,\"start\":31466},{\"end\":34106,\"start\":32276},{\"end\":35543,\"start\":34147},{\"end\":36810,\"start\":35565},{\"end\":37869,\"start\":36837},{\"end\":38947,\"start\":37871},{\"end\":39799,\"start\":38949},{\"end\":41677,\"start\":39801},{\"end\":42963,\"start\":41692},{\"end\":43161,\"start\":42995},{\"end\":43450,\"start\":43182},{\"end\":43777,\"start\":43475},{\"end\":43875,\"start\":43789}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":13156,\"start\":13149},{\"end\":15507,\"start\":15500},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":15790,\"start\":15783},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":16341,\"start\":16334},{\"end\":16871,\"start\":16864},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":17508,\"start\":17501},{\"end\":17757,\"start\":17750},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":23937,\"start\":23930}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":3021,\"start\":3009},{\"attributes\":{\"n\":\"2.\"},\"end\":10304,\"start\":10283},{\"attributes\":{\"n\":\"2.1.\"},\"end\":10328,\"start\":10307},{\"attributes\":{\"n\":\"2.1.1.\"},\"end\":10717,\"start\":10706},{\"attributes\":{\"n\":\"2.1.2.\"},\"end\":11615,\"start\":11604},{\"attributes\":{\"n\":\"2.1.3.\"},\"end\":12301,\"start\":12290},{\"attributes\":{\"n\":\"2.2.\"},\"end\":13855,\"start\":13842},{\"attributes\":{\"n\":\"3.\"},\"end\":15531,\"start\":15511},{\"attributes\":{\"n\":\"3.1.\"},\"end\":15701,\"start\":15694},{\"attributes\":{\"n\":\"3.2.\"},\"end\":17373,\"start\":17364},{\"attributes\":{\"n\":\"3.3.\"},\"end\":17827,\"start\":17818},{\"attributes\":{\"n\":\"3.4.\"},\"end\":20092,\"start\":20052},{\"attributes\":{\"n\":\"4.\"},\"end\":22043,\"start\":22036},{\"attributes\":{\"n\":\"4.1.\"},\"end\":22073,\"start\":22046},{\"attributes\":{\"n\":\"4.2.\"},\"end\":24706,\"start\":24670},{\"attributes\":{\"n\":\"4.3.\"},\"end\":26119,\"start\":26102},{\"attributes\":{\"n\":\"4.4.\"},\"end\":28037,\"start\":27992},{\"attributes\":{\"n\":\"5.\"},\"end\":29439,\"start\":29429},{\"attributes\":{\"n\":\"5.1.\"},\"end\":32274,\"start\":32247},{\"attributes\":{\"n\":\"5.2.\"},\"end\":34145,\"start\":34109},{\"attributes\":{\"n\":\"5.3.\"},\"end\":35563,\"start\":35546},{\"attributes\":{\"n\":\"5.4.\"},\"end\":36835,\"start\":36813},{\"attributes\":{\"n\":\"6.\"},\"end\":41690,\"start\":41680},{\"end\":42993,\"start\":42966},{\"end\":43180,\"start\":43164},{\"end\":43473,\"start\":43453},{\"end\":43787,\"start\":43780},{\"end\":43887,\"start\":43877},{\"end\":44031,\"start\":44022},{\"end\":44594,\"start\":44585},{\"end\":44847,\"start\":44838}]", "table": "[{\"end\":44583,\"start\":44070},{\"end\":44836,\"start\":44633},{\"end\":45795,\"start\":45555}]", "figure_caption": "[{\"end\":44020,\"start\":43889},{\"end\":44070,\"start\":44033},{\"end\":44633,\"start\":44596},{\"end\":45555,\"start\":44849}]", "figure_ref": "[{\"end\":15614,\"start\":15606},{\"end\":22791,\"start\":22781},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":23743,\"start\":23733},{\"end\":26042,\"start\":26034},{\"end\":26768,\"start\":26760},{\"end\":29258,\"start\":29248},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":33686,\"start\":33678},{\"end\":34608,\"start\":34598},{\"end\":35880,\"start\":35870},{\"end\":38039,\"start\":38029}]", "bib_author_first_name": "[{\"end\":47167,\"start\":47166},{\"end\":47169,\"start\":47168},{\"end\":47178,\"start\":47177},{\"end\":47180,\"start\":47179},{\"end\":47512,\"start\":47511},{\"end\":47524,\"start\":47523},{\"end\":47531,\"start\":47530},{\"end\":47543,\"start\":47542},{\"end\":47977,\"start\":47976},{\"end\":47979,\"start\":47978},{\"end\":47992,\"start\":47991},{\"end\":47994,\"start\":47993},{\"end\":48006,\"start\":48005},{\"end\":48008,\"start\":48007},{\"end\":48335,\"start\":48334},{\"end\":48337,\"start\":48336},{\"end\":48644,\"start\":48643},{\"end\":48658,\"start\":48657},{\"end\":48667,\"start\":48666},{\"end\":48680,\"start\":48679},{\"end\":48682,\"start\":48681},{\"end\":48693,\"start\":48692},{\"end\":48701,\"start\":48700},{\"end\":49049,\"start\":49048},{\"end\":49059,\"start\":49058},{\"end\":49068,\"start\":49067},{\"end\":49076,\"start\":49075},{\"end\":49082,\"start\":49081},{\"end\":49100,\"start\":49094},{\"end\":49104,\"start\":49103},{\"end\":49436,\"start\":49435},{\"end\":49445,\"start\":49444},{\"end\":49451,\"start\":49450},{\"end\":49453,\"start\":49452},{\"end\":49806,\"start\":49805},{\"end\":49813,\"start\":49812},{\"end\":49825,\"start\":49824},{\"end\":49837,\"start\":49836},{\"end\":50173,\"start\":50172},{\"end\":50180,\"start\":50179},{\"end\":50192,\"start\":50191},{\"end\":50520,\"start\":50519},{\"end\":50534,\"start\":50533},{\"end\":50547,\"start\":50546},{\"end\":50549,\"start\":50548},{\"end\":50896,\"start\":50895},{\"end\":50898,\"start\":50897},{\"end\":50912,\"start\":50911},{\"end\":50914,\"start\":50913},{\"end\":50933,\"start\":50932},{\"end\":50935,\"start\":50934},{\"end\":51279,\"start\":51278},{\"end\":51287,\"start\":51286},{\"end\":51289,\"start\":51288},{\"end\":51606,\"start\":51605},{\"end\":51614,\"start\":51613},{\"end\":51616,\"start\":51615},{\"end\":51956,\"start\":51955},{\"end\":51966,\"start\":51965},{\"end\":51977,\"start\":51976},{\"end\":51985,\"start\":51984},{\"end\":51994,\"start\":51993},{\"end\":52004,\"start\":52003},{\"end\":52350,\"start\":52349},{\"end\":52631,\"start\":52630},{\"end\":52642,\"start\":52641},{\"end\":52655,\"start\":52654},{\"end\":52668,\"start\":52667},{\"end\":52682,\"start\":52681},{\"end\":52692,\"start\":52691},{\"end\":53124,\"start\":53123},{\"end\":53135,\"start\":53134},{\"end\":53147,\"start\":53146},{\"end\":53149,\"start\":53148},{\"end\":53159,\"start\":53158},{\"end\":53504,\"start\":53503},{\"end\":53515,\"start\":53514},{\"end\":53807,\"start\":53806},{\"end\":53809,\"start\":53808},{\"end\":53821,\"start\":53820},{\"end\":53828,\"start\":53827},{\"end\":54164,\"start\":54163},{\"end\":54166,\"start\":54165},{\"end\":54178,\"start\":54177},{\"end\":54182,\"start\":54179},{\"end\":54190,\"start\":54189},{\"end\":54401,\"start\":54400},{\"end\":54409,\"start\":54408},{\"end\":54418,\"start\":54417},{\"end\":54752,\"start\":54751},{\"end\":54765,\"start\":54764},{\"end\":54777,\"start\":54776},{\"end\":55095,\"start\":55094},{\"end\":55108,\"start\":55107},{\"end\":55125,\"start\":55124},{\"end\":55138,\"start\":55137},{\"end\":55154,\"start\":55153},{\"end\":55163,\"start\":55162},{\"end\":55165,\"start\":55164},{\"end\":55549,\"start\":55548},{\"end\":55558,\"start\":55557},{\"end\":55568,\"start\":55567},{\"end\":55580,\"start\":55579},{\"end\":55593,\"start\":55592},{\"end\":55607,\"start\":55606},{\"end\":55964,\"start\":55963},{\"end\":55971,\"start\":55970},{\"end\":55979,\"start\":55978},{\"end\":55987,\"start\":55986},{\"end\":56251,\"start\":56250},{\"end\":56261,\"start\":56260},{\"end\":56470,\"start\":56469},{\"end\":56484,\"start\":56483},{\"end\":56790,\"start\":56789},{\"end\":56792,\"start\":56791},{\"end\":56802,\"start\":56801},{\"end\":57168,\"start\":57167},{\"end\":57177,\"start\":57176},{\"end\":57511,\"start\":57510},{\"end\":57924,\"start\":57923},{\"end\":57926,\"start\":57925},{\"end\":57936,\"start\":57935},{\"end\":58147,\"start\":58146},{\"end\":58161,\"start\":58160},{\"end\":58174,\"start\":58173},{\"end\":58176,\"start\":58175},{\"end\":58464,\"start\":58463},{\"end\":58476,\"start\":58475},{\"end\":58478,\"start\":58477},{\"end\":58487,\"start\":58486},{\"end\":58498,\"start\":58497},{\"end\":58846,\"start\":58845},{\"end\":58858,\"start\":58857},{\"end\":58869,\"start\":58868},{\"end\":59434,\"start\":59433},{\"end\":59436,\"start\":59435},{\"end\":59445,\"start\":59444},{\"end\":59447,\"start\":59446},{\"end\":59772,\"start\":59771},{\"end\":59781,\"start\":59780},{\"end\":59791,\"start\":59790},{\"end\":59801,\"start\":59800},{\"end\":60048,\"start\":60047},{\"end\":60055,\"start\":60054},{\"end\":60062,\"start\":60061},{\"end\":60070,\"start\":60069},{\"end\":60085,\"start\":60078},{\"end\":60089,\"start\":60088},{\"end\":60261,\"start\":60260},{\"end\":60272,\"start\":60271},{\"end\":60283,\"start\":60282},{\"end\":60285,\"start\":60284},{\"end\":60609,\"start\":60605},{\"end\":60616,\"start\":60615},{\"end\":60624,\"start\":60621},{\"end\":60628,\"start\":60627},{\"end\":61107,\"start\":61106},{\"end\":61124,\"start\":61119},{\"end\":61128,\"start\":61127},{\"end\":61130,\"start\":61129},{\"end\":61434,\"start\":61433},{\"end\":61443,\"start\":61442},{\"end\":61452,\"start\":61451},{\"end\":61467,\"start\":61466},{\"end\":61469,\"start\":61468},{\"end\":61478,\"start\":61477},{\"end\":61487,\"start\":61486},{\"end\":61890,\"start\":61889},{\"end\":61902,\"start\":61901},{\"end\":61913,\"start\":61912},{\"end\":61924,\"start\":61922},{\"end\":61931,\"start\":61930},{\"end\":62220,\"start\":62219},{\"end\":62233,\"start\":62232},{\"end\":62244,\"start\":62243},{\"end\":62536,\"start\":62535},{\"end\":62538,\"start\":62537},{\"end\":62548,\"start\":62547},{\"end\":62557,\"start\":62556},{\"end\":62577,\"start\":62576},{\"end\":62589,\"start\":62588},{\"end\":63049,\"start\":63048},{\"end\":63059,\"start\":63058},{\"end\":63068,\"start\":63067},{\"end\":63078,\"start\":63077},{\"end\":63404,\"start\":63403},{\"end\":63406,\"start\":63405},{\"end\":63425,\"start\":63424},{\"end\":63427,\"start\":63426},{\"end\":63436,\"start\":63435},{\"end\":63449,\"start\":63448},{\"end\":63460,\"start\":63459},{\"end\":63462,\"start\":63461},{\"end\":63470,\"start\":63469},{\"end\":63472,\"start\":63471},{\"end\":63844,\"start\":63843},{\"end\":63846,\"start\":63845},{\"end\":63856,\"start\":63855},{\"end\":63858,\"start\":63857},{\"end\":63867,\"start\":63866},{\"end\":63880,\"start\":63879},{\"end\":63891,\"start\":63890},{\"end\":63893,\"start\":63892},{\"end\":63903,\"start\":63902},{\"end\":63905,\"start\":63904},{\"end\":64302,\"start\":64301},{\"end\":64304,\"start\":64303},{\"end\":64315,\"start\":64314},{\"end\":64317,\"start\":64316},{\"end\":64335,\"start\":64334},{\"end\":64348,\"start\":64347},{\"end\":64350,\"start\":64349},{\"end\":64362,\"start\":64361},{\"end\":64364,\"start\":64363},{\"end\":64764,\"start\":64763},{\"end\":64776,\"start\":64775},{\"end\":64778,\"start\":64777},{\"end\":65029,\"start\":65028},{\"end\":65031,\"start\":65030},{\"end\":65304,\"start\":65303},{\"end\":65318,\"start\":65317},{\"end\":65328,\"start\":65327},{\"end\":65342,\"start\":65341},{\"end\":65355,\"start\":65354},{\"end\":65709,\"start\":65708},{\"end\":65717,\"start\":65716},{\"end\":66058,\"start\":66057},{\"end\":66075,\"start\":66074},{\"end\":66086,\"start\":66085},{\"end\":66093,\"start\":66092},{\"end\":66095,\"start\":66094},{\"end\":66105,\"start\":66104},{\"end\":66117,\"start\":66116},{\"end\":66472,\"start\":66471},{\"end\":66489,\"start\":66488},{\"end\":66504,\"start\":66503},{\"end\":66514,\"start\":66513},{\"end\":66522,\"start\":66521},{\"end\":66532,\"start\":66531},{\"end\":66946,\"start\":66945},{\"end\":66958,\"start\":66957},{\"end\":66967,\"start\":66966},{\"end\":67335,\"start\":67334},{\"end\":67337,\"start\":67336},{\"end\":67346,\"start\":67345},{\"end\":67348,\"start\":67347},{\"end\":67359,\"start\":67358},{\"end\":67361,\"start\":67360},{\"end\":67690,\"start\":67689},{\"end\":67701,\"start\":67697},{\"end\":67705,\"start\":67704},{\"end\":68029,\"start\":68028},{\"end\":68036,\"start\":68035},{\"end\":68043,\"start\":68042},{\"end\":68051,\"start\":68050},{\"end\":68058,\"start\":68057},{\"end\":68066,\"start\":68065},{\"end\":68346,\"start\":68345},{\"end\":68353,\"start\":68352},{\"end\":68543,\"start\":68542},{\"end\":68554,\"start\":68553}]", "bib_author_last_name": "[{\"end\":47175,\"start\":47170},{\"end\":47187,\"start\":47181},{\"end\":47521,\"start\":47513},{\"end\":47528,\"start\":47525},{\"end\":47540,\"start\":47532},{\"end\":47552,\"start\":47544},{\"end\":47989,\"start\":47980},{\"end\":48003,\"start\":47995},{\"end\":48014,\"start\":48009},{\"end\":48344,\"start\":48338},{\"end\":48655,\"start\":48645},{\"end\":48664,\"start\":48659},{\"end\":48677,\"start\":48668},{\"end\":48690,\"start\":48683},{\"end\":48698,\"start\":48694},{\"end\":48717,\"start\":48702},{\"end\":49056,\"start\":49050},{\"end\":49065,\"start\":49060},{\"end\":49073,\"start\":49069},{\"end\":49079,\"start\":49077},{\"end\":49092,\"start\":49083},{\"end\":49442,\"start\":49437},{\"end\":49448,\"start\":49446},{\"end\":49469,\"start\":49454},{\"end\":49810,\"start\":49807},{\"end\":49822,\"start\":49814},{\"end\":49834,\"start\":49826},{\"end\":49846,\"start\":49838},{\"end\":50177,\"start\":50174},{\"end\":50189,\"start\":50181},{\"end\":50201,\"start\":50193},{\"end\":50531,\"start\":50521},{\"end\":50544,\"start\":50535},{\"end\":50555,\"start\":50550},{\"end\":50909,\"start\":50899},{\"end\":50930,\"start\":50915},{\"end\":50941,\"start\":50936},{\"end\":51284,\"start\":51280},{\"end\":51295,\"start\":51290},{\"end\":51611,\"start\":51607},{\"end\":51622,\"start\":51617},{\"end\":51963,\"start\":51957},{\"end\":51974,\"start\":51967},{\"end\":51982,\"start\":51978},{\"end\":51991,\"start\":51986},{\"end\":52001,\"start\":51995},{\"end\":52013,\"start\":52005},{\"end\":52358,\"start\":52351},{\"end\":52639,\"start\":52632},{\"end\":52652,\"start\":52643},{\"end\":52665,\"start\":52656},{\"end\":52679,\"start\":52669},{\"end\":52689,\"start\":52683},{\"end\":52700,\"start\":52693},{\"end\":53132,\"start\":53125},{\"end\":53144,\"start\":53136},{\"end\":53156,\"start\":53150},{\"end\":53167,\"start\":53160},{\"end\":53512,\"start\":53505},{\"end\":53522,\"start\":53516},{\"end\":53818,\"start\":53810},{\"end\":53825,\"start\":53822},{\"end\":53839,\"start\":53829},{\"end\":54175,\"start\":54167},{\"end\":54187,\"start\":54183},{\"end\":54201,\"start\":54191},{\"end\":54406,\"start\":54402},{\"end\":54415,\"start\":54410},{\"end\":54425,\"start\":54419},{\"end\":54762,\"start\":54753},{\"end\":54774,\"start\":54766},{\"end\":54786,\"start\":54778},{\"end\":55105,\"start\":55096},{\"end\":55122,\"start\":55109},{\"end\":55135,\"start\":55126},{\"end\":55151,\"start\":55139},{\"end\":55160,\"start\":55155},{\"end\":55171,\"start\":55166},{\"end\":55555,\"start\":55550},{\"end\":55565,\"start\":55559},{\"end\":55577,\"start\":55569},{\"end\":55590,\"start\":55581},{\"end\":55604,\"start\":55594},{\"end\":55614,\"start\":55608},{\"end\":55968,\"start\":55965},{\"end\":55976,\"start\":55972},{\"end\":55984,\"start\":55980},{\"end\":55993,\"start\":55988},{\"end\":56258,\"start\":56252},{\"end\":56269,\"start\":56262},{\"end\":56481,\"start\":56471},{\"end\":56496,\"start\":56485},{\"end\":56799,\"start\":56793},{\"end\":56810,\"start\":56803},{\"end\":57174,\"start\":57169},{\"end\":57185,\"start\":57178},{\"end\":57517,\"start\":57512},{\"end\":57933,\"start\":57927},{\"end\":57939,\"start\":57937},{\"end\":58158,\"start\":58148},{\"end\":58171,\"start\":58162},{\"end\":58183,\"start\":58177},{\"end\":58473,\"start\":58465},{\"end\":58484,\"start\":58479},{\"end\":58495,\"start\":58488},{\"end\":58504,\"start\":58499},{\"end\":58855,\"start\":58847},{\"end\":58866,\"start\":58859},{\"end\":58875,\"start\":58870},{\"end\":59442,\"start\":59437},{\"end\":59452,\"start\":59448},{\"end\":59778,\"start\":59773},{\"end\":59788,\"start\":59782},{\"end\":59798,\"start\":59792},{\"end\":59809,\"start\":59802},{\"end\":60052,\"start\":60049},{\"end\":60059,\"start\":60056},{\"end\":60067,\"start\":60063},{\"end\":60076,\"start\":60071},{\"end\":60269,\"start\":60262},{\"end\":60280,\"start\":60273},{\"end\":60292,\"start\":60286},{\"end\":60613,\"start\":60610},{\"end\":60619,\"start\":60617},{\"end\":61117,\"start\":61108},{\"end\":61440,\"start\":61435},{\"end\":61449,\"start\":61444},{\"end\":61464,\"start\":61453},{\"end\":61475,\"start\":61470},{\"end\":61484,\"start\":61479},{\"end\":61494,\"start\":61488},{\"end\":61899,\"start\":61891},{\"end\":61910,\"start\":61903},{\"end\":61920,\"start\":61914},{\"end\":61928,\"start\":61925},{\"end\":62230,\"start\":62221},{\"end\":62241,\"start\":62234},{\"end\":62251,\"start\":62245},{\"end\":62545,\"start\":62539},{\"end\":62554,\"start\":62549},{\"end\":62574,\"start\":62558},{\"end\":62586,\"start\":62578},{\"end\":62599,\"start\":62590},{\"end\":63056,\"start\":63050},{\"end\":63065,\"start\":63060},{\"end\":63075,\"start\":63069},{\"end\":63087,\"start\":63079},{\"end\":63422,\"start\":63407},{\"end\":63433,\"start\":63428},{\"end\":63446,\"start\":63437},{\"end\":63457,\"start\":63450},{\"end\":63467,\"start\":63463},{\"end\":63489,\"start\":63473},{\"end\":63853,\"start\":63847},{\"end\":63864,\"start\":63859},{\"end\":63877,\"start\":63868},{\"end\":63888,\"start\":63881},{\"end\":63900,\"start\":63894},{\"end\":63911,\"start\":63906},{\"end\":64312,\"start\":64305},{\"end\":64332,\"start\":64318},{\"end\":64345,\"start\":64336},{\"end\":64359,\"start\":64351},{\"end\":64372,\"start\":64365},{\"end\":64773,\"start\":64765},{\"end\":64786,\"start\":64779},{\"end\":65048,\"start\":65032},{\"end\":65315,\"start\":65305},{\"end\":65325,\"start\":65319},{\"end\":65339,\"start\":65329},{\"end\":65352,\"start\":65343},{\"end\":65369,\"start\":65356},{\"end\":65714,\"start\":65710},{\"end\":65720,\"start\":65718},{\"end\":66072,\"start\":66059},{\"end\":66083,\"start\":66076},{\"end\":66090,\"start\":66087},{\"end\":66102,\"start\":66096},{\"end\":66114,\"start\":66106},{\"end\":66126,\"start\":66118},{\"end\":66486,\"start\":66473},{\"end\":66501,\"start\":66490},{\"end\":66511,\"start\":66505},{\"end\":66519,\"start\":66515},{\"end\":66529,\"start\":66523},{\"end\":66541,\"start\":66533},{\"end\":66955,\"start\":66947},{\"end\":66964,\"start\":66959},{\"end\":66976,\"start\":66968},{\"end\":67343,\"start\":67338},{\"end\":67356,\"start\":67349},{\"end\":67366,\"start\":67362},{\"end\":67695,\"start\":67691},{\"end\":68033,\"start\":68030},{\"end\":68040,\"start\":68037},{\"end\":68048,\"start\":68044},{\"end\":68055,\"start\":68052},{\"end\":68063,\"start\":68059},{\"end\":68069,\"start\":68067},{\"end\":68350,\"start\":68347},{\"end\":68359,\"start\":68354},{\"end\":68551,\"start\":68544},{\"end\":68560,\"start\":68555}]", "bib_entry": "[{\"attributes\":{\"doi\":\"10.1097/AUD.0b013e31816453dc\",\"id\":\"b0\",\"matched_paper_id\":43037341},\"end\":47374,\"start\":47117},{\"attributes\":{\"doi\":\"10.1109/TNSRE.2016.2571900\",\"id\":\"b1\",\"matched_paper_id\":27835175},\"end\":47901,\"start\":47376},{\"attributes\":{\"doi\":\"10.1523/JNEUROSCI.0584-19.2019\",\"id\":\"b2\",\"matched_paper_id\":199380132},\"end\":48257,\"start\":47903},{\"attributes\":{\"doi\":\"10.1121/1.1907229\",\"id\":\"b3\",\"matched_paper_id\":16308483},\"end\":48537,\"start\":48259},{\"attributes\":{\"doi\":\"10.1038/s41598-019-47795-0\",\"id\":\"b4\",\"matched_paper_id\":91320213},\"end\":49009,\"start\":48539},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":8604637},\"end\":49356,\"start\":49011},{\"attributes\":{\"doi\":\"10.1088/1741-2552/ab0ab5\",\"id\":\"b6\",\"matched_paper_id\":73508614},\"end\":49703,\"start\":49358},{\"attributes\":{\"doi\":\"10.1088/1741-2560/13/5/056014\",\"id\":\"b7\",\"matched_paper_id\":25324045},\"end\":50123,\"start\":49705},{\"attributes\":{\"id\":\"b8\"},\"end\":50348,\"start\":50125},{\"attributes\":{\"doi\":\"10.5281/zenodo.3377911\",\"id\":\"b9\"},\"end\":50409,\"start\":50350},{\"attributes\":{\"doi\":\"10.1111/ejn.13790\",\"id\":\"b10\",\"matched_paper_id\":5275290},\"end\":50813,\"start\":50411},{\"attributes\":{\"doi\":\"10.1016/j.cub.2015.08.030\",\"id\":\"b11\",\"matched_paper_id\":15888974},\"end\":51182,\"start\":50815},{\"attributes\":{\"doi\":\"10.1152/jn.00297.2011\",\"id\":\"b12\",\"matched_paper_id\":682727},\"end\":51522,\"start\":51184},{\"attributes\":{\"doi\":\"10.3389/fnhum.2014.00311\",\"id\":\"b13\",\"matched_paper_id\":10229413},\"end\":51848,\"start\":51524},{\"attributes\":{\"doi\":\"10.1145/3197517.3201357\",\"id\":\"b14\"},\"end\":52321,\"start\":51850},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":17857099},\"end\":52517,\"start\":52323},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":3872341},\"end\":53009,\"start\":52519},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":53102062},\"end\":53454,\"start\":53011},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":53388625},\"end\":53723,\"start\":53456},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":5319132},\"end\":54084,\"start\":53725},{\"attributes\":{\"id\":\"b20\"},\"end\":54398,\"start\":54086},{\"attributes\":{\"id\":\"b21\"},\"end\":54627,\"start\":54400},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":202025584},\"end\":55092,\"start\":54629},{\"attributes\":{\"id\":\"b23\"},\"end\":55546,\"start\":55094},{\"attributes\":{\"id\":\"b24\"},\"end\":55893,\"start\":55548},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":2238772},\"end\":56200,\"start\":55895},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":6199399},\"end\":56443,\"start\":56202},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":1915014},\"end\":56668,\"start\":56445},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":6985854},\"end\":57071,\"start\":56670},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":5808102},\"end\":57445,\"start\":57073},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":121077660},\"end\":57877,\"start\":57447},{\"attributes\":{\"id\":\"b31\"},\"end\":58079,\"start\":57879},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":195908774},\"end\":58380,\"start\":58081},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":231786459},\"end\":58755,\"start\":58382},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":221386705},\"end\":59331,\"start\":58757},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":6699749},\"end\":59712,\"start\":59333},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":14542261},\"end\":60004,\"start\":59714},{\"attributes\":{\"id\":\"b37\"},\"end\":60258,\"start\":60006},{\"attributes\":{\"id\":\"b38\"},\"end\":60528,\"start\":60260},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":11169209},\"end\":61015,\"start\":60530},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":4320045},\"end\":61337,\"start\":61017},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":14051440},\"end\":61777,\"start\":61339},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":22076362},\"end\":62217,\"start\":61779},{\"attributes\":{\"id\":\"b43\"},\"end\":62472,\"start\":62219},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":211296708},\"end\":63046,\"start\":62474},{\"attributes\":{\"id\":\"b45\"},\"end\":63309,\"start\":63048},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":2934551},\"end\":63791,\"start\":63311},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":1300841},\"end\":64169,\"start\":63793},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":56531160},\"end\":64720,\"start\":64171},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":18375389},\"end\":64982,\"start\":64722},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":11662344},\"end\":65234,\"start\":64984},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":6844431},\"end\":65585,\"start\":65236},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":218634596},\"end\":65955,\"start\":65587},{\"attributes\":{\"id\":\"b53\"},\"end\":66394,\"start\":65957},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":73422002},\"end\":66825,\"start\":66396},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":1735366},\"end\":67245,\"start\":66827},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":2057175},\"end\":67621,\"start\":67247},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":206603045},\"end\":67967,\"start\":67623},{\"attributes\":{\"id\":\"b58\"},\"end\":68259,\"start\":67969},{\"attributes\":{\"id\":\"b59\"},\"end\":68540,\"start\":68261},{\"attributes\":{\"id\":\"b60\"},\"end\":68759,\"start\":68542}]", "bib_title": "[{\"end\":47164,\"start\":47117},{\"end\":47509,\"start\":47376},{\"end\":47974,\"start\":47903},{\"end\":48332,\"start\":48259},{\"end\":48641,\"start\":48539},{\"end\":49046,\"start\":49011},{\"end\":49433,\"start\":49358},{\"end\":49803,\"start\":49705},{\"end\":50517,\"start\":50411},{\"end\":50893,\"start\":50815},{\"end\":51276,\"start\":51184},{\"end\":51603,\"start\":51524},{\"end\":52347,\"start\":52323},{\"end\":52628,\"start\":52519},{\"end\":53121,\"start\":53011},{\"end\":53501,\"start\":53456},{\"end\":53804,\"start\":53725},{\"end\":54749,\"start\":54629},{\"end\":55961,\"start\":55895},{\"end\":56248,\"start\":56202},{\"end\":56467,\"start\":56445},{\"end\":56787,\"start\":56670},{\"end\":57165,\"start\":57073},{\"end\":57508,\"start\":57447},{\"end\":58144,\"start\":58081},{\"end\":58461,\"start\":58382},{\"end\":58843,\"start\":58757},{\"end\":59431,\"start\":59333},{\"end\":59769,\"start\":59714},{\"end\":60603,\"start\":60530},{\"end\":61104,\"start\":61017},{\"end\":61431,\"start\":61339},{\"end\":61887,\"start\":61779},{\"end\":62533,\"start\":62474},{\"end\":63401,\"start\":63311},{\"end\":63841,\"start\":63793},{\"end\":64299,\"start\":64171},{\"end\":64761,\"start\":64722},{\"end\":65026,\"start\":64984},{\"end\":65301,\"start\":65236},{\"end\":65706,\"start\":65587},{\"end\":66469,\"start\":66396},{\"end\":66943,\"start\":66827},{\"end\":67332,\"start\":67247},{\"end\":67687,\"start\":67623}]", "bib_author": "[{\"end\":47177,\"start\":47166},{\"end\":47189,\"start\":47177},{\"end\":47523,\"start\":47511},{\"end\":47530,\"start\":47523},{\"end\":47542,\"start\":47530},{\"end\":47554,\"start\":47542},{\"end\":47991,\"start\":47976},{\"end\":48005,\"start\":47991},{\"end\":48016,\"start\":48005},{\"end\":48346,\"start\":48334},{\"end\":48657,\"start\":48643},{\"end\":48666,\"start\":48657},{\"end\":48679,\"start\":48666},{\"end\":48692,\"start\":48679},{\"end\":48700,\"start\":48692},{\"end\":48719,\"start\":48700},{\"end\":49058,\"start\":49048},{\"end\":49067,\"start\":49058},{\"end\":49075,\"start\":49067},{\"end\":49081,\"start\":49075},{\"end\":49094,\"start\":49081},{\"end\":49103,\"start\":49094},{\"end\":49107,\"start\":49103},{\"end\":49444,\"start\":49435},{\"end\":49450,\"start\":49444},{\"end\":49471,\"start\":49450},{\"end\":49812,\"start\":49805},{\"end\":49824,\"start\":49812},{\"end\":49836,\"start\":49824},{\"end\":49848,\"start\":49836},{\"end\":50179,\"start\":50172},{\"end\":50191,\"start\":50179},{\"end\":50203,\"start\":50191},{\"end\":50533,\"start\":50519},{\"end\":50546,\"start\":50533},{\"end\":50557,\"start\":50546},{\"end\":50911,\"start\":50895},{\"end\":50932,\"start\":50911},{\"end\":50943,\"start\":50932},{\"end\":51286,\"start\":51278},{\"end\":51297,\"start\":51286},{\"end\":51613,\"start\":51605},{\"end\":51624,\"start\":51613},{\"end\":51965,\"start\":51955},{\"end\":51976,\"start\":51965},{\"end\":51984,\"start\":51976},{\"end\":51993,\"start\":51984},{\"end\":52003,\"start\":51993},{\"end\":52015,\"start\":52003},{\"end\":52360,\"start\":52349},{\"end\":52641,\"start\":52630},{\"end\":52654,\"start\":52641},{\"end\":52667,\"start\":52654},{\"end\":52681,\"start\":52667},{\"end\":52691,\"start\":52681},{\"end\":52702,\"start\":52691},{\"end\":53134,\"start\":53123},{\"end\":53146,\"start\":53134},{\"end\":53158,\"start\":53146},{\"end\":53169,\"start\":53158},{\"end\":53514,\"start\":53503},{\"end\":53524,\"start\":53514},{\"end\":53820,\"start\":53806},{\"end\":53827,\"start\":53820},{\"end\":53841,\"start\":53827},{\"end\":54177,\"start\":54163},{\"end\":54189,\"start\":54177},{\"end\":54203,\"start\":54189},{\"end\":54408,\"start\":54400},{\"end\":54417,\"start\":54408},{\"end\":54427,\"start\":54417},{\"end\":54764,\"start\":54751},{\"end\":54776,\"start\":54764},{\"end\":54788,\"start\":54776},{\"end\":55107,\"start\":55094},{\"end\":55124,\"start\":55107},{\"end\":55137,\"start\":55124},{\"end\":55153,\"start\":55137},{\"end\":55162,\"start\":55153},{\"end\":55173,\"start\":55162},{\"end\":55557,\"start\":55548},{\"end\":55567,\"start\":55557},{\"end\":55579,\"start\":55567},{\"end\":55592,\"start\":55579},{\"end\":55606,\"start\":55592},{\"end\":55616,\"start\":55606},{\"end\":55970,\"start\":55963},{\"end\":55978,\"start\":55970},{\"end\":55986,\"start\":55978},{\"end\":55995,\"start\":55986},{\"end\":56260,\"start\":56250},{\"end\":56271,\"start\":56260},{\"end\":56483,\"start\":56469},{\"end\":56498,\"start\":56483},{\"end\":56801,\"start\":56789},{\"end\":56812,\"start\":56801},{\"end\":57176,\"start\":57167},{\"end\":57187,\"start\":57176},{\"end\":57519,\"start\":57510},{\"end\":57935,\"start\":57923},{\"end\":57941,\"start\":57935},{\"end\":58160,\"start\":58146},{\"end\":58173,\"start\":58160},{\"end\":58185,\"start\":58173},{\"end\":58475,\"start\":58463},{\"end\":58486,\"start\":58475},{\"end\":58497,\"start\":58486},{\"end\":58506,\"start\":58497},{\"end\":58857,\"start\":58845},{\"end\":58868,\"start\":58857},{\"end\":58877,\"start\":58868},{\"end\":59444,\"start\":59433},{\"end\":59454,\"start\":59444},{\"end\":59780,\"start\":59771},{\"end\":59790,\"start\":59780},{\"end\":59800,\"start\":59790},{\"end\":59811,\"start\":59800},{\"end\":60054,\"start\":60047},{\"end\":60061,\"start\":60054},{\"end\":60069,\"start\":60061},{\"end\":60078,\"start\":60069},{\"end\":60088,\"start\":60078},{\"end\":60092,\"start\":60088},{\"end\":60271,\"start\":60260},{\"end\":60282,\"start\":60271},{\"end\":60294,\"start\":60282},{\"end\":60615,\"start\":60605},{\"end\":60621,\"start\":60615},{\"end\":60627,\"start\":60621},{\"end\":60631,\"start\":60627},{\"end\":61119,\"start\":61106},{\"end\":61127,\"start\":61119},{\"end\":61133,\"start\":61127},{\"end\":61442,\"start\":61433},{\"end\":61451,\"start\":61442},{\"end\":61466,\"start\":61451},{\"end\":61477,\"start\":61466},{\"end\":61486,\"start\":61477},{\"end\":61496,\"start\":61486},{\"end\":61901,\"start\":61889},{\"end\":61912,\"start\":61901},{\"end\":61922,\"start\":61912},{\"end\":61930,\"start\":61922},{\"end\":61934,\"start\":61930},{\"end\":62232,\"start\":62219},{\"end\":62243,\"start\":62232},{\"end\":62253,\"start\":62243},{\"end\":62547,\"start\":62535},{\"end\":62556,\"start\":62547},{\"end\":62576,\"start\":62556},{\"end\":62588,\"start\":62576},{\"end\":62601,\"start\":62588},{\"end\":63058,\"start\":63048},{\"end\":63067,\"start\":63058},{\"end\":63077,\"start\":63067},{\"end\":63089,\"start\":63077},{\"end\":63424,\"start\":63403},{\"end\":63435,\"start\":63424},{\"end\":63448,\"start\":63435},{\"end\":63459,\"start\":63448},{\"end\":63469,\"start\":63459},{\"end\":63491,\"start\":63469},{\"end\":63855,\"start\":63843},{\"end\":63866,\"start\":63855},{\"end\":63879,\"start\":63866},{\"end\":63890,\"start\":63879},{\"end\":63902,\"start\":63890},{\"end\":63913,\"start\":63902},{\"end\":64314,\"start\":64301},{\"end\":64334,\"start\":64314},{\"end\":64347,\"start\":64334},{\"end\":64361,\"start\":64347},{\"end\":64374,\"start\":64361},{\"end\":64775,\"start\":64763},{\"end\":64788,\"start\":64775},{\"end\":65050,\"start\":65028},{\"end\":65317,\"start\":65303},{\"end\":65327,\"start\":65317},{\"end\":65341,\"start\":65327},{\"end\":65354,\"start\":65341},{\"end\":65371,\"start\":65354},{\"end\":65716,\"start\":65708},{\"end\":65722,\"start\":65716},{\"end\":66074,\"start\":66057},{\"end\":66085,\"start\":66074},{\"end\":66092,\"start\":66085},{\"end\":66104,\"start\":66092},{\"end\":66116,\"start\":66104},{\"end\":66128,\"start\":66116},{\"end\":66488,\"start\":66471},{\"end\":66503,\"start\":66488},{\"end\":66513,\"start\":66503},{\"end\":66521,\"start\":66513},{\"end\":66531,\"start\":66521},{\"end\":66543,\"start\":66531},{\"end\":66957,\"start\":66945},{\"end\":66966,\"start\":66957},{\"end\":66978,\"start\":66966},{\"end\":67345,\"start\":67334},{\"end\":67358,\"start\":67345},{\"end\":67368,\"start\":67358},{\"end\":67697,\"start\":67689},{\"end\":67704,\"start\":67697},{\"end\":67708,\"start\":67704},{\"end\":68035,\"start\":68028},{\"end\":68042,\"start\":68035},{\"end\":68050,\"start\":68042},{\"end\":68057,\"start\":68050},{\"end\":68065,\"start\":68057},{\"end\":68071,\"start\":68065},{\"end\":68352,\"start\":68345},{\"end\":68361,\"start\":68352},{\"end\":68553,\"start\":68542},{\"end\":68562,\"start\":68553}]", "bib_venue": "[{\"end\":49164,\"start\":49153},{\"end\":57242,\"start\":57237},{\"end\":57623,\"start\":57582},{\"end\":59023,\"start\":59011},{\"end\":60779,\"start\":60721},{\"end\":62733,\"start\":62724},{\"end\":68603,\"start\":68597},{\"end\":47225,\"start\":47217},{\"end\":47617,\"start\":47580},{\"end\":48057,\"start\":48046},{\"end\":48381,\"start\":48363},{\"end\":48753,\"start\":48745},{\"end\":49151,\"start\":49107},{\"end\":49508,\"start\":49495},{\"end\":49890,\"start\":49877},{\"end\":50170,\"start\":50125},{\"end\":50590,\"start\":50574},{\"end\":50978,\"start\":50968},{\"end\":51333,\"start\":51318},{\"end\":51668,\"start\":51648},{\"end\":51953,\"start\":51850},{\"end\":52405,\"start\":52389},{\"end\":52739,\"start\":52726},{\"end\":53211,\"start\":53201},{\"end\":53565,\"start\":53540},{\"end\":53883,\"start\":53873},{\"end\":54161,\"start\":54086},{\"end\":54488,\"start\":54443},{\"end\":54839,\"start\":54802},{\"end\":55286,\"start\":55213},{\"end\":55690,\"start\":55632},{\"end\":56028,\"start\":55995},{\"end\":56304,\"start\":56286},{\"end\":56537,\"start\":56524},{\"end\":56848,\"start\":56833},{\"end\":57235,\"start\":57191},{\"end\":57580,\"start\":57536},{\"end\":57921,\"start\":57879},{\"end\":58211,\"start\":58200},{\"end\":58554,\"start\":58531},{\"end\":59009,\"start\":58907},{\"end\":59502,\"start\":59486},{\"end\":59837,\"start\":59827},{\"end\":60045,\"start\":60006},{\"end\":60368,\"start\":60310},{\"end\":60719,\"start\":60652},{\"end\":61158,\"start\":61152},{\"end\":61535,\"start\":61520},{\"end\":61976,\"start\":61963},{\"end\":62320,\"start\":62269},{\"end\":62722,\"start\":62633},{\"end\":63152,\"start\":63105},{\"end\":63525,\"start\":63512},{\"end\":63950,\"start\":63941},{\"end\":64409,\"start\":64398},{\"end\":64831,\"start\":64805},{\"end\":65092,\"start\":65076},{\"end\":65390,\"start\":65371},{\"end\":65759,\"start\":65746},{\"end\":66055,\"start\":65957},{\"end\":66584,\"start\":66573},{\"end\":67015,\"start\":67006},{\"end\":67411,\"start\":67394},{\"end\":67776,\"start\":67734},{\"end\":68026,\"start\":67969},{\"end\":68343,\"start\":68261},{\"end\":68595,\"start\":68562}]"}}}, "year": 2023, "month": 12, "day": 17}