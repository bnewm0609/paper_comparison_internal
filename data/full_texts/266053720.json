{"id": 266053720, "updated": "2023-12-11 02:56:02.952", "metadata": {"title": "Cocktail: A Multidimensional Optimization for Model Serving in Cloud", "authors": "[{\"first\":\"Jashwant\",\"last\":\"Gunasekaran\",\"middle\":[\"Raj\"]},{\"first\":\"Cyan\",\"last\":\"Mishra\",\"middle\":[\"Subhra\"]},{\"first\":\"Prashanth\",\"last\":\"Thinakaran\",\"middle\":[]},{\"first\":\"Bikash\",\"last\":\"Sharma\",\"middle\":[]},{\"first\":\"Mahmut\",\"last\":\"Kandemir\",\"middle\":[\"T.\"]},{\"first\":\"Chita\",\"last\":\"Das\",\"middle\":[\"R.\"]}]", "venue": "NSDI", "journal": "1041-1057", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "With a growing demand for adopting ML models for a variety of application services, it is vital that the frameworks serving these models are capable of delivering highly accurate predictions with minimal latency along with reduced deployment costs in a public cloud environment. Despite high latency, prior works in this domain are crucially limited by the accuracy offered by individual models. Intuitively, model ensembling can address the accuracy gap by intelligently combining different models in parallel. However, selecting the appropriate models dynamically at runtime to meet the desired accuracy with low latency at minimal deployment cost is a nontrivial problem. Towards this, we propose Cocktail , a cost effective ensembling-based model serving framework. Cocktail comprises of two key components: (i) a dynamic model selection framework, which reduces the number of models in the ensemble, while satisfying the accuracy and latency requirements; (ii) an adaptive resource management (RM) framework that employs a distributed proactive autoscaling policy, to ef\ufb01ciently allocate resources for the models. The RM framework leverages transient virtual machine (VM) instances to reduce the deployment cost in a public cloud. A prototype implementation of Cocktail on the AWS EC2 platform and exhaustive evaluations using a variety of workloads demonstrate that Cocktail can reduce deployment cost by 1.45 \u00d7 , while providing 2 \u00d7 reduction in latency and satisfying the target accuracy for up to 96% of the requests, when compared to state-of-the-art model-serving frameworks.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nsdi/GunasekaranMTSK22", "doi": null}}, "content": {"source": {"pdf_hash": "fae036956f76122ce35aec3a1672e9f58637c453", "pdf_src": "ScienceParsePlus", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "8e233ba9272e7ee83bb68520f9fd74d0a71ab787", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/fae036956f76122ce35aec3a1672e9f58637c453.txt", "contents": "\nCocktail: A Multidimensional Optimization for Model Serving in Cloud\n\n\nJashwant Raj Gunasekaran \nThe Pennsylvania State University\nUniversity ParkPA\n\nCyan Subhra Mishra \nThe Pennsylvania State University\nUniversity ParkPA\n\nPrashanth Thinakaran \nThe Pennsylvania State University\nUniversity ParkPA\n\nMahmutBikash Sharma \nThe Pennsylvania State University\nUniversity ParkPA\n\nTaylan Kandemir \nThe Pennsylvania State University\nUniversity ParkPA\n\nChita R Das \nThe Pennsylvania State University\nUniversity ParkPA\n\nCocktail: A Multidimensional Optimization for Model Serving in Cloud\n\nWith a growing demand for adopting ML models for a variety of application services, it is vital that the frameworks serving these models are capable of delivering highly accurate predictions with minimal latency along with reduced deployment costs in a public cloud environment. Despite high latency, prior works in this domain are crucially limited by the accuracy offered by individual models. Intuitively, model ensembling can address the accuracy gap by intelligently combining different models in parallel. However, selecting the appropriate models dynamically at runtime to meet the desired accuracy with low latency at minimal deployment cost is a nontrivial problem. Towards this, we propose Cocktail, a cost effective ensembling-based model serving framework. Cocktail comprises of two key components: (i) a dynamic model selection framework, which reduces the number of models in the ensemble, while satisfying the accuracy and latency requirements; (ii) an adaptive resource management (RM) framework that employs a distributed proactive autoscaling policy, to efficiently allocate resources for the models. The RM framework leverages transient virtual machine (VM) instances to reduce the deployment cost in a public cloud. A prototype implementation of Cocktail on the AWS EC2 platform and exhaustive evaluations using a variety of workloads demonstrate that Cocktail can reduce deployment cost by 1.45\u00d7, while providing 2\u00d7 reduction in latency and satisfying the target accuracy for up to 96% of the requests, when compared to state-of-the-art model-serving frameworks.\n\nIntroduction\n\nMachine Learning (ML) has revolutionized user experience in various cloud-based application domains such as product recommendations [70], personalized advertisements [44], and computer vision [13,43]. For instance, Facebook [44,82] serves trillions of inference requests for user-interactive applications like ranking new-feeds, classifying photos, etc. It is imperative for these applications to deliver accurate predictions at sub-millisecond latencies [27,34,35,39,44,83] as they critically impact the user experience. This trend is expected to perpetuate as a number of applications adopt a variety of ML models to augment their services. These ML models are typically trained and hosted on cloud platforms as service endpoints, also known as model-serving framework [6,28,60]. From the myriad of ML flavours, Deep Neural Networks (DNNs) [54] due to their multi-faceted nature, and highly generalized and accurate learning patterns [45,73] are dominating the landscape by making these model-serving frameworks accessible to developers. However, their high variance due to the fluctuations in training data along with compute and memory intensiveness [59,65,84] has been a major impediment in designing models with high accuracy and low latency. Prior model-serving frameworks like InFaas [83] are confined by the accuracy and latency offered by such individual models. Unlike single-model inferences, more sophisticated techniques like ensemble learning [15] have been instrumental in allowing model-serving to further improve accuracy with multiple models. For example, by using the ensembling 1 technique, images can be classified using multiple models in parallel and results can be combined to give a final prediction. This significantly boosts accuracy compared to single-models, and for this obvious advantage, frameworks like Clipper [27] leverage ensembling techniques. Nevertheless, with ensembling, the very high resource footprint due to sheer number of models that need to be run for each request [27,56], exacerbates the public cloud deployment costs, as well as leads to high variation in latencies. Since cost plays a crucial role in application-provider consideration, it is quintessential to minimize the deployment costs, while maximizing accuracy with low latency. Hence, the non-trivial challenge here lies in making the cost of ensembling predictions analogous to single model predictions, while satisfying these requirements.\n\nStudying the state-of-the-art ensemble model-serving frameworks, we observe the following critical shortcomings:\n\n\u2022 Ensemble model selection policies used in frameworks like Clipper [27] are static, as they ensemble all available models and focus solely on minimizing loss in accuracy. This leads to higher latencies and further inflates the resource footprint, thereby accentuating the deployment costs.\n\n\u2022 Existing ensemble weight estimation [87] has high computational complexity and in practice is limited to a small set of off-the-shelf models. This leads to significant loss in accuracy. Besides, employing linear ensembling techniques such as model averaging are compute intensive [80] and not scalable for a large number of available models.\n\n\u2022 Ensemble systems [27,80] are not focused towards model deployment in a public cloud infrastructure, where resource selection and procurement play a pivotal role in minimizing the latency and deployment costs. Further, the resource provisioning strategies employed in single model-serving systems are not directly extendable to ensemble systems.\n\nThese shortcomings collectively motivate the central premise of this work: how to solve the complex optimization problem of cost, accuracy and latency for an ensembling framework? In this paper, we present and evaluate Cocktail 2 , which to our knowledge is the first work that proposes a cost-effective model-serving system by exploiting ensembling techniques for classification-based inference, to deliver high accuracy and low latency predictions. Cocktail adopts a three-pronged approach to solve the optimization problem. First, it uses a dynamic model selection policy to significantly reduce the number of models used in an ensemble, while meeting the latency and accuracy requirements. Second, it utilizes distributed autoscaling policies to reduce the latency variability and resource consumption of hosting ensemble models. Third, it minimizes the cost of deploying ensembles in a public cloud by taking advantage of transient VMs, as they can be 70-90% cheaper [3] than traditional VMs. Cocktail, by coalescing these benefits, is capable of operating in a region of optimal cost, accuracy and latency (shown in Figure 1) that prior works cannot achieve. Towards this, the key contributions of the paper are summarized below:\n\n1. By characterizing accuracy vs. latency of ensemble models, we identify that prudently selecting a subset of available models under a given latency can achieve the target accuracy. We leverage this in Cocktail, to design a novel dynamic model selection policy, which ensures accuracy with significantly reduced number of models. 2. Focusing on classification-based inferences, it is important to minimize the bias in predictions resulting from multiple models. In Cocktail, we employ a per-class weighted majority voting policy, that makes it scalable and effectively breaks ties when compared to traditional weighted averaging, thereby minimizing the accuracy loss. 3. We show that uniformly scaling resources for all models in the ensemble leads to over-provisioning of resources and towards minimizing it, we build a distributed weighted auto-scaling policy that utilizes the importance sampling technique to proactively allocate resources to every model. Further, Cocktail leverages transient VMs as they are cheaper, to drastically minimize the cost for hosting modelserving infrastructure in a public cloud. 2 Cocktail is ascribed to having the perfect blend of models in an ensemble. 4. We implement a prototype of Cocktail using both CPU and GPU instances on AWS EC2 [5] platform and extensively evaluate it using different request-arrival traces.\n\n\nApplications\n\nOur results from exhaustive experimental analysis demonstrate that Cocktail can minimize deployment cost by 1.4\u00d7 while meeting the accuracy for up-to 96% of the requests and providing 2\u00d7 reduction in latency, when compared to state-of-the-art model serving systems. 5. We show that ensemble models are inherently faulttolerant over single models, since in the former, failure of a model would incur some accuracy loss without complete failure of the requests. It is observed from our failureresilience results that Cocktail can adapt to instance failures by limiting the accuracy loss within 0.6%.\n\n\nBackground and Motivation\n\nWe start by providing a brief overview of model-serving in public cloud and ensembling, followed by a detailed analysis of their performance to motivate the need for Cocktail. Figure 2 shows the overall architecture of a model-serving framework. There are diverse applications that are typically developed, trained and hosted as web services. These services allow end-users to submit queries via web server interface. Since these inference requests are often user-facing, it is imperative to administer them under a strict service level objective (SLO). We define SLO as the end-to-end response latency required by an application. Services like Ads and News Feed [39,44]   classes of images. These 11 models are a representative set to classify all images belonging to 1000 classes in Imagenet.\n\n\nModel Serving in Public Cloud\n\nDepending on the application type, the maximum ensemble size can vary from tens to hundreds of models. The entire model framework is typically hosted on resources like VMs or containers in public cloud. These resources are available in different types including CPU/GPU instances, burstables and transient instances. Transient instances [69] are similar to traditional VMs but can be revoked at any time by the cloud provider with an interruption notice. The provisioning latency, instance permanence and packing factor of these resources have a direct impact on the latency and cost of hosting model-serving. We explain instance \"packing factor\" and its relationship with latency in Section 2.3.2. In this paper, we focus on improving the accuracy and latency from the model selection perspective and consider instances types from a cost perspective. A majority of the model serving systems [6,83,86] in public cloud support individual model selection from available models. For instance, InFaas [83] can choose variants among a same model to maintain accuracy and latency requirements. However, denser models tend to have up to 6\u00d7 the size and twice the latency of smaller models to achieve increased accuracy of about 2-3%. Besides using dense models, ensembling [15] techniques have been used to achieve higher accuracy. Why Ensembling? An Ensemble is defined as a set of classifiers whose individual decisions combined in some way to classify new examples. This has proved to be more accurate than traditional single large models because it inherently reduces incorrect predictions due to variance and bias. The commonly used ensemble method in classification problems is bagging [33] that considers homogeneous weak learners, learns them independently from each other in parallel, and combines them following some kind of deterministic averaging process [18] or majority voting [49] process. For further details on ensemble models, we refer the reader to prior works [14,57,58,61,64,77,78,88].\n\n\nRelated Work\n\nEnsembling in practice: Ensembling is supported by commercial cloud providers like Azure ML-studio [11] and AWS Autogluon [31] to boost the accuracy compared to single models. Azure initially starts with 5 models and scales up to\n\n\nFeatures\n\nClipper [27] Rafiki [80] Infaas [83] MArk [  200 using a hill-climb policy [17] to meet the target accuracy. AWS combines about 6-12 models to give the best possible accuracy. Users also have the option to manually mention the ensemble size. Unlike them, Cocktail's model selection policy tries to right-size the ensemble for a given latency, while maximizing accuracy.\n\nModel-serving in Cloud: The most relevant prior works to Cocktail are InFaas [83] and Clipper [27], which have been extensively discussed and compared to in Section 6. Recently FrugalML [20] was proposed to cost-effectively choose from commercial MLaaS APIs. While striking a few similarities with Cocktail, it is practically limited to image-classification applications with very few classes and does not address resource provisioning challenges. Several works [37,38] like MArk [86] proposed SLO and cost aware resource procurement policies for model-serving. Although our heterogeneous instance procurement policy has some similarities with MArk, it is significantly different because we consider ensemble models. Rafiki [80] considers small model sets and scales up and down the ensemble size by trading off accuracy to match throughput demands. However, Cocktail's resource management is more adaptive to changing request loads and does not drop accuracy. Pretzel [52] and Inferline [26] are built on top of Clipper to optimize the prediction pipeline and cost due to load variations, respectively. Many prior works [2,25,35,63,74,75] have extensively tried to reduce model latency by reducing overheads due to shared resources and hardware interference. We believe that our proposed policies can be complementary and beneficial to these prior works to reduce the cost and resource footprint of ensembling. There are mainstream commercial systems which automate single model-serving like TF-Serving [60], SageMaker [6], AzureML [10], Deep-Studio [28] etc. Autoscaling in Public Cloud: There are several research works that optimize the resource provisioning cost in public cloud. These works are broadly categorized into: (i) multiplexing the different instance types (e.g., Spot, On-Demand) [12,23,34,41,42,68,79], (ii) proactive resource provisioning based on prediction policies [34,36,40,41,69,86].\n\nCocktail uses similar load prediction models and auto-scales VMs in a distributed fashion with respect to model ensembling. Swayam [34] is relatively similar to our work as it han-  dles container provisioning and load-balancing, specifically catered for single model inferences. Cocktail's autoscaling policy strikes parallels with Swayam's distributed autoscaling; however, we further incorporate novel importance sampling techniques to reduce over-provisioning for under-used models. Table 2 provides a comprehensive comparison of Cocktail with the most relevant works across key dimensions.\n\n\nPros and Cons of Model Ensembling\n\nIn this section, we quantitatively evaluate (i) how effective ensembles are in terms of accuracy and latency compared to single models, and (ii) the challenges in deploying ensemble frameworks in a cost-effective fashion on a public cloud. For relevance in comparison to prior work [27,83] we chose image inference as our ensemble workload. While ensembling is applicable in other classification workloads like product recommendations [24,53], text classification [71] etc, the observations drawn are generic and applicable to other applications.\n\n\nEnsembling Compared to Single Models\n\nTo analyze the accuracy offered by ensemble models, we conduct an experiment using 10000 images from ImageNet [29] test dataset, on a C5.xlarge [8] instances in AWS EC2 [5].\n\nFor a given baseline model, we combine all models whose latency is lower than that of the baseline, and call it fullensemble. We perform ensembling on the predictions using a simple majority voting policy. The latency numbers for the baseline models and the corresponding ensemble models along with the size of the ensemble are shown in Table 3. In majority voting, every model votes for a prediction for each input, and the final output prediction is the one that receives more than half of the votes. Figure 3a, shows the accuracy comparison of the baseline (single) and static ensemble (explained in Section 3) compared to the full-ensemble. It is evident that full-ensemble can achieve up to 1.65% better accuracy than single models. Besides accuracy again, ensembling can also achieve lower latency. The latency of the ensemble is calculated as the time between start and end of the longest running model.As shown in Table 3, in the case of NASLarge, the ensemble latency is 2\u00d7 lower (151ms) than the baseline latency (311ms). Even a 10ms reduction in latency is of significant importance to the providers [35]. We observe a similar trend of higher ensemble accuracy for other four baseline models with a latency reduction of up to 1.3\u00d7. Thus, depending on the model subset used in the ensemble, it achieves better accuracy than the baseline at lower latencies. Note that in our example model-set, the benefits of ensembling will diminish for lower  accuracies (< 75%) because single models can reach those accuracies. Hence, based on the user constraints, Cocktail chooses between ensemble and single models.\n\n\nEnsembling Overhead\n\nWhile ensembling can boost accuracy with low latency, their distinctive resource hungry nature drastically increases the deployment costs when compared to single models. This is because more VMs or containers have to be procured to match the resource demands. However, note that the \"Packing factor\" (P f ) for each model also impacts the deployment costs. P f in this context is defined as the number of inferences that can be executed concurrently in a single instance without violating the inference latency (on average). Table 1 provides the P f for 11 different models when executed on a C5.xlarge instance.\n\nThere is a linear relationship between P f and the instance size. It can be seen that smaller models (MNet, NASMob) can be packed 2-5\u00d7 more when compared to larger models (IRV2, NASLarge). Thus, the ensembles with models of higher P f have significantly lower cost.\n\nThe benefits of P f is contingent upon the models chosen by the model selection policy. Existing ensemble model selection policies used in systems like Clipper use all off-theshelf models and assign weights to them to calculate accuracy. However, they do not right-size the model selection to include models which primarily contribute to the majority voting. We compare the cost of hosting ensembles using both spot (ensemble-spot) and OD (ensemble-OD) instances with the single models hosted on OD (single-OD) instances. Ensemble-spot is explained further in the next section. We run the experiment over a period of 1 hour for 10 requests/second. The cost is calculated as the cost per hour of EC2 c5.xlarge instance use, billed by AWS [5]. We ensure all instances are fully utilized by packing multiple requests in accordance to the P f . As shown in Figure 3b, Ensemble-OD is always expensive than single-OD for the all the models. Therefore, it is important to ensemble an \"optimal\" number of less compute intensive models to reduce the cost.\n\n\nPrelude to Cocktail\n\nTo specifically address the cost of hosting an ensemblingbased model-serving framework in public clouds without sacrificing the accuracy, this section introduces an overview of the two primary design choices employed in Cocktail. How to reduce resource footprint? The first step towards making model ensembling cost effective is to minimize the number of models by pruning the ensemble, which reduces the overall resource footprint. In order to estimate the right number of models to participate in a given ensemble, we conduct an experiment where we chose top N 2 accurate models (static) from the full-ensemble of size N. From Figure 3a, it can be seen that the static policy has an accuracy loss of up to 1.45% when compared to full-ensemble, but is still better than single models. This implies that the models other than top N 2 yields a significant 1.45% accuracy improvement in the full-ensemble but they cannot be statically determined. Therefore, a full-ensemble model participation is not required for all the inputs because, every model is individually suited to classify certain classes of images when compared to other classes. Figure 4 shows the class-wise accuracy for three models on 5 distinct classes. It can be seen that for simpler classes like Slug, MNetV2 can achieve similar accuracy as the bigger models, while for difficult classes, like Cup and Quill, it experiences up to 3% loss in accuracy. Since the model participation for ensembling can vary based on the class of input images being classified, there is a scope to develop a dynamic model selection policy that can leverage this class-wise variability to intelligently determine the number of models required for a given input. Key Takeaway: Full ensemble model-selection is an overkill, while static-ensemble leads to accuracy loss. This calls for a dynamic model selection policy which can accurately determine the number of models required, contingent upon the accuracy and scalability of the model selection policy. How to save cost? Although dynamic model selection policies can significantly reduce the resource footprint as shown in Figure 3b, the cost is still 20-30% higher when compared to a single model inference. Most cloud providers offer transient VMs such as Amazon Spot instances [69], Google Preemptible VMs [9], and Azure Low-priority VMs [7], that can reduce cloud computing costs by as much as 10\u00d7 [3]. In Cocktail, we leverage these transient VMs such as spot instances to drastically reduce the cost of deploying ensembling model framework. As an example, we host full-ensembling on AWS spot instances. Figure 3b shows that ensemble-spot can reduce the cost by up to 3.3\u00d7 when compared to ensemble-OD. For certain baselines like IRV2, ensemble-spot is also 1.5\u00d7 cheaper than single-OD. However, the crucial downside of using transient VMs is that they can be unilaterally preempted by the cloud provider at any given point due to reasons like increase in bid-price or provider-induced random interruptions. As we will discuss further, Cocktail is resilient to instance failures owing to the fault-tolerance of ensembling by computing multiple inferences for a single request. Key takeaway: The cost-effectiveness of transient instances, is naturally suitable for hosting ensemble models.  \n\n\nOverall Design of Cocktail\n\nMotivated by our observations, we design a novel modelserving framework, Cocktail, that can deliver high-accuracy and low-latency predictions at reduced cost. Figure 5 depicts the high-level design of Cocktail. Users submit requests to a master VM, which runs a model selection algorithm, 1a to decide the models to participate in the ensemble. The participating models are made available in a model cache 1b for faster access and avoid re-computation for requests having similar constraints. Then, individual queries are dispatched to instances pools 2 dedicated for each model. The results from the workers are ensembled using an weighted majority voting aggregator 3 to agree upon a correct prediction. To efficiently address the resource management and scalability challenges, Cocktail applies multiple strategies.\n\nFirst, it maintains dedicated instance pools to serve individual models which simplifies the management and load balancing overheads for every model. Next, the resource controller 4 handles instance procurement, by exploiting both CPU and GPU instances 4a in a cost-aware 4b fashion, while the load balancer 5 ensures all procured instances are binpacked by assigning queries to appropriate instances. We also design an autoscaler 6 , which utilizes a prediction policy 6a to forecast the request load and scale instances for every model pool, thereby minimizing over-provisioning of resources. The autoscaler further employs an importance sampling 6b algorithm to estimate the importance of each model pool by calculating percentage of request served by it in a given time interval. The key components of the design are explained in detail below.\n\n\nDynamic Model Selection Policy\n\nWe use a window-based dynamic model selection policy using two objective functions as described below. Objective functions: In order to reduce cost and latency while maximizing the accuracy, we define a latency-accuracy metric (\u00b5 AL ) and cost metric (\u00b5 c ):\n\u00b5 AL = Acc target Lat target \u00b5 C = k \u00d7 N \u2211 m=1 inst_cost P f m\nwhere N is the number of models used to ensemble and inst_cost is the VM cost. Each model m has a packing factor P f m and k is a constant which depends on the VM size in terms of vCPUs (xlarge, 2xlarge, etc). Our first objective function (O 1 ) is to the maximize \u00b5 AL such that target accuracy (Acc target ) is reached within the target latency (Lat target ). max \u00b5 AL : Acc target \u2265 Acc target \u00b1 Acc margin Lat target \u2264 Lat target \u00b1 Lat margin\n\nTo solve O 1 , we determine an initial model list by choosing the individual models satisfying Lat target and then create a probabilistic ensemble that satisfies the Acc target . Cocktail takes the accuracy of each model as a probability of correctness and then iteratively constructs a model list, where the joint probability of them performing the classification is within the accuracy target. We tolerate a 0.2% (Acc margin ) and 5ms (Lat margin ) variance in Acc target and Lat target , respectively. Next, we solve for the second objective function (O 2 ) by minimizing \u00b5 C , while maintaining the target accuracy.\n\nmin \u00b5 C : Acc target \u2265 Acc target \u00b1 Acc margin (O 2 ) is solved by resizing the model list of size N and further through intelligence resource procurement (described in section 4.2), and thus maximizing P f and minimizing k simultaneously. For N models, where each model has a minimum accuracy 'a', we model the ensemble as a coin-toss problem, where N biased coins (with probability of head being a) are tossed together, and we need to find the probability of majority of them being heads. For this, we need at least N 2 + 1 models to give the same results. The probability of correct prediction is given by\nN \u2211 i= N 2 +1 N i a i (1 \u2212 a) (N\u2212i)\nModel Selection Algorithm: To minimize \u00b5 C , we design a policy to downscale the number of models, if more than N/2+1 models vote for the same classification result. Algorithm 1 describes the overall design of the model selection policy 1a . For every monitoring interval, we keep track of the accuracy obtained from predicting all input images within the interval. If the accuracy of the interval reaches the threshold accuracy (target + error_margin), we scale down the number of available models in the ensemble. For consecutive sampling intervals, we calculate the Mode (most frequently occurring) of the majority vote received for every input. If the Mode is greater than needed votes N/2 + 1 we prune the models to N/2 + 1. While down-scaling, we drop the models with the least prediction accuracy in that interval. If there is a tie, we drop the model with least packing factor (P f ). It can so happen that dropping models can lead to drop in accuracy for certain intervals, because the class of images being predicted are different. In such cases, we up-size the models (one at a time) by adding most accurate model from the remaining unused models. The model selection policy described above ensures that we only use the necessary models in the majority voting. In order to increase the accuracy of majority voting, we design a weighted majority voting policy 3 . The weight matrix is designed by considering the accuracy of each model for each class, giving us a weight matrix of L \u00d7 N dimension, where L is the number of unique labels and N is the number of models used in the ensemble. The majority vote is calculated as a sum of model-weights for each unique class in the individual prediction of the ensemble. For instance, if there are 3 unique classes predicted by all the ensemble models, we sum the weights for all models of the same class. The class with the maximum weight (P class ) is the output of the majority vote. Hence, classes that did not get the highest votes can still be the final output if the models associated with that class has a higher weight, than the combined weights of highest voted class. Unlike commonly used voting policies which assign weights based on overall correct predictions, our policy incorporates class-wise information to the weights, thus making it more adaptable to different images classes.\n\nIn order to determine the weight of every class, we use a per-class dictionary that keeps track of the correct predictions of every model per class. We populate the dictionary at runtime to avoid any inherent bias that could result from varying images over time. Similarly, our model selection policy is also changed at runtime based on correct predictions seen during every interval. An important concern in majority voting is tie-breaking. Ties occur when two sets of equal number of models predict a different result. The effectiveness Algorithm 2 Predictive Weighted Instance Auto Scaling 1: procedure WEIGHTED_AUTOSCALING(Stages) 2:\n\nPredicted_load \u2190 DeepARN_Predict(load) 3:\n\nfor every Interval do 4:\n\nfor model in \u2200Models do 5:\n\nmodel weight \u2190 get_popularity(model) 6:\n\nWeight.append(model weight ) 7:\n\nend for 8: end for 9:\n\nif Predicted_load \u2265 Current_load then 10:\n\nfor model in \u2200Models do 11:\n\nI_n \u2190 (Predicted_load -Current_load) \u00d7 model weight 12:\n\nlaunch_workers(est_VMs) 13:\n\nmodel.workers.append(est_VMs) 14:\n\nend for 15:\n\nend if 16: end procedure of weighted voting in breaking ties is discussed in Section 6.\n\n\nResource Management\n\nBesides model selection, it is crucial to design an optimized resource provisioning and management scheme to host the models cost-effectively. We explain in detail the resource procurement and autoscaling policy employed in Cocktail.\n\n\nResource Controller\n\nResource controller determines the cost-effective combination of instances to be procured. We explain the details below. Resource Types: We use both CPU and GPU instances 4a depending on the request arrival load. GPU instances are cost-effective when packed with a large batch of requests for execution. Hence, inspired from prior work [27,86], we design an adaptive packing policy such that it takes into account the number of requests to schedule at time T and P f for every instance. The requests are sent to GPU instances only if the load matches the P f of the instance. Cost-aware Procurement: The cost of executing in a fully packed instance determines how expensive is each instance. Prior to scaling-up instances, we need to estimate the cost 4b of running them along with existing instances. At any given time T , based on the predicted load (L p ) and running instances R N , we use a cost-aware greedy policy to determine the number of additional instances required to serve as A n = L p \u2212C r , where C r = \u2211 N i=1 P f i , is the request load which can be handled with R N . To procure A n instances, we greedily calculate the least cost instance as min \u2200i\u2208instances Cost i \u00d7 A n /P f i . Depending on the cost-effectiveness ratio of A n /P f i , GPUs will be preferred over CPU instances. Load Balancer: Apart from procuring instances, it is quintessential to design a load balancing and bin-packing 5 strategy to fully utilize all the provisioned instances. We maintain a request queue at every model pool. In order to increase the utilization of all instances in a pool at any given time, the load balancer submits every request from the queue to the lease remaining free slots (viz. instance packing factor P f ). This is similar to an online bin-packing algorithm. We use an idle-timeout limit for 10 minutes to recycle unused instances from every model pool. Hence, greedily assigning requests enables early scale down of lightly loaded instances.\n\n\nAutoscaler\n\nAlong with resource procurement, we need to autoscale instances to satisfy the incoming query load. Though reactive policies (used in Clipper and InFaas) can be employed which take into account metrics like CPU utilization [83], these policies are slow to react when there is dynamism in request rates. Proactive policies with request prediction are know to have superior performance [86] and can co-exist with reactive policies. In Cocktail, we use a load prediction model that can accurately forecast the anticipated load for a given time interval. Using the predicted load 6a , Cocktail spawns additional instances, if necessary, for every instance pool. In addition, we sample SLO violations for every 10s interval and reactively spawn additional instances to every pool based on aggregate resource utilization of all instances. This captures SLO violations due to mis-predictions. Prediction Policy:\n\nTo effectively capture the  different load arrival patterns, we design a DeepARestimator (DeepARest) based prediction model. We zeroed in on the choice of using DeepARest by conducting (Table 4) an in-depth comparison of the accuracy loss when compared with other state-of-the-art traditional and ML-based prediction models used in prior works [47,86]. As shown in Algorithm 2, for every model under a periodic scheduling interval of 1 minute (T s ), we use the Predicted_load (L p ) at time T + T p and compare it with the current_load to determine the number of instances (I n ).T p is defined as the average launch time for new instances. (T s ) is set to 1 minute as it is the typical instance provisioning time for EC2 VMs. To calculate (L p ), we sample the arrival rate in adjacent windows of size W over the past S seconds. Using the global arrival rate from all windows, the model predicts (L p ) for T p time units from T . T p is set to 10 minutes because it is sufficient time to capture the variations in long-term future. All these parameters are tunable based on the system needs. Importance Sampling: An important concern in autoscaling is that the model selection policy dynamically determines the models in the ensemble for a given request constraints.\n\nAutoscaling the instances equally for every model based on predicted load, would inherently lead to over-provisioned instances for under-used models. To address this concern, we design a weighted autoscaling policy which intelligently auto-scales instances for every pool based on the weights. As shown in Algorithm 2, weights are determined by frequency in which a particular model is chosen for requests (get_popularity) with respect to other models in the ensemble.\n\nThe weights are multiplied with the predicted load to scale instances (launch_workers) for every model pool. We name this as an importance sampling 6b technique, because the model pools are scaled proportional to their popularity.\n\n\nImplementation and Evaluation\n\nWe implemented a prototype of Cocktail and deployed it on AWS EC2 [5] platform The details of the implementation are described below. Cocktail is open-sourced at https:// github.com/jashwantraj92/cocktail\n\n\nCocktail Prototype Implementation\n\nCocktail is implemented using 10KLOC of Python. We de-  chooses a set of single or ensemble models required to meet the developer specified constraints. Discussion: Our accuracy and latency constraints are limited to the measurements from the available pretrained models. Note that changing the models or/and framework would lead to minor deviations. While providing latency and top-1% accuracy of the pretrained models is an offline step in Cocktail, we can calculate these values through one-time profiling and use them in the framework. All decisions related to VM autoscaling, bin-packing and load-prediction are reliant on the centralized mongodb database, which can become a potential bottleneck in terms of scalability and consistency. This can be mitigated by using fast distributed solutions like Redis [16] and Zookeeper [46]. The DeepARest model is pre-trained using 60% of the arrival trace. For varying load patterns, the model parameters can be updated by re-training in the background with new arrival rates.\n\n\nEvaluation Methodology\n\nWe evaluate our prototype implementation on AWS EC2 [8] platforms. Specifically, we use C5.xlarge, 2xlarge, 4xlarge, 8xlarge for CPU instances and p2.xlarge for GPU instances. Load Generator: We use different traces which are given as input to the load generator. Firstly, we use real-world request arrival traces from Wikipedia [76], which exhibit typical characteristics of ML inference workloads as it has recurring diurnal patterns. The second trace is production twitter [48] trace which is bursty with unexpected load spikes. We use the first 1 hour sample of both the traces and they are scaled to have an average request rate of 50 req/sec. Workload: As shown in Table 5 we use image-classification and Sentiment Analysis (text) applications with two datasets each for our evaluation. Sentiment analysis outputs the sentiment of a given sentence as positive negative and (or) neutral. We use 9 different prominently used text-classification models from transformers library [81] (details available in appendix) designed using Google BERT [30] architecture trained on SST [72] and SemEval [66] dataset. Each request from the load-generator is modelled after a query with specific <latency,accuracy> constraints. The queries consist of images or sentences, which are randomly picked from the test dataset. In our experiments, we use five different types of these constraints.\n\nAs an example for the Imagenet dataset shown in Figure 6, each constraint is a representative of <latency, accuracy> combination offered by single models (shown in Table 1). We use one constraint (blue dots) each from five different regions (categorized by dotted lines) picked in the increasing order of accuracy. Each of these picked constraints (named const1 -const5 in the Figure) represents a single baseline model, whose corresponding ensemble size ranges from small (2) to large (10), as shown in Table 3. Note that the latency is the raw model execution latency, and does not include the additional network-transfer overheads incurred. We picked the constraints using a similar procedure by ordering constraints across five different categories for CIFAR-100, SST-2 and SemEval (twitter tweets) datasets. The list of models used for them are given in the Appendix. We model two different workload mixes by using a combination of these five query constraint types. Based on the decreasing order of accuracy, we categorize them into Strict and Relaxed workloads.\n\n\nEvaluation Metrics\n\nMost of our evaluations of Cocktail for image-classification are performed using the Imagenet dataset. To further demonstrate the sensitivity of Cocktail to dataset and applicability to other classification applications, we also evaluate it using CIFAR-100 and Sentiment-Analysis application. We use three important metrics: response latency, cost and accuracy for evaluating and comparing our design to other state-ofthe-art systems. The response latency metric includes model inference latency, communication/network latency and synchronization overheads. Queries that do not meet response latency requirements (>700ms) are considered as SLO violations. The cost metric is the billing cost from AWS, and the accuracy metric is measured as the percentage of requests that meet the target accuracy requirements. We compare these metrics for Cocktail against (i) In-Faas [83], which is our baseline that employs single model selection policy; (ii) Clipper [27], which uses static full model selection policy (analogous to AWS AutoGluon); and (iii) Clipper-X which is an enhancement to Clipper with a simple model selection (drop one model at a time) that does not utilize the mode-based policy enforced in Cocktail. Both InFaas and Clipper share Cocktail's implementation setup to ensure a fair comparison with respect to our design and execution environment. For instance, both Clipper and InFaas employ variants of a reactive autoscaler as described in Section 4.2.2. However, in our setup, both benefit from the distributed autoscaling and prediction policies, thus eliminating variability. Also note that InFaas is deployed using OnDemand instances, while both Clipper and Cocktail use spot instances.\n\n\nAnalysis of Results\n\nThis section discusses the experimental results of Cocktail using the Wiki and Twitter traces. To summarize the overall results, Cocktail providing 2\u00d7 reduction in latency, while meeting the accuracy for up-to 96% of the requests under reduced deployment cost by 1.4\u00d7, when compared to InFaaS and Clipper.\n\n\nLatency, Accuracy and Cost Reduction\n\nLatency Distribution: Figure 7 shows the distribution of total response latency in a standard box-and-whisker plot. The boundaries of the box-plots depict the 1st quartile (25th percentile (PCTL)) and 3rd quartile (75th PCTL), the whiskers plot the minimum and maximum (tail) latency and the middle line inside the box depict the median (50 PCTL). The total response latency includes additional 200-300ms incurred for query serialization and data transfer over network. It can be seen that the maximum latency of Cocktail is similar to the 75th PCTL latency of InFaas. This is because the single model inference have up to 2x higher latency to achieve higher accuracy. Consequently, this leads to 35% SLO violations for InFaas in the case of Strict workload. In contrast, both Cocktail and Clipper can reach the accuracy at lower latency due to ensembling, thus minimizing SLO violations to 1%.\n\nAlso, the tail latency is higher for Twitter trace (Figure 7c, 7d) owing to its bursty nature. Note that the tail latency of Clipper is still higher than Cocktail because Clipper ensembles more models than Cocktail, thereby resulting in straggler tasks in the VMs. The difference in latency between Cocktail and InFaas is lower for Relaxed workload when compared to Strict workload (20% lower in tail). Since the Relaxed workload has much lower accuracy constraints, smaller models are able to singularly achieve the accuracy requirements at lower latency.\n\n\nAccuracy violations:\n\nThe accuracy is measured as a moving window average with size 200 for all the requests in the workload.  Both Clipper and Cocktail can meet the accuracy for 56% of requests, which is 26% and 9% more than In-Faas and Clipper respectively. This is because, intuitively ensembling leads to higher accuracy than single models. However, Cocktail is still 9% better than Clipper because the class-based weighted voting, is efficient in breaking ties when compared to weighting averaging used in Clipper. Since majority voting can include ties in votes, we analyzed the number of ties, which were correctly predicted for all the queries. Cocktail was able to deliver correct predictions for 35% of the tied votes, whereas breaking the ties in Clipper led only to 20% correct predictions.     Note that, changing the target accuracy to tolerate a 0.5% loss, increases the percentage of requests that meet accuracy to 81% for Cocktail, when compared to 61% for InFaas. The requests meeting accuracy are generally higher for the Relaxed workload because the target accuracy is much lower. Overall, Cocktail was able to deliver an accuracy of 83% and 79.5% on average for the Strict and Relaxed workloads, respectively. This translates to 1.5% and 1% better accuracy than Clipper and InFaas. We do not plot the results for Clipper-X, which achieves similar accuracy to Cocktail, but uses more models as explained in Section 6.2.1.\n\nCost Comparison: Figure 8 plots the cost savings of Cocktail when compared to InFaas, Clipper and Clipper-X policies. It can be seen that, Cocktail is up to 1.45\u00d7 more cost effective than InFaas for Strict workload. In addition, Cocktail reduces cost by 1.35\u00d7 and 1.27\u00d7 compared to Clipper and Clipper-X policies, owing to its dynamic model selection policy, which minimizes the resource footprint of ensembling. On the other hand, Clipper uses all models in ensemble and the Clipper-X policy does not right size the models as aggressively as Clipper, hence they are more expensive. Note that, all the schemes incur higher cost for twitter trace (Figure 8b) compared to wiki trace (Figure 8a). This is because the twitter workload is bursty, thereby leading to intermittent over-provisioned VMs.\n\n\nKey Sources of Improvements\n\nThe major improvements in terms of cost, latency, and accu-racy in Cocktail are explained below. For brevity in explanation, the results are averaged across Wiki and Twitter traces for strict workload. Here, Cocktail reduces the number of models by up to 55% for all four query types. This is because our dynamic policy ensures that the number of models are well within N/2 most of the time, whereas the Clipper-X policy does not aggressively scale down models. Clipper, on the other hand, is static and always uses all the models. The percentage of model-reduction is lower for Const2, 3 and 4 because, the total models used in the ensemble is less than Const1 (8, 7 and 6 models, respectively). Still, the savings in terms of cost will be significant because even removing one model from the ensemble amounts to \u223c20% cost savings in the long run (Clipper vs Clipper-X ensemble in Figure 8). Thus, the benefits of Cocktail are substantial for large ensembles while reducing the number of models for medium-sized ensembles. Figure 9b shows the breakdown of the percentage of requests (Const1) served by the each model. As seen, Incep-tionResNetV2, Densenet-201, Densenet121, NasnetMobile and Xception are the top-5 most used models in the ensemble. Based on Table 1, if we had statically taken the top N/2 most accurate models, NasNetmobile would not have been included in the ensemble. However, based on the input images sent in each query, our model selection policy has been able to identify NasNetMobile to be a significantly contributing model in the ensemble. Further, the other 5 models are used by up to 25% of the images. Not including them in the ensemble would have led to severe loss in accuracy. But, our dynamic policy with the class-based weighted voting, adapts to input images in a given interval by accurately selecting the best performing model for each class. To further demonstrate the effectiveness of our dynamic model selection, Figure 10b,10c plots the number models in every sampling interval along with cumulative accuracy and window accuracy within each sampling interval for three schemes. We observe that Cocktail can effectively scale up and scale down the models while maintaining the cumulative accuracy well within the threshold. More than 50% of the time the number of models are maintained between 4 to 5, because the dynamic policy is quick in detecting accuracy failures and recovers immediately   by scaling up models. However, Clipper-X does not scale down models as frequently as Cocktail, while ensuring similar accuracy. Clipper is less accurate than Cocktail and further it uses all 10 models throughout. Figure 11 plots the reduction in the number of VMs used by all four schemes. It can be seen that both Cocktail and Clipper-X spawn 49% and 20% fewer VMs than Clipper for workload-1 on Twitter trace. Cocktail spawns 29% lesser VMs on top of Clipper-X, because it is not aggressive enough like Cocktail to downscale more models at every interval. It is to be noted that the savings are lower for Relaxed workload because, the number of models in the ensemble are inherently low, thus leading to reduced benefits from scaling down the models. Intuitively, InFaas has the least number of VMs spawned because it does not ensemble models. Cocktail spawns upto 50% more VMs than InFaas, but in turns reduces accuracy loss by up to 96%. To further capture the benefits of the weighted autoscaling policy, Figure 12a plots the number of VMs spawned over time for the top-3 most used models in the ensemble for Const1. The Bline denotes number of VMs that would be spawned without applying the weights. Not adopting an importance sampling based weighted policy would result in equivalent number of VMs as the Bline for all models. However, since Cocktail exploits importance sampling by keeping track of the frequency in which models are selected, the num-ber of VMs spawned for model1, model2 and model-3 is upto 3\u00d7 times lesser than uniform scaling. Figure 9b shows the most used models in decreasing order of importance. The autoscaling policy effectively utilizes this importance factor in regular intervals of 5 minutes. Despite using multiple models for a single inference, importance sampling combined with aggressive model pruning, greatly reduces the resource footprint which directly translates to the cost savings in Cocktail.\n\n\nBenefits from dynamic model selection\n\n\nBenefits from Autoscaling\n\n\nBenefits of Transient VMs\n\nThe cost-reductions in Cocktail are akin to cost-savings of transient VMs compared to On-Demand (OD) VMs. We profile the spot price of 4 types of C5 EC2 VMs over a 2-week period in August 2020. It was seen that, the spot instance prices have predictable fluctuations. When compared to the OD price , they were up to 70% cheaper. This price gap is capitalized in Cocktail to reduce the cost of instances consumed by ensembling. Note that, we set the bidding price conservatively to 40% of OD. Although, Cocktail spawns about 50% more VMs than InFaas, the high P f of small models and spot-instance price reductions combined with autoscaling policies lead to the overall 30-40% cost savings.\n\n\nSensitivity Analysis\n\nIn this section, we analyze the sensitivity of Cocktail with respect to various design choices which include (i) sampling interval of the accuracy measurements, (ii) spot-instance failure rate and (iii) type of datasets and applications.\n\n\nSampling Interval\n\nTo study the sensitivity with respect to the sampling interval for measure accuracy loss/gain, we use four different intervals of 10s, 30s, 60s and 120s. Figure 13 plots the average number of models (bar-left y-axis) and cumulative accuracy (lineright y-axis) for the different sampling intervals for queries with three different constraints. It can be seen that the 30s interval strikes the right balance with less than 0.2% loss in accuracy and has average number models much lesser than other intervals. This is because, increasing the interval leads to lower number of scale down operations, thus resulting in a bigger ensemble. As a result, the 120s interval has the highest number of models.    Figure 13: Sensitivity analysis of model selection with respect to sampling interval. The average number of models is in primary axis and cumulative accuracy in secondary axis.\n\n\nCocktail Failure Resilience\n\nWe use spot instances to host models in Cocktail. As previously discussed in Section 3, spot instances interruptions can lead to intermittent loss in accuracy as certain models will be unavailable in the ensemble. However for large ensembles (5 models are more), the intermittent accuracy loss is very low. Figure 12b plots the failure analysis results for top three constraints by comparing the ensemble accuracy to the target accuracy. The desired accuracy for all three constraints are plotted as BL1, BL2 and BL3. We induce failures in the instances using chaosmonkey [19] tool with a 20% failure probability. It can be seen that queries in all three constraints suffer an intermittent loss in accuracy of 0.6% between the time period 240s and 800s. Beyond 800s, they quickly recover back to the required accuracy because additional instances are spawned in place of failed instances. However, in the case of InFaas, this would lead to 1% failed requests due to requests being dropped from the failed instances.\n\nAn alternate solution would be to restart the queries in running instances but that leads to increased latencies for the 1% requests. In contrast, Cocktail incurs a modest accuracy loss of well within 0.6% and quickly adapts to reach the target accuracy. Thus, Cocktail is inherently fault-tolerant owing to the parallel nature in computing multiple inferences for a single request. We observe similar accuracy loss or lower for different probability failures of 5%, 10% and 25%, respectively (results/charts omitted in the interest of space). Discussion: For applications that are latency tolerant, we can potentially redirect requests from failed instances to existing instances, which would lead to increased tail latency. The results we how are only for latency intolerant applications. Note that, the ensembles used in our experiments are at-least 4 models or more. For smaller ensembles, instance failures might lead to higher accuracy loss, but in our experiments, single models typically satisfy their constraints. Figure 14 plots the sensitivity of model selection policy under a wide-range of latency and accuracy constraints. In Figure 14a, we vary the latency under six different constant accuracy categories. It can be seen that for fixed accuracy of 72%, 78% and 80%, the average number of models increase with increase in latency, but drops to 1 for the highest latency. Intuitively, singe large models with higher latency can satisfy    the accuracy, while short latency models need to be ensembled to reach the same accuracy. For accuracy greater than 80%, the ensemble size drops with higher latencies. This is because the models which offer higher accuracy are typically dense and hence, smaller ensembles are sufficient. In Figure 14b, we vary the accuracy under six different constant latency categories. It can be seen that for higher accuracies, Cocktail tries to ensemble more models to reach the accuracy, while for lower accuracy it resorts to using single models.\n\n\nSensitivity to Constraints\n\n\nSensitivity to Dataset\n\nTo demonstrate the applicability of Cocktail to multiple datasets, we conducted similar experiments as elucidated in Section 5.2.1 using the CIFAR-100 dataset [50]. It comprises of 100 distinct image classes and we trained 11 different models including the nine that are common from Table 1. Figure 15a plots the average number of models used by the three policies for the top four constraints. It can be seen that Cocktail shows similar reduction (as Imagenet) while using only 4.4 models on average. As expected, Clipper and Clipper-X use more models than Cocktail (11 and 5.4, respectively) due to non-aggressive scaling down of the models used. Figure 16a plots the latency reduction and accuracy boost when compared to InFaaS (baseline). While able to reduce 60% of the models used in the ensemble, Cocktail also reduces latency by up to 50% and boosts accuracy by up to 1.2%. Cocktail was also able to deliver modest accuracy gain   of 0.5% than Clipper (not plotted). The accuracy gain seen in CIFAR-100 is lesser than ImageNet dataset because the class-based weighted voting works effectively when handling large number of classes (100 in CIFAR vs 1000 in ImageNet). Nevertheless, Cocktail is able to deliver the accuracy at 2x lower latency than InFaaS and 1.35x lower cost than Clipper.\n\n\nGeneral Applicability of Cocktail\n\nTo demonstrate the general applicability of Cocktail to other classification tasks, we evaluated Cocktail using a Sentiment Analysis application for two datasets. The results reported are averaged across both the datasets. Figure 15b plots the average number of models used by the three policies for the top four constraints. As shown for Const1, Cocktail shows similar reduction (as image-classification) with only using 4.8 models on average, which is 40% and 26% lower than Clipper and Clipper-X, respectively. Cocktail is also able to reduce the number of models by 30% and 50% for medium ensembles (Const2 & Const3) as well. Figure 16b plots the latency reduction and accuracy gain, compared to InFaaS (baseline). While being able to reduce 50% of the models used in the ensemble, Cocktail also reduces latency by up to 50% and improves accuracy by up to 1.3%. Both Cocktail and Clipper deliver the same overall accuracy (96%, 94.5%, 93.5%, and 92%)). Since sentiment analysis only has 2-3 classes, there are no additional accuracy gains by using the class-based weighted voting. However, the model selection policy effectively switches between different models based on the structure of input text (equivalent to classes in images). For instance, complex sentences are more accurately classified by denser models compared to smaller. Despite the lower accuracy gains, Cocktail is able to reduce the cost (Figure 17) of model-serving by 1.45\u00d7 and 1.37\u00d7 for Wiki trace compared to InFaaS and Clipper, respectively.\n\n\nConcluding Remarks\n\nThere is an imminent need to develop model serving systems that can deliver highly accurate, low latency predictions at reduced cost. In this paper, we propose and evaluate Cocktail, a cost-effective model serving system that exploits ensembling techniques to meet high accuracy under low latency goals. In Cocktail, we adopt a three-fold approach to reduce the resource footprint of model ensembling. More specifically, we (i) develop a novel dynamic model selection, (ii) design a prudent resource management scheme that utilizes weighted autoscaling for efficient resource allocation, and (iii) leverage transient VM instances to reduce the deployment costs. Our results from extensive evaluations using both CPU and GPU instances on AWS EC2 cloud platform demonstrate that Cocktail can reduce deployment cost by 1.4\u00d7, while reducing latency by 2\u00d7 and satisfying accuracy for 96% of requests, compared to the state-of-the-art model-serving systems.\n\n\nA Modeling of Ensembling\n\nWhile performing an ensemble it is important to be sure that we can reach the desired accuracy by combining more models. In our design, we solve our first objective function (described in Section 4.1) by combining all available models which meet the latency SLO. To be sure that the combination will give us the desired accuracy of the larger model, we try to theoretically analyse the scenario. We formulate the problem conservatively as following.\n\nWe perform an inference by ensembling 'N' models, and each of these models have accuracy 'a'. Therefore the probability of any model giving a correct classification is 'a'. We assume the output to be correct if majority of them, i.e. N/2 + 1 of them give the same result. Then, the final accuracy of this ensemble would be the probability of at least N/2 + 1 of them giving a correct result.\n\nTo we model this problem as a coin-toss problem involving N biased coins with having probability of occurrence of head to be a. Relating this to our problem, each coin represents a model, and an occurrence of head represents the model giving the correct classification. Hence, the problem boils down to find the probability of at least N/2 + 1 heads when all N coins are tossed together. This is a standard binomial distribution problem and can be solved by using the following formula:\nP head = N \u2211 i= N 2 +1 N i a i (1 \u2212 a) (N\u2212i) .\nTo further quantify, let us consider the case where we need to determine if we can reach the accuracy of NasNetLarge (82%) by combining rest of the smaller models which have lesser latency than NasNetLarge. We have 10 (therefore N = 10) such models and among them the least accurate model is MobileNetV1 (accuracy 70%, therefore a = 0.70). We need to find the probability of at least 6 of them being correct. Using the equation above we find the probability to be This corresponds to an accuracy of 83%, which is greater than our required accuracy of 82%). Given all the other models have higher accuracy, the least accuracy we can expect with such an ensemble is 83%. This analysis forms the base of our ensemble technique, and hence proving the combination of multiple available models can be more accurate than the most accurate individual model.\n\n\nB Why DeepARest Model?\n\nWe quantitatively justify the choice of using DeepARest by conducting a brick-by-brick comparison of the accuracy loss when compared with other state-of-the-art prediction models used in prior work. Table 4 shows the root mean squared error (RMSE) incurred by all the models. The ML models used in these experiments are pre-trained with 60% of the Twitter arrival trace. It is evident that the LSTM and DeepAREst have lowest RMSE value. DeepARest is 10% better than LSTM model. Since the primary contribution in Cocktail is to provide high accuracy and low latency predictions at cheaper cost, application developers can adapt the prediction algorithm to their needs or even plug-in their own prediction models.\n\n\nC System Overheads\n\nWe characterize the system-level overheads incurred due to the design choices in Cocktail. The mongodb database is a centralized server, which resides on the head-node. We measure the overall average latency incurred due to all reads/writes in the database, which is well within 1.5ms. The DeepARest prediction model which is not in the critical decision-making path runs as a background process incurring 2.2 ms latency on average. The weighted majority voting takes 0.5ms and the model selection policy takes 0.7ms. The time taken to spawn new VM takes about 60s to 100s depending on the size of the VM instance. The time taken to choose models from the model-cache is less than 1ms. The end-to-end response time to send the image to a worker VM and get the prediction back, was dominated by about 300ms (at maximum) of payload transfer time.    Similarly Table 9 shows the different models trained for BERT-based sentiment analysis on twitter dataset.  \n\n\nD Instance configuration and Pricing\n\n\nE CIFAR-100 and BERT Models\n\n\nF Spot Instance Price Variation\n\nWe profile the spot price of 4 types of C5 EC2 VMs over a 2-week period in August 2020. The price variation is shown in Fig18. \n\nFigure 1 :\n1Benefits of Cocktail. Results are normalized (higher the better).\n\nFigure 2 :\n2The overall framework for model-serving in public cloud.\n\n\nCost of full-ensembling hosted on OD and Spot instances.\n\nFigure 3 :\n3Cost and accuracy of ensembling vs single models.\n\nFigure 4 :\n4Class-wise Accuracy.\n\nFigure 5 :\n5High-level overview of Cocktail design.\n\nFigure 6 :\n6Constraints used in our workloads.\n\n)\nTwitter-trace: Relaxed workload.\n\nFigure 7 :\n7Latency Distribution of InFaas, Clipper and Cocktail for two workload mixes using both Wiki and Twitter traces.\n\nFigure 8 :\n8Cost savings of Cocktail compared to three schemes. Average number of models used in the ensemble.\n\n\nDistribution of requests served by each individual model.\n\nFigure 9 :\n9Benefits of dynamic model selection policy.\n\nFigure 9a\n9aplots the average number of models used for queries falling under the first four different constraint (const) types.\n\nFigure 10 :\n10Figures (a), (b)and (c) shows the number of models used in ensemble with corresponding cumulative accuracy and window accuracy over a 1 hour period for requests under Const1.Figure (d)shows the effects of distributed autoscaling with importance sampling.\n\nFigure 11 :\n11Number of VMs spawned for all four schemes.\n\nFigure 12 :\n12Sensitivity analysis of VMs.\n\nFigure 14 :\n14Sensitivity Constraints under fixed latency and accuracy. Bar graphs (latency) plotted using primary y-axis and line graph (#models) plotted using secondary y-axis. Sentiment analysis.\n\nFigure 15 :\n15Average number of models used in the ensemble.\n\nFigure 16 :\n16Latency reduction (%) plotted as bar graph(primary yaxis) and accuracy gains (%) plotted as line graph (secondary y-axis) over InFaaS.\n\nFigure 17 :\n17Cost savings of Cocktail for Sentiment Analysis.\n\nFigure 18 :\n18Spot instance price variation (time is in hours).\n\nTable 1 :\n1Collection of pretrained models used for image classification.\n\nTable 2 :\n2Comparing Cocktail with other related frameworks.\n\nTable 3 :\n3Comparinglatency of Ensembling (E_Latency) with single \n(baseline) models. \n\n\n\n\nAlgorithm 1 Model Selection and Weighted Majority Voting end procedure 4.1.1 Class-based Weighted Majority Voting1: procedure FULL_ENSEMBLE(MODELLIST, SLO) \n2: \nfor model \u2208 ModelList do \n3: \nif model.latency \u2264 SLO.latency then \n4: \nModel.add(model) \n5: \nend if \n6: \nend for O 1 \n\n7: end procedure \n8: procedure DYNAMIC_MODEL_SCALING(Models) \n9: \nif curr_accuracy \u2265 accuracy_threshold then \n\n10: \nif max vote > N \n2 + 1 then O 2 \n11: \nto_be_dropped \u2190 max vote \u2212 N \n2 + 1 \n12: \nModels.drop(to_be_dropped) \n13: \nend if \n14: \nelse \n15: \naddModel \u2190 f ind_models(remaining_models) \n16: \nModels.append(addModel) \n17: \nend if \n18: end procedure \n19: procedure WEIGHTED_VOTING(Models) \n20: \nfor model in \u2200Models do \n21: \nclass \u2190 model.predicted_class \n22: \nweighted_vote[class]+ = weights[model.class] \n23: \nend for \n24: \nP class \u2190 max(weighted_vote, key = class) \n25: \nreturnP class \n26: \n\nTable 4 :\n4Prediction models.\n\n\nVMs. Also all VM specific metrics such as current_load, CPU utilization, etc. reside in the master node. It runs on a C5.16x [8] large instance to handle these large volume of diverse tasks. Each worker VMs runs a client process to serve its corresponding model. The requests are served as independent parallel threads to ensure timely predictions. We use Python Sanic web-server for communication with the master and worker VMs. Each worker VM runs tensorflow-serving[60] to serve the inference requests. Load Balancer: The master VMs runs a separate thread to monitor the importance sampling of all individual model pools. It keeps track of the number of requests served per model in the past 5 minutes. This information is used for calculating the weights per model for autoscaling decisions. We integrate a mongodb[21] database in the master node to maintain all information about procured instances, spot-instance price list, and instance utilization. The load prediction model resides in the master VM which constantly records the arrival rate in adjacent windows. Recall that the details of the pre-signed Cocktail as a client-server architecture, where one \nmaster VM receives all the incoming requests which are sent \nto individual model worker VMs. \nMaster-Worker Architecture: The master node handles the \nmajor tasks such as (i) concord model selection policy, (ii) \nrequest dispatch to workers VMs as asynchronous future tasks \nusing Python asyncio library, and (iii) ensembling the pre-\ndiction from the worker diction were described in Section 4.2.2. The DeepAREst [4] \nmodel was trained using Keras [22] and Tensorflow, over \n100 epochs with 2 layers, 32 neurons and a batch-size of 1. \nModel Cache: We keep track of the model selected for en-\nsembling on a per request constraint basis. The constraints are \ndefined as <latency,accuracy> pair. The queries arriving \nwith similar constraints can read the model cache to avoid \nre-computation for selecting the models. The model cache \nis implemented as a hash-map using Redis [16] in-memory \nkey-value store for fast access. \nConstraint specification: We expose a simple API to de-\nvelopers, where they can specify the type of inference task \n(e.g., classification) along with the <latency,accuracy> \nconstraints. Developers also need to indicate the primary ob-\njective between these two constraints. Cocktail automatically \n\nDataset \nApplication \nClasses \nTrain-set \nTest-set \nImageNet [29] \nImage \n1000 \n1.2M \n50K \nCIFAR-100 [50] \nImage \n100 \n50K \n10K \nSST-2 [72] \nText \n2 \n9.6K \n1.8K \nSemEval [66] \nText \n3 \n50.3K \n12.2K \n\n\n\nTable 5 :\n5Benchmark Applications and datasets.\n\nTable 6 :\n6Requests meeting target accuracy averaged for both Trace.\n\nTable 7 :\n7Configuration and Pricing for EC2 C5 instances.\n\nTable 8\n8shows the different models available for image prediction, that are pretrained on Keras using CIFAR-100 dataset.Model \nParams \n(M) \n\nTop-1 \nAccuracy(%) \n\nLatency \n(ms) \nP f \n\nAlbert-base [51] \n11 \n91.4 \n55 \n7 \nCodeBert [32] \n125 \n89 \n79 \n6 \nDistilBert [67] \n66 \n90.6 \n92 \n5 \nAlbert-large \n17 \n92.5 \n120 \n4 \nXLNet [85] \n110 \n94.6 \n165 \n3 \nBert [30] \n110 \n92 \n185 \n3 \nRoberta [55] \n355 \n94.3 \n200 \n2 \nAlbert-xlarge \n58 \n93.8 \n220 \n1 \nAlbert-xxlarge \n223 \n95.9 \n350 \n1 \n\n\n\nTable 9 :\n9Pretrained models for Sentiment Analysis using BERT.\n\nTable 8 :\n8Pretrained models for CIFAR-100 using Imagenet.\nWe refer to ensemble-learning as ensembling throughout the paper.\nAcknowledgmentsWe are indebted to our shepherd Manya Ghobadi, the anonymous reviewers and Anup Sarma for their insightful comments to improve the clarity of the presentation. Special mention to Nachiappan Chidambaram N. for his intellectual contributions. This research was partially supported by NSF grants #1931531, #1955815, #1763681, #1908793, #1526750, #2116962, #2122155, #2028929 ,and we thank NSF Chameleon Cloud project CH-819640 for their generous compute grant. All product names used in this publication are for identification purposes only and may be trademarks of their respective companies.Appendix\nTensorflow: learning functions at scale. Mart\u00edn Abadi, Acm Sigplan Notices. ACMMart\u00edn Abadi. Tensorflow: learning functions at scale. In Acm Sigplan Notices. ACM, 2016.\n\nLaser: A scalable response prediction platform for online advertising. Deepak Agarwal, Bo Long, Jonathan Traupman, Doris Xin, Liang Zhang, Proceedings of the 7th ACM international conference on Web search and data mining. the 7th ACM international conference on Web search and data miningDeepak Agarwal, Bo Long, Jonathan Traupman, Doris Xin, and Liang Zhang. Laser: A scalable response prediction platform for online advertising. In Proceedings of the 7th ACM international conference on Web search and data mining, pages 173-182, 2014.\n\nSpotweb: Running latency-sensitive distributed web services on transient cloud servers. Ahmed Ali-Eldin, Jonathan Westin, Bin Wang, Prateek Sharma, Prashant Shenoy, Proceedings of the 28th International Symposium on High-Performance Parallel and Distributed Computing. the 28th International Symposium on High-Performance Parallel and Distributed ComputingAhmed Ali-Eldin, Jonathan Westin, Bin Wang, Prateek Sharma, and Prashant Shenoy. Spotweb: Running latency-sensitive distributed web services on transient cloud servers. In Proceedings of the 28th Inter- national Symposium on High-Performance Parallel and Distributed Computing, pages 1-12, 2019.\n\n. Amazon Deepar, Amazon. Deepar estimator. https://docs.aws.amazon.com/ sagemaker/latest/dg/deepar.html,February2020.\n\n. Amazon Pricing, Amazon. EC2 pricing. https://aws.amazon.com/ec2/pricing/.\n\n. Amazon, Sagemaker, Amazon. Sagemaker. https://aws.amazon.com/sagemaker/, February 2018.\n\nAzure Low priority batch VMs. Amazon, Amazon. Azure Low priority batch VMs., February 2018.\n\n. Amazon, Google Preemptible, Vms, Amazon. Google Preemptible VMs., February 2018. https://cloud.google.com/preemptible-vms .\n\n. Azure. Machine Learning as a Service. Azure. Machine Learning as a Service., February 2018.\n\n. Azure. Ensembling in Azure ML Studio. Azure. Ensembling in Azure ML Studio., February 2020.\n\nBurscale: Using burstable instances for cost-effective autoscaling in the public cloud. Timothy Ataollah Fatahi Baarzi, Bhuvan Zhu, Urgaonkar, Proceedings of the ACM Symposium on Cloud Computing. the ACM Symposium on Cloud ComputingNew York, NY, USAAssociation for Computing MachineryAtaollah Fatahi Baarzi, Timothy Zhu, and Bhuvan Urgaonkar. Burscale: Using burstable instances for cost-effective autoscaling in the public cloud. In Proceedings of the ACM Symposium on Cloud Computing, New York, NY, USA, 2019. Association for Computing Machinery.\n\nRecognizing facial expression: machine learning and application to spontaneous behavior. Marian Stewart Bartlett, Gwen Littlewort, Mark Frank, IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05). IEEE2Claudia Lainscsek, Ian Fasel, and Javier MovellanMarian Stewart Bartlett, Gwen Littlewort, Mark Frank, Claudia Lain- scsek, Ian Fasel, and Javier Movellan. Recognizing facial expression: machine learning and application to spontaneous behavior. In 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05), volume 2, pages 568-573. IEEE, 2005.\n\nAn empirical comparison of voting classification algorithms: Bagging, boosting, and variants. Machine learning. Eric Bauer, Ron Kohavi, 36Eric Bauer and Ron Kohavi. An empirical comparison of voting classification algorithms: Bagging, boosting, and variants. Machine learning, 36(1-2):105-139, 1999.\n\nThe power of ensembles for active learning in image classification. H William, Tim Beluch, Andreas Genewein, Jan M N\u00fcrnberger, K\u00f6hler, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionWilliam H Beluch, Tim Genewein, Andreas N\u00fcrnberger, and Jan M K\u00f6hler. The power of ensembles for active learning in image classifi- cation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9368-9377, 2018.\n\nRedis in action. Josiah L Carlson, Manning Publications CoJosiah L Carlson. Redis in action. Manning Publications Co., 2013.\n\nEnsemble selection from libraries of models. Rich Caruana, Alexandru Niculescu-Mizil, Geoff Crew, Alex Ksikes, Proceedings of the twenty-first international conference on Machine learning. the twenty-first international conference on Machine learning18Rich Caruana, Alexandru Niculescu-Mizil, Geoff Crew, and Alex Ksikes. Ensemble selection from libraries of models. In Proceedings of the twenty-first international conference on Machine learning, page 18, 2004.\n\nRobust bayesian linear classifier ensembles. Jes\u00fas Cerquides, Ramon L\u00f3pez De M\u00e1ntaras, European Conference on Machine Learning. SpringerJes\u00fas Cerquides and Ramon L\u00f3pez De M\u00e1ntaras. Robust bayesian linear classifier ensembles. In European Conference on Machine Learning, pages 72-83. Springer, 2005.\n\nChaos monkey: Increasing sdn reliability through systematic network destruction. Bredan Michael Alan Chang, Theophilus Tschaen, Laurent Benson, Vanbever, Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication. the 2015 ACM Conference on Special Interest Group on Data CommunicationMichael Alan Chang, Bredan Tschaen, Theophilus Benson, and Lau- rent Vanbever. Chaos monkey: Increasing sdn reliability through systematic network destruction. In Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication, pages 371-372, 2015.\n\nFrugalml: How to use ml prediction apis more accurately and cheaply. Lingjiao Chen, Matei Zaharia, James Zou, Advances in Neural Information Processing Systems (NeurIPS). 2020Lingjiao Chen, Matei Zaharia, and James Zou. Frugalml: How to use ml prediction apis more accurately and cheaply. In Advances in Neural Information Processing Systems (NeurIPS), 2020.\n\nMongoDB: the definitive guide: powerful and scalable data storage. Kristina Chodorow, Reilly Media, IncKristina Chodorow. MongoDB: the definitive guide: powerful and scalable data storage. \" O'Reilly Media, Inc.\", 2013.\n\nFrancois Chollet, Deep Learning mit Python und Keras: Das Praxis-Handbuch vom Entwickler der Keras-Bibliothek. MITP-Verlags GmbH & Co. KGFrancois Chollet. Deep Learning mit Python und Keras: Das Praxis- Handbuch vom Entwickler der Keras-Bibliothek. MITP-Verlags GmbH & Co. KG, 2018.\n\nStratus: Cost-aware container scheduling in the public cloud. Andrew Chung, Jun Woo Park, Gregory R Ganger, In SoCC. Andrew Chung, Jun Woo Park, and Gregory R. Ganger. Stratus: Cost-aware container scheduling in the public cloud. In SoCC, 2018.\n\nDeep neural networks for youtube recommendations. Paul Covington, Jay Adams, Emre Sargin, Proceedings of the 10th ACM conference on recommender systems. the 10th ACM conference on recommender systemsPaul Covington, Jay Adams, and Emre Sargin. Deep neural networks for youtube recommendations. In Proceedings of the 10th ACM con- ference on recommender systems, pages 191-198, 2016.\n\nThe missing piece in complex analytics: Low latency, scalable model management and serving with velox. Daniel Crankshaw, Peter Bailis, Joseph E Gonzalez, Haoyuan Li, Zhao Zhang, J Michael, Ali Franklin, Michael I Jordan Ghodsi, arXiv:1409.3809arXiv preprintDaniel Crankshaw, Peter Bailis, Joseph E Gonzalez, Haoyuan Li, Zhao Zhang, Michael J Franklin, Ali Ghodsi, and Michael I Jordan. The missing piece in complex analytics: Low latency, scalable model man- agement and serving with velox. arXiv preprint arXiv:1409.3809, 2014.\n\nInferline: ML inference pipeline composition framework. Daniel Crankshaw, Gur-Eyal, Corey Sela, Xiangxi Zumar, Joseph E Mo, Ion Gonzalez, Alexey Stoica, Tumanov, abs/1812.01776CoRRDaniel Crankshaw, Gur-Eyal Sela, Corey Zumar, Xiangxi Mo, Joseph E. Gonzalez, Ion Stoica, and Alexey Tumanov. Inferline: ML inference pipeline composition framework. CoRR, abs/1812.01776, 2018.\n\nClipper: A low-latency online prediction serving system. Daniel Crankshaw, Xin Wang, Guilio Zhou, Michael J Franklin, Joseph E Gonzalez, Ion Stoica, 14th USENIX Symposium on Networked Systems Design and Implementation (NSDI 17). Boston, MAUSENIX AssociationDaniel Crankshaw, Xin Wang, Guilio Zhou, Michael J. Franklin, Joseph E. Gonzalez, and Ion Stoica. Clipper: A low-latency online pre- diction serving system. In 14th USENIX Symposium on Networked Sys- tems Design and Implementation (NSDI 17), pages 613-627, Boston, MA, March 2017. USENIX Association.\n\nDeep Learning Dtudio. Deepstudio, Deepstudio. Deep Learning Dtudio, February 2020. https://docs.deepcognition.ai/ .\n\nA large-scale hierarchical image database. J Deng, W Dong, R Socher, L Li, Imagenet, 2009 IEEE Conference on Computer Vision and Pattern Recognition. J. Deng, W. Dong, R. Socher, L. Li, and and. Imagenet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, June 2009.\n\nBert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language un- derstanding, 2019.\n\nAutogluon-tabular: Robust and accurate automl for structured data. Nick Erickson, Jonas Mueller, Alexander Shirkov, Hang Zhang, Pedro Larroy, Mu Li, Alexander Smola, Nick Erickson, Jonas Mueller, Alexander Shirkov, Hang Zhang, Pedro Larroy, Mu Li, and Alexander Smola. Autogluon-tabular: Robust and accurate automl for structured data, 2020.\n\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, arXiv:2002.08155A pre-trained model for programming and natural languages. arXiv preprintZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al. Codebert: A pre-trained model for programming and natural languages. arXiv preprint arXiv:2002.08155, 2020.\n\nA review on ensembles for the class imbalance problem: Bagging-, boosting-, and hybrid-based approaches. Mikel Galar, Alberto Fernandez, Edurne Barrenechea, Humberto Bustince, Francisco Herrera, IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews). 424Mikel Galar, Alberto Fernandez, Edurne Barrenechea, Humberto Bustince, and Francisco Herrera. A review on ensembles for the class imbalance problem: Bagging-, boosting-, and hybrid-based ap- proaches. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), 42(4):463-484, 2012.\n\nSwayam: Distributed Autoscaling to Meet SLAs of Machine Learning Inference Services with Resource Efficiency. Arpan Gujarati, Sameh Elnikety, Yuxiong He, Kathryn S Mckinley, Bj\u00f6rn B Brandenburg, USENIX Middleware Conference. Arpan Gujarati, Sameh Elnikety, Yuxiong He, Kathryn S. McKinley, and Bj\u00f6rn B. Brandenburg. Swayam: Distributed Autoscaling to Meet SLAs of Machine Learning Inference Services with Resource Efficiency. In USENIX Middleware Conference, 2017.\n\nServing dnns like clockwork: Performance predictability from the bottom up. Arpan Gujarati, Reza Karimi, Safya Alzayat, Antoine Kaufmann, Ymir Vigfusson, Jonathan Mace, 14th USENIX Symposium on Operating Systems Design and Implementation. Banff, AlbertaUSENIX Association20Arpan Gujarati, Reza Karimi, Safya Alzayat, Antoine Kaufmann, Ymir Vigfusson, and Jonathan Mace. Serving dnns like clockwork: Perfor- mance predictability from the bottom up. In 14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20), Banff, Alberta, November 2020. USENIX Association.\n\nFifer: Tackling Resource Underutilization in the Serverless Era. Prashanth Jashwant Raj Gunasekaran, Thinakaran, C Nachiappan, Nachiappan, Chita R Mahmut Taylan Kandemir, Das, USENIX Middleware Conference. Jashwant Raj Gunasekaran, Prashanth Thinakaran, Nachiappan C.Nachiappan, Mahmut Taylan Kandemir, and Chita R. Das. Fifer: Tackling Resource Underutilization in the Serverless Era. In USENIX Middleware Conference, 2020.\n\nSpock: Exploiting serverless functions for slo and cost aware resource procurement in public cloud. Prashanth Jashwant Raj Gunasekaran, Thinakaran, Bhuvan Mahmut Taylan Kandemir, George Urgaonkar, Chita Kesidis, Das, IEEE CLOUDJashwant Raj Gunasekaran, Prashanth Thinakaran, Mahmut Taylan Kandemir, Bhuvan Urgaonkar, George Kesidis, and Chita Das. Spock: Exploiting serverless functions for slo and cost aware resource procure- ment in public cloud. In IEEE CLOUD, 2019.\n\nTowards designing a self-managed machine learning inference serving system inpublic cloud. Prashanth Jashwant Raj Gunasekaran, Cyan Thinakaran, Subhra Mishra, Chita R Mahmut Taylan Kandemir, Das, Jashwant Raj Gunasekaran, Prashanth Thinakaran, Cyan Subhra Mishra, Mahmut Taylan Kandemir, and Chita R. Das. Towards designing a self-managed machine learning inference serving system inpublic cloud, 2020.\n\nDeeprecsys: A system for optimizing end-to-end at-scale neural recommendation inference. U Gupta, S Hsia, V Saraph, X Wang, B Reagen, G Wei, H S Lee, D Brooks, C Wu, 2020U. Gupta, S. Hsia, V. Saraph, X. Wang, B. Reagen, G. Wei, H. S. Lee, D. Brooks, and C. Wu. Deeprecsys: A system for optimiz- ing end-to-end at-scale neural recommendation inference. In 2020\n\nACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA). ACM/IEEE 47th Annual International Symposium on Computer Archi- tecture (ISCA), pages 982-995, 2020.\n\nEnabling cost-aware and adaptive elasticity of multi-tier cloud applications. Rui Han, M Moustafa, Li Ghanem, Yike Guo, Michelle Guo, Osmond, Future Gener. Comput. Syst. 32CRui Han, Moustafa M. Ghanem, Li Guo, Yike Guo, and Michelle Osmond. Enabling cost-aware and adaptive elasticity of multi-tier cloud applications. Future Gener. Comput. Syst., 32(C):82-98, March 2014.\n\nTributary: spot-dancing for elastic services with latency SLOs. Aaron Harlap, Andrew Chung, Alexey Tumanov, Gregory R Ganger, Phillip B Gibbons, In ATC. Aaron Harlap, Andrew Chung, Alexey Tumanov, Gregory R. Ganger, and Phillip B. Gibbons. Tributary: spot-dancing for elastic services with latency SLOs. In ATC, 2018.\n\nProteus: Agile ML Elasticity Through Tiered Reliability in Dynamic Resource Markets. Aaron Harlap, Alexey Tumanov, Andrew Chung, Gregory R Ganger, Phillip B Gibbons, Eurosys. Aaron Harlap, Alexey Tumanov, Andrew Chung, Gregory R. Ganger, and Phillip B. Gibbons. Proteus: Agile ML Elasticity Through Tiered Reliability in Dynamic Resource Markets. In Eurosys, 2017.\n\nSirius: An open end-to-end voice and vision personal assistant and its implications for future warehouse scale computers. Johann Hauswald, Michael A Laurenzano, Yunqi Zhang, Cheng Li, Austin Rovinski, Arjun Khurana, Ronald G Dreslinski, Trevor Mudge, Vinicius Petrucci, Lingjia Tang, Jason Mars, ASPLOS. Johann Hauswald, Michael A. Laurenzano, Yunqi Zhang, Cheng Li, Austin Rovinski, Arjun Khurana, Ronald G. Dreslinski, Trevor Mudge, Vinicius Petrucci, Lingjia Tang, and Jason Mars. Sirius: An open end-to-end voice and vision personal assistant and its implications for future warehouse scale computers. In ASPLOS, 2015.\n\nApplied machine learning at facebook: A datacenter infrastructure perspective. K Hazelwood, S Bird, D Brooks, S Chintala, U Diril, D Dzhulgakov, M Fawzy, B Jia, Y Jia, A Kalro, J Law, K Lee, J Lu, P Noordhuis, M Smelyanskiy, L Xiong, X Wang, 2018 IEEE International Symposium on High Performance Computer Architecture (HPCA). K. Hazelwood, S. Bird, D. Brooks, S. Chintala, U. Diril, D. Dzhulgakov, M. Fawzy, B. Jia, Y. Jia, A. Kalro, J. Law, K. Lee, J. Lu, P. Noordhuis, M. Smelyanskiy, L. Xiong, and X. Wang. Applied machine learning at facebook: A datacenter infrastructure perspective. In 2018 IEEE International Symposium on High Performance Computer Architecture (HPCA), pages 620-629, Feb 2018.\n\nSearching for mobilenetv3. Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionAndrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vi- jay Vasudevan, et al. Searching for mobilenetv3. In Proceedings of the IEEE International Conference on Computer Vision, pages 1314-1324, 2019.\n\nZookeeper: Wait-free coordination for internet-scale systems. Patrick Hunt, Mahadev Konar, Benjamin Flavio Paiva Junqueira, Reed, USENIX annual technical conference. Patrick Hunt, Mahadev Konar, Flavio Paiva Junqueira, and Benjamin Reed. Zookeeper: Wait-free coordination for internet-scale systems. In USENIX annual technical conference, 2010.\n\nHourly thermal load prediction for the next 24 hours by arima, ewma, lr and an artificial neural network. Minoru Kawashima, John W Charles E Dorgan, Mitchell, American Society of HeatingTechnical reportRefrigerating and Air-Conditioning EngineersMinoru Kawashima, Charles E Dorgan, and John W Mitchell. Hourly thermal load prediction for the next 24 hours by arima, ewma, lr and an artificial neural network. Technical report, American Society of Heating, Refrigerating and Air-Conditioning Engineers . . . , 1995.\n\nCloud-based disaster management as a service: A microservice approach for hurricane twitter data analysis. Abdel Abeer, Ilkyeun Khaleq, Ra, GHTC. Abeer Abdel Khaleq and Ilkyeun Ra. Cloud-based disaster manage- ment as a service: A microservice approach for hurricane twitter data analysis. In GHTC, 2018.\n\nDynamic weighted majority: An ensemble method for drifting concepts. Zico Kolter, Marcus A Maloof, Journal of Machine Learning Research. 8J Zico Kolter and Marcus A Maloof. Dynamic weighted majority: An ensemble method for drifting concepts. Journal of Machine Learning Research, 8(Dec):2755-2790, 2007.\n\nCifar-100 (canadian institute for advanced research). Alex Krizhevsky, Vinod Nair, Geoffrey Hinton, Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-100 (cana- dian institute for advanced research), 2010. http://www.cs.toronto. edu/~kriz/cifar.html.\n\nAlbert: A lite bert for self-supervised learning of language representations. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut, arXiv:1909.11942arXiv preprintZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gim- pel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942, 2019.\n\nPRETZEL: Opening the black box of machine learning prediction serving systems. Yunseong Lee, Alberto Scolari, Byung-Gon Chun, Marco Domenico Santambrogio, Markus Weimer, Matteo Interlandi, 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18). Carlsbad, CAUSENIX AssociationYunseong Lee, Alberto Scolari, Byung-Gon Chun, Marco Domenico Santambrogio, Markus Weimer, and Matteo Interlandi. PRETZEL: Opening the black box of machine learning prediction serving systems. In 13th USENIX Symposium on Operating Systems Design and Imple- mentation (OSDI 18), pages 611-626, Carlsbad, CA, October 2018. USENIX Association.\n\nLarge-scale real-time product recommendation at criteo. Romain Lerallut, Diane Gasselin, Nicolas Le Roux, Proceedings of the 9th ACM Conference on Recommender Systems. the 9th ACM Conference on Recommender SystemsRomain Lerallut, Diane Gasselin, and Nicolas Le Roux. Large-scale real-time product recommendation at criteo. In Proceedings of the 9th ACM Conference on Recommender Systems, pages 232-232, 2015.\n\nA survey of deep neural network architectures and their applications. Weibo Liu, Zidong Wang, Xiaohui Liu, Nianyin Zeng, Yurong Liu, Fuad E Alsaadi, Neurocomputing. 234Weibo Liu, Zidong Wang, Xiaohui Liu, Nianyin Zeng, Yurong Liu, and Fuad E Alsaadi. A survey of deep neural network architectures and their applications. Neurocomputing, 234:11-26, 2017.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, Roberta, arXiv:1907.11692A robustly optimized bert pretraining approach. arXiv preprintYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoy- anov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.\n\nEnsemble pruning via individual contribution ordering. Zhenyu Lu, Xindong Wu, Xingquan Zhu, Josh Bongard, Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '10. the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '10New York, NY, USAAssociation for Computing MachineryZhenyu Lu, Xindong Wu, Xingquan Zhu, and Josh Bongard. Ensemble pruning via individual contribution ordering. In Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '10, page 871-880, New York, NY, USA, 2010. Association for Computing Machinery.\n\nOrigin: Enabling on-device intelligence for human activity recognition using energy harvesting wireless sensor networks. Jack Cyan Subhra Mishra, Sampson, Vijaykrishnan Mahmut Taylan Kandemir, Narayanan, 2021 Design, Automation Test in Europe Conference Exhibition (DATE). Cyan Subhra Mishra, Jack Sampson, Mahmut Taylan Kandemir, and Vijaykrishnan Narayanan. Origin: Enabling on-device intelligence for human activity recognition using energy harvesting wireless sensor networks. In 2021 Design, Automation Test in Europe Conference Exhibition (DATE), pages 1414-1419, 2021.\n\nAlembic: Automated model inference for stateful network functions. Soo-Jin Moon, Jeffrey Helt, Yifei Yuan, Yves Bieri, Sujata Banerjee, Vyas Sekar, Wenfei Wu, Mihalis Yannakakis, Ying Zhang, 16th USENIX Symposium on Networked Systems Design and Implementation (NSDI 19). Boston, MAUSENIX AssociationSoo-Jin Moon, Jeffrey Helt, Yifei Yuan, Yves Bieri, Sujata Banerjee, Vyas Sekar, Wenfei Wu, Mihalis Yannakakis, and Ying Zhang. Alem- bic: Automated model inference for stateful network functions. In 16th USENIX Symposium on Networked Systems Design and Implementa- tion (NSDI 19), pages 699-718, Boston, MA, February 2019. USENIX Association.\n\nPipedream: generalized pipeline parallelism for dnn training. Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, R Nikhil, Devanur, R Gregory, Ganger, B Phillip, Matei Gibbons, Zaharia, Proceedings of the 27th ACM Symposium on Operating Systems Principles. the 27th ACM Symposium on Operating Systems PrinciplesDeepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R Devanur, Gregory R Ganger, Phillip B Gibbons, and Matei Zaharia. Pipedream: generalized pipeline parallelism for dnn training. In Proceedings of the 27th ACM Symposium on Operating Systems Principles, pages 1-15, 2019.\n\nTensorflow-serving: Flexible, high-performance ml serving. Christopher Olston, Noah Fiedel, Kiril Gorovoy, Jeremiah Harmsen, Li Lao, Fangwei Li, Vinu Rajashekhar, Sukriti Ramesh, Jordan Soyke, arXiv:1712.06139arXiv preprintChristopher Olston, Noah Fiedel, Kiril Gorovoy, Jeremiah Harmsen, Li Lao, Fangwei Li, Vinu Rajashekhar, Sukriti Ramesh, and Jordan Soyke. Tensorflow-serving: Flexible, high-performance ml serving. arXiv preprint arXiv:1712.06139, 2017.\n\nOnline bagging and boosting. C Nikunj, Oza, IEEE international conference on systems. 3Ieeeman and cyberneticsNikunj C Oza. Online bagging and boosting. In 2005 IEEE interna- tional conference on systems, man and cybernetics, volume 3, pages 2340-2345. Ieee, 2005.\n\nPytorch: An imperative style, high-performance deep learning library. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Advances in neural information processing systems. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Brad- bury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In Advances in neural information processing systems, pages 8026-8037, 2019.\n\nSwift machine learning model serving scheduling: a region based reinforcement learning approach. Heyang Qin, Syed Zawad, Yanqi Zhou, Lei Yang, Dongfang Zhao, Feng Yan, Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. the International Conference for High Performance Computing, Networking, Storage and AnalysisHeyang Qin, Syed Zawad, Yanqi Zhou, Lei Yang, Dongfang Zhao, and Feng Yan. Swift machine learning model serving scheduling: a region based reinforcement learning approach. In Proceedings of the Inter- national Conference for High Performance Computing, Networking, Storage and Analysis, pages 1-23, 2019.\n\nEnsemble deep learning for regression and time series forecasting. Xueheng Qiu, Le Zhang, Ye Ren, N Ponnuthurai, Gehan Suganthan, Amaratunga, 2014 IEEE symposium on computational intelligence in ensemble learning (CIEL). IEEEXueheng Qiu, Le Zhang, Ye Ren, Ponnuthurai N Suganthan, and Gehan Amaratunga. Ensemble deep learning for regression and time series forecasting. In 2014 IEEE symposium on computational intelligence in ensemble learning (CIEL), pages 1-6. IEEE, 2014.\n\nEfficient fpga acceleration of convolutional neural networks using logical-3d compute array. Atul Rahman, Jongeun Lee, Kiyoung Choi, 2016 Design, Automation & Test in Europe Conference & Exhibition (DATE). IEEEAtul Rahman, Jongeun Lee, and Kiyoung Choi. Efficient fpga acceler- ation of convolutional neural networks using logical-3d compute array. In 2016 Design, Automation & Test in Europe Conference & Exhibition (DATE), pages 1393-1398. IEEE, 2016.\n\nSemEval-2017 task 4: Sentiment analysis in Twitter. Sara Rosenthal, Noura Farra, Preslav Nakov, Proceedings of the 11th International Workshop on Semantic Evaluation. the 11th International Workshop on Semantic EvaluationVancouver, CanadaAssociation for Computational LinguisticsSara Rosenthal, Noura Farra, and Preslav Nakov. SemEval-2017 task 4: Sentiment analysis in Twitter. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 502-518, Vancouver, Canada, August 2017. Association for Computational Linguistics.\n\nDistilbert, a distilled version of bert: smaller, faster, cheaper and lighter. Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf, arXiv:1910.01108arXiv preprintVictor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.\n\nPortfolio-driven resource management for transient cloud servers. Prateek Sharma, David Irwin, Prashant Shenoy, Proc. ACM Meas. Anal. Comput. Syst. 11Prateek Sharma, David Irwin, and Prashant Shenoy. Portfolio-driven resource management for transient cloud servers. Proc. ACM Meas. Anal. Comput. Syst., 1(1), June 2017.\n\nSpotcheck: Designing a derivative iaas cloud on the spot market. Prateek Sharma, Stephen Lee, Tian Guo, David Irwin, Prashant Shenoy, Proceedings of the Tenth European Conference on Computer Systems. the Tenth European Conference on Computer SystemsPrateek Sharma, Stephen Lee, Tian Guo, David Irwin, and Prashant Shenoy. Spotcheck: Designing a derivative iaas cloud on the spot market. In Proceedings of the Tenth European Conference on Computer Systems, pages 1-15, 2015.\n\nIntelligent performance-based product recommendation system. A Steven, Neal Shaya, John Anthony Matheson, Nikiforos Singarayar, Jeffrey Adam Kollias, Bloom, US Patent. 7601Steven A Shaya, Neal Matheson, John Anthony Singarayar, Nikiforos Kollias, and Jeffrey Adam Bloom. Intelligent performance-based prod- uct recommendation system, October 5 2010. US Patent 7,809,601.\n\nDeep learning for nlp. Richard Socher, Yoshua Bengio, Chris Manning, Tutorial at Association of Computational Logistics (ACL). Richard Socher, Yoshua Bengio, and Chris Manning. Deep learning for nlp. Tutorial at Association of Computational Logistics (ACL), 2012.\n\nRecursive deep models for semantic compositionality over a sentiment treebank. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, D Christopher, Manning, Y Andrew, Christopher Ng, Potts, Proceedings of the 2013 conference on empirical methods in natural language processing. the 2013 conference on empirical methods in natural language processingRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631-1642, 2013.\n\nMingxing Tan, V Quoc, Le, Efficientnet, arXiv:1905.11946Rethinking model scaling for convolutional neural networks. arXiv preprintMingxing Tan and Quoc V Le. Efficientnet: Rethinking model scaling for convolutional neural networks. arXiv preprint arXiv:1905.11946, 2019.\n\nPhoenix: A constraint-aware scheduler for heterogeneous datacenters. P Thinakaran, J R Gunasekaran, B Sharma, M T Kandemir, C R Das, 2017 IEEE 37th International Conference on Distributed Computing Systems (ICDCS). P. Thinakaran, J. R. Gunasekaran, B. Sharma, M. T. Kandemir, and C. R. Das. Phoenix: A constraint-aware scheduler for heteroge- neous datacenters. In 2017 IEEE 37th International Conference on Distributed Computing Systems (ICDCS), June 2017.\n\nKube-Knots: Resource Harvesting through Dynamic Container Orchestration in GPU-based Datacenters. P Thinakaran, J R Gunasekaran, B Sharma, M T Kandemir, C R Das, CLUSTER. P. Thinakaran, J. R. Gunasekaran, B. Sharma, M. T. Kandemir, and C. R. Das. Kube-Knots: Resource Harvesting through Dynamic Container Orchestration in GPU-based Datacenters. In CLUSTER, 2019.\n\nWikipedia workload analysis for decentralized hosting. Guido Urdaneta, Guillaume Pierre, Maarten Van Steen, Computer Networks. Guido Urdaneta, Guillaume Pierre, and Maarten Van Steen. Wikipedia workload analysis for decentralized hosting. Computer Networks, 2009.\n\nModest adaboostteaching adaboost to generalize better. Alexander Vezhnevets, Vladimir Vezhnevets, Graphicon. Alexander Vezhnevets and Vladimir Vezhnevets. Modest adaboost- teaching adaboost to generalize better. In Graphicon, pages 987-997, 2005.\n\nTreatment of uncertainty using ensemble methods: Comparison of sequential data assimilation and bayesian model averaging. A Jasper, Vrugt, Bruce A Robinson, Water Resources Research. 431Jasper A Vrugt and Bruce A Robinson. Treatment of uncertainty using ensemble methods: Comparison of sequential data assimilation and bayesian model averaging. Water Resources Research, 43(1), 2007.\n\nUsing burstable instances in the public cloud: Why, when and how? SIGMETRICS. Cheng Wang, Bhuvan Urgaonkar, Neda Nasiriani, George Kesidis, Cheng Wang, Bhuvan Urgaonkar, Neda Nasiriani, and George Kesidis. Using burstable instances in the public cloud: Why, when and how? SIGMETRICS, June 2017.\n\nRafiki: machine learning as an analytics service system. Wei Wang, Jinyang Gao, Meihui Zhang, Sheng Wang, Gang Chen, Teck Khim Ng, Beng Chin Ooi, Jie Shao, Moaz Reyad, Proceedings of the VLDB Endowment. the VLDB Endowment12Wei Wang, Jinyang Gao, Meihui Zhang, Sheng Wang, Gang Chen, Teck Khim Ng, Beng Chin Ooi, Jie Shao, and Moaz Reyad. Rafiki: machine learning as an analytics service system. Proceedings of the VLDB Endowment, 12(2):128-140, 2018.\n\nHuggingface's transformers: State-ofthe-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, arXiv:1910.03771arXiv preprintThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, et al. Huggingface's transformers: State-of- the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.\n\nMachine learning at facebook: Understanding inference at the edge. Carole-Jean Wu, David Brooks, Kevin Chen, Douglas Chen, Sy Choudhury, Marat Dukhan, Kim Hazelwood, Eldad Isaac, Yangqing Jia, Bill Jia, 2019 IEEE International Symposium on High Performance Computer Architecture (HPCA). IEEECarole-Jean Wu, David Brooks, Kevin Chen, Douglas Chen, Sy Choud- hury, Marat Dukhan, Kim Hazelwood, Eldad Isaac, Yangqing Jia, Bill Jia, et al. Machine learning at facebook: Understanding inference at the edge. In 2019 IEEE International Symposium on High Performance Computer Architecture (HPCA), pages 331-344. IEEE, 2019.\n\nA case for managed and model-less inference serving. J Neeraja, Francisco Yadwadkar, Qian Romero, Christos Li, Kozyrakis, Proceedings of the Workshop on Hot Topics in Operating Systems. the Workshop on Hot Topics in Operating SystemsNew York, NY, USAAssociation for Computing MachineryNeeraja J. Yadwadkar, Francisco Romero, Qian Li, and Christos Kozyrakis. A case for managed and model-less inference serving. In Proceedings of the Workshop on Hot Topics in Operating Systems, New York, NY, USA, 2019. Association for Computing Machinery.\n\nNetadapt: Platform-aware neural network adaptation for mobile applications. Tien-Ju Yang, Andrew G Howard, Bo Chen, Xiao Zhang, Alec Go, Vivienne Sze, Hartwig Adam, abs/1804.03230CoRRTien-Ju Yang, Andrew G. Howard, Bo Chen, Xiao Zhang, Alec Go, Vivienne Sze, and Hartwig Adam. Netadapt: Platform-aware neural network adaptation for mobile applications. CoRR, abs/1804.03230, 2018.\n\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V Le, Xlnet, arXiv:1906.08237Generalized autoregressive pretraining for language understanding. arXiv preprintZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pre- training for language understanding. arXiv preprint arXiv:1906.08237, 2019.\n\nMark: Exploiting cloud services for cost-effective, slo-aware machine learning inference serving. Chengliang Zhang, Minchen Yu, Wei Wang, Feng Yan, ATC. Chengliang Zhang, Minchen Yu, Wei Wang, and Feng Yan. Mark: Exploiting cloud services for cost-effective, slo-aware machine learning inference serving. In ATC, 2019.\n\nIdentifying outlier arms in multi-armed bandit. Honglei Zhuang, Chi Wang, Yifan Wang, Advances in Neural Information Processing Systems. Honglei Zhuang, Chi Wang, and Yifan Wang. Identifying outlier arms in multi-armed bandit. In Advances in Neural Information Processing Systems, pages 5204-5213, 2017.\n\nIris recognition performance enhancement using weighted majority voting. Sheikh Ziauddin, N Matthew, Dailey, 15th IEEE International Conference on Image Processing. IEEESheikh Ziauddin and Matthew N Dailey. Iris recognition performance enhancement using weighted majority voting. In 2008 15th IEEE International Conference on Image Processing, pages 277-280. IEEE, 2008.\n", "annotations": {"author": "[{\"end\":150,\"start\":72},{\"end\":223,\"start\":151},{\"end\":298,\"start\":224},{\"end\":372,\"start\":299},{\"end\":442,\"start\":373},{\"end\":508,\"start\":443}]", "publisher": null, "author_last_name": "[{\"end\":96,\"start\":85},{\"end\":169,\"start\":163},{\"end\":244,\"start\":234},{\"end\":318,\"start\":312},{\"end\":388,\"start\":380},{\"end\":454,\"start\":451}]", "author_first_name": "[{\"end\":80,\"start\":72},{\"end\":84,\"start\":81},{\"end\":155,\"start\":151},{\"end\":162,\"start\":156},{\"end\":233,\"start\":224},{\"end\":311,\"start\":305},{\"end\":379,\"start\":373},{\"end\":448,\"start\":443},{\"end\":450,\"start\":449}]", "author_affiliation": "[{\"end\":149,\"start\":98},{\"end\":222,\"start\":171},{\"end\":297,\"start\":246},{\"end\":371,\"start\":320},{\"end\":441,\"start\":390},{\"end\":507,\"start\":456}]", "title": "[{\"end\":69,\"start\":1},{\"end\":577,\"start\":509}]", "venue": null, "abstract": "[{\"end\":2162,\"start\":579}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b69\"},\"end\":2314,\"start\":2310},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":2348,\"start\":2344},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2374,\"start\":2370},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":2377,\"start\":2374},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":2406,\"start\":2402},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":2409,\"start\":2406},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2637,\"start\":2633},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":2640,\"start\":2637},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2643,\"start\":2640},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":2646,\"start\":2643},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":2649,\"start\":2646},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":2652,\"start\":2649},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2952,\"start\":2949},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2955,\"start\":2952},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":2958,\"start\":2955},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":3024,\"start\":3020},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":3118,\"start\":3114},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":3121,\"start\":3118},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":3336,\"start\":3332},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":3339,\"start\":3336},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":3342,\"start\":3339},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":3474,\"start\":3470},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3640,\"start\":3636},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3778,\"start\":3777},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4027,\"start\":4023},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4195,\"start\":4191},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":4198,\"start\":4195},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4817,\"start\":4813},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":5079,\"start\":5075},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":5323,\"start\":5319},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":5405,\"start\":5401},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":5408,\"start\":5405},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6705,\"start\":6702},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8084,\"start\":8083},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8247,\"start\":8244},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8608,\"start\":8607},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":9635,\"start\":9631},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":9638,\"start\":9635},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":10137,\"start\":10133},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10691,\"start\":10688},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":10694,\"start\":10691},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":10697,\"start\":10694},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":10797,\"start\":10793},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11066,\"start\":11062},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":11485,\"start\":11481},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":11660,\"start\":11656},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":11684,\"start\":11680},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11773,\"start\":11769},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":11776,\"start\":11773},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":11779,\"start\":11776},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":11782,\"start\":11779},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":11785,\"start\":11782},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":11788,\"start\":11785},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":11791,\"start\":11788},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":11793,\"start\":11791},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":11915,\"start\":11911},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":11938,\"start\":11934},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":12066,\"start\":12062},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":12078,\"start\":12074},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":12090,\"start\":12086},{\"end\":12097,\"start\":12096},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":12133,\"start\":12129},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":12506,\"start\":12502},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":12523,\"start\":12519},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12615,\"start\":12611},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":12891,\"start\":12887},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":12894,\"start\":12891},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":12909,\"start\":12905},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":13153,\"start\":13149},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":13398,\"start\":13394},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":13417,\"start\":13413},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":13549,\"start\":13546},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":13552,\"start\":13549},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":13555,\"start\":13552},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":13558,\"start\":13555},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":13561,\"start\":13558},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":13564,\"start\":13561},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":13933,\"start\":13929},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":13948,\"start\":13945},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":13962,\"start\":13958},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":13980,\"start\":13976},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":14226,\"start\":14222},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":14229,\"start\":14226},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":14232,\"start\":14229},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":14235,\"start\":14232},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":14238,\"start\":14235},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":14241,\"start\":14238},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":14244,\"start\":14241},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":14316,\"start\":14312},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":14319,\"start\":14316},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":14322,\"start\":14319},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":14325,\"start\":14322},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":14328,\"start\":14325},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":14331,\"start\":14328},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":14469,\"start\":14465},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":15252,\"start\":15248},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":15255,\"start\":15252},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":15405,\"start\":15401},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":15408,\"start\":15405},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":15434,\"start\":15430},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":15667,\"start\":15663},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":15725,\"start\":15722},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":16843,\"start\":16839},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":18987,\"start\":18984},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":21600,\"start\":21596},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":21628,\"start\":21625},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":21660,\"start\":21657},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":21721,\"start\":21718},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":30479,\"start\":30475},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":30482,\"start\":30479},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":32346,\"start\":32342},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":32507,\"start\":32503},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":33373,\"start\":33369},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":33376,\"start\":33373},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":35100,\"start\":35097},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":36089,\"start\":36085},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":36108,\"start\":36104},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":36656,\"start\":36652},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":36803,\"start\":36799},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":37309,\"start\":37305},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":37373,\"start\":37369},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":37406,\"start\":37402},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":37423,\"start\":37419},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":38196,\"start\":38192},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":39671,\"start\":39667},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":39756,\"start\":39752},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":51528,\"start\":51524},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":54178,\"start\":54174},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":65502,\"start\":65498},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":65852,\"start\":65848}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":62125,\"start\":62047},{\"attributes\":{\"id\":\"fig_1\"},\"end\":62195,\"start\":62126},{\"attributes\":{\"id\":\"fig_2\"},\"end\":62254,\"start\":62196},{\"attributes\":{\"id\":\"fig_3\"},\"end\":62317,\"start\":62255},{\"attributes\":{\"id\":\"fig_4\"},\"end\":62351,\"start\":62318},{\"attributes\":{\"id\":\"fig_5\"},\"end\":62404,\"start\":62352},{\"attributes\":{\"id\":\"fig_6\"},\"end\":62452,\"start\":62405},{\"attributes\":{\"id\":\"fig_7\"},\"end\":62488,\"start\":62453},{\"attributes\":{\"id\":\"fig_8\"},\"end\":62613,\"start\":62489},{\"attributes\":{\"id\":\"fig_9\"},\"end\":62725,\"start\":62614},{\"attributes\":{\"id\":\"fig_10\"},\"end\":62785,\"start\":62726},{\"attributes\":{\"id\":\"fig_11\"},\"end\":62842,\"start\":62786},{\"attributes\":{\"id\":\"fig_12\"},\"end\":62972,\"start\":62843},{\"attributes\":{\"id\":\"fig_13\"},\"end\":63242,\"start\":62973},{\"attributes\":{\"id\":\"fig_14\"},\"end\":63301,\"start\":63243},{\"attributes\":{\"id\":\"fig_15\"},\"end\":63345,\"start\":63302},{\"attributes\":{\"id\":\"fig_21\"},\"end\":63545,\"start\":63346},{\"attributes\":{\"id\":\"fig_22\"},\"end\":63607,\"start\":63546},{\"attributes\":{\"id\":\"fig_24\"},\"end\":63757,\"start\":63608},{\"attributes\":{\"id\":\"fig_25\"},\"end\":63821,\"start\":63758},{\"attributes\":{\"id\":\"fig_27\"},\"end\":63886,\"start\":63822},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":63961,\"start\":63887},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":64023,\"start\":63962},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":64113,\"start\":64024},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":64996,\"start\":64114},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":65027,\"start\":64997},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":67623,\"start\":65028},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":67672,\"start\":67624},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":67742,\"start\":67673},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":67802,\"start\":67743},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":68281,\"start\":67803},{\"attributes\":{\"id\":\"tab_17\",\"type\":\"table\"},\"end\":68346,\"start\":68282},{\"attributes\":{\"id\":\"tab_19\",\"type\":\"table\"},\"end\":68406,\"start\":68347}]", "paragraph": "[{\"end\":4629,\"start\":2178},{\"end\":4743,\"start\":4631},{\"end\":5035,\"start\":4745},{\"end\":5380,\"start\":5037},{\"end\":5728,\"start\":5382},{\"end\":6965,\"start\":5730},{\"end\":8324,\"start\":6967},{\"end\":8938,\"start\":8341},{\"end\":9762,\"start\":8968},{\"end\":11795,\"start\":9796},{\"end\":12041,\"start\":11812},{\"end\":12423,\"start\":12054},{\"end\":14332,\"start\":12425},{\"end\":14928,\"start\":14334},{\"end\":15512,\"start\":14966},{\"end\":15726,\"start\":15553},{\"end\":17342,\"start\":15728},{\"end\":17978,\"start\":17366},{\"end\":18245,\"start\":17980},{\"end\":19293,\"start\":18247},{\"end\":22611,\"start\":19317},{\"end\":23460,\"start\":22642},{\"end\":24309,\"start\":23462},{\"end\":24602,\"start\":24344},{\"end\":25112,\"start\":24666},{\"end\":25733,\"start\":25114},{\"end\":26343,\"start\":25735},{\"end\":28730,\"start\":26380},{\"end\":29369,\"start\":28732},{\"end\":29412,\"start\":29371},{\"end\":29438,\"start\":29414},{\"end\":29466,\"start\":29440},{\"end\":29507,\"start\":29468},{\"end\":29540,\"start\":29509},{\"end\":29563,\"start\":29542},{\"end\":29606,\"start\":29565},{\"end\":29635,\"start\":29608},{\"end\":29692,\"start\":29637},{\"end\":29721,\"start\":29694},{\"end\":29756,\"start\":29723},{\"end\":29769,\"start\":29758},{\"end\":29858,\"start\":29771},{\"end\":30115,\"start\":29882},{\"end\":32104,\"start\":30139},{\"end\":33023,\"start\":32119},{\"end\":34295,\"start\":33025},{\"end\":34765,\"start\":34297},{\"end\":34997,\"start\":34767},{\"end\":35235,\"start\":35031},{\"end\":36296,\"start\":35273},{\"end\":37704,\"start\":36323},{\"end\":38774,\"start\":37706},{\"end\":40501,\"start\":38797},{\"end\":40830,\"start\":40525},{\"end\":41765,\"start\":40871},{\"end\":42323,\"start\":41767},{\"end\":43767,\"start\":42348},{\"end\":44564,\"start\":43769},{\"end\":48972,\"start\":44596},{\"end\":49759,\"start\":49070},{\"end\":50021,\"start\":49784},{\"end\":50920,\"start\":50043},{\"end\":51967,\"start\":50952},{\"end\":53959,\"start\":51969},{\"end\":55311,\"start\":54015},{\"end\":56867,\"start\":55349},{\"end\":57841,\"start\":56890},{\"end\":58319,\"start\":57870},{\"end\":58712,\"start\":58321},{\"end\":59200,\"start\":58714},{\"end\":60097,\"start\":59248},{\"end\":60835,\"start\":60124},{\"end\":61814,\"start\":60858},{\"end\":62046,\"start\":61919}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":24665,\"start\":24603},{\"attributes\":{\"id\":\"formula_1\"},\"end\":26379,\"start\":26344},{\"attributes\":{\"id\":\"formula_2\"},\"end\":59247,\"start\":59201}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":14828,\"start\":14821},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":16072,\"start\":16065},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":16657,\"start\":16650},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":17898,\"start\":17891},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":33218,\"start\":33210},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":37001,\"start\":36994},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":37877,\"start\":37870},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":38217,\"start\":38210},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":45861,\"start\":45854},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":54305,\"start\":54298},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":60330,\"start\":60323},{\"attributes\":{\"ref_id\":\"tab_17\"},\"end\":61723,\"start\":61716}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2176,\"start\":2164},{\"end\":8339,\"start\":8327},{\"attributes\":{\"n\":\"2\"},\"end\":8966,\"start\":8941},{\"attributes\":{\"n\":\"2.1\"},\"end\":9794,\"start\":9765},{\"attributes\":{\"n\":\"2.2\"},\"end\":11810,\"start\":11798},{\"end\":12052,\"start\":12044},{\"attributes\":{\"n\":\"2.3\"},\"end\":14964,\"start\":14931},{\"attributes\":{\"n\":\"2.3.1\"},\"end\":15551,\"start\":15515},{\"attributes\":{\"n\":\"2.3.2\"},\"end\":17364,\"start\":17345},{\"attributes\":{\"n\":\"3\"},\"end\":19315,\"start\":19296},{\"attributes\":{\"n\":\"4\"},\"end\":22640,\"start\":22614},{\"attributes\":{\"n\":\"4.1\"},\"end\":24342,\"start\":24312},{\"attributes\":{\"n\":\"4.2\"},\"end\":29880,\"start\":29861},{\"attributes\":{\"n\":\"4.2.1\"},\"end\":30137,\"start\":30118},{\"attributes\":{\"n\":\"4.2.2\"},\"end\":32117,\"start\":32107},{\"attributes\":{\"n\":\"5\"},\"end\":35029,\"start\":35000},{\"attributes\":{\"n\":\"5.1\"},\"end\":35271,\"start\":35238},{\"attributes\":{\"n\":\"5.2\"},\"end\":36321,\"start\":36299},{\"attributes\":{\"n\":\"5.2.1\"},\"end\":38795,\"start\":38777},{\"attributes\":{\"n\":\"6\"},\"end\":40523,\"start\":40504},{\"attributes\":{\"n\":\"6.1\"},\"end\":40869,\"start\":40833},{\"end\":42346,\"start\":42326},{\"attributes\":{\"n\":\"6.2\"},\"end\":44594,\"start\":44567},{\"attributes\":{\"n\":\"6.2.1\"},\"end\":49012,\"start\":48975},{\"attributes\":{\"n\":\"6.2.2\"},\"end\":49040,\"start\":49015},{\"attributes\":{\"n\":\"6.2.3\"},\"end\":49068,\"start\":49043},{\"attributes\":{\"n\":\"6.3\"},\"end\":49782,\"start\":49762},{\"attributes\":{\"n\":\"6.3.1\"},\"end\":50041,\"start\":50024},{\"attributes\":{\"n\":\"6.3.2\"},\"end\":50950,\"start\":50923},{\"attributes\":{\"n\":\"6.3.3\"},\"end\":53988,\"start\":53962},{\"attributes\":{\"n\":\"6.3.4\"},\"end\":54013,\"start\":53991},{\"attributes\":{\"n\":\"6.4\"},\"end\":55347,\"start\":55314},{\"attributes\":{\"n\":\"7\"},\"end\":56888,\"start\":56870},{\"end\":57868,\"start\":57844},{\"end\":60122,\"start\":60100},{\"end\":60856,\"start\":60838},{\"end\":61853,\"start\":61817},{\"end\":61883,\"start\":61856},{\"end\":61917,\"start\":61886},{\"end\":62058,\"start\":62048},{\"end\":62137,\"start\":62127},{\"end\":62266,\"start\":62256},{\"end\":62329,\"start\":62319},{\"end\":62363,\"start\":62353},{\"end\":62416,\"start\":62406},{\"end\":62455,\"start\":62454},{\"end\":62500,\"start\":62490},{\"end\":62625,\"start\":62615},{\"end\":62797,\"start\":62787},{\"end\":62853,\"start\":62844},{\"end\":62985,\"start\":62974},{\"end\":63255,\"start\":63244},{\"end\":63314,\"start\":63303},{\"end\":63358,\"start\":63347},{\"end\":63558,\"start\":63547},{\"end\":63620,\"start\":63609},{\"end\":63770,\"start\":63759},{\"end\":63834,\"start\":63823},{\"end\":63897,\"start\":63888},{\"end\":63972,\"start\":63963},{\"end\":64034,\"start\":64025},{\"end\":65007,\"start\":64998},{\"end\":67634,\"start\":67625},{\"end\":67683,\"start\":67674},{\"end\":67753,\"start\":67744},{\"end\":67811,\"start\":67804},{\"end\":68292,\"start\":68283},{\"end\":68357,\"start\":68348}]", "table": "[{\"end\":64113,\"start\":64045},{\"end\":64996,\"start\":64229},{\"end\":67623,\"start\":66136},{\"end\":68281,\"start\":67925}]", "figure_caption": "[{\"end\":62125,\"start\":62060},{\"end\":62195,\"start\":62139},{\"end\":62254,\"start\":62198},{\"end\":62317,\"start\":62268},{\"end\":62351,\"start\":62331},{\"end\":62404,\"start\":62365},{\"end\":62452,\"start\":62418},{\"end\":62488,\"start\":62456},{\"end\":62613,\"start\":62502},{\"end\":62725,\"start\":62627},{\"end\":62785,\"start\":62728},{\"end\":62842,\"start\":62799},{\"end\":62972,\"start\":62856},{\"end\":63242,\"start\":62988},{\"end\":63301,\"start\":63258},{\"end\":63345,\"start\":63317},{\"end\":63545,\"start\":63361},{\"end\":63607,\"start\":63561},{\"end\":63757,\"start\":63623},{\"end\":63821,\"start\":63773},{\"end\":63886,\"start\":63837},{\"end\":63961,\"start\":63899},{\"end\":64023,\"start\":63974},{\"end\":64045,\"start\":64036},{\"end\":64229,\"start\":64116},{\"end\":65027,\"start\":65009},{\"end\":66136,\"start\":65030},{\"end\":67672,\"start\":67636},{\"end\":67742,\"start\":67685},{\"end\":67802,\"start\":67755},{\"end\":67925,\"start\":67813},{\"end\":68346,\"start\":68294},{\"end\":68406,\"start\":68359}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6860,\"start\":6852},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":9152,\"start\":9144},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":16240,\"start\":16231},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":19109,\"start\":19100},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":19955,\"start\":19946},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":20466,\"start\":20458},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":21448,\"start\":21439},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":21934,\"start\":21925},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":22809,\"start\":22801},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":37762,\"start\":37754},{\"end\":38090,\"start\":38083},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":40901,\"start\":40893},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":41828,\"start\":41818},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":43794,\"start\":43786},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":44425,\"start\":44415},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":44460,\"start\":44450},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":45487,\"start\":45478},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":45629,\"start\":45620},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":46559,\"start\":46549},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":47254,\"start\":47245},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":48052,\"start\":48042},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":48596,\"start\":48587},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":50206,\"start\":50197},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":50753,\"start\":50744},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":51269,\"start\":51259},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":53001,\"start\":52992},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":53119,\"start\":53109},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":53723,\"start\":53713},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":54317,\"start\":54307},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":54674,\"start\":54664},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":55582,\"start\":55572},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":55989,\"start\":55979},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":56769,\"start\":56759}]", "bib_author_first_name": "[{\"end\":69134,\"start\":69128},{\"end\":69334,\"start\":69328},{\"end\":69346,\"start\":69344},{\"end\":69361,\"start\":69353},{\"end\":69377,\"start\":69372},{\"end\":69388,\"start\":69383},{\"end\":69889,\"start\":69884},{\"end\":69909,\"start\":69901},{\"end\":69921,\"start\":69918},{\"end\":69935,\"start\":69928},{\"end\":69952,\"start\":69944},{\"end\":70457,\"start\":70451},{\"end\":70576,\"start\":70570},{\"end\":71241,\"start\":71234},{\"end\":71272,\"start\":71266},{\"end\":71791,\"start\":71785},{\"end\":71799,\"start\":71792},{\"end\":71814,\"start\":71810},{\"end\":71831,\"start\":71827},{\"end\":72425,\"start\":72421},{\"end\":72436,\"start\":72433},{\"end\":72679,\"start\":72678},{\"end\":72692,\"start\":72689},{\"end\":72708,\"start\":72701},{\"end\":72722,\"start\":72719},{\"end\":72724,\"start\":72723},{\"end\":73307,\"start\":73303},{\"end\":73326,\"start\":73317},{\"end\":73349,\"start\":73344},{\"end\":73360,\"start\":73356},{\"end\":73772,\"start\":73767},{\"end\":73798,\"start\":73784},{\"end\":74109,\"start\":74103},{\"end\":74140,\"start\":74130},{\"end\":74157,\"start\":74150},{\"end\":74685,\"start\":74677},{\"end\":74697,\"start\":74692},{\"end\":74712,\"start\":74707},{\"end\":75043,\"start\":75035},{\"end\":75197,\"start\":75189},{\"end\":75541,\"start\":75535},{\"end\":75556,\"start\":75549},{\"end\":75570,\"start\":75563},{\"end\":75572,\"start\":75571},{\"end\":75773,\"start\":75769},{\"end\":75788,\"start\":75785},{\"end\":75800,\"start\":75796},{\"end\":76211,\"start\":76205},{\"end\":76228,\"start\":76223},{\"end\":76243,\"start\":76237},{\"end\":76245,\"start\":76244},{\"end\":76263,\"start\":76256},{\"end\":76272,\"start\":76268},{\"end\":76281,\"start\":76280},{\"end\":76294,\"start\":76291},{\"end\":76321,\"start\":76305},{\"end\":76694,\"start\":76688},{\"end\":76721,\"start\":76716},{\"end\":76735,\"start\":76728},{\"end\":76749,\"start\":76743},{\"end\":76751,\"start\":76750},{\"end\":76759,\"start\":76756},{\"end\":76776,\"start\":76770},{\"end\":77070,\"start\":77064},{\"end\":77085,\"start\":77082},{\"end\":77098,\"start\":77092},{\"end\":77112,\"start\":77105},{\"end\":77114,\"start\":77113},{\"end\":77131,\"start\":77125},{\"end\":77133,\"start\":77132},{\"end\":77147,\"start\":77144},{\"end\":77727,\"start\":77726},{\"end\":77735,\"start\":77734},{\"end\":77743,\"start\":77742},{\"end\":77753,\"start\":77752},{\"end\":78098,\"start\":78093},{\"end\":78115,\"start\":78107},{\"end\":78129,\"start\":78123},{\"end\":78143,\"start\":78135},{\"end\":78383,\"start\":78379},{\"end\":78399,\"start\":78394},{\"end\":78418,\"start\":78409},{\"end\":78432,\"start\":78428},{\"end\":78445,\"start\":78440},{\"end\":78456,\"start\":78454},{\"end\":78470,\"start\":78461},{\"end\":78663,\"start\":78655},{\"end\":78674,\"start\":78670},{\"end\":78684,\"start\":78680},{\"end\":78694,\"start\":78691},{\"end\":78710,\"start\":78701},{\"end\":78721,\"start\":78717},{\"end\":78734,\"start\":78728},{\"end\":78745,\"start\":78741},{\"end\":78755,\"start\":78751},{\"end\":78766,\"start\":78761},{\"end\":79208,\"start\":79203},{\"end\":79223,\"start\":79216},{\"end\":79241,\"start\":79235},{\"end\":79263,\"start\":79255},{\"end\":79283,\"start\":79274},{\"end\":79808,\"start\":79803},{\"end\":79824,\"start\":79819},{\"end\":79842,\"start\":79835},{\"end\":79854,\"start\":79847},{\"end\":79856,\"start\":79855},{\"end\":79872,\"start\":79867},{\"end\":79874,\"start\":79873},{\"end\":80240,\"start\":80235},{\"end\":80255,\"start\":80251},{\"end\":80269,\"start\":80264},{\"end\":80286,\"start\":80279},{\"end\":80301,\"start\":80297},{\"end\":80321,\"start\":80313},{\"end\":80816,\"start\":80807},{\"end\":80856,\"start\":80855},{\"end\":80886,\"start\":80881},{\"end\":80888,\"start\":80887},{\"end\":81277,\"start\":81268},{\"end\":81322,\"start\":81316},{\"end\":81353,\"start\":81347},{\"end\":81370,\"start\":81365},{\"end\":81740,\"start\":81731},{\"end\":81771,\"start\":81767},{\"end\":81804,\"start\":81799},{\"end\":81806,\"start\":81805},{\"end\":82134,\"start\":82133},{\"end\":82143,\"start\":82142},{\"end\":82151,\"start\":82150},{\"end\":82161,\"start\":82160},{\"end\":82169,\"start\":82168},{\"end\":82179,\"start\":82178},{\"end\":82186,\"start\":82185},{\"end\":82188,\"start\":82187},{\"end\":82195,\"start\":82194},{\"end\":82205,\"start\":82204},{\"end\":82666,\"start\":82663},{\"end\":82673,\"start\":82672},{\"end\":82686,\"start\":82684},{\"end\":82699,\"start\":82695},{\"end\":82713,\"start\":82705},{\"end\":83028,\"start\":83023},{\"end\":83043,\"start\":83037},{\"end\":83057,\"start\":83051},{\"end\":83074,\"start\":83067},{\"end\":83076,\"start\":83075},{\"end\":83092,\"start\":83085},{\"end\":83094,\"start\":83093},{\"end\":83368,\"start\":83363},{\"end\":83383,\"start\":83377},{\"end\":83399,\"start\":83393},{\"end\":83414,\"start\":83407},{\"end\":83416,\"start\":83415},{\"end\":83432,\"start\":83425},{\"end\":83434,\"start\":83433},{\"end\":83772,\"start\":83766},{\"end\":83790,\"start\":83783},{\"end\":83792,\"start\":83791},{\"end\":83810,\"start\":83805},{\"end\":83823,\"start\":83818},{\"end\":83834,\"start\":83828},{\"end\":83850,\"start\":83845},{\"end\":83866,\"start\":83860},{\"end\":83868,\"start\":83867},{\"end\":83887,\"start\":83881},{\"end\":83903,\"start\":83895},{\"end\":83921,\"start\":83914},{\"end\":83933,\"start\":83928},{\"end\":84348,\"start\":84347},{\"end\":84361,\"start\":84360},{\"end\":84369,\"start\":84368},{\"end\":84379,\"start\":84378},{\"end\":84391,\"start\":84390},{\"end\":84400,\"start\":84399},{\"end\":84414,\"start\":84413},{\"end\":84423,\"start\":84422},{\"end\":84430,\"start\":84429},{\"end\":84437,\"start\":84436},{\"end\":84446,\"start\":84445},{\"end\":84453,\"start\":84452},{\"end\":84460,\"start\":84459},{\"end\":84466,\"start\":84465},{\"end\":84479,\"start\":84478},{\"end\":84494,\"start\":84493},{\"end\":84503,\"start\":84502},{\"end\":85003,\"start\":84997},{\"end\":85016,\"start\":85012},{\"end\":85031,\"start\":85026},{\"end\":85048,\"start\":85037},{\"end\":85057,\"start\":85055},{\"end\":85072,\"start\":85064},{\"end\":85084,\"start\":85078},{\"end\":85096,\"start\":85091},{\"end\":85109,\"start\":85102},{\"end\":85121,\"start\":85116},{\"end\":85591,\"start\":85584},{\"end\":85605,\"start\":85598},{\"end\":85621,\"start\":85613},{\"end\":85980,\"start\":85974},{\"end\":85998,\"start\":85992},{\"end\":86496,\"start\":86491},{\"end\":86511,\"start\":86504},{\"end\":86763,\"start\":86759},{\"end\":86778,\"start\":86772},{\"end\":86780,\"start\":86779},{\"end\":87053,\"start\":87049},{\"end\":87071,\"start\":87066},{\"end\":87086,\"start\":87078},{\"end\":87340,\"start\":87331},{\"end\":87352,\"start\":87346},{\"end\":87368,\"start\":87359},{\"end\":87383,\"start\":87378},{\"end\":87398,\"start\":87392},{\"end\":87411,\"start\":87407},{\"end\":87752,\"start\":87744},{\"end\":87765,\"start\":87758},{\"end\":87784,\"start\":87775},{\"end\":87796,\"start\":87791},{\"end\":87805,\"start\":87797},{\"end\":87826,\"start\":87820},{\"end\":87841,\"start\":87835},{\"end\":88368,\"start\":88362},{\"end\":88384,\"start\":88379},{\"end\":88402,\"start\":88395},{\"end\":88405,\"start\":88403},{\"end\":88791,\"start\":88786},{\"end\":88803,\"start\":88797},{\"end\":88817,\"start\":88810},{\"end\":88830,\"start\":88823},{\"end\":88843,\"start\":88837},{\"end\":88853,\"start\":88849},{\"end\":88855,\"start\":88854},{\"end\":89077,\"start\":89071},{\"end\":89087,\"start\":89083},{\"end\":89098,\"start\":89093},{\"end\":89113,\"start\":89106},{\"end\":89124,\"start\":89118},{\"end\":89137,\"start\":89132},{\"end\":89148,\"start\":89144},{\"end\":89159,\"start\":89155},{\"end\":89171,\"start\":89167},{\"end\":89192,\"start\":89185},{\"end\":89586,\"start\":89580},{\"end\":89598,\"start\":89591},{\"end\":89611,\"start\":89603},{\"end\":89621,\"start\":89617},{\"end\":90308,\"start\":90304},{\"end\":90351,\"start\":90338},{\"end\":90834,\"start\":90827},{\"end\":90848,\"start\":90841},{\"end\":90860,\"start\":90855},{\"end\":90871,\"start\":90867},{\"end\":90885,\"start\":90879},{\"end\":90900,\"start\":90896},{\"end\":90914,\"start\":90908},{\"end\":90926,\"start\":90919},{\"end\":90943,\"start\":90939},{\"end\":91472,\"start\":91466},{\"end\":91489,\"start\":91484},{\"end\":91502,\"start\":91498},{\"end\":91521,\"start\":91516},{\"end\":91533,\"start\":91532},{\"end\":91552,\"start\":91551},{\"end\":91571,\"start\":91570},{\"end\":91586,\"start\":91581},{\"end\":92095,\"start\":92084},{\"end\":92108,\"start\":92104},{\"end\":92122,\"start\":92117},{\"end\":92140,\"start\":92132},{\"end\":92152,\"start\":92150},{\"end\":92165,\"start\":92158},{\"end\":92174,\"start\":92170},{\"end\":92195,\"start\":92188},{\"end\":92210,\"start\":92204},{\"end\":92515,\"start\":92514},{\"end\":92825,\"start\":92821},{\"end\":92837,\"start\":92834},{\"end\":92854,\"start\":92845},{\"end\":92866,\"start\":92862},{\"end\":92879,\"start\":92874},{\"end\":92897,\"start\":92890},{\"end\":92912,\"start\":92906},{\"end\":92928,\"start\":92922},{\"end\":92941,\"start\":92934},{\"end\":92958,\"start\":92954},{\"end\":93424,\"start\":93418},{\"end\":93434,\"start\":93430},{\"end\":93447,\"start\":93442},{\"end\":93457,\"start\":93454},{\"end\":93472,\"start\":93464},{\"end\":93483,\"start\":93479},{\"end\":94072,\"start\":94065},{\"end\":94080,\"start\":94078},{\"end\":94090,\"start\":94088},{\"end\":94097,\"start\":94096},{\"end\":94116,\"start\":94111},{\"end\":94571,\"start\":94567},{\"end\":94587,\"start\":94580},{\"end\":94600,\"start\":94593},{\"end\":94985,\"start\":94981},{\"end\":95002,\"start\":94997},{\"end\":95017,\"start\":95010},{\"end\":95573,\"start\":95567},{\"end\":95588,\"start\":95580},{\"end\":95602,\"start\":95596},{\"end\":95619,\"start\":95613},{\"end\":95911,\"start\":95904},{\"end\":95925,\"start\":95920},{\"end\":95941,\"start\":95933},{\"end\":96231,\"start\":96224},{\"end\":96247,\"start\":96240},{\"end\":96257,\"start\":96253},{\"end\":96268,\"start\":96263},{\"end\":96284,\"start\":96276},{\"end\":96696,\"start\":96695},{\"end\":96709,\"start\":96705},{\"end\":96721,\"start\":96717},{\"end\":96729,\"start\":96722},{\"end\":96749,\"start\":96740},{\"end\":96769,\"start\":96762},{\"end\":96774,\"start\":96770},{\"end\":97036,\"start\":97029},{\"end\":97051,\"start\":97045},{\"end\":97065,\"start\":97060},{\"end\":97357,\"start\":97350},{\"end\":97370,\"start\":97366},{\"end\":97386,\"start\":97382},{\"end\":97396,\"start\":97391},{\"end\":97406,\"start\":97405},{\"end\":97430,\"start\":97429},{\"end\":97450,\"start\":97439},{\"end\":97937,\"start\":97929},{\"end\":97944,\"start\":97943},{\"end\":98271,\"start\":98270},{\"end\":98285,\"start\":98284},{\"end\":98287,\"start\":98286},{\"end\":98302,\"start\":98301},{\"end\":98312,\"start\":98311},{\"end\":98314,\"start\":98313},{\"end\":98326,\"start\":98325},{\"end\":98328,\"start\":98327},{\"end\":98759,\"start\":98758},{\"end\":98773,\"start\":98772},{\"end\":98775,\"start\":98774},{\"end\":98790,\"start\":98789},{\"end\":98800,\"start\":98799},{\"end\":98802,\"start\":98801},{\"end\":98814,\"start\":98813},{\"end\":98816,\"start\":98815},{\"end\":99084,\"start\":99079},{\"end\":99104,\"start\":99095},{\"end\":99120,\"start\":99113},{\"end\":99353,\"start\":99344},{\"end\":99374,\"start\":99366},{\"end\":99660,\"start\":99659},{\"end\":100005,\"start\":100000},{\"end\":100018,\"start\":100012},{\"end\":100034,\"start\":100030},{\"end\":100052,\"start\":100046},{\"end\":100278,\"start\":100275},{\"end\":100292,\"start\":100285},{\"end\":100304,\"start\":100298},{\"end\":100317,\"start\":100312},{\"end\":100328,\"start\":100324},{\"end\":100344,\"start\":100335},{\"end\":100353,\"start\":100349},{\"end\":100358,\"start\":100354},{\"end\":100367,\"start\":100364},{\"end\":100378,\"start\":100374},{\"end\":100749,\"start\":100743},{\"end\":100764,\"start\":100756},{\"end\":100778,\"start\":100772},{\"end\":100791,\"start\":100785},{\"end\":100809,\"start\":100802},{\"end\":100827,\"start\":100820},{\"end\":100840,\"start\":100833},{\"end\":100852,\"start\":100849},{\"end\":100864,\"start\":100860},{\"end\":100877,\"start\":100871},{\"end\":101265,\"start\":101254},{\"end\":101275,\"start\":101270},{\"end\":101289,\"start\":101284},{\"end\":101303,\"start\":101296},{\"end\":101312,\"start\":101310},{\"end\":101329,\"start\":101324},{\"end\":101341,\"start\":101338},{\"end\":101358,\"start\":101353},{\"end\":101374,\"start\":101366},{\"end\":101384,\"start\":101380},{\"end\":101859,\"start\":101858},{\"end\":101878,\"start\":101869},{\"end\":101894,\"start\":101890},{\"end\":101911,\"start\":101903},{\"end\":102429,\"start\":102422},{\"end\":102442,\"start\":102436},{\"end\":102444,\"start\":102443},{\"end\":102455,\"start\":102453},{\"end\":102466,\"start\":102462},{\"end\":102478,\"start\":102474},{\"end\":102491,\"start\":102483},{\"end\":102504,\"start\":102497},{\"end\":102734,\"start\":102728},{\"end\":102747,\"start\":102741},{\"end\":102759,\"start\":102753},{\"end\":102771,\"start\":102766},{\"end\":102789,\"start\":102783},{\"end\":102811,\"start\":102805},{\"end\":103236,\"start\":103226},{\"end\":103251,\"start\":103244},{\"end\":103259,\"start\":103256},{\"end\":103270,\"start\":103266},{\"end\":103503,\"start\":103496},{\"end\":103515,\"start\":103512},{\"end\":103527,\"start\":103522},{\"end\":103832,\"start\":103826},{\"end\":103844,\"start\":103843}]", "bib_author_last_name": "[{\"end\":69140,\"start\":69135},{\"end\":69342,\"start\":69335},{\"end\":69351,\"start\":69347},{\"end\":69370,\"start\":69362},{\"end\":69381,\"start\":69378},{\"end\":69394,\"start\":69389},{\"end\":69899,\"start\":69890},{\"end\":69916,\"start\":69910},{\"end\":69926,\"start\":69922},{\"end\":69942,\"start\":69936},{\"end\":69959,\"start\":69953},{\"end\":70464,\"start\":70458},{\"end\":70584,\"start\":70577},{\"end\":70653,\"start\":70647},{\"end\":70664,\"start\":70655},{\"end\":70772,\"start\":70766},{\"end\":70837,\"start\":70831},{\"end\":70857,\"start\":70839},{\"end\":70862,\"start\":70859},{\"end\":71264,\"start\":71242},{\"end\":71276,\"start\":71273},{\"end\":71287,\"start\":71278},{\"end\":71808,\"start\":71800},{\"end\":71825,\"start\":71815},{\"end\":71837,\"start\":71832},{\"end\":72431,\"start\":72426},{\"end\":72443,\"start\":72437},{\"end\":72687,\"start\":72680},{\"end\":72699,\"start\":72693},{\"end\":72717,\"start\":72709},{\"end\":72735,\"start\":72725},{\"end\":72743,\"start\":72737},{\"end\":73165,\"start\":73149},{\"end\":73315,\"start\":73308},{\"end\":73342,\"start\":73327},{\"end\":73354,\"start\":73350},{\"end\":73367,\"start\":73361},{\"end\":73782,\"start\":73773},{\"end\":73807,\"start\":73799},{\"end\":74128,\"start\":74110},{\"end\":74148,\"start\":74141},{\"end\":74164,\"start\":74158},{\"end\":74174,\"start\":74166},{\"end\":74690,\"start\":74686},{\"end\":74705,\"start\":74698},{\"end\":74716,\"start\":74713},{\"end\":75052,\"start\":75044},{\"end\":75205,\"start\":75198},{\"end\":75547,\"start\":75542},{\"end\":75561,\"start\":75557},{\"end\":75579,\"start\":75573},{\"end\":75783,\"start\":75774},{\"end\":75794,\"start\":75789},{\"end\":75807,\"start\":75801},{\"end\":76221,\"start\":76212},{\"end\":76235,\"start\":76229},{\"end\":76254,\"start\":76246},{\"end\":76266,\"start\":76264},{\"end\":76278,\"start\":76273},{\"end\":76289,\"start\":76282},{\"end\":76303,\"start\":76295},{\"end\":76328,\"start\":76322},{\"end\":76704,\"start\":76695},{\"end\":76714,\"start\":76706},{\"end\":76726,\"start\":76722},{\"end\":76741,\"start\":76736},{\"end\":76754,\"start\":76752},{\"end\":76768,\"start\":76760},{\"end\":76783,\"start\":76777},{\"end\":76792,\"start\":76785},{\"end\":77080,\"start\":77071},{\"end\":77090,\"start\":77086},{\"end\":77103,\"start\":77099},{\"end\":77123,\"start\":77115},{\"end\":77142,\"start\":77134},{\"end\":77154,\"start\":77148},{\"end\":77598,\"start\":77588},{\"end\":77732,\"start\":77728},{\"end\":77740,\"start\":77736},{\"end\":77750,\"start\":77744},{\"end\":77756,\"start\":77754},{\"end\":77766,\"start\":77758},{\"end\":78105,\"start\":78099},{\"end\":78121,\"start\":78116},{\"end\":78133,\"start\":78130},{\"end\":78153,\"start\":78144},{\"end\":78392,\"start\":78384},{\"end\":78407,\"start\":78400},{\"end\":78426,\"start\":78419},{\"end\":78438,\"start\":78433},{\"end\":78452,\"start\":78446},{\"end\":78459,\"start\":78457},{\"end\":78476,\"start\":78471},{\"end\":78668,\"start\":78664},{\"end\":78678,\"start\":78675},{\"end\":78689,\"start\":78685},{\"end\":78699,\"start\":78695},{\"end\":78715,\"start\":78711},{\"end\":78726,\"start\":78722},{\"end\":78739,\"start\":78735},{\"end\":78749,\"start\":78746},{\"end\":78759,\"start\":78756},{\"end\":78772,\"start\":78767},{\"end\":79214,\"start\":79209},{\"end\":79233,\"start\":79224},{\"end\":79253,\"start\":79242},{\"end\":79272,\"start\":79264},{\"end\":79291,\"start\":79284},{\"end\":79817,\"start\":79809},{\"end\":79833,\"start\":79825},{\"end\":79845,\"start\":79843},{\"end\":79865,\"start\":79857},{\"end\":79886,\"start\":79875},{\"end\":80249,\"start\":80241},{\"end\":80262,\"start\":80256},{\"end\":80277,\"start\":80270},{\"end\":80295,\"start\":80287},{\"end\":80311,\"start\":80302},{\"end\":80326,\"start\":80322},{\"end\":80841,\"start\":80817},{\"end\":80853,\"start\":80843},{\"end\":80867,\"start\":80857},{\"end\":80879,\"start\":80869},{\"end\":80911,\"start\":80889},{\"end\":80916,\"start\":80913},{\"end\":81302,\"start\":81278},{\"end\":81314,\"start\":81304},{\"end\":81345,\"start\":81323},{\"end\":81363,\"start\":81354},{\"end\":81378,\"start\":81371},{\"end\":81383,\"start\":81380},{\"end\":81765,\"start\":81741},{\"end\":81782,\"start\":81772},{\"end\":81797,\"start\":81784},{\"end\":81829,\"start\":81807},{\"end\":81834,\"start\":81831},{\"end\":82140,\"start\":82135},{\"end\":82148,\"start\":82144},{\"end\":82158,\"start\":82152},{\"end\":82166,\"start\":82162},{\"end\":82176,\"start\":82170},{\"end\":82183,\"start\":82180},{\"end\":82192,\"start\":82189},{\"end\":82202,\"start\":82196},{\"end\":82208,\"start\":82206},{\"end\":82670,\"start\":82667},{\"end\":82682,\"start\":82674},{\"end\":82693,\"start\":82687},{\"end\":82703,\"start\":82700},{\"end\":82717,\"start\":82714},{\"end\":82725,\"start\":82719},{\"end\":83035,\"start\":83029},{\"end\":83049,\"start\":83044},{\"end\":83065,\"start\":83058},{\"end\":83083,\"start\":83077},{\"end\":83102,\"start\":83095},{\"end\":83375,\"start\":83369},{\"end\":83391,\"start\":83384},{\"end\":83405,\"start\":83400},{\"end\":83423,\"start\":83417},{\"end\":83442,\"start\":83435},{\"end\":83781,\"start\":83773},{\"end\":83803,\"start\":83793},{\"end\":83816,\"start\":83811},{\"end\":83826,\"start\":83824},{\"end\":83843,\"start\":83835},{\"end\":83858,\"start\":83851},{\"end\":83879,\"start\":83869},{\"end\":83893,\"start\":83888},{\"end\":83912,\"start\":83904},{\"end\":83926,\"start\":83922},{\"end\":83938,\"start\":83934},{\"end\":84358,\"start\":84349},{\"end\":84366,\"start\":84362},{\"end\":84376,\"start\":84370},{\"end\":84388,\"start\":84380},{\"end\":84397,\"start\":84392},{\"end\":84411,\"start\":84401},{\"end\":84420,\"start\":84415},{\"end\":84427,\"start\":84424},{\"end\":84434,\"start\":84431},{\"end\":84443,\"start\":84438},{\"end\":84450,\"start\":84447},{\"end\":84457,\"start\":84454},{\"end\":84463,\"start\":84461},{\"end\":84476,\"start\":84467},{\"end\":84491,\"start\":84480},{\"end\":84500,\"start\":84495},{\"end\":84508,\"start\":84504},{\"end\":85010,\"start\":85004},{\"end\":85024,\"start\":85017},{\"end\":85035,\"start\":85032},{\"end\":85053,\"start\":85049},{\"end\":85062,\"start\":85058},{\"end\":85076,\"start\":85073},{\"end\":85089,\"start\":85085},{\"end\":85100,\"start\":85097},{\"end\":85114,\"start\":85110},{\"end\":85131,\"start\":85122},{\"end\":85596,\"start\":85592},{\"end\":85611,\"start\":85606},{\"end\":85644,\"start\":85622},{\"end\":85650,\"start\":85646},{\"end\":85990,\"start\":85981},{\"end\":86015,\"start\":85999},{\"end\":86025,\"start\":86017},{\"end\":86502,\"start\":86497},{\"end\":86518,\"start\":86512},{\"end\":86522,\"start\":86520},{\"end\":86770,\"start\":86764},{\"end\":86787,\"start\":86781},{\"end\":87064,\"start\":87054},{\"end\":87076,\"start\":87072},{\"end\":87093,\"start\":87087},{\"end\":87344,\"start\":87341},{\"end\":87357,\"start\":87353},{\"end\":87376,\"start\":87369},{\"end\":87390,\"start\":87384},{\"end\":87405,\"start\":87399},{\"end\":87419,\"start\":87412},{\"end\":87756,\"start\":87753},{\"end\":87773,\"start\":87766},{\"end\":87789,\"start\":87785},{\"end\":87818,\"start\":87806},{\"end\":87833,\"start\":87827},{\"end\":87852,\"start\":87842},{\"end\":88377,\"start\":88369},{\"end\":88393,\"start\":88385},{\"end\":88410,\"start\":88406},{\"end\":88795,\"start\":88792},{\"end\":88808,\"start\":88804},{\"end\":88821,\"start\":88818},{\"end\":88835,\"start\":88831},{\"end\":88847,\"start\":88844},{\"end\":88863,\"start\":88856},{\"end\":89081,\"start\":89078},{\"end\":89091,\"start\":89088},{\"end\":89104,\"start\":89099},{\"end\":89116,\"start\":89114},{\"end\":89130,\"start\":89125},{\"end\":89142,\"start\":89138},{\"end\":89153,\"start\":89149},{\"end\":89165,\"start\":89160},{\"end\":89183,\"start\":89172},{\"end\":89201,\"start\":89193},{\"end\":89210,\"start\":89203},{\"end\":89589,\"start\":89587},{\"end\":89601,\"start\":89599},{\"end\":89615,\"start\":89612},{\"end\":89629,\"start\":89622},{\"end\":90327,\"start\":90309},{\"end\":90336,\"start\":90329},{\"end\":90374,\"start\":90352},{\"end\":90385,\"start\":90376},{\"end\":90839,\"start\":90835},{\"end\":90853,\"start\":90849},{\"end\":90865,\"start\":90861},{\"end\":90877,\"start\":90872},{\"end\":90894,\"start\":90886},{\"end\":90906,\"start\":90901},{\"end\":90917,\"start\":90915},{\"end\":90937,\"start\":90927},{\"end\":90949,\"start\":90944},{\"end\":91482,\"start\":91473},{\"end\":91496,\"start\":91490},{\"end\":91514,\"start\":91503},{\"end\":91530,\"start\":91522},{\"end\":91540,\"start\":91534},{\"end\":91549,\"start\":91542},{\"end\":91560,\"start\":91553},{\"end\":91568,\"start\":91562},{\"end\":91579,\"start\":91572},{\"end\":91594,\"start\":91587},{\"end\":91603,\"start\":91596},{\"end\":92102,\"start\":92096},{\"end\":92115,\"start\":92109},{\"end\":92130,\"start\":92123},{\"end\":92148,\"start\":92141},{\"end\":92156,\"start\":92153},{\"end\":92168,\"start\":92166},{\"end\":92186,\"start\":92175},{\"end\":92202,\"start\":92196},{\"end\":92216,\"start\":92211},{\"end\":92522,\"start\":92516},{\"end\":92527,\"start\":92524},{\"end\":92832,\"start\":92826},{\"end\":92843,\"start\":92838},{\"end\":92860,\"start\":92855},{\"end\":92872,\"start\":92867},{\"end\":92888,\"start\":92880},{\"end\":92904,\"start\":92898},{\"end\":92920,\"start\":92913},{\"end\":92932,\"start\":92929},{\"end\":92952,\"start\":92942},{\"end\":92965,\"start\":92959},{\"end\":93428,\"start\":93425},{\"end\":93440,\"start\":93435},{\"end\":93452,\"start\":93448},{\"end\":93462,\"start\":93458},{\"end\":93477,\"start\":93473},{\"end\":93487,\"start\":93484},{\"end\":94076,\"start\":94073},{\"end\":94086,\"start\":94081},{\"end\":94094,\"start\":94091},{\"end\":94109,\"start\":94098},{\"end\":94126,\"start\":94117},{\"end\":94138,\"start\":94128},{\"end\":94578,\"start\":94572},{\"end\":94591,\"start\":94588},{\"end\":94605,\"start\":94601},{\"end\":94995,\"start\":94986},{\"end\":95008,\"start\":95003},{\"end\":95023,\"start\":95018},{\"end\":95578,\"start\":95574},{\"end\":95594,\"start\":95589},{\"end\":95611,\"start\":95603},{\"end\":95624,\"start\":95620},{\"end\":95918,\"start\":95912},{\"end\":95931,\"start\":95926},{\"end\":95948,\"start\":95942},{\"end\":96238,\"start\":96232},{\"end\":96251,\"start\":96248},{\"end\":96261,\"start\":96258},{\"end\":96274,\"start\":96269},{\"end\":96291,\"start\":96285},{\"end\":96703,\"start\":96697},{\"end\":96715,\"start\":96710},{\"end\":96738,\"start\":96730},{\"end\":96760,\"start\":96750},{\"end\":96782,\"start\":96775},{\"end\":96789,\"start\":96784},{\"end\":97043,\"start\":97037},{\"end\":97058,\"start\":97052},{\"end\":97073,\"start\":97066},{\"end\":97364,\"start\":97358},{\"end\":97380,\"start\":97371},{\"end\":97389,\"start\":97387},{\"end\":97403,\"start\":97397},{\"end\":97418,\"start\":97407},{\"end\":97427,\"start\":97420},{\"end\":97437,\"start\":97431},{\"end\":97453,\"start\":97451},{\"end\":97460,\"start\":97455},{\"end\":97941,\"start\":97938},{\"end\":97949,\"start\":97945},{\"end\":97953,\"start\":97951},{\"end\":97967,\"start\":97955},{\"end\":98282,\"start\":98272},{\"end\":98299,\"start\":98288},{\"end\":98309,\"start\":98303},{\"end\":98323,\"start\":98315},{\"end\":98332,\"start\":98329},{\"end\":98770,\"start\":98760},{\"end\":98787,\"start\":98776},{\"end\":98797,\"start\":98791},{\"end\":98811,\"start\":98803},{\"end\":98820,\"start\":98817},{\"end\":99093,\"start\":99085},{\"end\":99111,\"start\":99105},{\"end\":99130,\"start\":99121},{\"end\":99364,\"start\":99354},{\"end\":99385,\"start\":99375},{\"end\":99667,\"start\":99661},{\"end\":99674,\"start\":99669},{\"end\":99692,\"start\":99676},{\"end\":100010,\"start\":100006},{\"end\":100028,\"start\":100019},{\"end\":100044,\"start\":100035},{\"end\":100060,\"start\":100053},{\"end\":100283,\"start\":100279},{\"end\":100296,\"start\":100293},{\"end\":100310,\"start\":100305},{\"end\":100322,\"start\":100318},{\"end\":100333,\"start\":100329},{\"end\":100347,\"start\":100345},{\"end\":100362,\"start\":100359},{\"end\":100372,\"start\":100368},{\"end\":100384,\"start\":100379},{\"end\":100754,\"start\":100750},{\"end\":100770,\"start\":100765},{\"end\":100783,\"start\":100779},{\"end\":100800,\"start\":100792},{\"end\":100818,\"start\":100810},{\"end\":100831,\"start\":100828},{\"end\":100847,\"start\":100841},{\"end\":100858,\"start\":100853},{\"end\":100869,\"start\":100865},{\"end\":100887,\"start\":100878},{\"end\":101268,\"start\":101266},{\"end\":101282,\"start\":101276},{\"end\":101294,\"start\":101290},{\"end\":101308,\"start\":101304},{\"end\":101322,\"start\":101313},{\"end\":101336,\"start\":101330},{\"end\":101351,\"start\":101342},{\"end\":101364,\"start\":101359},{\"end\":101378,\"start\":101375},{\"end\":101388,\"start\":101385},{\"end\":101867,\"start\":101860},{\"end\":101888,\"start\":101879},{\"end\":101901,\"start\":101895},{\"end\":101914,\"start\":101912},{\"end\":101925,\"start\":101916},{\"end\":102434,\"start\":102430},{\"end\":102451,\"start\":102445},{\"end\":102460,\"start\":102456},{\"end\":102472,\"start\":102467},{\"end\":102481,\"start\":102479},{\"end\":102495,\"start\":102492},{\"end\":102509,\"start\":102505},{\"end\":102739,\"start\":102735},{\"end\":102751,\"start\":102748},{\"end\":102764,\"start\":102760},{\"end\":102781,\"start\":102772},{\"end\":102803,\"start\":102790},{\"end\":102814,\"start\":102812},{\"end\":102821,\"start\":102816},{\"end\":103242,\"start\":103237},{\"end\":103254,\"start\":103252},{\"end\":103264,\"start\":103260},{\"end\":103274,\"start\":103271},{\"end\":103510,\"start\":103504},{\"end\":103520,\"start\":103516},{\"end\":103532,\"start\":103528},{\"end\":103841,\"start\":103833},{\"end\":103852,\"start\":103845},{\"end\":103860,\"start\":103854}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":207239749},\"end\":69255,\"start\":69087},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":14358607},\"end\":69794,\"start\":69257},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":155099493},\"end\":70447,\"start\":69796},{\"attributes\":{\"id\":\"b3\"},\"end\":70566,\"start\":70449},{\"attributes\":{\"id\":\"b4\"},\"end\":70643,\"start\":70568},{\"attributes\":{\"id\":\"b5\"},\"end\":70734,\"start\":70645},{\"attributes\":{\"id\":\"b6\"},\"end\":70827,\"start\":70736},{\"attributes\":{\"id\":\"b7\"},\"end\":70954,\"start\":70829},{\"attributes\":{\"id\":\"b8\"},\"end\":71049,\"start\":70956},{\"attributes\":{\"id\":\"b9\"},\"end\":71144,\"start\":71051},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":203650565},\"end\":71694,\"start\":71146},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":15823252},\"end\":72307,\"start\":71696},{\"attributes\":{\"id\":\"b12\"},\"end\":72608,\"start\":72309},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":52838058},\"end\":73130,\"start\":72610},{\"attributes\":{\"id\":\"b14\"},\"end\":73256,\"start\":73132},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":1345193},\"end\":73720,\"start\":73258},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":7169506},\"end\":74020,\"start\":73722},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":1563373},\"end\":74606,\"start\":74022},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":219687810},\"end\":74966,\"start\":74608},{\"attributes\":{\"id\":\"b19\"},\"end\":75187,\"start\":74968},{\"attributes\":{\"id\":\"b20\"},\"end\":75471,\"start\":75189},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":52895458},\"end\":75717,\"start\":75473},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":207240067},\"end\":76100,\"start\":75719},{\"attributes\":{\"doi\":\"arXiv:1409.3809\",\"id\":\"b23\"},\"end\":76630,\"start\":76102},{\"attributes\":{\"doi\":\"abs/1812.01776\",\"id\":\"b24\"},\"end\":77005,\"start\":76632},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":1701442},\"end\":77564,\"start\":77007},{\"attributes\":{\"id\":\"b26\"},\"end\":77681,\"start\":77566},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":208098497},\"end\":78009,\"start\":77683},{\"attributes\":{\"id\":\"b28\"},\"end\":78310,\"start\":78011},{\"attributes\":{\"id\":\"b29\"},\"end\":78653,\"start\":78312},{\"attributes\":{\"doi\":\"arXiv:2002.08155\",\"id\":\"b30\"},\"end\":79096,\"start\":78655},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":263873695},\"end\":79691,\"start\":79098},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":8367854},\"end\":80157,\"start\":79693},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":219303421},\"end\":80740,\"start\":80159},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":224821159},\"end\":81166,\"start\":80742},{\"attributes\":{\"id\":\"b35\"},\"end\":81638,\"start\":81168},{\"attributes\":{\"id\":\"b36\"},\"end\":82042,\"start\":81640},{\"attributes\":{\"id\":\"b37\"},\"end\":82403,\"start\":82044},{\"attributes\":{\"id\":\"b38\"},\"end\":82583,\"start\":82405},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":209319636},\"end\":82957,\"start\":82585},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":46653426},\"end\":83276,\"start\":82959},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":16054402},\"end\":83642,\"start\":83278},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":13627618},\"end\":84266,\"start\":83644},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":3541031},\"end\":84968,\"start\":84268},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":146808333},\"end\":85520,\"start\":84970},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":2747585},\"end\":85866,\"start\":85522},{\"attributes\":{\"id\":\"b46\"},\"end\":86382,\"start\":85868},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":57754828},\"end\":86688,\"start\":86384},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":13177510},\"end\":86993,\"start\":86690},{\"attributes\":{\"id\":\"b49\"},\"end\":87251,\"start\":86995},{\"attributes\":{\"doi\":\"arXiv:1909.11942\",\"id\":\"b50\"},\"end\":87663,\"start\":87253},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":52984791},\"end\":88304,\"start\":87665},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":12795875},\"end\":88714,\"start\":88306},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":207116476},\"end\":89069,\"start\":88716},{\"attributes\":{\"doi\":\"arXiv:1907.11692\",\"id\":\"b54\"},\"end\":89523,\"start\":89071},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":7836524},\"end\":90181,\"start\":89525},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":236149789},\"end\":90758,\"start\":90183},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":73725045},\"end\":91402,\"start\":90760},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":202488191},\"end\":92023,\"start\":91404},{\"attributes\":{\"doi\":\"arXiv:1712.06139\",\"id\":\"b59\"},\"end\":92483,\"start\":92025},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":2811715},\"end\":92749,\"start\":92485},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":202786778},\"end\":93319,\"start\":92751},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":207953474},\"end\":93996,\"start\":93321},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":14251247},\"end\":94472,\"start\":93998},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":15265070},\"end\":94927,\"start\":94474},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":219302617},\"end\":95486,\"start\":94929},{\"attributes\":{\"doi\":\"arXiv:1910.01108\",\"id\":\"b66\"},\"end\":95836,\"start\":95488},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":10528156},\"end\":96157,\"start\":95838},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":3066749},\"end\":96632,\"start\":96159},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":64710803},\"end\":97004,\"start\":96634},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":62259501},\"end\":97269,\"start\":97006},{\"attributes\":{\"id\":\"b71\",\"matched_paper_id\":990233},\"end\":97927,\"start\":97271},{\"attributes\":{\"doi\":\"arXiv:1905.11946\",\"id\":\"b72\"},\"end\":98199,\"start\":97929},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":4706010},\"end\":98658,\"start\":98201},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":202709593},\"end\":99022,\"start\":98660},{\"attributes\":{\"id\":\"b75\",\"matched_paper_id\":13936383},\"end\":99287,\"start\":99024},{\"attributes\":{\"id\":\"b76\",\"matched_paper_id\":16793346},\"end\":99535,\"start\":99289},{\"attributes\":{\"id\":\"b77\",\"matched_paper_id\":39397729},\"end\":99920,\"start\":99537},{\"attributes\":{\"id\":\"b78\"},\"end\":100216,\"start\":99922},{\"attributes\":{\"id\":\"b79\",\"matched_paper_id\":4898729},\"end\":100668,\"start\":100218},{\"attributes\":{\"doi\":\"arXiv:1910.03771\",\"id\":\"b80\"},\"end\":101185,\"start\":100670},{\"attributes\":{\"id\":\"b81\",\"matched_paper_id\":89617717},\"end\":101803,\"start\":101187},{\"attributes\":{\"id\":\"b82\",\"matched_paper_id\":153314676},\"end\":102344,\"start\":101805},{\"attributes\":{\"doi\":\"abs/1804.03230\",\"id\":\"b83\"},\"end\":102726,\"start\":102346},{\"attributes\":{\"doi\":\"arXiv:1906.08237\",\"id\":\"b84\"},\"end\":103126,\"start\":102728},{\"attributes\":{\"id\":\"b85\",\"matched_paper_id\":196810567},\"end\":103446,\"start\":103128},{\"attributes\":{\"id\":\"b86\",\"matched_paper_id\":64697},\"end\":103751,\"start\":103448},{\"attributes\":{\"id\":\"b87\",\"matched_paper_id\":14684707},\"end\":104123,\"start\":103753}]", "bib_title": "[{\"end\":69126,\"start\":69087},{\"end\":69326,\"start\":69257},{\"end\":69882,\"start\":69796},{\"end\":71232,\"start\":71146},{\"end\":71783,\"start\":71696},{\"end\":72676,\"start\":72610},{\"end\":73301,\"start\":73258},{\"end\":73765,\"start\":73722},{\"end\":74101,\"start\":74022},{\"end\":74675,\"start\":74608},{\"end\":75533,\"start\":75473},{\"end\":75767,\"start\":75719},{\"end\":77062,\"start\":77007},{\"end\":77724,\"start\":77683},{\"end\":79201,\"start\":79098},{\"end\":79801,\"start\":79693},{\"end\":80233,\"start\":80159},{\"end\":80805,\"start\":80742},{\"end\":82661,\"start\":82585},{\"end\":83021,\"start\":82959},{\"end\":83361,\"start\":83278},{\"end\":83764,\"start\":83644},{\"end\":84345,\"start\":84268},{\"end\":84995,\"start\":84970},{\"end\":85582,\"start\":85522},{\"end\":86489,\"start\":86384},{\"end\":86757,\"start\":86690},{\"end\":87742,\"start\":87665},{\"end\":88360,\"start\":88306},{\"end\":88784,\"start\":88716},{\"end\":89578,\"start\":89525},{\"end\":90302,\"start\":90183},{\"end\":90825,\"start\":90760},{\"end\":91464,\"start\":91404},{\"end\":92512,\"start\":92485},{\"end\":92819,\"start\":92751},{\"end\":93416,\"start\":93321},{\"end\":94063,\"start\":93998},{\"end\":94565,\"start\":94474},{\"end\":94979,\"start\":94929},{\"end\":95902,\"start\":95838},{\"end\":96222,\"start\":96159},{\"end\":96693,\"start\":96634},{\"end\":97027,\"start\":97006},{\"end\":97348,\"start\":97271},{\"end\":98268,\"start\":98201},{\"end\":98756,\"start\":98660},{\"end\":99077,\"start\":99024},{\"end\":99342,\"start\":99289},{\"end\":99657,\"start\":99537},{\"end\":100273,\"start\":100218},{\"end\":101252,\"start\":101187},{\"end\":101856,\"start\":101805},{\"end\":103224,\"start\":103128},{\"end\":103494,\"start\":103448},{\"end\":103824,\"start\":103753}]", "bib_author": "[{\"end\":69142,\"start\":69128},{\"end\":69344,\"start\":69328},{\"end\":69353,\"start\":69344},{\"end\":69372,\"start\":69353},{\"end\":69383,\"start\":69372},{\"end\":69396,\"start\":69383},{\"end\":69901,\"start\":69884},{\"end\":69918,\"start\":69901},{\"end\":69928,\"start\":69918},{\"end\":69944,\"start\":69928},{\"end\":69961,\"start\":69944},{\"end\":70466,\"start\":70451},{\"end\":70586,\"start\":70570},{\"end\":70655,\"start\":70647},{\"end\":70666,\"start\":70655},{\"end\":70774,\"start\":70766},{\"end\":70839,\"start\":70831},{\"end\":70859,\"start\":70839},{\"end\":70864,\"start\":70859},{\"end\":71266,\"start\":71234},{\"end\":71278,\"start\":71266},{\"end\":71289,\"start\":71278},{\"end\":71810,\"start\":71785},{\"end\":71827,\"start\":71810},{\"end\":71839,\"start\":71827},{\"end\":72433,\"start\":72421},{\"end\":72445,\"start\":72433},{\"end\":72689,\"start\":72678},{\"end\":72701,\"start\":72689},{\"end\":72719,\"start\":72701},{\"end\":72737,\"start\":72719},{\"end\":72745,\"start\":72737},{\"end\":73167,\"start\":73149},{\"end\":73317,\"start\":73303},{\"end\":73344,\"start\":73317},{\"end\":73356,\"start\":73344},{\"end\":73369,\"start\":73356},{\"end\":73784,\"start\":73767},{\"end\":73809,\"start\":73784},{\"end\":74130,\"start\":74103},{\"end\":74150,\"start\":74130},{\"end\":74166,\"start\":74150},{\"end\":74176,\"start\":74166},{\"end\":74692,\"start\":74677},{\"end\":74707,\"start\":74692},{\"end\":74718,\"start\":74707},{\"end\":75054,\"start\":75035},{\"end\":75207,\"start\":75189},{\"end\":75549,\"start\":75535},{\"end\":75563,\"start\":75549},{\"end\":75581,\"start\":75563},{\"end\":75785,\"start\":75769},{\"end\":75796,\"start\":75785},{\"end\":75809,\"start\":75796},{\"end\":76223,\"start\":76205},{\"end\":76237,\"start\":76223},{\"end\":76256,\"start\":76237},{\"end\":76268,\"start\":76256},{\"end\":76280,\"start\":76268},{\"end\":76291,\"start\":76280},{\"end\":76305,\"start\":76291},{\"end\":76330,\"start\":76305},{\"end\":76706,\"start\":76688},{\"end\":76716,\"start\":76706},{\"end\":76728,\"start\":76716},{\"end\":76743,\"start\":76728},{\"end\":76756,\"start\":76743},{\"end\":76770,\"start\":76756},{\"end\":76785,\"start\":76770},{\"end\":76794,\"start\":76785},{\"end\":77082,\"start\":77064},{\"end\":77092,\"start\":77082},{\"end\":77105,\"start\":77092},{\"end\":77125,\"start\":77105},{\"end\":77144,\"start\":77125},{\"end\":77156,\"start\":77144},{\"end\":77600,\"start\":77588},{\"end\":77734,\"start\":77726},{\"end\":77742,\"start\":77734},{\"end\":77752,\"start\":77742},{\"end\":77758,\"start\":77752},{\"end\":77768,\"start\":77758},{\"end\":78107,\"start\":78093},{\"end\":78123,\"start\":78107},{\"end\":78135,\"start\":78123},{\"end\":78155,\"start\":78135},{\"end\":78394,\"start\":78379},{\"end\":78409,\"start\":78394},{\"end\":78428,\"start\":78409},{\"end\":78440,\"start\":78428},{\"end\":78454,\"start\":78440},{\"end\":78461,\"start\":78454},{\"end\":78478,\"start\":78461},{\"end\":78670,\"start\":78655},{\"end\":78680,\"start\":78670},{\"end\":78691,\"start\":78680},{\"end\":78701,\"start\":78691},{\"end\":78717,\"start\":78701},{\"end\":78728,\"start\":78717},{\"end\":78741,\"start\":78728},{\"end\":78751,\"start\":78741},{\"end\":78761,\"start\":78751},{\"end\":78774,\"start\":78761},{\"end\":79216,\"start\":79203},{\"end\":79235,\"start\":79216},{\"end\":79255,\"start\":79235},{\"end\":79274,\"start\":79255},{\"end\":79293,\"start\":79274},{\"end\":79819,\"start\":79803},{\"end\":79835,\"start\":79819},{\"end\":79847,\"start\":79835},{\"end\":79867,\"start\":79847},{\"end\":79888,\"start\":79867},{\"end\":80251,\"start\":80235},{\"end\":80264,\"start\":80251},{\"end\":80279,\"start\":80264},{\"end\":80297,\"start\":80279},{\"end\":80313,\"start\":80297},{\"end\":80328,\"start\":80313},{\"end\":80843,\"start\":80807},{\"end\":80855,\"start\":80843},{\"end\":80869,\"start\":80855},{\"end\":80881,\"start\":80869},{\"end\":80913,\"start\":80881},{\"end\":80918,\"start\":80913},{\"end\":81304,\"start\":81268},{\"end\":81316,\"start\":81304},{\"end\":81347,\"start\":81316},{\"end\":81365,\"start\":81347},{\"end\":81380,\"start\":81365},{\"end\":81385,\"start\":81380},{\"end\":81767,\"start\":81731},{\"end\":81784,\"start\":81767},{\"end\":81799,\"start\":81784},{\"end\":81831,\"start\":81799},{\"end\":81836,\"start\":81831},{\"end\":82142,\"start\":82133},{\"end\":82150,\"start\":82142},{\"end\":82160,\"start\":82150},{\"end\":82168,\"start\":82160},{\"end\":82178,\"start\":82168},{\"end\":82185,\"start\":82178},{\"end\":82194,\"start\":82185},{\"end\":82204,\"start\":82194},{\"end\":82210,\"start\":82204},{\"end\":82672,\"start\":82663},{\"end\":82684,\"start\":82672},{\"end\":82695,\"start\":82684},{\"end\":82705,\"start\":82695},{\"end\":82719,\"start\":82705},{\"end\":82727,\"start\":82719},{\"end\":83037,\"start\":83023},{\"end\":83051,\"start\":83037},{\"end\":83067,\"start\":83051},{\"end\":83085,\"start\":83067},{\"end\":83104,\"start\":83085},{\"end\":83377,\"start\":83363},{\"end\":83393,\"start\":83377},{\"end\":83407,\"start\":83393},{\"end\":83425,\"start\":83407},{\"end\":83444,\"start\":83425},{\"end\":83783,\"start\":83766},{\"end\":83805,\"start\":83783},{\"end\":83818,\"start\":83805},{\"end\":83828,\"start\":83818},{\"end\":83845,\"start\":83828},{\"end\":83860,\"start\":83845},{\"end\":83881,\"start\":83860},{\"end\":83895,\"start\":83881},{\"end\":83914,\"start\":83895},{\"end\":83928,\"start\":83914},{\"end\":83940,\"start\":83928},{\"end\":84360,\"start\":84347},{\"end\":84368,\"start\":84360},{\"end\":84378,\"start\":84368},{\"end\":84390,\"start\":84378},{\"end\":84399,\"start\":84390},{\"end\":84413,\"start\":84399},{\"end\":84422,\"start\":84413},{\"end\":84429,\"start\":84422},{\"end\":84436,\"start\":84429},{\"end\":84445,\"start\":84436},{\"end\":84452,\"start\":84445},{\"end\":84459,\"start\":84452},{\"end\":84465,\"start\":84459},{\"end\":84478,\"start\":84465},{\"end\":84493,\"start\":84478},{\"end\":84502,\"start\":84493},{\"end\":84510,\"start\":84502},{\"end\":85012,\"start\":84997},{\"end\":85026,\"start\":85012},{\"end\":85037,\"start\":85026},{\"end\":85055,\"start\":85037},{\"end\":85064,\"start\":85055},{\"end\":85078,\"start\":85064},{\"end\":85091,\"start\":85078},{\"end\":85102,\"start\":85091},{\"end\":85116,\"start\":85102},{\"end\":85133,\"start\":85116},{\"end\":85598,\"start\":85584},{\"end\":85613,\"start\":85598},{\"end\":85646,\"start\":85613},{\"end\":85652,\"start\":85646},{\"end\":85992,\"start\":85974},{\"end\":86017,\"start\":85992},{\"end\":86027,\"start\":86017},{\"end\":86504,\"start\":86491},{\"end\":86520,\"start\":86504},{\"end\":86524,\"start\":86520},{\"end\":86772,\"start\":86759},{\"end\":86789,\"start\":86772},{\"end\":87066,\"start\":87049},{\"end\":87078,\"start\":87066},{\"end\":87095,\"start\":87078},{\"end\":87346,\"start\":87331},{\"end\":87359,\"start\":87346},{\"end\":87378,\"start\":87359},{\"end\":87392,\"start\":87378},{\"end\":87407,\"start\":87392},{\"end\":87421,\"start\":87407},{\"end\":87758,\"start\":87744},{\"end\":87775,\"start\":87758},{\"end\":87791,\"start\":87775},{\"end\":87820,\"start\":87791},{\"end\":87835,\"start\":87820},{\"end\":87854,\"start\":87835},{\"end\":88379,\"start\":88362},{\"end\":88395,\"start\":88379},{\"end\":88412,\"start\":88395},{\"end\":88797,\"start\":88786},{\"end\":88810,\"start\":88797},{\"end\":88823,\"start\":88810},{\"end\":88837,\"start\":88823},{\"end\":88849,\"start\":88837},{\"end\":88865,\"start\":88849},{\"end\":89083,\"start\":89071},{\"end\":89093,\"start\":89083},{\"end\":89106,\"start\":89093},{\"end\":89118,\"start\":89106},{\"end\":89132,\"start\":89118},{\"end\":89144,\"start\":89132},{\"end\":89155,\"start\":89144},{\"end\":89167,\"start\":89155},{\"end\":89185,\"start\":89167},{\"end\":89203,\"start\":89185},{\"end\":89212,\"start\":89203},{\"end\":89591,\"start\":89580},{\"end\":89603,\"start\":89591},{\"end\":89617,\"start\":89603},{\"end\":89631,\"start\":89617},{\"end\":90329,\"start\":90304},{\"end\":90338,\"start\":90329},{\"end\":90376,\"start\":90338},{\"end\":90387,\"start\":90376},{\"end\":90841,\"start\":90827},{\"end\":90855,\"start\":90841},{\"end\":90867,\"start\":90855},{\"end\":90879,\"start\":90867},{\"end\":90896,\"start\":90879},{\"end\":90908,\"start\":90896},{\"end\":90919,\"start\":90908},{\"end\":90939,\"start\":90919},{\"end\":90951,\"start\":90939},{\"end\":91484,\"start\":91466},{\"end\":91498,\"start\":91484},{\"end\":91516,\"start\":91498},{\"end\":91532,\"start\":91516},{\"end\":91542,\"start\":91532},{\"end\":91551,\"start\":91542},{\"end\":91562,\"start\":91551},{\"end\":91570,\"start\":91562},{\"end\":91581,\"start\":91570},{\"end\":91596,\"start\":91581},{\"end\":91605,\"start\":91596},{\"end\":92104,\"start\":92084},{\"end\":92117,\"start\":92104},{\"end\":92132,\"start\":92117},{\"end\":92150,\"start\":92132},{\"end\":92158,\"start\":92150},{\"end\":92170,\"start\":92158},{\"end\":92188,\"start\":92170},{\"end\":92204,\"start\":92188},{\"end\":92218,\"start\":92204},{\"end\":92524,\"start\":92514},{\"end\":92529,\"start\":92524},{\"end\":92834,\"start\":92821},{\"end\":92845,\"start\":92834},{\"end\":92862,\"start\":92845},{\"end\":92874,\"start\":92862},{\"end\":92890,\"start\":92874},{\"end\":92906,\"start\":92890},{\"end\":92922,\"start\":92906},{\"end\":92934,\"start\":92922},{\"end\":92954,\"start\":92934},{\"end\":92967,\"start\":92954},{\"end\":93430,\"start\":93418},{\"end\":93442,\"start\":93430},{\"end\":93454,\"start\":93442},{\"end\":93464,\"start\":93454},{\"end\":93479,\"start\":93464},{\"end\":93489,\"start\":93479},{\"end\":94078,\"start\":94065},{\"end\":94088,\"start\":94078},{\"end\":94096,\"start\":94088},{\"end\":94111,\"start\":94096},{\"end\":94128,\"start\":94111},{\"end\":94140,\"start\":94128},{\"end\":94580,\"start\":94567},{\"end\":94593,\"start\":94580},{\"end\":94607,\"start\":94593},{\"end\":94997,\"start\":94981},{\"end\":95010,\"start\":94997},{\"end\":95025,\"start\":95010},{\"end\":95580,\"start\":95567},{\"end\":95596,\"start\":95580},{\"end\":95613,\"start\":95596},{\"end\":95626,\"start\":95613},{\"end\":95920,\"start\":95904},{\"end\":95933,\"start\":95920},{\"end\":95950,\"start\":95933},{\"end\":96240,\"start\":96224},{\"end\":96253,\"start\":96240},{\"end\":96263,\"start\":96253},{\"end\":96276,\"start\":96263},{\"end\":96293,\"start\":96276},{\"end\":96705,\"start\":96695},{\"end\":96717,\"start\":96705},{\"end\":96740,\"start\":96717},{\"end\":96762,\"start\":96740},{\"end\":96784,\"start\":96762},{\"end\":96791,\"start\":96784},{\"end\":97045,\"start\":97029},{\"end\":97060,\"start\":97045},{\"end\":97075,\"start\":97060},{\"end\":97366,\"start\":97350},{\"end\":97382,\"start\":97366},{\"end\":97391,\"start\":97382},{\"end\":97405,\"start\":97391},{\"end\":97420,\"start\":97405},{\"end\":97429,\"start\":97420},{\"end\":97439,\"start\":97429},{\"end\":97455,\"start\":97439},{\"end\":97462,\"start\":97455},{\"end\":97943,\"start\":97929},{\"end\":97951,\"start\":97943},{\"end\":97955,\"start\":97951},{\"end\":97969,\"start\":97955},{\"end\":98284,\"start\":98270},{\"end\":98301,\"start\":98284},{\"end\":98311,\"start\":98301},{\"end\":98325,\"start\":98311},{\"end\":98334,\"start\":98325},{\"end\":98772,\"start\":98758},{\"end\":98789,\"start\":98772},{\"end\":98799,\"start\":98789},{\"end\":98813,\"start\":98799},{\"end\":98822,\"start\":98813},{\"end\":99095,\"start\":99079},{\"end\":99113,\"start\":99095},{\"end\":99132,\"start\":99113},{\"end\":99366,\"start\":99344},{\"end\":99387,\"start\":99366},{\"end\":99669,\"start\":99659},{\"end\":99676,\"start\":99669},{\"end\":99694,\"start\":99676},{\"end\":100012,\"start\":100000},{\"end\":100030,\"start\":100012},{\"end\":100046,\"start\":100030},{\"end\":100062,\"start\":100046},{\"end\":100285,\"start\":100275},{\"end\":100298,\"start\":100285},{\"end\":100312,\"start\":100298},{\"end\":100324,\"start\":100312},{\"end\":100335,\"start\":100324},{\"end\":100349,\"start\":100335},{\"end\":100364,\"start\":100349},{\"end\":100374,\"start\":100364},{\"end\":100386,\"start\":100374},{\"end\":100756,\"start\":100743},{\"end\":100772,\"start\":100756},{\"end\":100785,\"start\":100772},{\"end\":100802,\"start\":100785},{\"end\":100820,\"start\":100802},{\"end\":100833,\"start\":100820},{\"end\":100849,\"start\":100833},{\"end\":100860,\"start\":100849},{\"end\":100871,\"start\":100860},{\"end\":100889,\"start\":100871},{\"end\":101270,\"start\":101254},{\"end\":101284,\"start\":101270},{\"end\":101296,\"start\":101284},{\"end\":101310,\"start\":101296},{\"end\":101324,\"start\":101310},{\"end\":101338,\"start\":101324},{\"end\":101353,\"start\":101338},{\"end\":101366,\"start\":101353},{\"end\":101380,\"start\":101366},{\"end\":101390,\"start\":101380},{\"end\":101869,\"start\":101858},{\"end\":101890,\"start\":101869},{\"end\":101903,\"start\":101890},{\"end\":101916,\"start\":101903},{\"end\":101927,\"start\":101916},{\"end\":102436,\"start\":102422},{\"end\":102453,\"start\":102436},{\"end\":102462,\"start\":102453},{\"end\":102474,\"start\":102462},{\"end\":102483,\"start\":102474},{\"end\":102497,\"start\":102483},{\"end\":102511,\"start\":102497},{\"end\":102741,\"start\":102728},{\"end\":102753,\"start\":102741},{\"end\":102766,\"start\":102753},{\"end\":102783,\"start\":102766},{\"end\":102805,\"start\":102783},{\"end\":102816,\"start\":102805},{\"end\":102823,\"start\":102816},{\"end\":103244,\"start\":103226},{\"end\":103256,\"start\":103244},{\"end\":103266,\"start\":103256},{\"end\":103276,\"start\":103266},{\"end\":103512,\"start\":103496},{\"end\":103522,\"start\":103512},{\"end\":103534,\"start\":103522},{\"end\":103843,\"start\":103826},{\"end\":103854,\"start\":103843},{\"end\":103862,\"start\":103854}]", "bib_venue": "[{\"end\":69545,\"start\":69479},{\"end\":70152,\"start\":70065},{\"end\":71395,\"start\":71342},{\"end\":72886,\"start\":72824},{\"end\":73508,\"start\":73447},{\"end\":74335,\"start\":74264},{\"end\":75918,\"start\":75872},{\"end\":77246,\"start\":77236},{\"end\":80412,\"start\":80398},{\"end\":85254,\"start\":85202},{\"end\":87946,\"start\":87934},{\"end\":88519,\"start\":88474},{\"end\":89849,\"start\":89740},{\"end\":91041,\"start\":91031},{\"end\":91730,\"start\":91676},{\"end\":93692,\"start\":93599},{\"end\":95167,\"start\":95096},{\"end\":96408,\"start\":96359},{\"end\":97621,\"start\":97550},{\"end\":100439,\"start\":100421},{\"end\":102055,\"start\":101991},{\"end\":69161,\"start\":69142},{\"end\":69477,\"start\":69396},{\"end\":70063,\"start\":69961},{\"end\":70764,\"start\":70736},{\"end\":70994,\"start\":70958},{\"end\":71089,\"start\":71053},{\"end\":71340,\"start\":71289},{\"end\":71924,\"start\":71839},{\"end\":72419,\"start\":72309},{\"end\":72822,\"start\":72745},{\"end\":73147,\"start\":73132},{\"end\":73445,\"start\":73369},{\"end\":73848,\"start\":73809},{\"end\":74262,\"start\":74176},{\"end\":74777,\"start\":74718},{\"end\":75033,\"start\":74968},{\"end\":75298,\"start\":75207},{\"end\":75588,\"start\":75581},{\"end\":75870,\"start\":75809},{\"end\":76203,\"start\":76102},{\"end\":76686,\"start\":76632},{\"end\":77234,\"start\":77156},{\"end\":77586,\"start\":77566},{\"end\":77831,\"start\":77768},{\"end\":78091,\"start\":78011},{\"end\":78377,\"start\":78312},{\"end\":78847,\"start\":78790},{\"end\":79378,\"start\":79293},{\"end\":79916,\"start\":79888},{\"end\":80396,\"start\":80328},{\"end\":80946,\"start\":80918},{\"end\":81266,\"start\":81168},{\"end\":81729,\"start\":81640},{\"end\":82131,\"start\":82044},{\"end\":82481,\"start\":82405},{\"end\":82753,\"start\":82727},{\"end\":83110,\"start\":83104},{\"end\":83451,\"start\":83444},{\"end\":83946,\"start\":83940},{\"end\":84592,\"start\":84510},{\"end\":85200,\"start\":85133},{\"end\":85686,\"start\":85652},{\"end\":85972,\"start\":85868},{\"end\":86528,\"start\":86524},{\"end\":86825,\"start\":86789},{\"end\":87047,\"start\":86995},{\"end\":87329,\"start\":87253},{\"end\":87932,\"start\":87854},{\"end\":88472,\"start\":88412},{\"end\":88879,\"start\":88865},{\"end\":89274,\"start\":89228},{\"end\":89738,\"start\":89631},{\"end\":90454,\"start\":90387},{\"end\":91029,\"start\":90951},{\"end\":91674,\"start\":91605},{\"end\":92082,\"start\":92025},{\"end\":92569,\"start\":92529},{\"end\":93016,\"start\":92967},{\"end\":93597,\"start\":93489},{\"end\":94217,\"start\":94140},{\"end\":94678,\"start\":94607},{\"end\":95094,\"start\":95025},{\"end\":95565,\"start\":95488},{\"end\":95984,\"start\":95950},{\"end\":96357,\"start\":96293},{\"end\":96800,\"start\":96791},{\"end\":97131,\"start\":97075},{\"end\":97548,\"start\":97462},{\"end\":98043,\"start\":97985},{\"end\":98414,\"start\":98334},{\"end\":98829,\"start\":98822},{\"end\":99149,\"start\":99132},{\"end\":99396,\"start\":99387},{\"end\":99718,\"start\":99694},{\"end\":99998,\"start\":99922},{\"end\":100419,\"start\":100386},{\"end\":100741,\"start\":100670},{\"end\":101472,\"start\":101390},{\"end\":101989,\"start\":101927},{\"end\":102420,\"start\":102346},{\"end\":102904,\"start\":102839},{\"end\":103279,\"start\":103276},{\"end\":103583,\"start\":103534},{\"end\":103916,\"start\":103862}]"}}}, "year": 2023, "month": 12, "day": 17}