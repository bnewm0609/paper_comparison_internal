{"id": 234357520, "updated": "2023-10-26 20:52:32.852", "metadata": {"title": "VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning", "authors": "[{\"first\":\"Adrien\",\"last\":\"Bardes\",\"middle\":[]},{\"first\":\"Jean\",\"last\":\"Ponce\",\"middle\":[]},{\"first\":\"Yann\",\"last\":\"LeCun\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Recent self-supervised methods for image representation learning are based on maximizing the agreement between embedding vectors from different views of the same image. A trivial solution is obtained when the encoder outputs constant vectors. This collapse problem is often avoided through implicit biases in the learning architecture, that often lack a clear justification or interpretation. In this paper, we introduce VICReg (Variance-Invariance-Covariance Regularization), a method that explicitly avoids the collapse problem with a simple regularization term on the variance of the embeddings along each dimension individually. VICReg combines the variance term with a decorrelation mechanism based on redundancy reduction and covariance regularization, and achieves results on par with the state of the art on several downstream tasks. In addition, we show that incorporating our new variance term into other methods helps stabilize the training and leads to performance improvements.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2105.04906", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iclr/BardesPL22", "doi": null}}, "content": {"source": {"pdf_hash": "0d0cf5f64c052aa7edc5bb638203616a620557f6", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2105.04906v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "6135e0d8037eb960cdfe17c83bf93e97acc3f40d", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/0d0cf5f64c052aa7edc5bb638203616a620557f6.txt", "contents": "\nVICREG: VARIANCE-INVARIANCE-COVARIANCE RE- GULARIZATION FOR SELF-SUPERVISED LEARNING\n\n\nAdrien Bardes \nInria\n\u00c9cole normale sup\u00e9rieure\nCNRS\nPSL Research University\n\n\nJean Ponce \nInria\n\u00c9cole normale sup\u00e9rieure\nCNRS\nPSL Research University\n\n\nCenter for Data Science\nNew York University\n\n\nYann Lecun \nCourant Institute\nNew York University\n\n\nCenter for Data Science\nNew York University\n\n\nFacebook Ai Research \nVICREG: VARIANCE-INVARIANCE-COVARIANCE RE- GULARIZATION FOR SELF-SUPERVISED LEARNING\nPublished as a conference paper at ICLR 2022\nRecent self-supervised methods for image representation learning maximize the agreement between embedding vectors produced by encoders fed with different views of the same image. The main challenge is to prevent a collapse in which the encoders produce constant or non-informative vectors. We introduce VICReg (Variance-Invariance-Covariance Regularization), a method that explicitly avoids the collapse problem with two regularizations terms applied to both embeddings separately: (1) a term that maintains the variance of each embedding dimension above a threshold, (2) a term that decorrelates each pair of variables. Unlike most other approaches to the same problem, VICReg does not require techniques such as: weight sharing between the branches, batch normalization, feature-wise normalization, output quantization, stop gradient, memory banks, etc., and achieves results on par with the state of the art on several downstream tasks. In addition, we show that our variance regularization term stabilizes the training of other methods and leads to performance improvements.\n\nINTRODUCTION\n\nSelf-supervised representation learning has made significant progress over the last years, almost reaching the performance of supervised baselines on many downstream tasks Bachman et al. (2019); Misra & Maaten (2020); He et al. (2020); Tian et al. (2020); Caron et al. (2020); Grill et al. (2020); Chen & He (2020); Gidaris et al. (2021); Zbontar et al. (2021). Several recent approaches rely on a joint embedding architecture in which two networks are trained to produce similar embeddings for different views of the same image. A popular instance is the Siamese network architecture Bromley et al. (1994), where the two networks share the same weights. The main challenge with joint embedding architectures is to prevent a collapse in which the two branches ignore the inputs and produce identical and constant output vectors. There are two main approaches to preventing collapse: contrastive methods and information maximization methods. Contrastive Bromley et al. (1994) Chen et al. (2020a) methods tend to be costly, require large batch sizes or memory banks, and use a loss that explicitly pushes the embeddings of dissimilar images away from each other. They often require a mining procedure to search for offending dissimilar samples from a memory bank He et al. (2020) or from the current batch Chen et al. (2020a). Quantization-based approaches Caron et al. (2020; force the embeddings of different samples to belong to different clusters on the unit sphere. Collapse is prevented by ensuring that the assignment of samples to clusters is as uniform as possible. A similarity term encourages the cluster assignment score vectors from the two branches to be similar. More recently, a few methods have appeared that do not rely on contrastive samples or vector quantization, yet produce high-quality representations, for example BYOL Grill et al. (2020) and SimSiam Chen & He (2020). They exploit several tricks: batch-wise or feature-wise normalization, a \"momentum encoder\" in which the parameter vector of one branch is a low-pass-filtered version of the parameter vector of the other branch Grill et al. (2020); Richemond et al. (2020), or a stop-gradient operation in one of the branches Chen & He (2020). The dynamics of learning in these methods, and how they avoid collapse, is not fully understood, although theoretical and empirical studies point to the crucial importance of batch-wise or feature-wise normalization Richemond et al. (2020); Tian et al. (2021). Finally, an \n\n\nVICREG: INTUITION\n\nWe introduce VICReg (Variance-Invariance-Covariance Regularization), a self-supervised method for training joint embedding architectures based on the principle of preserving the information content of the embeddings. The basic idea is to use a loss function with three terms:\n\n\u2022 Invariance: the mean square distance between the embedding vectors.\n\n\u2022 Variance: a hinge loss to maintain the standard deviation (over a batch) of each variable of the embedding above a given threshold. This term forces the embedding vectors of samples within a batch to be different.\n\n\u2022 Covariance: a term that attracts the covariances (over a batch) between every pair of (centered) embedding variables towards zero. This term decorrelates the variables of each embedding and prevents an informational collapse in which the variables would vary together or be highly correlated.\n\nVariance and Covariance terms are applied to both branches of the architecture separately, thereby preserving the information content of each embedding at a certain level and preventing informational collapse independently for the two branches. The main contribution of this paper is the Variance preservation term, which explicitly prevents a collapse due to a shrinkage of the embedding vectors towards zero. The Covariance criterion is borrowed from the Barlow Twins method and prevents informational collapse due to redundancy between the embedding variables Zbontar et al. (2021). VICReg is more generally applicable than most of the aforementioned methods because of fewer constraints on the architecture. In particular, VICReg:\n\n\u2022 does not require that the weights of the two branches be shared, not that the architectures be identical, nor that the inputs be of the same nature;\n\n\u2022 does not require a memory bank, nor contrastive samples, nor a large batch size;\n\n\u2022 does not require batch-wise nor feature-wise normalization; and \u2022 does not require vector quantization nor a predictor module.\n\nOther methods require asymmetric stop gradient operations, as in SimSiam Chen & He (2020), weight sharing between the two branches as in classical Siamese nets, or weight sharing through exponential moving average dampening with stop gradient in one branch, as in BYOL and MoCo He et al. (2020) (2015). These methods train a student network to predict the representations of a teacher network, for which the weights are a running average of the student network's weights Grill et al. (2020), or are shared with the student network, but no gradient is back-propagated through the teacher Chen & He (2020). These methods are effective, but there is no clear understanding of why and how they avoid collapse. Alternatively, the images can be represented as bags of word over a dictionary of visual features, which effectively prevents collapse. In OBoW Gidaris et al. (2020) and Gidaris et al. (2021) the dictionary is obtained by off-line or on-line clustering. By contrast, our method explicitly prevents collapse in the two branches independently, which removes the requirement for shared weights and identical architecture, opening the door to the application of joint-embedding SSL to multi-modal signals.\n\nInformation maximization methods. A principle to prevent collapse is to maximize the information content of the embeddings. Two such methods were recently proposed: W-MSE Ermolov et al. (2021) and Barlow Twins Zbontar et al. (2021). In W-MSE, an extra module transforms the embeddings into the eigenspace of their covariance matrix (whitening or Karhunen-Lo\u00e8ve transform), and forces the vectors thereby obtained to be uniformly distributed on the unit sphere. In Barlow Twins, a loss term attempts to make the normalized cross-correlation matrix of the embedding vectors from the two branches to be close to the identity. Both methods attempt to produce embedding variables that are decorrelated from each other, thus preventing an informational collapse in which the variables carry redundant information. Because all variables are normalized over a batch, there is no incentive for them to shrink nor expand. This seems to sufficient to prevent collapse. Our method borrows the decorrelation mechanism of Barlow Twins. But it includes an explicit variance-preservation term for each variable of the two embeddings and thus does not require any normalization.  Chen et al. (2020a) and is based on a joint embedding architecture. Contrary to many previous approaches, our architecture may be completely symmetric or completely asymmetric with no shared structure or parameters between the two branches. In most of our experiments, we use a Siamese net architecture in which the two branches are identical and share weights. Each branch consists of an encoder f \u03b8 that outputs the representations (used for downstream tasks), followed by an expander h \u03c6 that maps the representations into an embedding space where the loss function will be computed. The role of the expander is twofold: (1) eliminate the information by which the two representations differ, (2) expand the dimension in a non-linear fashion so that decorrelating the embedding variables will reduce the dependencies (not just the correlations) between the variables of the representation vector. The loss function uses a term s that learns invariance to data transformations and is regularized with a variance term v that prevents norm collapse and a covariance term c that prevents informational collapse by decorrelating the different dimensions of the vectors. After pretraining, the expander is discarded and the representations of the encoder are used for downstream tasks.\n\n\nMETHOD\n\nGiven an image i sampled from a dataset D, two transformations t and t are sampled from a distribution T to produce two different views x = t(i) and x = t (i) of i. These transformations are random crops of the image, followed by color distortions. The distribution T is described in Appendix C. The views x and x are first encoded by f \u03b8 into their representations y = f \u03b8 (x) and y = f \u03b8 (x ), which are then mapped by the expander h \u03c6 onto the embeddings z = h \u03c6 (y) and z = h \u03c6 (y ). The loss is computed at the embedding level on z and z .\n\nWe describe here the variance, invariance and covariance terms that compose our loss function. The images are processed in batches, and we denote Z = [z 1 , . . . , z n ] and Z = [z 1 , . . . , z n ] the two batches composed of n vectors of dimension d, of embeddings coming out of the two branches of the siamese architecture. We denote by z j the vector composed of each value at dimension j in all vectors in Z. We define the variance regularization term v as a hinge function on the standard deviation of the embeddings along the batch dimension:\nv(Z) = 1 d d j=1 max(0, \u03b3 \u2212 S(z j , )),(1)\nwhere S is the regularized standard deviation defined by:\nS(x, ) = Var(x) + ,(2)\n\u03b3 is a constant target value for the standard deviation, fixed to 1 in our experiments, is a small scalar preventing numerical instabilities. This criterion encourages the variance inside the current batch to be equal to \u03b3 along each dimension, preventing collapse with all the inputs mapped on the same vector. Using the standard deviation and not directly the variance is crucial. Indeed, if we take S(x) = Var(x) in the hinge function, the gradient of S with respect to x becomes close to 0 when x is close tox. In this case, the gradient of v also becomes close to 0 and the embeddings collapse. We define the covariance matrix of Z as:\nC(Z) = 1 n \u2212 1 n i=1 (z i \u2212z)(z i \u2212z) T , wherez = 1 n n i=1 z i .(3)\nInspired by Barlow Twins Zbontar et al. (2021), we can then define the covariance regularization term c as the sum of the squared off-diagonal coefficients of C(Z), with a factor 1/d that scales the criterion as a function of the dimension:\nc(Z) = 1 d i =j [C(Z)] 2 i,j .(4)\nThis term encourages the off-diagonal coefficients of C(Z) to be close to 0, decorrelating the different dimensions of the embeddings and preventing them from encoding similar information. Decorrelation at the embedding level ultimately has a decorrelation effect at the representation level, which is a non trivial phenomenon that we study in Appendix D. We finally define the invariance criterion s between Z and Z as the mean-squared euclidean distance between each pair of vectors, without any normalization:\ns(Z, Z ) = 1 n i z i \u2212 z i 2 2 .(5)\nThe overall loss function is a weighted average of the invariance, variance and covariance terms:\n(Z, Z ) = \u03bbs(Z, Z ) + \u00b5[v(Z) + v(Z )] + \u03bd[c(Z) + c(Z )],(6)\nwhere \u03bb, \u00b5 and \u03bd are hyper-parameters controlling the importance of each term in the loss. In our experiments, we set \u03bd = 1 and perform a grid search on the values of \u03bb and \u00b5 with the base condition \u03bb = \u00b5 > 1. The overall objective function taken on all images over an unlabelled dataset D is given by:\nL = I\u2208D t,t \u223cT (Z I , Z I ),(7)\nwhere Z I and Z I are the batches of embeddings corresponding to the batch of images I transformed by t and t . The objective is minimized for several epochs, over the encoder parameters \u03b8 and expander parameters \u03c6. We illustrate the architecture and loss function of VICReg in Figure 1.\n\n\nIMPLEMENTATION DETAILS\n\nImplementation details for pretraining with VICReg on the 1000-classes ImagetNet dataset without labels are as follows. Coefficients \u03bb and \u00b5 are 25 and \u03bd is 1 in Eq. (6) \n\n\nRESULTS\n\nIn this section, we evaluate the representations obtained after self-supervised pretraining of a ResNet-50 He et al. (2016) backbone with VICReg during 1000 epochs, on the training set of ImageNet, using the training protocol described in section 4. We also pretrain on pairs of image and text data and evaluate on retrieval tasks on the MS-COCO dataset. We also evaluate the performance of the backbone when fine-tuned with a linear classifier on a subset of ImageNet's training set using 1% or 10% of the labels, using the split of Chen et al. (2020a). We give implementation details about the optimization procedure for these tasks in Appendix C. We have applied the training procedure described in section 4 with three different random initialization. The numbers reported in Table 1 for VICReg are the mean scores, and we have observed that the difference between worse and best run is lower than 0.1% accuracy for linear classification, which shows that VICReg is a very stable algorithm. Lack of time has prevented us from doing the same for the semi-supervised classification experiments, and the experiments of section 5.2 and 6, but we expect similar conclusion to hold. We compare in Table 1 our results on both tasks against other methods on the validation set of ImageNet. The performance of VICReg is on par with the state of the art without using the negative pairs of SimCLR, the clusters of SwAV, the bag-of-words representations of OBoW, or any asymmetric networks architectural tricks such as the momentum encoder of BYOL and the stop-gradient operation of SimSiam. The performance is comparable to that of Barlow Twins, which shows that VICReg's more explicit way of constraining the variance and comparing views has the same power than maximizing cross-correlations between pairs of twin dimensions. The main advantage of VICReg is the modularity of its objective function and the applicability to multi-modal setups. We report the performance in Table 2,  VICReg performs on par with most concurrent methods, and better than Barlow Twins, across all classification tasks, but is slightly behind the top-3 on detection tasks.\n\n\nTRANSFER TO\n\n\nMULTI-MODAL PRETRAINING ON MS-COCO\n\nOne fundamental difference of VICReg compared to Barlow Twins is the way the branches are regularized. In VICReg, both branches are regularized independently, as the covariance term is applied on each branch separately, which works better in the scenarios where the branches are completely different, have different types of architecture and process different types of data. Indeed, the statistics of the output of the two branches can be very different, and the amount of regularization required for each may vary a lot. In Barlow Twins, the regularization is applied on the cross-correlation matrix, which favors the scenarios where the branches produce outputs with similar statistics. We demonstrate the capabilities of VICReg in a multi-modal experiment where we pretrain on pairs of images and corresponding captions on the MS-COCO dataset. We regularize each branch with a different coefficient, which is not possible with Barlow Twins, and we show that VICReg outperforms Barlow Twins on image and text retrieval downstream tasks. Table 3 reports the performance of VICReg against the contrastive loss proposed by VSE++ Faghri et al. (2018), and against Barlow Twins, in the identical setting proposed in Faghri et al. (2018). VICReg outperforms the two by a significant margin. \n\n\nANALYSIS\n\nIn this section we study how the different components of our method contribute to its performance, as well as how they interact with components from other self-supervised methods. We also evaluate different scenarios where the branches have different weights and architecture. All reported results are obtained on the linear evaluation protocol, using a ResNet-50 backbone if not mentioned otherwise, and 100 epochs of pretraining, which gives results consistent with those obtained with 1000 epochs of pretraining. The optimization setting used for each experiment is described in Appendix C.\n\nAsymmetric networks. We study the impact of different components used in asymmetric architectures and the effects of adding variance and covariance regularization, in terms of performance and training stability. Starting from a simple symmetric architecture with an encoder and an expander without batch normalization, which correspond to VICReg without batch normalization in the expander, we progressively add batch normalization in the inner layers of the expander, a predictor, a stop-gradient operation and a momentum encoder. We use the training protocol and architecture of SimSiam Chen & He (2020) when a stop-gradient is used and the training protocol and architecture of BYOL Grill et al. (2020) when a momentum encoder is used. The predictor as used in SimSiam and BYOL is a learnable module g \u03c8 that predicts the embedding of a view given the embedding of the other view of the same image. If z and z are the embeddings of two views of an image, then p = g \u03c8 (z) and p = g \u03c8 (z ) are the predictions of each view. The invariance loss function of Eq. (5) is now computed between a batch of embeddings Z = [z 1 , . . . , z n ] and the corresponding batch of predictions P = [p 1 , . . . , p n ], then symmetrized:\ns(Z, Z , P, P ) = 1 2n i D(z i \u2212 p i ) + 1 2n i D(z i \u2212 p i ),(8)\nwhere D is a distance function that depends on the method used. BYOL uses the mean square error between l 2 -normalized vectors, SimSiam uses the negative cosine similarity loss and VICReg uses the mean square error without l 2 -normalization. The variance and covariance terms are regularizing the output Z and Z of the expander, which we empirically found to work better than regularizing the output of the predictor. We compare different settings in Table 4, based on the default data augmentation, optimization and architecture settings of the original BYOL, SimSiam and VICReg methods. In all settings, the absence of BN indicates that BN is also removed in the predictor when one is used.\n\nWe analyse first the impact of variance regularization (VR) in the different settings. When using VR, adding a predictor (PR) to VICReg does not lead to a significant change of the performance, which indicates that PR is redundant with VR. In comparison, without VR, the representations collapse, and both stop-gradient (SG) and PR are necessary. Batch normalization in the inner layers of the expander (BN) in VICReg leads to a 1.0% increase in the performance, which is not a big improvement considering that SG and PR without BN is performing very poorly at 35.1%. Finally, incorporating VR with SG or ME further improves the performance by small margins of respectively 0.2% and 0.9%, which might be explained by the fact that these architectural tricks that prevent collapse are not perfectly maintaining the variance of the representations, i.e. very slow collapse is happening with these methods. We explain this intuition by studying the evolution of the standard deviation of the representations during pretraining for BYOL and SimSiam in Appendix D.\n\nWe then analyse the impact of adding additional covariance regularization (CR) in the different settings, along with variance regularization. We found that optimization with SG and CR is hard, even if our analysis of the average correlation coefficient of the representations during pretraining in Appendix D shows that both fulfill the same objective.\n\nThe performance of BYOL and SimSiam slightly drops compared to VR only, except when PR is removed, where SG becomes useless. BN is still useful and improves the performance by 1.3%. Finally with CR, PR does not harm the performance and even improves it by a very small margin. VICReg+PR with 1000 epochs of pretraining exactly matches the score of VICReg (73.2% on linear classification).\n\nWeight sharing. Contrary to most self-supervised learning approaches based on Siamese architectures, VICReg has several unique properties: (1) weights do not need to be shared between the branches, each branch's weights are updated independently of the other branch's weights;\n\n(2) the branches are regularized independently, the variance and covariance terms are computed on each branch individually; (3) no predictor is necessary unlike with methods where one branch predicts outputs of the other branch. We compare the robustness of VICReg against other methods in different scenarios where the weights of the branches can be shared (SW), not shared (DW), and where the encoders can have different architectures (DA). Among other self-supervised methods, SimCLR and Barlow Twins are the only ones that can handle these scenarios. The asymmetric methods that are based on a discrepancy between the branches requires either the architecture or the weights to be shared between the branches. The performance drops by 2.1% with VICReg and 4.5% with Barlow Twins, between the shared weights scenario (SW) and the different weight scenario (DW). The difference between VICReg and Barlow Twins is also significant in scenarios with different architectures, in particular VICReg performs better than Barlow Twins by 2.8% with ResNet-50/ResNet-101 and better by 2.3% with ResNet-50/ViT-S Dosovitskiy et al. (2021). This shows that VICReg is more robust than Barlow Twins in these kind of scenarios. The performance of SimCLR remains stable across scenarios, but is significantly worse than the performance of VICReg. Importantly, the ability of VICReg to function with different parameters, architectures, and input modalities for the branches widens the applicability to joint-embedding SSL to many applications, including multi-modal signals.\n\n\nCONCLUSION\n\nWe introduced VICReg, a simple approach to self-supervised learning based on a triple objective: learning invariance to different views with a invariance term, avoiding collapse of the representations with a variance preservation term, and maximizing the information content of the representation with a covariance regularization term. VICReg achieves results on par with the state of the art on many downstream tasks, but is not subject to the same limitations as most other methods, particularly because it does not require the embedding branches to be identical or even similar. # covariance loss z_a = z_a -z_a.mean(dim=0) z_b = z_b -z_b.mean(dim=0) cov_z_a = (z_a.T @ z_a) / (N -1) cov_z_b = (z_b.T @ z_b) / (N -1) cov_loss = off_diagonal(cov_z_a).pow_(2).sum() / D + off_diagonal(cov_z_b).pow_(2).sum() / D # loss loss = lambda * sim_loss + mu * std_loss + nu * cov_loss # optimization step loss.backward() optimizer.step()\n\n\nB RELATION TO OTHER SELF-SUPERVISED METHODS\n\nWe compare here VICReg with other methods in terms of methodology, and we discuss the mechanisms used by these methods to avoid collapse and to learn representations, and how they relate to VICReg. We synthesize and illustrate the differences between these methods in Figure 2.\n\nRelation to Barlow Twins Zbontar et al. (2021). VICReg uses the same decorrelation mechanism as Barlow Twins, which consists in penalizing the off-diagonal terms of a covariance matrix computed on the embeddings. However, Barlow Twins uses the cross-correlation matrix where each entry in the matrix is a cross-correlation between two vectors z i and z j , from the two branches of the siamese architecture. Instead of using cross-correlations, we simply use the covariance matrix of each branch individually, and the variance term of VICReg allows us to get rid of standardization. Indeed, Barlow Twins forces the correlations between pairs of vectors z i and z i from the same dimension i to be 1. Without normalization, this target value of 1 becomes arbitrary and the vectors take values in a wider range. Moreover, there is an undesirable phenomenon happening in Barlow Twins, the embeddings before standardization can shrink and become constant to numerical precision, which could cause numerical instabilities. In practice, this is solved by adding a constant scalar in the denominator of standardization of the embeddings. Without normalization, VICReg naturally avoids this edge case.\n\n\nRelation to W-MSE Ermolov et al. (2021). The whitening operation of W-MSE consists in comput-\n\ning the inverse covariance matrix of the embeddings and use its square root as a whitening operator on the embeddings. Using this operator has two downsides. First, matrix inversion is a very costly and potentially unstable operation. VICReg does not need to inverse the covariance matrix. Second, as mentioned in Ermolov et al. (2021) the whitening operator is constructed over several consecutive iteration batches and therefore might have a high variance, which biases the estimation of the meansquared error. This issue is overcome in practice by a batch slicing strategy, where the whitening operator is computed over randomly constructed sub-batches. VICReg does not apply any operator on the embeddings, but instead regularizes the variance and covariance of the embeddings using an additional constraint.\n\nRelation to BYOL and SimSiam Grill et al. (2020); Chen & He (2020). The core components that avoid collapse in BYOL and SimSiam are the average moving weights and the stop-gradient operation on one side of their asymmetric architecture, which play the role of the repulsive term used in other methods. Our experiments in Appendix D.8 show that in addition to preventing collapse, these components also have a decorrelation effect. In addition, we have conducted the following experiment: We compute the correlation matrix of the final representations obtained with SimSiam, BYOL, VICReg and VICReg without covariance regularization. We measure the average correlation coefficient and observe that this coefficient is much smaller for SimSiam, BYOL and VICReg, compared to VICReg without covariance regularization. We observe in Figure 5 that even without covariance regularization, SimSiam and BYOL naturally minimize the average correlation coefficient of the representations. VICReg replaces the moving average weights and the stop-gradient operation, which are architectural trick that require some dependency between the branches, by an explicit constraint on the variance and the covariance of both embeddings separately, which achieves the same goal of decorrelating the representations and avoiding collapse, while being clearer, more interpretable, and working with independent branches. Contrastive and clustering based self-supervised algorithms rely on direct comparisons between elements of negative pairs. In the case of SimCLR, the negative pairs involve embeddings mined from the current batch, and large batch sizes are required. Despite the fact that SwAV computes clusters using elements in the current batch, it does not seem to have the same dependency on batch size. However, it still requires a lot of prototype vectors for negative comparisons between embeddings and codes. VICReg eliminates the negative comparisons and replace them by an explicit constraint on the variance of the embeddings, which efficiently plays the role of a negative term between the vectors. SwAV can also be interpreted as a distillation method, where a teacher network produces quantized vectors, used as target for a student network. Ensuring an equal partition of the quantized vectors in different bins or clusters effectively prevents collapse. OBOW can also be interpreted under the same framework. The embeddings are bag-of-words over a vocabulary of visual features, and collapse is avoided by the underlying quantization operation.  (2020); Zbontar et al. (2021). Two random crops from the input image are sampled and resized to 224 \u00d7 224, followed by random horizontal flip, color jittering of brightness, contrast, saturation and hue, Gaussian blur and random grayscale. Each crop is normalized in each color channel using the ImageNet mean and standard deviation pixel values. In more details, the exact set of augmentations is based on BYOL Grill et al. (2020) data augmentation pipeline but is symmetrised. The following operations are performed sequentially to produce each view:\n\n\nRelation to SimCLR, SwAV and OBoW\n(Z, Z\u2032) (a) VICReg (Z) X Y Z X\u2032 Y\u2032 Z\u2032 (Z\u2032) (Z) (Z\u2032) (b) Barlow Twins X Y Z X\u2032 Y\u2032 Z\u2032 (Z, Z\u2032) B-norm (Z, Z\u2032) \u2032 !\" ! \u210e # \u210e\u2032 #\" \u210e # \u210e # ! ! X Y Z X\u2032 Y\u2032 Z\u2032 (Z, Z\u2032) Batch slicing \u210e # \u210e # ! ! (c) W-MSE F-norm + PCA Batch slicing B-norm F-norm + PCA (Z, Z\u2032) (d) BYOL X Y Z X\u2032 Y\u2032 Z\u2032 ! \u210e # \u210e # ! ema ! ! ema F-norm $ P F-norm (e) SimSiam X Y Z X\u2032 Y\u2032 Z\u2032 \u210e ! \u210e ! \" \" X Y Z X\u2032 Y\u2032 Z\u2032 InfoNCE(Z, Z\u2032) \u210e ! \u210e ! \" \" (f) SimCLR F-norm F-norm X Y Z X\u2032 Y\u2032 Z\u2032 (Z, Z\u2032) \u210e ! \u210e ! \" \" (g) SwAV Quantization F-norm F-norm X Y Z X\u2032 Y # Z\u2032 ce(Z, Z\u2032) BoW \" (h) OBoW\n\u2022 Random cropping with an area uniformly sampled with size ratio between 0.08 to 1.0, followed by resizing to size 224 \u00d7 224. RandomResizedCrop(224, scale=(0.08, 0.1)) in PyTorch. \u2022 Random horizontal flip with probability 0.5.\n\n\u2022 Color jittering of brightness, contrast, saturation and hue, with probability 0.8.\n\nColorJitter ( \n\n\nC.2 IMAGENET EVALUATION\n\nLinear evaluation. We follow standard procedure and train a linear classifier on top of the frozen representations of a ResNet-50 pretrained with VICReg. We use the SGD optimizer with a learning rate of 0.02, a weight decay of 10 \u22126 , a batch size of 256, and train for 100 epochs. The learning rate follows a cosine decay. The training data augmentation pipeline is composed of random cropping and resize of ratio 0.2 to 1.0 with size 224 \u00d7 224, and random horizontal flips. During evaluation the validation images are simply center cropped and resized to 224 \u00d7 224.\n\nSemi-supervised evaluation. We train a linear classifier and fine-tune the representations using 1 and 10% of the labels. We use the SGD optimizer with no weight decay and a batch size of 256, and train for 20 epochs. We perform a grid search on the values of the encoder and linear head learning rates. In the 10% of labels case, we use a learning rate of 0.01 for the encoder and 0.1 for the linear head. In the 1% of labels case we use 0.03 for the encoder and 0.08 for the linear head. The two learning rates follow a cosine decay schedule. The training data and validation augmentation pipelines are identical to the linear evaluation data augmentation pipelines. R-CNN C-4 backbone for 24K iterations with a batch size of 16. The backbone is initialized with our pretrained ResNet-50 backbone. We use a learning rate of 0.1, divided by 10 at iteration 18K and 22K, a linear warmup with slope of 0.333 for 1000 iterations, and a region proposal network loss weight of 0.2. For COCO we use Mask R-CNN FPN backbone for 90K iterations with a batch size of 16, a learning rate of 0.04, divided by 10 at iteration 60K and 80K and with 50 warmup iterations.\n\n\nC.4 ANALYSIS\n\nWe give here implementation details on the results of Table 4 with BYOL and SimSiam, as well as the default setup for VICReg with 100 epochs of pretraining, used in all our ablations included in Appendix D. For both BYOL and SimSiam experiments, the variance criterion has coefficient \u00b5 = 1 and the covariance criterion has coefficient \u03bd = 0.01, the data augmentation pipeline and the architectures of the expander and predictor exactly follow the pipeline and architectures described in their paper. The linear evaluation setup of each methods follows closely the setup described in the original papers.\n\nBYOL setup. We use our own BYOL implementation in PyTorch, which outperforms the original implementation for 100 epochs of pretraining (69.3% accuracy on the linear evaluation protocol against 66.5% for the original implementation) and matches its performance for 1000 epochs of pretraining. We use the LARS optimizer You et al. (2017), with a learning rate of base_lr * batch_size/256 where base_lr = 0.45, and batch_size = 4096, a weight decay of 10 \u22126 , an eta value of 0.001 and a momentum of 0.9, for 100 epoch of pretraining with 10 epochs of warmup. The learning rate follows a cosine decay schedule. The initial value of the exponential moving average factor is 0.99 and follows a cosine decay schedule.\n\nSimSiam setup. We use our own implementation of SimSiam, which reproduces exactly the performance reported in the paper Chen & He (2020). We use SGD with a learning rate of base_lr * batch_size/256 where base_lr = 0.05, batch_size = 2048, with a weight decay of 0.0001 and a momentum of 0.9 for 100 epochs of pretraining and 10 epochs of warmup. The learning rate of the encoder and the expander follow a cosine decay schedule while the learning rate of the predictor is kept fixed.\n\nVICReg setup. The setting of VICReg's experiments is identical to the setting described in section 4.2, except that the number of pretraining epochs is 100 and the base learning rate is 0.3. The base learning rates used for the batch size study are 0.8, 0.5 and 0.4 for batch size 128, 256 and 512 respectively, and 0.3 for all other batch sizes. When a predictor is used, it has a similar architecture as the expander described in section 4.2, but with 2 layers instead of 3, which gives better results in practice. Table 9  (2020a) and multiple by 2 or 4 the number of filters in every convolutional layer, which also has the effect of multiplying the dimensionality of the representations. Second, as originally proposed in Zagoruyko & Komodakis (2016), we only multiply the number of filters in the bottleneck layers, which does not increases the dimensionality of the representations. We call this architecture Narrow ResNet (with prefix N-in Table 9). The main observation we make is the dependency of VICReg on the dimensionality of the representation. Using the narrow architecture, the performance of VICReg, jumps from 73.2% top-1 accuracy on linear classification with a ResNet-50, to 74.7% with Narrow ResNet-50 (x2), which is a 1.5% improvement and 76.0% with Narrow ResNet-50 (x4), which is a 2.8% improvement. We observe a similar trend going from ResNet-50 to , which is a 2.3% improvement but the performance completely saturates with ResNet-50 (x4), which is a 0.1% improvement over . Table 10 reports the performance of VICReg on semi-supervised classification with large ResNet architectures. VICReg combined with a ResNet-50 (x2) outperforms the current state-of-the-art methods BYOL and SimCLR, using this encoder architecture. Our largest  (2015), which is an environmental sound classification dataset with 50 classes. We jointly embedded a raw audio time-series representation on one branch, with its corresponding time-frequency representation on the other branch. We use the standard split of ESC-50 Piczak (2015), composed of 1600 training audio samples and 400 validation sample. The raw audio encoder is a 1-dimensional ResNet-18 with output dimension 384. The time-frequency image representation is the mel spectrogram with 1 channel of the raw audio, that we normalize between 0 and 1, and that is processed be a ResNet-18 with output dimension of 512. We use the AdamW optimizer with learning rate 0.0005 for 100 epochs of pretraining. Table 6 reports the performance of a linear classifier trained one the frozen representations obtained with VICReg and Barlow Twins to a simple supervised baseline where we train a ResNet-18 on the time-frequency representation in a supervised way. VICReg performs better by 5.7% than our supervised baseline, and better by 3.0% than Barlow Twins. We give more details in Appendix ??. Current best approaches that report around 95% accuracy on this task uses tricks such as heavy data augmentation or pretraining on larger audio and video datasets. With this experiment, our purpose is not to push the state of the art on ESC-50, but merely to demonstrate the applicability of VICReg to settings with multiple architectures and input modalities.\n\n\nD ADDITIONAL RESULTS\n\n\nD.1 OTHER RESNET ARCHITECTURES\n\n\nD.3 K-NEAREST-NEIGHBORS\n\nFollowing recent protocols Caron et al. (2020); Wu et al. (2018); Zhuang et al. (2019), we evaluate the learnt representations using K-nearest-neighbors classifiers built on the training set of ImageNet and evaluated on the validation set of ImageNet. We report the results with K=20 and K=200 in Table 11. VICReg performs slightly lower than other methods in the 20-NN case but remains competitive in the 200-NN case. These results with K-NN classifiers demonstrate the potential applicability of VICReg to downstream tasks based on nearest neighbors search, such as content retrieval in images or videos.\n\nD.4 LOSS FUNCTION COEFFICIENTS. Table 7 reports the performance for various values of the loss term coefficients in Eq. (6). Without variance regularization the representations immediately collapse to a single vector and the covariance term, which has no repulsive effect preventing collapse, has no impact. The invariance term is absolutely necessary and without it the network can not learn any good representations. By simply using the invariance term and variance regularization, which is a very simple baseline, VICReg still reaches an accuracy of 57.5%. These results show that variance and covariance regularizations have complementary effects, and that both are required.\n\nOn ImageNet, we choose the final coefficients the following way. First, we have empirically found that using very different values for \u03bb and \u00b5, or taking \u03bb = \u00b5 with \u03bd > \u00b5 leads to unstable training. On the other hand taking \u03bb = \u00b5 and picking \u03bd < \u00b5 leads to stable convergence, with the exact value picked for mu having very limited influence on the final linear classification accuracy. We have found  that setting lambda = mu = 25 and nu = 1 works best (by a small margin) for Imagenet but we have also obtained excellent results on MNIST and Cifar-10 and 100 using these exact same values. We could easily have tuned these parameters by cross-validation on the validation sets of these two smaller datasets.\n\n\nD.5 NORMALIZATIONS\n\nVICReg is the first self-supervised method for joint-embedding architectures we are aware of that does not require normalization. Contrary to SimSiam, W-MSE, SwAV and BYOL, and others, the embedding vectors are not projected on the unit sphere. Contrary to Barlow Twins, they are not standardized (equivalent to batch normalization without the adaptive parameters). Table 8 shows that the best settings do not involve any normalization of the embeddings, whether it is batch-wise or feature-wise (as in l 2 normalization). Whenever the embeddings are standardized (lines 3 and 5 in the table) the covariance matrix of Eq. (3) becomes the normalized auto-correlation matrix with coefficients between -1 and 1. This hurts the accuracy by 0.2%. We observe that when unconstrained, the coefficients in the covariance matrix take values in a wider range, which seems to facilitate the training process. Standardization is still an important component that helps stabilize the training when used in the hidden layers of the expander, and the performance drops by 1.2% when it is removed. Projecting the embeddings on the unit sphere implicitly constrains their standard deviation along the batch dimension to be 1/ \u221a d, where d is the dimension of the vectors. We change the invariance term of Eq. (5) to be the mean square error between l 2 -normalized vectors, and the target \u03b3 in the variance term of Eq. (1) is set to 1/ \u221a d instead of 1, forcing the standard deviation to get closer to 1/ \u221a d, and the vectors to be spread out on the unit sphere. This puts a lot more constraints on the network and the performance drops by 3.5%.      (2020) rely on a effective but difficult to interpret mechanism for preventing collapse, which may lead to instabilities during the training. We incorporate our variance regularization loss into BYOL and SimSiam and show that it helps stabilize the training and offers a small performance improvement. For both methods, the results are obtained using our own implementation and the exact same data augmentation and optimization settings as in their original paper. The variance and covariance regularization losses are incorporated with a factor of \u00b5 = 1 for variance and \u03bd = 0.01 for covariance. We report in Figure 3 the improvement obtained over these methods on the linear evaluation protocol for different number of pre-training epochs. For BYOL the improvement is of 0.9% with 100 epochs and becomes less significant as the number of pre-training epochs increases with a 0.2% improvement with 1000 epochs. This indicates that variance regularization makes BYOL converge faster. In SimSiam the improvement is not as significant. We plot in Figure 4 the evolution of the standard deviation computed along each dimension and averaged across the dimensions of the representation and the embeddings, during BYOL and SimSiam pretraining. For both methods, the standard deviation computed on the embeddings perfectly matches 1/ \u221a d where d is the dimension of the embeddings, which indicates that the embeddings are perfectly spread-out across the unit sphere. This translates in an increased standard deviation at the representation level, which seems to be correlated to the performance improvement. We finally study in Figure 5 the evolution of the average correlation coefficient, during pretraining of BYOL and SimSiam, with and without variance and covariance regularization. The average correlation coefficient is computed by averaging the off-diagonal coefficients of the \n1 2d(d \u2212 1) i =j C(Y ) 2 i,j + C(Y ) 2 i,j ,(9)\nwhere Y and Y are the standardized representations and C is defined in Eq. (3). In BYOL this coefficient is much lower using covariance regularization, which translate in a small improvement of the performance, according to Table 4. We do not observe the same improvement in SimSiam, both in terms of correlation coefficient, and in terms of performance on linear classification. The average correlation coefficient is correlated with the performance, which motivates the fact that decorrelation and redundancy reduction are core mechanisms for learning self-supervised representations.\n\n\nE RUNNING TIME\n\nWe report in Table 14, the running time of VICReg in comparison with other methods. All methods are run by us on 32 Tesla V100 GPUs. Each method offers a different trade-off between running time, memory and performance. SwAV is a very fast algorithm which use less memory and run faster than the other methods but with a lower performance, multi-crop helps the performance at the cost of additional compute and memory usage. BYOL has the highest memory requirement, which is due to the need of storing the target network weights. Finally, Barlow Twins and VICReg offer an interesting trade-off, consuming less memory than BYOL and SwAV with multi-crop, and running faster than SwAV with multi-crop, but with a slightly worse performance. The difference of 1h running time between Barlow Twins and VICReg is probably due to implementation details not related to the method.  \n\n\n; Chopra et al. (2005); He et al. (2020); Hjelm et al. (2019);\n\nFigure 1 :\n1VICReg: joint embedding architecture with variance, invariance and covariance regularization. Given a batch of images I, two batches of different views X and X are produced and are then encoded into representations Y and Y . The representations are fed to an expander producing the embeddings Z and Z . The distance between two embeddings from the same image is minimized, the variance of each embedding variable over a batch is maintained above a threshold, and the covariance between pairs of embedding variables over a batch are attracted to zero, decorrelating the variables from each other. Although the two branches do not require identical architectures nor share weights, in most of our experiments, they are Siamese with shared weights: the encoders areResNet-50 backbones with output dimension 2048. The expanders have 3 fully-connected layers of size 8192. alternative class of collapse prevention methods relies on maximizing the information content of the embedding Zbontar et al. (2021); Ermolov et al. (2021). These methods prevent informational collapse by decorrelating every pair of variables of the embedding vectors. This indirectly maximizes the information content of the embedding vectors. The Barlow Twins method drives the normalized cross-correlation matrix of the two embeddings towards the identity Zbontar et al. (2021), while the Whitening-MSE method whitens and spreads out the embedding vectors on the unit sphere Ermolov et al. (2021).\n\n\nrecent trends in self-supervised learning Caron et al. (2020); Grill et al. (2020); Chen & He (2020); Zbontar et al. (2021);\n\n\n, and is 0.0001 in Eq. (1). We give more details on how we choose the coefficients of the loss function in Appendix D.4. The encoder network f \u03b8 is a standard ResNet-50 backbone He et al. (2016) with 2048 output units. The expander h \u03c6 is composed of two fully-connected layers with batch normalization (BN) Ioffe & Szegedy(2015)and ReLU, and a third linear layer. The sizes of all 3 layers were set to 8192. As with Barlow Twins, performance improves when the size of the expander layers is larger than the dimension of the representation. The impact of the expander dimension on performance is studied in Appendix D. The training protocol follows those of BYOL and Barlow Twins: LARS optimizerYou et al. (2017); Goyal et al. (2017)  run for 1000 epochs with a weight decay of 10 \u22126 and a learning rate lr = batch_size/256 \u00d7 base_lr, where batch_size is set to 2048 by default and base_lr is a base learning rate set to 0.2. The learning rate follows a cosine decay scheduleLoshchilov & Hutter (2017), starting from 0 with 10 warmup epochs and with final value of 0.002.\n\n\nOTHER DOWNSTREAM TASKS Following the setup from Misra & Maaten (2020), we train a linear classifier on top of the frozen representations learnt by our pretrained ResNet-50 backbone on a variety of different datasets: the Places205 Zhou et al. (2014) scene classification dataset, the VOC07 Everingham et al. (2010) multi-label image classification dataset and the iNaturalist2018 Horn et al. (2018) fine-grained image classification dataset. We then evaluate the quality of the representations by transferring to other vision tasks including VOC07+12 Everingham et al. (2010) object detection using Faster R-CNN Ren et al. (2015) with a R50-C4 backbone, and COCO Lin et al. (2014) instance segmentation using Mask-R-CNN He et al. (2017) with a R50-FPN backbone.\n\n\nCaron et al. (2020); Chen et al. (2020a); Gidaris et al. (2021).\n\nFigure 2 :\n2Conceptual comparison between different self-supervised methods. The inputs X and X are fed to an encoder f with weights \u03b8. The representations Y and Y are further processed by a network h with weights \u03c8. h can be a projector (narrowing trapeze) that reduces the dimensionality of the representations, or an expander (widening trapeze) that increases their dimensionality. A criterion is finally applied on the embeddings Z and Z . VICReg (a) works when both branches have encoders f and f with different architectures and sets of weights \u03b8 and \u03b8 . Each branch's variance and covariance are regularized by regularizers v and c, and the distance between both branches is minimized with a mean-squared error loss s. Barlow Twins (b) uses a loss c to decorrelate pairs of different dimensions in the batch-wise normalized (B-Norm) embeddings, and learns invariance with a loss i that makes similar dimensions highly correlated. W-MSE (c) uses a batch slicing operation that shuffles batches into small sub-batches, and apply PCA as a whitening operation on the featurewise normalized (F-Norm) embeddings of each sub-batch. BYOL (d) has an asymmetric architecture where the weights \u03b8 m of one encoder are an exponential moving average (ema) of the other encoder's weights \u03b8. A predictor g with weights \u03c8 is used in the branch with learnable weights. SimSiam (e) uses a predictor on one branch and a stop-gradient operation (sg) on the other one. SimCLR (f) uses the InfoNCE contrastive loss where all the feature-wise normalized embeddings are compared between them inside a batch. Samples from distorted versions of the same input are brought close to each other, while other samples are pushed away. SwAV (g) quantizes the feature-wise normalized embeddings of a branch and use it as target for the other one. OBoW (h) uses bag-of-words (BoW) representations and a cross-entropy loss to compare the BoW generated by a teacher network from the feature maps Y F of the encoder, to the BoW predicted by a student network.Green blocks: parametric functions; yellow boxes: non-parametric functions; blue boxes: objective functions. the image augmentation protocol first introduced in SimCLR Chen et al. (2020a) and now commonly used by similar approaches based on siamese networks Caron et al. (2020); Grill et al. (2020); Chen & He\n\nC. 3\n3TRANSFER LEARNING We use the VISSL library Goyal et al. (2021) for linear classification tasks and the detectron2 library Wu et al. (2019) for object detection and segmentation tasks.Linear classification. We follow standard protocols Misra & Maaten (2020); Caron et al. (2020); Zbontar et al. (2021) and train linear models on top of the frozen representations. For VOC07 Everingham et al. (2010), we train a linear SVM with LIBLINEAR Fan et al. (2008). The images are center cropped and resized to 224 \u00d7 224, and the C values are computed with cross-validation. For Places205 Zhou et al. (2014) we use SGD with a learning rate of 0.003, a weight decay of 0.0001, a momentum of 0.9 and a batch size of 256, for 28 epochs. The learning rate is divided by 10 at epochs 4, 8 and 12. For Inaturalist2018 Horn et al. (2018), we use SGD with a learning rate of 0.005, a weight decay of 0.0001, a momentum of 0.9 and a batch size of 256, for 84 epochs. The learning rate is divided by 10 at epochs 24, 48 and 72. Object detection and instance segmentation. Following the setup of He et al. (2020); Zbontar et al. (2021), we use the trainval split of VOC07+12 with 16K images for training and a Faster\n\n\nreports the performance of VICReg on linear classification with large ResNet architectures. We focus on the wider family of ResNet Zagoruyko & Komodakis (2016) and aggregated ResNet Xie et al. (2017), and we consider two ways of widening a standard ResNet. First, we follow standard practice in recent self-supervised learning work Caron et al. (2020); Grill et al. (2020); Chen et al.\n\nFigure 3 :\n3Incorporating variance regularization in BYOL and SimSiam. Top-1 accuracy on the linear evaluation protocol for different number of pretraining epochs. For both methods pre-training follows the optimization and data augmentation protocol of their original paper but is based on our implementation. Var indicates variance regularization correlation matrix of the representations:\n\nFigure 4 :Figure 5 :\n45Standard deviation of the features during BYOL and SimSiam pretraining. Evolution of the average standard deviation of each dimension of the features with and without variance regularization (Var). left: the standard deviation is measured on the representations, right: the standard deviation is measured on the embeddings. Average correlation coefficient of the features during BYOL and SimSiam pretraining. Evolution of the average correlation coefficient measured by averaging the off-diagonal terms of the correlation matrix of the representations with BYOL, BYOL with variance-covariance regularization (BYOL VarCov), SimSiam, and SimSiam with variance-covariance regularization (SimSiam VarCov).\n\n\nContrastive learning. In contrastive SSL methods applied to joint embedding architectures, the output embeddings for a sample and its distorted version are brought close to each other, while other samples and their distortions are pushed away. The method is most often applied to Siamese architectures in which the two branches have identical architectures and share weights Misra & Maaten (2020); He et al. (2020); Bromley et al. (1994); Hjelm et al. (2019); Chen et al. Many authors use the InfoNCE loss van den Oord et al. (2018) in which the repulsive force is larger for contrastive samples that are closer to the reference. While these methods yield good performance, they require large amounts of contrastive pairs in order to work well. These contrastive pairs can be sampled from a memory bank as in MoCo He et al. (2020), or given by the current batch of data as in SimCLR Chen et al. (2020a), with a significant memory footprint. This downside of contrastive methods motivates a search for alternatives.Clustering methods. Instead of viewing each sample as its own class, clustering-based methods group them into clusters based on some similarity measure Caron et al. These clustering approaches can be viewed as contrastive learning at the level of clusters which still requires a lot of negative comparisons to work well.; Grill et al. (2020); Chen et al. (2020c), large batches of contrastive samples, as in \nSimCLR Chen et al. (2020a), or batch-wise and/or feature-wise normalization Caron et al. (2020); \nGrill et al. (2020); Chen & He (2020); Zbontar et al. (2021); Ermolov et al. (2021). One of the \nmost interesting feature of VICReg is the fact that the two branches are not required to share the \nsame parameters, architecture, or input modality. This opens the door to the use of non-contrastive \nself-supervised joint-embedding for multi-modal signals, such as video and audio. We demonstrate \nthe effectiveness of the proposed approach by evaluating the representations learned with VICReg on \nseveral downstream image recognition tasks including linear head and semi-supervised evaluation \nprotocols for image classification on ImageNet Deng et al. (2009), and other classification, detec-\ntion, instance segmentation, and retrieval tasks. Furthermore, we show that incorporating variance \npreservation into other self-supervised joint-embedding methods yields better training stability and \nperformance improvement on downstream tasks. More generally, we show that VICReg is an explicit \nand effective, yet simple method for preventing collapse in self-supervised joint-embedding learning. \n\n3 RELATED WORK \n\n(2020a;c); \nHadsell et al. (2006); Ye et al. (2019); Wu et al. (2018); van den Oord et al. (2018); Chen et al. \n(2020b). (2020; 2018); Bautista et al. \n(2016); Yang et al. (2016); Xie et al. (2016); Huang et al. (2019); Zhuang et al. (2019); Caron \net al. (2019); Asano et al. (2020); Yan et al. (2020). DeepCluster Caron et al. (2018) uses k-means \nassignments of representations from previous iterations as pseudo-labels for the new representations, \nwhich requires an expensive clustering phase done asynchronously, and makes the method hard \nto scale up. SwAV Caron et al. (2020) mitigates this issue by learning the clusters online while \nmaintaining a balanced partition of the assignments through the Sinkhorn-Knopp transform Cuturi \n(2013). Distillation methods. Recent proposals such as BYOL, SimSiam, OBoW and variants Grill et al. \n(2020); Chen & He (2020); Gidaris et al. (2021); Richemond et al. (2020); Gidaris et al. (2020) have \nshown that collapse can be avoided by using architectural tricks inspired by knowledge distillation \nHinton et al. \n\nTable 1 :\n1Evaluation on ImageNet.Evaluation of the representations obtained with a ResNet-50 \nbackbone pretrained with VICReg on: (1) linear classification on top of the frozen representations \nfrom ImageNet; (2) semi-supervised classification on top of the fine-tuned representations from \n1% and 10% of ImageNet samples. We report Top-1 and Top-5 accuracies (in %). Top-3 best \nself-supervised methods are underlined. \n\nLinear \nSemi-supervised \n\nMethod \nTop-1 Top-5 \nTop-1 \nTop-5 \n1% \n10% \n1% \n10% \n\nSupervised \n76.5 \n-\n25.4 56.4 48.4 80.4 \n\nMoCo He et al. (2020) \n60.6 \n-\n-\n-\n-\n-\nPIRL Misra & Maaten (2020) \n63.6 \n-\n-\n-\n57.2 83.8 \nCPC v2 H\u00e9naff et al. (2019) \n63.8 \n-\n-\n-\n-\n-\nCMC Tian et al. (2019) \n66.2 \n-\n-\n-\n-\n-\nSimCLR Chen et al. (2020a) \n69.3 \n89.0 \n48.3 65.6 75.5 87.8 \nMoCo v2 Chen et al. (2020c) \n71.1 \n-\n-\n-\n-\n-\nSimSiam Chen & He (2020) \n71.3 \n-\n-\n-\n-\n-\nSwAV Caron et al. (2020) \n71.8 \n-\n-\n-\n-\n-\nInfoMin Aug Tian et al. (2020) \n73.0 \n91.1 \n-\n-\n-\n-\nOBoW Gidaris et al. (2021) \n73.8 \n-\n-\n-\n82.9 90.7 \nBYOL Grill et al. (2020) \n74.3 \n91.6 \n53.2 68.8 78.4 89.0 \nSwAV (w/ multi-crop) Caron et al. (2020) \n75.3 \n-\n53.9 70.2 78.5 89.9 \nBarlow Twins Zbontar et al. (2021) \n73.2 \n91.0 \n55.0 69.7 79.2 89.3 \nVICReg (ours) \n73.2 \n91.1 \n54.8 69.5 79.4 89.5 \n\n5.1 EVALUATION ON IMAGENET \n\nFollowing the ImageNet Deng et al. (2009) linear evaluation protocol, we train a linear classifier \non top of the frozen representations of the ResNet-50 backbone pretrained with VICReg. \n\nTable 2 :\n2Transfer learning on downstream tasks. Evaluation of the representations from a ResNet-50 backbone pretrained with VICReg on: (1) linear classification tasks on top of frozen representations, we report Top-1 accuracy (in %) for Places205Zhou et al. (2014) and iNat18 Horn et al. (2018), and mAP for VOC07 Everingham et al.(2010);(2) object detection with fine-tunning, we report AP 50 for VOC07+12 using Faster R-CNN with C4 backboneRen et al. (2015); (3) object detection and instance segmentation, we report AP for COCOLin et al. (2014) using Mask R-CNN with FPN  backbone He et al. (2017. We use \u2020 to denote the experiments run by us. Top-3 best self-supervised methods are underlined.Linear Classification \nObject Detection \n\nMethod \nPlaces205 VOC07 iNat18 \nVOC07+12 COCO det COCO seg \n\nSupervised \n53.2 \n87.5 \n46.7 \n81.3 \n39.0 \n35.4 \n\nMoCo He et al. (2020) \n46.9 \n79.8 \n31.5 \n-\n-\n-\nPIRL Misra & Maaten (2020) \n49.8 \n81.1 \n34.1 \n-\n-\n-\nSimCLR Chen et al. (2020a) \n52.5 \n85.5 \n37.2 \n-\n-\n-\nMoCo v2 Chen et al. (2020c) \n51.8 \n86.4 \n38.6 \n82.5 \n39.8 \n36.1 \nSimSiam Chen & He (2020) \n-\n-\n-\n82.4 \n-\n-\nBYOL Grill et al. (2020) \n54.0 \n86.6 \n47.6 \n-\n40.4  \u2020 \n37.0  \u2020 \nSwAV (m-c) Caron et al. (2020) \n56.7 \n88.9 \n48.6 \n82.6 \n41.6 \n37.8 \nOBoW Gidaris et al. (2021) \n56.8 \n89.3 \n-\n82.9 \n-\n-\nBarlow Twins Grill et al. (2020) \n54.1 \n86.2 \n46.5 \n82.6 \n40.0  \u2020 \n36.7  \u2020 \nVICReg (ours) \n54.3 \n86.6 \n47.0 \n82.4 \n39.4 \n36.4 \n\n\n\nTable 3 :\n3Evaluation on MS-COCO 5K retrieval tasks. Comparison of VICReg with the contrastive loss of VSE++ Faghri et al. (2018), and with Barlow Twins, pretrain on the training set of MS-COCO.In all settings, the encoder for text is a word embedding followed by a GRU layer, the encoder for images is a ResNet-152.Method \nImage-to-text \nText-to-Image \nR@1 \nR@5 \nR@10 \nR@1 \nR@5 \nR@10 \n\nContrastive (VSE++) \n30.3 \n59.4 \n72.4 \n41.3 \n71.1 \n81.2 \nBarlow Twins \n31.4 \n60,4 \n75.1 \n42.9 \n74.0 \n83.5 \nVICReg \n33.6 \n62.7 \n77.9 \n45.2 \n76.1 \n84.2 \n\n\n\nTable 4 :\n4Effect of incorporating variance and covariance regularization in different methods.Top-1 ImageNet accuracy with the linear evaluation protocol after 100 pretraining epochs. For all methods, pretraining follows the architecture, the optimization and the data augmentation protocol of the original method using our reimplementation. ME: Momentum Encoder. SG: stop-gradient. PR: predictor. BN: Batch normalization layers after input and inner linear layers in the expander. No Reg: No additional regularization. Var Reg: Variance regularization. Var/Cov Reg: Variance and Covariance regularization. Unmodified original setups are marked by a \u2020.Method \nME \nSG \nPR \nBN \nNo Reg \nVar Reg \nVar/Cov Reg \n\nBYOL \n\n\n\n\n69.3  \u2020 \n70.2 \n69.5 \nSimSiam \n\n\n\n67.9  \u2020 \n68.1 \n67.6 \nSimSiam \n\n\n35.1 \n67.3 \n67.1 \nSimSiam \n\ncollapse \n56.8 \n66.1 \nVICReg \n\ncollapse \n56.2 \n67.3 \nVICReg \n\n\ncollapse \n57.1 \n68.7 \nVICReg \n\ncollapse \n57.5 \n68.6  \u2020 \nVICReg \ncollapse \n56.5 \n67.4 \n\n\n\nTable 5 :\n5Impact of sharing weights or not between branches. Top-1 accuracy on linear classification with 100 pretraining epochs. The encoder and expander of both branches can share the same architecture and share their weights (SW), share the same architecture with different weights (DW), or have different architectures (DA). The encoders can be ResNet-50, ResNet-101 or ViT-S.SW R50 DW R50 DA R50/R101 DA R50/ViT-S \n\nBYOL \n69.3 \n\n\n\nSimCLR \n64.4 \n63.1 \n63.9 \n63.5 \nBarlow Twins \n68.7 \n64.2 \n65.3 \n63.9 \nVICReg \n68.6 \n66.5 \n68.1 \n66.2 \n\n\n\nA ALGORITHM\nALGORITHMAlgorithm 1: VICReg pytorch pseudocode.# f: encoder network, lambda, mu, nu: coefficients of the \ninvariance, variance and covariance losses, N: batch size \n, D: dimension of the representations \n# mse_loss: Mean square error loss function, off_diagonal: \noff-diagonal elements of a matrix, relu: ReLU activation \nfunction \n\nfor x in loader: # load a batch with N samples \n# two randomly augmented versions of x \nx_a, x_b = augment(x) \n\n# compute representations \nz_a = f(x_a) # N x D \nz_b = f(x_b) # N x D \n\n# invariance loss \nsim_loss = mse_loss(z_a, z_b) \n\n# variance loss \nstd_z_a = torch.sqrt(z_a.var(dim=0) + 1e-04) \nstd_z_b = torch.sqrt(z_b.var(dim=0) + 1e-04) \nstd_loss = torch.mean(relu(1 -std_z_a)) + torch.mean( \nrelu(1 -std_z_b)) \n\n\n\nTable 6 :\n6Evaluation on ESC-50.Evaluation of the representations obtained with a ResNet-18 \nbackbone pretrained with VICReg on ESC-50 Piczak (2015) by processing jointly a raw audio \ntime-series and its corresponding time-frequency representation. The supervised baseline corresponds \nto a ResNet-18 trained on the time-frequency representation in a supervised way. We report Top-1 \naccuracy on the validation set (in %). \n\nMethod \nTop-1 \n\nSupervised baseline \n72.7 \nBarlow Twins \n75.4 \nVICReg \n78.4 \n\nmodel ResNet-200 (x2) performs lower than BYOL when 1% of the labels are used but is on par \nwith 10% of the labels. These results demonstrate the capabilities of VICReg to scale up when large \narchitectures are used. \n\nD.2 PRETRAINING AND EVALUATION ON ESC-50 AUDIO CLASSIFICATION \n\nWe demonstrate the ability of VICReg to function in a setting where the branches have different \narchitectures by pretraining on the ESC-50 audio dataset Piczak \n\nTable 7 :\n7Impactof variance-covariance regu-\nlarization. Inv: a invariance loss is used, \u03bb > 0, \nVar: variance regularization, \u00b5 > 0, Cov: covari-\nance regularization, \u03bd > 0, in Eq. (6). \n\nMethod \n\u03bb \u00b5 \u03bd Top-1 \n\nInv \n1 0 0 collapse \nInv + Cov \n25 0 1 collapse \nInv + Cov \n0 25 1 collapse \nInv + Var \n1 1 0 \n57.5 \n\nInv + Var + Cov (VICReg) 1 1 1 collapse \n1 10 1 collapse \n10 1 1 collapse \n5 5 1 \n68.1 \n10 10 1 \n68.2 \n25 25 1 \n68.6 \n50 50 1 \n68.3 \n\n\n\nTable 8 :\n8Impact of normalization.Std: vari-\n\n\nTable 9 :\n9Linear classification with large architectures. Top-1 accuracy comparison between different methods using various encoder architectures. For all VICReg results, the output dimensionality of the expander is 8192. N-R stands for Narrow ResNet, where only the bottleneck convolutional layers are widen.Method \nArch. \nParam. \nRepr. Top-1 Top-5 \n\nSimCLR Chen et al. (2020a) R50 (x2) \n93M \n4096 \n74.2 \n92.0 \nR50 (x4) \n375M \n8192 \n76.5 \n93.2 \n\nSwAV Caron et al. (2020) \nR50 (x2) \n93M \n4096 \n77.3 \n-\nR50 (x4) \n375M \n8192 \n77.9 \n-\nR50 (x5) \n586M 10240 \n78.5 \n-\n\nBYOL Grill et al. (2020) \nR50 (x2) \n93M \n4096 \n77.4 \n93.6 \nR50 (x4) \n375M \n8192 \n78.6 \n94.2 \nR200 (x2) \n250M \n4096 \n79.6 \n94.8 \n\nVICReg (ours) \nN-R50 (x2) \n66M \n2048 \n74.7 \n91.9 \nN-R50 (x4) \n221M \n2048 \n76.0 \n92.4 \nR50 (x2) \n93M \n4096 \n75.5 \n92.1 \nR50 (x4) \n375M \n8192 \n75.6 \n92.2 \nRNXT101-32-16 \n191M \n2048 \n76.1 \n92.3 \nR200 (x2) \n250M \n4096 \n77.3 \n93.3 \n\n\n\nTable 10 :\n10Semi-supervised classification with large architectures. Top-1 accuracy comparison between different methods using various encoder architectures. For all VICReg results, the output dimensionality of the expander is 8192.Method \nArch. \nParam. Repr. \nTop-1 \nTop-5 \n1% 10% 1% 10 % \n\nSimCLR Chen et al. (2020a) R50 (x2) \n93M 4096 58.5 71.7 83.0 91.2 \nR50 (x4) \n375M 8192 63.0 74.4 85.8 92.6 \n\nBYOL Grill et al. (2020) \nR50 (x2) \n93M 4096 62.2 73.5 84.1 91.7 \nR50 (x4) \n375M 8192 69.1 75.7 87.9 92.5 \nR200 (x2) \n250M 4096 71.2 77.7 89.5 93.7 \n\nVICReg (ours) \nR50 (x2) \n93M 4096 62.6 73.9 84.5 91.8 \nR200 (x2) \n250M 4096 68.8 77.3 88.2 93.6 \n\n\n\nTable 11 :\n11K-NN classifiers on ImageNet. Top-1 accuracy with 20 and 200 nearest neighbors.Method \n20-NN 200-NN \n\nNPID Wu et al. (2018) \n-\n46.5 \nLA Zhuang et al. (2019) \n-\n49.4 \nPCL Li et al. (2021) \n54.5 \n-\nBYOL Grill et al. (2020) \n66.7 \n64.9 \nSwAV Caron et al. (2020) \n65.7 \n62.7 \nBarlow Twins Zbontar et al. (2021) \n64.8 \n62.9 \nVICReg \n64.5 \n62.8 \n\n\nTable 12 :\n12Impact of expander dimensionality. Top-1 accuracy on the linear evaluation protocol with 100 pretraining epochs.Dimensionality \n256 \n512 \n1024 \n2048 \n4096 \n8192 \n16834 \n\nTop-1 \n55.9 \n59.2 \n62.4 \n65.1 \n67.3 \n68.6 \n68.8 \n\n\n\nTable 13 :\n13Impact of batch size. Top-1 accuracy on the linear evaluation protocol with 100 pretraining epochs.VICReg borrows the decorrelation mechanism of Barlow TwinsZbontar et al. (2021) and we observe that it therefore has the same dependency on the dimensionality of the expander network.Table 12reports the impact of the width and depth of the expander network. The dimensionality corresponds the number of hidden and output units in the expander network during pretraining. As the dimensionality increases, the performance dramatically increases from 55.9% top-1 accuracy on linear evaluation with a dimensionality of 256, to 68.8% with dimensionality 16384. The performance tends to saturate as the difference between dimensionality 8192 and 16384 is only of 0.2%.D.7 BATCH SIZEContrastive methods suffer from the need of a lot of negative examples which can translate into the need for very large batch sizesChen et al. (2020a).Table 13reports the performance on linear classification when the size of the batch varies between 128 and 4096. For each value of batch size, we perform a grid search on the base learning rate described in Appendix C.4. We observe a 0.7% and 1.2% drop in accuracy with small batch size of 256 and 128 which is comparable with the robustness to batch size of Barlow TwinsZbontar et al. (2021) andSimSiam Chen & He (2020), and a 0.8% drop with a batch size of 4096, which is reasonable and allows our method to be very easily parallelized on multiple GPUs.D.8 COMBINATION WITH BYOL AND SIMSIAM BYOL Grill et al. (2020) and SimSiam Chen & HeBatch size \n128 \n256 \n512 \n1024 \n2048 \n4096 \n\nTop-1 \n67.3 \n67.9 \n68.2 \n68.3 \n68.6 \n67.8 \n\nD.6 EXPANDER NETWORK ARCHITECTURE \n\n\n\nTable 14 :\n14Running time and peak memory. Comparison between different methods, the training is distributed on 32 Tesla V100 GPUs, the running time is measured over 100 epochs and the peak memory is measured on a single GPU. We report top-1 accuracy (%) on linear classification on top of the frozen representations.Methodtime / 100 epochs peak memory / GPU Top-1 accuracy (%)SwAV \n9h \n9.5G \n71.8 \nSwAV (w/ multi-crop) \n13h \n12.9G \n75.3 \nBYOL \n10h \n14.6G \n74.3 \nBarlow Twins \n12h \n11.3G \n73.2 \nVICReg \n11h \n11.3G \n73.2 \n\nAcknowledgement. Jean Ponce was supported in part by the French government under management of Agence Nationale de la Recherche as part of the \"Investissements d'avenir\" program, reference ANR-19-P3IA-0001 (PRAIRIE 3IA Institute), the Louis Vuitton/ENS Chair in Artificial Intelligence and the Inria/NYU collaboration. Adrien Bardes was supported in part by a FAIR/Prairie CIFRE PhD Fellowship. The authors wish to thank Jure Zbontar for the BYOL implementation, St\u00e9phane Deny for useful comments on the paper, and Li Jing, Yubei Chen, Mikael Henaff, Pascal Vincent and Geoffrey Zweig for useful discussions. We thank Quentin Duval and the VISSL team for help obtaining the results of table 2.\nSelf-labelling via simultaneous clustering and representation learning. Yuki Markus Asano, Christian Rupprecht, Andrea Vedaldi, ICLR. 2020Yuki Markus Asano, Christian Rupprecht, and Andrea Vedaldi. Self-labelling via simultaneous clustering and representation learning. In ICLR, 2020. 3\n\nLearning representations by maximizing mutual information across views. Philip Bachman, Devon Hjelm, William Buchwalter, NeurIPS. Philip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing mutual information across views. In NeurIPS, 2019. 1\n\nCliquecnn: Deep unsupervised exemplar learning. Miguel A Bautista, Artsiom Sanakoyeu, Ekaterina Sutter, Bj\u00f6rn Ommer, NeurIPS. Miguel A. Bautista, Artsiom Sanakoyeu, Ekaterina Sutter, and Bj\u00f6rn Ommer. Cliquecnn: Deep unsupervised exemplar learning. In NeurIPS, 2016. 3\n\nSignature verification using a \"siamese\" time delay neural network. Jane Bromley, Isabelle Guyon, Yann Lecun, Eduard Sackinger, Roopak Shah, NeurIPS. 13Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard Sackinger, and Roopak Shah. Signature verification using a \"siamese\" time delay neural network. In NeurIPS, 1994. 1, 3\n\nDeep clustering for unsupervised learning. Mathilde Caron, Piotr Bojanowski, Armand Joulin, Matthijs Douze, ECCV. 13Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsuper- vised learning. In ECCV, 2018. 1, 3\n\nUnsupervised pre-training of image features on non-curated data. Mathilde Caron, Piotr Bojanowski, Julien Mairal, Armand Joulin, ICCV. Mathilde Caron, Piotr Bojanowski, Julien Mairal, and Armand Joulin. Unsupervised pre-training of image features on non-curated data. In ICCV, 2019. 3\n\nUnsupervised learning of visual features by contrasting cluster assignments. Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, Armand Joulin, NeurIPS. 1820Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In NeurIPS, 2020. 1, 3, 4, 6, 7, 14, 16, 17, 18, 20\n\nA simple framework for contrastive learning of visual representations. Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey E Hinton, ICML. 2021Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for contrastive learning of visual representations. In ICML, 2020a. 1, 3, 4, 6, 7, 14, 16, 17, 20, 21\n\nBig self-supervised models are strong semi-supervised learners. Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, Geoffrey Hinton, NeurIPS. Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton. Big self-supervised models are strong semi-supervised learners. In NeurIPS, 2020b. 3\n\nExploring simple siamese representation learning. Xinlei Chen, Kaiming He, CVPR. 1721Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In CVPR, 2020. 1, 3, 4, 6, 7, 8, 14, 16, 17, 21\n\nSgdr: stochastic gradient descent with warm restarts. Ilya Loshchilov, Frank Hutter, ICLR. Ilya Loshchilov and Frank Hutter. Sgdr: stochastic gradient descent with warm restarts. In ICLR, 2017. 5\n\nSelf-supervised learning of pretext-invariant representations. Ishan Misra, Laurens Van Der Maaten, CVPR. 716Ishan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant representations. In CVPR, 2020. 1, 3, 6, 7, 16\n\nESC: Dataset for Environmental Sound Classification. Karol J Piczak, Proceedings of the 23rd Annual ACM Conference on Multimedia. the 23rd Annual ACM Conference on Multimedia18Karol J. Piczak. ESC: Dataset for Environmental Sound Classification. In Proceedings of the 23rd Annual ACM Conference on Multimedia, 2015. 18\n\nFaster r-cnn: Towards real-time object detection with region proposal networks. Kaiming Shaoqing Ren, Ross He, Jian Girshick, Sun, NeurIPS. 67Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In NeurIPS, 2015. 6, 7\n\nPierre H Richemond, Jean-Bastien Grill, Florent Altch\u00e9, Corentin Tallec, Florian Strub, Andrew Brock, Samuel Smith, Soham De, Razvan Pascanu, Bilal Piot, Michal Valko, arXiv:2010.10241Byol works even without batch statistics. 13arXiv preprintPierre H. Richemond, Jean-Bastien Grill, Florent Altch\u00e9, Corentin Tallec, Florian Strub, Andrew Brock, Samuel Smith, Soham De, Razvan Pascanu, Bilal Piot, and Michal Valko. Byol works even without batch statistics. arXiv preprint arXiv:2010.10241, 2020. 1, 3\n\nYonglong Tian, Dilip Krishnan, Phillip Isola, arXiv:1906.05849v4Contrastive multiview coding. arXiv preprintYonglong Tian, Dilip Krishnan, , and Phillip Isola. Contrastive multiview coding. arXiv preprint arXiv:1906.05849v4, 2019. 6\n\nWhat makes for good views for contrastive learning. Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, Phillip Isola, NeurIPS. 16Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What makes for good views for contrastive learning. In NeurIPS, 2020. 1, 6\n\nUnderstanding self-supervised learning dynamics without contrastive pairs. Yuandong Tian, Xinlei Chen, Surya Ganguli, arXiv:2102.06810arXiv preprintYuandong Tian, Xinlei Chen, and Surya Ganguli. Understanding self-supervised learning dynamics without contrastive pairs. arXiv preprint arXiv:2102.06810, 2021. 1\n\nAaron Van Den Oord, Yazhe Li, Oriol Vinyals, arXiv:1807.03748Representation learning with contrastive predictive coding. arXiv preprintAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. 3\n\n. Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, Ross Girshick, Detectron2, 16Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2. https://github.com/facebookresearch/detectron2, 2019. 16\n\nUnsupervised feature learning via nonparametric instance discrimination. Zhirong Wu, Yuanjun Xiong, Stella Yu, Dahua Lin, CVPR. 1820Zhirong Wu, Yuanjun Xiong, Stella Yu, , and Dahua Lin. Unsupervised feature learning via non- parametric instance discrimination. In CVPR, 2018. 3, 18, 20\n\nUnsupervised deep embedding for clustering analysis. Junyuan Xie, Ross Girshick, Ali Farhadi, ICML. Junyuan Xie, Ross Girshick, and Ali Farhadi. Unsupervised deep embedding for clustering analysis. In ICML, 2016. 3\n\nAggregated residual transformations for deep neural networks. Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, Kaiming He, CVPR. 17Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In CVPR, 2017. 17\n\nClusterfit: Improving generalization of visual representations. Xueting Yan, Ishan Misra, Abhinav Gupta, Deepti Ghadiyaram, Dhruv Mahajan, CVPR. 2020Xueting Yan, Ishan Misra, Abhinav Gupta, Deepti Ghadiyaram, and Dhruv Mahajan. Clusterfit: Improving generalization of visual representations. In CVPR, 2020. 3\n\nJoint unsupervised learning of deep representations and image clusters. Jianwei Yang, Devi Parikh, Dhruv Batra, CVPR. Jianwei Yang, Devi Parikh, and Dhruv Batra. Joint unsupervised learning of deep representations and image clusters. In CVPR, 2016. 3\n\nUnsupervised embedding learning via invariant and spreading instance feature. Mang Ye, Xu Zhang, C Pong, Shih-Fu Yuen, Chang, CVPR. Mang Ye, Xu Zhang, Pong C Yuen, and Shih-Fu Chang. Unsupervised embedding learning via invariant and spreading instance feature. In CVPR, 2019. 3\n\nLarge batch training of convolutional networks. Yang You, Igor Gitman, Boris Ginsburg, arXiv:1708.03888517arXiv preprintYang You, Igor Gitman, and Boris Ginsburg. Large batch training of convolutional networks. arXiv preprint arXiv:1708.03888, 2017. 5, 17\n\nWide residual networks. Sergey Zagoruyko, Nikos Komodakis, arXiv:1605.0714617arXiv preprintSergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016. 17\n\nBarlow twins: Self-supervised learning via redundancy reduction. Jure Zbontar, Li Jing, Ishan Misra, Yann Lecun, St\u00e9phane Deny, arxiv:2103.032301621arXiv preprintJure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and St\u00e9phane Deny. Barlow twins: Self-supervised learning via redundancy reduction. arXiv preprint arxiv:2103.03230, 2021. 1, 2, 3, 4, 5, 6, 14, 16, 20, 21\n\nLearning deep features for scene recognition using places database. Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, Aude Oliva, NeurIPS. 716Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude Oliva. Learning deep features for scene recognition using places database. In NeurIPS, 2014. 6, 7, 16\n\nLocal aggregation for unsupervised learning of visual embeddings. Chengxu Zhuang, Alex Lin Zhai, Daniel Yamins, ICCV. 1820Chengxu Zhuang, Alex Lin Zhai, and Daniel Yamins. Local aggregation for unsupervised learning of visual embeddings. In ICCV, 2019. 3, 18, 20\n", "annotations": {"author": "[{\"end\":164,\"start\":88},{\"end\":284,\"start\":165},{\"end\":382,\"start\":285},{\"end\":404,\"start\":383}]", "publisher": null, "author_last_name": "[{\"end\":101,\"start\":95},{\"end\":175,\"start\":170},{\"end\":295,\"start\":290},{\"end\":403,\"start\":395}]", "author_first_name": "[{\"end\":94,\"start\":88},{\"end\":169,\"start\":165},{\"end\":289,\"start\":285},{\"end\":391,\"start\":383},{\"end\":394,\"start\":392}]", "author_affiliation": "[{\"end\":163,\"start\":103},{\"end\":237,\"start\":177},{\"end\":283,\"start\":239},{\"end\":335,\"start\":297},{\"end\":381,\"start\":337}]", "title": "[{\"end\":85,\"start\":1},{\"end\":489,\"start\":405}]", "venue": null, "abstract": "[{\"end\":1613,\"start\":535}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1822,\"start\":1801},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":1845,\"start\":1824},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":1863,\"start\":1847},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":1883,\"start\":1865},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":1904,\"start\":1885},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":1925,\"start\":1906},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":1943,\"start\":1927},{\"end\":1966,\"start\":1945},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":1989,\"start\":1968},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2235,\"start\":2214},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2603,\"start\":2582},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2623,\"start\":2604},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2906,\"start\":2890},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2952,\"start\":2933},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3003,\"start\":2984},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3490,\"start\":3471},{\"end\":3519,\"start\":3495},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3751,\"start\":3732},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3776,\"start\":3753},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3846,\"start\":3830},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4087,\"start\":4064},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4107,\"start\":4089},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":5588,\"start\":5567},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6195,\"start\":6179},{\"end\":6407,\"start\":6401},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6596,\"start\":6577},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6709,\"start\":6693},{\"end\":6977,\"start\":6956},{\"end\":7003,\"start\":6982},{\"end\":7507,\"start\":7480},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7546,\"start\":7525},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8497,\"start\":8478},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":11748,\"start\":11727},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":14068,\"start\":14049},{\"end\":16947,\"start\":16927},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":24555,\"start\":24534},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":29185,\"start\":29164},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":33318,\"start\":33301},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":34935,\"start\":34907},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":37524,\"start\":37505},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":37542,\"start\":37526},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":37564,\"start\":37544},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":46930,\"start\":46912},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":47218,\"start\":47192},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":58615,\"start\":58593},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":58806,\"start\":58789},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":66385,\"start\":66364},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":67132,\"start\":67113},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":67525,\"start\":67504}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":44605,\"start\":44541},{\"attributes\":{\"id\":\"fig_1\"},\"end\":46087,\"start\":44606},{\"attributes\":{\"id\":\"fig_2\"},\"end\":46214,\"start\":46088},{\"attributes\":{\"id\":\"fig_3\"},\"end\":47288,\"start\":46215},{\"attributes\":{\"id\":\"fig_4\"},\"end\":48052,\"start\":47289},{\"attributes\":{\"id\":\"fig_5\"},\"end\":48119,\"start\":48053},{\"attributes\":{\"id\":\"fig_6\"},\"end\":50458,\"start\":48120},{\"attributes\":{\"id\":\"fig_8\"},\"end\":51660,\"start\":50459},{\"attributes\":{\"id\":\"fig_9\"},\"end\":52048,\"start\":51661},{\"attributes\":{\"id\":\"fig_10\"},\"end\":52440,\"start\":52049},{\"attributes\":{\"id\":\"fig_11\"},\"end\":53166,\"start\":52441},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":56864,\"start\":53167},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":58343,\"start\":56865},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":59766,\"start\":58344},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":60307,\"start\":59767},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":61270,\"start\":60308},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":61812,\"start\":61271},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":62579,\"start\":61813},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":63529,\"start\":62580},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":63979,\"start\":63530},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":64027,\"start\":63980},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":64950,\"start\":64028},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":65602,\"start\":64951},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":65957,\"start\":65603},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":66192,\"start\":65958},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":67898,\"start\":66193},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":68421,\"start\":67899}]", "paragraph": "[{\"end\":4121,\"start\":1629},{\"end\":4418,\"start\":4143},{\"end\":4489,\"start\":4420},{\"end\":4706,\"start\":4491},{\"end\":5002,\"start\":4708},{\"end\":5738,\"start\":5004},{\"end\":5890,\"start\":5740},{\"end\":5974,\"start\":5892},{\"end\":6104,\"start\":5976},{\"end\":7313,\"start\":6106},{\"end\":9759,\"start\":7315},{\"end\":10314,\"start\":9770},{\"end\":10866,\"start\":10316},{\"end\":10967,\"start\":10910},{\"end\":11631,\"start\":10991},{\"end\":11942,\"start\":11702},{\"end\":12489,\"start\":11977},{\"end\":12623,\"start\":12526},{\"end\":12986,\"start\":12684},{\"end\":13306,\"start\":13019},{\"end\":13503,\"start\":13333},{\"end\":15661,\"start\":13515},{\"end\":17001,\"start\":15714},{\"end\":17607,\"start\":17014},{\"end\":18832,\"start\":17609},{\"end\":19593,\"start\":18899},{\"end\":20654,\"start\":19595},{\"end\":21008,\"start\":20656},{\"end\":21398,\"start\":21010},{\"end\":21676,\"start\":21400},{\"end\":23238,\"start\":21678},{\"end\":24182,\"start\":23253},{\"end\":24507,\"start\":24230},{\"end\":25702,\"start\":24509},{\"end\":26612,\"start\":25800},{\"end\":29708,\"start\":26614},{\"end\":30505,\"start\":30279},{\"end\":30591,\"start\":30507},{\"end\":30607,\"start\":30593},{\"end\":31202,\"start\":30635},{\"end\":32360,\"start\":31204},{\"end\":32981,\"start\":32377},{\"end\":33694,\"start\":32983},{\"end\":34178,\"start\":33696},{\"end\":37394,\"start\":34180},{\"end\":38084,\"start\":37478},{\"end\":38765,\"start\":38086},{\"end\":39476,\"start\":38767},{\"end\":43012,\"start\":39499},{\"end\":43647,\"start\":43061},{\"end\":44540,\"start\":43666}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10909,\"start\":10867},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10990,\"start\":10968},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11701,\"start\":11632},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11976,\"start\":11943},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12525,\"start\":12490},{\"attributes\":{\"id\":\"formula_5\"},\"end\":12683,\"start\":12624},{\"attributes\":{\"id\":\"formula_6\"},\"end\":13018,\"start\":12987},{\"attributes\":{\"id\":\"formula_7\"},\"end\":18898,\"start\":18833},{\"attributes\":{\"id\":\"formula_8\"},\"end\":30278,\"start\":29745},{\"attributes\":{\"id\":\"formula_9\"},\"end\":43060,\"start\":43013}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":14302,\"start\":14295},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":14717,\"start\":14710},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":15490,\"start\":15483},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":16760,\"start\":16753},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":19359,\"start\":19352},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":32438,\"start\":32431},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":34704,\"start\":34697},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":35135,\"start\":35128},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":35691,\"start\":35683},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":36656,\"start\":36649},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":37783,\"start\":37775},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":38125,\"start\":38118},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":39872,\"start\":39865},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":43292,\"start\":43285},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":43687,\"start\":43679}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1627,\"start\":1615},{\"attributes\":{\"n\":\"2\"},\"end\":4141,\"start\":4124},{\"attributes\":{\"n\":\"4.1\"},\"end\":9768,\"start\":9762},{\"attributes\":{\"n\":\"4.2\"},\"end\":13331,\"start\":13309},{\"attributes\":{\"n\":\"5\"},\"end\":13513,\"start\":13506},{\"attributes\":{\"n\":\"5.2\"},\"end\":15675,\"start\":15664},{\"attributes\":{\"n\":\"5.3\"},\"end\":15712,\"start\":15678},{\"attributes\":{\"n\":\"6\"},\"end\":17012,\"start\":17004},{\"attributes\":{\"n\":\"7\"},\"end\":23251,\"start\":23241},{\"end\":24228,\"start\":24185},{\"end\":25798,\"start\":25705},{\"end\":29744,\"start\":29711},{\"end\":30633,\"start\":30610},{\"end\":32375,\"start\":32363},{\"end\":37417,\"start\":37397},{\"end\":37450,\"start\":37420},{\"end\":37476,\"start\":37453},{\"end\":39497,\"start\":39479},{\"end\":43664,\"start\":43650},{\"end\":44617,\"start\":44607},{\"end\":48131,\"start\":48121},{\"end\":50464,\"start\":50460},{\"end\":52060,\"start\":52050},{\"end\":52462,\"start\":52442},{\"end\":56875,\"start\":56866},{\"end\":58354,\"start\":58345},{\"end\":59777,\"start\":59768},{\"end\":60318,\"start\":60309},{\"end\":61281,\"start\":61272},{\"end\":61825,\"start\":61814},{\"end\":62590,\"start\":62581},{\"end\":63540,\"start\":63531},{\"end\":63990,\"start\":63981},{\"end\":64038,\"start\":64029},{\"end\":64962,\"start\":64952},{\"end\":65614,\"start\":65604},{\"end\":65969,\"start\":65959},{\"end\":66204,\"start\":66194},{\"end\":67910,\"start\":67900}]", "table": "[{\"end\":56864,\"start\":54503},{\"end\":58343,\"start\":56900},{\"end\":59766,\"start\":59044},{\"end\":60307,\"start\":60084},{\"end\":61270,\"start\":60962},{\"end\":61812,\"start\":61653},{\"end\":62579,\"start\":61874},{\"end\":63529,\"start\":62613},{\"end\":63979,\"start\":63548},{\"end\":64027,\"start\":64016},{\"end\":64950,\"start\":64339},{\"end\":65602,\"start\":65185},{\"end\":65957,\"start\":65696},{\"end\":66192,\"start\":66084},{\"end\":67898,\"start\":67772},{\"end\":68421,\"start\":68277}]", "figure_caption": "[{\"end\":44605,\"start\":44543},{\"end\":46087,\"start\":44619},{\"end\":46214,\"start\":46090},{\"end\":47288,\"start\":46217},{\"end\":48052,\"start\":47291},{\"end\":48119,\"start\":48055},{\"end\":50458,\"start\":48133},{\"end\":51660,\"start\":50466},{\"end\":52048,\"start\":51663},{\"end\":52440,\"start\":52062},{\"end\":53166,\"start\":52465},{\"end\":54503,\"start\":53169},{\"end\":56900,\"start\":56877},{\"end\":59044,\"start\":58356},{\"end\":60084,\"start\":59779},{\"end\":60962,\"start\":60320},{\"end\":61653,\"start\":61283},{\"end\":61874,\"start\":61835},{\"end\":62613,\"start\":62592},{\"end\":63548,\"start\":63542},{\"end\":64016,\"start\":63992},{\"end\":64339,\"start\":64040},{\"end\":65185,\"start\":64965},{\"end\":65696,\"start\":65617},{\"end\":66084,\"start\":65972},{\"end\":67772,\"start\":66207},{\"end\":68277,\"start\":67913}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13305,\"start\":13297},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":24506,\"start\":24498},{\"end\":27450,\"start\":27442},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":41751,\"start\":41743},{\"end\":42186,\"start\":42178},{\"end\":42762,\"start\":42754}]", "bib_author_first_name": "[{\"end\":69192,\"start\":69188},{\"end\":69199,\"start\":69193},{\"end\":69216,\"start\":69207},{\"end\":69234,\"start\":69228},{\"end\":69482,\"start\":69476},{\"end\":69497,\"start\":69492},{\"end\":69512,\"start\":69505},{\"end\":69736,\"start\":69730},{\"end\":69738,\"start\":69737},{\"end\":69756,\"start\":69749},{\"end\":69777,\"start\":69768},{\"end\":69791,\"start\":69786},{\"end\":70023,\"start\":70019},{\"end\":70041,\"start\":70033},{\"end\":70053,\"start\":70049},{\"end\":70067,\"start\":70061},{\"end\":70085,\"start\":70079},{\"end\":70323,\"start\":70315},{\"end\":70336,\"start\":70331},{\"end\":70355,\"start\":70349},{\"end\":70372,\"start\":70364},{\"end\":70596,\"start\":70588},{\"end\":70609,\"start\":70604},{\"end\":70628,\"start\":70622},{\"end\":70643,\"start\":70637},{\"end\":70894,\"start\":70886},{\"end\":70907,\"start\":70902},{\"end\":70921,\"start\":70915},{\"end\":70935,\"start\":70930},{\"end\":70948,\"start\":70943},{\"end\":70967,\"start\":70961},{\"end\":71288,\"start\":71284},{\"end\":71300,\"start\":71295},{\"end\":71320,\"start\":71312},{\"end\":71338,\"start\":71330},{\"end\":71340,\"start\":71339},{\"end\":71619,\"start\":71615},{\"end\":71631,\"start\":71626},{\"end\":71648,\"start\":71643},{\"end\":71666,\"start\":71658},{\"end\":71684,\"start\":71676},{\"end\":71926,\"start\":71920},{\"end\":71940,\"start\":71933},{\"end\":72140,\"start\":72136},{\"end\":72158,\"start\":72153},{\"end\":72347,\"start\":72342},{\"end\":72362,\"start\":72355},{\"end\":72580,\"start\":72575},{\"end\":72582,\"start\":72581},{\"end\":72929,\"start\":72922},{\"end\":72948,\"start\":72944},{\"end\":72957,\"start\":72953},{\"end\":73149,\"start\":73143},{\"end\":73151,\"start\":73150},{\"end\":73175,\"start\":73163},{\"end\":73190,\"start\":73183},{\"end\":73207,\"start\":73199},{\"end\":73223,\"start\":73216},{\"end\":73237,\"start\":73231},{\"end\":73251,\"start\":73245},{\"end\":73264,\"start\":73259},{\"end\":73275,\"start\":73269},{\"end\":73290,\"start\":73285},{\"end\":73303,\"start\":73297},{\"end\":73653,\"start\":73645},{\"end\":73665,\"start\":73660},{\"end\":73683,\"start\":73676},{\"end\":73939,\"start\":73931},{\"end\":73950,\"start\":73946},{\"end\":73959,\"start\":73956},{\"end\":73972,\"start\":73967},{\"end\":73991,\"start\":73983},{\"end\":74007,\"start\":74000},{\"end\":74273,\"start\":74265},{\"end\":74286,\"start\":74280},{\"end\":74298,\"start\":74293},{\"end\":74507,\"start\":74502},{\"end\":74527,\"start\":74522},{\"end\":74537,\"start\":74532},{\"end\":74795,\"start\":74790},{\"end\":74809,\"start\":74800},{\"end\":74829,\"start\":74820},{\"end\":74844,\"start\":74837},{\"end\":74853,\"start\":74849},{\"end\":75106,\"start\":75099},{\"end\":75118,\"start\":75111},{\"end\":75132,\"start\":75126},{\"end\":75142,\"start\":75137},{\"end\":75374,\"start\":75367},{\"end\":75384,\"start\":75380},{\"end\":75398,\"start\":75395},{\"end\":75599,\"start\":75592},{\"end\":75609,\"start\":75605},{\"end\":75625,\"start\":75620},{\"end\":75641,\"start\":75634},{\"end\":75653,\"start\":75646},{\"end\":75888,\"start\":75881},{\"end\":75899,\"start\":75894},{\"end\":75914,\"start\":75907},{\"end\":75928,\"start\":75922},{\"end\":75946,\"start\":75941},{\"end\":76206,\"start\":76199},{\"end\":76217,\"start\":76213},{\"end\":76231,\"start\":76226},{\"end\":76461,\"start\":76457},{\"end\":76468,\"start\":76466},{\"end\":76477,\"start\":76476},{\"end\":76491,\"start\":76484},{\"end\":76710,\"start\":76706},{\"end\":76720,\"start\":76716},{\"end\":76734,\"start\":76729},{\"end\":76945,\"start\":76939},{\"end\":76962,\"start\":76957},{\"end\":77180,\"start\":77176},{\"end\":77192,\"start\":77190},{\"end\":77204,\"start\":77199},{\"end\":77216,\"start\":77212},{\"end\":77232,\"start\":77224},{\"end\":77551,\"start\":77546},{\"end\":77563,\"start\":77558},{\"end\":77584,\"start\":77575},{\"end\":77598,\"start\":77591},{\"end\":77613,\"start\":77609},{\"end\":77881,\"start\":77874},{\"end\":77894,\"start\":77890},{\"end\":77898,\"start\":77895},{\"end\":77911,\"start\":77905}]", "bib_author_last_name": "[{\"end\":69205,\"start\":69200},{\"end\":69226,\"start\":69217},{\"end\":69242,\"start\":69235},{\"end\":69490,\"start\":69483},{\"end\":69503,\"start\":69498},{\"end\":69523,\"start\":69513},{\"end\":69747,\"start\":69739},{\"end\":69766,\"start\":69757},{\"end\":69784,\"start\":69778},{\"end\":69797,\"start\":69792},{\"end\":70031,\"start\":70024},{\"end\":70047,\"start\":70042},{\"end\":70059,\"start\":70054},{\"end\":70077,\"start\":70068},{\"end\":70090,\"start\":70086},{\"end\":70329,\"start\":70324},{\"end\":70347,\"start\":70337},{\"end\":70362,\"start\":70356},{\"end\":70378,\"start\":70373},{\"end\":70602,\"start\":70597},{\"end\":70620,\"start\":70610},{\"end\":70635,\"start\":70629},{\"end\":70650,\"start\":70644},{\"end\":70900,\"start\":70895},{\"end\":70913,\"start\":70908},{\"end\":70928,\"start\":70922},{\"end\":70941,\"start\":70936},{\"end\":70959,\"start\":70949},{\"end\":70974,\"start\":70968},{\"end\":71293,\"start\":71289},{\"end\":71310,\"start\":71301},{\"end\":71328,\"start\":71321},{\"end\":71347,\"start\":71341},{\"end\":71624,\"start\":71620},{\"end\":71641,\"start\":71632},{\"end\":71656,\"start\":71649},{\"end\":71674,\"start\":71667},{\"end\":71691,\"start\":71685},{\"end\":71931,\"start\":71927},{\"end\":71943,\"start\":71941},{\"end\":72151,\"start\":72141},{\"end\":72165,\"start\":72159},{\"end\":72353,\"start\":72348},{\"end\":72377,\"start\":72363},{\"end\":72589,\"start\":72583},{\"end\":72942,\"start\":72930},{\"end\":72951,\"start\":72949},{\"end\":72966,\"start\":72958},{\"end\":72971,\"start\":72968},{\"end\":73161,\"start\":73152},{\"end\":73181,\"start\":73176},{\"end\":73197,\"start\":73191},{\"end\":73214,\"start\":73208},{\"end\":73229,\"start\":73224},{\"end\":73243,\"start\":73238},{\"end\":73257,\"start\":73252},{\"end\":73267,\"start\":73265},{\"end\":73283,\"start\":73276},{\"end\":73295,\"start\":73291},{\"end\":73309,\"start\":73304},{\"end\":73658,\"start\":73654},{\"end\":73674,\"start\":73666},{\"end\":73689,\"start\":73684},{\"end\":73944,\"start\":73940},{\"end\":73954,\"start\":73951},{\"end\":73965,\"start\":73960},{\"end\":73981,\"start\":73973},{\"end\":73998,\"start\":73992},{\"end\":74013,\"start\":74008},{\"end\":74278,\"start\":74274},{\"end\":74291,\"start\":74287},{\"end\":74306,\"start\":74299},{\"end\":74520,\"start\":74508},{\"end\":74530,\"start\":74528},{\"end\":74545,\"start\":74538},{\"end\":74798,\"start\":74796},{\"end\":74818,\"start\":74810},{\"end\":74835,\"start\":74830},{\"end\":74847,\"start\":74845},{\"end\":74862,\"start\":74854},{\"end\":74874,\"start\":74864},{\"end\":75109,\"start\":75107},{\"end\":75124,\"start\":75119},{\"end\":75135,\"start\":75133},{\"end\":75146,\"start\":75143},{\"end\":75378,\"start\":75375},{\"end\":75393,\"start\":75385},{\"end\":75406,\"start\":75399},{\"end\":75603,\"start\":75600},{\"end\":75618,\"start\":75610},{\"end\":75632,\"start\":75626},{\"end\":75644,\"start\":75642},{\"end\":75656,\"start\":75654},{\"end\":75892,\"start\":75889},{\"end\":75905,\"start\":75900},{\"end\":75920,\"start\":75915},{\"end\":75939,\"start\":75929},{\"end\":75954,\"start\":75947},{\"end\":76211,\"start\":76207},{\"end\":76224,\"start\":76218},{\"end\":76237,\"start\":76232},{\"end\":76464,\"start\":76462},{\"end\":76474,\"start\":76469},{\"end\":76482,\"start\":76478},{\"end\":76496,\"start\":76492},{\"end\":76503,\"start\":76498},{\"end\":76714,\"start\":76711},{\"end\":76727,\"start\":76721},{\"end\":76743,\"start\":76735},{\"end\":76955,\"start\":76946},{\"end\":76972,\"start\":76963},{\"end\":77188,\"start\":77181},{\"end\":77197,\"start\":77193},{\"end\":77210,\"start\":77205},{\"end\":77222,\"start\":77217},{\"end\":77237,\"start\":77233},{\"end\":77556,\"start\":77552},{\"end\":77573,\"start\":77564},{\"end\":77589,\"start\":77585},{\"end\":77607,\"start\":77599},{\"end\":77619,\"start\":77614},{\"end\":77888,\"start\":77882},{\"end\":77903,\"start\":77899},{\"end\":77918,\"start\":77912}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":207930156},\"end\":69402,\"start\":69116},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":173990164},\"end\":69680,\"start\":69404},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":11416453},\"end\":69949,\"start\":69682},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":16394033},\"end\":70270,\"start\":69951},{\"attributes\":{\"id\":\"b4\"},\"end\":70521,\"start\":70272},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":199552270},\"end\":70807,\"start\":70523},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":219721240},\"end\":71211,\"start\":70809},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":211096730},\"end\":71549,\"start\":71213},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":219721239},\"end\":71868,\"start\":71551},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":227118869},\"end\":72080,\"start\":71870},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":14337532},\"end\":72277,\"start\":72082},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":208617491},\"end\":72520,\"start\":72279},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":17567398},\"end\":72840,\"start\":72522},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":10328909},\"end\":73141,\"start\":72842},{\"attributes\":{\"doi\":\"arXiv:2010.10241\",\"id\":\"b14\"},\"end\":73643,\"start\":73143},{\"attributes\":{\"doi\":\"arXiv:1906.05849v4\",\"id\":\"b15\"},\"end\":73877,\"start\":73645},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":218719252},\"end\":74188,\"start\":73879},{\"attributes\":{\"doi\":\"arXiv:2102.06810\",\"id\":\"b17\"},\"end\":74500,\"start\":74190},{\"attributes\":{\"doi\":\"arXiv:1807.03748\",\"id\":\"b18\"},\"end\":74786,\"start\":74502},{\"attributes\":{\"id\":\"b19\"},\"end\":75024,\"start\":74788},{\"attributes\":{\"id\":\"b20\"},\"end\":75312,\"start\":75026},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":6779105},\"end\":75528,\"start\":75314},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":8485068},\"end\":75815,\"start\":75530},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":208909958},\"end\":76125,\"start\":75817},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":8105340},\"end\":76377,\"start\":76127},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":102350974},\"end\":76656,\"start\":76379},{\"attributes\":{\"doi\":\"arXiv:1708.03888\",\"id\":\"b26\"},\"end\":76913,\"start\":76658},{\"attributes\":{\"doi\":\"arXiv:1605.07146\",\"id\":\"b27\"},\"end\":77109,\"start\":76915},{\"attributes\":{\"doi\":\"arxiv:2103.03230\",\"id\":\"b28\"},\"end\":77476,\"start\":77111},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":1849990},\"end\":77806,\"start\":77478},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":88523028},\"end\":78070,\"start\":77808}]", "bib_title": "[{\"end\":69186,\"start\":69116},{\"end\":69474,\"start\":69404},{\"end\":69728,\"start\":69682},{\"end\":70017,\"start\":69951},{\"end\":70313,\"start\":70272},{\"end\":70586,\"start\":70523},{\"end\":70884,\"start\":70809},{\"end\":71282,\"start\":71213},{\"end\":71613,\"start\":71551},{\"end\":71918,\"start\":71870},{\"end\":72134,\"start\":72082},{\"end\":72340,\"start\":72279},{\"end\":72573,\"start\":72522},{\"end\":72920,\"start\":72842},{\"end\":73929,\"start\":73879},{\"end\":75097,\"start\":75026},{\"end\":75365,\"start\":75314},{\"end\":75590,\"start\":75530},{\"end\":75879,\"start\":75817},{\"end\":76197,\"start\":76127},{\"end\":76455,\"start\":76379},{\"end\":77544,\"start\":77478},{\"end\":77872,\"start\":77808}]", "bib_author": "[{\"end\":69207,\"start\":69188},{\"end\":69228,\"start\":69207},{\"end\":69244,\"start\":69228},{\"end\":69492,\"start\":69476},{\"end\":69505,\"start\":69492},{\"end\":69525,\"start\":69505},{\"end\":69749,\"start\":69730},{\"end\":69768,\"start\":69749},{\"end\":69786,\"start\":69768},{\"end\":69799,\"start\":69786},{\"end\":70033,\"start\":70019},{\"end\":70049,\"start\":70033},{\"end\":70061,\"start\":70049},{\"end\":70079,\"start\":70061},{\"end\":70092,\"start\":70079},{\"end\":70331,\"start\":70315},{\"end\":70349,\"start\":70331},{\"end\":70364,\"start\":70349},{\"end\":70380,\"start\":70364},{\"end\":70604,\"start\":70588},{\"end\":70622,\"start\":70604},{\"end\":70637,\"start\":70622},{\"end\":70652,\"start\":70637},{\"end\":70902,\"start\":70886},{\"end\":70915,\"start\":70902},{\"end\":70930,\"start\":70915},{\"end\":70943,\"start\":70930},{\"end\":70961,\"start\":70943},{\"end\":70976,\"start\":70961},{\"end\":71295,\"start\":71284},{\"end\":71312,\"start\":71295},{\"end\":71330,\"start\":71312},{\"end\":71349,\"start\":71330},{\"end\":71626,\"start\":71615},{\"end\":71643,\"start\":71626},{\"end\":71658,\"start\":71643},{\"end\":71676,\"start\":71658},{\"end\":71693,\"start\":71676},{\"end\":71933,\"start\":71920},{\"end\":71945,\"start\":71933},{\"end\":72153,\"start\":72136},{\"end\":72167,\"start\":72153},{\"end\":72355,\"start\":72342},{\"end\":72379,\"start\":72355},{\"end\":72591,\"start\":72575},{\"end\":72944,\"start\":72922},{\"end\":72953,\"start\":72944},{\"end\":72968,\"start\":72953},{\"end\":72973,\"start\":72968},{\"end\":73163,\"start\":73143},{\"end\":73183,\"start\":73163},{\"end\":73199,\"start\":73183},{\"end\":73216,\"start\":73199},{\"end\":73231,\"start\":73216},{\"end\":73245,\"start\":73231},{\"end\":73259,\"start\":73245},{\"end\":73269,\"start\":73259},{\"end\":73285,\"start\":73269},{\"end\":73297,\"start\":73285},{\"end\":73311,\"start\":73297},{\"end\":73660,\"start\":73645},{\"end\":73676,\"start\":73660},{\"end\":73691,\"start\":73676},{\"end\":73946,\"start\":73931},{\"end\":73956,\"start\":73946},{\"end\":73967,\"start\":73956},{\"end\":73983,\"start\":73967},{\"end\":74000,\"start\":73983},{\"end\":74015,\"start\":74000},{\"end\":74280,\"start\":74265},{\"end\":74293,\"start\":74280},{\"end\":74308,\"start\":74293},{\"end\":74522,\"start\":74502},{\"end\":74532,\"start\":74522},{\"end\":74547,\"start\":74532},{\"end\":74800,\"start\":74790},{\"end\":74820,\"start\":74800},{\"end\":74837,\"start\":74820},{\"end\":74849,\"start\":74837},{\"end\":74864,\"start\":74849},{\"end\":74876,\"start\":74864},{\"end\":75111,\"start\":75099},{\"end\":75126,\"start\":75111},{\"end\":75137,\"start\":75126},{\"end\":75148,\"start\":75137},{\"end\":75380,\"start\":75367},{\"end\":75395,\"start\":75380},{\"end\":75408,\"start\":75395},{\"end\":75605,\"start\":75592},{\"end\":75620,\"start\":75605},{\"end\":75634,\"start\":75620},{\"end\":75646,\"start\":75634},{\"end\":75658,\"start\":75646},{\"end\":75894,\"start\":75881},{\"end\":75907,\"start\":75894},{\"end\":75922,\"start\":75907},{\"end\":75941,\"start\":75922},{\"end\":75956,\"start\":75941},{\"end\":76213,\"start\":76199},{\"end\":76226,\"start\":76213},{\"end\":76239,\"start\":76226},{\"end\":76466,\"start\":76457},{\"end\":76476,\"start\":76466},{\"end\":76484,\"start\":76476},{\"end\":76498,\"start\":76484},{\"end\":76505,\"start\":76498},{\"end\":76716,\"start\":76706},{\"end\":76729,\"start\":76716},{\"end\":76745,\"start\":76729},{\"end\":76957,\"start\":76939},{\"end\":76974,\"start\":76957},{\"end\":77190,\"start\":77176},{\"end\":77199,\"start\":77190},{\"end\":77212,\"start\":77199},{\"end\":77224,\"start\":77212},{\"end\":77239,\"start\":77224},{\"end\":77558,\"start\":77546},{\"end\":77575,\"start\":77558},{\"end\":77591,\"start\":77575},{\"end\":77609,\"start\":77591},{\"end\":77621,\"start\":77609},{\"end\":77890,\"start\":77874},{\"end\":77905,\"start\":77890},{\"end\":77920,\"start\":77905}]", "bib_venue": "[{\"end\":72696,\"start\":72652},{\"end\":69248,\"start\":69244},{\"end\":69532,\"start\":69525},{\"end\":69806,\"start\":69799},{\"end\":70099,\"start\":70092},{\"end\":70384,\"start\":70380},{\"end\":70656,\"start\":70652},{\"end\":70983,\"start\":70976},{\"end\":71353,\"start\":71349},{\"end\":71700,\"start\":71693},{\"end\":71949,\"start\":71945},{\"end\":72171,\"start\":72167},{\"end\":72383,\"start\":72379},{\"end\":72650,\"start\":72591},{\"end\":72980,\"start\":72973},{\"end\":73367,\"start\":73327},{\"end\":73737,\"start\":73709},{\"end\":74022,\"start\":74015},{\"end\":74263,\"start\":74190},{\"end\":74621,\"start\":74563},{\"end\":75152,\"start\":75148},{\"end\":75412,\"start\":75408},{\"end\":75662,\"start\":75658},{\"end\":75960,\"start\":75956},{\"end\":76243,\"start\":76239},{\"end\":76509,\"start\":76505},{\"end\":76704,\"start\":76658},{\"end\":76937,\"start\":76915},{\"end\":77174,\"start\":77111},{\"end\":77628,\"start\":77621},{\"end\":77924,\"start\":77920}]"}}}, "year": 2023, "month": 12, "day": 17}