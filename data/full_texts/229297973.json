{"id": 229297973, "updated": "2023-10-06 07:27:46.766", "metadata": {"title": "Taming Transformers for High-Resolution Image Synthesis", "authors": "[{\"first\":\"Patrick\",\"last\":\"Esser\",\"middle\":[]},{\"first\":\"Robin\",\"last\":\"Rombach\",\"middle\":[]},{\"first\":\"Bjorn\",\"last\":\"Ommer\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 12, "day": 17}, "abstract": "Designed to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to CNNs, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images. We show how to (i) use CNNs to learn a context-rich vocabulary of image constituents, and in turn (ii) utilize transformers to efficiently model their composition within high-resolution images. Our approach is readily applied to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information, such as segmentations, can control the generated image. In particular, we present the first results on semantically-guided synthesis of megapixel images with transformers and obtain the state of the art among autoregressive models on class-conditional ImageNet. Code and pretrained models can be found at https://github.com/CompVis/taming-transformers .", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2012.09841", "mag": "3111551570", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/EsserRO21", "doi": "10.1109/cvpr46437.2021.01268"}}, "content": {"source": {"pdf_hash": "75400de0e514abd0e98663513d348562772db36d", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2012.09841v3.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2012.09841", "status": "GREEN"}}, "grobid": {"id": "4875bf2383fb31c3b9a856a6891daf61167cf9ff", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/75400de0e514abd0e98663513d348562772db36d.txt", "contents": "\nTaming Transformers for High-Resolution Image Synthesis\n\n\nPatrick Esser \nHeidelberg Collaboratory for Image Processing\nIWR\nHeidelberg University\nGermany\n\nRobin Rombach \nHeidelberg Collaboratory for Image Processing\nIWR\nHeidelberg University\nGermany\n\nBj\u00f6rn Ommer \nHeidelberg Collaboratory for Image Processing\nIWR\nHeidelberg University\nGermany\n\nTaming Transformers for High-Resolution Image Synthesis\n*Both authors contributed equally to this work Figure 1. Our approach enables transformers to synthesize high-resolution images like this one, which contains 1280x460 pixels.\nDesigned to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to CNNs, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images. We show how to (i) use CNNs to learn a contextrich vocabulary of image constituents, and in turn (ii) utilize transformers to efficiently model their composition within high-resolution images. Our approach is readily applied to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information, such as segmentations, can control the generated image. In particular, we present the first results on semanticallyguided synthesis of megapixel images with transformers and obtain the state of the art among autoregressive models on class-conditional ImageNet. Code and pretrained models can be found at https://git.io/JnyvK.\n\nIntroduction\n\nTransformers are on the rise-they are now the de-facto standard architecture for language tasks [74,57,58,5] and are increasingly adapted in other areas such as audio [12] and vision [8,16]. In contrast to the predominant vision architecture, convolutional neural networks (CNNs), the transformer architecture contains no built-in inductive prior on the locality of interactions and is therefore free to learn complex relationships among its inputs. However, this generality also implies that it has to learn all relationships, whereas CNNs have been designed to exploit prior knowledge about strong local correlations within images. Thus, the increased expressivity of transformers comes with quadratically increasing computational costs, because all pairwise interactions are taken into account. The resulting energy and time requirements of state-of-the-art transformer models thus pose fundamental problems for scaling them to high-resolution images with millions of pixels.\n\nObservations that transformers tend to learn convolutional structures [16] thus beg the question: Do we have to re-learn everything we know about the local structure and regularity of images from scratch each time we train a vision model, or can we efficiently encode inductive image biases while still retaining the flexibility of transformers? We hypothesize that low-level image structure is well described by a local connectivity, i.e. a convolutional architecture, whereas this structural assumption ceases to be effective on higher semantic levels. Moreover, CNNs not only exhibit a strong locality bias, but also a bias towards spatial invariance through the use of shared weights across all positions. This makes them ineffective if a more holistic understanding of the input is required.\n\nOur key insight to obtain an effective and expressive model is that, taken together, convolutional and transformer architectures can model the compositional nature of our visual world [51]: We use a convolutional approach to efficiently learn a codebook of context-rich visual parts and, subsequently, learn a model of their global compositions. The long-range interactions within these compositions require an expressive transformer architecture to model distributions over their consituent visual parts. Furthermore, we utilize an adversarial approach to ensure that the dictionary of local parts captures perceptually important local structure to alleviate the need for modeling low-level statistics with the transformer architecture. Allowing transformers to concentrate on their unique strength-modeling long-range relations-enables them to generate high-resolution images as in Fig. 1, a feat which previously has been out of reach. Our formulationgives control over the generated images by means of conditioning information regarding desired object classes or spatial layouts. Finally, experiments demonstrate that our approach retains the advantages of transformers by outperforming previous codebook-based state-of-the-art approaches based on convolutional architectures.\n\n\nRelated Work\n\nThe Transformer Family The defining characteristic of the transformer architecture [74] is that it models interactions between its inputs solely through attention [2,36,52] which enables them to faithfully handle interactions between inputs regardless of their relative position to one another. Originally applied to language tasks, inputs to the transformer were given by tokens, but other signals, such as those obtained from audio [41] or images [8], can be used. Each layer of the transformer then consists of an attention mechanism, which allows for interaction between inputs at different positions, followed by a position-wise fully connected network, which is applied to all positions independently. More specifically, the (self-)attention mechanism can be described by mapping an intermediate representation with three position-wise linear layers into three representations, query Q \u2208 R N \u00d7d k , key K \u2208 R N \u00d7d k and value V \u2208 R N \u00d7dv , to compute the output as\nAttn(Q, K, V ) = softmax QK t \u221a d k V \u2208 R N \u00d7dv . (1)\nWhen performing autoregressive maximum-likelihood learning, non-causal entries of QK t , i.e. all entries below its diagonal, are set to \u2212\u221e and the final output of the transformer is given after a linear, point-wise transformation to predict logits of the next sequence element. Since the attention mechanism relies on the computation of inner products between all pairs of elements in the sequence, its computational complexity increases quadratically with the sequence length. While the ability to consider interactions between all elements is the reason transformers efficiently learn long-range interactions, it is also the reason transformers quickly become infeasible, especially on images, where the sequence length itself scales quadratically with the resolution. Different approaches have been proposed to reduce the computational requirements to make transformers feasible for longer sequences. [55] and [76] restrict the receptive fields of the attention modules, which reduces the expressivity and, especially for high-resolution images, introduces assumptions on the independence of pixels. [12] and [26] retain the full receptive field but can reduce costs for a sequence of length n only from n 2 to n \u221a n, which makes resolutions beyond 64 pixels still prohibitively expensive.\n\n\nConvolutional Approaches\n\nThe two-dimensional structure of images suggests that local interactions are particularly important. CNNs exploit this structure by restricting interactions between input variables to a local neighborhood defined by the kernel size of the convolutional kernel. Applying a kernel thus results in costs that scale linearly with the overall sequence length (the number of pixels in the case of images) and quadratically in the kernel size, which, in modern CNN architectures, is often fixed to a small constant such as 3 \u00d7 3. This inductive bias towards local interactions thus leads to efficient computations, but the wide range of specialized layers which are introduced into CNNs to handle different synthesis tasks [53,80,68,85,84] suggest that this bias is often too restrictive.\n\nConvolutional architectures have been used for autoregressive modeling of images [70,71,10] but, for lowresolution images, previous works [55,12,26] demonstrated that transformers consistently outperform their convolutional counterparts. Our approach allows us to efficiently model high-resolution images with transformers while retaining their advantages over state-of-the-art convolutional approaches.\n\nTwo-Stage Approaches Closest to ours are two-stage approaches which first learn an encoding of data and afterwards learn, in a second stage, a probabilistic model of this encoding. [13] demonstrated both theoretical and empirical evidence on the advantages of first learning a data representation with a Variational Autoencoder (VAE) [38,62], and then again learning its distribution with a VAE. [18,78] demonstrate similar gains when using an unconditional normalizing flow for the second stage, and [63,64] when using a conditional normalizing flow. To improve training efficiency of Generative Adversarial Networks (GANs), [43] learns a GAN [20] on representations of an autoencoder and [21] on low-resolution wavelet coefficients which are then Figure 2. Our approach uses a convolutional VQGAN to learn a codebook of context-rich visual parts, whose composition is subsequently modeled with an autoregressive transformer architecture. A discrete codebook provides the interface between these architectures and a patch-based discriminator enables strong compression while retaining high perceptual quality. This method introduces the efficiency of convolutional approaches to transformer based high resolution image synthesis. decoded to images with a learned generator.\n\n[72] presents the Vector Quantised Variational Autoencoder (VQVAE), an approach to learn discrete representations of images, and models their distribution autoregressively with a convolutional architecture. [61] extends this approach to use a hierarchy of learned representations. However, these methods still rely on convolutional density estimation, which makes it difficult to capture long-range interactions in high-resolution images. [8] models images autoregressively with transformers in order to evaluate the suitability of generative pretraining to learn image representations for downstream tasks. Since input resolutions of 32 \u00d7 32 pixels are still quite computationally expensive [8], a VQVAE is used to encode images up to a resolution of 192 \u00d7 192. In an effort to keep the learned discrete representation as spatially invariant as possible with respect to the pixels, a shallow VQVAE with small receptive field is employed. In contrast, we demonstrate that a powerful first stage, which captures as much context as possible in the learned representation, is critical to enable efficient highresolution image synthesis with transformers.\n\n\nApproach\n\nOur goal is to exploit the highly promising learning capabilities of transformer models [74] and introduce them to high-resolution image synthesis up to the megapixel range. Previous work [55,8] which applied transformers to image generation demonstrated promising results for images up to a size of 64 \u00d7 64 pixels but, due to the quadratically increasing cost in sequence length, cannot simply be scaled to higher resolutions.\n\nHigh-resolution image synthesis requires a model that understands the global composition of images, enabling it to generate locally realistic as well as globally consistent patterns. Therefore, instead of representing an image with pixels, we represent it as a composition of perceptually rich image constituents from a codebook. By learning an effective code, as described in Sec. 3.1, we can significantly reduce the description length of compositions, which allows us to efficiently model their global interrelations within images with a transformer architecture as described in Sec. 3.2. This approach, summarized in Fig. 2, is able to generate realistic and consistent high resolution images both in an unconditional and a conditional setting.\n\n\nLearning an Effective Codebook of Image Constituents for Use in Transformers\n\nTo utilize the highly expressive transformer architecture for image synthesis, we need to express the constituents of an image in the form of a sequence. Instead of building on individual pixels, complexity necessitates an approach that uses a discrete codebook of learned representations, such that any image x \u2208 R H\u00d7W \u00d73 can be represented by a spatial collection of codebook entries z q \u2208 R h\u00d7w\u00d7nz , where n z is the dimensionality of codes. An equivalent representation is a sequence of h \u00b7 w indices which specify the respective entries in the learned codebook. To effectively learn such a discrete spatial codebook, we propose to directly incorporate the inductive biases of CNNs and incorporate ideas from neural discrete representation learning [72]. First, we learn a convolutional model consisting of an encoder E and a decoder G, such that taken together, they learn to represent images with codes from a learned, discrete codebook Z = {z k } K k=1 \u2282 R nz (see Fig. 2 for an overview). More precisely, we approximate a given image x byx = G(z q ). We obtain z q using the encoding\u1e91 = E(x) \u2208 R h\u00d7w\u00d7nz and a subsequent element-wise quantization q(\u00b7) of each spatial code\u1e91 ij \u2208 R nz onto its closest codebook entry z k :\nz q = q(\u1e91) := arg min z k \u2208Z \u1e91 ij \u2212 z k \u2208 R h\u00d7w\u00d7nz . (2)\nThe reconstructionx \u2248 x is then given b\u0177\nx = G(z q ) = G (q(E(x))) .(3)\nBackpropagation through the non-differentiable quantization operation in Eq. (3) is achieved by a straight-through gradient estimator, which simply copies the gradients from the decoder to the encoder [3], such that the model and codebook can be trained end-to-end via the loss function\nL VQ (E, G, Z) = x \u2212x 2 + sg[E(x)] \u2212 z q 2 2 + sg[z q ] \u2212 E(x) 2 2 . (4)\nHere, L rec = x \u2212x 2 is a reconstruction loss, sg[\u00b7] denotes the stop-gradient operation, and sg[z q ] \u2212 E(x) 2 2 is the socalled \"commitment loss\" [72].\n\nLearning a Perceptually Rich Codebook Using transformers to represent images as a distribution over latent image constituents requires us to push the limits of compression and learn a rich codebook. To do so, we propose VQ-GAN, a variant of the original VQVAE, and use a discriminator and perceptual loss [40,30,39,17,47] to keep good perceptual quality at increased compression rate. Note that this is in contrast to previous works which applied pixelbased [71,61] and transformer-based autoregressive models [8] on top of only a shallow quantization model. More specifically, we replace the L 2 loss used in [72] for L rec by a perceptual loss and introduce an adversarial training procedure with a patch-based discriminator D [28] that aims to differentiate between real and reconstructed images:\nL GAN ({E, G, Z}, D) = [log D(x) + log(1 \u2212 D(x))] (5)\nThe complete objective for finding the optimal compression model Q * = {E * , G * , Z * } then reads\nQ * = arg min E,G,Z max D E x\u223cp(x) L VQ (E, G, Z) +\u03bbL GAN ({E, G, Z}, D) ,(6)\nwhere we compute the adaptive weight \u03bb according to\n\u03bb = \u2207 G L [L rec ] \u2207 G L [L GAN ] + \u03b4(7)\nwhere L rec is the perceptual reconstruction loss [81],\n\u2207 G L [\u00b7]\ndenotes the gradient of its input w.r.t. the last layer L of the decoder, and \u03b4 = 10 \u22126 is used for numerical stability. To aggregate context from everywhere, we apply a single attention layer on the lowest resolution. This training procedure significantly reduces the sequence length when unrolling the latent code and thereby enables the application of powerful transformer models.\n\n\nLearning the Composition of Images with Transformers\n\nLatent Transformers With E and G available, we can now represent images in terms of the codebook-indices of their encodings. More precisely, the quantized encoding of an image x is given by z q = q(E(x)) \u2208 R h\u00d7w\u00d7nz and is equivalent to a sequence s \u2208 {0, . . . , |Z|\u22121} h\u00d7w of indices from the codebook, which is obtained by replacing each code by its index in the codebook Z:\ns ij = k such that (z q ) ij = z k .(8)\nBy mapping indices of a sequence s back to their corresponding codebook entries, z q = z sij is readily recovered and decoded to an imagex = G(z q ). Thus, after choosing some ordering of the indices in s, image-generation can be formulated as autoregressive next-index prediction: Given indices s <i , the transformer learns to predict the distribution of possible next indices, i.e. p(s i |s <i ) to compute the likelihood of the full representation as p(s) = i p(s i |s <i ). This allows us to directly maximize the log-likelihood of the data representations:\nL Transformer = E x\u223cp(x) [\u2212 log p(s)] .(9)\nConditioned Synthesis In many image synthesis tasks a user demands control over the generation process by providing additional information from which an example shall be synthesized. This information, which we will call c, could be a single label describing the overall image class or even another image itself. The task is then to learn the likelihood of the sequence given this information c:\np(s|c) = i p(s i |s <i , c).(10)\nIf the conditioning information c has spatial extent, we first learn another VQGAN to obtain again an index-based representation r \u2208 {0, . . . , |Z c |\u22121} hc\u00d7wc with the newly obtained codebook Z c Due to the autoregressive structure of the transformer, we can then simply prepend r to s and restrict the computation of the negative log-likelihood to entries p(s i |s <i , r). This \"decoder-only\" strategy has also been successfully used for text-summarization tasks [44].\n\nGenerating High-Resolution Images The attention mechanism of the transformer puts limits on the sequence length h \u00b7 w of its inputs s. While we can adapt the number of downsampling blocks m of our VQGAN to reduce images of size H \u00d7 W to h = H/2 m \u00d7 w = W/2 m , we observe degradation of the reconstruction quality beyond a critical value of m, which depends on the considered dataset. To generate images in the megapixel regime, we therefore have to work patch-wise and crop images to restrict the length of s to a maximally feasible size during training. To sample images, we then use the transformer in a sliding-window manner as illustrated in Fig. 3. Our VQGAN ensures that the available context is still sufficient to faithfully model images, as long as either the statistics of the dataset are approximately spatially invariant or spatial conditioning information is available. In practice, this is not a restrictive requirement, because when it is violated, i.e. unconditional image synthesis on aligned data, we can simply condition on image coordinates, similar to [42].\n\n\nExperiments\n\nThis section evaluates the ability of our approach to retain the advantages of transformers over their convolutional counterparts (Sec. 4.1) while integrating the effectiveness of convolutional architectures to enable high-resolution image synthesis (Sec. 4.2). Furthermore, in Sec. 4.3, we investigate how codebook quality affects our approach. We close the analysis by providing a quantitative comparison to a wide range of existing approches for generative image synthesis in Sec. 4.4. Based on initial experiments, we usually set |Z|= 1024 and train all subsequent transformer models to predict sequences of length 16 \u00b7 16, as this is the maximum feasible length to train a GPT2-medium architecture (307 M parameters) [58] on a GPU with 12GB VRAM. More details on architectures and hyperparameters can be found in the appendix (Tab. 7 and Tab. 8).\n\n\nAttention Is All You Need in the Latent Space\n\nTransformers show state-of-the-art results on a wide variety of tasks, including autoregressive image modeling. However, evaluations of previous works were limited to transformers working directly on (low-resolution) pixels [55,12,26], or to deliberately shallow pixel encodings [8]. This raises the question if our approach retains the advantages of transformers over convolutional approaches.\n\nTo answer this question, we use a variety of conditional and unconditional tasks and compare the performance between our transformer-based approach and a convolutional approach. For each task, we train a VQGAN with m = 4 downsampling blocks, and, if needed, another one for the Negative Log-Likelihood (NLL) Data  conditioning information, and then train both a transformer and a PixelSNAIL [10] model on the same representations, as the latter has been used in previous state-of-the-art twostage approaches [61]. For a thorough comparison, we vary the model capacities between 85M and 310M parameters and adjust the number of layers in each model to match one another. We observe that PixelSNAIL trains roughly twice as fast as the transformer and thus, for a fair comparison, report the negative log-likelihood both for the same amount of training time (P-SNAIL time) and for the same amount of training steps (P-SNAIL steps).\n\nResults Tab. 1 reports results for unconditional image modeling on ImageNet (IN) [14], Restricted ImageNet (RIN) [65], consisting of a subset of animal classes from ImageNet, LSUN Churches and Towers (LSUN-CT) [79], and for conditional image modeling of RIN conditioned on depth maps obtained with the approach of [60] (D-RIN) and of landscape images collected from Flickr conditioned on semantic layouts (S-FLCKR) obtained with the approach of [7]. Note that for the semantic layouts, we train the first-stage using a cross-entropy reconstruction loss due to their discrete nature. The results shows that the transformer consistently outperforms PixelSNAIL across all tasks when trained for the same amount of time and the gap increases even further when trained for the same number of steps. These results demonstrate that gains of transformers carry over to our proposed two-stage setting.\n\n\nA Unified Model for Image Synthesis Tasks\n\nThe versatility and generality of the transformer architecture makes it a promising candidate for image synthesis. In the conditional case, additional information c such as class labels or segmentation maps are used and the goal is to learn the distribution of images as described in Eq. (10). Using the same setting as in Sec  Fig. 4 and Fig. 8, respectively. All of these examples make use of the same methodology. Instead of requiring task specific architectures or modules, the flexibility of the transformer allows us to learn appropriate interactions for each task, while the VQGAN -which can be reused across different tasks -leads to short sequence lengths. In combination, the presented approach can be understood as an efficient, general purpose mechanism for conditional image synthesis. Note that additional results for each experiment can be found in the appendix, Sec. D.  (Fig. 29-39). Note that this approach can in principle be used to generate images of arbitrary ratio and size, given that the image statistics of the dataset of interest are approximately spatially invariant or spatial information is available. Impressive results can be achieved by applying this method to image generation from semantic layouts on S-FLCKR, where a strong VQGAN can be learned with m = 5, so that its codebook together with the conditioning information provides the transformer with enough context for image generation in the megapixel regime.\n\n\nHigh-Resolution Synthesis\n\n\nBuilding Context-Rich Vocabularies\n\nHow important are context-rich vocabularies? To investigate this question, we ran experiments where the transformer architecture is kept fixed while the amount of context encoded into the representation of the first stage is varied through the number of downsampling blocks of our VQ-GAN. We specify the amount of context encoded in terms of reduction factor in the side-length between image inputs and the resulting representations, i.e. a first stage encoding images of size H \u00d7 W into discrete codes of size H/f \u00d7 W/f is denoted by a factor f . For f = 1, we reproduce the approach of [8] and replace our VQGAN by a k-means clustering of RGB values with k = 512. During training, we always crop images to obtain inputs of size 16 \u00d7 16 for the transformer, i.e. when modeling images with a factor f in the first stage, we use crops of size 16f \u00d7 16f . To sample from the models, we always apply them in a sliding window manner as described in Sec. 3.\n\nResults Fig. 7 shows results for unconditional synthesis of faces on FacesHQ, the combination of CelebA-HQ [31] and  To assess the effectiveness of our approach quantitatively, we compare results between training a transformer directly on pixels, and training it on top of a VQGAN's latent code with f = 2, given a fixed computational budget. Again, we follow [8] and learn a dictionary of 512 RGB values on CI-FAR10 to operate directly on pixel space and train the same transformer architecture on top of our VQGAN with a latent code of size 16 \u00d7 16 = 256. We observe improvements of 18.63% for FIDs and 14.08\u00d7 faster sampling of images.  \n\n\nBenchmarking Image Synthesis Results\n\nIn this section we investigate how our approach quantitatively compares to existing models for generative image synthesis. In particular, we assess the performance of our model in terms of FID and compare to a variety of established models (GANs, VAEs, Flows, AR, Hybrid). The results on semantic synthesis are shown in Tab. 2, where we compare to [53,75,35,9], and the results on unconditional face synthesis are shown in Tab. 3. While some task-specialized GAN models report better FID scores, our approach provides a unified model that works well across a wide range of tasks while retaining the ability to encode and reconstruct images. It thereby bridges the gap between purely adversarial and likelihood-based approaches.   Autoregressive models are typically sampled with a decoding strategy [27] such as beam-search, top-k or nucleus sampling. For most of our results, including those in Tab. 2, we use top-k sampling with k = 100 unless stated otherwise. For the results on face synthesis in Tab. 3, we computed scores for k \u2208 {100, 200, 300, 400, 500} and report the best results, obtained with k = 400 for CelebA-HQ and k = 300 for FFHQ. Fig. 10 in the supplementary shows FID and Inception scores as a function of k.\n\nClass-Conditional Synthesis on ImageNet To address a direct comparison with the previous state-of-the-art for autoregressive modeling of class-conditional image synthesis on ImageNet, VQVAE-2 [61], we train a class-conditional ImageNet transformer on 256 \u00d7 256 images, using a VQ-GAN with dim Z = 16384 and f = 16, and additionally compare to BigGAN [4], IDDPM [49], DCTransformer [48] and ADM [15] in Tab. 4. Note that our model uses 10\u00d7 less parameters than VQVAE-2, which has an estimated parameter count of 13.5B (estimate based on [67]). Samples of this model for different ImageNet classes are shown in Fig. 8. We observe that the adversarial training of the corresponding VQGAN enables sampling of highquality images with realistic textures, of comparable or higher quality than existing approaches such as BigGAN and VQVAE-2, see also Fig. 14-17   Quantitative results are summarized in Tab. 4. We report FID and Inception Scores for the best k/p in top-k/top-p sampling. Following [61], we can further increase quality via classifier-rejection, which keeps only the best m-outof-n samples in terms of the classifier's score, i.e. with an acceptance rate of m /n. We use a ResNet-101 classifier [22]. We observe that our model outperforms other autoregressive approaches (VQVAE-2, DCTransformer) in terms of FID and IS, surpasses BigGAN and IDDPM even for low rejection rates and yields scores close to the state of the art for higher rejection rates, see also Fig. 9.\n\nHow good is the VQGAN? Reconstruction FIDs obtained via the codebook provide an estimate on the achievable FID of the generative model trained on it. To quantify the per-    Table 5. FID on ImageNet between reconstructed validation split and original validation (FID/val) and training (FID/train) splits. * trained with Gumbel-Softmax reparameterization as in [59,29].\n\nformance gains of our VQGAN over discrete VAEs trained without perceptual and adversarial losses (e.g. VQVAE-2, DALL-E [59]), we evaluate this metric on ImageNet and report results in Tab. 5. Our VQGAN outperforms nonadversarial models while providing significantly more compression (seq. length of 256 vs. 5120 = 32 2 + 64 2 for VQVAE-2, 256 vs 1024 for DALL-E). As expected, larger versions of VQGAN (either in terms of larger codebook sizes or increased code lengths) further improve performance. Using the same hierarchical codebook setting as in VQVAE-2 with our model provides the best reconstruction FID, albeit at the cost of a very long and thus impractical sequence. The qualitative comparison corresponding to the results in Tab. 5 can be found in Fig. 12.\n\n\nConclusion\n\nThis paper adressed the fundamental challenges that previously confined transformers to low-resolution images. We proposed an approach which represents images as a composition of perceptually rich image constituents and thereby overcomes the infeasible quadratic complexity when modeling images directly in pixel space. Modeling constituents with a CNN architecture and their compositions with a transformer architecture taps into the full potential of their complementary strengths and thereby allowed us to represent the first results on high-resolution image synthesis with a transformer-based architecture. In experiments, our approach demonstrates the efficiency of convolutional inductive biases and the expressivity of transformers by synthesizing images in the megapixel range and outperforming state-of-the-art convolutional approaches. Equipped with a general mechanism for conditional synthesis, it offers many opportunities for novel neural rendering approaches.\n\nThis work has been supported by the German Research Foundation (DFG) projects 371923335, 421703927 and a hardware donation from NVIDIA corporation.\n\n\nTaming Transformers for High-Resolution\n\nImage Synthesis -\n\n\nSupplementary Material\n\nThe supplementary material for our work Taming Transformers for High-Resolution Image Synthesis is structured as follows: First, Sec. A summarizes changes to a previous version of this paper. In Sec. B, we present hyperparameters and architectures which were used to train our models. Next, extending the discussion of Sec. 4.3, Sec. C presents additional evidence for the importance of perceptually rich codebooks and its interpretation as a trade-off between reconstruction fidelity and sampling capability. Additional results on high-resolution image synthesis for a wide range of tasks are then presented in Sec. D, and Sec. E shows nearest neighbors of samples. Finally, Sec. F contains results regarding the ordering of image representations.\n\n\nA. Changelog\n\nWe summarize changes between this version 1 of the paper and its previous version 2 .\n\nIn the previous version, Eq. (4) had a weighting term \u03b2 on the commitment loss, and Tab. 8 reported a value of \u03b2 = 0.25 for all models. However, due to a bug in the implementation, \u03b2 was never used and all models have been trained with \u03b2 = 1.0. Thus, we removed \u03b2 in Eq. (4).\n\nWe updated class-conditional synthesis results on ImageNet in Sec To provide a better overview, we also include results from works that became available after the previous version of our work. Specifically, we include results on reconstruction quality of the VQVAE from [59] in Tab. 5 and Fig. 12 (which replaces the previous qualitative comparison), and results on class-conditional ImageNet sampling from [49,48,15] in Tab. 4. Note that with the exception of BigGAN and BigGAN-deep [4], no models or sampling results are available for the methods we compare to in Tab. 4. Thus, we can only report the numbers from the respective papers but cannot re-evaluate them with the same code. We follow the common evaluation protocol for class-conditional ImageNet synthesis from [4] and evaluate 50k samples from the model against the whole training split of ImageNet. However, it is not clear how different implementations resize the training images. In our code, we use the largest center-crop and resize it bilinearly with anti-aliasing to 256 \u00d7 256 using Pillow [73]. FID and Inception Scores are then computed with torch-fidelity [50].\n\nWe updated face-synthesis results in Tab. 3 based on a slightly different implementation as in the case of class-conditional ImageNet results and improve the previous results slightly. In addition, we evaluate the ability of our NLL-based training to detect overfitting. We train larger models (FFHQ (big) and CelebA-HQ (big) in Tab. 8) on the face datasets, and show nearest neighbors of samples obtained from checkpoints with the best NLL on the validation split and the training split in Sec. E. We also added Fig. 10, which visualizes the effect of tuning k in top-k sampling on FID and IS.\n\n\nB. Implementation Details\n\nThe hyperparameters for all experiments presented in the main paper and supplementary material can be found in Tab. 8. Except for the c-IN (big), COCO-Stuff and ADE20K models, these hyperparameters are set such that each transformer model can be trained with a batch-size of at least 2 on a GPU with 12GB VRAM, but we generally train on 2-4 GPUs with an accumulated VRAM of 48 GB. If hardware permits, 16-bit precision training is enabled.   Encoder Decoder Table 7. High-level architecture of the encoder and decoder of our VQGAN. The design of the networks follows the architecture presented in [25] with no skip-connections. For the discriminator, we use a patch-based model as in [28]. Note that h = H 2 m , w = W 2 m and f = 2 m .\nx \u2208 R H\u00d7W \u00d7C z q \u2208 R h\u00d7w\u00d7nz Conv2D \u2192 R H\u00d7W \u00d7C Conv2D \u2192 R h\u00d7w\u00d7C m\u00d7 { Residual Block, Downsample Block} \u2192 R h\u00d7w\u00d7C Residual Block \u2192 R h\u00d7w\u00d7C Residual Block \u2192 R h\u00d7w\u00d7C Non-Local Block \u2192 R h\u00d7w\u00d7C Non-Local Block \u2192 R h\u00d7w\u00d7C Residual Block \u2192 R h\u00d7w\u00d7C Residual Block \u2192 R h\u00d7w\u00d7C m\u00d7 { Residual Block, Upsample Block} \u2192 R H\u00d7W \u00d7C GroupNorm, Swish, Conv2D \u2192 R h\u00d7w\u00d7nz GroupNorm, Swish, Conv2D \u2192 R H\u00d7W \u00d7C\n\nVQGAN Architecture\n\nThe architecture of our convolutional encoder and decoder models used in the VQGAN experiments is described in Tab. 7. Note that we adopt the compression rate by tuning the number of downsampling steps m. Further note that \u03bb in Eq. 5 is set to zero in an initial warm-up phase. Empirically, we found that longer warm-ups generally lead to better reconstructions. As a rule of thumb, we recommend setting \u03bb = 0 for at least one epoch.\n\nTransformer Architecture Our transformer model is identical to the GPT2 architecture [58] and we vary its capacity mainly through varying the amount of layers (see Tab. 8). Furthermore, we generally produce samples with a temperature t = 1.0 and a top-k cutoff at k = 100 (with higher top-k values for larger codebooks).\n\n\nC. On Context-Rich Vocabularies\n\nSec. 4.3 investigated the effect of the downsampling factor f used for encoding images. As demonstrated in Fig. 7, large factors are crucial for our approach, since they enable the transformer to model long-range interactions efficiently. However, since larger f correspond to larger compression rates, the reconstruction quality of the VQGAN starts to decrease after a certain point, which is analyzed in Fig. 11. The left part shows the reconstruction error (measured by LPIPS [81]) versus the negative log-likelihood obtained by the transformer for values of f ranging from 1 to 64. The latter provides a measure of the ability to model the distribution of the image representation, which increases with f . The reconstruction error on the other hand decreases with f and the qualitative results on the right part show that beyond a critical value of f , in this case f = 16, reconstruction errors become severe. At this point, even when the image representations are modeled faithfully, as suggested by a low negative log-likelihood, sampled images are of low-fidelity, because the reconstruction capabilities provide an upper bound on the quality that can be achieved.\n\nHence, Fig. 11 shows that we must learn perceptually rich encodings, i.e. encodings with a large f and perceptually faithful reconstructions. This is the goal of our VQGAN and Fig. 12 Table 8. Hyperparameters. For every experiment, we set the number of attention heads in the transformer to n h = 16. n layer denotes the number of transformer blocks, # params the number of transformer parameters, nz the dimensionality of codebook entries, |Z| the number of codebook entries, dropout the dropout rate for training the transformer, length(s) the total length of the sequence, ne the embedding dimensionality and m the number of downsampling steps in the VQGAN. D-RINv1 is the experiment which compares to Pixel-SNAIL in Sec. 4.1. Note that the experiment (FacesHQ, f = 1) * does not use a learned VQGAN but a fixed k-means clustering algorithm as in [8] with K = 512 centroids. A prefix \"c\" refers to a class-conditional model. The models marked with a ' * * ' are trained on the same VQGAN.\n\nused in DALL-E [59]. We observe that for f = 8 and 8192 codebook entries, both the VQVAE and VQGAN capture the global structure faithfully. However, the textures produced by the VQVAE are blurry, whereas those of the VQGAN are crisp and realistic looking (e.g. the stone texture and the fur and tail of the squirrel). When we increase the compression rate of the VQGAN further to f = 16, we see that some reconstructed parts are not perfectly aligned with the input anymore (e.g. the paw of the squirrel), but, especially with slightly larger codebooks, the reconstructions still look realistic. This demonstrates how the VQGAN provides high-fidelity reconstructions at large factors, and thereby enables efficient high-resolution image synthesis with transformers.\n\nTo illustrate how the choice of f depends on the dataset, Fig. 13 presents results on S-FLCKR. In the left part, it shows, analogous to Fig. 7, how the quality of samples increases with increasing f . However, in the right part, it shows that reconstructions remain faithful perceptually faithful even for f 32, which is in contrast to the corresponding results on faces in Fig. 11. These results might be explained by a higher perceptual sensitivity to facial features as compared to textures, and allow us to generate high-resolution landscapes even more efficiently with f = 32.\n\n\nD. Additional Results\n\nQualitative Comparisons The qualitative comparison corresponding to Tab. 4 and Tab. 6 can be found in Fig. 14, 15, 16 and 17. Since no models are available for VQVAE-2 and MSP, we extracted results directly from the supplementary 3 and from the provided samples 4 , respectively. For BigGAN, we produced the samples via the provided model 5 . Similarly, the qualitative comparison with the best competitor model (SPADE) for semantic synthesis on standard benchmarks (see Tab. 2) can be found in Fig. 40 (ADE20K) and Fig. 41 (COCO-Stuff) 6 .\n\nComparison to Image-GPT To further evaluate the effectiveness of our approach, we compare to the state-of-the-art generative transformer model on images, ImageGPT [8]. By using immense amounts of compute the authors demonstrated that transformer models can be applied to the pixel-representation of images and thereby achieved impressive results both in representation learning and image synthesis. However, as their approach is confined to pixel-space, it does not scale beyond a resolution of 192 \u00d7 192. As our approach leverages a strong compression method to obtain context-rich representations of images and then learns a transformer model, we can synthesize images of much higher resolution. We compare both approaches in Fig. 27 and Fig. 28, where completions of images are depicted. Both plots show that our approach is able to synthesize consistent completions of dramatically increased fidelity. The results of [8] are obtained from https:// openai.com/blog/image-gpt/.\n\nAdditional High-Resolution Results Fig. 29, 30, 31 and Fig. 32 contain additional HR results on the S-FLCKR dataset for both f = 16 (m = 4) and f = 32 (m = 5) (semantically guided). In particular, we provide an enlarged version of Fig. 5 from the main text, which had to be scaled down due to space constraints. Additionally, we use our sliding window approach (see Sec. 3) to produce high-resolution samples for the depth-to-image setting on RIN in Fig. 33 \n\n\nE. Nearest Neighbors of Samples\n\nOne advantage of likelihood-based generative models over, e.g., GANs is the ability to evaluate NLL on training data and validation data to detect overfitting. To test this, we trained large models for face synthesis, which can easily overfit them, and retained two checkpoints on each dataset: One for the best validation NLL (at the 10th and 13th epoch for FFHQ and CelebA-HQ, respectively), and another for the best training NLL (at epoch 1000). We then produced samples from both checkpoints and retrieved nearest neighbors from the training data based on the LPIPS similarity metric [81]. The results are shown in Fig. 45, where it can be observed that the checkpoints with best training NLL (best train NLL) reproduce the training examples, whereas samples from the checkpoints with best validation NLL (best val. NLL) depict new faces which are not found in the training data.\n\nBased on these results, we can conclude that early-stopping based on validation NLL can prevent overfitting. Furthermore, the bottleneck for our approach on face synthesis is given by the dataset size since it has the capacity to almost perfectly fit the training data. Unfortunately, FID scores cannot detect such an overfitting. Indeed, the best train NLL checkpoints achieve FID scores of 3.86 on CelebA-HQ and 2.68 on FFHQ, compared to 10.2 and 9.6 for the best val. NLL checkpoints. While validation NLL provides a way to detect overfitting for likelihood-based models, it is not clear if early-stopping based on it is optimal if one is mainly interested in the quality of samples. To address this and the evaluation of GANs, new metrics will be required which can differentiate between models that produce new, high-quality samples and those that simply reproduce the training data.\n\nOur class-conditional ImageNet model does not display overfitting according to validation NLL, and the nearest neighbors shown in Fig. 46 also provide evidence that the model produces new, high-quality samples.\n\n\nF. On the Ordering of Image Representations\n\nFor the \"classical\" domain of transformer models, NLP, the order of tokens is defined by the language at hand. For images and their discrete representations, in contrast, it is not clear which linear ordering to use. In particular, our sliding-window approach depends on a row-major ordering and we thus investigate the performance of the following five different permutations of the input sequence of codebook indices: (i) row major, or raster scan order, where the image representation is unrolled from top left to bottom right. (ii) spiral out, which incorporates the prior assumption that most images show a centered object. (iii) z-curve, also known as z-order or morton curve, which introduces the prior of preserved locality when mapping a 2D image representation onto a 1D sequence. (iv) subsample, where prefixes correspond to subsampled representations, see also [46]. (v) alternate, which is related to row major, but alternates the direction of unrolling every row. (vi) spiral in, a reversed version of spiral out which provides the most context for predicting the center of the image. A graphical visualization of these permutation variants is shown in Fig. 47. Given a VQGAN trained on ImageNet, we train a transformer for each permutation in a controlled setting, i.e. we fix initialization and computational budget.\n\nResults Fig.47 depicts the evolution of negative log-likelihood for each variant as a function of training iterations, with final values given by (i) 4.767, (ii) 4.889, (iii) 4.810, (iv) 5.015, (v) 4.812, (vi) 4.901. Interestingly, row major performs best in terms of this metric, whereas the more hierarchical subsample prior does not induce any helpful bias. We also include qualitative samples in Fig. 48 and observe that the two worst performing models in terms of NLL (subsample and spiral in) tend to produce more textural samples, while the other variants synthesize samples with much more recognizable structures. Overall, we can conclude that the autoregressive codebook modeling is not permutation-invariant, but the common row major ordering [71,8] Figure 11. Trade-off between negative log-likelihood (nll) and reconstruction error. While context-rich encodings obtained with large factors f allow the transformer to effectively model long-range interactions, the reconstructions capabilities and hence quality of samples suffer after a critical value (here, f = 16). For more details, see Sec. C.               Figure 47. Top: All sequence permutations we investigate, illustrated on a 4 \u00d7 4 grid. Bottom: The transformer architecture is permutation invariant but next-token prediction is not: The average loss on the validation split of ImageNet, corresponding to the negative log-likelihood, differs significantly between different prediction orderings. Among our choices, the commonly used row-major order performs best.\n\n\nRow Major\n\nSubsample Z-Curve Spiral Out Alternating Spiral In Figure 48. Random samples from transformer models trained with different orderings for autoregressive prediction as described in Sec. F.\n\nFigure 3 .\n3Sliding attention window.\n\nFigure 4 .\n4. 4.1 (i.e. image size 256 \u00d7 256, latent size 16 \u00d7 16), we perform various conditional image synthesis experiments: Transformers within our setting unify a wide range of image synthesis tasks. We show 256 \u00d7 256 synthesis results across different conditioning inputs and datasets, all obtained with the same approach to exploit inductive biases of effective CNN based VQGAN architectures in combination with the expressivity of transformer architectures. Top row: Completions from unconditional training on ImageNet. 2nd row: Depth-to-Image on RIN. 3rd row: Semantically guided synthesis on ADE20K. 4th row: Pose-guided person generation on DeepFashion. Bottom row: Class-conditional samples on RIN. (i): Semantic image synthesis, where we condition on semantic segmentation masks of ADE20K [83], a webscraped landscapes dataset (S-FLCKR) and COCO-Stuff [6]. Results are depicted in Figure 4, 5 and Fig. 6. (ii): Structure-to-image, where we use either depth or edge information to synthesize images from both RIN and IN (see Sec. 4.1). The resulting depth-to-image and edge-toimage translations are visualized inFig. 4andFig. 6. (iii): Pose-guided synthesis: Instead of using the semantically rich information of either segmentation or depth maps,Fig. 4shows that the same approach as for the previous experiments can be used to build a shape-conditional generative model on the DeepFashion[45] dataset. (iv): Stochastic superresolution, where low-resolution images serve as the conditioning information and are thereby upsampled. We train our model for an upsampling factor of 8 on ImageNet and show results inFig. 6.(v): Class-conditional image synthesis: Here, the conditioning information c is a single index describing the class label of interest. Results for the RIN and IN dataset are demonstrated in\n\n\nThe sliding window approach introduced in Sec. 3.2 enables image synthesis beyond a resolution of 256 \u00d7 256 pixels. We evaluate this approach on unconditional image generation on LSUN-CT and FacesHQ (see Sec. 4.3) and conditional synthesis on D-RIN, COCO-Stuff and S-FLCKR, where we show results in Fig. 1, 6 and the supplementary\n\nFigure 5 .\n5Samples generated from semantic layouts on S-FLCKR. Sizes from top-to-bottom: 1280 \u00d7 832, 1024 \u00d7 416 and 1280 \u00d7 240 pixels. Best viewed zoomed in. A larger visualization can be found in the appendix, see Fig 29.\n\nFFHQ\n[33]. It clearly demonstrates the benefits of powerful VQGANs by increasing the effective receptive field of the transformer. For small receptive fields, or equivalently small f , the model cannot capture coherent structures. For an intermediate value of f = 8, the overall structure of images can be approximated, but inconsistencies of facial features such as a half-bearded face and of viewpoints in different parts of the image arise. Only our full setting of f = 16 can synthesize high-fidelity samples. For analogous results in the conditional setting on S-FLCKR, we refer to the appendix(Fig. 13and Sec. C).\n\nFigure 6 .\n6Applying the sliding attention window approach (Fig. 3) to various conditional image synthesis tasks. Top: Depth-to-image on RIN, 2nd row: Stochastic superresolution on IN, 3rd and 4th row: Semantic synthesis on S-FLCKR, bottom: Edge-guided synthesis on IN. The resulting images vary between 368 \u00d7 496 and 1024 \u00d7 576, hence they are best viewed zoomed in.\n\nFigure 7 .\n7Evaluating the importance of effective codebook for HQ-Faces (CelebA-HQ and FFHQ) for a fixed sequence length |s|= 16\u00b716 = 256. Globally consistent structures can only be modeled with a context-rich vocabulary (right). All samples are generated with temperature t = 1.0 and top-k sampling with k = 100. Last row reports the speedup over the f1 baseline which operates directly on pixels and takes 7258 seconds to produce a sample on a NVIDIA GeForce GTX Titan X.\n\nFigure 8 .\n8Samples from our class-conditional ImageNet model trained on 256 \u00d7 256 images.\n\nFigure 9 .\n9FID and Inception Score as a function of top-k, nucleus and rejection filtering.\n\n. 4 . 4 .\n44The previous results, included here in Tab. 6 for completeness, were based on a slightly different implementation where the transformer did not predict the distribution of the first token but used a histogram for it. The new model has been trained for 2.4 million steps with a batch size of 16 accumulated over 8 batches, which took 45.8 days on a single A100 GPU. The previous model had been trained for 1.0 million steps. Furthermore, the FID values were based on 50k (18k) samples against 50k (18k) training examples (to compare with MSP). For better comparison with other works, the current version reports FIDs based on 50k samples against all training examples of ImageNet using torch-fidelity [50]. We updated all qualitative figures showing samples from this model and added visualizations of the effect of tuning top-k/p or rejection rate in Fig. 14-26.\n\nFigure 10 .\n10FID and Inception Score as a function of top-k for CelebA-HQ (left) and FFHQ (right).\n\n\nand Fig. 34, edge-to-image on IN in Fig. 35, stochastic superresolution on IN in Fig. 36, more examples on semantically guided landscape synthesis on S-FLCKR in Fig. 37 with f = 16 and in Fig. 38 with f = 32, and unconditional image generation on LSUN-CT (see Sec. 4.1) in Fig. 39. Moreover, for images of size 256 \u00d7 256, we provide results for generation from semantic layout on (i) ADE20K in Fig. 40 and (ii) COCO-Stuff in Fig. 41, depth-to-image on IN in Fig. 42, pose-guided person generation in Fig. 43 and class-conditional synthesis on RIN in Fig. 44.\n\nFigure 12 .Figure 13 .\n1213Comparing reconstruction capabilities between VQVAEs and VQGANs. Numbers in parentheses denote compression factor and codebook size. With the same compression factor and codebook size, VQGANs produce more realistic reconstructions compared to blurry reconstructions of VQVAEs. This enables increased compression rates for VQGAN while retaining realistic reconstructions. See Sec. Samples on landscape dataset (left) obtained with different factors f , analogous toFig. 7. In contrast to faces, a factor of f = 32 still allows for faithful reconstructions (right). See also Sec. C.\n\nFigure 19 .Figure 20 .\n1920Visualizing the effect of varying k in top-k sampling (i.e. truncating the probability distribution per image token) by using a ResNet-101 classifier trained on ImageNet and samples from our class-conditional ImageNet model. Lower values of k produce more uniform, low-entropic images compared to samples obtained with full k. Here, an acceptance rate of 1.0 and p = 1.0 are fixed for all samples. Note that k = 973 is the effective size of the VQGAN's codebook, i.e. it describes how many entries of the codebook with dim Z = 16384 are actually used. Visualizing the effect of varying p in top-p sampling (or nucleus sampling[27]) by using a ResNet-101 classifier trained on ImageNet and samples from our class-conditional ImageNet model. Lowering p has similar effects as decreasing k, seeFig. 19. Here, an acceptance rate of 1.0 and k = 973 are fixed for all samples.\n\nFigure 21 .\n21Random samples on 256 \u00d7 256 class-conditional ImageNet with k \u2208 [100, 200, 250, 300, 350, 400, 500, 600, 800, 973] , p = 1.0, acceptance rate 1.0. FID: 17.04, IS: 70.6 \u00b1 1.8. Please see https://git.io/JLlvY for an uncompressed version.\n\nFigure 22 .\n22Random samples on 256 \u00d7 256 class-conditional ImageNet with k = 600, p = 1.0, acceptance rate 0.05. FID: 5.20, IS: 280.3 \u00b1 5.5. Please see https://git.io/JLlvY for an uncompressed version.\n\nFigure 23 .\n23Random samples on 256 \u00d7 256 class-conditional ImageNet with k = 250, p = 1.0, acceptance rate 1.0. FID: 15.98, IS: 78.6 \u00b1 1.1. Please see https://git.io/JLlvY for an uncompressed version.\n\nFigure 24 .\n24Random samples on 256 \u00d7 256 class-conditional ImageNet with k = 973, p = 0.88, acceptance rate 1.0. FID: 15.78, IS: 74.3 \u00b1 1.8. Please see https://git.io/JLlvY for an uncompressed version.\n\nFigure 25 .\n25Random samples on 256 \u00d7 256 class-conditional ImageNet with k \u2208 [100, 200, 250, 300, 350, 400, 500, 600, 800, 973] , p = 1.0, acceptance rate 0.005. FID: 6.59, IS: 402.7 \u00b1 2.9. Please see https://git.io/JLlvY for an uncompressed version.\n\nFigure 26 .Figure 27 .Figure 28 .\n262728Random samples on 256 \u00d7 256 class-conditional ImageNet with k \u2208 [100, 200, 250, 300, 350, 400, 500, 600, 800, 973] , p = 1.0, acceptance rate 0.05. FID: 5.88, IS: 304.8 \u00b1 3.6. Please see https://git.io/JLlvY for an uncompressed version.conditioning ours (top) vs iGPT[8] (bottom) Comparing our approach with the pixel-based approach of[8]. Here, we use our f = 16 S-FLCKR model to obtain high-fidelity image completions of the inputs depicted on the left (half completions). For each conditioning, we show three of our samples (top) and three of[8] (bottom). conditioning ours (top) vs iGPT[8] (bottom) Comparing our approach with the pixel-based approach of[8]. Here, we use our f = 16 S-FLCKR model to obtain high-fidelity image completions of the inputs depicted on the left (half completions). For each conditioning, we show three of our samples (top) and three of[8] (bottom).\n\nFigure 29 .\n29Samples generated from semantic layouts on S-FLCKR. Sizes from top-to-bottom: 1280 \u00d7 832, 1024 \u00d7 416 and 1280 \u00d7 240 pixels.\n\nFigure 30 .\n30Samples generated from semantic layouts on S-FLCKR. Sizes from top-to-bottom: 1536 \u00d7 512, 1840 \u00d7 1024, and 1536 \u00d7 620 pixels.\n\nFigure 31 .\n31Samples generated from semantic layouts on S-FLCKR. Sizes from top-to-bottom: 2048 \u00d7 512, 1460 \u00d7 440, 2032 \u00d7 448 and 2016 \u00d7 672 pixels.\n\nFigure 32 .Figure 33 .Figure 34 .Figure 35 .Figure 36 .Figure 37 .Figure 38 .\n32333435363738Samples generated from semantic layouts on S-FLCKR. Sizes from top-to-bottom: 1280 \u00d7 832, 1024 \u00d7 416 and 1280 \u00d7 240 pixels. conditioning samples Depth-guided neural rendering on RIN with f = 16 using the sliding attention window. conditioning samples Depth-guided neural rendering on RIN with f = 16 using the sliding attention window. Intentionally limiting the receptive field can lead to interesting creative applications like this one: Edge-to-Image synthesis on IN with f = 8, using the sliding attention window. conditioning samples Additional results for stochastic superresolution with an f = 16 model on IN, using the sliding attention window.conditioning samples Samples generated from semantic layouts on S-FLCKR with f = 16, using the sliding attention window. Samples generated from semantic layouts on S-FLCKR with f = 32, using the sliding attention window.\n\nFigure 39 .Figure 40 .Figure 41 .Figure 42 .Figure 43 .Figure 44 .Figure 45 .Figure 46 .\n3940414243444546Unconditional samples from a model trained on LSUN Churches & Towers, using the sliding attention window. Qualitative comparison to[53] on 256 \u00d7 256 images from the ADE20K dataset. Qualitative comparison to[53] on 256 \u00d7 256 images from the COCO-Stuff dataset. Conditional samples for the depth-to-image model on IN. Conditional samples for the pose-guided synthesis model via keypoints on DeepFashion. Samples produced by the class-conditional model trained on RIN. Nearest neighbors for our face-models trained on FFHQ and CelebA-HQ (256 \u00d7 256 pix), based on the LPIPS[82] distance. The left column shows a sample from our model, while the 10 examples to the right show the nearest neighbors from the corresponding class (increasing distance) in the training dataset. We evaluate two different model checkpoints for each dataset: Best val. NLL denotes the minimal NLL over the course of training, evaluated on unseen testdata. For this checkpoint, both models generate crisp, high-quality samples not present in the training data. However, when drastically overfitting the model, it reproduces samples from the training data (best train NLL). Although not an ideal measure of image quality, NLL thus provides a proxy on model selection, whereas FID does not. See also Sec. E. Nearest neighbors for our class-conditional ImageNet model (256 \u00d7 256 pix), based on the LPIPS[82] distance. The left column shows a sample from our model, while the 10 examples to the right show the nearest neighbors from the corresponding class (increasing distance) in the training dataset. Our model produces new, unseen high-quality images, not present in the training data.\n\nTable 3 .\n3FID score comparison for face image synthesis. CelebA-HQ results reproduced from[1,54,77,24], FFHQ from[66,32].\n\n\nin the supplementary.Model \nacceptance rate \nFID \nIS \n\nmixed k, p = 1.0 \n1.0 \n17.04 \n70.6 \u00b1 1.8 \nk = 973, p = 1.0 \n1.0 \n29.20 \n47.3 \u00b1 1.3 \nk = 250, p = 1.0 \n1.0 \n15.98 \n78.6 \u00b1 1.1 \nk = 973, p = 0.88 \n1.0 \n15.78 \n74.3 \u00b1 1.8 \nk = 600, p = 1.0 \n0.05 \n5.20 \n280.3 \u00b1 5.5 \n\nmixed k, p = 1.0 \n0.5 \n10.26 \n125.5 \u00b1 2.4 \nmixed k, p = 1.0 \n0.25 \n7.35 \n188.6 \u00b1 3.3 \nmixed k, p = 1.0 \n0.05 \n5.88 \n304.8 \u00b1 3.6 \nmixed k, p = 1.0 \n0.005 \n6.59 \n402.7 \u00b1 2.9 \n\nDCTransformer [48] \n1.0 \n36.5 \nn/a \nVQVAE-2 [61] \n1.0 \n\u223c31 \n\u223c45 \nVQVAE-2 \nn/a \n\u223c10 \n\u223c330 \nBigGAN [4] \n1.0 \n7.53 \n168.6 \u00b1 2.5 \nBigGAN-deep \n1.0 \n6.84 \n203.6 \u00b1 2.6 \nIDDPM [49] \n1.0 \n12.3 \nn/a \nADM-G, no guid. [15] \n1.0 \n10.94 \n100.98 \nADM-G, 1.0 guid. \n1.0 \n4.59 \n186.7 \nADM-G, 10.0 guid. \n1.0 \n9.11 \n283.92 \n\nval. data \n1.0 \n1.62 \n234.0 \u00b1 3.9 \n\n\n\nTable 4 .\n4FID score comparison for class-conditional synthesis \non 256 \u00d7 256 ImageNet, evaluated between 50k samples and the \ntraining split. Classifier-based rejection sampling as in VQVAE-2 \nuses a ResNet-101 [22] classifier. BigGAN(-deep) evaluated via \nhttps://tfhub.dev/deepmind truncated at 1.0. \"Mixed\" \nk refers to samples generated with different top-k values, here k \u2208 \n{100, 200, 250, 300, 350, 400, 500, 600, 800, 973}. \n\n\n\nTable 6 .\n6Results from a previous version of this paper, see also Sec. A. Left: Previous results on class-conditional ImageNet synthesis with a slightly different implementation and evaluated against 50k and 18k training examples instead of the whole training split. See Tab. 4 for new, improved results evaluated against the whole training split. Right: Previous results on face-synthesis with a slightly different implementation compared to the new implementation. See also Tab. 3 for comparison with other methods.\n\n\ncompares its reconstruction capabilities against the VQVAE[72] Experiment \n\nn layer # params [M ] n z \n|Z| \ndropout length(s) \nn e \nm \n\nRIN \n12 \n85 \n64 \n768 \n0.0 \n512 \n1024 4 \nc-RIN \n18 \n128 \n64 \n768 \n0.0 \n257 \n768 \n4 \nD-RINv1 \n14 \n180 \n256 1024 \n0.0 \n512 \n768 \n4 \nD-RINv2 \n24 \n307 \n256 1024 \n0.0 \n512 \n1024 4 \nIN \n24 \n307 \n256 1024 \n0.0 \n256 \n1024 4 \nc-IN \n24 \n307 \n256 1024 \n0.0 \n257 \n1024 4 \nc-IN (big) \n48 \n1400 \n256 16384 \n0.0 \n257 \n1536 4 \nIN-Edges \n24 \n307 \n256 1024 \n0.0 \n512 \n1024 3 \nIN-SR \n12 \n153 \n256 1024 \n0.0 \n512 \n1024 3 \nS-FLCKR, f = 4 \n24 \n307 \n256 1024 \n0.0 \n512 \n1024 2 \nS-FLCKR, f = 16 \n24 \n307 \n256 1024 \n0.0 \n512 \n1024 4 \nS-FLCKR, f = 32 \n24 \n307 \n256 1024 \n0.0 \n512 \n1024 5 \n(FacesHQ, f = 1)  *  \n24 \n307 \n-\n512 \n0.0 \n512 \n1024 -\nFacesHQ, f = 2 \n24 \n307 \n256 1024 \n0.0 \n512 \n1024 1 \nFacesHQ, f = 4 \n24 \n307 \n256 1024 \n0.0 \n512 \n1024 2 \nFacesHQ, f = 8 \n24 \n307 \n256 1024 \n0.0 \n512 \n1024 3 \nFacesHQ  *  *  , f = 16 \n24 \n307 \n256 1024 \n0.0 \n512 \n1024 4 \nFFHQ  *  *  , f = 16 \n28 \n355 \n256 1024 \n0.0 \n256 \n1024 4 \nCelebA-HQ  *  *  , f = 16 \n28 \n355 \n256 1024 \n0.0 \n256 \n1024 4 \nFFHQ (big) \n24 \n801 \n256 1024 \n0.0 \n256 \n1664 4 \nCelebA-HQ (big) \n24 \n801 \n256 1024 \n0.0 \n256 \n1664 4 \nCOCO-Stuff \n32 \n651 \n256 8192 \n0.0 \n512 \n1280 4 \nADE20K \n28 \n405 \n256 4096 \n0.1 \n512 \n1024 4 \nDeepFashion \n18 \n129 \n256 1024 \n0.0 \n340 \n768 \n4 \nLSUN-CT \n24 \n307 \n256 1024 \n0.0 \n256 \n1024 4 \nCIFAR-10 \n24 \n307 \n256 1024 \n0.0 \n256 \n1024 1 \n\n\n\n\noutperforms other orderings.0.0 \n\n0.2 \n0.4 \n0.6 \n0.8 \n\nreconstruction error \n\n10 2 \n\n10 3 \n\n10 4 \n\n10 5 \n\nnegative log-likelihood \n\narea of \nhigh-fidelity \n\narea of \nlow-fidelity \n\nf1 \nf2 \nf4 \nf8 \nf16 \nf32 \nf64 \n\nf2 \nf4 \nf8 \nf16 \nf32 \nf64 \n\nrecon-\nstruction \n\nsample \n\nsamples \n\nrec. error \n\n0.11 \u00b1 0.02 \n0.20 \u00b1 0.03 \n0.23 \u00b1 0.04 \n0.38 \u00b1 0.07 \n0.63 \u00b1 0.08 \n0.66 \u00b1 0.11 \n\nnll \n\n5.66 \u00b7 10 4 \n1.29 \u00b7 10 4 \n4.10 \u00b7 10 3 \n2.32 \u00b7 10 3 \n2.28 \u00b7 10 2 \n6.75 \u00b7 10 1 \n\n\n\n\nFigure 14. Qualitative assessment of various models for class-conditional image synthesis on ImageNet. Depicted classes: 28: spotted salamander (top) and 97: drake (bottom). We report class labels as in VQVAE-2[61].Figure 15. Qualitative assessment of various models for class-conditional image synthesis on ImageNet. Depicted classes: 108: sea anemone (top) and 141: redshank (bottom). We report class labels as in VQVAE-2 [61].Figure 16. Qualitative assessment of various models for class-conditional image synthesis on ImageNet. Depicted classes: 11: goldfinch (top) and 22: bald eagle (bottom).Figure 17. Qualitative assessment of various models for class-conditional image synthesis on ImageNet. Depicted classes: 0: tench (top) and 9: ostrich (bottom).Figure 18. Visualizing the effect of increased rejection rate (i.e. lower acceptance rate) by using a ResNet-101 classifier trained on ImageNet and samples from our class-conditional ImageNet model. Higher rejection rates tend to produce images showing more central, recognizable objects compared to the unguided samples. Here, k = 973, p = 1.0 are fixed for all samples. Note that k = 973 is the effective size of the VQGAN's codebook, i.e. it describes how many entries of the codebook with dim Z = 16384 are actually used.ours \n\nVQVAE-2 [61] \nBigGAN [4] \nMSP [19] \n\nours \n\nVQVAE-2 [61] \nBigGAN [4] \nMSP [19] \n\nours \n\nVQVAE-2 [61] \nBigGAN [4] \nMSP [19] \n\nours \n\nVQVAE-2 [61] \nBigGAN [4] \nMSP [19] \n\n933: cheeseburger \nacc. rate 1.0 \nacc. rate 0.5 \nacc. rate 0.1 \n\n992: agaric \nacc. rate 1.0 \nacc. rate 0.5 \nacc. rate 0.1 \n\n200: tibetian terrier \nacc. rate 1.0 \nacc. rate 0.5 \nacc. rate 0.1 \n\n933: cheeseburger \nk = 973 \nk = 300 \nk = 100 \n\n992: agaric \nk = 973 \nk = 300 \nk = 100 \n\n200: tibetian terrier \nk = 973 \nk = 300 \nk = 100 \n\n\nhttps://arxiv.org/abs/2012.09841v3 2 https://arxiv.org/abs/2012.09841v2\nhttps://drive.google.com/file/d/1H2nr_Cu7OK18tRemsWn_6o5DGMNYentM/view?usp=sharing\nhttps://bit.ly/2FJkvhJ 5 https://tfhub.dev/deepmind/biggan-deep-256/1 6 samples were reproduced with the authors' official implementation available at https://github.com/nvlabs/spade/\n\nNCP-VAE: variational autoencoders with noise contrastive priors. CoRR, abs. Jyoti Aneja, Alexander G Schwing, Jan Kautz, Arash Vahdat, Jyoti Aneja, Alexander G. Schwing, Jan Kautz, and Arash Vahdat. NCP-VAE: variational autoencoders with noise contrastive priors. CoRR, abs/2010.02917, 2020. 8\n\nNeural machine translation by jointly learning to align and translate. Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio, Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate, 2016. 2\n\nEstimating or propagating gradients through stochastic neurons for conditional computation. CoRR, abs/1308. Yoshua Bengio, Nicholas L\u00e9onard, Aaron C Courville, 3432Yoshua Bengio, Nicholas L\u00e9onard, and Aaron C. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. CoRR, abs/1308.3432, 2013. 4\n\nLarge Scale GAN Training for High Fidelity Natural Image Synthesis. Andrew Brock, Jeff Donahue, Karen Simonyan, 7th International Conference on Learning Representations, ICLR. 1619Andrew Brock, Jeff Donahue, and Karen Simonyan. Large Scale GAN Training for High Fidelity Natural Image Synthesis. In 7th International Conference on Learning Representations, ICLR, 2019. 8, 10, 16, 17, 18, 19\n\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, arXiv:2005.14165Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec RadfordarXiv preprintTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. arXiv preprint arXiv:2005.14165, 2020. 1\n\nCOCO-Stuff: Thing and stuff classes in context. Holger Caesar, Jasper Uijlings, Vittorio Ferrari, Computer vision and pattern recognition (CVPR). Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. COCO-Stuff: Thing and stuff classes in context. In Computer vision and pattern recognition (CVPR), 2018 IEEE conference on. IEEE, 2018. 6\n\nDeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs. G Liang-Chieh Chen, I Papandreou, Kevin Kokkinos, A Murphy, Yuille, IEEE Transactions on Pattern Analysis and Machine Intelligence. 5Liang-Chieh Chen, G. Papandreou, I. Kokkinos, Kevin Murphy, and A. Yuille. DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018. 5\n\nGenerative pretraining from pixels. Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, Ilya Sutskever, 2930Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, and Ilya Sutskever. Generative pretraining from pixels. 2020. 1, 2, 3, 4, 5, 6, 7, 12, 13, 14, 29, 30\n\nPhotographic image synthesis with cascaded refinement networks. Qifeng Chen, Vladlen Koltun, IEEE International Conference on Computer Vision. Venice, ItalyIEEE Computer SocietyQifeng Chen and Vladlen Koltun. Photographic image synthesis with cascaded refinement networks. In IEEE International Con- ference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017, pages 1520-1529. IEEE Computer Society, 2017. 7\n\nPixelsnail: An improved autoregressive generative model. Xi Chen, Nikhil Mishra, Mostafa Rohaninejad, Pieter Abbeel, PMLRICML. 805Xi Chen, Nikhil Mishra, Mostafa Rohaninejad, and Pieter Abbeel. Pixelsnail: An improved autoregressive generative model. In ICML, volume 80 of Proceedings of Machine Learning Research, pages 863-871. PMLR, 2018. 2, 5\n\nVery deep vaes generalize autoregressive models and can outperform them on images. CoRR, abs. Rewon Child. Very deep vaes generalize autoregressive models and can outperform them on images. CoRR, abs/2011.10650, 2020. 8\n\nGenerating long sequences with sparse transformers. Rewon Child, Scott Gray, Alec Radford, Ilya Sutskever, Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers, 2019. 1, 2, 5\n\nDiagnosing and enhancing VAE models. Bin Dai, David P Wipf, 7th International Conference on Learning Representations. Bin Dai and David P. Wipf. Diagnosing and enhancing VAE models. In 7th International Conference on Learning Representations, ICLR, 2019. 2\n\nImagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, IEEE Computer Society Conference on Computer Vision and Pattern Recognition CVPR. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition CVPR, 2009. 5\n\nDiffusion models beat gans on image synthesis. Prafulla Dhariwal, Alex Nichol, 810Prafulla Dhariwal and Alex Nichol. Diffusion models beat gans on image synthesis, 2021. 8, 10\n\nAn image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, 2020Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. 2020. 1\n\nGenerating Images with Perceptual Similarity Metrics based on Deep Networks. Alexey Dosovitskiy, Thomas Brox, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems. Alexey Dosovitskiy and Thomas Brox. Generating Images with Perceptual Similarity Metrics based on Deep Networks. In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems, NeurIPS, 2016. 4\n\nA Disentangling Invertible Interpretation Network for Explaining Latent Representations. Patrick Esser, Robin Rombach, Bj\u00f6rn Ommer, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Patrick Esser, Robin Rombach, and Bj\u00f6rn Ommer. A Disentangling Invertible Interpretation Network for Explaining Latent Repre- sentations. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR, 2020. 2\n\nHierarchical autoregressive image models with auxiliary decoders. CoRR, abs/1903.04933. Jeffrey De Fauw, Sander Dieleman, Karen Simonyan, 1619Jeffrey De Fauw, Sander Dieleman, and Karen Simonyan. Hierarchical autoregressive image models with auxiliary decoders. CoRR, abs/1903.04933, 2019. 16, 17, 18, 19\n\nGenerative Adversarial Nets. Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C Courville, Yoshua Bengio, Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems. Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. Generative Adversarial Nets. In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems, NeurIPS, 2014. 2\n\nnot-so-biggan: Generating high-fidelity images on a small compute budget. Seungwook Han, Akash Srivastava, Cole L Hurwitz, Prasanna Sattigeri, David D Cox, abs/2009.04433Seungwook Han, Akash Srivastava, Cole L. Hurwitz, Prasanna Sattigeri, and David D. Cox. not-so-biggan: Generating high-fidelity images on a small compute budget. CoRR, abs/2009.04433, 2020. 2\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, abs/1512.03385CoRRKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. CoRR, abs/1512.03385, 2015. 8\n\nPioneer networks: Progressively growing generative autoencoder. Ari Heljakka, Arno Solin, Juho Kannala, Computer Vision -ACCV 2018 -14th Asian Conference on Computer Vision. C. V. Jawahar, Hongdong Li, Greg Mori, and Konrad SchindlerPerth, AustraliaRevised Selected PapersAri Heljakka, Arno Solin, and Juho Kannala. Pioneer networks: Progressively growing generative autoencoder. In C. V. Jawahar, Hongdong Li, Greg Mori, and Konrad Schindler, editors, Computer Vision -ACCV 2018 -14th Asian Conference on Computer Vision, Perth, Australia, December 2-6, 2018, Revised Selected Papers, Part I, 2018. 8\n\nTowards photographic image manipulation with balanced growing of generative autoencoders. Ari Heljakka, Arno Solin, Juho Kannala, IEEE Winter Conference on Applications of Computer Vision. Snowmass Village, CO, USAIEEE2020Ari Heljakka, Arno Solin, and Juho Kannala. Towards photographic image manipulation with balanced growing of generative autoencoders. In IEEE Winter Conference on Applications of Computer Vision, WACV 2020, Snowmass Village, CO, USA, March 1-5, 2020, pages 3109-3118. IEEE, 2020. 8\n\nDenoising diffusion probabilistic models. Jonathan Ho, Ajay Jain, Pieter Abbeel, Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models, 2020. 11\n\nAxial attention in multidimensional transformers. CoRR, abs/1912.12180. Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, Tim Salimans, 25Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in multidimensional transformers. CoRR, abs/1912.12180, 2019. 2, 5\n\nThe curious case of neural text degeneration. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, Yejin Choi, ICLR. OpenReview.net. 822Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In ICLR. OpenRe- view.net, 2020. 8, 22\n\nImage-to-Image Translation with Conditional Adversarial Networks. Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A Efros, 2017 IEEE Conference on Computer Vision and Pattern Recognition. 411Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-Image Translation with Conditional Adversarial Networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR, 2017. 4, 11\n\nCategorical reparameterization with gumbel-softmax. Eric Jang, Shixiang Gu, Ben Poole, arXiv:1611.01144arXiv preprintEric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016. 9\n\nPerceptual losses for real-time style transfer and super-resolution. Justin Johnson, Alexandre Alahi, Li Fei-Fei, ECCV. Springer9906Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In ECCV (2), volume 9906 of Lecture Notes in Computer Science, pages 694-711. Springer, 2016. 4\n\nProgressive growing of gans for improved quality, stability, and variation. CoRR, abs/1710.10196. Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen, 6Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. CoRR, abs/1710.10196, 2017. 6, 8\n\nTraining generative adversarial networks with limited data. Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, Timo Aila, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020. Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin2020Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training generative adversarial networks with limited data. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Ad- vances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. 8\n\nA style-based generator architecture for generative adversarial networks. Tero Karras, Samuli Laine, Timo Aila, IEEE Conference on Computer Vision and Pattern Recognition, (CVPR) 2019. Long Beach, CA, USAComputer Vision Foundation / IEEETero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In IEEE Conference on Computer Vision and Pattern Recognition, (CVPR) 2019, Long Beach, CA, USA, June 16-20, 2019, pages 4401-4410. Computer Vision Foundation / IEEE, 2019. 7\n\nAnalyzing and improving the image quality of stylegan. Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, Timo Aila, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Seattle, WA, USAIEEE2020Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 8107-8116. IEEE, 2020. 8\n\nImproving augmentation and evaluation schemes for semantic image synthesis. Prateek Katiyar, Anna Khoreva, Prateek Katiyar and Anna Khoreva. Improving augmentation and evaluation schemes for semantic image synthesis, 2021. 7\n\nStructured attention networks. Yoon Kim, Carl Denton, Luong Hoang, Alexander M Rush, Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks, 2017. 2\n\nGlow: Generative Flow with Invertible 1x1 Convolutions. P Diederik, Prafulla Kingma, Dhariwal, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems. Diederik P. Kingma and Prafulla Dhariwal. Glow: Generative Flow with Invertible 1x1 Convolutions. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS, 2018. 8\n\nAuto-Encoding Variational Bayes. P Diederik, Max Kingma, Welling, 2nd International Conference on Learning Representations. Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In 2nd International Conference on Learning Representa- tions, ICLR, 2014. 2\n\nDiscriminative regularization for generative models. Alex Lamb, Aaron C Vincent Dumoulin, Courville, abs/1602.03220CoRRAlex Lamb, Vincent Dumoulin, and Aaron C. Courville. Discriminative regularization for generative models. CoRR, abs/1602.03220, 2016. 4\n\nAutoencoding beyond pixels using a learned similarity metric. Anders Boesen Lindbo Larsen, S\u00f8ren Kaae S\u00f8nderby, Hugo Larochelle, Ole Winther, Anders Boesen Lindbo Larsen, S\u00f8ren Kaae S\u00f8nderby, Hugo Larochelle, and Ole Winther. Autoencoding beyond pixels using a learned similarity metric, 2015. 4\n\nNeural speech synthesis with transformer network. Naihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, Ming Liu, AAAI. AAAI PressNaihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, and Ming Liu. Neural speech synthesis with transformer network. In AAAI, pages 6706-6713. AAAI Press, 2019. 2\n\nCOCO-GAN: generation by parts via conditional coordinating. Chieh Hubert Lin, Chia-Che Chang, Yu-Sheng Chen, Da-Cheng Juan, Wei Wei, Hwann-Tzong Chen, ICCV. IEEEChieh Hubert Lin, Chia-Che Chang, Yu-Sheng Chen, Da-Cheng Juan, Wei Wei, and Hwann-Tzong Chen. COCO-GAN: generation by parts via conditional coordinating. In ICCV, pages 4511-4520. IEEE, 2019. 5\n\nAn acceleration framework for high resolution image synthesis. Jinlin Liu, Yuan Yao, Jianqiang Ren, abs/1909.03611CoRRJinlin Liu, Yuan Yao, and Jianqiang Ren. An acceleration framework for high resolution image synthesis. CoRR, abs/1909.03611, 2019. 2\n\nGenerating wikipedia by summarizing long sequences. J Peter, Mohammad Liu, Etienne Saleh, Ben Pot, Ryan Goodrich, Lukasz Sepassi, Noam Kaiser, Shazeer, ICLR (Poster). Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. Generating wikipedia by summarizing long sequences. In ICLR (Poster). OpenReview.net, 2018. 4\n\nDeepfashion: Powering robust clothes recognition and retrieval with rich annotations. Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, Xiaoou Tang, Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou Tang. Deepfashion: Powering robust clothes recognition and retrieval with rich annotations. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016. 6\n\nGenerating high fidelity images with subscale pixel networks and multidimensional upscaling. Jacob Menick, Nal Kalchbrenner, 7th International Conference on Learning Representations, ICLR 2019. New Orleans, LA, USA14Jacob Menick and Nal Kalchbrenner. Generating high fidelity images with subscale pixel networks and multidimensional upscaling. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. 14\n\nHigh-fidelity generative image compression. Fabian Mentzer, George Toderici, Michael Tschannen, Eirikur Agustsson, 2020. 4Fabian Mentzer, George Toderici, Michael Tschannen, and Eirikur Agustsson. High-fidelity generative image compression, 2020. 4\n\nGenerating images with sparse representations. Charlie Nash, Jacob Menick, Sander Dieleman, Peter W Battaglia, 810Charlie Nash, Jacob Menick, Sander Dieleman, and Peter W. Battaglia. Generating images with sparse representations, 2021. 8, 10\n\nImproved denoising diffusion probabilistic models. Alex Nichol, Prafulla Dhariwal, 810Alex Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models, 2021. 8, 10\n\nHigh-fidelity performance metrics for generative models in pytorch. Anton Obukhov, Maximilian Seitzer, Po-Wei Wu, Semen Zhydenko, Jonathan Kyl, Elvis Yu-Jing Lin, 10.5281/zenodo.4957738710Anton Obukhov, Maximilian Seitzer, Po-Wei Wu, Semen Zhydenko, Jonathan Kyl, and Elvis Yu-Jing Lin. High-fidelity performance metrics for generative models in pytorch, 2020. Version: 0.3.0, DOI: 10.5281/zenodo.4957738. 7, 10\n\nLearning the compositional nature of visual objects. B Ommer, J M Buhmann, 2007 IEEE Conference on Computer Vision and Pattern Recognition. B. Ommer and J. M. Buhmann. Learning the compositional nature of visual objects. In 2007 IEEE Conference on Computer Vision and Pattern Recognition, pages 1-8, 2007. 2\n\nA decomposable attention model for natural language inference. P Ankur, Oscar Parikh, Dipanjan T\u00e4ckstr\u00f6m, Jakob Das, Uszkoreit, Ankur P. Parikh, Oscar T\u00e4ckstr\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model for natural language inference, 2016. 2\n\nSemantic Image Synthesis with Spatially-Adaptive Normalization. Taesung Park, Ming-Yu Liu, Ting-Chun Wang, Jun-Yan Zhu, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition4243Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic Image Synthesis with Spatially-Adaptive Normalization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, CVPR, 2019. 2, 7, 42, 43\n\nDual contradistinctive generative autoencoder. Gaurav Parmar, Dacheng Li, Kwonjoon Lee, Zhuowen Tu, Gaurav Parmar, Dacheng Li, Kwonjoon Lee, and Zhuowen Tu. Dual contradistinctive generative autoencoder, 2020. 8\n\nImage transformer. Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, Dustin Tran, PMLRICML. 805Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In ICML, volume 80 of Proceedings of Machine Learning Research, pages 4052-4061. PMLR, 2018. 2, 3, 5\n\nAdversarial latent autoencoders. Stanislav Pidhorskyi, Donald A Adjeroh, Gianfranco Doretto, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Seattle, WA, USAIEEE2020Stanislav Pidhorskyi, Donald A. Adjeroh, and Gianfranco Doretto. Adversarial latent autoencoders. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 14092-14101. IEEE, 2020. 8\n\nImproving language understanding by generative pre-training. A Radford, A. Radford. Improving language understanding by generative pre-training. 2018. 1\n\nLanguage models are unsupervised multitask learners. A Radford, Jeffrey Wu, R Child, David Luan, Dario Amodei, Ilya Sutskever, 511A. Radford, Jeffrey Wu, R. Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. 1, 5, 11\n\nZero-shot text-to-image generation. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever, 1012Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation, 2021. 9, 10, 12\n\nTowards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. Ren\u00e9 Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, Vladlen Koltun, 2020. 5IEEE Transactions on Pattern Analysis and Machine Intelligence. TPAMIRen\u00e9 Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2020. 5\n\nGenerating diverse high-fidelity images with vq-vae-2. Ali Razavi, Aaron Van Den Oord, Oriol Vinyals, 1819Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2, 2019. 3, 4, 5, 8, 16, 17, 18, 19\n\nStochastic backpropagation and approximate inference in deep generative models. Danilo Jimenez Rezende, Shakir Mohamed, Daan Wierstra, Proceedings of the 31st International Conference on International Conference on Machine Learning. the 31st International Conference on International Conference on Machine LearningDanilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In Proceedings of the 31st International Conference on International Conference on Machine Learning, ICML, 2014. 2\n\nMaking sense of cnns: Interpreting deep representations and their invariances with inns. Robin Rombach, Patrick Esser, Bj\u00f6rn Ommer, Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael FrahmRobin Rombach, Patrick Esser, and Bj\u00f6rn Ommer. Making sense of cnns: Interpreting deep representations and their invariances with inns. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm, editors, Computer Vision -ECCV 2020 -16th\n\nProceedings, Part XVII. Part XVIIGlasgow, UKSpringer12362European ConferenceEuropean Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XVII, volume 12362 of Lecture Notes in Computer Science, pages 647-664. Springer, 2020. 2\n\nNetwork-to-network translation with conditional invertible neural networks. Robin Rombach, Patrick Esser, Bjorn Ommer, Advances in Neural Information Processing Systems. H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. LinCurran Associates, Inc33Robin Rombach, Patrick Esser, and Bjorn Ommer. Network-to-network translation with conditional invertible neural networks. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 2784-2797. Curran Associates, Inc., 2020. 2\n\nComputer vision with a single (robust) classifier. Shibani Santurkar, Dimitris Tsipras, Brandon Tran, Andrew Ilyas, Logan Engstrom, Aleksander Madry, arXiv:1906.09453In ArXiv preprintShibani Santurkar, Dimitris Tsipras, Brandon Tran, Andrew Ilyas, Logan Engstrom, and Aleksander Madry. Computer vision with a single (robust) classifier. In ArXiv preprint arXiv:1906.09453, 2019. 5\n\nA u-net based discriminator for generative adversarial networks. Edgar Sch\u00f6nfeld, Bernt Schiele, Anna Khoreva, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Seattle, WA, USAIEEE2020Edgar Sch\u00f6nfeld, Bernt Schiele, and Anna Khoreva. A u-net based discriminator for generative adversarial networks. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 8204- 8213. IEEE, 2020. 8\n\nImplementation of generating diverse high-fidelity images with vq-vae-2 in pytorch. Kim Seonghyeon, Kim Seonghyeon. Implementation of generating diverse high-fidelity images with vq-vae-2 in pytorch, 2020. 8\n\nFirst order motion model for image animation. Aliaksandr Siarohin, St\u00e9phane Lathuili\u00e8re, Sergey Tulyakov, Elisa Ricci, Nicu Sebe, Conference on Neural Information Processing Systems (NeurIPS). Aliaksandr Siarohin, St\u00e9phane Lathuili\u00e8re, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. First order motion model for image anima- tion. In Conference on Neural Information Processing Systems (NeurIPS), December 2019. 2\n\nNVAE: A deep hierarchical variational autoencoder. Arash Vahdat, Jan Kautz, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020. Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin2020Arash Vahdat and Jan Kautz. NVAE: A deep hierarchical variational autoencoder. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. 8\n\nPixel recurrent neural networks. A\u00e4ron Van Den Oord, Nal Kalchbrenner, Koray Kavukcuoglu, ICML, volume 48 of JMLR Workshop and Conference Proceedings. A\u00e4ron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. In ICML, volume 48 of JMLR Workshop and Conference Proceedings, pages 1747-1756. JMLR.org, 2016. 2\n\nConditional image generation with pixelcnn decoders. Aaron Van Den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, Koray Kavukcuoglu, 14Aaron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, and Koray Kavukcuoglu. Conditional image generation with pixelcnn decoders, 2016. 2, 4, 14\n\nNeural discrete representation learning. Aaron Van Den Oord, Oriol Vinyals, Koray Kavukcuoglu, 311Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning, 2018. 3, 4, 11\n\n. Andrew Hugo Van Kemenade, Alex Murray, Alexander Clark, Ondrej Karpinsky, Christoph Baranovi\u010d, Jon Gohlke, Brian Dufresne, David Crowell, Konstantin Schmidt, Alastair Kopachev, Sandro Houghton, Steve Mani, Josh Landey, Jason Ware, David Douglas, Uriel Caro, Steve Martinez, Riley Kossouho, Lahd, T Stanislau, Antony Lee, Eric W Brown, Oliver Tonnhofer, Mickael Bonfill, Peter Rowlands, Fahad Al-Saidi, German Novikov, Micha\u0142 G\u00f3rny, python-pillow/pillow: 8.2.0Hugo van Kemenade, wiredfool, Andrew Murray, Alex Clark, Alexander Karpinsky, Ondrej Baranovi\u010d, Christoph Gohlke, Jon Dufresne, Brian Crowell, David Schmidt, Konstantin Kopachev, Alastair Houghton, Sandro Mani, Steve Landey, vashek, Josh Ware, Jason Douglas, David Caro, Uriel Martinez, Steve Kossouho, Riley Lahd, Stanislau T., Antony Lee, Eric W. Brown, Oliver Tonnhofer, Mickael Bonfill, Peter Rowlands, Fahad Al-Saidi, German Novikov, and Micha\u0142 G\u00f3rny. python-pillow/pillow: 8.2.0, Apr. 2021. 10\n\nAttention is All you Need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. 23Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is All you Need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems, NeurIPS, 2017. 1, 2, 3\n\nHigh-resolution image synthesis and semantic manipulation with conditional gans. Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, Bryan Catanzaro, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionTing-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. High-resolution image synthesis and semantic manipulation with conditional gans. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018. 7\n\nScaling autoregressive video models. Dirk Weissenborn, Oscar T\u00e4ckstr\u00f6m, Jakob Uszkoreit, ICLR. OpenReview.net. Dirk Weissenborn, Oscar T\u00e4ckstr\u00f6m, and Jakob Uszkoreit. Scaling autoregressive video models. In ICLR. OpenReview.net, 2020. 2\n\nVaebm: A symbiosis between variational autoencoders and energy-based models. Zhisheng Xiao, Karsten Kreis, Jan Kautz, Arash Vahdat, Zhisheng Xiao, Karsten Kreis, Jan Kautz, and Arash Vahdat. Vaebm: A symbiosis between variational autoencoders and energy-based models, 2021. 8\n\nGenerative latent flow: A framework for non-adversarial image generation. Zhisheng Xiao, Qing Yan, Yi-An Chen, Yali Amit, abs/1905.10485CoRRZhisheng Xiao, Qing Yan, Yi-an Chen, and Yali Amit. Generative latent flow: A framework for non-adversarial image generation. CoRR, abs/1905.10485, 2019. 2\n\nFisher Yu, Yinda Zhang, Shuran Song, Ari Seff, Jianxiong Xiao, Lsun, arXiv:1506.03365Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprintFisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015. 5\n\nCross-Domain Correspondence Learning for Exemplar-Based Image Translation. Pan Zhang, Bo Zhang, Dong Chen, Lu Yuan, Fang Wen, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Pan Zhang, Bo Zhang, Dong Chen, Lu Yuan, and Fang Wen. Cross-Domain Correspondence Learning for Exemplar-Based Image Translation. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR, 2020. 2\n\nThe Unreasonable Effectiveness of Deep Features as a Perceptual Metric. Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, Oliver Wang, CVPR. 413Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The Unreasonable Effectiveness of Deep Features as a Perceptual Metric. In CVPR, 2018. 4, 11, 13\n\nThe unreasonable effectiveness of deep features as a perceptual metric. Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, Oliver Wang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition4546Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, CVPR, 2018. 45, 46\n\nBolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, Antonio Torralba, arXiv:1608.05442Semantic understanding of scenes through the ade20k dataset. arXiv preprintBolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic understanding of scenes through the ade20k dataset. arXiv preprint arXiv:1608.05442, 2016. 6\n\nView synthesis by appearance flow. Tinghui Zhou, Shubham Tulsiani, Weilun Sun, Jitendra Malik, Alexei A Efros, Tinghui Zhou, Shubham Tulsiani, Weilun Sun, Jitendra Malik, and Alexei A. Efros. View synthesis by appearance flow, 2017. 2\n\nSean: Image synthesis with semantic region-adaptive normalization. Peihao Zhu, Rameen Abdal, Yipeng Qin, Peter Wonka, Peihao Zhu, Rameen Abdal, Yipeng Qin, and Peter Wonka. Sean: Image synthesis with semantic region-adaptive normalization, 2019. 2\n", "annotations": {"author": "[{\"end\":154,\"start\":59},{\"end\":250,\"start\":155},{\"end\":344,\"start\":251}]", "publisher": null, "author_last_name": "[{\"end\":72,\"start\":67},{\"end\":168,\"start\":161},{\"end\":262,\"start\":257}]", "author_first_name": "[{\"end\":66,\"start\":59},{\"end\":160,\"start\":155},{\"end\":256,\"start\":251}]", "author_affiliation": "[{\"end\":153,\"start\":74},{\"end\":249,\"start\":170},{\"end\":343,\"start\":264}]", "title": "[{\"end\":56,\"start\":1},{\"end\":400,\"start\":345}]", "venue": null, "abstract": "[{\"end\":1777,\"start\":576}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b74\"},\"end\":1893,\"start\":1889},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":1896,\"start\":1893},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":1899,\"start\":1896},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1901,\"start\":1899},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":1964,\"start\":1960},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":1979,\"start\":1976},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":1982,\"start\":1979},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2847,\"start\":2843},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":3759,\"start\":3755},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":4955,\"start\":4951},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5034,\"start\":5031},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":5037,\"start\":5034},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":5040,\"start\":5037},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":5306,\"start\":5302},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5320,\"start\":5317},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":6802,\"start\":6798},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":6811,\"start\":6807},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7001,\"start\":6997},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7010,\"start\":7006},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":7935,\"start\":7931},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":7938,\"start\":7935},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":7941,\"start\":7938},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":7944,\"start\":7941},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":7947,\"start\":7944},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":8083,\"start\":8079},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":8086,\"start\":8083},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8089,\"start\":8086},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":8140,\"start\":8136},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8143,\"start\":8140},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8146,\"start\":8143},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8588,\"start\":8584},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":8741,\"start\":8737},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":8744,\"start\":8741},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8803,\"start\":8799},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":8806,\"start\":8803},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":8908,\"start\":8904},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":8911,\"start\":8908},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":9033,\"start\":9029},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9051,\"start\":9047},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9097,\"start\":9093},{\"end\":9160,\"start\":9152},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":9890,\"start\":9886},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10121,\"start\":10118},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10374,\"start\":10371},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":10935,\"start\":10931},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":11035,\"start\":11031},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":11037,\"start\":11035},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":12858,\"start\":12854},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":13663,\"start\":13660},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":13971,\"start\":13967},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":14283,\"start\":14279},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":14286,\"start\":14283},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":14289,\"start\":14286},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":14292,\"start\":14289},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":14295,\"start\":14292},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":14436,\"start\":14432},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":14439,\"start\":14436},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":14487,\"start\":14484},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":14588,\"start\":14584},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":14707,\"start\":14703},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":15154,\"start\":15150},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":17528,\"start\":17524},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":18609,\"start\":18605},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":19352,\"start\":19348},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":19755,\"start\":19751},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":19758,\"start\":19755},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":19761,\"start\":19758},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":19809,\"start\":19806},{\"end\":20235,\"start\":20231},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":20435,\"start\":20431},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":20938,\"start\":20934},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":20970,\"start\":20966},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":21067,\"start\":21063},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":21171,\"start\":21167},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":21301,\"start\":21298},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":22083,\"start\":22079},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":23896,\"start\":23893},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":24256,\"start\":24255},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":24370,\"start\":24366},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":24622,\"start\":24619},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":25292,\"start\":25288},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":25295,\"start\":25292},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":25298,\"start\":25295},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":25300,\"start\":25298},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":25743,\"start\":25739},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":26366,\"start\":26362},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":26523,\"start\":26520},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":26535,\"start\":26531},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":26555,\"start\":26551},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":26568,\"start\":26564},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":26710,\"start\":26706},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":27164,\"start\":27160},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":27377,\"start\":27373},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":28012,\"start\":28008},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":28015,\"start\":28012},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":28141,\"start\":28137},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":31414,\"start\":31410},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":31551,\"start\":31547},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":31554,\"start\":31551},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":31557,\"start\":31554},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":31627,\"start\":31624},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":31916,\"start\":31913},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":32204,\"start\":32200},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":32273,\"start\":32269},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":33501,\"start\":33497},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":33588,\"start\":33584},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":34565,\"start\":34561},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":35315,\"start\":35311},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":36860,\"start\":36857},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":37019,\"start\":37015},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":38714,\"start\":38713},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":38912,\"start\":38911},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":39082,\"start\":39079},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":39840,\"start\":39837},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":40983,\"start\":40979},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":43301,\"start\":43297},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":44515,\"start\":44511},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":44517,\"start\":44515},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":46942,\"start\":46938},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":47924,\"start\":47920},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":52371,\"start\":52367},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":54038,\"start\":54035},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":54106,\"start\":54103},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":54316,\"start\":54313},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":54361,\"start\":54358},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":54429,\"start\":54426},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":54639,\"start\":54636},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":56288,\"start\":56284},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":56363,\"start\":56359},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":56726,\"start\":56722},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":57528,\"start\":57524},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":57905,\"start\":57902},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":57908,\"start\":57905},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":57911,\"start\":57908},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":57914,\"start\":57911},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":57929,\"start\":57925},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":57932,\"start\":57929},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":59744,\"start\":59740},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":61796,\"start\":61792}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":45534,\"start\":45496},{\"attributes\":{\"id\":\"fig_1\"},\"end\":47355,\"start\":45535},{\"attributes\":{\"id\":\"fig_2\"},\"end\":47688,\"start\":47356},{\"attributes\":{\"id\":\"fig_3\"},\"end\":47913,\"start\":47689},{\"attributes\":{\"id\":\"fig_4\"},\"end\":48534,\"start\":47914},{\"attributes\":{\"id\":\"fig_5\"},\"end\":48903,\"start\":48535},{\"attributes\":{\"id\":\"fig_6\"},\"end\":49379,\"start\":48904},{\"attributes\":{\"id\":\"fig_7\"},\"end\":49471,\"start\":49380},{\"attributes\":{\"id\":\"fig_8\"},\"end\":49565,\"start\":49472},{\"attributes\":{\"id\":\"fig_9\"},\"end\":50441,\"start\":49566},{\"attributes\":{\"id\":\"fig_10\"},\"end\":50542,\"start\":50442},{\"attributes\":{\"id\":\"fig_11\"},\"end\":51103,\"start\":50543},{\"attributes\":{\"id\":\"fig_12\"},\"end\":51712,\"start\":51104},{\"attributes\":{\"id\":\"fig_13\"},\"end\":52611,\"start\":51713},{\"attributes\":{\"id\":\"fig_14\"},\"end\":52862,\"start\":52612},{\"attributes\":{\"id\":\"fig_15\"},\"end\":53066,\"start\":52863},{\"attributes\":{\"id\":\"fig_16\"},\"end\":53269,\"start\":53067},{\"attributes\":{\"id\":\"fig_17\"},\"end\":53473,\"start\":53270},{\"attributes\":{\"id\":\"fig_18\"},\"end\":53726,\"start\":53474},{\"attributes\":{\"id\":\"fig_19\"},\"end\":54649,\"start\":53727},{\"attributes\":{\"id\":\"fig_20\"},\"end\":54788,\"start\":54650},{\"attributes\":{\"id\":\"fig_21\"},\"end\":54929,\"start\":54789},{\"attributes\":{\"id\":\"fig_22\"},\"end\":55080,\"start\":54930},{\"attributes\":{\"id\":\"fig_23\"},\"end\":56046,\"start\":55081},{\"attributes\":{\"id\":\"fig_24\"},\"end\":57809,\"start\":56047},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":57933,\"start\":57810},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":58722,\"start\":57934},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":59159,\"start\":58723},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":59679,\"start\":59160},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":61120,\"start\":59680},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":61579,\"start\":61121},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":63373,\"start\":61580}]", "paragraph": "[{\"end\":2771,\"start\":1793},{\"end\":3569,\"start\":2773},{\"end\":4851,\"start\":3571},{\"end\":5838,\"start\":4868},{\"end\":7186,\"start\":5893},{\"end\":7996,\"start\":7215},{\"end\":8401,\"start\":7998},{\"end\":9677,\"start\":8403},{\"end\":10830,\"start\":9679},{\"end\":11270,\"start\":10843},{\"end\":12020,\"start\":11272},{\"end\":13329,\"start\":12101},{\"end\":13427,\"start\":13387},{\"end\":13745,\"start\":13459},{\"end\":13972,\"start\":13819},{\"end\":14773,\"start\":13974},{\"end\":14928,\"start\":14828},{\"end\":15058,\"start\":15007},{\"end\":15155,\"start\":15100},{\"end\":15549,\"start\":15166},{\"end\":15982,\"start\":15606},{\"end\":16585,\"start\":16023},{\"end\":17023,\"start\":16629},{\"end\":17529,\"start\":17057},{\"end\":18610,\"start\":17531},{\"end\":19477,\"start\":18626},{\"end\":19921,\"start\":19527},{\"end\":20851,\"start\":19923},{\"end\":21745,\"start\":20853},{\"end\":23238,\"start\":21791},{\"end\":24257,\"start\":23305},{\"end\":24899,\"start\":24259},{\"end\":26168,\"start\":24940},{\"end\":27646,\"start\":26170},{\"end\":28016,\"start\":27648},{\"end\":28785,\"start\":28018},{\"end\":29774,\"start\":28800},{\"end\":29923,\"start\":29776},{\"end\":29984,\"start\":29967},{\"end\":30759,\"start\":30011},{\"end\":30861,\"start\":30776},{\"end\":31138,\"start\":30863},{\"end\":32274,\"start\":31140},{\"end\":32870,\"start\":32276},{\"end\":33635,\"start\":32900},{\"end\":34474,\"start\":34041},{\"end\":34796,\"start\":34476},{\"end\":36005,\"start\":34832},{\"end\":36998,\"start\":36007},{\"end\":37765,\"start\":37000},{\"end\":38348,\"start\":37767},{\"end\":38914,\"start\":38374},{\"end\":39895,\"start\":38916},{\"end\":40355,\"start\":39897},{\"end\":41274,\"start\":40391},{\"end\":42164,\"start\":41276},{\"end\":42376,\"start\":42166},{\"end\":43756,\"start\":42424},{\"end\":45294,\"start\":43758},{\"end\":45495,\"start\":45308}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":5892,\"start\":5839},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13386,\"start\":13330},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13458,\"start\":13428},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13818,\"start\":13746},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14827,\"start\":14774},{\"attributes\":{\"id\":\"formula_5\"},\"end\":15006,\"start\":14929},{\"attributes\":{\"id\":\"formula_6\"},\"end\":15099,\"start\":15059},{\"attributes\":{\"id\":\"formula_7\"},\"end\":15165,\"start\":15156},{\"attributes\":{\"id\":\"formula_8\"},\"end\":16022,\"start\":15983},{\"attributes\":{\"id\":\"formula_9\"},\"end\":16628,\"start\":16586},{\"attributes\":{\"id\":\"formula_10\"},\"end\":17056,\"start\":17024},{\"attributes\":{\"id\":\"formula_11\"},\"end\":34019,\"start\":33636}]", "table_ref": "[{\"end\":27829,\"start\":27822},{\"end\":33365,\"start\":33358},{\"end\":36198,\"start\":36191},{\"end\":38849,\"start\":38845}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1791,\"start\":1779},{\"attributes\":{\"n\":\"2.\"},\"end\":4866,\"start\":4854},{\"end\":7213,\"start\":7189},{\"attributes\":{\"n\":\"3.\"},\"end\":10841,\"start\":10833},{\"attributes\":{\"n\":\"3.1.\"},\"end\":12099,\"start\":12023},{\"attributes\":{\"n\":\"3.2.\"},\"end\":15604,\"start\":15552},{\"attributes\":{\"n\":\"4.\"},\"end\":18624,\"start\":18613},{\"attributes\":{\"n\":\"4.1.\"},\"end\":19525,\"start\":19480},{\"attributes\":{\"n\":\"4.2.\"},\"end\":21789,\"start\":21748},{\"end\":23266,\"start\":23241},{\"attributes\":{\"n\":\"4.3.\"},\"end\":23303,\"start\":23269},{\"attributes\":{\"n\":\"4.4.\"},\"end\":24938,\"start\":24902},{\"attributes\":{\"n\":\"5.\"},\"end\":28798,\"start\":28788},{\"end\":29965,\"start\":29926},{\"end\":30009,\"start\":29987},{\"end\":30774,\"start\":30762},{\"end\":32898,\"start\":32873},{\"end\":34039,\"start\":34021},{\"end\":34830,\"start\":34799},{\"end\":38372,\"start\":38351},{\"end\":40389,\"start\":40358},{\"end\":42422,\"start\":42379},{\"end\":45306,\"start\":45297},{\"end\":45507,\"start\":45497},{\"end\":45546,\"start\":45536},{\"end\":47700,\"start\":47690},{\"end\":47919,\"start\":47915},{\"end\":48546,\"start\":48536},{\"end\":48915,\"start\":48905},{\"end\":49391,\"start\":49381},{\"end\":49483,\"start\":49473},{\"end\":49576,\"start\":49567},{\"end\":50454,\"start\":50443},{\"end\":51127,\"start\":51105},{\"end\":51736,\"start\":51714},{\"end\":52624,\"start\":52613},{\"end\":52875,\"start\":52864},{\"end\":53079,\"start\":53068},{\"end\":53282,\"start\":53271},{\"end\":53486,\"start\":53475},{\"end\":53761,\"start\":53728},{\"end\":54662,\"start\":54651},{\"end\":54801,\"start\":54790},{\"end\":54942,\"start\":54931},{\"end\":55159,\"start\":55082},{\"end\":56136,\"start\":56048},{\"end\":57820,\"start\":57811},{\"end\":58733,\"start\":58724},{\"end\":59170,\"start\":59161}]", "table": "[{\"end\":58722,\"start\":57957},{\"end\":59159,\"start\":58735},{\"end\":61120,\"start\":59745},{\"end\":61579,\"start\":61151},{\"end\":63373,\"start\":62865}]", "figure_caption": "[{\"end\":45534,\"start\":45509},{\"end\":47355,\"start\":45548},{\"end\":47688,\"start\":47358},{\"end\":47913,\"start\":47702},{\"end\":48534,\"start\":47920},{\"end\":48903,\"start\":48548},{\"end\":49379,\"start\":48917},{\"end\":49471,\"start\":49393},{\"end\":49565,\"start\":49485},{\"end\":50441,\"start\":49579},{\"end\":50542,\"start\":50457},{\"end\":51103,\"start\":50545},{\"end\":51712,\"start\":51132},{\"end\":52611,\"start\":51741},{\"end\":52862,\"start\":52627},{\"end\":53066,\"start\":52878},{\"end\":53269,\"start\":53082},{\"end\":53473,\"start\":53285},{\"end\":53726,\"start\":53489},{\"end\":54649,\"start\":53768},{\"end\":54788,\"start\":54665},{\"end\":54929,\"start\":54804},{\"end\":55080,\"start\":54945},{\"end\":56046,\"start\":55174},{\"end\":57809,\"start\":56153},{\"end\":57933,\"start\":57822},{\"end\":57957,\"start\":57936},{\"end\":59679,\"start\":59172},{\"end\":59745,\"start\":59682},{\"end\":61151,\"start\":61123},{\"end\":62865,\"start\":61582}]", "figure_ref": "[{\"end\":4461,\"start\":4455},{\"end\":11899,\"start\":11893},{\"end\":13079,\"start\":13073},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":18184,\"start\":18178},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22125,\"start\":22119},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":22136,\"start\":22130},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":22689,\"start\":22678},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":24273,\"start\":24267},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":26096,\"start\":26089},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":26785,\"start\":26779},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":27023,\"start\":27013},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":27645,\"start\":27639},{\"end\":28784,\"start\":28777},{\"end\":31436,\"start\":31429},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":32796,\"start\":32789},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":34945,\"start\":34939},{\"end\":35245,\"start\":35238},{\"end\":36021,\"start\":36014},{\"end\":36190,\"start\":36183},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":37832,\"start\":37825},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":37909,\"start\":37903},{\"end\":38148,\"start\":38141},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":38491,\"start\":38476},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":38885,\"start\":38869},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":38897,\"start\":38890},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":39651,\"start\":39644},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":39663,\"start\":39656},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":39947,\"start\":39932},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":39959,\"start\":39952},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":40134,\"start\":40128},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":40354,\"start\":40347},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":41017,\"start\":41010},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":42303,\"start\":42296},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":43598,\"start\":43591},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":43772,\"start\":43766},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":44165,\"start\":44158},{\"end\":44527,\"start\":44518},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":44891,\"start\":44882},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":45368,\"start\":45359}]", "bib_author_first_name": "[{\"end\":63795,\"start\":63790},{\"end\":63812,\"start\":63803},{\"end\":63814,\"start\":63813},{\"end\":63827,\"start\":63824},{\"end\":63840,\"start\":63835},{\"end\":64087,\"start\":64080},{\"end\":64107,\"start\":64098},{\"end\":64119,\"start\":64113},{\"end\":64374,\"start\":64368},{\"end\":64391,\"start\":64383},{\"end\":64406,\"start\":64401},{\"end\":64408,\"start\":64407},{\"end\":64677,\"start\":64671},{\"end\":64689,\"start\":64685},{\"end\":64704,\"start\":64699},{\"end\":64998,\"start\":64995},{\"end\":65000,\"start\":64999},{\"end\":65016,\"start\":65008},{\"end\":65027,\"start\":65023},{\"end\":65042,\"start\":65035},{\"end\":65057,\"start\":65052},{\"end\":65074,\"start\":65066},{\"end\":65091,\"start\":65085},{\"end\":65111,\"start\":65105},{\"end\":65125,\"start\":65119},{\"end\":65140,\"start\":65134},{\"end\":65157,\"start\":65149},{\"end\":65172,\"start\":65167},{\"end\":65195,\"start\":65187},{\"end\":65208,\"start\":65205},{\"end\":65224,\"start\":65219},{\"end\":65238,\"start\":65232},{\"end\":65253,\"start\":65247},{\"end\":65255,\"start\":65254},{\"end\":65272,\"start\":65265},{\"end\":65284,\"start\":65277},{\"end\":65304,\"start\":65293},{\"end\":65316,\"start\":65312},{\"end\":65327,\"start\":65323},{\"end\":65343,\"start\":65336},{\"end\":66161,\"start\":66155},{\"end\":66176,\"start\":66170},{\"end\":66195,\"start\":66187},{\"end\":66560,\"start\":66559},{\"end\":66580,\"start\":66579},{\"end\":66598,\"start\":66593},{\"end\":66610,\"start\":66609},{\"end\":66993,\"start\":66989},{\"end\":67004,\"start\":67000},{\"end\":67019,\"start\":67014},{\"end\":67031,\"start\":67027},{\"end\":67042,\"start\":67036},{\"end\":67056,\"start\":67048},{\"end\":67072,\"start\":67067},{\"end\":67083,\"start\":67079},{\"end\":67362,\"start\":67356},{\"end\":67376,\"start\":67369},{\"end\":67775,\"start\":67773},{\"end\":67788,\"start\":67782},{\"end\":67804,\"start\":67797},{\"end\":67824,\"start\":67818},{\"end\":68342,\"start\":68337},{\"end\":68355,\"start\":68350},{\"end\":68366,\"start\":68362},{\"end\":68380,\"start\":68376},{\"end\":68558,\"start\":68555},{\"end\":68569,\"start\":68564},{\"end\":68571,\"start\":68570},{\"end\":68832,\"start\":68829},{\"end\":68842,\"start\":68839},{\"end\":68856,\"start\":68849},{\"end\":68871,\"start\":68865},{\"end\":68879,\"start\":68876},{\"end\":68886,\"start\":68884},{\"end\":69256,\"start\":69248},{\"end\":69271,\"start\":69267},{\"end\":69460,\"start\":69454},{\"end\":69479,\"start\":69474},{\"end\":69496,\"start\":69487},{\"end\":69513,\"start\":69509},{\"end\":69534,\"start\":69527},{\"end\":69547,\"start\":69541},{\"end\":69568,\"start\":69561},{\"end\":69587,\"start\":69579},{\"end\":69603,\"start\":69598},{\"end\":69620,\"start\":69613},{\"end\":69981,\"start\":69975},{\"end\":70001,\"start\":69995},{\"end\":70466,\"start\":70459},{\"end\":70479,\"start\":70474},{\"end\":70494,\"start\":70489},{\"end\":70891,\"start\":70884},{\"end\":70907,\"start\":70901},{\"end\":70923,\"start\":70918},{\"end\":71134,\"start\":71131},{\"end\":71136,\"start\":71135},{\"end\":71153,\"start\":71149},{\"end\":71174,\"start\":71169},{\"end\":71186,\"start\":71182},{\"end\":71196,\"start\":71191},{\"end\":71218,\"start\":71211},{\"end\":71231,\"start\":71226},{\"end\":71233,\"start\":71232},{\"end\":71251,\"start\":71245},{\"end\":71756,\"start\":71747},{\"end\":71767,\"start\":71762},{\"end\":71784,\"start\":71780},{\"end\":71786,\"start\":71785},{\"end\":71804,\"start\":71796},{\"end\":71821,\"start\":71816},{\"end\":71823,\"start\":71822},{\"end\":72089,\"start\":72082},{\"end\":72101,\"start\":72094},{\"end\":72117,\"start\":72109},{\"end\":72127,\"start\":72123},{\"end\":72350,\"start\":72347},{\"end\":72365,\"start\":72361},{\"end\":72377,\"start\":72373},{\"end\":72979,\"start\":72976},{\"end\":72994,\"start\":72990},{\"end\":73006,\"start\":73002},{\"end\":73441,\"start\":73433},{\"end\":73450,\"start\":73446},{\"end\":73463,\"start\":73457},{\"end\":73647,\"start\":73639},{\"end\":73655,\"start\":73652},{\"end\":73674,\"start\":73670},{\"end\":73691,\"start\":73688},{\"end\":73904,\"start\":73901},{\"end\":73918,\"start\":73915},{\"end\":73927,\"start\":73925},{\"end\":73939,\"start\":73932},{\"end\":73953,\"start\":73948},{\"end\":74207,\"start\":74200},{\"end\":74222,\"start\":74215},{\"end\":74235,\"start\":74228},{\"end\":74248,\"start\":74242},{\"end\":74250,\"start\":74249},{\"end\":74598,\"start\":74594},{\"end\":74613,\"start\":74605},{\"end\":74621,\"start\":74618},{\"end\":74867,\"start\":74861},{\"end\":74886,\"start\":74877},{\"end\":74896,\"start\":74894},{\"end\":75241,\"start\":75237},{\"end\":75254,\"start\":75250},{\"end\":75267,\"start\":75261},{\"end\":75281,\"start\":75275},{\"end\":75526,\"start\":75522},{\"end\":75540,\"start\":75535},{\"end\":75555,\"start\":75550},{\"end\":75572,\"start\":75566},{\"end\":75586,\"start\":75580},{\"end\":75601,\"start\":75597},{\"end\":76333,\"start\":76329},{\"end\":76348,\"start\":76342},{\"end\":76360,\"start\":76356},{\"end\":76843,\"start\":76839},{\"end\":76858,\"start\":76852},{\"end\":76871,\"start\":76866},{\"end\":76886,\"start\":76881},{\"end\":76903,\"start\":76897},{\"end\":76918,\"start\":76914},{\"end\":77397,\"start\":77390},{\"end\":77411,\"start\":77407},{\"end\":77575,\"start\":77571},{\"end\":77585,\"start\":77581},{\"end\":77599,\"start\":77594},{\"end\":77616,\"start\":77607},{\"end\":77618,\"start\":77617},{\"end\":77781,\"start\":77780},{\"end\":77800,\"start\":77792},{\"end\":78205,\"start\":78204},{\"end\":78219,\"start\":78216},{\"end\":78499,\"start\":78495},{\"end\":78511,\"start\":78506},{\"end\":78513,\"start\":78512},{\"end\":78766,\"start\":78760},{\"end\":78794,\"start\":78789},{\"end\":78799,\"start\":78795},{\"end\":78814,\"start\":78810},{\"end\":78830,\"start\":78827},{\"end\":79051,\"start\":79045},{\"end\":79062,\"start\":79056},{\"end\":79075,\"start\":79068},{\"end\":79086,\"start\":79081},{\"end\":79097,\"start\":79093},{\"end\":79343,\"start\":79338},{\"end\":79364,\"start\":79356},{\"end\":79380,\"start\":79372},{\"end\":79395,\"start\":79387},{\"end\":79405,\"start\":79402},{\"end\":79422,\"start\":79411},{\"end\":79704,\"start\":79698},{\"end\":79714,\"start\":79710},{\"end\":79729,\"start\":79720},{\"end\":79941,\"start\":79940},{\"end\":79957,\"start\":79949},{\"end\":79970,\"start\":79963},{\"end\":79981,\"start\":79978},{\"end\":79991,\"start\":79987},{\"end\":80008,\"start\":80002},{\"end\":80022,\"start\":80018},{\"end\":80345,\"start\":80340},{\"end\":80355,\"start\":80351},{\"end\":80364,\"start\":80361},{\"end\":80378,\"start\":80370},{\"end\":80391,\"start\":80385},{\"end\":80890,\"start\":80885},{\"end\":80902,\"start\":80899},{\"end\":81321,\"start\":81315},{\"end\":81337,\"start\":81331},{\"end\":81355,\"start\":81348},{\"end\":81374,\"start\":81367},{\"end\":81575,\"start\":81568},{\"end\":81587,\"start\":81582},{\"end\":81602,\"start\":81596},{\"end\":81618,\"start\":81613},{\"end\":81620,\"start\":81619},{\"end\":81819,\"start\":81815},{\"end\":81836,\"start\":81828},{\"end\":82022,\"start\":82017},{\"end\":82042,\"start\":82032},{\"end\":82058,\"start\":82052},{\"end\":82068,\"start\":82063},{\"end\":82087,\"start\":82079},{\"end\":82106,\"start\":82093},{\"end\":82416,\"start\":82415},{\"end\":82425,\"start\":82424},{\"end\":82427,\"start\":82426},{\"end\":82735,\"start\":82734},{\"end\":82748,\"start\":82743},{\"end\":82765,\"start\":82757},{\"end\":82782,\"start\":82777},{\"end\":83011,\"start\":83004},{\"end\":83025,\"start\":83018},{\"end\":83040,\"start\":83031},{\"end\":83054,\"start\":83047},{\"end\":83490,\"start\":83484},{\"end\":83506,\"start\":83499},{\"end\":83519,\"start\":83511},{\"end\":83532,\"start\":83525},{\"end\":83673,\"start\":83669},{\"end\":83688,\"start\":83682},{\"end\":83703,\"start\":83698},{\"end\":83721,\"start\":83715},{\"end\":83734,\"start\":83730},{\"end\":83753,\"start\":83744},{\"end\":83764,\"start\":83758},{\"end\":84053,\"start\":84044},{\"end\":84072,\"start\":84066},{\"end\":84074,\"start\":84073},{\"end\":84094,\"start\":84084},{\"end\":84510,\"start\":84509},{\"end\":84656,\"start\":84655},{\"end\":84673,\"start\":84666},{\"end\":84679,\"start\":84678},{\"end\":84692,\"start\":84687},{\"end\":84704,\"start\":84699},{\"end\":84717,\"start\":84713},{\"end\":84923,\"start\":84917},{\"end\":84939,\"start\":84932},{\"end\":84955,\"start\":84948},{\"end\":84966,\"start\":84961},{\"end\":84980,\"start\":84973},{\"end\":84991,\"start\":84987},{\"end\":85005,\"start\":85001},{\"end\":85016,\"start\":85012},{\"end\":85301,\"start\":85297},{\"end\":85316,\"start\":85310},{\"end\":85332,\"start\":85327},{\"end\":85347,\"start\":85341},{\"end\":85366,\"start\":85359},{\"end\":85769,\"start\":85766},{\"end\":85783,\"start\":85778},{\"end\":85803,\"start\":85798},{\"end\":86043,\"start\":86037},{\"end\":86067,\"start\":86061},{\"end\":86081,\"start\":86077},{\"end\":86620,\"start\":86615},{\"end\":86637,\"start\":86630},{\"end\":86650,\"start\":86645},{\"end\":87292,\"start\":87287},{\"end\":87309,\"start\":87302},{\"end\":87322,\"start\":87317},{\"end\":87839,\"start\":87832},{\"end\":87859,\"start\":87851},{\"end\":87876,\"start\":87869},{\"end\":87889,\"start\":87883},{\"end\":87902,\"start\":87897},{\"end\":87923,\"start\":87913},{\"end\":88233,\"start\":88228},{\"end\":88250,\"start\":88245},{\"end\":88264,\"start\":88260},{\"end\":88721,\"start\":88718},{\"end\":88899,\"start\":88889},{\"end\":88918,\"start\":88910},{\"end\":88938,\"start\":88932},{\"end\":88954,\"start\":88949},{\"end\":88966,\"start\":88962},{\"end\":89312,\"start\":89307},{\"end\":89324,\"start\":89321},{\"end\":89944,\"start\":89939},{\"end\":89962,\"start\":89959},{\"end\":89982,\"start\":89977},{\"end\":90309,\"start\":90304},{\"end\":90327,\"start\":90324},{\"end\":90347,\"start\":90342},{\"end\":90362,\"start\":90357},{\"end\":90377,\"start\":90373},{\"end\":90391,\"start\":90386},{\"end\":90627,\"start\":90622},{\"end\":90647,\"start\":90642},{\"end\":90662,\"start\":90657},{\"end\":90802,\"start\":90796},{\"end\":90826,\"start\":90822},{\"end\":90844,\"start\":90835},{\"end\":90858,\"start\":90852},{\"end\":90879,\"start\":90870},{\"end\":90894,\"start\":90891},{\"end\":90908,\"start\":90903},{\"end\":90924,\"start\":90919},{\"end\":90944,\"start\":90934},{\"end\":90962,\"start\":90954},{\"end\":90979,\"start\":90973},{\"end\":90995,\"start\":90990},{\"end\":91006,\"start\":91002},{\"end\":91020,\"start\":91015},{\"end\":91032,\"start\":91027},{\"end\":91047,\"start\":91042},{\"end\":91059,\"start\":91054},{\"end\":91075,\"start\":91070},{\"end\":91093,\"start\":91092},{\"end\":91111,\"start\":91105},{\"end\":91121,\"start\":91117},{\"end\":91123,\"start\":91122},{\"end\":91137,\"start\":91131},{\"end\":91156,\"start\":91149},{\"end\":91171,\"start\":91166},{\"end\":91187,\"start\":91182},{\"end\":91204,\"start\":91198},{\"end\":91220,\"start\":91214},{\"end\":91789,\"start\":91783},{\"end\":91803,\"start\":91799},{\"end\":91817,\"start\":91813},{\"end\":91831,\"start\":91826},{\"end\":91848,\"start\":91843},{\"end\":91861,\"start\":91856},{\"end\":91863,\"start\":91862},{\"end\":91877,\"start\":91871},{\"end\":91891,\"start\":91886},{\"end\":92404,\"start\":92395},{\"end\":92418,\"start\":92411},{\"end\":92431,\"start\":92424},{\"end\":92443,\"start\":92437},{\"end\":92452,\"start\":92449},{\"end\":92465,\"start\":92460},{\"end\":92917,\"start\":92913},{\"end\":92936,\"start\":92931},{\"end\":92953,\"start\":92948},{\"end\":93199,\"start\":93191},{\"end\":93213,\"start\":93206},{\"end\":93224,\"start\":93221},{\"end\":93237,\"start\":93232},{\"end\":93473,\"start\":93465},{\"end\":93484,\"start\":93480},{\"end\":93495,\"start\":93490},{\"end\":93506,\"start\":93502},{\"end\":93694,\"start\":93688},{\"end\":93704,\"start\":93699},{\"end\":93718,\"start\":93712},{\"end\":93728,\"start\":93725},{\"end\":93744,\"start\":93735},{\"end\":94158,\"start\":94155},{\"end\":94168,\"start\":94166},{\"end\":94180,\"start\":94176},{\"end\":94189,\"start\":94187},{\"end\":94200,\"start\":94196},{\"end\":94571,\"start\":94564},{\"end\":94586,\"start\":94579},{\"end\":94600,\"start\":94594},{\"end\":94602,\"start\":94601},{\"end\":94613,\"start\":94610},{\"end\":94631,\"start\":94625},{\"end\":94902,\"start\":94895},{\"end\":94917,\"start\":94910},{\"end\":94931,\"start\":94925},{\"end\":94933,\"start\":94932},{\"end\":94944,\"start\":94941},{\"end\":94962,\"start\":94956},{\"end\":95371,\"start\":95366},{\"end\":95382,\"start\":95378},{\"end\":95395,\"start\":95389},{\"end\":95407,\"start\":95402},{\"end\":95421,\"start\":95416},{\"end\":95439,\"start\":95432},{\"end\":95774,\"start\":95767},{\"end\":95788,\"start\":95781},{\"end\":95805,\"start\":95799},{\"end\":95819,\"start\":95811},{\"end\":95833,\"start\":95827},{\"end\":95835,\"start\":95834},{\"end\":96041,\"start\":96035},{\"end\":96053,\"start\":96047},{\"end\":96067,\"start\":96061},{\"end\":96078,\"start\":96073}]", "bib_author_last_name": "[{\"end\":63801,\"start\":63796},{\"end\":63822,\"start\":63815},{\"end\":63833,\"start\":63828},{\"end\":63847,\"start\":63841},{\"end\":64096,\"start\":64088},{\"end\":64111,\"start\":64108},{\"end\":64126,\"start\":64120},{\"end\":64381,\"start\":64375},{\"end\":64399,\"start\":64392},{\"end\":64418,\"start\":64409},{\"end\":64683,\"start\":64678},{\"end\":64697,\"start\":64690},{\"end\":64713,\"start\":64705},{\"end\":65006,\"start\":65001},{\"end\":65021,\"start\":65017},{\"end\":65033,\"start\":65028},{\"end\":65050,\"start\":65043},{\"end\":65064,\"start\":65058},{\"end\":65083,\"start\":65075},{\"end\":65103,\"start\":65092},{\"end\":65117,\"start\":65112},{\"end\":65132,\"start\":65126},{\"end\":65147,\"start\":65141},{\"end\":65165,\"start\":65158},{\"end\":65185,\"start\":65173},{\"end\":65203,\"start\":65196},{\"end\":65217,\"start\":65209},{\"end\":65230,\"start\":65225},{\"end\":65245,\"start\":65239},{\"end\":65263,\"start\":65256},{\"end\":65275,\"start\":65273},{\"end\":65291,\"start\":65285},{\"end\":65310,\"start\":65305},{\"end\":65321,\"start\":65317},{\"end\":65334,\"start\":65328},{\"end\":65350,\"start\":65344},{\"end\":66168,\"start\":66162},{\"end\":66185,\"start\":66177},{\"end\":66203,\"start\":66196},{\"end\":66577,\"start\":66561},{\"end\":66591,\"start\":66581},{\"end\":66607,\"start\":66599},{\"end\":66617,\"start\":66611},{\"end\":66625,\"start\":66619},{\"end\":66998,\"start\":66994},{\"end\":67012,\"start\":67005},{\"end\":67025,\"start\":67020},{\"end\":67034,\"start\":67032},{\"end\":67046,\"start\":67043},{\"end\":67065,\"start\":67057},{\"end\":67077,\"start\":67073},{\"end\":67093,\"start\":67084},{\"end\":67367,\"start\":67363},{\"end\":67383,\"start\":67377},{\"end\":67780,\"start\":67776},{\"end\":67795,\"start\":67789},{\"end\":67816,\"start\":67805},{\"end\":67831,\"start\":67825},{\"end\":68348,\"start\":68343},{\"end\":68360,\"start\":68356},{\"end\":68374,\"start\":68367},{\"end\":68390,\"start\":68381},{\"end\":68562,\"start\":68559},{\"end\":68576,\"start\":68572},{\"end\":68837,\"start\":68833},{\"end\":68847,\"start\":68843},{\"end\":68863,\"start\":68857},{\"end\":68874,\"start\":68872},{\"end\":68882,\"start\":68880},{\"end\":68894,\"start\":68887},{\"end\":69265,\"start\":69257},{\"end\":69278,\"start\":69272},{\"end\":69472,\"start\":69461},{\"end\":69485,\"start\":69480},{\"end\":69507,\"start\":69497},{\"end\":69525,\"start\":69514},{\"end\":69539,\"start\":69535},{\"end\":69559,\"start\":69548},{\"end\":69577,\"start\":69569},{\"end\":69596,\"start\":69588},{\"end\":69611,\"start\":69604},{\"end\":69626,\"start\":69621},{\"end\":69993,\"start\":69982},{\"end\":70006,\"start\":70002},{\"end\":70472,\"start\":70467},{\"end\":70487,\"start\":70480},{\"end\":70500,\"start\":70495},{\"end\":70899,\"start\":70892},{\"end\":70916,\"start\":70908},{\"end\":70932,\"start\":70924},{\"end\":71147,\"start\":71137},{\"end\":71167,\"start\":71154},{\"end\":71180,\"start\":71175},{\"end\":71189,\"start\":71187},{\"end\":71209,\"start\":71197},{\"end\":71224,\"start\":71219},{\"end\":71243,\"start\":71234},{\"end\":71258,\"start\":71252},{\"end\":71760,\"start\":71757},{\"end\":71778,\"start\":71768},{\"end\":71794,\"start\":71787},{\"end\":71814,\"start\":71805},{\"end\":71827,\"start\":71824},{\"end\":72092,\"start\":72090},{\"end\":72107,\"start\":72102},{\"end\":72121,\"start\":72118},{\"end\":72131,\"start\":72128},{\"end\":72359,\"start\":72351},{\"end\":72371,\"start\":72366},{\"end\":72385,\"start\":72378},{\"end\":72988,\"start\":72980},{\"end\":73000,\"start\":72995},{\"end\":73014,\"start\":73007},{\"end\":73444,\"start\":73442},{\"end\":73455,\"start\":73451},{\"end\":73470,\"start\":73464},{\"end\":73650,\"start\":73648},{\"end\":73668,\"start\":73656},{\"end\":73686,\"start\":73675},{\"end\":73700,\"start\":73692},{\"end\":73913,\"start\":73905},{\"end\":73923,\"start\":73919},{\"end\":73930,\"start\":73928},{\"end\":73946,\"start\":73940},{\"end\":73958,\"start\":73954},{\"end\":74213,\"start\":74208},{\"end\":74226,\"start\":74223},{\"end\":74240,\"start\":74236},{\"end\":74256,\"start\":74251},{\"end\":74603,\"start\":74599},{\"end\":74616,\"start\":74614},{\"end\":74627,\"start\":74622},{\"end\":74875,\"start\":74868},{\"end\":74892,\"start\":74887},{\"end\":74904,\"start\":74897},{\"end\":75248,\"start\":75242},{\"end\":75259,\"start\":75255},{\"end\":75273,\"start\":75268},{\"end\":75290,\"start\":75282},{\"end\":75533,\"start\":75527},{\"end\":75548,\"start\":75541},{\"end\":75564,\"start\":75556},{\"end\":75578,\"start\":75573},{\"end\":75595,\"start\":75587},{\"end\":75606,\"start\":75602},{\"end\":76340,\"start\":76334},{\"end\":76354,\"start\":76349},{\"end\":76365,\"start\":76361},{\"end\":76850,\"start\":76844},{\"end\":76864,\"start\":76859},{\"end\":76879,\"start\":76872},{\"end\":76895,\"start\":76887},{\"end\":76912,\"start\":76904},{\"end\":76923,\"start\":76919},{\"end\":77405,\"start\":77398},{\"end\":77419,\"start\":77412},{\"end\":77579,\"start\":77576},{\"end\":77592,\"start\":77586},{\"end\":77605,\"start\":77600},{\"end\":77623,\"start\":77619},{\"end\":77790,\"start\":77782},{\"end\":77807,\"start\":77801},{\"end\":77817,\"start\":77809},{\"end\":78214,\"start\":78206},{\"end\":78226,\"start\":78220},{\"end\":78235,\"start\":78228},{\"end\":78504,\"start\":78500},{\"end\":78530,\"start\":78514},{\"end\":78541,\"start\":78532},{\"end\":78787,\"start\":78767},{\"end\":78808,\"start\":78800},{\"end\":78825,\"start\":78815},{\"end\":78838,\"start\":78831},{\"end\":79054,\"start\":79052},{\"end\":79066,\"start\":79063},{\"end\":79079,\"start\":79076},{\"end\":79091,\"start\":79087},{\"end\":79101,\"start\":79098},{\"end\":79354,\"start\":79344},{\"end\":79370,\"start\":79365},{\"end\":79385,\"start\":79381},{\"end\":79400,\"start\":79396},{\"end\":79409,\"start\":79406},{\"end\":79427,\"start\":79423},{\"end\":79708,\"start\":79705},{\"end\":79718,\"start\":79715},{\"end\":79733,\"start\":79730},{\"end\":79947,\"start\":79942},{\"end\":79961,\"start\":79958},{\"end\":79976,\"start\":79971},{\"end\":79985,\"start\":79982},{\"end\":80000,\"start\":79992},{\"end\":80016,\"start\":80009},{\"end\":80029,\"start\":80023},{\"end\":80038,\"start\":80031},{\"end\":80349,\"start\":80346},{\"end\":80359,\"start\":80356},{\"end\":80368,\"start\":80365},{\"end\":80383,\"start\":80379},{\"end\":80396,\"start\":80392},{\"end\":80897,\"start\":80891},{\"end\":80915,\"start\":80903},{\"end\":81329,\"start\":81322},{\"end\":81346,\"start\":81338},{\"end\":81365,\"start\":81356},{\"end\":81384,\"start\":81375},{\"end\":81580,\"start\":81576},{\"end\":81594,\"start\":81588},{\"end\":81611,\"start\":81603},{\"end\":81630,\"start\":81621},{\"end\":81826,\"start\":81820},{\"end\":81845,\"start\":81837},{\"end\":82030,\"start\":82023},{\"end\":82050,\"start\":82043},{\"end\":82061,\"start\":82059},{\"end\":82077,\"start\":82069},{\"end\":82091,\"start\":82088},{\"end\":82110,\"start\":82107},{\"end\":82422,\"start\":82417},{\"end\":82435,\"start\":82428},{\"end\":82741,\"start\":82736},{\"end\":82755,\"start\":82749},{\"end\":82775,\"start\":82766},{\"end\":82786,\"start\":82783},{\"end\":82797,\"start\":82788},{\"end\":83016,\"start\":83012},{\"end\":83029,\"start\":83026},{\"end\":83045,\"start\":83041},{\"end\":83058,\"start\":83055},{\"end\":83497,\"start\":83491},{\"end\":83509,\"start\":83507},{\"end\":83523,\"start\":83520},{\"end\":83535,\"start\":83533},{\"end\":83680,\"start\":83674},{\"end\":83696,\"start\":83689},{\"end\":83713,\"start\":83704},{\"end\":83728,\"start\":83722},{\"end\":83742,\"start\":83735},{\"end\":83756,\"start\":83754},{\"end\":83769,\"start\":83765},{\"end\":84064,\"start\":84054},{\"end\":84082,\"start\":84075},{\"end\":84102,\"start\":84095},{\"end\":84518,\"start\":84511},{\"end\":84664,\"start\":84657},{\"end\":84676,\"start\":84674},{\"end\":84685,\"start\":84680},{\"end\":84697,\"start\":84693},{\"end\":84711,\"start\":84705},{\"end\":84727,\"start\":84718},{\"end\":84930,\"start\":84924},{\"end\":84946,\"start\":84940},{\"end\":84959,\"start\":84956},{\"end\":84971,\"start\":84967},{\"end\":84985,\"start\":84981},{\"end\":84999,\"start\":84992},{\"end\":85010,\"start\":85006},{\"end\":85026,\"start\":85017},{\"end\":85308,\"start\":85302},{\"end\":85325,\"start\":85317},{\"end\":85339,\"start\":85333},{\"end\":85357,\"start\":85348},{\"end\":85373,\"start\":85367},{\"end\":85776,\"start\":85770},{\"end\":85796,\"start\":85784},{\"end\":85811,\"start\":85804},{\"end\":86059,\"start\":86044},{\"end\":86075,\"start\":86068},{\"end\":86090,\"start\":86082},{\"end\":86628,\"start\":86621},{\"end\":86643,\"start\":86638},{\"end\":86656,\"start\":86651},{\"end\":87300,\"start\":87293},{\"end\":87315,\"start\":87310},{\"end\":87328,\"start\":87323},{\"end\":87849,\"start\":87840},{\"end\":87867,\"start\":87860},{\"end\":87881,\"start\":87877},{\"end\":87895,\"start\":87890},{\"end\":87911,\"start\":87903},{\"end\":87929,\"start\":87924},{\"end\":88243,\"start\":88234},{\"end\":88258,\"start\":88251},{\"end\":88272,\"start\":88265},{\"end\":88732,\"start\":88722},{\"end\":88908,\"start\":88900},{\"end\":88930,\"start\":88919},{\"end\":88947,\"start\":88939},{\"end\":88960,\"start\":88955},{\"end\":88971,\"start\":88967},{\"end\":89319,\"start\":89313},{\"end\":89330,\"start\":89325},{\"end\":89957,\"start\":89945},{\"end\":89975,\"start\":89963},{\"end\":89994,\"start\":89983},{\"end\":90322,\"start\":90310},{\"end\":90340,\"start\":90328},{\"end\":90355,\"start\":90348},{\"end\":90371,\"start\":90363},{\"end\":90384,\"start\":90378},{\"end\":90403,\"start\":90392},{\"end\":90640,\"start\":90628},{\"end\":90655,\"start\":90648},{\"end\":90674,\"start\":90663},{\"end\":90820,\"start\":90803},{\"end\":90833,\"start\":90827},{\"end\":90850,\"start\":90845},{\"end\":90868,\"start\":90859},{\"end\":90889,\"start\":90880},{\"end\":90901,\"start\":90895},{\"end\":90917,\"start\":90909},{\"end\":90932,\"start\":90925},{\"end\":90952,\"start\":90945},{\"end\":90971,\"start\":90963},{\"end\":90988,\"start\":90980},{\"end\":91000,\"start\":90996},{\"end\":91013,\"start\":91007},{\"end\":91025,\"start\":91021},{\"end\":91040,\"start\":91033},{\"end\":91052,\"start\":91048},{\"end\":91068,\"start\":91060},{\"end\":91084,\"start\":91076},{\"end\":91090,\"start\":91086},{\"end\":91103,\"start\":91094},{\"end\":91115,\"start\":91112},{\"end\":91129,\"start\":91124},{\"end\":91147,\"start\":91138},{\"end\":91164,\"start\":91157},{\"end\":91180,\"start\":91172},{\"end\":91196,\"start\":91188},{\"end\":91212,\"start\":91205},{\"end\":91226,\"start\":91221},{\"end\":91797,\"start\":91790},{\"end\":91811,\"start\":91804},{\"end\":91824,\"start\":91818},{\"end\":91841,\"start\":91832},{\"end\":91854,\"start\":91849},{\"end\":91869,\"start\":91864},{\"end\":91884,\"start\":91878},{\"end\":91902,\"start\":91892},{\"end\":92409,\"start\":92405},{\"end\":92422,\"start\":92419},{\"end\":92435,\"start\":92432},{\"end\":92447,\"start\":92444},{\"end\":92458,\"start\":92453},{\"end\":92475,\"start\":92466},{\"end\":92929,\"start\":92918},{\"end\":92946,\"start\":92937},{\"end\":92963,\"start\":92954},{\"end\":93204,\"start\":93200},{\"end\":93219,\"start\":93214},{\"end\":93230,\"start\":93225},{\"end\":93244,\"start\":93238},{\"end\":93478,\"start\":93474},{\"end\":93488,\"start\":93485},{\"end\":93500,\"start\":93496},{\"end\":93511,\"start\":93507},{\"end\":93697,\"start\":93695},{\"end\":93710,\"start\":93705},{\"end\":93723,\"start\":93719},{\"end\":93733,\"start\":93729},{\"end\":93749,\"start\":93745},{\"end\":93755,\"start\":93751},{\"end\":94164,\"start\":94159},{\"end\":94174,\"start\":94169},{\"end\":94185,\"start\":94181},{\"end\":94194,\"start\":94190},{\"end\":94204,\"start\":94201},{\"end\":94577,\"start\":94572},{\"end\":94592,\"start\":94587},{\"end\":94608,\"start\":94603},{\"end\":94623,\"start\":94614},{\"end\":94636,\"start\":94632},{\"end\":94908,\"start\":94903},{\"end\":94923,\"start\":94918},{\"end\":94939,\"start\":94934},{\"end\":94954,\"start\":94945},{\"end\":94967,\"start\":94963},{\"end\":95376,\"start\":95372},{\"end\":95387,\"start\":95383},{\"end\":95400,\"start\":95396},{\"end\":95414,\"start\":95408},{\"end\":95430,\"start\":95422},{\"end\":95448,\"start\":95440},{\"end\":95779,\"start\":95775},{\"end\":95797,\"start\":95789},{\"end\":95809,\"start\":95806},{\"end\":95825,\"start\":95820},{\"end\":95841,\"start\":95836},{\"end\":96045,\"start\":96042},{\"end\":96059,\"start\":96054},{\"end\":96071,\"start\":96068},{\"end\":96084,\"start\":96079}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":64007,\"start\":63714},{\"attributes\":{\"id\":\"b1\"},\"end\":64258,\"start\":64009},{\"attributes\":{\"id\":\"b2\"},\"end\":64601,\"start\":64260},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":52889459},\"end\":64993,\"start\":64603},{\"attributes\":{\"doi\":\"arXiv:2005.14165\",\"id\":\"b4\"},\"end\":66105,\"start\":64995},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":4396518},\"end\":66444,\"start\":66107},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":3429309},\"end\":66951,\"start\":66446},{\"attributes\":{\"id\":\"b7\"},\"end\":67290,\"start\":66953},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":8191987},\"end\":67714,\"start\":67292},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b9\",\"matched_paper_id\":6199526},\"end\":68062,\"start\":67716},{\"attributes\":{\"id\":\"b10\"},\"end\":68283,\"start\":68064},{\"attributes\":{\"id\":\"b11\"},\"end\":68516,\"start\":68285},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":76666188},\"end\":68774,\"start\":68518},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":57246310},\"end\":69199,\"start\":68776},{\"attributes\":{\"id\":\"b14\"},\"end\":69376,\"start\":69201},{\"attributes\":{\"id\":\"b15\"},\"end\":69896,\"start\":69378},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":8758543},\"end\":70368,\"start\":69898},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":216562905},\"end\":70794,\"start\":70370},{\"attributes\":{\"id\":\"b18\"},\"end\":71100,\"start\":70796},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":1033682},\"end\":71671,\"start\":71102},{\"attributes\":{\"doi\":\"abs/2009.04433\",\"id\":\"b20\"},\"end\":72034,\"start\":71673},{\"attributes\":{\"doi\":\"abs/1512.03385\",\"id\":\"b21\"},\"end\":72281,\"start\":72036},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":49658725},\"end\":72884,\"start\":72283},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":118647892},\"end\":73389,\"start\":72886},{\"attributes\":{\"id\":\"b24\"},\"end\":73565,\"start\":73391},{\"attributes\":{\"id\":\"b25\"},\"end\":73853,\"start\":73567},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":127986954},\"end\":74132,\"start\":73855},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":6200260},\"end\":74540,\"start\":74134},{\"attributes\":{\"doi\":\"arXiv:1611.01144\",\"id\":\"b28\"},\"end\":74790,\"start\":74542},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":980236},\"end\":75137,\"start\":74792},{\"attributes\":{\"id\":\"b30\"},\"end\":75460,\"start\":75139},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":219636053},\"end\":76253,\"start\":75462},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":54482423},\"end\":76782,\"start\":76255},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":209202273},\"end\":77312,\"start\":76784},{\"attributes\":{\"id\":\"b34\"},\"end\":77538,\"start\":77314},{\"attributes\":{\"id\":\"b35\"},\"end\":77722,\"start\":77540},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":49657329},\"end\":78169,\"start\":77724},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":216078090},\"end\":78440,\"start\":78171},{\"attributes\":{\"doi\":\"abs/1602.03220\",\"id\":\"b38\"},\"end\":78696,\"start\":78442},{\"attributes\":{\"id\":\"b39\"},\"end\":78993,\"start\":78698},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":59413863},\"end\":79276,\"start\":78995},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":90262507},\"end\":79633,\"start\":79278},{\"attributes\":{\"doi\":\"abs/1909.03611\",\"id\":\"b42\"},\"end\":79886,\"start\":79635},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":3608234},\"end\":80252,\"start\":79888},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":206593370},\"end\":80790,\"start\":80254},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":54458552},\"end\":81269,\"start\":80792},{\"attributes\":{\"doi\":\"2020. 4\",\"id\":\"b46\"},\"end\":81519,\"start\":81271},{\"attributes\":{\"id\":\"b47\"},\"end\":81762,\"start\":81521},{\"attributes\":{\"id\":\"b48\"},\"end\":81947,\"start\":81764},{\"attributes\":{\"doi\":\"10.5281/zenodo.4957738\",\"id\":\"b49\"},\"end\":82360,\"start\":81949},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":7005637},\"end\":82669,\"start\":82362},{\"attributes\":{\"id\":\"b51\"},\"end\":82938,\"start\":82671},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":81981856},\"end\":83435,\"start\":82940},{\"attributes\":{\"id\":\"b53\"},\"end\":83648,\"start\":83437},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b54\",\"matched_paper_id\":3353110},\"end\":84009,\"start\":83650},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":215548657},\"end\":84446,\"start\":84011},{\"attributes\":{\"id\":\"b56\"},\"end\":84600,\"start\":84448},{\"attributes\":{\"id\":\"b57\"},\"end\":84879,\"start\":84602},{\"attributes\":{\"id\":\"b58\"},\"end\":85198,\"start\":84881},{\"attributes\":{\"doi\":\"2020. 5\",\"id\":\"b59\",\"matched_paper_id\":195776274},\"end\":85709,\"start\":85200},{\"attributes\":{\"id\":\"b60\"},\"end\":85955,\"start\":85711},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":16895865},\"end\":86524,\"start\":85957},{\"attributes\":{\"id\":\"b62\"},\"end\":86970,\"start\":86526},{\"attributes\":{\"id\":\"b63\"},\"end\":87209,\"start\":86972},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":226290226},\"end\":87779,\"start\":87211},{\"attributes\":{\"doi\":\"arXiv:1906.09453\",\"id\":\"b65\"},\"end\":88161,\"start\":87781},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":211572480},\"end\":88632,\"start\":88163},{\"attributes\":{\"id\":\"b67\"},\"end\":88841,\"start\":88634},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":202767986},\"end\":89254,\"start\":88843},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":220403493},\"end\":89904,\"start\":89256},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":8142135},\"end\":90249,\"start\":89906},{\"attributes\":{\"id\":\"b71\"},\"end\":90579,\"start\":90251},{\"attributes\":{\"id\":\"b72\"},\"end\":90792,\"start\":90581},{\"attributes\":{\"id\":\"b73\"},\"end\":91754,\"start\":90794},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":13756489},\"end\":92312,\"start\":91756},{\"attributes\":{\"id\":\"b75\",\"matched_paper_id\":41805341},\"end\":92874,\"start\":92314},{\"attributes\":{\"id\":\"b76\",\"matched_paper_id\":174802916},\"end\":93112,\"start\":92876},{\"attributes\":{\"id\":\"b77\"},\"end\":93389,\"start\":93114},{\"attributes\":{\"doi\":\"abs/1905.10485\",\"id\":\"b78\"},\"end\":93686,\"start\":93391},{\"attributes\":{\"doi\":\"arXiv:1506.03365\",\"id\":\"b79\"},\"end\":94078,\"start\":93688},{\"attributes\":{\"id\":\"b80\",\"matched_paper_id\":215745600},\"end\":94490,\"start\":94080},{\"attributes\":{\"id\":\"b81\",\"matched_paper_id\":4766599},\"end\":94821,\"start\":94492},{\"attributes\":{\"id\":\"b82\",\"matched_paper_id\":4766599},\"end\":95364,\"start\":94823},{\"attributes\":{\"doi\":\"arXiv:1608.05442\",\"id\":\"b83\"},\"end\":95730,\"start\":95366},{\"attributes\":{\"id\":\"b84\"},\"end\":95966,\"start\":95732},{\"attributes\":{\"id\":\"b85\"},\"end\":96215,\"start\":95968}]", "bib_title": "[{\"end\":64669,\"start\":64603},{\"end\":66153,\"start\":66107},{\"end\":66557,\"start\":66446},{\"end\":67354,\"start\":67292},{\"end\":67771,\"start\":67716},{\"end\":68553,\"start\":68518},{\"end\":68827,\"start\":68776},{\"end\":69973,\"start\":69898},{\"end\":70457,\"start\":70370},{\"end\":71129,\"start\":71102},{\"end\":72345,\"start\":72283},{\"end\":72974,\"start\":72886},{\"end\":73899,\"start\":73855},{\"end\":74198,\"start\":74134},{\"end\":74859,\"start\":74792},{\"end\":75520,\"start\":75462},{\"end\":76327,\"start\":76255},{\"end\":76837,\"start\":76784},{\"end\":77778,\"start\":77724},{\"end\":78202,\"start\":78171},{\"end\":79043,\"start\":78995},{\"end\":79336,\"start\":79278},{\"end\":79938,\"start\":79888},{\"end\":80338,\"start\":80254},{\"end\":80883,\"start\":80792},{\"end\":82413,\"start\":82362},{\"end\":83002,\"start\":82940},{\"end\":83667,\"start\":83650},{\"end\":84042,\"start\":84011},{\"end\":85295,\"start\":85200},{\"end\":86035,\"start\":85957},{\"end\":87285,\"start\":87211},{\"end\":88226,\"start\":88163},{\"end\":88887,\"start\":88843},{\"end\":89305,\"start\":89256},{\"end\":89937,\"start\":89906},{\"end\":91781,\"start\":91756},{\"end\":92393,\"start\":92314},{\"end\":92911,\"start\":92876},{\"end\":94153,\"start\":94080},{\"end\":94562,\"start\":94492},{\"end\":94893,\"start\":94823}]", "bib_author": "[{\"end\":63803,\"start\":63790},{\"end\":63824,\"start\":63803},{\"end\":63835,\"start\":63824},{\"end\":63849,\"start\":63835},{\"end\":64098,\"start\":64080},{\"end\":64113,\"start\":64098},{\"end\":64128,\"start\":64113},{\"end\":64383,\"start\":64368},{\"end\":64401,\"start\":64383},{\"end\":64420,\"start\":64401},{\"end\":64685,\"start\":64671},{\"end\":64699,\"start\":64685},{\"end\":64715,\"start\":64699},{\"end\":65008,\"start\":64995},{\"end\":65023,\"start\":65008},{\"end\":65035,\"start\":65023},{\"end\":65052,\"start\":65035},{\"end\":65066,\"start\":65052},{\"end\":65085,\"start\":65066},{\"end\":65105,\"start\":65085},{\"end\":65119,\"start\":65105},{\"end\":65134,\"start\":65119},{\"end\":65149,\"start\":65134},{\"end\":65167,\"start\":65149},{\"end\":65187,\"start\":65167},{\"end\":65205,\"start\":65187},{\"end\":65219,\"start\":65205},{\"end\":65232,\"start\":65219},{\"end\":65247,\"start\":65232},{\"end\":65265,\"start\":65247},{\"end\":65277,\"start\":65265},{\"end\":65293,\"start\":65277},{\"end\":65312,\"start\":65293},{\"end\":65323,\"start\":65312},{\"end\":65336,\"start\":65323},{\"end\":65352,\"start\":65336},{\"end\":66170,\"start\":66155},{\"end\":66187,\"start\":66170},{\"end\":66205,\"start\":66187},{\"end\":66579,\"start\":66559},{\"end\":66593,\"start\":66579},{\"end\":66609,\"start\":66593},{\"end\":66619,\"start\":66609},{\"end\":66627,\"start\":66619},{\"end\":67000,\"start\":66989},{\"end\":67014,\"start\":67000},{\"end\":67027,\"start\":67014},{\"end\":67036,\"start\":67027},{\"end\":67048,\"start\":67036},{\"end\":67067,\"start\":67048},{\"end\":67079,\"start\":67067},{\"end\":67095,\"start\":67079},{\"end\":67369,\"start\":67356},{\"end\":67385,\"start\":67369},{\"end\":67782,\"start\":67773},{\"end\":67797,\"start\":67782},{\"end\":67818,\"start\":67797},{\"end\":67833,\"start\":67818},{\"end\":68350,\"start\":68337},{\"end\":68362,\"start\":68350},{\"end\":68376,\"start\":68362},{\"end\":68392,\"start\":68376},{\"end\":68564,\"start\":68555},{\"end\":68578,\"start\":68564},{\"end\":68839,\"start\":68829},{\"end\":68849,\"start\":68839},{\"end\":68865,\"start\":68849},{\"end\":68876,\"start\":68865},{\"end\":68884,\"start\":68876},{\"end\":68896,\"start\":68884},{\"end\":69267,\"start\":69248},{\"end\":69280,\"start\":69267},{\"end\":69474,\"start\":69454},{\"end\":69487,\"start\":69474},{\"end\":69509,\"start\":69487},{\"end\":69527,\"start\":69509},{\"end\":69541,\"start\":69527},{\"end\":69561,\"start\":69541},{\"end\":69579,\"start\":69561},{\"end\":69598,\"start\":69579},{\"end\":69613,\"start\":69598},{\"end\":69628,\"start\":69613},{\"end\":69995,\"start\":69975},{\"end\":70008,\"start\":69995},{\"end\":70474,\"start\":70459},{\"end\":70489,\"start\":70474},{\"end\":70502,\"start\":70489},{\"end\":70901,\"start\":70884},{\"end\":70918,\"start\":70901},{\"end\":70934,\"start\":70918},{\"end\":71149,\"start\":71131},{\"end\":71169,\"start\":71149},{\"end\":71182,\"start\":71169},{\"end\":71191,\"start\":71182},{\"end\":71211,\"start\":71191},{\"end\":71226,\"start\":71211},{\"end\":71245,\"start\":71226},{\"end\":71260,\"start\":71245},{\"end\":71762,\"start\":71747},{\"end\":71780,\"start\":71762},{\"end\":71796,\"start\":71780},{\"end\":71816,\"start\":71796},{\"end\":71829,\"start\":71816},{\"end\":72094,\"start\":72082},{\"end\":72109,\"start\":72094},{\"end\":72123,\"start\":72109},{\"end\":72133,\"start\":72123},{\"end\":72361,\"start\":72347},{\"end\":72373,\"start\":72361},{\"end\":72387,\"start\":72373},{\"end\":72990,\"start\":72976},{\"end\":73002,\"start\":72990},{\"end\":73016,\"start\":73002},{\"end\":73446,\"start\":73433},{\"end\":73457,\"start\":73446},{\"end\":73472,\"start\":73457},{\"end\":73652,\"start\":73639},{\"end\":73670,\"start\":73652},{\"end\":73688,\"start\":73670},{\"end\":73702,\"start\":73688},{\"end\":73915,\"start\":73901},{\"end\":73925,\"start\":73915},{\"end\":73932,\"start\":73925},{\"end\":73948,\"start\":73932},{\"end\":73960,\"start\":73948},{\"end\":74215,\"start\":74200},{\"end\":74228,\"start\":74215},{\"end\":74242,\"start\":74228},{\"end\":74258,\"start\":74242},{\"end\":74605,\"start\":74594},{\"end\":74618,\"start\":74605},{\"end\":74629,\"start\":74618},{\"end\":74877,\"start\":74861},{\"end\":74894,\"start\":74877},{\"end\":74906,\"start\":74894},{\"end\":75250,\"start\":75237},{\"end\":75261,\"start\":75250},{\"end\":75275,\"start\":75261},{\"end\":75292,\"start\":75275},{\"end\":75535,\"start\":75522},{\"end\":75550,\"start\":75535},{\"end\":75566,\"start\":75550},{\"end\":75580,\"start\":75566},{\"end\":75597,\"start\":75580},{\"end\":75608,\"start\":75597},{\"end\":76342,\"start\":76329},{\"end\":76356,\"start\":76342},{\"end\":76367,\"start\":76356},{\"end\":76852,\"start\":76839},{\"end\":76866,\"start\":76852},{\"end\":76881,\"start\":76866},{\"end\":76897,\"start\":76881},{\"end\":76914,\"start\":76897},{\"end\":76925,\"start\":76914},{\"end\":77407,\"start\":77390},{\"end\":77421,\"start\":77407},{\"end\":77581,\"start\":77571},{\"end\":77594,\"start\":77581},{\"end\":77607,\"start\":77594},{\"end\":77625,\"start\":77607},{\"end\":77792,\"start\":77780},{\"end\":77809,\"start\":77792},{\"end\":77819,\"start\":77809},{\"end\":78216,\"start\":78204},{\"end\":78228,\"start\":78216},{\"end\":78237,\"start\":78228},{\"end\":78506,\"start\":78495},{\"end\":78532,\"start\":78506},{\"end\":78543,\"start\":78532},{\"end\":78789,\"start\":78760},{\"end\":78810,\"start\":78789},{\"end\":78827,\"start\":78810},{\"end\":78840,\"start\":78827},{\"end\":79056,\"start\":79045},{\"end\":79068,\"start\":79056},{\"end\":79081,\"start\":79068},{\"end\":79093,\"start\":79081},{\"end\":79103,\"start\":79093},{\"end\":79356,\"start\":79338},{\"end\":79372,\"start\":79356},{\"end\":79387,\"start\":79372},{\"end\":79402,\"start\":79387},{\"end\":79411,\"start\":79402},{\"end\":79429,\"start\":79411},{\"end\":79710,\"start\":79698},{\"end\":79720,\"start\":79710},{\"end\":79735,\"start\":79720},{\"end\":79949,\"start\":79940},{\"end\":79963,\"start\":79949},{\"end\":79978,\"start\":79963},{\"end\":79987,\"start\":79978},{\"end\":80002,\"start\":79987},{\"end\":80018,\"start\":80002},{\"end\":80031,\"start\":80018},{\"end\":80040,\"start\":80031},{\"end\":80351,\"start\":80340},{\"end\":80361,\"start\":80351},{\"end\":80370,\"start\":80361},{\"end\":80385,\"start\":80370},{\"end\":80398,\"start\":80385},{\"end\":80899,\"start\":80885},{\"end\":80917,\"start\":80899},{\"end\":81331,\"start\":81315},{\"end\":81348,\"start\":81331},{\"end\":81367,\"start\":81348},{\"end\":81386,\"start\":81367},{\"end\":81582,\"start\":81568},{\"end\":81596,\"start\":81582},{\"end\":81613,\"start\":81596},{\"end\":81632,\"start\":81613},{\"end\":81828,\"start\":81815},{\"end\":81847,\"start\":81828},{\"end\":82032,\"start\":82017},{\"end\":82052,\"start\":82032},{\"end\":82063,\"start\":82052},{\"end\":82079,\"start\":82063},{\"end\":82093,\"start\":82079},{\"end\":82112,\"start\":82093},{\"end\":82424,\"start\":82415},{\"end\":82437,\"start\":82424},{\"end\":82743,\"start\":82734},{\"end\":82757,\"start\":82743},{\"end\":82777,\"start\":82757},{\"end\":82788,\"start\":82777},{\"end\":82799,\"start\":82788},{\"end\":83018,\"start\":83004},{\"end\":83031,\"start\":83018},{\"end\":83047,\"start\":83031},{\"end\":83060,\"start\":83047},{\"end\":83499,\"start\":83484},{\"end\":83511,\"start\":83499},{\"end\":83525,\"start\":83511},{\"end\":83537,\"start\":83525},{\"end\":83682,\"start\":83669},{\"end\":83698,\"start\":83682},{\"end\":83715,\"start\":83698},{\"end\":83730,\"start\":83715},{\"end\":83744,\"start\":83730},{\"end\":83758,\"start\":83744},{\"end\":83771,\"start\":83758},{\"end\":84066,\"start\":84044},{\"end\":84084,\"start\":84066},{\"end\":84104,\"start\":84084},{\"end\":84520,\"start\":84509},{\"end\":84666,\"start\":84655},{\"end\":84678,\"start\":84666},{\"end\":84687,\"start\":84678},{\"end\":84699,\"start\":84687},{\"end\":84713,\"start\":84699},{\"end\":84729,\"start\":84713},{\"end\":84932,\"start\":84917},{\"end\":84948,\"start\":84932},{\"end\":84961,\"start\":84948},{\"end\":84973,\"start\":84961},{\"end\":84987,\"start\":84973},{\"end\":85001,\"start\":84987},{\"end\":85012,\"start\":85001},{\"end\":85028,\"start\":85012},{\"end\":85310,\"start\":85297},{\"end\":85327,\"start\":85310},{\"end\":85341,\"start\":85327},{\"end\":85359,\"start\":85341},{\"end\":85375,\"start\":85359},{\"end\":85778,\"start\":85766},{\"end\":85798,\"start\":85778},{\"end\":85813,\"start\":85798},{\"end\":86061,\"start\":86037},{\"end\":86077,\"start\":86061},{\"end\":86092,\"start\":86077},{\"end\":86630,\"start\":86615},{\"end\":86645,\"start\":86630},{\"end\":86658,\"start\":86645},{\"end\":87302,\"start\":87287},{\"end\":87317,\"start\":87302},{\"end\":87330,\"start\":87317},{\"end\":87851,\"start\":87832},{\"end\":87869,\"start\":87851},{\"end\":87883,\"start\":87869},{\"end\":87897,\"start\":87883},{\"end\":87913,\"start\":87897},{\"end\":87931,\"start\":87913},{\"end\":88245,\"start\":88228},{\"end\":88260,\"start\":88245},{\"end\":88274,\"start\":88260},{\"end\":88734,\"start\":88718},{\"end\":88910,\"start\":88889},{\"end\":88932,\"start\":88910},{\"end\":88949,\"start\":88932},{\"end\":88962,\"start\":88949},{\"end\":88973,\"start\":88962},{\"end\":89321,\"start\":89307},{\"end\":89332,\"start\":89321},{\"end\":89959,\"start\":89939},{\"end\":89977,\"start\":89959},{\"end\":89996,\"start\":89977},{\"end\":90324,\"start\":90304},{\"end\":90342,\"start\":90324},{\"end\":90357,\"start\":90342},{\"end\":90373,\"start\":90357},{\"end\":90386,\"start\":90373},{\"end\":90405,\"start\":90386},{\"end\":90642,\"start\":90622},{\"end\":90657,\"start\":90642},{\"end\":90676,\"start\":90657},{\"end\":90822,\"start\":90796},{\"end\":90835,\"start\":90822},{\"end\":90852,\"start\":90835},{\"end\":90870,\"start\":90852},{\"end\":90891,\"start\":90870},{\"end\":90903,\"start\":90891},{\"end\":90919,\"start\":90903},{\"end\":90934,\"start\":90919},{\"end\":90954,\"start\":90934},{\"end\":90973,\"start\":90954},{\"end\":90990,\"start\":90973},{\"end\":91002,\"start\":90990},{\"end\":91015,\"start\":91002},{\"end\":91027,\"start\":91015},{\"end\":91042,\"start\":91027},{\"end\":91054,\"start\":91042},{\"end\":91070,\"start\":91054},{\"end\":91086,\"start\":91070},{\"end\":91092,\"start\":91086},{\"end\":91105,\"start\":91092},{\"end\":91117,\"start\":91105},{\"end\":91131,\"start\":91117},{\"end\":91149,\"start\":91131},{\"end\":91166,\"start\":91149},{\"end\":91182,\"start\":91166},{\"end\":91198,\"start\":91182},{\"end\":91214,\"start\":91198},{\"end\":91228,\"start\":91214},{\"end\":91799,\"start\":91783},{\"end\":91813,\"start\":91799},{\"end\":91826,\"start\":91813},{\"end\":91843,\"start\":91826},{\"end\":91856,\"start\":91843},{\"end\":91871,\"start\":91856},{\"end\":91886,\"start\":91871},{\"end\":91904,\"start\":91886},{\"end\":92411,\"start\":92395},{\"end\":92424,\"start\":92411},{\"end\":92437,\"start\":92424},{\"end\":92449,\"start\":92437},{\"end\":92460,\"start\":92449},{\"end\":92477,\"start\":92460},{\"end\":92931,\"start\":92913},{\"end\":92948,\"start\":92931},{\"end\":92965,\"start\":92948},{\"end\":93206,\"start\":93191},{\"end\":93221,\"start\":93206},{\"end\":93232,\"start\":93221},{\"end\":93246,\"start\":93232},{\"end\":93480,\"start\":93465},{\"end\":93490,\"start\":93480},{\"end\":93502,\"start\":93490},{\"end\":93513,\"start\":93502},{\"end\":93699,\"start\":93688},{\"end\":93712,\"start\":93699},{\"end\":93725,\"start\":93712},{\"end\":93735,\"start\":93725},{\"end\":93751,\"start\":93735},{\"end\":93757,\"start\":93751},{\"end\":94166,\"start\":94155},{\"end\":94176,\"start\":94166},{\"end\":94187,\"start\":94176},{\"end\":94196,\"start\":94187},{\"end\":94206,\"start\":94196},{\"end\":94579,\"start\":94564},{\"end\":94594,\"start\":94579},{\"end\":94610,\"start\":94594},{\"end\":94625,\"start\":94610},{\"end\":94638,\"start\":94625},{\"end\":94910,\"start\":94895},{\"end\":94925,\"start\":94910},{\"end\":94941,\"start\":94925},{\"end\":94956,\"start\":94941},{\"end\":94969,\"start\":94956},{\"end\":95378,\"start\":95366},{\"end\":95389,\"start\":95378},{\"end\":95402,\"start\":95389},{\"end\":95416,\"start\":95402},{\"end\":95432,\"start\":95416},{\"end\":95450,\"start\":95432},{\"end\":95781,\"start\":95767},{\"end\":95799,\"start\":95781},{\"end\":95811,\"start\":95799},{\"end\":95827,\"start\":95811},{\"end\":95843,\"start\":95827},{\"end\":96047,\"start\":96035},{\"end\":96061,\"start\":96047},{\"end\":96073,\"start\":96061},{\"end\":96086,\"start\":96073}]", "bib_venue": "[{\"end\":63788,\"start\":63714},{\"end\":64078,\"start\":64009},{\"end\":64366,\"start\":64260},{\"end\":64777,\"start\":64715},{\"end\":65439,\"start\":65368},{\"end\":66251,\"start\":66205},{\"end\":66689,\"start\":66627},{\"end\":66987,\"start\":66953},{\"end\":67433,\"start\":67385},{\"end\":67841,\"start\":67837},{\"end\":68156,\"start\":68064},{\"end\":68335,\"start\":68285},{\"end\":68634,\"start\":68578},{\"end\":68976,\"start\":68896},{\"end\":69246,\"start\":69201},{\"end\":69452,\"start\":69378},{\"end\":70120,\"start\":70008},{\"end\":70569,\"start\":70502},{\"end\":70882,\"start\":70796},{\"end\":71372,\"start\":71260},{\"end\":71745,\"start\":71673},{\"end\":72080,\"start\":72036},{\"end\":72455,\"start\":72387},{\"end\":73073,\"start\":73016},{\"end\":73431,\"start\":73391},{\"end\":73637,\"start\":73567},{\"end\":73980,\"start\":73960},{\"end\":74321,\"start\":74258},{\"end\":74592,\"start\":74542},{\"end\":74910,\"start\":74906},{\"end\":75235,\"start\":75139},{\"end\":75725,\"start\":75608},{\"end\":76438,\"start\":76367},{\"end\":76992,\"start\":76925},{\"end\":77388,\"start\":77314},{\"end\":77569,\"start\":77540},{\"end\":77931,\"start\":77819},{\"end\":78293,\"start\":78237},{\"end\":78493,\"start\":78442},{\"end\":78758,\"start\":78698},{\"end\":79107,\"start\":79103},{\"end\":79433,\"start\":79429},{\"end\":79696,\"start\":79635},{\"end\":80053,\"start\":80040},{\"end\":80478,\"start\":80398},{\"end\":80984,\"start\":80917},{\"end\":81313,\"start\":81271},{\"end\":81566,\"start\":81521},{\"end\":81813,\"start\":81764},{\"end\":82015,\"start\":81949},{\"end\":82500,\"start\":82437},{\"end\":82732,\"start\":82671},{\"end\":83137,\"start\":83060},{\"end\":83482,\"start\":83437},{\"end\":83779,\"start\":83775},{\"end\":84171,\"start\":84104},{\"end\":84507,\"start\":84448},{\"end\":84653,\"start\":84602},{\"end\":84915,\"start\":84881},{\"end\":85444,\"start\":85382},{\"end\":85764,\"start\":85711},{\"end\":86188,\"start\":86092},{\"end\":86613,\"start\":86526},{\"end\":86994,\"start\":86972},{\"end\":87379,\"start\":87330},{\"end\":87830,\"start\":87781},{\"end\":88341,\"start\":88274},{\"end\":88716,\"start\":88634},{\"end\":89034,\"start\":88973},{\"end\":89449,\"start\":89332},{\"end\":90055,\"start\":89996},{\"end\":90302,\"start\":90251},{\"end\":90620,\"start\":90581},{\"end\":92016,\"start\":91904},{\"end\":92554,\"start\":92477},{\"end\":92985,\"start\":92965},{\"end\":93189,\"start\":93114},{\"end\":93463,\"start\":93391},{\"end\":93860,\"start\":93773},{\"end\":94273,\"start\":94206},{\"end\":94642,\"start\":94638},{\"end\":95046,\"start\":94969},{\"end\":95525,\"start\":95466},{\"end\":95765,\"start\":95732},{\"end\":96033,\"start\":95968},{\"end\":65529,\"start\":65441},{\"end\":67448,\"start\":67435},{\"end\":72532,\"start\":72516},{\"end\":73100,\"start\":73075},{\"end\":76459,\"start\":76440},{\"end\":77010,\"start\":76994},{\"end\":80545,\"start\":80480},{\"end\":81006,\"start\":80986},{\"end\":83201,\"start\":83139},{\"end\":84189,\"start\":84173},{\"end\":86271,\"start\":86190},{\"end\":87016,\"start\":86996},{\"end\":88359,\"start\":88343},{\"end\":92618,\"start\":92556},{\"end\":95110,\"start\":95048}]"}}}, "year": 2023, "month": 12, "day": 17}