{"id": 11977588, "updated": "2023-09-29 21:53:16.831", "metadata": {"title": "Unsupervised Learning of Depth and Ego-Motion from Video", "authors": "[{\"first\":\"Tinghui\",\"last\":\"Zhou\",\"middle\":[]},{\"first\":\"Matthew\",\"last\":\"Brown\",\"middle\":[]},{\"first\":\"Noah\",\"last\":\"Snavely\",\"middle\":[]},{\"first\":\"David\",\"last\":\"Lowe\",\"middle\":[\"G.\"]}]", "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2017, "month": 4, "day": 25}, "abstract": "We present an unsupervised learning framework for the task of monocular depth and camera motion estimation from unstructured video sequences. We achieve this by simultaneously training depth and camera pose estimation networks using the task of view synthesis as the supervisory signal. The networks are thus coupled via the view synthesis objective during training, but can be applied independently at test time. Empirical evaluation on the KITTI dataset demonstrates the effectiveness of our approach: 1) monocular depth performing comparably with supervised methods that use either ground-truth pose or depth for training, and 2) pose estimation performing favorably with established SLAM systems under comparable input settings.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1704.07813", "mag": "2951261569", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/ZhouBSL17", "doi": "10.1109/cvpr.2017.700"}}, "content": {"source": {"pdf_hash": "027387a93d8d0ae60e17468aae1f11b55ee94ef6", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1704.07813v2.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1704.07813", "status": "GREEN"}}, "grobid": {"id": "eea177810a31380607ba3345c86ff3ecdade162e", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/027387a93d8d0ae60e17468aae1f11b55ee94ef6.txt", "contents": "\nUnsupervised Learning of Depth and Ego-Motion from Video\n\n\nTinghui Zhou \nUC Berkeley\n\n\nMatthew Brown \nUC Berkeley\n\n\nGoogle Noah \nUC Berkeley\n\n\nSnavely Google \nUC Berkeley\n\n\nDavid G Lowe Google \nUC Berkeley\n\n\nUnsupervised Learning of Depth and Ego-Motion from Video\n\nWe present an unsupervised learning framework for the task of monocular depth and camera motion estimation from unstructured video sequences. In common with recent work[10,14,16], we use an end-to-end learning approach with view synthesis as the supervisory signal. In contrast to the previous work, our method is completely unsupervised, requiring only monocular video sequences for training. Our method uses single-view depth and multiview pose networks, with a loss based on warping nearby views to the target using the computed depth and pose. The networks are thus coupled by the loss during training, but can be applied independently at test time. Empirical evaluation on the KITTI dataset demonstrates the effectiveness of our approach: 1) monocular depth performs comparably with supervised methods that use either ground-truth pose or depth for training, and 2) pose estimation performs favorably compared to established SLAM systems under comparable input settings.\n\nIntroduction\n\nHumans are remarkably capable of inferring ego-motion and the 3D structure of a scene even over short timescales. For instance, in navigating along a street, we can easily locate obstacles and react quickly to avoid them. Years of research in geometric computer vision has failed to recreate similar modeling capabilities for real-world scenes (e.g., where non-rigidity, occlusion and lack of texture are present). So why do humans excel at this task? One hypothesis is that we develop a rich, structural understanding of the world through our past visual experience that has largely consisted of moving around and observing vast numbers of scenes and developing consistent modeling of our observations. From millions of such observations, we have learned about the regularities of the world-roads are flat, buildings are straight, cars are supported by roads etc., and we can apply this knowledge when perceiving a new scene, even from a single monocular image. * The majority of the work was done while interning at Google.  Our training procedure produces two models that operate independently, one for single-view depth prediction, and one for multiview camera pose estimation.\n\nIn this work, we mimic this approach by training a model that observes sequences of images and aims to explain its observations by predicting likely camera motion and the scene structure (as shown in Fig. 1). We take an end-toend approach in allowing the model to map directly from input pixels to an estimate of ego-motion (parameterized as 6-DoF transformation matrices) and the underlying scene structure (parameterized as per-pixel depth maps under a reference view). We are particularly inspired by prior work that has suggested view synthesis as a metric [44] and recent work that tackles the calibrated, multi-view 3D case in an end-to-end framework [10]. Our method is unsupervised, and can be trained simply using sequences of images with no manual labeling or even camera motion information.\n\nOur approach builds upon the insight that a geometric view synthesis system only performs consistently well when its intermediate predictions of the scene geometry and the camera poses correspond to the physical ground-truth. While imperfect geometry and/or pose estimation can cheat with reasonable synthesized views for certain types of scenes (e.g., textureless), the same model would fail miserably when presented with another set of scenes with more diverse layout and appearance structures. Thus, our goal is to formulate the entire view synthesis pipeline as the inference procedure of a convolutional neural network, so that by training the network on large-scale video data for the 'meta'-task of view synthesis the network is forced to learn about intermediate tasks of depth and camera pose estimation in order to come up with a consistent explanation of the visual world. Empirical evaluation on the KITTI [15] benchmark demonstrates the effectiveness of our approach on both single-view depth and camera pose estimation. Our code will be made available at https: //github.com/tinghuiz/SfMLearner.\n\n\nRelated work\n\nStructure from motion The simultaneous estimation of structure and motion is a well studied problem with an established toolchain of techniques [12,50,38]. Whilst the traditional toolchain is effective and efficient in many cases, its reliance on accurate image correspondence can cause problems in areas of low texture, complex geometry/photometry, thin structures, and occlusions. To address these issues, several of the pipeline stages have been recently tackled using deep learning, e.g., feature matching [18], pose estimation [26], and stereo [10,27,53]. These learning-based techniques are attractive in that they are able to leverage external supervision during training, and potentially overcome the above issues when applied to test data.\n\nWarping-based view synthesis One important application of geometric scene understanding is the task of novel view synthesis, where the goal is to synthesize the appearance of the scene seen from novel camera viewpoints. A classic paradigm for view synthesis is to first either estimate the underlying 3D geometry explicitly or establish pixel correspondence among input views, and then synthesize the novel views by compositing image patches from the input views (e.g., [4,55,43,6,9]). Recently, end-toend learning has been applied to reconstruct novel views by transforming the input based on depth or flow, e.g., DeepStereo [10], Deep3D [51] and Appearance Flows [54]. In these methods, the underlying geometry is represented by quantized depth planes (DeepStereo), probabilistic disparity maps (Deep3D) and viewdependent flow fields (Appearance Flows), respectively. Unlike methods that directly map from input views to the target view (e.g., [45]), warping-based methods are forced to learn intermediate predictions of geometry and/or correspondence. In this work, we aim to distill such geometric reasoning capability from CNNs trained to perform warping-based view synthesis.\n\nLearning single-view 3D from registered 2D views Our work is closely related to a line of recent research on learning single-view 3D inference from registered 2D observations. Garg et al. [14] propose to learn a single-view depth estimation CNN using projection errors to a calibrated stereo twin for supervision. Concurrently, Deep3D [51] predicts a second stereo viewpoint from an input image using stereoscopic film footage as training data. A similar approach was taken by Godard et al. [16], with the addition of a left-right consistency constraint, and a better architecture design that led to impressive performance. Like our approach, these techniques only learn from image observations of the world, unlike methods that require explicit depth for training, e.g., [20,42,7,27,30].\n\nThese techniques bear some resemblance to direct methods for structure and motion estimation [22], where the camera parameters and scene depth are adjusted to minimize a pixel-based error function. However, rather than directly minimizing the error to obtain the estimation, the CNN-based methods only take a gradient step for each batch of input instances, which allows the network to learn an implicit prior from a large corpus of related imagery. Several authors have explored building differentiable rendering operations into their models that are trained in this way, e.g., [19,29,34].\n\nWhile most of the above techniques (including ours) are mainly focused on inferring depth maps as the scene geometry output, recent work (e.g., [13,41,46,52]) has also shown success in learning 3D volumetric representations from 2D observations based on similar principles of projective geometry. Fouhey et al. [11] further show that it is even possible to learn 3D inference without 3D labels (or registered 2D views) by utilizing scene regularity. Unsupervised/Self-supervised learning from video Another line of related work to ours is visual representation learning from video, where the general goal is to design pretext tasks for learning generic visual features from video data that can later be re-purposed for other vision tasks such as object detection and semantic segmentation. Such pretext tasks include ego-motion estimation [2,24], tracking [49], temporal coherence [17], temporal order verification [36], and object motion mask prediction [39]. While we focus on inferring the explicit scene geometry and ego-motion in this work, intuitively, the internal representation learned by the deep network (especially the single-view depth CNN) should capture some level of semantics that could generalize to other tasks as well.\n\nConcurrent to our work, Vijayanarasimhan et al. [48] independently propose a framework for joint training of depth, camera motion and scene motion from videos. While both methods are conceptually similar, ours is focused on the unsupervised aspect, whereas their framework adds the capability to incorporate supervision (e.g., depth, camera motion or scene motion). There are significant differences in how scene dynamics are modeled during training, in which they explicitly solve for object motion whereas our explainability mask discounts regions undergoing motion, occlusion and other factors.\n\n\nApproach\n\nHere we propose a framework for jointly training a single-view depth CNN and a camera pose estimation CNN from unlabeled video sequences. Despite being jointly trained, the depth model and the pose estimation model can be used independently during test-time inference. Training examples to our model consist of short image sequences of scenes captured by a moving camera. While our training procedure is robust to some degree of scene Figure 2. Overview of the supervision pipeline based on view synthesis. The depth network takes only the target view as input, and outputs a per-pixel depth mapDt. The pose network takes both the target view (It) and the nearby/source views (e.g., It\u22121 and It+1) as input, and outputs the relative camera poses (Tt\u2192t\u22121,Tt\u2192t+1). The outputs of both networks are then used to inverse warp the source views (see Sec. 3.2) to reconstruct the target view, and the photometric reconstruction loss is used for training the CNNs. By utilizing view synthesis as supervision, we are able to train the entire framework in an unsupervised manner from videos. motion, we assume that the scenes we are interested in are mostly rigid, i.e., the scene appearance change across different frames is dominated by the camera motion.\nT t!t+1 T t!t 1 I t I t 1 I t+1D t (p) p p t+1 p t 1 Project Project Pose CNN Depth CNN\n\nView synthesis as supervision\n\nThe key supervision signal for our depth and pose prediction CNNs comes from the task of novel view synthesis: given one input view of a scene, synthesize a new image of the scene seen from a different camera pose. We can synthesize a target view given a per-pixel depth in that image, plus the pose and visibility in a nearby view. As we will show next, this synthesis process can be implemented in a fully differentiable manner with CNNs as the geometry and pose estimation modules. Visibility can be handled, along with non-rigidity and other non-modeled factors, using an \"explanability\" mask, which we discuss later (Sec. 3.3).\n\nLet us denote < I1, . . . , IN > as a training image sequence with one of the frames It being the target view and the rest being the source views Is(1 \u2264 s \u2264 N, s = t). The view synthesis objective can be formulated as\nLvs = s p |It(p) \u2212\u00ces(p)| ,(1)\nwhere p indexes over pixel coordinates, and\u00ces is the source view Is warped to the target coordinate frame based on a depth imagebased rendering module [8] (described in Sec. 3.2), taking the predicted depthDt, the predicted 4\u00d74 camera transformation matrix 1 Tt\u2192s and the source view Is as input.\n\nNote that the idea of view synthesis as supervision has also been recently explored for learning single-view depth estimation [14,16] and multi-view stereo [10]. However, to the best of our knowledge, all previous work requires posed image sets during training (and testing too in the case of DeepStereo), while our framework can be applied to standard videos without pose information. Furthermore, it predicts the poses as part of the learning framework. See Figure 2 for an illustration of our learning pipeline for depth and pose estimation.\n\n\nDifferentiable depth image-based rendering\n\nAs indicated in Eq. 1, a key component of our learning framework is a differentiable depth image-based renderer that reconstructs the target view It by sampling pixels from a source view Is based on the predicted depth mapDt and the relative poseTt\u2192s.\n\nLet pt denote the homogeneous coordinates of a pixel in the target view, and K denote the camera intrinsics matrix. We can obtain pt's projected coordinates onto the source view ps by 2\nps \u223c KTt\u2192sDt(pt)K \u22121 pt(2)\nNotice that the projected coordinates ps are continuous values. To obtain Is(ps) for populating the value of\u00ces(pt) (see Figure 3), we then use the differentiable bilinear sampling mechanism proposed in the spatial transformer networks [23] that linearly interpolates the values of the 4-pixel neighbors (top-left, top-right, bottom-left, and bottom-right) of ps to approximate Is(ps), i.e. Is(pt) = Is(ps) = i\u2208{t,b},j\u2208{l,r} w ij Is(p ij s ), where w ij is linearly proportional to the spatial proximity between ps and p ij s , and i,j w ij = 1. A similar strategy is used in [54] for learning to directly warp between different views, while here the coordinates for pixel warping are obtained through projective geometry that enables the factorization of depth and camera pose.\n\n\nModeling the model limitation\n\nNote that when applied to monocular videos the above view synthesis formulation implicitly assumes 1) the scene is static without moving objects; 2) there is no occlusion/disocclusion between the target view and the source views; 3) the surface is Lambertian so that the photo-consistency error is meaningful. If any of these assumptions are violated in a training sequence, the gradients could be corrupted and potentially inhibit training. To improve the robustness of our learning pipeline to these factors, we additionally train a explainability prediction network (jointly and simultaneously with the depth and pose networks) that outputs a per-pixel soft mask\u00cas for each target-source pair, indicating the . . .  \n\nSince we do not have direct supervision for\u00cas, training with the above loss would result in a trivial solution of the network always predicting\u00cas to be zero, which perfectly minimizes the loss. To resolve this, we add a regularization term Lreg(\u00cas) that encourages nonzero predictions by minimizing the cross-entropy loss with constant label 1 at each pixel location. In other words, the network is encouraged to minimize the view synthesis objective, but allowed a certain amount of slack for discounting the factors not considered by the model.\n\n\nOvercoming the gradient locality\n\nOne remaining issue with the above learning pipeline is that the gradients are mainly derived from the pixel intensity difference between I(pt) and the four neighbors of I(ps), which would inhibit training if the correct ps (projected using the ground-truth depth and pose) is located in a low-texture region or far from the current estimation. This is a well known issue in motion estimation [3]. Empirically, we found two strategies to be effective for overcoming this issue: 1) using a convolutional encoder-decoder architecture with a small bottleneck for the depth network that implicitly constrains the output to be globally smooth and facilitates gradients to propagate from meaningful regions to nearby regions; 2) explicit multi-scale and smoothness loss (e.g., as in [14,16]) that allows gradients to be derived from larger spatial regions directly. We adopt the second strategy in this work as it is less sensitive to architectural choices. For smoothness, we minimize the L1 norm of the second-order gradients for the predicted depth maps (similar to [48]).\n\nOur final objective becomes\nL f inal = l L l vs + \u03bbsL l smooth + \u03bbe s Lreg(\u00ca l s ) ,(4)\nwhere l indexes over different image scales, s indexes over source images, and \u03bbs and \u03bbe are the weighting for the depth smoothness loss and the explainability regularization, respectively.\n\n\nNetwork architecture\n\nSingle-view depth For single-view depth prediction, we adopt the DispNet architecture proposed in [35] that is mainly based on an encoder-decoder design with skip connections and multi-scale side predictions (see Figure 4). All conv layers are followed by ReLU activation except for the prediction layers, where we use 1/(\u03b1 * sigmoid(x) + \u03b2) with \u03b1 = 10 and \u03b2 = 0.01 to constrain the predicted depth to be always positive within a reasonable range. We also experimented with using multiple views as input to the depth network, but did not find this to improve the results. This is in line with the observations in [47], where optical flow constraints need to be enforced to utilize multiple views effectively.\n\nPose The input to the pose estimation network is the target view concatenated with all the source views (along the color channels), and the outputs are the relative poses between the target view and each of the source views. The network consists of 7 stride-2 convolutions followed by a 1 \u00d7 1 convolution with 6 * (N \u2212 1) output channels (corresponding to 3 Euler angles and 3-D translation for each source view). Finally, global average pooling is applied to aggregate predictions at all spatial locations. All conv layers are followed by ReLU except for the last layer where no nonlinear activation is applied.\n\nExplainability mask The explainability prediction network shares the first five feature encoding layers with the pose network, followed by 5 deconvolution layers with multi-scale side predictions. All conv/deconv layers are followed by ReLU except for the prediction layers with no nonlinear activation. The number of output channels for each prediction layer is 2 * (N \u2212 1), with every two channels normalized by softmax to obtain the explainability prediction for the corresponding source-target pair (the second channel after normalization is\u00cas and used in computing the loss in Eq. 3).\n\n\nExperiments\n\nHere we evaluate the performance of our system, and compare with prior approaches on single-view depth as well as ego-motion estimation. We mainly use the KITTI dataset [15] for benchmarking, but also use the Make3D dataset [42] for evaluating crossdataset generalization ability.\n\nTraining Details We implemented the system using the publicly available TensorFlow [1] framework. For all the experiments, we set \u03bbs = 0.5/l (l is the downscaling factor for the corresponding scale) and \u03bbe = 0.2. During training, we used batch normalization [21] for all the layers except for the output layers, and the Adam [28] optimizer with \u03b21 = 0.9, \u03b22 = 0.999, learning rate of 0.0002 and mini-batch size of 4. The training typically converges after about 150K iterations. All the experiments are performed with image sequences captured with a monocular camera. We resize the images to 128 \u00d7 416 during training, but both the depth and pose networks can be run fully-convolutionally for images of arbitrary size at test time.\n\n\nSingle-view depth estimation\n\nWe train our system on the split provided by [7], and exclude all the frames from the testing scenes as well as static sequences with mean optical flow magnitude less than 1 pixel for training. We fix the length of image sequences to be 3 frames, and treat the central frame as the target view and the \u00b11 frames as the source views. We use images captured by both color cameras, but treated them independently when forming training sequences. This results in a total of 44, 540 sequences, out of which we use 40, 109 for training and 4, 431 for validation.\n\nTo the best of our knowledge, no previous systems exist that learn single-view depth estimation in an unsupervised manner from monocular videos. Nonetheless, here we provide comparison with prior methods with depth supervision [7] and recent methods that use calibrated stereo images (i.e. with pose supervision) for\n\n\nInput image\n\nOur prediction training [14,16]. Since the depth predicted by our method is defined up to a scale factor, for evaluation we multiply the predicted depth maps by a scalar\u015d that matches the median with the groundtruth, i.e.\u015d = median(Dgt)/median(D pred ). Similar to [16], we also experimented with first pre-training the system on the larger Cityscapes dataset [5] (sample predictions are shown in Figure 5), and then fine-tune on KITTI, which results in slight performance improvement.\n\nKITTI Here we evaluate the single-view depth performance on the 697 images from the test split of [7]. As shown in Table 1, our unsupervised method performs comparably with several supervised methods (e.g. Eigen et al. [7] and Garg et al. [14]), but falls short of concurrent work by Godard et al. [16] that uses calibrated stereo images (i.e. with pose supervision) with left-right cycle consistency loss for training. For future work, it would be interesting to see if incorporating the similar cycle consistency loss into our framework could further improve the results. Figure 6 provides examples of visual comparison between our results and some supervised baselines over a variety of examples. One can see that although trained in an unsupervised manner, our results are comparable to that of the supervised baselines, and sometimes preserve the depth boundaries and thin structures such as trees and street lights better.\n\nWe show sample predictions made by our initial Cityscapes model and the final model (pre-trained on Cityscapes and then fine-tuned on KITTI) in Figure 7. Due to the domain gap between the two datasets, our Cityscapes model sometimes has difficulty in recovering the complete shape of the car/bushes, and mistakes them with distant objects.\n\nWe also performed an ablation study of the explainability modeling (see Table 1), which turns out only offering a modest performance boost. This is likely because 1) most of the KITTI scenes are static without significant scene motions, and 2) the occlusion/visibility effects only occur in small regions in sequences Figure 6. Comparison of single-view depth estimation between Eigen et al. [7] (with ground-truth depth supervision), Garg et al. [14] (with ground-truth pose supervision), and ours (unsupervised). The ground-truth depth map is interpolated from sparse measurements for visualization purpose. The last two rows show typical failure cases of our model, which sometimes struggles in vast open scenes and objects close to the front of the camera. across a short time span (3-frames), which make the explainability modeling less essential to the success of training. Nonetheless, our explainability prediction network does seem to capture the factors like scene motion and visibility well (see Sec. 4.3), and could potentially be more important for other more challenging datasets.\n\n\nMake3D\n\nTo evaluate the generalization ability of our singleview depth model, we directly apply our model trained on Cityscapes + KITTI to the Make3D dataset unseen during training. While there still remains a significant performance gap between our method and others supervised using Make3D groundtruth depth (see Table 2), our predictions are able to capture the global scene layout reasonably well without any training on the Make3D images (see Figure 8).\n\n\nPose estimation\n\nTo evaluate the performance of our pose estimation network, we applied our system to the official KITTI odometry split (containing 11 driving sequences with ground truth odometry obtained through the IMU/GPS readings, which we use for evaluation purpose only), and used sequences 00-08 for training and 09-10 for testing. In this experiment, we fix the length of input image sequences to our system to 5 frames. We compare our ego-motion estimation with two variants of monocular ORB-SLAM [37] (a well-established SLAM system): 1) ORB-SLAM (full), which recovers odometry using all frames of the driving sequence (i.e. allowing loop closure and re-localization), and 2) ORB-SLAM (short), which runs on 5-frame snippets (same as our input setting). Another baseline we compare with is the dataset mean  [15] using the split of Eigen et al. [7] (Baseline numbers taken from [16]). For training, K = KITTI, and CS = Cityscapes [5]. All methods we compare with use some form of supervision (either ground-truth depth or calibrated camera pose) during training. Note: results from Garg et al. [14] are capped at 50m depth, so we break these out separately in the lower part of the table.\n\nInput image Ours (CS + KITTI) Ours (CS) Figure 7. Comparison of single-view depth predictions on the KITTI dataset by our initial Cityscapes model and the final model (pre-trained on Cityscapes and then fine-tuned on KITTI). The Cityscapes model sometimes makes structural mistakes (e.g. holes on car body) likely due to the domain gap between the two datasets.  Table 2. Results on the Make3D dataset [42]. Similar to ours, Godard et al. [16] do not utilize any of the Make3D data during training, and directly apply the model trained on KITTI+Cityscapes to the test set. Following the evaluation protocol of [16], the errors are only computed where depth is less than 70 meters in a central image crop.\n\nof car motion (using ground-truth odometry) for 5-frame snippets.\n\nTo resolve scale ambiguity during evaluation, we first optimize Input Ground-truth Ours the scaling factor for the predictions made by each method to best align with the ground truth, and then measure the Absolute Trajectory Error (ATE) [37] as the metric. ATE is computed on 5-frame snippets and averaged over the full sequence. 3 As shown in Table 3 and Fig. 9, our method outperforms both baselines (mean odometry and ORB-SLAM (short)) that share the same input setting as ours, but falls short of ORB-SLAM (full), which leverages whole sequences (1591 for seq. 09 and 1201 for seq. 10) for loop closure and re-localization. For better understanding of our pose estimation results, we show in Figure 9 the ATE curve with varying amount of side- 0.032 \u00b1 0.026 0.028 \u00b1 0.023 Ours 0.021 \u00b1 0.017 0.020 \u00b1 0.015 Table 3. Absolute Trajectory Error (ATE) on the KITTI odometry split averaged over all 5-frame snippets (lower is better). Our method outperforms baselines with the same input setting, but falls short of ORB-SLAM (full) that uses strictly more data. . Absolute Trajectory Error (ATE) at different left/right turning magnitude (coordinate difference in the side-direction between the start and ending frame of a testing sequence). Our method performs significantly better than ORB-SLAM (short) when side rotation is small, and is comparable with ORB-SLAM (full) across the entire spectrum.\n\nrotation by the car between the beginning and the end of a sequence. Figure 9 suggests that our method is significantly better than ORB-SLAM (short) when the side-rotation is small (i.e. car mostly driving forward), and comparable to ORB-SLAM (full) across the entire spectrum. The large performance gap between ours and ORB-SLAM (short) suggests that our learned ego-motion could potentially be used as an alternative to the local estimation modules in monocular SLAM systems.\n\n\nVisualizing the explainability prediction\n\nWe visualize example explainability masks predicted by our network in Figure 10. The first three rows suggest that the network has learned to identify dynamic objects in the scene as unexplainable by our model, and similarly, rows 4-5 are examples of objects that disappear from the frame in subsequent views. The last two rows demonstrate the potential downside of explainabilityweighted loss: the depth CNN has low confidence in predicting thin structures well, and tends to mask them as unexplainable.\n\n\nDiscussion\n\nWe have presented an end-to-end learning pipeline that utilizes the task of view synthesis for supervision of single-view depth and camera pose estimation. The system is trained on unlabeled videos, and yet performs comparably with approaches that require ground-truth depth or pose for training. Despite good performance on the benchmark evaluation, our method is by no means close to solving the general problem of unsupervised learning of 3D scene structure inference. A number of major challenges are yet to be Target view Explanability mask Source view Figure 10. Sample visualizations of the explainability masks. Highlighted pixels are predicted to be unexplainable by the network due to motion (rows 1-3), occlusion/visibility (rows 4-5), or other factors (rows 7-8).\n\naddressed: 1) our current framework does not explicitly estimate scene dynamics and occlusions (although they are implicitly taken into account by the explainability masks), both of which are critical factors in 3D scene understanding. Direct modeling of scene dynamics through motion segmentation (e.g. [48,40]) could be a potential solution; 2) our framework assumes the camera intrinsics are given, which forbids the use of random Internet videos with unknown camera types/calibration -we plan to address this in future work; 3) depth maps are a simplified representation of the underlying 3D scene. It would be interesting to extend our framework to learn full 3D volumetric representations (e.g. [46]). Another interesting area for future work would be to investigate in more detail the representation learned by our system. In particular, the pose network likely uses some form of image correspondence in estimating the camera motion, whereas the depth estimation network likely recognizes common structural features of scenes and objects. It would be interesting to probe these, and investigate the extent to which our network already performs, or could be re-purposed to perform, tasks such as object detection and semantic segmentation.\n\n\nTesting: single-view depth and multi-view pose estimation.\n\nFigure 1 .\n1The training data to our system consists solely of unlabeled image sequences capturing scene appearance from different viewpoints, where the poses of the images are not provided.\n\nFigure 3 .\n3Illustration of the differentiable image warping process. For each point pt in the target view, we first project it onto the source view based on the predicted depth and camera pose, and then use bilinear interpolation to obtain the value of the warped image\u00ces at location pt.\n\nFigure 4 .\n4Network architecture for our depth/pose/explainability prediction modules. The width and height of each rectangular block indicates the output channels and the spatial dimension of the feature map at the corresponding layer respectively, and each reduction/increase in size indicates a change by the factor of 2. (a) For single-view depth, we adopt the DispNet[35] architecture with multi-scale side predictions. The kernel size is 3 for all the layers except for the first 4 conv layers with 7, 7, 5, 5, respectively. The number of output channels for the first conv layer is 32. (b) The pose and explainabilty networks share the first few conv layers, and then branch out to predict 6-DoF relative pose and multi-scale explainability masks, respectively. The number of output channels for the first conv layer is 16, and the kernel size is 3 for all the layers except for the first two conv and the last two deconv/prediction layers where we use 7, 5, 5, 7, respectively. See Section 3.5 for more details.network's belief in where direct view synthesis will be successfully modeled for each target pixel. Based on the predicted\u00cas, the view synthesis objective is weighted correspondingly byLvs = <I 1 ,...,I N >\u2208S p\u00ca s(p)|It(p) \u2212\u00ces(p)| .\n\nFigure 5 .\n5Our sample predictions on the Cityscapes dataset using the model trained on Cityscapes only.\n\nFigure 8 .\n8Our sample predictions on the Make3D dataset. Note that our model is trained on KITTI + Cityscapes only, and directly tested on Make3D.\n\nFigure 9\n9Figure 9. Absolute Trajectory Error (ATE) at different left/right turning magnitude (coordinate difference in the side-direction between the start and ending frame of a testing sequence). Our method performs significantly better than ORB-SLAM (short) when side rotation is small, and is comparable with ORB-SLAM (full) across the entire spectrum.\nIn practice, the CNN estimates the Euler angles and the 3D translation vector, which are then converted to the transformation matrix.\nFor notation simplicity, we omit showing the necessary conversion to homogeneous coordinates along the steps of matrix multiplication.\nFor evaluating ORB-SLAM (full) we break down the trajectory of the full sequence into 5-frame snippets with the reference coordinate frame adjusted to the central frame of each snippet.\nAcknowledgments: We thank our colleagues, Sudheendra Vijayanarasimhan, Susanna Ricco, Cordelia Schmid, Rahul Sukthankar, and Katerina Fragkiadaki for their help. We also thank the anonymous reviewers for their valuable comments. TZ would like to thank Shubham Tulsiani for helpful discussions, and Clement Godard for sharing the evaluation code. This work is also partially funded by Intel/NSF VEC award IIS-1539099.\nM Abadi, A Agarwal, P Barham, E Brevdo, Z Chen, C Citro, G S Corrado, A Davis, J Dean, M Devin, arXiv:1603.04467Large-scale machine learning on heterogeneous distributed systems. arXiv preprintM. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, et al. TensorFlow: Large-scale machine learning on heteroge- neous distributed systems. arXiv preprint arXiv:1603.04467, 2016. 5\n\nLearning to see by moving. P Agrawal, J Carreira, J Malik, Int. Conf. Computer Vision. P. Agrawal, J. Carreira, and J. Malik. Learning to see by moving. In Int. Conf. Computer Vision, 2015. 2\n\nHierarchical model-based motion estimation. J Bergen, P Anandan, K Hanna, R Hingorani, Computer Vi-sionECCV'92. SpringerJ. Bergen, P. Anandan, K. Hanna, and R. Hingorani. Hier- archical model-based motion estimation. In Computer Vi- sionECCV'92, pages 237-252. Springer, 1992. 4\n\nView interpolation for image synthesis. S E Chen, L Williams, Proceedings of the 20th annual conference on Computer graphics and interactive techniques. the 20th annual conference on Computer graphics and interactive techniquesACMS. E. Chen and L. Williams. View interpolation for image synthesis. In Proceedings of the 20th annual conference on Computer graphics and interactive techniques, pages 279- 288. ACM, 1993. 2\n\nThe Cityscapes dataset for semantic urban scene understanding. M Cordts, M Omran, S Ramos, T Rehfeld, M Enzweiler, R Benenson, U Franke, S Roth, B Schiele, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition57M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele. The Cityscapes dataset for semantic urban scene understanding. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3213-3223, 2016. 5, 7\n\nModeling and rendering architecture from photographs: A hybrid geometryand image-based approach. P E Debevec, C J Taylor, J Malik, Proceedings of the 23rd annual conference on Computer graphics and interactive techniques. the 23rd annual conference on Computer graphics and interactive techniquesACMP. E. Debevec, C. J. Taylor, and J. Malik. Modeling and ren- dering architecture from photographs: A hybrid geometry- and image-based approach. In Proceedings of the 23rd an- nual conference on Computer graphics and interactive tech- niques, pages 11-20. ACM, 1996. 2\n\nDepth map prediction from a single image using a multi-scale deep network. D Eigen, C Puhrsch, R Fergus, Advances in Neural Information Processing Systems. 67D. Eigen, C. Puhrsch, and R. Fergus. Depth map prediction from a single image using a multi-scale deep network. In Advances in Neural Information Processing Systems, 2014. 2, 5, 6, 7\n\nDepth-image-based rendering (dibr), compression, and transmission for a new approach on 3d-tv. C Fehn, International Society for Optics and Photonics. Electronic ImagingC. Fehn. Depth-image-based rendering (dibr), compression, and transmission for a new approach on 3d-tv. In Electronic Imaging 2004, pages 93-104. International Society for Op- tics and Photonics, 2004. 3\n\nImage-based rendering using image-based priors. A Fitzgibbon, Y Wexler, A Zisserman, Int. Journal of Computer Vision. 632A. Fitzgibbon, Y. Wexler, and A. Zisserman. Image-based rendering using image-based priors. Int. Journal of Com- puter Vision, 63(2):141-151, 2005. 2\n\nDeep-Stereo: Learning to predict new views from the world's imagery. J Flynn, I Neulander, J Philbin, N Snavely, Computer Vision and Pattern Recognition. 13J. Flynn, I. Neulander, J. Philbin, and N. Snavely. Deep- Stereo: Learning to predict new views from the world's im- agery. In Computer Vision and Pattern Recognition, 2016. 1, 2, 3\n\nSingle image 3D without a single 3D image. D F Fouhey, W Hussain, A Gupta, M Hebert, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionD. F. Fouhey, W. Hussain, A. Gupta, and M. Hebert. Single image 3D without a single 3D image. In Proceedings of the IEEE International Conference on Computer Vision, pages 1053-1061, 2015. 2\n\nTowards internet-scale multi-view stereo. Y Furukawa, B Curless, S M Seitz, R Szeliski, Computer Vision and Pattern Recognition. Y. Furukawa, B. Curless, S. M. Seitz, and R. Szeliski. To- wards internet-scale multi-view stereo. In Computer Vision and Pattern Recognition, pages 1434-1441. IEEE, 2010. 2\n\nM Gadelha, S Maji, R Wang, arXiv:1612.058723d shape induction from 2d views of multiple objects. arXiv preprintM. Gadelha, S. Maji, and R. Wang. 3d shape induc- tion from 2d views of multiple objects. arXiv preprint arXiv:1612.05872, 2016. 2\n\nUnsupervised CNN for single view depth estimation: Geometry to the rescue. R Garg, V K Bg, G Carneiro, I Reid, European Conf. Computer Vision. 67R. Garg, V. K. BG, G. Carneiro, and I. Reid. Unsupervised CNN for single view depth estimation: Geometry to the res- cue. In European Conf. Computer Vision, 2016. 1, 2, 3, 4, 5, 6, 7\n\nAre we ready for autonomous driving? The KITTI vision benchmark suite. A Geiger, P Lenz, R Urtasun, Computer Vision and Pattern Recognition (CVPR). A. Geiger, P. Lenz, and R. Urtasun. Are we ready for autonomous driving? The KITTI vision benchmark suite. In Computer Vision and Pattern Recognition (CVPR), 2012\n\nIEEE Conference on. 7IEEE Conference on, pages 3354-3361. IEEE, 2012. 2, 5, 7\n\nUnsupervised monocular depth estimation with left-right consistency. C Godard, O Mac Aodha, G J Brostow, Computer Vision and Pattern Recognition. 57C. Godard, O. Mac Aodha, and G. J. Brostow. Unsupervised monocular depth estimation with left-right consistency. In Computer Vision and Pattern Recognition, 2017. 1, 2, 3, 4, 5, 7\n\nUnsupervised learning of spatiotemporally coherent metrics. R Goroshin, J Bruna, J Tompson, D Eigen, Y Le-Cun, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionR. Goroshin, J. Bruna, J. Tompson, D. Eigen, and Y. Le- Cun. Unsupervised learning of spatiotemporally coherent metrics. In Proceedings of the IEEE International Confer- ence on Computer Vision, pages 4086-4093, 2015. 2\n\nMatchNet: Unifying feature and metric learning for patchbased matching. X Han, T Leung, Y Jia, R Sukthankar, A C Berg, Computer Vision and Pattern Recognition. X. Han, T. Leung, Y. Jia, R. Sukthankar, and A. C. Berg. MatchNet: Unifying feature and metric learning for patch- based matching. In Computer Vision and Pattern Recogni- tion, pages 3279-3286, 2015. 2\n\nA Handa, M Bloesch, V Patraucean, S Stent, J Mccormac, A Davison, arXiv:1607.07405gvnn: Neural network library for geometric computer vision. arXiv preprintA. Handa, M. Bloesch, V. Patraucean, S. Stent, J. McCor- mac, and A. Davison. gvnn: Neural network library for ge- ometric computer vision. arXiv preprint arXiv:1607.07405, 2016. 2\n\nAutomatic photo pop-up. D Hoiem, A A Efros, M Hebert, Proc. SIGGRAPH. SIGGRAPHD. Hoiem, A. A. Efros, and M. Hebert . Automatic photo pop-up. In Proc. SIGGRAPH, 2005. 2\n\nBatch normalization: Accelerating deep network training by reducing internal covariate shift. S Ioffe, C Szegedy, arXiv:1502.03167arXiv preprintS. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015. 5\n\nAbout direct methods. M Irani, P Anandan, International Workshop on Vision Algorithms. SpringerM. Irani and P. Anandan. About direct methods. In In- ternational Workshop on Vision Algorithms, pages 267-277. Springer, 1999. 2\n\nSpatial transformer networks. M Jaderberg, K Simonyan, A Zisserman, Advances in Neural Information Processing Systems. M. Jaderberg, K. Simonyan, A. Zisserman, et al. Spatial transformer networks. In Advances in Neural Information Processing Systems, pages 2017-2025, 2015. 3\n\nLearning image representations tied to egomotion. D Jayaraman, K Grauman, Int. Conf. Computer Vision. D. Jayaraman and K. Grauman. Learning image representa- tions tied to egomotion. In Int. Conf. Computer Vision, 2015. 2\n\nDepth transfer: Depth extraction from video using non-parametric sampling. K Karsch, C Liu, S B Kang, IEEE transactions on pattern analysis and machine intelligence. 36K. Karsch, C. Liu, and S. B. Kang. Depth transfer: Depth extraction from video using non-parametric sampling. IEEE transactions on pattern analysis and machine intelligence, 36(11):2144-2158, 2014. 7\n\nPoseNet: A convolutional network for real-time 6-DOF camera relocalization. A Kendall, M Grimes, R Cipolla, Int. Conf. Computer Vision. A. Kendall, M. Grimes, and R. Cipolla. PoseNet: A convo- lutional network for real-time 6-DOF camera relocalization. In Int. Conf. Computer Vision, pages 2938-2946, 2015. 2\n\nEnd-to-end learning of geometry and context for deep stereo regression. A Kendall, H Martirosyan, S Dasgupta, P Henry, R Kennedy, A Bachrach, A Bry, arXiv:1703.04309arXiv preprintA. Kendall, H. Martirosyan, S. Dasgupta, P. Henry, R. Kennedy, A. Bachrach, and A. Bry. End-to-end learning of geometry and context for deep stereo regression. arXiv preprint arXiv:1703.04309, 2017. 2\n\nAdam: A method for stochastic optimization. D Kingma, J Ba, arXiv:1412.6980arXiv preprintD. Kingma and J. Ba. Adam: A method for stochastic opti- mization. arXiv preprint arXiv:1412.6980, 2014. 5\n\nDeep convolutional inverse graphics network. T D Kulkarni, W F Whitney, P Kohli, J Tenenbaum, Advances in Neural Information Processing Systems. C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. GarnettCurran Associates, IncT. D. Kulkarni, W. F. Whitney, P. Kohli, and J. Tenenbaum. Deep convolutional inverse graphics network. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems, pages 2539-2547. Curran Associates, Inc., 2015. 2\n\nSemi-supervised deep learning for monocular depth map prediction. Y Kuznietsov, J St\u00fcckler, B Leibe, arXiv:1702.02706arXiv preprintY. Kuznietsov, J. St\u00fcckler, and B. Leibe. Semi-supervised deep learning for monocular depth map prediction. arXiv preprint arXiv:1702.02706, 2017. 2\n\nDeeper depth prediction with fully convolutional residual networks. I Laina, C Rupprecht, V Belagiannis, F Tombari, N Navab, 3D Vision (3DV), 2016 Fourth International Conference on. IEEEI. Laina, C. Rupprecht, V. Belagiannis, F. Tombari, and N. Navab. Deeper depth prediction with fully convolutional residual networks. In 3D Vision (3DV), 2016 Fourth Interna- tional Conference on, pages 239-248. IEEE, 2016. 7\n\nLearning depth from single monocular images using deep convolutional neural fields. F Liu, C Shen, G Lin, I Reid, IEEE transactions on pattern analysis and machine intelligence. 38F. Liu, C. Shen, G. Lin, and I. Reid. Learning depth from sin- gle monocular images using deep convolutional neural fields. IEEE transactions on pattern analysis and machine intelli- gence, 38(10):2024-2039, 2016. 7\n\nDiscrete-continuous depth estimation from a single image. M Liu, M Salzmann, X He, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionM. Liu, M. Salzmann, and X. He. Discrete-continuous depth estimation from a single image. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 716-723, 2014. 7\n\nOpenDR: An approximate differentiable renderer. M M Loper, M J Black, European Conf. Computer Vision. SpringerM. M. Loper and M. J. Black. OpenDR: An approximate differentiable renderer. In European Conf. Computer Vision, pages 154-169. Springer, 2014. 2\n\nA large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. N Mayer, E Ilg, P Hausser, P Fischer, D Cremers, A Dosovitskiy, T Brox, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionN. Mayer, E. Ilg, P. Hausser, P. Fischer, D. Cremers, A. Dosovitskiy, and T. Brox. A large dataset to train con- volutional networks for disparity, optical flow, and scene flow estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4040- 4048, 2016. 4\n\nShuffle and learn: unsupervised learning using temporal order verification. I Misra, C L Zitnick, M Hebert, European Conference on Computer Vision. SpringerI. Misra, C. L. Zitnick, and M. Hebert. Shuffle and learn: unsupervised learning using temporal order verification. In European Conference on Computer Vision, pages 527-544. Springer, 2016. 2\n\nORB-SLAM: a versatile and accurate monocular SLAM system. R Mur-Artal, J M M Montiel, J D Tardos, IEEE Transactions on Robotics. 3157R. Mur-Artal, J. M. M. Montiel, and J. D. Tardos. ORB- SLAM: a versatile and accurate monocular SLAM system. IEEE Transactions on Robotics, 31(5), 2015. 6, 7\n\nDTAM: Dense tracking and mapping in real-time. R A Newcombe, S J Lovegrove, A J Davison, Int. Conf. Computer Vision. R. A. Newcombe, S. J. Lovegrove, and A. J. Davison. DTAM: Dense tracking and mapping in real-time. In Int. Conf. Computer Vision, pages 2320-2327. IEEE, 2011. 2\n\nLearning features by watching objects move. D Pathak, R Girshick, P Doll\u00e1r, T Darrell, B Hariharan, CVPR. D. Pathak, R. Girshick, P. Doll\u00e1r, T. Darrell, and B. Hariha- ran. Learning features by watching objects move. In CVPR, 2017. 2\n\nDense monocular depth estimation in complex dynamic scenes. R Ranftl, V Vineet, Q Chen, V Koltun, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionR. Ranftl, V. Vineet, Q. Chen, and V. Koltun. Dense monoc- ular depth estimation in complex dynamic scenes. In Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4058-4066, 2016. 8\n\nUnsupervised learning of 3d structure from images. D J Rezende, S A Eslami, S Mohamed, P Battaglia, M Jaderberg, N Heess, Advances In Neural Information Processing Systems. D. J. Rezende, S. A. Eslami, S. Mohamed, P. Battaglia, M. Jaderberg, and N. Heess. Unsupervised learning of 3d structure from images. In Advances In Neural Information Processing Systems, pages 4997-5005, 2016. 2\n\nMake3D: Learning 3D scene structure from a single still image. Pattern Analysis and Machine Intelligence. A Saxena, M Sun, A Y Ng, 317A. Saxena, M. Sun, and A. Y. Ng. Make3D: Learning 3D scene structure from a single still image. Pattern Analysis and Machine Intelligence, 31(5):824-840, May 2009. 2, 5, 7\n\nView morphing. S M Seitz, C R Dyer, Proceedings of the 23rd annual conference on Computer graphics and interactive techniques. the 23rd annual conference on Computer graphics and interactive techniquesACMS. M. Seitz and C. R. Dyer. View morphing. In Proceedings of the 23rd annual conference on Computer graphics and interactive techniques, pages 21-30. ACM, 1996. 2\n\nPrediction error as a quality metric for motion and stereo. R Szeliski, Int. Conf. Computer Vision. IEEE2R. Szeliski. Prediction error as a quality metric for motion and stereo. In Int. Conf. Computer Vision, volume 2, pages 781-788. IEEE, 1999. 1\n\nMulti-view 3d models from single images with a convolutional network. M Tatarchenko, A Dosovitskiy, T Brox, European Conference on Computer Vision. SpringerM. Tatarchenko, A. Dosovitskiy, and T. Brox. Multi-view 3d models from single images with a convolutional network. In European Conference on Computer Vision, pages 322-337. Springer, 2016. 2\n\nMulti-view supervision for single-view reconstruction via differentiable ray consistency. S Tulsiani, T Zhou, A A Efros, J Malik, Computer Vision and Pattern Recognition. 2S. Tulsiani, T. Zhou, A. A. Efros, and J. Malik. Multi-view supervision for single-view reconstruction via differentiable ray consistency. In Computer Vision and Pattern Recogni- tion, 2017. 2, 8\n\nDeMoN: Depth and motion network for learning monocular stereo. B Ummenhofer, H Zhou, J Uhrig, N Mayer, E Ilg, A Dosovitskiy, T Brox, arXiv:1612.02401arXiv preprintB. Ummenhofer, H. Zhou, J. Uhrig, N. Mayer, E. Ilg, A. Dosovitskiy, and T. Brox. DeMoN: Depth and mo- tion network for learning monocular stereo. arXiv preprint arXiv:1612.02401, 2016. 4\n\nS Vijayanarasimhan, S Ricco, C Schmid, R Sukthankar, K Fragkiadaki, SfM-Net: Learning of structure and motion from video. arXiv preprintS. Vijayanarasimhan, S. Ricco, C. Schmid, R. Sukthankar, and K. Fragkiadaki. SfM-Net: Learning of structure and mo- tion from video. arXiv preprint, 2017. 2, 4, 8\n\nUnsupervised learning of visual representations using videos. X Wang, A Gupta, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionX. Wang and A. Gupta. Unsupervised learning of visual rep- resentations using videos. In Proceedings of the IEEE Inter- national Conference on Computer Vision, pages 2794-2802, 2015. 2\n\nVisualSFM: A visual structure from motion system. C Wu, C. Wu. VisualSFM: A visual structure from motion system. http://ccwu.me/vsfm, 2011. 2\n\nDeep3D: Fully automatic 2D-to-3D video conversion with deep convolutional neural networks. J Xie, R B Girshick, A Farhadi, European Conf. Computer Vision. J. Xie, R. B. Girshick, and A. Farhadi. Deep3D: Fully au- tomatic 2D-to-3D video conversion with deep convolutional neural networks. In European Conf. Computer Vision, 2016. 2\n\nPerspective transformer nets: Learning single-view 3d object reconstruction without 3d supervision. X Yan, J Yang, E Yumer, Y Guo, H Lee, Advances in Neural Information Processing Systems. X. Yan, J. Yang, E. Yumer, Y. Guo, and H. Lee. Perspective transformer nets: Learning single-view 3d object reconstruc- tion without 3d supervision. In Advances in Neural Informa- tion Processing Systems, pages 1696-1704, 2016. 2\n\nStereo matching by training a convolutional neural network to compare image patches. J Zbontar, Y Lecun, Journal of Machine Learning Research. 172J. Zbontar and Y. LeCun. Stereo matching by training a con- volutional neural network to compare image patches. Jour- nal of Machine Learning Research, 17(1-32):2, 2016. 2\n\nView synthesis by appearance flow. T Zhou, S Tulsiani, W Sun, J Malik, A A Efros, European Conference on Computer Vision. Springer23T. Zhou, S. Tulsiani, W. Sun, J. Malik, and A. A. Efros. View synthesis by appearance flow. In European Conference on Computer Vision, pages 286-301. Springer, 2016. 2, 3\n\nHigh-quality video view interpolation using a layered representation. C L Zitnick, S B Kang, M Uyttendaele, S Winder, R Szeliski, In ACM Transactions on Graphics. 232ACMC. L. Zitnick, S. B. Kang, M. Uyttendaele, S. Winder, and R. Szeliski. High-quality video view interpolation using a layered representation. In ACM Transactions on Graphics (TOG), volume 23, pages 600-608. ACM, 2004. 2\n", "annotations": {"author": "[{\"end\":87,\"start\":60},{\"end\":116,\"start\":88},{\"end\":143,\"start\":117},{\"end\":173,\"start\":144},{\"end\":208,\"start\":174}]", "publisher": null, "author_last_name": "[{\"end\":72,\"start\":68},{\"end\":101,\"start\":96},{\"end\":128,\"start\":124},{\"end\":158,\"start\":152},{\"end\":193,\"start\":182}]", "author_first_name": "[{\"end\":67,\"start\":60},{\"end\":95,\"start\":88},{\"end\":123,\"start\":117},{\"end\":151,\"start\":144},{\"end\":179,\"start\":174},{\"end\":181,\"start\":180}]", "author_affiliation": "[{\"end\":86,\"start\":74},{\"end\":115,\"start\":103},{\"end\":142,\"start\":130},{\"end\":172,\"start\":160},{\"end\":207,\"start\":195}]", "title": "[{\"end\":57,\"start\":1},{\"end\":265,\"start\":209}]", "venue": null, "abstract": "[{\"end\":1242,\"start\":267}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b44\"},\"end\":3006,\"start\":3002},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3102,\"start\":3098},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4166,\"start\":4162},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4518,\"start\":4514},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":4521,\"start\":4518},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":4524,\"start\":4521},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4884,\"start\":4880},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":4906,\"start\":4902},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4923,\"start\":4919},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4926,\"start\":4923},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":4929,\"start\":4926},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5593,\"start\":5590},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":5596,\"start\":5593},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":5599,\"start\":5596},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5601,\"start\":5599},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5603,\"start\":5601},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5750,\"start\":5746},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":5763,\"start\":5759},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":5789,\"start\":5785},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":6070,\"start\":6066},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6495,\"start\":6491},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":6642,\"start\":6638},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6798,\"start\":6794},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7079,\"start\":7075},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":7082,\"start\":7079},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7084,\"start\":7082},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7087,\"start\":7084},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7090,\"start\":7087},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7190,\"start\":7186},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7676,\"start\":7672},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7679,\"start\":7676},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":7682,\"start\":7679},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7833,\"start\":7829},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":7836,\"start\":7833},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":7839,\"start\":7836},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":7842,\"start\":7839},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8000,\"start\":7996},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8527,\"start\":8524},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8530,\"start\":8527},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":8545,\"start\":8541},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8570,\"start\":8566},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8604,\"start\":8600},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":8644,\"start\":8640},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":8977,\"start\":8973},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":11939,\"start\":11936},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":12043,\"start\":12042},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12213,\"start\":12209},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12216,\"start\":12213},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12243,\"start\":12239},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":13379,\"start\":13375},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":13719,\"start\":13715},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":15651,\"start\":15648},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":16036,\"start\":16032},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":16039,\"start\":16036},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":16322,\"start\":16318},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":16730,\"start\":16726},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":17246,\"start\":17242},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":18731,\"start\":18727},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":18786,\"start\":18782},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":18926,\"start\":18923},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":19102,\"start\":19098},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":19169,\"start\":19165},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":19652,\"start\":19649},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":20392,\"start\":20389},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":20522,\"start\":20518},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":20525,\"start\":20522},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":20763,\"start\":20759},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":20857,\"start\":20854},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":21082,\"start\":21079},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":21203,\"start\":21200},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":21224,\"start\":21220},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":21283,\"start\":21279},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":22647,\"start\":22644},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":22703,\"start\":22699},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":24320,\"start\":24316},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":24633,\"start\":24629},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":24669,\"start\":24666},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":24703,\"start\":24699},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":24754,\"start\":24751},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":24919,\"start\":24915},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":25417,\"start\":25413},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":25454,\"start\":25450},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":25625,\"start\":25621},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":26025,\"start\":26021},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":26115,\"start\":26114},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":29310,\"start\":29306},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":29313,\"start\":29310},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":29707,\"start\":29703},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":31168,\"start\":31164}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":30308,\"start\":30248},{\"attributes\":{\"id\":\"fig_1\"},\"end\":30500,\"start\":30309},{\"attributes\":{\"id\":\"fig_2\"},\"end\":30790,\"start\":30501},{\"attributes\":{\"id\":\"fig_4\"},\"end\":32043,\"start\":30791},{\"attributes\":{\"id\":\"fig_5\"},\"end\":32149,\"start\":32044},{\"attributes\":{\"id\":\"fig_6\"},\"end\":32298,\"start\":32150},{\"attributes\":{\"id\":\"fig_7\"},\"end\":32656,\"start\":32299}]", "paragraph": "[{\"end\":2439,\"start\":1258},{\"end\":3242,\"start\":2441},{\"end\":4353,\"start\":3244},{\"end\":5118,\"start\":4370},{\"end\":6301,\"start\":5120},{\"end\":7091,\"start\":6303},{\"end\":7683,\"start\":7093},{\"end\":8923,\"start\":7685},{\"end\":9522,\"start\":8925},{\"end\":10782,\"start\":9535},{\"end\":11535,\"start\":10903},{\"end\":11754,\"start\":11537},{\"end\":12081,\"start\":11785},{\"end\":12627,\"start\":12083},{\"end\":12925,\"start\":12674},{\"end\":13112,\"start\":12927},{\"end\":13917,\"start\":13140},{\"end\":14670,\"start\":13951},{\"end\":15218,\"start\":14672},{\"end\":16324,\"start\":15255},{\"end\":16353,\"start\":16326},{\"end\":16603,\"start\":16414},{\"end\":17337,\"start\":16628},{\"end\":17951,\"start\":17339},{\"end\":18542,\"start\":17953},{\"end\":18838,\"start\":18558},{\"end\":19571,\"start\":18840},{\"end\":20160,\"start\":19604},{\"end\":20478,\"start\":20162},{\"end\":20979,\"start\":20494},{\"end\":21909,\"start\":20981},{\"end\":22250,\"start\":21911},{\"end\":23346,\"start\":22252},{\"end\":23807,\"start\":23357},{\"end\":25009,\"start\":23827},{\"end\":25715,\"start\":25011},{\"end\":25782,\"start\":25717},{\"end\":27181,\"start\":25784},{\"end\":27660,\"start\":27183},{\"end\":28210,\"start\":27706},{\"end\":29000,\"start\":28225},{\"end\":30247,\"start\":29002}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10870,\"start\":10783},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11784,\"start\":11755},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13139,\"start\":13113},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16413,\"start\":16354}]", "table_ref": "[{\"end\":21103,\"start\":21096},{\"end\":22331,\"start\":22324},{\"end\":23671,\"start\":23664},{\"end\":25381,\"start\":25374},{\"end\":26135,\"start\":26128},{\"end\":26600,\"start\":26593}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1256,\"start\":1244},{\"attributes\":{\"n\":\"2.\"},\"end\":4368,\"start\":4356},{\"attributes\":{\"n\":\"3.\"},\"end\":9533,\"start\":9525},{\"attributes\":{\"n\":\"3.1.\"},\"end\":10901,\"start\":10872},{\"attributes\":{\"n\":\"3.2.\"},\"end\":12672,\"start\":12630},{\"attributes\":{\"n\":\"3.3.\"},\"end\":13949,\"start\":13920},{\"attributes\":{\"n\":\"3.4.\"},\"end\":15253,\"start\":15221},{\"attributes\":{\"n\":\"3.5.\"},\"end\":16626,\"start\":16606},{\"attributes\":{\"n\":\"4.\"},\"end\":18556,\"start\":18545},{\"attributes\":{\"n\":\"4.1.\"},\"end\":19602,\"start\":19574},{\"end\":20492,\"start\":20481},{\"end\":23355,\"start\":23349},{\"attributes\":{\"n\":\"4.2.\"},\"end\":23825,\"start\":23810},{\"attributes\":{\"n\":\"4.3.\"},\"end\":27704,\"start\":27663},{\"attributes\":{\"n\":\"5.\"},\"end\":28223,\"start\":28213},{\"end\":30320,\"start\":30310},{\"end\":30512,\"start\":30502},{\"end\":30802,\"start\":30792},{\"end\":32055,\"start\":32045},{\"end\":32161,\"start\":32151},{\"end\":32308,\"start\":32300}]", "table": null, "figure_caption": "[{\"end\":30308,\"start\":30250},{\"end\":30500,\"start\":30322},{\"end\":30790,\"start\":30514},{\"end\":32043,\"start\":30804},{\"end\":32149,\"start\":32057},{\"end\":32298,\"start\":32163},{\"end\":32656,\"start\":32310}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":2647,\"start\":2641},{\"end\":9978,\"start\":9970},{\"end\":12551,\"start\":12543},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":13268,\"start\":13260},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":16849,\"start\":16841},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":20899,\"start\":20891},{\"end\":21563,\"start\":21555},{\"end\":22063,\"start\":22055},{\"end\":22578,\"start\":22570},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":23805,\"start\":23797},{\"end\":25059,\"start\":25051},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":26146,\"start\":26140},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":26488,\"start\":26480},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":27260,\"start\":27252},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":27785,\"start\":27776},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":28792,\"start\":28783}]", "bib_author_first_name": "[{\"end\":33530,\"start\":33529},{\"end\":33539,\"start\":33538},{\"end\":33550,\"start\":33549},{\"end\":33560,\"start\":33559},{\"end\":33570,\"start\":33569},{\"end\":33578,\"start\":33577},{\"end\":33587,\"start\":33586},{\"end\":33589,\"start\":33588},{\"end\":33600,\"start\":33599},{\"end\":33609,\"start\":33608},{\"end\":33617,\"start\":33616},{\"end\":33987,\"start\":33986},{\"end\":33998,\"start\":33997},{\"end\":34010,\"start\":34009},{\"end\":34197,\"start\":34196},{\"end\":34207,\"start\":34206},{\"end\":34218,\"start\":34217},{\"end\":34227,\"start\":34226},{\"end\":34473,\"start\":34472},{\"end\":34475,\"start\":34474},{\"end\":34483,\"start\":34482},{\"end\":34918,\"start\":34917},{\"end\":34928,\"start\":34927},{\"end\":34937,\"start\":34936},{\"end\":34946,\"start\":34945},{\"end\":34957,\"start\":34956},{\"end\":34970,\"start\":34969},{\"end\":34982,\"start\":34981},{\"end\":34992,\"start\":34991},{\"end\":35000,\"start\":34999},{\"end\":35531,\"start\":35530},{\"end\":35533,\"start\":35532},{\"end\":35544,\"start\":35543},{\"end\":35546,\"start\":35545},{\"end\":35556,\"start\":35555},{\"end\":36077,\"start\":36076},{\"end\":36086,\"start\":36085},{\"end\":36097,\"start\":36096},{\"end\":36439,\"start\":36438},{\"end\":36766,\"start\":36765},{\"end\":36780,\"start\":36779},{\"end\":36790,\"start\":36789},{\"end\":37059,\"start\":37058},{\"end\":37068,\"start\":37067},{\"end\":37081,\"start\":37080},{\"end\":37092,\"start\":37091},{\"end\":37372,\"start\":37371},{\"end\":37374,\"start\":37373},{\"end\":37384,\"start\":37383},{\"end\":37395,\"start\":37394},{\"end\":37404,\"start\":37403},{\"end\":37769,\"start\":37768},{\"end\":37781,\"start\":37780},{\"end\":37792,\"start\":37791},{\"end\":37794,\"start\":37793},{\"end\":37803,\"start\":37802},{\"end\":38031,\"start\":38030},{\"end\":38042,\"start\":38041},{\"end\":38050,\"start\":38049},{\"end\":38349,\"start\":38348},{\"end\":38357,\"start\":38356},{\"end\":38359,\"start\":38358},{\"end\":38365,\"start\":38364},{\"end\":38377,\"start\":38376},{\"end\":38674,\"start\":38673},{\"end\":38684,\"start\":38683},{\"end\":38692,\"start\":38691},{\"end\":39063,\"start\":39062},{\"end\":39073,\"start\":39072},{\"end\":39077,\"start\":39074},{\"end\":39086,\"start\":39085},{\"end\":39088,\"start\":39087},{\"end\":39383,\"start\":39382},{\"end\":39395,\"start\":39394},{\"end\":39404,\"start\":39403},{\"end\":39415,\"start\":39414},{\"end\":39424,\"start\":39423},{\"end\":39848,\"start\":39847},{\"end\":39855,\"start\":39854},{\"end\":39864,\"start\":39863},{\"end\":39871,\"start\":39870},{\"end\":39885,\"start\":39884},{\"end\":39887,\"start\":39886},{\"end\":40139,\"start\":40138},{\"end\":40148,\"start\":40147},{\"end\":40159,\"start\":40158},{\"end\":40173,\"start\":40172},{\"end\":40182,\"start\":40181},{\"end\":40194,\"start\":40193},{\"end\":40501,\"start\":40500},{\"end\":40510,\"start\":40509},{\"end\":40512,\"start\":40511},{\"end\":40521,\"start\":40520},{\"end\":40740,\"start\":40739},{\"end\":40749,\"start\":40748},{\"end\":40973,\"start\":40972},{\"end\":40982,\"start\":40981},{\"end\":41207,\"start\":41206},{\"end\":41220,\"start\":41219},{\"end\":41232,\"start\":41231},{\"end\":41504,\"start\":41503},{\"end\":41517,\"start\":41516},{\"end\":41752,\"start\":41751},{\"end\":41762,\"start\":41761},{\"end\":41769,\"start\":41768},{\"end\":41771,\"start\":41770},{\"end\":42122,\"start\":42121},{\"end\":42133,\"start\":42132},{\"end\":42143,\"start\":42142},{\"end\":42428,\"start\":42427},{\"end\":42439,\"start\":42438},{\"end\":42454,\"start\":42453},{\"end\":42466,\"start\":42465},{\"end\":42475,\"start\":42474},{\"end\":42486,\"start\":42485},{\"end\":42498,\"start\":42497},{\"end\":42781,\"start\":42780},{\"end\":42791,\"start\":42790},{\"end\":42979,\"start\":42978},{\"end\":42981,\"start\":42980},{\"end\":42993,\"start\":42992},{\"end\":42995,\"start\":42994},{\"end\":43006,\"start\":43005},{\"end\":43015,\"start\":43014},{\"end\":43517,\"start\":43516},{\"end\":43531,\"start\":43530},{\"end\":43543,\"start\":43542},{\"end\":43800,\"start\":43799},{\"end\":43809,\"start\":43808},{\"end\":43822,\"start\":43821},{\"end\":43837,\"start\":43836},{\"end\":43848,\"start\":43847},{\"end\":44230,\"start\":44229},{\"end\":44237,\"start\":44236},{\"end\":44245,\"start\":44244},{\"end\":44252,\"start\":44251},{\"end\":44601,\"start\":44600},{\"end\":44608,\"start\":44607},{\"end\":44620,\"start\":44619},{\"end\":45011,\"start\":45010},{\"end\":45013,\"start\":45012},{\"end\":45022,\"start\":45021},{\"end\":45024,\"start\":45023},{\"end\":45323,\"start\":45322},{\"end\":45332,\"start\":45331},{\"end\":45339,\"start\":45338},{\"end\":45350,\"start\":45349},{\"end\":45361,\"start\":45360},{\"end\":45372,\"start\":45371},{\"end\":45387,\"start\":45386},{\"end\":45910,\"start\":45909},{\"end\":45919,\"start\":45918},{\"end\":45921,\"start\":45920},{\"end\":45932,\"start\":45931},{\"end\":46241,\"start\":46240},{\"end\":46254,\"start\":46253},{\"end\":46258,\"start\":46255},{\"end\":46269,\"start\":46268},{\"end\":46271,\"start\":46270},{\"end\":46522,\"start\":46521},{\"end\":46524,\"start\":46523},{\"end\":46536,\"start\":46535},{\"end\":46538,\"start\":46537},{\"end\":46551,\"start\":46550},{\"end\":46553,\"start\":46552},{\"end\":46798,\"start\":46797},{\"end\":46808,\"start\":46807},{\"end\":46820,\"start\":46819},{\"end\":46830,\"start\":46829},{\"end\":46841,\"start\":46840},{\"end\":47049,\"start\":47048},{\"end\":47059,\"start\":47058},{\"end\":47069,\"start\":47068},{\"end\":47077,\"start\":47076},{\"end\":47497,\"start\":47496},{\"end\":47499,\"start\":47498},{\"end\":47510,\"start\":47509},{\"end\":47512,\"start\":47511},{\"end\":47522,\"start\":47521},{\"end\":47533,\"start\":47532},{\"end\":47546,\"start\":47545},{\"end\":47559,\"start\":47558},{\"end\":47939,\"start\":47938},{\"end\":47949,\"start\":47948},{\"end\":47956,\"start\":47955},{\"end\":47958,\"start\":47957},{\"end\":48155,\"start\":48154},{\"end\":48157,\"start\":48156},{\"end\":48166,\"start\":48165},{\"end\":48168,\"start\":48167},{\"end\":48568,\"start\":48567},{\"end\":48827,\"start\":48826},{\"end\":48842,\"start\":48841},{\"end\":48857,\"start\":48856},{\"end\":49195,\"start\":49194},{\"end\":49207,\"start\":49206},{\"end\":49215,\"start\":49214},{\"end\":49217,\"start\":49216},{\"end\":49226,\"start\":49225},{\"end\":49537,\"start\":49536},{\"end\":49551,\"start\":49550},{\"end\":49559,\"start\":49558},{\"end\":49568,\"start\":49567},{\"end\":49577,\"start\":49576},{\"end\":49584,\"start\":49583},{\"end\":49599,\"start\":49598},{\"end\":49825,\"start\":49824},{\"end\":49845,\"start\":49844},{\"end\":49854,\"start\":49853},{\"end\":49864,\"start\":49863},{\"end\":49878,\"start\":49877},{\"end\":50187,\"start\":50186},{\"end\":50195,\"start\":50194},{\"end\":50561,\"start\":50560},{\"end\":50745,\"start\":50744},{\"end\":50752,\"start\":50751},{\"end\":50754,\"start\":50753},{\"end\":50766,\"start\":50765},{\"end\":51086,\"start\":51085},{\"end\":51093,\"start\":51092},{\"end\":51101,\"start\":51100},{\"end\":51110,\"start\":51109},{\"end\":51117,\"start\":51116},{\"end\":51491,\"start\":51490},{\"end\":51502,\"start\":51501},{\"end\":51760,\"start\":51759},{\"end\":51768,\"start\":51767},{\"end\":51780,\"start\":51779},{\"end\":51787,\"start\":51786},{\"end\":51796,\"start\":51795},{\"end\":51798,\"start\":51797},{\"end\":52099,\"start\":52098},{\"end\":52101,\"start\":52100},{\"end\":52112,\"start\":52111},{\"end\":52114,\"start\":52113},{\"end\":52122,\"start\":52121},{\"end\":52137,\"start\":52136},{\"end\":52147,\"start\":52146}]", "bib_author_last_name": "[{\"end\":33536,\"start\":33531},{\"end\":33547,\"start\":33540},{\"end\":33557,\"start\":33551},{\"end\":33567,\"start\":33561},{\"end\":33575,\"start\":33571},{\"end\":33584,\"start\":33579},{\"end\":33597,\"start\":33590},{\"end\":33606,\"start\":33601},{\"end\":33614,\"start\":33610},{\"end\":33623,\"start\":33618},{\"end\":33995,\"start\":33988},{\"end\":34007,\"start\":33999},{\"end\":34016,\"start\":34011},{\"end\":34204,\"start\":34198},{\"end\":34215,\"start\":34208},{\"end\":34224,\"start\":34219},{\"end\":34237,\"start\":34228},{\"end\":34480,\"start\":34476},{\"end\":34492,\"start\":34484},{\"end\":34925,\"start\":34919},{\"end\":34934,\"start\":34929},{\"end\":34943,\"start\":34938},{\"end\":34954,\"start\":34947},{\"end\":34967,\"start\":34958},{\"end\":34979,\"start\":34971},{\"end\":34989,\"start\":34983},{\"end\":34997,\"start\":34993},{\"end\":35008,\"start\":35001},{\"end\":35541,\"start\":35534},{\"end\":35553,\"start\":35547},{\"end\":35562,\"start\":35557},{\"end\":36083,\"start\":36078},{\"end\":36094,\"start\":36087},{\"end\":36104,\"start\":36098},{\"end\":36444,\"start\":36440},{\"end\":36777,\"start\":36767},{\"end\":36787,\"start\":36781},{\"end\":36800,\"start\":36791},{\"end\":37065,\"start\":37060},{\"end\":37078,\"start\":37069},{\"end\":37089,\"start\":37082},{\"end\":37100,\"start\":37093},{\"end\":37381,\"start\":37375},{\"end\":37392,\"start\":37385},{\"end\":37401,\"start\":37396},{\"end\":37411,\"start\":37405},{\"end\":37778,\"start\":37770},{\"end\":37789,\"start\":37782},{\"end\":37800,\"start\":37795},{\"end\":37812,\"start\":37804},{\"end\":38039,\"start\":38032},{\"end\":38047,\"start\":38043},{\"end\":38055,\"start\":38051},{\"end\":38354,\"start\":38350},{\"end\":38362,\"start\":38360},{\"end\":38374,\"start\":38366},{\"end\":38382,\"start\":38378},{\"end\":38681,\"start\":38675},{\"end\":38689,\"start\":38685},{\"end\":38700,\"start\":38693},{\"end\":39070,\"start\":39064},{\"end\":39083,\"start\":39078},{\"end\":39096,\"start\":39089},{\"end\":39392,\"start\":39384},{\"end\":39401,\"start\":39396},{\"end\":39412,\"start\":39405},{\"end\":39421,\"start\":39416},{\"end\":39431,\"start\":39425},{\"end\":39852,\"start\":39849},{\"end\":39861,\"start\":39856},{\"end\":39868,\"start\":39865},{\"end\":39882,\"start\":39872},{\"end\":39892,\"start\":39888},{\"end\":40145,\"start\":40140},{\"end\":40156,\"start\":40149},{\"end\":40170,\"start\":40160},{\"end\":40179,\"start\":40174},{\"end\":40191,\"start\":40183},{\"end\":40202,\"start\":40195},{\"end\":40507,\"start\":40502},{\"end\":40518,\"start\":40513},{\"end\":40528,\"start\":40522},{\"end\":40746,\"start\":40741},{\"end\":40757,\"start\":40750},{\"end\":40979,\"start\":40974},{\"end\":40990,\"start\":40983},{\"end\":41217,\"start\":41208},{\"end\":41229,\"start\":41221},{\"end\":41242,\"start\":41233},{\"end\":41514,\"start\":41505},{\"end\":41525,\"start\":41518},{\"end\":41759,\"start\":41753},{\"end\":41766,\"start\":41763},{\"end\":41776,\"start\":41772},{\"end\":42130,\"start\":42123},{\"end\":42140,\"start\":42134},{\"end\":42151,\"start\":42144},{\"end\":42436,\"start\":42429},{\"end\":42451,\"start\":42440},{\"end\":42463,\"start\":42455},{\"end\":42472,\"start\":42467},{\"end\":42483,\"start\":42476},{\"end\":42495,\"start\":42487},{\"end\":42502,\"start\":42499},{\"end\":42788,\"start\":42782},{\"end\":42794,\"start\":42792},{\"end\":42990,\"start\":42982},{\"end\":43003,\"start\":42996},{\"end\":43012,\"start\":43007},{\"end\":43025,\"start\":43016},{\"end\":43528,\"start\":43518},{\"end\":43540,\"start\":43532},{\"end\":43549,\"start\":43544},{\"end\":43806,\"start\":43801},{\"end\":43819,\"start\":43810},{\"end\":43834,\"start\":43823},{\"end\":43845,\"start\":43838},{\"end\":43854,\"start\":43849},{\"end\":44234,\"start\":44231},{\"end\":44242,\"start\":44238},{\"end\":44249,\"start\":44246},{\"end\":44257,\"start\":44253},{\"end\":44605,\"start\":44602},{\"end\":44617,\"start\":44609},{\"end\":44623,\"start\":44621},{\"end\":45019,\"start\":45014},{\"end\":45030,\"start\":45025},{\"end\":45329,\"start\":45324},{\"end\":45336,\"start\":45333},{\"end\":45347,\"start\":45340},{\"end\":45358,\"start\":45351},{\"end\":45369,\"start\":45362},{\"end\":45384,\"start\":45373},{\"end\":45392,\"start\":45388},{\"end\":45916,\"start\":45911},{\"end\":45929,\"start\":45922},{\"end\":45939,\"start\":45933},{\"end\":46251,\"start\":46242},{\"end\":46266,\"start\":46259},{\"end\":46278,\"start\":46272},{\"end\":46533,\"start\":46525},{\"end\":46548,\"start\":46539},{\"end\":46561,\"start\":46554},{\"end\":46805,\"start\":46799},{\"end\":46817,\"start\":46809},{\"end\":46827,\"start\":46821},{\"end\":46838,\"start\":46831},{\"end\":46851,\"start\":46842},{\"end\":47056,\"start\":47050},{\"end\":47066,\"start\":47060},{\"end\":47074,\"start\":47070},{\"end\":47084,\"start\":47078},{\"end\":47507,\"start\":47500},{\"end\":47519,\"start\":47513},{\"end\":47530,\"start\":47523},{\"end\":47543,\"start\":47534},{\"end\":47556,\"start\":47547},{\"end\":47565,\"start\":47560},{\"end\":47946,\"start\":47940},{\"end\":47953,\"start\":47950},{\"end\":47961,\"start\":47959},{\"end\":48163,\"start\":48158},{\"end\":48173,\"start\":48169},{\"end\":48577,\"start\":48569},{\"end\":48839,\"start\":48828},{\"end\":48854,\"start\":48843},{\"end\":48862,\"start\":48858},{\"end\":49204,\"start\":49196},{\"end\":49212,\"start\":49208},{\"end\":49223,\"start\":49218},{\"end\":49232,\"start\":49227},{\"end\":49548,\"start\":49538},{\"end\":49556,\"start\":49552},{\"end\":49565,\"start\":49560},{\"end\":49574,\"start\":49569},{\"end\":49581,\"start\":49578},{\"end\":49596,\"start\":49585},{\"end\":49604,\"start\":49600},{\"end\":49842,\"start\":49826},{\"end\":49851,\"start\":49846},{\"end\":49861,\"start\":49855},{\"end\":49875,\"start\":49865},{\"end\":49890,\"start\":49879},{\"end\":50192,\"start\":50188},{\"end\":50201,\"start\":50196},{\"end\":50564,\"start\":50562},{\"end\":50749,\"start\":50746},{\"end\":50763,\"start\":50755},{\"end\":50774,\"start\":50767},{\"end\":51090,\"start\":51087},{\"end\":51098,\"start\":51094},{\"end\":51107,\"start\":51102},{\"end\":51114,\"start\":51111},{\"end\":51121,\"start\":51118},{\"end\":51499,\"start\":51492},{\"end\":51508,\"start\":51503},{\"end\":51765,\"start\":51761},{\"end\":51777,\"start\":51769},{\"end\":51784,\"start\":51781},{\"end\":51793,\"start\":51788},{\"end\":51804,\"start\":51799},{\"end\":52109,\"start\":52102},{\"end\":52119,\"start\":52115},{\"end\":52134,\"start\":52123},{\"end\":52144,\"start\":52138},{\"end\":52156,\"start\":52148}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1603.04467\",\"id\":\"b0\"},\"end\":33957,\"start\":33529},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":1637703},\"end\":34150,\"start\":33959},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":6267598},\"end\":34430,\"start\":34152},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":7680709},\"end\":34852,\"start\":34432},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":502946},\"end\":35431,\"start\":34854},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":2609415},\"end\":35999,\"start\":35433},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":2255738},\"end\":36341,\"start\":36001},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":62566129},\"end\":36715,\"start\":36343},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":9987199},\"end\":36987,\"start\":36717},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":14517241},\"end\":37326,\"start\":36989},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":8006310},\"end\":37724,\"start\":37328},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":2499865},\"end\":38028,\"start\":37726},{\"attributes\":{\"doi\":\"arXiv:1612.05872\",\"id\":\"b12\"},\"end\":38271,\"start\":38030},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":299085},\"end\":38600,\"start\":38273},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":6724907},\"end\":38912,\"start\":38602},{\"attributes\":{\"id\":\"b15\"},\"end\":38991,\"start\":38914},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":206596513},\"end\":39320,\"start\":38993},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":98541},\"end\":39773,\"start\":39322},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":15179402},\"end\":40136,\"start\":39775},{\"attributes\":{\"doi\":\"arXiv:1607.07405\",\"id\":\"b19\"},\"end\":40474,\"start\":40138},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":834882},\"end\":40643,\"start\":40476},{\"attributes\":{\"doi\":\"arXiv:1502.03167\",\"id\":\"b21\"},\"end\":40948,\"start\":40645},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":16235200},\"end\":41174,\"start\":40950},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":6099034},\"end\":41451,\"start\":41176},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":1144566},\"end\":41674,\"start\":41453},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":12230426},\"end\":42043,\"start\":41676},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":12888763},\"end\":42353,\"start\":42045},{\"attributes\":{\"doi\":\"arXiv:1703.04309\",\"id\":\"b27\"},\"end\":42734,\"start\":42355},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b28\"},\"end\":42931,\"start\":42736},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":14020873},\"end\":43448,\"start\":42933},{\"attributes\":{\"doi\":\"arXiv:1702.02706\",\"id\":\"b30\"},\"end\":43729,\"start\":43450},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":11091110},\"end\":44143,\"start\":43731},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":15774646},\"end\":44540,\"start\":44145},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":6340621},\"end\":44960,\"start\":44542},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":17868098},\"end\":45216,\"start\":44962},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":206594275},\"end\":45831,\"start\":45218},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":9348728},\"end\":46180,\"start\":45833},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":206775100},\"end\":46472,\"start\":46182},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":1336659},\"end\":46751,\"start\":46474},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":10054272},\"end\":46986,\"start\":46753},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":6385934},\"end\":47443,\"start\":46988},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":5395254},\"end\":47830,\"start\":47445},{\"attributes\":{\"id\":\"b42\"},\"end\":48137,\"start\":47832},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":6405929},\"end\":48505,\"start\":48139},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":1849941},\"end\":48754,\"start\":48507},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":512676},\"end\":49102,\"start\":48756},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":73431591},\"end\":49471,\"start\":49104},{\"attributes\":{\"doi\":\"arXiv:1612.02401\",\"id\":\"b47\"},\"end\":49822,\"start\":49473},{\"attributes\":{\"id\":\"b48\"},\"end\":50122,\"start\":49824},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":2057504},\"end\":50508,\"start\":50124},{\"attributes\":{\"id\":\"b50\"},\"end\":50651,\"start\":50510},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":5458522},\"end\":50983,\"start\":50653},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":1608002},\"end\":51403,\"start\":50985},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":6913648},\"end\":51722,\"start\":51405},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":6002134},\"end\":52026,\"start\":51724},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":261631},\"end\":52415,\"start\":52028}]", "bib_title": "[{\"end\":33984,\"start\":33959},{\"end\":34194,\"start\":34152},{\"end\":34470,\"start\":34432},{\"end\":34915,\"start\":34854},{\"end\":35528,\"start\":35433},{\"end\":36074,\"start\":36001},{\"end\":36436,\"start\":36343},{\"end\":36763,\"start\":36717},{\"end\":37056,\"start\":36989},{\"end\":37369,\"start\":37328},{\"end\":37766,\"start\":37726},{\"end\":38346,\"start\":38273},{\"end\":38671,\"start\":38602},{\"end\":39060,\"start\":38993},{\"end\":39380,\"start\":39322},{\"end\":39845,\"start\":39775},{\"end\":40498,\"start\":40476},{\"end\":40970,\"start\":40950},{\"end\":41204,\"start\":41176},{\"end\":41501,\"start\":41453},{\"end\":41749,\"start\":41676},{\"end\":42119,\"start\":42045},{\"end\":42976,\"start\":42933},{\"end\":43797,\"start\":43731},{\"end\":44227,\"start\":44145},{\"end\":44598,\"start\":44542},{\"end\":45008,\"start\":44962},{\"end\":45320,\"start\":45218},{\"end\":45907,\"start\":45833},{\"end\":46238,\"start\":46182},{\"end\":46519,\"start\":46474},{\"end\":46795,\"start\":46753},{\"end\":47046,\"start\":46988},{\"end\":47494,\"start\":47445},{\"end\":48152,\"start\":48139},{\"end\":48565,\"start\":48507},{\"end\":48824,\"start\":48756},{\"end\":49192,\"start\":49104},{\"end\":50184,\"start\":50124},{\"end\":50742,\"start\":50653},{\"end\":51083,\"start\":50985},{\"end\":51488,\"start\":51405},{\"end\":51757,\"start\":51724},{\"end\":52096,\"start\":52028}]", "bib_author": "[{\"end\":33538,\"start\":33529},{\"end\":33549,\"start\":33538},{\"end\":33559,\"start\":33549},{\"end\":33569,\"start\":33559},{\"end\":33577,\"start\":33569},{\"end\":33586,\"start\":33577},{\"end\":33599,\"start\":33586},{\"end\":33608,\"start\":33599},{\"end\":33616,\"start\":33608},{\"end\":33625,\"start\":33616},{\"end\":33997,\"start\":33986},{\"end\":34009,\"start\":33997},{\"end\":34018,\"start\":34009},{\"end\":34206,\"start\":34196},{\"end\":34217,\"start\":34206},{\"end\":34226,\"start\":34217},{\"end\":34239,\"start\":34226},{\"end\":34482,\"start\":34472},{\"end\":34494,\"start\":34482},{\"end\":34927,\"start\":34917},{\"end\":34936,\"start\":34927},{\"end\":34945,\"start\":34936},{\"end\":34956,\"start\":34945},{\"end\":34969,\"start\":34956},{\"end\":34981,\"start\":34969},{\"end\":34991,\"start\":34981},{\"end\":34999,\"start\":34991},{\"end\":35010,\"start\":34999},{\"end\":35543,\"start\":35530},{\"end\":35555,\"start\":35543},{\"end\":35564,\"start\":35555},{\"end\":36085,\"start\":36076},{\"end\":36096,\"start\":36085},{\"end\":36106,\"start\":36096},{\"end\":36446,\"start\":36438},{\"end\":36779,\"start\":36765},{\"end\":36789,\"start\":36779},{\"end\":36802,\"start\":36789},{\"end\":37067,\"start\":37058},{\"end\":37080,\"start\":37067},{\"end\":37091,\"start\":37080},{\"end\":37102,\"start\":37091},{\"end\":37383,\"start\":37371},{\"end\":37394,\"start\":37383},{\"end\":37403,\"start\":37394},{\"end\":37413,\"start\":37403},{\"end\":37780,\"start\":37768},{\"end\":37791,\"start\":37780},{\"end\":37802,\"start\":37791},{\"end\":37814,\"start\":37802},{\"end\":38041,\"start\":38030},{\"end\":38049,\"start\":38041},{\"end\":38057,\"start\":38049},{\"end\":38356,\"start\":38348},{\"end\":38364,\"start\":38356},{\"end\":38376,\"start\":38364},{\"end\":38384,\"start\":38376},{\"end\":38683,\"start\":38673},{\"end\":38691,\"start\":38683},{\"end\":38702,\"start\":38691},{\"end\":39072,\"start\":39062},{\"end\":39085,\"start\":39072},{\"end\":39098,\"start\":39085},{\"end\":39394,\"start\":39382},{\"end\":39403,\"start\":39394},{\"end\":39414,\"start\":39403},{\"end\":39423,\"start\":39414},{\"end\":39433,\"start\":39423},{\"end\":39854,\"start\":39847},{\"end\":39863,\"start\":39854},{\"end\":39870,\"start\":39863},{\"end\":39884,\"start\":39870},{\"end\":39894,\"start\":39884},{\"end\":40147,\"start\":40138},{\"end\":40158,\"start\":40147},{\"end\":40172,\"start\":40158},{\"end\":40181,\"start\":40172},{\"end\":40193,\"start\":40181},{\"end\":40204,\"start\":40193},{\"end\":40509,\"start\":40500},{\"end\":40520,\"start\":40509},{\"end\":40530,\"start\":40520},{\"end\":40748,\"start\":40739},{\"end\":40759,\"start\":40748},{\"end\":40981,\"start\":40972},{\"end\":40992,\"start\":40981},{\"end\":41219,\"start\":41206},{\"end\":41231,\"start\":41219},{\"end\":41244,\"start\":41231},{\"end\":41516,\"start\":41503},{\"end\":41527,\"start\":41516},{\"end\":41761,\"start\":41751},{\"end\":41768,\"start\":41761},{\"end\":41778,\"start\":41768},{\"end\":42132,\"start\":42121},{\"end\":42142,\"start\":42132},{\"end\":42153,\"start\":42142},{\"end\":42438,\"start\":42427},{\"end\":42453,\"start\":42438},{\"end\":42465,\"start\":42453},{\"end\":42474,\"start\":42465},{\"end\":42485,\"start\":42474},{\"end\":42497,\"start\":42485},{\"end\":42504,\"start\":42497},{\"end\":42790,\"start\":42780},{\"end\":42796,\"start\":42790},{\"end\":42992,\"start\":42978},{\"end\":43005,\"start\":42992},{\"end\":43014,\"start\":43005},{\"end\":43027,\"start\":43014},{\"end\":43530,\"start\":43516},{\"end\":43542,\"start\":43530},{\"end\":43551,\"start\":43542},{\"end\":43808,\"start\":43799},{\"end\":43821,\"start\":43808},{\"end\":43836,\"start\":43821},{\"end\":43847,\"start\":43836},{\"end\":43856,\"start\":43847},{\"end\":44236,\"start\":44229},{\"end\":44244,\"start\":44236},{\"end\":44251,\"start\":44244},{\"end\":44259,\"start\":44251},{\"end\":44607,\"start\":44600},{\"end\":44619,\"start\":44607},{\"end\":44625,\"start\":44619},{\"end\":45021,\"start\":45010},{\"end\":45032,\"start\":45021},{\"end\":45331,\"start\":45322},{\"end\":45338,\"start\":45331},{\"end\":45349,\"start\":45338},{\"end\":45360,\"start\":45349},{\"end\":45371,\"start\":45360},{\"end\":45386,\"start\":45371},{\"end\":45394,\"start\":45386},{\"end\":45918,\"start\":45909},{\"end\":45931,\"start\":45918},{\"end\":45941,\"start\":45931},{\"end\":46253,\"start\":46240},{\"end\":46268,\"start\":46253},{\"end\":46280,\"start\":46268},{\"end\":46535,\"start\":46521},{\"end\":46550,\"start\":46535},{\"end\":46563,\"start\":46550},{\"end\":46807,\"start\":46797},{\"end\":46819,\"start\":46807},{\"end\":46829,\"start\":46819},{\"end\":46840,\"start\":46829},{\"end\":46853,\"start\":46840},{\"end\":47058,\"start\":47048},{\"end\":47068,\"start\":47058},{\"end\":47076,\"start\":47068},{\"end\":47086,\"start\":47076},{\"end\":47509,\"start\":47496},{\"end\":47521,\"start\":47509},{\"end\":47532,\"start\":47521},{\"end\":47545,\"start\":47532},{\"end\":47558,\"start\":47545},{\"end\":47567,\"start\":47558},{\"end\":47948,\"start\":47938},{\"end\":47955,\"start\":47948},{\"end\":47963,\"start\":47955},{\"end\":48165,\"start\":48154},{\"end\":48175,\"start\":48165},{\"end\":48579,\"start\":48567},{\"end\":48841,\"start\":48826},{\"end\":48856,\"start\":48841},{\"end\":48864,\"start\":48856},{\"end\":49206,\"start\":49194},{\"end\":49214,\"start\":49206},{\"end\":49225,\"start\":49214},{\"end\":49234,\"start\":49225},{\"end\":49550,\"start\":49536},{\"end\":49558,\"start\":49550},{\"end\":49567,\"start\":49558},{\"end\":49576,\"start\":49567},{\"end\":49583,\"start\":49576},{\"end\":49598,\"start\":49583},{\"end\":49606,\"start\":49598},{\"end\":49844,\"start\":49824},{\"end\":49853,\"start\":49844},{\"end\":49863,\"start\":49853},{\"end\":49877,\"start\":49863},{\"end\":49892,\"start\":49877},{\"end\":50194,\"start\":50186},{\"end\":50203,\"start\":50194},{\"end\":50566,\"start\":50560},{\"end\":50751,\"start\":50744},{\"end\":50765,\"start\":50751},{\"end\":50776,\"start\":50765},{\"end\":51092,\"start\":51085},{\"end\":51100,\"start\":51092},{\"end\":51109,\"start\":51100},{\"end\":51116,\"start\":51109},{\"end\":51123,\"start\":51116},{\"end\":51501,\"start\":51490},{\"end\":51510,\"start\":51501},{\"end\":51767,\"start\":51759},{\"end\":51779,\"start\":51767},{\"end\":51786,\"start\":51779},{\"end\":51795,\"start\":51786},{\"end\":51806,\"start\":51795},{\"end\":52111,\"start\":52098},{\"end\":52121,\"start\":52111},{\"end\":52136,\"start\":52121},{\"end\":52146,\"start\":52136},{\"end\":52158,\"start\":52146}]", "bib_venue": "[{\"end\":33706,\"start\":33641},{\"end\":34044,\"start\":34018},{\"end\":34262,\"start\":34239},{\"end\":34583,\"start\":34494},{\"end\":35087,\"start\":35010},{\"end\":35653,\"start\":35564},{\"end\":36155,\"start\":36106},{\"end\":36492,\"start\":36446},{\"end\":36833,\"start\":36802},{\"end\":37141,\"start\":37102},{\"end\":37480,\"start\":37413},{\"end\":37853,\"start\":37814},{\"end\":38125,\"start\":38073},{\"end\":38414,\"start\":38384},{\"end\":38748,\"start\":38702},{\"end\":38932,\"start\":38914},{\"end\":39137,\"start\":39098},{\"end\":39500,\"start\":39433},{\"end\":39933,\"start\":39894},{\"end\":40278,\"start\":40220},{\"end\":40544,\"start\":40530},{\"end\":40737,\"start\":40645},{\"end\":41035,\"start\":40992},{\"end\":41293,\"start\":41244},{\"end\":41553,\"start\":41527},{\"end\":41840,\"start\":41778},{\"end\":42179,\"start\":42153},{\"end\":42425,\"start\":42355},{\"end\":42778,\"start\":42736},{\"end\":43076,\"start\":43027},{\"end\":43514,\"start\":43450},{\"end\":43912,\"start\":43856},{\"end\":44321,\"start\":44259},{\"end\":44702,\"start\":44625},{\"end\":45062,\"start\":45032},{\"end\":45471,\"start\":45394},{\"end\":45979,\"start\":45941},{\"end\":46309,\"start\":46280},{\"end\":46589,\"start\":46563},{\"end\":46857,\"start\":46853},{\"end\":47163,\"start\":47086},{\"end\":47616,\"start\":47567},{\"end\":47936,\"start\":47832},{\"end\":48264,\"start\":48175},{\"end\":48605,\"start\":48579},{\"end\":48902,\"start\":48864},{\"end\":49273,\"start\":49234},{\"end\":49534,\"start\":49473},{\"end\":49944,\"start\":49892},{\"end\":50270,\"start\":50203},{\"end\":50558,\"start\":50510},{\"end\":50806,\"start\":50776},{\"end\":51172,\"start\":51123},{\"end\":51546,\"start\":51510},{\"end\":51844,\"start\":51806},{\"end\":52189,\"start\":52158},{\"end\":34659,\"start\":34585},{\"end\":35151,\"start\":35089},{\"end\":35729,\"start\":35655},{\"end\":37534,\"start\":37482},{\"end\":39554,\"start\":39502},{\"end\":40554,\"start\":40546},{\"end\":44766,\"start\":44704},{\"end\":45535,\"start\":45473},{\"end\":47227,\"start\":47165},{\"end\":48340,\"start\":48266},{\"end\":50324,\"start\":50272}]"}}}, "year": 2023, "month": 12, "day": 17}