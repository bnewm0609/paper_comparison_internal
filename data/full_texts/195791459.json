{"id": 195791459, "updated": "2023-09-28 06:59:35.304", "metadata": {"title": "On the Weaknesses of Reinforcement Learning for Neural Machine Translation", "authors": "[{\"first\":\"Leshem\",\"last\":\"Choshen\",\"middle\":[]},{\"first\":\"Lior\",\"last\":\"Fox\",\"middle\":[]},{\"first\":\"Zohar\",\"last\":\"Aizenbud\",\"middle\":[]},{\"first\":\"Omri\",\"last\":\"Abend\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "Reinforcement learning (RL) is frequently used to increase performance in text generation tasks, including machine translation (MT), notably through the use of Minimum Risk Training (MRT) and Generative Adversarial Networks (GAN). However, little is known about what and how these methods learn in the context of MT. We prove that one of the most common RL methods for MT does not optimize the expected reward, as well as show that other methods take an infeasibly long time to converge. In fact, our results suggest that RL practices in MT are likely to improve performance only where the pre-trained parameters are already close to yielding the correct translation. Our \ufb01ndings further suggest that observed gains may be due to effects unrelated to the training signal, but rather from changes in the shape of the distribution curve.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1907.01752", "mag": "2995338136", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iclr/ChoshenFAA20", "doi": null}}, "content": {"source": {"pdf_hash": "c3172ea74996bc7d390a1bebdc53e373db903b1d", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1907.01752v4.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "e0edf8c9f0a56215f985b0f0a2623bace21ce65e", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/c3172ea74996bc7d390a1bebdc53e373db903b1d.txt", "contents": "\nOn the Weaknesses of Reinforcement Learning for Neural Machine Translation\n\n\nLeshem Choshen leshem.choshen@mail.huji.ac.il \nSchool of Computer Science and Engineering\n\n\nLior Fox lior.fox@mail.huji.ac.il \nSchool of Computer Science and Engineering\n\n\nZohar Aizenbud zohar.aizenbud@mail.huji.ac.il \nSchool of Computer Science and Engineering\n\n\nOmri Abend oabend@cs.huji.ac.il \nSchool of Computer Science and Engineering\n\n\nDepartment of Cognitive Sciences\nThe Hebrew University of Jerusalem\n\n\nOn the Weaknesses of Reinforcement Learning for Neural Machine Translation\n\nReinforcement learning (RL) is frequently used to increase performance in text generation tasks, including machine translation (MT), notably through the use of Minimum Risk Training (MRT) and Generative Adversarial Networks (GAN). However, little is known about what and how these methods learn in the context of MT. We prove that one of the most common RL methods for MT does not optimize the expected reward, as well as show that other methods take an infeasibly long time to converge. In fact, our results suggest that RL practices in MT are likely to improve performance only where the pre-trained parameters are already close to yielding the correct translation. Our findings further suggest that observed gains may be due to effects unrelated to the training signal, but rather from changes in the shape of the distribution curve. Lior Fox, Leshem Choshen, and Yonatan Loewenstein. 2018. Dora the explorer: Directed outreaching reinforcement action-selection. ICLR, abs/1804.04012. . 2016. Optimization of image description metrics using policy gradient methods. CoRR, abs/1612.00370, 2.Peter Makarov and Simon Clematide. 2018. Neural transition-based string transduction for limitedresource setting in morphology. In COLING.\n\nIntroduction\n\nReinforcement learning (RL) is an appealing path for advancement in Machine Translation (MT), as it allows training systems to optimize nondifferentiable score functions, common in MT evaluation, as well as its ability to tackle the \"exposure bias\" (Ranzato et al., 2015) in standard training, namely that the model is not exposed during training to incorrectly generated tokens, and is thus unlikely to recover from generating such tokens at test time. These motivations have led to much interest in RL for text generation in general and MT in particular. Various policy gradient methods have been used, notably REINFORCE (Williams, 1992) and variants thereof (e.g., Ranzato et al., 2015;Edunov et al., 2018) and Minimum Risk Training (MRT;e.g., Och, 2003;. Another popular use of RL is for training GANs (Yang et al., 2018;Tevet et al., 2018). See \u00a72. Nevertheless, despite increasing interest and strong results, little is known about what accounts for these performance gains, and the training dynamics involved.\n\nWe present the following contributions. First, our theoretical analysis shows that commonly used approximation methods are theoretically illfounded, and may converge to parameter values that do not minimize the risk, nor are local minima thereof ( \u00a72.3).\n\nSecond, using both naturalistic experiments and carefully constructed simulations, we show that performance gains observed in the literature likely stem not from making target tokens the most probable, but from unrelated effects, such as increasing the peakiness of the output distribution (i.e., the probability mass of the most probable tokens). We do so by comparing a setting where the reward is informative, vs. one where it is constant. In \u00a74 we discuss this peakiness effect (PKE).\n\nThird, we show that promoting the target token to be the mode is likely to take a prohibitively long time. The only case we find, where improvements are likely, is where the target token is among the first 2-3 most probable tokens according to the pretrained model. These findings suggest that REIN-FORCE ( \u00a75) and CMRT ( \u00a76) are likely to improve over the pre-trained model only under the best possible conditions, i.e., where the pre-trained model is \"nearly\" correct.\n\nWe conclude by discussing other RL practices in MT which should be avoided for practical and theoretical reasons, and briefly discuss alternative RL approaches that will allow RL to tackle a larger class of errors in pre-trained models ( \u00a77).\n\n\nRL in Machine Translation\n\n\nSetting and Notation\n\nAn MT system generates tokens y = (y 1 , ..., y n ) from a vocabulary V one token at a time. The probability of generating y i given preceding tokens y <i is given by P \u03b8 (\u00b7|x, y <i ), where x is the source sentence and \u03b8 are the model parameters. For each generated token y, we denote with r(y; y <i , x, y (ref ) ) the score, or reward, for generating y given y <i , x, and the reference sentence y (ref ) . For brevity, we omit parameters where they are fixed within context. For simplicity, we assume r does not depend on following tokens y >i .\n\nWe also assume there is exactly one valid target token, as in practice MT systems are trained against a single reference translation per sentence (Schulz et al., 2018). In practice, either a tokenlevel reward is approximated using Monte-Carlo methods (e.g., Yang et al., 2018), or a sentencelevel (sparse) reward is given at the end of the episode (sentence). The latter is equivalent to a uniform token-level reward.\n\nr is often either the negative log-likelihood, or based on standard MT metrics, e.g., BLEU (Papineni et al., 2002). When applying RL in MT, we seek to maximize the expected reward (denoted with R); i.e., to find\n\u03b8 * = argmax \u03b8 R(\u03b8) = argmax \u03b8 E y\u223cP \u03b8 [r(y)]\n(1) 2.2 REINFORCE For a given source sentence, and partially generated sentence y <i , REINFORCE (Williams, 1992) samples k tokens (k is a hyperparameter) S = y (1) , ..., y (k) from P \u03b8 and updates \u03b8 according to this rule:\n\u2206\u03b8 \u221d 1 k k i=1 r(y i )\u2207 log(P \u03b8 (y i ))(2)\nThe right-hand side of Eq. (2) is an unbiased estimator of the gradient of the objective function, i.e., E [\u2206\u03b8] \u221d \u2207 \u03b8 R (\u03b8). Therefore, REINFORCE is performing a form of stochastic gradient ascent on R, and has similar formal guarantees. From here follows that if R is constant with respect to \u03b8, then the expected \u2206\u03b8 prescribed by REINFORCE is zero. We note that r may be shifted by a constant term (called a \"baseline\"), without affecting the optimal value for \u03b8.\n\nREINFORCE is used by a variety of works in MT, text generation, and image-to-text tasks Wu et al., 2018;Rennie et al., 2017;Shetty et al., 2017;Hendricks et al., 2016) -in isolation, or as a part of training (Ranzato et al., 2015). Lately, an especially prominent use for REINFORCE is adversarial training with discrete data, where another network predicts the reward (GAN). For some recent work on RL for NMT, see (Zhang et al., 2016;Li et al., 2017;Wu et al., 2017;Yu et al., 2017;Yang et al., 2018).\n\n\nMinimum Risk Training\n\nThe term Minimum Risk Training (MRT) is used ambiguously in MT to refer either to the application of REINFORCE to minimizing the risk (equivalently, to maximizing the expected reward, the negative loss), or more commonly to a somewhat different estimation method, which we term Contrastive MRT (CMRT) and turn now to analyzing. CMRT was proposed by Och (2003), adapted to NMT by , and often used since (Ayana et al., 2016;Neubig, 2016;Edunov et al., 2018;Makarov and Clematide, 2018;Neubig et al., 2018).\n\nThe method works as follows: at each iteration, sample k tokens S = {y 1 , . . . , y k } from P \u03b8 , and update \u03b8 according to the gradient of\nR(\u03b8, S) = k i=1 Q \u03b8,S (y i )r(y i ) = E y\u223cQ r(y)\nwhere Q \u03b8,S (y i ) = P (y i ) \u03b1 y j \u2208S P (y j ) \u03b1 Commonly (but not universally), deduplication is performed, so R sums over a set of unique values (Sennrich et al., 2017). This changes little in our empirical results and theoretical analysis.\n\nDespite the resemblance in definitions of R (Eq. (1)) and R (indeed, R is sometimes presented as an approximation of R), they differ in two important aspects. First, Q's support is S, so increasing Q(y i ) for some y i necessarily comes at the expense of Q(y) for some y \u2208 S. In contrast, increasing P (y i ), as in REINFORCE, may come at the expense of P (y) for any y \u2208 V . Second, \u03b1 is a smoothness parameter: the closer \u03b1 is to 0, the closer Q is to be uniform.\n\nWe show in Appendix A that despite its name, CMRT does not optimize R, nor does it optimize E[ R]. That is, it may well converge to values that are not local maxima of R, making it theoretically ill-founded. 1 However, given that CMRT is often used in practice, the strong results it yielded and the absence of theory for explaining it, we discuss it here. Given a sample S, the gradient of R is given by\n\u2207 R =\u03b1 k i=1 Q(y i ) \u00b7 r(y i ) \u00b7 \u2207 log P (y i ) \u2212 E Q [r]\u2207 log Z(S) (3) where Z(S) = i P (y i ) \u03b1 . See Appendix B.\nComparing Equations (2) and (3), the differences between REINFORCE and CMRT are reflected again. First, \u2207 R has an additional term, proportional to \u2207 log Z(S), which yields the contrastive effect. This contrast may improve the rate of convergence since it counters the decrease of probability mass for non-sampled tokens.\n\nSecond, for a given S, the relative weighting of the gradients \u2207 log P (y i ) is proportional to r(y i )Q(y i ), or equivalently to r(y i )P (y i ) \u03b1 . CMRT with deduplication sums over distinct values in S (Eq. (3)), while REINFORCE sums over all values. This means that the relative weight of the unique value y i is r(y i )|{y i \u2208S}| k in REINFORCE. For \u03b1 = 1 the expected value of these relative weights is the same, and so for \u03b1 < 1 (as is commonly used), more weight is given to improbable tokens, which could also have a positive effect on the convergence rate. 2 However, if \u03b1 is too close to 0, \u2207 R vanishes. This tradeoff explains the importance of tuning \u03b1 reported in the literature. In \u00a76 we present simulations with CMRT, showing very similar trends as presented by REINFORCE.\n\n\nMotivating Discussion\n\nImplementing a stochastic gradient ascent, REIN-FORCE is guaranteed to converge to a stationary point of R under broad conditions. However, not much is known about its convergence rate under the prevailing conditions in NMT.\n\nWe begin with a qualitative, motivating analysis of these questions. As work on language generation empirically showed, RNNs quickly learn to output very peaky distributions (Press et al., 2017). This tendency is advantageous for generating fluent sentences with high probability, but may also entail slower convergence rates when using RL to fine-tune the model, because RL methods used in text generation sample from the (pretrained) policy distribution, which means they mostly sample what the pretrained model deems to be likely. Since the pretrained model (or policy) is peaky, exploration of other potentially more rewarding tokens will be limited, hampering convergence.\n\nIntuitively, REINFORCE increases the probabilities of successful (positively rewarding) observations, weighing updates by how rewarding they were. When sampling a handful of tokens in each context (source sentence x and generated prefix y <i ), and where the number of epochs is not large, it is unlikely that more than a few unique tokens will be sampled from P \u03b8 (\u00b7|x, y <i ). (In practice, k is typically between 1 and 20, and the number of epochs between 1 and 100.) It is thus unlikely that anything but the initially most probable candidates will be observed. Consequently, REIN-FORCE initially raises their probabilities, even if more rewarding tokens can be found down the list.\n\nWe thus hypothesize the peakiness of the distribution, i.e., the probability mass allocated to the most probable tokens, will increase, at least in the first phase. We call this the peakiness-effect (PKE), and show it occurs both in simulations ( \u00a74.1) and in full-scale NMT experiments ( \u00a74.2).\n\nWith more iterations, the most-rewarding tokens will be eventually sampled, and gradually gain probability mass. This discussion suggests that training will be extremely sample-inefficient. We assess the rate of convergence empirically in \u00a75, finding this to be indeed the case.\n\n\nThe Peakiness Effect\n\nWe turn to demonstrate that the initially most probable tokens will initially gain probability mass, even if they are not the most rewarding, yielding a PKE. Caccia et al. (2018) recently observed in the context of language modeling using GANs that performance gains similar to those GAN yield can be achieved by decreasing the temperature for the prediction softmax (i.e., making it peakier). However, they proposed no account as to what causes this effect. Our findings propose an underlying mechanism leading to this trend. We return to this point in \u00a77. Furthermore, given their findings, it is reasonable to assume that our results are relevant for RL use in other generation tasks, whose output space too is discrete, high-dimensional and concentrated.\n\n\nControlled Simulations\n\nWe experiment with a 1-layer softmax model, that predicts a token i \u2208 V with probability e \u03b8 i j e \u03b8 j . \u03b8 = {\u03b8 j } j\u2208V are the model's parameters. This model simulates the top of any MT decoder that ends with a softmax layer, as essentially all NMT decoders do. To make experiments realistic, we use similar parameters as those reported in the influential Transformer NMT system (Vaswani et al., 2017). Specifically, the size of V (distinct BPE tokens) is 30715, and the initial values for \u03b8 were sampled from 1000 sets of logits taken from decoding the standard newstest2013 development set, using a pretrained Transformer model. The model was pretrained on WMT2015 training data (Bojar et al., 2015). Hyperparameters are reported in Appendix C. We define one of the tokens in V to be the target token and denote it with y best . We experiment with two reward functions:\n\n1. Simulated Reward: r(y) = 2 for y = y best , r(y) = 1 if y is one of the 10 initially highest scoring tokens, and r(y) = 0 otherwise. This simulates a condition where the pretrained model is of decent but sub-optimal quality. r here is at the scale of popular rewards used in MT, such as GAN-based rewards or BLEU (which are between 0 and 1).\n\n2. Constant Reward: r is constantly equal to 1, for all tokens. This setting is aimed to confirm that PKE is not a result of the signal carried by the reward.\n\nExperiments with the first setting were run 100 times, each time for 50K steps, updating \u03b8 after each step. With the second setting, it is sufficient to take a single step at a time, as the expected update after each step is zero, and so any PKE seen in a single step is only accentuated in the next. It is, therefore, more telling to run more repetitions rather than more steps per initialization. We, therefore, sample 10000 pretrained distributions, and perform a single REINFORCE step.\n\nAs RL training in NMT lasts about 30 epochs before stopping, samples about 100K tokens per epoch, and as the network already predicts y best in about two thirds of the contexts, 3 we estimate the number of steps used in practice to be in the order of magnitude of 1M. For visual clarity, we present figures for 50K-100K steps. However, full experiments (with 1M steps) exhibit similar trends: where REINFORCE was not close to converging after 50K steps, the same was true after 1M steps.\n\nWe evaluate the peakiness of a distribution in terms of the probability of the most probable token (the mode), the total probability of the ten most probable tokens, and the entropy of the distribution (lower entropy indicates more peakiness).\n\nResults. The distributions become peakier in terms of all three measures: on average, the mode's probability and the 10 most probable tokens increases, and the entropy decreases. Figure  1a presents the histogram of the difference in the probability of the 10 most probable tokens in the Constant Reward setting, after a single step. Figure 1b depicts similar statistics for the mode. The average entropy in the pretrained model is 2.9 is reduced to 2.85 after one REINFORCE step.\n\nSimulated Reward setting shows similar trends. For example, entropy decreases from 3 to about 0.001 in 100K steps. This extreme decrease suggests it is effectively a deterministic policy. PKE is achieved in a few hundred steps, usually before other effects become prominent (see Figure 2), and is stronger than for Constant Reward.\n\n\nNMT Experiments\n\nWe turn to analyzing a real-world application of REINFORCE to NMT. Important differences between this and the previous simulations are: (1) it is rare in NMT for REINFORCE to sample from the same conditional distribution more than a handful of times, given the number of source sentences x and sentence prefixes y <i (contexts); and (2) in NMT P \u03b8 (\u00b7|x, y <i ) shares parameters between contexts, which means that updating P \u03b8 for one context may influence P \u03b8 for another.\n\nWe follow the same pretraining as in \u00a74.1. We then follow Yang et al. (2018) in defining the reward function based on the expected BLEU score. Expected BLEU is computed by sampling suffixes for the sentence, and averaging the BLEU score of the sampled sentences against the reference.\n\nWe use early stopping with a patience of 10 epochs, where each epoch consists of 5000 sentences sampled from the WMT2015 (Bojar et al., 2015) German-English training data. We use k = 1. We retuned the learning-rate, and positive baseline settings against the development set. Other hyper-parameters were an exact replication of the experiments reported in (Yang et al., 2018).\n\n\nResults\n\n. Results indicate an increase in the peakiness of the conditional distributions. Our results are based on a sample of 1000 contexts from the pretrained model, and another (independent) sample from the reinforced model.\n\nIndeed, the modes of the conditional distributions tend to increase. Figure 3 presents the distribution of the modes' probability in the reinforced conditional distributions compared with the pretrained model, showing a shift of probability mass towards higher probabilities for the mode, following RL. Another indication of the increased peakiness is the decrease in the average entropy of P \u03b8 , which was reduced from 3.45 in the pretrained model to an average of 2.82 following RL. This more modest reduction in entropy (compared to \u00a74.1) might also suggest that the procedure did not converge to the optimal value for \u03b8, as then we would have expected the entropy to substantially drop if not to 0 (overfit), then to the average entropy of valid next tokens (given the source and a prefix of the sentence).\n\n\nPerformance following REINFORCE\n\nWe now turn to assessing under what conditions it is likely that REINFORCE will lead to an improvement in the performance of an NMT system. As in the previous section, we use both controlled simulations and NMT experiments.\n\n\nControlled Simulations\n\nWe use the same model and experimental setup described in Section 4.1, this time only exploring the Simulated Reward setting, as a Constant Reward is not expected to converge to any meaningful \u03b8.\n\nResults are averaged over 100 conditional distributions sampled from the pretrained model. Caution should be exercised when determining the learning rate (LR). Common LRs used in the NMT literature are of the scale of 10 \u22124 . However, in our simulations, no LR smaller than 0.1 yielded any improvement in R. We thus set the LR to be 0.1. We note that in our simulations, a higher learning rate means faster convergence as our reward is noise-free: it is always highest for the best option. In practice, increasing the learning rate may deteriorate results, as it may cause the system to overfit to the sampled instances. Indeed, when increasing the learning rate in our NMT experiments (see below) by an order of magnitude, early stopping caused the RL procedure to stop without any parameter updates. Figure 2 shows how P \u03b8 changes over the first 50K steps of REINFORCE (probabilities are averaged over 100 repetitions), for a case where y best was initially the second, third and fourth most probable. Although these are the easiest settings for REINFORCE, and despite the high learning rate, it fails to make y best the mode of the distribution within 100K steps, unless y best was initially the second most probable. In cases where y best is initially of a lower rank than four, it is hard to see any increase in its probability, even after 1M steps.\n\n\nNMT Experiments\n\nWe trained an NMT system, using the same procedure as in Section 4.2, and report BLEU scores over the news2014 test set. After training with an expected BLEU reward, we indeed see a minor improvement which is consistent between trials and pretrained models. While the pretrain BLEU score is 30.31, the reinforced one is 30.73.\n\nAnalyzing what words were influenced by the RL procedure, we begin by computing the cumulative probability of the target token y best to be Figure 4: Cumulative percentage of contexts where the pretrained model ranks y best in rank x or below and where it does not rank y best first (x = 0). In about half the cases it is ranked fourth or below. ranked lower than a given rank according to the pretrained model. Results (Figure 4) show that in about half of the cases, y best is not among the top three choices of the pretrained model, and we thus expect it not to gain substantial probability following REINFORCE, according to our simulations.\n\nWe next turn to compare the ranks the reinforced model assigns to the target tokens, and their ranks according to the pretrained model. Figure 5 presents the difference in the probability that y best is ranked at a given rank following RL and the probability it is ranked there initially. Results indicate that indeed more target tokens are ranked first, and less second, but little consistent shift of probability mass occurs otherwise across the ten first ranks. It is possible that RL has managed to push y best in some cases between very low ranks (<1000) to medium-low ranks (between 10 and 1000). However, token probabilities in these ranks are so low that it is unlikely to affect the system outputs in any way. This fits well with the results of our simulations that predicted that only the initially top-ranked tokens are likely to change.\n\nIn an attempt to explain the improved BLEU score following RL with PKE, we repeat the NMT experiment this time using a constant reward of 1. Our results present a nearly identical improvement in BLEU, achieving 30.72, and a similar pattern in the change of the target tokens' ranks (see Appendix 8). Therefore, there is room to suspect that even in cases where RL yields an improvement in BLEU, it may partially result from reward-independent factors, such as PKE. 4 Figure 5: Difference between the ranks of y best in the reinforced and the pretrained model. Each column x corresponds to the difference in the probability that y best is ranked in rank x in the reinforced model, and the same probability in the pretrained model.\n\n\nExperiments with Contrastive MRT\n\nIn \u00a72.3 we showed that CMRT does not, in fact, maximize R, and so does not enjoy the same theoretical guarantees as REINFORCE and similar policy gradient methods. However, it is still the RL procedure of choice in much recent work on NMT. We therefore repeat the simulations described in \u00a74 and \u00a75, assessing the performance of MRT in these conditions. We experiment with \u03b1 = 0.005 and k = 20, common settings in the literature, and average over 100 trials. Figure 6 shows how the distribution P \u03b8 changes over the course of 50K update steps to \u03b8, where y best is taken to be the second and third initially most probable token (Simulated Reward setting). Results are similar in trends to those obtained with REINFORCE: MRT succeeds in pushing y best to be the highest ranked token if it was initially second, but struggles where it was initially ranked third or below. We only observe a small PKE in MRT. This is probably due to the contrastive effect, which means that tokens that were not sampled do not lose probability mass.\n\nAll graphs we present here allow sampling the same token more than once in each batch (i.e., S is a sample with replacements). Simulations with deduplication show similar results.\n\n\nDiscussion\n\nIn this paper, we showed that the type of distributions used in NMT entail that promoting the which got BLEU scores of 30.73-30.84. This improvement is very stable across metrics, trials and pretrained models. target token to be the mode is likely to take a prohibitively long times for existing RL practices, except under the best conditions (where the pretrained model is \"nearly\" correct). This leads us to conclude that observed improvements from using RL for NMT are likely due either to fine-tuning the most probable tokens in the pretrained model (an effect which may be more easily achieved using reranking methods, and uses but little of the power of RL methods), or to effects unrelated to the signal carried by the reward, such as PKE. Another contribution of this paper is in showing that CMRT does not optimize the expected reward and is thus theoretically unmotivated.\n\nA number of reasons lead us to believe that in our NMT experiments, improvements are not due to the reward function, but to artefacts such as PKE. First, reducing a constant baseline from r, so as to make the expected reward zero, disallows learning. This is surprising, as REINFORCE, generally and in our simulations, converges faster where the reward is centered around zero, and so the fact that this procedure here disallows learning hints that other factors are in play. As PKE can be observed even where the reward is constant (if the expected reward is positive; see \u00a74.1), this suggests PKE may play a role here. Second, we observe more peakiness in the reinforced model and in such cases, we expect improvements in BLEU (Caccia et al., 2018). Third, we achieve similar results with a constant reward in our NMT experiments ( \u00a75.2). Fourth, our controlled simulations show that asymptotic convergence is not reached in any but the easiest conditions ( \u00a75.1).\n\nOur analysis further suggests that gradient clipping, sometimes used in NMT (Zhang et al., 2016), is expected to hinder convergence further. It should be avoided when using REINFORCE as it violates REINFORCE's assumptions.\n\nThe per-token sampling as done in our experiments is more exploratory than beam search (Wu et al., 2018), reducing PKE. Furthermore, the latter does not sample from the behavior policy, but does not properly account for being off-policy in the parameter updates.\n\nAdding the reference to the sample S, which some implementations allow (Sennrich et al., 2017) may help reduce the problems of never sampling the target tokens. However, as Edunov et al. (2018) point out, this practice may lower results, as it may destabilize training by leading the model Figure 6: The probability of different tokens following CMRT, in the controlled simulations in the Simulated Reward setting. The left/right figures correspond to simulations where the target token (y best ) was initially the second/third most probable token. The green line corresponds to the target token, yellow lines to medium-reward tokens and red lines to tokens with r(y) = 0.\n\nto improve over outputs it cannot generalize over, as they are very different from anything the model assigns a high probability to, at the cost of other outputs.\n\n\nConclusion\n\nThe standard MT scenario poses several uncommon challenges for RL. First, the action space in MT problems is a high-dimensional discrete space (generally in the size of the vocabulary of the target language or the product thereof for sentences). This contrasts with the more common scenario studied by contemporary RL methods, which focuses mostly on much smaller discrete action spaces (e.g., video games (Mnih et al., 2015(Mnih et al., , 2016), or continuous action spaces of relatively low dimensions (e.g., simulation of robotic control tasks (Lillicrap et al., 2015)). Second, reward for MT is naturally very sparse -almost all possible sentences are \"wrong\" (hence, not rewarding) in a given context. Finally, it is common in MT to use RL for tuning a pretrained model. Using a pretrained model ameliorates the last problem. But then, these pretrained models are in general quite peaky, and because training is done on-policy -that is, actions are being sampled from the same model being optimized -exploration is inherently limited.\n\nHere we argued that, taken together, these challenges result in significant weaknesses for current RL practices for NMT, that may ultimately prevent them from being truly useful. At least some of these challenges have been widely studied in the RL literature, with numerous techniques de-veloped to address them, but were not yet adopted in NLP. We turn to discuss some of them.\n\nOff-policy methods, in which observations are sampled from a different policy than the one being currently optimized, are prominent in RL (Watkins and Dayan, 1992;Sutton and Barto, 1998), and were also studied in the context of policy gradient methods (Degris et al., 2012;Silver et al., 2014). In principle, such methods allow learning from a more \"exploratory\" policy. Moreover, a key motivation for using \u03b1 in CMRT is smoothing; offpolicy sampling allows smoothing while keeping convergence guarantees.\n\nIn its basic form, exploration in REINFORCE relies on stochasticity in the action-selection (in MT, this is due to sampling). More sophisticated exploration methods have been extensively studied, for example using measures for the exploratory usefulness of states or actions (Fox et al., 2018), or relying on parameter-space noise rather than actionspace noise (Plappert et al., 2017).\n\nFor MT, an additional challenge is that even effective exploration (sampling diverse sets of observations), may not be enough, since the stateaction space is too large to be effectively covered, with almost all sentences being not rewarding. Recently, diversity-based and multi-goal methods for RL were proposed to tackle similar challenges (Andrychowicz et al., 2017;Ghosh et al., 2018;Eysenbach et al., 2019). We believe the adoption of such methods is a promising path forward for the application of RL in NLP.\n\n\nA Contrastive MRT does not Maximize the Expected Reward\n\nWe hereby detail a simple example where following the Contrastive MRT method (see \u00a72.3) does not converge to the parameter value that maximizes R.\n\nLet \u03b8 be a real number in [0, 0.5], and let P \u03b8 be a family of distributions over three values a, b, c such that:\nP \u03b8 (x) = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 \u03b8 x = a 2\u03b8 2 x = b 1 \u2212 \u03b8 \u2212 2\u03b8 2 x = c\nLet r(a) = 1, r(b) = 0, r(c) = 0.5. The expected reward as a function of \u03b8 is:\nR(\u03b8) = \u03b8 + 0.5(1 \u2212 \u03b8 \u2212 2\u03b8 2 )\nR(\u03b8) is uniquely maximized by \u03b8 * = 0.25. Table 1 details the possible samples of size k = 2, their probabilities, the corresponding R and its gradient. Standard numerical methods show that E[\u2207 R] over possible samples S is positive for \u03b8 \u2208 (0, \u03b3) and negative for \u03b8 \u2208 (\u03b3, 0.5], where \u03b3 \u2248 0.33. This means that for any initialization of \u03b8 \u2208 (0, 0.5], Contrastive MRT will converge to \u03b3 if the learning rate is sufficiently small. For \u03b8 = 0, R \u2261 0.5, and there will be no gradient updates, so the method will converge to \u03b8 = 0. Neither of these values maximizes R(\u03b8).\n\nWe further note that resorting to maximizing E[ R] instead, does not maximize R(\u03b8) either. Indeed, plotting E[ R] as a function of \u03b8 for this example, yields a maximum at \u03b8 \u2248 0.32.\nS P (S) R \u2207 R {a, b} 4\u03b8 3 1 1+2\u03b8 \u22122 (1+2\u03b8) 2 {a, c} 2\u03b8(1-\u03b8-\u03b8 2 ) 0.5 + \u03b8 2\u22124\u03b8 2 1+2\u03b8 2 2(1\u22122\u03b8 2 ) {b, c} 4\u03b8 2 (1-\u03b8-2\u03b8 2 ) 1\u2212\u03b8\u22122\u03b8 2 2\u22122\u03b8 \u03b8 2 \u22123\u03b8 2(1\u2212\u03b8) 2 a, a \u03b8 2 1 0 b, b 4\u03b8 4 0 0 c, c\n(1-\u03b8-2\u03b8 2 ) 2 0.5 0  \n\n\nC NMT Implementation Details\n\nTrue casing and tokenization were used (Koehn et al., 2007), including escaping html symbols and \"-\" that represents a compound was changed into a separate token of =. Some preprocessing used before us converted the latter to ##AT##-##AT## but standard tokenizers in use process that into 11 different tokens, which over-represents the significance of that character when BLEU is calculated. BPE (Sennrich et al., 2016) extracted 30715 tokens. For the MT experiments we used 6 layers in the encoder and the decoder. The size of the embeddings was 512. Gradient clipping was used with size of 5 for pre-training (see Discussion on why not to use it in training). We did not use attention dropout, but 0.1 residual dropout rate was used. In pretraining and training sentences of more than 50 tokens were discarded. Pretraining and training were considered finished when BLEU did not increase in the development set for 10 consecutive evaluations, and evaluation was done every 1000 and 5000 for batches of size 100 and 256 for pretraining and training respectively. Learning rate used for rmsprop (Tieleman and Hinton, 2012) was 0.01 in pretraining and for adam (Kingma and Ba, 2015) with decay was 0.005 for training. 4000 learning rate warm up steps were used. Pretraining took about 7 days with 4 GPUs, afterwards, training took roughly the same time.\n\nMonte Carlo used 20 sentence rolls per word.  Figure 7: The probability of different tokens following REINFORCE, in the controlled simulations in the Constant Reward setting. The left/center/right figures correspond to simulations where the target token (y best ) was initially the second/third/fourth most probable token. The green line corresponds to the target token, yellow lines to medium-reward tokens and red lines to tokens with r(y) = 0.\n\n\nD Detailed Results for Constant Reward Setting\n\nWe present graphs for the constant reward setting in Figures 8 and 7. Trends are similar to the ones obtained for the Simulated Reward setting. Figure 8: Difference between the ranks of y best in the reinforced with constant reward and the pretrained model. Each column x corresponds to the difference in the probability that y best is ranked in rank x in the reinforced model, and the same probability in the pretrained model.\n\nFigure 1 :Figure 2 :\n12A histogram of the update size (x-axis) to the total probability of the 10 most probable tokens (left) or the most probable token (right) in the Constant Reward setting. An update is overwhelmingly more probable to increase this probability than to decrease it. Token probabilities through REINFORCE training, in the controlled simulations in the Simulated Reward setting. The left/center/right figures correspond to simulations where the target token (y best ) was initially the second/third/fourth most probable token. The green line corresponds to the target token, yellow lines to mediumreward tokens and red lines to no-reward tokens.\n\nFigure 3 :\n3The cumulative distribution of the probability of the most likely token in the NMT experiments. The green distribution corresponds to the pretrained model, and the blue corresponds to the reinforced model. The y-axis is the proportion of conditional probabilities with a mode of value \u2264 x (the x-axis).\n\n\n\u2207P (y) \u00b7 \u03b1P (y) \u03b1\u22121 \u00b7 Z(S) \u2212 \u2207Z(S) \u00b7 P (y) yi)Q(yi) \u03b1\u2207 log P (yi) \u2212 \u2207 log Z(S) yi)Q(yi)\u2207 log P (yi) \u2212 EQ[r]\u2207 log Z(S)\n\nTable 1 :\n1The gradients of R for each possible sample S. The batch size is k = 2. Rows correspond to different sampled outcomes. \u2207 R is the gradient of R given the corresponding value for S.\nSakaguchi et al. (2017) discuss the relation between CMRT and REINFORCE, claiming that CMRT is a variant of REINFORCE. Appendix A shows that CMRT does not in fact optimize the same objective.\nNot performing deduplication results in assigning higher relative weight to high-probability tokens, which may have an adverse effect on convergence rate. For an implementation without deduplication, see THUMT.\nBased on our NMT experiments, which we assume to be representative of the error rate of other NMT systems.\nWe tried several other reward functions as well, all of\n\nOpenAI Pieter Abbeel, and Wojciech Zaremba. 2017. Hindsight experience replay. Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob Mc-Grew, Josh Tobin, Advances in Neural Information Processing Systems. Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob Mc- Grew, Josh Tobin, OpenAI Pieter Abbeel, and Woj- ciech Zaremba. 2017. Hindsight experience replay. In Advances in Neural Information Processing Sys- tems, pages 5048-5058.\n\nNeural headline generation with minimum risk training. Zhiyuan Shiqi Shen Ayana, Maosong Liu, Sun, arXiv:1604.01904arXiv preprintShiqi Shen Ayana, Zhiyuan Liu, and Maosong Sun. 2016. Neural headline generation with minimum risk training. arXiv preprint arXiv:1604.01904.\n\nOndrej Bojar, Rajen Chatterjee, Christian Federmann, Barry Haddow, Matthias Huck, Chris Hokamp, Philipp Koehn, Varvara Logacheva, Christof Monz, Matteo Negri, Matt Post, Carolina Scarton, Lucia Specia, Marco Turchi, Findings of the 2015 workshop on statistical machine translation. WMT@EMNLPOndrej Bojar, Rajen Chatterjee, Christian Federmann, Barry Haddow, Matthias Huck, Chris Hokamp, Philipp Koehn, Varvara Logacheva, Christof Monz, Matteo Negri, Matt Post, Carolina Scarton, Lucia Specia, and Marco Turchi. 2015. Findings of the 2015 workshop on statistical machine translation. In WMT@EMNLP.\n\nMassimo Caccia, Lucas Caccia, William Fedus, Hugo Larochelle, Joelle Pineau, Laurent Charlin, arXiv:1811.02549Language gans falling short. arXiv preprintMassimo Caccia, Lucas Caccia, William Fedus, Hugo Larochelle, Joelle Pineau, and Laurent Charlin. 2018. Language gans falling short. arXiv preprint arXiv:1811.02549.\n\nThomas Degris, Martha White, Richard S Sutton, arXiv:1205.4839Off-policy actor-critic. arXiv preprintThomas Degris, Martha White, and Richard S Sut- ton. 2012. Off-policy actor-critic. arXiv preprint arXiv:1205.4839.\n\nClassical structured prediction losses for sequence to sequence learning. Sergey Edunov, Myle Ott, Michael Auli, David Grangier, Marc&apos;aurelio Ranzato, 10.18653/v1/N18-1033Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies1Long Papers. Association for Computational LinguisticsSergey Edunov, Myle Ott, Michael Auli, David Grang- ier, and Marc'Aurelio Ranzato. 2018. Classical structured prediction losses for sequence to se- quence learning. In Proceedings of the 2018 Con- ference of the North American Chapter of the Asso- ciation for Computational Linguistics: Human Lan- guage Technologies, Volume 1 (Long Papers), pages 355-364. Association for Computational Linguis- tics.\n\nDiversity is all you need: Learning skills without a reward function. Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, Sergey Levine, International Conference on Learning Representations. the 40th annual meeting on association for computational linguistics. Association for Computational LinguisticsBenjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. 2019. Diversity is all you need: Learning skills without a reward function. In Inter- national Conference on Learning Representations. the 40th annual meeting on association for compu- tational linguistics, pages 311-318. Association for Computational Linguistics.\n\nMatthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Y Richard, Xi Chen, Chen, arXiv:1706.01905Tamim Asfour, Pieter Abbeel, and Marcin Andrychowicz. 2017. Parameter space noise for exploration. arXiv preprintMatthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Richard Y Chen, Xi Chen, Tamim Asfour, Pieter Abbeel, and Marcin Andrychowicz. 2017. Parameter space noise for exploration. arXiv preprint arXiv:1706.01905.\n\nLanguage generation with recurrent generative adversarial networks without pre-training. O Press, A Bar, B Bogin, J Berant, L Wolf, Fist Workshop on Learning to Generate Natural Language@ICML. O. Press, A. Bar, B. Bogin, J. Berant, and L. Wolf. 2017. Language generation with recurrent genera- tive adversarial networks without pre-training. In Fist Workshop on Learning to Generate Natural Language@ICML.\n\nAurelio Marc, Sumit Ranzato, Michael Chopra, Wojciech Auli, Zaremba, arXiv:1511.06732Sequence level training with recurrent neural networks. arXiv preprintMarc'Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. 2015. Sequence level train- ing with recurrent neural networks. arXiv preprint arXiv:1511.06732.\n\nSelf-critical sequence training for image captioning. J Steven, Etienne Rennie, Youssef Marcheret, Jerret Mroueh, Vaibhava Ross, Goel, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionSteven J Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. 2017. Self-critical sequence training for image captioning. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7008-7024.\n\nKeisuke Sakaguchi, Matt Post, Benjamin Van Durme, arXiv:1707.00299Grammatical error correction with neural reinforcement learning. arXiv preprintKeisuke Sakaguchi, Matt Post, and Benjamin Van Durme. 2017. Grammatical error correction with neural reinforcement learning. arXiv preprint arXiv:1707.00299.\n\nA stochastic decoder for neural machine translation. Philip Schulz, Wilker Aziz, Trevor Cohn, ACL. Philip Schulz, Wilker Aziz, and Trevor Cohn. 2018. A stochastic decoder for neural machine translation. In ACL.\n\nNematus: a toolkit for neural machine translation. Rico Sennrich, Orhan Firat, Kyunghyun Cho, Alexandra Birch, Barry Haddow, Julian Hitschler, Marcin Junczys-Dowmunt, Samuel L\u00e4ubli, Antonio Valerio Miceli, Jozef Barone, Maria Mokry, Nadejde, EACL. Rico Sennrich, Orhan Firat, Kyunghyun Cho, Alexan- dra Birch, Barry Haddow, Julian Hitschler, Marcin Junczys-Dowmunt, Samuel L\u00e4ubli, Antonio Vale- rio Miceli Barone, Jozef Mokry, and Maria Nadejde. 2017. Nematus: a toolkit for neural machine trans- lation. In EACL.\n\nNeural machine translation of rare words with subword units. Rico Sennrich, Barry Haddow, Alexandra Birch, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational Linguistics1Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Lin- guistics (Volume 1: Long Papers), volume 1, pages 1715-1725.\n\nMinimum risk training for neural machine translation. Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua Wu, Maosong Sun, Yang Liu, 10.18653/v1/P16-1159Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational LinguisticsLong Papers1Association for Computational LinguisticsShiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua Wu, Maosong Sun, and Yang Liu. 2016. Minimum risk training for neural machine translation. In Pro- ceedings of the 54th Annual Meeting of the Associa- tion for Computational Linguistics (Volume 1: Long Papers), pages 1683-1692. Association for Compu- tational Linguistics.\n\nOptimizing non-decomposable evaluation metrics for neural machine translation. Shiqi Shen, Yang Liu, Maosong Sun, Journal of Computer Science and Technology. 32Shiqi Shen, Yang Liu, and Maosong Sun. 2017. Op- timizing non-decomposable evaluation metrics for neural machine translation. Journal of Computer Science and Technology, 32:796-804.\n\nSpeaking the same language: Matching machine to human captions by adversarial training. Rakshith Shetty, Marcus Rohrbach, Lisa Anne Hendricks, Mario Fritz, Bernt Schiele, 2017 IEEE International Conference on Computer Vision (ICCV). IEEERakshith Shetty, Marcus Rohrbach, Lisa Anne Hen- dricks, Mario Fritz, and Bernt Schiele. 2017. Speak- ing the same language: Matching machine to human captions by adversarial training. In 2017 IEEE In- ternational Conference on Computer Vision (ICCV), pages 4155-4164. IEEE.\n\nDeterministic policy gradient algorithms. David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, Martin Riedmiller, ICML. David Silver, Guy Lever, Nicolas Heess, Thomas De- gris, Daan Wierstra, and Martin Riedmiller. 2014. Deterministic policy gradient algorithms. In ICML.\n\nReinforcement learning: An introduction. S Richard, Andrew G Sutton, Barto, MIT pressRichard S Sutton and Andrew G Barto. 1998. Rein- forcement learning: An introduction. MIT press.\n\nG Tevet, G Habib, V Shwartz, J Berant, arXiv:1810.12686Evaluating text GANs as language models. arXiv preprintG. Tevet, G. Habib, V. Shwartz, and J. Berant. 2018. Evaluating text GANs as language models. arXiv preprint arXiv:1810.12686.\n\nLecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning. Tijmen Tieleman, Geoffrey Hinton, 4Tijmen Tieleman and Geoffrey Hinton. 2012. Lecture 6.5-rmsprop: Divide the gradient by a running av- erage of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26-31.\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Pro- cessing Systems, pages 5998-6008.\n\nQlearning. Jch Christopher, Peter Watkins, Dayan, Machine learning. 83-4Christopher JCH Watkins and Peter Dayan. 1992. Q- learning. Machine learning, 8(3-4):279-292.\n\nSimple statistical gradientfollowing algorithms for connectionist reinforcement learning. J Ronald, Williams, Machine learning. 83-4Ronald J Williams. 1992. Simple statistical gradient- following algorithms for connectionist reinforce- ment learning. Machine learning, 8(3-4):229-256.\n\nA study of reinforcement learning for neural machine translation. Lijun Wu, Fei Tian, Tao Qin, Jianhuang Lai, Tie-Yan Liu, EMNLP. Lijun Wu, Fei Tian, Tao Qin, Jianhuang Lai, and Tie- Yan Liu. 2018. A study of reinforcement learning for neural machine translation. In EMNLP.\n\nLijun Wu, Yingce Xia, Li Zhao, Fei Tian, Tao Qin, Jianhuang Lai, Tie-Yan Liu, arXiv:1704.06933Adversarial neural machine translation. arXiv preprintLijun Wu, Yingce Xia, Li Zhao, Fei Tian, Tao Qin, Jianhuang Lai, and Tie-Yan Liu. 2017. Adver- sarial neural machine translation. arXiv preprint arXiv:1704.06933.\n\nImproving neural machine translation with conditional sequence generative adversarial nets. Zhen Yang, Wei Chen, Feng Wang, Bo Xu, 10.18653/v1/N18-1122Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational LinguisticsLong PapersZhen Yang, Wei Chen, Feng Wang, and Bo Xu. 2018. Improving neural machine translation with condi- tional sequence generative adversarial nets. In Pro- ceedings of the 2018 Conference of the North Amer- ican Chapter of the Association for Computational Linguistics: Human Language Technologies, Vol- ume 1 (Long Papers), pages 1346-1355. Association for Computational Linguistics.\n\nSeqgan: Sequence generative adversarial nets with policy gradient. Lantao Yu, Weinan Zhang, Jun Wang, Yong Yu, AAAI. Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. 2017. Seqgan: Sequence generative adversarial nets with policy gradient. In AAAI, pages 2852-2858.\n\nThumt: An open source toolkit for neural machine translation. Yanzhuo Jiac Heng Zhang, Shiqi Ding, Yong Shen, Maosong Cheng, Huanbo Sun, Yang Luan, Liu, arXiv:1706.06415arXiv preprintJiac heng Zhang, Yanzhuo Ding, Shiqi Shen, Yong Cheng, Maosong Sun, Huanbo Luan, and Yang Liu. 2017. Thumt: An open source toolkit for neural ma- chine translation. arXiv preprint arXiv:1706.06415.\n\nGenerating text via adversarial training. Yizhe Zhang, Zhe Gan, Lawrence Carin, NIPS workshop on Adversarial Training. 21Yizhe Zhang, Zhe Gan, and Lawrence Carin. 2016. Generating text via adversarial training. In NIPS workshop on Adversarial Training, volume 21.\n", "annotations": {"author": "[{\"end\":169,\"start\":78},{\"end\":249,\"start\":170},{\"end\":341,\"start\":250},{\"end\":489,\"start\":342}]", "publisher": null, "author_last_name": "[{\"end\":92,\"start\":85},{\"end\":178,\"start\":175},{\"end\":264,\"start\":256},{\"end\":352,\"start\":347}]", "author_first_name": "[{\"end\":84,\"start\":78},{\"end\":174,\"start\":170},{\"end\":255,\"start\":250},{\"end\":346,\"start\":342}]", "author_affiliation": "[{\"end\":168,\"start\":125},{\"end\":248,\"start\":205},{\"end\":340,\"start\":297},{\"end\":418,\"start\":375},{\"end\":488,\"start\":420}]", "title": "[{\"end\":75,\"start\":1},{\"end\":564,\"start\":490}]", "venue": null, "abstract": "[{\"end\":1797,\"start\":566}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2084,\"start\":2062},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2452,\"start\":2436},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2502,\"start\":2481},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2522,\"start\":2502},{\"end\":2554,\"start\":2527},{\"end\":2570,\"start\":2554},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2638,\"start\":2619},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2657,\"start\":2638},{\"end\":4751,\"start\":4745},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5062,\"start\":5041},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":5171,\"start\":5153},{\"end\":5428,\"start\":5405},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6411,\"start\":6395},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6431,\"start\":6411},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6451,\"start\":6431},{\"end\":6473,\"start\":6451},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6537,\"start\":6515},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6742,\"start\":6722},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6758,\"start\":6742},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6774,\"start\":6758},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":6790,\"start\":6774},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6808,\"start\":6790},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7257,\"start\":7237},{\"end\":7270,\"start\":7257},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7290,\"start\":7270},{\"end\":7318,\"start\":7290},{\"end\":7338,\"start\":7318},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7703,\"start\":7680},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10324,\"start\":10304},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12275,\"start\":12255},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":13284,\"start\":13262},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":13584,\"start\":13564},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":16871,\"start\":16853},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":17222,\"start\":17202},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":17456,\"start\":17437},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":25805,\"start\":25784},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":26119,\"start\":26099},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":26351,\"start\":26334},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":26605,\"start\":26582},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":26704,\"start\":26684},{\"end\":27786,\"start\":27768},{\"end\":27806,\"start\":27786},{\"end\":27933,\"start\":27909},{\"end\":28946,\"start\":28918},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":28969,\"start\":28946},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":29056,\"start\":29035},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":29076,\"start\":29056},{\"end\":29583,\"start\":29565},{\"end\":29674,\"start\":29651},{\"end\":30045,\"start\":30018},{\"end\":30064,\"start\":30045},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":30087,\"start\":30064},{\"end\":31726,\"start\":31706},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":32086,\"start\":32063},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":32788,\"start\":32762},{\"end\":32848,\"start\":32827},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":35262,\"start\":35239}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":34609,\"start\":33946},{\"attributes\":{\"id\":\"fig_1\"},\"end\":34925,\"start\":34610},{\"attributes\":{\"id\":\"fig_2\"},\"end\":35045,\"start\":34926},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":35238,\"start\":35046}]", "paragraph": "[{\"end\":2829,\"start\":1813},{\"end\":3085,\"start\":2831},{\"end\":3575,\"start\":3087},{\"end\":4047,\"start\":3577},{\"end\":4291,\"start\":4049},{\"end\":4893,\"start\":4344},{\"end\":5312,\"start\":4895},{\"end\":5525,\"start\":5314},{\"end\":5796,\"start\":5572},{\"end\":6305,\"start\":5840},{\"end\":6809,\"start\":6307},{\"end\":7339,\"start\":6835},{\"end\":7482,\"start\":7341},{\"end\":7775,\"start\":7532},{\"end\":8242,\"start\":7777},{\"end\":8648,\"start\":8244},{\"end\":9086,\"start\":8765},{\"end\":9878,\"start\":9088},{\"end\":10128,\"start\":9904},{\"end\":10807,\"start\":10130},{\"end\":11495,\"start\":10809},{\"end\":11792,\"start\":11497},{\"end\":12072,\"start\":11794},{\"end\":12855,\"start\":12097},{\"end\":13754,\"start\":12882},{\"end\":14100,\"start\":13756},{\"end\":14260,\"start\":14102},{\"end\":14751,\"start\":14262},{\"end\":15240,\"start\":14753},{\"end\":15485,\"start\":15242},{\"end\":15967,\"start\":15487},{\"end\":16300,\"start\":15969},{\"end\":16793,\"start\":16320},{\"end\":17079,\"start\":16795},{\"end\":17457,\"start\":17081},{\"end\":17688,\"start\":17469},{\"end\":18500,\"start\":17690},{\"end\":18759,\"start\":18536},{\"end\":18981,\"start\":18786},{\"end\":20337,\"start\":18983},{\"end\":20683,\"start\":20357},{\"end\":21329,\"start\":20685},{\"end\":22179,\"start\":21331},{\"end\":22910,\"start\":22181},{\"end\":23975,\"start\":22947},{\"end\":24156,\"start\":23977},{\"end\":25053,\"start\":24171},{\"end\":26021,\"start\":25055},{\"end\":26245,\"start\":26023},{\"end\":26509,\"start\":26247},{\"end\":27183,\"start\":26511},{\"end\":27347,\"start\":27185},{\"end\":28401,\"start\":27362},{\"end\":28781,\"start\":28403},{\"end\":29288,\"start\":28783},{\"end\":29675,\"start\":29290},{\"end\":30190,\"start\":29677},{\"end\":30396,\"start\":30250},{\"end\":30511,\"start\":30398},{\"end\":30648,\"start\":30570},{\"end\":31245,\"start\":30679},{\"end\":31427,\"start\":31247},{\"end\":31634,\"start\":31613},{\"end\":33019,\"start\":31667},{\"end\":33467,\"start\":33021},{\"end\":33945,\"start\":33518}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":5571,\"start\":5526},{\"attributes\":{\"id\":\"formula_1\"},\"end\":5839,\"start\":5797},{\"attributes\":{\"id\":\"formula_2\"},\"end\":7531,\"start\":7483},{\"attributes\":{\"id\":\"formula_3\"},\"end\":8764,\"start\":8649},{\"attributes\":{\"id\":\"formula_4\"},\"end\":30569,\"start\":30512},{\"attributes\":{\"id\":\"formula_5\"},\"end\":30678,\"start\":30649},{\"attributes\":{\"id\":\"formula_6\"},\"end\":31612,\"start\":31428}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":30728,\"start\":30721}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1811,\"start\":1799},{\"attributes\":{\"n\":\"2\"},\"end\":4319,\"start\":4294},{\"attributes\":{\"n\":\"2.1\"},\"end\":4342,\"start\":4322},{\"attributes\":{\"n\":\"2.3\"},\"end\":6833,\"start\":6812},{\"attributes\":{\"n\":\"3\"},\"end\":9902,\"start\":9881},{\"attributes\":{\"n\":\"4\"},\"end\":12095,\"start\":12075},{\"attributes\":{\"n\":\"4.1\"},\"end\":12880,\"start\":12858},{\"attributes\":{\"n\":\"4.2\"},\"end\":16318,\"start\":16303},{\"end\":17467,\"start\":17460},{\"attributes\":{\"n\":\"5\"},\"end\":18534,\"start\":18503},{\"attributes\":{\"n\":\"5.1\"},\"end\":18784,\"start\":18762},{\"attributes\":{\"n\":\"5.2\"},\"end\":20355,\"start\":20340},{\"attributes\":{\"n\":\"6\"},\"end\":22945,\"start\":22913},{\"attributes\":{\"n\":\"7\"},\"end\":24169,\"start\":24159},{\"attributes\":{\"n\":\"8\"},\"end\":27360,\"start\":27350},{\"end\":30248,\"start\":30193},{\"end\":31665,\"start\":31637},{\"end\":33516,\"start\":33470},{\"end\":33967,\"start\":33947},{\"end\":34621,\"start\":34611},{\"end\":35056,\"start\":35047}]", "table": null, "figure_caption": "[{\"end\":34609,\"start\":33970},{\"end\":34925,\"start\":34623},{\"end\":35045,\"start\":34928},{\"end\":35238,\"start\":35058}]", "figure_ref": "[{\"end\":15676,\"start\":15666},{\"end\":15827,\"start\":15821},{\"end\":16257,\"start\":16248},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":17767,\"start\":17759},{\"end\":19793,\"start\":19785},{\"end\":20833,\"start\":20825},{\"end\":21115,\"start\":21105},{\"end\":21475,\"start\":21467},{\"end\":22656,\"start\":22648},{\"end\":23413,\"start\":23405},{\"end\":26809,\"start\":26801},{\"end\":33075,\"start\":33067},{\"end\":33586,\"start\":33571},{\"end\":33670,\"start\":33662}]", "bib_author_first_name": "[{\"end\":35891,\"start\":35885},{\"end\":35911,\"start\":35906},{\"end\":35924,\"start\":35920},{\"end\":35935,\"start\":35930},{\"end\":35953,\"start\":35947},{\"end\":35965,\"start\":35960},{\"end\":35979,\"start\":35976},{\"end\":35993,\"start\":35989},{\"end\":36386,\"start\":36379},{\"end\":36412,\"start\":36405},{\"end\":36602,\"start\":36596},{\"end\":36615,\"start\":36610},{\"end\":36637,\"start\":36628},{\"end\":36654,\"start\":36649},{\"end\":36671,\"start\":36663},{\"end\":36683,\"start\":36678},{\"end\":36699,\"start\":36692},{\"end\":36714,\"start\":36707},{\"end\":36734,\"start\":36726},{\"end\":36747,\"start\":36741},{\"end\":36759,\"start\":36755},{\"end\":36774,\"start\":36766},{\"end\":36789,\"start\":36784},{\"end\":36803,\"start\":36798},{\"end\":37201,\"start\":37194},{\"end\":37215,\"start\":37210},{\"end\":37231,\"start\":37224},{\"end\":37243,\"start\":37239},{\"end\":37262,\"start\":37256},{\"end\":37278,\"start\":37271},{\"end\":37520,\"start\":37514},{\"end\":37535,\"start\":37529},{\"end\":37552,\"start\":37543},{\"end\":37812,\"start\":37806},{\"end\":37825,\"start\":37821},{\"end\":37838,\"start\":37831},{\"end\":37850,\"start\":37845},{\"end\":37878,\"start\":37861},{\"end\":38715,\"start\":38707},{\"end\":38735,\"start\":38727},{\"end\":38749,\"start\":38743},{\"end\":38763,\"start\":38757},{\"end\":39280,\"start\":39272},{\"end\":39295,\"start\":39291},{\"end\":39315,\"start\":39307},{\"end\":39332,\"start\":39326},{\"end\":39341,\"start\":39340},{\"end\":39353,\"start\":39351},{\"end\":39811,\"start\":39810},{\"end\":39820,\"start\":39819},{\"end\":39827,\"start\":39826},{\"end\":39836,\"start\":39835},{\"end\":39846,\"start\":39845},{\"end\":40135,\"start\":40128},{\"end\":40147,\"start\":40142},{\"end\":40164,\"start\":40157},{\"end\":40181,\"start\":40173},{\"end\":40508,\"start\":40507},{\"end\":40524,\"start\":40517},{\"end\":40540,\"start\":40533},{\"end\":40558,\"start\":40552},{\"end\":40575,\"start\":40567},{\"end\":40982,\"start\":40975},{\"end\":40998,\"start\":40994},{\"end\":41013,\"start\":41005},{\"end\":41338,\"start\":41332},{\"end\":41353,\"start\":41347},{\"end\":41366,\"start\":41360},{\"end\":41546,\"start\":41542},{\"end\":41562,\"start\":41557},{\"end\":41579,\"start\":41570},{\"end\":41594,\"start\":41585},{\"end\":41607,\"start\":41602},{\"end\":41622,\"start\":41616},{\"end\":41640,\"start\":41634},{\"end\":41664,\"start\":41658},{\"end\":41680,\"start\":41673},{\"end\":41702,\"start\":41697},{\"end\":41716,\"start\":41711},{\"end\":42071,\"start\":42067},{\"end\":42087,\"start\":42082},{\"end\":42105,\"start\":42096},{\"end\":42597,\"start\":42592},{\"end\":42608,\"start\":42604},{\"end\":42624,\"start\":42616},{\"end\":42632,\"start\":42629},{\"end\":42640,\"start\":42637},{\"end\":42652,\"start\":42645},{\"end\":42662,\"start\":42658},{\"end\":43309,\"start\":43304},{\"end\":43320,\"start\":43316},{\"end\":43333,\"start\":43326},{\"end\":43664,\"start\":43656},{\"end\":43679,\"start\":43673},{\"end\":43694,\"start\":43690},{\"end\":43699,\"start\":43695},{\"end\":43716,\"start\":43711},{\"end\":43729,\"start\":43724},{\"end\":44128,\"start\":44123},{\"end\":44140,\"start\":44137},{\"end\":44155,\"start\":44148},{\"end\":44169,\"start\":44163},{\"end\":44182,\"start\":44178},{\"end\":44199,\"start\":44193},{\"end\":44413,\"start\":44412},{\"end\":44429,\"start\":44423},{\"end\":44431,\"start\":44430},{\"end\":44555,\"start\":44554},{\"end\":44564,\"start\":44563},{\"end\":44573,\"start\":44572},{\"end\":44584,\"start\":44583},{\"end\":44933,\"start\":44927},{\"end\":44952,\"start\":44944},{\"end\":45188,\"start\":45182},{\"end\":45202,\"start\":45198},{\"end\":45216,\"start\":45212},{\"end\":45230,\"start\":45225},{\"end\":45247,\"start\":45242},{\"end\":45260,\"start\":45255},{\"end\":45262,\"start\":45261},{\"end\":45276,\"start\":45270},{\"end\":45290,\"start\":45285},{\"end\":45600,\"start\":45597},{\"end\":45619,\"start\":45614},{\"end\":45844,\"start\":45843},{\"end\":46110,\"start\":46105},{\"end\":46118,\"start\":46115},{\"end\":46128,\"start\":46125},{\"end\":46143,\"start\":46134},{\"end\":46156,\"start\":46149},{\"end\":46319,\"start\":46314},{\"end\":46330,\"start\":46324},{\"end\":46338,\"start\":46336},{\"end\":46348,\"start\":46345},{\"end\":46358,\"start\":46355},{\"end\":46373,\"start\":46364},{\"end\":46386,\"start\":46379},{\"end\":46722,\"start\":46718},{\"end\":46732,\"start\":46729},{\"end\":46743,\"start\":46739},{\"end\":46752,\"start\":46750},{\"end\":47554,\"start\":47548},{\"end\":47565,\"start\":47559},{\"end\":47576,\"start\":47573},{\"end\":47587,\"start\":47583},{\"end\":47815,\"start\":47808},{\"end\":47838,\"start\":47833},{\"end\":47849,\"start\":47845},{\"end\":47863,\"start\":47856},{\"end\":47877,\"start\":47871},{\"end\":47887,\"start\":47883},{\"end\":48175,\"start\":48170},{\"end\":48186,\"start\":48183},{\"end\":48200,\"start\":48192}]", "bib_author_last_name": "[{\"end\":35904,\"start\":35892},{\"end\":35918,\"start\":35912},{\"end\":35928,\"start\":35925},{\"end\":35945,\"start\":35936},{\"end\":35958,\"start\":35954},{\"end\":35974,\"start\":35966},{\"end\":35987,\"start\":35980},{\"end\":35999,\"start\":35994},{\"end\":36403,\"start\":36387},{\"end\":36416,\"start\":36413},{\"end\":36421,\"start\":36418},{\"end\":36608,\"start\":36603},{\"end\":36626,\"start\":36616},{\"end\":36647,\"start\":36638},{\"end\":36661,\"start\":36655},{\"end\":36676,\"start\":36672},{\"end\":36690,\"start\":36684},{\"end\":36705,\"start\":36700},{\"end\":36724,\"start\":36715},{\"end\":36739,\"start\":36735},{\"end\":36753,\"start\":36748},{\"end\":36764,\"start\":36760},{\"end\":36782,\"start\":36775},{\"end\":36796,\"start\":36790},{\"end\":36810,\"start\":36804},{\"end\":37208,\"start\":37202},{\"end\":37222,\"start\":37216},{\"end\":37237,\"start\":37232},{\"end\":37254,\"start\":37244},{\"end\":37269,\"start\":37263},{\"end\":37286,\"start\":37279},{\"end\":37527,\"start\":37521},{\"end\":37541,\"start\":37536},{\"end\":37559,\"start\":37553},{\"end\":37819,\"start\":37813},{\"end\":37829,\"start\":37826},{\"end\":37843,\"start\":37839},{\"end\":37859,\"start\":37851},{\"end\":37886,\"start\":37879},{\"end\":38725,\"start\":38716},{\"end\":38741,\"start\":38736},{\"end\":38755,\"start\":38750},{\"end\":38770,\"start\":38764},{\"end\":39289,\"start\":39281},{\"end\":39305,\"start\":39296},{\"end\":39324,\"start\":39316},{\"end\":39338,\"start\":39333},{\"end\":39349,\"start\":39342},{\"end\":39358,\"start\":39354},{\"end\":39364,\"start\":39360},{\"end\":39817,\"start\":39812},{\"end\":39824,\"start\":39821},{\"end\":39833,\"start\":39828},{\"end\":39843,\"start\":39837},{\"end\":39851,\"start\":39847},{\"end\":40140,\"start\":40136},{\"end\":40155,\"start\":40148},{\"end\":40171,\"start\":40165},{\"end\":40186,\"start\":40182},{\"end\":40195,\"start\":40188},{\"end\":40515,\"start\":40509},{\"end\":40531,\"start\":40525},{\"end\":40550,\"start\":40541},{\"end\":40565,\"start\":40559},{\"end\":40580,\"start\":40576},{\"end\":40586,\"start\":40582},{\"end\":40992,\"start\":40983},{\"end\":41003,\"start\":40999},{\"end\":41023,\"start\":41014},{\"end\":41345,\"start\":41339},{\"end\":41358,\"start\":41354},{\"end\":41371,\"start\":41367},{\"end\":41555,\"start\":41547},{\"end\":41568,\"start\":41563},{\"end\":41583,\"start\":41580},{\"end\":41600,\"start\":41595},{\"end\":41614,\"start\":41608},{\"end\":41632,\"start\":41623},{\"end\":41656,\"start\":41641},{\"end\":41671,\"start\":41665},{\"end\":41695,\"start\":41681},{\"end\":41709,\"start\":41703},{\"end\":41722,\"start\":41717},{\"end\":41731,\"start\":41724},{\"end\":42080,\"start\":42072},{\"end\":42094,\"start\":42088},{\"end\":42111,\"start\":42106},{\"end\":42602,\"start\":42598},{\"end\":42614,\"start\":42609},{\"end\":42627,\"start\":42625},{\"end\":42635,\"start\":42633},{\"end\":42643,\"start\":42641},{\"end\":42656,\"start\":42653},{\"end\":42666,\"start\":42663},{\"end\":43314,\"start\":43310},{\"end\":43324,\"start\":43321},{\"end\":43337,\"start\":43334},{\"end\":43671,\"start\":43665},{\"end\":43688,\"start\":43680},{\"end\":43709,\"start\":43700},{\"end\":43722,\"start\":43717},{\"end\":43737,\"start\":43730},{\"end\":44135,\"start\":44129},{\"end\":44146,\"start\":44141},{\"end\":44161,\"start\":44156},{\"end\":44176,\"start\":44170},{\"end\":44191,\"start\":44183},{\"end\":44210,\"start\":44200},{\"end\":44421,\"start\":44414},{\"end\":44438,\"start\":44432},{\"end\":44445,\"start\":44440},{\"end\":44561,\"start\":44556},{\"end\":44570,\"start\":44565},{\"end\":44581,\"start\":44574},{\"end\":44591,\"start\":44585},{\"end\":44942,\"start\":44934},{\"end\":44959,\"start\":44953},{\"end\":45196,\"start\":45189},{\"end\":45210,\"start\":45203},{\"end\":45223,\"start\":45217},{\"end\":45240,\"start\":45231},{\"end\":45253,\"start\":45248},{\"end\":45268,\"start\":45263},{\"end\":45283,\"start\":45277},{\"end\":45301,\"start\":45291},{\"end\":45612,\"start\":45601},{\"end\":45627,\"start\":45620},{\"end\":45634,\"start\":45629},{\"end\":45851,\"start\":45845},{\"end\":45861,\"start\":45853},{\"end\":46113,\"start\":46111},{\"end\":46123,\"start\":46119},{\"end\":46132,\"start\":46129},{\"end\":46147,\"start\":46144},{\"end\":46160,\"start\":46157},{\"end\":46322,\"start\":46320},{\"end\":46334,\"start\":46331},{\"end\":46343,\"start\":46339},{\"end\":46353,\"start\":46349},{\"end\":46362,\"start\":46359},{\"end\":46377,\"start\":46374},{\"end\":46390,\"start\":46387},{\"end\":46727,\"start\":46723},{\"end\":46737,\"start\":46733},{\"end\":46748,\"start\":46744},{\"end\":46755,\"start\":46753},{\"end\":47557,\"start\":47555},{\"end\":47571,\"start\":47566},{\"end\":47581,\"start\":47577},{\"end\":47590,\"start\":47588},{\"end\":47831,\"start\":47816},{\"end\":47843,\"start\":47839},{\"end\":47854,\"start\":47850},{\"end\":47869,\"start\":47864},{\"end\":47881,\"start\":47878},{\"end\":47892,\"start\":47888},{\"end\":47897,\"start\":47894},{\"end\":48181,\"start\":48176},{\"end\":48190,\"start\":48187},{\"end\":48206,\"start\":48201}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":36322,\"start\":35806},{\"attributes\":{\"doi\":\"arXiv:1604.01904\",\"id\":\"b1\"},\"end\":36594,\"start\":36324},{\"attributes\":{\"id\":\"b2\"},\"end\":37192,\"start\":36596},{\"attributes\":{\"doi\":\"arXiv:1811.02549\",\"id\":\"b3\"},\"end\":37512,\"start\":37194},{\"attributes\":{\"doi\":\"arXiv:1205.4839\",\"id\":\"b4\"},\"end\":37730,\"start\":37514},{\"attributes\":{\"doi\":\"10.18653/v1/N18-1033\",\"id\":\"b5\",\"matched_paper_id\":3718988},\"end\":38635,\"start\":37732},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":3521071},\"end\":39270,\"start\":38637},{\"attributes\":{\"doi\":\"arXiv:1706.01905\",\"id\":\"b7\"},\"end\":39719,\"start\":39272},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":20610732},\"end\":40126,\"start\":39721},{\"attributes\":{\"doi\":\"arXiv:1511.06732\",\"id\":\"b9\"},\"end\":40451,\"start\":40128},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":206594923},\"end\":40973,\"start\":40453},{\"attributes\":{\"doi\":\"arXiv:1707.00299\",\"id\":\"b11\"},\"end\":41277,\"start\":40975},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":44130927},\"end\":41489,\"start\":41279},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":905565},\"end\":42004,\"start\":41491},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":1114678},\"end\":42536,\"start\":42006},{\"attributes\":{\"doi\":\"10.18653/v1/P16-1159\",\"id\":\"b15\",\"matched_paper_id\":3913537},\"end\":43223,\"start\":42538},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":25636741},\"end\":43566,\"start\":43225},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":6093112},\"end\":44079,\"start\":43568},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":13928442},\"end\":44369,\"start\":44081},{\"attributes\":{\"id\":\"b19\"},\"end\":44552,\"start\":44371},{\"attributes\":{\"doi\":\"arXiv:1810.12686\",\"id\":\"b20\"},\"end\":44790,\"start\":44554},{\"attributes\":{\"id\":\"b21\"},\"end\":45153,\"start\":44792},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":13756489},\"end\":45584,\"start\":45155},{\"attributes\":{\"id\":\"b23\"},\"end\":45751,\"start\":45586},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":2332513},\"end\":46037,\"start\":45753},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":52100616},\"end\":46312,\"start\":46039},{\"attributes\":{\"doi\":\"arXiv:1704.06933\",\"id\":\"b26\"},\"end\":46624,\"start\":46314},{\"attributes\":{\"doi\":\"10.18653/v1/N18-1122\",\"id\":\"b27\",\"matched_paper_id\":4702087},\"end\":47479,\"start\":46626},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":3439214},\"end\":47744,\"start\":47481},{\"attributes\":{\"doi\":\"arXiv:1706.06415\",\"id\":\"b29\"},\"end\":48126,\"start\":47746},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":29797603},\"end\":48391,\"start\":48128}]", "bib_title": "[{\"end\":35883,\"start\":35806},{\"end\":37804,\"start\":37732},{\"end\":38705,\"start\":38637},{\"end\":39808,\"start\":39721},{\"end\":40505,\"start\":40453},{\"end\":41330,\"start\":41279},{\"end\":41540,\"start\":41491},{\"end\":42065,\"start\":42006},{\"end\":42590,\"start\":42538},{\"end\":43302,\"start\":43225},{\"end\":43654,\"start\":43568},{\"end\":44121,\"start\":44081},{\"end\":45180,\"start\":45155},{\"end\":45595,\"start\":45586},{\"end\":45841,\"start\":45753},{\"end\":46103,\"start\":46039},{\"end\":46716,\"start\":46626},{\"end\":47546,\"start\":47481},{\"end\":48168,\"start\":48128}]", "bib_author": "[{\"end\":35906,\"start\":35885},{\"end\":35920,\"start\":35906},{\"end\":35930,\"start\":35920},{\"end\":35947,\"start\":35930},{\"end\":35960,\"start\":35947},{\"end\":35976,\"start\":35960},{\"end\":35989,\"start\":35976},{\"end\":36001,\"start\":35989},{\"end\":36405,\"start\":36379},{\"end\":36418,\"start\":36405},{\"end\":36423,\"start\":36418},{\"end\":36610,\"start\":36596},{\"end\":36628,\"start\":36610},{\"end\":36649,\"start\":36628},{\"end\":36663,\"start\":36649},{\"end\":36678,\"start\":36663},{\"end\":36692,\"start\":36678},{\"end\":36707,\"start\":36692},{\"end\":36726,\"start\":36707},{\"end\":36741,\"start\":36726},{\"end\":36755,\"start\":36741},{\"end\":36766,\"start\":36755},{\"end\":36784,\"start\":36766},{\"end\":36798,\"start\":36784},{\"end\":36812,\"start\":36798},{\"end\":37210,\"start\":37194},{\"end\":37224,\"start\":37210},{\"end\":37239,\"start\":37224},{\"end\":37256,\"start\":37239},{\"end\":37271,\"start\":37256},{\"end\":37288,\"start\":37271},{\"end\":37529,\"start\":37514},{\"end\":37543,\"start\":37529},{\"end\":37561,\"start\":37543},{\"end\":37821,\"start\":37806},{\"end\":37831,\"start\":37821},{\"end\":37845,\"start\":37831},{\"end\":37861,\"start\":37845},{\"end\":37888,\"start\":37861},{\"end\":38727,\"start\":38707},{\"end\":38743,\"start\":38727},{\"end\":38757,\"start\":38743},{\"end\":38772,\"start\":38757},{\"end\":39291,\"start\":39272},{\"end\":39307,\"start\":39291},{\"end\":39326,\"start\":39307},{\"end\":39340,\"start\":39326},{\"end\":39351,\"start\":39340},{\"end\":39360,\"start\":39351},{\"end\":39366,\"start\":39360},{\"end\":39819,\"start\":39810},{\"end\":39826,\"start\":39819},{\"end\":39835,\"start\":39826},{\"end\":39845,\"start\":39835},{\"end\":39853,\"start\":39845},{\"end\":40142,\"start\":40128},{\"end\":40157,\"start\":40142},{\"end\":40173,\"start\":40157},{\"end\":40188,\"start\":40173},{\"end\":40197,\"start\":40188},{\"end\":40517,\"start\":40507},{\"end\":40533,\"start\":40517},{\"end\":40552,\"start\":40533},{\"end\":40567,\"start\":40552},{\"end\":40582,\"start\":40567},{\"end\":40588,\"start\":40582},{\"end\":40994,\"start\":40975},{\"end\":41005,\"start\":40994},{\"end\":41025,\"start\":41005},{\"end\":41347,\"start\":41332},{\"end\":41360,\"start\":41347},{\"end\":41373,\"start\":41360},{\"end\":41557,\"start\":41542},{\"end\":41570,\"start\":41557},{\"end\":41585,\"start\":41570},{\"end\":41602,\"start\":41585},{\"end\":41616,\"start\":41602},{\"end\":41634,\"start\":41616},{\"end\":41658,\"start\":41634},{\"end\":41673,\"start\":41658},{\"end\":41697,\"start\":41673},{\"end\":41711,\"start\":41697},{\"end\":41724,\"start\":41711},{\"end\":41733,\"start\":41724},{\"end\":42082,\"start\":42067},{\"end\":42096,\"start\":42082},{\"end\":42113,\"start\":42096},{\"end\":42604,\"start\":42592},{\"end\":42616,\"start\":42604},{\"end\":42629,\"start\":42616},{\"end\":42637,\"start\":42629},{\"end\":42645,\"start\":42637},{\"end\":42658,\"start\":42645},{\"end\":42668,\"start\":42658},{\"end\":43316,\"start\":43304},{\"end\":43326,\"start\":43316},{\"end\":43339,\"start\":43326},{\"end\":43673,\"start\":43656},{\"end\":43690,\"start\":43673},{\"end\":43711,\"start\":43690},{\"end\":43724,\"start\":43711},{\"end\":43739,\"start\":43724},{\"end\":44137,\"start\":44123},{\"end\":44148,\"start\":44137},{\"end\":44163,\"start\":44148},{\"end\":44178,\"start\":44163},{\"end\":44193,\"start\":44178},{\"end\":44212,\"start\":44193},{\"end\":44423,\"start\":44412},{\"end\":44440,\"start\":44423},{\"end\":44447,\"start\":44440},{\"end\":44563,\"start\":44554},{\"end\":44572,\"start\":44563},{\"end\":44583,\"start\":44572},{\"end\":44593,\"start\":44583},{\"end\":44944,\"start\":44927},{\"end\":44961,\"start\":44944},{\"end\":45198,\"start\":45182},{\"end\":45212,\"start\":45198},{\"end\":45225,\"start\":45212},{\"end\":45242,\"start\":45225},{\"end\":45255,\"start\":45242},{\"end\":45270,\"start\":45255},{\"end\":45285,\"start\":45270},{\"end\":45303,\"start\":45285},{\"end\":45614,\"start\":45597},{\"end\":45629,\"start\":45614},{\"end\":45636,\"start\":45629},{\"end\":45853,\"start\":45843},{\"end\":45863,\"start\":45853},{\"end\":46115,\"start\":46105},{\"end\":46125,\"start\":46115},{\"end\":46134,\"start\":46125},{\"end\":46149,\"start\":46134},{\"end\":46162,\"start\":46149},{\"end\":46324,\"start\":46314},{\"end\":46336,\"start\":46324},{\"end\":46345,\"start\":46336},{\"end\":46355,\"start\":46345},{\"end\":46364,\"start\":46355},{\"end\":46379,\"start\":46364},{\"end\":46392,\"start\":46379},{\"end\":46729,\"start\":46718},{\"end\":46739,\"start\":46729},{\"end\":46750,\"start\":46739},{\"end\":46757,\"start\":46750},{\"end\":47559,\"start\":47548},{\"end\":47573,\"start\":47559},{\"end\":47583,\"start\":47573},{\"end\":47592,\"start\":47583},{\"end\":47833,\"start\":47808},{\"end\":47845,\"start\":47833},{\"end\":47856,\"start\":47845},{\"end\":47871,\"start\":47856},{\"end\":47883,\"start\":47871},{\"end\":47894,\"start\":47883},{\"end\":47899,\"start\":47894},{\"end\":48183,\"start\":48170},{\"end\":48192,\"start\":48183},{\"end\":48208,\"start\":48192}]", "bib_venue": "[{\"end\":38179,\"start\":38052},{\"end\":40729,\"start\":40667},{\"end\":42274,\"start\":42202},{\"end\":42849,\"start\":42777},{\"end\":47048,\"start\":46921},{\"end\":36050,\"start\":36001},{\"end\":36377,\"start\":36324},{\"end\":36876,\"start\":36812},{\"end\":37331,\"start\":37304},{\"end\":37599,\"start\":37576},{\"end\":38050,\"start\":37908},{\"end\":38894,\"start\":38772},{\"end\":39479,\"start\":39382},{\"end\":39912,\"start\":39853},{\"end\":40267,\"start\":40213},{\"end\":40665,\"start\":40588},{\"end\":41104,\"start\":41041},{\"end\":41376,\"start\":41373},{\"end\":41737,\"start\":41733},{\"end\":42200,\"start\":42113},{\"end\":42775,\"start\":42688},{\"end\":43381,\"start\":43339},{\"end\":43799,\"start\":43739},{\"end\":44216,\"start\":44212},{\"end\":44410,\"start\":44371},{\"end\":44648,\"start\":44609},{\"end\":44925,\"start\":44792},{\"end\":45352,\"start\":45303},{\"end\":45652,\"start\":45636},{\"end\":45879,\"start\":45863},{\"end\":46167,\"start\":46162},{\"end\":46446,\"start\":46408},{\"end\":46919,\"start\":46777},{\"end\":47596,\"start\":47592},{\"end\":47806,\"start\":47746},{\"end\":48245,\"start\":48208}]"}}}, "year": 2023, "month": 12, "day": 17}