{"id": 203737365, "updated": "2023-10-06 22:20:24.125", "metadata": {"title": "Generalized Inner Loop Meta-Learning", "authors": "[{\"first\":\"Edward\",\"last\":\"Grefenstette\",\"middle\":[]},{\"first\":\"Brandon\",\"last\":\"Amos\",\"middle\":[]},{\"first\":\"Denis\",\"last\":\"Yarats\",\"middle\":[]},{\"first\":\"Phu\",\"last\":\"Htut\",\"middle\":[\"Mon\"]},{\"first\":\"Artem\",\"last\":\"Molchanov\",\"middle\":[]},{\"first\":\"Franziska\",\"last\":\"Meier\",\"middle\":[]},{\"first\":\"Douwe\",\"last\":\"Kiela\",\"middle\":[]},{\"first\":\"Kyunghyun\",\"last\":\"Cho\",\"middle\":[]},{\"first\":\"Soumith\",\"last\":\"Chintala\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2019, "month": 10, "day": 3}, "abstract": "Many (but not all) approaches self-qualifying as\"meta-learning\"in deep learning and reinforcement learning fit a common pattern of approximating the solution to a nested optimization problem. In this paper, we give a formalization of this shared pattern, which we call GIMLI, prove its general requirements, and derive a general-purpose algorithm for implementing similar approaches. Based on this analysis and algorithm, we describe a library of our design, higher, which we share with the community to assist and enable future research into these kinds of meta-learning approaches. We end the paper by showcasing the practical applications of this framework and library through illustrative experiments and ablation studies which they facilitate.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "1910.01727", "mag": "2978409868", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-1910-01727", "doi": null}}, "content": {"source": {"pdf_hash": "f94dd0b22fd2fe322a45c2143bd8eb48ca485b5f", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1910.01727v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "39719a1c4dfd781f824017ebceabe39c0b2e0664", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/f94dd0b22fd2fe322a45c2143bd8eb48ca485b5f.txt", "contents": "\nGENERALIZED INNER LOOP META-LEARNING\n\n\nEdward Grefenstette \nBrandon Amos \nDenis Yarats denisyarats@cs.nyu.com \nArtem Molchanov molchano@usc.edu \nFranziska Meier fmeier@fb.com \nDouwe Kiela dkiela@fb.com \nKyunghyun Cho kyunghyuncho@fb.com \nSoumith Chintala soumith@fb.com \n\nFacebook AI Research\nFacebook AI Research\nFacebook AI Research\nFacebook AI Research\nFacebook AI Research\nNYU &\nUniversity of Southern\nCalifornia\n\n\nFacebook AI Research\nFacebook AI Research\nNYU &\n\n\nGENERALIZED INNER LOOP META-LEARNING\nPhu Mon Htut NYU\nMany (but not all) approaches self-qualifying as \"meta-learning\" in deep learning and reinforcement learning fit a common pattern of approximating the solution to a nested optimization problem. In this paper, we give a formalization of this shared pattern, which we call GIMLI, prove its general requirements, and derive a general-purpose algorithm for implementing similar approaches. Based on this analysis and algorithm, we describe a library of our design, higher, which we share with the community to assist and enable future research into these kinds of meta-learning approaches. We end the paper by showcasing the practical applications of this framework and library through illustrative experiments and ablation studies which they facilitate.\n\nINTRODUCTION\n\nAlthough it is by no means a new subfield of machine learning research (see e.g. Schmidhuber, 1987;Bengio, 2000;Hochreiter et al., 2001), there has recently been a surge of interest in metalearning (e.g. Maclaurin et al., 2015;Andrychowicz et al., 2016;Finn et al., 2017). This is due to the methods meta-learning provides, amongst other things, for producing models that perform well beyond the confines of a single task, outside the constraints of a static dataset, or simply with greater data efficiency or sample complexity. Due to the wealth of options in what could be considered \"meta-\" to a learning problem, the term itself may have been used with some degree of underspecification. However, it turns out that many meta-learning approaches, in particular in the recent literature, follow the pattern of optimizing the \"meta-parameters\" of the training process by nesting one or more inner loops in an outer training loop. Such nesting enables training a model for several steps, evaluating it, calculating or approximating the gradients of that evaluation with respect to the meta-parameters, and subsequently updating these meta-parameters. This paper makes three contributions. First, we propose a formalization of this general process, which we call Generalized Inner Loop Meta-Learning (GIMLI), and show that it subsumes several recent approaches. The proposed formalism provides tooling for analysing the provable requirements of the meta-optimization process, and allows for describing the process in general terms. Second, we derive a general algorithm that supports the implementation of various kinds of metalearning fitting within the GIMLI framework and its requirements. Third, based on this analysis and algorithm, we describe a lightweight PyTorch library that enables the straightforward implementation of any meta-learning approach that fits within the GIMLI framework in canonical PyTorch, such that existing codebases require minimal changes, supporting third party module implementations and a variety of optimizers. Through a set of indicative experiments, we showcase the sort of research directions that are facilitated by our formalization and the corresponding library.\n\nThe overarching aim of this paper is not-emphatically-to purport some notion of ownership or precedence in any sense over existing efforts by virtue of having proposed a unifying formulation. Rather, in pointing out similarities under such unification, it provides theoretical and practical tools for facilitating further research in this exciting domain.\n\n\nGENERALIZED INNER LOOP META-LEARNING\n\nWhereby \"meta-learning\" is taken to mean the process of \"learning to learn\", we can describe it as a nested optimization problem according to which an outer loop optimizes meta-variables controlling the optimization of model parameters within an inner loop. The aim of the outer loop should be to improve the meta-variables such that the inner loop produces models which are more suitable according to some criterion. In this section, we will formalize this process, which we call Generalized Inner Loop Meta-Learning (GIMLI). We will define our terms in Section 2.1, give a formal description of the inner and outer loop's respective optimization problems in Section 2.2 and 2.3, followed by a description and proof of the requirements under which the process can be run in Section 2.4. Finally, in Section 2.5 we describe an algorithm which permits an efficient and exact implementation of GIMLI, agnostic to the model and optimizers used (subject to the requirements of Section 2.4 being satisfied).\n\n\nDEFINITIONS\n\nWithout loss of generality, let us assume we wish to train a model parameterized by \u03b8. Let \u03d5 describe some collection of possibly unrelated \"meta-parameters\" describing some aspect of the process by which we train our model. These could be, for example the learning rate or some other real-valued hyperparameters, task loss weights in multi-task learning, or perhaps the initialization of the model weights at the beginning of training. For disambiguation, we will refer to the subset of \u03d5 describing optimization parameters by \u03d5 opt , and those meta-parameters which parameterize aspects of the training loss calculation at every step (e.g. the loss itself, the task mixture, regularization terms, etc) by \u03d5 loss , i.e. \u03d5 = \u03d5 opt \u222a \u03d5 loss . We will give several concrete examples in Section 3.\n\nLet L train be a function of \u03b8 and \u03d5 corresponding to the overall training objective we seek to minimize directly or indirectly (e.g. via an upper bound, a Monte Carlo estimate of an expectation, etc). Other elements of the objective such as the dataset, and hyperparameters or other aspects of training not covered by \u03d5, are assumed to be implicit and held fixed in L train for notational simplicity. With this, the process by which we train a model to obtain test-time parameters \u03b8 * can be formalized as shown in Equation 1 1 .\n\u03b8 * = argmin(\u03b8; L train (\u03b8, \u03d5))(1)\nWe assume that this search will be performed using an iterative gradient-based method such as some form of gradient descent as formalized in Section 2.2, and thus that the derivative \u2207 \u03b8 L train (\u03b8, \u03d5) exists, as an obvious requirement.\n\nFurthermore, and without loss of generality, let L val be a function exclusively of \u03b8 (except the specific case where \u03d5 describes some aspect of the model, e.g. initialization, as is done in MAML (Finn et al., 2017)) describing some objective we wish to measure the post-training performance of the model against. This could be a validation set from the same task the model was trained on, from a collection of other tasks, or some other measure of the quality of the model parameterized by \u03b8 * . Typically, the aforementioned iterative process is not run to convergence, but rather to the point where an intermediate set of model parameters is considered satisfactory according to some criterion (e.g. best model under L val for a fixed budget of training steps, or after no improvement is seen for a certain number of training steps, etc.), and these intermediate parameters will serve as \u03b8 * .\n\n\nTRAINING\n\nThe process by which we typically estimate \u03b8 * against L train decomposes into a sequence of updates of \u03b8. At timestep t, we compute \u03b8 t+1 from \u03b8 t by first computing a step-specific loss train t (\u03b8 t , \u03d5 loss ). Note that this loss may itself also be a function of step-specific factors, such as the training data used for that timestep, which we leave implicit here for notational simplicity. Following this, we typically compute or estimate (e.g. as in reinforcement learning) the gradients \u2207 \u03b8t train t (\u03b8 t , \u03d5 loss ) of this loss with regard to \u03b8 t , by using algorithms such as backpropagation (Rumelhart et al., 1985). We then typically use an optimizer to \"apply\" these gradients to the parameters \u03b8 t to obtain updated parameters \u03b8 t+1 . As such, optimization processes such as e.g. Adagrad (Duchi et al., 2011) may be stateful, in that they exploit gradient history to produce adaptive \"local\" learning rates. Because some aspects of the optimization process may be covered by our choice of \u03d5, we denote this optimization step at time-step t by opt t as shown in Equation 2, where timestep-specific attributes of the optimizer are left implicit by our use of the step subscript.\n\u03b8 t+1 = opt t (\u03b8 t , \u03d5 opt , G t ) where G t = \u2207 \u03b8t train t (\u03b8 t , \u03d5 loss )(2)\nFor example, where we might use SGD as an optimizer, with the learning-rate being a meta-variable \u03d5 opt , we could instantiate equation 2 as follows:\nopt t (\u03b8 t , \u03d5 opt , G t ) := \u03b8 t \u2212 \u03d5 opt \u00b7 G t where G t = \u2207 \u03b8t train t (\u03b8 t , \u03d5 loss )\nThe estimation of \u03b8 * from Equation 1 using T + 1 training updates according to Equation 2 yields a double recurrence in both \u03b8 t and G t (as it is a function of \u03b8 t , as outlined in Equation 3).\n\u03b8 * \u2248 opt T (\u03b8 T , \u03d5 opt , G T ) = opt T (P T , \u03d5 opt , \u2207 \u03b8 T train T (P T , \u03d5 loss )) where P T = opt T \u22121 (\u03b8 T \u22121 , \u03d5 opt , G T \u22121 ) = opt T \u22121 (P T \u22121 , \u03d5 opt , \u2207 \u03b8 T \u22121 train T \u22121 (P T \u22121 , \u03d5 loss )) . . . where P 1 = opt 0 (\u03b8 0 , \u03d5 opt , G 0 ) = opt 0 (\u03b8 0 , \u03d5 opt , \u2207 \u03b80 train 0 (\u03b8 0 , \u03d5 loss ))(3)\nFrom this we see that the optimization process used to train a variety of model types, with a variety of optimization methods, can be described as a function yielding test-time model parameters \u03b8 * potentially as a function of parameter history \u03b8 1 , . . . , \u03b8 T and of meta-parameters \u03d5 (if any exist). While this may seem like a fairly trivial formalization of a ubiquitous training process, we will see in Section 2.4 that if a few key requirements are met, this process can be nested as the inner loop within an outer loop containing-amongst other things-a meta-training process. From this process, described in Section 2.3, we estimate values of \u03d5 which improve our training process against some external metric.\n\n\nMETA-TRAINING\n\nWe now describe the outer loop optimization problem which wraps around the inner loop described in Section 2.2. Through this process, we seek a value \u03d5 * which ensures that the training process L train (\u03b8, \u03d5 * ) produces parameters \u03b8 * which perform best against some metric L val (\u03b8 * ) which we care about. The formalization and decomposition of this \"meta-training process\" into nested optimization problems is shown in Equation 4.\n\u03d5 * = argmin \u03d5; L val (\u03b8 * ) = argmin \u03d5; L val argmin \u03b8; L train (\u03b8, \u03d5)(4)\nIn this section, we introduce a formalization of an iterative process allowing us to approximate this nested optimization process. Furthermore, we describe a general iterative algorithm by which the process in Equation 4 can be approximated by gradient-based methods while jointly estimating \u03b8 * according to the process in Equation 1.\n\nAn iterative process by which we can estimate \u03d5 * given Equation 4 following the sort of decomposition of the training process described Equation 1 into the training process described in Equations 2-3 is described below. Essentially, an estimate of \u03b8 * is obtained following T + 1 steps training as outlined in Equation 3, which is then evaluated against L val . The gradient of this evaluation, \u2207 \u03d5 L val (\u03b8 * ) is then obtained through backpropagation, and used to update \u03d5. Using \u03c4 as time-step counter to symbolize the time-scale being different from that used in the \"inner loop\", we formalize this update in Equation 5 where metaopt \u03c4 denotes the optimization process used to update \u03d5 \u03c4 using M \u03c4 at that time-step.\n\u03d5 \u03c4 +1 = metaopt \u03c4 (\u03d5 \u03c4 , M \u03c4 ) where M \u03c4 = \u2207 \u03d5\u03c4 L val (\u03b8 * ) (5) = metaopt \u03c4 (\u03d5 \u03c4 , \u2207 \u03d5\u03c4 L val (opt T (P T , \u03d5 opt , G T ))) where P T = opt T \u22121 (P T \u22121 , \u03d5 opt , \u2207 \u03b8 T \u22121 train T \u22121 (P T \u22121 , \u03d5 loss )) . . . where P 1 = opt 0 (\u03b8 0 , \u03d5 opt , G 0 )\n\nKEY REQUIREMENTS\n\nFor gradient-based meta-learning as formalized in Section 2.3-in particular through the process formalized in Equation 5-to be possible, a few key requirements must be met. We enumerate them here, and then discuss the conditions under which they are met, either analytically or through choice of model, loss function, or optimizer.\n\nI. L val is a differentiable function of its input, i.e. the derivative \u2207 \u03b8 * L val (\u03b8 * ) exists. II. The optimization process opt in Equation 2 is a differentiable 2 function of \u03b8 and G. III. Either or both of the following conditions hold:\n\n(a) there exist continuous optimization hyperparameters (e.g. a learning rate \u03b1) covered by \u03d5 opt (e.g. \u03b1 \u2286 \u03d5 opt ) and opt in Equation 2 is a differentiable function of \u03d5 opt , or (b) the gradient G t for one or more time-steps in Equations 1-3 is a function of \u03d5 loss (i.e. the derivative \u2207 \u03d5 loss train (\u03b8, \u03d5 loss ) exists).\n\nIn the remainder of this section, we show that based on the assumptions outlined in Section 2.1, namely the existence of gradients on the validation objective L val with regard to model parameters \u03b8 and of gradients on the training objective L train with regard to both model parameters \u03b8 and metaparameters \u03d5, there exist gradients on the validation objective L val with regard to \u03d5. We will then, in the next section, demonstrate how to implement this process by specifying an update algorithm for meta-variables.\n\nImplementing an iterative process as described in Equation 5 will exploit the chain rule for partial derivatives in order to run backpropagation. The structure of the recurrence means we need to ensure that \u2207 \u03b8t L val (\u03b8 * ) exists for t \u2208 {0, . . . , T } in order to compute, for all such \u03b8 t , gradient paths (\u2207 \u03b8t L val (\u03b8 * )) \u00b7 (\u2207 \u03d5 \u03b8 t ). We can prove this exists in virtue of the chain rule for partial derivatives and the requirements above:\n1. By the chain rule, \u2207 \u03b8t L val (\u03b8 * ) = (\u2207 \u03b8 * L val (\u03b8 * )) \u00b7 (\u2207 \u03b8t \u03b8 * ) exists if \u2207 \u03b8 * L val (\u03b8 * ) ex- ists (and it does, by Requirement I) and \u2207 \u03b8t \u03b8 * = \u2207 \u03b8t \u03b8 T +1 exists. 2. Idem, \u2207 \u03b8t \u03b8 T +1 = (\u2207 \u03b8t+1 \u03b8 T +1 ) \u00b7 (\u2207 \u03b8t \u03b8 t+1 ) exists by recursion over t if for all i \u2208 [1, T ], \u2207 \u03b8i \u03b8 i+1 exists, which is precisely what Requirement II guarantees.\nTherefore \u2207 \u03b8t L val (\u03b8 * ) exists for t \u2208 {0, . . . , T }, leaving us to demonstrate that \u2207 \u03d5 \u03b8 t is defined for all relevant values of t as a consequence of requirements I-III. We separately consider the case of \u03d5 loss and \u03d5 opt as defined in Section 2.1:\n\n1. For \u2207 \u03d5 opt \u03b8 t , the gradients trivially exist as a consequence of Requirement IIIa. 2. For \u2207 \u03d5 loss \u03b8 t , by the chain rule, \u2207 \u03d5 loss \u03b8 t = (\u2207 Gt\u22121 \u03b8 t ) \u00b7 (\u2207 \u03d5 loss G t\u22121 ). From Requirement II, it follows that \u2207 Gt\u22121 \u03b8 t exists, and from Requirement IIIb, it follows that \u2207 \u03d5 loss G t\u22121 exists, therefore so does \u2207 \u03d5 loss \u03b8 t .\n\nPutting these both together, and having covered the union of \u03d5 opt and \u03d5 loss by exhaustion, we have shown that the gradients \u2207 \u03d5 L val (\u03b8 * ) can be obtained by composition over the gradient paths\n(\u2207 \u03b8t L val (\u03b8 * ))(\u2207 \u03d5 \u03b8 t ) for all t \u2208 [1, T ].\nIn Section 2.5 we show how to implement the exact and efficient calculation of \u2207 \u03d5 L val (\u03b8 * ). To complete this section, we indicate the conditions under which requirements I-III hold in practice.\n\nRequirement I is simply a function of the choice of evaluation metric used to evaluate the model after training as part of L val . If this is not a differentiable function of \u03b8 * , e.g. BLEU (Papineni et al., 2002) in machine translation, then a proxy metric can be selected for meta-training (e.g. negative log-likelihood of held out data), or gradient estimation methods such as REINFORCE (Williams, 1992) can be used.\n\nRequirement II is a function of the choice of optimizer, but is satisfied for most popular optimizers. We directly prove that this requirement holds for SGD (Robbins & Monro, 1951) and for ADA-GRAD (Duchi et al., 2011) in Appendix A, and prove it by construction for a wider class of common optimizers in the implementation and tests of the software library described in Section 4.\n\nRequirement IIIa is a function of the choice of hyperparameters and optimizer, but is satisfied for at least the learning rate in most popular optimizers. Requirement IIIb is a function of the choice of loss function train (or class thereof), in that \u2207 \u03b8t train t (\u03b8 t , \u03d5 loss ) needs to exist and be a differentiable function of \u03d5. Usually, this requirement is held where \u03d5 is a multiplicative modifier of \u03b8 t . For algorithms such as Model Agnostic Meta-Learning (Finn et al., 2017, MAML), this requirement is equivalent to saying that the Hessian of the loss function with regard to the parameters exists.\n\n\nTHE GIMLI UPDATE ALGORITHM\n\nIn Algorithm 1, we present the algorithm which permits the computation of updates to metavariables through the nested iterative optimization process laid out above. To clearly disentangle different gradient paths, we employ the mathematical fiction that is the \"stop-gradient\" operator, which is defined as an operator which maintains the truth of the following expression:\n\u2212 stop(x) : \u2200f f (x) stop(x) = f (x) \u2227 \u2207 x f (x) stop(x) = 0\nAs will be shown below, this will allow us to decompose the computation of updates to the metavariables through an arbitrarily complex training process, agnostic to the models and optimizers used (subject to the requirements of Section 2.4 being satisfied), into a series of local updates passing gradient with regard to the loss back through the inner loop steps. This is akin to backpropagation through time (BPTT; Rumelhart et al., 1985), a method which has been adapted to other nested optimization processes with various constraints or restrictions (Andrychowicz et al., 2016;Franceschi et al., 2017;.\n\nEach iteration through the loop defined by lines 1-17 does one gradient-based update of metaparameters \u03d5 using the optimizer employed in line 16. Each such iteration, we first (line 3) copy the model and optimizer state (generally updated through some outer loop within which this update loop sits). We then (lines 4-7) compute a series of J updates on a copy of the model, preserving the intermediate gradient computations G 0 , . . . , G J\u22121 , intermediate model parameters \u03b8 0 , . . . , \u03b8 J (sometimes confusingly referred to as \"fast weights\", following Hinton & Plaut (1987), within the meta-learning literature), and all associated activations. These will be reused in the second stage (lines 10-15) to backpropagate higher-order gradients of the meta-loss, computed on line 9 through the optimization process that was run in lines 4-7. In particular, in lines 12 and 13, local (i.e. time step-specific) gradient calculations compute part of the gradient of \u2207 \u03d5 opt i L val (\u03b8 J ) and \u2207 \u03d5 loss i L val (\u03b8 J ), which is stored in accumulators which contain the exact respective gradients by the end of loop. What permits this efficient local computation is the dynamic programming calculation of the partial derivative B j = \u2207 \u03b8 j as function of only B j +1 and timestep-specific gradients, implementing a second-order variant of BPTT.\n\n\nAlgorithm 1 The GIMLI update loop\n\nRequire: Current model parameters \u03b8t, meta-parameters \u03d5\u03c4 Require: Number of meta-parameter updates I, length of unrolled inner loop J 1: for i = 0 to I \u2212 1 do 2: Segment meta-parameters \u03d5 opt i , \u03d5 loss i \u2190 split(\u03d5\u03c4+i) 3:\n\nCopy model state \u03b8 0 \u2190 \u03b8t, optimizer state opt 0 \u2190 opt t 4:\n\nfor j = 0 to J \u2212 1 do 5:\n\nCompute inner gradient Gj \u2190 \u2207 \u03b8 j train t+j (\u03b8 j , \u03d5 loss i ) and retain gradient graph state 6: Update inner model \u03b8 j+1 \u2190 opt j (\u03b8 j , \u03d5 opt i , Gj) 7:\n\nend for 8:\nInitialize accumulators: A opt i \u2190 zerosLike(\u03d5 opt i ); A loss i \u2190 zerosLike(\u03d5 loss i ) 9: Compute BJ \u2190 \u2207 \u03b8 J L val (\u03b8 J ) 10: for j = J \u2212 1 to 0 do 11: Compute optimizer-gradient derivative O j \u2190 \u2207G j opt j (\u03b8 j , \u03d5 opt i , G j ) 12: Update A opt i \u2190 A opt i + B j +1 \u00b7 (\u2207 \u03d5 opt i opt j ( \u03b8 j stop(\u03d5 opt i ) , \u03d5 opt i , G j stop(\u03d5 opt i ) )) 13: Update A loss i \u2190 A loss i + B j +1 \u00b7 O j \u00b7 (\u2207 \u03d5 loss i \u2207 \u03b8 j train t+j ( \u03b8 j stop(\u03d5 loss i ) , \u03d5 loss i )) 14: Compute B j \u2190 B j +1 \u00b7 ((\u2207 \u03b8 j opt j (\u03b8 j , \u03d5 opt i , G j stop(\u03b8 j ) )) + O j \u00b7 (\u2207 2 \u03b8 j train t+j (\u03b8 j , \u03d5 loss i )))\n15: end for 16:\n\nUpdate meta-parameters \u03d5\u03c4+i+1 \u2190 metaopt(\u03d5\u03c4+i, join(A opt i , A loss i )) 17: end for 18: return Updated meta-parameters \u03d5\u03c4+I\n\n\nEXAMPLES AND RELATED WORK\n\nIn this section, we highlight some instances of meta-learning which are instances of GIMLI, before discussing related approaches involving support for nested optimization, with applications to similar problems. The aim is not to provide a comprehensive literature review, which space would not permit. Rather, in pointing out similarity under our GIMLI formulation, we aim to showcase that rich and diverse research has been done using this class of approaches, where yet more progress indubitably remains to be made. This is, we believe, the strongest motivation for the development of libraries such as the one we present in Section 4 to support the implementation of algorithms that fall under the general algorithm derived in Section 2.\n\n\nEXAMPLES\n\nMany of the papers referenced below contain excellent and thorough reviews of the literature most related to the type of meta-learning they approach. In the interest of brevity, we will not attempt such a review here, but rather focus on giving examples of a few forms of meta-learning that fit the GIMLI framework (and thus are supported by the library presented in Section 4), and briefly explain why.\n\nOne popular meta-learning problem is that of learning to optimize hyperparameters through gradient-based methods (Bengio, 2000;Maclaurin et al., 2015;Luketina et al., 2016;Franceschi et al., 2017), as an alternative to grid/random search (Bergstra & Bengio, 2012) or Bayesian Optimization (Mockus et al., 1978;Pelikan et al., 1999;Bergstra et al., 2011;Snoek et al., 2012). Here, select continuous-valued hyperparameters are meta-optimized against a meta-objective, subject to the differentiability of the optimization step, and, where relevant, the loss function. This corresponds to Requirements II and IIIa of Section 2.4 being met, i.e. GIMLI being run with select optimizer hyperparameters as part of \u03d5 opt .\n\nA related problem is that of learning the optimizer wholesale as a parametric model (Hochreiter et al., 2001;Andrychowicz et al., 2016;Duan et al., 2016), typically based on recurrent architectures. Again, here the optimizer's own parameters are the optimizer hyperparameters, and constitute the entirety of \u03d5 opt as used within GIMLI. Requirements II and IIIa are trivially met through the precondition that such optimizers models have parameters with regard to which their output (and losses that are a function thereof) is differentiable.\n\nMore recently, meta-learning approaches such as MAML (Finn et al., 2017;Antoniou et al., 2018) and its variants/extensions have sought to use higher-order gradients to meta-learn model/policy initializations in few-shot learning settings. In GIMLI, this corresponds to setting \u03b8 0 = \u03d5 loss , which then is not an explicit function of train in Equation 3, but rather is implicitly its argument through updates to the inner model over the unrolled optimization. All requirements in Section 2.4 must be satisfied (save IIIa, with Requirement IIIb further entailing that train be defined such that the second derivative of the function with regard to \u03b8 exists (i.e. is non-zero).\n\nFinally, recent work by Bechtle et al. (2019) has introduced the ML 3 framework for learning unconstrained loss functions as parametric models, through exploiting second-order gradients of a meta-loss with regard to the parameters of the inner loss. This corresponds, in GIMLI, to learning a parametric model of the loss parameterized by \u03d5 loss .\n\n\nRELATED WORK\n\nIn a sense, many if not all of the approaches discussed in Section 3.1 qualify as \"related work\", but here we will briefly discuss approaches to the general problem of formalizing and supporting implementations of problems that fit within the nested optimization specified by GIMLI.\n\nThe first is work by Franceschi et al. (2018) which describes how several meta-learning and hyperparameter optimization approaches can be cast as a bi-level optimization process, akin to our own formalization in 2.3. This fascinating and relevant work is highly complementary to the formalization and discussion presented in our paper. Whereas we focus on the requirements according to which gradient-based solutions to approaches based on nested optimization problems can be found in order to drive the development of a library which permits such approaches to be easily and scalably implemented, their work focuses on analysis of the conditions under which exact gradient-based solutions to bi-level optimization processes can be approximated, and what convergence guarantees exist for such guarantees. In this sense, this is more relevant to those who wish to analyze and extend alternatives to first-order approximations of algorithms such as MAML, e.g. see work of Nichol & Schulman (2018) or Rajeswaran et al. (2019).\n\nOn the software front, the library learn2learn (Arnold et al., 2019) 3 addresses similar problems to that which we will present in Section 4. This library focuses primarily on providing implementations of existing meta-learning algorithms and their training loops that can be extended with new models. In contrast, the library we present in Section 4 is \"closer to the metal\", aiming to support the development of new meta-learning algorithms fitting the GIMLI definitions with as little resort to non-canonical PyTorch as possible. A recent parallel effort, Torchmeta (Deleu et al., 2019) also provides a library aiming to assist the implementation of meta-learning algorithms, supplying useful data-loaders for meta-training. However, unlike our approach described in 4, it requires re-implementation of models using their functional/stateless building blocks, and for users to reimplement the optimizers in a differentiable manner.\n\n\nTHE higher LIBRARY\n\nIn this section, we provide a high-level description of the design and capabilities of higher, 4 a PyTorch (Paszke et al., 2017) library aimed at enabling implementations of GIMLI with as little reliance on non-vanilla PyTorch as possible. In this section, we first discuss the obstacles that would prevent us from implementing this in popular deep learning frameworks, how we overcame these in PyTorch to implement GIMLI. Additional features, helper functions, and other considerations when using/extending the library are provided in its documentation.\n\n\nOBSTACLES\n\nMany deep learning frameworks offer the technical functionality required to implement GIMLI, namely the ability to take gradients of gradients. However, there are two aspects of how we imple-ment and train parametric models in such frameworks which inhibit our ability to flexibly implement Algorithm 1.\n\nThe first obstacle is that models are typically implemented statefully (e.g. torch.nn in PyTorch, keras.layers in Keras (Chollet et al., 2015), etc.), meaning that the model's parameters are encapsulated in the model implementation, and are implicitly relied upon during the forward pass. Therefore while such models can be considered as functions theoretically, they are not pure functions practically, as their output is not uniquely determined by their explicit input, and equivalently the parameters for a particular forward pass typically cannot be trivially overridden or supplied at call time. This prevents us from tracking and backpropagating over the successive values of the model parameters \u03b8 within the inner loop described by Equation 3, through an implicit or explicit graph.\n\nThe second issue is that, as discussed at the end of Section 2.4 and in Appendix A, even though the operations used within popular optimizers are mathematically differentiable functions of the parameters, gradients, and select hyperparameters, these operations are not tracked in various framework's implicit or explicit graph/gradient tape implementations when an optimization step is run. This is with good reason: updating model parameters in-place is memory efficient, as typically there is no need to keep references to the previous version of parameters. Ignoring the gradient dependency formed by allowing backpropagation through an optimization step essentially makes it safe to release memory allocated to historical parameters and intermediate model states once an update has been completed. Together, these obstacles essentially prevent us from practically satisfying Requirement II of Section 2.4.\n\n\nMAKING STATEFUL MODULES STATELESS\n\nAs we wish to track and backpropagate through intermediate states of parameters during the inner loop, we keep a record of such states which can be referenced during the backward pass stage of the outer loop in Algorithm 1. The typical way this is done in implementations of meta-learning algorithms such as MAML is to rewrite a \"stateless\" version of the inner loop's model, permitting the use, in each invocation of the model's forward pass, of weights which are otherwise tracked on the gradient graph/tape. While this addresses the issue, it is an onerous and limiting solution, as exploring new models within such algorithms invariably requires their reimplementation in a stateless style. This typically prevents the researcher from experimenting with third-party codebases, complicated models, or those which requiring loading pre-trained weights, without addressing a significant and unwelcome engineering challenge.\n\nA more generic solution, permitting the use of existing stateful modules (including with pre-loaded activations), agnostic to the complexity or origin of the code which defines them, is to modify the run-time instance of the model's parent class to render them effectively function, a technique often referred to as \"monkey-patching\". The high-level function higher.monkeypatch() does this by taking as argument a torch.nn.Module instance and the structure of its nested sub-modules. As it traverses this structure, it clones the parent classes of submodule instances, leaving their functionality intact save for that of the forward method which implements the forward pass. Here, it replaces the call to the forward method with one which first replaces the stateful parameters of the submodule with ones provided as additional arguments to the patched forward, before calling the original class's bound forward method, which will now used the parameters provided at call time. This method is generic and derived from first-principles analysis of the torch.nn.Module implementation, ensuring that any first or third-party implementation of parametric models which are subclasses of torch.nn.Module and do not abuse the parent class at runtime will be supported by this recursive patching process.\n\n\nMAKING OPTIMIZERS DIFFERENTIABLE\n\nAgain, as part of our need to make the optimization process differentiable in order to satisfy Requirement II of Section 2.4, the typical solution is to write a version of SGD which does not modify parameters in-place, but treated as a differentiable function of the input parameters and hyperparameters akin to any other module in the training process. While this, again, is often considered a satisfactory solution in the meta-learning literature due to its simplicity, it too is limiting. Not only does the inability to experiment with other inner loop optimizers prevent research into the applicability of meta-learning algorithms to other optimization processes, the restriction to SGD also means that existing state-of-the-art methods used in practical domains cannot be extended using meta-learning methods such as those described in Section 3, lest they perform competitively when trained with SGD.\n\nHere, while less generic, the solution provided by the high-level function higher.get diff optim() is to render a PyTorch optimizer instance differentiable by mapping its parent class to a differentiable reimplementation of the instance's parent class. The reimplementation is typically a copy of the optimizer's step logic, with in-place operations being replaced with gradient-tracking ones (a process which is syntactically simple to execute in PyTorch). To this, we add wrapper code which copies the optimizer's state, and allows safe branching off of it, to permit \"unrolling\" of the optimization process within an inner loop (cf. the recurrence from Equation 3) without modifying the initial state of the optimizer (e.g. to permit several such unrolls, or to preserve state if inner loop optimizer is used elsewhere in the outer loop). Most of the optimizers in torch.optim are covered by this method. Here too, a runtime modification of the parent optimizer class could possibly be employed as was done for torch.nn.Modules, but this would involve modifying Python objects at a far finer level of granularity. We find that supporting a wide and varied class of optimizers is a sufficient compromise to enable further research. 5\n\n\nEXPERIMENTS\n\nIn this section, we briefly present some results of experiments using select existing methods from Section 3.1, to show case how higher can be used to simply implement ablation studies and searches over model architectures, optimizers, and other aspects of an experiment. This could, naturally, be done without appeal to the library. However, in such cases, changing the model architecture or optimizer requires re-implementing the model functionally or optimization step differentiably. Here such changes require writing no new code (excluding the line where the model is defined).\n\n\nMETA-LEARNING LEARNING RATES WITH HIGHER GRANULARITY\n\nAs the first application for higher, we set up a simple meta-learning task where we meta-optimize the learning rate. Employing a handcrafted annealing schedule for learning rates has been the de facto approach to improving a learning algorithm. While scheduled annealing can help to boost the final performance, it usually requires significant hand-tuning of the schedule. In contrast, we adjust learning rates automatically using meta-learning, which higher enables for arbitary models and optimizers.\n\nWe take an image classification model DenseNet-BC(k=12) (Huang et al., 2016), that provides competitive state-of-the-art results on CIFAR10, and modify its training procedure by replacing the multi-step annealing schedule of learning rate with a meta-optimizer. Specifically, we treat learning rate for each inner optimizer's parameter group as a separate meta-parameter that we will metalearn. Doing so allows individual model parameters to have finer learning rates, that are adjusted automatically. The GIMLI algorithm provides for a clean implementation of such training procedure.\n\nWe first train two baseline variants of DenseNet-BC(k=12) using setup from (Huang et al., 2016). In the first variant we keep the learning rate fixed to 0.1 for all 300 epochs, while the second configuration takes advantage of a manually designed multi-step annealing schedule, which drops learning rate by 10 after 150 and 225 epochs. For the meta-learning variant, we split the training set into two disjoint pieces, one for training and another for validation, in proportion of 99 : 1. We then use per parameter group learning rates (299 in total) for meta-optimization, initializing each learning rate to 0.1. We perform one meta-update step after each epoch, where we unroll inner optimizer for 15 steps using batches from the training set, and compute meta-test error on the validation set over 10 batches from the validation set. We use batch size of 16 for meta-update, rather than 64 as in the base training loop. We use Adam (Kingma & Ba, 2014) with default parameters as a choice for meta-optimizer. We average results over 3 random seeds. Figure 1 demonstrates that our method is able reach the state-of-the-art performance faster. Learning rates Figure 1: Comparison of meta-learned learning rates against fixed and multi-step annealed for training DenseNet-BC(k=12) on CIFAR10. We observe convergence near state-of-the-art with better sample complexity that using a hand-designed annealing schedule. The higher library enables the exploration of new MAML-like models and inner-loop optimizers, which historically has required non-trivial implementations of the fast-weights for the model parameters and inner optimizers as done in Antoniou et al. (2018); Deleu et al. (2019). These ablations can be important for squeezing the last few bits of accuracy on well-established tasks and baselines that are already near-optimal as shown in Chen et al. (2019), and is even more important for developing new approaches and tasks that deal with different kinds of data and require adaptation to be done over non-standard operations.\n\nTo illustrate how easy higher makes these ablations, in this section we take a closer look at different model architecture and optimizer choices for the MAML++ approach (Antoniou et al., 2018). MAML++ uses a VGG network with a SGD inner optimizer for the the Omniglot (Lake et al., 2015) and Mini-Imagenet (Vinyals et al., 2016;Ravi & Larochelle, 2016) tasks. We start with the official MAML++ code and evaluation procedure and use higher to ablate across VGG, ResNet, and DenseNet models and SGD and Adam optimizers. We provide more details about these in Appendix B. We denote the combination of model and inner optimizer choice with <model>+<opt>. One finding of standalone interest is that we have kept most of the features from MAML++, except we significantly increase the base VGG+SGD performance by using batch normalization in training mode everywhere as in (Finn et al., 2017) instead of using per-timestep statistics and parameters as MAML++ proposes. In theory, this enables more inner optimization steps to be rolled out at test time, which otherwise is not possible with MAML++ because of the per-timestep information, for simplicity in this paper we have not explored this and keep every algorithm, optimizer, and mode to five inner optimization steps. When using Adam as the inner optimizer, we initialize the first and second moment statistics to the statistics from the outer optimizer, which is also Adam, and learn per-parameter-group learning rates and rolling average \u03b2 coefficients. Our results in Table 1 show that we are able to push the accuracy of MAML++ slightly up with this ablation. We note that this pushes the performance of MAML++ closer to that of state-of-the-art methods such as LEO (Rusu et al., 2018). Appendix B shows our full experimental results, and we note that in some cases Adam slightly outperforms SGD for a particular model.\n\n\nCONCLUSION\n\nTo summarize, we have presented GIMLI, a general formulation of a wide class of existing and potential meta-learning approaches, listed and proved the requirements that must be satisfied for such approaches to be possible, and specified a general algorithmic formulation of such approaches. We've described a lightweight library, higher, which extends PyTorch to enable the easy and natural implementation of such meta-learning approaches at scale. Finally we've demonstrated some of its potential applications. We hope to have made the case not only for the use of the mathematical and software tools we present here, but have also provided suitable encouragement for other researchers to use them and explore the boundaries of what can be done within this broad class of meta-learning approaches. Table 2 shows all of the architectures and optimizers we ablated. The table is missing some rows as we only report the results that successfully completed running three seeds within three days on our cluster.\n\n\nB MAML++ EXPERIMENTS: ADDITIONAL INFORMATION\n\nWe use the following model architectures, which closely resemble the vanilla PyTorch examples for these architectures but are modified to be smaller for the few-shot classification setting:\n\n\u2022 vgg is the VGG architecture (Simonyan & Zisserman, 2014) variant used in MAML++ (Antoniou et al., 2018) \u2022 resnet-N is the ResNet architecture (He et al., 2016). resnet-4 corresponds to the four blocks having just a single layer, and resnet-8 and resnet-12 have respectively 2 and 3 layers in each block\n\n\u2022 densenet-8 is the DenseNet architecture (Huang et al., 2017) with 2 layers in each block\n\nWe follow TADAM (Oreshkin et al., 2018) and do not use the initial convolutional projection layer common in the full-size variants of the ResNet and DenseNet.\n\nWe can also visualize how the learning rates and rolling momentum terms (with Adam) change over time when they are being learned. We show this in Figure 2 and Figure 3 for the 20-way 1-shot Omniglot experiment with the resnet-4 architecture, which is especially interesting as Adam outperforms SGD in this case. We find that most of the SGD learning rates are decreased to near-zero except for a few select few that seem especially important for the adaptation. Adam exhibits similar behavior with the learning rates where most except for a select for are zeroed for the adaptation, and the rolling moment coefficients are particularly interesting where the first moment coefficient \u03b2 1 becomes relatively evenly spread throughout the space while \u03b2 2 splits many parameter groups between low and high regions of the space.  \n\nFigure 2 :Figure 3 :\n23The learning rates during a training run of a VGG network with SGD as the inner optimizer for 20-way 1-shot mini-imagenet classification. The learning rates during a training run of a VGG network with Adam as the inner optimizer for 20-way 1-shot mini-imagenet classification.\n\nTable 1 :\n1Results from ablating MAML++ architecture and inner optimizers. \"Our base\" results are from our VGG model trained with SGD in our MAML++ variant, and \"our best\" results show the best test accuracy found by our ablation, with the best model and optimizer combination shown in the text belowAntoniou et al., 2018) 99.53 \u00b1 0.26% 99.93 \u00b1 0.09% 97.65 \u00b1 0.05% 99.33 \u00b1 0.03% 52.15 \u00b1 0.26% 68.32 \u00b1 0.44 MAML++ (Our base) 99.62 \u00b1 0.08% 99.86 \u00b1 0.02% 97.21 \u00b1 0.11% 99.13 \u00b1 0.13% 56.33 \u00b1 0.27% 75.13 \u00b1 0.67% MAML++ (Our best) 99.91 \u00b1 0.05% 99.87 \u00b1 0.03% 99.00 \u00b1 0.33% 99.76 \u00b1 0.01% 56.33 \u00b1 0.27% 76.73 \u00b1 0.52% resnet-4+SGD resnet-4+SGD resnet-12+SGD resnet-8+SGD vgg+SGD resnet-8+SGD 5.2 ABLATING MAML'S MODEL ARCHITECTURE AND INNER OPTIMIZERomniglot test accuracy \nminiImageNet test accuracy \n5 Way \n20 Way \n5 Way \nApproach \n1 Shot \n5 Shot \n1 Shot \n5 Shot \n1 Shot \n5 Shot \nMAML++ (\n\nTable 2 :\n2Full MAML++ model and inner optimizer sweep search results.mean acc \nstd \n\nFor stateful optimizers, we assume that, where the state is itself a function of previous values of the network parameters (e.g. moving averages of parameter weights), it is included in the computation of gradients of the meta-loss with regard to those parameters.\nTo cite this effort, we have ordered authors as shown on the repository contributors page (https:// github.com/learnables/learn2learn/graphs/contributors). We hope this will not offend. 4 Available at https://github.com/facebookresearch/higher.\nWe also provide documentation as to how to write differentiable third party optimizers and supply helper functions such as higher.register optim() to register them for use with the library at runtime.\nwhere \u03b7 is the global learning rate, and\nLearning to learn by gradient descent by gradient descent. Marcin Andrychowicz, Misha Denil, Sergio Gomez, W Matthew, David Hoffman, Tom Pfau, Brendan Schaul, Nando De Shillingford, Freitas, Advances in neural information processing systems. Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul, Brendan Shillingford, and Nando De Freitas. Learning to learn by gradient descent by gradient descent. In Advances in neural information processing systems, pp. 3981-3989, 2016.\n\nHow to train your maml. Antreas Antoniou, Harrison Edwards, Amos Storkey, arXiv:1810.09502arXiv preprintAntreas Antoniou, Harrison Edwards, and Amos Storkey. How to train your maml. arXiv preprint arXiv:1810.09502, 2018.\n\n. S\u00e9b Arnold, Praateek Mahajan, Debajyoti Datta, Ian Bunner, S\u00e9b Arnold, Praateek Mahajan, Debajyoti Datta, and Ian Bunner. learn2learn. https:// github.com/learnables/learn2learn, 2019.\n\nMeta-learning via learned loss. Sarah Bechtle, Artem Molchanov, Yevgen Chebotar, Edward Grefenstette, Ludovic Righetti, Gaurav Sukhatme, Franziska Meier, arXiv:1906.05374arXiv preprintSarah Bechtle, Artem Molchanov, Yevgen Chebotar, Edward Grefenstette, Ludovic Righetti, Gaurav Sukhatme, and Franziska Meier. Meta-learning via learned loss. arXiv preprint arXiv:1906.05374, 2019.\n\nGradient-based optimization of hyperparameters. Yoshua Bengio, Neural computation. 128Yoshua Bengio. Gradient-based optimization of hyperparameters. Neural computation, 12(8):1889- 1900, 2000.\n\nRandom search for hyper-parameter optimization. James Bergstra, Yoshua Bengio, Journal of Machine Learning Research. 13James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journal of Machine Learning Research, 13(Feb):281-305, 2012.\n\nAlgorithms for hyper-parameter optimization. S James, R\u00e9mi Bergstra, Yoshua Bardenet, Bal\u00e1zs Bengio, K\u00e9gl, Advances in neural information processing systems. James S Bergstra, R\u00e9mi Bardenet, Yoshua Bengio, and Bal\u00e1zs K\u00e9gl. Algorithms for hyper-parameter optimization. In Advances in neural information processing systems, pp. 2546-2554, 2011.\n\nA closer look at few-shot classification. Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, Jia-Bin Huang, Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A closer look at few-shot classification, 2019.\n\n. Fran\u00e7ois Chollet, Fran\u00e7ois Chollet et al. Keras, 2015.\n\nTorchmeta: A Meta-Learning library for PyTorch. Tristan Deleu, Tobias W\u00fcrfl, Mandana Samiei, Joseph Paul Cohen, Yoshua Bengio, Tristan Deleu, Tobias W\u00fcrfl, Mandana Samiei, Joseph Paul Cohen, and Yoshua Bengio. Torch- meta: A Meta-Learning library for PyTorch, 2019. URL https://arxiv.org/abs/1909. 06576. Available at: https://github.com/tristandeleu/pytorch-meta.\n\nRl2: Fast reinforcement learning via slow reinforcement learning. Yan Duan, John Schulman, Xi Chen, L Peter, Ilya Bartlett, Pieter Sutskever, Abbeel, arXiv:1611.02779arXiv preprintYan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl2: Fast reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016.\n\nAdaptive subgradient methods for online learning and stochastic optimization. John Duchi, Elad Hazan, Yoram Singer, Journal of Machine Learning Research. 12John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.\n\nModel-agnostic meta-learning for fast adaptation of deep networks. Chelsea Finn, Pieter Abbeel, Sergey Levine, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine Learning70Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In Proceedings of the 34th International Conference on Machine Learning- Volume 70, pp. 1126-1135. JMLR. org, 2017.\n\nForward and reverse gradient-based hyperparameter optimization. Luca Franceschi, Michele Donini, Paolo Frasconi, Massimiliano Pontil, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine Learning70Luca Franceschi, Michele Donini, Paolo Frasconi, and Massimiliano Pontil. Forward and reverse gradient-based hyperparameter optimization. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 1165-1173. JMLR. org, 2017.\n\nBilevel programming for hyperparameter optimization and meta-learning. Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, Massimilano Pontil, arXiv:1806.04910arXiv preprintLuca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimilano Pontil. Bilevel programming for hyperparameter optimization and meta-learning. arXiv preprint arXiv:1806.04910, 2018.\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog- nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016.\n\nUsing fast weights to deblur old memories. E Geoffrey, David C Hinton, Plaut, Proceedings of the ninth annual conference of the Cognitive Science Society. the ninth annual conference of the Cognitive Science SocietyGeoffrey E Hinton and David C Plaut. Using fast weights to deblur old memories. In Proceedings of the ninth annual conference of the Cognitive Science Society, pp. 177-186, 1987.\n\nLearning to learn using gradient descent. Sepp Hochreiter, Steven Younger, Peter R Conwell, International Conference on Artificial Neural Networks. SpringerSepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to learn using gradient descent. In International Conference on Artificial Neural Networks, pp. 87-94. Springer, 2001.\n\n. Gao Huang, Zhuang Liu, Laurens Van Der Maaten, Kilian Q Weinberger, CoRRGao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected convolutional networks. CoRR, 2016.\n\nDensely connected convolutional networks. Gao Huang, Zhuang Liu, Laurens Van Der Maaten, Kilian Q Weinberger, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionGao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4700-4708, 2017.\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980arXiv preprintDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n\nHuman-level concept learning through probabilistic program induction. Ruslan Brenden M Lake, Joshua B Salakhutdinov, Tenenbaum, Science. 3506266Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning through probabilistic program induction. Science, 350(6266):1332-1338, 2015.\n\nScalable gradient-based tuning of continuous regularization hyperparameters. Jelena Luketina, Mathias Berglund, Klaus Greff, Tapani Raiko, International conference on machine learning. Jelena Luketina, Mathias Berglund, Klaus Greff, and Tapani Raiko. Scalable gradient-based tuning of continuous regularization hyperparameters. In International conference on machine learning, pp. 2952-2960, 2016.\n\nGradient-based hyperparameter optimization through reversible learning. Dougal Maclaurin, David Duvenaud, Ryan Adams, International Conference on Machine Learning. Dougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyperparameter optimiza- tion through reversible learning. In International Conference on Machine Learning, pp. 2113- 2122, 2015.\n\nThe application of bayesian methods for seeking the extremum. Vytautas Tiesis, and Antanas Zilinskas. 2Jonas MockusJonas Mockus, Vytautas Tiesis, and Antanas Zilinskas. The application of bayesian methods for seeking the extremum. Towards global optimization, 2(117-129):2, 1978.\n\nAlex Nichol, John Schulman, arXiv:1803.02999Reptile: a scalable metalearning algorithm. 2arXiv preprintAlex Nichol and John Schulman. Reptile: a scalable metalearning algorithm. arXiv preprint arXiv:1803.02999, 2, 2018.\n\nTadam: Task dependent adaptive metric for improved few-shot learning. Boris Oreshkin, Alexandre Pau Rodr\u00edguez L\u00f3pez, Lacoste, Advances in Neural Information Processing Systems. Boris Oreshkin, Pau Rodr\u00edguez L\u00f3pez, and Alexandre Lacoste. Tadam: Task dependent adaptive metric for improved few-shot learning. In Advances in Neural Information Processing Systems, pp. 721-731, 2018.\n\nBleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting on association for computational linguistics. the 40th annual meeting on association for computational linguisticsAssociation for Computational LinguisticsKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pp. 311-318. Association for Computational Linguistics, 2002.\n\nAutomatic differentiation in PyTorch. Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary Devito, Zeming Lin, Alban Desmaison, Luca Antiga, Adam Lerer, NIPS Autodiff Workshop. Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in PyTorch. In NIPS Autodiff Workshop, 2017.\n\nBoa: The bayesian optimization algorithm. Martin Pelikan, E David, Erick Goldberg, Cant\u00fa-Paz, Proceedings of the 1st Annual Conference on Genetic and Evolutionary Computation. the 1st Annual Conference on Genetic and Evolutionary ComputationMorgan Kaufmann Publishers Inc1Martin Pelikan, David E Goldberg, and Erick Cant\u00fa-Paz. Boa: The bayesian optimization algorithm. In Proceedings of the 1st Annual Conference on Genetic and Evolutionary Computation-Volume 1, pp. 525-532. Morgan Kaufmann Publishers Inc., 1999.\n\n. Aravind Rajeswaran, Chelsea Finn, Sham Kakade, Sergey Levine, arXiv:1909.04630Meta-learning with implicit gradients. arXiv preprintAravind Rajeswaran, Chelsea Finn, Sham Kakade, and Sergey Levine. Meta-learning with implicit gradients. arXiv preprint arXiv:1909.04630, 2019.\n\nOptimization as a model for few-shot learning. Sachin Ravi, Hugo Larochelle, CoRRSachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. CoRR, 2016.\n\nA stochastic approximation method. The annals of mathematical statistics. Herbert Robbins, Sutton Monro, Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathemati- cal statistics, pp. 400-407, 1951.\n\nLearning internal representations by error propagation. Geoffrey E David E Rumelhart, Ronald J Hinton, Williams, California Univ San Diego La Jolla Inst for Cognitive ScienceTechnical reportDavid E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning internal representations by error propagation. Technical report, California Univ San Diego La Jolla Inst for Cognitive Science, 1985.\n\nSimon Osindero, and Raia Hadsell. Meta-learning with latent embedding optimization. Dushyant Andrei A Rusu, Jakub Rao, Oriol Sygnowski, Razvan Vinyals, Pascanu, arXiv:1807.05960arXiv preprintAndrei A Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osin- dero, and Raia Hadsell. Meta-learning with latent embedding optimization. arXiv preprint arXiv:1807.05960, 2018.\n\nEvolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook. PhD thesis. J\u00fcrgen Schmidhuber, Technische Universit\u00e4t M\u00fcnchenJ\u00fcrgen Schmidhuber. Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook. PhD thesis, Technische Universit\u00e4t M\u00fcnchen, 1987.\n\nVery deep convolutional networks for large-scale image recognition. Karen Simonyan, Andrew Zisserman, arXiv:1409.1556arXiv preprintKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\n\nPractical bayesian optimization of machine learning algorithms. Jasper Snoek, Hugo Larochelle, Ryan P Adams, Advances in neural information processing systems. Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimization of machine learning algorithms. In Advances in neural information processing systems, pp. 2951-2959, 2012.\n\nMatching networks for one shot learning. Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, Advances in neural information processing systems. Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning. In Advances in neural information processing systems, pp. 3630-3638, 2016.\n\nSimple statistical gradient-following algorithms for connectionist reinforcement learning. J Ronald, Williams, Machine learning. 83-4Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229-256, 1992.\n", "annotations": {"author": "[{\"end\":60,\"start\":40},{\"end\":74,\"start\":61},{\"end\":111,\"start\":75},{\"end\":145,\"start\":112},{\"end\":176,\"start\":146},{\"end\":203,\"start\":177},{\"end\":238,\"start\":204},{\"end\":271,\"start\":239},{\"end\":418,\"start\":272},{\"end\":469,\"start\":419}]", "publisher": null, "author_last_name": "[{\"end\":59,\"start\":47},{\"end\":73,\"start\":69},{\"end\":87,\"start\":81},{\"end\":127,\"start\":118},{\"end\":161,\"start\":156},{\"end\":188,\"start\":183},{\"end\":217,\"start\":214},{\"end\":255,\"start\":247}]", "author_first_name": "[{\"end\":46,\"start\":40},{\"end\":68,\"start\":61},{\"end\":80,\"start\":75},{\"end\":117,\"start\":112},{\"end\":155,\"start\":146},{\"end\":182,\"start\":177},{\"end\":213,\"start\":204},{\"end\":246,\"start\":239}]", "author_affiliation": "[{\"end\":417,\"start\":273},{\"end\":468,\"start\":420}]", "title": "[{\"end\":37,\"start\":1},{\"end\":506,\"start\":470}]", "venue": null, "abstract": "[{\"end\":1274,\"start\":524}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b35\"},\"end\":1389,\"start\":1371},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1402,\"start\":1389},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":1426,\"start\":1402},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":1517,\"start\":1494},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1543,\"start\":1517},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":1561,\"start\":1543},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6723,\"start\":6704},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8042,\"start\":8018},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8238,\"start\":8218},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":15489,\"start\":15466},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":15682,\"start\":15666},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":15877,\"start\":15854},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":15915,\"start\":15895},{\"end\":16571,\"start\":16546},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":17595,\"start\":17572},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":17736,\"start\":17709},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":17760,\"start\":17736},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":18342,\"start\":18321},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":21651,\"start\":21637},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":21674,\"start\":21651},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":21696,\"start\":21674},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":21720,\"start\":21696},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":21787,\"start\":21762},{\"end\":21834,\"start\":21800},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":21855,\"start\":21834},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":21877,\"start\":21855},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":21896,\"start\":21877},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":22348,\"start\":22323},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":22374,\"start\":22348},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":22392,\"start\":22374},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":22854,\"start\":22835},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":22876,\"start\":22854},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":23504,\"start\":23483},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":24151,\"start\":24127},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":25100,\"start\":25076},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":25128,\"start\":25104},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":25720,\"start\":25700},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":26216,\"start\":26195},{\"end\":27110,\"start\":27081},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":34337,\"start\":34317},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":34943,\"start\":34923},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":35802,\"start\":35783},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":36515,\"start\":36493},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":36536,\"start\":36517},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":36715,\"start\":36697},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":37080,\"start\":37057},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":37216,\"start\":37194},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":37240,\"start\":37216},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":37773,\"start\":37754},{\"end\":38626,\"start\":38603},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":40080,\"start\":40052},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":40127,\"start\":40104},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":40183,\"start\":40166},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":40390,\"start\":40370},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":40459,\"start\":40436},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":42029,\"start\":42007}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":41705,\"start\":41405},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":42589,\"start\":41706},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":42676,\"start\":42590}]", "paragraph": "[{\"end\":3492,\"start\":1290},{\"end\":3849,\"start\":3494},{\"end\":4892,\"start\":3890},{\"end\":5702,\"start\":4908},{\"end\":6234,\"start\":5704},{\"end\":6506,\"start\":6270},{\"end\":7404,\"start\":6508},{\"end\":8606,\"start\":7417},{\"end\":8835,\"start\":8686},{\"end\":9120,\"start\":8925},{\"end\":10143,\"start\":9426},{\"end\":10595,\"start\":10161},{\"end\":11006,\"start\":10671},{\"end\":11729,\"start\":11008},{\"end\":12330,\"start\":11999},{\"end\":12574,\"start\":12332},{\"end\":12903,\"start\":12576},{\"end\":13420,\"start\":12905},{\"end\":13871,\"start\":13422},{\"end\":14488,\"start\":14231},{\"end\":14824,\"start\":14490},{\"end\":15023,\"start\":14826},{\"end\":15273,\"start\":15075},{\"end\":15695,\"start\":15275},{\"end\":16078,\"start\":15697},{\"end\":16689,\"start\":16080},{\"end\":17093,\"start\":16720},{\"end\":17761,\"start\":17155},{\"end\":19103,\"start\":17763},{\"end\":19362,\"start\":19141},{\"end\":19423,\"start\":19364},{\"end\":19449,\"start\":19425},{\"end\":19604,\"start\":19451},{\"end\":19616,\"start\":19606},{\"end\":20210,\"start\":20195},{\"end\":20336,\"start\":20212},{\"end\":21106,\"start\":20366},{\"end\":21522,\"start\":21119},{\"end\":22237,\"start\":21524},{\"end\":22780,\"start\":22239},{\"end\":23457,\"start\":22782},{\"end\":23805,\"start\":23459},{\"end\":24104,\"start\":23822},{\"end\":25129,\"start\":24106},{\"end\":26065,\"start\":25131},{\"end\":26642,\"start\":26088},{\"end\":26959,\"start\":26656},{\"end\":27751,\"start\":26961},{\"end\":28662,\"start\":27753},{\"end\":29624,\"start\":28700},{\"end\":30922,\"start\":29626},{\"end\":31865,\"start\":30959},{\"end\":33102,\"start\":31867},{\"end\":33700,\"start\":33118},{\"end\":34259,\"start\":33757},{\"end\":34846,\"start\":34261},{\"end\":36886,\"start\":34848},{\"end\":38760,\"start\":36888},{\"end\":39782,\"start\":38775},{\"end\":40020,\"start\":39831},{\"end\":40326,\"start\":40022},{\"end\":40418,\"start\":40328},{\"end\":40578,\"start\":40420},{\"end\":41404,\"start\":40580}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":6269,\"start\":6235},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8685,\"start\":8607},{\"attributes\":{\"id\":\"formula_2\"},\"end\":8924,\"start\":8836},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9425,\"start\":9121},{\"attributes\":{\"id\":\"formula_4\"},\"end\":10670,\"start\":10596},{\"attributes\":{\"id\":\"formula_5\"},\"end\":11979,\"start\":11730},{\"attributes\":{\"id\":\"formula_6\"},\"end\":14230,\"start\":13872},{\"attributes\":{\"id\":\"formula_7\"},\"end\":15074,\"start\":15024},{\"attributes\":{\"id\":\"formula_8\"},\"end\":17154,\"start\":17094},{\"attributes\":{\"id\":\"formula_9\"},\"end\":20194,\"start\":19617}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":38415,\"start\":38408},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":39581,\"start\":39574}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1288,\"start\":1276},{\"attributes\":{\"n\":\"2\"},\"end\":3888,\"start\":3852},{\"attributes\":{\"n\":\"2.1\"},\"end\":4906,\"start\":4895},{\"attributes\":{\"n\":\"2.2\"},\"end\":7415,\"start\":7407},{\"attributes\":{\"n\":\"2.3\"},\"end\":10159,\"start\":10146},{\"attributes\":{\"n\":\"2.4\"},\"end\":11997,\"start\":11981},{\"attributes\":{\"n\":\"2.5\"},\"end\":16718,\"start\":16692},{\"end\":19139,\"start\":19106},{\"attributes\":{\"n\":\"3\"},\"end\":20364,\"start\":20339},{\"attributes\":{\"n\":\"3.1\"},\"end\":21117,\"start\":21109},{\"attributes\":{\"n\":\"3.2\"},\"end\":23820,\"start\":23808},{\"attributes\":{\"n\":\"4\"},\"end\":26086,\"start\":26068},{\"attributes\":{\"n\":\"4.1\"},\"end\":26654,\"start\":26645},{\"attributes\":{\"n\":\"4.2\"},\"end\":28698,\"start\":28665},{\"attributes\":{\"n\":\"4.3\"},\"end\":30957,\"start\":30925},{\"attributes\":{\"n\":\"5\"},\"end\":33116,\"start\":33105},{\"attributes\":{\"n\":\"5.1\"},\"end\":33755,\"start\":33703},{\"attributes\":{\"n\":\"6\"},\"end\":38773,\"start\":38763},{\"end\":39829,\"start\":39785},{\"end\":41426,\"start\":41406},{\"end\":41716,\"start\":41707},{\"end\":42600,\"start\":42591}]", "table": "[{\"end\":42589,\"start\":42449},{\"end\":42676,\"start\":42661}]", "figure_caption": "[{\"end\":41705,\"start\":41429},{\"end\":42449,\"start\":41718},{\"end\":42661,\"start\":42602}]", "figure_ref": "[{\"end\":35907,\"start\":35899},{\"end\":36015,\"start\":36007},{\"end\":40734,\"start\":40726},{\"end\":40747,\"start\":40739}]", "bib_author_first_name": "[{\"end\":43494,\"start\":43488},{\"end\":43514,\"start\":43509},{\"end\":43528,\"start\":43522},{\"end\":43537,\"start\":43536},{\"end\":43552,\"start\":43547},{\"end\":43565,\"start\":43562},{\"end\":43579,\"start\":43572},{\"end\":43596,\"start\":43588},{\"end\":43972,\"start\":43965},{\"end\":43991,\"start\":43983},{\"end\":44005,\"start\":44001},{\"end\":44168,\"start\":44165},{\"end\":44185,\"start\":44177},{\"end\":44204,\"start\":44195},{\"end\":44215,\"start\":44212},{\"end\":44388,\"start\":44383},{\"end\":44403,\"start\":44398},{\"end\":44421,\"start\":44415},{\"end\":44438,\"start\":44432},{\"end\":44460,\"start\":44453},{\"end\":44477,\"start\":44471},{\"end\":44497,\"start\":44488},{\"end\":44787,\"start\":44781},{\"end\":44980,\"start\":44975},{\"end\":44997,\"start\":44991},{\"end\":45236,\"start\":45235},{\"end\":45248,\"start\":45244},{\"end\":45265,\"start\":45259},{\"end\":45282,\"start\":45276},{\"end\":45582,\"start\":45576},{\"end\":45598,\"start\":45589},{\"end\":45609,\"start\":45604},{\"end\":45631,\"start\":45616},{\"end\":45645,\"start\":45638},{\"end\":45793,\"start\":45785},{\"end\":45896,\"start\":45889},{\"end\":45910,\"start\":45904},{\"end\":45925,\"start\":45918},{\"end\":45940,\"start\":45934},{\"end\":45945,\"start\":45941},{\"end\":45959,\"start\":45953},{\"end\":46276,\"start\":46273},{\"end\":46287,\"start\":46283},{\"end\":46300,\"start\":46298},{\"end\":46308,\"start\":46307},{\"end\":46320,\"start\":46316},{\"end\":46337,\"start\":46331},{\"end\":46662,\"start\":46658},{\"end\":46674,\"start\":46670},{\"end\":46687,\"start\":46682},{\"end\":46994,\"start\":46987},{\"end\":47007,\"start\":47001},{\"end\":47022,\"start\":47016},{\"end\":47456,\"start\":47452},{\"end\":47476,\"start\":47469},{\"end\":47490,\"start\":47485},{\"end\":47513,\"start\":47501},{\"end\":47976,\"start\":47972},{\"end\":47994,\"start\":47989},{\"end\":48012,\"start\":48005},{\"end\":48028,\"start\":48020},{\"end\":48048,\"start\":48037},{\"end\":48340,\"start\":48333},{\"end\":48352,\"start\":48345},{\"end\":48368,\"start\":48360},{\"end\":48378,\"start\":48374},{\"end\":48774,\"start\":48773},{\"end\":48790,\"start\":48785},{\"end\":48792,\"start\":48791},{\"end\":49171,\"start\":49167},{\"end\":49190,\"start\":49184},{\"end\":49471,\"start\":49468},{\"end\":49485,\"start\":49479},{\"end\":49498,\"start\":49491},{\"end\":49521,\"start\":49515},{\"end\":49523,\"start\":49522},{\"end\":49713,\"start\":49710},{\"end\":49727,\"start\":49721},{\"end\":49740,\"start\":49733},{\"end\":49765,\"start\":49757},{\"end\":50182,\"start\":50181},{\"end\":50198,\"start\":50193},{\"end\":50431,\"start\":50425},{\"end\":50454,\"start\":50448},{\"end\":50456,\"start\":50455},{\"end\":50751,\"start\":50745},{\"end\":50769,\"start\":50762},{\"end\":50785,\"start\":50780},{\"end\":50799,\"start\":50793},{\"end\":51145,\"start\":51139},{\"end\":51162,\"start\":51157},{\"end\":51177,\"start\":51173},{\"end\":51712,\"start\":51708},{\"end\":51725,\"start\":51721},{\"end\":52004,\"start\":51999},{\"end\":52024,\"start\":52015},{\"end\":52381,\"start\":52374},{\"end\":52397,\"start\":52392},{\"end\":52410,\"start\":52406},{\"end\":52425,\"start\":52417},{\"end\":52943,\"start\":52939},{\"end\":52955,\"start\":52952},{\"end\":52970,\"start\":52963},{\"end\":52988,\"start\":52981},{\"end\":53003,\"start\":52997},{\"end\":53017,\"start\":53010},{\"end\":53032,\"start\":53026},{\"end\":53043,\"start\":53038},{\"end\":53059,\"start\":53055},{\"end\":53072,\"start\":53068},{\"end\":53369,\"start\":53363},{\"end\":53380,\"start\":53379},{\"end\":53393,\"start\":53388},{\"end\":53846,\"start\":53839},{\"end\":53866,\"start\":53859},{\"end\":53877,\"start\":53873},{\"end\":53892,\"start\":53886},{\"end\":54168,\"start\":54162},{\"end\":54179,\"start\":54175},{\"end\":54370,\"start\":54363},{\"end\":54386,\"start\":54380},{\"end\":54588,\"start\":54580},{\"end\":54590,\"start\":54589},{\"end\":54618,\"start\":54610},{\"end\":55011,\"start\":55003},{\"end\":55032,\"start\":55027},{\"end\":55043,\"start\":55038},{\"end\":55061,\"start\":55055},{\"end\":55438,\"start\":55432},{\"end\":55733,\"start\":55728},{\"end\":55750,\"start\":55744},{\"end\":56005,\"start\":55999},{\"end\":56017,\"start\":56013},{\"end\":56036,\"start\":56030},{\"end\":56330,\"start\":56325},{\"end\":56347,\"start\":56340},{\"end\":56365,\"start\":56358},{\"end\":56381,\"start\":56377},{\"end\":56726,\"start\":56725}]", "bib_author_last_name": "[{\"end\":43507,\"start\":43495},{\"end\":43520,\"start\":43515},{\"end\":43534,\"start\":43529},{\"end\":43545,\"start\":43538},{\"end\":43560,\"start\":43553},{\"end\":43570,\"start\":43566},{\"end\":43586,\"start\":43580},{\"end\":43609,\"start\":43597},{\"end\":43618,\"start\":43611},{\"end\":43981,\"start\":43973},{\"end\":43999,\"start\":43992},{\"end\":44013,\"start\":44006},{\"end\":44175,\"start\":44169},{\"end\":44193,\"start\":44186},{\"end\":44210,\"start\":44205},{\"end\":44222,\"start\":44216},{\"end\":44396,\"start\":44389},{\"end\":44413,\"start\":44404},{\"end\":44430,\"start\":44422},{\"end\":44451,\"start\":44439},{\"end\":44469,\"start\":44461},{\"end\":44486,\"start\":44478},{\"end\":44503,\"start\":44498},{\"end\":44794,\"start\":44788},{\"end\":44989,\"start\":44981},{\"end\":45004,\"start\":44998},{\"end\":45242,\"start\":45237},{\"end\":45257,\"start\":45249},{\"end\":45274,\"start\":45266},{\"end\":45289,\"start\":45283},{\"end\":45295,\"start\":45291},{\"end\":45587,\"start\":45583},{\"end\":45602,\"start\":45599},{\"end\":45614,\"start\":45610},{\"end\":45636,\"start\":45632},{\"end\":45651,\"start\":45646},{\"end\":45801,\"start\":45794},{\"end\":45902,\"start\":45897},{\"end\":45916,\"start\":45911},{\"end\":45932,\"start\":45926},{\"end\":45951,\"start\":45946},{\"end\":45966,\"start\":45960},{\"end\":46281,\"start\":46277},{\"end\":46296,\"start\":46288},{\"end\":46305,\"start\":46301},{\"end\":46314,\"start\":46309},{\"end\":46329,\"start\":46321},{\"end\":46347,\"start\":46338},{\"end\":46355,\"start\":46349},{\"end\":46668,\"start\":46663},{\"end\":46680,\"start\":46675},{\"end\":46694,\"start\":46688},{\"end\":46999,\"start\":46995},{\"end\":47014,\"start\":47008},{\"end\":47029,\"start\":47023},{\"end\":47467,\"start\":47457},{\"end\":47483,\"start\":47477},{\"end\":47499,\"start\":47491},{\"end\":47520,\"start\":47514},{\"end\":47987,\"start\":47977},{\"end\":48003,\"start\":47995},{\"end\":48018,\"start\":48013},{\"end\":48035,\"start\":48029},{\"end\":48055,\"start\":48049},{\"end\":48343,\"start\":48341},{\"end\":48358,\"start\":48353},{\"end\":48372,\"start\":48369},{\"end\":48382,\"start\":48379},{\"end\":48783,\"start\":48775},{\"end\":48799,\"start\":48793},{\"end\":48806,\"start\":48801},{\"end\":49182,\"start\":49172},{\"end\":49198,\"start\":49191},{\"end\":49215,\"start\":49200},{\"end\":49477,\"start\":49472},{\"end\":49489,\"start\":49486},{\"end\":49513,\"start\":49499},{\"end\":49534,\"start\":49524},{\"end\":49719,\"start\":49714},{\"end\":49731,\"start\":49728},{\"end\":49755,\"start\":49741},{\"end\":49776,\"start\":49766},{\"end\":50191,\"start\":50183},{\"end\":50205,\"start\":50199},{\"end\":50209,\"start\":50207},{\"end\":50446,\"start\":50432},{\"end\":50470,\"start\":50457},{\"end\":50481,\"start\":50472},{\"end\":50760,\"start\":50752},{\"end\":50778,\"start\":50770},{\"end\":50791,\"start\":50786},{\"end\":50805,\"start\":50800},{\"end\":51155,\"start\":51146},{\"end\":51171,\"start\":51163},{\"end\":51183,\"start\":51178},{\"end\":51719,\"start\":51713},{\"end\":51734,\"start\":51726},{\"end\":52013,\"start\":52005},{\"end\":52044,\"start\":52025},{\"end\":52053,\"start\":52046},{\"end\":52390,\"start\":52382},{\"end\":52404,\"start\":52398},{\"end\":52415,\"start\":52411},{\"end\":52429,\"start\":52426},{\"end\":52950,\"start\":52944},{\"end\":52961,\"start\":52956},{\"end\":52979,\"start\":52971},{\"end\":52995,\"start\":52989},{\"end\":53008,\"start\":53004},{\"end\":53024,\"start\":53018},{\"end\":53036,\"start\":53033},{\"end\":53053,\"start\":53044},{\"end\":53066,\"start\":53060},{\"end\":53078,\"start\":53073},{\"end\":53377,\"start\":53370},{\"end\":53386,\"start\":53381},{\"end\":53402,\"start\":53394},{\"end\":53413,\"start\":53404},{\"end\":53857,\"start\":53847},{\"end\":53871,\"start\":53867},{\"end\":53884,\"start\":53878},{\"end\":53899,\"start\":53893},{\"end\":54173,\"start\":54169},{\"end\":54190,\"start\":54180},{\"end\":54378,\"start\":54371},{\"end\":54392,\"start\":54387},{\"end\":54608,\"start\":54591},{\"end\":54625,\"start\":54619},{\"end\":54635,\"start\":54627},{\"end\":55025,\"start\":55012},{\"end\":55036,\"start\":55033},{\"end\":55053,\"start\":55044},{\"end\":55069,\"start\":55062},{\"end\":55078,\"start\":55071},{\"end\":55450,\"start\":55439},{\"end\":55742,\"start\":55734},{\"end\":55760,\"start\":55751},{\"end\":56011,\"start\":56006},{\"end\":56028,\"start\":56018},{\"end\":56042,\"start\":56037},{\"end\":56338,\"start\":56331},{\"end\":56356,\"start\":56348},{\"end\":56375,\"start\":56366},{\"end\":56390,\"start\":56382},{\"end\":56733,\"start\":56727},{\"end\":56743,\"start\":56735}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":2928017},\"end\":43939,\"start\":43429},{\"attributes\":{\"doi\":\"arXiv:1810.09502\",\"id\":\"b1\"},\"end\":44161,\"start\":43941},{\"attributes\":{\"id\":\"b2\"},\"end\":44349,\"start\":44163},{\"attributes\":{\"doi\":\"arXiv:1906.05374\",\"id\":\"b3\"},\"end\":44731,\"start\":44351},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":5671899},\"end\":44925,\"start\":44733},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":15700257},\"end\":45188,\"start\":44927},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":11688126},\"end\":45532,\"start\":45190},{\"attributes\":{\"id\":\"b7\"},\"end\":45781,\"start\":45534},{\"attributes\":{\"id\":\"b8\"},\"end\":45839,\"start\":45783},{\"attributes\":{\"id\":\"b9\"},\"end\":46205,\"start\":45841},{\"attributes\":{\"doi\":\"arXiv:1611.02779\",\"id\":\"b10\"},\"end\":46578,\"start\":46207},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":538820},\"end\":46918,\"start\":46580},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":6719686},\"end\":47386,\"start\":46920},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":8026824},\"end\":47899,\"start\":47388},{\"attributes\":{\"doi\":\"arXiv:1806.04910\",\"id\":\"b14\"},\"end\":48285,\"start\":47901},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":206594692},\"end\":48728,\"start\":48287},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":16710884},\"end\":49123,\"start\":48730},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":52872549},\"end\":49464,\"start\":49125},{\"attributes\":{\"id\":\"b18\"},\"end\":49666,\"start\":49466},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":9433631},\"end\":50135,\"start\":49668},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b20\"},\"end\":50353,\"start\":50137},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":11790493},\"end\":50666,\"start\":50355},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":6281930},\"end\":51065,\"start\":50668},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":8540522},\"end\":51425,\"start\":51067},{\"attributes\":{\"id\":\"b24\"},\"end\":51706,\"start\":51427},{\"attributes\":{\"doi\":\"arXiv:1803.02999\",\"id\":\"b25\"},\"end\":51927,\"start\":51708},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":44061218},\"end\":52308,\"start\":51929},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":11080756},\"end\":52899,\"start\":52310},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":40027675},\"end\":53319,\"start\":52901},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":2355296},\"end\":53835,\"start\":53321},{\"attributes\":{\"doi\":\"arXiv:1909.04630\",\"id\":\"b30\"},\"end\":54113,\"start\":53837},{\"attributes\":{\"id\":\"b31\"},\"end\":54287,\"start\":54115},{\"attributes\":{\"id\":\"b32\"},\"end\":54522,\"start\":54289},{\"attributes\":{\"id\":\"b33\"},\"end\":54917,\"start\":54524},{\"attributes\":{\"doi\":\"arXiv:1807.05960\",\"id\":\"b34\"},\"end\":55311,\"start\":54919},{\"attributes\":{\"id\":\"b35\"},\"end\":55658,\"start\":55313},{\"attributes\":{\"doi\":\"arXiv:1409.1556\",\"id\":\"b36\"},\"end\":55933,\"start\":55660},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":632197},\"end\":56282,\"start\":55935},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":8909022},\"end\":56632,\"start\":56284},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":2332513},\"end\":56916,\"start\":56634}]", "bib_title": "[{\"end\":43486,\"start\":43429},{\"end\":44779,\"start\":44733},{\"end\":44973,\"start\":44927},{\"end\":45233,\"start\":45190},{\"end\":46656,\"start\":46580},{\"end\":46985,\"start\":46920},{\"end\":47450,\"start\":47388},{\"end\":48331,\"start\":48287},{\"end\":48771,\"start\":48730},{\"end\":49165,\"start\":49125},{\"end\":49708,\"start\":49668},{\"end\":50423,\"start\":50355},{\"end\":50743,\"start\":50668},{\"end\":51137,\"start\":51067},{\"end\":51487,\"start\":51427},{\"end\":51997,\"start\":51929},{\"end\":52372,\"start\":52310},{\"end\":52937,\"start\":52901},{\"end\":53361,\"start\":53321},{\"end\":55997,\"start\":55935},{\"end\":56323,\"start\":56284},{\"end\":56723,\"start\":56634}]", "bib_author": "[{\"end\":43509,\"start\":43488},{\"end\":43522,\"start\":43509},{\"end\":43536,\"start\":43522},{\"end\":43547,\"start\":43536},{\"end\":43562,\"start\":43547},{\"end\":43572,\"start\":43562},{\"end\":43588,\"start\":43572},{\"end\":43611,\"start\":43588},{\"end\":43620,\"start\":43611},{\"end\":43983,\"start\":43965},{\"end\":44001,\"start\":43983},{\"end\":44015,\"start\":44001},{\"end\":44177,\"start\":44165},{\"end\":44195,\"start\":44177},{\"end\":44212,\"start\":44195},{\"end\":44224,\"start\":44212},{\"end\":44398,\"start\":44383},{\"end\":44415,\"start\":44398},{\"end\":44432,\"start\":44415},{\"end\":44453,\"start\":44432},{\"end\":44471,\"start\":44453},{\"end\":44488,\"start\":44471},{\"end\":44505,\"start\":44488},{\"end\":44796,\"start\":44781},{\"end\":44991,\"start\":44975},{\"end\":45006,\"start\":44991},{\"end\":45244,\"start\":45235},{\"end\":45259,\"start\":45244},{\"end\":45276,\"start\":45259},{\"end\":45291,\"start\":45276},{\"end\":45297,\"start\":45291},{\"end\":45589,\"start\":45576},{\"end\":45604,\"start\":45589},{\"end\":45616,\"start\":45604},{\"end\":45638,\"start\":45616},{\"end\":45653,\"start\":45638},{\"end\":45803,\"start\":45785},{\"end\":45904,\"start\":45889},{\"end\":45918,\"start\":45904},{\"end\":45934,\"start\":45918},{\"end\":45953,\"start\":45934},{\"end\":45968,\"start\":45953},{\"end\":46283,\"start\":46273},{\"end\":46298,\"start\":46283},{\"end\":46307,\"start\":46298},{\"end\":46316,\"start\":46307},{\"end\":46331,\"start\":46316},{\"end\":46349,\"start\":46331},{\"end\":46357,\"start\":46349},{\"end\":46670,\"start\":46658},{\"end\":46682,\"start\":46670},{\"end\":46696,\"start\":46682},{\"end\":47001,\"start\":46987},{\"end\":47016,\"start\":47001},{\"end\":47031,\"start\":47016},{\"end\":47469,\"start\":47452},{\"end\":47485,\"start\":47469},{\"end\":47501,\"start\":47485},{\"end\":47522,\"start\":47501},{\"end\":47989,\"start\":47972},{\"end\":48005,\"start\":47989},{\"end\":48020,\"start\":48005},{\"end\":48037,\"start\":48020},{\"end\":48057,\"start\":48037},{\"end\":48345,\"start\":48333},{\"end\":48360,\"start\":48345},{\"end\":48374,\"start\":48360},{\"end\":48384,\"start\":48374},{\"end\":48785,\"start\":48773},{\"end\":48801,\"start\":48785},{\"end\":48808,\"start\":48801},{\"end\":49184,\"start\":49167},{\"end\":49200,\"start\":49184},{\"end\":49217,\"start\":49200},{\"end\":49479,\"start\":49468},{\"end\":49491,\"start\":49479},{\"end\":49515,\"start\":49491},{\"end\":49536,\"start\":49515},{\"end\":49721,\"start\":49710},{\"end\":49733,\"start\":49721},{\"end\":49757,\"start\":49733},{\"end\":49778,\"start\":49757},{\"end\":50193,\"start\":50181},{\"end\":50207,\"start\":50193},{\"end\":50211,\"start\":50207},{\"end\":50448,\"start\":50425},{\"end\":50472,\"start\":50448},{\"end\":50483,\"start\":50472},{\"end\":50762,\"start\":50745},{\"end\":50780,\"start\":50762},{\"end\":50793,\"start\":50780},{\"end\":50807,\"start\":50793},{\"end\":51157,\"start\":51139},{\"end\":51173,\"start\":51157},{\"end\":51185,\"start\":51173},{\"end\":51721,\"start\":51708},{\"end\":51736,\"start\":51721},{\"end\":52015,\"start\":51999},{\"end\":52046,\"start\":52015},{\"end\":52055,\"start\":52046},{\"end\":52392,\"start\":52374},{\"end\":52406,\"start\":52392},{\"end\":52417,\"start\":52406},{\"end\":52431,\"start\":52417},{\"end\":52952,\"start\":52939},{\"end\":52963,\"start\":52952},{\"end\":52981,\"start\":52963},{\"end\":52997,\"start\":52981},{\"end\":53010,\"start\":52997},{\"end\":53026,\"start\":53010},{\"end\":53038,\"start\":53026},{\"end\":53055,\"start\":53038},{\"end\":53068,\"start\":53055},{\"end\":53080,\"start\":53068},{\"end\":53379,\"start\":53363},{\"end\":53388,\"start\":53379},{\"end\":53404,\"start\":53388},{\"end\":53415,\"start\":53404},{\"end\":53859,\"start\":53839},{\"end\":53873,\"start\":53859},{\"end\":53886,\"start\":53873},{\"end\":53901,\"start\":53886},{\"end\":54175,\"start\":54162},{\"end\":54192,\"start\":54175},{\"end\":54380,\"start\":54363},{\"end\":54394,\"start\":54380},{\"end\":54610,\"start\":54580},{\"end\":54627,\"start\":54610},{\"end\":54637,\"start\":54627},{\"end\":55027,\"start\":55003},{\"end\":55038,\"start\":55027},{\"end\":55055,\"start\":55038},{\"end\":55071,\"start\":55055},{\"end\":55080,\"start\":55071},{\"end\":55452,\"start\":55432},{\"end\":55744,\"start\":55728},{\"end\":55762,\"start\":55744},{\"end\":56013,\"start\":55999},{\"end\":56030,\"start\":56013},{\"end\":56044,\"start\":56030},{\"end\":56340,\"start\":56325},{\"end\":56358,\"start\":56340},{\"end\":56377,\"start\":56358},{\"end\":56392,\"start\":56377},{\"end\":56735,\"start\":56725},{\"end\":56745,\"start\":56735}]", "bib_venue": "[{\"end\":47154,\"start\":47101},{\"end\":47645,\"start\":47592},{\"end\":48525,\"start\":48463},{\"end\":48945,\"start\":48885},{\"end\":49919,\"start\":49857},{\"end\":52584,\"start\":52516},{\"end\":53562,\"start\":53497},{\"end\":43669,\"start\":43620},{\"end\":43963,\"start\":43941},{\"end\":44381,\"start\":44351},{\"end\":44814,\"start\":44796},{\"end\":45042,\"start\":45006},{\"end\":45346,\"start\":45297},{\"end\":45574,\"start\":45534},{\"end\":45887,\"start\":45841},{\"end\":46271,\"start\":46207},{\"end\":46732,\"start\":46696},{\"end\":47099,\"start\":47031},{\"end\":47590,\"start\":47522},{\"end\":47970,\"start\":47901},{\"end\":48461,\"start\":48384},{\"end\":48883,\"start\":48808},{\"end\":49271,\"start\":49217},{\"end\":49855,\"start\":49778},{\"end\":50179,\"start\":50137},{\"end\":50490,\"start\":50483},{\"end\":50851,\"start\":50807},{\"end\":51229,\"start\":51185},{\"end\":51527,\"start\":51489},{\"end\":51794,\"start\":51752},{\"end\":52104,\"start\":52055},{\"end\":52514,\"start\":52431},{\"end\":53102,\"start\":53080},{\"end\":53495,\"start\":53415},{\"end\":54160,\"start\":54115},{\"end\":54361,\"start\":54289},{\"end\":54578,\"start\":54524},{\"end\":55001,\"start\":54919},{\"end\":55430,\"start\":55313},{\"end\":55726,\"start\":55660},{\"end\":56093,\"start\":56044},{\"end\":56441,\"start\":56392},{\"end\":56761,\"start\":56745}]"}}}, "year": 2023, "month": 12, "day": 17}