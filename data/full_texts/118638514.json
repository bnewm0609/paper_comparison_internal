{"id": 118638514, "updated": "2023-10-01 20:39:18.625", "metadata": {"title": "Bridging Theory and Algorithm for Domain Adaptation", "authors": "[{\"first\":\"Yuchen\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Tianle\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Mingsheng\",\"last\":\"Long\",\"middle\":[]},{\"first\":\"Michael\",\"last\":\"Jordan\",\"middle\":[\"I.\"]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2019, "month": 4, "day": 11}, "abstract": "This paper addresses the problem of unsupervised domain adaption from theoretical and algorithmic perspectives. Existing domain adaptation theories naturally imply minimax optimization algorithms, which connect well with the adversarial-learning based domain adaptation methods. However, several disconnections still form the gap between theory and algorithm. We extend previous theories (Ben-David et al., 2010; Mansour et al., 2009c) to multiclass classification in domain adaptation, where classifiers based on scoring functions and margin loss are standard algorithmic choices. We introduce a novel measurement, margin disparity discrepancy, that is tailored both to distribution comparison with asymmetric margin loss, and to minimax optimization for easier training. Using this discrepancy, we derive new generalization bounds in terms of Rademacher complexity. Our theory can be seamlessly transformed into an adversarial learning algorithm for domain adaptation, successfully bridging the gap between theory and algorithm. A series of empirical studies show that our algorithm achieves the state-of-the-art accuracies on challenging domain adaptation tasks.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "1904.05801", "mag": "2963094258", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icml/0002LLJ19", "doi": null}}, "content": {"source": {"pdf_hash": "c6a1e619e731720a3768bf6b788164fd74f70982", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1904.05801v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "807f0851876867b3e09f5caec73a6f69aa48e1af", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/c6a1e619e731720a3768bf6b788164fd74f70982.txt", "contents": "\nBridging Theory and Algorithm for Domain Adaptation\n\n\nYuchen Zhang \nTianle Liu \nMingsheng Long \nMichael I Jordan \nBridging Theory and Algorithm for Domain Adaptation\n\nThis paper addresses the problem of unsupervised domain adaption from theoretical and algorithmic perspectives. Existing domain adaptation theories naturally imply minimax optimization algorithms, which connect well with the adversarial-learning based domain adaptation methods. However, several disconnections still form the gap between theory and algorithm. We extend previous theories(Ben-David et al., 2010;Mansour et al., 2009c)to multiclass classification in domain adaptation, where classifiers based on scoring functions and margin loss are standard algorithmic choices. We introduce a novel measurement, margin disparity discrepancy, that is tailored both to distribution comparison with asymmetric margin loss, and to minimax optimization for easier training. Using this discrepancy, we derive new generalization bounds in terms of Rademacher complexity. Our theory can be seamlessly transformed into an adversarial learning algorithm for domain adaptation, successfully bridging the gap between theory and algorithm. A series of empirical studies show that our algorithm achieves the state-of-the-art accuracies on challenging domain adaptation tasks.\n\nIntroduction\n\nIt is a common assumption in learning theories that training and test data are drawn from identical distribution. Thus, if the source domain from which we train a supervised learner, is substantially dissimilar to the target domain on which the learner is applied, there are no possibilities for good generalization across domains. However, we may expect to train a model by leveraging labeled data from similar domains. Domain adaptation is the particular machine learning task dealing with the setting where the distributions of training and test data are distinct with one another (Quionero-Candela et al., 2009;Pan & Yang, 2010 Remarkable theoretical advances have been achieved in domain adaptation. Ben-David et al. (2010); Mansour et al. (2009c) provide rigorous learning bounds for unsupervised domain adaptation, a most challenging scenario in this field. These earliest theories have later been extended in many ways, from loss functions to bayesian settings to regression problems (Mohri & Medina, 2012;Germain et al., 2013;Cortes et al., 2015). In addition, theories based on weighted combination of hypotheses have also been developed for multiple source domain adaptation Mansour et al., 2009b;a;Hoffman et al., 2018a).\n\nOn par with the theoretical findings, there are rich advances in domain adaptation algorithms. Previous work explored various techniques for statistics matching (Pan et al., 2011;Tzeng et al., 2014;Long et al., 2015; and discrepancy minimization (Ganin & Lempitsky, 2015;Ganin et al., 2016). Among them, adversarial-learning methods come with relatively strong theoretical guarantees. Inspired by Goodfellow et al. (2014), these methods build on the twoplayer game between domain discriminator and feature extractor. Recent works explored adversarial learning in diverse ways, yielding state-of-the-art results on many tasks (Tzeng et al., 2017;Saito et al., 2018;Long et al., 2018). While many domain adaptation algorithms can be roughly interpreted as minimizing the distribution discrepancy in domain adaptation theories, several disconnections form nonnegligible gap between the theories and algorithms. When designing a domain adaptation algorithm using scoring functions, we are in risk that our algorithm is not theoretically guaranteed since there is a gap between the loss used in the theories and algorithms. Furthermore, there is another gap between the hypothesis-induced discrepancies in theories and the widely-used divergences in algorithms, including Jensen Shannon Divergence (Ganin & Lempitsky, 2015), Maximum Mean Discrepancy (Gretton et al., 2012;Long et al., 2015), and Wasserstein Distance (Courty et al., 2017).\n\nThis work aims to bridge the gaps between the theories and algorithms for domain adaptation. We present a novel theoretical analysis of classification task in domain adaptation towards explicit guidance for algorithm design. We extend existing theories to classifiers based on scoring functions and margin loss, which are standard choices for real tasks. We define a new divergence, margin disparity discrepancy, arXiv:1904.05801v1 [cs.\n\nLG] 11 Apr 2019 and provide margin-aware generalization bounds based on Rademacher complexity. This generalization bound shows that there is a trade-off between generalization error and the choice of margin. Our theory can be seamlessly transformed into an adversarial learning algorithm for domain adaptation. A series of empirical studies show that our algorithm achieves state-of-the-art accuracies on challenging tasks.\n\n\nPreliminaries\n\nIn this section we introduce some basic notations and assumptions for classification problems in domain adaptation.\n\n\nLearning Setup\n\nIn supervised learning setting, the learner receives a sample of n labeled points {(x i , y i )} n i=1 from X \u00d7 Y, where X is an input space and Y is an output space, which is {0, 1} in binary classification and {1, \u00b7 \u00b7 \u00b7 , k} in multiclass classification. The sample is denoted by D if independently drawn according to the distribution D.\n\nIn unsupervised domain adaptation, there are two different distributions, the source P and the target Q. The learner is trained on a set consisting of a labeled sample\n{(x s i , y s i )} n i=1\ndrawn from the source distribution and an unlabeled sample {x t i } m i=1 drawn from the target distribution. Following the notations of , we consider multiclass classification with hypothesis space F of scoring functions f : X \u2192 R |Y| = R k , where the outputs on each dimension indicate the confidence of prediction. The predicted label associated to point x is the one resulting in the largest score f (x, y). Thus it induces a labeling function space H containing h f from X to Y:\nh f : x \u2192 arg max y\u2208Y f (x, y).(1)\nThe (expected) error rate and empirical error rate of a classifier h \u2208 H with respect to distribution D are given by\nerr D (h) E (x,y)\u223cD 1[h(x) = y)], err D (h) E (x,y)\u223c D 1[h(x) = y)] = 1 n n i=1 1[h(x i ) = y i )],(2)\nwhere 1 is the indicator function.\n\nBefore further discussion, we assume the constant classifier 1 \u2208 H and H is closed under permutations of Y. For binary classification, this is equivalent to the assumption that for any h \u2208 H, we have 1 \u2212 h \u2208 H.\n\n\nMargin Loss\n\nIn practice, the margin between data points and the classification surface plays a significant role in achieving strong generalization performance. Thus a margin theory for classification was developed by Koltchinskii et al. (2002), where the 0-1 loss is replaced by the margin loss.\n\nDefine the margin of a hypothesis f at a labeled example (x, y) as\n\u03c1 f (x, y) 1 2 (f (x, y) \u2212 max y =y f (x, y )).(3)\nThe corresponding margin loss and empirical margin loss of a hypothesis f is\nerr (\u03c1) D (f ) E x\u223cD \u03a6 \u03c1 \u2022 \u03c1 f (x, y), err (\u03c1) D (f ) E x\u223c D \u03a6 \u03c1 \u2022 \u03c1 f (x, y) = 1 n n i=1 \u03a6 \u03c1 (\u03c1 f (x i , y i )),(4)\nwhere \u2022 denotes function composition, and \u03a6 \u03c1 is\n\u03a6 \u03c1 (x) \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 0 \u03c1 \u2264 x 1 \u2212 x/\u03c1 0 \u2264 x \u2264 \u03c1 1 x \u2264 0 .(5)\nAn important property is that err\n(\u03c1) D (f ) \u2265 err D (h f )\nfor any \u03c1 > 0 and f \u2208 F. Koltchinskii et al. (2002) showed that the margin loss leads to an informative generalization bound for classification. Based on this seminal work, we shall develop margin bounds for classification in domain adaptation.\n\n\nTheoretical Guarantees\n\nIn this section, we give theoretical guarantees for domain adaptation. All proofs can be found in the Appendix.\n\nTo reduce the error rate on the target with labeled training data only on the source, the distributions P and Q should not be dissimilar substantially. Thus a measurement of their discrepancy is crucial in domain adaptation theory.\n\nIn the seminal work of Ben-David et al. (2010), the H\u2206Hdivergence was proposed to measure such discrepancy, Mansour et al. (2009c) extended the H\u2206H-divergence to general loss functions, leading to the discrepancy distance:\nd H\u2206H = sup h,h \u2208H |E Q 1[h = h] \u2212 E P 1[h = h]| . (6)disc L = sup h,h \u2208H |E Q L(h , h) \u2212 E P L(h , h)|,(7)\nwhere L should be a bounded function satisfying symmetry and triangle inequality. Note that many widely-used losses, e.g. margin loss, do not satisfy these requirements.\n\nWith these discrepancy measures, generalization bounds based on VC-dimension and Rademacher complexity were rigorously derived for domain adaptation. While these theories have made influential impact in advancing algorithm designs, there are two crucial directions for improvement:\n\n1. Generalization bound for classification with scoring functions has not been formally studied in the domain adaptation setting. As scoring functions with margin loss provide informative generalization bound in standard classification, there is strong motivation to develop a margin theory for domain adaptation.\n\n2. The hypothesis-induced discrepancies require taking supremums over hypothesis space H\u2206H, while achieving lower generalization bound requires minimizing these discrepancies. These naturally result in minimax optimization problems that will become hard to reach equilibrium since H\u2206H is generally overly large.\n\nThese directions are the pain points in practical algorithm designs. When a practitioner designs a domain adaptation algorithm using scoring functions, he may suspect whether his algorithm is theoretically guaranteed since there is a gap between the loss functions used in the theories and algorithms. Furthermore, there is another gap between the hypothesis-induced discrepancies in theories and the widelyused divergences in domain adaptation algorithms, including Jensen Shannon Divergence (Ganin & Lempitsky, 2015), Maximum Mean Discrepancy (Gretton et al., 2012;Long et al., 2015), and Wasserstein Distance (Courty et al., 2017).\n\nIn this work, we aim to bridge the gaps between the theories and algorithms for domain adaptation. Our margin theory is developed based on a novel margin disparity discrepancy.\n\n\nMargin Disparity Discrepancy\n\nFirst, we give an improved discrepancy for measuring the distribution difference by restricting the hypothesis space.\n\nGiven two hypotheses h, h \u2208 H, we define the (expected) 0-1 disparity between them as\ndisp D (h, h ) E D 1[h = h],(8)\nand the empirical 0-1 disparity as\ndisp D (h, h ) E D 1[h = h] = 1 n n i=1 1[h (x i ) = h(x i )].(9)\nDefinition 3.1 (Disparity Discrepancy, DD). Given a hypothesis space H and a specific classifier h \u2208 H, the Dispar-ity Discrepancy induced by h \u2208 H is defined by\nd h,H (P, Q) sup h \u2208H (disp Q (h, h ) \u2212 disp P (h, h )) = sup h \u2208H (E Q 1[h = h] \u2212 E P 1[h = h]).\n(10) Similarly, the empirical disparity discrepancy is\nd h,H ( P , Q) sup h \u2208H (disp Q (h, h ) \u2212 disp P (h, h )). (11)\nNote that the disparity discrepancy is not only dependent on the hypothesis space H, but also on a chosen classifier h. In the Appendix we shall prove that this discrepancy can well measure the difference of distributions (actually a pseudo-metric in the binary case). Furthermore, for binary classification the target expected risk can be controlled by a VC-dimension generalization bound via disparity discrepancy. Compared with the H\u2206H-divergence, the supremum in the disparity discrepancy is taken only over the hypothesis space H and thus can be optimized more easily. This will significantly ease the minimax optimization widely used in many domain adaptation algorithms.\n\nIn the case of multiclass classification, the margin of scoring functions becomes an important factor for informative generalization bound, as envisioned by Koltchinskii et al. (2002). Existing domain adaptation theories (Ben-David et al., 2007;Blitzer et al., 2008;Mansour et al., 2009c) do not give a formal analysis of generalization bound with scoring functions and margin loss. Hence, to bridge the gap between theories that typically analyze labeling functions and losses of symmetry and subadditivity, and algorithms that widely adopt scoring functions and margin losses, we propose a margin based disparity discrepancy.\n\nThe margin disparity, i.e. disparity by changing the 0-1 loss to the margin loss, and its empirical version from hypothesis f to f are defined as\ndisp (\u03c1) D (f, f ) E D \u03a6 \u03c1 \u2022 \u03c1 f (\u00b7, h f ) disp (\u03c1) D (f, f ) E D \u03a6 \u03c1 \u2022 \u03c1 f (\u00b7, h f ) = 1 n n i=1 \u03a6 \u03c1 \u2022 \u03c1 f (x i , h f (x i )).(12)\nNote that here f and f are scoring functions while h f and h f are their labeling functions. Note also that the margin disparity is not a symmetric function on f and f , and the generalization theory w.r.t. this loss could be quite different from that for the discrepancy distance (Mansour et al., 2009c), which requires symmetry and subadditivity.\n\nDefinition 3.2 (Margin Disparity Discrepancy, MDD).\n\nWith the definition of margin disparity, we define Margin Disparity Discrepancy and its empirical version by\nd (\u03c1) f,F (P, Q) sup f \u2208F disp (\u03c1) Q (f, f ) \u2212 disp (\u03c1) P (f, f ) , d (\u03c1) f,F ( P , Q) sup f \u2208F disp (\u03c1) Q (f, f ) \u2212 disp (\u03c1) P (f, f ) .(13)\nWe demonstrate several properties to show that Margin Disparity Discrepancy (MDD) is well-defined and has the ability to measure distribution difference. First it is trivial to justify that d (\u03c1)\n\nf,F (P, P ) = 0 and satisfies nonnegativity and subadditivity. Despite of its asymmetry, MDD is a welldefined discrepancy for domain adaptation due to the following proposition.\n\nProposition 3.3. For any scoring function f ,\nerr Q (h f ) \u2264 err (\u03c1) P (f ) + d (\u03c1) f,F (P, Q) + \u03bb,(14)\nwhere\n\u03bb = \u03bb(\u03c1, F, P, Q) is a constant independent of f .\nThis upper bound has a similar form with the learning bound proposed by Ben-David et al. (2010). \u03bb is determined by the learning problem and can be reduced to a rather small value if the hypothesis space is rich enough. err \n\n\nDomain Adaptation: Generalization Bounds\n\nIn this subsection, we provide several generalization bounds for multiclass domain adaptation based on margin loss and disparity discrepancy. First, we present a Rademacher complexity bound for the difference between MDD and its empirical version. Then, we combine the Rademacher bound for MDD and Proposition 3.3 to derive the final generalization bound. In addition, we bound the empirical Rademacher term by the notions of covering number and fat-shattering dimension for further interpretation.\n\nTo begin with, we introduce a new function class \u03a0 H F serving as a \"scoring\" version of the symmetric difference hypothesis space H\u2206H in (Ben-David et al., 2010).\n\nDefinition 3.4. Given a class of scoring functions F and a class of the induced classifiers H, we define \u03a0 H F as\n\u03a0 H F = {x \u2192 f (x, h(x))|h \u2208 H, f \u2208 F}. (15)\nThere is a geometric interpretation of the set \u03a0 H F (Galbis & Maestre, 2012 \n\u03a0 H F = H, F = { h, f h \u2208 H, f \u2208 F}. (16)\nNow we introduce the Rademacher complexity, commonly used in the generalization theory as a measurement of richness for a particular hypothesis space .\n\nDefinition 3.5 (Rademacher Complexity). Let F be a family of functions mapping from Z = X \u00d7 Y to [a, b] and D = {z 1 , \u00b7 \u00b7 \u00b7 , z n } a fixed sample of size n drawn from the distribution D over Z. Then, the empirical Rademacher complexity of F with respect to the sample D is defined as\nR D (F) E \u03c3 sup f \u2208F 1 n n i=1 \u03c3 i f (z i ).(17)\nwhere \u03c3 i 's are independent uniform random variables taking values in {\u22121, +1}. The Rademacher complexity is\nR n,D (F) E D\u223cD n R D (F).(18)\nWith the Rademacher complexity, we proceed to show that MDD can be well estimated through finite samples.\n\nLemma 3.6. For any \u03b4 > 0, with probability 1 \u2212 2\u03b4,\n|d (\u03c1) f,F ( P , Q) \u2212 d (\u03c1) f,F (P, Q)| \u2264 k \u03c1 R n,P (\u03a0 H F) + k \u03c1 R m,Q (\u03a0 H F) + log 2 \u03b4 2n + log 2 \u03b4 2m .(19)\nCombining Proposition 3.3 and Lemma 3.6, we obtain a Rademacher complexity based generalization bound of the expected target error through the empirical MDD.\n\nTheorem 3.7 (Generalization Bound). For all \u03b4 > 0, with probability 1 \u2212 3\u03b4,\nerr Q (f ) \u2264err (\u03c1) P (f ) + d (\u03c1) f,F ( P , Q) + \u03bb + 2k 2 \u03c1 R n,P (\u03a0 1 F) + k \u03c1 R n,P (\u03a0 H F) + 2 log 2 \u03b4 2n + k \u03c1 R m,Q (\u03a0 H F) + log 2 \u03b4 2m .(20)\nCompared with the bounds based on 0-1 loss and H\u2206Hdivergence in Ben-David et al. (2010); Mansour et al. (2009c), this generalization bound is sharper and more informative. Through choosing a better margin \u03c1, we could get better generalizablity on the target domain. Moreover, we point out that there is a trade-off between generalization and optimization in the choice of \u03c1. For relatively small \u03c1 and rich hypothesis space, the first two terms do not differ too much according to \u03c1 so the right-hand side becomes smaller with the increase of \u03c1. However, when \u03c1 is too large, these terms cannot be optimized to reach an acceptable small value.\n\nAlthough we have developed the margin bound, the value of the empirical Rademacher complexity in Theorem 3.8 is still not explicit enough. We need to check the variation of R n,D (\u03a0 H F) with the growth of n. To this end, we describe the notion of covering number from Zhou (2002) Intuitively a covering number N 2 (\u03c4, G) is the minimal number of L 2 balls of radius \u03c4 > 0 needed to cover a class G of bounded functions g : X \u2192 R and can be interpreted as a measure of the richness of the class G at scale \u03c4 . A rigorous definition is given in the Appendix together with a proof of the following covering number bound for MDD.\n\nTheorem 3.8 (Generalization Bound by Covering Number). Suppose \u03a0 1 F is bounded in L 2 by L. For \u03b4 > 0, with probability 1 \u2212 3\u03b4,\nerr Q (f ) \u2264 err (\u03c1) P (f ) + d (\u03c1) f,F ( P , Q) + \u03bb + 2 log 2 \u03b4 2n + log 2 \u03b4 2m + 16k 2 \u221a k \u03c1 inf \u22650 + 3 1 \u221a n + 1 \u221a m L log N 2 (\u03c4, \u03a0 1 F)d\u03c4 +L 1 /L log N 2 (\u03c4, \u03a0 1 H)d\u03c4(21)\nAlthough we have shown the generalization bounds for multiclass classification via covering number, a natural question is raised: what is the relationship between the covering number bound and traditional VC-dimension bound, especially when k = 2, i.e. multiclass classification degenerating into binary classification. To answer this we need the notion of fat-shattering dimension (Mendelson & Vershynin, 2003;Rakhlin & Sridharan, 2014). For concision, we only present the final result here and leave the definition and proof to the Appendix.\n\nCorollary 3.9. Let Fat \u03b3 (\u03a0 1 F) be the fat-shattering dimension of \u03a0 1 F with scale \u03b3 and VC(\u03a0 1 H) be the VCdimension of \u03a0 1 H. Then there exist constants C 1 , C 2 , C 3 > 0, 0 < c < 1 independent of n, m and F, H such that for \u03b4 > 0, with probability 1 \u2212 3\u03b4,\nerr Q (f ) \u2264 err (\u03c1) P (f ) + d (\u03c1) f,F ( P , Q) + \u03bb + k 2 \u221a k \u03c1 C 1 L 1 \u221a n + 1 \u221a m VC(\u03a0 1 H) + k 2 \u221a k \u03c1 inf \u22650 C 2 +C 3 1 \u221a n + 1 \u221a m L Fat c\u03c4 (\u03a0 1 F)log 2 \u03c4 d\u03c4 + 2 log 2 \u03b4 2n + log 2 \u03b4 2m .(22)\nNote that when the class number k = 2, the VC dimension VC(\u03a0 1 H) = VC(H). Thus this term coincides with Ben-David et al. (2010) in the order of sample complexity.\n\nIn summary, our theory is a bold attempt towards filling the two gaps mentioned at the beginning of this section. Firstly, we provide a thorough analysis for multiclass classification with domain adaptation. Secondly, our bound is based on scoring functions and margin loss. Thirdly, as the measure of distribution shift, MDD is defined by taking supremum over the hypothesis space F which can be much smaller than H\u2206H, making the minimax optimization problem easier to solve.\n\n\nAlgorithm\n\nAccording to the above theory, we propose an adversarial representation learning method for domain adaptation. Note that MDD not only sheds lights on understanding the role of margin in domain adaptation, but also leads to a strong and efficient algorithm with theoretical guarantees.\n\n\nMinimax Optimization Problem\n\nRecall that the expected error err Q (f ) on target domain is bounded by the sum of four terms: empirical margin error on the source domain err\n(\u03c1) P (f ), empirical MDD d (\u03c1)\nf,F ( P , Q), the ideal error \u03bb and complexity terms. We need to solve the following minimization problem in hypothesis space F:\nmin f \u2208F err (\u03c1) P (f ) + d (\u03c1) f,F ( P , Q).(23)\nMinimizing margin disparity discrepancy is a minimax problem since MDD is defined as the supremum over hypothesis space F. Denote the feature extractor by \u03c8. Applying \u03c8 to the source and target distributions, the overall optimization problem can be written as\nmin f,\u03c8 err (\u03c1) \u03c8( P ) (f ) + (disp (\u03c1) \u03c8( Q) (f, f * ) \u2212 disp (\u03c1) \u03c8( P ) (f, f * )), f * = max f (disp (\u03c1) \u03c8( Q) (f, f ) \u2212 disp (\u03c1) \u03c8( P ) (f, f )).(24)\nTo enable representation-based domain adaptation, we need to learn new representation \u03c8 such that MDD is minimized. Now we design an adversarial learning algorithm to solve this problem by introducing an auxiliary classifier f sharing the same hypothesis space with f . Also since the margin loss is hard to optimize via stochastic gradient descent in practice, we shall use a combination of loss functions L and L in substitution to the margin loss, which well preserve the key property of the margin. The practical optimization problem in the adversarial learning can be stated as\nmin f,\u03c8 E( P ) + \u03b7D \u03b3 ( P , Q), max f D \u03b3 ( P , Q),(25)\nwhere \u03b7 is the trade-off coefficient between source error E( P ) and MDD D \u03b3 ( P , Q), \u03b3 exp \u03c1 is designed to attain the margin \u03c1 (detailed in the next subsection) and\nE( P ) = E (x s ,y s )\u223c P L[f (\u03c8(x s )), y s ], D \u03b3 ( P , Q) = E x t \u223c Q L [f (\u03c8(x t )), f (\u03c8(x t ))] \u2212 \u03b3E x s \u223c P L[f (\u03c8(x s )), f (\u03c8(x s ))].(26)\n\nCombined Cross-Entropy Loss\n\nAs we have pointed above, multiclass margin loss or hinge loss causes the problem of gradient vanishing in stochastic gradient descent, and thus cannot be optimized efficiently, especially for representation learning that significantly relies on gradient propagation. To overcome this common issue, we choose different loss functions on source and target and reweight them to approximate MDD.\n\nDenote the softmax function by \u03c3, i.e. for z \u2208 R k \u03c3 j (z) = e zj k i=1 e z k for j = 1, \u00b7 \u00b7 \u00b7 , k.\n\nOn the source domain, err \nL(f (\u03c8(x s )), y s ) \u2212 log[\u03c3 y s (f (\u03c8(x s )))], L(f (\u03c8(x s )), f (x s )) \u2212 log[\u03c3 h f (\u03c8(x s )) (f (\u03c8(x s )))].(28)\nOn the target domain, we use a modified cross-entropy loss\nL (f (\u03c8(x t )), f (\u03c8(x t ))) log[1\u2212\u03c3 h f (\u03c8(x t )) (f (\u03c8(x t )))].\n(29) Note that this modification was introduced in Goodfellow et al. (2014) to mitigate the burden of exploding or vanishing gradients when performing adversarial learning.\n\nCombining the above two terms with a coefficient \u03b3, the objective of the auxiliary classifier f can be formulated as\nmax f \u03b3 E x s \u223c P log[\u03c3 h f (\u03c8(x s )) (f (\u03c8(x s )))] + E x t \u223c Q log[1 \u2212 \u03c3 h f (\u03c8(x t )) (f (\u03c8(x t )))].(30)\nWe shall see that training the feature extractor \u03c8 to minimize loss function (30) will lead to \u03c8( P ) \u2248 \u03c8( Q). For \u03b3 > 1, we claim that the value of \u03c3 h f (f (\u00b7)) will reach \u03b3 \u03b3+1 and the margin of f will be around log \u03b3 at equilibrium. Thus we call it the margin factor. The detailed explanation is written in the Appendix.\n\nOn the other hand, as we have explained in Section 3, we cannot let \u03c1 or \u03b3 go to infinity. In fact, from an empirical view \u03c1 can only be chosen far beyond the theoretical optimal value since performing SGD for a large \u03b3 might lead to exploding gradients. In summary, the choice of \u03b3 is crucial in our method and we always prefer relatively larger \u03b3 in practice when exploding gradients are not encountered.\n\n\nExperiments\n\nWe validate the proposed learning method on three benchmark datasets against deep domain adaptation methods. Codes and datasets will be available online.\n\n\nSetup\n\nOffice-31 (Saenko et al., 2010) is a standard domain adaptation dataset of three diverse domains, Amazon from Amazon website, Webcam by web camera and DSLR by digital SLR camera with 31 unbalanced classes and 4,652 images.\n\nOffice-Home (Venkateswara et al., 2017) is a more complex dataset containing 15,500 images from four extremely distinct domains: Artistic images, Clip Art, Product images, and Real-world images. We do experiments with our methods on all twelve transfer tasks among these four domains.\n\nVisDA2017 (Peng et al., 2017) is simulation-to-real dataset with two domains: Synthetic renderings of 3D models generated from different angles and with different lighting conditions and Real collected from photo-realistic or real-image datasets. Since the 3D models were generated in clean environment, the Synthetic domain is very different from the Real domain. With 280K images across 12 classes, the scale of VisDA2017 also brings challenges to domain adaptation. We compare our model based on Margin Disparity Discrepancy (MDD) with state-of-the-art domain adaptation methods: Deep Adaptation Network (DAN) (Long et al., 2015), Domain Adversarial Neural Network (DANN) (Ganin et al., 2016), Joint Adaptation Network (JAN) (Long et al., 2017), Adversarial Discriminative Domain Adaptation (ADDA) (Tzeng et al., 2017), Generate to Adapt (GTA) (Sankaranarayanan et al., 2018), Maximum Classifier Discrepancy  (Long et al., 2017) 61.6 GTA (Sankaranarayanan et al., 2018) 69.5 MCD (Saito et al., 2018) 69.8 CDAN (Long et al., 2018) 70.0 MDD 74.6 (MCD) (Saito et al., 2018), and Conditional Domain Adversarial Network (CDAN) (Long et al., 2018).\n\nWe follow the commonly used experiment protocol for unsupervised domain adaptation from Ganin & Lempitsky (2015); Long et al. (2018). We report the average accuracies of five independent experiments. The importance-weighted cross-validation (IWCV) is employed in all experiments for the selection of hyper-parameters. The asymptotic value of coefficient \u03b7 is fixed to 0.1 and \u03b3 is chosen from {2, 3, 4} and kept the same for all tasks on the same dataset.\n\nWe implement our algorithm in PyTorch. For the deep learning experiments, ResNet-50 (He et al., 2016) is adopted as the feature extractor with parameters fine-tuned from the pre-trained ImageNet (Russakovsky et al., 2014). The main classifier and auxiliary classifier are both 2-layer neural networks with width 1024. For optimization, we use the mini-batch SGD with the Nestorov momentum 0.9. The minimax problem is tackled by introducing a gradient reversal layer (Ganin et al., 2016). The learning rate of the classifiers are set 10 times to that of the feature extractor, the value of which is adjusted according to Ganin et al. (2016). \n\n\nResults\n\nThe results on Office-31 are reported in Table 1. We could see that MDD achieves state-of-the-art accuracies on five out of six transfer tasks. We notice that in previous works, feature alignment methods (JAN, CDAN) generally perform better for large-to-small tasks (A\u2192W, A\u2192D) while pixellevel adaptation methods (GTA) attend to get higher accuracy for small-to-large ones (W\u2192A, D\u2192A). Nevertheless our algorithm outperforms both types of methods on almost all task, demonstrating its effectiveness and universality. Table 2 and 3 present the accuracies of our algorithm on Office-Home and VisDA. We make remarkable performance boost on all tasks. Remark that some of the methods listed in the tables use additional techniques such as the entropy minimization to enhance their performance. Our method possesses both simplicity and performance strength.\n\n\nVerification and Analysis\n\nIn our adversarial learning algorithm, we reasonably use the combined cross-entropy loss instead of the margin loss and disparity discrepancy in our proposed theory. We need to show that despite the technical modification, our algorithm can well reduce empirical MDD computed according to f :\ndisp (\u03c1) Q (f, f ) \u2212 disp (\u03c1) P (f, f ).(31)\nWe choose \u03b3 = 1, 2, 4 for comparison. The expected margin should reach log 2 and log 4 in the last two cases while there is no guarantee for margin with \u03b3 = 1. Correspondingly, we examine DD (based on 0-1 loss), log 2-MDD and log 4-MDD for task D\u2192A and show the results in Figures 1\u223c2.\n\nFirst, we verify that without the minimization part of the adversarial training, the auxiliary classifier f in Eq. (30) is close to the f that maximizes MDD over F. We solve this optimization problem by directly training without the auxiliary classifier and show our results in 2(a), where MDD reaches 1 shortly after training begins, implying that the loss function we use can well substitute MDD.\n\nNext, we consider the equilibrium of the minimax optimization. The average values of \u03c3 h f \u2022 f over source and target are presented in Figures 1(b) and 1(c). We could see that at the final training stage, \u03c3 h f \u2022 f is close to the predicted value, which gives rise to large margin.\n\nLast, by visualizing the values of DD, log 2-MDD and log 4-MDD and test accuracy computed over the whole dataset every 100 steps, we could see that larger \u03b3 leads to smaller MDD and higher test accuracy. This conforms to our theory.\n\n\nRelated Work\n\nDomain Adaptation Theory The earliest theoretical work in this field was conducted by Ben-David et al. (2007). They proposed the H\u2206H-divergence as a substitution of traditional distribution discrepancies (e.g. total variation, KL-divergence), which overcame the difficulties of estimation from finite samples. Analysis via the H\u2206H-divergence yields a generalization bound for target expected risk measured by 0-1 loss in the setting of binary classification. Advancing this work, Mansour et al. (2009c) considered a general class of loss functions satisfying symmetry and subadditivity and developed a generalization theory with respect to the newly proposed discrepancy distance. Mohri & Medina (2012); Zhang et al. (2012) proposed Y-disc for domain adaptation with partially labeled target data. Cortes & Mohri (2014); Cortes et al. (2015) further proposed a theory for regression tasks in the presence of domain adaptation via the discrepancy distance and the generalized discrepancy, together with discrepancy minimization and importance reweighting algorithms. In addition, Germain et al. (2013) proposed a PAC-Bayesian theory for domain adaptation using the domain disagreement pseudometric. Previous works mainly developed the theories based on symmetric loss such as 0-1 loss and L 2 loss. We extend them to margin loss and scoring functions for multiclass classification. Domain Adaptation Algorithm With the increasing prevalence of deep learning, the domain adaptation methods based on deep neural networks have also achieved great success in recent years (Long et al., 2015;Ganin & Lempitsky, 2015). These works aim to learn domain-invariant representations by minimizing a certain discrepancy between distributions of source and target features extracted by a shared representation learner. With insights both from the theory of Ben-David et al. (2010) and the practice of adversaral learning (Goodfellow et al., 2014), Ganin & Lempitsky (2015) put forward the domain adversarial neural network (DANN). Similar to the two-player game in Goodfellow et al. (2014), an auxiliary network named discriminator was trained to distinguish the source from target domain features and a feature extractor to confuse the discriminator by stochastic gradient descent. They also pointed out in that work that the success of DANN possibly resulted from the minimization of H\u2206H-divergence between feature distributions. Since then, a series of works have appeared and achieved significantly better performance (Tzeng et al., 2017;Long et al., 2018;Sankaranarayanan et al., 2018;Hoffman et al., 2018b). Tzeng et al. (2017) proposed an architecture that employed asymmetric encodings for target and source data. Long et al. (2018) presented a principled framework that conducted the adversarial adaptation models using semantic information. Hoffman et al. (2018b); Sankaranarayanan et al. (2018) unified pixel-level and feature-level adversarial learning for domain adaptation. Saito et al. (2018) considered the classifiers instead of features and designed an original adversarial learning method by maximizing classifier discrepancy in approximation of H\u2206H. However, they trained two classifiers on the source domain and used L 1 loss to approximate 0-1 loss, which may not be rigorously guaranteed by the theory.\n\nP\n(f ) depicts the performance of f on source domain and MDD bounds the performance gap caused by domain shift. This margin bound gives a new perspective for analyzing domain adaptation with respect to scoring functions and margin loss.\n\nP\n(f, f ) are replaced by the cross-entropy loss\n\nFigure 1 .Figure 2 .\n12Test accuracy and the values of \u03c3 h f \u2022 f on transfer task D \u2192 A. Empirical values of the margin disparity discrepancy (MDD) computed by auxiliary classifier f .\n\n\n). Equal contribution 1 Tsinghua University, Beijing 100084, China 2 University of California, Berkeley, CA, USA. Correspondence to: Mingsheng Long <mingsheng@tsinghua.edu.cn>.* Preliminary work. \n\n\n\n\n). Assuming X is a manifold, assigning a vector space R k to each point in X yields a vector bundle B. Now regarding the values of H as one-hot vectors in R k , F and H are both sets of sections of B containing (probably piecewise continuous) vector fields. \u03a0 H F can be seen as the space of inner products of vector fields from H and F,\n\nTable 1 .\n1Accuracy (%) on Office-31 for unsupervised domain adaptation Method \nA \u2192 W \nD \u2192 W \nW \u2192 D \nA \u2192 D \nD \u2192 A \nW \u2192 A \nAvg \n\nResNet-50 (He et al., 2016) \n68.4\u00b10.2 \n96.7\u00b10.1 \n99.3\u00b10.1 \n68.9\u00b10.2 \n62.5\u00b10.3 \n60.7\u00b10.3 \n76.1 \nDAN (Long et al., 2015) \n80.5\u00b10.4 \n97.1\u00b10.2 \n99.6\u00b10.1 \n78.6\u00b10.2 \n63.6\u00b10.3 \n62.8\u00b10.2 \n80.4 \nDANN (Ganin et al., 2016) \n82.0\u00b10.4 \n96.9\u00b10.2 \n99.1\u00b10.1 \n79.7\u00b10.4 \n68.2\u00b10.4 \n67.4\u00b10.5 \n82.2 \nADDA (Tzeng et al., 2017) \n86.2\u00b10.5 \n96.2\u00b10.3 \n98.4\u00b10.3 \n77.8\u00b10.3 \n69.5\u00b10.4 \n68.9\u00b10.5 \n82.9 \nJAN (Long et al., 2017) \n85.4\u00b10.3 \n97.4\u00b10.2 \n99.8\u00b10.2 \n84.7\u00b10.3 \n68.6\u00b10.3 \n70.0\u00b10.4 \n84.3 \nGTA (Sankaranarayanan et al., 2018) \n89.5\u00b10.5 \n97.9\u00b10.3 \n99.8\u00b10.4 \n87.7\u00b10.5 \n72.8\u00b10.3 \n71.4\u00b10.4 \n86.5 \nMCD(Saito et al., 2018) \n89.6\u00b10.2 \n98.5\u00b10.1 \n100.0\u00b1.0 \n91.3\u00b10.2 \n69.6\u00b10.1 \n70.8\u00b10.3 \n86.6 \nCDAN (Long et al., 2018) \n94.1\u00b10.1 \n98.6\u00b10.1 \n100.0\u00b1.0 \n92.9\u00b10.2 \n71.0\u00b10.3 \n69.3\u00b10.3 \n87.7 \nMDD \n94.5\u00b10.3 \n98.4\u00b10.1 \n100.0\u00b1.0 \n93.5\u00b10.2 \n74.6\u00b10.3 \n72.2\u00b10.1 \n88.9 \n\nTable 2. Accuracy (%) on Office-Home for unsupervised domain adaptation (ResNet-50) \n\nMethod \nAr Cl Ar Pr Ar Rw Cl Ar Cl Pr Cl Rw Pr Ar Pr Cl Pr Rw Rw Ar Rw Cl Rw Pr Avg \n\nResNet-50 (He et al., 2016) 34.9 50.0 58.0 37.4 41.9 46.2 38.5 31.2 60.4 \n53.9 \n41.2 \n59.9 46.1 \nDAN (Long et al., 2015) 43.6 57.0 67.9 45.8 56.5 60.4 44.0 43.6 67.7 \n63.1 \n51.5 \n74.3 56.3 \nDANN (Ganin et al., 2016) 45.6 59.3 70.1 47.0 58.5 60.9 46.1 43.7 68.5 \n63.2 \n51.8 \n76.8 57.6 \nJAN (Long et al., 2017) 45.9 61.2 68.9 50.4 59.7 61.0 45.8 43.4 70.3 \n63.9 \n52.4 \n76.8 58.3 \nCDAN (Long et al., 2018) 50.7 70.6 76.0 57.6 70.0 70.0 57.4 50.9 77.3 \n70.9 \n56.7 \n81.6 65.8 \nMDD \n54.9 73.7 77.8 60.0 71.4 71.8 61.2 53.6 78.1 \n72.5 \n60.2 \n82.3 68.1 \n\nTable 3. Accuracy (%) on VisDA-2017 (ResNet-50) \n\nMethod \nSynthetic \u2192 Real \n\nJAN \n\nTable 4 .\n4Accuracy (%) on Office-31 with different margins \u03b3\u03b3 \nA \u2192 W \nD \u2192 A \nAvg on Office-31 \n\n1 \n92.5 \n70.9 \n87.5 \n2 \n93.7 \n73.0 \n88.1 \n3 \n94.0 \n73.7 \n88.5 \n4 \n94.5 \n74.6 \n88.9 \n\n\nConclusionWe present a theoretical and algorithmic analysis of domain adaptation, which bridges the gap between theory and algorithm. We derive new generalization bounds in terms of Rademacher complexity. Our analysis is more suitable for analyzing real-world domain adaptation problems, and the theory-induced algorithm achieves state-of-the-art results.\nNeural network learning: Theoretical foundations. M Anthony, P L Bartlett, cambridge university pressAnthony, M. and Bartlett, P. L. Neural network learn- ing: Theoretical foundations. cambridge university press, 2009.\n\nAnalysis of representations for domain adaptation. S Ben-David, J Blitzer, K Crammer, F Pereira, Advances in Neural Information Processing Systems (NIPS). Ben-David, S., Blitzer, J., Crammer, K., and Pereira, F. Analysis of representations for domain adaptation. In Ad- vances in Neural Information Processing Systems (NIPS), 2007.\n\nA theory of learning from different domains. S Ben-David, J Blitzer, K Crammer, A Kulesza, F Pereira, J W Vaughan, Machine Learning. 79Ben-David, S., Blitzer, J., Crammer, K., Kulesza, A., Pereira, F., and Vaughan, J. W. A theory of learning from different domains. Machine Learning, 79(1-2):151-175, 2010.\n\nLearning bounds for domain adaptation. J Blitzer, K Crammer, A Kulesza, F Pereira, J Wortman, Advances in Neural Information Processing Systems. 20Blitzer, J., Crammer, K., Kulesza, A., Pereira, F., and Wort- man, J. Learning bounds for domain adaptation. In Advances in Neural Information Processing Systems 20, pp. 129-136. 2008.\n\nDomain adaptation and sample bias correction theory and algorithm for regression. C Cortes, M Mohri, Theoretical Computer Science. 519Cortes, C. and Mohri, M. Domain adaptation and sam- ple bias correction theory and algorithm for regression. Theoretical Computer Science, 519:103-126, 2014.\n\nAdaptation algorithm and theory based on generalized discrepancy. C Cortes, M Mohri, Mu\u00f1oz Medina, A , Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data MiningACMCortes, C., Mohri, M., and Mu\u00f1oz Medina, A. Adaptation algorithm and theory based on generalized discrepancy. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 169-178. ACM, 2015.\n\nJoint distribution optimal transportation for domain adaptation. N Courty, R Flamary, A Habrard, A Rakotomamonjy, Advances in Neural Information Processing Systems (NIPS). Courty, N., Flamary, R., Habrard, A., and Rakotomamonjy, A. Joint distribution optimal transportation for domain adaptation. In Advances in Neural Information Process- ing Systems (NIPS), pp. 3730-3739. 2017.\n\nLearning from multiple sources. K Crammer, M Kearns, J Wortman, Journal of Machine Learning Research. 9Crammer, K., Kearns, M., and Wortman, J. Learning from multiple sources. Journal of Machine Learning Research, 9(Aug):1757-1774, 2008.\n\nVector analysis versus vector calculus. A Galbis, M Maestre, Springer Science & Business MediaGalbis, A. and Maestre, M. Vector analysis versus vector calculus. Springer Science & Business Media, 2012.\n\nUnsupervised domain adaptation by backpropagation. Y Ganin, V Lempitsky, International Conference on Machine Learning (ICML). Ganin, Y. and Lempitsky, V. Unsupervised domain adapta- tion by backpropagation. In International Conference on Machine Learning (ICML), 2015.\n\nDomain-adversarial training of neural networks. Y Ganin, E Ustinova, H Ajakan, P Germain, H Larochelle, F Laviolette, M Marchand, V Lempitsky, The Journal of Machine Learning Research. 171JMLRGanin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., Marchand, M., and Lempitsky, V. Domain-adversarial training of neural networks. The Journal of Machine Learning Research (JMLR), 17(1): 2096-2030, 2016.\n\nA pac-bayesian approach for domain adaptation with specialization to linear classifiers. P Germain, A Habrard, F Laviolette, E Morvant, Proceedings of the 30th International Conference on Machine Learning. the 30th International Conference on Machine LearningPMLRGermain, P., Habrard, A., Laviolette, F., and Morvant, E. A pac-bayesian approach for domain adaptation with spe- cialization to linear classifiers. In Proceedings of the 30th International Conference on Machine Learning, pp. 738-746. PMLR, 2013.\n\nGenerative adversarial nets. I Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, Y Bengio, Advances in Neural Information Processing Systems (NIPS). Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. Generative adversarial nets. In Advances in Neural Information Processing Systems (NIPS), 2014.\n\nA kernel two-sample test. A Gretton, K Borgwardt, M Rasch, B Sch\u00f6lkopf, A Smola, Journal of Machine Learning Research (JMLR). 13Gretton, A., Borgwardt, K., Rasch, M., Sch\u00f6lkopf, B., and Smola, A. A kernel two-sample test. Journal of Machine Learning Research (JMLR), 13:723-773, 2012.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\n\nAlgorithms and theory for multiple-source adaptation. J Hoffman, M Mohri, N Zhang, Advances in Neural Information Processing Systems. 31Hoffman, J., Mohri, M., and Zhang, N. Algorithms and theory for multiple-source adaptation. In Advances in Neural Information Processing Systems 31, pp. 8256- 8266. 2018a.\n\nCyCADA: Cycle-consistent adversarial domain adaptation. J Hoffman, E Tzeng, T Park, J.-Y Zhu, P Isola, K Saenko, A Efros, Darrell , T , Proceedings of the 35th International Conference on Machine Learning. the 35th International Conference on Machine LearningHoffman, J., Tzeng, E., Park, T., Zhu, J.-Y., Isola, P., Saenko, K., Efros, A., and Darrell, T. CyCADA: Cycle-consistent adversarial domain adaptation. In Proceedings of the 35th International Conference on Machine Learning, pp. 1989-1998, 2018b.\n\nEmpirical margin distributions and bounding the generalization error of combined classifiers. The Annals of Statistics. V Koltchinskii, D Panchenko, 30Koltchinskii, V., Panchenko, D., et al. Empirical margin distributions and bounding the generalization error of combined classifiers. The Annals of Statistics, 30(1): 1-50, 2002.\n\nLearning transferable features with deep adaptation networks. M Long, Y Cao, J Wang, Jordan , M I , International Conference on Machine Learning (ICML). Long, M., Cao, Y., Wang, J., and Jordan, M. I. Learning transferable features with deep adaptation networks. In International Conference on Machine Learning (ICML), 2015.\n\nDeep transfer learning with joint adaptation networks. M Long, J Wang, Jordan , M I , International Conference on Machine Learning (ICML). Long, M., Wang, J., and Jordan, M. I. Deep transfer learning with joint adaptation networks. In International Confer- ence on Machine Learning (ICML), 2017.\n\nConditional adversarial domain adaptation. M Long, Z Cao, J Wang, Jordan , M I , Advances in Neural Information Processing Systems. 31Long, M., CAO, Z., Wang, J., and Jordan, M. I. Conditional adversarial domain adaptation. In Advances in Neural In- formation Processing Systems 31, pp. 1647-1657. 2018.\n\nMultiple source adaptation and the r\u00e9nyi divergence. Y Mansour, M Mohri, A Rostamizadeh, Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence. the Twenty-Fifth Conference on Uncertainty in Artificial IntelligenceAUAI PressMansour, Y., Mohri, M., and Rostamizadeh, A. Multiple source adaptation and the r\u00e9nyi divergence. In Proceed- ings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, pp. 367-374. AUAI Press, 2009a.\n\nDomain adaptation with multiple sources. Y Mansour, M Mohri, A Rostamizadeh, Advances in Neural Information Processing Systems. 21Mansour, Y., Mohri, M., and Rostamizadeh, A. Domain adaptation with multiple sources. In Advances in Neu- ral Information Processing Systems 21, pp. 1041-1048. 2009b.\n\nDomain adaptation: Learning bounds and algorithms. Y Mansour, M Mohri, A Rostamizadeh, 22nd Conference on Learning Theory. Mansour, Y., Mohri, M., and Rostamizadeh, A. Domain adaptation: Learning bounds and algorithms. In 22nd Conference on Learning Theory, COLT 2009, 2009c.\n\nEntropy and the combinatorial dimension. S Mendelson, R Vershynin, Inventiones mathematicae. 1521Mendelson, S. and Vershynin, R. Entropy and the combi- natorial dimension. Inventiones mathematicae, 152(1): 37-55, 2003.\n\nNew analysis and algorithm for learning with drifting distributions. M Mohri, A Medina, International Conference on Algorithmic Learning Theory. SpringerMohri, M. and Medina, A. M. New analysis and algorithm for learning with drifting distributions. In International Conference on Algorithmic Learning Theory, pp. 124- 138. Springer, 2012.\n\nFoundations of machine learning. M Mohri, A Rostamizadeh, A Talwalkar, Mohri, M., Rostamizadeh, A., and Talwalkar, A. Founda- tions of machine learning. 2012.\n\nA survey on transfer learning. S J Pan, Q Yang, IEEE Transactions on Knowledge and Data Engineering (TKDE). 2210Pan, S. J. and Yang, Q. A survey on transfer learning. IEEE Transactions on Knowledge and Data Engineering (TKDE), 22(10):1345-1359, 2010.\n\nDomain adaptation via transfer component analysis. S J Pan, I W Tsang, J T Kwok, Yang , Q , IEEE Transactions on Neural Networks (TNN). 222Pan, S. J., Tsang, I. W., Kwok, J. T., and Yang, Q. Do- main adaptation via transfer component analysis. IEEE Transactions on Neural Networks (TNN), 22(2):199-210, 2011.\n\nX Peng, B Usman, N Kaushik, J Hoffman, D Wang, K Saenko, Visda, abs/1710.06924The visual domain adaptation challenge. CoRR. Peng, X., Usman, B., Kaushik, N., Hoffman, J., Wang, D., and Saenko, K. Visda: The visual domain adaptation challenge. CoRR, abs/1710.06924, 2017.\n\nDataset shift in machine learning. J Quionero-Candela, M Sugiyama, A Schwaighofer, N D Lawrence, The MIT PressQuionero-Candela, J., Sugiyama, M., Schwaighofer, A., and Lawrence, N. D. Dataset shift in machine learning. The MIT Press, 2009.\n\nStatistical learning and sequential prediction. A Rakhlin, K Sridharan, Book DraftRakhlin, A. and Sridharan, K. Statistical learning and se- quential prediction. Book Draft, 2014.\n\n. O Russakovsky, J Deng, H Su, J Krause, S Satheesh, S Ma, Z Huang, A Karpathy, A Khosla, M Bernstein, A C Berg, L Fei-Fei, ImageNet Large Scale Visual Recognition ChallengeRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. ImageNet Large Scale Visual Recognition Challenge. 2014.\n\nAdapting visual category models to new domains. K Saenko, B Kulis, M Fritz, Darrell , T , European Conference on Computer Vision (ECCV). Saenko, K., Kulis, B., Fritz, M., and Darrell, T. Adapting visual category models to new domains. In European Conference on Computer Vision (ECCV), 2010.\n\nMaximum classifier discrepancy for unsupervised domain adaptation. K Saito, K Watanabe, Y Ushiku, T Harada, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionSaito, K., Watanabe, K., Ushiku, Y., and Harada, T. Max- imum classifier discrepancy for unsupervised domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3723- 3732, 2018.\n\nGenerate to adapt: Aligning domains using generative adversarial networks. S Sankaranarayanan, Y Balaji, C D Castillo, R Chellappa, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Sankaranarayanan, S., Balaji, Y., Castillo, C. D., and Chel- lappa, R. Generate to adapt: Aligning domains using generative adversarial networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.\n\nUpper and lower bounds for stochastic processes: modern methods and classical problems. M Talagrand, Springer Science & Business Media60Talagrand, M. Upper and lower bounds for stochastic pro- cesses: modern methods and classical problems, vol- ume 60. Springer Science & Business Media, 2014.\n\nDeep domain confusion: Maximizing for domain invariance. E Tzeng, J Hoffman, N Zhang, K Saenko, Darrell , T , abs/1412.3474CoRRTzeng, E., Hoffman, J., Zhang, N., Saenko, K., and Darrell, T. Deep domain confusion: Maximizing for domain invariance. CoRR, abs/1412.3474, 2014.\n\nAdversarial discriminative domain adaptation. E Tzeng, J Hoffman, K Saenko, Darrell , T , IEEE Conference on Computer Vision and Pattern Recognition. Tzeng, E., Hoffman, J., Saenko, K., and Darrell, T. Ad- versarial discriminative domain adaptation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\n\nDeep hashing network for unsupervised domain adaptation. H Venkateswara, J Eusebio, S Chakraborty, S Panchanathan, IEEE Conference on Computer Vision and Pattern Recognition (CVPR. Venkateswara, H., Eusebio, J., Chakraborty, S., and Pan- chanathan, S. Deep hashing network for unsupervised domain adaptation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\n\nGeneralization bounds for domain adaptation. C Zhang, L Zhang, Ye , J , Advances in Neural Information Processing Systems. 25Zhang, C., Zhang, L., and Ye, J. Generalization bounds for domain adaptation. In Advances in Neural Information Processing Systems 25, pp. 3320-3328. 2012.\n\nThe covering number in learning theory. D.-X Zhou, Journal of Complexity. 183Zhou, D.-X. The covering number in learning theory. Jour- nal of Complexity, 18(3):739-767, 2002.\n", "annotations": {"author": "[{\"end\":68,\"start\":55},{\"end\":80,\"start\":69},{\"end\":96,\"start\":81},{\"end\":114,\"start\":97}]", "publisher": null, "author_last_name": "[{\"end\":67,\"start\":62},{\"end\":79,\"start\":76},{\"end\":95,\"start\":91},{\"end\":113,\"start\":107}]", "author_first_name": "[{\"end\":61,\"start\":55},{\"end\":75,\"start\":69},{\"end\":90,\"start\":81},{\"end\":104,\"start\":97},{\"end\":106,\"start\":105}]", "author_affiliation": null, "title": "[{\"end\":52,\"start\":1},{\"end\":166,\"start\":115}]", "venue": null, "abstract": "[{\"end\":1330,\"start\":168}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b30\"},\"end\":1961,\"start\":1930},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":1977,\"start\":1961},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2074,\"start\":2051},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2098,\"start\":2076},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2360,\"start\":2338},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2381,\"start\":2360},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2401,\"start\":2381},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2554,\"start\":2532},{\"end\":2556,\"start\":2554},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2578,\"start\":2556},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2760,\"start\":2742},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":2779,\"start\":2760},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2797,\"start\":2779},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2852,\"start\":2827},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2871,\"start\":2852},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3002,\"start\":2978},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":3226,\"start\":3206},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":3245,\"start\":3226},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3263,\"start\":3245},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3899,\"start\":3874},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3948,\"start\":3926},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3966,\"start\":3948},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4014,\"start\":3993},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6797,\"start\":6771},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7380,\"start\":7354},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7992,\"start\":7969},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8076,\"start\":8054},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9877,\"start\":9852},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9926,\"start\":9904},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9944,\"start\":9926},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9992,\"start\":9971},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11783,\"start\":11757},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":11845,\"start\":11821},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11866,\"start\":11845},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11888,\"start\":11866},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":12811,\"start\":12788},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":13793,\"start\":13770},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":14629,\"start\":14605},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":14867,\"start\":14844},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":16305,\"start\":16283},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":18183,\"start\":18154},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":18209,\"start\":18183},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":18906,\"start\":18883},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":22343,\"start\":22319},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":23609,\"start\":23589},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":23842,\"start\":23815},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":24118,\"start\":24099},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":24721,\"start\":24702},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":24784,\"start\":24764},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":24836,\"start\":24817},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":24910,\"start\":24890},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":24967,\"start\":24936},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":25020,\"start\":25001},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":25061,\"start\":25030},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":25091,\"start\":25071},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":25121,\"start\":25102},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":25162,\"start\":25142},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":25233,\"start\":25214},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":25348,\"start\":25324},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":25368,\"start\":25350},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":25793,\"start\":25777},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":25914,\"start\":25888},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":26179,\"start\":26159},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":26332,\"start\":26313},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":28893,\"start\":28870},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":29286,\"start\":29264},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":29486,\"start\":29465},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":29507,\"start\":29488},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":29603,\"start\":29582},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":29625,\"start\":29605},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":29884,\"start\":29863},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":30370,\"start\":30351},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":30394,\"start\":30370},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":30649,\"start\":30626},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":30715,\"start\":30690},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":30741,\"start\":30717},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":30858,\"start\":30834},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":31311,\"start\":31291},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":31329,\"start\":31311},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":31359,\"start\":31329},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":31381,\"start\":31359},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":31402,\"start\":31383},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":31509,\"start\":31491},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":31642,\"start\":31620},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":31674,\"start\":31644},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":31776,\"start\":31757}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":32332,\"start\":32095},{\"attributes\":{\"id\":\"fig_2\"},\"end\":32382,\"start\":32333},{\"attributes\":{\"id\":\"fig_3\"},\"end\":32568,\"start\":32383},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":32769,\"start\":32569},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":33109,\"start\":32770},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":34860,\"start\":33110},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":35044,\"start\":34861}]", "paragraph": "[{\"end\":2579,\"start\":1346},{\"end\":4015,\"start\":2581},{\"end\":4453,\"start\":4017},{\"end\":4878,\"start\":4455},{\"end\":5011,\"start\":4896},{\"end\":5369,\"start\":5030},{\"end\":5538,\"start\":5371},{\"end\":6048,\"start\":5564},{\"end\":6200,\"start\":6084},{\"end\":6338,\"start\":6304},{\"end\":6550,\"start\":6340},{\"end\":6849,\"start\":6566},{\"end\":6917,\"start\":6851},{\"end\":7045,\"start\":6969},{\"end\":7211,\"start\":7163},{\"end\":7302,\"start\":7269},{\"end\":7573,\"start\":7329},{\"end\":7711,\"start\":7600},{\"end\":7944,\"start\":7713},{\"end\":8168,\"start\":7946},{\"end\":8446,\"start\":8277},{\"end\":8729,\"start\":8448},{\"end\":9044,\"start\":8731},{\"end\":9357,\"start\":9046},{\"end\":9993,\"start\":9359},{\"end\":10171,\"start\":9995},{\"end\":10321,\"start\":10204},{\"end\":10408,\"start\":10323},{\"end\":10475,\"start\":10441},{\"end\":10703,\"start\":10542},{\"end\":10856,\"start\":10802},{\"end\":11598,\"start\":10921},{\"end\":12227,\"start\":11600},{\"end\":12374,\"start\":12229},{\"end\":12855,\"start\":12507},{\"end\":12908,\"start\":12857},{\"end\":13018,\"start\":12910},{\"end\":13356,\"start\":13161},{\"end\":13535,\"start\":13358},{\"end\":13582,\"start\":13537},{\"end\":13646,\"start\":13641},{\"end\":13922,\"start\":13698},{\"end\":14465,\"start\":13967},{\"end\":14630,\"start\":14467},{\"end\":14745,\"start\":14632},{\"end\":14868,\"start\":14791},{\"end\":15062,\"start\":14911},{\"end\":15349,\"start\":15064},{\"end\":15508,\"start\":15399},{\"end\":15645,\"start\":15540},{\"end\":15697,\"start\":15647},{\"end\":15967,\"start\":15810},{\"end\":16044,\"start\":15969},{\"end\":16837,\"start\":16194},{\"end\":17465,\"start\":16839},{\"end\":17595,\"start\":17467},{\"end\":18315,\"start\":17772},{\"end\":18579,\"start\":18317},{\"end\":18941,\"start\":18778},{\"end\":19419,\"start\":18943},{\"end\":19717,\"start\":19433},{\"end\":19893,\"start\":19750},{\"end\":20054,\"start\":19926},{\"end\":20364,\"start\":20105},{\"end\":21101,\"start\":20519},{\"end\":21325,\"start\":21158},{\"end\":21896,\"start\":21504},{\"end\":21997,\"start\":21898},{\"end\":22025,\"start\":21999},{\"end\":22200,\"start\":22142},{\"end\":22440,\"start\":22268},{\"end\":22558,\"start\":22442},{\"end\":22992,\"start\":22668},{\"end\":23400,\"start\":22994},{\"end\":23569,\"start\":23416},{\"end\":23801,\"start\":23579},{\"end\":24087,\"start\":23803},{\"end\":25234,\"start\":24089},{\"end\":25691,\"start\":25236},{\"end\":26334,\"start\":25693},{\"end\":27197,\"start\":26346},{\"end\":27519,\"start\":27227},{\"end\":27850,\"start\":27565},{\"end\":28250,\"start\":27852},{\"end\":28533,\"start\":28252},{\"end\":28767,\"start\":28535},{\"end\":32094,\"start\":28784}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":5563,\"start\":5539},{\"attributes\":{\"id\":\"formula_1\"},\"end\":6083,\"start\":6049},{\"attributes\":{\"id\":\"formula_2\"},\"end\":6303,\"start\":6201},{\"attributes\":{\"id\":\"formula_3\"},\"end\":6968,\"start\":6918},{\"attributes\":{\"id\":\"formula_4\"},\"end\":7162,\"start\":7046},{\"attributes\":{\"id\":\"formula_5\"},\"end\":7268,\"start\":7212},{\"attributes\":{\"id\":\"formula_6\"},\"end\":7328,\"start\":7303},{\"attributes\":{\"id\":\"formula_7\"},\"end\":8223,\"start\":8169},{\"attributes\":{\"id\":\"formula_8\"},\"end\":8276,\"start\":8223},{\"attributes\":{\"id\":\"formula_9\"},\"end\":10440,\"start\":10409},{\"attributes\":{\"id\":\"formula_10\"},\"end\":10541,\"start\":10476},{\"attributes\":{\"id\":\"formula_11\"},\"end\":10801,\"start\":10704},{\"attributes\":{\"id\":\"formula_12\"},\"end\":10920,\"start\":10857},{\"attributes\":{\"id\":\"formula_13\"},\"end\":12506,\"start\":12375},{\"attributes\":{\"id\":\"formula_14\"},\"end\":13160,\"start\":13019},{\"attributes\":{\"id\":\"formula_15\"},\"end\":13640,\"start\":13583},{\"attributes\":{\"id\":\"formula_16\"},\"end\":13697,\"start\":13647},{\"attributes\":{\"id\":\"formula_17\"},\"end\":14790,\"start\":14746},{\"attributes\":{\"id\":\"formula_18\"},\"end\":14910,\"start\":14869},{\"attributes\":{\"id\":\"formula_19\"},\"end\":15398,\"start\":15350},{\"attributes\":{\"id\":\"formula_20\"},\"end\":15539,\"start\":15509},{\"attributes\":{\"id\":\"formula_21\"},\"end\":15809,\"start\":15698},{\"attributes\":{\"id\":\"formula_22\"},\"end\":16193,\"start\":16045},{\"attributes\":{\"id\":\"formula_23\"},\"end\":17771,\"start\":17596},{\"attributes\":{\"id\":\"formula_24\"},\"end\":18777,\"start\":18580},{\"attributes\":{\"id\":\"formula_25\"},\"end\":19925,\"start\":19894},{\"attributes\":{\"id\":\"formula_26\"},\"end\":20104,\"start\":20055},{\"attributes\":{\"id\":\"formula_27\"},\"end\":20518,\"start\":20365},{\"attributes\":{\"id\":\"formula_28\"},\"end\":21157,\"start\":21102},{\"attributes\":{\"id\":\"formula_29\"},\"end\":21473,\"start\":21326},{\"attributes\":{\"id\":\"formula_31\"},\"end\":22141,\"start\":22026},{\"attributes\":{\"id\":\"formula_32\"},\"end\":22267,\"start\":22201},{\"attributes\":{\"id\":\"formula_33\"},\"end\":22667,\"start\":22559},{\"attributes\":{\"id\":\"formula_34\"},\"end\":27564,\"start\":27520}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":26394,\"start\":26387},{\"end\":26869,\"start\":26862}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1344,\"start\":1332},{\"attributes\":{\"n\":\"2.\"},\"end\":4894,\"start\":4881},{\"attributes\":{\"n\":\"2.1.\"},\"end\":5028,\"start\":5014},{\"attributes\":{\"n\":\"2.2.\"},\"end\":6564,\"start\":6553},{\"attributes\":{\"n\":\"3.\"},\"end\":7598,\"start\":7576},{\"attributes\":{\"n\":\"3.1.\"},\"end\":10202,\"start\":10174},{\"attributes\":{\"n\":\"3.2.\"},\"end\":13965,\"start\":13925},{\"attributes\":{\"n\":\"4.\"},\"end\":19431,\"start\":19422},{\"attributes\":{\"n\":\"4.1.\"},\"end\":19748,\"start\":19720},{\"attributes\":{\"n\":\"4.2.\"},\"end\":21502,\"start\":21475},{\"attributes\":{\"n\":\"5.\"},\"end\":23414,\"start\":23403},{\"attributes\":{\"n\":\"5.1.\"},\"end\":23577,\"start\":23572},{\"attributes\":{\"n\":\"5.2.\"},\"end\":26344,\"start\":26337},{\"attributes\":{\"n\":\"5.3.\"},\"end\":27225,\"start\":27200},{\"attributes\":{\"n\":\"6.\"},\"end\":28782,\"start\":28770},{\"end\":32097,\"start\":32096},{\"end\":32335,\"start\":32334},{\"end\":32404,\"start\":32384},{\"end\":33120,\"start\":33111},{\"end\":34871,\"start\":34862}]", "table": "[{\"end\":32769,\"start\":32747},{\"end\":34860,\"start\":33183},{\"end\":35044,\"start\":34923}]", "figure_caption": "[{\"end\":32332,\"start\":32098},{\"end\":32382,\"start\":32336},{\"end\":32568,\"start\":32407},{\"end\":32747,\"start\":32571},{\"end\":33109,\"start\":32772},{\"end\":33183,\"start\":33122},{\"end\":34923,\"start\":34873}]", "figure_ref": "[{\"end\":27849,\"start\":27838},{\"end\":28399,\"start\":28387}]", "bib_author_first_name": "[{\"end\":35452,\"start\":35451},{\"end\":35463,\"start\":35462},{\"end\":35465,\"start\":35464},{\"end\":35673,\"start\":35672},{\"end\":35686,\"start\":35685},{\"end\":35697,\"start\":35696},{\"end\":35708,\"start\":35707},{\"end\":36000,\"start\":35999},{\"end\":36013,\"start\":36012},{\"end\":36024,\"start\":36023},{\"end\":36035,\"start\":36034},{\"end\":36046,\"start\":36045},{\"end\":36057,\"start\":36056},{\"end\":36059,\"start\":36058},{\"end\":36302,\"start\":36301},{\"end\":36313,\"start\":36312},{\"end\":36324,\"start\":36323},{\"end\":36335,\"start\":36334},{\"end\":36346,\"start\":36345},{\"end\":36678,\"start\":36677},{\"end\":36688,\"start\":36687},{\"end\":36955,\"start\":36954},{\"end\":36965,\"start\":36964},{\"end\":36978,\"start\":36973},{\"end\":36988,\"start\":36987},{\"end\":37481,\"start\":37480},{\"end\":37491,\"start\":37490},{\"end\":37502,\"start\":37501},{\"end\":37513,\"start\":37512},{\"end\":37830,\"start\":37829},{\"end\":37841,\"start\":37840},{\"end\":37851,\"start\":37850},{\"end\":38077,\"start\":38076},{\"end\":38087,\"start\":38086},{\"end\":38291,\"start\":38290},{\"end\":38300,\"start\":38299},{\"end\":38558,\"start\":38557},{\"end\":38567,\"start\":38566},{\"end\":38579,\"start\":38578},{\"end\":38589,\"start\":38588},{\"end\":38600,\"start\":38599},{\"end\":38614,\"start\":38613},{\"end\":38628,\"start\":38627},{\"end\":38640,\"start\":38639},{\"end\":39027,\"start\":39026},{\"end\":39038,\"start\":39037},{\"end\":39049,\"start\":39048},{\"end\":39063,\"start\":39062},{\"end\":39478,\"start\":39477},{\"end\":39492,\"start\":39491},{\"end\":39509,\"start\":39508},{\"end\":39518,\"start\":39517},{\"end\":39524,\"start\":39523},{\"end\":39540,\"start\":39539},{\"end\":39549,\"start\":39548},{\"end\":39562,\"start\":39561},{\"end\":39866,\"start\":39865},{\"end\":39877,\"start\":39876},{\"end\":39890,\"start\":39889},{\"end\":39899,\"start\":39898},{\"end\":39912,\"start\":39911},{\"end\":40172,\"start\":40171},{\"end\":40178,\"start\":40177},{\"end\":40187,\"start\":40186},{\"end\":40194,\"start\":40193},{\"end\":40485,\"start\":40484},{\"end\":40496,\"start\":40495},{\"end\":40505,\"start\":40504},{\"end\":40796,\"start\":40795},{\"end\":40807,\"start\":40806},{\"end\":40816,\"start\":40815},{\"end\":40827,\"start\":40823},{\"end\":40834,\"start\":40833},{\"end\":40843,\"start\":40842},{\"end\":40853,\"start\":40852},{\"end\":40868,\"start\":40861},{\"end\":40872,\"start\":40871},{\"end\":41367,\"start\":41366},{\"end\":41383,\"start\":41382},{\"end\":41640,\"start\":41639},{\"end\":41648,\"start\":41647},{\"end\":41655,\"start\":41654},{\"end\":41668,\"start\":41662},{\"end\":41672,\"start\":41671},{\"end\":41674,\"start\":41673},{\"end\":41958,\"start\":41957},{\"end\":41966,\"start\":41965},{\"end\":41979,\"start\":41973},{\"end\":41983,\"start\":41982},{\"end\":41985,\"start\":41984},{\"end\":42243,\"start\":42242},{\"end\":42251,\"start\":42250},{\"end\":42258,\"start\":42257},{\"end\":42271,\"start\":42265},{\"end\":42275,\"start\":42274},{\"end\":42277,\"start\":42276},{\"end\":42558,\"start\":42557},{\"end\":42569,\"start\":42568},{\"end\":42578,\"start\":42577},{\"end\":43022,\"start\":43021},{\"end\":43033,\"start\":43032},{\"end\":43042,\"start\":43041},{\"end\":43330,\"start\":43329},{\"end\":43341,\"start\":43340},{\"end\":43350,\"start\":43349},{\"end\":43597,\"start\":43596},{\"end\":43610,\"start\":43609},{\"end\":43845,\"start\":43844},{\"end\":43854,\"start\":43853},{\"end\":44150,\"start\":44149},{\"end\":44159,\"start\":44158},{\"end\":44175,\"start\":44174},{\"end\":44308,\"start\":44307},{\"end\":44310,\"start\":44309},{\"end\":44317,\"start\":44316},{\"end\":44580,\"start\":44579},{\"end\":44582,\"start\":44581},{\"end\":44589,\"start\":44588},{\"end\":44591,\"start\":44590},{\"end\":44600,\"start\":44599},{\"end\":44602,\"start\":44601},{\"end\":44613,\"start\":44609},{\"end\":44617,\"start\":44616},{\"end\":44839,\"start\":44838},{\"end\":44847,\"start\":44846},{\"end\":44856,\"start\":44855},{\"end\":44867,\"start\":44866},{\"end\":44878,\"start\":44877},{\"end\":44886,\"start\":44885},{\"end\":45146,\"start\":45145},{\"end\":45166,\"start\":45165},{\"end\":45178,\"start\":45177},{\"end\":45194,\"start\":45193},{\"end\":45196,\"start\":45195},{\"end\":45400,\"start\":45399},{\"end\":45411,\"start\":45410},{\"end\":45535,\"start\":45534},{\"end\":45550,\"start\":45549},{\"end\":45558,\"start\":45557},{\"end\":45564,\"start\":45563},{\"end\":45574,\"start\":45573},{\"end\":45586,\"start\":45585},{\"end\":45592,\"start\":45591},{\"end\":45601,\"start\":45600},{\"end\":45613,\"start\":45612},{\"end\":45623,\"start\":45622},{\"end\":45636,\"start\":45635},{\"end\":45638,\"start\":45637},{\"end\":45646,\"start\":45645},{\"end\":45962,\"start\":45961},{\"end\":45972,\"start\":45971},{\"end\":45981,\"start\":45980},{\"end\":45996,\"start\":45989},{\"end\":46000,\"start\":45999},{\"end\":46273,\"start\":46272},{\"end\":46282,\"start\":46281},{\"end\":46294,\"start\":46293},{\"end\":46304,\"start\":46303},{\"end\":46756,\"start\":46755},{\"end\":46776,\"start\":46775},{\"end\":46786,\"start\":46785},{\"end\":46788,\"start\":46787},{\"end\":46800,\"start\":46799},{\"end\":47204,\"start\":47203},{\"end\":47468,\"start\":47467},{\"end\":47477,\"start\":47476},{\"end\":47488,\"start\":47487},{\"end\":47497,\"start\":47496},{\"end\":47513,\"start\":47506},{\"end\":47517,\"start\":47516},{\"end\":47732,\"start\":47731},{\"end\":47741,\"start\":47740},{\"end\":47752,\"start\":47751},{\"end\":47768,\"start\":47761},{\"end\":47772,\"start\":47771},{\"end\":48070,\"start\":48069},{\"end\":48086,\"start\":48085},{\"end\":48097,\"start\":48096},{\"end\":48112,\"start\":48111},{\"end\":48444,\"start\":48443},{\"end\":48453,\"start\":48452},{\"end\":48463,\"start\":48461},{\"end\":48467,\"start\":48466},{\"end\":48724,\"start\":48720}]", "bib_author_last_name": "[{\"end\":35460,\"start\":35453},{\"end\":35474,\"start\":35466},{\"end\":35683,\"start\":35674},{\"end\":35694,\"start\":35687},{\"end\":35705,\"start\":35698},{\"end\":35716,\"start\":35709},{\"end\":36010,\"start\":36001},{\"end\":36021,\"start\":36014},{\"end\":36032,\"start\":36025},{\"end\":36043,\"start\":36036},{\"end\":36054,\"start\":36047},{\"end\":36067,\"start\":36060},{\"end\":36310,\"start\":36303},{\"end\":36321,\"start\":36314},{\"end\":36332,\"start\":36325},{\"end\":36343,\"start\":36336},{\"end\":36354,\"start\":36347},{\"end\":36685,\"start\":36679},{\"end\":36694,\"start\":36689},{\"end\":36962,\"start\":36956},{\"end\":36971,\"start\":36966},{\"end\":36985,\"start\":36979},{\"end\":37488,\"start\":37482},{\"end\":37499,\"start\":37492},{\"end\":37510,\"start\":37503},{\"end\":37527,\"start\":37514},{\"end\":37838,\"start\":37831},{\"end\":37848,\"start\":37842},{\"end\":37859,\"start\":37852},{\"end\":38084,\"start\":38078},{\"end\":38095,\"start\":38088},{\"end\":38297,\"start\":38292},{\"end\":38310,\"start\":38301},{\"end\":38564,\"start\":38559},{\"end\":38576,\"start\":38568},{\"end\":38586,\"start\":38580},{\"end\":38597,\"start\":38590},{\"end\":38611,\"start\":38601},{\"end\":38625,\"start\":38615},{\"end\":38637,\"start\":38629},{\"end\":38650,\"start\":38641},{\"end\":39035,\"start\":39028},{\"end\":39046,\"start\":39039},{\"end\":39060,\"start\":39050},{\"end\":39071,\"start\":39064},{\"end\":39489,\"start\":39479},{\"end\":39506,\"start\":39493},{\"end\":39515,\"start\":39510},{\"end\":39521,\"start\":39519},{\"end\":39537,\"start\":39525},{\"end\":39546,\"start\":39541},{\"end\":39559,\"start\":39550},{\"end\":39569,\"start\":39563},{\"end\":39874,\"start\":39867},{\"end\":39887,\"start\":39878},{\"end\":39896,\"start\":39891},{\"end\":39909,\"start\":39900},{\"end\":39918,\"start\":39913},{\"end\":40175,\"start\":40173},{\"end\":40184,\"start\":40179},{\"end\":40191,\"start\":40188},{\"end\":40198,\"start\":40195},{\"end\":40493,\"start\":40486},{\"end\":40502,\"start\":40497},{\"end\":40511,\"start\":40506},{\"end\":40804,\"start\":40797},{\"end\":40813,\"start\":40808},{\"end\":40821,\"start\":40817},{\"end\":40831,\"start\":40828},{\"end\":40840,\"start\":40835},{\"end\":40850,\"start\":40844},{\"end\":40859,\"start\":40854},{\"end\":41380,\"start\":41368},{\"end\":41393,\"start\":41384},{\"end\":41645,\"start\":41641},{\"end\":41652,\"start\":41649},{\"end\":41660,\"start\":41656},{\"end\":41963,\"start\":41959},{\"end\":41971,\"start\":41967},{\"end\":42248,\"start\":42244},{\"end\":42255,\"start\":42252},{\"end\":42263,\"start\":42259},{\"end\":42566,\"start\":42559},{\"end\":42575,\"start\":42570},{\"end\":42591,\"start\":42579},{\"end\":43030,\"start\":43023},{\"end\":43039,\"start\":43034},{\"end\":43055,\"start\":43043},{\"end\":43338,\"start\":43331},{\"end\":43347,\"start\":43342},{\"end\":43363,\"start\":43351},{\"end\":43607,\"start\":43598},{\"end\":43620,\"start\":43611},{\"end\":43851,\"start\":43846},{\"end\":43861,\"start\":43855},{\"end\":44156,\"start\":44151},{\"end\":44172,\"start\":44160},{\"end\":44185,\"start\":44176},{\"end\":44314,\"start\":44311},{\"end\":44322,\"start\":44318},{\"end\":44586,\"start\":44583},{\"end\":44597,\"start\":44592},{\"end\":44607,\"start\":44603},{\"end\":44844,\"start\":44840},{\"end\":44853,\"start\":44848},{\"end\":44864,\"start\":44857},{\"end\":44875,\"start\":44868},{\"end\":44883,\"start\":44879},{\"end\":44893,\"start\":44887},{\"end\":44900,\"start\":44895},{\"end\":45163,\"start\":45147},{\"end\":45175,\"start\":45167},{\"end\":45191,\"start\":45179},{\"end\":45205,\"start\":45197},{\"end\":45408,\"start\":45401},{\"end\":45421,\"start\":45412},{\"end\":45547,\"start\":45536},{\"end\":45555,\"start\":45551},{\"end\":45561,\"start\":45559},{\"end\":45571,\"start\":45565},{\"end\":45583,\"start\":45575},{\"end\":45589,\"start\":45587},{\"end\":45598,\"start\":45593},{\"end\":45610,\"start\":45602},{\"end\":45620,\"start\":45614},{\"end\":45633,\"start\":45624},{\"end\":45643,\"start\":45639},{\"end\":45654,\"start\":45647},{\"end\":45969,\"start\":45963},{\"end\":45978,\"start\":45973},{\"end\":45987,\"start\":45982},{\"end\":46279,\"start\":46274},{\"end\":46291,\"start\":46283},{\"end\":46301,\"start\":46295},{\"end\":46311,\"start\":46305},{\"end\":46773,\"start\":46757},{\"end\":46783,\"start\":46777},{\"end\":46797,\"start\":46789},{\"end\":46810,\"start\":46801},{\"end\":47214,\"start\":47205},{\"end\":47474,\"start\":47469},{\"end\":47485,\"start\":47478},{\"end\":47494,\"start\":47489},{\"end\":47504,\"start\":47498},{\"end\":47738,\"start\":47733},{\"end\":47749,\"start\":47742},{\"end\":47759,\"start\":47753},{\"end\":48083,\"start\":48071},{\"end\":48094,\"start\":48087},{\"end\":48109,\"start\":48098},{\"end\":48125,\"start\":48113},{\"end\":48450,\"start\":48445},{\"end\":48459,\"start\":48454},{\"end\":48729,\"start\":48725}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":35619,\"start\":35401},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":10908021},\"end\":35952,\"start\":35621},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":8577357},\"end\":36260,\"start\":35954},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":2497886},\"end\":36593,\"start\":36262},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":17840600},\"end\":36886,\"start\":36595},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":13962232},\"end\":37413,\"start\":36888},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":9634599},\"end\":37795,\"start\":37415},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":2709712},\"end\":38034,\"start\":37797},{\"attributes\":{\"id\":\"b8\"},\"end\":38237,\"start\":38036},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":6755881},\"end\":38507,\"start\":38239},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":2871880},\"end\":38935,\"start\":38509},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":13474174},\"end\":39446,\"start\":38937},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":1033682},\"end\":39837,\"start\":39448},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":10742222},\"end\":40123,\"start\":39839},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":206594692},\"end\":40428,\"start\":40125},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":46895919},\"end\":40737,\"start\":40430},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":7646250},\"end\":41244,\"start\":40739},{\"attributes\":{\"id\":\"b17\"},\"end\":41575,\"start\":41246},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":556999},\"end\":41900,\"start\":41577},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":12757870},\"end\":42197,\"start\":41902},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":46784066},\"end\":42502,\"start\":42199},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":6907693},\"end\":42978,\"start\":42504},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":3026868},\"end\":43276,\"start\":42980},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":6178817},\"end\":43553,\"start\":43278},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":338196},\"end\":43773,\"start\":43555},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":8702561},\"end\":44114,\"start\":43775},{\"attributes\":{\"id\":\"b26\"},\"end\":44274,\"start\":44116},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":740063},\"end\":44526,\"start\":44276},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":788838},\"end\":44836,\"start\":44528},{\"attributes\":{\"doi\":\"abs/1710.06924\",\"id\":\"b29\"},\"end\":45108,\"start\":44838},{\"attributes\":{\"id\":\"b30\"},\"end\":45349,\"start\":45110},{\"attributes\":{\"id\":\"b31\"},\"end\":45530,\"start\":45351},{\"attributes\":{\"id\":\"b32\"},\"end\":45911,\"start\":45532},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":7534823},\"end\":46203,\"start\":45913},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":4619542},\"end\":46678,\"start\":46205},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":4547917},\"end\":47113,\"start\":46680},{\"attributes\":{\"id\":\"b36\"},\"end\":47408,\"start\":47115},{\"attributes\":{\"doi\":\"abs/1412.3474\",\"id\":\"b37\"},\"end\":47683,\"start\":47410},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":4357800},\"end\":48010,\"start\":47685},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":2928248},\"end\":48396,\"start\":48012},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":1138957},\"end\":48678,\"start\":48398},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":19484770},\"end\":48854,\"start\":48680}]", "bib_title": "[{\"end\":35670,\"start\":35621},{\"end\":35997,\"start\":35954},{\"end\":36299,\"start\":36262},{\"end\":36675,\"start\":36595},{\"end\":36952,\"start\":36888},{\"end\":37478,\"start\":37415},{\"end\":37827,\"start\":37797},{\"end\":38288,\"start\":38239},{\"end\":38555,\"start\":38509},{\"end\":39024,\"start\":38937},{\"end\":39475,\"start\":39448},{\"end\":39863,\"start\":39839},{\"end\":40169,\"start\":40125},{\"end\":40482,\"start\":40430},{\"end\":40793,\"start\":40739},{\"end\":41637,\"start\":41577},{\"end\":41955,\"start\":41902},{\"end\":42240,\"start\":42199},{\"end\":42555,\"start\":42504},{\"end\":43019,\"start\":42980},{\"end\":43327,\"start\":43278},{\"end\":43594,\"start\":43555},{\"end\":43842,\"start\":43775},{\"end\":44305,\"start\":44276},{\"end\":44577,\"start\":44528},{\"end\":45959,\"start\":45913},{\"end\":46270,\"start\":46205},{\"end\":46753,\"start\":46680},{\"end\":47729,\"start\":47685},{\"end\":48067,\"start\":48012},{\"end\":48441,\"start\":48398},{\"end\":48718,\"start\":48680}]", "bib_author": "[{\"end\":35462,\"start\":35451},{\"end\":35476,\"start\":35462},{\"end\":35685,\"start\":35672},{\"end\":35696,\"start\":35685},{\"end\":35707,\"start\":35696},{\"end\":35718,\"start\":35707},{\"end\":36012,\"start\":35999},{\"end\":36023,\"start\":36012},{\"end\":36034,\"start\":36023},{\"end\":36045,\"start\":36034},{\"end\":36056,\"start\":36045},{\"end\":36069,\"start\":36056},{\"end\":36312,\"start\":36301},{\"end\":36323,\"start\":36312},{\"end\":36334,\"start\":36323},{\"end\":36345,\"start\":36334},{\"end\":36356,\"start\":36345},{\"end\":36687,\"start\":36677},{\"end\":36696,\"start\":36687},{\"end\":36964,\"start\":36954},{\"end\":36973,\"start\":36964},{\"end\":36987,\"start\":36973},{\"end\":36991,\"start\":36987},{\"end\":37490,\"start\":37480},{\"end\":37501,\"start\":37490},{\"end\":37512,\"start\":37501},{\"end\":37529,\"start\":37512},{\"end\":37840,\"start\":37829},{\"end\":37850,\"start\":37840},{\"end\":37861,\"start\":37850},{\"end\":38086,\"start\":38076},{\"end\":38097,\"start\":38086},{\"end\":38299,\"start\":38290},{\"end\":38312,\"start\":38299},{\"end\":38566,\"start\":38557},{\"end\":38578,\"start\":38566},{\"end\":38588,\"start\":38578},{\"end\":38599,\"start\":38588},{\"end\":38613,\"start\":38599},{\"end\":38627,\"start\":38613},{\"end\":38639,\"start\":38627},{\"end\":38652,\"start\":38639},{\"end\":39037,\"start\":39026},{\"end\":39048,\"start\":39037},{\"end\":39062,\"start\":39048},{\"end\":39073,\"start\":39062},{\"end\":39491,\"start\":39477},{\"end\":39508,\"start\":39491},{\"end\":39517,\"start\":39508},{\"end\":39523,\"start\":39517},{\"end\":39539,\"start\":39523},{\"end\":39548,\"start\":39539},{\"end\":39561,\"start\":39548},{\"end\":39571,\"start\":39561},{\"end\":39876,\"start\":39865},{\"end\":39889,\"start\":39876},{\"end\":39898,\"start\":39889},{\"end\":39911,\"start\":39898},{\"end\":39920,\"start\":39911},{\"end\":40177,\"start\":40171},{\"end\":40186,\"start\":40177},{\"end\":40193,\"start\":40186},{\"end\":40200,\"start\":40193},{\"end\":40495,\"start\":40484},{\"end\":40504,\"start\":40495},{\"end\":40513,\"start\":40504},{\"end\":40806,\"start\":40795},{\"end\":40815,\"start\":40806},{\"end\":40823,\"start\":40815},{\"end\":40833,\"start\":40823},{\"end\":40842,\"start\":40833},{\"end\":40852,\"start\":40842},{\"end\":40861,\"start\":40852},{\"end\":40871,\"start\":40861},{\"end\":40875,\"start\":40871},{\"end\":41382,\"start\":41366},{\"end\":41395,\"start\":41382},{\"end\":41647,\"start\":41639},{\"end\":41654,\"start\":41647},{\"end\":41662,\"start\":41654},{\"end\":41671,\"start\":41662},{\"end\":41677,\"start\":41671},{\"end\":41965,\"start\":41957},{\"end\":41973,\"start\":41965},{\"end\":41982,\"start\":41973},{\"end\":41988,\"start\":41982},{\"end\":42250,\"start\":42242},{\"end\":42257,\"start\":42250},{\"end\":42265,\"start\":42257},{\"end\":42274,\"start\":42265},{\"end\":42280,\"start\":42274},{\"end\":42568,\"start\":42557},{\"end\":42577,\"start\":42568},{\"end\":42593,\"start\":42577},{\"end\":43032,\"start\":43021},{\"end\":43041,\"start\":43032},{\"end\":43057,\"start\":43041},{\"end\":43340,\"start\":43329},{\"end\":43349,\"start\":43340},{\"end\":43365,\"start\":43349},{\"end\":43609,\"start\":43596},{\"end\":43622,\"start\":43609},{\"end\":43853,\"start\":43844},{\"end\":43863,\"start\":43853},{\"end\":44158,\"start\":44149},{\"end\":44174,\"start\":44158},{\"end\":44187,\"start\":44174},{\"end\":44316,\"start\":44307},{\"end\":44324,\"start\":44316},{\"end\":44588,\"start\":44579},{\"end\":44599,\"start\":44588},{\"end\":44609,\"start\":44599},{\"end\":44616,\"start\":44609},{\"end\":44620,\"start\":44616},{\"end\":44846,\"start\":44838},{\"end\":44855,\"start\":44846},{\"end\":44866,\"start\":44855},{\"end\":44877,\"start\":44866},{\"end\":44885,\"start\":44877},{\"end\":44895,\"start\":44885},{\"end\":44902,\"start\":44895},{\"end\":45165,\"start\":45145},{\"end\":45177,\"start\":45165},{\"end\":45193,\"start\":45177},{\"end\":45207,\"start\":45193},{\"end\":45410,\"start\":45399},{\"end\":45423,\"start\":45410},{\"end\":45549,\"start\":45534},{\"end\":45557,\"start\":45549},{\"end\":45563,\"start\":45557},{\"end\":45573,\"start\":45563},{\"end\":45585,\"start\":45573},{\"end\":45591,\"start\":45585},{\"end\":45600,\"start\":45591},{\"end\":45612,\"start\":45600},{\"end\":45622,\"start\":45612},{\"end\":45635,\"start\":45622},{\"end\":45645,\"start\":45635},{\"end\":45656,\"start\":45645},{\"end\":45971,\"start\":45961},{\"end\":45980,\"start\":45971},{\"end\":45989,\"start\":45980},{\"end\":45999,\"start\":45989},{\"end\":46003,\"start\":45999},{\"end\":46281,\"start\":46272},{\"end\":46293,\"start\":46281},{\"end\":46303,\"start\":46293},{\"end\":46313,\"start\":46303},{\"end\":46775,\"start\":46755},{\"end\":46785,\"start\":46775},{\"end\":46799,\"start\":46785},{\"end\":46812,\"start\":46799},{\"end\":47216,\"start\":47203},{\"end\":47476,\"start\":47467},{\"end\":47487,\"start\":47476},{\"end\":47496,\"start\":47487},{\"end\":47506,\"start\":47496},{\"end\":47516,\"start\":47506},{\"end\":47520,\"start\":47516},{\"end\":47740,\"start\":47731},{\"end\":47751,\"start\":47740},{\"end\":47761,\"start\":47751},{\"end\":47771,\"start\":47761},{\"end\":47775,\"start\":47771},{\"end\":48085,\"start\":48069},{\"end\":48096,\"start\":48085},{\"end\":48111,\"start\":48096},{\"end\":48127,\"start\":48111},{\"end\":48452,\"start\":48443},{\"end\":48461,\"start\":48452},{\"end\":48466,\"start\":48461},{\"end\":48470,\"start\":48466},{\"end\":48731,\"start\":48720}]", "bib_venue": "[{\"end\":35449,\"start\":35401},{\"end\":35774,\"start\":35718},{\"end\":36085,\"start\":36069},{\"end\":36405,\"start\":36356},{\"end\":36724,\"start\":36696},{\"end\":37089,\"start\":36991},{\"end\":37585,\"start\":37529},{\"end\":37897,\"start\":37861},{\"end\":38074,\"start\":38036},{\"end\":38363,\"start\":38312},{\"end\":38692,\"start\":38652},{\"end\":39141,\"start\":39073},{\"end\":39627,\"start\":39571},{\"end\":39963,\"start\":39920},{\"end\":40265,\"start\":40200},{\"end\":40562,\"start\":40513},{\"end\":40943,\"start\":40875},{\"end\":41364,\"start\":41246},{\"end\":41728,\"start\":41677},{\"end\":42039,\"start\":41988},{\"end\":42329,\"start\":42280},{\"end\":42677,\"start\":42593},{\"end\":43106,\"start\":43057},{\"end\":43399,\"start\":43365},{\"end\":43646,\"start\":43622},{\"end\":43918,\"start\":43863},{\"end\":44147,\"start\":44116},{\"end\":44382,\"start\":44324},{\"end\":44662,\"start\":44620},{\"end\":44960,\"start\":44916},{\"end\":45143,\"start\":45110},{\"end\":45397,\"start\":45351},{\"end\":46048,\"start\":46003},{\"end\":46390,\"start\":46313},{\"end\":46881,\"start\":46812},{\"end\":47201,\"start\":47115},{\"end\":47465,\"start\":47410},{\"end\":47833,\"start\":47775},{\"end\":48191,\"start\":48127},{\"end\":48519,\"start\":48470},{\"end\":48752,\"start\":48731},{\"end\":37174,\"start\":37091},{\"end\":39196,\"start\":39143},{\"end\":40998,\"start\":40945},{\"end\":42748,\"start\":42679},{\"end\":46454,\"start\":46392}]"}}}, "year": 2023, "month": 12, "day": 17}