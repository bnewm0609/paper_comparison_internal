{"id": 232478414, "updated": "2023-10-06 05:10:55.861", "metadata": {"title": "NeuralRecon: Real-Time Coherent 3D Reconstruction from Monocular Video", "authors": "[{\"first\":\"Jiaming\",\"last\":\"Sun\",\"middle\":[]},{\"first\":\"Yiming\",\"last\":\"Xie\",\"middle\":[]},{\"first\":\"Linghao\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Xiaowei\",\"last\":\"Zhou\",\"middle\":[]},{\"first\":\"Hujun\",\"last\":\"Bao\",\"middle\":[]}]", "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2021, "month": 4, "day": 1}, "abstract": "We present a novel framework named NeuralRecon for real-time 3D scene reconstruction from a monocular video. Unlike previous methods that estimate single-view depth maps separately on each key-frame and fuse them later, we propose to directly reconstruct local surfaces represented as sparse TSDF volumes for each video fragment sequentially by a neural network. A learning-based TSDF fusion module based on gated recurrent units is used to guide the network to fuse features from previous fragments. This design allows the network to capture local smoothness prior and global shape prior of 3D surfaces when sequentially reconstructing the surfaces, resulting in accurate, coherent, and real-time surface reconstruction. The experiments on ScanNet and 7-Scenes datasets show that our system outperforms state-of-the-art methods in terms of both accuracy and speed. To the best of our knowledge, this is the first learning-based system that is able to reconstruct dense coherent 3D geometry in real-time.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2104.00681", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/SunXCZB21", "doi": "10.1109/cvpr46437.2021.01534"}}, "content": {"source": {"pdf_hash": "147178c37dd083c6005ed7c38106b7fcd33cb0b3", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2104.00681", "status": "GREEN"}}, "grobid": {"id": "68ada5f4ad8f087752c801b241d705c7f5624e7c", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/147178c37dd083c6005ed7c38106b7fcd33cb0b3.txt", "contents": "\nNeuralRecon: Real-Time Coherent 3D Reconstruction from Monocular Video\n\n\nJiaming Sun \nZhejiang University\n\n\nSenseTime Research\n\n\nYiming Xie \nZhejiang University\n\n\nLinghao Chen \nZhejiang University\n\n\nXiaowei Zhou \nZhejiang University\n\n\nHujun Bao \nZhejiang University\n\n\nNeuralRecon: Real-Time Coherent 3D Reconstruction from Monocular Video\n10.1109/CVPR46437.2021.01534\nWe present a novel framework named NeuralRecon for real-time 3D scene reconstruction from a monocular video. Unlike previous methods that estimate single-view depth maps separately on each key-frame and fuse them later, we propose to directly reconstruct local surfaces represented as sparse TSDF volumes for each video fragment sequentially by a neural network. A learning-based TSDF fusion module based on gated recurrent units is used to guide the network to fuse features from previous fragments. This design allows the network to capture local smoothness prior and global shape prior of 3D surfaces when sequentially reconstructing the surfaces, resulting in accurate, coherent, and real-time surface reconstruction. The experiments on ScanNet and 7-Scenes datasets show that our system outperforms state-of-the-art methods in terms of both accuracy and speed. To the best of our knowledge, this is the first learning-based system that is able to reconstruct dense coherent 3D geometry in real-time. Code is available at the project page: https://zju3dv.github.io/ neuralrecon/.\n\nIntroduction\n\n3D scene reconstruction is one of the central tasks in 3D computer vision with many applications. In augmented reality (AR) for example, to enable realistic and immersive interactions between AR effects and the surrounding physical scene, 3D reconstruction needs to be accurate, coherent and performed in real-time. While camera motion can be tracked accurately with state-of-the-art visual-inertial SLAM systems [3,35,1], real-time image-based dense reconstruction remains to be a challenging problem due to low reconstruction quality and high computation demands.\n\nMost image-based real-time 3D reconstruction pipelines [38,52] adopt the depth map fusion approach, which resemble RGB-D reconstruction methods like KinectFusion * The first two authors contributed equally. The authors are affiliated with the State Key Lab of CAD&CG and ZJU-SenseTime Joint Lab of 3D Vision. \u2020 Corresponding author: Hujun Bao.  Figure 1. Comparison between depth-based 3D reconstruction methods and the proposed method. In depth-based methods, key-frame depths are estimated separately from each key frame, and later fused into a TSDF volume. In the proposed method, the TSDF volume is directly predicted with all the key frames in a local window. This design leads to a much more coherent reconstruction and real-time speed. [31]. Single-view depth maps from each key frame are first estimated with real-time multi-view depth estimation methods like [48,24,13,46]. The estimated depth maps are later filtered with criteria like multi-view consistency and temporal smoothness, and fused into a Truncated Signed Distance Function (TSDF) volume. The reconstructed mesh can be extracted from the fused TSDF volume with the Marching Cubes algorithm [27]. This depth-based pipeline has two major drawbacks. First, since single-view depth maps are estimated individually on each key frame, each depth estimation is from scratch instead of conditioned on the previous estimations even the view-overlapping is substantial. As a result, the scale-factor may vary even with the correct camera ego-motion. Due to depth inconsistencies between different views, the reconstruction result is prone to be either layered or scattered. One example is shown in the red boxes in Fig. 1, where the depth-based method struggles to produce coherent depth estimations on the chairs and wall. Second, since key-frame depth maps need to be estimated separately in overlapped local windows, geometry of the same 3D surface is estimated multiple times in different key frames, causing redundant computation.\n\nIn this paper, we propose a novel framework for realtime monocular reconstruction named NeuralRecon that jointly reconstructs and fuses the 3D geometry directly in the volumetric TSDF representation. Given a sequence of monocular images and their corresponding camera poses estimated by a SLAM system, NeuralRecon incrementally reconstructs local geometry in a view-independent 3D volume instead of view-dependent depth maps. Specifically, it unprojects the image features to form a 3D feature volume and then uses sparse convolutions to process the feature volume to output a sparse TSDF volume. With a coarseto-fine design, the predicted TSDF is gradually refined at each level. By directly reconstructing the implicit surface (TSDF), the network is able to learn the local smoothness and global shape prior of natural 3D surfaces. Different from depth-based methods that predict depth maps for each key frame separately, the surface geometry within a local fragment window is jointly predicted in NeuralRecon, and thus locally coherent geometry estimation can be produced. To make the current-fragment reconstruction to be globally consistent with the previously reconstructed fragments, a learning-based TSDF fusion module using the Gated Recurrent Unit (GRU) is proposed. The GRU fusion makes the current-fragment reconstruction conditioned on the previously reconstructed global volume, yielding a joint reconstruction and fusion approach. As a result, the reconstructed mesh is dense, accurate and globally coherent in scale. Furthermore, predicting the volumetric representation also removes the redundant computation in depth-based methods, which allows us to use a larger 3D CNN while maintaining the real-time performance. We validate our system on the ScanNet and 7-Scenes datasets. The experimental results show that NeuralRecon outperforms multiple state-of-the-art multi-view depth estimation methods and the volume-based reconstruction method Atlas [30] by a large margin, while achieving a realtime performance at 33 key frames per second, \u223c10\u00d7 faster compared to Atlas. As shown in the supplementary video, our method is able to reconstruct large-scale 3D scenes from a video stream on a laptop GPU in real-time. To the best of our knowledge, this is the first learning-based system that is able to reconstruct dense and coherent 3D scene geometry in real-time.\n\n\nRelated Work\n\nMulti-view Depth Estimation. The most related line of research is real-time methods for multi-view depth estimation. Before the age of deep learning, many renowned works in monocular 3D reconstruction [47,21,38,34] have achieved good performance with plane-sweeping stereo and depth filters under the assumption of photo-consistency. [46,51] optimize this line of research towards low power consumption on mobile platforms. Learning-based methods on real-time multi-view depth estimation try to alleviate the photo-consistency assumption with a data-driven approach. Notably, MVDepthNet [48] and Neural RGB->D [24] use 2D CNNs to process the 2D depth cost volume constructed from multi-view image features. CNMNet [26] further leverages the planar structure in indoor scenes to constrain the surface normals calculated from the predicted depth maps to obtain smooth depth estimation. These learning-based methods use 2D CNNs to process the depth cost volume to maintain a low computational cost for near real-time performance.\n\nWhen the input images are high-resolution and offline computation is allowed, multi-view depth estimation is also known as the Multiple View Stereo (MVS) problem. PatchMatch-based methods [56,37] have achieved impressive accuracy and are still the most popular methods applicable to high-resolution images. Learning-based approaches in MVS have recently dominated several benchmarks [2,20] in terms of accuracy, but are only limited to processing mid-resolution images due to the GPU memory constraint. Different from the real-time methods, 3D cost volumes are constructed and 3D CNNs are used to process the cost volume as proposed in MVSNet [53]. Some recent works [12,4] improve this pipeline with a coarse-to-fine approach. Similar design can also be found in many learningbased SLAM systems [45,57,42,44].\n\nAll the above-mentioned works adopt single-view depth maps as intermediate representations. SurfaceNet [15,16] takes a different approach and uses a unified volumetric representation to predict the volume occupancy. Recently, Atlas [30] also proposes a volumetric design and direct predicts TSDF and semantic labels with 3D CNN. As an offline method, Atlas aggregates the image features of the entire sequence and then predicts the global TSDF volume only once with a decoder module. We further elaborate the relationship between the proposed method and Atlas in the supplementary material. The proposed method is also related to [5,18] in terms of using recurrent networks for multi-view feature fusion. However, their recurrent fusion is applied to only the global features and their focus is to reconstruct single objects.\n\n3D Surface Reconstruction. After depth maps are estimated and converted to point clouds, the remaining task for 3D reconstruction is to estimate the 3D surface position and produce the reconstructed mesh. In an offline MVS pipeline [37], Poisson reconstruction [19] and Delaunay triagulation [22] are often used to fulfill this purpose. Proposed by the seminal work KinectFusion [31], incremental volumetric TSDF fusion [7] gets widely adopted in real-time reconstruction scenarios due to its simplicity and parallelization capability. [32,10] improve KinectFusion by making it \nF 1 t F 2 t F 3 t S 3 t S 2 t S 1 t S g t S g t\u22121 S l t {I t , \u03be t } N Output: Pred. Geo.\n\n(Sparse TSDF) Unprojection\n\nExtract Replace Figure 2. NeuralRecon architecture. NeuralRecon predicts TSDF with a three-level coarse-to-fine approach that gradually increases the density of sparse voxels. Key-frame images in the local fragment are first passed through the image backbone to extract the multi-level features. These image features are later back-projected along each ray and aggregated into a 3D feature volume F l t , where l represents the level index. At the first level (l = 1), a dense TSDF volume S 1 t is predicted. At the second and third levels, the upsampled S l\u22121 t from the last level is concatenated with F l t and used as the input for the GRU Fusion and MLP modules. A feature volume defined in the world frame is maintained at each level as the global hidden state of the GRU. At the last level, the output S l t is used to replace corresponding voxels in the global TSDF volume S g t , yielding the final reconstruction at time t.\n\nmore scalable and robust. RoutedFusion [49,50] changes the fusion operation from a simple linear addition into a data-dependent process.\n\nNeural Implicit Representations. Recently, neural implicit representations [29,33,36,17,54,25] have gained significant advances. Our work also learns a neural implicit representation by predicting SDF with the neural network from the encoded image features similar to PIFu [36]. The key difference is that we are using sparse 3D convolution to predict a discrete TSDF volume, instead of querying the MLP network with image features and 3D coordinates.\n\n\nMethods\n\nGiven a sequence of monocular images {I t } and camera pose trajectory {\u03be t } \u2208 SE(3) provided by a SLAM system, the goal is to reconstruct dense 3D scene geometry accurately in real-time. We denote the global TSDF volume to reconstruct as S g t , where t represents the current time step. The system architecture is illustrated in Fig. 2. \n\n\nKey Frame Selection\n\nTo achieve real-time 3D reconstruction that is suitable for interactive applications, the reconstruction process needs to be incremental and the input images should be processed sequentially in local fragments [40]. We seek to find a set of suitable key frames from the incoming image stream as input for the networks. To provide enough motion parallax while keeping multi-view co-visibility for reconstruction, the selected key frames should be neither too close nor far from each other. Following [13], a new incoming frame is selected as a key frame if its relative translation is greater than t max and the relative rotation angle is greater R max . A window with N key frames is defined as a local fragment. After key frames are selected, a cubic-shaped fragment bounding volume (FBV) that encloses all the key frame view-frustums is computed with a fixed max depth range d max in each view. Only the region within the FBV is considered during the reconstruction of each fragment.\n\n\nJoint Fragment Reconstruction and Fusion\n\nWe propose to simultaneously reconstruct the TSDF volume of a local fragment S l t and fuse it with global TSDF volume S g t with a learning-based approach. The joint reconstruction and fusion is carried out in the local coordinate system. The definition of the local and global coordinate systems as well as the construction of FBV are illustrated in Fig. 1 of the supplementary material.\n\nImage Feature Volume Construction. The N images in the local fragment are first passed through the image backbone to extract the multi-level features. Similar to previous works on volumetric reconstruction [18,15,30], the extracted features are back-projected along each ray into the 3D feature volume. The image feature volume F l t is  obtained by averaging the features from different views according to the visibility weight of each voxel. The visibility weight is defined as the number of views from which a voxel can be observed in the local fragment. A visualization of this unprojection process can be found in Fig.3 i.\n{ x \u2208 (\u22121, 1) Feature 2\u03bb H g t\u22121 H g t G l t H l t\u22121 H l t F l t o > \u03b8 o \u03b8 S l t\nCoarse-to-fine TSDF Reconstruction. We adopt a coarseto-fine approach to gradually refine the predicted TSDF volume at each level. We use 3D sparse convolution to efficiently process the feature volume F l t . The sparse volumetric representation also naturally integrates with the coarseto-fine design. Specifically, each voxel in the TSDF volume S l t contains two values, the occupancy score o and the SDF value x. At each level, both o and x are predicted by the MLP. The occupancy score represents the confidence of a voxel being within the TSDF truncation distance \u03bb. The voxel whose occupancy score is lower than the sparsification threshold \u03b8 is defined as void space and will be sparsified. This representation of sparse TSDF volume is visually illustrated in Fig.3 iii. After the sparsification, S l t is upsampled by 2\u00d7 and concatenated with the F l+1 t as the input for the GRU Fusion module (introduced later) in the next level.\n\nInstead of estimating single-view depth maps for each key frame, NeuralRecon jointly reconstructs the implicit surface within the bounding volume of the local fragment window. This design guides the network to learn the natural surface prior directly from the training data. As a result, the reconstructed surface is locally smooth and coherent in scale. Notably, this design also leads to less redundant computation compared to depth-based methods since each area on the 3D surface is estimated only once during the fragment reconstruction.\n\nGRU Fusion. To make the reconstruction consistent between fragments, we propose to make the current-fragment reconstruction to be conditioned on the reconstructions in previous fragments. We use a 3D convolutional variant of Gated Recurrent Unit (GRU) [6] module for this purpose. As illustrated in Fig.3 ii, at each level the image feature volume F l t is first passed through the 3D sparse convolu-tion layers to extract 3D geometric features G l t . The hidden state H l t\u22121 is extracted from the global hidden state H g t\u22121\n\nwithin the fragment bounding volume. GRU fuses G l t with hidden state H l t\u22121 and produces the updated hidden state H l t , which will be passed through the MLP layers to predict the TSDF volume S l t at this level. The hidden state H l t will also be updated to global hidden state H g t by directly replacing the corresponding voxels. Formally, denoting z t as the update gate, r t as the reset gate, \u03c3 as the sigmoid function and W * as the weight for sparse convolution, GRU fuses G l t with hidden state H l t\u22121 with the following operations:\nz t = \u03c3(SparseConv([H l t\u22121 , G l t ], W z )) r t = \u03c3(SparseConv([H l t\u22121 , G l t ], W r )) H l t = tanh(SparseConv([r t H l t\u22121 , G l t ], W h )) H l t = (1 \u2212 z t ) H l t\u22121 + z t Hl t\nIntuitively, in the context of joint reconstruction and fusion of TSDF, the update gate z t and forget gate r t in the GRU determine how much information from the previous reconstructions (i.e. hidden state H l t\u22121 ) is fused to the current-fragment geometric feature G l t , as well as how much information from the current-fragment will be fused into the hidden state H l t . As a data-driven approach, the GRU serves as a selective attention mechanism that replaces the linear running-average operation in conventional TSDF fusion [31]. By predicting S l t after the GRU, the MLP network can leverage the context information accumulated from history fragments to produce consistent surface geometry across local fragments. This is also conceptually analogous to the depth filter in a non-learning-based 3D reconstruction pipeline [38,34], where the current observation and the temporally-fused depths are fused with the Bayesian filter. The effectiveness of joint reconstruction and fusion is validated in the ablation study.\n\nIntegration to the Global TSDF Volume. At the last coarse-to-fine level, S 3 t is predicted and further sparsified to S l t . Since the fusion between S l t and S g t has been done in GRU Fusion, S l t is integrated into S g t by directly replacing the corresponding voxels after being transformed into the global coordinate. At each time step t, Marching Cubes is performed on S g t to reconstruct the mesh. Supervision. Following [9], two loss functions are used to supervise the network. The occupancy loss is defined as the binary cross-entropy (BCE) between the predicted occupancy values and the ground-truth occupancy values. The SDF loss is defined as the 1 distance between the predicted SDF values and the ground-truth SDF values. We log-transform the SDF values of predictions and groundtruth before applying the 1 loss. The supervision is applied to all the coarse-to-fine levels.\n\n\nImplementation Details\n\nWe use torchsparse [43] as the implementation of 3D sparse convolution. The image backbone is a variant of MnasNet [41] and is initialized with the weights pretrained from ImageNet. Feature Pyramid Network [23] is used in the backbone to extract more representative multi-level features. The entire network is trained end-to-end with randomly initialized weights except for the image backbone. The occupancy score o is predicted with a Sigmoid layer. The voxel size of the last level is 4cm and the TSDF truncation distance \u03bb is set to 12cm. d max is set to 3m. R max and t max are set to 15\u00b0and 0.1m respectively. \u03b8 is set to 0.5. Nearest-neighbor interpolation is used in the upsampling between coarse-to-fine levels.\n\n\nExperiments\n\nIn this section, we conduct a series of experiments to evaluate the reconstruction quality and different design considerations of NeuralRecon.\n\n\nDatasets, Metrics, Baselines and Protocols.\n\nDatasets. We perform the experiments on two indoor datasets, ScanNet (V2) [8] and 7-Scenes [39]. The ScanNet dataset contains 1613 indoor scenes with ground-truth camera poses, surface reconstructions, and semantic segmentation labels. There are two training/validation splits commonly used in previous works (defined in [30] and [42]) for the ScanNet dataset. We use the same training and validation data with the corresponding baseline methods to make a fair comparison. The 7-Scenes dataset is another challenging RGB-D dataset captured in indoor scenes. Following the baseline method [26], we use the model trained on ScanNet to perform the validation on 7-Scenes.\n\nMetrics. The 3D reconstruction quality is evaluated using 3D geometry metrics presented in [30], as well as standard 2D depth metrics defined in [11]. The definitions of these metrics are detailed in the supplementary material. Among these 3D and 2D metrics, we consider F-score as the most suitable metrics to measure 3D reconstruction quality since both the accuracy and completeness of the reconstruction are considered.\n\nBaselines. We compare our method with the following baseline methods in three categories: 1) Real-time methods for multi-view depth estimation [48,13,24,26]. Due to the efficiency constraints, the estimated depth accuracy by these methods is rather limited. We compare with these methods to demonstrate the better reconstruction accuracy of NeuralRecon given the same efficiency. 2) Multiple View Stereo methods [37,14,53,30,28]. These offline methods have much higher accuracy compared to real-time methods. These baselines are used to demonstrate that NeuralRecon achieves a reconstruction quality on-par with offline methods but runs in real-time. 3) Learning-based SLAM methods [45,42,44]. These monocular SLAM methods estimate camera poses and perform reconstruction simultaneously, thus the scale factor of pose and depth is usually not accurately estimated. For a fair comparison, we use groundtruth camera poses for these methods and apply a scaling factor to the predicted depth map using ground-truth depth. Among all these baseline methods, GPMVS [13] and Atlas [30] are the most relevant real-time and offline methods, respectively.\n\nEvaluation Protocols. Since our method does not estimate depth maps explicitly, we render the reconstructed mesh to the image plane and obtain depth map estimations [30]. Key frames used for evaluation are sampled from the video sequence with an interval of 10 frames for both depth-based methods and Atlas. Following [30,26], [53,48,14,13] are fine-tuned on ScanNet. To evaluate depth-based methods [37,48,13,14] in 3D, we use the point cloud fusion to obtain the 3D reconstruction following Atlas. For other depthbased methods, we use the standard TSDF fusion proposed in [31,7]. For the reasons we detailed in the supplementary material, in order to make a fair comparison with Atlas, we also report the evaluation results using the double-layered mesh (same as Atlas). The evaluation of 3D geometry on 7-Scenes uses the single-layered mesh. We also evaluate the depth filtering operation with multi-view consistency check, which will be elaborated in the supplementary material.\n\n\nEvaluation Results\n\nScanNet. 2D depth metrics and 3D geometry metrics are used on the ScanNet dataset. The 3D geometry evaluation results are shown in Tab. 1. Our method produces much better performance than recent learning-based methods and achieves slightly better results than COLMAP. We believe that the improvements come from the joint reconstruction and fusion design achieved by the GRU Fusion module. Compared to depth-based methods, NeuralRecon can produce coherent reconstructions both locally and globally. Our method also surpasses the volumetric baseline method Atlas [30] on the accuracy, precision, and F-score. The improvements potentially come from the design of local fragment separation in our method, which can act as a view-selection mechanism that avoids irrelevant image features to be fused into the 3D volume. In terms of completeness and recall, the proposed method has an inferior performance compared to both depth-based methods and Atlas. Since depth-based methods predict pixel-wise depth maps on each view, the coverage of their predictions is high by nature, but with the cost of accuracy. Being an offline approach, Atlas has the advantage of having a global context from the entire sequence before predicting the geometry. As a result, Atlas sometimes achieves even better completeness compared to the ground-truth due to its TSDF completion capability. However, Atlas tends to predict over-smoothed geometries, and the completed regions may be inaccurate. As for 2D depth metrics, NeuralRecon also outperforms previous state-of-the-art methods for almost all 2D depth metrics, as shown in Tab. 2.\n\n7-Scenes. 2D depth metrics and 3D geometry metrics are evaluated on the 7-Scenes dataset. As shown in Tab. 3, our method achieves comparable performance to the stateof-the-art method CNMNet [26] and outperforms all other methods. We believe that the accuracy of the proposed method can be further improved by leveraging the planar structure information as in CNMNet. Since the model used here is only trained on ScanNet, the results also demonstrate that NeuralRecon can generalize well beyond the domain of the training data.\n\nEfficiency. We also report the average running time of the baselines and our method in Tab. 1. Only the inference time on key frames is computed. A detailed timing analysis for each module of NeuralRecon is presented in Table 4. For volumetric methods (Atlas and ours), the running time is obtained by dividing the time of reconstructing the TSDF volume of a local fragment by the number of key frames in the local fragment. Notice that the time for TSDF fusion is not included for depth-based methods. The running time for [44,28,24,26,45] and NeuralRecon is measured on an NVIDIA RTX 2080Ti GPU. We use running time reported in [30] and [55] for [48,14,37,13,30] and [53], respectively. As shown in Tab. 1, our time cost is 30ms per key frame, achieving real-time speed at 33 key frames per second and outperforming all previous methods. Specifically, our method runs \u223c10\u00d7 faster than Atlas, and 77\u00d7 faster than Consistent Depth. Predicting the volumetric representation removes the redundant computation in depth-based methods, which contributes to the fast running speed of our method. Compared to Atlas, incrementally reconstructing geometry in local fragment avoids processing a huge 3D volume, leading to a faster speed than Atlas. of sparse convolution also contributes to the superior efficiency of NeuralRecon.\n\n\nAblation Study\n\nIn this section, we conduct several ablation experiments on the ScanNet dataset to discuss the effectiveness of components in our method.\n\nGRU Fusion. We validate the GRU Fusion design by comparing rows from (i) to (iv) in Tab. 5.\n\nTo validate the benefit of feature fusion, we compare row (i) and row (ii) in Tab. 5. Using feature fusion with the average operation obtains nearly 5% improvement for the precision metric than conventional linear TSDF fusion. Visualization in Fig. 5 shows that feature fusion with the average operation can reconstruct smoother geometry. These results demonstrate that feature fusion can be more effective than TSDF fusion using the same average operation.\n\nComparing row (ii) and row (iii) in Tab. 5 shows that replacing average operation with GRU gives 4% improvement in terms of recall. The mesh in Fig. 5 (iii) is also more complete than that in Fig. 5 (ii). These results demonstrate that the GRU is more effective to selectively integrate only the consistent information from the current-fragment to the hidden state.\n\nThe recalls in row (iii) and row (iv) in Tab. 5 show that fusion in the fragment bounding volume can produce much more complete results. Visualization results in Fig. 5 (iii) and (iv) show that, with fusion in the fragment bounding volume, our method produces fewer artifacts on the ground. Fusion in the fragment bounding volume can leverage the context information in boundaries and produce more consistent and complete surface estimation.\n\nNumber of views. We set 5, 7, 9 and 11 views as the length of a fragment respectively. As shown in row (v) in Tab. 5, the F-score has over 2% improvement when 9 views are used as a fragment. As shown in visualization results in Fig. 5 (v), with more views in a fragment, the geometry can be reconstructed more accurately compared to Fig. 5 Table 5. Ablation study. We report 3D geometry metrics on Scan-Net. OCC: fuse 3D geometric features G l t within the occupied area where occupancy score o > \u03b8. FBV: fuse 3D geometric features G l t within the fragment bounding volume. Linear: remove GRU-Fusion and use the conventional running-average-based linear TSDF fusion to update the global TSDF volume. Avg: fuse 3D geometric features G l t with the average operation. GRU: fuse 3D geometric features G l t with GRU. We use row (v) in all other experiments. More details about ablation experiments can be found in the supplementary material.\n\nQualitative Results. We provide the qualitative results and the corresponding analysis in Fig. 4.\n\n\nConclusion\n\nIn this paper, we introduced a novel system NeuralRecon for real-time 3D reconstruction with monocular video. The key idea is to jointly reconstruct and fuse sparse TSDF volumes for each video fragment incrementally by 3D sparse convolutions and GRU. This design enables NeuralRecon to output accurate and coherent reconstruction in real-time. Experiments show that NeuralRecon outperforms state-ofthe-art methods in both reconstruction quality and running speed. The sparse TSDF volume reconstructed by Neural-Recon can be directly used in downstream tasks like 3D object detection, 3D semantic segmentation and neural rendering. We believe that, by jointly training with the downstream tasks end-to-end, NeuralRecon enables new possibilities in learning-based multi-view perception and recognition systems.  Figure 4. Qualitative results on ScanNet. Compared to depth-based methods, NeuralRecon can produce much more coherent reconstruction results. Notice that our method also recovers sharper geometry compared to Atlas [30], which illustrates the effectiveness of the local fragment design in our method. Reconstructing only within the local fragment window avoids irrelevant image features from far-away camera views to be fused into the 3D volume. The color indicates surface normal. More qualitative results can be found in the supplementary material and the project webpage. Zoom in for details. \n\nFigure 3 .\n32D toy examples to illustrate the unprojection, GRU fusion and sparse TSDF representation. In figure i and ii, the colored grids mean different features. In figure iii, the colored grids mean different TSDF values. Best viewed in color.\n\nFigure 5 .\n5Ablation study. The indications of Roman numerals are in Tab. 5. The analysis is presented in Sec. 4.3.\n\n\nMethodLayer Comp \u2193 Acc \u2193 Recall \u2191 Prec \u2191 F-score \u2191 Time (ms) \u2193MVDepthNet [48] \nsingle \n0.040 \n0.240 \n0.831 \n0.208 \n0.329 \n48 \nGPMVS [13] \nsingle \n0.031 \n0.879 \n0.871 \n0.188 \n0.304 \n51 \nDPSNet [14] \nsingle \n0.045 \n0.284 \n0.793 \n0.223 \n0.344 \n322 \nCOLMAP [37] \nsingle \n0.069 \n0.135 \n0.634 \n0.505 \n0.558 \n2076 \nOurs \nsingle \n0.128 \n0.054 \n0.479 \n0.684 \n0.562 \n30 \nAtlas [30] \ndouble \n0.062 \n0.128 \n0.732 \n0.382 \n0.499 \n292 \nOurs \ndouble \n0.106 \n0.073 \n0.609 \n0.450 \n0.516 \n30 \nDeepV2D [44] \nsingle \n0.057 \n0.239 \n0.646 \n0.329 \n0.431 \n347 \nConsistent Depth [28] single \n0.091 \n0.344 \n0.461 \n0.266 \n0.331 \n2321 \nOurs \nsingle \n0.120 \n0.062 \n0.428 \n0.592 \n0.494 \n30 \n\nTable 1. 3D geometry metrics on ScanNet. We use two different training/validation splits following Atlas [30] (top block) and BA-Net \n[42] (bottom block). We elaborate the meaning of the single and double layer in the supplementary material. \n\nMethod \nAbs Rel \u2193 Abs Diff \u2193 Sq Rel \u2193 \nRMSE \u2193 \n\u03b4 < 1.25 \u2191 Comp \u2191 \nCOLMAP [37] \n0.137 \n0.264 \n0.138 \n0.502 \n83.4 \n0.871 \nMVDepthNet [48] \n0.098 \n0.191 \n0.061 \n0.293 \n89.6 \n0.928 \nGPMVS [13] \n0.130 \n0.239 \n0.339 \n0.472 \n90.6 \n0.928 \nDPSNet [14] \n0.087 \n0.158 \n0.035 \n0.232 \n92.5 \n0.928 \nAtlas [30] \n0.065 \n0.123 \n0.045 \n0.251 \n93.6 \n0.999 \nOurs \n0.065 \n0.106 \n0.031 \n0.195 \n94.8 \n0.909 \nMethod \nAbs Rel \u2193 \nSq Rel \u2193 \nRMSE \u2193 RMSE log \u2193 \nSc Inv \u2193 \n-\nDeMoN [45] \n0.231 \n0.520 \n0.761 \n0.289 \n0.284 \n-\nBA-Net [42] \n0.161 \n0.092 \n0.346 \n0.214 \n0.184 \n-\nDeepV2D [44] \n0.057 \n0.010 \n0.168 \n0.080 \n0.077 \n-\nConsistent Depth [28] \n0.073 \n0.037 \n0.217 \n0.105 \n0.103 \n-\nOurs \n0.047 \n0.024 \n0.164 \n0.093 \n0.092 \n-\n\nTable 2. 2D depth metrics on ScanNet. We use two different training/validation splits following Atlas [30] (top block) and BA-Net [42] \n(bottom block). \n\n\n\n\nThe use Abs Rel \u2193 Sq Rel \u2193 RMSE \u2193Table 3. 3D geometry metrics (top block) and 2D depth metrics (bottom block) on 7-Scenes. Time is measured in milliseconds.Method \n\nComp \u2193 \nAcc \u2193 \nRecall \u2191 \nPrec \u2191 \nF-score \u2191 \nDeepV2D [44] \n0.180 \n0.518 \n0.175 \n0.087 \n0.115 \nCNMNet [26] \n0.150 \n0.398 \n0.246 \n0.111 \n0.149 \nOurs \n0.228 \n0.100 \n0.227 \n0.389 \n0.282 \nMethod \n\n\u03b4 < 1.25 \u2191 \n\nTime \u2193 \nDeMoN [45] \n31.88 \n0.3888 \n0.4198 \n0.8549 \n110 \nMVSNet [53] \n64.09 \n0.2339 \n0.1904 \n0.5078 \n1050 \nN-RGBD [24] \n69.26 \n0.1758 \n0.1123 \n0.4408 \n202 \nMVDNet [48] \n71.79 \n0.1925 \n0.2350 \n0.4585 \n48 \nDPSNet [14] \n70.96 \n0.1991 \n0.1420 \n0.4382 \n322 \nDeepV2D [44] \n42.80 \n0.4370 \n0.5530 \n0.8690 \n347 \nCNMNet [26] \n76.64 \n0.1612 \n0.0832 \n0.3614 \n80 \nOurs \n82.00 \n0.1550 \n0.1040 \n0.3470 \n30 \n\n\n\n\n(iv).Img. Enc. \nUnproj. Sparse Conv. GRU Total \n\n4.03 \n\nLevel 1 \n1.27 \n3.70 \n2.18 \n29.56 \nLevel 2 \n1.21 \n3.84 \n2.24 \nLevel 3 \n2.18 \n5.11 \n3.80 \n\n\n\nTable 4 .\n4Timing analysis of NeuralRecon measured in milliseconds per key frame. The level number indicates the different coarse-to-fine level. Img. Enc. stands for image encoder, Unproj. stands for unprojection.#views \nFusion \n3D Geometry Metrics \nArea Method Recall Prec F-score \ni \n5 \nOCC Linear \n0.576 0.386 \n0.462 \nii \n5 \nOCC \nAvg \n0.535 0.432 \n0.478 \niii \n5 \nOCC \nGRU \n0.572 0.426 \n0.488 \niv \n5 \nFBV \nGRU \n0.613 0.421 \n0.494 \n-\n7 \nFBV \nGRU \n0.607 0.435 \n0.507 \nv \n9 \nFBV \nGRU \n0.609 0.450 \n0.516 \n-\n11 \nFBV \nGRU \n0.593 0.398 \n0.474 \n\n\n\n. Augmented Reality with ARKit-Apple Developer. 1Augmented Reality with ARKit-Apple Developer. 1\n\nEngin Tola, and Anders Bjorholm Dahl. Large-Scale Data for Multiple-View Stereopsis. Henrik Aanaes, George Rasmus Ramsb\u00f8l Jensen, Vogiatzis, IJCV. 2Henrik Aanaes, Rasmus Ramsb\u00f8l Jensen, George Vogiatzis, Engin Tola, and Anders Bjorholm Dahl. Large-Scale Data for Multiple-View Stereopsis. IJCV, 2016. 2\n\nORB-SLAM3: An Accurate Open-Source Library for Visual. Carlos Campos, Richard Elvira, Juan J G\u00f3mez Rodr\u00edguez, M M Jos\u00e9, Juan D Montiel, Tard\u00f3s, Visual-Inertial and Multi-Map SLAM. ArXiv. Carlos Campos, Richard Elvira, Juan J. G\u00f3mez Rodr\u00edguez, Jos\u00e9 M. M. Montiel, and Juan D. Tard\u00f3s. ORB-SLAM3: An Accurate Open-Source Library for Visual, Visual-Inertial and Multi-Map SLAM. ArXiv, 2020. 1\n\nDeep stereo using adaptive thin volume representation with uncertainty awareness. Shuo Cheng, Zexiang Xu, Shilin Zhu, Zhuwen Li, Li Erran Li, Ravi Ramamoorthi, Hao Su, CVPR. Shuo Cheng, Zexiang Xu, Shilin Zhu, Zhuwen Li, Li Erran Li, Ravi Ramamoorthi, and Hao Su. Deep stereo using adap- tive thin volume representation with uncertainty awareness. In CVPR, 2020. 2\n\n3D-R2N2: A unified approach for single and multi-view 3D object reconstruction. B Christopher, Danfei Choy, Junyoung Xu, Kevin Gwak, Silvio Chen, Savarese, ECCV. Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin Chen, and Silvio Savarese. 3D-R2N2: A unified approach for single and multi-view 3D object reconstruction. In ECCV, 2016. 2\n\nEmpirical evaluation of gated recurrent neural networks on sequence modeling. Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, Yoshua Bengio, NeurIPS 2014 Workshop on Deep Learning. 3Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neu- ral networks on sequence modeling. In NeurIPS 2014 Work- shop on Deep Learning, 2014. 3.2\n\nA Volumetric Method for Building Complex Models from Range Images. Brian Curless, Marc Levoy, In SIG-GRAPH, 1996. 2, 4.1Brian Curless and Marc Levoy. A Volumetric Method for Building Complex Models from Range Images. In SIG- GRAPH, 1996. 2, 4.1\n\nScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes. Angela Dai, X Angel, Manolis Chang, Maciej Savva, Thomas Halber, Matthias Funkhouser, Nie\u00dfner, CVPR. 4Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal- ber, Thomas Funkhouser, and Matthias Nie\u00dfner. ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes. In CVPR, 2017. 4.1\n\nSG-NN: Sparse Generative Neural Networks for Self-Supervised Scene Completion of RGB-D Scans. Angela Dai, Christian Diller, Matthias Nie\u00dfner, CVPR. Angela Dai, Christian Diller, and Matthias Nie\u00dfner. SG- NN: Sparse Generative Neural Networks for Self-Supervised Scene Completion of RGB-D Scans. In CVPR, 2020. 3.2\n\nBundleFusion: Real-time globally consistent 3d reconstruction using on-the-fly surface reintegration. Angela Dai, Matthias Nie\u00dfner, Michael Zollh\u00f6fer, Shahram Izadi, Christian Theobalt, ACM TOGAngela Dai, Matthias Nie\u00dfner, Michael Zollh\u00f6fer, Shahram Izadi, and Christian Theobalt. BundleFusion: Real-time globally consistent 3d reconstruction using on-the-fly surface reintegration. ACM TOG, 2017. 2\n\nDepth map prediction from a single image using a multi-scale deep network. David Eigen, Christian Puhrsch, Rob Fergus, NeurIPS. 4David Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from a single image using a multi-scale deep net- work. In NeurIPS, 2014. 4.1\n\nFeitong Tan, and Ping Tan. Cascade cost volume for high-resolution multi-view stereo and stereo matching. Xiaodong Gu, Zhiwen Fan, Siyu Zhu, Zuozhuo Dai, CVPR. 2020Xiaodong Gu, Zhiwen Fan, Siyu Zhu, Zuozhuo Dai, Feitong Tan, and Ping Tan. Cascade cost volume for high-resolution multi-view stereo and stereo matching. In CVPR, 2020. 2\n\nMulti-view stereo by temporal nonparametric fusion. Yuxin Hou, Juho Kannala, Arno Solin, ICCV. . 1, 3.1, 4.1, 4.1, 4.2Yuxin Hou, Juho Kannala, and Arno Solin. Multi-view stereo by temporal nonparametric fusion. In ICCV, 2019. 1, 3.1, 4.1, 4.1, 4.2\n\nDPSNet: End-to-end Deep Plane Sweep Stereo. Sunghoon Im, Hae-Gon Jeon, Stephen Lin, In So Kweon, ICLR, 2019. 4.1, 4.1, 4.2Sunghoon Im, Hae-Gon Jeon, Stephen Lin, and In So Kweon. DPSNet: End-to-end Deep Plane Sweep Stereo. In ICLR, 2019. 4.1, 4.1, 4.2\n\nSurfaceNet: An end-to-end 3D neural network for multiview stereopsis. Mengqi Ji, Juergen Gall, Haitian Zheng, Yebin Liu, Lu Fang, ICCV. 2Mengqi Ji, Juergen Gall, Haitian Zheng, Yebin Liu, and Lu Fang. SurfaceNet: An end-to-end 3D neural network for multiview stereopsis. In ICCV, 2017. 2, 3.2\n\nSur-faceNet+: An End-to-End 3D Neural Network for Very Sparse Multi-View Stereopsis. Mengqi Ji, Jinzhi Zhang, Qionghai Dai, Lu Fang, IEEE TPAMI. 20202Mengqi Ji, Jinzhi Zhang, Qionghai Dai, and Lu Fang. Sur- faceNet+: An End-to-End 3D Neural Network for Very Sparse Multi-View Stereopsis. IEEE TPAMI, 2020. 2\n\nLocal implicit grid representations for 3d scenes. Chiyu Jiang, Avneesh Sud, Ameesh Makadia, Jingwei Huang, Matthias Nie\u00dfner, Thomas Funkhouser, CVPR. Chiyu Jiang, Avneesh Sud, Ameesh Makadia, Jingwei Huang, Matthias Nie\u00dfner, and Thomas Funkhouser. Local implicit grid representations for 3d scenes. In CVPR, 2020. 2\n\nLearning a Multi-View Stereo Machine. Abhishek Kar, Christian H\u00e4ne, Jitendra Malik, NeurIPS. 2Abhishek Kar, Christian H\u00e4ne, and Jitendra Malik. Learning a Multi-View Stereo Machine. In NeurIPS, 2017. 2, 3.2\n\nScreened Poisson Surface Reconstruction. Michael Kazhdan, Hugues Hoppe, ACM TOGMichael Kazhdan and Hugues Hoppe. Screened Poisson Sur- face Reconstruction. ACM TOG, 2013. 2\n\nTanks and Temples: Benchmarking Large-Scale Scene Reconstruction. Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, Vladlen Koltun, ACM TOGArno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and Temples: Benchmarking Large-Scale Scene Reconstruction. ACM TOG, 2017. 2\n\nTurning Mobile Phones into 3D Scanners. Kalin Kolev, Petri Tanskanen, Pablo Speciale, Marc Pollefeys, CVPR. Kalin Kolev, Petri Tanskanen, Pablo Speciale, and Marc Pollefeys. Turning Mobile Phones into 3D Scanners. In CVPR, 2014. 2\n\nRobust and Efficient Surface Reconstruction From Range Data. P Labatut, J.-P Pons, R Keriven, Computer Graphics Forum. 2P. Labatut, J.-P. Pons, and R. Keriven. Robust and Efficient Surface Reconstruction From Range Data. Computer Graph- ics Forum, 2009. 2\n\nKaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. Tsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, CVPR. 3Tsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In CVPR, 2017. 3.3\n\nNeural RGB-> D Sensing: Depth and uncertainty from a video camera. Chao Liu, Jinwei Gu, Kihwan Kim, G Srinivasa, Jan Narasimhan, Kautz, CVPR. . 1, 2, 4.1, 4.2Chao Liu, Jinwei Gu, Kihwan Kim, Srinivasa G Narasimhan, and Jan Kautz. Neural RGB-> D Sensing: Depth and uncer- tainty from a video camera. In CVPR, 2019. 1, 2, 4.1, 4.2\n\nLingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. Neural Sparse Voxel Fields. 2020NeurIPSLingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. Neural Sparse Voxel Fields. In NeurIPS, 2020. 2\n\nOcclusion-Aware Depth Estimation with Adaptive Normal Constraints. Xiaoxiao Long, Lingjie Liu, Christian Theobalt, Wenping Wang, In ECCV, 2020. 2, 4.1, 4.2Xiaoxiao Long, Lingjie Liu, Christian Theobalt, and Wen- ping Wang. Occlusion-Aware Depth Estimation with Adap- tive Normal Constraints. In ECCV, 2020. 2, 4.1, 4.2\n\nMarching Cubes: A High Resolution 3D Surface Construction Algorithm. SIGGRAPH. William E Lorensen, Harvey E Cline, William E. Lorensen and Harvey E. Cline. Marching Cubes: A High Resolution 3D Surface Construction Algorithm. SIGGRAPH, 1987. 1\n\n. Xuan Luo, Jia-Bin Huang, Richard Szeliski, Kevin Matzen, Johannes Kopf, Consistent Video Depth Estimation. ACM TOG, 2020. 4.1, 4.1, 4.2Xuan Luo, Jia-Bin Huang, Richard Szeliski, Kevin Matzen, and Johannes Kopf. Consistent Video Depth Estimation. ACM TOG, 2020. 4.1, 4.1, 4.2\n\nOccupancy Networks: Learning 3d reconstruction in function space. Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, Andreas Geiger, CVPR. Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se- bastian Nowozin, and Andreas Geiger. Occupancy Net- works: Learning 3d reconstruction in function space. In CVPR, 2019. 2\n\nAtlas: Endto-End 3D Scene Reconstruction from Posed Images. Zak Murez, James Tarrence Van As, Ayan Bartolozzi, Vijay Sinha, Andrew Badrinarayanan, Rabinovich, In ECCV, 2020. 1, 2, 3.2, 4.1, 1, 4.1, 2, 4.2, 4Zak Murez, Tarrence van As, James Bartolozzi, Ayan Sinha, Vijay Badrinarayanan, and Andrew Rabinovich. Atlas: End- to-End 3D Scene Reconstruction from Posed Images. In ECCV, 2020. 1, 2, 3.2, 4.1, 1, 4.1, 2, 4.2, 4\n\nKinectFusion: Real-time dense surface mapping and tracking. R A Newcombe, S Izadi, O Hilliges, D Molyneaux, D Kim, A J Davison, P Kohi, J Shotton, S Hodges, A Fitzgibbon, ISMAR, 2011. 1, 2, 3.2, 4.1R. A. Newcombe, S. Izadi, O. Hilliges, D. Molyneaux, D. Kim, A. J. Davison, P. Kohi, J. Shotton, S. Hodges, and A. Fitzgibbon. KinectFusion: Real-time dense surface mapping and tracking. In ISMAR, 2011. 1, 2, 3.2, 4.1\n\nReal-Time 3D Reconstruction at Scale Using Voxel Hashing. Matthias Nie\u00dfner, Michael Zollh\u00f6fer, Shahram Izadi, Marc Stamminger, ACM TOGMatthias Nie\u00dfner, Michael Zollh\u00f6fer, Shahram Izadi, and Marc Stamminger. Real-Time 3D Reconstruction at Scale Using Voxel Hashing. ACM TOG, 2013. 2\n\nDeepSDF: Learning Continuous Signed Distance Functions for Shape Representation. Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, Steven Lovegrove, CVPR. Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. DeepSDF: Learning Continuous Signed Distance Functions for Shape Represen- tation. In CVPR, 2019. 2\n\nREMODE: Probabilistic, Monocular Dense Reconstruction in Real Time. Matia Pizzoli, Christian Forster, Davide Scaramuzza, ICRA. 2Matia Pizzoli, Christian Forster, and Davide Scaramuzza. REMODE: Probabilistic, Monocular Dense Reconstruction in Real Time. In ICRA, 2014. 2, 3.2\n\nA General Optimization-Based Framework for Local Odometry Estimation with Multiple Sensors. Jie Tong Qin, Shaozu Pan, Shaojie Cao, Shen, ArXiv. 1Tong Qin, Jie Pan, Shaozu Cao, and Shaojie Shen. A Gen- eral Optimization-Based Framework for Local Odometry Es- timation with Multiple Sensors. ArXiv, 2019. 1\n\nPIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization. Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa, Hao Li, ICCV. Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Mor- ishima, Angjoo Kanazawa, and Hao Li. PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Dig- itization. In ICCV, 2019. 2\n\nPixelwise View Selection for Unstructured Multi-View Stereo. Johannes L Sch\u00f6nberger, Enliang Zheng, Jan-Michael Frahm, Marc Pollefeys, In ECCV. 2016. 2, 2, 4.1, 4.1, 4.2Johannes L. Sch\u00f6nberger, Enliang Zheng, Jan-Michael Frahm, and Marc Pollefeys. Pixelwise View Selection for Unstructured Multi-View Stereo. In ECCV. 2016. 2, 2, 4.1, 4.1, 4.2\n\n3D Modeling on the Go: Interactive 3D Reconstruction of Large-Scale Scenes on Mobile Devices. Thomas Schops, Torsten Sattler, Christian Hane, Marc Pollefeys, 3DV. Thomas Schops, Torsten Sattler, Christian Hane, and Marc Pollefeys. 3D Modeling on the Go: Interactive 3D Recon- struction of Large-Scale Scenes on Mobile Devices. In 3DV, 2015. 1, 2, 3.2\n\nScene Coordinate Regression Forests for Camera Relocalization in RGB-D Images. Jamie Shotton, Ben Glocker, Christopher Zach, Shahram Izadi, Antonio Criminisi, Andrew Fitzgibbon, CVPR. 4Jamie Shotton, Ben Glocker, Christopher Zach, Shahram Izadi, Antonio Criminisi, and Andrew Fitzgibbon. Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images. In CVPR, 2013. 4.1\n\nRobust reconstruction of indoor scenes. Sungjoon Choi, Q Zhou, V Koltun, CVPR. 3Sungjoon Choi, Q. Zhou, and V. Koltun. Robust reconstruc- tion of indoor scenes. In CVPR, 2015. 3.1\n\nMnas-Net: Platform-aware neural architecture search for mobile. Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, Quoc V Le, CVPR. 3Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and Quoc V Le. Mnas- Net: Platform-aware neural architecture search for mobile. In CVPR, 2019. 3.3\n\nBA-Net: Dense Bundle Adjustment Networks. Chengzhou Tang, Ping Tan, ICLR, 2019. 2, 4.1, 1, 4.1, 2Chengzhou Tang and Ping Tan. BA-Net: Dense Bundle Ad- justment Networks. In ICLR, 2019. 2, 4.1, 1, 4.1, 2\n\nSearching Efficient 3D Architectures with Sparse Point-Voxel Convolution. Haotian Tang, Zhijian Liu, Shengyu Zhao, Yujun Lin, Ji Lin, Hanrui Wang, Song Han, In ECCV, 2020. 3.3Haotian Tang, Zhijian Liu, Shengyu Zhao, Yujun Lin, Ji Lin, Hanrui Wang, and Song Han. Searching Efficient 3D Ar- chitectures with Sparse Point-Voxel Convolution. In ECCV, 2020. 3.3\n\nDeepV2D: Video to Depth with Differentiable Structure from Motion. Zachary Teed, Jia Deng, ICLR, 2020. 2, 4.1, 4.1, 4.2Zachary Teed and Jia Deng. DeepV2D: Video to Depth with Differentiable Structure from Motion. In ICLR, 2020. 2, 4.1, 4.1, 4.2\n\nDeMoN: Depth and Motion Network for Learning Monocular Stereo. Benjamin Ummenhofer, Huizhong Zhou, Jonas Uhrig, Nikolaus Mayer, Eddy Ilg, Alexey Dosovitskiy, Thomas Brox, CVPR, 2017. . 2, 4.1, 4.1, 4.2Benjamin Ummenhofer, Huizhong Zhou, Jonas Uhrig, Niko- laus Mayer, Eddy Ilg, Alexey Dosovitskiy, and Thomas Brox. DeMoN: Depth and Motion Network for Learning Monocular Stereo. In CVPR, 2017. 2, 4.1, 4.1, 4.2\n\nSean Fanello, Shahram Izadi, and Christoph Rhemann. Depth from Motion for Smartphone AR. Julien Valentin, Adarsh Kowdle, Jonathan T Barron, Neal Wadhwa, Max Dzitsiuk, Michael Schoenberg, Vivek Verma, Ambrus Csaszar, Eric Turner, Ivan Dryanovski, Joao Afonso, Jose Pascoal, Konstantine Tsotsos, Mira Leung, Mirko Schmidt, Onur Guleryuz, ACM TOG1Sameh Khamis, Vladimir TankovitchJulien Valentin, Adarsh Kowdle, Jonathan T. Barron, Neal Wadhwa, Max Dzitsiuk, Michael Schoenberg, Vivek Verma, Ambrus Csaszar, Eric Turner, Ivan Dryanovski, Joao Afonso, Jose Pascoal, Konstantine Tsotsos, Mira Leung, Mirko Schmidt, Onur Guleryuz, Sameh Khamis, Vladimir Tankovitch, Sean Fanello, Shahram Izadi, and Christoph Rhemann. Depth from Motion for Smartphone AR. ACM TOG, 2019. 1, 2\n\nVideo-Based, Real-Time Multi-View Stereo. Image and Vision Computing. George Vogiatzis, Carlos Hern\u00e1ndez, George Vogiatzis and Carlos Hern\u00e1ndez. Video-Based, Real- Time Multi-View Stereo. Image and Vision Computing, 2011. 2\n\nMVDepthNet: Real-Time Multiview Depth Estimation Neural Network. Kaixuan Wang, Shaojie Shen, 3DV. . 1, 2, 4.1, 4.1, 4.2Kaixuan Wang and Shaojie Shen. MVDepthNet: Real-Time Multiview Depth Estimation Neural Network. In 3DV, 2018. 1, 2, 4.1, 4.1, 4.2\n\nRoutedFusion: Learning Real-Time Depth Map Fusion. Silvan Weder, Johannes Sch\u00f6nberger, Marc Pollefeys, Martin R Oswald, CVPR, 2020. Silvan Weder, Johannes Sch\u00f6nberger, Marc Pollefeys, and Martin R. Oswald. RoutedFusion: Learning Real-Time Depth Map Fusion. In CVPR, 2020. 2\n\nNeuralFusion: Online Depth Fusion in Latent Space. Silvan Weder, Johannes L Sch\u00f6nberger, Marc Pollefeys, Martin R Oswald, 2020Silvan Weder, Johannes L. Sch\u00f6nberger, Marc Pollefeys, and Martin R. Oswald. NeuralFusion: Online Depth Fusion in Latent Space, 2020. 2\n\nMobile3DRecon: Realtime Monocular 3D Reconstruction on a Mobile Phone. Xingbin Yang, L Zhou, Hanqing Jiang, Z Tang, Yuanbo Wang, H Bao, Guofeng Zhang, IEEE TVCG. 2Xingbin Yang, L. Zhou, Hanqing Jiang, Z. Tang, Yuanbo Wang, H. Bao, and Guofeng Zhang. Mobile3DRecon: Real- time Monocular 3D Reconstruction on a Mobile Phone. IEEE TVCG, 2020. 2\n\nReal-Time Monocular Dense Mapping on Aerial Robots Using Visual-Inertial Fusion. Zhenfei Yang, Fei Gao, Shaojie Shen, ICRA. Zhenfei Yang, Fei Gao, and Shaojie Shen. Real-Time Monocular Dense Mapping on Aerial Robots Using Visual- Inertial Fusion. In ICRA, 2017. 1\n\nMVSNet: Depth Inference for Unstructured Multi-View Stereo. Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, Long Quan, ECCV. 2, 4.1, 4.2Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan. MVSNet: Depth Inference for Unstructured Multi-View Stereo. In ECCV, 2018. 2, 4.1, 4.2\n\nMultiview Neural Surface Reconstruction by Disentangling Geometry and Appearance. Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, Yaron Lipman, NeurIPS. 20202Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and Yaron Lipman. Multiview Neu- ral Surface Reconstruction by Disentangling Geometry and Appearance. In NeurIPS, 2020. 2\n\nFast-MVSNet: Sparse-todense multi-view stereo with learned propagation and gaussnewton refinement. Zehao Yu, Shenghua Gao, CVPR, 2020. 4Zehao Yu and Shenghua Gao. Fast-MVSNet: Sparse-to- dense multi-view stereo with learned propagation and gauss- newton refinement. In CVPR, 2020. 4.2\n\nPatchMatch Based Joint View Selection and Depthmap Estimation. Enliang Zheng, Enrique Dunn, Vladimir Jojic, Jan-Michael Frahm, CVPR. Enliang Zheng, Enrique Dunn, Vladimir Jojic, and Jan- Michael Frahm. PatchMatch Based Joint View Selection and Depthmap Estimation. In CVPR, 2014. 2\n\nDeepTAM: Deep Tracking and Mapping. Huizhong Zhou, Benjamin Ummenhofer, Thomas Brox, ECCV. Huizhong Zhou, Benjamin Ummenhofer, and Thomas Brox. DeepTAM: Deep Tracking and Mapping. In ECCV, 2018. 2\n", "annotations": {"author": "[{\"end\":129,\"start\":74},{\"end\":163,\"start\":130},{\"end\":199,\"start\":164},{\"end\":235,\"start\":200},{\"end\":268,\"start\":236}]", "publisher": null, "author_last_name": "[{\"end\":85,\"start\":82},{\"end\":140,\"start\":137},{\"end\":176,\"start\":172},{\"end\":212,\"start\":208},{\"end\":245,\"start\":242}]", "author_first_name": "[{\"end\":81,\"start\":74},{\"end\":136,\"start\":130},{\"end\":171,\"start\":164},{\"end\":207,\"start\":200},{\"end\":241,\"start\":236}]", "author_affiliation": "[{\"end\":107,\"start\":87},{\"end\":128,\"start\":109},{\"end\":162,\"start\":142},{\"end\":198,\"start\":178},{\"end\":234,\"start\":214},{\"end\":267,\"start\":247}]", "title": "[{\"end\":71,\"start\":1},{\"end\":339,\"start\":269}]", "venue": null, "abstract": "[{\"end\":1452,\"start\":369}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1884,\"start\":1881},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":1887,\"start\":1884},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1889,\"start\":1887},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":2094,\"start\":2090},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":2097,\"start\":2094},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2782,\"start\":2778},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":2907,\"start\":2903},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2910,\"start\":2907},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2913,\"start\":2910},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":2916,\"start\":2913},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3201,\"start\":3197},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6003,\"start\":5999},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":6635,\"start\":6631},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6638,\"start\":6635},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":6641,\"start\":6638},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":6644,\"start\":6641},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":6768,\"start\":6764},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":6771,\"start\":6768},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":7021,\"start\":7017},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7044,\"start\":7040},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7148,\"start\":7144},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":7650,\"start\":7646},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":7653,\"start\":7650},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7844,\"start\":7841},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7847,\"start\":7844},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":8105,\"start\":8101},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8129,\"start\":8125},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8131,\"start\":8129},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":8258,\"start\":8254},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":8261,\"start\":8258},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8264,\"start\":8261},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":8267,\"start\":8264},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8377,\"start\":8373},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8380,\"start\":8377},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8506,\"start\":8502},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8903,\"start\":8900},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8906,\"start\":8903},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9333,\"start\":9329},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9362,\"start\":9358},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9393,\"start\":9389},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9480,\"start\":9476},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9520,\"start\":9517},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9637,\"start\":9633},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9640,\"start\":9637},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":10773,\"start\":10769},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":10776,\"start\":10773},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10947,\"start\":10943},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":10950,\"start\":10947},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":10953,\"start\":10950},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10956,\"start\":10953},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":10959,\"start\":10956},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10962,\"start\":10959},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":11145,\"start\":11141},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":11909,\"start\":11905},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":12198,\"start\":12194},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":13326,\"start\":13322},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":13329,\"start\":13326},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":13332,\"start\":13329},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":15566,\"start\":15563},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":17112,\"start\":17108},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":17411,\"start\":17407},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":17414,\"start\":17411},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":18039,\"start\":18036},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":18546,\"start\":18542},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":18642,\"start\":18638},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":18733,\"start\":18729},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":19525,\"start\":19522},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":19543,\"start\":19539},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":19773,\"start\":19769},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":19782,\"start\":19778},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":20040,\"start\":20036},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":20213,\"start\":20209},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":20267,\"start\":20263},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":20690,\"start\":20686},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":20693,\"start\":20690},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":20696,\"start\":20693},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":20699,\"start\":20696},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":20959,\"start\":20955},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":20962,\"start\":20959},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":20965,\"start\":20962},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":20968,\"start\":20965},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":20971,\"start\":20968},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":21229,\"start\":21225},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":21232,\"start\":21229},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":21235,\"start\":21232},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":21605,\"start\":21601},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":21620,\"start\":21616},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":21858,\"start\":21854},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":22011,\"start\":22007},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":22014,\"start\":22011},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":22020,\"start\":22016},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":22023,\"start\":22020},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":22026,\"start\":22023},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":22029,\"start\":22026},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":22093,\"start\":22089},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":22096,\"start\":22093},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":22099,\"start\":22096},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":22102,\"start\":22099},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":22267,\"start\":22263},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":22269,\"start\":22267},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":23259,\"start\":23255},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":24501,\"start\":24497},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":25363,\"start\":25359},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":25366,\"start\":25363},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":25369,\"start\":25366},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":25372,\"start\":25369},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":25375,\"start\":25372},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":25469,\"start\":25465},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":25478,\"start\":25474},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":25487,\"start\":25483},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":25490,\"start\":25487},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":25493,\"start\":25490},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":25496,\"start\":25493},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":25499,\"start\":25496},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":25508,\"start\":25504},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":29756,\"start\":29752}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":30383,\"start\":30134},{\"attributes\":{\"id\":\"fig_3\"},\"end\":30500,\"start\":30384},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":32261,\"start\":30501},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":33025,\"start\":32262},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":33173,\"start\":33026},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":33716,\"start\":33174}]", "paragraph": "[{\"end\":2033,\"start\":1468},{\"end\":4032,\"start\":2035},{\"end\":6413,\"start\":4034},{\"end\":7456,\"start\":6430},{\"end\":8268,\"start\":7458},{\"end\":9095,\"start\":8270},{\"end\":9675,\"start\":9097},{\"end\":10728,\"start\":9795},{\"end\":10866,\"start\":10730},{\"end\":11319,\"start\":10868},{\"end\":11671,\"start\":11331},{\"end\":12680,\"start\":11695},{\"end\":13114,\"start\":12725},{\"end\":13743,\"start\":13116},{\"end\":14766,\"start\":13825},{\"end\":15309,\"start\":14768},{\"end\":15838,\"start\":15311},{\"end\":16388,\"start\":15840},{\"end\":17602,\"start\":16574},{\"end\":18496,\"start\":17604},{\"end\":19242,\"start\":18523},{\"end\":19400,\"start\":19258},{\"end\":20116,\"start\":19448},{\"end\":20541,\"start\":20118},{\"end\":21687,\"start\":20543},{\"end\":22671,\"start\":21689},{\"end\":24305,\"start\":22694},{\"end\":24833,\"start\":24307},{\"end\":26155,\"start\":24835},{\"end\":26311,\"start\":26174},{\"end\":26404,\"start\":26313},{\"end\":26863,\"start\":26406},{\"end\":27230,\"start\":26865},{\"end\":27673,\"start\":27232},{\"end\":28614,\"start\":27675},{\"end\":28713,\"start\":28616},{\"end\":30133,\"start\":28728}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9765,\"start\":9676},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13824,\"start\":13744},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16573,\"start\":16389}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":25062,\"start\":25055},{\"end\":28022,\"start\":28015}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1466,\"start\":1454},{\"attributes\":{\"n\":\"2.\"},\"end\":6428,\"start\":6416},{\"end\":9793,\"start\":9767},{\"attributes\":{\"n\":\"3.\"},\"end\":11329,\"start\":11322},{\"attributes\":{\"n\":\"3.1.\"},\"end\":11693,\"start\":11674},{\"attributes\":{\"n\":\"3.2.\"},\"end\":12723,\"start\":12683},{\"attributes\":{\"n\":\"3.3.\"},\"end\":18521,\"start\":18499},{\"attributes\":{\"n\":\"4.\"},\"end\":19256,\"start\":19245},{\"attributes\":{\"n\":\"4.1.\"},\"end\":19446,\"start\":19403},{\"attributes\":{\"n\":\"4.2.\"},\"end\":22692,\"start\":22674},{\"attributes\":{\"n\":\"4.3.\"},\"end\":26172,\"start\":26158},{\"attributes\":{\"n\":\"5.\"},\"end\":28726,\"start\":28716},{\"end\":30145,\"start\":30135},{\"end\":30395,\"start\":30385},{\"end\":33184,\"start\":33175}]", "table": "[{\"end\":32261,\"start\":30565},{\"end\":33025,\"start\":32420},{\"end\":33173,\"start\":33033},{\"end\":33716,\"start\":33388}]", "figure_caption": "[{\"end\":30383,\"start\":30147},{\"end\":30500,\"start\":30397},{\"end\":30565,\"start\":30503},{\"end\":32420,\"start\":32264},{\"end\":33033,\"start\":33028},{\"end\":33388,\"start\":33186}]", "figure_ref": "[{\"end\":2388,\"start\":2380},{\"end\":3718,\"start\":3712},{\"end\":9819,\"start\":9811},{\"end\":11670,\"start\":11663},{\"end\":13083,\"start\":13077},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13740,\"start\":13735},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14599,\"start\":14594},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15615,\"start\":15610},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26656,\"start\":26650},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":27015,\"start\":27009},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":27063,\"start\":27057},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":27400,\"start\":27394},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":27909,\"start\":27903},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":28014,\"start\":28008},{\"end\":28712,\"start\":28706},{\"end\":29546,\"start\":29538}]", "bib_author_first_name": "[{\"end\":33907,\"start\":33901},{\"end\":33922,\"start\":33916},{\"end\":34181,\"start\":34175},{\"end\":34197,\"start\":34190},{\"end\":34210,\"start\":34206},{\"end\":34212,\"start\":34211},{\"end\":34231,\"start\":34230},{\"end\":34233,\"start\":34232},{\"end\":34244,\"start\":34240},{\"end\":34246,\"start\":34245},{\"end\":34596,\"start\":34592},{\"end\":34611,\"start\":34604},{\"end\":34622,\"start\":34616},{\"end\":34634,\"start\":34628},{\"end\":34641,\"start\":34639},{\"end\":34647,\"start\":34642},{\"end\":34656,\"start\":34652},{\"end\":34673,\"start\":34670},{\"end\":34957,\"start\":34956},{\"end\":34977,\"start\":34971},{\"end\":34992,\"start\":34984},{\"end\":35002,\"start\":34997},{\"end\":35015,\"start\":35009},{\"end\":35301,\"start\":35293},{\"end\":35315,\"start\":35309},{\"end\":35335,\"start\":35326},{\"end\":35347,\"start\":35341},{\"end\":35672,\"start\":35667},{\"end\":35686,\"start\":35682},{\"end\":35915,\"start\":35909},{\"end\":35922,\"start\":35921},{\"end\":35937,\"start\":35930},{\"end\":35951,\"start\":35945},{\"end\":35965,\"start\":35959},{\"end\":35982,\"start\":35974},{\"end\":36294,\"start\":36288},{\"end\":36309,\"start\":36300},{\"end\":36326,\"start\":36318},{\"end\":36617,\"start\":36611},{\"end\":36631,\"start\":36623},{\"end\":36648,\"start\":36641},{\"end\":36667,\"start\":36660},{\"end\":36684,\"start\":36675},{\"end\":36990,\"start\":36985},{\"end\":37007,\"start\":36998},{\"end\":37020,\"start\":37017},{\"end\":37301,\"start\":37293},{\"end\":37312,\"start\":37306},{\"end\":37322,\"start\":37318},{\"end\":37335,\"start\":37328},{\"end\":37580,\"start\":37575},{\"end\":37590,\"start\":37586},{\"end\":37604,\"start\":37600},{\"end\":37824,\"start\":37816},{\"end\":37836,\"start\":37829},{\"end\":37850,\"start\":37843},{\"end\":37858,\"start\":37856},{\"end\":37861,\"start\":37859},{\"end\":38101,\"start\":38095},{\"end\":38113,\"start\":38106},{\"end\":38127,\"start\":38120},{\"end\":38140,\"start\":38135},{\"end\":38148,\"start\":38146},{\"end\":38410,\"start\":38404},{\"end\":38421,\"start\":38415},{\"end\":38437,\"start\":38429},{\"end\":38445,\"start\":38443},{\"end\":38684,\"start\":38679},{\"end\":38699,\"start\":38692},{\"end\":38711,\"start\":38705},{\"end\":38728,\"start\":38721},{\"end\":38744,\"start\":38736},{\"end\":38760,\"start\":38754},{\"end\":38992,\"start\":38984},{\"end\":39007,\"start\":38998},{\"end\":39022,\"start\":39014},{\"end\":39202,\"start\":39195},{\"end\":39218,\"start\":39212},{\"end\":39398,\"start\":39394},{\"end\":39416,\"start\":39410},{\"end\":39430,\"start\":39423},{\"end\":39444,\"start\":39437},{\"end\":39652,\"start\":39647},{\"end\":39665,\"start\":39660},{\"end\":39682,\"start\":39677},{\"end\":39697,\"start\":39693},{\"end\":39901,\"start\":39900},{\"end\":39915,\"start\":39911},{\"end\":39923,\"start\":39922},{\"end\":40202,\"start\":40194},{\"end\":40213,\"start\":40208},{\"end\":40226,\"start\":40222},{\"end\":40476,\"start\":40472},{\"end\":40488,\"start\":40482},{\"end\":40499,\"start\":40493},{\"end\":40506,\"start\":40505},{\"end\":40521,\"start\":40518},{\"end\":40742,\"start\":40735},{\"end\":40754,\"start\":40748},{\"end\":40767,\"start\":40759},{\"end\":41052,\"start\":41044},{\"end\":41066,\"start\":41059},{\"end\":41081,\"start\":41072},{\"end\":41099,\"start\":41092},{\"end\":41383,\"start\":41376},{\"end\":41385,\"start\":41384},{\"end\":41402,\"start\":41396},{\"end\":41404,\"start\":41403},{\"end\":41547,\"start\":41543},{\"end\":41560,\"start\":41553},{\"end\":41575,\"start\":41568},{\"end\":41591,\"start\":41586},{\"end\":41608,\"start\":41600},{\"end\":41889,\"start\":41885},{\"end\":41908,\"start\":41901},{\"end\":41925,\"start\":41918},{\"end\":41945,\"start\":41936},{\"end\":41962,\"start\":41955},{\"end\":42218,\"start\":42215},{\"end\":42231,\"start\":42226},{\"end\":42253,\"start\":42249},{\"end\":42271,\"start\":42266},{\"end\":42285,\"start\":42279},{\"end\":42638,\"start\":42637},{\"end\":42640,\"start\":42639},{\"end\":42652,\"start\":42651},{\"end\":42661,\"start\":42660},{\"end\":42673,\"start\":42672},{\"end\":42686,\"start\":42685},{\"end\":42693,\"start\":42692},{\"end\":42695,\"start\":42694},{\"end\":42706,\"start\":42705},{\"end\":42714,\"start\":42713},{\"end\":42725,\"start\":42724},{\"end\":42735,\"start\":42734},{\"end\":43060,\"start\":43052},{\"end\":43077,\"start\":43070},{\"end\":43096,\"start\":43089},{\"end\":43108,\"start\":43104},{\"end\":43368,\"start\":43358},{\"end\":43380,\"start\":43375},{\"end\":43397,\"start\":43391},{\"end\":43413,\"start\":43406},{\"end\":43430,\"start\":43424},{\"end\":43710,\"start\":43705},{\"end\":43729,\"start\":43720},{\"end\":43745,\"start\":43739},{\"end\":44008,\"start\":44005},{\"end\":44025,\"start\":44019},{\"end\":44038,\"start\":44031},{\"end\":44313,\"start\":44305},{\"end\":44325,\"start\":44321},{\"end\":44338,\"start\":44333},{\"end\":44354,\"start\":44348},{\"end\":44372,\"start\":44366},{\"end\":44386,\"start\":44383},{\"end\":44664,\"start\":44656},{\"end\":44666,\"start\":44665},{\"end\":44687,\"start\":44680},{\"end\":44706,\"start\":44695},{\"end\":44718,\"start\":44714},{\"end\":45040,\"start\":45034},{\"end\":45056,\"start\":45049},{\"end\":45075,\"start\":45066},{\"end\":45086,\"start\":45082},{\"end\":45376,\"start\":45371},{\"end\":45389,\"start\":45386},{\"end\":45410,\"start\":45399},{\"end\":45424,\"start\":45417},{\"end\":45439,\"start\":45432},{\"end\":45457,\"start\":45451},{\"end\":45727,\"start\":45719},{\"end\":45735,\"start\":45734},{\"end\":45743,\"start\":45742},{\"end\":45932,\"start\":45924},{\"end\":45940,\"start\":45938},{\"end\":45954,\"start\":45947},{\"end\":45966,\"start\":45961},{\"end\":45982,\"start\":45978},{\"end\":45998,\"start\":45992},{\"end\":46013,\"start\":46007},{\"end\":46259,\"start\":46250},{\"end\":46270,\"start\":46266},{\"end\":46493,\"start\":46486},{\"end\":46507,\"start\":46500},{\"end\":46520,\"start\":46513},{\"end\":46532,\"start\":46527},{\"end\":46540,\"start\":46538},{\"end\":46552,\"start\":46546},{\"end\":46563,\"start\":46559},{\"end\":46844,\"start\":46837},{\"end\":46854,\"start\":46851},{\"end\":47087,\"start\":47079},{\"end\":47108,\"start\":47100},{\"end\":47120,\"start\":47115},{\"end\":47136,\"start\":47128},{\"end\":47148,\"start\":47144},{\"end\":47160,\"start\":47154},{\"end\":47180,\"start\":47174},{\"end\":47522,\"start\":47516},{\"end\":47539,\"start\":47533},{\"end\":47556,\"start\":47548},{\"end\":47558,\"start\":47557},{\"end\":47571,\"start\":47567},{\"end\":47583,\"start\":47580},{\"end\":47601,\"start\":47594},{\"end\":47619,\"start\":47614},{\"end\":47633,\"start\":47627},{\"end\":47647,\"start\":47643},{\"end\":47660,\"start\":47656},{\"end\":47677,\"start\":47673},{\"end\":47690,\"start\":47686},{\"end\":47711,\"start\":47700},{\"end\":47725,\"start\":47721},{\"end\":47738,\"start\":47733},{\"end\":47752,\"start\":47748},{\"end\":48273,\"start\":48267},{\"end\":48291,\"start\":48285},{\"end\":48494,\"start\":48487},{\"end\":48508,\"start\":48501},{\"end\":48729,\"start\":48723},{\"end\":48745,\"start\":48737},{\"end\":48763,\"start\":48759},{\"end\":48781,\"start\":48775},{\"end\":48783,\"start\":48782},{\"end\":49004,\"start\":48998},{\"end\":49020,\"start\":49012},{\"end\":49022,\"start\":49021},{\"end\":49040,\"start\":49036},{\"end\":49058,\"start\":49052},{\"end\":49060,\"start\":49059},{\"end\":49288,\"start\":49281},{\"end\":49296,\"start\":49295},{\"end\":49310,\"start\":49303},{\"end\":49319,\"start\":49318},{\"end\":49332,\"start\":49326},{\"end\":49340,\"start\":49339},{\"end\":49353,\"start\":49346},{\"end\":49641,\"start\":49634},{\"end\":49651,\"start\":49648},{\"end\":49664,\"start\":49657},{\"end\":49881,\"start\":49878},{\"end\":49892,\"start\":49887},{\"end\":49904,\"start\":49898},{\"end\":49913,\"start\":49909},{\"end\":49924,\"start\":49920},{\"end\":50179,\"start\":50175},{\"end\":50191,\"start\":50187},{\"end\":50204,\"start\":50200},{\"end\":50218,\"start\":50212},{\"end\":50231,\"start\":50226},{\"end\":50245,\"start\":50240},{\"end\":50258,\"start\":50253},{\"end\":50586,\"start\":50581},{\"end\":50599,\"start\":50591},{\"end\":50838,\"start\":50831},{\"end\":50853,\"start\":50846},{\"end\":50868,\"start\":50860},{\"end\":50887,\"start\":50876},{\"end\":51095,\"start\":51087},{\"end\":51110,\"start\":51102},{\"end\":51129,\"start\":51123}]", "bib_author_last_name": "[{\"end\":33914,\"start\":33908},{\"end\":33944,\"start\":33923},{\"end\":33955,\"start\":33946},{\"end\":34188,\"start\":34182},{\"end\":34204,\"start\":34198},{\"end\":34228,\"start\":34213},{\"end\":34238,\"start\":34234},{\"end\":34254,\"start\":34247},{\"end\":34262,\"start\":34256},{\"end\":34602,\"start\":34597},{\"end\":34614,\"start\":34612},{\"end\":34626,\"start\":34623},{\"end\":34637,\"start\":34635},{\"end\":34650,\"start\":34648},{\"end\":34668,\"start\":34657},{\"end\":34676,\"start\":34674},{\"end\":34969,\"start\":34958},{\"end\":34982,\"start\":34978},{\"end\":34995,\"start\":34993},{\"end\":35007,\"start\":35003},{\"end\":35020,\"start\":35016},{\"end\":35030,\"start\":35022},{\"end\":35307,\"start\":35302},{\"end\":35324,\"start\":35316},{\"end\":35339,\"start\":35336},{\"end\":35354,\"start\":35348},{\"end\":35680,\"start\":35673},{\"end\":35692,\"start\":35687},{\"end\":35919,\"start\":35916},{\"end\":35928,\"start\":35923},{\"end\":35943,\"start\":35938},{\"end\":35957,\"start\":35952},{\"end\":35972,\"start\":35966},{\"end\":35993,\"start\":35983},{\"end\":36002,\"start\":35995},{\"end\":36298,\"start\":36295},{\"end\":36316,\"start\":36310},{\"end\":36334,\"start\":36327},{\"end\":36621,\"start\":36618},{\"end\":36639,\"start\":36632},{\"end\":36658,\"start\":36649},{\"end\":36673,\"start\":36668},{\"end\":36693,\"start\":36685},{\"end\":36996,\"start\":36991},{\"end\":37015,\"start\":37008},{\"end\":37027,\"start\":37021},{\"end\":37304,\"start\":37302},{\"end\":37316,\"start\":37313},{\"end\":37326,\"start\":37323},{\"end\":37339,\"start\":37336},{\"end\":37584,\"start\":37581},{\"end\":37598,\"start\":37591},{\"end\":37610,\"start\":37605},{\"end\":37827,\"start\":37825},{\"end\":37841,\"start\":37837},{\"end\":37854,\"start\":37851},{\"end\":37867,\"start\":37862},{\"end\":38104,\"start\":38102},{\"end\":38118,\"start\":38114},{\"end\":38133,\"start\":38128},{\"end\":38144,\"start\":38141},{\"end\":38153,\"start\":38149},{\"end\":38413,\"start\":38411},{\"end\":38427,\"start\":38422},{\"end\":38441,\"start\":38438},{\"end\":38450,\"start\":38446},{\"end\":38690,\"start\":38685},{\"end\":38703,\"start\":38700},{\"end\":38719,\"start\":38712},{\"end\":38734,\"start\":38729},{\"end\":38752,\"start\":38745},{\"end\":38771,\"start\":38761},{\"end\":38996,\"start\":38993},{\"end\":39012,\"start\":39008},{\"end\":39028,\"start\":39023},{\"end\":39210,\"start\":39203},{\"end\":39224,\"start\":39219},{\"end\":39408,\"start\":39399},{\"end\":39421,\"start\":39417},{\"end\":39435,\"start\":39431},{\"end\":39451,\"start\":39445},{\"end\":39658,\"start\":39653},{\"end\":39675,\"start\":39666},{\"end\":39691,\"start\":39683},{\"end\":39707,\"start\":39698},{\"end\":39909,\"start\":39902},{\"end\":39920,\"start\":39916},{\"end\":39931,\"start\":39924},{\"end\":40206,\"start\":40203},{\"end\":40220,\"start\":40214},{\"end\":40235,\"start\":40227},{\"end\":40480,\"start\":40477},{\"end\":40491,\"start\":40489},{\"end\":40503,\"start\":40500},{\"end\":40516,\"start\":40507},{\"end\":40532,\"start\":40522},{\"end\":40539,\"start\":40534},{\"end\":40746,\"start\":40743},{\"end\":40757,\"start\":40755},{\"end\":40771,\"start\":40768},{\"end\":41057,\"start\":41053},{\"end\":41070,\"start\":41067},{\"end\":41090,\"start\":41082},{\"end\":41104,\"start\":41100},{\"end\":41394,\"start\":41386},{\"end\":41410,\"start\":41405},{\"end\":41551,\"start\":41548},{\"end\":41566,\"start\":41561},{\"end\":41584,\"start\":41576},{\"end\":41598,\"start\":41592},{\"end\":41613,\"start\":41609},{\"end\":41899,\"start\":41890},{\"end\":41916,\"start\":41909},{\"end\":41934,\"start\":41926},{\"end\":41953,\"start\":41946},{\"end\":41969,\"start\":41963},{\"end\":42224,\"start\":42219},{\"end\":42247,\"start\":42232},{\"end\":42264,\"start\":42254},{\"end\":42277,\"start\":42272},{\"end\":42300,\"start\":42286},{\"end\":42312,\"start\":42302},{\"end\":42649,\"start\":42641},{\"end\":42658,\"start\":42653},{\"end\":42670,\"start\":42662},{\"end\":42683,\"start\":42674},{\"end\":42690,\"start\":42687},{\"end\":42703,\"start\":42696},{\"end\":42711,\"start\":42707},{\"end\":42722,\"start\":42715},{\"end\":42732,\"start\":42726},{\"end\":42746,\"start\":42736},{\"end\":43068,\"start\":43061},{\"end\":43087,\"start\":43078},{\"end\":43102,\"start\":43097},{\"end\":43119,\"start\":43109},{\"end\":43373,\"start\":43369},{\"end\":43389,\"start\":43381},{\"end\":43404,\"start\":43398},{\"end\":43422,\"start\":43414},{\"end\":43440,\"start\":43431},{\"end\":43718,\"start\":43711},{\"end\":43737,\"start\":43730},{\"end\":43756,\"start\":43746},{\"end\":44017,\"start\":44009},{\"end\":44029,\"start\":44026},{\"end\":44042,\"start\":44039},{\"end\":44048,\"start\":44044},{\"end\":44319,\"start\":44314},{\"end\":44331,\"start\":44326},{\"end\":44346,\"start\":44339},{\"end\":44364,\"start\":44355},{\"end\":44381,\"start\":44373},{\"end\":44389,\"start\":44387},{\"end\":44678,\"start\":44667},{\"end\":44693,\"start\":44688},{\"end\":44712,\"start\":44707},{\"end\":44728,\"start\":44719},{\"end\":45047,\"start\":45041},{\"end\":45064,\"start\":45057},{\"end\":45080,\"start\":45076},{\"end\":45096,\"start\":45087},{\"end\":45384,\"start\":45377},{\"end\":45397,\"start\":45390},{\"end\":45415,\"start\":45411},{\"end\":45430,\"start\":45425},{\"end\":45449,\"start\":45440},{\"end\":45468,\"start\":45458},{\"end\":45732,\"start\":45728},{\"end\":45740,\"start\":45736},{\"end\":45750,\"start\":45744},{\"end\":45936,\"start\":45933},{\"end\":45945,\"start\":45941},{\"end\":45959,\"start\":45955},{\"end\":45976,\"start\":45967},{\"end\":45990,\"start\":45983},{\"end\":46005,\"start\":45999},{\"end\":46016,\"start\":46014},{\"end\":46264,\"start\":46260},{\"end\":46274,\"start\":46271},{\"end\":46498,\"start\":46494},{\"end\":46511,\"start\":46508},{\"end\":46525,\"start\":46521},{\"end\":46536,\"start\":46533},{\"end\":46544,\"start\":46541},{\"end\":46557,\"start\":46553},{\"end\":46567,\"start\":46564},{\"end\":46849,\"start\":46845},{\"end\":46859,\"start\":46855},{\"end\":47098,\"start\":47088},{\"end\":47113,\"start\":47109},{\"end\":47126,\"start\":47121},{\"end\":47142,\"start\":47137},{\"end\":47152,\"start\":47149},{\"end\":47172,\"start\":47161},{\"end\":47185,\"start\":47181},{\"end\":47531,\"start\":47523},{\"end\":47546,\"start\":47540},{\"end\":47565,\"start\":47559},{\"end\":47578,\"start\":47572},{\"end\":47592,\"start\":47584},{\"end\":47612,\"start\":47602},{\"end\":47625,\"start\":47620},{\"end\":47641,\"start\":47634},{\"end\":47654,\"start\":47648},{\"end\":47671,\"start\":47661},{\"end\":47684,\"start\":47678},{\"end\":47698,\"start\":47691},{\"end\":47719,\"start\":47712},{\"end\":47731,\"start\":47726},{\"end\":47746,\"start\":47739},{\"end\":47761,\"start\":47753},{\"end\":48283,\"start\":48274},{\"end\":48301,\"start\":48292},{\"end\":48499,\"start\":48495},{\"end\":48513,\"start\":48509},{\"end\":48735,\"start\":48730},{\"end\":48757,\"start\":48746},{\"end\":48773,\"start\":48764},{\"end\":48790,\"start\":48784},{\"end\":49010,\"start\":49005},{\"end\":49034,\"start\":49023},{\"end\":49050,\"start\":49041},{\"end\":49067,\"start\":49061},{\"end\":49293,\"start\":49289},{\"end\":49301,\"start\":49297},{\"end\":49316,\"start\":49311},{\"end\":49324,\"start\":49320},{\"end\":49337,\"start\":49333},{\"end\":49344,\"start\":49341},{\"end\":49359,\"start\":49354},{\"end\":49646,\"start\":49642},{\"end\":49655,\"start\":49652},{\"end\":49669,\"start\":49665},{\"end\":49885,\"start\":49882},{\"end\":49896,\"start\":49893},{\"end\":49907,\"start\":49905},{\"end\":49918,\"start\":49914},{\"end\":49929,\"start\":49925},{\"end\":50185,\"start\":50180},{\"end\":50198,\"start\":50192},{\"end\":50210,\"start\":50205},{\"end\":50224,\"start\":50219},{\"end\":50238,\"start\":50232},{\"end\":50251,\"start\":50246},{\"end\":50265,\"start\":50259},{\"end\":50589,\"start\":50587},{\"end\":50603,\"start\":50600},{\"end\":50844,\"start\":50839},{\"end\":50858,\"start\":50854},{\"end\":50874,\"start\":50869},{\"end\":50893,\"start\":50888},{\"end\":51100,\"start\":51096},{\"end\":51121,\"start\":51111},{\"end\":51134,\"start\":51130}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":33814,\"start\":33718},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":14450404},\"end\":34118,\"start\":33816},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":220713377},\"end\":34508,\"start\":34120},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":208310189},\"end\":34874,\"start\":34510},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":6325059},\"end\":35213,\"start\":34876},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":5201925},\"end\":35598,\"start\":35215},{\"attributes\":{\"id\":\"b6\"},\"end\":35844,\"start\":35600},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":7684883},\"end\":36192,\"start\":35846},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":208527159},\"end\":36507,\"start\":36194},{\"attributes\":{\"id\":\"b9\"},\"end\":36908,\"start\":36509},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":2255738},\"end\":37185,\"start\":36910},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":209370711},\"end\":37521,\"start\":37187},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":119313661},\"end\":37770,\"start\":37523},{\"attributes\":{\"id\":\"b13\"},\"end\":38023,\"start\":37772},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":9035204},\"end\":38317,\"start\":38025},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":218889671},\"end\":38626,\"start\":38319},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":214606025},\"end\":38944,\"start\":38628},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":19285959},\"end\":39152,\"start\":38946},{\"attributes\":{\"id\":\"b18\"},\"end\":39326,\"start\":39154},{\"attributes\":{\"id\":\"b19\"},\"end\":39605,\"start\":39328},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":286854},\"end\":39837,\"start\":39607},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":16774546},\"end\":40094,\"start\":39839},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":10716717},\"end\":40403,\"start\":40096},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":57759271},\"end\":40733,\"start\":40405},{\"attributes\":{\"id\":\"b24\"},\"end\":40975,\"start\":40735},{\"attributes\":{\"id\":\"b25\"},\"end\":41295,\"start\":40977},{\"attributes\":{\"id\":\"b26\"},\"end\":41539,\"start\":41297},{\"attributes\":{\"id\":\"b27\"},\"end\":41817,\"start\":41541},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":54465161},\"end\":42153,\"start\":41819},{\"attributes\":{\"id\":\"b29\"},\"end\":42575,\"start\":42155},{\"attributes\":{\"id\":\"b30\"},\"end\":42992,\"start\":42577},{\"attributes\":{\"id\":\"b31\"},\"end\":43275,\"start\":42994},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":58007025},\"end\":43635,\"start\":43277},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":9632354},\"end\":43911,\"start\":43637},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":57825758},\"end\":44217,\"start\":43913},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":152282359},\"end\":44593,\"start\":44219},{\"attributes\":{\"id\":\"b36\"},\"end\":44938,\"start\":44595},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":24541507},\"end\":45290,\"start\":44940},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":8632684},\"end\":45677,\"start\":45292},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":771247},\"end\":45858,\"start\":45679},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":51891697},\"end\":46206,\"start\":45860},{\"attributes\":{\"id\":\"b41\"},\"end\":46410,\"start\":46208},{\"attributes\":{\"id\":\"b42\"},\"end\":46768,\"start\":46412},{\"attributes\":{\"id\":\"b43\"},\"end\":47014,\"start\":46770},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":6159584},\"end\":47425,\"start\":47016},{\"attributes\":{\"id\":\"b45\"},\"end\":48195,\"start\":47427},{\"attributes\":{\"id\":\"b46\"},\"end\":48420,\"start\":48197},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":49901637},\"end\":48670,\"start\":48422},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":210165091},\"end\":48945,\"start\":48672},{\"attributes\":{\"id\":\"b49\"},\"end\":49208,\"start\":48947},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":221841701},\"end\":49551,\"start\":49210},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":22360616},\"end\":49816,\"start\":49553},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":4712004},\"end\":50091,\"start\":49818},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":225077557},\"end\":50480,\"start\":50093},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":214713824},\"end\":50766,\"start\":50482},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":3063100},\"end\":51049,\"start\":50768},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":51929808},\"end\":51247,\"start\":51051}]", "bib_title": "[{\"end\":33899,\"start\":33816},{\"end\":34173,\"start\":34120},{\"end\":34590,\"start\":34510},{\"end\":34954,\"start\":34876},{\"end\":35291,\"start\":35215},{\"end\":35907,\"start\":35846},{\"end\":36286,\"start\":36194},{\"end\":36983,\"start\":36910},{\"end\":37291,\"start\":37187},{\"end\":37573,\"start\":37523},{\"end\":38093,\"start\":38025},{\"end\":38402,\"start\":38319},{\"end\":38677,\"start\":38628},{\"end\":38982,\"start\":38946},{\"end\":39645,\"start\":39607},{\"end\":39898,\"start\":39839},{\"end\":40192,\"start\":40096},{\"end\":40470,\"start\":40405},{\"end\":41883,\"start\":41819},{\"end\":43356,\"start\":43277},{\"end\":43703,\"start\":43637},{\"end\":44003,\"start\":43913},{\"end\":44303,\"start\":44219},{\"end\":45032,\"start\":44940},{\"end\":45369,\"start\":45292},{\"end\":45717,\"start\":45679},{\"end\":45922,\"start\":45860},{\"end\":47077,\"start\":47016},{\"end\":48485,\"start\":48422},{\"end\":48721,\"start\":48672},{\"end\":49279,\"start\":49210},{\"end\":49632,\"start\":49553},{\"end\":49876,\"start\":49818},{\"end\":50173,\"start\":50093},{\"end\":50579,\"start\":50482},{\"end\":50829,\"start\":50768},{\"end\":51085,\"start\":51051}]", "bib_author": "[{\"end\":33916,\"start\":33901},{\"end\":33946,\"start\":33916},{\"end\":33957,\"start\":33946},{\"end\":34190,\"start\":34175},{\"end\":34206,\"start\":34190},{\"end\":34230,\"start\":34206},{\"end\":34240,\"start\":34230},{\"end\":34256,\"start\":34240},{\"end\":34264,\"start\":34256},{\"end\":34604,\"start\":34592},{\"end\":34616,\"start\":34604},{\"end\":34628,\"start\":34616},{\"end\":34639,\"start\":34628},{\"end\":34652,\"start\":34639},{\"end\":34670,\"start\":34652},{\"end\":34678,\"start\":34670},{\"end\":34971,\"start\":34956},{\"end\":34984,\"start\":34971},{\"end\":34997,\"start\":34984},{\"end\":35009,\"start\":34997},{\"end\":35022,\"start\":35009},{\"end\":35032,\"start\":35022},{\"end\":35309,\"start\":35293},{\"end\":35326,\"start\":35309},{\"end\":35341,\"start\":35326},{\"end\":35356,\"start\":35341},{\"end\":35682,\"start\":35667},{\"end\":35694,\"start\":35682},{\"end\":35921,\"start\":35909},{\"end\":35930,\"start\":35921},{\"end\":35945,\"start\":35930},{\"end\":35959,\"start\":35945},{\"end\":35974,\"start\":35959},{\"end\":35995,\"start\":35974},{\"end\":36004,\"start\":35995},{\"end\":36300,\"start\":36288},{\"end\":36318,\"start\":36300},{\"end\":36336,\"start\":36318},{\"end\":36623,\"start\":36611},{\"end\":36641,\"start\":36623},{\"end\":36660,\"start\":36641},{\"end\":36675,\"start\":36660},{\"end\":36695,\"start\":36675},{\"end\":36998,\"start\":36985},{\"end\":37017,\"start\":36998},{\"end\":37029,\"start\":37017},{\"end\":37306,\"start\":37293},{\"end\":37318,\"start\":37306},{\"end\":37328,\"start\":37318},{\"end\":37341,\"start\":37328},{\"end\":37586,\"start\":37575},{\"end\":37600,\"start\":37586},{\"end\":37612,\"start\":37600},{\"end\":37829,\"start\":37816},{\"end\":37843,\"start\":37829},{\"end\":37856,\"start\":37843},{\"end\":37869,\"start\":37856},{\"end\":38106,\"start\":38095},{\"end\":38120,\"start\":38106},{\"end\":38135,\"start\":38120},{\"end\":38146,\"start\":38135},{\"end\":38155,\"start\":38146},{\"end\":38415,\"start\":38404},{\"end\":38429,\"start\":38415},{\"end\":38443,\"start\":38429},{\"end\":38452,\"start\":38443},{\"end\":38692,\"start\":38679},{\"end\":38705,\"start\":38692},{\"end\":38721,\"start\":38705},{\"end\":38736,\"start\":38721},{\"end\":38754,\"start\":38736},{\"end\":38773,\"start\":38754},{\"end\":38998,\"start\":38984},{\"end\":39014,\"start\":38998},{\"end\":39030,\"start\":39014},{\"end\":39212,\"start\":39195},{\"end\":39226,\"start\":39212},{\"end\":39410,\"start\":39394},{\"end\":39423,\"start\":39410},{\"end\":39437,\"start\":39423},{\"end\":39453,\"start\":39437},{\"end\":39660,\"start\":39647},{\"end\":39677,\"start\":39660},{\"end\":39693,\"start\":39677},{\"end\":39709,\"start\":39693},{\"end\":39911,\"start\":39900},{\"end\":39922,\"start\":39911},{\"end\":39933,\"start\":39922},{\"end\":40208,\"start\":40194},{\"end\":40222,\"start\":40208},{\"end\":40237,\"start\":40222},{\"end\":40482,\"start\":40472},{\"end\":40493,\"start\":40482},{\"end\":40505,\"start\":40493},{\"end\":40518,\"start\":40505},{\"end\":40534,\"start\":40518},{\"end\":40541,\"start\":40534},{\"end\":40748,\"start\":40735},{\"end\":40759,\"start\":40748},{\"end\":40773,\"start\":40759},{\"end\":41059,\"start\":41044},{\"end\":41072,\"start\":41059},{\"end\":41092,\"start\":41072},{\"end\":41106,\"start\":41092},{\"end\":41396,\"start\":41376},{\"end\":41412,\"start\":41396},{\"end\":41553,\"start\":41543},{\"end\":41568,\"start\":41553},{\"end\":41586,\"start\":41568},{\"end\":41600,\"start\":41586},{\"end\":41615,\"start\":41600},{\"end\":41901,\"start\":41885},{\"end\":41918,\"start\":41901},{\"end\":41936,\"start\":41918},{\"end\":41955,\"start\":41936},{\"end\":41971,\"start\":41955},{\"end\":42226,\"start\":42215},{\"end\":42249,\"start\":42226},{\"end\":42266,\"start\":42249},{\"end\":42279,\"start\":42266},{\"end\":42302,\"start\":42279},{\"end\":42314,\"start\":42302},{\"end\":42651,\"start\":42637},{\"end\":42660,\"start\":42651},{\"end\":42672,\"start\":42660},{\"end\":42685,\"start\":42672},{\"end\":42692,\"start\":42685},{\"end\":42705,\"start\":42692},{\"end\":42713,\"start\":42705},{\"end\":42724,\"start\":42713},{\"end\":42734,\"start\":42724},{\"end\":42748,\"start\":42734},{\"end\":43070,\"start\":43052},{\"end\":43089,\"start\":43070},{\"end\":43104,\"start\":43089},{\"end\":43121,\"start\":43104},{\"end\":43375,\"start\":43358},{\"end\":43391,\"start\":43375},{\"end\":43406,\"start\":43391},{\"end\":43424,\"start\":43406},{\"end\":43442,\"start\":43424},{\"end\":43720,\"start\":43705},{\"end\":43739,\"start\":43720},{\"end\":43758,\"start\":43739},{\"end\":44019,\"start\":44005},{\"end\":44031,\"start\":44019},{\"end\":44044,\"start\":44031},{\"end\":44050,\"start\":44044},{\"end\":44321,\"start\":44305},{\"end\":44333,\"start\":44321},{\"end\":44348,\"start\":44333},{\"end\":44366,\"start\":44348},{\"end\":44383,\"start\":44366},{\"end\":44391,\"start\":44383},{\"end\":44680,\"start\":44656},{\"end\":44695,\"start\":44680},{\"end\":44714,\"start\":44695},{\"end\":44730,\"start\":44714},{\"end\":45049,\"start\":45034},{\"end\":45066,\"start\":45049},{\"end\":45082,\"start\":45066},{\"end\":45098,\"start\":45082},{\"end\":45386,\"start\":45371},{\"end\":45399,\"start\":45386},{\"end\":45417,\"start\":45399},{\"end\":45432,\"start\":45417},{\"end\":45451,\"start\":45432},{\"end\":45470,\"start\":45451},{\"end\":45734,\"start\":45719},{\"end\":45742,\"start\":45734},{\"end\":45752,\"start\":45742},{\"end\":45938,\"start\":45924},{\"end\":45947,\"start\":45938},{\"end\":45961,\"start\":45947},{\"end\":45978,\"start\":45961},{\"end\":45992,\"start\":45978},{\"end\":46007,\"start\":45992},{\"end\":46018,\"start\":46007},{\"end\":46266,\"start\":46250},{\"end\":46276,\"start\":46266},{\"end\":46500,\"start\":46486},{\"end\":46513,\"start\":46500},{\"end\":46527,\"start\":46513},{\"end\":46538,\"start\":46527},{\"end\":46546,\"start\":46538},{\"end\":46559,\"start\":46546},{\"end\":46569,\"start\":46559},{\"end\":46851,\"start\":46837},{\"end\":46861,\"start\":46851},{\"end\":47100,\"start\":47079},{\"end\":47115,\"start\":47100},{\"end\":47128,\"start\":47115},{\"end\":47144,\"start\":47128},{\"end\":47154,\"start\":47144},{\"end\":47174,\"start\":47154},{\"end\":47187,\"start\":47174},{\"end\":47533,\"start\":47516},{\"end\":47548,\"start\":47533},{\"end\":47567,\"start\":47548},{\"end\":47580,\"start\":47567},{\"end\":47594,\"start\":47580},{\"end\":47614,\"start\":47594},{\"end\":47627,\"start\":47614},{\"end\":47643,\"start\":47627},{\"end\":47656,\"start\":47643},{\"end\":47673,\"start\":47656},{\"end\":47686,\"start\":47673},{\"end\":47700,\"start\":47686},{\"end\":47721,\"start\":47700},{\"end\":47733,\"start\":47721},{\"end\":47748,\"start\":47733},{\"end\":47763,\"start\":47748},{\"end\":48285,\"start\":48267},{\"end\":48303,\"start\":48285},{\"end\":48501,\"start\":48487},{\"end\":48515,\"start\":48501},{\"end\":48737,\"start\":48723},{\"end\":48759,\"start\":48737},{\"end\":48775,\"start\":48759},{\"end\":48792,\"start\":48775},{\"end\":49012,\"start\":48998},{\"end\":49036,\"start\":49012},{\"end\":49052,\"start\":49036},{\"end\":49069,\"start\":49052},{\"end\":49295,\"start\":49281},{\"end\":49303,\"start\":49295},{\"end\":49318,\"start\":49303},{\"end\":49326,\"start\":49318},{\"end\":49339,\"start\":49326},{\"end\":49346,\"start\":49339},{\"end\":49361,\"start\":49346},{\"end\":49648,\"start\":49634},{\"end\":49657,\"start\":49648},{\"end\":49671,\"start\":49657},{\"end\":49887,\"start\":49878},{\"end\":49898,\"start\":49887},{\"end\":49909,\"start\":49898},{\"end\":49920,\"start\":49909},{\"end\":49931,\"start\":49920},{\"end\":50187,\"start\":50175},{\"end\":50200,\"start\":50187},{\"end\":50212,\"start\":50200},{\"end\":50226,\"start\":50212},{\"end\":50240,\"start\":50226},{\"end\":50253,\"start\":50240},{\"end\":50267,\"start\":50253},{\"end\":50591,\"start\":50581},{\"end\":50605,\"start\":50591},{\"end\":50846,\"start\":50831},{\"end\":50860,\"start\":50846},{\"end\":50876,\"start\":50860},{\"end\":50895,\"start\":50876},{\"end\":51102,\"start\":51087},{\"end\":51123,\"start\":51102},{\"end\":51136,\"start\":51123}]", "bib_venue": "[{\"end\":33764,\"start\":33720},{\"end\":33961,\"start\":33957},{\"end\":34305,\"start\":34264},{\"end\":34682,\"start\":34678},{\"end\":35036,\"start\":35032},{\"end\":35394,\"start\":35356},{\"end\":35665,\"start\":35600},{\"end\":36008,\"start\":36004},{\"end\":36340,\"start\":36336},{\"end\":36609,\"start\":36509},{\"end\":37036,\"start\":37029},{\"end\":37345,\"start\":37341},{\"end\":37616,\"start\":37612},{\"end\":37814,\"start\":37772},{\"end\":38159,\"start\":38155},{\"end\":38462,\"start\":38452},{\"end\":38777,\"start\":38773},{\"end\":39037,\"start\":39030},{\"end\":39193,\"start\":39154},{\"end\":39392,\"start\":39328},{\"end\":39713,\"start\":39709},{\"end\":39956,\"start\":39933},{\"end\":40241,\"start\":40237},{\"end\":40545,\"start\":40541},{\"end\":40838,\"start\":40773},{\"end\":41042,\"start\":40977},{\"end\":41374,\"start\":41297},{\"end\":41975,\"start\":41971},{\"end\":42213,\"start\":42155},{\"end\":42635,\"start\":42577},{\"end\":43050,\"start\":42994},{\"end\":43446,\"start\":43442},{\"end\":43762,\"start\":43758},{\"end\":44055,\"start\":44050},{\"end\":44395,\"start\":44391},{\"end\":44654,\"start\":44595},{\"end\":45101,\"start\":45098},{\"end\":45474,\"start\":45470},{\"end\":45756,\"start\":45752},{\"end\":46022,\"start\":46018},{\"end\":46248,\"start\":46208},{\"end\":46484,\"start\":46412},{\"end\":46835,\"start\":46770},{\"end\":47197,\"start\":47187},{\"end\":47514,\"start\":47427},{\"end\":48265,\"start\":48197},{\"end\":48518,\"start\":48515},{\"end\":48802,\"start\":48792},{\"end\":48996,\"start\":48947},{\"end\":49370,\"start\":49361},{\"end\":49675,\"start\":49671},{\"end\":49935,\"start\":49931},{\"end\":50274,\"start\":50267},{\"end\":50615,\"start\":50605},{\"end\":50899,\"start\":50895},{\"end\":51140,\"start\":51136}]"}}}, "year": 2023, "month": 12, "day": 17}