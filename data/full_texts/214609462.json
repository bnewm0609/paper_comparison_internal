{"id": 214609462, "updated": "2023-10-06 18:24:17.724", "metadata": {"title": "Learning from THEODORE: A Synthetic Omnidirectional Top-View Indoor Dataset for Deep Transfer Learning", "authors": "[{\"first\":\"Tobias\",\"last\":\"Scheck\",\"middle\":[]},{\"first\":\"Roman\",\"last\":\"Seidel\",\"middle\":[]},{\"first\":\"Gangolf\",\"last\":\"Hirtz\",\"middle\":[]}]", "venue": "2020 IEEE Winter Conference on Applications of Computer Vision (WACV)", "journal": "2020 IEEE Winter Conference on Applications of Computer Vision (WACV)", "publication_date": {"year": 2020, "month": 11, "day": 11}, "abstract": "Recent work about synthetic indoor datasets from perspective views has shown significant improvements of object detection results with Convolutional Neural Networks(CNNs). In this paper, we introduce THEODORE: a novel, large-scale indoor dataset containing 100,000 high-resolution diversified fisheye images with 14 classes. To this end, we create 3D virtual environments of living rooms, different human characters and interior textures. Beside capturing fisheye images from virtual environments we create annotations for semantic segmentation, instance masks and bounding boxes for object detection tasks. We compare our synthetic dataset to state of the art real-world datasets for omnidirectional images. Based on MS COCO weights, we show that our dataset is well suited for fine-tuning CNNs for object detection. Through a high generalization of our models by means of image synthesis and domain randomization, we reach an AP up to 0.84 for class person on High-Definition Analytics dataset.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2011.05719", "mag": "3100669159", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2011-05719", "doi": "10.1109/wacv45572.2020.9093563"}}, "content": {"source": {"pdf_hash": "9b7a248fe239f50fb857c2a870c87765aef137e7", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2011.05719v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2011.05719", "status": "GREEN"}}, "grobid": {"id": "6c2045f2191ead0c47a3c28fea15b145f8188075", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/9b7a248fe239f50fb857c2a870c87765aef137e7.txt", "contents": "\nLearning from THEODORE: A Synthetic Omnidirectional Top-View Indoor Dataset for Deep Transfer Learning\n\n\nTobias Scheck tobias.scheck@etit.tu-chemnitz.de \nFaculty of Electrical Engineering and Information Technology\nChemnitz University of Technology\n09126ChemnitzGermany\n\nRoman Seidel roman.seidel@etit.tu-chemnitz.de \nFaculty of Electrical Engineering and Information Technology\nChemnitz University of Technology\n09126ChemnitzGermany\n\nGangolf Hirtz g.hirtz@etit.tu-chemnitz.de \nFaculty of Electrical Engineering and Information Technology\nChemnitz University of Technology\n09126ChemnitzGermany\n\nLearning from THEODORE: A Synthetic Omnidirectional Top-View Indoor Dataset for Deep Transfer Learning\n\nRecent work about synthetic indoor datasets from perspective views has shown significant improvements of object detection results with Convolutional Neural Networks (CNNs). In this paper, we introduce THEODORE: a novel, large-scale indoor dataset containing 100,000 highresolution diversified fisheye images with 16 classes. To this end, we create 3D virtual environments of living rooms, different human characters and interior textures. Beside capturing fisheye images from virtual environments we create annotations for semantic segmentation, instance masks and bounding boxes for object detection. We compare our synthetic dataset to state of the art real-world datasets for omnidirectional images. Based on MS COCO weights, we show that our dataset is well suited for fine-tuning CNNs for object detection and semantic segmentation. Through a high generalization of our models by means of image synthesis and domain randomization we reach a AP up to 0.90 for class person on our own annotated fisheye evaluation suite (FES). Additionally, the evaluation of six classes was done through object detection and semantic segmentation on FES. The segmentation task on FES leads to 0.36 mIoU on all classes and to a mAP of 0.61 for the object detection.\n\nIntroduction\n\nSynthetic images and labels from modeled 3D indoor scenes has been an increasing research field in computer vision in the last few years. In contrast to manually labeled indoor front-view perspective images for action recognition [28,37,39] that are widely explored, image data from topview indoor scenes of omnidirectional images are rarely available. Invariance against the perspective of objects, e.g. missing images from top-view scenes makes common * Authors contributed equally datasets not adaptable to computer vision tasks on omnidirectional cameras. The most widely used projection of fisheye images is the equirectangular camera model, where all image points are mapped to the inside of a lower half sphere through elevation and azimuth. This projection formulates the distortion of omnidirectional images and leads to a high variation of the shape of objects depending on their location in the image.\n\nIn this paper we introduce THEODORE -a synTHEtic tOp-view inDoOR scEnes dataset that contains diversified rendered fisheye images of indoor environments with instance segmentation masks and bounding boxes. The indoor world was created with the game engine Unity3D and rendered images were captured with a camera that follows the omnidirectional projection. To bridge the gap between real-world and synthetic images we perform domain randomization with different rooms, persons, objects and camera positions. With THEODORE we release a dataset that improves the accuracy of state-of-the-art CNNs on omnidirectional images in indoor environments. A few application fields are navigation of autonomous systems through visual odometry, personal security in public transportation services or in virtual reality. We expect a strong growth of the research field on omnidirectional images in computer vision.\n\nOur contribution is twofold:\n\n\u2022 Generating THEODORE: a dataset with diversified omnidirectional images and labels for indoor scenes \u2022 Improvement of accuracy of state-of-the-art CNNs for object detection in omnidirectional images\n\nThe paper is structured as follows. Following this introduction in chapter 2 we treat related works to synthetic image data and one-and two-stage object detection. In chapter 3 we describe the data generation process and its properties. In chapter 4 we describe the behaviour of state of the art single-and two-stage object de-tectors in terms of THEODORE. The evaluation on publicly available databases for omnidirectional images is shown in Chapter 5. We summarize our results and give future research directions in Chapter 6. Our dataset can be found at https://www.tu-chemnitz.de/etit/ dst/forschung/comp_vision/theodore.\n\n\nRelated Work\n\nSynthetic data for omnidirectional images isn't well explored and data for training CNNs are sparsely available. In this section the most relevant indoor datasets and CNN architectures for object detection are introduced. Synthetic Data Synthetic data of persons in perspective views was widely studied [47] for tasks like object detection, segmentation or human pose estimation [21,22]. For the analysis of multi-object tracking Gaidon et al. created the Virtual KITTI dataset [16], including different environment conditions, camera position and instance-level segmentation ground truth. A couple of 3D model repositories for indoor scenes [18,25] in perspective views with focus on depth, physical based rendering and volumetric ground truth, namely the SUNCG dataset [41] and the Matterport dataset [6], were released. Based on these datasets the work of [42] generates a RGB-D panorama dataset for different camera configurations, but without different camera models and top-view images. While multisensory models for goal-directed navigation in complex indoor environments from ego-perspective MINOS [38] was published, extensive research in terms of semantic descriptions, acoustics and multi agent support from 3D visual renderings led to HoME [5]. With the goal to create houshold activities in virtual homes the work of [33] delivers instance and semantic label annotation, depth, pose and optical flow. The novelty of this approach is the formulation of the automatic generation of program episodes from text and creatable avatar videos. Our approach differs from this work in terms of camera geometry, domain randomization and viewing angle. House3D [48] provides 3D scenes of visually realistic houses that are equipped with a diverse set of fully labeled 3D objects and textures based on the SUNCG dataset including RGB images, depth, segmentation masks and topdown 2D map views. In terms of selecting the viewing angle of indoor scenes automatically, the work of [17] uses per class statistics to find the best viewing angle for semantic segmentation.\n\nExisting datasets of omnidirectional image data ( [7,11,12,13,15]) have low in-class variance, missing ground truth labels or contain less variations of scenes [26]. Object Detection One-stage object detectors [29,35] that treat object detection as a simple regression task learn class probabilities and bounding box coordinates. Two-stage detectors such as [36] and [10] generate regions of interest (ROIs) by a Region Proposal Network in the first and for-ward these ROIs to the object classification and bounding box regression pipeline. Object detection in distorted fisheye images is not widely explored. The authors of [9] and [8] adapt the network architecture of CNNs to spherical representations of the regular convolution operations. However, [9] wraps the sampled locations of convolutional filters to the sphere and effectively reverses the distortions of the omnidirectional camera model. [8] avoids translational weight sharing and creates building blocks that satisfy a generalized Fourier theorem, to detect patterns independently from their location on the sphere.\n\nCurrent frameworks with AI agents ( [23,48]) concentrate on embodied question answering or navigation (Point-Goal, ObjectGoal and RoomGoal). Images and corresponding labels (segmentation masks, surface normals, object IDs, depth) can be created, but are missing for omnidirectional camera model. In contrast to our work, viewing angle of AI agent frameworks is front-view from an egoperspective.\n\nTaylor et al. presents with a virtual worlds environment the possibility to create foreground masks, bounding boxes and target centroids in top-view omnidirectional images [44].\n\n\nTHEODORE Dataset\n\nIn this section all relevant steps for the generation of our synthetic dataset THEODORE are presented. We show the properties of the dataset in terms of distribution and variations of viewing angle.\n\n\nData Generation\n\nGame Engine An advantage of the usage of a game engine, compared to rendering software like Blender, is the opportunity to generate data in a less time-consuming manner. In this work we are generating synthetic data using the game engine Unity3D. To configure the walking path of the characters in the virtual environment, Unity3D provides a NavMesh component that allows avoiding obstacles by approximating the walkable areas.\n\nThe generated virtual environments consist of indoor scenes, where typical objects like tables, sofas and chairs are placed at fixed positions. Human 3D characters are generated using the Skinned Multi-Person Linear Model (SMPL) [30]. SMPL is a model of the human body with focus of realism based on thousands of 3D body scans. Human characters are able to move randomly in the area determined to be valid by NavMesh. Each character moves from a random start position to a randomly selected object position as destination.\n\nWe need to capture the whole scene from an omnidirectional camera placed on the ceiling of the room. However, Unity3D only provides a camera model for perspective and  orthographic projection. This limitation can be overcome by combining four perspective cameras in order to generate an omnidirectional image as described in the following.\n\nFisheye Projection Real omnidirectional images can be obtained by using fisheye lenses which results in a barrel distortion. Inside Unity3D fisheye images can be generated using the approach described by Bourke et al. [3,4].\n\nThis method is based on a modified cube map rendering (see Figure 1) using 4 of the cube faces to form a fisheye distorted image. Each face is the result of a rendered image captured by a camera with a field of view of 90 \u2022 . As shown in Figure 2, the final result is created by warping and combining these images on four meshes, whose texture coordinates model a fisheye projection. Afterwards the generated and distorted fisheye image is captured with an orthographic camera and rendered to the display.\n\nFor THEODORE we are using a resolution of 1024 \u00d7 1024 pixels which allows us, in combination with a native plugin, to reach an output of 15 FPS on an Intel i7-7700 and a Nvidia GTX 1080. The native plugin allows us to manage the transfer of the textures from GPU to the CPU memory in a faster way than the conventional methods available in Unity3D. Image Synthesis In addition to a rendered image we are generating segmentation-and instance masks. This is done by cloning the virtual omnidirectional camera setup and replacing the assigned shaders of each object with a unique color shader. In the case of segmentation, the colors are assigned according to the object classes. For instance masks the color is selected based on the unique object ID. With these modifications, the approach presented in [1] fits the previously described fisheye projection. In this case the shader replacement is performed for all four perspective cameras before generating the final segmentation and instance masks. Domain Randomization An approach for bridging the gap between synthetic and real images is domain randomization [45,46] that we also apply in our implementation. Every room changes after 25 seconds, which we call a level change. With each level change a new room is selected and object textures are randomly replaced. Furthermore, human characters are generated with random parameters (like height or weight) and textures, using the texture set from [47]. However, the replacement occurs inside a predefined texture set (e.g. wood, concrete, cotton, etc.), to prevent inappropriate texture assignments. Additionally the camera position is changed over time in order to create different points of view. The trajectory of the camera follows a Lissajous curve. Light sources are defined as point lights with a fixed range and intensity. The number of enabled light sources in each room is selected to ensure a well illuminated scene. To create different lighting situations with each level change some randomly selected light sources are disabled, however with the restriction that at least one light source remains active. Post Processing The final image, the segmentation and instance masks are combined in order to extract the necessary bounding boxes for the CNN training. By segmenting per color on the instance mask, a binary mask for each object  is generated. Then these masks are applied on the segmentation mask to identify each object with its corresponding label and the bounding box coordinates x min , x max , y min and y max are calculated. Finally the fisheye images together with the extracted bounding boxes and their corresponding labels are used to perform a conversion into common dataset formats (e.g. TFRecords [20] or PASCAL VOC2012 [14]).\n\n\nDataset Analysis\n\nThe creation pipeline of our synthetic omnidirectional data is visualized in Figure 1. Apart from the final image we extract the segmentation and label mask from the rendering process. Based on these masks we are able to select specific objects to calculate the bounding box. For THEODORE we have exported 100k images and bounding boxes for the classes person, chair, table, armchair, wheeled walker, tv generated. The dataset contains different rooms with randomly selected textures, as described in section 3. For this approach we downloaded and categorized 120 textures 1 , so that each textured 3D object can theoretically choose one of them. We recorded the scene with 8 frames per second. In combination with a level change parameter of 25 seconds and the texture randomization, the dataset contains about 500 various textured indoor scenes.\n\nAn example for three randomized men and women with varying body shape and height, wearing different clothes and additional attributes of our simulation is depicted in Figure 2. The amount of instances per class is visualized in Figure 3(a) and the statistical distribution from the center point of persons bounding box in Figure 3(b). Through camera movement and random selection of destinations for a person we ensure well distributed positions over all fisheye images.\n\n\nApproach\n\nIn this section we describe the functionality of three meta-architectures of CNNs for object detection and two semantic segmentation networks and show the corresponding training setup. We train the architectures with our synthetic data using an open source framework for object detection [20] and a own implementation for the segmentation task. For object detection task we choose one-and two-stage object detectors and for segmentation we use pixel-wise classifiers as following described:\n\nFaster R-CNN The Faster R-CNN architecture uses two stages for the detection. The first stage, the region proposal network (RPN), is used to predict and extract box proposals. For this stage a feature extractor is used to extract the features of an image at various intermediate maps. In the second stage proposed boxes are cropped from an intermediate feature map and fed to the remainder of the feature extractor to refine and predict classes of the box proposals. As feature extractor we use ResNet50 [19]. R-FCN R-FCN is similar to R-CNN. The difference is in the cropping approach. An R-FCN only crops the result of the last layer while a Faster R-CNN crops the features from layers where the region proposal is predicted. This reduces the pro-region computation because cropping happens only at the end of the network and results in a faster run time. Here, ResNet101 [19] is used as feature extractor. Single Shot Detector The Single Shot Detector (SSD) uses a single feed-forward convolutional network to predict box anchors and classes directly without using a second stage per-proposal classification. The final detections are the results of a non-maximum suppression step applied to the prior predictions. For our approach we use the feature pyramid network (FPN) [27] implementation of ResNet50. SegNet SegNet [2] is a CNN architecture for semantic pixel-wise segmentation. It consists of an encoder network which topology is identical to VGG-16 network [40]. However, fully connected layers were removed to improve the size of the network and the training process. The decoder network of SegNet restores the gradually reduced spatial dimension from the encoder network. To realise this, SegNet uses pooling indices of the corresponding encoder for a non-linear upsampling. PSPNet Pyramid Scene Parsing Network (PSPNet) [49] is a scene parsing framework that uses a pyramid pooling module to aggregate different regional contexts. This modules is appended to an pre-trained ResNet network, in our case a ResNet101. In addition, ResNet was modified to use dilated convolutions to enlarge the field of view. Training All selected object detection networks are pretrained on MS COCO [28]. As configuration for each architecture we use the proposed settings from the framework [20]. Adjustments are made on the training settings. For all experiments, a value of 0.9 for the momentum optimizer [34] is selected. We apply cosine decay [31] as learning rate strategy for the SSD meta architecture. As parameters we select a learning rate and warmup learning rate of 3e\u22125 over 20, 000 steps. The training for the R-FCN and Faster R-CNN is manually stopped if the performance on the validation set begins to saturate. As learning rate strategy we reduce the learning rate by a factor of 10 every 20, 000 steps. The input dimensions for all networks change to a 3-channel RGB image with a fixed resolution of 640 \u00d7 640 pixels and batch size is set to 16. For a better generalization of the fine-tuned model data augmentation methods are applied ( [24,43,45]). We select random brightness, random contrast, random crop, random Gaussian noise and horizontal flip for all meta-architectures during the training. For the semantic segmentation approach we use pre-trained ImageNet weights for PSPNet to fine-tune the architectures. SegNet is trained from scratch without the usage of pre-trained weights. As in our object detection setup we use the momentum optimizer with an momentum of 0.9 and a learning rate of 0.001. Furthermore, the training uses a batch size of 4 and is done for 150 epochs. The data augmentation methods random noise, horizontal flip and brightness are applied during training.\n\n\nEvaluation\n\nWith fine-tuning of CNNs with THEODORE we evaluate on labeled real world images by meta-architectures for object detection and segmentation as described in section 4. We choose publicly available real world datasets such as High-Definition Analytics (HDA) [32] and Bogazi\u00e7i University Multi-Omnidirectional (Bomni) [12] for object detection and an own annotated dataset Fisheye Evaluation Suite (FES) for semantic segmentation (see subsection 5.2) to validate the meta-architectures fine-tuned on THEODORE.\n\nAs metric for evaluation the average precision (AP) [14] per class and mean average precision (mAP) is reported for all classes. Detections will be judged to be true positive, if the intersection over union (IoU) between the detected and ground truth bounding box is at least 0.5. Our evaluation results shows exemplary bounding box detections in the first row of Figure 5. For the evaluation on semantic segmentation we choose the mean intersection over union (mIoU) for the classes armchair, chair, person, table, tv and wheeled walker. Exemplary results for semantic segmentation can be found in the second row of Figure 5.\n\n\nNumber of images\n\nIn order to evaluate the amount of images that are relevant for THEODORE, we measured the mAP for the SSD meta-architecture validated on FES and summarize the results in Figure 6. From the generated images we choose 12k, 25k, 50k and 100k images and train the SSD for 20, 000 steps each. With the parameters described in section 3 the 100k images contain 500 differently textured scenes in the training set.\n\nIn general, we carried out two approaches to reduce the number of images. First, we sub-sample the 100k images to keep the number of differently textured scenes constant. Second, the number of images is halved and consequently the number of textured scenes. We observe that a increas-\n\n\nObject Detection Segmentation\n\n\nPerson\n\nChair Table  Armchair TV Wheeled Walker ing number of images not necessarily leads to better results. The sub-sampled approach with 25k images results to the highest mAP. This experiment shows that the number of scene variations have a higher impact on mAP than the absolute number of images for training. For further experiments we use a subset of THEODORE with 25k images and 500 scenes.\n\n\nObject Detection\n\nIn this section we describe the evaluation of THEODORE on three real-world datasets, the HDA dataset, Bomni and FES. The HDA and Bomni dataset only contains labels for the person class, so the evaluation was done on person class, as long as there are no other classes in the datasets publicly available. The evaluation on our own dataset (FES) was done on six classes. Validation on HDA The HDA dataset [32] contains images captured with multiple cameras. The dataset was created for the research on high-definition surveillance. For our evaluation we use the 1388 labeled images from Cam 02. These images, with a resolution of 640 \u00d7 480 pixel were captured at 5 Hz from the top-view position with a full 140 \u2022 field of view. The images of the HDA dataset are barrel distorted, which makes them more comparable to omnidirectional images. Validation on Bomni Bomni Video Tracking Database contains video frames with a resolution of 640 \u00d7 480 pixel from an omnidirectional camera in a single room. The dataset was created in the context of human tracking and action recognition. For our evaluation we use all frames from top-view cameras of scenario #1 and crop them to a resolution of 480 \u00d7 480 pixel to remove most of the black borders. Validation on FES To the best of our knowledge FES is the first dataset with real world fisheye top-view images. The dataset contains of 301 images and six class labels (person, armchair, chair, table, tv and wheeled walker) which were annotated manually. All images have a resolution of 1680 \u00d7 1680 pixel with overlapping persons. The images of the dataset, segmentation masks and bounding boxes are available at https://www.tu-chemnitz. de/etit/dst/forschung/comp_vision/fes. For the evaluation of THEODORE we report the AP for person class for the HDA, Bomni and FES datasets in Table 1. As baseline, we choose MS COCO in the left three columns, while the right three colums indicate the APs with fine-tuning on THEODORE. We achieve in all three meta-architectures for person class a significant improvement with THEODORE with respect to the baseline. The mAP of experiments on FES with six classes are shown in Table 2. With 0.613 the highest mAP is reached with the Faster R-CNN. The per-class winners are highlighted bold in Table 2. The classes person and table have the highest APs which can explained through a good representation in the training data, i.e. various viewing angle and texture. Improvements needs to be done in the classes armchair, chair and TV. The low AP values can have different reasons. First, the objects in the training data have too less variations in terms of illumination, texture and viewing angle. Second, the training data doesn't fit well to the test data, which ends up with the creation of a more generalized model for these classes. Another effect we observed through the evaluation is the non-detection of the class TV with R-FCN. For this we suspect a too high shrinking of the image as input for the net, so the filter sizes are to big for the whole image to detect small objects with the R-FCN.\n\n\nSemantic Segmentation\n\nBeside object detection we show that THEODORE is eglible for training segmentation networks. Due to the lack of publicy available top-view fisheye label masks for evaluation of THEODORE we annotate own data, namely FES. The report of the class IoU and mIoU on two state of the art architectures for segmentation, SegNet and PSPNet, is shown in Table 3.\n\nIn Table 3 we evaluate THEODORE by fine-tuned Seg-Net and PSPNet on the FES. We observe class IoUs of 0.67 We belief that the texture of the synthetically generated furniture is different from the real-world furniture texture.\n\n\nConclusion\n\nIn this paper we introduce THEODORE -a synTHEtic tOp-view inDoOR scEnes dataset with omnidirectional images. This dataset contains 100,000 rendered images of diversified indoor environments, segmentation and instance masks for 16 classes and bounding boxes for the person class. Additionally, we have shown that the usage of synthetically generated images could compensate the lack of real omnidirectional images during training of CNNs. We have addressed the task of object detection and semantic segmentation for evaluating the performance of state-ofthe-art CNNs trained on THEODORE. The evaluation process of our dataset works as follows: the training baseline is MS COCO, which contains front-views of perspective images. We fine-tune three meta-architectures for object detection, namely SSD, R-FCN and Faster R-CNN for the person class on THEODORE. In addition we train two meta-architectures for semantic segmenation, the Seg-Net and PSPNet for six classes in an indoor environment. Both object detectors and segmentation approaches were evaluated on our own annotated fisheye evaluation suite dataset (FES), that contains segmentation and object detection ground truth for six classes. With this we have shown the adaptation of the front-view to the top-view by finetuning CNNs with our generated data. While labels for fixed objects are not available in public real world databases, we use six classes for evaluation of THEODORE, which leads to significant improvement of the AP and mIoU over the baselines in all tested meta-architectures.\n\nFuture research will address the balancing of the classes of THEODORE. While the FES evaluation dataset only contains one scenario, we plan to add more real world indoor scenes. Beyond the segmentation and detection masks we intend to create omnidirectional depth, skeletons and optical flow ground truth from rendered scenes.\n\nFigure 1 :\n1Pipeline showing the image generation for THEODORE. Starting with four cameras pointing in top, left, right and bottom direction with a field of view of 90 degrees per camera, a fisheye distorted image is generated. In addition to the rendered RGB image, instance and segmentation masks are created, distorted and saved as training data. In post-processing, bounding boxes are extracted and converted into common dataset formats such as TFRecord.\n\nFigure 2 :\n2Overview of random characters and room floor plans used in THEODORE. Additionally, some sample images with the applied domain randomization are depicted. For each room three random textured scenes with random camera positions are selected.\n\nFigure 3 :\n3In Figure 3a we show the number of annotations per class. In Figure 3b the distribution of centroid location of a person over all images is illustrated. See text for details.\n\nFigure 4 :\n4Example of detection results on HDA, Bomni and FES dataset using SSD meta-architecture for person class. The first column contains the ground truth bounding boxes. In the second column the prediction results for the pre-trained CNN with MS COCO weights is shown. The last column indicates the detection boxes which we reach while fine-tuning on THEODORE. Statistics of the dataset and the evaluation through object detection is provided in the supplementary material.\n\nFigure 5 :Figure 6 :\n56FES evaluation samples of object detection and segmentation architectures trained on THEODORE. The first row shows detection results from SSD, the second row the segmentation results of SegNet. SSD is trained on MS COCO and fine-tuned on THEODORE, while SegNet is trained from scratch on THEODORE. Number of images in the training set with corresponding mAP@0.5 trained on the SSD meta-architecture. Sampled: constant number of textures with variable number of images; halved: halved number of images and halved number of textures\n\nTable 1 :\n1Quantitative evaluation of THEODORE for person class based on AP@0.5Person AP@0.5 \nMS COCO \nMS COCO + THEODORE \nHDA Bomni DST \nHDA Bomni \nDST \nSSD \n0.586 0.052 0.484 0.802 0.579 \n0.904 \nR-FCN \n0.303 0.069 0.525 0.694 0.675 \n0.849 \nFaster R-CNN \n0.627 0.067 0.630 0.704 0.740 \n0.873 \n\n\n\nTable 2 :\n2Quantitative evaluation of THEODORE for all classes based on mAP@0.5Class AP@0.5 Armchair Chair Person Table \nTV \nWheeled \nWalker \nmAP \n\nSSD \n0.021 \n0.231 0.904 0.824 0.545 \n0.623 \n0.525 \nR-FCN \n0.262 \n0.039 0.849 0.859 0.000 \n0.640 \n0.441 \nFaster R-CNN \n0.148 \n0.141 0.873 0.980 0.943 \n0.596 \n0.613 \n\n\n\nTable 3 :\n3Quantitative evaluation of THEODORE by finetuning CNN meta-architectures for semantic segmentation. for person and 0.53 for TV. The mIoU lies at 0.36 for the SegNet and 0.23 for the PSPNet. Both segmentation architectures are realtively good in the class person, while classes like chair, armchair and table needs further investigations.Class IoU Armchair Chair Person Table \nTV \nWheeled \nWalker \nmIoU \n\nSegNet \n0.009 \n0.016 0.674 0.012 0.53 \n0.33 \n0.359 \nPSPNet \n0.005 \n0.023 0.434 0.003 0.195 \n0.034 \n0.229 \n\n\nhttps://www.cc0textures.com\n\nUnity-technologies: Image synthesis for machine learning -bitbucket. Unity-technologies: Image synthesis for machine learning -bitbucket. https://bitbucket.org/ Unity-Technologies/ml-imagesynthesis.\n\nSegnet: A deep convolutional encoder-decoder architecture for image segmentation. V Badrinarayanan, A Kendall, R Cipolla, IEEE transactions on pattern analysis and machine intelligence. 39V. Badrinarayanan, A. Kendall, and R. Cipolla. Segnet: A deep convolutional encoder-decoder architecture for image segmentation. IEEE transactions on pattern analysis and machine intelligence, 39(12):2481-2495, 2017. 5\n\nidome: Immersive gaming with the unity3d game engine. P Bourke, 9P. Bourke. idome: Immersive gaming with the unity3d game engine. volume 9, pages 265-272, 2009. 4\n\nBlender and immersive gaming in a hemispherical dome. P D Bourke, D Q Felinto, 104P. D. Bourke and D. Q. Felinto. Blender and immersive gam- ing in a hemispherical dome. volume 10, pages 280-284, 2010. 4\n\nHome: A household multimodal environment. S Brodeur, E Perez, A Anand, F Golemo, L Celotti, F Strub, J Rouat, H Larochelle, A Courville, arXiv:1711.11017arXiv preprintS. Brodeur, E. Perez, A. Anand, F. Golemo, L. Celotti, F. Strub, J. Rouat, H. Larochelle, and A. Courville. Home: A household multimodal environment. arXiv preprint arXiv:1711.11017, 2017. 2\n\nMatterport3d: Learning from rgb-d data in indoor environments. A Chang, A Dai, T Funkhouser, M Halber, M Niessner, M Savva, S Song, A Zeng, Y Zhang, International Conference on 3D Vision (3DV). A. Chang, A. Dai, T. Funkhouser, M. Halber, M. Niessner, M. Savva, S. Song, A. Zeng, and Y. Zhang. Matterport3d: Learning from rgb-d data in indoor environments. Interna- tional Conference on 3D Vision (3DV), 2017. 2\n\nA direct approach for human detection with catadioptric omnidirectional cameras. I Cinaroglu, Y Bastanlar, Signal Processing and Communications Applications Conference (SIU). I. Cinaroglu and Y. Bastanlar. A direct approach for hu- man detection with catadioptric omnidirectional cameras. In Signal Processing and Communications Applications Con- ference (SIU), 2014 22nd, pages 2275-2279. IEEE, 2014. 2\n\nSpherical cnns. T S Cohen, M Geiger, J Koehler, M Welling, International Conference on Learning Representations (ICLR). T. S. Cohen, M. Geiger, J. Koehler, and M. Welling. Spher- ical cnns. In International Conference on Learning Repre- sentations (ICLR), April 2018. 2\n\nSpherenet: Learning spherical representations for detection and classification in omnidirectional images. B Coors, A Paul Condurache, A Geiger, The European Conference on Computer Vision (ECCV). B. Coors, A. Paul Condurache, and A. Geiger. Spherenet: Learning spherical representations for detection and classifi- cation in omnidirectional images. In The European Confer- ence on Computer Vision (ECCV), September 2018. 2\n\nR-fcn: Object detection via region-based fully convolutional networks. J Dai, Y Li, K He, J Sun, Advances in Neural Information Processing Systems. D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors292Curran Associates, IncJ. Dai, Y. Li, K. He, and J. Sun. R-fcn: Object detection via region-based fully convolutional networks. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, edi- tors, Advances in Neural Information Processing Systems 29, pages 379-387. Curran Associates, Inc., 2016. 2\n\nThe piropo database (people in indoor rooms with perspective and omnidirectional cameras. C R Blanco, P Carballeira, C. R. del Blanco and P. Carballeira. The piropo database (people in indoor rooms with perspective and omnidi- rectional cameras). https://sites.google.com/ site/piropodatabase/, unpublished dataset, 2016. 2\n\nFeature-based tracking on a multi-omnidirectional camera dataset. B E Demir\u00f6z, \u0130 Ari, O Eroglu, A A Salah, L Akarun, Communications Control and Signal Processing. 262012 5th International Symposium onB. E. Demir\u00f6z,\u0130. Ari, O. Eroglu, A. A. Salah, and L. Akarun. Feature-based tracking on a multi-omnidirectional camera dataset. In Communications Control and Signal Processing (ISCCSP), 2012 5th International Symposium on, pages 1-5. IEEE, 2012. 2, 6\n\nA data set providing synthetic and real-world fisheye video sequences. A Eichenseer, A Kaup, Acoustics, Speech and Signal Processing. 2016 IEEE International Conference onA. Eichenseer and A. Kaup. A data set providing synthetic and real-world fisheye video sequences. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on, pages 1541-1545. IEEE, 2016. 2\n\nThe PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results. M Everingham, L Van Gool, C K I Williams, J Winn, A Zisserman, 46M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results. 4, 6\n\nThe hda+ data set for research on fully automated re-identification systems. D Figueira, M Taiana, A Nambiar, J Nascimento, A Bernardino, European Conference on Computer Vision. SpringerD. Figueira, M. Taiana, A. Nambiar, J. Nascimento, and A. Bernardino. The hda+ data set for research on fully auto- mated re-identification systems. In European Conference on Computer Vision, pages 241-255. Springer, 2014. 2\n\nVirtual worlds as proxy for multi-object tracking analysis. A Gaidon, Q Wang, Y Cabon, E Vig, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionA. Gaidon, Q. Wang, Y. Cabon, and E. Vig. Virtual worlds as proxy for multi-object tracking analysis. In Proceedings of the IEEE conference on computer vision and pattern recog- nition, pages 4340-4349, 2016. 2\n\nLearning where to look: Data-driven viewpoint set selection for 3d scenes. K Genova, M Savva, A X Chang, T Funkhouser, arXiv:1704.02393arXiv preprintK. Genova, M. Savva, A. X. Chang, and T. Funkhouser. Learning where to look: Data-driven viewpoint set selection for 3d scenes. arXiv preprint arXiv:1704.02393, 2017. 2\n\nUnderstanding real world indoor scenes with synthetic data. A Handa, V Patraucean, V Badrinarayanan, S Stent, R Cipolla, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionA. Handa, V. Patraucean, V. Badrinarayanan, S. Stent, and R. Cipolla. Understanding real world indoor scenes with synthetic data. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4077- 4085, 2016. 2\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR), pages 770- 778, June 2016. 5\n\nSpeed/accuracy trade-offs for modern convolutional object detectors. J Huang, V Rathod, C Sun, M Zhu, A Korattikara, A Fathi, I Fischer, Z Wojna, Y Song, S Guadarrama, K Murphy, 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 45J. Huang, V. Rathod, C. Sun, M. Zhu, A. Korattikara, A. Fathi, I. Fischer, Z. Wojna, Y. Song, S. Guadarrama, and K. Murphy. Speed/accuracy trade-offs for modern convolu- tional object detectors. In 2017 IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR), pages 3296- 3297, July 2017. 4, 5\n\nLatent structured models for human pose estimation. C Ionescu, F Li, C Sminchisescu, Computer Vision (ICCV), 2011 IEEE International Conference on. C. Ionescu, F. Li, and C. Sminchisescu. Latent struc- tured models for human pose estimation. In Computer Vi- sion (ICCV), 2011 IEEE International Conference on, pages 2220-2227. IEEE, 2011. 2\n\nHuman3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments. C Ionescu, D Papava, V Olaru, C Sminchisescu, IEEE transactions on pattern analysis and machine intelligence. 36C. Ionescu, D. Papava, V. Olaru, and C. Sminchisescu. Human3. 6m: Large scale datasets and predictive meth- ods for 3d human sensing in natural environments. IEEE transactions on pattern analysis and machine intelligence, 36(7):1325-1339, 2014. 2\n\nAi2-thor: An interactive 3d environment for visual ai. E Kolve, R Mottaghi, W Han, E Vanderbilt, L Weihs, A Herrasti, D Gordon, Y Zhu, A Gupta, A Farhadi, E. Kolve, R. Mottaghi, W. Han, E. VanderBilt, L. Weihs, A. Herrasti, D. Gordon, Y. Zhu, A. Gupta, and A. Farhadi. Ai2-thor: An interactive 3d environment for visual ai. arXiv, 2017. 2\n\nImagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, Advances in neural information processing systems. A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097-1105, 2012. 5\n\nInteriornet: Mega-scale multi-sensor photo-realistic indoor scenes dataset. W Li, S Saeedi, J Mccormac, R Clark, D Tzoumanikas, Q Ye, Y Huang, R Tang, S Leutenegger, British Machine Vision Conference (BMVC). W. Li, S. Saeedi, J. McCormac, R. Clark, D. Tzoumanikas, Q. Ye, Y. Huang, R. Tang, and S. Leutenegger. Interior- net: Mega-scale multi-sensor photo-realistic indoor scenes dataset. In British Machine Vision Conference (BMVC), 2018. 2\n\nPerson Re-identification Dataset with RGB-D Camera in a Top-View Configuration. D Liciotti, M Paolanti, E Frontoni, A Mancini, P Zingaretti, Springer International PublishingChamD. Liciotti, M. Paolanti, E. Frontoni, A. Mancini, and P. Zin- garetti. Person Re-identification Dataset with RGB-D Cam- era in a Top-View Configuration, pages 1-11. Springer In- ternational Publishing, Cham, 2017. 2\n\nFeature pyramid networks for object detection. T Lin, P Doll\u00e1r, R Girshick, K He, B Hariharan, S Belongie, 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). T. Lin, P. Doll\u00e1r, R. Girshick, K. He, B. Hariharan, and S. Belongie. Feature pyramid networks for object detection. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 936-944, July 2017. 5\n\nMicrosoft coco: Common objects in context. T.-Y Lin, M Maire, S Belongie, J Hays, P Perona, D Ramanan, P Doll\u00e1r, C L Zitnick, Computer Vision -ECCV 2014. D. Fleet, T. Pajdla, B. Schiele, and T. TuytelaarsChamSpringer International Publishing15T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra- manan, P. Doll\u00e1r, and C. L. Zitnick. Microsoft coco: Com- mon objects in context. In D. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars, editors, Computer Vision -ECCV 2014, pages 740-755, Cham, 2014. Springer International Publishing. 1, 5\n\nSsd: Single shot multibox detector. W Liu, D Anguelov, D Erhan, C Szegedy, S Reed, C.-Y Fu, A C Berg, Computer Vision -ECCV 2016. B. Leibe, J. Matas, N. Sebe, and M. WellingChamSpringer International PublishingW. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.- Y. Fu, and A. C. Berg. Ssd: Single shot multibox detec- tor. In B. Leibe, J. Matas, N. Sebe, and M. Welling, editors, Computer Vision -ECCV 2016, pages 21-37, Cham, 2016. Springer International Publishing. 2\n\nSMPL: A skinned multi-person linear model. M Loper, N Mahmood, J Romero, G Pons-Moll, M J Black, Proc. SIGGRAPH Asia). SIGGRAPH Asia)34M. Loper, N. Mahmood, J. Romero, G. Pons-Moll, and M. J. Black. SMPL: A skinned multi-person linear model. ACM Trans. Graphics (Proc. SIGGRAPH Asia), 34(6):248:1- 248:16, Oct. 2015. 2\n\nSgdr: Stochastic gradient descent with warm restarts. I Loshchilov, F Hutter, arXiv:1608.03983arXiv preprintI. Loshchilov and F. Hutter. Sgdr: Stochastic gradient de- scent with warm restarts. arXiv preprint arXiv:1608.03983, 2016. 5\n\nA multi-camera video dataset for research on high-definition surveillance. A Nambiar, M Taiana, D Figueira, J Nascimento, A Bernardino, International Journal of Machine Intelligence and Sensory Signal Processing. 137A. Nambiar, M. Taiana, D. Figueira, J. Nascimento, and A. Bernardino. A multi-camera video dataset for research on high-definition surveillance. International Journal of Ma- chine Intelligence and Sensory Signal Processing, 1(3):267- 286, 2014. 6, 7\n\nVirtualhome: Simulating household activities via programs. X Puig, K Ra, M Boben, J Li, T Wang, S Fidler, A Torralba, Computer Vision and Pattern Recognition (CVPR). X. Puig, K. Ra, M. Boben, J. Li, T. Wang, S. Fidler, and A. Torralba. Virtualhome: Simulating household activities via programs. In Computer Vision and Pattern Recognition (CVPR), 2018. 2\n\nOn the momentum term in gradient descent learning algorithms. N Qian, Neural networks. 121N. Qian. On the momentum term in gradient descent learning algorithms. Neural networks, 12(1):145-151, 1999. 5\n\nJ Redmon, A Farhadi, arXiv:1804.02767Yolov3: An incremental improvement. arXiv preprintJ. Redmon and A. Farhadi. Yolov3: An incremental improve- ment. arXiv preprint arXiv:1804.02767, 2018. 2\n\nFaster r-cnn: Towards real-time object detection with region proposal networks. S Ren, K He, R Girshick, J Sun, Advances in Neural Information Processing Systems. C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. GarnettCurran Associates, Inc28S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Process- ing Systems 28, pages 91-99. Curran Associates, Inc., 2015. 2\n\nDescribing common human visual actions in images. M R Ronchi, P Perona, Proceedings of the British Machine Vision Conference (BMVC). the British Machine Vision Conference (BMVC)BMVA Press12M. R. Ronchi and P. Perona. Describing common human visual actions in images. In Proceedings of the British Ma- chine Vision Conference (BMVC), pages 52.1-52.12. BMVA Press, September 2015. 1\n\nM Savva, A X Chang, A Dosovitskiy, T Funkhouser, V Koltun, Minos, arXiv:1712.03931Multimodal indoor simulator for navigation in complex environments. M. Savva, A. X. Chang, A. Dosovitskiy, T. Funkhouser, and V. Koltun. Minos: Multimodal indoor simulator for navi- gation in complex environments. arXiv:1712.03931, 2017. 2\n\nRecognizing human actions: A local svm approach. C Schuldt, I Laptev, B Caputo, Proceedings of the Pattern Recognition, 17th International Conference on (ICPR'04. the Pattern Recognition, 17th International Conference on (ICPR'04IEEE Computer Society3C. Schuldt, I. Laptev, and B. Caputo. Recognizing human ac- tions: A local svm approach. In Proceedings of the Pattern Recognition, 17th International Conference on (ICPR'04) Volume 3-Volume 03, pages 32-36. IEEE Computer Society, 2004. 1\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, arXiv:1409.1556arXiv preprintK. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. 5\n\nSemantic scene completion from a single depth image. S Song, F Yu, A Zeng, A X Chang, M Savva, T Funkhouser, IEEE Conference on Computer Vision and Pattern Recognition. S. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, and T. Funkhouser. Semantic scene completion from a single depth image. IEEE Conference on Computer Vision and Pat- tern Recognition, 2017. 2\n\nIm2pano3d: Extrapolating 360 structure and semantics beyond the field of view. S Song, A Zeng, A X Chang, M Savva, S Savarese, T Funkhouser, Proceedings of 31th IEEE Conference on Computer Vision and Pattern Recognition. 31th IEEE Conference on Computer Vision and Pattern RecognitionS. Song, A. Zeng, A. X. Chang, M. Savva, S. Savarese, and T. Funkhouser. Im2pano3d: Extrapolating 360 structure and semantics beyond the field of view. Proceedings of 31th IEEE Conference on Computer Vision and Pattern Recog- nition, 2018. 2\n\nRicap: Random image cropping and patching data augmentation for deep cnns. R Takahashi, T Matsubara, K Uehara, PMLRProceedings of The 10th Asian Conference on Machine Learning. The 10th Asian Conference on Machine Learning95R. Takahashi, T. Matsubara, and K. Uehara. Ricap: Ran- dom image cropping and patching data augmentation for deep cnns. In Proceedings of The 10th Asian Conference on Machine Learning, volume 95 of Proceedings of Ma- chine Learning Research, pages 786-798. PMLR, 14-16\n\nOvvv: Using virtual worlds to design and evaluate surveillance systems. G R Taylor, A J Chosak, P C Brewer, 2007 IEEE conference on computer vision and pattern recognition. G. R. Taylor, A. J. Chosak, and P. C. Brewer. Ovvv: Using virtual worlds to design and evaluate surveillance systems. In 2007 IEEE conference on computer vision and pattern recognition, pages 1-8. IEEE, 2007. 2\n\nDomain randomization for transferring deep neural networks from simulation to the real world. J Tobin, R Fong, A Ray, J Schneider, W Zaremba, P Abbeel, J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel. Domain randomization for transferring deep neu- ral networks from simulation to the real world. In 2017\n\nIEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 45IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 23-30, Sep. 2017. 4, 5\n\nTraining deep networks with synthetic data: Bridging the reality gap by domain randomization. J Tremblay, A Prakash, D Acuna, M Brophy, V Jampani, C Anil, T To, E Cameracci, S Boochoon, S Birchfield, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). J. Tremblay, A. Prakash, D. Acuna, M. Brophy, V. Jampani, C. Anil, T. To, E. Cameracci, S. Boochoon, and S. Birchfield. Training deep networks with synthetic data: Bridging the re- ality gap by domain randomization. In 2018 IEEE/CVF Con- ference on Computer Vision and Pattern Recognition Work- shops (CVPRW), pages 1082-10828, June 2018. 4\n\nLearning from synthetic humans. G Varol, J Romero, X Martin, N Mahmood, M J Black, I Laptev, C Schmid, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition24G. Varol, J. Romero, X. Martin, N. Mahmood, M. J. Black, I. Laptev, and C. Schmid. Learning from synthetic humans. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 109-117, 2017. 2, 4\n\nBuilding generalizable agents with a realistic and rich 3d environment. Y Wu, Y Wu, G Gkioxari, Y Tian, arXiv:1801.02209arXiv preprintY. Wu, Y. Wu, G. Gkioxari, and Y. Tian. Building general- izable agents with a realistic and rich 3d environment. arXiv preprint arXiv:1801.02209, 2018. 2\n\nPyramid scene parsing network. H Zhao, J Shi, X Qi, X Wang, J Jia, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionH. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia. Pyramid scene parsing network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2881-2890, 2017. 5\n", "annotations": {"author": "[{\"end\":271,\"start\":106},{\"end\":435,\"start\":272},{\"end\":595,\"start\":436}]", "publisher": null, "author_last_name": "[{\"end\":119,\"start\":113},{\"end\":284,\"start\":278},{\"end\":449,\"start\":444}]", "author_first_name": "[{\"end\":112,\"start\":106},{\"end\":277,\"start\":272},{\"end\":443,\"start\":436}]", "author_affiliation": "[{\"end\":270,\"start\":155},{\"end\":434,\"start\":319},{\"end\":594,\"start\":479}]", "title": "[{\"end\":103,\"start\":1},{\"end\":698,\"start\":596}]", "venue": null, "abstract": "[{\"end\":1951,\"start\":700}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2201,\"start\":2197},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":2204,\"start\":2201},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":2207,\"start\":2204},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":4963,\"start\":4959},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5039,\"start\":5035},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5042,\"start\":5039},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5138,\"start\":5134},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5302,\"start\":5298},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5305,\"start\":5302},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":5431,\"start\":5427},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5462,\"start\":5459},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":5519,\"start\":5515},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":5766,\"start\":5762},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5911,\"start\":5908},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":5990,\"start\":5986},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":6322,\"start\":6318},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6638,\"start\":6634},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6777,\"start\":6774},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6780,\"start\":6777},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6783,\"start\":6780},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6786,\"start\":6783},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6789,\"start\":6786},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6888,\"start\":6884},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":6938,\"start\":6934},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":6941,\"start\":6938},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":7086,\"start\":7082},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7095,\"start\":7091},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7352,\"start\":7349},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7360,\"start\":7357},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7480,\"start\":7477},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7629,\"start\":7626},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7847,\"start\":7843},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":7850,\"start\":7847},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":8380,\"start\":8376},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9282,\"start\":9278},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10135,\"start\":10132},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10137,\"start\":10135},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11451,\"start\":11448},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":11761,\"start\":11757},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":11764,\"start\":11761},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":12099,\"start\":12095},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":13380,\"start\":13376},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13403,\"start\":13399},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":15050,\"start\":15046},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":15758,\"start\":15754},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":16128,\"start\":16124},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":16529,\"start\":16525},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":16720,\"start\":16716},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":17086,\"start\":17082},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":17446,\"start\":17442},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":17539,\"start\":17535},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":17655,\"start\":17651},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":17695,\"start\":17691},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":18303,\"start\":18299},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":18306,\"start\":18303},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":18309,\"start\":18306},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":19224,\"start\":19220},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":19283,\"start\":19279},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":19528,\"start\":19524},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":21672,\"start\":21668}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":27301,\"start\":26842},{\"attributes\":{\"id\":\"fig_1\"},\"end\":27554,\"start\":27302},{\"attributes\":{\"id\":\"fig_2\"},\"end\":27742,\"start\":27555},{\"attributes\":{\"id\":\"fig_3\"},\"end\":28223,\"start\":27743},{\"attributes\":{\"id\":\"fig_4\"},\"end\":28778,\"start\":28224},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":29075,\"start\":28779},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":29390,\"start\":29076},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":29914,\"start\":29391}]", "paragraph": "[{\"end\":2879,\"start\":1967},{\"end\":3781,\"start\":2881},{\"end\":3811,\"start\":3783},{\"end\":4012,\"start\":3813},{\"end\":4639,\"start\":4014},{\"end\":6722,\"start\":4656},{\"end\":7805,\"start\":6724},{\"end\":8202,\"start\":7807},{\"end\":8381,\"start\":8204},{\"end\":8600,\"start\":8402},{\"end\":9047,\"start\":8620},{\"end\":9571,\"start\":9049},{\"end\":9912,\"start\":9573},{\"end\":10138,\"start\":9914},{\"end\":10645,\"start\":10140},{\"end\":13405,\"start\":10647},{\"end\":14273,\"start\":13426},{\"end\":14745,\"start\":14275},{\"end\":15248,\"start\":14758},{\"end\":18949,\"start\":15250},{\"end\":19470,\"start\":18964},{\"end\":20098,\"start\":19472},{\"end\":20526,\"start\":20119},{\"end\":20812,\"start\":20528},{\"end\":21244,\"start\":20855},{\"end\":24342,\"start\":21265},{\"end\":24720,\"start\":24368},{\"end\":24948,\"start\":24722},{\"end\":26513,\"start\":24963},{\"end\":26841,\"start\":26515}]", "formula": null, "table_ref": "[{\"end\":20876,\"start\":20861},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":23091,\"start\":23084},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":23424,\"start\":23417},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":23575,\"start\":23533},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":24719,\"start\":24712},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":24732,\"start\":24725}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1965,\"start\":1953},{\"attributes\":{\"n\":\"2.\"},\"end\":4654,\"start\":4642},{\"attributes\":{\"n\":\"3.\"},\"end\":8400,\"start\":8384},{\"attributes\":{\"n\":\"3.1.\"},\"end\":8618,\"start\":8603},{\"attributes\":{\"n\":\"3.2.\"},\"end\":13424,\"start\":13408},{\"attributes\":{\"n\":\"4.\"},\"end\":14756,\"start\":14748},{\"attributes\":{\"n\":\"5.\"},\"end\":18962,\"start\":18952},{\"attributes\":{\"n\":\"5.1.\"},\"end\":20117,\"start\":20101},{\"end\":20844,\"start\":20815},{\"end\":20853,\"start\":20847},{\"attributes\":{\"n\":\"5.2.\"},\"end\":21263,\"start\":21247},{\"attributes\":{\"n\":\"5.3.\"},\"end\":24366,\"start\":24345},{\"attributes\":{\"n\":\"6.\"},\"end\":24961,\"start\":24951},{\"end\":26853,\"start\":26843},{\"end\":27313,\"start\":27303},{\"end\":27566,\"start\":27556},{\"end\":27754,\"start\":27744},{\"end\":28245,\"start\":28225},{\"end\":28789,\"start\":28780},{\"end\":29086,\"start\":29077},{\"end\":29401,\"start\":29392}]", "table": "[{\"end\":29075,\"start\":28859},{\"end\":29390,\"start\":29156},{\"end\":29914,\"start\":29740}]", "figure_caption": "[{\"end\":27301,\"start\":26855},{\"end\":27554,\"start\":27315},{\"end\":27742,\"start\":27568},{\"end\":28223,\"start\":27756},{\"end\":28778,\"start\":28248},{\"end\":28859,\"start\":28791},{\"end\":29156,\"start\":29088},{\"end\":29740,\"start\":29403}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10207,\"start\":10199},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":10386,\"start\":10378},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13511,\"start\":13503},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14450,\"start\":14442},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":14511,\"start\":14503},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":14605,\"start\":14597},{\"end\":19844,\"start\":19836},{\"end\":20097,\"start\":20089},{\"end\":20297,\"start\":20289}]", "bib_author_first_name": "[{\"end\":30227,\"start\":30226},{\"end\":30245,\"start\":30244},{\"end\":30256,\"start\":30255},{\"end\":30607,\"start\":30606},{\"end\":30771,\"start\":30770},{\"end\":30773,\"start\":30772},{\"end\":30783,\"start\":30782},{\"end\":30785,\"start\":30784},{\"end\":30964,\"start\":30963},{\"end\":30975,\"start\":30974},{\"end\":30984,\"start\":30983},{\"end\":30993,\"start\":30992},{\"end\":31003,\"start\":31002},{\"end\":31014,\"start\":31013},{\"end\":31023,\"start\":31022},{\"end\":31032,\"start\":31031},{\"end\":31046,\"start\":31045},{\"end\":31344,\"start\":31343},{\"end\":31353,\"start\":31352},{\"end\":31360,\"start\":31359},{\"end\":31374,\"start\":31373},{\"end\":31384,\"start\":31383},{\"end\":31396,\"start\":31395},{\"end\":31405,\"start\":31404},{\"end\":31413,\"start\":31412},{\"end\":31421,\"start\":31420},{\"end\":31774,\"start\":31773},{\"end\":31787,\"start\":31786},{\"end\":32114,\"start\":32113},{\"end\":32116,\"start\":32115},{\"end\":32125,\"start\":32124},{\"end\":32135,\"start\":32134},{\"end\":32146,\"start\":32145},{\"end\":32475,\"start\":32474},{\"end\":32484,\"start\":32483},{\"end\":32503,\"start\":32502},{\"end\":32863,\"start\":32862},{\"end\":32870,\"start\":32869},{\"end\":32876,\"start\":32875},{\"end\":32882,\"start\":32881},{\"end\":33414,\"start\":33413},{\"end\":33416,\"start\":33415},{\"end\":33426,\"start\":33425},{\"end\":33715,\"start\":33714},{\"end\":33717,\"start\":33716},{\"end\":33728,\"start\":33727},{\"end\":33735,\"start\":33734},{\"end\":33745,\"start\":33744},{\"end\":33747,\"start\":33746},{\"end\":33756,\"start\":33755},{\"end\":34171,\"start\":34170},{\"end\":34185,\"start\":34184},{\"end\":34560,\"start\":34559},{\"end\":34574,\"start\":34573},{\"end\":34586,\"start\":34585},{\"end\":34590,\"start\":34587},{\"end\":34602,\"start\":34601},{\"end\":34610,\"start\":34609},{\"end\":34849,\"start\":34848},{\"end\":34861,\"start\":34860},{\"end\":34871,\"start\":34870},{\"end\":34882,\"start\":34881},{\"end\":34896,\"start\":34895},{\"end\":35244,\"start\":35243},{\"end\":35254,\"start\":35253},{\"end\":35262,\"start\":35261},{\"end\":35271,\"start\":35270},{\"end\":35706,\"start\":35705},{\"end\":35716,\"start\":35715},{\"end\":35725,\"start\":35724},{\"end\":35727,\"start\":35726},{\"end\":35736,\"start\":35735},{\"end\":36010,\"start\":36009},{\"end\":36019,\"start\":36018},{\"end\":36033,\"start\":36032},{\"end\":36051,\"start\":36050},{\"end\":36060,\"start\":36059},{\"end\":36497,\"start\":36496},{\"end\":36503,\"start\":36502},{\"end\":36512,\"start\":36511},{\"end\":36519,\"start\":36518},{\"end\":36857,\"start\":36856},{\"end\":36866,\"start\":36865},{\"end\":36876,\"start\":36875},{\"end\":36883,\"start\":36882},{\"end\":36890,\"start\":36889},{\"end\":36905,\"start\":36904},{\"end\":36914,\"start\":36913},{\"end\":36925,\"start\":36924},{\"end\":36934,\"start\":36933},{\"end\":36942,\"start\":36941},{\"end\":36956,\"start\":36955},{\"end\":37399,\"start\":37398},{\"end\":37410,\"start\":37409},{\"end\":37416,\"start\":37415},{\"end\":37791,\"start\":37790},{\"end\":37802,\"start\":37801},{\"end\":37812,\"start\":37811},{\"end\":37821,\"start\":37820},{\"end\":38206,\"start\":38205},{\"end\":38215,\"start\":38214},{\"end\":38227,\"start\":38226},{\"end\":38234,\"start\":38233},{\"end\":38248,\"start\":38247},{\"end\":38257,\"start\":38256},{\"end\":38269,\"start\":38268},{\"end\":38279,\"start\":38278},{\"end\":38286,\"start\":38285},{\"end\":38295,\"start\":38294},{\"end\":38556,\"start\":38555},{\"end\":38570,\"start\":38569},{\"end\":38583,\"start\":38582},{\"end\":38585,\"start\":38584},{\"end\":38914,\"start\":38913},{\"end\":38920,\"start\":38919},{\"end\":38930,\"start\":38929},{\"end\":38942,\"start\":38941},{\"end\":38951,\"start\":38950},{\"end\":38966,\"start\":38965},{\"end\":38972,\"start\":38971},{\"end\":38981,\"start\":38980},{\"end\":38989,\"start\":38988},{\"end\":39361,\"start\":39360},{\"end\":39373,\"start\":39372},{\"end\":39385,\"start\":39384},{\"end\":39397,\"start\":39396},{\"end\":39408,\"start\":39407},{\"end\":39724,\"start\":39723},{\"end\":39731,\"start\":39730},{\"end\":39741,\"start\":39740},{\"end\":39753,\"start\":39752},{\"end\":39759,\"start\":39758},{\"end\":39772,\"start\":39771},{\"end\":40123,\"start\":40119},{\"end\":40130,\"start\":40129},{\"end\":40139,\"start\":40138},{\"end\":40151,\"start\":40150},{\"end\":40159,\"start\":40158},{\"end\":40169,\"start\":40168},{\"end\":40180,\"start\":40179},{\"end\":40190,\"start\":40189},{\"end\":40192,\"start\":40191},{\"end\":40659,\"start\":40658},{\"end\":40666,\"start\":40665},{\"end\":40678,\"start\":40677},{\"end\":40687,\"start\":40686},{\"end\":40698,\"start\":40697},{\"end\":40709,\"start\":40705},{\"end\":40715,\"start\":40714},{\"end\":40717,\"start\":40716},{\"end\":41141,\"start\":41140},{\"end\":41150,\"start\":41149},{\"end\":41161,\"start\":41160},{\"end\":41171,\"start\":41170},{\"end\":41184,\"start\":41183},{\"end\":41186,\"start\":41185},{\"end\":41472,\"start\":41471},{\"end\":41486,\"start\":41485},{\"end\":41728,\"start\":41727},{\"end\":41739,\"start\":41738},{\"end\":41749,\"start\":41748},{\"end\":41761,\"start\":41760},{\"end\":41775,\"start\":41774},{\"end\":42179,\"start\":42178},{\"end\":42187,\"start\":42186},{\"end\":42193,\"start\":42192},{\"end\":42202,\"start\":42201},{\"end\":42208,\"start\":42207},{\"end\":42216,\"start\":42215},{\"end\":42226,\"start\":42225},{\"end\":42537,\"start\":42536},{\"end\":42677,\"start\":42676},{\"end\":42687,\"start\":42686},{\"end\":42950,\"start\":42949},{\"end\":42957,\"start\":42956},{\"end\":42963,\"start\":42962},{\"end\":42975,\"start\":42974},{\"end\":43474,\"start\":43473},{\"end\":43476,\"start\":43475},{\"end\":43486,\"start\":43485},{\"end\":43806,\"start\":43805},{\"end\":43815,\"start\":43814},{\"end\":43817,\"start\":43816},{\"end\":43826,\"start\":43825},{\"end\":43841,\"start\":43840},{\"end\":43855,\"start\":43854},{\"end\":44178,\"start\":44177},{\"end\":44189,\"start\":44188},{\"end\":44199,\"start\":44198},{\"end\":44688,\"start\":44687},{\"end\":44700,\"start\":44699},{\"end\":44934,\"start\":44933},{\"end\":44942,\"start\":44941},{\"end\":44948,\"start\":44947},{\"end\":44956,\"start\":44955},{\"end\":44958,\"start\":44957},{\"end\":44967,\"start\":44966},{\"end\":44976,\"start\":44975},{\"end\":45320,\"start\":45319},{\"end\":45328,\"start\":45327},{\"end\":45336,\"start\":45335},{\"end\":45338,\"start\":45337},{\"end\":45347,\"start\":45346},{\"end\":45356,\"start\":45355},{\"end\":45368,\"start\":45367},{\"end\":45843,\"start\":45842},{\"end\":45856,\"start\":45855},{\"end\":45869,\"start\":45868},{\"end\":46334,\"start\":46333},{\"end\":46336,\"start\":46335},{\"end\":46346,\"start\":46345},{\"end\":46348,\"start\":46347},{\"end\":46358,\"start\":46357},{\"end\":46360,\"start\":46359},{\"end\":46741,\"start\":46740},{\"end\":46750,\"start\":46749},{\"end\":46758,\"start\":46757},{\"end\":46765,\"start\":46764},{\"end\":46778,\"start\":46777},{\"end\":46789,\"start\":46788},{\"end\":47250,\"start\":47249},{\"end\":47262,\"start\":47261},{\"end\":47273,\"start\":47272},{\"end\":47282,\"start\":47281},{\"end\":47292,\"start\":47291},{\"end\":47303,\"start\":47302},{\"end\":47311,\"start\":47310},{\"end\":47317,\"start\":47316},{\"end\":47330,\"start\":47329},{\"end\":47342,\"start\":47341},{\"end\":47817,\"start\":47816},{\"end\":47826,\"start\":47825},{\"end\":47836,\"start\":47835},{\"end\":47846,\"start\":47845},{\"end\":47857,\"start\":47856},{\"end\":47859,\"start\":47858},{\"end\":47868,\"start\":47867},{\"end\":47878,\"start\":47877},{\"end\":48327,\"start\":48326},{\"end\":48333,\"start\":48332},{\"end\":48339,\"start\":48338},{\"end\":48351,\"start\":48350},{\"end\":48576,\"start\":48575},{\"end\":48584,\"start\":48583},{\"end\":48591,\"start\":48590},{\"end\":48597,\"start\":48596},{\"end\":48605,\"start\":48604}]", "bib_author_last_name": "[{\"end\":30242,\"start\":30228},{\"end\":30253,\"start\":30246},{\"end\":30264,\"start\":30257},{\"end\":30614,\"start\":30608},{\"end\":30780,\"start\":30774},{\"end\":30793,\"start\":30786},{\"end\":30972,\"start\":30965},{\"end\":30981,\"start\":30976},{\"end\":30990,\"start\":30985},{\"end\":31000,\"start\":30994},{\"end\":31011,\"start\":31004},{\"end\":31020,\"start\":31015},{\"end\":31029,\"start\":31024},{\"end\":31043,\"start\":31033},{\"end\":31056,\"start\":31047},{\"end\":31350,\"start\":31345},{\"end\":31357,\"start\":31354},{\"end\":31371,\"start\":31361},{\"end\":31381,\"start\":31375},{\"end\":31393,\"start\":31385},{\"end\":31402,\"start\":31397},{\"end\":31410,\"start\":31406},{\"end\":31418,\"start\":31414},{\"end\":31427,\"start\":31422},{\"end\":31784,\"start\":31775},{\"end\":31797,\"start\":31788},{\"end\":32122,\"start\":32117},{\"end\":32132,\"start\":32126},{\"end\":32143,\"start\":32136},{\"end\":32154,\"start\":32147},{\"end\":32481,\"start\":32476},{\"end\":32500,\"start\":32485},{\"end\":32510,\"start\":32504},{\"end\":32867,\"start\":32864},{\"end\":32873,\"start\":32871},{\"end\":32879,\"start\":32877},{\"end\":32886,\"start\":32883},{\"end\":33423,\"start\":33417},{\"end\":33438,\"start\":33427},{\"end\":33725,\"start\":33718},{\"end\":33732,\"start\":33729},{\"end\":33742,\"start\":33736},{\"end\":33753,\"start\":33748},{\"end\":33763,\"start\":33757},{\"end\":34182,\"start\":34172},{\"end\":34190,\"start\":34186},{\"end\":34571,\"start\":34561},{\"end\":34583,\"start\":34575},{\"end\":34599,\"start\":34591},{\"end\":34607,\"start\":34603},{\"end\":34620,\"start\":34611},{\"end\":34858,\"start\":34850},{\"end\":34868,\"start\":34862},{\"end\":34879,\"start\":34872},{\"end\":34893,\"start\":34883},{\"end\":34907,\"start\":34897},{\"end\":35251,\"start\":35245},{\"end\":35259,\"start\":35255},{\"end\":35268,\"start\":35263},{\"end\":35275,\"start\":35272},{\"end\":35713,\"start\":35707},{\"end\":35722,\"start\":35717},{\"end\":35733,\"start\":35728},{\"end\":35747,\"start\":35737},{\"end\":36016,\"start\":36011},{\"end\":36030,\"start\":36020},{\"end\":36048,\"start\":36034},{\"end\":36057,\"start\":36052},{\"end\":36068,\"start\":36061},{\"end\":36500,\"start\":36498},{\"end\":36509,\"start\":36504},{\"end\":36516,\"start\":36513},{\"end\":36523,\"start\":36520},{\"end\":36863,\"start\":36858},{\"end\":36873,\"start\":36867},{\"end\":36880,\"start\":36877},{\"end\":36887,\"start\":36884},{\"end\":36902,\"start\":36891},{\"end\":36911,\"start\":36906},{\"end\":36922,\"start\":36915},{\"end\":36931,\"start\":36926},{\"end\":36939,\"start\":36935},{\"end\":36953,\"start\":36943},{\"end\":36963,\"start\":36957},{\"end\":37407,\"start\":37400},{\"end\":37413,\"start\":37411},{\"end\":37429,\"start\":37417},{\"end\":37799,\"start\":37792},{\"end\":37809,\"start\":37803},{\"end\":37818,\"start\":37813},{\"end\":37834,\"start\":37822},{\"end\":38212,\"start\":38207},{\"end\":38224,\"start\":38216},{\"end\":38231,\"start\":38228},{\"end\":38245,\"start\":38235},{\"end\":38254,\"start\":38249},{\"end\":38266,\"start\":38258},{\"end\":38276,\"start\":38270},{\"end\":38283,\"start\":38280},{\"end\":38292,\"start\":38287},{\"end\":38303,\"start\":38296},{\"end\":38567,\"start\":38557},{\"end\":38580,\"start\":38571},{\"end\":38592,\"start\":38586},{\"end\":38917,\"start\":38915},{\"end\":38927,\"start\":38921},{\"end\":38939,\"start\":38931},{\"end\":38948,\"start\":38943},{\"end\":38963,\"start\":38952},{\"end\":38969,\"start\":38967},{\"end\":38978,\"start\":38973},{\"end\":38986,\"start\":38982},{\"end\":39001,\"start\":38990},{\"end\":39370,\"start\":39362},{\"end\":39382,\"start\":39374},{\"end\":39394,\"start\":39386},{\"end\":39405,\"start\":39398},{\"end\":39419,\"start\":39409},{\"end\":39728,\"start\":39725},{\"end\":39738,\"start\":39732},{\"end\":39750,\"start\":39742},{\"end\":39756,\"start\":39754},{\"end\":39769,\"start\":39760},{\"end\":39781,\"start\":39773},{\"end\":40127,\"start\":40124},{\"end\":40136,\"start\":40131},{\"end\":40148,\"start\":40140},{\"end\":40156,\"start\":40152},{\"end\":40166,\"start\":40160},{\"end\":40177,\"start\":40170},{\"end\":40187,\"start\":40181},{\"end\":40200,\"start\":40193},{\"end\":40663,\"start\":40660},{\"end\":40675,\"start\":40667},{\"end\":40684,\"start\":40679},{\"end\":40695,\"start\":40688},{\"end\":40703,\"start\":40699},{\"end\":40712,\"start\":40710},{\"end\":40722,\"start\":40718},{\"end\":41147,\"start\":41142},{\"end\":41158,\"start\":41151},{\"end\":41168,\"start\":41162},{\"end\":41181,\"start\":41172},{\"end\":41192,\"start\":41187},{\"end\":41483,\"start\":41473},{\"end\":41493,\"start\":41487},{\"end\":41736,\"start\":41729},{\"end\":41746,\"start\":41740},{\"end\":41758,\"start\":41750},{\"end\":41772,\"start\":41762},{\"end\":41786,\"start\":41776},{\"end\":42184,\"start\":42180},{\"end\":42190,\"start\":42188},{\"end\":42199,\"start\":42194},{\"end\":42205,\"start\":42203},{\"end\":42213,\"start\":42209},{\"end\":42223,\"start\":42217},{\"end\":42235,\"start\":42227},{\"end\":42542,\"start\":42538},{\"end\":42684,\"start\":42678},{\"end\":42695,\"start\":42688},{\"end\":42954,\"start\":42951},{\"end\":42960,\"start\":42958},{\"end\":42972,\"start\":42964},{\"end\":42979,\"start\":42976},{\"end\":43483,\"start\":43477},{\"end\":43493,\"start\":43487},{\"end\":43812,\"start\":43807},{\"end\":43823,\"start\":43818},{\"end\":43838,\"start\":43827},{\"end\":43852,\"start\":43842},{\"end\":43862,\"start\":43856},{\"end\":43869,\"start\":43864},{\"end\":44186,\"start\":44179},{\"end\":44196,\"start\":44190},{\"end\":44206,\"start\":44200},{\"end\":44697,\"start\":44689},{\"end\":44710,\"start\":44701},{\"end\":44939,\"start\":44935},{\"end\":44945,\"start\":44943},{\"end\":44953,\"start\":44949},{\"end\":44964,\"start\":44959},{\"end\":44973,\"start\":44968},{\"end\":44987,\"start\":44977},{\"end\":45325,\"start\":45321},{\"end\":45333,\"start\":45329},{\"end\":45344,\"start\":45339},{\"end\":45353,\"start\":45348},{\"end\":45365,\"start\":45357},{\"end\":45379,\"start\":45369},{\"end\":45853,\"start\":45844},{\"end\":45866,\"start\":45857},{\"end\":45876,\"start\":45870},{\"end\":46343,\"start\":46337},{\"end\":46355,\"start\":46349},{\"end\":46367,\"start\":46361},{\"end\":46747,\"start\":46742},{\"end\":46755,\"start\":46751},{\"end\":46762,\"start\":46759},{\"end\":46775,\"start\":46766},{\"end\":46786,\"start\":46779},{\"end\":46796,\"start\":46790},{\"end\":47259,\"start\":47251},{\"end\":47270,\"start\":47263},{\"end\":47279,\"start\":47274},{\"end\":47289,\"start\":47283},{\"end\":47300,\"start\":47293},{\"end\":47308,\"start\":47304},{\"end\":47314,\"start\":47312},{\"end\":47327,\"start\":47318},{\"end\":47339,\"start\":47331},{\"end\":47353,\"start\":47343},{\"end\":47823,\"start\":47818},{\"end\":47833,\"start\":47827},{\"end\":47843,\"start\":47837},{\"end\":47854,\"start\":47847},{\"end\":47865,\"start\":47860},{\"end\":47875,\"start\":47869},{\"end\":47885,\"start\":47879},{\"end\":48330,\"start\":48328},{\"end\":48336,\"start\":48334},{\"end\":48348,\"start\":48340},{\"end\":48356,\"start\":48352},{\"end\":48581,\"start\":48577},{\"end\":48588,\"start\":48585},{\"end\":48594,\"start\":48592},{\"end\":48602,\"start\":48598},{\"end\":48609,\"start\":48606}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":30142,\"start\":29944},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":60814714},\"end\":30550,\"start\":30144},{\"attributes\":{\"id\":\"b2\"},\"end\":30714,\"start\":30552},{\"attributes\":{\"id\":\"b3\"},\"end\":30919,\"start\":30716},{\"attributes\":{\"doi\":\"arXiv:1711.11017\",\"id\":\"b4\"},\"end\":31278,\"start\":30921},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":21435690},\"end\":31690,\"start\":31280},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":11516429},\"end\":32095,\"start\":31692},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":3525710},\"end\":32366,\"start\":32097},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":51883314},\"end\":32789,\"start\":32368},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":7428689},\"end\":33321,\"start\":32791},{\"attributes\":{\"id\":\"b10\"},\"end\":33646,\"start\":33323},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":2385068},\"end\":34097,\"start\":33648},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":18920307},\"end\":34490,\"start\":34099},{\"attributes\":{\"id\":\"b13\"},\"end\":34769,\"start\":34492},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":2010257},\"end\":35181,\"start\":34771},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":1203247},\"end\":35628,\"start\":35183},{\"attributes\":{\"doi\":\"arXiv:1704.02393\",\"id\":\"b16\"},\"end\":35947,\"start\":35630},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":8622436},\"end\":36448,\"start\":35949},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":206594692},\"end\":36785,\"start\":36450},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":206595627},\"end\":37344,\"start\":36787},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":8421376},\"end\":37686,\"start\":37346},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":4244548},\"end\":38148,\"start\":37688},{\"attributes\":{\"id\":\"b22\"},\"end\":38488,\"start\":38150},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":195908774},\"end\":38835,\"start\":38490},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":52155203},\"end\":39278,\"start\":38837},{\"attributes\":{\"id\":\"b25\"},\"end\":39674,\"start\":39280},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":10716717},\"end\":40074,\"start\":39676},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":14113767},\"end\":40620,\"start\":40076},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":2141740},\"end\":41095,\"start\":40622},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":5328073},\"end\":41415,\"start\":41097},{\"attributes\":{\"doi\":\"arXiv:1608.03983\",\"id\":\"b30\"},\"end\":41650,\"start\":41417},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":53740947},\"end\":42117,\"start\":41652},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":49317780},\"end\":42472,\"start\":42119},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":2783597},\"end\":42674,\"start\":42474},{\"attributes\":{\"doi\":\"arXiv:1804.02767\",\"id\":\"b34\"},\"end\":42867,\"start\":42676},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":10328909},\"end\":43421,\"start\":42869},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":13895986},\"end\":43803,\"start\":43423},{\"attributes\":{\"doi\":\"arXiv:1712.03931\",\"id\":\"b37\"},\"end\":44126,\"start\":43805},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":8777811},\"end\":44617,\"start\":44128},{\"attributes\":{\"doi\":\"arXiv:1409.1556\",\"id\":\"b39\"},\"end\":44878,\"start\":44619},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":20416090},\"end\":45238,\"start\":44880},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":31296482},\"end\":45765,\"start\":45240},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b42\",\"matched_paper_id\":53659095},\"end\":46259,\"start\":45767},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":17255895},\"end\":46644,\"start\":46261},{\"attributes\":{\"id\":\"b44\"},\"end\":46969,\"start\":46646},{\"attributes\":{\"id\":\"b45\"},\"end\":47153,\"start\":46971},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":4929980},\"end\":47782,\"start\":47155},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":12289484},\"end\":48252,\"start\":47784},{\"attributes\":{\"doi\":\"arXiv:1801.02209\",\"id\":\"b48\"},\"end\":48542,\"start\":48254},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":5299559},\"end\":48934,\"start\":48544}]", "bib_title": "[{\"end\":30224,\"start\":30144},{\"end\":31341,\"start\":31280},{\"end\":31771,\"start\":31692},{\"end\":32111,\"start\":32097},{\"end\":32472,\"start\":32368},{\"end\":32860,\"start\":32791},{\"end\":33712,\"start\":33648},{\"end\":34168,\"start\":34099},{\"end\":34846,\"start\":34771},{\"end\":35241,\"start\":35183},{\"end\":36007,\"start\":35949},{\"end\":36494,\"start\":36450},{\"end\":36854,\"start\":36787},{\"end\":37396,\"start\":37346},{\"end\":37788,\"start\":37688},{\"end\":38553,\"start\":38490},{\"end\":38911,\"start\":38837},{\"end\":39721,\"start\":39676},{\"end\":40117,\"start\":40076},{\"end\":40656,\"start\":40622},{\"end\":41138,\"start\":41097},{\"end\":41725,\"start\":41652},{\"end\":42176,\"start\":42119},{\"end\":42534,\"start\":42474},{\"end\":42947,\"start\":42869},{\"end\":43471,\"start\":43423},{\"end\":44175,\"start\":44128},{\"end\":44931,\"start\":44880},{\"end\":45317,\"start\":45240},{\"end\":45840,\"start\":45767},{\"end\":46331,\"start\":46261},{\"end\":47247,\"start\":47155},{\"end\":47814,\"start\":47784},{\"end\":48573,\"start\":48544}]", "bib_author": "[{\"end\":30244,\"start\":30226},{\"end\":30255,\"start\":30244},{\"end\":30266,\"start\":30255},{\"end\":30616,\"start\":30606},{\"end\":30782,\"start\":30770},{\"end\":30795,\"start\":30782},{\"end\":30974,\"start\":30963},{\"end\":30983,\"start\":30974},{\"end\":30992,\"start\":30983},{\"end\":31002,\"start\":30992},{\"end\":31013,\"start\":31002},{\"end\":31022,\"start\":31013},{\"end\":31031,\"start\":31022},{\"end\":31045,\"start\":31031},{\"end\":31058,\"start\":31045},{\"end\":31352,\"start\":31343},{\"end\":31359,\"start\":31352},{\"end\":31373,\"start\":31359},{\"end\":31383,\"start\":31373},{\"end\":31395,\"start\":31383},{\"end\":31404,\"start\":31395},{\"end\":31412,\"start\":31404},{\"end\":31420,\"start\":31412},{\"end\":31429,\"start\":31420},{\"end\":31786,\"start\":31773},{\"end\":31799,\"start\":31786},{\"end\":32124,\"start\":32113},{\"end\":32134,\"start\":32124},{\"end\":32145,\"start\":32134},{\"end\":32156,\"start\":32145},{\"end\":32483,\"start\":32474},{\"end\":32502,\"start\":32483},{\"end\":32512,\"start\":32502},{\"end\":32869,\"start\":32862},{\"end\":32875,\"start\":32869},{\"end\":32881,\"start\":32875},{\"end\":32888,\"start\":32881},{\"end\":33425,\"start\":33413},{\"end\":33440,\"start\":33425},{\"end\":33727,\"start\":33714},{\"end\":33734,\"start\":33727},{\"end\":33744,\"start\":33734},{\"end\":33755,\"start\":33744},{\"end\":33765,\"start\":33755},{\"end\":34184,\"start\":34170},{\"end\":34192,\"start\":34184},{\"end\":34573,\"start\":34559},{\"end\":34585,\"start\":34573},{\"end\":34601,\"start\":34585},{\"end\":34609,\"start\":34601},{\"end\":34622,\"start\":34609},{\"end\":34860,\"start\":34848},{\"end\":34870,\"start\":34860},{\"end\":34881,\"start\":34870},{\"end\":34895,\"start\":34881},{\"end\":34909,\"start\":34895},{\"end\":35253,\"start\":35243},{\"end\":35261,\"start\":35253},{\"end\":35270,\"start\":35261},{\"end\":35277,\"start\":35270},{\"end\":35715,\"start\":35705},{\"end\":35724,\"start\":35715},{\"end\":35735,\"start\":35724},{\"end\":35749,\"start\":35735},{\"end\":36018,\"start\":36009},{\"end\":36032,\"start\":36018},{\"end\":36050,\"start\":36032},{\"end\":36059,\"start\":36050},{\"end\":36070,\"start\":36059},{\"end\":36502,\"start\":36496},{\"end\":36511,\"start\":36502},{\"end\":36518,\"start\":36511},{\"end\":36525,\"start\":36518},{\"end\":36865,\"start\":36856},{\"end\":36875,\"start\":36865},{\"end\":36882,\"start\":36875},{\"end\":36889,\"start\":36882},{\"end\":36904,\"start\":36889},{\"end\":36913,\"start\":36904},{\"end\":36924,\"start\":36913},{\"end\":36933,\"start\":36924},{\"end\":36941,\"start\":36933},{\"end\":36955,\"start\":36941},{\"end\":36965,\"start\":36955},{\"end\":37409,\"start\":37398},{\"end\":37415,\"start\":37409},{\"end\":37431,\"start\":37415},{\"end\":37801,\"start\":37790},{\"end\":37811,\"start\":37801},{\"end\":37820,\"start\":37811},{\"end\":37836,\"start\":37820},{\"end\":38214,\"start\":38205},{\"end\":38226,\"start\":38214},{\"end\":38233,\"start\":38226},{\"end\":38247,\"start\":38233},{\"end\":38256,\"start\":38247},{\"end\":38268,\"start\":38256},{\"end\":38278,\"start\":38268},{\"end\":38285,\"start\":38278},{\"end\":38294,\"start\":38285},{\"end\":38305,\"start\":38294},{\"end\":38569,\"start\":38555},{\"end\":38582,\"start\":38569},{\"end\":38594,\"start\":38582},{\"end\":38919,\"start\":38913},{\"end\":38929,\"start\":38919},{\"end\":38941,\"start\":38929},{\"end\":38950,\"start\":38941},{\"end\":38965,\"start\":38950},{\"end\":38971,\"start\":38965},{\"end\":38980,\"start\":38971},{\"end\":38988,\"start\":38980},{\"end\":39003,\"start\":38988},{\"end\":39372,\"start\":39360},{\"end\":39384,\"start\":39372},{\"end\":39396,\"start\":39384},{\"end\":39407,\"start\":39396},{\"end\":39421,\"start\":39407},{\"end\":39730,\"start\":39723},{\"end\":39740,\"start\":39730},{\"end\":39752,\"start\":39740},{\"end\":39758,\"start\":39752},{\"end\":39771,\"start\":39758},{\"end\":39783,\"start\":39771},{\"end\":40129,\"start\":40119},{\"end\":40138,\"start\":40129},{\"end\":40150,\"start\":40138},{\"end\":40158,\"start\":40150},{\"end\":40168,\"start\":40158},{\"end\":40179,\"start\":40168},{\"end\":40189,\"start\":40179},{\"end\":40202,\"start\":40189},{\"end\":40665,\"start\":40658},{\"end\":40677,\"start\":40665},{\"end\":40686,\"start\":40677},{\"end\":40697,\"start\":40686},{\"end\":40705,\"start\":40697},{\"end\":40714,\"start\":40705},{\"end\":40724,\"start\":40714},{\"end\":41149,\"start\":41140},{\"end\":41160,\"start\":41149},{\"end\":41170,\"start\":41160},{\"end\":41183,\"start\":41170},{\"end\":41194,\"start\":41183},{\"end\":41485,\"start\":41471},{\"end\":41495,\"start\":41485},{\"end\":41738,\"start\":41727},{\"end\":41748,\"start\":41738},{\"end\":41760,\"start\":41748},{\"end\":41774,\"start\":41760},{\"end\":41788,\"start\":41774},{\"end\":42186,\"start\":42178},{\"end\":42192,\"start\":42186},{\"end\":42201,\"start\":42192},{\"end\":42207,\"start\":42201},{\"end\":42215,\"start\":42207},{\"end\":42225,\"start\":42215},{\"end\":42237,\"start\":42225},{\"end\":42544,\"start\":42536},{\"end\":42686,\"start\":42676},{\"end\":42697,\"start\":42686},{\"end\":42956,\"start\":42949},{\"end\":42962,\"start\":42956},{\"end\":42974,\"start\":42962},{\"end\":42981,\"start\":42974},{\"end\":43485,\"start\":43473},{\"end\":43495,\"start\":43485},{\"end\":43814,\"start\":43805},{\"end\":43825,\"start\":43814},{\"end\":43840,\"start\":43825},{\"end\":43854,\"start\":43840},{\"end\":43864,\"start\":43854},{\"end\":43871,\"start\":43864},{\"end\":44188,\"start\":44177},{\"end\":44198,\"start\":44188},{\"end\":44208,\"start\":44198},{\"end\":44699,\"start\":44687},{\"end\":44712,\"start\":44699},{\"end\":44941,\"start\":44933},{\"end\":44947,\"start\":44941},{\"end\":44955,\"start\":44947},{\"end\":44966,\"start\":44955},{\"end\":44975,\"start\":44966},{\"end\":44989,\"start\":44975},{\"end\":45327,\"start\":45319},{\"end\":45335,\"start\":45327},{\"end\":45346,\"start\":45335},{\"end\":45355,\"start\":45346},{\"end\":45367,\"start\":45355},{\"end\":45381,\"start\":45367},{\"end\":45855,\"start\":45842},{\"end\":45868,\"start\":45855},{\"end\":45878,\"start\":45868},{\"end\":46345,\"start\":46333},{\"end\":46357,\"start\":46345},{\"end\":46369,\"start\":46357},{\"end\":46749,\"start\":46740},{\"end\":46757,\"start\":46749},{\"end\":46764,\"start\":46757},{\"end\":46777,\"start\":46764},{\"end\":46788,\"start\":46777},{\"end\":46798,\"start\":46788},{\"end\":47261,\"start\":47249},{\"end\":47272,\"start\":47261},{\"end\":47281,\"start\":47272},{\"end\":47291,\"start\":47281},{\"end\":47302,\"start\":47291},{\"end\":47310,\"start\":47302},{\"end\":47316,\"start\":47310},{\"end\":47329,\"start\":47316},{\"end\":47341,\"start\":47329},{\"end\":47355,\"start\":47341},{\"end\":47825,\"start\":47816},{\"end\":47835,\"start\":47825},{\"end\":47845,\"start\":47835},{\"end\":47856,\"start\":47845},{\"end\":47867,\"start\":47856},{\"end\":47877,\"start\":47867},{\"end\":47887,\"start\":47877},{\"end\":48332,\"start\":48326},{\"end\":48338,\"start\":48332},{\"end\":48350,\"start\":48338},{\"end\":48358,\"start\":48350},{\"end\":48583,\"start\":48575},{\"end\":48590,\"start\":48583},{\"end\":48596,\"start\":48590},{\"end\":48604,\"start\":48596},{\"end\":48611,\"start\":48604}]", "bib_venue": "[{\"end\":30011,\"start\":29944},{\"end\":30328,\"start\":30266},{\"end\":30604,\"start\":30552},{\"end\":30768,\"start\":30716},{\"end\":30961,\"start\":30921},{\"end\":31472,\"start\":31429},{\"end\":31865,\"start\":31799},{\"end\":32215,\"start\":32156},{\"end\":32561,\"start\":32512},{\"end\":32937,\"start\":32888},{\"end\":33411,\"start\":33323},{\"end\":33809,\"start\":33765},{\"end\":34231,\"start\":34192},{\"end\":34557,\"start\":34492},{\"end\":34947,\"start\":34909},{\"end\":35354,\"start\":35277},{\"end\":35703,\"start\":35630},{\"end\":36147,\"start\":36070},{\"end\":36595,\"start\":36525},{\"end\":37035,\"start\":36965},{\"end\":37492,\"start\":37431},{\"end\":37898,\"start\":37836},{\"end\":38203,\"start\":38150},{\"end\":38643,\"start\":38594},{\"end\":39043,\"start\":39003},{\"end\":39358,\"start\":39280},{\"end\":39853,\"start\":39783},{\"end\":40228,\"start\":40202},{\"end\":40750,\"start\":40724},{\"end\":41214,\"start\":41194},{\"end\":41469,\"start\":41417},{\"end\":41863,\"start\":41788},{\"end\":42283,\"start\":42237},{\"end\":42559,\"start\":42544},{\"end\":42747,\"start\":42713},{\"end\":43030,\"start\":42981},{\"end\":43554,\"start\":43495},{\"end\":43953,\"start\":43887},{\"end\":44289,\"start\":44208},{\"end\":44685,\"start\":44619},{\"end\":45047,\"start\":44989},{\"end\":45459,\"start\":45381},{\"end\":45942,\"start\":45882},{\"end\":46432,\"start\":46369},{\"end\":46738,\"start\":46646},{\"end\":47045,\"start\":46971},{\"end\":47440,\"start\":47355},{\"end\":47964,\"start\":47887},{\"end\":48324,\"start\":48254},{\"end\":48688,\"start\":48611},{\"end\":35418,\"start\":35356},{\"end\":36211,\"start\":36149},{\"end\":40284,\"start\":40280},{\"end\":40799,\"start\":40795},{\"end\":41230,\"start\":41216},{\"end\":43600,\"start\":43556},{\"end\":44357,\"start\":44291},{\"end\":45524,\"start\":45461},{\"end\":45989,\"start\":45944},{\"end\":48028,\"start\":47966},{\"end\":48752,\"start\":48690}]"}}}, "year": 2023, "month": 12, "day": 17}