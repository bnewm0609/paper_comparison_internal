{"id": 226239804, "updated": "2022-01-18 12:12:06.692", "metadata": {"title": "Progressive Refinement Network for Occluded Pedestrian Detection", "authors": "[{\"middle\":[],\"last\":\"Song\",\"first\":\"Xiaolin\"},{\"middle\":[],\"last\":\"Zhao\",\"first\":\"Kaili\"},{\"middle\":[\"Chu\",\"Honggang\"],\"last\":\"Zhang\",\"first\":\"Wen-Sheng\"},{\"middle\":[],\"last\":\"Guo\",\"first\":\"Jun\"}]", "venue": "Computer Vision \u2013 ECCV 2020", "journal": "Computer Vision \u2013 ECCV 2020", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "We present Progressive Refinement Network (PRNet), a novel single-stage detector that tackles occluded pedestrian detection. Motivated by human\u2019s progressive process on annotating occluded pedestrians, PRNet achieves sequential refinement by three phases: Finding highconfident anchors of visible parts, calibrating such anchors to a full-body template derived from occlusion statistics, and then adjusting the calibrated anchors to final full-body regions. Unlike conventional methods that exploit predefined anchors, the confidence-aware calibration offers adaptive anchor initialization for detection with occlusions, and helps reduce the gap between visible-part and full-body detection. In addition, we introduce an occlusion loss to up-weigh hard examples, and a Receptive Field Backfeed (RFB) module to diversify receptive fields in early layers that commonly fire only on visible parts or small-size full-body regions. Experiments were performed within and across CityPersons, ETH, and Caltech datasets. Results show that PRNet can match the speed of existing single-stage detectors, consistently outperforms alternatives in terms of overall miss rate, and offers significantly better cross-dataset generalization. Code is available.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "3095638603", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/eccv/SongZCZG20", "doi": "10.1007/978-3-030-58592-1_3"}}, "content": {"source": {"pdf_hash": "2569621f69ab0a787ef67c6402f2b682ea492703", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "25562074062015335425e1935b257d4d7f467862", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/2569621f69ab0a787ef67c6402f2b682ea492703.txt", "contents": "\nProgressive Refinement Network for Occluded Pedestrian Detection\n\n\nXiaolin Song \nBeijing University of Posts and Telecommunications\nBeijingChina\n\nKaili Zhao \nBeijing University of Posts and Telecommunications\nBeijingChina\n\nWen-Sheng Chu \nBeijing University of Posts and Telecommunications\nBeijingChina\n\nHonggang Zhang \nBeijing University of Posts and Telecommunications\nBeijingChina\n\nJun Guo \nBeijing University of Posts and Telecommunications\nBeijingChina\n\nProgressive Refinement Network for Occluded Pedestrian Detection\nOccluded pedestrian detectionProgressive Refinement Net- workanchor calibrationocclusion lossReceptive Field Backfeed\nWe present Progressive Refinement Network (PRNet), a novel single-stage detector that tackles occluded pedestrian detection. Motivated by human's progressive process on annotating occluded pedestrians, PRNet achieves sequential refinement by three phases: Finding highconfident anchors of visible parts, calibrating such anchors to a full-body template derived from occlusion statistics, and then adjusting the calibrated anchors to final full-body regions. Unlike conventional methods that exploit predefined anchors, the confidence-aware calibration offers adaptive anchor initialization for detection with occlusions, and helps reduce the gap between visible-part and full-body detection. In addition, we introduce an occlusion loss to up-weigh hard examples, and a Receptive Field Backfeed (RFB) module to diversify receptive fields in early layers that commonly fire only on visible parts or small-size full-body regions. Experiments were performed within and across CityPersons, ETH, and Caltech datasets. Results show that PRNet can match the speed of existing single-stage detectors, consistently outperforms alternatives in terms of overall miss rate, and offers significantly better cross-dataset generalization. Code is available. 1\n Fig. 1\n. Progressive Refinement Network (PRNet) imitates human's progressive annotation process on occluded pedestrians (e.g., [5,39]), and gradually infers full-body regions from visible parts.\n\nvisibility classifiers to fuse prediction confidence into final scores [22,42,44]. Although these methods could benefit occluded pedestrian detection, at least three issues remain. First, independent detectors are computationally expensive, as each detector is trained for individual occlusion patterns, which are difficult to enumerate in practice. Second, attention-based methods can be slow for inference because attention modules are usually exhausted with proposals in architectures like Faster R-CNN [29]. Attention-based methods emphasize only visible parts, and thus could be suboptimal for full-body detection. Finally, detectors are usually initialized with predefined anchors, which, as will be demonstrated in Sec. 4, are suboptimal to generalize across diverse datasets.\n\nTo address the above challenges, we propose Progressive Refinement Network (PRNet), a novel single-stage detector for occluded pedestrian detection. Fig. 1 illustrates our main idea. Inspired by human's progressive annotation process of occluded pedestrians (e.g., [5,39]), PRNet performs pedestrian detection in three phases. First, visible-part estimation generates high-confident anchors of visible parts from one single-stage detector (e.g., SSD-based [14,18]). Second, anchor calibration adjusts the visible-part anchors to a full-body template according to occlusion statistics, which is derived from over 20,000 annotations of occluded pedestrians. Finally, we train a full-body refiner using the calibrated anchors and a separate detection head from the one for visible-part estimation. Using two separate detection heads allows us to fit the progressive design into a singlestage detector without adding much complexity. In addition, to improve training effectiveness, we introduce an occlusion loss to up-weigh hard examples, and a Receptive Field Backfeed (RFB) module to provide more diverse receptive fields, which help shallow layers to detect pedestrians in various sizes. Experiments on three public datasets, CityPersons [39], ETH [8], and Caltech [5], validate the feasibility of the proposed PRNet. Our contributions in this paper can be summarized as follows:\n\n1. Present a novel Progressive Refinement Network (PRNet) that embodies three-phase progression into a single-stage detector. With helps of the proposed occlusion loss and RFB modules, PRNet achieves competitive results with little extra complexity. 2. Analyze statistically on 20,000 visible-part and full-body regions, and derive an anchor calibration strategy that covers \u223c97% occlusion patterns in both CityPersons and Caltech datasets. 3. Offer comprehensive ablation study, and experiments showing that PRNet achieves state-of-the-art within-dataset performance on R and HO subsets on CityPersons, and the best cross-dataset generalization over ETH and Caltech benchmarks. 4. Provide analysis on extreme occlusions, showing insights behind the metrics and suggesting a realistic evaluation subset for the community.\n\n\nRelated Work\n\nCNN-based Pedestrian Detection: Along with the development of CNNbased object detection, pedestrian detection has achieved promising results. We broadly group these methods into two categories: anchor-based and anchor-free. For anchor-based methods, two-stage detectors (e.g., Faster R-CNN [29]) and one-stage detectors (e.g., SSD [17]) are two common designs. Most twostage detectors [1,2,11,13,27,35,37,[40][41][42]44] generate coarse region proposals of pedestrians and then refine the proposals by exploiting domain knowledge (e.g., hard mining [37], extra learning task [2,27,40,44], or cascaded labeling policy [1]). RPN+BF [37] used a boosted forest to replace second stage learning and leveraged hard mining for proposals. However, involving such downstream classifier could bring more training complexity. SDS-RCNN [2] jointly learned pedestrian detection and bounding-box aware semantic segmentation, thus encouraged model learning more on pedestrian regions. AR-Ped [1] exploited sequential labeling policy in region proposal network to gradually filter out better proposals. These two-stage detectors need to generate proposal in first stage, and thus are slow for inference in practice. On the other hand, single-stage detectors [14,18,22] enjoy real-time inference due to the one-shot design. GDFL [14] included semantic segmentation task from end to end, which guided feature layers to emphasize on pedestrian regions. Generally, detection accuracy and inference time are trade-offs between single-stage and two-stage detectors. To obtain both accuracy and speed, ALFNet [18] involved anchor refinement into SSD training process. The proposed PRNet takes advantage of high speed of single-stage detector, and simultaneously outperforms these conventional methods in consideration of occlusion-aware supervision.\n\nFor anchor-free methods [19,32], topological points of pedestrians and predefined aspect ratio are introduced as new annotations to replace original bbox annotations. TLL [32] predicts the top and bottom vertexes of the somatic topological line while CSP [19] predicts central points and scales of pedestrian in-stances. Although the above CNN-based pedestrian detectors obtains potential performance, occluded pedestrian detection is still a challenging problem.\n\nOccluded Pedestrian Detection: Methods tackling occluded pedestrians can be broadly categorized into four types: part-based, attention-based, scorebased, and crowd-specific. Part-based methods have been widely received in the community, where each detector was separately trained for individual occlusion pattern with inference done by fusing all predictions. See [6, 7, 20, 23-25, 30, 34, 36,43] for comprehensive reviews. Moreover, exhaustively enumerating occlusion patterns is non-practical, computationally expensive, and generally infeasible. Instead of considering each occlusion pattern, [22,41] partitioned a proposal or bounding box into fixed number parts and predict their visibility scores. Although training complexity was decreased, these methods still require manually designing the partitions.\n\nIn recent years, learning robust representations and better anchor scoring have become a popular topic. On one hand, attention-based methods [27,40] learn robust features using guidance from attention maps. Zhang et al. [40] and MGAN [27] exploited channel-wise and pixel-wise attention maps respectively in feature layers, to highlight visible parts and suppress occluded parts. However, emphasizing visible-parts solely could be sub-optimal for full-body prediction. On the other hand, score-aware methods learn extra anchor scores by introducing additional learning task in the second stage of Faster R-CNN. For instance, Bi-box [44] constructs two classification and regression tasks for visible-part and full-body anchors, and then fuses the two anchor scores during inference. Similarly, [42] uses a separate discriminative classification by enforcing heavily occluded anchors to be close to easier anchors, and high confident scores were obtained for anchors. In addition, other studies [16,26,28,33,35] focused on tackling crowded pedestrians. RepLoss [35] designs a novel regression loss to prevent target proposals from shifting to surrounding pedestrians. The aforementioned methods are generally initialized with predefined anchors. In contrast, the proposed PRNet learns occluded pedestrian detection under confidence-aware and adaptive anchor initialization, which helps improve detection accuracy and generalization across dataset.\n\n\nProgressive Refinement Network (PRNet)\n\n\nPRNet Architecture\n\nMotivated by human's progressive process on annotating occluded pedestrians (e.g., CityPersons [39] and Caltech [5]), we construct PRNet to gradually migrate high-confident detection on visible parts toward more challenging full-body localization. For this purpose, we propose to adopt a single-stage detector with three training phases: Visible-part Estimation (VE), Anchor Calibration (AC), and Full-body Refinement (FR). Unlike most methods that detect full bodies only [18,35] or independently with visible parts [44], we interweave them into a single-stage framework. To bridge the detection gap between visible parts and full bodies, we introduce AC to align anchors from VE to FR.     [12] with modification of appending 1 extra stage with 3x3 filters and stride 2, which provide diverse receptive fields and help capture pedestrian with various scales. Out of the 6-stage backbone, we treat the last four as detection outputs. The network is trained following three phases: Visible-part Estimation (VE), Anchor Calibration (AC), and Full-body Refinement (FR). VE and FR are trained with visible-part and full-body ground truth, respectively; AC leverages occlusion statistics to bridge the gap between visible-part anchors and full-body anchors. Details of each module are illustrated later in this section. On top of each detection layer, we attach a detection head (DH) separately for VE and FR.\n\nSpecifically, denote x as an input image, \u03a6(x) as feature maps from backbone, A 0 as a set of predefined anchors (as in SSD [17]), B * as the predicted bounding boxes that are obtained by post-processing anchors collected from all layers (i.e., via Non-Maximum Suppression). Given an initial set of feature maps and anchors, PRNet can be formulated as a progressive detector: where\nDetections = F (E f (C(E v (\u03a6(x), A 0 )))) = {B * , s * },(1)E v (\u03a6(x), A 0 )\nis the 1st-phase visible-part estimation (VE) whose outputs are a set of visible-part anchors and confidence scores {A 1 , s 1 }, C(\u00b7) is the 2ndphase anchor calibration (AC) that aligns visible-part anchors A 1 to full-body anchors A 2 , and E f (\u03a6(x), A 2 ) is the 3rd-phase full-body refiner (FR) that outputs the final full-body anchors to compute B * and their scores s * using inference F (see Sec. 3.2). Note that \u03a6(x) represents different feature maps during VE and FR due to their complementary objectives. Below we discuss each phase in turn.\n\n\nVisible-part Estimation (VE):\n\nTo train the visible-part estimation E v (\u00b7), we adopt a standard detection approach that learns to localize anchors A 1 as regression (from predefined anchors A 0 ), and anchor scores as classification. Fig. 3(a) depicts the detection head, whose loss can be written as:\nL V E = L f ocal + \u03bb v [y = 1]L smoothL1 ,(2)\nwhere L f ocal is focal loss [15] for classification, L smoothL1 is a smooth-L1 loss for regression (as adopted in Faster R-CNN [29]), [y = 1] is an indicator for positive samples, and \u03bb v is a tuning parameter. As VE is trained on visible parts, its prediction (i.e., A 1 ) on visible parts is generally more confident and accurate than detectors trained with occlusions. Anchor Calibration (AC): After VE obtains confident visible-part anchors A 1 , we propose a simple and effective anchor calibration C(\u00b7) to migrate visiblepart anchors toward full-body anchors A 2 , which are then passed to the next phase for bull-body refinement. Briefly, PRNet updates anchors as:\nA 0 Ev \u2212 \u2212 \u2192 A 1 C \u2212 \u2192 A 2 .\nThree are our motivations: 1. The aspect ratio of visible-part boxes is much more diverse than that of fullbody boxes [5,39], making regression from visible-part to full-body boxes rather challenging. 2. Adaptive anchor initialization can reduce unnecessary search space and lead to better detection (e.g., [3]), compared to most methods that use predefined anchors (e.g., [18,39,41,42]). 3. The IoU discrepancy between visible-part anchors and full-body ground truth boxes is large; proper calibration can significantly improve IoU. Fig. 4 shows the distribution of IoU between ground truth full-body boxes and visible-part boxes before/after Anchor Calibration (AC) in CityPersons dataset [39]. The visible-part boxes before AC were taken from the annota-IoU Distribution 8   tions in the original dataset. As can be seen, calibration significantly shifts the distribution toward higher IoU, e.g., +21% for IoU in (0.8, 1.0], and thus can help detectors approximate final full-body regions. In addition, AC addresses discrepancy during anchor assignment between VE and FR, i.e., without AC, a positive A 1 could be assigned as a negative anchor for FR, making VE and FR fail to complement each other. To achieve AC, we first derive a statistical analysis of occlusion patterns on two popular datasets CityPersons [39] and Caltech [5] using their standardized 0.41 box aspect ratio. Please see supplementary materials for detailed process. Fig. 5 illustrates occlusion distribution over a full-body box and four occlusion types (i.e., horizontal, vertical, non-occlusion, and others, similar to [40]) with respective likelihood in each dataset. As can be seen in Fig. 5(a), over the two datasets, the upper box is consistently visible (i.e., the head), with most occlusions appearing in the lower box (i.e., the feet). This serves as strong evidence for humans and PRNet to leverage visible parts for full-body detection.\n(b) (c) (d) (e) Caltech (a) (b) (c) (d) (e)\nObserving the occlusion statistics, we reach two types of anchor updates according to the aspect ratio of A 1 , as depicted in Fig. 3(b). For the anchors with ratio >0.41, we vertically stretch them downwards until 0.41 aspect ratio, due to heads being frequently visible, as shown in Fig. 5(b) and [5]. Anchors with ratio <0.41 are horizontally extended to 0.41 w.r.t. the center of A 1 , as they likely involve vertical occlusion, as shown in Fig. 5(c). Anchors with 0.41 ratio (i.e., Fig. 5(d)) remain unchanged. The anchor updates can also be rationalized with human's annotation protocol in CityPersons [39], where a full-body box is generated by fitting a fixed-ratio (0.41) box onto a line drawn from head to feet. According to Fig. 5(b)-(d), we justify the two simple updates can cover \u223c97% data in both datasets, while the remaining \u223c3% is shown in Fig. 5(e).\n\nFull-body Refinement (FR): With the calibrated anchors A 2 from AC, PRNet's last phase trains a full-body refiner E f (\u00b7) that refines the final fullbody localization. Similar to VE, FR also uses the same backbone, yet performs training on a separate detection head. Different from VE that sees only visible parts, FR starts to see hard positive samples whose anchor boxes are still far from ground truth full-body region. As L smoothL1 in Eq. (2) treats every positive sample equally, it could be less effective when dealing with hard samples in FR.\n\nTo encourage learning on hard positive samples, we weigh the regression loss L smoothL1 with an occlusion weight, which is defined as a reverse IoU between A 2 and ground truth full-body boxes B gt . Given a \u2208 A 2 and its corresponding b \u2208 B gt , the weighted loss, termed as occlusion loss, can be rewritten as:\nL occ = a\u2208A2 (1 \u2212 IoU(a, b)) [|s| < 1] 0.5s 2 + [|s| >= 1] (|s| \u2212 0.5) ,(3)\nwhere s is the difference between predicted offsets and ground truth offsets (see [29] for details). The less overlap between the calibrated anchors A 2 and B gt , the higher L occ is. As a result, the loss for FR becomes:\nL F R = L f ocal + \u03bb f [y = 1]L occ .(4)\nDespite of up-weighting hard positive anchors, another challenge in FR regards training shallow layers, which often activate on visible parts or small-size full-body regions due to limited receptive field. In every layer of FR, we introduce a Receptive Field Backfeed (RFB) module to diversify receptive fields, as depicted in Fig. 3(c). RFB aims to enlarge the receptive fields of shallower layers by back-feeding features from deeper layers to the previous layer with 2X upsampling, and then summing up their feature maps in a pixelwise manner. Fig. 6 shows the saliency maps [31] of the 2nd layer (denoted as \"shallow\") and the 3rd layer (i.e., \"deep\") with/without the RFB module. As can be seen in Fig. 6(a), without RFB, visible parts are identified in the shallow layer, while the deeper layer emphasizes full-body regions. The effects of RFB can be clearly observed in Fig. 6(b). In the shallow layer, RFB not only enhances visible parts but also complements the full-body region. Similar observation can be made on the deep layer, showing that RFB can propagate larger receptive fields to shallower layers and help refine full-body detection.\n\n\nTraining & Inference\n\nTraining: In training, a batch of pedestrian images goes through the three phases (i.e., VE, AC, and FR) sequentially-the first phase VE is trained independently and then the first detection head is frozen to train FR. Fig. 2 illustrates the architecture and examples of pedestrian annotation. Given predefined anchors A 0 and visible-part ground truth boxes associated to the image batch, we first train VE with loss L V E in Eq. (2), and obtain visible-part anchors A 1 . Then AC transforms A 1 into more adaptive anchors A 2 , which better approximates full-body regions. Finally, initialized with A 2 , FR is trained with loss L F R in Eq. (4). Note that VE and FR use two different detection heads in one single-stage detector, so they learn complementary outputs.\n\nAn anchor is assigned as positive if intersection-over-union (IoU) between an anchor bbox and ground truth bbox is above a threshold \u03b8 p , as negative if IoU is lower than \u03b8 n , and otherwise ignored during training. Note that VE and FR adopt different annotation of boxes, i.e., VE consumes visible-part boxes, while FR uses full-body boxes.\n\nInference: In inference, we obtain predicted anchor boxes from FR, and associate anchor scores by multiplying the scores from VE and FR. The score fusion provides complementary guidance so to improve detection robustness (similar to [44]). We obtain the final bounding boxes B * by first filtering out candidate anchor boxes with scores lower than 0.05 and then merging them with NMS (0.5 threshold is used here).\n\n\nComparisons with Related Work\n\nThe closest studies to PRNet are ALFNet [18] and Bi-box [44]. As most cascade designs, ALFNet tackles successively the same task (FR\u2192FR), which requires occlusion patterns to be extensively illustrated in training data. Mimicking human's annotation process, PRNet exploits different tasks (VE\u2192AC\u2192FR), starting from detecting only visible parts (regardless of occlusion patterns as in full-body boxes), and thus relaxes training data requirements. Note that jointly tackling different tasks is non-trivial. Instead, we interweave these tasks with occlusion loss and the RFB module (Sec. 3.1) to up-weigh hard samples and facilitate training for shallow layers. As can be seen in Sec. 4.4, PRNet achieved impressive cross-dataset generalizability compared to ALFNet, showing that the PRNet structure is more effective. Similar to ALFNet [18], PRNet enjoys competitive inference time due to the use of a single-stage detector.\n\nIn terms of involving different tasks, Bi-box [44] also takes visible parts into account but by training a two-branch detector for visible parts and full body in the second stage of Faster R-CNN. During training, there is no interaction between the two branches, making their complementary benefits relatively indirect. PRNet leverages the hybrid cascade structure to progressively refine predictions from visible-part to full-body regions, providing adaptive anchor initialization to achieve the final full-body estimation.\n\n\nExperiments\n\n\nSettings\n\nDatasets: We conducted experiments on three public datasets: CityPersons [39], ETH [8], and Caltech [5]. CityPersons [39] has high-res 2048\u00d71024 images with visible-part and full-body annotations, where 2,975 images are for training and 500 for validation. We trained PRNet on the training set and reported performance on the validation set in ablations and within-dataset experiments. To evaluate model generalizability, we performed cross-dataset analysis using ETH [8] and Caltech [5]. ETH dataset [8] contains 11,941 labeled persons, providing a benchmark in evaluating model's robustness to occluded pedestrians. For Caltech [5], we adopted published test set with 4,024 images with both old [5] and new annotations [38]. Both ETH and Caltech have lower-res 640\u00d7480 images that represent more cross-dataset challenges. Following [35,40], we performed training and evaluation on pedestrians with height larger than 50 pixels. Metrics: Evaluation was reported on the standard MR \u22122 (%) [5], which computes the log-average miss rate at 9 False Positive Per Image (FPPI  [35]. To complement the visibility range covered by R and HO, we added EO (extreme occlusion) to represent visibility in [0, 0.2].\n\nImplementation details: We augmented our pedestrian images following standard techniques [18,19]. When assigning labels to anchor boxes, \u03b8 p = 0.5 and \u03b8 n = 0.3 for VE, and \u03b8 p = 0.7 and \u03b8 n = 0.5 for FR. We set \u03bb v = 1 and \u03bb f = 4 empirically. The backbone ResNet-50 is pre-trained on ImageNet [4]. PRNet is then fine-tuned with 160k iterations, a learning rate of 10 \u22124 , batch size 8 and an Adam optimizer. All experiments were performed on 2 GTX 1080Ti GPUs.\n\n\nAblation Study\n\nTo analyze PRNet, we performed extensive ablations on CityPersons validation set [39] using subsets of R (reasonable) and HO (heavy occlusion).\n\nThree-phase components: To analyze the effect of PRNet's three-phase design, we performed ablation study on each phase without occlusion loss and RFB module in FR. In Table 1, we trained a standalone FR (denoted as PRNet-F) initialized by predefined full-body anchors. PRNet-VA used only VE+AC, treating calibrated anchors A 2 as the detection outputs. PRNet-VAF employed all three phases (VE+AC+FR), using calibrated anchors A 2 to initialize FR. Comparing PRNet-F and PRNet-VA, PRNet-VA performs 3.9 points better in R while 5.6 points worse in HO. This shows that plain calibrated anchors A 2 in PRNet-VA can achieve better result while occlusion level is reasonable. In contrast, PRNet-F better addressed heavy occlusions. PRNet-VAF combines the benefits from both, showing a consistent improvement over both R and HO. Please see supplementary for detection examples of the three-phase progression.\n\nAnchor calibration vs. box regression: A possible alternative to AC is a box regressor from the visible-part anchors A 1 to full-body boxes. Here we reused FR for the regression task. To perform a fair comparison, we implemented PRNet-VRF by replacing AC with the regressor. Table 1 summarizes the results. As can be seen, PRNet-VAF consistently outperformed PRNet-VRF by 9.5% in R, showing no significant benefits of adding an extra box regressor. An explanation can be that the visible boxes change rapidly due to various occlusion types, and make the regressor hard to map the coordinates to full-body boxes with relatively constant aspect ratio. Unlike a regression network that require extra complexity and training efforts, AC provides a more generalizable strategy that better fits into the proposed three-phase approach.\n\nOcclusion loss and RFB: Table 2 studies PRNet w/ and w/o occlusion loss and RFB module. PRNet-VAF was reused as the baseline that considers neither occlusion loss nor RFB, and compared against PRNet-VAF-OCC (with only occlusion loss) and PRNet-VAF-RFB (with only RFB). Including occlusion loss alone, PRNet-VAF-OCC improved the baseline 0.4 points on R yet lowered 0.4 points on HO. This shows that occlusion loss improves detection with reasonable occlusion (i.e., over 0.65 visibility), yet could be insufficient to address heavy occlusion (i.e., 0.2 to 0.65 visibility). Including RFB alone, PRNet-VAF-RFB improved the baseline 0.4 points on HO yet lowered 0.2 points on R. This suggests that the feedback from RFB could supply full-body info by enlarging receptive field, and thus offers improvement when occlusion is severe. Otherwise, when occlusion level is light, enlarging receptive field may introduce unnecessary context and hence slightly hurt. PRNet couples occlusion loss and RFB, achieving significant improvement over R and especially HO.\n\n\nWithin-dataset Comparisons\n\nThis section compares PRNet in a within-dataset setting against 3 types of alternatives: Occlusion-free, occlusion-aware, and closest to PRNet. We reported MR \u22122 on all 6 subsets, where R is the major evaluation criteria in CityPersons Challenge 2 . Table 3 shows comparisons with scale \u00d71 and \u00d71.3 of original resolution (2048\u00d71024).\n\nOcclusion-free methods: Occlusion-free methods aim to detect pedestrians without considering occlusion info. Adapted FasterRCNN [39] is an anchor-based benchmark, while TLL+MRF [32] and CSP [19] are anchor-free. Among the three methods, CSP achieved the state-of-the-art results without considering occlusion, as summarized in Table 3. PRNet, on the other hand, takes occlusion info into account, and provides performance gain over CSP on R, Partial, and Table 3. Comparisons on CityPersons [39]. Results of alternatives were obtained from original paper. On scale\u00d71, bracketed and bold numbers indicate the best and the second best results, respectively. Inference time (sec) is measured on scale\u00d71 images.\n\n\nMethod\n\nOcc Bare subsets, but not the Heavy subset. One possible reason is because CSP used box-free annotations, which is different from the original annotations and might help reduce ground truth noises in heavily occluded cases.\n\nOcclusion-aware methods: Occlusion-aware methods consider occlusion information in training, including FasterRCNN+ATT [40], RepLoss [35], OR-CNN [41], FRCN+A+DT [42], and MGAN [27]. Table 3 summarizes the results. On the R subset (CityPersons' evaluation criteria), occlusion-aware methods are generally better than occlusion-free methods, except for CSP that used different box-free annotations. In contrast, PRNet consistently achieved the best MR \u22122 of (10.8, 42.0, 25.6, 10.0) on (R, HO, R+HO, Partial) and compared favorably with the best performer for Bare. The comparisons firmly validate PRNet's effectiveness by dealing with occlusion using progressive refinement.\n\nClosest alternatives: Closest to PRNet are ALFNet [18] and Bi-box [44] per discussion in Sec. 3.3. We reported ALFNet results using the same settings and the authors' released code. We did not reproduce Bi-box due to lack of source code. Regarding inference time, PRNet performed comparably with ALFNet, as both methods are single-stage based. We infer that PRNet is substantially faster than Bi-box due to the Faster-RCNN-like design in Bi-box (e.g., 2-6X speedup as demonstrated in [1,18]). Compared to ALFNet and Bi-box, PRNet is also preferred in detection performance because of better anchor initialization and its ability to recover full-body region from confident visible parts. Due to space constraint, please refer to supplementary for examples that are mis-detected by alternative methods but successfully detected by PRNet. Observing the last three rows in Table 3, PRNet consistently outperformed Bi-box and provided performance gain upon ALFNet in all cases except for the Heavy subset.\n\nBreakdowns in Heavy: In the Heavy subset, we noticed the occlusionaware methods, including PRNet, were less effective than occlusion-free methods (e.g., ALFNet). We performed an analysis by partitioning Heavy into HO and EO, i.e., Heavy=HO \u222a EO. EO represents the most extreme occlusion with visibility in only [0, 0.2]. In HO, PRNet outperformed all other methods (e.g., 1.8 Table 4. Cross-dataset on ETH [8].   Fig. 7 shows the distribution of visibility ratio and examples of EO from CityPersons validation set. As can be seen, visible parts are barely visible and sometimes very low-res, making it perceptually challenging even for human to detect. Ground truth boxes by human annotators in EO can thus be noisy and make performance comparisons on EO less meaningful. In addition, the proportion of EO is relatively small. As shown in top-left of Fig. 7, less than 10% are in EO and more than 90% belong to R and HO. These findings reveal that R and HO render more realistic occlusion scenarios than EO. The above analyses suggest the proposed PRNet achieved state-of-the-art performance.\n\n\nCross-dataset Generalization\n\nTo validate generalizability of the proposed method, we performed cross-dataset experiments on ETH [8] and Caltech [5] datasets. For comparison, we picked two top-performing methods, CSP [19] and ALFNet [18], where the models are available from the authors' GitHub release. For fair comparisons, PRNet was also trained on CityPersons training set and shared the same pre-processing. Table 4 shows ETH results on the R+HO as in [40]. For reference, we also included numbers reported in the Faster-RCNN and FasterRCNN+ATT [40] without reproducing their results. CSP and ALFNet showed surprising opposite results comparing their performance within-and cross-dataset. In cross-dataset setting, ALFNet outperformed CSP by 6.1 points (from 37.2 to 31.1), while CSP reported consistently better performance in within-dataset setting (see Table 3). On the contrary, our method achieved the state-of-the-art MR \u22122 on R+HO by a significant margin. For Caltech [5], we reported R+HO and R using the old [5] (denoted as \"(o)\") and the new [38] annotations (denoted \"(n)\"), as summarized in Table 5. PRNet consistently outperformed CSP and ALFNet in R using both old and new annotations. On R+HO, PRNet performed comparably with CSP.\n\nRationale: PRNet's gain is evident in cross-dataset settings for two major reasons. One, PRNet's progressive structure imitates human's annotation process, which formulates the principles humans have established for annotating occluded pedestrians (e.g., CityPersons, Caltech). PRNet mimics every step in human's principles, and thus fits the problem more naturally. Two, most methods (e.g., ALFNet) consider only full body detection, which demands training data with various occlusions (e.g., cars, trees, other pedestrians). When the occlusion pattern is rare or unseen in training data (i.e., cross-dataset settings), such methods tend to perform less favorably. As shown in supplementary, ALFNet tends to fire false positives on uncommon objects (e.g., wheel, car windshield). On the contrary, PRNet propagates detection from visible parts (regardless of occlusion patterns as in full-body boxes), and thus provides better generalizability.\n\n\nConclusion\n\nWe have proposed PRNet, a novel one-stage approach for occluded pedestrian detection. PRNet incorporates three phases (VE, AC, and FR) to evolve anchors toward full-body localization. We introduced an occlusion loss to encourage learning on hard samples, and an RFB module to diversify receptive fields for shallow layers. We provided extensive ablation studies to justify the threephase design. Within-dataset experiments validated PRNet's effectiveness with 6 occlusion scenarios. On cross-dataset settings, PRNet outperformed alternatives on ETH and Caltech datasets by a noticeable margin. Analysis on extreme occlusions provided insights behind metrics and suggested a more realistic choice for evaluation. Potential extensions of PRNet include providing weak annotations of visible parts for occluded pedestrian datasets. Acknowledgement. Research reported in this paper was supported in part by the Natural Science Foundation of China under grant 61701032 to KZ, and Beijing Municiple Science and Technology Commission project under Grant No.Z181100001918005 to XS and HZ. We thank Jayakorn Vongkulbhisal for helpful comments.\n\nFig. 2 .\n2Architecture of PRNet. From top to bottom, PRNet uses a detection backbone illustrated with four blocks of features maps. The network is trained in three phases: Visible-part Estimation (VE), Anchor Calibration (AC), and Full-body Refinement (FR). VE and FR take visible-part and full-body ground truth as references, respectively. Given initial anchors (A0), VE learns to predict visible-part anchors (A1), which are improved by AC to obtain calibrated anchors (A2). Final detection is obtained by post-processing anchors and scores from VE and FR. Detection Head (DH), Calibrator, and RFB modules are depicted inFig. 3and detailed in Sec. 3.1.\n\nFig. 2\n2illustrates the PRNet architecture. The top row depicts the backbone, where we truncated first 5 stages of ResNet-50\n\nFig. 3 .\n3Modules used in PRNet architecture (as inFig. 2): DH, Calibrator and RFB.\n\nFig. 5 .\n5Occlusion statistics from CityPersons[39] (left) and Caltech[5] (right): (a) Occlusion statistics with blue indicating occlusion; red indicates visible parts, (b) Horizontal occlusions, (c) Vertical occlusions, (d) Nonocclusion, (e) Others. Percentage (%) denotes the likelihood of each occlusion pattern.\n\nFig. 6 .\n6Saliency maps highlighted by the third FR phase: (a) w/o RFB and (b) w/ RFB. \"Shallow\" indicates the 2nd layer, and \"Deep\" indicates the 3rd layer.\n\nFig. 7 .\n7Distribution of visibility ratio on CityPersons (top left), and examples of Extreme Occlusion (EO), such as partial head, arm, leg, and torso. Blue and green boxes indicate visible parts and full-body boxes, respectively.\n\n\nCityPerson 19.3% 12.2% 65.2% 3.3% 58.4% 13.9% 24.8% 2.9%3 \n9 5 \n12 \n7 \n\n21 \n14 \n\n50 \n\n71 \n\nFig. 4. IoU distribution be-\nfore and after anchor cali-\nbration on the CityPersons \ndataset. IoU is measured \nbetween anchors and full-\nbody ground truth. \n\n(a) \n\nvis \nocc \n\n\n\nTable 1 .\n1Ablations of three-phase components and an alternative.Table 2. Ablations of occlusion loss and the RFB module.Architecture \nVE AC FR R HO \n\nPRNet-F \n15.6 45.7 \nPRNet-VA \n11.7 51.3 \nPRNet-VAF \n11.4 45.3 \n\nPRNet-VRF \nreg \n12.6 44.7 \n\nArchitecture \n+Occ. +RFB \nR \nHO \n\nPRNet-VAF \n11.4 45.3 \nPRNet-VAF-OCC \n11.0 45.7 \nPRNet-VAF-RFB \n11.6 44.9 \nPRNet (ours) \n10.8 42.0 \n\n\n\n\n). The lower MR \u22122 , the better. To ensure the results are directly comparable with the literature, we represented each test set as 6 subsets according to visibility ratio of each pedestrian. Specifically, we reported R (reasonable occlusion with visibility in [0.65,1]), HO (heavy occlusion with [0.2, 0.65]), R+HO with [0.2, 1] from Zhang et al [40], and Bare with [0.9, 1.0], Partial with [0.65, 0.9], and Heavy with [0, 0.65] from\n\nTable 5 .\n5Cross-dataset results on Caltech[5].points better than the state-of-the-art ALFNet), while being 1.4 points worse than ALFNet in Heavy. We hypothesize that PRNet fails to compete against ALFNet only in EO, and re-evaluated their performance on EO. Not surprisingly, PRNet and ALFNet result in very high MR 2 at 80.8 and 70.2 respectively.Method \nR (o) R+HO (o) R (n) Time \n\nALFNet [18] \n25.0 \n35.0 \n19.0 \n39.2 \nCSP [19] \n20.0 \n[27.8] \n11.7 \n61.3 \nPRNet (ours) [18.3] \n28.4 \n[10.7] \n42.1 \n\n\nhttps://bitbucket.org/shanshanzhang/citypersons/\n\nPedestrian detection with autoregressive network phases. G Brazil, X Liu, CVPRBrazil, G., Liu, X.: Pedestrian detection with autoregressive network phases. In: CVPR (2019)\n\nIlluminating pedestrians via simultaneous detection & segmentation. G Brazil, X Yin, X Liu, Brazil, G., Yin, X., Liu, X.: Illuminating pedestrians via simultaneous detection & segmentation. In: ICCV (2017)\n\nSelective refinement network for high performance face detection. C Chi, S Zhang, J Xing, Z Lei, S Z Li, X Zou, AAAIChi, C., Zhang, S., Xing, J., Lei, Z., Li, S.Z., Zou, X.: Selective refinement network for high performance face detection. In: AAAI (2019)\n\nJ Deng, W Dong, R Socher, L Li, Kai Li, Li Fei-Fei, Imagenet: A large-scale hierarchical image database. CVPRDeng, J., Dong, W., Socher, R., Li, L., Kai Li, Li Fei-Fei: Imagenet: A large-scale hierarchical image database. In: CVPR (2009)\n\nPedestrian detection: An evaluation of the state of the art. P Doll\u00e1r, C Wojek, B Schiele, P Perona, TPAMI. 34Doll\u00e1r, P., Wojek, C., Schiele, B., Perona, P.: Pedestrian detection: An evaluation of the state of the art. TPAMI 34 (2012)\n\nA structural filter approach to human detection. G Duan, H Ai, S Lao, ECCVDuan, G., Ai, H., Lao, S.: A structural filter approach to human detection. In: ECCV (2010)\n\nMulti-cue pedestrian classification with partial occlusion handling. M Enzweiler, A Eigenstetter, B Schiele, D M Gavrila, CVPREnzweiler, M., Eigenstetter, A., Schiele, B., Gavrila, D.M.: Multi-cue pedestrian classification with partial occlusion handling. In: CVPR (2010)\n\nDepth and appearance for mobile scene analysis. A Ess, B Leibe, L Van Gool, ICCVEss, A., Leibe, B., Van Gool, L.: Depth and appearance for mobile scene analysis. In: ICCV (2007)\n\nVision meets robotics: The kitti dataset. A Geiger, P Lenz, C Stiller, R Urtasun, The International Journal of Robotics Research. 32Geiger, A., Lenz, P., Stiller, C., Urtasun, R.: Vision meets robotics: The kitti dataset. The International Journal of Robotics Research 32 (2013)\n\nAre we ready for autonomous driving? the kitti vision benchmark suite. A Geiger, P Lenz, R Urtasun, CVPRGeiger, A., Lenz, P., Urtasun, R.: Are we ready for autonomous driving? the kitti vision benchmark suite. In: CVPR (2012)\n\nR Girshick, Fast R-CNN. In: ICCV. Girshick, R.: Fast R-CNN. In: ICCV (2015)\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, CVPRHe, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR (2016)\n\nScale-aware fast R-CNN for pedestrian detection. J Li, X Liang, S Shen, T Xu, J Feng, S Yan, TMM. 20Li, J., Liang, X., Shen, S., Xu, T., Feng, J., Yan, S.: Scale-aware fast R-CNN for pedestrian detection. TMM 20 (2018)\n\nGraininess-aware deep feature learning for pedestrian detection. C Lin, J Lu, G Wang, J Zhou, ECCVLin, C., Lu, J., Wang, G., Zhou, J.: Graininess-aware deep feature learning for pedestrian detection. In: ECCV (2018)\n\nFocal loss for dense object detection. T Lin, P Goyal, R Girshick, K He, P Dollar, CVPRLin, T., Goyal, P., Girshick, R., He, K., Dollar, P.: Focal loss for dense object detection. In: CVPR (2017)\n\nAdaptive nms: Refining pedestrian detection in a crowd. S Liu, D Huang, Y Wang, CVPRLiu, S., Huang, D., Wang, Y.: Adaptive nms: Refining pedestrian detection in a crowd. In: CVPR (2019)\n\nSSD: Single shot multibox detector. W Liu, D Anguelov, D Erhan, C Szegedy, S Reed, C Y Fu, A C Berg, ECCVLiu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.Y., Berg, A.C.: SSD: Single shot multibox detector. In: ECCV (2016)\n\nLearning efficient single-stage pedestrian detectors by asymptotic localization fitting. W Liu, S Liao, W Hu, X Liang, X Chen, ECCVLiu, W., Liao, S., Hu, W., Liang, X., Chen, X.: Learning efficient single-stage pedestrian detectors by asymptotic localization fitting. In: ECCV (2018)\n\nHigh-level semantic feature detection: A new perspective for pedestrian detection. W Liu, S Liao, W Ren, W Hu, Y Yu, CVPRLiu, W., Liao, S., Ren, W., Hu, W., Yu, Y.: High-level semantic feature detection: A new perspective for pedestrian detection. In: CVPR (2019)\n\nHandling occlusions with franken-classifiers. M Mathias, R Benenson, R Timofte, L V Gool, ICCVMathias, M., Benenson, R., Timofte, R., Gool, L.V.: Handling occlusions with franken-classifiers. In: ICCV (2013)\n\nPerformance evaluation of object detection algorithms for video surveillance. J C Nascimento, J S Marques, TMM. 8Nascimento, J.C., Marques, J.S.: Performance evaluation of object detection algo- rithms for video surveillance. TMM 8 (2006)\n\nImproving occlusion and hard negative handling for single-stage pedestrian detectors. J Noh, S Lee, B Kim, G Kim, CVPRNoh, J., Lee, S., Kim, B., Kim, G.: Improving occlusion and hard negative handling for single-stage pedestrian detectors. In: CVPR (2018)\n\nA discriminative deep model for pedestrian detection with occlusion handling. W Ouyang, X Wang, CVPROuyang, W., Wang, X.: A discriminative deep model for pedestrian detection with occlusion handling. In: CVPR (2012)\n\nJoint deep learning for pedestrian detection. W Ouyang, X Wang, ICCVOuyang, W., Wang, X.: Joint deep learning for pedestrian detection. In: ICCV (2013)\n\nModeling mutual visibility relationship in pedestrian detection. W Ouyang, X Zeng, X Wang, CVPROuyang, W., Zeng, X., Wang, X.: Modeling mutual visibility relationship in pedes- trian detection. In: CVPR (2013)\n\nSingle-pedestrian detection aided by multi-pedestrian detection. W Ouyang, X Wang, CVPROuyang, W., Wang, X.: Single-pedestrian detection aided by multi-pedestrian de- tection. In: CVPR (2013)\n\nMask-guided attention network for occluded pedestrian detection. Y Pang, J Xie, M H Khan, R M Anwer, F S Khan, L Shao, ICCVPang, Y., Xie, J., Khan, M.H., Anwer, R.M., Khan, F.S., Shao, L.: Mask-guided attention network for occluded pedestrian detection. In: ICCV (2019)\n\nOcclusion patterns for object class detection. B Pepikj, M Stark, P Gehler, B Schiele, CVPRPepikj, B., Stark, M., Gehler, P., Schiele, B.: Occlusion patterns for object class detection. In: CVPR (2013)\n\nFaster R-CNN: Towards real-time object detection with region proposal networks. S Ren, K He, R Girshick, J Sun, NIPSRen, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: Towards real-time object detection with region proposal networks. In: NIPS (2015)\n\nBilattice-based logical reasoning for human detection. V D Shet, J Neumann, V Ramesh, L S Davis, CVPRShet, V.D., Neumann, J., Ramesh, V., Davis, L.S.: Bilattice-based logical reasoning for human detection. In: CVPR (2007)\n\nK Simonyan, A Vedaldi, A Zisserman, arXiv:1312.6034Deep inside convolutional networks: Visualising image classification models and saliency maps. Simonyan, K., Vedaldi, A., Zisserman, A.: Deep inside convolutional networks: Visualising image classification models and saliency maps. arXiv:1312.6034 (2013)\n\nSmall-scale pedestrian detection based on topological line localization and temporal feature aggregation. T Song, L Sun, D Xie, H Sun, S Pu, ECCVSong, T., Sun, L., Xie, D., Sun, H., Pu, S.: Small-scale pedestrian detection based on topological line localization and temporal feature aggregation. In: ECCV (2018)\n\nDetection and tracking of occluded people. S Tang, M Andriluka, B Schiele, IJCV. 110Tang, S., Andriluka, M., Schiele, B.: Detection and tracking of occluded people. IJCV 110 (2014)\n\nDeep learning strong parts for pedestrian detection. Y Tian, P Luo, X Wang, X Tang, ICCVTian, Y., Luo, P., Wang, X., Tang, X.: Deep learning strong parts for pedestrian detection. In: ICCV (2015)\n\nRepulsion loss: Detecting pedestrians in a crowd. X Wang, T Xiao, Y Jiang, S Shao, J Sun, C Shen, CVPRWang, X., Xiao, T., Jiang, Y., Shao, S., Sun, J., Shen, C.: Repulsion loss: Detecting pedestrians in a crowd. In: CVPR (2018)\n\nDetection of multiple, partially occluded humans in a single image by bayesian combination of edgelet part detectors. B Wu, R Nevatia, ICCVWu, B., Nevatia, R.: Detection of multiple, partially occluded humans in a single image by bayesian combination of edgelet part detectors. In: ICCV (2005)\n\nIs faster r-cnn doing well for pedestrian detection. L Zhang, L Lin, X Liang, K He, ECCVZhang, L., Lin, L., Liang, X., He, K.: Is faster r-cnn doing well for pedestrian detection? In: ECCV (2016)\n\nHow far are we from solving pedestrian detection?. S Zhang, R Benenson, M Omran, J Hosang, B Schiele, CVPRZhang, S., Benenson, R., Omran, M., Hosang, J., Schiele, B.: How far are we from solving pedestrian detection? In: CVPR (2016)\n\nCitypersons: A diverse dataset for pedestrian detection. S Zhang, R Benenson, B Schiele, CVPRZhang, S., Benenson, R., Schiele, B.: Citypersons: A diverse dataset for pedestrian detection. In: CVPR (2017)\n\nOccluded pedestrian detection through guided attention in CNNs. S Zhang, J Yang, B Schiele, CVPRZhang, S., Yang, J., Schiele, B.: Occluded pedestrian detection through guided attention in CNNs. In: CVPR (2018)\n\nOcclusion-aware R-CNN: Detecting pedestrians in a crowd. S Zhang, L Wen, X Bian, Z Lei, S Z Li, ECCVZhang, S., Wen, L., Bian, X., Lei, Z., Li, S.Z.: Occlusion-aware R-CNN: Detecting pedestrians in a crowd. In: ECCV (2018)\n\nDiscriminative feature transformation for occluded pedestrian detection. C Zhou, M Yang, J Yuan, ICCVZhou, C., Yang, M., Yuan, J.: Discriminative feature transformation for occluded pedestrian detection. In: ICCV (2019)\n\nMulti-label learning of part detectors for heavily occluded pedestrian detection. C Zhou, J Yuan, Zhou, C., Yuan, J.: Multi-label learning of part detectors for heavily occluded pedestrian detection. In: ICCV (2017)\n\nBi-box regression for pedestrian detection and occlusion estimation. C Zhou, J Yuan, ECCVZhou, C., Yuan, J.: Bi-box regression for pedestrian detection and occlusion esti- mation. In: ECCV (2018)\n", "annotations": {"author": "[{\"start\":\"68\",\"end\":\"146\"},{\"start\":\"147\",\"end\":\"223\"},{\"start\":\"224\",\"end\":\"303\"},{\"start\":\"304\",\"end\":\"384\"},{\"start\":\"385\",\"end\":\"458\"}]", "publisher": null, "author_last_name": "[{\"start\":\"76\",\"end\":\"80\"},{\"start\":\"153\",\"end\":\"157\"},{\"start\":\"234\",\"end\":\"237\"},{\"start\":\"313\",\"end\":\"318\"},{\"start\":\"389\",\"end\":\"392\"}]", "author_first_name": "[{\"start\":\"68\",\"end\":\"75\"},{\"start\":\"147\",\"end\":\"152\"},{\"start\":\"224\",\"end\":\"233\"},{\"start\":\"304\",\"end\":\"312\"},{\"start\":\"385\",\"end\":\"388\"}]", "author_affiliation": "[{\"start\":\"82\",\"end\":\"145\"},{\"start\":\"159\",\"end\":\"222\"},{\"start\":\"239\",\"end\":\"302\"},{\"start\":\"320\",\"end\":\"383\"},{\"start\":\"394\",\"end\":\"457\"}]", "title": "[{\"start\":\"1\",\"end\":\"65\"},{\"start\":\"459\",\"end\":\"523\"}]", "venue": null, "abstract": "[{\"start\":\"642\",\"end\":\"1885\"}]", "bib_ref": "[{\"start\":\"2014\",\"end\":\"2017\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"2017\",\"end\":\"2020\",\"attributes\":{\"ref_id\":\"b38\"}},{\"start\":\"2154\",\"end\":\"2158\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"2158\",\"end\":\"2161\",\"attributes\":{\"ref_id\":\"b41\"}},{\"start\":\"2161\",\"end\":\"2164\",\"attributes\":{\"ref_id\":\"b43\"}},{\"start\":\"2589\",\"end\":\"2593\",\"attributes\":{\"ref_id\":\"b28\"}},{\"start\":\"2810\",\"end\":\"2811\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"3133\",\"end\":\"3136\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"3136\",\"end\":\"3139\",\"attributes\":{\"ref_id\":\"b38\"}},{\"start\":\"3324\",\"end\":\"3328\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"3328\",\"end\":\"3331\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"4106\",\"end\":\"4110\",\"attributes\":{\"ref_id\":\"b38\"}},{\"start\":\"4116\",\"end\":\"4119\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"4133\",\"end\":\"4136\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"5377\",\"end\":\"5381\",\"attributes\":{\"ref_id\":\"b28\"}},{\"start\":\"5418\",\"end\":\"5422\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"5472\",\"end\":\"5475\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"5475\",\"end\":\"5477\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"5477\",\"end\":\"5480\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"5480\",\"end\":\"5483\",\"attributes\":{\"ref_id\":\"b12\"}},{\"start\":\"5483\",\"end\":\"5486\",\"attributes\":{\"ref_id\":\"b26\"}},{\"start\":\"5486\",\"end\":\"5489\",\"attributes\":{\"ref_id\":\"b34\"}},{\"start\":\"5489\",\"end\":\"5492\",\"attributes\":{\"ref_id\":\"b36\"}},{\"start\":\"5492\",\"end\":\"5496\",\"attributes\":{\"ref_id\":\"b39\"}},{\"start\":\"5496\",\"end\":\"5500\",\"attributes\":{\"ref_id\":\"b40\"}},{\"start\":\"5500\",\"end\":\"5504\",\"attributes\":{\"ref_id\":\"b41\"}},{\"start\":\"5504\",\"end\":\"5507\",\"attributes\":{\"ref_id\":\"b43\"}},{\"start\":\"5636\",\"end\":\"5640\",\"attributes\":{\"ref_id\":\"b36\"}},{\"start\":\"5662\",\"end\":\"5665\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"5665\",\"end\":\"5668\",\"attributes\":{\"ref_id\":\"b26\"}},{\"start\":\"5668\",\"end\":\"5671\",\"attributes\":{\"ref_id\":\"b39\"}},{\"start\":\"5671\",\"end\":\"5674\",\"attributes\":{\"ref_id\":\"b43\"}},{\"start\":\"5704\",\"end\":\"5707\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"5717\",\"end\":\"5721\",\"attributes\":{\"ref_id\":\"b36\"}},{\"start\":\"5911\",\"end\":\"5914\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"6064\",\"end\":\"6067\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"6329\",\"end\":\"6333\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"6333\",\"end\":\"6336\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"6336\",\"end\":\"6339\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"6399\",\"end\":\"6403\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"6673\",\"end\":\"6677\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"6939\",\"end\":\"6943\",\"attributes\":{\"ref_id\":\"b18\"}},{\"start\":\"6943\",\"end\":\"6946\",\"attributes\":{\"ref_id\":\"b31\"}},{\"start\":\"7086\",\"end\":\"7090\",\"attributes\":{\"ref_id\":\"b31\"}},{\"start\":\"7170\",\"end\":\"7174\",\"attributes\":{\"ref_id\":\"b18\"}},{\"start\":\"7744\",\"end\":\"7776\"},{\"start\":\"7976\",\"end\":\"7980\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"7980\",\"end\":\"7983\",\"attributes\":{\"ref_id\":\"b40\"}},{\"start\":\"8333\",\"end\":\"8337\",\"attributes\":{\"ref_id\":\"b26\"}},{\"start\":\"8337\",\"end\":\"8340\",\"attributes\":{\"ref_id\":\"b39\"}},{\"start\":\"8412\",\"end\":\"8416\",\"attributes\":{\"ref_id\":\"b39\"}},{\"start\":\"8426\",\"end\":\"8430\",\"attributes\":{\"ref_id\":\"b26\"}},{\"start\":\"8824\",\"end\":\"8828\",\"attributes\":{\"ref_id\":\"b43\"}},{\"start\":\"8986\",\"end\":\"8990\",\"attributes\":{\"ref_id\":\"b41\"}},{\"start\":\"9186\",\"end\":\"9190\",\"attributes\":{\"ref_id\":\"b15\"}},{\"start\":\"9190\",\"end\":\"9193\",\"attributes\":{\"ref_id\":\"b25\"}},{\"start\":\"9193\",\"end\":\"9196\",\"attributes\":{\"ref_id\":\"b27\"}},{\"start\":\"9196\",\"end\":\"9199\",\"attributes\":{\"ref_id\":\"b32\"}},{\"start\":\"9199\",\"end\":\"9202\",\"attributes\":{\"ref_id\":\"b34\"}},{\"start\":\"9252\",\"end\":\"9256\",\"attributes\":{\"ref_id\":\"b34\"}},{\"start\":\"9797\",\"end\":\"9801\",\"attributes\":{\"ref_id\":\"b38\"}},{\"start\":\"9814\",\"end\":\"9817\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"10175\",\"end\":\"10179\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"10179\",\"end\":\"10182\",\"attributes\":{\"ref_id\":\"b34\"}},{\"start\":\"10219\",\"end\":\"10223\",\"attributes\":{\"ref_id\":\"b43\"}},{\"start\":\"10394\",\"end\":\"10398\",\"attributes\":{\"ref_id\":\"b11\"}},{\"start\":\"11233\",\"end\":\"11237\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"12502\",\"end\":\"12506\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"12601\",\"end\":\"12605\",\"attributes\":{\"ref_id\":\"b28\"}},{\"start\":\"13293\",\"end\":\"13296\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"13296\",\"end\":\"13299\",\"attributes\":{\"ref_id\":\"b38\"}},{\"start\":\"13482\",\"end\":\"13485\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"13548\",\"end\":\"13552\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"13552\",\"end\":\"13555\",\"attributes\":{\"ref_id\":\"b38\"}},{\"start\":\"13555\",\"end\":\"13558\",\"attributes\":{\"ref_id\":\"b40\"}},{\"start\":\"13558\",\"end\":\"13561\",\"attributes\":{\"ref_id\":\"b41\"}},{\"start\":\"13866\",\"end\":\"13870\",\"attributes\":{\"ref_id\":\"b38\"}},{\"start\":\"13949\",\"end\":\"13950\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"14490\",\"end\":\"14494\",\"attributes\":{\"ref_id\":\"b38\"}},{\"start\":\"14507\",\"end\":\"14510\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"14771\",\"end\":\"14775\",\"attributes\":{\"ref_id\":\"b39\"}},{\"start\":\"15441\",\"end\":\"15444\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"15750\",\"end\":\"15754\",\"attributes\":{\"ref_id\":\"b38\"}},{\"start\":\"17035\",\"end\":\"17039\",\"attributes\":{\"ref_id\":\"b28\"}},{\"start\":\"17795\",\"end\":\"17799\",\"attributes\":{\"ref_id\":\"b30\"}},{\"start\":\"18824\",\"end\":\"18827\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"19741\",\"end\":\"19745\",\"attributes\":{\"ref_id\":\"b43\"}},{\"start\":\"19995\",\"end\":\"19999\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"20011\",\"end\":\"20015\",\"attributes\":{\"ref_id\":\"b43\"}},{\"start\":\"20790\",\"end\":\"20794\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"20926\",\"end\":\"20930\",\"attributes\":{\"ref_id\":\"b43\"}},{\"start\":\"21504\",\"end\":\"21508\",\"attributes\":{\"ref_id\":\"b38\"}},{\"start\":\"21514\",\"end\":\"21517\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"21531\",\"end\":\"21534\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"21548\",\"end\":\"21552\",\"attributes\":{\"ref_id\":\"b38\"}},{\"start\":\"21899\",\"end\":\"21902\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"21915\",\"end\":\"21918\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"21932\",\"end\":\"21935\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"22061\",\"end\":\"22064\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"22128\",\"end\":\"22131\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"22152\",\"end\":\"22156\",\"attributes\":{\"ref_id\":\"b37\"}},{\"start\":\"22265\",\"end\":\"22269\",\"attributes\":{\"ref_id\":\"b34\"}},{\"start\":\"22269\",\"end\":\"22272\",\"attributes\":{\"ref_id\":\"b39\"}},{\"start\":\"22420\",\"end\":\"22423\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"22503\",\"end\":\"22507\",\"attributes\":{\"ref_id\":\"b34\"}},{\"start\":\"22724\",\"end\":\"22728\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"22728\",\"end\":\"22731\",\"attributes\":{\"ref_id\":\"b18\"}},{\"start\":\"22930\",\"end\":\"22933\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"23197\",\"end\":\"23201\",\"attributes\":{\"ref_id\":\"b38\"}},{\"start\":\"26544\",\"end\":\"26548\",\"attributes\":{\"ref_id\":\"b38\"}},{\"start\":\"26593\",\"end\":\"26597\",\"attributes\":{\"ref_id\":\"b31\"}},{\"start\":\"26606\",\"end\":\"26610\",\"attributes\":{\"ref_id\":\"b18\"}},{\"start\":\"26907\",\"end\":\"26911\",\"attributes\":{\"ref_id\":\"b38\"}},{\"start\":\"27477\",\"end\":\"27481\",\"attributes\":{\"ref_id\":\"b39\"}},{\"start\":\"27491\",\"end\":\"27495\",\"attributes\":{\"ref_id\":\"b34\"}},{\"start\":\"27504\",\"end\":\"27508\",\"attributes\":{\"ref_id\":\"b40\"}},{\"start\":\"27520\",\"end\":\"27524\",\"attributes\":{\"ref_id\":\"b41\"}},{\"start\":\"27535\",\"end\":\"27539\",\"attributes\":{\"ref_id\":\"b26\"}},{\"start\":\"28084\",\"end\":\"28088\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"28100\",\"end\":\"28104\",\"attributes\":{\"ref_id\":\"b43\"}},{\"start\":\"28518\",\"end\":\"28521\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"28521\",\"end\":\"28524\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"29442\",\"end\":\"29445\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"30260\",\"end\":\"30263\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"30276\",\"end\":\"30279\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"30348\",\"end\":\"30352\",\"attributes\":{\"ref_id\":\"b18\"}},{\"start\":\"30364\",\"end\":\"30368\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"30588\",\"end\":\"30592\",\"attributes\":{\"ref_id\":\"b39\"}},{\"start\":\"30681\",\"end\":\"30685\",\"attributes\":{\"ref_id\":\"b39\"}},{\"start\":\"31111\",\"end\":\"31114\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"31153\",\"end\":\"31156\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"31188\",\"end\":\"31192\",\"attributes\":{\"ref_id\":\"b37\"}},{\"start\":\"34392\",\"end\":\"34396\",\"attributes\":{\"ref_id\":\"b38\"}},{\"start\":\"34415\",\"end\":\"34418\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"36183\",\"end\":\"36186\",\"attributes\":{\"ref_id\":\"b4\"}}]", "figure": "[{\"start\":\"33476\",\"end\":\"34132\",\"attributes\":{\"id\":\"fig_1\"}},{\"start\":\"34133\",\"end\":\"34258\",\"attributes\":{\"id\":\"fig_2\"}},{\"start\":\"34259\",\"end\":\"34343\",\"attributes\":{\"id\":\"fig_3\"}},{\"start\":\"34344\",\"end\":\"34660\",\"attributes\":{\"id\":\"fig_4\"}},{\"start\":\"34661\",\"end\":\"34819\",\"attributes\":{\"id\":\"fig_5\"}},{\"start\":\"34820\",\"end\":\"35052\",\"attributes\":{\"id\":\"fig_6\"}},{\"start\":\"35053\",\"end\":\"35321\",\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"}},{\"start\":\"35322\",\"end\":\"35701\",\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"}},{\"start\":\"35702\",\"end\":\"36138\",\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"}},{\"start\":\"36139\",\"end\":\"36640\",\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"}}]", "paragraph": "[{\"start\":\"1894\",\"end\":\"2081\"},{\"start\":\"2083\",\"end\":\"2866\"},{\"start\":\"2868\",\"end\":\"4247\"},{\"start\":\"4249\",\"end\":\"5070\"},{\"start\":\"5087\",\"end\":\"6913\"},{\"start\":\"6915\",\"end\":\"7378\"},{\"start\":\"7380\",\"end\":\"8190\"},{\"start\":\"8192\",\"end\":\"9638\"},{\"start\":\"9702\",\"end\":\"11107\"},{\"start\":\"11109\",\"end\":\"11490\"},{\"start\":\"11569\",\"end\":\"12121\"},{\"start\":\"12155\",\"end\":\"12426\"},{\"start\":\"12473\",\"end\":\"13145\"},{\"start\":\"13175\",\"end\":\"15097\"},{\"start\":\"15142\",\"end\":\"16010\"},{\"start\":\"16012\",\"end\":\"16562\"},{\"start\":\"16564\",\"end\":\"16876\"},{\"start\":\"16953\",\"end\":\"17175\"},{\"start\":\"17217\",\"end\":\"18368\"},{\"start\":\"18393\",\"end\":\"19162\"},{\"start\":\"19164\",\"end\":\"19506\"},{\"start\":\"19508\",\"end\":\"19921\"},{\"start\":\"19955\",\"end\":\"20878\"},{\"start\":\"20880\",\"end\":\"21404\"},{\"start\":\"21431\",\"end\":\"22633\"},{\"start\":\"22635\",\"end\":\"23097\"},{\"start\":\"23116\",\"end\":\"23259\"},{\"start\":\"23261\",\"end\":\"24163\"},{\"start\":\"24165\",\"end\":\"24993\"},{\"start\":\"24995\",\"end\":\"26049\"},{\"start\":\"26080\",\"end\":\"26414\"},{\"start\":\"26416\",\"end\":\"27123\"},{\"start\":\"27134\",\"end\":\"27357\"},{\"start\":\"27359\",\"end\":\"28032\"},{\"start\":\"28034\",\"end\":\"29034\"},{\"start\":\"29036\",\"end\":\"30128\"},{\"start\":\"30161\",\"end\":\"31381\"},{\"start\":\"31383\",\"end\":\"32327\"},{\"start\":\"32342\",\"end\":\"33475\"}]", "formula": "[{\"start\":\"11491\",\"end\":\"11552\",\"attributes\":{\"id\":\"formula_0\"}},{\"start\":\"11552\",\"end\":\"11568\",\"attributes\":{\"id\":\"formula_1\"}},{\"start\":\"12427\",\"end\":\"12472\",\"attributes\":{\"id\":\"formula_2\"}},{\"start\":\"13146\",\"end\":\"13174\",\"attributes\":{\"id\":\"formula_3\"}},{\"start\":\"15098\",\"end\":\"15141\",\"attributes\":{\"id\":\"formula_4\"}},{\"start\":\"16877\",\"end\":\"16952\",\"attributes\":{\"id\":\"formula_5\"}},{\"start\":\"17176\",\"end\":\"17216\",\"attributes\":{\"id\":\"formula_6\"}}]", "table_ref": "[{\"start\":\"23428\",\"end\":\"23435\",\"attributes\":{\"ref_id\":\"tab_2\"}},{\"start\":\"24440\",\"end\":\"24447\",\"attributes\":{\"ref_id\":\"tab_2\"}},{\"start\":\"25019\",\"end\":\"25026\"},{\"start\":\"26330\",\"end\":\"26337\"},{\"start\":\"26743\",\"end\":\"26750\"},{\"start\":\"26871\",\"end\":\"26878\"},{\"start\":\"27541\",\"end\":\"27548\"},{\"start\":\"28903\",\"end\":\"28910\"},{\"start\":\"29412\",\"end\":\"29419\"},{\"start\":\"30544\",\"end\":\"30551\"},{\"start\":\"30992\",\"end\":\"30999\"},{\"start\":\"31239\",\"end\":\"31246\",\"attributes\":{\"ref_id\":\"tab_6\"}}]", "section_header": "[{\"start\":\"5073\",\"end\":\"5085\",\"attributes\":{\"n\":\"2\"}},{\"start\":\"9641\",\"end\":\"9679\",\"attributes\":{\"n\":\"3\"}},{\"start\":\"9682\",\"end\":\"9700\",\"attributes\":{\"n\":\"3.1\"}},{\"start\":\"12124\",\"end\":\"12153\"},{\"start\":\"18371\",\"end\":\"18391\",\"attributes\":{\"n\":\"3.2\"}},{\"start\":\"19924\",\"end\":\"19953\",\"attributes\":{\"n\":\"3.3\"}},{\"start\":\"21407\",\"end\":\"21418\",\"attributes\":{\"n\":\"4\"}},{\"start\":\"21421\",\"end\":\"21429\",\"attributes\":{\"n\":\"4.1\"}},{\"start\":\"23100\",\"end\":\"23114\",\"attributes\":{\"n\":\"4.2\"}},{\"start\":\"26052\",\"end\":\"26078\",\"attributes\":{\"n\":\"4.3\"}},{\"start\":\"27126\",\"end\":\"27132\"},{\"start\":\"30131\",\"end\":\"30159\",\"attributes\":{\"n\":\"4.4\"}},{\"start\":\"32330\",\"end\":\"32340\",\"attributes\":{\"n\":\"5\"}},{\"start\":\"33477\",\"end\":\"33485\"},{\"start\":\"34134\",\"end\":\"34140\"},{\"start\":\"34260\",\"end\":\"34268\"},{\"start\":\"34345\",\"end\":\"34353\"},{\"start\":\"34662\",\"end\":\"34670\"},{\"start\":\"34821\",\"end\":\"34829\"},{\"start\":\"35323\",\"end\":\"35332\"},{\"start\":\"36140\",\"end\":\"36149\"}]", "table": "[{\"start\":\"35111\",\"end\":\"35321\"},{\"start\":\"35445\",\"end\":\"35701\"},{\"start\":\"36489\",\"end\":\"36640\"}]", "figure_caption": "[{\"start\":\"33487\",\"end\":\"34132\"},{\"start\":\"34142\",\"end\":\"34258\"},{\"start\":\"34270\",\"end\":\"34343\"},{\"start\":\"34355\",\"end\":\"34660\"},{\"start\":\"34672\",\"end\":\"34819\"},{\"start\":\"34831\",\"end\":\"35052\"},{\"start\":\"35055\",\"end\":\"35111\"},{\"start\":\"35334\",\"end\":\"35445\"},{\"start\":\"35704\",\"end\":\"36138\"},{\"start\":\"36151\",\"end\":\"36489\"}]", "figure_ref": "[{\"start\":\"1887\",\"end\":\"1893\"},{\"start\":\"3017\",\"end\":\"3023\"},{\"start\":\"12359\",\"end\":\"12365\",\"attributes\":{\"ref_id\":\"fig_3\"}},{\"start\":\"13709\",\"end\":\"13715\"},{\"start\":\"14616\",\"end\":\"14622\",\"attributes\":{\"ref_id\":\"fig_4\"}},{\"start\":\"14839\",\"end\":\"14848\",\"attributes\":{\"ref_id\":\"fig_4\"}},{\"start\":\"15269\",\"end\":\"15278\",\"attributes\":{\"ref_id\":\"fig_3\"}},{\"start\":\"15427\",\"end\":\"15433\",\"attributes\":{\"ref_id\":\"fig_4\"}},{\"start\":\"15587\",\"end\":\"15596\",\"attributes\":{\"ref_id\":\"fig_4\"}},{\"start\":\"15629\",\"end\":\"15638\",\"attributes\":{\"ref_id\":\"fig_4\"}},{\"start\":\"15877\",\"end\":\"15890\",\"attributes\":{\"ref_id\":\"fig_4\"}},{\"start\":\"16000\",\"end\":\"16009\",\"attributes\":{\"ref_id\":\"fig_4\"}},{\"start\":\"17544\",\"end\":\"17553\",\"attributes\":{\"ref_id\":\"fig_3\"}},{\"start\":\"17764\",\"end\":\"17770\",\"attributes\":{\"ref_id\":\"fig_5\"}},{\"start\":\"17920\",\"end\":\"17929\",\"attributes\":{\"ref_id\":\"fig_5\"}},{\"start\":\"18094\",\"end\":\"18103\",\"attributes\":{\"ref_id\":\"fig_5\"}},{\"start\":\"18612\",\"end\":\"18618\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"29449\",\"end\":\"29455\",\"attributes\":{\"ref_id\":\"fig_6\"}},{\"start\":\"29887\",\"end\":\"29893\",\"attributes\":{\"ref_id\":\"fig_6\"}}]", "bib_author_first_name": "[{\"start\":\"36748\",\"end\":\"36749\"},{\"start\":\"36758\",\"end\":\"36759\"},{\"start\":\"36932\",\"end\":\"36933\"},{\"start\":\"36942\",\"end\":\"36943\"},{\"start\":\"36949\",\"end\":\"36950\"},{\"start\":\"37137\",\"end\":\"37138\"},{\"start\":\"37144\",\"end\":\"37145\"},{\"start\":\"37153\",\"end\":\"37154\"},{\"start\":\"37161\",\"end\":\"37162\"},{\"start\":\"37168\",\"end\":\"37169\"},{\"start\":\"37170\",\"end\":\"37171\"},{\"start\":\"37176\",\"end\":\"37177\"},{\"start\":\"37328\",\"end\":\"37329\"},{\"start\":\"37336\",\"end\":\"37337\"},{\"start\":\"37344\",\"end\":\"37345\"},{\"start\":\"37354\",\"end\":\"37355\"},{\"start\":\"37360\",\"end\":\"37363\"},{\"start\":\"37368\",\"end\":\"37370\"},{\"start\":\"37628\",\"end\":\"37629\"},{\"start\":\"37638\",\"end\":\"37639\"},{\"start\":\"37647\",\"end\":\"37648\"},{\"start\":\"37658\",\"end\":\"37659\"},{\"start\":\"37852\",\"end\":\"37853\"},{\"start\":\"37860\",\"end\":\"37861\"},{\"start\":\"37866\",\"end\":\"37867\"},{\"start\":\"38039\",\"end\":\"38040\"},{\"start\":\"38052\",\"end\":\"38053\"},{\"start\":\"38068\",\"end\":\"38069\"},{\"start\":\"38079\",\"end\":\"38080\"},{\"start\":\"38081\",\"end\":\"38082\"},{\"start\":\"38291\",\"end\":\"38292\"},{\"start\":\"38298\",\"end\":\"38299\"},{\"start\":\"38307\",\"end\":\"38308\"},{\"start\":\"38464\",\"end\":\"38465\"},{\"start\":\"38474\",\"end\":\"38475\"},{\"start\":\"38482\",\"end\":\"38483\"},{\"start\":\"38493\",\"end\":\"38494\"},{\"start\":\"38773\",\"end\":\"38774\"},{\"start\":\"38783\",\"end\":\"38784\"},{\"start\":\"38791\",\"end\":\"38792\"},{\"start\":\"38929\",\"end\":\"38930\"},{\"start\":\"39052\",\"end\":\"39053\"},{\"start\":\"39058\",\"end\":\"39059\"},{\"start\":\"39067\",\"end\":\"39068\"},{\"start\":\"39074\",\"end\":\"39075\"},{\"start\":\"39234\",\"end\":\"39235\"},{\"start\":\"39240\",\"end\":\"39241\"},{\"start\":\"39249\",\"end\":\"39250\"},{\"start\":\"39257\",\"end\":\"39258\"},{\"start\":\"39263\",\"end\":\"39264\"},{\"start\":\"39271\",\"end\":\"39272\"},{\"start\":\"39470\",\"end\":\"39471\"},{\"start\":\"39477\",\"end\":\"39478\"},{\"start\":\"39483\",\"end\":\"39484\"},{\"start\":\"39491\",\"end\":\"39492\"},{\"start\":\"39661\",\"end\":\"39662\"},{\"start\":\"39668\",\"end\":\"39669\"},{\"start\":\"39677\",\"end\":\"39678\"},{\"start\":\"39689\",\"end\":\"39690\"},{\"start\":\"39695\",\"end\":\"39696\"},{\"start\":\"39875\",\"end\":\"39876\"},{\"start\":\"39882\",\"end\":\"39883\"},{\"start\":\"39891\",\"end\":\"39892\"},{\"start\":\"40042\",\"end\":\"40043\"},{\"start\":\"40049\",\"end\":\"40050\"},{\"start\":\"40061\",\"end\":\"40062\"},{\"start\":\"40070\",\"end\":\"40071\"},{\"start\":\"40081\",\"end\":\"40082\"},{\"start\":\"40089\",\"end\":\"40090\"},{\"start\":\"40091\",\"end\":\"40092\"},{\"start\":\"40097\",\"end\":\"40098\"},{\"start\":\"40099\",\"end\":\"40100\"},{\"start\":\"40332\",\"end\":\"40333\"},{\"start\":\"40339\",\"end\":\"40340\"},{\"start\":\"40347\",\"end\":\"40348\"},{\"start\":\"40353\",\"end\":\"40354\"},{\"start\":\"40362\",\"end\":\"40363\"},{\"start\":\"40611\",\"end\":\"40612\"},{\"start\":\"40618\",\"end\":\"40619\"},{\"start\":\"40626\",\"end\":\"40627\"},{\"start\":\"40633\",\"end\":\"40634\"},{\"start\":\"40639\",\"end\":\"40640\"},{\"start\":\"40839\",\"end\":\"40840\"},{\"start\":\"40850\",\"end\":\"40851\"},{\"start\":\"40862\",\"end\":\"40863\"},{\"start\":\"40873\",\"end\":\"40874\"},{\"start\":\"40875\",\"end\":\"40876\"},{\"start\":\"41080\",\"end\":\"41081\"},{\"start\":\"41082\",\"end\":\"41083\"},{\"start\":\"41096\",\"end\":\"41097\"},{\"start\":\"41098\",\"end\":\"41099\"},{\"start\":\"41328\",\"end\":\"41329\"},{\"start\":\"41335\",\"end\":\"41336\"},{\"start\":\"41342\",\"end\":\"41343\"},{\"start\":\"41349\",\"end\":\"41350\"},{\"start\":\"41577\",\"end\":\"41578\"},{\"start\":\"41587\",\"end\":\"41588\"},{\"start\":\"41762\",\"end\":\"41763\"},{\"start\":\"41772\",\"end\":\"41773\"},{\"start\":\"41934\",\"end\":\"41935\"},{\"start\":\"41944\",\"end\":\"41945\"},{\"start\":\"41952\",\"end\":\"41953\"},{\"start\":\"42145\",\"end\":\"42146\"},{\"start\":\"42155\",\"end\":\"42156\"},{\"start\":\"42338\",\"end\":\"42339\"},{\"start\":\"42346\",\"end\":\"42347\"},{\"start\":\"42353\",\"end\":\"42354\"},{\"start\":\"42355\",\"end\":\"42356\"},{\"start\":\"42363\",\"end\":\"42364\"},{\"start\":\"42365\",\"end\":\"42366\"},{\"start\":\"42374\",\"end\":\"42375\"},{\"start\":\"42376\",\"end\":\"42377\"},{\"start\":\"42384\",\"end\":\"42385\"},{\"start\":\"42591\",\"end\":\"42592\"},{\"start\":\"42601\",\"end\":\"42602\"},{\"start\":\"42610\",\"end\":\"42611\"},{\"start\":\"42620\",\"end\":\"42621\"},{\"start\":\"42827\",\"end\":\"42828\"},{\"start\":\"42834\",\"end\":\"42835\"},{\"start\":\"42840\",\"end\":\"42841\"},{\"start\":\"42852\",\"end\":\"42853\"},{\"start\":\"43055\",\"end\":\"43056\"},{\"start\":\"43057\",\"end\":\"43058\"},{\"start\":\"43065\",\"end\":\"43066\"},{\"start\":\"43076\",\"end\":\"43077\"},{\"start\":\"43086\",\"end\":\"43087\"},{\"start\":\"43088\",\"end\":\"43089\"},{\"start\":\"43223\",\"end\":\"43224\"},{\"start\":\"43235\",\"end\":\"43236\"},{\"start\":\"43246\",\"end\":\"43247\"},{\"start\":\"43636\",\"end\":\"43637\"},{\"start\":\"43644\",\"end\":\"43645\"},{\"start\":\"43651\",\"end\":\"43652\"},{\"start\":\"43658\",\"end\":\"43659\"},{\"start\":\"43665\",\"end\":\"43666\"},{\"start\":\"43886\",\"end\":\"43887\"},{\"start\":\"43894\",\"end\":\"43895\"},{\"start\":\"43907\",\"end\":\"43908\"},{\"start\":\"44078\",\"end\":\"44079\"},{\"start\":\"44086\",\"end\":\"44087\"},{\"start\":\"44093\",\"end\":\"44094\"},{\"start\":\"44101\",\"end\":\"44102\"},{\"start\":\"44272\",\"end\":\"44273\"},{\"start\":\"44280\",\"end\":\"44281\"},{\"start\":\"44288\",\"end\":\"44289\"},{\"start\":\"44297\",\"end\":\"44298\"},{\"start\":\"44305\",\"end\":\"44306\"},{\"start\":\"44312\",\"end\":\"44313\"},{\"start\":\"44569\",\"end\":\"44570\"},{\"start\":\"44575\",\"end\":\"44576\"},{\"start\":\"44799\",\"end\":\"44800\"},{\"start\":\"44808\",\"end\":\"44809\"},{\"start\":\"44815\",\"end\":\"44816\"},{\"start\":\"44824\",\"end\":\"44825\"},{\"start\":\"44994\",\"end\":\"44995\"},{\"start\":\"45003\",\"end\":\"45004\"},{\"start\":\"45015\",\"end\":\"45016\"},{\"start\":\"45024\",\"end\":\"45025\"},{\"start\":\"45034\",\"end\":\"45035\"},{\"start\":\"45234\",\"end\":\"45235\"},{\"start\":\"45243\",\"end\":\"45244\"},{\"start\":\"45255\",\"end\":\"45256\"},{\"start\":\"45446\",\"end\":\"45447\"},{\"start\":\"45455\",\"end\":\"45456\"},{\"start\":\"45463\",\"end\":\"45464\"},{\"start\":\"45650\",\"end\":\"45651\"},{\"start\":\"45659\",\"end\":\"45660\"},{\"start\":\"45666\",\"end\":\"45667\"},{\"start\":\"45674\",\"end\":\"45675\"},{\"start\":\"45681\",\"end\":\"45682\"},{\"start\":\"45683\",\"end\":\"45684\"},{\"start\":\"45889\",\"end\":\"45890\"},{\"start\":\"45897\",\"end\":\"45898\"},{\"start\":\"45905\",\"end\":\"45906\"},{\"start\":\"46119\",\"end\":\"46120\"},{\"start\":\"46127\",\"end\":\"46128\"},{\"start\":\"46323\",\"end\":\"46324\"},{\"start\":\"46331\",\"end\":\"46332\"}]", "bib_author_last_name": "[{\"start\":\"36750\",\"end\":\"36756\"},{\"start\":\"36760\",\"end\":\"36763\"},{\"start\":\"36934\",\"end\":\"36940\"},{\"start\":\"36944\",\"end\":\"36947\"},{\"start\":\"36951\",\"end\":\"36954\"},{\"start\":\"37139\",\"end\":\"37142\"},{\"start\":\"37146\",\"end\":\"37151\"},{\"start\":\"37155\",\"end\":\"37159\"},{\"start\":\"37163\",\"end\":\"37166\"},{\"start\":\"37172\",\"end\":\"37174\"},{\"start\":\"37178\",\"end\":\"37181\"},{\"start\":\"37330\",\"end\":\"37334\"},{\"start\":\"37338\",\"end\":\"37342\"},{\"start\":\"37346\",\"end\":\"37352\"},{\"start\":\"37356\",\"end\":\"37358\"},{\"start\":\"37364\",\"end\":\"37366\"},{\"start\":\"37371\",\"end\":\"37378\"},{\"start\":\"37630\",\"end\":\"37636\"},{\"start\":\"37640\",\"end\":\"37645\"},{\"start\":\"37649\",\"end\":\"37656\"},{\"start\":\"37660\",\"end\":\"37666\"},{\"start\":\"37854\",\"end\":\"37858\"},{\"start\":\"37862\",\"end\":\"37864\"},{\"start\":\"37868\",\"end\":\"37871\"},{\"start\":\"38041\",\"end\":\"38050\"},{\"start\":\"38054\",\"end\":\"38066\"},{\"start\":\"38070\",\"end\":\"38077\"},{\"start\":\"38083\",\"end\":\"38090\"},{\"start\":\"38293\",\"end\":\"38296\"},{\"start\":\"38300\",\"end\":\"38305\"},{\"start\":\"38309\",\"end\":\"38317\"},{\"start\":\"38466\",\"end\":\"38472\"},{\"start\":\"38476\",\"end\":\"38480\"},{\"start\":\"38484\",\"end\":\"38491\"},{\"start\":\"38495\",\"end\":\"38502\"},{\"start\":\"38775\",\"end\":\"38781\"},{\"start\":\"38785\",\"end\":\"38789\"},{\"start\":\"38793\",\"end\":\"38800\"},{\"start\":\"38931\",\"end\":\"38939\"},{\"start\":\"39054\",\"end\":\"39056\"},{\"start\":\"39060\",\"end\":\"39065\"},{\"start\":\"39069\",\"end\":\"39072\"},{\"start\":\"39076\",\"end\":\"39079\"},{\"start\":\"39236\",\"end\":\"39238\"},{\"start\":\"39242\",\"end\":\"39247\"},{\"start\":\"39251\",\"end\":\"39255\"},{\"start\":\"39259\",\"end\":\"39261\"},{\"start\":\"39265\",\"end\":\"39269\"},{\"start\":\"39273\",\"end\":\"39276\"},{\"start\":\"39472\",\"end\":\"39475\"},{\"start\":\"39479\",\"end\":\"39481\"},{\"start\":\"39485\",\"end\":\"39489\"},{\"start\":\"39493\",\"end\":\"39497\"},{\"start\":\"39663\",\"end\":\"39666\"},{\"start\":\"39670\",\"end\":\"39675\"},{\"start\":\"39679\",\"end\":\"39687\"},{\"start\":\"39691\",\"end\":\"39693\"},{\"start\":\"39697\",\"end\":\"39703\"},{\"start\":\"39877\",\"end\":\"39880\"},{\"start\":\"39884\",\"end\":\"39889\"},{\"start\":\"39893\",\"end\":\"39897\"},{\"start\":\"40044\",\"end\":\"40047\"},{\"start\":\"40051\",\"end\":\"40059\"},{\"start\":\"40063\",\"end\":\"40068\"},{\"start\":\"40072\",\"end\":\"40079\"},{\"start\":\"40083\",\"end\":\"40087\"},{\"start\":\"40093\",\"end\":\"40095\"},{\"start\":\"40101\",\"end\":\"40105\"},{\"start\":\"40334\",\"end\":\"40337\"},{\"start\":\"40341\",\"end\":\"40345\"},{\"start\":\"40349\",\"end\":\"40351\"},{\"start\":\"40355\",\"end\":\"40360\"},{\"start\":\"40364\",\"end\":\"40368\"},{\"start\":\"40613\",\"end\":\"40616\"},{\"start\":\"40620\",\"end\":\"40624\"},{\"start\":\"40628\",\"end\":\"40631\"},{\"start\":\"40635\",\"end\":\"40637\"},{\"start\":\"40641\",\"end\":\"40643\"},{\"start\":\"40841\",\"end\":\"40848\"},{\"start\":\"40852\",\"end\":\"40860\"},{\"start\":\"40864\",\"end\":\"40871\"},{\"start\":\"40877\",\"end\":\"40881\"},{\"start\":\"41084\",\"end\":\"41094\"},{\"start\":\"41100\",\"end\":\"41107\"},{\"start\":\"41330\",\"end\":\"41333\"},{\"start\":\"41337\",\"end\":\"41340\"},{\"start\":\"41344\",\"end\":\"41347\"},{\"start\":\"41351\",\"end\":\"41354\"},{\"start\":\"41579\",\"end\":\"41585\"},{\"start\":\"41589\",\"end\":\"41593\"},{\"start\":\"41764\",\"end\":\"41770\"},{\"start\":\"41774\",\"end\":\"41778\"},{\"start\":\"41936\",\"end\":\"41942\"},{\"start\":\"41946\",\"end\":\"41950\"},{\"start\":\"41954\",\"end\":\"41958\"},{\"start\":\"42147\",\"end\":\"42153\"},{\"start\":\"42157\",\"end\":\"42161\"},{\"start\":\"42340\",\"end\":\"42344\"},{\"start\":\"42348\",\"end\":\"42351\"},{\"start\":\"42357\",\"end\":\"42361\"},{\"start\":\"42367\",\"end\":\"42372\"},{\"start\":\"42378\",\"end\":\"42382\"},{\"start\":\"42386\",\"end\":\"42390\"},{\"start\":\"42593\",\"end\":\"42599\"},{\"start\":\"42603\",\"end\":\"42608\"},{\"start\":\"42612\",\"end\":\"42618\"},{\"start\":\"42622\",\"end\":\"42629\"},{\"start\":\"42829\",\"end\":\"42832\"},{\"start\":\"42836\",\"end\":\"42838\"},{\"start\":\"42842\",\"end\":\"42850\"},{\"start\":\"42854\",\"end\":\"42857\"},{\"start\":\"43059\",\"end\":\"43063\"},{\"start\":\"43067\",\"end\":\"43074\"},{\"start\":\"43078\",\"end\":\"43084\"},{\"start\":\"43090\",\"end\":\"43095\"},{\"start\":\"43225\",\"end\":\"43233\"},{\"start\":\"43237\",\"end\":\"43244\"},{\"start\":\"43248\",\"end\":\"43257\"},{\"start\":\"43638\",\"end\":\"43642\"},{\"start\":\"43646\",\"end\":\"43649\"},{\"start\":\"43653\",\"end\":\"43656\"},{\"start\":\"43660\",\"end\":\"43663\"},{\"start\":\"43667\",\"end\":\"43669\"},{\"start\":\"43888\",\"end\":\"43892\"},{\"start\":\"43896\",\"end\":\"43905\"},{\"start\":\"43909\",\"end\":\"43916\"},{\"start\":\"44080\",\"end\":\"44084\"},{\"start\":\"44088\",\"end\":\"44091\"},{\"start\":\"44095\",\"end\":\"44099\"},{\"start\":\"44103\",\"end\":\"44107\"},{\"start\":\"44274\",\"end\":\"44278\"},{\"start\":\"44282\",\"end\":\"44286\"},{\"start\":\"44290\",\"end\":\"44295\"},{\"start\":\"44299\",\"end\":\"44303\"},{\"start\":\"44307\",\"end\":\"44310\"},{\"start\":\"44314\",\"end\":\"44318\"},{\"start\":\"44571\",\"end\":\"44573\"},{\"start\":\"44577\",\"end\":\"44584\"},{\"start\":\"44801\",\"end\":\"44806\"},{\"start\":\"44810\",\"end\":\"44813\"},{\"start\":\"44817\",\"end\":\"44822\"},{\"start\":\"44826\",\"end\":\"44828\"},{\"start\":\"44996\",\"end\":\"45001\"},{\"start\":\"45005\",\"end\":\"45013\"},{\"start\":\"45017\",\"end\":\"45022\"},{\"start\":\"45026\",\"end\":\"45032\"},{\"start\":\"45036\",\"end\":\"45043\"},{\"start\":\"45236\",\"end\":\"45241\"},{\"start\":\"45245\",\"end\":\"45253\"},{\"start\":\"45257\",\"end\":\"45264\"},{\"start\":\"45448\",\"end\":\"45453\"},{\"start\":\"45457\",\"end\":\"45461\"},{\"start\":\"45465\",\"end\":\"45472\"},{\"start\":\"45652\",\"end\":\"45657\"},{\"start\":\"45661\",\"end\":\"45664\"},{\"start\":\"45668\",\"end\":\"45672\"},{\"start\":\"45676\",\"end\":\"45679\"},{\"start\":\"45685\",\"end\":\"45687\"},{\"start\":\"45891\",\"end\":\"45895\"},{\"start\":\"45899\",\"end\":\"45903\"},{\"start\":\"45907\",\"end\":\"45911\"},{\"start\":\"46121\",\"end\":\"46125\"},{\"start\":\"46129\",\"end\":\"46133\"},{\"start\":\"46325\",\"end\":\"46329\"},{\"start\":\"46333\",\"end\":\"46337\"}]", "bib_entry": "[{\"start\":\"36691\",\"end\":\"36862\",\"attributes\":{\"id\":\"b0\"}},{\"start\":\"36864\",\"end\":\"37069\",\"attributes\":{\"id\":\"b1\"}},{\"start\":\"37071\",\"end\":\"37326\",\"attributes\":{\"id\":\"b2\"}},{\"start\":\"37328\",\"end\":\"37565\",\"attributes\":{\"id\":\"b3\"}},{\"start\":\"37567\",\"end\":\"37801\",\"attributes\":{\"matched_paper_id\":\"206764948\",\"id\":\"b4\"}},{\"start\":\"37803\",\"end\":\"37968\",\"attributes\":{\"id\":\"b5\"}},{\"start\":\"37970\",\"end\":\"38241\",\"attributes\":{\"id\":\"b6\"}},{\"start\":\"38243\",\"end\":\"38420\",\"attributes\":{\"id\":\"b7\"}},{\"start\":\"38422\",\"end\":\"38700\",\"attributes\":{\"matched_paper_id\":\"9455111\",\"id\":\"b8\"}},{\"start\":\"38702\",\"end\":\"38927\",\"attributes\":{\"id\":\"b9\"}},{\"start\":\"38929\",\"end\":\"39004\",\"attributes\":{\"id\":\"b10\"}},{\"start\":\"39006\",\"end\":\"39183\",\"attributes\":{\"id\":\"b11\"}},{\"start\":\"39185\",\"end\":\"39403\",\"attributes\":{\"matched_paper_id\":\"3894476\",\"id\":\"b12\"}},{\"start\":\"39405\",\"end\":\"39620\",\"attributes\":{\"id\":\"b13\"}},{\"start\":\"39622\",\"end\":\"39817\",\"attributes\":{\"id\":\"b14\"}},{\"start\":\"39819\",\"end\":\"40004\",\"attributes\":{\"id\":\"b15\"}},{\"start\":\"40006\",\"end\":\"40241\",\"attributes\":{\"id\":\"b16\"}},{\"start\":\"40243\",\"end\":\"40526\",\"attributes\":{\"id\":\"b17\"}},{\"start\":\"40528\",\"end\":\"40791\",\"attributes\":{\"id\":\"b18\"}},{\"start\":\"40793\",\"end\":\"41000\",\"attributes\":{\"id\":\"b19\"}},{\"start\":\"41002\",\"end\":\"41240\",\"attributes\":{\"matched_paper_id\":\"1393851\",\"id\":\"b20\"}},{\"start\":\"41242\",\"end\":\"41497\",\"attributes\":{\"id\":\"b21\"}},{\"start\":\"41499\",\"end\":\"41714\",\"attributes\":{\"id\":\"b22\"}},{\"start\":\"41716\",\"end\":\"41867\",\"attributes\":{\"id\":\"b23\"}},{\"start\":\"41869\",\"end\":\"42078\",\"attributes\":{\"id\":\"b24\"}},{\"start\":\"42080\",\"end\":\"42271\",\"attributes\":{\"id\":\"b25\"}},{\"start\":\"42273\",\"end\":\"42542\",\"attributes\":{\"id\":\"b26\"}},{\"start\":\"42544\",\"end\":\"42745\",\"attributes\":{\"id\":\"b27\"}},{\"start\":\"42747\",\"end\":\"42998\",\"attributes\":{\"id\":\"b28\"}},{\"start\":\"43000\",\"end\":\"43221\",\"attributes\":{\"id\":\"b29\"}},{\"start\":\"43223\",\"end\":\"43528\",\"attributes\":{\"id\":\"b30\",\"doi\":\"arXiv:1312.6034\"}},{\"start\":\"43530\",\"end\":\"43841\",\"attributes\":{\"id\":\"b31\"}},{\"start\":\"43843\",\"end\":\"44023\",\"attributes\":{\"matched_paper_id\":\"72295\",\"id\":\"b32\"}},{\"start\":\"44025\",\"end\":\"44220\",\"attributes\":{\"id\":\"b33\"}},{\"start\":\"44222\",\"end\":\"44449\",\"attributes\":{\"id\":\"b34\"}},{\"start\":\"44451\",\"end\":\"44744\",\"attributes\":{\"id\":\"b35\"}},{\"start\":\"44746\",\"end\":\"44941\",\"attributes\":{\"id\":\"b36\"}},{\"start\":\"44943\",\"end\":\"45175\",\"attributes\":{\"id\":\"b37\"}},{\"start\":\"45177\",\"end\":\"45380\",\"attributes\":{\"id\":\"b38\"}},{\"start\":\"45382\",\"end\":\"45591\",\"attributes\":{\"id\":\"b39\"}},{\"start\":\"45593\",\"end\":\"45814\",\"attributes\":{\"id\":\"b40\"}},{\"start\":\"45816\",\"end\":\"46035\",\"attributes\":{\"id\":\"b41\"}},{\"start\":\"46037\",\"end\":\"46252\",\"attributes\":{\"id\":\"b42\"}},{\"start\":\"46254\",\"end\":\"46449\",\"attributes\":{\"id\":\"b43\"}}]", "bib_title": "[{\"start\":\"37567\",\"end\":\"37626\"},{\"start\":\"38422\",\"end\":\"38462\"},{\"start\":\"39185\",\"end\":\"39232\"},{\"start\":\"41002\",\"end\":\"41078\"},{\"start\":\"43843\",\"end\":\"43884\"}]", "bib_author": "[{\"start\":\"36748\",\"end\":\"36758\"},{\"start\":\"36758\",\"end\":\"36765\"},{\"start\":\"36932\",\"end\":\"36942\"},{\"start\":\"36942\",\"end\":\"36949\"},{\"start\":\"36949\",\"end\":\"36956\"},{\"start\":\"37137\",\"end\":\"37144\"},{\"start\":\"37144\",\"end\":\"37153\"},{\"start\":\"37153\",\"end\":\"37161\"},{\"start\":\"37161\",\"end\":\"37168\"},{\"start\":\"37168\",\"end\":\"37176\"},{\"start\":\"37176\",\"end\":\"37183\"},{\"start\":\"37328\",\"end\":\"37336\"},{\"start\":\"37336\",\"end\":\"37344\"},{\"start\":\"37344\",\"end\":\"37354\"},{\"start\":\"37354\",\"end\":\"37360\"},{\"start\":\"37360\",\"end\":\"37368\"},{\"start\":\"37368\",\"end\":\"37380\"},{\"start\":\"37628\",\"end\":\"37638\"},{\"start\":\"37638\",\"end\":\"37647\"},{\"start\":\"37647\",\"end\":\"37658\"},{\"start\":\"37658\",\"end\":\"37668\"},{\"start\":\"37852\",\"end\":\"37860\"},{\"start\":\"37860\",\"end\":\"37866\"},{\"start\":\"37866\",\"end\":\"37873\"},{\"start\":\"38039\",\"end\":\"38052\"},{\"start\":\"38052\",\"end\":\"38068\"},{\"start\":\"38068\",\"end\":\"38079\"},{\"start\":\"38079\",\"end\":\"38092\"},{\"start\":\"38291\",\"end\":\"38298\"},{\"start\":\"38298\",\"end\":\"38307\"},{\"start\":\"38307\",\"end\":\"38319\"},{\"start\":\"38464\",\"end\":\"38474\"},{\"start\":\"38474\",\"end\":\"38482\"},{\"start\":\"38482\",\"end\":\"38493\"},{\"start\":\"38493\",\"end\":\"38504\"},{\"start\":\"38773\",\"end\":\"38783\"},{\"start\":\"38783\",\"end\":\"38791\"},{\"start\":\"38791\",\"end\":\"38802\"},{\"start\":\"38929\",\"end\":\"38941\"},{\"start\":\"39052\",\"end\":\"39058\"},{\"start\":\"39058\",\"end\":\"39067\"},{\"start\":\"39067\",\"end\":\"39074\"},{\"start\":\"39074\",\"end\":\"39081\"},{\"start\":\"39234\",\"end\":\"39240\"},{\"start\":\"39240\",\"end\":\"39249\"},{\"start\":\"39249\",\"end\":\"39257\"},{\"start\":\"39257\",\"end\":\"39263\"},{\"start\":\"39263\",\"end\":\"39271\"},{\"start\":\"39271\",\"end\":\"39278\"},{\"start\":\"39470\",\"end\":\"39477\"},{\"start\":\"39477\",\"end\":\"39483\"},{\"start\":\"39483\",\"end\":\"39491\"},{\"start\":\"39491\",\"end\":\"39499\"},{\"start\":\"39661\",\"end\":\"39668\"},{\"start\":\"39668\",\"end\":\"39677\"},{\"start\":\"39677\",\"end\":\"39689\"},{\"start\":\"39689\",\"end\":\"39695\"},{\"start\":\"39695\",\"end\":\"39705\"},{\"start\":\"39875\",\"end\":\"39882\"},{\"start\":\"39882\",\"end\":\"39891\"},{\"start\":\"39891\",\"end\":\"39899\"},{\"start\":\"40042\",\"end\":\"40049\"},{\"start\":\"40049\",\"end\":\"40061\"},{\"start\":\"40061\",\"end\":\"40070\"},{\"start\":\"40070\",\"end\":\"40081\"},{\"start\":\"40081\",\"end\":\"40089\"},{\"start\":\"40089\",\"end\":\"40097\"},{\"start\":\"40097\",\"end\":\"40107\"},{\"start\":\"40332\",\"end\":\"40339\"},{\"start\":\"40339\",\"end\":\"40347\"},{\"start\":\"40347\",\"end\":\"40353\"},{\"start\":\"40353\",\"end\":\"40362\"},{\"start\":\"40362\",\"end\":\"40370\"},{\"start\":\"40611\",\"end\":\"40618\"},{\"start\":\"40618\",\"end\":\"40626\"},{\"start\":\"40626\",\"end\":\"40633\"},{\"start\":\"40633\",\"end\":\"40639\"},{\"start\":\"40639\",\"end\":\"40645\"},{\"start\":\"40839\",\"end\":\"40850\"},{\"start\":\"40850\",\"end\":\"40862\"},{\"start\":\"40862\",\"end\":\"40873\"},{\"start\":\"40873\",\"end\":\"40883\"},{\"start\":\"41080\",\"end\":\"41096\"},{\"start\":\"41096\",\"end\":\"41109\"},{\"start\":\"41328\",\"end\":\"41335\"},{\"start\":\"41335\",\"end\":\"41342\"},{\"start\":\"41342\",\"end\":\"41349\"},{\"start\":\"41349\",\"end\":\"41356\"},{\"start\":\"41577\",\"end\":\"41587\"},{\"start\":\"41587\",\"end\":\"41595\"},{\"start\":\"41762\",\"end\":\"41772\"},{\"start\":\"41772\",\"end\":\"41780\"},{\"start\":\"41934\",\"end\":\"41944\"},{\"start\":\"41944\",\"end\":\"41952\"},{\"start\":\"41952\",\"end\":\"41960\"},{\"start\":\"42145\",\"end\":\"42155\"},{\"start\":\"42155\",\"end\":\"42163\"},{\"start\":\"42338\",\"end\":\"42346\"},{\"start\":\"42346\",\"end\":\"42353\"},{\"start\":\"42353\",\"end\":\"42363\"},{\"start\":\"42363\",\"end\":\"42374\"},{\"start\":\"42374\",\"end\":\"42384\"},{\"start\":\"42384\",\"end\":\"42392\"},{\"start\":\"42591\",\"end\":\"42601\"},{\"start\":\"42601\",\"end\":\"42610\"},{\"start\":\"42610\",\"end\":\"42620\"},{\"start\":\"42620\",\"end\":\"42631\"},{\"start\":\"42827\",\"end\":\"42834\"},{\"start\":\"42834\",\"end\":\"42840\"},{\"start\":\"42840\",\"end\":\"42852\"},{\"start\":\"42852\",\"end\":\"42859\"},{\"start\":\"43055\",\"end\":\"43065\"},{\"start\":\"43065\",\"end\":\"43076\"},{\"start\":\"43076\",\"end\":\"43086\"},{\"start\":\"43086\",\"end\":\"43097\"},{\"start\":\"43223\",\"end\":\"43235\"},{\"start\":\"43235\",\"end\":\"43246\"},{\"start\":\"43246\",\"end\":\"43259\"},{\"start\":\"43636\",\"end\":\"43644\"},{\"start\":\"43644\",\"end\":\"43651\"},{\"start\":\"43651\",\"end\":\"43658\"},{\"start\":\"43658\",\"end\":\"43665\"},{\"start\":\"43665\",\"end\":\"43671\"},{\"start\":\"43886\",\"end\":\"43894\"},{\"start\":\"43894\",\"end\":\"43907\"},{\"start\":\"43907\",\"end\":\"43918\"},{\"start\":\"44078\",\"end\":\"44086\"},{\"start\":\"44086\",\"end\":\"44093\"},{\"start\":\"44093\",\"end\":\"44101\"},{\"start\":\"44101\",\"end\":\"44109\"},{\"start\":\"44272\",\"end\":\"44280\"},{\"start\":\"44280\",\"end\":\"44288\"},{\"start\":\"44288\",\"end\":\"44297\"},{\"start\":\"44297\",\"end\":\"44305\"},{\"start\":\"44305\",\"end\":\"44312\"},{\"start\":\"44312\",\"end\":\"44320\"},{\"start\":\"44569\",\"end\":\"44575\"},{\"start\":\"44575\",\"end\":\"44586\"},{\"start\":\"44799\",\"end\":\"44808\"},{\"start\":\"44808\",\"end\":\"44815\"},{\"start\":\"44815\",\"end\":\"44824\"},{\"start\":\"44824\",\"end\":\"44830\"},{\"start\":\"44994\",\"end\":\"45003\"},{\"start\":\"45003\",\"end\":\"45015\"},{\"start\":\"45015\",\"end\":\"45024\"},{\"start\":\"45024\",\"end\":\"45034\"},{\"start\":\"45034\",\"end\":\"45045\"},{\"start\":\"45234\",\"end\":\"45243\"},{\"start\":\"45243\",\"end\":\"45255\"},{\"start\":\"45255\",\"end\":\"45266\"},{\"start\":\"45446\",\"end\":\"45455\"},{\"start\":\"45455\",\"end\":\"45463\"},{\"start\":\"45463\",\"end\":\"45474\"},{\"start\":\"45650\",\"end\":\"45659\"},{\"start\":\"45659\",\"end\":\"45666\"},{\"start\":\"45666\",\"end\":\"45674\"},{\"start\":\"45674\",\"end\":\"45681\"},{\"start\":\"45681\",\"end\":\"45689\"},{\"start\":\"45889\",\"end\":\"45897\"},{\"start\":\"45897\",\"end\":\"45905\"},{\"start\":\"45905\",\"end\":\"45913\"},{\"start\":\"46119\",\"end\":\"46127\"},{\"start\":\"46127\",\"end\":\"46135\"},{\"start\":\"46323\",\"end\":\"46331\"},{\"start\":\"46331\",\"end\":\"46339\"}]", "bib_venue": "[{\"start\":\"36691\",\"end\":\"36746\"},{\"start\":\"36864\",\"end\":\"36930\"},{\"start\":\"37071\",\"end\":\"37135\"},{\"start\":\"37380\",\"end\":\"37431\"},{\"start\":\"37668\",\"end\":\"37673\"},{\"start\":\"37803\",\"end\":\"37850\"},{\"start\":\"37970\",\"end\":\"38037\"},{\"start\":\"38243\",\"end\":\"38289\"},{\"start\":\"38504\",\"end\":\"38550\"},{\"start\":\"38702\",\"end\":\"38771\"},{\"start\":\"38941\",\"end\":\"38961\"},{\"start\":\"39006\",\"end\":\"39050\"},{\"start\":\"39278\",\"end\":\"39281\"},{\"start\":\"39405\",\"end\":\"39468\"},{\"start\":\"39622\",\"end\":\"39659\"},{\"start\":\"39819\",\"end\":\"39873\"},{\"start\":\"40006\",\"end\":\"40040\"},{\"start\":\"40243\",\"end\":\"40330\"},{\"start\":\"40528\",\"end\":\"40609\"},{\"start\":\"40793\",\"end\":\"40837\"},{\"start\":\"41109\",\"end\":\"41112\"},{\"start\":\"41242\",\"end\":\"41326\"},{\"start\":\"41499\",\"end\":\"41575\"},{\"start\":\"41716\",\"end\":\"41760\"},{\"start\":\"41869\",\"end\":\"41932\"},{\"start\":\"42080\",\"end\":\"42143\"},{\"start\":\"42273\",\"end\":\"42336\"},{\"start\":\"42544\",\"end\":\"42589\"},{\"start\":\"42747\",\"end\":\"42825\"},{\"start\":\"43000\",\"end\":\"43053\"},{\"start\":\"43274\",\"end\":\"43367\"},{\"start\":\"43530\",\"end\":\"43634\"},{\"start\":\"43918\",\"end\":\"43922\"},{\"start\":\"44025\",\"end\":\"44076\"},{\"start\":\"44222\",\"end\":\"44270\"},{\"start\":\"44451\",\"end\":\"44567\"},{\"start\":\"44746\",\"end\":\"44797\"},{\"start\":\"44943\",\"end\":\"44992\"},{\"start\":\"45177\",\"end\":\"45232\"},{\"start\":\"45382\",\"end\":\"45444\"},{\"start\":\"45593\",\"end\":\"45648\"},{\"start\":\"45816\",\"end\":\"45887\"},{\"start\":\"46037\",\"end\":\"46117\"},{\"start\":\"46254\",\"end\":\"46321\"}]"}}}, "year": 2023, "month": 12, "day": 17}