{"id": 8632684, "updated": "2023-04-05 22:48:13.287", "metadata": {"title": "Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images", "authors": "[{\"first\":\"Jamie\",\"last\":\"Shotton\",\"middle\":[]},{\"first\":\"Ben\",\"last\":\"Glocker\",\"middle\":[]},{\"first\":\"Christopher\",\"last\":\"Zach\",\"middle\":[]},{\"first\":\"Shahram\",\"last\":\"Izadi\",\"middle\":[]},{\"first\":\"Antonio\",\"last\":\"Criminisi\",\"middle\":[]},{\"first\":\"Andrew\",\"last\":\"Fitzgibbon\",\"middle\":[]}]", "venue": "2013 IEEE Conference on Computer Vision and Pattern Recognition", "journal": "2013 IEEE Conference on Computer Vision and Pattern Recognition", "publication_date": {"year": 2013, "month": null, "day": null}, "abstract": "We address the problem of inferring the pose of an RGB-D camera relative to a known 3D scene, given only a single acquired image. Our approach employs a regression forest that is capable of inferring an estimate of each pixel's correspondence to 3D points in the scene's world coordinate frame. The forest uses only simple depth and RGB pixel comparison features, and does not require the computation of feature descriptors. The forest is trained to be capable of predicting correspondences at any pixel, so no interest point detectors are required. The camera pose is inferred using a robust optimization scheme. This starts with an initial set of hypothesized camera poses, constructed by applying the forest at a small fraction of image pixels. Preemptive RANSAC then iterates sampling more pixels at which to evaluate the forest, counting inliers, and refining the hypothesized poses. We evaluate on several varied scenes captured with an RGB-D camera and observe that the proposed technique achieves highly accurate relocalization and substantially out-performs two state of the art baselines.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "1989476314", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/ShottonGZICF13", "doi": "10.1109/cvpr.2013.377"}}, "content": {"source": {"pdf_hash": "00f2a152454db273b0d6831b6550beb37a135890", "pdf_src": "Grobid", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "b8214e208012770543e1915a7e0097abacb472bb", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/00f2a152454db273b0d6831b6550beb37a135890.txt", "contents": "\nScene Coordinate Regression Forests for Camera Relocalization in RGB-D Images\n\n\nJamie Shotton \nMicrosoft Research\nCambridgeUK\n\nBen Glocker \nMicrosoft Research\nCambridgeUK\n\nChristopher Zach \nMicrosoft Research\nCambridgeUK\n\nShahram Izadi \nMicrosoft Research\nCambridgeUK\n\nAntonio Criminisi \nMicrosoft Research\nCambridgeUK\n\nAndrew Fitzgibbon \nMicrosoft Research\nCambridgeUK\n\nScene Coordinate Regression Forests for Camera Relocalization in RGB-D Images\n10.1109/CVPR.2013.377\nWe address the problem of inferring the pose of an RGB-D camera relative to a known 3D scene, given only a single acquired image. Our approach employs a regression forest that is capable of inferring an estimate of each pixel's correspondence to 3D points in the scene's world coordinate frame. The forest uses only simple depth and RGB pixel comparison features, and does not require the computation of feature descriptors. The forest is trained to be capable of predicting correspondences at any pixel, so no interest point detectors are required. The camera pose is inferred using a robust optimization scheme. This starts with an initial set of hypothesized camera poses, constructed by applying the forest at a small fraction of image pixels. Preemptive RANSAC then iterates sampling more pixels at which to evaluate the forest, counting inliers, and refining the hypothesized poses. We evaluate on several varied scenes captured with an RGB-D camera and observe that the proposed technique achieves highly accurate relocalization and substantially out-performs two state of the art baselines.\n\nIntroduction\n\nThis paper presents a new, efficient algorithm for estimating the camera pose from a single RGB-D image, relative to a known scene (or environment). This has important applications in robotics (the 'lost robot' problem), SLAM, augmented reality, and navigation. A standard approach for solving the problem is first to find a set of putative correspondences between image pixels and 3D points in the scene, and second to optimize the camera pose to minimize some energy function defined over these correspondences. In this work, we first demonstrate how regression forests can be used to predict the correspondences, and further show how to optimize the camera pose efficiently.\n\nOur main contribution is the scene coordinate regression forest (SCoRe Forest). As illustrated in Fig. 1, the forest is trained to directly predict correspondences from any image pixel to points in the scene's 3D world coordinate frame. The aim is that, in one go, the forest can remove the need for the traditional pipeline of feature detection, description, and matching. A SCoRe Forest is trained on a particular scene,  3D representation of a scene's shared world coordinate frame, with overlaid ground truth camera frusta for the images below. The color visualization maps scene coordinates to the RGB cube. A scene coordinate regression forest (SCoRe Forest) is trained to infer the scene coordinates at any image pixel. (Bottom) Three test frames: the input RGB and depth images; the ground truth scene coordinate pixel labels; and the inliers inferred by the SCoRe Forest after camera pose optimization. For this visualization we show all inlier pixels, but note that the optimization algorithm only actually evaluates the forest at a much sparser set of pixels. Fig. 7 shows example inferred camera poses.\n\nusing RGB-D images with known camera poses. The depth maps and camera poses are sufficient to compute scene coordinate training labels at every pixel. These labels are used in a standard regression objective to learn the forest. SCoRe Forests employ only simple RGB and depth image pixel comparison features which are fast to compute. Our second contribution is an efficient test-time camera pose optimization algorithm based on RANSAC; see Fig. 2.\n\nThe algorithm assumes only a single RGB-D image as input. We optimize an energy function that measures the number of pixels for which the SCoRe Forest predictions agree with a given pose hypothesis. Because we trained the forest densely, we are free to test the forest at whichever (valid) image pixels we please. The RANSAC algorithm can thus efficiently evaluate the SCoRe Forest at only a sparse, randomly sampled set of pixels. Note that the optimization does not require an explicit 3D model of the scene; instead, the forest implicitly encodes the scene structure. This could be useful when a 3D model is not available, or in low power (e.g. cellphone) scenarios. Our approach is also completely stateless: we are able to localize the camera from a single frame without tracking.\n\nWe validate the proposed camera pose estimation technique on a new dataset of seven varied scenes which will be made available for research purposes. Our experiments demonstrate superior localization accuracy compared to two state of the art baselines, and high accuracy at recognizing to which scene a particular image belongs.\n\n\nRelated work\n\nTwo common approaches for image-based camera relocalization use global image matching and sparse feature matching, though other approaches exist e.g. [23].\n\nIn global image matching (e.g. [17]), an approximate camera pose is hypothesized by computing whole-image similarity measures between a query image and a set of keyframes with known associated keyposes. Given the set of best matching images, the final camera pose is computed as a weighted average of the keyposes. This approach was extended to use synthetic RGB-D views in [11].\n\nA perhaps more prevalent alternative uses sparse keypoint matching. Here a set of interest points are detected in the image, assigned a descriptor based on appearance, and matched against a database of descriptors. Much work has focused on efficiency, scalability, and learning of feature detection [13,26], description [4,27,34], and matching [18,25,29]. Closely related to our approach, Lepetit et al. [18] learn to match particular discrete features in images with random forests. Our approach goes beyond this by using a regression approach such that every pixel can provide correspondence to the scene, without feature detectors.\n\nGiven sparse keypoint matches, pose estimation is achieved either via intermediate image retrieval [2,8,14] or by directly establishing 2D-3D correspondences for a robust pose optimization [19,28]. Efficient keypoint-based relocalization modules have been proposed for visual SLAM [9,33] and robotics [30].\n\nA limitation of keypoint-based approaches lies in their inherent sparse representation of the scene: the number of correctly detected and matched points directly impacts the accuracy of the pose estimation. Our SCoRe Forests can be applied as sparsely or densely as required, avoiding this problem.\n\nSharing a similar spirit to our work, the LayoutCRF [12,35] defines a coarse grid of classification labels that roughly cover an object, and then exploits layout consistency terms in an MRF optimization framework to segment the objects. Our approach instead regresses more precise labels defined on 3D world space, and avoids expensive image-space optimization. Taylor et al. [32] use a regression forest to infer correspondences between each depth image pixels and points in a canonical articulated 3D human mesh. Our approach extends [32] by: introducing an efficient camera pose optimization that evaluates the forest sparsely; showing that the regression approach works well on arbitrary scene model topologies and on RGB-D input data; and demonstrating how to effectively exploit the several predictions given by multiple trees in a forest.\n\n\nScene Coordinate Regression Forests\n\nIn this section we provide some background on regression forests, detail our new scene coordinate pixel labeling, and discuss how this labeling is used to train a forest for each scene. Sec. 3 will then describe how the camera pose is optimized by evaluating the forest at a sparse set of pixels.\n\n\nRegression forests\n\nWe give a brief introduction to regression forests; for more details, please see [7]. Decision forests have proven highly capable learning machines. They are most commonly used for classification tasks, e.g. [1,18,31], though increasingly for regression problems, e.g. [10,20,32]. We employ a fairly standard regression forest approach, optimizing a reduction-in-spatial-variance objective.\n\nA regression forest is an ensemble of T decision trees, each consisting of split (internal) and leaf nodes. Each split node contains a 'weak learner' represented by its parameters \u03b8 = (\u03c6, \u03c4): feature parameters \u03c6 (see below), and a scalar threshold \u03c4 . To evaluate a regression tree at a 2D pixel location p in an image, we start at the root node and descend to a leaf by repeatedly evaluating the weak learner:\nh(p; \u03b8 n ) = f \u03c6 n (p) \u2265 \u03c4 n ,(1)\nwhere n denotes the index of a node in the tree, [\u00b7] is the 0-1 indicator, and f \u03c6 is one of the feature response functions we define below in Sec. 2.2. If h(p; \u03b8 n ) evaluates to 0, the path branches to the left child of n, otherwise it branches to the right. This repeats until leaf node l t (p) is reached for tree t in the forest. Each leaf node in a regression tree stores a distribution P l (m) over a continuous variable m. In our application these m \u2208 R 3 are the coordinates in the scene's 3D world space, as described in Sec. 2.3. For efficiency, we represent the distribution P l (m) as a set M l of modes of the distribution, which are found using mean shift. The final prediction of the forest at pixel p is simply the union of these modes across all trees: M(p) = t M lt(p) .\n\n\nImage features\n\nWe investigate three variants of regression forests, each of which uses a different combination of RGB and depth features. We will refer to the variants as 'Depth', 'Depth-Adaptive RGB' (DA-RGB), and 'Depth-Adaptive RGB + Depth' (DA-RGB + D) forests.\n\nAll features are based on simple pixel comparisons [18,31] and so are extremely fast to evaluate. The two types of feature responses can be computed as follows:\nf depth \u03c6 (p) = D p + \u03b4 1 D(p) \u2212 D p + \u03b4 2 D(p) (2) f da-rgb \u03c6 (p) = I p + \u03b4 1 D(p) , c 1 \u2212 I p + \u03b4 2 D(p) , c 2(3)\nHere, \u03b4 indicates a 2D offset, D(p) indicates a depth pixel lookup, and I(p, c) indicates an RGB pixel lookup in channel c. Each split node in the forest stores a unique set of pa-\nrameters \u03c6 n \u2286 {\u03b4 1 , \u03b4 2 , c 1 , c 2 , z}, with z \u2208 {depth, da-rgb}\nindicating the type of feature to use. Pixels with undefined depth and those outside the image boundary are assigned D = 6m, and are not used as examples for training or test. These features can implicitly encode contextual information, as the offsets can be fairly long range. The division by D(p) makes the features largely depth invariant, and is similar in spirit to [36]. We assume a reasonable registration of depth and RGB images, such as is provided by standard RGB-D camera APIs. However, the registration need not be perfect as the forest will learn some degree of tolerance to misregistration.\n\n\nScene coordinate labels\n\nOne of the main contributions of this work is the use of scene coordinates to define the labels used to train the regression forest. By using scene coordinate labels, the forest will learn to directly predict the position in the scene's world space that corresponds to a test pixel.\n\nOur training set consists of a set of RGB-D frames with known 6 d.o.f. camera pose matrices H that encode the 3D rotation and translation from camera space to world space. This data could be captured in several ways, for example by tracking from depth camera input [15,21], or by using dense reconstruction and tracking from RGB input [22].\n\nOur labels are defined as follows. At pixel p, the calibrated depth D(p) allows us to compute the 3D camera space coordinate x. Using homogeneous coordinates, this camera position can be transformed into the scene's world coordinate frame as m = Hx. Our labels are simply defined as these scene world positions, m.\n\nWe train the forest using pixels drawn from all training images, so the forest can be applied at any test image pixel. In particular, one can evaluate the forest at any sparse set of test pixels. If the forest were a perfect predictor, only three pixel predictions would be required to infer the camera pose. In practice, the forest instead makes noisy predictions, and so we employ the efficient optimization described in Sec. 3 to accurately infer the camera pose.\n\n\nForest training\n\nGiven the scene coordinate pixel labeling defined above, we can now grow the regression forest using the standard greedy forest training algorithm [7], summarized next. For each tree, we randomly choose a set S of labeled example pixels (p, m). 1 The tree growing then proceeds recursively, starting at the root node. At each node n, a set of candidate weak learner parameters \u03b8 is sampled at random. Each candidate \u03b8 is evaluated in turn by (1) to partition the set S n into left and right subsets S L n and S R n respectively. Given this partition, a tree training objective function Q(\u03b8) is computed. We have found the following simple reduction-inspatial-variance objective to work well:\nQ(S n , \u03b8) = V (S n ) \u2212 d\u2208{L,R} |S d n (\u03b8)| |S n | V (S d n (\u03b8)) , (4) with V (S) = 1 |S| (p,m)\u2208S m \u2212m 2 2 ,(5)\nandm being the mean of m in S. The candidate parameter set \u03b8 which results in the largest reduction in variance is taken, and the training recurses on the resulting left and right children. Tree growing terminates when a node reaches a maximum depth D max , or the set S n has only one element. Mode fitting.\n\nOnce the tree has been grown, the final stage of training is to summarize the distribution over m as a set of modes M l , for each leaf node l. To reduce training time, we sub-sample the set S l of training pixels reaching leaf l to at most N ss = 500 examples. We then run mean shift mode detection [6] with a Gaussian kernel of bandwidth \u03ba = 0.1m. This clusters the points m in S l into a small set of modes. In our current implementation, we keep only a single mode at a leaf node: the mode to which the largest number of examples was assigned.\n\n\nCamera Pose Optimization\n\nThe regression forest described in the previous section is capable of associating scene coordinates with any 2D image pixel. We now discuss how to use this information to estimate the camera location and orientation. The problem is cast as the energy minimization\nH * = argmin H E(H)(6)\nover the camera pose matrix H. An overview of the method is given in Fig. 2.\n\n\nEnergy function\n\nWe define our energy function as follows\nE(H) = i\u2208I \u03c1 min m\u2208Mi m \u2212 Hx i 2 = i\u2208I e i (H) ,(7)\nwhere: i \u2208 I is a pixel index; \u03c1 is a robust error function; M i = M(p i ) represents the set of modes (3D locations in the scene's world space) predicted by the trees in the forest at pixel p i ; and x i are the 3D coordinates in camera space corresponding to pixel p i , obtained by back-projecting the depth image pixels. We use a top-hat error function \u03c1 with a width of 0.1m. Pixels for which \u03c1 evaluates to 0 are considered inliers, and pixels for which \u03c1 evaluates to 1 are considered outliers. The energy function above thus counts the number of outliers for a given camera hypothesis H.\n\nNote that computing this energy does not require an explicit 3D model of the scene: the model is implicitly encoded in the regression forest. Because the forest has been trained to work at any image pixel, we can randomly sample pixels at test time. This sampling avoids both the need to compute interest points and the expense of densely evaluating the forest. The summation in (7) is thus computed over a subset I of all possible image pixels; the larger the size of this subset, the more useful the energy E can be at ranking pose hypotheses. The minimization over predictions m \u2208 M i means that at each pixel, the mode that is closest to the transformed observed pixel will be chosen. A consequence of this is that the minimization will infer at each pixel which tree in the forest gave the best prediction under a given hypothesis.\n\n\nOptimization\n\nTo optimize this energy, we use an adapted version of preemptive RANSAC [24] (see Algorithm 1). The algorithm starts by sampling a set of K init initial hypotheses. It then randomly samples a new batch of B pixels, evaluates the forest at these pixels, and updates the energies for each hypothesis based on the forest predictions. The hypotheses are re-ranked by energy, and the highest energy half of the hypotheses is discarded ('preempted'). Each remaining hypothesis is then refined [5] based on the set of inliers computed as a by-product of evaluating the energy at Line 9 of Algorithm 1. The while loop terminates when only a single hypothesis remains.\n\nThe iteration could instead be stopped earlier, either for efficiency, or if desired to obtain the top K hypotheses. The algorithm as presented evaluates the forest on-the-fly; alternatively the forest could be evaluated at the B log 2 K init required pixels in advance, though this would require extra storage. Other RANSAC schedules (e.g. no preemption) are possible, though not investigated here. Initial hypothesis sampling. Each initial pose hypothesis is sampled as follows. The forest is evaluated at three pixels (the minimal number required), and at each of those pixels a random mode m \u2208 M i is sampled. These putative correspondences are passed to the Kabsch algorithm [16] (also known as orthogonal Procrustes alignment) which uses a singular value decomposition (SVD) to solve for the camera pose hypothesis with least squared error. Pose refinement.\n\nExperimentally we found a pose refinement step [5] to be crucial to achieving accurate localization. We simply re-run the Kabsch algorithm on the enlarged set of inliers. For efficiency, we only store and update refine hypotheses H k for k = 1, . . . , K 13: return best pose H1 and energy E1 the means and covariance matrices used by the SVD. Note that the refinement step means the final pose can be much more accurate than the individual forest pixel predictions of the inliers.\n\n\nEvaluation\n\n\nDataset\n\nWe introduce a new RGB-D dataset, '7 Scenes', to evaluate our technique and compare with other approaches. We will release this dataset to facilitate future research and comparison with our approach. All scenes were recorded from a handheld Kinect RGB-D camera at 640\u00d7480 resolution. We use an implementation of the KinectFusion system [15,21] to obtain the 'ground truth' camera tracks, and a dense 3D model (used only for visualization and the ICP experiments below). Several sequences were recorded per scene by different users, and split into distinct training and testing sequence sets. An overview of the scenes and camera tracks are shown in Fig. 3, and more details are given in Table 1. Both RGB and depth images exhibit ambiguities (e.g. repeated steps in Stairs), specularities (e.g. reflective cupboards in RedKitchen), motion-blur, lighting conditions, flat surfaces, and sensor noise. The varying difficulties of the scenes are reflected in the error metrics, consistently across all approaches.\n\n\nMetrics\n\nAs our main test metric we report the percentage of test frames for which the inferred camera pose is essentially 'correct'. We employ a fairly strict definition of correct: the pose must be within 5cm translational error and 5 \u2022 angular error of the ground truth (for comparison, our scenes have size up to 6m 3 ). The belief is that correctly localizing the pose under this metric might already be sufficient for some augmented reality applications, and should certainly be sufficient to restart any good model-based tracking system.\n\n\nBaselines\n\nWe compare our approach against two complementary baseline methods: the 'sparse' baseline exploits matches between extracted features and a sparse 3D point cloud model, while the 'tiny-image' baseline matches downsampled whole images to hypothesize putative camera poses. Sparse baseline. This baseline uses state of the art featurematching techniques. The approach employs the fast ORB descriptor [27] for feature detection and description. We build a (sparse) 3D point cloud with attached ORB descriptors using the RGB-D training images. At test time (using only RGB), features are matched to the 3D point cloud by hashing 14-bit descriptor substrings. We consider a 2D-3D correspondence a putative match if the Hamming distance between descriptor bit-strings is at most 50. The set of potential inliers is geometrically verified using RANSAC and the perspective 3-point method. The final camera pose is refined using a gold-standard method computed on all inlier 2D-3D correspondences. Tiny-image baseline. This baseline represents state of the art whole-image matching approaches [11,17]. All training images are stored with their camera poses, after downsampling to 40 \u00d7 30 pixels and applying a Gaussian blur of \u03c3 = 2.5 pixels [17]. At test time, to achieve the best possible accuracy for this baseline (at the expense of speed), we use brute-force matching against the entire training set, using the normalized Euclidean distance of [11]. The camera pose is finally computed as a weighted average of the poses of the 100 closest matches [11]. We compare below with RGB, depth, and RGB-D variants of this approach.\n\n\nMain results\n\nWe present our main results and comparisons in Table 1. Our approach considerably outperforms both baselines on all but one of the scenes. The tiny-image baseline fails almost completely under our strict metric: the poses of the images in the training set are simply too far from the poses in the test set, and this approach cannot generalize well. 2 The sparse baseline does a reasonable job, though struggles when too few features are detected. This can happen under motion blur, when viewing textureless surfaces, and when noise levels are high. While our approach is also sparse, the SCoRe Forests can be evaluated at any pixel in the image, and so we are not dependent on feature detection. We present some qualitative results in Fig. 7, showing both successes and failures of our algorithm and the sparse baseline.\n\nOf the three forest feature modalities we test, depth-only always performs worst. This is to be expected given the large amount of noise (especially holes) in the depth im-  Fig. 1 Table 1. Main results. We list: properties of our dataset; the results of the two baselines; the results of our approach using SCoRe Forests trained with three different feature types; and results from a simple frame-to-frame extension of our approach (Sec. 4.5), here using DA-RGB features. The percentages are the proportion of 'correct' test frames (within 5cm translational and 5 \u2022 angular error). Our approach outperforms both baselines on all but one scene. The tiny-image baseline gives only imprecise localization and so fails almost completely under our tight metric (though see also Fig. 6). We tested multiple random restarts of RANSAC; all standard deviations were < 1%.\n\nages. We also observe that DA-RGB performs better than DA-RGB + D in 5 out of 7 scenes. This slightly surprising result may be due to sub-optimal settings of the forest training parameters (see below), but in any case both modalities outperform the baseline in all but one scene. In Fig. 4 we see that our inferred camera poses form a remarkably smooth track compared to the sparse baseline. This is likely because our algorithm is able to sample more putative matches than the sparse feature-based approach. Of course, if one were building a real tracking system, both approaches could be smoothed much further. Failure modes. Our main failure modes are (i) imprecise localization, and (ii) failures due to ambiguities. As an example, if we widen our metric to allow 10cm translational errors, our DA-RGB forests achieve 100% accuracy on Chess, 58.8% on Heads, and 49.0% on Stairs. If we further use an oracle to pick the most accurate pose from the top 10 hypotheses, we achieve 68.4% on Stairs, highlighting the inherent ambiguity in this sequence. (See also Fig. 7 bottom right). Pose refinement. The pose refinement step in Algorithm 1 proved crucial to achieving good results. With this turned off, our DA-RGB forests achieve only 31% accuracy on Chess and 9.5% on Office, for example. Parameter settings. In all the experiments presented, the following parameters were used: initial number of hypotheses K init = 1024; batch size B = 500; T = 5 trees in the forest, trained to depth D max = 16; 500 training images per tree and 5000 example pixels per training image, both randomly chosen; feature offsets \u03b4 having a maximum of \u00b1130 pixel meters; and |M(l)| = 1, i.e. one mode prediction m per tree. The sparse baseline uses almost identical RANSAC parameters. The tiny-image baseline uses the parameters from [11,17].\n\nWe have not optimized the parameters of our approach: they were chosen by hand once and fixed. Preliminary investigation suggests considerable insensitivity of the pose optimization to changes in the training parameters, though it is reasonable to assume a more thorough optimization of these parameters would improve our accuracy. Image resolution. Fig. 5(a) illustrates that our approach is robust to nearest-neighbor image downsampling. Timings. Our unoptimized C++ test time implementation takes approximately 100ms per frame on a single modern CPU core. Of course, this depends heavily on the RANSAC parameters, and one could easily trade speed for accuracy. In comparison, the sparse baseline (also not optimized) takes approximately 250ms per frame, and the (deliberately inefficient brute-force) tiny-image baseline takes 500ms-3000ms according to the number of images in the training database. Training takes around 1-10 minutes per scene according to parameter settings.\n\n\nFrame-to-frame tracking\n\nA simple adaptation to our approach allows a basic form of frame-to-frame tracking. Here, we initialize one hypothesis (out of the K init ) with the camera pose inferred at the previous frame. This can bias the optimization to finding a good solution when the camera motion is not too great. We achieved the improvements in pose accuracy shown in the final column of Table 1. For all scenes there is an improve- Our algorithm gives remarkably smooth camera tracks from single frames at a time. ment over our standard algorithm, greater than 10 percentage points for the Office, RedKitchen, and Stairs results. We expect an even greater improvement could be obtained by initializing (possibly multiple) hypotheses from the previous frame using a motion model.\n\n\nScene recognition\n\nBeyond relocalizing within a particular scene, one might also want to recognize to which scene a particular camera view belongs. We built the following simple scene recognition system to investigate this. For a given test frame, our pose optimization algorithm is run, separately for each scene's SCoRe Forest. The scene with the lowest energy (largest number of inliers) is then taken as the result.\n\nEvaluated on every 20 th frame in our entire test set using the DA-RGB modality, the confusion matrix is given in Fig. 5(b). We observe very high scene recognition accuracy: the mean of the diagonal is 96.6%, with several scenes achieving perfect recognition. The energy (7) returned by the RANSAC optimization appears to be an accurate measure for disambiguating scenes.\n\nAn alternative, not investigated here, would be to embed all the scenes in a shared 3D world so that the coordinate pixel labels uniquely identify a point in a particular scene. A single forest could thus encapsulate all scenes, and scene recognition would simply be a by-product of pose estimation. A downside of this approach would be the need to retrain everything in order to add new scenes.\n\n\nModel-based pose refinement\n\nThe results above demonstrate that accurate relocalization is achievable without an explicit 3D model. However, if such a model is available (e.g. from [15,21]), then it is possible to refine our inferred camera pose against that model. To investigate, we ran an experiment which performs an ICP-based pose refinement [3] starting at our inferred pose. In Fig. 6 we compare the pose accuracy after ICP for our algorithm and the two baselines. We further  Figure 6. Accuracy after ICP to a 3D model. As expected, all results improve compared to Table 1. add a brute-force 'ICP Residual' baseline which, for each test frame, runs ICP starting from each training keyposes (for tractability, a frame becomes a keypose if it is > 5cm away from the previous selected keypose), and takes the resulting pose with lowest ICP residual error. Our DA-RGB and DA-RGB + D forests outperform all baselines on all scenes, and even give a perfect result for Chess. Note that the complete failure of the tiny-image baseline in Table 1 is much improved, suggesting that this baseline approach is only useful when paired with a refinement step.\n\n\nConclusions\n\nWe have demonstrated how scene coordinate regression forests (SCoRe Forests) can be trained to directly predict correspondences between image pixels and a scene's world space. For known environments, SCoRe Forests thus offer an alternative to the traditional pipeline of sparse feature detection, description, and matching. Our efficient RANSAC optimization is able to sparsely sample image pixels at which to evaluate the forest. The comparison on the challenging new 7 Scenes dataset with two baselines indicates that our algorithm achieves state of the art camera relocalization.\n\nAs future work we believe that our approach will extend very naturally to RGB-only test images. We also plan to investigate sampling additional training data by rendering new views from 3D scene reconstructions. Finally, we hope that an optimized implementation could be extended to support on-line model learning and relocalization. Figure 7. Example results. In each test example, we show the RGB and depth images, and a 3D visualization of the underlying scene with camera frusta overlaid: the ground truth pose (red), our inferred pose using a DA-RGB SCoRe Forest (blue), the sparse baseline result (green), and the closest pose in the training set (yellow). Observe good generalization across a variety of scenes to views not present during training. The bottom row shows failure cases, e.g. due to repeated structures in the scene. Best viewed digitally at high zoom.\n\nFigure 1 .\n1Scene coordinate regression labels. (Top) A\n\nFigure 2 .\n2Camera pose estimation. Top: An example RGB-D test input. Middle: Our RANSAC optimization uses a scene coordinate regression forest (SCoRe Forest) to obtain image to scene correspondences. The algorithm maintains a set of inlier pixels for each of several camera pose hypotheses. Bottom right: The hypothesis with the lowest energy (highest number of inliers) is chosen as the final inferred pose (shown as the blue frustum; the ground truth is shown in red). For clarity of exposition, we show all image pixels that are inliers to each hypothesis; in fact our algorithm samples pixels sparsely and does not need to evaluate the SCoRe Forest at every pixel.\n\nFigure 3 .\n3The 7 Scenes dataset. These renderings show the scene coordinate representations for each scene as RGB colors. The green and red camera tracks show the positions of the cameras in the training and test sequences. The Office scene is shown in\n\nFigure 4 .\n4Camera tracks for the Chess scene. Red: ground truth. Blue: our result using a DA-RGB forest. Green: sparse baseline.\n\nFigure 5 .\n5(a) Effect of image resolution on accuracy for DA-RGB Chess. (b) Scene recognition confusion matrix. For Chess, Heads, and Pumpkin we achieve correct recognition for all test frames.\n\n\nAlgorithm 1 Pseudocode for RANSAC optimization 1: K \u2190 Kinit 2: sample initial hypotheses {H k } K k=1 3: initialize energies E k \u2190 0 for k = 1, . . . , K 4: while K > 1 do for all k \u2208 {1, . . . , K} doE k \u2190 E k + ei(H k )5: \n\nsample random set of B test pixels I \n\n6: \n\nfor all i \u2208 I do \n\n7: \n\nevaluate forest to obtain Mi \n\n8: \n\n9: \n\n10: \n\nsort hypotheses (H k , E k ) by E k \n\n11: \n\nK \u2190 K \n\n2 \n\n12: \n\n\n\n\n.Spatial \n# Frames \nBaselines \nOur Results \nFrame-to-Frame \nScene \nExtent \nTrain Test \nTiny-image RGB-D Sparse RGB \nDepth \nDA-RGB DA-RGB + D \nTracking \nChess \n3m 3 \n4k \n2k \n0.0% \n70.7% \n82.7% \n92.6% \n91.5% \n95.5% \nFire \n4m 3 \n2k \n2k \n0.5% \n49.9% \n44.7% \n82.9% \n74.7% \n86.2% \nHeads \n2m 3 \n1k \n1k \n0.0% \n67.6% \n27.0% \n49.4% \n46.8% \n50.7% \nOffice \n5.5m 3 \n6k \n4k \n0.0% \n36.6% \n65.5% \n74.9% \n79.1% \n86.8% \nPumpkin \n6m 3 \n4k \n2k \n0.0% \n21.3% \n58.6% \n73.7% \n72.7% \n76.1% \nRedKitchen \n6m 3 \n7k \n5k \n0.0% \n29.8% \n61.3% \n71.8% \n72.9% \n82.4% \nStairs \n5m 3 \n2k \n1k \n0.0% \n9.2% \n12.2% \n27.8% \n24.4% \n39.2% \n\n\nFor notational clarity p is used to uniquely index a pixel within a particular image. Training pixels are sampled up to the image boundary.\nThe tiny-image baseline does however provide a sensible rough localization, good enough for model-based pose refinement (Sec. 4.7).\nAcknowledgements. We thank Pushmeet Kohli and the anonymous reviewers for their valuable input.\nShape quantization and recognition with randomized trees. Y Amit, D Geman, Neural Computation. 97Y. Amit and D. Geman. Shape quantization and recognition with randomized trees. Neural Computation, 9(7), 1997. 2\n\nLeveraging 3D city models for rotation invariant place-of-interest recognition. G Baatz, K K\u00f6ser, D Chen, R Grzeszczuk, M Pollefeys, IJCV. 2G. Baatz, K. K\u00f6ser, D. Chen, R. Grzeszczuk, and M. Pollefeys. Leveraging 3D city models for rotation invariant place-of-interest recognition. IJCV, 2011. 2\n\nA method for registration of 3-D shapes. P Besl, N Mckay, 1992. 7IEEE Trans. PAMI. 142P. Besl and N. McKay. A method for registration of 3-D shapes. IEEE Trans. PAMI, 14(2), 1992. 7\n\nBRIEF: Binary robust independent elementary features. M Calonder, V Lepetit, C Strecha, P Fua, Proc. ECCV. ECCVM. Calonder, V. Lepetit, C. Strecha, and P. Fua. BRIEF: Binary robust independent elementary features. In Proc. ECCV, 2010. 2\n\nLocally optimized RANSAC. O Chum, J Matas, J Kittler, Proc. DAGM. DAGM4O. Chum, J. Matas, and J. Kittler. Locally optimized RANSAC. In In Proc. DAGM, 2003. 4\n\nMean shift: A robust approach toward feature space analysis. D Comaniciu, P Meer, IEEE Trans. PAMI. 245D. Comaniciu and P. Meer. Mean shift: A robust approach toward feature space analysis. IEEE Trans. PAMI, 24(5), 2002. 3\n\nDecision Forests for Computer Vision and Medical Image Analysis. A Criminisi, J Shotton, Springer23A. Criminisi and J. Shotton. Decision Forests for Computer Vision and Medical Image Analysis. Springer, 2013. 2, 3\n\nKeyframe-based real-time camera tracking. Z Dong, G Zhang, J Jia, H Bao, Proc. ICCV. ICCVZ. Dong, G. Zhang, J. Jia, and H. Bao. Keyframe-based real-time camera tracking. In Proc. ICCV, 2009. 2\n\nUnified loop closing and recovery for real time monocular SLAM. E Eade, T Drummond, Proc. BMVC. BMVCE. Eade and T. Drummond. Unified loop closing and recovery for real time monocular SLAM. In Proc. BMVC, 2008. 2\n\nHough Forests for object detection, tracking, and action recognition. J Gall, A Yao, N Razavi, L Van Gool, V Lempitsky, IEEE Trans. PAMI. 3311J. Gall, A. Yao, N. Razavi, L. Van Gool, and V. Lempitsky. Hough Forests for object detection, tracking, and action recognition. IEEE Trans. PAMI, 33(11), 2011. 2\n\n6D relocalisation for RGBD cameras using synthetic view regression. A Gee, W Mayol-Cuevas, Proc. BMVC. BMVC6A. Gee and W. Mayol-Cuevas. 6D relocalisation for RGBD cameras using synthetic view regression. In Proc. BMVC, 2012. 2, 5, 6\n\n3D LayoutCRF for multi-view object class recognition and segmentation. D Hoiem, C Rother, J Winn, Proc. CVPR. CVPRD. Hoiem, C. Rother, and J. Winn. 3D LayoutCRF for multi-view object class recognition and segmentation. In Proc. CVPR, 2007. 2\n\nLearning to efficiently detect repeatable interest points in depth data. S Holzer, J Shotton, P Kohli, Proc. ECCV. ECCVS. Holzer, J. Shotton, and P. Kohli. Learning to efficiently detect repeatable interest points in depth data. In Proc. ECCV, 2012. 2\n\nFrom SfM point clouds to fast location recognition. A Irschara, C Zach, J.-M Frahm, H Bischof, Proc. CVPR. CVPRA. Irschara, C. Zach, J.-M. Frahm, and H. Bischof. From SfM point clouds to fast location recognition. In Proc. CVPR, 2009. 2\n\nKinectFusion: real-time 3D reconstruction and interaction using a moving depth camera. S Izadi, D Kim, O Hilliges, D Molyneaux, R Newcombe, P Kohli, J Shotton, S Hodges, D Freeman, A Davison, A Fitzgibbon, Proc. UIST. UIST57S. Izadi, D. Kim, O. Hilliges, D. Molyneaux, R. Newcombe, P. Kohli, J. Shotton, S. Hodges, D. Freeman, A. Davison, and A. Fitzgibbon. KinectFusion: real-time 3D reconstruction and interaction using a moving depth camera. In Proc. UIST, 2011. 3, 5, 7\n\nA solution for the best rotation to relate two sets of vectors. W Kabsch, Acta Crystallographica. 4W. Kabsch. A solution for the best rotation to relate two sets of vectors. Acta Crystallographica, 1976. 4\n\nImproving the agility of keyframe-based slam. G Klein, D Murray, Proc. ECCV. ECCV6G. Klein and D. Murray. Improving the agility of keyframe-based slam. In Proc. ECCV, 2008. 2, 5, 6\n\nKeypoint recognition using randomized trees. V Lepetit, P Fua, IEEE Trans. PAMI. 2893V. Lepetit and P. Fua. Keypoint recognition using randomized trees. IEEE Trans. PAMI, 28(9), 2006. 2, 3\n\nLocation recognition using prioritized feature matching. Y Li, N Snavely, D Huttenlocher, Proc. ECCV. ECCVY. Li, N. Snavely, and D. Huttenlocher. Location recognition using prioritized feature matching. In Proc. ECCV, 2010. 2\n\nAge regression from faces using random forests. A Montillo, H Ling, ICIP. A. Montillo and H. Ling. Age regression from faces using random forests. In ICIP, 2009. 2\n\nKinectFusion: Real-time dense surface mapping and tracking. R Newcombe, S Izadi, O Hilliges, D Molyneaux, D Kim, A Davison, P Kohli, J Shotton, S Hodges, A Fitzgibbon, Proc. IS-MAR. IS-MAR57R. Newcombe, S. Izadi, O. Hilliges, D. Molyneaux, D. Kim, A. Davi- son, P. Kohli, J. Shotton, S. Hodges, and A. Fitzgibbon. KinectFu- sion: Real-time dense surface mapping and tracking. In Proc. IS- MAR, 2011. 3, 5, 7\n\nDTAM: Dense tracking and mapping in real-time. R A Newcombe, S J Lovegrove, A J Davison, Proc. CVPR. CVPRR. A. Newcombe, S. J. Lovegrove, and A. J. Davison. DTAM: Dense tracking and mapping in real-time. In Proc. CVPR, 2011. 3\n\nEpitomic location recognition. K Ni, A Kannan, A Criminisi, J Winn, IEEE Trans. PAMI. 2K. Ni, A. Kannan, A. Criminisi, and J. Winn. Epitomic location recognition. IEEE Trans. PAMI, 2009. 2\n\nPreemptive RANSAC for live structure and motion estimation. D Nist\u00e9r, Proc. ICCV. ICCV4D. Nist\u00e9r. Preemptive RANSAC for live structure and motion esti- mation. In Proc. ICCV, 2003. 4\n\nScalable recognition with a vocabulary tree. D Nist\u00e9r, H Stew\u00e9nius, Proc. CVPR. CVPRD. Nist\u00e9r and H. Stew\u00e9nius. Scalable recognition with a vocabulary tree. In Proc. CVPR, 2006. 2\n\nFASTER and better: A machine learning approach to corner detection. E Rosten, R Porter, T Drummond, IEEE Trans. PAMI. 322E. Rosten, R. Porter, and T. Drummond. FASTER and better: A machine learning approach to corner detection. IEEE Trans. PAMI, 32, 2010. 2\n\nORB: an efficient alternative to SIFT or SURF. E Rublee, V Rabaud, K Konolige, G Bradski, Proc. ICCV. ICCV25E. Rublee, V. Rabaud, K. Konolige, and G. Bradski. ORB: an effi- cient alternative to SIFT or SURF. In Proc. ICCV, 2011. 2, 5\n\nFast image-based localization using direct 2D-to-3D matching. T Sattler, B Leibe, L Kobbelt, Proc. ICCV. ICCVT. Sattler, B. Leibe, and L. Kobbelt. Fast image-based localization using direct 2D-to-3D matching. In Proc. ICCV, 2011. 2\n\nCity-scale location recognition. G Schindler, M Brown, R Szeliski, Proc. CVPR. CVPRG. Schindler, M. Brown, and R. Szeliski. City-scale location recog- nition. In Proc. CVPR, 2007. 2\n\nVision-based global localization and mapping for mobile robots. S Se, D Lowe, J Little, IEEE Trans. on Robotics. 213S. Se, D. Lowe, and J. Little. Vision-based global localization and mapping for mobile robots. IEEE Trans. on Robotics, 21(3), 2005. 2\n\nReal-time human pose recognition in parts from a single depth image. J Shotton, A Fitzgibbon, M Cook, T Sharp, M Finocchio, R Moore, A Kipman, A Blake, Proc. CVPR. CVPR23J. Shotton, A. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio, R. Moore, A. Kipman, and A. Blake. Real-time human pose recog- nition in parts from a single depth image. In Proc. CVPR, 2011. 2, 3\n\nThe Vitruvian Manifold: Inferring dense correspondences for one-shot human pose estimation. J Taylor, J Shotton, T Sharp, A Fitzgibbon, Proc. CVPR. CVPRJ. Taylor, J. Shotton, T. Sharp, and A. Fitzgibbon. The Vitruvian Manifold: Inferring dense correspondences for one-shot human pose estimation. In Proc. CVPR, 2012. 2\n\nAutomatic relocalization and loop closing for real-time monocular SLAM. B Williams, G Klein, I Reid, IEEE Trans. PAMI. 339B. Williams, G. Klein, and I. Reid. Automatic relocalization and loop closing for real-time monocular SLAM. IEEE Trans. PAMI, 33(9), 2011. 2\n\nLearning local image descriptors. S Winder, M Brown, Proc. CVPR. CVPRS. Winder and M. Brown. Learning local image descriptors. In Proc. CVPR, 2007. 2\n\nThe layout consistent random field for recognizing and segmenting partially occluded objects. J Winn, J Shotton, Proc. CVPR. CVPRJ. Winn and J. Shotton. The layout consistent random field for rec- ognizing and segmenting partially occluded objects. In Proc. CVPR, 2006. 2\n\n3D model matching with viewpoint-invariant patches (VIP). C Wu, B Clipp, X Li, J Frahm, M Pollefeys, Proc. CVPR. CVPRC. Wu, B. Clipp, X. Li, J. Frahm, and M. Pollefeys. 3D model match- ing with viewpoint-invariant patches (VIP). In Proc. CVPR, 2008. 3\n", "annotations": {"author": "[{\"end\":127,\"start\":81},{\"end\":172,\"start\":128},{\"end\":222,\"start\":173},{\"end\":269,\"start\":223},{\"end\":320,\"start\":270},{\"end\":371,\"start\":321}]", "publisher": null, "author_last_name": "[{\"end\":94,\"start\":87},{\"end\":139,\"start\":132},{\"end\":189,\"start\":185},{\"end\":236,\"start\":231},{\"end\":287,\"start\":278},{\"end\":338,\"start\":328}]", "author_first_name": "[{\"end\":86,\"start\":81},{\"end\":131,\"start\":128},{\"end\":184,\"start\":173},{\"end\":230,\"start\":223},{\"end\":277,\"start\":270},{\"end\":327,\"start\":321}]", "author_affiliation": "[{\"end\":126,\"start\":96},{\"end\":171,\"start\":141},{\"end\":221,\"start\":191},{\"end\":268,\"start\":238},{\"end\":319,\"start\":289},{\"end\":370,\"start\":340}]", "title": "[{\"end\":78,\"start\":1},{\"end\":449,\"start\":372}]", "venue": null, "abstract": "[{\"end\":1570,\"start\":472}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5117,\"start\":5113},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5155,\"start\":5151},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5498,\"start\":5494},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5804,\"start\":5800},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":5807,\"start\":5804},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5824,\"start\":5821},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5827,\"start\":5824},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":5830,\"start\":5827},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5849,\"start\":5845},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5852,\"start\":5849},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":5855,\"start\":5852},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5909,\"start\":5905},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6239,\"start\":6236},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6241,\"start\":6239},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6244,\"start\":6241},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6330,\"start\":6326},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6333,\"start\":6330},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6421,\"start\":6418},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6424,\"start\":6421},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6442,\"start\":6438},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6801,\"start\":6797},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":6804,\"start\":6801},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7125,\"start\":7121},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7285,\"start\":7281},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8033,\"start\":8030},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8160,\"start\":8157},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8163,\"start\":8160},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8166,\"start\":8163},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8222,\"start\":8218},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8225,\"start\":8222},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8228,\"start\":8225},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9902,\"start\":9898},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9905,\"start\":9902},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":10749,\"start\":10745},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11559,\"start\":11555},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":11562,\"start\":11559},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":11629,\"start\":11625},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":12584,\"start\":12581},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":12680,\"start\":12679},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":12879,\"start\":12876},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":13851,\"start\":13848},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":16126,\"start\":16122},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":16540,\"start\":16537},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":17395,\"start\":17391},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":17626,\"start\":17623},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":18422,\"start\":18418},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":18425,\"start\":18422},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":20054,\"start\":20050},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":20740,\"start\":20736},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":20743,\"start\":20740},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":20889,\"start\":20885},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":21096,\"start\":21092},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":21200,\"start\":21196},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":21639,\"start\":21638},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":24797,\"start\":24793},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":24800,\"start\":24797},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":27949,\"start\":27945},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":27952,\"start\":27949},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":28114,\"start\":28111}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":30447,\"start\":30391},{\"attributes\":{\"id\":\"fig_1\"},\"end\":31118,\"start\":30448},{\"attributes\":{\"id\":\"fig_2\"},\"end\":31373,\"start\":31119},{\"attributes\":{\"id\":\"fig_3\"},\"end\":31504,\"start\":31374},{\"attributes\":{\"id\":\"fig_4\"},\"end\":31700,\"start\":31505},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":32106,\"start\":31701},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":32705,\"start\":32107}]", "paragraph": "[{\"end\":2263,\"start\":1586},{\"end\":3379,\"start\":2265},{\"end\":3829,\"start\":3381},{\"end\":4616,\"start\":3831},{\"end\":4946,\"start\":4618},{\"end\":5118,\"start\":4963},{\"end\":5499,\"start\":5120},{\"end\":6135,\"start\":5501},{\"end\":6443,\"start\":6137},{\"end\":6743,\"start\":6445},{\"end\":7590,\"start\":6745},{\"end\":7926,\"start\":7630},{\"end\":8339,\"start\":7949},{\"end\":8752,\"start\":8341},{\"end\":9576,\"start\":8787},{\"end\":9845,\"start\":9595},{\"end\":10007,\"start\":9847},{\"end\":10304,\"start\":10124},{\"end\":10978,\"start\":10374},{\"end\":11288,\"start\":11006},{\"end\":11630,\"start\":11290},{\"end\":11946,\"start\":11632},{\"end\":12414,\"start\":11948},{\"end\":13125,\"start\":12434},{\"end\":13546,\"start\":13238},{\"end\":14095,\"start\":13548},{\"end\":14387,\"start\":14124},{\"end\":14487,\"start\":14411},{\"end\":14547,\"start\":14507},{\"end\":15195,\"start\":14600},{\"end\":16033,\"start\":15197},{\"end\":16709,\"start\":16050},{\"end\":17574,\"start\":16711},{\"end\":18057,\"start\":17576},{\"end\":19091,\"start\":18082},{\"end\":19638,\"start\":19103},{\"end\":21272,\"start\":19652},{\"end\":22109,\"start\":21289},{\"end\":22974,\"start\":22111},{\"end\":24801,\"start\":22976},{\"end\":25783,\"start\":24803},{\"end\":26569,\"start\":25811},{\"end\":26991,\"start\":26591},{\"end\":27364,\"start\":26993},{\"end\":27761,\"start\":27366},{\"end\":28917,\"start\":27793},{\"end\":29515,\"start\":28933},{\"end\":30390,\"start\":29517}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8786,\"start\":8753},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10123,\"start\":10008},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10373,\"start\":10305},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13237,\"start\":13126},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14410,\"start\":14388},{\"attributes\":{\"id\":\"formula_5\"},\"end\":14599,\"start\":14548}]", "table_ref": "[{\"end\":18776,\"start\":18769},{\"end\":21343,\"start\":21336},{\"end\":22299,\"start\":22292},{\"end\":26185,\"start\":26178},{\"end\":28345,\"start\":28337},{\"end\":28809,\"start\":28802}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1584,\"start\":1572},{\"attributes\":{\"n\":\"1.1.\"},\"end\":4961,\"start\":4949},{\"attributes\":{\"n\":\"2.\"},\"end\":7628,\"start\":7593},{\"attributes\":{\"n\":\"2.1.\"},\"end\":7947,\"start\":7929},{\"attributes\":{\"n\":\"2.2.\"},\"end\":9593,\"start\":9579},{\"attributes\":{\"n\":\"2.3.\"},\"end\":11004,\"start\":10981},{\"attributes\":{\"n\":\"2.4.\"},\"end\":12432,\"start\":12417},{\"attributes\":{\"n\":\"3.\"},\"end\":14122,\"start\":14098},{\"attributes\":{\"n\":\"3.1.\"},\"end\":14505,\"start\":14490},{\"attributes\":{\"n\":\"3.2.\"},\"end\":16048,\"start\":16036},{\"attributes\":{\"n\":\"4.\"},\"end\":18070,\"start\":18060},{\"attributes\":{\"n\":\"4.1.\"},\"end\":18080,\"start\":18073},{\"attributes\":{\"n\":\"4.2.\"},\"end\":19101,\"start\":19094},{\"attributes\":{\"n\":\"4.3.\"},\"end\":19650,\"start\":19641},{\"attributes\":{\"n\":\"4.4.\"},\"end\":21287,\"start\":21275},{\"attributes\":{\"n\":\"4.5.\"},\"end\":25809,\"start\":25786},{\"attributes\":{\"n\":\"4.6.\"},\"end\":26589,\"start\":26572},{\"attributes\":{\"n\":\"4.7.\"},\"end\":27791,\"start\":27764},{\"attributes\":{\"n\":\"5.\"},\"end\":28931,\"start\":28920},{\"end\":30402,\"start\":30392},{\"end\":30459,\"start\":30449},{\"end\":31130,\"start\":31120},{\"end\":31385,\"start\":31375},{\"end\":31516,\"start\":31506}]", "table": "[{\"end\":32106,\"start\":31924},{\"end\":32705,\"start\":32110}]", "figure_caption": "[{\"end\":30447,\"start\":30404},{\"end\":31118,\"start\":30461},{\"end\":31373,\"start\":31132},{\"end\":31504,\"start\":31387},{\"end\":31700,\"start\":31518},{\"end\":31924,\"start\":31703},{\"end\":32110,\"start\":32109}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":2369,\"start\":2363},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":2691,\"start\":2689},{\"end\":3342,\"start\":3336},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":3828,\"start\":3822},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14486,\"start\":14480},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18737,\"start\":18731},{\"end\":22030,\"start\":22024},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":22291,\"start\":22285},{\"end\":22891,\"start\":22885},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":23265,\"start\":23259},{\"end\":24044,\"start\":24038},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":25159,\"start\":25153},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":27116,\"start\":27107},{\"end\":28155,\"start\":28149},{\"end\":28256,\"start\":28248},{\"end\":29859,\"start\":29851}]", "bib_author_first_name": "[{\"end\":33133,\"start\":33132},{\"end\":33141,\"start\":33140},{\"end\":33367,\"start\":33366},{\"end\":33376,\"start\":33375},{\"end\":33385,\"start\":33384},{\"end\":33393,\"start\":33392},{\"end\":33407,\"start\":33406},{\"end\":33625,\"start\":33624},{\"end\":33633,\"start\":33632},{\"end\":33821,\"start\":33820},{\"end\":33833,\"start\":33832},{\"end\":33844,\"start\":33843},{\"end\":33855,\"start\":33854},{\"end\":34031,\"start\":34030},{\"end\":34039,\"start\":34038},{\"end\":34048,\"start\":34047},{\"end\":34225,\"start\":34224},{\"end\":34238,\"start\":34237},{\"end\":34453,\"start\":34452},{\"end\":34466,\"start\":34465},{\"end\":34645,\"start\":34644},{\"end\":34653,\"start\":34652},{\"end\":34662,\"start\":34661},{\"end\":34669,\"start\":34668},{\"end\":34861,\"start\":34860},{\"end\":34869,\"start\":34868},{\"end\":35080,\"start\":35079},{\"end\":35088,\"start\":35087},{\"end\":35095,\"start\":35094},{\"end\":35105,\"start\":35104},{\"end\":35117,\"start\":35116},{\"end\":35384,\"start\":35383},{\"end\":35391,\"start\":35390},{\"end\":35621,\"start\":35620},{\"end\":35630,\"start\":35629},{\"end\":35640,\"start\":35639},{\"end\":35866,\"start\":35865},{\"end\":35876,\"start\":35875},{\"end\":35887,\"start\":35886},{\"end\":36098,\"start\":36097},{\"end\":36110,\"start\":36109},{\"end\":36121,\"start\":36117},{\"end\":36130,\"start\":36129},{\"end\":36371,\"start\":36370},{\"end\":36380,\"start\":36379},{\"end\":36387,\"start\":36386},{\"end\":36399,\"start\":36398},{\"end\":36412,\"start\":36411},{\"end\":36424,\"start\":36423},{\"end\":36433,\"start\":36432},{\"end\":36444,\"start\":36443},{\"end\":36454,\"start\":36453},{\"end\":36465,\"start\":36464},{\"end\":36476,\"start\":36475},{\"end\":36823,\"start\":36822},{\"end\":37012,\"start\":37011},{\"end\":37021,\"start\":37020},{\"end\":37193,\"start\":37192},{\"end\":37204,\"start\":37203},{\"end\":37395,\"start\":37394},{\"end\":37401,\"start\":37400},{\"end\":37412,\"start\":37411},{\"end\":37613,\"start\":37612},{\"end\":37625,\"start\":37624},{\"end\":37790,\"start\":37789},{\"end\":37802,\"start\":37801},{\"end\":37811,\"start\":37810},{\"end\":37823,\"start\":37822},{\"end\":37836,\"start\":37835},{\"end\":37843,\"start\":37842},{\"end\":37854,\"start\":37853},{\"end\":37863,\"start\":37862},{\"end\":37874,\"start\":37873},{\"end\":37884,\"start\":37883},{\"end\":38186,\"start\":38185},{\"end\":38188,\"start\":38187},{\"end\":38200,\"start\":38199},{\"end\":38202,\"start\":38201},{\"end\":38215,\"start\":38214},{\"end\":38217,\"start\":38216},{\"end\":38398,\"start\":38397},{\"end\":38404,\"start\":38403},{\"end\":38414,\"start\":38413},{\"end\":38427,\"start\":38426},{\"end\":38617,\"start\":38616},{\"end\":38786,\"start\":38785},{\"end\":38796,\"start\":38795},{\"end\":38990,\"start\":38989},{\"end\":39000,\"start\":38999},{\"end\":39010,\"start\":39009},{\"end\":39228,\"start\":39227},{\"end\":39238,\"start\":39237},{\"end\":39248,\"start\":39247},{\"end\":39260,\"start\":39259},{\"end\":39478,\"start\":39477},{\"end\":39489,\"start\":39488},{\"end\":39498,\"start\":39497},{\"end\":39682,\"start\":39681},{\"end\":39695,\"start\":39694},{\"end\":39704,\"start\":39703},{\"end\":39896,\"start\":39895},{\"end\":39902,\"start\":39901},{\"end\":39910,\"start\":39909},{\"end\":40153,\"start\":40152},{\"end\":40164,\"start\":40163},{\"end\":40178,\"start\":40177},{\"end\":40186,\"start\":40185},{\"end\":40195,\"start\":40194},{\"end\":40208,\"start\":40207},{\"end\":40217,\"start\":40216},{\"end\":40227,\"start\":40226},{\"end\":40539,\"start\":40538},{\"end\":40549,\"start\":40548},{\"end\":40560,\"start\":40559},{\"end\":40569,\"start\":40568},{\"end\":40839,\"start\":40838},{\"end\":40851,\"start\":40850},{\"end\":40860,\"start\":40859},{\"end\":41065,\"start\":41064},{\"end\":41075,\"start\":41074},{\"end\":41276,\"start\":41275},{\"end\":41284,\"start\":41283},{\"end\":41513,\"start\":41512},{\"end\":41519,\"start\":41518},{\"end\":41528,\"start\":41527},{\"end\":41534,\"start\":41533},{\"end\":41543,\"start\":41542}]", "bib_author_last_name": "[{\"end\":33138,\"start\":33134},{\"end\":33147,\"start\":33142},{\"end\":33373,\"start\":33368},{\"end\":33382,\"start\":33377},{\"end\":33390,\"start\":33386},{\"end\":33404,\"start\":33394},{\"end\":33417,\"start\":33408},{\"end\":33630,\"start\":33626},{\"end\":33639,\"start\":33634},{\"end\":33830,\"start\":33822},{\"end\":33841,\"start\":33834},{\"end\":33852,\"start\":33845},{\"end\":33859,\"start\":33856},{\"end\":34036,\"start\":34032},{\"end\":34045,\"start\":34040},{\"end\":34056,\"start\":34049},{\"end\":34235,\"start\":34226},{\"end\":34243,\"start\":34239},{\"end\":34463,\"start\":34454},{\"end\":34474,\"start\":34467},{\"end\":34650,\"start\":34646},{\"end\":34659,\"start\":34654},{\"end\":34666,\"start\":34663},{\"end\":34673,\"start\":34670},{\"end\":34866,\"start\":34862},{\"end\":34878,\"start\":34870},{\"end\":35085,\"start\":35081},{\"end\":35092,\"start\":35089},{\"end\":35102,\"start\":35096},{\"end\":35114,\"start\":35106},{\"end\":35127,\"start\":35118},{\"end\":35388,\"start\":35385},{\"end\":35404,\"start\":35392},{\"end\":35627,\"start\":35622},{\"end\":35637,\"start\":35631},{\"end\":35645,\"start\":35641},{\"end\":35873,\"start\":35867},{\"end\":35884,\"start\":35877},{\"end\":35893,\"start\":35888},{\"end\":36107,\"start\":36099},{\"end\":36115,\"start\":36111},{\"end\":36127,\"start\":36122},{\"end\":36138,\"start\":36131},{\"end\":36377,\"start\":36372},{\"end\":36384,\"start\":36381},{\"end\":36396,\"start\":36388},{\"end\":36409,\"start\":36400},{\"end\":36421,\"start\":36413},{\"end\":36430,\"start\":36425},{\"end\":36441,\"start\":36434},{\"end\":36451,\"start\":36445},{\"end\":36462,\"start\":36455},{\"end\":36473,\"start\":36466},{\"end\":36487,\"start\":36477},{\"end\":36830,\"start\":36824},{\"end\":37018,\"start\":37013},{\"end\":37028,\"start\":37022},{\"end\":37201,\"start\":37194},{\"end\":37208,\"start\":37205},{\"end\":37398,\"start\":37396},{\"end\":37409,\"start\":37402},{\"end\":37425,\"start\":37413},{\"end\":37622,\"start\":37614},{\"end\":37630,\"start\":37626},{\"end\":37799,\"start\":37791},{\"end\":37808,\"start\":37803},{\"end\":37820,\"start\":37812},{\"end\":37833,\"start\":37824},{\"end\":37840,\"start\":37837},{\"end\":37851,\"start\":37844},{\"end\":37860,\"start\":37855},{\"end\":37871,\"start\":37864},{\"end\":37881,\"start\":37875},{\"end\":37895,\"start\":37885},{\"end\":38197,\"start\":38189},{\"end\":38212,\"start\":38203},{\"end\":38225,\"start\":38218},{\"end\":38401,\"start\":38399},{\"end\":38411,\"start\":38405},{\"end\":38424,\"start\":38415},{\"end\":38432,\"start\":38428},{\"end\":38624,\"start\":38618},{\"end\":38793,\"start\":38787},{\"end\":38806,\"start\":38797},{\"end\":38997,\"start\":38991},{\"end\":39007,\"start\":39001},{\"end\":39019,\"start\":39011},{\"end\":39235,\"start\":39229},{\"end\":39245,\"start\":39239},{\"end\":39257,\"start\":39249},{\"end\":39268,\"start\":39261},{\"end\":39486,\"start\":39479},{\"end\":39495,\"start\":39490},{\"end\":39506,\"start\":39499},{\"end\":39692,\"start\":39683},{\"end\":39701,\"start\":39696},{\"end\":39713,\"start\":39705},{\"end\":39899,\"start\":39897},{\"end\":39907,\"start\":39903},{\"end\":39917,\"start\":39911},{\"end\":40161,\"start\":40154},{\"end\":40175,\"start\":40165},{\"end\":40183,\"start\":40179},{\"end\":40192,\"start\":40187},{\"end\":40205,\"start\":40196},{\"end\":40214,\"start\":40209},{\"end\":40224,\"start\":40218},{\"end\":40233,\"start\":40228},{\"end\":40546,\"start\":40540},{\"end\":40557,\"start\":40550},{\"end\":40566,\"start\":40561},{\"end\":40580,\"start\":40570},{\"end\":40848,\"start\":40840},{\"end\":40857,\"start\":40852},{\"end\":40865,\"start\":40861},{\"end\":41072,\"start\":41066},{\"end\":41081,\"start\":41076},{\"end\":41281,\"start\":41277},{\"end\":41292,\"start\":41285},{\"end\":41516,\"start\":41514},{\"end\":41525,\"start\":41520},{\"end\":41531,\"start\":41529},{\"end\":41540,\"start\":41535},{\"end\":41553,\"start\":41544}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":12470146},\"end\":33284,\"start\":33074},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":7882599},\"end\":33581,\"start\":33286},{\"attributes\":{\"doi\":\"1992. 7\",\"id\":\"b2\",\"matched_paper_id\":21874346},\"end\":33764,\"start\":33583},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":1815530},\"end\":34002,\"start\":33766},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":15181392},\"end\":34161,\"start\":34004},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":691081},\"end\":34385,\"start\":34163},{\"attributes\":{\"id\":\"b6\"},\"end\":34600,\"start\":34387},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":3196427},\"end\":34794,\"start\":34602},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":15277074},\"end\":35007,\"start\":34796},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":7286062},\"end\":35313,\"start\":35009},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":2366967},\"end\":35547,\"start\":35315},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":9759858},\"end\":35790,\"start\":35549},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":17109572},\"end\":36043,\"start\":35792},{\"attributes\":{\"id\":\"b13\"},\"end\":36281,\"start\":36045},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":3345516},\"end\":36756,\"start\":36283},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":97383637},\"end\":36963,\"start\":36758},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":9158502},\"end\":37145,\"start\":36965},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":12980089},\"end\":37335,\"start\":37147},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":6853153},\"end\":37562,\"start\":37337},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":10431868},\"end\":37727,\"start\":37564},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":11830123},\"end\":38136,\"start\":37729},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":1336659},\"end\":38364,\"start\":38138},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":5934788},\"end\":38554,\"start\":38366},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":2379541},\"end\":38738,\"start\":38556},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":1654266},\"end\":38919,\"start\":38740},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":206764370},\"end\":39178,\"start\":38921},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":206769866},\"end\":39413,\"start\":39180},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":11836057},\"end\":39646,\"start\":39415},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":14222017},\"end\":39829,\"start\":39648},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":15214394},\"end\":40081,\"start\":39831},{\"attributes\":{\"id\":\"b30\"},\"end\":40444,\"start\":40083},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":12975900},\"end\":40764,\"start\":40446},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":12462257},\"end\":41028,\"start\":40766},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":12439604},\"end\":41179,\"start\":41030},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":11200969},\"end\":41452,\"start\":41181},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":7069706},\"end\":41705,\"start\":41454}]", "bib_title": "[{\"end\":33130,\"start\":33074},{\"end\":33364,\"start\":33286},{\"end\":33622,\"start\":33583},{\"end\":33818,\"start\":33766},{\"end\":34028,\"start\":34004},{\"end\":34222,\"start\":34163},{\"end\":34642,\"start\":34602},{\"end\":34858,\"start\":34796},{\"end\":35077,\"start\":35009},{\"end\":35381,\"start\":35315},{\"end\":35618,\"start\":35549},{\"end\":35863,\"start\":35792},{\"end\":36095,\"start\":36045},{\"end\":36368,\"start\":36283},{\"end\":36820,\"start\":36758},{\"end\":37009,\"start\":36965},{\"end\":37190,\"start\":37147},{\"end\":37392,\"start\":37337},{\"end\":37610,\"start\":37564},{\"end\":37787,\"start\":37729},{\"end\":38183,\"start\":38138},{\"end\":38395,\"start\":38366},{\"end\":38614,\"start\":38556},{\"end\":38783,\"start\":38740},{\"end\":38987,\"start\":38921},{\"end\":39225,\"start\":39180},{\"end\":39475,\"start\":39415},{\"end\":39679,\"start\":39648},{\"end\":39893,\"start\":39831},{\"end\":40150,\"start\":40083},{\"end\":40536,\"start\":40446},{\"end\":40836,\"start\":40766},{\"end\":41062,\"start\":41030},{\"end\":41273,\"start\":41181},{\"end\":41510,\"start\":41454}]", "bib_author": "[{\"end\":33140,\"start\":33132},{\"end\":33149,\"start\":33140},{\"end\":33375,\"start\":33366},{\"end\":33384,\"start\":33375},{\"end\":33392,\"start\":33384},{\"end\":33406,\"start\":33392},{\"end\":33419,\"start\":33406},{\"end\":33632,\"start\":33624},{\"end\":33641,\"start\":33632},{\"end\":33832,\"start\":33820},{\"end\":33843,\"start\":33832},{\"end\":33854,\"start\":33843},{\"end\":33861,\"start\":33854},{\"end\":34038,\"start\":34030},{\"end\":34047,\"start\":34038},{\"end\":34058,\"start\":34047},{\"end\":34237,\"start\":34224},{\"end\":34245,\"start\":34237},{\"end\":34465,\"start\":34452},{\"end\":34476,\"start\":34465},{\"end\":34652,\"start\":34644},{\"end\":34661,\"start\":34652},{\"end\":34668,\"start\":34661},{\"end\":34675,\"start\":34668},{\"end\":34868,\"start\":34860},{\"end\":34880,\"start\":34868},{\"end\":35087,\"start\":35079},{\"end\":35094,\"start\":35087},{\"end\":35104,\"start\":35094},{\"end\":35116,\"start\":35104},{\"end\":35129,\"start\":35116},{\"end\":35390,\"start\":35383},{\"end\":35406,\"start\":35390},{\"end\":35629,\"start\":35620},{\"end\":35639,\"start\":35629},{\"end\":35647,\"start\":35639},{\"end\":35875,\"start\":35865},{\"end\":35886,\"start\":35875},{\"end\":35895,\"start\":35886},{\"end\":36109,\"start\":36097},{\"end\":36117,\"start\":36109},{\"end\":36129,\"start\":36117},{\"end\":36140,\"start\":36129},{\"end\":36379,\"start\":36370},{\"end\":36386,\"start\":36379},{\"end\":36398,\"start\":36386},{\"end\":36411,\"start\":36398},{\"end\":36423,\"start\":36411},{\"end\":36432,\"start\":36423},{\"end\":36443,\"start\":36432},{\"end\":36453,\"start\":36443},{\"end\":36464,\"start\":36453},{\"end\":36475,\"start\":36464},{\"end\":36489,\"start\":36475},{\"end\":36832,\"start\":36822},{\"end\":37020,\"start\":37011},{\"end\":37030,\"start\":37020},{\"end\":37203,\"start\":37192},{\"end\":37210,\"start\":37203},{\"end\":37400,\"start\":37394},{\"end\":37411,\"start\":37400},{\"end\":37427,\"start\":37411},{\"end\":37624,\"start\":37612},{\"end\":37632,\"start\":37624},{\"end\":37801,\"start\":37789},{\"end\":37810,\"start\":37801},{\"end\":37822,\"start\":37810},{\"end\":37835,\"start\":37822},{\"end\":37842,\"start\":37835},{\"end\":37853,\"start\":37842},{\"end\":37862,\"start\":37853},{\"end\":37873,\"start\":37862},{\"end\":37883,\"start\":37873},{\"end\":37897,\"start\":37883},{\"end\":38199,\"start\":38185},{\"end\":38214,\"start\":38199},{\"end\":38227,\"start\":38214},{\"end\":38403,\"start\":38397},{\"end\":38413,\"start\":38403},{\"end\":38426,\"start\":38413},{\"end\":38434,\"start\":38426},{\"end\":38626,\"start\":38616},{\"end\":38795,\"start\":38785},{\"end\":38808,\"start\":38795},{\"end\":38999,\"start\":38989},{\"end\":39009,\"start\":38999},{\"end\":39021,\"start\":39009},{\"end\":39237,\"start\":39227},{\"end\":39247,\"start\":39237},{\"end\":39259,\"start\":39247},{\"end\":39270,\"start\":39259},{\"end\":39488,\"start\":39477},{\"end\":39497,\"start\":39488},{\"end\":39508,\"start\":39497},{\"end\":39694,\"start\":39681},{\"end\":39703,\"start\":39694},{\"end\":39715,\"start\":39703},{\"end\":39901,\"start\":39895},{\"end\":39909,\"start\":39901},{\"end\":39919,\"start\":39909},{\"end\":40163,\"start\":40152},{\"end\":40177,\"start\":40163},{\"end\":40185,\"start\":40177},{\"end\":40194,\"start\":40185},{\"end\":40207,\"start\":40194},{\"end\":40216,\"start\":40207},{\"end\":40226,\"start\":40216},{\"end\":40235,\"start\":40226},{\"end\":40548,\"start\":40538},{\"end\":40559,\"start\":40548},{\"end\":40568,\"start\":40559},{\"end\":40582,\"start\":40568},{\"end\":40850,\"start\":40838},{\"end\":40859,\"start\":40850},{\"end\":40867,\"start\":40859},{\"end\":41074,\"start\":41064},{\"end\":41083,\"start\":41074},{\"end\":41283,\"start\":41275},{\"end\":41294,\"start\":41283},{\"end\":41518,\"start\":41512},{\"end\":41527,\"start\":41518},{\"end\":41533,\"start\":41527},{\"end\":41542,\"start\":41533},{\"end\":41555,\"start\":41542}]", "bib_venue": "[{\"end\":33877,\"start\":33873},{\"end\":34074,\"start\":34070},{\"end\":34691,\"start\":34687},{\"end\":34896,\"start\":34892},{\"end\":35422,\"start\":35418},{\"end\":35663,\"start\":35659},{\"end\":35911,\"start\":35907},{\"end\":36156,\"start\":36152},{\"end\":36505,\"start\":36501},{\"end\":37046,\"start\":37042},{\"end\":37443,\"start\":37439},{\"end\":37917,\"start\":37911},{\"end\":38243,\"start\":38239},{\"end\":38642,\"start\":38638},{\"end\":38824,\"start\":38820},{\"end\":39286,\"start\":39282},{\"end\":39524,\"start\":39520},{\"end\":39731,\"start\":39727},{\"end\":40251,\"start\":40247},{\"end\":40598,\"start\":40594},{\"end\":41099,\"start\":41095},{\"end\":41310,\"start\":41306},{\"end\":41571,\"start\":41567},{\"end\":33167,\"start\":33149},{\"end\":33423,\"start\":33419},{\"end\":33664,\"start\":33648},{\"end\":33871,\"start\":33861},{\"end\":34068,\"start\":34058},{\"end\":34261,\"start\":34245},{\"end\":34450,\"start\":34387},{\"end\":34685,\"start\":34675},{\"end\":34890,\"start\":34880},{\"end\":35145,\"start\":35129},{\"end\":35416,\"start\":35406},{\"end\":35657,\"start\":35647},{\"end\":35905,\"start\":35895},{\"end\":36150,\"start\":36140},{\"end\":36499,\"start\":36489},{\"end\":36854,\"start\":36832},{\"end\":37040,\"start\":37030},{\"end\":37226,\"start\":37210},{\"end\":37437,\"start\":37427},{\"end\":37636,\"start\":37632},{\"end\":37909,\"start\":37897},{\"end\":38237,\"start\":38227},{\"end\":38450,\"start\":38434},{\"end\":38636,\"start\":38626},{\"end\":38818,\"start\":38808},{\"end\":39037,\"start\":39021},{\"end\":39280,\"start\":39270},{\"end\":39518,\"start\":39508},{\"end\":39725,\"start\":39715},{\"end\":39942,\"start\":39919},{\"end\":40245,\"start\":40235},{\"end\":40592,\"start\":40582},{\"end\":40883,\"start\":40867},{\"end\":41093,\"start\":41083},{\"end\":41304,\"start\":41294},{\"end\":41565,\"start\":41555}]"}}}, "year": 2023, "month": 12, "day": 17}