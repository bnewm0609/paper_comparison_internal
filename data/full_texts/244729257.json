{"id": 244729257, "updated": "2023-10-05 19:05:38.105", "metadata": {"title": "NeuSample: Neural Sample Field for Efficient View Synthesis", "authors": "[{\"first\":\"Jiemin\",\"last\":\"Fang\",\"middle\":[]},{\"first\":\"Lingxi\",\"last\":\"Xie\",\"middle\":[]},{\"first\":\"Xinggang\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Xiaopeng\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Wenyu\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Qi\",\"last\":\"Tian\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Neural radiance fields (NeRF) have shown great potentials in representing 3D scenes and synthesizing novel views, but the computational overhead of NeRF at the inference stage is still heavy. To alleviate the burden, we delve into the coarse-to-fine, hierarchical sampling procedure of NeRF and point out that the coarse stage can be replaced by a lightweight module which we name a neural sample field. The proposed sample field maps rays into sample distributions, which can be transformed into point coordinates and fed into radiance fields for volume rendering. The overall framework is named as NeuSample. We perform experiments on Realistic Synthetic 360$^{\\circ}$ and Real Forward-Facing, two popular 3D scene sets, and show that NeuSample achieves better rendering quality than NeRF while enjoying a faster inference speed. NeuSample is further compressed with a proposed sample field extraction method towards a better trade-off between quality and speed.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2111.15552", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2111-15552", "doi": null}}, "content": {"source": {"pdf_hash": "18110437652532366b76ed9d2a42f1ee10f17836", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2111.15552v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "3f1bf5b2b6e824dde58673688cd4d3570154470f", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/18110437652532366b76ed9d2a42f1ee10f17836.txt", "contents": "\nNeuSample: Neural Sample Field for Efficient View Synthesis\n\n\nJiemin Fang \nInstitute of Artificial Intelligence\nHuazhong University of Science & Technology\n\n\nSchool of EIC\nHuazhong University of Science & Technology\n\n\nLingxi Xie \nHuawei Inc\n\n\nXinggang Wang xgwang@hust.edu.cn \nSchool of EIC\nHuazhong University of Science & Technology\n\n\n\u2020 \nXiaopeng Zhang \nHuawei Inc\n\n\nWenyu Liu \nSchool of EIC\nHuazhong University of Science & Technology\n\n\nQi Tian \nHuawei Inc\n\n\nNeuSample: Neural Sample Field for Efficient View Synthesis\n\nNeural radiance fields (NeRF) have shown great potentials in representing 3D scenes and synthesizing novel views, but the computational overhead of NeRF at the inference stage is still heavy. To alleviate the burden, we delve into the coarse-to-fine, hierarchical sampling procedure of NeRF and point out that the coarse stage can be replaced by a lightweight module which we name a neural sample field. The proposed sample field maps rays into sample distributions, which can be transformed into point coordinates and fed into radiance fields for volume rendering. The overall framework is named as NeuSample. We perform experiments on Realistic Synthetic 360 \u2022 and Real Forward-Facing, two popular 3D scene sets, and show that NeuSample achieves better rendering quality than NeRF while enjoying a faster inference speed. NeuSample is further compressed with a proposed sample field extraction method towards a better trade-off between quality and speed.\n\nIntroduction\n\nNovel view synthesis is a long-standing and important problem in computer vision [4,36], aiming at reconstructing a 3D object/scene with sparsely sampled views and generating images from unseen views. It has a wide range of applications, such as rendering interactive objects in virtual reality and offering 3D preview of objects/scenes.\n\nRecently, researchers propose the concept of neural radiance fields (NeRF) [17] that represent a scene with a continuous 5D function, which is often formulated using a deep neural network and hence can be optimized via gradient descent. The function takes point coordinates (3D) and the observer's view direction (2D) as input and outputs the pixel radiance and opacity. To connect real 3D points with image pixels, a classical technique volume rendering [10] is used. \u2020 Corresponding author.  It samples a number of points along the ray (starting from the observer, extending along the view direction), computes their density (a.k.a. opacity) values and color properties, and eventually accumulate them into the final output. Compared to conventional methods (i.e., voxel-based or meshbased representations), NeRF saves considerable storage by compressing each scene/object into a neural network.\n\nTo guarantee high rendering quality, NeRF and most subsequent methods [3,7,17,29] adopted a hierarchical sampling method. As shown in Fig. 1, the algorithm first uniformly samples a set of N c (usually taken as 64) coarselevel points along the ray, and feeds them into the radiance field to obtain the densities and color properties of these points, as well as an updated distribution to sample more fine-level points (for another round of inference). Finally, the outputs of coarse-level and fine-level points are accumulated as the final output. Although the strategy leads to improved rendering performance, the two-stage inference incurs heavy computational overheads, making it difficult to apply NeRF to real-life or interactive scenarios.\n\nTo alleviate the burden, we propose a novel framework named NeuSample that only requires single-shot inference for sampling -in other words, we discard the costly coarse stage and instead use a neural sample field, which takes a ray representation as input and outputs a sampling distribution along the ray. As shown in Fig. 1, the sample field is also a 5D function and formulated using a neural network, which takes the observer's coordinate (x o , y o , z o ) and view direction (\u03b8, \u03c6) as input and outputs N numbers, corresponding to the distances between the points-to-sample and the observer. These points are then fed into the neural radiance field to accomplish the volume rendering procedure.\n\nWe perform experiments on two commonly used benchmarks, namely, Realistic Synthetic 360 \u2022 and Real Forward-Facing. Compared to NeRF, NeuSample demonstrates superior rendering quality and saves around a quarter of inference time. In addition, a sample field extraction method assists NeuSample to achieve competitive rendering quality with 25% of inference time. Diagnostic studies show that the improved quality-complexity trade-off owes to the neural sample field that learns an efficient way of sampling points. The contribution of this work is summarized as follows.\n\n\u2022 We propose a sample field to map one ray into a sample distribution. This field is parameterized as a neural network with fully connected layers. \u2022 The proposed sample field can be integrated with radiance fields to perform volume rendering, which not only saves computation cost from coarse networks used in the conventional hierarchical sampling strategy but also shows stronger rendering quality than NeRF. \u2022 A sample field extraction method is proposed to reduce the number of sampled points. This method further accelerates rendering while maintaining competitive rendering quality.\n\n\nRelated Work\n\nNeural Implicit Representations Using neural representations to modelling 3D structures or geometry [2, 6, 13-15, 19, 23, 34, 35] has shown great success. Neural representations model 3D scenes in a continuous space and can be optimized with a differentiable manner. The storage can be saved with network inference. Most of above methods require explicit supervision. NeRF [17] and subsequent works [3,9,22,25,31,44,45] use neural radiance fields to map 3D coordinates into color and opacity values, which achieve strong performance on synthesizing photo-realistic images from novel views. Besides neural radiance fields, some works adopt other types of neural fields to represent or model 3D scenes, e.g. textured material [8,20,26,27], indirect illumination values [30], surfacing reconstruction [21,40] and light fields [33]. Our work is inspired by the neural field concept and proposes a sample field, which maps rays into sample distributions. This field is efficient and the predicted samples help radiance fields render high-quality images.\n\nAccelerating Rendering with Caching One main stream of methods accelerate rendering by pre-computing and storing explicit data structures. For inference, properties of points can be looked up and latency of neural network inference can be drastically reduced. Neural Sparse Voxel Fields (NSVF) [12] allows for empty space skipping and early ray termination by constructing an octree structure. KiloNeRF [29] represents a scene with thousands of tiny MLPs, each of which is only responsible for a cell in the space grid. SNeRG [7], FastNeRF [5] and PlenOctrees [43] et al. adopt a similar methodology by pre-computing properties/features with sparse voxel grid-like structures using a learned NeRF. Directly querying these properties during inference can effectively save cost. NeX [42] represents scenes based on the multiplane image (MPI) with MPLs. The MPI grid can be cached for fast rendering. Most of the above methods achieve real-time rendering speed. This methodology can be taken as using storage to complement inference. Though storage may be compressed via techniques like quantization, for high resolution, storage is still a non-negligible element. From a different perspective, our method aims at improving rendering speed without caching and additional storage. NeuSample is a parallel work to the above ones and can be integrated with caching-aided frameworks.\n\nAccelerating Rendering from Inference Times A series of methods promote the rendering efficiency by focusing on the neural representation itself and reducing the inference times. AutoInt [11] proposes to automatically integrate output colors for rendering with neural networks, which approximates the integral along rays in piecewise sections. DONeRF [18] uses the depth information to guide sampling by training a \"depth oracle\" network, which greatly reduces the sample number for rendering. However, depth information is usually hard to obtain in real scenarios. Light Filed Networks [33] directly map a ray into the color, only requiring one single inference for rendering rather than mapping hundreds of points and significantly promoting efficiency. This method discards explicit 3D point modelling so additional supervision may be needed to construct multi-view consistency towards complicated geometry. NeRF-ID [1] reduces the sample number by training an importance predictor, where a proposal network with Transformer/MLP-Mixer architectures is used for sampling based on coarse network features. TermiNeRF [24] predicts weights of ray segments and further sample points based on the estimated weights. Our work maintains advantage of volume rendering for modelling multi-view consistency with explicit 3D point coordinates, but saves computation cost from cumbersome coarse fields. Samples are directly obtained by the proposed sample field.\n\n\nMethod\n\nIn this section, we first review formulations in NeRF [17]. Second, we introduce the proposed neural sample field and the overall framework for rendering. We finally introduce a sample field extraction method to produce fewer samples for further acceleration.\n\n\nReview of NeRF\n\nField Construction and Optimization The neural radiance field is first proposed in [17], which is designed to represent a scene with emitted radiance for each position in the space. The field is constructed by mapping the coordinate of a point into its volume density and color with neural networks. Denoting a point x with a 5D-coordinate (x, y, z, \u03b8, \u03c6), we predict its volume density and viewing color via neural networks as\n\u03c3, c = R \u0398 (x, y, z, \u03b8, \u03c6),(1)\nwhere x, y, z indicate the point location, \u03b8, \u03c6 are the view direction, and R \u0398 is the radiance field instantiated as a neural network. To connect these points in the real space and images taken from the camera, one pixel in an image can be rendered by computing densities and colors of points along the according ray and performing the classic volume rendering [10]. Specifically, the expected color\u0108(r) of the pixel along camera ray r(t) = o + td is computed with the quadrature rule as:\nC(r) = N i=1 T i (1 \u2212 exp(\u2212\u03c3 i \u03b4 i ))c(x i , d), T i = exp(\u2212 i\u22121 j=1 \u03c3(x j )\u03b4 j ),(2)\nwhere o, d denote the origin and direction of the ray respectively, t denotes the distance of the sample from the origin, and \u03b4 i is defined as the distance between two adjacent samples, i.e., \u03b4 i = t i+1 \u2212 t i . The field defined in Eq. 1 can be optimized with a differentiable manner by minimizing the loss between the rendered and ground truth color in the image:\nL = \u0108 (r) \u2212 C(r) 2 2 .(3)\nVolume Samling in Radiance Fields In real implementation, it is impossible to achieve the rendering procedure defined in Eq. 2 by traversing all the points along the ray. Therefore, samples carrying useful information need to be obtained for rendering. As shown in Fig. 1, NeRF [17] and most of its variants [3,7,17,29] adopt a hierarchical sampling strategy, which requires two field networks. In the first stage, N c coarse points are sampled by randomly drawing one from each of N c evenly-partitioned bins:\nt i \u223c U[t n + i \u2212 1 N c (t f \u2212 t n ), t n + i N c (t f \u2212 t n )],(4)\nwhere t n and t f denote the near and far bound respectively. The N c coarse samples are mapped by the first field network into colors and densities. In the second stage, N f fine samples are generated based on properties from N c coarse samples in the first stage. In [17],\nweight w i = T i (1 \u2212 exp(\u2212\u03c3 i \u03b4 i )\n) of each coarse sample computed in Eq. 2 serves as the probability for sampling fine points. Then both coarse and fine samples are fed into the second field network for final rendering.\n\n\nNeural Sample Field\n\nAs aforementioned, color and density properties of N c coarse samples need to be first inferred by a field network to generate fine samples for final rendering. This procedure takes a large amount of computation cost, i.e. 25% of the total cost for 64 coarse samples and 128 fine samples. Inspired by the neural field concept as Eq. 1, we propose a neural sample field S \u0398 which maps a ray r(t) = o + td directly into a series of samples for volume rendering:\nx 1 , x 2 , ..., x N \u2190 S \u0398 (o, d),(5)\nwhere x i denotes the coordinates of the ith sample, N denotes the number of desired samples. Specifically, we first obtain N scalars by feeding the ray origin coordinates and direction into S \u0398 :\nt 1 ,t 2 , ...,t N = S \u0398 (x o , y o , z o , \u03b8, \u03c6),(6)\nwheret i \u223c [0, 1] represents the relative sample position between the near and far bound along the ray.t i is mapped to a absolute position by computing\nt i = (1 \u2212t i )t n +t i t f .\nThen we compute the coordinates of the ith sample with\nx i = o + t i d.(7)\nArchitecture We show the architecture of the sample field network S \u0398 in Fig. 3. The ray vector with origin coordinates and direction are first mapped into a higher dimension with position encoding, which is widely used in previous works [3,17,32,38]. Then the mapped input is passed through 8 fully connected (FC) layers with ReLU activations. A skip connection is included by concatenating input with the 4-th layer's output. The hidden dimension ...\n\n\nRGB, \u03c3\n\nNeural Sample Field Neural Radiance Field i Figure 2. Overall framework of NeuSample. To render a pixel in the image, the ray passing through the pixel is first fed into a sample field network S\u0398, which is mapped to a distribution along the ray. The distribution is then transformed to 3D-point coordinates, which are fed into the radiance field R\u0398 to obtain colors and densities. Finally, volume rendering is performed on these points.  of features between FC layers is set as 256. At the end of the network, an additional FC layer maps 256-dimension features into an N -dimension vector. The vector is finally fed into a sigmoid activation layer and becomes the relative sample positions (t 1 ,t 2 , ...,t N ) defined in Eq. 6.\n\nOverall Framework As shown in Fig. 2, we integrate the proposed neural sample fields with radiance fields as the overall framework. To render a pixel in the image, we first compute the camera pose and transform the pose to the ray origin and direction according to the pixel position. Then we feed the ray origin coordinates and normalized direction vector into the neural sample field network S \u0398 . The sample field network outputs distributions within [0, 1] which are transformed into 3D coordinates. Samples are then fed into the second neural radiance field to obtain corresponding colors and densities. Finally, the pixel color is generated using volume rendering as Eq. 2. Noting that both the sample field and radiance field network are optimized by minimizing the final rendered color loss as Eq. 3. The whole framework can be trained end to end via gradient descent. The sample field is intuitively learning how to sample points along rays, which is intrinsically modelling geometry structures of the scene.\n\n\nSample Field Extraction\n\nBesides saving computation cost from coarse fields, we propose to extract the learned sample field for further acceleration. As shown in Fig. 4, we first train a regular sample field network S \u0398 which predicts N substantial samples (e.g. N = 192 as in the fine network of NeRF [17]) for the radiance field R \u0398 learning. Then we reduce the output number of S \u0398 to N e < N and obtain an extracted sample field S e \u0398 followed by radiance field R e \u0398 . Parameters of the regular sample field S \u0398 and radiance field R \u0398 are mapped to the extracted ones S e \u0398 and R e \u0398 . The two radiance field networks share the same architecture, so parameters from R \u0398 can be directly copied to R e \u0398 . For the sample fields, only the final FC layers for sample prediction differ where parameters are evenly mapped from S \u0398 to S e \u0398 on the output channel dimension. All the other parameters are directly copied. With parameters mapped, we fine-tune the extracted fields S e \u0398 and R e \u0398 only for a few iterations to fit the new distribution. For real-world scenes which usually have complicated geometry structures or depth distribution, we use the depth information predict by the regular fields to help initialize the extracted sampling filed network. We name this procedure as depth boost. Specifically, we sample some camera poses of the scene and feed the corresponding rays to the regular fields. For a ray r(t) = o + td, the depth d r is predicted as:\nt 1 , t 2 , ..., t N = S \u0398 (o, d), \u03c3 1 , \u03c3 2 , ..., \u03c3 N = R \u0398 (t 1 , t 2 , ..., t N ), d r = N i=1 T i (1 \u2212 exp(\u2212\u03c3 i \u03b4 i ))t i .(8)\nWe make the mean value of the extracted sample field output fit the predicted depth value d r by minimizing the loss:   [17], i.e. time for rendering one image which is measured on one V100 GPU. * N e of NeuSample denotes the sample number of the extracted sample field. * \"N = 32\" for AutoInt [11] denotes the number of piecewise sections.\n\nDepth boost helps sample fields with fewer points quickly converge to positions with useful information along the ray. Noting that this procedure only requires gradients for the extracted sample field and is not used in subsequent finetuning, which is efficient and can be finished with negligible cost.\n\n\nExperiments\n\nIn this section, we first describe the implementation and experimental details (Sec \n\n\nImplementation Details\n\nArchitecture Settings Our framework is implemented based on PyTorch. We use the same network architecture for all the evaluated scenes. The sample field network is set as Fig. 3 depicts and the radiance field network is used as the same one in NeRF [17]. For the sample field input, we apply 10-frequency position encoding to both ray origin o and direction d, and the same position encoding as NeRF for radiance field input. The output number of the sample field is set as 192 without specified in the following part.\n\nTraining Hyperparameters We use the Adam optimizer with a batch size of 4, 096 rays. The learning rate decays from 5 \u00d7 10 \u22124 to 5 \u00d7 10 \u22126 following a polynomial strategy with power 1. For the Realistic Synthetic 360 \u2022 dataset, we train each scene for 400k iterations in total. For the Real Forward-Facing scenes, we train each one for 100 epochs, where each epoch is completed by randomly sampling 4, 096 rays from the whole training set for each iteration. Random noise with 0 mean and unit variance is added to the radiance field's output densities as NeRF.\n\nExtraction Hyperparameters For Realistic Synthetic 360 \u2022 scenes, the geometry structure is not complicated so we directly map parameters of regular fields to the extracted ones without depth boost (which we find no additional gain in experiments). For Real Forward-Facing scenes, we sample 120 camera poses with a spiral path to perform depth boost. 8, 192 rays are randomly sampled in each iteration. This procedure takes only one epoch with a 5 \u00d7 10 \u22125 learning rate. The subsequent fine-tuning takes 40k iterations for synthetic scenes and 20 epochs for real scenes.\n\nDatasets We use two datasets, Realistic Synthetic 360 \u2022 and Real Forward-Facing, to evaluate our method, which are also used in NeRF [17] and most subsequent related methods [1,3,7,11,29,42]. The Realistic Synthetic 360 \u2022 dataset consists of 8 scenes and each one includes 100 views for training and 200 views for testing. We take all views with an 800 \u00d7 800 resolution. The Real Forward-Facing dataset contains 8 complex real-world scenes from NeRF [17] and LLFF [16]. Each scene includes 20 -62 images. Following [17], we hold out 1 8 images for testing and the rest are for training. All images are at 1008 \u00d7 756 pixels for experiments if unspecified.\n\n\nResults and Comparisons\n\nRealistic Synthetic 360 \u2022 We show main PSNR results in Tab. 1 and compare with NeRF [17] and AutoInt [11], which is a very related work focusing on reducing inference cost. Though \u223c 25% computation cost of course network inference is saved, our NeuSample still achieves similar or better PSNR performance compared with NeRF [17], e.g. +0.59 PSNR for the Ficus scene and +0.63 for Lego. When the sample field is extracted to output 64 points, NeuSample achieves significantly better performance than AutoInt, i.e. 1.56 better average PSNR. The qualitative visualization results are shown in Fig. 7. Noting that though NeRF has produced high-quality synthesis results, NeuSample shows advantages in modelling some details.\n\nReal Forward-Facing The Real Forward-Facing dataset is more challenging as it contains complex scenes in realworld scenarios, which requires more elaborate geometry structure modelling than synthetic ones. As shown in Tab. 2,   Fig. 7 and NeuSample performs better in detail modelling as well.\n\nEvaluation with Diverse Sample Numbers To comprehensively compare with the state-of-the-art method NeRF-ID [1], we evaluate NeuSample with diverse output numbers of the sample field, i.e. 192, 128, 64 and 32, which reduce the computation cost of NeRF to 75.4%, 50.4%, 25.4% and 12.9% respectively. As shown in Fig. 5, NeuSample achieves evidently better trade-off between rendering qual-1 NeRF-ID uses a very large batch size, i.e. 66k, on 16 Cloud TPUs. This training strategy can significantly promotes the baseline PSNR by 0.82 on synthetic datasets and 0.16 on real datasets, which we find it hard to reproduce. Therefore we only compare with NeRF-ID on Real Forward-Facing scenes for fairness. ity and computation cost than both NeRF and NeRF-ID for almost all scenes.\n\nThe above experiments demonstrate that the proposed sample field can not only save cost from course network inference, but also capture more elaborate samples for learning scenes and rendering images. Even though the sample field is extracted, the radiance field can still render highquality images with samples which carry useful information.\n\n\nAblation Study\n\nWe perform ablation studies on two scenes, i.e., Realistic Synthetic 360 \u2022 lego at 800 \u00d7 800 and Real Forward-Facing fern at 1008 \u00d7 756.  bers, i.e. 64 and 32. For the 64-sample setting, extracting the sample field without depth boost leads to 0.12 PSNR decay. When the samples become fewer to 32, PSNR degrades more by 0.28. We show the rendering results in Fig. 6 and find without depth boost, some small objects in the scene are omitted while the field with depth boost models accurate outline of these objects. These defects though cause small changes to PSNR values but are evident in the final rendered image. This experiment reveals that a few iterations of depth boost to initialize the sample field effectively helps to locate points with high importance. Even with few samples, the real pixel color can be rendered accurately.\n\n\nLayer Numbers of Sample Field Network\n\nWe study the layer number design for constructing the sample field network. As shown Tab. 5, three layer settings are evaluated, i.e. 8 (the default setting), 4 and 2, on the two scenes of lego and fern. We observe that the layer number decrease causes slight impact for the fern scene rendering within \u223c 0.29 PSNR decay. However, smaller layer numbers lead to drastically rendering quality degradation for the lego scene. Setting to 4 layers drops PSNR by 3.88; and setting to 2 causes 4.36 decay. We analyze this phenomenon and deduce the reason as follows. The synthetic lego scene contains more views in a larger range than fern, so it requires a deeper neural network with more parameters to fit diverse viewdependent sampling distributions. \n\n\nFrequencies of Position Encoding\n\nWe study frequencies of position encoding in the sample field network. As shown in Tab. 6, we evaluate three frequency numbers 5, 10 (the default setting) and 15 on two scenes of lego and fern. We find changing frequencies in this range does not affect the sample field much, < 0.2 PSNR for both scenes.\n\nSample Visualization As shown in Fig. 8, we visualize the obtained samples of three models, i.e. NeRF [17], our NeuSample with 192 points and an extrated NeuSample with 64 points. We observe NeRF tends to sample points in a wide range from the near bound to far bound but locate too many samples at a similar position (the orange mass). On the contrary, NeuSample locates points in a narrower range but the points are more even. With fewer points, NeuSample can still obtain the key samples which render accurate colors. We deduce that an extremely dense distribution around a local point is not beneficial for the field training, e.g. NeRF predicts wrong opacity values for key points in this example.\n\n\nConclusion\n\nIn this paper, we propose a neural sample field which maps a ray to sampling distributions. The proposed sample field can be integrated with neural radiance fields for volume rendering, which we name NeuSample. Our method obtains samples for rendering with a lightweight network module, which saves computation cost from widely used course networks and shows stronger ability of synthesizing novel views for 3D scenes. With a learned NeuSample, the sample field can be extracted for further acceleration with maintaining high rendering quality.\n\nLimitations The proposed NeuSample approach has similar limitations as radiance fields in previous methods [3,17,45] which needs to be optimized towards every independent scene, as the sample field is directly related to the geometry structure of each specific scene. The methods for generalizing the learned field or speeding up the optimization procedure [9,33,37,39,44] could be integrated with our method for further reducing the training cost.\n\nBesides, though NeuSample accelerates rendering, it is unlikely that purely using NeuSample can achieve real-time rendering like methods with caching-based techniques [5,7,29,42,43]. In comparison, NeuSample and [1,11,18] accelerate rendering from another path that focuses on the field itself and requires no additional storage. Overall, this is a problem of achieving better trade-off between storage and speed, and we believe that integrating the above methodologies is a promising future research direction.  [17], i.e. time for rendering one image which is measured on one V100 GPU. * N e of NeuSample denotes the sample number of the extracted sample field. * \"N = 32\" for AutoInt [11] denotes the number of piecewise sections. \n\nFigure 1 .\n1Comparisons between the conventional hierarchical sampling method and our NeuSample. Previous hierarchical sampling needs to first infer a set of coarse points. Based on opacity properties of coarse ones, fine samples are obtained for elaborate rendering. NeuSample directly maps a ray r = (o, d) into a sampling distribution with single inference by constructing a sample field. The obtained samples can directly render high-quality images.\n\nFigure 3 .\n3Architecture of the neural sample field network. \"\u03b3\" denotes position encoding, \"C\" denotes concatenation. The blue blocks indicate fully connected layers followed by ReLU activation and yellow blocks indicate layers with sigmoid activation.\n\nFigure 4 .\n4Depth boost for initializing an extracted sample field. The output mean value of the extracted field is forced to fit the depth predicted by a learned regular field.\n\nt e 1\n1, t e 2 , ..., t e N = S e \u0398 (o, d), L d = |(t e 1 + t e 2 + ... + t e N )/N e \u2212 d r |.\n\n\n. 4.1). Then we show results on two widely used benchmarks, i.e. Realistic Synthetic 360 \u2022 and Real Forward-Facing datasets, and compare with other methods (Sec. 4.2). Some ablation studies are performed and shown in Sec. 4.3.\n\nFigure 5 .Figure 6 .\n56Speedup comparisons on the Real Forward-Facing dataset with NeRF[17] and NeRF-ID[1]. Rendering quality comparisons of sample field extraction w/o and w/ depth boost on the 1008 \u00d7 756 fern scene. \"GT\" denotes ground truth and \"DB\" denotes depth boost. The extracted field without depth boost fails to render some small objects in the scene.\n\nFigure 7 .Figure 8 .\n78Visualization comparisons on the Realistic Synthetic 360 \u2022 (left) and Real Forward-Facing (right) dataset. Our method shows higher quality on rendering detailed of thin structures in the scene. Sample visualization comparisons along the ray in the real scene \"fern\". Samples are colored with \"RGBA\" values, which are set as predicted colors and densities. The edge of each sample is of the orange color, while the orange mass in the NeRF ray represents that lots of points are gathered. The start point is with the finally rendered color.\n\nTable 1 .\n1PSNR comparisons on the Realistic Synthetic 360 \u2022 dataset.Method \nInf. Cost Average Chair Drums Ficus Hotdog Lego Materials \nMic \nShip \n\nNeRF [17] \n1.00 \n31.01 \n33.00 25.01 30.13 \n36.18 \n32.54 \n29.62 \n32.91 28.65 \nNeuSample \n0.76 \n31.15 \n33.02 24.99 30.72 \n36.29 \n33.17 \n29.66 \n32.68 28.65 \n\nAutoInt [11] (N = 32) \n0.31 \n26.83 \n25.82 22.02 25.51 \n31.84 \n27.26 \n28.58 \n28.42 25.18 \nNeuSample (N e = 64) \n0.25 \n28.39 \n29.96 23.43 27.53 \n34.41 \n29.14 \n27.76 \n29.42 25.47 \n\n* \"Inf. Cost\" denotes the relative inference cost compared with NeRF \n\nTable 2 .\n2PSNR comparisons on the Real Forward-Facing dataset.Method \nInf. Cost Average Fern Flower Fortress Horns Leaves Orchids Room T-Rex \n\n1008 \u00d7 756 Resolution \nNeRF [17] \n1.00 \n26.50 \n25.17 27.40 \n31.16 \n27.45 \n20.92 \n20.36 \n32.70 26.80 \nNeRF-ID  \u2020 \n>1.00 \n26.76 \n25.01 27.85 \n31.51 \n27.88 \n21.09 \n20.38 \n32.93 27.45 \nNeuSample \n0.76 \n26.83 \n24.99 28.14 \n31.26 \n28.32 \n21.10 \n20.08 \n33.26 27.46 \nNeuSample (N e = 64) \n0.25 \n26.50 \n24.77 28.03 \n31.09 \n27.45 \n21.06 \n20.03 \n32.71 26.82 \n\n504 \u00d7 378 Resolution \nNeRF [17] \n1.00 \n27.93 \n26.92 28.57 \n32.94 \n29.26 \n22.50 \n21.37 \n33.60 28.26 \nNeuSample \n0.76 \n28.14 \n26.84 28.36 \n32.76 \n30.20 \n22.50 \n20.99 \n34.22 29.23 \n\nAutoInt [11] (N = 32) \n0.31 \n25.53 \n23.51 28.11 \n28.95 \n27.64 \n20.84 \n17.30 \n30.72 27.18 \nNeuSample (N e = 64) \n0.25 \n27.80 \n26.74 28.24 \n32.33 \n29.37 \n22.45 \n20.89 \n33.61 28.79 \nNeuSample (N e = 32) \n0.13 \n26.94 \n26.24 27.95 \n31.87 \n27.48 \n22.33 \n20.50 \n31.56 27.55 \n\n*  \u2020 denotes training with a larger batch size, i.e. 66k in NeRF-ID [1]. \n* Inference cost for NeRF-ID includes the additional proposal network. \n\n\n\nTable 3 .\n3Comparison with state-of-the-art methods on Real Forward-Facing scenes.SRN [34] LLFF [16] NeRF [17] DeRF [28] IBRNet [41] GRF [39] SNeRG [7] NeRF-ID [1] NeuSample \n\nPSNR \u2191 22.84 \n24.13 \n26.50 \n24.81 \n26.73 \n26.64 \n25.63 \n26.76 \n26.83 \nSSIM \u2191 0.668 \n0.798 \n0.811 \n0.767 \n0.851 \n0.837 \n0.818 \n0.822 \n0.823 \nLPIPS \u2193 0.378 \n0.212 \n0.250 \n0.274 \n0.175 \n0.178 \n0.183 \n\\ \n0.231 \n\nwe perform experiments on two resolutions, i.e. 1008 \u00d7 756 \nand 504\u00d7378, for better comparisons. Under the large reso-\nlution, our method achieves a high average PSNR compared \nwith both NeRF and NeRF-ID 1 . When we reduce the sam-\nple number to 64, NeuSample still achieves 26.50 PSNR \nas high as NeRF while the computation cost has been de-\ncreased to 25.4% of NeRF. For the half resolution set-\nting, NeuSample shows 0.21 higher PSNR than NeRF. With \nfewer samples, NeuSample consistently outperforms Au-\ntoInt for all scenes by a large margin, +2.27 PNSR for 64 \npoints +1.41 for 32 points. We comprehensively compare \nwith other state-of-the-art methods on the Real Forward-\nFacing dataset in Tab. 3. NeuSample still achieves compet-\nitive rendering quality. The qualitative results are provided \nin \n\nTable 4 .\n4Depth boost effectiveness study on the \"fern\" scene of the Real Forward-Facing dataset.#Sample Depth Boost PSNR \u2191 SSIM \u2191 LPIPS \u2193128 \n-\n24.99 \n0.798 \n0.272 \n\n64 \n24.77 \n0.785 \n0.289 \n\n24.65 \n0.782 \n0.290 \n\n32 \n24.33 \n0.765 \n0.307 \n\n24.05 \n0.753 \n0.318 \n\nDepth Boost for Field Extraction We evaluate effective-\nness of depth boost proposed in Sec. 3.3. As shown in \nTab. 4, we perform this ablation study on two sample num-\n\n\nTable 5 .\n5Layer number study of the sample field network on the synthetic lego and real fern scenes.Scene #Layers PSNR \u2191 SSIM \u2191 LPIPS \u2193 \n\nlego \n\n8 \n33.17 \n0.965 \n0.048 \n4 \n29.29 \n0.934 \n0.106 \n2 \n28.81 \n0.937 \n0.090 \n\nfern \n\n8 \n24.99 \n0.798 \n0.272 \n4 \n24.70 \n0.778 \n0.303 \n2 \n24.79 \n0.790 \n0.283 \n\n\n\nTable 6 .\n6Frequency study of position encoding on the synthetic lego and real fern scenes.Scene #Frequencies PSNR \u2191 SSIM \u2191 LPIPS \u2193 \n\nlego \n\n5 \n33.09 \n0.966 \n0.047 \n10 \n33.17 \n0.965 \n0.048 \n15 \n33.02 \n0.965 \n0.047 \n\nfern \n\n5 \n24.80 \n0.790 \n0.283 \n10 \n24.99 \n0.798 \n0.272 \n15 \n25.12 \n0.800 \n0.270 \n\n\n\nTable 7 .\n7Per-scene quantitative comparisons on the Realistic Synthetic 360 \u2022 dataset. Inf. Cost Average Chair Drums Ficus Hotdog Lego Materials Mic Ship * \"Inf. Cost\" denotes the relative inference cost compared with NeRFMethod \nPSNR \u2191 \nNeRF [17] \n1.00 \n31.01 \n33.00 25.01 30.13 \n36.18 \n32.54 \n29.62 \n32.91 28.65 \nNeuSample \n0.76 \n31.15 \n33.02 24.99 30.72 \n36.29 \n33.17 \n29.66 \n32.68 28.65 \n\nAutoInt [11] (N = 32) \n0.31 \n26.83 \n25.82 22.02 25.51 \n31.84 \n27.26 \n28.58 \n28.42 25.18 \nNeuSample (N e = 64) \n0.25 \n28.39 \n29.96 23.43 27.53 \n34.41 \n29.14 \n27.76 \n29.42 25.47 \n\nSSIM \u2191 \nNeRF [17] \n1.00 \n0.947 \n0.967 0.925 0.964 \n0.974 \n0.961 \n0.949 \n0.980 0.856 \nNeuSample \n0.76 \n0.949 \n0.968 0.924 0.968 \n0.977 \n0.965 \n0.949 \n0.980 0.863 \n\nAutoInt [11] (N = 32) \n0.31 \n0.927 \n0.926 0.885 0.926 \n0.973 \n0.929 \n0.953 \n0.951 0.869 \nNeuSample (N e = 64) \n0.25 \n0.923 \n0.941 0.897 0.942 \n0.966 \n0.931 \n0.927 \n0.964 0.819 \n\nLPIPS \u2193 \nNeRF [17] \n1.00 \n0.081 \n0.046 0.091 0.044 \n0.121 \n0.050 \n0.063 \n0.028 0.206 \nNeuSample \n0.76 \n0.068 \n0.045 0.091 0.036 \n0.043 \n0.048 \n0.073 \n0.027 0.183 \n\nAutoInt [11] (N = 32) \n0.31 \n0.152 \n0.149 0.209 0.109 \n0.088 \n0.135 \n0.100 \n0.127 0.295 \nNeuSample (N e = 64) \n0.25 \n0.105 \n0.077 0.133 0.073 \n0.067 \n0.097 \n0.095 \n0.057 0.238 \n\n\n\nTable 8 .\n8Per-scene quantitative comparisons on the Real Forward-Facing dataset. Inf. Cost Average Fern Flower Fortress Horns Leaves Orchids Room T-Rex \u2020 denotes training with a larger batch size, i.e. 66k in NeRF-ID[1]. \u2021 Inference cost for NeRF-ID includes the additional proposal network.Method \nPSNR \u2191 \nNeRF [17] \n1.00 \n26.50 \n25.17 27.40 \n31.16 \n27.45 \n20.92 \n20.36 \n32.70 26.80 \nNeRF-ID  \u2020 [1] \n>1.00  \u2021 \n26.76 \n25.01 27.85 \n31.51 \n27.88 \n21.09 \n20.38 \n32.93 27.45 \nNeuSample \n0.76 \n26.83 \n24.99 28.14 \n31.26 \n28.32 \n21.10 \n20.08 \n33.26 27.46 \nNeuSample (N e = 64) \n0.25 \n26.50 \n24.77 28.03 \n31.09 \n27.45 \n21.06 \n20.03 \n32.71 26.82 \n\nSSIM \u2191 \nNeRF [17] \n1.00 \n0.811 \n0.792 0.827 \n0.881 \n0.828 \n0.690 \n0.641 \n0.948 0.880 \nNeRF-ID  \u2020 [1] \n>1.00  \u2021 \n0.822 \n0.800 0.840 \n0.890 \n0.840 \n0.710 \n0.640 \n0.950 0.900 \nNeuSample \n0.76 \n0.823 \n0.798 0.845 \n0.888 \n0.859 \n0.708 \n0.630 \n0.958 0.899 \nNeuSample (N e = 64) \n0.25 \n0.811 \n0.785 0.840 \n0.882 \n0.826 \n0.702 \n0.623 \n0.950 0.882 \n\nLPIPS \u2193 \nNeRF [17] \n1.00 \n0.250 \n0.280 0.219 \n0.171 \n0.268 \n0.316 \n0.321 \n0.178 0.249 \nNeuSample \n0.76 \n0.231 \n0.272 0.192 \n0.158 \n0.218 \n0.296 \n0.331 \n0.154 0.224 \nNeuSample (N e = 64) \n0.25 \n0.251 \n0.289 0.201 \n0.167 \n0.261 \n0.306 \n0.361 \n0.173 0.248 \n\n \nAcknowledgementsWe sincerely thank Liangchen Song, Yingqing Rao and Yuzhu Sun for their generous assistance and discussion.A. Appendix A.1. Detailed Quantitative ResultsAs shown Tab. 7 and Tab. 8, we provide additional quantitative results for both Realistic Synthetic 360 \u2022 scenes at 800 \u00d7 800 and Real Forward-Facing scenes at 1008 \u00d7 756 with three evaluation metrics of PSNR, SSIM and LPIPS. Our NeuSample consistently outperforms NeRF[17]with only 76% computation cost. With fewer samples for rendering, NeuSample still shows promising rendering quality with a significantly better trade-off between computation budget and quality than compared methods[1,11].\nNerf in detail: Learning to sample for view synthesis. Relja Arandjelovi\u0107, Andrew Zisserman, arXiv:2106.052641011Relja Arandjelovi\u0107 and Andrew Zisserman. Nerf in detail: Learning to sample for view synthesis. arXiv:2106.05264, 2021. 2, 5, 6, 7, 9, 10, 11\n\nControlling neural level sets. Matan Atzmon, Niv Haim, Lior Yariv, Ofer Israelov, Haggai Maron, Yaron Lipman, Proceedings of the 33rd International Conference on Neural Information Processing Systems. the 33rd International Conference on Neural Information Processing SystemsMatan Atzmon, Niv Haim, Lior Yariv, Ofer Israelov, Haggai Maron, and Yaron Lipman. Controlling neural level sets. In Proceedings of the 33rd International Conference on Neural Information Processing Systems, 2019. 2\n\nMip-nerf: A multiscale representation for anti-aliasing neural radiance fields. Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, Pratul P Srinivasan, ICCV. Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul P. Srinivasan. Mip-nerf: A multiscale representation for anti-aliasing neu- ral radiance fields. In ICCV, 2021. 1, 2, 3, 5, 8\n\nFree-viewpoint video of human actors. Joel Carranza, Christian Theobalt, A Marcus, Hans-Peter Magnor, Seidel, ACM transactions on graphics. 1Joel Carranza, Christian Theobalt, Marcus A Magnor, and Hans-Peter Seidel. Free-viewpoint video of human actors. ACM transactions on graphics (TOG), 2003. 1\n\nFastnerf: High-fidelity neural rendering at 200fps. Stephan J Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, Julien Valentin, ICCV, 2021. 29Stephan J. Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, and Julien Valentin. Fastnerf: High-fidelity neural rendering at 200fps. In ICCV, 2021. 2, 9\n\nLearning shape templates with structured implicit functions. Kyle Genova, Forrester Cole, Daniel Vlasic, Aaron Sarna, T William, Thomas Freeman, Funkhouser, ICCV. Kyle Genova, Forrester Cole, Daniel Vlasic, Aaron Sarna, William T Freeman, and Thomas Funkhouser. Learning shape templates with structured implicit functions. In ICCV, 2019. 2\n\nBaking neural radiance fields for real-time view synthesis. Peter Hedman, P Pratul, Ben Srinivasan, Jonathan T Mildenhall, Paul Barron, Debevec, ICCV. 69Peter Hedman, Pratul P. Srinivasan, Ben Mildenhall, Jonathan T. Barron, and Paul Debevec. Baking neural ra- diance fields for real-time view synthesis. In ICCV, 2021. 1, 2, 3, 5, 6, 9\n\nLearning a neural 3d texture space from 2d exemplars. Philipp Henzler, J Niloy, Tobias Mitra, Ritschel, CVPR. 2020Philipp Henzler, Niloy J Mitra, and Tobias Ritschel. Learn- ing a neural 3d texture space from 2d exemplars. In CVPR, 2020. 2\n\nPutting nerf on a diet: Semantically consistent few-shot view synthesis. Ajay Jain, Matthew Tancik, Pieter Abbeel, ICCV, 2021. 2Ajay Jain, Matthew Tancik, and Pieter Abbeel. Putting nerf on a diet: Semantically consistent few-shot view synthesis. In ICCV, 2021. 2, 8\n\nRay tracing volume densities. T James, Brian P Von Kajiya, Herzen, ACM SIGGRAPH computer graphics. 13James T Kajiya and Brian P Von Herzen. Ray tracing volume densities. ACM SIGGRAPH computer graphics, 1984. 1, 3\n\nAutoint: Automatic integration for fast neural volume rendering. B David, Lindell, N P Julien, Gordon Martel, Wetzstein, CVPR. 1011David B Lindell, Julien NP Martel, and Gordon Wetzstein. Autoint: Automatic integration for fast neural volume ren- dering. In CVPR, 2021. 2, 5, 6, 9, 10, 11\n\nNeural sparse voxel fields. Lingjie Liu, Jiatao Gu, Tat-Seng Kyaw Zaw Lin, Christian Chua, Theobalt, NeurIPS. 2020Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. Neural sparse voxel fields. In NeurIPS, 2020. 2\n\nLingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. Neural sparse voxel fields. Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. Neural sparse voxel fields. NeurIPS, 2020. 2\n\nOccupancy networks: Learning 3d reconstruction in function space. Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, Andreas Geiger, CVPR. Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se- bastian Nowozin, and Andreas Geiger. Occupancy networks: Learning 3d reconstruction in function space. In CVPR, 2019. 2\n\nImplicit surface representations as layers in neural networks. Mateusz Michalkiewicz, K Jhony, Dominic Pontes, Mahsa Jack, Anders Baktashmotlagh, Eriksson, In ICCV. 2Mateusz Michalkiewicz, Jhony K Pontes, Dominic Jack, Mahsa Baktashmotlagh, and Anders Eriksson. Implicit sur- face representations as layers in neural networks. In ICCV, 2019. 2\n\nLocal light field fusion: Practical view synthesis with prescriptive sampling guidelines. Ben Mildenhall, P Pratul, Rodrigo Srinivasan, Nima Khademi Ortiz-Cayon, Ravi Kalantari, Ren Ramamoorthi, Abhishek Ng, Kar, Proceedings of SIGGRAPH). SIGGRAPH)56Ben Mildenhall, Pratul P. Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and Abhishek Kar. Local light field fusion: Practical view syn- thesis with prescriptive sampling guidelines. ACM Transac- tions on Graphics (Proceedings of SIGGRAPH), 2019. 5, 6\n\nNerf: Representing scenes as neural radiance fields for view synthesis. Ben Mildenhall, P Pratul, Matthew Srinivasan, Jonathan T Tancik, Ravi Barron, Ren Ramamoorthi, Ng, ECCV. 1011Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view syn- thesis. In ECCV, 2020. 1, 2, 3, 4, 5, 6, 7, 8, 10, 11\n\nDONeRF: Towards Real-Time Rendering of Compact Neural Radiance Fields using Depth Oracle Networks. Thomas Neff, Pascal Stadlbauer, Mathias Parger, Andreas Kurz, H Joerg, Chakravarty R Alla Mueller, Anton S Chaitanya, Markus Kaplanyan, Steinberger, Computer Graphics Forum. 29Thomas Neff, Pascal Stadlbauer, Mathias Parger, Andreas Kurz, Joerg H. Mueller, Chakravarty R. Alla Chaitanya, An- ton S. Kaplanyan, and Markus Steinberger. DONeRF: To- wards Real-Time Rendering of Compact Neural Radiance Fields using Depth Oracle Networks. Computer Graphics Forum, 2021. 2, 9\n\nOccupancy flow: 4d reconstruction by learning particle dynamics. Michael Niemeyer, Lars Mescheder, Michael Oechsle, Andreas Geiger, ICCV. Michael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger. Occupancy flow: 4d reconstruction by learning particle dynamics. In ICCV, 2019. 2\n\nTexture fields: Learning texture representations in function space. Michael Oechsle, Lars Mescheder, Michael Niemeyer, Thilo Strauss, Andreas Geiger, ICCV. Michael Oechsle, Lars Mescheder, Michael Niemeyer, Thilo Strauss, and Andreas Geiger. Texture fields: Learning tex- ture representations in function space. In ICCV, 2019. 2\n\nUnisurf: Unifying neural implicit surfaces and radiance fields for multi-view reconstruction. Michael Oechsle, Songyou Peng, Andreas Geiger, International Conference on Computer Vision (ICCV). 2021Michael Oechsle, Songyou Peng, and Andreas Geiger. Unisurf: Unifying neural implicit surfaces and radiance fields for multi-view reconstruction. In International Con- ference on Computer Vision (ICCV), 2021. 2\n\nKeunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman, Steven M Seitz, Ricardo Martin-Brualla, Nerfies: Deformable neural radiance fields. In ICCV. Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Steven M. Seitz, and Ricardo Martin-Brualla. Nerfies: Deformable neural radiance fields. In ICCV, 2021. 2\n\nConvolutional occupancy networks. Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, Andreas Geiger, ECCV. Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, and Andreas Geiger. Convolutional occupancy networks. In ECCV, 2020. 2\n\nTerminerf: Ray termination prediction for efficient neural rendering. Martin Piala, Ronald Clark, 3Martin Piala and Ronald Clark. Terminerf: Ray termination prediction for efficient neural rendering. In 3DV, 2021. 3\n\nD-nerf: Neural radiance fields for dynamic scenes. Albert Pumarola, Enric Corona, Gerard Pons-Moll, Francesc Moreno-Noguer, CVPR. Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf: Neural radiance fields for dynamic scenes. In CVPR, 2021. 2\n\nUnified neural encoding of BTFs. Gilles Rainer, Abhijeet Ghosh, Jakob Wenzel, Tim Weyrich, Computer Graphics Forum (Proc. Eurographics). 2020Gilles Rainer, Abhijeet Ghosh, Wenzel Jakob, and Tim Weyrich. Unified neural encoding of BTFs. Computer Graphics Forum (Proc. Eurographics), 2020. 2\n\nNeural btf compression and interpolation. Gilles Rainer, Wenzel Jakob, Abhijeet Ghosh, Tim Weyrich, Computer Graphics Forum. Gilles Rainer, Wenzel Jakob, Abhijeet Ghosh, and Tim Weyrich. Neural btf compression and interpolation. In Com- puter Graphics Forum, 2019. 2\n\nDerf: Decomposed radiance fields. Daniel Rebain, Wei Jiang, Soroosh Yazdani, Ke Li, Kwang Moo Yi, Andrea Tagliasacchi, CVPR. Daniel Rebain, Wei Jiang, Soroosh Yazdani, Ke Li, Kwang Moo Yi, and Andrea Tagliasacchi. Derf: Decom- posed radiance fields. In CVPR, 2021. 6\n\nKilonerf: Speeding up neural radiance fields with thousands of tiny mlps. Christian Reiser, Songyou Peng, Yiyi Liao, Andreas Geiger, ICCV. 59Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas Geiger. Kilonerf: Speeding up neural radiance fields with thousands of tiny mlps. In ICCV, 2021. 1, 2, 3, 5, 9\n\nGlobal illumination with radiance regression functions. Peiran Ren, Jiaping Wang, Minmin Gong, Stephen Lin, Xin Tong, Baining Guo, ACM Transactions on Graphics. 2Peiran Ren, Jiaping Wang, Minmin Gong, Stephen Lin, Xin Tong, and Baining Guo. Global illumination with radiance regression functions. ACM Transactions on Graphics (TOG), 2013. 2\n\nGraf: Generative radiance fields for 3d-aware image synthesis. Katja Schwarz, Yiyi Liao, Michael Niemeyer, Andreas Geiger, NeurIPS. 2Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas Geiger. Graf: Generative radiance fields for 3d-aware image synthesis. NeurIPS, 2020. 2\n\nImplicit neural representations with periodic activation functions. Vincent Sitzmann, N P Julien, Alexander W Martel, David B Bergman, Gordon Lindell, Wetzstein, NeurIPS. 2020Vincent Sitzmann, Julien N.P. Martel, Alexander W. Bergman, David B. Lindell, and Gordon Wetzstein. Implicit neural representations with periodic activation functions. In NeurIPS, 2020. 3\n\nLight field networks: Neural scene representations with single-evaluation rendering. Vincent Sitzmann, Semon Rezchikov, William T Freeman, Joshua B Tenenbaum, Fredo Durand, NeurIPS, 2021. 2Vincent Sitzmann, Semon Rezchikov, William T. Freeman, Joshua B. Tenenbaum, and Fredo Durand. Light field net- works: Neural scene representations with single-evaluation rendering. In NeurIPS, 2021. 2, 8\n\nScene representation networks: Continuous 3d-structure-aware neural scene representations. Vincent Sitzmann, Michael Zollhoefer, Gordon Wetzstein, NeurIPS. 26Vincent Sitzmann, Michael Zollhoefer, and Gordon Wet- zstein. Scene representation networks: Continuous 3d- structure-aware neural scene representations. NeurIPS, 2019. 2, 6\n\nNerv: Neural reflectance and visibility fields for relighting and view synthesis. P Pratul, Boyang Srinivasan, Xiuming Deng, Matthew Zhang, Ben Tancik, Jonathan T Mildenhall, Barron, CVPR. Pratul P Srinivasan, Boyang Deng, Xiuming Zhang, Matthew Tancik, Ben Mildenhall, and Jonathan T Barron. Nerv: Neural reflectance and visibility fields for relighting and view synthesis. In CVPR, 2021. 2\n\nComputer vision: Algorithms and applications. Instructor. R Szeliski, R Szeliski. Computer vision: Algorithms and applications. Instructor, 2019. 1\n\nLearned initializations for optimizing coordinate-based neural representations. Matthew Tancik, Ben Mildenhall, Terrance Wang, Divi Schmidt, P Pratul, Jonathan T Srinivasan, Ren Barron, Ng, CVPR. Matthew Tancik, Ben Mildenhall, Terrance Wang, Divi Schmidt, Pratul P Srinivasan, Jonathan T Barron, and Ren Ng. Learned initializations for optimizing coordinate-based neural representations. In CVPR, 2021. 8\n\nFourier features let networks learn high frequency functions in low dimensional domains. Matthew Tancik, P Pratul, Ben Srinivasan, Sara Mildenhall, Nithin Fridovich-Keil, Utkarsh Raghavan, Ravi Singhal, Jonathan T Ramamoorthi, Ren Barron, Ng, NeurIPS. Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ra- mamoorthi, Jonathan T. Barron, and Ren Ng. Fourier fea- tures let networks learn high frequency functions in low di- mensional domains. In NeurIPS, 2020. 3\n\nGrf: Learning a general radiance field for 3d representation and rendering. Alex Trevithick, Bo Yang, ICCV. 6Alex Trevithick and Bo Yang. Grf: Learning a general ra- diance field for 3d representation and rendering. In ICCV, 2021. 6, 8\n\nNeus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, Wenping Wang, NeurIPS. 2Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. NeurIPS, 2021. 2\n\nIbrnet: Learning multi-view image-based rendering. Qianqian Wang, Zhicheng Wang, Kyle Genova, P Pratul, Howard Srinivasan, Jonathan T Zhou, Ricardo Barron, Noah Martin-Brualla, Thomas Snavely, Funkhouser, CVPR. Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P Srinivasan, Howard Zhou, Jonathan T Barron, Ricardo Martin-Brualla, Noah Snavely, and Thomas Funkhouser. Ibr- net: Learning multi-view image-based rendering. In CVPR, 2021. 6\n\nNex: Real-time view synthesis with neural basis expansion. Suttisak Wizadwongsa, Pakkapon Phongthawee, Jiraphon Yenphraphai, Supasorn Suwajanakorn, CVPR, 2021. 59Suttisak Wizadwongsa, Pakkapon Phongthawee, Jiraphon Yenphraphai, and Supasorn Suwajanakorn. Nex: Real-time view synthesis with neural basis expansion. In CVPR, 2021. 2, 5, 9\n\nPlenoctrees for real-time rendering of neural radiance fields. Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, Angjoo Kanazawa, ICCV, 2021. 29Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. Plenoctrees for real-time rendering of neural radiance fields. In ICCV, 2021. 2, 9\n\npixelnerf: Neural radiance fields from one or few images. Alex Yu, Vickie Ye, Matthew Tancik, Angjoo Kanazawa, CVPR, 2021. 2Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images. In CVPR, 2021. 2, 8\n\nKai Zhang, Gernot Riegler, Noah Snavely, Vladlen Koltun, arXiv:2010.07492Nerf++: Analyzing and improving neural radiance fields. 2Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen Koltun. Nerf++: Analyzing and improving neural radiance fields. arXiv:2010.07492, 2020. 2, 8\n", "annotations": {"author": "[{\"end\":218,\"start\":63},{\"end\":243,\"start\":219},{\"end\":337,\"start\":244},{\"end\":340,\"start\":338},{\"end\":369,\"start\":341},{\"end\":440,\"start\":370},{\"end\":462,\"start\":441}]", "publisher": null, "author_last_name": "[{\"end\":74,\"start\":70},{\"end\":229,\"start\":226},{\"end\":257,\"start\":253},{\"end\":355,\"start\":350},{\"end\":379,\"start\":376},{\"end\":448,\"start\":444}]", "author_first_name": "[{\"end\":69,\"start\":63},{\"end\":225,\"start\":219},{\"end\":252,\"start\":244},{\"end\":339,\"start\":338},{\"end\":349,\"start\":341},{\"end\":375,\"start\":370},{\"end\":443,\"start\":441}]", "author_affiliation": "[{\"end\":157,\"start\":76},{\"end\":217,\"start\":159},{\"end\":242,\"start\":231},{\"end\":336,\"start\":278},{\"end\":368,\"start\":357},{\"end\":439,\"start\":381},{\"end\":461,\"start\":450}]", "title": "[{\"end\":60,\"start\":1},{\"end\":522,\"start\":463}]", "venue": null, "abstract": "[{\"end\":1480,\"start\":524}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1580,\"start\":1577},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":1583,\"start\":1580},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":1914,\"start\":1910},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2294,\"start\":2290},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2807,\"start\":2804},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2809,\"start\":2807},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2812,\"start\":2809},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2815,\"start\":2812},{\"end\":5490,\"start\":5461},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5738,\"start\":5734},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5763,\"start\":5760},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5765,\"start\":5763},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5768,\"start\":5765},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5771,\"start\":5768},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5774,\"start\":5771},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":5777,\"start\":5774},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":5780,\"start\":5777},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6088,\"start\":6085},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6091,\"start\":6088},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6094,\"start\":6091},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6097,\"start\":6094},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6132,\"start\":6128},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6163,\"start\":6159},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":6166,\"start\":6163},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6188,\"start\":6184},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6709,\"start\":6705},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":6818,\"start\":6814},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6940,\"start\":6937},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6954,\"start\":6951},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":6975,\"start\":6971},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":7196,\"start\":7192},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7980,\"start\":7976},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8144,\"start\":8140},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8380,\"start\":8376},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8711,\"start\":8708},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8910,\"start\":8906},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9310,\"start\":9306},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9617,\"start\":9613},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":10355,\"start\":10351},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":11240,\"start\":11236},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11269,\"start\":11266},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":11271,\"start\":11269},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":11274,\"start\":11271},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":11277,\"start\":11274},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":11810,\"start\":11806},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":13307,\"start\":13304},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":13310,\"start\":13307},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":13313,\"start\":13310},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":13316,\"start\":13313},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":15586,\"start\":15582},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":17000,\"start\":16996},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":17174,\"start\":17170},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":17901,\"start\":17897},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":19437,\"start\":19433},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":19477,\"start\":19474},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":19479,\"start\":19477},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":19481,\"start\":19479},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":19484,\"start\":19481},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":19487,\"start\":19484},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":19490,\"start\":19487},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":19754,\"start\":19750},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":19768,\"start\":19764},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":19819,\"start\":19815},{\"end\":19836,\"start\":19833},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":20070,\"start\":20066},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":20087,\"start\":20083},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":20310,\"start\":20306},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":21109,\"start\":21106},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":24209,\"start\":24205},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":25476,\"start\":25473},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":25479,\"start\":25476},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":25482,\"start\":25479},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":25726,\"start\":25723},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":25729,\"start\":25726},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":25732,\"start\":25729},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":25735,\"start\":25732},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":25738,\"start\":25735},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":25986,\"start\":25983},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":25988,\"start\":25986},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":25991,\"start\":25988},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":25994,\"start\":25991},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":25997,\"start\":25994},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":26031,\"start\":26028},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":26034,\"start\":26031},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":26037,\"start\":26034},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":26333,\"start\":26329},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":26507,\"start\":26503},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":27857,\"start\":27853},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":27872,\"start\":27869},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":34039,\"start\":34036}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":27005,\"start\":26551},{\"attributes\":{\"id\":\"fig_2\"},\"end\":27260,\"start\":27006},{\"attributes\":{\"id\":\"fig_3\"},\"end\":27439,\"start\":27261},{\"attributes\":{\"id\":\"fig_4\"},\"end\":27535,\"start\":27440},{\"attributes\":{\"id\":\"fig_5\"},\"end\":27764,\"start\":27536},{\"attributes\":{\"id\":\"fig_6\"},\"end\":28128,\"start\":27765},{\"attributes\":{\"id\":\"fig_7\"},\"end\":28691,\"start\":28129},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":29243,\"start\":28692},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":30333,\"start\":29244},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":31524,\"start\":30334},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":31959,\"start\":31525},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":32260,\"start\":31960},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":32560,\"start\":32261},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":33817,\"start\":32561},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":35057,\"start\":33818}]", "paragraph": "[{\"end\":1833,\"start\":1496},{\"end\":2732,\"start\":1835},{\"end\":3479,\"start\":2734},{\"end\":4182,\"start\":3481},{\"end\":4753,\"start\":4184},{\"end\":5344,\"start\":4755},{\"end\":6409,\"start\":5361},{\"end\":7787,\"start\":6411},{\"end\":9241,\"start\":7789},{\"end\":9511,\"start\":9252},{\"end\":9957,\"start\":9530},{\"end\":10478,\"start\":9989},{\"end\":10931,\"start\":10565},{\"end\":11468,\"start\":10958},{\"end\":11811,\"start\":11537},{\"end\":12035,\"start\":11849},{\"end\":12518,\"start\":12059},{\"end\":12753,\"start\":12557},{\"end\":12960,\"start\":12808},{\"end\":13045,\"start\":12991},{\"end\":13518,\"start\":13066},{\"end\":14258,\"start\":13529},{\"end\":15277,\"start\":14260},{\"end\":16743,\"start\":15305},{\"end\":17216,\"start\":16876},{\"end\":17521,\"start\":17218},{\"end\":17621,\"start\":17537},{\"end\":18166,\"start\":17648},{\"end\":18727,\"start\":18168},{\"end\":19298,\"start\":18729},{\"end\":19954,\"start\":19300},{\"end\":20702,\"start\":19982},{\"end\":20997,\"start\":20704},{\"end\":21772,\"start\":20999},{\"end\":22117,\"start\":21774},{\"end\":22972,\"start\":22136},{\"end\":23761,\"start\":23014},{\"end\":24101,\"start\":23798},{\"end\":24805,\"start\":24103},{\"end\":25364,\"start\":24820},{\"end\":25814,\"start\":25366},{\"end\":26550,\"start\":25816}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9988,\"start\":9958},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10564,\"start\":10479},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10957,\"start\":10932},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11536,\"start\":11469},{\"attributes\":{\"id\":\"formula_4\"},\"end\":11848,\"start\":11812},{\"attributes\":{\"id\":\"formula_5\"},\"end\":12556,\"start\":12519},{\"attributes\":{\"id\":\"formula_6\"},\"end\":12807,\"start\":12754},{\"attributes\":{\"id\":\"formula_7\"},\"end\":12990,\"start\":12961},{\"attributes\":{\"id\":\"formula_8\"},\"end\":13065,\"start\":13046},{\"attributes\":{\"id\":\"formula_9\"},\"end\":16875,\"start\":16744}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1494,\"start\":1482},{\"attributes\":{\"n\":\"2.\"},\"end\":5359,\"start\":5347},{\"attributes\":{\"n\":\"3.\"},\"end\":9250,\"start\":9244},{\"attributes\":{\"n\":\"3.1.\"},\"end\":9528,\"start\":9514},{\"attributes\":{\"n\":\"3.2.\"},\"end\":12057,\"start\":12038},{\"end\":13527,\"start\":13521},{\"attributes\":{\"n\":\"3.3.\"},\"end\":15303,\"start\":15280},{\"attributes\":{\"n\":\"4.\"},\"end\":17535,\"start\":17524},{\"attributes\":{\"n\":\"4.1.\"},\"end\":17646,\"start\":17624},{\"attributes\":{\"n\":\"4.2.\"},\"end\":19980,\"start\":19957},{\"attributes\":{\"n\":\"4.3.\"},\"end\":22134,\"start\":22120},{\"end\":23012,\"start\":22975},{\"end\":23796,\"start\":23764},{\"attributes\":{\"n\":\"5.\"},\"end\":24818,\"start\":24808},{\"end\":26562,\"start\":26552},{\"end\":27017,\"start\":27007},{\"end\":27272,\"start\":27262},{\"end\":27446,\"start\":27441},{\"end\":27786,\"start\":27766},{\"end\":28150,\"start\":28130},{\"end\":28702,\"start\":28693},{\"end\":29254,\"start\":29245},{\"end\":30344,\"start\":30335},{\"end\":31535,\"start\":31526},{\"end\":31970,\"start\":31961},{\"end\":32271,\"start\":32262},{\"end\":32571,\"start\":32562},{\"end\":33828,\"start\":33819}]", "table": "[{\"end\":29243,\"start\":28762},{\"end\":30333,\"start\":29308},{\"end\":31524,\"start\":30417},{\"end\":31959,\"start\":31665},{\"end\":32260,\"start\":32062},{\"end\":32560,\"start\":32353},{\"end\":33817,\"start\":32785},{\"end\":35057,\"start\":34111}]", "figure_caption": "[{\"end\":27005,\"start\":26564},{\"end\":27260,\"start\":27019},{\"end\":27439,\"start\":27274},{\"end\":27535,\"start\":27448},{\"end\":27764,\"start\":27538},{\"end\":28128,\"start\":27789},{\"end\":28691,\"start\":28153},{\"end\":28762,\"start\":28704},{\"end\":29308,\"start\":29256},{\"end\":30417,\"start\":30346},{\"end\":31665,\"start\":31537},{\"end\":32062,\"start\":31972},{\"end\":32353,\"start\":32273},{\"end\":32785,\"start\":32573},{\"end\":34111,\"start\":33830}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":2874,\"start\":2868},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":3807,\"start\":3801},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11229,\"start\":11223},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":13145,\"start\":13139},{\"end\":13581,\"start\":13573},{\"end\":14296,\"start\":14290},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":15448,\"start\":15442},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":17825,\"start\":17819},{\"end\":20578,\"start\":20572},{\"end\":20938,\"start\":20932},{\"end\":21315,\"start\":21309},{\"end\":22501,\"start\":22495},{\"end\":24142,\"start\":24136}]", "bib_author_first_name": "[{\"end\":35782,\"start\":35777},{\"end\":35803,\"start\":35797},{\"end\":36014,\"start\":36009},{\"end\":36026,\"start\":36023},{\"end\":36037,\"start\":36033},{\"end\":36049,\"start\":36045},{\"end\":36066,\"start\":36060},{\"end\":36079,\"start\":36074},{\"end\":36558,\"start\":36550},{\"end\":36560,\"start\":36559},{\"end\":36572,\"start\":36569},{\"end\":36592,\"start\":36585},{\"end\":36606,\"start\":36601},{\"end\":36622,\"start\":36615},{\"end\":36645,\"start\":36639},{\"end\":36647,\"start\":36646},{\"end\":36936,\"start\":36932},{\"end\":36956,\"start\":36947},{\"end\":36968,\"start\":36967},{\"end\":36987,\"start\":36977},{\"end\":37252,\"start\":37245},{\"end\":37254,\"start\":37253},{\"end\":37268,\"start\":37263},{\"end\":37286,\"start\":37279},{\"end\":37301,\"start\":37296},{\"end\":37317,\"start\":37311},{\"end\":37568,\"start\":37564},{\"end\":37586,\"start\":37577},{\"end\":37599,\"start\":37593},{\"end\":37613,\"start\":37608},{\"end\":37622,\"start\":37621},{\"end\":37638,\"start\":37632},{\"end\":37909,\"start\":37904},{\"end\":37919,\"start\":37918},{\"end\":37931,\"start\":37928},{\"end\":37952,\"start\":37944},{\"end\":37954,\"start\":37953},{\"end\":37971,\"start\":37967},{\"end\":38243,\"start\":38236},{\"end\":38254,\"start\":38253},{\"end\":38268,\"start\":38262},{\"end\":38500,\"start\":38496},{\"end\":38514,\"start\":38507},{\"end\":38529,\"start\":38523},{\"end\":38722,\"start\":38721},{\"end\":38741,\"start\":38730},{\"end\":38971,\"start\":38970},{\"end\":38989,\"start\":38988},{\"end\":38991,\"start\":38990},{\"end\":39006,\"start\":39000},{\"end\":39230,\"start\":39223},{\"end\":39242,\"start\":39236},{\"end\":39255,\"start\":39247},{\"end\":39279,\"start\":39270},{\"end\":39442,\"start\":39435},{\"end\":39454,\"start\":39448},{\"end\":39467,\"start\":39459},{\"end\":39733,\"start\":39729},{\"end\":39752,\"start\":39745},{\"end\":39769,\"start\":39762},{\"end\":39789,\"start\":39780},{\"end\":39806,\"start\":39799},{\"end\":40067,\"start\":40060},{\"end\":40084,\"start\":40083},{\"end\":40099,\"start\":40092},{\"end\":40113,\"start\":40108},{\"end\":40126,\"start\":40120},{\"end\":40435,\"start\":40432},{\"end\":40449,\"start\":40448},{\"end\":40465,\"start\":40458},{\"end\":40482,\"start\":40478},{\"end\":40490,\"start\":40483},{\"end\":40508,\"start\":40504},{\"end\":40523,\"start\":40520},{\"end\":40545,\"start\":40537},{\"end\":40956,\"start\":40953},{\"end\":40970,\"start\":40969},{\"end\":40986,\"start\":40979},{\"end\":41007,\"start\":40999},{\"end\":41009,\"start\":41008},{\"end\":41022,\"start\":41018},{\"end\":41034,\"start\":41031},{\"end\":41390,\"start\":41384},{\"end\":41403,\"start\":41397},{\"end\":41423,\"start\":41416},{\"end\":41439,\"start\":41432},{\"end\":41447,\"start\":41446},{\"end\":41466,\"start\":41455},{\"end\":41473,\"start\":41467},{\"end\":41488,\"start\":41483},{\"end\":41490,\"start\":41489},{\"end\":41508,\"start\":41502},{\"end\":41927,\"start\":41920},{\"end\":41942,\"start\":41938},{\"end\":41961,\"start\":41954},{\"end\":41978,\"start\":41971},{\"end\":42222,\"start\":42215},{\"end\":42236,\"start\":42232},{\"end\":42255,\"start\":42248},{\"end\":42271,\"start\":42266},{\"end\":42288,\"start\":42281},{\"end\":42578,\"start\":42571},{\"end\":42595,\"start\":42588},{\"end\":42609,\"start\":42602},{\"end\":42893,\"start\":42885},{\"end\":42907,\"start\":42900},{\"end\":42923,\"start\":42915},{\"end\":42925,\"start\":42924},{\"end\":42940,\"start\":42934},{\"end\":42953,\"start\":42950},{\"end\":42955,\"start\":42954},{\"end\":42971,\"start\":42965},{\"end\":42973,\"start\":42972},{\"end\":42988,\"start\":42981},{\"end\":43287,\"start\":43280},{\"end\":43301,\"start\":43294},{\"end\":43316,\"start\":43312},{\"end\":43332,\"start\":43328},{\"end\":43351,\"start\":43344},{\"end\":43578,\"start\":43572},{\"end\":43592,\"start\":43586},{\"end\":43776,\"start\":43770},{\"end\":43792,\"start\":43787},{\"end\":43807,\"start\":43801},{\"end\":43827,\"start\":43819},{\"end\":44034,\"start\":44028},{\"end\":44051,\"start\":44043},{\"end\":44064,\"start\":44059},{\"end\":44076,\"start\":44073},{\"end\":44334,\"start\":44328},{\"end\":44349,\"start\":44343},{\"end\":44365,\"start\":44357},{\"end\":44376,\"start\":44373},{\"end\":44594,\"start\":44588},{\"end\":44606,\"start\":44603},{\"end\":44621,\"start\":44614},{\"end\":44633,\"start\":44631},{\"end\":44647,\"start\":44638},{\"end\":44658,\"start\":44652},{\"end\":44905,\"start\":44896},{\"end\":44921,\"start\":44914},{\"end\":44932,\"start\":44928},{\"end\":44946,\"start\":44939},{\"end\":45192,\"start\":45186},{\"end\":45205,\"start\":45198},{\"end\":45218,\"start\":45212},{\"end\":45232,\"start\":45225},{\"end\":45241,\"start\":45238},{\"end\":45255,\"start\":45248},{\"end\":45540,\"start\":45535},{\"end\":45554,\"start\":45550},{\"end\":45568,\"start\":45561},{\"end\":45586,\"start\":45579},{\"end\":45825,\"start\":45818},{\"end\":45837,\"start\":45836},{\"end\":45839,\"start\":45838},{\"end\":45857,\"start\":45848},{\"end\":45859,\"start\":45858},{\"end\":45873,\"start\":45868},{\"end\":45875,\"start\":45874},{\"end\":45891,\"start\":45885},{\"end\":46206,\"start\":46199},{\"end\":46222,\"start\":46217},{\"end\":46241,\"start\":46234},{\"end\":46243,\"start\":46242},{\"end\":46259,\"start\":46253},{\"end\":46261,\"start\":46260},{\"end\":46278,\"start\":46273},{\"end\":46606,\"start\":46599},{\"end\":46624,\"start\":46617},{\"end\":46643,\"start\":46637},{\"end\":46924,\"start\":46923},{\"end\":46939,\"start\":46933},{\"end\":46959,\"start\":46952},{\"end\":46973,\"start\":46966},{\"end\":46984,\"start\":46981},{\"end\":47001,\"start\":46993},{\"end\":47003,\"start\":47002},{\"end\":47293,\"start\":47292},{\"end\":47470,\"start\":47463},{\"end\":47482,\"start\":47479},{\"end\":47503,\"start\":47495},{\"end\":47514,\"start\":47510},{\"end\":47525,\"start\":47524},{\"end\":47542,\"start\":47534},{\"end\":47544,\"start\":47543},{\"end\":47560,\"start\":47557},{\"end\":47886,\"start\":47879},{\"end\":47896,\"start\":47895},{\"end\":47908,\"start\":47905},{\"end\":47925,\"start\":47921},{\"end\":47944,\"start\":47938},{\"end\":47968,\"start\":47961},{\"end\":47983,\"start\":47979},{\"end\":48001,\"start\":47993},{\"end\":48003,\"start\":48002},{\"end\":48020,\"start\":48017},{\"end\":48397,\"start\":48393},{\"end\":48412,\"start\":48410},{\"end\":48649,\"start\":48645},{\"end\":48663,\"start\":48656},{\"end\":48673,\"start\":48669},{\"end\":48688,\"start\":48679},{\"end\":48703,\"start\":48699},{\"end\":48719,\"start\":48712},{\"end\":48989,\"start\":48981},{\"end\":49004,\"start\":48996},{\"end\":49015,\"start\":49011},{\"end\":49025,\"start\":49024},{\"end\":49040,\"start\":49034},{\"end\":49061,\"start\":49053},{\"end\":49063,\"start\":49062},{\"end\":49077,\"start\":49070},{\"end\":49090,\"start\":49086},{\"end\":49113,\"start\":49107},{\"end\":49436,\"start\":49428},{\"end\":49458,\"start\":49450},{\"end\":49480,\"start\":49472},{\"end\":49502,\"start\":49494},{\"end\":49774,\"start\":49770},{\"end\":49786,\"start\":49779},{\"end\":49798,\"start\":49791},{\"end\":49810,\"start\":49807},{\"end\":49818,\"start\":49815},{\"end\":49829,\"start\":49823},{\"end\":50074,\"start\":50070},{\"end\":50085,\"start\":50079},{\"end\":50097,\"start\":50090},{\"end\":50112,\"start\":50106},{\"end\":50275,\"start\":50272},{\"end\":50289,\"start\":50283},{\"end\":50303,\"start\":50299},{\"end\":50320,\"start\":50313}]", "bib_author_last_name": "[{\"end\":35795,\"start\":35783},{\"end\":35813,\"start\":35804},{\"end\":36021,\"start\":36015},{\"end\":36031,\"start\":36027},{\"end\":36043,\"start\":36038},{\"end\":36058,\"start\":36050},{\"end\":36072,\"start\":36067},{\"end\":36086,\"start\":36080},{\"end\":36567,\"start\":36561},{\"end\":36583,\"start\":36573},{\"end\":36599,\"start\":36593},{\"end\":36613,\"start\":36607},{\"end\":36637,\"start\":36623},{\"end\":36658,\"start\":36648},{\"end\":36945,\"start\":36937},{\"end\":36965,\"start\":36957},{\"end\":36975,\"start\":36969},{\"end\":36994,\"start\":36988},{\"end\":37002,\"start\":36996},{\"end\":37261,\"start\":37255},{\"end\":37277,\"start\":37269},{\"end\":37294,\"start\":37287},{\"end\":37309,\"start\":37302},{\"end\":37326,\"start\":37318},{\"end\":37575,\"start\":37569},{\"end\":37591,\"start\":37587},{\"end\":37606,\"start\":37600},{\"end\":37619,\"start\":37614},{\"end\":37630,\"start\":37623},{\"end\":37646,\"start\":37639},{\"end\":37658,\"start\":37648},{\"end\":37916,\"start\":37910},{\"end\":37926,\"start\":37920},{\"end\":37942,\"start\":37932},{\"end\":37965,\"start\":37955},{\"end\":37978,\"start\":37972},{\"end\":37987,\"start\":37980},{\"end\":38251,\"start\":38244},{\"end\":38260,\"start\":38255},{\"end\":38274,\"start\":38269},{\"end\":38284,\"start\":38276},{\"end\":38505,\"start\":38501},{\"end\":38521,\"start\":38515},{\"end\":38536,\"start\":38530},{\"end\":38728,\"start\":38723},{\"end\":38748,\"start\":38742},{\"end\":38756,\"start\":38750},{\"end\":38977,\"start\":38972},{\"end\":38986,\"start\":38979},{\"end\":38998,\"start\":38992},{\"end\":39013,\"start\":39007},{\"end\":39024,\"start\":39015},{\"end\":39234,\"start\":39231},{\"end\":39245,\"start\":39243},{\"end\":39268,\"start\":39256},{\"end\":39284,\"start\":39280},{\"end\":39294,\"start\":39286},{\"end\":39446,\"start\":39443},{\"end\":39457,\"start\":39455},{\"end\":39471,\"start\":39468},{\"end\":39743,\"start\":39734},{\"end\":39760,\"start\":39753},{\"end\":39778,\"start\":39770},{\"end\":39797,\"start\":39790},{\"end\":39813,\"start\":39807},{\"end\":40081,\"start\":40068},{\"end\":40090,\"start\":40085},{\"end\":40106,\"start\":40100},{\"end\":40118,\"start\":40114},{\"end\":40141,\"start\":40127},{\"end\":40151,\"start\":40143},{\"end\":40446,\"start\":40436},{\"end\":40456,\"start\":40450},{\"end\":40476,\"start\":40466},{\"end\":40502,\"start\":40491},{\"end\":40518,\"start\":40509},{\"end\":40535,\"start\":40524},{\"end\":40548,\"start\":40546},{\"end\":40553,\"start\":40550},{\"end\":40967,\"start\":40957},{\"end\":40977,\"start\":40971},{\"end\":40997,\"start\":40987},{\"end\":41016,\"start\":41010},{\"end\":41029,\"start\":41023},{\"end\":41046,\"start\":41035},{\"end\":41050,\"start\":41048},{\"end\":41395,\"start\":41391},{\"end\":41414,\"start\":41404},{\"end\":41430,\"start\":41424},{\"end\":41444,\"start\":41440},{\"end\":41453,\"start\":41448},{\"end\":41481,\"start\":41474},{\"end\":41500,\"start\":41491},{\"end\":41518,\"start\":41509},{\"end\":41531,\"start\":41520},{\"end\":41936,\"start\":41928},{\"end\":41952,\"start\":41943},{\"end\":41969,\"start\":41962},{\"end\":41985,\"start\":41979},{\"end\":42230,\"start\":42223},{\"end\":42246,\"start\":42237},{\"end\":42264,\"start\":42256},{\"end\":42279,\"start\":42272},{\"end\":42295,\"start\":42289},{\"end\":42586,\"start\":42579},{\"end\":42600,\"start\":42596},{\"end\":42616,\"start\":42610},{\"end\":42898,\"start\":42894},{\"end\":42913,\"start\":42908},{\"end\":42932,\"start\":42926},{\"end\":42948,\"start\":42941},{\"end\":42963,\"start\":42956},{\"end\":42979,\"start\":42974},{\"end\":43003,\"start\":42989},{\"end\":43292,\"start\":43288},{\"end\":43310,\"start\":43302},{\"end\":43326,\"start\":43317},{\"end\":43342,\"start\":43333},{\"end\":43358,\"start\":43352},{\"end\":43584,\"start\":43579},{\"end\":43598,\"start\":43593},{\"end\":43785,\"start\":43777},{\"end\":43799,\"start\":43793},{\"end\":43817,\"start\":43808},{\"end\":43841,\"start\":43828},{\"end\":44041,\"start\":44035},{\"end\":44057,\"start\":44052},{\"end\":44071,\"start\":44065},{\"end\":44084,\"start\":44077},{\"end\":44341,\"start\":44335},{\"end\":44355,\"start\":44350},{\"end\":44371,\"start\":44366},{\"end\":44384,\"start\":44377},{\"end\":44601,\"start\":44595},{\"end\":44612,\"start\":44607},{\"end\":44629,\"start\":44622},{\"end\":44636,\"start\":44634},{\"end\":44650,\"start\":44648},{\"end\":44671,\"start\":44659},{\"end\":44912,\"start\":44906},{\"end\":44926,\"start\":44922},{\"end\":44937,\"start\":44933},{\"end\":44953,\"start\":44947},{\"end\":45196,\"start\":45193},{\"end\":45210,\"start\":45206},{\"end\":45223,\"start\":45219},{\"end\":45236,\"start\":45233},{\"end\":45246,\"start\":45242},{\"end\":45259,\"start\":45256},{\"end\":45548,\"start\":45541},{\"end\":45559,\"start\":45555},{\"end\":45577,\"start\":45569},{\"end\":45593,\"start\":45587},{\"end\":45834,\"start\":45826},{\"end\":45846,\"start\":45840},{\"end\":45866,\"start\":45860},{\"end\":45883,\"start\":45876},{\"end\":45899,\"start\":45892},{\"end\":45910,\"start\":45901},{\"end\":46215,\"start\":46207},{\"end\":46232,\"start\":46223},{\"end\":46251,\"start\":46244},{\"end\":46271,\"start\":46262},{\"end\":46285,\"start\":46279},{\"end\":46615,\"start\":46607},{\"end\":46635,\"start\":46625},{\"end\":46653,\"start\":46644},{\"end\":46931,\"start\":46925},{\"end\":46950,\"start\":46940},{\"end\":46964,\"start\":46960},{\"end\":46979,\"start\":46974},{\"end\":46991,\"start\":46985},{\"end\":47014,\"start\":47004},{\"end\":47022,\"start\":47016},{\"end\":47302,\"start\":47294},{\"end\":47477,\"start\":47471},{\"end\":47493,\"start\":47483},{\"end\":47508,\"start\":47504},{\"end\":47522,\"start\":47515},{\"end\":47532,\"start\":47526},{\"end\":47555,\"start\":47545},{\"end\":47567,\"start\":47561},{\"end\":47571,\"start\":47569},{\"end\":47893,\"start\":47887},{\"end\":47903,\"start\":47897},{\"end\":47919,\"start\":47909},{\"end\":47936,\"start\":47926},{\"end\":47959,\"start\":47945},{\"end\":47977,\"start\":47969},{\"end\":47991,\"start\":47984},{\"end\":48015,\"start\":48004},{\"end\":48027,\"start\":48021},{\"end\":48031,\"start\":48029},{\"end\":48408,\"start\":48398},{\"end\":48417,\"start\":48413},{\"end\":48654,\"start\":48650},{\"end\":48667,\"start\":48664},{\"end\":48677,\"start\":48674},{\"end\":48697,\"start\":48689},{\"end\":48710,\"start\":48704},{\"end\":48724,\"start\":48720},{\"end\":48994,\"start\":48990},{\"end\":49009,\"start\":49005},{\"end\":49022,\"start\":49016},{\"end\":49032,\"start\":49026},{\"end\":49051,\"start\":49041},{\"end\":49068,\"start\":49064},{\"end\":49084,\"start\":49078},{\"end\":49105,\"start\":49091},{\"end\":49121,\"start\":49114},{\"end\":49133,\"start\":49123},{\"end\":49448,\"start\":49437},{\"end\":49470,\"start\":49459},{\"end\":49492,\"start\":49481},{\"end\":49515,\"start\":49503},{\"end\":49777,\"start\":49775},{\"end\":49789,\"start\":49787},{\"end\":49805,\"start\":49799},{\"end\":49813,\"start\":49811},{\"end\":49821,\"start\":49819},{\"end\":49838,\"start\":49830},{\"end\":50077,\"start\":50075},{\"end\":50088,\"start\":50086},{\"end\":50104,\"start\":50098},{\"end\":50121,\"start\":50113},{\"end\":50281,\"start\":50276},{\"end\":50297,\"start\":50290},{\"end\":50311,\"start\":50304},{\"end\":50327,\"start\":50321}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:2106.05264\",\"id\":\"b0\"},\"end\":35976,\"start\":35722},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":167217554},\"end\":36468,\"start\":35978},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":232352655},\"end\":36892,\"start\":36470},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":2375324},\"end\":37191,\"start\":36894},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":232270138},\"end\":37501,\"start\":37193},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":118642996},\"end\":37842,\"start\":37503},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":232379923},\"end\":38180,\"start\":37844},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":208909975},\"end\":38421,\"start\":38182},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":232478424},\"end\":38689,\"start\":38423},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":6722621},\"end\":38903,\"start\":38691},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":227255123},\"end\":39193,\"start\":38905},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":220686483},\"end\":39433,\"start\":39195},{\"attributes\":{\"id\":\"b12\"},\"end\":39661,\"start\":39435},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":54465161},\"end\":39995,\"start\":39663},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":207985696},\"end\":40340,\"start\":39997},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":219947110},\"end\":40879,\"start\":40342},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":213175590},\"end\":41283,\"start\":40881},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":234365467},\"end\":41853,\"start\":41285},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":203643676},\"end\":42145,\"start\":41855},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":158046789},\"end\":42475,\"start\":42147},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":233307004},\"end\":42883,\"start\":42477},{\"attributes\":{\"id\":\"b21\"},\"end\":43244,\"start\":42885},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":212646575},\"end\":43500,\"start\":43246},{\"attributes\":{\"id\":\"b23\"},\"end\":43717,\"start\":43502},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":227227965},\"end\":43993,\"start\":43719},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":218664573},\"end\":44284,\"start\":43995},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":84178815},\"end\":44552,\"start\":44286},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":227162149},\"end\":44820,\"start\":44554},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":232352619},\"end\":45128,\"start\":44822},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":4395124},\"end\":45470,\"start\":45130},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":220364071},\"end\":45748,\"start\":45472},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":219720931},\"end\":46112,\"start\":45750},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":235352518},\"end\":46506,\"start\":46114},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":174798113},\"end\":46839,\"start\":46508},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":227348246},\"end\":47232,\"start\":46841},{\"attributes\":{\"id\":\"b35\"},\"end\":47381,\"start\":47234},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":227254925},\"end\":47788,\"start\":47383},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":219791950},\"end\":48315,\"start\":47790},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":236975860},\"end\":48552,\"start\":48317},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":235490453},\"end\":48928,\"start\":48554},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":232045969},\"end\":49367,\"start\":48930},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":232168851},\"end\":49705,\"start\":49369},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":232352425},\"end\":50010,\"start\":49707},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":227254854},\"end\":50270,\"start\":50012},{\"attributes\":{\"doi\":\"arXiv:2010.07492\",\"id\":\"b44\"},\"end\":50547,\"start\":50272}]", "bib_title": "[{\"end\":36007,\"start\":35978},{\"end\":36548,\"start\":36470},{\"end\":36930,\"start\":36894},{\"end\":37243,\"start\":37193},{\"end\":37562,\"start\":37503},{\"end\":37902,\"start\":37844},{\"end\":38234,\"start\":38182},{\"end\":38494,\"start\":38423},{\"end\":38719,\"start\":38691},{\"end\":38968,\"start\":38905},{\"end\":39221,\"start\":39195},{\"end\":39727,\"start\":39663},{\"end\":40058,\"start\":39997},{\"end\":40430,\"start\":40342},{\"end\":40951,\"start\":40881},{\"end\":41382,\"start\":41285},{\"end\":41918,\"start\":41855},{\"end\":42213,\"start\":42147},{\"end\":42569,\"start\":42477},{\"end\":43278,\"start\":43246},{\"end\":43768,\"start\":43719},{\"end\":44026,\"start\":43995},{\"end\":44326,\"start\":44286},{\"end\":44586,\"start\":44554},{\"end\":44894,\"start\":44822},{\"end\":45184,\"start\":45130},{\"end\":45533,\"start\":45472},{\"end\":45816,\"start\":45750},{\"end\":46197,\"start\":46114},{\"end\":46597,\"start\":46508},{\"end\":46921,\"start\":46841},{\"end\":47461,\"start\":47383},{\"end\":47877,\"start\":47790},{\"end\":48391,\"start\":48317},{\"end\":48643,\"start\":48554},{\"end\":48979,\"start\":48930},{\"end\":49426,\"start\":49369},{\"end\":49768,\"start\":49707},{\"end\":50068,\"start\":50012}]", "bib_author": "[{\"end\":35797,\"start\":35777},{\"end\":35815,\"start\":35797},{\"end\":36023,\"start\":36009},{\"end\":36033,\"start\":36023},{\"end\":36045,\"start\":36033},{\"end\":36060,\"start\":36045},{\"end\":36074,\"start\":36060},{\"end\":36088,\"start\":36074},{\"end\":36569,\"start\":36550},{\"end\":36585,\"start\":36569},{\"end\":36601,\"start\":36585},{\"end\":36615,\"start\":36601},{\"end\":36639,\"start\":36615},{\"end\":36660,\"start\":36639},{\"end\":36947,\"start\":36932},{\"end\":36967,\"start\":36947},{\"end\":36977,\"start\":36967},{\"end\":36996,\"start\":36977},{\"end\":37004,\"start\":36996},{\"end\":37263,\"start\":37245},{\"end\":37279,\"start\":37263},{\"end\":37296,\"start\":37279},{\"end\":37311,\"start\":37296},{\"end\":37328,\"start\":37311},{\"end\":37577,\"start\":37564},{\"end\":37593,\"start\":37577},{\"end\":37608,\"start\":37593},{\"end\":37621,\"start\":37608},{\"end\":37632,\"start\":37621},{\"end\":37648,\"start\":37632},{\"end\":37660,\"start\":37648},{\"end\":37918,\"start\":37904},{\"end\":37928,\"start\":37918},{\"end\":37944,\"start\":37928},{\"end\":37967,\"start\":37944},{\"end\":37980,\"start\":37967},{\"end\":37989,\"start\":37980},{\"end\":38253,\"start\":38236},{\"end\":38262,\"start\":38253},{\"end\":38276,\"start\":38262},{\"end\":38286,\"start\":38276},{\"end\":38507,\"start\":38496},{\"end\":38523,\"start\":38507},{\"end\":38538,\"start\":38523},{\"end\":38730,\"start\":38721},{\"end\":38750,\"start\":38730},{\"end\":38758,\"start\":38750},{\"end\":38979,\"start\":38970},{\"end\":38988,\"start\":38979},{\"end\":39000,\"start\":38988},{\"end\":39015,\"start\":39000},{\"end\":39026,\"start\":39015},{\"end\":39236,\"start\":39223},{\"end\":39247,\"start\":39236},{\"end\":39270,\"start\":39247},{\"end\":39286,\"start\":39270},{\"end\":39296,\"start\":39286},{\"end\":39448,\"start\":39435},{\"end\":39459,\"start\":39448},{\"end\":39473,\"start\":39459},{\"end\":39745,\"start\":39729},{\"end\":39762,\"start\":39745},{\"end\":39780,\"start\":39762},{\"end\":39799,\"start\":39780},{\"end\":39815,\"start\":39799},{\"end\":40083,\"start\":40060},{\"end\":40092,\"start\":40083},{\"end\":40108,\"start\":40092},{\"end\":40120,\"start\":40108},{\"end\":40143,\"start\":40120},{\"end\":40153,\"start\":40143},{\"end\":40448,\"start\":40432},{\"end\":40458,\"start\":40448},{\"end\":40478,\"start\":40458},{\"end\":40504,\"start\":40478},{\"end\":40520,\"start\":40504},{\"end\":40537,\"start\":40520},{\"end\":40550,\"start\":40537},{\"end\":40555,\"start\":40550},{\"end\":40969,\"start\":40953},{\"end\":40979,\"start\":40969},{\"end\":40999,\"start\":40979},{\"end\":41018,\"start\":40999},{\"end\":41031,\"start\":41018},{\"end\":41048,\"start\":41031},{\"end\":41052,\"start\":41048},{\"end\":41397,\"start\":41384},{\"end\":41416,\"start\":41397},{\"end\":41432,\"start\":41416},{\"end\":41446,\"start\":41432},{\"end\":41455,\"start\":41446},{\"end\":41483,\"start\":41455},{\"end\":41502,\"start\":41483},{\"end\":41520,\"start\":41502},{\"end\":41533,\"start\":41520},{\"end\":41938,\"start\":41920},{\"end\":41954,\"start\":41938},{\"end\":41971,\"start\":41954},{\"end\":41987,\"start\":41971},{\"end\":42232,\"start\":42215},{\"end\":42248,\"start\":42232},{\"end\":42266,\"start\":42248},{\"end\":42281,\"start\":42266},{\"end\":42297,\"start\":42281},{\"end\":42588,\"start\":42571},{\"end\":42602,\"start\":42588},{\"end\":42618,\"start\":42602},{\"end\":42900,\"start\":42885},{\"end\":42915,\"start\":42900},{\"end\":42934,\"start\":42915},{\"end\":42950,\"start\":42934},{\"end\":42965,\"start\":42950},{\"end\":42981,\"start\":42965},{\"end\":43005,\"start\":42981},{\"end\":43294,\"start\":43280},{\"end\":43312,\"start\":43294},{\"end\":43328,\"start\":43312},{\"end\":43344,\"start\":43328},{\"end\":43360,\"start\":43344},{\"end\":43586,\"start\":43572},{\"end\":43600,\"start\":43586},{\"end\":43787,\"start\":43770},{\"end\":43801,\"start\":43787},{\"end\":43819,\"start\":43801},{\"end\":43843,\"start\":43819},{\"end\":44043,\"start\":44028},{\"end\":44059,\"start\":44043},{\"end\":44073,\"start\":44059},{\"end\":44086,\"start\":44073},{\"end\":44343,\"start\":44328},{\"end\":44357,\"start\":44343},{\"end\":44373,\"start\":44357},{\"end\":44386,\"start\":44373},{\"end\":44603,\"start\":44588},{\"end\":44614,\"start\":44603},{\"end\":44631,\"start\":44614},{\"end\":44638,\"start\":44631},{\"end\":44652,\"start\":44638},{\"end\":44673,\"start\":44652},{\"end\":44914,\"start\":44896},{\"end\":44928,\"start\":44914},{\"end\":44939,\"start\":44928},{\"end\":44955,\"start\":44939},{\"end\":45198,\"start\":45186},{\"end\":45212,\"start\":45198},{\"end\":45225,\"start\":45212},{\"end\":45238,\"start\":45225},{\"end\":45248,\"start\":45238},{\"end\":45261,\"start\":45248},{\"end\":45550,\"start\":45535},{\"end\":45561,\"start\":45550},{\"end\":45579,\"start\":45561},{\"end\":45595,\"start\":45579},{\"end\":45836,\"start\":45818},{\"end\":45848,\"start\":45836},{\"end\":45868,\"start\":45848},{\"end\":45885,\"start\":45868},{\"end\":45901,\"start\":45885},{\"end\":45912,\"start\":45901},{\"end\":46217,\"start\":46199},{\"end\":46234,\"start\":46217},{\"end\":46253,\"start\":46234},{\"end\":46273,\"start\":46253},{\"end\":46287,\"start\":46273},{\"end\":46617,\"start\":46599},{\"end\":46637,\"start\":46617},{\"end\":46655,\"start\":46637},{\"end\":46933,\"start\":46923},{\"end\":46952,\"start\":46933},{\"end\":46966,\"start\":46952},{\"end\":46981,\"start\":46966},{\"end\":46993,\"start\":46981},{\"end\":47016,\"start\":46993},{\"end\":47024,\"start\":47016},{\"end\":47304,\"start\":47292},{\"end\":47479,\"start\":47463},{\"end\":47495,\"start\":47479},{\"end\":47510,\"start\":47495},{\"end\":47524,\"start\":47510},{\"end\":47534,\"start\":47524},{\"end\":47557,\"start\":47534},{\"end\":47569,\"start\":47557},{\"end\":47573,\"start\":47569},{\"end\":47895,\"start\":47879},{\"end\":47905,\"start\":47895},{\"end\":47921,\"start\":47905},{\"end\":47938,\"start\":47921},{\"end\":47961,\"start\":47938},{\"end\":47979,\"start\":47961},{\"end\":47993,\"start\":47979},{\"end\":48017,\"start\":47993},{\"end\":48029,\"start\":48017},{\"end\":48033,\"start\":48029},{\"end\":48410,\"start\":48393},{\"end\":48419,\"start\":48410},{\"end\":48656,\"start\":48645},{\"end\":48669,\"start\":48656},{\"end\":48679,\"start\":48669},{\"end\":48699,\"start\":48679},{\"end\":48712,\"start\":48699},{\"end\":48726,\"start\":48712},{\"end\":48996,\"start\":48981},{\"end\":49011,\"start\":48996},{\"end\":49024,\"start\":49011},{\"end\":49034,\"start\":49024},{\"end\":49053,\"start\":49034},{\"end\":49070,\"start\":49053},{\"end\":49086,\"start\":49070},{\"end\":49107,\"start\":49086},{\"end\":49123,\"start\":49107},{\"end\":49135,\"start\":49123},{\"end\":49450,\"start\":49428},{\"end\":49472,\"start\":49450},{\"end\":49494,\"start\":49472},{\"end\":49517,\"start\":49494},{\"end\":49779,\"start\":49770},{\"end\":49791,\"start\":49779},{\"end\":49807,\"start\":49791},{\"end\":49815,\"start\":49807},{\"end\":49823,\"start\":49815},{\"end\":49840,\"start\":49823},{\"end\":50079,\"start\":50070},{\"end\":50090,\"start\":50079},{\"end\":50106,\"start\":50090},{\"end\":50123,\"start\":50106},{\"end\":50283,\"start\":50272},{\"end\":50299,\"start\":50283},{\"end\":50313,\"start\":50299},{\"end\":50329,\"start\":50313}]", "bib_venue": "[{\"end\":35775,\"start\":35722},{\"end\":36177,\"start\":36088},{\"end\":36664,\"start\":36660},{\"end\":37032,\"start\":37004},{\"end\":37338,\"start\":37328},{\"end\":37664,\"start\":37660},{\"end\":37993,\"start\":37989},{\"end\":38290,\"start\":38286},{\"end\":38548,\"start\":38538},{\"end\":38788,\"start\":38758},{\"end\":39030,\"start\":39026},{\"end\":39303,\"start\":39296},{\"end\":39538,\"start\":39473},{\"end\":39819,\"start\":39815},{\"end\":40160,\"start\":40153},{\"end\":40579,\"start\":40555},{\"end\":41056,\"start\":41052},{\"end\":41556,\"start\":41533},{\"end\":41991,\"start\":41987},{\"end\":42301,\"start\":42297},{\"end\":42668,\"start\":42618},{\"end\":43056,\"start\":43005},{\"end\":43364,\"start\":43360},{\"end\":43570,\"start\":43502},{\"end\":43847,\"start\":43843},{\"end\":44130,\"start\":44086},{\"end\":44409,\"start\":44386},{\"end\":44677,\"start\":44673},{\"end\":44959,\"start\":44955},{\"end\":45289,\"start\":45261},{\"end\":45602,\"start\":45595},{\"end\":45919,\"start\":45912},{\"end\":46300,\"start\":46287},{\"end\":46662,\"start\":46655},{\"end\":47028,\"start\":47024},{\"end\":47290,\"start\":47234},{\"end\":47577,\"start\":47573},{\"end\":48040,\"start\":48033},{\"end\":48423,\"start\":48419},{\"end\":48733,\"start\":48726},{\"end\":49139,\"start\":49135},{\"end\":49527,\"start\":49517},{\"end\":49850,\"start\":49840},{\"end\":50133,\"start\":50123},{\"end\":50399,\"start\":50345},{\"end\":36253,\"start\":36179},{\"end\":40590,\"start\":40581}]"}}}, "year": 2023, "month": 12, "day": 17}