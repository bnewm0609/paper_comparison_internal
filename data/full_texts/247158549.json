{"id": 247158549, "updated": "2023-10-05 16:33:37.401", "metadata": {"title": "A Systematic Evaluation of Large Language Models of Code", "authors": "[{\"first\":\"Frank\",\"last\":\"Xu\",\"middle\":[\"F.\"]},{\"first\":\"Uri\",\"last\":\"Alon\",\"middle\":[]},{\"first\":\"Graham\",\"last\":\"Neubig\",\"middle\":[]},{\"first\":\"Vincent\",\"last\":\"Hellendoorn\",\"middle\":[\"J.\"]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Large language models (LMs) of code have recently shown tremendous promise in completing code and synthesizing code from natural language descriptions. However, the current state-of-the-art code LMs (e.g., Codex (Chen et al., 2021)) are not publicly available, leaving many questions about their model and data design decisions. We aim to fill in some of these blanks through a systematic evaluation of the largest existing models: Codex, GPT-J, GPT-Neo, GPT-NeoX-20B, and CodeParrot, across various programming languages. Although Codex itself is not open-source, we find that existing open-source models do achieve close results in some programming languages, although targeted mainly for natural language modeling. We further identify an important missing piece in the form of a large open-source model trained exclusively on a multi-lingual corpus of code. We release a new model, PolyCoder, with 2.7B parameters based on the GPT-2 architecture, which was trained on 249GB of code across 12 programming languages on a single machine. In the C programming language, PolyCoder outperforms all models including Codex. Our trained models are open-source and publicly available at https://github.com/VHellendoorn/Code-LMs, which enables future research and application in this area.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2202.13169", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/pldi/Xu0NH22", "doi": "10.1145/3520312.3534862"}}, "content": {"source": {"pdf_hash": "b32a6f6ef7dd775e0f876b4713ceccebc56e651e", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2202.13169v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "976077ee55b120285076e68b3dd716276dcf015a", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/b32a6f6ef7dd775e0f876b4713ceccebc56e651e.txt", "contents": "\nA SYSTEMATIC EVALUATION OF LARGE LANGUAGE MODELS OF CODE\n\n\nFrank F Xu \nSchool of Computer Science\nCarnegie Mellon University\n\n\nUri Alon ualon@cs.cmu.edu \nSchool of Computer Science\nCarnegie Mellon University\n\n\nGraham Neubig gneubig@cs.cmu.edu \nSchool of Computer Science\nCarnegie Mellon University\n\n\nVincent J Hellendoorn vhellendoorn@cmu.edu \nSchool of Computer Science\nCarnegie Mellon University\n\n\nA SYSTEMATIC EVALUATION OF LARGE LANGUAGE MODELS OF CODE\nPublished as a workshop paper at DL4C @ ICLR 2022\nLarge language models (LMs) of code have recently shown tremendous promise in completing code and synthesizing code from natural language descriptions. However, the current state-of-the-art code LMs (e.g., Codex (Chen et al., 2021)) are not publicly available, leaving many questions about their model and data design decisions. We aim to fill in some of these blanks through a systematic evaluation of the largest existing models: Codex, GPT-J, GPT-Neo, GPT-NeoX-20B, and CodeParrot, across various programming languages. Although Codex itself is not open-source, we find that existing open-source models do achieve close results in some programming languages, although targeted mainly for natural language modeling. We further identify an important missing piece in the form of a large open-source model trained exclusively on a multi-lingual corpus of code. We release a new model, PolyCoder, with 2.7B parameters based on the GPT-2 architecture, that was trained on 249GB of code across 12 programming languages on a single machine. In the C programming language, PolyCoder outperforms all models including Codex. Our trained models are open-source and publicly available at https://github.com/VHellendoorn/Code-LMs, which enables future research and application in this area.Despite the great success of large language models of code, the strongest models are not publicly available. This prevents the application of these models outside of well-resourced companies and limits research in this field for low-resourced organizations. For example, Codex provides non-free access to the model's output through black-box API calls, 2 but the model's weights and training data are unavailable. This prevents researchers from fine-tuning and adapting this model to domains and tasks other than code completion. The lack of access to the model's internals also prevents the research community from studying other key aspects of these models, such as interpretability, distillation of the model for more efficient deployment, and incorporating additional components such as retrieval.Several medium to large-sized pre-trained language models are publicly available, such as GPT-\n\nINTRODUCTION\n\nLanguage models (LMs) assign probabilities to sequences of tokens, and are widely applied to natural language text (Bengio et al., 2003;Baevski & Auli, 2018;Brown et al., 2020). Recently, LMs have shown impressive performance in modeling also source code, written in programming languages (Hindle et al., 2016;Hellendoorn & Devanbu, 2017;Alon et al., 2020;Karampatsis et al., 2020). These models excel at useful downstream tasks like code completion (Raychev et al., 2014) and synthesizing code from natural language descriptions (Desai et al., 2016). The current state-of-the-art large language models for code, such as Austin et al. (2021), have shown significant progress for AI-based programming assistance. Most notably, one of the largest of these models, Codex (Chen et al., 2021) has been deployed in the real-world production tool GitHub Copilot 1 , as an in-IDE developer assistant that automatically generates code based on the user's context.\n\nDespite being trained on a mixture of a wide variety of text including news articles, online forums, and just a modest selection of (GitHub) software repositories (Gao et al., 2020), these language models can be used to generate source code with a reasonable performance Chen et al. (2021). In addition, there are a few open-source language models that are trained solely on source code. For example, CodeParrot (Tunstall et al., 2022) was trained on 180 GB of Python code.\n\nGiven the variety of model sizes and training schemes involved in these models and lack of comparisons between these, the impact of many modeling and training design decisions remains unclear. For instance, we do not know the precise selection of data on which Codex and other private models were trained; however, we do know that some public models (e.g., GPT-J) were trained on a mix of natural language and code in multiple programming languages, while other models (e.g., CodeParrot) were trained solely on code in one particular programming language. Multilingual models potentially provide better generalization, because different programming languages share similar keywords and properties, as shown by the success of multilingual models for natural language (Conneau & Lample, 2019) and for code (Z\u00fcgner et al., 2021). This may hint that multilingual LMs can generalize across languages, outperform monolingual models and be useful for modeling low-resource programming languages, but this is yet to be verified empirically.\n\nIn this paper, we present a systematic evaluation of existing models of code -Codex, GPT-J, GPT-Neo, GPT-NeoX, and CodeParrot -across various programming languages. We aim to shed more light on the landscape of code modeling design decisions by comparing and contrasting these models, as well as providing a key missing link: thus far, no large open-source language model was trained exclusively on code from multiple programming languages. We provide three such models, ranging from 160M to 2.7B parameters, which we release under the umbrella name \"PolyCoder\". First, we perform an extensive comparison of the training and evaluation settings between PolyCoder, open-source models, and Codex. Second, we evaluate the models on the HumanEval benchmark (Chen et al., 2021) and compare how do models of different sizes and training steps scale, and how different temperatures affect the generation quality. Finally, since HumanEval only evaluates the natural language to Python synthesis, we curate an unseen evaluation dataset 3 in each of the 12 languages, to evaluate the perplexity of different models. We find that although Codex is allegedly focused on Python (Chen et al. (2021) \u00a73.1), Codex performs surprisingly well in other programming languages too, and even better than GPT-J and GPT-NeoX that were trained on the Pile (Gao et al., 2020). Nonetheless, in the C programming language, our PolyCoder model achieves a lower perplexity than all these models, including Codex.\n\nAlthough most current models perform worse than Codex, we hope that this systematic study helps future research in this area to design more efficient and effective models. More importantly, through this systematic evaluation of different models, we encourage the community to study and release medium-large scale language models for code, in response to the concerns expressed by Hellendoorn & Sawant (2021):\n\n[...] this exploding trend in cost to achieve the state of the art has left the ability to train and test such models limited to a select few large technology companies-and way beyond the resources of virtually all academic labs.\n\nWe believe that our efforts are a significant step towards democratization of large language models of code.\n\n\nRELATED WORK\n\nAt the core of code modeling lies ongoing work on pretraining of language models (LMs). Large-scale pretraining of LMs has had an astounding impact on natural language processing in recent years (Han et al., 2021). Figure 1 provides an overview of how different models compare in size and availability.\n\n\nPRETRAINING METHODS\n\nWe discuss three popular pretraining methods used in code language modeling. An illustration of these methods are shown in Figure 2.  Left-to-Right Language Models (Figure 2, left) Auto-regressive, Left-to-right LMs, predict the probability of a token given the previous tokens. In code modeling, CodeGPT (124M) (Lu et al., 2021), CodeParrot (1.5B) (Tunstall et al., 2022), GPT-Neo (2.7B) (Black et al., 2021), GPT-J (6B) (Wang & Komatsuzaki, 2021), Codex (12B) (Chen et al., 2021), GPT-NeoX (20B) (Black et al., 2022), and Google's (137B) (Austin et al., 2021) belong to this category. The left-to-right nature of these models makes them highly useful for program generation tasks, such as code completion. On the other hand, as code is usually not written in a single, left-to-write pass, it is not trivial to leverage context that appears \"after\" the location of the generation. In this paper, we focus on this family of models and will discuss the existing models in more detail in the following sections.\n\nMasked Language Models (Figure 2, middle) While auto-regressive language models are powerful for modeling the probability of sequences, their unidirectional nature makes them less suitable for producing effective whole-sequence representations for downstream tasks such as classification.\n\nOne popular bidirectional objective function used widely in representation learning is masked language modeling (Devlin et al., 2018), where the aim is to predict masked text pieces based on surrounding context. CodeBERT (125M) (Feng et al., 2020) and CuBERT (345M) (Kanade et al., 2020) are examples of such models in code. In programming contexts, these methods provide useful representations of a sequence of code for downstream tasks such as code classification, clone detection, and defect detection.\n\nEncoder-decoder Models (Figure 2, right) An encoder-decoder model first uses an encoder to encode an input sequence, and then uses a left-to-right LM to decode an output sequence conditioned on the input sequence. Popular pretraining objectives include masked span prediction (Raffel et al., 2019) where the input sequence is randomly masked with multiple masks and the output sequence are the masked contents in order, and denoising sequence reconstruction (Lewis et al., 2019) where the input is a corrupted sequence and the output is the original sequence. These pretrained models are useful in many sequence-to-sequence tasks (Raffel et al., 2019). In code, CodeT5 (220M) , and PLBART (406M) (Ahmad et al., 2021) use the two objectives mentioned above respectively, and performs well in conditional generation downstream tasks such as code commenting, or natural language to code generation.\n\n\nPRETRAINING DATA\n\nSome models (e.g. CodeParrot and CodeT5) are trained on GitHub code only, with corpora extracted using either Google BigQuery's GitHub dataset 4 , or CodeSearchNet (Husain et al., 2019). Others (e.g., GPT-Neo and GPT-J) are trained on \"the Pile\" (Gao et al., 2020), a large corpus containing a blend of natural language texts and code from various domains, including Stack Exchange dumps, software documentations, and popular (>100 stars) GitHub repositories. The datasets on which other proprietary models (Codex, Google's) were trained on are unknown. One goal of our study is to try to shed light on what corpora might be the most useful for pretraining models of code.\n\n\nEVALUATION SETTINGS\n\nWe evaluate all models using both extrinsic and intrinsic benchmarks, as described below.\n\nExtrinsic Evaluation One of the most popular downstream tasks for code modeling is code generation given a natural language description. Following Chen et al. (2021), we evaluate all models on the HumanEval dataset. The dataset contains 164 prompts with descriptions in the form of code comments and function definitions, including argument names and function names, and test cases to judge whether the generated code is correct. To generate code given a prompt, we use the same sampling strategy as Chen et al. (2021), using softmax with a temperature parameter softmax(x/T ). We evaluate using a wide range of temperatures T = [0.2, 0.4, 0.6, 0.8] to control for the confidence of the model's predictions. Similarly to Codex, we use nucleus sampling (Holtzman et al., 2019) with top-p = 0.95. We sample tokens from the model until we encounter one of the following stop sequences that indicate the end of a method: 5 '\\nclass', '\\ndef', '\\n#', '\\nif', or '\\nprint'. We randomly sample 100 examples per prompt in the evaluation dataset.\n\nIntrinsic Evaluation To evaluate the intrinsic performance of different models, we compute the perplexity for each language on an unseen set of GitHub repositories. To prevent training-to-test data leakage for models such as GPT-Neo and GPT-J, we remove repositories in our evaluation dataset that appeared in the GitHub portion of the Pile training dataset 6 . To evaluate Codex, we use OpenAI's API 7 , choosing the code-davinci-001 engine. We note that the data that this model was trained on is unknown, so we cannot prevent data leakage from the training to the test set for Codex. We sampled 100 random files for each of the 12 programming languages in our evaluation dataset. To make perplexity comparable across different tokenization methods used in different models, we use Pygments 8 to equally normalize the log-likelihood sum of each model, when computing perplexity. 9 4 COMPARED MODELS 4.1 EXISTING MODELS As discussed in Section 2, we mainly focus on auto-regressive left-to-right pretrained language models, most suitable for code completion tasks.\n\nWe evaluate Codex, as it is currently deployed in real-world and has impressive performance in code completion (Chen et al., 2021). Codex uses the GPT-3 language model (Brown et al., 2020) as its underlying model architecture. Codex was trained on a dataset spanning 179GB (after deduplication) covering over 54 million public Python repositories obtained from GitHub on May 2020. As reflected in its impressive results in other programming languages than Python, we suspect that Codex was also trained on large corpora of additional programming languages. The model available for querying through a non-free API.\n\nAs for open-source models, we compare GPT-Neo, GPT-J and GPT-NeoX, the largest variants having 2.7, 6 and 20 billion parameters, respectively. GPT-NeoX is the largest open-source pretrained language models available. These models are trained on the Pile dataset, so they are a good representatives of models that were trained on both natural language texts from various domains and source code from GitHub. We also compare CodeParrot with at most 1.5 billion parameters, a model that was only trained on Python code from GitHub. CodeParrot follows the process used in Chen et al. (2021) that obtained over 20M files Python files from Google BigQuery Github database, resulting in a 180GB dataset, which is comparable to Codex's Python training data, but the model itself is much smaller.\n\nThere was no large open-source language model trained almost exclusively on code from multiple programming languages. To fill this gap, we train a 2.7 billion model, PolyCoder, on a mixture of repositories from GitHub in 12 different programming languages. \n\n\nPOLYCODER'S DATA\n\n\nRaw Code Corpus Collection\n\nGitHub is an excellent source for publicly available source code of various programming languages. We cloned the most popular repositories for 12 popular programming languages with at least 50 stars (stopping at about 25K per language to avoid a too heavy skew towards popular programming languages) from GitHub in October 2021. For each project, each file belonging to the majority-language of that project was extracted, yielding the initial training set. This initial, unfiltered dataset spanned 631GB and 38.9M files.\n\n\nData Preprocessing\n\nThe detailed data preprocessing strategy comparison with other models are analyzed in Table 2. In general, we tried to follow Codex's design decisions, although there is a fair bit of ambiguity in the description of its data preprocessing.\n\nDeduplication and Filtering Similarly to Codex and CodeParrot, very large (>1MB) and very short (<100 tokens) files were filtered out, reducing the size of the dataset by 33%, from 631GB to 424GB. This only reduced the total number of files by 8%, showing that a small number of files were responsible for a large part of the corpus. 10 Allamanis (2019) has shown that code duplication that commonly manifests in datasets of code adversely effects language modeling of code. Therefore, we deduplicated files based on a hash of their content, which reduced the number of files by nearly 30%, and the dataset size by additional 29%, leaving 24.1M files and 254GB of data.\n\nOverall, the filtering of very large and very short files plus deduplication, reduced the number of files by 38%, and the dataset size by 61%, roughly on par with the 70% dataset size reduction reported by CodeParrot. A key difference that remains is that other approaches use more fine-grained filtering 10 Codex additionally mentions removing \"auto-generated\" files, but the definition of this was not clear, so we omitted this step.  The dataset statistics are shown in Table 1, showcasing data sizes per language before and after filtering. Our dataset contains less Python code (only 16G) than Codex or CodeParrot, and instead covers many different programming languages.\n\nTokenizer We train a GPT-2 tokenizer (using BPE (Sennrich et al., 2015)) on a random 5% subset of all the pretraining data, containing all the languages. Codex uses an existing trained GPT-3 tokenizer, with the addition of multi-whitespace tokens to reduce the sequence length after tokenization, as consecutive whitespaces are more common in code than in text.\n\n\nPOLYCODER'S TRAINING\n\nConsidering our budget, we chose the GPT-2 (Radford et al., 2019) as our model architecture. To study the effect of scaling of model size, we train 3 different sized models, with 2.7 billion, 400 million and 160 million parameters, as the largest 2.7B model being on par with GPT-Neo for fair comparison. The 2.7 billion model is a 32 layer, 2,560 dimensional Transformer model, with a max context window of 2048 tokens, trained with a batch size of 128 sequences (262K tokens). The model is trained for 150K steps. The 400 million model is a 24 layer, 1,024 dimensional variant, and the 160 million model is a 12 layer, 768 dimensional variant, otherwise idem. We use GPT-NeoX toolkit 11 to train the model efficiently in parallel with 8 Nvidia RTX 8000 GPUs on a single machine. The wall time used to train the largest 2.7B model is about 6 weeks. In its default configuration, this model should train for 320K steps, which was not feasible with our resources. Instead, we adjusted the learning rate decay to half this number and trained for up to 150K steps (near-convergence). The training and validation loss curves for different sized models are shown in Figure 3. We see that even after training for 150K steps, the validation losses are still decreasing. This, combined with the shorter training schedule and faster learning rate decay, strongly signals that the models are still under-fitting and could benefit from longer training.\n\nWe compare the training setting and hyperparameters with CodeParrot and Codex in Table 3. Due to high computational costs, we were unable to perform hyperparameter search. Most hyperparameters are the same as those used in their respective GPT-2 model training 12 to provide a good default with regards to the corresponding model size. Some key differences include context window sizes to allow for more tokens as context, batch sizes and tokens trained, as well as model initialization with or without natural language knowledge.   \n\n\nRESULTS\n\n\nEXTRINSIC EVALUATION\n\nThe overall results are shown in Table 4. 13 The numbers are obtained by sampling with different temperatures and picking the best value for each metric. Among existing models, PolyCoder is worse than similarly sized GPT-Neo and the even smaller Codex 300M. Overall, PolyCoder lies after Codex, GPT-Neo/J, while performing stronger than CodeParrot. PolyCoder, which was trained only on code, falls behind a similar sized model (GPT-Neo 2.7B) trained on the Pile, a blend of natural language texts and code. Looking at the rightmost columns in Table 4 offers a potential explanation: in terms of total Python tokens seen during training, all models substantially exceed ours. This in partly because they use a higher proportion of Python code (we aimed to balance data volume across programming languages), and in part because of resource limitations, which lead to PolyCoder not observing its entire training data. In addition, the natural language blend in the training corpus may help code language modeling as well, especially with code-related texts such as Stack Exchange dumps being included.\n\nCompared to GPT-Neo (2.7B), PolyCoder has seen fewer Python tokens, but more code tokens in other programming languages, hinting that transfer from other languages to Python helps to achieve a similar performance. This suggests that future research could benefit from blending code in different programming languages, as well as natural language text.  Table 4: Results of different models on the HumanEval benchmark, and the number of different types of tokens seen during the training process. Scaling Effect To further understand the effect of the number of model parameters with respect to HumanEval code completion performance, we show the Pass@1, Pass@10 and Pass@100 percentage with respect the the model size in Figure 4. We can see that the performance of the Codex models are significantly better than all the other open-source models across all numbers of parameters. The performance on HumanEval benchmark increases linearly with the magnitude (log scale) of the number of parameters in the model. Similar scaling effects could be found on PolyCoder and GPT-Neo/J models. Interestingly, the CodeParrot models that are trained only on Python seem to have reached a saturating performance with respect to increasing number of parameters, where the training corpus being focused on Python may have some effect. With higher number of parameters (2.7B), PolyCoder's performance is trending worse than that of GPT-Neo/J. Comparing GPT-Neo/J that is trained on Pile dataset containing a blend of text, Stack Exchange dumps and GitHub data, with PolyCoder that are trained on only GitHub repositories of popular programming languages, we hypothesize that the added text, especially texts in technical and software engineering domains, may be crucial for the larger model to boost the performance. We also compare the performance difference between the model trained after 100K steps versus the model after 150K steps in Appendix A, and find that training for longer helps the larger model more as it is still under-fitted.\n\nTemperature Effect All the above results are obtained by sampling the language model with different temperatures and picking the best value for each metric. We are also interested in how different choices of temperature affects the final generation quality. We summarize the results in Figure 5. The general trend is for Pass@1, lower temperatures are better, and for Pass@100, a higher temperature will help, while for Pass@10 a temperature in the middle is better suited. We hypothesize that this is because a higher temperature during generation makes the model less confident in its predictions and thus allow for more exploration and more diverse outputs, resulting in better accuracy at Pass@100. Too high a temperature (0.8) is also hurtful if the model is capable enough. * Since the exact training set of Codex is unknown, it may include files from these test sets rendering Codex's results overly-optimistic. Figure 6: Perplexity comparison on our evaluation dataset of different models on different programming languages. Note that the y-axis is capped at 4; CodeParrot's entropy on all languages other than Python is much higher than shown here (see Table 5). On the contrary, a lower temperature makes the model output very confident in its prediction and thus will be better suited for generating very few correct examples, and thus the better performance for Pass@1. In Appendix B we repeat these experiments with the smaller models as well. This suggests the importance of temperature and the need to tune it individually for different generation scenarios.\n\n\nINTRINSIC EVALUATION\n\nThe perplexity results on the evaluation datasets are shown in Figure 6, with detailed numbers in Appendix C. The plot caps the perplexity score to 4 as CodeParrot performs poorly in languages other than Python. It is important to note that although Codex's perplexities are lower than other models in most languages, Codex might have been trained on the test sets, and its results are thus over-optimistic.\n\nNotably, PolyCoder outperforms Codex and all other models in the C language. Comparing the open-source models only, PolyCoder performs better than the similarly sized GPT-Neo 2.7B in C, JavaScript, Rust, Scala and TypeScript.\n\nIn the other 11 languages other than C, all other open-source models, including ours, are significantly worse (higher perplexity) than Codex. We hypothesize that this is due to the fact that PolyCoder is trained on an imbalanced mixture of different languages, with C and C++ being closely related and the two most dominant in the entire training corpus (Section 4.2). Thus, the larger volume in total (because of long files) makes C the most \"favored\" language by PolyCoder. The reason why PolyCoder does not outperform Codex in C++ is possibly due to the complexity of C++ language and Codex's significantly longer context window size (4096, compared to PolyCoder's 2048), or because Codex is possibly trained on more C++ training data.\n\nWith the same pretraining corpus, the gain from a 2.7B model (GPT-Neo) to a 6B model (GPT-J) is significant over all languages. However, when increasing the model size further to 20B, the improvement varies across different languages. For example, the performance on Go, Java, Rust, Scala, TypeScript do not increase significantly when the model size increases by 3 times. This suggests that for some programming languages, and given the amounts of data, the capacity of GPT-J is sufficient. Interestingly, these languages seem to coincide with languages where PolyCoder outperforms a similarly sized model trained on Pile. This may hint that for the languages in which larger models do not provide additional gains, training the model only using code may be enough or slightly more helpful than training on both natural language and code.\n\nWe can see that comparing different models, perplexity trends for Python correlates well with the HumanEval benchmark performance of the extrinsic evaluation (Section 5.1). This suggests that perplexity is a useful and low-cost metric to estimate other, downstream, metrics.\n\n\nCONCLUSION\n\nIn this paper, we perform a systematic evaluation of large language models for code. The performance generally benefits from larger models and longer training time. We also believe that the better results of GPT-Neo over PolyCoder in some languages show that training on natural language text and code can benefit the modeling of code. To help future research in the area, we release PolyCoder, a large open-source language model for code, trained exclusively on code in 12 different programming languages. In the C programming language, PolyCoder achieves lower perplexity than all models including Codex.\n\nWe compare the performance difference between the model trained after 100K steps versus the model after 150K steps in Figure 7. We can see that in the larger 2.7B model, by training the model longer till 150K steps, the performance increases uniformly, with Pass@100 increasing the most. However, for a smaller model such as the 400M model, by training the model longer till 100K steps, the improvements are subdued and Pass@100 drops. This suggests that with the larger model, training for longer may provide additional boost in performance. This echoes with the observation from the training curve (Figure 3) as well. \n\n\nB TEMPERATURE EFFECT: SMALLER MODELS\n\nWe show how temperature affects HumanEval performance on model of all three sizes in Figure 8. We find that for a larger model, e.g., the 2.7B model, a temperature as high as 0.8 is actually hurting the performance for Pass@100, suggesting that if the model is good enough, a very high temperature may cause the outputs to be too diverse, thus hurting the correctness. This suggests the importance of temperature and the need to tune it individually for different model capacity and different generation scenarios.\n\n\nC DETAILED PERPLEXITY RESULTS\n\nWe show the detailed perplexity of different models on different languages in Table 5. The number of tokens shown in the table is obtained after tokenizing the code in each language using their respective lexers, by Pygments. This number of tokens is used to normalize the perplexity scores to make them comparable across models. Note that CodeParrot is only trained on Python data and thus performs poorly in other languages.   \n\nFigure 2 :\n2Three types of pretrained language models.\n\nFigure 3 :\n3Training and validation loss during the 150K step training process.\n\nFigure 4 :\n4The scaling effect of HumanEval performance on different models.\n\nFigure 5 :\n5HumanEval performance with different softmax temperatures during generation.\n\nFigure 7 :\n7HumanEval performance comparison after training the model for longer.\n\nFigure 8 :\n8HumanEval performance using different softmax temperatures during generation.\n\nTable 2 :\n2Comparison of data preprocessing strategies of different models.strategies, such as limiting the maximum line length or average line length, filtering of probable \nauto-generated files, etc. For example, Chen et al. (2021) have filtered only 11% of their training data. \n\n\n\nTable 3 :\n3Comparison of design decisions and hyper-parameters in training different models of code.0 \n25 \n50 \n75 100 125 150 \n\u00d71000 steps \n\n0.50 \n\n0.75 \n\n1.00 \n\n1.25 \n\n1.50 \n\nTraining Loss \n\n2.7B \n400M \n160M \n\n(a) Training \n\n0 \n25 \n50 \n75 100 125 150 \n\u00d71000 steps \n\n0.50 \n\n0.75 \n\n1.00 \n\n1.25 \n\n1.50 \n\nValidation Loss \n\n2.7B \n400M \n160M \n\n(b) Validation \n\n\n\n\nLanguage#tokens Codex* PolyCoder 2.7B GPT-Neo 2.7B GPT-J 6B GPT-NeoX CodeParrot Since the exact training set of Codex is unknown, it might have been trained on these test sets, and Codex's results are over-optimistic.C \n55,333 \n2.55 \n2.33 \n3.69 \n2.82 \n2.37 \n19.23 \nC# \n67,306 \n1.72 \n2.58 \n2.49 \n2.20 \n2.12 \n7.16 \nC++ \n69,627 \n1.95 \n2.99 \n2.87 \n2.47 \n2.32 \n8.48 \nGo \n79,947 \n1.39 \n2.57 \n2.19 \n1.89 \n1.85 \n10.00 \nJava \n65,484 \n1.94 \n2.92 \n2.78 \n2.49 \n2.47 \n6.79 \nJavaScript \n54,620 \n2.17 \n3.06 \n3.07 \n2.73 \n2.62 \n9.23 \nPHP \n45,682 \n1.98 \n3.70 \n3.61 \n2.81 \n2.45 \n19.91 \nPython \n79,653 \n1.47 \n3.18 \n3.00 \n2.68 \n2.61 \n2.95 \nRuby \n46,537 \n1.39 \n3.96 \n3.77 \n3.13 \n2.89 \n14.26 \nRust \n107,717 \n1.96 \n3.24 \n3.30 \n2.92 \n2.92 \n8.68 \nScala \n65,756 \n1.75 \n3.87 \n3.88 \n3.37 \n3.33 \n12.91 \nTypeScript \n55,895 \n2.40 \n3.61 \n3.90 \n3.43 \n3.41 \n12.54 \n* \n\nTable 5 :\n5Perplexity of different models for different programming languages on our evaluation dataset.\nThe exact training set that Codex was trained on is unknown.\nhttps://cloud.google.com/blog/topics/public-datasets/github-on-bigquery-analyze-allthe-open-source-code 5 The absence of whitespace, which is significant in Python, signals an exit from the method body. 6 https://github.com/EleutherAI/github-downloader 7 https://beta.openai.com/docs/engines/codex-series-private-beta 8 https://pygments.org/docs/lexers/ 9 Every model uses its original tokenizer for predicting the next token. We use the shared tokenizer only for computing the perplexity given the log-likelihood sum.\nhttps://github.com/EleutherAI/gpt-neox 12 https://github.com/EleutherAI/gpt-neox/tree/main/configs 13 Due to the large model size of GPT-NeoX (20B) and limited computational budget, we did not include it in the HumanEval experiment.\n\nUnified pre-training for program understanding and generation. Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, Kai-Wei Chang, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnlineAssociation for Computational LinguisticsWasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. Unified pre-training for program understanding and generation. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, pp. 2655-2668, Online, June 2021. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/2021.naacl-main.211.\n\nThe adverse effects of code duplication in machine learning models of code. Miltiadis Allamanis, Proceedings of the 2019 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software. the 2019 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and SoftwareMiltiadis Allamanis. The adverse effects of code duplication in machine learning models of code. In Proceedings of the 2019 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software, pp. 143-153, 2019.\n\nStructural language models of code. Uri Alon, Roy Sadaka, Omer Levy, Eran Yahav, International Conference on Machine Learning. PMLRUri Alon, Roy Sadaka, Omer Levy, and Eran Yahav. Structural language models of code. In International Conference on Machine Learning, pp. 245-256. PMLR, 2020.\n\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, arXiv:2108.07732Program synthesis with large language models. arXiv preprintJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.\n\nAlexei Baevski, Michael Auli, arXiv:1809.10853Adaptive input representations for neural language modeling. arXiv preprintAlexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. arXiv preprint arXiv:1809.10853, 2018.\n\nA neural probabilistic language model. Yoshua Bengio, R\u00e9jean Ducharme, Pascal Vincent, Christian Jauvin, Journal of machine learning research. 3Yoshua Bengio, R\u00e9jean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic language model. Journal of machine learning research, 3(Feb):1137-1155, 2003.\n\nSid Black, Leo Gao, Phil Wang, Connor Leahy, Stella Biderman, Gpt-Neo, 10.5281/zenodo.5297715Large Scale Autoregressive Language Modeling with Mesh-Tensorflow. If you use this software, please cite it using these metadataSid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, March 2021. URL https://doi.or g/10.5281/zenodo.5297715. If you use this software, please cite it using these metadata.\n\nSid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle Mcdonell, Jason Phang, Michael Pieler, Shivanshu Usvsn Sai Prashanth, Laria Purohit, Reynolds, GPT-NeoX-20B: An open-source autoregressive language model. Jonathan Tow, Ben Wang, and Samuel WeinbachSid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. GPT-NeoX-20B: An open-source autoregressive language model. 2022.\n\nLanguage models are few-shot learners. Benjamin Tom B Brown, Nick Mann, Melanie Ryder, Jared Subbiah, Prafulla Kaplan, Arvind Dhariwal, Pranav Neelakantan, Girish Shyam, Amanda Sastry, Askell, arXiv:2005.14165arXiv preprintTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.\n\n. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harri Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374arXiv preprintet al. Evaluating large language models trained on codeMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harri Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.\n\nCross-lingual language model pretraining. Alexis Conneau, Guillaume Lample, Advances in Neural Information Processing Systems. 32Alexis Conneau and Guillaume Lample. Cross-lingual language model pretraining. Advances in Neural Information Processing Systems, 32:7059-7069, 2019.\n\nProgram synthesis using natural language. Aditya Desai, Sumit Gulwani, Vineet Hingorani, Nidhi Jain, Amey Karkare, Mark Marron, Subhajit Roy, Proceedings of the 38th International Conference on Software Engineering. the 38th International Conference on Software EngineeringAditya Desai, Sumit Gulwani, Vineet Hingorani, Nidhi Jain, Amey Karkare, Mark Marron, and Subhajit Roy. Program synthesis using natural language. In Proceedings of the 38th International Conference on Software Engineering, pp. 345-356, 2016.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova Bert, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. arXiv preprintJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, arXiv:2002.08155A pre-trained model for programming and natural languages. arXiv preprintZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al. Codebert: A pre-trained model for programming and natural languages. arXiv preprint arXiv:2002.08155, 2020.\n\nThe pile: An 800gb dataset of diverse text for language modeling. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, arXiv:2101.00027arXiv preprintLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.\n\nPre-trained models: Past, present and future. Xu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, Xiao Liu, Yuqi Huo, Jiezhong Qiu, Liang Zhang, Wentao Han, Minlie Huang, AI Open2021Xu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, Xiao Liu, Yuqi Huo, Jiezhong Qiu, Liang Zhang, Wentao Han, Minlie Huang, et al. Pre-trained models: Past, present and future. AI Open, 2021.\n\nAre deep neural networks the best choice for modeling source code?. J Vincent, Premkumar Hellendoorn, Devanbu, Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering. the 2017 11th Joint Meeting on Foundations of Software EngineeringVincent J Hellendoorn and Premkumar Devanbu. Are deep neural networks the best choice for modeling source code? In Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering, pp. 763-773, 2017.\n\nThe growing cost of deep learning for source code. J Vincent, Anand Ashok Hellendoorn, Sawant, 10.1145/3501261Commun. ACM. 651Vincent J. Hellendoorn and Anand Ashok Sawant. The growing cost of deep learning for source code. Commun. ACM, 65(1):31-33, dec 2021. ISSN 0001-0782. doi: 10.1145/3501261. URL https://doi.org/10.1145/3501261.\n\nOn the naturalness of software. Abram Hindle, T Earl, Mark Barr, Zhendong Gabel, Premkumar Su, Devanbu, Communications of the ACM. 595Abram Hindle, Earl T Barr, Mark Gabel, Zhendong Su, and Premkumar Devanbu. On the naturalness of software. Communications of the ACM, 59(5):122-131, 2016.\n\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, Yejin Choi, arXiv:1904.09751The curious case of neural text degeneration. arXiv preprintAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. arXiv preprint arXiv:1904.09751, 2019.\n\nCodesearchnet challenge: Evaluating the state of semantic code search. Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, Marc Brockschmidt, arXiv:1909.09436arXiv preprintHamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. Code- searchnet challenge: Evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436, 2019.\n\nLearning and evaluating contextual embedding of source code. Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, Kensen Shi, International Conference on Machine Learning. PMLRAditya Kanade, Petros Maniatis, Gogul Balakrishnan, and Kensen Shi. Learning and evaluating contextual embedding of source code. In International Conference on Machine Learning, pp. 5110-5121. PMLR, 2020.\n\nBig code!= big vocabulary: Open-vocabulary models for source code. Rafael-Michael Karampatsis, Hlib Babii, Romain Robbes, Charles Sutton, Andrea Janes, 2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE). IEEERafael-Michael Karampatsis, Hlib Babii, Romain Robbes, Charles Sutton, and Andrea Janes. Big code!= big vocabulary: Open-vocabulary models for source code. In 2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE), pp. 1073-1085. IEEE, 2020.\n\nBart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer, arXiv:1910.13461arXiv preprintMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019.\n\nCodeXGLUE: A machine learning benchmark dataset for code understanding and generation. Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shengyu Shao Kun Deng, Shujie Fu, Liu, Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2021Round 1Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, MING GONG, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu, and Shujie LIU. CodeXGLUE: A machine learning benchmark dataset for code understanding and generation. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021. URL https://openreview.net/forum?id= 6lE4dQXaUcb.\n\nLanguage models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 189Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n\nExploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, arXiv:1910.10683arXiv preprintColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019.\n\nCode completion with statistical language models. Veselin Raychev, Martin Vechev, Eran Yahav, Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation. the 35th ACM SIGPLAN Conference on Programming Language Design and ImplementationVeselin Raychev, Martin Vechev, and Eran Yahav. Code completion with statistical language models. In Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation, pp. 419-428, 2014.\n\nRico Sennrich, Barry Haddow, Alexandra Birch, arXiv:1508.07909Neural machine translation of rare words with subword units. arXiv preprintRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015.\n\nNatural Language Processing with Transformers. Lewis Tunstall, Thomas Leandro Von Werra, Wolf, O'Reilly Media, Inc2022Lewis Tunstall, Leandro von Werra, and Thomas Wolf. Natural Language Processing with Trans- formers. \" O'Reilly Media, Inc.\", 2022.\n\nBen Wang, Aran Komatsuzaki, Gpt-J-6b, A 6 Billion Parameter Autoregressive Language Model. Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax, May 2021.\n\nCodet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. Yue Wang, Weishi Wang, Shafiq Joty, C H Steven, Hoi, arXiv:2109.00859arXiv preprintYue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. arXiv preprint arXiv:2109.00859, 2021.\n\nLanguage-agnostic representation learning of source code from structure and context. Daniel Z\u00fcgner, Tobias Kirschstein, Michele Catasta, Jure Leskovec, Stephan G\u00fcnnemann, International Conference on Learning Representations. Daniel Z\u00fcgner, Tobias Kirschstein, Michele Catasta, Jure Leskovec, and Stephan G\u00fcnnemann. Language-agnostic representation learning of source code from structure and context. In Inter- national Conference on Learning Representations, 2021. URL https://openreview.net/for um?id=Xh5eMZVONGF.\n", "annotations": {"author": "[{\"end\":127,\"start\":60},{\"end\":210,\"start\":128},{\"end\":300,\"start\":211},{\"end\":400,\"start\":301}]", "publisher": null, "author_last_name": "[{\"end\":70,\"start\":68},{\"end\":136,\"start\":132},{\"end\":224,\"start\":218},{\"end\":322,\"start\":311}]", "author_first_name": "[{\"end\":65,\"start\":60},{\"end\":67,\"start\":66},{\"end\":131,\"start\":128},{\"end\":217,\"start\":211},{\"end\":308,\"start\":301},{\"end\":310,\"start\":309}]", "author_affiliation": "[{\"end\":126,\"start\":72},{\"end\":209,\"start\":155},{\"end\":299,\"start\":245},{\"end\":399,\"start\":345}]", "title": "[{\"end\":57,\"start\":1},{\"end\":457,\"start\":401}]", "venue": null, "abstract": "[{\"end\":2683,\"start\":508}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2835,\"start\":2814},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2856,\"start\":2835},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2875,\"start\":2856},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3009,\"start\":2988},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3037,\"start\":3009},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3055,\"start\":3037},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3080,\"start\":3055},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3171,\"start\":3149},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3249,\"start\":3229},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3340,\"start\":3320},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3486,\"start\":3467},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3836,\"start\":3818},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3944,\"start\":3926},{\"end\":4090,\"start\":4067},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4920,\"start\":4896},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":4955,\"start\":4934},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5936,\"start\":5917},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6348,\"start\":6329},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6513,\"start\":6495},{\"end\":7627,\"start\":7609},{\"end\":8069,\"start\":8052},{\"end\":8112,\"start\":8089},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8149,\"start\":8129},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8188,\"start\":8162},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8221,\"start\":8202},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8258,\"start\":8238},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8301,\"start\":8280},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9174,\"start\":9153},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9288,\"start\":9269},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9328,\"start\":9307},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9845,\"start\":9824},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10026,\"start\":10006},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10199,\"start\":10178},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10264,\"start\":10244},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10649,\"start\":10628},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10728,\"start\":10710},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":11416,\"start\":11398},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":11769,\"start\":11751},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12026,\"start\":12003},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":13487,\"start\":13468},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":14558,\"start\":14540},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":17273,\"start\":17250},{\"end\":19643,\"start\":19641}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":29133,\"start\":29078},{\"attributes\":{\"id\":\"fig_1\"},\"end\":29214,\"start\":29134},{\"attributes\":{\"id\":\"fig_2\"},\"end\":29292,\"start\":29215},{\"attributes\":{\"id\":\"fig_4\"},\"end\":29382,\"start\":29293},{\"attributes\":{\"id\":\"fig_5\"},\"end\":29465,\"start\":29383},{\"attributes\":{\"id\":\"fig_6\"},\"end\":29556,\"start\":29466},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":29841,\"start\":29557},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":30199,\"start\":29842},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":31034,\"start\":30200},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":31140,\"start\":31035}]", "paragraph": "[{\"end\":3653,\"start\":2699},{\"end\":4128,\"start\":3655},{\"end\":5162,\"start\":4130},{\"end\":6646,\"start\":5164},{\"end\":7056,\"start\":6648},{\"end\":7287,\"start\":7058},{\"end\":7397,\"start\":7289},{\"end\":7716,\"start\":7414},{\"end\":8749,\"start\":7740},{\"end\":9039,\"start\":8751},{\"end\":9546,\"start\":9041},{\"end\":10443,\"start\":9548},{\"end\":11136,\"start\":10464},{\"end\":11249,\"start\":11160},{\"end\":12288,\"start\":11251},{\"end\":13355,\"start\":12290},{\"end\":13970,\"start\":13357},{\"end\":14759,\"start\":13972},{\"end\":15018,\"start\":14761},{\"end\":15589,\"start\":15068},{\"end\":15851,\"start\":15612},{\"end\":16522,\"start\":15853},{\"end\":17200,\"start\":16524},{\"end\":17563,\"start\":17202},{\"end\":19029,\"start\":17588},{\"end\":19564,\"start\":19031},{\"end\":20697,\"start\":19599},{\"end\":22725,\"start\":20699},{\"end\":24300,\"start\":22727},{\"end\":24732,\"start\":24325},{\"end\":24959,\"start\":24734},{\"end\":25699,\"start\":24961},{\"end\":26540,\"start\":25701},{\"end\":26816,\"start\":26542},{\"end\":27437,\"start\":26831},{\"end\":28059,\"start\":27439},{\"end\":28614,\"start\":28100},{\"end\":29077,\"start\":28648}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":15705,\"start\":15698},{\"end\":17004,\"start\":16997},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":19119,\"start\":19112},{\"end\":19639,\"start\":19632},{\"end\":20149,\"start\":20142},{\"end\":21059,\"start\":21052},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":23896,\"start\":23889},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":28733,\"start\":28726}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2697,\"start\":2685},{\"attributes\":{\"n\":\"2\"},\"end\":7412,\"start\":7400},{\"attributes\":{\"n\":\"2.1\"},\"end\":7738,\"start\":7719},{\"attributes\":{\"n\":\"2.2\"},\"end\":10462,\"start\":10446},{\"attributes\":{\"n\":\"3\"},\"end\":11158,\"start\":11139},{\"attributes\":{\"n\":\"4.2\"},\"end\":15037,\"start\":15021},{\"end\":15066,\"start\":15040},{\"end\":15610,\"start\":15592},{\"attributes\":{\"n\":\"4.3\"},\"end\":17586,\"start\":17566},{\"attributes\":{\"n\":\"5\"},\"end\":19574,\"start\":19567},{\"attributes\":{\"n\":\"5.1\"},\"end\":19597,\"start\":19577},{\"attributes\":{\"n\":\"5.2\"},\"end\":24323,\"start\":24303},{\"attributes\":{\"n\":\"6\"},\"end\":26829,\"start\":26819},{\"end\":28098,\"start\":28062},{\"end\":28646,\"start\":28617},{\"end\":29089,\"start\":29079},{\"end\":29145,\"start\":29135},{\"end\":29226,\"start\":29216},{\"end\":29304,\"start\":29294},{\"end\":29394,\"start\":29384},{\"end\":29477,\"start\":29467},{\"end\":29567,\"start\":29558},{\"end\":29852,\"start\":29843},{\"end\":31045,\"start\":31036}]", "table": "[{\"end\":29841,\"start\":29633},{\"end\":30199,\"start\":29943},{\"end\":31034,\"start\":30419}]", "figure_caption": "[{\"end\":29133,\"start\":29091},{\"end\":29214,\"start\":29147},{\"end\":29292,\"start\":29228},{\"end\":29382,\"start\":29306},{\"end\":29465,\"start\":29396},{\"end\":29556,\"start\":29479},{\"end\":29633,\"start\":29569},{\"end\":29943,\"start\":29854},{\"end\":30419,\"start\":30202},{\"end\":31140,\"start\":31047}]", "figure_ref": "[{\"end\":7637,\"start\":7629},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7871,\"start\":7863},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7913,\"start\":7904},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8783,\"start\":8774},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9580,\"start\":9571},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18757,\"start\":18749},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21427,\"start\":21419},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":23021,\"start\":23013},{\"end\":23654,\"start\":23646},{\"end\":24396,\"start\":24388},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":27565,\"start\":27557},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":28048,\"start\":28039},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":28193,\"start\":28185}]", "bib_author_first_name": "[{\"end\":32022,\"start\":32018},{\"end\":32036,\"start\":32030},{\"end\":32059,\"start\":32050},{\"end\":32072,\"start\":32065},{\"end\":32897,\"start\":32888},{\"end\":33453,\"start\":33450},{\"end\":33463,\"start\":33460},{\"end\":33476,\"start\":33472},{\"end\":33487,\"start\":33483},{\"end\":33710,\"start\":33705},{\"end\":33727,\"start\":33719},{\"end\":33742,\"start\":33735},{\"end\":33755,\"start\":33748},{\"end\":33769,\"start\":33763},{\"end\":33788,\"start\":33783},{\"end\":33801,\"start\":33796},{\"end\":33815,\"start\":33809},{\"end\":33828,\"start\":33821},{\"end\":33840,\"start\":33836},{\"end\":34160,\"start\":34154},{\"end\":34177,\"start\":34170},{\"end\":34454,\"start\":34448},{\"end\":34469,\"start\":34463},{\"end\":34486,\"start\":34480},{\"end\":34505,\"start\":34496},{\"end\":34728,\"start\":34725},{\"end\":34739,\"start\":34736},{\"end\":34749,\"start\":34745},{\"end\":34762,\"start\":34756},{\"end\":34776,\"start\":34770},{\"end\":35212,\"start\":35209},{\"end\":35226,\"start\":35220},{\"end\":35241,\"start\":35237},{\"end\":35259,\"start\":35252},{\"end\":35272,\"start\":35269},{\"end\":35286,\"start\":35278},{\"end\":35302,\"start\":35296},{\"end\":35313,\"start\":35307},{\"end\":35325,\"start\":35321},{\"end\":35341,\"start\":35336},{\"end\":35356,\"start\":35349},{\"end\":35374,\"start\":35365},{\"end\":35401,\"start\":35396},{\"end\":35895,\"start\":35887},{\"end\":35913,\"start\":35909},{\"end\":35927,\"start\":35920},{\"end\":35940,\"start\":35935},{\"end\":35958,\"start\":35950},{\"end\":35973,\"start\":35967},{\"end\":35990,\"start\":35984},{\"end\":36010,\"start\":36004},{\"end\":36024,\"start\":36018},{\"end\":36317,\"start\":36313},{\"end\":36329,\"start\":36324},{\"end\":36344,\"start\":36338},{\"end\":36356,\"start\":36350},{\"end\":36371,\"start\":36363},{\"end\":36384,\"start\":36379},{\"end\":36398,\"start\":36393},{\"end\":36412,\"start\":36408},{\"end\":36428,\"start\":36420},{\"end\":36441,\"start\":36437},{\"end\":36821,\"start\":36815},{\"end\":36840,\"start\":36831},{\"end\":37101,\"start\":37095},{\"end\":37114,\"start\":37109},{\"end\":37130,\"start\":37124},{\"end\":37147,\"start\":37142},{\"end\":37158,\"start\":37154},{\"end\":37172,\"start\":37168},{\"end\":37189,\"start\":37181},{\"end\":37574,\"start\":37569},{\"end\":37591,\"start\":37583},{\"end\":37605,\"start\":37599},{\"end\":37619,\"start\":37611},{\"end\":37629,\"start\":37620},{\"end\":37938,\"start\":37930},{\"end\":37949,\"start\":37945},{\"end\":37959,\"start\":37955},{\"end\":37969,\"start\":37966},{\"end\":37985,\"start\":37976},{\"end\":37996,\"start\":37992},{\"end\":38009,\"start\":38003},{\"end\":38020,\"start\":38016},{\"end\":38030,\"start\":38026},{\"end\":38041,\"start\":38036},{\"end\":38442,\"start\":38439},{\"end\":38454,\"start\":38448},{\"end\":38468,\"start\":38465},{\"end\":38484,\"start\":38476},{\"end\":38500,\"start\":38494},{\"end\":38515,\"start\":38508},{\"end\":38529,\"start\":38524},{\"end\":38543,\"start\":38537},{\"end\":38553,\"start\":38548},{\"end\":38564,\"start\":38561},{\"end\":38904,\"start\":38902},{\"end\":38918,\"start\":38910},{\"end\":38930,\"start\":38926},{\"end\":38943,\"start\":38937},{\"end\":38952,\"start\":38948},{\"end\":38962,\"start\":38958},{\"end\":38976,\"start\":38968},{\"end\":38987,\"start\":38982},{\"end\":39001,\"start\":38995},{\"end\":39013,\"start\":39007},{\"end\":39289,\"start\":39288},{\"end\":39308,\"start\":39299},{\"end\":39750,\"start\":39749},{\"end\":39771,\"start\":39760},{\"end\":40071,\"start\":40066},{\"end\":40081,\"start\":40080},{\"end\":40092,\"start\":40088},{\"end\":40107,\"start\":40099},{\"end\":40124,\"start\":40115},{\"end\":40327,\"start\":40324},{\"end\":40341,\"start\":40338},{\"end\":40350,\"start\":40348},{\"end\":40362,\"start\":40355},{\"end\":40376,\"start\":40371},{\"end\":40684,\"start\":40679},{\"end\":40702,\"start\":40693},{\"end\":40714,\"start\":40707},{\"end\":40731,\"start\":40722},{\"end\":40747,\"start\":40743},{\"end\":41059,\"start\":41053},{\"end\":41074,\"start\":41068},{\"end\":41090,\"start\":41085},{\"end\":41111,\"start\":41105},{\"end\":41454,\"start\":41440},{\"end\":41472,\"start\":41468},{\"end\":41486,\"start\":41480},{\"end\":41502,\"start\":41495},{\"end\":41517,\"start\":41511},{\"end\":41987,\"start\":41983},{\"end\":42001,\"start\":41995},{\"end\":42012,\"start\":42007},{\"end\":42026,\"start\":42020},{\"end\":42053,\"start\":42042},{\"end\":42067,\"start\":42063},{\"end\":42077,\"start\":42074},{\"end\":42092,\"start\":42088},{\"end\":42510,\"start\":42505},{\"end\":42519,\"start\":42515},{\"end\":42529,\"start\":42525},{\"end\":42541,\"start\":42535},{\"end\":42555,\"start\":42549},{\"end\":42578,\"start\":42570},{\"end\":42592,\"start\":42587},{\"end\":42606,\"start\":42602},{\"end\":42619,\"start\":42614},{\"end\":42631,\"start\":42627},{\"end\":42640,\"start\":42638},{\"end\":42651,\"start\":42645},{\"end\":42664,\"start\":42658},{\"end\":42675,\"start\":42671},{\"end\":42689,\"start\":42682},{\"end\":42702,\"start\":42698},{\"end\":42713,\"start\":42709},{\"end\":42723,\"start\":42720},{\"end\":42734,\"start\":42730},{\"end\":42754,\"start\":42747},{\"end\":42776,\"start\":42770},{\"end\":43488,\"start\":43484},{\"end\":43505,\"start\":43498},{\"end\":43515,\"start\":43510},{\"end\":43528,\"start\":43523},{\"end\":43540,\"start\":43535},{\"end\":43553,\"start\":43549},{\"end\":43838,\"start\":43833},{\"end\":43851,\"start\":43847},{\"end\":43865,\"start\":43861},{\"end\":43884,\"start\":43875},{\"end\":43896,\"start\":43890},{\"end\":43912,\"start\":43905},{\"end\":43926,\"start\":43921},{\"end\":43936,\"start\":43933},{\"end\":43948,\"start\":43941},{\"end\":44289,\"start\":44282},{\"end\":44305,\"start\":44299},{\"end\":44318,\"start\":44314},{\"end\":44728,\"start\":44724},{\"end\":44744,\"start\":44739},{\"end\":44762,\"start\":44753},{\"end\":45064,\"start\":45059},{\"end\":45081,\"start\":45075},{\"end\":45266,\"start\":45263},{\"end\":45277,\"start\":45273},{\"end\":45621,\"start\":45618},{\"end\":45634,\"start\":45628},{\"end\":45647,\"start\":45641},{\"end\":45655,\"start\":45654},{\"end\":45657,\"start\":45656},{\"end\":45994,\"start\":45988},{\"end\":46009,\"start\":46003},{\"end\":46030,\"start\":46023},{\"end\":46044,\"start\":46040},{\"end\":46062,\"start\":46055}]", "bib_author_last_name": "[{\"end\":32028,\"start\":32023},{\"end\":32048,\"start\":32037},{\"end\":32063,\"start\":32060},{\"end\":32078,\"start\":32073},{\"end\":32907,\"start\":32898},{\"end\":33458,\"start\":33454},{\"end\":33470,\"start\":33464},{\"end\":33481,\"start\":33477},{\"end\":33493,\"start\":33488},{\"end\":33717,\"start\":33711},{\"end\":33733,\"start\":33728},{\"end\":33746,\"start\":33743},{\"end\":33761,\"start\":33756},{\"end\":33781,\"start\":33770},{\"end\":33794,\"start\":33789},{\"end\":33807,\"start\":33802},{\"end\":33819,\"start\":33816},{\"end\":33834,\"start\":33829},{\"end\":33843,\"start\":33841},{\"end\":34168,\"start\":34161},{\"end\":34182,\"start\":34178},{\"end\":34461,\"start\":34455},{\"end\":34478,\"start\":34470},{\"end\":34494,\"start\":34487},{\"end\":34512,\"start\":34506},{\"end\":34734,\"start\":34729},{\"end\":34743,\"start\":34740},{\"end\":34754,\"start\":34750},{\"end\":34768,\"start\":34763},{\"end\":34785,\"start\":34777},{\"end\":34794,\"start\":34787},{\"end\":35218,\"start\":35213},{\"end\":35235,\"start\":35227},{\"end\":35250,\"start\":35242},{\"end\":35267,\"start\":35260},{\"end\":35276,\"start\":35273},{\"end\":35294,\"start\":35287},{\"end\":35305,\"start\":35303},{\"end\":35319,\"start\":35314},{\"end\":35334,\"start\":35326},{\"end\":35347,\"start\":35342},{\"end\":35363,\"start\":35357},{\"end\":35394,\"start\":35375},{\"end\":35409,\"start\":35402},{\"end\":35419,\"start\":35411},{\"end\":35907,\"start\":35896},{\"end\":35918,\"start\":35914},{\"end\":35933,\"start\":35928},{\"end\":35948,\"start\":35941},{\"end\":35965,\"start\":35959},{\"end\":35982,\"start\":35974},{\"end\":36002,\"start\":35991},{\"end\":36016,\"start\":36011},{\"end\":36031,\"start\":36025},{\"end\":36039,\"start\":36033},{\"end\":36322,\"start\":36318},{\"end\":36336,\"start\":36330},{\"end\":36348,\"start\":36345},{\"end\":36361,\"start\":36357},{\"end\":36377,\"start\":36372},{\"end\":36391,\"start\":36385},{\"end\":36406,\"start\":36399},{\"end\":36418,\"start\":36413},{\"end\":36435,\"start\":36429},{\"end\":36450,\"start\":36442},{\"end\":36829,\"start\":36822},{\"end\":36847,\"start\":36841},{\"end\":37107,\"start\":37102},{\"end\":37122,\"start\":37115},{\"end\":37140,\"start\":37131},{\"end\":37152,\"start\":37148},{\"end\":37166,\"start\":37159},{\"end\":37179,\"start\":37173},{\"end\":37193,\"start\":37190},{\"end\":37581,\"start\":37575},{\"end\":37597,\"start\":37592},{\"end\":37609,\"start\":37606},{\"end\":37634,\"start\":37630},{\"end\":37943,\"start\":37939},{\"end\":37953,\"start\":37950},{\"end\":37964,\"start\":37960},{\"end\":37974,\"start\":37970},{\"end\":37990,\"start\":37986},{\"end\":38001,\"start\":37997},{\"end\":38014,\"start\":38010},{\"end\":38024,\"start\":38021},{\"end\":38034,\"start\":38031},{\"end\":38047,\"start\":38042},{\"end\":38446,\"start\":38443},{\"end\":38463,\"start\":38455},{\"end\":38474,\"start\":38469},{\"end\":38492,\"start\":38485},{\"end\":38506,\"start\":38501},{\"end\":38522,\"start\":38516},{\"end\":38535,\"start\":38530},{\"end\":38546,\"start\":38544},{\"end\":38559,\"start\":38554},{\"end\":38574,\"start\":38565},{\"end\":38908,\"start\":38905},{\"end\":38924,\"start\":38919},{\"end\":38935,\"start\":38931},{\"end\":38946,\"start\":38944},{\"end\":38956,\"start\":38953},{\"end\":38966,\"start\":38963},{\"end\":38980,\"start\":38977},{\"end\":38993,\"start\":38988},{\"end\":39005,\"start\":39002},{\"end\":39019,\"start\":39014},{\"end\":39297,\"start\":39290},{\"end\":39320,\"start\":39309},{\"end\":39329,\"start\":39322},{\"end\":39758,\"start\":39751},{\"end\":39783,\"start\":39772},{\"end\":39791,\"start\":39785},{\"end\":40078,\"start\":40072},{\"end\":40086,\"start\":40082},{\"end\":40097,\"start\":40093},{\"end\":40113,\"start\":40108},{\"end\":40127,\"start\":40125},{\"end\":40136,\"start\":40129},{\"end\":40336,\"start\":40328},{\"end\":40346,\"start\":40342},{\"end\":40353,\"start\":40351},{\"end\":40369,\"start\":40363},{\"end\":40381,\"start\":40377},{\"end\":40691,\"start\":40685},{\"end\":40705,\"start\":40703},{\"end\":40720,\"start\":40715},{\"end\":40741,\"start\":40732},{\"end\":40760,\"start\":40748},{\"end\":41066,\"start\":41060},{\"end\":41083,\"start\":41075},{\"end\":41103,\"start\":41091},{\"end\":41115,\"start\":41112},{\"end\":41466,\"start\":41455},{\"end\":41478,\"start\":41473},{\"end\":41493,\"start\":41487},{\"end\":41509,\"start\":41503},{\"end\":41523,\"start\":41518},{\"end\":41993,\"start\":41988},{\"end\":42005,\"start\":42002},{\"end\":42018,\"start\":42013},{\"end\":42040,\"start\":42027},{\"end\":42061,\"start\":42054},{\"end\":42072,\"start\":42068},{\"end\":42086,\"start\":42078},{\"end\":42104,\"start\":42093},{\"end\":42513,\"start\":42511},{\"end\":42523,\"start\":42520},{\"end\":42533,\"start\":42530},{\"end\":42547,\"start\":42542},{\"end\":42568,\"start\":42556},{\"end\":42585,\"start\":42579},{\"end\":42600,\"start\":42593},{\"end\":42612,\"start\":42607},{\"end\":42625,\"start\":42620},{\"end\":42636,\"start\":42632},{\"end\":42643,\"start\":42641},{\"end\":42656,\"start\":42652},{\"end\":42669,\"start\":42665},{\"end\":42680,\"start\":42676},{\"end\":42696,\"start\":42690},{\"end\":42707,\"start\":42703},{\"end\":42718,\"start\":42714},{\"end\":42728,\"start\":42724},{\"end\":42745,\"start\":42735},{\"end\":42768,\"start\":42755},{\"end\":42779,\"start\":42777},{\"end\":42784,\"start\":42781},{\"end\":43496,\"start\":43489},{\"end\":43508,\"start\":43506},{\"end\":43521,\"start\":43516},{\"end\":43533,\"start\":43529},{\"end\":43547,\"start\":43541},{\"end\":43563,\"start\":43554},{\"end\":43845,\"start\":43839},{\"end\":43859,\"start\":43852},{\"end\":43873,\"start\":43866},{\"end\":43888,\"start\":43885},{\"end\":43903,\"start\":43897},{\"end\":43919,\"start\":43913},{\"end\":43931,\"start\":43927},{\"end\":43939,\"start\":43937},{\"end\":43952,\"start\":43949},{\"end\":44297,\"start\":44290},{\"end\":44312,\"start\":44306},{\"end\":44324,\"start\":44319},{\"end\":44737,\"start\":44729},{\"end\":44751,\"start\":44745},{\"end\":44768,\"start\":44763},{\"end\":45073,\"start\":45065},{\"end\":45099,\"start\":45082},{\"end\":45105,\"start\":45101},{\"end\":45271,\"start\":45267},{\"end\":45289,\"start\":45278},{\"end\":45299,\"start\":45291},{\"end\":45626,\"start\":45622},{\"end\":45639,\"start\":45635},{\"end\":45652,\"start\":45648},{\"end\":45664,\"start\":45658},{\"end\":45669,\"start\":45666},{\"end\":46001,\"start\":45995},{\"end\":46021,\"start\":46010},{\"end\":46038,\"start\":46031},{\"end\":46053,\"start\":46045},{\"end\":46072,\"start\":46063}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":232185260},\"end\":32810,\"start\":31955},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":56482376},\"end\":33412,\"start\":32812},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":211066355},\"end\":33703,\"start\":33414},{\"attributes\":{\"doi\":\"arXiv:2108.07732\",\"id\":\"b3\"},\"end\":34152,\"start\":33705},{\"attributes\":{\"doi\":\"arXiv:1809.10853\",\"id\":\"b4\"},\"end\":34407,\"start\":34154},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":221275765},\"end\":34723,\"start\":34409},{\"attributes\":{\"doi\":\"10.5281/zenodo.5297715\",\"id\":\"b6\"},\"end\":35207,\"start\":34725},{\"attributes\":{\"id\":\"b7\"},\"end\":35846,\"start\":35209},{\"attributes\":{\"doi\":\"arXiv:2005.14165\",\"id\":\"b8\"},\"end\":36309,\"start\":35848},{\"attributes\":{\"doi\":\"arXiv:2107.03374\",\"id\":\"b9\"},\"end\":36771,\"start\":36311},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":58981712},\"end\":37051,\"start\":36773},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":13234077},\"end\":37567,\"start\":37053},{\"attributes\":{\"doi\":\"arXiv:1810.04805\",\"id\":\"b12\"},\"end\":37928,\"start\":37569},{\"attributes\":{\"doi\":\"arXiv:2002.08155\",\"id\":\"b13\"},\"end\":38371,\"start\":37930},{\"attributes\":{\"doi\":\"arXiv:2101.00027\",\"id\":\"b14\"},\"end\":38854,\"start\":38373},{\"attributes\":{\"id\":\"b15\"},\"end\":39218,\"start\":38856},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":21164835},\"end\":39696,\"start\":39220},{\"attributes\":{\"doi\":\"10.1145/3501261\",\"id\":\"b17\",\"matched_paper_id\":245274891},\"end\":40032,\"start\":39698},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":2846066},\"end\":40322,\"start\":40034},{\"attributes\":{\"doi\":\"arXiv:1904.09751\",\"id\":\"b19\"},\"end\":40606,\"start\":40324},{\"attributes\":{\"doi\":\"arXiv:1909.09436\",\"id\":\"b20\"},\"end\":40990,\"start\":40608},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":220425306},\"end\":41371,\"start\":40992},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":211161525},\"end\":41866,\"start\":41373},{\"attributes\":{\"doi\":\"arXiv:1910.13461\",\"id\":\"b23\"},\"end\":42416,\"start\":41868},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":231855531},\"end\":43429,\"start\":42418},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":160025533},\"end\":43748,\"start\":43431},{\"attributes\":{\"doi\":\"arXiv:1910.10683\",\"id\":\"b26\"},\"end\":44230,\"start\":43750},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":13040187},\"end\":44722,\"start\":44232},{\"attributes\":{\"doi\":\"arXiv:1508.07909\",\"id\":\"b28\"},\"end\":45010,\"start\":44724},{\"attributes\":{\"id\":\"b29\"},\"end\":45261,\"start\":45012},{\"attributes\":{\"id\":\"b30\"},\"end\":45509,\"start\":45263},{\"attributes\":{\"doi\":\"arXiv:2109.00859\",\"id\":\"b31\"},\"end\":45901,\"start\":45511},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":232307359},\"end\":46417,\"start\":45903}]", "bib_title": "[{\"end\":32016,\"start\":31955},{\"end\":32886,\"start\":32812},{\"end\":33448,\"start\":33414},{\"end\":34446,\"start\":34409},{\"end\":36813,\"start\":36773},{\"end\":37093,\"start\":37053},{\"end\":39286,\"start\":39220},{\"end\":39747,\"start\":39698},{\"end\":40064,\"start\":40034},{\"end\":41051,\"start\":40992},{\"end\":41438,\"start\":41373},{\"end\":42503,\"start\":42418},{\"end\":43482,\"start\":43431},{\"end\":44280,\"start\":44232},{\"end\":45986,\"start\":45903}]", "bib_author": "[{\"end\":32030,\"start\":32018},{\"end\":32050,\"start\":32030},{\"end\":32065,\"start\":32050},{\"end\":32080,\"start\":32065},{\"end\":32909,\"start\":32888},{\"end\":33460,\"start\":33450},{\"end\":33472,\"start\":33460},{\"end\":33483,\"start\":33472},{\"end\":33495,\"start\":33483},{\"end\":33719,\"start\":33705},{\"end\":33735,\"start\":33719},{\"end\":33748,\"start\":33735},{\"end\":33763,\"start\":33748},{\"end\":33783,\"start\":33763},{\"end\":33796,\"start\":33783},{\"end\":33809,\"start\":33796},{\"end\":33821,\"start\":33809},{\"end\":33836,\"start\":33821},{\"end\":33845,\"start\":33836},{\"end\":34170,\"start\":34154},{\"end\":34184,\"start\":34170},{\"end\":34463,\"start\":34448},{\"end\":34480,\"start\":34463},{\"end\":34496,\"start\":34480},{\"end\":34514,\"start\":34496},{\"end\":34736,\"start\":34725},{\"end\":34745,\"start\":34736},{\"end\":34756,\"start\":34745},{\"end\":34770,\"start\":34756},{\"end\":34787,\"start\":34770},{\"end\":34796,\"start\":34787},{\"end\":35220,\"start\":35209},{\"end\":35237,\"start\":35220},{\"end\":35252,\"start\":35237},{\"end\":35269,\"start\":35252},{\"end\":35278,\"start\":35269},{\"end\":35296,\"start\":35278},{\"end\":35307,\"start\":35296},{\"end\":35321,\"start\":35307},{\"end\":35336,\"start\":35321},{\"end\":35349,\"start\":35336},{\"end\":35365,\"start\":35349},{\"end\":35396,\"start\":35365},{\"end\":35411,\"start\":35396},{\"end\":35421,\"start\":35411},{\"end\":35909,\"start\":35887},{\"end\":35920,\"start\":35909},{\"end\":35935,\"start\":35920},{\"end\":35950,\"start\":35935},{\"end\":35967,\"start\":35950},{\"end\":35984,\"start\":35967},{\"end\":36004,\"start\":35984},{\"end\":36018,\"start\":36004},{\"end\":36033,\"start\":36018},{\"end\":36041,\"start\":36033},{\"end\":36324,\"start\":36313},{\"end\":36338,\"start\":36324},{\"end\":36350,\"start\":36338},{\"end\":36363,\"start\":36350},{\"end\":36379,\"start\":36363},{\"end\":36393,\"start\":36379},{\"end\":36408,\"start\":36393},{\"end\":36420,\"start\":36408},{\"end\":36437,\"start\":36420},{\"end\":36452,\"start\":36437},{\"end\":36831,\"start\":36815},{\"end\":36849,\"start\":36831},{\"end\":37109,\"start\":37095},{\"end\":37124,\"start\":37109},{\"end\":37142,\"start\":37124},{\"end\":37154,\"start\":37142},{\"end\":37168,\"start\":37154},{\"end\":37181,\"start\":37168},{\"end\":37195,\"start\":37181},{\"end\":37583,\"start\":37569},{\"end\":37599,\"start\":37583},{\"end\":37611,\"start\":37599},{\"end\":37636,\"start\":37611},{\"end\":37945,\"start\":37930},{\"end\":37955,\"start\":37945},{\"end\":37966,\"start\":37955},{\"end\":37976,\"start\":37966},{\"end\":37992,\"start\":37976},{\"end\":38003,\"start\":37992},{\"end\":38016,\"start\":38003},{\"end\":38026,\"start\":38016},{\"end\":38036,\"start\":38026},{\"end\":38049,\"start\":38036},{\"end\":38448,\"start\":38439},{\"end\":38465,\"start\":38448},{\"end\":38476,\"start\":38465},{\"end\":38494,\"start\":38476},{\"end\":38508,\"start\":38494},{\"end\":38524,\"start\":38508},{\"end\":38537,\"start\":38524},{\"end\":38548,\"start\":38537},{\"end\":38561,\"start\":38548},{\"end\":38576,\"start\":38561},{\"end\":38910,\"start\":38902},{\"end\":38926,\"start\":38910},{\"end\":38937,\"start\":38926},{\"end\":38948,\"start\":38937},{\"end\":38958,\"start\":38948},{\"end\":38968,\"start\":38958},{\"end\":38982,\"start\":38968},{\"end\":38995,\"start\":38982},{\"end\":39007,\"start\":38995},{\"end\":39021,\"start\":39007},{\"end\":39299,\"start\":39288},{\"end\":39322,\"start\":39299},{\"end\":39331,\"start\":39322},{\"end\":39760,\"start\":39749},{\"end\":39785,\"start\":39760},{\"end\":39793,\"start\":39785},{\"end\":40080,\"start\":40066},{\"end\":40088,\"start\":40080},{\"end\":40099,\"start\":40088},{\"end\":40115,\"start\":40099},{\"end\":40129,\"start\":40115},{\"end\":40138,\"start\":40129},{\"end\":40338,\"start\":40324},{\"end\":40348,\"start\":40338},{\"end\":40355,\"start\":40348},{\"end\":40371,\"start\":40355},{\"end\":40383,\"start\":40371},{\"end\":40693,\"start\":40679},{\"end\":40707,\"start\":40693},{\"end\":40722,\"start\":40707},{\"end\":40743,\"start\":40722},{\"end\":40762,\"start\":40743},{\"end\":41068,\"start\":41053},{\"end\":41085,\"start\":41068},{\"end\":41105,\"start\":41085},{\"end\":41117,\"start\":41105},{\"end\":41468,\"start\":41440},{\"end\":41480,\"start\":41468},{\"end\":41495,\"start\":41480},{\"end\":41511,\"start\":41495},{\"end\":41525,\"start\":41511},{\"end\":41995,\"start\":41983},{\"end\":42007,\"start\":41995},{\"end\":42020,\"start\":42007},{\"end\":42042,\"start\":42020},{\"end\":42063,\"start\":42042},{\"end\":42074,\"start\":42063},{\"end\":42088,\"start\":42074},{\"end\":42106,\"start\":42088},{\"end\":42515,\"start\":42505},{\"end\":42525,\"start\":42515},{\"end\":42535,\"start\":42525},{\"end\":42549,\"start\":42535},{\"end\":42570,\"start\":42549},{\"end\":42587,\"start\":42570},{\"end\":42602,\"start\":42587},{\"end\":42614,\"start\":42602},{\"end\":42627,\"start\":42614},{\"end\":42638,\"start\":42627},{\"end\":42645,\"start\":42638},{\"end\":42658,\"start\":42645},{\"end\":42671,\"start\":42658},{\"end\":42682,\"start\":42671},{\"end\":42698,\"start\":42682},{\"end\":42709,\"start\":42698},{\"end\":42720,\"start\":42709},{\"end\":42730,\"start\":42720},{\"end\":42747,\"start\":42730},{\"end\":42770,\"start\":42747},{\"end\":42781,\"start\":42770},{\"end\":42786,\"start\":42781},{\"end\":43498,\"start\":43484},{\"end\":43510,\"start\":43498},{\"end\":43523,\"start\":43510},{\"end\":43535,\"start\":43523},{\"end\":43549,\"start\":43535},{\"end\":43565,\"start\":43549},{\"end\":43847,\"start\":43833},{\"end\":43861,\"start\":43847},{\"end\":43875,\"start\":43861},{\"end\":43890,\"start\":43875},{\"end\":43905,\"start\":43890},{\"end\":43921,\"start\":43905},{\"end\":43933,\"start\":43921},{\"end\":43941,\"start\":43933},{\"end\":43954,\"start\":43941},{\"end\":44299,\"start\":44282},{\"end\":44314,\"start\":44299},{\"end\":44326,\"start\":44314},{\"end\":44739,\"start\":44724},{\"end\":44753,\"start\":44739},{\"end\":44770,\"start\":44753},{\"end\":45075,\"start\":45059},{\"end\":45101,\"start\":45075},{\"end\":45107,\"start\":45101},{\"end\":45273,\"start\":45263},{\"end\":45291,\"start\":45273},{\"end\":45301,\"start\":45291},{\"end\":45628,\"start\":45618},{\"end\":45641,\"start\":45628},{\"end\":45654,\"start\":45641},{\"end\":45666,\"start\":45654},{\"end\":45671,\"start\":45666},{\"end\":46003,\"start\":45988},{\"end\":46023,\"start\":46003},{\"end\":46040,\"start\":46023},{\"end\":46055,\"start\":46040},{\"end\":46074,\"start\":46055}]", "bib_venue": "[{\"end\":32222,\"start\":32080},{\"end\":33041,\"start\":32909},{\"end\":33539,\"start\":33495},{\"end\":33905,\"start\":33861},{\"end\":34259,\"start\":34200},{\"end\":34550,\"start\":34514},{\"end\":34883,\"start\":34818},{\"end\":35479,\"start\":35421},{\"end\":35885,\"start\":35848},{\"end\":36898,\"start\":36849},{\"end\":37267,\"start\":37195},{\"end\":37726,\"start\":37652},{\"end\":38122,\"start\":38065},{\"end\":38437,\"start\":38373},{\"end\":38900,\"start\":38856},{\"end\":39412,\"start\":39331},{\"end\":39819,\"start\":39808},{\"end\":40163,\"start\":40138},{\"end\":40443,\"start\":40399},{\"end\":40677,\"start\":40608},{\"end\":41161,\"start\":41117},{\"end\":41599,\"start\":41525},{\"end\":41981,\"start\":41868},{\"end\":42880,\"start\":42786},{\"end\":43576,\"start\":43565},{\"end\":43831,\"start\":43750},{\"end\":44422,\"start\":44326},{\"end\":44845,\"start\":44786},{\"end\":45057,\"start\":45012},{\"end\":45352,\"start\":45301},{\"end\":45616,\"start\":45511},{\"end\":46126,\"start\":46074},{\"end\":32357,\"start\":32224},{\"end\":33160,\"start\":33043},{\"end\":35524,\"start\":35481},{\"end\":37326,\"start\":37269},{\"end\":39480,\"start\":39414},{\"end\":44505,\"start\":44424}]"}}}, "year": 2023, "month": 12, "day": 17}