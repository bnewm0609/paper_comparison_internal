{"id": 21685171, "updated": "2023-10-08 09:15:01.55", "metadata": {"title": "Multiclass Learning With Partially Corrupted Labels", "authors": "[{\"first\":\"Ruxin\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Tongliang\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Dacheng\",\"last\":\"Tao\",\"middle\":[]}]", "venue": "IEEE Transactions on Neural Networks and Learning Systems", "journal": "IEEE Transactions on Neural Networks and Learning Systems", "publication_date": {"year": 2018, "month": null, "day": null}, "abstract": "Traditional classification systems rely heavily on sufficient training data with accurate labels. However, the quality of the collected data depends on the labelers, among which inexperienced labelers may exist and produce unexpected labels that may degrade the performance of a learning system. In this paper, we investigate the multiclass classification problem where a certain amount of training examples are randomly labeled. Specifically, we show that this issue can be formulated as a label noise problem. To perform multiclass classification, we employ the widely used importance reweighting strategy to enable the learning on noisy data to more closely reflect the results on noise-free data. We illustrate the applicability of this strategy to any surrogate loss functions and to different classification settings. The proportion of randomly labeled examples is proved to be upper bounded and can be estimated under a mild condition. The convergence analysis ensures the consistency of the learned classifier to the optimal classifier with respect to clean data. Two instantiations of the proposed strategy are also introduced. Experiments on synthetic and real data verify that our approach yields improvements over the traditional classifiers as well as the robust classifiers. Moreover, we empirically demonstrate that the proposed strategy is effective even on asymmetrically noisy data.", "fields_of_study": "[\"Medicine\"]", "external_ids": {"arxiv": null, "mag": "2616630373", "acl": null, "pubmed": "28534794", "pubmedcentral": null, "dblp": "journals/tnn/WangLT18", "doi": "10.1109/tnnls.2017.2699783"}}, "content": {"source": {"pdf_hash": "883771fa3ef46c69414f9dea03d403680140ef6b", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "126c31cdaae6feee3f5377a258958a0a8b63508a", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/883771fa3ef46c69414f9dea03d403680140ef6b.txt", "contents": "\nMulticlass Learning With Partially Corrupted Labels\nJUNE 2018\n\nRuxin Wang \nTongliang Liu \nFellow, IEEEDacheng Tao \nMulticlass Learning With Partially Corrupted Labels\n\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS\n296JUNE 201810.1109/TNNLS.2017.26997832568Index Terms-Importance reweightinglabel noisemulticlass classificationrandom labels\nTraditional classification systems rely heavily on sufficient training data with accurate labels. However, the quality of the collected data depends on the labelers, among which inexperienced labelers may exist and produce unexpected labels that may degrade the performance of a learning system. In this paper, we investigate the multiclass classification problem where a certain amount of training examples are randomly labeled. Specifically, we show that this issue can be formulated as a label noise problem. To perform multiclass classification, we employ the widely used importance reweighting strategy to enable the learning on noisy data to more closely reflect the results on noisefree data. We illustrate the applicability of this strategy to any surrogate loss functions and to different classification settings.The proportion of randomly labeled examples is proved to be upper bounded and can be estimated under a mild condition.The convergence analysis ensures the consistency of the learned classifier to the optimal classifier with respect to clean data. Two instantiations of the proposed strategy are also introduced. Experiments on synthetic and real data verify that our approach yields improvements over the traditional classifiers as well as the robust classifiers. Moreover, we empirically demonstrate that the proposed strategy is effective even on asymmetrically noisy data.\n\nI. INTRODUCTION\n\nB IG data form a new frontier for innovation and productivity. The recent explosion in the availability of big data through social networks brings challenges but also opportunities for artificial intelligence and machine learning, including visualization, classification, and prediction. The ability to comprehend and extract actionable information and knowledge from data is thus essential for building reliable intelligent systems [1]- [3].\n\nThe performance of a system largely depends on the quality of the training data, which need to be manually collected. Undoubtedly, it is highly expected that one will correct the inaccurate information during collection, such that the quality of the obtained data can be ensured. However, such a training data collection mission is expensive and time-consuming, which often involves unaffordable expert effort. Considering that data are shared and processed by many groups, more and more labeling about the data offered by nonexperts becomes available. For example, the crowdsourcing mechanism [4], [5] (e.g., Amazon Mechanical Turk) offers an open platform for requesters to collect labels at low cost. It facilitates requesters to post tasks and enables online users (mostly nonexperts) to accomplish the labeling tasks for payment. Through such a platform, it is possible to either ask all labelers to do the task on the same data set, or ask different labelers to work on different subsets of the whole data without overlap. Here, we consider the latter case. Among the nonexperts, there may exist a certain number of labelers who just want to maximize personal profit without conscientiousness, and the rest are labelers who can provide accurate labels. In such a case, it is likely that parts of (not all) data examples are randomly labeled by the first kind of labelers. Since there is no oracle who can verify the obtained labels during the labeling process, the correctness of the produced labels cannot be guaranteed. This raises an inevitable issue that an inaccurate labeling process introduces noisy labels. More specifically, such noise can be seen as label flip noise, where an instance is erroneously given the label of another class within the data set. Considering that the distribution of the examples in each class can be well characterized, the flipped labels may confuse the learning of classifiers, thus rendering most low-capacity (e.g., linear) classification approaches suboptimal, or even poor in some situations [6]. This motivates the necessity of label noisetolerant classification algorithms.\n\nExplicitly, the aforementioned noise refers to observed labels that are incorrect. During label collection, no supervisor can provide an indication of correctness or a measure of uncertainty on the observed labels. We should clarify that only the observed labels of the examples are affected, while the clean labels are unavailable, and then, this induces the label noise problem.\n\nUntil now, a wide range of attention has been focused on the label noise problem for binary classification [7]- [13], whereas limited efforts have been conducted on multiclass classification with noisy labels. For example, a robust multiclass Gaussian process classifier was proposed [14], which suppresses the label noise effects by ignoring the distances of the incorrectly labeled examples to the decision boundaries of the classifier. This is achieved by introducing, in the Gaussian process model, a binary indictor of whether the label is correct or not. Bootkrajang and Kab\u00e1n [15] developed a modelspecified approach that extended the multiclass quadratic normal discriminant analysis to a robust variant, and empirically demonstrated its efficacy. A large-scale multiclass classifi-cation problem on the ImageNet benchmark has also been investigated in the case of label noise [16]. A noisy layer, which is instantiated by a label probability transition matrix, is built upon Convnet. The label noise-tolerant property is obtained via a thorough learning process. S\u00e1ez et al. [17] suggested to decompose the multiclass classification into one-vs-one subproblems to alleviate the influence of noisy labels. Sun et al. [18] designed a robust multiclass AdaBoost (Rob_MAd) algorithm, which explicitly detected the noisy labels and trained the classifiers based on a noise-aware loss function. In the loss function, an indicator function of the example being noisy or not was used to penalize the misclassified nonnoisy examples and the correctly classified noisy examples.\n\nThe above-mentioned methods have proven promising in handling different types of label noise in multiclass settings. However, a critical issue restricts them from benefitting from the previous research efforts on multiclass classification, which is, they are designed for specific surrogate loss functions or classifiers, and cannot be directly applied to other effective learning machines and classifiers. Hence, a general label noise-tolerant strategy is needed, which is capable of cooperating with any traditional multiclass classification methods, or specifically, with any surrogate loss functions and any multiclass classification settings [19].\n\nIn this paper, we consider the setting that when collecting the labels for multiclass classification, a proportion of data is randomly labeled. We show that this issue can be formulated as a label noise problem under the assumption of symmetric noise, which means that the labels have some constant probability of being flipped [7]. To tackle the problem of multiclass classification with noisy labels, we investigate the importance reweighting method [10]. Note that this investigation for multiclass classification is not trivial, since two major problems exist: one is how to handle the noisy data by estimating the influence of each example on the learning of classifiers, while another is how to estimate the noise rate. In the multiclass case, the label noise disrupts the distribution of the original clean data, thus changing the structure of the data space. Inspired by the idea of importance reweighting, the example that is incorrectly labeled is expected to have a weak influence on the learning of the classifiers, where the influence is revealed by a weight estimated from the distribution of the training data. We illustrate that in the multiclass case, this strategy is applicable to different surrogate loss functions and different classification settings. The proportion of randomly labeled examples is proved to be upper bounded, and can be estimated according to the distribution of the noisy data. We provide theoretical results to show that the classifier learned on noisy data by using our method is consistent with the optimal classifier learned on noise-free data, under certain conditions. Note that the classifier is regarded to be optimal only if it is the minimizer of the expected risk defined by a surrogate loss function. To verify the above-claimed advantages, we then propose an importance reweighted multinomial logistic regression (IWMLR) model and an importance reweighted multiclass support vector machine (IWSVM). Experimental results on synthetic and real data sets demonstrate the effectiveness of the proposed strategy. The trials on the data generated under an asymmetric noise model also verify that the proposed strategy can improve the traditional classification methods even when the symmetric assumption is not fulfilled.\n\nThe remainder of this paper is organized as follows. Section II reviews related work on label noise. Section III elaborates the formulation of multiclass classification when parts of training data are randomly labeled. The problem is modeled as a label noise issue. In Section IV, we introduce how the proposed importance reweighting strategy reduces the uncertainty caused by noisy labels, and discuss how to estimate the weights and the proportion of randomly labeled data; the convergence analysis is provided; two instantiations of the proposed strategy are detailed; and the differences between the proposed method and the previous ones are also discussed in this section. Empirical analyses on both synthetic data and real data are presented in Section V, with the conclusion drawn in Section VI.\n\n\nII. RELATED WORK IN THE LITERATURE\n\nExtensive efforts have been conducted on the label noise problem for the case of binary classification, and the literature reflects the progress. Label noise affects learning requirements or the complexity of designed models. In the classical setting, the random classification noise (RCN) model where each label is flipped independently with some probability is widely investigated. It has been shown [7] that the presence of RCN in the probably approximately correct (PAC) framework [20] increases the number of necessary examples for PAC identification. Several works [8], [21] analyze the PAC-learnability of the RCN model by deriving the upper bounds for the necessary examples.\n\nTo address RCN, there exist several trends in designing reliable algorithms. Along one trend, robust surrogate loss functions [22]- [24] or robust classifiers [25]- [27] are developed. However, label noise is not explicitly considered in these algorithms. The second trend concerns filtering [28]- [31]. In such a case, the mislabeled examples are typically identified, and then are either relabeled or removed. The preprocessed data are fed into the classification methods. An insufficiency is that it is likely to remove a substantial quantity of data, reducing the reliability of the learned classifiers. The third trend pays attention to algorithms that model label noise directly by using limited examples, such as the statistical query model [32], Bayesian model [33], SVM learning [34], loss factorization [35], [36], stochastic programming [37], and the perceptron algorithms [38]- [40]. The advantage is to explicitly model the effects of the label noise model, allowing exploiting the nature of label noise. There is one more type of approach, which is called label distribution learning [41]. Geng and Xia [42] proposed the so-called multivariate label distribution learning to correct the inaccurate labels in the application of head pose estimation. A review [43] could be referred to for missing references. In these works, there is a restriction that the proposed algorithm can only be used as shown in their paper and is not general. It is difficult or even impossible to broadcast the algorithm to different surrogate loss functions and classifiers designed for the traditional noise-free classification problem.\n\nEven though Aslam and Decatur [8] proved that the RCN model for the \"0-1\" loss is PAC-learnable under the condition of finite Vapnik-Chervonenkis-dimension, and van Rooyen et al. [13] summarized that the classification-calibrated loss functions are asymptotically robust to RCN, Manwani and Sastry [23] indicated that linear classifiers obtained under many other surrogate loss functions (e.g., hinge loss, exponential loss, and logistic loss) are not tolerant of label noise. Targeting a general learning framework in the presence of label noise, Natarajan et al. [9] investigated the problem of risk minimization under asymmetric RCN, where the noise rate is class-conditional. Two methods workable for most surrogate loss functions were proposed. Liu and Tao [10] addressed the class-conditional label noise problem by proposing a more general framework, which employs the importance reweighting strategy. Their theoretical results suggested that the classifiers learned by using this strategy would converge to the optimal classifiers for noise-free examples, and that the noise rate in data can be estimated with a proven upper bound. This method closely relates to the proposed method, while the differences are discussed in Section IV-E.\n\n\nIII. FORMULATION OF MULTICLASS CLASSIFICATION WITH NOISY LABELS\n\nThe goal of multiclass classification [44]- [48] is to leverage a set of n training examples to design a classifier that is capable of distinguishing m classes based on an input vector of dimension d. We follow the common technique of representing the class labels as a \"1-of-m\" indicator vector\nY = [Y 1 , Y 2 , . . . , Y m ] , such that m k=1 Y k = 1, Y k = 1\nif the corresponding example X \u2208 R d belongs to the kth class and Y k = 0 otherwise. The n training examples are represented as {(X 1 , Y 1 ), . . . , (X n , Y n )}, which are independent and identically distributed examples drawn from an underlying (noise-free) distribution D. Here, we consider to transform the multiclass classification problem into binary classification subproblems in two different ways, including one-vs-rest and one-vs-one. The one-vs-rest setting states that we will learn m classifiers { f k } m k=1 where the classifier f k separates the kth class against all the other classes. Denoting\nf = { f k } m k=1\nand predefining a surrogate loss function , we need to optimize the following expected risk:\nR ,D ( f ) = m k=1 E (X,Y )\u223cD [ ( f k (X), Y )](1)\nwhere E (X,Y )\u223cD denotes the expectation operator where the variables (X, Y ) are drawn from the distribution D. Considering that D is unknown, generally, the empirical risk is utilized to approximate (1), that i\u015d\nR ,D ( f ) = m k=1 1 n n i=1 ( f k (X i ), Y i ) .(2)\nThe one-vs-one setting involves multiple two-class classifiers with each classifying one category against another. The corresponding expected risk can be written as\nR ,D ({ f tk }) = m\u22121 t =1 m k=t +1 E (X,Y )\u223cD tk [ ( f tk (X), Y )] (3)\nwhere f tk is the classifier separating the tth and kth classes, and D tk is the distribution generating the examples of the tth and kth classes. Considering the setting that the labels are partially corrupted, we denote the obtained labels as\u0176 and the true labels as Y . Given that a proportion \u03b3 < 1 of the examples are randomly labeled, we assume that the examples in each class have the same probability of \u03b3 to be randomly processed. Without loss of generality, we denote the probability of the examples belonging to the kth class as P(Y k = 1) = \u03c1 k . Then, the correctly labeled examples in the kth class have a probability of\nP(\u0176 k = 1, Y k = 1) = \u03c1 k (1 \u2212 \u03b3 ) + \u03c1 k \u03b3 m .\nComplementarily, the examples that do not belong to the kth class but are labeled as this class have the ratio of\nP(\u0176 k = 1, Y k = 1) = (1 \u2212 \u03c1 k ) \u03b3 m .\nHence, we can obtain the following conditional probabilities via simple derivations:\nP(\u0176 k = 1|Y k = 1) = 1 \u2212 \u03b3 + \u03b3 m (4) P(\u0176 k = 1|Y k = 1) = \u03b3 \u2212 \u03b3 m (5) P(\u0176 k = 1|Y k = 1) = \u03b3 m(6)\nfor k \u2208 {1, . . . , m}.\n\nFrom (5) and (6), we know that for the kth class, the labels are randomly flipped to other classes with the probability of \u03b3 \u2212 (\u03b3 /m) (the out-flip probability); the labels of other classes are flipped to the kth with the probability of (\u03b3 /m) (the in-flip probability). This can be regarded as a label noise problem in the multiclass case; more specifically, it closely relates to the RCN model in the classic binary classification setting, where each label is flipped independently with some probability \u03c1 \u2208 [0, 0.5). More discussions about the relationship between the proposed setting and the binary setting can be found in Section IV-E. We assume that either the out-flip probability or the in-flip probability is the same for all k, indicating that we impose a symmetric RCN model on the multiclass setting. How to estimate \u03b3 is a critical issue that will be addressed in this paper. However, an asymmetric RCN model may attract more attention in practice. In this regard, we investigate in experiments the tolerance of the proposed method to the violation of the assumption, i.e., the noise is generated according to an asymmetric RCN model. On the other hand, in the asymmetric setting, the noise rate in each class is possible to be estimated, which could be a further extension of this paper; see the binary case [10].\n\nLet D \u03b3 be the distribution of the collected noisy examples {(X 1 ,\u0176 1 ), . . . , (X n ,\u0176 n )}. In the above-mentioned setting, the classifiers need to be learned by exploiting the knowledge from D \u03b3 . Many traditional classification methods crucially rely on correct labels. Inaccurate label information may increase the uncertainty of classifiers during learning, inevitably degrading the classification performances. Therefore, the challenging problems are: 1) how to estimate the proportion of the randomly labeled data (or the noise rate), which, if known, benefits the statistical learning of the classifiers; 2) how to guarantee that the learned classifiers on noisy data can converge to the optimal classifiers with respect to clean data; and 3) how to design a strategy that is applicable to any surrogate loss functions and algorithms. With this regard, we reexplore the importance reweighting strategy for multiclass classification in Section IV.\n\n\nIV. MULTICLASS CLASSIFICATION WITH IMPORTANCE REWEIGHTING\n\nDomain adaptation [49]- [51], where importance reweighting is widely used, is to exploit the knowledge of a source domain to improve the model performance in a target domain. From this perspective, the true distribution D (regarded as the target domain) is transformed to the noisy distribution D \u03b3 (regarded as the source domain) due to the noisy labels. To make a clear presentation, we use\nP D (X, Y ) [or P D \u03b3 (X, Y )] to denote the joint probability of the variables (X, Y ) under the distribution D (or D \u03b3 ), similar to P D (Y |X) and P D \u03b3 (Y |X)\n. Without the subscript D or D \u03b3 , we use Y and\u0176 to indicate the clean and noisy labels, respectively, as in (4)- (6).\n\nGiven the condition that the input vectors X are noise-free [i.e., P D (X) = P D \u03b3 (X)], we have 1\nP D (Y |X) = P D (X, Y ) P D \u03b3 (X, Y ) P D \u03b3 (Y |X).(7)\nReplacing the conditional probabilities in the above-mentioned equation with the surrogate loss function, and considering (1), we can deduce the expected risk under D \u03b3 as\nR ,D ( f ) = m k=1 E (X,Y )\u223cD [ ( f k (X), Y )] = m k=1 E (X,Y )\u223cD \u03b3 P D (X, Y ) P D \u03b3 (X, Y ) ( f k (X), Y ) .\nWe assume that the ratio (P D (X, Y )/P D \u03b3 (X, Y )) determines the contribution of the loss of each noisy example to R ,D ( f ).\n\nIn this sense, we define \u03b2(X,\nY ) = (P D (X, Y )/P D \u03b3 (X, Y )),\nwhich can be used to suppress the influence of the incorrect labels on R ,D ( f ). Accordingly, we obtain a reweighted expected risk On the other hand, in D \u03b3 , X 2 is labeled as dot, whereas it is labeled as circle in D. This indicates that P D\u03b3 (Y 2 = dot|X 2 ) and P D (Y 2 = circle|X 2 ) reach to high values, but importantly, P D (Y 2 = dot|X 2 ) has a low value (even zero). According to the definition of \u03b2, we thus have that \u03b2(X 1 ,\nR ,D ( f ) = m k=1 E (X,Y )\u223cD \u03b3 [\u03b2(X, Y ) ( f k (X), Y )] = R \u03b2 ,D \u03b3 ( f ).(8)Y 1 = dot) is large, while \u03b2(X 2 , Y 2 = dot) is small.\nThis is similar to the idea from the field of importance sampling [52] [except for the condition that P D (X) = P D \u03b3 (X)].\n\nThe same operation can also be applied to (3) to reach the corresponding reweighted risk for the one-vs-one setting, that is\nR ,D ({ f tk }) = R \u03b2 ,D \u03b3 ({ f tk }).(9)\nGiven the true distribution D (although it is unknown in practice) and the noisy distribution D \u03b3 , \u03b2(X, Y ) varies according to the example position in the data space and its label. It is expected that a correct-labeled example has a large \u03b2(X, Y ) and contributes more to the risk, while an incorrectlabeled example has a small \u03b2(X, Y ) and contributes less. Fig. 1 shows this scenario clearly.\n\nIn conclusion, the above-mentioned reweighting strategy reveals the following three remarks.\n\n1) \u03b2(X, Y ) will likely assign a low ratio to the examples that are incorrectly labeled, since P D (X, Y ) induces small values for those examples, while P D \u03b3 (X, Y ) has normal values. 2) \u03b2(X, Y ) is independent of the surrogate loss function and the multiclass classification setting (i.e., one-vs-rest or one-vs-one). 3) According to (7) \n\u03b2(X, Y ) = P D (X, Y ) P D \u03b3 (X, Y ) = P D (Y |X) P D \u03b3 (Y |X)\nwhich is useful in the estimation of \u03b2 and will be discussed in the following. In (8), the pair (X, Y ) follows the distribution D \u03b3 , meaning that the weight is estimated based on noisy example (X,\u0176 ). Thus, we can equivalently write the weight as \u03b2(X,\u0176 ).\n\n\nA. Estimation of \u03b2(X,\u0176 )\n\nSince the true distribution D is unknown, the estimation of \u03b2(X,\u0176 ) becomes difficult and can only rely on the collected noisy data. We note that for each class label k \u2208 {1, . . . , m}, \u03b2(X,\u0176 k ) can be estimated independently. Regarding this, we have the following lemma.\n\nLemma 1: For multiclass classification, when a proportion \u03b3 of data are randomly labeled, we can estimate the weights through the noisy data\n\u03b2(X,\u0176 k ) = P(\u0176 k |X) \u2212\u0176 k \u00b7 \u03b3 m \u2212 (1 \u2212\u0176 k ) \u00b7 \u03b3 1 \u2212 1 m (1 \u2212 \u03b3 )P(\u0176 k |X) . (10) \u03b2(X,\u0176 k ) is nonnegative. If P(\u0176 k |X) = 0, we set \u03b2(X,\u0176 k ) = 0.\nProof: Starting with the\u0176 k = 1 case, we have\nP(\u0176 k = 1|X) = m l=1 P(\u0176 k = 1, Y l = 1|X) = m l=1 P(\u0176 k = 1|Y l = 1, X)P(Y l = 1|X) = m l=1 P(\u0176 k = 1|Y l = 1)P(Y l = 1|X).\nAccording to (4) and (6) and m k=1 Y k = 1\nP(\u0176 k = 1|X) = (1 \u2212 \u03b3 ) + \u03b3 m P(Y k = 1|X) + l =k \u03b3 m P(Y l = 1|X) = (1 \u2212 \u03b3 ) + \u03b3 m P(Y k = 1|X) + \u03b3 m (1 \u2212 P(Y k = 1|X)) = (1 \u2212 \u03b3 )P(Y k = 1|X) + \u03b3 m .(11)\nIn the\u0176 k = 0 case, by using\nY k ={0,1} P(Y k |X) = \u0176 k ={0,1} P(\u0176 k |X) = 1(12)\nwe have\nP(\u0176 k = 0|X) = (1 \u2212 \u03b3 )P(Y k = 0|X) + \u03b3 1 \u2212 1 m .(13)\nCombining (11) and (13), we obtain\nP(\u0176 k |X) = (1 \u2212 \u03b3 )P(Y k |X) +\u0176 k \u00b7 \u03b3 m + (1 \u2212\u0176 k ) \u00b7 \u03b3 1 \u2212 1 m(14)\nwhere Y k \u2208 {0, 1}. Thus\n\u03b2(X,\u0176 k ) = P(Y k |X) P(\u0176 k |X) = P(\u0176 k |X) \u2212\u0176 k \u00b7 \u03b3 m \u2212 (1 \u2212\u0176 k ) \u00b7 \u03b3 1 \u2212 1 m (1 \u2212 \u03b3 )P(\u0176 k |X) .(15)\n\u03b2(X,\u0176 k ) is nonnegative, because P(Y k |X) \u2265 0 and P(\u0176 k |X) \u2265 0. Thus, the lemma is proved. From Lemma 1, we see that \u03b2(X,\u0176 k ) has different values for different k values, which means that each instance (X,\u0176 ) is assigned multiple weights {\u03b2(X,\u0176 1 ), . . . , \u03b2(X,\u0176 m )}. The expected risk of the one-vs-rest case is accordingly modified as\nR \u03b2 ,D \u03b3 ( f ) = m k=1 E D \u03b3 [\u03b2(X, Y k ) ( f k (X), Y k )](16)\nand that of the one-vs-one case is similar\nR \u03b2 ,D \u03b3 ({ f tk }) = m\u22121 t =1 m k=t +1 E D tk \u03b3 [\u03b2(X, Y tk ) ( f tk (X), Y tk )](17)\nwhere the reweighting strategy is reduced to a binary case. Y tk = 1 if X belongs to the tth class, while Y tk = 0 if X belongs to the kth class. \u03b2(X, Y tk ) denotes the estimation of \u03b2 for the tth and kth classes under the distribution D tk \u03b3 . From (15), we know that the computation of \u03b2(X,\u0176 k ) relies on two quantities, which are P(\u0176 k |X) and \u03b3 . These two quantities need to be estimated from noisy examples.\n\nTo estimate P(\u0176 k |X), there exist three types of approaches, including the probabilistic classification approach, the kernel density estimation approach, and the density ratio estimation approach. The probabilistic classification method highly depends on the selected classification model. When the model is misspecified, this method may introduce a large approximation error for learning the conditional distribution. The kernel density estimation method has a slow convergence rate when estimating P(X|\u0176 k ) and P(X) separately. Also, it needs a large amount of data examples for estimation. Considering these shortages, we choose the density ratio estimation approach [53], which is an effective way to alleviate the curse of dimensionality for the estimation of high-dimensional variables.\n\nThe density ratio estimation can be realized by three types of approaches [54]: probabilistic classification, moment matching, and ratio matching. As already proved [55], the moment matching approaches and the ratio matching approaches can produce lower approximation error and higher efficiency than the probabilistic classification methods. Therefore, we employ a ratio matching method, namely, KLIEP [56], to estimate P(\u0176 k |X). The reliability and the consistency assurance of this method have already been proved in [10].\n\n\nB. Estimation of \u03b3\n\nThe noise rate is a critical element and needs to be known in most algorithms designed for the RCN problem [57]- [59], as well as in our work. According to (5) and (6), the noise rate depends on the proportion \u03b3 , which is thus required. How to estimate \u03b3 is still a challenging problem, and there exist very limited approaches to accomplish this task. Next, we provide a theoretical result on the upper bound of \u03b3 , and illustrate that under mild assumptions, \u03b3 can be efficiently estimated through the proven upper bound.\n\nTheorem 1:  (19) where X is the support of X. Proof: According to (11), if there exists X \\k \u2208 X , such that P D (Y k = 1|X \\k ) = 0, where X \\k denotes the example that does not belong to the kth class under the distribution D, we have\nLet A X = m \u00b7 min k P D \u03b3 (Y k = 1|X) and B X = (m/m \u2212 1) \u00b7 min k P D \u03b3 (Y k = 0|X), we have \u03b3 \u2264 min(A X , B X ).(18)P D \u03b3 (Y k = 1|X \\k ) = \u03b3 m .(20)\nSimilarly, if there exists X k \u2208 X , such that P D (Y k = 0|X k ) = 0, where X k is the example of the kth class under D, (13) means that\nP D \u03b3 (Y k = 0|X k ) = \u03b3 1 \u2212 1 m .(21)\nConsidering P D (Y k = 1|X) \u2265 0 and P D (Y k = 0|X) \u2265 0 for all k \u2208 {1, . . . , m}, we have\nP D \u03b3 (Y k = 1|X) \u2265 \u03b3 m(22)\nand\nP D \u03b3 (Y k = 0|X) \u2265 \u03b3 1 \u2212 1 m .(23)\nBy combining (22) and (23), both \u03b3 \u2264 A X and \u03b3 \u2264 B X hold. Thus, the inequality in (18) is proved. For multiclass classification, it is possible to find an example X \u2208 X , which is far from the classification hyperplane, such that P D (Y k = 1|X) [or P D (Y k = 0|X)] tends to zero. In this case, the above-mentioned assumption is satisfied, and the equality in (18) holds. Hence, the estimation of \u03b3 can be directly deduced as in (19).\n\nThe theorem is proved. Theorem 1 provides a consistent estimator of \u03b3 under the condition that there exists X \\k or X k \u2208 X , such that P D (Y k = 1|X \\k ) = 0 or P D (Y k = 0|X k ) = 0. According to (19), the convergence rate of estimating \u03b3 is the same as that of estimating the conditional distribution P D \u03b3 (Y k |X). The estimation is thus efficient, and its effectiveness is verified in experiments.\n\n\nC. Convergence Analysis\n\nHere, we provide theoretical analyses on the convergence of the proposed importance reweighting strategy in multiclass classification. We show that with a sufficiently large amount of noisy data, the classifier learned using the proposed strategy can converge to the classifier, which is optimal for the clean data.\n\nTheorem 2: Assume that there exists X \\k or X k \u2208 X , such that P D (Y k = 1|X \\k ) = 0 or P D (Y k = 0|X k ) = 0. When the loss function is convex and the number of training examples n \u2192 \u221e, we have\nf n = arg min f \u2208FR\u03b2 ,D \u03b3 \u2192 f * = arg min f \u2208F R ,D(24)\nwhere f n is the minimizer of the empirical riskR\u03b2 ,D \u03b3 with n examples,\u03b2 is the weight by empirically learning the conditional density function P D \u03b3 (Y k |X) and the proportion \u03b3 , R denotes the empirical risk, and F is a predefined hypothesis class.\n\nProof: When the sample size goes to infinity, the conditional distribution P D \u03b3 (Y k |X) can be unbiasedly estimated by empolying some nonparametric methods, such as the Parzen window approach. This means that under the assumption in Theorem 2, the weight \u03b2(X,\u0176 k ) can also be unbiasedly estimated. We have that\nR\u03b2 ,D \u03b3 ( f n ) \u2212 R\u03b2 ,D \u03b3 ( f * ) = R\u03b2 ,D \u03b3 ( f n ) \u2212R\u03b2 ,D \u03b3 ( f n ) +R\u03b2 ,D \u03b3 ( f n ) \u2212R\u03b2 ,D \u03b3 ( f * ) +R\u03b2 ,D \u03b3 ( f * ) \u2212 R\u03b2 ,D \u03b3 ( f * ) \u2264 R\u03b2 ,D \u03b3 ( f n ) \u2212R\u03b2 ,D \u03b3 ( f n ) +R\u03b2 ,D \u03b3 ( f * ) \u2212 R\u03b2 ,D \u03b3 ( f * ) \u2264 2 sup f \u2208F |R\u03b2 ,D \u03b3 ( f ) \u2212R\u03b2 ,D \u03b3 ( f )|.(25)\nThe first inequality holds, because f n is the minimizer of and (8), which means that f n approaches f * .\nR\u03b2 ,D \u03b3 ( f ) goes to R \u03b2 ,D \u03b3 ( f ) = R ,D ( f ) because of\n\nD. Two Instantiations\n\nAccording to (8) and (9), we can state that any surrogate loss functions designed for either the one-vs-rest or the onevs-one setting of multiclass classification can be applied in the presence of randomly labeled data by employing the proposed importance reweighting strategy. Given that the conditional probability P D \u03b3 (Y k |X) and the proportion \u03b3 are accurately estimated, as the number of training examples increases, the classifiers learned using the noisy data are toward the optimal classifiers for the clean data according to Theorem 2. For empirical verification, in this section, we give two instantiations of the importance reweighting strategy, including MLR, which belongs to the one-vs-rest setting, and SVM, which belongs to the one-vs-one setting.\n\n1) Importance Reweighted Multinomial Logistic Regression: MLR [60] predicts the probability of each example belonging to each class. Specifically, the probability that X belongs to the kth class is\nP(Y k = 1|X, W) = exp W k X m l=1 exp W l X(26)\nwhere W is the parameter matrix for multiclass classification, W k encodes the parameters for the kth class, and T denotes the transpose of a matrix. The associated empirical risk for optimization is written as the reweighted negative log-likelihood\nR(W) = \u2212 1 n n i=1 log P(Y i |X i , W).(27)\nConsidering that noisy data are observed, the empirical risk is then modified by applying the importance reweighting strategy according to (8), and we obtain\nR \u03b2 (W) = \u2212 1 n n i=1 \u03b2(X i ,\u0176 i ) log P(\u0176 i |X i , W) = \u2212 1 n n i=1 m k=1 \u03b2 X i ,\u0176 k i log P \u0176 k i |X i , W k = \u2212 1 n n i=1 m k=1 \u03b2 X i ,\u0176 k i \u00d7 \u0176 k i W k X i \u2212 log m l=1 exp W l X i . (28)\nTo avoid overfitting, a Frobenius regularization on W can be imposed. The objective function become\u015d\nR \u03b2 (W) = \u2212 1 n n i=1 m k=1 \u03b2 X i ,\u0176 k i \u00d7 \u0176 k i W k X i \u2212 log m l=1 exp W l X i + \u03bb W 2 F(29)\nwhere \u03bb is the regularization parameter. The above-mentioned function can be optimized by utilizing the Limited-memory Broyden-Fletcher-Goldfarb-Shanno algorithm algorithm, as in the original MLR.\n\n2) Importance Reweighted Support Vector Machine: Regarding SVM, we consider the one-vs-one setting [61]. There are (m(m \u2212 1)/2) classifiers involved in this setting, with each one learned independently. Assume that the classifier between the tth and kth classes is (30) where (W tk , b tk ) denotes the parameters to be optimized. Then, the associated objective function is\nf (X; W tk ) = W tk X + b tkmin W tk ,b tk ,\u03be tk 1 2 W tk 2 + C n i=1 \u03be tk i s.t. W tk X i + b tk \u2265 1 \u2212 \u03be tk i , if\u0176 tk i = 1 W tk X i + b tk \u2264 \u22121 + \u03be tk i , if\u0176 tk i = 0 \u03be tk i \u2265 0, i = 1, . . . , n(31)\nwhere \u03be tk i is the slack variable. In (31), \u03be tk i controls the margin of the example X i to the classification hyperplane. In other words, \u03be tk i reflects the influence of X i on the learning of the classifier. Hence, by reweighting \u03be tk i , we can accomplish the goal of (8). The modified objective function is as follows:\nmin W tk ,b tk ,\u03be tk 1 2 W tk 2 + C n i=1 \u03b2(X i ,\u0176 tk i )\u03be tk i s.t. W tk X i + b tk \u2265 1 \u2212 \u03be tk i , if\u0176 tk i = 1 W tk X i + b tk \u2264 \u22121 + \u03be tk i , if\u0176 tk i = 0 \u03be tk i \u2265 0, i = 1, . . . , n.(32)\nThe optimization of the above-mentioned problem is similar to that of the original SVM [see (31)], by using the conventional quadratic programming algorithm 2 or more effective algorithms [62], [63]. While (28) and (32) consider the linear case, using a nonlinear kernel is preferred to improve the classification performance on nonlinear data. Specifically, the polynomial kernel is used in this paper. However, due to the heavy computational costs of calculating the kernel matrix, we use linear classifiers in experiments, if unspecified.\n\n\nE. Discussion\n\nBoth our method and [10] employ the importance reweighting strategy to handle the label noise problem, but in different classification settings. The achieved results have noticeable differences. The weight estimation is generalized from binary classes to multiple classes. Specifically, assuming the noise rate for class\u0176 \u2208 {+1, \u22121} is \u03c1\u0176 (i.e., \u03c1 +1 for the positive class and \u03c1 \u22121 for the negative class), the weight is estimated via\n\u03b2(X,\u0176 ) = P(\u0176 |X) \u2212 \u03c1 \u2212\u0176 (1 \u2212 \u03c1 +1 \u2212 \u03c1 \u22121 )P(\u0176 |X)(33)\nwhere \u2212\u0176 is the opposite label of\u0176 . This assumes asymmetric RCN, i.e., \u03c1 +1 , differs from \u03c1 \u22121 . In this paper, we analyze the noise rate for each class according to (5) and (6), and obtain\n\u03c1 k 0 = P(\u0176 k = 1|Y k = 1) (34) \u03c1 k 1 = P(\u0176 k = 1|Y k = 1).(35)\nAccording to Lemma 1,(15) can be rewritten as\n\u03b2(X,\u0176 k ) = P(\u0176 k |X) \u2212\u0176 k \u00b7 \u03c1 k 1 \u2212 (1 \u2212\u0176 k ) \u00b7 \u03c1 k 0 1 \u2212 \u03c1 k 1 \u2212 \u03c1 k 0 P(\u0176 k |X) .(36)\nWe notice that \u03c1 k 0 differs from \u03c1 k 1 in general, which reveals an asymmetric attribute for class k. Considering all classes, either \u03c1 k 0 or \u03c1 k 1 is different with respect to each class. Hence, all noise rates are required to be estimated, which is substantially different from binary classification. Considering the difficulty of such estimation, we simplify the classification setting by enforcing the in-flip probability \u03c1 k 1 as well as the out-flip probability \u03c1 k 0 to be the same for all classes. Then, the noise rates for different classes are determined by a quantity \u03b3 , which can be estimation according to Theorem 1. Theoretical analyses are provided to guarantee the convergence of the learned classifier by using our strategy toward the optimal classifier on noise-free data.\n\nWe also investigate the relationship between our method and the deep learning-based method [16], [64], which models label flip noise by employing a labeling transition matrix and estimates it using plenty of data. Specifically, in [16], the m\u00d7m labeling transition matrix is\nQ = \u23a1 \u23a2 \u23a3 P(\u0176 1 = 1|Y 1 = 1) . . . P(\u0176 1 = 1|Y m = 1)\n. . . . . . . . .\nP(\u0176 m = 1|Y 1 = 1) . . . P(\u0176 m = 1|Y m = 1) \u23a4 \u23a5 \u23a6.(37)\n2 LIBSVM has published an application program interface for the weighted SVM: https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/#weights_for_data_instances. Q is realized as a linear fully connected layer built upon the base deep model. As expected, the output of the base model predicts the true label of an example. By using the Q layer, the true label is transformed to a noisy label. In this way, the learning of the model can be supervised by the collected noisy data. To restrict the parameter space of Q, a regularizer tr(Q) is added to the objective, where tr(\u00b7) is the trace of a matrix. In their method, the authors impose an asymmetric noise assumption on Q, i.e., the elements in Q are different with respect to each other. This introduces an extremely difficult problem, which is alleviated through a thorough learning on extensive data. However, the accuracy of the learned Q cannot be guaranteed, even though a good performance improvement is achieved in experiments. Differently, in our method, we assume a symmetric noise model, meaning that the diagonal elements of Q are the same, and the off-diagonal elements are identical. All parameters in Q are determined by the proportion \u03b3 , thus alleviating the difficulty of the problem. Applying the proposed importance reweighting strategy to handle asymmetric RCN in the multiclass case is possible, but beyond the scope of this paper. A possible solution is to derive an upper bound for each element of Q by considering the statistics of the examples in each class.\n\n\nV. EXPERIMENTS\n\nIn this section, we conduct experiments on both synthetic and real data. To verify the effectiveness of the proposed method, the traditional classifiers, including MLR and SVM, are selected as baselines, and the robust models, including robust Gaussian process (RGP) [14] and Rob_MAd [18], are employed as competitors for comparison. Following [18], we use the decision tree as the base learners of Rob_MAd. We denote the two instantiations in Section IV-D as the IWMLR and the IWSVM, respectively. In each experiment, we conduct 5 \u00d7 2-fold cross validation to compare the abovementioned methods, that means, we repeat twofold cross validation for five times. The accuracy of the classification result is defined as the ratio of the correctly classified examples with respect to the true distribution D. Average accuracy in the 5\u00d72-fold cross validation is calculated in each experiment setting. Note that during training, there are hyperparameters that should be set for the classifiers, for example, \u03bb in MLR and C in SVM. For this, a held-out validation set with 20% training data is randomly collected from the training set, and is used for selecting the best hyperparameter from a finite set of possible values.\n\nWe synthesize noisy data from clean data by stochastically changing a set of the labels: \u03b3 percent of data examples are randomly selected from the training set by ignoring the true labels, and then, these examples are assigned to the labels that are generated from a uniform distribution on {1, . . . , m}. This operation is also applied to the validation data, but not to the test data. In experiments, the considered values of \u03b3 include \n\n\nA. Synthetic Data\n\nWe synthesized the 2-D linearly separable data set, as shown in Fig. 2(a), which has five classes: cyan diamond  Fig. 2(b). In this experiment, MLR is utilized as the classifier. We visualize the classification results of the methods in Fig. 2(d)-(f), where we denote by IWMLR \u03b3 * the IWMLR method that uses the true value of \u03b3 rather than the estimated value. As shown in Fig. 2(d)-(f), the examples that are the most difficult for classification are around the coordinate origin, due to the influence of noisy examples. In the case of MLR [ Fig. 2(d)], we see that the classification hyperplanes of azure versus cyan, azure versus red, purple versus red, and blue versus cyan are significantly biased. We note that the predicted labels tend to the cyan and red classes, which may be due to the imbalanced distribution of the classes and the influence of noisy examples. By contrast, the classification hyperplanes move toward the ground truth [ Fig. 2(c)] by using the proposed weighting strategy, as shown in Fig. 2(e). If we   Fig. 2 indicates that the proposed method can achieve a considerable improvement (above 2%) over the original MLR.\n\nNext, we investigate the influence of the estimation of \u03b3 with respect to the classification accuracy. We compare the performances of MLR, IWMLR, and IWMLR \u03b3 * for the considered \u03b3 values. Fig. 3 plots the results, from which we see that there exist perceived biases between the estimated \u03b3 values and the true values. Despite this, the biases do not have significant impacts on the classification accuracy, and IWMLR works almost as well as IWMLR \u03b3 * , and is better than MLR in all cases. In the noise-free case, i.e., \u03b3 = 0, all the classifiers achieve the same accuracy. All these inform us that the method of estimating \u03b3 proposed in Theorem 1 is effective, and moreover, the proposed importance reweighting strategy is tolerant to the biases introduced by the \u03b3 estimation.\n\n\nB. Real Data\n\nWe carried out experiments on seven data sets from the LIBSVM repository 3 to evaluate the classification performance of RGP, Rob_MAd, SVM, IWSVM, MLR, and IWMLR when different proportions of noisy labels were present in the data. The information about the data sets is summarized in Table I reported. We also note that RGP and Rob_MAd are nonlinear classifiers, which perform better than the linear classifiers on nonlinear data. To make a fair comparison on smallsized data sets, including glass and vehicle, we realized the other methods, including SVM, IWSVM, MLR, and IWMLR, by using the polynomial kernel to boost their performances. On the large-sized data sets, including dna, letter, protein, satimage, and sector, linear SVM and MLR classifiers were utilized, while the decision tree was used in Rob_MAd. Table II details the results of different methods on each data set. We have applied a significance test (i.e., the F-test) on the 5 \u00d7 2-fold cross-validation results obtained by SVM and IWSVM, and also on the results obtained by MLR and IWMLR. Specifically, in each data set and \u03b3 case, the 5 \u00d7 2-fold cross-validation results in five averaged accuracies for each classifier. The F-test is conducted on, for example, the five accuracies of SVM and the five accuracies of IWSVM. Such a significance test is to statistically evaluate the significance of the performance difference between IWSVM and SVM, and the significance of the difference between IWMLR and MLR. The comparison indicates that the proposed importance reweighting strategy significantly improves the performance of the classifiers in most settings. For example, in all cases, IWMLR and IWSVM perform better than MLR and SVM, respectively. RGP and Rob_MAd perform poorly compared with the proposed methods. Notably, the strategy used in either RGP or Rob_MAd is nontrivial to be applied to other surrogate loss functions, whereas our method can be easily used to assist a well-performing classifier when handling noisy data. The standard deviations of the results on small data sets are larger than that on large data sets due to insufficient training samples. Overall, we observe that the proposed method can enhance the noise tolerance of the classifiers to high amounts of label noise in the data.\n\nNote that the efficiency of the proposed strategy is independent of any specified surrogate loss function and any multiclass classification setting. One main factor which may affect the applicability of our method to large data sets is the estimation of the conditional probability P D \u03b3 (Y |X), where we resort to the method of KLIEP. Since KLIEP requires numerical optimization to obtain the solution [56], this process would be slow when the number of training examples becomes too large. However, empirical experience tells us that KLIEP can be applied on a moderately sized data set.\n\n\nC. Effects of Asymmetric RCN Model\n\nThe previous experiments are based on a symmetric noise assumption, saying that the proportion \u03b3 is identical for all classes. In this section, we investigate the performance of the proposed method when the assumption is violated. To simulate the data generation process under an asymmetric RCN model, we operate as follows. Given a data set containing m classes, we randomly choose 40% \u00d7 m classes, such that the examples in those classes are not corrupted by noise. For the rest 60% \u00d7 m classes, \u03b3 proportion of those examples are reassigned random labels as before. In this way, the resultant label noise exhibits an asymmetric property. The compared methods include RGP, Rob_MAd, SVM, IWSVM, MLR, and IWMLR. The specifications for each method and the data sets selected in this experiment are the same as those in Section V-B. Note that \u03b3 is estimated over the whole data set, rather than over the corrupted classes. The weight \u03b2(X,\u0176 ) is estimated for all examples in a data set. Table III lists the performances of different methods on the generated data. F-test is again employed to statistically assess the significance of the performance difference between IWSVM and SVM and that between IWMLR and MLR. From those results, we can conclude that even though the symmetric noise assumption is violated, the performances of all methods are consistent with those in Table II. Specifically, the proposed importance reweighting strategy can improve the performances of both SVM and MLR according to the marked values by significant test. On average, the IW variants achieve the best performances in all data sets. Notably, although the asymmetric assumption may bias the estimation of \u03b3 according to Section IV-B, the classifiers can still benefit from the proposed importance reweighting strategy. The results in Tables II and III suggest that in the presence of either asymmetric or symmetric noise, the learning of the classifiers can be easily biased by equally treating the influence of each example. Instead, reweighting according to the importance of the examples is a preferred way to suppress the effects of noise, demonstrating the effectiveness of the proposed method.\n\n\nVI. CONCLUSION\n\nCollecting extensive training data for an intelligent learning system is an extremely time-consuming and labor-consuming task. Employing nonexperts may reduce the cost, but cannot guarantee the correctness of the obtained labels. Therefore, how to alleviate the difficulty of the learning system by utilizing inaccurate data remains an open issue. In this paper, we target the problem of multiclass classification of the data, a proportion of which are randomly labeled. We show that such an issue is indeed a label noise problem, or more specifically, a label flip noise problem in the multiclass classification. Considering that the in-flip and out-flip probabilities of any one class are the same as those of the other classes, we impose a symmetric RCN model on the multiclass setting. We reexplore the importance reweighting strategy to address this problem, which is applicable to any traditional surrogate loss functions and to different multiclass classification settings. Our theoretical results provide an effective method of weight estimation, and indicate that the proportion of randomly labeled examples can be estimated with a proven upper bound. Convergence analyses are also provided to assure that the learned classifier is consistent with the optimal classifier for noise-free data. We apply the proposed strategy to two conventional classifiers, including MLR and SVM. Experimental results on synthetic and real data demonstrate the effectiveness of the proposed strategy for handling the noisy labels, and also verify that our method is not significantly affected when the symmetric noise assumption is violated.\n\nFurther work will concentrate on asymmetric noise in the multiclass setting. Specifically, an accurate estimation of the weights will be researched. In addition, the estimation of noise rate for each class is a critical yet difficult issue that needs to be solved.\n\nFig. 1 .\n1Illustration of noisy labels. Left: two examples including a correctly labeled example X 1 and an incorrectly labeled example X 2 . Right: underlying noiseless examples. In both D \u03b3 and D, X 1 is labeled as dot, saying that P D\u03b3 (Y 1 = dot|X 1 ) and P D (Y 1 = dot|X 1 ) reach to high values (ideally one).\n\n\nR\u03b2 ,D \u03b3 and thusR\u03b2 ,D \u03b3 ( f n )\u2212R\u03b2 ,D \u03b3 ( f * ) \u2264 0. Using the law of large numbers, when there are a sufficiently large number of examples, we have that R\u03b2 ,D \u03b3 ( f ) \u2212R\u03b2 ,D \u03b3 ( f ) goes to zero and that\u03b2 goes to \u03b2. Hence, R\u03b2 ,D \u03b3 ( f n ) goes to R\u03b2 ,D \u03b3 ( f * )\n\n\n[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7].\n\nFig. 2 .\n2Classification of the synthetic data. The classification planes are plotted as lines among the classes. (a) Noise-free data. (b) Noisy training data when \u03b3 = 0.3. (c) Test data. (d) Classification results of MLR (89.80%). (e) Classification results of IWMLR (91.96%). (f) Classification results of IWMLR \u03b3 * (94.40%). (top-left corner), blue plus (top-right), purple circle (right), red cross (bottom-right), and azure triangle (bottom-left). The classification hyperplanes pass through the coordinate origin. The training and test sets contain 7000 and 5000 examples, respectively. The percentages of the examples in those classes are 31%, 16%, 11%, 29%, and 13%, respectively. The noisy training data are synthesized by setting \u03b3 = 0.3, as in\n\nFig. 3 .\n3Accuracy comparison of MLR, IWMLR, and IWMLR \u03b3 * on synthetic data. \u03b3 e denotes the estimated value of \u03b3 in IWMLR.\n\n\nAuthorized licensed use limited to: University of Sydney. Downloaded on May 16,2020 at 12:10:35 UTC from IEEE Xplore. Restrictions apply.Furthermore, \u03b3 can be estimated via \n\n\u03b3 = min min \n\nX\u2208X \n\nA X , min \n\nX\u2208X \n\nB X \n\n\nTABLE I\nISTATISTICS OF THE DATA SETS use the true value \u03b3 * , the predicted hyperplanes are more accurate [see Fig. 2(f)].\n\n\n. The competitor RGP was shown to have the complexity of O(n 3 ) [14], failing to be applied on data sets of large size. Thus, the results of RGP on glass and vehicle are only 3 https://www.csie.ntu.edu.tw/ cjlin/libsvmtools/datasets/multiclass.html\n\nTABLE II\nIIACCURACY (%) COMPARISON OF DIFFERENT MULTICLASS CLASSIFICATION ALGORITHMS ON REAL DATA. \u2022 INDICATES THAT IWSVM (IWMLR) IS SIGNIFICANTLY BETTER THAN SVM (MLR) AT LEVEL p < 0.05 IN F-TEST. THE BEST AVERAGE RESULT OF ALL \u03b3 CASES IN EACH DATA SET IS MARKED BY BOLD\n\nTABLE III\nIIIACCURACY (%) COMPARISON OF DIFFERENT MULTICLASS CLASSIFICATION ALGORITHMS ON ASYMMETRICALLY NOISY DATA. \u2022 INDICATES THAT IWSVM (IWMLR) IS SIGNIFICANTLY BETTER THAN SVM (MLR) AT LEVEL p < 0.05 IN F-TEST. THE BEST AVERAGE RESULT OF ALL \u03b3 CASES IN EACH DATA SET IS MARKED BY BOLD\nAuthorized licensed use limited to: University of Sydney. Downloaded on May 16,2020 at 12:10:35 UTC from IEEE Xplore. Restrictions apply.\nAccording to the definitions in Section III, D is a distribution generating the examples {(X i , Y i )}, rather than the examples {(X i , Y i )} form the distribution D. This is important for understanding that P D (Y |X) = 1. The same condition is applied in P D\u03b3 (Y |X).\n\nImageNet: A large-scale hierarchical image database. J Deng, W Dong, R Socher, L.-J Li, K Li, L Fei-Fei, Proc. CVPR. CVPRJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, \"ImageNet: A large-scale hierarchical image database,\" in Proc. CVPR, 2009, pp. 248-255.\n\nRobust extreme multi-label learning. C Xu, D Tao, C Xu, Proc. SIGKDD. SIGKDDC. Xu, D. Tao, and C. Xu, \"Robust extreme multi-label learning,\" in Proc. SIGKDD, Aug. 2016, pp. 13-17.\n\nBeyond RPCA: Flattening complex noise in the frequency domain. Y Wang, C Xu, C Xu, D Tao, Proc. AAAI. AAAIY. Wang, C. Xu, C. Xu, and D. Tao, \"Beyond RPCA: Flattening complex noise in the frequency domain,\" in Proc. AAAI, 2017, pp. 500-505.\n\nCrowdsourcing: How the Power of the Crowd is Driving the Future of Business. J Howe, Random HouseNew York, NY, USAJ. Howe, Crowdsourcing: How the Power of the Crowd is Driving the Future of Business. New York, NY, USA: Random House, 2008.\n\nQuality management on Amazon mechanical turk. P G Ipeirotis, F Provost, J Wang, Proc. ACM SIGKDD Workshop Human Comput. ACM SIGKDD Workshop Human ComputP. G. Ipeirotis, F. Provost, and J. Wang, \"Quality management on Amazon mechanical turk,\" in Proc. ACM SIGKDD Workshop Human Comput., 2010, pp. 64-67.\n\nRandom classification noise defeats all convex potential boosters. P M Long, R A Servedio, Mach. Learn. 783P. M. Long and R. A. Servedio, \"Random classification noise defeats all convex potential boosters,\" Mach. Learn., vol. 78, no. 3, pp. 287-304, 2010.\n\nLearning from noisy examples. D Angluin, P Laird, Mach. Learn. 24D. Angluin and P. Laird, \"Learning from noisy examples,\" Mach. Learn., vol. 2, no. 4, pp. 343-370, 1988.\n\nOn the sample complexity of noisetolerant learning. J A Aslam, S E Decatur, Inf. Process. Lett. 574J. A. Aslam and S. E. Decatur, \"On the sample complexity of noise- tolerant learning,\" Inf. Process. Lett., vol. 57, no. 4, pp. 189-195, 1996.\n\nLearning with noisy labels. N Natarajan, I S Dhillon, P K Ravikumar, A Tewari, Proc. NIPS. NIPSN. Natarajan, I. S. Dhillon, P. K. Ravikumar, and A. Tewari, \"Learning with noisy labels,\" in Proc. NIPS, 2013, pp. 1196-1204.\n\nClassification with noisy labels by importance reweighting. T Liu, D Tao, IEEE Trans. Pattern Anal. Mach. Intell. 383T. Liu and D. Tao, \"Classification with noisy labels by importance reweighting,\" IEEE Trans. Pattern Anal. Mach. Intell., vol. 38, no. 3, pp. 447-461, Mar. 2016.\n\nRobust regression using maximum-likelihood weighting and assuming cauchy-distributed random error. H R Moore, Tech. Rep. DTICH. R. Moore et al., \"Robust regression using maximum-likelihood weighting and assuming cauchy-distributed random error,\" DTIC, Fort Belvoir, VA, USA, Tech. Rep., Jun. 1977.\n\nLearning via Gaussian herding. K Crammer, D D Lee, Proc. NIPS. NIPSK. Crammer and D. D. Lee, \"Learning via Gaussian herding,\" in Proc. NIPS, 2010, pp. 451-459.\n\nLearning with symmetric label noise: The importance of being unhinged. B Van Rooyen, A Menon, R C Williamson, Proc. NIPS. NIPSB. van Rooyen, A. Menon, and R. C. Williamson, \"Learning with symmetric label noise: The importance of being unhinged,\" in Proc. NIPS, 2015, pp. 10-18.\n\nRobust multi-class Gaussian process classification. D Hern\u00e1ndez-Lobato, J M Hern\u00e1ndez-Lobato, P Dupont, Proc. NIPS. NIPSD. Hern\u00e1ndez-Lobato, J. M. Hern\u00e1ndez-Lobato, and P. Dupont, \"Robust multi-class Gaussian process classification,\" in Proc. NIPS, 2011, pp. 280-288.\n\nMulti-class classification in the presence of labelling errors. J Bootkrajang, A Kab\u00e1n, Proc. ESANN. ESANNJ. Bootkrajang and A. Kab\u00e1n, \"Multi-class classification in the presence of labelling errors,\" in Proc. ESANN, 2011, pp. 345-350.\n\nTraining convolution networks with noisy labels. S Sukhbaatar, J Bruna, M Paluri, L Bourdev, R Fergus, Proc. ICLR Workshop Track. ICLR Workshop TrackS. Sukhbaatar, J. Bruna, M. Paluri, L. Bourdev, and R. Fergus, \"Training convolution networks with noisy labels,\" in Proc. ICLR Workshop Track, 2015, pp. 1-11.\n\nAnalyzing the presence of noise in multi-class problems: Alleviating its influence with the onevs-one decomposition. J A S\u00e1ez, M Galar, J Luengo, F Herrera, Knowl. Inf. Syst. 381J. A. S\u00e1ez, M. Galar, J. Luengo, and F. Herrera, \"Analyzing the presence of noise in multi-class problems: Alleviating its influence with the one- vs-one decomposition,\" Knowl. Inf. Syst., vol. 38, no. 1, pp. 179-206, 2014.\n\nA robust multi-class adaboost algorithm for mislabeled noisy data. B Sun, S Chen, J Wang, H Chen, Knowl.-Based Syst. 102B. Sun, S. Chen, J. Wang, and H. Chen, \"A robust multi-class adaboost algorithm for mislabeled noisy data,\" Knowl.-Based Syst., vol. 102, pp. 87-102, Jun. 2016.\n\nMulticlass semisupervised learning based upon kernel spectral clustering. S Mehrkanoon, C Alzate, R Mall, R Langone, J A K Suykens, IEEE Trans. Neural Netw. Learn. Syst. 264S. Mehrkanoon, C. Alzate, R. Mall, R. Langone, and J. A. K. Suykens, \"Multiclass semisupervised learning based upon kernel spectral cluster- ing,\" IEEE Trans. Neural Netw. Learn. Syst., vol. 26, no. 4, pp. 720-733, Apr. 2015.\n\nA theory of the learnable. L G Valiant, Commun. ACM. 2711L. G. Valiant, \"A theory of the learnable,\" Commun. ACM, vol. 27, no. 11, pp. 1134-1142, 1984.\n\nImproved lower bounds for learning from noisy examples: An information-theoretic approach. C Gentile, D P Helmbold, Proc. Annu. Conf. Comput. Learn. Theory. Annu. Conf. Comput. Learn. TheoryC. Gentile and D. P. Helmbold, \"Improved lower bounds for learning from noisy examples: An information-theoretic approach,\" in Proc. Annu. Conf. Comput. Learn. Theory, 1998, pp. 104-115.\n\nCorrentropy: Properties and applications in non-Gaussian signal processing. W Liu, P P Pokharel, J C Pr\u00edncipe, IEEE Trans. Signal Process. 5511W. Liu, P. P. Pokharel, and J. C. Pr\u00edncipe, \"Correntropy: Properties and applications in non-Gaussian signal processing,\" IEEE Trans. Signal Process., vol. 55, no. 11, pp. 5286-5298, Nov. 2007.\n\nNoise tolerance under risk minimization. N Manwani, P Sastry, IEEE Trans. Cybern. 433N. Manwani and P. Sastry, \"Noise tolerance under risk minimization,\" IEEE Trans. Cybern., vol. 43, no. 3, pp. 1146-1151, Jun. 2013.\n\nBayesian robust tensor factorization for incomplete multiway data. Q Zhao, G Zhou, L Zhang, A Cichocki, S.-I Amari, IEEE Trans. Neural Netw. Learn. Syst. 274Q. Zhao, G. Zhou, L. Zhang, A. Cichocki, and S.-I. Amari, \"Bayesian robust tensor factorization for incomplete multiway data,\" IEEE Trans. Neural Netw. Learn. Syst., vol. 27, no. 4, pp. 736-748, Apr. 2016.\n\nRBoost: Label noise-robust boosting algorithm based on a nonconvex loss function and the numerically stable base learners. Q Miao, Y Cao, G Xia, M Gong, J Liu, J Song, IEEE Trans. Neural Netw. Learn. Syst. 2711Q. Miao, Y. Cao, G. Xia, M. Gong, J. Liu, and J. Song, \"RBoost: Label noise-robust boosting algorithm based on a nonconvex loss function and the numerically stable base learners,\" IEEE Trans. Neural Netw. Learn. Syst., vol. 27, no. 11, pp. 2216-2228, Nov. 2016.\n\nAdditive logistic regression: A statistical view of boosting (with discussion and a rejoinder by the authors). J Friedman, T Hastie, R Tibshirani, Ann. Statist. 282J. Friedman, T. Hastie, and R. Tibshirani, \"Additive logistic regression: A statistical view of boosting (with discussion and a rejoinder by the authors),\" Ann. Statist., vol. 28, no. 2, pp. 337-407, 2000.\n\nAn adaptive version of the boost by majority algorithm. Y Freund, Mach. Learn. 433Y. Freund, \"An adaptive version of the boost by majority algorithm,\" Mach. Learn., vol. 43, no. 3, pp. 293-318, 2001.\n\nIdentifying and correcting mislabeled training instances. J.-W Sun, F.-Y Zhao, C.-J Wang, S.-F Chen, Proc. Future Generat. Future Generat1J.-W. Sun, F.-Y. Zhao, C.-J. Wang, and S.-F. Chen, \"Identifying and correcting mislabeled training instances,\" in Proc. Future Generat. Commun. Netw., vol. 1. Dec. 2007, pp. 244-250.\n\nIdentifying and handling mislabelled instances. F Muhlenbach, S Lallich, D A Zighed, J. Intell. Inf. Syst. 221F. Muhlenbach, S. Lallich, and D. A. Zighed, \"Identifying and handling mislabelled instances,\" J. Intell. Inf. Syst., vol. 22, no. 1, pp. 89-109, 2004.\n\nRobust decision trees: Removing outliers from databases. G H John, Proc. KDD. KDDG. H. John, \"Robust decision trees: Removing outliers from databases,\" in Proc. KDD, 1995, pp. 174-179.\n\nInstance-based learning algorithms. D W Aha, D Kibler, M K Albert, Mach. Learn. 61D. W. Aha, D. Kibler, and M. K. Albert, \"Instance-based learning algorithms,\" Mach. Learn., vol. 6, no. 1, pp. 37-66, 1991.\n\nEfficient noise-tolerant learning from statistical queries. M Kearns, J. ACM. 456M. Kearns, \"Efficient noise-tolerant learning from statistical queries,\" J. ACM, vol. 45, no. 6, pp. 983-1006, 1998.\n\nEstimating a kernel fisher discriminant in the presence of label noise. N D Lawrence, B Sch\u00f6lkopf, Proc. ICML. ICMLN. D. Lawrence and B. Sch\u00f6lkopf, \"Estimating a kernel fisher dis- criminant in the presence of label noise,\" in Proc. ICML, 2001, pp. 306-313.\n\nSupport vector machines under adversarial label noise. B Biggio, B Nelson, P Laskov, Proc. ACML. ACMLB. Biggio, B. Nelson, and P. Laskov, \"Support vector machines under adversarial label noise,\" in Proc. ACML, 2011, pp. 97-112.\n\nLoss factorization, weakly supervised learning and label noise robustness. G Patrini, F Nielsen, R Nock, M Carioni, G. Patrini, F. Nielsen, R. Nock, and M. Carioni. (2016). \"Loss factoriza- tion, weakly supervised learning and label noise robustness.\" [Online].\n\nRisk minimization in the presence of label noise. W Gao, L Wang, Y.-F Li, Z.-H Zhou, Proc. AAAI. AAAIW. Gao, L. Wang, Y.-F. Li, and Z.-H. Zhou, \"Risk minimization in the presence of label noise,\" in Proc. AAAI, 2016, pp. 1575-1581.\n\nMultiple kernel learning from noisy labels by stochastic programming. T Yang, M Mahdavi, R Jin, L Zhang, Y Zhou, Proc. ICML. ICMLT. Yang, M. Mahdavi, R. Jin, L. Zhang, and Y. Zhou, \"Multiple kernel learning from noisy labels by stochastic programming,\" in Proc. ICML, 2012, pp. 233-240.\n\nLearning noisy perceptrons by a perceptron in polynomial time. E Cohen, Proc. Annu. Symp. Found. Annu. Symp. FoundE. Cohen, \"Learning noisy perceptrons by a perceptron in polyno- mial time,\" in Proc. Annu. Symp. Found. Comput. Sci., Oct. 1997, pp. 514-523.\n\nLearning kernel perceptrons on noisy data using random projections. G Stempfel, L Ralaivola, Algorithmic Learning Theory. Sendai. JapanSpringerG. Stempfel and L. Ralaivola, \"Learning kernel perceptrons on noisy data using random projections,\" in Algorithmic Learning Theory. Sendai, Japan: Springer, 2007, pp. 328-342.\n\nNoise tolerant variants of the perceptron algorithm. R Khardon, G Wachman, J. Mach. Learn. Res. 8R. Khardon and G. Wachman, \"Noise tolerant variants of the perceptron algorithm,\" J. Mach. Learn. Res., vol. 8, pp. 227-248, 2007.\n\nLabel distribution learning. X Geng, IEEE Trans. Knowl. Data Eng. 287X. Geng, \"Label distribution learning,\" IEEE Trans. Knowl. Data Eng., vol. 28, no. 7, pp. 1734-1748, Jul. 2016.\n\nHead pose estimation based on multivariate label distribution. X Geng, Y Xia, Proc. CVPR. CVPRX. Geng and Y. Xia, \"Head pose estimation based on multivariate label distribution,\" in Proc. CVPR, 2014, pp. 1837-1842.\n\nClassification in the presence of label noise: A survey. B Frenay, M Verleysen, IEEE Trans. Neural Netw. Learn. Syst. 255B. Frenay and M. Verleysen, \"Classification in the presence of label noise: A survey,\" IEEE Trans. Neural Netw. Learn. Syst., vol. 25, no. 5, pp. 845-869, May 2014.\n\nMulticlass from binary: Expanding one-versus-all, one-versus-one and ECOC-based approaches. A Rocha, S Klein Goldenstein, IEEE Trans. Neural Netw. Learn. Syst. 252A. Rocha and S. Klein Goldenstein, \"Multiclass from binary: Expand- ing one-versus-all, one-versus-one and ECOC-based approaches,\" IEEE Trans. Neural Netw. Learn. Syst., vol. 25, no. 2, pp. 289-302, Feb. 2014.\n\nSemisupervised multiclass classification problems with scarcity of labeled data: A theoretical study. J Ortigosa-Hernandez, I Inza, J A Lozano, IEEE Trans. Neural Netw. Learn. Syst. 2712J. Ortigosa-Hernandez, I. Inza, and J. A. Lozano, \"Semisupervised multiclass classification problems with scarcity of labeled data: A theoretical study,\" IEEE Trans. Neural Netw. Learn. Syst., vol. 27, no. 12, pp. 2602-2614, Dec. 2016.\n\nDynamic sampling approach to training neural networks for multiclass imbalance classification. M Lin, K Tang, X Yao, IEEE Trans. Neural Netw. Learn. Syst. 244M. Lin, K. Tang, and X. Yao, \"Dynamic sampling approach to training neural networks for multiclass imbalance classification,\" IEEE Trans. Neural Netw. Learn. Syst., vol. 24, no. 4, pp. 647-660, Apr. 2013.\n\nLearning imbalanced multiclass data with optimal dichotomy weights. X.-Y Liu, Q.-Q Li, Z.-H Zhou, Proc. ICDM. ICDMX.-Y. Liu, Q.-Q. Li, and Z.-H. Zhou, \"Learning imbalanced multi- class data with optimal dichotomy weights,\" in Proc. ICDM, 2013, pp. 478-487.\n\nOn multi-class cost-sensitive learning. Z.-H Zhou, X.-Y Liu, Comput. Intell. 263Z.-H. Zhou and X.-Y. Liu, \"On multi-class cost-sensitive learning,\" Comput. Intell., vol. 26, no. 3, pp. 232-257, 2010.\n\nDomain adaptation problems: A DASVM classification technique and a circular validation strategy. L Bruzzone, M Marconcini, IEEE Trans. Pattern Anal. Mach. Intell. 325L. Bruzzone and M. Marconcini, \"Domain adaptation problems: A DASVM classification technique and a circular validation strategy,\" IEEE Trans. Pattern Anal. Mach. Intell., vol. 32, no. 5, pp. 770-787, May 2010.\n\nMulti-source domain adaptation: A causal view. K Zhang, M Gong, B Sch\u00f6lkopf, Proc. AAAI. AAAIK. Zhang, M. Gong, and B. Sch\u00f6lkopf, \"Multi-source domain adapta- tion: A causal view,\" in Proc. AAAI, 2015, pp. 3150-3157.\n\nDomain adaptation with conditional transferable components. M Gong, K Zhang, T Liu, D Tao, C Glymour, B Sch\u00f6lkopf, Proc. ICML. ICMLM. Gong, K. Zhang, T. Liu, D. Tao, C. Glymour, and B. Sch\u00f6lkopf, \"Domain adaptation with conditional transferable components,\" in Proc. ICML, 2016, pp. 2839-2848.\n\nCovariate shift by kernel mean matching. A Gretton, A Smola, J Huang, M Schmittfull, K Borgwardt, B Sch\u00f6lkopf, Dataset Shift Mach. Learn. 345A. Gretton, A. Smola, J. Huang, M. Schmittfull, K. Borgwardt, and B. Sch\u00f6lkopf, \"Covariate shift by kernel mean matching,\" Dataset Shift Mach. Learn., vol. 3, no. 4, p. 5, 2009.\n\nConstructive setting of the density ratio estimation problem and its rigorous solution. V Vapnik, I Braga, R Izmailov, V. Vapnik, I. Braga, and R. Izmailov. (2013). \"Constructive setting of the density ratio estimation problem and its rigorous solution.\" [Online]. Available: https://arxiv.org/abs/1306.0407\n\nDensity ratio estimation: A comprehensive review. T Kanamori, RIMS Kokyuroku. 1703T. Kanamori, \"Density ratio estimation: A comprehensive review,\" RIMS Kokyuroku, vol. 1703, pp. 10-31, Mar. 2010.\n\nTheoretical analysis of density ratio estimation. T Kanamori, T Suzuki, M Sugiyama, IEICE Trans. Fundam. Electron., Commun. Comput. Sci. 934T. Kanamori, T. Suzuki, and M. Sugiyama, \"Theoretical analysis of density ratio estimation,\" IEICE Trans. Fundam. Electron., Commun. Comput. Sci., vol. 93, no. 4, pp. 787-798, 2010.\n\nDirect importance estimation with model selection and its application to covariate shift adaptation. M Sugiyama, S Nakajima, H Kashima, P V Buenau, M Kawanabe, Proc. NIPS. NIPSM. Sugiyama, S. Nakajima, H. Kashima, P. V. Buenau, and M. Kawanabe, \"Direct importance estimation with model selection and its application to covariate shift adaptation,\" in Proc. NIPS, 2008, pp. 1433-1440.\n\nClassification with asymmetric label noise: Consistency and maximal denoising. C Scott, G Blanchard, G Handy, Proc. COLT. COLTC. Scott, G. Blanchard, and G. Handy, \"Classification with asymmetric label noise: Consistency and maximal denoising,\" in Proc. COLT, 2013, pp. 489-511.\n\nA rate of convergence for mixture proportion estimation, with application to learning from noisy labels. C Scott, Proc. AISTATS. AISTATSC. Scott, \"A rate of convergence for mixture proportion estimation, with application to learning from noisy labels,\" in Proc. AISTATS, 2015, pp. 838-846.\n\nLearning from corrupted binary labels via class-probability estimation. A K Menon, B Van Rooyen, C S Ong, R C Williamson, Proc. ICML. ICMLA. K. Menon, B. van Rooyen, C. S. Ong, and R. C. Williamson, \"Learning from corrupted binary labels via class-probability estimation,\" in Proc. ICML, 2015, pp. 125-134.\n\nSparse multinomial logistic regression: Fast algorithms and generalization bounds. B Krishnapuram, L Carin, M A T Figueiredo, A J Hartemink, IEEE Trans. Pattern Anal. Mach. Intell. 276B. Krishnapuram, L. Carin, M. A. T. Figueiredo, and A. J. Hartemink, \"Sparse multinomial logistic regression: Fast algorithms and generaliza- tion bounds,\" IEEE Trans. Pattern Anal. Mach. Intell., vol. 27, no. 6, pp. 957-968, Jun. 2005.\n\nA comparison of methods for multiclass support vector machines. C.-W Hsu, C.-J Lin, IEEE Trans. Neural Netw. 132C.-W. Hsu and C.-J. Lin, \"A comparison of methods for multiclass support vector machines,\" IEEE Trans. Neural Netw., vol. 13, no. 2, pp. 415-425, Mar. 2002.\n\nSparse LSSVM in primal using Cholesky factorization for large-scale problems. S Zhou, IEEE Trans. Neural Netw. Learn. Syst. 274S. Zhou, \"Sparse LSSVM in primal using Cholesky factorization for large-scale problems,\" IEEE Trans. Neural Netw. Learn. Syst., vol. 27, no. 4, pp. 783-795, Apr. 2016.\n\nA simple method for solving the svm regularization path for semidefinite kernels. C G Sentelle, G C Anagnostopoulos, M Georgiopoulos, IEEE Trans. Neural Netw. Learn. Syst. 274C. G. Sentelle, G. C. Anagnostopoulos, and M. Georgiopoulos, \"A simple method for solving the svm regularization path for semidefi- nite kernels,\" IEEE Trans. Neural Netw. Learn. Syst., vol. 27, no. 4, pp. 709-722, Apr. 2016.\n\nMaking neural networks robust to label noise: A loss correction approach. G Patrini, A Rozza, A Menon, R Nock, L Qu, G. Patrini, A. Rozza, A. Menon, R. Nock, and L. Qu. (2016). \"Making neural networks robust to label noise: A loss correction approach.\" [Online]. Available: https://arxiv.org/abs/1609.03683\n", "annotations": {"author": "[{\"end\":75,\"start\":64},{\"end\":90,\"start\":76},{\"end\":115,\"start\":91}]", "publisher": null, "author_last_name": "[{\"end\":74,\"start\":70},{\"end\":89,\"start\":86},{\"end\":114,\"start\":111}]", "author_first_name": "[{\"end\":69,\"start\":64},{\"end\":85,\"start\":76},{\"end\":110,\"start\":103}]", "author_affiliation": null, "title": "[{\"end\":52,\"start\":1},{\"end\":167,\"start\":116}]", "venue": "[{\"end\":226,\"start\":169}]", "abstract": "[{\"end\":1750,\"start\":353}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2205,\"start\":2202},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2210,\"start\":2207},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2810,\"start\":2807},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2815,\"start\":2812},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4256,\"start\":4253},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4830,\"start\":4827},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4836,\"start\":4832},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5008,\"start\":5004},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5307,\"start\":5303},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5609,\"start\":5605},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5808,\"start\":5804},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5949,\"start\":5945},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6950,\"start\":6946},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7284,\"start\":7281},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7409,\"start\":7405},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10470,\"start\":10467},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10554,\"start\":10550},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10639,\"start\":10636},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10645,\"start\":10641},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10880,\"start\":10876},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10886,\"start\":10882},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10913,\"start\":10909},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10919,\"start\":10915},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":11046,\"start\":11042},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":11052,\"start\":11048},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":11502,\"start\":11498},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":11523,\"start\":11519},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":11542,\"start\":11538},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":11567,\"start\":11563},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":11573,\"start\":11569},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":11602,\"start\":11598},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":11638,\"start\":11634},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":11644,\"start\":11640},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":11852,\"start\":11848},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":11871,\"start\":11867},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":12026,\"start\":12022},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":12414,\"start\":12411},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":12564,\"start\":12560},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12683,\"start\":12679},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":12949,\"start\":12946},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":13147,\"start\":13143},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":13735,\"start\":13731},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":13741,\"start\":13737},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":17707,\"start\":17703},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":18751,\"start\":18747},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":18757,\"start\":18753},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":19402,\"start\":19399},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":20685,\"start\":20681},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":21740,\"start\":21737},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":21890,\"start\":21887},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":24191,\"start\":24187},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":25029,\"start\":25025},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":25227,\"start\":25223},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":25318,\"start\":25314},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":25556,\"start\":25552},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":25674,\"start\":25670},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":25809,\"start\":25805},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":25815,\"start\":25811},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":25857,\"start\":25854},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":26239,\"start\":26235},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":26293,\"start\":26289},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":27314,\"start\":27310},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":27383,\"start\":27379},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":27590,\"start\":27586},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":29424,\"start\":29421},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":30242,\"start\":30238},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":30858,\"start\":30855},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":31562,\"start\":31558},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":31728,\"start\":31724},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":32080,\"start\":32076},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":32651,\"start\":32647},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":32747,\"start\":32743},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":32753,\"start\":32749},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":32765,\"start\":32761},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":32774,\"start\":32770},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":33138,\"start\":33134},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":33776,\"start\":33773},{\"end\":33882,\"start\":33874},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":33886,\"start\":33882},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":34886,\"start\":34882},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":34892,\"start\":34888},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":35026,\"start\":35022},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":37011,\"start\":37007},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":37028,\"start\":37024},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":37088,\"start\":37084},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":43051,\"start\":43047}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":47687,\"start\":47370},{\"attributes\":{\"id\":\"fig_1\"},\"end\":47953,\"start\":47688},{\"attributes\":{\"id\":\"fig_2\"},\"end\":47992,\"start\":47954},{\"attributes\":{\"id\":\"fig_3\"},\"end\":48748,\"start\":47993},{\"attributes\":{\"id\":\"fig_4\"},\"end\":48874,\"start\":48749},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":49095,\"start\":48875},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":49219,\"start\":49096},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":49471,\"start\":49220},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":49744,\"start\":49472},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":50035,\"start\":49745}]", "paragraph": "[{\"end\":2211,\"start\":1769},{\"end\":4336,\"start\":2213},{\"end\":4718,\"start\":4338},{\"end\":6297,\"start\":4720},{\"end\":6951,\"start\":6299},{\"end\":9222,\"start\":6953},{\"end\":10026,\"start\":9224},{\"end\":10748,\"start\":10065},{\"end\":12379,\"start\":10750},{\"end\":13625,\"start\":12381},{\"end\":13988,\"start\":13693},{\"end\":14669,\"start\":14055},{\"end\":14780,\"start\":14688},{\"end\":15045,\"start\":14832},{\"end\":15264,\"start\":15100},{\"end\":15971,\"start\":15338},{\"end\":16132,\"start\":16019},{\"end\":16256,\"start\":16172},{\"end\":16378,\"start\":16355},{\"end\":17708,\"start\":16380},{\"end\":18667,\"start\":17710},{\"end\":19121,\"start\":18729},{\"end\":19403,\"start\":19285},{\"end\":19503,\"start\":19405},{\"end\":19731,\"start\":19560},{\"end\":19973,\"start\":19844},{\"end\":20004,\"start\":19975},{\"end\":20480,\"start\":20040},{\"end\":20738,\"start\":20615},{\"end\":20864,\"start\":20740},{\"end\":21303,\"start\":20907},{\"end\":21397,\"start\":21305},{\"end\":21741,\"start\":21399},{\"end\":22062,\"start\":21805},{\"end\":22364,\"start\":22091},{\"end\":22506,\"start\":22366},{\"end\":22700,\"start\":22655},{\"end\":22868,\"start\":22826},{\"end\":23054,\"start\":23026},{\"end\":23114,\"start\":23107},{\"end\":23203,\"start\":23169},{\"end\":23297,\"start\":23273},{\"end\":23743,\"start\":23401},{\"end\":23849,\"start\":23807},{\"end\":24351,\"start\":23936},{\"end\":25147,\"start\":24353},{\"end\":25675,\"start\":25149},{\"end\":26221,\"start\":25698},{\"end\":26459,\"start\":26223},{\"end\":26748,\"start\":26611},{\"end\":26879,\"start\":26788},{\"end\":26911,\"start\":26908},{\"end\":27384,\"start\":26948},{\"end\":27791,\"start\":27386},{\"end\":28134,\"start\":27819},{\"end\":28334,\"start\":28136},{\"end\":28643,\"start\":28391},{\"end\":28958,\"start\":28645},{\"end\":29322,\"start\":29216},{\"end\":30174,\"start\":29408},{\"end\":30373,\"start\":30176},{\"end\":30671,\"start\":30422},{\"end\":30873,\"start\":30716},{\"end\":31165,\"start\":31065},{\"end\":31457,\"start\":31261},{\"end\":31832,\"start\":31459},{\"end\":32362,\"start\":32037},{\"end\":33096,\"start\":32555},{\"end\":33549,\"start\":33114},{\"end\":33796,\"start\":33605},{\"end\":33906,\"start\":33861},{\"end\":34789,\"start\":33996},{\"end\":35065,\"start\":34791},{\"end\":35137,\"start\":35120},{\"end\":36721,\"start\":35193},{\"end\":37956,\"start\":36740},{\"end\":38397,\"start\":37958},{\"end\":39564,\"start\":38419},{\"end\":40345,\"start\":39566},{\"end\":42642,\"start\":40362},{\"end\":43232,\"start\":42644},{\"end\":45452,\"start\":43271},{\"end\":47103,\"start\":45471},{\"end\":47369,\"start\":47105}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":14054,\"start\":13989},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14687,\"start\":14670},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14831,\"start\":14781},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15099,\"start\":15046},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15337,\"start\":15265},{\"attributes\":{\"id\":\"formula_5\"},\"end\":16018,\"start\":15972},{\"attributes\":{\"id\":\"formula_6\"},\"end\":16171,\"start\":16133},{\"attributes\":{\"id\":\"formula_7\"},\"end\":16354,\"start\":16257},{\"attributes\":{\"id\":\"formula_8\"},\"end\":19284,\"start\":19122},{\"attributes\":{\"id\":\"formula_9\"},\"end\":19559,\"start\":19504},{\"attributes\":{\"id\":\"formula_10\"},\"end\":19843,\"start\":19732},{\"attributes\":{\"id\":\"formula_11\"},\"end\":20039,\"start\":20005},{\"attributes\":{\"id\":\"formula_12\"},\"end\":20559,\"start\":20481},{\"attributes\":{\"id\":\"formula_13\"},\"end\":20614,\"start\":20559},{\"attributes\":{\"id\":\"formula_14\"},\"end\":20906,\"start\":20865},{\"attributes\":{\"id\":\"formula_15\"},\"end\":21804,\"start\":21742},{\"attributes\":{\"id\":\"formula_16\"},\"end\":22654,\"start\":22507},{\"attributes\":{\"id\":\"formula_17\"},\"end\":22825,\"start\":22701},{\"attributes\":{\"id\":\"formula_18\"},\"end\":23025,\"start\":22869},{\"attributes\":{\"id\":\"formula_19\"},\"end\":23106,\"start\":23055},{\"attributes\":{\"id\":\"formula_20\"},\"end\":23168,\"start\":23115},{\"attributes\":{\"id\":\"formula_21\"},\"end\":23272,\"start\":23204},{\"attributes\":{\"id\":\"formula_22\"},\"end\":23400,\"start\":23298},{\"attributes\":{\"id\":\"formula_23\"},\"end\":23806,\"start\":23744},{\"attributes\":{\"id\":\"formula_24\"},\"end\":23935,\"start\":23850},{\"attributes\":{\"id\":\"formula_25\"},\"end\":26577,\"start\":26460},{\"attributes\":{\"id\":\"formula_26\"},\"end\":26610,\"start\":26577},{\"attributes\":{\"id\":\"formula_27\"},\"end\":26787,\"start\":26749},{\"attributes\":{\"id\":\"formula_28\"},\"end\":26907,\"start\":26880},{\"attributes\":{\"id\":\"formula_29\"},\"end\":26947,\"start\":26912},{\"attributes\":{\"id\":\"formula_30\"},\"end\":28390,\"start\":28335},{\"attributes\":{\"id\":\"formula_31\"},\"end\":29215,\"start\":28959},{\"attributes\":{\"id\":\"formula_32\"},\"end\":29383,\"start\":29323},{\"attributes\":{\"id\":\"formula_33\"},\"end\":30421,\"start\":30374},{\"attributes\":{\"id\":\"formula_34\"},\"end\":30715,\"start\":30672},{\"attributes\":{\"id\":\"formula_35\"},\"end\":31064,\"start\":30874},{\"attributes\":{\"id\":\"formula_36\"},\"end\":31260,\"start\":31166},{\"attributes\":{\"id\":\"formula_37\"},\"end\":31861,\"start\":31833},{\"attributes\":{\"id\":\"formula_38\"},\"end\":32036,\"start\":31861},{\"attributes\":{\"id\":\"formula_39\"},\"end\":32554,\"start\":32363},{\"attributes\":{\"id\":\"formula_40\"},\"end\":33604,\"start\":33550},{\"attributes\":{\"id\":\"formula_41\"},\"end\":33860,\"start\":33797},{\"attributes\":{\"id\":\"formula_42\"},\"end\":33995,\"start\":33907},{\"attributes\":{\"id\":\"formula_43\"},\"end\":35119,\"start\":35066},{\"attributes\":{\"id\":\"formula_44\"},\"end\":35192,\"start\":35138}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":40653,\"start\":40646},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":41185,\"start\":41177},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":44265,\"start\":44256},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":44649,\"start\":44641},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":45104,\"start\":45087}]", "section_header": "[{\"end\":1767,\"start\":1752},{\"end\":10063,\"start\":10029},{\"end\":13691,\"start\":13628},{\"end\":18727,\"start\":18670},{\"end\":22089,\"start\":22065},{\"end\":25696,\"start\":25678},{\"end\":27817,\"start\":27794},{\"end\":29406,\"start\":29385},{\"end\":33112,\"start\":33099},{\"end\":36738,\"start\":36724},{\"end\":38417,\"start\":38400},{\"end\":40360,\"start\":40348},{\"end\":43269,\"start\":43235},{\"end\":45469,\"start\":45455},{\"end\":47379,\"start\":47371},{\"end\":48002,\"start\":47994},{\"end\":48758,\"start\":48750},{\"end\":49104,\"start\":49097},{\"end\":49481,\"start\":49473},{\"end\":49755,\"start\":49746}]", "table": "[{\"end\":49095,\"start\":49014}]", "figure_caption": "[{\"end\":47687,\"start\":47381},{\"end\":47953,\"start\":47690},{\"end\":47992,\"start\":47956},{\"end\":48748,\"start\":48004},{\"end\":48874,\"start\":48760},{\"end\":49014,\"start\":48877},{\"end\":49219,\"start\":49106},{\"end\":49471,\"start\":49222},{\"end\":49744,\"start\":49484},{\"end\":50035,\"start\":49759}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":21274,\"start\":21268},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":38492,\"start\":38483},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":38541,\"start\":38532},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":38662,\"start\":38656},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":38805,\"start\":38792},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":38971,\"start\":38962},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":39375,\"start\":39366},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":39440,\"start\":39431},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":39456,\"start\":39450},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":39761,\"start\":39755}]", "bib_author_first_name": "[{\"end\":50502,\"start\":50501},{\"end\":50510,\"start\":50509},{\"end\":50518,\"start\":50517},{\"end\":50531,\"start\":50527},{\"end\":50537,\"start\":50536},{\"end\":50543,\"start\":50542},{\"end\":50759,\"start\":50758},{\"end\":50765,\"start\":50764},{\"end\":50772,\"start\":50771},{\"end\":50966,\"start\":50965},{\"end\":50974,\"start\":50973},{\"end\":50980,\"start\":50979},{\"end\":50986,\"start\":50985},{\"end\":51221,\"start\":51220},{\"end\":51430,\"start\":51429},{\"end\":51432,\"start\":51431},{\"end\":51445,\"start\":51444},{\"end\":51456,\"start\":51455},{\"end\":51755,\"start\":51754},{\"end\":51757,\"start\":51756},{\"end\":51765,\"start\":51764},{\"end\":51767,\"start\":51766},{\"end\":51975,\"start\":51974},{\"end\":51986,\"start\":51985},{\"end\":52168,\"start\":52167},{\"end\":52170,\"start\":52169},{\"end\":52179,\"start\":52178},{\"end\":52181,\"start\":52180},{\"end\":52387,\"start\":52386},{\"end\":52400,\"start\":52399},{\"end\":52402,\"start\":52401},{\"end\":52413,\"start\":52412},{\"end\":52415,\"start\":52414},{\"end\":52428,\"start\":52427},{\"end\":52642,\"start\":52641},{\"end\":52649,\"start\":52648},{\"end\":52961,\"start\":52960},{\"end\":52963,\"start\":52962},{\"end\":53192,\"start\":53191},{\"end\":53203,\"start\":53202},{\"end\":53205,\"start\":53204},{\"end\":53393,\"start\":53392},{\"end\":53407,\"start\":53406},{\"end\":53416,\"start\":53415},{\"end\":53418,\"start\":53417},{\"end\":53653,\"start\":53652},{\"end\":53673,\"start\":53672},{\"end\":53675,\"start\":53674},{\"end\":53695,\"start\":53694},{\"end\":53934,\"start\":53933},{\"end\":53949,\"start\":53948},{\"end\":54156,\"start\":54155},{\"end\":54170,\"start\":54169},{\"end\":54179,\"start\":54178},{\"end\":54189,\"start\":54188},{\"end\":54200,\"start\":54199},{\"end\":54534,\"start\":54533},{\"end\":54536,\"start\":54535},{\"end\":54544,\"start\":54543},{\"end\":54553,\"start\":54552},{\"end\":54563,\"start\":54562},{\"end\":54887,\"start\":54886},{\"end\":54894,\"start\":54893},{\"end\":54902,\"start\":54901},{\"end\":54910,\"start\":54909},{\"end\":55176,\"start\":55175},{\"end\":55190,\"start\":55189},{\"end\":55200,\"start\":55199},{\"end\":55208,\"start\":55207},{\"end\":55219,\"start\":55218},{\"end\":55223,\"start\":55220},{\"end\":55529,\"start\":55528},{\"end\":55531,\"start\":55530},{\"end\":55746,\"start\":55745},{\"end\":55757,\"start\":55756},{\"end\":55759,\"start\":55758},{\"end\":56109,\"start\":56108},{\"end\":56116,\"start\":56115},{\"end\":56118,\"start\":56117},{\"end\":56130,\"start\":56129},{\"end\":56132,\"start\":56131},{\"end\":56412,\"start\":56411},{\"end\":56423,\"start\":56422},{\"end\":56656,\"start\":56655},{\"end\":56664,\"start\":56663},{\"end\":56672,\"start\":56671},{\"end\":56681,\"start\":56680},{\"end\":56696,\"start\":56692},{\"end\":57076,\"start\":57075},{\"end\":57084,\"start\":57083},{\"end\":57091,\"start\":57090},{\"end\":57098,\"start\":57097},{\"end\":57106,\"start\":57105},{\"end\":57113,\"start\":57112},{\"end\":57537,\"start\":57536},{\"end\":57549,\"start\":57548},{\"end\":57559,\"start\":57558},{\"end\":57853,\"start\":57852},{\"end\":58059,\"start\":58055},{\"end\":58069,\"start\":58065},{\"end\":58080,\"start\":58076},{\"end\":58091,\"start\":58087},{\"end\":58368,\"start\":58367},{\"end\":58382,\"start\":58381},{\"end\":58393,\"start\":58392},{\"end\":58395,\"start\":58394},{\"end\":58640,\"start\":58639},{\"end\":58642,\"start\":58641},{\"end\":58805,\"start\":58804},{\"end\":58807,\"start\":58806},{\"end\":58814,\"start\":58813},{\"end\":58824,\"start\":58823},{\"end\":58826,\"start\":58825},{\"end\":59036,\"start\":59035},{\"end\":59247,\"start\":59246},{\"end\":59249,\"start\":59248},{\"end\":59261,\"start\":59260},{\"end\":59489,\"start\":59488},{\"end\":59499,\"start\":59498},{\"end\":59509,\"start\":59508},{\"end\":59738,\"start\":59737},{\"end\":59749,\"start\":59748},{\"end\":59760,\"start\":59759},{\"end\":59768,\"start\":59767},{\"end\":59976,\"start\":59975},{\"end\":59983,\"start\":59982},{\"end\":59994,\"start\":59990},{\"end\":60003,\"start\":59999},{\"end\":60229,\"start\":60228},{\"end\":60237,\"start\":60236},{\"end\":60248,\"start\":60247},{\"end\":60255,\"start\":60254},{\"end\":60264,\"start\":60263},{\"end\":60510,\"start\":60509},{\"end\":60773,\"start\":60772},{\"end\":60785,\"start\":60784},{\"end\":61078,\"start\":61077},{\"end\":61089,\"start\":61088},{\"end\":61283,\"start\":61282},{\"end\":61499,\"start\":61498},{\"end\":61507,\"start\":61506},{\"end\":61709,\"start\":61708},{\"end\":61719,\"start\":61718},{\"end\":62031,\"start\":62030},{\"end\":62040,\"start\":62039},{\"end\":62415,\"start\":62414},{\"end\":62437,\"start\":62436},{\"end\":62445,\"start\":62444},{\"end\":62447,\"start\":62446},{\"end\":62831,\"start\":62830},{\"end\":62838,\"start\":62837},{\"end\":62846,\"start\":62845},{\"end\":63171,\"start\":63167},{\"end\":63181,\"start\":63177},{\"end\":63190,\"start\":63186},{\"end\":63401,\"start\":63397},{\"end\":63412,\"start\":63408},{\"end\":63656,\"start\":63655},{\"end\":63668,\"start\":63667},{\"end\":63983,\"start\":63982},{\"end\":63992,\"start\":63991},{\"end\":64000,\"start\":63999},{\"end\":64214,\"start\":64213},{\"end\":64222,\"start\":64221},{\"end\":64231,\"start\":64230},{\"end\":64238,\"start\":64237},{\"end\":64245,\"start\":64244},{\"end\":64256,\"start\":64255},{\"end\":64490,\"start\":64489},{\"end\":64501,\"start\":64500},{\"end\":64510,\"start\":64509},{\"end\":64519,\"start\":64518},{\"end\":64534,\"start\":64533},{\"end\":64547,\"start\":64546},{\"end\":64857,\"start\":64856},{\"end\":64867,\"start\":64866},{\"end\":64876,\"start\":64875},{\"end\":65128,\"start\":65127},{\"end\":65325,\"start\":65324},{\"end\":65337,\"start\":65336},{\"end\":65347,\"start\":65346},{\"end\":65699,\"start\":65698},{\"end\":65711,\"start\":65710},{\"end\":65723,\"start\":65722},{\"end\":65734,\"start\":65733},{\"end\":65736,\"start\":65735},{\"end\":65746,\"start\":65745},{\"end\":66062,\"start\":66061},{\"end\":66071,\"start\":66070},{\"end\":66084,\"start\":66083},{\"end\":66368,\"start\":66367},{\"end\":66626,\"start\":66625},{\"end\":66628,\"start\":66627},{\"end\":66637,\"start\":66636},{\"end\":66651,\"start\":66650},{\"end\":66653,\"start\":66652},{\"end\":66660,\"start\":66659},{\"end\":66662,\"start\":66661},{\"end\":66945,\"start\":66944},{\"end\":66961,\"start\":66960},{\"end\":66970,\"start\":66969},{\"end\":66974,\"start\":66971},{\"end\":66988,\"start\":66987},{\"end\":66990,\"start\":66989},{\"end\":67351,\"start\":67347},{\"end\":67361,\"start\":67357},{\"end\":67632,\"start\":67631},{\"end\":67932,\"start\":67931},{\"end\":67934,\"start\":67933},{\"end\":67946,\"start\":67945},{\"end\":67948,\"start\":67947},{\"end\":67967,\"start\":67966},{\"end\":68326,\"start\":68325},{\"end\":68337,\"start\":68336},{\"end\":68346,\"start\":68345},{\"end\":68355,\"start\":68354},{\"end\":68363,\"start\":68362}]", "bib_author_last_name": "[{\"end\":50507,\"start\":50503},{\"end\":50515,\"start\":50511},{\"end\":50525,\"start\":50519},{\"end\":50534,\"start\":50532},{\"end\":50540,\"start\":50538},{\"end\":50551,\"start\":50544},{\"end\":50762,\"start\":50760},{\"end\":50769,\"start\":50766},{\"end\":50775,\"start\":50773},{\"end\":50971,\"start\":50967},{\"end\":50977,\"start\":50975},{\"end\":50983,\"start\":50981},{\"end\":50990,\"start\":50987},{\"end\":51226,\"start\":51222},{\"end\":51442,\"start\":51433},{\"end\":51453,\"start\":51446},{\"end\":51461,\"start\":51457},{\"end\":51762,\"start\":51758},{\"end\":51776,\"start\":51768},{\"end\":51983,\"start\":51976},{\"end\":51992,\"start\":51987},{\"end\":52176,\"start\":52171},{\"end\":52189,\"start\":52182},{\"end\":52397,\"start\":52388},{\"end\":52410,\"start\":52403},{\"end\":52425,\"start\":52416},{\"end\":52435,\"start\":52429},{\"end\":52646,\"start\":52643},{\"end\":52653,\"start\":52650},{\"end\":52969,\"start\":52964},{\"end\":53200,\"start\":53193},{\"end\":53209,\"start\":53206},{\"end\":53404,\"start\":53394},{\"end\":53413,\"start\":53408},{\"end\":53429,\"start\":53419},{\"end\":53670,\"start\":53654},{\"end\":53692,\"start\":53676},{\"end\":53702,\"start\":53696},{\"end\":53946,\"start\":53935},{\"end\":53955,\"start\":53950},{\"end\":54167,\"start\":54157},{\"end\":54176,\"start\":54171},{\"end\":54186,\"start\":54180},{\"end\":54197,\"start\":54190},{\"end\":54207,\"start\":54201},{\"end\":54541,\"start\":54537},{\"end\":54550,\"start\":54545},{\"end\":54560,\"start\":54554},{\"end\":54571,\"start\":54564},{\"end\":54891,\"start\":54888},{\"end\":54899,\"start\":54895},{\"end\":54907,\"start\":54903},{\"end\":54915,\"start\":54911},{\"end\":55187,\"start\":55177},{\"end\":55197,\"start\":55191},{\"end\":55205,\"start\":55201},{\"end\":55216,\"start\":55209},{\"end\":55231,\"start\":55224},{\"end\":55539,\"start\":55532},{\"end\":55754,\"start\":55747},{\"end\":55768,\"start\":55760},{\"end\":56113,\"start\":56110},{\"end\":56127,\"start\":56119},{\"end\":56141,\"start\":56133},{\"end\":56420,\"start\":56413},{\"end\":56430,\"start\":56424},{\"end\":56661,\"start\":56657},{\"end\":56669,\"start\":56665},{\"end\":56678,\"start\":56673},{\"end\":56690,\"start\":56682},{\"end\":56702,\"start\":56697},{\"end\":57081,\"start\":57077},{\"end\":57088,\"start\":57085},{\"end\":57095,\"start\":57092},{\"end\":57103,\"start\":57099},{\"end\":57110,\"start\":57107},{\"end\":57118,\"start\":57114},{\"end\":57546,\"start\":57538},{\"end\":57556,\"start\":57550},{\"end\":57570,\"start\":57560},{\"end\":57860,\"start\":57854},{\"end\":58063,\"start\":58060},{\"end\":58074,\"start\":58070},{\"end\":58085,\"start\":58081},{\"end\":58096,\"start\":58092},{\"end\":58379,\"start\":58369},{\"end\":58390,\"start\":58383},{\"end\":58402,\"start\":58396},{\"end\":58647,\"start\":58643},{\"end\":58811,\"start\":58808},{\"end\":58821,\"start\":58815},{\"end\":58833,\"start\":58827},{\"end\":59043,\"start\":59037},{\"end\":59258,\"start\":59250},{\"end\":59271,\"start\":59262},{\"end\":59496,\"start\":59490},{\"end\":59506,\"start\":59500},{\"end\":59516,\"start\":59510},{\"end\":59746,\"start\":59739},{\"end\":59757,\"start\":59750},{\"end\":59765,\"start\":59761},{\"end\":59776,\"start\":59769},{\"end\":59980,\"start\":59977},{\"end\":59988,\"start\":59984},{\"end\":59997,\"start\":59995},{\"end\":60008,\"start\":60004},{\"end\":60234,\"start\":60230},{\"end\":60245,\"start\":60238},{\"end\":60252,\"start\":60249},{\"end\":60261,\"start\":60256},{\"end\":60269,\"start\":60265},{\"end\":60516,\"start\":60511},{\"end\":60782,\"start\":60774},{\"end\":60795,\"start\":60786},{\"end\":61086,\"start\":61079},{\"end\":61097,\"start\":61090},{\"end\":61288,\"start\":61284},{\"end\":61504,\"start\":61500},{\"end\":61511,\"start\":61508},{\"end\":61716,\"start\":61710},{\"end\":61729,\"start\":61720},{\"end\":62037,\"start\":62032},{\"end\":62058,\"start\":62041},{\"end\":62434,\"start\":62416},{\"end\":62442,\"start\":62438},{\"end\":62454,\"start\":62448},{\"end\":62835,\"start\":62832},{\"end\":62843,\"start\":62839},{\"end\":62850,\"start\":62847},{\"end\":63175,\"start\":63172},{\"end\":63184,\"start\":63182},{\"end\":63195,\"start\":63191},{\"end\":63406,\"start\":63402},{\"end\":63416,\"start\":63413},{\"end\":63665,\"start\":63657},{\"end\":63679,\"start\":63669},{\"end\":63989,\"start\":63984},{\"end\":63997,\"start\":63993},{\"end\":64010,\"start\":64001},{\"end\":64219,\"start\":64215},{\"end\":64228,\"start\":64223},{\"end\":64235,\"start\":64232},{\"end\":64242,\"start\":64239},{\"end\":64253,\"start\":64246},{\"end\":64266,\"start\":64257},{\"end\":64498,\"start\":64491},{\"end\":64507,\"start\":64502},{\"end\":64516,\"start\":64511},{\"end\":64531,\"start\":64520},{\"end\":64544,\"start\":64535},{\"end\":64557,\"start\":64548},{\"end\":64864,\"start\":64858},{\"end\":64873,\"start\":64868},{\"end\":64885,\"start\":64877},{\"end\":65137,\"start\":65129},{\"end\":65334,\"start\":65326},{\"end\":65344,\"start\":65338},{\"end\":65356,\"start\":65348},{\"end\":65708,\"start\":65700},{\"end\":65720,\"start\":65712},{\"end\":65731,\"start\":65724},{\"end\":65743,\"start\":65737},{\"end\":65755,\"start\":65747},{\"end\":66068,\"start\":66063},{\"end\":66081,\"start\":66072},{\"end\":66090,\"start\":66085},{\"end\":66374,\"start\":66369},{\"end\":66634,\"start\":66629},{\"end\":66648,\"start\":66638},{\"end\":66657,\"start\":66654},{\"end\":66673,\"start\":66663},{\"end\":66958,\"start\":66946},{\"end\":66967,\"start\":66962},{\"end\":66985,\"start\":66975},{\"end\":67000,\"start\":66991},{\"end\":67355,\"start\":67352},{\"end\":67365,\"start\":67362},{\"end\":67637,\"start\":67633},{\"end\":67943,\"start\":67935},{\"end\":67964,\"start\":67949},{\"end\":67981,\"start\":67968},{\"end\":68334,\"start\":68327},{\"end\":68343,\"start\":68338},{\"end\":68352,\"start\":68347},{\"end\":68360,\"start\":68356},{\"end\":68366,\"start\":68364}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":57246310},\"end\":50719,\"start\":50448},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":2905985},\"end\":50900,\"start\":50721},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":29166765},\"end\":51141,\"start\":50902},{\"attributes\":{\"id\":\"b3\"},\"end\":51381,\"start\":51143},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":14888472},\"end\":51685,\"start\":51383},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":53861},\"end\":51942,\"start\":51687},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":5508562},\"end\":52113,\"start\":51944},{\"attributes\":{\"id\":\"b7\"},\"end\":52356,\"start\":52115},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":423350},\"end\":52579,\"start\":52358},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":11777930},\"end\":52859,\"start\":52581},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":119769984},\"end\":53158,\"start\":52861},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":9056472},\"end\":53319,\"start\":53160},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":6788443},\"end\":53598,\"start\":53321},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":817700},\"end\":53867,\"start\":53600},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":8721019},\"end\":54104,\"start\":53869},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":6458072},\"end\":54414,\"start\":54106},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":5945917},\"end\":54817,\"start\":54416},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":8760204},\"end\":55099,\"start\":54819},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":15353712},\"end\":55499,\"start\":55101},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":59712},\"end\":55652,\"start\":55501},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":19021839},\"end\":56030,\"start\":55654},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":7496788},\"end\":56368,\"start\":56032},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":391854},\"end\":56586,\"start\":56370},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":14499463},\"end\":56950,\"start\":56588},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":2622234},\"end\":57423,\"start\":56952},{\"attributes\":{\"id\":\"b25\"},\"end\":57794,\"start\":57425},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":975467},\"end\":57995,\"start\":57796},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":13994980},\"end\":58317,\"start\":57997},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":26816270},\"end\":58580,\"start\":58319},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":2884977},\"end\":58766,\"start\":58582},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":44971656},\"end\":58973,\"start\":58768},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":6392609},\"end\":59172,\"start\":58975},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":6597362},\"end\":59431,\"start\":59174},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":2649314},\"end\":59660,\"start\":59433},{\"attributes\":{\"id\":\"b34\"},\"end\":59923,\"start\":59662},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":18603428},\"end\":60156,\"start\":59925},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":1873993},\"end\":60444,\"start\":60158},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":27244147},\"end\":60702,\"start\":60446},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":16832324},\"end\":61022,\"start\":60704},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":10647676},\"end\":61251,\"start\":61024},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":1442493},\"end\":61433,\"start\":61253},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":7600274},\"end\":61649,\"start\":61435},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":6054025},\"end\":61936,\"start\":61651},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":14945585},\"end\":62310,\"start\":61938},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":18830691},\"end\":62733,\"start\":62312},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":6760636},\"end\":63097,\"start\":62735},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":10124820},\"end\":63355,\"start\":63099},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":1592270},\"end\":63556,\"start\":63357},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":14739721},\"end\":63933,\"start\":63558},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":10115836},\"end\":64151,\"start\":63935},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":5807252},\"end\":64446,\"start\":64153},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":108301245},\"end\":64766,\"start\":64448},{\"attributes\":{\"id\":\"b52\"},\"end\":65075,\"start\":64768},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":10275872},\"end\":65272,\"start\":65077},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":14356500},\"end\":65595,\"start\":65274},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":9133542},\"end\":65980,\"start\":65597},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":5370766},\"end\":66260,\"start\":65982},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":7110638},\"end\":66551,\"start\":66262},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":10708717},\"end\":66859,\"start\":66553},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":5927350},\"end\":67281,\"start\":66861},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":15874442},\"end\":67551,\"start\":67283},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":16353968},\"end\":67847,\"start\":67553},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":5856827},\"end\":68249,\"start\":67849},{\"attributes\":{\"id\":\"b63\"},\"end\":68557,\"start\":68251}]", "bib_title": "[{\"end\":50499,\"start\":50448},{\"end\":50756,\"start\":50721},{\"end\":50963,\"start\":50902},{\"end\":51427,\"start\":51383},{\"end\":51752,\"start\":51687},{\"end\":51972,\"start\":51944},{\"end\":52165,\"start\":52115},{\"end\":52384,\"start\":52358},{\"end\":52639,\"start\":52581},{\"end\":52958,\"start\":52861},{\"end\":53189,\"start\":53160},{\"end\":53390,\"start\":53321},{\"end\":53650,\"start\":53600},{\"end\":53931,\"start\":53869},{\"end\":54153,\"start\":54106},{\"end\":54531,\"start\":54416},{\"end\":54884,\"start\":54819},{\"end\":55173,\"start\":55101},{\"end\":55526,\"start\":55501},{\"end\":55743,\"start\":55654},{\"end\":56106,\"start\":56032},{\"end\":56409,\"start\":56370},{\"end\":56653,\"start\":56588},{\"end\":57073,\"start\":56952},{\"end\":57534,\"start\":57425},{\"end\":57850,\"start\":57796},{\"end\":58053,\"start\":57997},{\"end\":58365,\"start\":58319},{\"end\":58637,\"start\":58582},{\"end\":58802,\"start\":58768},{\"end\":59033,\"start\":58975},{\"end\":59244,\"start\":59174},{\"end\":59486,\"start\":59433},{\"end\":59973,\"start\":59925},{\"end\":60226,\"start\":60158},{\"end\":60507,\"start\":60446},{\"end\":60770,\"start\":60704},{\"end\":61075,\"start\":61024},{\"end\":61280,\"start\":61253},{\"end\":61496,\"start\":61435},{\"end\":61706,\"start\":61651},{\"end\":62028,\"start\":61938},{\"end\":62412,\"start\":62312},{\"end\":62828,\"start\":62735},{\"end\":63165,\"start\":63099},{\"end\":63395,\"start\":63357},{\"end\":63653,\"start\":63558},{\"end\":63980,\"start\":63935},{\"end\":64211,\"start\":64153},{\"end\":64487,\"start\":64448},{\"end\":65125,\"start\":65077},{\"end\":65322,\"start\":65274},{\"end\":65696,\"start\":65597},{\"end\":66059,\"start\":65982},{\"end\":66365,\"start\":66262},{\"end\":66623,\"start\":66553},{\"end\":66942,\"start\":66861},{\"end\":67345,\"start\":67283},{\"end\":67629,\"start\":67553},{\"end\":67929,\"start\":67849}]", "bib_author": "[{\"end\":50509,\"start\":50501},{\"end\":50517,\"start\":50509},{\"end\":50527,\"start\":50517},{\"end\":50536,\"start\":50527},{\"end\":50542,\"start\":50536},{\"end\":50553,\"start\":50542},{\"end\":50764,\"start\":50758},{\"end\":50771,\"start\":50764},{\"end\":50777,\"start\":50771},{\"end\":50973,\"start\":50965},{\"end\":50979,\"start\":50973},{\"end\":50985,\"start\":50979},{\"end\":50992,\"start\":50985},{\"end\":51228,\"start\":51220},{\"end\":51444,\"start\":51429},{\"end\":51455,\"start\":51444},{\"end\":51463,\"start\":51455},{\"end\":51764,\"start\":51754},{\"end\":51778,\"start\":51764},{\"end\":51985,\"start\":51974},{\"end\":51994,\"start\":51985},{\"end\":52178,\"start\":52167},{\"end\":52191,\"start\":52178},{\"end\":52399,\"start\":52386},{\"end\":52412,\"start\":52399},{\"end\":52427,\"start\":52412},{\"end\":52437,\"start\":52427},{\"end\":52648,\"start\":52641},{\"end\":52655,\"start\":52648},{\"end\":52971,\"start\":52960},{\"end\":53202,\"start\":53191},{\"end\":53211,\"start\":53202},{\"end\":53406,\"start\":53392},{\"end\":53415,\"start\":53406},{\"end\":53431,\"start\":53415},{\"end\":53672,\"start\":53652},{\"end\":53694,\"start\":53672},{\"end\":53704,\"start\":53694},{\"end\":53948,\"start\":53933},{\"end\":53957,\"start\":53948},{\"end\":54169,\"start\":54155},{\"end\":54178,\"start\":54169},{\"end\":54188,\"start\":54178},{\"end\":54199,\"start\":54188},{\"end\":54209,\"start\":54199},{\"end\":54543,\"start\":54533},{\"end\":54552,\"start\":54543},{\"end\":54562,\"start\":54552},{\"end\":54573,\"start\":54562},{\"end\":54893,\"start\":54886},{\"end\":54901,\"start\":54893},{\"end\":54909,\"start\":54901},{\"end\":54917,\"start\":54909},{\"end\":55189,\"start\":55175},{\"end\":55199,\"start\":55189},{\"end\":55207,\"start\":55199},{\"end\":55218,\"start\":55207},{\"end\":55233,\"start\":55218},{\"end\":55541,\"start\":55528},{\"end\":55756,\"start\":55745},{\"end\":55770,\"start\":55756},{\"end\":56115,\"start\":56108},{\"end\":56129,\"start\":56115},{\"end\":56143,\"start\":56129},{\"end\":56422,\"start\":56411},{\"end\":56432,\"start\":56422},{\"end\":56663,\"start\":56655},{\"end\":56671,\"start\":56663},{\"end\":56680,\"start\":56671},{\"end\":56692,\"start\":56680},{\"end\":56704,\"start\":56692},{\"end\":57083,\"start\":57075},{\"end\":57090,\"start\":57083},{\"end\":57097,\"start\":57090},{\"end\":57105,\"start\":57097},{\"end\":57112,\"start\":57105},{\"end\":57120,\"start\":57112},{\"end\":57548,\"start\":57536},{\"end\":57558,\"start\":57548},{\"end\":57572,\"start\":57558},{\"end\":57862,\"start\":57852},{\"end\":58065,\"start\":58055},{\"end\":58076,\"start\":58065},{\"end\":58087,\"start\":58076},{\"end\":58098,\"start\":58087},{\"end\":58381,\"start\":58367},{\"end\":58392,\"start\":58381},{\"end\":58404,\"start\":58392},{\"end\":58649,\"start\":58639},{\"end\":58813,\"start\":58804},{\"end\":58823,\"start\":58813},{\"end\":58835,\"start\":58823},{\"end\":59045,\"start\":59035},{\"end\":59260,\"start\":59246},{\"end\":59273,\"start\":59260},{\"end\":59498,\"start\":59488},{\"end\":59508,\"start\":59498},{\"end\":59518,\"start\":59508},{\"end\":59748,\"start\":59737},{\"end\":59759,\"start\":59748},{\"end\":59767,\"start\":59759},{\"end\":59778,\"start\":59767},{\"end\":59982,\"start\":59975},{\"end\":59990,\"start\":59982},{\"end\":59999,\"start\":59990},{\"end\":60010,\"start\":59999},{\"end\":60236,\"start\":60228},{\"end\":60247,\"start\":60236},{\"end\":60254,\"start\":60247},{\"end\":60263,\"start\":60254},{\"end\":60271,\"start\":60263},{\"end\":60518,\"start\":60509},{\"end\":60784,\"start\":60772},{\"end\":60797,\"start\":60784},{\"end\":61088,\"start\":61077},{\"end\":61099,\"start\":61088},{\"end\":61290,\"start\":61282},{\"end\":61506,\"start\":61498},{\"end\":61513,\"start\":61506},{\"end\":61718,\"start\":61708},{\"end\":61731,\"start\":61718},{\"end\":62039,\"start\":62030},{\"end\":62060,\"start\":62039},{\"end\":62436,\"start\":62414},{\"end\":62444,\"start\":62436},{\"end\":62456,\"start\":62444},{\"end\":62837,\"start\":62830},{\"end\":62845,\"start\":62837},{\"end\":62852,\"start\":62845},{\"end\":63177,\"start\":63167},{\"end\":63186,\"start\":63177},{\"end\":63197,\"start\":63186},{\"end\":63408,\"start\":63397},{\"end\":63418,\"start\":63408},{\"end\":63667,\"start\":63655},{\"end\":63681,\"start\":63667},{\"end\":63991,\"start\":63982},{\"end\":63999,\"start\":63991},{\"end\":64012,\"start\":63999},{\"end\":64221,\"start\":64213},{\"end\":64230,\"start\":64221},{\"end\":64237,\"start\":64230},{\"end\":64244,\"start\":64237},{\"end\":64255,\"start\":64244},{\"end\":64268,\"start\":64255},{\"end\":64500,\"start\":64489},{\"end\":64509,\"start\":64500},{\"end\":64518,\"start\":64509},{\"end\":64533,\"start\":64518},{\"end\":64546,\"start\":64533},{\"end\":64559,\"start\":64546},{\"end\":64866,\"start\":64856},{\"end\":64875,\"start\":64866},{\"end\":64887,\"start\":64875},{\"end\":65139,\"start\":65127},{\"end\":65336,\"start\":65324},{\"end\":65346,\"start\":65336},{\"end\":65358,\"start\":65346},{\"end\":65710,\"start\":65698},{\"end\":65722,\"start\":65710},{\"end\":65733,\"start\":65722},{\"end\":65745,\"start\":65733},{\"end\":65757,\"start\":65745},{\"end\":66070,\"start\":66061},{\"end\":66083,\"start\":66070},{\"end\":66092,\"start\":66083},{\"end\":66376,\"start\":66367},{\"end\":66636,\"start\":66625},{\"end\":66650,\"start\":66636},{\"end\":66659,\"start\":66650},{\"end\":66675,\"start\":66659},{\"end\":66960,\"start\":66944},{\"end\":66969,\"start\":66960},{\"end\":66987,\"start\":66969},{\"end\":67002,\"start\":66987},{\"end\":67357,\"start\":67347},{\"end\":67367,\"start\":67357},{\"end\":67639,\"start\":67631},{\"end\":67945,\"start\":67931},{\"end\":67966,\"start\":67945},{\"end\":67983,\"start\":67966},{\"end\":68336,\"start\":68325},{\"end\":68345,\"start\":68336},{\"end\":68354,\"start\":68345},{\"end\":68362,\"start\":68354},{\"end\":68368,\"start\":68362}]", "bib_venue": "[{\"end\":50563,\"start\":50553},{\"end\":50789,\"start\":50777},{\"end\":51002,\"start\":50992},{\"end\":51218,\"start\":51143},{\"end\":51501,\"start\":51463},{\"end\":51789,\"start\":51778},{\"end\":52005,\"start\":51994},{\"end\":52209,\"start\":52191},{\"end\":52447,\"start\":52437},{\"end\":52693,\"start\":52655},{\"end\":52980,\"start\":52971},{\"end\":53221,\"start\":53211},{\"end\":53441,\"start\":53431},{\"end\":53714,\"start\":53704},{\"end\":53968,\"start\":53957},{\"end\":54234,\"start\":54209},{\"end\":54589,\"start\":54573},{\"end\":54934,\"start\":54917},{\"end\":55269,\"start\":55233},{\"end\":55552,\"start\":55541},{\"end\":55809,\"start\":55770},{\"end\":56169,\"start\":56143},{\"end\":56450,\"start\":56432},{\"end\":56740,\"start\":56704},{\"end\":57156,\"start\":57120},{\"end\":57584,\"start\":57572},{\"end\":57873,\"start\":57862},{\"end\":58118,\"start\":58098},{\"end\":58424,\"start\":58404},{\"end\":58658,\"start\":58649},{\"end\":58846,\"start\":58835},{\"end\":59051,\"start\":59045},{\"end\":59283,\"start\":59273},{\"end\":59528,\"start\":59518},{\"end\":59735,\"start\":59662},{\"end\":60020,\"start\":60010},{\"end\":60281,\"start\":60271},{\"end\":60541,\"start\":60518},{\"end\":60832,\"start\":60797},{\"end\":61118,\"start\":61099},{\"end\":61317,\"start\":61290},{\"end\":61523,\"start\":61513},{\"end\":61767,\"start\":61731},{\"end\":62096,\"start\":62060},{\"end\":62492,\"start\":62456},{\"end\":62888,\"start\":62852},{\"end\":63207,\"start\":63197},{\"end\":63432,\"start\":63418},{\"end\":63719,\"start\":63681},{\"end\":64022,\"start\":64012},{\"end\":64278,\"start\":64268},{\"end\":64584,\"start\":64559},{\"end\":64854,\"start\":64768},{\"end\":65153,\"start\":65139},{\"end\":65409,\"start\":65358},{\"end\":65767,\"start\":65757},{\"end\":66102,\"start\":66092},{\"end\":66389,\"start\":66376},{\"end\":66685,\"start\":66675},{\"end\":67040,\"start\":67002},{\"end\":67390,\"start\":67367},{\"end\":67675,\"start\":67639},{\"end\":68019,\"start\":67983},{\"end\":68323,\"start\":68251},{\"end\":50569,\"start\":50565},{\"end\":50797,\"start\":50791},{\"end\":51008,\"start\":51004},{\"end\":51535,\"start\":51503},{\"end\":52453,\"start\":52449},{\"end\":53227,\"start\":53223},{\"end\":53447,\"start\":53443},{\"end\":53720,\"start\":53716},{\"end\":53975,\"start\":53970},{\"end\":54255,\"start\":54236},{\"end\":55844,\"start\":55811},{\"end\":58134,\"start\":58120},{\"end\":58663,\"start\":58660},{\"end\":59289,\"start\":59285},{\"end\":59534,\"start\":59530},{\"end\":60026,\"start\":60022},{\"end\":60287,\"start\":60283},{\"end\":60560,\"start\":60543},{\"end\":60839,\"start\":60834},{\"end\":61529,\"start\":61525},{\"end\":63213,\"start\":63209},{\"end\":64028,\"start\":64024},{\"end\":64284,\"start\":64280},{\"end\":65773,\"start\":65769},{\"end\":66108,\"start\":66104},{\"end\":66398,\"start\":66391},{\"end\":66691,\"start\":66687}]"}}}, "year": 2023, "month": 12, "day": 17}