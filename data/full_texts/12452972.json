{"id": 12452972, "updated": "2023-11-11 00:23:51.043", "metadata": {"title": "Richer Convolutional Features for Edge Detection", "authors": "[{\"first\":\"Yun\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Ming-Ming\",\"last\":\"Cheng\",\"middle\":[]},{\"first\":\"Xiaowei\",\"last\":\"Hu\",\"middle\":[]},{\"first\":\"Kai\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Xiang\",\"last\":\"Bai\",\"middle\":[]}]", "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2017, "month": null, "day": null}, "abstract": "In this paper, we propose an accurate edge detector using richer convolutional features (RCF). Since objects in natural images possess various scales and aspect ratios, learning the rich hierarchical representations is very critical for edge detection. CNNs have been proved to be effective for this task. In addition, the convolutional features in CNNs gradually become coarser with the increase of the receptive fields. According to these observations, we attempt to adopt richer convolutional features in such a challenging vision task. The proposed network fully exploits multiscale and multilevel information of objects to perform the image-to-image prediction by combining all the meaningful convolutional features in a holistic manner. Using VGG16 network, we achieve state-of-the-art performance on several available datasets. When evaluating on the well-known BSDS500 benchmark, we achieve ODS F-measure of 0.811 while retaining a fast speed (8 FPS). Besides, our fast version of RCF achieves ODS F-measure of 0.806 with 30 FPS.", "fields_of_study": "[\"Medicine\"]", "external_ids": {"arxiv": null, "mag": "2899283782", "acl": null, "pubmed": "30387723", "pubmedcentral": null, "dblp": "journals/pami/LiuCHBZBT19", "doi": "10.1109/tpami.2018.2878849"}}, "content": {"source": {"pdf_hash": "bfacf87dc68c2c089cbb1e58fa673e7e1ecf4b6c", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1612.02103v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1612.02103", "status": "GREEN"}}, "grobid": {"id": "62f6c8fa1b5703ea3bb830e40c5c00215ebd1577", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/bfacf87dc68c2c089cbb1e58fa673e7e1ecf4b6c.txt", "contents": "\nRicher Convolutional Features for Edge Detection\n\n\nYun Liu \nNankai University\n\n\nMing-Ming Cheng \nNankai University\n\n\nXiaowei Hu \nNankai University\n\n\nKai Wang Ccce \nNankai University\n\n\nRicher Convolutional Features for Edge Detection\nhttps://mmcheng.net/rcfEdge/ Xiang Bai HUST\nIn this paper, we propose an accurate edge detector using richer convolutional features (RCF). Since objects in nature images have various scales and aspect ratios, the automatically learned rich hierarchical representations by CNNs are very critical and effective to detect edges and object boundaries. And the convolutional features gradually become coarser with receptive fields increasing. Based on these observations, our proposed network architecture makes full use of multiscale and multi-level information to perform the image-to-image edge prediction by combining all of the useful convolutional features into a holistic framework. It is the first attempt to adopt such rich convolutional features in computer vision tasks. Using VGG16 network, we achieve state-of-the-art results on several available datasets. When evaluating on the well-known BSDS500 benchmark, we achieve ODS F-measure of .811 while retaining a fast speed (8 FPS). Besides, our fast version of RCF achieves ODS F-measure of .806 with 30 FPS.\n\nIntroduction\n\nEdge detection, which aims to extract visually salient edges and object boundaries from natural images, has retained one of the main challenges in computer vision for several decades. It is usually considered as a low-level technique, and varieties of high-level tasks have greatly benefited from the development of edge detection, such as object detection [15,48], object proposal [47,52] and image segmentation [1,3,8].\n\nTypically, traditional methods first extract local cues of brightness, colors, gradients and textures, or other manual designed futures like Pb [33], gPb [2], and Sketch tokens [29], then sophisticated learning paradigms [12,49] are used to classify edge and non-edge pixels. Although edge detection approaches using low-level features have made great improvement in these years, their limitations are also obvious. For example, edges and boundaries are often defined to be semantically meaningful, however, low-level cues are (a) original image (b) ground truth (c) conv3 1 (d) conv3 2 (e) conv3 3 (f) conv4 1 (g) conv4 2 (h) conv4 3 Figure 1: We build a simple network based on VGG16 [43] to obtain side outputs of conv3 1, conv3 2, conv3 3, conv4 1, conv4 2 and conv4 3. One can clearly see that convolutional features become coarser gradually, and the intermediate layers conv3 1, conv3 2, conv4 1, and conv4 2 contain lots of useful fine details that do not appear in other layers.\n\ncertainly difficult to represent object-level information. Under these circumstances, gPb [2] and Structured Edges [12] try to use complex strategies to capture global features as much as possible, but global information extracted in these ways is tiny.\n\nIn the last several years, convolutional neural networks (CNNs) have led fashion in the vision community by substantially advancing the state-of-the-art of various tasks, including image classification [27,43,45], object detection [18,19,36] and semantic segmentation [7,31] etc. Since CNNs have strong capability to learn high-level representations of natural images automatically, there is a trend of using convolutional networks to perform edge detection recently. Some well-known CNN methods have pushed forward this field significantly, such as DeepEdge [4], N 4 -Fields [17], CSCNN [23], DeepContour [40], and HED [50]. Our algorithm falls into this category as well.\n\nTo see the effectiveness of different convolution layers in edge detection, we build a simple network to obtain side outputs of intermediate layers using VGG16 [43] which has five convolution stages. Fig. 1 shows an example. We find that convolutional features become coarser gradually and intermediate layers contain lots of useful fine details. On the other hand, we know that richer convolutional features are important for many vision tasks, so many researchers try to develop deeper networks. However, the networks will be hard to converge when going deeper because of vanishing/exploding gradients and training data shortage. So why don't we make full use the CNN features we have now? Our motivation is based on these observations. Unlike previous CNN methods, our novel network makes full use of multi-level CNN features to make the pixel-wise prediction in an image-to-image fashion, and thus could obtain accurate representations for objects or object parts in different scales. Concretely speaking, we make a bold attempt to utilize the CNN features from all the convolution layers in a unified framework that can be easily generalized to other vision tasks. By carefully designing a universal strategy to combine hierarchical CNN features, our system performs very well in edge detection.\n\nWhen evaluating on BSDS500 dataset, we achieve stateof-the-art results with ODS F-measure of .811, which is higher than human eye (ODS F-measure 0.800). Our detector is also very efficient, achieving 8 FPS. The fast version of RCF achieves ODS F-measure of .806 with 30 FPS.\n\n\nRelated Works\n\nSince edge detection was set as one of the most fundamental problems in computer vision [13,16,39], researchers have struggled on it for nearly 50 years, and there have emerged a large number of materials. Broadly speaking, we can roughly categorize these approaches into three groups: early pioneering ones, manually designed features and learning based ones and deep learning based ones. Here we briefly review some important issues happened in the past few decades.\n\nEarly pioneering methods mainly focused on the utilization of intensity and color gradients. Robinson [39] discussed a quantitative measure in choosing color coordinates for the extraction of visually significant edges and boundaries. [32,46] presented zero-crossing theory based algorithms. Sobel [44] proposed the famous Sobel operator to compute the gradient map of an image, and then obtain edges by thresholding the gradient map. An extended version of Sobel, named Canny [6], added Gaussian smooth as a preprocessing step and used bi-threshold to get edges. In this way, Canny is more robust to noise. And it is still very popular across various tasks now because of its notable efficiency. However, these early methods seem to have poor accuracy and thus are difficult to adapt to today's applications.\n\nLater, researchers tended to manually design features using low-level cues intensity, gradient, and texture, and then employ sophisticated learning paradigm to classify edge and non-edge pixels [11,37]. Konishi et al. [26] proposed the first data-driven methods by learning the probability distributions of responses that correspond to two sets of edge filters. Martin et al. [33] formulated changes in brightness, color, and texture as Pb feature, and trained a classifier to combine the information from these features. Arbel\u00e1ez et al. [2] developed Pb into gPb by using standard Normalized Cuts [41] to combine above local cues into a globalization framework. Lim [29] proposed novel features, Sketch tokens that can be used to represent the mid-level information. Doll\u00e1r et al. [12] employed random decision forests to represent the structure presented in local image patches. Inputting color and gradient features, the structured forests output high-quality edges. Although above edge detectors have made brilliant achievements, development towards this direction is very slow, especially in recent years. The main reason is that using non-deep learning models to represent object-level information is a big challenge.\n\nWith the vigorous development of deep learning recently, a series of deep learning based approaches have been invented. Ganin et al. [17] proposed N 4 -Fields that combines CNNs with the nearest neighbor search. Shen et al. [40] partitioned contour data into subclasses and fit each subclass by learning model parameters. Hwang et al. [23] considered contour detection as a per-pixel classification problem. They employed DenseNet [24] to extract a feature vector for each pixel, and then SVM classier is used to classify each pixel into the edge or non-edge class. Xie et al. [50] recently developed an efficient and accurate edge detector, HED which performs image-to-image training and prediction. This holistically-nested architecture connects their side output layers, which is composed of one convolution layer with kernel size 1, one deconvolution layer and one softmax layer, to the last convolution layer of each stage in VGG16 [43]. More recently, Liu et al. [30] used relaxed label generated by bottom-up edges to guide the training process of HED, and achieved a little bit of improvement. Li et al. [28] proposed a complex model for unsupervised learning of edge detection, but the performance is worse than training on the limited BSDS500 dataset.\n\nAbove CNNs advance the state-of-the-art significantly in a short span of two years, but all of them can not make full use of CNN features that represent different object-level information respectively. In this paper, we propose a fully convolutional network to combine features from each CNN layer efficiently. As far as we know, our novel network architecture is the first attempt to use all convolution layers in this community. We will detail our method below.\n\n\nRicher Convolutional Features (RCF)\n\n\nNetwork Architecture\n\nInspired by previous literature in deep learning, we design our network by modifying VGG16 network [43]. VGG16 network that composes of 13 convolution layers and 3 fully connected layers has achieved state-of-the-art in a variety of tasks, such as image classification [43] Figure 2: Our RCF network architecture. The input is an image with arbitrary sizes, and our network ouputs an edge possibility map in the same size.\n\nject detection [18,19,36] etc. Its convolution layers are divided into five stages, in which a pooling layer is connected after each stage. The useful information obtained by each convolution layer becomes coarser with its receptive field size increasing. Detailed receptive field sizes of different layers can be seen in Tab. 1. Making full use of this rich hierarchical information will help a lot. The motivation of our network design lies here. The novel network proposed by us is shown in Fig. 2. Compared with VGG16, our modifications can be described as following:\n\n\u2022 We cut all the fully connected layers and the pool5 layer. On the one side, because fully connected layers are computationally expensive, we trim them to save executable time and memory. On the other hand, adding pool5 layer will increase the stride by two times, and it's harmful for edge localization.\n\n\u2022 Each convolution layer in VGG16 is connected to a convolution layer with kernel size 1 \u00d7 1 and channel depth 21. And the resulting layers in each stage are concatenated to obtain hybrid features.\n\n\u2022 An 1 \u00d7 1 \u00d7 1 convolution layer follows each concat layer. Then, a deconvolution layer is used to up-sample this feature map.\n\n\u2022 A cross-entropy loss/sigmoid layer is connected to the up-sampling layer in each stage.\n\n\u2022 All the up-sampling layers are concatenated. Then an 1 \u00d7 1 convolution layer and a cross-entropy loss /sigmoid layer are followed.\n\nHence, we combine hierarchical features from all the convolution layers into a holistic framework, in which all conjoint parameters are learned automatically. Since receptive field sizes of convolution layers in VGG16 are different from each other, our network can learn multiscale, including low-level and object-level, information that is helpful to  Fig. 3. From top to bottom, the edge response becomes coarser while obtaining strong response at the larger object or object part boundaries. It is consistent with our expectation, in which convolution layers will learn to detect the larger objects with the receptive field size increasing. Since our RCF model combines all the accessible convolution layers to employ richer features, it is expected to achieve a boost in accuracy.\n\nIn order to investigate whether including additional nonlinearity helps, we connecting ReLU layer after 1 \u00d7 1 \u00d7 21 or 1 \u00d7 1 \u00d7 1 convolution layers in each stage. However, the network performs worse. Especially, when we attempt to add nonlinear layers to 1 \u00d7 1 \u00d7 1 convolution layers, the network can not converge properly.\n\n\nImproved Loss Function\n\nEdge datasets in this community are usually labeled by several annotators using their knowledge about the presences of objects and object parts. Though humans vary in cognition, these human-labeled edges for the same image share high consistency. For each image, we average all the ground truth to generate an edge probability map, which ranges from 0 to 1. 0 means no annotator labeled at this pixel, and 1 means all annotators have labeled at this pixel. We consider the pixels with edge probability higher than \u03b7 as positive samples and the pixels with edge proba-bility equal to 0 as negative samples. Otherwise, if a pixel is marked by fewer than \u03b7 of the annotators, this pixel may be semantically controversial to be an edge point. Thus, whether regarding it as positive or negative samples will confuse networks. So we ignore pixels in this category.\n\nWe compute the loss at every pixel with respect to pixel label as\nl(X i ; W ) = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 \u03b1 \u00b7 log (1 \u2212 P (X i ; W )) if y i = 0 0 if 0 < y i \u2264 \u03b7 \u03b2 \u00b7 log P (X i ; W ) otherwise,(1)in which \u03b1 = \u03bb \u00b7 |Y + | |Y + | + |Y \u2212 | \u03b2 = |Y \u2212 | |Y + | + |Y \u2212 | .(2)\nY + and Y \u2212 denote positive sample set and negative sample set respectively. \u03bb is a hyper-parameter to balance positive and negative samples. X i and y i are activation value (CNN feature vector) and ground truth edge probability at pixel i, respectively. P (X) is the standard sigmoid function. And W denotes all the parameters that will be learned in our architecture. Therefore, our novel loss function can be formulated as\nL(W ) = |I| i=1 K k=1 l(X (k) i ; W ) + l(X f use i ; W ) ,(3)\nwhere X (k) i is the activation value from stage k while X f use i is from fusion layer. |I| is the number of pixels in image I, and K is the number of stages (equals to 5 here).\n\n\nMultiscale Hierarchical Edge Detection\n\nIn single scale edge detection, we input an original image into our fine-tuned RCF network, then, the output is an edge probability map. To further explore the effectiveness of multiscale hierarchies, we use image pyramids at the test. Concretely speaking, we resize an image to construct an image pyramid, and each of these images is input to our singlescale detector separately. Then, all resulting edge probability maps are resized to original image size using bilinear interpolation. At last, these maps are averaged to get a final prediction map. Fig. 4 shows a visualized pipeline of our multiscale algorithm. We also try to use weighted sum, but we find the simple average works very well. Considering the trade-off between accuracy and speed, we use three scales 0.5, 1.0, and 1.5 in this paper. When evaluating on BSDS500 [2] dataset, this simple multiscale strategy improves the ODS F-measure from 0.806 to 0.811. See Sec. 4 for details.\n\n\nRCF Forward\n\n\nBilinear Interpolation Average\n\nOutput Groundtruth Image Pyramid Figure 4: The pipeline of our multiscale algorithm. The original image is resized to construct an image pyramid. And these multiscale images are input to RCF network for a forward pass. Then, we use bilinear interpolation to restore resulting edge respondence maps to original sizes. A simple average of these edge maps will output high quality edges.\n\n\nComparison With HED\n\nThe most obvious difference between our RCF and HED [50] is in three parts. First, HED only considers the last convolution layer in each stage of VGG16, in which lots of helpful information to edge detection is missed. In contrast to it, RCF uses richer features from all the convolution layers, thus it can capture more object or object part boundaries accurately across a larger range of scales. Second, a novel loss function is proposed to treat training examples properly. We only consider the edge pixels that most annotators labeled as positive samples, since these edges are highly consistent and thus easy to train. Besides, we ignore edge pixels that are marked by a few annotators because of their confusing attributes. Thirdly, a simple way to obtain multiscale hierarchy is proposed to enhance edges. Therefore, RCF is not only a reasonably extended version of HED, but also a bold attempt for edge detection in many ways. As far as I am concerned, it is the first framework to combine all the convolution layers in this community. Our evaluation results demonstrate the effectiveness (1.4% improvement in ODS F-measure over HED) of these choices. See Sec. 4 for details.\n\n\nExperiment\n\nWe implement our network using the publicly available Caffe [25] which is well-known in this community. The VGG16 model that is pre-trained on ImageNet [10] is used to initialize our network. We change the stride of pool4 layer to 1 and use the atrous algorithm to fill the holes. In RCF training, the weights of 1 \u00d7 1 \u00d7 1 convolution layer in each stage are initialized from zero-mean Gaussian distributions with standard deviation 0.01 and biases are ini-tialized to 0. The fusion layer weights are initialized to 0.2. Stochastic gradient descent (SGD) minibatch samples 1 image randomly in each iteration, and the iter size is set to 10. For other SGD hyper-parameters, the global learning rate is set depending on different dataset and will be divided by 10 after every 10k iterations. The momentum and weight decay are set to 0.9 and 0.0002 respectively. We run SGD for 40k iterations totally. The parameters \u03b7 and \u03bb in loss function are also set depending on training data. All experiments in this paper are finished using a NVIDIA TITAN X GPU.\n\nGiven an edge probability map, a threshold is needed to obtain the edge image. There are two choices to set this threshold. The first one is referred as optimal dataset scale (ODS) which employs a fixed threshold for all images in the dataset. And the second is called optimal image scale (OIS) which selects an optimal threshold for each image. We use F-measure ( 2\u00b7P recision\u00b7Recall P recision+Recall ) of both ODS and OIS in our experiments.\n\n\nBSDS500 Dataset\n\nBSDS500 [2] is a widely used dataset in edge detection. It is composed of 200 training, 100 validation and 200 test images, and each image is labeled by 4 to 9 annotators. We utilize the training and validation sets for fine-tuning, and test set for evaluation. Data augmentation is the same as [50]. Inspired by RDS [30] and CEDN [51], we mix augmentation data of BSDS500 with flipped PASCAL VOC Context dataset [35] as training data. When training, we set loss parameters \u03b7 and \u03bb to 0.5 and 1.1, respectively. And initial learning rate is set to 1e-6. When evaluating, standard non-maximum suppression (NMS) [12] is applied to thin detected edges. We compare our method with some   Figure 5: The evaluation results on standard BSDS500 [2] dataset. Both single-scale and multiscale versions of RCF achieve better performance than the human eye.\n\nnon-deep-learning algorithms, including Canny [6], EGB [14], gPb-UCM [2], ISCRA [38], MCG [3], MShift [9], NCut [41], SE [12], and OEF [22], and some recent deep learning based approaches, including DeepContour [40], DeepEdge [4], HED [50], HFL [5] and etc. Fig. 5 shows the evaluation results. The performance of human eye in edge detection is known as .800 ODS Fmeasure. Both single-scale and multiscale versions of RCF achieve better results than human eye. When comparing with HED [50], ODS F-measures of our multiscale and single-scale RCF are 2.3% and 1.8% higher than it, respectively. And the precision-recall curves of our methods are also higher than HED's. These significant improvements demonstrate the effectiveness of our richer convolutional features. All the convolution layers contain helpful hierarchical information, not only the last one in each convolution stage.\n\nFrom single-scale RCF to multiscale version, the accuracy also has some improvement. It proves the effectiveness of our multiscale strategy. We also observe an interesting phenomenon in which the RCF curves are not as long as other methods when evaluated using the default parameters in BSDS500 benchmark. It may suggest that RCF tends only to remain very confident edges. However, it doesn't matter to us, since the best ODS F-measure is much more important in practice. If we consider more pixels as edges, the recall will be higher, but it is meaningless. Others, one can find that all the deep learning based approaches outper- \n\n\nMethod\n\nODS OIS FPS Canny [6] .611 .676 28 EGB [14] .614 .658 10 MShift [9] .598 .645 1/5 gPb-UCM [2] .729 .755 1/240 Sketch Tokens [29] .727 .746 1 MCG [3] .744 .777 1/18 SE [12] .743 .763 2.5 OEF [22] .746 .770 2/3 DeepContour [40] .757 .776 1/30 \u2020 DeepEdge [4] .753 .772 1/1000 \u2020 HFL [5] .767 .788 5/6 \u2020 N 4 -Fields [17] .753 .769 1/6 \u2020 HED [50] .788 .808 30 \u2020 RDS [30] .792 .810 30 \u2020 CEDN [51] . form non-deep ones. We think it is because CNNs can learn rich object-level information automatically, and this automatically learned information is more reliable than humandesigned features. If more training data is accessible, the gap between deep and non-deep methods will be bigger. Others, NCut, EGB, Canny and MShift seem to perform worse than nowadays methods. We show statistic comparison in Tab. 2. RCF substantially achieves the best performance. This fact demonstrates the effectiveness of out novel network architecture. RCF is also one of the fastest edge detectors. The single-scale RCF achieves 30 FPS, and the multiscale RCF can also achieve 8 FPS. Note that we only add some 1 \u00d7 1 convolution layers to HED, so the time consumption is almost same as HED. Comparing with other deep learning based methods, like DeepEdge [4] and DeepContour [40], RCF is hundreds of times faster than them. Thus, we achieve the state-of-theart in both accuracy and efficiency. And this speed of 30 FPS makes it easy to be applied in various tasks. If higher accuracy is needed, the multiscale version which is slightly slower is also a good choice.\n\n\nNetwork Discussion\n\nTo further explore the effectiveness of our network architecture, we implement some thought networks using VGG16 [43] by connecting our richer feature side outputs to some convolution stages while connecting side outputs of HED to the other stages. With training only on BSDS500 [2] dataset, evaluation results of these thought networks are shown in Tab. 3. The last two lines of this table corre-spond to HED and RCF, respectively. We can observe that all of these thought networks perform better than HED and worse than RCF that is fully connected to RCF side outputs. It clearly demonstrates the importance of our strategy of richer convolutional features. \n\n\nNYUD Dataset\n\nNYUD [42] dataset is composed of 1449 densely labeled pairs of aligned RGB and depth images. Recently many works have conducted edge evaluation on it, such as [12,49]. Gupta et al. [20] split NYUD dataset into 381 training, 414 validation and 654 testing images. We follow their settings and train our RCF network using training and validation sets in full resolution as in HED [50].   Figure 6: The evaluation results on NYUD [42] dataset. RCF is referred to single-scale version here.\n\nWe utilize depth information as HED [50] by using HHA [21], in which depth information is encoded into three channels: horizontal disparity, height above ground, and angle with gravity. Thus HHA features can be represented as a color image. Then, two models for RGB images and HHA feature images are trained separately. In the training process, \u03bb is set to 1.2, and global learning rates for RGB and HHA are both set to 1e-6. Since NYUD only has one ground truth for each image, \u03b7 is useless here. Other network settings are the same as used for BSDS500. At testing, the final edge predictions are defined by averaging the outputs of RGB model and HHA model. When evaluating, we increase localization tolerance, which controls the maximum allowed distance in matches between predicted edges and ground truth, from 0.0075 to 0.011. Table 4: The comparison with some competitors on NYUD dataset [42]. \u2020means GPU time.\n\n\nMethod\n\nODS OIS FPS OEF [22] .651 .667 1/2 gPb-UCM [2] .631 .661 1/360 gPb+NG [20] .687 .716 1/375 SE [12] .695 .708 5 SE+NG+ [21] .706 .734 1/15 HED-HHA [50] .681 .695 20 \u2020 HED-RGB [50] .717 .732 20 \u2020 HED-RGB-HHA [50] .741 . 757  We only compare our single-scale version of RCF with some famous competitors. OEF [22] and gPb-UCM [2] only use RGB images, while other methods employ both depth and RGB information. The precision-recall curves are shown in Fig. 6. RCF achieves the best performance on NYUD dataset, and the second place is HED [50]. Tab. 4 shows statistical comparison. We can see that RCF achieves better results than HED not only on separate HHA or RGB data, but also on the merged HHA-RGB data. For HHA and RGB data, ODS F-measure of RCF is 2.4% and 1.2% higher than HED, respectively. For merging HHA-RGB data, RCF is 1.6% higher than HED. Others, HHA edges performance worse than RGB, but averaging HHA and RGB edges achieving much higher results. It suggests that combining different types of information is very useful for edge detection, and it's also the reason why OEF and gPb-UCM perform worse than other methods.\n\n\nMulticue Dataset\n\nRecently, Multicue dataset is proposed by M\u00e9ly et al. [34] to study psychophysics theory for boundary detection. It is composed of short binocular video sequences of 100 challenging natural scenes captured by a stereo camera. Each scene contains a left and right view short (10- Figure 7: Some examples of RCF. From top to bottom: BSDS500 [2], NYUD [42], Multicue-Boundary [34], and Multicue-Edge [34]. From left to right: origin image, ground truth, RCF edge map, origin image, ground truth, and RCF edge map. frame) color sequence. The last frame of the left image for each scene is labeled for two annotations, object boundaries and low-level edges. Unlike people who usually use boundary and edge interchangeably, they strictly defined boundary and edge according to visual perception at different stages. Thus, boundaries are referred to the boundary pixels of meaningful objects, and edges are abrupt pixels at which the luminance, color or stereo change sharply. In this subsection, we use boundary and edge as defined by M\u00e9ly et al. [34] while considering boundary and edge having the same meaning in previous sections. As done in [34] and recent version of HED 1 , we randomly split these humanlabeled images into 80 training and 20 test samples, and average the scores of three independent trials as final results. When training on Multicue, \u03bb and initial learning rate are set to 1.1 and 1e-6 respectively. And \u03b7 is set to 0.4 for boundary task and 0.3 for edge task. Since the image resolution of Multicue is very high, we randomly crop 400\u00d7400 patches. When evaluating, we adjust the localization tolerance of benchmark [2] to 0.011 as on NYUD dataset.\n\nWe show evaluation results in Tab. 5. Our proposed RCF achieve substantially higher results than HED. For boundary task, RCF is 2.6% ODS F-measure higher and 2.8% OIS F-measure higher than HED. For edge task, RCF is 4.5% ODS F-measure higher and 5.1% OIS F-measure higher than HED. The great improvement on Multicue edge task may be due to more fine information extracted by RCF. Thus, we can infer that RCF can obtain not only more semantic features but also more fine detail features. This char-  [34].\n\n\nMethod\n\nODS OIS Human-Boundary [34] .760 (.017) -Multicue-Boundary [34] .720 (.014) -HED-Boundary [50] .821 (.021) .826 (.012) RCF-Boundary .847 (.007) .854 (.007) Human-Edge [34] .750 (.024) -Multicue-Edge [34] .830 (.002) -HED-Edge [50] .827 (.013) .829 (.007) RCF-Edge .872 (.006) .880 (.005) acteristic is very useful in many vision tasks. Note that the fluctuation of RCF is also much smaller than HED, which suggests RCF is more robust over different kinds of images.\n\n\nConclusion\n\nIn this paper, we propose a novel CNN architecture, RCF, that makes full use of semantic and fine detail features to carry out edge detection. We carefully design it as an extensible architecture. The resulting RCF method can obtain high-quality edges very efficiently, and this makes it promising to be easily applied to various tasks. RCF architecture can be seen as a development direction of fully connected network, like FCN [31] and HED [50]. It would be interesting to explore the effectiveness of our network architecture in other hot topics, such as salient object detection and semantic segmentation. Source code will be published with this paper.\n\nFigure 3 :\n3Several examples of the outputs in each stage of RCF. The top line is the original images from BSDS500 [2]. From second to sixth line is the output of stage 1, stage 2 and stage 3, stage 4 and stage 5 respectively. edge detection. We show the intermediate results from each stage in\n\n\nand ob-3\u00d73\u00d764 conv \n\n2\u00d72 pool \n\n3\u00d73\u00d7128 conv \n\n1\u00d71\u00d721 conv \n1\u00d71\u00d71 conv \nloss/sigmoid \n\nupsample \n\nconcat \n\nstage 1 \n\nstage 2 \n\nstage 3 \n\nstage 4 \n\nimage \n\nRCF \n\nconcat \n\n1\u00d71\u00d721 conv \n\n1\u00d71\u00d721 conv \n\n1\u00d71\u00d721 conv \n\n1\u00d71\u00d721 conv \n\n1\u00d71\u00d721 conv \n\n1\u00d71\u00d721 conv \n\n1\u00d71\u00d721 conv \n\n1\u00d71\u00d721 conv \n\n1\u00d71\u00d721 conv \n\n1\u00d71\u00d721 conv \n\n1\u00d71\u00d721 conv \n\n1\u00d71\u00d721 conv \n\n1\u00d71\u00d71 conv \n\n1\u00d71\u00d71 conv \n\n1\u00d71\u00d71 conv \n\n1\u00d71\u00d71 conv \n\n1\u00d71\u00d71 conv \n\n3\u00d73\u00d764 conv \n\n3\u00d73\u00d7128 conv \n\n3\u00d73\u00d7256 conv \n\n3\u00d73\u00d7256 conv \n\n3\u00d73\u00d7256 conv \n\n3\u00d73\u00d7512 conv \n\n3\u00d73\u00d7512 conv \n\n3\u00d73\u00d7512 conv \n\n3\u00d73\u00d7512 conv \n\n3\u00d73\u00d7512 conv \n\n3\u00d73\u00d7512 conv \n\nloss/sigmoid \n\nloss/sigmoid \n\nloss/sigmoid \n\nloss/sigmoid \n\nloss/sigmoid \n\n2\u00d72 pool \n\n2\u00d72 pool \n\n2\u00d72 pool \n\nstage 5 \n\nupsample \n\nupsample \n\nupsample \n\nconcat \n\nconcat \n\nconcat \n\nconcat \n\n\n\nTable 1 :\n1Detailed receptive field and stride sizes of standard \nVGG16 net [43]. \n\nlayer conv1 1 conv1 2 pool1 conv2 1 conv2 2 pool2 \nrf size \n3 \n5 \n6 \n10 \n14 \n16 \nstride \n1 \n1 \n2 \n2 \n2 \n4 \nlayer conv3 1 conv3 2 conv3 3 pool3 conv4 1 conv4 2 \nrf size \n24 \n32 \n40 \n44 \n60 \n76 \nstride \n4 \n4 \n4 \n8 \n8 \n8 \nlayer conv4 3 pool4 conv5 1 conv5 2 conv5 3 pool5 \nrf size \n92 \n100 \n132 \n164 \n196 \n212 \nstride \n8 \n16 \n16 \n16 \n16 \n32 \n\n\n\nTable 2 :\n2The comparison with some competitors on BSDS500[2] dataset. \u2020means GPU time.\n\nTable 3 :\n3Results of some thought networks.RCF Stage HED Stage ODS OIS \n1, 2 \n3, 4, 5 \n.792 .810 \n2, 4 \n1, 3, 5 \n.795 .812 \n4, 5 \n1, 2, 3 \n.790 .810 \n1, 3, 5 \n2, 4 \n.794 .810 \n3, 4, 5 \n1, 2 \n.796 .812 \n-\n1, 2, 3, 4, 5 .788 .808 \n1, 2, 3, 4, 5 \n-\n.798 .815 \n\n\n\n\n10 \u2020 RCF-HHA .705 .715 20 \u2020 RCF-RGB .729 .742 20 \u2020 RCF-HHA-RGB .757 .771 10 \u2020\n\nTable 5 :\n5The comparison with some competitors on Multicue dataset\nDetails can be seen in recent journal version of HED which is available at the authors' homepage (http://pages.ucsd.edu/\u02dcztu/ Publication.htm).\n\nFrom contours to regions: An empirical evaluation. P Arbel\u00e1ez, M Maire, C Fowlkes, J Malik, IEEE CVPR. IEEEP. Arbel\u00e1ez, M. Maire, C. Fowlkes, and J. Malik. From con- tours to regions: An empirical evaluation. In IEEE CVPR, pages 2294-2301. IEEE, 2009. 1\n\nContour detection and hierarchical image segmentation. P Arbel\u00e1ez, M Maire, C Fowlkes, J Malik, IEEE TPAMI. 335P. Arbel\u00e1ez, M. Maire, C. Fowlkes, and J. Malik. Contour de- tection and hierarchical image segmentation. IEEE TPAMI, 33(5):898-916, 2011. 1, 2, 4, 5, 6, 7, 8\n\nMultiscale combinatorial grouping. P Arbel\u00e1ez, J Pont-Tuset, J T Barron, F Marques, J Malik, IEEE CVPR. 16P. Arbel\u00e1ez, J. Pont-Tuset, J. T. Barron, F. Marques, and J. Malik. Multiscale combinatorial grouping. In IEEE CVPR, pages 328-335, 2014. 1, 6\n\nDeepEdge: A multiscale bifurcated deep network for top-down contour detection. G Bertasius, J Shi, L Torresani, IEEE CVPR. 16G. Bertasius, J. Shi, and L. Torresani. DeepEdge: A multi- scale bifurcated deep network for top-down contour detec- tion. In IEEE CVPR, pages 4380-4389, 2015. 1, 6\n\nHigh-for-low and lowfor-high: Efficient boundary detection from deep object features and its applications to high-level vision. G Bertasius, J Shi, L Torresani, IEEE ICCV. G. Bertasius, J. Shi, and L. Torresani. High-for-low and low- for-high: Efficient boundary detection from deep object fea- tures and its applications to high-level vision. In IEEE ICCV, pages 504-512, 2015. 6\n\nA computational approach to edge detection. J Canny, IEEE TPAMI. 266J. Canny. A computational approach to edge detection. IEEE TPAMI, (6):679-698, 1986. 2, 6\n\nSemantic image segmentation with deep convolutional nets and fully connected crfs. L.-C Chen, G Papandreou, I Kokkinos, K Murphy, A L Yuille, arXiv:1412.7062arXiv preprintL.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. Semantic image segmentation with deep con- volutional nets and fully connected crfs. arXiv preprint arXiv:1412.7062, 2014. 1\n\nHfs: Hierarchical feature selection for efficient image segmentation. M.-M Cheng, Y Liu, Q Hou, J Bian, P Torr, S.-M Hu, Z Tu, ECCV. SpringerM.-M. Cheng, Y. Liu, Q. Hou, J. Bian, P. Torr, S.-M. Hu, and Z. Tu. Hfs: Hierarchical feature selection for efficient image segmentation. In ECCV, pages 867-882. Springer, 2016. 1\n\nMean shift: A robust approach toward feature space analysis. D Comaniciu, P Meer, IEEE TPAMI. 245D. Comaniciu and P. Meer. Mean shift: A robust approach toward feature space analysis. IEEE TPAMI, 24(5):603-619, 2002. 6\n\nImagenet: A large-scale hierarchical image database. J Deng, W Dong, R Socher, L.-J Li, K Li, L Fei-Fei, IEEE CVPR. IEEEJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei- Fei. Imagenet: A large-scale hierarchical image database. In IEEE CVPR, pages 248-255. IEEE, 2009. 5\n\nSupervised learning of edges and object boundaries. P Doll\u00e1r, Z Tu, S Belongie, IEEE CVPR. 2P. Doll\u00e1r, Z. Tu, and S. Belongie. Supervised learning of edges and object boundaries. In IEEE CVPR, volume 2, pages 1964-1971. IEEE, 2006. 2\n\nFast edge detection using structured forests. P Doll\u00e1r, C L Zitnick, IEEE TPAMI. 3787P. Doll\u00e1r and C. L. Zitnick. Fast edge detection using struc- tured forests. IEEE TPAMI, 37(8):1558-1570, 2015. 1, 2, 5, 6, 7\n\nPattern classification and scene analysis. R O Duda, P E Hart, Wiley3New YorkR. O. Duda, P. E. Hart, et al. Pattern classification and scene analysis, volume 3. Wiley New York, 1973. 2\n\nEfficient graphbased image segmentation. P F Felzenszwalb, D P Huttenlocher, IJCV. 592P. F. Felzenszwalb and D. P. Huttenlocher. Efficient graph- based image segmentation. IJCV, 59(2):167-181, 2004. 6\n\nGroups of adjacent contour segments for object detection. V Ferrari, L Fevrier, F Jurie, C Schmid, IEEE TPAMI. 301V. Ferrari, L. Fevrier, F. Jurie, and C. Schmid. Groups of ad- jacent contour segments for object detection. IEEE TPAMI, 30(1):36-51, 2008. 1\n\nOn the quantitative evaluation of edge detection schemes and their comparison with human performance. J R Fram, E S Deutsch, IEEE TOC. 1006J. R. Fram and E. S. Deutsch. On the quantitative evaluation of edge detection schemes and their comparison with human performance. IEEE TOC, 100(6):616-628, 1975. 2\n\nN 4 -Fields: Neural network nearest neighbor fields for image transforms. Y Ganin, V Lempitsky, ACCV. Springer6Y. Ganin and V. Lempitsky. N 4 -Fields: Neural network near- est neighbor fields for image transforms. In ACCV, pages 536-551. Springer, 2014. 1, 2, 6\n\nFast R-CNN. R Girshick, IEEE ICCV. 13R. Girshick. Fast R-CNN. In IEEE ICCV, pages 1440-1448, 2015. 1, 3\n\nRich feature hierarchies for accurate object detection and semantic segmentation. R Girshick, J Donahue, T Darrell, J Malik, IEEE CVPR. 13R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea- ture hierarchies for accurate object detection and semantic segmentation. In IEEE CVPR, pages 580-587, 2014. 1, 3\n\nPerceptual organization and recognition of indoor scenes from rgb-d images. S Gupta, P Arbelaez, J Malik, IEEE CVPR. [20] S. Gupta, P. Arbelaez, and J. Malik. Perceptual organization and recognition of indoor scenes from rgb-d images. In IEEE CVPR, pages 564-571, 2013. 7\n\nLearning rich features from rgb-d images for object detection and segmentation. S Gupta, R Girshick, P Arbel\u00e1ez, J Malik, ECCV. SpringerS. Gupta, R. Girshick, P. Arbel\u00e1ez, and J. Malik. Learning rich features from rgb-d images for object detection and seg- mentation. In ECCV, pages 345-360. Springer, 2014. 7\n\nOriented edge forests for boundary detection. S Hallman, C C Fowlkes, IEEE CVPR. 67S. Hallman and C. C. Fowlkes. Oriented edge forests for boundary detection. In IEEE CVPR, pages 1732-1740, 2015. 6, 7\n\nPixel-wise deep learning for contour detection. J.-J Hwang, T.-L Liu, arXiv:1504.019891arXiv preprintJ.-J. Hwang and T.-L. Liu. Pixel-wise deep learning for con- tour detection. arXiv preprint arXiv:1504.01989, 2015. 1, 2\n\nF Iandola, M Moskewicz, S Karayev, R Girshick, T Darrell, K Keutzer, arXiv:1404.1869Densenet: Implementing efficient convnet descriptor pyramids. arXiv preprintF. Iandola, M. Moskewicz, S. Karayev, R. Girshick, T. Dar- rell, and K. Keutzer. Densenet: Implementing efficient con- vnet descriptor pyramids. arXiv preprint arXiv:1404.1869, 2014. 2\n\nCaffe: Convolutional architecture for fast feature embedding. Y Jia, E Shelhamer, J Donahue, S Karayev, J Long, R Girshick, S Guadarrama, T Darrell, ACM MM. ACMY. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir- shick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. In ACM MM, pages 675-678. ACM, 2014. 5\n\nStatistical edge detection: Learning and evaluating edge cues. S Konishi, A L Yuille, J M Coughlan, S C Zhu, IEEE TPAMI. 251S. Konishi, A. L. Yuille, J. M. Coughlan, and S. C. Zhu. Sta- tistical edge detection: Learning and evaluating edge cues. IEEE TPAMI, 25(1):57-74, 2003. 2\n\nImagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, NIPS. A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, pages 1097-1105, 2012. 1\n\nUnsupervised learning of edges. Y Li, M Paluri, J M Rehg, P Doll\u00e1r, IEEE CVPR. Y. Li, M. Paluri, J. M. Rehg, and P. Doll\u00e1r. Unsupervised learning of edges. In IEEE CVPR, pages 1619-1627, 2016. 2\n\nSketch tokens: A learned mid-level representation for contour and object detection. J J Lim, C L Zitnick, P Doll\u00e1r, IEEE CVPR. 26J. J. Lim, C. L. Zitnick, and P. Doll\u00e1r. Sketch tokens: A learned mid-level representation for contour and object de- tection. In IEEE CVPR, pages 3158-3165, 2013. 1, 2, 6\n\nLearning relaxed deep supervision for better edge detection. Y Liu, M S Lew, IEEE CVPR. 6Y. Liu and M. S. Lew. Learning relaxed deep supervision for better edge detection. In IEEE CVPR, pages 231-240, 2016. 2, 5, 6\n\nFully convolutional networks for semantic segmentation. J Long, E Shelhamer, T Darrell, IEEE CVPR. 1J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In IEEE CVPR, pages 3431-3440, 2015. 1, 8\n\nTheory of edge detection. Proceedings of the. D Marr, E Hildreth, Royal Society of London B: Biological Sciences. 2072D. Marr and E. Hildreth. Theory of edge detection. Proceed- ings of the Royal Society of London B: Biological Sciences, 207(1167):187-217, 1980. 2\n\nLearning to detect natural image boundaries using local brightness, color, and texture cues. D R Martin, C C Fowlkes, J Malik, IEEE TPAMI. 265D. R. Martin, C. C. Fowlkes, and J. Malik. Learning to detect natural image boundaries using local brightness, color, and texture cues. IEEE TPAMI, 26(5):530-549, 2004. 1, 2\n\nA systematic comparison between visual cues for boundary detection. D A M\u00e9ly, J Kim, M Mcgill, Y Guo, T Serre, Vision research. 120D. A. M\u00e9ly, J. Kim, M. McGill, Y. Guo, and T. Serre. A sys- tematic comparison between visual cues for boundary detec- tion. Vision research, 120:93-107, 2016. 7, 8\n\nThe role of context for object detection and semantic segmentation in the wild. R Mottaghi, X Chen, X Liu, N.-G Cho, S.-W Lee, S Fidler, R Urtasun, A Yuille, IEEE CVPR. R. Mottaghi, X. Chen, X. Liu, N.-G. Cho, S.-W. Lee, S. Fi- dler, R. Urtasun, and A. Yuille. The role of context for object detection and semantic segmentation in the wild. In IEEE CVPR, pages 891-898, 2014. 5\n\nFaster R-CNN: Towards real-time object detection with region proposal networks. S Ren, K He, R Girshick, J Sun, NIPS. 13S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: To- wards real-time object detection with region proposal net- works. In NIPS, pages 91-99, 2015. 1, 3\n\nMulti-scale improves boundary detection in natural images. X Ren, ECCV. SpringerX. Ren. Multi-scale improves boundary detection in natural images. In ECCV, pages 533-545. Springer, 2008. 2\n\nImage segmentation by cascaded region agglomeration. Z Ren, G Shakhnarovich, IEEE CVPR. Z. Ren and G. Shakhnarovich. Image segmentation by cas- caded region agglomeration. In IEEE CVPR, pages 2011- 2018, 2013. 6\n\nColor edge detection. G S Robinson, Optical Engineering. 165G. S. Robinson. Color edge detection. Optical Engineering, 16(5):165479-165479, 1977. 2\n\nDeep-Contour: A deep convolutional feature learned by positivesharing loss for contour detection. W Shen, X Wang, Y Wang, X Bai, Z Zhang, IEEE CVPR. 16W. Shen, X. Wang, Y. Wang, X. Bai, and Z. Zhang. Deep- Contour: A deep convolutional feature learned by positive- sharing loss for contour detection. In IEEE CVPR, pages 3982-3991, 2015. 1, 2, 6\n\nNormalized cuts and image segmentation. J Shi, J Malik, IEEE TPAMI. 2286J. Shi and J. Malik. Normalized cuts and image segmenta- tion. IEEE TPAMI, 22(8):888-905, 2000. 2, 6\n\nIndoor segmentation and support inference from rgbd images. N Silberman, D Hoiem, P Kohli, R Fergus, European Conference on Computer Vision. Springer7N. Silberman, D. Hoiem, P. Kohli, and R. Fergus. Indoor segmentation and support inference from rgbd images. In European Conference on Computer Vision, pages 746-760. Springer, 2012. 7, 8\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, arXiv:1409.15566arXiv preprintK. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. 1, 2, 3, 6\n\nCamera models and machine perception. I Sobel, DTIC DocumentTechnical reportI. Sobel. Camera models and machine perception. Technical report, DTIC Document, 1970. 2\n\nGoing deeper with convolutions. C Szegedy, W Liu, Y Jia, P Sermanet, S Reed, D Anguelov, D Erhan, V Vanhoucke, A Rabinovich, IEEE CVPR. C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In IEEE CVPR, pages 1-9, 2015. 1\n\nOn edge detection. V Torre, T A Poggio, IEEE TPAMI. 2V. Torre and T. A. Poggio. On edge detection. IEEE TPAMI, (2):147-163, 1986. 2\n\nSelective search for object recognition. J R Uijlings, K E Van De Sande, T Gevers, A W Smeulders, IJCV. 1042J. R. Uijlings, K. E. van de Sande, T. Gevers, and A. W. Smeulders. Selective search for object recognition. IJCV, 104(2):154-171, 2013. 1\n\nRecognition by linear combinations of models. S Ullman, R Basri, IEEE TPAMI. 1310S. Ullman and R. Basri. Recognition by linear combinations of models. IEEE TPAMI, 13(10):992-1006, 1991. 1\n\nDiscriminatively trained sparse code gradients for contour detection. R Xiaofeng, L Bo, NIPS. 17R. Xiaofeng and L. Bo. Discriminatively trained sparse code gradients for contour detection. In NIPS, pages 584-592, 2012. 1, 7\n\nHolistically-nested edge detection. S Xie, Z Tu, IEEE ICCV. 7S. Xie and Z. Tu. Holistically-nested edge detection. In IEEE ICCV, pages 1395-1403, 2015. 1, 2, 5, 6, 7, 8\n\nObject contour detection with a fully convolutional encoder-decoder network. J Yang, B Price, S Cohen, H Lee, M.-H Yang, arXiv:1603.0453056arXiv preprintJ. Yang, B. Price, S. Cohen, H. Lee, and M.-H. Yang. Object contour detection with a fully convolutional encoder-decoder network. arXiv preprint arXiv:1603.04530, 2016. 5, 6\n\nEdge boxes: Locating object proposals from edges. C L Zitnick, P Doll\u00e1r, ECCV. SpringerC. L. Zitnick and P. Doll\u00e1r. Edge boxes: Locating object proposals from edges. In ECCV, pages 391-405. Springer, 2014. 1\n", "annotations": {"author": "[{\"end\":80,\"start\":52},{\"end\":117,\"start\":81},{\"end\":149,\"start\":118},{\"end\":184,\"start\":150}]", "publisher": null, "author_last_name": "[{\"end\":59,\"start\":56},{\"end\":96,\"start\":91},{\"end\":128,\"start\":126},{\"end\":163,\"start\":159}]", "author_first_name": "[{\"end\":55,\"start\":52},{\"end\":90,\"start\":81},{\"end\":125,\"start\":118},{\"end\":153,\"start\":150},{\"end\":158,\"start\":154}]", "author_affiliation": "[{\"end\":79,\"start\":61},{\"end\":116,\"start\":98},{\"end\":148,\"start\":130},{\"end\":183,\"start\":165}]", "title": "[{\"end\":49,\"start\":1},{\"end\":233,\"start\":185}]", "venue": null, "abstract": "[{\"end\":1299,\"start\":278}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b14\"},\"end\":1676,\"start\":1672},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":1679,\"start\":1676},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":1701,\"start\":1697},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":1704,\"start\":1701},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1731,\"start\":1728},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1733,\"start\":1731},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":1735,\"start\":1733},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":1886,\"start\":1882},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1895,\"start\":1892},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":1919,\"start\":1915},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":1963,\"start\":1959},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":1966,\"start\":1963},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":2428,\"start\":2424},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2819,\"start\":2816},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2845,\"start\":2841},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3187,\"start\":3183},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":3190,\"start\":3187},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":3193,\"start\":3190},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3216,\"start\":3212},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3219,\"start\":3216},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":3222,\"start\":3219},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3252,\"start\":3249},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3255,\"start\":3252},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3543,\"start\":3540},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3561,\"start\":3557},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3573,\"start\":3569},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":3591,\"start\":3587},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":3605,\"start\":3601},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":3820,\"start\":3816},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5342,\"start\":5338},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5345,\"start\":5342},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":5348,\"start\":5345},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":5826,\"start\":5822},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":5959,\"start\":5955},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":5962,\"start\":5959},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":6022,\"start\":6018},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6200,\"start\":6197},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6729,\"start\":6725},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":6732,\"start\":6729},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6753,\"start\":6749},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6911,\"start\":6907},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7072,\"start\":7069},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":7133,\"start\":7129},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7202,\"start\":7198},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7317,\"start\":7313},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7893,\"start\":7889},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":7984,\"start\":7980},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8095,\"start\":8091},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8191,\"start\":8187},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":8337,\"start\":8333},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":8697,\"start\":8693},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8729,\"start\":8725},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8872,\"start\":8868},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":9648,\"start\":9644},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":9818,\"start\":9814},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9988,\"start\":9984},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9991,\"start\":9988},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9994,\"start\":9991},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":15192,\"start\":15189},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":15818,\"start\":15814},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":17024,\"start\":17020},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":17116,\"start\":17112},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":18487,\"start\":18484},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":18775,\"start\":18771},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":18797,\"start\":18793},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":18811,\"start\":18807},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":18893,\"start\":18889},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":19090,\"start\":19086},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":19216,\"start\":19213},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":19372,\"start\":19369},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":19382,\"start\":19378},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":19395,\"start\":19392},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":19407,\"start\":19403},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":19416,\"start\":19413},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":19428,\"start\":19425},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":19439,\"start\":19435},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":19448,\"start\":19444},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":19462,\"start\":19458},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":19538,\"start\":19534},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":19552,\"start\":19549},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":19562,\"start\":19558},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":19571,\"start\":19568},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":19812,\"start\":19808},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":20873,\"start\":20870},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":20895,\"start\":20891},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":20919,\"start\":20916},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":20945,\"start\":20942},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":20980,\"start\":20976},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":21000,\"start\":20997},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":21023,\"start\":21019},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":21046,\"start\":21042},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":21077,\"start\":21073},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":21107,\"start\":21104},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":21134,\"start\":21131},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":21167,\"start\":21163},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":21192,\"start\":21188},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":21216,\"start\":21212},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":21241,\"start\":21237},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":22083,\"start\":22080},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":22104,\"start\":22100},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":22530,\"start\":22526},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":22695,\"start\":22692},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":23099,\"start\":23095},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":23253,\"start\":23249},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":23256,\"start\":23253},{\"end\":23275,\"start\":23258},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":23472,\"start\":23468},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":23521,\"start\":23517},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":23618,\"start\":23614},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":23636,\"start\":23632},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":24475,\"start\":24471},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":24524,\"start\":24520},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":24550,\"start\":24547},{\"end\":24578,\"start\":24574},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":24602,\"start\":24598},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":24626,\"start\":24622},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":24654,\"start\":24650},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":24682,\"start\":24678},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":24714,\"start\":24710},{\"end\":24725,\"start\":24722},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":24813,\"start\":24809},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":24829,\"start\":24826},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":25042,\"start\":25038},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":25714,\"start\":25710},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":25998,\"start\":25995},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":26009,\"start\":26005},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":26033,\"start\":26029},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":26057,\"start\":26053},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":26701,\"start\":26697},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":26799,\"start\":26795},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":27292,\"start\":27289},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":27826,\"start\":27822},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":27865,\"start\":27861},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":27901,\"start\":27897},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":27932,\"start\":27928},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":28009,\"start\":28005},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":28041,\"start\":28037},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":28068,\"start\":28064},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":28752,\"start\":28748},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":28765,\"start\":28761},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":30517,\"start\":30514}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":29271,\"start\":28976},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":30028,\"start\":29272},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":30454,\"start\":30029},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":30543,\"start\":30455},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":30804,\"start\":30544},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":30884,\"start\":30805},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":30953,\"start\":30885}]", "paragraph": "[{\"end\":1736,\"start\":1315},{\"end\":2724,\"start\":1738},{\"end\":2979,\"start\":2726},{\"end\":3654,\"start\":2981},{\"end\":4956,\"start\":3656},{\"end\":5232,\"start\":4958},{\"end\":5718,\"start\":5250},{\"end\":6529,\"start\":5720},{\"end\":7754,\"start\":6531},{\"end\":9017,\"start\":7756},{\"end\":9482,\"start\":9019},{\"end\":9967,\"start\":9545},{\"end\":10540,\"start\":9969},{\"end\":10847,\"start\":10542},{\"end\":11046,\"start\":10849},{\"end\":11174,\"start\":11048},{\"end\":11265,\"start\":11176},{\"end\":11399,\"start\":11267},{\"end\":12185,\"start\":11401},{\"end\":12509,\"start\":12187},{\"end\":13394,\"start\":12536},{\"end\":13461,\"start\":13396},{\"end\":14073,\"start\":13647},{\"end\":14315,\"start\":14137},{\"end\":15305,\"start\":14358},{\"end\":15738,\"start\":15354},{\"end\":16945,\"start\":15762},{\"end\":18010,\"start\":16960},{\"end\":18456,\"start\":18012},{\"end\":19321,\"start\":18476},{\"end\":20207,\"start\":19323},{\"end\":20841,\"start\":20209},{\"end\":22390,\"start\":20852},{\"end\":23073,\"start\":22413},{\"end\":23576,\"start\":23090},{\"end\":24493,\"start\":23578},{\"end\":25635,\"start\":24504},{\"end\":27321,\"start\":25656},{\"end\":27827,\"start\":27323},{\"end\":28303,\"start\":27838},{\"end\":28975,\"start\":28318}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13575,\"start\":13462},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13646,\"start\":13575},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14136,\"start\":14074}]", "table_ref": "[{\"end\":24416,\"start\":24409}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1313,\"start\":1301},{\"attributes\":{\"n\":\"2.\"},\"end\":5248,\"start\":5235},{\"attributes\":{\"n\":\"3.\"},\"end\":9520,\"start\":9485},{\"attributes\":{\"n\":\"3.1.\"},\"end\":9543,\"start\":9523},{\"attributes\":{\"n\":\"3.2.\"},\"end\":12534,\"start\":12512},{\"attributes\":{\"n\":\"3.3.\"},\"end\":14356,\"start\":14318},{\"end\":15319,\"start\":15308},{\"end\":15352,\"start\":15322},{\"attributes\":{\"n\":\"3.4.\"},\"end\":15760,\"start\":15741},{\"attributes\":{\"n\":\"4.\"},\"end\":16958,\"start\":16948},{\"attributes\":{\"n\":\"4.1.\"},\"end\":18474,\"start\":18459},{\"end\":20850,\"start\":20844},{\"attributes\":{\"n\":\"4.2.\"},\"end\":22411,\"start\":22393},{\"attributes\":{\"n\":\"4.3.\"},\"end\":23088,\"start\":23076},{\"end\":24502,\"start\":24496},{\"attributes\":{\"n\":\"4.4.\"},\"end\":25654,\"start\":25638},{\"end\":27836,\"start\":27830},{\"attributes\":{\"n\":\"5.\"},\"end\":28316,\"start\":28306},{\"end\":28987,\"start\":28977},{\"end\":30039,\"start\":30030},{\"end\":30465,\"start\":30456},{\"end\":30554,\"start\":30545},{\"end\":30895,\"start\":30886}]", "table": "[{\"end\":30028,\"start\":29281},{\"end\":30454,\"start\":30041},{\"end\":30804,\"start\":30589}]", "figure_caption": "[{\"end\":29271,\"start\":28989},{\"end\":29281,\"start\":29274},{\"end\":30543,\"start\":30467},{\"end\":30589,\"start\":30556},{\"end\":30884,\"start\":30807},{\"end\":30953,\"start\":30897}]", "figure_ref": "[{\"end\":2381,\"start\":2373},{\"end\":3862,\"start\":3856},{\"end\":9827,\"start\":9819},{\"end\":10469,\"start\":10463},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11760,\"start\":11754},{\"end\":14916,\"start\":14910},{\"end\":15395,\"start\":15387},{\"end\":19168,\"start\":19160},{\"end\":19587,\"start\":19581},{\"end\":23484,\"start\":23476},{\"end\":24957,\"start\":24951},{\"end\":25943,\"start\":25935}]", "bib_author_first_name": "[{\"end\":31151,\"start\":31150},{\"end\":31163,\"start\":31162},{\"end\":31172,\"start\":31171},{\"end\":31183,\"start\":31182},{\"end\":31410,\"start\":31409},{\"end\":31422,\"start\":31421},{\"end\":31431,\"start\":31430},{\"end\":31442,\"start\":31441},{\"end\":31661,\"start\":31660},{\"end\":31673,\"start\":31672},{\"end\":31687,\"start\":31686},{\"end\":31689,\"start\":31688},{\"end\":31699,\"start\":31698},{\"end\":31710,\"start\":31709},{\"end\":31955,\"start\":31954},{\"end\":31968,\"start\":31967},{\"end\":31975,\"start\":31974},{\"end\":32295,\"start\":32294},{\"end\":32308,\"start\":32307},{\"end\":32315,\"start\":32314},{\"end\":32593,\"start\":32592},{\"end\":32794,\"start\":32790},{\"end\":32802,\"start\":32801},{\"end\":32816,\"start\":32815},{\"end\":32828,\"start\":32827},{\"end\":32838,\"start\":32837},{\"end\":32840,\"start\":32839},{\"end\":33147,\"start\":33143},{\"end\":33156,\"start\":33155},{\"end\":33163,\"start\":33162},{\"end\":33170,\"start\":33169},{\"end\":33178,\"start\":33177},{\"end\":33189,\"start\":33185},{\"end\":33195,\"start\":33194},{\"end\":33457,\"start\":33456},{\"end\":33470,\"start\":33469},{\"end\":33669,\"start\":33668},{\"end\":33677,\"start\":33676},{\"end\":33685,\"start\":33684},{\"end\":33698,\"start\":33694},{\"end\":33704,\"start\":33703},{\"end\":33710,\"start\":33709},{\"end\":33948,\"start\":33947},{\"end\":33958,\"start\":33957},{\"end\":33964,\"start\":33963},{\"end\":34177,\"start\":34176},{\"end\":34187,\"start\":34186},{\"end\":34189,\"start\":34188},{\"end\":34386,\"start\":34385},{\"end\":34388,\"start\":34387},{\"end\":34396,\"start\":34395},{\"end\":34398,\"start\":34397},{\"end\":34570,\"start\":34569},{\"end\":34572,\"start\":34571},{\"end\":34588,\"start\":34587},{\"end\":34590,\"start\":34589},{\"end\":34789,\"start\":34788},{\"end\":34800,\"start\":34799},{\"end\":34811,\"start\":34810},{\"end\":34820,\"start\":34819},{\"end\":35090,\"start\":35089},{\"end\":35092,\"start\":35091},{\"end\":35100,\"start\":35099},{\"end\":35102,\"start\":35101},{\"end\":35368,\"start\":35367},{\"end\":35377,\"start\":35376},{\"end\":35569,\"start\":35568},{\"end\":35744,\"start\":35743},{\"end\":35756,\"start\":35755},{\"end\":35767,\"start\":35766},{\"end\":35778,\"start\":35777},{\"end\":36052,\"start\":36051},{\"end\":36061,\"start\":36060},{\"end\":36073,\"start\":36072},{\"end\":36329,\"start\":36328},{\"end\":36338,\"start\":36337},{\"end\":36350,\"start\":36349},{\"end\":36362,\"start\":36361},{\"end\":36606,\"start\":36605},{\"end\":36617,\"start\":36616},{\"end\":36619,\"start\":36618},{\"end\":36813,\"start\":36809},{\"end\":36825,\"start\":36821},{\"end\":36985,\"start\":36984},{\"end\":36996,\"start\":36995},{\"end\":37009,\"start\":37008},{\"end\":37020,\"start\":37019},{\"end\":37032,\"start\":37031},{\"end\":37043,\"start\":37042},{\"end\":37393,\"start\":37392},{\"end\":37400,\"start\":37399},{\"end\":37413,\"start\":37412},{\"end\":37424,\"start\":37423},{\"end\":37435,\"start\":37434},{\"end\":37443,\"start\":37442},{\"end\":37455,\"start\":37454},{\"end\":37469,\"start\":37468},{\"end\":37757,\"start\":37756},{\"end\":37768,\"start\":37767},{\"end\":37770,\"start\":37769},{\"end\":37780,\"start\":37779},{\"end\":37782,\"start\":37781},{\"end\":37794,\"start\":37793},{\"end\":37796,\"start\":37795},{\"end\":38039,\"start\":38038},{\"end\":38053,\"start\":38052},{\"end\":38066,\"start\":38065},{\"end\":38068,\"start\":38067},{\"end\":38263,\"start\":38262},{\"end\":38269,\"start\":38268},{\"end\":38279,\"start\":38278},{\"end\":38281,\"start\":38280},{\"end\":38289,\"start\":38288},{\"end\":38511,\"start\":38510},{\"end\":38513,\"start\":38512},{\"end\":38520,\"start\":38519},{\"end\":38522,\"start\":38521},{\"end\":38533,\"start\":38532},{\"end\":38790,\"start\":38789},{\"end\":38797,\"start\":38796},{\"end\":38799,\"start\":38798},{\"end\":39001,\"start\":39000},{\"end\":39009,\"start\":39008},{\"end\":39022,\"start\":39021},{\"end\":39229,\"start\":39228},{\"end\":39237,\"start\":39236},{\"end\":39542,\"start\":39541},{\"end\":39544,\"start\":39543},{\"end\":39554,\"start\":39553},{\"end\":39556,\"start\":39555},{\"end\":39567,\"start\":39566},{\"end\":39834,\"start\":39833},{\"end\":39836,\"start\":39835},{\"end\":39844,\"start\":39843},{\"end\":39851,\"start\":39850},{\"end\":39861,\"start\":39860},{\"end\":39868,\"start\":39867},{\"end\":40143,\"start\":40142},{\"end\":40155,\"start\":40154},{\"end\":40163,\"start\":40162},{\"end\":40173,\"start\":40169},{\"end\":40183,\"start\":40179},{\"end\":40190,\"start\":40189},{\"end\":40200,\"start\":40199},{\"end\":40211,\"start\":40210},{\"end\":40522,\"start\":40521},{\"end\":40529,\"start\":40528},{\"end\":40535,\"start\":40534},{\"end\":40547,\"start\":40546},{\"end\":40779,\"start\":40778},{\"end\":40963,\"start\":40962},{\"end\":40970,\"start\":40969},{\"end\":41145,\"start\":41144},{\"end\":41147,\"start\":41146},{\"end\":41370,\"start\":41369},{\"end\":41378,\"start\":41377},{\"end\":41386,\"start\":41385},{\"end\":41394,\"start\":41393},{\"end\":41401,\"start\":41400},{\"end\":41659,\"start\":41658},{\"end\":41666,\"start\":41665},{\"end\":41853,\"start\":41852},{\"end\":41866,\"start\":41865},{\"end\":41875,\"start\":41874},{\"end\":41884,\"start\":41883},{\"end\":42200,\"start\":42199},{\"end\":42212,\"start\":42211},{\"end\":42441,\"start\":42440},{\"end\":42601,\"start\":42600},{\"end\":42612,\"start\":42611},{\"end\":42619,\"start\":42618},{\"end\":42626,\"start\":42625},{\"end\":42638,\"start\":42637},{\"end\":42646,\"start\":42645},{\"end\":42658,\"start\":42657},{\"end\":42667,\"start\":42666},{\"end\":42680,\"start\":42679},{\"end\":42896,\"start\":42895},{\"end\":42905,\"start\":42904},{\"end\":42907,\"start\":42906},{\"end\":43051,\"start\":43050},{\"end\":43053,\"start\":43052},{\"end\":43065,\"start\":43064},{\"end\":43067,\"start\":43066},{\"end\":43083,\"start\":43082},{\"end\":43093,\"start\":43092},{\"end\":43095,\"start\":43094},{\"end\":43304,\"start\":43303},{\"end\":43314,\"start\":43313},{\"end\":43517,\"start\":43516},{\"end\":43529,\"start\":43528},{\"end\":43708,\"start\":43707},{\"end\":43715,\"start\":43714},{\"end\":43919,\"start\":43918},{\"end\":43927,\"start\":43926},{\"end\":43936,\"start\":43935},{\"end\":43945,\"start\":43944},{\"end\":43955,\"start\":43951},{\"end\":44220,\"start\":44219},{\"end\":44222,\"start\":44221},{\"end\":44233,\"start\":44232}]", "bib_author_last_name": "[{\"end\":31160,\"start\":31152},{\"end\":31169,\"start\":31164},{\"end\":31180,\"start\":31173},{\"end\":31189,\"start\":31184},{\"end\":31419,\"start\":31411},{\"end\":31428,\"start\":31423},{\"end\":31439,\"start\":31432},{\"end\":31448,\"start\":31443},{\"end\":31670,\"start\":31662},{\"end\":31684,\"start\":31674},{\"end\":31696,\"start\":31690},{\"end\":31707,\"start\":31700},{\"end\":31716,\"start\":31711},{\"end\":31965,\"start\":31956},{\"end\":31972,\"start\":31969},{\"end\":31985,\"start\":31976},{\"end\":32305,\"start\":32296},{\"end\":32312,\"start\":32309},{\"end\":32325,\"start\":32316},{\"end\":32599,\"start\":32594},{\"end\":32799,\"start\":32795},{\"end\":32813,\"start\":32803},{\"end\":32825,\"start\":32817},{\"end\":32835,\"start\":32829},{\"end\":32847,\"start\":32841},{\"end\":33153,\"start\":33148},{\"end\":33160,\"start\":33157},{\"end\":33167,\"start\":33164},{\"end\":33175,\"start\":33171},{\"end\":33183,\"start\":33179},{\"end\":33192,\"start\":33190},{\"end\":33198,\"start\":33196},{\"end\":33467,\"start\":33458},{\"end\":33475,\"start\":33471},{\"end\":33674,\"start\":33670},{\"end\":33682,\"start\":33678},{\"end\":33692,\"start\":33686},{\"end\":33701,\"start\":33699},{\"end\":33707,\"start\":33705},{\"end\":33718,\"start\":33711},{\"end\":33955,\"start\":33949},{\"end\":33961,\"start\":33959},{\"end\":33973,\"start\":33965},{\"end\":34184,\"start\":34178},{\"end\":34197,\"start\":34190},{\"end\":34393,\"start\":34389},{\"end\":34403,\"start\":34399},{\"end\":34585,\"start\":34573},{\"end\":34603,\"start\":34591},{\"end\":34797,\"start\":34790},{\"end\":34808,\"start\":34801},{\"end\":34817,\"start\":34812},{\"end\":34827,\"start\":34821},{\"end\":35097,\"start\":35093},{\"end\":35110,\"start\":35103},{\"end\":35374,\"start\":35369},{\"end\":35387,\"start\":35378},{\"end\":35578,\"start\":35570},{\"end\":35753,\"start\":35745},{\"end\":35764,\"start\":35757},{\"end\":35775,\"start\":35768},{\"end\":35784,\"start\":35779},{\"end\":36058,\"start\":36053},{\"end\":36070,\"start\":36062},{\"end\":36079,\"start\":36074},{\"end\":36335,\"start\":36330},{\"end\":36347,\"start\":36339},{\"end\":36359,\"start\":36351},{\"end\":36368,\"start\":36363},{\"end\":36614,\"start\":36607},{\"end\":36627,\"start\":36620},{\"end\":36819,\"start\":36814},{\"end\":36829,\"start\":36826},{\"end\":36993,\"start\":36986},{\"end\":37006,\"start\":36997},{\"end\":37017,\"start\":37010},{\"end\":37029,\"start\":37021},{\"end\":37040,\"start\":37033},{\"end\":37051,\"start\":37044},{\"end\":37397,\"start\":37394},{\"end\":37410,\"start\":37401},{\"end\":37421,\"start\":37414},{\"end\":37432,\"start\":37425},{\"end\":37440,\"start\":37436},{\"end\":37452,\"start\":37444},{\"end\":37466,\"start\":37456},{\"end\":37477,\"start\":37470},{\"end\":37765,\"start\":37758},{\"end\":37777,\"start\":37771},{\"end\":37791,\"start\":37783},{\"end\":37800,\"start\":37797},{\"end\":38050,\"start\":38040},{\"end\":38063,\"start\":38054},{\"end\":38075,\"start\":38069},{\"end\":38266,\"start\":38264},{\"end\":38276,\"start\":38270},{\"end\":38286,\"start\":38282},{\"end\":38296,\"start\":38290},{\"end\":38517,\"start\":38514},{\"end\":38530,\"start\":38523},{\"end\":38540,\"start\":38534},{\"end\":38794,\"start\":38791},{\"end\":38803,\"start\":38800},{\"end\":39006,\"start\":39002},{\"end\":39019,\"start\":39010},{\"end\":39030,\"start\":39023},{\"end\":39234,\"start\":39230},{\"end\":39246,\"start\":39238},{\"end\":39551,\"start\":39545},{\"end\":39564,\"start\":39557},{\"end\":39573,\"start\":39568},{\"end\":39841,\"start\":39837},{\"end\":39848,\"start\":39845},{\"end\":39858,\"start\":39852},{\"end\":39865,\"start\":39862},{\"end\":39874,\"start\":39869},{\"end\":40152,\"start\":40144},{\"end\":40160,\"start\":40156},{\"end\":40167,\"start\":40164},{\"end\":40177,\"start\":40174},{\"end\":40187,\"start\":40184},{\"end\":40197,\"start\":40191},{\"end\":40208,\"start\":40201},{\"end\":40218,\"start\":40212},{\"end\":40526,\"start\":40523},{\"end\":40532,\"start\":40530},{\"end\":40544,\"start\":40536},{\"end\":40551,\"start\":40548},{\"end\":40783,\"start\":40780},{\"end\":40967,\"start\":40964},{\"end\":40984,\"start\":40971},{\"end\":41156,\"start\":41148},{\"end\":41375,\"start\":41371},{\"end\":41383,\"start\":41379},{\"end\":41391,\"start\":41387},{\"end\":41398,\"start\":41395},{\"end\":41407,\"start\":41402},{\"end\":41663,\"start\":41660},{\"end\":41672,\"start\":41667},{\"end\":41863,\"start\":41854},{\"end\":41872,\"start\":41867},{\"end\":41881,\"start\":41876},{\"end\":41891,\"start\":41885},{\"end\":42209,\"start\":42201},{\"end\":42222,\"start\":42213},{\"end\":42447,\"start\":42442},{\"end\":42609,\"start\":42602},{\"end\":42616,\"start\":42613},{\"end\":42623,\"start\":42620},{\"end\":42635,\"start\":42627},{\"end\":42643,\"start\":42639},{\"end\":42655,\"start\":42647},{\"end\":42664,\"start\":42659},{\"end\":42677,\"start\":42668},{\"end\":42691,\"start\":42681},{\"end\":42902,\"start\":42897},{\"end\":42914,\"start\":42908},{\"end\":43062,\"start\":43054},{\"end\":43080,\"start\":43068},{\"end\":43090,\"start\":43084},{\"end\":43105,\"start\":43096},{\"end\":43311,\"start\":43305},{\"end\":43320,\"start\":43315},{\"end\":43526,\"start\":43518},{\"end\":43532,\"start\":43530},{\"end\":43712,\"start\":43709},{\"end\":43718,\"start\":43716},{\"end\":43924,\"start\":43920},{\"end\":43933,\"start\":43928},{\"end\":43942,\"start\":43937},{\"end\":43949,\"start\":43946},{\"end\":43960,\"start\":43956},{\"end\":44230,\"start\":44223},{\"end\":44240,\"start\":44234}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":15291355},\"end\":31352,\"start\":31099},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":206764694},\"end\":31623,\"start\":31354},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":4517687},\"end\":31873,\"start\":31625},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":1718856},\"end\":32164,\"start\":31875},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":14001254},\"end\":32546,\"start\":32166},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":13284142},\"end\":32705,\"start\":32548},{\"attributes\":{\"doi\":\"arXiv:1412.7062\",\"id\":\"b6\"},\"end\":33071,\"start\":32707},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":27989532},\"end\":33393,\"start\":33073},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":691081},\"end\":33613,\"start\":33395},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":57246310},\"end\":33893,\"start\":33615},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":6382669},\"end\":34128,\"start\":33895},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":13874285},\"end\":34340,\"start\":34130},{\"attributes\":{\"id\":\"b12\"},\"end\":34526,\"start\":34342},{\"attributes\":{\"id\":\"b13\"},\"end\":34728,\"start\":34528},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":215513937},\"end\":34985,\"start\":34730},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":11702802},\"end\":35291,\"start\":34987},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":9615581},\"end\":35554,\"start\":35293},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":206770307},\"end\":35659,\"start\":35556},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":215827080},\"end\":35973,\"start\":35661},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":12061055},\"end\":36246,\"start\":35975},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":13259596},\"end\":36557,\"start\":36248},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":1820544},\"end\":36759,\"start\":36559},{\"attributes\":{\"doi\":\"arXiv:1504.01989\",\"id\":\"b22\"},\"end\":36982,\"start\":36761},{\"attributes\":{\"doi\":\"arXiv:1404.1869\",\"id\":\"b23\"},\"end\":37328,\"start\":36984},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":1799558},\"end\":37691,\"start\":37330},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":11835209},\"end\":37971,\"start\":37693},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":195908774},\"end\":38228,\"start\":37973},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":3166882},\"end\":38424,\"start\":38230},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":2792395},\"end\":38726,\"start\":38426},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":31753935},\"end\":38942,\"start\":38728},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":1629541},\"end\":39180,\"start\":38944},{\"attributes\":{\"id\":\"b31\"},\"end\":39446,\"start\":39182},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":8165754},\"end\":39763,\"start\":39448},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":17255053},\"end\":40060,\"start\":39765},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":6529084},\"end\":40439,\"start\":40062},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":10328909},\"end\":40717,\"start\":40441},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":1930439},\"end\":40907,\"start\":40719},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":5653091},\"end\":41120,\"start\":40909},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":7236304},\"end\":41269,\"start\":41122},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":3130125},\"end\":41616,\"start\":41271},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":14848918},\"end\":41790,\"start\":41618},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":545361},\"end\":42129,\"start\":41792},{\"attributes\":{\"doi\":\"arXiv:1409.1556\",\"id\":\"b42\"},\"end\":42400,\"start\":42131},{\"attributes\":{\"id\":\"b43\"},\"end\":42566,\"start\":42402},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":206592484},\"end\":42874,\"start\":42568},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":1065794},\"end\":43007,\"start\":42876},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":216077384},\"end\":43255,\"start\":43009},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":8989489},\"end\":43444,\"start\":43257},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":14828052},\"end\":43669,\"start\":43446},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":6423078},\"end\":43839,\"start\":43671},{\"attributes\":{\"doi\":\"arXiv:1603.04530\",\"id\":\"b50\"},\"end\":44167,\"start\":43841},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":5984060},\"end\":44376,\"start\":44169}]", "bib_title": "[{\"end\":31148,\"start\":31099},{\"end\":31407,\"start\":31354},{\"end\":31658,\"start\":31625},{\"end\":31952,\"start\":31875},{\"end\":32292,\"start\":32166},{\"end\":32590,\"start\":32548},{\"end\":33141,\"start\":33073},{\"end\":33454,\"start\":33395},{\"end\":33666,\"start\":33615},{\"end\":33945,\"start\":33895},{\"end\":34174,\"start\":34130},{\"end\":34567,\"start\":34528},{\"end\":34786,\"start\":34730},{\"end\":35087,\"start\":34987},{\"end\":35365,\"start\":35293},{\"end\":35566,\"start\":35556},{\"end\":35741,\"start\":35661},{\"end\":36049,\"start\":35975},{\"end\":36326,\"start\":36248},{\"end\":36603,\"start\":36559},{\"end\":37390,\"start\":37330},{\"end\":37754,\"start\":37693},{\"end\":38036,\"start\":37973},{\"end\":38260,\"start\":38230},{\"end\":38508,\"start\":38426},{\"end\":38787,\"start\":38728},{\"end\":38998,\"start\":38944},{\"end\":39226,\"start\":39182},{\"end\":39539,\"start\":39448},{\"end\":39831,\"start\":39765},{\"end\":40140,\"start\":40062},{\"end\":40519,\"start\":40441},{\"end\":40776,\"start\":40719},{\"end\":40960,\"start\":40909},{\"end\":41142,\"start\":41122},{\"end\":41367,\"start\":41271},{\"end\":41656,\"start\":41618},{\"end\":41850,\"start\":41792},{\"end\":42598,\"start\":42568},{\"end\":42893,\"start\":42876},{\"end\":43048,\"start\":43009},{\"end\":43301,\"start\":43257},{\"end\":43514,\"start\":43446},{\"end\":43705,\"start\":43671},{\"end\":44217,\"start\":44169}]", "bib_author": "[{\"end\":31162,\"start\":31150},{\"end\":31171,\"start\":31162},{\"end\":31182,\"start\":31171},{\"end\":31191,\"start\":31182},{\"end\":31421,\"start\":31409},{\"end\":31430,\"start\":31421},{\"end\":31441,\"start\":31430},{\"end\":31450,\"start\":31441},{\"end\":31672,\"start\":31660},{\"end\":31686,\"start\":31672},{\"end\":31698,\"start\":31686},{\"end\":31709,\"start\":31698},{\"end\":31718,\"start\":31709},{\"end\":31967,\"start\":31954},{\"end\":31974,\"start\":31967},{\"end\":31987,\"start\":31974},{\"end\":32307,\"start\":32294},{\"end\":32314,\"start\":32307},{\"end\":32327,\"start\":32314},{\"end\":32601,\"start\":32592},{\"end\":32801,\"start\":32790},{\"end\":32815,\"start\":32801},{\"end\":32827,\"start\":32815},{\"end\":32837,\"start\":32827},{\"end\":32849,\"start\":32837},{\"end\":33155,\"start\":33143},{\"end\":33162,\"start\":33155},{\"end\":33169,\"start\":33162},{\"end\":33177,\"start\":33169},{\"end\":33185,\"start\":33177},{\"end\":33194,\"start\":33185},{\"end\":33200,\"start\":33194},{\"end\":33469,\"start\":33456},{\"end\":33477,\"start\":33469},{\"end\":33676,\"start\":33668},{\"end\":33684,\"start\":33676},{\"end\":33694,\"start\":33684},{\"end\":33703,\"start\":33694},{\"end\":33709,\"start\":33703},{\"end\":33720,\"start\":33709},{\"end\":33957,\"start\":33947},{\"end\":33963,\"start\":33957},{\"end\":33975,\"start\":33963},{\"end\":34186,\"start\":34176},{\"end\":34199,\"start\":34186},{\"end\":34395,\"start\":34385},{\"end\":34405,\"start\":34395},{\"end\":34587,\"start\":34569},{\"end\":34605,\"start\":34587},{\"end\":34799,\"start\":34788},{\"end\":34810,\"start\":34799},{\"end\":34819,\"start\":34810},{\"end\":34829,\"start\":34819},{\"end\":35099,\"start\":35089},{\"end\":35112,\"start\":35099},{\"end\":35376,\"start\":35367},{\"end\":35389,\"start\":35376},{\"end\":35580,\"start\":35568},{\"end\":35755,\"start\":35743},{\"end\":35766,\"start\":35755},{\"end\":35777,\"start\":35766},{\"end\":35786,\"start\":35777},{\"end\":36060,\"start\":36051},{\"end\":36072,\"start\":36060},{\"end\":36081,\"start\":36072},{\"end\":36337,\"start\":36328},{\"end\":36349,\"start\":36337},{\"end\":36361,\"start\":36349},{\"end\":36370,\"start\":36361},{\"end\":36616,\"start\":36605},{\"end\":36629,\"start\":36616},{\"end\":36821,\"start\":36809},{\"end\":36831,\"start\":36821},{\"end\":36995,\"start\":36984},{\"end\":37008,\"start\":36995},{\"end\":37019,\"start\":37008},{\"end\":37031,\"start\":37019},{\"end\":37042,\"start\":37031},{\"end\":37053,\"start\":37042},{\"end\":37399,\"start\":37392},{\"end\":37412,\"start\":37399},{\"end\":37423,\"start\":37412},{\"end\":37434,\"start\":37423},{\"end\":37442,\"start\":37434},{\"end\":37454,\"start\":37442},{\"end\":37468,\"start\":37454},{\"end\":37479,\"start\":37468},{\"end\":37767,\"start\":37756},{\"end\":37779,\"start\":37767},{\"end\":37793,\"start\":37779},{\"end\":37802,\"start\":37793},{\"end\":38052,\"start\":38038},{\"end\":38065,\"start\":38052},{\"end\":38077,\"start\":38065},{\"end\":38268,\"start\":38262},{\"end\":38278,\"start\":38268},{\"end\":38288,\"start\":38278},{\"end\":38298,\"start\":38288},{\"end\":38519,\"start\":38510},{\"end\":38532,\"start\":38519},{\"end\":38542,\"start\":38532},{\"end\":38796,\"start\":38789},{\"end\":38805,\"start\":38796},{\"end\":39008,\"start\":39000},{\"end\":39021,\"start\":39008},{\"end\":39032,\"start\":39021},{\"end\":39236,\"start\":39228},{\"end\":39248,\"start\":39236},{\"end\":39553,\"start\":39541},{\"end\":39566,\"start\":39553},{\"end\":39575,\"start\":39566},{\"end\":39843,\"start\":39833},{\"end\":39850,\"start\":39843},{\"end\":39860,\"start\":39850},{\"end\":39867,\"start\":39860},{\"end\":39876,\"start\":39867},{\"end\":40154,\"start\":40142},{\"end\":40162,\"start\":40154},{\"end\":40169,\"start\":40162},{\"end\":40179,\"start\":40169},{\"end\":40189,\"start\":40179},{\"end\":40199,\"start\":40189},{\"end\":40210,\"start\":40199},{\"end\":40220,\"start\":40210},{\"end\":40528,\"start\":40521},{\"end\":40534,\"start\":40528},{\"end\":40546,\"start\":40534},{\"end\":40553,\"start\":40546},{\"end\":40785,\"start\":40778},{\"end\":40969,\"start\":40962},{\"end\":40986,\"start\":40969},{\"end\":41158,\"start\":41144},{\"end\":41377,\"start\":41369},{\"end\":41385,\"start\":41377},{\"end\":41393,\"start\":41385},{\"end\":41400,\"start\":41393},{\"end\":41409,\"start\":41400},{\"end\":41665,\"start\":41658},{\"end\":41674,\"start\":41665},{\"end\":41865,\"start\":41852},{\"end\":41874,\"start\":41865},{\"end\":41883,\"start\":41874},{\"end\":41893,\"start\":41883},{\"end\":42211,\"start\":42199},{\"end\":42224,\"start\":42211},{\"end\":42449,\"start\":42440},{\"end\":42611,\"start\":42600},{\"end\":42618,\"start\":42611},{\"end\":42625,\"start\":42618},{\"end\":42637,\"start\":42625},{\"end\":42645,\"start\":42637},{\"end\":42657,\"start\":42645},{\"end\":42666,\"start\":42657},{\"end\":42679,\"start\":42666},{\"end\":42693,\"start\":42679},{\"end\":42904,\"start\":42895},{\"end\":42916,\"start\":42904},{\"end\":43064,\"start\":43050},{\"end\":43082,\"start\":43064},{\"end\":43092,\"start\":43082},{\"end\":43107,\"start\":43092},{\"end\":43313,\"start\":43303},{\"end\":43322,\"start\":43313},{\"end\":43528,\"start\":43516},{\"end\":43534,\"start\":43528},{\"end\":43714,\"start\":43707},{\"end\":43720,\"start\":43714},{\"end\":43926,\"start\":43918},{\"end\":43935,\"start\":43926},{\"end\":43944,\"start\":43935},{\"end\":43951,\"start\":43944},{\"end\":43962,\"start\":43951},{\"end\":44232,\"start\":44219},{\"end\":44242,\"start\":44232}]", "bib_venue": "[{\"end\":31200,\"start\":31191},{\"end\":31460,\"start\":31450},{\"end\":31727,\"start\":31718},{\"end\":31996,\"start\":31987},{\"end\":32336,\"start\":32327},{\"end\":32611,\"start\":32601},{\"end\":32788,\"start\":32707},{\"end\":33204,\"start\":33200},{\"end\":33487,\"start\":33477},{\"end\":33729,\"start\":33720},{\"end\":33984,\"start\":33975},{\"end\":34209,\"start\":34199},{\"end\":34383,\"start\":34342},{\"end\":34609,\"start\":34605},{\"end\":34839,\"start\":34829},{\"end\":35120,\"start\":35112},{\"end\":35393,\"start\":35389},{\"end\":35589,\"start\":35580},{\"end\":35795,\"start\":35786},{\"end\":36090,\"start\":36081},{\"end\":36374,\"start\":36370},{\"end\":36638,\"start\":36629},{\"end\":36807,\"start\":36761},{\"end\":37128,\"start\":37068},{\"end\":37485,\"start\":37479},{\"end\":37812,\"start\":37802},{\"end\":38081,\"start\":38077},{\"end\":38307,\"start\":38298},{\"end\":38551,\"start\":38542},{\"end\":38814,\"start\":38805},{\"end\":39041,\"start\":39032},{\"end\":39294,\"start\":39248},{\"end\":39585,\"start\":39575},{\"end\":39891,\"start\":39876},{\"end\":40229,\"start\":40220},{\"end\":40557,\"start\":40553},{\"end\":40789,\"start\":40785},{\"end\":40995,\"start\":40986},{\"end\":41177,\"start\":41158},{\"end\":41418,\"start\":41409},{\"end\":41684,\"start\":41674},{\"end\":41931,\"start\":41893},{\"end\":42197,\"start\":42131},{\"end\":42438,\"start\":42402},{\"end\":42702,\"start\":42693},{\"end\":42926,\"start\":42916},{\"end\":43111,\"start\":43107},{\"end\":43332,\"start\":43322},{\"end\":43538,\"start\":43534},{\"end\":43729,\"start\":43720},{\"end\":43916,\"start\":43841},{\"end\":44246,\"start\":44242}]"}}}, "year": 2023, "month": 12, "day": 17}