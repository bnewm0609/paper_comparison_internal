{"id": 52341649, "updated": "2023-09-30 16:12:46.334", "metadata": {"title": "Towards Automated Factchecking: Developing an Annotation Schema and Benchmark for Consistent Automated Claim Detection", "authors": "[{\"first\":\"Lev\",\"last\":\"Konstantinovskiy\",\"middle\":[]},{\"first\":\"Oliver\",\"last\":\"Price\",\"middle\":[]},{\"first\":\"Mevan\",\"last\":\"Babakar\",\"middle\":[]},{\"first\":\"Arkaitz\",\"last\":\"Zubiaga\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2018, "month": 9, "day": 21}, "abstract": "In an effort to assist factcheckers in the process of factchecking, we tackle the claim detection task, one of the necessary stages prior to determining the veracity of a claim. It consists of identifying the set of sentences, out of a long text, deemed capable of being factchecked. This paper is a collaborative work between Full Fact, an independent factchecking charity, and academic partners. Leveraging the expertise of professional factcheckers, we develop an annotation schema and a benchmark for automated claim detection that is more consistent across time, topics and annotators than previous approaches. Our annotation schema has been used to crowdsource the annotation of a dataset with sentences from UK political TV shows. We introduce an approach based on universal sentence representations to perform the classification, achieving an F1 score of 0.83, with over 5% relative improvement over the state-of-the-art methods ClaimBuster and ClaimRank. The system was deployed in production and received positive user feedback.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1809.08193", "mag": "2953243986", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-1809-08193", "doi": null}}, "content": {"source": {"pdf_hash": "73fbda99661ad0ed8d367530dba254cdc3d9094c", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1809.08193v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "012aab35051d78d63b4bc70bffd840ee64c88dfd", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/73fbda99661ad0ed8d367530dba254cdc3d9094c.txt", "contents": "\nTowards Automated Factchecking: Developing an Annotation Schema and Benchmark for Consistent Automated Claim Detection\n\n\nLev Konstantinovskiy \nFull Fact\nLondonUK\n\nOliver Price \nUniversity of Warwick\nCoventryUK\n\nMevan Babakar \nFull Fact\nLondonUK\n\nArkaitz Zubiaga \nUniversity of Warwick\nCoventryUK\n\nTowards Automated Factchecking: Developing an Annotation Schema and Benchmark for Consistent Automated Claim Detection\n\nIn an effort to assist factcheckers in the process of factchecking, we tackle the claim detection task, one of the necessary stages prior to determining the veracity of a claim. It consists of identifying the set of sentences, out of a long text, deemed capable of being factchecked. This paper is a collaborative work between Full Fact, an independent factchecking charity, and academic partners. Leveraging the expertise of professional factcheckers, we develop an annotation schema and a benchmark for automated claim detection that is more consistent across time, topics and annotators than previous approaches. Our annotation schema has been used to crowdsource the annotation of a dataset with sentences from UK political TV shows. We introduce an approach based on universal sentence representations to perform the classification, achieving an F1 score of 0.83, with over 5% relative improvement over the state-of-the-art methods ClaimBuster and ClaimRank. The system was deployed in production and received positive user feedback.\n\nIntroduction\n\nMisinformation has recently become more central in public discourse (Shu, Sliva, Wang, Tang, & Liu, 2017;Zubiaga, Aker, Bontcheva, Liakata, & Procter, 2018;Ciampaglia, 2018). As a consequence, interest has increased in the scientific community to further NLP approaches that can help alleviate the burdensome and time-consuming human activity of factchecking (Vlachos & Riedel, 2014;. Factchecking is known as the task of producing an informed assessment of the veracity of a claim (Graves, Nyhan, & Reifler, 2016;Graves, 2018). When considered inside a factchecking organisation, it is a series of tasks.\n\nThe vast majority of the scientific research has focused on the part visible to the general public -the determination of veracity (Huynh & Papotti, 2018;Thorne, Vlachos, Christodoulopoulos, & Mittal, 2018), often referred to as fake news detection (Wang, 2017;Long, Lu, Xiang, Li, & Huang, 2017) or arguably rumour detection (Yang, Liu, Yu, & Yang, 2012;Ma, Gao, Wei, Lu, & Wong, 2015;Kwon, Cha, & Jung, 2017). In the industrial arXiv:1809.08193v1 [cs.CL] 21 Sep 2018 setting of a factchecking organisation, we believe the demands for automatically establishing veracity are too high to be satisfied by current methods. High levels of generality and precision would be required for it to become a part of the pipeline. However, there is a user need for another component which we believed to be more tractable, and for which there is limited scientific literature. It is the stage before veracity called 'claim detection', i.e. monitoring news sources and identifying if a particular sentence constitutes a claim that could be factchecked.\n\nIn this work, we set out to present the outcome of development of the first stage in the automated factchecking pipeline. It is an automated claim detection system developed by the UK's independent factchecking charity, Full Fact 1 together with academic collaborators. The contributions of our work are as follows:\n\n\u2022 We introduce the first annotation schema for claim detection, iteratively developed by experts at Full Fact, comprising 7 different labels. \u2022 We describe a crowdsourcing methodology that enabled us to collect a dataset with 5,571 sentences labelled according to this schema. \u2022 We develop a novel claim detection system that leverages transfer learning and universal sentence representations, as opposed to previous work that was limited to wordlevel representations. Our experiments show that our claim detection system outperforms the state-of-the-art claim detection systems, ClaimBuster and ClaimRank. \u2022 With the annotation schema, crowdsourcing methodology and task definition, we set forth a benchmark methodology for further development of claim detection systems.\n\n\nThe Factchecking Process\n\nMost factcheckers say that their mission is to give citizens information to make political choices, improve the quality of public political discourse and to hold politicians accountable (Graves et al., 2016). Misinformation and misperceptions can undermine this goal (Fridkin, Kenney, & Wintersieck, 2015;Flynn, Nyhan, & Reifler, 2017). There is a very small number of factchecking organisations in the world, about 150 (Stencel, 2017), compared to the volume of media items produced daily. The speed at which information now flows means there is less time to verify the claims made and myths spread more easily. The task of factchecking has never been bigger or more challenging. Hence, automating any parts of the factchecking process would save factcheckers time to work on the more difficult tasks needing human judgement.\n\nThe factchecking process has been formally defined by Full Fact as consisting of the following four stages (Babakar & Moy, 2016):\n\nFirst, the media is recorded and the content extracted, e.g. HTML markup is stripped off online articles or speech is transcribed to text. Second, the sentences containing factual claims are highlighted. This generally removes at least 70% (Hassan, Zhang, et al., 2017;Gencheva, Nakov, M\u00e0rquez, Barr\u00f3n-Cede\u00f1o, & Koychev, 2017) of sentences that do not require further processing. Third, the claims are checked manually against datasets and/or by calling up the claimant. And finally, the results are published to the public if they are considered a meaningful contribution to the debate. A meaningful contribution is not necessarily one that ends in a 'True' or 'False' classification. A factcheck may also clarify a controversy, unravel the many interpretations of data, or supply context as a means of getting to a more nuanced answer. There are activities that take part after publication too, such as lobbying for improvements in data or getting publications to correct the record.\n\nThe timeframe of this process varies from minutes during live factchecking to possibly weeks when writing a complex article. At each step in the process there is editorial selection due to finite human resources, impact goals and other considerations. It involves deciding which media to monitor, which claims to check and which findings to publish. These are difficult tasks as they require knowledge of the current political and media landscape; thus, are out of scope of automation in the near future. However monitoring media, detecting claims, clustering claims, automatically checking simple claims or surfacing relevant datasets, all aid the factchecker, and all involve a level of automation. Thus automation could help increase the volume of factchecker output, or help select claims that are more valuable. But there is only a limited amount that automation can do to speed up factchecking. Complex checking remains the bottleneck and usually requires careful human judgement and expertise.\n\nClaim detection, the stage we tackle in this paper, is a necessary pre-requisite for future work on claim clustering. It could maximise the impact of a small number of factcheckers, as they will be able to see which claims are the most popular across a large scale media landscape and therefore could be the best targets to intervene on.\n\n\nRelated Work\n\nMost work around automatically factchecking claims has focused on the later stages of determining the veracity of claims, usually by building knowledge graphs out of knowledge bases, such as Wikidata (Wu, Agarwal, Li, Yang, & Yu, 2014;Ciampaglia et al., 2015;Shiralkar, 2017;Shi & Weninger, 2016;Cao, Li, Luo, & Yao, 2018). Less work has been reported on the preceding stage of claim detection, which is still in its infancy.\n\nOne of the best-known approaches to claim detection is ClaimBuster (Hassan, Zhang, et al., 2017). They collected a large annotated corpus of televised debates in the USA. Their model combines TF-IDF, POS tags and NER features on an SVM classifier and produces a score of how important a claim is to factcheck. This has the caveat of then having to choose a cut-off score to determine the claims that will be considered for factchecking. Our approach is instead to define an annotation schema that is binary, and built on several types of claims. It better fits the use case in this factchecking pipeline -in a live stream of subtitles we are unable to know in advance which sentence will make it to the top ranking until the end of the entire programme.\n\nIn (Hanto & Tostrup, 2018) annotations were collected using ClaimBuster-inspired annotation guidance from volunteers together with their age, gender and education. With possible labels being verifiable check-worthy (VCW), verifiable not check-worthy (VNCW), and not verifiable (NV), they obtained 2,100 labelled sentences (reduced to 264 high-quality labelled sentences). They find that annotators with a natural sciences background agreed internally about what constitutes a check-worthy claim, whereas those with humanities, medicine, and ontology backgrounds saw more internal disagreement on check-worthiness. Furthermore, using 35 labelled control claims to test annotator skill, they find that the age group 40-49 obtains a higher average score, and label more claims, than that of 30-39, which correspondingly scores higher than age group 20-29.\n\nAnother recent approach to claim detection is ClaimRank (Gencheva et al., 2017). They compiled a dataset by taking the outputs of factchecking of a political debate, published by 9 organisations simultaneously. Models were created to predict if the claim would be highlighted by at least one or by a specific organisation. The modelling is done with a large variety of features from both the individual sentence and the wider context around it. A subsequent version of the dataset (Jaradat, Gencheva, Barr\u00f3n-Cede\u00f1o, M\u00e0rquez, & Nakov, 2018;Nakov et al., 2018) includes a larger set of sentences in two languages, English and Arabic. These datasets are similar to ours in order of magnitude, however use a different definition of claim, as we further elaborate in the next subsection.\n\nA slightly different approach to claim detection is that of context-dependent claim detection (CDCD) (Levy, Bilu, Hershcovich, Aharoni, & Slonim, 2014). This study proposes identifying claims given a specific context. Articles relevant to a topic are used to detect claims on that topic.\n\nAnother piece of work worth mentioning is the development of the FEVER (Fact Extraction and VERification) dataset . Whilst this is primarily aimed at work regarding claim veracity, the mere presence of a vast quantity of claims in the dataset allow it to be extended for claim detection in the future.\n\n\nPrevious Attempts at Defining Claims\n\nThere is a body of work on claim detection that has not formalised the definition of a claim e. g. (Gencheva et al., 2017), (Jaradat et al., 2018). Instead it directly relies on what has been identified by external organisations. The lack of a formal definition prevents others from replicating or extending their work. These studies used claims identified by 9 organisations in a political speech as a proxy. The annotations were sourced from publicly available online articles. This is different from our approach where we crowdsourced annotations following our definition of the task. The authors in (Gencheva et al., 2017) acknowledge this limitation, which led to high number of false positives in their experiments. For example, an article consisting of a debate transcript with editorial comments will not highlight repeated instances of claims. This creates inconsistent annotation -during a TV debate, popular claims are discussed on repeated occasions. This had to be re-annotated by researchers in (Nakov et al., 2018). Another caveat is that only 3 of the 9 annotator organisations contributing to those online articles sign up to be neutral and transparent in their selection of claims as verified signatories of the International Factchecking Network's Code of Principles. 2 ClaimBuster (Hassan, Zhang, et al., 2017) provides a definition of a claim which revolves around the question: \"Will the general public be interested in knowing whether this sentence is true or false?\". Claims are considered to be those sentences for which the answer to this question is yes. Their aim was for anyone to be able to feed in a source, e.g. a political speech, and for the system to produce a list of claims ranked by importance, which could directly feed into the editorial process. This definition of a claim includes the judgement of 'importance' which we avoid in our work. We believe it is an editorial judgement best left to factcheckers. ClaimBuster annotators were journalists, students and professors. Annotations that agreed with the authors of that study were selected to ensure good agreement and shared understanding of the assumptions. Researchers from the ClaimBuster team also defined an annotation schema called PolitiTax, 3 a taxonomy of political claims which we considered. However the categories were not useful for the downstream task of checking the veracity of the claim by routing it to the right dataset or team at Full Fact, in part due to the level of granularity in the taxonomy and in part because the teams are split by topic.\n\nThere was also a taxonomy defined by factcheckers during the HeroX factchecking challenge (Francis & Babakar, 2016), which is less granular than PolitiTax. It has four claim types -numerical, political stance, quotes, objects. During this work we discovered that the latter three categories are rare and intersect with others, so we did not use them in our schema.\n\n\nDataset Our Claim Definition and Process\n\nWriting the annotation guidance was not straightforward. During 2015 UK election Full Fact defined a claim as \"an assertion about the world that can be checked\". Media monitoring volunteers were encouraged to ask a factchecker if they had doubts on whether something was assessable. We tried to codify some of this thinking in conversations with the factcheckers.\n\nAfter discussions with factcheckers, we chose to decouple the importance of the claim from the claim itself. We felt that importance was heavily subjective, reliant on context and best left to factcheckers. Importance is a subtle, and forever changing feature. Even though the most \"important\" issues in the view of the UK public are often about the economy, immigration and health, their relative positions change 4 . In some cases new issues become important, e.g. in the UK, importance of claims about the EU increased significantly after the 2016 EU referendum. 5 Different claims are important to different people. There is empirical confirmation of this link when claim annotations are contrasted with volunteer's educational background in the crowdsourced annotations in (Hanto & Tostrup, 2018). For example, \"Norway has a long coast, and it will take at least three days to sail it from one end to the other\" was one of the sentences that volunteers agreed to be a claim but disagreed on check-worthiness. Those with \"natural science\" education thought it was not check-worthy while those with \"humanities\" and \"other\" thought it was.\n\nWe also chose to decouple the topic from the definition. By making our definition descriptive of the claim and not, by proxy, the topic, we would have a more consistent final dataset. In some cases selection of topics is an inherently political choice, e.g. it varies across the population whether \"drugs\" relate to the topic of \"crime\" or \"health\". This kind of classification was avoided.\n\nOur goal was to come up with a claim detection system that is more consistent than ClaimBuster and ClaimRank over time, across topics and across different annotators.\n\nWith factcheckers, we identified what was definitely not a claim. We iterated on potential rules and found examples that broke them. We identified some constraints, like a claim has to be checkable. 6 We were most concretely able to exclude claims based on an individual's personal experience, as more often than not they were un-checkable. This is similar to 'verifiable experiential' statements (Park & Cardie, 2014).\n\nWe went through several versions of the guidance with different taxonomies. We trialled them within Full Fact, and then two versions with external volunteers. The first version applied the 2015 thinking and was a binary accept/reject classification task, accompanied by a guidance. It listed several types of qualities of claims and non-claims. Claims, for example, may be explicit, implicit, or trivial. Non-claims in this version were formed of personal experience and opinion. We decided against these categories in the end as they sometimes involve explicit judgements from our annotators -these choices can sometimes be highly political. For example, in the case of \"The EU is made up of 27 [instead of 28] countries\" or \"The NHS is there for everyone\" some annotators could classify them as trivial while others might consider them explicit legal claims. The implicit/explicit categories were also removed, because whether the claim is implicit or explicit is not important for the next downstream task in the factchecking process after claim detection -factchecking.\n\nFor the second version, we looked at Full Fact's factchecks. They mostly covered statistical claims. We also identified claims around current laws or rules of operation and correlation/causation claims e.g. \"there's no clear correlation between prisons' performance ratings and whether they're publicly-run or contracted out to the private sector.\". This became the basis of our claim categories. We believed if we joined these categories, and removed personal experience, we would have a good proxy for claims. There were many other types of claims that we identified, such as definitions, voting records, and expressions of support. We limited our categories to 7 to make the task realistic for annotators. We also wanted to minimise the overlap between categories to make the task single-choice.\n\n\nAnnotation Guidance\n\nOur annotation schema is the first to be created with a factchecking organisation, building on years of experience manually detecting claims.\n\nIt comprises the following 7 categories, only one of which can be assigned to each sentence:\n\n1. Personal experience. Claims that aren't capable of being checked using publiclyavailable information, e.g. \"I can't save for a deposit.\"\n\n2. Quantity in the past or present. Current value of something e.g. \"1 in 4 wait longer than 6 weeks to be seen by a doctor.\" Changing quantity, e.g. \"The Coalition Government has created 1,000 jobs for every day it's been in office.\" Comparison, e.g. \"Free schools are outperforming state schools.\". Ranking, e.g. \"The UK's the largest importer from the Eurozone.\" 3. Correlation or causation, Correlation e.g. \"GCSEs are a better predictor than AS if a student will get a good degree.\" Causation, e.g. \"Tetanus vaccine causes infertility.\" Absence of a link, e.g. \"Grammar schools don't aid social mobility.\" 4. Current laws or rules of operation, e.g. \"The UK allows a single adult to care for fewer children than other European countries.\" Procedures of public institutions, e.g. \"Local decisions about commissioning services are now taken by organisations that are led by clinicians.\" Rules and changes, e.g. \"EU residents cannot claim Jobseeker's Allowance if they have been in the country for 6 months and have not been able to find work.\" 5. Prediction, Hypothetical claims about the future e.g. \"Indeed, the IFS says that school funding will have fallen by 5% in real terms by 2019 as a result of government policies.\" 6. Other type of claim, Voting records e.g \"You voted to leave, didn't you?\" Public Opinion e.g \"Public satisfaction with the NHS in Wales is lower than it is in England.\" Support e.g. \"The party promised free childcare\" Definitions, e.g. \"Illegal killing of people is what's known as murder.\" Any other sentence that you think is a claim. 7. Not a claim, These are sentences that don't fall into any categories and aren't claims.\n\ne.g. \"What do you think?.\", \"Questions to the Prime Minister!\"\n\nThese categories have proven to broadly cover sentences from political TV shows that Full Fact has encountered over several years. Categories have different levels of occurrence (see Table 1). As previously found (Jaradat et al., 2018;Hanto & Tostrup, 2018;Hassan, Zhang, et al., 2017), \"Not a claim\" is the most popular category, amounting to about 55% of the annotations.\n\n\"Other\" is the second largest category with 952 instances, 23% of the whole. It can be broken down into claims that are less well-defined, with formal sub-categories being: 'Definitions', 'Voting records', 'Public opinion', 'Trivial claim', 'Support', 'Quote', 'Other other'. We amalgamate these because they are likely to overlap and we wanted our annotators to only select one option. For example, \"She said she voted to keep free school meals.\" is both a quote and a voting record. Furthermore, high granularity of categories allows one to perpetually think of rarer categories and a high number of categories slows down annotation unnecessarily. To verify if our level of granularity was correct, we split a sample of 160 sentences in 'Other' into sub-categories. The vast majority are in the 'Other other' category (see Table 1), supporting our chosen level of granularity.\n\n\nCrowdsourced Annotation\n\nThe annotations were done by 80 volunteers recruited through Full Fact's newsletter -this meant that volunteers were keen on factchecking. 28,100 annotations were collected for a set of 6,304 sentences extracted from subtitles of four UK political TV shows, 14 The software used for collecting annotations was Prodigy 89 , a self-hosted annotation platform. It was customised to support multiple annotators -a login and password screen routing via nginx to a specific pre-started instance of Prodigy running in a Docker container. Sentences were shown in random order. The preceding two sentences were also shown on the screen to provide context and assist with potential co-references. Once a sentence was annotated 5 times by different annotators, it was not shown again.\n\nAnnotators were encouraged to contact us for any clarifications needed, with thoughtful questions such as: \"Where it appears that a claim is dressed up as rhetorical question, should we classify it as a claim? e.g. 'Why should unelected officials in Brussels make rules to stop bananas being sold in bunches of more than 2 or 3?\"' To answer this, questions are classified as the claims that they implicitly contain.\n\n\nFigure 1 . Annotation UI in Prodigy\n\n\nAgreement\n\nAt the level of all the 7 granular categories the inter-annotator agreement is moderate, with a Krippendorff's alpha (Krippendorff, 1980) of 0.46. However, we attain higher values of alpha of 0.70 and 0.53 if we convert the annotations into a binary claim/non-claim annotation task, following the two methods shown in Table 3.\n\nMost of the disagreement was between \"Not a claim\" and \"Other claim\". This showed that it is hard to define the boundary and explicitly list all kinds of claims as we saw in the 7 \"News consumption in the UK:2016\" by Ofcom https://perma.cc/5FDK-BRHD 8 https://prodi.gy/ 9 https://fullfact.org/blog/2018/feb/how-we-customised-prodigy-ai/  Table 2 Annotation disagreements. The most prominent disagreement is between \"Other claim\" and \"Not a claim\". The labels are shortened versions of those in Figure 1 due to space limitations. (Qu: quantity, Corr: correlation and causation, Pred: predictions, Pers: personal experience.) \n\n\nClaim\n\nNon-claim Omitted \u03b1 N 2 3, 4, 6, 7 1, 5 0.70 6,095 2, 3, 4, 5 1, 6, 7 -0.53 4,777 Table 3 From 7 categories to binary claim vs \"not a claim\" classification. N = number of sentences annotated by majority.\n\n\n(1) Personal experience, (2) Quantity in the past or present, (3) Correlation or causation, (4) Current laws or rules of operation, (5) Prediction, (6) Other type of claim, (7) Not a claim.\n\nprocess of creating the annotation guidance. The disagreements across all sentence types can be seen in Table 2.\n\nTo solve the disagreements we reframe the task as binary classification by grouping some categories together. Another reason for the binary instead of multi-class classification is that the end user application is binary, i.e. a sentence either makes it to the excerpted list of claims or not. The binary classification system still captures the expert factcheckers input by using the granular positive claim categories during annotation. Most importantly it achieves dataset robustness by reducing the annotator disagreement. See the two possible binary re-formulations in Table 3. The are no negative sides to this binary re-formulation for this use case. The claims are routed to the relevant factchecking team or a dataset based on their topic (e.g. healthcare or immigration) and not claim type.\n\nThe most agreement and easiest to model view of the data is in the first row of Table  3. 'Quantity' was selected as the positive class because it is the majority category of claims (received most annotations). It's also the category that Full Fact most frequently writes about. We established this after annotating 800 of their claims. The categories of 'Personal experience' and 'Prediction' were excluded for ease of modelling because they might contain quantities outside of the 'Quantity' claim class.\n\nOnce we found models that perform with 92% precision and 88% recall on identifying quantitative claims, we moved to evaluate them on more realistic data in the second row of Table 3. Here 'Other type of claim' is not in the positive class for two reasons. First of all there is a lot of disagreement between it and the 'Not a claim' class. Secondly, the kinds of claim in the 'Other' section -voting records, quotes, statements about public opinion polls, are less frequently written about by Full Fact. We are aware that this choice encodes the peculiarity of one organisation at one point in time into our model and needs to be re-considered if, say, an organisation wishing to use the model specialises in voting records.\n\nThe agreement of 60% is still low -that is a lot of sentences to throw away if we were only to consider agreement among all the annotators. So instead we choose a majority vote where at least 3 annotators marked the sentence and more than half of them agree. Out of the initial 6,304 sentences this filter selects 4,777 sentences, 3,973 not claims and 804 claims. This is in line with previous studies where the proportion of claims is 10-30% in political TV (Hassan, Zhang, et al., 2017;Gencheva et al., 2017). As extra training data we add 794 claims from the Full Fact database. Out of them 766 are annotated by us as positive because they fall into our claim categories, for example \"The courts have said that the so-called 'bedroom tax' is illegal.\" The remaining 28 are in the 'Other type of claim' category, for example \"The British economy is not only getting better, it is healing.\"\n\nIn the same way of choosing the majority votes we can produce a dataset of 4080 sentences annotated across the 7 classes with majority voting, see Table 1.\n\n\nMethods\n\nTo capture the diversity of sentences observed during political TV shows, we propose to leverage universal sentence representations. We use InferSent (Conneau, Kiela, Schwenk, Barrault, & Bordes, 2017) as a method to achieve sentence embeddings. These embeddings are different from averaging word embeddings because they take word order into account using a recurrent neural network. The method provided by InferSent involves words being converted to their common crawl GloVe implementations before being passed through a bidirectional long-short-term memory (BiLSTM) network (Hochreiter & Schmidhuber, 1997). The sentence embeddings were pre-trained on a large dataset of Natural Language Inference tasks 10 . Additionally, we also tried concatenating POS and NER information to the embeddings. For each sentence, the POS/NER feature vector was the count of each POS/NER tag in the corpus. We input our sentence representations to a range of supervised classifiers implemented using scikit-learn (Pedregosa et al., 2011), with the classifiers set to their default parameters. The four classifiers we tested include Logistic Regression, Linear SVM, Gaussian Na\u00efve Bayes and Random Forests.\n\nWe use a number of other features as baselines:\n\n1. A number of variants of the state-of-the-art claim detection system by ClaimBuster, using different combinations of TF-IDF, POS and NER features, as in (Hassan, Zhang, et al., 2017).\n\n2. Averaging pre-trained word embedding vectors for all words in a sentence. We evaluate:\n\n\u2022 Word2vec (Mikolov, Chen, Corrado, & Dean, 2013) via the Gensim implementation (\u0158eh\u016f\u0159ek & Sojka, 2010), using the GoogleNews embedding. \u2022 GloVe (Pennington, Socher, & Manning, 2014) trained on Common Crawl, as well as combining them with dimensionality reduction using principal component analysis (PCA).\n\n3. TF-IDF representations of sentences with logistic regression. Numbers have a significant role in claims -the \"Cardinal Number\" part-of-speech tag is the second most discriminating feature in (Hassan, Arslan, Li, & Tremayne, 2017)), so we try a Spacy NER to replace numbers with '*NUMBER*' during preprocessing.\n\nFor our implementation of the ClaimBuster system, we use the Watson Natural Language Understanding API, the updated version of the Alchemy API used by the original authors. All other features were implemented as outlined in (Hassan, Zhang, et al., 2017).\n\nThe ClaimRank (Gencheva et al., 2017) model was harder to re-implement. In order to maintain impartiality Full Fact can't use the data on sentence speakers when selecting which claims to check. This special care is also encouraged in the computer science research for systems that are integrated into the infrastructure of society by the ACM code of ethics (ACM Code of Ethics, 2018). Additionally, our dataset didn't have data on applause, laughing, or speaker crossover. Even though we unfortunately couldn't use one third of ClaimRank's features 11 , we trained the FNN, SVM and logistic regression classifiers on the remaining features in our dataset (Gencheva et al., 2017).\n\n\nExperiment Settings\n\nThe dataset consists of 5,571 sentences (4,777 from annotations and 794 from the Full Fact database of claims), of which 1,570 are claims and 4,001 are not claims, which gives a 30/70 class imbalance. We use stratified 5-fold cross-validation to train and test our models. We use precision, recall and F 1 -score measures to assess classifier performance.\n\nWe show the best-performing classifier for any given feature set. We also show 95% confidence interval for the precision and recall using binomial distributions. This demonstrates possible overlap in results between different models. The interval is wide for recall due to the small number of positive examples. The next section will present the results of applying these methods.\n\n\nClaim Detection\n\nHere we present results for the binary classification, using the class grouping shown in the second row of Table 3. Table 4 shows the results of our experiments. Interestingly, the simple approach of TF-IDF achieves high precision but low recall. We call our new model 'CNC' which stands for \"Claim/No Claim\". It achieves a better balance of precision and recall; logistic regression classifier gives the highest overall F1 score of 0.83, outperforming all other techniques. The use of POS and NER features in our model has no effect on the performance. GloVe embeddings achieve performance close to our method with F1 scores 2% lower and substantially lower recall scores. Despite the overlap in precision scores between GloVe and our method, the overlap is minimal in terms of recall. Our CNC model also clearly outperforms the state-of-the-art method by ClaimBuster at 0.79 F1; hence our method yields F1 scores that improve ClaimBuster by over 5% in relative terms. ClaimBuster performs similarly to 11 https://github.com/pgencheva/claim-rank  (Kohavi et al., 1995).\n\n\nAnalysis of Results\n\nCNC in terms of precision, albeit with substantially lower recall scores. ClaimRank has the best precision scores across the board, but with the lower recall scores. CNC achieves a 6% relative improvement in F1-score over ClaimRank.\n\n\nMulti-class Classification\n\nFor the multi-class classification into 7 different labels, we use the subset with 4,080 sentences where there was enough agreement at this level of granularity. We train logistic regression on the features from CNC, the best performing binary classifier. Table 5 shows the results for the multi-class classification experiments. These results reaffirm our expectations that, beyond the binary classification of claims and non-claims, classification at a finer granularity becomes more challenging. This is especially true for the categories with the smallest number of instances, such as \"Current laws\" or \"Correlation or causation\". We achieve low F1 score for these categories, however the small number of instances may have a significant impact on this. Unfortunately we couldn't source a lot of claims of these categories from the Full Fact's database of claims. This is an example of how the definition of a 'usual claim' is specific to an organisation -Full Fact hasn't had a fulltime legal factchecker since 2016 but has plans to re-instate one. To achieve more accurate classification and less organisational specificity at this level of granularity, we would need to annotate more sentences to expand the dataset. Looking at bigger classes, \"Quantity\" (relatively easy to identify by looking for numbers and quantitative words) and \"Not a claim\" (the most popular category) yield the best F 1 -scores. The results for \"Other type of claim\" are also good, which only tend to be confused with \"Not a claim\" -this is in line with human annotator disagreement in Figure 1. Overall results are reasonably good when we measure with a microaveraged F1 score of 0.70, however it shows significant room for improvement when we measure it by macroaveraged F1 score of 0.48. We aim to expand our dataset in the near future to circumvent these issues.\n\n\nAnalysis of Results\n\n\nDeployment and Impact\n\nModel deployment has impacted Full Fact and the crowdsourcing exercise has educated the volunteer community.\n\nFull Fact's factcheckers are provided with a UI that shows a live feed of transcripts from television in a tool called \"Live\" which aids live factchecking. When the claim detection model identifies a claim in these sentences, it highlights it in bold. In addition to this, the factcheckers have the ability to manually highlight (in yellow) any claim they believe to be of interest. The integration of claim detection model has provided several benefits. It has saved time -factcheckers can quickly skim through the text looking for claims, instead of reading the entire text. (Though, to ensure 100% recall there are still designated people who watch the entire programme or read the entire transcript.) As a consequence, some factcheckers have started skimming just the claims in the transcripts in areas that are outside of their immediate domain. Going through the entire transcript was not viable for them prior to this automation.\n\nWe analysed 4 live factchecking sessions and noted that all claims manually highlighted in yellow had also been detected by the model, demonstrating the high recall. The precision however is harder to measure during user deployment. The model does detect claims that aren't highlighted in yellow, but this is to be expected. The reason for this is that factcheckers rely on their domain expertise and awareness of current affairs to decide which claims are of specific interest to them -as opposed to highlighting all claims that they notice. These considerations are impossible to encode in a model as discussed in our claim definition section.\n\nA factchecker gave the following feedback on the system: \"Claim detection is very useful after I have finished live fact-checking a show and reviewing it to decide what to write a longer piece about. I no longer have to read the whole transcript, just the highlighted bits.\"\n\nOutside of the Full Fact factcheckers, there has also been anecdotal evidence of impact on the volunteer community. The annotation exercise has been educational for them. They became more scrupulous media consumers. This was also a notable side product in other crowdsourced factchecking initiatives such as TruthSquad 12 .\n\n. Figure 2 . Claims highlighted in bold in the \"Live\" factchecking tool during Prime Minister's Questions on 12 Sept 2018. The transcript errors are from the closed captions broadcast.\n\n\nFuture Work\n\nOur plans for future work include expanding our dataset to other languages and collaborating with other factchecking organisations. We also wish to collect more data using the same annotation schema from other news sources, such as social media, digital and print outlets. Another small potential improvement is using the counts of first/second person pronouns to detect \"Personal Experience\" category as proven useful in (Park & Cardie, 2014).\n\n\nConclusion\n\nThrough leveraging the professional factcheckers at Full Fact, and through academiaindustry collaboration, we have developed the first annotation schema for claim detection informed by experts. This has enabled us to create an annotated dataset made of sentences extracted from transcripts of political TV shows. We have introduced and tested a classifier that leverages universal sentence representations which, with an F1 score of 0.83, outperforms a range of baseline classifiers, including a well-known method by ClaimBuster, by over 5% in relative terms. While we achieve competitive results for binary claim classification, there is room for improvement when we need finer granularity of classification into the 7 categories in the annotation schema.\n\n\nepisodes in total. TV subtitles were chosen because 69% of UK population get their news from TV. 7Category \nSubcategory \nCounts Example \nNot a claim \n54.8% \"Give it all to them, I really don't mind.\" \n\nOther \n\nOther other \n10.4%* \n\"Molly gives so much of who she is away \nthroughout the film.\" \n\nSupport/policy \n5.5%* \"He has advocated for a junk food tax.\" \n\nQuote \n4.7%* \n\"The Brexit secretary said he would guarantee \nfree movement of bankers.\" \n\nTrivial claim \n1.6%* \n\"It was a close call.\" \n\nVoting record \n0.7%* \"She just lost a parliamentary vote.\" \n\nPublic opinion \n0.4%* \n\"A poll showed that most people who voted \nBrexit were concerned with immigration.\" \n\nDefinition \n0.0%* \n\"The unemployed are only those actively \nlooking for work.\" \n\nQuantity \n\nCurrent value \n9.9% \n\"1 in 4 people wait longer than 6 weeks \nto see a doctor.\" \n\nChanging quantity \n\nComparison \n\nRanking \n\nPrediction \n\nHypothetical \nstatements \n4.4% \n\"The IFS says that school funding will \nhave fallen by 5% by 2019.\" \n\nClaims about \nthe future \n\nPersonal experience \nUncheckable \n3.0% \n\"I can't save for a deposit\" \n\nCorrelation/causation \n\nCorrelation \n2.6% \n\"Tetanus vaccine causes infertility\" \n\nCausation \n\nAbsence of \na link \n\nLaws/rules of operation \n\nPublic institution-\nal procedures \n1.9% \n\n\"The UK allows a single adult to care \nfor fewer children than other European \ncountries.\" \n\nRules/rule \nchanges \n\nTable 1 \nA breakdown of the 4,080 sentences where majority agreement achieved. The \"Other\" cate-\ngory is 23%. *The proportions for 'Other' sub-categories are taken from a random sample \nof 160 claims labelled as 'Other'. \n\n\n\nResults for the Claim/No claim experiments. CB: ClaimBuster. The 95% confidence intervals are from binomial distribution adaptingFeatures \nClassifier \nP \nR \nF 1 P-interval R-interval \nTF-IDF \nLogReg \n.90 .59 .70 \n.89 -.91 \n.56 -.61 \nTF-IDF+spacy number preproc. LogReg \n.91 .59 .70 \n.90 -.92 \n.56 -.61 \nWord2Vec \nSVM \n.85 .75 .78 \n.84 -.86 \n.73 -.77 \nGloVe \nLogReg \n.89 .76 .81 \n.88 -.90 \n.74 -.78 \nGloVe+PCA \nLogReg \n.89 .75 .81 \n.88 -.90 \n.73 -.77 \nCB: TF-IDF \nLogReg \n.90 .59 .70 \n.89 -.91 \n.56 -.61 \nCB: TF-IDF+POS \nLogReg \n.88 .68 .76 \n.86 -.89 \n.66 -.71 \nCB: TF-IDF+NER \nLogReg \n.88 .60 .71 \n.87 -.89 \n.58 -.63 \nCB: TF-IDF+POS+NER \nLogReg \n.87 .71 .78 \n.86 -.88 \n.68 -.73 \nCB: TF-IDF \nSVM \n.84 .70 .76 \n.83 -.85 \n.69 -.73 \nCB: TF-IDF+POS \nSVM \n.86 .74 .79 \n.85 -.87 \n.72 -.76 \nCB: TF-IDF+NER \nSVM \n.84 .71 .77 \n.83 -.85 \n.69 -.73 \nCB: TF-IDF+POS+NER \nSVM \n.86 .75 .79 \n.85 -.87 \n.73 -.77 \nClaimRank \nLogReg \n.93 .65 .77 \n.92 -.94 \n.63 -.67 \nClaimRank \nSVM \n.93 .53 .67 \n.92 -.94 \n.51 -.55 \nClaimRank \nFNN \n.89 .61 .72 \n.87 -.91 \n.58 -.62 \nCNC \nLogReg \n.88 .80 .83 \n.87 -.89 \n.78 -.82 \nCNC+POS \nLogReg \n.88 .80 .83 \n.87 -.89 \n.78 -.82 \nCNC+NER \nLogReg \n.88 .80 .83 \n.87 -.89 \n.78 -.82 \nCNC+POS+NER \nLogReg \n.88 .80 .83 \n.87 -.89 \n.78 -.82 \nTable 4 \n\n\n\nMulti-class classifier performance. CNC model.Class \nP \nR F1 \nN \nNot a claim \n.77 .90 .83 2235 \nOther type of claim \n.59 .55 .57 952 \nQuantity (past/present) .80 .79 .79 403 \nPrediction \n.60 .27 .37 181 \nPersonal experience \n.72 .39 .50 124 \nCorrelation/causation \n.50 .13 .21 107 \nCurrent laws/rules \n.27 .04 .07 \n78 \nmicroavg / total \n.71 .73 .70 4080 \nmacroavg / total \n.61 .44 .48 4080 \nTable 5 \n\nhttps://perma.cc/BM43-SJ4N\n\"PolitiTax A Taxonomy of Political Claims\" by IDIR Lab https://perma.cc/4RQF-FCPV 4 \"Ipsos MORI Issues Index: 2017 in review\" https://perma.cc/9SMV-CQR8 5 \"July 2016 Economist/Ipsos MORI Issues Index\" https://perma.cc/DPA5-4XV5\nAlthough, it is difficult to preempt what data is publicly available -as an example, statistics on the relative happiness of cities actually do exist, a topic whose claim might seem initially not checkable. https:// fullfact.org/health/heartache-hertsmere-unhappiest-place-uk/\nhttps://nlp.stanford.edu/projects/snli/\n\"Crowdsourced Fact-Checking? What We Learned from Truthsquad\" 2010 https://perma.cc/J8AS-YU8E\nAcknowledgmentsWe would like to thank all 80 volunteers who participated in the annotation task, in particular Andreas Sampson Geroski. Also, thanks to Joseph O'Leary, Amy Sippitt, Will Moy, Ed Ingold, Vigdis Hanto, Mats Tostrup, Heri Ramampiaro, Jari Bakken, Benj Petitt, Michal Lopuszynski, Ines Montani, Gael Varoquaux. This work has been partially funded by Open Society Foundation (OR2017-35395) and Omidyar Network.\nThe state of automated factchecking. M Moy, W , 2018-01-09Tech. Rep.). Full Fact. Acm code of ethics.Acm code of ethics. (2018). Retrieved 2018-01-09, from https://www.acm.org/code-of-ethics Babakar, M., & Moy, W. (2016). The state of automated factchecking (Tech. Rep.). Full Fact.\n\nTowards automatic numerical cross-checking: Extracting formulas from text. Y Cao, H Li, P Luo, J Yao, Proceedings of the 2018 world wide web conference on world wide web. the 2018 world wide web conference on world wide webCao, Y., Li, H., Luo, P., & Yao, J. (2018). Towards automatic numerical cross-checking: Extracting formulas from text. In Proceedings of the 2018 world wide web conference on world wide web (pp. 1795-1804).\n\nThe digital misinformation pipeline. G L Ciampaglia, Positive learning in the age of information. SpringerCiampaglia, G. L. (2018). The digital misinformation pipeline. In Positive learning in the age of information (pp. 413-421). Springer.\n\nComputational fact checking from knowledge networks. G L Ciampaglia, P Shiralkar, L M Rocha, J Bollen, F Menczer, A Flammini, PloS one. 106128193Ciampaglia, G. L., Shiralkar, P., Rocha, L. M., Bollen, J., Menczer, F., & Flammini, A. (2015). Computational fact checking from knowledge networks. PloS one, 10 (6), e0128193.\n\nSupervised learning of universal sentence representations from natural language inference data. A Conneau, D Kiela, H Schwenk, L Barrault, A Bordes, Proceedings of the 2017 conference on Empirical Methods in Natural Language Processing. the 2017 conference on Empirical Methods in Natural Language ProcessingConneau, A., Kiela, D., Schwenk, H., Barrault, L., & Bordes, A. (2017). Supervised learning of universal sentence representations from natural language inference data. In Proceedings of the 2017 conference on Empirical Methods in Natural Language Processing (pp. 670-680).\n\nThe nature and origins of misperceptions: Understanding false and unsupported beliefs about politics. D Flynn, B Nyhan, J Reifler, Political Psychology. 38S1Flynn, D., Nyhan, B., & Reifler, J. (2017). The nature and origins of misperceptions: Understanding false and unsupported beliefs about politics. Political Psychology, 38 (S1), 127-150.\n\nHerox factchecking challenge. D Francis, M Babakar, 2017-01-11Francis, D., & Babakar, M. (2016). Herox factchecking challenge. Retrieved 2017-01-11, from https://www.herox.com/factcheck/5-practise-claims\n\nLiar, liar, pants on fire: How fact-checking influences citizens\u00e2\u0102\u0179 reactions to negative advertising. K Fridkin, P J Kenney, A Wintersieck, Political Communication. 321Fridkin, K., Kenney, P. J., & Wintersieck, A. (2015). Liar, liar, pants on fire: How fact-checking influences citizens\u00e2\u0102\u0179 reactions to negative advertising. Political Communication, 32 (1), 127-151.\n\nA contextaware approach for detecting worth-checking claims in political debates. P Gencheva, P Nakov, L M\u00e0rquez, A Barr\u00f3n-Cede\u00f1o, I Koychev, Proceedings of the international conference recent advances in natural language processing. the international conference recent advances in natural language processingGencheva, P., Nakov, P., M\u00e0rquez, L., Barr\u00f3n-Cede\u00f1o, A., & Koychev, I. (2017). A context- aware approach for detecting worth-checking claims in political debates. In Proceedings of the international conference recent advances in natural language processing (pp. 267-276).\n\n. Incoma Ltd, 10.26615/978-954-452-049-6_037doi: 10.26615/978-954-452-049-6_037INCOMA Ltd. Retrieved from https://doi.org/10.26615/978-954-452-049-6_037 doi: 10.26615/978-954-452-049-6_037\n\nFactsheet: Understanding the promise and limits of automated fact-checking (Tech. Rep.). Reuters Institute for the Study of Journalism. L Graves, University of OxfordGraves, L. (2018). Factsheet: Understanding the promise and limits of automated fact-checking (Tech. Rep.). Reuters Institute for the Study of Journalism, University of Oxford.\n\nUnderstanding innovations in journalistic practice: A field experiment examining motivations for fact-checking. L Graves, B Nyhan, J Reifler, Journal of Communication. 661Graves, L., Nyhan, B., & Reifler, J. (2016). Understanding innovations in journalistic practice: A field experiment examining motivations for fact-checking. Journal of Communication, 66 (1), 102-138.\n\nTowards automated fake news classification -on building collections for claim analysis research. &amp; Hanto, Tostrup, 2018-01-09Hanto, & Tostrup. (2018). Towards automated fake news classification -on building collections for claim analysis research. Retrieved 2018-01-09, from http://hdl.handle.net/11250/ 2560779\n\nToward automated fact-checking: Detecting check-worthy factual claims by ClaimBuster. N Hassan, F Arslan, C Li, M Tremayne, Proceedings of the 23rd acm sigkdd international conference on knowledge discovery and data mining. the 23rd acm sigkdd international conference on knowledge discovery and data miningHassan, N., Arslan, F., Li, C., & Tremayne, M. (2017). Toward automated fact-checking: De- tecting check-worthy factual claims by ClaimBuster. In Proceedings of the 23rd acm sigkdd international conference on knowledge discovery and data mining (pp. 1803-1812).\n\nClaim-Buster: the first-ever end-to-end fact-checking system. N Hassan, G Zhang, F Arslan, J Caraballo, D Jimenez, S Gawsane, Proceedings of the VLDB Endowment. the VLDB Endowment10Hassan, N., Zhang, G., Arslan, F., Caraballo, J., Jimenez, D., Gawsane, S., . . . others (2017). Claim- Buster: the first-ever end-to-end fact-checking system. Proceedings of the VLDB Endowment, 10 (12), 1945-1948.\n\nLong short-term memory. S Hochreiter, J Schmidhuber, Neural computation. 98Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9 (8), 1735-1780.\n\nTowards a benchmark for fact checking with knowledge bases. V.-P Huynh, P Papotti, In Companion of the the web conference 2018 on the web conference 2018Huynh, V.-P., & Papotti, P. (2018). Towards a benchmark for fact checking with knowledge bases. In Companion of the the web conference 2018 on the web conference 2018 (pp. 1595-1598).\n\nClaimrank: Detecting check-worthy claims in Arabic and English. I Jaradat, P Gencheva, A Barr\u00f3n-Cede\u00f1o, L M\u00e0rquez, P Nakov, Proceedings of the annual conference of the north american chapter of the association for computational linguistics: Human language technologies. the annual conference of the north american chapter of the association for computational linguistics: Human language technologiesJaradat, I., Gencheva, P., Barr\u00f3n-Cede\u00f1o, A., M\u00e0rquez, L., & Nakov, P. (2018). Claimrank: De- tecting check-worthy claims in Arabic and English. In Proceedings of the annual conference of the north american chapter of the association for computational linguistics: Human language technologies.\n\nA study of cross-validation and bootstrap for accuracy estimation and model selection. R Kohavi, International joint conference on artificial intelligence. Kohavi, R., et al. (1995). A study of cross-validation and bootstrap for accuracy estimation and model selection. In International joint conference on artificial intelligence (pp. 1137-1145).\n\n. K Krippendorff, Wiley Online LibraryKrippendorff, K. (1980). Reliability. Wiley Online Library.\n\nRumor detection over varying time windows. S Kwon, M Cha, K Jung, PloS one. 121168344Kwon, S., Cha, M., & Jung, K. (2017). Rumor detection over varying time windows. PloS one, 12 (1), e0168344.\n\nContext dependent claim detection. R Levy, Y Bilu, D Hershcovich, E Aharoni, N Slonim, Proceedings of coling 2014, the 25th international conference on computational linguistics: Technical papers. coling 2014, the 25th international conference on computational linguistics: Technical papersLevy, R., Bilu, Y., Hershcovich, D., Aharoni, E., & Slonim, N. (2014). Context dependent claim detection. In Proceedings of coling 2014, the 25th international conference on computational linguistics: Technical papers (pp. 1489-1500).\n\nFake news detection through multiperspective speaker profiles. Y Long, Q Lu, R Xiang, M Li, C.-R Huang, Proceedings of the eighth international joint conference on natural language processing. the eighth international joint conference on natural language processing2Short papersLong, Y., Lu, Q., Xiang, R., Li, M., & Huang, C.-R. (2017). Fake news detection through multi- perspective speaker profiles. In Proceedings of the eighth international joint conference on natural language processing (volume 2: Short papers) (Vol. 2, pp. 252-256).\n\nDetect rumors using time series of social context information on microblogging websites. J Ma, W Gao, Z Wei, Y Lu, K.-F Wong, Proceedings of the 24th acm international on conference on information and knowledge management. the 24th acm international on conference on information and knowledge managementMa, J., Gao, W., Wei, Z., Lu, Y., & Wong, K.-F. (2015). Detect rumors using time series of social context information on microblogging websites. In Proceedings of the 24th acm international on conference on information and knowledge management (pp. 1751-1754).\n\nEfficient estimation of word representations in vector space. T Mikolov, K Chen, G Corrado, J Dean, arXiv:1301.3781arXiv preprintMikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781 .\n\n. P Nakov, A Barr\u00f3n-Cede\u00f1o, T Elsayed, R Suwaileh, L M\u00e0rquez, W Zaghouani, Nakov, P., Barr\u00f3n-Cede\u00f1o, A., Elsayed, T., Suwaileh, R., M\u00e0rquez, L., Zaghouani, W., . . .\n\nCLEF-2018 lab on automatic identification and verification of claims in political debates. G Da San Martino, Working notes of CLEF 2018 -conference and labs of the evaluation forum. Avignon, FranceDa San Martino, G. (2018, September). CLEF-2018 lab on automatic identification and verification of claims in political debates. In Working notes of CLEF 2018 -conference and labs of the evaluation forum. Avignon, France.\n\nIdentifying appropriate support for propositions in online user comments. J Park, C Cardie, Proceedings of the first workshop on argumentation mining. the first workshop on argumentation miningPark, J., & Cardie, C. (2014). Identifying appropriate support for propositions in online user comments. In Proceedings of the first workshop on argumentation mining (pp. 29-38).\n\n. F Pedregosa, G Varoquaux, A Gramfort, V Michel, B Thirion, O Grisel, Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., . . .\n\nScikit-learn: Machine learning in Python. E Duchesnay, Journal of Machine Learning Research. 12Duchesnay, E. (2011). Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12 , 2825-2830.\n\nGlove: Global vectors for word representation. J Pennington, R Socher, C Manning, Proceedings of the 2014 conference on Empirical Methods in Natural Language Processing (EMNLP). the 2014 conference on Empirical Methods in Natural Language Processing (EMNLP)Pennington, J., Socher, R., & Manning, C. (2014). Glove: Global vectors for word representation. In Proceedings of the 2014 conference on Empirical Methods in Natural Language Processing (EMNLP) (pp. 1532-1543).\n\nSoftware Framework for Topic Modelling with Large Corpora. R \u0158eh\u016f\u0159ek, P Sojka, Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks. ELRA.the LREC 2010 Workshop on New Challenges for NLP FrameworksValletta, Malta\u0158eh\u016f\u0159ek, R., & Sojka, P. (2010, May 22). Software Framework for Topic Modelling with Large Corpora. In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks (pp. 45-50). Valletta, Malta: ELRA. (http://is.muni.cz/publication/884893/en)\n\nDiscriminative predicate path mining for fact checking in knowledge graphs. Knowledge-Based Systems. B Shi, T Weninger, 104Shi, B., & Weninger, T. (2016). Discriminative predicate path mining for fact checking in knowledge graphs. Knowledge-Based Systems, 104 , 123-133.\n\nComputational fact checking by mining knowledge graphs (Unpublished doctoral dissertation). P Shiralkar, Indiana UniversityShiralkar, P. (2017). Computational fact checking by mining knowledge graphs (Unpublished doctoral dissertation). Indiana University.\n\nFake news detection on social media: A data mining perspective. K Shu, A Sliva, S Wang, J Tang, H Liu, ACM SIGKDD Explorations Newsletter. 191Shu, K., Sliva, A., Wang, S., Tang, J., & Liu, H. (2017). Fake news detection on social media: A data mining perspective. ACM SIGKDD Explorations Newsletter, 19 (1), 22-36.\n\nInternational fact checking gains ground, Duke census finds. M Stencel, Durham, NCDuke Reporters' Lab (Tech. Rep.). Duke UniversityStencel, M. (2017). International fact checking gains ground, Duke census finds. Duke Reporters' Lab (Tech. Rep.). Duke University, Durham, NC.\n\nAutomated fact checking: Task formulations, methods and future directions. J Thorne, A Vlachos, arXiv:1806.07687arXiv preprintThorne, J., & Vlachos, A. (2018). Automated fact checking: Task formulations, methods and future directions. arXiv preprint arXiv:1806.07687 .\n\nFever: a large-scale dataset for fact extraction and verification. J Thorne, A Vlachos, C Christodoulopoulos, A Mittal, arXiv:1803.05355Vlachos, A., & Riedel, S.arXiv preprintThorne, J., Vlachos, A., Christodoulopoulos, C., & Mittal, A. (2018). Fever: a large-scale dataset for fact extraction and verification. arXiv preprint arXiv:1803.05355 . Vlachos, A., & Riedel, S. (2014).\n\nProceedings of the acl 2014 workshop on language technologies and computational social science. the acl 2014 workshop on language technologies and computational social scienceIn Proceedings of the acl 2014 workshop on language technologies and computational social science (pp. 18-22).\n\nliar, liar pants on fire\": A new benchmark dataset for fake news detection. W Y Wang, Proceedings of the 55th annual meeting of the association for computational linguistics. the 55th annual meeting of the association for computational linguistics2Short papersWang, W. Y. (2017). \" liar, liar pants on fire\": A new benchmark dataset for fake news detection. In Proceedings of the 55th annual meeting of the association for computational linguistics (volume 2: Short papers) (Vol. 2, pp. 422-426).\n\nToward computational fact-checking. Y Wu, P K Agarwal, C Li, J Yang, C Yu, Proceedings of the VLDB Endowment. the VLDB Endowment7Wu, Y., Agarwal, P. K., Li, C., Yang, J., & Yu, C. (2014). Toward computational fact-checking. Proceedings of the VLDB Endowment, 7 (7), 589-600.\n\nAutomatic detection of rumor on Sina Weibo. F Yang, Y Liu, X Yu, M Yang, Proceedings of the acm sigkdd workshop on mining data semantics. the acm sigkdd workshop on mining data semantics13Yang, F., Liu, Y., Yu, X., & Yang, M. (2012). Automatic detection of rumor on Sina Weibo. In Proceedings of the acm sigkdd workshop on mining data semantics (p. 13).\n\nDetection and resolution of rumours in social media: A survey. A Zubiaga, A Aker, K Bontcheva, M Liakata, R Procter, ACM Computing Surveys (CSUR). 51232Zubiaga, A., Aker, A., Bontcheva, K., Liakata, M., & Procter, R. (2018). Detection and resolution of rumours in social media: A survey. ACM Computing Surveys (CSUR), 51 (2), 32.\n", "annotations": {"author": "[{\"end\":163,\"start\":122},{\"end\":211,\"start\":164},{\"end\":246,\"start\":212},{\"end\":297,\"start\":247}]", "publisher": null, "author_last_name": "[{\"end\":142,\"start\":126},{\"end\":176,\"start\":171},{\"end\":225,\"start\":218},{\"end\":262,\"start\":255}]", "author_first_name": "[{\"end\":125,\"start\":122},{\"end\":170,\"start\":164},{\"end\":217,\"start\":212},{\"end\":254,\"start\":247}]", "author_affiliation": "[{\"end\":162,\"start\":144},{\"end\":210,\"start\":178},{\"end\":245,\"start\":227},{\"end\":296,\"start\":264}]", "title": "[{\"end\":119,\"start\":1},{\"end\":416,\"start\":298}]", "venue": null, "abstract": "[{\"end\":1456,\"start\":418}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b34\"},\"end\":1577,\"start\":1540},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":1628,\"start\":1577},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1645,\"start\":1628},{\"end\":1855,\"start\":1831},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":1986,\"start\":1954},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":1999,\"start\":1986},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2232,\"start\":2209},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":2284,\"start\":2232},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":2339,\"start\":2327},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2374,\"start\":2339},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":2433,\"start\":2404},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2464,\"start\":2433},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2488,\"start\":2464},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4445,\"start\":4424},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4543,\"start\":4505},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4573,\"start\":4543},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":4673,\"start\":4658},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5466,\"start\":5437},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5523,\"start\":5466},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":7775,\"start\":7740},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7799,\"start\":7775},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7815,\"start\":7799},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7836,\"start\":7815},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7862,\"start\":7836},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8063,\"start\":8034},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8748,\"start\":8725},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9655,\"start\":9632},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10115,\"start\":10057},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10134,\"start\":10115},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10511,\"start\":10461},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11113,\"start\":11090},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11137,\"start\":11115},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11617,\"start\":11594},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12020,\"start\":12000},{\"end\":12279,\"start\":12266},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12321,\"start\":12292},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":13668,\"start\":13643},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":15128,\"start\":15105},{\"end\":16231,\"start\":16230},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":16449,\"start\":16428},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":20686,\"start\":20664},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":20708,\"start\":20686},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":20736,\"start\":20708},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":23111,\"start\":23091},{\"end\":23925,\"start\":23818},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":26971,\"start\":26942},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":26993,\"start\":26971},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":27744,\"start\":27693},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":28151,\"start\":28119},{\"end\":28564,\"start\":28540},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":28967,\"start\":28938},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":29110,\"start\":29072},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":29164,\"start\":29141},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":29243,\"start\":29206},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":29600,\"start\":29562},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":29936,\"start\":29907},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":29976,\"start\":29953},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":30617,\"start\":30594},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":32468,\"start\":32447},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":37592,\"start\":37571}]", "figure": "[{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":39984,\"start\":38365},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":41241,\"start\":39985},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":41644,\"start\":41242}]", "paragraph": "[{\"end\":2077,\"start\":1472},{\"end\":3118,\"start\":2079},{\"end\":3435,\"start\":3120},{\"end\":4209,\"start\":3437},{\"end\":5064,\"start\":4238},{\"end\":5195,\"start\":5066},{\"end\":6182,\"start\":5197},{\"end\":7184,\"start\":6184},{\"end\":7523,\"start\":7186},{\"end\":7965,\"start\":7540},{\"end\":8720,\"start\":7967},{\"end\":9574,\"start\":8722},{\"end\":10358,\"start\":9576},{\"end\":10647,\"start\":10360},{\"end\":10950,\"start\":10649},{\"end\":13551,\"start\":10991},{\"end\":13917,\"start\":13553},{\"end\":14325,\"start\":13962},{\"end\":15469,\"start\":14327},{\"end\":15861,\"start\":15471},{\"end\":16029,\"start\":15863},{\"end\":16450,\"start\":16031},{\"end\":17525,\"start\":16452},{\"end\":18325,\"start\":17527},{\"end\":18490,\"start\":18349},{\"end\":18584,\"start\":18492},{\"end\":18725,\"start\":18586},{\"end\":20385,\"start\":18727},{\"end\":20449,\"start\":20387},{\"end\":20824,\"start\":20451},{\"end\":21704,\"start\":20826},{\"end\":22505,\"start\":21732},{\"end\":22922,\"start\":22507},{\"end\":23300,\"start\":22974},{\"end\":23926,\"start\":23302},{\"end\":24139,\"start\":23936},{\"end\":24445,\"start\":24333},{\"end\":25247,\"start\":24447},{\"end\":25755,\"start\":25249},{\"end\":26481,\"start\":25757},{\"end\":27374,\"start\":26483},{\"end\":27531,\"start\":27376},{\"end\":28732,\"start\":27543},{\"end\":28781,\"start\":28734},{\"end\":28968,\"start\":28783},{\"end\":29059,\"start\":28970},{\"end\":29366,\"start\":29061},{\"end\":29681,\"start\":29368},{\"end\":29937,\"start\":29683},{\"end\":30618,\"start\":29939},{\"end\":30997,\"start\":30642},{\"end\":31379,\"start\":30999},{\"end\":32469,\"start\":31399},{\"end\":32725,\"start\":32493},{\"end\":34605,\"start\":32756},{\"end\":34761,\"start\":34653},{\"end\":35699,\"start\":34763},{\"end\":36346,\"start\":35701},{\"end\":36622,\"start\":36348},{\"end\":36947,\"start\":36624},{\"end\":37133,\"start\":36949},{\"end\":37593,\"start\":37149},{\"end\":38364,\"start\":37608}]", "formula": null, "table_ref": "[{\"end\":20641,\"start\":20634},{\"end\":21658,\"start\":21651},{\"end\":23299,\"start\":23292},{\"end\":23647,\"start\":23640},{\"end\":24025,\"start\":24018},{\"end\":24444,\"start\":24437},{\"end\":25028,\"start\":25021},{\"end\":25337,\"start\":25329},{\"end\":25938,\"start\":25931},{\"end\":27530,\"start\":27523},{\"end\":31513,\"start\":31506},{\"end\":31522,\"start\":31515},{\"end\":33019,\"start\":33012}]", "section_header": "[{\"end\":1470,\"start\":1458},{\"end\":4236,\"start\":4212},{\"end\":7538,\"start\":7526},{\"end\":10989,\"start\":10953},{\"end\":13960,\"start\":13920},{\"end\":18347,\"start\":18328},{\"end\":21730,\"start\":21707},{\"end\":22960,\"start\":22925},{\"end\":22972,\"start\":22963},{\"end\":23934,\"start\":23929},{\"end\":24331,\"start\":24142},{\"end\":27541,\"start\":27534},{\"end\":30640,\"start\":30621},{\"end\":31397,\"start\":31382},{\"end\":32491,\"start\":32472},{\"end\":32754,\"start\":32728},{\"end\":34627,\"start\":34608},{\"end\":34651,\"start\":34630},{\"end\":37147,\"start\":37136},{\"end\":37606,\"start\":37596}]", "table": "[{\"end\":39984,\"start\":38465},{\"end\":41241,\"start\":40116},{\"end\":41644,\"start\":41290}]", "figure_caption": "[{\"end\":38465,\"start\":38367},{\"end\":40116,\"start\":39987},{\"end\":41290,\"start\":41244}]", "figure_ref": "[{\"end\":23804,\"start\":23796},{\"end\":34333,\"start\":34325},{\"end\":36959,\"start\":36951}]", "bib_author_first_name": "[{\"end\":42771,\"start\":42770},{\"end\":42778,\"start\":42777},{\"end\":43093,\"start\":43092},{\"end\":43100,\"start\":43099},{\"end\":43106,\"start\":43105},{\"end\":43113,\"start\":43112},{\"end\":43486,\"start\":43485},{\"end\":43488,\"start\":43487},{\"end\":43744,\"start\":43743},{\"end\":43746,\"start\":43745},{\"end\":43760,\"start\":43759},{\"end\":43773,\"start\":43772},{\"end\":43775,\"start\":43774},{\"end\":43784,\"start\":43783},{\"end\":43794,\"start\":43793},{\"end\":43805,\"start\":43804},{\"end\":44110,\"start\":44109},{\"end\":44121,\"start\":44120},{\"end\":44130,\"start\":44129},{\"end\":44141,\"start\":44140},{\"end\":44153,\"start\":44152},{\"end\":44698,\"start\":44697},{\"end\":44707,\"start\":44706},{\"end\":44716,\"start\":44715},{\"end\":44970,\"start\":44969},{\"end\":44981,\"start\":44980},{\"end\":45248,\"start\":45247},{\"end\":45259,\"start\":45258},{\"end\":45261,\"start\":45260},{\"end\":45271,\"start\":45270},{\"end\":45596,\"start\":45595},{\"end\":45608,\"start\":45607},{\"end\":45617,\"start\":45616},{\"end\":45628,\"start\":45627},{\"end\":45645,\"start\":45644},{\"end\":46422,\"start\":46421},{\"end\":46742,\"start\":46741},{\"end\":46752,\"start\":46751},{\"end\":46761,\"start\":46760},{\"end\":47103,\"start\":47098},{\"end\":47405,\"start\":47404},{\"end\":47415,\"start\":47414},{\"end\":47425,\"start\":47424},{\"end\":47431,\"start\":47430},{\"end\":47951,\"start\":47950},{\"end\":47961,\"start\":47960},{\"end\":47970,\"start\":47969},{\"end\":47980,\"start\":47979},{\"end\":47993,\"start\":47992},{\"end\":48004,\"start\":48003},{\"end\":48310,\"start\":48309},{\"end\":48324,\"start\":48323},{\"end\":48529,\"start\":48525},{\"end\":48538,\"start\":48537},{\"end\":48868,\"start\":48867},{\"end\":48879,\"start\":48878},{\"end\":48891,\"start\":48890},{\"end\":48908,\"start\":48907},{\"end\":48919,\"start\":48918},{\"end\":49585,\"start\":49584},{\"end\":49849,\"start\":49848},{\"end\":49989,\"start\":49988},{\"end\":49997,\"start\":49996},{\"end\":50004,\"start\":50003},{\"end\":50176,\"start\":50175},{\"end\":50184,\"start\":50183},{\"end\":50192,\"start\":50191},{\"end\":50207,\"start\":50206},{\"end\":50218,\"start\":50217},{\"end\":50730,\"start\":50729},{\"end\":50738,\"start\":50737},{\"end\":50744,\"start\":50743},{\"end\":50753,\"start\":50752},{\"end\":50762,\"start\":50758},{\"end\":51299,\"start\":51298},{\"end\":51305,\"start\":51304},{\"end\":51312,\"start\":51311},{\"end\":51319,\"start\":51318},{\"end\":51328,\"start\":51324},{\"end\":51837,\"start\":51836},{\"end\":51848,\"start\":51847},{\"end\":51856,\"start\":51855},{\"end\":51867,\"start\":51866},{\"end\":52057,\"start\":52056},{\"end\":52066,\"start\":52065},{\"end\":52083,\"start\":52082},{\"end\":52094,\"start\":52093},{\"end\":52106,\"start\":52105},{\"end\":52117,\"start\":52116},{\"end\":52313,\"start\":52312},{\"end\":52716,\"start\":52715},{\"end\":52724,\"start\":52723},{\"end\":53017,\"start\":53016},{\"end\":53030,\"start\":53029},{\"end\":53043,\"start\":53042},{\"end\":53055,\"start\":53054},{\"end\":53065,\"start\":53064},{\"end\":53076,\"start\":53075},{\"end\":53216,\"start\":53215},{\"end\":53435,\"start\":53434},{\"end\":53449,\"start\":53448},{\"end\":53459,\"start\":53458},{\"end\":53917,\"start\":53916},{\"end\":53928,\"start\":53927},{\"end\":54450,\"start\":54449},{\"end\":54457,\"start\":54456},{\"end\":54713,\"start\":54712},{\"end\":54943,\"start\":54942},{\"end\":54950,\"start\":54949},{\"end\":54959,\"start\":54958},{\"end\":54967,\"start\":54966},{\"end\":54975,\"start\":54974},{\"end\":55256,\"start\":55255},{\"end\":55546,\"start\":55545},{\"end\":55556,\"start\":55555},{\"end\":55808,\"start\":55807},{\"end\":55818,\"start\":55817},{\"end\":55829,\"start\":55828},{\"end\":55851,\"start\":55850},{\"end\":56485,\"start\":56484},{\"end\":56487,\"start\":56486},{\"end\":56943,\"start\":56942},{\"end\":56949,\"start\":56948},{\"end\":56951,\"start\":56950},{\"end\":56962,\"start\":56961},{\"end\":56968,\"start\":56967},{\"end\":56976,\"start\":56975},{\"end\":57227,\"start\":57226},{\"end\":57235,\"start\":57234},{\"end\":57242,\"start\":57241},{\"end\":57248,\"start\":57247},{\"end\":57601,\"start\":57600},{\"end\":57612,\"start\":57611},{\"end\":57620,\"start\":57619},{\"end\":57633,\"start\":57632},{\"end\":57644,\"start\":57643}]", "bib_author_last_name": "[{\"end\":42775,\"start\":42772},{\"end\":43097,\"start\":43094},{\"end\":43103,\"start\":43101},{\"end\":43110,\"start\":43107},{\"end\":43117,\"start\":43114},{\"end\":43499,\"start\":43489},{\"end\":43757,\"start\":43747},{\"end\":43770,\"start\":43761},{\"end\":43781,\"start\":43776},{\"end\":43791,\"start\":43785},{\"end\":43802,\"start\":43795},{\"end\":43814,\"start\":43806},{\"end\":44118,\"start\":44111},{\"end\":44127,\"start\":44122},{\"end\":44138,\"start\":44131},{\"end\":44150,\"start\":44142},{\"end\":44160,\"start\":44154},{\"end\":44704,\"start\":44699},{\"end\":44713,\"start\":44708},{\"end\":44724,\"start\":44717},{\"end\":44978,\"start\":44971},{\"end\":44989,\"start\":44982},{\"end\":45256,\"start\":45249},{\"end\":45268,\"start\":45262},{\"end\":45283,\"start\":45272},{\"end\":45605,\"start\":45597},{\"end\":45614,\"start\":45609},{\"end\":45625,\"start\":45618},{\"end\":45642,\"start\":45629},{\"end\":45653,\"start\":45646},{\"end\":46107,\"start\":46097},{\"end\":46429,\"start\":46423},{\"end\":46749,\"start\":46743},{\"end\":46758,\"start\":46753},{\"end\":46769,\"start\":46762},{\"end\":47109,\"start\":47104},{\"end\":47118,\"start\":47111},{\"end\":47412,\"start\":47406},{\"end\":47422,\"start\":47416},{\"end\":47428,\"start\":47426},{\"end\":47440,\"start\":47432},{\"end\":47958,\"start\":47952},{\"end\":47967,\"start\":47962},{\"end\":47977,\"start\":47971},{\"end\":47990,\"start\":47981},{\"end\":48001,\"start\":47994},{\"end\":48012,\"start\":48005},{\"end\":48321,\"start\":48311},{\"end\":48336,\"start\":48325},{\"end\":48535,\"start\":48530},{\"end\":48546,\"start\":48539},{\"end\":48876,\"start\":48869},{\"end\":48888,\"start\":48880},{\"end\":48905,\"start\":48892},{\"end\":48916,\"start\":48909},{\"end\":48925,\"start\":48920},{\"end\":49592,\"start\":49586},{\"end\":49862,\"start\":49850},{\"end\":49994,\"start\":49990},{\"end\":50001,\"start\":49998},{\"end\":50009,\"start\":50005},{\"end\":50181,\"start\":50177},{\"end\":50189,\"start\":50185},{\"end\":50204,\"start\":50193},{\"end\":50215,\"start\":50208},{\"end\":50225,\"start\":50219},{\"end\":50735,\"start\":50731},{\"end\":50741,\"start\":50739},{\"end\":50750,\"start\":50745},{\"end\":50756,\"start\":50754},{\"end\":50768,\"start\":50763},{\"end\":51302,\"start\":51300},{\"end\":51309,\"start\":51306},{\"end\":51316,\"start\":51313},{\"end\":51322,\"start\":51320},{\"end\":51333,\"start\":51329},{\"end\":51845,\"start\":51838},{\"end\":51853,\"start\":51849},{\"end\":51864,\"start\":51857},{\"end\":51872,\"start\":51868},{\"end\":52063,\"start\":52058},{\"end\":52080,\"start\":52067},{\"end\":52091,\"start\":52084},{\"end\":52103,\"start\":52095},{\"end\":52114,\"start\":52107},{\"end\":52127,\"start\":52118},{\"end\":52328,\"start\":52314},{\"end\":52721,\"start\":52717},{\"end\":52731,\"start\":52725},{\"end\":53027,\"start\":53018},{\"end\":53040,\"start\":53031},{\"end\":53052,\"start\":53044},{\"end\":53062,\"start\":53056},{\"end\":53073,\"start\":53066},{\"end\":53083,\"start\":53077},{\"end\":53226,\"start\":53217},{\"end\":53446,\"start\":53436},{\"end\":53456,\"start\":53450},{\"end\":53467,\"start\":53460},{\"end\":53925,\"start\":53918},{\"end\":53934,\"start\":53929},{\"end\":54454,\"start\":54451},{\"end\":54466,\"start\":54458},{\"end\":54723,\"start\":54714},{\"end\":54947,\"start\":54944},{\"end\":54956,\"start\":54951},{\"end\":54964,\"start\":54960},{\"end\":54972,\"start\":54968},{\"end\":54979,\"start\":54976},{\"end\":55264,\"start\":55257},{\"end\":55553,\"start\":55547},{\"end\":55564,\"start\":55557},{\"end\":55815,\"start\":55809},{\"end\":55826,\"start\":55819},{\"end\":55848,\"start\":55830},{\"end\":55858,\"start\":55852},{\"end\":56492,\"start\":56488},{\"end\":56946,\"start\":56944},{\"end\":56959,\"start\":56952},{\"end\":56965,\"start\":56963},{\"end\":56973,\"start\":56969},{\"end\":56979,\"start\":56977},{\"end\":57232,\"start\":57228},{\"end\":57239,\"start\":57236},{\"end\":57245,\"start\":57243},{\"end\":57253,\"start\":57249},{\"end\":57609,\"start\":57602},{\"end\":57617,\"start\":57613},{\"end\":57630,\"start\":57621},{\"end\":57641,\"start\":57634},{\"end\":57652,\"start\":57645}]", "bib_entry": "[{\"attributes\":{\"doi\":\"2018-01-09\",\"id\":\"b0\"},\"end\":43015,\"start\":42733},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":4901319},\"end\":43446,\"start\":43017},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":158091211},\"end\":43688,\"start\":43448},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":16004059},\"end\":44011,\"start\":43690},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":28971531},\"end\":44593,\"start\":44013},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":2381114},\"end\":44937,\"start\":44595},{\"attributes\":{\"doi\":\"2017-01-11\",\"id\":\"b6\"},\"end\":45142,\"start\":44939},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":143495044},\"end\":45511,\"start\":45144},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":12940167},\"end\":46093,\"start\":45513},{\"attributes\":{\"doi\":\"10.26615/978-954-452-049-6_037\",\"id\":\"b9\"},\"end\":46283,\"start\":46095},{\"attributes\":{\"id\":\"b10\"},\"end\":46627,\"start\":46285},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":55700325},\"end\":46999,\"start\":46629},{\"attributes\":{\"id\":\"b12\"},\"end\":47316,\"start\":47001},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":4651333},\"end\":47886,\"start\":47318},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":4663082},\"end\":48283,\"start\":47888},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":1915014},\"end\":48463,\"start\":48285},{\"attributes\":{\"id\":\"b16\"},\"end\":48801,\"start\":48465},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":5039661},\"end\":49495,\"start\":48803},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":2702042},\"end\":49844,\"start\":49497},{\"attributes\":{\"id\":\"b19\"},\"end\":49943,\"start\":49846},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":7246441},\"end\":50138,\"start\":49945},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":18847466},\"end\":50664,\"start\":50140},{\"attributes\":{\"id\":\"b22\"},\"end\":51207,\"start\":50666},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":17025981},\"end\":51772,\"start\":51209},{\"attributes\":{\"id\":\"b24\"},\"end\":52052,\"start\":51774},{\"attributes\":{\"id\":\"b25\"},\"end\":52219,\"start\":52054},{\"attributes\":{\"id\":\"b26\"},\"end\":52639,\"start\":52221},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":14764893},\"end\":53012,\"start\":52641},{\"attributes\":{\"id\":\"b28\"},\"end\":53171,\"start\":53014},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":10659969},\"end\":53385,\"start\":53173},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":1957433},\"end\":53855,\"start\":53387},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":18593743},\"end\":54346,\"start\":53857},{\"attributes\":{\"id\":\"b32\"},\"end\":54618,\"start\":54348},{\"attributes\":{\"id\":\"b33\"},\"end\":54876,\"start\":54620},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":207718082},\"end\":55192,\"start\":54878},{\"attributes\":{\"id\":\"b35\"},\"end\":55468,\"start\":55194},{\"attributes\":{\"id\":\"b36\"},\"end\":55738,\"start\":55470},{\"attributes\":{\"id\":\"b37\"},\"end\":56119,\"start\":55740},{\"attributes\":{\"id\":\"b38\"},\"end\":56406,\"start\":56121},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":10326133},\"end\":56904,\"start\":56408},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":16389319},\"end\":57180,\"start\":56906},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":207197084},\"end\":57535,\"start\":57182},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":207743293},\"end\":57866,\"start\":57537}]", "bib_title": "[{\"end\":42768,\"start\":42733},{\"end\":43090,\"start\":43017},{\"end\":43483,\"start\":43448},{\"end\":43741,\"start\":43690},{\"end\":44107,\"start\":44013},{\"end\":44695,\"start\":44595},{\"end\":45245,\"start\":45144},{\"end\":45593,\"start\":45513},{\"end\":46739,\"start\":46629},{\"end\":47402,\"start\":47318},{\"end\":47948,\"start\":47888},{\"end\":48307,\"start\":48285},{\"end\":48865,\"start\":48803},{\"end\":49582,\"start\":49497},{\"end\":49986,\"start\":49945},{\"end\":50173,\"start\":50140},{\"end\":50727,\"start\":50666},{\"end\":51296,\"start\":51209},{\"end\":52310,\"start\":52221},{\"end\":52713,\"start\":52641},{\"end\":53213,\"start\":53173},{\"end\":53432,\"start\":53387},{\"end\":53914,\"start\":53857},{\"end\":54940,\"start\":54878},{\"end\":56482,\"start\":56408},{\"end\":56940,\"start\":56906},{\"end\":57224,\"start\":57182},{\"end\":57598,\"start\":57537}]", "bib_author": "[{\"end\":42777,\"start\":42770},{\"end\":42781,\"start\":42777},{\"end\":43099,\"start\":43092},{\"end\":43105,\"start\":43099},{\"end\":43112,\"start\":43105},{\"end\":43119,\"start\":43112},{\"end\":43501,\"start\":43485},{\"end\":43759,\"start\":43743},{\"end\":43772,\"start\":43759},{\"end\":43783,\"start\":43772},{\"end\":43793,\"start\":43783},{\"end\":43804,\"start\":43793},{\"end\":43816,\"start\":43804},{\"end\":44120,\"start\":44109},{\"end\":44129,\"start\":44120},{\"end\":44140,\"start\":44129},{\"end\":44152,\"start\":44140},{\"end\":44162,\"start\":44152},{\"end\":44706,\"start\":44697},{\"end\":44715,\"start\":44706},{\"end\":44726,\"start\":44715},{\"end\":44980,\"start\":44969},{\"end\":44991,\"start\":44980},{\"end\":45258,\"start\":45247},{\"end\":45270,\"start\":45258},{\"end\":45285,\"start\":45270},{\"end\":45607,\"start\":45595},{\"end\":45616,\"start\":45607},{\"end\":45627,\"start\":45616},{\"end\":45644,\"start\":45627},{\"end\":45655,\"start\":45644},{\"end\":46109,\"start\":46097},{\"end\":46431,\"start\":46421},{\"end\":46751,\"start\":46741},{\"end\":46760,\"start\":46751},{\"end\":46771,\"start\":46760},{\"end\":47111,\"start\":47098},{\"end\":47120,\"start\":47111},{\"end\":47414,\"start\":47404},{\"end\":47424,\"start\":47414},{\"end\":47430,\"start\":47424},{\"end\":47442,\"start\":47430},{\"end\":47960,\"start\":47950},{\"end\":47969,\"start\":47960},{\"end\":47979,\"start\":47969},{\"end\":47992,\"start\":47979},{\"end\":48003,\"start\":47992},{\"end\":48014,\"start\":48003},{\"end\":48323,\"start\":48309},{\"end\":48338,\"start\":48323},{\"end\":48537,\"start\":48525},{\"end\":48548,\"start\":48537},{\"end\":48878,\"start\":48867},{\"end\":48890,\"start\":48878},{\"end\":48907,\"start\":48890},{\"end\":48918,\"start\":48907},{\"end\":48927,\"start\":48918},{\"end\":49594,\"start\":49584},{\"end\":49864,\"start\":49848},{\"end\":49996,\"start\":49988},{\"end\":50003,\"start\":49996},{\"end\":50011,\"start\":50003},{\"end\":50183,\"start\":50175},{\"end\":50191,\"start\":50183},{\"end\":50206,\"start\":50191},{\"end\":50217,\"start\":50206},{\"end\":50227,\"start\":50217},{\"end\":50737,\"start\":50729},{\"end\":50743,\"start\":50737},{\"end\":50752,\"start\":50743},{\"end\":50758,\"start\":50752},{\"end\":50770,\"start\":50758},{\"end\":51304,\"start\":51298},{\"end\":51311,\"start\":51304},{\"end\":51318,\"start\":51311},{\"end\":51324,\"start\":51318},{\"end\":51335,\"start\":51324},{\"end\":51847,\"start\":51836},{\"end\":51855,\"start\":51847},{\"end\":51866,\"start\":51855},{\"end\":51874,\"start\":51866},{\"end\":52065,\"start\":52056},{\"end\":52082,\"start\":52065},{\"end\":52093,\"start\":52082},{\"end\":52105,\"start\":52093},{\"end\":52116,\"start\":52105},{\"end\":52129,\"start\":52116},{\"end\":52330,\"start\":52312},{\"end\":52723,\"start\":52715},{\"end\":52733,\"start\":52723},{\"end\":53029,\"start\":53016},{\"end\":53042,\"start\":53029},{\"end\":53054,\"start\":53042},{\"end\":53064,\"start\":53054},{\"end\":53075,\"start\":53064},{\"end\":53085,\"start\":53075},{\"end\":53228,\"start\":53215},{\"end\":53448,\"start\":53434},{\"end\":53458,\"start\":53448},{\"end\":53469,\"start\":53458},{\"end\":53927,\"start\":53916},{\"end\":53936,\"start\":53927},{\"end\":54456,\"start\":54449},{\"end\":54468,\"start\":54456},{\"end\":54725,\"start\":54712},{\"end\":54949,\"start\":54942},{\"end\":54958,\"start\":54949},{\"end\":54966,\"start\":54958},{\"end\":54974,\"start\":54966},{\"end\":54981,\"start\":54974},{\"end\":55266,\"start\":55255},{\"end\":55555,\"start\":55545},{\"end\":55566,\"start\":55555},{\"end\":55817,\"start\":55807},{\"end\":55828,\"start\":55817},{\"end\":55850,\"start\":55828},{\"end\":55860,\"start\":55850},{\"end\":56494,\"start\":56484},{\"end\":56948,\"start\":56942},{\"end\":56961,\"start\":56948},{\"end\":56967,\"start\":56961},{\"end\":56975,\"start\":56967},{\"end\":56981,\"start\":56975},{\"end\":57234,\"start\":57226},{\"end\":57241,\"start\":57234},{\"end\":57247,\"start\":57241},{\"end\":57255,\"start\":57247},{\"end\":57611,\"start\":57600},{\"end\":57619,\"start\":57611},{\"end\":57632,\"start\":57619},{\"end\":57643,\"start\":57632},{\"end\":57654,\"start\":57643}]", "bib_venue": "[{\"end\":42813,\"start\":42791},{\"end\":43186,\"start\":43119},{\"end\":43544,\"start\":43501},{\"end\":43824,\"start\":43816},{\"end\":44248,\"start\":44162},{\"end\":44746,\"start\":44726},{\"end\":44967,\"start\":44939},{\"end\":45308,\"start\":45285},{\"end\":45745,\"start\":45655},{\"end\":46419,\"start\":46285},{\"end\":46795,\"start\":46771},{\"end\":47096,\"start\":47001},{\"end\":47540,\"start\":47442},{\"end\":48047,\"start\":48014},{\"end\":48356,\"start\":48338},{\"end\":48523,\"start\":48465},{\"end\":49071,\"start\":48927},{\"end\":49651,\"start\":49594},{\"end\":50019,\"start\":50011},{\"end\":50335,\"start\":50227},{\"end\":50857,\"start\":50770},{\"end\":51430,\"start\":51335},{\"end\":51834,\"start\":51774},{\"end\":52401,\"start\":52330},{\"end\":52790,\"start\":52733},{\"end\":53264,\"start\":53228},{\"end\":53563,\"start\":53469},{\"end\":54010,\"start\":53936},{\"end\":54447,\"start\":54348},{\"end\":54710,\"start\":54620},{\"end\":55015,\"start\":54981},{\"end\":55253,\"start\":55194},{\"end\":55543,\"start\":55470},{\"end\":55805,\"start\":55740},{\"end\":56215,\"start\":56121},{\"end\":56581,\"start\":56494},{\"end\":57014,\"start\":56981},{\"end\":57318,\"start\":57255},{\"end\":57682,\"start\":57654},{\"end\":43240,\"start\":43188},{\"end\":44321,\"start\":44250},{\"end\":45822,\"start\":45747},{\"end\":47625,\"start\":47542},{\"end\":48067,\"start\":48049},{\"end\":49202,\"start\":49073},{\"end\":50430,\"start\":50337},{\"end\":50931,\"start\":50859},{\"end\":51512,\"start\":51432},{\"end\":52418,\"start\":52403},{\"end\":52834,\"start\":52792},{\"end\":53644,\"start\":53565},{\"end\":54091,\"start\":54017},{\"end\":56296,\"start\":56217},{\"end\":56655,\"start\":56583},{\"end\":57034,\"start\":57016},{\"end\":57368,\"start\":57320}]"}}}, "year": 2023, "month": 12, "day": 17}