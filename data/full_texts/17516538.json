{"id": 17516538, "updated": "2023-08-30 17:05:15.689", "metadata": {"title": "A survey on visual surveillance of object motion and behaviors", "authors": "[{\"first\":\"\",\"last\":\"Weiming Hu\",\"middle\":[]},{\"first\":\"\",\"last\":\"Tieniu Tan\",\"middle\":[]},{\"first\":\"\",\"last\":\"Liang Wang\",\"middle\":[]},{\"first\":\"S.\",\"last\":\"Maybank\",\"middle\":[]}]", "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)", "journal": "IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)", "publication_date": {"year": 2004, "month": null, "day": null}, "abstract": "Visual surveillance in dynamic scenes, especially for humans and vehicles, is currently one of the most active research topics in computer vision. It has a wide spectrum of promising applications, including access control in special areas, human identification at a distance, crowd flux statistics and congestion analysis, detection of anomalous behaviors, and interactive surveillance using multiple cameras, etc. In general, the processing framework of visual surveillance in dynamic scenes includes the following stages: modeling of environments, detection of motion, classification of moving objects, tracking, understanding and description of behaviors, human identification, and fusion of data from multiple cameras. We review recent developments and general strategies of all these stages. Finally, we analyze possible research directions, e.g., occlusion handling, a combination of twoand three-dimensional tracking, a combination of motion analysis and biometrics, anomaly detection and behavior prediction, content-based retrieval of surveillance videos, behavior understanding and natural language description, fusion of information from multiple sensors, and remote surveillance.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2118572719", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/tsmc/HuTWM04", "doi": "10.1109/tsmcc.2004.829274"}}, "content": {"source": {"pdf_hash": "461efc87636ff4e48323ffcb9f8fdf79cf736fb0", "pdf_src": "ScienceParseMerged", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "http://www.cbsr.ia.ac.cn/student_corner/hu-group/papers/a%20survey%20on%20visual%20surveillance%20of%20object%20motion%20and%20behaviors.pdf", "status": "GREEN"}}, "grobid": {"id": "b57526e918b19a0e5eb962e47d8851d859aa0ac7", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/461efc87636ff4e48323ffcb9f8fdf79cf736fb0.txt", "contents": "\nA Survey on Visual Surveillance of Object Motion and Behaviors\nAUGUST 2004\n\nIeee \nMan Systems \nCybernetics-Part C \nA Survey on Visual Surveillance of Object Motion and Behaviors\n\nAPPLICATIONS AND REVIEWS\n343AUGUST 200410.1109/TSMCC.2004.829274334\nVisual surveillance in dynamic scenes, especially for humans and vehicles, is currently one of the most active research topics in computer vision. It has a wide spectrum of promising applications, including access control in special areas, human identification at a distance, crowd flux statistics and congestion analysis, detection of anomalous behaviors, and interactive surveillance using multiple cameras, etc. In general, the processing framework of visual surveillance in dynamic scenes includes the following stages: modeling of environments, detection of motion, classification of moving objects, tracking, understanding and description of behaviors, human identification, and fusion of data from multiple cameras. We review recent developments and general strategies of all these stages. Finally, we analyze possible research directions, e.g., occlusion handling, a combination of twoand three-dimensional tracking, a combination of motion analysis and biometrics, anomaly detection and behavior prediction, content-based retrieval of surveillance videos, behavior understanding and natural language description, fusion of information from multiple sensors, and remote surveillance.Index Terms-Behavior understanding and description, fusion of data from multiple cameras, motion detection, personal identification, tracking, visual surveillance.\n\nI. INTRODUCTION\n\nA S AN ACTIVE research topic in computer vision, visual surveillance in dynamic scenes attempts to detect, recognize and track certain objects from image sequences, and more generally to understand and describe object behaviors. The aim is to develop intelligent visual surveillance to replace the traditional passive video surveillance that is proving ineffective as the number of cameras exceeds the capability of human operators to monitor them. In short, the goal of visual surveillance is not only to put cameras in the place of human eyes, but also to accomplish the entire surveillance task as automatically as possible.\n\nVisual surveillance in dynamic scenes has a wide range of potential applications, such as a security guard for communities and important buildings, traffic surveillance in cities and expressways, detection of military targets, etc. We focus in this paper on applications involving the surveillance of people or vehicles, as they are typical of surveillance applications in general, and include the full range of surveillance methods. Surveillance applications involving people or vehicles include the following.\n\n1) Access control in special areas. In some security-sensitive locations such as military bases and important governmental units, only people with a special identity are allowed to enter. A biometric feature database including legal visitors is built beforehand using biometric techniques. When somebody is about to enter, the system could automatically obtain the visitor's features, such as height, facial appearance and walking gait from images taken in real time, and then decide whether the visitor can be cleared for entry. 2) Person-specific identification in certain scenes. Personal identification at a distance by a smart surveillance system can help the police to catch suspects. The police may build a biometric feature database of suspects, and place visual surveillance systems at locations where the suspects usually appear, e.g., subway stations, casinos, etc. The systems automatically recognize and judge whether or not the people in view are suspects. If yes, alarms are given immediately. Such systems with face recognition have already been used at public sites, but the reliability is too low for police requirements. 3) Crowd flux statistics and congestion analysis. Using techniques for human detection, visual surveillance systems can automatically compute the flux of people at important public areas such as stores and travel sites, and then provide congestion analysis to assist in the management of the people. In the same way, visual surveillance systems can monitor expressways and junctions of the road network, and further analyze the traffic flow and the status of road congestion, which are of great importance for traffic management. 4) Anomaly detection and alarming. In some circumstances, it is necessary to analyze the behaviors of people and vehicles and determine whether these behaviors are normal or abnormal. For example, visual surveillance systems set in parking lots and supermarkets could analyze abnormal behaviors indicative of theft. Normally, there are two ways of giving an alarm. One way is to automatically make a recorded public announcement whenever any abnormal behavior is detected. The other is to contact the police automatically. 5) Interactive surveillance using multiple cameras. For social security, cooperative surveillance using multiple cameras could be used to ensure the security of an entire community, for example by tracking suspects over a wide area by using the cooperation of multiple cameras. For traffic management, interactive surveillance using multiple cameras can help the traffic police discover, track, and catch vehicles involved in traffic offences. It is the broad range of applications that motivates the interests of researchers worldwide. For example, the IEEE has sponsored the IEEE International Workshop on Visual Surveillance on three occasions, in India (1998), the U.S. (1999), and Ireland (2000). In [68] and [1], a special section on visual surveillance was published in June and August of 2000, respectively. In [78], a special issue on visual analysis of human motion was published in March 2001. In [69], a special issue on third-generation surveillance systems was published in October 2001. In [130], a special issue on understanding visual behavior was published in October 2002. Recent developments in human motion analysis are briefly introduced in our previous paper [75]. It is noticeable that, after the 9/11 event, visual surveillance has received more attention not only from the academic community, but also from industry and governments.\n\nVisual surveillance has been investigated worldwide under several large research projects. For example, the Defense Advanced Research Projection Agency (DARPA) supported the Visual Surveillance and Monitoring (VSAM) project [3] in 1997, whose purpose was to develop automatic video understanding technologies that enable a single human operator to monitor behaviors over complex areas such as battlefields and civilian scenes. Furthermore, to enhance protection from terrorist attacks, the Human Identification at a Distance (HID) program sponsored by DARPA in 2000 aims to develop a full range of multimodal surveillance technologies for successfully detecting, classifying, and identifying humans at great distances. The European Union's Framework V Programme sponsored Advisor, a core project on visual surveillance in metrostations.\n\nThere have been a number of famous visual surveillance systems. The real-time visual surveillance system W4 [4] employs a combination of shape analysis and tracking, and constructs models of people's appearances in order to detect and track groups of people as well as monitor their behaviors even in the presence of occlusion and in outdoor environments. This system uses the single camera and grayscale sensor. The VIEWS system [87] at the University of Reading is a three-dimensional (3-D) model based vehicle tracking system. The Pfinder system developed by Wren et al. [8] is used to recover a 3-D description of a person in a large room. It tracks a single nonoccluded person in complex scenes, and has been used in many applications. As a single-person tracking system, TI, developed by Olsen et al. [9], detects moving objects in indoor scenes using motion detection, tracks them using first-order prediction, and recognizes behaviors by applying predicates to a graph formed by linking corresponding objects in successive frames. This system cannot handle small motions of background objects. The system at CMU [10] can monitor activities over a large area using multiple cameras that are connected into a network. It can detect and track multiple persons and vehicles within cluttered scenes and monitor their activities over long periods of time. The above comments on [8]- [10] are derived from [4]. Please see [4] for more details.\n\nAs far as hardware is concerned, companies like Sony and Intel have designed equipment suitable for visual surveillance, e.g., active cameras, smart cameras [76], omni-directional cameras [23], [77], etc.\n\nAll of the above activities are evidence of a great and growing interest in visual surveillance in dynamic scenes. The primary purpose of this paper is to give a general review on the overall process of a visual surveillance system. Fig. 1 shows the general framework of visual surveillance in dynamic scenes. The prerequisites for effective automatic surveillance using a single camera include the following stages: modeling of environments, detection of motion, classification of moving objects, tracking, understanding and description of behaviors, and human identification. In order to extend the surveillance area and overcome occlusion, fusion of data from multiple cameras is needed. This fusion can involve all the above stages. In this paper we review recent developments and analyze future open directions in visual surveillance in dynamic scenes. The main contributions of this paper are as follows.\n\n\u2022 Low-level vision, intermediate-level vision, and high-level vision are discussed in a clearly organized hierarchical manner according to the general framework of visual surveillance. This, we believe, can help readers, especially newcomers to this area, not only to obtain an understanding of the state-of-the-art in visual surveillance, but also to appreciate the major components of a visual surveillance system and their inter-component links. \u2022 Instead of detailed summaries of individual publications, our emphasis is on discussing various methods for different tasks involved in a general visual surveillance system. Each issue is accordingly divided into subprocesses or categories of various methods to examine the state of the art. Only the principles of each group of methods are described. The merits and demerits of a variety of different algorithms, especially for motion detection and tracking, are summarized. \u2022 We give a detailed review of the state of the art in personal identification at a distance and fusion of data from multiple cameras. \u2022 We provide detailed discussions on future research directions in visual surveillance, e.g., occlusion handling, combination of two-dimensional (2-D) tracking and 3-D tracking, combination of motion analysis and biometrics, anomaly detection and behavior prediction, behavior understanding and nature language description, content-based retrieval of surveillance videos, fusion of information from multiple sensors, and remote surveillance.\n\nThe remainder of this paper is organized as follows. Section II reviews the work related to motion detection including modeling of environments, segmentation of motion, classification of moving objects. Section III discusses tracking of objects, and Section IV details understanding and description of behaviors. Sections V and VI cover, respectively, personal identification at a distance and fusion of data from multiple cameras. Section VII analyzes some possible directions for future research. The last section summarizes the paper.\n\n\nII. MOTION DETECTION\n\nNearly every visual surveillance system starts with motion detection. Motion detection aims at segmenting regions corresponding to moving objects from the rest of an image. Subsequent processes such as tracking and behavior recognition are greatly dependent on it. The process of motion detection usually involves environment modeling, motion segmentation, and object classification, which intersect each other during processing.\n\n\nA. Environment Modeling\n\nThe active construction and updating of environmental models are indispensable to visual surveillance. Environmental models can be classified into 2-D models in the image plane and 3-D models in real world coordinates. Due to their simplicity, 2-D models have more applications.\n\n\u2022 For fixed cameras, the key problem is to automatically recover and update background images from a dynamic sequence. Unfavorable factors, such as illumination variance, shadows and shaking branches, bring many difficulties to the acquirement and updating of background images. There are many algorithms for resolving these problems including temporal average of an image sequence [15], [82], adaptive Gaussian estimation [70], and parameter estimation based on pixel processes [79], [80], etc.\n\nRidder et al. [81] model each pixel value with a Kalman Filter to compensate for illumination variance. Stauffer et al. [12], [80] present a theoretic framework for recovering and updating background images based on a process in which a mixed Gaussian model is used for each pixel value and online estimation is used to update background images in order to adapt to illumination variance and disturbance in backgrounds. Toyama et al. [83] propose the Wallflower algorithm in which background maintenance and background subtraction are carried out at three levels: the pixel level, the region level, and the frame level. Haritaoglu et al. [4] build a statistical model by representing each pixel with three values: its minimum and maximum intensity values, and the maximum intensity difference between consecutive frames observed during the training period.\n\nThese three values are updated periodically. McKenna et al. [11] use an adaptive background model with color and gradient information to reduce the influences of shadows and unreliable color cues. \u2022 For pure translation (PT) cameras, an environment model can be made by patching up a panorama graph to acquire a holistic background image [84]. Homography matrices can be used to describe the transformation relationship between different images. \u2022 For mobile cameras, motion compensation is needed to construct temporary background images [85]. Regarding 3-D environmental models [86], current work is still limited to indoor scenes because of the difficulty of 3-D reconstructions of outdoor scenes.\n\n\nB. Motion Segmentation\n\nMotion segmentation in image sequences aims at detecting regions corresponding to moving objects such as vehicles and humans. Detecting moving regions provides a focus of attention for later processes such as tracking and behavior analysis because only these regions need be considered in the later processes. At present, most segmentation methods use either temporal or spatial information in the image sequence. Several conventional approaches for motion segmentation are outlined in the following. 1) Background subtraction. Background subtraction is a popular method for motion segmentation, especially under those situations with a relatively static background. It detects moving regions in an image by taking the difference between the current image and the reference background image in a pixel-by-pixel fashion. It is simple, but extremely sensitive to changes in dynamic scenes derived from lighting and extraneous events etc. Therefore, it is highly dependent on a good background model to reduce the influence of these changes [4], [11], [12], as part of environment modeling. 2) Temporal differencing. Temporal differencing makes use of the pixel-wise differences between two or three consecutive frames in an image sequence to extract moving regions. Temporal differencing is very adaptive to dynamic environments, but generally does a poor job of extracting all the relevant pixels, e.g., there may be holes left inside moving entities. As an example of this method, Lipton et al. [10] detect moving targets in real video streams using temporal differencing. After the absolute difference between the current and the previous frame is obtained, a threshold function is used to determine changes. By using a connected component analysis, the extracted moving sections are clustered into motion regions. An improved version uses three-frame instead of two-frame differencing. 3) Optical flow. Optical-flow-based motion segmentation uses characteristics of flow vectors of moving objects over time to detect moving regions in an image sequence. For example, Meyer et al. [13], [21] compute the displacement vector field to initialize a contour based tracking algorithm, called active rays, for the extraction of articulated objects. The results are used for gait analysis. Optical-flow-based methods can be used to detect independently moving objects even in the presence of camera motion. However, most flow computation methods are computationally complex and very sensitive to noise, and cannot be applied to video streams in real time without specialized hardware. More detailed discussion of optical flow can be found in Barron's work [14].\n\nOf course, besides the basic methods described above, there are some other approaches for motion segmentation. Using the extended expectation maximization (EM) algorithm, Friedman et al. [15] implement a mixed Gaussian classification model for each pixel. This model classifies the pixel values into three separate predetermined distributions corresponding to background, foreground and shadow. It also updates the mixed component automatically for each class according to the likelihood of membership. Hence, slowly moving objects are handled perfectly, while shadows are eliminated much more effectively. VSAM [3] has successfully developed a hybrid algorithm for motion segmentation by combining an adaptive background subtraction algorithm with a three-frame differencing technique. This hybrid algorithm is very fast and surprisingly effective for detecting moving objects in image sequences. In addition, Stringa [16] proposes a novel morphological algorithm for detecting motion in scenes. This algorithm obtains stable segmentation results even under varying environmental conditions.\n\n\nC. Object Classification\n\nDifferent moving regions may correspond to different moving targets in natural scenes. For instance, the image sequences captured by surveillance cameras mounted in road traffic scenes probably include humans, vehicles and other moving objects such as flying birds and moving clouds, etc. To further track objects and analyze their behaviors, it is essential to correctly classify moving objects. Object classification can be considered as a standard pattern recognition issue. At present, there are two main categories of approaches for classifying moving objects.\n\n1) Shape-based classification. Different descriptions of shape information of motion regions such as points, boxes, silhouettes and blobs are available for classifying moving objects. VASM [3] takes image blob dispersedness, image blob area, apparent aspect ratio of the blob bounding box, etc, as key features, and classifies moving-object blobs into four classes: single human, vehicles, human groups, and clutter, using a viewpoint-specific three-layer neural network classifier. Lipton et al. [10] use the dispersedness and area of image blobs as classification metrics to classify all moving-object blobs into humans, vehicles and clutter. Temporal consistency constraints are considered so as to make classification results more precise. Kuno et al. [17] use simple shape parameters of human silhouette patterns to separate humans from other moving objects. 2) Motion-based classification. In general, nonrigid articulated human motion shows a periodic property, so this has been used as a strong cue for classification of moving objects. Cutler et al. [18] describe a similarity-based technique to detect and analyze periodic motion. By tracking an interesting moving object, its self-similarity is computed as it evolves over time. As we know, for periodic motion, its self-similarity measure is also periodic. Therefore time-frequency analysis is applied to detect and characterize the periodic motion, and tracking and classification of moving objects are implemented using periodicity. In Lipton's work [19], residual flow is used to analyze rigidity and periodicity of moving objects. It is expected that rigid objects present little residual flow, whereas a nonrigid moving object such as a human being has a higher average residual flow and even display a periodic component. Based on this useful cue, human motion is distinguished from motion of other objects, such as vehicles.\n\nThe two common approaches mentioned above, namely shape-based and motion-based classification, can also be effectively combined for classification of moving objects. Furthermore, Stauffer [20] proposes a novel method based on a time co-occurrence matrix to hierarchically classify both objects and behaviors. It is expected that more precise classification results can be obtained by using extra features such as color and velocity.\n\n\nIII. OBJECT TRACKING\n\nAfter motion detection, surveillance systems generally track moving objects from one frame to another in an image sequence. The tracking algorithms usually have considerable intersection with motion detection during processing. Tracking over time typically involves matching objects in consecutive frames using features such as points, lines or blobs. Useful mathematical tools for tracking include the Kalman filter, the Condensation algorithm, the dynamic Bayesian network, the geodesic method, etc. Tracking methods are divided into four major categories: region-based tracking, active-contour-based tracking, featurebased tracking, and model-based tracking. It should be pointed out that this classification is not absolute in that algorithms from different categories can be integrated together [169].\n\n\nA. Region-Based Tracking\n\nRegion-based tracking algorithms track objects according to variations of the image regions corresponding to the moving objects. For these algorithms, the background image is maintained dynamically [90], [91], and motion regions are usually detected by subtracting the background from the current image. Wren et al. [8] explore the use of small blob features to track a single human in an indoor environment. In their work, a human body is considered as a combination of some blobs respectively representing various body parts such as head, torso and the four limbs. Meanwhile, both human body and background scene are modeled with Gaussian distributions of pixel values. Finally, the pixels belonging to the human body are assigned to the different body part's blobs using the log-likelihood measure. Therefore, by tracking each small blob, the moving human is successfully tracked. Recently, McKenna et al. [11] propose an adaptive background subtraction method in which color and gradient information are combined to cope with shadows and unreliable color cues in motion segmentation. Tracking is then performed at three levels of abstraction: regions, people, and groups. Each region has a bounding box and regions can merge and split. A human is composed of one or more regions grouped together under the condition of geometric structure constraints on the human body, and a human group consists of one or more people grouped together. Therefore, using the region tracker and the individual color appearance model, perfect tracking of multiple people is achieved, even during occlusion. As far as region-based vehicle tracking is concerned, there are some typical systems such as the CMS mobilizer system supported by the Federal Highway Administration (FHWA), at the Jet Propulsion Laboratory (JPL) [92], and the PATH system developed by the Berkeley group [93].\n\nAlthough they work well in scenes containing only a few objects (such as highways), region-based tracking algorithms cannot reliably handle occlusion between objects. Furthermore, as these algorithms only obtain the tracking results at the region level and are essentially procedures for motion detection, the outline or 3-D pose of objects cannot be acquired. (The 3-D pose of an object consists of the position and orientation of the object). Accordingly, these algorithms cannot satisfy the requirements for surveillance against a cluttered background or with multiple moving objects.\n\n\nB. Active Contour-Based Tracking\n\nActive contour-based tracking algorithms track objects by representing their outlines as bounding contours and updating these contours dynamically in successive frames [6], [71], [72], [74]. These algorithms aim at directly extracting shapes of subjects and provide more effective descriptions of objects than region-based algorithms. Paragios et al. [30] detect and track multiple moving objects in image sequences using a geodesic active contour objective function and a level set formulation scheme. Peterfreund [31] explores a new active contour model based on a Kalman filter for tracking nonrigid moving targets such as people in spatio-velocity space. Isard et al. [32] adopt stochastic differential equations to describe complex motion models, and combine this approach with deformable templates to cope with people tracking. Malik et al. [82], [94] have successfully applied active contour-based methods to vehicle tracking.\n\nIn contrast to region-based tracking algorithms, active contour-based algorithms describe objects more simply and more effectively and reduce computational complexity. Even under disturbance or partial occlusion, these algorithms may track objects continuously. However, the tracking precision is limited at the contour level. The recovery of the 3-D pose of an object from its contour on the image plane is a demanding problem. A further difficulty is that the active contour-based algorithms are highly sensitive to the initialization of tracking, making it difficult to start tracking automatically.\n\n\nC. Feature-Based Tracking\n\nFeature-based tracking algorithms perform recognition and tracking of objects by extracting elements, clustering them into higher level features and then matching the features between images. Feature-based tracking algorithms can further be classified into three subcategories according to the nature of selected features: global feature-based algorithms, local feature-based algorithms, and dependence-graph-based algorithms.\n\n\u2022 The features used in global feature-based algorithms include centroids, perimeters, areas, some orders of quadratures and colors [100], [101], etc. Polana et al. [33] provide a good example of global feature-based tracking. A person is bounded with a rectangular box whose centroid is selected as the feature for tracking. Even when occlusion happens between two persons during tracking, as long as the velocity of the centroids can be distinguished effectively, tracking is still successful. \u2022 The features used in local feature-based algorithms include line segments, curve segments, and corner vertices [98], [99], etc.\n\n\u2022 The features used in dependence-graph-based algorithms include a variety of distances and geometric relations between features [97]. Theabovethreemethodscanbecombined.Intherecentworkof Jang et al. [34], an active template that characterizes regional and structuralfeaturesofanobjectisbuiltdynamicallybasedontheinformation of shape, texture, color, and edge features of the region. UsingmotionestimationbasedonaKalmanfilter,thetrackingofa nonrigid moving object is successfully performed by minimizing a feature energy function during the matching process.\n\nIn general, as they operate on 2-D image planes, feature-based tracking algorithms can adapt successfully and rapidly to allow real-time processing and tracking of multiple objects which are required in heavy thruway scenes, etc. However, dependencegraph-based algorithms cannot be used in real-time tracking because they need time-consuming searching and matching of graphs. Feature-based tracking algorithms can handle partial occlusion by using information on object motion, local features and dependence graphs. However, there are several serious deficiencies in feature-based tracking algorithms.\n\n\u2022 The recognition rate of objects based on 2-D image features is low, because of the nonlinear distortion during perspective projection and the image variations with the viewpoint's movement. \u2022 These algorithms are generally unable to recover 3-D pose of objects. \u2022 The stability of dealing effectively with occlusion, overlapping and interference of unrelated structures is generally poor.\n\n\nD. Model-Based Tracking\n\nModel-based tracking algorithms track objects by matching projected object models, produced with prior knowledge, to image data. The models are usually constructed off-line with manual measurement, CAD tools or computer vision techniques. As model-based rigid object tracking and model-based nonrigid object tracking are quite different, we review separately model-based human body tracking (nonrigid object tracking) and model-based vehicle tracking (rigid object tracking).\n\n\n1) Model-Based Human Body Tracking:\n\nThe general approach for model-based human body tracing is known as analysis-by-synthesis, and it is used in a predict-match-update style. Firstly, the pose of the model for the next frame is predicted according to prior knowledge and tracking history. Then, the predicted model is synthesized and projected into the image plane for comparison with the image data. A specific pose evaluation function is needed to measure the similarity between the projected model and the image data. According to different search strategies, this is done either recursively or using sampling techniques until the correct pose is finally found and is used to update the model. Pose estimation in the first frame needs to be handled specially. Generally, model-based human body tracking involves three main issues:\n\n\u2022 construction of human body models; \u2022 representation of prior knowledge of motion models and motion constraints; \u2022 prediction and search strategies.\n\nPrevious work on these three issues is briefly and respectively reviewed as follows.\n\na) Human body models: Construction of human body models is the base of model-based human body tracking [24]. Generally, the more complex a human body model, the more accurate the tracking results, but the more expensive the computation. Traditionally, the geometric structure of human body can be represented in the following four styles.\n\n\u2022 Stick figure. The essence of human motion is typically contained in the movements of the torso, the head and the four limbs, so the stick-figure method is to represent the parts of a human body as sticks and link the sticks with joints. Karaulova et al. [25] use a stick figure representation to build a novel hierarchical model of human dynamics encoded using hidden Markov models (HMMs), and realize view-independent tracking of a human body in monocular image sequences. \u2022 2-D contour. This kind of human body model is directly relevant to human body projections in an image plane. The human body segments are modeled by 2-D ribbons or blobs. For instance, Ju et al. [26] propose a cardboard human body model, in which the human limbs are represented by a set of jointed planar ribbons. The parameterized image motion of these patches is constrained to enforce the articulated movement of human limbs. Niyogi et al. [27] use the spatial-temporal pattern in XYT space to track, analyze and recognize walking figures. They examine the characteristic braided pattern produced by the lower limbs of a walking human, the projections of head movements are then located in the spatio-temporal domain, followed by the identification of the joint trajectories; The contour of a walking figure is outlined by utilizing these joint trajectories, and a more accurate gait analysis is carried out using the outlined 2-D contour for the recognition of the specific human. \u2022 Volumetric models. The main disadvantage of 2-D models is that they require restrictions on the viewing angle. To overcome this disadvantage, many researchers use 3-D volumetric models such as elliptical cylinders, cones [102], [103], spheres, super-quadrics [104], etc. Volumetric models require more parameters than image-based models and lead to more expensive computation during the matching process. Rohr [28]  b) Motion models: Motion models of human limbs and joints are widely used in tracking. They are effective because the movements of the limbs are strongly constrained. These motion models serve as prior knowledge to predict motion parameters [106], [107], to interpret and recognize human behaviors [108], or to constrain the estimation of low-level image measurements [109]. For instance, Bregler [108] decomposes a human behavior into multiple abstractions, and represents the high-level abstraction by HMMs built from phases of simple movements. This representation is used for both tracking and recognition. Zhao et al. [106] learn a highly structured motion model for ballet dancing under the minimum description length (MDL) paradigm. This motion model is similar to a finite-state machine (FSM). The multivariate principal component analysis (MPCA) is used to train a walking model in Sidenbladh et al.'s work [109]. Similarly, Ong et al. [110] employ the hierarchical PCA to learn their motion model which is based on the matrices of transition probabilities between different subspaces in a global eigensapce and the matrix of transition probabilities between global eigenspaces. Ning et al. [7] learn a motion model from semi-automatically acquired training examples and represent it using Gaussian distributions. c) Search strategies: Pose estimation in a high-dimensional body configuration space is intrinsically difficult, so, search strategies are often carefully designed to reduce the solution space. Generally, there are four main classes of search strategies: dynamics, Taylor models, Kalman filtering, and stochastic sampling. Dynamical strategies use physical forces applied to each rigid part of the 3-D model of the tracked object. These forces, as heuristic information, guide the minimization of the difference between the pose of the 3-D model and the pose of the real object [102]. The strategy based on the Taylor models incrementally improves an existing estimation, using differentials of motion parameters with respect to the observation to predict better search directions [112]. It at least finds local minima, but cannot guarantee finding the global minimum. As a recursive linear estimator, Kalman filtering can thoroughly deal with the tracking of shape and position over time in the relatively clutter-free case in which the density of the motion parameters can be modeled satisfactorily as Gaussian [29], [114]. To handle clutter that causes the probability density function for motion parameters to be multimodal and non-Gaussian, stochastic sampling strategies, such as Markov Chain Monte Carlo [115], Genetic Algorithms, and CONDENSATION [116], [117], are designed to represent simultaneous alternative hypotheses. Among the stochastic sampling strategies in visual tracking, CONDENSATION is perhaps the most popular.\n\n2) Model-Based Vehicle Tracking: As to model-based vehicle tracking, 3-D wire-frame vehicle models are mainly used [95]. The research groups at the University of Reading [87], [88], the National Laboratory of Pattern Recognition (NLPR) [111], [174] and the University of Karlsruhe [124]- [126] have made important contributes to 3-D model-based vehicle localization and tracking.\n\nThe research group at the University of Reading adopts 3-D wire-frame vehicle models. Tan et al. [87], [119] propose the ground-plane constraint (GPC), under which vehicles are restricted to move on the ground plane. Thus the degrees of freedom of vehicle pose are reduced to three from six. This greatly decreases the computational cost of searching for the optimal pose. Moreover, under the weak perspective assumption, the pose parameters are decomposed into two independent sets: translation parameters and rotation parameters. Tan et al. [120] propose a generalized Hough transformation algorithm based on a single characteristic line segment matching to estimate vehicle pose. Further, Tan et al. [121] analyze the one-dimensional (1-D) correlation of image gradients and determine the vehicle pose by voting. As to the refinement of the vehicle pose, the research group in the University of Reading have utilized an independent 1-D searching method [121] in their past work. Recently, Pece et al. [122], [123] introduce a statistical Newton method for estimating the vehicle pose.\n\nThe NLPR group has extended the work of the research group at the University of Reading. Yang et al. [111] propose a new 3-D model-based vehicle localization algorithm, in which the edge points in the image are directly used as features, and the degree of matching between the edge points and the projected model is measured by a pose evaluation function. Lou et al. [174] present an algorithm for vehicle tracking based on an improved extended Kalman filter. In the algorithm, the turn of the steering wheel and the distance between the front and rear wheels are taken into account. As there is a direct link between the behavior of the driver who controls the motion of the vehicle and the assumed dynamic model, the improved extended Kalman filter outperforms the traditional extended Kalman filter when the vehicle carries out a complicated maneuver.\n\nThe Karlsruhe group [124]  The above reviews model-based human body tracking and model-based vehicle tracking. Compared with other tracking algorithms, model-based tracking algorithms have the following main advantages.\n\n\u2022 By making use of the prior knowledge of the 3-D contours or surfaces of objects, the algorithms are intrinsically robust. The algorithms can obtain better results even under occlusion (including self-occlusion for humans) or interference between nearby image motions. \u2022 As far as model-based human body tracking is concerned, the structure of human body, the constraint of human motion, and other prior knowledge can be fused.\n\n\u2022 As far as 3-D model-based tracking is concerned, after setting up the geometric correspondence between 2-D image coordinates and 3-D world coordinates by camera calibration, the algorithms naturally acquire the 3-D pose of objects. \u2022 The 3-D model-based tracking algorithms can be applied even when objects greatly change their orientations during the motion. Ineluctably, model-based tracking algorithms have some disadvantages such as the necessity of constructing the models, high computational cost, etc.\n\n\nIV. UNDERSTANDING AND DESCRIPTION OF BEHAVIORS\n\nAfter successfully tracking the moving objects from one frame to another in an image sequence, the problem of understanding-object behaviors from image sequences follows naturally. Behavior understanding involves the analysis and recognition of motion patterns, and the production of high-level description of actions and interactions.\n\n\nA. Behavior Understanding\n\nUnderstanding of behaviors may simply be thought as the classification of time varying feature data, i.e., matching an unknown test sequence with a group of labeled reference sequences representing typical behaviors. It is then obvious that a fundamental problem of behavior understanding is to learn the reference behavior sequences from training samples, and to devise both training and matching methods for coping effectively with small variations of the feature data within each class of motion patterns. Some efforts have been made in this direction [176] and the major existing methods for behavior understanding are outlined in the following. a) Dynamic time warping (DTW): DTW is a template-based dynamic programming matching technique widely used in the algorithms for speech recognition. It has the advantage of conceptual simplicity and robust performance, and has been used recently in the matching of human movement patterns [127], [128]. For instance, Bobick et al. [128] use DTW to match a test sequence to a deterministic sequence of states to recognize human gestures. Even if the time scale between a test sequence and a reference sequence is inconsistent, DTW can still successfully establish matching as long as the time ordering constraints hold.\n\n\nb) Finite-state machine (FSM):\n\nThe most important feature of a FSM is its state-transition function. The states are used to decide which reference sequence matches with the test sequence. Wilson et al. [129] analyze the explicit structure of natural gestures where the structure is implemented by an equivalent of a FSM but with no learning involved. State-machine representations of behaviors have also been employed in higher level description. For instance, Bremond et al. [131] use handcrafted deterministic automata to recognize airborne surveillance scenarios describing vehicle behaviors in aerial imagery. c) HMMs: A HMM is a kind of stochastic state machines [35]. It allows a more sophisticated analysis of data with spatio-temporal variability. The use of HMMs consists of two stages: training and classification. In the training stage, the number of states of a HMM must be specified, and the corresponding state transition and output probabilities are optimized in order that the generated symbols can correspond to the observed image features of the examples within a specific movement class. In the matching stage, the probability with which a particular HMM generates the test symbol sequence corresponding to the observed image features is computed. HMMs generally outperform DTW for undivided time series data, and are therefore extensively applied to behavior understanding. For instance, Starner et al. [132] propose HMMs for the recognition of sign language. Oliver et al. [133] propose and compare two different state-based learning architectures, namely, HMMs and coupled hidden Markov models (CHMMs) for modeling people behaviors and interactions such as following and meeting. The CHMMs are shown to work much more efficiently and accurately than HMMs. Brand et al. [134] show that, by the use of the entropy of the joint distribution to learn the HMM, a HMM's internal state machine can be made to organize observed behaviors into meaningful states. This technique has found applications in video monitoring and annotation, in low bit-rate coding of scene behaviors, and in anomaly detection.\n\n\nd) Time-delay neural network (TDNN):\n\nTDNN is also an interesting approach to analyzing time-varying data. In TDNN, delay units are added to a general static network, and some of the preceding values in a time-varying sequence are used to predict the next value. As larger data sets become available, more emphasis is being placed on neural networks for representing temporal information. TDNN has been successfully applied to hand gesture recognition [135] and lip-reading [136]. e) Syntactic techniques [137]: The syntactic approach in machine vision has been studied mostly in the context of pattern recognition in static images. Recently the grammatical approach has been used for visual behavior recognition. Brand [138] uses a simple nonprobabilistic grammar to recognize sequences of discrete behaviors. Ivanov et al. [137] describe a probabilistic syntactic approach to the detection and recognition of temporally extended behaviors and interactions between multiple agents. The fundamental idea is to divide the recognition problem into two levels. The lower level is performed using standard independent probabilistic temporal behavior detectors, such as HMMs, to output possible low-level temporal features. These outputs provide the input stream for a stochastic context-free parser. The grammar and parser provide longer range temporal constraints, disambiguate uncertain low-level detection, and allow the inclusion of a priori knowledge about the structure of temporal behaviors in a given domain.\n\nf) Non-deterministic finite automaton (NFA): Wada et al. [139] employ NFA as a sequence analyzer, because it is a simple example satisfying the following properties: instantaneousness and pure-nondeterminism. They present an approach for multiobject behavior recognition based on behavior driven selective attention. g) Self-organizing neural network: The methods discussed in (a)-(f) all involve supervised learning. They are applicable for known scenes where the types of object motions are already known. The self-organizing neural networks are suited to behavior understanding when the object motions are unrestricted. Johnson et al. [140] describe the movement of an object in terms of a sequence of flow vectors, each of which consists of 4 components representing the positions and velocities of the object in the image plane. A statistical model of object trajectories is formed with two competitive learning networks that are connected with leaky neurons. Sumpter et al. [141] introduce feedback to the second competitive network in [140] giving a more efficient prediction of object behaviors. Hu et al. [175] improve the work in [140] by introducing a new neural network structure that has smaller scale and faster learning speed, and is thus more effective. Owens et al. [142] apply the Kohonen self-organizing feature map to find the flow vector distribution patterns. These patterns are used to determine whether a point on a trajectory is normal or abnormal.\n\n\nB. Natural Language Description of Behaviors\n\nIn many applications it is important to describe object behaviors in natural language suitable for nonspecialist operator of visual surveillance [22], [147]. For example, Herzog et al. [143] have developed the VITRA project that uses natural language to describe visual scenes. In 1995, MIT [147] convened a workshop to discuss how to link natural language and computer vision. Generally, there are two main categories of behavior description methods: statistical models and formalized reasoning. a) Statistical models: A representative statistical model is the Bayesian network model [144], [145]. This model interprets certain events and behaviors by analysis of time sequences and statistical modeling. For example, Remagnino et al. [148] describe interactions between objects using a two-layer agent-based Bayesian network. These methods rest on lower-level recognition based on motion concepts, and do not yet involve high-level concepts, such as events and scenarios, and the relationships between these concepts. These concepts need high-level reasoning based on a large amount of prior knowledge. b) Formalized reasoning: Formalized reasoning [146] uses symbol systems to represent behavior patterns, and reasoning methods such as predication logic to recognize and classify events. Recently, Kojima et al. [36], [37] propose a new method for generating natural language descriptions of human behaviors appearing in real image sequences. First, a head region of a human is extracted from each image frame, and the 3-D pose and position of the head are estimated using a model-based approach. Next, the head motion trajectory is divided into the segments of monotonous movement. The conceptual features for each segment, such as degrees of changes of pose and position and the relative distances from other objects in the surroundings, are evaluated. Meanwhile, the most suitable verbs and other syntactic elements are selected. Finally, the natural language text for interpreting human behaviors is generated by machine translation technology. Kollnig et al. [118] use fuzzy membership functions to associate verbs with quantitative details obtained by automatic image sequence analysis for generating natural language descriptions of a traffic scene. In their scheme, each occurrence is defined by three predicates: a precondition, monotonicity condition and post-condition. The most significant disadvantage of the formalized reasoning methods is that they cannot handle uncertainty of events [96].\n\nAlthough there is some progress in description of behaviors, some key problems remain open, for example how to properly represent semantic concepts, how to map motion characteristics to semantic concepts and how to choose efficient representations to interpret the scene meanings.\n\n\nV. PERSONAL IDENTIFICATION FOR VISUAL SURVEILLANCE\n\nThe problem of \"who is now entering the area under surveillance\" is of increasing importance for visual surveillance. Such personal identification can be treated as a special behavior-understanding problem. Human face and gait are now regarded as the main biometric features that can be used for personal identification in visual surveillance systems [2]. In recent years, great progress in face recognition [46]- [50] has been achieved. The main steps in the face recognition for visual surveillance are face detection, face tracking, face feature detection and face recognition [51]- [55]. Usually, these steps are studied separately. Therefore, developing an integrated face recognition system involving all of the above steps is critical for visual surveillance. As the length of this paper is restricted, we review here only recent researches on the major existing methods for gait recognition.\n\n\nA. Model-Based Methods\n\nIn model-based methods, parameters, such as joint trajectories, limb lengths, and angular speeds, are measured [156]- [162], [180], [181].\n\nCunado et al. [156], [157] model gait as the movement of an articulated pendulum and use the dynamic Hough transform [158] to extract the lines representing the thigh in each frame. The least squares method is used to smooth the inclination data of the thigh and to fill the missing points caused by self-occlusion of the legs. Phase-weighted magnitude spectra are used as gait features for recognition.\n\nYam et al. [159], [160] propose a new model-based gait recognition algorithm. Biomechanical models of walking and running are used to form a type of new anatomical model called a dynamically coupled oscillator, for the hip motion, and the structure and motion of the thigh and the lower leg. Temporal template matching [161] is used to extract the rotation angles of the thigh and the lower legs. Then gait signatures are obtained from the lower-order phase-weighted magnitude spectra.\n\nAnother recent paper [162] uses dynamic features from trajectories of lower-body joint angles such as the hip and the knee to recognize individuals. This work first projects the 3-D positions of markers attached to the body into the walking plane. Then a simple method is applied to estimate the planar offsets between the markers and the underlying skeleton or joints. Finally, given these offsets, the trajectories of joint angles are computed.\n\nTracking and localizing the human body accurately in 3-D space is still difficult despite the recent work on structure-based methods. In theory, joint angles are sufficient for recognition of people by their gait. However, accurately recovering joint angles from a walking video is still an unsolved or not well-solved problem. In addition, the computational cost of the model-based approaches is quite high.\n\n\nB. Statistical Methods\n\nStatistical recognition techniques usually characterize the statistical description of motion image sets, and have been well developed in automatic gait recognition [60], [163]- [168], [182], [184].\n\nMurase et al. [163] use a parametric eigenspace representation to reduce computational cost and to improve the robustness of gait estimation. Huang et al. [164]- [166] have successfully extended Murase et al.'s work by adding canonical space analysis to obtain better discrimination. The eigenspace transformation (EST) has the advantage of reducing the dimensionality, but it cannot optimize class discrimination. Therefore, Huang et al. [165] describe an integrated gait recognition system using EST and canonical space analysis (CSA).\n\nShutler et al. [167] develop a velocity-moment-based method for describing the object motion in image sequences. Similarly, Lee et al. [60], [168] use the moment features of image regions to recognize individuals. Assuming that people walk frontalparallel toward a fixed camera, the silhouette region is divided into seven subregions. A set of moment-based region features is used to recognize people and to predict the gender of an unknown person by his/her walking appearance.\n\nStatistical methods are relatively robust to noise and change of time interval in input image sequences. Compared with model-based approaches, the computational cost of statistical methods is low.\n\n\nC. Physical-Parameter-Based Methods\n\nPhysical-parameter-based methods make use of geometric structural properties of a human body to characterize a person's gait pattern. The parameters used include height, weight, stride cadence and length, etc. [56], [170]- [173], [183].\n\nFor example, a gait recognition technique using specific behavior parameters is recently proposed by Bobick et al. [170], [171]. This method does not directly analyze the dynamics of gait patterns, but uses walking activities to recover the static body parameters of walking such as the vertical distance between head and feet, the distance between head and pelvis, the distance between feet and pelvis, and the distance between the left and right feet. The method is assessed using an expected confusion metric [172] to predict how well a given feature vector can identify an individual in a large population. Some recent work [56], [173] also uses human stature, stride length and cadence as the input features for parametric gait classification. Given the calibration parameters of the camera and the walking plane, the method uses the walking periodicity to accurately estimate cadence and stride [56].\n\nPhysical-parameter-based methods are intuitively understandable, and independent of viewing angles because these parameters usually are recovered in the 3-D space. However, they depend greatly on the vision techniques used to recover the required parameters, e.g., body-part labeling, depth compensation, camera calibration, shadow removal, etc. In addition, the parameters used for recognition may be not effective enough across a large population.\n\n\nD. Spatio-Temporal Motion-Based Methods\n\nFor motion recognition based on spatio-temporal analysis, the action or motion is characterized via the entire 3-D spatio-temporal data volume spanned by the moving person in the image sequence. These methods generally consider motion as a whole to characterize its spatio-temporal distributions [27], [58], [59], [61], [62], [177], [178], [185].\n\nPerhaps the earliest approach to recognizing people is to obtain gait features from the spatio-temporal pattern of a walking figure [27]. In translation and time (XT) space, the motions of the head and legs have different patterns. These patterns are first processed to determine the bounding box of a moving body, and then fitted to a five-stick model. Gait signatures could be acquired from the velocity-normalized fitted model. Later, Niyogi et al. [58] extend their own work by using the spatio-temporal surface to analyze gait. After motion detection, the XYT pattern (2-D space and 1-D time) is fitted with a smooth spatio-temporal surface. This surface is represented as a combination of a standard parametric surface and a difference surface that can be used to recognize some simple actions.\n\nUsing the image self-similarity in XYT, BenAbdelkader et al. [59], [177] propose a motion-based gait-recognition technique. The similarity plots (SPs) of the image sequence of a moving object are projections of its planar dynamics [61]. Hence, these SPs include much information of gait motion.\n\nKale et al. [62] propose a HMM-based method for representing and recognizing gait. First, a set of key frames that occur during a walk cycle is chosen. The widths of the walking figure's binary silhouettes, in such a set of key frames, are chosen as the input features. Then, a low-dimensional measurement vector is produced using the Euclidean distance between a given image and the set of key frames. These measurement vector sequences are used to train the HMMs.\n\nSpatio-temporal motion-based methods are able to better capture both spatial and temporal information of gait motion. Their advantage is low computational complexity and a simple implementation. However, they are susceptible to noise and to variations of the timings of movements.\n\n\nE. Fusion of Gait With Other Biometrics\n\nThe fusion of gait information with other biometrics can further increase recognition robustness and reliability. Shakhnarovich et al. [64] develop a view-normalized method for solving the problem of integrated face and gait recognition from multiple views. For optimal face recognition, they set a virtual camera to capture the frontal face. For gait recognition, they set a virtual camera to capture the side-view walking sequence. Results show that the integrated face and gait recognition outperforms recognition which only uses a single mode. In extended work, Shakhnarovich et al. [65] evaluate the recognition performances of several different probabilistic combinations for fusing view-normalized face and gait.\n\nAlthough many researchers have been working on gait recognition, current research of gait recognition is still in its infancy.\n\nFirst, most experiments are carried out under constrained circumstances, e.g., no occlusion happens while objects are usually moving, the background is simple, etc. Second, existing algorithms are evaluated on small databases. Future work on gait recognition will focus on handling these two problems.\n\n\nVI. FUSION OF DATA FROM MULTIPLE CAMERAS\n\nMotion detection, tracking, behavior understanding, and personal identification at a distance discussed above can be realized by single camera-based visual surveillance systems. Multiple camera-based visual surveillance systems can be extremely helpful because the surveillance area is expanded and multiple view information can overcome occlusion. Tracking with a single camera easily generates ambiguity due to occlusion or depth. This ambiguity may be eliminated from another view. However, visual surveillance using multicameras also brings problems such as camera installation (how to cover the entire scene with the minimum number of cameras), camera calibration, object matching, automated camera switching, and data fusion.\n\n\nA. Installation\n\nThe deployment of the cameras has a great influence on the real-time performance and the cost of the system. Cameras cannot be employed arbitrarily due to factors such as the topography of the area. Redundant cameras increase not only processing time and algorithmic complexity, but also the installation cost. In contrast, a lack of cameras may cause some blind angles, which reduce the reliability of a surveillance system. So the question of how to cover the entire scene with the minimum number of cameras is important. Pavlidis et al. [149] provide an optimum algorithm for solving the problem of multiple-camera installation in parking lots. The basic idea is to place camera 1 on a certain position at first, then to search the rest of the space to place the second camera at a point where there is a 25%-50% overlap region between the fields of view of camera 1 and camera 2. The other cameras are added one by one, subject to the constraint that the field of view of each new camera should have a 25%-50% overlap with the combined fields of view of all the previous cameras.\n\n\nB. Calibration\n\nTraditional calibration methods use the 3-D coordinates and the image coordinates of some known points to compute the parameters of a camera. Calibration is more complex when multiple cameras are concerned. Current multiple camera self-calibration methods use temporal information. Stein and Lee et al. [38], [39] use the motion trajectory and the ground plane constraint to determine the projection transformation matrix, and then such matrix is decomposed to obtain the extrinsic parameters of the camera. However, this method is inaccurate, and cannot be used if there is no ground plane.\n\n\nC. Object Matching\n\nObject matching among multiple cameras involves finding the correspondences between the objects in different image sequences taken by different cameras. There are two popular methods: one is the geometry-based method that establishes correspondence according to geometric features transformed to the same space; and the other is the recognition-based method. As an example of the geometry-based method, Cai et al. [41], [42] use features of location, intensity and geometry to match between images taken by different cameras. As an example of recognition-based methods, Krumm et al. [57] use color histograms to match regions. In general, the methods for object matching need camera calibration. However, some researchers also develop methods without calibration. For example, Javed et al. [150] use the spatial relationships between view fields of cameras to establish the corresponding relationships of images.\n\n\nD. Switching\n\nWhen an object moves out of the view field of an active camera, or the camera cannot give a good view of the moving object, then the system should switch to another camera that may give a better view of the object. The key problems are how to find the better camera and how to minimize the number of switches during tracking. Cai et al. [41], [42] establish a tracking confidence for each object. When the tracking confidence is below a certain threshold, the system begins a global search and selects the camera with the highest tracking confidence as the active camera.\n\n\nE. Data Fusion\n\nData fusion is important for occlusion handling and continuous tracking. Dockstader et al. [151] use a Bayesian network to fuse 2-D state vectors acquired from various image sequences to obtain a 3-D state vector. Collins et al. [152] design an algorithm that obtains an integrated representation of an entire scene by fusing information from every camera into a 3-D geometric coordinate system. Kettnaker et al. [43] synthesize the tracking results of different cameras to obtain an integrated trajectory.\n\n\nF. Occlusion Handling\n\nIn practice, self-occlusion, and occlusions between different moving objects or between moving objects and the background are inevitable. Multiple camera systems offer efficient and promising methods for coping with occlusion. Utsumi et al. [40] utilize multiple cameras to track people, successfully resolving the mutual-occlusion and self-occlusion by choosing the \"best\" view. Dockstader et al. [151] describe a multiple camera surveillance system that is used to track partly occluded people. Tsutsui et al. [153] apply the multiple camera surveillance system to optical flow-based human tracking. When a static object in one camera occludes an object, the system predicts the 3-D coordinate position and moving speed of the occluded object according to information from other cameras. Mittal et al. [154] resolve human tracking in complex scenes using multiple cameras. First, using the Bayesian classification rule, images are segmented according to the human model and the estimated position of each person. Then, data from multiple cameras are fused to estimate the positions of the humans on the ground plane. Finally, a Kalman filter is used for tracking.\n\n\nVII. FUTURE DEVELOPMENTS\n\nIn Sections II-VI, we have reviewed the state-of-the-art of visual surveillance for humans and vehicles sorted by a general framework of visual surveillance systems. Although a large amount of work has been done in visual surveillance for humans and vehicles, many issues are still open and deserve further research, especially in the following areas.\n\n\nA. Occlusion Handling\n\nOcclusion handing is a major problem in visual surveillance. Typically, during occlusion, only portions of each object are visible and often at very low resolution. This problem is generally intractable, and motion segmentation based on background subtraction may become unreliable. To reduce ambiguities due to occlusion, better models need be developed to cope with the correspondence between features and body parts, and thus eliminate correspondence errors that occur during tracking multiple objects. When objects are occluded by fixed objects such as buildings and street lamps, some resolution is possible through motion region analysis and partial matching. However, when multiple moving objects occlude each other, especially when their speeds, directions and shapes are very close, their motion regions coalesce, which makes the location and tracking of objects particularly difficult. The self-occlusion of a human body is also a significant and difficult problem. Interesting progress is being made using statistical methods to predict object pose, position, and so on, from available image information. Perhaps the most promising practical method for addressing occlusion is through the use of multiple cameras.\n\n\nB. Fusion of 2-D and 3-D Tracking\n\nTwo-dimensional tracking is simple and rapid, and it has shown some early successes in visual surveillance, especially for low-resolution application areas where the precise posture reconstruction is not needed, e.g., pedestrian and vehicle tracking in a traffic surveillance setting. However, the major drawback of the 2-D approach is its restriction of the camera angle.\n\nCompared with 2-D approaches, 3-D approaches are more effective for accurate estimation of position in space, more effective handling of occlusion, and high-level judgments about complex object movements such as wandering around, shaking hands, dancing, and vehicle overtaking. However, applying 3-D tracking requires more parameters and more computation during the matching process. Also, vision-based 3-D tracking brings a number of challenges such as the acquisition of object models, occlusion handling, parameterized object modeling, etc.\n\nIn fact, the combination of 2-D tracking and 3-D tracking is a significant research direction that few researches have attempted. This combination is expected to fuse the merits of the 2-D tracking algorithms and those of the 3-D tracking algorithms. The main difficulties of this combination are:\n\n\u2022 deciding when 2-D tracking should be used and when 3-D tracking should be used;\n\n\u2022 how to initialize pose parameters for 3-D tracking according to the results from 2-D tracking, when the tracking algorithm is switched from 2-D to 3-D.\n\n\nC. Three-Dimensional Modeling of Humans and Vehicles\n\nWe think that it is feasible to build 3-D models for humans and vehicles. As far as vehicles are concerned, they can be treated as rigid objects, drawn from only a few classes and with invariable 3-D shapes during normal usage. It is possible to establish 3-D models of vehicles using CAD tools, etc. A generic and parametric model can be established for each class [125], [155]. As far as human beings are concerned, the shapes of human bodies are similar, so it is possible to build a uniform parametric model for human bodies. The parametric models and their applications in tracking and identification are important research directions in visual surveillance. 3-D modeling deserves more attention in future work.\n\n\nD. Combination of Visual Surveillance and Personal Identification\n\nAs mentioned in Section V, vision-based human identification at a distance has become increasingly important. Gait is a most attractive modality used for this purpose. Generally, future work on gait recognition will focus on the following directions.\n\n\n1) Establishing a large common database and a standard\n\ntest protocol. The database with an independent subdatabase for the test just like the FERET protocol [113] is necessary for convincing test. Any realistic database should include the factors affecting gait perception, e.g., clothing, environments, distance, carried objects such as briefcases [179], and viewing angle. Such a database allows one to explore the limitations of the extracted gait signatures as well as the confidence estimation associated with the use of gait to buttress other biometric measures [66]. 2) Combining dynamic features and static features. Gait includes both individual appearances and the dynamics of walking. Developing the underlying static parameters of a human body and the dynamic characteristics of joint angles is helpful to recognition. 3) Developing multiple biometric feature-based systems in which gait is a basic biometric feature. A multiple biometric system either fuses multiple biometric features or automatically switches among different biometric features according to operational conditions. For example, at a distance, gait can be used for recognition; when an individual is near to the camera, the face image provides a powerful cue; at intermediate distances, the information from both face and gait can be fused to improve the recognition accuracy. 4) Obtaining the view-invariant gait signatures from the tracked image sequences [67]. To extract and localize arbitrarily articulated shapes, view-invariant gait signatures from the tracked image sequences need to be obtained in future recognition systems.\n\n\nE. Behavior Understanding\n\nOne of the objectives of visual surveillance is to analyze and interpret individual behaviors and interactions between objects to decide for example whether people are carrying, depositing or exchanging objects, whether people are getting on or getting off a vehicle, or whether a vehicle is overtaking another vehicle, etc. Recently, related research has still focused on some basic problems like recognition of standard gestures and simple behaviors. Some progress has been made in building the statistical models of human behaviors using machine learning. Behavior recognition is complex, as the same behavior may have several different meanings depending upon the scene and task context in which it is performed. This ambiguity is exacerbated when several objects are present in a scene [130]. The following problems within behavior understanding are challenging: statistical learning for modeling behaviors, context-dependent learning from example images, real-time performance required by behavior interpretation, classification and labeling of motion trajectories of tracked objects, automated learning of the priori knowledge [63] implied in object behaviors, visually mediated interaction, and attention mechanisms.\n\n\nF. Anomaly Detection and Behavior Prediction\n\nAnomaly detection and behavior prediction are significant in practice. In applications of visual surveillance, not only should visual surveillance systems detect anomalies such as traffic accidents and car theft etc, according to requirements of functions, but also predict what will happen according to the current situation and raise an alarm for a predicted abnormal behavior. Implementations are usually based on one or other of the following two methods.\n\n\n1) Probability reasoning and prior rules combined methods.\n\nA behavior with small probability, or against the prior rules would be regarded as an anomaly. 2) Behavior-pattern-based methods. Based on learned patterns of behaviors, we can detect anomalies and predict object behaviors. When a detected behavior does not match the learned patterns, it is classed as an anomaly. We can predict an object behavior by matching the observed subbehavior of the object with the learned patterns. Generally, patterns of behaviors in a scene can be constructed by supervised or unsupervised learning of each object's velocities and trajectories, etc. Supervised learning is used for known scenes where objects move in pre-defined ways. For unknown scenes, patterns of behaviors should be constructed by self-organizing and self-learning of image sequences. Fernyhough et al. [5] establish the spatio-temporal region by learning results of tracking objects in a image sequence, and construct a qualitative behavior model by qualitative reasoning and statistical analysis.\n\n\nG. Content-Based Retrieval of Surveillance Videos\n\nThe task in content-based retrieval of surveillance videos is to retrieve video clips from surveillance video databases according to video contents, based on automatic image and video understanding. At present, research on video retrieval focuses on the low-level perceptively meaningful representations of pictorial data (such as color, texture, shape, etc) and simple motion information. These retrieval techniques cannot accurately and effectively search the videos for sequences related to specified behaviors. Semantic-based video retrieval (SBVR) aims to bridge the gap between low-level features and high-level semantic meanings. Based on automatic interpretation of contents in surveillance videos, SBVR may classify and further access the surveillance video clips that are related to specific behaviors, and supply a more high-level, more intuitive and more humanistic retrieval mode. Semantic-based retrieval of surveillance videos brings the following difficult problems: automatic extraction of semantic behavior features, combination between low-level visual features and behavior features, hierarchical organization of image and video features, semantic video indexing, inquire interface, etc.\n\n\nH. Natural Language Description of Object Behaviors\n\nDescribing object behaviors by natural language in accord with human habits is a challenging research subject. The key task is to obtain the mapping relationships between object behaviors in image sequences and the natural language. These mapping relationships are related to the following two problems.\n\n\n1) Relationships between behaviors and semantic con-\n\ncepts. Each semantic concept of motion describes a class of behaviors, but each behavior may be related to multiple semantic concepts. After the mapping has been clearly defined, we could construct the relationship between the results of low-level image processing and object behaviors.\n\nThe key problems include the modeling of semantic concepts of motions, and the automatic learning of semantic concepts of behaviors. 2) Semantic recognition and natural language description of object behaviors. People usually describe developments and transformations of objects with concepts at different levels. The higher level concepts require greater background knowledge. It is a key problem to analyze the behaviors of moving objects using the tracking results from low-level systems, and further recognize the more abstract semantic concepts at higher layers. We can use the corresponding relationships between semantic concepts and object behaviors, semantic networks with different layers and reasoning theory to explore this problem. Natural language is the most convenient and natural way for humans to communicate each other.\n\nOrganizing recognized concepts and further representing object behaviors in brief and clear natural language is one of the ultimate goals of visual surveillance. In addition, the synchronous description, i.e., giving the description before a behavior finishes (during the behavior is progressing), is also a challenge. We should design an incremental description method which is able to predict object behaviors.\n\n\nI. Fusion of Data From Multiple Sensors\n\nIt is obvious that future visual surveillance systems will greatly benefit from the use of multiple cameras [44], [45], [73]. The cooperation between multiple cameras relies greatly on fusion of data from each camera. Data fusion is primarily feature-level based rather than image-level based or decision-making-level based. It happens in single view tracking, correspondence of cross-cameras, automatic camera switching (i.e., best view selection), etc. The main problems involve how to fuse different types of features, e.g., color, geometric features, into one group to track and recognize objects, and further understand their behaviors; how to fuse features extracted from different viewpoints to correspond objects; and how to communicate data about the same object between multiple cameras.\n\nBesides video, sensors for surveillance include audio, infrared, ultrasonic, and radar, etc. Each of these sensors has its own characteristics. Surveillance using multiple different sensors seems to be a very interesting subject. The main problem is how to make use of their respective merits and fuse information from such kinds of sensors.\n\n\nJ. Remote Surveillance\n\nRemote surveillance becomes more and more important for many promising applications, e.g., military combat, prevention of forest fires, etc. Video data are acquired from distributed sensors and transmitted to a remote control center. The transmission process must satisfy the following requirements.\n\n\u2022 The upload bandwidth (from sensors to the control center) should be much wider than the download bandwidth (from the control center to sensors). \u2022 The security of transmission must be guaranteed. Because some surveillance data involve privacy, commercial secrets and even national security, and nevertheless are transmitted through public networks, information security becomes a key problem. This needs the developments of the techniques such as digital watermarking and encryption [89]. The demand for remote surveillance and surveillance using multiple cameras and multiple sensors motivates the combination of network and visual surveillance, which brings new challenges in intelligent surveillance.\n\n\nVIII. CONCLUSIONS\n\nVisual surveillance in dynamic scenes is an active and important research area, strongly driven by many potential and promising applications, such as access control in special areas, person-specific identification in certain scenes, crowd flux statistics and congestion analysis, and anomaly detection and alarming, etc.\n\nWe have presented an overview of recent developments in visual surveillance within a general processing framework for visual surveillance systems. The state-of-the-art of existing methods in each key issue is described with the focus on the following tasks: detection, tracking, understanding and description of behaviors, personal identification for visual surveillance, and interactive surveillance using multiple cameras. As for the detection of moving objects, it involves environmental modeling, motion segmentation and object classification. Three techniques for motion segmentation are addressed: background subtraction, temporal differencing, and optical flow. We have discussed four intensively studied approaches to tracking: region based, active-contour based, feature based, and model based. We have reviewed several approaches to behavior understanding, including DTW, FSM, HMMs, and TDNN. In addition, we examine the state-of-the-art of behavior description. As to personal identification at a distance, we have divided gait recognition methods into four classes: mode based, statistics, physical-parameter based, and spatio-temporal motion based. As to fusion of data from multiple cameras, we have reviewed installation, object matching, switching, and data fusion.\n\nAt the end of this survey, we have given some detailed discussions on future directions, such as occlusion handling, fusion of 2-D tracking and 3-D tracking, 3-D modeling of humans and vehicles, combination of visual surveillance and personal identification, anomaly detection and behavior prediction, content-based retrieval of surveillance videos, natural language description of object behaviors, fusion of data from multiple sensors, and remote surveillance. \n\n\nManuscript received April 14, 2003; revised September 26, 2003 and January 8, 2004. This work was supported in part by the National Science Foundation of China (NSFC) under Grants 60105002, 60335010, 60121302, and 60373046, by the Natural Science Foundation of Beijing under Grant 4031004 and Grant 4041004, by the National 863 High-Tech R&D Program of China under Grant 2002AA117010-11 and Grant 2002AA142100, and by the International Cooperation Project of Beijing, the LIAMA Project. This paper was recommended by Associate Editor D. Zhang. W. Hu, T. Tan, and L. Wang are with the National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing 100080, China (e-mail: wmhu@nlpr.ia.ac.cn; tnt@nlpr.ia.ac.cn; lwang@nlpr.ia.ac.cn). S. Maybank is with the School of Computer Science and Information Systems, Birkbeck College, London WC1E 7HX, U.K. (e-mail: sjmay-bank@dcs.bbk.ac.uk). Digital Object Identifier 10.1109/TSMCC.2004.829274\n\nFig. 1 .\n1General framework of visual surveillance.\n\n\nuses the 3-D wire-frame vehicle model. The image features used in the algorithm are edges. The initial values for the vehicle pose parameters are obtained from the correspondence between the segments in an image and those in the projection model. The correspondence is calculated using viewpoint consistent constraints and some clustering rules. The maximum a posterior (MAP) estimate of the vehicle position is obtained using the Levenberg-Marquardt optimization technique. The algorithm is data-driven and dependent on the accuracy of edge detection. Kollnig et al. [125] also propose an algorithm based on image gradients, in which virtual gradients in an image are produced by spreading the Gaussian distribution around line segments. Under the assumption that the real gradient at each point in an image is the sum of a virtual gradient and a Gaussian white noise, the pose parameters can be estimated using the extended Kalman filter (EKF). Furthermore, Haag et al. [126] integrate Kollnig et al.'s algorithm based on image gradients with that based on optic flow. The method uses image gradients evaluated in the neighborhoods of the image features. However, the optic flow uses global information on image features, integrated across the whole region of interest (ROI). So the gradients and the optic flow are complementary sources of information.\n\n\nmakes use of fourteen elliptical cylinders to model a human body in 3-D volumes. Wachter et al. [29] establish a 3-D body model using connected elliptical cones. \u2022 Hierarchical model. Plankers et al. [105] present a hierarchical human model for achieving more accurate results. It includes four levels: skeleton, ellipsoid meatballs simulating tissues and fats, polygonal surface representing skin, and shaded rendering.\n\n\nSteve Maybank received the B.A. degree in mathematics from King's College, Cambridge, U.K., in 1976 and the Ph.D. degree in computer science from Birkbeck College, University of London, London, U.K., in 1988. He joined the Pattern Recognition Group, Marconi Command and Control Systems, Frimley, U.K., in 1980 and moved to the GEC Hirst Research Centre, Wembley, U.K., in 1989. During 1993-1995, he was a Royal Society/EPSRC Industrial Fellow in the Department of Engineering Science, University of Oxford, Oxford, U.K. In 1995, he joined the University of Reading, Reading, U.K., as a Lecturer in the Department of Computer Science. In 2004, he became a Professor in the School of Computer Science and Information Systems, Birkbeck College. His research interests include the geometry of multiple images, camera calibration, visual surveillance, information geometry, and the applications of statistics to computer vision. Dr. Maybank is a Fellow of the Royal Statistical Society and the Institute of Mathematics and its Applications, and is a member of the British Machine Vision Association and the Societe Mathematique de France.\nAuthorized licensed use limited to: National Tsing Hua University. Downloaded on November 27, 2008 at 05:21 from IEEE Xplore. Restrictions apply.\nACKNOWLEDGMENTThe authors thank J. Lou, Q. Liu, H. Ning, M. Hu, D. Xie, and G. Xu from the NLPR for their valuable suggestions and assistance in preparing this paper.\nIntroduction to the special section on video surveillance. R T Collins, A J Lipton, T Kanade, IEEE Trans. Pattern Anal. Machine Intell. 22R. T. Collins, A. J. Lipton, and T. Kanade, \"Introduction to the special section on video surveillance,\" IEEE Trans. Pattern Anal. Machine In- tell., vol. 22, pp. 745-746, Aug. 2000.\n\nPerson spotter-fast and robust system for human detection, tracking and recognition. J Steffens, E Elagin, H Neven, Proc. IEEE Int. Conf. Automatic Face and Gesture Recognition. IEEE Int. Conf. Automatic Face and Gesture RecognitionJ. Steffens, E. Elagin, and H. Neven, \"Person spotter-fast and robust system for human detection, tracking and recognition,\" in Proc. IEEE Int. Conf. Automatic Face and Gesture Recognition, 1998, pp. 516-521.\n\nA system for video surveillance and monitoring. R T Collins, A J Lipton, T Kanade, H Fujiyoshi, D Duggins, Y Tsin, D Tolliver, N Enomoto, O Hasegawa, P Burt, L Wixson, CMU-RI-TR-00-12Carnegie Mellon Univ. Tech. Rep.R. T. Collins, A. J. Lipton, T. Kanade, H. Fujiyoshi, D. Duggins, Y. Tsin, D. Tolliver, N. Enomoto, O. Hasegawa, P. Burt, and L. Wixson, \"A system for video surveillance and monitoring,\" Carnegie Mellon Univ., Pittsburgh, PA, Tech. Rep., CMU-RI-TR-00-12, 2000.\n\nW : Real-time surveillance of people and their activities. I Haritaoglu, D Harwood, L S Davis, IEEE Trans. Pattern Anal. Machine Intell. 22I. Haritaoglu, D. Harwood, and L. S. Davis, \"W : Real-time surveil- lance of people and their activities,\" IEEE Trans. Pattern Anal. Machine Intell., vol. 22, pp. 809-830, Aug. 2000.\n\nConstructing qualitative event models automatically from video input. J Fernyhough, A G Cohn, D C Hogg, Image Vis. Comput. 189J. Fernyhough, A. G. Cohn, and D. C. Hogg, \"Constructing qualitative event models automatically from video input,\" Image Vis. Comput., vol. 18, no. 9, pp. 81-103, 2000.\n\nLearning deformable models for tracking the human body. A Baumberg, D C Hogg, Motion-Based Recognition, M. Shah and R. JainKluwerNorwell, MAA. Baumberg and D. C. Hogg, \"Learning deformable models for tracking the human body,\" in Motion-Based Recognition, M. Shah and R. Jain, Eds. Norwell, MA: Kluwer, 1996, pp. 39-60.\n\nArticulated model based people tracking using motion models. H Z Ning, L Wang, W M Hu, T N Tan, Proc. Int. Conf. Multi-Model Interfaces. Int. Conf. Multi-Model InterfacesH. Z. Ning, L. Wang, W. M. Hu, and T. N. Tan, \"Articulated model based people tracking using motion models,\" in Proc. Int. Conf. Multi-Model Interfaces, 2002, pp. 115-120.\n\nPfinder: real-time tracking of the human body. C R Wren, A Azarbayejani, T Darrell, A P Pentland, IEEE Trans. Pattern Anal. Machine Intell. 19C. R. Wren, A. Azarbayejani, T. Darrell, and A. P. Pentland, \"Pfinder: real-time tracking of the human body,\" IEEE Trans. Pattern Anal. Ma- chine Intell., vol. 19, pp. 780-785, July 1997.\n\nMoving object detection and event recognition algorithms for smart cameras. T Olson, F Brill, Proc. DARPA Image Understanding Workshop. DARPA Image Understanding WorkshopT. Olson and F. Brill, \"Moving object detection and event recognition algorithms for smart cameras,\" in Proc. DARPA Image Understanding Workshop, 1997, pp. 159-175.\n\nMoving target classification and tracking from real-time video. A J Lipton, H Fujiyoshi, R S Patil, Proc. IEEE Workshop Applications of Computer Vision. IEEE Workshop Applications of Computer VisionA. J. Lipton, H. Fujiyoshi, and R. S. Patil, \"Moving target classification and tracking from real-time video,\" in Proc. IEEE Workshop Applica- tions of Computer Vision, 1998, pp. 8-14.\n\nTracking groups of people. S Mckenna, S Jabri, Z Duric, A Rosenfeld, H Wechsler, Comput. Vis. Image Understanding. 801S. McKenna, S. Jabri, Z. Duric, A. Rosenfeld, and H. Wechsler, \"Tracking groups of people,\" Comput. Vis. Image Understanding, vol. 80, no. 1, pp. 42-56, 2000.\n\nAdaptive background mixture models for real-time tracking. C Stauffer, W Grimson, Proc. IEEE Conf. Computer Vision and Pattern Recognition. IEEE Conf. Computer Vision and Pattern Recognition2C. Stauffer and W. Grimson, \"Adaptive background mixture models for real-time tracking,\" in Proc. IEEE Conf. Computer Vision and Pattern Recognition, vol. 2, 1999, pp. 246-252.\n\nModel based extraction of articulated objects in image sequences for gait analysis. D Meyer, J Denzler, H Niemann, Proc. IEEE Int. Conf. Image Processing. IEEE Int. Conf. Image essingD. Meyer, J. Denzler, and H. Niemann, \"Model based extraction of ar- ticulated objects in image sequences for gait analysis,\" in Proc. IEEE Int. Conf. Image Processing, 1998, pp. 78-81.\n\nPerformance of optical flow techniques. J Barron, D Fleet, S Beauchemin, Int. J. Comput.Vis. 121J. Barron, D. Fleet, and S. Beauchemin, \"Performance of optical flow techniques,\" Int. J. Comput.Vis., vol. 12, no. 1, pp. 42-77, 1994.\n\nImage segmentation in video sequences: a probabilistic approach. N Friedman, S Russell, Proc. 13th Conf. Uncertainty in Artificial Intelligence. 13th Conf. Uncertainty in Artificial IntelligenceN. Friedman and S. Russell, \"Image segmentation in video sequences: a probabilistic approach,\" in Proc. 13th Conf. Uncertainty in Artificial Intelligence, 1997, pp. 1-3.\n\nMorphological change detection algorithms for surveillance applications. E Stringa, Proc. British Machine Vision Conf. British Machine Vision ConfE. Stringa, \"Morphological change detection algorithms for surveillance applications,\" in Proc. British Machine Vision Conf., 2000, pp. 402-412.\n\nAutomated detection of human for visual surveillance system. Y Kuno, T Watanabe, Y Shimosakoda, S Nakagawa, Proc. Int. Conf. Pattern Recognition. Int. Conf. Pattern RecognitionY. Kuno, T. Watanabe, Y. Shimosakoda, and S. Nakagawa, \"Automated detection of human for visual surveillance system,\" in Proc. Int. Conf. Pattern Recognition, 1996, pp. 865-869.\n\nRobust real-time periodic motion detection, analysis, and applications. R Cutler, L S Davis, IEEE Trans. Pattern Anal. Machine Intell. 22R. Cutler and L. S. Davis, \"Robust real-time periodic motion detection, analysis, and applications,\" IEEE Trans. Pattern Anal. Machine Intell., vol. 22, pp. 781-796, Aug. 2000.\n\nLocal application of optic flow to analyze rigid versus nonrigid motion. A J Lipton, Proc. Int. Conf. Computer Vision Workshop Frame-Rate Vision. Int. Conf. Computer Vision Workshop Frame-Rate VisionCorfu, GreeceA. J. Lipton, \"Local application of optic flow to analyze rigid versus nonrigid motion,\" in Proc. Int. Conf. Computer Vision Workshop Frame-Rate Vision, Corfu, Greece, 1999.\n\nAutomatic hierarchical classification using time-base co-occurrences. C Stauffer, Proc. IEEE Conf. Computer Vision and Pattern Recognition. IEEE Conf. Computer Vision and Pattern Recognition2C. Stauffer, \"Automatic hierarchical classification using time-base co-occurrences,\" in Proc. IEEE Conf. Computer Vision and Pattern Recognition, vol. 2, 1999, pp. 335-339.\n\nGait classification with HMM's for trajectories of body parts extracted by mixture densities. D Meyer, J Psl, H Niemann, Proc. British Machine Vision Conf. British Machine Vision ConfD. Meyer, J. Psl, and H. Niemann, \"Gait classification with HMM's for trajectories of body parts extracted by mixture densities,\" in Proc. British Machine Vision Conf., 1998, pp. 459-468.\n\nSemantic interpretation of object activities in a surveillance system. J G Lou, Q F Liu, W M Hu, T N Tan, Proc. Int. Conf. Pattern Recognition. Int. Conf. Pattern RecognitionJ. G. Lou, Q. F. Liu, W. M. Hu, and T. N. Tan, \"Semantic interpretation of object activities in a surveillance system,\" in Proc. Int. Conf. Pattern Recognition, 2002, pp. 777-780.\n\nFrame-rate multi-body tracking for surveillance. T Boult, Proc. DARPA Image Understanding Workshop. DARPA Image Understanding WorkshopMonterey, CAT. Boult, \"Frame-rate multi-body tracking for surveillance,\" in Proc. DARPA Image Understanding Workshop, Monterey, CA, Nov. 1998, pp. 305-308.\n\nNon-rigid motion analysis: articulated & elastic motion. J K Aggarwal, Q Cai, W Liao, B Sabata, Comput.Vis. Image Understanding. 702J. K. Aggarwal, Q. Cai, W. Liao, and B. Sabata, \"Non-rigid motion anal- ysis: articulated & elastic motion,\" Comput.Vis. Image Understanding, vol. 70, no. 2, pp. 142-156, 1998.\n\nA hierarchical model of dynamics for tracking people with a single video camera. I A Karaulova, P M Hall, A D Marshall, Proc. British Machine Vision Conf. British Machine Vision ConfI. A. Karaulova, P. M. Hall, and A. D. Marshall, \"A hierarchical model of dynamics for tracking people with a single video camera,\" in Proc. British Machine Vision Conf., 2000, pp. 262-352.\n\nCardboard people: a parameterized model of articulated image motion. S Ju, M Black, Y Yaccob, Proc. IEEE Int. Conf. Automatic Face and Gesture Recognition. IEEE Int. Conf. Automatic Face and Gesture RecognitionS. Ju, M. Black, and Y. Yaccob, \"Cardboard people: a parameterized model of articulated image motion,\" in Proc. IEEE Int. Conf. Automatic Face and Gesture Recognition, 1996, pp. 38-44.\n\nAnalyzing and recognizing walking figures in XYT. S A Niyogi, E H Adelson, Proc. IEEE Conf. Computer Vision and Pattern Recognition. IEEE Conf. Computer Vision and Pattern RecognitionS. A. Niyogi and E. H. Adelson, \"Analyzing and recognizing walking figures in XYT,\" in Proc. IEEE Conf. Computer Vision and Pattern Recognition, 1994, pp. 469-474.\n\nToward model-based recognition of human movements in image sequences. K Rohr, CVGIP: Image Understanding. 591K. Rohr, \"Toward model-based recognition of human movements in image sequences,\" CVGIP: Image Understanding, vol. 59, no. 1, pp. 94-115, 1994.\n\nTracking persons in monocular image sequences. S Wachter, H.-H Nagel, Comput. Vis. Image Understanding. 743S. Wachter and H.-H. Nagel, \"Tracking persons in monocular image sequences,\" Comput. Vis. Image Understanding, vol. 74, no. 3, pp. 174-192, 1999.\n\nGeodesic active contours and level sets for the detection and tracking of moving objects. N Paragios, R Deriche, IEEE Trans. Pattern Anal. Machine Intell. 22N. Paragios and R. Deriche, \"Geodesic active contours and level sets for the detection and tracking of moving objects,\" IEEE Trans. Pattern Anal. Machine Intell., vol. 22, pp. 266-280, Mar. 2000.\n\nRobust tracking of position and velocity with Kalman snakes. N Peterfreund, IEEE Trans. Pattern Anal. Machine Intell. 22N. Peterfreund, \"Robust tracking of position and velocity with Kalman snakes,\" IEEE Trans. Pattern Anal. Machine Intell., vol. 22, pp. 564-569, June 2000.\n\nContour tracking by stochastic propagation of conditional density. M Isard, A Blake, Proc. European Conf. Computer Vision. European Conf. Computer VisionM. Isard and A. Blake, \"Contour tracking by stochastic propagation of conditional density,\" in Proc. European Conf. Computer Vision, 1996, pp. 343-356.\n\nLow level recognition of human motion. R Polana, R Nelson, Proc. IEEE Workshop Motion of Non-Rigid and Articulated Objects. IEEE Workshop Motion of Non-Rigid and Articulated ObjectsAustin, TXR. Polana and R. Nelson, \"Low level recognition of human motion,\" in Proc. IEEE Workshop Motion of Non-Rigid and Articulated Objects, Austin, TX, 1994, pp. 77-82.\n\nActive models for tracking moving objects. D.-S Jang, H.-I Choi, Pattern Recognit. 337D.-S. Jang and H.-I. Choi, \"Active models for tracking moving objects,\" Pattern Recognit., vol. 33, no. 7, pp. 1135-1146, 2000.\n\nCoupled hidden Markov models for complex action recognition. M Brand, N Oliver, A Pentland, Proc. IEEE Conf. Computer Vision and Pattern Recognition. IEEE Conf. Computer Vision and Pattern RecognitionM. Brand, N. Oliver, and A. Pentland, \"Coupled hidden Markov models for complex action recognition,\" in Proc. IEEE Conf. Computer Vision and Pattern Recognition, 1997, pp. 994-999.\n\nGenerating natural language description of human behaviors from video images. M Izumi, A Kojima, Proc. Int. Conf. Pattern Recognition. Int. Conf. Pattern RecognitionM. Izumi and A. Kojima, \"Generating natural language description of human behaviors from video images,\" in Proc. Int. Conf. Pattern Recog- nition, 2000, pp. 728-731.\n\nNatural language description of human activities from video images based on concept hierarchy of actions. A Kojima, T Tamura, K Fukunaga, Int. J. Comput. Vis. 502A. Kojima, T. Tamura, and K. Fukunaga, \"Natural language description of human activities from video images based on concept hierarchy of actions,\" Int. J. Comput. Vis., vol. 50, no. 2, pp. 171-184, 2002.\n\nTracking from multiple view points: self-calibration of space and time. G P Stein, Proc. IEEE Conf. Computer Vision and Pattern Recognition. IEEE Conf. Computer Vision and Pattern RecognitionIG. P. Stein, \"Tracking from multiple view points: self-calibration of space and time,\" in Proc. IEEE Conf. Computer Vision and Pattern Recognition, vol. I, 1999, pp. 521-527.\n\nMonitoring activities from multiple video streams: establishing a common coordinate frame. L Lee, R Romano, G Stein, IEEE Trans. Pattern Anal. Machine Intell. 22L. Lee, R. Romano, and G. Stein, \"Monitoring activities from multiple video streams: establishing a common coordinate frame,\" IEEE Trans. Pattern Anal. Machine Intell., vol. 22, pp. 758-767, Aug. 2000.\n\nMultiple-view-based tracking of multiple humans. A Utsumi, H Mori, J Ohya, M Yachida, Proc. Int. Conf. Pattern Recognition. Int. Conf. Pattern RecognitionA. Utsumi, H. Mori, J. Ohya, and M. Yachida, \"Multiple-view-based tracking of multiple humans,\" in Proc. Int. Conf. Pattern Recognition, 1998, pp. 197-601.\n\nTracking human motion using multiple cameras. Q Cai, J K Aggarwal, Proc. Int. Conf. Pattern Recognition. Int. Conf. Pattern RecognitionVienna, AustriaQ. Cai and J. K. Aggarwal, \"Tracking human motion using multiple cameras,\" in Proc. Int. Conf. Pattern Recognition, Vienna, Austria, 1996, pp. 68-72.\n\nTracking human motion in structured environments using a distributed-camera system. IEEE Trans. Pattern Anal. Machine Intell. 2111, \"Tracking human motion in structured environments using a dis- tributed-camera system,\" IEEE Trans. Pattern Anal. Machine Intell., vol. 21, no. 11, pp. 1241-1247, 1999.\n\nBayesian multi-camera surveillance. V Kettnaker, R Zabih, Proc. IEEE Conf. Computer Vision and Pattern Recognition. IEEE Conf. Computer Vision and Pattern RecognitionV. Kettnaker and R. Zabih, \"Bayesian multi-camera surveillance,\" in Proc. IEEE Conf. Computer Vision and Pattern Recognition, 1999, pp. 253-259.\n\nTracking multiple people under occlusion using multiple cameras. T H Chang, S Gong, E J Ong, Proc. British Machine Vision Conf. British Machine Vision ConfT. H. Chang, S. Gong, and E. J. Ong, \"Tracking multiple people under occlusion using multiple cameras,\" in Proc. British Machine Vision Conf., 2000, pp. 566-576.\n\nSpatio-temporal alignment of sequences. Y Caspi, M Irani, IEEE Trans. Pattern Anal. Machine Intell. 24Y. Caspi and M. Irani, \"Spatio-temporal alignment of sequences,\" IEEE Trans. Pattern Anal. Machine Intell., vol. 24, pp. 1409-1424, Nov. 2002.\n\nAutomatic recognition and analysis of human faces and facial expressions: a survey. A Samal, P A Iyengar, Pattern Recognit. 251A. Samal and P. A. Iyengar, \"Automatic recognition and analysis of human faces and facial expressions: a survey,\" Pattern Recognit., vol. 25, no. 1, pp. 65-77, 1992.\n\nHuman and machine recognition of faces: A survey. R Chellappa, C L Wilson, S Sirohey, Proc. IEEE. IEEE83R. Chellappa, C. L. Wilson, and S. Sirohey, \"Human and machine recog- nition of faces: A survey,\" Proc. IEEE, vol. 83, pp. 705-741, May 1995.\n\nDiscriminant analysis and eigenspace partition tree for face and object recognition from views. D Swets, J Weng, Proc. Int. Conf. Automatic Face and Gesture Recognition. Int. Conf. Automatic Face and Gesture RecognitionD. Swets and J. Weng, \"Discriminant analysis and eigenspace partition tree for face and object recognition from views,\" in Proc. Int. Conf. Au- tomatic Face and Gesture Recognition, 1996, pp. 182-187.\n\nBeyond eigenfaces: probabilistic matching for face recognition. B Moghaddam, W Wahid, A Pentland, Proc. IEEE Int. Conf. Automatic Face and Gesture Recognition. IEEE Int. Conf. Automatic Face and Gesture RecognitionB. Moghaddam, W. Wahid, and A. Pentland, \"Beyond eigenfaces: prob- abilistic matching for face recognition,\" in Proc. IEEE Int. Conf. Auto- matic Face and Gesture Recognition, 1998, pp. 30-35.\n\nFace recognition by support vector machines. G Guo, S Li, K Chan, Proc. Int. Conf. Automatic Face and Gesture Recognition. Int. Conf. Automatic Face and Gesture RecognitionG. Guo, S. Li, and K. Chan, \"Face recognition by support vector ma- chines,\" in Proc. Int. Conf. Automatic Face and Gesture Recognition, 2000, pp. 196-201.\n\nNeural network based face detection. H Rowley, S Baluja, T Kanade, IEEE Trans. Pattern Anal. Machine Intell. 20H. Rowley, S. Baluja, and T. Kanade, \"Neural network based face de- tection,\" IEEE Trans. Pattern Anal. Machine Intell., vol. 20, pp. 23-38, Jan. 1998.\n\nFace detection using quantified skin color regions merging and wavelet packet analysis. C Garcia, G Tziritas, IEEE Trans. Multimedia. 1C. Garcia and G. Tziritas, \"Face detection using quantified skin color regions merging and wavelet packet analysis,\" IEEE Trans. Multimedia, vol. 1, pp. 264-277, Sept. 1999.\n\nSegmentation and tracking of facial regions in color image sequences. B Menser, M Wien, Proc. SPIE Visual Communications and Image Processing. SPIE Visual Communications and Image essingPerth, Australia4067B. Menser and M. Wien, \"Segmentation and tracking of facial regions in color image sequences,\" in Proc. SPIE Visual Communications and Image Processing, vol. 4067, Perth, Australia, 2000, pp. 731-740.\n\nFrontal-view face detection and facial feature extraction using color, shape and symmetry based cost functions. A Saber, A M Tekalp, Pattern Recognit. Lett. 198A. Saber and A. M. Tekalp, \"Frontal-view face detection and facial fea- ture extraction using color, shape and symmetry based cost functions,\" Pattern Recognit. Lett., vol. 19, no. 8, pp. 669-680, 1998.\n\nA software-based system for real-time face detection and tracking using pan-tilt-zoom controllable camera. G Xu, T Sugimoto, Proc. Int. Conf. Pattern Recognition. Int. Conf. Pattern RecognitionG. Xu and T. Sugimoto, \"A software-based system for real-time face de- tection and tracking using pan-tilt-zoom controllable camera,\" in Proc. Int. Conf. Pattern Recognition, 1998, pp. 1194-1197.\n\nStride and cadence as a biometric in automatic person identification and verification. C Benabdelkader, R Culter, L Davis, Proc. Int. Conf. Automatic Face and Gesture Recognition. Int. Conf. Automatic Face and Gesture RecognitionWashington, DCC. BenAbdelkader, R. Culter, and L. Davis, \"Stride and cadence as a biometric in automatic person identification and verification,\" in Proc. Int. Conf. Automatic Face and Gesture Recognition, Washington, DC, 2002, pp. 372-377.\n\nMulti-camera multi-person tracking for EasyLiving. J Krumm, S Harris, B Meyers, B Brumitt, M Hale, S Shafer, Proc. IEEE Int. Workshop Visual Surveillance. IEEE Int. Workshop Visual SurveillanceDublin, IrelandJ. Krumm, S. Harris, B. Meyers, B. Brumitt, M. Hale, and S. Shafer, \"Multi-camera multi-person tracking for EasyLiving,\" in Proc. IEEE Int. Workshop Visual Surveillance, Dublin, Ireland, July 2000, pp. 3-10.\n\nAnalyzing gait with spatio-temporal surface. S A Niyogi, E H Adelson, Proc. IEEE Workshop Motion of Non-Rigid and Articulated Objects. IEEE Workshop Motion of Non-Rigid and Articulated ObjectsS. A. Niyogi and E. H. Adelson, \"Analyzing gait with spatio-temporal surface,\" in Proc. IEEE Workshop Motion of Non-Rigid and Articulated Objects, 1994, pp. 64-69.\n\nEigenGait: motion-based recognition of people using image self-similarity. C Benabdelkader, R Cutler, H Nanda, L Davis, Proc. Int. Conf. Audio-and Video-Based Biometric Person Authentication. Int. Conf. Audio-and Video-Based Biometric Person AuthenticationC. BenAbdelkader, R. Cutler, H. Nanda, and L. Davis, \"EigenGait: mo- tion-based recognition of people using image self-similarity,\" in Proc. Int. Conf. Audio-and Video-Based Biometric Person Authentication, 2001, pp. 312-317.\n\nGait analysis for recognition and classification. L Lee, W Grimson, Proc. Int. Conf. Automatic Face and Gesture Recognition. Int. Conf. Automatic Face and Gesture RecognitionWashington, DCL. Lee and W. Grimson, \"Gait analysis for recognition and classifi- cation,\" in Proc. Int. Conf. Automatic Face and Gesture Recognition, Washington, DC, 2002, pp. 155-162.\n\nRobust real-time periodic motion detection, analysis and applications. R Culter, L Davis, IEEE Trans. Pattern Recognit. Machine Intell. 13R. Culter and L. Davis, \"Robust real-time periodic motion detection, analysis and applications,\" IEEE Trans. Pattern Recognit. Machine In- tell., vol. 13, pp. 129-155, Feb. 2000.\n\nGait-based recognition of humans using continuous HMMs. A Kale, A Rajagopalan, N Cuntoor, V Kruger, Proc. Int. Conf. Automatic Face and Gesture Recognition. Int. Conf. Automatic Face and Gesture RecognitionWashington, DCA. Kale, A. Rajagopalan, N. Cuntoor, and V. Kruger, \"Gait-based recognition of humans using continuous HMMs,\" in Proc. Int. Conf. Automatic Face and Gesture Recognition, Washington, DC, 2002, pp. 336-341.\n\nPath detection in video surveillance. D Makris, T Ellis, Image Vis. Comput. 2012D. Makris and T. Ellis, \"Path detection in video surveillance,\" Image Vis. Comput., vol. 20, no. 12, pp. 895-903, 2002.\n\nIntegrated face and gait recognition from multiple views. G Shakhnarovich, L Lee, T Darrell, Proc. IEEE Conf. Computer Vision and Pattern Recognition. IEEE Conf. Computer Vision and Pattern RecognitionG. Shakhnarovich, L. Lee, and T. Darrell, \"Integrated face and gait recognition from multiple views,\" in Proc. IEEE Conf. Computer Vision and Pattern Recognition, 2001, pp. (I)439-(I)446.\n\nOn probabilistic combination of face and gait cues for identification. G Shakhnarovich, T Darrell, Proc. Int. Conf. Automatic Face and Gesture Recognition. Int. Conf. Automatic Face and Gesture RecognitionWashington, DCG. Shakhnarovich and T. Darrell, \"On probabilistic combination of face and gait cues for identification,\" in Proc. Int. Conf. Automatic Face and Gesture Recognition, Washington, DC, 2002, pp. 176-181.\n\nAutomatic gait recognition. M S Nixon, J N Carter, D Cunado, P S Huang, S V Stevenage, BIOMETRICS Personal Identification in Networked Society. A. K. JainNorwell, MAKluwer11M. S. Nixon, J. N. Carter, D. Cunado, P. S. Huang, and S. V. Stevenage, \"Automatic gait recognition,\" in BIOMETRICS Personal Identification in Networked Society, A. K. Jain, Ed. Norwell, MA: Kluwer, 1999, ch. 11.\n\nViewpoint invariance in automatic gait recognition. N Spencer, J Carter, BMVA (British Machine Vision Association) Symp. Advancing Biometric Techniques at the Royal Statistical Society. London, U.K.N. Spencer and J. Carter, \"Viewpoint invariance in automatic gait recog- nition,\" in BMVA (British Machine Vision Association) Symp. Advancing Biometric Techniques at the Royal Statistical Society, London, U.K., Mar. 6, 2002, pp. 1-6.\n\nSpecial section on visual surveillance-introduction. S J Maybank, T N Tan, Int. J. Comput. Vis. 372S. J. Maybank and T. N. Tan, \"Special section on visual surveil- lance-introduction,\" Int. J. Comput. Vis., vol. 37, no. 2, pp. 173-174, 2000.\n\nSpecial issue on video communications, processing, and understanding for third generation surveillance systems. C Regazzoni, V Ramesh, Proc. IEEE. IEEE89C. Regazzoni and V. Ramesh, \"Special issue on video communica- tions, processing, and understanding for third generation surveillance systems,\" Proc. IEEE, vol. 89, pp. 1355-1367, Oct. 2001.\n\nClinical gait analysis by neural networks: Issues and experiences. M K\u00f6hle, D Merkl, J Kastner, Proc. IEEE Symp. IEEE SympM. K\u00f6hle, D. Merkl, and J. Kastner, \"Clinical gait analysis by neural net- works: Issues and experiences,\" in Proc. IEEE Symp. Computer-Based Medical Systems, 1997, pp. 138-143.\n\nExample-based object detection in images by components. A Mohan, C Papageorgiou, T Poggio, IEEE Trans. Pattern Recognit. Machine Intell. 23A. Mohan, C. Papageorgiou, and T. Poggio, \"Example-based object de- tection in images by components,\" IEEE Trans. Pattern Recognit. Ma- chine Intell., vol. 23, pp. 349-361, Apr. 2001.\n\nLearning variable-length Markov models of behavior. A Galata, N Johnson, D Hogg, Comput. Vis. Image Understanding. 813A. Galata, N. Johnson, and D. Hogg, \"Learning variable-length Markov models of behavior,\" Comput. Vis. Image Understanding, vol. 81, no. 3, pp. 398-413, 2001.\n\nMulti-view constraints on homographies. L Z Manor, M Irani, IEEE Trans. Pattern Anal. Machine Intell. 24L. Z. Manor and M. Irani, \"Multi-view constraints on homographies,\" IEEE Trans. Pattern Anal. Machine Intell., vol. 24, pp. 214-223, Feb. 2002.\n\nA co-inference approach to robust visual tracking. Y Wu, T S Huang, Proc. Int. Conf. Computer Vision. Int. Conf. Computer VisionIIY. Wu and T. S. Huang, \"A co-inference approach to robust visual tracking,\" in Proc. Int. Conf. Computer Vision, vol. II, 2001, pp. 26-33.\n\nRecent developments in human motion analysis. L Wang, W Hu, T Tan, Pattern Recognit. 363L. Wang, W. Hu, and T. Tan, \"Recent developments in human motion analysis,\" Pattern Recognit., vol. 36, no. 3, pp. 585-601, 2003.\n\nMulti-resolution image sensor. S E Kemeny, R Panicacci, B Pain, L Matthies, E R Fossum, IEEE Trans. Circuits Syst. Video Technol. 7S. E. Kemeny, R. Panicacci, B. Pain, L. Matthies, and E. R. Fossum, \"Multi-resolution image sensor,\" IEEE Trans. Circuits Syst. Video Technol., vol. 7, no. Aug., pp. 575-583, 1997.\n\nOmni-directional sensors for pipe inspection. A Basu, D Southwell, Proc. IEEE Int. Conf. Systems, Man and Cybernetics. IEEE Int. Conf. Systems, Man and Cybernetics4A. Basu and D. Southwell, \"Omni-directional sensors for pipe inspec- tion,\" in Proc. IEEE Int. Conf. Systems, Man and Cybernetics, vol. 4, 1995, pp. 3107-3112.\n\nForeword: modeling people toward vision-based understanding of a person's shape, appearance, and movement. A Hilton, P Fua, Comput. Vis. Image Understanding. 813A. Hilton and P. Fua, \"Foreword: modeling people toward vision-based understanding of a person's shape, appearance, and movement,\" Comput. Vis. Image Understanding, vol. 81, no. 3, pp. 227-230, 2001.\n\nRobust extraction of moving objects from image sequences. H Z Sun, T Feng, T N Tan ; Taiwan, R O C , Proc. Asian Conf. Computer Vision. Asian Conf. Computer VisionH. Z. Sun, T. Feng, and T. N. Tan, \"Robust extraction of moving objects from image sequences,\" in Proc. Asian Conf. Computer Vision, Taiwan, R.O.C., 2000, pp. 961-964.\n\nUsing adaptive tracking to classify and monitor activities in a site. W E L Grimson, C Stauffer, R Romano, L Lee, Proc. IEEE Conf. Computure Vision and Pattern Recognition. IEEE Conf. Computure Vision and Pattern RecognitionSanta Barbara, CAW. E. L. Grimson, C. Stauffer, R. Romano, and L. Lee, \"Using adaptive tracking to classify and monitor activities in a site,\" in Proc. IEEE Conf. Computure Vision and Pattern Recognition, Santa Barbara, CA, 1998, pp. 22-31.\n\nAdaptive background estimation and foreground detection using Kalman-filtering. C Ridder, O Munkelt, H Kirchner, Proc. Int. Conf. Recent Advances in Mechatronics. Int. Conf. Recent Advances in MechatronicsC. Ridder, O. Munkelt, and H. Kirchner, \"Adaptive background esti- mation and foreground detection using Kalman-filtering,\" in Proc. Int. Conf. Recent Advances in Mechatronics, 1995, pp. 193-199.\n\nToward robust automatic traffic scene analysis in real-time. D Koller, J Weber, T Huang, J Malik, G Ogasawara, B Rao, S Russel, Proc. Int. Conf. Pattern Recognition. Int. Conf. Pattern RecognitionIsraelD. Koller, J. Weber, T. Huang, J. Malik, G. Ogasawara, B. Rao, and S. Russel, \"Toward robust automatic traffic scene analysis in real-time,\" in Proc. Int. Conf. Pattern Recognition, Israel, 1994, pp. 126-131.\n\nWallflower: principles and practice of background maintenance. K Toyama, J Krumm, B Brumitt, B Meyers, Proc. Int. Conf. Computer Vision. Int. Conf. Computer VisionK. Toyama, J. Krumm, B. Brumitt, and B. Meyers, \"Wallflower: princi- ples and practice of background maintenance,\" in Proc. Int. Conf. Com- puter Vision, 1999, pp. 255-261.\n\nInteractive construction of 3D models from panoramic mosaics. H.-Y Shum, M Han, R Szeliski, Proc. IEEE Conf. Computer Vision and Pattern Recognition. IEEE Conf. Computer Vision and Pattern RecognitionSanta Barbara, CAH.-Y. Shum, M. Han, and R. Szeliski, \"Interactive construction of 3D models from panoramic mosaics,\" in Proc. IEEE Conf. Computer Vision and Pattern Recognition, Santa Barbara, CA, 1998, pp. 427-433.\n\nComparison of approaches to egomotion computation. T Tian, C Tomasi, Proc. IEEE Conf. Computer Vision and Pattern Recognition. IEEE Conf. Computer Vision and Pattern RecognitionT. Tian and C. Tomasi, \"Comparison of approaches to egomotion com- putation,\" in Proc. IEEE Conf. Computer Vision and Pattern Recogni- tion, 1996, pp. 315-320.\n\nModeling geometric structure and illumination variation of a scene from real images. Z Y Zhang, Proc. Int. Conf. Computer Vision. Int. Conf. Computer VisionBombay, IndiaZ. Y. Zhang, \"Modeling geometric structure and illumination variation of a scene from real images,\" in Proc. Int. Conf. Computer Vision, Bombay, India, 1998, pp. 4-7.\n\nModel-based localization and recognition of road vehicles. T N Tan, G D Sullivan, K D Baker, Int. J. Comput. Vis. 291T. N. Tan, G. D. Sullivan, and K. D. Baker, \"Model-based localization and recognition of road vehicles,\" Int. J. Comput. Vis., vol. 29, no. 1, pp. 22-25, 1998.\n\nRecognizing objects on the ground-plane. Image Vis. Comput. 123, \"Recognizing objects on the ground-plane,\" Image Vis. Comput., vol. 12, no. 3, pp. 164-172, 1994.\n\nImage authentication techniques for surveillance applications. F Bartoleni, A Tefas, M Barni, I Pitas, Proc. IEEE. IEEE89F. Bartoleni, A. Tefas, M. Barni, and I. Pitas, \"Image authentication techniques for surveillance applications,\" Proc. IEEE, vol. 89, pp. 1403-1417, Oct. 2001.\n\nMoving object recognition using an adaptive background memory. K Karmann, A Brandt, Time-Varying Image Processing and Moving Object Recognition. V. CappelliniEd. Amsterdam, The NetherlandsElsevier2K. Karmann and A. Brandt, \"Moving object recognition using an adaptive background memory,\" in Time-Varying Image Processing and Moving Object Recognition, V. Cappellini, Ed. Amsterdam, The Netherlands: Elsevier, 1990, vol. 2.\n\nA shadow handler in a video-based real-time traffic monitoring system. M Kilger, Proc. IEEE Workshop Applications of Computer Vision. IEEE Workshop Applications of Computer VisionPalm Springs, CAM. Kilger, \"A shadow handler in a video-based real-time traffic moni- toring system,\" in Proc. IEEE Workshop Applications of Computer Vi- sion, Palm Springs, CA, 1992, pp. 11-18.\n\nTraffic surveillance and detection technology development. Jet Propulsion Laboratory Publication. JPLJPL, \"Traffic surveillance and detection technology development,\" Sensor Development Final Rep., Jet Propulsion Laboratory Publication no. 97-10, 1997.\n\nA machine vision based surveillance system for Californaia roads. J Malik, S Russell, J Weber, T Huang, D Koller, Univ. of California, PATH project MOU-83 Final RepJ. Malik, S. Russell, J. Weber, T. Huang, and D. Koller, \"A machine vision based surveillance system for Californaia roads,\" Univ. of Cali- fornia, PATH project MOU-83 Final Rep., Nov. 1994.\n\nTraffic Surveillance and Detection Technology Development: New Traffic Sensor Technology. J Malik, S Russell, UCB-ITS-PRR-97-6Univ. of California. J. Malik and S. Russell, \"Traffic Surveillance and Detection Technology Development: New Traffic Sensor Technology,\" Univ. of California, Berkeley, California PATH Research Final Rep., UCB-ITS-PRR-97-6, 1997.\n\nInteractive model-based vehicle tracking. W F Gardner, D T Lawton, IEEE Trans. Pattern Anal. Machine Intell. 18W. F. Gardner and D. T. Lawton, \"Interactive model-based vehicle tracking,\" IEEE Trans. Pattern Anal. Machine Intell., vol. 18, pp. 1115-1121, Nov. 1996.\n\nDynamic image sequence analysis using fuzzy measures. Z Q Liu, L T Bruton, J C Bezdek, J M Keller, S Dance, N R Bartley, C Zhang, IEEE Trans. Syst. 31Man, Cybern. BZ. Q. Liu, L. T. Bruton, J. C. Bezdek, J. M. Keller, S. Dance, N. R. Bartley, and C. Zhang, \"Dynamic image sequence analysis using fuzzy measures,\" IEEE Trans. Syst., Man, Cybern. B, vol. 31, pp. 557-571, Aug. 2001.\n\nRecognizing 3-D objects using surface descriptions. T J Fan, G Medioni, G Nevatia, IEEE Trans. Pattern Recognit. Machine Intell. 11T. J. Fan, G. Medioni, and G. Nevatia, \"Recognizing 3-D objects using surface descriptions,\" IEEE Trans. Pattern Recognit. Machine Intell., vol. 11, pp. 1140-1157, Nov. 1989.\n\nA real-time computer vision system for vehicle tracking and traffic surveillance. B Coifman, D Beymer, P Mclauchlan, J Malik, Transportation Res.: Part C. 64B. Coifman, D. Beymer, P. McLauchlan, and J. Malik, \"A real-time com- puter vision system for vehicle tracking and traffic surveillance,\" Trans- portation Res.: Part C, vol. 6, no. 4, pp. 271-288, 1998.\n\nTraffic surveillance and detection technology development (new traffic sensor technology). J Malik, S Russell, Univ. of California. J. Malik and S. Russell, \"Traffic surveillance and detection technology development (new traffic sensor technology),\" Univ. of California, Berkeley, 1996.\n\nTraffic sensor using a color vision method. C A Pau, A Barber, Proc. SPIE-Transportation Sensors and Controls: Collision Avoidance. SPIE-Transportation Sensors and Controls: Collision Avoidance2902C. A. Pau and A. Barber, \"Traffic sensor using a color vision method,\" in Proc. SPIE-Transportation Sensors and Controls: Collision Avoid- ance, Traffic Management, and ITS, vol. 2902, 1996, pp. 156-165.\n\nVodel-free tracking of cars and people based on color regions. B Schiele, Proc. IEEE Int. Workshop Performance Evaluation of Tracking and Surveillance. IEEE Int. Workshop Performance Evaluation of Tracking and SurveillanceGrenobleB. Schiele, \"Vodel-free tracking of cars and people based on color regions,\" in Proc. IEEE Int. Workshop Performance Evaluation of Tracking and Surveillance, Grenoble, France, 2000, pp. 61-71.\n\n3D articulated models and multi-view tracking with physical forces. Q Delamarre, O Faugeras, Comput. Vis. Image Understanding. 813Q. Delamarre and O. Faugeras, \"3D articulated models and multi-view tracking with physical forces,\" Comput. Vis. Image Understanding, vol. 81, no. 3, pp. 328-357, 2001.\n\n3D articulated models and multi-view tracking with silhouettes. Proc. Int. Conf. Computer Vision. Int. Conf. Computer VisionKerkyra, Greece, \"3D articulated models and multi-view tracking with silhou- ettes,\" in Proc. Int. Conf. Computer Vision, Kerkyra, Greece, 1999, pp. 716-721.\n\nCovariance scaled sampling for monocular 3D body tracking. C Sminchisescu, B Triggs, Proc. IEEE Conf. Computer Vision and Pattern Recognition. IEEE Conf. Computer Vision and Pattern RecognitionKauai, HI454C. Sminchisescu and B. Triggs, \"Covariance scaled sampling for monoc- ular 3D body tracking,\" in Proc. IEEE Conf. Computer Vision and Pat- tern Recognition, Kauai, HI, 2001, pp. I:447-I:454.\n\nArticulated soft objects for video-based body modeling. R Plankers, P Fua, Proc. Int. Conf. Computer Vision. Int. Conf. Computer VisionVancouver, BC, CanadaR. Plankers and P. Fua, \"Articulated soft objects for video-based body modeling,\" in Proc. Int. Conf. Computer Vision, Vancouver, BC, Canada, 2001, pp. 394-401.\n\nLearning a highly structured motion model for 3D human tracking. T Zhao, T S Wang, H Y Shum, Proc. Asian Conf. Computer Vision. Asian Conf. Computer VisionMelbourne, AustraliaT. Zhao, T. S. Wang, and H. Y. Shum, \"Learning a highly structured motion model for 3D human tracking,\" in Proc. Asian Conf. Computer Vision, Melbourne, Australia, 2002, pp. 144-149.\n\nCapture and representation of human walking in live video sequence. J C Cheng, J M F Moura, IEEE Trans. Multimedia. 1J. C. Cheng and J. M. F. Moura, \"Capture and representation of human walking in live video sequence,\" IEEE Trans. Multimedia, vol. 1, pp. 144-156, June 1999.\n\nLearning and recognizing human dynamics in video sequences. C Bregler, Proc. IEEE Conf. Computer Vision and Pattern Recognition. IEEE Conf. Computer Vision and Pattern RecognitionSan Juan, Puerto RicoC. Bregler, \"Learning and recognizing human dynamics in video se- quences,\" in Proc. IEEE Conf. Computer Vision and Pattern Recogni- tion, San Juan, Puerto Rico, 1997, pp. 568-574.\n\nStochastic tracking of 3D human figures using 2D image motion. H Sidenbladh, M Black, Proc. European Conf. Computer Vision. European Conf. Computer VisionDublin, IrelandH. Sidenbladh and M. Black, \"Stochastic tracking of 3D human figures using 2D image motion,\" in Proc. European Conf. Computer Vision, Dublin, Ireland, 2000, pp. 702-718.\n\nA dynamic human model using hybrid 2D-3D representation in hierarchical PCA space. E Ong, S Gong, Proc. British Machine Vision Conf. British Machine Vision ConfU.KE. Ong and S. Gong, \"A dynamic human model using hybrid 2D-3D representation in hierarchical PCA space,\" in Proc. British Machine Vi- sion Conf., U.K., 1999, pp. 33-42.\n\nEfficient and robust vehicle localization. H Yang, J G Lou, H Z Sun, W M Hu, T N Tan, Proc. IEEE Int. Conf. Image Processing. IEEE Int. Conf. Image essingH. Yang, J. G. Lou, H. Z. Sun, W. M. Hu, and T. N. Tan, \"Efficient and robust vehicle localization,\" in Proc. IEEE Int. Conf. Image Processing, 2001, pp. 355-358.\n\nFitting parameterized 3-D models to images. D G Lowe, IEEE Trans. Pattern Anal. Machine Intell. 13D. G. Lowe, \"Fitting parameterized 3-D models to images,\" IEEE Trans. Pattern Anal. Machine Intell., vol. 13, pp. 441-450, May 1991.\n\nThe FERET evaluation methodology for face recognition algorithms. J Phillips, H Moon, S Rizvi, P Rause, IEEE Trans. Pattern Anal. Machine Intell. 22J. Phillips, H. Moon, S. Rizvi, and P. Rause, \"The FERET evaluation methodology for face recognition algorithms,\" IEEE Trans. Pattern Anal. Machine Intell., vol. 22, pp. 1090-1104, Oct. 2000.\n\nA match moving technique for merging CG cloth and human video. J Hoshino, H Saito, M Yamamoto, J. Visualiz. Comput. Animation. 121J. Hoshino, H. Saito, and M. Yamamoto, \"A match moving technique for merging CG cloth and human video,\" J. Visualiz. Comput. Animation, vol. 12, no. 1, pp. 23-29, 2001.\n\nMCMC for nonlinear hierarchical models. J E Bennett, A Racine-Poon, J C Wakefield, Markov Chain Monte Carlo in Practice, W. R. Gilks, S. Richardson, and D. J. SpiegelhalterChapman and HallLondon, U.KJ. E. Bennett, A. Racine-Poon, and J. C. Wakefield, \"MCMC for non- linear hierarchical models,\" in Markov Chain Monte Carlo in Prac- tice, W. R. Gilks, S. Richardson, and D. J. Spiegelhalter, Eds. London, U.K.: Chapman and Hall, 1996, pp. 339-357.\n\nCONDENSATION-Conditional density propagation for visual tracking. M Isard, A Blake, Int. J. Comput. Vis. 291M. Isard and A. Blake, \"CONDENSATION-Conditional density prop- agation for visual tracking,\" Int. J. Comput. Vis., vol. 29, no. 1, pp. 5-28, 1998.\n\nCondensation: unifying low-level and high-level tracking in a stochastic framework. Proc. European Conf. Computer Vision. European Conf. Computer Vision1, \"Condensation: unifying low-level and high-level tracking in a stochastic framework,\" in Proc. European Conf. Computer Vision, vol. 1, 1998, pp. 893-909.\n\nAssociation of motion verbs with vehicle movements extracted from dense optical flow fields. H Kollnig, H H Nagel, M Otte, Proc. European Conf. Computer Vision. European Conf. Computer VisionH. Kollnig, H. H. Nagel, and M. Otte, \"Association of motion verbs with vehicle movements extracted from dense optical flow fields,\" in Proc. European Conf. Computer Vision, 1994, pp. 338-347.\n\nEfficient image gradient based vehicle localization. T N Tan, K D Baker, IEEE Trans. Image Processing. 9T. N. Tan and K. D. Baker, \"Efficient image gradient based vehicle lo- calization,\" IEEE Trans. Image Processing, vol. 9, pp. 1343-1356, Aug. 2000.\n\nPose determination and recognition of vehicles in traffic scenes. T N Tan, G D Sullivan, K D Baker, European Conf. Computer Vision-Lecture Notes in Computer Science. J. O. EklundhStockholm, Sweden1T. N. Tan, G. D. Sullivan, and K. D. Baker, \"Pose determination and recognition of vehicles in traffic scenes,\" in European Conf. Computer Vision-Lecture Notes in Computer Science, vol. 1, J. O. Eklundh, Ed., Stockholm, Sweden, 1994, pp. 501-506.\n\nFast vehicle localization and recognition without line extraction. Proc. British Machine Vision Conf. British Machine Vision Conf, \"Fast vehicle localization and recognition without line extraction,\" in Proc. British Machine Vision Conf., 1994, pp. 85-94.\n\nTracking without feature detection. A E C Pece, A D Worrall, Proc. IEEE Int. Workshop Performance Evaluation of Tracking and Surveillance. IEEE Int. Workshop Performance Evaluation of Tracking and SurveillanceGrenoble, FranceA. E. C. Pece and A. D. Worrall, \"Tracking without feature detection,\" in Proc. IEEE Int. Workshop Performance Evaluation of Tracking and Surveillance, Grenoble, France, 2000, pp. 29-37.\n\nA statistically-based Newton method for pose refinement. Image Vis. Comput. 168, \"A statistically-based Newton method for pose refinement,\" Image Vis. Comput., vol. 16, no. 8, pp. 541-544, June 1998.\n\nModel-based object tracking in monocular image sequences of road traffic scenes. D Koller, K Daniilidis, H.-H Nagel, Int. J. Comput. Vis. 103D. Koller, K. Daniilidis, and H.-H. Nagel, \"Model-based object tracking in monocular image sequences of road traffic scenes,\" Int. J. Comput. Vis., vol. 10, no. 3, pp. 257-281, 1993.\n\n3D pose estimation by directly matching polyhedral models to gray value gradients. H Kollnig, H.-H Nagel, Int. J. Comput. Vis. 233H. Kollnig and H.-H. Nagel, \"3D pose estimation by directly matching polyhedral models to gray value gradients,\" Int. J. Comput. Vis., vol. 23, no. 3, pp. 283-302, 1997.\n\nCombination of edge element and optical flow estimates for 3D-model-based vehicle tracking in traffic image sequences. M Haag, H.-H Nagel, Int. J. Comput. Vis. 353M. Haag and H.-H. Nagel, \"Combination of edge element and optical flow estimates for 3D-model-based vehicle tracking in traffic image se- quences,\" Int. J. Comput. Vis., vol. 35, no. 3, pp. 295-319, 1999.\n\nRecognition of dexterous manipulations from time varying images. K Takahashi, S Seki, H Kojima, R Oka, Proc. IEEE Workshop Motion of Non-Rigid and Articulated Objects. IEEE Workshop Motion of Non-Rigid and Articulated ObjectsAustin, TXK. Takahashi, S. Seki, H. Kojima, and R. Oka, \"Recognition of dexterous manipulations from time varying images,\" in Proc. IEEE Workshop Mo- tion of Non-Rigid and Articulated Objects, Austin, TX, 1994, pp. 23-28.\n\nA state-based technique to the representation and recognition of gesture. A F Bobick, A D Wilson, IEEE Trans. Pattern Anal. Machine Intell. 19A. F. Bobick and A. D. Wilson, \"A state-based technique to the represen- tation and recognition of gesture,\" IEEE Trans. Pattern Anal. Machine Intell., vol. 19, pp. 1325-1337, Dec. 1997.\n\nTemporal classification of natural gesture and application to video coding. A D Wilson, A F Bobick, J Cassell, Proc. IEEE Conf. Computer Vision and Pattern Recognition. IEEE Conf. Computer Vision and Pattern RecognitionA. D. Wilson, A. F. Bobick, and J. Cassell, \"Temporal classification of natural gesture and application to video coding,\" in Proc. IEEE Conf. Computer Vision and Pattern Recognition, 1997, pp. 948-954.\n\nEditorial: understanding visual behavior. S G Gong, H Buxton, Image Vis. Comput. 2012S. G. Gong and H. Buxton, \"Editorial: understanding visual behavior,\" Image Vis. Comput., vol. 20, no. 12, pp. 825-826, 2002.\n\nScenario recognition in airborne video imagery. F Bremond, G Medioni, Proc. Int. Workshop Interpretation of Visual Motion. Int. Workshop Interpretation of Visual MotionF. Bremond and G. Medioni, \"Scenario recognition in airborne video imagery,\" in Proc. Int. Workshop Interpretation of Visual Motion, 1998, pp. 57-64.\n\nReal-time American sign language recognition using desk and wearable computer-based video. T Starner, J Weaver, A Pentland, IEEE Trans. Pattern Anal. Machine Intell. 20T. Starner, J. Weaver, and A. Pentland, \"Real-time American sign language recognition using desk and wearable computer-based video,\" IEEE Trans. Pattern Anal. Machine Intell., vol. 20, pp. 1371-1375, Dec. 1998.\n\nA Bayesian computer vision system for modeling human interactions. N M Oliver, B Rosario, A P Pentland, IEEE Trans. Pattern Anal. Machine Intell. 22N. M. Oliver, B. Rosario, and A. P. Pentland, \"A Bayesian computer vision system for modeling human interactions,\" IEEE Trans. Pattern Anal. Machine Intell., vol. 22, pp. 831-843, Aug. 2000.\n\nDiscovery and segmentation of activities in video. M Brand, V Kettnaker, IEEE Trans. Pattern Anal. Machine Intell. 22M. Brand and V. Kettnaker, \"Discovery and segmentation of activities in video,\" IEEE Trans. Pattern Anal. Machine Intell., vol. 22, pp. 844-851, Aug. 2000.\n\nExtraction and classification of visual motion pattern recognition. M Yang, N Ahuja, Proc. IEEE Conf. Computer Vision and Pattern Recognition. IEEE Conf. Computer Vision and Pattern RecognitionM. Yang and N. Ahuja, \"Extraction and classification of visual motion pattern recognition,\" in Proc. IEEE Conf. Computer Vision and Pattern Recognition, 1998, pp. 892-897.\n\nToward unrestricted lip reading. U Meier, R Stiefelhagen, J Yang, A Waibel, Int. J. Pattern Recognit. Artificial Intell. 145U. Meier, R. Stiefelhagen, J. Yang, and A. Waibel, \"Toward unrestricted lip reading,\" Int. J. Pattern Recognit. Artificial Intell., vol. 14, no. 5, pp. 571-585, Aug 2000.\n\nRecognition of visual activities and interactions by stochastic parsing. Y A Ivanov, A F Boblic, IEEE Trans. Pattern Anal. Machine Intell. 22Y. A. Ivanov and A. F. Boblic, \"Recognition of visual activities and in- teractions by stochastic parsing,\" IEEE Trans. Pattern Anal. Machine Intell., vol. 22, pp. 852-872, Aug. 2000.\n\nUnderstanding manipulation in video. M Brand, Proc. Int. Conf. Automatic Face and Gesture Recognition. Int. Conf. Automatic Face and Gesture RecognitionM. Brand, \"Understanding manipulation in video,\" in Proc. Int. Conf. Automatic Face and Gesture Recognition, 1996, pp. 94-99.\n\nMulti-object behavior recognition by event driven selective attention method. T Wada, T Matsuyama, IEEE Trans. Pattern Anal. Machine Intell. 22T. Wada and T. Matsuyama, \"Multi-object behavior recognition by event driven selective attention method,\" IEEE Trans. Pattern Anal. Machine Intell., vol. 22, pp. 873-887, Aug. 2000.\n\nLearning the distribution of object trajectories for event recognition. N Johnson, D Hogg, Image Vis. Comput. 148N. Johnson and D. Hogg, \"Learning the distribution of object trajectories for event recognition,\" Image Vis. Comput., vol. 14, no. 8, pp. 609-615, 1996.\n\nLearning spatio-temporal patterns for predicting object behavior. N Sumpter, A Bulpitt, Image Vis. Comput. 189N. Sumpter and A. Bulpitt, \"Learning spatio-temporal patterns for predicting object behavior,\" Image Vis. Comput., vol. 18, no. 9, pp. 697-704, 2000.\n\nApplication of the self-organizing map to trajectory classification. J Owens, A Hunter, Proc. IEEE Int. Workshop Visual Surveillance. IEEE Int. Workshop Visual SurveillanceJ. Owens and A. Hunter, \"Application of the self-organizing map to tra- jectory classification,\" in Proc. IEEE Int. Workshop Visual Surveillance, 2000, pp. 77-83.\n\nVisual translator: linking perceptions and natural language descriptions. G Herzog, P Wazinski, Artific. Intell. Rev. 82G. Herzog and P. Wazinski, \"Visual translator: linking perceptions and natural language descriptions,\" Artific. Intell. Rev., vol. 8, no. 2, pp. 175-187, 1994.\n\nAutomatic symbolic traffic scene analysis using belief networks. T Huang, D Koller, J Malik, G Ogasawara, B Rao, S Russell, J Weber, Proc. National Conf. Artificial Intelligence. National Conf. Artificial IntelligenceT. Huang, D. Koller, J. Malik, G. Ogasawara, B. Rao, S. Russell, and J. Weber, \"Automatic symbolic traffic scene analysis using belief networks,\" in Proc. National Conf. Artificial Intelligence, 1994, pp. 966-972.\n\nAgent orientated annotation in model based visual surveillance. P Remagnino, T Tan, K Baker, Proc. IEEE Int. Conf. Computer Vision. IEEE Int. Conf. Computer VisionP. Remagnino, T. Tan, and K. Baker, \"Agent orientated annotation in model based visual surveillance,\" in Proc. IEEE Int. Conf. Computer Vision, 1998, pp. 857-862.\n\nOn the use of motion concepts for top-down control in traffic scene. M Mohnhaupy, B Neumann, Proc. European Conf. Computer Vision. European Conf. Computer VisionM. Mohnhaupy and B. Neumann, \"On the use of motion concepts for top-down control in traffic scene,\" in Proc. European Conf. Computer Vision, 1990, pp. 542-550.\n\nComputational Models for Integrating Language and Vision. Proc. AAAI Fall Symp. R. K. SrihariAAAI Fall SympProc. AAAI Fall Symp. Computational Models for Integrating Language and Vision, R. K. Srihari, Ed..\n\n. M A Cambridge, Cambridge, MA, November 1995.\n\nMulti-agent visual surveillance of dynamic scenes. P Remagnino, T N Tan, A D Worrall, K D Baker, Image Vis. Comput. 168P. Remagnino, T. N. Tan, A. D. Worrall, and K. D. Baker, \"Multi-agent visual surveillance of dynamic scenes,\" Image Vis. Comput., vol. 16, no. 8, pp. 529-532, 1998.\n\nUrban surveillance system: from the laboratory to the commercial world. I Pavlidis, V Morellas, P Tsiamyrtzis, S Harp, Proc. IEEE. IEEE89I. Pavlidis, V. Morellas, P. Tsiamyrtzis, and S. Harp, \"Urban surveillance system: from the laboratory to the commercial world,\" Proc. IEEE, vol. 89, pp. 1478-1497, Oct. 2001.\n\nCamera handoff: tracking in multiple uncalibrated stationary cameras. O Javed, S Khan, Z Rasheed, M Shah, Proc. IEEE Workshop Human Motion (HUMO'00). IEEE Workshop Human Motion (HUMO'00)Austin, TXO. Javed, S. Khan, Z. Rasheed, and M. Shah, \"Camera handoff: tracking in multiple uncalibrated stationary cameras,\" in Proc. IEEE Workshop Human Motion (HUMO'00), Austin, TX, 2000, pp. 113-118.\n\nMultiple camera tracking of interacting and occluded human motion. S L Dockstader, A M Tekalp, Proc. IEEE. IEEE89S. L. Dockstader and A. M. Tekalp, \"Multiple camera tracking of interacting and occluded human motion,\" Proc. IEEE, vol. 89, pp. 1441-1455, Oct. 2001.\n\nAlgorithms for cooperative multi-sensor surveillance. R T Collins, A J Lipton, H Fujiyoshi, T Kanade, Proc. IEEE. IEEE89R. T. Collins, A. J. Lipton, H. Fujiyoshi, and T. Kanade, \"Algorithms for cooperative multi-sensor surveillance,\" Proc. IEEE, vol. 89, pp. 1456-1477, Oct. 2001.\n\nOptical flow-based person tracking by multiple cameras. H Tsutsui, J Miura, Y Shirai, Proc. IEEE Conf. Multisensor Fusion and Integration in Intelligent Systems. IEEE Conf. Multisensor Fusion and Integration in Intelligent SystemsH. Tsutsui, J. Miura, and Y. Shirai, \"Optical flow-based person tracking by multiple cameras,\" in Proc. IEEE Conf. Multisensor Fusion and In- tegration in Intelligent Systems, 2001, pp. 91-96.\n\nM2 tracker: a multi-view approach to segmenting and tracking people in a cluttered scene. A Mittal, L S Davis, Proc. European Conf. Computer Vision. European Conf. Computer Vision1A. Mittal and L. S. Davis, \"M2 tracker: a multi-view approach to seg- menting and tracking people in a cluttered scene,\" in Proc. European Conf. Computer Vision, vol. 1, 2002, pp. 18-36.\n\nA generic deformable model for vehicle recognition. J M Ferryman, A D Worrall, G D Sullivan, K D Baker, Proc. British Machine Vision Conf. British Machine Vision ConfJ. M. Ferryman, A. D. Worrall, G. D. Sullivan, and K. D. Baker, \"A generic deformable model for vehicle recognition,\" in Proc. British Ma- chine Vision Conf., 1995, pp. 127-136.\n\nUsing gait as a biometric: via phase-weighted magnitude spectra. D Cunado, M S Nixon, J N Carter, Proc. Int. Conf. Audio-and Video-Based Biometric Person Authentication. Int. Conf. Audio-and Video-Based Biometric Person AuthenticationD. Cunado, M. S. Nixon, and J. N. Carter, \"Using gait as a biometric: via phase-weighted magnitude spectra,\" in Proc. Int. Conf. Audio-and Video-Based Biometric Person Authentication, 1997, pp. 95-102.\n\nExtracting a human gait model for use as a biometric. Proc. null, \"Extracting a human gait model for use as a biometric,\" in Proc.\n\n(IEE) Colloq. Computer Vision for Virtual Human Modeling. Inst. Elect. Eng.Inst. Elect. Eng. (IEE) Colloq. Computer Vision for Virtual Human Mod- eling, 1998, pp. 11/1-11/4.\n\nDynamic feature extraction via the velocity Hough transform. J M Nash, J N Carter, M S Nixon, Pattern Recognit. Lett. 1810J. M. Nash, J. N. Carter, and M. S. Nixon, \"Dynamic feature extraction via the velocity Hough transform,\" Pattern Recognit. Lett., vol. 18, no. 10, pp. 1035-1047, 1997.\n\nExtended model-based automatic gait recognition of walking and running. C Y Yam, M S Nixon, J N Carter, Proc. Int. Conf. Audio-and Video-Based Biometric Person Authentication. Int. Conf. Audio-and Video-Based Biometric Person AuthenticationC. Y. Yam, M. S. Nixon, and J. N. Carter, \"Extended model-based automatic gait recognition of walking and running,\" in Proc. Int. Conf. Audio-and Video-Based Biometric Person Authentication, 2001, pp. 278-283.\n\nGait recognition by walking and running: a model-based approach. Proc. Asia Conf. Computer Vision. Asia Conf. Computer VisionMelbourne, Australian, \"Gait recognition by walking and running: a model-based ap- proach,\" in Proc. Asia Conf. Computer Vision, Melbourne, Australian, 2002, pp. 1-6.\n\nGait extraction and description by evidence gathering. D Cunado, J Nash, M S Nixon, J N Carter, Proc. Int. Conf. Audio-and Video-Based Biometric Person Authentication. Int. Conf. Audio-and Video-Based Biometric Person AuthenticationD. Cunado, J. Nash, M. S. Nixon, and J. N. Carter, \"Gait extraction and description by evidence gathering,\" in Proc. Int. Conf. Audio-and Video- Based Biometric Person Authentication, 1999, pp. 43-48.\n\nGait recognition from time-normalized joint-angle trajectories in the walking plane. R Tanawongsuwan, A Bobick, Proc. IEEE Conf. Computer Vision and Pattern Recognition. IEEE Conf. Computer Vision and Pattern Recognition731II. IIR. Tanawongsuwan and A. Bobick, \"Gait recognition from time-normal- ized joint-angle trajectories in the walking plane,\" in Proc. IEEE Conf. Computer Vision and Pattern Recognition, 2001, pp. (II)726-(II)731.\n\nMoving object recognition in eigenspace representation: gait analysis and lip reading. H Murase, R Sakai, Pattern Recognit. Lett. 172H. Murase and R. Sakai, \"Moving object recognition in eigenspace rep- resentation: gait analysis and lip reading,\" Pattern Recognit. Lett., vol. 17, no. 2, pp. 155-162, 1996.\n\nHuman gait recognition in canonical space using temporal templates. P S Huang, C J Harris, M S Nixon, Proc. Inst. Elect. Eng. (IEE) Vision Image and Signal Processing. Inst. Elect. Eng. (IEE) Vision Image and Signal essing146P. S. Huang, C. J. Harris, and M. S. Nixon, \"Human gait recognition in canonical space using temporal templates,\" Proc. Inst. Elect. Eng. (IEE) Vision Image and Signal Processing, vol. 146, no. 2, pp. 93-100, 1999.\n\nComparing different template features for recognizing people by their gait. Proc. British Machine Vision Conf. British Machine Vision Conf, \"Comparing different template features for recognizing people by their gait,\" in Proc. British Machine Vision Conf., 1998, pp. 639-643.\n\nCanonical space representation for recognizing humans by gait or face. Proc. IEEE Southwest Symp. Image Analysis and Interpretation. IEEE Southwest Symp. Image Analysis and Interpretation, \"Canonical space representation for recognizing humans by gait or face,\" in Proc. IEEE Southwest Symp. Image Analysis and Interpre- tation, 1998, pp. 180-185.\n\nStatistical gait recognition via temporal moments. J D Shutler, M S Nixon, C J Harris, Proc. IEEE Southwest Symp. Image Analysis and Interpretation. IEEE Southwest Symp. Image Analysis and InterpretationJ. D. Shutler, M. S. Nixon, and C. J. Harris, \"Statistical gait recognition via temporal moments,\" in Proc. IEEE Southwest Symp. Image Analysis and Interpretation, 2000, pp. 291-295.\n\nGait Dynamics for Recognition and Classification. L Lee, AIM-2001-019MIT AI Lab. Tech. Rep.L. Lee, \"Gait Dynamics for Recognition and Classification,\" MIT AI Lab, Cambridge, MA, Tech. Rep. AIM-2001-019, 2001.\n\nTracking and object classification for automated surveillance. O Javed, M Shah, Proc. European Conf. Computer Vision. European Conf. Computer Vision4O. Javed and M. Shah, \"Tracking and object classification for automated surveillance,\" in Proc. European Conf. Computer Vision, vol. 4, 2002, pp. 343-357.\n\nGait recognition using static, activity-specific parameters. A Bobick, A Johnson, Proc. IEEE Conf. Computer Vision and Pattern Recognition. IEEE Conf. Computer Vision and Pattern Recognition430A. Bobick and A. Johnson, \"Gait recognition using static, activity-spe- cific parameters,\" in Proc. IEEE Conf. Computer Vision and Pattern Recognition, 2001, pp. (I)423-(I)430.\n\nA multi-view method for gait recognition using static body parameters. A Johnson, A Bobick, Proc. Int. Conf. Audio-and Video-Based Biometric Person Authentication. Int. Conf. Audio-and Video-Based Biometric Person AuthenticationA. Johnson and A. Bobick, \"A multi-view method for gait recognition using static body parameters,\" in Proc. Int. Conf. Audio-and Video- Based Biometric Person Authentication, 2001, pp. 301-311.\n\nExpected Confusion as a Method of Evaluating Recognition Techniques. A Bobick, A Johnson, GIT-GVU-01-10Tech. Rep.A. Bobick and A. Johnson. (2001) Expected Confusion as a Method of Evaluating Recognition Techniques. [Online]GVU Tech. Rep. GIT-GVU-01-10\n\nPerson identification using automatic height and stride estimation. C Benabdelkader, R Culter, L Davis, Proc. Int. Conf. Pattern Recognition. Int. Conf. Pattern RecognitionQu\u00e9bec, Canada4C. BenAbdelkader, R. Culter, and L. Davis, \"Person identification using automatic height and stride estimation,\" in Proc. Int. Conf. Pattern Recognition, vol. 4, Qu\u00e9bec, Canada, 2002, pp. 377-380.\n\nVisual vehicle tracking using an improved EKF. J G Lou, H Yang, W M Hu, T N Tan, Proc. Asian Conf. Computer Vision. Asian Conf. Computer VisionJ. G. Lou, H. Yang, W. M. Hu, and T. N. Tan, \"Visual vehicle tracking using an improved EKF,\" in Proc. Asian Conf. Computer Vision, 2002, pp. 296-301.\n\nA hierarchical self-organizing approach for learning the patterns of motion trajectories. W M Hu, D Xie, T N Tan, Chin. J. Comput. 264W. M. Hu, D. Xie, and T. N. Tan, \"A hierarchical self-organizing approach for learning the patterns of motion trajectories,\" Chin. J. Comput., vol. 26, no. 4, pp. 417-426, 2003.\n\nThe recognition of human movement using temporal templates. A Bobick, J Davis, IEEE Trans. Pattern Recognit. Machine Intell. 23A. Bobick and J. Davis, \"The recognition of human movement using temporal templates,\" IEEE Trans. Pattern Recognit. Machine Intell., vol. 23, pp. 257-267, Mar. 2001.\n\nMotion-based recognition of people in eigengait space. C Benabdelkader, R Culter, L Davis, Proc. Int. Conf. Automatic Face and Gesture Recognition. Int. Conf. Automatic Face and Gesture RecognitionWashington, DCC. BenAbdelkader, R. Culter, and L. Davis, \"Motion-based recognition of people in eigengait space,\" in Proc. Int. Conf. Automatic Face and Gesture Recognition, Washington, DC, 2002, pp. 267-274.\n\nSilhouette-based human identification from body shape and gait. R Collins, R Gross, J Shi, Proc. Int. Conf. Automatic Face and Gesture Recognition. Int. Conf. Automatic Face and Gesture RecognitionWashington, DCR. Collins, R. Gross, and J. Shi, \"Silhouette-based human identification from body shape and gait,\" in Proc. Int. Conf. Automatic Face and Ges- ture Recognition, Washington, DC, 2002, pp. 366-371.\n\nDetection of load-carrying people for gait and activity recognition. C Benabdelkader, L Davis, Proc. Int. Conf. Automatic Face and Gesture Recognition. Int. Conf. Automatic Face and Gesture RecognitionWashington, DC, USAC. BenAbdelkader and L. Davis, \"Detection of load-carrying people for gait and activity recognition,\" in Proc. Int. Conf. Automatic Face and Gesture Recognition, Washington, DC, USA, 2002, pp. 378-383.\n\nIndividual recognition by kniematic-based gait analysis. B Bhanu, J Han, Proc. Int. Conf. Pattern Recognition. Int. Conf. Pattern RecognitionQu\u00e9bec, Canada3B. Bhanu and J. Han, \"Individual recognition by kniematic-based gait analysis,\" in Proc. Int. Conf. Pattern Recognition, vol. 3, Qu\u00e9bec, Canada, 2002, pp. 343-346.\n\nBiologically-motivated human gait classifiers. V Laxmi, J Carter, R Damper, Proc. IEEE Workshop Automatic Identification Advanced Technologies. IEEE Workshop Automatic Identification Advanced TechnologiesV. Laxmi, J. Carter, and R. Damper, \"Biologically-motivated human gait classifiers,\" in Proc. IEEE Workshop Automatic Identification Advanced Technologies, 2002, pp. 17-22.\n\nExperiments on gait analysis by exploiting nonstationaryity in the distribution of feature relationships. I Robledo, S Sarkar, Proc. Int. Conf. Pattern Recognition. Int. Conf. Pattern RecognitionQu\u00e9bec, Canada1I. Robledo and S. Sarkar, \"Experiments on gait analysis by exploiting nonstationaryity in the distribution of feature relationships,\" in Proc. Int. Conf. Pattern Recognition, vol. 1, Qu\u00e9bec, Canada, 2002, pp. 1-4.\n\nView-invariant estimation of height and stride for gait recognition. C Benabdelkader, R Cutler, L Davis, Proc. Workshop Biometric Authentication at European Conf. Computer Vision. Workshop Biometric Authentication at European Conf. Computer VisionC. BenAbdelkader, R. Cutler, and L. Davis, \"View-invariant estimation of height and stride for gait recognition,\" in Proc. Workshop Biometric Authentication at European Conf. Computer Vision, 2002, pp. 155-167.\n\nA new attempt to gait-based human identification. L Wang, W M Hu, T N Tan, Proc. Int. Conf. Pattern Recognition. Int. Conf. Pattern RecognitionL. Wang, W. M. Hu, and T. N. Tan, \"A new attempt to gait-based human identification,\" in Proc. Int. Conf. Pattern Recognition, 2002, pp. 115-118.\n\nGait recognition based on procrustes statistical shape analysis. L Wang, H Z Ning, W M Hu, Proc. IEEE Int. Conf. Image Processing. IEEE Int. Conf. Image essingL. Wang, H. Z. Ning, and W. M. Hu, \"Gait recognition based on pro- crustes statistical shape analysis,\" in Proc. IEEE Int. Conf. Image Pro- cessing, 2002, pp. III/433-III/436.\n", "annotations": {"author": "[{\"end\":82,\"start\":77},{\"end\":95,\"start\":83},{\"end\":115,\"start\":96}]", "publisher": null, "author_last_name": "[{\"end\":81,\"start\":77},{\"end\":94,\"start\":87},{\"end\":114,\"start\":96}]", "author_first_name": "[{\"end\":86,\"start\":83}]", "author_affiliation": null, "title": "[{\"end\":63,\"start\":1},{\"end\":178,\"start\":116}]", "venue": "[{\"end\":204,\"start\":180}]", "abstract": "[{\"end\":1602,\"start\":248}]", "bib_ref": "[{\"end\":5636,\"start\":5630},{\"end\":5656,\"start\":5650},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":5665,\"start\":5661},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5673,\"start\":5670},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":5779,\"start\":5775},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":5868,\"start\":5864},{\"attributes\":{\"ref_id\":\"b129\"},\"end\":5966,\"start\":5961},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":6142,\"start\":6138},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6543,\"start\":6540},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7265,\"start\":7262},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":7588,\"start\":7584},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7731,\"start\":7728},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7964,\"start\":7961},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8278,\"start\":8274},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8537,\"start\":8534},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8543,\"start\":8539},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8564,\"start\":8561},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8580,\"start\":8577},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":8761,\"start\":8757},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8792,\"start\":8788},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":8798,\"start\":8794},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12908,\"start\":12904},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":12914,\"start\":12910},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":12949,\"start\":12945},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":13005,\"start\":13001},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":13011,\"start\":13007},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":13037,\"start\":13033},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":13143,\"start\":13139},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":13149,\"start\":13145},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":13457,\"start\":13453},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":13660,\"start\":13657},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":13941,\"start\":13937},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":14219,\"start\":14215},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":14420,\"start\":14416},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":14461,\"start\":14457},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":15645,\"start\":15642},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":15651,\"start\":15647},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":15657,\"start\":15653},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":16103,\"start\":16099},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":16690,\"start\":16686},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":16696,\"start\":16692},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":17258,\"start\":17254},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":17452,\"start\":17448},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":17876,\"start\":17873},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":18184,\"start\":18180},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":19141,\"start\":19138},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":19450,\"start\":19446},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":19709,\"start\":19705},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":20012,\"start\":20008},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":20467,\"start\":20463},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":21036,\"start\":21032},{\"attributes\":{\"ref_id\":\"b170\"},\"end\":22106,\"start\":22101},{\"attributes\":{\"ref_id\":\"b89\"},\"end\":22338,\"start\":22334},{\"attributes\":{\"ref_id\":\"b90\"},\"end\":22344,\"start\":22340},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":22455,\"start\":22452},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":23049,\"start\":23045},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":23945,\"start\":23941},{\"attributes\":{\"ref_id\":\"b92\"},\"end\":24003,\"start\":23999},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":24801,\"start\":24798},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":24807,\"start\":24803},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":24813,\"start\":24809},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":24819,\"start\":24815},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":24985,\"start\":24981},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":25149,\"start\":25145},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":25306,\"start\":25302},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":25481,\"start\":25477},{\"attributes\":{\"ref_id\":\"b93\"},\"end\":25487,\"start\":25483},{\"attributes\":{\"ref_id\":\"b99\"},\"end\":26761,\"start\":26756},{\"attributes\":{\"ref_id\":\"b100\"},\"end\":26768,\"start\":26763},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":26793,\"start\":26789},{\"attributes\":{\"ref_id\":\"b97\"},\"end\":27237,\"start\":27233},{\"attributes\":{\"ref_id\":\"b98\"},\"end\":27243,\"start\":27239},{\"attributes\":{\"ref_id\":\"b96\"},\"end\":27384,\"start\":27380},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":27454,\"start\":27450},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":30489,\"start\":30485},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":30982,\"start\":30978},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":31398,\"start\":31394},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":31647,\"start\":31643},{\"attributes\":{\"ref_id\":\"b101\"},\"end\":32413,\"start\":32408},{\"attributes\":{\"ref_id\":\"b102\"},\"end\":32420,\"start\":32415},{\"attributes\":{\"ref_id\":\"b103\"},\"end\":32451,\"start\":32446},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":32601,\"start\":32597},{\"attributes\":{\"ref_id\":\"b105\"},\"end\":32849,\"start\":32844},{\"attributes\":{\"ref_id\":\"b106\"},\"end\":32856,\"start\":32851},{\"attributes\":{\"ref_id\":\"b107\"},\"end\":32906,\"start\":32901},{\"attributes\":{\"ref_id\":\"b108\"},\"end\":32976,\"start\":32971},{\"attributes\":{\"ref_id\":\"b107\"},\"end\":33005,\"start\":33000},{\"attributes\":{\"ref_id\":\"b105\"},\"end\":33231,\"start\":33226},{\"attributes\":{\"ref_id\":\"b108\"},\"end\":33524,\"start\":33519},{\"attributes\":{\"ref_id\":\"b109\"},\"end\":33553,\"start\":33548},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":33806,\"start\":33803},{\"attributes\":{\"ref_id\":\"b101\"},\"end\":34509,\"start\":34504},{\"attributes\":{\"ref_id\":\"b111\"},\"end\":34712,\"start\":34707},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":35043,\"start\":35039},{\"attributes\":{\"ref_id\":\"b113\"},\"end\":35050,\"start\":35045},{\"attributes\":{\"ref_id\":\"b114\"},\"end\":35242,\"start\":35237},{\"attributes\":{\"ref_id\":\"b115\"},\"end\":35286,\"start\":35281},{\"attributes\":{\"ref_id\":\"b116\"},\"end\":35293,\"start\":35288},{\"attributes\":{\"ref_id\":\"b94\"},\"end\":35581,\"start\":35577},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":35636,\"start\":35632},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":35642,\"start\":35638},{\"attributes\":{\"ref_id\":\"b110\"},\"end\":35703,\"start\":35698},{\"attributes\":{\"ref_id\":\"b175\"},\"end\":35710,\"start\":35705},{\"attributes\":{\"ref_id\":\"b123\"},\"end\":35748,\"start\":35743},{\"attributes\":{\"ref_id\":\"b125\"},\"end\":35755,\"start\":35750},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":35944,\"start\":35940},{\"attributes\":{\"ref_id\":\"b118\"},\"end\":35951,\"start\":35946},{\"attributes\":{\"ref_id\":\"b119\"},\"end\":36391,\"start\":36386},{\"attributes\":{\"ref_id\":\"b120\"},\"end\":36551,\"start\":36546},{\"attributes\":{\"ref_id\":\"b120\"},\"end\":36804,\"start\":36799},{\"attributes\":{\"ref_id\":\"b121\"},\"end\":36852,\"start\":36847},{\"attributes\":{\"ref_id\":\"b122\"},\"end\":36859,\"start\":36854},{\"attributes\":{\"ref_id\":\"b110\"},\"end\":37038,\"start\":37033},{\"attributes\":{\"ref_id\":\"b175\"},\"end\":37304,\"start\":37299},{\"attributes\":{\"ref_id\":\"b123\"},\"end\":37813,\"start\":37808},{\"attributes\":{\"ref_id\":\"b177\"},\"end\":39925,\"start\":39920},{\"attributes\":{\"ref_id\":\"b126\"},\"end\":40308,\"start\":40303},{\"attributes\":{\"ref_id\":\"b127\"},\"end\":40315,\"start\":40310},{\"attributes\":{\"ref_id\":\"b127\"},\"end\":40350,\"start\":40345},{\"attributes\":{\"ref_id\":\"b128\"},\"end\":40843,\"start\":40838},{\"attributes\":{\"ref_id\":\"b130\"},\"end\":41117,\"start\":41112},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":41308,\"start\":41304},{\"attributes\":{\"ref_id\":\"b131\"},\"end\":42064,\"start\":42059},{\"attributes\":{\"ref_id\":\"b132\"},\"end\":42135,\"start\":42130},{\"attributes\":{\"ref_id\":\"b133\"},\"end\":42432,\"start\":42427},{\"attributes\":{\"ref_id\":\"b134\"},\"end\":43214,\"start\":43209},{\"attributes\":{\"ref_id\":\"b135\"},\"end\":43236,\"start\":43231},{\"attributes\":{\"ref_id\":\"b136\"},\"end\":43267,\"start\":43262},{\"attributes\":{\"ref_id\":\"b137\"},\"end\":43482,\"start\":43477},{\"attributes\":{\"ref_id\":\"b136\"},\"end\":43587,\"start\":43582},{\"attributes\":{\"ref_id\":\"b138\"},\"end\":44333,\"start\":44328},{\"attributes\":{\"ref_id\":\"b139\"},\"end\":44914,\"start\":44909},{\"attributes\":{\"ref_id\":\"b140\"},\"end\":45256,\"start\":45251},{\"attributes\":{\"ref_id\":\"b139\"},\"end\":45318,\"start\":45313},{\"attributes\":{\"ref_id\":\"b176\"},\"end\":45390,\"start\":45385},{\"attributes\":{\"ref_id\":\"b139\"},\"end\":45416,\"start\":45411},{\"attributes\":{\"ref_id\":\"b141\"},\"end\":45559,\"start\":45554},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":45942,\"start\":45938},{\"attributes\":{\"ref_id\":\"b146\"},\"end\":45949,\"start\":45944},{\"attributes\":{\"ref_id\":\"b142\"},\"end\":45983,\"start\":45978},{\"attributes\":{\"ref_id\":\"b146\"},\"end\":46089,\"start\":46084},{\"attributes\":{\"ref_id\":\"b143\"},\"end\":46383,\"start\":46378},{\"attributes\":{\"ref_id\":\"b144\"},\"end\":46390,\"start\":46385},{\"attributes\":{\"ref_id\":\"b148\"},\"end\":46534,\"start\":46529},{\"attributes\":{\"ref_id\":\"b145\"},\"end\":46949,\"start\":46944},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":47112,\"start\":47108},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":47118,\"start\":47114},{\"attributes\":{\"ref_id\":\"b117\"},\"end\":47865,\"start\":47860},{\"attributes\":{\"ref_id\":\"b95\"},\"end\":48300,\"start\":48296},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":48992,\"start\":48989},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":49050,\"start\":49046},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":49056,\"start\":49052},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":49222,\"start\":49218},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":49228,\"start\":49224},{\"attributes\":{\"ref_id\":\"b156\"},\"end\":49680,\"start\":49675},{\"attributes\":{\"ref_id\":\"b163\"},\"end\":49687,\"start\":49682},{\"attributes\":{\"ref_id\":\"b181\"},\"end\":49694,\"start\":49689},{\"attributes\":{\"ref_id\":\"b182\"},\"end\":49701,\"start\":49696},{\"attributes\":{\"ref_id\":\"b156\"},\"end\":49723,\"start\":49718},{\"attributes\":{\"ref_id\":\"b157\"},\"end\":49730,\"start\":49725},{\"attributes\":{\"ref_id\":\"b159\"},\"end\":49826,\"start\":49821},{\"attributes\":{\"ref_id\":\"b160\"},\"end\":50125,\"start\":50120},{\"attributes\":{\"ref_id\":\"b161\"},\"end\":50132,\"start\":50127},{\"attributes\":{\"ref_id\":\"b162\"},\"end\":50433,\"start\":50428},{\"attributes\":{\"ref_id\":\"b163\"},\"end\":50622,\"start\":50617},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":51648,\"start\":51644},{\"attributes\":{\"ref_id\":\"b164\"},\"end\":51655,\"start\":51650},{\"attributes\":{\"ref_id\":\"b169\"},\"end\":51662,\"start\":51657},{\"attributes\":{\"ref_id\":\"b183\"},\"end\":51669,\"start\":51664},{\"attributes\":{\"ref_id\":\"b185\"},\"end\":51676,\"start\":51671},{\"attributes\":{\"ref_id\":\"b164\"},\"end\":51698,\"start\":51693},{\"attributes\":{\"ref_id\":\"b165\"},\"end\":51839,\"start\":51834},{\"attributes\":{\"ref_id\":\"b167\"},\"end\":51846,\"start\":51841},{\"attributes\":{\"ref_id\":\"b166\"},\"end\":52123,\"start\":52118},{\"attributes\":{\"ref_id\":\"b168\"},\"end\":52238,\"start\":52233},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":52357,\"start\":52353},{\"attributes\":{\"ref_id\":\"b169\"},\"end\":52364,\"start\":52359},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":53148,\"start\":53144},{\"attributes\":{\"ref_id\":\"b171\"},\"end\":53155,\"start\":53150},{\"attributes\":{\"ref_id\":\"b174\"},\"end\":53162,\"start\":53157},{\"attributes\":{\"ref_id\":\"b184\"},\"end\":53169,\"start\":53164},{\"attributes\":{\"ref_id\":\"b171\"},\"end\":53292,\"start\":53287},{\"attributes\":{\"ref_id\":\"b172\"},\"end\":53299,\"start\":53294},{\"attributes\":{\"ref_id\":\"b173\"},\"end\":53689,\"start\":53684},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":53804,\"start\":53800},{\"attributes\":{\"ref_id\":\"b174\"},\"end\":53811,\"start\":53806},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":54077,\"start\":54073},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":54873,\"start\":54869},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":54879,\"start\":54875},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":54885,\"start\":54881},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":54891,\"start\":54887},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":54897,\"start\":54893},{\"attributes\":{\"ref_id\":\"b178\"},\"end\":54904,\"start\":54899},{\"attributes\":{\"ref_id\":\"b179\"},\"end\":54911,\"start\":54906},{\"attributes\":{\"ref_id\":\"b186\"},\"end\":54918,\"start\":54913},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":55057,\"start\":55053},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":55377,\"start\":55373},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":55788,\"start\":55784},{\"attributes\":{\"ref_id\":\"b178\"},\"end\":55795,\"start\":55790},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":55958,\"start\":55954},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":56035,\"start\":56031},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":56949,\"start\":56945},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":57401,\"start\":57397},{\"attributes\":{\"ref_id\":\"b149\"},\"end\":59301,\"start\":59296},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":60165,\"start\":60161},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":60171,\"start\":60167},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":60890,\"start\":60886},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":60896,\"start\":60892},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":61059,\"start\":61055},{\"attributes\":{\"ref_id\":\"b150\"},\"end\":61267,\"start\":61262},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":61742,\"start\":61738},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":61748,\"start\":61744},{\"attributes\":{\"ref_id\":\"b151\"},\"end\":62087,\"start\":62082},{\"attributes\":{\"ref_id\":\"b152\"},\"end\":62225,\"start\":62220},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":62408,\"start\":62404},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":62768,\"start\":62764},{\"attributes\":{\"ref_id\":\"b151\"},\"end\":62926,\"start\":62921},{\"attributes\":{\"ref_id\":\"b153\"},\"end\":63040,\"start\":63035},{\"attributes\":{\"ref_id\":\"b154\"},\"end\":63332,\"start\":63327},{\"attributes\":{\"ref_id\":\"b124\"},\"end\":67238,\"start\":67233},{\"attributes\":{\"ref_id\":\"b155\"},\"end\":67245,\"start\":67240},{\"attributes\":{\"ref_id\":\"b112\"},\"end\":68069,\"start\":68064},{\"attributes\":{\"ref_id\":\"b180\"},\"end\":68261,\"start\":68256},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":68479,\"start\":68475},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":69350,\"start\":69346},{\"attributes\":{\"ref_id\":\"b129\"},\"end\":70348,\"start\":70343},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":70690,\"start\":70686},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":72154,\"start\":72151},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":75719,\"start\":75715},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":75725,\"start\":75721},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":75731,\"start\":75727},{\"attributes\":{\"ref_id\":\"b88\"},\"end\":77564,\"start\":77560}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":80849,\"start\":79871},{\"attributes\":{\"id\":\"fig_1\"},\"end\":80902,\"start\":80850},{\"attributes\":{\"id\":\"fig_2\"},\"end\":82260,\"start\":80903},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":82683,\"start\":82261},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":83819,\"start\":82684}]", "paragraph": "[{\"end\":2248,\"start\":1621},{\"end\":2761,\"start\":2250},{\"end\":6314,\"start\":2763},{\"end\":7152,\"start\":6316},{\"end\":8598,\"start\":7154},{\"end\":8804,\"start\":8600},{\"end\":9716,\"start\":8806},{\"end\":11221,\"start\":9718},{\"end\":11760,\"start\":11223},{\"end\":12214,\"start\":11785},{\"end\":12520,\"start\":12242},{\"end\":13017,\"start\":12522},{\"end\":13875,\"start\":13019},{\"end\":14577,\"start\":13877},{\"end\":17259,\"start\":14604},{\"end\":18353,\"start\":17261},{\"end\":18947,\"start\":18382},{\"end\":20842,\"start\":18949},{\"end\":21276,\"start\":20844},{\"end\":22107,\"start\":21301},{\"end\":24004,\"start\":22136},{\"end\":24593,\"start\":24006},{\"end\":25563,\"start\":24630},{\"end\":26167,\"start\":25565},{\"end\":26623,\"start\":26197},{\"end\":27249,\"start\":26625},{\"end\":27808,\"start\":27251},{\"end\":28411,\"start\":27810},{\"end\":28803,\"start\":28413},{\"end\":29306,\"start\":28831},{\"end\":30143,\"start\":29346},{\"end\":30294,\"start\":30145},{\"end\":30380,\"start\":30296},{\"end\":30720,\"start\":30382},{\"end\":35460,\"start\":30722},{\"end\":35841,\"start\":35462},{\"end\":36930,\"start\":35843},{\"end\":37786,\"start\":36932},{\"end\":38007,\"start\":37788},{\"end\":38437,\"start\":38009},{\"end\":38949,\"start\":38439},{\"end\":39335,\"start\":39000},{\"end\":40632,\"start\":39365},{\"end\":42754,\"start\":40667},{\"end\":44269,\"start\":42795},{\"end\":45744,\"start\":44271},{\"end\":48301,\"start\":45793},{\"end\":48583,\"start\":48303},{\"end\":49537,\"start\":48638},{\"end\":49702,\"start\":49564},{\"end\":50107,\"start\":49704},{\"end\":50594,\"start\":50109},{\"end\":51042,\"start\":50596},{\"end\":51452,\"start\":51044},{\"end\":51677,\"start\":51479},{\"end\":52216,\"start\":51679},{\"end\":52696,\"start\":52218},{\"end\":52894,\"start\":52698},{\"end\":53170,\"start\":52934},{\"end\":54078,\"start\":53172},{\"end\":54529,\"start\":54080},{\"end\":54919,\"start\":54573},{\"end\":55721,\"start\":54921},{\"end\":56017,\"start\":55723},{\"end\":56484,\"start\":56019},{\"end\":56766,\"start\":56486},{\"end\":57529,\"start\":56810},{\"end\":57657,\"start\":57531},{\"end\":57960,\"start\":57659},{\"end\":58736,\"start\":58005},{\"end\":59839,\"start\":58756},{\"end\":60449,\"start\":59858},{\"end\":61384,\"start\":60472},{\"end\":61972,\"start\":61401},{\"end\":62497,\"start\":61991},{\"end\":63688,\"start\":62523},{\"end\":64068,\"start\":63717},{\"end\":65318,\"start\":64094},{\"end\":65728,\"start\":65356},{\"end\":66273,\"start\":65730},{\"end\":66572,\"start\":66275},{\"end\":66655,\"start\":66574},{\"end\":66810,\"start\":66657},{\"end\":67583,\"start\":66867},{\"end\":67903,\"start\":67653},{\"end\":69522,\"start\":67962},{\"end\":70776,\"start\":69552},{\"end\":71284,\"start\":70825},{\"end\":72346,\"start\":71347},{\"end\":73607,\"start\":72400},{\"end\":73966,\"start\":73663},{\"end\":74309,\"start\":74023},{\"end\":75149,\"start\":74311},{\"end\":75563,\"start\":75151},{\"end\":76404,\"start\":75607},{\"end\":76747,\"start\":76406},{\"end\":77073,\"start\":76774},{\"end\":77780,\"start\":77075},{\"end\":78122,\"start\":77802},{\"end\":79405,\"start\":78124},{\"end\":79870,\"start\":79407}]", "formula": null, "table_ref": null, "section_header": "[{\"end\":1619,\"start\":1604},{\"end\":11783,\"start\":11763},{\"end\":12240,\"start\":12217},{\"end\":14602,\"start\":14580},{\"end\":18380,\"start\":18356},{\"end\":21299,\"start\":21279},{\"end\":22134,\"start\":22110},{\"end\":24628,\"start\":24596},{\"end\":26195,\"start\":26170},{\"end\":28829,\"start\":28806},{\"end\":29344,\"start\":29309},{\"end\":38998,\"start\":38952},{\"end\":39363,\"start\":39338},{\"end\":40665,\"start\":40635},{\"end\":42793,\"start\":42757},{\"end\":45791,\"start\":45747},{\"end\":48636,\"start\":48586},{\"end\":49562,\"start\":49540},{\"end\":51477,\"start\":51455},{\"end\":52932,\"start\":52897},{\"end\":54571,\"start\":54532},{\"end\":56808,\"start\":56769},{\"end\":58003,\"start\":57963},{\"end\":58754,\"start\":58739},{\"end\":59856,\"start\":59842},{\"end\":60470,\"start\":60452},{\"end\":61399,\"start\":61387},{\"end\":61989,\"start\":61975},{\"end\":62521,\"start\":62500},{\"end\":63715,\"start\":63691},{\"end\":64092,\"start\":64071},{\"end\":65354,\"start\":65321},{\"end\":66865,\"start\":66813},{\"end\":67651,\"start\":67586},{\"end\":67960,\"start\":67906},{\"end\":69550,\"start\":69525},{\"end\":70823,\"start\":70779},{\"end\":71345,\"start\":71287},{\"end\":72398,\"start\":72349},{\"end\":73661,\"start\":73610},{\"end\":74021,\"start\":73969},{\"end\":75605,\"start\":75566},{\"end\":76772,\"start\":76750},{\"end\":77800,\"start\":77783},{\"end\":80859,\"start\":80851}]", "table": null, "figure_caption": "[{\"end\":80849,\"start\":79873},{\"end\":80902,\"start\":80861},{\"end\":82260,\"start\":80905},{\"end\":82683,\"start\":82263},{\"end\":83819,\"start\":82686}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":9045,\"start\":9039}]", "bib_author_first_name": "[{\"end\":84193,\"start\":84192},{\"end\":84195,\"start\":84194},{\"end\":84206,\"start\":84205},{\"end\":84208,\"start\":84207},{\"end\":84218,\"start\":84217},{\"end\":84541,\"start\":84540},{\"end\":84553,\"start\":84552},{\"end\":84563,\"start\":84562},{\"end\":84946,\"start\":84945},{\"end\":84948,\"start\":84947},{\"end\":84959,\"start\":84958},{\"end\":84961,\"start\":84960},{\"end\":84971,\"start\":84970},{\"end\":84981,\"start\":84980},{\"end\":84994,\"start\":84993},{\"end\":85005,\"start\":85004},{\"end\":85013,\"start\":85012},{\"end\":85025,\"start\":85024},{\"end\":85036,\"start\":85035},{\"end\":85048,\"start\":85047},{\"end\":85056,\"start\":85055},{\"end\":85434,\"start\":85433},{\"end\":85448,\"start\":85447},{\"end\":85459,\"start\":85458},{\"end\":85461,\"start\":85460},{\"end\":85768,\"start\":85767},{\"end\":85782,\"start\":85781},{\"end\":85784,\"start\":85783},{\"end\":85792,\"start\":85791},{\"end\":85794,\"start\":85793},{\"end\":86050,\"start\":86049},{\"end\":86062,\"start\":86061},{\"end\":86064,\"start\":86063},{\"end\":86375,\"start\":86374},{\"end\":86377,\"start\":86376},{\"end\":86385,\"start\":86384},{\"end\":86393,\"start\":86392},{\"end\":86395,\"start\":86394},{\"end\":86401,\"start\":86400},{\"end\":86403,\"start\":86402},{\"end\":86704,\"start\":86703},{\"end\":86706,\"start\":86705},{\"end\":86714,\"start\":86713},{\"end\":86730,\"start\":86729},{\"end\":86741,\"start\":86740},{\"end\":86743,\"start\":86742},{\"end\":87064,\"start\":87063},{\"end\":87073,\"start\":87072},{\"end\":87388,\"start\":87387},{\"end\":87390,\"start\":87389},{\"end\":87400,\"start\":87399},{\"end\":87413,\"start\":87412},{\"end\":87415,\"start\":87414},{\"end\":87735,\"start\":87734},{\"end\":87746,\"start\":87745},{\"end\":87755,\"start\":87754},{\"end\":87764,\"start\":87763},{\"end\":87777,\"start\":87776},{\"end\":88045,\"start\":88044},{\"end\":88057,\"start\":88056},{\"end\":88439,\"start\":88438},{\"end\":88448,\"start\":88447},{\"end\":88459,\"start\":88458},{\"end\":88765,\"start\":88764},{\"end\":88775,\"start\":88774},{\"end\":88784,\"start\":88783},{\"end\":89023,\"start\":89022},{\"end\":89035,\"start\":89034},{\"end\":89396,\"start\":89395},{\"end\":89676,\"start\":89675},{\"end\":89684,\"start\":89683},{\"end\":89696,\"start\":89695},{\"end\":89711,\"start\":89710},{\"end\":90042,\"start\":90041},{\"end\":90052,\"start\":90051},{\"end\":90054,\"start\":90053},{\"end\":90358,\"start\":90357},{\"end\":90360,\"start\":90359},{\"end\":90742,\"start\":90741},{\"end\":91131,\"start\":91130},{\"end\":91140,\"start\":91139},{\"end\":91147,\"start\":91146},{\"end\":91480,\"start\":91479},{\"end\":91482,\"start\":91481},{\"end\":91489,\"start\":91488},{\"end\":91491,\"start\":91490},{\"end\":91498,\"start\":91497},{\"end\":91500,\"start\":91499},{\"end\":91506,\"start\":91505},{\"end\":91508,\"start\":91507},{\"end\":91813,\"start\":91812},{\"end\":92112,\"start\":92111},{\"end\":92114,\"start\":92113},{\"end\":92126,\"start\":92125},{\"end\":92133,\"start\":92132},{\"end\":92141,\"start\":92140},{\"end\":92446,\"start\":92445},{\"end\":92448,\"start\":92447},{\"end\":92461,\"start\":92460},{\"end\":92463,\"start\":92462},{\"end\":92471,\"start\":92470},{\"end\":92473,\"start\":92472},{\"end\":92807,\"start\":92806},{\"end\":92813,\"start\":92812},{\"end\":92822,\"start\":92821},{\"end\":93184,\"start\":93183},{\"end\":93186,\"start\":93185},{\"end\":93196,\"start\":93195},{\"end\":93198,\"start\":93197},{\"end\":93552,\"start\":93551},{\"end\":93782,\"start\":93781},{\"end\":93796,\"start\":93792},{\"end\":94079,\"start\":94078},{\"end\":94091,\"start\":94090},{\"end\":94404,\"start\":94403},{\"end\":94686,\"start\":94685},{\"end\":94695,\"start\":94694},{\"end\":94964,\"start\":94963},{\"end\":94974,\"start\":94973},{\"end\":95326,\"start\":95322},{\"end\":95337,\"start\":95333},{\"end\":95556,\"start\":95555},{\"end\":95565,\"start\":95564},{\"end\":95575,\"start\":95574},{\"end\":95955,\"start\":95954},{\"end\":95964,\"start\":95963},{\"end\":96315,\"start\":96314},{\"end\":96325,\"start\":96324},{\"end\":96335,\"start\":96334},{\"end\":96648,\"start\":96647},{\"end\":96650,\"start\":96649},{\"end\":97035,\"start\":97034},{\"end\":97042,\"start\":97041},{\"end\":97052,\"start\":97051},{\"end\":97357,\"start\":97356},{\"end\":97367,\"start\":97366},{\"end\":97375,\"start\":97374},{\"end\":97383,\"start\":97382},{\"end\":97665,\"start\":97664},{\"end\":97672,\"start\":97671},{\"end\":97674,\"start\":97673},{\"end\":98258,\"start\":98257},{\"end\":98271,\"start\":98270},{\"end\":98599,\"start\":98598},{\"end\":98601,\"start\":98600},{\"end\":98610,\"start\":98609},{\"end\":98618,\"start\":98617},{\"end\":98620,\"start\":98619},{\"end\":98892,\"start\":98891},{\"end\":98901,\"start\":98900},{\"end\":99182,\"start\":99181},{\"end\":99191,\"start\":99190},{\"end\":99193,\"start\":99192},{\"end\":99442,\"start\":99441},{\"end\":99455,\"start\":99454},{\"end\":99457,\"start\":99456},{\"end\":99467,\"start\":99466},{\"end\":99735,\"start\":99734},{\"end\":99744,\"start\":99743},{\"end\":100124,\"start\":100123},{\"end\":100137,\"start\":100136},{\"end\":100146,\"start\":100145},{\"end\":100513,\"start\":100512},{\"end\":100520,\"start\":100519},{\"end\":100526,\"start\":100525},{\"end\":100834,\"start\":100833},{\"end\":100844,\"start\":100843},{\"end\":100854,\"start\":100853},{\"end\":101149,\"start\":101148},{\"end\":101159,\"start\":101158},{\"end\":101441,\"start\":101440},{\"end\":101451,\"start\":101450},{\"end\":101891,\"start\":101890},{\"end\":101900,\"start\":101899},{\"end\":101902,\"start\":101901},{\"end\":102250,\"start\":102249},{\"end\":102256,\"start\":102255},{\"end\":102620,\"start\":102619},{\"end\":102637,\"start\":102636},{\"end\":102647,\"start\":102646},{\"end\":103055,\"start\":103054},{\"end\":103064,\"start\":103063},{\"end\":103074,\"start\":103073},{\"end\":103084,\"start\":103083},{\"end\":103095,\"start\":103094},{\"end\":103103,\"start\":103102},{\"end\":103466,\"start\":103465},{\"end\":103468,\"start\":103467},{\"end\":103478,\"start\":103477},{\"end\":103480,\"start\":103479},{\"end\":103853,\"start\":103852},{\"end\":103870,\"start\":103869},{\"end\":103880,\"start\":103879},{\"end\":103889,\"start\":103888},{\"end\":104311,\"start\":104310},{\"end\":104318,\"start\":104317},{\"end\":104693,\"start\":104692},{\"end\":104703,\"start\":104702},{\"end\":104996,\"start\":104995},{\"end\":105004,\"start\":105003},{\"end\":105019,\"start\":105018},{\"end\":105030,\"start\":105029},{\"end\":105404,\"start\":105403},{\"end\":105414,\"start\":105413},{\"end\":105625,\"start\":105624},{\"end\":105642,\"start\":105641},{\"end\":105649,\"start\":105648},{\"end\":106028,\"start\":106027},{\"end\":106045,\"start\":106044},{\"end\":106406,\"start\":106405},{\"end\":106408,\"start\":106407},{\"end\":106417,\"start\":106416},{\"end\":106419,\"start\":106418},{\"end\":106429,\"start\":106428},{\"end\":106439,\"start\":106438},{\"end\":106441,\"start\":106440},{\"end\":106450,\"start\":106449},{\"end\":106452,\"start\":106451},{\"end\":106817,\"start\":106816},{\"end\":106828,\"start\":106827},{\"end\":107252,\"start\":107251},{\"end\":107254,\"start\":107253},{\"end\":107265,\"start\":107264},{\"end\":107267,\"start\":107266},{\"end\":107554,\"start\":107553},{\"end\":107567,\"start\":107566},{\"end\":107854,\"start\":107853},{\"end\":107863,\"start\":107862},{\"end\":107872,\"start\":107871},{\"end\":108144,\"start\":108143},{\"end\":108153,\"start\":108152},{\"end\":108169,\"start\":108168},{\"end\":108464,\"start\":108463},{\"end\":108474,\"start\":108473},{\"end\":108485,\"start\":108484},{\"end\":108730,\"start\":108729},{\"end\":108732,\"start\":108731},{\"end\":108741,\"start\":108740},{\"end\":108990,\"start\":108989},{\"end\":108996,\"start\":108995},{\"end\":108998,\"start\":108997},{\"end\":109255,\"start\":109254},{\"end\":109263,\"start\":109262},{\"end\":109269,\"start\":109268},{\"end\":109459,\"start\":109458},{\"end\":109461,\"start\":109460},{\"end\":109471,\"start\":109470},{\"end\":109484,\"start\":109483},{\"end\":109492,\"start\":109491},{\"end\":109504,\"start\":109503},{\"end\":109506,\"start\":109505},{\"end\":109787,\"start\":109786},{\"end\":109795,\"start\":109794},{\"end\":110173,\"start\":110172},{\"end\":110183,\"start\":110182},{\"end\":110486,\"start\":110485},{\"end\":110488,\"start\":110487},{\"end\":110495,\"start\":110494},{\"end\":110503,\"start\":110502},{\"end\":110505,\"start\":110504},{\"end\":110521,\"start\":110520},{\"end\":110525,\"start\":110522},{\"end\":110830,\"start\":110829},{\"end\":110834,\"start\":110831},{\"end\":110845,\"start\":110844},{\"end\":110857,\"start\":110856},{\"end\":110867,\"start\":110866},{\"end\":111306,\"start\":111305},{\"end\":111316,\"start\":111315},{\"end\":111327,\"start\":111326},{\"end\":111689,\"start\":111688},{\"end\":111699,\"start\":111698},{\"end\":111708,\"start\":111707},{\"end\":111717,\"start\":111716},{\"end\":111726,\"start\":111725},{\"end\":111739,\"start\":111738},{\"end\":111746,\"start\":111745},{\"end\":112103,\"start\":112102},{\"end\":112113,\"start\":112112},{\"end\":112122,\"start\":112121},{\"end\":112133,\"start\":112132},{\"end\":112442,\"start\":112438},{\"end\":112450,\"start\":112449},{\"end\":112457,\"start\":112456},{\"end\":112846,\"start\":112845},{\"end\":112854,\"start\":112853},{\"end\":113218,\"start\":113217},{\"end\":113220,\"start\":113219},{\"end\":113529,\"start\":113528},{\"end\":113531,\"start\":113530},{\"end\":113538,\"start\":113537},{\"end\":113540,\"start\":113539},{\"end\":113552,\"start\":113551},{\"end\":113554,\"start\":113553},{\"end\":113975,\"start\":113974},{\"end\":113988,\"start\":113987},{\"end\":113997,\"start\":113996},{\"end\":114006,\"start\":114005},{\"end\":114257,\"start\":114256},{\"end\":114268,\"start\":114267},{\"end\":114689,\"start\":114688},{\"end\":115313,\"start\":115312},{\"end\":115322,\"start\":115321},{\"end\":115333,\"start\":115332},{\"end\":115342,\"start\":115341},{\"end\":115351,\"start\":115350},{\"end\":115693,\"start\":115692},{\"end\":115702,\"start\":115701},{\"end\":116002,\"start\":116001},{\"end\":116004,\"start\":116003},{\"end\":116015,\"start\":116014},{\"end\":116017,\"start\":116016},{\"end\":116280,\"start\":116279},{\"end\":116282,\"start\":116281},{\"end\":116289,\"start\":116288},{\"end\":116291,\"start\":116290},{\"end\":116301,\"start\":116300},{\"end\":116303,\"start\":116302},{\"end\":116313,\"start\":116312},{\"end\":116315,\"start\":116314},{\"end\":116325,\"start\":116324},{\"end\":116334,\"start\":116333},{\"end\":116336,\"start\":116335},{\"end\":116347,\"start\":116346},{\"end\":116659,\"start\":116658},{\"end\":116661,\"start\":116660},{\"end\":116668,\"start\":116667},{\"end\":116679,\"start\":116678},{\"end\":116996,\"start\":116995},{\"end\":117007,\"start\":117006},{\"end\":117017,\"start\":117016},{\"end\":117031,\"start\":117030},{\"end\":117366,\"start\":117365},{\"end\":117375,\"start\":117374},{\"end\":117607,\"start\":117606},{\"end\":117609,\"start\":117608},{\"end\":117616,\"start\":117615},{\"end\":118028,\"start\":118027},{\"end\":118457,\"start\":118456},{\"end\":118470,\"start\":118469},{\"end\":119031,\"start\":119030},{\"end\":119047,\"start\":119046},{\"end\":119425,\"start\":119424},{\"end\":119437,\"start\":119436},{\"end\":119752,\"start\":119751},{\"end\":119760,\"start\":119759},{\"end\":119762,\"start\":119761},{\"end\":119770,\"start\":119769},{\"end\":119772,\"start\":119771},{\"end\":120114,\"start\":120113},{\"end\":120116,\"start\":120115},{\"end\":120125,\"start\":120124},{\"end\":120129,\"start\":120126},{\"end\":120382,\"start\":120381},{\"end\":120767,\"start\":120766},{\"end\":120781,\"start\":120780},{\"end\":121127,\"start\":121126},{\"end\":121134,\"start\":121133},{\"end\":121420,\"start\":121419},{\"end\":121428,\"start\":121427},{\"end\":121430,\"start\":121429},{\"end\":121437,\"start\":121436},{\"end\":121439,\"start\":121438},{\"end\":121446,\"start\":121445},{\"end\":121448,\"start\":121447},{\"end\":121454,\"start\":121453},{\"end\":121456,\"start\":121455},{\"end\":121739,\"start\":121738},{\"end\":121741,\"start\":121740},{\"end\":121993,\"start\":121992},{\"end\":122005,\"start\":122004},{\"end\":122013,\"start\":122012},{\"end\":122022,\"start\":122021},{\"end\":122331,\"start\":122330},{\"end\":122342,\"start\":122341},{\"end\":122351,\"start\":122350},{\"end\":122608,\"start\":122607},{\"end\":122610,\"start\":122609},{\"end\":122621,\"start\":122620},{\"end\":122636,\"start\":122635},{\"end\":122638,\"start\":122637},{\"end\":123082,\"start\":123081},{\"end\":123091,\"start\":123090},{\"end\":123675,\"start\":123674},{\"end\":123686,\"start\":123685},{\"end\":123688,\"start\":123687},{\"end\":123697,\"start\":123696},{\"end\":124020,\"start\":124019},{\"end\":124022,\"start\":124021},{\"end\":124029,\"start\":124028},{\"end\":124031,\"start\":124030},{\"end\":124286,\"start\":124285},{\"end\":124288,\"start\":124287},{\"end\":124295,\"start\":124294},{\"end\":124297,\"start\":124296},{\"end\":124309,\"start\":124308},{\"end\":124311,\"start\":124310},{\"end\":124958,\"start\":124957},{\"end\":124962,\"start\":124959},{\"end\":124970,\"start\":124969},{\"end\":124972,\"start\":124971},{\"end\":125617,\"start\":125616},{\"end\":125627,\"start\":125626},{\"end\":125644,\"start\":125640},{\"end\":125944,\"start\":125943},{\"end\":125958,\"start\":125954},{\"end\":126281,\"start\":126280},{\"end\":126292,\"start\":126288},{\"end\":126596,\"start\":126595},{\"end\":126609,\"start\":126608},{\"end\":126617,\"start\":126616},{\"end\":126627,\"start\":126626},{\"end\":127053,\"start\":127052},{\"end\":127055,\"start\":127054},{\"end\":127065,\"start\":127064},{\"end\":127067,\"start\":127066},{\"end\":127385,\"start\":127384},{\"end\":127387,\"start\":127386},{\"end\":127397,\"start\":127396},{\"end\":127399,\"start\":127398},{\"end\":127409,\"start\":127408},{\"end\":127773,\"start\":127772},{\"end\":127775,\"start\":127774},{\"end\":127783,\"start\":127782},{\"end\":127991,\"start\":127990},{\"end\":128002,\"start\":128001},{\"end\":128353,\"start\":128352},{\"end\":128364,\"start\":128363},{\"end\":128374,\"start\":128373},{\"end\":128709,\"start\":128708},{\"end\":128711,\"start\":128710},{\"end\":128721,\"start\":128720},{\"end\":128732,\"start\":128731},{\"end\":128734,\"start\":128733},{\"end\":129033,\"start\":129032},{\"end\":129042,\"start\":129041},{\"end\":129324,\"start\":129323},{\"end\":129332,\"start\":129331},{\"end\":129655,\"start\":129654},{\"end\":129664,\"start\":129663},{\"end\":129680,\"start\":129679},{\"end\":129688,\"start\":129687},{\"end\":129991,\"start\":129990},{\"end\":129993,\"start\":129992},{\"end\":130003,\"start\":130002},{\"end\":130005,\"start\":130004},{\"end\":130281,\"start\":130280},{\"end\":130601,\"start\":130600},{\"end\":130609,\"start\":130608},{\"end\":130921,\"start\":130920},{\"end\":130932,\"start\":130931},{\"end\":131182,\"start\":131181},{\"end\":131193,\"start\":131192},{\"end\":131446,\"start\":131445},{\"end\":131455,\"start\":131454},{\"end\":131787,\"start\":131786},{\"end\":131797,\"start\":131796},{\"end\":132059,\"start\":132058},{\"end\":132068,\"start\":132067},{\"end\":132078,\"start\":132077},{\"end\":132087,\"start\":132086},{\"end\":132100,\"start\":132099},{\"end\":132107,\"start\":132106},{\"end\":132118,\"start\":132117},{\"end\":132490,\"start\":132489},{\"end\":132503,\"start\":132502},{\"end\":132510,\"start\":132509},{\"end\":132822,\"start\":132821},{\"end\":132835,\"start\":132834},{\"end\":133285,\"start\":133284},{\"end\":133287,\"start\":133286},{\"end\":133382,\"start\":133381},{\"end\":133395,\"start\":133394},{\"end\":133397,\"start\":133396},{\"end\":133404,\"start\":133403},{\"end\":133406,\"start\":133405},{\"end\":133417,\"start\":133416},{\"end\":133419,\"start\":133418},{\"end\":133688,\"start\":133687},{\"end\":133700,\"start\":133699},{\"end\":133712,\"start\":133711},{\"end\":133727,\"start\":133726},{\"end\":134000,\"start\":133999},{\"end\":134009,\"start\":134008},{\"end\":134017,\"start\":134016},{\"end\":134028,\"start\":134027},{\"end\":134388,\"start\":134387},{\"end\":134390,\"start\":134389},{\"end\":134404,\"start\":134403},{\"end\":134406,\"start\":134405},{\"end\":134640,\"start\":134639},{\"end\":134642,\"start\":134641},{\"end\":134653,\"start\":134652},{\"end\":134655,\"start\":134654},{\"end\":134665,\"start\":134664},{\"end\":134678,\"start\":134677},{\"end\":134924,\"start\":134923},{\"end\":134935,\"start\":134934},{\"end\":134944,\"start\":134943},{\"end\":135382,\"start\":135381},{\"end\":135392,\"start\":135391},{\"end\":135394,\"start\":135393},{\"end\":135712,\"start\":135711},{\"end\":135714,\"start\":135713},{\"end\":135726,\"start\":135725},{\"end\":135728,\"start\":135727},{\"end\":135739,\"start\":135738},{\"end\":135741,\"start\":135740},{\"end\":135753,\"start\":135752},{\"end\":135755,\"start\":135754},{\"end\":136070,\"start\":136069},{\"end\":136080,\"start\":136079},{\"end\":136082,\"start\":136081},{\"end\":136091,\"start\":136090},{\"end\":136093,\"start\":136092},{\"end\":136810,\"start\":136809},{\"end\":136812,\"start\":136811},{\"end\":136820,\"start\":136819},{\"end\":136822,\"start\":136821},{\"end\":136832,\"start\":136831},{\"end\":136834,\"start\":136833},{\"end\":137113,\"start\":137112},{\"end\":137115,\"start\":137114},{\"end\":137122,\"start\":137121},{\"end\":137124,\"start\":137123},{\"end\":137133,\"start\":137132},{\"end\":137135,\"start\":137134},{\"end\":137840,\"start\":137839},{\"end\":137850,\"start\":137849},{\"end\":137858,\"start\":137857},{\"end\":137860,\"start\":137859},{\"end\":137869,\"start\":137868},{\"end\":137871,\"start\":137870},{\"end\":138304,\"start\":138303},{\"end\":138321,\"start\":138320},{\"end\":138745,\"start\":138744},{\"end\":138755,\"start\":138754},{\"end\":139035,\"start\":139034},{\"end\":139037,\"start\":139036},{\"end\":139046,\"start\":139045},{\"end\":139048,\"start\":139047},{\"end\":139058,\"start\":139057},{\"end\":139060,\"start\":139059},{\"end\":140085,\"start\":140084},{\"end\":140087,\"start\":140086},{\"end\":140098,\"start\":140097},{\"end\":140100,\"start\":140099},{\"end\":140109,\"start\":140108},{\"end\":140111,\"start\":140110},{\"end\":140471,\"start\":140470},{\"end\":140694,\"start\":140693},{\"end\":140703,\"start\":140702},{\"end\":140997,\"start\":140996},{\"end\":141007,\"start\":141006},{\"end\":141378,\"start\":141377},{\"end\":141389,\"start\":141388},{\"end\":141799,\"start\":141798},{\"end\":141809,\"start\":141808},{\"end\":142051,\"start\":142050},{\"end\":142068,\"start\":142067},{\"end\":142078,\"start\":142077},{\"end\":142415,\"start\":142414},{\"end\":142417,\"start\":142416},{\"end\":142424,\"start\":142423},{\"end\":142432,\"start\":142431},{\"end\":142434,\"start\":142433},{\"end\":142440,\"start\":142439},{\"end\":142442,\"start\":142441},{\"end\":142753,\"start\":142752},{\"end\":142755,\"start\":142754},{\"end\":142761,\"start\":142760},{\"end\":142768,\"start\":142767},{\"end\":142770,\"start\":142769},{\"end\":143036,\"start\":143035},{\"end\":143046,\"start\":143045},{\"end\":143325,\"start\":143324},{\"end\":143342,\"start\":143341},{\"end\":143352,\"start\":143351},{\"end\":143741,\"start\":143740},{\"end\":143752,\"start\":143751},{\"end\":143761,\"start\":143760},{\"end\":144155,\"start\":144154},{\"end\":144172,\"start\":144171},{\"end\":144566,\"start\":144565},{\"end\":144575,\"start\":144574},{\"end\":144877,\"start\":144876},{\"end\":144886,\"start\":144885},{\"end\":144896,\"start\":144895},{\"end\":145314,\"start\":145313},{\"end\":145325,\"start\":145324},{\"end\":145702,\"start\":145701},{\"end\":145719,\"start\":145718},{\"end\":145729,\"start\":145728},{\"end\":146142,\"start\":146141},{\"end\":146150,\"start\":146149},{\"end\":146152,\"start\":146151},{\"end\":146158,\"start\":146157},{\"end\":146160,\"start\":146159},{\"end\":146447,\"start\":146446},{\"end\":146455,\"start\":146454},{\"end\":146457,\"start\":146456},{\"end\":146465,\"start\":146464},{\"end\":146467,\"start\":146466}]", "bib_author_last_name": "[{\"end\":84203,\"start\":84196},{\"end\":84215,\"start\":84209},{\"end\":84225,\"start\":84219},{\"end\":84550,\"start\":84542},{\"end\":84560,\"start\":84554},{\"end\":84569,\"start\":84564},{\"end\":84956,\"start\":84949},{\"end\":84968,\"start\":84962},{\"end\":84978,\"start\":84972},{\"end\":84991,\"start\":84982},{\"end\":85002,\"start\":84995},{\"end\":85010,\"start\":85006},{\"end\":85022,\"start\":85014},{\"end\":85033,\"start\":85026},{\"end\":85045,\"start\":85037},{\"end\":85053,\"start\":85049},{\"end\":85063,\"start\":85057},{\"end\":85445,\"start\":85435},{\"end\":85456,\"start\":85449},{\"end\":85467,\"start\":85462},{\"end\":85779,\"start\":85769},{\"end\":85789,\"start\":85785},{\"end\":85799,\"start\":85795},{\"end\":86059,\"start\":86051},{\"end\":86069,\"start\":86065},{\"end\":86382,\"start\":86378},{\"end\":86390,\"start\":86386},{\"end\":86398,\"start\":86396},{\"end\":86407,\"start\":86404},{\"end\":86711,\"start\":86707},{\"end\":86727,\"start\":86715},{\"end\":86738,\"start\":86731},{\"end\":86752,\"start\":86744},{\"end\":87070,\"start\":87065},{\"end\":87079,\"start\":87074},{\"end\":87397,\"start\":87391},{\"end\":87410,\"start\":87401},{\"end\":87421,\"start\":87416},{\"end\":87743,\"start\":87736},{\"end\":87752,\"start\":87747},{\"end\":87761,\"start\":87756},{\"end\":87774,\"start\":87765},{\"end\":87786,\"start\":87778},{\"end\":88054,\"start\":88046},{\"end\":88065,\"start\":88058},{\"end\":88445,\"start\":88440},{\"end\":88456,\"start\":88449},{\"end\":88467,\"start\":88460},{\"end\":88772,\"start\":88766},{\"end\":88781,\"start\":88776},{\"end\":88795,\"start\":88785},{\"end\":89032,\"start\":89024},{\"end\":89043,\"start\":89036},{\"end\":89404,\"start\":89397},{\"end\":89681,\"start\":89677},{\"end\":89693,\"start\":89685},{\"end\":89708,\"start\":89697},{\"end\":89720,\"start\":89712},{\"end\":90049,\"start\":90043},{\"end\":90060,\"start\":90055},{\"end\":90367,\"start\":90361},{\"end\":90751,\"start\":90743},{\"end\":91137,\"start\":91132},{\"end\":91144,\"start\":91141},{\"end\":91155,\"start\":91148},{\"end\":91486,\"start\":91483},{\"end\":91495,\"start\":91492},{\"end\":91503,\"start\":91501},{\"end\":91512,\"start\":91509},{\"end\":91819,\"start\":91814},{\"end\":92123,\"start\":92115},{\"end\":92130,\"start\":92127},{\"end\":92138,\"start\":92134},{\"end\":92148,\"start\":92142},{\"end\":92458,\"start\":92449},{\"end\":92468,\"start\":92464},{\"end\":92482,\"start\":92474},{\"end\":92810,\"start\":92808},{\"end\":92819,\"start\":92814},{\"end\":92829,\"start\":92823},{\"end\":93193,\"start\":93187},{\"end\":93206,\"start\":93199},{\"end\":93557,\"start\":93553},{\"end\":93790,\"start\":93783},{\"end\":93802,\"start\":93797},{\"end\":94088,\"start\":94080},{\"end\":94099,\"start\":94092},{\"end\":94416,\"start\":94405},{\"end\":94692,\"start\":94687},{\"end\":94701,\"start\":94696},{\"end\":94971,\"start\":94965},{\"end\":94981,\"start\":94975},{\"end\":95331,\"start\":95327},{\"end\":95342,\"start\":95338},{\"end\":95562,\"start\":95557},{\"end\":95572,\"start\":95566},{\"end\":95584,\"start\":95576},{\"end\":95961,\"start\":95956},{\"end\":95971,\"start\":95965},{\"end\":96322,\"start\":96316},{\"end\":96332,\"start\":96326},{\"end\":96344,\"start\":96336},{\"end\":96656,\"start\":96651},{\"end\":97039,\"start\":97036},{\"end\":97049,\"start\":97043},{\"end\":97058,\"start\":97053},{\"end\":97364,\"start\":97358},{\"end\":97372,\"start\":97368},{\"end\":97380,\"start\":97376},{\"end\":97391,\"start\":97384},{\"end\":97669,\"start\":97666},{\"end\":97683,\"start\":97675},{\"end\":98268,\"start\":98259},{\"end\":98277,\"start\":98272},{\"end\":98607,\"start\":98602},{\"end\":98615,\"start\":98611},{\"end\":98624,\"start\":98621},{\"end\":98898,\"start\":98893},{\"end\":98907,\"start\":98902},{\"end\":99188,\"start\":99183},{\"end\":99201,\"start\":99194},{\"end\":99452,\"start\":99443},{\"end\":99464,\"start\":99458},{\"end\":99475,\"start\":99468},{\"end\":99741,\"start\":99736},{\"end\":99749,\"start\":99745},{\"end\":100134,\"start\":100125},{\"end\":100143,\"start\":100138},{\"end\":100155,\"start\":100147},{\"end\":100517,\"start\":100514},{\"end\":100523,\"start\":100521},{\"end\":100531,\"start\":100527},{\"end\":100841,\"start\":100835},{\"end\":100851,\"start\":100845},{\"end\":100861,\"start\":100855},{\"end\":101156,\"start\":101150},{\"end\":101168,\"start\":101160},{\"end\":101448,\"start\":101442},{\"end\":101456,\"start\":101452},{\"end\":101897,\"start\":101892},{\"end\":101909,\"start\":101903},{\"end\":102253,\"start\":102251},{\"end\":102265,\"start\":102257},{\"end\":102634,\"start\":102621},{\"end\":102644,\"start\":102638},{\"end\":102653,\"start\":102648},{\"end\":103061,\"start\":103056},{\"end\":103071,\"start\":103065},{\"end\":103081,\"start\":103075},{\"end\":103092,\"start\":103085},{\"end\":103100,\"start\":103096},{\"end\":103110,\"start\":103104},{\"end\":103475,\"start\":103469},{\"end\":103488,\"start\":103481},{\"end\":103867,\"start\":103854},{\"end\":103877,\"start\":103871},{\"end\":103886,\"start\":103881},{\"end\":103895,\"start\":103890},{\"end\":104315,\"start\":104312},{\"end\":104326,\"start\":104319},{\"end\":104700,\"start\":104694},{\"end\":104709,\"start\":104704},{\"end\":105001,\"start\":104997},{\"end\":105016,\"start\":105005},{\"end\":105027,\"start\":105020},{\"end\":105037,\"start\":105031},{\"end\":105411,\"start\":105405},{\"end\":105420,\"start\":105415},{\"end\":105639,\"start\":105626},{\"end\":105646,\"start\":105643},{\"end\":105657,\"start\":105650},{\"end\":106042,\"start\":106029},{\"end\":106053,\"start\":106046},{\"end\":106414,\"start\":106409},{\"end\":106426,\"start\":106420},{\"end\":106436,\"start\":106430},{\"end\":106447,\"start\":106442},{\"end\":106462,\"start\":106453},{\"end\":106825,\"start\":106818},{\"end\":106835,\"start\":106829},{\"end\":107262,\"start\":107255},{\"end\":107271,\"start\":107268},{\"end\":107564,\"start\":107555},{\"end\":107574,\"start\":107568},{\"end\":107860,\"start\":107855},{\"end\":107869,\"start\":107864},{\"end\":107880,\"start\":107873},{\"end\":108150,\"start\":108145},{\"end\":108166,\"start\":108154},{\"end\":108176,\"start\":108170},{\"end\":108471,\"start\":108465},{\"end\":108482,\"start\":108475},{\"end\":108490,\"start\":108486},{\"end\":108738,\"start\":108733},{\"end\":108747,\"start\":108742},{\"end\":108993,\"start\":108991},{\"end\":109004,\"start\":108999},{\"end\":109260,\"start\":109256},{\"end\":109266,\"start\":109264},{\"end\":109273,\"start\":109270},{\"end\":109468,\"start\":109462},{\"end\":109481,\"start\":109472},{\"end\":109489,\"start\":109485},{\"end\":109501,\"start\":109493},{\"end\":109513,\"start\":109507},{\"end\":109792,\"start\":109788},{\"end\":109805,\"start\":109796},{\"end\":110180,\"start\":110174},{\"end\":110187,\"start\":110184},{\"end\":110492,\"start\":110489},{\"end\":110500,\"start\":110496},{\"end\":110518,\"start\":110506},{\"end\":110842,\"start\":110835},{\"end\":110854,\"start\":110846},{\"end\":110864,\"start\":110858},{\"end\":110871,\"start\":110868},{\"end\":111313,\"start\":111307},{\"end\":111324,\"start\":111317},{\"end\":111336,\"start\":111328},{\"end\":111696,\"start\":111690},{\"end\":111705,\"start\":111700},{\"end\":111714,\"start\":111709},{\"end\":111723,\"start\":111718},{\"end\":111736,\"start\":111727},{\"end\":111743,\"start\":111740},{\"end\":111753,\"start\":111747},{\"end\":112110,\"start\":112104},{\"end\":112119,\"start\":112114},{\"end\":112130,\"start\":112123},{\"end\":112140,\"start\":112134},{\"end\":112447,\"start\":112443},{\"end\":112454,\"start\":112451},{\"end\":112466,\"start\":112458},{\"end\":112851,\"start\":112847},{\"end\":112861,\"start\":112855},{\"end\":113226,\"start\":113221},{\"end\":113535,\"start\":113532},{\"end\":113549,\"start\":113541},{\"end\":113560,\"start\":113555},{\"end\":113985,\"start\":113976},{\"end\":113994,\"start\":113989},{\"end\":114003,\"start\":113998},{\"end\":114012,\"start\":114007},{\"end\":114265,\"start\":114258},{\"end\":114275,\"start\":114269},{\"end\":114696,\"start\":114690},{\"end\":115319,\"start\":115314},{\"end\":115330,\"start\":115323},{\"end\":115339,\"start\":115334},{\"end\":115348,\"start\":115343},{\"end\":115358,\"start\":115352},{\"end\":115699,\"start\":115694},{\"end\":115710,\"start\":115703},{\"end\":116012,\"start\":116005},{\"end\":116024,\"start\":116018},{\"end\":116286,\"start\":116283},{\"end\":116298,\"start\":116292},{\"end\":116310,\"start\":116304},{\"end\":116322,\"start\":116316},{\"end\":116331,\"start\":116326},{\"end\":116344,\"start\":116337},{\"end\":116353,\"start\":116348},{\"end\":116665,\"start\":116662},{\"end\":116676,\"start\":116669},{\"end\":116687,\"start\":116680},{\"end\":117004,\"start\":116997},{\"end\":117014,\"start\":117008},{\"end\":117028,\"start\":117018},{\"end\":117037,\"start\":117032},{\"end\":117372,\"start\":117367},{\"end\":117383,\"start\":117376},{\"end\":117613,\"start\":117610},{\"end\":117623,\"start\":117617},{\"end\":118036,\"start\":118029},{\"end\":118467,\"start\":118458},{\"end\":118479,\"start\":118471},{\"end\":119044,\"start\":119032},{\"end\":119054,\"start\":119048},{\"end\":119434,\"start\":119426},{\"end\":119441,\"start\":119438},{\"end\":119757,\"start\":119753},{\"end\":119767,\"start\":119763},{\"end\":119777,\"start\":119773},{\"end\":120122,\"start\":120117},{\"end\":120135,\"start\":120130},{\"end\":120390,\"start\":120383},{\"end\":120778,\"start\":120768},{\"end\":120787,\"start\":120782},{\"end\":121131,\"start\":121128},{\"end\":121139,\"start\":121135},{\"end\":121425,\"start\":121421},{\"end\":121434,\"start\":121431},{\"end\":121443,\"start\":121440},{\"end\":121451,\"start\":121449},{\"end\":121460,\"start\":121457},{\"end\":121746,\"start\":121742},{\"end\":122002,\"start\":121994},{\"end\":122010,\"start\":122006},{\"end\":122019,\"start\":122014},{\"end\":122028,\"start\":122023},{\"end\":122339,\"start\":122332},{\"end\":122348,\"start\":122343},{\"end\":122360,\"start\":122352},{\"end\":122618,\"start\":122611},{\"end\":122633,\"start\":122622},{\"end\":122648,\"start\":122639},{\"end\":123088,\"start\":123083},{\"end\":123097,\"start\":123092},{\"end\":123683,\"start\":123676},{\"end\":123694,\"start\":123689},{\"end\":123702,\"start\":123698},{\"end\":124026,\"start\":124023},{\"end\":124037,\"start\":124032},{\"end\":124292,\"start\":124289},{\"end\":124306,\"start\":124298},{\"end\":124317,\"start\":124312},{\"end\":124967,\"start\":124963},{\"end\":124980,\"start\":124973},{\"end\":125624,\"start\":125618},{\"end\":125638,\"start\":125628},{\"end\":125650,\"start\":125645},{\"end\":125952,\"start\":125945},{\"end\":125964,\"start\":125959},{\"end\":126286,\"start\":126282},{\"end\":126298,\"start\":126293},{\"end\":126606,\"start\":126597},{\"end\":126614,\"start\":126610},{\"end\":126624,\"start\":126618},{\"end\":126631,\"start\":126628},{\"end\":127062,\"start\":127056},{\"end\":127074,\"start\":127068},{\"end\":127394,\"start\":127388},{\"end\":127406,\"start\":127400},{\"end\":127417,\"start\":127410},{\"end\":127780,\"start\":127776},{\"end\":127790,\"start\":127784},{\"end\":127999,\"start\":127992},{\"end\":128010,\"start\":128003},{\"end\":128361,\"start\":128354},{\"end\":128371,\"start\":128365},{\"end\":128383,\"start\":128375},{\"end\":128718,\"start\":128712},{\"end\":128729,\"start\":128722},{\"end\":128743,\"start\":128735},{\"end\":129039,\"start\":129034},{\"end\":129052,\"start\":129043},{\"end\":129329,\"start\":129325},{\"end\":129338,\"start\":129333},{\"end\":129661,\"start\":129656},{\"end\":129677,\"start\":129665},{\"end\":129685,\"start\":129681},{\"end\":129695,\"start\":129689},{\"end\":130000,\"start\":129994},{\"end\":130012,\"start\":130006},{\"end\":130287,\"start\":130282},{\"end\":130606,\"start\":130602},{\"end\":130619,\"start\":130610},{\"end\":130929,\"start\":130922},{\"end\":130937,\"start\":130933},{\"end\":131190,\"start\":131183},{\"end\":131201,\"start\":131194},{\"end\":131452,\"start\":131447},{\"end\":131462,\"start\":131456},{\"end\":131794,\"start\":131788},{\"end\":131806,\"start\":131798},{\"end\":132065,\"start\":132060},{\"end\":132075,\"start\":132069},{\"end\":132084,\"start\":132079},{\"end\":132097,\"start\":132088},{\"end\":132104,\"start\":132101},{\"end\":132115,\"start\":132108},{\"end\":132124,\"start\":132119},{\"end\":132500,\"start\":132491},{\"end\":132507,\"start\":132504},{\"end\":132516,\"start\":132511},{\"end\":132832,\"start\":132823},{\"end\":132843,\"start\":132836},{\"end\":133297,\"start\":133288},{\"end\":133392,\"start\":133383},{\"end\":133401,\"start\":133398},{\"end\":133414,\"start\":133407},{\"end\":133425,\"start\":133420},{\"end\":133697,\"start\":133689},{\"end\":133709,\"start\":133701},{\"end\":133724,\"start\":133713},{\"end\":133732,\"start\":133728},{\"end\":134006,\"start\":134001},{\"end\":134014,\"start\":134010},{\"end\":134025,\"start\":134018},{\"end\":134033,\"start\":134029},{\"end\":134401,\"start\":134391},{\"end\":134413,\"start\":134407},{\"end\":134650,\"start\":134643},{\"end\":134662,\"start\":134656},{\"end\":134675,\"start\":134666},{\"end\":134685,\"start\":134679},{\"end\":134932,\"start\":134925},{\"end\":134941,\"start\":134936},{\"end\":134951,\"start\":134945},{\"end\":135389,\"start\":135383},{\"end\":135400,\"start\":135395},{\"end\":135723,\"start\":135715},{\"end\":135736,\"start\":135729},{\"end\":135750,\"start\":135742},{\"end\":135761,\"start\":135756},{\"end\":136077,\"start\":136071},{\"end\":136088,\"start\":136083},{\"end\":136100,\"start\":136094},{\"end\":136817,\"start\":136813},{\"end\":136829,\"start\":136823},{\"end\":136840,\"start\":136835},{\"end\":137119,\"start\":137116},{\"end\":137130,\"start\":137125},{\"end\":137142,\"start\":137136},{\"end\":137847,\"start\":137841},{\"end\":137855,\"start\":137851},{\"end\":137866,\"start\":137861},{\"end\":137878,\"start\":137872},{\"end\":138318,\"start\":138305},{\"end\":138328,\"start\":138322},{\"end\":138752,\"start\":138746},{\"end\":138761,\"start\":138756},{\"end\":139043,\"start\":139038},{\"end\":139055,\"start\":139049},{\"end\":139066,\"start\":139061},{\"end\":140095,\"start\":140088},{\"end\":140106,\"start\":140101},{\"end\":140118,\"start\":140112},{\"end\":140475,\"start\":140472},{\"end\":140700,\"start\":140695},{\"end\":140708,\"start\":140704},{\"end\":141004,\"start\":140998},{\"end\":141015,\"start\":141008},{\"end\":141386,\"start\":141379},{\"end\":141396,\"start\":141390},{\"end\":141806,\"start\":141800},{\"end\":141817,\"start\":141810},{\"end\":142065,\"start\":142052},{\"end\":142075,\"start\":142069},{\"end\":142084,\"start\":142079},{\"end\":142421,\"start\":142418},{\"end\":142429,\"start\":142425},{\"end\":142437,\"start\":142435},{\"end\":142446,\"start\":142443},{\"end\":142758,\"start\":142756},{\"end\":142765,\"start\":142762},{\"end\":142774,\"start\":142771},{\"end\":143043,\"start\":143037},{\"end\":143052,\"start\":143047},{\"end\":143339,\"start\":143326},{\"end\":143349,\"start\":143343},{\"end\":143358,\"start\":143353},{\"end\":143749,\"start\":143742},{\"end\":143758,\"start\":143753},{\"end\":143765,\"start\":143762},{\"end\":144169,\"start\":144156},{\"end\":144178,\"start\":144173},{\"end\":144572,\"start\":144567},{\"end\":144579,\"start\":144576},{\"end\":144883,\"start\":144878},{\"end\":144893,\"start\":144887},{\"end\":144903,\"start\":144897},{\"end\":145322,\"start\":145315},{\"end\":145332,\"start\":145326},{\"end\":145716,\"start\":145703},{\"end\":145726,\"start\":145720},{\"end\":145735,\"start\":145730},{\"end\":146147,\"start\":146143},{\"end\":146155,\"start\":146153},{\"end\":146164,\"start\":146161},{\"end\":146452,\"start\":146448},{\"end\":146462,\"start\":146458},{\"end\":146470,\"start\":146468}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":16650281},\"end\":84453,\"start\":84133},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":39994885},\"end\":84895,\"start\":84455},{\"attributes\":{\"doi\":\"CMU-RI-TR-00-12\",\"id\":\"b2\",\"matched_paper_id\":9132354},\"end\":85372,\"start\":84897},{\"attributes\":{\"id\":\"b3\"},\"end\":85695,\"start\":85374},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":17649932},\"end\":85991,\"start\":85697},{\"attributes\":{\"id\":\"b5\"},\"end\":86311,\"start\":85993},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":10496544},\"end\":86654,\"start\":86313},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":9458767},\"end\":86985,\"start\":86656},{\"attributes\":{\"id\":\"b8\"},\"end\":87321,\"start\":86987},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":13428926},\"end\":87705,\"start\":87323},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":8714546},\"end\":87983,\"start\":87707},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":8195115},\"end\":88352,\"start\":87985},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":18574665},\"end\":88722,\"start\":88354},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":1290100},\"end\":88955,\"start\":88724},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":1041214},\"end\":89320,\"start\":88957},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":2966760},\"end\":89612,\"start\":89322},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":9335855},\"end\":89967,\"start\":89614},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":1940219},\"end\":90282,\"start\":89969},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":9976863},\"end\":90669,\"start\":90284},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":2156291},\"end\":91034,\"start\":90671},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":626655},\"end\":91406,\"start\":91036},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":1520377},\"end\":91761,\"start\":91408},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":1060901},\"end\":92052,\"start\":91763},{\"attributes\":{\"id\":\"b23\"},\"end\":92362,\"start\":92054},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":17637130},\"end\":92735,\"start\":92364},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":5170789},\"end\":93131,\"start\":92737},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":18566850},\"end\":93479,\"start\":93133},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":122238372},\"end\":93732,\"start\":93481},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":5916504},\"end\":93986,\"start\":93734},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":35342661},\"end\":94340,\"start\":93988},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":17708510},\"end\":94616,\"start\":94342},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":8369379},\"end\":94922,\"start\":94618},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":6353138},\"end\":95277,\"start\":94924},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":39610918},\"end\":95492,\"start\":95279},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":5391819},\"end\":95874,\"start\":95494},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":27525133},\"end\":96206,\"start\":95876},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":16139212},\"end\":96573,\"start\":96208},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":17296153},\"end\":96941,\"start\":96575},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":15194488},\"end\":97305,\"start\":96943},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":206843200},\"end\":97616,\"start\":97307},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":15213458},\"end\":97917,\"start\":97618},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":34497642},\"end\":98219,\"start\":97919},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":8617365},\"end\":98531,\"start\":98221},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":17690148},\"end\":98849,\"start\":98533},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":156022},\"end\":99095,\"start\":98851},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":205013679},\"end\":99389,\"start\":99097},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":62185766},\"end\":99636,\"start\":99391},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":16456691},\"end\":100057,\"start\":99638},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":14026813},\"end\":100465,\"start\":100059},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":14555654},\"end\":100794,\"start\":100467},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":40120983},\"end\":101058,\"start\":100796},{\"attributes\":{\"id\":\"b51\"},\"end\":101368,\"start\":101060},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":12956334},\"end\":101776,\"start\":101370},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":206037537},\"end\":102140,\"start\":101778},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":7035602},\"end\":102530,\"start\":102142},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":16999156},\"end\":103001,\"start\":102532},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":17900058},\"end\":103418,\"start\":103003},{\"attributes\":{\"id\":\"b57\"},\"end\":103775,\"start\":103420},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":2974306},\"end\":104258,\"start\":103777},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":15390360},\"end\":104619,\"start\":104260},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":1940219},\"end\":104937,\"start\":104621},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":6522426},\"end\":105363,\"start\":104939},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":14345618},\"end\":105564,\"start\":105365},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":14230118},\"end\":105954,\"start\":105566},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":14333695},\"end\":106375,\"start\":105956},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":1815007},\"end\":106762,\"start\":106377},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":53912965},\"end\":107196,\"start\":106764},{\"attributes\":{\"id\":\"b67\"},\"end\":107439,\"start\":107198},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":59923875},\"end\":107784,\"start\":107441},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":14498684},\"end\":108085,\"start\":107786},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":2559322},\"end\":108409,\"start\":108087},{\"attributes\":{\"id\":\"b71\",\"matched_paper_id\":1917965},\"end\":108687,\"start\":108411},{\"attributes\":{\"id\":\"b72\"},\"end\":108936,\"start\":108689},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":3706389},\"end\":109206,\"start\":108938},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":11459751},\"end\":109425,\"start\":109208},{\"attributes\":{\"id\":\"b75\"},\"end\":109738,\"start\":109427},{\"attributes\":{\"id\":\"b76\",\"matched_paper_id\":61683057},\"end\":110063,\"start\":109740},{\"attributes\":{\"id\":\"b77\",\"matched_paper_id\":20477174},\"end\":110425,\"start\":110065},{\"attributes\":{\"id\":\"b78\"},\"end\":110757,\"start\":110427},{\"attributes\":{\"id\":\"b79\",\"matched_paper_id\":17957171},\"end\":111223,\"start\":110759},{\"attributes\":{\"id\":\"b80\",\"matched_paper_id\":16234680},\"end\":111625,\"start\":111225},{\"attributes\":{\"id\":\"b81\",\"matched_paper_id\":18343101},\"end\":112037,\"start\":111627},{\"attributes\":{\"id\":\"b82\",\"matched_paper_id\":12060323},\"end\":112374,\"start\":112039},{\"attributes\":{\"id\":\"b83\",\"matched_paper_id\":1090303},\"end\":112792,\"start\":112376},{\"attributes\":{\"id\":\"b84\",\"matched_paper_id\":8462354},\"end\":113130,\"start\":112794},{\"attributes\":{\"id\":\"b85\",\"matched_paper_id\":6038969},\"end\":113467,\"start\":113132},{\"attributes\":{\"id\":\"b86\",\"matched_paper_id\":7171595},\"end\":113745,\"start\":113469},{\"attributes\":{\"id\":\"b87\",\"matched_paper_id\":45889997},\"end\":113909,\"start\":113747},{\"attributes\":{\"id\":\"b88\",\"matched_paper_id\":62651264},\"end\":114191,\"start\":113911},{\"attributes\":{\"id\":\"b89\",\"matched_paper_id\":58316549},\"end\":114615,\"start\":114193},{\"attributes\":{\"id\":\"b90\",\"matched_paper_id\":8813649},\"end\":114990,\"start\":114617},{\"attributes\":{\"id\":\"b91\",\"matched_paper_id\":107001390},\"end\":115244,\"start\":114992},{\"attributes\":{\"id\":\"b92\"},\"end\":115600,\"start\":115246},{\"attributes\":{\"doi\":\"UCB-ITS-PRR-97-6\",\"id\":\"b93\",\"matched_paper_id\":107001390},\"end\":115957,\"start\":115602},{\"attributes\":{\"id\":\"b94\",\"matched_paper_id\":5311267},\"end\":116223,\"start\":115959},{\"attributes\":{\"id\":\"b95\",\"matched_paper_id\":7502395},\"end\":116604,\"start\":116225},{\"attributes\":{\"id\":\"b96\",\"matched_paper_id\":34699195},\"end\":116911,\"start\":116606},{\"attributes\":{\"id\":\"b97\",\"matched_paper_id\":7849052},\"end\":117272,\"start\":116913},{\"attributes\":{\"id\":\"b98\",\"matched_paper_id\":107001390},\"end\":117560,\"start\":117274},{\"attributes\":{\"id\":\"b99\",\"matched_paper_id\":63787450},\"end\":117962,\"start\":117562},{\"attributes\":{\"id\":\"b100\",\"matched_paper_id\":8539117},\"end\":118386,\"start\":117964},{\"attributes\":{\"id\":\"b101\",\"matched_paper_id\":8150245},\"end\":118686,\"start\":118388},{\"attributes\":{\"id\":\"b102\",\"matched_paper_id\":5402638},\"end\":118969,\"start\":118688},{\"attributes\":{\"id\":\"b103\",\"matched_paper_id\":1842570},\"end\":119366,\"start\":118971},{\"attributes\":{\"id\":\"b104\",\"matched_paper_id\":15433078},\"end\":119684,\"start\":119368},{\"attributes\":{\"id\":\"b105\",\"matched_paper_id\":10962002},\"end\":120043,\"start\":119686},{\"attributes\":{\"id\":\"b106\",\"matched_paper_id\":10724139},\"end\":120319,\"start\":120045},{\"attributes\":{\"id\":\"b107\",\"matched_paper_id\":14497446},\"end\":120701,\"start\":120321},{\"attributes\":{\"id\":\"b108\",\"matched_paper_id\":46737148},\"end\":121041,\"start\":120703},{\"attributes\":{\"id\":\"b109\",\"matched_paper_id\":15059404},\"end\":121374,\"start\":121043},{\"attributes\":{\"id\":\"b110\",\"matched_paper_id\":18291721},\"end\":121692,\"start\":121376},{\"attributes\":{\"id\":\"b111\",\"matched_paper_id\":17305597},\"end\":121924,\"start\":121694},{\"attributes\":{\"id\":\"b112\",\"matched_paper_id\":497801},\"end\":122265,\"start\":121926},{\"attributes\":{\"id\":\"b113\"},\"end\":122565,\"start\":122267},{\"attributes\":{\"id\":\"b114\"},\"end\":123013,\"start\":122567},{\"attributes\":{\"id\":\"b115\",\"matched_paper_id\":6821810},\"end\":123269,\"start\":123015},{\"attributes\":{\"id\":\"b116\",\"matched_paper_id\":18939846},\"end\":123579,\"start\":123271},{\"attributes\":{\"id\":\"b117\",\"matched_paper_id\":40895626},\"end\":123964,\"start\":123581},{\"attributes\":{\"id\":\"b118\",\"matched_paper_id\":20308379},\"end\":124217,\"start\":123966},{\"attributes\":{\"id\":\"b119\",\"matched_paper_id\":44714424},\"end\":124662,\"start\":124219},{\"attributes\":{\"id\":\"b120\"},\"end\":124919,\"start\":124664},{\"attributes\":{\"id\":\"b121\",\"matched_paper_id\":15378301},\"end\":125332,\"start\":124921},{\"attributes\":{\"id\":\"b122\",\"matched_paper_id\":22954008},\"end\":125533,\"start\":125334},{\"attributes\":{\"id\":\"b123\",\"matched_paper_id\":17642180},\"end\":125858,\"start\":125535},{\"attributes\":{\"id\":\"b124\",\"matched_paper_id\":20692972},\"end\":126159,\"start\":125860},{\"attributes\":{\"id\":\"b125\",\"matched_paper_id\":34669177},\"end\":126528,\"start\":126161},{\"attributes\":{\"id\":\"b126\",\"matched_paper_id\":61151940},\"end\":126976,\"start\":126530},{\"attributes\":{\"id\":\"b127\"},\"end\":127306,\"start\":126978},{\"attributes\":{\"id\":\"b128\",\"matched_paper_id\":19748810},\"end\":127728,\"start\":127308},{\"attributes\":{\"id\":\"b129\"},\"end\":127940,\"start\":127730},{\"attributes\":{\"id\":\"b130\",\"matched_paper_id\":376561},\"end\":128259,\"start\":127942},{\"attributes\":{\"id\":\"b131\",\"matched_paper_id\":13308074},\"end\":128639,\"start\":128261},{\"attributes\":{\"id\":\"b132\",\"matched_paper_id\":1545504},\"end\":128979,\"start\":128641},{\"attributes\":{\"id\":\"b133\",\"matched_paper_id\":11878199},\"end\":129253,\"start\":128981},{\"attributes\":{\"id\":\"b134\"},\"end\":129619,\"start\":129255},{\"attributes\":{\"id\":\"b135\",\"matched_paper_id\":16001336},\"end\":129915,\"start\":129621},{\"attributes\":{\"id\":\"b136\",\"matched_paper_id\":5180959},\"end\":130241,\"start\":129917},{\"attributes\":{\"id\":\"b137\",\"matched_paper_id\":17759901},\"end\":130520,\"start\":130243},{\"attributes\":{\"id\":\"b138\",\"matched_paper_id\":5956460},\"end\":130846,\"start\":130522},{\"attributes\":{\"id\":\"b139\",\"matched_paper_id\":2925120},\"end\":131113,\"start\":130848},{\"attributes\":{\"id\":\"b140\",\"matched_paper_id\":906184},\"end\":131374,\"start\":131115},{\"attributes\":{\"id\":\"b141\",\"matched_paper_id\":14343358},\"end\":131710,\"start\":131376},{\"attributes\":{\"id\":\"b142\",\"matched_paper_id\":683832},\"end\":131991,\"start\":131712},{\"attributes\":{\"id\":\"b143\",\"matched_paper_id\":1127387},\"end\":132423,\"start\":131993},{\"attributes\":{\"id\":\"b144\",\"matched_paper_id\":14509510},\"end\":132750,\"start\":132425},{\"attributes\":{\"id\":\"b145\",\"matched_paper_id\":42887466},\"end\":133072,\"start\":132752},{\"attributes\":{\"id\":\"b146\"},\"end\":133280,\"start\":133074},{\"attributes\":{\"id\":\"b147\"},\"end\":133328,\"start\":133282},{\"attributes\":{\"id\":\"b148\",\"matched_paper_id\":11060551},\"end\":133613,\"start\":133330},{\"attributes\":{\"id\":\"b149\",\"matched_paper_id\":10321229},\"end\":133927,\"start\":133615},{\"attributes\":{\"id\":\"b150\",\"matched_paper_id\":13910347},\"end\":134318,\"start\":133929},{\"attributes\":{\"id\":\"b151\",\"matched_paper_id\":18446469},\"end\":134583,\"start\":134320},{\"attributes\":{\"id\":\"b152\"},\"end\":134865,\"start\":134585},{\"attributes\":{\"id\":\"b153\",\"matched_paper_id\":2849376},\"end\":135289,\"start\":134867},{\"attributes\":{\"id\":\"b154\",\"matched_paper_id\":8946004},\"end\":135657,\"start\":135291},{\"attributes\":{\"id\":\"b155\",\"matched_paper_id\":9724011},\"end\":136002,\"start\":135659},{\"attributes\":{\"id\":\"b156\",\"matched_paper_id\":12012544},\"end\":136439,\"start\":136004},{\"attributes\":{\"id\":\"b157\",\"matched_paper_id\":60455395},\"end\":136571,\"start\":136441},{\"attributes\":{\"id\":\"b158\"},\"end\":136746,\"start\":136573},{\"attributes\":{\"id\":\"b159\",\"matched_paper_id\":15407300},\"end\":137038,\"start\":136748},{\"attributes\":{\"id\":\"b160\",\"matched_paper_id\":6689329},\"end\":137489,\"start\":137040},{\"attributes\":{\"id\":\"b161\",\"matched_paper_id\":13119692},\"end\":137782,\"start\":137491},{\"attributes\":{\"id\":\"b162\",\"matched_paper_id\":12672},\"end\":138216,\"start\":137784},{\"attributes\":{\"id\":\"b163\",\"matched_paper_id\":7536268},\"end\":138655,\"start\":138218},{\"attributes\":{\"id\":\"b164\",\"matched_paper_id\":14297649},\"end\":138964,\"start\":138657},{\"attributes\":{\"id\":\"b165\",\"matched_paper_id\":122585981},\"end\":139405,\"start\":138966},{\"attributes\":{\"id\":\"b166\",\"matched_paper_id\":2689158},\"end\":139682,\"start\":139407},{\"attributes\":{\"id\":\"b167\",\"matched_paper_id\":123395038},\"end\":140031,\"start\":139684},{\"attributes\":{\"id\":\"b168\",\"matched_paper_id\":59655490},\"end\":140418,\"start\":140033},{\"attributes\":{\"doi\":\"AIM-2001-019\",\"id\":\"b169\",\"matched_paper_id\":15209191},\"end\":140628,\"start\":140420},{\"attributes\":{\"id\":\"b170\",\"matched_paper_id\":52802388},\"end\":140933,\"start\":140630},{\"attributes\":{\"id\":\"b171\",\"matched_paper_id\":2200276},\"end\":141304,\"start\":140935},{\"attributes\":{\"id\":\"b172\",\"matched_paper_id\":15796248},\"end\":141727,\"start\":141306},{\"attributes\":{\"doi\":\"GIT-GVU-01-10\",\"id\":\"b173\"},\"end\":141980,\"start\":141729},{\"attributes\":{\"id\":\"b174\",\"matched_paper_id\":16750295},\"end\":142365,\"start\":141982},{\"attributes\":{\"id\":\"b175\",\"matched_paper_id\":18601560},\"end\":142660,\"start\":142367},{\"attributes\":{\"id\":\"b176\",\"matched_paper_id\":63074013},\"end\":142973,\"start\":142662},{\"attributes\":{\"id\":\"b177\",\"matched_paper_id\":2006961},\"end\":143267,\"start\":142975},{\"attributes\":{\"id\":\"b178\",\"matched_paper_id\":14117770},\"end\":143674,\"start\":143269},{\"attributes\":{\"id\":\"b179\",\"matched_paper_id\":1049642},\"end\":144083,\"start\":143676},{\"attributes\":{\"id\":\"b180\"},\"end\":144506,\"start\":144085},{\"attributes\":{\"id\":\"b181\",\"matched_paper_id\":33028117},\"end\":144827,\"start\":144508},{\"attributes\":{\"id\":\"b182\",\"matched_paper_id\":60096114},\"end\":145205,\"start\":144829},{\"attributes\":{\"id\":\"b183\",\"matched_paper_id\":5675687},\"end\":145630,\"start\":145207},{\"attributes\":{\"id\":\"b184\",\"matched_paper_id\":15976819},\"end\":146089,\"start\":145632},{\"attributes\":{\"id\":\"b185\",\"matched_paper_id\":2655924},\"end\":146379,\"start\":146091},{\"attributes\":{\"id\":\"b186\"},\"end\":146715,\"start\":146381}]", "bib_title": "[{\"end\":84190,\"start\":84133},{\"end\":84538,\"start\":84455},{\"end\":84943,\"start\":84897},{\"end\":85431,\"start\":85374},{\"end\":85765,\"start\":85697},{\"end\":86372,\"start\":86313},{\"end\":86701,\"start\":86656},{\"end\":87061,\"start\":86987},{\"end\":87385,\"start\":87323},{\"end\":87732,\"start\":87707},{\"end\":88042,\"start\":87985},{\"end\":88436,\"start\":88354},{\"end\":88762,\"start\":88724},{\"end\":89020,\"start\":88957},{\"end\":89393,\"start\":89322},{\"end\":89673,\"start\":89614},{\"end\":90039,\"start\":89969},{\"end\":90355,\"start\":90284},{\"end\":90739,\"start\":90671},{\"end\":91128,\"start\":91036},{\"end\":91477,\"start\":91408},{\"end\":91810,\"start\":91763},{\"end\":92109,\"start\":92054},{\"end\":92443,\"start\":92364},{\"end\":92804,\"start\":92737},{\"end\":93181,\"start\":93133},{\"end\":93549,\"start\":93481},{\"end\":93779,\"start\":93734},{\"end\":94076,\"start\":93988},{\"end\":94401,\"start\":94342},{\"end\":94683,\"start\":94618},{\"end\":94961,\"start\":94924},{\"end\":95320,\"start\":95279},{\"end\":95553,\"start\":95494},{\"end\":95952,\"start\":95876},{\"end\":96312,\"start\":96208},{\"end\":96645,\"start\":96575},{\"end\":97032,\"start\":96943},{\"end\":97354,\"start\":97307},{\"end\":97662,\"start\":97618},{\"end\":98001,\"start\":97919},{\"end\":98255,\"start\":98221},{\"end\":98596,\"start\":98533},{\"end\":98889,\"start\":98851},{\"end\":99179,\"start\":99097},{\"end\":99439,\"start\":99391},{\"end\":99732,\"start\":99638},{\"end\":100121,\"start\":100059},{\"end\":100510,\"start\":100467},{\"end\":100831,\"start\":100796},{\"end\":101146,\"start\":101060},{\"end\":101438,\"start\":101370},{\"end\":101888,\"start\":101778},{\"end\":102247,\"start\":102142},{\"end\":102617,\"start\":102532},{\"end\":103052,\"start\":103003},{\"end\":103463,\"start\":103420},{\"end\":103850,\"start\":103777},{\"end\":104308,\"start\":104260},{\"end\":104690,\"start\":104621},{\"end\":104993,\"start\":104939},{\"end\":105401,\"start\":105365},{\"end\":105622,\"start\":105566},{\"end\":106025,\"start\":105956},{\"end\":106403,\"start\":106377},{\"end\":106814,\"start\":106764},{\"end\":107249,\"start\":107198},{\"end\":107551,\"start\":107441},{\"end\":107851,\"start\":107786},{\"end\":108141,\"start\":108087},{\"end\":108461,\"start\":108411},{\"end\":108727,\"start\":108689},{\"end\":108987,\"start\":108938},{\"end\":109252,\"start\":109208},{\"end\":109456,\"start\":109427},{\"end\":109784,\"start\":109740},{\"end\":110170,\"start\":110065},{\"end\":110483,\"start\":110427},{\"end\":110827,\"start\":110759},{\"end\":111303,\"start\":111225},{\"end\":111686,\"start\":111627},{\"end\":112100,\"start\":112039},{\"end\":112436,\"start\":112376},{\"end\":112843,\"start\":112794},{\"end\":113215,\"start\":113132},{\"end\":113526,\"start\":113469},{\"end\":113786,\"start\":113747},{\"end\":113972,\"start\":113911},{\"end\":114254,\"start\":114193},{\"end\":114686,\"start\":114617},{\"end\":115049,\"start\":114992},{\"end\":115690,\"start\":115602},{\"end\":115999,\"start\":115959},{\"end\":116277,\"start\":116225},{\"end\":116656,\"start\":116606},{\"end\":116993,\"start\":116913},{\"end\":117363,\"start\":117274},{\"end\":117604,\"start\":117562},{\"end\":118025,\"start\":117964},{\"end\":118454,\"start\":118388},{\"end\":118750,\"start\":118688},{\"end\":119028,\"start\":118971},{\"end\":119422,\"start\":119368},{\"end\":119749,\"start\":119686},{\"end\":120111,\"start\":120045},{\"end\":120379,\"start\":120321},{\"end\":120764,\"start\":120703},{\"end\":121124,\"start\":121043},{\"end\":121417,\"start\":121376},{\"end\":121736,\"start\":121694},{\"end\":121990,\"start\":121926},{\"end\":122328,\"start\":122267},{\"end\":123079,\"start\":123015},{\"end\":123353,\"start\":123271},{\"end\":123672,\"start\":123581},{\"end\":124017,\"start\":123966},{\"end\":124283,\"start\":124219},{\"end\":124729,\"start\":124664},{\"end\":124955,\"start\":124921},{\"end\":125389,\"start\":125334},{\"end\":125614,\"start\":125535},{\"end\":125941,\"start\":125860},{\"end\":126278,\"start\":126161},{\"end\":126593,\"start\":126530},{\"end\":127050,\"start\":126978},{\"end\":127382,\"start\":127308},{\"end\":127770,\"start\":127730},{\"end\":127988,\"start\":127942},{\"end\":128350,\"start\":128261},{\"end\":128706,\"start\":128641},{\"end\":129030,\"start\":128981},{\"end\":129321,\"start\":129255},{\"end\":129652,\"start\":129621},{\"end\":129988,\"start\":129917},{\"end\":130278,\"start\":130243},{\"end\":130598,\"start\":130522},{\"end\":130918,\"start\":130848},{\"end\":131179,\"start\":131115},{\"end\":131443,\"start\":131376},{\"end\":131784,\"start\":131712},{\"end\":132056,\"start\":131993},{\"end\":132487,\"start\":132425},{\"end\":132819,\"start\":132752},{\"end\":133130,\"start\":133074},{\"end\":133379,\"start\":133330},{\"end\":133685,\"start\":133615},{\"end\":133997,\"start\":133929},{\"end\":134385,\"start\":134320},{\"end\":134637,\"start\":134585},{\"end\":134921,\"start\":134867},{\"end\":135379,\"start\":135291},{\"end\":135709,\"start\":135659},{\"end\":136067,\"start\":136004},{\"end\":136493,\"start\":136441},{\"end\":136807,\"start\":136748},{\"end\":137110,\"start\":137040},{\"end\":137554,\"start\":137491},{\"end\":137837,\"start\":137784},{\"end\":138301,\"start\":138218},{\"end\":138742,\"start\":138657},{\"end\":139032,\"start\":138966},{\"end\":139481,\"start\":139407},{\"end\":139753,\"start\":139684},{\"end\":140082,\"start\":140033},{\"end\":140468,\"start\":140420},{\"end\":140691,\"start\":140630},{\"end\":140994,\"start\":140935},{\"end\":141375,\"start\":141306},{\"end\":142048,\"start\":141982},{\"end\":142412,\"start\":142367},{\"end\":142750,\"start\":142662},{\"end\":143033,\"start\":142975},{\"end\":143322,\"start\":143269},{\"end\":143738,\"start\":143676},{\"end\":144152,\"start\":144085},{\"end\":144563,\"start\":144508},{\"end\":144874,\"start\":144829},{\"end\":145311,\"start\":145207},{\"end\":145699,\"start\":145632},{\"end\":146139,\"start\":146091},{\"end\":146444,\"start\":146381}]", "bib_author": "[{\"end\":84205,\"start\":84192},{\"end\":84217,\"start\":84205},{\"end\":84227,\"start\":84217},{\"end\":84552,\"start\":84540},{\"end\":84562,\"start\":84552},{\"end\":84571,\"start\":84562},{\"end\":84958,\"start\":84945},{\"end\":84970,\"start\":84958},{\"end\":84980,\"start\":84970},{\"end\":84993,\"start\":84980},{\"end\":85004,\"start\":84993},{\"end\":85012,\"start\":85004},{\"end\":85024,\"start\":85012},{\"end\":85035,\"start\":85024},{\"end\":85047,\"start\":85035},{\"end\":85055,\"start\":85047},{\"end\":85065,\"start\":85055},{\"end\":85447,\"start\":85433},{\"end\":85458,\"start\":85447},{\"end\":85469,\"start\":85458},{\"end\":85781,\"start\":85767},{\"end\":85791,\"start\":85781},{\"end\":85801,\"start\":85791},{\"end\":86061,\"start\":86049},{\"end\":86071,\"start\":86061},{\"end\":86384,\"start\":86374},{\"end\":86392,\"start\":86384},{\"end\":86400,\"start\":86392},{\"end\":86409,\"start\":86400},{\"end\":86713,\"start\":86703},{\"end\":86729,\"start\":86713},{\"end\":86740,\"start\":86729},{\"end\":86754,\"start\":86740},{\"end\":87072,\"start\":87063},{\"end\":87081,\"start\":87072},{\"end\":87399,\"start\":87387},{\"end\":87412,\"start\":87399},{\"end\":87423,\"start\":87412},{\"end\":87745,\"start\":87734},{\"end\":87754,\"start\":87745},{\"end\":87763,\"start\":87754},{\"end\":87776,\"start\":87763},{\"end\":87788,\"start\":87776},{\"end\":88056,\"start\":88044},{\"end\":88067,\"start\":88056},{\"end\":88447,\"start\":88438},{\"end\":88458,\"start\":88447},{\"end\":88469,\"start\":88458},{\"end\":88774,\"start\":88764},{\"end\":88783,\"start\":88774},{\"end\":88797,\"start\":88783},{\"end\":89034,\"start\":89022},{\"end\":89045,\"start\":89034},{\"end\":89406,\"start\":89395},{\"end\":89683,\"start\":89675},{\"end\":89695,\"start\":89683},{\"end\":89710,\"start\":89695},{\"end\":89722,\"start\":89710},{\"end\":90051,\"start\":90041},{\"end\":90062,\"start\":90051},{\"end\":90369,\"start\":90357},{\"end\":90753,\"start\":90741},{\"end\":91139,\"start\":91130},{\"end\":91146,\"start\":91139},{\"end\":91157,\"start\":91146},{\"end\":91488,\"start\":91479},{\"end\":91497,\"start\":91488},{\"end\":91505,\"start\":91497},{\"end\":91514,\"start\":91505},{\"end\":91821,\"start\":91812},{\"end\":92125,\"start\":92111},{\"end\":92132,\"start\":92125},{\"end\":92140,\"start\":92132},{\"end\":92150,\"start\":92140},{\"end\":92460,\"start\":92445},{\"end\":92470,\"start\":92460},{\"end\":92484,\"start\":92470},{\"end\":92812,\"start\":92806},{\"end\":92821,\"start\":92812},{\"end\":92831,\"start\":92821},{\"end\":93195,\"start\":93183},{\"end\":93208,\"start\":93195},{\"end\":93559,\"start\":93551},{\"end\":93792,\"start\":93781},{\"end\":93804,\"start\":93792},{\"end\":94090,\"start\":94078},{\"end\":94101,\"start\":94090},{\"end\":94418,\"start\":94403},{\"end\":94694,\"start\":94685},{\"end\":94703,\"start\":94694},{\"end\":94973,\"start\":94963},{\"end\":94983,\"start\":94973},{\"end\":95333,\"start\":95322},{\"end\":95344,\"start\":95333},{\"end\":95564,\"start\":95555},{\"end\":95574,\"start\":95564},{\"end\":95586,\"start\":95574},{\"end\":95963,\"start\":95954},{\"end\":95973,\"start\":95963},{\"end\":96324,\"start\":96314},{\"end\":96334,\"start\":96324},{\"end\":96346,\"start\":96334},{\"end\":96658,\"start\":96647},{\"end\":97041,\"start\":97034},{\"end\":97051,\"start\":97041},{\"end\":97060,\"start\":97051},{\"end\":97366,\"start\":97356},{\"end\":97374,\"start\":97366},{\"end\":97382,\"start\":97374},{\"end\":97393,\"start\":97382},{\"end\":97671,\"start\":97664},{\"end\":97685,\"start\":97671},{\"end\":98270,\"start\":98257},{\"end\":98279,\"start\":98270},{\"end\":98609,\"start\":98598},{\"end\":98617,\"start\":98609},{\"end\":98626,\"start\":98617},{\"end\":98900,\"start\":98891},{\"end\":98909,\"start\":98900},{\"end\":99190,\"start\":99181},{\"end\":99203,\"start\":99190},{\"end\":99454,\"start\":99441},{\"end\":99466,\"start\":99454},{\"end\":99477,\"start\":99466},{\"end\":99743,\"start\":99734},{\"end\":99751,\"start\":99743},{\"end\":100136,\"start\":100123},{\"end\":100145,\"start\":100136},{\"end\":100157,\"start\":100145},{\"end\":100519,\"start\":100512},{\"end\":100525,\"start\":100519},{\"end\":100533,\"start\":100525},{\"end\":100843,\"start\":100833},{\"end\":100853,\"start\":100843},{\"end\":100863,\"start\":100853},{\"end\":101158,\"start\":101148},{\"end\":101170,\"start\":101158},{\"end\":101450,\"start\":101440},{\"end\":101458,\"start\":101450},{\"end\":101899,\"start\":101890},{\"end\":101911,\"start\":101899},{\"end\":102255,\"start\":102249},{\"end\":102267,\"start\":102255},{\"end\":102636,\"start\":102619},{\"end\":102646,\"start\":102636},{\"end\":102655,\"start\":102646},{\"end\":103063,\"start\":103054},{\"end\":103073,\"start\":103063},{\"end\":103083,\"start\":103073},{\"end\":103094,\"start\":103083},{\"end\":103102,\"start\":103094},{\"end\":103112,\"start\":103102},{\"end\":103477,\"start\":103465},{\"end\":103490,\"start\":103477},{\"end\":103869,\"start\":103852},{\"end\":103879,\"start\":103869},{\"end\":103888,\"start\":103879},{\"end\":103897,\"start\":103888},{\"end\":104317,\"start\":104310},{\"end\":104328,\"start\":104317},{\"end\":104702,\"start\":104692},{\"end\":104711,\"start\":104702},{\"end\":105003,\"start\":104995},{\"end\":105018,\"start\":105003},{\"end\":105029,\"start\":105018},{\"end\":105039,\"start\":105029},{\"end\":105413,\"start\":105403},{\"end\":105422,\"start\":105413},{\"end\":105641,\"start\":105624},{\"end\":105648,\"start\":105641},{\"end\":105659,\"start\":105648},{\"end\":106044,\"start\":106027},{\"end\":106055,\"start\":106044},{\"end\":106416,\"start\":106405},{\"end\":106428,\"start\":106416},{\"end\":106438,\"start\":106428},{\"end\":106449,\"start\":106438},{\"end\":106464,\"start\":106449},{\"end\":106827,\"start\":106816},{\"end\":106837,\"start\":106827},{\"end\":107264,\"start\":107251},{\"end\":107273,\"start\":107264},{\"end\":107566,\"start\":107553},{\"end\":107576,\"start\":107566},{\"end\":107862,\"start\":107853},{\"end\":107871,\"start\":107862},{\"end\":107882,\"start\":107871},{\"end\":108152,\"start\":108143},{\"end\":108168,\"start\":108152},{\"end\":108178,\"start\":108168},{\"end\":108473,\"start\":108463},{\"end\":108484,\"start\":108473},{\"end\":108492,\"start\":108484},{\"end\":108740,\"start\":108729},{\"end\":108749,\"start\":108740},{\"end\":108995,\"start\":108989},{\"end\":109006,\"start\":108995},{\"end\":109262,\"start\":109254},{\"end\":109268,\"start\":109262},{\"end\":109275,\"start\":109268},{\"end\":109470,\"start\":109458},{\"end\":109483,\"start\":109470},{\"end\":109491,\"start\":109483},{\"end\":109503,\"start\":109491},{\"end\":109515,\"start\":109503},{\"end\":109794,\"start\":109786},{\"end\":109807,\"start\":109794},{\"end\":110182,\"start\":110172},{\"end\":110189,\"start\":110182},{\"end\":110494,\"start\":110485},{\"end\":110502,\"start\":110494},{\"end\":110520,\"start\":110502},{\"end\":110528,\"start\":110520},{\"end\":110844,\"start\":110829},{\"end\":110856,\"start\":110844},{\"end\":110866,\"start\":110856},{\"end\":110873,\"start\":110866},{\"end\":111315,\"start\":111305},{\"end\":111326,\"start\":111315},{\"end\":111338,\"start\":111326},{\"end\":111698,\"start\":111688},{\"end\":111707,\"start\":111698},{\"end\":111716,\"start\":111707},{\"end\":111725,\"start\":111716},{\"end\":111738,\"start\":111725},{\"end\":111745,\"start\":111738},{\"end\":111755,\"start\":111745},{\"end\":112112,\"start\":112102},{\"end\":112121,\"start\":112112},{\"end\":112132,\"start\":112121},{\"end\":112142,\"start\":112132},{\"end\":112449,\"start\":112438},{\"end\":112456,\"start\":112449},{\"end\":112468,\"start\":112456},{\"end\":112853,\"start\":112845},{\"end\":112863,\"start\":112853},{\"end\":113228,\"start\":113217},{\"end\":113537,\"start\":113528},{\"end\":113551,\"start\":113537},{\"end\":113562,\"start\":113551},{\"end\":113987,\"start\":113974},{\"end\":113996,\"start\":113987},{\"end\":114005,\"start\":113996},{\"end\":114014,\"start\":114005},{\"end\":114267,\"start\":114256},{\"end\":114277,\"start\":114267},{\"end\":114698,\"start\":114688},{\"end\":115321,\"start\":115312},{\"end\":115332,\"start\":115321},{\"end\":115341,\"start\":115332},{\"end\":115350,\"start\":115341},{\"end\":115360,\"start\":115350},{\"end\":115701,\"start\":115692},{\"end\":115712,\"start\":115701},{\"end\":116014,\"start\":116001},{\"end\":116026,\"start\":116014},{\"end\":116288,\"start\":116279},{\"end\":116300,\"start\":116288},{\"end\":116312,\"start\":116300},{\"end\":116324,\"start\":116312},{\"end\":116333,\"start\":116324},{\"end\":116346,\"start\":116333},{\"end\":116355,\"start\":116346},{\"end\":116667,\"start\":116658},{\"end\":116678,\"start\":116667},{\"end\":116689,\"start\":116678},{\"end\":117006,\"start\":116995},{\"end\":117016,\"start\":117006},{\"end\":117030,\"start\":117016},{\"end\":117039,\"start\":117030},{\"end\":117374,\"start\":117365},{\"end\":117385,\"start\":117374},{\"end\":117615,\"start\":117606},{\"end\":117625,\"start\":117615},{\"end\":118038,\"start\":118027},{\"end\":118469,\"start\":118456},{\"end\":118481,\"start\":118469},{\"end\":119046,\"start\":119030},{\"end\":119056,\"start\":119046},{\"end\":119436,\"start\":119424},{\"end\":119443,\"start\":119436},{\"end\":119759,\"start\":119751},{\"end\":119769,\"start\":119759},{\"end\":119779,\"start\":119769},{\"end\":120124,\"start\":120113},{\"end\":120137,\"start\":120124},{\"end\":120392,\"start\":120381},{\"end\":120780,\"start\":120766},{\"end\":120789,\"start\":120780},{\"end\":121133,\"start\":121126},{\"end\":121141,\"start\":121133},{\"end\":121427,\"start\":121419},{\"end\":121436,\"start\":121427},{\"end\":121445,\"start\":121436},{\"end\":121453,\"start\":121445},{\"end\":121462,\"start\":121453},{\"end\":121748,\"start\":121738},{\"end\":122004,\"start\":121992},{\"end\":122012,\"start\":122004},{\"end\":122021,\"start\":122012},{\"end\":122030,\"start\":122021},{\"end\":122341,\"start\":122330},{\"end\":122350,\"start\":122341},{\"end\":122362,\"start\":122350},{\"end\":122620,\"start\":122607},{\"end\":122635,\"start\":122620},{\"end\":122650,\"start\":122635},{\"end\":123090,\"start\":123081},{\"end\":123099,\"start\":123090},{\"end\":123685,\"start\":123674},{\"end\":123696,\"start\":123685},{\"end\":123704,\"start\":123696},{\"end\":124028,\"start\":124019},{\"end\":124039,\"start\":124028},{\"end\":124294,\"start\":124285},{\"end\":124308,\"start\":124294},{\"end\":124319,\"start\":124308},{\"end\":124969,\"start\":124957},{\"end\":124982,\"start\":124969},{\"end\":125626,\"start\":125616},{\"end\":125640,\"start\":125626},{\"end\":125652,\"start\":125640},{\"end\":125954,\"start\":125943},{\"end\":125966,\"start\":125954},{\"end\":126288,\"start\":126280},{\"end\":126300,\"start\":126288},{\"end\":126608,\"start\":126595},{\"end\":126616,\"start\":126608},{\"end\":126626,\"start\":126616},{\"end\":126633,\"start\":126626},{\"end\":127064,\"start\":127052},{\"end\":127076,\"start\":127064},{\"end\":127396,\"start\":127384},{\"end\":127408,\"start\":127396},{\"end\":127419,\"start\":127408},{\"end\":127782,\"start\":127772},{\"end\":127792,\"start\":127782},{\"end\":128001,\"start\":127990},{\"end\":128012,\"start\":128001},{\"end\":128363,\"start\":128352},{\"end\":128373,\"start\":128363},{\"end\":128385,\"start\":128373},{\"end\":128720,\"start\":128708},{\"end\":128731,\"start\":128720},{\"end\":128745,\"start\":128731},{\"end\":129041,\"start\":129032},{\"end\":129054,\"start\":129041},{\"end\":129331,\"start\":129323},{\"end\":129340,\"start\":129331},{\"end\":129663,\"start\":129654},{\"end\":129679,\"start\":129663},{\"end\":129687,\"start\":129679},{\"end\":129697,\"start\":129687},{\"end\":130002,\"start\":129990},{\"end\":130014,\"start\":130002},{\"end\":130289,\"start\":130280},{\"end\":130608,\"start\":130600},{\"end\":130621,\"start\":130608},{\"end\":130931,\"start\":130920},{\"end\":130939,\"start\":130931},{\"end\":131192,\"start\":131181},{\"end\":131203,\"start\":131192},{\"end\":131454,\"start\":131445},{\"end\":131464,\"start\":131454},{\"end\":131796,\"start\":131786},{\"end\":131808,\"start\":131796},{\"end\":132067,\"start\":132058},{\"end\":132077,\"start\":132067},{\"end\":132086,\"start\":132077},{\"end\":132099,\"start\":132086},{\"end\":132106,\"start\":132099},{\"end\":132117,\"start\":132106},{\"end\":132126,\"start\":132117},{\"end\":132502,\"start\":132489},{\"end\":132509,\"start\":132502},{\"end\":132518,\"start\":132509},{\"end\":132834,\"start\":132821},{\"end\":132845,\"start\":132834},{\"end\":133299,\"start\":133284},{\"end\":133394,\"start\":133381},{\"end\":133403,\"start\":133394},{\"end\":133416,\"start\":133403},{\"end\":133427,\"start\":133416},{\"end\":133699,\"start\":133687},{\"end\":133711,\"start\":133699},{\"end\":133726,\"start\":133711},{\"end\":133734,\"start\":133726},{\"end\":134008,\"start\":133999},{\"end\":134016,\"start\":134008},{\"end\":134027,\"start\":134016},{\"end\":134035,\"start\":134027},{\"end\":134403,\"start\":134387},{\"end\":134415,\"start\":134403},{\"end\":134652,\"start\":134639},{\"end\":134664,\"start\":134652},{\"end\":134677,\"start\":134664},{\"end\":134687,\"start\":134677},{\"end\":134934,\"start\":134923},{\"end\":134943,\"start\":134934},{\"end\":134953,\"start\":134943},{\"end\":135391,\"start\":135381},{\"end\":135402,\"start\":135391},{\"end\":135725,\"start\":135711},{\"end\":135738,\"start\":135725},{\"end\":135752,\"start\":135738},{\"end\":135763,\"start\":135752},{\"end\":136079,\"start\":136069},{\"end\":136090,\"start\":136079},{\"end\":136102,\"start\":136090},{\"end\":136819,\"start\":136809},{\"end\":136831,\"start\":136819},{\"end\":136842,\"start\":136831},{\"end\":137121,\"start\":137112},{\"end\":137132,\"start\":137121},{\"end\":137144,\"start\":137132},{\"end\":137849,\"start\":137839},{\"end\":137857,\"start\":137849},{\"end\":137868,\"start\":137857},{\"end\":137880,\"start\":137868},{\"end\":138320,\"start\":138303},{\"end\":138330,\"start\":138320},{\"end\":138754,\"start\":138744},{\"end\":138763,\"start\":138754},{\"end\":139045,\"start\":139034},{\"end\":139057,\"start\":139045},{\"end\":139068,\"start\":139057},{\"end\":140097,\"start\":140084},{\"end\":140108,\"start\":140097},{\"end\":140120,\"start\":140108},{\"end\":140477,\"start\":140470},{\"end\":140702,\"start\":140693},{\"end\":140710,\"start\":140702},{\"end\":141006,\"start\":140996},{\"end\":141017,\"start\":141006},{\"end\":141388,\"start\":141377},{\"end\":141398,\"start\":141388},{\"end\":141808,\"start\":141798},{\"end\":141819,\"start\":141808},{\"end\":142067,\"start\":142050},{\"end\":142077,\"start\":142067},{\"end\":142086,\"start\":142077},{\"end\":142423,\"start\":142414},{\"end\":142431,\"start\":142423},{\"end\":142439,\"start\":142431},{\"end\":142448,\"start\":142439},{\"end\":142760,\"start\":142752},{\"end\":142767,\"start\":142760},{\"end\":142776,\"start\":142767},{\"end\":143045,\"start\":143035},{\"end\":143054,\"start\":143045},{\"end\":143341,\"start\":143324},{\"end\":143351,\"start\":143341},{\"end\":143360,\"start\":143351},{\"end\":143751,\"start\":143740},{\"end\":143760,\"start\":143751},{\"end\":143767,\"start\":143760},{\"end\":144171,\"start\":144154},{\"end\":144180,\"start\":144171},{\"end\":144574,\"start\":144565},{\"end\":144581,\"start\":144574},{\"end\":144885,\"start\":144876},{\"end\":144895,\"start\":144885},{\"end\":144905,\"start\":144895},{\"end\":145324,\"start\":145313},{\"end\":145334,\"start\":145324},{\"end\":145718,\"start\":145701},{\"end\":145728,\"start\":145718},{\"end\":145737,\"start\":145728},{\"end\":146149,\"start\":146141},{\"end\":146157,\"start\":146149},{\"end\":146166,\"start\":146157},{\"end\":146454,\"start\":146446},{\"end\":146464,\"start\":146454},{\"end\":146472,\"start\":146464}]", "bib_venue": "[{\"end\":84267,\"start\":84227},{\"end\":84631,\"start\":84571},{\"end\":85100,\"start\":85080},{\"end\":85509,\"start\":85469},{\"end\":85818,\"start\":85801},{\"end\":86047,\"start\":85993},{\"end\":86448,\"start\":86409},{\"end\":86794,\"start\":86754},{\"end\":87121,\"start\":87081},{\"end\":87474,\"start\":87423},{\"end\":87820,\"start\":87788},{\"end\":88123,\"start\":88067},{\"end\":88507,\"start\":88469},{\"end\":88815,\"start\":88797},{\"end\":89100,\"start\":89045},{\"end\":89439,\"start\":89406},{\"end\":89758,\"start\":89722},{\"end\":90102,\"start\":90062},{\"end\":90428,\"start\":90369},{\"end\":90809,\"start\":90753},{\"end\":91190,\"start\":91157},{\"end\":91550,\"start\":91514},{\"end\":91861,\"start\":91821},{\"end\":92181,\"start\":92150},{\"end\":92517,\"start\":92484},{\"end\":92891,\"start\":92831},{\"end\":93264,\"start\":93208},{\"end\":93585,\"start\":93559},{\"end\":93836,\"start\":93804},{\"end\":94141,\"start\":94101},{\"end\":94458,\"start\":94418},{\"end\":94739,\"start\":94703},{\"end\":95046,\"start\":94983},{\"end\":95360,\"start\":95344},{\"end\":95642,\"start\":95586},{\"end\":96009,\"start\":95973},{\"end\":96365,\"start\":96346},{\"end\":96714,\"start\":96658},{\"end\":97100,\"start\":97060},{\"end\":97429,\"start\":97393},{\"end\":97721,\"start\":97685},{\"end\":98043,\"start\":98003},{\"end\":98335,\"start\":98279},{\"end\":98659,\"start\":98626},{\"end\":98949,\"start\":98909},{\"end\":99219,\"start\":99203},{\"end\":99487,\"start\":99477},{\"end\":99806,\"start\":99751},{\"end\":100217,\"start\":100157},{\"end\":100588,\"start\":100533},{\"end\":100903,\"start\":100863},{\"end\":101192,\"start\":101170},{\"end\":101511,\"start\":101458},{\"end\":101933,\"start\":101911},{\"end\":102303,\"start\":102267},{\"end\":102710,\"start\":102655},{\"end\":103156,\"start\":103112},{\"end\":103553,\"start\":103490},{\"end\":103967,\"start\":103897},{\"end\":104383,\"start\":104328},{\"end\":104755,\"start\":104711},{\"end\":105094,\"start\":105039},{\"end\":105439,\"start\":105422},{\"end\":105715,\"start\":105659},{\"end\":106110,\"start\":106055},{\"end\":106519,\"start\":106464},{\"end\":106948,\"start\":106837},{\"end\":107292,\"start\":107273},{\"end\":107586,\"start\":107576},{\"end\":107897,\"start\":107882},{\"end\":108222,\"start\":108178},{\"end\":108524,\"start\":108492},{\"end\":108789,\"start\":108749},{\"end\":109038,\"start\":109006},{\"end\":109291,\"start\":109275},{\"end\":109555,\"start\":109515},{\"end\":109857,\"start\":109807},{\"end\":110221,\"start\":110189},{\"end\":110561,\"start\":110528},{\"end\":110930,\"start\":110873},{\"end\":111386,\"start\":111338},{\"end\":111791,\"start\":111755},{\"end\":112174,\"start\":112142},{\"end\":112524,\"start\":112468},{\"end\":112919,\"start\":112863},{\"end\":113260,\"start\":113228},{\"end\":113581,\"start\":113562},{\"end\":113805,\"start\":113788},{\"end\":114024,\"start\":114014},{\"end\":114336,\"start\":114277},{\"end\":114749,\"start\":114698},{\"end\":115088,\"start\":115051},{\"end\":115310,\"start\":115246},{\"end\":115747,\"start\":115728},{\"end\":116066,\"start\":116026},{\"end\":116371,\"start\":116355},{\"end\":116733,\"start\":116689},{\"end\":117066,\"start\":117039},{\"end\":117404,\"start\":117385},{\"end\":117692,\"start\":117625},{\"end\":118114,\"start\":118038},{\"end\":118513,\"start\":118481},{\"end\":118784,\"start\":118752},{\"end\":119112,\"start\":119056},{\"end\":119475,\"start\":119443},{\"end\":119812,\"start\":119779},{\"end\":120159,\"start\":120137},{\"end\":120448,\"start\":120392},{\"end\":120825,\"start\":120789},{\"end\":121174,\"start\":121141},{\"end\":121500,\"start\":121462},{\"end\":121788,\"start\":121748},{\"end\":122070,\"start\":122030},{\"end\":122392,\"start\":122362},{\"end\":122605,\"start\":122567},{\"end\":123118,\"start\":123099},{\"end\":123391,\"start\":123355},{\"end\":123740,\"start\":123704},{\"end\":124067,\"start\":124039},{\"end\":124383,\"start\":124319},{\"end\":124764,\"start\":124731},{\"end\":125058,\"start\":124982},{\"end\":125408,\"start\":125391},{\"end\":125671,\"start\":125652},{\"end\":125985,\"start\":125966},{\"end\":126319,\"start\":126300},{\"end\":126696,\"start\":126633},{\"end\":127116,\"start\":127076},{\"end\":127475,\"start\":127419},{\"end\":127809,\"start\":127792},{\"end\":128063,\"start\":128012},{\"end\":128425,\"start\":128385},{\"end\":128785,\"start\":128745},{\"end\":129094,\"start\":129054},{\"end\":129396,\"start\":129340},{\"end\":129740,\"start\":129697},{\"end\":130054,\"start\":130014},{\"end\":130344,\"start\":130289},{\"end\":130661,\"start\":130621},{\"end\":130956,\"start\":130939},{\"end\":131220,\"start\":131203},{\"end\":131508,\"start\":131464},{\"end\":131828,\"start\":131808},{\"end\":132170,\"start\":132126},{\"end\":132555,\"start\":132518},{\"end\":132881,\"start\":132845},{\"end\":133152,\"start\":133132},{\"end\":133444,\"start\":133427},{\"end\":133744,\"start\":133734},{\"end\":134077,\"start\":134035},{\"end\":134425,\"start\":134415},{\"end\":134697,\"start\":134687},{\"end\":135027,\"start\":134953},{\"end\":135438,\"start\":135402},{\"end\":135796,\"start\":135763},{\"end\":136172,\"start\":136102},{\"end\":136499,\"start\":136495},{\"end\":136629,\"start\":136573},{\"end\":136864,\"start\":136842},{\"end\":137214,\"start\":137144},{\"end\":137588,\"start\":137556},{\"end\":137950,\"start\":137880},{\"end\":138386,\"start\":138330},{\"end\":138785,\"start\":138763},{\"end\":139132,\"start\":139068},{\"end\":139516,\"start\":139483},{\"end\":139815,\"start\":139755},{\"end\":140180,\"start\":140120},{\"end\":140499,\"start\":140489},{\"end\":140746,\"start\":140710},{\"end\":141073,\"start\":141017},{\"end\":141468,\"start\":141398},{\"end\":141796,\"start\":141729},{\"end\":142122,\"start\":142086},{\"end\":142481,\"start\":142448},{\"end\":142791,\"start\":142776},{\"end\":143098,\"start\":143054},{\"end\":143415,\"start\":143360},{\"end\":143822,\"start\":143767},{\"end\":144235,\"start\":144180},{\"end\":144617,\"start\":144581},{\"end\":144971,\"start\":144905},{\"end\":145370,\"start\":145334},{\"end\":145810,\"start\":145737},{\"end\":146202,\"start\":146166},{\"end\":146510,\"start\":146472},{\"end\":84687,\"start\":84633},{\"end\":86483,\"start\":86450},{\"end\":87157,\"start\":87123},{\"end\":87521,\"start\":87476},{\"end\":88175,\"start\":88125},{\"end\":88537,\"start\":88509},{\"end\":89151,\"start\":89102},{\"end\":89468,\"start\":89441},{\"end\":89790,\"start\":89760},{\"end\":90496,\"start\":90430},{\"end\":90861,\"start\":90811},{\"end\":91219,\"start\":91192},{\"end\":91582,\"start\":91552},{\"end\":91909,\"start\":91863},{\"end\":92546,\"start\":92519},{\"end\":92947,\"start\":92893},{\"end\":93316,\"start\":93266},{\"end\":94771,\"start\":94741},{\"end\":95115,\"start\":95048},{\"end\":95694,\"start\":95644},{\"end\":96041,\"start\":96011},{\"end\":96766,\"start\":96716},{\"end\":97461,\"start\":97431},{\"end\":97768,\"start\":97723},{\"end\":98387,\"start\":98337},{\"end\":98688,\"start\":98661},{\"end\":99493,\"start\":99489},{\"end\":99857,\"start\":99808},{\"end\":100273,\"start\":100219},{\"end\":100639,\"start\":100590},{\"end\":101572,\"start\":101513},{\"end\":102335,\"start\":102305},{\"end\":102775,\"start\":102712},{\"end\":103211,\"start\":103158},{\"end\":103612,\"start\":103555},{\"end\":104033,\"start\":103969},{\"end\":104448,\"start\":104385},{\"end\":105159,\"start\":105096},{\"end\":105767,\"start\":105717},{\"end\":106175,\"start\":106112},{\"end\":106542,\"start\":106531},{\"end\":106962,\"start\":106950},{\"end\":107592,\"start\":107588},{\"end\":107908,\"start\":107899},{\"end\":109066,\"start\":109040},{\"end\":109903,\"start\":109859},{\"end\":110590,\"start\":110563},{\"end\":111000,\"start\":110932},{\"end\":111430,\"start\":111388},{\"end\":111829,\"start\":111793},{\"end\":112202,\"start\":112176},{\"end\":112593,\"start\":112526},{\"end\":112971,\"start\":112921},{\"end\":113301,\"start\":113262},{\"end\":114030,\"start\":114026},{\"end\":114381,\"start\":114351},{\"end\":114812,\"start\":114751},{\"end\":117755,\"start\":117694},{\"end\":118194,\"start\":118116},{\"end\":118827,\"start\":118786},{\"end\":119173,\"start\":119114},{\"end\":119524,\"start\":119477},{\"end\":119861,\"start\":119814},{\"end\":120521,\"start\":120450},{\"end\":120872,\"start\":120827},{\"end\":121206,\"start\":121176},{\"end\":121530,\"start\":121502},{\"end\":123423,\"start\":123393},{\"end\":123772,\"start\":123742},{\"end\":124415,\"start\":124398},{\"end\":124793,\"start\":124766},{\"end\":125146,\"start\":125060},{\"end\":126765,\"start\":126698},{\"end\":127527,\"start\":127477},{\"end\":128110,\"start\":128065},{\"end\":129448,\"start\":129398},{\"end\":130395,\"start\":130346},{\"end\":131548,\"start\":131510},{\"end\":132210,\"start\":132172},{\"end\":132588,\"start\":132557},{\"end\":132913,\"start\":132883},{\"end\":133181,\"start\":133167},{\"end\":133750,\"start\":133746},{\"end\":134125,\"start\":134079},{\"end\":134431,\"start\":134427},{\"end\":134703,\"start\":134699},{\"end\":135097,\"start\":135029},{\"end\":135470,\"start\":135440},{\"end\":135825,\"start\":135798},{\"end\":136238,\"start\":136174},{\"end\":136505,\"start\":136501},{\"end\":137280,\"start\":137216},{\"end\":137637,\"start\":137590},{\"end\":138016,\"start\":137952},{\"end\":138438,\"start\":138388},{\"end\":139188,\"start\":139134},{\"end\":139545,\"start\":139518},{\"end\":139871,\"start\":139817},{\"end\":140236,\"start\":140182},{\"end\":140778,\"start\":140748},{\"end\":141125,\"start\":141075},{\"end\":141534,\"start\":141470},{\"end\":142168,\"start\":142124},{\"end\":142510,\"start\":142483},{\"end\":143480,\"start\":143417},{\"end\":143887,\"start\":143824},{\"end\":144305,\"start\":144237},{\"end\":144663,\"start\":144619},{\"end\":145033,\"start\":144973},{\"end\":145416,\"start\":145372},{\"end\":145879,\"start\":145812},{\"end\":146234,\"start\":146204},{\"end\":146540,\"start\":146512}]"}}}, "year": 2023, "month": 12, "day": 17}