{"id": 253881035, "updated": "2023-11-11 05:48:56.606", "metadata": {"title": "A Prediction based Autoscaling in Serverless Computing", "authors": "[{\"first\":\"Ha-Duong\",\"last\":\"Phung\",\"middle\":[]},{\"first\":\"Younghan\",\"last\":\"Kim\",\"middle\":[]}]", "venue": "2022 13th International Conference on Information and Communication Technology Convergence (ICTC)", "journal": "2022 13th International Conference on Information and Communication Technology Convergence (ICTC)", "publication_date": {"year": 2022, "month": 10, "day": 19}, "abstract": "Nowadays, serverless computing has risen rapidly as an emerging cloud computing paradigm in recent years. Auto-scaling is a crucial enabler for adapting to workload changes by scaling instances corresponding to the number of incoming requests. Recently, a popular Kubernetes-based platform for managing serverless workloads known as Knative has been proposed. Its scaling algorithm increases and decreases resources based on configured parameter values such as a maximum number of requests that can be processed in parallel per pod. However, users have no idea about the appropriate parameter values, while those predefined values can strongly impact the performance. The negative influence can cause response time increase and throughput reduction if those are overprovisioning or under-provisioning. Besides, Knative uses a moving average method to calculate the number of pods based on past data but cannot reflect the future workload trend, leading to the delay effect. In this paper, we focus on maximizing performance with the lowest resource utilization and optimizing response time to satisfy the quality of service (QoS) requirements so that it is content with users' experience while users only must pay the least cost. To do that, our proposed approach first finds effective scaling policies per pod to maximize the performance with the acceptable latency. Then we apply the forecasting model Bi-LSTM to optimize the number of pods calculation and make Knative more adaptive to the workload changes to reduce the delay time. Our preliminary experiments show that our proposed model can significantly improve performance with an effective scaling policy and an adaptive pod calculation method compared to the default Knative auto-scaling scheme.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/ictc/PhungK22", "doi": "10.1109/ictc55196.2022.9952609"}}, "content": {"source": {"pdf_hash": "7274fb972de3ac02f6eca7bec221eb6c2dc0a2f6", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "744f651729b8ae9f9b70d89a7ab24b4acb8b9a25", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/7274fb972de3ac02f6eca7bec221eb6c2dc0a2f6.txt", "contents": "\nA Prediction based Autoscaling in Serverless Computing\n\n\nHa-Duong Phung phunghaduong@dcn.ssu.ac.kr \nSchool of Electronic Engineering\nSoongsil University\nSeoulKorea\n\nYounghan Kim younghak@dcn.ssu.ac.kr \nSchool of Electronic Engineering\nSoongsil University\nSeoulKorea\n\nA Prediction based Autoscaling in Serverless Computing\n10.1109/ICTC55196.2022.9952609serverless computingauto-scalingKnativeBi- LSTM\nNowadays, serverless computing has risen rapidly as an emerging cloud computing paradigm in recent years. Auto-scaling is a crucial enabler for adapting to workload changes by scaling instances corresponding to the number of incoming requests. Recently, a popular Kubernetes-based platform for managing serverless workloads known as Knative has been proposed. Its scaling algorithm increases and decreases resources based on configured parameter values such as a maximum number of requests that can be processed in parallel per pod. However, users have no idea about the appropriate parameter values, while those predefined values can strongly impact the performance. The negative influence can cause response time increase and throughput reduction if those are overprovisioning or under-provisioning. Besides, Knative uses a moving average method to calculate the number of pods based on past data but cannot reflect the future workload trend, leading to the delay effect. In this paper, we focus on maximizing performance with the lowest resource utilization and optimizing response time to satisfy the quality of service (QoS) requirements so that it is content with users' experience while users only must pay the least cost. To do that, our proposed approach first finds effective scaling policies per pod to maximize the performance with the acceptable latency. Then we apply the forecasting model Bi-LSTM to optimize the number of pods calculation and make Knative more adaptive to the workload changes to reduce the delay time. Our preliminary experiments show that our proposed model can significantly improve performance with an effective scaling policy and an adaptive pod calculation method compared to the default Knative auto-scaling scheme.\n\nI. INTRODUCTION\n\nCloud-native systems are emerging, driven by the promising properties of cloud computing, such as pay-as-yougo, flexibility, and on-demand. In recent years, serverless computing has risen rapidly as a new type of cloud computing paradigm [1]. It is an emerging paradigm for running userspecified functions on provider resources with virtually unlimited scalability. Users only need to focus on writing and maintaining the code, and the remaining tasks will be done by the cloud provider in charge of setting up and maintaining the infrastructure [2]. The cloud provider is responsible for provisioning and managing the server resources that help the user eliminate the administrative burden. Depending on the incoming workload, server resources are very flexible and scaled out or in automatically. Furthermore, one function that makes serverless become a lucrative choice is a fine-grained pay-as-you-go pricing function, which indicates that the user only has to pay whenever resources are actually used.\n\nDue to the fast growth of serverless computing offerings, the academic community is becoming more interested in comparing various solutions, and one of the crucial criteria for evaluation is scalability [3]. Several studies have compared the performance of several proprietary serverless systems, concentrating on Amazon Lambda, Microsoft Azure Functions [4], Google Cloud Functions [5], and IBM Cloud Functions [6]. Li et al. clearly contrasts workload-based and resource-based scaling policies [3]. The authors compare the effectiveness of various workload situations by adjusting concurrency levels in Knative, and they strongly advise more research directions for this auto-scaling capability. In the former, scaling decisions are made on predefined thresholds and are most popular among public cloud providers. Despite the simplistic implementation, identifying suitable thresholds requires expert knowledge [7] or explicit application understanding. Our work in the preliminary section shows that, depending on different types of applications, different parameter levels such as the concurrency levels can impact performance and cause latency changes of up to multiple seconds. Since when the concurrency level configuration is too high than the process capability of the application, additional queued requests pile up, resulting in the latency increase.\n\nAlthough serverless computing brings many advantages, it also contains some main performance challenges: cold start latency, performance isolation of function, and workload scheduling challenge [8]. As one of the most popular opensource serverless solutions, Knative can deal with these scheduling problems by adding components for deploying, running, and managing serverless, cloud-native applications to Kubernetes [9]. It supports parallel request processing up to a predefined number of concurrent requests per instance. [9] When increasing workload reaches the so-called concurrency threshold, Knative Pod Autoscaler (KPA) will calculate the appropriate number of instances and deploy the corresponding number into Kubernetes infrastructure. Nevertheless, Knative uses a moving average method to calculate the number of pods based on past data and cannot adapt well to the fluctuation of a request's trend.\n\nTo address the above issues, our proposed method automatically optimizes an effective scaling policy to a specific application without needing any prior knowledge. With the same amount of resource per pod, our purpose focuses on outputting an appropriate parameter level showing the throughput maximum and an acceptable latency to satisfy SLO violation. Based on that, a new service revision with the new scheduling policy is created, and then we apply the forecasting model Bi-LSTM to optimize the calculation of the number of pods. The structure of this paper is organized as follows. In Section II, we briefly describe the serverless platform Knative and analyze some related forecasting models. In Section III, we demonstrate a detail of our proposed method structure. Section IV presents the results showing how different concurrency limits impact performance and the efficiency of our prediction model Bi-*Corresponding author: YoungHan Kim (younghak@dcn.ssu.ac.kr) LSTM compared to the Knative default. Finally, in Section V, we conclude and point out future work plans.\n\n\nII. BACKGROUND\n\n\nA. Knative Serverless Platform\n\nAs an open source serverless platform, Knative provides two main components: Eventing components and Serving components which are based on Kubernetes to support deploying and serving modern serverless applications. The Serving components help deploy serverless applications as Knative services and can scale them automatically, even down to zero. The Knative Eventing components are responsible for addressing most cloud-native message needs. In this paper, we focus on optimizing Knative's scheduling, so we mainly describe Serving components.  The auto-scaling function is implemented by different serving components, as described by the request flow in Fig.  2. The activator is responsible for buffering the coming requests and reporting to the autoscaler. The autoscaler collects metrics on the application instances and decides how many pods the application's deployment should be scaled to. The queue-proxy allows and limits a certain number of requests to come to the user-container simultaneously, and queues exceed demands. By default, Knative Pod Autoscaler supports auto-scaling based on two metrics those are concurrency and rps metrics. Knative allows users to custom those required threshold values of one service revision.\n\nBecause the Knative autoscaler scheme acts the same way on each scheduling policy, we choose the concurrency metrics for our examples. The autoscaler calculates the number of pods depending on the moving average value of parameters. For example, suppose \uf001 \uf002 is a setting value that represents the maximum number of concurrent requests that one pod can handle, \uf003 \uf004 is the actual average number of the concurrency per pod in each bucket. We calculate a number of desired pods \uf005 by \uf006 steps moving average method as below equation:\n\uf005 = \uf003 \uf004 + \uf003 \uf004\uf00a\uf00b + \u22ef + \uf003 \uf004\uf00a\uf00d\uf00e\uf00b \uf001 \uf002 * \uf006\nAlthough the current calculation method can guarantee that the number of pods is adaptive with the extension of the workload, it can't meet the request needs very well. It can't reflect the request trend immediately; this leads to a significant delay effect that directly influences the user's experience.\n\nOn the other hand, by Knative default, the value of the concurrency target is set to 100. This configuration means that the autoscaling process only occurs when the current number of parallel requests per pod at a given time climbs up to 100. Users can custom this concurrency level between 0 and 1000. The problem is whether the default value is good enough and, if not, how to determine the appropriate concurrency level to maximize user performance.\n\n\nB. Forecasting model\n\nOur purpose is to make the current Knative calculation method adaptive to the future of the workload coming to the service. The workload here can be the current number of concurrency or the current number of requests per second on a pod. These days, applying machine learning algorithms to time series forecasting is a very demanding area for researchers due to its enormous potential. There are several time-series forecasting models Autoregressive (AR), Autoregressive\n\nMoving Average (ARMA), and Autoregressive Integrated Moving Average (ARIMA) [10] [11] [12]. Some significant patterns depending on traditional methods such as the linear regression methods cannot output results precisely due to the complex nature of time series models. The workload data chain can be considered a nonlinear time series, so it is hard to foresee or estimate this future value without nonlinear techniques [13]. We need a model to help us foresee the future value of the workload with our exceptionally nonlinear demonstrating data, which deep neural methods can deal with. A neural network makes an effort to map and emphasize the information needed to understand a function and, as a result, produce a better anticipated output. The Gated Recurrent Unit (GRU) deep learning technique [14] uses a similar structure like the LSTM model, but the architecture of the memory cell is simplified. GRU takes less time to train and performs well enough on a small amount of data. In comparison, LSTM is more reliable and efficient for non-linearity in larger datasets [15]. LSTM is a particular type of recurrent neural network which can solve the problem of vanishing and exploding gradients in RNN. For the LSTM model, three different gates known as the update gate, forget gate, and output gate are used to pass recently experienced data from one cell to another. It includes two outputs from one cell, one is the activation, and another is the candidate value. Meanwhile, Bi-LSTM is an extension of LSTM, improving the execution of the model for sequence classification types of problems [16]. To deal with the constraints of a conventional RNN, Bi-LSTM combines two LSTMs in the training process of the input sequence rather than using an independent LSTM. Better execution and performance were achieved when utilizing Bi-LSTM. Moreover, Bi-LSTM was likewise used to forecast the traffic arrival rate due to its outstanding ability.\n\n\nIII. PROPOSED ARCHITECTURE\n\n\nA. Architecture\n\nOur proposed approach has two primary states to address these above challenges. The purpose of the first state is to find the appropriate parameter values for the primary service function to optimize performance and the latency of service. The other state is responsible for improving the calculation of the number of pods to be more adaptive to the changes in request trends by applying the prediction model Bidirectional LSTM (Bi-LSTM). Although our two states run separately, our activator controls them. This activator decides if parameter values need to be optimized directly activates state 1. After finding appropriate configurations for parameters of the service version and using them to create a new service revision, Knative automatically transfers all 100% current workload to the latest revision, and the activator redirects the system to state 2. At that time, the time series forecasting model Bi-LSTM will be applied to improve the calculation of the number of pods. The decision controller controls the flow and executes results received from two states by communicating with Knative components.\n\n\nB. Parameter values optimization\n\nTo investigate the performance of different configurations, a flexible Kubernetes-based system is designed, as shown in figure 3. The Knative resources are installed on the Kubernetes environment with the help of service mesh Istio. All incoming requests will pass through the Istio ingress before coming to Knative serving components, which control the state of the deployed sample service and enable autoscaling of additional pods based on demand. Due to Knative serving flexibility, we split the workload into two kinds of revisions: a stage revision and a production revision. The stage revision is responsible for testing and calculating the appropriate parameter values for the current load, while the production revision is used to serve the requests continuously. We configure Route components to distribute inbound requests between the stage and production revision on a 25-75% basis. We choose the value of 25% of traffic directed to the stage revision because results extracted from lower values of traffic (e.g., 5-10%) could be affected by the noisy behavior of the system.\n\nThe basic process flow is that we run performance tests for varying concurrency levels according to an individual application. First, we deploy an application and simulate a lot of user requests at the same time over the 50s to reduce other factor effects such as creating new instances time, and the result is more reliable. We collect the output, including information about maximum throughput, request latency distribution, and corresponding concurrency level for each test iteration. The concurrency limit can take any value from 0 to 1000. We begin at a concurrency limit of 10 and use steps of 10 to update the next limit. Until the latency is over than SLO violation, we will stop the process and output the concurrency level with max throughput and acceptable latency. Then we use this output to create a new service revision and transfer 100% workload to the latest revision.\n\n\nC. Workload Forecasting\n\nIn this state, our prediction model receives the metrics extracted from Prometheus, especially the value of the current workload each time interval. Then we sum it up and have the dataset for training the model. We split the dataset into two parts: 80% of the total number for training and 20% remaining for testing. Because Bi-LSTM model input requires historic sub-sequences data, which uses information about n previous time steps to output the next time step prediction. For our datasets, we used ten historic workload values to predict the future workload. We optimize our model by tuning many other parameters so that MEAN Square Error (RMSE) and Mean Absolute Error (MAE) are small as they can. This means the model performance is more precise, and the future workload value is closer than the actual value.\n\n\nIV. PRELIMINARY EXPERIMENTAL RESULTS\n\nWe compare our proposed method to the Knative default configuration of the 100-concurrency level through two different applications. We deploy two applications with different resource consumption per request, application A consumes CPU and memory exclusively, and application B is made for low resource usage. An SLO violation is considered to maintain user experience, we configure 250ms in this paper. Figure 4 show the performance of intensive source application A and B with different concurrency levels. There is a clear trend that could be identified with application A. When the concurrency limit is under 50, all incoming requests can be processed perfectly with very low latency, and the throughput rises steadily. Suddenly, the overall performance plummets greatly when the concurrency level exceeds 50. The number throughput witnessed a significant fall from over 50 to around 35, and the service latency went up surprisingly to more than 2 seconds and caused SLO violation. The appropriate concurrency level of application A is 50 with 50 rps. Compared with the 100-concurrency level, our result not only is the latency low and acceptable but also the throughput is higher by approximately 40%.\n\nMeanwhile, with application B, SLO violation happens when the concurrency level exceeds 120, but the service latency is acceptable at this level. The appropriate concurrency level of application B is 120 with 120 rps. Compared with the 100-concurrency level, although the latency values of the two levels are acceptable max throughput of our method is higher than 20%.  Figure 5 illustrates two methods that calculated the number of pods compared to the number of ideal pods in application with a 10-concurrency level. It can be seen that the number of pods obtained based on the Bi-LSTM model is closer to the number of ideal pods. The reason is that the predicted workload from the Bi-LSTM model is more accurate than the moving average, so the predicted number of pods value can be quickly adaptive to the workload trend.\n\n\nV. CONCLUSION AND FUTURE WORKS\n\nThe serverless architecture brings many advantages to application development and operations, as well as performance challenges. In this work, we find the appropriate parameter values for Knative service revision to improve performance and keep acceptable latency. Besides, we also optimize the calculation of the number of pods for Knative based on the prediction model Bi-LSTM. The preliminary experiments worked well and show better performance than Knative scheme. In the future, we plan to implement Knative's Autoscaler based on our experiments based on the production environment.\n\nFig. 1 .\n1The Serving component objects relationship Knative Serving defines a set of objects as Kubernetes Custom Resource Definitions (CRDs). These objects are used to define and control how your serverless workload behaves on the cluster. The relationship between the resource objects is shown inFig.1. Traffic comes into the cluster via the Knative Route. With the Service resource, a deployed service will automatically have a matching route and configuration created. Each time the service is updated, a new revision is created. By default, the Route sends 100 percent of the traffic to the latest revision, which is configurable. The Knative Pod Autoscaler (KPA) is watching the number of requests received for the revision and will automatically scale the number of pods in the Kubernetes Deployment up and down as needed.\n\nFig. 2 .\n2The Serving component objects relationship\n\nFigure 3 .\n3System architecture of proposed method\n\nFig 5 .\n5The Serving component objects relationship\n\nThe server is dead, long live the server: Rise of serverless computing, overview of current state and future trends in research and industry. P Castro, V Ishakian, V Muthusamy, A Slominski, arXiv:1906.02888arXiv preprintP. Castro, V. Ishakian, V. Muthusamy, and A. Slominski, \"The server is dead, long live the server: Rise of serverless computing, overview of current state and future trends in research and industry,\" arXiv preprint arXiv:1906.02888, 2019.\n\nCloud programming simplified: A berkeley view on serverless computing. E Jonas, Et Al, UC BerkeleyTech. rep.JONAS, E., ET AL. Cloud programming simplified: A berkeley view on serverless computing. Tech. rep., UC Berkeley, 2019\n\nUnderstanding open source serverless platforms: Design considerations and performance. J Li, S G Kulkarni, K Ramakrishnan, D Li, Proceedings of the 5th International Workshop on Serverless Computing. the 5th International Workshop on Serverless ComputingJ. Li, S. G. Kulkarni, K. Ramakrishnan, and D. Li, \"Understanding open source serverless platforms: Design considerations and performance,\" in Proceedings of the 5th International Workshop on Serverless Computing, 2019, pp. 37-42\n\nServerless computing: An investigation of factors influencing microservice performance. W Lloyd, S Ramesh, S Chinthalapati, L Ly, S Pallickara, 2018 IEEE International Conference on Cloud Engineering (IC2E). IEEEW. Lloyd, S. Ramesh, S. Chinthalapati, L. Ly, and S. Pallickara, \"Serverless computing: An investigation of factors influencing microservice performance,\" in 2018 IEEE International Conference on Cloud Engineering (IC2E). IEEE, 2018, pp. 159-169.\n\nPeeking behind the curtains of serverless platforms. L Wang, M Li, Y Zhang, T Ristenpart, M Swift, 2018 {USENIX} Annual Technical Conference ({USENIX}{ATC} 18). L. Wang, M. Li, Y. Zhang, T. Ristenpart, and M. Swift, \"Peeking behind the curtains of serverless platforms,\" in 2018 {USENIX} Annual Technical Conference ({USENIX}{ATC} 18), 2018, pp. 133- 146\n\nEvaluation of production serverless computing environments. H Lee, K Satyam, G Fox, 2018 IEEE 11th International Conference on Cloud Computing (CLOUD). IEEEH. Lee, K. Satyam, and G. Fox, \"Evaluation of production serverless computing environments,\" in 2018 IEEE 11th International Conference on Cloud Computing (CLOUD). IEEE, 2018, pp. 442-450.\n\nResearch on auto-scaling of web applications in cloud: survey, trends and future directions. P Singh, P Gupta, K Jyoti, A Nayyar, Scalable Computing: Practice and Experience. 202P. Singh, P. Gupta, K. Jyoti, and A. Nayyar, \"Research on auto-scaling of web applications in cloud: survey, trends and future directions,\" Scalable Computing: Practice and Experience, vol. 20, no. 2, pp. 399- 432, 2019.\n\nA SPEC RG cloud group's vision on the performance challenges of FaaS cloud architectures. Van Eyk, Alexandru Erwin, Cristina L Iosup, Johannes Abad, Simon Grohmann, Eismann, Companion of the 2018 ACM/SPEC International Conference on Performance Engineering. Van Eyk, Erwin, Alexandru Iosup, Cristina L. Abad, Johannes Grohmann, and Simon Eismann. \"A SPEC RG cloud group's vision on the performance challenges of FaaS cloud architectures.\" In Companion of the 2018 ACM/SPEC International Conference on Performance Engineering, pp. 21-24. 2018.\n\nKnative serving autoscaling system. \"Knative serving autoscaling system,\" https://github.com/knative/ serving/blob/master/docs/scaling/SYSTEM.md, accessed: 2020-03- 26.\n\nMultiresolution analysis of s&p500 time \u02d8 series. D K K\u0131l\u0131c\u00b8, O , U \u00a8 Gur, Annals of Operations Research. 2601-2D. K. K\u0131l\u0131c\u00b8 and O. U \u00a8 gur, \"Multiresolution analysis of s&p500 time \u02d8 series,\" Annals of Operations Research, vol. 260, no. 1-2, pp. 197- 216, 2018.\n\nAutoregressive moving average modeling in the financial sector. P Li, C Jing, T Liang, M Liu, Z Chen, L Guo, 2015 2nd International Conference on Information Technology, Computer, and Electrical Engineering (ICITACEE). IEEEP. Li, C. Jing, T. Liang, M. Liu, Z. Chen, and L. Guo, \"Autoregressive moving average modeling in the financial sector,\" in 2015 2nd International Conference on Information Technology, Computer, and Electrical Engineering (ICITACEE). IEEE, 2015, pp. 68-71.\n\nForecasting financial time series using a methodology based on autoregressive integrated moving average and taylor expansion. G Zhang, X Zhang, H Feng, Expert Systems. 335G. Zhang, X. Zhang, and H. Feng, \"Forecasting financial time series using a methodology based on autoregressive integrated moving average and taylor expansion,\" Expert Systems, vol. 33, no. 5, pp. 501- 516, 2016.\n\nForecasting energy demand in China and India: Using single-linear, hybrid-linear, and nonlinear time series forecast techniques. Qiang ; Wang, Li, ; Shuyu, Rongrong Li, Energy. 161WANG, Qiang; LI, Shuyu; LI, Rongrong. Forecasting energy demand in China and India: Using single-linear, hybrid-linear, and nonlinear time series forecast techniques. Energy, 2018, 161: 821-831.\n\nLearning phrase representations using rnn encoder-decoder for statistical machine translation. K Cho, B Van Merrienboer, C Gulcehre, D Bahdanau, F Bougares, \u00a8 H Schwenk, Y Bengio, arXiv:1406.1078arXiv preprintK. Cho, B. Van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, \u00a8 H. Schwenk, and Y. Bengio, \"Learning phrase representations using rnn encoder-decoder for statistical machine translation,\" arXiv preprint arXiv:1406.1078, 2014.\n\nSingle layer & multi-layer long short-term memory (lstm) model with intermediate variables for weather forecasting. A G Salman, Y Heryadi, E Abdurahman, W Suparta, Procedia CImputer Science. 135A. G. Salman, Y. Heryadi, E. Abdurahman, and W. Suparta, \"Single layer & multi-layer long short-term memory (lstm) model with intermediate variables for weather forecasting,\" Procedia CImputer Science, vol. 135, pp. 89-98, 2018.\n\nThe performance of LSTM and BiLSTM in forecasting time series. Siami-Namini, ; Sima, Tavakoli, ; Neda, Akbar Namin, Siami, 2019 IEEE International Conference on Big Data (Big Data). IEEESIAMI-NAMINI, Sima; TAVAKOLI, Neda; NAMIN, Akbar Siami. The performance of LSTM and BiLSTM in forecasting time series. In: 2019 IEEE International Conference on Big Data (Big Data). IEEE, 2019. p. 3285-3\n", "annotations": {"author": "[{\"end\":165,\"start\":58},{\"end\":267,\"start\":166}]", "publisher": null, "author_last_name": "[{\"end\":72,\"start\":67},{\"end\":178,\"start\":175}]", "author_first_name": "[{\"end\":66,\"start\":58},{\"end\":174,\"start\":166}]", "author_affiliation": "[{\"end\":164,\"start\":101},{\"end\":266,\"start\":203}]", "title": "[{\"end\":55,\"start\":1},{\"end\":322,\"start\":268}]", "venue": null, "abstract": "[{\"end\":2156,\"start\":401}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2416,\"start\":2413},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2724,\"start\":2721},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3389,\"start\":3386},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3541,\"start\":3538},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3569,\"start\":3566},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3598,\"start\":3595},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3682,\"start\":3679},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4099,\"start\":4096},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4743,\"start\":4740},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4966,\"start\":4963},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5074,\"start\":5071},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9730,\"start\":9726},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9740,\"start\":9736},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10075,\"start\":10071},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":10455,\"start\":10451},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10730,\"start\":10726},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11254,\"start\":11250}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":19134,\"start\":18303},{\"attributes\":{\"id\":\"fig_1\"},\"end\":19188,\"start\":19135},{\"attributes\":{\"id\":\"fig_2\"},\"end\":19240,\"start\":19189},{\"attributes\":{\"id\":\"fig_3\"},\"end\":19293,\"start\":19241}]", "paragraph": "[{\"end\":3181,\"start\":2175},{\"end\":4544,\"start\":3183},{\"end\":5457,\"start\":4546},{\"end\":6536,\"start\":5459},{\"end\":7826,\"start\":6588},{\"end\":8355,\"start\":7828},{\"end\":8699,\"start\":8394},{\"end\":9153,\"start\":8701},{\"end\":9648,\"start\":9178},{\"end\":11595,\"start\":9650},{\"end\":12756,\"start\":11644},{\"end\":13879,\"start\":12793},{\"end\":14765,\"start\":13881},{\"end\":15607,\"start\":14793},{\"end\":16854,\"start\":15648},{\"end\":17680,\"start\":16856},{\"end\":18302,\"start\":17715}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8393,\"start\":8356}]", "table_ref": null, "section_header": "[{\"end\":2173,\"start\":2158},{\"end\":6553,\"start\":6539},{\"end\":6586,\"start\":6556},{\"end\":9176,\"start\":9156},{\"end\":11624,\"start\":11598},{\"end\":11642,\"start\":11627},{\"end\":12791,\"start\":12759},{\"end\":14791,\"start\":14768},{\"end\":15646,\"start\":15610},{\"end\":17713,\"start\":17683},{\"end\":18312,\"start\":18304},{\"end\":19144,\"start\":19136},{\"end\":19200,\"start\":19190},{\"end\":19249,\"start\":19242}]", "table": null, "figure_caption": "[{\"end\":19134,\"start\":18314},{\"end\":19188,\"start\":19146},{\"end\":19240,\"start\":19202},{\"end\":19293,\"start\":19251}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":7251,\"start\":7244},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12921,\"start\":12913},{\"end\":16060,\"start\":16052},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":17234,\"start\":17226}]", "bib_author_first_name": "[{\"end\":19438,\"start\":19437},{\"end\":19448,\"start\":19447},{\"end\":19460,\"start\":19459},{\"end\":19473,\"start\":19472},{\"end\":19827,\"start\":19826},{\"end\":20071,\"start\":20070},{\"end\":20077,\"start\":20076},{\"end\":20079,\"start\":20078},{\"end\":20091,\"start\":20090},{\"end\":20107,\"start\":20106},{\"end\":20557,\"start\":20556},{\"end\":20566,\"start\":20565},{\"end\":20576,\"start\":20575},{\"end\":20593,\"start\":20592},{\"end\":20599,\"start\":20598},{\"end\":20982,\"start\":20981},{\"end\":20990,\"start\":20989},{\"end\":20996,\"start\":20995},{\"end\":21005,\"start\":21004},{\"end\":21019,\"start\":21018},{\"end\":21345,\"start\":21344},{\"end\":21352,\"start\":21351},{\"end\":21362,\"start\":21361},{\"end\":21724,\"start\":21723},{\"end\":21733,\"start\":21732},{\"end\":21742,\"start\":21741},{\"end\":21751,\"start\":21750},{\"end\":22138,\"start\":22129},{\"end\":22154,\"start\":22146},{\"end\":22156,\"start\":22155},{\"end\":22172,\"start\":22164},{\"end\":22184,\"start\":22179},{\"end\":22795,\"start\":22794},{\"end\":22797,\"start\":22796},{\"end\":22807,\"start\":22806},{\"end\":22813,\"start\":22810},{\"end\":23073,\"start\":23072},{\"end\":23079,\"start\":23078},{\"end\":23087,\"start\":23086},{\"end\":23096,\"start\":23095},{\"end\":23103,\"start\":23102},{\"end\":23111,\"start\":23110},{\"end\":23616,\"start\":23615},{\"end\":23625,\"start\":23624},{\"end\":23634,\"start\":23633},{\"end\":24008,\"start\":24003},{\"end\":24010,\"start\":24009},{\"end\":24022,\"start\":24021},{\"end\":24038,\"start\":24030},{\"end\":24346,\"start\":24345},{\"end\":24353,\"start\":24352},{\"end\":24372,\"start\":24371},{\"end\":24384,\"start\":24383},{\"end\":24396,\"start\":24395},{\"end\":24408,\"start\":24407},{\"end\":24410,\"start\":24409},{\"end\":24421,\"start\":24420},{\"end\":24808,\"start\":24807},{\"end\":24810,\"start\":24809},{\"end\":24820,\"start\":24819},{\"end\":24831,\"start\":24830},{\"end\":24845,\"start\":24844},{\"end\":25193,\"start\":25192},{\"end\":25211,\"start\":25210},{\"end\":25223,\"start\":25218}]", "bib_author_last_name": "[{\"end\":19445,\"start\":19439},{\"end\":19457,\"start\":19449},{\"end\":19470,\"start\":19461},{\"end\":19483,\"start\":19474},{\"end\":19833,\"start\":19828},{\"end\":19840,\"start\":19835},{\"end\":20074,\"start\":20072},{\"end\":20088,\"start\":20080},{\"end\":20104,\"start\":20092},{\"end\":20110,\"start\":20108},{\"end\":20563,\"start\":20558},{\"end\":20573,\"start\":20567},{\"end\":20590,\"start\":20577},{\"end\":20596,\"start\":20594},{\"end\":20610,\"start\":20600},{\"end\":20987,\"start\":20983},{\"end\":20993,\"start\":20991},{\"end\":21002,\"start\":20997},{\"end\":21016,\"start\":21006},{\"end\":21025,\"start\":21020},{\"end\":21349,\"start\":21346},{\"end\":21359,\"start\":21353},{\"end\":21366,\"start\":21363},{\"end\":21730,\"start\":21725},{\"end\":21739,\"start\":21734},{\"end\":21748,\"start\":21743},{\"end\":21758,\"start\":21752},{\"end\":22127,\"start\":22120},{\"end\":22144,\"start\":22139},{\"end\":22162,\"start\":22157},{\"end\":22177,\"start\":22173},{\"end\":22193,\"start\":22185},{\"end\":22202,\"start\":22195},{\"end\":22804,\"start\":22798},{\"end\":22817,\"start\":22814},{\"end\":23076,\"start\":23074},{\"end\":23084,\"start\":23080},{\"end\":23093,\"start\":23088},{\"end\":23100,\"start\":23097},{\"end\":23108,\"start\":23104},{\"end\":23115,\"start\":23112},{\"end\":23622,\"start\":23617},{\"end\":23631,\"start\":23626},{\"end\":23639,\"start\":23635},{\"end\":24015,\"start\":24011},{\"end\":24019,\"start\":24017},{\"end\":24028,\"start\":24023},{\"end\":24041,\"start\":24039},{\"end\":24350,\"start\":24347},{\"end\":24369,\"start\":24354},{\"end\":24381,\"start\":24373},{\"end\":24393,\"start\":24385},{\"end\":24405,\"start\":24397},{\"end\":24418,\"start\":24411},{\"end\":24428,\"start\":24422},{\"end\":24817,\"start\":24811},{\"end\":24828,\"start\":24821},{\"end\":24842,\"start\":24832},{\"end\":24853,\"start\":24846},{\"end\":25190,\"start\":25178},{\"end\":25198,\"start\":25194},{\"end\":25208,\"start\":25200},{\"end\":25216,\"start\":25212},{\"end\":25229,\"start\":25224},{\"end\":25236,\"start\":25231}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1906.02888\",\"id\":\"b0\"},\"end\":19753,\"start\":19295},{\"attributes\":{\"id\":\"b1\"},\"end\":19981,\"start\":19755},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":208139154},\"end\":20466,\"start\":19983},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":21727593},\"end\":20926,\"start\":20468},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":51870894},\"end\":21282,\"start\":20928},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":4740131},\"end\":21628,\"start\":21284},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":155420596},\"end\":22028,\"start\":21630},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":4718290},\"end\":22572,\"start\":22030},{\"attributes\":{\"id\":\"b8\"},\"end\":22742,\"start\":22574},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":35875362},\"end\":23006,\"start\":22744},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":9477094},\"end\":23487,\"start\":23008},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":30143685},\"end\":23872,\"start\":23489},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":116058925},\"end\":24248,\"start\":23874},{\"attributes\":{\"doi\":\"arXiv:1406.1078\",\"id\":\"b13\"},\"end\":24689,\"start\":24250},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":69331029},\"end\":25113,\"start\":24691},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":211297310},\"end\":25504,\"start\":25115}]", "bib_title": "[{\"end\":20068,\"start\":19983},{\"end\":20554,\"start\":20468},{\"end\":20979,\"start\":20928},{\"end\":21342,\"start\":21284},{\"end\":21721,\"start\":21630},{\"end\":22118,\"start\":22030},{\"end\":22792,\"start\":22744},{\"end\":23070,\"start\":23008},{\"end\":23613,\"start\":23489},{\"end\":24001,\"start\":23874},{\"end\":24805,\"start\":24691},{\"end\":25176,\"start\":25115}]", "bib_author": "[{\"end\":19447,\"start\":19437},{\"end\":19459,\"start\":19447},{\"end\":19472,\"start\":19459},{\"end\":19485,\"start\":19472},{\"end\":19835,\"start\":19826},{\"end\":19842,\"start\":19835},{\"end\":20076,\"start\":20070},{\"end\":20090,\"start\":20076},{\"end\":20106,\"start\":20090},{\"end\":20112,\"start\":20106},{\"end\":20565,\"start\":20556},{\"end\":20575,\"start\":20565},{\"end\":20592,\"start\":20575},{\"end\":20598,\"start\":20592},{\"end\":20612,\"start\":20598},{\"end\":20989,\"start\":20981},{\"end\":20995,\"start\":20989},{\"end\":21004,\"start\":20995},{\"end\":21018,\"start\":21004},{\"end\":21027,\"start\":21018},{\"end\":21351,\"start\":21344},{\"end\":21361,\"start\":21351},{\"end\":21368,\"start\":21361},{\"end\":21732,\"start\":21723},{\"end\":21741,\"start\":21732},{\"end\":21750,\"start\":21741},{\"end\":21760,\"start\":21750},{\"end\":22129,\"start\":22120},{\"end\":22146,\"start\":22129},{\"end\":22164,\"start\":22146},{\"end\":22179,\"start\":22164},{\"end\":22195,\"start\":22179},{\"end\":22204,\"start\":22195},{\"end\":22806,\"start\":22794},{\"end\":22810,\"start\":22806},{\"end\":22819,\"start\":22810},{\"end\":23078,\"start\":23072},{\"end\":23086,\"start\":23078},{\"end\":23095,\"start\":23086},{\"end\":23102,\"start\":23095},{\"end\":23110,\"start\":23102},{\"end\":23117,\"start\":23110},{\"end\":23624,\"start\":23615},{\"end\":23633,\"start\":23624},{\"end\":23641,\"start\":23633},{\"end\":24017,\"start\":24003},{\"end\":24021,\"start\":24017},{\"end\":24030,\"start\":24021},{\"end\":24043,\"start\":24030},{\"end\":24352,\"start\":24345},{\"end\":24371,\"start\":24352},{\"end\":24383,\"start\":24371},{\"end\":24395,\"start\":24383},{\"end\":24407,\"start\":24395},{\"end\":24420,\"start\":24407},{\"end\":24430,\"start\":24420},{\"end\":24819,\"start\":24807},{\"end\":24830,\"start\":24819},{\"end\":24844,\"start\":24830},{\"end\":24855,\"start\":24844},{\"end\":25192,\"start\":25178},{\"end\":25200,\"start\":25192},{\"end\":25210,\"start\":25200},{\"end\":25218,\"start\":25210},{\"end\":25231,\"start\":25218},{\"end\":25238,\"start\":25231}]", "bib_venue": "[{\"end\":19435,\"start\":19295},{\"end\":19824,\"start\":19755},{\"end\":20181,\"start\":20112},{\"end\":20674,\"start\":20612},{\"end\":21087,\"start\":21027},{\"end\":21434,\"start\":21368},{\"end\":21803,\"start\":21760},{\"end\":22286,\"start\":22204},{\"end\":22608,\"start\":22574},{\"end\":22848,\"start\":22819},{\"end\":23225,\"start\":23117},{\"end\":23655,\"start\":23641},{\"end\":24049,\"start\":24043},{\"end\":24343,\"start\":24250},{\"end\":24880,\"start\":24855},{\"end\":25295,\"start\":25238},{\"end\":20237,\"start\":20183}]"}}}, "year": 2023, "month": 12, "day": 17}